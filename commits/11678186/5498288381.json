[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 103, "deletions": 62, "changes": 165, "file_content_changes": "@@ -19,25 +19,28 @@ jobs:\n   Runner-Preparation:\n     runs-on: ubuntu-latest\n     outputs:\n-      matrix: ${{ steps.set-matrix.outputs.matrix }}\n+      matrix-required: ${{ steps.set-matrix.outputs.matrix-required }}\n+      matrix-optional: ${{ steps.set-matrix.outputs.matrix-optional }}\n     steps:\n       - name: Prepare runner matrix\n         id: set-matrix\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n-            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"H100\"], [\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]'\n+            echo '::set-output name=matrix-required::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"H100\"]]'\n+            echo '::set-output name=matrix-optional::[[\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]'\n           else\n-            echo '::set-output name=matrix::[\"ubuntu-latest\"]'\n+            echo '::set-output name=matrix-required::[\"ubuntu-latest\"]'\n+            echo '::set-output name=matrix-optional::[\"ubuntu-latest\"]'\n           fi\n \n-  Integration-Tests:\n+  Integration-Tests-Nvidia:\n     needs: Runner-Preparation\n \n     runs-on: ${{ matrix.runner }}\n \n     strategy:\n       matrix:\n-        runner: ${{fromJson(needs.Runner-Preparation.outputs.matrix)}}\n+        runner: ${{fromJson(needs.Runner-Preparation.outputs.matrix-required)}}\n \n     steps:\n       - name: Checkout\n@@ -48,6 +51,84 @@ jobs:\n         run: |\n           echo \"BACKEND=CUDA\" >> \"${GITHUB_ENV}\"\n \n+      - name: Clear cache\n+        run: |\n+          rm -rf ~/.triton\n+\n+      - name: Update PATH\n+        run: |\n+          echo \"PATH=${HOME}/.local/bin:${PATH}\" >> \"${GITHUB_ENV}\"\n+\n+      - name: Install Triton\n+        if: ${{ env.BACKEND == 'CUDA'}}\n+        run: |\n+          cd python\n+          python3 -m pip install --upgrade pip\n+          python3 -m pip install cmake==3.24\n+          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n+          python3 -m pip install pytest-xdist\n+\n+      - name: Run lit tests\n+        if: ${{ env.BACKEND == 'CUDA'}}\n+        run: |\n+          python3 -m pip install lit\n+          cd python\n+          LIT_TEST_DIR=\"build/$(ls build | grep -i cmake)/test\"\n+          if [ ! -d \"${LIT_TEST_DIR}\" ]; then\n+            echo \"Coult not find '${LIT_TEST_DIR}'\" ; exit -1\n+          fi\n+          lit -v \"${LIT_TEST_DIR}\"\n+\n+      - name: Run python tests on CUDA\n+        if: ${{ env.BACKEND == 'CUDA'}}\n+        run: |\n+          cd python/test/unit\n+          python3 -m pytest -n 8 --ignore=runtime\n+          # run runtime tests serially to avoid race condition with cache handling.\n+          python3 -m pytest runtime/\n+\n+      - name: Create artifacts archive\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n+        run: |\n+          cd ~/.triton\n+          tar -czvf artifacts.tar.gz cache\n+\n+      - name: Upload artifacts archive\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n+        uses: actions/upload-artifact@v2\n+        with:\n+          name: artifacts ${{ matrix.runner[1] }}\n+          path: ~/.triton/artifacts.tar.gz\n+\n+      - name: Run CXX unittests\n+        if: ${{ env.BACKEND == 'CUDA'}}\n+        run: |\n+          cd python\n+          cd \"build/$(ls build | grep -i cmake)\"\n+          ctest\n+\n+      - name: Regression tests\n+        if: ${{ contains(matrix.runner, 'A100') }}\n+        run: |\n+          cd python/test/regression\n+          sudo nvidia-smi -i 0 -pm 1\n+          sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n+          python3 -m pytest -vs .\n+          sudo nvidia-smi -i 0 -rgc\n+\n+  Integration-Tests-Third-Party:\n+    needs: Runner-Preparation\n+\n+    runs-on: ${{ matrix.runner }}\n+\n+    strategy:\n+      matrix:\n+        runner: ${{fromJson(needs.Runner-Preparation.outputs.matrix-optional)}}\n+\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v2\n+\n       - name: Set ROCM ENV\n         if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'gfx908')}}\n         run: |\n@@ -81,14 +162,6 @@ jobs:\n           python3 -m pip install --upgrade pre-commit\n           python3 -m pre_commit run --all-files\n \n-      - name: Install Triton\n-        if: ${{ env.BACKEND == 'CUDA'}}\n-        run: |\n-          cd python\n-          python3 -m pip install --upgrade pip\n-          python3 -m pip install cmake==3.24\n-          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n-\n       - name: Install Triton on ROCM\n         if: ${{ env.BACKEND == 'ROCM'}}\n         run: |\n@@ -113,43 +186,6 @@ jobs:\n           python3 setup.py build\n           python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n-      - name: Run lit tests\n-        if: ${{ env.BACKEND == 'CUDA'}}\n-        run: |\n-          python3 -m pip install lit\n-          cd python\n-          LIT_TEST_DIR=\"build/$(ls build | grep -i cmake)/test\"\n-          if [ ! -d \"${LIT_TEST_DIR}\" ]; then\n-            echo \"Coult not find '${LIT_TEST_DIR}'\" ; exit -1\n-          fi\n-          lit -v \"${LIT_TEST_DIR}\"\n-\n-      - name: Run python tests on CUDA\n-        if: ${{ env.BACKEND == 'CUDA'}}\n-        run: |\n-          cd python/test/unit\n-          python3 -m pytest\n-\n-      - name: Create artifacts archive\n-        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n-        run: |\n-          cd ~/.triton\n-          tar -czvf artifacts.tar.gz cache\n-\n-      - name: Upload artifacts archive\n-        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n-        uses: actions/upload-artifact@v2\n-        with:\n-          name: artifacts ${{ matrix.runner[1] }}\n-          path: ~/.triton/artifacts.tar.gz\n-\n-      - name: Run CXX unittests\n-        if: ${{ env.BACKEND == 'CUDA'}}\n-        run: |\n-          cd python\n-          cd \"build/$(ls build | grep -i cmake)\"\n-          ctest\n-\n       - name: Run python tests on ROCM\n         if: ${{ env.BACKEND == 'ROCM'}}\n         run: |\n@@ -165,17 +201,8 @@ jobs:\n           cd python/test/backend/third_party_backends\n           python3 -m pytest --capture=tee-sys -rfs --verbose --backend xpu\n \n-      - name: Regression tests\n-        if: ${{ contains(matrix.runner, 'A100') }}\n-        run: |\n-          cd python/test/regression\n-          sudo nvidia-smi -i 0 -pm 1\n-          sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n-          python3 -m pytest -vs .\n-          sudo nvidia-smi -i 0 -rgc\n-\n   Compare-artifacts:\n-    needs: Integration-Tests\n+    needs: Integration-Tests-Nvidia\n \n     runs-on: ubuntu-latest\n \n@@ -193,7 +220,7 @@ jobs:\n       - name: Download latest main artifacts\n         env:\n           ARTIFACT_NAME: artifacts A100\n-          ARTIFACT_JOB_NAME: Integration-Tests\n+          ARTIFACT_JOB_NAME: Integration-Tests-Nvidia\n           GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n         run: |\n           OWNER_REPO=\"${{ github.repository }}\"\n@@ -202,7 +229,19 @@ jobs:\n \n           BRANCH_NAME=$(gh api repos/$OWNER_REPO/pulls/$PR_NUMBER --jq '.head.ref')\n           echo \"BRANCH_NAME: $BRANCH_NAME\"\n-          WORKFLOW_RUN_ID=$(gh api --method GET repos/$OWNER_REPO/actions/runs | jq --arg branch_name \"$BRANCH_NAME\" '.workflow_runs[] | select(.head_branch == $branch_name)' | jq '.id' | head -1)\n+\n+          page=1\n+          while true; do\n+            run_id=$(gh api --method GET \"repos/$OWNER_REPO/actions/runs?page=$page&per_page=100\" | jq --arg branch_name \"$BRANCH_NAME\" '.workflow_runs[] | select(.head_branch == $branch_name)' | jq '.id' | head -1)\n+            if [ \"$run_id\" != \"\" ]; then\n+              echo \"First run ID on branch $BRANCH_NAME is: $run_id\"\n+              WORKFLOW_RUN_ID=$run_id\n+              break\n+            fi\n+\n+            ((page++))\n+          done\n+\n           echo \"WORKFLOW_RUN_ID: $WORKFLOW_RUN_ID\"\n           ARTIFACT_URL=$(gh api repos/$OWNER_REPO/actions/runs/$WORKFLOW_RUN_ID/artifacts | jq --arg artifact_name \"$ARTIFACT_NAME\" '.artifacts[] | select(.name == $artifact_name).archive_download_url' --raw-output)\n           echo \"ARTIFACT_URL: $ARTIFACT_URL\"\n@@ -220,6 +259,7 @@ jobs:\n             mv cache reference\n           else\n             echo \"No artifact found with the name: $ARTIFACT_NAME\"\n+            exit 1\n           fi\n       - name: Download current job artifacts\n         uses: actions/download-artifact@v2\n@@ -270,6 +310,7 @@ jobs:\n         if: ${{ env.COMPARISON_RESULT == 'false' }}\n         uses: actions/github-script@v5\n         with:\n+          github-token: ${{ secrets.CI_ACCESS_TOKEN }}\n           script: |\n             const run_id = ${{ env.RUN_ID }};\n             const issue_number = context.payload.pull_request.number;"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 53, "deletions": 53, "changes": 106, "file_content_changes": "@@ -86,59 +86,59 @@ if(NOT MLIR_DIR)\n   else()\n     set(LLVM_LDFLAGS \"-L${LLVM_LIBRARY_DIR}\")\n     set(LLVM_LIBRARIES\n-      libLLVMNVPTXCodeGen.a\n-      libLLVMNVPTXDesc.a\n-      libLLVMNVPTXInfo.a\n-      libLLVMAMDGPUDisassembler.a\n-      libLLVMMCDisassembler.a\n-      libLLVMAMDGPUCodeGen.a\n-      libLLVMMIRParser.a\n-      libLLVMGlobalISel.a\n-      libLLVMSelectionDAG.a\n-      libLLVMipo.a\n-      libLLVMInstrumentation.a\n-      libLLVMVectorize.a\n-      libLLVMLinker.a\n-      libLLVMIRReader.a\n-      libLLVMAsmParser.a\n-      libLLVMFrontendOpenMP.a\n-      libLLVMAsmPrinter.a\n-      libLLVMDebugInfoDWARF.a\n-      libLLVMCodeGen.a\n-      libLLVMTarget.a\n-      libLLVMScalarOpts.a\n-      libLLVMInstCombine.a\n-      libLLVMAggressiveInstCombine.a\n-      libLLVMTransformUtils.a\n-      libLLVMBitWriter.a\n-      libLLVMAnalysis.a\n-      libLLVMProfileData.a\n-      libLLVMObject.a\n-      libLLVMTextAPI.a\n-      libLLVMBitReader.a\n-      libLLVMAMDGPUAsmParser.a\n-      libLLVMMCParser.a\n-      libLLVMAMDGPUDesc.a\n-      libLLVMAMDGPUUtils.a\n-      libLLVMMC.a\n-      libLLVMDebugInfoCodeView.a\n-      libLLVMDebugInfoMSF.a\n-      libLLVMCore.a\n-      libLLVMRemarks.a\n-      libLLVMBitstreamReader.a\n-      libLLVMBinaryFormat.a\n-      libLLVMAMDGPUInfo.a\n-      libLLVMSupport.a\n-      libLLVMDemangle.a\n-      libLLVMPasses.a\n-      libLLVMAnalysis.a\n-      libLLVMTransformUtils.a\n-      libLLVMScalarOpts.a\n-      libLLVMTransformUtils.a\n-      libLLVMipo.a\n-      libLLVMObjCARCOpts.a\n-      libLLVMCoroutines.a\n-      libLLVMAnalysis.a\n+      LLVMNVPTXCodeGen\n+      LLVMNVPTXDesc\n+      LLVMNVPTXInfo\n+      LLVMAMDGPUDisassembler\n+      LLVMMCDisassembler\n+      LLVMAMDGPUCodeGen\n+      LLVMMIRParser\n+      LLVMGlobalISel\n+      LLVMSelectionDAG\n+      LLVMipo\n+      LLVMInstrumentation\n+      LLVMVectorize\n+      LLVMLinker\n+      LLVMIRReader\n+      LLVMAsmParser\n+      LLVMFrontendOpenMP\n+      LLVMAsmPrinter\n+      LLVMDebugInfoDWARF\n+      LLVMCodeGen\n+      LLVMTarget\n+      LLVMScalarOpts\n+      LLVMInstCombine\n+      LLVMAggressiveInstCombine\n+      LLVMTransformUtils\n+      LLVMBitWriter\n+      LLVMAnalysis\n+      LLVMProfileData\n+      LLVMObject\n+      LLVMTextAPI\n+      LLVMBitReader\n+      LLVMAMDGPUAsmParser\n+      LLVMMCParser\n+      LLVMAMDGPUDesc\n+      LLVMAMDGPUUtils\n+      LLVMMC\n+      LLVMDebugInfoCodeView\n+      LLVMDebugInfoMSF\n+      LLVMCore\n+      LLVMRemarks\n+      LLVMBitstreamReader\n+      LLVMBinaryFormat\n+      LLVMAMDGPUInfo\n+      LLVMSupport\n+      LLVMDemangle\n+      LLVMPasses\n+      LLVMAnalysis\n+      LLVMTransformUtils\n+      LLVMScalarOpts\n+      LLVMTransformUtils\n+      LLVMipo\n+      LLVMObjCARCOpts\n+      LLVMCoroutines\n+      LLVMAnalysis\n     )\n   endif()\n "}, {"filename": "docs/meetups/07-19-2023.md", "status": "added", "additions": 13, "deletions": 0, "changes": 13, "file_content_changes": "@@ -0,0 +1,13 @@\n+#### Agenda:\n+1. Alternative backend development approach (e.g. AMD, Intel)\n+2. State of the documentation, is there a planned effort? If yes, what do you think is the priority?\n+3. Mechanisms for smaller technical discussions: Slack channel per topic? Dedicated meetings for some topics?\n+4. Stability, testing, regressions: Improving CI and conformance/testing for validating new back-ends.\n+5. Language improvements/pain points\n+6. Discussion of known/anticipated design changes for H100\n+7. Some specific more tactical areas:\n+   - int8.\n+   - A low hanging fruit is to let tl.dot take int8 and leverage mma.\n+   - Sm75.\n+   - device functions. How hard is this to support while Triton frontend traverses AST?\n+   - remove torch dependencies from the frontend. (it sounds like there is already progress on this but could be worth discussing)"}, {"filename": "docs/python-api/triton.language.rst", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -111,6 +111,16 @@ Reduction Ops\n     sum\n     xor_sum\n \n+Scan Ops\n+-------------\n+\n+.. autosummary::\n+    :toctree: generated\n+    :nosignatures:\n+\n+    associative_scan\n+    cumsum\n+    cumprod\n \n Atomic Ops\n ----------"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 44, "deletions": 0, "changes": 44, "file_content_changes": "@@ -63,6 +63,50 @@ class ReduceOpHelper {\n   int axis;\n };\n \n+class ScanLoweringHelper {\n+public:\n+  explicit ScanLoweringHelper(triton::ScanOp op) : scanOp(op) {\n+    auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+    srcEncoding = type.getEncoding();\n+  }\n+  // Return true if the lowering of the scan op is supported.\n+  bool isSupported();\n+  // Return the number of elements per thread along axis dim.\n+  unsigned getAxisNumElementsPerThread();\n+  // Return the number of elements per thread along non-axis dims.\n+  unsigned getNonAxisNumElementsPerThread();\n+  // Return the number of threads per warp along non-axis dims.\n+  unsigned getNonAxisNumThreadsPerWarp();\n+  // Return the flat numbers of threads computing independent scan results.\n+  unsigned getNonAxisNumThreadsPerCTA();\n+  // Return the number of warps per CTA along axis dim.\n+  unsigned getAxisNumWarps();\n+  // Return the number of threads per warp along axis dim.\n+  unsigned getAxisNumThreadsPerWarp();\n+  // Return the number of blocks along axis dim.\n+  unsigned getAxisNumBlocks();\n+  // Return the number of blocks along non axis dim.\n+  unsigned getNonAxisNumBlocks();\n+  // Return the size of the scratch space needed for scan lowering.\n+  unsigned getScratchSizeInBytes();\n+\n+  // Stride between contiguous element along axis dim.\n+  unsigned getAxisElementStride();\n+  // Stride between contiguous threads along axis dim.\n+  unsigned getAxisThreadStride();\n+  // Stride between contiguous blocks along axis dim.\n+  unsigned getAxisBlockStride();\n+\n+  Location getLoc() { return scanOp.getLoc(); }\n+  unsigned getAxis() { return scanOp.getAxis(); }\n+  triton::gpu::BlockedEncodingAttr getEncoding();\n+  Region &getCombineOp();\n+\n+private:\n+  triton::ScanOp scanOp;\n+  Attribute srcEncoding;\n+};\n+\n bool maybeSharedAllocationOp(Operation *op);\n \n bool maybeAliasOp(Operation *op);"}, {"filename": "include/triton/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,2 +1,3 @@\n add_subdirectory(Conversion)\n add_subdirectory(Dialect)\n+add_subdirectory(Target)"}, {"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -12,6 +12,7 @@ def TT_CacheModifierAttr : I32EnumAttr<\n         I32EnumAttrCase<\"CG\", 3, \"cg\">,\n         I32EnumAttrCase<\"WB\", 4, \"wb\">,\n         I32EnumAttrCase<\"CS\", 5, \"cs\">,\n+        I32EnumAttrCase<\"WT\", 6, \"wt\">,\n     ]> {\n     let cppNamespace = \"::mlir::triton\";\n }"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 26, "deletions": 0, "changes": 26, "file_content_changes": "@@ -426,6 +426,32 @@ def TT_ReduceReturnOp: TT_Op<\"reduce.return\",\n     let assemblyFormat = \"$result attr-dict `:` type($result)\";\n }\n \n+//\n+// Scan Op\n+//\n+def TT_ScanOp: TT_Op<\"scan\",\n+                       [Pure,\n+                        SameOperandsAndResultEncoding,\n+                        SameOperandsAndResultElementType,\n+                        SingleBlock,\n+                        DeclareOpInterfaceMethods<InferTypeOpInterface>]> {\n+    let summary = \"Reduction using generic combination algorithm\";\n+    let arguments = (ins Variadic<TT_Tensor>:$operands, I32Attr:$axis);\n+    let results = (outs Variadic<TT_Tensor>:$result);\n+    let regions = (region SizedRegion<1>:$combineOp);\n+    let builders = [\n+        OpBuilder<(ins \"ValueRange\":$operands, \"int\":$axis)>,\n+    ];\n+    let hasVerifier = 1;\n+}\n+\n+def TT_ScanReturnOp: TT_Op<\"scan.return\",\n+                             [HasParent<\"ScanOp\">, Pure, Terminator, ReturnLike]> {\n+    let summary = \"terminator for scan operator\";\n+    let arguments = (ins Variadic<AnyType>:$result);\n+    let assemblyFormat = \"$result attr-dict `:` type($result)\";\n+}\n+\n \n //\n // External Elementwise op"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Attributes.h", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -0,0 +1,7 @@\n+#ifndef TRITON_DIALECT_TRITONGPU_IR_ATTRIBUTES_H_\n+#define TRITON_DIALECT_TRITONGPU_IR_ATTRIBUTES_H_\n+\n+#define GET_ATTRDEF_CLASSES\n+#include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.h.inc\"\n+\n+#endif // TRITON_DIALECT_TRITONGPU_IR_ATTRIBUTES_H_"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -8,12 +8,10 @@\n \n // TritonGPU depends on Triton\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Attributes.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h.inc\"\n #include \"triton/Dialect/TritonGPU/IR/Traits.h\"\n \n-#define GET_ATTRDEF_CLASSES\n-#include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.h.inc\"\n-\n #define GET_OP_CLASSES\n #include \"triton/Dialect/TritonGPU/IR/Ops.h.inc\"\n \n@@ -51,7 +49,8 @@ SmallVector<unsigned> getContigPerThread(Attribute layout);\n // for thread 0 would be [A_{0, 0}, A_{0, 0}, A_{0, 0}, A_{0, 0}], returns [1,\n // 1]. Whereas for a tensor shape [128, 128], the elements for thread 0 would be\n // [A_{0, 0}, A_{0, 1}, A_{0, 2}, A_{0, 3}], returns [1, 4].\n-SmallVector<unsigned> getUniqueContigPerThread(Type type);\n+SmallVector<unsigned> getUniqueContigPerThread(Attribute layout,\n+                                               ArrayRef<int64_t> tensorShape);\n \n // Returns the number of threads per warp that have access to non-replicated\n // elements of the tensor. E.g. for a blocked layout with sizePerThread = [1,\n@@ -82,9 +81,10 @@ bool isaDistributedLayout(Attribute layout);\n \n bool isSharedEncoding(Value value);\n \n+bool isExpensiveCat(CatOp cat, Attribute &targetEncoding);\n+\n } // namespace gpu\n } // namespace triton\n-\n } // namespace mlir\n \n #endif // TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -18,8 +18,6 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n \n bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n \n-bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding);\n-\n bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding);\n \n // skipInit is True when we only consider the operands of the initOp but"}, {"filename": "include/triton/Target/CMakeLists.txt", "status": "added", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -0,0 +1 @@\n+add_subdirectory(LLVMIR)"}, {"filename": "include/triton/Target/LLVMIR/CMakeLists.txt", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -0,0 +1,3 @@\n+set(LLVM_TARGET_DEFINITIONS Passes.td)\n+mlir_tablegen(Passes.h.inc -gen-pass-decls -name LLVMIR)\n+add_public_tablegen_target(LLVMIRIncGen)"}, {"filename": "include/triton/Target/LLVMIR/LLVMIRTranslation.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n-#ifndef TRITON_TARGET_LLVMIRTRANSLATION_H\n-#define TRITON_TARGET_LLVMIRTRANSLATION_H\n+#ifndef TRITON_TARGET_LLVM_IR_LLVM_IR_TRANSLATION_H\n+#define TRITON_TARGET_LLVM_IR_LLVM_IR_TRANSLATION_H\n #include \"llvm/ADT/StringRef.h\"\n #include <memory>\n #include <string>"}, {"filename": "include/triton/Target/LLVMIR/Passes.h", "status": "added", "additions": 17, "deletions": 0, "changes": 17, "file_content_changes": "@@ -0,0 +1,17 @@\n+#ifndef TRITON_TARGET_LLVM_IR_PASSES_H\n+#define TRITON_TARGET_LLVM_IR_PASSES_H\n+\n+#include \"mlir/Pass/Pass.h\"\n+\n+namespace mlir {\n+\n+/// Create a pass to add DIScope\n+std::unique_ptr<Pass> createLLVMDIScopePass();\n+\n+/// Generate the code for registering conversion passes.\n+#define GEN_PASS_REGISTRATION\n+#include \"triton/Target/LLVMIR/Passes.h.inc\"\n+\n+} // namespace mlir\n+\n+#endif // TRITON_TARGET_LLVM_IR_PASSES_H"}, {"filename": "include/triton/Target/LLVMIR/Passes.td", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -0,0 +1,15 @@\n+#ifndef TRITON_TARGET_LLVMIR_PASSES\n+#define TRITON_TARGET_LLVMIR_PASSES\n+\n+include \"mlir/Pass/PassBase.td\"\n+\n+def LLVMDIScope: Pass<\"enable-line-info\", \"mlir::ModuleOp\"> {\n+  let summary = \"Materialize LLVM line info\";\n+  let description = [{\n+    This pass materializes line mapping information for LLVM IR dialect operations.\n+  }];\n+\n+  let constructor = \"mlir::createLLVMDIScopePass()\";\n+}\n+\n+#endif"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -168,6 +168,10 @@ class AllocationAnalysis {\n       ReduceOpHelper helper(reduceOp);\n       unsigned bytes = helper.getScratchSizeInBytes();\n       allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+    } else if (auto scanOp = dyn_cast<triton::ScanOp>(op)) {\n+      ScanLoweringHelper helper(scanOp);\n+      unsigned bytes = helper.getScratchSizeInBytes();\n+      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n       auto srcTy = cvtLayout.getSrc().getType().cast<RankedTensorType>();\n       auto dstTy = cvtLayout.getResult().getType().cast<RankedTensorType>();"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -918,7 +918,8 @@ unsigned ModuleAxisInfoAnalysis::getPtrContiguity(Value ptr) {\n   auto order = triton::gpu::getOrder(layout);\n   unsigned align = getPtrAlignment(ptr);\n \n-  auto uniqueContigPerThread = triton::gpu::getUniqueContigPerThread(tensorTy);\n+  auto uniqueContigPerThread =\n+      triton::gpu::getUniqueContigPerThread(layout, tensorTy.getShape());\n   assert(order[0] < uniqueContigPerThread.size() &&\n          \"Unxpected uniqueContigPerThread size\");\n   unsigned contiguity = uniqueContigPerThread[order[0]];"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -86,7 +86,8 @@ void MembarAnalysis::visitTerminator(Operation *op,\n     return;\n   }\n   // Otherwise, it could be a return op\n-  if (isa<triton::ReduceReturnOp>(op) || isa<triton::ReturnOp>(op)) {\n+  if (isa<triton::ReduceReturnOp>(op) || isa<triton::ScanReturnOp>(op) ||\n+      isa<triton::ReturnOp>(op)) {\n     return;\n   }\n   llvm_unreachable(\"Unknown terminator encountered in membar analysis\");"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 152, "deletions": 2, "changes": 154, "file_content_changes": "@@ -10,11 +10,31 @@\n \n namespace mlir {\n \n+namespace {\n+\n+int getParentAxis(Attribute layout, int axis) {\n+  if (auto sliceEncoding = layout.dyn_cast<triton::gpu::SliceEncodingAttr>()) {\n+    axis = axis < sliceEncoding.getDim() ? axis : axis + 1;\n+    return getParentAxis(sliceEncoding.getParent(), axis);\n+  }\n+  return axis;\n+}\n+\n+SmallVector<unsigned> getParentOrder(Attribute layout) {\n+  if (auto sliceEncoding = layout.dyn_cast<triton::gpu::SliceEncodingAttr>()) {\n+    return getParentOrder(sliceEncoding.getParent());\n+  }\n+  return triton::gpu::getOrder(layout);\n+}\n+\n+} // namespace\n+\n bool ReduceOpHelper::isFastReduction() {\n   // Disable fast reduction only for debugging purpose\n   if (::triton::tools::getBoolEnv(\"DISABLE_FAST_REDUCTION\"))\n     return false;\n-  return axis == triton::gpu::getOrder(getSrcLayout())[0];\n+  return getParentAxis(getSrcLayout(), axis) ==\n+         getParentOrder(getSrcLayout())[0];\n }\n \n unsigned ReduceOpHelper::getInterWarpSize() {\n@@ -40,7 +60,9 @@ unsigned ReduceOpHelper::getInterWarpSizeWithUniqueData() {\n \n unsigned ReduceOpHelper::getIntraWarpSizeWithUniqueData() {\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n-  return std::min(srcReduceDimSize,\n+  unsigned elementPerThreads = triton::gpu::getUniqueContigPerThread(\n+      getSrcLayout(), getSrcShape())[axis];\n+  return std::min(srcReduceDimSize / elementPerThreads,\n                   triton::gpu::getThreadsPerWarpWithUniqueData(\n                       getSrcLayout(), getSrcShape())[axis]);\n }\n@@ -117,6 +139,134 @@ bool ReduceOpHelper::isSupportedLayout() {\n   return false;\n }\n \n+unsigned ScanLoweringHelper::getAxisNumElementsPerThread() {\n+  return getEncoding().getSizePerThread()[getAxis()];\n+}\n+\n+unsigned ScanLoweringHelper::getNonAxisNumElementsPerThread() {\n+  SmallVector<unsigned> sizePerThreads = getContigPerThread(getEncoding());\n+  sizePerThreads[getAxis()] = 1;\n+  return product<unsigned>(sizePerThreads);\n+}\n+\n+Region &ScanLoweringHelper::getCombineOp() { return scanOp.getCombineOp(); }\n+\n+unsigned ScanLoweringHelper::getAxisNumThreadsPerWarp() {\n+  return triton::gpu::getThreadsPerWarp(getEncoding())[getAxis()];\n+}\n+\n+unsigned ScanLoweringHelper::getNonAxisNumThreadsPerWarp() {\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(getEncoding());\n+  threadsPerWarp[getAxis()] = 1;\n+  return product<unsigned>(threadsPerWarp);\n+}\n+\n+// Return the flat numbers of threads computing independent scan results.\n+unsigned ScanLoweringHelper::getNonAxisNumThreadsPerCTA() {\n+  unsigned numParallelThreadsPerWarp = getNonAxisNumThreadsPerWarp();\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(getEncoding());\n+  warpsPerCTA[getAxis()] = 1;\n+  unsigned numParallelWarpsPerCTA = product<unsigned>(warpsPerCTA);\n+  return numParallelThreadsPerWarp * numParallelWarpsPerCTA;\n+}\n+unsigned ScanLoweringHelper::getAxisNumWarps() {\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  return warpsPerCTA[getAxis()];\n+}\n+\n+unsigned ScanLoweringHelper::getAxisNumBlocks() {\n+  auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+  auto sizePerThreads = triton::gpu::getSizePerThread(srcEncoding);\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  unsigned axis = getAxis();\n+  return ceil<unsigned>(\n+      type.getShape()[axis],\n+      (sizePerThreads[axis] * threadsPerWarp[axis] * warpsPerCTA[axis]));\n+}\n+\n+unsigned ScanLoweringHelper::getNonAxisNumBlocks() {\n+  auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+  auto sizePerThreads = triton::gpu::getSizePerThread(srcEncoding);\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  unsigned axis = getAxis();\n+  unsigned numBlocks = 1;\n+  for (unsigned i = 0; i < sizePerThreads.size(); i++) {\n+    if (i == axis)\n+      continue;\n+    numBlocks *= ceil<unsigned>(\n+        type.getShape()[i],\n+        (sizePerThreads[i] * threadsPerWarp[i] * warpsPerCTA[i]));\n+  }\n+  return numBlocks;\n+}\n+\n+bool ScanLoweringHelper::isSupported() {\n+  // TODO: Support the following cases:\n+  // 1. Scan on non-blocking encodings\n+  // 2. Scan with multiple operands\n+  if (!isa<triton::gpu::BlockedEncodingAttr>(srcEncoding))\n+    return false;\n+  if (scanOp.getNumOperands() != 1)\n+    return false;\n+  return true;\n+}\n+\n+unsigned ScanLoweringHelper::getScratchSizeInBytes() {\n+  auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+  unsigned elementSizeInBytes = type.getElementTypeBitWidth() / 8;\n+  auto mod = scanOp->getParentOfType<ModuleOp>();\n+  unsigned numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+  unsigned numNonAxisElementsPerWapr =\n+      getNonAxisNumThreadsPerWarp() * getNonAxisNumElementsPerThread();\n+  unsigned numElements = numWarps * numNonAxisElementsPerWapr *\n+                         getAxisNumBlocks() * getNonAxisNumBlocks();\n+  return elementSizeInBytes * numElements;\n+}\n+\n+triton::gpu::BlockedEncodingAttr ScanLoweringHelper::getEncoding() {\n+  return srcEncoding.cast<triton::gpu::BlockedEncodingAttr>();\n+}\n+\n+unsigned ScanLoweringHelper::getAxisElementStride() {\n+  auto order = triton::gpu::getOrder(srcEncoding);\n+  unsigned stride = 1;\n+  for (unsigned dim : order) {\n+    if (dim == getAxis())\n+      return stride;\n+    stride *= getContigPerThread(getEncoding())[dim];\n+  }\n+  llvm_unreachable(\"Axis not found in order\");\n+}\n+\n+unsigned ScanLoweringHelper::getAxisThreadStride() {\n+  auto order = triton::gpu::getOrder(srcEncoding);\n+  unsigned stride = 1;\n+  for (unsigned dim : order) {\n+    if (dim == getAxis())\n+      return stride;\n+    stride *= getEncoding().getThreadsPerWarp()[dim];\n+  }\n+  llvm_unreachable(\"Axis not found in order\");\n+}\n+\n+unsigned ScanLoweringHelper::getAxisBlockStride() {\n+  auto order = triton::gpu::getOrder(srcEncoding);\n+  unsigned stride = 1;\n+  auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+  auto sizePerThreads = triton::gpu::getSizePerThread(srcEncoding);\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  for (unsigned dim : order) {\n+    if (dim == getAxis())\n+      return stride;\n+    stride *= type.getShape()[dim] /\n+              (sizePerThreads[dim] * threadsPerWarp[dim] * warpsPerCTA[dim]);\n+  }\n+  llvm_unreachable(\"Axis not found in order\");\n+}\n+\n bool maybeSharedAllocationOp(Operation *op) {\n   // TODO(Keren): This function can be replaced by adding\n   // MemoryEffectOpInterface. We can then use the MemoryEffectOpInterface to"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -17,6 +17,7 @@ add_mlir_conversion_library(TritonGPUToLLVM\n     TritonGPUToLLVMPass.cpp\n     PTXAsmFormat.cpp\n     ReduceOpToLLVM.cpp\n+    ScanOpToLLVM.cpp\n     Utility.cpp\n     TypeConverter.cpp\n     ViewOpToLLVM.cpp"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -20,7 +20,7 @@ namespace SharedToDotOperandMMAv1 {\n using CoordTy = SmallVector<Value>;\n using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n \n-SmallVector<CoordTy> getMNCoords(Value thread,\n+SmallVector<CoordTy> getMNCoords(Value thread, Location loc,\n                                  ConversionPatternRewriter &rewriter,\n                                  ArrayRef<unsigned int> wpt,\n                                  const MmaEncodingAttr &mmaLayout,\n@@ -187,8 +187,8 @@ struct ConvertLayoutOpConversion\n         auto [isARow, isBRow, isAVec4, isBVec4, _] =\n             mmaLayout.decodeVoltaLayoutStates();\n         auto coords = SharedToDotOperandMMAv1::getMNCoords(\n-            threadId, rewriter, mmaLayout.getWarpsPerCTA(), mmaLayout, shape,\n-            isARow, isBRow, isAVec4, isBVec4);\n+            threadId, loc, rewriter, mmaLayout.getWarpsPerCTA(), mmaLayout,\n+            shape, isARow, isBRow, isAVec4, isBVec4);\n         return coords[elemId];\n       } else {\n         llvm_unreachable(\"Unexpected MMALayout version\");"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -339,7 +339,7 @@ namespace SharedToDotOperandMMAv1 {\n using CoordTy = SmallVector<Value>;\n using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n \n-SmallVector<CoordTy> getMNCoords(Value thread,\n+SmallVector<CoordTy> getMNCoords(Value thread, Location loc,\n                                  ConversionPatternRewriter &rewriter,\n                                  ArrayRef<unsigned int> wpt,\n                                  const MmaEncodingAttr &mmaLayout,\n@@ -348,7 +348,6 @@ SmallVector<CoordTy> getMNCoords(Value thread,\n   static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n \n   auto *ctx = thread.getContext();\n-  auto loc = UnknownLoc::get(ctx);\n   Value _1 = i32_val(1);\n   Value _2 = i32_val(2);\n   Value _4 = i32_val(4);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -357,6 +357,7 @@ struct StoreOpConversion\n               .o(\"wb\", op.getCache() == triton::CacheModifier::WB)\n               .o(\"cg\", op.getCache() == triton::CacheModifier::CG)\n               .o(\"cs\", op.getCache() == triton::CacheModifier::CS)\n+              .o(\"wt\", op.getCache() == triton::CacheModifier::WT)\n               .o(\"L1::evict_first\",\n                  op.getEvict() == triton::EvictionPolicy::EVICT_FIRST)\n               .o(\"L1::evict_last\","}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 12, "deletions": 11, "changes": 23, "file_content_changes": "@@ -89,13 +89,14 @@ struct ReduceOpConversion\n   void getWriteIndexBasic(ConversionPatternRewriter &rewriter, Location loc,\n                           Attribute layout, SmallVector<Value> &index,\n                           SmallVector<Value> &writeIdx,\n-                          std::map<int, Value> &ints, unsigned axis) const {\n+                          std::map<int, Value> &ints, unsigned originalAxis,\n+                          unsigned axis) const {\n     if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-      auto dim = sliceLayout.getDim();\n-      assert(dim != axis && \"Reduction axis cannot be sliced\");\n+      // Recover the axis in the parent layout\n+      auto parentAxis = axis < sliceLayout.getDim() ? axis : axis + 1;\n       auto parentLayout = sliceLayout.getParent();\n       getWriteIndexBasic(rewriter, loc, parentLayout, index, writeIdx, ints,\n-                         axis);\n+                         originalAxis, parentAxis);\n       return;\n     }\n \n@@ -110,21 +111,21 @@ struct ReduceOpConversion\n       // we would have a single accumulation every `axisSizePerThread`\n       // contiguous values in the original tensor, so we would need\n       // to map every `axisSizePerThread` to 1 value in smem as:\n-      // writeIdx[axis] = index[axis] / axisSizePerThread\n-      writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n+      // writeIdx[originalAxis] = index[originalAxis] / axisSizePerThread\n+      writeIdx[originalAxis] = udiv(index[originalAxis], axisSizePerThread);\n     } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n       if (!mmaLayout.isAmpere()) {\n         llvm::report_fatal_error(\"Unsupported layout\");\n       }\n-      if (axis == 0) {\n+      if (originalAxis == 0) {\n         // Because warpTileSize = [16, 8] and threadsPerWarp = [8, 4], each 8\n         // rows in smem would correspond to a warp. The mapping\n         // is: (warp_index) x 8 + (row index within warp)\n-        writeIdx[axis] =\n-            add(mul(udiv(index[axis], _16), _8), urem(index[axis], _8));\n+        writeIdx[originalAxis] = add(mul(udiv(index[originalAxis], _16), _8),\n+                                     urem(index[originalAxis], _8));\n       } else {\n         // Same as BlockedEncodingAttr case\n-        writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n+        writeIdx[originalAxis] = udiv(index[originalAxis], axisSizePerThread);\n       }\n     } else {\n       llvm::report_fatal_error(\"Unsupported layout\");\n@@ -214,7 +215,7 @@ struct ReduceOpConversion\n       // get the writeIdx at which to write in smem\n       SmallVector<Value> writeIdx;\n       getWriteIndexBasic(rewriter, loc, srcLayout, indices[key], writeIdx, ints,\n-                         axis);\n+                         axis, axis);\n \n       // calculate the offset in smem for that writeIdx\n       Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, srcOrd);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ScanOpToLLVM.cpp", "status": "added", "additions": 324, "deletions": 0, "changes": 324, "file_content_changes": "@@ -0,0 +1,324 @@\n+#include \"ScanOpToLLVM.h\"\n+#include \"TritonGPUToLLVMBase.h\"\n+#include \"triton/Analysis/Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::LLVM::delinearize;\n+using ::mlir::LLVM::linearize;\n+using ::mlir::LLVM::shflUpSync;\n+using ::mlir::LLVM::storeShared;\n+\n+// Apply the region of the scan op to the acc and cur values and update acc\n+// inplace with the result.\n+static void accumulate(ConversionPatternRewriter &rewriter, Region &combineOp,\n+                       Value &acc, Value cur) {\n+  if (!acc) {\n+    acc = cur;\n+    return;\n+  }\n+  // Create a new copy of the reduce block, and inline it\n+  Block *currentBlock = rewriter.getBlock();\n+  Region &parent = *currentBlock->getParent();\n+  rewriter.cloneRegionBefore(combineOp, &parent.front());\n+  auto &newScan = parent.front();\n+  auto returnOp = dyn_cast<triton::ScanReturnOp>(newScan.getTerminator());\n+  llvm::SmallVector<Value> combineArgs = {acc, cur};\n+  rewriter.inlineBlockBefore(&newScan, &*rewriter.getInsertionPoint(),\n+                             combineArgs);\n+  auto results = returnOp.getResult();\n+  acc = results[0];\n+  // Delete the terminator, which is no longer used\n+  rewriter.eraseOp(returnOp);\n+}\n+\n+// Scan a contiguous elements within a thread and update `srcValues` in place.\n+static void scanThreadContiguousElements(SmallVector<Value> &srcValues,\n+                                         ConversionPatternRewriter &rewriter,\n+                                         ScanLoweringHelper &helper) {\n+  // Depending on layout contiguous elements along axis dim may not be\n+  // contiguous in srcValues. Keep track of what elements belong to the same\n+  // chunk of contiguous elements.\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThread();\n+  unsigned parallelElementsPerThread = helper.getAxisNumElementsPerThread();\n+  unsigned numChunks = srcValues.size() / scanElementsPerThreads;\n+  unsigned stride = helper.getAxisElementStride();\n+  SmallVector<Value> accs(numChunks);\n+  for (unsigned srcIndex = 0; srcIndex < srcValues.size(); srcIndex++) {\n+    unsigned accIndex = (srcIndex % stride) +\n+                        ((srcIndex / stride) / scanElementsPerThreads) * stride;\n+\n+    accumulate(rewriter, helper.getCombineOp(), accs[accIndex],\n+               srcValues[srcIndex]);\n+    srcValues[srcIndex] = accs[accIndex];\n+  }\n+}\n+\n+// Apply a scan across threads of the warp for the last element of each\n+// contiguous group of elements.\n+static void warpScan(SmallVector<Value> &srcValues,\n+                     ConversionPatternRewriter &rewriter,\n+                     ScanLoweringHelper &helper, Value laneId) {\n+  Location loc = helper.getLoc();\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThread();\n+  unsigned elementStride = helper.getAxisElementStride();\n+  unsigned threadStride = helper.getAxisThreadStride();\n+  unsigned scanDim = helper.getAxisNumThreadsPerWarp();\n+  for (unsigned srcIndex = 0; srcIndex < srcValues.size(); srcIndex++) {\n+    unsigned elementIdx = (srcIndex / elementStride) % scanElementsPerThreads;\n+    // Only consider the last element of each contiguous chunk of elements.\n+    if (elementIdx != scanElementsPerThreads - 1)\n+      continue;\n+    // Reduce within warps.\n+    Value acc = srcValues[srcIndex];\n+    for (unsigned i = 1; i <= (scanDim) / 2; i = i << 1) {\n+      Value shfl = shflUpSync(loc, rewriter, acc, i * threadStride);\n+      Value tempAcc = acc;\n+      accumulate(rewriter, helper.getCombineOp(), tempAcc, shfl);\n+      Value mask = icmp_slt(laneId, i32_val(i));\n+      acc = select(mask, acc, tempAcc);\n+    }\n+    srcValues[srcIndex] = acc;\n+  }\n+}\n+\n+// For each set of contiguous elements within a thread we store the partial\n+// reduction into shared memory. Each parallel scan and each warp will store its\n+// own partial reductions. The shared memory is organized as follow:\n+//          -----------------------------------------------------------------\n+// chunk 0: | acc[0] warp 0 | acc[1] warp 0 | acc[0] warp 1 | acc[1] warp 1 |\n+// chunk 1: | acc[0] warp 0 | acc[1] warp 0 | acc[0] warp 1 | acc[1] warp 1 |\n+static void storeWarpAccumulator(SmallVector<Value> &srcValues,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 ScanLoweringHelper &helper, Value laneId,\n+                                 Value warpId, Value baseSharedMemPtr,\n+                                 Value parallelLaneId) {\n+  Location loc = helper.getLoc();\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThread();\n+  unsigned scanDim = helper.getAxisNumThreadsPerWarp();\n+  unsigned numParallelLane = helper.getNonAxisNumThreadsPerCTA();\n+  unsigned numWarps = helper.getAxisNumWarps();\n+  unsigned chunkId = 0;\n+  unsigned elementStride = helper.getAxisElementStride();\n+  for (unsigned srcIndex = 0; srcIndex < srcValues.size(); srcIndex++) {\n+    unsigned elementIdx = (srcIndex / elementStride) % scanElementsPerThreads;\n+    // Only consider the last element of each contiguous chunk of elements.\n+    if (elementIdx != scanElementsPerThreads - 1)\n+      continue;\n+    Value lastElement = srcValues[srcIndex];\n+    Value mask = icmp_eq(laneId, i32_val(scanDim - 1));\n+    Value index = add(parallelLaneId, mul(warpId, i32_val(numParallelLane)));\n+    index = add(index, i32_val(chunkId * numParallelLane * numWarps));\n+    Value writePtr = gep(baseSharedMemPtr.getType(), baseSharedMemPtr, index);\n+    storeShared(rewriter, loc, writePtr, lastElement, mask);\n+    chunkId++;\n+  }\n+}\n+\n+// Read the partial reductions from shared memory from each chunk of contiguous\n+// elements for each warp and parallel scan. Then combine the partial reduction\n+// with the right elements. Within a given contiguous element chunk we update\n+// all the elements by accumulating the value from the last element of the\n+// reduced value from the previous lane.\n+static void AddPartialReduce(SmallVector<Value> &srcValues,\n+                             ConversionPatternRewriter &rewriter,\n+                             ScanLoweringHelper &helper, Value sharedMemoryPtr,\n+                             Value warpId, Value laneId, Value parallelLaneId) {\n+  Location loc = helper.getLoc();\n+  unsigned numParallelLane = helper.getNonAxisNumThreadsPerCTA();\n+  unsigned numWarps = helper.getAxisNumWarps();\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThread();\n+  unsigned parallelElementsPerThread = helper.getNonAxisNumElementsPerThread();\n+  unsigned elementStride = helper.getAxisElementStride();\n+  unsigned threadStride = helper.getAxisThreadStride();\n+  Value maskFirstWarp = icmp_eq(warpId, i32_val(0));\n+  Value maskFirstLane = icmp_eq(laneId, i32_val(0));\n+  Value maskFirstThread = and_(maskFirstWarp, maskFirstLane);\n+  struct Accumulator {\n+    Value acc;\n+    Value maskedAcc;\n+  };\n+  unsigned numScanBlocks = helper.getAxisNumBlocks();\n+  unsigned numParallelBlocks = helper.getNonAxisNumBlocks();\n+  assert(numScanBlocks * numParallelBlocks * parallelElementsPerThread *\n+             scanElementsPerThreads ==\n+         srcValues.size());\n+  SmallVector<Accumulator> accumulators(numParallelBlocks *\n+                                        parallelElementsPerThread);\n+  unsigned chunkId = 0;\n+  unsigned blockStride = helper.getAxisBlockStride();\n+  for (unsigned srcIndex = 0; srcIndex < srcValues.size(); srcIndex++) {\n+    unsigned elementIdx = (srcIndex / elementStride) % scanElementsPerThreads;\n+    // Only consider the last element of each contiguous chunk of elements.\n+    if (elementIdx != scanElementsPerThreads - 1)\n+      continue;\n+    // Accumulate the partial reduction from shared memory. Decide which\n+    // accumulator to combine based on whether the elements belong to the same\n+    // dimension along axis.\n+    unsigned blockId = chunkId / parallelElementsPerThread;\n+    unsigned parallelBlockId =\n+        blockId % blockStride +\n+        ((blockId / blockStride) / numScanBlocks) * blockStride;\n+    unsigned accumulatorIndex = chunkId % parallelElementsPerThread +\n+                                parallelBlockId * parallelElementsPerThread;\n+    Accumulator &accumulator = accumulators[accumulatorIndex];\n+    for (unsigned i = 0; i < numWarps; ++i) {\n+      Value index = add(parallelLaneId,\n+                        i32_val(numParallelLane * (i + chunkId * numWarps)));\n+      Value ptr = gep(sharedMemoryPtr.getType(), sharedMemoryPtr, index);\n+      Value partialReduce = load(ptr);\n+      if (!accumulator.acc) {\n+        accumulator.acc = partialReduce;\n+        accumulator.maskedAcc = partialReduce;\n+        continue;\n+      }\n+      accumulate(rewriter, helper.getCombineOp(), accumulator.acc,\n+                 partialReduce);\n+      Value mask = icmp_slt(warpId, i32_val(i + 1));\n+      accumulator.maskedAcc =\n+          select(mask, accumulator.maskedAcc, accumulator.acc);\n+    }\n+    Value temp = srcValues[srcIndex];\n+    accumulate(rewriter, helper.getCombineOp(), temp, accumulator.maskedAcc);\n+    unsigned axisBlockId = (blockId / blockStride) % numScanBlocks;\n+    if (axisBlockId == 0) {\n+      // For the first warp and first chunk we don't have anything to\n+      // accumulate.\n+      temp = select(maskFirstWarp, srcValues[srcIndex], temp);\n+    }\n+    srcValues[srcIndex] = temp;\n+    // Update the rest of the contiguous elements.\n+    Value lastElement =\n+        shflUpSync(loc, rewriter, srcValues[srcIndex], threadStride);\n+    lastElement = select(maskFirstLane, accumulator.maskedAcc, lastElement);\n+    for (unsigned i = 1; i < scanElementsPerThreads; ++i) {\n+      Value laneValue = srcValues[srcIndex - i * elementStride];\n+      accumulate(rewriter, helper.getCombineOp(), laneValue, lastElement);\n+      if (axisBlockId == 0) {\n+        // For the first warp and first chunk we don't have anything to\n+        // accumulate.\n+        laneValue = select(maskFirstThread,\n+                           srcValues[srcIndex - i * elementStride], laneValue);\n+      }\n+      srcValues[srcIndex - i * elementStride] = laneValue;\n+    }\n+    // For the next chunk start back from the value containing the\n+    // accumulated value of all the warps.\n+    accumulator.maskedAcc = accumulator.acc;\n+    chunkId++;\n+  }\n+}\n+\n+namespace {\n+struct ScanOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::ScanOp> {\n+public:\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::ScanOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ScanOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    if (succeeded(emitFastScan(op, adaptor, rewriter)))\n+      return success();\n+    return failure();\n+  }\n+\n+private:\n+  std::tuple<Value, Value, Value>\n+  getDelinearizedIds(ConversionPatternRewriter &rewriter,\n+                     ScanLoweringHelper &helper) const;\n+  LogicalResult emitFastScan(triton::ScanOp op, triton::ScanOpAdaptor adaptor,\n+                             ConversionPatternRewriter &rewriter) const;\n+};\n+\n+// Break up the threadId into lane and warp id along the scan dimension and\n+// compute a flat id for the parallel dimensions.\n+std::tuple<Value, Value, Value>\n+ScanOpConversion::getDelinearizedIds(ConversionPatternRewriter &rewriter,\n+                                     ScanLoweringHelper &helper) const {\n+  auto loc = helper.getLoc();\n+  unsigned axis = helper.getAxis();\n+  auto srcEncoding = helper.getEncoding();\n+\n+  Value threadId = getThreadId(rewriter, loc);\n+  Value warpSize = i32_val(32);\n+  Value warpId = udiv(threadId, warpSize);\n+  Value laneId = urem(threadId, warpSize);\n+\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  auto order = triton::gpu::getOrder(srcEncoding);\n+  SmallVector<Value> multiDimLaneId =\n+      delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n+  SmallVector<Value> multiDimWarpId =\n+      delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+\n+  Value laneIdAxis = multiDimLaneId[axis];\n+  Value warpIdAxis = multiDimWarpId[axis];\n+\n+  multiDimLaneId[axis] = i32_val(0);\n+  threadsPerWarp[axis] = 1;\n+  Value laneIdParallel =\n+      linearize(rewriter, loc, multiDimLaneId, threadsPerWarp, order);\n+  multiDimWarpId[axis] = i32_val(0);\n+  warpsPerCTA[axis] = 1;\n+  Value warpIdParallel =\n+      linearize(rewriter, loc, multiDimWarpId, warpsPerCTA, order);\n+  Value flatIdParallel =\n+      add(laneIdParallel,\n+          mul(warpIdParallel, i32_val(helper.getNonAxisNumThreadsPerWarp())));\n+  return std::make_tuple(laneIdAxis, warpIdAxis, flatIdParallel);\n+}\n+\n+// Lowering using warp shuffle operations to do warp level scan.\n+LogicalResult\n+ScanOpConversion::emitFastScan(triton::ScanOp op, triton::ScanOpAdaptor adaptor,\n+                               ConversionPatternRewriter &rewriter) const {\n+  ScanLoweringHelper helper(op);\n+  auto loc = helper.getLoc();\n+  if (!helper.isSupported())\n+    return failure();\n+\n+  auto [laneIdAxis, warpIdAxis, flatIdParallel] =\n+      getDelinearizedIds(rewriter, helper);\n+  auto input = adaptor.getOperands()[0];\n+  auto type = op.getOperand(0).getType().cast<RankedTensorType>();\n+  SmallVector<Value> srcValues =\n+      getTypeConverter()->unpackLLElements(loc, input, rewriter, type);\n+\n+  // Scan contigous elements in a thread and update `srcValues`.\n+  scanThreadContiguousElements(srcValues, rewriter, helper);\n+  // Apply warp level scan to the last element of each chunk of contiguous\n+  // elements.\n+  warpScan(srcValues, rewriter, helper, laneIdAxis);\n+\n+  // Store the partial reducing for each warp into shared memory.\n+  Type elemPtrTys = LLVM::LLVMPointerType::get(srcValues[0].getType(), 3);\n+  Value baseSharedMemPtr = bitcast(\n+      getSharedMemoryBase(loc, rewriter, op.getOperation()), elemPtrTys);\n+  storeWarpAccumulator(srcValues, rewriter, helper, laneIdAxis, warpIdAxis,\n+                       baseSharedMemPtr, flatIdParallel);\n+  barrier();\n+  // Read back the partial reduction of each warp and accumulate them based on\n+  // warpId. Then update each chunk of contiguous elements by adding the\n+  // accumulated value from the previous lane.\n+  AddPartialReduce(srcValues, rewriter, helper, baseSharedMemPtr, warpIdAxis,\n+                   laneIdAxis, flatIdParallel);\n+\n+  Value results = getTypeConverter()->packLLElements(loc, srcValues, rewriter,\n+                                                     input.getType());\n+  rewriter.replaceOp(op, results);\n+  return success();\n+}\n+} // namespace\n+\n+void populateScanOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    ModuleAllocation &allocation,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n+  patterns.add<ScanOpConversion>(typeConverter, allocation, indexCacheInfo,\n+                                 benefit);\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ScanOpToLLVM.h", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -0,0 +1,15 @@\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_SCAN_OP_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_SCAN_OP_H\n+\n+#include \"TritonGPUToLLVMBase.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+void populateScanOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    ModuleAllocation &allocation,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n+\n+#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -129,9 +129,15 @@ struct PrintOpConversion\n     } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n       return \"%f\";\n     } else if (type.isSignedInteger()) {\n-      return \"%i\";\n+      if (type.getIntOrFloatBitWidth() == 64)\n+        return \"%lli\";\n+      else\n+        return \"%i\";\n     } else if (type.isUnsignedInteger() || type.isSignlessInteger()) {\n-      return \"%u\";\n+      if (type.getIntOrFloatBitWidth() == 64)\n+        return \"%llu\";\n+      else\n+        return \"%u\";\n     }\n     assert(false && \"not supported type\");\n     return \"\";"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -26,6 +26,7 @@\n #include \"ElementwiseOpToLLVM.h\"\n #include \"LoadStoreOpToLLVM.h\"\n #include \"ReduceOpToLLVM.h\"\n+#include \"ScanOpToLLVM.h\"\n #include \"TritonGPUToLLVM.h\"\n #include \"TypeConverter.h\"\n #include \"ViewOpToLLVM.h\"\n@@ -372,6 +373,8 @@ class ConvertTritonGPUToLLVM\n                                       /*benefit=*/1);\n     populateReduceOpToLLVMPatterns(typeConverter, patterns, allocation,\n                                    indexCacheInfo, /*benefit=*/1);\n+    populateScanOpToLLVMPatterns(typeConverter, patterns, allocation,\n+                                 indexCacheInfo, /*benefit=*/1);\n     populateViewOpToLLVMPatterns(typeConverter, patterns, /*benefit=*/1);\n \n     // Native lowering patterns"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.cpp", "status": "modified", "additions": 17, "deletions": 6, "changes": 23, "file_content_changes": "@@ -160,34 +160,45 @@ Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n   return builder.launch(rewriter, loc, void_ty(ctx));\n }\n \n-Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n-               int i) {\n+static Value commonShflSync(Location loc, ConversionPatternRewriter &rewriter,\n+                            Value val, int i, const std::string &shuffleType,\n+                            const std::string &clamp) {\n   unsigned bits = val.getType().getIntOrFloatBitWidth();\n \n   if (bits == 64) {\n     Type vecTy = vec_ty(f32_ty, 2);\n     Value vec = bitcast(val, vecTy);\n     Value val0 = extract_element(f32_ty, vec, i32_val(0));\n     Value val1 = extract_element(f32_ty, vec, i32_val(1));\n-    val0 = shflSync(loc, rewriter, val0, i);\n-    val1 = shflSync(loc, rewriter, val1, i);\n+    val0 = commonShflSync(loc, rewriter, val0, i, shuffleType, clamp);\n+    val1 = commonShflSync(loc, rewriter, val1, i, shuffleType, clamp);\n     vec = undef(vecTy);\n     vec = insert_element(vecTy, vec, val0, i32_val(0));\n     vec = insert_element(vecTy, vec, val1, i32_val(1));\n     return bitcast(vec, val.getType());\n   }\n \n   PTXBuilder builder;\n-  auto &shfl = builder.create(\"shfl.sync\")->o(\"bfly\").o(\"b32\");\n+  auto &shfl = builder.create(\"shfl.sync\")->o(shuffleType).o(\"b32\");\n   auto *dOpr = builder.newOperand(\"=r\");\n   auto *aOpr = builder.newOperand(val, \"r\");\n   auto *bOpr = builder.newConstantOperand(i);\n-  auto *cOpr = builder.newConstantOperand(\"0x1f\");\n+  auto *cOpr = builder.newConstantOperand(clamp);\n   auto *maskOpr = builder.newConstantOperand(\"0xffffffff\");\n   shfl(dOpr, aOpr, bOpr, cOpr, maskOpr);\n   return builder.launch(rewriter, loc, val.getType(), false);\n }\n \n+Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n+               int i) {\n+  return commonShflSync(loc, rewriter, val, i, \"bfly\", \"0x1f\");\n+}\n+\n+Value shflUpSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n+                 int i) {\n+  return commonShflSync(loc, rewriter, val, i, \"up\", \"0x0\");\n+}\n+\n Value addStringToModule(Location loc, ConversionPatternRewriter &rewriter,\n                         StringRef key, StringRef content) {\n   auto moduleOp = rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -285,6 +285,8 @@ Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n \n Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n                int i);\n+Value shflUpSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n+                 int i);\n \n Value addStringToModule(Location loc, ConversionPatternRewriter &rewriter,\n                         StringRef key, StringRef content);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 20, "deletions": 1, "changes": 21, "file_content_changes": "@@ -22,7 +22,26 @@ struct SplatOpConversion\n                                   ConversionPatternRewriter &rewriter,\n                                   Location loc) {\n     auto tensorTy = resType.cast<RankedTensorType>();\n-    auto srcType = typeConverter->convertType(elemType);\n+    // Check the converted type for the tensor as depending on the encoding the\n+    // converter may pick different element types.\n+    auto srcType = typeConverter->convertType(tensorTy);\n+    if (auto structTy = dyn_cast<LLVM::LLVMStructType>(srcType))\n+      srcType = structTy.getBody()[0];\n+    // If the type sizes don't match we need to pack constants.\n+    if (srcType.isIntOrFloat() && constVal.getType().getIntOrFloatBitWidth() !=\n+                                      srcType.getIntOrFloatBitWidth()) {\n+      unsigned cstBitWidth = constVal.getType().getIntOrFloatBitWidth();\n+      unsigned srcBitWidth = srcType.getIntOrFloatBitWidth();\n+      assert(cstBitWidth <= srcBitWidth && srcBitWidth % cstBitWidth == 0);\n+      unsigned ratio = srcBitWidth / cstBitWidth;\n+      Type intTy = IntegerType::get(elemType.getContext(), cstBitWidth);\n+      VectorType vecType = VectorType::get(ratio, intTy);\n+      Value intCst = bitcast(constVal, intTy);\n+      Value vec = undef(vecType);\n+      for (unsigned i = 0; i < ratio; ++i)\n+        vec = insert_element(vecType, vec, intCst, int_val(32, i));\n+      constVal = vec;\n+    }\n     auto llSrc = bitcast(constVal, srcType);\n     size_t elemsPerThread = getTotalElemsPerThread(tensorTy);\n     llvm::SmallVector<Value> elems(elemsPerThread, llSrc);"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 54, "deletions": 7, "changes": 61, "file_content_changes": "@@ -154,10 +154,24 @@ class StdSelectPattern : public OpConversionPattern<arith::SelectOp> {\n   matchAndRewrite(arith::SelectOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n-                      op, retType, adaptor.getCondition(),\n-                      adaptor.getTrueValue(), adaptor.getFalseValue()),\n-                  adaptor.getAttributes());\n+\n+    Value cond = adaptor.getCondition();\n+    if (llvm::isa<RankedTensorType>(retType) &&\n+        !llvm::isa<TensorType>(cond.getType())) {\n+      // triton_gpu.select doesn't support scalar condition values, so add a\n+      // splat\n+      auto retTypeTensor = llvm::cast<RankedTensorType>(retType);\n+      auto retShape = retTypeTensor.getShape();\n+      auto retEncoding = retTypeTensor.getEncoding();\n+      Type condTy =\n+          RankedTensorType::get(retShape, cond.getType(), retEncoding);\n+      cond = rewriter.create<triton::SplatOp>(op.getLoc(), condTy, cond);\n+    }\n+\n+    addNamedAttrs(\n+        rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n+            op, retType, cond, adaptor.getTrueValue(), adaptor.getFalseValue()),\n+        adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -537,6 +551,38 @@ struct TritonReduceReturnPattern\n   }\n };\n \n+struct TritonScanPattern : public OpConversionPattern<triton::ScanOp> {\n+  using OpConversionPattern<triton::ScanOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ScanOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto newScan = rewriter.create<triton::ScanOp>(\n+        op.getLoc(), adaptor.getOperands(), adaptor.getAxis());\n+    addNamedAttrs(newScan, adaptor.getAttributes());\n+\n+    auto &newCombineOp = newScan.getCombineOp();\n+    rewriter.cloneRegionBefore(op.getCombineOp(), newCombineOp,\n+                               newCombineOp.end());\n+    rewriter.replaceOp(op, newScan.getResult());\n+    return success();\n+  }\n+};\n+\n+struct TritonScanReturnPattern\n+    : public OpConversionPattern<triton::ScanReturnOp> {\n+  using OpConversionPattern<triton::ScanReturnOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ScanReturnOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::ScanReturnOp>(\n+                      op, adaptor.getResult()),\n+                  adaptor.getAttributes());\n+    return success();\n+  }\n+};\n+\n struct TritonPrintPattern : public OpConversionPattern<triton::PrintOp> {\n   using OpConversionPattern<triton::PrintOp>::OpConversionPattern;\n \n@@ -623,9 +669,10 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n           TritonGenericPattern<triton::PtrToIntOp>,\n           TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n           TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n-          TritonReducePattern, TritonReduceReturnPattern, TritonTransPattern,\n-          TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n-          TritonLoadPattern, TritonStorePattern,\n+          TritonReducePattern, TritonReduceReturnPattern, TritonScanPattern,\n+          TritonScanReturnPattern, TritonTransPattern, TritonExpandDimsPattern,\n+          TritonMakeRangePattern, TritonDotPattern, TritonLoadPattern,\n+          TritonStorePattern,\n           TritonExternElementwisePattern<triton::PureExternElementwiseOp>,\n           TritonExternElementwisePattern<triton::ImpureExternElementwiseOp>,\n           TritonPrintPattern, TritonAssertPattern, TritonAtomicRMWPattern,"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 31, "deletions": 1, "changes": 32, "file_content_changes": "@@ -6,7 +6,7 @@\n #include \"mlir/IR/OperationSupport.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Attributes.h\"\n \n namespace mlir {\n namespace triton {\n@@ -559,6 +559,36 @@ llvm::SmallVector<Type> ReduceOp::getElementTypes() {\n \n unsigned ReduceOp::getNumOperands() { return this->getOperands().size(); }\n \n+//-- ScanOp --\n+void ScanOp::build(mlir::OpBuilder &builder, mlir::OperationState &state,\n+                   mlir::ValueRange operands, int axis) {\n+  SmallVector<Type> inferredReturnTypes;\n+  for (auto arg : operands)\n+    inferredReturnTypes.push_back(arg.getType());\n+  ReduceOp::build(builder, state, inferredReturnTypes, operands, axis);\n+}\n+\n+mlir::LogicalResult mlir::triton::ScanOp::inferReturnTypes(\n+    MLIRContext *context, std::optional<Location> location, ValueRange operands,\n+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,\n+    SmallVectorImpl<Type> &inferredReturnTypes) {\n+  for (auto arg : operands)\n+    inferredReturnTypes.push_back(arg.getType());\n+  return success();\n+}\n+\n+mlir::LogicalResult mlir::triton::ScanOp::verify() {\n+  if (this->getOperands().size() < 1) {\n+    return this->emitOpError() << \"must have at least 1 operand\";\n+  }\n+  for (const auto &operand : this->getOperands()) {\n+    if (!dyn_cast<RankedTensorType>(operand.getType())) {\n+      return this->emitOpError() << \"operands must be RankedTensorType\";\n+    }\n+  }\n+  return success();\n+}\n+\n //-- SplatOp --\n OpFoldResult SplatOp::fold(FoldAdaptor adaptor) {\n   auto value = adaptor.getSrc();"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 88, "deletions": 1, "changes": 89, "file_content_changes": "@@ -36,7 +36,6 @@ bool isBroadcastConstantCombinable(Attribute value) {\n \n DenseElementsAttr getConstantValue(Builder &builder, Attribute value,\n                                    Value bcast_res) {\n-\n   auto resType = bcast_res.getType().cast<ShapedType>();\n   DenseElementsAttr res;\n   if (auto denseValue = value.dyn_cast<DenseElementsAttr>()) {\n@@ -101,6 +100,93 @@ class CombineSelectMaskedLoadPattern : public mlir::RewritePattern {\n   }\n };\n \n+// sum(x[:, :, None] * y[None, :, :], 1)\n+// -> dot(x, y)\n+class CombineBroadcastMulReducePattern : public mlir::RewritePattern {\n+private:\n+  static bool isAddF32(const Operation *op) {\n+    if (auto addf = dyn_cast_or_null<arith::AddFOp>(op))\n+      return addf.getType().getIntOrFloatBitWidth() <= 32;\n+    return false;\n+  }\n+\n+  static SmallVector<int> getEqualIndices(ArrayRef<int64_t> x,\n+                                          ArrayRef<int64_t> y) {\n+    SmallVector<int> res;\n+    for (int i = 0; i < x.size(); ++i)\n+      if (x[i] == y[i])\n+        res.push_back(i);\n+    return res;\n+  }\n+\n+public:\n+  CombineBroadcastMulReducePattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(triton::ReduceOp::getOperationName(), 1, context) {\n+  }\n+\n+  mlir::LogicalResult matchAndRewrite(mlir::Operation *op,\n+                                      mlir::PatternRewriter &rewriter) const {\n+    auto reduceOp = llvm::dyn_cast<triton::ReduceOp>(op);\n+    if (!reduceOp)\n+      return mlir::failure();\n+    // only support reduce with simple addition\n+    Region &combineOp = reduceOp.getCombineOp();\n+    bool isReduceAdd = combineOp.hasOneBlock() &&\n+                       combineOp.front().getOperations().size() == 2 &&\n+                       isAddF32(&*combineOp.front().getOperations().begin());\n+    if (!isReduceAdd)\n+      return mlir::failure();\n+    // operand of reduce has to be mul\n+    auto mulOp = llvm::dyn_cast_or_null<arith::MulFOp>(\n+        reduceOp.getOperand(0).getDefiningOp());\n+    if (!mulOp)\n+      return mlir::failure();\n+    // mul operand has to be broadcast\n+    auto broadcastLhsOp = llvm::dyn_cast_or_null<triton::BroadcastOp>(\n+        mulOp.getOperand(0).getDefiningOp());\n+    if (!broadcastLhsOp)\n+      return mlir::failure();\n+    auto broadcastRhsOp = llvm::dyn_cast_or_null<triton::BroadcastOp>(\n+        mulOp.getOperand(1).getDefiningOp());\n+    if (!broadcastRhsOp)\n+      return mlir::failure();\n+    // broadcast operand is expand dims\n+    auto expandLhsOp = llvm::dyn_cast_or_null<triton::ExpandDimsOp>(\n+        broadcastLhsOp.getOperand().getDefiningOp());\n+    if (!expandLhsOp)\n+      return mlir::failure();\n+    auto expandRhsOp = llvm::dyn_cast_or_null<triton::ExpandDimsOp>(\n+        broadcastRhsOp.getOperand().getDefiningOp());\n+    if (!expandRhsOp)\n+      return mlir::failure();\n+    // get not-broadcast dimensions\n+    int expandLhsAxis = expandLhsOp.getAxis();\n+    int expandRhsAxis = expandRhsOp.getAxis();\n+    if (expandLhsAxis != 2 || expandRhsAxis != 0)\n+      return mlir::failure();\n+    auto broadcastLhsShape =\n+        broadcastLhsOp.getType().cast<ShapedType>().getShape();\n+    auto broadcastRhsShape =\n+        broadcastLhsOp.getType().cast<ShapedType>().getShape();\n+    if (broadcastLhsShape[2] < 16 || broadcastRhsShape[0] < 16)\n+      return mlir::failure();\n+    Type newAccType =\n+        RankedTensorType::get({broadcastLhsShape[0], broadcastRhsShape[2]},\n+                              broadcastLhsOp.getOperand()\n+                                  .getType()\n+                                  .cast<ShapedType>()\n+                                  .getElementType());\n+    rewriter.setInsertionPoint(op);\n+    auto newAcc = rewriter.create<triton::SplatOp>(\n+        op->getLoc(), newAccType,\n+        rewriter.create<arith::ConstantOp>(op->getLoc(),\n+                                           rewriter.getF32FloatAttr(0)));\n+    rewriter.replaceOpWithNewOp<triton::DotOp>(\n+        op, expandLhsOp.getOperand(), expandRhsOp.getOperand(), newAcc, true);\n+    return mlir::success();\n+  }\n+};\n+\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/Triton/Transforms/Passes.h.inc\"\n \n@@ -120,6 +206,7 @@ class CombineOpsPass : public TritonCombineOpsBase<CombineOpsPass> {\n     patterns.add<CombineSelectMaskedLoadPattern>(context);\n     // patterns.add<CombineAddPtrPattern>(context);\n     patterns.add<CombineBroadcastConstantPattern>(context);\n+    patterns.add<CombineBroadcastMulReducePattern>(context);\n \n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n       signalPassFailure();"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 19, "deletions": 13, "changes": 32, "file_content_changes": "@@ -7,7 +7,6 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.cpp.inc\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"llvm/ADT/TypeSwitch.h\"\n \n using namespace mlir;\n@@ -222,28 +221,23 @@ SmallVector<unsigned> getContigPerThread(Attribute layout) {\n   }\n }\n \n-SmallVector<unsigned> getUniqueContigPerThread(Type type) {\n-  if (type.isIntOrIndexOrFloat() || type.isa<triton::PointerType>())\n-    return SmallVector<unsigned>(1, 1);\n-  auto tensorType = type.cast<RankedTensorType>();\n-  auto shape = tensorType.getShape();\n+SmallVector<unsigned> getUniqueContigPerThread(Attribute layout,\n+                                               ArrayRef<int64_t> shape) {\n   // If slice layout, call recursively on parent layout, and drop\n   // sliced dim\n-  if (auto sliceLayout =\n-          tensorType.getEncoding().dyn_cast<SliceEncodingAttr>()) {\n+  if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parentLayout = sliceLayout.getParent();\n     auto parentShape = sliceLayout.paddedShape(shape);\n-    auto parentTy = RankedTensorType::get(\n-        parentShape, tensorType.getElementType(), parentLayout);\n-    auto parentUniqueContigPerThread = getUniqueContigPerThread(parentTy);\n+    auto parentUniqueContigPerThread =\n+        getUniqueContigPerThread(parentLayout, parentShape);\n     parentUniqueContigPerThread.erase(parentUniqueContigPerThread.begin() +\n                                       sliceLayout.getDim());\n     return parentUniqueContigPerThread;\n   }\n   // Base case\n   auto rank = shape.size();\n   SmallVector<unsigned> ret(rank);\n-  auto contigPerThread = getContigPerThread(tensorType.getEncoding());\n+  auto contigPerThread = getContigPerThread(layout);\n   assert(contigPerThread.size() == rank && \"Unexpected contigPerThread size\");\n   for (int d = 0; d < rank; ++d) {\n     ret[d] = std::min<unsigned>(shape[d], contigPerThread[d]);\n@@ -368,9 +362,21 @@ bool isSharedEncoding(Value value) {\n   return false;\n }\n \n+bool isExpensiveCat(CatOp cat, Attribute &targetEncoding) {\n+  // If the new elements per thread is less than the old one, we will need to do\n+  // convert encoding that goes through shared memory anyway. So we consider it\n+  // as expensive.\n+  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n+  auto totalElemsPerThread = gpu::getTotalElemsPerThread(tensorTy);\n+  auto shape = tensorTy.getShape();\n+  auto elemTy = tensorTy.getElementType();\n+  auto newTotalElemsPerThread =\n+      gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n+  return newTotalElemsPerThread < totalElemsPerThread;\n+}\n+\n } // namespace gpu\n } // namespace triton\n-\n } // namespace mlir\n \n static LogicalResult parseIntAttrValue(AsmParser &parser, Attribute attr,"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 15, "deletions": 2, "changes": 17, "file_content_changes": "@@ -238,7 +238,9 @@ updateDotEncodingLayout(SmallVector<ConvertLayoutOp> &convertsToDotEncoding,\n                                 convertsToDotEncoding.end());\n   // Collect all the operations where the type needs to be propagated.\n   for (auto cvt : convertsToDotEncoding) {\n-    auto filter = [&](Operation *op) {\n+    auto forwardFilter = [&](Operation *op) {\n+      if (op == cvt.getOperation())\n+        return true;\n       for (Value operand : op->getOperands()) {\n         auto tensorType = operand.getType().dyn_cast<RankedTensorType>();\n         if (tensorType &&\n@@ -247,7 +249,18 @@ updateDotEncodingLayout(SmallVector<ConvertLayoutOp> &convertsToDotEncoding,\n       }\n       return false;\n     };\n-    mlir::getForwardSlice(cvt.getResult(), &slices, {filter});\n+    auto backwardFilter = [&](Operation *op) {\n+      for (Value results : op->getResults()) {\n+        auto tensorType = results.getType().dyn_cast<RankedTensorType>();\n+        if (tensorType &&\n+            tensorType.getEncoding().isa<DotOperandEncodingAttr>())\n+          return true;\n+      }\n+      return false;\n+    };\n+    SetVector<Operation *> opSlice =\n+        getSlice(cvt.getOperation(), {backwardFilter}, {forwardFilter});\n+    slices.insert(opSlice.begin(), opSlice.end());\n   }\n   // Apply the type change by walking ops in topological order.\n   slices = mlir::topologicalSort(slices);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 19, "deletions": 16, "changes": 35, "file_content_changes": "@@ -345,6 +345,14 @@ class RematerializeForward : public mlir::RewritePattern {\n     // heuristics for flash attention\n     if (srcEncoding.isa<triton::gpu::SliceEncodingAttr>())\n       return failure();\n+    // For cases like:\n+    // %0 = convert_layout %arg0\n+    // We should try to move %0 out of scf.for first, if it couldn't be moved\n+    // out additional conversions will be added to the loop body.\n+    if (!cvt.getOperand().getDefiningOp() &&\n+        isa<scf::ForOp>(cvt->getParentOp()))\n+      return failure();\n+\n     SetVector<Operation *> cvtSlices;\n     auto filter = [&](Operation *op) {\n       return op->getBlock() == cvt->getBlock() &&\n@@ -363,7 +371,8 @@ class RematerializeForward : public mlir::RewritePattern {\n       // don't rematerialize non-element-wise\n       if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n           !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n-          !isa<triton::StoreOp, triton::ReduceOp>(op))\n+          !isa<triton::StoreOp, triton::AssertOp, triton::PrintOp,\n+               triton::ReduceOp>(op))\n         return failure();\n       // don't rematerialize if it adds an extra conversion that can't\n       // be removed\n@@ -380,6 +389,10 @@ class RematerializeForward : public mlir::RewritePattern {\n       }\n     }\n \n+    // Call SimplifyReduceCvt instead of the general push conversion forward\n+    if (isa<triton::ReduceOp>(cvtSlices.front()))\n+      return failure();\n+\n     pushConversionForward(cvt, cvtSlices, rewriter);\n     return success();\n   }\n@@ -425,8 +438,8 @@ class RematerializeBackward : public mlir::RewritePattern {\n \n     IRMapping mapping;\n     rematerializeConversionChain(toConvert, rewriter, processed, mapping);\n-\n     rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n+\n     return mlir::success();\n   }\n };\n@@ -459,37 +472,27 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n     for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n       mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n     mapping.map(origConversion.getResult(), newForOp.getRegionIterArgs()[i]);\n-    // the iter arg of interest may have other uses than the conversion\n-    // we're hoisting out of the loop. If that's the case we will\n-    // need to add extra conversions for all uses... which is only useful\n-    // if these extra conversions can be removed by another pattern\n-    auto oldArg = forOp.getRegionIterArgs()[i];\n-    auto newArg = newForOp.getRegionIterArgs()[i];\n-    auto newArgFallback = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        newForOp.getLoc(), origType, newArg);\n \n     mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n     for (Operation &op : forOp.getBody()->without_terminator()) {\n       if (&op == (Operation *)(&origConversion))\n         continue;\n       Operation *newOp = rewriter.clone(op, mapping);\n-      if (find(oldArg.getUsers(), &op) != oldArg.getUsers().end())\n-        newOp->replaceUsesOfWith(newArg, newArgFallback);\n     }\n-\n     // create yield, inserting conversions if necessary\n     auto yieldOp = forOp.getBody()->getTerminator();\n     SmallVector<Value, 4> newYieldArgs;\n     for (Value arg : yieldOp->getOperands())\n       newYieldArgs.push_back(mapping.lookup(arg));\n-    newYieldArgs[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        yieldOp->getLoc(), newType, newYieldArgs[i]);\n+    if (newYieldArgs[i].getType() != newType)\n+      newYieldArgs[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+          yieldOp->getLoc(), newType, newYieldArgs[i]);\n     rewriter.create<scf::YieldOp>(forOp.getLoc(), newYieldArgs);\n \n     // replace\n     SmallVector<Value, 4> newResults = newForOp->getResults();\n     newResults[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        rewriter.getUnknownLoc(), origType, newForOp->getResult(i));\n+        newForOp.getLoc(), origType, newForOp->getResult(i));\n     newResults[i].getDefiningOp()->moveAfter(newForOp);\n     return newResults;\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/ReorderInstructions.cpp", "status": "modified", "additions": 13, "deletions": 6, "changes": 19, "file_content_changes": "@@ -1,6 +1,7 @@\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/Dominance.h\"\n #include \"mlir/IR/IRMapping.h\"\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/IR/PatternMatch.h\"\n@@ -42,6 +43,7 @@ class TritonGPUReorderInstructionsPass\n \n   void runOnOperation() override {\n     ModuleOp m = getOperation();\n+    mlir::DominanceInfo dom(m);\n     // Sink conversions into loops when they will increase\n     // register pressure\n     DenseMap<Operation *, Operation *> opToMove;\n@@ -78,26 +80,31 @@ class TritonGPUReorderInstructionsPass\n         return;\n       op->moveAfter(argOp);\n     });\n-    // Move `dot` operand so that conversions to opIdx=0 happens before\n-    // conversions to opIdx=1\n+    // Move `dot` operand so that conversions to opIdx=1 happens after\n+    // conversions to opIdx=0\n     m.walk([&](triton::gpu::ConvertLayoutOp op) {\n       auto dstType = op.getResult().getType().cast<RankedTensorType>();\n       auto dstEncoding =\n           dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n       if (!dstEncoding)\n         return;\n       int opIdx = dstEncoding.getOpIdx();\n-      if (opIdx != 0)\n+      if (opIdx != 1)\n         return;\n       if (op->getUsers().empty())\n         return;\n       auto dotUser = dyn_cast<triton::DotOp>(*op->user_begin());\n       if (!dotUser)\n         return;\n-      auto BOp = dotUser.getOperand(1).getDefiningOp();\n-      if (!BOp)\n+      auto AOp =\n+          dotUser.getOperand(0).getDefiningOp<triton::gpu::ConvertLayoutOp>();\n+      if (!AOp)\n         return;\n-      op->moveBefore(BOp);\n+      // Check that the conversion to OpIdx=1 happens before and can be moved\n+      // after the conversion to OpIdx=0.\n+      if (!dom.dominates(op.getOperation(), AOp.getOperation()))\n+        return;\n+      op->moveAfter(AOp);\n     });\n     return;\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 23, "deletions": 17, "changes": 40, "file_content_changes": "@@ -104,26 +104,13 @@ bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n   return true;\n }\n \n-bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding) {\n-  // If the new elements per thread is less than the old one, we will need to do\n-  // convert encoding that goes through shared memory anyway. So we consider it\n-  // as expensive.\n-  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n-  auto totalElemsPerThread = triton::gpu::getTotalElemsPerThread(tensorTy);\n-  auto shape = tensorTy.getShape();\n-  auto elemTy = tensorTy.getElementType();\n-  auto newTotalElemsPerThread =\n-      triton::gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n-  return newTotalElemsPerThread < totalElemsPerThread;\n-}\n-\n bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   if (!op)\n     return true;\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n     return isExpensiveLoadOrStore(op, targetEncoding);\n   if (isa<triton::CatOp>(op))\n-    return isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n+    return triton::gpu::isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n           triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n           triton::AtomicCASOp, triton::DotOp>(op))\n@@ -136,7 +123,8 @@ bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n \n bool canFoldConversion(Operation *op, Attribute &targetEncoding) {\n   if (isa<triton::CatOp>(op))\n-    return !isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n+    return !triton::gpu::isExpensiveCat(cast<triton::CatOp>(op),\n+                                        targetEncoding);\n   return isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n              triton::MakeRangeOp, triton::SplatOp, triton::ViewOp>(op);\n }\n@@ -297,7 +285,7 @@ LogicalResult canMoveOutOfLoop(BlockArgument arg,\n   // 2. There is only a single conversion\n   // 3. Moving this conversion out of the loop will not generate any extra\n   // non-removable conversion\n-  DenseSet<Type> cvtTypes;\n+  SetVector<RankedTensorType> cvtTypes;\n   SetVector<Operation *> others;\n   auto oldType = arg.getType().cast<RankedTensorType>();\n   for (auto user : arg.getUsers()) {\n@@ -326,16 +314,34 @@ LogicalResult canMoveOutOfLoop(BlockArgument arg,\n     // Second condition\n     if (others.empty())\n       return success();\n-    // Third condition: not complete\n+    // Third condition - part 1:\n     // If the other or the cvt is in the different block, we cannot push the\n     // conversion forward or backward\n     for (auto *cvt : cvts) {\n       if (cvt->getBlock() != forOp.getBody())\n         return failure();\n     }\n+    auto targetEncoding = cvtTypes.front().getEncoding();\n     for (auto *other : others) {\n+      // Third condition - part 2:\n+      // If the other non-cvt op is in the different block, we cannot push the\n+      // conversion forward or backward\n       if (other->getBlock() != forOp.getBody())\n         return failure();\n+      // Third condition - part 3:\n+      // %0 (enc1) = cvt %arg (enc0)\n+      // other %0 (enc1), %1 (enc0) => other %0 (enc1), %1 (enc1)\n+      // Check if %2 (enc1) = cvt %1 (enc0) can be eliminated\n+      SetVector<Operation *> processed;\n+      SetVector<Attribute> layout;\n+      llvm::MapVector<Value, Attribute> toConvert;\n+      for (auto operand : other->getOperands()) {\n+        auto argOp = operand.getDefiningOp();\n+        if (argOp && !isa<triton::gpu::ConvertLayoutOp>(argOp) &&\n+            simulateBackwardRematerialization(argOp, processed, layout,\n+                                              toConvert, targetEncoding) > 0)\n+          return failure();\n+      }\n     }\n     return success();\n   }"}, {"filename": "lib/Target/LLVMIR/CMakeLists.txt", "status": "modified", "additions": 7, "deletions": 1, "changes": 8, "file_content_changes": "@@ -1,10 +1,16 @@\n add_mlir_translation_library(TritonLLVMIR\n         LLVMIRTranslation.cpp\n+        LLVMDIScope.cpp\n \n         LINK_COMPONENTS\n         Core\n \n-        LINK_LIBS PUBLIC\n+        DEPENDS\n+        LLVMIRIncGen\n+\n+        LINK_LIBS\n+        ${CMAKE_DL_LIBS}\n+        PUBLIC\n         MLIRArithToLLVM\n         MLIRBuiltinToLLVMIRTranslation\n         MLIRExecutionEngineUtils"}, {"filename": "lib/Target/LLVMIR/LLVMDIScope.cpp", "status": "added", "additions": 148, "deletions": 0, "changes": 148, "file_content_changes": "@@ -0,0 +1,148 @@\n+#include \"triton/Target/LLVMIR/Passes.h\"\n+\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"llvm/BinaryFormat/Dwarf.h\"\n+#include \"llvm/Support/Debug.h\"\n+#include \"llvm/Support/Path.h\"\n+\n+//===----------------------------------------------------------------------===//\n+// This file implements a pass to add debug info scope to LLVM operations, and\n+// is inspired by the DIScopeForLLVMFuncOpPass in LLVM/MLIR. Different from the\n+// DIScopeForLLVMFuncOpPass, this pass also handles inlined functions.\n+//===----------------------------------------------------------------------===//\n+\n+using namespace mlir;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Target/LLVMIR/Passes.h.inc\"\n+\n+namespace {\n+\n+/// Attempt to extract a filename for the given loc.\n+FileLineColLoc extractFileLoc(Location loc) {\n+  if (auto fileLoc = dyn_cast<FileLineColLoc>(loc))\n+    return fileLoc;\n+  if (auto nameLoc = dyn_cast<NameLoc>(loc))\n+    return extractFileLoc(nameLoc.getChildLoc());\n+  if (auto opaqueLoc = dyn_cast<OpaqueLoc>(loc))\n+    return extractFileLoc(opaqueLoc.getFallbackLocation());\n+  if (auto fusedLoc = dyn_cast<FusedLoc>(loc))\n+    return extractFileLoc(fusedLoc.getLocations().front());\n+  if (auto callerLoc = dyn_cast<CallSiteLoc>(loc))\n+    return extractFileLoc(callerLoc.getCaller());\n+  StringAttr unknownFile = mlir::StringAttr::get(loc.getContext(), \"<unknown>\");\n+  return mlir::FileLineColLoc::get(unknownFile, 0, 0);\n+}\n+\n+/// Add a debug info scope to LLVMFuncOp that are missing it.\n+struct LLVMDIScopePass : public LLVMDIScopeBase<LLVMDIScopePass> {\n+  LLVMDIScopePass() = default;\n+\n+  void setSubprogramAttr(LLVM::LLVMFuncOp funcOp) {\n+    Location loc = funcOp.getLoc();\n+    if (loc->findInstanceOf<mlir::FusedLocWith<LLVM::DISubprogramAttr>>())\n+      return;\n+\n+    MLIRContext *context = &getContext();\n+\n+    // To find a DICompileUnitAttr attached to a parent (the module for\n+    // example), otherwise create a default one.\n+    LLVM::DICompileUnitAttr compileUnitAttr;\n+    if (ModuleOp module = funcOp->getParentOfType<ModuleOp>()) {\n+      auto fusedCompileUnitAttr =\n+          module->getLoc()\n+              ->findInstanceOf<mlir::FusedLocWith<LLVM::DICompileUnitAttr>>();\n+      if (fusedCompileUnitAttr)\n+        compileUnitAttr = fusedCompileUnitAttr.getMetadata();\n+    }\n+\n+    // Filename, line and colmun to associate to the function.\n+    LLVM::DIFileAttr fileAttr;\n+    int64_t line = 1, col = 1;\n+    FileLineColLoc fileLoc = extractFileLoc(loc);\n+    if (!fileLoc && compileUnitAttr) {\n+      fileAttr = compileUnitAttr.getFile();\n+    } else if (!fileLoc) {\n+      fileAttr = LLVM::DIFileAttr::get(context, \"<unknown>\", \"\");\n+    } else {\n+      line = fileLoc.getLine();\n+      col = fileLoc.getColumn();\n+      StringRef inputFilePath = fileLoc.getFilename().getValue();\n+      fileAttr = LLVM::DIFileAttr::get(\n+          context, llvm::sys::path::filename(inputFilePath),\n+          llvm::sys::path::parent_path(inputFilePath));\n+    }\n+    if (!compileUnitAttr) {\n+      compileUnitAttr = LLVM::DICompileUnitAttr::get(\n+          context, llvm::dwarf::DW_LANG_C, fileAttr,\n+          StringAttr::get(context, \"triton\"), /*isOptimized=*/true,\n+          LLVM::DIEmissionKind::LineTablesOnly);\n+    }\n+    auto subroutineTypeAttr =\n+        LLVM::DISubroutineTypeAttr::get(context, llvm::dwarf::DW_CC_normal, {});\n+\n+    StringAttr funcNameAttr = funcOp.getNameAttr();\n+    // Note that scopeline is set differently from LLVM's\n+    // DIScopeForLLVMFuncOpPass. I don't find reasons why scopeline should be\n+    // the column offset\n+    auto subprogramAttr =\n+        LLVM::DISubprogramAttr::get(context, compileUnitAttr, fileAttr,\n+                                    funcNameAttr, funcNameAttr, fileAttr,\n+                                    /*line=*/line,\n+                                    /*scopeline=*/line,\n+                                    LLVM::DISubprogramFlags::Definition |\n+                                        LLVM::DISubprogramFlags::Optimized,\n+                                    subroutineTypeAttr);\n+    funcOp->setLoc(FusedLoc::get(context, {loc}, subprogramAttr));\n+  }\n+\n+  // Get a nested loc for inlined functions\n+  Location getNestedLoc(Operation *op, LLVM::DIScopeAttr scopeAttr,\n+                        Location calleeLoc) {\n+    auto calleeFileName = extractFileLoc(calleeLoc).getFilename();\n+    auto context = op->getContext();\n+    LLVM::DIFileAttr calleeFileAttr = LLVM::DIFileAttr::get(\n+        context, llvm::sys::path::filename(calleeFileName),\n+        llvm::sys::path::parent_path(calleeFileName));\n+    auto lexicalBlockFileAttr = LLVM::DILexicalBlockFileAttr::get(\n+        context, scopeAttr, calleeFileAttr, /*discriminator=*/0);\n+    Location loc = op->getLoc();\n+    if (calleeLoc.isa<CallSiteLoc>()) {\n+      auto nestedLoc = calleeLoc.cast<CallSiteLoc>().getCallee();\n+      loc = getNestedLoc(op, lexicalBlockFileAttr, nestedLoc);\n+    }\n+    return FusedLoc::get(context, {loc}, lexicalBlockFileAttr);\n+  }\n+\n+  void setLexicalBlockFileAttr(Operation *op) {\n+    auto opLoc = op->getLoc();\n+    if (auto callSiteLoc = dyn_cast<CallSiteLoc>(opLoc)) {\n+      auto callerLoc = callSiteLoc.getCaller();\n+      auto calleeLoc = callSiteLoc.getCallee();\n+      LLVM::DIScopeAttr scopeAttr;\n+      // We assemble the full inline stack so the parent of this loc must be a\n+      // function\n+      auto funcOp = op->getParentOfType<LLVM::LLVMFuncOp>();\n+      auto funcOpLoc = funcOp.getLoc().cast<FusedLoc>();\n+      scopeAttr = funcOpLoc.getMetadata().cast<LLVM::DISubprogramAttr>();\n+      auto loc = getNestedLoc(op, scopeAttr, calleeLoc);\n+      op->setLoc(loc);\n+    }\n+  }\n+\n+  void runOnOperation() override {\n+    getOperation()->walk<WalkOrder::PreOrder>([&](Operation *op) -> void {\n+      if (isa<LLVM::LLVMFuncOp>(op))\n+        setSubprogramAttr(cast<LLVM::LLVMFuncOp>(op));\n+      else\n+        setLexicalBlockFileAttr(op);\n+    });\n+  }\n+};\n+\n+} // end anonymous namespace\n+\n+std::unique_ptr<Pass> mlir::createLLVMDIScopePass() {\n+  return std::make_unique<LLVMDIScopePass>();\n+}"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -2,6 +2,7 @@\n \n #include \"mlir/Conversion/Passes.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Dialect/LLVMIR/Transforms/Passes.h\"\n #include \"mlir/ExecutionEngine/ExecutionEngine.h\"\n #include \"mlir/ExecutionEngine/OptUtils.h\"\n #include \"mlir/IR/Dialect.h\"\n@@ -15,6 +16,7 @@\n #include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n #include \"mlir/Transforms/Passes.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n+#include \"triton/Target/LLVMIR/Passes.h\"\n #include \"triton/Tools/Sys/GetEnv.hpp\"\n #include \"triton/Tools/Sys/GetPlatform.hpp\"\n #include \"llvm/ADT/APInt.h\"\n@@ -329,6 +331,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n   }\n   auto printingFlags = mlir::OpPrintingFlags();\n   printingFlags.elideLargeElementsAttrs(16);\n+  printingFlags.enableDebugInfo();\n   pm.enableIRPrinting(\n       /*shouldPrintBeforePass=*/nullptr,\n       /*shouldPrintAfterPass=*/\n@@ -347,6 +350,8 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n   // Simplify the IR\n   pm.addPass(mlir::createCSEPass());\n   pm.addPass(mlir::createSymbolDCEPass());\n+  if (!::triton::tools::getBoolEnv(\"TRITON_DISABLE_LINE_INFO\"))\n+    pm.addPass(mlir::createLLVMDIScopePass());\n \n   if (failed(pm.run(module))) {\n     llvm::errs() << \"Pass execution failed\";"}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -211,6 +211,7 @@ def build_extension(self, ext):\n         # python directories\n         python_include_dir = sysconfig.get_path(\"platinclude\")\n         cmake_args = [\n+            \"-DCMAKE_EXPORT_COMPILE_COMMANDS=ON\",\n             \"-DLLVM_ENABLE_WERROR=ON\",\n             \"-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=\" + extdir,\n             \"-DTRITON_BUILD_TUTORIALS=OFF\","}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 509, "deletions": 505, "changes": 1014, "file_content_changes": "@@ -72,6 +72,90 @@ void init_triton_runtime(py::module &&m) {\n       .export_values();\n }\n \n+// A custom op builder that keeps track of the last location\n+class TritonOpBuilder {\n+public:\n+  TritonOpBuilder(mlir::MLIRContext *context) {\n+    builder = std::make_unique<mlir::OpBuilder>(context);\n+    lastLoc = std::make_unique<mlir::Location>(builder->getUnknownLoc());\n+  }\n+\n+  mlir::OpBuilder &getBuilder() { return *builder; }\n+\n+  bool isLineInfoEnabled() { return lineInfoEnabled; }\n+\n+  void setLastLoc(mlir::Location loc) {\n+    if (lineInfoEnabled)\n+      lastLoc = std::make_unique<mlir::Location>(loc);\n+  }\n+\n+  void setLastLoc(const std::string &fileName, int line, int column) {\n+    auto context = builder->getContext();\n+    setLastLoc(mlir::FileLineColLoc::get(context, fileName, line, column));\n+  }\n+\n+  mlir::Location getLastLoc() {\n+    assert(lastLoc);\n+    return *lastLoc;\n+  }\n+\n+  void setInsertionPointToStart(mlir::Block &block) {\n+    if (!block.empty())\n+      setLastLoc(block.begin()->getLoc());\n+    else\n+      setLastLoc(builder->getUnknownLoc());\n+    builder->setInsertionPointToStart(&block);\n+  }\n+\n+  void setInsertionPointToEnd(mlir::Block &block) {\n+    if (!block.empty())\n+      setLastLoc(block.back().getLoc());\n+    else\n+      setLastLoc(builder->getUnknownLoc());\n+    builder->setInsertionPointToEnd(&block);\n+  }\n+\n+  void setInsertionPointAfter(mlir::Operation &op) {\n+    setLastLoc(op.getLoc());\n+    builder->setInsertionPointAfter(&op);\n+  }\n+\n+  void restoreInsertionPoint(mlir::OpBuilder::InsertPoint pt) {\n+    if (pt.isSet() && pt.getPoint() != pt.getBlock()->end())\n+      setLastLoc(pt.getPoint()->getLoc());\n+    else\n+      setLastLoc(builder->getUnknownLoc());\n+    builder->restoreInsertionPoint(pt);\n+  }\n+\n+  template <typename OpTy, typename... Args> OpTy create(Args &&...args) {\n+    auto loc = getLastLoc();\n+    return builder->create<OpTy>(loc, std::forward<Args>(args)...);\n+  }\n+\n+  // Overload to create or fold a single result operation.\n+  template <typename OpTy, typename... Args>\n+  std::enable_if_t<OpTy::template hasTrait<mlir::OpTrait::OneResult>(),\n+                   mlir::Value>\n+  createOrFold(Args &&...args) {\n+    auto loc = getLastLoc();\n+    return builder->createOrFold<OpTy>(loc, std::forward<Args>(args)...);\n+  }\n+\n+  // Overload to create or fold a zero result operation.\n+  template <typename OpTy, typename... Args>\n+  std::enable_if_t<OpTy::template hasTrait<mlir::OpTrait::ZeroResults>(), OpTy>\n+  createOrFold(Args &&...args) {\n+    auto loc = getLastLoc();\n+    return builder->createOrFold<OpTy>(loc, std::forward<Args>(args)...);\n+  }\n+\n+private:\n+  std::unique_ptr<mlir::OpBuilder> builder;\n+  std::unique_ptr<mlir::Location> lastLoc;\n+  bool lineInfoEnabled = !triton::tools::getBoolEnv(\"TRITON_DISABLE_LINE_INFO\");\n+};\n+\n /*****************************************************************************/\n /* Python bindings for triton::ir                                            */\n /*****************************************************************************/\n@@ -91,6 +175,7 @@ void init_triton_ir(py::module &&m) {\n       .value(\"CG\", mlir::triton::CacheModifier::CG)\n       .value(\"WB\", mlir::triton::CacheModifier::WB)\n       .value(\"CS\", mlir::triton::CacheModifier::CS)\n+      .value(\"WT\", mlir::triton::CacheModifier::WT)\n       .export_values();\n \n   py::enum_<mlir::triton::MemSemantic>(m, \"MEM_SEMANTIC\")\n@@ -190,6 +275,14 @@ void init_triton_ir(py::module &&m) {\n                                        self.getInputs().end());\n       });\n \n+  py::class_<mlir::Location>(m, \"location\")\n+      .def(\"__str__\", [](mlir::Location &self) {\n+        std::string str;\n+        llvm::raw_string_ostream os(str);\n+        self.print(os);\n+        return os.str();\n+      });\n+\n   py::class_<mlir::Value>(m, \"value\")\n       .def(\"set_attr\",\n            [](mlir::Value &self, std::string &name,\n@@ -480,367 +573,347 @@ void init_triton_ir(py::module &&m) {\n \n   py::class_<mlir::OpBuilder::InsertPoint>(m, \"InsertPoint\");\n \n-  py::class_<mlir::OpBuilder>(m, \"builder\", py::dynamic_attr())\n+  py::class_<TritonOpBuilder>(m, \"builder\", py::dynamic_attr())\n       .def(py::init<mlir::MLIRContext *>())\n-      // // getters\n-      .def_property_readonly(\"context\", &mlir::OpBuilder::getContext,\n-                             ret::reference)\n+      // getters\n       .def(\"create_module\",\n-           [](mlir::OpBuilder &self) -> mlir::ModuleOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::ModuleOp>(loc);\n-           })\n-      .def(\"ret\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Value> &vals) -> void {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::triton::ReturnOp>(loc, vals);\n-           })\n-      .def(\"call\",\n-           [](mlir::OpBuilder &self, mlir::triton::FuncOp &func,\n-              std::vector<mlir::Value> &args) -> mlir::OpState {\n-             auto loc = self.getUnknownLoc();\n-             auto callOp = self.create<mlir::triton::CallOp>(loc, func, args);\n-             return callOp;\n+           [](TritonOpBuilder &self) -> mlir::ModuleOp {\n+             return self.create<mlir::ModuleOp>();\n            })\n       // insertion block/point\n       .def(\"set_insertion_point_to_start\",\n-           [](mlir::OpBuilder &self, mlir::Block &block) -> void {\n-             self.setInsertionPointToStart(&block);\n+           [](TritonOpBuilder &self, mlir::Block &block) -> void {\n+             self.setInsertionPointToStart(block);\n            })\n       .def(\"set_insertion_point_to_end\",\n-           [](mlir::OpBuilder &self, mlir::Block &block) {\n-             self.setInsertionPointToEnd(&block);\n+           [](TritonOpBuilder &self, mlir::Block &block) {\n+             self.setInsertionPointToEnd(block);\n            })\n       .def(\"set_insertion_point_after\",\n-           [](mlir::OpBuilder &self, mlir::Operation &op) {\n-             self.setInsertionPointAfter(&op);\n+           [](TritonOpBuilder &self, mlir::Operation &op) {\n+             self.setInsertionPointAfter(op);\n            })\n       .def(\n           \"get_insertion_block\",\n-          [](mlir::OpBuilder &self) -> mlir::Block * {\n-            return self.getInsertionBlock();\n+          [](TritonOpBuilder &self) -> mlir::Block * {\n+            return self.getBuilder().getInsertionBlock();\n           },\n           ret::reference)\n-      .def(\"get_insertion_point\", &mlir::OpBuilder::saveInsertionPoint)\n-      .def(\"restore_insertion_point\", &mlir::OpBuilder::restoreInsertionPoint)\n-      // .def(\"set_insert_point\", [](ir::builder *self,\n-      // std::pair<ir::basic_block*, ir::instruction*> pt) {\n-      //   ir::basic_block *bb = pt.first;\n-      //   ir::instruction *instr = pt.second;\n-      //   if (instr) {\n-      //     if (bb != instr->get_parent())\n-      //       throw std::runtime_error(\"invalid insertion point, instr not in\n-      //       bb\");\n-      //     self->set_insert_point(instr);\n-      //   } else {\n-      //     assert(bb);\n-      //     self->set_insert_point(bb);\n-      //   }\n-      // })\n+      .def(\"get_insertion_point\",\n+           [](TritonOpBuilder &self) {\n+             return self.getBuilder().saveInsertionPoint();\n+           })\n+      .def(\"restore_insertion_point\",\n+           [](TritonOpBuilder &self, mlir::OpBuilder::InsertPoint pt) {\n+             self.restoreInsertionPoint(pt);\n+           })\n       // Attr\n-      .def(\"get_bool_attr\", &mlir::OpBuilder::getBoolAttr)\n-      .def(\"get_int32_attr\", &mlir::OpBuilder::getI32IntegerAttr)\n+      .def(\"get_bool_attr\",\n+           [](TritonOpBuilder &self, bool value) {\n+             return self.getBuilder().getBoolAttr(value);\n+           })\n+      .def(\"get_int32_attr\",\n+           [](TritonOpBuilder &self, int32_t value) {\n+             return self.getBuilder().getI32IntegerAttr(value);\n+           })\n       // Use arith.ConstantOp to create constants\n       // Constants\n       .def(\"get_int1\",\n-           [](mlir::OpBuilder &self, bool v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, bool v) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n-                 loc, v, self.getI1Type()));\n+                 v, self.getBuilder().getI1Type()));\n            })\n       .def(\"get_int8\",\n-           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int64_t v) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n-                 loc, v, self.getI8Type()));\n+                 v, self.getBuilder().getI8Type()));\n            })\n       .def(\"get_int16\",\n-           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int64_t v) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n-                 loc, v, self.getI16Type()));\n+                 v, self.getBuilder().getI16Type()));\n            })\n       .def(\"get_int32\",\n-           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int64_t v) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n-                 loc, v, self.getI32Type()));\n+                 v, self.getBuilder().getI32Type()));\n            })\n       .def(\"get_int64\",\n-           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int64_t v) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n-                 loc, v, self.getI64Type()));\n+                 v, self.getBuilder().getI64Type()));\n            })\n       .def(\"get_bf16\",\n-           [](mlir::OpBuilder &self, float v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             auto type = self.getBF16Type();\n+           [](TritonOpBuilder &self, float v) -> mlir::Value {\n+             auto type = self.getBuilder().getBF16Type();\n              return self.create<mlir::arith::ConstantFloatOp>(\n-                 loc,\n                  mlir::APFloat(type.getFloatSemantics(), std::to_string(v)),\n                  type);\n            })\n       .def(\"get_fp16\",\n-           [](mlir::OpBuilder &self, float v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, float v) -> mlir::Value {\n              return self.create<mlir::arith::ConstantOp>(\n-                 loc, self.getF16FloatAttr(v));\n+                 self.getBuilder().getF16FloatAttr(v));\n            })\n       .def(\"get_fp32\",\n-           [](mlir::OpBuilder &self, float v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, float v) -> mlir::Value {\n              return self.create<mlir::arith::ConstantOp>(\n-                 loc, self.getF32FloatAttr(v));\n+                 self.getBuilder().getF32FloatAttr(v));\n            })\n       .def(\"get_fp64\",\n-           [](mlir::OpBuilder &self, double v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, double v) -> mlir::Value {\n              return self.create<mlir::arith::ConstantOp>(\n-                 loc, self.getF64FloatAttr(v));\n+                 self.getBuilder().getF64FloatAttr(v));\n            })\n       .def(\"get_null_value\",\n-           [](mlir::OpBuilder &self, mlir::Type type) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, mlir::Type type) -> mlir::Value {\n              if (auto floatTy = type.dyn_cast<mlir::FloatType>())\n                return self.create<mlir::arith::ConstantFloatOp>(\n-                   loc, mlir::APFloat(floatTy.getFloatSemantics(), 0), floatTy);\n+                   mlir::APFloat(floatTy.getFloatSemantics(), 0), floatTy);\n              else if (auto intTy = type.dyn_cast<mlir::IntegerType>())\n-               return self.create<mlir::arith::ConstantIntOp>(loc, 0, intTy);\n+               return self.create<mlir::arith::ConstantIntOp>(0, intTy);\n              else\n                throw std::runtime_error(\"Not implemented\");\n            })\n       .def(\"get_all_ones_value\",\n-           [](mlir::OpBuilder &self, mlir::Type type) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, mlir::Type type) -> mlir::Value {\n              uint64_t val = 0xFFFFFFFFFFFFFFFF;\n              if (auto intTy = type.dyn_cast<mlir::IntegerType>())\n-               return self.create<mlir::arith::ConstantIntOp>(loc, val, intTy);\n+               return self.create<mlir::arith::ConstantIntOp>(val, intTy);\n              else\n                throw std::runtime_error(\"Not implemented\");\n            })\n \n       // Types\n       .def(\"get_void_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getNoneType();\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getNoneType();\n            })\n       .def(\"get_int1_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getI1Type();\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getI1Type();\n            }) // or ret::copy?\n       .def(\"get_int8_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type { return self.getI8Type(); })\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getI8Type();\n+           })\n       .def(\"get_int16_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getType<mlir::IntegerType>(16);\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getType<mlir::IntegerType>(16);\n+           })\n+      .def(\"get_int32_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getI32Type();\n+           })\n+      .def(\"get_int64_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getI64Type();\n            })\n-      .def(\n-          \"get_int32_ty\",\n-          [](mlir::OpBuilder &self) -> mlir::Type { return self.getI32Type(); })\n-      .def(\n-          \"get_int64_ty\",\n-          [](mlir::OpBuilder &self) -> mlir::Type { return self.getI64Type(); })\n       .def(\"get_fp8e4_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getType<mlir::Float8E4M3FNUZType>();\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getType<mlir::Float8E4M3FNUZType>();\n            })\n       .def(\"get_fp8e4b15_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n+           [](TritonOpBuilder &self) -> mlir::Type {\n              // TODO: upstream FP8E4B15 into MLIR, or find a way to externally\n              // have a float-like type compatible with float only native ops\n-             return self.getType<mlir::Float8E4M3B11FNUZType>();\n+             return self.getBuilder().getType<mlir::Float8E4M3B11FNUZType>();\n            })\n       .def(\"get_fp8e5_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getType<mlir::Float8E5M2Type>();\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getType<mlir::Float8E5M2Type>();\n+           })\n+      .def(\"get_half_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getF16Type();\n            })\n-      .def(\n-          \"get_half_ty\",\n-          [](mlir::OpBuilder &self) -> mlir::Type { return self.getF16Type(); })\n       .def(\"get_bf16_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getBF16Type();\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getBF16Type();\n+           })\n+      .def(\"get_float_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getF32Type();\n+           })\n+      .def(\"get_double_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getF64Type();\n            })\n-      .def(\n-          \"get_float_ty\",\n-          [](mlir::OpBuilder &self) -> mlir::Type { return self.getF32Type(); })\n-      .def(\n-          \"get_double_ty\",\n-          [](mlir::OpBuilder &self) -> mlir::Type { return self.getF64Type(); })\n       .def(\"get_ptr_ty\",\n-           [](mlir::OpBuilder &self, mlir::Type &type,\n+           [](TritonOpBuilder &self, mlir::Type &type,\n               int addrSpace) -> mlir::Type {\n              return mlir::triton::PointerType::get(type, addrSpace);\n            })\n       .def(\"get_block_ty\",\n-           [](mlir::OpBuilder &self, mlir::Type &elementType,\n+           [](TritonOpBuilder &self, mlir::Type &elementType,\n               std::vector<int64_t> &shape) -> mlir::Type {\n              return mlir::RankedTensorType::get(shape, elementType);\n            })\n       .def(\"get_function_ty\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Type> inTypes,\n+           [](TritonOpBuilder &self, std::vector<mlir::Type> inTypes,\n               std::vector<mlir::Type> outTypes) -> mlir::Type {\n-             return self.getFunctionType(inTypes, outTypes);\n+             return self.getBuilder().getFunctionType(inTypes, outTypes);\n+           })\n+      // locs\n+      .def(\"set_loc\", [](TritonOpBuilder &self,\n+                         mlir::Location loc) { self.setLastLoc(loc); })\n+      .def(\"set_loc\",\n+           [](TritonOpBuilder &self, const std::string &fileName, int line,\n+              int column) { self.setLastLoc(fileName, line, column); })\n+      .def(\"get_loc\",\n+           [](TritonOpBuilder &self) -> mlir::Location {\n+             return self.getLastLoc();\n            })\n \n       // Ops\n       .def(\"get_or_insert_function\",\n-           [](mlir::OpBuilder &self, mlir::ModuleOp &module,\n+           [](TritonOpBuilder &self, mlir::ModuleOp &module,\n               std::string &funcName, mlir::Type &funcType,\n               std::string &visibility, bool noinline) -> mlir::triton::FuncOp {\n              if (mlir::Operation *funcOperation = module.lookupSymbol(funcName))\n                return llvm::dyn_cast<mlir::triton::FuncOp>(funcOperation);\n-             auto loc = self.getUnknownLoc();\n              if (auto funcTy = funcType.dyn_cast<mlir::FunctionType>()) {\n                llvm::SmallVector<mlir::NamedAttribute> attrs = {\n-                   mlir::NamedAttribute(self.getStringAttr(\"sym_visibility\"),\n-                                        self.getStringAttr(visibility)),\n-                   mlir::NamedAttribute(self.getStringAttr(\"noinline\"),\n-                                        self.getBoolAttr(noinline))};\n-               return self.create<mlir::triton::FuncOp>(loc, funcName, funcTy,\n+                   mlir::NamedAttribute(\n+                       self.getBuilder().getStringAttr(\"sym_visibility\"),\n+                       self.getBuilder().getStringAttr(visibility)),\n+                   mlir::NamedAttribute(\n+                       self.getBuilder().getStringAttr(\"noinline\"),\n+                       self.getBuilder().getBoolAttr(noinline))};\n+               return self.create<mlir::triton::FuncOp>(funcName, funcTy,\n                                                         attrs);\n              }\n              throw std::runtime_error(\"invalid function type\");\n            })\n       .def(\n           \"create_block\",\n-          [](mlir::OpBuilder &self) -> mlir::Block * {\n-            mlir::Region *parent = self.getBlock()->getParent();\n-            return self.createBlock(parent);\n+          [](TritonOpBuilder &self) -> mlir::Block * {\n+            mlir::Region *parent = self.getBuilder().getBlock()->getParent();\n+            return self.getBuilder().createBlock(parent);\n           },\n           ret::reference)\n       .def(\n           \"create_block_with_parent\",\n-          [](mlir::OpBuilder &self, mlir::Region &parent,\n+          [](TritonOpBuilder &self, mlir::Region &parent,\n              std::vector<mlir::Type> &argTypes) -> mlir::Block * {\n-            auto argLoc = self.getUnknownLoc();\n-            llvm::SmallVector<mlir::Location, 8> argLocs(argTypes.size(),\n-                                                         argLoc);\n-            return self.createBlock(&parent, {}, argTypes, argLocs);\n+            // TODO: update arg loc\n+            auto loc = self.getBuilder().getUnknownLoc();\n+            llvm::SmallVector<mlir::Location, 8> argLocs(argTypes.size(), loc);\n+            return self.getBuilder().createBlock(&parent, {}, argTypes,\n+                                                 argLocs);\n           },\n           ret::reference)\n       .def(\n           \"new_block\",\n-          [](mlir::OpBuilder &self) -> mlir::Block * {\n+          [](TritonOpBuilder &self) -> mlir::Block * {\n             return new mlir::Block();\n           },\n           ret::reference)\n+      // Function\n+      .def(\"ret\",\n+           [](TritonOpBuilder &self,\n+              std::vector<mlir::Value> &vals) -> mlir::OpState {\n+             return self.create<mlir::triton::ReturnOp>(vals);\n+           })\n+      .def(\"call\",\n+           [](TritonOpBuilder &self, mlir::triton::FuncOp &func,\n+              std::vector<mlir::Value> &args) -> mlir::OpState {\n+             return self.create<mlir::triton::CallOp>(func, args);\n+           })\n       // Unstructured control flow\n       .def(\"create_cond_branch\",\n-           [](mlir::OpBuilder &self, mlir::Value condition,\n-              mlir::Block *trueDest, mlir::Block *falseDest) {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::cf::CondBranchOp>(loc, condition, trueDest,\n-                                                 falseDest);\n-             return;\n+           [](TritonOpBuilder &self, mlir::Value condition,\n+              mlir::Block *trueDest, mlir::Block *falseDest) -> mlir::OpState {\n+             return self.create<mlir::cf::CondBranchOp>(condition, trueDest,\n+                                                        falseDest);\n            })\n       .def(\"create_branch\",\n-           [](mlir::OpBuilder &self, mlir::Block *dest,\n-              std::vector<mlir::Value> &args) {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::cf::BranchOp>(loc, dest, args);\n-             return;\n+           [](TritonOpBuilder &self, mlir::Block *dest,\n+              std::vector<mlir::Value> &args) -> mlir::OpState {\n+             return self.create<mlir::cf::BranchOp>(dest, args);\n            })\n       // Structured control flow\n       .def(\"create_for_op\",\n-           [](mlir::OpBuilder &self, mlir::Value &lb, mlir::Value &ub,\n+           [](TritonOpBuilder &self, mlir::Value &lb, mlir::Value &ub,\n               mlir::Value &step,\n               std::vector<mlir::Value> &initArgs) -> mlir::scf::ForOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::scf::ForOp>(loc, lb, ub, step, initArgs);\n+             return self.create<mlir::scf::ForOp>(lb, ub, step, initArgs);\n            })\n       .def(\"create_if_op\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Type> &retTypes,\n+           [](TritonOpBuilder &self, std::vector<mlir::Type> &retTypes,\n               mlir::Value &condition, bool withElse) -> mlir::scf::IfOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::scf::IfOp>(loc, retTypes, condition,\n-                                                 withElse);\n+             return self.create<mlir::scf::IfOp>(retTypes, condition, withElse);\n            })\n       .def(\"create_yield_op\",\n-           [](mlir::OpBuilder &self,\n+           [](TritonOpBuilder &self,\n               std::vector<mlir::Value> &yields) -> mlir::scf::YieldOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::scf::YieldOp>(loc, yields);\n+             return self.create<mlir::scf::YieldOp>(yields);\n            })\n       .def(\"create_while_op\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Type> &retTypes,\n+           [](TritonOpBuilder &self, std::vector<mlir::Type> &retTypes,\n               std::vector<mlir::Value> &initArgs) -> mlir::scf::WhileOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::scf::WhileOp>(loc, retTypes, initArgs);\n+             return self.create<mlir::scf::WhileOp>(retTypes, initArgs);\n            })\n       .def(\"create_condition_op\",\n-           [](mlir::OpBuilder &self, mlir::Value &cond,\n+           [](TritonOpBuilder &self, mlir::Value &cond,\n               std::vector<mlir::Value> &args) -> mlir::scf::ConditionOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::scf::ConditionOp>(loc, cond, args);\n+             return self.create<mlir::scf::ConditionOp>(cond, args);\n            })\n \n       // miscellaneous\n       .def(\"create_make_range\",\n-           [](mlir::OpBuilder &self, int start, int end) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             auto retType =\n-                 mlir::RankedTensorType::get({end - start}, self.getI32Type());\n-             return self.create<mlir::triton::MakeRangeOp>(loc, retType, start,\n-                                                           end);\n+           [](TritonOpBuilder &self, int start, int end) -> mlir::Value {\n+             auto retType = mlir::RankedTensorType::get(\n+                 {end - start}, self.getBuilder().getI32Type());\n+             return self.create<mlir::triton::MakeRangeOp>(retType, start, end);\n            })\n \n       // Cast instructions\n       // Conversions for custom FP types (FP8)\n       .def(\"create_fp_to_fp\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::FpToFpOp>(loc, dstType, src);\n+             return self.create<mlir::triton::FpToFpOp>(dstType, src);\n            })\n       // Conversions for standard LLVM builtin types\n       .def(\"create_bitcast\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::BitcastOp>(loc, dstType, src);\n+             return self.create<mlir::triton::BitcastOp>(dstType, src);\n            })\n       .def(\"create_si_to_fp\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::SIToFPOp>(loc, dstType, src);\n+             return self.create<mlir::arith::SIToFPOp>(dstType, src);\n            })\n       .def(\"create_ui_to_fp\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::UIToFPOp>(loc, dstType, src);\n+             return self.create<mlir::arith::UIToFPOp>(dstType, src);\n            })\n       .def(\"create_fp_to_si\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::FPToSIOp>(loc, dstType, src);\n+             return self.create<mlir::arith::FPToSIOp>(dstType, src);\n            })\n       .def(\"create_fp_to_ui\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::FPToUIOp>(loc, dstType, src);\n+             return self.create<mlir::arith::FPToUIOp>(dstType, src);\n            })\n       .def(\"create_fp_ext\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::ExtFOp>(loc, dstType, src);\n+             return self.create<mlir::arith::ExtFOp>(dstType, src);\n            })\n       .def(\"create_fp_trunc\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::TruncFOp>(loc, dstType, src);\n+             return self.create<mlir::arith::TruncFOp>(dstType, src);\n            })\n       .def(\"create_int_cast\",\n-           [](mlir::OpBuilder &self, mlir::Value &src, mlir::Type &dstType,\n+           [](TritonOpBuilder &self, mlir::Value &src, mlir::Type &dstType,\n               bool isSigned) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              // get element type if necessary\n              mlir::Type srcType = src.getType();\n              auto srcTensorType = srcType.dyn_cast<mlir::RankedTensorType>();\n@@ -854,387 +927,330 @@ void init_triton_ir(py::module &&m) {\n              unsigned srcWidth = srcEltType.getIntOrFloatBitWidth();\n              unsigned dstWidth = dstEltType.getIntOrFloatBitWidth();\n              if (srcWidth == dstWidth)\n-               return self.create<mlir::arith::BitcastOp>(loc, dstType, src);\n+               return self.create<mlir::arith::BitcastOp>(dstType, src);\n              else if (srcWidth > dstWidth)\n-               return self.create<mlir::arith::TruncIOp>(loc, dstType, src);\n+               return self.create<mlir::arith::TruncIOp>(dstType, src);\n              else if (isSigned)\n-               return self.create<mlir::arith::ExtSIOp>(loc, dstType, src);\n+               return self.create<mlir::arith::ExtSIOp>(dstType, src);\n              else\n-               return self.create<mlir::arith::ExtUIOp>(loc, dstType, src);\n+               return self.create<mlir::arith::ExtUIOp>(dstType, src);\n            })\n       .def(\"create_to_index\",\n-           [](mlir::OpBuilder &self, mlir::Value &input) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, mlir::Value &input) -> mlir::Value {\n              return self.create<mlir::arith::IndexCastOp>(\n-                 loc, self.getIndexType(), input);\n+                 self.getBuilder().getIndexType(), input);\n            })\n       .def(\"create_index_to_si\",\n-           [](mlir::OpBuilder &self, mlir::Value &input) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, mlir::Value &input) -> mlir::Value {\n              return self.create<mlir::arith::IndexCastOp>(\n-                 loc, self.getI64Type(), input);\n+                 self.getBuilder().getI64Type(), input);\n            })\n       .def(\"create_fmul\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::MulFOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::MulFOp>(lhs, rhs);\n            })\n       .def(\"create_fdiv\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::DivFOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::DivFOp>(lhs, rhs);\n            })\n       .def(\"create_frem\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::RemFOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::RemFOp>(lhs, rhs);\n            })\n       .def(\"create_fadd\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::AddFOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::AddFOp>(lhs, rhs);\n            })\n       .def(\"create_fsub\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::SubFOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::SubFOp>(lhs, rhs);\n            })\n       .def(\"create_mul\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::MulIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::MulIOp>(lhs, rhs);\n            })\n       .def(\"create_sdiv\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::DivSIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::DivSIOp>(lhs, rhs);\n            })\n       .def(\"create_udiv\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::DivUIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::DivUIOp>(lhs, rhs);\n            })\n       .def(\"create_srem\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::RemSIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::RemSIOp>(lhs, rhs);\n            })\n       .def(\"create_urem\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::RemUIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::RemUIOp>(lhs, rhs);\n            })\n       .def(\"create_add\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::AddIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::AddIOp>(lhs, rhs);\n            })\n       .def(\"create_sub\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return mlir::Value(\n-                 self.create<mlir::arith::SubIOp>(loc, lhs, rhs));\n+             return mlir::Value(self.create<mlir::arith::SubIOp>(lhs, rhs));\n            })\n       .def(\"create_shl\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return mlir::Value(\n-                 self.create<mlir::arith::ShLIOp>(loc, lhs, rhs));\n+             return mlir::Value(self.create<mlir::arith::ShLIOp>(lhs, rhs));\n            })\n       .def(\"create_lshr\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return mlir::Value(\n-                 self.create<mlir::arith::ShRUIOp>(loc, lhs, rhs));\n+             return mlir::Value(self.create<mlir::arith::ShRUIOp>(lhs, rhs));\n            })\n       .def(\"create_ashr\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return mlir::Value(\n-                 self.create<mlir::arith::ShRSIOp>(loc, lhs, rhs));\n+             return mlir::Value(self.create<mlir::arith::ShRSIOp>(lhs, rhs));\n            })\n       // AddPtr (similar to GEP)\n       .def(\"create_addptr\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptr,\n+           [](TritonOpBuilder &self, mlir::Value &ptr,\n               mlir::Value &offset) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::AddPtrOp>(loc, ptr.getType(), ptr,\n+             return self.create<mlir::triton::AddPtrOp>(ptr.getType(), ptr,\n                                                         offset);\n            })\n       // Comparison (int)\n       .def(\"create_icmpSLE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::sle, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::sle, lhs, rhs);\n            })\n       .def(\"create_icmpSLT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::slt, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::slt, lhs, rhs);\n            })\n       .def(\"create_icmpSGE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::sge, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::sge, lhs, rhs);\n            })\n       .def(\"create_icmpSGT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::sgt, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::sgt, lhs, rhs);\n            })\n       .def(\"create_icmpULE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::ule, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::ule, lhs, rhs);\n            })\n       .def(\"create_icmpULT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::ult, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::ult, lhs, rhs);\n            })\n       .def(\"create_icmpUGE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::uge, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::uge, lhs, rhs);\n            })\n       .def(\"create_icmpUGT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::ugt, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::ugt, lhs, rhs);\n            })\n       .def(\"create_icmpEQ\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::eq, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::eq, lhs, rhs);\n            })\n       .def(\"create_icmpNE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::ne, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::ne, lhs, rhs);\n            })\n       // Comparison (float)\n       .def(\"create_fcmpOLT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::OLT, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::OLT, lhs, rhs);\n            })\n       .def(\"create_fcmpOGT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::OGT, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::OGT, lhs, rhs);\n            })\n       .def(\"create_fcmpOLE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::OLE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::OLE, lhs, rhs);\n            })\n       .def(\"create_fcmpOGE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::OGE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::OGE, lhs, rhs);\n            })\n       .def(\"create_fcmpOEQ\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::OEQ, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::OEQ, lhs, rhs);\n            })\n       .def(\"create_fcmpONE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::ONE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::ONE, lhs, rhs);\n            })\n       .def(\"create_fcmpULT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::ULT, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::ULT, lhs, rhs);\n            })\n       .def(\"create_fcmpUGT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::UGT, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::UGT, lhs, rhs);\n            })\n       .def(\"create_fcmpULE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::ULE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::ULE, lhs, rhs);\n            })\n       .def(\"create_fcmpUGE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::UGE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::UGE, lhs, rhs);\n            })\n       .def(\"create_fcmpUEQ\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::UEQ, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::UEQ, lhs, rhs);\n            })\n       .def(\"create_fcmpUNE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::UNE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::UNE, lhs, rhs);\n            })\n       // // Logical\n       .def(\"create_and\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::AndIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::AndIOp>(lhs, rhs);\n            })\n       .def(\"create_xor\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::XOrIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::XOrIOp>(lhs, rhs);\n            })\n       .def(\"create_or\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::OrIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::OrIOp>(lhs, rhs);\n            })\n       // Input/Output\n       .def(\"create_load\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptrs,\n+           [](TritonOpBuilder &self, mlir::Value &ptrs,\n               mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy,\n               bool isVolatile) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::triton::LoadOp>(\n-                 loc, ptrs, cacheModifier, evictionPolicy, isVolatile);\n+                 ptrs, cacheModifier, evictionPolicy, isVolatile);\n            })\n       .def(\"create_store\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptrs, mlir::Value &value,\n+           [](TritonOpBuilder &self, mlir::Value &ptrs, mlir::Value &value,\n               mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy) -> void {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::triton::StoreOp>(loc, ptrs, value, cacheModifier,\n+             self.create<mlir::triton::StoreOp>(ptrs, value, cacheModifier,\n                                                 evictionPolicy);\n            })\n       .def(\"create_tensor_pointer_load\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptr,\n+           [](TritonOpBuilder &self, mlir::Value &ptr,\n               std::vector<int32_t> &boundaryCheck,\n               std::optional<mlir::triton::PaddingOption> paddingOption,\n               mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy,\n               bool isVolatile) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::triton::LoadOp>(\n-                 loc, ptr, boundaryCheck, paddingOption, cacheModifier,\n+                 ptr, boundaryCheck, paddingOption, cacheModifier,\n                  evictionPolicy, isVolatile);\n            })\n       .def(\"create_tensor_pointer_store\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptr, mlir::Value &val,\n+           [](TritonOpBuilder &self, mlir::Value &ptr, mlir::Value &val,\n               std::vector<int32_t> &boundaryCheck,\n               mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy) -> void {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::triton::StoreOp>(loc, ptr, val, boundaryCheck,\n+             self.create<mlir::triton::StoreOp>(ptr, val, boundaryCheck,\n                                                 cacheModifier, evictionPolicy);\n            })\n       .def(\"create_masked_load\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptrs, mlir::Value &mask,\n+           [](TritonOpBuilder &self, mlir::Value &ptrs, mlir::Value &mask,\n               std::optional<mlir::Value> &other,\n               mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy,\n               bool isVolatile) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::triton::LoadOp>(\n-                 loc, ptrs, mask, other.value_or(mlir::Value()), cacheModifier,\n+                 ptrs, mask, other.value_or(mlir::Value()), cacheModifier,\n                  evictionPolicy, isVolatile);\n            })\n       .def(\"create_masked_store\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptrs, mlir::Value &val,\n+           [](TritonOpBuilder &self, mlir::Value &ptrs, mlir::Value &val,\n               mlir::Value &mask, mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy) -> void {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::triton::StoreOp>(loc, ptrs, val, mask,\n-                                                cacheModifier, evictionPolicy);\n+             self.create<mlir::triton::StoreOp>(ptrs, val, mask, cacheModifier,\n+                                                evictionPolicy);\n            })\n       .def(\"create_view\",\n-           [](mlir::OpBuilder &self, mlir::Value &arg,\n+           [](TritonOpBuilder &self, mlir::Value &arg,\n               std::vector<int64_t> &shape) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              auto argType = arg.getType()\n                                 .dyn_cast<mlir::RankedTensorType>()\n                                 .getElementType();\n              return self.create<mlir::triton::ViewOp>(\n-                 loc, mlir::RankedTensorType::get(shape, argType), arg);\n+                 mlir::RankedTensorType::get(shape, argType), arg);\n            })\n       .def(\n           \"create_expand_dims\",\n-          [](mlir::OpBuilder &self, mlir::Value &arg, int axis) -> mlir::Value {\n-            auto loc = self.getUnknownLoc();\n+          [](TritonOpBuilder &self, mlir::Value &arg, int axis) -> mlir::Value {\n             auto argType = arg.getType().dyn_cast<mlir::RankedTensorType>();\n             auto argEltType = argType.getElementType();\n             std::vector<int64_t> retShape = argType.getShape();\n             retShape.insert(retShape.begin() + axis, 1);\n             return self.create<mlir::triton::ExpandDimsOp>(\n-                loc, mlir::RankedTensorType::get(retShape, argEltType), arg,\n-                axis);\n+                mlir::RankedTensorType::get(retShape, argEltType), arg, axis);\n           })\n       .def(\"create_cat\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              auto lhsType = lhs.getType().dyn_cast<mlir::RankedTensorType>();\n              auto rhsType = rhs.getType().dyn_cast<mlir::RankedTensorType>();\n              if (!(lhsType.getShape().size() == 1 &&\n@@ -1244,47 +1260,41 @@ void init_triton_ir(py::module &&m) {\n              std::vector<int64_t> shape{lhsType.getShape()[0] +\n                                         rhsType.getShape()[0]};\n              return self.create<mlir::triton::CatOp>(\n-                 loc,\n                  mlir::RankedTensorType::get(shape, lhsType.getElementType()),\n                  lhs, rhs);\n            })\n       .def(\"create_trans\",\n-           [](mlir::OpBuilder &self, mlir::Value &arg) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, mlir::Value &arg) -> mlir::Value {\n              auto argType = arg.getType().dyn_cast<mlir::RankedTensorType>();\n              auto argEltType = argType.getElementType();\n              std::vector<int64_t> retShape = argType.getShape();\n              std::reverse(retShape.begin(), retShape.end());\n              return self.create<mlir::triton::TransOp>(\n-                 loc, mlir::RankedTensorType::get(retShape, argEltType), arg);\n+                 mlir::RankedTensorType::get(retShape, argEltType), arg);\n            })\n       .def(\"create_broadcast\",\n-           [](mlir::OpBuilder &self, mlir::Value &arg,\n+           [](TritonOpBuilder &self, mlir::Value &arg,\n               std::vector<int64_t> &shape) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              if (auto argType =\n                      arg.getType().dyn_cast<mlir::RankedTensorType>())\n                return self.createOrFold<mlir::triton::BroadcastOp>(\n-                   loc,\n                    mlir::RankedTensorType::get(shape, argType.getElementType()),\n                    arg);\n              throw std::runtime_error(\n                  \"arg is not of RankedTensorType, use create_splat\");\n            })\n       .def(\"create_splat\",\n-           [](mlir::OpBuilder &self, mlir::Value &arg,\n+           [](TritonOpBuilder &self, mlir::Value &arg,\n               std::vector<int64_t> &shape) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              auto argType = arg.getType();\n              auto ret = self.createOrFold<mlir::triton::SplatOp>(\n-                 loc, mlir::RankedTensorType::get(shape, argType), arg);\n+                 mlir::RankedTensorType::get(shape, argType), arg);\n              return ret;\n            })\n       // // atomic\n       .def(\"create_atomic_cas\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptr, mlir::Value &cmp,\n+           [](TritonOpBuilder &self, mlir::Value &ptr, mlir::Value &cmp,\n               mlir::Value &val, mlir::triton::MemSemantic sem) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              mlir::Type dstType;\n              if (auto srcTensorType =\n                      ptr.getType().dyn_cast<mlir::RankedTensorType>()) {\n@@ -1298,14 +1308,13 @@ void init_triton_ir(py::module &&m) {\n                                   .cast<mlir::triton::PointerType>();\n                dstType = ptrType.getPointeeType();\n              }\n-             return self.create<mlir::triton::AtomicCASOp>(loc, dstType, ptr,\n-                                                           cmp, val, sem);\n+             return self.create<mlir::triton::AtomicCASOp>(dstType, ptr, cmp,\n+                                                           val, sem);\n            })\n       .def(\"create_atomic_rmw\",\n-           [](mlir::OpBuilder &self, mlir::triton::RMWOp rmwOp,\n+           [](TritonOpBuilder &self, mlir::triton::RMWOp rmwOp,\n               mlir::Value &ptr, mlir::Value &val, mlir::Value &mask,\n               mlir::triton::MemSemantic sem) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              mlir::Type dstType;\n              if (auto srcTensorType =\n                      ptr.getType().dyn_cast<mlir::RankedTensorType>()) {\n@@ -1319,174 +1328,163 @@ void init_triton_ir(py::module &&m) {\n                                   .cast<mlir::triton::PointerType>();\n                dstType = ptrType.getPointeeType();\n              }\n-             return self.create<mlir::triton::AtomicRMWOp>(loc, dstType, rmwOp,\n-                                                           ptr, val, mask, sem);\n+             return self.create<mlir::triton::AtomicRMWOp>(dstType, rmwOp, ptr,\n+                                                           val, mask, sem);\n            })\n       // External\n       .def(\"create_extern_elementwise\",\n-           [](mlir::OpBuilder &self, const std::string &libName,\n+           [](TritonOpBuilder &self, const std::string &libName,\n               const std::string &libPath, const std::string &symbol,\n               std::vector<mlir::Value> &argList, mlir::Type retType,\n               bool isPure) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              if (isPure)\n                return self.create<mlir::triton::PureExternElementwiseOp>(\n-                   loc, retType, argList, libName, libPath, symbol);\n+                   retType, argList, libName, libPath, symbol);\n              else\n                return self.create<mlir::triton::ImpureExternElementwiseOp>(\n-                   loc, retType, argList, libName, libPath, symbol);\n+                   retType, argList, libName, libPath, symbol);\n            })\n       // Built-in instruction\n       .def(\"create_get_program_id\",\n-           [](mlir::OpBuilder &self, int axis) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int axis) -> mlir::Value {\n              if (axis < 0 || axis > 3)\n                throw std::runtime_error(\"program_id must be in [0,3]\");\n              return self.create<mlir::triton::GetProgramIdOp>(\n-                 loc, self.getI32Type(),\n+                 self.getBuilder().getI32Type(),\n                  mlir::triton::ProgramIDDimAttr::get(\n-                     loc.getContext(), mlir::triton::ProgramIDDim(axis)));\n+                     self.getBuilder().getContext(),\n+                     mlir::triton::ProgramIDDim(axis)));\n            })\n       .def(\"create_get_num_programs\",\n-           [](mlir::OpBuilder &self, int axis) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int axis) -> mlir::Value {\n              return self.create<mlir::triton::GetNumProgramsOp>(\n-                 loc, self.getI32Type(), self.getI32IntegerAttr(axis));\n+                 self.getBuilder().getI32Type(),\n+                 self.getBuilder().getI32IntegerAttr(axis));\n            })\n       .def(\"create_dot\",\n-           [](mlir::OpBuilder &self, mlir::Value &a, mlir::Value &b,\n+           [](TritonOpBuilder &self, mlir::Value &a, mlir::Value &b,\n               mlir::Value &c, bool allowTF32) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::DotOp>(loc, c.getType(), a, b, c,\n+             return self.create<mlir::triton::DotOp>(c.getType(), a, b, c,\n                                                      allowTF32);\n            })\n       .def(\"create_exp\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::ExpOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::ExpOp>(val);\n            })\n       .def(\"create_cos\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::CosOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::CosOp>(val);\n            })\n       .def(\"create_sin\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::SinOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::SinOp>(val);\n            })\n       .def(\"create_log\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::LogOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::LogOp>(val);\n            })\n       .def(\"create_sqrt\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::SqrtOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::SqrtOp>(val);\n            })\n       .def(\"create_fabs\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::AbsFOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::AbsFOp>(val);\n            })\n       .def(\"create_iabs\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::AbsIOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::AbsIOp>(val);\n            })\n       .def(\"create_reduce\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Value> operands,\n+           [](TritonOpBuilder &self, std::vector<mlir::Value> operands,\n               int axis) -> mlir::OpState {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::ReduceOp>(loc, operands, axis);\n+             return self.create<mlir::triton::ReduceOp>(operands, axis);\n            })\n       .def(\"create_reduce_ret\",\n-           [](mlir::OpBuilder &self, py::args args) -> mlir::OpState {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, py::args args) -> mlir::OpState {\n              llvm::SmallVector<mlir::Value> return_values;\n              for (const auto &arg : args) {\n                return_values.push_back(py::cast<mlir::Value>(arg));\n              }\n-             return self.create<mlir::triton::ReduceReturnOp>(loc,\n-                                                              return_values);\n+             return self.create<mlir::triton::ReduceReturnOp>(return_values);\n+           })\n+      .def(\"create_scan\",\n+           [](TritonOpBuilder &self, std::vector<mlir::Value> operands,\n+              int axis) -> mlir::OpState {\n+             return self.create<mlir::triton::ScanOp>(operands, axis);\n+           })\n+      .def(\"create_scan_ret\",\n+           [](TritonOpBuilder &self, py::args args) -> mlir::OpState {\n+             llvm::SmallVector<mlir::Value> return_values;\n+             for (const auto &arg : args) {\n+               return_values.push_back(py::cast<mlir::Value>(arg));\n+             }\n+             return self.create<mlir::triton::ScanReturnOp>(return_values);\n            })\n       .def(\"create_ptr_to_int\",\n-           [](mlir::OpBuilder &self, mlir::Value &val,\n+           [](TritonOpBuilder &self, mlir::Value &val,\n               mlir::Type &type) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::PtrToIntOp>(loc, type, val);\n+             return self.create<mlir::triton::PtrToIntOp>(type, val);\n            })\n       .def(\"create_int_to_ptr\",\n-           [](mlir::OpBuilder &self, mlir::Value &val,\n+           [](TritonOpBuilder &self, mlir::Value &val,\n               mlir::Type &type) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::IntToPtrOp>(loc, type, val);\n+             return self.create<mlir::triton::IntToPtrOp>(type, val);\n            })\n       .def(\"create_select\",\n-           [](mlir::OpBuilder &self, mlir::Value &condition,\n+           [](TritonOpBuilder &self, mlir::Value &condition,\n               mlir::Value &trueValue, mlir::Value &falseValue) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::SelectOp>(loc, condition,\n-                                                       trueValue, falseValue);\n+             return self.create<mlir::arith::SelectOp>(condition, trueValue,\n+                                                       falseValue);\n            })\n       .def(\"create_print\",\n-           [](mlir::OpBuilder &self, const std::string &prefix,\n+           [](TritonOpBuilder &self, const std::string &prefix,\n               const std::vector<mlir::Value> &values) -> void {\n-             auto loc = self.getUnknownLoc();\n              self.create<mlir::triton::PrintOp>(\n-                 loc,\n-                 mlir::StringAttr::get(self.getContext(),\n+                 mlir::StringAttr::get(self.getBuilder().getContext(),\n                                        llvm::StringRef(prefix)),\n                  values);\n            })\n       .def(\"create_assert\",\n-           [](mlir::OpBuilder &self, mlir::Value &condition,\n+           [](TritonOpBuilder &self, mlir::Value &condition,\n               const std::string &message, const std::string &fileName,\n               const std::string &funcName, unsigned lineNo) -> void {\n-             auto loc = self.getUnknownLoc();\n-             auto messageAttr = mlir::StringAttr::get(self.getContext(),\n-                                                      llvm::StringRef(message));\n+             auto messageAttr = mlir::StringAttr::get(\n+                 self.getBuilder().getContext(), llvm::StringRef(message));\n              auto fileNameAttr = mlir::StringAttr::get(\n-                 self.getContext(), llvm::StringRef(fileName));\n+                 self.getBuilder().getContext(), llvm::StringRef(fileName));\n              auto funcNameAttr = mlir::StringAttr::get(\n-                 self.getContext(), llvm::StringRef(funcName));\n-             auto lineNoAttr = self.getI32IntegerAttr(lineNo);\n-             self.create<mlir::triton::AssertOp>(loc, condition, messageAttr,\n+                 self.getBuilder().getContext(), llvm::StringRef(funcName));\n+             auto lineNoAttr = self.getBuilder().getI32IntegerAttr(lineNo);\n+             self.create<mlir::triton::AssertOp>(condition, messageAttr,\n                                                  fileNameAttr, funcNameAttr,\n                                                  lineNoAttr);\n            })\n       // Undef\n       .def(\"create_undef\",\n-           [](mlir::OpBuilder &self, mlir::Type &type) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<::mlir::LLVM::UndefOp>(loc, type);\n+           [](TritonOpBuilder &self, mlir::Type &type) -> mlir::Value {\n+             return self.create<::mlir::LLVM::UndefOp>(type);\n            })\n       // Force GPU barrier\n       .def(\"create_barrier\",\n-           [](mlir::OpBuilder &self) {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::gpu::BarrierOp>(loc);\n-           })\n+           [](TritonOpBuilder &self) { self.create<mlir::gpu::BarrierOp>(); })\n       // Make a block pointer (tensor pointer in Triton IR)\n       .def(\"create_make_block_ptr\",\n-           [](mlir::OpBuilder &self, mlir::Value &base,\n+           [](TritonOpBuilder &self, mlir::Value &base,\n               std::vector<mlir::Value> &shape,\n               std::vector<mlir::Value> &strides,\n               std::vector<mlir::Value> &offsets,\n               std::vector<int32_t> &tensorShape,\n               std::vector<int32_t> &order) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::triton::MakeTensorPtrOp>(\n-                 loc, base, shape, strides, offsets, tensorShape, order);\n+                 base, shape, strides, offsets, tensorShape, order);\n            })\n       // Advance a block pointer\n       .def(\"create_advance\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptr,\n+           [](TritonOpBuilder &self, mlir::Value &ptr,\n               std::vector<mlir::Value> &offsets) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::AdvanceOp>(loc, ptr.getType(),\n-                                                         ptr, offsets);\n+             return self.create<mlir::triton::AdvanceOp>(ptr.getType(), ptr,\n+                                                         offsets);\n            });\n \n   py::class_<mlir::PassManager>(m, \"pass_manager\")\n@@ -1495,6 +1493,7 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self) {\n              auto printingFlags = mlir::OpPrintingFlags();\n              printingFlags.elideLargeElementsAttrs(16);\n+             printingFlags.enableDebugInfo();\n              self.enableIRPrinting(\n                  /*shouldPrintBeforePass=*/nullptr,\n                  /*shouldPrintAfterPass=*/\n@@ -1634,71 +1633,76 @@ void init_triton_translation(py::module &m) {\n               \"failed to parse IR: \" + error.getMessage() +\n               \"lineno: \" + std::to_string(error.getLineNo()));\n         }\n-\n         // translate module to PTX\n         auto ptxCode =\n             triton::translateLLVMIRToPTX(*module, capability, version);\n         return ptxCode;\n       },\n       ret::take_ownership);\n \n-  m.def(\"compile_ptx_to_cubin\",\n-        [](const std::string &ptxCode, const std::string &ptxasPath,\n-           int capability) -> py::object {\n-          std::string cubin;\n-          {\n-            py::gil_scoped_release allow_threads;\n-\n-            // compile ptx with ptxas\n-            llvm::SmallString<64> fsrc;\n-            llvm::SmallString<64> flog;\n-            llvm::sys::fs::createTemporaryFile(\"compile-ptx-src\", \"\", fsrc);\n-            llvm::sys::fs::createTemporaryFile(\"compile-ptx-log\", \"\", flog);\n-            std::string fbin = std::string(fsrc) + \".o\";\n-            llvm::FileRemover logRemover(flog);\n-            llvm::FileRemover binRemover(fbin);\n-            const char *_fsrc = fsrc.c_str();\n-            const char *_flog = flog.c_str();\n-            const char *_fbin = fbin.c_str();\n-            std::ofstream ofs(_fsrc);\n-            ofs << ptxCode << std::endl;\n-            ofs.close();\n-            std::string cmd;\n-            int err;\n-            cmd = ptxasPath + \" -v --gpu-name=sm_\" +\n-                  std::to_string(capability) + (capability == 90 ? \"a \" : \" \") +\n-                  _fsrc + \" -o \" + _fsrc + \".o 2> \" + _flog;\n-\n-            err = system(cmd.c_str());\n-            if (err != 0) {\n-              err >>= 8;\n-              std::ifstream _log(_flog);\n-              std::string log(std::istreambuf_iterator<char>(_log), {});\n-              if (err == 255) {\n-                throw std::runtime_error(\n-                    \"Internal Triton PTX codegen error: \\n\" + log);\n-              } else if (err == 128 + SIGSEGV) {\n-                throw std::runtime_error(\"Please run `ptxas \" +\n-                                         fsrc.str().str() +\n-                                         \"` to confirm that this is a \"\n-                                         \"bug in `ptxas`\\n\" +\n-                                         log);\n-              } else {\n-                throw std::runtime_error(\"`ptxas` failed with error code \" +\n-                                         std::to_string(err) + \": \\n\" + log);\n-              }\n-              return {};\n+  m.def(\n+      \"compile_ptx_to_cubin\",\n+      [](const std::string &ptxCode, const std::string &ptxasPath,\n+         int capability) -> py::object {\n+        std::string cubin;\n+        {\n+          py::gil_scoped_release allow_threads;\n+\n+          // compile ptx with ptxas\n+          llvm::SmallString<64> fsrc;\n+          llvm::SmallString<64> flog;\n+          llvm::sys::fs::createTemporaryFile(\"compile-ptx-src\", \"\", fsrc);\n+          llvm::sys::fs::createTemporaryFile(\"compile-ptx-log\", \"\", flog);\n+          std::string fbin = std::string(fsrc) + \".o\";\n+          llvm::FileRemover logRemover(flog);\n+          llvm::FileRemover binRemover(fbin);\n+          const char *_fsrc = fsrc.c_str();\n+          const char *_flog = flog.c_str();\n+          const char *_fbin = fbin.c_str();\n+          std::ofstream ofs(_fsrc);\n+          ofs << ptxCode << std::endl;\n+          ofs.close();\n+\n+          auto lineInfoOption =\n+              triton::tools::getBoolEnv(\"TRITON_DISABLE_LINE_INFO\")\n+                  ? \"\"\n+                  : \" -lineinfo\";\n+          auto capabilitySuffix = (capability == 90) ? \"a \" : \" \";\n+          auto outputFileName = std::string(_fsrc) + \".o\";\n+          auto logRedirect = \" 2> \" + std::string(_flog);\n+          std::string cmd = ptxasPath + lineInfoOption + \" -v --gpu-name=sm_\" +\n+                            std::to_string(capability) + capabilitySuffix +\n+                            _fsrc + \" -o \" + outputFileName + logRedirect;\n+\n+          int err = system(cmd.c_str());\n+          if (err != 0) {\n+            err >>= 8;\n+            std::ifstream _log(_flog);\n+            std::string log(std::istreambuf_iterator<char>(_log), {});\n+            if (err == 255) {\n+              throw std::runtime_error(\"Internal Triton PTX codegen error: \\n\" +\n+                                       log);\n+            } else if (err == 128 + SIGSEGV) {\n+              throw std::runtime_error(\"Please run `ptxas \" + fsrc.str().str() +\n+                                       \"` to confirm that this is a \"\n+                                       \"bug in `ptxas`\\n\" +\n+                                       log);\n             } else {\n-              llvm::FileRemover srcRemover(fsrc);\n-              std::ifstream _cubin(_fbin, std::ios::binary);\n-              cubin = std::string(std::istreambuf_iterator<char>(_cubin), {});\n-              _cubin.close();\n-              // Do not return here, exit the gil scope and return below\n+              throw std::runtime_error(\"`ptxas` failed with error code \" +\n+                                       std::to_string(err) + \": \\n\" + log);\n             }\n+            return {};\n+          } else {\n+            llvm::FileRemover srcRemover(fsrc);\n+            std::ifstream _cubin(_fbin, std::ios::binary);\n+            cubin = std::string(std::istreambuf_iterator<char>(_cubin), {});\n+            _cubin.close();\n+            // Do not return here, exit the gil scope and return below\n           }\n-          py::bytes bytes(cubin);\n-          return std::move(bytes);\n-        });\n+        }\n+        py::bytes bytes(cubin);\n+        return std::move(bytes);\n+      });\n \n   m.def(\"add_external_libs\",\n         [](mlir::ModuleOp &op, const std::vector<std::string> &names,"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 221, "deletions": 52, "changes": 273, "file_content_changes": "@@ -710,7 +710,7 @@ def test_abs(dtype_x, device):\n def test_abs_fp8(in_dtype, device):\n \n     @triton.jit\n-    def abs_kernel(Z, X, SIZE: tl.constexpr):\n+    def abs_kernel(X, Z, SIZE: tl.constexpr):\n         off = tl.arange(0, SIZE)\n         x = tl.load(X + off)\n         z = tl.abs(x)\n@@ -728,7 +728,7 @@ def abs_kernel(Z, X, SIZE: tl.constexpr):\n     f32_tensor = convert_float_to_float32(f8_tensor, in_dtype)\n     expect = f32_tensor.abs()\n     actual_f8 = convert_float_to_float32(out_f8, in_dtype)\n-    torch.testing.assert_allclose(expect, actual_f8)\n+    torch.testing.assert_allclose(actual_f8, expect)\n \n \n # ----------------\n@@ -1494,15 +1494,133 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n             np.testing.assert_equal(z_ref, z_tri)\n \n \n+scan2d_shapes = [(8, 32), (16, 32), (32, 16), (2, 1024), (1024, 2), (32, 32), (1, 1024)]\n+\n+scan_configs = [\n+    (op, type, shape, axis, num_warps)\n+    for num_warps in [4, 16]\n+    for type in ['int32', 'float32']\n+    for axis in [1, 0]\n+    for shape in scan2d_shapes\n+    for op in ['cumsum', 'cumprod']\n+]\n+\n+\n+@pytest.mark.parametrize(\"op, dtype_str, shape, axis, num_warps\", scan_configs)\n+def test_scan2d(op, dtype_str, shape, axis, num_warps, device):\n+    check_type_supported(dtype_str, device)\n+\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n+        range_m = tl.arange(0, BLOCK_M)\n+        range_n = tl.arange(0, BLOCK_N)\n+        x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n+        z = GENERATE_TEST_HERE\n+        tl.store(Z + range_m[:, None] * BLOCK_N + range_n[None, :], z)\n+\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis={axis})'})\n+    # input\n+    rs = RandomState(17)\n+    x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n+    z = np.empty_like(x)\n+    x_tri = to_triton(x, device=device)\n+    numpy_op = {'cumsum': np.cumsum, 'cumprod': np.cumprod}[op]\n+    z_dtype_str = dtype_str\n+    z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+    # triton result\n+    z_tri = to_triton(z, device=device)\n+    kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis, num_warps=num_warps)\n+    z_tri = to_numpy(z_tri)\n+    # compare\n+    if dtype_str == 'float32':\n+        if op == 'cumprod':\n+            np.testing.assert_allclose(z_ref, z_tri, rtol=0.01, atol=1e-3)\n+        else:\n+            np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n+    else:\n+        np.testing.assert_equal(z_ref, z_tri)\n+\n+\n+scan_layouts = [\n+    BlockedLayout([1, 4], [4, 8], [4, 1], [0, 1]),\n+    BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1]),\n+    BlockedLayout([4, 1], [4, 8], [1, 4], [0, 1]),\n+    BlockedLayout([2, 2], [4, 8], [2, 2], [0, 1]),\n+    BlockedLayout([2, 2], [8, 4], [2, 2], [0, 1]),\n+\n+    BlockedLayout([1, 4], [4, 8], [4, 1], [1, 0]),\n+    BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n+    BlockedLayout([4, 1], [4, 8], [1, 4], [1, 0]),\n+    BlockedLayout([2, 2], [4, 8], [2, 2], [1, 0]),\n+    BlockedLayout([2, 2], [8, 4], [2, 2], [1, 0]),\n+]\n+\n+\n+@pytest.mark.parametrize(\"M, N\", [[32, 32], [32, 64], [64, 32]])\n+@pytest.mark.parametrize(\"src_layout\", scan_layouts)\n+@pytest.mark.parametrize(\"axis\", [0, 1])\n+def test_scan_layouts(M, N, src_layout, axis, device):\n+    ir = f\"\"\"\n+    #blocked = {src_layout}\n+    module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n+    tt.func public @kernel_0d1d(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n+      %cst = arith.constant dense<{N}> : tensor<{M}x1xi32, #blocked>\n+      %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>\n+      %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>\n+      %2 = arith.muli %1, %cst : tensor<{M}x1xi32, #blocked>\n+      %3 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x1x!tt.ptr<i32>, #blocked>\n+      %4 = tt.addptr %3, %2 : tensor<{M}x1x!tt.ptr<i32>, #blocked>, tensor<{M}x1xi32, #blocked>\n+      %5 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>\n+      %6 = tt.expand_dims %5 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x{N}xi32, #blocked>\n+      %7 = tt.broadcast %4 : (tensor<{M}x1x!tt.ptr<i32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<i32>, #blocked>\n+      %8 = tt.broadcast %6 : (tensor<1x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #blocked>\n+      %9 = tt.addptr %7, %8 : tensor<{M}x{N}x!tt.ptr<i32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n+      %10 = tt.load %9 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #blocked>\n+      %11 = \"tt.scan\"(%10) <{{axis = {axis} : i32}}> ({{\n+      ^bb0(%arg2: i32, %arg3: i32):\n+        %16 = arith.addi %arg2, %arg3 : i32\n+        tt.scan.return %16 : i32\n+      }}) : (tensor<{M}x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #blocked>\n+      %12 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<{M}x1x!tt.ptr<i32>, #blocked>\n+      %13 = tt.addptr %12, %2 : tensor<{M}x1x!tt.ptr<i32>, #blocked>, tensor<{M}x1xi32, #blocked>\n+      %14 = tt.broadcast %13 : (tensor<{M}x1x!tt.ptr<i32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<i32>, #blocked>\n+      %15 = tt.addptr %14, %8 : tensor<{M}x{N}x!tt.ptr<i32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n+      tt.store %15, %11 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{M}x{N}xi32, #blocked>\n+      tt.return\n+    }}\n+    }}\n+    \"\"\"\n+\n+    import tempfile\n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n+        f.write(ir)\n+        f.flush()\n+        kernel = triton.compile(f.name)\n+    rs = RandomState(17)\n+    x = rs.randint(-100, 100, (M, N)).astype('int32')\n+\n+    z = np.zeros((M, N)).astype('int32')\n+    x_tri = torch.tensor(x, device=device)\n+    z_tri = torch.tensor(z, device=device)\n+\n+    kernel[(1, 1, 1)](x_tri, z_tri)\n+\n+    z_ref = np.cumsum(x, axis=axis)\n+\n+    np.testing.assert_equal(z_ref, z_tri.cpu().numpy())\n+\n+\n layouts = [\n     BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n     BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1]),\n+    BlockedLayout([4, 4], [2, 16], [4, 1], [1, 0]),\n     MmaLayout(version=(2, 0), warps_per_cta=[4, 1]),\n     MmaLayout(version=(2, 0), warps_per_cta=[2, 2])\n ]\n \n \n-@pytest.mark.parametrize(\"M, N\", [[128, 16], [128, 128], [32, 128]])\n+@pytest.mark.parametrize(\"M, N\", [[128, 16], [128, 128], [32, 128], [32, 32]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"axis\", [0, 1])\n def test_reduce_layouts(M, N, src_layout, axis, device):\n@@ -1513,31 +1631,30 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n     #blocked = #triton_gpu.blocked<{{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}}>\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n-    tt.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}, %arg1: i32 {{tt.divisibility = 16 : i32}}, %arg2: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}) {{\n+    tt.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: i32 {{tt.divisibility = 16 : i32}}, %arg2: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n         %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>\n         %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>\n         %2 = tt.splat %arg1 : (i32) -> tensor<{M}x1xi32, #blocked>\n         %3 = arith.muli %1, %2 : tensor<{M}x1xi32, #blocked>\n-        %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<{M}x1x!tt.ptr<f32>, #blocked>\n-        %5 = tt.addptr %4, %3 : tensor<{M}x1x!tt.ptr<f32>, #blocked>, tensor<{M}x1xi32, #blocked>\n+        %4 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x1x!tt.ptr<i32>, #blocked>\n+        %5 = tt.addptr %4, %3 : tensor<{M}x1x!tt.ptr<i32>, #blocked>, tensor<{M}x1xi32, #blocked>\n         %6 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>\n         %7 = tt.expand_dims %6 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x{N}xi32, #blocked>\n-        %8 = tt.broadcast %5 : (tensor<{M}x1x!tt.ptr<f32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<f32>, #blocked>\n+        %8 = tt.broadcast %5 : (tensor<{M}x1x!tt.ptr<i32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<i32>, #blocked>\n         %9 = tt.broadcast %7 : (tensor<1x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #blocked>\n-        %10 = tt.addptr %8, %9 : tensor<{M}x{N}x!tt.ptr<f32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n-        %11 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<{rdims_2d}x!tt.ptr<f32>, #blocked>\n-        %12 = tt.addptr %11, {store_range} : tensor<{rdims_2d}x!tt.ptr<f32>, #blocked>, tensor<{rdims_2d}xi32, #blocked>\n-        %13 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xf32, #blocked>\n-        %14 = triton_gpu.convert_layout %13 : (tensor<{M}x{N}xf32, #blocked>) -> tensor<{M}x{N}xf32, #src>\n+        %10 = tt.addptr %8, %9 : tensor<{M}x{N}x!tt.ptr<i32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n+        %11 = tt.splat %arg2 : (!tt.ptr<i32>) -> tensor<{rdims_2d}x!tt.ptr<i32>, #blocked>\n+        %12 = tt.addptr %11, {store_range} : tensor<{rdims_2d}x!tt.ptr<i32>, #blocked>, tensor<{rdims_2d}xi32, #blocked>\n+        %13 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #blocked>\n+        %14 = triton_gpu.convert_layout %13 : (tensor<{M}x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #src>\n         %15 = \"tt.reduce\"(%14) ({{\n-        ^bb0(%arg3: f32, %arg4: f32):\n-          %16 = \"triton_gpu.cmpf\"(%arg3, %arg4) {{predicate = 2 : i64}} : (f32, f32) -> i1\n-          %17 = arith.select %16, %arg3, %arg4 : f32\n-          tt.reduce.return %17 : f32\n-        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xf32, #src>) -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n-        %18 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n-        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xf32, #blocked>\n-        tt.store %12, %19 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xf32, #blocked>\n+        ^bb0(%arg3: i32, %arg4: i32):\n+          %17 = arith.addi %arg3, %arg4 : i32\n+          tt.reduce.return %17 : i32\n+        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n+        %18 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n+        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xi32, #blocked>\n+        tt.store %12, %19 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xi32, #blocked>\n         tt.return\n     }}\n     }}\n@@ -1550,22 +1667,21 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n         kernel = triton.compile(f.name)\n \n     rs = RandomState(17)\n-    x = rs.randint(0, 4, (M, N)).astype('float32')\n-    x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+    x = rs.randint(0, 20, (M, N)).astype('int32')\n \n     if axis == 0:\n-        z = np.zeros((1, N)).astype('float32')\n+        z = np.zeros((1, N)).astype('int32')\n     else:\n-        z = np.zeros((M, 1)).astype('float32')\n+        z = np.zeros((M, 1)).astype('int32')\n \n     x_tri = torch.tensor(x, device=device)\n     z_tri = torch.tensor(z, device=device)\n \n     pgm = kernel[(1, 1, 4)](x_tri, x_tri.stride(0), z_tri)\n \n-    z_ref = np.max(x, axis=axis, keepdims=True)\n+    z_ref = np.sum(x, axis=axis, keepdims=True)\n \n-    np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+    np.testing.assert_equal(z_ref, z_tri.cpu().numpy())\n \n \n layouts = [\n@@ -1684,7 +1800,8 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n @pytest.mark.parametrize(\"M, N\", [[128, 128], [256, 128], [256, 256], [128, 256]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"op\", [\"sum\", \"max\"])\n-def test_chain_reduce(M, N, src_layout, op, device):\n+@pytest.mark.parametrize(\"first_axis\", [0, 1])\n+def test_chain_reduce(M, N, src_layout, op, device, first_axis):\n     op_str = \"\"\n     if op == \"sum\":\n         op_str = f\"\"\"\n@@ -1714,11 +1831,11 @@ def test_chain_reduce(M, N, src_layout, op, device):\n         %11 = \"tt.reduce\"(%10) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n         {op_str}\n-        }}) {{axis = 1 : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+        }}) {{axis = {first_axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M if first_axis == 1 else N}xi32, #triton_gpu.slice<{{dim = {first_axis}, parent = #src}}>>\n         %12 = \"tt.reduce\"(%11) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n         {op_str}\n-        }}) {{axis = 0 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> i32\n+        }}) {{axis = 0 : i32}} : (tensor<{M if first_axis == 1 else N}xi32, #triton_gpu.slice<{{dim = {first_axis}, parent = #src}}>>) -> i32\n         tt.store %arg1, %12 {{cache = 1 : i32, evict = 1 : i32}} : i32\n         tt.return\n     }}\n@@ -2014,6 +2131,46 @@ def kernel(X, stride_xm, stride_xk,\n         assert 'mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16' in ptx\n \n \n+@pytest.mark.parametrize('in_dtype', ['float32'])\n+def test_dot_mulbroadcastred(in_dtype, device):\n+    @triton.jit\n+    def kernel(Z, X, Y,\n+               M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n+               BM: tl.constexpr, BN: tl.constexpr, BK: tl.constexpr):\n+        pidn = tl.program_id(1)\n+        pidm = tl.program_id(0)\n+        offm = tl.arange(0, BM)[:, None]\n+        offn = tl.arange(0, BN)[None, :]\n+        offak = tl.arange(0, BK)[None, :]\n+        offbk = tl.arange(0, BK)[:, None]\n+        acc = tl.full((BM, BN), 0.0, tl.float32)\n+        for ridx5 in range(0, K // BK):\n+            x = tl.load(X + ((pidm * K * BM) + (offm * K) + (ridx5 * BK) + offak))\n+            y = tl.load(Y + ((pidn * BN) + (offbk * N) + (ridx5 * N * BK) + offn))\n+            x = tl.expand_dims(x, axis=2)\n+            y = tl.expand_dims(y, axis=0)\n+            t = tl.sum(x * y, axis=1)\n+            acc = t + acc\n+        tl.store(Z + ((pidm * BM * N) + (pidn * BN) + (offm * N) + offn), acc)\n+    M, N, K = 256, 192, 160\n+    BM, BN, BK = 128, 32, 32\n+    rs = RandomState(17)\n+    x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)\n+    y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)\n+    x = x * 0.1\n+    y = y * 0.1\n+    z = numpy_random((M, N), dtype_str=in_dtype, rs=rs)\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(y, device=device)\n+    z_tri = to_triton(z, device=device)\n+    grid = M // BM, N // BN\n+    h = kernel[grid](z_tri, x_tri, y_tri, M, N, K, BM, BN, BK)\n+    z_ref = np.matmul(x, y)\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), atol=0.01)\n+    assert \"tt.dot\" in h.asm['ttir']\n+    assert \"triton_gpu.async_wait {num = 2 : i32}\" in h.asm['ttgir']\n+\n+\n @pytest.mark.parametrize(\"dtype_str\", int_dtypes + float_dtypes + ['bfloat16'])\n def test_full(dtype_str, device):\n     dtype = getattr(torch, dtype_str)\n@@ -2057,24 +2214,23 @@ def kernel(out_ptr):\n     h = kernel_patched[(1,)](out)\n     assert re.search(r\"arith.constant .* : \" + dtype_str, h.asm[\"ttir\"]) is not None\n \n-# TODO: uncomment once DotOperandEncoding::getElemsPerThread is implemented\n-# @pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n-# def test_dot_without_load(dtype_str, device=device):\n-#     @triton.jit\n-#     def _kernel(out):\n-#         a = GENERATE_TEST_HERE\n-#         b = GENERATE_TEST_HERE\n-#         c = tl.dot(a, b)\n-#         out_ptr = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n-#         tl.store(out_ptr, c)\n-\n-#     kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.full((32, 32), 1.0, tl.{dtype_str})\"})\n-#     a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n-#     b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n-#     out_ref = torch.matmul(a, b)\n-#     out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=device)\n-#     kernel[(1,)](out)\n-#     assert torch.all(out == out_ref)\n+\n+@pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n+def test_dot_without_load(dtype_str, device):\n+    @triton.jit\n+    def _kernel(out):\n+        a = GENERATE_TEST_HERE\n+        b = GENERATE_TEST_HERE\n+        c = tl.dot(a, b)\n+        out_ptr = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n+        tl.store(out_ptr, c)\n+    kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.full((32, 32), 1.0, tl.{dtype_str})\"})\n+    a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n+    b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n+    out_ref = torch.matmul(a, b)\n+    out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=device)\n+    kernel[(1,)](out)\n+    assert torch.all(out == out_ref)\n \n # ---------------\n # test arange\n@@ -2254,7 +2410,7 @@ def _kernel(dst, src, off, N, BLOCK_SIZE: tl.constexpr, HINT: tl.constexpr):\n # ---------------\n \n \n-@pytest.mark.parametrize(\"cache\", [\"\", \".wb\", \".cg\", \".cs\"])\n+@pytest.mark.parametrize(\"cache\", [\"\", \".wb\", \".cg\", \".cs\", \".wt\"])\n def test_store_cache_modifier(cache):\n     src = torch.empty(128, device='cuda')\n     dst = torch.empty(128, device='cuda')\n@@ -2271,18 +2427,27 @@ def _kernel(dst, src, CACHE: tl.constexpr):\n         assert 'st.global.wb' not in ptx\n         assert 'st.global.cg' not in ptx\n         assert 'st.global.cs' not in ptx\n+        assert 'st.global.wt' not in ptx\n     if cache == '.wb':\n         assert 'st.global.wb' in ptx\n         assert 'st.global.cg' not in ptx\n         assert 'st.global.cs' not in ptx\n+        assert 'st.global.wt' not in ptx\n     if cache == '.cg':\n         assert 'st.global.wb' not in ptx\n         assert 'st.global.cg' in ptx\n         assert 'st.global.cs' not in ptx\n+        assert 'st.global.wt' not in ptx\n     if cache == '.cs':\n         assert 'st.global.wb' not in ptx\n         assert 'st.global.cg' not in ptx\n         assert 'st.global.cs' in ptx\n+        assert 'st.global.wt' not in ptx\n+    if cache == '.wt':\n+        assert 'st.global.wb' not in ptx\n+        assert 'st.global.cg' not in ptx\n+        assert 'st.global.cs' not in ptx\n+        assert 'st.global.wt' in ptx\n \n # ---------------\n # test if\n@@ -2875,24 +3040,28 @@ def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n def test_while(device):\n \n     @triton.jit\n-    def kernel(InitI, Bound, CutOff, OutI, OutJ):\n+    def kernel(InitI, Bound, CutOff, OutI, OutInitI, OutJ):\n         init_i = tl.load(InitI)\n         curr_i = init_i\n         j = 0\n-        while curr_i == init_i and j < tl.load(Bound):\n+        # Check that init_i is not updated by the loop\n+        while j < tl.load(Bound):\n             curr_i = curr_i + (j == tl.load(CutOff))\n             j += 1\n+            tl.store(OutInitI, init_i)\n         tl.store(OutI, curr_i)\n         tl.store(OutJ, j)\n \n     out_i = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     out_j = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     init_i = to_triton(np.full((1,), 1, dtype=np.int32), device=device)\n+    out_init_i = to_triton(np.full((1,), 0, dtype=np.int32), device=device)\n     bound = to_triton(np.full((1,), 10, dtype=np.int32), device=device)\n     cut_off = to_triton(np.full((1,), 5, dtype=np.int32), device=device)\n-    kernel[(1,)](init_i, bound, cut_off, out_i, out_j)\n+    kernel[(1,)](init_i, bound, cut_off, out_i, out_init_i, out_j)\n+    assert out_init_i[0] == init_i[0]\n     assert out_i[0] == init_i[0] + 1\n-    assert out_j[0] == cut_off[0] + 1\n+    assert out_j[0] == bound[0]\n \n # def test_for_if(device):\n "}, {"filename": "python/test/unit/language/test_line_info.py", "status": "added", "additions": 120, "deletions": 0, "changes": 120, "file_content_changes": "@@ -0,0 +1,120 @@\n+import subprocess\n+import tempfile\n+\n+import pytest\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def kernel_single(X,\n+                  Y,\n+                  BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+@triton.jit\n+def device_inline(x):\n+    return x + x\n+\n+\n+@triton.jit\n+def kernel_call(X,\n+                Y,\n+                BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    y = device_inline(x)\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+\n+@triton.jit(noinline=True)\n+def device_noinline(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    y = x + x\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+\n+@triton.jit\n+def kernel_call_noinline(X, Y, BLOCK: tl.constexpr):\n+    device_noinline(X, Y, BLOCK)\n+\n+\n+@triton.jit\n+def kernel_multi_files(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    y = tl.softmax(x)\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+\n+def extract_file_lines(asm):\n+    fd, path = tempfile.mkstemp()\n+    with open(fd, 'wb') as cubin:\n+        cubin.write(asm)\n+    asm = subprocess.check_output([\"nvdisasm\", \"-g\", path]).decode(\"utf-8\")\n+    file_lines = []\n+    lines = asm.splitlines()\n+    for line in lines:\n+        if \"## File\" in line:\n+            entries = line[line.index(\"## File\"):].split(\",\")\n+            file_lines.append((entries[0].strip(), entries[1].strip()))\n+    return file_lines\n+\n+\n+def check_file_lines(file_lines, file_name, lineno):\n+    for file, line in file_lines:\n+        # -1 means do not check line number\n+        if lineno == -1:\n+            if file_name in file:\n+                return True\n+        if file_name in file and str(lineno) in line:\n+            return True\n+    return False\n+\n+\n+func_types = [\"single\", \"call\", \"call_noinline\", \"multi_files\"]\n+\n+\n+@pytest.mark.parametrize(\"func\", func_types)\n+def test_line_info(func: str):\n+    try:\n+        subprocess.check_output([\"nvdisasm\", \"-h\"])\n+    except BaseException:\n+        pytest.skip(\"nvdisasm is not available\")\n+\n+    shape = (128, )\n+    x = torch.arange(0, shape[0], dtype=torch.float32, device='cuda')\n+    y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n+    kernel_info = {}\n+    if func == \"single\":\n+        kernel_info = kernel_single[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"call\":\n+        kernel_info = kernel_call[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"call_noinline\":\n+        kernel_info = kernel_call_noinline[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"multi_files\":\n+        kernel_info = kernel_multi_files[(1,)](x, y, BLOCK=shape[0])\n+\n+    file_lines = extract_file_lines(kernel_info.asm[\"cubin\"])\n+    if func == \"single\":\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 15))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 16))\n+    elif func == \"call\":\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 28))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 21))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 30))\n+    elif func == \"call_noinline\":\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 42))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 35))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 36))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 37))\n+    elif func == \"multi_files\":\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 47))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 49))\n+        assert (check_file_lines(file_lines, \"standard.py\", 33))\n+        assert (check_file_lines(file_lines, \"standard.py\", 34))\n+        assert (check_file_lines(file_lines, \"standard.py\", 36))\n+        # core.py is changed frequently, so we only check if it exists\n+        assert (check_file_lines(file_lines, \"core.py\", -1))"}, {"filename": "python/test/unit/language/test_subprocess.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -9,7 +9,7 @@\n assert_path = os.path.join(dir_path, \"assert_helper.py\")\n \n # TODO: bfloat16 after LLVM-15\n-func_types = [\"device_assert\", \"assert\", \"static_assert\", \"no_debug\"]\n+assert_types = [\"device_assert\", \"assert\", \"static_assert\", \"no_debug\"]\n nested_types = [(caller, callee) for caller in [\"true\", \"false\", \"none\"] for callee in [\"true\", \"false\", \"none\"]]\n torch_types = [\"int8\", \"uint8\", \"int16\", \"int32\", \"long\", \"float16\", \"float32\", \"float64\"]\n \n@@ -37,7 +37,7 @@ def test_print(func_type: str, data_type: str):\n         assert len(new_lines) == 1\n \n \n-@pytest.mark.parametrize(\"func_type\", func_types)\n+@pytest.mark.parametrize(\"func_type\", assert_types)\n def test_assert(func_type: str):\n     os.environ[\"TRITON_DEBUG\"] = \"1\"\n     proc = subprocess.Popen([sys.executable, assert_path, func_type], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)"}, {"filename": "python/test/unit/operators/test_cross_entropy.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -33,7 +33,8 @@ def test_op(M, N, dtype, mode):\n         tt_y.backward(dy)\n         tt_dx = x.grad.clone()\n         # torch backward\n-        x.grad.zero_()\n+        x.grad = None\n         th_y.backward(dy)\n         th_dx = x.grad.clone()\n+\n         torch.testing.assert_allclose(th_dx, tt_dx)"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 15, "deletions": 14, "changes": 29, "file_content_changes": "@@ -63,25 +63,24 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (64, 64, 16, 4, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 (64, 64, 16, 8, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 # variable input\n-                (128, 128, 32, 1, 4, 2, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 384, 128, 640, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 256, 384, 160, AT, BT, DTYPE, DTYPE),\n                 (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE, DTYPE),\n                 (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE, DTYPE),\n-                (128, 256, 64, 1, 8, 3, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n+                (128, 256, 64, 1, 8, 3, 256, 512, 160, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n         # n-stage\n         *[\n             [\n-                (16, 16, 16, 1, 1, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n-                (64, 32, 64, 1, 2, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n-                (128, 64, 16, 1, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n-                (256, 128, 32, 1, 8, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, STAGES, 384, 128, 640, AT, BT, DTYPE, DTYPE),\n+                (16, 16, 16, 1, 1, STAGES, 32, 32, 80, AT, BT, DTYPE, DTYPE),\n+                (64, 32, 64, 1, 2, STAGES, 128, 64, 128, AT, BT, DTYPE, DTYPE),\n+                (128, 64, 16, 1, 4, STAGES, 256, 128, 80, AT, BT, DTYPE, DTYPE),\n+                (256, 128, 32, 1, 8, STAGES, 512, 256, 160, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, STAGES, 256, 256, 160, AT, BT, DTYPE, DTYPE),\n                 # split-k\n-                (64, 64, 16, 8, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n-                (64, 64, 16, 8, 4, STAGES, 1024, 1024, 32, AT, BT, DTYPE, DTYPE),\n-            ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [2, 3, 4]\n+                (64, 64, 16, 8, 4, STAGES, 128, 128, 768, AT, BT, DTYPE, DTYPE),\n+                (64, 64, 16, 8, 4, STAGES, 128, 128, 32, AT, BT, DTYPE, DTYPE),\n+            ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [4]\n         ],\n         # mixed-precision\n         *[\n@@ -90,8 +89,9 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (32, 64, 16, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n-                (128, 128, 32, 8, 4, 2, 1024, 1024, 1024, AT, BT, ADTYPE, BDTYPE),\n-            ] for ADTYPE, BDTYPE in [(\"float8\", \"float16\")] for AT in [False, True] for BT in [False, True]\n+                (128, 128, 32, 8, 4, 2, 256, 256, 128, AT, BT, ADTYPE, BDTYPE),\n+            ] for ADTYPE, BDTYPE in [(\"float8\", \"float16\"), (\"float16\", \"float32\"), (\"float32\", \"float16\"),\n+                                     (\"bfloat16\", \"float32\"), (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n         ]\n     ),\n )\n@@ -125,11 +125,12 @@ def get_input(n, m, t, dtype):\n             return f8_to_f16(x)\n         dtype = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[dtype]\n         return .1 * torch.randn((n, m), device=\"cuda\", dtype=dtype)\n+\n     # allocate/transpose inputs\n     a = get_input(M, K, AT, ADTYPE)\n     b = get_input(K, N, BT, BDTYPE)\n     # run test\n-    th_c = torch.matmul(a, b)\n+    th_c = torch.matmul(a.to(torch.float32), b.to(torch.float32))\n     try:\n         tt_c = triton.ops.matmul(a, b)\n         atol, rtol = 1e-2, 0"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 58, "deletions": 17, "changes": 75, "file_content_changes": "@@ -75,6 +75,20 @@ def _check_fn_args(node, fn, args):\n                 raise UnsupportedLanguageConstruct(fn.src, node, f'Function {fn.__name__} is marked noinline, but was called with non-scalar argument {fn.arg_names[idx]}:{arg}')\n \n \n+def _get_fn_file_line(fn):\n+    base_fn = fn\n+    while not isinstance(base_fn, JITFunction):\n+        base_fn = base_fn.fn\n+    file_name = base_fn.fn.__code__.co_filename\n+    lines, begin_line = inspect.getsourcelines(base_fn.fn)\n+    for line in lines:\n+        if line.strip().startswith('@'):\n+            begin_line += 1\n+        else:\n+            break\n+    return file_name, begin_line\n+\n+\n _condition_types = {bool, int, type(None)}  # Python types accepted for conditionals inside kernels\n \n \n@@ -191,8 +205,13 @@ def visit_Call(self, node: ast.Call) -> bool:\n class CodeGenerator(ast.NodeVisitor):\n     def __init__(self, context, prototype, gscope, attributes, constants, function_name, arch,\n                  module=None, is_kernel=False, function_types: Optional[Dict] = None,\n-                 debug=False, noinline=False):\n+                 debug=False, noinline=False, file_name: Optional[str] = None, begin_line=0):\n+        self.context = context\n         self.builder = ir.builder(context)\n+        self.file_name = file_name\n+        # node.lineno starts from 1, so we need to subtract 1\n+        self.begin_line = begin_line - 1\n+        self.builder.set_loc(file_name, begin_line, 0)\n         self.builder.arch = arch\n         self.module = self.builder.create_module() if module is None else module\n         self.function_ret_types = {} if function_types is None else function_types\n@@ -249,6 +268,18 @@ def set_value(self, name: str,\n         self.lscope[name] = value\n         self.local_defs[name] = value\n \n+    def _get_insertion_point_and_loc(self):\n+        # XXX: this is a hack to get the location of the insertion point.\n+        # The insertion point's location could be invalid sometimes,\n+        # so we need to explicitly set the location\n+        loc = self.builder.get_loc()\n+        ip = self.builder.get_insertion_point()\n+        return ip, loc\n+\n+    def _set_insertion_point_and_loc(self, ip, loc):\n+        self.builder.restore_insertion_point(ip)\n+        self.builder.set_loc(loc)\n+\n     #\n     # AST visitor\n     #\n@@ -534,13 +565,13 @@ def visit_if_top_level(self, cond, node):\n     def visit_if_scf(self, cond, node):\n         with enter_sub_region(self) as sr:\n             liveins, _ = sr\n-            ip = self.builder.get_insertion_point()\n+            ip, last_loc = self._get_insertion_point_and_loc()\n             then_block = self.builder.create_block()\n             else_block = self.builder.create_block() if node.orelse else None\n             then_defs, else_defs, then_block, else_block, names, ret_types, _ = \\\n                 self.visit_then_else_blocks(node, liveins, then_block, else_block)\n             # create if op\n-            self.builder.restore_insertion_point(ip)\n+            self._set_insertion_point_and_loc(ip, last_loc)\n             if_op = self.builder.create_if_op([ty.to_ir(self.builder) for ty in ret_types], cond.handle, True)\n             then_block.merge_block_before(if_op.get_then_block())\n             self.builder.set_insertion_point_to_end(if_op.get_then_block())\n@@ -564,8 +595,11 @@ def visit_If(self, node):\n             cond = cond.to(language.int1, _builder=self.builder)\n             contains_return = ContainsReturnChecker(self.gscope).visit(node)\n             if self.scf_stack and contains_return:\n-                raise UnsupportedLanguageConstruct(None, node,\n-                                                   \"Cannot have `return` statements inside `while` or `for` statements in triton\")\n+                raise UnsupportedLanguageConstruct(\n+                    None, node,\n+                    \"Cannot have `return` statements inside `while` or `for` statements in triton \"\n+                    \"(note that this also applies to `return` statements that are inside functions \"\n+                    \"transitively called from within `while`/`for` statements)\")\n             elif self.scf_stack or not contains_return:\n                 self.visit_if_scf(cond, node)\n             else:\n@@ -683,10 +717,6 @@ def visit_While(self, node):\n                     yields.append(loop_defs[name])\n             self.builder.create_yield_op([y.handle for y in yields])\n \n-        # update global uses in while_op\n-        for i, name in enumerate(names):\n-            after_block.replace_use_in_block_with(init_args[i].handle, after_block.arg(i))\n-\n         # WhileOp defines new values, update the symbol table (lscope, local_defs)\n         for i, name in enumerate(names):\n             new_def = language.core.tensor(while_op.get_result(i), ret_types[i])\n@@ -762,7 +792,7 @@ def visit_For(self, node):\n \n         with enter_sub_region(self) as sr:\n             liveins, insert_block = sr\n-            ip = self.builder.get_insertion_point()\n+            ip, last_loc = self._get_insertion_point_and_loc()\n \n             # create loop body block\n             block = self.builder.create_block()\n@@ -792,7 +822,7 @@ def visit_For(self, node):\n                     yields.append(language.core._to_tensor(self.local_defs[name], self.builder))\n \n             # create ForOp\n-            self.builder.restore_insertion_point(ip)\n+            self._set_insertion_point_and_loc(ip, last_loc)\n             for_op = self.builder.create_for_op(lb, ub, step, [arg.handle for arg in init_args])\n \n             self.scf_stack.append(node)\n@@ -869,9 +899,10 @@ def call_JitFunction(self, fn: JITFunction, args, kwargs):\n             gscope = sys.modules[fn.fn.__module__].__dict__\n             # If the callee is not set, we use the same debug setting as the caller\n             debug = self.debug if fn.debug is None else fn.debug\n-            generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module,\n+            file_name, begin_line = _get_fn_file_line(fn)\n+            generator = CodeGenerator(self.context, prototype, gscope, attributes, constants, module=self.module,\n                                       function_name=fn_name, function_types=self.function_ret_types, debug=debug, noinline=fn.noinline,\n-                                      arch=self.builder.arch)\n+                                      file_name=file_name, begin_line=begin_line, arch=self.builder.arch)\n             generator.visit(fn.parse())\n             callee_ret_type = generator.last_ret_type\n             self.function_ret_types[fn_name] = callee_ret_type\n@@ -969,14 +1000,23 @@ def visit_JoinedStr(self, node):\n         return ''.join(values)\n \n     def visit(self, node):\n-        if node is not None:\n-            self.last_node = node\n+        if node is None:\n+            return\n         with warnings.catch_warnings():\n             # The ast library added visit_Constant and deprecated some other\n             # methods but we can't move to that without breaking Python 3.6 and 3.7.\n             warnings.simplefilter(\"ignore\", DeprecationWarning)  # python 3.9\n             warnings.simplefilter(\"ignore\", PendingDeprecationWarning)  # python 3.8\n-            return super().visit(node)\n+            self.last_node = node\n+            last_loc = self.builder.get_loc()\n+            if hasattr(node, 'lineno') and hasattr(node, 'col_offset'):\n+                self.builder.set_loc(self.file_name, self.begin_line + node.lineno, node.col_offset)\n+                last_loc = self.builder.get_loc()\n+            ret = super().visit(node)\n+            # Reset the location to the last one before the visit\n+            if last_loc:\n+                self.builder.set_loc(last_loc)\n+            return ret\n \n     def generic_visit(self, node):\n         raise UnsupportedLanguageConstruct(None, node, \"unsupported AST node type: {}\".format(type(node).__name__))\n@@ -1071,11 +1111,12 @@ def ast_to_ttir(fn, signature, specialization, constants, debug, arch):\n     all_constants = constants.copy()\n     all_constants.update(new_constants)\n     arg_types = [str_to_ty(v) for k, v in signature.items() if k not in constants]\n+    file_name, begin_line = _get_fn_file_line(fn)\n \n     prototype = language.function_type([], arg_types)\n     generator = CodeGenerator(context, prototype, gscope=gscope, constants=all_constants,\n                               function_name=function_name, attributes=new_attrs,\n-                              is_kernel=True, debug=debug,\n+                              is_kernel=True, debug=debug, file_name=file_name, begin_line=begin_line,\n                               arch=arch)\n     try:\n         generator.visit(fn.parse())"}, {"filename": "python/triton/interpreter/tl_lang.py", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -623,3 +623,15 @@ def sum(self, input, axis=None):\n     @_tensor_operation\n     def xor_sum(self, input, axis):\n         raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def cumsum(self, input, axis=None):\n+        if axis is None:\n+            return torch.cumsum(input)\n+        return torch.cumsum(input, dim=axis)\n+\n+    @_tensor_operation\n+    def cumprod(self, input, axis=None):\n+        if axis is None:\n+            return torch.cumprod(input)\n+        return torch.cumprod(input, dim=axis)"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -13,11 +13,13 @@\n     zeros_like,\n )\n from .core import (\n+    TRITON_MAX_TENSOR_NUMEL,\n     abs,\n     advance,\n     arange,\n     argmin,\n     argmax,\n+    associative_scan,\n     atomic_add,\n     atomic_and,\n     atomic_cas,\n@@ -33,6 +35,8 @@\n     cat,\n     constexpr,\n     cos,\n+    cumprod,\n+    cumsum,\n     debug_barrier,\n     device_assert,\n     device_print,\n@@ -104,11 +108,13 @@\n \n \n __all__ = [\n+    \"TRITON_MAX_TENSOR_NUMEL\",\n     \"abs\",\n     \"advance\",\n     \"arange\",\n     \"argmin\",\n     \"argmax\",\n+    \"associative_scan\",\n     \"atomic_add\",\n     \"atomic_and\",\n     \"atomic_cas\",\n@@ -126,6 +132,8 @@\n     \"cdiv\",\n     \"constexpr\",\n     \"cos\",\n+    \"cumprod\",\n+    \"cumsum\",\n     \"debug_barrier\",\n     \"device_assert\",\n     \"device_print\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 95, "deletions": 11, "changes": 106, "file_content_changes": "@@ -1521,6 +1521,81 @@ def xor_sum(input, axis=None, _builder=None, _generator=None):\n                   _builder=_builder, _generator=_generator)\n \n \n+# -----------------------\n+# Scans\n+# -----------------------\n+\n+def _add_scan_docstr(name: str, return_indices_arg: str = None, tie_break_arg: str = None) -> Callable[[T], T]:\n+\n+    def _decorator(func: T) -> T:\n+        docstr = \"\"\"\n+    Returns the {name} of all elements in the :code:`input` tensor along the provided :code:`axis`\n+\n+    :param input: the input values\n+    :param axis: the dimension along which the scan should be done\"\"\"\n+        func.__doc__ = docstr.format(name=name)\n+        return func\n+\n+    return _decorator\n+\n+\n+@builtin\n+def associative_scan(input, axis, combine_fn, _builder=None, _generator=None):\n+    \"\"\"Applies the combine_fn to each elements with a carry in :code:`input` tensors along the provided :code:`axis` and update the carry\n+\n+    :param input: the input tensor, or tuple of tensors\n+    :param axis: the dimension along which the reduction should be done\n+    :param combine_fn: a function to combine two groups of scalar tensors (must be marked with @triton.jit)\n+\n+    \"\"\"\n+    if isinstance(input, tensor):\n+        return associative_scan((input,), axis, combine_fn,\n+                                _builder=_builder, _generator=_generator)[0]\n+\n+    def make_combine_region(scan_op):\n+        in_scalar_tys = [t.type.scalar for t in input]\n+        prototype = function_type(in_scalar_tys, in_scalar_tys * 2)\n+\n+        region = scan_op.get_region(0)\n+        with _insertion_guard(_builder):\n+            param_types = [ty.to_ir(_builder) for ty in prototype.param_types]\n+            block = _builder.create_block_with_parent(region, param_types)\n+            args = [tensor(block.arg(i), ty)\n+                    for i, ty in enumerate(prototype.param_types)]\n+            results = _generator.call_JitFunction(combine_fn, args, kwargs={})\n+            if isinstance(results, tensor):\n+                handles = [results.handle]\n+            else:\n+                handles = [r.handle for r in results]\n+            _builder.create_scan_ret(*handles)\n+    axis = _constexpr_to_value(axis)\n+    return semantic.associative_scan(input, axis, make_combine_region, _builder)\n+\n+# cumsum\n+\n+\n+@jit\n+@_add_scan_docstr(\"cumsum\")\n+def cumsum(input, axis=0):\n+    # todo rename this to a generic function name\n+    input = _promote_reduction_input(input)\n+    return associative_scan(input, axis, _sum_combine)\n+\n+# cumprod\n+\n+\n+@jit\n+def _prod_combine(a, b):\n+    return a * b\n+\n+\n+@jit\n+@_add_scan_docstr(\"cumprod\")\n+def cumprod(input, axis=0):\n+    # todo rename this to a generic function name\n+    input = _promote_reduction_input(input)\n+    return associative_scan(input, axis, _prod_combine)\n+\n # -----------------------\n # Compiler Hint Ops\n # -----------------------\n@@ -1573,15 +1648,15 @@ def max_contiguous(input, values, _builder=None):\n @builtin\n def static_print(*values, sep: str = \" \", end: str = \"\\n\", file=None, flush=False, _builder=None):\n     '''\n-    Print the values at compile time. The parameters are the same as the Python builtin :code:`print`.\n+    Print the values at compile time.  The parameters are the same as the builtin :code:`print`.\n \n-    Calling the Python builtin :code:`print` inside your kernel is the same as calling this.\n+    NOTE: Calling the Python builtin :code:`print` is not the same as calling this, it instead maps to :code:`device_print`,\n+    which has special requirements for the arguments.\n \n     .. highlight:: python\n     .. code-block:: python\n \n         tl.static_print(f\"{BLOCK_SIZE=}\")\n-        print(f\"{BLOCK_SIZE=}\")\n     '''\n     pass\n \n@@ -1603,13 +1678,18 @@ def static_assert(cond, msg=\"\", _builder=None):\n @builtin\n def device_print(prefix, *args, _builder=None):\n     '''\n-    Print the values at runtime from the device.  String formatting does not work, so you should\n-    provide the values you want to print as arguments.\n+    Print the values at runtime from the device.  String formatting does not work for runtime values, so you should\n+    provide the values you want to print as arguments.  The first value must be a string, all following values must\n+    be scalars or tensors.\n+\n+    Calling the Python builtin :code:`print` is the same as calling this function, and the requirements for the arguments will match\n+    this function (not the normal requirements for :code:`print`).\n \n     .. highlight:: python\n     .. code-block:: python\n \n         tl.device_print(\"pid\", pid)\n+        print(\"pid\", pid)\n \n     :param prefix: a prefix to print before the values. This is required to be a string literal.\n     :param args: the values to print. They can be any tensor or scalar.\n@@ -1657,12 +1737,16 @@ def device_assert(cond, msg=\"\", _builder=None):\n     while hasattr(module, \"__name__\"):\n         frame = frame.f_back\n         module = inspect.getmodule(frame)\n-    func_name = frame.f_code.co_name\n-    file_name = frame.f_back.f_code.co_filename\n-    # TODO: The line number currently indicates the line\n-    # where the triton function is called but not where the\n-    # device_assert is called. Need to enhance this.\n-    lineno = frame.f_back.f_lineno\n+    lineno = 0\n+    func_name = 'unknown'\n+    file_name = 'unknown'\n+    if frame is not None:\n+        func_name = frame.f_code.co_name\n+        file_name = frame.f_back.f_code.co_filename\n+        # TODO: The line number currently indicates the line\n+        # where the triton function is called but not where the\n+        # device_assert is called. Need to enhance this.\n+        lineno = frame.f_back.f_lineno\n     return semantic.device_assert(_to_tensor(cond, _builder), msg, file_name, func_name, lineno, _builder)\n \n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "file_content_changes": "@@ -807,6 +807,8 @@ def _str_to_store_cache_modifier(cache_modifier):\n             cache = ir.CACHE_MODIFIER.CG\n         elif cache_modifier == \".cs\":\n             cache = ir.CACHE_MODIFIER.CS\n+        elif cache_modifier == \".wt\":\n+            cache = ir.CACHE_MODIFIER.WT\n         else:\n             raise ValueError(f\"Cache modifier {cache_modifier} not supported\")\n     return cache\n@@ -1330,6 +1332,32 @@ def wrap_tensor(x, scalar_ty):\n     )\n \n \n+# ===----------------------------------------------------------------------===\n+#                               Associative Scan\n+# ===----------------------------------------------------------------------===\n+\n+\n+def associative_scan(\n+    inputs: Sequence[tl.tensor], axis: int, region_builder_fn, builder: ir.builder\n+) -> Tuple[tl.tensor, ...]:\n+    if len(inputs) != 1:\n+        raise ValueError(\"Current implementation only support single tensor input\")\n+    shape = inputs[0].type.shape\n+\n+    def wrap_tensor(x, scalar_ty):\n+        res_ty = tl.block_type(scalar_ty, shape)\n+        return tl.tensor(x, res_ty)\n+\n+    scan_op = builder.create_scan([t.handle for t in inputs], axis)\n+    region_builder_fn(scan_op)\n+    scan_op.verify()\n+\n+    return tuple(\n+        wrap_tensor(scan_op.get_result(i), inputs[i].type.scalar)\n+        for i in range(len(inputs))\n+    )\n+\n+\n # ===----------------------------------------------------------------------===\n #                               Math\n # ===----------------------------------------------------------------------==="}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 21, "deletions": 2, "changes": 23, "file_content_changes": "@@ -4,6 +4,22 @@\n from .. import language as tl\n from .matmul_perf_model import early_config_prune, estimate_matmul_time\n \n+_ordered_datatypes = [torch.float16, torch.bfloat16, torch.float32]\n+\n+\n+def get_higher_dtype(a, b):\n+    if a is b:\n+        return a\n+\n+    assert a in _ordered_datatypes\n+    assert b in _ordered_datatypes\n+\n+    for d in _ordered_datatypes:\n+        if a is d:\n+            return b\n+        if b is d:\n+            return a\n+\n \n def init_to_zero(name):\n     return lambda nargs: nargs[name].zero_()\n@@ -97,6 +113,8 @@ def _kernel(A, B, C, M, N, K,\n             k_remaining = K - k * (BLOCK_K * SPLIT_K)\n             a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.)\n             b = tl.load(B, mask=rk[:, None] < k_remaining, other=0.)\n+        a = a.to(C.dtype.element_ty)\n+        b = b.to(C.dtype.element_ty)\n         acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n         A += BLOCK_K * SPLIT_K * stride_ak\n         B += BLOCK_K * SPLIT_K * stride_bk\n@@ -131,9 +149,10 @@ def _call(a, b, dot_out_dtype):\n         M, K = a.shape\n         _, N = b.shape\n         # allocates output\n-        c = torch.empty((M, N), device=device, dtype=a.dtype)\n+        c_dtype = get_higher_dtype(a.dtype, b.dtype)\n+        c = torch.empty((M, N), device=device, dtype=c_dtype)\n         if dot_out_dtype is None:\n-            if a.dtype in [torch.float16, torch.float32, torch.bfloat16]:\n+            if c_dtype in [torch.float16, torch.float32, torch.bfloat16]:\n                 dot_out_dtype = tl.float32\n             else:\n                 dot_out_dtype = tl.int32"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 13, "deletions": 5, "changes": 18, "file_content_changes": "@@ -25,7 +25,7 @@ def __reduce__(self):\n \n \n class Autotuner(KernelInterface):\n-    def __init__(self, fn, arg_names, configs, key, reset_to_zero, prune_configs_by: Dict = None):\n+    def __init__(self, fn, arg_names, configs, key, reset_to_zero, prune_configs_by: Dict = None, warmup=25, rep=100):\n         '''\n         :param prune_configs_by: a dict of functions that are used to prune configs, fields:\n             'perf_model': performance model used to predicate running time with different configs, returns running time\n@@ -58,6 +58,8 @@ def _hook(args):\n         self.perf_model, self.configs_top_k = perf_model, top_k\n         self.early_config_prune = early_config_prune\n         self.fn = fn\n+        self.warmup = warmup\n+        self.rep = rep\n \n     def _bench(self, *args, config, **meta):\n         # check for conflicts, i.e. meta-parameters both provided\n@@ -78,7 +80,7 @@ def kernel_call():\n             self.hook(args)\n             self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **current)\n         try:\n-            return do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n+            return do_bench(kernel_call, warmup=self.warmup, rep=self.rep, quantiles=(0.5, 0.2, 0.8))\n         except OutOfResources:\n             return [float('inf'), float('inf'), float('inf')]\n \n@@ -109,7 +111,9 @@ def run(self, *args, **kwargs):\n         if config.pre_hook is not None:\n             full_nargs = {**self.nargs, **kwargs, **self.best_config.kwargs}\n             config.pre_hook(full_nargs)\n-        return self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n+        ret = self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n+        self.nargs = None\n+        return ret\n \n     def prune_configs(self, kwargs):\n         pruned_configs = self.configs\n@@ -173,7 +177,7 @@ def __str__(self):\n         return ', '.join(res)\n \n \n-def autotune(configs, key, prune_configs_by=None, reset_to_zero=None):\n+def autotune(configs, key, prune_configs_by=None, reset_to_zero=None, warmup=25, rep=100):\n     \"\"\"\n     Decorator for auto-tuning a :code:`triton.jit`'d function.\n \n@@ -204,9 +208,13 @@ def kernel(x_ptr, x_size, **META):\n         'early_config_prune'(optional): a function used to do early prune (eg, num_stages). It takes configs:List[Config] as its input, and returns pruned configs.\n     :param reset_to_zero: a list of argument names whose value will be reset to zero before evaluating any configs.\n     :type reset_to_zero: list[str]\n+    :param warmup: Warmup time (in ms) to pass to benchmarking, defaults to 25.\n+    :type warmup: int\n+    :param rep: Repetition time (in ms) to pass to benchmarking, defaults to 100.\n+    :type rep: int\n     \"\"\"\n     def decorator(fn):\n-        return Autotuner(fn, fn.arg_names, configs, key, reset_to_zero, prune_configs_by)\n+        return Autotuner(fn, fn.arg_names, configs, key, reset_to_zero, prune_configs_by, warmup, rep)\n \n     return decorator\n "}, {"filename": "python/triton/runtime/backends/cuda.c", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -69,6 +69,13 @@ static PyObject *loadBinary(PyObject *self, PyObject *args) {\n   int32_t n_regs = 0;\n   int32_t n_spills = 0;\n   // create driver handles\n+  CUcontext pctx = 0;\n+  CUDA_CHECK(cuCtxGetCurrent(&pctx));\n+  if (!pctx) {\n+    CUDA_CHECK(cuDevicePrimaryCtxRetain(&pctx, device));\n+    CUDA_CHECK(cuCtxSetCurrent(pctx));\n+  }\n+\n   CUDA_CHECK(cuModuleLoadData(&mod, data));\n   CUDA_CHECK(cuModuleGetFunction(&fun, mod, name));\n   // get allocated registers and spilled registers from the function"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -187,3 +187,19 @@ tt.func @print_no_arg(%arg0: !tt.ptr<f32>) {\n   tt.store %arg0, %0 {cache = 1 : i32, evict = 1 : i32} : f32\n   tt.return\n }\n+\n+// CHECK-LABEL: scan_op\n+tt.func @scan_op(%ptr: tensor<1x2x4x!tt.ptr<f32>>, %v : tensor<1x2x4xf32>) {\n+  // CHECK: tt.scan\n+  // CHECK-SAME: axis = 1\n+  // CHECK: tt.scan.return\n+  // CHECK-NEXT: (tensor<1x2x4xf32>) -> tensor<1x2x4xf32>\n+  %a = \"tt.scan\"(%v) <{axis = 1 : i32}>({\n+  ^bb0(%arg0: f32, %arg1: f32):\n+    %add = arith.addf %arg0, %arg1 : f32\n+    tt.scan.return %add : f32\n+  }) : (tensor<1x2x4xf32>) -> tensor<1x2x4xf32>\n+  tt.store %ptr, %a : tensor<1x2x4xf32>\n+  tt.return\n+\n+}"}, {"filename": "test/Conversion/triton_to_tritongpu.mlir", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "file_content_changes": "@@ -67,3 +67,24 @@ tt.func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n \n   tt.return\n }\n+\n+\n+// -----\n+\n+tt.func public @select_op(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i1) attributes {noinline = false} {\n+  // CHECK-LABEL: select_op\n+  %cst = arith.constant dense<0.000000e+00> : tensor<128xf32>\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  %1 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n+  %2 = tt.addptr %1, %0 : tensor<128x!tt.ptr<f32>>, tensor<128xi32>\n+  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128xf32>\n+\n+  // CHECK: %[[splat:.*]] = tt.splat %arg2 : (i1) -> tensor<128xi1, #blocked>\n+  // CHECK-NEXT: %{{.*}} = \"triton_gpu.select\"(%[[splat]], %{{.*}}, %{{.*}}) : (tensor<128xi1, #blocked>, tensor<128xf32, #blocked>, tensor<128xf32, #blocked>) -> tensor<128xf32, #blocked>\n+  %4 = arith.select %arg2, %cst, %3 : tensor<128xf32>\n+\n+  %5 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %0 : tensor<128x!tt.ptr<f32>>, tensor<128xi32>\n+  tt.store %6, %4 {cache = 1 : i32, evict = 1 : i32} : tensor<128xf32>\n+  tt.return\n+}"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 64, "deletions": 0, "changes": 64, "file_content_changes": "@@ -1159,3 +1159,67 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n       tt.return\n   }\n }\n+\n+// -----\n+\n+#mma = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[2, 2]}>\n+#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma, kWidth=1}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma, kWidth=1}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: matmul_tf32_cst_b\n+  tt.func @matmul_tf32_cst_b(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  %a: tensor<32x16xf32, #dot_operand_a>, %c: tensor<32x32xf32, #mma>) {\n+  // CHECK: %[[CST:.+]] = llvm.mlir.constant(1.000000e+00 : f32) : f32\n+  // CHECK: %[[BC:.+]] = llvm.bitcast %[[CST]] : f32 to i32\n+  // CHECK: %[[SI:.+]] = llvm.mlir.undef : !llvm.struct<(i32, i32, i32, i32, i32, i32, i32, i32)>\n+  // CHECK: llvm.insertvalue %[[BC]], %[[SI]][0] : !llvm.struct<(i32, i32, i32, i32, i32, i32, i32, i32)>\n+    %b_mat = arith.constant dense<1.000000e+00> : tensor<16x32xf32, #dot_operand_b>\n+    %28 = tt.dot %a, %b_mat, %c {allowTF32 = true, transA = false, transB = false} : tensor<32x16xf32, #dot_operand_a> * tensor<16x32xf32, #dot_operand_b> -> tensor<32x32xf32, #mma>\n+    %38 = triton_gpu.convert_layout %28 : (tensor<32x32xf32, #mma>) -> tensor<32x32xf32, #blocked>\n+    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n+    %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n+    tt.store %36, %38 : tensor<32x32xf32, #blocked>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [2, 2]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  // CHECK-LABEL: matmul_f16_cst_operands\n+  tt.func public @matmul_f16_cst_operands(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n+  // CHECK: %[[C1f:.+]] = llvm.mlir.constant(1.000000e+00 : f16) : f16\n+  // CHECK: %[[Ci16:.+]] = llvm.bitcast %[[C1f]] : f16 to i16\n+  // CHECK: %[[U:.+]] = llvm.mlir.undef : vector<2xi16>\n+  // CHECK: %[[C0:.+]] = llvm.mlir.constant(0 : i32) : i32\n+  // CHECK: %[[V0:.+]] = llvm.insertelement %[[Ci16]], %[[U]][%[[C0]] : i32] : vector<2xi16>\n+  // CHECK: %[[C1:.+]] = llvm.mlir.constant(1 : i32) : i32\n+  // CHECK: %[[V1:.+]] = llvm.insertelement %[[Ci16]], %[[V0]][%[[C1]] : i32] : vector<2xi16>\n+  // CHECK: %[[BC:.+]] = llvm.bitcast %[[V1]] : vector<2xi16> to i32\n+  // CHECK: %[[SU:.+]] = llvm.mlir.undef : !llvm.struct<(i32, i32, i32, i32, i32, i32, i32, i32)>\n+  // CHECK: llvm.insertvalue %[[BC]], %[[SU]][0] : !llvm.struct<(i32, i32, i32, i32, i32, i32, i32, i32)>\n+    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>>\n+    %cst_1 = arith.constant dense<1.000000e+00> : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>>\n+    %cst_2 = arith.constant dense<32> : tensor<32x1xi32, #blocked>\n+    %0 = tt.dot %cst_0, %cst_1, %cst {allowTF32 = true} : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> -> tensor<32x32xf32, #mma>\n+    %1 = triton_gpu.convert_layout %0 : (tensor<32x32xf32, #mma>) -> tensor<32x32xf32, #blocked>\n+    %2 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+    %3 = tt.expand_dims %2 {axis = 1 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<32x1xi32, #blocked>\n+    %4 = arith.muli %3, %cst_2 : tensor<32x1xi32, #blocked>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<32x1x!tt.ptr<f16>, #blocked>\n+    %6 = tt.addptr %5, %4 : tensor<32x1x!tt.ptr<f16>, #blocked>, tensor<32x1xi32, #blocked>\n+    %7 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+    %8 = tt.expand_dims %7 {axis = 0 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x32xi32, #blocked>\n+    %9 = tt.broadcast %6 : (tensor<32x1x!tt.ptr<f16>, #blocked>) -> tensor<32x32x!tt.ptr<f16>, #blocked>\n+    %10 = tt.broadcast %8 : (tensor<1x32xi32, #blocked>) -> tensor<32x32xi32, #blocked>\n+    %11 = tt.addptr %9, %10 : tensor<32x32x!tt.ptr<f16>, #blocked>, tensor<32x32xi32, #blocked>\n+    %12 = arith.truncf %1 : tensor<32x32xf32, #blocked> to tensor<32x32xf16, #blocked>\n+    tt.store %11, %12 {cache = 1 : i32, evict = 1 : i32} : tensor<32x32xf16, #blocked>\n+    tt.return\n+  }\n+}"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 298, "deletions": 1, "changes": 299, "file_content_changes": "@@ -1122,7 +1122,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n \n // -----\n \n-// Check if the SimplifyReduceCvt handles convert_layout lifted from the for loop.\n+// Check if SimplifyReduceCvt handles convert_layout lifted from the for loop.\n // CHECK-LABEL: reduce_cvt2\n // Match the reduction\n // CHECK: tt.reduce\n@@ -1202,3 +1202,300 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     tt.return\n   }\n }\n+\n+// -----\n+\n+// Ensure that RematerializeForward doesn't apply when a convert has multiple uses\n+// CHECK-LABEL: loop_convert_multi_uses\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [16, 2], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n+#blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#blocked4 = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @loop_convert_multi_uses(%arg0: i32 {tt.divisibility = 16 : i32}, %arg1: i32, %arg2: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32, %arg10: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32, %arg13: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32 {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %cst = arith.constant dense<0xFF800000> : tensor<16xf32, #blocked>\n+    %c1_i32 = arith.constant 1 : i32\n+    %c0_i32 = arith.constant 0 : i32\n+    %cst_0 = arith.constant dense<0.000000e+00> : tensor<16xf32, #blocked>\n+    %cst_1 = arith.constant dense<1> : tensor<16xi32, #blocked>\n+    %cst_2 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked1>\n+    %cst_3 = arith.constant dense<1> : tensor<16x1xi32, #blocked1>\n+    %c16_i32 = arith.constant 16 : i32\n+    %0 = tt.get_program_id x : i32\n+    %1 = tt.get_program_id y : i32\n+    %2 = arith.divsi %1, %arg0 : i32\n+    %3 = arith.remsi %1, %arg0 : i32\n+    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked>\n+    %5 = arith.muli %0, %c16_i32 : i32\n+    %6 = tt.splat %5 : (i32) -> tensor<16xi32, #blocked>\n+    %7 = arith.addi %6, %4 : tensor<16xi32, #blocked>\n+    %8 = arith.muli %2, %arg3 : i32\n+    %9 = arith.muli %3, %arg4 : i32\n+    %10 = arith.addi %8, %9 : i32\n+    %11 = triton_gpu.convert_layout %7 : (tensor<16xi32, #blocked>) -> tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+    %12 = tt.expand_dims %11 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<16x1xi32, #blocked2>\n+    %13 = triton_gpu.convert_layout %12 : (tensor<16x1xi32, #blocked2>) -> tensor<16x1xi32, #blocked1>\n+    %14 = tt.splat %arg6 : (i32) -> tensor<16x1xi32, #blocked1>\n+    %15 = arith.muli %13, %14 : tensor<16x1xi32, #blocked1>\n+    %16 = triton_gpu.convert_layout %4 : (tensor<16xi32, #blocked>) -> tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>\n+    %17 = tt.expand_dims %16 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>) -> tensor<1x16xi32, #blocked3>\n+    %18 = tt.broadcast %15 : (tensor<16x1xi32, #blocked1>) -> tensor<16x16xi32, #blocked1>\n+    %19 = tt.broadcast %17 : (tensor<1x16xi32, #blocked3>) -> tensor<16x16xi32, #blocked3>\n+    %20 = triton_gpu.convert_layout %19 : (tensor<16x16xi32, #blocked3>) -> tensor<16x16xi32, #blocked1>\n+    %21 = arith.addi %18, %20 : tensor<16x16xi32, #blocked1>\n+    %22 = tt.splat %arg2 : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #blocked1>\n+    %23 = \"triton_gpu.cmpi\"(%13, %cst_3) <{predicate = 2 : i64}> : (tensor<16x1xi32, #blocked1>, tensor<16x1xi32, #blocked1>) -> tensor<16x1xi1, #blocked1>\n+    %24 = tt.broadcast %23 : (tensor<16x1xi1, #blocked1>) -> tensor<16x16xi1, #blocked1>\n+    %25 = arith.truncf %cst_2 : tensor<16x16xf32, #blocked1> to tensor<16x16xf16, #blocked1>\n+    %26 = arith.muli %2, %arg11 : i32\n+    %27 = arith.muli %3, %arg12 : i32\n+    %28 = arith.addi %26, %27 : i32\n+    %29 = tt.splat %arg10 : (!tt.ptr<f32>) -> tensor<16x!tt.ptr<f32>, #blocked>\n+    %30 = \"triton_gpu.cmpi\"(%7, %cst_1) <{predicate = 2 : i64}> : (tensor<16xi32, #blocked>, tensor<16xi32, #blocked>) -> tensor<16xi1, #blocked>\n+    %31 = arith.muli %2, %arg8 : i32\n+    %32 = arith.muli %3, %arg9 : i32\n+    %33 = arith.addi %31, %32 : i32\n+    %34 = tt.splat %arg7 : (!tt.ptr<f32>) -> tensor<16x!tt.ptr<f32>, #blocked>\n+    %35:3 = scf.for %arg17 = %c0_i32 to %arg1 step %c1_i32 iter_args(%arg18 = %cst_2, %arg19 = %cst_0, %arg20 = %cst) -> (tensor<16x16xf32, #blocked1>, tensor<16xf32, #blocked>, tensor<16xf32, #blocked>)  : i32 {\n+      %60 = arith.muli %arg17, %arg5 : i32\n+      %61 = arith.addi %10, %60 : i32\n+      %62 = tt.splat %61 : (i32) -> tensor<16x16xi32, #blocked1>\n+      %63 = arith.addi %62, %21 : tensor<16x16xi32, #blocked1>\n+      %64 = tt.addptr %22, %63 : tensor<16x16x!tt.ptr<f16>, #blocked1>, tensor<16x16xi32, #blocked1>\n+      %65 = triton_gpu.convert_layout %64 : (tensor<16x16x!tt.ptr<f16>, #blocked1>) -> tensor<16x16x!tt.ptr<f16>, #blocked4>\n+      %66 = triton_gpu.convert_layout %24 : (tensor<16x16xi1, #blocked1>) -> tensor<16x16xi1, #blocked4>\n+      %67 = triton_gpu.convert_layout %25 : (tensor<16x16xf16, #blocked1>) -> tensor<16x16xf16, #blocked4>\n+      %68 = tt.load %65, %66, %67 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blocked4>\n+      %69 = triton_gpu.convert_layout %68 : (tensor<16x16xf16, #blocked4>) -> tensor<16x16xf16, #blocked1>\n+      %70 = arith.addi %28, %arg17 : i32\n+      %71 = tt.splat %70 : (i32) -> tensor<16xi32, #blocked>\n+      %72 = arith.addi %71, %7 : tensor<16xi32, #blocked>\n+      %73 = tt.addptr %29, %72 : tensor<16x!tt.ptr<f32>, #blocked>, tensor<16xi32, #blocked>\n+      %74 = triton_gpu.convert_layout %73 : (tensor<16x!tt.ptr<f32>, #blocked>) -> tensor<16x!tt.ptr<f32>, #blocked>\n+      %75 = triton_gpu.convert_layout %30 : (tensor<16xi1, #blocked>) -> tensor<16xi1, #blocked>\n+      %76 = triton_gpu.convert_layout %cst_0 : (tensor<16xf32, #blocked>) -> tensor<16xf32, #blocked>\n+      %77 = tt.load %74, %75, %76 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16xf32, #blocked>\n+      %78 = arith.addi %33, %arg17 : i32\n+      %79 = tt.splat %78 : (i32) -> tensor<16xi32, #blocked>\n+      %80 = arith.addi %79, %7 : tensor<16xi32, #blocked>\n+      %81 = tt.addptr %34, %80 : tensor<16x!tt.ptr<f32>, #blocked>, tensor<16xi32, #blocked>\n+      %82 = triton_gpu.convert_layout %81 : (tensor<16x!tt.ptr<f32>, #blocked>) -> tensor<16x!tt.ptr<f32>, #blocked>\n+      %83 = triton_gpu.convert_layout %30 : (tensor<16xi1, #blocked>) -> tensor<16xi1, #blocked>\n+      %84 = triton_gpu.convert_layout %cst_0 : (tensor<16xf32, #blocked>) -> tensor<16xf32, #blocked>\n+      %85 = tt.load %82, %83, %84 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16xf32, #blocked>\n+      %86 = \"triton_gpu.cmpf\"(%arg20, %85) <{predicate = 2 : i64}> : (tensor<16xf32, #blocked>, tensor<16xf32, #blocked>) -> tensor<16xi1, #blocked>\n+      %87 = \"triton_gpu.select\"(%86, %arg20, %85) : (tensor<16xi1, #blocked>, tensor<16xf32, #blocked>, tensor<16xf32, #blocked>) -> tensor<16xf32, #blocked>\n+      %88 = arith.subf %arg20, %87 : tensor<16xf32, #blocked>\n+      %89 = math.exp %88 : tensor<16xf32, #blocked>\n+      %90 = arith.subf %85, %87 : tensor<16xf32, #blocked>\n+      %91 = math.exp %90 : tensor<16xf32, #blocked>\n+      %92 = arith.mulf %89, %arg19 : tensor<16xf32, #blocked>\n+      %93 = arith.mulf %91, %77 : tensor<16xf32, #blocked>\n+      %94 = arith.addf %92, %93 : tensor<16xf32, #blocked>\n+      %95 = arith.divf %91, %94 : tensor<16xf32, #blocked>\n+      %96 = arith.divf %arg19, %94 : tensor<16xf32, #blocked>\n+      %97 = arith.mulf %96, %89 : tensor<16xf32, #blocked>\n+      %98 = triton_gpu.convert_layout %97 : (tensor<16xf32, #blocked>) -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %99 = tt.expand_dims %98 {axis = 1 : i32} : (tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<16x1xf32, #blocked2>\n+      %100 = triton_gpu.convert_layout %99 : (tensor<16x1xf32, #blocked2>) -> tensor<16x1xf32, #blocked1>\n+      %101 = tt.broadcast %100 : (tensor<16x1xf32, #blocked1>) -> tensor<16x16xf32, #blocked1>\n+      %102 = arith.mulf %arg18, %101 : tensor<16x16xf32, #blocked1>\n+      %103 = triton_gpu.convert_layout %95 : (tensor<16xf32, #blocked>) -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %104 = tt.expand_dims %103 {axis = 1 : i32} : (tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<16x1xf32, #blocked2>\n+      %105 = triton_gpu.convert_layout %104 : (tensor<16x1xf32, #blocked2>) -> tensor<16x1xf32, #blocked1>\n+      %106 = tt.broadcast %105 : (tensor<16x1xf32, #blocked1>) -> tensor<16x16xf32, #blocked1>\n+      %107 = arith.extf %69 : tensor<16x16xf16, #blocked1> to tensor<16x16xf32, #blocked1>\n+      %108 = arith.mulf %107, %106 : tensor<16x16xf32, #blocked1>\n+      %109 = arith.addf %102, %108 : tensor<16x16xf32, #blocked1>\n+      scf.yield %109, %94, %87 : tensor<16x16xf32, #blocked1>, tensor<16xf32, #blocked>, tensor<16xf32, #blocked>\n+    }\n+    %36 = arith.muli %2, %arg14 : i32\n+    %37 = arith.muli %3, %arg15 : i32\n+    %38 = arith.addi %36, %37 : i32\n+    %39 = triton_gpu.convert_layout %7 : (tensor<16xi32, #blocked>) -> tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+    %40 = tt.expand_dims %39 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<16x1xi32, #blocked2>\n+    %41 = triton_gpu.convert_layout %40 : (tensor<16x1xi32, #blocked2>) -> tensor<16x1xi32, #blocked1>\n+    %42 = tt.splat %arg16 : (i32) -> tensor<16x1xi32, #blocked1>\n+    %43 = arith.muli %41, %42 : tensor<16x1xi32, #blocked1>\n+    %44 = triton_gpu.convert_layout %4 : (tensor<16xi32, #blocked>) -> tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>\n+    %45 = tt.expand_dims %44 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>) -> tensor<1x16xi32, #blocked3>\n+    %46 = tt.broadcast %43 : (tensor<16x1xi32, #blocked1>) -> tensor<16x16xi32, #blocked1>\n+    %47 = tt.broadcast %45 : (tensor<1x16xi32, #blocked3>) -> tensor<16x16xi32, #blocked3>\n+    %48 = triton_gpu.convert_layout %47 : (tensor<16x16xi32, #blocked3>) -> tensor<16x16xi32, #blocked1>\n+    %49 = arith.addi %46, %48 : tensor<16x16xi32, #blocked1>\n+    %50 = tt.splat %38 : (i32) -> tensor<16x16xi32, #blocked1>\n+    %51 = arith.addi %50, %49 : tensor<16x16xi32, #blocked1>\n+    %52 = tt.splat %arg13 : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #blocked1>\n+    %53 = tt.addptr %52, %51 : tensor<16x16x!tt.ptr<f16>, #blocked1>, tensor<16x16xi32, #blocked1>\n+    %54 = \"triton_gpu.cmpi\"(%41, %cst_3) <{predicate = 2 : i64}> : (tensor<16x1xi32, #blocked1>, tensor<16x1xi32, #blocked1>) -> tensor<16x1xi1, #blocked1>\n+    %55 = tt.broadcast %54 : (tensor<16x1xi1, #blocked1>) -> tensor<16x16xi1, #blocked1>\n+    %56 = arith.truncf %35#0 : tensor<16x16xf32, #blocked1> to tensor<16x16xf16, #blocked1>\n+    %57 = triton_gpu.convert_layout %53 : (tensor<16x16x!tt.ptr<f16>, #blocked1>) -> tensor<16x16x!tt.ptr<f16>, #blocked4>\n+    %58 = triton_gpu.convert_layout %56 : (tensor<16x16xf16, #blocked1>) -> tensor<16x16xf16, #blocked4>\n+    %59 = triton_gpu.convert_layout %55 : (tensor<16x16xi1, #blocked1>) -> tensor<16x16xi1, #blocked4>\n+    tt.store %57, %58, %59 {cache = 1 : i32, evict = 1 : i32} : tensor<16x16xf16, #blocked4>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+// Check if MoveConvertOutOfLoop hangs because of adding additional conversions\n+// CHECK-LABEL: loop_print\n+// CHECK-NOT: triton_gpu.convert_layout\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+#blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#blocked4 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#blocked5 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @loop_print(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %c32_i32 = arith.constant 32 : i32\n+    %c31_i32 = arith.constant 31 : i32\n+    %c0_i32 = arith.constant 0 : i32\n+    %c1_i32 = arith.constant 1 : i32\n+    %cst = arith.constant dense<32> : tensor<32x128xi32, #blocked>\n+    %cst_0 = arith.constant dense<32> : tensor<128x32xi32, #blocked1>\n+    %cst_1 = arith.constant 0.000000e+00 : f32\n+    %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked2>\n+    %1 = triton_gpu.convert_layout %0 : (tensor<128xi32, #blocked2>) -> tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+    %2 = tt.expand_dims %1 {axis = 1 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<128x1xi32, #blocked1>\n+    %3 = tt.splat %arg6 : (i32) -> tensor<128x1xi32, #blocked1>\n+    %4 = arith.muli %2, %3 : tensor<128x1xi32, #blocked1>\n+    %5 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked2>\n+    %6 = triton_gpu.convert_layout %5 : (tensor<32xi32, #blocked2>) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>\n+    %7 = tt.expand_dims %6 {axis = 0 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>) -> tensor<1x32xi32, #blocked3>\n+    %8 = tt.broadcast %4 : (tensor<128x1xi32, #blocked1>) -> tensor<128x32xi32, #blocked1>\n+    %9 = tt.broadcast %7 : (tensor<1x32xi32, #blocked3>) -> tensor<128x32xi32, #blocked3>\n+    %10 = triton_gpu.convert_layout %9 : (tensor<128x32xi32, #blocked3>) -> tensor<128x32xi32, #blocked1>\n+    %11 = arith.addi %8, %10 : tensor<128x32xi32, #blocked1>\n+    %12 = triton_gpu.convert_layout %5 : (tensor<32xi32, #blocked2>) -> tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+    %13 = tt.expand_dims %12 {axis = 1 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<32x1xi32, #blocked1>\n+    %14 = triton_gpu.convert_layout %13 : (tensor<32x1xi32, #blocked1>) -> tensor<32x1xi32, #blocked>\n+    %15 = triton_gpu.convert_layout %0 : (tensor<128xi32, #blocked2>) -> tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>\n+    %16 = tt.expand_dims %15 {axis = 0 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>) -> tensor<1x128xi32, #blocked3>\n+    %17 = tt.broadcast %14 : (tensor<32x1xi32, #blocked>) -> tensor<32x128xi32, #blocked>\n+    %18 = tt.broadcast %16 : (tensor<1x128xi32, #blocked3>) -> tensor<32x128xi32, #blocked3>\n+    %19 = triton_gpu.convert_layout %18 : (tensor<32x128xi32, #blocked3>) -> tensor<32x128xi32, #blocked>\n+    %20 = arith.addi %17, %19 : tensor<32x128xi32, #blocked>\n+    %21 = arith.addi %arg5, %c31_i32 : i32\n+    %22 = arith.divsi %21, %c32_i32 : i32\n+    %23 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #blocked1>\n+    %24 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #blocked>\n+    %25:3 = scf.for %arg7 = %c0_i32 to %22 step %c1_i32 iter_args(%arg8 = %cst_1, %arg9 = %11, %arg10 = %20) -> (f32, tensor<128x32xi32, #blocked1>, tensor<32x128xi32, #blocked>)  : i32 {\n+      tt.print \"a_offsets: \" : %arg9 : tensor<128x32xi32, #blocked1>\n+      %27 = tt.addptr %23, %arg9 : tensor<128x32x!tt.ptr<f16>, #blocked1>, tensor<128x32xi32, #blocked1>\n+      %28 = triton_gpu.convert_layout %27 : (tensor<128x32x!tt.ptr<f16>, #blocked1>) -> tensor<128x32x!tt.ptr<f16>, #blocked4>\n+      %29 = tt.load %28 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #blocked4>\n+      %30 = triton_gpu.convert_layout %29 : (tensor<128x32xf16, #blocked4>) -> tensor<128x32xf16, #blocked1>\n+      %31 = tt.addptr %24, %arg10 : tensor<32x128x!tt.ptr<f16>, #blocked>, tensor<32x128xi32, #blocked>\n+      %32 = triton_gpu.convert_layout %31 : (tensor<32x128x!tt.ptr<f16>, #blocked>) -> tensor<32x128x!tt.ptr<f16>, #blocked5>\n+      %33 = tt.load %32 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #blocked5>\n+      %34 = triton_gpu.convert_layout %33 : (tensor<32x128xf16, #blocked5>) -> tensor<32x128xf16, #blocked>\n+      %35 = \"tt.reduce\"(%30) <{axis = 0 : i32}> ({\n+      ^bb0(%arg11: f16, %arg12: f16):\n+        %46 = arith.addf %arg11, %arg12 : f16\n+        tt.reduce.return %46 : f16\n+      }) : (tensor<128x32xf16, #blocked1>) -> tensor<32xf16, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>\n+      %36 = triton_gpu.convert_layout %35 : (tensor<32xf16, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>) -> tensor<32xf16, #blocked2>\n+      %37 = \"tt.reduce\"(%36) <{axis = 0 : i32}> ({\n+      ^bb0(%arg11: f16, %arg12: f16):\n+        %46 = arith.addf %arg11, %arg12 : f16\n+        tt.reduce.return %46 : f16\n+      }) : (tensor<32xf16, #blocked2>) -> f16\n+      %38 = \"tt.reduce\"(%34) <{axis = 0 : i32}> ({\n+      ^bb0(%arg11: f16, %arg12: f16):\n+        %46 = arith.addf %arg11, %arg12 : f16\n+        tt.reduce.return %46 : f16\n+      }) : (tensor<32x128xf16, #blocked>) -> tensor<128xf16, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+      %39 = triton_gpu.convert_layout %38 : (tensor<128xf16, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<128xf16, #blocked2>\n+      %40 = \"tt.reduce\"(%39) <{axis = 0 : i32}> ({\n+      ^bb0(%arg11: f16, %arg12: f16):\n+        %46 = arith.addf %arg11, %arg12 : f16\n+        tt.reduce.return %46 : f16\n+      }) : (tensor<128xf16, #blocked2>) -> f16\n+      %41 = arith.addf %37, %40 : f16\n+      %42 = arith.extf %41 : f16 to f32\n+      %43 = arith.addf %arg8, %42 : f32\n+      %44 = arith.addi %arg9, %cst_0 : tensor<128x32xi32, #blocked1>\n+      %45 = arith.addi %arg10, %cst : tensor<32x128xi32, #blocked>\n+      scf.yield %43, %44, %45 : f32, tensor<128x32xi32, #blocked1>, tensor<32x128xi32, #blocked>\n+    }\n+    %26 = arith.truncf %25#0 : f32 to f16\n+    tt.store %arg2, %26 {cache = 1 : i32, evict = 1 : i32} : f16\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+// Check if SimplifyReduceCvt handles the cvt,reduce->reduce,cvt conversion but not the general push forward conversion\n+// CHECK-LABEL: reduce_cvt3\n+// CHECK: tt.dot\n+// CHECK-NEXT: tt.reduce\n+// CHECK: triton_gpu.convert_layout\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n+#blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#blocked4 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#blocked5 = #triton_gpu.blocked<{sizePerThread = [2, 2], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0, 1]}>\n+#shared1 = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @reduce_cvt3(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<i32> {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #blocked>\n+    %cst_0 = arith.constant dense<32> : tensor<32x1xi32, #blocked>\n+    %0 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked1>\n+    %1 = triton_gpu.convert_layout %0 : (tensor<32xi32, #blocked1>) -> tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+    %2 = tt.expand_dims %1 {axis = 1 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<32x1xi32, #blocked2>\n+    %3 = triton_gpu.convert_layout %2 : (tensor<32x1xi32, #blocked2>) -> tensor<32x1xi32, #blocked>\n+    %4 = arith.muli %3, %cst_0 : tensor<32x1xi32, #blocked>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<32x1x!tt.ptr<f16>, #blocked>\n+    %6 = tt.addptr %5, %4 : tensor<32x1x!tt.ptr<f16>, #blocked>, tensor<32x1xi32, #blocked>\n+    %7 = triton_gpu.convert_layout %0 : (tensor<32xi32, #blocked1>) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>\n+    %8 = tt.expand_dims %7 {axis = 0 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>) -> tensor<1x32xi32, #blocked3>\n+    %9 = tt.broadcast %6 : (tensor<32x1x!tt.ptr<f16>, #blocked>) -> tensor<32x32x!tt.ptr<f16>, #blocked>\n+    %10 = tt.broadcast %8 : (tensor<1x32xi32, #blocked3>) -> tensor<32x32xi32, #blocked3>\n+    %11 = triton_gpu.convert_layout %10 : (tensor<32x32xi32, #blocked3>) -> tensor<32x32xi32, #blocked>\n+    %12 = tt.addptr %9, %11 : tensor<32x32x!tt.ptr<f16>, #blocked>, tensor<32x32xi32, #blocked>\n+    %13 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<32x1x!tt.ptr<f16>, #blocked>\n+    %14 = tt.addptr %13, %4 : tensor<32x1x!tt.ptr<f16>, #blocked>, tensor<32x1xi32, #blocked>\n+    %15 = tt.broadcast %14 : (tensor<32x1x!tt.ptr<f16>, #blocked>) -> tensor<32x32x!tt.ptr<f16>, #blocked>\n+    %16 = tt.addptr %15, %11 : tensor<32x32x!tt.ptr<f16>, #blocked>, tensor<32x32xi32, #blocked>\n+    %17 = triton_gpu.convert_layout %12 : (tensor<32x32x!tt.ptr<f16>, #blocked>) -> tensor<32x32x!tt.ptr<f16>, #blocked4>\n+    %18 = tt.load %17 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf16, #blocked4>\n+    %19 = triton_gpu.convert_layout %18 : (tensor<32x32xf16, #blocked4>) -> tensor<32x32xf16, #blocked>\n+    %20 = triton_gpu.convert_layout %16 : (tensor<32x32x!tt.ptr<f16>, #blocked>) -> tensor<32x32x!tt.ptr<f16>, #blocked4>\n+    %21 = tt.load %20 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf16, #blocked4>\n+    %22 = triton_gpu.convert_layout %21 : (tensor<32x32xf16, #blocked4>) -> tensor<32x32xf16, #blocked>\n+    %23 = triton_gpu.convert_layout %22 : (tensor<32x32xf16, #blocked>) -> tensor<32x32xf16, #shared>\n+    %24 = tt.trans %23 : (tensor<32x32xf16, #shared>) -> tensor<32x32xf16, #shared1>\n+    %25 = triton_gpu.convert_layout %24 : (tensor<32x32xf16, #shared1>) -> tensor<32x32xf16, #blocked>\n+    %26 = triton_gpu.convert_layout %19 : (tensor<32x32xf16, #blocked>) -> tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked5}>>\n+    %27 = triton_gpu.convert_layout %25 : (tensor<32x32xf16, #blocked>) -> tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked5}>>\n+    %28 = triton_gpu.convert_layout %cst : (tensor<32x32xf32, #blocked>) -> tensor<32x32xf32, #blocked5>\n+    %29 = tt.dot %26, %27, %28 {allowTF32 = true} : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked5}>> * tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked5}>> -> tensor<32x32xf32, #blocked5>\n+    %30 = triton_gpu.convert_layout %29 : (tensor<32x32xf32, #blocked5>) -> tensor<32x32xf32, #blocked>\n+    %31:2 = \"tt.reduce\"(%30, %11) <{axis = 1 : i32}> ({\n+    ^bb0(%arg3: f32, %arg4: i32, %arg5: f32, %arg6: i32):\n+      %37 = \"triton_gpu.cmpf\"(%arg3, %arg5) <{predicate = 1 : i64}> : (f32, f32) -> i1\n+      %38 = \"triton_gpu.cmpi\"(%arg4, %arg6) <{predicate = 2 : i64}> : (i32, i32) -> i1\n+      %39 = arith.andi %37, %38 : i1\n+      %40 = \"triton_gpu.cmpf\"(%arg3, %arg5) <{predicate = 2 : i64}> : (f32, f32) -> i1\n+      %41 = arith.ori %40, %39 : i1\n+      %42 = arith.select %41, %arg3, %arg5 : f32\n+      %43 = arith.select %41, %arg4, %arg6 : i32\n+      tt.reduce.return %42, %43 : f32, i32\n+    }) : (tensor<32x32xf32, #blocked>, tensor<32x32xi32, #blocked>) -> (tensor<32xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>, tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>)\n+    %32 = triton_gpu.convert_layout %31#1 : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<32xi32, #blocked1>\n+    %33 = tt.splat %arg2 : (!tt.ptr<i32>) -> tensor<32x!tt.ptr<i32>, #blocked1>\n+    %34 = tt.addptr %33, %0 : tensor<32x!tt.ptr<i32>, #blocked1>, tensor<32xi32, #blocked1>\n+    %35 = triton_gpu.convert_layout %34 : (tensor<32x!tt.ptr<i32>, #blocked1>) -> tensor<32x!tt.ptr<i32>, #blocked1>\n+    %36 = triton_gpu.convert_layout %32 : (tensor<32xi32, #blocked1>) -> tensor<32xi32, #blocked1>\n+    tt.store %35, %36 {cache = 1 : i32, evict = 1 : i32} : tensor<32xi32, #blocked1>\n+    tt.return\n+  }\n+}"}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 39, "deletions": 0, "changes": 39, "file_content_changes": "@@ -184,3 +184,42 @@ tt.func @push_convert_both_operands(\n }\n \n }\n+\n+// -----\n+\n+#blockedA = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#blockedB = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n+// CHECK: #[[BA:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+// CHECK: #[[BB:.*]] = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n+// CHECK: #[[MMA:.*]] = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n+\n+// CHECK: tt.func @update_kwidth_slice\n+// CHECK: %[[CST:.+]] = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[ALOAD:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #[[BA]]>\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]] : (tensor<16x16xf16, #[[BA]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[BLOAD:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #[[BB]]>\n+// CHECK: %[[BCVT:.*]] = triton_gpu.convert_layout %[[BLOAD]] : (tensor<16x16xf16, #[[BB]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[AEXT:.*]] = arith.extf %[[ACVT]] : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>> to tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[BEXT:.*]] = arith.extf %[[BCVT]] : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>> to tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[ADD:.+]] = arith.addf %[[BEXT]], %[[CST]] : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: tt.dot %[[AEXT]], %[[ADD]], %{{.*}} {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>> -> tensor<16x16xf32, #mma>\n+tt.func @update_kwidth_slice(\n+                   %pa: tensor<16x16x!tt.ptr<f16>, #blockedA> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #blockedB> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %c: tensor<16x16xf32, #mma>) -> tensor<16x16xf32, #mma>{\n+  %cst = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #blockedB>\n+  %a = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blockedA>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blockedB>\n+  %ae = arith.extf %a : tensor<16x16xf16, #blockedA> to tensor<16x16xf32, #blockedA>\n+  %be = arith.extf %b : tensor<16x16xf16, #blockedB> to tensor<16x16xf32, #blockedB>\n+  %add = arith.addf %be, %cst : tensor<16x16xf32, #blockedB>\n+  %al = triton_gpu.convert_layout %ae : (tensor<16x16xf32, #blockedA>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n+  %bl = triton_gpu.convert_layout %add : (tensor<16x16xf32, #blockedB>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+  %r = tt.dot %al, %bl, %c {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>> -> tensor<16x16xf32, #mma>\n+  tt.return %r : tensor<16x16xf32, #mma>\n+}\n+\n+}"}, {"filename": "test/TritonGPU/reorder-instructions.mlir", "status": "added", "additions": 49, "deletions": 0, "changes": 49, "file_content_changes": "@@ -0,0 +1,49 @@\n+// RUN: triton-opt %s -split-input-file -tritongpu-reorder-instructions | FileCheck %s\n+\n+// check that we don't hoist convert_layout above its operand definition.\n+// CHECK-LABEL: convert_cannot_hoist\n+//       CHECK:   %[[CVTS:.+]] = triton_gpu.convert_layout\n+//       CHECK:   triton_gpu.convert_layout %[[CVTS]]\n+//       CHECK:   tt.dot\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [2, 2]}>\n+#shared = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [0, 1]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @convert_cannot_hoist(%arg0: tensor<32x32x!tt.ptr<f32>, #blocked>) attributes {noinline = false} {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n+    %cst_0 = arith.constant dense<1.230000e+02> : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+    %9 = tt.load %arg0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #blocked>\n+    %10 = triton_gpu.convert_layout %9 : (tensor<32x32xf32, #blocked>) -> tensor<32x32xf32, #shared>\n+    %11 = triton_gpu.convert_layout %10 : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n+    %12 = tt.dot %11, %cst_0, %cst {allowTF32 = true} : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>> * tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>> -> tensor<32x32xf32, #mma>\n+    %13 = triton_gpu.convert_layout %12 : (tensor<32x32xf32, #mma>) -> tensor<32x32xf32, #blocked>\n+    tt.store %arg0, %13 {cache = 1 : i32, evict = 1 : i32} : tensor<32x32xf32, #blocked>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: sink_convert_idx_1\n+//       CHECK: triton_gpu.convert_layout %{{.*}} : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n+//       CHECK: triton_gpu.convert_layout %{{.*}} : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+//       CHECK: tt.dot\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [2, 2]}>\n+#shared = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [0, 1]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @sink_convert_idx_1(%arg0: tensor<32x32x!tt.ptr<f32>, #blocked>) attributes {noinline = false} {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n+    %B = tt.load %arg0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #blocked>\n+    %BS = triton_gpu.convert_layout %B : (tensor<32x32xf32, #blocked>) -> tensor<32x32xf32, #shared>\n+    %BD = triton_gpu.convert_layout %BS : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+    %cst_0 = arith.constant dense<1.230000e+02> : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+    %A = tt.load %arg0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #blocked>\n+    %AS = triton_gpu.convert_layout %A : (tensor<32x32xf32, #blocked>) -> tensor<32x32xf32, #shared>\n+    %AD = triton_gpu.convert_layout %AS : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n+    %12 = tt.dot %AD, %BD, %cst {allowTF32 = true} : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>> * tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>> -> tensor<32x32xf32, #mma>\n+    %13 = triton_gpu.convert_layout %12 : (tensor<32x32xf32, #mma>) -> tensor<32x32xf32, #blocked>\n+    tt.store %arg0, %13 {cache = 1 : i32, evict = 1 : i32} : tensor<32x32xf32, #blocked>\n+    tt.return\n+  }\n+}"}]