[{"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 57, "deletions": 78, "changes": 135, "file_content_changes": "@@ -34,11 +34,27 @@ unsigned getElementBitWidth(const Value &val) {\n   return typeForMem.getIntOrFloatBitWidth();\n }\n \n-typedef DenseMap<Value, std::function<Type(Type)>> LayoutMap;\n+static Value getMemAccessPtr(Operation *op) {\n+  if (auto ld = dyn_cast<triton::LoadOp>(op))\n+    return ld.getPtr();\n+  if (auto atomic = dyn_cast<triton::AtomicRMWOp>(op))\n+    return atomic.getPtr();\n+  if (auto atomic = dyn_cast<triton::AtomicCASOp>(op))\n+    return atomic.getPtr();\n+  if (auto insert = dyn_cast<triton::gpu::InsertSliceAsyncOp>(op))\n+    return insert.getSrc();\n+  if (auto store = dyn_cast<triton::StoreOp>(op))\n+    return store.getPtr();\n+  return nullptr;\n+}\n+\n+typedef DenseMap<Operation *, std::function<Type(Type)>> LayoutMap;\n \n struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n   Attribute getCoalescedEncoding(ModuleAxisInfoAnalysis &axisInfoAnalysis,\n-                                 Value ptr, int numWarps, int threadsPerWarp) {\n+                                 Operation *memAccess, int numWarps,\n+                                 int threadsPerWarp) {\n+    Value ptr = getMemAccessPtr(memAccess);\n     auto refType = ptr.getType();\n     if (refType.isa<PointerType>())\n       refType = refType.cast<PointerType>().getPointeeType();\n@@ -87,7 +103,7 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n       order = argSort(queryAxisInfo(ptr).getContiguity());\n     }\n \n-    auto matchesOrder = [&refTensorType](const Value &val) {\n+    auto matchesShape = [&refTensorType](const Value &val) {\n       if (val.getType() == refTensorType) {\n         return true;\n       }\n@@ -103,17 +119,18 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     // among all dependent pointers who have the same order as\n     // `ptr`.\n     // We only do it for normal tensors of pointers, not tensor pointers.\n-    SetVector<Value> withSameOrder;\n-    withSameOrder.insert(ptr);\n+    llvm::SmallSetVector<Operation *, 32> memAccessesSameOrder;\n     if (refType.isa<RankedTensorType>() && ptr.getDefiningOp()) {\n       for (Operation *op : mlir::multiRootGetSlice(ptr.getDefiningOp())) {\n-        for (Value val : op->getResults()) {\n-          if (!matchesOrder(val))\n-            continue;\n-          auto currOrder =\n-              argSort(axisInfoAnalysis.getAxisInfo(val)->getContiguity());\n-          if (order == currOrder)\n-            withSameOrder.insert(val);\n+        Value val = getMemAccessPtr(op);\n+        if (!val)\n+          continue;\n+        if (!matchesShape(val))\n+          continue;\n+        auto currOrder =\n+            argSort(axisInfoAnalysis.getAxisInfo(val)->getContiguity());\n+        if (order == currOrder) {\n+          memAccessesSameOrder.insert(op);\n         }\n       }\n     }\n@@ -134,8 +151,8 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n \n     // Thread tile size depends on memory alignment\n     SmallVector<unsigned, 4> sizePerThread(refTensorType.getRank(), 1);\n-    unsigned perThread = 1;\n-    for (Value val : withSameOrder) {\n+    auto getNumElementPerThread = [&](Operation *op) {\n+      Value val = getMemAccessPtr(op);\n       auto valInfo = queryAxisInfo(val);\n       unsigned elemNumBits = getElementBitWidth(val);\n       unsigned elemNumBytes = std::max(elemNumBits / 8, 1u);\n@@ -145,8 +162,18 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n           std::min(valInfo.getContiguity(order[0]), shapePerCTA[order[0]]);\n       unsigned alignment = std::min(maxMultiple, maxContig);\n       unsigned currPerThread = std::min(alignment, 128 / elemNumBits);\n+      return currPerThread;\n+    };\n+    unsigned perThread = getNumElementPerThread(memAccess);\n+    for (Operation *op : memAccessesSameOrder) {\n+      unsigned currPerThread = getNumElementPerThread(op);\n       perThread = std::max(perThread, currPerThread);\n     }\n+    // For loads we only consider other loads connected. For other ops we try to\n+    // get the minimum to all the loads and the native num elements supported.\n+    if (!isa<triton::LoadOp>(memAccess)) {\n+      perThread = std::min(perThread, getNumElementPerThread(memAccess));\n+    }\n     sizePerThread[order[0]] = std::min<int>(perThread, numElemsPerThread);\n \n     auto CTALayout = triton::gpu::getCTALayout(refTensorType.getEncoding());\n@@ -156,28 +183,26 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n   }\n \n   std::function<Type(Type)>\n-  getTypeConverter(ModuleAxisInfoAnalysis &axisInfoAnalysis, Value ptr,\n-                   int numWarps, int threadsPerWarp) {\n-    Attribute encoding =\n-        getCoalescedEncoding(axisInfoAnalysis, ptr, numWarps, threadsPerWarp);\n+  getTypeConverter(ModuleAxisInfoAnalysis &axisInfoAnalysis,\n+                   Operation *memAccess, int numWarps, int threadsPerWarp) {\n+    Attribute encoding = getCoalescedEncoding(axisInfoAnalysis, memAccess,\n+                                              numWarps, threadsPerWarp);\n     return [encoding](Type type) {\n       RankedTensorType tensorType = type.cast<RankedTensorType>();\n       return RankedTensorType::get(tensorType.getShape(),\n                                    tensorType.getElementType(), encoding);\n     };\n   }\n \n-  template <class T>\n-  void coalesceOp(LayoutMap &layoutMap, Operation *op, Value ptr,\n-                  OpBuilder builder) {\n-    if (!layoutMap.count(ptr))\n+  void coalesceOp(LayoutMap &layoutMap, Operation *op, OpBuilder builder) {\n+    if (!layoutMap.count(op))\n       return;\n \n     // Convert operands\n     // For load/store with tensor pointers, we don't have to change the\n     // operands' type, we do this by changing the outputs' type of\n     // `make_tensor_ptr`\n-    auto convertType = layoutMap.lookup(ptr);\n+    auto convertType = layoutMap.lookup(op);\n     SmallVector<Value, 4> newArgs;\n     for (auto operand : op->getOperands()) {\n       auto tensorType = operand.getType().dyn_cast<RankedTensorType>();\n@@ -192,13 +217,14 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     // Convert output types\n     SmallVector<Type, 4> newTypes;\n     for (auto t : op->getResultTypes()) {\n-      bool isAsync = std::is_same<T, triton::gpu::InsertSliceAsyncOp>::value;\n+      bool isAsync = isa<triton::gpu::InsertSliceAsyncOp>(op);\n       newTypes.push_back(isAsync ? t : convertType(t));\n     }\n \n     // Construct new op with the new encoding\n     Operation *newOp =\n-        builder.create<T>(op->getLoc(), newTypes, newArgs, op->getAttrs());\n+        builder.create(op->getLoc(), op->getName().getIdentifier(), newArgs,\n+                       newTypes, op->getAttrs());\n \n     // Cast the results back to the original layout\n     for (size_t i = 0; i < op->getNumResults(); i++) {\n@@ -212,25 +238,6 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     op->erase();\n   }\n \n-  void coalesceMakeTensorPtrOpResult(LayoutMap &layoutMap, Operation *op,\n-                                     Value ptr, OpBuilder builder) {\n-    if (!layoutMap.count(ptr))\n-      return;\n-\n-    // Convert result type\n-    auto convertType = layoutMap.lookup(ptr);\n-    auto ptrType = ptr.getType().cast<PointerType>();\n-    auto resultTensorType = convertType(ptrType.getPointeeType());\n-    auto newResultType =\n-        PointerType::get(resultTensorType, ptrType.getAddressSpace());\n-\n-    // Build new operation and replace\n-    Operation *newOp = builder.create<MakeTensorPtrOp>(\n-        op->getLoc(), newResultType, op->getOperands(), op->getAttrs());\n-    op->getResult(0).replaceAllUsesWith(newOp->getResult(0));\n-    op->erase();\n-  }\n-\n   void runOnOperation() override {\n     // Run axis info analysis\n     ModuleOp moduleOp = getOperation();\n@@ -240,17 +247,7 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     // the pointers should have for best memory coalescing\n     LayoutMap layoutMap;\n     moduleOp.walk([&](Operation *curr) {\n-      Value ptr;\n-      if (auto op = dyn_cast<triton::LoadOp>(curr))\n-        ptr = op.getPtr();\n-      if (auto op = dyn_cast<triton::AtomicRMWOp>(curr))\n-        ptr = op.getPtr();\n-      if (auto op = dyn_cast<triton::AtomicCASOp>(curr))\n-        ptr = op.getPtr();\n-      if (auto op = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr))\n-        ptr = op.getSrc();\n-      if (auto op = dyn_cast<triton::StoreOp>(curr))\n-        ptr = op.getPtr();\n+      Value ptr = getMemAccessPtr(curr);\n       if (!ptr)\n         return;\n       // We only convert `tensor<tt.ptr<>>` or `tt.ptr<tensor<>>` load/store\n@@ -266,8 +263,8 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n       int threadsPerWarp =\n           triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n       auto convertType =\n-          getTypeConverter(axisInfoAnalysis, ptr, numWarps, threadsPerWarp);\n-      layoutMap[ptr] = convertType;\n+          getTypeConverter(axisInfoAnalysis, curr, numWarps, threadsPerWarp);\n+      layoutMap[curr] = convertType;\n     });\n \n     // For each memory op that has a layout L1:\n@@ -279,27 +276,9 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     // 5. Replace all the uses of the original memory op by the new one\n     moduleOp.walk([&](Operation *curr) {\n       OpBuilder builder(curr);\n-      if (auto load = dyn_cast<triton::LoadOp>(curr)) {\n-        coalesceOp<triton::LoadOp>(layoutMap, curr, load.getPtr(), builder);\n-        return;\n-      }\n-      if (auto op = dyn_cast<triton::AtomicRMWOp>(curr)) {\n-        coalesceOp<triton::AtomicRMWOp>(layoutMap, curr, op.getPtr(), builder);\n-        return;\n-      }\n-      if (auto op = dyn_cast<triton::AtomicCASOp>(curr)) {\n-        coalesceOp<triton::AtomicCASOp>(layoutMap, curr, op.getPtr(), builder);\n-        return;\n-      }\n-      if (auto load = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr)) {\n-        coalesceOp<triton::gpu::InsertSliceAsyncOp>(layoutMap, curr,\n-                                                    load.getSrc(), builder);\n-        return;\n-      }\n-      if (auto store = dyn_cast<triton::StoreOp>(curr)) {\n-        coalesceOp<triton::StoreOp>(layoutMap, curr, store.getPtr(), builder);\n-        return;\n-      }\n+      if (isa<triton::LoadOp, triton::AtomicRMWOp, triton::AtomicCASOp,\n+              triton::gpu::InsertSliceAsyncOp, triton::StoreOp>(curr))\n+        coalesceOp(layoutMap, curr, builder);\n     });\n   }\n };"}, {"filename": "test/TritonGPU/coalesce.mlir", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "file_content_changes": "@@ -75,9 +75,9 @@ tt.func @load_tensor(%arg0: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg1:\n #blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n module attributes {\"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n \n-// CHECK-NOT: sizePerThread = [4]\n-// CHECK: #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n-// CHECK-NOT: sizePerThread = [4]\n+// CHECK: #[[BLOCKED:.+]] = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+// CHECK: #[[BLOCKED1:.+]] = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+\n tt.func public @load_tensors_two_types(%arg0: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg3: i32) attributes {noinline = false} {\n     %c1024_i32 = arith.constant 1024 : i32\n     %0 = tt.get_program_id x : i32\n@@ -89,14 +89,17 @@ tt.func public @load_tensors_two_types(%arg0: !tt.ptr<f32, 1> {tt.divisibility =\n     %6 = \"triton_gpu.cmpi\"(%4, %5) <{predicate = 2 : i64}> : (tensor<1024xi32, #blocked>, tensor<1024xi32, #blocked>) -> tensor<1024xi1, #blocked>\n     %7 = tt.splat %arg0 : (!tt.ptr<f32, 1>) -> tensor<1024x!tt.ptr<f32, 1>, #blocked>\n     %8 = tt.addptr %7, %4 : tensor<1024x!tt.ptr<f32, 1>, #blocked>, tensor<1024xi32, #blocked>\n+    // CHECK: tt.load %{{.*}} : tensor<1024xf32, #[[BLOCKED]]>\n     %9 = tt.load %8, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked>\n     %10 = tt.splat %arg1 : (!tt.ptr<f16, 1>) -> tensor<1024x!tt.ptr<f16, 1>, #blocked>\n     %11 = tt.addptr %10, %4 : tensor<1024x!tt.ptr<f16, 1>, #blocked>, tensor<1024xi32, #blocked>\n+    // CHECK: tt.load %{{.*}} : tensor<1024xf16, #[[BLOCKED]]>\n     %12 = tt.load %11, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf16, #blocked>\n     %13 = arith.extf %12 : tensor<1024xf16, #blocked> to tensor<1024xf32, #blocked>\n     %14 = arith.addf %9, %13 : tensor<1024xf32, #blocked>\n     %15 = tt.splat %arg2 : (!tt.ptr<f32, 1>) -> tensor<1024x!tt.ptr<f32, 1>, #blocked>\n     %16 = tt.addptr %15, %4 : tensor<1024x!tt.ptr<f32, 1>, #blocked>, tensor<1024xi32, #blocked>\n+    // CHECK: tt.store {{.*}} : tensor<1024xf32, #[[BLOCKED1]]>\n     tt.store %16, %14, %6 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32, #blocked>\n     tt.return\n }"}]