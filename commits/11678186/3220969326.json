[{"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "file_content_changes": "@@ -7,7 +7,7 @@\n import os\n import subprocess\n import textwrap\n-from collections import namedtuple\n+from collections import defaultdict, namedtuple\n \n import torch\n \n@@ -252,7 +252,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     if stream is None and not warmup:\n       stream = get_cuda_stream(device)\n     try:\n-      bin = cache[key]\n+      bin = cache[device][key]\n       if not warmup:\n           bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, triton.compiler.CompiledKernel.launch_enter_hook, triton.compiler.CompiledKernel.launch_exit_hook, bin, {args})\n       return bin\n@@ -271,12 +271,11 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n       for i, arg in constants.items():\n         if callable(arg):\n           raise TypeError(f\"Callable constexpr at index {i} is not supported\")\n-      device = 0\n       if not self._call_hook(key, signature, device, constants, num_warps, num_stages, extern_libs, configs):\n         bin = triton.compile(self, signature, device, constants, num_warps, num_stages, extern_libs=extern_libs, configs=configs)\n         if not warmup:\n             bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, triton.compiler.CompiledKernel.launch_enter_hook, triton.compiler.CompiledKernel.launch_exit_hook, bin, *args)\n-        self.cache[key] = bin\n+        self.cache[device][key] = bin\n         return bin\n       return None\n \"\"\"\n@@ -301,7 +300,7 @@ def __init__(self, fn, version=None, do_not_specialize=None):\n         self.src = textwrap.dedent(inspect.getsource(fn))\n         self.src = self.src[self.src.find(\"def\"):]\n         # cache of just-in-time compiled kernels\n-        self.cache = dict()\n+        self.cache = defaultdict(dict)\n         self.hash = None\n         # JITFunction can be instantiated as kernel\n         # when called with a grid using __getitem__"}, {"filename": "python/tutorials/01-vector-add.py", "status": "modified", "additions": 35, "deletions": 35, "changes": 70, "file_content_changes": "@@ -3,9 +3,9 @@\n =================\n In this tutorial, you will write a simple vector addition using Triton and learn about:\n \n-- The basic programming model of Triton\n+- The basic programming model of Triton.\n - The `triton.jit` decorator, which is used to define Triton kernels.\n-- The best practices for validating and benchmarking your custom ops against native reference implementations\n+- The best practices for validating and benchmarking your custom ops against native reference implementations.\n \"\"\"\n \n # %%\n@@ -20,51 +20,51 @@\n \n @triton.jit\n def add_kernel(\n-    x_ptr,  # *Pointer* to first input vector\n-    y_ptr,  # *Pointer* to second input vector\n-    output_ptr,  # *Pointer* to output vector\n-    n_elements,  # Size of the vector\n-    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n-                 # NOTE: `constexpr` so it can be used as a shape value\n+    x_ptr,  # *Pointer* to first input vector.\n+    y_ptr,  # *Pointer* to second input vector.\n+    output_ptr,  # *Pointer* to output vector.\n+    n_elements,  # Size of the vector.\n+    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n+                 # NOTE: `constexpr` so it can be used as a shape value.\n ):\n-    # There are multiple 'program's processing different data. We identify which program\n-    # we are here\n-    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0\n+    # There are multiple 'programs' processing different data. We identify which program\n+    # we are here:\n+    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n     # This program will process inputs that are offset from the initial data.\n-    # for instance, if you had a vector of length 256 and block_size of 64, the programs\n+    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n     # would each access the elements [0:64, 64:128, 128:192, 192:256].\n-    # Note that offsets is a list of pointers\n+    # Note that offsets is a list of pointers:\n     block_start = pid * BLOCK_SIZE\n     offsets = block_start + tl.arange(0, BLOCK_SIZE)\n-    # Create a mask to guard memory operations against out-of-bounds accesses\n+    # Create a mask to guard memory operations against out-of-bounds accesses.\n     mask = offsets < n_elements\n     # Load x and y from DRAM, masking out any extra elements in case the input is not a\n-    # multiple of the block size\n+    # multiple of the block size.\n     x = tl.load(x_ptr + offsets, mask=mask)\n     y = tl.load(y_ptr + offsets, mask=mask)\n     output = x + y\n-    # Write x + y back to DRAM\n+    # Write x + y back to DRAM.\n     tl.store(output_ptr + offsets, output, mask=mask)\n \n \n # %%\n # Let's also declare a helper function to (1) allocate the `z` tensor\n-# and (2) enqueue the above kernel with appropriate grid/block sizes.\n+# and (2) enqueue the above kernel with appropriate grid/block sizes:\n \n \n def add(x: torch.Tensor, y: torch.Tensor):\n-    # We need to preallocate the output\n+    # We need to preallocate the output.\n     output = torch.empty_like(x)\n     assert x.is_cuda and y.is_cuda and output.is_cuda\n     n_elements = output.numel()\n     # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n-    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int]\n-    # In this case, we use a 1D grid where the size is the number of blocks\n+    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n+    # In this case, we use a 1D grid where the size is the number of blocks:\n     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     # NOTE:\n-    #  - each torch.tensor object is implicitly converted into a pointer to its first element.\n-    #  - `triton.jit`'ed functions can be index with a launch grid to obtain a callable GPU kernel\n-    #  - don't forget to pass meta-parameters as keywords arguments\n+    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n+    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n+    #  - Don't forget to pass meta-parameters as keywords arguments.\n     add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n     # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n     # running asynchronously at this point.\n@@ -94,24 +94,24 @@ def add(x: torch.Tensor, y: torch.Tensor):\n # Benchmark\n # -----------\n # We can now benchmark our custom op on vectors of increasing sizes to get a sense of how it does relative to PyTorch.\n-# To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of your custom ops\n+# To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of your custom ops.\n # for different problem sizes.\n \n \n @triton.testing.perf_report(\n     triton.testing.Benchmark(\n-        x_names=['size'],  # argument names to use as an x-axis for the plot\n+        x_names=['size'],  # Argument names to use as an x-axis for the plot.\n         x_vals=[\n             2 ** i for i in range(12, 28, 1)\n-        ],  # different possible values for `x_name`\n-        x_log=True,  # x axis is logarithmic\n-        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n-        line_vals=['triton', 'torch'],  # possible values for `line_arg`\n-        line_names=['Triton', 'Torch'],  # label name for the lines\n-        styles=[('blue', '-'), ('green', '-')],  # line styles\n-        ylabel='GB/s',  # label name for the y-axis\n-        plot_name='vector-add-performance',  # name for the plot. Used also as a file name for saving the plot.\n-        args={},  # values for function arguments not in `x_names` and `y_name`\n+        ],  # Different possible values for `x_name`.\n+        x_log=True,  # x axis is logarithmic.\n+        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n+        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\n+        line_names=['Triton', 'Torch'],  # Label name for the lines.\n+        styles=[('blue', '-'), ('green', '-')],  # Line styles.\n+        ylabel='GB/s',  # Label name for the y-axis.\n+        plot_name='vector-add-performance',  # Name for the plot. Used also as a file name for saving the plot.\n+        args={},  # Values for function arguments not in `x_names` and `y_name`.\n     )\n )\n def benchmark(size, provider):\n@@ -127,5 +127,5 @@ def benchmark(size, provider):\n \n # %%\n # We can now run the decorated function above. Pass `print_data=True` to see the performance number, `show_plots=True` to plot them, and/or\n-# `save_path='/path/to/results/' to save them to disk along with raw CSV data\n+# `save_path='/path/to/results/' to save them to disk along with raw CSV data:\n benchmark.run(print_data=True, show_plots=True)"}]