[{"filename": "python/tutorials/02-fused-softmax.py", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -110,7 +110,7 @@ def softmax(x):\n     y = torch.empty_like(x)\n     # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o\n     # f the input matrix\n-    pgm = softmax_kernel[(n_rows,)](\n+    softmax_kernel[(n_rows,)](\n         y,\n         x,\n         x.stride(0),\n@@ -119,7 +119,6 @@ def softmax(x):\n         num_warps=num_warps,\n         BLOCK_SIZE=BLOCK_SIZE,\n     )\n-    print(pgm.asm[\"ttgir\"])\n     return y\n \n \n@@ -132,7 +131,7 @@ def softmax(x):\n # This will allow us to verify that our padding mechanism works.\n \n torch.manual_seed(0)\n-x = torch.randn(1024, 1024, device='cuda')\n+x = torch.randn(1823, 781, device='cuda')\n y_triton = softmax(x)\n y_torch = torch.softmax(x, axis=1)\n assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)\n@@ -182,7 +181,7 @@ def benchmark(M, N, provider):\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n \n \n-benchmark.run(show_plots=False, print_data=True)\n+benchmark.run(show_plots=True, print_data=True)\n \n # %%\n # In the above plot, we can see that:"}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -144,7 +144,7 @@ def forward(ctx, x, normalized_shape, weight, bias, eps):\n         # heuristics for number of warps\n         num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n         # enqueue kernel\n-        pgm = _layer_norm_fwd_fused[(M,)](x_arg, y, weight, bias, mean, rstd,\n+        _layer_norm_fwd_fused[(M,)](x_arg, y, weight, bias, mean, rstd,\n                                     x_arg.stride(0), N, eps,\n                                     BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n         ctx.save_for_backward(x, weight, bias, mean, rstd)\n@@ -173,14 +173,14 @@ def backward(ctx, dy):\n         # also compute partial sums for DW and DB\n         x_arg = x.reshape(-1, x.shape[-1])\n         M, N = x_arg.shape\n-        pgm = _layer_norm_bwd_dx_fused[(M,)](dx, dy, _dw, _db, x, w, b, m, v, locks,\n+        _layer_norm_bwd_dx_fused[(M,)](dx, dy, _dw, _db, x, w, b, m, v, locks,\n                                        x_arg.stride(0), N, ctx.eps,\n                                        BLOCK_SIZE_N=ctx.BLOCK_SIZE,\n                                        GROUP_SIZE_M=GROUP_SIZE_M,\n                                        num_warps=ctx.num_warps)\n         grid = lambda meta: [triton.cdiv(N, meta['BLOCK_SIZE_N'])]\n         # accumulate partial sums in separate kernel\n-        pgm = _layer_norm_bwd_dwdb[grid](_dw, _db, dw, db, GROUP_SIZE_M, N,\n+        _layer_norm_bwd_dwdb[grid](_dw, _db, dw, db, GROUP_SIZE_M, N,\n                                    BLOCK_SIZE_M=32,\n                                    BLOCK_SIZE_N=128)\n         return dx, None, dw, db, None\n@@ -258,5 +258,5 @@ def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='c\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n \n \n-test_layer_norm(1151, 8192, torch.float16)\n-# bench_layer_norm.run(save_path='.', print_data=True)\n+# test_layer_norm(1151, 8192, torch.float16)\n+bench_layer_norm.run(save_path='.', print_data=True)"}]