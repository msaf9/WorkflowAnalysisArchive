[{"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -142,6 +142,7 @@ def warmup(self, *args, **kwargs):\n class Config:\n     \"\"\"\n     An object that represents a possible kernel configuration for the auto-tuner to try.\n+\n     :ivar meta: a dictionary of meta-parameters to pass to the kernel as keyword arguments.\n     :type meta: dict[Str, Any]\n     :ivar num_warps: the number of warps to use for the kernel when compiled for GPUs. For example, if\n@@ -173,8 +174,10 @@ def __str__(self):\n def autotune(configs, key, prune_configs_by=None, reset_to_zero=None):\n     \"\"\"\n     Decorator for auto-tuning a :code:`triton.jit`'d function.\n+\n     .. highlight:: python\n     .. code-block:: python\n+\n         @triton.autotune(configs=[\n             triton.Config(meta={'BLOCK_SIZE': 128}, num_warps=4),\n             triton.Config(meta={'BLOCK_SIZE': 1024}, num_warps=8),\n@@ -223,8 +226,10 @@ def heuristics(values):\n     \"\"\"\n     Decorator for specifying how the values of certain meta-parameters may be computed.\n     This is useful for cases where auto-tuning is prohibitevely expensive, or just not applicable.\n+\n     .. highlight:: python\n     .. code-block:: python\n+\n         @triton.heuristics(values={'BLOCK_SIZE': lambda args: 2 ** int(math.ceil(math.log2(args[1])))})\n         @triton.jit\n         def kernel(x_ptr, x_size, **META):"}]