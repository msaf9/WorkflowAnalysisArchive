[{"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 31, "deletions": 0, "changes": 31, "file_content_changes": "@@ -469,6 +469,36 @@ class SplatOpAxisInfoVisitor final\n   }\n };\n \n+class LoadOpAxisInfoVisitor final : public AxisInfoVisitorImpl<triton::LoadOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::LoadOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(triton::LoadOp op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    // If pointers and mask both have constancy properties, those properties\n+    // will also extend to output.\n+    AxisInfo ptrInfo = operands[0]->getValue();\n+    std::optional<AxisInfo> maskInfo;\n+    if (operands.size() > 1) {\n+      maskInfo = operands[1]->getValue();\n+    }\n+    AxisInfo::DimVectorT contiguity;\n+    AxisInfo::DimVectorT divisibility;\n+    AxisInfo::DimVectorT constancy;\n+\n+    for (int d = 0; d < ptrInfo.getRank(); ++d) {\n+      contiguity.push_back(1);\n+      divisibility.push_back(1);\n+      constancy.push_back(\n+          gcd(ptrInfo.getConstancy(d),\n+              maskInfo.has_value() ? maskInfo->getConstancy(d) : 0));\n+    }\n+\n+    return AxisInfo(contiguity, divisibility, constancy);\n+  }\n+};\n+\n class ExpandDimsOpAxisInfoVisitor final\n     : public AxisInfoVisitorImpl<triton::ExpandDimsOp> {\n public:\n@@ -871,6 +901,7 @@ AxisInfoAnalysis::AxisInfoAnalysis(DataFlowSolver &solver)\n                   MaxMinOpAxisInfoVisitor<arith::MaxUIOp>,\n                   MaxMinOpAxisInfoVisitor<arith::MinSIOp>,\n                   MaxMinOpAxisInfoVisitor<arith::MinUIOp>>();\n+  visitors.append<LoadOpAxisInfoVisitor>();\n }\n \n void AxisInfoAnalysis::visitOperation("}, {"filename": "python/setup.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -125,8 +125,8 @@ def download_and_copy_ptxas():\n \n     base_dir = os.path.dirname(__file__)\n     src_path = \"bin/ptxas\"\n-    version = \"12.2.91\"\n-    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.2.0/linux-64/cuda-nvcc-{version}-0.tar.bz2\"\n+    version = \"12.1.105\"\n+    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.1.1/linux-64/cuda-nvcc-{version}-0.tar.bz2\"\n     dst_prefix = os.path.join(base_dir, \"triton\")\n     dst_suffix = os.path.join(\"third_party\", \"cuda\", src_path)\n     dst_path = os.path.join(dst_prefix, dst_suffix)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 8, "deletions": 19, "changes": 27, "file_content_changes": "@@ -1297,7 +1297,7 @@ def deserialize_fp8(np_data, in_dtype):\n \n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n-@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16, torch.float32])\n+@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.float32])\n def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     \"\"\"\n     For all possible float8 values (ref_fp8 = range(0, 256)), test that:\n@@ -1306,13 +1306,6 @@ def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     this is only possible if both conversions are correct\n     \"\"\"\n     check_type_supported(out_dtype, device)\n-    from contextlib import nullcontext as does_not_raise\n-    expectation = does_not_raise()\n-    err_msg = None\n-    if (in_dtype == tl.float8e4b15 and out_dtype != torch.float16) or\\\n-       (in_dtype != torch.float16 and out_dtype == tl.float8e4b15):\n-        expectation = pytest.raises(triton.CompilationError)\n-        err_msg = \"fp8e4b15 can only be converted to/from fp16\"\n \n     @triton.jit\n     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n@@ -1331,19 +1324,15 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     ref_fp8[is_subnormal] = 0\n     tri_fp8 = torch.from_numpy(serialize_fp8(ref_fp8, in_dtype)).cuda()\n     tri_fp16 = torch.empty(256, dtype=out_dtype, device=\"cuda\")\n-    with expectation as e:\n-        copy_kernel[(1,)](triton.reinterpret(tri_fp8, in_dtype), tri_fp16, tri_fp16.shape[0], BLOCK_SIZE=1024)\n+    copy_kernel[(1,)](triton.reinterpret(tri_fp8, in_dtype), tri_fp16, tri_fp16.shape[0], BLOCK_SIZE=1024)\n \n-        ref_fp8 = torch.from_numpy(ref_fp8).cuda()\n-        ref_fp16 = convert_float_to_float32(ref_fp8, in_dtype)\n-        assert torch.all(tri_fp16[~is_subnormal] == ref_fp16[~is_subnormal])\n+    ref_fp8 = torch.from_numpy(ref_fp8).cuda()\n+    ref_fp16 = convert_float_to_float32(ref_fp8, in_dtype)\n+    assert torch.all(tri_fp16[~is_subnormal] == ref_fp16[~is_subnormal])\n \n-        ref_fp8 = torch.empty_like(tri_fp16, dtype=torch.int8)\n-        copy_kernel[(1,)](tri_fp16, triton.reinterpret(ref_fp8, in_dtype), tri_fp16.shape[0], BLOCK_SIZE=1024)\n-        assert torch.all(tri_fp8 == ref_fp8)\n-\n-    if err_msg is not None:\n-        assert err_msg in str(e)\n+    ref_fp8 = torch.empty_like(tri_fp16, dtype=torch.int8)\n+    copy_kernel[(1,)](tri_fp16, triton.reinterpret(ref_fp8, in_dtype), tri_fp16.shape[0], BLOCK_SIZE=1024)\n+    assert torch.all(tri_fp8 == ref_fp8)\n \n # ---------------\n # test reduce"}, {"filename": "python/triton/common/backend.py", "status": "modified", "additions": 23, "deletions": 0, "changes": 23, "file_content_changes": "@@ -1,6 +1,10 @@\n \n+import functools\n import importlib\n import importlib.util\n+import os\n+import re\n+import subprocess\n from typing import Dict\n \n from ..runtime.driver import DriverBase\n@@ -94,3 +98,22 @@ def get_backend(device_type: str):\n         else:\n             return None\n     return _backends[device_type] if device_type in _backends else None\n+\n+\n+@functools.lru_cache()\n+def path_to_ptxas():\n+    base_dir = os.path.join(os.path.dirname(__file__), os.pardir)\n+    paths = [\n+        os.environ.get(\"TRITON_PTXAS_PATH\", \"\"),\n+        os.path.join(base_dir, \"third_party\", \"cuda\", \"bin\", \"ptxas\")\n+    ]\n+\n+    for ptxas in paths:\n+        ptxas_bin = ptxas.split(\" \")[0]\n+        if os.path.exists(ptxas_bin) and os.path.isfile(ptxas_bin):\n+            result = subprocess.check_output([ptxas_bin, \"--version\"], stderr=subprocess.STDOUT)\n+            if result is not None:\n+                version = re.search(r\".*release (\\d+\\.\\d+).*\", result.decode(\"utf-8\"), flags=re.MULTILINE)\n+                if version is not None:\n+                    return ptxas, version.group(1)\n+    raise RuntimeError(\"Cannot find ptxas\")"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 1, "deletions": 20, "changes": 21, "file_content_changes": "@@ -15,7 +15,7 @@\n                                    get_shared_memory_size, ir,\n                                    translate_llvmir_to_hsaco, translate_llvmir_to_ptx,\n                                    translate_triton_gpu_to_llvmir)\n-from ..common.backend import get_backend\n+from ..common.backend import get_backend, path_to_ptxas\n # from ..runtime import driver, jit, JITFunction\n # TODO: runtime.errors\n from ..runtime.autotuner import OutOfResources\n@@ -127,25 +127,6 @@ def ptx_get_version(cuda_version) -> int:\n     raise RuntimeError(\"Triton only support CUDA 10.0 or higher\")\n \n \n-@functools.lru_cache()\n-def path_to_ptxas():\n-    base_dir = os.path.join(os.path.dirname(__file__), os.pardir)\n-    paths = [\n-        os.environ.get(\"TRITON_PTXAS_PATH\", \"\"),\n-        os.path.join(base_dir, \"third_party\", \"cuda\", \"bin\", \"ptxas\")\n-    ]\n-\n-    for ptxas in paths:\n-        ptxas_bin = ptxas.split(\" \")[0]\n-        if os.path.exists(ptxas_bin) and os.path.isfile(ptxas_bin):\n-            result = subprocess.check_output([ptxas_bin, \"--version\"], stderr=subprocess.STDOUT)\n-            if result is not None:\n-                version = re.search(r\".*release (\\d+\\.\\d+).*\", result.decode(\"utf-8\"), flags=re.MULTILINE)\n-                if version is not None:\n-                    return ptxas, version.group(1)\n-    raise RuntimeError(\"Cannot find ptxas\")\n-\n-\n def llir_to_ptx(mod: Any, arch: int, ptx_version: int = None) -> str:\n     '''\n     Translate TritonGPU module to PTX code."}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 0, "deletions": 5, "changes": 5, "file_content_changes": "@@ -687,11 +687,6 @@ def cast(input: tl.tensor,\n         warnings.warn(\"Standard tl.float8e4 format will be deprecated on SM < 89. \"\n                       \"Please use tl.float8e4b15.\", DeprecationWarning)\n \n-    # Unsupported conversion:\n-    if (src_sca_ty.is_fp8e4b15() and not dst_sca_ty.is_fp16()) or \\\n-       (dst_sca_ty.is_fp8e4b15() and not src_sca_ty.is_fp16()):\n-        raise ValueError('fp8e4b15 can only be converted to/from fp16')\n-\n     # Casting with customized floating types involved: fp8 <=> bf16, fp16, fp32, fp64\n     if (src_sca_ty.is_fp8() and dst_sca_ty.is_floating()) or \\\n        (src_sca_ty.is_floating() and dst_sca_ty.is_fp8()):"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "file_content_changes": "@@ -11,7 +11,7 @@\n from typing import (Callable, Generic, Iterable, List, Optional, TypeVar, Union, cast,\n                     overload)\n \n-from ..common.backend import get_backend\n+from ..common.backend import get_backend, path_to_ptxas\n \n TRITON_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n TRITON_VERSION = \"2.1.0\"\n@@ -117,10 +117,8 @@ def version_key():\n         with open(lib.module_finder.find_spec(lib.name).origin, \"rb\") as f:\n             contents += [hashlib.md5(f.read()).hexdigest()]\n     # ptxas version\n-    try:\n-        ptxas_version = hashlib.md5(subprocess.check_output([\"ptxas\", \"--version\"])).hexdigest()\n-    except Exception:\n-        ptxas_version = ''\n+    ptxas = path_to_ptxas()[0]\n+    ptxas_version = hashlib.md5(subprocess.check_output([ptxas, \"--version\"])).hexdigest()\n     return '-'.join(TRITON_VERSION) + '-' + ptxas_version + '-' + '-'.join(contents)\n \n "}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 29, "deletions": 0, "changes": 29, "file_content_changes": "@@ -402,6 +402,35 @@ tt.func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32\n \n // -----\n \n+// CHECK-LABEL: @load_constancy\n+tt.func @load_constancy(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 1 : i32}) {\n+  // CHECK: divisibility = [16]\n+  %sixteen = arith.constant dense<16> : tensor<1024xi32>\n+  // CHECK-NEXT: divisibility = [8]\n+  %eight = arith.constant dense<8> : tensor<1024xi32>\n+  // CHECK-NEXT: contiguity = [1024], divisibility = [1073741824], constancy = [1]\n+  %1 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>\n+  // CHECK-NEXT: constancy = [16]\n+  %2 = arith.divsi %1, %sixteen : tensor<1024xi32>\n+  // CHECK-NEXT: constancy = [1024]\n+  %3 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+  // CHECK-NEXT: constancy = [1024]\n+  %4 = tt.splat %arg1 : (i32) -> tensor<1024xi32>\n+  // CHECK-NEXT: constancy = [8]\n+  %5 = arith.divsi %1, %eight : tensor<1024xi32>\n+  // CHECK-NEXT: constancy = [8]\n+  %6 = arith.cmpi slt, %5, %4 : tensor<1024xi32>\n+  // CHECK-NEXT: constancy = [16]\n+  %7 = tt.addptr %3, %2 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+  // CHECK-NEXT: constancy = [16]\n+  %8 = tt.load %7 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n+  // CHECK-NEXT: constancy = [8]\n+  %9 = tt.load %7, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n+  tt.return\n+}\n+\n+// -----\n+\n // This is a tiny test for verifying StoreOp-related alignment, It simply store a constant to a buffer.\n // CHECK-LABEL: @store_constant_align\n tt.func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n: i32 {tt.divisibility = 16 : i32}) {"}]