[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 12, "deletions": 12, "changes": 24, "file_content_changes": "@@ -52,7 +52,7 @@ different cuda threads in the programs, via shared memory. In other words,\n for all indices i \\in R^d, \\mathcal{L}(i) = {0, 1, ..., 32*num_warps - 1}.\n \n In order to avoid shared memory bank conflicts, elements may be swizzled\n-in memory. For example, a swizzled row-major layout could store its data \n+in memory. For example, a swizzled row-major layout could store its data\n as follows:\n \n A_{0, 0}  A_{0, 1}  A_{0, 2}  A_{0, 3} ...   [phase 0] \\ per_phase = 2\n@@ -215,9 +215,9 @@ def MmaEncodingAttr : DistributedEncoding<\"MmaEncoding\"> {\n An encoding for tensors that have been produced by tensor cores.\n It is characterized by two parameters:\n - A 'version' which specifies the generation the tensor cores\n-whose output is being partitioned: 1 for first-gen tensor cores (Volta), \n+whose output is being partitioned: 1 for first-gen tensor cores (Volta),\n and 2 for second-gen tensor cores (Turing/Ampere).\n-- A `blockTileSize` to indicate how data should be \n+- A `blockTileSize` to indicate how data should be\n partitioned between warps.\n \n // -------------------------------- version = 1 --------------------------- //\n@@ -229,7 +229,7 @@ https://docs.nvidia.com/cuda/parallel-thread-execution/index.html\n \n For example, the matrix L corresponding to blockTileSize=[32,16] is:\n \n-                               warp 0              \n+                               warp 0\n --------------------------------/\\-------------------------------\n [ 0   0   2   2   0   0   2   2    4   4   6   6   4   4   6   6 ]\n [ 1   1   3   3   1   1   3   3    5   5   7   7   5   5   7   7 ]\n@@ -246,7 +246,7 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n [ 24  24  26  26  24  24  26  26   28  28  30  30  28  28  30  30]\n [ 25  25  27  27  25  25  27  27   29  29  31  31  29  29  31  31]\n \n-                         warp 1 = warp0 + 32             \n+                         warp 1 = warp0 + 32\n --------------------------------/\\-------------------------------\n [ 32  32  34  34  32  32  34  34   36  36  38  38  36  36  38  38]\n [ 33  33  35  35  33  33  35  35   37  37  39  39  37  37  39  39]\n@@ -260,29 +260,29 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n For second-gen tensor cores, the implicit warpTileSize is [16, 8].\n Information about this layout can be found in the official PTX documentation\n https://docs.nvidia.com/cuda/parallel-thread-execution/index.html\n-(mma.16816 section, FP32 accumulator).             \n+(mma.16816 section, FP32 accumulator).\n \n For example, the matrix L corresponding to blockTileSize=[32,16] is:\n                 warp 0                          warp 1\n -----------------/\\-------------  ----------------/\\-------------\n [ 0   0   1   1   2   2   3   3   32  32  33  33  34  34  35  35\n [ 4   4   5   5   6   6   7   7   36  36  37  37  38  38  39  39\n-[ ..............................  ..............................                      \n+[ ..............................  ..............................\n [ 28  28  29  29  30  30  31  31  60  60  61  61  62  62  63  63\n [ 0   0   1   1   2   2   3   3   32  32  33  33  34  34  35  35\n [ 4   4   5   5   6   6   7   7   36  36  37  37  38  38  39  39\n-[ ..............................  ..............................                \n+[ ..............................  ..............................\n [ 28  28  29  29  30  30  31  31  60  60  61  61  62  62  63  63\n-                \n+\n               warp 3                           warp 4\n ----------------/\\-------------   ----------------/\\-------------\n [ 64  64  65  65  66  66  67  67  96  96  97  97  98  98  99  99\n [ 68  68  69  69  70  70  71  71  100 100 101 101 102 102 103 103\n-[ ..............................  ...............................                   \n+[ ..............................  ...............................\n [ 92  92  93  93  94  94  95  95  124 124 125 125 126 126 127 127\n [ 64  64  65  65  66  66  67  67  96  96  97  97  98  98  99  99\n [ 68  68  69  69  70  70  71  71  100 100 101 101 102 102 103 103\n-[ ..............................  ...............................                   \n+[ ..............................  ...............................\n [ 92  92  93  93  94  94  95  95  124 124 125 125 126 126 127 127\n \n }];\n@@ -316,7 +316,7 @@ def SliceEncodingAttr : DistributedEncoding<\"SliceEncoding\"> {\n     This is useful for constructing the inverse layout of an expand_dims operation during some optimization passes.\n \n   }];\n-  \n+\n   let parameters = (\n     ins\n     \"unsigned\":$dim,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 205, "deletions": 91, "changes": 296, "file_content_changes": "@@ -29,6 +29,7 @@\n using namespace mlir;\n using namespace mlir::triton;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getElemsPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n@@ -1565,13 +1566,18 @@ struct ConvertLayoutOpConversion\n         dstLayout.isa<SharedEncodingAttr>()) {\n       return lowerBlockedToShared(op, adaptor, rewriter);\n     }\n+    if (srcLayout.isa<SharedEncodingAttr>() &&\n+        dstLayout.isa<DotOperandEncodingAttr>()) {\n+      return lowerSharedToDotOperand(op, adaptor, rewriter);\n+    }\n     if ((!srcLayout.isa<BlockedEncodingAttr>() &&\n          !srcLayout.isa<MmaEncodingAttr>()) ||\n         (!dstLayout.isa<BlockedEncodingAttr>() &&\n          !dstLayout.isa<MmaEncodingAttr>())) {\n       // TODO: to be implemented\n       return failure();\n     }\n+\n     return lowerDistributedToDistributed(op, adaptor, rewriter);\n   }\n \n@@ -1609,6 +1615,11 @@ struct ConvertLayoutOpConversion\n   LogicalResult lowerBlockedToShared(triton::gpu::ConvertLayoutOp op,\n                                      OpAdaptor adaptor,\n                                      ConversionPatternRewriter &rewriter) const;\n+\n+  // shared -> mma_operand\n+  LogicalResult\n+  lowerSharedToDotOperand(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+                          ConversionPatternRewriter &rewriter) const;\n };\n \n void ConvertLayoutOpConversion::processReplica(\n@@ -1915,6 +1926,7 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n   rewriter.replaceOp(op, smemBase);\n   return success();\n }\n+\n /// ====================== dot codegen begin ==========================\n \n // Data loader for mma.16816 instruction.\n@@ -2383,16 +2395,16 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n \n private:\n   // Convert to mma.m16n8k16\n-  LogicalResult convertMMA16816(triton::DotOp a, OpAdaptor adapter,\n+  LogicalResult convertMMA16816(triton::DotOp a, OpAdaptor adaptor,\n                                 ConversionPatternRewriter &rewriter) const;\n   /// Convert to mma.m8n8k4\n-  LogicalResult convertMMA884(triton::DotOp op, OpAdaptor adapter,\n+  LogicalResult convertMMA884(triton::DotOp op, OpAdaptor adaptor,\n                               ConversionPatternRewriter &rewriter) const {\n     assert(false && \"Not implemented yet.\");\n     return failure();\n   }\n \n-  LogicalResult convertFMADot(triton::DotOp op, OpAdaptor adapter,\n+  LogicalResult convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n                               ConversionPatternRewriter &rewriter) const {\n     assert(false && \"Not implemented yet.\");\n     return failure();\n@@ -2402,28 +2414,18 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n struct DotOpConversionHelper {\n   using TensorCoreType = DotOpConversion::TensorCoreType;\n \n-  Value A, B, C, D;\n   MmaEncodingAttr mmaLayout;\n-  RankedTensorType ATensorTy, BTensorTy, DTensorTy;\n   MLIRContext *ctx{};\n \n-  explicit DotOpConversionHelper(DotOp dot)\n-      : dot(dot), mmaType(getMmaType(dot)) {\n-    A = dot.a();\n-    B = dot.b();\n-    C = dot.c();\n-    D = dot.d();\n-    ctx = dot->getContext();\n-    mmaLayout = C.getType()\n-                    .cast<RankedTensorType>()\n-                    .getEncoding()\n-                    .cast<MmaEncodingAttr>();\n+  explicit DotOpConversionHelper(MmaEncodingAttr mmaLayout)\n+      : mmaLayout(mmaLayout) {\n+    ctx = mmaLayout.getContext();\n   }\n \n   // Load SplatLike C which contains a constVal. It simply returns 4 fp32\n   // constVal.\n   SmallVector<Value> loadSplatLikeC(Value C, Location loc,\n-                                    ConversionPatternRewriter &rewriter) {\n+                                    ConversionPatternRewriter &rewriter) const {\n     assert(isSplatLike(C));\n \n     int numRes = getMmaInstrShape()[0] * getMmaInstrShape()[1] / 32;\n@@ -2451,6 +2453,11 @@ struct DotOpConversionHelper {\n     return {};\n   }\n \n+  void deduceMmaType(DotOp op) const { mmaType = getMmaType(op); }\n+  void deduceMmaType(Type operandTy) const {\n+    mmaType = getTensorCoreTypeFromOperand(operandTy);\n+  }\n+\n   Type getShemPtrTy() const {\n     switch (mmaType) {\n     case TensorCoreType::FP32_FP16_FP16_FP32:\n@@ -2554,6 +2561,22 @@ struct DotOpConversionHelper {\n     return mmaMatShape.at(mmaType);\n   }\n \n+  // Deduce the TensorCoreType from either $a or $b's type. This method is not\n+  // safe, but we cannot get the DotOp in some getmaMatShape usage case.\n+  TensorCoreType getTensorCoreTypeFromOperand(Type operandTy) const {\n+    auto tensorTy = operandTy.cast<RankedTensorType>();\n+    auto elemTy = tensorTy.getElementType();\n+    if (elemTy.isF16())\n+      return TensorCoreType::FP32_FP16_FP16_FP32;\n+    if (elemTy.isF32())\n+      return TensorCoreType::FP32_TF32_TF32_FP32;\n+    if (elemTy.isBF16())\n+      return TensorCoreType::FP32_BF16_BF16_FP32;\n+    if (elemTy.isInteger(8))\n+      return TensorCoreType::INT32_INT8_INT8_INT32;\n+    return TensorCoreType::NOT_APPLICABLE;\n+  }\n+\n   int getVec() const {\n     assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n            \"Unknown mma type found.\");\n@@ -2593,7 +2616,7 @@ struct DotOpConversionHelper {\n   }\n \n private:\n-  TensorCoreType mmaType;\n+  mutable TensorCoreType mmaType{TensorCoreType::NOT_APPLICABLE};\n \n   // Used on nvidia GPUs mma layout .version == 2\n   // Refer to\n@@ -2655,9 +2678,6 @@ struct DotOpConversionHelper {\n       {TensorCoreType::INT32_INT4_INT4_INT32, 32},\n       {TensorCoreType::INT32_INT8_INT8_INT32, 16},\n   };\n-\n-private:\n-  DotOp dot;\n };\n \n // This class helps to adapt the existing DotOpConversion to the latest\n@@ -2666,86 +2686,88 @@ struct DotOpConversionHelper {\n // 1. loading the specific operand matrix(for $a, $b, $c) from smem\n // 2. passing the loaded value and perform the mma codegen\n struct MMA16816ConversionHelper {\n-  Value A, B, C, D;\n-  RankedTensorType aTensorTy, bTensorTy, dTensorTy;\n-  ArrayRef<int64_t> aShape, bShape, dShape;\n   MmaEncodingAttr mmaLayout;\n   ArrayRef<unsigned int> wpt;\n \n-  int mmaInstrM{-1}, mmaInstrN{-1}, mmaInstrK{-1};\n-  int matShapeM{-1}, matShapeN{-1}, matShapeK{-1};\n-  int numRepM{-1}, numRepN{-1}, numRepK{-1};\n   Value thread, lane, warp, warpMN, warpN, warpM;\n-  size_t aElemBytes{}, bElemBytes{};\n \n   DotOpConversionHelper helper;\n-  triton::DotOp op;\n-  DotOpAdaptor adapter;\n   ConversionPatternRewriter &rewriter;\n   TypeConverter *typeConverter;\n   Location loc;\n   MLIRContext *ctx{};\n \n   using ValueTable = std::map<std::pair<unsigned, unsigned>, Value>;\n \n-  MMA16816ConversionHelper(triton::DotOp op, Value thread, DotOpAdaptor adapter,\n+  MMA16816ConversionHelper(MmaEncodingAttr mmaLayout, Value thread,\n                            ConversionPatternRewriter &rewriter,\n                            TypeConverter *typeConverter, Location loc)\n-      : helper(op), op(op), adapter(adapter), rewriter(rewriter),\n-        typeConverter(typeConverter), loc(loc), ctx(op.getContext()),\n+      : mmaLayout(mmaLayout), helper(mmaLayout), rewriter(rewriter),\n+        typeConverter(typeConverter), loc(loc), ctx(mmaLayout.getContext()),\n         thread(thread) {\n-    A = op.a();\n-    B = op.b();\n-    C = op.c();\n-    D = op.getResult();\n-\n-    aTensorTy = A.getType().cast<RankedTensorType>();\n-    bTensorTy = B.getType().cast<RankedTensorType>();\n-    dTensorTy = D.getType().cast<RankedTensorType>();\n-\n-    aShape = aTensorTy.getShape();\n-    bShape = bTensorTy.getShape();\n-    dShape = dTensorTy.getShape();\n-\n-    mmaLayout = dTensorTy.getEncoding().cast<MmaEncodingAttr>();\n-\n     wpt = mmaLayout.getWarpsPerCTA();\n \n-    auto mmaInstrShape = helper.getMmaInstrShape();\n-    mmaInstrM = mmaInstrShape[0];\n-    mmaInstrN = mmaInstrShape[1];\n-    mmaInstrK = mmaInstrShape[2];\n-\n-    auto matShape = helper.getMmaMatShape();\n-    matShapeM = matShape[0];\n-    matShapeN = matShape[1];\n-    matShapeK = matShape[2];\n-\n-    int NK = aShape[1];\n-    // shape / shape_per_cta\n-    numRepM = std::max<int>(dShape[0] / (wpt[0] * mmaInstrM), 1);\n-    numRepN = std::max<int>(dShape[1] / (wpt[1] * mmaInstrN), 1);\n-    numRepK = std::max<int>(NK / mmaInstrK, 1);\n-\n     Value _32 = i32_val(32);\n     lane = urem(thread, _32);\n     warp = udiv(thread, _32);\n     warpMN = udiv(warp, i32_val(wpt[0]));\n     warpM = urem(warp, i32_val(wpt[0]));\n     warpN = urem(warpMN, i32_val(wpt[1]));\n+  }\n \n-    aElemBytes = aTensorTy.getElementTypeBitWidth() / 8;\n-    bElemBytes = bTensorTy.getElementTypeBitWidth() / 8;\n+  // Get the mmaInstrShape from either $a or $b.\n+  std::tuple<int, int, int> getMmaInstrShape(Type operand) const {\n+    helper.deduceMmaType(operand);\n+    auto mmaInstrShape = helper.getMmaInstrShape();\n+    int mmaInstrM = mmaInstrShape[0];\n+    int mmaInstrN = mmaInstrShape[1];\n+    int mmaInstrK = mmaInstrShape[2];\n+    return std::make_tuple(mmaInstrM, mmaInstrN, mmaInstrK);\n+  }\n+\n+  std::tuple<int, int, int> getMmaMatShape(Type operand) const {\n+    helper.deduceMmaType(operand);\n+    auto matShape = helper.getMmaMatShape();\n+    int matShapeM = matShape[0];\n+    int matShapeN = matShape[1];\n+    int matShapeK = matShape[2];\n+    return std::make_tuple(matShapeM, matShapeN, matShapeK);\n+  }\n+\n+  // \\param operand is either $a or $b's type.\n+  inline int getNumRepM(Type operand, int M) const {\n+    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(operand);\n+    return std::max<int>(M / (wpt[0] * mmaInstrM), 1);\n+  }\n+\n+  // \\param operand is either $a or $b's type.\n+  inline int getNumRepN(Type operand, int N) const {\n+    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(operand);\n+    return std::max<int>(N / (wpt[1] * mmaInstrN), 1);\n+  }\n+\n+  // \\param operand is either $a or $b's type.\n+  inline int getNumRepK(Type operand, int K) const {\n+    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(operand);\n+    return std::max<int>(K / mmaInstrK, 1);\n   }\n \n   // Loading $a from smem to registers, returns a LLVM::Struct.\n-  Value loadA() {\n+  Value loadA(Value tensor, Value llTensor) const {\n+    auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n+    auto shape = aTensorTy.getShape();\n+\n     ValueTable ha;\n     std::function<void(int, int)> loadFn;\n+    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n+    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(aTensorTy);\n+    int numRepM = getNumRepM(aTensorTy, shape[0]);\n+    int numRepK = getNumRepK(aTensorTy, shape[1]);\n+\n     if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n       // load from smem\n       loadFn = getLoadMatrixFn(\n-          A, adapter.a() /*llTensor*/, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n+          tensor, llTensor, mmaLayout, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n           1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShpae*/,\n           {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/);\n     } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n@@ -2770,10 +2792,17 @@ struct MMA16816ConversionHelper {\n   }\n \n   // Loading $b from smem to registers, returns a LLVM::Struct.\n-  Value loadB() {\n+  Value loadB(Value tensor, Value llTensor) {\n     ValueTable hb;\n+    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+    auto shape = tensorTy.getShape();\n+    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(tensorTy);\n+    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(tensorTy);\n+    int numRepK = getNumRepK(tensorTy, shape[0]);\n+    int numRepN = getNumRepN(tensorTy, shape[1]);\n+\n     auto loadFn = getLoadMatrixFn(\n-        B, adapter.b() /*llTensor*/, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n+        tensor, llTensor, mmaLayout, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n         0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShpae*/,\n         {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/);\n \n@@ -2789,24 +2818,47 @@ struct MMA16816ConversionHelper {\n \n   // Loading $c from smem(?) to registers, returns a Value.\n   // NOTE Only SplatLike tensor is supported now.\n-  Value loadC() {\n+  Value loadC(Value tensor) const {\n     // Currently, we only support a SplatLike C. For the other cases, e.g., C in\n     // shared layout or blocked layout, we will support them by expanding\n     // convert_layout.\n-    auto hc = helper.loadSplatLikeC(C, loc, rewriter);\n+    auto hc = helper.loadSplatLikeC(tensor, loc, rewriter);\n     assert(hc.size() == 4UL && \"Only splat-like C is supported now\");\n     return hc[0];\n   }\n \n   // Conduct the Dot conversion.\n-  // Input the \\param a, \\param b, \\param c, all of them are result of loading.\n-  LogicalResult convertDot(Value a, Value b, Value c) {\n-    ValueTable ha = getValuesFromDotOperandLayoutStruct(a, numRepM, numRepK);\n+  // \\param a, \\param b, \\param c and \\param d are DotOp operands.\n+  // \\param loadedA, \\param loadedB, \\param loadedC, all of them are result of\n+  // loading.\n+  LogicalResult convertDot(Value a, Value b, Value c, Value d, Value loadedA,\n+                           Value loadedB, Value loadedC, DotOp op,\n+                           DotOpAdaptor adaptor) const {\n+    helper.deduceMmaType(op);\n+\n+    auto aTensorTy = a.getType().cast<RankedTensorType>();\n+    auto bTensorTy = b.getType().cast<RankedTensorType>();\n+    auto cTensorTy = c.getType().cast<RankedTensorType>();\n+    auto dTensorTy = d.getType().cast<RankedTensorType>();\n+\n+    auto aShape = aTensorTy.getShape();\n+    auto dShape = dTensorTy.getShape();\n+\n+    int NK = aShape[1];\n+    // shape / shape_per_cta\n+    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n+    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(aTensorTy);\n+    int numRepM = getNumRepM(aTensorTy, dShape[0]);\n+    int numRepN = getNumRepN(aTensorTy, dShape[1]);\n+    int numRepK = getNumRepK(aTensorTy, aShape[1]);\n+\n+    ValueTable ha =\n+        getValuesFromDotOperandLayoutStruct(loadedA, numRepM, numRepK);\n     ValueTable hb = getValuesFromDotOperandLayoutStruct(\n-        b, std::max(numRepN / 2, 1), numRepK);\n+        loadedB, std::max(numRepN / 2, 1), numRepK);\n \n     const int fcSize = 4 * numRepM * numRepN;\n-    SmallVector<Value> fc(fcSize, c);\n+    SmallVector<Value> fc(fcSize, loadedC);\n \n     auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n       unsigned colsPerThread = numRepN * 2;\n@@ -2855,10 +2907,10 @@ struct MMA16816ConversionHelper {\n \n private:\n   std::function<void(int, int)>\n-  getLoadMatrixFn(Value tensor, Value llTensor, int wpt, int kOrder,\n-                  ArrayRef<int> instrShape, ArrayRef<int> matShape,\n-                  Value warpId, ValueTable &vals) {\n-\n+  getLoadMatrixFn(Value tensor, Value llTensor, MmaEncodingAttr mmaLayout,\n+                  int wpt, int kOrder, ArrayRef<int> instrShape,\n+                  ArrayRef<int> matShape, Value warpId,\n+                  ValueTable &vals) const {\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n     // We assumes that the input operand of Dot should be from shared layout.\n     // TODO(Superjomn) Consider other layouts if needed later.\n@@ -2928,7 +2980,7 @@ struct MMA16816ConversionHelper {\n   // i \\in [0, n0) and j \\in [0, n1)\n   // There should be \\param n0 * \\param n1 elements in the output Struct.\n   Value composeValuesToDotOperandLayoutStruct(const ValueTable &vals, int n0,\n-                                              int n1) {\n+                                              int n1) const {\n     std::vector<Value> elems;\n     for (unsigned m = 0; m < n0; ++m)\n       for (unsigned k = 0; k < n1; ++k) {\n@@ -2940,15 +2992,16 @@ struct MMA16816ConversionHelper {\n \n     assert(!elems.empty());\n \n-    Type fp16Ty = aTensorTy.getElementType();\n+    Type fp16Ty = type::f16Ty(ctx);\n     Type fp16x2Ty = vec_ty(fp16Ty, 2);\n     Type structTy = LLVM::LLVMStructType::getLiteral(\n         ctx, SmallVector<Type>(elems.size(), fp16x2Ty));\n     auto result = getStructFromElements(loc, elems, rewriter, structTy);\n     return result;\n   }\n \n-  ValueTable getValuesFromDotOperandLayoutStruct(Value value, int n0, int n1) {\n+  ValueTable getValuesFromDotOperandLayoutStruct(Value value, int n0,\n+                                                 int n1) const {\n     auto elems = ConvertTritonGPUOpToLLVMPatternBase::getElementsFromStruct(\n         loc, value, rewriter);\n \n@@ -2966,18 +3019,79 @@ struct MMA16816ConversionHelper {\n   }\n };\n \n+LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n+    triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto loc = op.getLoc();\n+  Value src = op.src();\n+  Value dst = op.result();\n+  auto srcTensorTy = src.getType().cast<RankedTensorType>();\n+  auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n+\n+  auto sharedLayout = srcTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto dotOperandLayout =\n+      dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+  MmaEncodingAttr mmaLayout =\n+      dotOperandLayout.getParent().dyn_cast_or_null<MmaEncodingAttr>();\n+  assert(mmaLayout);\n+\n+  MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n+                                     rewriter, getTypeConverter(), op.getLoc());\n+\n+  Value res;\n+  if (dotOperandLayout.getOpIdx() == 0) {\n+    // operand $a\n+    res = mmaHelper.loadA(src, adaptor.src());\n+  } else if (dotOperandLayout.getOpIdx() == 1) {\n+    // operand $b\n+    res = mmaHelper.loadB(src, adaptor.src());\n+  } else if (dotOperandLayout.getOpIdx() == 2) {\n+    // operand $c\n+    res = mmaHelper.loadC(src);\n+  }\n+\n+  rewriter.replaceOp(op, res);\n+  return success();\n+}\n+\n LogicalResult\n-DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n+DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adaptor,\n                                  ConversionPatternRewriter &rewriter) const {\n   auto loc = op.getLoc();\n-  MMA16816ConversionHelper mmaHelper(op, getThreadId(rewriter, loc), adapter,\n+  auto mmaLayout = op.getResult()\n+                       .getType()\n+                       .cast<RankedTensorType>()\n+                       .getEncoding()\n+                       .cast<MmaEncodingAttr>();\n+  MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n                                      rewriter, getTypeConverter(), loc);\n \n-  auto A = mmaHelper.loadA();\n-  auto B = mmaHelper.loadB();\n-  auto C = mmaHelper.loadC();\n-\n-  return mmaHelper.convertDot(A, B, C);\n+  Value A = op.a();\n+  Value B = op.b();\n+  Value C = op.c();\n+  auto ATensorTy = A.getType().cast<RankedTensorType>();\n+  auto BTensorTy = B.getType().cast<RankedTensorType>();\n+\n+  Value loadedA, loadedB, loadedC;\n+  // We support two kinds of operand layouts: 1. both $a, $b are dot_operand\n+  // layout, 2. both of them are shared layout.\n+  if (ATensorTy.getEncoding().isa<DotOperandEncodingAttr>()) {\n+    assert(BTensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n+           \"Both $a and %b should be DotOperand layout.\");\n+    loadedA = adaptor.a();\n+    loadedB = adaptor.b();\n+  } else {\n+    loadedA = mmaHelper.loadA(op.a(), adaptor.a());\n+    loadedB = mmaHelper.loadB(op.b(), adaptor.b());\n+  }\n+\n+  // TODO[Superjomn]: Process C as a mma layout.\n+  // Currently, C is simply treated as a Splat Op, and the data layout is not\n+  // mattered.\n+  loadedC = mmaHelper.loadC(op.c());\n+\n+  return mmaHelper.convertDot(A, B, C, op.d(), loadedA, loadedB, loadedC, op,\n+                              adaptor);\n }\n \n /// ====================== mma codegen end ============================"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -640,4 +640,4 @@ LogicalResult TritonGPUDialect::verifyOperationAttribute(Operation *op,\n                                                          NamedAttribute attr) {\n   // TODO: fill this.\n   return success();\n-}\n\\ No newline at end of file\n+}"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 24, "deletions": 0, "changes": 24, "file_content_changes": "@@ -716,3 +716,27 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     return\n   }\n }\n+\n+// -----\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n+#mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  func @matmul_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  %a:tensor<128x32xf16, #shared>, %b:tensor<32x256xf16, #shared>) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<128x256xf32, #mma>\n+    // CHECK: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n+    %a_mat = triton_gpu.convert_layout %a : (tensor<128x32xf16, #shared>) -> tensor<128x32xf16, #dot_operand_a>\n+    %b_mat = triton_gpu.convert_layout %b : (tensor<32x256xf16, #shared>) -> tensor<32x256xf16, #dot_operand_b>\n+\n+    %28 = tt.dot %a_mat, %b_mat, %cst {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #dot_operand_a> * tensor<32x256xf16, #dot_operand_b> -> tensor<128x256xf32, #mma>\n+    %38 = triton_gpu.convert_layout %28 : (tensor<128x256xf32, #mma>) -> tensor<128x256xf32, #blocked>\n+\n+    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #blocked>\n+    %36 = tt.broadcast %30 : (tensor<128x1x!tt.ptr<f32>, #blocked>) -> tensor<128x256x!tt.ptr<f32>, #blocked>\n+    tt.store %36, %38 : tensor<128x256xf32, #blocked>\n+    return\n+  }\n+}"}]