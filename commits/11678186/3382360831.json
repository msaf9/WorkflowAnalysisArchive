[{"filename": "include/triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -320,7 +320,7 @@ struct PTXInstrExecution {\n   // Prefix a !predicate to the instruction.\n   PTXInstrExecution &predicateNot(mlir::Value value, StringRef constraint) {\n     pred = instr->builder->newOperand(value, constraint);\n-    pred->repr = [](int idx) { return \"@!%\" + std::to_string(idx); };\n+    pred->repr = [](int idx) { return \"@!$\" + std::to_string(idx); };\n     return *this;\n   }\n "}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -357,9 +357,9 @@ def TT_ExtElemwiseOp : TT_Op<\"ext_elemwise\", [NoSideEffect, Elementwise, SameOpe\n         return $libpath/$libname:$symbol($args...)\n     }];\n \n-    let arguments = (ins Variadic<TT_Tensor>:$args, StrAttr:$libname, StrAttr:$libpath, StrAttr:$symbol);\n+    let arguments = (ins Variadic<TT_Type>:$args, StrAttr:$libname, StrAttr:$libpath, StrAttr:$symbol);\n \n-    let results = (outs TT_Tensor:$result);\n+    let results = (outs TT_Type:$result);\n \n     let assemblyFormat = \"operands attr-dict `:` type(operands) `->` type($result)\";\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/PtxAsmFormat.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -125,7 +125,7 @@ std::string PTXBuilder::dump() const {\n     lines.push_back(exec->dump());\n   }\n \n-  return strJoin(lines, \"\\r\\n\");\n+  return strJoin(lines, \"\\n\\t\");\n }\n \n PTXInstrExecution &PTXInstrCommon::call(ArrayRef<Operand *> oprs) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -963,7 +963,7 @@ struct LoadOpConversion\n       if (other) {\n         for (size_t ii = 0; ii < nWords; ++ii) {\n           PTXInstr &mov = *ptxBuilder.create<>(\"mov\");\n-          mov.o(\"u\", width);\n+          mov.o(\"u\" + std::to_string(width));\n \n           size_t size = width / valueElemNbits;\n "}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -408,7 +408,7 @@ def __init__(self, handle, type: dtype):\n         # IR handle\n         self.handle = handle\n         # Block shape\n-        self.shape = (1, )\n+        self.shape = ()\n         if type.is_block():\n             self.shape = type.shape\n         self.numel = 1"}, {"filename": "python/tutorials/02-fused-softmax.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -81,7 +81,7 @@ def softmax_kernel(\n     # Subtract maximum for numerical stability\n     row_minus_max = row - tl.max(row, axis=0)\n     # Note that exponentials in Triton are fast but approximate (i.e., think __expf in CUDA)\n-    numerator = tl.exp(row_minus_max)\n+    numerator = tl.libdevice.exp(row_minus_max)\n     denominator = tl.sum(numerator, axis=0)\n     softmax_output = numerator / denominator\n     # Write back output to DRAM"}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -35,7 +35,7 @@ def _layer_norm_fwd_fused(X, Y, W, B, M, V, stride, N, eps,\n     # compute std\n     xmean = tl.where(mask, x - mean, 0.)\n     var = tl.sum(xmean * xmean, axis=0) / N\n-    rstd = 1 / tl.sqrt(var + eps)\n+    rstd = 1 / tl.libdevice.sqrt(var + eps)\n     xhat = xmean * rstd\n     # write-back mean/rstd\n     tl.store(M + row, mean)"}]