[{"filename": ".gitignore", "status": "modified", "additions": 0, "deletions": 7, "changes": 7, "file_content_changes": "@@ -24,10 +24,3 @@ venv.bak/\n # JetBrains project files\n .idea\n cmake-build-*\n-\n-# rocm\n-log*\n-*.so\n-ptxas\n-scripts\n-.gitignore\n\\ No newline at end of file"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 12, "deletions": 11, "changes": 23, "file_content_changes": "@@ -1063,8 +1063,8 @@ void init_triton_ir(py::module &&m) {\n              unsigned typeWidth = elementType.getIntOrFloatBitWidth();\n              auto constValue = builder.create<mlir::arith::ConstantIntOp>(\n                  loc, typeWidth, elementType);\n-             auto zeroConst =\n-                 builder.create<mlir::arith::ConstantIntOp>(loc, 0, elementType);\n+             auto zeroConst = builder.create<mlir::arith::ConstantIntOp>(\n+                 loc, 0, elementType);\n              if (lhs.getType().isIntOrIndex()) {\n                auto cmpValue = builder.create<mlir::arith::CmpIOp>(\n                    loc, mlir::arith::CmpIPredicate::ult, rhs, constValue);\n@@ -1095,8 +1095,8 @@ void init_triton_ir(py::module &&m) {\n              unsigned typeWidth = elementType.getIntOrFloatBitWidth();\n              auto constValue = builder.create<mlir::arith::ConstantIntOp>(\n                  loc, typeWidth, elementType);\n-             auto zeroConst =\n-                 builder.create<mlir::arith::ConstantIntOp>(loc, 0, elementType);\n+             auto zeroConst = builder.create<mlir::arith::ConstantIntOp>(\n+                 loc, 0, elementType);\n              if (lhs.getType().isIntOrIndex()) {\n                auto cmpValue = builder.create<mlir::arith::CmpIOp>(\n                    loc, mlir::arith::CmpIPredicate::ult, rhs, constValue);\n@@ -1127,16 +1127,17 @@ void init_triton_ir(py::module &&m) {\n              unsigned typeWidth = elementType.getIntOrFloatBitWidth();\n              auto constValue = builder.create<mlir::arith::ConstantIntOp>(\n                  loc, typeWidth, elementType);\n-             auto zeroConst =\n-                 builder.create<mlir::arith::ConstantIntOp>(loc, 0, elementType);\n+             auto zeroConst = builder.create<mlir::arith::ConstantIntOp>(\n+                 loc, 0, elementType);\n              uint64_t ones_val = 0xFFFFFFFFFFFFFFFF;\n              auto onesConst = builder.create<mlir::arith::ConstantIntOp>(\n                  loc, ones_val, elementType);\n              if (lhs.getType().isIntOrIndex()) {\n                auto negativeCmpValue = builder.create<mlir::arith::CmpIOp>(\n                    loc, mlir::arith::CmpIPredicate::slt, lhs, zeroConst);\n-               auto otherValue = mlir::Value(builder.create<mlir::arith::SelectOp>(\n-                   loc, negativeCmpValue, onesConst, zeroConst));\n+               auto otherValue =\n+                   mlir::Value(builder.create<mlir::arith::SelectOp>(\n+                       loc, negativeCmpValue, onesConst, zeroConst));\n                auto cmpValue = builder.create<mlir::arith::CmpIOp>(\n                    loc, mlir::arith::CmpIPredicate::ult, rhs, constValue);\n                auto shiftValue =\n@@ -1152,8 +1153,9 @@ void init_triton_ir(py::module &&m) {\n                    loc, lhs.getType(), onesConst);\n                auto negativeCmpValue = builder.create<mlir::arith::CmpIOp>(\n                    loc, mlir::arith::CmpIPredicate::slt, lhs, zeroValue);\n-               auto otherValue = mlir::Value(builder.create<mlir::arith::SelectOp>(\n-                   loc, negativeCmpValue, onesValue, zeroValue));\n+               auto otherValue =\n+                   mlir::Value(builder.create<mlir::arith::SelectOp>(\n+                       loc, negativeCmpValue, onesValue, zeroValue));\n                auto cmpValue = builder.create<mlir::arith::CmpIOp>(\n                    loc, mlir::arith::CmpIPredicate::ult, rhs, splatValue);\n                auto shiftValue =\n@@ -1857,7 +1859,6 @@ void init_triton_translation(py::module &m) {\n            const std::vector<std::string> &paths) {\n           ::mlir::triton::addExternalLibs(op, names, paths);\n         });\n-\n }\n \n void init_triton(py::module &m) {"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 17, "deletions": 17, "changes": 34, "file_content_changes": "@@ -930,7 +930,7 @@ def noinline_multi_values_fn(x, y, Z):\n def test_noinline(mode, device):\n     if is_hip() and mode == \"shared\":\n         pytest.skip('test_noinline[\"shared\"] not supported on HIP.')\n-    \n+\n     @triton.jit\n     def kernel(X, Y, Z):\n         x = tl.load(X)\n@@ -1018,7 +1018,7 @@ def kernel(X, Z):\n     sem_str = \"acq_rel\" if sem is None else sem\n     if is_hip():\n         return\n-    \n+\n     assert f\"atom.global.gpu.{sem_str}\" in h.asm[\"ptx\"]\n \n \n@@ -1140,7 +1140,6 @@ def test_cast(dtype_x, dtype_z, bitcast, device):\n     if is_hip() and (dtype_z == \"bfloat16\"):\n         pytest.skip(f'test_cast{(dtype_x, dtype_z)} cast to bfloat16 not supported on HIP.')\n \n-\n     size = 1024\n     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n     if dtype_x.startswith('bfloat'):\n@@ -1238,7 +1237,7 @@ def kernel(in_out_ptr):\n     for _ in range(1000):\n         x = torch.ones((65536,), device=device, dtype=torch.float32)\n         if is_hip():\n-            kernel[(65536,)](x, num_warps=16) # threads per Warp for ROCM is 64\n+            kernel[(65536,)](x, num_warps=16)  # threads per Warp for ROCM is 64\n         else:\n             kernel[(65536,)](x, num_warps=32)\n         assert torch.all(x == 2)\n@@ -1610,7 +1609,7 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n def test_scan_layouts(M, N, src_layout, axis, device):\n     if is_hip():\n         pytest.skip(\"test_scan_layouts is not supported in HIP\")\n-        \n+\n     ir = f\"\"\"\n     #blocked = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n@@ -1676,9 +1675,9 @@ def test_scan_layouts(M, N, src_layout, axis, device):\n def test_reduce_layouts(M, N, src_layout, axis, device):\n     if is_hip():\n         pytest.skip(\"test_reduce_layouts is not supported in HIP\")\n-        gpu_dialect =\"triton_gpu_rocm\"\n+        gpu_dialect = \"triton_gpu_rocm\"\n     else:\n-        gpu_dialect =\"triton_gpu\"\n+        gpu_dialect = \"triton_gpu\"\n \n     rdims_2d = f\"1x{N}\" if axis == 0 else f\"{M}x1\"\n     rdims_1d = f\"{N}\" if axis == 0 else f\"{M}\"\n@@ -1751,9 +1750,9 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n @pytest.mark.parametrize(\"src_layout\", layouts)\n def test_store_op(M, src_layout, device):\n     if is_hip():\n-        gpu_dialect =\"triton_gpu_rocm\"\n+        gpu_dialect = \"triton_gpu_rocm\"\n     else:\n-        gpu_dialect =\"triton_gpu\"\n+        gpu_dialect = \"triton_gpu\"\n \n     ir = f\"\"\"\n     #src = {src_layout}\n@@ -1806,9 +1805,9 @@ def test_store_op(M, src_layout, device):\n def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device):\n     if is_hip():\n         pytest.skip(\"test_reduce_layouts is not supported in HIP\")\n-        gpu_dialect =\"triton_gpu_rocm\"\n+        gpu_dialect = \"triton_gpu_rocm\"\n     else:\n-        gpu_dialect =\"triton_gpu\"\n+        gpu_dialect = \"triton_gpu\"\n     ir = f\"\"\"\n     #dst = {dst_layout}\n     #src = {src_layout}\n@@ -1869,10 +1868,10 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n @pytest.mark.parametrize(\"first_axis\", [0, 1])\n def test_chain_reduce(M, N, src_layout, op, device, first_axis):\n     if is_hip():\n-        gpu_dialect =\"triton_gpu_rocm\"\n+        gpu_dialect = \"triton_gpu_rocm\"\n         pytest.skip(\"test_chain_reduce is not supported in HIP\")\n     else:\n-        gpu_dialect =\"triton_gpu\"\n+        gpu_dialect = \"triton_gpu\"\n \n     op_str = \"\"\n     if op == \"sum\":\n@@ -1975,7 +1974,7 @@ def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n def test_permute(dtype_str, shape, perm, device):\n     check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n     if is_hip():\n-        if dtype_str == \"float32\" and shape == (128,128):\n+        if dtype_str == \"float32\" and shape == (128, 128):\n             pytest.skip(f\"test_permute{(dtype_str, shape)} Not supported: memory out of resource on HIP.\")\n \n     # triton kernel\n@@ -2341,6 +2340,7 @@ def kernel(out_ptr):\n def test_dot_without_load(dtype_str, device):\n     if is_hip() and dtype_str == \"float16\":\n         pytest.skip(\"test_dot_without_load[float16] not supported in HIP\")\n+\n     @triton.jit\n     def _kernel(out):\n         a = GENERATE_TEST_HERE\n@@ -2558,7 +2558,7 @@ def _kernel(dst, src, CACHE: tl.constexpr):\n         tl.store(dst + offsets, x, cache_modifier=CACHE)\n \n     if is_hip():\n-        return \n+        return\n     pgm = _kernel[(1,)](dst, src, CACHE=cache)\n     ptx = pgm.asm['ptx']\n     if cache == '':\n@@ -3320,9 +3320,9 @@ def kernel(Out):\n def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device):\n     if is_hip():\n         pytest.skip(\"test_convert2d is not supported in HIP\")\n-        gpu_dialect =\"triton_gpu_rocm\"\n+        gpu_dialect = \"triton_gpu_rocm\"\n     else:\n-        gpu_dialect =\"triton_gpu\"\n+        gpu_dialect = \"triton_gpu\"\n     if str(src_layout) == str(dst_layout):\n         pytest.skip()\n     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "file_content_changes": "@@ -27,7 +27,7 @@\n from .code_generator import ast_to_ttir\n from .make_launcher import make_stub\n \n-rocm_module_path = os.path.join(Path(__file__).parents[3],\"third_party\",\"amd_hip_backend\",\"python\", \"rocm.py\")\n+rocm_module_path = os.path.join(Path(__file__).parents[3], \"third_party\", \"amd_hip_backend\", \"python\", \"rocm.py\")\n spec = importlib.util.spec_from_file_location(\"rocm\", rocm_module_path)\n rocm = importlib.util.module_from_spec(spec)\n spec.loader.exec_module(rocm)\n@@ -155,9 +155,6 @@ def ptx_to_cubin(ptx: str, arch: int):\n     return compile_ptx_to_cubin(ptx, ptxas, arch)\n \n \n-\n-\n-\n # ------------------------------------------------------------------------------\n # compiler\n # ------------------------------------------------------------------------------"}, {"filename": "python/triton/language/math.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -9,12 +9,12 @@\n @functools.lru_cache()\n def libdevice_path():\n     if is_hip():\n-        third_party_dir = os.path.join(Path(__file__).parents[3],\"third_party\",\"amd_hip_backend\",\"python\",\"triton\",\"third_party\")\n+        third_party_dir = os.path.join(Path(__file__).parents[3], \"third_party\", \"amd_hip_backend\", \"python\", \"triton\", \"third_party\")\n         default = os.path.join(third_party_dir, \"rocm\", \"lib\", \"bitcode\", \"cuda2gcn.bc\")\n     else:\n         third_party_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\")\n         default = os.path.join(third_party_dir, \"cuda\", \"lib\", \"libdevice.10.bc\")\n-       \n+\n     return os.getenv(\"TRITON_LIBDEVICE_PATH\", default)\n \n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -11,7 +11,7 @@\n from ..common.build import is_hip\n from . import core as tl\n \n-rocm_module_path = os.path.join(Path(__file__).parents[3],\"third_party\",\"amd_hip_backend\",\"python\", \"rocm.py\")\n+rocm_module_path = os.path.join(Path(__file__).parents[3], \"third_party\", \"amd_hip_backend\", \"python\", \"rocm.py\")\n spec = importlib.util.spec_from_file_location(\"rocm\", rocm_module_path)\n rocm = importlib.util.module_from_spec(spec)\n spec.loader.exec_module(rocm)\n@@ -532,7 +532,6 @@ def full(shape: List[int], value, dtype: tl.dtype, builder: ir.builder) -> tl.te\n         return tl.tensor(builder.create_splat(value, shape), ret_ty)\n \n \n-\n def ones(shape: List[int], dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n     _1 = builder.get_one_value(dtype.to_ir(builder))\n     ret_ty = tl.block_type(dtype, shape)\n@@ -1262,19 +1261,22 @@ def atomic_xchg(ptr: tl.tensor,\n #                               Linear Algebra\n # ===----------------------------------------------------------------------===//\n \n+\n def gpu_has_mfma() -> bool:\n     if not is_hip():\n         return False\n     arch_info = rocm.get_amdgpu_arch_fulldetails()\n     gfx_arch = arch_info[1]\n     return gfx_arch in ['gfx908', 'gfx90a']\n \n+\n def mfma_supported(M, N, K, allow_tf32, ret_scalar_ty) -> bool:\n     if not gpu_has_mfma():\n         return False\n     # TODO: Add check for configurations and types.\n     return True\n \n+\n def dot(lhs: tl.tensor,\n         rhs: tl.tensor,\n         allow_tf32: bool,"}]