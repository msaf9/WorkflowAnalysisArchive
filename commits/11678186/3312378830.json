[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 44, "deletions": 16, "changes": 60, "file_content_changes": "@@ -622,6 +622,8 @@ class ConvertTritonGPUOpToLLVMPattern\n   Value smem;\n };\n \n+Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr& layout,  Type resType, Type elemType, Value constVal, TypeConverter*typeConverter, ConversionPatternRewriter& rewriter, Location loc);\n+\n // Convert SplatOp or arith::ConstantOp with SplatElementsAttr to a\n // LLVM::StructType value.\n //\n@@ -632,16 +634,23 @@ Value convertSplatLikeOp(Type elemType, Type resType, Value constVal,\n                          TypeConverter *typeConverter,\n                          ConversionPatternRewriter &rewriter, Location loc) {\n   auto tensorTy = resType.cast<RankedTensorType>();\n-  auto layout = tensorTy.getEncoding();\n-  auto srcType = typeConverter->convertType(elemType);\n-  auto llSrc = bitcast(srcType, constVal);\n-  size_t elemsPerThread = getElemsPerThread(layout, tensorTy.getShape());\n-  llvm::SmallVector<Value, 4> elems(elemsPerThread, llSrc);\n-  llvm::SmallVector<Type, 4> elemTypes(elems.size(), srcType);\n-  auto structTy =\n-      LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);\n-\n-  return getStructFromElements(loc, elems, rewriter, structTy);\n+  if (tensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n+    llvm::outs() << \"Convert splat op \" << constVal << \"\\n\";\n+    auto tensorTy = resType.cast<RankedTensorType>();\n+    auto layout = tensorTy.getEncoding();\n+    auto srcType = typeConverter->convertType(elemType);\n+    auto llSrc = bitcast(srcType, constVal);\n+    size_t elemsPerThread = getElemsPerThread(layout, tensorTy.getShape());\n+    llvm::outs() << \"splat.size: \" << elemsPerThread <<\"\\n\";\n+    llvm::SmallVector<Value, 4> elems(elemsPerThread, llSrc);\n+    llvm::SmallVector<Type, 4> elemTypes(elems.size(), srcType);\n+    auto structTy =\n+        LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);\n+\n+    return getStructFromElements(loc, elems, rewriter, structTy);\n+  } else if (auto mmaLayout = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>()) {\n+    return convertSplatLikeOpWithMmaLayout(mmaLayout, resType, elemType, constVal, typeConverter, rewriter, loc);\n+  }\n }\n \n struct SplatOpConversion\n@@ -2692,6 +2701,8 @@ struct DotOpConversionHelper {\n   };\n };\n \n+\n+\n // This class helps to adapt the existing DotOpConversion to the latest\n // DotOpOperand layout design. It decouples the exising implementation to two\n // parts:\n@@ -2828,9 +2839,9 @@ struct MMA16816ConversionHelper {\n     return result;\n   }\n \n-  // Loading $c from smem(?) to registers, returns a Value.\n-  // NOTE Only SplatLike tensor is supported now.\n   Value loadC(Value tensor, Value llTensor) const {\n+    llvm::outs() << \"C is \" << tensor << \"\\n\";\n+    llvm::outs() << \"llC is \" << llTensor << \"\\n\";\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n     auto [repM, repN] = DotOpConversionHelper::getRepMN(tensorTy);\n     size_t fcSize = 4 * repM * repN;\n@@ -2847,9 +2858,10 @@ struct MMA16816ConversionHelper {\n     // Else load a normal C tensor with mma layout, that should be a\n     // LLVM::struct with fcSize elements.\n     auto structTy = llTensor.getType().cast<LLVM::LLVMStructType>();\n-    assert(structTy.getBody().size() == fcSize &&\n-           \"DotOp's $c operand should pass the same number of values as $d in \"\n-           \"mma layout.\");\n+    printf(\"struct.size: %d, fcSize: %d\\n\", structTy.getBody().size(), fcSize);\n+    //assert(structTy.getBody().size() == fcSize &&\n+           //\"DotOp's $c operand should pass the same number of values as $d in \"\n+           //\"mma layout.\");\n     return llTensor;\n   }\n \n@@ -2883,7 +2895,8 @@ struct MMA16816ConversionHelper {\n     ValueTable hb = getValuesFromDotOperandLayoutStruct(\n         loadedB, std::max(numRepN / 2, 1), numRepK);\n     auto fc = ConvertTritonGPUOpToLLVMPatternBase::getElementsFromStruct(\n-        loc, c, rewriter);\n+        loc, adaptor.c(), rewriter);\n+    printf(\"fc.size: %d\\n\", fc.size());\n \n     auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n       unsigned colsPerThread = numRepN * 2;\n@@ -3119,6 +3132,21 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adaptor,\n                               adaptor);\n }\n \n+Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr& layout,  Type resType, Type elemType, Value constVal, TypeConverter*typeConverter, ConversionPatternRewriter& rewriter, Location loc) {\n+  if (layout.getVersion() == 2) {\n+    auto tensorTy = resType.cast<RankedTensorType>();\n+    auto [repM, repN] = DotOpConversionHelper::getRepMN(tensorTy);\n+    size_t fcSize = 4 * repM * repN;\n+\n+    auto structTy =\n+        LLVM::LLVMStructType::getLiteral(rewriter.getContext(), SmallVector<Type>(fcSize, elemType));\n+    return getStructFromElements(loc, SmallVector<Value>(fcSize, constVal), rewriter, structTy);\n+  }\n+\n+  assert(false && \"Unsupported mma layout found\") ;\n+}\n+\n+\n /// ====================== mma codegen end ============================\n \n class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -147,7 +147,6 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n     llvm::errs() << \"Pass execution failed\";\n     return nullptr;\n   }\n-  llvm::outs() << module << \"\\n\";\n \n   auto llvmir = translateLLVMToLLVMIR(llvmContext, module);\n   if (!llvmir) {"}]