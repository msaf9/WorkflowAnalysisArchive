[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 5, "deletions": 244, "changes": 249, "file_content_changes": "@@ -83,6 +83,7 @@ static Value createLLVMIntegerConstant(OpBuilder &builder, Location loc,\n \n } // namespace\n \n+// A helper function for using printf in LLVM conversion.\n void llPrintf(StringRef msg, ValueRange args,\n               ConversionPatternRewriter &rewriter);\n \n@@ -2984,7 +2985,10 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                               ConversionPatternRewriter &rewriter) const;\n \n   LogicalResult convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n-                              ConversionPatternRewriter &rewriter) const;\n+                              ConversionPatternRewriter &rewriter) const {\n+    assert(false && \"Not implemented yet.\");\n+    return failure();\n+  }\n };\n \n // Helper for conversion of DotOp with mma<version=1>, that is sm<80\n@@ -3506,31 +3510,6 @@ struct MMA16816ConversionHelper {\n       for (int k = 0; k < numRepK; ++k)\n         loadFn(2 * m, 2 * k);\n \n-#define PRINT_AB 0\n-        // DEBUG: print all things in ha\n-#if PRINT_AB\n-    for (auto &item : ha) {\n-      llvm::outs() << item.second << \"\\n\";\n-      llvm::outs() << \"type: \" << item.second.getType() << \"\\n\";\n-\n-      auto getIntAttr = [&](int v) {\n-        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n-      };\n-\n-      SmallVector<Value> i8x4;\n-      for (int i = 0; i < 4; i++) {\n-        auto val = extract_element(i8_ty, item.second, i32_val(i));\n-        i8x4.push_back(val);\n-      }\n-\n-      mlir::LLVM::llPrintf(\"thread:%d, m:%d, k:%d: (%d,%d,%d,%d)\",\n-                           {thread, i32_val(item.first.first),\n-                            i32_val(item.first.second), i8x4[0], i8x4[1],\n-                            i8x4[2], i8x4[3]},\n-                           rewriter);\n-    }\n-#endif\n-\n     // step2. Format the values to LLVM::Struct to passing to mma codegen.\n     Value result = composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n     return result;\n@@ -3556,31 +3535,6 @@ struct MMA16816ConversionHelper {\n         loadFn(2 * n, 2 * k);\n     }\n \n-#if PRINT_AB\n-    // Debug: print all things in hb\n-    for (auto &item : hb) {\n-      llvm::outs() << item.second << \"\\n\";\n-      llvm::outs() << \"type: \" << item.second.getType() << \"\\n\";\n-      auto i8s = bitcast(item.second, vec_ty(i8_ty, 4));\n-\n-      auto getIntAttr = [&](int v) {\n-        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n-      };\n-\n-      SmallVector<Value> i8x4;\n-      for (int i = 0; i < 4; i++) {\n-        auto val = extract_element(i8_ty, i8s, i32_val(i));\n-        i8x4.push_back(val);\n-      }\n-\n-      mlir::LLVM::llPrintf(\"thread:%d, hb, (%d,%d): (%d,%d,%d,%d)\",\n-                           {thread, i32_val(item.first.first),\n-                            i32_val(item.first.second), i8x4[0], i8x4[1],\n-                            i8x4[2], i8x4[3]},\n-                           rewriter);\n-    }\n-#endif\n-\n     Value result = composeValuesToDotOperandLayoutStruct(\n         hb, std::max(numRepN / 2, 1), numRepK);\n     return result;\n@@ -3630,7 +3584,6 @@ struct MMA16816ConversionHelper {\n     auto fc = ConvertTritonGPUOpToLLVMPatternBase::getElementsFromStruct(\n         loc, loadedC, rewriter);\n \n-    llvm::outs() << \"mma.instr: \" << helper.getMmaInstr() << \"\\n\";\n     auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n       unsigned colsPerThread = numRepN * 2;\n       PTXBuilder builder;\n@@ -3651,32 +3604,6 @@ struct MMA16816ConversionHelper {\n         // reuse the output registers\n       }\n \n-      // DEBUG: print $a\n-      auto print_val = [&](int m, int k, ValueTable &dic, StringRef hint) {\n-        auto _val = dic[{m, k}];\n-        auto val = bitcast(_val, vec_ty(i8_ty, 4));\n-\n-        llvm::outs() << \"val: \" << val << \"\\n\";\n-        SmallVector<Value> vals;\n-        for (int i = 0; i < 4; i++)\n-          vals.push_back(extract_element(i8_ty, val, i32_val(i)));\n-        LLVM::llPrintf(\"t-%d $\" + hint.str() + \": (m%d, n%d) = [%d,%d,%d,%d]\",\n-                       {thread, i32_val(m), i32_val(n),\n-                        // data\n-                        vals[0], vals[1], vals[2], vals[3]},\n-                       rewriter);\n-      };\n-\n-      print_val(m, k, ha, \"a\");\n-      // print_a(m+1, k, ha);\n-      // print_a(m, k+1, ha);\n-      // print_a(m+1, k+1, ha);\n-\n-      print_val(n, k, hb, \"b\");\n-\n-      // DEBUG: print $b\n-      // DEBUG: print $c\n-\n       mma(retArgs, aArgs, bArgs, cArgs);\n       Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n \n@@ -4354,172 +4281,6 @@ DotOpMmaV1ConversionHelper::extractLoadedOperand(\n   return rcds;\n }\n \n-template <typename T> void print_vec(ArrayRef<T> vec) {\n-  for (int v : vec)\n-    llvm::outs() << v << \" \";\n-  llvm::outs() << \"\\n\";\n-}\n-\n-LogicalResult\n-DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n-                               ConversionPatternRewriter &rewriter) const {\n-  auto *ctx = rewriter.getContext();\n-  auto loc = op.getLoc();\n-  auto threadId = getThreadId(rewriter, loc);\n-\n-  using ValueTable = std::map<std::pair<int, int>, Value>;\n-\n-  auto A = op.a();\n-  auto B = op.b();\n-  auto C = op.c();\n-  auto D = op.getResult();\n-\n-  auto aTensorTy = A.getType().cast<RankedTensorType>();\n-  auto bTensorTy = B.getType().cast<RankedTensorType>();\n-  auto cTensorTy = C.getType().cast<RankedTensorType>();\n-  auto dTensorTy = D.getType().cast<RankedTensorType>();\n-\n-  auto aShape = aTensorTy.getShape();\n-  auto bShape = bTensorTy.getShape();\n-  auto cShape = cTensorTy.getShape();\n-\n-  auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto cLayout = cTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n-  auto dLayout = dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n-\n-  auto aOrder = aLayout.getOrder();\n-  auto bOrder = bLayout.getOrder();\n-\n-  auto order = dLayout.getOrder();\n-\n-  bool isARow = aOrder[0] == 1;\n-  bool isBRow = bOrder[0] == 1;\n-\n-  int strideAM = isARow ? aShape[1] : 1;\n-  int strideAK = isARow ? 1 : aShape[0];\n-  int strideBN = isBRow ? 1 : bShape[0];\n-  int strideBK = isBRow ? bShape[1] : 1;\n-  int strideA0 = isARow ? strideAK : strideAM;\n-  int strideA1 = isARow ? strideAM : strideAK;\n-  int strideB0 = isBRow ? strideBN : strideBK;\n-  int strideB1 = isBRow ? strideBK : strideBN;\n-  int lda = isARow ? strideAM : strideAK;\n-  int ldb = isBRow ? strideBK : strideBN;\n-  int aPerPhase = aLayout.getPerPhase();\n-  int aMaxPhase = aLayout.getMaxPhase();\n-  int bPerPhase = bLayout.getPerPhase();\n-  int bMaxPhase = bLayout.getMaxPhase();\n-  int aNumPtr = 8;\n-  int bNumPtr = 8;\n-  int NK = aShape[1];\n-\n-  auto shapePerCTA = getShapePerCTA(dLayout);\n-\n-  auto sizePerThread = getSizePerThread(dLayout);\n-\n-  llvm::outs() << \"strideA: \" << strideAM << \" \" << strideAK << \"\\n\";\n-  llvm::outs() << \"strideB: \" << strideBN << \" \" << strideBK << \"\\n\";\n-  llvm::outs() << \"shapePerCTA: \";\n-  print_vec<unsigned>(shapePerCTA);\n-  llvm::outs() << \"\\n\";\n-\n-  llvm::outs() << \"sizePerThread: \";\n-  print_vec<unsigned>(sizePerThread);\n-  llvm::outs() << \"\\n\";\n-\n-  Value _0 = i32_val(0);\n-\n-  Value mContig = i32_val(sizePerThread[order[1]]);\n-  Value nContig = i32_val(sizePerThread[order[0]]);\n-\n-  // threadId in blocked layout\n-  SmallVector<Value> threadIds;\n-  {\n-    int dim = cShape.size();\n-    threadIds.resize(dim);\n-    for (unsigned k = 0; k < dim - 1; k++) {\n-      Value dimK = i32_val(shapePerCTA[order[k]]);\n-      Value rem = urem(threadId, dimK);\n-      threadId = udiv(threadId, dimK);\n-      threadIds[order[k]] = rem;\n-    }\n-    Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n-    threadIds[order[dim - 1]] = urem(threadId, dimK);\n-  }\n-\n-  Value threadIdM = threadIds[0];\n-  Value threadIdN = threadIds[1];\n-\n-  Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n-  Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n-  SmallVector<Value> aOff(aNumPtr);\n-  for (int i = 0; i < aNumPtr; ++i) {\n-    aOff[i] = add(mul(offA0, i32_val(strideA0)), mul(offA1, i32_val(strideA1)));\n-  }\n-\n-  Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n-  Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n-  SmallVector<Value> bOff(bNumPtr);\n-  for (int i = 0; i < bNumPtr; ++i) {\n-    bOff[i] = add(mul(offB0, i32_val(strideB0)), mul(offB1, i32_val(strideB1)));\n-  }\n-\n-  auto aSmem = getSharedMemoryObjectFromStruct(loc, adaptor.a(), rewriter);\n-  auto bSmem = getSharedMemoryObjectFromStruct(loc, adaptor.b(), rewriter);\n-\n-  Type f32PtrTy = ptr_ty(f32_ty);\n-  SmallVector<Value> aPtrs(aNumPtr);\n-  for (int i = 0; i < aNumPtr; ++i)\n-    aPtrs[i] = gep(f32PtrTy, aSmem.base, aOff[i]);\n-\n-  SmallVector<Value> bPtrs(bNumPtr);\n-  for (int i = 0; i < bNumPtr; ++i)\n-    bPtrs[i] = gep(f32PtrTy, bSmem.base, bOff[i]);\n-\n-  ValueTable has, hbs;\n-  auto cc = getElementsFromStruct(loc, adaptor.c(), rewriter);\n-  SmallVector<Value> ret = cc;\n-  // is this compatible with blocked layout?\n-\n-  for (unsigned k = 0; k < NK; k++) {\n-    int z = 0;\n-    for (unsigned i = 0; i < cShape[order[1]]; i += shapePerCTA[order[1]])\n-      for (unsigned j = 0; j < cShape[order[0]]; j += shapePerCTA[order[0]])\n-        for (unsigned ii = 0; ii < sizePerThread[order[1]]; ++ii)\n-          for (unsigned jj = 0; jj < sizePerThread[order[0]]; ++jj) {\n-            unsigned m = order[0] == 1 ? i : j;\n-            unsigned n = order[0] == 1 ? j : i;\n-            unsigned mm = order[0] == 1 ? ii : jj;\n-            unsigned nn = order[0] == 1 ? jj : ii;\n-            if (!has.count({m + mm, k})) {\n-              Value pa = gep(f32PtrTy, aPtrs[0],\n-                             i32_val((m + mm) * strideAM + k * strideAK));\n-              Value va = load(pa);\n-              has[{m + mm, k}] = va;\n-            }\n-            if (!hbs.count({n + nn, k})) {\n-              Value pb = gep(f32PtrTy, bPtrs[0],\n-                             i32_val((n + nn) * strideBN + k * strideBK));\n-              Value vb = load(pb);\n-              hbs[{n + nn, k}] = vb;\n-            }\n-\n-            llvm::outs() << z << \": \" << m + mm << \" \" << n + nn << \"\\n\";\n-            ret[z] = rewriter.create<LLVM::FMulAddOp>(loc, has[{m + mm, k}],\n-                                                      hbs[{n + nn, k}], ret[z]);\n-            ++z;\n-          }\n-  }\n-\n-  auto res = getStructFromElements(\n-      loc, ret, rewriter,\n-      struct_ty(SmallVector<Type>(ret.size(), ret[0].getType())));\n-  rewriter.replaceOp(op, res);\n-\n-  return success();\n-}\n-\n /// ====================== mma codegen end ============================\n \n Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 1, "deletions": 10, "changes": 11, "file_content_changes": "@@ -122,8 +122,7 @@ class SimplifyConversion : public mlir::RewritePattern {\n       rewriter.replaceOpWithNewOp<triton::gpu::InsertSliceAsyncOp>(\n           op, newType, insert_slice.src(), newArg.getResult(),\n           insert_slice.index(), insert_slice.mask(), insert_slice.other(),\n-          insert_slice.cache(), insert_slice.evict(),\n-          insert_slice.isVolatile(), insert_slice.axis());\n+          insert_slice.cache(), insert_slice.evict(), insert_slice.axis());\n       return mlir::success();\n     }\n     // cvt(extract_slice(x), type2) -> extract_slice(cvt(x, type2))\n@@ -576,14 +575,6 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto oldRetType = dotOp.getResult().getType().cast<RankedTensorType>();\n     if (oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n       return failure();\n-\n-    auto A = dotOp.getOperand(0).getType().cast<RankedTensorType>();\n-    auto B = dotOp.getOperand(1).getType().cast<RankedTensorType>();\n-    // for FMA, should retain the blocked layout.\n-    if (A.getElementType().isF32() && B.getElementType().isF32() &&\n-        !dotOp.allowTF32())\n-      return failure();\n-\n     // get MMA encoding for the given number of warps\n     auto retShape = oldRetType.getShape();\n     auto mod = op->getParentOfType<mlir::ModuleOp>();"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 20, "deletions": 22, "changes": 42, "file_content_changes": "@@ -30,26 +30,26 @@ def matmul_no_scf_kernel(\n # TODO: num_warps could only be 4 for now\n \n \n-# @pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n-#     [128, 256, 32, 4],\n-#     [256, 128, 16, 4],\n-#     [128, 16, 32, 4],\n-#     [32, 128, 64, 4],\n-# ])\n-# def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n-#     a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n-#     b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n-#     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n-#     grid = lambda META: (1, )\n-#     matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n-#                                stride_am=a.stride(0), stride_ak=a.stride(1),\n-#                                stride_bk=b.stride(0), stride_bn=b.stride(1),\n-#                                stride_cm=c.stride(0), stride_cn=c.stride(1),\n-#                                M=SIZE_M, N=SIZE_N, K=SIZE_K,\n-#                                num_warps=NUM_WARPS)\n-#     golden = torch.matmul(a, b)\n-#     torch.set_printoptions(profile=\"full\")\n-#     assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n+@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n+    [128, 256, 32, 4],\n+    [256, 128, 16, 4],\n+    [128, 16, 32, 4],\n+    [32, 128, 64, 4],\n+])\n+def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n+    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n+    grid = lambda META: (1, )\n+    matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n+                               stride_am=a.stride(0), stride_ak=a.stride(1),\n+                               stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                               stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                               M=SIZE_M, N=SIZE_N, K=SIZE_K,\n+                               num_warps=NUM_WARPS)\n+    golden = torch.matmul(a, b)\n+    torch.set_printoptions(profile=\"full\")\n+    assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n \n \n @pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n@@ -76,8 +76,6 @@ def test_gemm_no_scf_int8(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n     bb = b.cpu()\n     golden = torch.matmul(aa.float(), bb.float()).int()\n     torch.set_printoptions(profile=\"full\")\n-    print(\"c\", c.cpu())\n-    print(\"gloden\", golden)\n     torch.testing.assert_close(c.cpu(), golden, check_dtype=False)\n \n "}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 1, "deletions": 20, "changes": 21, "file_content_changes": "@@ -347,7 +347,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_extract_slice\n   func @basic_extract_slice() {\n     // CHECK: llvm.mlir.addressof @global_smem\n-    // CHECK: llvm.extractvalue \n+    // CHECK: llvm.extractvalue\n     // CHECK-NEXT: llvm.extractvalue\n     // CHECK-NEXT: llvm.extractvalue\n     // CHECK-NEXT: llvm.extractvalue\n@@ -811,22 +811,3 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     return\n   }\n }\n-\n-// -----\n-#blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n-#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n-#mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n-#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n-#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func @matmul_fmadot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n-  %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n-    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #blocked>\n-    // CHECK: llvm.intr.fmuladd\n-    %28 = tt.dot %a, %b, %cst {allowTF32 = false, transA = false, transB = false} : tensor<32x16xf32, #shared> * tensor<16x32xf32, #shared> -> tensor<32x32xf32, #blocked>\n-    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n-    %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n-    tt.store %36, %28 : tensor<32x32xf32, #blocked>\n-    return\n-  }\n-}"}]