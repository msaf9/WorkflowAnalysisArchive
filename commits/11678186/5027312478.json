[{"filename": "include/triton/Dialect/Triton/IR/TritonDialect.td", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -37,6 +37,7 @@ def Triton_Dialect : Dialect {\n \n   let hasConstantMaterializer = 1;\n   let useDefaultTypePrinterParser = 1;\n+  let usePropertiesForAttributes = 1;\n }\n \n include \"triton/Dialect/Triton/IR/TritonTypes.td\""}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -479,7 +479,7 @@ def TT_PrintOp : TT_Op<\"print\", [MemoryEffects<[MemWrite]>]>,\n     format are generated automatically from the arguments.\n   }];\n   let assemblyFormat = [{\n-    $prefix attr-dict `:` ($args^ `:` type($args))?\n+    $prefix attr-dict (`:` $args^ `:` type($args))?\n   }];\n }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 38, "deletions": 3, "changes": 41, "file_content_changes": "@@ -77,6 +77,10 @@ struct ConvertLayoutOpConversion\n         dstLayout.isa<DotOperandEncodingAttr>()) {\n       return lowerMmaToDotOperand(op, adaptor, rewriter);\n     }\n+    if (srcLayout.isa<SharedEncodingAttr>() &&\n+        isaDistributedLayout(dstLayout)) {\n+      return lowerSharedToDistributed(op, adaptor, rewriter);\n+    }\n     // TODO: to be implemented\n     llvm_unreachable(\"unsupported layout conversion\");\n     return failure();\n@@ -482,9 +486,40 @@ struct ConvertLayoutOpConversion\n       }\n     }\n \n-    SmallVector<Type> types(outElems, llvmElemTy);\n-    auto *ctx = llvmElemTy.getContext();\n-    Type structTy = struct_ty(types);\n+    Value result =\n+        getTypeConverter()->packLLElements(loc, outVals, rewriter, dstTy);\n+    rewriter.replaceOp(op, result);\n+\n+    return success();\n+  }\n+\n+  LogicalResult\n+  lowerSharedToDistributed(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+                           ConversionPatternRewriter &rewriter) const {\n+    auto loc = op.getLoc();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto srcShape = srcTy.getShape();\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto dstShape = dstTy.getShape();\n+    assert(dstShape.size() == 2 &&\n+           \"Unexpected rank of ConvertLayout(shared->blocked)\");\n+    auto srcSharedLayout = srcTy.getEncoding().cast<SharedEncodingAttr>();\n+    auto dstLayout = dstTy.getEncoding();\n+    auto inOrd = getOrder(srcSharedLayout);\n+\n+    auto smemObj =\n+        getSharedMemoryObjectFromStruct(loc, adaptor.getSrc(), rewriter);\n+    auto elemTy = getTypeConverter()->convertType(dstTy.getElementType());\n+\n+    auto srcStrides =\n+        getStridesFromShapeAndOrder(srcShape, inOrd, loc, rewriter);\n+    auto dstIndices = emitIndices(loc, rewriter, dstLayout, dstTy);\n+\n+    SmallVector<Value> outVals = loadSharedToDistributed(\n+        dst, dstIndices, src, smemObj, elemTy, loc, rewriter);\n+\n     Value result =\n         getTypeConverter()->packLLElements(loc, outVals, rewriter, dstTy);\n     rewriter.replaceOp(op, result);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 49, "deletions": 5, "changes": 54, "file_content_changes": "@@ -359,6 +359,55 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return ret;\n   }\n \n+  SmallVector<Value>\n+  loadSharedToDistributed(Value dst, ArrayRef<SmallVector<Value>> dstIndices,\n+                          Value src, SharedMemoryObject smemObj, Type elemTy,\n+                          Location loc,\n+                          ConversionPatternRewriter &rewriter) const {\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto dstShape = dstTy.getShape();\n+    assert(dstShape.size() == 2 &&\n+           \"Unexpected rank of loadSharedToDistributed\");\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto dstDistributedLayout = dstTy.getEncoding();\n+    if (auto mmaLayout = dstDistributedLayout.dyn_cast<MmaEncodingAttr>()) {\n+      assert((!mmaLayout.isVolta()) &&\n+             \"ConvertLayout Shared->MMAv1 is not supported yet\");\n+    }\n+    auto srcSharedLayout =\n+        srcTy.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n+    auto srcElemTy = srcTy.getElementType();\n+    auto dstElemTy = dstTy.getElementType();\n+    auto inOrd = triton::gpu::getOrder(srcSharedLayout);\n+    auto outOrd = triton::gpu::getOrder(dstDistributedLayout);\n+    unsigned outVec =\n+        inOrd == outOrd\n+            ? triton::gpu::getContigPerThread(dstDistributedLayout)[outOrd[0]]\n+            : 1;\n+    unsigned inVec = srcSharedLayout.getVec();\n+    unsigned minVec = std::min(outVec, inVec);\n+    unsigned outElems = triton::gpu::getTotalElemsPerThread(dstTy);\n+    assert(outElems == dstIndices.size());\n+\n+    DenseMap<unsigned, Value> sharedPtrs = getSwizzledSharedPtrs(\n+        loc, outVec, dstTy, srcSharedLayout, srcElemTy, smemObj, rewriter,\n+        smemObj.offsets, smemObj.strides);\n+    assert(outElems % minVec == 0 && \"Unexpected number of elements\");\n+    unsigned numVecs = outElems / minVec;\n+    auto wordTy = vec_ty(elemTy, minVec);\n+    SmallVector<Value> outVals(outElems);\n+    for (unsigned i = 0; i < numVecs; ++i) {\n+      Value smemAddr = sharedPtrs[i * minVec];\n+      smemAddr = bitcast(smemAddr, ptr_ty(wordTy, 3));\n+      Value valVec = load(smemAddr);\n+      for (unsigned v = 0; v < minVec; ++v) {\n+        Value currVal = extract_element(dstElemTy, valVec, i32_val(v));\n+        outVals[i * minVec + v] = currVal;\n+      }\n+    }\n+    return outVals;\n+  }\n+\n   void storeDistributedToShared(Value src, Value llSrc,\n                                 ArrayRef<Value> dstStrides,\n                                 ArrayRef<SmallVector<Value>> srcIndices,\n@@ -386,16 +435,11 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n             : 1;\n     unsigned outVec = dstSharedLayout.getVec();\n     unsigned minVec = std::min(outVec, inVec);\n-    unsigned perPhase = dstSharedLayout.getPerPhase();\n-    unsigned maxPhase = dstSharedLayout.getMaxPhase();\n     unsigned numElems = triton::gpu::getTotalElemsPerThread(srcTy);\n     assert(numElems == srcIndices.size());\n     auto inVals =\n         getTypeConverter()->unpackLLElements(loc, llSrc, rewriter, srcTy);\n     auto wordTy = vec_ty(elemTy, minVec);\n-    auto elemPtrTy = ptr_ty(elemTy);\n-    Value outVecVal = i32_val(outVec);\n-    Value minVecVal = i32_val(minVec);\n     Value word;\n \n     SmallVector<Value> srcStrides = {dstStrides[0], dstStrides[1]};"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -446,10 +446,11 @@ mlir::LogicalResult mlir::triton::ReduceOp::inferReturnTypes(\n     MLIRContext *context, std::optional<Location> location, ValueRange operands,\n     DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,\n     SmallVectorImpl<Type> &inferredReturnTypes) {\n+  Properties *prop = properties.as<Properties *>();\n+  int axis = prop->axis.getInt();\n   for (auto arg : operands) {\n     auto argTy = arg.getType().cast<RankedTensorType>();\n     auto retEltTy = argTy.getElementType();\n-    int axis = attributes.get(\"axis\").cast<IntegerAttr>().getInt();\n     if (inferReduceReturnShape(argTy, retEltTy, axis, inferredReturnTypes)\n             .failed()) {\n       return failure();\n@@ -557,7 +558,8 @@ mlir::LogicalResult mlir::triton::ExpandDimsOp::inferReturnTypes(\n   auto arg = operands[0];\n   auto argTy = arg.getType().cast<RankedTensorType>();\n   auto retShape = argTy.getShape().vec();\n-  int axis = attributes.get(\"axis\").cast<IntegerAttr>().getInt();\n+  Properties *prop = properties.as<Properties *>();\n+  int axis = prop->axis.getInt();\n   retShape.insert(retShape.begin() + axis, 1);\n   // infer encoding\n   Attribute argEncoding = argTy.getEncoding();\n@@ -740,7 +742,7 @@ void triton::FuncOp::print(OpAsmPrinter &printer) {\n LogicalResult\n triton::CallOp::verifySymbolUses(mlir::SymbolTableCollection &symbolTable) {\n   // Check that the callee attribute was specified.\n-  auto fnAttr = (*this)->getAttrOfType<FlatSymbolRefAttr>(\"callee\");\n+  auto fnAttr = (*this).getProperties().callee;\n   if (!fnAttr)\n     return emitOpError(\"requires a 'callee' symbol reference attribute\");\n   FuncOp fn = symbolTable.lookupNearestSymbolFrom<FuncOp>(*this, fnAttr);"}, {"filename": "python/test/regression/test_functional_regressions.py", "status": "modified", "additions": 68, "deletions": 0, "changes": 68, "file_content_changes": "@@ -1,4 +1,6 @@\n+import numpy as np\n import torch\n+from numpy.random import RandomState\n \n import triton\n import triton.language as tl\n@@ -66,3 +68,69 @@ def chained_matmul_kernel(\n                                 block_k=block_k)\n \n     assert (torch_result == triton_result).all()\n+\n+\n+def test_vecmat():\n+    @triton.jit\n+    def batched_vecmat(\n+        # inputs\n+        A,  # shape: [dim_m, dim_k]\n+        B,  # shape: [dim_m, dim_n, dim_k]\n+        # dimensions\n+        dim_m, dim_n, dim_k,\n+        # outputs\n+        output,\n+        # block information\n+        block_m: tl.constexpr, block_n: tl.constexpr, block_k: tl.constexpr\n+    ):\n+        m_index = tl.program_id(0)\n+        n_index = tl.program_id(1)\n+        # Output tile\n+        output_tile = (m_index * block_m + tl.arange(0, block_m))[:, None] * dim_n \\\n+            + (n_index * block_n + tl.arange(0, block_n))[None, :]\n+\n+        vecmat = tl.zeros([block_m, block_n], dtype=A.dtype.element_ty)\n+        k_blocks = dim_k // block_k\n+        for k_index in range(k_blocks):\n+            # Load A tile\n+            a_tile = (m_index * block_m + tl.arange(0, block_m))[:, None] * dim_k \\\n+                + (k_index * block_k + tl.arange(0, block_k))[None, :]\n+            a = tl.load(A + a_tile)\n+\n+            # Load B tile, transposed to [n, m, k] in order to broadcast A on a\n+            # leading dimension.\n+            b_tile = (m_index * block_m + tl.arange(0, block_m))[None, :, None] * dim_n * dim_k \\\n+                + (n_index * block_n + tl.arange(0, block_n))[:, None, None] * dim_k \\\n+                + (k_index * block_k + tl.arange(0, block_k))[None, None, :]\n+            b = tl.load(B + b_tile)\n+\n+            expanded_a, _ = tl.broadcast(a, b)\n+            vecmat += tl.trans(tl.sum(expanded_a * b, axis=2))\n+\n+        tl.store(output + output_tile, vecmat)\n+\n+    M, N, K = 128, 128, 128\n+    block_m, block_n, block_k = 16, 32, 64\n+\n+    rs = RandomState(17)\n+    A_vec = rs.randint(0, 4, (M, K)).astype('float32')\n+    B_vec = rs.randint(0, 4, (M, N, K)).astype('float32')\n+    A = A_vec\n+    B = B_vec\n+\n+    A_tri = torch.tensor(A, device='cuda')\n+    B_tri = torch.tensor(B, device='cuda')\n+    C_tri = torch.zeros((M, N), dtype=torch.float32, device='cuda')\n+\n+    grid = (M // block_m, N // block_n)\n+\n+    batched_vecmat[grid](A_tri, B_tri, M, N, K, C_tri,\n+                         block_m=block_m, block_n=block_n, block_k=block_k,\n+                         num_warps=4, num_stages=1)\n+\n+    A_expanded = A[:, np.newaxis, :]\n+    A_broadcasted = np.broadcast_to(A_expanded, (M, N, K))\n+    AB = A_broadcasted * B\n+    C_ref = np.sum(AB, axis=2)\n+\n+    np.testing.assert_allclose(C_ref, C_tri.cpu().numpy(), rtol=0.01, atol=1e-3)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 45, "deletions": 8, "changes": 53, "file_content_changes": "@@ -130,6 +130,17 @@ def __str__(self):\n         return f\"#triton_gpu.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}}}>\"\n \n \n+class SharedLayout:\n+    def __init__(self, vec, per_phase, max_phase, order):\n+        self.vec = str(vec)\n+        self.per_phase = str(per_phase)\n+        self.max_phase = str(max_phase)\n+        self.order = str(order)\n+\n+    def __str__(self):\n+        return f\"#triton_gpu.shared<{{vec={self.vec}, perPhase={self.per_phase}, maxPhase={self.max_phase}, order={self.order}}}>\"\n+\n+\n @pytest.mark.parametrize(\"dtype_x\", list(dtypes) + [\"bfloat16\"])\n def test_empty_kernel(dtype_x, device='cuda'):\n     SIZE = 128\n@@ -2810,22 +2821,49 @@ def kernel(Out):\n     BlockedLayout([4, 4], [1, 32], [4, 1], [1, 0])\n ]\n \n+intermediate_layouts = [\n+    None,\n+    SharedLayout(1, 1, 1, [1, 0]),\n+    SharedLayout(4, 2, 4, [1, 0]),\n+    SharedLayout(2, 2, 4, [1, 0]),\n+]\n+\n \n @pytest.mark.parametrize(\"shape\", [(128, 128)])\n @pytest.mark.parametrize(\"dtype\", ['float16'])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n+@pytest.mark.parametrize(\"interm_layout\", intermediate_layouts)\n @pytest.mark.parametrize(\"dst_layout\", layouts)\n-def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n+def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device='cuda'):\n     if str(src_layout) == str(dst_layout):\n         pytest.skip()\n     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):\n         pytest.skip()\n \n-    ir = f\"\"\"\n-#src = {src_layout}\n-#dst = {dst_layout}\n-\"\"\" + \"\"\"\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+    layouts = f\"\"\"\n+    #src = {src_layout}\n+    #dst = {dst_layout}\n+    \"\"\" if interm_layout is None else f\"\"\"\n+    #src = {src_layout}\n+    #interm = {interm_layout}\n+    #dst = {dst_layout}\n+    \"\"\"\n+\n+    conversion = f\"\"\"\n+    %12 = triton_gpu.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n+    %13 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n+    \"\"\" if interm_layout is None else f\"\"\"\n+    %15 = triton_gpu.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #interm>\n+    %16 = triton_gpu.convert_layout %15 : (tensor<128x128xi32, #interm>) -> tensor<128x128xi32, #src>\n+    %17 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #interm>\n+    %18 = triton_gpu.convert_layout %17 : (tensor<128x128xf16, #interm>) -> tensor<128x128xf16, #src>\n+\n+    %12 = triton_gpu.convert_layout %16 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n+    %13 = triton_gpu.convert_layout %18 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n+    \"\"\"\n+\n+    ir = layouts + \"\"\"\n+    module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   tt.func public @kernel_0d1d(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n     %cst = arith.constant dense<128> : tensor<128x1xi32, #src>\n     %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #src}>>\n@@ -2840,8 +2878,7 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n     %10 = tt.addptr %2, %9 : tensor<128x128x!tt.ptr<f16>, #src>, tensor<128x128xi32, #src>\n     %11 = tt.load %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf16, #src>\n     %3 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>, #dst>\n-    %12 = triton_gpu.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n-    %13 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n+    \"\"\" + conversion + \"\"\"\n     %14 = tt.addptr %3, %12 : tensor<128x128x!tt.ptr<f16>, #dst>, tensor<128x128xi32, #dst>\n     tt.store %14, %13 : tensor<128x128xf16, #dst>\n     tt.return"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 39, "deletions": 6, "changes": 45, "file_content_changes": "@@ -76,40 +76,64 @@ tt.func @load_store_ops_scalar(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   tt.return\n }\n \n+// CHECK-LABEL: reduce_ops_infer\n tt.func @reduce_ops_infer(%ptr: !tt.ptr<f32>, %v : tensor<1x2x4xf32>) {\n   // Test if reduce ops infer types correctly\n \n-  // CHECK: }) {axis = 0 : i32} : (tensor<1x2x4xf32>) -> tensor<2x4xf32>\n+  // CHECK: tt.reduce\n+  // CHECK-SAME: axis = 0\n+  // CHECK: tt.reduce.return\n+  // CHECK-NEXT: (tensor<1x2x4xf32>) -> tensor<2x4xf32>\n   %a = \"tt.reduce\" (%v) ({\n   ^bb0(%arg0: f32, %arg1: f32):\n     %add = arith.addf %arg0, %arg1 : f32\n     tt.reduce.return %add : f32\n   }) {axis = 0 : i32}  : (tensor<1x2x4xf32>) -> tensor<2x4xf32>\n-  // CHECK: }) {axis = 1 : i32}  : (tensor<1x2x4xf32>) -> tensor<1x4xf32>\n+\n+  // CHECK: tt.reduce\n+  // CHECK-SAME: axis = 1\n+  // CHECK: tt.reduce.return\n+  // CHECK-NEXT: (tensor<1x2x4xf32>) -> tensor<1x4xf32>\n   %b = \"tt.reduce\" (%v) ({\n   ^bb0(%arg0: f32, %arg1: f32):\n     %add = arith.addf %arg0, %arg1 : f32\n     tt.reduce.return %add : f32\n   }) {axis = 1 : i32}  : (tensor<1x2x4xf32>) -> tensor<1x4xf32>\n-  // CHECK: }) {axis = 2 : i32}  : (tensor<1x2x4xf32>) -> tensor<1x2xf32>\n+\n+  // CHECK: tt.reduce\n+  // CHECK-SAME: axis = 2\n+  // CHECK: tt.reduce.return\n+  // CHECK-NEXT: (tensor<1x2x4xf32>) -> tensor<1x2xf32>\n   %c = \"tt.reduce\" (%v) ({\n   ^bb0(%arg0: f32, %arg1: f32):\n     %add = arith.addf %arg0, %arg1 : f32\n     tt.reduce.return %add : f32\n   }) {axis = 2 : i32}  : (tensor<1x2x4xf32>) -> tensor<1x2xf32>\n-  // CHECK: }) {axis = 1 : i32}  : (tensor<1x4xf32>) -> tensor<1xf32>\n+\n+  // CHECK: tt.reduce\n+  // CHECK-SAME: axis = 1\n+  // CHECK: tt.reduce.return\n+  // CHECK-NEXT: (tensor<1x4xf32>) -> tensor<1xf32>\n   %e = \"tt.reduce\" (%b) ({\n   ^bb0(%arg0: f32, %arg1: f32):\n     %add = arith.addf %arg0, %arg1 : f32\n     tt.reduce.return %add : f32\n   }) {axis = 1 : i32}  : (tensor<1x4xf32>) -> tensor<1xf32>\n-  // CHECK: }) {axis = 0 : i32}  : (tensor<2x4xf32>) -> tensor<4xf32>\n+\n+  // CHECK: tt.reduce\n+  // CHECK-SAME: axis = 0\n+  // CHECK: tt.reduce.return\n+  // CHECK-NEXT: (tensor<2x4xf32>) -> tensor<4xf32>\n   %f = \"tt.reduce\" (%a) ({\n   ^bb0(%arg0: f32, %arg1: f32):\n     %add = arith.addf %arg0, %arg1 : f32\n     tt.reduce.return %add : f32\n   }) {axis = 0 : i32}  : (tensor<2x4xf32>) -> tensor<4xf32>\n-  // CHECK: }) {axis = 0 : i32}  : (tensor<4xf32>) -> f32\n+\n+  // CHECK: tt.reduce\n+  // CHECK-SAME: axis = 0\n+  // CHECK: tt.reduce.return\n+  // CHECK-NEXT: (tensor<4xf32>) -> f32\n   %g = \"tt.reduce\" (%f) ({\n   ^bb0(%arg0: f32, %arg1: f32):\n     %add = arith.addf %arg0, %arg1 : f32\n@@ -154,3 +178,12 @@ tt.func @dot_ops_infer(%ptr: !tt.ptr<f32>, %v : f32) {\n   tt.store %ptr1x1, %r4 : tensor<1x1xf32>\n   tt.return\n }\n+\n+// CHECK-LABEL: @print_no_arg\n+tt.func @print_no_arg(%arg0: !tt.ptr<f32>) {\n+// CHECK: tt.print \"test\"\n+  tt.print \"test\"\n+  %0 = tt.load %arg0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : f32\n+  tt.store %arg0, %0 {cache = 1 : i32, evict = 1 : i32} : f32\n+  tt.return\n+}"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -1106,7 +1106,9 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n // Check if the SimplifyReduceCvt handles convert_layout lifted from the for loop.\n // CHECK-LABEL: reduce_cvt2\n // Match the reduction\n-// CHECK: }) {axis = 1 : i32} : (tensor<1x256xf32, #blocked>) -> tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+// CHECK: tt.reduce\n+// CHECK-SAME: axis = 1\n+// CHECK: (tensor<1x256xf32, #blocked>) -> tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n // CHECK-NEXT: triton_gpu.convert_layout\n // CHECK-NOT: triton_gpu.convert_layout\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>"}]