[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "file_content_changes": "@@ -332,7 +332,7 @@ MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> offs,\n     if (isActualTrans)\n       std::swap(vptrs[1], vptrs[2]);\n     // pack loaded vectors into 4 32-bit values\n-    int inc = isActualTrans ? 1 : kWidth;\n+    int inc = needTrans ? 1 : kWidth;\n     VectorType packedTy = vec_ty(int_ty(8 * elemBytes), inc);\n     int canonBits = std::min(32, 8 * elemBytes * inc);\n     int canonWidth = (8 * elemBytes * inc) / canonBits;\n@@ -462,9 +462,6 @@ getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n   auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n   const int perPhase = sharedLayout.getPerPhase();\n   const int maxPhase = sharedLayout.getMaxPhase();\n-  llvm::outs() << isA << \" \" << sharedLayout.getVec() << \" \"\n-               << sharedLayout.getPerPhase() << \" \"\n-               << sharedLayout.getMaxPhase() << \"\\n\";\n   const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n   auto order = sharedLayout.getOrder();\n "}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -978,10 +978,10 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    // CHECK-SAME: (vector<1xf32>, vector<1xf32>, vector<1xf32>, vector<1xf32>)\n+    // CHECK-SAME: (i32, i32, i32, i32)\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    // CHECK-SAME: (vector<1xf32>, vector<1xf32>, vector<1xf32>, vector<1xf32>)\n+    // CHECK-SAME: (i32, i32, i32, i32)\n     %a_mat = triton_gpu.convert_layout %a : (tensor<32x16xf32, #shared>) -> tensor<32x16xf32, #dot_operand_a>\n     %b_mat = triton_gpu.convert_layout %b : (tensor<16x32xf32, #shared>) -> tensor<16x32xf32, #dot_operand_b>\n "}]