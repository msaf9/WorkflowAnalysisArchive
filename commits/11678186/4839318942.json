[{"filename": "include/triton/Analysis/Allocation.h", "status": "modified", "additions": 76, "deletions": 19, "changes": 95, "file_content_changes": "@@ -1,6 +1,7 @@\n #ifndef TRITON_ANALYSIS_ALLOCATION_H\n #define TRITON_ANALYSIS_ALLOCATION_H\n \n+#include \"triton/Analysis/Utility.h\"\n #include \"llvm/ADT/DenseMap.h\"\n #include \"llvm/ADT/MapVector.h\"\n #include \"llvm/ADT/SetVector.h\"\n@@ -54,13 +55,18 @@ class Allocation {\n   /// A unique identifier for shared memory buffers\n   using BufferId = size_t;\n   using BufferIdSetT = DenseSet<BufferId>;\n+  using FuncAllocMapT = CallGraph<Allocation>::FuncDataMapT;\n \n   static constexpr BufferId InvalidBufferId =\n       std::numeric_limits<BufferId>::max();\n \n+  Allocation() = default;\n   /// Creates a new Allocation analysis that computes the shared memory\n   /// information for all associated shared memory values.\n-  Allocation(Operation *operation) : operation(operation) { run(); }\n+  explicit Allocation(Operation *operation) : operation(operation) {}\n+\n+  /// Runs allocation analysis on the given top-level operation.\n+  void run(FuncAllocMapT &funcAllocMap);\n \n   /// Returns the operation this analysis was constructed from.\n   Operation *getOperation() const { return operation; }\n@@ -75,6 +81,12 @@ class Allocation {\n     return bufferSet.at(bufferId).size;\n   }\n \n+  /// Returns the allocated interval of the given buffer.\n+  Interval<size_t> getAllocatedInterval(BufferId bufferId) const {\n+    auto &buffer = bufferSet.at(bufferId);\n+    return Interval<size_t>(buffer.offset, buffer.offset + buffer.size);\n+  }\n+\n   /// Returns the buffer id of the given value.\n   /// This interface only returns the allocated buffer id.\n   /// If you want to get all the buffer ids that are associated with the given\n@@ -104,26 +116,28 @@ class Allocation {\n   BufferId getBufferId(Operation *operation) const {\n     if (opScratch.count(operation)) {\n       return opScratch.lookup(operation)->id;\n+    } else if (opVirtual.count(operation)) {\n+      return opVirtual.lookup(operation)->id;\n     } else {\n       return InvalidBufferId;\n     }\n   }\n \n+  /// Returns the size of the given buffer is a virtual buffer.\n+  bool isVirtualBuffer(BufferId bufferId) const {\n+    return bufferSet.at(bufferId).kind == BufferT::BufferKind::Virtual;\n+  }\n+\n   /// Returns the size of total shared memory allocated\n   size_t getSharedMemorySize() const { return sharedMemorySize; }\n \n-  bool isIntersected(BufferId lhsId, BufferId rhsId) const {\n-    if (lhsId == InvalidBufferId || rhsId == InvalidBufferId)\n-      return false;\n-    auto lhsBuffer = bufferSet.at(lhsId);\n-    auto rhsBuffer = bufferSet.at(rhsId);\n-    return lhsBuffer.intersects(rhsBuffer);\n-  }\n-\n private:\n   /// A class that represents a shared memory buffer\n   struct BufferT {\n-    enum class BufferKind { Explicit, Scratch };\n+    /// Explicit: triton_gpu.alloc_tensor\n+    /// Scratch: triton_gpu.convert_layout\n+    /// Virtual: triton.call\n+    enum class BufferKind { Explicit, Scratch, Virtual };\n \n     /// MT: thread-safe\n     inline static std::atomic<BufferId> nextId = 0;\n@@ -142,12 +156,6 @@ class Allocation {\n     BufferT(BufferKind kind, size_t size) : BufferT(kind, size, 0) {}\n     BufferT(BufferKind kind, size_t size, size_t offset)\n         : kind(kind), id(nextId++), size(size), offset(offset) {}\n-\n-    bool intersects(const BufferT &other) const {\n-      return Interval<size_t>(offset, offset + size)\n-          .intersects(\n-              Interval<size_t>(other.offset, other.offset + other.size));\n-    }\n   };\n \n   /// Op -> Scratch Buffer\n@@ -158,8 +166,6 @@ class Allocation {\n   using AliasBufferMapT = llvm::MapVector<Value, llvm::SetVector<BufferT *>>;\n   /// BufferId -> Buffer\n   using BufferSetT = std::map<BufferId, BufferT>;\n-  /// Runs allocation analysis on the given top-level operation.\n-  void run();\n \n private:\n   template <BufferT::BufferKind Kind, typename KeyType, typename... Args>\n@@ -168,6 +174,8 @@ class Allocation {\n     bufferSet[buffer.id] = std::move(buffer);\n     if constexpr (Kind == BufferT::BufferKind::Explicit) {\n       valueBuffer[key] = &bufferSet[buffer.id];\n+    } else if constexpr (Kind == BufferT::BufferKind::Virtual) {\n+      opVirtual[key] = &bufferSet[buffer.id];\n     } else {\n       opScratch[key] = &bufferSet[buffer.id];\n     }\n@@ -178,8 +186,9 @@ class Allocation {\n   }\n \n private:\n-  Operation *operation;\n+  Operation *operation = nullptr;\n   OpScratchMapT opScratch;\n+  OpScratchMapT opVirtual;\n   ValueBufferMapT valueBuffer;\n   AliasBufferMapT aliasBuffer;\n   BufferSetT bufferSet;\n@@ -188,6 +197,54 @@ class Allocation {\n   friend class triton::AllocationAnalysis;\n };\n \n+/// Static analysis that computes the allocation of shared memory buffers\n+/// of the entire call graph.\n+/// The allocation is performed in a post-order walk of the call graph.\n+/// Each call op is treated like convert_layout that allocates a scratch buffer.\n+/// At each call, we compute the start offset of the scratch buffer and pass it\n+/// as an argument to the callee.\n+class ModuleAllocation : public CallGraph<Allocation> {\n+public:\n+  using FuncOffsetMapT = DenseMap<FunctionOpInterface, Value>;\n+\n+  explicit ModuleAllocation(ModuleOp moduleOp)\n+      : CallGraph<Allocation>(moduleOp) {\n+    walk<WalkOrder::PreOrder, WalkOrder::PostOrder>(\n+        // Pre-order edge walk callback\n+        [](CallOpInterface callOp, FunctionOpInterface funcOp) {},\n+        // Post-order node walk callback\n+        [&](FunctionOpInterface funcOp) {\n+          auto [iter, inserted] = funcMap.try_emplace(funcOp, funcOp);\n+          if (inserted)\n+            iter->second.run(funcMap);\n+        });\n+  }\n+\n+  size_t getSharedMemorySize() {\n+    size_t size = 0;\n+    for (auto funcOp : getRoots()) {\n+      auto *alloc = getFuncData(funcOp);\n+      size = std::max(size, alloc->getSharedMemorySize());\n+    }\n+    return size;\n+  }\n+\n+  size_t getSharedMemorySize(FunctionOpInterface funcOp) {\n+    return getFuncData(funcOp)->getSharedMemorySize();\n+  }\n+\n+  void setFunctionSharedMemoryValue(FunctionOpInterface funcOp, Value value) {\n+    sharedMemoryValue[funcOp] = value;\n+  }\n+\n+  Value getFunctionSharedMemoryBase(FunctionOpInterface funcOp) {\n+    return sharedMemoryValue[funcOp];\n+  }\n+\n+private:\n+  FuncOffsetMapT sharedMemoryValue;\n+};\n+\n template <typename T> Interval(T, T) -> Interval<T>;\n \n } // namespace mlir"}, {"filename": "include/triton/Analysis/AxisInfo.h", "status": "modified", "additions": 55, "deletions": 0, "changes": 55, "file_content_changes": "@@ -286,16 +286,71 @@ class AxisInfoAnalysis\n   AxisInfoAnalysis(DataFlowSolver &solver);\n   using dataflow::SparseDataFlowAnalysis<\n       dataflow::Lattice<AxisInfo>>::getLatticeElement;\n+  using FuncAxisInfoMapT = DenseMap<FunctionOpInterface, AxisInfo>;\n \n   void visitOperation(Operation *op,\n                       ArrayRef<const dataflow::Lattice<AxisInfo> *> operands,\n                       ArrayRef<dataflow::Lattice<AxisInfo> *> results) override;\n+};\n+\n+/// Module level axis info analysis based on the call graph, assuming that we\n+/// do not have recursive functions.\n+/// Since each function will be called multiple times, we need to\n+/// calculate the axis info based on the axis info of all the callers.\n+/// In the future, we can perform optimization using function cloning so that\n+/// each call site will have unique axis info.\n+using AxisInfoMapT = DenseMap<Value, AxisInfo>;\n+class ModuleAxisInfoAnalysis : public CallGraph<AxisInfoMapT> {\n+public:\n+  explicit ModuleAxisInfoAnalysis(ModuleOp moduleOp)\n+      : CallGraph<AxisInfoMapT>(moduleOp) {\n+    SmallVector<FunctionOpInterface> funcs;\n+    for (auto root : getRoots()) {\n+      walk<WalkOrder::PreOrder, WalkOrder::PostOrder>(\n+          // Pre-order edge walk callback\n+          [](CallOpInterface callOp, FunctionOpInterface funcOp) {},\n+          // Post-order node walk callback\n+          [&](FunctionOpInterface funcOp) {\n+            funcs.push_back(funcOp);\n+            funcMap.try_emplace(funcOp, AxisInfoMapT{});\n+          });\n+    }\n+    SetVector<FunctionOpInterface> sortedFuncs(funcs.begin(), funcs.end());\n+    SymbolTableCollection symbolTable;\n+    for (auto funcOp : llvm::reverse(sortedFuncs)) {\n+      initialize(funcOp);\n+      funcOp.walk([&](CallOpInterface callOp) {\n+        auto callee =\n+            dyn_cast<FunctionOpInterface>(callOp.resolveCallable(&symbolTable));\n+        update(callOp, callee);\n+      });\n+    }\n+  }\n+\n+  AxisInfo *getAxisInfo(Value value) {\n+    auto funcOp =\n+        value.getParentRegion()->getParentOfType<FunctionOpInterface>();\n+    auto *axisInfoMap = getFuncData(funcOp);\n+    if (!axisInfoMap) {\n+      return nullptr;\n+    }\n+    auto it = axisInfoMap->find(value);\n+    if (it == axisInfoMap->end()) {\n+      return nullptr;\n+    }\n+    return &(it->second);\n+  }\n \n   unsigned getPtrContiguity(Value ptr);\n \n   unsigned getPtrAlignment(Value ptr);\n \n   unsigned getMaskAlignment(Value mask);\n+\n+private:\n+  void initialize(FunctionOpInterface funcOp);\n+\n+  void update(CallOpInterface callOp, FunctionOpInterface funcOp);\n };\n \n } // namespace mlir"}, {"filename": "include/triton/Analysis/Membar.h", "status": "modified", "additions": 93, "deletions": 69, "changes": 162, "file_content_changes": "@@ -4,15 +4,70 @@\n #include \"Allocation.h\"\n #include \"llvm/ADT/SmallPtrSet.h\"\n \n+#include <set>\n+\n namespace mlir {\n \n class OpBuilder;\n \n+struct BlockInfo {\n+  using BufferIdSetT = Allocation::BufferIdSetT;\n+  using IntervalSetT = std::set<Interval<size_t>>;\n+\n+  IntervalSetT syncReadIntervals;\n+  IntervalSetT syncWriteIntervals;\n+\n+  BlockInfo() = default;\n+\n+  /// Unions two BlockInfo objects.\n+  BlockInfo &join(const BlockInfo &other) {\n+    syncReadIntervals.insert(other.syncReadIntervals.begin(),\n+                             other.syncReadIntervals.end());\n+    syncWriteIntervals.insert(other.syncWriteIntervals.begin(),\n+                              other.syncWriteIntervals.end());\n+    return *this;\n+  }\n+\n+  /// Returns true if intervals in two BlockInfo objects are intersected.\n+  bool isIntersected(const BlockInfo &other) const {\n+    return /*RAW*/ isIntersected(syncWriteIntervals, other.syncReadIntervals) ||\n+           /*WAR*/\n+           isIntersected(syncReadIntervals, other.syncWriteIntervals) ||\n+           /*WAW*/\n+           isIntersected(syncWriteIntervals, other.syncWriteIntervals);\n+  }\n+\n+  /// Clears the intervals because a barrier is inserted.\n+  void sync() {\n+    syncReadIntervals.clear();\n+    syncWriteIntervals.clear();\n+  }\n+\n+  /// Compares two BlockInfo objects.\n+  bool operator==(const BlockInfo &other) const {\n+    return syncReadIntervals == other.syncReadIntervals &&\n+           syncWriteIntervals == other.syncWriteIntervals;\n+  }\n+\n+  bool operator!=(const BlockInfo &other) const { return !(*this == other); }\n+\n+private:\n+  bool isIntersected(const IntervalSetT &lhsIntervalSet,\n+                     const IntervalSetT &rhsIntervalSet) const {\n+    for (auto &lhs : lhsIntervalSet)\n+      for (auto &rhs : rhsIntervalSet)\n+        if (lhs.intersects(rhs))\n+          return true;\n+    return false;\n+  }\n+};\n+\n //===----------------------------------------------------------------------===//\n // Shared Memory Barrier Analysis\n //===----------------------------------------------------------------------===//\n class MembarAnalysis {\n public:\n+  using FuncBlockInfoMapT = CallGraph<BlockInfo>::FuncDataMapT;\n   /// Creates a new Membar analysis that generates the shared memory barrier\n   /// in the following circumstances:\n   /// - RAW: If a shared memory write is followed by a shared memory read, and\n@@ -26,75 +81,14 @@ class MembarAnalysis {\n   /// a shared memory read. If the temporary storage is written but not read,\n   /// it is considered as the problem of the operation itself but not the membar\n   /// analysis.\n-  /// The following circumstances are not considered yet:\n-  /// - Double buffers\n-  /// - N buffers\n-  MembarAnalysis(Allocation *allocation) : allocation(allocation) {}\n+  MembarAnalysis() = default;\n+  explicit MembarAnalysis(Allocation *allocation) : allocation(allocation) {}\n \n   /// Runs the membar analysis to the given operation, inserts a barrier if\n   /// necessary.\n-  void run();\n+  void run(FuncBlockInfoMapT &funcBlockInfoMap);\n \n private:\n-  struct BlockInfo {\n-    using BufferIdSetT = Allocation::BufferIdSetT;\n-\n-    BufferIdSetT syncReadBuffers;\n-    BufferIdSetT syncWriteBuffers;\n-\n-    BlockInfo() = default;\n-    BlockInfo(const BufferIdSetT &syncReadBuffers,\n-              const BufferIdSetT &syncWriteBuffers)\n-        : syncReadBuffers(syncReadBuffers), syncWriteBuffers(syncWriteBuffers) {\n-    }\n-\n-    /// Unions two BlockInfo objects.\n-    BlockInfo &join(const BlockInfo &other) {\n-      syncReadBuffers.insert(other.syncReadBuffers.begin(),\n-                             other.syncReadBuffers.end());\n-      syncWriteBuffers.insert(other.syncWriteBuffers.begin(),\n-                              other.syncWriteBuffers.end());\n-      return *this;\n-    }\n-\n-    /// Returns true if buffers in two BlockInfo objects are intersected.\n-    bool isIntersected(const BlockInfo &other, Allocation *allocation) const {\n-      return /*RAW*/ isIntersected(syncWriteBuffers, other.syncReadBuffers,\n-                                   allocation) ||\n-             /*WAR*/\n-             isIntersected(syncReadBuffers, other.syncWriteBuffers,\n-                           allocation) ||\n-             /*WAW*/\n-             isIntersected(syncWriteBuffers, other.syncWriteBuffers,\n-                           allocation);\n-    }\n-\n-    /// Clears the buffers because a barrier is inserted.\n-    void sync() {\n-      syncReadBuffers.clear();\n-      syncWriteBuffers.clear();\n-    }\n-\n-    /// Compares two BlockInfo objects.\n-    bool operator==(const BlockInfo &other) const {\n-      return syncReadBuffers == other.syncReadBuffers &&\n-             syncWriteBuffers == other.syncWriteBuffers;\n-    }\n-\n-    bool operator!=(const BlockInfo &other) const { return !(*this == other); }\n-\n-  private:\n-    /// Returns true if buffers in two sets are intersected.\n-    bool isIntersected(const BufferIdSetT &lhs, const BufferIdSetT &rhs,\n-                       Allocation *allocation) const {\n-      return std::any_of(lhs.begin(), lhs.end(), [&](auto lhsId) {\n-        return std::any_of(rhs.begin(), rhs.end(), [&](auto rhsId) {\n-          return allocation->isIntersected(lhsId, rhsId);\n-        });\n-      });\n-    }\n-  };\n-\n   /// Applies the barrier analysis based on the SCF dialect, in which each\n   /// region has a single basic block only.\n   /// Example:\n@@ -109,18 +103,48 @@ class MembarAnalysis {\n   ///        op6\n   ///   op7\n   /// TODO: Explain why we don't use ForwardAnalysis:\n-  void resolve(Operation *operation, OpBuilder *builder);\n+  void resolve(FunctionOpInterface funcOp, FuncBlockInfoMapT *funcBlockInfoMap,\n+               OpBuilder *builder);\n \n   /// Updates the BlockInfo operation based on the operation.\n-  void update(Operation *operation, BlockInfo *blockInfo, OpBuilder *builder);\n+  void update(Operation *operation, BlockInfo *blockInfo,\n+              FuncBlockInfoMapT *funcBlockInfoMap, OpBuilder *builder);\n \n   /// Collects the successors of the terminator\n   void visitTerminator(Operation *operation, SmallVector<Block *> &successors);\n \n private:\n-  Allocation *allocation;\n-  DenseMap<Block *, BlockInfo> inputBlockInfoMap;\n-  DenseMap<Block *, BlockInfo> outputBlockInfoMap;\n+  Allocation *allocation = nullptr;\n+};\n+\n+/// Postorder traversal on the callgraph to insert membar instructions\n+/// of each function.\n+/// Each function maintains a BlockInfo map that includes all potential buffers\n+/// after returning. This way users do not have to explicitly insert membars\n+/// before and after function calls, but might be a bit conservative.\n+class ModuleMembarAnalysis : public CallGraph<BlockInfo> {\n+public:\n+  ModuleMembarAnalysis(ModuleAllocation *moduleAllocation)\n+      : CallGraph<BlockInfo>(moduleAllocation->getModuleOp()),\n+        moduleAllocation(moduleAllocation) {}\n+\n+  void run() {\n+    walk<WalkOrder::PreOrder, WalkOrder::PostOrder>(\n+        // Pre-order walk callback\n+        [](CallOpInterface callOp, FunctionOpInterface funcOp) {},\n+        // Post-order walk callback\n+        [&](FunctionOpInterface funcOp) {\n+          auto *allocation = moduleAllocation->getFuncData(funcOp);\n+          auto [it, inserted] = funcMap.try_emplace(funcOp, BlockInfo());\n+          if (inserted) {\n+            MembarAnalysis analysis(allocation);\n+            analysis.run(funcMap);\n+          }\n+        });\n+  }\n+\n+private:\n+  ModuleAllocation *moduleAllocation;\n };\n \n } // namespace mlir"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 141, "deletions": 2, "changes": 143, "file_content_changes": "@@ -116,14 +116,153 @@ bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy);\n SetVector<Operation *>\n multiRootTopologicalSort(const SetVector<Operation *> &toSort);\n \n-// This uses the toplogicalSort above\n+/// This uses the toplogicalSort above\n SetVector<Operation *>\n multiRootGetSlice(Operation *op, TransitiveFilter backwardFilter = nullptr,\n                   TransitiveFilter forwardFilter = nullptr);\n \n-// Create a basic DataFlowSolver with constant and dead code analysis included.\n+/// Create a basic DataFlowSolver with constant and dead code analysis included.\n std::unique_ptr<DataFlowSolver> createDataFlowSolver();\n \n+/// This class represents a call graph for a given ModuleOp and holds\n+/// data of type T associated with each FunctionOpInterface.\n+template <typename T> class CallGraph {\n+public:\n+  using FuncDataMapT = DenseMap<FunctionOpInterface, T>;\n+\n+  /// Constructor that builds the call graph for the given moduleOp.\n+  explicit CallGraph(ModuleOp moduleOp) : moduleOp(moduleOp) { build(); }\n+\n+  /// Walks the call graph and applies the provided update functions\n+  /// to the edges and nodes.\n+  template <WalkOrder UpdateEdgeOrder = WalkOrder::PreOrder,\n+            WalkOrder UpdateNodeOrder = WalkOrder::PreOrder,\n+            typename UpdateEdgeFn, typename UpdateNodeFn>\n+  void walk(UpdateEdgeFn updateEdgeFn, UpdateNodeFn updateNodeFn) {\n+    DenseSet<FunctionOpInterface> visited;\n+    for (auto root : roots) {\n+      doWalk<UpdateEdgeOrder, UpdateNodeOrder>(root, visited, updateEdgeFn,\n+                                               updateNodeFn);\n+    }\n+  }\n+\n+  /// Retrieves the data associated with a function\n+  T *getFuncData(FunctionOpInterface funcOp) {\n+    if (funcMap.count(funcOp)) {\n+      return &funcMap[funcOp];\n+    }\n+    return nullptr;\n+  }\n+\n+  /// Getters\n+  ModuleOp getModuleOp() const { return moduleOp; }\n+  SmallVector<FunctionOpInterface> getRoots() const { return roots; }\n+  size_t getNumFunctions() const { return funcMap.size(); }\n+\n+  /// Returns true if the given function is a root.\n+  bool isRoot(FunctionOpInterface funcOp) const {\n+    return llvm::is_contained(roots, funcOp);\n+  }\n+\n+  /// Maps the data and the graph nodes associated with a funcOp to a\n+  /// targetFuncOp.\n+  template <typename FROM, typename TO>\n+  void mapFuncOp(FROM funcOp, TO targetFuncOp) {\n+    // Iterate over graph and replace\n+    for (auto &kv : graph) {\n+      for (auto &edge : kv.second) {\n+        if (edge.second == funcOp) {\n+          edge.second = targetFuncOp;\n+        }\n+      }\n+    }\n+    graph[targetFuncOp] = graph[funcOp];\n+    // Replace in roots\n+    for (auto it = roots.begin(); it != roots.end(); ++it) {\n+      if (*it == funcOp) {\n+        *it = targetFuncOp;\n+        break;\n+      }\n+    }\n+    // Replace in funcMap\n+    funcMap[targetFuncOp] = funcMap[funcOp];\n+  }\n+\n+  /// Maps the graph edges associated with a callOp to a targetCallOp.\n+  template <typename FROM, typename TO>\n+  void mapCallOp(FROM callOp, TO targetCallOp) {\n+    // Iterate over graph and replace\n+    for (auto &kv : graph) {\n+      for (auto &edge : kv.second) {\n+        if (edge.first == callOp) {\n+          edge.first = targetCallOp;\n+        }\n+      }\n+    }\n+  }\n+\n+private:\n+  void build() {\n+    SymbolTableCollection symbolTable;\n+    DenseSet<FunctionOpInterface> visited;\n+    // Build graph\n+    moduleOp.walk([&](Operation *op) {\n+      auto caller = op->getParentOfType<FunctionOpInterface>();\n+      if (auto callOp = dyn_cast<CallOpInterface>(op)) {\n+        auto *callee = callOp.resolveCallable(&symbolTable);\n+        auto funcOp = dyn_cast_or_null<FunctionOpInterface>(callee);\n+        if (funcOp) {\n+          graph[caller].emplace_back(\n+              std::pair<CallOpInterface, FunctionOpInterface>(callOp, funcOp));\n+          visited.insert(funcOp);\n+        }\n+      }\n+    });\n+    // Find roots\n+    moduleOp.walk([&](FunctionOpInterface funcOp) {\n+      if (!visited.count(funcOp)) {\n+        roots.push_back(funcOp);\n+      }\n+    });\n+  }\n+\n+  template <WalkOrder UpdateEdgeOrder = WalkOrder::PreOrder,\n+            WalkOrder UpdateNodeOrder = WalkOrder::PreOrder,\n+            typename UpdateEdgeFn, typename UpdateNodeFn>\n+  void doWalk(FunctionOpInterface funcOp,\n+              DenseSet<FunctionOpInterface> &visited, UpdateEdgeFn updateEdgeFn,\n+              UpdateNodeFn updateNodeFn) {\n+    if (visited.count(funcOp)) {\n+      llvm::report_fatal_error(\"Cycle detected in call graph\");\n+    }\n+    if constexpr (UpdateNodeOrder == WalkOrder::PreOrder) {\n+      updateNodeFn(funcOp);\n+    }\n+    for (auto [callOp, callee] : graph[funcOp]) {\n+      if constexpr (UpdateEdgeOrder == WalkOrder::PreOrder) {\n+        updateEdgeFn(callOp, callee);\n+      }\n+      doWalk<UpdateEdgeOrder, UpdateNodeOrder>(callee, visited, updateEdgeFn,\n+                                               updateNodeFn);\n+      if constexpr (UpdateEdgeOrder == WalkOrder::PostOrder) {\n+        updateEdgeFn(callOp, callee);\n+      }\n+    }\n+    if constexpr (UpdateNodeOrder == WalkOrder::PostOrder) {\n+      updateNodeFn(funcOp);\n+    }\n+    visited.erase(funcOp);\n+  }\n+\n+protected:\n+  ModuleOp moduleOp;\n+  DenseMap<FunctionOpInterface,\n+           SmallVector<std::pair<CallOpInterface, FunctionOpInterface>>>\n+      graph;\n+  FuncDataMapT funcMap;\n+  SmallVector<FunctionOpInterface> roots;\n+};\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_UTILITY_H"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -33,6 +33,8 @@ SmallVector<unsigned> getSizePerThread(Attribute layout);\n \n SmallVector<unsigned> getContigPerThread(Attribute layout);\n \n+SmallVector<unsigned> getUniqueContigPerThread(Type type);\n+\n SmallVector<unsigned> getThreadsPerCTA(Attribute layout);\n \n SmallVector<unsigned>"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -57,7 +57,6 @@ def TritonGPUAccelerateMatmul : Pass<\"tritongpu-accelerate-matmul\", \"mlir::Modul\n            \"int32_t\", /*default*/\"80\",\n            \"device compute capability\">\n   ];\n-\n }\n \n def TritonGPUOptimizeDotOperands : Pass<\"tritongpu-optimize-dot-operands\", \"mlir::ModuleOp\"> {"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 29, "deletions": 12, "changes": 41, "file_content_changes": "@@ -117,8 +117,11 @@ SmallVector<unsigned> getScratchConfigForAtomicCAS(triton::AtomicCASOp op) {\n \n class AllocationAnalysis {\n public:\n-  AllocationAnalysis(Operation *operation, Allocation *allocation)\n-      : operation(operation), allocation(allocation) {\n+  AllocationAnalysis(Operation *operation,\n+                     Allocation::FuncAllocMapT *funcAllocMap,\n+                     Allocation *allocation)\n+      : operation(operation), funcAllocMap(funcAllocMap),\n+        allocation(allocation) {\n     run();\n   }\n \n@@ -219,6 +222,12 @@ class AllocationAnalysis {\n                        ? elems * kPtrBitWidth / 8\n                        : elems * elemTy.getIntOrFloatBitWidth() / 8;\n       allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+    } else if (auto callOp = dyn_cast<CallOpInterface>(op)) {\n+      auto callable = callOp.resolveCallable();\n+      auto funcOp = dyn_cast<FunctionOpInterface>(callable);\n+      auto *funcAlloc = &(*funcAllocMap)[funcOp];\n+      auto bytes = funcAlloc->getSharedMemorySize();\n+      allocation->addBuffer<BufferT::BufferKind::Virtual>(op, bytes);\n     }\n   }\n \n@@ -298,15 +307,19 @@ class AllocationAnalysis {\n   /// allocated, but is used to store intermediate results.\n   void resolveScratchBufferLiveness(\n       const DenseMap<Operation *, size_t> &operationId) {\n-    // Analyze liveness of scratch buffers\n-    for (auto opScratchIter : allocation->opScratch) {\n-      // Any scratch memory's live range is the current operation's live\n-      // range.\n-      auto *op = opScratchIter.first;\n-      auto *buffer = opScratchIter.second;\n-      bufferRange.insert({buffer, Interval(operationId.lookup(op),\n-                                           operationId.lookup(op) + 1)});\n-    }\n+    // Analyze liveness of scratch buffers and vritual buffers.\n+    auto processScratchMemory = [&](const auto &container) {\n+      for (auto opScratchIter : container) {\n+        // Any scratch memory's live range is the current operation's live\n+        // range.\n+        auto *op = opScratchIter.first;\n+        auto *buffer = opScratchIter.second;\n+        bufferRange.insert({buffer, Interval(operationId.lookup(op),\n+                                             operationId.lookup(op) + 1)});\n+      }\n+    };\n+    processScratchMemory(allocation->opScratch);\n+    processScratchMemory(allocation->opVirtual);\n   }\n \n   /// Resolves liveness of all values involved under the root operation.\n@@ -499,11 +512,15 @@ class AllocationAnalysis {\n \n private:\n   Operation *operation;\n+  Allocation::FuncAllocMapT *funcAllocMap;\n   Allocation *allocation;\n   BufferRangeMapT bufferRange;\n };\n+\n } // namespace triton\n \n-void Allocation::run() { triton::AllocationAnalysis(getOperation(), this); }\n+void Allocation::run(FuncAllocMapT &funcAllocMap) {\n+  triton::AllocationAnalysis(getOperation(), &funcAllocMap, this);\n+}\n \n } // namespace mlir"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 73, "deletions": 20, "changes": 93, "file_content_changes": "@@ -77,7 +77,7 @@ AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n \n   if (blockArg && blockArg.getOwner()->isEntryBlock()) {\n     Operation *op = blockArg.getOwner()->getParentOp();\n-    if (auto fun = dyn_cast<triton::FuncOp>(op))\n+    if (auto fun = dyn_cast<FunctionOpInterface>(op))\n       initPessimisticStateFromFunc(blockArg.getArgNumber(), fun,\n                                    &knownContiguity, &knownDivisibility,\n                                    &knownConstancy);\n@@ -696,13 +696,13 @@ class LogicalOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n                                           const AxisInfo &rhs) override {\n     if (lhs.getConstantValue().has_value() &&\n         rhs.getConstantValue().has_value()) {\n-      if constexpr (std::is_same<OpTy, arith::AndIOp>::value) {\n+      if constexpr (std::is_same_v<OpTy, arith::AndIOp>) {\n         return {lhs.getConstantValue().value() &\n                 rhs.getConstantValue().value()};\n-      } else if constexpr (std::is_same<OpTy, arith::OrIOp>::value) {\n+      } else if constexpr (std::is_same_v<OpTy, arith::OrIOp>) {\n         return {lhs.getConstantValue().value() |\n                 rhs.getConstantValue().value()};\n-      } else if constexpr (std::is_same<OpTy, arith::XOrIOp>::value) {\n+      } else if constexpr (std::is_same_v<OpTy, arith::XOrIOp>) {\n         return {lhs.getConstantValue().value() ^\n                 rhs.getConstantValue().value()};\n       }\n@@ -907,7 +907,7 @@ void AxisInfoAnalysis::visitOperation(\n     propagateIfChanged(result, result->join(curr));\n }\n \n-unsigned AxisInfoAnalysis::getPtrContiguity(Value ptr) {\n+unsigned ModuleAxisInfoAnalysis::getPtrContiguity(Value ptr) {\n   auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n   if (!tensorTy)\n     return 1;\n@@ -919,43 +919,96 @@ unsigned AxisInfoAnalysis::getPtrContiguity(Value ptr) {\n   auto order = triton::gpu::getOrder(layout);\n   unsigned align = getPtrAlignment(ptr);\n \n-  unsigned contigPerThread = triton::gpu::getSizePerThread(layout)[order[0]];\n-  contigPerThread = std::min(align, contigPerThread);\n-  contigPerThread = std::min<unsigned>(shape[order[0]], contigPerThread);\n+  auto uniqueContigPerThread = triton::gpu::getUniqueContigPerThread(tensorTy);\n+  assert(order[0] < uniqueContigPerThread.size() &&\n+         \"Unxpected uniqueContigPerThread size\");\n+  unsigned contiguity = uniqueContigPerThread[order[0]];\n+  contiguity = std::min(align, contiguity);\n \n-  return contigPerThread;\n+  return contiguity;\n }\n \n-unsigned AxisInfoAnalysis::getPtrAlignment(Value ptr) {\n+unsigned ModuleAxisInfoAnalysis::getPtrAlignment(Value ptr) {\n   auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n   if (!tensorTy)\n     return 1;\n-  dataflow::Lattice<AxisInfo> *latticeElement = getLatticeElement(ptr);\n-  if (!latticeElement)\n+  auto *axisInfo = getAxisInfo(ptr);\n+  if (!axisInfo)\n     return 1;\n-  auto axisInfo = latticeElement->getValue();\n   auto layout = tensorTy.getEncoding();\n   auto order = triton::gpu::getOrder(layout);\n-  auto maxMultipleBytes = axisInfo.getDivisibility(order[0]);\n-  auto maxContig = axisInfo.getContiguity(order[0]);\n+  auto maxMultipleBytes = axisInfo->getDivisibility(order[0]);\n+  auto maxContig = axisInfo->getContiguity(order[0]);\n   auto elemNumBits = triton::getPointeeBitWidth(tensorTy);\n   auto elemNumBytes = std::max<unsigned>(elemNumBits / 8, 1);\n   auto maxMultiple = std::max<int64_t>(maxMultipleBytes / elemNumBytes, 1);\n   unsigned alignment = std::min(maxMultiple, maxContig);\n   return alignment;\n }\n \n-unsigned AxisInfoAnalysis::getMaskAlignment(Value mask) {\n+unsigned ModuleAxisInfoAnalysis::getMaskAlignment(Value mask) {\n   auto tensorTy = mask.getType().dyn_cast<RankedTensorType>();\n   if (!tensorTy)\n     return 1;\n-  dataflow::Lattice<AxisInfo> *latticeElement = getLatticeElement(mask);\n-  if (!latticeElement)\n+  auto *axisInfo = getAxisInfo(mask);\n+  if (!axisInfo)\n     return 1;\n-  auto maskAxis = latticeElement->getValue();\n   auto maskOrder = triton::gpu::getOrder(tensorTy.getEncoding());\n-  auto alignment = std::max<unsigned>(maskAxis.getConstancy(maskOrder[0]), 1);\n+  auto alignment = std::max<unsigned>(axisInfo->getConstancy(maskOrder[0]), 1);\n   return alignment;\n }\n \n+void ModuleAxisInfoAnalysis::initialize(FunctionOpInterface funcOp) {\n+  std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n+  AxisInfoAnalysis *analysis = solver->load<AxisInfoAnalysis>();\n+  if (failed(solver->initializeAndRun(funcOp)))\n+    return;\n+  auto *axisInfoMap = getFuncData(funcOp);\n+  auto updateAxisInfoMap = [&](Value value) {\n+    auto axisInfo = analysis->getLatticeElement(value)->getValue();\n+    AxisInfo curAxisInfo;\n+    if (axisInfoMap->count(value)) {\n+      curAxisInfo = AxisInfo::join(axisInfo, axisInfoMap->lookup(value));\n+    } else {\n+      curAxisInfo = axisInfo;\n+    }\n+    (*axisInfoMap)[value] = curAxisInfo;\n+  };\n+  funcOp.walk([&](Operation *op) {\n+    for (auto value : op->getResults()) {\n+      updateAxisInfoMap(value);\n+    }\n+  });\n+  funcOp.walk([&](Block *block) {\n+    for (auto value : block->getArguments()) {\n+      updateAxisInfoMap(value);\n+    }\n+  });\n+}\n+\n+void ModuleAxisInfoAnalysis::update(CallOpInterface callOp,\n+                                    FunctionOpInterface callee) {\n+  auto caller = callOp->getParentOfType<FunctionOpInterface>();\n+  auto *axisInfoMap = getFuncData(caller);\n+  for (auto entry : llvm::enumerate(callOp->getOperands())) {\n+    auto index = entry.index();\n+    auto value = entry.value();\n+    auto setAttrFn = [&](StringRef attrName, int64_t prevValue) {\n+      auto curValue = highestPowOf2Divisor<int64_t>(0);\n+      if (callee.getArgAttrOfType<IntegerAttr>(index, attrName)) {\n+        curValue =\n+            callee.getArgAttrOfType<IntegerAttr>(index, attrName).getInt();\n+      }\n+      auto attr = IntegerAttr::get(IntegerType::get(callee.getContext(), 64),\n+                                   gcd(prevValue, curValue));\n+      callee.setArgAttr(index, attrName, attr);\n+    };\n+    auto axisInfo = axisInfoMap->lookup(value);\n+    assert(axisInfo.getRank() == 1 && \"only scalar arguments are supported\");\n+    setAttrFn(\"tt.contiguity\", axisInfo.getContiguity(0));\n+    setAttrFn(\"tt.divisibility\", axisInfo.getDivisibility(0));\n+    setAttrFn(\"tt.constancy\", axisInfo.getConstancy(0));\n+  }\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 60, "deletions": 31, "changes": 91, "file_content_changes": "@@ -9,16 +9,21 @@\n \n namespace mlir {\n \n-void MembarAnalysis::run() {\n-  auto *operation = allocation->getOperation();\n-  OpBuilder builder(operation);\n-  resolve(operation, &builder);\n+void MembarAnalysis::run(FuncBlockInfoMapT &funcBlockInfoMap) {\n+  FunctionOpInterface funcOp =\n+      dyn_cast<FunctionOpInterface>(allocation->getOperation());\n+  OpBuilder builder(funcOp.getContext());\n+  resolve(funcOp, &funcBlockInfoMap, &builder);\n }\n \n-void MembarAnalysis::resolve(Operation *operation, OpBuilder *builder) {\n+void MembarAnalysis::resolve(FunctionOpInterface funcOp,\n+                             FuncBlockInfoMapT *funcBlockInfoMap,\n+                             OpBuilder *builder) {\n   // Initialize the blockList\n+  DenseMap<Block *, BlockInfo> inputBlockInfoMap;\n+  DenseMap<Block *, BlockInfo> outputBlockInfoMap;\n   std::deque<Block *> blockList;\n-  operation->walk<WalkOrder::PreOrder>([&](Block *block) {\n+  funcOp.walk<WalkOrder::PreOrder>([&](Block *block) {\n     for (auto &op : block->getOperations()) {\n       // Check if the operation belongs to scf dialect, if so, we need to\n       // throw an error\n@@ -38,13 +43,13 @@ void MembarAnalysis::resolve(Operation *operation, OpBuilder *builder) {\n     auto *block = blockList.front();\n     blockList.pop_front();\n     // Make a copy of the inputblockInfo but not update\n-    auto inputBlockInfo = inputBlockInfoMap.lookup(block);\n+    auto inputBlockInfo = inputBlockInfoMap[block];\n     SmallVector<Block *> successors;\n     for (auto &op : block->getOperations()) {\n       if (op.hasTrait<OpTrait::IsTerminator>()) {\n         visitTerminator(&op, successors);\n       } else {\n-        update(&op, &inputBlockInfo, builder);\n+        update(&op, &inputBlockInfo, funcBlockInfoMap, builder);\n       }\n     }\n     // Get the reference because we want to update if it changed\n@@ -62,6 +67,14 @@ void MembarAnalysis::resolve(Operation *operation, OpBuilder *builder) {\n       blockList.emplace_back(successor);\n     }\n   }\n+\n+  // Update the final dangling buffers that haven't been synced\n+  auto &funcBlockInfo = (*funcBlockInfoMap)[funcOp];\n+  funcOp.walk<WalkOrder::PreOrder>([&](Block *block) {\n+    block->walk([&](triton::ReturnOp returnOp) {\n+      funcBlockInfo.join(outputBlockInfoMap[block]);\n+    });\n+  });\n }\n \n void MembarAnalysis::visitTerminator(Operation *op,\n@@ -81,6 +94,7 @@ void MembarAnalysis::visitTerminator(Operation *op,\n }\n \n void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n+                            FuncBlockInfoMapT *funcBlockInfoMap,\n                             OpBuilder *builder) {\n   if (isa<triton::gpu::ExtractSliceOp>(op) ||\n       isa<triton::gpu::AllocTensorOp>(op) || isa<triton::TransOp>(op)) {\n@@ -108,36 +122,51 @@ void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n   }\n \n   BlockInfo curBlockInfo;\n-  for (Value value : op->getOperands()) {\n-    for (auto bufferId : allocation->getBufferIds(value)) {\n-      if (bufferId != Allocation::InvalidBufferId) {\n-        if (isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n-            isa<tensor::InsertSliceOp>(op)) {\n-          // FIXME(Keren): insert_slice and insert_slice_async are always\n-          // alias for now\n-          curBlockInfo.syncWriteBuffers.insert(bufferId);\n-        } else {\n-          // ConvertLayoutOp: shared memory -> registers\n-          curBlockInfo.syncReadBuffers.insert(bufferId);\n+  if (isa<triton::CallOp>(op)) {\n+    // Inter-function dependencies\n+    auto callOpInterface = dyn_cast<CallOpInterface>(op);\n+    if (auto callee =\n+            dyn_cast<FunctionOpInterface>(callOpInterface.resolveCallable())) {\n+      curBlockInfo = funcBlockInfoMap->lookup(callee);\n+    }\n+  } else {\n+    // Intra-function dependencies\n+    for (Value value : op->getOperands()) {\n+      for (auto bufferId : allocation->getBufferIds(value)) {\n+        if (bufferId != Allocation::InvalidBufferId) {\n+          if (isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n+              isa<tensor::InsertSliceOp>(op)) {\n+            // FIXME(Keren): insert_slice and insert_slice_async are always\n+            // alias for now\n+            curBlockInfo.syncWriteIntervals.insert(\n+                allocation->getAllocatedInterval(bufferId));\n+          } else {\n+            // ConvertLayoutOp: shared memory -> registers\n+            curBlockInfo.syncReadIntervals.insert(\n+                allocation->getAllocatedInterval(bufferId));\n+          }\n         }\n       }\n     }\n-  }\n-  for (Value value : op->getResults()) {\n-    // ConvertLayoutOp: registers -> shared memory\n-    auto bufferId = allocation->getBufferId(value);\n+    for (Value value : op->getResults()) {\n+      // ConvertLayoutOp: registers -> shared memory\n+      auto bufferId = allocation->getBufferId(value);\n+      if (bufferId != Allocation::InvalidBufferId) {\n+        curBlockInfo.syncWriteIntervals.insert(\n+            allocation->getAllocatedInterval(bufferId));\n+      }\n+    }\n+    // Scratch buffer is considered as both shared memory write & read\n+    auto bufferId = allocation->getBufferId(op);\n     if (bufferId != Allocation::InvalidBufferId) {\n-      curBlockInfo.syncWriteBuffers.insert(bufferId);\n+      curBlockInfo.syncWriteIntervals.insert(\n+          allocation->getAllocatedInterval(bufferId));\n+      curBlockInfo.syncReadIntervals.insert(\n+          allocation->getAllocatedInterval(bufferId));\n     }\n   }\n-  // Scratch buffer is considered as both shared memory write & read\n-  auto bufferId = allocation->getBufferId(op);\n-  if (bufferId != Allocation::InvalidBufferId) {\n-    curBlockInfo.syncWriteBuffers.insert(bufferId);\n-    curBlockInfo.syncReadBuffers.insert(bufferId);\n-  }\n \n-  if (blockInfo->isIntersected(curBlockInfo, allocation)) {\n+  if (blockInfo->isIntersected(curBlockInfo)) {\n     OpBuilder::InsertionGuard g(*builder);\n     builder->setInsertionPoint(op);\n     builder->create<gpu::BarrierOp>(op->getLoc());"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 11, "deletions": 8, "changes": 19, "file_content_changes": "@@ -106,13 +106,17 @@ struct ConvertLayoutOpConversion\n     if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n       unsigned dim = sliceLayout.getDim();\n       auto parentEncoding = sliceLayout.getParent();\n+      auto parentSizePerThread = getSizePerThread(parentEncoding);\n+      unsigned stride = 1;\n+      if (getOrder(parentEncoding)[0] == dim)\n+        stride = parentSizePerThread[dim];\n       auto parentShape = sliceLayout.paddedShape(shape);\n       auto parentTy = RankedTensorType::get(parentShape, type.getElementType(),\n                                             parentEncoding);\n-      auto multiDimOffsetParent =\n-          getMultiDimOffset(parentEncoding, loc, rewriter, elemId, parentTy,\n-                            sliceLayout.paddedShape(multiDimCTAInRepId),\n-                            sliceLayout.paddedShape(shapePerCTA));\n+      auto multiDimOffsetParent = getMultiDimOffset(\n+          parentEncoding, loc, rewriter, elemId * stride, parentTy,\n+          sliceLayout.paddedShape(multiDimCTAInRepId),\n+          sliceLayout.paddedShape(shapePerCTA));\n       SmallVector<Value> multiDimOffset(rank);\n       for (unsigned d = 0; d < rank + 1; ++d) {\n         if (d == dim)\n@@ -672,14 +676,13 @@ struct ConvertLayoutOpConversion\n     }\n     return res;\n   }\n-}; // namespace triton::gpu::ConvertLayoutOp>\n+}; // namespace triton::gpu::ConvertLayoutOp\n \n void populateConvertLayoutOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem,\n+    ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit) {\n-  patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n+  patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation,\n                                           indexCacheInfo, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.h", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -10,8 +10,7 @@ using ::mlir::triton::gpu::DotOperandEncodingAttr;\n \n void populateConvertLayoutOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem,\n+    ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -62,9 +62,8 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n };\n \n void populateDotOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n-                                 RewritePatternSet &patterns, int numWarps,\n-                                 AxisInfoAnalysis &axisInfoAnalysis,\n-                                 const Allocation *allocation, Value smem,\n+                                 RewritePatternSet &patterns,\n+                                 ModuleAllocation &allocation,\n                                  PatternBenefit benefit) {\n-  patterns.add<DotOpConversion>(typeConverter, allocation, smem, benefit);\n+  patterns.add<DotOpConversion>(typeConverter, allocation, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.h", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -7,9 +7,8 @@ using namespace mlir;\n using namespace mlir::triton;\n \n void populateDotOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n-                                 RewritePatternSet &patterns, int numWarps,\n-                                 AxisInfoAnalysis &axisInfoAnalysis,\n-                                 const Allocation *allocation, Value smem,\n+                                 RewritePatternSet &patterns,\n+                                 ModuleAllocation &allocation,\n                                  PatternBenefit benefit);\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -1055,8 +1055,7 @@ struct AbsFOpConversion\n \n void populateElementwiseOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem, PatternBenefit benefit) {\n+    PatternBenefit benefit) {\n #define POPULATE_TERNARY_OP(SRC_OP, DST_OP)                                    \\\n   patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n   POPULATE_TERNARY_OP(triton::gpu::SelectOp, LLVM::SelectOp)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.h", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -8,8 +8,7 @@ using namespace mlir::triton;\n \n void populateElementwiseOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem, PatternBenefit benefit);\n+    PatternBenefit benefit);\n \n bool isLegalElementwiseOp(Operation *op);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 27, "deletions": 24, "changes": 51, "file_content_changes": "@@ -13,7 +13,7 @@ using ::mlir::triton::gpu::SharedEncodingAttr;\n \n // Contains some helper functions for both Load and Store conversions.\n struct LoadStoreConversionBase {\n-  explicit LoadStoreConversionBase(AxisInfoAnalysis &axisAnalysisPass)\n+  explicit LoadStoreConversionBase(ModuleAxisInfoAnalysis &axisAnalysisPass)\n       : axisAnalysisPass(axisAnalysisPass) {}\n \n   unsigned getContiguity(Value ptr) const {\n@@ -38,7 +38,7 @@ struct LoadStoreConversionBase {\n   }\n \n protected:\n-  AxisInfoAnalysis &axisAnalysisPass;\n+  ModuleAxisInfoAnalysis &axisAnalysisPass;\n };\n \n struct LoadOpConversion\n@@ -48,7 +48,8 @@ struct LoadOpConversion\n       triton::LoadOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   LoadOpConversion(TritonGPUToLLVMTypeConverter &converter,\n-                   AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n+                   ModuleAxisInfoAnalysis &axisAnalysisPass,\n+                   PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>(converter, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n \n@@ -254,7 +255,8 @@ struct StoreOpConversion\n       triton::StoreOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   StoreOpConversion(TritonGPUToLLVMTypeConverter &converter,\n-                    AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n+                    ModuleAxisInfoAnalysis &axisAnalysisPass,\n+                    PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::StoreOp>(converter, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n \n@@ -380,11 +382,11 @@ struct AtomicCASOpConversion\n       triton::AtomicCASOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   AtomicCASOpConversion(TritonGPUToLLVMTypeConverter &converter,\n-                        const Allocation *allocation, Value smem,\n-                        AxisInfoAnalysis &axisAnalysisPass,\n+                        ModuleAllocation &allocation,\n+                        ModuleAxisInfoAnalysis &axisAnalysisPass,\n                         PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::AtomicCASOp>(\n-            converter, allocation, smem, benefit),\n+            converter, allocation, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n \n   LogicalResult\n@@ -457,11 +459,11 @@ struct AtomicRMWOpConversion\n       triton::AtomicRMWOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   AtomicRMWOpConversion(TritonGPUToLLVMTypeConverter &converter,\n-                        const Allocation *allocation, Value smem,\n-                        AxisInfoAnalysis &axisAnalysisPass,\n+                        ModuleAllocation &allocation,\n+                        ModuleAxisInfoAnalysis &axisAnalysisPass,\n                         PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>(\n-            converter, allocation, smem, benefit),\n+            converter, allocation, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n \n   LogicalResult\n@@ -629,7 +631,9 @@ struct InsertSliceOpConversion\n     Value dst = op.getDest();\n     Value src = op.getSource();\n     Value res = op.getResult();\n-    assert(allocation->getBufferId(res) == Allocation::InvalidBufferId &&\n+    auto funcOp = op->getParentOfType<FunctionOpInterface>();\n+    auto *funcAllocation = allocation->getFuncData(funcOp);\n+    assert(funcAllocation->getBufferId(res) == Allocation::InvalidBufferId &&\n            \"Only support in-place insert_slice for now\");\n \n     auto srcTy = src.getType().dyn_cast<RankedTensorType>();\n@@ -689,12 +693,11 @@ struct InsertSliceAsyncOpConversion\n       triton::gpu::InsertSliceAsyncOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   InsertSliceAsyncOpConversion(\n-      TritonGPUToLLVMTypeConverter &converter, const Allocation *allocation,\n-      Value smem,\n+      TritonGPUToLLVMTypeConverter &converter, ModuleAllocation &allocation,\n       ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n-      AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n+      ModuleAxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::gpu::InsertSliceAsyncOp>(\n-            converter, allocation, smem, indexCacheInfo, benefit),\n+            converter, allocation, indexCacheInfo, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n \n   LogicalResult\n@@ -707,7 +710,9 @@ struct InsertSliceAsyncOpConversion\n     Value res = op.getResult();\n     Value mask = op.getMask();\n     Value other = op.getOther();\n-    assert(allocation->getBufferId(res) == Allocation::InvalidBufferId &&\n+    auto funcOp = op->getParentOfType<FunctionOpInterface>();\n+    auto *funcAllocation = allocation->getFuncData(funcOp);\n+    assert(funcAllocation->getBufferId(res) == Allocation::InvalidBufferId &&\n            \"Only support in-place insert_slice_async for now\");\n \n     auto srcTy = src.getType().cast<RankedTensorType>();\n@@ -847,19 +852,17 @@ struct InsertSliceAsyncOpConversion\n \n void populateLoadStoreOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem,\n+    ModuleAxisInfoAnalysis &axisInfoAnalysis, ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit) {\n   patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<StoreOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n-  patterns.add<AtomicCASOpConversion>(typeConverter, allocation, smem,\n+  patterns.add<AtomicCASOpConversion>(typeConverter, allocation,\n                                       axisInfoAnalysis, benefit);\n-  patterns.add<AtomicRMWOpConversion>(typeConverter, allocation, smem,\n+  patterns.add<AtomicRMWOpConversion>(typeConverter, allocation,\n                                       axisInfoAnalysis, benefit);\n-  patterns.add<InsertSliceOpConversion>(typeConverter, allocation, smem,\n+  patterns.add<InsertSliceOpConversion>(typeConverter, allocation,\n                                         indexCacheInfo, benefit);\n-  patterns.add<InsertSliceAsyncOpConversion>(typeConverter, allocation, smem,\n-                                             indexCacheInfo, axisInfoAnalysis,\n-                                             benefit);\n+  patterns.add<InsertSliceAsyncOpConversion>(\n+      typeConverter, allocation, indexCacheInfo, axisInfoAnalysis, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.h", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -8,8 +8,7 @@ using namespace mlir::triton;\n \n void populateLoadStoreOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem,\n+    ModuleAxisInfoAnalysis &axisInfoAnalysis, ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -486,10 +486,9 @@ struct ReduceOpConversion\n \n void populateReduceOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem,\n+    ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit) {\n-  patterns.add<ReduceOpConversion>(typeConverter, allocation, smem,\n-                                   indexCacheInfo, benefit);\n+  patterns.add<ReduceOpConversion>(typeConverter, allocation, indexCacheInfo,\n+                                   benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.h", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -8,8 +8,7 @@ using namespace mlir::triton;\n \n void populateReduceOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem,\n+    ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 4, "deletions": 6, "changes": 10, "file_content_changes": "@@ -345,8 +345,7 @@ struct MakeRangeOpConversion\n       ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n       PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp>(\n-            converter, /*Allocation*/ nullptr, Value{}, indexCacheInfo,\n-            benefit) {}\n+            converter, indexCacheInfo, benefit) {}\n \n   LogicalResult\n   matchAndRewrite(triton::MakeRangeOp op, OpAdaptor adaptor,\n@@ -613,18 +612,17 @@ void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n \n void populateTritonGPUToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem,\n+    ModuleAllocation &moduleAllocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit) {\n   patterns.add<AddPtrOpConversion>(typeConverter, benefit);\n-  patterns.add<AllocTensorOpConversion>(typeConverter, allocation, smem,\n+  patterns.add<AllocTensorOpConversion>(typeConverter, moduleAllocation,\n                                         benefit);\n   patterns.add<AsyncCommitGroupOpConversion>(typeConverter, benefit);\n   patterns.add<AsyncWaitOpConversion>(typeConverter, benefit);\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n \n-  patterns.add<ExtractSliceOpConversion>(typeConverter, allocation, smem,\n+  patterns.add<ExtractSliceOpConversion>(typeConverter, moduleAllocation,\n                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n   patterns.add<GetNumProgramsOpConversion>(typeConverter, benefit);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -8,8 +8,7 @@ using namespace mlir::triton;\n \n void populateTritonGPUToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem,\n+    ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 76, "deletions": 41, "changes": 117, "file_content_changes": "@@ -11,7 +11,7 @@\n #include \"Utility.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n-\n+#include <set>\n using namespace mlir;\n using namespace mlir::triton;\n \n@@ -41,7 +41,7 @@ void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n // All the rights are reserved by the LLVM community.\n \n struct FuncOpConversionBase : public ConvertOpToLLVMPattern<triton::FuncOp> {\n-private:\n+protected:\n   /// Only retain those attributes that are not constructed by\n   /// `LLVMFuncOp::build`. If `filterArgAttrs` is set, also filter out argument\n   /// attributes.\n@@ -184,14 +184,18 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       : converter(&typeConverter) {}\n \n   explicit ConvertTritonGPUOpToLLVMPatternBase(\n-      TritonGPUToLLVMTypeConverter &typeConverter, const Allocation *allocation,\n-      Value smem)\n-      : converter(&typeConverter), allocation(allocation), smem(smem) {}\n+      TritonGPUToLLVMTypeConverter &typeConverter,\n+      IndexCacheInfo indexCacheInfo)\n+      : converter(&typeConverter), indexCacheInfo(indexCacheInfo) {}\n \n   explicit ConvertTritonGPUOpToLLVMPatternBase(\n-      TritonGPUToLLVMTypeConverter &typeConverter, const Allocation *allocation,\n-      Value smem, IndexCacheInfo indexCacheInfo)\n-      : converter(&typeConverter), allocation(allocation), smem(smem),\n+      TritonGPUToLLVMTypeConverter &typeConverter, ModuleAllocation &allocation)\n+      : converter(&typeConverter), allocation(&allocation) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(\n+      TritonGPUToLLVMTypeConverter &typeConverter, ModuleAllocation &allocation,\n+      IndexCacheInfo indexCacheInfo)\n+      : converter(&typeConverter), allocation(&allocation),\n         indexCacheInfo(indexCacheInfo) {}\n \n   TritonGPUToLLVMTypeConverter *getTypeConverter() const { return converter; }\n@@ -228,9 +232,17 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                             T value) const {\n     auto ptrTy = LLVM::LLVMPointerType::get(\n         this->getTypeConverter()->convertType(rewriter.getI8Type()), 3);\n-    auto bufferId = allocation->getBufferId(value);\n+    FunctionOpInterface funcOp;\n+    if constexpr (std::is_pointer_v<T>)\n+      funcOp = value->template getParentOfType<FunctionOpInterface>();\n+    else\n+      funcOp = value.getParentRegion()\n+                   ->template getParentOfType<FunctionOpInterface>();\n+    auto *funcAllocation = allocation->getFuncData(funcOp);\n+    auto smem = allocation->getFunctionSharedMemoryBase(funcOp);\n+    auto bufferId = funcAllocation->getBufferId(value);\n     assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n-    size_t offset = allocation->getOffset(bufferId);\n+    size_t offset = funcAllocation->getOffset(bufferId);\n     Value offVal = i32_val(offset);\n     Value base = gep(ptrTy, smem, offVal);\n     return base;\n@@ -505,13 +517,13 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                                             RankedTensorType type) const {\n     IndexCacheKeyT key = std::make_pair(layout, type);\n     auto cache = indexCacheInfo.baseIndexCache;\n-    assert(cache && \"baseIndexCache is nullptr\");\n     auto insertPt = indexCacheInfo.indexInsertPoint;\n-    if (cache->count(key) > 0) {\n+    if (cache && cache->count(key) > 0) {\n       return cache->lookup(key);\n     } else {\n       ConversionPatternRewriter::InsertionGuard guard(rewriter);\n-      restoreInsertionPointIfSet(insertPt, rewriter);\n+      if (cache)\n+        restoreInsertionPointIfSet(insertPt, rewriter);\n       SmallVector<Value> result;\n       if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n         result =\n@@ -521,11 +533,20 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n           result = emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, type);\n         if (mmaLayout.isAmpere())\n           result = emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, type);\n+      } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+        auto parentLayout = sliceLayout.getParent();\n+        auto parentShape = sliceLayout.paddedShape(type.getShape());\n+        RankedTensorType parentTy = RankedTensorType::get(\n+            parentShape, type.getElementType(), parentLayout);\n+        result = emitBaseIndexForLayout(loc, rewriter, parentLayout, parentTy);\n+        result.erase(result.begin() + sliceLayout.getDim());\n       } else {\n         llvm_unreachable(\"unsupported emitBaseIndexForLayout\");\n       }\n-      cache->insert(std::make_pair(key, result));\n-      *insertPt = rewriter.saveInsertionPoint();\n+      if (cache) {\n+        cache->insert(std::make_pair(key, result));\n+        *insertPt = rewriter.saveInsertionPoint();\n+      }\n       return result;\n     }\n   }\n@@ -540,6 +561,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       if (mmaLayout.isAmpere())\n         return emitOffsetForMmaLayoutV2(mmaLayout, type);\n     }\n+    if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>())\n+      return emitOffsetForSliceLayout(sliceLayout, type);\n     llvm_unreachable(\"unsupported emitOffsetForLayout\");\n   }\n \n@@ -552,27 +575,29 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                                               RankedTensorType type) const {\n     IndexCacheKeyT key(layout, type);\n     auto cache = indexCacheInfo.indexCache;\n-    assert(cache && \"indexCache is nullptr\");\n     auto insertPt = indexCacheInfo.indexInsertPoint;\n-    if (cache->count(key) > 0) {\n+    if (cache && cache->count(key) > 0) {\n       return cache->lookup(key);\n     } else {\n       ConversionPatternRewriter::InsertionGuard guard(b);\n-      restoreInsertionPointIfSet(insertPt, b);\n+      if (cache)\n+        restoreInsertionPointIfSet(insertPt, b);\n       SmallVector<SmallVector<Value>> result;\n       if (auto blocked = layout.dyn_cast<BlockedEncodingAttr>()) {\n         result = emitIndicesForDistributedLayout(loc, b, blocked, type);\n       } else if (auto mma = layout.dyn_cast<MmaEncodingAttr>()) {\n         result = emitIndicesForDistributedLayout(loc, b, mma, type);\n       } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n-        result = emitIndicesForSliceLayout(loc, b, slice, type);\n+        result = emitIndicesForDistributedLayout(loc, b, slice, type);\n       } else {\n         llvm_unreachable(\n             \"emitIndices for layouts other than blocked & slice not \"\n             \"implemented yet\");\n       }\n-      cache->insert(std::make_pair(key, result));\n-      *insertPt = b.saveInsertionPoint();\n+      if (cache) {\n+        cache->insert(std::make_pair(key, result));\n+        *insertPt = b.saveInsertionPoint();\n+      }\n       return result;\n     }\n   }\n@@ -879,30 +904,34 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return multiDimIdx;\n   }\n \n-  SmallVector<SmallVector<Value>>\n-  emitIndicesForSliceLayout(Location loc, ConversionPatternRewriter &rewriter,\n-                            const SliceEncodingAttr &sliceLayout,\n-                            RankedTensorType type) const {\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForSliceLayout(const SliceEncodingAttr &sliceLayout,\n+                           RankedTensorType type) const {\n     auto parentEncoding = sliceLayout.getParent();\n     unsigned dim = sliceLayout.getDim();\n     auto parentShape = sliceLayout.paddedShape(type.getShape());\n     RankedTensorType parentTy = RankedTensorType::get(\n         parentShape, type.getElementType(), parentEncoding);\n-    auto parentIndices = emitIndices(loc, rewriter, parentEncoding, parentTy);\n-    unsigned numIndices = parentIndices.size();\n-    SmallVector<SmallVector<Value>> resultIndices;\n-    for (unsigned i = 0; i < numIndices; ++i) {\n-      SmallVector<Value> indices = parentIndices[i];\n-      indices.erase(indices.begin() + dim);\n-      resultIndices.push_back(indices);\n+    auto parentOffsets = emitOffsetForLayout(parentEncoding, parentTy);\n+\n+    unsigned numOffsets = parentOffsets.size();\n+    SmallVector<SmallVector<unsigned>> resultOffsets;\n+    std::set<SmallVector<unsigned>> uniqueOffsets;\n+\n+    for (unsigned i = 0; i < numOffsets; ++i) {\n+      SmallVector<unsigned> offsets = parentOffsets[i];\n+      offsets.erase(offsets.begin() + dim);\n+      if (uniqueOffsets.find(offsets) == uniqueOffsets.end()) {\n+        resultOffsets.push_back(offsets);\n+        uniqueOffsets.insert(offsets);\n+      }\n     }\n-    return resultIndices;\n+    return resultOffsets;\n   }\n \n protected:\n   TritonGPUToLLVMTypeConverter *converter;\n-  const Allocation *allocation;\n-  Value smem;\n+  ModuleAllocation *allocation;\n   IndexCacheInfo indexCacheInfo;\n };\n \n@@ -919,16 +948,22 @@ class ConvertTritonGPUOpToLLVMPattern\n         ConvertTritonGPUOpToLLVMPatternBase(typeConverter) {}\n \n   explicit ConvertTritonGPUOpToLLVMPattern(\n-      TritonGPUToLLVMTypeConverter &typeConverter, const Allocation *allocation,\n-      Value smem, PatternBenefit benefit = 1)\n+      TritonGPUToLLVMTypeConverter &typeConverter,\n+      IndexCacheInfo indexCacheInfo, PatternBenefit benefit = 1)\n+      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, indexCacheInfo) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPattern(\n+      TritonGPUToLLVMTypeConverter &typeConverter, ModuleAllocation &allocation,\n+      PatternBenefit benefit = 1)\n       : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n-        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation, smem) {}\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation) {}\n \n   explicit ConvertTritonGPUOpToLLVMPattern(\n-      TritonGPUToLLVMTypeConverter &typeConverter, const Allocation *allocation,\n-      Value smem, IndexCacheInfo indexCacheInfo, PatternBenefit benefit = 1)\n+      TritonGPUToLLVMTypeConverter &typeConverter, ModuleAllocation &allocation,\n+      IndexCacheInfo indexCacheInfo, PatternBenefit benefit = 1)\n       : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n-        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation, smem,\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation,\n                                             indexCacheInfo) {}\n \n protected:"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 227, "deletions": 66, "changes": 293, "file_content_changes": "@@ -2,7 +2,7 @@\n \n #include \"mlir/Analysis/DataFlowFramework.h\"\n #include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n-#include \"mlir/Conversion/ControlFlowToLLVM//ControlFlowToLLVM.h\"\n+#include \"mlir/Conversion/ControlFlowToLLVM/ControlFlowToLLVM.h\"\n #include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n #include \"mlir/Conversion/GPUToROCDL/GPUToROCDLPass.h\"\n #include \"mlir/Conversion/LLVMCommon/VectorPattern.h\"\n@@ -61,17 +61,22 @@ struct ReturnOpConversion : public ConvertOpToLLVMPattern<triton::ReturnOp> {\n   LogicalResult\n   matchAndRewrite(triton::ReturnOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    unsigned numArguments = op.getNumOperands();\n-\n-    // Currently, Triton kernel function always return nothing.\n-    // TODO(Superjomn) add support for non-inline device function\n-    if (numArguments > 0) {\n-      return rewriter.notifyMatchFailure(\n-          op, \"Only kernel function with nothing returned is supported.\");\n+    auto funcOp = op->getParentOfType<LLVM::LLVMFuncOp>();\n+    if (funcOp->hasAttr(\"nvvm.kernel\")) {\n+      // A GPU kernel\n+      if (op.getNumOperands() > 0) {\n+        return rewriter.notifyMatchFailure(\n+            op, \"Kernel functions do not support return with operands\");\n+      }\n+      rewriter.replaceOpWithNewOp<LLVM::ReturnOp>(op, TypeRange(), ValueRange(),\n+                                                  op->getAttrs());\n+    } else {\n+      // A device function\n+      auto newOp =\n+          rewriter.create<LLVM::ReturnOp>(op.getLoc(), adaptor.getOperands());\n+      newOp->setAttrs(op->getAttrs());\n+      rewriter.replaceOp(op, newOp->getResults());\n     }\n-\n-    rewriter.replaceOpWithNewOp<LLVM::ReturnOp>(op, TypeRange(), ValueRange(),\n-                                                op->getAttrs());\n     return success();\n   }\n };\n@@ -81,33 +86,173 @@ struct ReturnOpConversion : public ConvertOpToLLVMPattern<triton::ReturnOp> {\n /// information.\n struct FuncOpConversion : public FuncOpConversionBase {\n   FuncOpConversion(LLVMTypeConverter &converter, int numWarps,\n-                   PatternBenefit benefit)\n-      : FuncOpConversionBase(converter, benefit), numWarps(numWarps) {}\n+                   ModuleAllocation &allocation, PatternBenefit benefit)\n+      : FuncOpConversionBase(converter, benefit), allocation(allocation),\n+        numWarps(numWarps) {}\n+\n+  triton::FuncOp amendFuncOp(triton::FuncOp funcOp,\n+                             ConversionPatternRewriter &rewriter) const {\n+    // Push back a variable that indicates the current stack pointer of shared\n+    // memory to the function arguments.\n+    auto loc = funcOp.getLoc();\n+    auto ctx = funcOp->getContext();\n+    auto ptrTy = LLVM::LLVMPointerType::get(\n+        this->getTypeConverter()->convertType(rewriter.getI8Type()), 3);\n+    // 1. Modify the function type to add the new argument.\n+    auto funcTy = funcOp.getFunctionType();\n+    auto amendedInputTy = llvm::to_vector<4>(funcTy.getInputs());\n+    amendedInputTy.push_back(ptrTy);\n+    auto amendedFuncTy = FunctionType::get(funcTy.getContext(), amendedInputTy,\n+                                           funcTy.getResults());\n+    // 2. Modify the argument attributes to add the new argument.\n+    SmallVector<NamedAttribute> amendedAttrs;\n+    filterFuncAttributes(funcOp, /*filterArgAttrs=*/true, amendedAttrs);\n+    auto amendedArgAttrs = llvm::to_vector<4>(funcOp.getAllArgAttrs());\n+    amendedArgAttrs.emplace_back(DictionaryAttr::get(ctx));\n+    amendedAttrs.push_back(rewriter.getNamedAttr(\n+        funcOp.getArgAttrsAttrName(), rewriter.getArrayAttr(amendedArgAttrs)));\n+    // 3. Add a new argument to the region\n+    auto amendedFuncOp = rewriter.create<triton::FuncOp>(\n+        funcOp.getLoc(), funcOp.getName(), amendedFuncTy, amendedAttrs);\n+    auto &region = funcOp.getBody();\n+    region.addArgument(ptrTy, loc);\n+    rewriter.inlineRegionBefore(region, amendedFuncOp.getBody(),\n+                                amendedFuncOp.end());\n+    return amendedFuncOp;\n+  }\n \n   LogicalResult\n   matchAndRewrite(triton::FuncOp funcOp, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto newFuncOp = convertFuncOpToLLVMFuncOp(funcOp, rewriter);\n+    // Prevent LLVM's inliner to inline this function\n+    auto amendedFuncOp = funcOp;\n+    if (!allocation.isRoot(funcOp))\n+      amendedFuncOp = amendFuncOp(funcOp, rewriter);\n+\n+    auto newFuncOp = convertFuncOpToLLVMFuncOp(amendedFuncOp, rewriter);\n     if (!newFuncOp) {\n       return failure();\n     }\n \n     auto ctx = funcOp->getContext();\n \n-    // Set an attribute to indicate this function is a kernel entry.\n-    newFuncOp->setAttr(\"nvvm.kernel\",\n-                       rewriter.getIntegerAttr(type::u1Ty(ctx), 1));\n-\n+    if (allocation.isRoot(funcOp)) {\n+      // Set an attribute to indicate this function is a kernel entry.\n+      newFuncOp->setAttr(\"nvvm.kernel\",\n+                         rewriter.getIntegerAttr(type::u1Ty(ctx), 1));\n+    } else {\n+      // The noinline attribute will be used by the LLVM codegen to prevent\n+      // inlining.\n+      // https://github.com/llvm/llvm-project/blob/main/mlir/lib/Dialect/LLVMIR/IR/LLVMInlining.cpp#L267\n+      newFuncOp.setPassthroughAttr(\n+          ArrayAttr::get(ctx, rewriter.getStringAttr(\"noinline\")));\n+      rewriter.eraseOp(amendedFuncOp);\n+    }\n     // Set an attribute for maxntidx, it could be used in latter LLVM codegen\n     // for `nvvm.annotation` metadata.\n     newFuncOp->setAttr(\"nvvm.maxntid\", rewriter.getI32ArrayAttr(32 * numWarps));\n+    // The call graph is updated by mapping the old function to the new one.\n+    allocation.mapFuncOp(funcOp, newFuncOp);\n \n     rewriter.eraseOp(funcOp);\n     return success();\n   }\n \n private:\n   int numWarps{0};\n+  ModuleAllocation &allocation;\n+};\n+\n+// CallOpInterfaceLowering is adapted from\n+// https://github.com/llvm/llvm-project/blob/fae656b2dd80246c3c6f01e9c77c49560368752c/mlir/lib/Conversion/FuncToLLVM/FuncToLLVM.cpp#L485\n+struct CallOpConversion : public ConvertOpToLLVMPattern<triton::CallOp> {\n+  CallOpConversion(LLVMTypeConverter &converter, int numWarps,\n+                   ModuleAllocation &allocation, PatternBenefit benefit)\n+      : ConvertOpToLLVMPattern<triton::CallOp>(converter, benefit),\n+        allocation(allocation), numWarps(numWarps) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::CallOp callOp,\n+                  typename triton::CallOp::Adaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto promotedOperands = promoteOperands(callOp, adaptor, rewriter);\n+    auto newCallOp =\n+        convertCallOpToLLVMCallOp(callOp, promotedOperands, rewriter);\n+    if (!newCallOp)\n+      return failure();\n+    allocation.mapCallOp(callOp, newCallOp);\n+    auto results = getCallOpResults(callOp, newCallOp, rewriter);\n+    rewriter.replaceOp(callOp, results);\n+    return success();\n+  }\n+\n+private:\n+  SmallVector<Value, 4>\n+  promoteOperands(triton::CallOp callOp,\n+                  typename triton::CallOp::Adaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const {\n+    // Get the last argument of the caller, which is the current stack pointer\n+    // of shared memory and append it to the operands of the callOp.\n+    auto loc = callOp.getLoc();\n+    auto caller = callOp->getParentOfType<FunctionOpInterface>();\n+    auto base = allocation.getFunctionSharedMemoryBase(caller);\n+    auto *funcAllocation = allocation.getFuncData(caller);\n+    auto bufferId = funcAllocation->getBufferId(callOp);\n+    auto offset = funcAllocation->getOffset(bufferId);\n+    auto ptrTy = LLVM::LLVMPointerType::get(\n+        this->getTypeConverter()->convertType(rewriter.getI8Type()),\n+        NVVM::kSharedMemorySpace);\n+    auto offsetValue = gep(ptrTy, base, i32_val(offset));\n+    auto promotedOperands = this->getTypeConverter()->promoteOperands(\n+        callOp.getLoc(), /*opOperands=*/callOp->getOperands(),\n+        adaptor.getOperands(), rewriter);\n+    promotedOperands.push_back(offsetValue);\n+    return promotedOperands;\n+  }\n+\n+  LLVM::CallOp\n+  convertCallOpToLLVMCallOp(triton::CallOp callOp,\n+                            ArrayRef<Value> promotedOperands,\n+                            ConversionPatternRewriter &rewriter) const {\n+    // Pack the result types into a struct.\n+    Type packedResult = nullptr;\n+    unsigned numResults = callOp.getNumResults();\n+    auto resultTypes = llvm::to_vector<4>(callOp.getResultTypes());\n+\n+    if (numResults != 0) {\n+      if (!(packedResult =\n+                this->getTypeConverter()->packFunctionResults(resultTypes)))\n+        return nullptr;\n+    }\n+\n+    auto newCallOp = rewriter.create<LLVM::CallOp>(\n+        callOp.getLoc(), packedResult ? TypeRange(packedResult) : TypeRange(),\n+        promotedOperands, callOp->getAttrs());\n+    return newCallOp;\n+  }\n+\n+  SmallVector<Value>\n+  getCallOpResults(triton::CallOp callOp, LLVM::CallOp newCallOp,\n+                   ConversionPatternRewriter &rewriter) const {\n+    auto numResults = callOp.getNumResults();\n+    SmallVector<Value> results;\n+    if (numResults < 2) {\n+      // If < 2 results, packing did not do anything and we can just return.\n+      results.append(newCallOp.result_begin(), newCallOp.result_end());\n+    } else {\n+      // Otherwise, it had been converted to an operation producing a structure.\n+      // Extract individual results from the structure and return them as list.\n+      results.reserve(numResults);\n+      for (unsigned i = 0; i < numResults; ++i) {\n+        results.push_back(rewriter.create<LLVM::ExtractValueOp>(\n+            callOp.getLoc(), newCallOp->getResult(0), i));\n+      }\n+    }\n+    return results;\n+  }\n+\n+  int numWarps{0};\n+  ModuleAllocation &allocation;\n };\n \n class TritonLLVMConversionTarget : public ConversionTarget {\n@@ -143,63 +288,75 @@ class ConvertTritonGPUToLLVM\n     TritonLLVMConversionTarget target(*context, isROCM);\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n \n-    /* preprocess */\n+    // Preprocess\n     decomposeMmaToDotOperand(mod, numWarps);\n     decomposeBlockedToDotOperand(mod);\n     if (failed(decomposeInsertSliceAsyncOp(mod)))\n       return signalPassFailure();\n \n-    /* allocate shared memory and set barrier */\n-    Allocation allocation(mod);\n-    MembarAnalysis membarPass(&allocation);\n+    // Allocate shared memory and set barrier\n+    ModuleAllocation allocation(mod);\n+    ModuleMembarAnalysis membarPass(&allocation);\n     membarPass.run();\n \n-    /* lower functions */\n+    // Lower functions\n     {\n       mlir::LowerToLLVMOptions option(context);\n       TritonGPUToLLVMTypeConverter typeConverter(context, option);\n       TritonLLVMFunctionConversionTarget funcTarget(*context, isROCM);\n       RewritePatternSet funcPatterns(context);\n-      funcPatterns.add<FuncOpConversion>(typeConverter, numWarps,\n+      funcPatterns.add<FuncOpConversion>(typeConverter, numWarps, allocation,\n                                          /*benefit=*/1);\n-      funcPatterns.add<ReturnOpConversion>(typeConverter);\n       mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n                                                             funcPatterns);\n       if (failed(\n               applyPartialConversion(mod, funcTarget, std::move(funcPatterns))))\n         return signalPassFailure();\n     }\n \n-    std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n-    AxisInfoAnalysis *axisInfoAnalysis = solver->load<AxisInfoAnalysis>();\n-    if (failed(solver->initializeAndRun(mod)))\n-      return signalPassFailure();\n-    initSharedMemory(allocation.getSharedMemorySize(), typeConverter);\n-    mod->setAttr(\"triton_gpu.shared\",\n-                 mlir::IntegerAttr::get(mlir::IntegerType::get(context, 32),\n-                                        allocation.getSharedMemorySize()));\n+    // initSharedMemory is run before the conversion of call and ret ops,\n+    // because the call op has to know the shared memory base address of each\n+    // function\n+    initSharedMemory(allocation, typeConverter);\n+\n+    // Convert call and ret ops\n+    {\n+      mlir::LowerToLLVMOptions option(context);\n+      TritonGPUToLLVMTypeConverter typeConverter(context, option);\n+      TritonLLVMFunctionConversionTarget funcTarget(*context, isROCM);\n+      RewritePatternSet funcPatterns(context);\n+      funcPatterns.add<CallOpConversion>(typeConverter, numWarps, allocation,\n+                                         /*benefit=*/1);\n+      funcPatterns.add<ReturnOpConversion>(typeConverter, /*benefit=*/1);\n+      if (failed(\n+              applyPartialConversion(mod, funcTarget, std::move(funcPatterns))))\n+        return signalPassFailure();\n+    }\n \n-    /* rewrite ops */\n+    ModuleAxisInfoAnalysis axisInfoAnalysis(mod);\n+    // Rewrite ops\n     RewritePatternSet patterns(context);\n     // TritonGPU lowering patterns\n     OpBuilder::InsertPoint indexInsertPoint;\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo indexCacheInfo{\n         &baseIndexCache, &indexCache, &indexInsertPoint};\n-    auto populatePatterns1 = [&](auto populateFunc) {\n-      populateFunc(typeConverter, patterns, numWarps, *axisInfoAnalysis,\n-                   &allocation, smem, indexCacheInfo, /*benefit*/ 1);\n-    };\n-    auto populatePatterns2 = [&](auto populateFunc) {\n-      populateFunc(typeConverter, patterns, numWarps, *axisInfoAnalysis,\n-                   &allocation, smem, /*benefit*/ 1);\n-    };\n-    populatePatterns1(populateTritonGPUToLLVMPatterns);\n-    populatePatterns1(populateConvertLayoutOpToLLVMPatterns);\n-    populatePatterns2(populateDotOpToLLVMPatterns);\n-    populatePatterns2(populateElementwiseOpToLLVMPatterns);\n-    populatePatterns1(populateLoadStoreOpToLLVMPatterns);\n-    populatePatterns1(populateReduceOpToLLVMPatterns);\n-    populatePatterns2(populateViewOpToLLVMPatterns);\n+    // TODO: enable index cache if there are multiple functions\n+    if (axisInfoAnalysis.getNumFunctions() > 1) {\n+      indexCacheInfo = {nullptr, nullptr, nullptr};\n+    }\n+    populateTritonGPUToLLVMPatterns(typeConverter, patterns, allocation,\n+                                    indexCacheInfo, /*benefit=*/1);\n+    populateConvertLayoutOpToLLVMPatterns(typeConverter, patterns, allocation,\n+                                          indexCacheInfo, /*benefit=*/1);\n+    populateDotOpToLLVMPatterns(typeConverter, patterns, allocation,\n+                                /*benefit=*/1);\n+    populateElementwiseOpToLLVMPatterns(typeConverter, patterns, /*benefit=*/1);\n+    populateLoadStoreOpToLLVMPatterns(typeConverter, patterns, axisInfoAnalysis,\n+                                      allocation, indexCacheInfo,\n+                                      /*benefit=*/1);\n+    populateReduceOpToLLVMPatterns(typeConverter, patterns, allocation,\n+                                   indexCacheInfo, /*benefit=*/1);\n+    populateViewOpToLLVMPatterns(typeConverter, patterns, /*benefit=*/1);\n \n     // Native lowering patterns\n     if (isROCM) {\n@@ -216,8 +373,6 @@ class ConvertTritonGPUToLLVM\n   }\n \n private:\n-  Value smem;\n-\n   using IndexCacheKeyT = std::pair<Attribute, RankedTensorType>;\n   DenseMap<IndexCacheKeyT, SmallVector<Value>, CacheKeyDenseMapInfo>\n       baseIndexCache;\n@@ -228,10 +383,11 @@ class ConvertTritonGPUToLLVM\n   int computeCapability{};\n   bool isROCM{};\n \n-  void initSharedMemory(size_t size,\n+  void initSharedMemory(ModuleAllocation &allocation,\n                         TritonGPUToLLVMTypeConverter &typeConverter) {\n     ModuleOp mod = getOperation();\n     OpBuilder b(mod.getBodyRegion());\n+    auto ctx = mod.getContext();\n     auto loc = mod.getLoc();\n     auto elemTy = typeConverter.convertType(b.getIntegerType(8));\n     // Set array size 0 and external linkage indicates that we use dynamic\n@@ -242,15 +398,23 @@ class ConvertTritonGPUToLLVM\n         \"global_smem\", /*value=*/Attribute(), /*alignment=*/0,\n         // Add ROCm support.\n         static_cast<unsigned>(NVVM::NVVMMemorySpace::kSharedMemorySpace));\n-    SmallVector<LLVM::LLVMFuncOp> funcs;\n-    mod.walk([&](LLVM::LLVMFuncOp func) { funcs.push_back(func); });\n-    assert(funcs.size() == 1 &&\n-           \"Inliner pass is expected before TritonGPUToLLVM\");\n-    b.setInsertionPointToStart(&funcs[0].getBody().front());\n-    smem = b.create<LLVM::AddressOfOp>(loc, global);\n-    auto ptrTy =\n-        LLVM::LLVMPointerType::get(typeConverter.convertType(b.getI8Type()), 3);\n-    smem = b.create<LLVM::BitcastOp>(loc, ptrTy, smem);\n+    mod.walk([&](FunctionOpInterface funcOp) {\n+      Value funcSmem;\n+      b.setInsertionPointToStart(&funcOp.getFunctionBody().front());\n+      if (allocation.isRoot(funcOp)) {\n+        funcSmem = b.create<LLVM::AddressOfOp>(loc, global);\n+      } else {\n+        funcSmem = funcOp.getArgument(funcOp.getNumArguments() - 1);\n+      }\n+      auto ptrTy =\n+          LLVM::LLVMPointerType::get(typeConverter.convertType(b.getI8Type()),\n+                                     NVVM::NVVMMemorySpace::kSharedMemorySpace);\n+      funcSmem = b.create<LLVM::BitcastOp>(loc, ptrTy, funcSmem);\n+      allocation.setFunctionSharedMemoryValue(funcOp, funcSmem);\n+    });\n+    mod->setAttr(\"triton_gpu.shared\",\n+                 mlir::IntegerAttr::get(mlir::IntegerType::get(ctx, 32),\n+                                        allocation.getSharedMemorySize()));\n   }\n \n   void decomposeMmaToDotOperand(ModuleOp mod, int numWarps) const {\n@@ -308,10 +472,7 @@ class ConvertTritonGPUToLLVM\n   }\n \n   LogicalResult decomposeInsertSliceAsyncOp(ModuleOp mod) const {\n-    std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n-    AxisInfoAnalysis *axisInfoAnalysis = solver->load<AxisInfoAnalysis>();\n-    if (failed(solver->initializeAndRun(mod)))\n-      return failure();\n+    ModuleAxisInfoAnalysis axisInfoAnalysis(mod);\n     // TODO(Keren): This is a hacky knob that may cause performance regression\n     // when decomposition has been performed. We should remove this knob once we\n     // have thorough analysis on async wait. Currently, we decompose\n@@ -345,7 +506,7 @@ class ConvertTritonGPUToLLVM\n       auto resSharedLayout =\n           dstTy.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n       auto resElemTy = dstTy.getElementType();\n-      unsigned inVec = axisInfoAnalysis->getPtrContiguity(src);\n+      unsigned inVec = axisInfoAnalysis.getPtrContiguity(src);\n       unsigned outVec = resSharedLayout.getVec();\n       unsigned minVec = std::min(outVec, inVec);\n       auto maxBitWidth ="}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 53, "deletions": 15, "changes": 68, "file_content_changes": "@@ -116,23 +116,64 @@ struct CatOpConversion : public ConvertTritonGPUOpToLLVMPattern<CatOp> {\n   }\n };\n \n-template <typename SourceOp>\n-struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n-  using OpAdaptor = typename SourceOp::Adaptor;\n-  explicit ViewLikeOpConversion(TritonGPUToLLVMTypeConverter &typeConverter,\n-                                PatternBenefit benefit = 1)\n-      : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n+struct ViewOpConversion : public ConvertTritonGPUOpToLLVMPattern<ViewOp> {\n+  using OpAdaptor = typename ViewOp::Adaptor;\n+  explicit ViewOpConversion(TritonGPUToLLVMTypeConverter &typeConverter,\n+                            PatternBenefit benefit = 1)\n+      : ConvertTritonGPUOpToLLVMPattern<ViewOp>(typeConverter, benefit) {}\n \n   LogicalResult\n-  matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n+  matchAndRewrite(ViewOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n     auto resultTy = op.getType().template cast<RankedTensorType>();\n     auto vals = this->getTypeConverter()->unpackLLElements(\n         loc, adaptor.getSrc(), rewriter, op.getOperand().getType());\n-    Value view =\n+    Value ret =\n         this->getTypeConverter()->packLLElements(loc, vals, rewriter, resultTy);\n-    rewriter.replaceOp(op, view);\n+    rewriter.replaceOp(op, ret);\n+    return success();\n+  }\n+};\n+\n+struct ExpandDimsOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<ExpandDimsOp> {\n+  using OpAdaptor = typename ExpandDimsOp::Adaptor;\n+  explicit ExpandDimsOpConversion(TritonGPUToLLVMTypeConverter &typeConverter,\n+                                  PatternBenefit benefit = 1)\n+      : ConvertTritonGPUOpToLLVMPattern<ExpandDimsOp>(typeConverter, benefit) {}\n+\n+  LogicalResult\n+  matchAndRewrite(ExpandDimsOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    auto srcVals = this->getTypeConverter()->unpackLLElements(\n+        loc, adaptor.getSrc(), rewriter, op.getOperand().getType());\n+\n+    auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n+    auto resultTy = op.getType().template cast<RankedTensorType>();\n+\n+    assert(srcTy.getEncoding().isa<SliceEncodingAttr>() &&\n+           \"ExpandDimsOp only support SliceEncodingAttr\");\n+    auto srcLayout = srcTy.getEncoding().dyn_cast<SliceEncodingAttr>();\n+    auto resultLayout = resultTy.getEncoding();\n+\n+    auto srcOffsets = emitOffsetForLayout(srcLayout, srcTy);\n+    auto resultOffsets = emitOffsetForLayout(resultLayout, resultTy);\n+    DenseMap<SmallVector<unsigned>, Value, SmallVectorKeyInfo> srcValues;\n+    for (size_t i = 0; i < srcOffsets.size(); i++) {\n+      srcValues[srcOffsets[i]] = srcVals[i];\n+    }\n+\n+    SmallVector<Value> resultVals;\n+    for (size_t i = 0; i < resultOffsets.size(); i++) {\n+      auto offset = resultOffsets[i];\n+      offset.erase(offset.begin() + srcLayout.getDim());\n+      resultVals.push_back(srcValues.lookup(offset));\n+    }\n+    Value ret = this->getTypeConverter()->packLLElements(loc, resultVals,\n+                                                         rewriter, resultTy);\n+    rewriter.replaceOp(op, ret);\n     return success();\n   }\n };\n@@ -161,13 +202,10 @@ struct TransOpConversion\n };\n \n void populateViewOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n-                                  RewritePatternSet &patterns, int numWarps,\n-                                  AxisInfoAnalysis &axisInfoAnalysis,\n-                                  const Allocation *allocation, Value smem,\n+                                  RewritePatternSet &patterns,\n                                   PatternBenefit benefit) {\n-  patterns.add<ViewLikeOpConversion<triton::ViewOp>>(typeConverter, benefit);\n-  patterns.add<ViewLikeOpConversion<triton::ExpandDimsOp>>(typeConverter,\n-                                                           benefit);\n+  patterns.add<ViewOpConversion>(typeConverter, benefit);\n+  patterns.add<ExpandDimsOpConversion>(typeConverter, benefit);\n   patterns.add<SplatOpConversion>(typeConverter, benefit);\n   patterns.add<ArithConstantSplatOpConversion>(typeConverter, benefit);\n   patterns.add<CatOpConversion>(typeConverter, benefit);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.h", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -7,9 +7,7 @@ using namespace mlir;\n using namespace mlir::triton;\n \n void populateViewOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n-                                  RewritePatternSet &patterns, int numWarps,\n-                                  AxisInfoAnalysis &axisInfoAnalysis,\n-                                  const Allocation *allocation, Value smem,\n+                                  RewritePatternSet &patterns,\n                                   PatternBenefit benefit);\n \n #endif"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 52, "deletions": 25, "changes": 77, "file_content_changes": "@@ -165,8 +165,6 @@ void populateStdPatternsAndLegality(TritonGPUTypeConverter &typeConverter,\n   MLIRContext *context = patterns.getContext();\n   // Rewrite rule\n   patterns.add<StdSelectPattern>(typeConverter, context);\n-  target.addLegalOp<triton::ReturnOp>(); // this is ok because all functions are\n-                                         // inlined by the frontend\n }\n \n void populateMathPatternsAndLegality(TritonGPUTypeConverter &typeConverter,\n@@ -529,6 +527,55 @@ struct TritonAssertPattern : public OpConversionPattern<triton::AssertOp> {\n   }\n };\n \n+class TritonFuncOpPattern : public OpConversionPattern<triton::FuncOp> {\n+public:\n+  using OpConversionPattern<triton::FuncOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::FuncOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto converter = getTypeConverter();\n+    auto newOp = rewriter.replaceOpWithNewOp<triton::FuncOp>(\n+        op, op.getName(), op.getFunctionType());\n+    addNamedAttrs(newOp, adaptor.getAttributes());\n+    rewriter.inlineRegionBefore(op.getBody(), newOp.getBody(),\n+                                newOp.getBody().end());\n+    if (failed(rewriter.convertRegionTypes(&newOp.getBody(), *converter)))\n+      return failure();\n+\n+    return success();\n+  }\n+};\n+\n+class TritonCallOpPattern : public OpConversionPattern<triton::CallOp> {\n+public:\n+  using OpConversionPattern<triton::CallOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::CallOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto converter = getTypeConverter();\n+    auto newOp = rewriter.replaceOpWithNewOp<triton::CallOp>(\n+        op, op.getCallee(), op.getResultTypes(), adaptor.getOperands());\n+    addNamedAttrs(newOp, adaptor.getAttributes());\n+    return success();\n+  }\n+};\n+\n+class TritonReturnOpPattern : public OpConversionPattern<ReturnOp> {\n+public:\n+  using OpConversionPattern<ReturnOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(ReturnOp op, ReturnOp::Adaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto converter = getTypeConverter();\n+    auto newOp =\n+        rewriter.replaceOpWithNewOp<ReturnOp>(op, adaptor.getOperands());\n+    return success();\n+  }\n+};\n+\n void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n                             RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n@@ -546,7 +593,8 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n           TritonLoadPattern, TritonStorePattern,\n           TritonExternElementwisePattern<triton::PureExternElementwiseOp>,\n           TritonExternElementwisePattern<triton::ImpureExternElementwiseOp>,\n-          TritonPrintPattern, TritonAssertPattern, TritonAtomicRMWPattern>(\n+          TritonPrintPattern, TritonAssertPattern, TritonAtomicRMWPattern,\n+          TritonFuncOpPattern, TritonReturnOpPattern, TritonCallOpPattern>(\n           typeConverter, context);\n }\n \n@@ -748,31 +796,10 @@ class CFCondBranchPattern : public OpConversionPattern<cf::CondBranchOp> {\n   }\n };\n \n-class FuncOpPattern : public OpConversionPattern<triton::FuncOp> {\n-public:\n-  using OpConversionPattern<triton::FuncOp>::OpConversionPattern;\n-\n-  LogicalResult\n-  matchAndRewrite(triton::FuncOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    auto converter = getTypeConverter();\n-    auto newOp = rewriter.replaceOpWithNewOp<triton::FuncOp>(\n-        op, op.getName(), op.getFunctionType());\n-    addNamedAttrs(newOp, adaptor.getAttributes());\n-    rewriter.inlineRegionBefore(op.getBody(), newOp.getBody(),\n-                                newOp.getBody().end());\n-    if (failed(rewriter.convertRegionTypes(&newOp.getBody(), *converter)))\n-      return failure();\n-\n-    return success();\n-  }\n-};\n-\n void populateCFPatterns(TritonGPUTypeConverter &typeConverter,\n                         RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n-  patterns.add<FuncOpPattern, CFCondBranchPattern, CFBranchPattern>(\n-      typeConverter, context);\n+  patterns.add<CFCondBranchPattern, CFBranchPattern>(typeConverter, context);\n }\n //\n "}, {"filename": "lib/Dialect/Triton/IR/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,5 +1,4 @@\n add_mlir_dialect_library(TritonIR\n-  Interfaces.cpp\n   Dialect.cpp\n   Ops.cpp\n   Types.cpp"}, {"filename": "lib/Dialect/Triton/IR/Dialect.cpp", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -22,14 +22,22 @@ using namespace mlir::triton;\n namespace {\n struct TritonInlinerInterface : public DialectInlinerInterface {\n   using DialectInlinerInterface::DialectInlinerInterface;\n+\n   bool isLegalToInline(Operation *call, Operation *callable,\n                        bool wouldBeCloned) const final {\n+    auto funcOp = dyn_cast<triton::FuncOp>(callable);\n+    if (!funcOp)\n+      return true;\n+    if (funcOp->hasAttr(\"noinline\"))\n+      return !funcOp->getAttrOfType<BoolAttr>(\"noinline\").getValue();\n     return true;\n   }\n+\n   bool isLegalToInline(Region *dest, Region *src, bool wouldBeCloned,\n                        IRMapping &valueMapping) const final {\n     return true;\n   }\n+\n   bool isLegalToInline(Operation *, Region *, bool wouldBeCloned,\n                        IRMapping &) const final {\n     return true;"}, {"filename": "lib/Dialect/Triton/IR/Interfaces.cpp", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 36, "deletions": 4, "changes": 40, "file_content_changes": "@@ -103,10 +103,9 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n     return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n                                  blockedLayout.getSizePerThread().end());\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-    auto ret = getSizePerThread(sliceLayout.getParent());\n-    return ret;\n-    // ret.erase(ret.begin() + sliceLayout.getDim());\n-    return ret;\n+    auto sizePerThread = getSizePerThread(sliceLayout.getParent());\n+    sizePerThread.erase(sizePerThread.begin() + sliceLayout.getDim());\n+    return sizePerThread;\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     if (mmaLayout.isAmpere()) {\n       return {2, 2};\n@@ -146,11 +145,43 @@ SmallVector<unsigned> getContigPerThread(Attribute layout) {\n   if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     assert(mmaLayout.isVolta() || mmaLayout.isAmpere());\n     return {1, 2};\n+  } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    auto parentLayout = sliceLayout.getParent();\n+    return getContigPerThread(parentLayout);\n   } else {\n     return getSizePerThread(layout);\n   }\n }\n \n+SmallVector<unsigned> getUniqueContigPerThread(Type type) {\n+  if (type.isIntOrIndexOrFloat() || type.isa<triton::PointerType>())\n+    return SmallVector<unsigned>(1, 1);\n+  auto tensorType = type.cast<RankedTensorType>();\n+  auto shape = tensorType.getShape();\n+  // If slice layout, call recursively on parent layout, and drop\n+  // sliced dim\n+  if (auto sliceLayout =\n+          tensorType.getEncoding().dyn_cast<SliceEncodingAttr>()) {\n+    auto parentLayout = sliceLayout.getParent();\n+    auto parentShape = sliceLayout.paddedShape(shape);\n+    auto parentTy = RankedTensorType::get(\n+        parentShape, tensorType.getElementType(), parentLayout);\n+    auto parentUniqueContigPerThread = getUniqueContigPerThread(parentTy);\n+    parentUniqueContigPerThread.erase(parentUniqueContigPerThread.begin() +\n+                                      sliceLayout.getDim());\n+    return parentUniqueContigPerThread;\n+  }\n+  // Base case\n+  auto rank = shape.size();\n+  SmallVector<unsigned> ret(rank);\n+  auto contigPerThread = getContigPerThread(tensorType.getEncoding());\n+  assert(contigPerThread.size() == rank && \"Unexpected contigPerThread size\");\n+  for (int d = 0; d < rank; ++d) {\n+    ret[d] = std::min<unsigned>(shape[d], contigPerThread[d]);\n+  }\n+  return ret;\n+}\n+\n SmallVector<unsigned> getThreadsPerCTA(Attribute layout) {\n   SmallVector<unsigned> threads;\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n@@ -375,6 +406,7 @@ SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n   auto parent = getParent();\n   auto parentElemsPerThread =\n       ::getElemsPerThread(parent, paddedShape(shape), eltTy);\n+  parentElemsPerThread.erase(parentElemsPerThread.begin() + getDim());\n   return parentElemsPerThread;\n }\n unsigned SliceEncodingAttr::getTotalElemsPerThread(ArrayRef<int64_t> shape,"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 18, "deletions": 23, "changes": 41, "file_content_changes": "@@ -22,16 +22,13 @@ template <class T> SmallVector<unsigned, 4> argSort(const T &arr) {\n typedef DenseMap<Value, std::function<Type(Type)>> LayoutMap;\n \n struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n-  Attribute getCoalescedEncoding(AxisInfoAnalysis &axisInfo, Value ptr,\n-                                 int numWarps) {\n+  Attribute getCoalescedEncoding(ModuleAxisInfoAnalysis &axisInfoAnalysis,\n+                                 Value ptr, int numWarps) {\n     auto origType = ptr.getType().cast<RankedTensorType>();\n     // Get the shape of the tensor.\n     size_t rank = origType.getRank();\n-    dataflow::Lattice<AxisInfo> *latticeElement =\n-        axisInfo.getLatticeElement(ptr);\n-    AxisInfo info = latticeElement ? latticeElement->getValue() : AxisInfo();\n     // Get the contiguity order of `ptr`\n-    auto order = argSort(info.getContiguity());\n+    auto order = argSort(axisInfoAnalysis.getAxisInfo(ptr)->getContiguity());\n     // The desired divisibility is the maximum divisibility\n     // among all dependent pointers who have the same order as\n     // `ptr`\n@@ -42,8 +39,8 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n         for (Value val : op->getResults()) {\n           if (val.getType() != origType)\n             continue;\n-          auto valInfo = axisInfo.getLatticeElement(val);\n-          auto currOrder = argSort(valInfo->getValue().getContiguity());\n+          auto currOrder =\n+              argSort(axisInfoAnalysis.getAxisInfo(val)->getContiguity());\n           if (order == currOrder)\n             withSameOrder.insert(val);\n         }\n@@ -57,10 +54,11 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     unsigned elemNumBytes = std::max(elemNumBits / 8, 1u);\n     unsigned perThread = 1;\n     for (Value val : withSameOrder) {\n-      AxisInfo info = axisInfo.getLatticeElement(val)->getValue();\n-      unsigned maxMultipleBytes = info.getDivisibility(order[0]);\n+      unsigned maxMultipleBytes =\n+          axisInfoAnalysis.getAxisInfo(val)->getDivisibility(order[0]);\n       unsigned maxMultiple = std::max(maxMultipleBytes / elemNumBytes, 1u);\n-      unsigned maxContig = info.getContiguity(order[0]);\n+      unsigned maxContig =\n+          axisInfoAnalysis.getAxisInfo(val)->getContiguity(order[0]);\n       unsigned alignment = std::min(maxMultiple, maxContig);\n       unsigned currPerThread = std::min(alignment, 128 / elemNumBits);\n       perThread = std::max(perThread, currPerThread);\n@@ -74,9 +72,10 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     return encoding;\n   }\n \n-  std::function<Type(Type)> getTypeConverter(AxisInfoAnalysis &axisInfo,\n-                                             Value ptr, int numWarps) {\n-    Attribute encoding = getCoalescedEncoding(axisInfo, ptr, numWarps);\n+  std::function<Type(Type)>\n+  getTypeConverter(ModuleAxisInfoAnalysis &axisInfoAnalysis, Value ptr,\n+                   int numWarps) {\n+    Attribute encoding = getCoalescedEncoding(axisInfoAnalysis, ptr, numWarps);\n     return [encoding](Type _type) {\n       RankedTensorType type = _type.cast<RankedTensorType>();\n       return RankedTensorType::get(type.getShape(), type.getElementType(),\n@@ -123,17 +122,14 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n   }\n \n   void runOnOperation() override {\n-    Operation *op = getOperation();\n     // Run axis info analysis\n-    std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n-    AxisInfoAnalysis *axisInfo = solver->load<AxisInfoAnalysis>();\n-    if (failed(solver->initializeAndRun(op)))\n-      return signalPassFailure();\n+    ModuleOp moduleOp = getOperation();\n+    ModuleAxisInfoAnalysis axisInfoAnalysis(moduleOp);\n \n     // For each i/o operation, we determine what layout\n     // the pointers should have for best memory coalescing\n     LayoutMap layoutMap;\n-    op->walk([&](Operation *curr) {\n+    moduleOp.walk([&](Operation *curr) {\n       Value ptr;\n       if (auto op = dyn_cast<triton::LoadOp>(curr))\n         ptr = op.getPtr();\n@@ -150,10 +146,9 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n       RankedTensorType ty = ptr.getType().template dyn_cast<RankedTensorType>();\n       if (!ty || !ty.getElementType().isa<PointerType>())\n         return;\n-      AxisInfo info = axisInfo->getLatticeElement(ptr)->getValue();\n       auto mod = curr->getParentOfType<ModuleOp>();\n       int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n-      auto convertType = getTypeConverter(*axisInfo, ptr, numWarps);\n+      auto convertType = getTypeConverter(axisInfoAnalysis, ptr, numWarps);\n       layoutMap[ptr] = convertType;\n     });\n \n@@ -164,7 +159,7 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     //    produces a tensor with layout L2\n     // 4. Convert the output of this new memory op back to L1\n     // 5. Replace all the uses of the original memory op by the new one\n-    op->walk([&](Operation *curr) {\n+    moduleOp.walk([&](Operation *curr) {\n       OpBuilder builder(curr);\n       if (auto load = dyn_cast<triton::LoadOp>(curr)) {\n         coalesceOp<triton::LoadOp>(layoutMap, curr, load.getPtr(), builder);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 4, "deletions": 9, "changes": 13, "file_content_changes": "@@ -185,24 +185,19 @@ ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(Operation *op,\n ///                                                  this pass?)\n LogicalResult LoopPipeliner::initialize() {\n   Block *loop = forOp.getBody();\n-\n-  std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n-  AxisInfoAnalysis *axisInfoAnalysis = solver->load<AxisInfoAnalysis>();\n-  if (failed(solver->initializeAndRun(forOp->getParentOfType<ModuleOp>()))) {\n-    return failure();\n-  }\n+  ModuleOp moduleOp = forOp->getParentOfType<ModuleOp>();\n+  ModuleAxisInfoAnalysis axisInfoAnalysis(moduleOp);\n \n   // can we use forOp.walk(...) here?\n   SmallVector<triton::LoadOp, 2> validLoads;\n   for (Operation &op : *loop)\n     if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n       auto ptr = loadOp.getPtr();\n-      unsigned vec = axisInfoAnalysis->getPtrContiguity(ptr);\n+      unsigned vec = axisInfoAnalysis.getPtrContiguity(ptr);\n \n       if (auto mask = loadOp.getMask())\n-        vec = std::min<unsigned>(vec, axisInfoAnalysis->getMaskAlignment(mask));\n+        vec = std::min<unsigned>(vec, axisInfoAnalysis.getMaskAlignment(mask));\n \n-      auto lattice = axisInfoAnalysis->getLatticeElement(ptr)->getValue();\n       auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n       if (!tensorTy || tensorTy.getRank() < 2)\n         continue;"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "file_content_changes": "@@ -452,7 +452,8 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::OpBuilder &self, mlir::triton::FuncOp &func,\n               std::vector<mlir::Value> &args) -> mlir::OpState {\n              auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::CallOp>(loc, func, args);\n+             auto callOp = self.create<mlir::triton::CallOp>(loc, func, args);\n+             return callOp;\n            })\n       // insertion block/point\n       .def(\"set_insertion_point_to_start\",\n@@ -634,14 +635,16 @@ void init_triton_ir(py::module &&m) {\n       .def(\"get_or_insert_function\",\n            [](mlir::OpBuilder &self, mlir::ModuleOp &module,\n               std::string &funcName, mlir::Type &funcType,\n-              std::string &visibility) -> mlir::triton::FuncOp {\n+              std::string &visibility, bool noinline) -> mlir::triton::FuncOp {\n              if (mlir::Operation *funcOperation = module.lookupSymbol(funcName))\n                return llvm::dyn_cast<mlir::triton::FuncOp>(funcOperation);\n              auto loc = self.getUnknownLoc();\n              if (auto funcTy = funcType.dyn_cast<mlir::FunctionType>()) {\n                llvm::SmallVector<mlir::NamedAttribute> attrs = {\n                    mlir::NamedAttribute(self.getStringAttr(\"sym_visibility\"),\n-                                        self.getStringAttr(visibility))};\n+                                        self.getStringAttr(visibility)),\n+                   mlir::NamedAttribute(self.getStringAttr(\"noinline\"),\n+                                        self.getBoolAttr(noinline))};\n                return self.create<mlir::triton::FuncOp>(loc, funcName, funcTy,\n                                                         attrs);\n              }"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 76, "deletions": 2, "changes": 78, "file_content_changes": "@@ -688,7 +688,7 @@ def catch_compilation_error(kernel):\n \n \n @triton.jit\n-def fn(a, b):\n+def tuples_fn(a, b):\n     return a + b, \\\n         a - b, \\\n         a * b\n@@ -701,7 +701,7 @@ def test_tuples():\n     def with_fn(X, Y, A, B, C):\n         x = tl.load(X)\n         y = tl.load(Y)\n-        a, b, c = fn(x, y)\n+        a, b, c = tuples_fn(x, y)\n         tl.store(A, a)\n         tl.store(B, b)\n         tl.store(C, c)\n@@ -728,6 +728,80 @@ def without_fn(X, Y, A, B, C):\n         assert c_tri == c_ref\n \n \n+@triton.jit(noinline=True)\n+def noinline_simple_fn(x, y, Z):\n+    z = x + y\n+    tl.store(Z, z)\n+\n+\n+@triton.jit(noinline=True)\n+def noinline_call_graph_fn1(x):\n+    return x + 1\n+\n+\n+@triton.jit(noinline=True)\n+def noinline_call_graph_fn2(y):\n+    return y + 2\n+\n+\n+@triton.jit(noinline=True)\n+def noinline_call_graph_fn(x, y, Z):\n+    t0 = noinline_call_graph_fn1(x)\n+    t1 = noinline_call_graph_fn2(y)\n+    z = t0 + t1\n+    tl.store(Z, z)\n+\n+\n+@triton.jit(noinline=True)\n+def noinline_shared_fn(x, y, Z):\n+    offs = tl.arange(0, 16)[:, None] * 16 + tl.arange(0, 16)[None, :]\n+    z = tl.load(Z + offs)\n+    z = tl.dot(z, z) + x + y\n+    tl.store(Z + offs, z)\n+\n+\n+@triton.jit(noinline=True)\n+def noinline_dynamic_fn(x, y, Z):\n+    if x >= 1:\n+        x = noinline_call_graph_fn1(x)\n+    else:\n+        x = noinline_call_graph_fn2(x)\n+    if y >= 2:\n+        y = noinline_call_graph_fn2(y)\n+    else:\n+        y = noinline_call_graph_fn1(y)\n+    z = x + y\n+    tl.store(Z, z)\n+\n+\n+@pytest.mark.parametrize(\"mode\", [\"simple\", \"call_graph\", \"shared\", \"dynamic\"])\n+def test_noinline(mode):\n+    device = 'cuda'\n+\n+    @triton.jit\n+    def kernel(X, Y, Z):\n+        x = tl.load(X)\n+        y = tl.load(Y)\n+        GENERATE_TEST_HERE(x, y, Z)\n+\n+    func_name = f'noinline_{mode}_fn'\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': func_name})\n+    x = torch.tensor([1.0], device=device, dtype=torch.float32)\n+    y = torch.tensor([2.0], device=device, dtype=torch.float32)\n+    if mode == \"shared\":\n+        z = torch.ones((16, 16), device=device, dtype=torch.float32)\n+    else:\n+        z = torch.tensor([0.0], device=device, dtype=torch.float32)\n+    kernel[(1,)](x, y, z, num_warps=1)\n+    if mode == \"simple\":\n+        assert torch.equal(z, x + y)\n+    elif mode == \"call_graph\" or mode == \"dynamic\":\n+        assert torch.equal(z, x + 1 + y + 2)\n+    elif mode == \"shared\":\n+        ref = torch.full((16, 16), 16, device=device, dtype=torch.float32)\n+        assert torch.equal(z, ref + x + y)\n+\n+\n # ---------------\n # test atomics\n # ---------------"}, {"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 48, "deletions": 17, "changes": 65, "file_content_changes": "@@ -170,31 +170,62 @@ def kernel_add(a, b, o, N: tl.constexpr):\n     assert bins[0].asm['ttir'] != bins[1].asm['ttir']\n \n \n-def test_compile_in_subproc() -> None:\n+@triton.jit\n+def add_fn(a, b, o, N: tl.constexpr):\n+    idx = tl.arange(0, N)\n+    tl.store(o + idx, tl.load(a + idx) + tl.load(b + idx))\n+\n+\n+def test_jit_noinline() -> None:\n+    @triton.jit\n+    def kernel_add_device(a, b, o, N: tl.constexpr):\n+        add_fn(a, b, o, N)\n+\n+    device = torch.cuda.current_device()\n+    assert len(kernel_add_device.cache[device]) == 0\n+    kernel_add_device.warmup(torch.float32, torch.float32, torch.float32, 32, grid=(1,))\n+    assert len(kernel_add_device.cache[device]) == 1\n+    bins = list(kernel_add_device.cache[device].values())\n+    inline_ttir = bins[0].asm['ttir']\n+    add_fn.noinline = True\n+    add_fn.hash = None\n+    kernel_add_device.hash = None\n+    kernel_add_device.cache[device].clear()\n+    kernel_add_device.warmup(torch.float32, torch.float32, torch.float32, 32, grid=(1,))\n+    assert len(kernel_add_device.cache[device]) == 1\n+    bins = list(kernel_add_device.cache[device].values())\n+    noinline_ttir = bins[0].asm['ttir']\n+    assert inline_ttir != noinline_ttir\n+\n+\n+instance_descriptor = namedtuple(\"instance_descriptor\", [\"divisible_by_16\", \"equal_to_1\"])\n+\n+\n+def compile_fn(config, cc):\n     @triton.jit\n     def kernel_sub(a, b, o, N: tl.constexpr):\n         idx = tl.arange(0, N)\n-        tl.store(o + idx,\n-                 tl.load(a + idx) - tl.load(b + idx) * 777)\n+        tl.store(o + idx, tl.load(a + idx) - tl.load(b + idx) * 777)\n+    triton.compile(\n+        fn=kernel_sub,\n+        signature={0: \"*fp32\", 1: \"*fp32\", 2: \"*fp32\"},\n+        device=0,\n+        constants={3: 32},\n+        configs=[config],\n+        warm_cache_only=True,\n+        cc=cc,\n+    )\n \n+\n+def test_compile_in_subproc() -> None:\n     major, minor = torch.cuda.get_device_capability(0)\n     cc = major * 10 + minor\n-    config = namedtuple(\"instance_descriptor\", [\n-        \"divisible_by_16\", \"equal_to_1\"])(\n-        tuple(range(4)),\n-        ())\n+    config = instance_descriptor(tuple(range(4)), ())\n \n+    multiprocessing.set_start_method('spawn')\n     proc = multiprocessing.Process(\n-        target=triton.compile,\n-        kwargs=dict(\n-            fn=kernel_sub,\n-            signature={0: \"*fp32\", 1: \"*fp32\", 2: \"*fp32\"},\n-            device=0,\n-            constants={3: 32},\n-            configs=[config],\n-            warm_cache_only=True,\n-            cc=cc,\n-        ))\n+        target=compile_fn,\n+        args=(config, cc))\n     proc.start()\n     proc.join()\n     assert proc.exitcode == 0"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -46,6 +46,8 @@ def mangle_fn(name, arg_tys, constants):\n     mangled_constants = '_'.join([f'{i}c{repr(constants[i])}' for i in sorted(constants)])\n     mangled_constants = mangled_constants.replace('.', '_d_')\n     mangled_constants = mangled_constants.replace(\"'\", '_sq_')\n+    # [ and ] are not allowed in LLVM identifiers\n+    mangled_constants = mangled_constants.replace('[', '_').replace(']', '_')\n     ret = f'{name}__{mangled_arg_names}__{mangled_constants}'\n     return ret\n \n@@ -86,7 +88,8 @@ def __exit__(self, *args, **kwargs):\n \n class CodeGenerator(ast.NodeVisitor):\n     def __init__(self, context, prototype, gscope, attributes, constants, function_name,\n-                 module=None, is_kernel=False, function_types: Optional[Dict] = None, debug=False):\n+                 module=None, is_kernel=False, function_types: Optional[Dict] = None,\n+                 debug=False, noinline=False):\n         self.builder = ir.builder(context)\n         self.module = self.builder.create_module() if module is None else module\n         self.function_ret_types = {} if function_types is None else function_types\n@@ -99,6 +102,7 @@ def __init__(self, context, prototype, gscope, attributes, constants, function_n\n         self.is_kernel = is_kernel\n         self.last_node = None\n         self.debug = debug\n+        self.noinline = noinline\n         self.scf_stack = []\n         # SSA-construction\n         # name => language.tensor\n@@ -228,7 +232,7 @@ def visit_FunctionDef(self, node):\n             self.visit(init_node)\n         # initialize function\n         visibility = \"public\" if self.is_kernel else \"private\"\n-        fn = self.builder.get_or_insert_function(self.module, self.function_name, self.prototype.to_ir(self.builder), visibility)\n+        fn = self.builder.get_or_insert_function(self.module, self.function_name, self.prototype.to_ir(self.builder), visibility, self.noinline)\n         self.module.push_back(fn)\n         entry = fn.add_entry_block()\n         arg_values = []\n@@ -773,7 +777,7 @@ def call_JitFunction(self, fn: JITFunction, args, kwargs):\n         if not self.module.has_function(fn_name):\n             prototype = language.function_type([], arg_types)\n             gscope = sys.modules[fn.fn.__module__].__dict__\n-            generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module, function_name=fn_name, function_types=self.function_ret_types, debug=self.debug)\n+            generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module, function_name=fn_name, function_types=self.function_ret_types, debug=fn.debug, noinline=fn.noinline)\n             generator.visit(fn.parse())\n             callee_ret_type = generator.last_ret_type\n             self.function_ret_types[fn_name] = callee_ret_type"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "file_content_changes": "@@ -83,7 +83,8 @@ def visit_Call(self, node):\n             finder = DependenciesFinder(func.__globals__, func.src)\n             finder.visit(tree)\n             func.hash = finder.ret\n-        self.ret = (self.ret + func.hash).encode(\"utf-8\")\n+        noinline = str(getattr(func, 'noinline', False))\n+        self.ret = (self.ret + func.hash + noinline).encode(\"utf-8\")\n         self.ret = hashlib.md5(self.ret).hexdigest()\n \n # -----------------------------------------------------------------------------\n@@ -334,7 +335,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n         exec(src, scope)\n         return scope[self.fn.__name__]\n \n-    def __init__(self, fn, version=None, do_not_specialize=None, debug=None):\n+    def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinline=None):\n         self.fn = fn\n         self.module = fn.__module__\n         self.version = version\n@@ -356,6 +357,7 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None):\n         self.kernel_decorators = []\n         self.kernel = None\n         self.debug = os.environ.get(\"TRITON_DEBUG\", \"0\") == \"1\" if debug is None else debug\n+        self.noinline = noinline\n         # annotations\n         normalize_ty = lambda ty: ty.__name__ if isinstance(ty, type) else ty\n         self.__annotations__ = {name: normalize_ty(ty) for name, ty in fn.__annotations__.items()}\n@@ -425,6 +427,7 @@ def jit(\n     version=None,\n     do_not_specialize: Optional[Iterable[int]] = None,\n     debug: Optional[bool] = None,\n+    noinline: Optional[bool] = None,\n ) -> Callable[[T], JITFunction[T]]:\n     ...\n \n@@ -435,6 +438,7 @@ def jit(\n     version=None,\n     do_not_specialize: Optional[Iterable[int]] = None,\n     debug: Optional[bool] = None,\n+    noinline: Optional[bool] = None,\n ) -> Union[JITFunction[T], Callable[[T], JITFunction[T]]]:\n     \"\"\"\n     Decorator for JIT-compiling a function using the Triton compiler.\n@@ -461,6 +465,7 @@ def decorator(fn: T) -> JITFunction[T]:\n             version=version,\n             do_not_specialize=do_not_specialize,\n             debug=debug,\n+            noinline=noinline,\n         )\n \n     if fn is not None:"}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 85, "deletions": 4, "changes": 89, "file_content_changes": "@@ -402,8 +402,6 @@ tt.func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32\n \n // -----\n \n-module {\n-\n // This is a tiny test for verifying StoreOp-related alignment, It simply store a constant to a buffer.\n // CHECK-LABEL: @store_constant_align\n tt.func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n: i32 {tt.divisibility = 16 : i32}) {\n@@ -433,8 +431,6 @@ tt.func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   tt.return\n }\n \n-}\n-\n // -----\n \n // This IR is dumped from vecadd test.\n@@ -491,3 +487,88 @@ tt.func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %\n   tt.store %15, %13, %10 : tensor<64xf32>\n   tt.return\n }\n+\n+// -----\n+\n+module {\n+\n+// We don't use function cloning here, so the alignment info is the gcd of all call sites.\n+// CHECK-LABEL: @addptr_hints\n+tt.func @addptr_hints(%arg0: !tt.ptr<i32>) {\n+  // CHECK: contiguity = [1], divisibility = [1], constancy = [1], constant_value = 1\n+  %cst1 = arith.constant 1 : i32\n+  // CHECK-NEXT: contiguity = [1], divisibility = [4], constancy = [1], constant_value = <none>\n+  %1 = tt.addptr %arg0, %cst1 : !tt.ptr<i32>, i32\n+  // CHECK-NEXT: contiguity = [1], divisibility = [4], constancy = [1], constant_value = 4\n+  %cst4 = arith.constant 4 : i32\n+  // CHECK-NEXT: contiguity = [1], divisibility = [4], constancy = [1], constant_value = <none>\n+  %2 = tt.addptr %arg0, %cst4 : !tt.ptr<i32>, i32\n+  // CHECK-NEXT: contiguity = [1], divisibility = [16], constancy = [1], constant_value = 16\n+  %cst16 = arith.constant 16 : i32\n+  // CHECK-NEXT: contiguity = [1], divisibility = [4], constancy = [1], constant_value = <none>\n+  %3 = tt.addptr %arg0, %cst4 : !tt.ptr<i32>, i32\n+  tt.return\n+}\n+\n+// CHECK-LABEL: @kernel_div16\n+tt.func @kernel_div16(%arg0: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  tt.call @addptr_hints(%arg0) : (!tt.ptr<i32>) -> ()\n+  tt.return\n+}\n+\n+// CHECK-LABEL: @kernel_div8\n+tt.func @kernel_div8(%arg0: !tt.ptr<i32> {tt.divisibility = 8 : i32}) {\n+  tt.call @addptr_hints(%arg0) : (!tt.ptr<i32>) -> ()\n+  tt.return\n+}\n+\n+// CHECK-LABEL: @kernel_div4\n+tt.func @kernel_div4(%arg0: !tt.ptr<i32> {tt.divisibility = 4 : i32}) {\n+  tt.call @addptr_hints(%arg0) : (!tt.ptr<i32>) -> ()\n+  tt.return\n+}\n+\n+}\n+\n+// -----\n+\n+module {\n+\n+// We don't use function cloning here, so the alignment info is the gcd of all call sites.\n+// CHECK-LABEL: @mul\n+tt.func @mul(%arg0: i32) {\n+  // CHECK: contiguity = [1], divisibility = [1], constancy = [1], constant_value = 1\n+  %cst1 = arith.constant 1 : i32\n+  // CHECK-NEXT: contiguity = [1], divisibility = [4], constancy = [1], constant_value = <none>\n+  %1 = arith.muli %arg0, %cst1 : i32\n+  tt.return\n+}\n+\n+// CHECK-LABEL: @bar\n+tt.func @bar(%arg0: i32) {\n+  tt.call @mul(%arg0) : (i32) -> ()\n+  tt.return\n+}\n+\n+// CHECK-LABEL: @foo\n+tt.func @foo(%arg0: i32) {\n+  tt.call @mul(%arg0) : (i32) -> ()\n+  tt.return\n+}\n+\n+// CHECK-LABEL: @call_graph\n+tt.func @call_graph(%arg0: i32) {\n+  // CHECK: contiguity = [1], divisibility = [4], constancy = [1], constant_value = 12\n+  %cst12 = arith.constant 12 : i32\n+  // CHECK: contiguity = [1], divisibility = [4], constancy = [1], constant_value = <none>\n+  %0 = arith.muli %arg0, %cst12 : i32\n+  tt.call @foo(%0) : (i32) -> ()\n+  // CHECK: contiguity = [1], divisibility = [8], constancy = [1], constant_value = 8\n+  %cst8 = arith.constant 8 : i32\n+  // CHECK: contiguity = [1], divisibility = [8], constancy = [1], constant_value = <none>\n+  %1 = arith.muli %arg0, %cst8 : i32\n+  tt.call @bar(%1) : (i32) -> ()\n+  tt.return\n+}\n+\n+}"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 130, "deletions": 6, "changes": 136, "file_content_changes": "@@ -28,10 +28,10 @@ tt.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>,\n \n   scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n     %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-    // CHECK: offset = 0, size = 4608\n+    // CHECK: scratch offset = 0, size = 4608\n     %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_DOT>\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n-    // CHECK-NEXT: offset = 0, size = 4224\n+    // CHECK-NEXT: scratch offset = 0, size = 4224\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B_DOT>\n \n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n@@ -56,17 +56,17 @@ tt.func @reusable(%A : !tt.ptr<f16>) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #AL>\n   %a1_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-  // CHECK-NEXT: offset = 0, size = 4608\n+  // CHECK-NEXT: scratch offset = 0, size = 4608\n   %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_DOT>\n   %a2_ = tt.load %b_ptr, %cst3, %cst4 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #AL>\n-  // CHECK-NEXT: offset = 0, size = 1152\n+  // CHECK-NEXT: scratch offset = 0, size = 1152\n   %a2 = triton_gpu.convert_layout %a2_ : (tensor<32x128xf16, #AL>) -> tensor<32x128xf16, #B_DOT>\n   %a3_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-  // CHECK-NEXT: offset = 0, size = 4608\n+  // CHECK-NEXT: scratch offset = 0, size = 4608\n   %a3 = triton_gpu.convert_layout %a3_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_DOT>\n   %c = tt.dot %a1, %a2, %c_init {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n   %a4_ = tt.load %b_ptr, %cst3, %cst4 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #AL>\n-  // CHECK-NEXT: offset = 0, size = 1152\n+  // CHECK-NEXT: scratch offset = 0, size = 1152\n   %a4 = triton_gpu.convert_layout %a4_ : (tensor<32x128xf16, #AL>) -> tensor<32x128xf16, #B_DOT>\n   %c1 = tt.dot %a3, %a4, %c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n   tt.return\n@@ -396,3 +396,127 @@ tt.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>,\n }\n \n }\n+\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n+// CHECK-LABEL: alloc1\n+tt.func @alloc1(%A : !tt.ptr<f16>) {\n+  // CHECK: offset = 0, size = 512\n+  %cst0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n+  tt.return\n+  // CHECK-NEXT: size = 512\n+}\n+\n+// CHECK-LABEL: alloc2\n+tt.func @alloc2(%A : !tt.ptr<f16>) {\n+  // CHECK: offset = 0, size = 1024\n+  %cst0 = triton_gpu.alloc_tensor : tensor<32x16xf16, #A_SHARED>\n+  tt.return\n+  // CHECK-NEXT: size = 1024\n+}\n+\n+// CHECK-LABEL: alloc3\n+tt.func @alloc3(%cond : i1) {\n+  scf.if %cond {\n+    // CHECK: offset = 0, size = 512\n+    %cst0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n+  } else {\n+    // CHECK-NEXT: offset = 0, size = 1024\n+    %cst0 = triton_gpu.alloc_tensor : tensor<16x32xf16, #A_SHARED>\n+  }\n+  tt.return\n+  // CHECK-NEXT: size = 1024\n+}\n+\n+// CHECK-LABEL: alloc4\n+tt.func @alloc4(%A : !tt.ptr<f16>, %cond : i1) {\n+  scf.if %cond {\n+    // CHECK: virtual offset = 0, size = 1024\n+    tt.call @alloc3(%cond) : (i1) -> ()\n+  } else {\n+    // CHECK-NEXT: virtual offset = 0, size = 512\n+    tt.call @alloc1(%A) : (!tt.ptr<f16>) -> ()\n+  }\n+  tt.return\n+  // CHECK-NEXT: size = 1024\n+}\n+\n+// CHECK-LABEL: single_call\n+tt.func @single_call(%A : !tt.ptr<f16>) {\n+  // CHECK: offset = 0, size = 512\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n+  // CHECK-NEXT: virtual offset = 0, size = 512\n+  tt.call @alloc1(%A) : (!tt.ptr<f16>) -> ()\n+  tt.return\n+  // CHECK-NEXT: size = 512\n+}\n+\n+// CHECK-LABEL: multiple_calls\n+tt.func @multiple_calls(%A : !tt.ptr<f16>) {\n+  // CHECK: offset = 0, size = 512\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  // CHECK-NEXT: virtual offset = 0, size = 512\n+  tt.call @alloc1(%A) : (!tt.ptr<f16>) -> ()\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n+  // CHECK-NEXT: virtual offset = 0, size = 1024\n+  tt.call @alloc2(%A) : (!tt.ptr<f16>) -> ()\n+  tt.return\n+  // CHECK-NEXT: size = 1024\n+}\n+\n+// CHECK-LABEL: if_else_calls\n+tt.func @if_else_calls(%A : !tt.ptr<f16>, %cond : i1) {\n+  scf.if %cond {\n+    // CHECK: offset = 0, size = 512\n+    %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+    // CHECK-NEXT: offset = 0, size = 1024\n+    %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n+    // CHECK-NEXT: virtual offset = 0, size = 512\n+    tt.call @alloc1(%A) : (!tt.ptr<f16>) -> ()\n+  } else {\n+    %cst0 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n+    // CHECK-NEXT: virtual offset = 0, size = 1024\n+    tt.call @alloc2(%A) : (!tt.ptr<f16>) -> ()\n+  }\n+  tt.return\n+  // CHECK-NEXT: size = 1024\n+}\n+\n+// CHECK-LABEL: for_calls\n+tt.func @for_calls(%A : !tt.ptr<f16>, %cond : i1) {\n+  // CHECK: offset = 0, size = 512\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n+  %lb = arith.constant 0 : index\n+  %ub = arith.constant 10 : index\n+  %step = arith.constant 1 : index\n+  scf.for %iv = %lb to %ub step %step {\n+    // CHECK-NEXT: virtual offset = 0, size = 512\n+    tt.call @alloc1(%A) : (!tt.ptr<f16>) -> ()\n+  }\n+  tt.return\n+  // CHECK-NEXT: size = 512\n+}\n+\n+// CHECK-LABEL: call_graph_1\n+tt.func @call_graph_1(%A : !tt.ptr<f16>, %cond : i1) {\n+  // CHECK: offset = 0, size = 512\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  // CHECK-NEXT: virtual offset = 0, size = 1024\n+  tt.call @alloc3(%cond) : (i1) -> ()\n+  tt.return\n+  // CHECK-NEXT: size = 1024\n+}\n+\n+// CHECK-LABEL: call_graph_2\n+tt.func @call_graph_2(%A : !tt.ptr<f16>, %cond : i1) {\n+  // CHECK: offset = 0, size = 512\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  // CHECK-NEXT: virtual offset = 0, size = 1024\n+  tt.call @alloc4(%A, %cond) : (!tt.ptr<f16>, i1) -> ()\n+  tt.return\n+  // CHECK-NEXT: size = 1024\n+}\n+\n+}"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 133, "deletions": 0, "changes": 133, "file_content_changes": "@@ -503,3 +503,136 @@ tt.func @cf_if_else_return(%i1 : i1) {\n }\n \n }\n+\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n+// CHECK-LABEL: convert_layout1\n+tt.func @convert_layout1(%A : !tt.ptr<f16>) {\n+  // CHECK-NOT: gpu.barrier\n+  %0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n+  %1 = triton_gpu.convert_layout %0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n+  tt.return\n+}\n+\n+// CHECK-LABEL: convert_layout2\n+tt.func @convert_layout2(%A : !tt.ptr<f16>) {\n+  %0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n+  %1 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n+  %2 = tt.cat %1, %1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  // CHECK: triton_gpu.convert_layout\n+  // CHECK-NEXT: gpu.barrier\n+  %3 = triton_gpu.convert_layout %0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n+  %4 = tt.cat %2, %2 {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #AL>\n+  tt.return\n+}\n+\n+// CHECK-LABEL: convert_layout3\n+tt.func @convert_layout3(%cond : i1) {\n+  scf.if %cond {\n+    %0 = triton_gpu.alloc_tensor : tensor<16x64xf16, #A_SHARED>\n+    // CHECK: triton_gpu.convert_layout\n+    // CHECK-NOT: gpu.barrier\n+    %1 = triton_gpu.convert_layout %0 : (tensor<16x64xf16, #A_SHARED>) -> tensor<16x64xf16, #AL>\n+  } else {\n+    %0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n+    // CHECK: triton_gpu.convert_layout\n+    // CHECK-NEXT: gpu.barrier\n+    // CHECK-NEXT: triton_gpu.convert_layout\n+    %1 = triton_gpu.convert_layout %0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n+    %2 = triton_gpu.convert_layout %1 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A_SHARED>\n+  }\n+  tt.return\n+}\n+\n+// CHEKC-LABEL: convert_layout4\n+tt.func @convert_layout4(%A : !tt.ptr<f16>, %cond : i1) {\n+  // CHECK-NOT: gpu.barrier\n+  scf.if %cond {\n+    tt.call @convert_layout3(%cond) : (i1) -> ()\n+  } else {\n+    tt.call @convert_layout2(%A) : (!tt.ptr<f16>) -> ()\n+  }\n+  tt.return\n+}\n+\n+// CHECK-LABEL: single_call_sync\n+tt.func @single_call_sync(%A : !tt.ptr<f16>) {\n+  %0 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n+  // CHECK: tt.call\n+  // CHECK-NEXT: gpu.barrier\n+  tt.call @convert_layout1(%A) : (!tt.ptr<f16>) -> ()\n+  %1 = triton_gpu.convert_layout %0 : (tensor<16x32xf16, #AL>) -> tensor<16x32xf16, #BL>\n+  tt.return\n+}\n+\n+// CHECK-LABEL: single_call_no_sync\n+// %1 can reuse %0 in convert_layout2, which has been synced\n+tt.func @single_call_no_sync(%A : !tt.ptr<f16>) {\n+  // CHECK-NOT: gpu.barrier\n+  %0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n+  tt.call @convert_layout2(%A) : (!tt.ptr<f16>) -> ()\n+  %1 = triton_gpu.convert_layout %0 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #BL>\n+  tt.return\n+}\n+\n+// CHECK-LABEL: multiple_calls\n+tt.func @multiple_calls(%A : !tt.ptr<f16>) {\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  tt.call @convert_layout1(%A) : (!tt.ptr<f16>) -> ()\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n+  tt.call @convert_layout2(%A) : (!tt.ptr<f16>) -> ()\n+  tt.return\n+}\n+\n+// CHECK-LABEL: if_else_calls\n+tt.func @if_else_calls(%A : !tt.ptr<f16>, %cond : i1) {\n+  scf.if %cond {\n+    %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.call\n+    // CHECK-NEXT: gpu.barrier\n+    tt.call @convert_layout1(%A) : (!tt.ptr<f16>) -> ()\n+    %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n+  } else {\n+    %cst0 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n+    // CHECK: tt.call\n+    // CHECK-NOT: gpu.barrier\n+    tt.call @convert_layout2(%A) : (!tt.ptr<f16>) -> ()\n+  }\n+  tt.return\n+}\n+\n+// CHECK-LABEL: for_calls\n+tt.func @for_calls(%A : !tt.ptr<f16>, %cond : i1) {\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n+  %lb = arith.constant 0 : index\n+  %ub = arith.constant 10 : index\n+  %step = arith.constant 1 : index\n+  scf.for %iv = %lb to %ub step %step {\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.call\n+    tt.call @convert_layout1(%A) : (!tt.ptr<f16>) -> ()\n+  }\n+  tt.return\n+}\n+\n+// CHECK-LABEL: call_graph_1\n+tt.func @call_graph_1(%A : !tt.ptr<f16>, %cond : i1) {\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.call\n+  tt.call @convert_layout3(%cond) : (i1) -> ()\n+  tt.return\n+}\n+\n+// CHECK-LABEL: call_graph_2\n+tt.func @call_graph_2(%A : !tt.ptr<f16>, %cond : i1) {\n+  tt.call @convert_layout4(%A, %cond) : (!tt.ptr<f16>, i1) -> ()\n+  // CHECK: tt.call\n+  // CHECK-NEXT: gpu.barrier\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  tt.return\n+}\n+\n+}"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -869,7 +869,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_blocked1d_to_slice1\n   tt.func @convert_blocked1d_to_slice1(%src:tensor<32xi32, #blocked0>) {\n-    // CHECK-COUNT-32: llvm.load {{.*}} : !llvm.ptr<vector<1xi32>, 3>\n+    // CHECK-COUNT-8: llvm.load {{.*}} : !llvm.ptr<vector<1xi32>, 3>\n     %cvt = triton_gpu.convert_layout %src : (tensor<32xi32, #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n     tt.return\n   }"}, {"filename": "test/Target/tritongpu_to_llvmir_noinline.mlir", "status": "added", "additions": 22, "deletions": 0, "changes": 22, "file_content_changes": "@@ -0,0 +1,22 @@\n+// RUN: %PYTHON -m triton.tools.aot %s --target=llvm-ir --sm=80 | FileCheck %s\n+\n+// == LLVM IR check begin ==\n+// CHECK-LABEL: ; ModuleID = 'LLVMDialectModule'\n+// CHECK: define void @test_func\n+// CHECK: define void @test_kernel\n+// CHECK: tail call void @test_func\n+\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n+tt.func @test_func(%lb : index, %A : !tt.ptr<f16>) attributes { noinline = true } {\n+  %0 = arith.constant 1.0 : f16\n+  tt.store %A, %0 : f16\n+  tt.return\n+}\n+\n+tt.func @test_kernel(%lb : index, %A : !tt.ptr<f16>) {\n+  tt.call @test_func(%lb, %A) : (index, !tt.ptr<f16>) -> ()\n+  tt.return\n+}\n+\n+}"}, {"filename": "test/lib/Analysis/TestAllocation.cpp", "status": "modified", "additions": 28, "deletions": 22, "changes": 50, "file_content_changes": "@@ -6,7 +6,7 @@ using namespace mlir;\n namespace {\n \n struct TestAllocationPass\n-    : public PassWrapper<TestAllocationPass, OperationPass<triton::FuncOp>> {\n+    : public PassWrapper<TestAllocationPass, OperationPass<ModuleOp>> {\n \n   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAllocationPass);\n \n@@ -16,31 +16,37 @@ struct TestAllocationPass\n   }\n \n   void runOnOperation() override {\n-    Operation *operation = getOperation();\n     auto &os = llvm::errs();\n+    ModuleOp moduleOp = getOperation();\n     // Convert to std::string can remove quotes from opName\n-    auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n-    os << opName << \"\\n\";\n-    Allocation allocation(operation);\n-    operation->walk([&](Operation *op) {\n-      auto scratchBufferId = allocation.getBufferId(op);\n-      if (scratchBufferId != Allocation::InvalidBufferId) {\n-        size_t offset = allocation.getOffset(scratchBufferId);\n-        size_t size = allocation.getAllocatedSize(scratchBufferId);\n-        os << \"scratch offset = \" << offset << \", size = \" << size << \"\\n\";\n-      }\n-      if (op->getNumResults() < 1)\n-        return;\n-      for (Value result : op->getResults()) {\n-        auto bufferId = allocation.getBufferId(result);\n-        if (bufferId != Allocation::InvalidBufferId) {\n-          size_t offset = allocation.getOffset(bufferId);\n-          size_t size = allocation.getAllocatedSize(bufferId);\n-          os << \"offset = \" << offset << \", size = \" << size << \"\\n\";\n+    ModuleAllocation moduleAllocation(moduleOp);\n+    moduleOp.walk([&](triton::FuncOp funcOp) {\n+      auto opName = SymbolTable::getSymbolName(funcOp).getValue().str();\n+      os << opName << \"\\n\";\n+      auto *allocation = moduleAllocation.getFuncData(funcOp);\n+      funcOp.walk([&](Operation *op) {\n+        auto scratchBufferId = allocation->getBufferId(op);\n+        if (scratchBufferId != Allocation::InvalidBufferId) {\n+          size_t offset = allocation->getOffset(scratchBufferId);\n+          size_t size = allocation->getAllocatedSize(scratchBufferId);\n+          if (allocation->isVirtualBuffer(scratchBufferId))\n+            os << \"virtual offset = \" << offset << \", size = \" << size << \"\\n\";\n+          else\n+            os << \"scratch offset = \" << offset << \", size = \" << size << \"\\n\";\n         }\n-      }\n+        if (op->getNumResults() < 1)\n+          return;\n+        for (Value result : op->getResults()) {\n+          auto bufferId = allocation->getBufferId(result);\n+          if (bufferId != Allocation::InvalidBufferId) {\n+            size_t offset = allocation->getOffset(bufferId);\n+            size_t size = allocation->getAllocatedSize(bufferId);\n+            os << \"offset = \" << offset << \", size = \" << size << \"\\n\";\n+          }\n+        }\n+      });\n+      os << \"size = \" << allocation->getSharedMemorySize() << \"\\n\";\n     });\n-    os << \"size = \" << allocation.getSharedMemorySize() << \"\\n\";\n   }\n };\n "}, {"filename": "test/lib/Analysis/TestAxisInfo.cpp", "status": "modified", "additions": 19, "deletions": 18, "changes": 37, "file_content_changes": "@@ -7,7 +7,7 @@ using namespace mlir;\n namespace {\n \n struct TestAxisInfoPass\n-    : public PassWrapper<TestAxisInfoPass, OperationPass<triton::FuncOp>> {\n+    : public PassWrapper<TestAxisInfoPass, OperationPass<ModuleOp>> {\n \n   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAxisInfoPass);\n \n@@ -18,23 +18,24 @@ struct TestAxisInfoPass\n \n   void runOnOperation() override {\n     Operation *operation = getOperation();\n-    auto &os = llvm::errs();\n-    auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n-    os << \"@\" << opName << \"\\n\";\n-\n-    std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n-    AxisInfoAnalysis *analysis = solver->load<AxisInfoAnalysis>();\n-    if (failed(solver->initializeAndRun(operation)))\n-      return signalPassFailure();\n-    operation->walk([&](Operation *op) {\n-      if (op->getNumResults() < 1)\n-        return;\n-      for (Value result : op->getResults()) {\n-        result.print(os);\n-        os << \" => \";\n-        analysis->getLatticeElement(result)->getValue().print(os);\n-        os << \"\\n\";\n-      }\n+    ModuleOp moduleOp = cast<ModuleOp>(operation);\n+    ModuleAxisInfoAnalysis moduleAxisInfoAnalysis(moduleOp);\n+    moduleOp.walk([&](triton::FuncOp funcOp) {\n+      auto &os = llvm::errs();\n+      auto opName = SymbolTable::getSymbolName(funcOp).getValue().str();\n+      os << \"@\" << opName << \"\\n\";\n+      funcOp.walk([&](Operation *op) {\n+        if (op->getNumResults() < 1)\n+          return;\n+        for (Value result : op->getResults()) {\n+          result.print(os);\n+          os << \" => \";\n+          auto *axisInfo = moduleAxisInfoAnalysis.getAxisInfo(result);\n+          if (axisInfo)\n+            axisInfo->print(os);\n+          os << \"\\n\";\n+        }\n+      });\n     });\n   }\n };"}, {"filename": "test/lib/Analysis/TestMembar.cpp", "status": "modified", "additions": 4, "deletions": 10, "changes": 14, "file_content_changes": "@@ -11,7 +11,7 @@ using namespace mlir;\n namespace {\n \n struct TestMembarPass\n-    : public PassWrapper<TestMembarPass, OperationPass<triton::FuncOp>> {\n+    : public PassWrapper<TestMembarPass, OperationPass<ModuleOp>> {\n \n   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestMembarPass);\n \n@@ -22,17 +22,11 @@ struct TestMembarPass\n \n   void runOnOperation() override {\n     Operation *operation = getOperation();\n-    auto &os = llvm::errs();\n-    // Convert to std::string can remove quotes from op_name\n-    auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n-    os << opName << \"\\n\";\n-\n+    ModuleOp moduleOp = cast<ModuleOp>(operation);\n     // Print all ops after membar pass\n-    Allocation allocation(operation);\n-    MembarAnalysis membarPass(&allocation);\n+    ModuleAllocation allocation(moduleOp);\n+    ModuleMembarAnalysis membarPass(&allocation);\n     membarPass.run();\n-\n-    os << *operation << \"\\n\";\n   }\n };\n "}]