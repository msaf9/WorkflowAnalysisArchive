[{"filename": ".github/workflows/torch-inductor-tests.yml", "status": "added", "additions": 39, "deletions": 0, "changes": 39, "file_content_changes": "@@ -0,0 +1,39 @@\n+name: Torchinductor\n+\n+on:\n+  workflow_dispatch:\n+\n+jobs:\n+  Runner-Preparation:\n+    runs-on: ubuntu-latest\n+    outputs:\n+      matrix: ${{ steps.set-matrix.outputs.matrix }}\n+    steps:\n+      - name: Prepare runner matrix\n+        id: set-matrix\n+        run: |\n+          echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"]]'\n+\n+  Integration-Tests:\n+    needs: Runner-Preparation\n+    runs-on: ${{ matrix.runner }}\n+    strategy:\n+      matrix:\n+        runner: ${{fromJson(needs.Runner-Preparation.outputs.matrix)}}\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v2\n+      #- name: Packages\n+      #  run: |\n+      #    ./.github/workflows/torchinductor/scripts/install_torchinductor.sh\n+      - name: Environment\n+        run: |\n+          source /opt/torchinductor_venv/bin/activate\n+          ./.github/workflows/torchinductor/scripts/install_triton.sh\n+      - name: Performance\n+        run: |\n+          ./.github/workflows/torchinductor/scripts/run_torchinductor_perf.sh\n+      # Runs too long time\n+      #- name: Accuracy\n+      #  run: |\n+      #    ./.github/workflows/torchinductor/scripts/run_torchinductor_acc.sh"}, {"filename": ".github/workflows/torchinductor/data/huggingface.csv", "status": "added", "additions": 37, "deletions": 0, "changes": 37, "file_content_changes": "@@ -0,0 +1,37 @@\n+dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio\n+cuda,AlbertForMaskedLM,4,1.5511,164.3373,26.8523,1.2647\n+cuda,AlbertForQuestionAnswering,4,1.5501,163.5580,25.7983,1.3145\n+cuda,BartForCausalLM,4,1.5080,71.7230,32.8907,0.9749\n+cuda,BertForMaskedLM,16,1.5350,67.9451,35.3286,1.0494\n+cuda,BertForQuestionAnswering,16,1.6735,53.2963,34.3754,1.1710\n+cuda,BlenderbotSmallForCausalLM,64,1.2106,46.6466,23.8058,0.9120\n+cuda,BlenderbotSmallForConditionalGeneration,64,1.3616,77.3013,55.3546,0.9803\n+cuda,CamemBert,16,1.4779,76.1809,35.3883,1.0469\n+cuda,DebertaForMaskedLM,4,0.8415,62.3395,35.9657,1.0418\n+cuda,DebertaForQuestionAnswering,8,1.0609,67.5151,35.7728,1.1528\n+cuda,DebertaV2ForMaskedLM,1,0.6026,134.6517,66.1783,0.9773\n+cuda,DistilBertForMaskedLM,128,1.2460,66.9382,18.3089,0.9624\n+cuda,DistilBertForQuestionAnswering,256,1.3997,72.4126,18.1956,1.1486\n+cuda,DistillGPT2,16,1.6656,60.5455,17.2280,1.0641\n+cuda,ElectraForCausalLM,32,1.8299,45.4841,37.0944,0.9717\n+cuda,ElectraForQuestionAnswering,64,2.0289,52.6890,35.9632,1.1928\n+cuda,GPT2ForSequenceClassification,4,2.2567,38.2969,30.0527,1.2323\n+cuda,LayoutLMForMaskedLM,16,1.5423,68.8018,36.5562,1.0495\n+cuda,LayoutLMForSequenceClassification,16,1.7058,53.9355,35.2225,1.1659\n+cuda,MBartForCausalLM,4,1.4945,71.4649,32.8653,0.9830\n+cuda,MegatronBertForCausalLM,4,1.4328,58.4404,70.6226,1.0951\n+cuda,MegatronBertForQuestionAnswering,8,1.5886,85.2533,69.1219,1.1152\n+cuda,MobileBertForMaskedLM,64,0.9007,131.7379,107.5275,1.0136\n+cuda,MobileBertForQuestionAnswering,128,0.8435,167.9066,106.7049,0.8579\n+cuda,PLBartForCausalLM,8,1.5261,68.9224,19.5826,0.9887\n+cuda,PLBartForConditionalGeneration,4,1.5298,71.2811,45.6902,1.0495\n+cuda,PegasusForCausalLM,32,1.2212,57.5436,33.3863,0.9736\n+cuda,PegasusForConditionalGeneration,32,1.2822,106.4678,69.8825,1.0689\n+cuda,RobertaForCausalLM,16,1.6128,67.5706,34.7355,1.0496\n+cuda,RobertaForQuestionAnswering,16,1.6800,53.6267,33.8527,1.1704\n+cuda,Speech2Text2ForCausalLM,256,1.8230,32.9145,18.7201,0.8760\n+cuda,T5ForConditionalGeneration,4,1.6592,59.5324,39.4406,1.1814\n+cuda,T5Small,4,1.6581,59.5930,37.0471,1.1814\n+cuda,TrOCRForCausalLM,32,1.2586,106.2633,32.5330,0.9583\n+cuda,XLNetLMHeadModel,8,1.8108,142.8795,84.8197,1.1240\n+cuda,YituTechConvBert,16,1.5207,81.4595,53.1565,1.0362"}, {"filename": ".github/workflows/torchinductor/data/timm_models.csv", "status": "added", "additions": 54, "deletions": 0, "changes": 54, "file_content_changes": "@@ -0,0 +1,54 @@\n+dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio\n+cuda,adv_inception_v3,128,1.5923,102.5292,51.6032,1.0472\n+cuda,beit_base_patch16_224,64,1.3390,75.3027,29.7471,1.0156\n+cuda,coat_lite_mini,128,2.0579,53.3689,37.1856,1.0437\n+cuda,convmixer_768_32,32,1.0470,275.5328,23.8037,0.9999\n+cuda,convnext_base,64,1.5084,80.1811,42.5659,1.0373\n+cuda,crossvit_9_240,128,1.5392,37.1806,44.9986,0.9193\n+cuda,cspdarknet53,64,1.4721,75.0403,35.2882,1.0547\n+cuda,deit_base_distilled_patch16_224,64,1.1432,55.9737,23.4038,0.9816\n+cuda,dla102,128,1.5282,123.7284,49.3612,1.0430\n+cuda,dm_nfnet_f0,128,1.4354,79.7518,34.8994,1.1038\n+cuda,dpn107,32,1.2412,83.8921,58.9111,0.9952\n+cuda,eca_botnext26ts_256,128,1.5425,71.2406,28.8920,1.0270\n+cuda,ese_vovnet19b_dw,128,1.4647,42.4837,18.0285,1.0135\n+cuda,fbnetc_100,128,1.5795,53.8033,33.0222,1.0082\n+cuda,gernet_l,128,1.1684,63.4230,26.8687,1.0053\n+cuda,ghostnet_100,128,1.7812,54.4211,47.6168,1.0484\n+cuda,gluon_inception_v3,128,1.5952,102.5018,50.0857,1.0469\n+cuda,gmixer_24_224,128,1.6749,69.2430,42.0841,1.1921\n+cuda,gmlp_s16_224,128,1.5886,79.2132,43.0142,1.2343\n+cuda,hrnet_w18,128,1.3743,221.5304,134.2573,1.0100\n+cuda,inception_v3,128,1.5847,102.8333,49.7648,1.0472\n+cuda,jx_nest_base,32,1.3747,71.4190,61.4053,0.9905\n+cuda,lcnet_050,128,1.8159,18.0047,18.8249,1.0005\n+cuda,mixer_b16_224,128,1.2795,90.9229,21.0438,1.0133\n+cuda,mixnet_l,128,1.2273,149.9722,47.7482,1.0129\n+cuda,mnasnet_100,128,1.6594,40.0512,26.5165,1.0047\n+cuda,mobilenetv2_100,128,1.6085,41.1217,27.4450,1.1731\n+cuda,mobilenetv3_large_100,128,1.6610,37.9995,29.8185,1.0052\n+cuda,mobilevit_s,64,1.5212,55.4152,53.6475,1.0258\n+cuda,nfnet_l0,128,1.4927,65.7078,32.4067,0.9980\n+cuda,pit_b_224,64,1.2286,57.9484,26.5321,0.9606\n+cuda,pnasnet5large,16,1.0000,198.2494,93.4641,1.3184\n+cuda,poolformer_m36,64,1.3486,103.9235,62.3196,1.1942\n+cuda,regnety_002,128,1.3030,32.4968,27.2439,1.0014\n+cuda,repvgg_a2,128,1.2485,59.7729,26.9209,1.0185\n+cuda,res2net101_26w_4s,64,1.0813,94.1773,86.6520,0.9655\n+cuda,res2net50_14w_8s,128,1.3251,109.5258,79.9578,0.9830\n+cuda,res2next50,128,1.2518,125.5008,43.9754,0.9756\n+cuda,resmlp_12_224,128,1.3060,45.2373,19.3709,1.1048\n+cuda,resnest101e,64,1.4346,108.1945,78.1993,1.1037\n+cuda,rexnet_100,128,1.4637,55.0121,41.2075,1.0862\n+cuda,selecsls42b,128,1.4284,44.6645,23.3892,1.0139\n+cuda,spnasnet_100,128,1.5908,45.3189,32.0148,1.0048\n+cuda,swin_base_patch4_window7_224,64,1.6164,89.5854,75.5848,0.9299\n+cuda,swsl_resnext101_32x16d,32,1.0175,110.0041,45.7853,1.0003\n+cuda,tf_efficientnet_b0,128,1.5271,55.7361,34.5551,1.1079\n+cuda,tf_mixnet_l,128,1.2369,155.9027,48.6695,1.0921\n+cuda,tinynet_a,128,1.3792,53.0640,40.6346,1.1108\n+cuda,tnt_s_patch16_224,128,3.1078,104.8486,59.6028,1.0660\n+cuda,twins_pcpvt_base,64,1.5921,67.4600,84.4977,1.0909\n+cuda,visformer_small,128,1.1952,72.8705,23.7303,1.0410\n+cuda,vit_base_patch16_224,64,1.1309,56.4866,22.0208,0.9804\n+cuda,volo_d1_224,64,1.6868,72.0957,65.3011,0.9729"}, {"filename": ".github/workflows/torchinductor/data/torchbench.csv", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -0,0 +1,53 @@\n+dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio\n+cuda,BERT_pytorch,16,1.7111,24.2741,35.7065,1.3212\n+cuda,LearningToPaint,96,1.0513,10.7557,11.1879,0.9896\n+cuda,Super_SloMo,6,1.3267,60.4328,28.2097,1.2392\n+cuda,alexnet,128,1.1754,8.3246,5.3319,1.0003\n+cuda,attention_is_all_you_need_pytorch,256,1.3416,36.4401,39.5927,1.1774\n+cuda,dcgan,32,0.9151,2.6249,3.2964,1.0082\n+cuda,densenet121,4,0.9225,51.3747,68.5841,0.9930\n+cuda,doctr_det_predictor,0,0.0000\n+cuda,doctr_reco_predictor,0,0.0000\n+cuda,drq,1,0.9500,3.4884,4.8028,0.9687\n+cuda,fastNLP_Bert,6,1.4328,34.7753,35.4863,1.2368\n+cuda,functorch_dp_cifar10,64,1.2015,8.1625,12.9040,1.0609\n+cuda,functorch_maml_omniglot,1,0.9322,2.5844,3.8640,1.0000\n+cuda,hf_Albert,8,2.1228,30.3377,26.8282,1.2676\n+cuda,hf_Bart,4,1.2899,39.1935,47.2373,1.0080\n+cuda,hf_Bert,4,1.3262,26.1063,35.0281,1.0656\n+cuda,hf_Bert_large,4,1.4163,55.1021,67.2825,1.0915\n+cuda,hf_DistilBert,8,1.4051,21.7191,18.0399,1.0242\n+cuda,hf_GPT2,4,1.6661,26.9039,29.9473,1.1555\n+cuda,hf_Longformer,0,0.0000\n+cuda,hf_Reformer,4,1.1709,64.6979,15.7035,0.9267\n+cuda,hf_T5_large,2,1.7215,107.0798,148.8805,1.1684\n+cuda,lennard_jones,1000,0.8428,1.8488,3.0609,1.0001\n+cuda,maml_omniglot,32,0.9648,2.6869,3.9775,0.9999\n+cuda,mnasnet1_0,32,1.0469,21.6251,25.8232,0.9996\n+cuda,mobilenet_v2,96,1.5604,31.9572,27.0225,1.1734\n+cuda,nvidia_deeprecommender,256,1.0605,9.2080,4.1318,0.9711\n+cuda,phlippe_densenet,128,1.0237,27.5988,28.0400,1.0023\n+cuda,phlippe_resnet,128,1.0493,10.9751,10.2485,1.0092\n+cuda,pytorch_CycleGAN_and_pix2pix,1,1.3724,8.2225,11.9561,1.0219\n+cuda,pytorch_stargan,16,1.1835,11.9178,10.0507,1.0868\n+cuda,pytorch_unet,1,1.3787,29.7543,13.7711,1.0100\n+cuda,resnet152,32,0.9834,63.2446,67.7935,0.9991\n+cuda,resnet18,16,0.9451,9.4977,11.7663,0.9948\n+cuda,resnet50,32,1.0513,24.5141,24.6629,1.0021\n+cuda,resnext50_32x4d,8,0.9216,22.2460,24.3420,0.9984\n+cuda,shufflenet_v2_x1_0,128,1.1943,25.4520,28.8611,1.0951\n+cuda,soft_actor_critic,256,0.8691,1.9637,3.3716,0.9996\n+cuda,speech_transformer,32,1.2718,35.2922,46.9957,1.0897\n+cuda,squeezenet1_1,32,1.1302,8.4540,7.9625,1.0771\n+cuda,timm_efficientdet,1,1.3370,80.0377,120.1814,1.2713\n+cuda,timm_efficientnet,32,1.1874,27.6302,33.9059,1.0971\n+cuda,timm_nfnet,128,1.4525,77.3461,34.3270,1.1056\n+cuda,timm_regnet,32,1.0644,50.6953,35.7562,1.0000\n+cuda,timm_resnest,32,1.6200,14.7763,17.2245,1.0906\n+cuda,timm_vision_transformer,32,1.0800,19.4188,22.0255,0.9966\n+cuda,timm_vision_transformer_large,32,1.0081,393.1742,127.8083,0.9735\n+cuda,timm_vovnet,32,1.1472,22.4727,22.7328,1.0120\n+cuda,torchrec_dlrm,0,0.0000\n+cuda,tts_angular,64,0.8974,6.5057,2.5555,0.9973\n+cuda,vgg16,64,1.2909,50.7405,6.1510,0.9828\n+cuda,yolov3,16,1.2930,54.8069,41.9269,1.0563"}, {"filename": ".github/workflows/torchinductor/scripts/check_acc.py", "status": "added", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -0,0 +1,12 @@\n+import sys\n+import csv\n+\n+\n+file_path = sys.argv[1]\n+with open(file_path) as f:\n+    reader = csv.reader(f)\n+    for i, row in enumerate(reader):\n+        if i == 0:\n+            continue\n+        if row[3] != \"pass\":\n+            print(f\"{row[1]} failed on device {row[0]} with batch size {row[2]}\")"}, {"filename": ".github/workflows/torchinductor/scripts/check_perf.py", "status": "added", "additions": 63, "deletions": 0, "changes": 63, "file_content_changes": "@@ -0,0 +1,63 @@\n+import argparse\n+import csv\n+from collections import namedtuple\n+\n+# Create a named tuple for the output of the benchmark\n+BenchmarkOutput = namedtuple(\n+    'BenchmarkOutput', ['dev', 'name', 'batch_size', 'speedup', 'latency'])\n+\n+\n+def parse_output(file_path: str) -> dict:\n+    entries = {}\n+    with open(file_path) as f:\n+        reader = csv.reader(f)\n+        for i, row in enumerate(reader):\n+            if i == 0 or len(row) < 5:\n+                continue\n+            dev = row[0]\n+            name = row[1]\n+            batch_size = row[2]\n+            speedup = float(row[3])\n+            latency = float(row[4])\n+            entries[name] = BenchmarkOutput(dev, name, batch_size, speedup, latency)\n+    return entries\n+\n+\n+def compare(baseline: dict, new: dict, threshold: float, geomean_threshold: float) -> bool:\n+    baseline_geomean = 1.0\n+    new_geomean = 1.0\n+    for key in new:\n+        if key not in baseline:\n+            print(f\"New benchmark {key} not found in baseline\")\n+        baseline_latency = baseline[key].latency\n+        new_latency = new[key].latency\n+        if new_latency < baseline_latency * (1 - threshold):\n+            print(\n+                f\"New benchmark {key} is faster than baseline: {new_latency} vs {baseline_latency}\")\n+        elif new_latency > baseline_latency * (1 + threshold):\n+            print(\n+                f\"New benchmark {key} is slower than baseline: {new_latency} vs {baseline_latency}\")\n+        baseline_geomean *= baseline[key].speedup\n+        new_geomean *= new[key].speedup\n+\n+    baseline_geomean = baseline_geomean ** (1 / len(baseline))\n+    new_geomean = new_geomean ** (1 / len(new))\n+    print(f\"Baseline geomean: {baseline_geomean}\")\n+    print(f\"New geomean: {new_geomean}\")\n+    assert new_geomean > baseline_geomean * (1 - geomean_threshold), \\\n+        f\"New geomean is slower than baseline: {new_geomean} vs {baseline_geomean}\"\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument('--baseline', required=True)\n+    parser.add_argument('--new', required=True)\n+    parser.add_argument('--threshold', type=float, default=0.1)\n+    parser.add_argument('--geomean-threshold', type=float, default=0.02)\n+    args = parser.parse_args()\n+    baseline = parse_output(args.baseline)\n+    new = parse_output(args.new)\n+    compare(baseline, new, args.threshold, args.geomean_threshold)\n+\n+\n+main()"}, {"filename": ".github/workflows/torchinductor/scripts/common.sh", "status": "added", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -0,0 +1,9 @@\n+#!/bin/bash\n+\n+TEST_REPORTS_DIR=/opt/torchinductor_reports\n+PYTORCH_DIR=/opt/pytorch\n+MODELS=(timm_models huggingface torchbench)\n+\n+echo \"$TEST_REPORTS_DIR\"\n+echo \"$PYTORCH_DIR\"\n+echo \"${MODELS[@]}\""}, {"filename": ".github/workflows/torchinductor/scripts/install_torchinductor.sh", "status": "added", "additions": 54, "deletions": 0, "changes": 54, "file_content_changes": "@@ -0,0 +1,54 @@\n+#!/bin/bash\n+\n+# remember where we started\n+ROOT=\"$(pwd)\"\n+\n+# torchinductor venv \n+whoami\n+python3 -m venv /opt/torchinductor_venv\n+# shellcheck source=/dev/null\n+source /opt/torchinductor_venv/bin/activate\n+# shellcheck source=/dev/null\n+source ./.github/workflows/torchinductor/scripts/common.sh\n+\n+# pytorch nightly\n+pip3 install --force-reinstall --pre torch torchtext torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu118\n+# pytorch source to get torchbench for dynamo\n+cd /opt || exit\n+git clone --recursive https://github.com/pytorch/pytorch\n+cd pytorch || exit\n+# if you are updating an existing checkout\n+git submodule sync\n+git submodule update --init --recursive\n+cd ..\n+\n+# required packages\n+pip3 install expecttest psutil\n+\n+# torchbench\n+pip3 install pyyaml\n+git clone https://github.com/pytorch/benchmark.git\n+cd benchmark || exit\n+python3 install.py\n+cd ..\n+\n+# timm\n+git clone https://github.com/huggingface/pytorch-image-models.git\n+cd pytorch-image-models || exit\n+pip3 install -e .\n+cd ..\n+\n+# build our own triton\n+cd \"$ROOT\" || exit\n+cd python || exit\n+rm -rf build\n+pip3 install -e .\n+pip3 uninstall pytorch-triton -y\n+\n+# clean up cache\n+rm -rf /tmp/torchinductor_root/\n+rm -rf ~/.triton/cache\n+rm -rf \"$TEST_REPORTS_DIR\"\n+\n+# go back to where we started\n+cd \"$ROOT\" || exit"}, {"filename": ".github/workflows/torchinductor/scripts/install_triton.sh", "status": "added", "additions": 24, "deletions": 0, "changes": 24, "file_content_changes": "@@ -0,0 +1,24 @@\n+#!/bin/bash\n+\n+# remember where we started\n+ROOT=\"$(pwd)\"\n+\n+# shellcheck source=/dev/null\n+source /opt/torchinductor_venv/bin/activate\n+# shellcheck source=/dev/null\n+source ./.github/workflows/torchinductor/scripts/common.sh\n+\n+# build our own triton\n+cd python || exit\n+pip3 install --pre pytorch-triton --extra-index-url https://download.pytorch.org/whl/nightly/cu118\n+rm -rf build\n+pip3 install -e .\n+pip3 uninstall pytorch-triton -y\n+\n+# clean up cache\n+rm -rf /tmp/torchinductor_root/\n+rm -rf ~/.triton/cache\n+rm -rf \"$TEST_REPORTS_DIR\"\n+\n+# go back to where we started\n+cd \"$ROOT\" || exit"}, {"filename": ".github/workflows/torchinductor/scripts/run_torchinductor_acc.sh", "status": "added", "additions": 35, "deletions": 0, "changes": 35, "file_content_changes": "@@ -0,0 +1,35 @@\n+#!/bin/bash\n+\n+# remember where we started\n+ROOT=\"$(pwd)\"\n+INDUCTOR=\"$ROOT\"/.github/workflows/torchinductor\n+\n+# shellcheck source=/dev/null\n+source /opt/torchinductor_venv/bin/activate\n+# shellcheck source=/dev/null\n+source \"$INDUCTOR\"/scripts/common.sh\n+\n+cd \"$PYTORCH_DIR\" || exit\n+TEST_REPORTS_DIR=$TEST_REPORTS_DIR/acc\n+mkdir -p \"$TEST_REPORTS_DIR\"\n+\n+for model in \"${MODELS[@]}\"; do\n+  echo \"Running accuracy test for $model\"\n+  python3 benchmarks/dynamo/\"$model\".py --ci --accuracy --timing --explain --inductor --device cuda \\\n+    --output \"$TEST_REPORTS_DIR\"/inference_\"$model\".csv\n+  python3 benchmarks/dynamo/\"$model\".py --ci --accuracy --timing --explain --inductor --training --amp --device cuda \\\n+    --output \"$TEST_REPORTS_DIR\"/training_\"$model\".csv\n+  python3 benchmarks/dynamo/\"$model\".py --ci --accuracy --timing --explain --inductor --dynamic-shapes --device cuda \\\n+    --output \"$TEST_REPORTS_DIR\"/dynamic_shapes_\"$model\".csv\n+done\n+\n+cd \"$ROOT\" || exit\n+for model in \"${MODELS[@]}\"; do\n+  echo \"Checking accuracy test for $model\"\n+  python3 \"$INDUCTOR\"/scripts/check_acc.py \"$TEST_REPORTS_DIR\"/inference_\"$model\".csv\n+  python3 \"$INDUCTOR\"/scripts/check_acc.py \"$TEST_REPORTS_DIR\"/training_\"$model\".csv\n+  python3 \"$INDUCTOR\"/scripts/check_acc.py \"$TEST_REPORTS_DIR\"/dynamic_shapes_\"$model\".csv\n+done\n+\n+# go back to where we started\n+cd \"$ROOT\" || exit"}, {"filename": ".github/workflows/torchinductor/scripts/run_torchinductor_perf.sh", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "file_content_changes": "@@ -0,0 +1,41 @@\n+#!/bin/bash\n+\n+# remember where we started\n+ROOT=\"$(pwd)\"\n+INDUCTOR=\"$ROOT\"/.github/workflows/torchinductor\n+\n+# shellcheck source=/dev/null\n+source /opt/torchinductor_venv/bin/activate\n+# shellcheck source=/dev/null\n+source \"$INDUCTOR\"/scripts/common.sh\n+\n+# lock GPU clocks to 1350 MHz\n+sudo nvidia-smi -i 0 -pm 1\n+sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n+\n+cd \"$PYTORCH_DIR\" || exit\n+TEST_REPORTS_DIR=$TEST_REPORTS_DIR/perf\n+mkdir -p \"$TEST_REPORTS_DIR\"\n+\n+for model in \"${MODELS[@]}\"; do\n+  echo \"Running performance test for $model\"\n+  python3 benchmarks/dynamo/\"$model\".py --ci --training --performance --disable-cudagraphs\\\n+    --device cuda --inductor --amp --output \"$TEST_REPORTS_DIR\"/\"$model\".csv\n+done\n+\n+cd \"$ROOT\" || exit\n+for model in \"${MODELS[@]}\"; do\n+  echo \"Checking performance test for $model\"\n+  python3 \"$INDUCTOR\"/scripts/check_perf.py --new \"$TEST_REPORTS_DIR\"/\"$model\".csv --baseline \"$INDUCTOR\"/data/\"$model\".csv\n+  EXIT_STATUS=$?\n+  if [ \"$EXIT_STATUS\" -ne 0 ]; then\n+    echo \"Performance test for $model failed\"\n+    exit \"$EXIT_STATUS\"\n+  fi\n+done\n+\n+# unlock GPU clocks\n+sudo nvidia-smi -i 0 -rgc\n+\n+# go back to where we started\n+cd \"$ROOT\" || exit"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -238,6 +238,10 @@ if(TRITON_BUILD_PYTHON_MODULE)\n   target_link_options(triton PRIVATE ${LLVM_LDFLAGS})\n endif()\n \n+if (UNIX AND NOT APPLE)\n+  set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -Wl,--exclude-libs,ALL\")\n+endif()\n+\n if(TRITON_BUILD_PYTHON_MODULE AND NOT WIN32)\n     set(CMAKE_SHARED_LIBRARY_SUFFIX \".so\")\n     # Check if the platform is MacOS"}, {"filename": "bin/triton-opt.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -32,7 +32,7 @@ int main(int argc, char **argv) {\n \n   // TODO: register Triton & TritonGPU passes\n   mlir::DialectRegistry registry;\n-  registry.insert<mlir::triton::TritonDialect,\n+  registry.insert<mlir::triton::TritonDialect, mlir::cf::ControlFlowDialect,\n                   mlir::triton::gpu::TritonGPUDialect, mlir::func::FuncDialect,\n                   mlir::math::MathDialect, mlir::arith::ArithDialect,\n                   mlir::scf::SCFDialect, mlir::gpu::GPUDialect>();"}, {"filename": "include/triton/Conversion/Passes.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -2,6 +2,7 @@\n #define TRITON_CONVERSION_PASSES_H\n \n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n "}, {"filename": "include/triton/Conversion/Passes.td", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -49,4 +49,18 @@ def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"\n     ];\n }\n \n+def TritonConvertArithToIndex : Pass<\"triton-convert-arith-to-index\", \"mlir::ModuleOp\"> {\n+\n+    let summary = \"Convert arith to index\";\n+    \n+    let constructor = \"mlir::triton::createTritonConvertArithToIndexPass()\";\n+\n+    let description = [{\n+      Convert arith operation on index values to corresponding ops in the index dialect.\n+      We need this because SCFToCF conversion currently generates arith ops on indices.\n+    }];\n+\n+    let dependentDialects = [\"mlir::arith::ArithDialect\"];\n+}\n+\n #endif"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h", "status": "added", "additions": 20, "deletions": 0, "changes": 20, "file_content_changes": "@@ -0,0 +1,20 @@\n+#ifndef TRITON_CONVERSION_ARITH_TO_INDEX_H\n+#define TRITON_CONVERSION_ARITH_TO_INDEX_H\n+\n+#include \"mlir/Conversion/LLVMCommon/TypeConverter.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include <memory>\n+\n+namespace mlir {\n+\n+class ModuleOp;\n+template <typename T> class OperationPass;\n+\n+namespace triton {\n+\n+std::unique_ptr<OperationPass<ModuleOp>> createTritonConvertArithToIndexPass();\n+\n+}\n+} // namespace mlir\n+\n+#endif\n\\ No newline at end of file"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 65, "deletions": 0, "changes": 65, "file_content_changes": "@@ -9,6 +9,8 @@ include \"triton/Dialect/Triton/IR/TritonAttrDefs.td\"\n include \"mlir/IR/OpBase.td\"\n include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n+include \"mlir/Interfaces/DestinationStyleOpInterface.td\"\n+include \"mlir/Interfaces/ViewLikeInterface.td\"\n \n def ResultsAreSharedEncoding: NativeOpTrait<\"ResultsAreSharedEncoding\">;\n \n@@ -105,6 +107,69 @@ def TTG_SelectOp : TTG_Op<\"select\", [Pure, Elementwise,\n }\n \n \n+\n+def TTG_ExtractSliceOp : TTG_Op<\"extract_slice\",\n+                                [AttrSizedOperandSegments,\n+                                 ResultsAreSharedEncoding,\n+                                 Pure,\n+                                 OffsetSizeAndStrideOpInterface\n+                                 ]> {\n+  let summary = \"extract slice operation\";\n+  let description = [{\n+    same as tensor.extract_slice, but with int32 index. The motivations for re-implementing it are:\n+    We reimplement ExtractSliceOp with int32 index, because:\n+    - we want to enforce int32 indexing on GPUs since Triton tensors fit in SRAM\n+    - we still want to use indexWidth = 64 when lowering to LLVM because our loops can have\n+      64-bit induction variables and scf.for uses indexType for bounds/ivs\n+  }];\n+\n+  let arguments = (ins\n+    AnyRankedTensor:$source,\n+    Variadic<I32>:$offsets,\n+    Variadic<I32>:$sizes,\n+    Variadic<I32>:$strides,\n+    DenseI64ArrayAttr:$static_offsets,\n+    DenseI64ArrayAttr:$static_sizes,\n+    DenseI64ArrayAttr:$static_strides\n+  );\n+  let results = (outs AnyRankedTensor:$result);\n+\n+  let builders = [\n+    // Build an ExtractSliceOp with mixed static and dynamic entries and custom\n+    // result type. If the type passed is nullptr, it is inferred.\n+    OpBuilder<(ins \"RankedTensorType\":$resultType, \"Value\":$source,\n+      \"ArrayRef<OpFoldResult>\":$offsets, \"ArrayRef<OpFoldResult>\":$sizes,\n+      \"ArrayRef<OpFoldResult>\":$strides,\n+      CArg<\"ArrayRef<NamedAttribute>\", \"{}\">:$attrs)>,\n+  ];\n+\n+  let extraClassDeclaration = [{\n+    /// Return the number of leading operands before the `offsets`, `sizes` and\n+    /// and `strides` operands.\n+    static unsigned getOffsetSizeAndStrideStartOperandIndex() { return 1; }\n+\n+    /// Returns the type of the base tensor operand.\n+    RankedTensorType getSourceType() {\n+      return getSource().getType().cast<RankedTensorType>();\n+    }\n+\n+    std::array<unsigned, 3> getArrayAttrMaxRanks() {\n+      unsigned rank = getSourceType().getRank();\n+      return {rank, rank, rank};\n+    }\n+  }];\n+\n+  let assemblyFormat = [{\n+    $source ``\n+    custom<DynamicIndexList>($offsets, $static_offsets)\n+    custom<DynamicIndexList>($sizes, $static_sizes)\n+    custom<DynamicIndexList>($strides, $static_strides)\n+    attr-dict `:` type($source) `to` type($result)\n+  }];\n+}\n+\n+//\n+\n def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n                                     [AttrSizedOperandSegments,\n                                      ResultsAreSharedEncoding,"}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -27,7 +27,7 @@ void SharedMemoryAliasAnalysis::visitOperation(\n     // These ops may allocate a new shared memory buffer.\n     auto result = op->getResult(0);\n     // XXX(Keren): the following ops are always aliasing for now\n-    if (isa<tensor::ExtractSliceOp, triton::TransOp>(op)) {\n+    if (isa<triton::gpu::ExtractSliceOp, triton::TransOp>(op)) {\n       // extract_slice %src\n       // trans %src\n       aliasInfo = AliasInfo(operands[0]->getValue());"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -846,10 +846,14 @@ AxisInfoAnalysis::AxisInfoAnalysis(DataFlowSolver &solver)\n void AxisInfoAnalysis::visitOperation(\n     Operation *op, ArrayRef<const dataflow::Lattice<AxisInfo> *> operands,\n     ArrayRef<dataflow::Lattice<AxisInfo> *> results) {\n+  // TODO: For sure not the right way to do this\n+  // but why is scf.if not initialized otherwise?\n+  for (auto op : operands)\n+    if (op->getValue().getRank() == 0)\n+      setToEntryState((dataflow::Lattice<AxisInfo> *)op);\n   AxisInfo curr = visitors.apply(op, operands);\n-  if (curr.getRank() == 0) {\n+  if (curr.getRank() == 0)\n     return setAllToEntryStates(results);\n-  }\n   // override with hint\n   auto newContiguity = curr.getContiguity();\n   auto newDivisibility = curr.getDivisibility();"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -78,8 +78,8 @@ void MembarAnalysis::visitTerminator(Operation *op,\n \n void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n                             OpBuilder *builder) {\n-  if (isa<tensor::ExtractSliceOp>(op) || isa<triton::gpu::AllocTensorOp>(op) ||\n-      isa<triton::TransOp>(op)) {\n+  if (isa<triton::gpu::ExtractSliceOp>(op) ||\n+      isa<triton::gpu::AllocTensorOp>(op) || isa<triton::TransOp>(op)) {\n     // alloc is an allocation op without memory write.\n     // FIXME(Keren): extract_slice is always alias for now\n     return;"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -114,7 +114,7 @@ bool maybeSharedAllocationOp(Operation *op) {\n }\n \n bool maybeAliasOp(Operation *op) {\n-  return isa<tensor::ExtractSliceOp>(op) || isa<triton::TransOp>(op) ||\n+  return isa<triton::gpu::ExtractSliceOp>(op) || isa<triton::TransOp>(op) ||\n          isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n          isa<tensor::InsertSliceOp>(op);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ArithToIndexPass.cpp", "status": "added", "additions": 90, "deletions": 0, "changes": 90, "file_content_changes": "@@ -0,0 +1,90 @@\n+#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n+#include \"mlir/Analysis/DataFlowFramework.h\"\n+#include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n+#include \"mlir/Conversion/ControlFlowToLLVM//ControlFlowToLLVM.h\"\n+#include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n+#include \"mlir/Conversion/LLVMCommon/VectorPattern.h\"\n+#include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n+#include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n+#include \"mlir/Dialect/Index/IR/IndexDialect.h\"\n+#include \"mlir/Dialect/Index/IR/IndexOps.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Conversion/Passes.h.inc\"\n+\n+namespace {\n+class TritonArithToIndexConversionTarget : public mlir::ConversionTarget {\n+public:\n+  static bool hasIndexResultOrOperand(Operation *op) {\n+    if (!op)\n+      return false;\n+    bool hasRetIndex = llvm::find_if(op->getResultTypes(), [](Type type) {\n+                         return type.isIndex();\n+                       }) != op->getResultTypes().end();\n+    bool hasArgIndex = llvm::find_if(op->getOperandTypes(), [](Type type) {\n+                         return type.isIndex();\n+                       }) != op->getOperandTypes().end();\n+    return !hasRetIndex && !hasArgIndex;\n+  }\n+\n+  explicit TritonArithToIndexConversionTarget(MLIRContext &ctx)\n+      : ConversionTarget(ctx) {\n+    addLegalDialect<index::IndexDialect>();\n+    addDynamicallyLegalDialect<arith::ArithDialect>(hasIndexResultOrOperand);\n+  }\n+};\n+\n+template <class SrcOp, class DstOp>\n+LogicalResult replaceArithWithIndex(SrcOp op, PatternRewriter &rewriter) {\n+  // if (!hasIndexResultOrOperand(&*op))\n+  //   return failure();\n+  rewriter.replaceOpWithNewOp<DstOp>(op, op->getResultTypes(),\n+                                     op->getOperands(), op->getAttrs());\n+  return success();\n+}\n+\n+LogicalResult replaceArithCmpWithIndexCmp(arith::CmpIOp op,\n+                                          PatternRewriter &rewriter) {\n+  // if (!hasIndexResultOrOperand(&*op))\n+  //   return failure();\n+  rewriter.replaceOpWithNewOp<index::CmpOp>(\n+      op, op.getResult().getType(), (index::IndexCmpPredicate)op.getPredicate(),\n+      op.getOperand(0), op.getOperand(1));\n+  return success();\n+}\n+\n+class ArithToIndex : public TritonConvertArithToIndexBase<ArithToIndex> {\n+public:\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp mod = getOperation();\n+    TritonArithToIndexConversionTarget target(*context);\n+    RewritePatternSet patterns(context);\n+    patterns.add(replaceArithWithIndex<arith::IndexCastOp, index::CastSOp>);\n+    patterns.add(replaceArithWithIndex<arith::ConstantOp, index::ConstantOp>);\n+    patterns.add(replaceArithWithIndex<arith::AddIOp, index::AddOp>);\n+    patterns.add(replaceArithCmpWithIndexCmp);\n+    if (failed(applyPartialConversion(mod, target, std::move(patterns)))) {\n+      return signalPassFailure();\n+    }\n+  }\n+};\n+} // namespace\n+\n+namespace mlir {\n+namespace triton {\n+\n+std::unique_ptr<OperationPass<ModuleOp>> createTritonConvertArithToIndexPass() {\n+  return std::make_unique<::ArithToIndex>();\n+}\n+\n+} // namespace triton\n+} // namespace mlir\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,4 +1,5 @@\n add_mlir_conversion_library(TritonGPUToLLVM\n+    ArithToIndexPass.cpp\n     ConvertLayoutOpToLLVM.cpp\n     DotOpToLLVM.cpp\n     ElementwiseOpToLLVM.cpp"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 17, "deletions": 17, "changes": 34, "file_content_changes": "@@ -68,7 +68,7 @@ struct ConvertLayoutOpConversion\n           elemId, getSizePerThread(layout), getOrder(layout));\n       for (unsigned d = 0; d < rank; ++d) {\n         multiDimOffset[d] = add(multiDimOffsetFirstElem[d],\n-                                idx_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n+                                i32_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n                                         multiDimElemId[d]));\n       }\n       return multiDimOffset;\n@@ -93,21 +93,21 @@ struct ConvertLayoutOpConversion\n       SmallVector<Value> mmaColIdx(4);\n       SmallVector<Value> mmaRowIdx(2);\n       Value threadId = getThreadId(rewriter, loc);\n-      Value warpSize = idx_val(32);\n+      Value warpSize = i32_val(32);\n       Value laneId = urem(threadId, warpSize);\n       Value warpId = udiv(threadId, warpSize);\n       // TODO: fix the bug in MMAEncodingAttr document\n       SmallVector<Value> multiDimWarpId(2);\n-      multiDimWarpId[0] = urem(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n-      multiDimWarpId[1] = udiv(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n-      Value _1 = idx_val(1);\n-      Value _2 = idx_val(2);\n-      Value _4 = idx_val(4);\n-      Value _8 = idx_val(8);\n-      Value _16 = idx_val(16);\n+      multiDimWarpId[0] = urem(warpId, i32_val(mmaLayout.getWarpsPerCTA()[0]));\n+      multiDimWarpId[1] = udiv(warpId, i32_val(mmaLayout.getWarpsPerCTA()[0]));\n+      Value _1 = i32_val(1);\n+      Value _2 = i32_val(2);\n+      Value _4 = i32_val(4);\n+      Value _8 = i32_val(8);\n+      Value _16 = i32_val(16);\n       if (mmaLayout.isAmpere()) {\n-        multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n-        multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 8));\n+        multiDimWarpId[0] = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n+        multiDimWarpId[1] = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n         Value mmaGrpId = udiv(laneId, _4);\n         Value mmaGrpIdP8 = add(mmaGrpId, _8);\n         Value mmaThreadIdInGrp = urem(laneId, _4);\n@@ -131,9 +131,9 @@ struct ConvertLayoutOpConversion\n         multiDimOffset[0] = elemId < 2 ? mmaRowIdx[0] : mmaRowIdx[1];\n         multiDimOffset[1] = elemId % 2 == 0 ? mmaColIdx[0] : mmaColIdx[1];\n         multiDimOffset[0] = add(\n-            multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n+            multiDimOffset[0], i32_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n         multiDimOffset[1] = add(\n-            multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n+            multiDimOffset[1], i32_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n       } else if (mmaLayout.isVolta()) {\n         auto [isARow, isBRow, isAVec4, isBVec4, mmaId] =\n             mmaLayout.decodeVoltaLayoutStates();\n@@ -212,13 +212,13 @@ struct ConvertLayoutOpConversion\n               currVal = zext(llvmElemTy, currVal);\n             else if (isPtr)\n               currVal = ptrtoint(llvmElemTy, currVal);\n-            valVec = insert_element(vecTy, valVec, currVal, idx_val(v));\n+            valVec = insert_element(vecTy, valVec, currVal, i32_val(v));\n           }\n           store(valVec, ptr);\n         } else {\n           Value valVec = load(ptr);\n           for (unsigned v = 0; v < vec; ++v) {\n-            Value currVal = extract_element(llvmElemTy, valVec, idx_val(v));\n+            Value currVal = extract_element(llvmElemTy, valVec, i32_val(v));\n             if (isInt1)\n               currVal = icmp_ne(currVal,\n                                 rewriter.create<LLVM::ConstantOp>(\n@@ -322,13 +322,13 @@ struct ConvertLayoutOpConversion\n         Value valVec = undef(vecTy);\n         for (unsigned v = 0; v < vec; ++v) {\n           auto currVal = coord2valT[elemId + v].second;\n-          valVec = insert_element(vecTy, valVec, currVal, idx_val(v));\n+          valVec = insert_element(vecTy, valVec, currVal, i32_val(v));\n         }\n         store(valVec, ptr);\n       } else {\n         Value valVec = load(ptr);\n         for (unsigned v = 0; v < vec; ++v) {\n-          Value currVal = extract_element(elemTy, valVec, idx_val(v));\n+          Value currVal = extract_element(elemTy, valVec, i32_val(v));\n           vals[elemId + v] = currVal;\n         }\n       }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 179, "deletions": 0, "changes": 179, "file_content_changes": "@@ -269,6 +269,17 @@ struct FpToFpOpConversion\n     return builder.launch(rewriter, loc, f32_ty, false);\n   }\n \n+  static Value convertFp16ToFp32(Location loc,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 const Value &v) {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.f32.f16\");\n+    auto res = builder.newOperand(\"=r\");\n+    auto operand = builder.newOperand(v, \"h\");\n+    cvt(res, operand);\n+    return builder.launch(rewriter, loc, f32_ty, false);\n+  }\n+\n   static Value convertFp32ToBf16(Location loc,\n                                  ConversionPatternRewriter &rewriter,\n                                  const Value &v) {\n@@ -282,6 +293,17 @@ struct FpToFpOpConversion\n     return builder.launch(rewriter, loc, i16_ty, false);\n   }\n \n+  static Value convertFp32ToFp16(Location loc,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 const Value &v) {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.rn.f16.f32\");\n+    auto res = builder.newOperand(\"=h\");\n+    auto operand = builder.newOperand(v, \"r\");\n+    cvt(res, operand);\n+    return builder.launch(rewriter, loc, f16_ty, false);\n+  }\n+\n   LogicalResult\n   matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n@@ -336,6 +358,12 @@ struct FpToFpOpConversion\n     } else if (srcEltType.isF32() && dstEltType.isBF16()) {\n       resultVals.emplace_back(\n           convertFp32ToBf16(loc, rewriter, adaptor.getFrom()));\n+    } else if (srcEltType.isF16() && dstEltType.isF32()) {\n+      resultVals.emplace_back(\n+          convertFp16ToFp32(loc, rewriter, adaptor.getFrom()));\n+    } else if (srcEltType.isF32() && dstEltType.isF16()) {\n+      resultVals.emplace_back(\n+          convertFp32ToFp16(loc, rewriter, adaptor.getFrom()));\n     } else {\n       assert(false && \"unsupported type casting\");\n     }\n@@ -860,3 +888,154 @@ void populateElementwiseOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   // __nv_expf for higher-precision calculation\n   patterns.add<ExpOpConversionApprox>(typeConverter, benefit);\n }\n+\n+struct FPExtOpConversion\n+    : ElementwiseOpConversionBase<LLVM::FPExtOp, FPExtOpConversion> {\n+  using Base = ElementwiseOpConversionBase<LLVM::FPExtOp, FPExtOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  static bool isLegalOp(LLVM::FPExtOp op) {\n+    auto retTy = op.getResult().getType();\n+    auto srcTy = op.getOperand().getType();\n+    if (retTy.isF32() && srcTy.isF16()) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  Value createDestOp(LLVM::FPExtOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    return FpToFpOpConversion::convertFp16ToFp32(loc, rewriter, operands[0]);\n+  }\n+};\n+\n+struct FPTruncOpConversion\n+    : ElementwiseOpConversionBase<LLVM::FPTruncOp, FPTruncOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<LLVM::FPTruncOp, FPTruncOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  static bool isLegalOp(LLVM::FPTruncOp op) {\n+    auto retTy = op.getResult().getType();\n+    auto srcTy = op.getOperand().getType();\n+    if (retTy.isF16() && srcTy.isF32()) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  Value createDestOp(LLVM::FPTruncOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    return FpToFpOpConversion::convertFp32ToFp16(loc, rewriter, operands[0]);\n+  }\n+};\n+\n+struct TruncOpConversion\n+    : ElementwiseOpConversionBase<LLVM::TruncOp, TruncOpConversion> {\n+  using Base = ElementwiseOpConversionBase<LLVM::TruncOp, TruncOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  static bool isLegalOp(LLVM::TruncOp op) {\n+    auto retTy = op.getResult().getType();\n+    auto srcTy = op.getOperand().getType();\n+    if (retTy.isInteger(16) && srcTy.isInteger(32)) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  Value createDestOp(LLVM::TruncOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.u16.u32\");\n+    auto res = builder.newOperand(\"=h\");\n+    auto operand = builder.newOperand(operands[0], \"r\");\n+    cvt(res, operand);\n+    return builder.launch(rewriter, loc, i16_ty, false);\n+  }\n+};\n+\n+struct SExtOpConversion\n+    : ElementwiseOpConversionBase<LLVM::SExtOp, SExtOpConversion> {\n+  using Base = ElementwiseOpConversionBase<LLVM::SExtOp, SExtOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  static bool isLegalOp(LLVM::SExtOp op) {\n+    auto retTy = op.getResult().getType();\n+    auto srcTy = op.getOperand().getType();\n+    if (retTy.isInteger(32) && srcTy.isInteger(16)) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  Value createDestOp(LLVM::SExtOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.s32.s16\");\n+    auto res = builder.newOperand(\"=r\");\n+    auto operand = builder.newOperand(operands[0], \"h\");\n+    cvt(res, operand);\n+    return builder.launch(rewriter, loc, i32_ty, false);\n+  }\n+};\n+\n+struct ZExtOpConversion\n+    : ElementwiseOpConversionBase<LLVM::ZExtOp, ZExtOpConversion> {\n+  using Base = ElementwiseOpConversionBase<LLVM::ZExtOp, ZExtOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  static bool isLegalOp(LLVM::ZExtOp op) {\n+    auto retTy = op.getResult().getType();\n+    auto srcTy = op.getOperand().getType();\n+    if (retTy.isInteger(32) && srcTy.isInteger(16)) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  Value createDestOp(LLVM::ZExtOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.u32.u16\");\n+    auto res = builder.newOperand(\"=r\");\n+    auto operand = builder.newOperand(operands[0], \"h\");\n+    cvt(res, operand);\n+    return builder.launch(rewriter, loc, i32_ty, false);\n+  }\n+};\n+\n+bool isLegalElementwiseOp(Operation *op) {\n+  if (isa<LLVM::FPExtOp>(op)) {\n+    return FPExtOpConversion::isLegalOp(cast<LLVM::FPExtOp>(op));\n+  } else if (isa<LLVM::FPTruncOp>(op)) {\n+    return FPTruncOpConversion::isLegalOp(cast<LLVM::FPTruncOp>(op));\n+  } else if (isa<LLVM::TruncOp>(op)) {\n+    return TruncOpConversion::isLegalOp(cast<LLVM::TruncOp>(op));\n+  } else if (isa<LLVM::SExtOp>(op)) {\n+    return SExtOpConversion::isLegalOp(cast<LLVM::SExtOp>(op));\n+  } else if (isa<LLVM::ZExtOp>(op)) {\n+    return ZExtOpConversion::isLegalOp(cast<LLVM::ZExtOp>(op));\n+  }\n+  return true;\n+}\n+\n+void populateElementwiseOpToPTXPatterns(mlir::LLVMTypeConverter &typeConverter,\n+                                        RewritePatternSet &patterns,\n+                                        PatternBenefit benefit) {\n+  patterns.add<FPExtOpConversion>(typeConverter, benefit);\n+  patterns.add<FPTruncOpConversion>(typeConverter, benefit);\n+  patterns.add<TruncOpConversion>(typeConverter, benefit);\n+  patterns.add<SExtOpConversion>(typeConverter, benefit);\n+  patterns.add<ZExtOpConversion>(typeConverter, benefit);\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -13,4 +13,10 @@ void populateElementwiseOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                          const Allocation *allocation,\n                                          Value smem, PatternBenefit benefit);\n \n+bool isLegalElementwiseOp(Operation *op);\n+\n+void populateElementwiseOpToPTXPatterns(mlir::LLVMTypeConverter &typeConverter,\n+                                        RewritePatternSet &patterns,\n+                                        PatternBenefit benefit);\n+\n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -565,7 +565,7 @@ struct AtomicRMWOpConversion\n         auto ret = ptxBuilderAtomicRMW.launch(rewriter, loc, retType);\n         for (int ii = 0; ii < vec; ++ii) {\n           resultVals[i + ii] =\n-              vec == 1 ? ret : extract_element(valueElemTy, ret, idx_val(ii));\n+              vec == 1 ? ret : extract_element(valueElemTy, ret, i32_val(ii));\n         }\n       } else {\n         PTXBuilder ptxBuilderMemfence;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 10, "deletions": 13, "changes": 23, "file_content_changes": "@@ -390,11 +390,9 @@ struct GetProgramIdOpConversion\n     Location loc = op->getLoc();\n     assert(op.getAxis() < 3);\n \n-    Value blockId = rewriter.create<::mlir::gpu::BlockIdOp>(\n-        loc, rewriter.getIndexType(), dims[op.getAxis()]);\n-    auto llvmIndexTy = getTypeConverter()->getIndexType();\n-    rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>(\n-        op, TypeRange{llvmIndexTy}, ValueRange{blockId});\n+    Value blockId =\n+        rewriter.create<::mlir::gpu::BlockIdOp>(loc, dims[op.getAxis()]);\n+    rewriter.replaceOpWithNewOp<arith::TruncIOp>(op, i32_ty, blockId);\n     return success();\n   }\n \n@@ -414,11 +412,10 @@ struct GetNumProgramsOpConversion\n     Location loc = op->getLoc();\n     assert(op.getAxis() < 3);\n \n-    Value blockId = rewriter.create<::mlir::gpu::GridDimOp>(\n-        loc, rewriter.getIndexType(), dims[op.getAxis()]);\n-    auto llvmIndexTy = getTypeConverter()->getIndexType();\n-    rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>(\n-        op, TypeRange{llvmIndexTy}, ValueRange{blockId});\n+    Value blockId =\n+        rewriter.create<::mlir::gpu::GridDimOp>(loc, dims[op.getAxis()]);\n+    rewriter.replaceOpWithNewOp<arith::TruncIOp>(op, i32_ty, blockId);\n+\n     return success();\n   }\n \n@@ -496,12 +493,12 @@ struct AllocTensorOpConversion\n };\n \n struct ExtractSliceOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<tensor::ExtractSliceOp> {\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ExtractSliceOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n-      tensor::ExtractSliceOp>::ConvertTritonGPUOpToLLVMPattern;\n+      triton::gpu::ExtractSliceOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   LogicalResult\n-  matchAndRewrite(tensor::ExtractSliceOp op, OpAdaptor adaptor,\n+  matchAndRewrite(triton::gpu::ExtractSliceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     // %dst = extract_slice %src[%offsets]\n     Location loc = op->getLoc();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 32, "deletions": 37, "changes": 69, "file_content_changes": "@@ -208,13 +208,9 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n   Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n     auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n-    auto cast = rewriter.create<UnrealizedConversionCastOp>(\n-        loc, TypeRange{llvmIndexTy},\n-        ValueRange{rewriter.create<::mlir::gpu::ThreadIdOp>(\n-            loc, rewriter.getIndexType(), ::mlir::gpu::Dimension::x)});\n-    Value threadId = cast.getResult(0);\n-\n-    return threadId;\n+    auto tid = rewriter.create<::mlir::gpu::ThreadIdOp>(\n+        loc, rewriter.getIndexType(), ::mlir::gpu::Dimension::x);\n+    return rewriter.create<arith::TruncIOp>(loc, i32_ty, tid);\n   }\n \n   // -----------------------------------------------------------------------\n@@ -223,13 +219,12 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   template <typename T>\n   Value getSharedMemoryBase(Location loc, ConversionPatternRewriter &rewriter,\n                             T value) const {\n-\n     auto ptrTy = LLVM::LLVMPointerType::get(\n         this->getTypeConverter()->convertType(rewriter.getI8Type()), 3);\n     auto bufferId = allocation->getBufferId(value);\n     assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n     size_t offset = allocation->getOffset(bufferId);\n-    Value offVal = idx_val(offset);\n+    Value offVal = i32_val(offset);\n     Value base = gep(ptrTy, smem, offVal);\n     return base;\n   }\n@@ -244,8 +239,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     // This utililty computes the pointers for accessing the provided swizzled\n     // shared memory layout `resSharedLayout`. More specifically, it computes,\n     // for all indices (row, col) of `srcEncoding` such that idx % inVec = 0,\n-    // the pointer: ptr[(row, col)] = base + (rowOff * strides[ord[1]] + colOff)\n-    // where :\n+    // the pointer: ptr[(row, col)] = base + (rowOff * strides[ord[1]] +\n+    // colOff) where :\n     //   compute phase = (row // perPhase) % maxPhase\n     //   rowOff = row\n     //   colOff = colOffSwizzled + colOffOrdered\n@@ -255,8 +250,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     // Note 1:\n     // -------\n     // Because swizzling happens at a granularity of outVec, we need to\n-    // decompose the offset into a swizzled factor and a non-swizzled (ordered)\n-    // factor\n+    // decompose the offset into a swizzled factor and a non-swizzled\n+    // (ordered) factor\n     //\n     // Note 2:\n     // -------\n@@ -291,10 +286,10 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n       // extract multi dimensional index for current element\n       auto idx = srcIndices[elemIdx];\n-      Value idxCol = idx[inOrder[0]]; // contiguous dimension\n-      Value idxRow = idx[inOrder[1]]; // discontiguous dimension\n-      Value strideCol = srcStrides[inOrder[0]];\n-      Value strideRow = srcStrides[inOrder[1]];\n+      Value idxCol = idx[outOrder[0]]; // contiguous dimension\n+      Value idxRow = idx[outOrder[1]]; // discontiguous dimension\n+      Value strideCol = srcStrides[outOrder[0]];\n+      Value strideRow = srcStrides[outOrder[1]];\n       // extract dynamic/static offset for immediate offseting\n       unsigned immedateOffCol = 0;\n       if (auto add = dyn_cast_or_null<LLVM::AddOp>(idxCol.getDefiningOp()))\n@@ -338,7 +333,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       Value currPtr = gep(dstPtrTy, dstPtrBase, offset);\n       // compute immediate offset\n       Value immedateOff =\n-          add(mul(i32_val(immedateOffRow), srcStrides[inOrder[1]]),\n+          add(mul(i32_val(immedateOffRow), srcStrides[outOrder[1]]),\n               i32_val(immedateOffCol));\n       ret[elemIdx] = gep(dstPtrTy, currPtr, immedateOff);\n     }\n@@ -435,7 +430,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     } else {\n       Value remained = linear;\n       for (auto &&en : llvm::enumerate(shape.drop_back())) {\n-        Value dimSize = idx_val(en.value());\n+        Value dimSize = i32_val(en.value());\n         multiDim[en.index()] = urem(remained, dimSize);\n         remained = udiv(remained, dimSize);\n       }\n@@ -454,12 +449,12 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n                   ArrayRef<Value> multiDim, ArrayRef<unsigned> shape) const {\n     auto rank = multiDim.size();\n-    Value linear = idx_val(0);\n+    Value linear = i32_val(0);\n     if (rank > 0) {\n       linear = multiDim.back();\n       for (auto [dim, dimShape] :\n            llvm::reverse(llvm::zip(multiDim.drop_back(), shape.drop_back()))) {\n-        Value dimSize = idx_val(dimShape);\n+        Value dimSize = i32_val(dimShape);\n         linear = add(mul(linear, dimSize), dim);\n       }\n     }\n@@ -469,7 +464,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   Value dot(ConversionPatternRewriter &rewriter, Location loc,\n             ArrayRef<Value> offsets, ArrayRef<Value> strides) const {\n     assert(offsets.size() == strides.size());\n-    Value ret = idx_val(0);\n+    Value ret = i32_val(0);\n     for (auto [offset, stride] : llvm::zip(offsets, strides)) {\n       ret = add(ret, mul(offset, stride));\n     }\n@@ -597,7 +592,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                                 const BlockedEncodingAttr &blocked_layout,\n                                 ArrayRef<int64_t> shape) const {\n     Value threadId = getThreadId(rewriter, loc);\n-    Value warpSize = idx_val(32);\n+    Value warpSize = i32_val(32);\n     Value laneId = urem(threadId, warpSize);\n     Value warpId = udiv(threadId, warpSize);\n     auto sizePerThread = blocked_layout.getSizePerThread();\n@@ -619,13 +614,13 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       auto maxWarps =\n           ceil<unsigned>(shape[k], sizePerThread[k] * threadsPerWarp[k]);\n       auto maxThreads = ceil<unsigned>(shape[k], sizePerThread[k]);\n-      multiDimWarpId[k] = urem(multiDimWarpId[k], idx_val(maxWarps));\n-      multiDimThreadId[k] = urem(multiDimThreadId[k], idx_val(maxThreads));\n+      multiDimWarpId[k] = urem(multiDimWarpId[k], i32_val(maxWarps));\n+      multiDimThreadId[k] = urem(multiDimThreadId[k], i32_val(maxThreads));\n       // multiDimBase[k] = (multiDimThreadId[k] +\n       //                    multiDimWarpId[k] * threadsPerWarp[k]) *\n       //                   sizePerThread[k];\n-      Value threadsPerWarpK = idx_val(threadsPerWarp[k]);\n-      Value sizePerThreadK = idx_val(sizePerThread[k]);\n+      Value threadsPerWarpK = i32_val(threadsPerWarp[k]);\n+      Value sizePerThreadK = i32_val(sizePerThread[k]);\n       multiDimBase[k] =\n           mul(sizePerThreadK, add(multiDimThreadId[k],\n                                   mul(multiDimWarpId[k], threadsPerWarpK)));\n@@ -799,21 +794,21 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                               ArrayRef<int64_t> shape) const {\n     auto _warpsPerCTA = mmaLayout.getWarpsPerCTA();\n     assert(_warpsPerCTA.size() == 2);\n-    SmallVector<Value> warpsPerCTA = {idx_val(_warpsPerCTA[0]),\n-                                      idx_val(_warpsPerCTA[1])};\n+    SmallVector<Value> warpsPerCTA = {i32_val(_warpsPerCTA[0]),\n+                                      i32_val(_warpsPerCTA[1])};\n     Value threadId = getThreadId(rewriter, loc);\n-    Value warpSize = idx_val(32);\n+    Value warpSize = i32_val(32);\n     Value laneId = urem(threadId, warpSize);\n     Value warpId = udiv(threadId, warpSize);\n-    Value warpId0 = urem(urem(warpId, warpsPerCTA[0]), idx_val(shape[0] / 16));\n+    Value warpId0 = urem(urem(warpId, warpsPerCTA[0]), i32_val(shape[0] / 16));\n     Value warpId1 = urem(urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]),\n-                         idx_val(shape[1] / 8));\n-    Value offWarp0 = mul(warpId0, idx_val(16));\n-    Value offWarp1 = mul(warpId1, idx_val(8));\n+                         i32_val(shape[1] / 8));\n+    Value offWarp0 = mul(warpId0, i32_val(16));\n+    Value offWarp1 = mul(warpId1, i32_val(8));\n \n     SmallVector<Value> multiDimBase(2);\n-    multiDimBase[0] = add(udiv(laneId, idx_val(4)), offWarp0);\n-    multiDimBase[1] = add(mul(idx_val(2), urem(laneId, idx_val(4))), offWarp1);\n+    multiDimBase[0] = add(udiv(laneId, i32_val(4)), offWarp0);\n+    multiDimBase[1] = add(mul(i32_val(2), urem(laneId, i32_val(4))), offWarp1);\n     return multiDimBase;\n   }\n \n@@ -850,7 +845,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                                                 SmallVector<Value>(rank));\n     for (unsigned n = 0; n < elemsPerThread; ++n)\n       for (unsigned k = 0; k < rank; ++k)\n-        multiDimIdx[n][k] = add(multiDimBase[k], idx_val(offset[n][k]));\n+        multiDimIdx[n][k] = add(multiDimBase[k], i32_val(offset[n][k]));\n     return multiDimIdx;\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 100, "deletions": 93, "changes": 193, "file_content_changes": "@@ -4,8 +4,11 @@\n #include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n #include \"mlir/Conversion/ControlFlowToLLVM//ControlFlowToLLVM.h\"\n #include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n+#include \"mlir/Conversion/LLVMCommon/VectorPattern.h\"\n #include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n #include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n+#include \"mlir/Dialect/Index/IR/IndexDialect.h\"\n+#include \"mlir/Dialect/Index/IR/IndexOps.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n #include \"mlir/Pass/Pass.h\"\n@@ -24,41 +27,58 @@\n #include \"TypeConverter.h\"\n #include \"ViewOpToLLVM.h\"\n \n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n+\n using namespace mlir;\n using namespace mlir::triton;\n \n #define GEN_PASS_CLASSES\n #include \"triton/Conversion/Passes.h.inc\"\n \n-namespace mlir {\n+namespace {\n \n-class TritonLLVMConversionTarget : public ConversionTarget {\n+class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n public:\n-  explicit TritonLLVMConversionTarget(MLIRContext &ctx)\n+  explicit TritonLLVMFunctionConversionTarget(MLIRContext &ctx)\n       : ConversionTarget(ctx) {\n+    addLegalDialect<index::IndexDialect>();\n     addLegalDialect<LLVM::LLVMDialect>();\n     addLegalDialect<NVVM::NVVMDialect>();\n-    addIllegalDialect<triton::TritonDialect>();\n-    addIllegalDialect<triton::gpu::TritonGPUDialect>();\n-    addIllegalDialect<mlir::gpu::GPUDialect>();\n+    addIllegalOp<mlir::func::FuncOp>();\n     addLegalOp<mlir::UnrealizedConversionCastOp>();\n   }\n };\n \n-class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n+class TritonPTXConversionTarget : public ConversionTarget {\n public:\n-  explicit TritonLLVMFunctionConversionTarget(MLIRContext &ctx)\n-      : ConversionTarget(ctx) {\n-    addLegalDialect<LLVM::LLVMDialect>();\n+  explicit TritonPTXConversionTarget(MLIRContext &ctx) : ConversionTarget(ctx) {\n+    addDynamicallyLegalDialect<LLVM::LLVMDialect>(\n+        [&](Operation *op) { return isLegalElementwiseOp(op); });\n     addLegalDialect<NVVM::NVVMDialect>();\n-    addIllegalOp<mlir::func::FuncOp>();\n     addLegalOp<mlir::UnrealizedConversionCastOp>();\n   }\n };\n \n-} // namespace mlir\n+struct ReturnOpConversion : public ConvertOpToLLVMPattern<func::ReturnOp> {\n+  using ConvertOpToLLVMPattern<func::ReturnOp>::ConvertOpToLLVMPattern;\n \n-namespace {\n+  LogicalResult\n+  matchAndRewrite(func::ReturnOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    unsigned numArguments = op.getNumOperands();\n+\n+    // Currently, Triton kernel function always return nothing.\n+    // TODO(Superjomn) add support for non-inline device function\n+    if (numArguments > 0) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"Only kernel function with nothing returned is supported.\");\n+    }\n+\n+    rewriter.replaceOpWithNewOp<LLVM::ReturnOp>(op, TypeRange(), ValueRange(),\n+                                                op->getAttrs());\n+    return success();\n+  }\n+};\n \n /// FuncOp legalization pattern that converts MemRef arguments to pointers to\n /// MemRef descriptors (LLVM struct data types) containing all the MemRef type\n@@ -72,8 +92,9 @@ struct FuncOpConversion : public FuncOpConversionBase {\n   matchAndRewrite(func::FuncOp funcOp, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     auto newFuncOp = convertFuncOpToLLVMFuncOp(funcOp, rewriter);\n-    if (!newFuncOp)\n+    if (!newFuncOp) {\n       return failure();\n+    }\n \n     auto ctx = funcOp->getContext();\n \n@@ -93,6 +114,22 @@ struct FuncOpConversion : public FuncOpConversionBase {\n   int numWarps{0};\n };\n \n+class TritonLLVMConversionTarget : public ConversionTarget {\n+public:\n+  explicit TritonLLVMConversionTarget(MLIRContext &ctx)\n+      : ConversionTarget(ctx) {\n+    addLegalDialect<LLVM::LLVMDialect>();\n+    addLegalDialect<NVVM::NVVMDialect>();\n+    addIllegalDialect<triton::TritonDialect>();\n+    addIllegalDialect<triton::gpu::TritonGPUDialect>();\n+    addIllegalDialect<mlir::gpu::GPUDialect>();\n+    addLegalOp<mlir::UnrealizedConversionCastOp>();\n+  }\n+};\n+\n+using FPTruncLowering =\n+    VectorConvertToLLVMPattern<LLVM::FPTruncOp, arith::TruncFOp>;\n+\n class ConvertTritonGPUToLLVM\n     : public ConvertTritonGPUToLLVMBase<ConvertTritonGPUToLLVM> {\n \n@@ -103,59 +140,39 @@ class ConvertTritonGPUToLLVM\n   void runOnOperation() override {\n     MLIRContext *context = &getContext();\n     ModuleOp mod = getOperation();\n-\n     mlir::LowerToLLVMOptions option(context);\n     option.overrideIndexBitwidth(32);\n     TritonGPUToLLVMTypeConverter typeConverter(context, option);\n-    TritonLLVMFunctionConversionTarget funcTarget(*context);\n     TritonLLVMConversionTarget target(*context);\n-\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n \n-    // Step 1: Decompose unoptimized layout conversions to use shared memory\n-    // Step 2: Decompose insert_slice_async to use load + insert_slice for\n-    //   pre-Ampere architectures or unsupported vectorized load sizes\n-    // Step 3: Convert SCF to CFG\n-    // Step 4: Allocate shared memories and insert barriers\n-    // Step 5: Convert FuncOp to LLVMFuncOp via partial conversion\n-    // Step 6: Get axis and shared memory info\n-    // Step 7: Convert the rest of ops via partial conversion\n-    //\n-    // The reason for a separation between 5/7 is that, step 6 is out of the\n-    // scope of Dialect Conversion, thus we need to make sure the smem is not\n-    // revised during the conversion of step 7.\n-\n-    // Step 1\n+    /* preprocess */\n     decomposeMmaToDotOperand(mod, numWarps);\n     decomposeBlockedToDotOperand(mod);\n-\n-    // Step 2\n     if (failed(decomposeInsertSliceAsyncOp(mod)))\n       return signalPassFailure();\n \n-    // Step 3\n-    RewritePatternSet scfPatterns(context);\n-    mlir::populateSCFToControlFlowConversionPatterns(scfPatterns);\n-    mlir::ConversionTarget scfTarget(*context);\n-    scfTarget.addIllegalOp<scf::ForOp, scf::IfOp, scf::ParallelOp, scf::WhileOp,\n-                           scf::ExecuteRegionOp>();\n-    scfTarget.markUnknownOpDynamicallyLegal([](Operation *) { return true; });\n-    if (failed(applyPartialConversion(mod, scfTarget, std::move(scfPatterns))))\n-      return signalPassFailure();\n-\n-    // Step 4\n+    /* allocate shared memory and set barrier */\n     Allocation allocation(mod);\n     MembarAnalysis membarPass(&allocation);\n     membarPass.run();\n \n-    // Step 5\n-    RewritePatternSet funcPatterns(context);\n-    funcPatterns.add<FuncOpConversion>(typeConverter, numWarps, /*benefit=*/1);\n-    if (failed(\n-            applyPartialConversion(mod, funcTarget, std::move(funcPatterns))))\n-      return signalPassFailure();\n+    /* lower functions */\n+    {\n+      mlir::LowerToLLVMOptions option(context);\n+      TritonGPUToLLVMTypeConverter typeConverter(context, option);\n+      TritonLLVMFunctionConversionTarget funcTarget(*context);\n+      RewritePatternSet funcPatterns(context);\n+      funcPatterns.add<FuncOpConversion>(typeConverter, numWarps,\n+                                         /*benefit=*/1);\n+      funcPatterns.add<ReturnOpConversion>(typeConverter);\n+      mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n+                                                            funcPatterns);\n+      if (failed(\n+              applyPartialConversion(mod, funcTarget, std::move(funcPatterns))))\n+        return signalPassFailure();\n+    }\n \n-    // Step 6 - get axis and shared memory info\n     std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n     AxisInfoAnalysis *axisInfoAnalysis = solver->load<AxisInfoAnalysis>();\n     if (failed(solver->initializeAndRun(mod)))\n@@ -165,54 +182,44 @@ class ConvertTritonGPUToLLVM\n                  mlir::IntegerAttr::get(mlir::IntegerType::get(context, 32),\n                                         allocation.getSharedMemorySize()));\n \n-    // Step 7 - rewrite rest of ops\n-    // We set a higher benefit here to ensure triton's patterns runs before\n-    // arith patterns for some encoding not supported by the community\n-    // patterns.\n+    /* rewrite ops */\n+    RewritePatternSet patterns(context);\n+    // TritonGPU lowering patterns\n     OpBuilder::InsertPoint indexInsertPoint;\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo indexCacheInfo{\n         &baseIndexCache, &indexCache, &indexInsertPoint};\n-\n-    RewritePatternSet patterns(context);\n-\n-    // Normal conversions\n-    populateTritonGPUToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                    *axisInfoAnalysis, &allocation, smem,\n-                                    indexCacheInfo, /*benefit=*/10);\n-    // ConvertLayoutOp\n-    populateConvertLayoutOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                          *axisInfoAnalysis, &allocation, smem,\n-                                          indexCacheInfo, /*benefit=*/10);\n-    // DotOp\n-    populateDotOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                *axisInfoAnalysis, &allocation, smem,\n-                                /*benefit=*/10);\n-    // ElementwiseOp\n-    populateElementwiseOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                        *axisInfoAnalysis, &allocation, smem,\n-                                        /*benefit=*/10);\n-    // LoadStoreOp\n-    populateLoadStoreOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                      *axisInfoAnalysis, &allocation, smem,\n-                                      indexCacheInfo, /*benefit=*/10);\n-    // ReduceOp\n-    populateReduceOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                   *axisInfoAnalysis, &allocation, smem,\n-                                   indexCacheInfo, /*benefit=*/10);\n-    // ViewOp\n-    populateViewOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                 *axisInfoAnalysis, &allocation, smem,\n-                                 /*benefit=*/10);\n-\n-    // Add arith/math's patterns to help convert scalar expression to LLVM.\n-    mlir::arith::populateArithToLLVMConversionPatterns(typeConverter, patterns);\n-    mlir::populateMathToLLVMConversionPatterns(typeConverter, patterns);\n-    mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n-                                                          patterns);\n+    auto populatePatterns1 = [&](auto populateFunc) {\n+      populateFunc(typeConverter, patterns, numWarps, *axisInfoAnalysis,\n+                   &allocation, smem, indexCacheInfo, /*benefit*/ 1);\n+    };\n+    auto populatePatterns2 = [&](auto populateFunc) {\n+      populateFunc(typeConverter, patterns, numWarps, *axisInfoAnalysis,\n+                   &allocation, smem, /*benefit*/ 1);\n+    };\n+    populatePatterns1(populateTritonGPUToLLVMPatterns);\n+    populatePatterns1(populateConvertLayoutOpToLLVMPatterns);\n+    populatePatterns2(populateDotOpToLLVMPatterns);\n+    populatePatterns2(populateElementwiseOpToLLVMPatterns);\n+    populatePatterns1(populateLoadStoreOpToLLVMPatterns);\n+    populatePatterns1(populateReduceOpToLLVMPatterns);\n+    populatePatterns2(populateViewOpToLLVMPatterns);\n+    // Native lowering patterns\n     mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n-\n     if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n       return signalPassFailure();\n+\n+    // Use our custom converters to convert some operations to PTX to avoid\n+    // using NVPTX for two reasons:\n+    // 1. NVPTX backend is flaky on data types like float16 and bfloat16\n+    // 2. In some cases, we may generate faster PTX code than NVPTX backend\n+    TritonPTXConversionTarget ptxTarget(*context);\n+    RewritePatternSet ptxPatterns(context);\n+    // Add patterns to convert LLVM to PTX\n+    populateElementwiseOpToPTXPatterns(typeConverter, ptxPatterns,\n+                                       /*benefits=*/10);\n+\n+    if (failed(applyPartialConversion(mod, ptxTarget, std::move(ptxPatterns))))\n+      return signalPassFailure();\n   }\n \n private:"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -79,6 +79,7 @@\n #define call(...) rewriter.create<LLVM::CallOp>(loc, __VA_ARGS__)\n \n // Types\n+#define i64_ty rewriter.getIntegerType(64)\n #define i32_ty rewriter.getIntegerType(32)\n #define i16_ty rewriter.getIntegerType(16)\n #define ui32_ty rewriter.getIntegerType(32, false)\n@@ -99,9 +100,6 @@\n #define i32_val(...) LLVM::createConstantI32(loc, rewriter, __VA_ARGS__)\n #define int_val(width, val)                                                    \\\n   LLVM::createLLVMIntegerConstant(rewriter, loc, width, val)\n-#define idx_val(...)                                                           \\\n-  LLVM::createIndexConstant(rewriter, loc, this->getTypeConverter(),           \\\n-                            __VA_ARGS__)\n #define tid_val() getThreadId(rewriter, loc)\n \n // Attributes"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 86, "deletions": 42, "changes": 128, "file_content_changes": "@@ -21,6 +21,13 @@ using namespace mlir::triton;\n \n namespace {\n \n+// pass named attrs (e.g., tt.contiguity) from Triton to Triton\n+static void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n+  for (const NamedAttribute attr : dictAttrs.getValue())\n+    if (!op->hasAttr(attr.getName()))\n+      op->setAttr(attr.getName(), attr.getValue());\n+}\n+\n template <class Op> class GenericOpPattern : public OpConversionPattern<Op> {\n public:\n   using OpConversionPattern<Op>::OpConversionPattern;\n@@ -29,7 +36,9 @@ template <class Op> class GenericOpPattern : public OpConversionPattern<Op> {\n   matchAndRewrite(Op op, typename Op::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    rewriter.replaceOpWithNewOp<Op>(op, retType, adaptor.getOperands());\n+    addNamedAttrs(\n+        rewriter.replaceOpWithNewOp<Op>(op, retType, adaptor.getOperands()),\n+        adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -43,8 +52,10 @@ class ArithCmpPattern : public OpConversionPattern<SrcOp> {\n   matchAndRewrite(SrcOp op, typename SrcOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    rewriter.replaceOpWithNewOp<DstOp>(op, retType, adaptor.getPredicate(),\n-                                       adaptor.getLhs(), adaptor.getRhs());\n+    addNamedAttrs(\n+        rewriter.replaceOpWithNewOp<DstOp>(op, retType, adaptor.getPredicate(),\n+                                           adaptor.getLhs(), adaptor.getRhs()),\n+        adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -65,7 +76,9 @@ class ArithConstantPattern : public OpConversionPattern<arith::ConstantOp> {\n     else\n       // This is a hack. We just want to add encoding\n       value = value.reshape(retType);\n-    rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, retType, value);\n+    addNamedAttrs(\n+        rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, retType, value),\n+        adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -137,9 +150,10 @@ class StdSelectPattern : public OpConversionPattern<arith::SelectOp> {\n   matchAndRewrite(arith::SelectOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n-        op, retType, adaptor.getCondition(), adaptor.getTrueValue(),\n-        adaptor.getFalseValue());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n+                      op, retType, adaptor.getCondition(),\n+                      adaptor.getTrueValue(), adaptor.getFalseValue()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -176,8 +190,9 @@ struct TritonMakeRangePattern\n   matchAndRewrite(triton::MakeRangeOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = getTypeConverter()->convertType(op.getType());\n-    rewriter.replaceOpWithNewOp<triton::MakeRangeOp>(\n-        op, retType, adaptor.getStart(), adaptor.getEnd());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::MakeRangeOp>(\n+                      op, retType, adaptor.getStart(), adaptor.getEnd()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -220,8 +235,9 @@ struct TritonExpandDimsPattern\n     // construct new op\n     auto newSrc = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         op.getLoc(), newArgType, adaptor.getSrc());\n-    rewriter.replaceOpWithNewOp<triton::ExpandDimsOp>(op, newSrc,\n-                                                      adaptor.getAxis());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::ExpandDimsOp>(\n+                      op, newSrc, adaptor.getAxis()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -273,8 +289,9 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n     }\n     c = rewriter.create<triton::gpu::ConvertLayoutOp>(c.getLoc(), retType, c);\n \n-    rewriter.replaceOpWithNewOp<triton::DotOp>(op, retType, a, b, c,\n-                                               adaptor.getAllowTF32());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::DotOp>(\n+                      op, retType, a, b, c, adaptor.getAllowTF32()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -289,8 +306,9 @@ struct TritonCatPattern : public OpConversionPattern<triton::CatOp> {\n     // For now, this behaves like generic, but this will evolve when\n     // we add support for `can_reorder=False`\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    rewriter.replaceOpWithNewOp<triton::CatOp>(op, retType,\n-                                               adaptor.getOperands());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::CatOp>(\n+                      op, retType, adaptor.getOperands()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -322,7 +340,8 @@ struct TritonTransPattern : public OpConversionPattern<triton::TransOp> {\n       src = rewriter.create<triton::gpu::ConvertLayoutOp>(src.getLoc(), srcType,\n                                                           src);\n     }\n-    rewriter.replaceOpWithNewOp<triton::TransOp>(op, src);\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::TransOp>(op, src),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -333,10 +352,12 @@ struct TritonLoadPattern : public OpConversionPattern<triton::LoadOp> {\n   LogicalResult\n   matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<triton::LoadOp>(\n-        op, typeConverter->convertType(op.getType()), adaptor.getPtr(),\n-        adaptor.getMask(), adaptor.getOther(), adaptor.getCache(),\n-        adaptor.getEvict(), adaptor.getIsVolatile());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::LoadOp>(\n+                      op, typeConverter->convertType(op.getType()),\n+                      adaptor.getPtr(), adaptor.getMask(), adaptor.getOther(),\n+                      adaptor.getCache(), adaptor.getEvict(),\n+                      adaptor.getIsVolatile()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -347,9 +368,11 @@ struct TritonStorePattern : public OpConversionPattern<triton::StoreOp> {\n   LogicalResult\n   matchAndRewrite(triton::StoreOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<triton::StoreOp>(\n-        op, adaptor.getPtr(), adaptor.getValue(), adaptor.getMask(),\n-        adaptor.getCache(), adaptor.getEvict());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::StoreOp>(\n+                      op, adaptor.getPtr(), adaptor.getValue(),\n+                      adaptor.getMask(), adaptor.getCache(),\n+                      adaptor.getEvict()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -361,9 +384,10 @@ struct TritonAtomicCASPattern\n   LogicalResult\n   matchAndRewrite(triton::AtomicCASOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<triton::AtomicCASOp>(\n-        op, typeConverter->convertType(op.getType()), adaptor.getPtr(),\n-        adaptor.getCmp(), adaptor.getVal());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::AtomicCASOp>(\n+                      op, typeConverter->convertType(op.getType()),\n+                      adaptor.getPtr(), adaptor.getCmp(), adaptor.getVal()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -375,9 +399,11 @@ struct TritonAtomicRMWPattern\n   LogicalResult\n   matchAndRewrite(triton::AtomicRMWOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<triton::AtomicRMWOp>(\n-        op, typeConverter->convertType(op.getType()), adaptor.getAtomicRmwOp(),\n-        adaptor.getPtr(), adaptor.getVal(), adaptor.getMask());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::AtomicRMWOp>(\n+                      op, typeConverter->convertType(op.getType()),\n+                      adaptor.getAtomicRmwOp(), adaptor.getPtr(),\n+                      adaptor.getVal(), adaptor.getMask()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -389,9 +415,11 @@ struct TritonExtElemwisePattern\n   LogicalResult\n   matchAndRewrite(triton::ExtElemwiseOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<triton::ExtElemwiseOp>(\n-        op, typeConverter->convertType(op.getType()), adaptor.getArgs(),\n-        adaptor.getLibname(), adaptor.getLibpath(), adaptor.getSymbol());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::ExtElemwiseOp>(\n+                      op, typeConverter->convertType(op.getType()),\n+                      adaptor.getArgs(), adaptor.getLibname(),\n+                      adaptor.getLibpath(), adaptor.getSymbol()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -404,7 +432,9 @@ struct TritonGenericPattern : public OpConversionPattern<Op> {\n   matchAndRewrite(Op op, typename Op::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    rewriter.replaceOpWithNewOp<Op>(op, retType, adaptor.getOperands());\n+    addNamedAttrs(\n+        rewriter.replaceOpWithNewOp<Op>(op, retType, adaptor.getOperands()),\n+        adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -425,8 +455,9 @@ struct TritonBroadcastPattern\n     Type retType = RankedTensorType::get(opType.getShape(),\n                                          opType.getElementType(), srcEncoding);\n     // Type retType = this->getTypeConverter()->convertType(op.getType());\n-    rewriter.replaceOpWithNewOp<triton::BroadcastOp>(op, retType,\n-                                                     adaptor.getOperands());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::BroadcastOp>(\n+                      op, retType, adaptor.getOperands()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -437,20 +468,25 @@ struct TritonReducePattern : public OpConversionPattern<triton::ReduceOp> {\n   LogicalResult\n   matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<triton::ReduceOp>(\n-        op, adaptor.getRedOp(), adaptor.getOperand(), adaptor.getAxis());\n+    addNamedAttrs(\n+        rewriter.replaceOpWithNewOp<triton::ReduceOp>(\n+            op, adaptor.getRedOp(), adaptor.getOperand(), adaptor.getAxis()),\n+        adaptor.getAttributes());\n     return success();\n   }\n };\n \n-struct TritonPrintfPattern : public OpConversionPattern<triton::PrintOp> {\n+struct TritonPrintPattern : public OpConversionPattern<triton::PrintOp> {\n   using OpConversionPattern<triton::PrintOp>::OpConversionPattern;\n \n   LogicalResult\n   matchAndRewrite(triton::PrintOp op, typename triton::PrintOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     rewriter.replaceOpWithNewOp<triton::PrintOp>(op, op.getPrefixAttr(),\n                                                  adaptor.getOperands());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::PrintOp>(\n+                      op, op.getPrefixAttr(), adaptor.getOperands()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -465,6 +501,10 @@ struct TritonAssertPattern : public OpConversionPattern<triton::AssertOp> {\n     rewriter.replaceOpWithNewOp<triton::AssertOp>(\n         op, adaptor.getCondition(), op.getMessageAttr(), op.getFileAttr(),\n         op.getFuncAttr(), op.getLineAttr());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::AssertOp>(\n+                      op, adaptor.getCondition(), op.getMessageAttr(),\n+                      op.getFileAttr(), op.getFuncAttr(), op.getLineAttr()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -483,7 +523,7 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n           TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n           TritonReducePattern, TritonTransPattern, TritonExpandDimsPattern,\n           TritonMakeRangePattern, TritonDotPattern, TritonLoadPattern,\n-          TritonStorePattern, TritonExtElemwisePattern, TritonPrintfPattern,\n+          TritonStorePattern, TritonExtElemwisePattern, TritonPrintPattern,\n           TritonAssertPattern, TritonAtomicRMWPattern>(typeConverter, context);\n }\n \n@@ -543,7 +583,9 @@ struct SCFYieldPattern : public OpConversionPattern<scf::YieldOp> {\n     // rewriter.setInsertionPointToEnd(rewriter.getInsertionBlock());\n     // rewriter.create<scf::YieldOp>(op.getLoc(), adaptor.getOperands());\n     // op.erase();\n-    rewriter.replaceOpWithNewOp<scf::YieldOp>(op, adaptor.getOperands());\n+    addNamedAttrs(\n+        rewriter.replaceOpWithNewOp<scf::YieldOp>(op, adaptor.getOperands()),\n+        adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -648,8 +690,9 @@ class CFBranchPattern : public OpConversionPattern<cf::BranchOp> {\n   LogicalResult\n   matchAndRewrite(cf::BranchOp op, cf::BranchOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<cf::BranchOp>(op, op.getSuccessor(),\n-                                              adaptor.getOperands());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<cf::BranchOp>(\n+                      op, op.getSuccessor(), adaptor.getOperands()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -666,6 +709,7 @@ class CFCondBranchPattern : public OpConversionPattern<cf::CondBranchOp> {\n         op, adaptor.getCondition(), op.getTrueDest(),\n         adaptor.getTrueDestOperands(), op.getFalseDest(),\n         adaptor.getFalseDestOperands());\n+    addNamedAttrs(newOp, adaptor.getAttributes());\n \n     if (failed(rewriter.convertRegionTypes(newOp.getTrueDest()->getParent(),\n                                            *converter)))"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 25, "deletions": 2, "changes": 27, "file_content_changes": "@@ -851,7 +851,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n     return mlir::success();\n   }\n   // cvt(extract_slice(x), type2) -> extract_slice(cvt(x, type2))\n-  auto extract_slice = dyn_cast<tensor::ExtractSliceOp>(arg);\n+  auto extract_slice = dyn_cast<triton::gpu::ExtractSliceOp>(arg);\n   if (extract_slice) {\n     if (!isSharedEncoding(op->getResult(0))) {\n       return mlir::failure();\n@@ -872,7 +872,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n     rewriter.setInsertionPoint(extract_slice);\n     auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         op->getLoc(), newType, extract_slice.getSource());\n-    rewriter.replaceOpWithNewOp<tensor::ExtractSliceOp>(\n+    rewriter.replaceOpWithNewOp<triton::gpu::ExtractSliceOp>(\n         op, resType, newArg.getResult(), extract_slice.offsets(),\n         extract_slice.sizes(), extract_slice.strides(),\n         extract_slice.static_offsets(), extract_slice.static_sizes(),\n@@ -925,6 +925,29 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n \n //===----------------------------------------------------------------------===//\n \n+/// Build an ExtractSliceOp with mixed static and dynamic entries and custom\n+/// result type. If the type passed is nullptr, it is inferred.\n+void ExtractSliceOp::build(OpBuilder &b, OperationState &result,\n+                           RankedTensorType resultType, Value source,\n+                           ArrayRef<OpFoldResult> offsets,\n+                           ArrayRef<OpFoldResult> sizes,\n+                           ArrayRef<OpFoldResult> strides,\n+                           ArrayRef<NamedAttribute> attrs) {\n+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;\n+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;\n+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);\n+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);\n+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);\n+  auto sourceRankedTensorType = source.getType().cast<RankedTensorType>();\n+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,\n+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),\n+        b.getDenseI64ArrayAttr(staticSizes),\n+        b.getDenseI64ArrayAttr(staticStrides));\n+  result.addAttributes(attrs);\n+}\n+\n+//===----------------------------------------------------------------------===//\n+\n void TritonGPUDialect::initialize() {\n   addAttributes<\n #define GET_ATTRDEF_LIST"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "file_content_changes": "@@ -394,7 +394,7 @@ void LoopPipeliner::emitPrologue() {\n     sliceType = RankedTensorType::get({bufferShape[1], bufferShape[2]},\n                                       sliceType.getElementType(),\n                                       loadsBufferType[loadOp].getEncoding());\n-    Value extractSlice = builder.create<tensor::ExtractSliceOp>(\n+    Value extractSlice = builder.create<triton::gpu::ExtractSliceOp>(\n         loadOp.getLoc(), sliceType, loadStageBuffer[loadOp][numStages - 1],\n         SmallVector<OpFoldResult>{int_attr(0), int_attr(0), int_attr(0)},\n         SmallVector<OpFoldResult>{int_attr(1),\n@@ -532,8 +532,6 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   Value extractSliceIndex = builder.create<arith::RemSIOp>(\n       nextIV.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n-  extractSliceIndex = builder.create<arith::IndexCastOp>(\n-      extractSliceIndex.getLoc(), builder.getIndexType(), extractSliceIndex);\n \n   for (Operation *op : orderedDeps)\n     if (!loads.contains(op->getResult(0))) {\n@@ -591,7 +589,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n                                         sliceType.getElementType(),\n                                         loadsBufferType[loadOp].getEncoding());\n \n-      nextOp = builder.create<tensor::ExtractSliceOp>(\n+      nextOp = builder.create<triton::gpu::ExtractSliceOp>(\n           op->getLoc(), sliceType, insertAsyncOp,\n           SmallVector<OpFoldResult>{extractSliceIndex, int_attr(0),\n                                     int_attr(0)},"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "file_content_changes": "@@ -103,11 +103,9 @@ Value Prefetcher::generatePrefetch(Value v, unsigned opIdx, bool isPrologue,\n   if (offsetK)\n     offset[kIdx] = *offsetK;\n \n-  Value newSmem = builder.create<tensor::ExtractSliceOp>(\n-      v.getLoc(),\n-      // TODO: encoding?\n-      RankedTensorType::get(shape, elementType, type.getEncoding()), v,\n-      SmallVector<OpFoldResult>{intAttr(offset[0]), intAttr(offset[1])},\n+  Value newSmem = builder.create<triton::gpu::ExtractSliceOp>(\n+      v.getLoc(), RankedTensorType::get(shape, elementType, type.getEncoding()),\n+      v, SmallVector<OpFoldResult>{intAttr(offset[0]), intAttr(offset[1])},\n       SmallVector<OpFoldResult>{intAttr(shape[0]), intAttr(shape[1])},\n       SmallVector<OpFoldResult>{intAttr(1), intAttr(1)});\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 22, "deletions": 22, "changes": 44, "file_content_changes": "@@ -174,35 +174,35 @@ inline bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n   // same\n   if (isSingleValue(op->getOperand(0)))\n     return false;\n-  auto ptr = op->getOperand(0);\n-  // Case 2: We assume that `evict_last` loads/stores have high hit rate\n-  if (auto load = dyn_cast<triton::LoadOp>(op))\n-    if (load.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n-      return false;\n-  if (auto store = dyn_cast<triton::StoreOp>(op))\n-    if (store.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n-      return false;\n-  if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n-    auto encoding = tensorTy.getEncoding();\n-    // Case 3: Different type conversion is expensive (e.g., mma <-> block)\n-    if (encoding.getTypeID() != targetEncoding.getTypeID())\n-      return true;\n-    auto sizePerThread = triton::gpu::getSizePerThread(encoding);\n-    auto targetSizePerThread = triton::gpu::getSizePerThread(targetEncoding);\n-    auto order = triton::gpu::getOrder(encoding);\n-    auto targetOrder = triton::gpu::getOrder(targetEncoding);\n-    // Case 4: The targeEncoding may expose more vectorization opportunities\n-    return sizePerThread[order[0]] >= targetSizePerThread[targetOrder[0]];\n-  }\n-  return false;\n+  // auto ptr = op->getOperand(0);\n+  //// Case 2: We assume that `evict_last` loads/stores have high hit rate\n+  // if (auto load = dyn_cast<triton::LoadOp>(op))\n+  //   if (load.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n+  //     return false;\n+  // if (auto store = dyn_cast<triton::StoreOp>(op))\n+  //   if (store.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n+  //     return false;\n+  // if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n+  //   auto encoding = tensorTy.getEncoding();\n+  //   // Case 3: Different type conversion is expensive (e.g., mma <-> block)\n+  //   if (encoding.getTypeID() != targetEncoding.getTypeID())\n+  //     return true;\n+  //   auto sizePerThread = triton::gpu::getSizePerThread(encoding);\n+  //   auto targetSizePerThread = triton::gpu::getSizePerThread(targetEncoding);\n+  //   auto order = triton::gpu::getOrder(encoding);\n+  //   auto targetOrder = triton::gpu::getOrder(targetEncoding);\n+  //   // Case 4: The targeEncoding may expose more vectorization opportunities\n+  //   return sizePerThread[order[0]] >= targetSizePerThread[targetOrder[0]];\n+  // }\n+  return true;\n }\n \n inline bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   if (!op)\n     return true;\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n     return expensiveLoadOrStore(op, targetEncoding);\n-  if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n+  if (isa<triton::gpu::ExtractSliceOp, triton::gpu::AllocTensorOp,\n           triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n           triton::AtomicCASOp, triton::DotOp>(op))\n     return true;"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -12,6 +12,7 @@\n #include \"mlir/Target/LLVMIR/Export.h\"\n #include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n #include \"mlir/Transforms/Passes.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Tools/Sys/GetEnv.hpp\"\n #include \"llvm/ADT/APInt.h\"\n@@ -294,12 +295,15 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n       /*printAfterOnlyOnChange=*/true,\n       /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n+  pm.addPass(mlir::createConvertSCFToCFPass());\n+  pm.addPass(createTritonConvertArithToIndexPass());\n+  pm.addPass(mlir::createConvertIndexToLLVMPass());\n   pm.addPass(createConvertTritonGPUToLLVMPass(computeCapability));\n-  // Canonicalize to eliminate the remaining UnrealizedConversionCastOp\n+  pm.addPass(mlir::createArithToLLVMConversionPass());\n   pm.addPass(mlir::createCanonicalizerPass());\n-  pm.addPass(mlir::createCSEPass()); // Simplify the IR to improve readability.\n+  // Simplify the IR\n+  pm.addPass(mlir::createCSEPass());\n   pm.addPass(mlir::createSymbolDCEPass());\n-  pm.addPass(mlir::createCanonicalizerPass());\n \n   if (failed(pm.run(module))) {\n     llvm::errs() << \"Pass execution failed\";"}, {"filename": "python/examples/copy_strided.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -15,5 +15,5 @@ def kernel(X, stride_xm,\n     tl.store(Zs, tl.load(Xs))\n \n \n-ret = triton.compile(kernel, signature=\"*fp32,i32,*fp32,i32\", constants={\"BLOCK_M\": 64, \"BLOCK_N\": 64}, output=\"ttgir\")\n-print(ret)\n+ret = triton.compile(kernel, signature=\"*fp32,i32,*fp32,i32\", constants={\"BLOCK_M\": 64, \"BLOCK_N\": 64})\n+print(ret.asm[\"ttgir\"])"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 49, "deletions": 21, "changes": 70, "file_content_changes": "@@ -13,6 +13,8 @@\n \n #include \"mlir/Dialect/ControlFlow/IR/ControlFlow.h\"\n #include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n+#include \"mlir/Dialect/Index/IR/IndexDialect.h\"\n+#include \"mlir/Dialect/Index/IR/IndexOps.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"triton/Analysis/Allocation.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n@@ -44,6 +46,7 @@\n #include <pybind11/stl.h>\n #include <pybind11/stl_bind.h>\n #include <regex>\n+#include <signal.h>\n #include <sstream>\n #include <stdexcept>\n #include <string>\n@@ -118,6 +121,7 @@ void init_triton_ir(py::module &&m) {\n       .def(py::init<>())\n       .def(\"load_triton\", [](mlir::MLIRContext &self) {\n         self.getOrLoadDialect<mlir::triton::TritonDialect>();\n+        self.getOrLoadDialect<mlir::index::IndexDialect>();\n         // we load LLVM because the frontend uses LLVM.undef for\n         // some placeholders\n         self.getOrLoadDialect<mlir::triton::TritonDialect>();\n@@ -392,8 +396,8 @@ void init_triton_ir(py::module &&m) {\n         registry.insert<mlir::triton::TritonDialect,\n                         mlir::triton::gpu::TritonGPUDialect,\n                         mlir::math::MathDialect, mlir::arith::ArithDialect,\n-                        mlir::func::FuncDialect, mlir::scf::SCFDialect,\n-                        mlir::cf::ControlFlowDialect>();\n+                        mlir::index::IndexDialect, mlir::func::FuncDialect,\n+                        mlir::scf::SCFDialect, mlir::cf::ControlFlowDialect>();\n         context.appendDialectRegistry(registry);\n         context.loadAllAvailableDialects();\n \n@@ -511,6 +515,12 @@ void init_triton_ir(py::module &&m) {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n                  loc, v, self.getI8Type()));\n            })\n+      .def(\"get_int16\",\n+           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 loc, v, self.getI16Type()));\n+           })\n       .def(\"get_int32\",\n            [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n@@ -523,16 +533,15 @@ void init_triton_ir(py::module &&m) {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n                  loc, v, self.getI64Type()));\n            })\n-      // bfloat16 cannot be initialized as it is treated as int16 for now\n-      //.def(\"get_bf16\",\n-      //     [](mlir::OpBuilder &self, float v) -> mlir::Value {\n-      //       auto loc = self.getUnknownLoc();\n-      //       auto type = self.getBF16Type();\n-      //       return self.create<mlir::arith::ConstantFloatOp>(\n-      //           loc,\n-      //           mlir::APFloat(type.getFloatSemantics(), std::to_string(v)),\n-      //           type);\n-      //     })\n+      .def(\"get_bf16\",\n+           [](mlir::OpBuilder &self, float v) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             auto type = self.getBF16Type();\n+             return self.create<mlir::arith::ConstantFloatOp>(\n+                 loc,\n+                 mlir::APFloat(type.getFloatSemantics(), std::to_string(v)),\n+                 type);\n+           })\n       .def(\"get_fp16\",\n            [](mlir::OpBuilder &self, float v) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n@@ -545,6 +554,12 @@ void init_triton_ir(py::module &&m) {\n              return self.create<mlir::arith::ConstantOp>(\n                  loc, self.getF32FloatAttr(v));\n            })\n+      .def(\"get_fp64\",\n+           [](mlir::OpBuilder &self, double v) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<mlir::arith::ConstantOp>(\n+                 loc, self.getF64FloatAttr(v));\n+           })\n       .def(\"get_null_value\",\n            [](mlir::OpBuilder &self, mlir::Type type) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n@@ -806,7 +821,7 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::OpBuilder &self, mlir::Value &input) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::IndexCastOp>(\n-                 loc, self.getI32Type(), input);\n+                 loc, self.getI64Type(), input);\n            })\n       .def(\"create_fmul\",\n            [](mlir::OpBuilder &self, mlir::Value &lhs,\n@@ -1525,7 +1540,6 @@ void init_triton_translation(py::module &m) {\n           llvm::sys::fs::createTemporaryFile(\"compile-ptx-src\", \"\", fsrc);\n           llvm::sys::fs::createTemporaryFile(\"compile-ptx-log\", \"\", flog);\n           std::string fbin = std::string(fsrc) + \".o\";\n-          llvm::FileRemover srcRemover(fsrc);\n           llvm::FileRemover logRemover(flog);\n           llvm::FileRemover binRemover(fbin);\n           const char *_fsrc = fsrc.c_str();\n@@ -1542,16 +1556,30 @@ void init_triton_translation(py::module &m) {\n \n           err = system(cmd.c_str());\n           if (err != 0) {\n+            err >>= 8;\n             std::ifstream _log(_flog);\n             std::string log(std::istreambuf_iterator<char>(_log), {});\n-            throw std::runtime_error(\"Internal Triton PTX codegen error: \\n\" +\n-                                     log);\n+            if (err == 255) {\n+              throw std::runtime_error(\"Internal Triton PTX codegen error: \\n\" +\n+                                       log);\n+            } else if (err == 128 + SIGSEGV) {\n+              throw std::runtime_error(\"Please run `ptxas \" + fsrc.str().str() +\n+                                       \"` to confirm that this is a \"\n+                                       \"bug in `ptxas`\\n\" +\n+                                       log);\n+            } else {\n+              throw std::runtime_error(\"`ptxas` failed with error code \" +\n+                                       std::to_string(err) + \": \\n\" + log);\n+            }\n+            return {};\n+          } else {\n+            llvm::FileRemover srcRemover(fsrc);\n+            std::ifstream _cubin(_fbin, std::ios::binary);\n+            std::string cubin(std::istreambuf_iterator<char>(_cubin), {});\n+            _cubin.close();\n+            py::bytes bytes(cubin);\n+            return std::move(bytes);\n           }\n-          std::ifstream _cubin(_fbin, std::ios::binary);\n-          std::string cubin(std::istreambuf_iterator<char>(_cubin), {});\n-          _cubin.close();\n-          py::bytes bytes(cubin);\n-          return std::move(bytes);\n         });\n \n   m.def(\"add_external_libs\","}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 116, "deletions": 6, "changes": 122, "file_content_changes": "@@ -1240,6 +1240,32 @@ def kernel(X, stride_xm, stride_xk,\n         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n \n \n+@pytest.mark.parametrize(\"dtype_str\", int_dtypes + float_dtypes + ['bfloat16'])\n+def test_full(dtype_str):\n+    dtype = getattr(torch, dtype_str)\n+    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+\n+    @triton.jit\n+    def kernel_static(out):\n+        a = GENERATE_TEST_HERE\n+        out_ptr = out + tl.arange(0, 128)[:]\n+        tl.store(out_ptr, a)\n+\n+    @triton.jit\n+    def kernel_dynamic(out, val, dtype: tl.constexpr):\n+        a = tl.full((128,), val, dtype)\n+        out_ptr = out + tl.arange(0, 128)[:]\n+        tl.store(out_ptr, a)\n+\n+    kernel_static_patched = patch_kernel(kernel_static, {'GENERATE_TEST_HERE': f\"tl.full((128,), 2, tl.{dtype_str})\"})\n+    out_static = torch.zeros((128), dtype=dtype, device=\"cuda\")\n+    kernel_static_patched[(1,)](out_static)\n+    out_dynamic = torch.zeros((128), dtype=dtype, device=\"cuda\")\n+    kernel_dynamic[(1,)](out_dynamic, 2, getattr(triton.language, dtype_str))\n+    assert torch.all(out_static == 2)\n+    assert torch.all(out_dynamic == 2)\n+\n+\n # TODO: uncomment once DotOperandEncoding::getElemsPerThread is implemented\n # @pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n # def test_dot_without_load(dtype_str):\n@@ -1409,6 +1435,28 @@ def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n         assert \"ld.global.b32\" in ptx\n     # triton.testing.assert_almost_equal(dst, src[:N])\n \n+\n+@pytest.mark.parametrize(\"has_hints\", [False, True])\n+def test_vectorization_hints(has_hints):\n+    src = torch.empty(1024, device='cuda')\n+    dst = torch.empty(1024, device='cuda')\n+    off = torch.zeros(1, device='cuda', dtype=torch.int32)\n+\n+    @triton.jit\n+    def _kernel(dst, src, off, N, BLOCK_SIZE: tl.constexpr, HINT: tl.constexpr):\n+        offsets = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+        offsets = offsets + tl.load(off)\n+        if HINT:\n+            tl.max_contiguous(tl.multiple_of(offsets, 1024), 1024)\n+        x = tl.load(src + offsets, mask=offsets < N)\n+        tl.store(dst + offsets, x, mask=offsets < N)\n+    pgm = _kernel[(1,)](dst, src, off, N=1024, BLOCK_SIZE=src.shape[0], HINT=has_hints)\n+    ptx = pgm.asm[\"ptx\"]\n+    if has_hints:\n+        assert \"ld.global.v4.b32\" in ptx\n+    else:\n+        assert \"ld.global.v4.b32\" not in ptx\n+\n # ---------------\n # test store\n # ---------------\n@@ -1479,7 +1527,7 @@ def kernel(x):\n \n @pytest.mark.parametrize(\"value, value_type\", [\n     (-1, 'i32'), (0, 'i32'), (-2**31, 'i32'), (2**31 - 1, 'i32'),\n-    (2**31, 'u32'), (2**32 - 1, 'u32'), (2**32, 'i64'), (2**63 - 1, 'i64'),\n+    (2**31, 'i64'), (2**32 - 1, 'i64'), (2**32, 'i64'), (2**63 - 1, 'i64'),\n     (-2**63, 'i64'), (2**63, 'u64'), (2**64 - 1, 'u64')\n ])\n def test_value_specialization(value: int, value_type: str, device='cuda') -> None:\n@@ -1528,7 +1576,7 @@ def kernel(VALUE, X):\n # test constexpr\n # ----------------\n \n-@pytest.mark.parametrize(\"op\", ['+', '-', '*', '/', '%', '<', '>'])\n+@pytest.mark.parametrize(\"op\", ['+', '-', '*', '/', '%', '<', '>', '<<', '>>', '&', '^', '|'])\n @pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n @pytest.mark.parametrize(\"is_rhs_constexpr\", [True, False])\n def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr):\n@@ -1540,11 +1588,17 @@ def kernel(Z, X, Y):\n         z = GENERATE_TEST_HERE\n         tl.store(Z, z)\n \n-    x_str = \"3.14\" if is_lhs_constexpr else \"x\"\n-    y_str = \"4.13\" if is_rhs_constexpr else \"y\"\n+    if op in ['<<', '>>', '&', '^', '|']:  # int op\n+        x_str = \"3\" if is_lhs_constexpr else \"x\"\n+        y_str = \"4\" if is_rhs_constexpr else \"y\"\n+        x = numpy_random((1,), dtype_str=\"int32\")\n+        y = numpy_random((1,), dtype_str=\"int32\")\n+    else:\n+        x_str = \"3.14\" if is_lhs_constexpr else \"x\"\n+        y_str = \"4.13\" if is_rhs_constexpr else \"y\"\n+        x = numpy_random((1,), dtype_str=\"float32\")\n+        y = numpy_random((1,), dtype_str=\"float32\")\n     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{x_str} {op} {y_str}\"})\n-    x = numpy_random((1,), dtype_str=\"float32\")\n-    y = numpy_random((1,), dtype_str=\"float32\")\n     z = np.array(eval(f\"{x_str} {op} {y_str}\"))\n     x_tri = to_triton(x)\n     y_tri = to_triton(y)\n@@ -1733,6 +1787,23 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n # -----------------------\n \n \n+def test_for_iv_int64():\n+\n+    @triton.jit\n+    def kernel(Out, lo, hi):\n+        acc = 0\n+        acc = acc.to(tl.int64)\n+        for i in range(lo, hi):\n+            acc += i\n+        tl.store(Out, acc)\n+\n+    lo = 2**35\n+    hi = 2**35 + 20\n+    out = to_triton(np.zeros((1,), dtype=np.int64), device='cuda')\n+    kernel[(1,)](out, lo, hi)\n+    assert out[0] == sum(range(lo, hi))\n+\n+\n def test_if_else():\n \n     @triton.jit\n@@ -1966,3 +2037,42 @@ def kernel(Input, Index, Out, N: int):\n     Out = torch.empty_like(Index, device='cuda')\n     kernel[(1,)](Input, Index, Out, Index.numel())\n     assert Out.data[0] == 0\n+\n+\n+# This test is used to test our own PTX codegen for float16 and int16 conversions\n+# maybe delete it later after ptxas has been fixed\n+@pytest.mark.parametrize(\"dtype_str\", ['float16', 'int16'])\n+def test_ptx_cast(dtype_str):\n+    @triton.jit\n+    def kernel(in_ptr0, out_ptr2, xnumel, rnumel, dtype: tl.constexpr, XBLOCK: tl.constexpr, RBLOCK: tl.constexpr):\n+        xoffset = tl.program_id(0) * XBLOCK\n+        xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n+        xmask = xindex < xnumel\n+        rbase = tl.arange(0, RBLOCK)[None, :]\n+        x0 = xindex\n+        _tmp4 = (tl.zeros([XBLOCK, RBLOCK], dtype) - 10000).to(dtype)\n+        for roffset in range(0, rnumel, RBLOCK):\n+            rindex = roffset + rbase\n+            rmask = rindex < rnumel\n+            r1 = rindex\n+            tmp0 = tl.load(in_ptr0 + (r1 + (197 * x0)), rmask & xmask).to(dtype)\n+            tmp1 = 2\n+            tmp2 = tmp0 * tmp1\n+            tmp3 = tmp2.to(dtype)\n+            tmp5 = _tmp4 < tmp3\n+            _tmp4 = tl.where(rmask & xmask & tmp5, tmp3, _tmp4)\n+            tl.store(out_ptr2 + (r1 + (197 * x0) + tl.zeros([XBLOCK, RBLOCK], tl.int32)), _tmp4, rmask & xmask)\n+\n+    torch.manual_seed(123)\n+    if dtype_str == 'int16':\n+        torch_dtype = torch.int16\n+        triton_dtype = tl.int32\n+    else:\n+        torch_dtype = torch.float16\n+        triton_dtype = tl.float32\n+\n+    s0 = 4\n+    buf11 = -torch.ones((6 * s0, 197, 197), device='cuda', dtype=torch_dtype)\n+    buf14 = -torch.ones((s0, 6, 197, 197), device='cuda', dtype=torch_dtype)\n+    kernel[(4728,)](buf11, buf14, 1182 * s0, 197, triton_dtype, 1, 256, num_warps=2)\n+    assert buf14.to(torch.float32).mean() == -2.0"}, {"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 0, "deletions": 28, "changes": 28, "file_content_changes": "@@ -1,6 +1,5 @@\n import multiprocessing\n import os\n-import re\n import shutil\n from collections import namedtuple\n \n@@ -107,33 +106,6 @@ def inc_counter(*args, **kwargs):\n     assert counter == target\n \n \n-@pytest.mark.parametrize(\"value, value_type\", [\n-    (-1, 'i32'), (0, 'i32'), (1, 'i32'), (-2**31, 'i32'), (2**31 - 1, 'i32'),\n-    (2**32, 'i64'), (2**63 - 1, 'i64'), (-2**63, 'i64'),\n-    (2**31, 'u32'), (2**32 - 1, 'u32'), (2**63, 'u64'), (2**64 - 1, 'u64')\n-])\n-def test_value_specialization(value: int, value_type: str, device='cuda') -> None:\n-\n-    @triton.jit\n-    def kernel(VALUE, X):\n-        pass\n-\n-    cache_str = None\n-\n-    def get_cache_str(*args, **kwargs):\n-        nonlocal cache_str\n-        cache_str = kwargs[\"repr\"]\n-    triton.JITFunction.cache_hook = get_cache_str\n-    reset_tmp_dir()\n-    x = torch.tensor([3.14159], device='cuda')\n-    kernel[(1, )](value, x)\n-    triton.JITFunction.cache_hook = None\n-\n-    cache_str_match = re.match(r\".*VALUE: (\\w+).*\", cache_str)\n-    spec_type = None if cache_str_match is None else cache_str_match.group(1)\n-    assert spec_type == value_type\n-\n-\n def test_constexpr_not_callable() -> None:\n     @triton.jit\n     def kernel(X, c: tl.constexpr):"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 38, "deletions": 25, "changes": 63, "file_content_changes": "@@ -668,17 +668,24 @@ def visit_For(self, node):\n             step = triton.language.constexpr(-step.value)\n             negative_step = True\n             lb, ub = ub, lb\n+        lb = triton.language.core._to_tensor(lb, self.builder)\n+        ub = triton.language.core._to_tensor(ub, self.builder)\n+        step = triton.language.core._to_tensor(step, self.builder)\n+        # induction variable type\n+        iv_type = triton.language.semantic.integer_promote_impl(lb.dtype, ub.dtype)\n+        iv_type = triton.language.semantic.integer_promote_impl(iv_type, step.dtype)\n+        iv_ir_type = iv_type.to_ir(self.builder)\n         # lb/ub/step might be constexpr, we need to cast them to tensor\n-        lb = triton.language.core._to_tensor(lb, self.builder).handle\n-        ub = triton.language.core._to_tensor(ub, self.builder).handle\n-        step = triton.language.core._to_tensor(step, self.builder).handle\n+        lb = lb.handle\n+        ub = ub.handle\n+        step = step.handle\n         # ForOp can only accept IndexType as lb/ub/step. Cast integer to Index\n         lb = self.builder.create_to_index(lb)\n         ub = self.builder.create_to_index(ub)\n         step = self.builder.create_to_index(step)\n         # Create placeholder for the loop induction variable\n-        iv = self.builder.create_undef(self.builder.get_int32_ty())\n-        self.set_value(node.target.id, triton.language.core.tensor(iv, triton.language.core.int32))\n+        iv = self.builder.create_undef(iv_ir_type)\n+        self.set_value(node.target.id, triton.language.core.tensor(iv, iv_type))\n \n         with enter_sub_region(self) as sr:\n             liveins, insert_block = sr\n@@ -735,11 +742,13 @@ def visit_For(self, node):\n             # update induction variable with actual value, and replace all uses\n             self.builder.set_insertion_point_to_start(for_op.get_body(0))\n             iv = self.builder.create_index_to_si(for_op.get_induction_var())\n+            iv = self.builder.create_int_cast(iv, iv_ir_type, True)\n             if negative_step:\n                 ub_si = self.builder.create_index_to_si(ub)\n+                ub_si = self.builder.create_int_cast(ub_si, iv_ir_type, True)\n                 iv = self.builder.create_sub(ub_si, iv)\n             self.lscope[node.target.id].handle.replace_all_uses_with(iv)\n-            self.set_value(node.target.id, triton.language.core.tensor(iv, triton.language.core.int32))\n+            self.set_value(node.target.id, triton.language.core.tensor(iv, iv_type))\n \n         # update lscope & local_defs (ForOp defines new values)\n         for i, name in enumerate(names):\n@@ -1203,7 +1212,7 @@ def format_of(ty):\n \n #define CUDA_CHECK(ans) {{ gpuAssert((ans), __FILE__, __LINE__); }}\n \n-void _launch(int gridX, int gridY, int gridZ, int num_warps, int shared_memory, CUstream stream, CUfunction function, {arg_decls}) {{\n+static void _launch(int gridX, int gridY, int gridZ, int num_warps, int shared_memory, CUstream stream, CUfunction function, {arg_decls}) {{\n   void *params[] = {{ {', '.join(f\"&arg{i}\" for i in signature.keys() if i not in constants)} }};\n   if(gridX*gridY*gridZ > 0){{\n     CUDA_CHECK(cuLaunchKernel(function, gridX, gridY, gridZ, 32*num_warps, 1, 1, shared_memory, stream, params, 0));\n@@ -1264,16 +1273,13 @@ def format_of(ty):\n   PyObject *launch_enter_hook = NULL;\n   PyObject *launch_exit_hook = NULL;\n   PyObject *compiled_kernel = NULL;\n-  PyObject *hook_ret = NULL;\n   {' '.join([f\"{_extracted_type(ty)} _arg{i}; \" for i, ty in signature.items()])}\n   if(!PyArg_ParseTuple(args, \\\"{format}\\\", &gridX, &gridY, &gridZ, &num_warps, &shared_memory, &_stream, &_function, &launch_enter_hook, &launch_exit_hook, &compiled_kernel, {', '.join(f\"&_arg{i}\" for i, ty in signature.items())})) {{\n     return NULL;\n   }}\n \n   if (launch_enter_hook != Py_None) {{\n-    PyObject *new_args = PyTuple_Pack(1, compiled_kernel);\n-    hook_ret = PyObject_CallObject(launch_enter_hook, new_args);\n-    Py_DECREF(new_args);\n+    PyObject_CallObject(launch_enter_hook, args);\n   }}\n \n \n@@ -1282,19 +1288,9 @@ def format_of(ty):\n   _launch(gridX, gridY, gridZ, num_warps, shared_memory, (CUstream)_stream, (CUfunction)_function, {', '.join(f\"ptr_info{i}.dev_ptr\" if ty[0]==\"*\" else f\"_arg{i}\"for i, ty in signature.items())});\n \n   if (launch_exit_hook != Py_None) {{\n-    PyObject *new_args = NULL;\n-    if (hook_ret) {{\n-        new_args = PyTuple_Pack(2, compiled_kernel, hook_ret);\n-    }} else {{\n-        new_args = PyTuple_Pack(1, compiled_kernel);\n-    }}\n-    hook_ret = PyObject_CallObject(launch_exit_hook, new_args);\n-    Py_DECREF(new_args);\n+    PyObject_CallObject(launch_exit_hook, args);\n   }}\n \n-  if (hook_ret) {{\n-      Py_DECREF(hook_ret);\n-  }}\n   if(PyErr_Occurred()) {{\n     return NULL;\n   }}\n@@ -1561,7 +1557,22 @@ def make_hash(fn, **kwargs):\n }\n \n \n+def _get_jsonable_constants(constants):\n+    def _is_jsonable(x):\n+        try:\n+            json.dumps(x)\n+            return True\n+        except (TypeError, OverflowError):\n+            return False\n+    serialized_constants = {}\n+    for constant in constants:\n+        if _is_jsonable(constants[constant]):\n+            serialized_constants[constant] = constants[constant]\n+    return serialized_constants\n+\n # def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, configs=None):\n+\n+\n def compile(fn, **kwargs):\n     capability = kwargs.get(\"cc\", None)\n     if capability is None:\n@@ -1634,7 +1645,7 @@ def compile(fn, **kwargs):\n         with open(fn_cache_manager._make_path(f\"{name}.json\")) as f:\n             metadata = json.load(f)\n     else:\n-        metadata = {\"num_warps\": num_warps, \"num_stages\": num_stages, \"ctime\": dict()}\n+        metadata = {\"num_warps\": num_warps, \"num_stages\": num_stages, \"constants\": _get_jsonable_constants(constants), \"ctime\": dict()}\n         if ext == \"ptx\":\n             assert \"shared\" in kwargs, \"ptx compilation must provide shared memory size\"\n             metadata[\"shared\"] = kwargs[\"shared\"]\n@@ -1665,7 +1676,7 @@ def compile(fn, **kwargs):\n     # write-back metadata\n     fn_cache_manager.put(json.dumps(metadata), f\"{name}.json\", binary=False)\n     # return handle to compiled kernel\n-    return CompiledKernel(so_path, metadata, asm)\n+    return CompiledKernel(fn, so_path, metadata, asm)\n \n \n class CompiledKernel:\n@@ -1674,17 +1685,19 @@ class CompiledKernel:\n     launch_enter_hook = None\n     launch_exit_hook = None\n \n-    def __init__(self, so_path, metadata, asm):\n+    def __init__(self, fn, so_path, metadata, asm):\n         # initialize launcher\n         import importlib.util\n         spec = importlib.util.spec_from_file_location(\"__triton_launcher\", so_path)\n         mod = importlib.util.module_from_spec(spec)\n+        self.fn = fn\n         spec.loader.exec_module(mod)\n         self.c_wrapper = getattr(mod, \"launch\")\n         # initialize metadata\n         self.shared = metadata[\"shared\"]\n         self.num_warps = metadata[\"num_warps\"]\n         self.num_stages = metadata[\"num_stages\"]\n+        self.constants = metadata[\"constants\"]\n         # initialize asm dict\n         self.asm = asm\n         # binaries are lazily initialized"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 31, "deletions": 0, "changes": 31, "file_content_changes": "@@ -419,6 +419,9 @@ def logical_and(self, other):\n     def __or__(self, other):\n         return constexpr(self.value | other.value)\n \n+    def __xor__(self, other):\n+        return constexpr(self.value ^ other.value)\n+\n     def logical_or(self, other):\n         return constexpr(self.value or other.value)\n \n@@ -535,21 +538,41 @@ def __and__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n         return semantic.and_(self, other, _builder)\n \n+    @builtin\n+    def __rand__(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n+        return semantic.and_(other, self, _builder)\n+\n     @builtin\n     def __or__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n         return semantic.or_(self, other, _builder)\n \n+    @builtin\n+    def __ror__(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n+        return semantic.or_(other, self, _builder)\n+\n     @builtin\n     def __xor__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n         return semantic.xor_(self, other, _builder)\n \n+    @builtin\n+    def __rxor__(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n+        return semantic.xor_(other, self, _builder)\n+\n     @builtin\n     def __lshift__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n         return semantic.shl(self, other, _builder)\n \n+    @builtin\n+    def __rlshift__(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n+        return semantic.shl(other, self, _builder)\n+\n     @builtin\n     def __rshift__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n@@ -558,6 +581,14 @@ def __rshift__(self, other, _builder=None):\n         else:\n             return semantic.lshr(self, other, _builder)\n \n+    @builtin\n+    def __rrshift__(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n+        if self.dtype.is_int_signed():\n+            return semantic.ashr(other, self, _builder)\n+        else:\n+            return semantic.lshr(other, self, _builder)\n+\n     # comparison operators\n \n     # >"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 15, "deletions": 6, "changes": 21, "file_content_changes": "@@ -491,13 +491,22 @@ def arange(start: int, end: int, builder: ir.builder) -> tl.tensor:\n \n \n def full(shape: List[int], value, dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n-    if value == 0:\n-        _value = builder.get_null_value(dtype.to_ir(builder))\n+    if isinstance(value, tl.tensor):\n+        assert value.numel.value == 1, \"only accepts size-1 tensor\"\n+        value = cast(value, dtype, builder)\n+        ret_ty = tl.block_type(value.dtype, shape)\n+        return tl.tensor(builder.create_splat(value.handle, shape), ret_ty)\n     else:\n-        get_value_fn = getattr(builder, f\"get_{dtype.name}\")\n-        _value = get_value_fn(value)\n-    ret_ty = tl.block_type(dtype, shape)\n-    return tl.tensor(builder.create_splat(_value, shape), ret_ty)\n+        # scalar\n+        if value == 0:\n+            value = builder.get_null_value(dtype.to_ir(builder))\n+        else:\n+            get_value_fn = getattr(builder, f\"get_{dtype.name}\")\n+            value = get_value_fn(value)\n+        if dtype is None:\n+            raise ValueError(\"dtype must be specified when value is not a tensor\")\n+        ret_ty = tl.block_type(dtype, shape)\n+        return tl.tensor(builder.create_splat(value, shape), ret_ty)\n \n \n # ===----------------------------------------------------------------------===//"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -125,8 +125,6 @@ def _key_of(arg):\n         elif isinstance(arg, int):\n             if -2**31 <= arg and arg <= 2**31 - 1:\n                 return \"i32\"\n-            elif 2**31 <= arg and arg <= 2**32 - 1:\n-                return \"u32\"\n             elif 2**63 <= arg and arg <= 2**64 - 1:\n                 return \"u64\"\n             else:\n@@ -244,7 +242,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     sig_key =  {sig_keys},\n     constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else ()}\n     spec_key = {f'{spec_keys},' if len(spec_keys) > 0 else ()}\n-    key = (version_key, sig_key, constexpr_key, spec_key)\n+    key = (version_key, sig_key, constexpr_key, spec_key, num_warps, num_stages)\n     if not extern_libs is None:\n       key = (key, tuple(extern_libs.items()))\n     assert num_warps > 0 and (num_warps & (num_warps - 1)) == 0, \"num_warps must be a power of 2\""}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -156,7 +156,7 @@\n \n @triton.autotune(\n     configs=[\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n     ],\n     key=['M', 'N', 'K'],\n )"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -93,9 +93,9 @@ func.func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n func.func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n-  %index = arith.constant 0 : index\n-  // CHECK-NEXT: %extracted_slice -> %cst\n-  %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n+  %index = arith.constant 0 : i32\n+  // CHECK-NEXT: %0 -> %cst\n+  %cst1 = triton_gpu.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n   return\n }\n \n@@ -169,9 +169,9 @@ func.func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   // CHECK-NEXT: %0#2 -> %cst,%cst_0\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     scf.if %i1 {\n-      %index = arith.constant 8 : index\n-      // CHECK-NEXT: %extracted_slice -> %cst,%cst_0\n-      %cst0 = tensor.extract_slice %a_shared[%index, 0][1, 32][1, 1] : tensor<128x32xf16, #A_SHARED> to tensor<32xf16, #A_SHARED>\n+      %index = arith.constant 8 : i32\n+      // CHECK-NEXT: %1 -> %cst,%cst_0\n+      %cst0 = triton_gpu.extract_slice %a_shared[%index, 0][1, 32][1, 1] : tensor<128x32xf16, #A_SHARED> to tensor<32xf16, #A_SHARED>\n       scf.yield\n     }\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -200,8 +200,8 @@ func.func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n func.func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n-  %index = arith.constant 0 : index\n-  %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1,1,1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n+  %index = arith.constant 0 : i32\n+  %cst1 = triton_gpu.extract_slice %cst0[%index, 0, 0][1, 16, 16][1,1,1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 512\n }\n@@ -284,8 +284,8 @@ func.func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f1\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     scf.if %i1 {\n-      %index = arith.constant 8 : index\n-      %cst0 = tensor.extract_slice %a_shared[%index, 0][1, 32][1, 1] : tensor<128x32xf16, #A_SHARED> to tensor<32xf16, #A_SHARED>\n+      %index = arith.constant 8 : i32\n+      %cst0 = triton_gpu.extract_slice %a_shared[%index, 0][1, 32][1, 1] : tensor<128x32xf16, #A_SHARED> to tensor<32xf16, #A_SHARED>\n       scf.yield\n     }\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file --mlir-disable-threading -test-print-membar 2>&1 | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --mlir-disable-threading --convert-scf-to-cf -test-print-membar 2>&1 | FileCheck %s\n \n #AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #sliceAd0 = #triton_gpu.slice<{dim = 0, parent = #AL}>\n@@ -109,8 +109,8 @@ func.func @alloc() {\n // CHECK-LABEL: extract_slice\n func.func @extract_slice() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n-  %index = arith.constant 0 : index\n-  %0 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n+  %index = arith.constant 0 : i32\n+  %0 = triton_gpu.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %1 = triton_gpu.convert_layout %0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -1,7 +1,7 @@\n // RUN: triton-opt %s -split-input-file --convert-triton-gpu-to-llvm | FileCheck %s\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  // CHECK: llvm.func @test_empty_kernel(%arg0: i32, %arg1: !llvm.ptr<f16, 1>)\n+  // CHECK: llvm.func @test_empty_kernel(%arg0: i64, %arg1: !llvm.ptr<f16, 1>)\n   // Here the 128 comes from the 4 in module attribute multiples 32\n   // CHECK:  attributes {nvvm.kernel = 1 : ui1, nvvm.maxntid = [128 : i32]} {{.*}}\n   func.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n@@ -412,9 +412,9 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK-NEXT: llvm.mul\n     // CHECK-NEXT: llvm.add\n     // CHECK-NEXT: llvm.getelementptr\n-    %index = arith.constant 1 : index\n+    %index = arith.constant 1 : i32\n     %0 = triton_gpu.alloc_tensor : tensor<128x16x32xf32, #shared0>\n-    %1 = tensor.extract_slice %0[%index, 0, 0][1, 16, 32][1, 1, 1] : tensor<128x16x32xf32, #shared0> to tensor<16x32xf32, #shared0>\n+    %1 = triton_gpu.extract_slice %0[%index, 0, 0][1, 16, 32][1, 1, 1] : tensor<128x16x32xf32, #shared0> to tensor<16x32xf32, #shared0>\n     return\n   }\n }"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 0, "deletions": 43, "changes": 43, "file_content_changes": "@@ -55,49 +55,6 @@ func.func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   // CHECK: return %6 : tensor<1024xi32, [[$target_layout]]>\n }\n \n-// CHECK-LABEL: remat_load_store\n-func.func @remat_load_store(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n-  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout0>\n-  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout0>\n-  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout0>, tensor<64xi32, #layout0>\n-  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout0>\n-  // CHECK-NOT: triton_gpu.convert_layout\n-  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout0>) -> tensor<64xi32, #layout1>\n-  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout0>) -> tensor<64x!tt.ptr<i32>, #layout1>\n-  tt.store %5, %4 : tensor<64xi32, #layout1>\n-  return\n-}\n-\n-// Don't rematerialize vectorized loads\n-// CHECK-LABEL: remat_expensive\n-func.func @remat_expensive(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n-  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout1>\n-  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout1>\n-  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout1>, tensor<64xi32, #layout1>\n-  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout1>\n-  // CHECK: triton_gpu.convert_layout\n-  // CHECK-NOT: triton_gpu.convert_layout\n-  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout1>) -> tensor<64xi32, #layout0>\n-  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout1>) -> tensor<64x!tt.ptr<i32>, #layout0>\n-  tt.store %5, %4 : tensor<64xi32, #layout0>\n-  return\n-}\n-\n-// Don't rematerialize loads when original and target layouts are different\n-// CHECK-LABEL: remat_multi_layout\n-func.func @remat_multi_layout(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n-  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout0>\n-  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout0>\n-  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout0>, tensor<64xi32, #layout0>\n-  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout0>\n-  // CHECK: triton_gpu.convert_layout\n-  // CHECK-NOT: triton_gpu.convert_layout\n-  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout0>) -> tensor<64xi32, #layout2>\n-  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout0>) -> tensor<64x!tt.ptr<i32>, #layout2>\n-  tt.store %5, %4 : tensor<64xi32, #layout2>\n-  return\n-}\n-\n // Always rematerialize single value loads\n // CHECK-LABEL: remat_single_value\n func.func @remat_single_value(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 13, "deletions": 16, "changes": 29, "file_content_changes": "@@ -29,20 +29,19 @@\n // CHECK-DAG: %[[LOOP_COND_1_SPLAT_B:.*]] = tt.splat %[[LOOP_COND_1]]\n // CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]], %[[LOOP_COND_1_SPLAT_B]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n-// CHECK: %[[A0:.*]] = tensor.extract_slice %[[A1BUFFER]][0, 0, 0]\n-// CHECK: %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n+// CHECK: %[[A0:.*]] = triton_gpu.extract_slice %[[A1BUFFER]][0, 0, 0]\n+// CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:   %[[arg_a0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_a0]]\n // CHECK:   %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n // CHECK:   tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_INT:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.index_cast %[[EXTRACT_INT]] : i32 to index\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:   %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n-// CHECK:   %[[NEXT_A:.*]] = tensor.extract_slice %[[NEXT_A_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n-// CHECK:   %[[NEXT_B:.*]] = tensor.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n+// CHECK:   %[[NEXT_A:.*]] = triton_gpu.extract_slice %[[NEXT_A_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n+// CHECK:   %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:   scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n@@ -101,20 +100,19 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index,\n // CHECK:   %[[A1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n // CHECK:   %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n-// CHECK:   %[[A0:.*]] = tensor.extract_slice %[[A1BUFFER]][0, 0, 0]\n-// CHECK:   %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n+// CHECK:   %[[A0:.*]] = triton_gpu.extract_slice %[[A1BUFFER]][0, 0, 0]\n+// CHECK:   %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK:   scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:     %[[arg_a0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_a0]]\n // CHECK:     %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n // CHECK:     tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_INT:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.index_cast %[[EXTRACT_INT]] : i32 to index\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:     %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:     %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:     triton_gpu.async_wait {num = 2 : i32}\n-// CHECK:   %[[NEXT_A:.*]] = tensor.extract_slice %[[NEXT_A_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n-// CHECK:   %[[NEXT_B:.*]] = tensor.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n+// CHECK:   %[[NEXT_A:.*]] = triton_gpu.extract_slice %[[NEXT_A_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n+// CHECK:   %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n@@ -170,16 +168,15 @@ func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n // CHECK: %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]]\n // CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n // CHECK: triton_gpu.async_wait {num = 1 : i32}\n-// CHECK: %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n+// CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:   %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n // CHECK:   tt.dot {{.*}}, %[[arg_b0_dot_op]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_INT:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.index_cast %[[EXTRACT_INT]] : i32 to index\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:   %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   triton_gpu.async_wait {num = 1 : i32}\n-// CHECK:   %[[NEXT_B:.*]] = tensor.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n+// CHECK:   %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:   scf.yield {{.*}}, {{.*}}, %[[NEXT_B_BUFFER]], %[[NEXT_B]], {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]"}, {"filename": "test/TritonGPU/prefetch.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -12,20 +12,20 @@\n \n \n // CHECK: func.func @matmul_loop\n-// CHECK-DAG: %[[A0_PREFETCH_SMEM:.*]] = tensor.extract_slice %[[A0:.*]][0, 0] [128, 16]\n+// CHECK-DAG: %[[A0_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice %[[A0:.*]][0, 0] [128, 16]\n // CHECK-DAG: %[[A0_PREFETCH:.*]] = triton_gpu.convert_layout %[[A0_PREFETCH_SMEM]]\n-// CHECK-DAG: %[[B0_PREFETCH_SMEM:.*]] = tensor.extract_slice %[[B0:.*]][0, 0] [16, 128]\n+// CHECK-DAG: %[[B0_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice %[[B0:.*]][0, 0] [16, 128]\n // CHECK-DAG: %[[B0_PREFETCH:.*]] = triton_gpu.convert_layout %[[B0_PREFETCH_SMEM]]\n // CHECK:     scf.for {{.*}} iter_args({{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, %[[a0_prefetch:.*]] = %[[A0_PREFETCH]], %[[b0_prefetch:.*]] = %[[B0_PREFETCH]]\n-// CHECK-DAG:   %[[A_REM_SMEM:.*]] = tensor.extract_slice %[[arg_a0]][0, 16] [128, 16]\n+// CHECK-DAG:   %[[A_REM_SMEM:.*]] = triton_gpu.extract_slice %[[arg_a0]][0, 16] [128, 16]\n // CHECK-DAG:   %[[A_REM:.*]] = triton_gpu.convert_layout %[[A_REM_SMEM]]\n-// CHECK-DAG:   %[[B_REM_SMEM:.*]] = tensor.extract_slice %[[arg_b0]][16, 0] [16, 128]\n+// CHECK-DAG:   %[[B_REM_SMEM:.*]] = triton_gpu.extract_slice %[[arg_b0]][16, 0] [16, 128]\n // CHECK-DAG:   %[[B_REM:.*]] = triton_gpu.convert_layout %[[B_REM_SMEM]]\n // CHECK:       %[[D_FIRST:.*]] = tt.dot %[[a0_prefetch]], %[[b0_prefetch:.*]], {{.*}}\n // CHECK:       tt.dot %[[A_REM]], %[[B_REM]], %[[D_FIRST:.*]]\n-// CHECK-DAG:   %[[NEXT_A_PREFETCH_SMEM:.*]] = tensor.extract_slice {{.*}}[0, 0] [128, 16]\n+// CHECK-DAG:   %[[NEXT_A_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice {{.*}}[0, 0] [128, 16]\n // CHECK-DAG:   %[[NEXT_A_PREFETCH:.*]] = triton_gpu.convert_layout %[[NEXT_A_PREFETCH_SMEM]]\n-// CHECK-DAG:   %[[NEXT_B_PREFETCH_SMEM:.*]] = tensor.extract_slice {{.*}}[0, 0] [16, 128]\n+// CHECK-DAG:   %[[NEXT_B_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice {{.*}}[0, 0] [16, 128]\n // CHECK-DAG:   %[[NEXT_B_PREFETCH:.*]] = triton_gpu.convert_layout %[[NEXT_B_PREFETCH_SMEM]]\n // CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_PREFETCH]], %[[NEXT_B_PREFETCH]]\n func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {"}, {"filename": "test/lib/Analysis/TestMembar.cpp", "status": "modified", "additions": 0, "deletions": 12, "changes": 12, "file_content_changes": "@@ -27,18 +27,6 @@ struct TestMembarPass\n     auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n     os << opName << \"\\n\";\n \n-    // Lower the module to the cf dialect\n-    auto *context = operation->getContext();\n-    RewritePatternSet scfPatterns(context);\n-    mlir::populateSCFToControlFlowConversionPatterns(scfPatterns);\n-    mlir::ConversionTarget scfTarget(*context);\n-    scfTarget.addIllegalOp<scf::ForOp, scf::IfOp, scf::ParallelOp, scf::WhileOp,\n-                           scf::ExecuteRegionOp>();\n-    scfTarget.markUnknownOpDynamicallyLegal([](Operation *) { return true; });\n-    if (failed(applyPartialConversion(operation, scfTarget,\n-                                      std::move(scfPatterns))))\n-      return signalPassFailure();\n-\n     // Print all ops after membar pass\n     Allocation allocation(operation);\n     MembarAnalysis membarPass(&allocation);"}]