[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -6319,9 +6319,9 @@ class ConvertTritonGPUToLLVM\n \n   void decomposeInsertSliceAsyncOp(ModuleOp mod,\n                                    TritonGPUToLLVMTypeConverter &converter) {\n-    //// cp.async is supported in Ampere and later\n-    // if (computeCapability >= 80)\n-    //   return;\n+    // cp.async is supported in Ampere and later\n+    if (computeCapability >= 80)\n+      return;\n \n     // insert_slice_async %src, %dst, %idx, %mask, %other\n     // =>"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -65,6 +65,20 @@ func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   return\n }\n \n+// CHECK-LABEL: insert_slice\n+func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n+  %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n+  %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n+  %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n+  // CHECK: %cst_0 -> %cst_0\n+  %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n+  %index = arith.constant 0 : index\n+  %a = tt.load %a_ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #AL>\n+  // CHECK: %3 -> %cst_0\n+  %b = tensor.insert_slice %a into %tensor[%index, 0, 0][1, 16, 16][1, 1, 1]: tensor<16x16xf16, #AL> into tensor<1x16x16xf16, #A_SHARED>\n+  return\n+}\n+\n // CHECK-LABEL: extract_slice\n func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 19, "deletions": 1, "changes": 20, "file_content_changes": "@@ -119,8 +119,26 @@ func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %tensor = triton_gpu.alloc_tensor : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n   %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SHARED>\n+  // CHECK: Membar 6\n   %b = tt.cat %a, %a {axis = 0} : (tensor<1x16x16xf16, #A_SHARED>, tensor<1x16x16xf16, #A_SHARED>) -> tensor<2x16x16xf16, #A_SHARED>\n-  // CHECK: Membar 7\n+  // CHECK: Membar 8\n+  %c = tt.cat %b, %b {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n+  return\n+}\n+\n+// CHECK-LABEL: insert_slice\n+func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n+  %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n+  %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n+  %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n+  %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n+  %index = arith.constant 0 : index\n+  %al = tt.load %a_ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #AL>\n+  // CHECK: Membar 6\n+  %a = tensor.insert_slice %al into %tensor[%index, 0, 0][1, 16, 16][1, 1, 1]: tensor<16x16xf16, #AL> into tensor<1x16x16xf16, #A_SHARED>\n+  // CHECK: Membar 8\n+  %b = tt.cat %a, %a {axis = 0} : (tensor<1x16x16xf16, #A_SHARED>, tensor<1x16x16xf16, #A_SHARED>) -> tensor<2x16x16xf16, #A_SHARED>\n+  // CHECK: Membar 10\n   %c = tt.cat %b, %b {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n   return\n }"}]