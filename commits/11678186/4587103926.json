[{"filename": "python/triton/ops/matmul_perf_model.py", "status": "modified", "additions": 9, "deletions": 7, "changes": 16, "file_content_changes": "@@ -4,23 +4,24 @@\n \n import triton\n import triton._C.libtriton.triton as _triton\n+from triton.runtime.driver.cuda import get_cuda_utils\n from triton.testing import get_dram_gbps, get_max_simd_tflops, get_max_tensorcore_tflops\n \n \n def get_tensorcore_tflops(backend, device, num_ctas, num_warps, dtype):\n     ''' return compute throughput in TOPS '''\n     total_warps = num_ctas * min(num_warps, 4)\n-    triton.compiler.init_cuda_utils()\n-\n-    num_subcores = triton.compiler.cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n+    cuda_utils = get_cuda_utils()\n+    num_subcores = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n     tflops = min(num_subcores, total_warps) / num_subcores * get_max_tensorcore_tflops(dtype, backend, device)\n     return tflops\n \n \n def get_simd_tflops(backend, device, num_ctas, num_warps, dtype):\n     ''' return compute throughput in TOPS '''\n     total_warps = num_ctas * min(num_warps, 4)\n-    num_subcores = triton.compiler.cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n+    cuda_utils = get_cuda_utils()\n+    num_subcores = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n     tflops = min(num_subcores, total_warps) / num_subcores * get_max_simd_tflops(dtype, backend, device)\n     return tflops\n \n@@ -61,7 +62,8 @@ def estimate_matmul_time(\n     compute_ms = total_ops / tput\n \n     # time to load data\n-    num_sm = triton.compiler.cuda_utils.get_device_properties(device)[\"multiprocessor_count\"]\n+    cuda_utils = get_cuda_utils()\n+    num_sm = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"]\n     active_cta_ratio = min(1, num_ctas / num_sm)\n     active_cta_ratio_bw1 = min(1, num_ctas / 32)  # 32 active ctas are enough to saturate\n     active_cta_ratio_bw2 = max(min(1, (num_ctas - 32) / (108 - 32)), 0)  # 32-108, remaining 5%\n@@ -113,8 +115,8 @@ def early_config_prune(configs, named_args):\n             kw['BLOCK_M'], kw['BLOCK_N'], kw['BLOCK_K'], config.num_stages\n \n         # TODO: move to `cuda_utils` submodule\n-        triton.compiler.init_cuda_utils()\n-        max_shared_memory = triton.compiler.cuda_utils.get_device_properties(device)[\"max_shared_mem\"]\n+        cuda_utils = get_cuda_utils()\n+        max_shared_memory = cuda_utils.get_device_properties(device)[\"max_shared_mem\"]\n         required_shared_memory = (BLOCK_M + BLOCK_N) * BLOCK_K * num_stages * dtsize\n         if required_shared_memory <= max_shared_memory:\n             pruned_configs.append(config)"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 10, "deletions": 9, "changes": 19, "file_content_changes": "@@ -4,8 +4,8 @@\n import sys\n from contextlib import contextmanager\n \n-import triton\n import triton._C.libtriton.triton as _triton\n+from .runtime.driver.cuda import get_cuda_utils\n \n \n def nvsmi(attrs):\n@@ -239,8 +239,9 @@ def get_dram_gbps(backend=None, device=None):\n         backend = _triton.runtime.backend.CUDA\n     if not device:\n         device = torch.cuda.current_device()\n-    mem_clock_khz = triton.compiler.cuda_utils.get_device_properties(device)[\"mem_clock_rate\"]  # in kHz\n-    bus_width = triton.compiler.cuda_utils.get_device_properties(device)[\"mem_bus_width\"]\n+    cuda_utils = get_cuda_utils()\n+    mem_clock_khz = cuda_utils.get_device_properties(device)[\"mem_clock_rate\"]  # in kHz\n+    bus_width = cuda_utils.get_device_properties(device)[\"mem_bus_width\"]\n     bw_gbps = mem_clock_khz * bus_width * 2 / 1e6 / 8  # In GB/s\n     return bw_gbps\n \n@@ -252,10 +253,10 @@ def get_max_tensorcore_tflops(dtype, backend=None, device=None, clock_rate=None)\n     if not device:\n         device = torch.cuda.current_device()\n \n-    triton.compiler.init_cuda_utils()\n-    num_subcores = triton.compiler.cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4\n+    cuda_utils = get_cuda_utils()\n+    num_subcores = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4\n     if not clock_rate:\n-        clock_rate = triton.compiler.cuda_utils.get_device_properties(device)[\"sm_clock_rate\"]  # in kHz\n+        clock_rate = cuda_utils.get_device_properties(device)[\"sm_clock_rate\"]  # in kHz\n     capability = torch.cuda.get_device_capability(device)\n     if capability[0] < 8:\n         assert dtype == torch.float16\n@@ -354,9 +355,9 @@ def get_max_simd_tflops(dtype, backend=None, device=None):\n     if not device:\n         device = torch.cuda.current_device()\n \n-    triton.compiler.init_cuda_utils()\n-    num_subcores = triton.compiler.cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4\n-    clock_rate = triton.compiler.cuda_utils.get_device_properties(device)[\"sm_clock_rate\"]  # in kHz\n+    cuda_utils = get_cuda_utils()\n+    num_subcores = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4\n+    clock_rate = cuda_utils.get_device_properties(device)[\"sm_clock_rate\"]  # in kHz\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8:\n         if dtype == torch.float32:"}]