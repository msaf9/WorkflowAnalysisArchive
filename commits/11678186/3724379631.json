[{"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 85, "deletions": 0, "changes": 85, "file_content_changes": "@@ -881,6 +881,81 @@ class BlockedToMMA : public mlir::RewritePattern {\n   }\n };\n \n+class UpdateMMAVersionMinorForVolta : public mlir::RewritePattern {\n+public:\n+  explicit UpdateMMAVersionMinorForVolta(mlir::MLIRContext *context)\n+      : RewritePattern(triton::DotOp::getOperationName(), 1, context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    using triton::gpu::ConvertLayoutOp;\n+    using triton::gpu::DotOperandEncodingAttr;\n+    using triton::gpu::MmaEncodingAttr;\n+\n+    // Prerequirement: DotOp can't have $c operand, or this pattern cannot works\n+    // Find the DotOp, get the MMA layout and check if it needs to update\n+    // Update steps:\n+    //   1. Create a new MMA layout with the updated versionMinor,\n+    //   2. Add a ConvertLayoutOp to $a, $b and $c with the new MMA layout,\n+    //   3. Replace the DotOp with a new one holding the new MMA layout.\n+    auto dotOp = cast<triton::DotOp>(op);\n+    auto *ctx = dotOp->getContext();\n+    auto AT = dotOp.a().getType().cast<RankedTensorType>();\n+    auto BT = dotOp.b().getType().cast<RankedTensorType>();\n+    auto DT = dotOp.d().getType().cast<RankedTensorType>();\n+    auto mmaLayout = DT.getEncoding().cast<MmaEncodingAttr>();\n+    if (!mmaLayout.isVolta())\n+      return failure();\n+    // NOTE Should run after the BlockedToMMA pattern.\n+    auto dotOperandA = AT.getEncoding().cast<DotOperandEncodingAttr>();\n+    auto dotOperandB = BT.getEncoding().cast<DotOperandEncodingAttr>();\n+    // NOTE Should run after OptimizeConvertToDotOperand pattern.\n+    bool isARow = dotOperandA.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    bool isBRow = dotOperandB.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    auto [isARow_, isBRow_, isAVec4, isBVec4] =\n+        mmaLayout.decodeVoltaLayoutStates();\n+    if (isARow_ == isARow && isBRow_ == isBRow) {\n+      return failure(); // No need to update\n+    }\n+\n+    auto newMmaLayout = MmaEncodingAttr::get(\n+        ctx, mmaLayout.getVersionMajor(), mmaLayout.getWarpsPerCTA(),\n+        AT.getShape(), BT.getShape(), isARow, isBRow);\n+\n+    auto newDotOperandA = DotOperandEncodingAttr::get(\n+        ctx, dotOperandA.getOpIdx(), newMmaLayout, dotOperandA.getIsMMAv1Row());\n+    auto newDotOperandB = DotOperandEncodingAttr::get(\n+        ctx, dotOperandB.getOpIdx(), newMmaLayout, dotOperandB.getIsMMAv1Row());\n+    auto newATy = RankedTensorType::get(AT.getShape(), AT.getElementType(),\n+                                        newDotOperandA);\n+    auto newBTy = RankedTensorType::get(BT.getShape(), BT.getElementType(),\n+                                        newDotOperandB);\n+\n+    auto newA =\n+        rewriter.create<ConvertLayoutOp>(dotOp->getLoc(), newATy, dotOp.a())\n+            .getResult();\n+    auto newB =\n+        rewriter.create<ConvertLayoutOp>(dotOp->getLoc(), newBTy, dotOp.b())\n+            .getResult();\n+    Value newC;\n+    if (dotOp.c()) {\n+      auto CT = dotOp.c().getType().cast<RankedTensorType>();\n+      auto newCTy = RankedTensorType::get(CT.getShape(), CT.getElementType(),\n+                                          newMmaLayout);\n+      newC =\n+          rewriter.create<ConvertLayoutOp>(dotOp->getLoc(), newCTy, dotOp.c())\n+              .getResult();\n+    }\n+\n+    auto newDotRetTy =\n+        RankedTensorType::get(DT.getShape(), DT.getElementType(), newMmaLayout);\n+    rewriter.replaceOpWithNewOp<triton::DotOp>(dotOp, newDotRetTy, newA, newB,\n+                                               newC, dotOp.allowTF32());\n+    return success();\n+  }\n+};\n+\n class FixupLoop : public mlir::RewritePattern {\n \n public:\n@@ -954,6 +1029,16 @@ class TritonGPUCombineOpsPass\n       signalPassFailure();\n     }\n \n+    mlir::RewritePatternSet updateMMAVersionMinorForVolta(context);\n+    updateMMAVersionMinorForVolta\n+        .add<UpdateMMAVersionMinorForVolta, SimplifyConversion>(context);\n+\n+    if (applyPatternsAndFoldGreedily(m,\n+                                     std::move(updateMMAVersionMinorForVolta))\n+            .failed()) {\n+      signalPassFailure();\n+    }\n+\n     mlir::RewritePatternSet loopFixup(context);\n     loopFixup.add<FixupLoop>(context);\n     if (applyPatternsAndFoldGreedily(m, std::move(loopFixup)).failed()) {"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 29, "deletions": 1, "changes": 30, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -tritongpu-combine 2>&1 | FileCheck %s\n+// RUN: triton-opt -split-input-file %s -tritongpu-combine 2>&1 | FileCheck %s\n \n #layout0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n #layout1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n@@ -184,3 +184,31 @@ func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f3\n   tt.store %21, %22 : tensor<256xf32, #layout1>\n   return\n }\n+\n+// -----\n+\n+// check the UpdateMMAVersionMinorForVolta pattern\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n+#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[1,1]}>\n+// Here, the isMMAv1Row of a and b's dot_operands mismatch #mma0's versionMinor,\n+// and the pattern should update the versionMinor.\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n+// It creates a new MMA layout to fit with $a and $b's dot_operand\n+// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 11, warpsPerCTA = [1, 1]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: dot_mmav1\n+  func @dot_mmav1(%A: tensor<16x16xf16, #blocked0>, %B: tensor<16x16xf16, #blocked0>) -> tensor<16x16xf32, #blocked0> {\n+    %C = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked0>\n+    %AA = triton_gpu.convert_layout %A : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #dot_operand_a>\n+    %BB = triton_gpu.convert_layout %B : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #dot_operand_b>\n+    %CC = triton_gpu.convert_layout %C : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #mma0>\n+\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, %cst {allowTF32 = true} : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<16x16xf32, [[new_mma]]>\n+    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<16x16xf16, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n+    %res = triton_gpu.convert_layout %D : (tensor<16x16xf32, #mma0>) -> tensor<16x16xf32, #blocked0>\n+\n+    return %res : tensor<16x16xf32, #blocked0>\n+  }\n+}"}]