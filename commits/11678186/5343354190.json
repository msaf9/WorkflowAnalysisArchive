[{"filename": ".github/workflows/wheels.yml", "status": "modified", "additions": 42, "deletions": 17, "changes": 59, "file_content_changes": "@@ -2,7 +2,7 @@ name: Wheels\n on:\n   workflow_dispatch:\n   schedule:\n-    - cron: \"0 2 * * *\"\n+    - cron: \"20 2 * * *\"\n \n jobs:\n \n@@ -18,25 +18,20 @@ jobs:\n       - name: Checkout\n         uses: actions/checkout@v3\n \n-      - name: Install Azure CLI\n+      # The LATEST_DATE here should be kept in sync with the one in Patch setup.py\n+      - id: check-version\n+        name: Check latest version\n         run: |\n-          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n-\n-      - name: Azure login\n-        uses: azure/login@v1\n-        with:\n-          client-id: ${{ secrets.AZURE_CLIENT_ID }}\n-          tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n-          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n-\n-      - id: generate-token\n-        name: Generate token\n-        run: |\n-          AZ_TOKEN=$(az account get-access-token --query accessToken)\n-          echo \"::add-mask::$AZ_TOKEN\"\n-          echo \"access_token=$AZ_TOKEN\" >> \"$GITHUB_OUTPUT\"\n+          export PACKAGE_DATE=$(python3 -m pip install --user --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ --dry-run triton-nightly== |& grep -oP '(?<=, )[0-9\\.]+dev[0-9]+(?=\\))' | grep -oP '(?<=dev)[0-9]+')\n+          export LATEST_DATE=$(TZ=UTC0 git show --quiet --date='format-local:%Y%m%d%H%M%S' --format=\"%cd\")\n+          if cmp -s <(echo $PACKAGE_DATE) <(echo $LATEST_DATE); then\n+            echo \"new_commit=false\" >> \"$GITHUB_OUTPUT\"\n+          else\n+            echo \"new_commit=true\" >> \"$GITHUB_OUTPUT\"\n+          fi\n \n       - name: Patch setup.py\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n         run: |\n           sed -i 's/name\\=\\\"triton\\\"/name=\"triton-nightly\"/g' python/setup.py\n           export LATEST_DATE=$(TZ=UTC0 git show --quiet --date='format-local:%Y%m%d%H%M%S' --format=\"%cd\")\n@@ -46,6 +41,7 @@ jobs:\n           echo \"base-dir=/project\" >> python/setup.cfg\n \n       - name: Build wheels\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n         run: |\n           export CIBW_MANYLINUX_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n           #export CIBW_MANYLINUX_PYPY_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n@@ -54,6 +50,35 @@ jobs:\n           export CIBW_BUILD=\"{cp,pp}3*-manylinux_x86_64\"\n           python3 -m cibuildwheel python --output-dir wheelhouse\n \n+      - name: Install Azure CLI\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n+        run: |\n+          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n+\n+      - name: Azure login\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n+        uses: azure/login@v1\n+        with:\n+          client-id: ${{ secrets.AZURE_CLIENT_ID }}\n+          tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n+          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n+\n+      - id: generate-token\n+        name: Generate token\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n+        run: |\n+          AZ_TOKEN=$(az account get-access-token --query accessToken)\n+          echo \"::add-mask::$AZ_TOKEN\"\n+          echo \"access_token=$AZ_TOKEN\" >> \"$GITHUB_OUTPUT\"\n+\n       - name: Publish wheels to Azure DevOps\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n         run: |\n           python3 -m twine upload -r Triton-Nightly -u TritonArtifactsSP -p ${{ steps.generate-token.outputs.access_token }} --config-file utils/nightly.pypirc --non-interactive --verbose wheelhouse/*\n+\n+      - name: Azure Logout\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' && (success() || failure()) }}\n+        run: |\n+          az logout\n+          az cache purge\n+          az account clear"}, {"filename": ".hypothesis/unicode_data/13.0.0/charmap.json.gz", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "README.md", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -25,7 +25,7 @@ You can install the latest stable release of Triton from pip:\n ```bash\n pip install triton\n ```\n-Binary wheels are available for CPython 3.8-3.11 and PyPy 3.8-3.9.\n+Binary wheels are available for CPython 3.7-3.11 and PyPy 3.8-3.9.\n \n And the latest nightly release:\n "}, {"filename": "docs/getting-started/installation.rst", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -12,7 +12,7 @@ You can install the latest stable release of Triton from pip:\n \n       pip install triton\n \n-Binary wheels are available for CPython 3.8-3.11 and PyPy 3.8-3.9.\n+Binary wheels are available for CPython 3.7-3.11 and PyPy 3.8-3.9.\n \n And the latest nightly release:\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 30, "deletions": 0, "changes": 30, "file_content_changes": "@@ -1180,6 +1180,35 @@ struct AbsFOpConversion\n   }\n };\n \n+/// The lowering of index_cast becomes an integer conversion since index\n+/// becomes an integer.  If the bit width of the source and target integer\n+/// types is the same, just erase the cast.  If the target type is wider,\n+/// sign-extend the value, otherwise truncate it.\n+struct IndexCastOpLowering\n+    : public ElementwiseOpConversionBase<arith::IndexCastOp,\n+                                         IndexCastOpLowering> {\n+  using Base =\n+      ElementwiseOpConversionBase<arith::IndexCastOp, IndexCastOpLowering>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(arith::IndexCastOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto inElemTy =\n+        this->getTypeConverter()->convertType(getElementType(op.getIn()));\n+    unsigned targetBits = elemTy.getIntOrFloatBitWidth();\n+    unsigned sourceBits = inElemTy.getIntOrFloatBitWidth();\n+\n+    if (targetBits == sourceBits)\n+      return operands[0];\n+    if (targetBits < sourceBits)\n+      return rewriter.replaceOpWithNewOp<LLVM::TruncOp>(op, elemTy,\n+                                                        operands[0]);\n+    return rewriter.replaceOpWithNewOp<LLVM::SExtOp>(op, elemTy, operands[0]);\n+  }\n+};\n+\n void populateElementwiseOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     PatternBenefit benefit) {\n@@ -1240,6 +1269,7 @@ void populateElementwiseOpToLLVMPatterns(\n   patterns.add<TruncFOpConversion>(typeConverter, benefit);\n   patterns.add<FPToSIOpConversion>(typeConverter, benefit);\n   patterns.add<SIToFPOpConversion>(typeConverter, benefit);\n+  patterns.add<IndexCastOpLowering>(typeConverter, benefit);\n \n   patterns.add<FpToFpOpConversion>(typeConverter, benefit);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -388,7 +388,7 @@ struct GetProgramIdOpConversion\n \n     Value blockId =\n         rewriter.create<::mlir::gpu::BlockIdOp>(loc, dims[op.getAxisAsInt()]);\n-    rewriter.replaceOpWithNewOp<arith::TruncIOp>(op, i32_ty, blockId);\n+    rewriter.replaceOpWithNewOp<arith::IndexCastOp>(op, i32_ty, blockId);\n     return success();\n   }\n \n@@ -410,7 +410,7 @@ struct GetNumProgramsOpConversion\n \n     Value blockId =\n         rewriter.create<::mlir::gpu::GridDimOp>(loc, dims[op.getAxis()]);\n-    rewriter.replaceOpWithNewOp<arith::TruncIOp>(op, i32_ty, blockId);\n+    rewriter.replaceOpWithNewOp<arith::IndexCastOp>(op, i32_ty, blockId);\n \n     return success();\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -217,10 +217,9 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   }\n \n   Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n-    auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n     auto tid = rewriter.create<::mlir::gpu::ThreadIdOp>(\n-        loc, rewriter.getIndexType(), ::mlir::gpu::Dimension::x);\n-    return rewriter.create<arith::TruncIOp>(loc, i32_ty, tid);\n+        loc, ::mlir::gpu::Dimension::x);\n+    return rewriter.create<arith::IndexCastOp>(loc, i32_ty, tid);\n   }\n \n   // -----------------------------------------------------------------------"}, {"filename": "python/MANIFEST.in", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,4 +1,5 @@\n graft src\n graft triton/third_party\n+graft triton/tools\n graft triton/runtime/backends/\n graft triton/language/extra"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 12, "deletions": 3, "changes": 15, "file_content_changes": "@@ -1374,7 +1374,8 @@ def get_reduced_dtype(dtype_str, op):\n                           for op in ['min', 'max',\n                                      'min-with-indices',\n                                      'max-with-indices',\n-                                     'argmin', 'argmax',\n+                                     'argmin-tie-break-left',\n+                                     'argmax-tie-break-left',\n                                      'sum']\n                           for dtype in dtypes_with_bfloat16\n                           for shape in [32, 64, 128, 512]])\n@@ -1390,18 +1391,26 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n     if 'with-indices' in op:\n         patch = f'z, _ = tl.{op.split(\"-\")[0]}(x, axis=0, return_indices=True)'\n+    elif 'arg' in op:\n+        tie_break_left = 'tie-break-left' in op\n+        patch = f'z = tl.{op.split(\"-\")[0]}(x, axis=0, tie_break_left={tie_break_left})'\n     else:\n         patch = f'z = tl.{op}(x, axis=0)'\n     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': patch})\n     # input\n     rs = RandomState(17)\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random((shape,), dtype_str=dtype_str, rs=rs)\n-    x_tri = to_triton(x, device=device)\n     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n                 'max-with-indices': np.max,\n                 'min-with-indices': np.min,\n-                'argmin': np.argmin, 'argmax': np.argmax}[op]\n+                'argmin-tie-break-fast': np.argmin,\n+                'argmin-tie-break-left': np.argmin,\n+                'argmax-tie-break-fast': np.argmax,\n+                'argmax-tie-break-left': np.argmax}[op]\n+    if 'tie-break-left' in op:\n+        x[3:10] = numpy_op(x)\n+    x_tri = to_triton(x, device=device)\n     # numpy result\n     z_dtype_str = 'int32' if op in ('argmin', 'argmax') else dtype_str\n     z_tri_dtype_str = z_dtype_str"}, {"filename": "python/test/unit/tools/test_aot.py", "status": "added", "additions": 205, "deletions": 0, "changes": 205, "file_content_changes": "@@ -0,0 +1,205 @@\n+import glob\n+import os\n+import subprocess\n+import sys\n+import tempfile\n+\n+import numpy as np\n+\n+import triton\n+from triton.common import cuda_include_dir, libcuda_dirs\n+\n+kernel_utils_src = \"\"\"\n+import triton\n+\n+@triton.jit\n+def mul(x, y):\n+    return x * y\n+\"\"\"\n+\n+kernel_src = \"\"\"\n+import triton\n+import triton.language as tl\n+import kernel_utils\n+\n+@triton.jit\n+def kernel(C, A, B,\n+          stride_cm, stride_cn,\n+          stride_am, stride_ak,\n+          stride_bk, stride_bn,\n+          BLOCK_M: tl.constexpr,\n+          BLOCK_N: tl.constexpr,\n+          BLOCK_K: tl.constexpr):\n+  ms = tl.arange(0, BLOCK_M)\n+  ns = tl.arange(0, BLOCK_N)\n+  ks = tl.arange(0, BLOCK_K)\n+  a = tl.load(A + ms[:, None] * stride_am + ks[None, :] * stride_ak)\n+  b = tl.load(B + ks[:, None] * stride_bk + ns[None, :] * stride_bn)\n+  c = tl.dot(a, b)\n+  c = kernel_utils.mul(c, c)\n+  tl.store(C + ms[:, None] * stride_cm + ns[None, :] * stride_cn, c)\n+\"\"\"\n+\n+test_src = \"\"\"\n+#include <cuda.h>\n+#include <stdio.h>\n+#include <stdint.h>\n+#include <string.h>\n+#include \"kernel.h\"\n+\n+static void write_buffer_to_csv(char *filename, int32_t *buffer, int size) {\n+    FILE *file = fopen(filename, \"w\");\n+    if (file == NULL) {\n+        printf(\\\"Could not open file %s\\\\n\\\", filename);\n+        return;\n+    }\n+    for (int i = 0; i < size; i++) {\n+        fprintf(file, \"%d\", buffer[i]);\n+        if (i < size - 1) {\n+            fprintf(file, \",\");\n+        }\n+    }\n+    fclose(file);\n+}\n+\n+static void read_csv_to_buffer(char *filename, int16_t *buffer, int size) {\n+    FILE *file = fopen(filename, \"r\");\n+    if (file == NULL) {\n+        printf(\\\"Could not open file %s\\\\n\\\", filename);\n+        return;\n+    }\n+    int index = 0;\n+    while (fscanf(file, \"%hd,\", &buffer[index]) != EOF && index < size) {\n+        index++;\n+    }\n+    fclose(file);\n+}\n+\n+int main(int argc, char **argv) {\n+  int M = 16, N = 16, K = 16;\n+  int BM = 16, BN = 16, BK = 16;\n+\n+  // initialize CUDA handles\n+  CUdevice dev;\n+  CUcontext ctx;\n+  CUstream stream;\n+  CUdeviceptr A, B, C;\n+  CUresult err = 0;\n+  cuInit(0);\n+  cuDeviceGet(&dev, 0);\n+  cuCtxCreate(&ctx, 0, dev);\n+  cuMemAlloc(&A, M * K * 2);\n+  cuMemAlloc(&B, K * N * 2);\n+  cuMemAlloc(&C, M * N * 4);\n+  cuStreamCreate(&stream, 0);\n+  load_kernel();\n+\n+  // initialize input data\n+  int16_t hA[M*K];\n+  int16_t hB[K*N];\n+  memset(hA, 0, M*K*2);\n+  memset(hB, 0, K*N*2);\n+  read_csv_to_buffer(argv[1], hA, M*K);\n+  read_csv_to_buffer(argv[2], hB, K*N);\n+  cuMemcpyHtoD(A, hA, M*K*2);\n+  cuMemcpyHtoD(B, hB, K*N*2);\n+\n+  // launch kernel\n+  int numWarps = 1;\n+  int gX = 1, gY = 1, gZ = 1;\n+  cuStreamSynchronize(stream);\n+  kernel(stream, M/BM, N/BN, 1, numWarps, C, A, B, N, K, N);\n+  cuStreamSynchronize(stream);\n+\n+  // read data\n+  int32_t hC[M*N];\n+  memset(hC, 0, M*N*4);\n+  cuMemcpyDtoH(hC, C, M*N*4);\n+  write_buffer_to_csv(argv[3], hC, M*N);\n+\n+\n+  // free cuda handles\n+  unload_kernel();\n+  cuMemFree(A);\n+  cuMemFree(B);\n+  cuMemFree(C);\n+  cuCtxDestroy(ctx);\n+}\n+\"\"\"\n+\n+\n+def test_compile_link_matmul():\n+    np.random.seed(3)\n+\n+    with tempfile.TemporaryDirectory() as tmp_dir:\n+        kernel_path = os.path.join(tmp_dir, \"kernel.py\")\n+        with open(kernel_path, \"w\") as file:\n+            file.write(kernel_src)\n+\n+        kernel_utils_path = os.path.join(tmp_dir, \"kernel_utils.py\")\n+        with open(kernel_utils_path, \"w\") as file:\n+            file.write(kernel_utils_src)\n+\n+        compiler_path = os.path.join(triton.tools.__path__[0], \"compile.py\")\n+        linker_path = os.path.join(triton.tools.__path__[0], \"link.py\")\n+\n+        dtype = \"fp16\"\n+        M, N, K = 16, 16, 16\n+        BM, BN, BK = 16, 16, 16\n+\n+        # compile all desired configs\n+        hints = [\":16\", \"\"]\n+        for ha in hints:\n+            for hb in hints:\n+                sig = f'*fp32:16, *{dtype}:16, *{dtype}:16, i32{ha}, 1, i32{hb}, 1, i32:16, 1, {BM}, {BN}, {BK}'\n+                name = f\"matmul_{dtype}x{dtype}_{BM}x{BN}x{BK}\"\n+                subprocess.run([sys.executable, compiler_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", name, kernel_path], check=True, cwd=tmp_dir)\n+\n+        # link all desired configs\n+        h_files = glob.glob(os.path.join(tmp_dir, \"*.h\"))\n+        subprocess.run([sys.executable, linker_path] + h_files + [\"-o\", \"kernel\"], check=True, cwd=tmp_dir)\n+\n+        # compile test case\n+        with open(os.path.join(tmp_dir, \"test.c\"), \"w\") as file:\n+            file.write(test_src)\n+        c_files = glob.glob(os.path.join(tmp_dir, \"*.c\"))\n+        subprocess.run([\"gcc\"] + c_files + [\"-I\", cuda_include_dir(),\n+                                            \"-L\", libcuda_dirs()[0],\n+                                            \"-l\", \"cuda\",\n+                                            \"-o\", \"test\"], check=True, cwd=tmp_dir)\n+\n+        # initialize test data\n+        a = np.random.randn(M * K).astype(np.float16).reshape((M, K))\n+        b = np.random.randn(M * K).astype(np.float16).reshape((K, N))\n+        a_path = os.path.join(tmp_dir, \"a.csv\")\n+        b_path = os.path.join(tmp_dir, \"b.csv\")\n+        c_path = os.path.join(tmp_dir, \"c.csv\")\n+        for x, path in [(a, a_path), (b, b_path)]:\n+            x.view(np.int16).ravel().tofile(path, sep=\",\")\n+\n+        # run test case\n+        subprocess.run([\"./test\", a_path, b_path, c_path], check=True, cwd=tmp_dir)\n+\n+        # read data and compare against reference\n+        c = np.genfromtxt(c_path, delimiter=\",\", dtype=np.int32)\n+        c_tri = c.reshape((M, N)).view(np.float32)\n+        c_ref = np.matmul(a.astype(np.float32), b.astype(np.float32))\n+        np.testing.assert_allclose(c_tri, c_ref * c_ref, atol=1e-4, rtol=0.)\n+\n+\n+def test_ttgir_to_ptx():\n+    src = \"\"\"\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @sum_kernel_0d1d(%arg0: !tt.ptr<i32>, %arg1: !tt.ptr<i32>) {\n+    tt.return\n+  }\n+}\n+\"\"\"\n+    with tempfile.TemporaryDirectory() as tmp_dir:\n+        kernel_path = os.path.join(tmp_dir, \"empty_kernel.ttgir\")\n+        with open(kernel_path, \"w\") as fp:\n+            fp.write(src)\n+        k = triton.compile(kernel_path, cc=80)\n+        ptx = k.asm[\"ptx\"]\n+        assert \".target sm_80\" in ptx\n+        assert \".address_size 64\" in ptx"}, {"filename": "python/triton/__init__.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -42,6 +42,7 @@\n     \"runtime\",\n     \"TensorWrapper\",\n     \"testing\",\n+    \"tools\",\n ]\n \n "}, {"filename": "python/triton/common/__init__.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,3 +1,3 @@\n-from .build import _build\n+from .build import _build, cuda_include_dir, libcuda_dirs\n \n-__all__ = [\"_build\"]\n+__all__ = [\"_build\", \"libcuda_dirs\", \"cuda_include_dir\"]"}, {"filename": "python/triton/common/build.py", "status": "modified", "additions": 8, "deletions": 9, "changes": 17, "file_content_changes": "@@ -37,21 +37,20 @@ def quiet():\n         sys.stdout, sys.stderr = old_stdout, old_stderr\n \n \n+@functools.lru_cache()\n+def cuda_include_dir():\n+    base_dir = os.path.join(os.path.dirname(__file__), os.path.pardir)\n+    cuda_path = os.path.join(base_dir, \"third_party\", \"cuda\")\n+    return os.path.join(cuda_path, \"include\")\n+\n+\n def _build(name, src, srcdir):\n     if is_hip():\n         hip_lib_dir = os.path.join(rocm_path_dir(), \"lib\")\n         hip_include_dir = os.path.join(rocm_path_dir(), \"include\")\n     else:\n         cuda_lib_dirs = libcuda_dirs()\n-        base_dir = os.path.join(os.path.dirname(__file__), os.path.pardir)\n-        cuda_path = os.path.join(base_dir, \"third_party\", \"cuda\")\n-\n-        cu_include_dir = os.path.join(cuda_path, \"include\")\n-        triton_include_dir = os.path.join(os.path.dirname(__file__), \"include\")\n-        cuda_header = os.path.join(cu_include_dir, \"cuda.h\")\n-        triton_cuda_header = os.path.join(triton_include_dir, \"cuda.h\")\n-        if not os.path.exists(cuda_header) and os.path.exists(triton_cuda_header):\n-            cu_include_dir = triton_include_dir\n+        cu_include_dir = cuda_include_dir()\n     suffix = sysconfig.get_config_var('EXT_SUFFIX')\n     so = os.path.join(srcdir, '{name}{suffix}'.format(name=name, suffix=suffix))\n     # try to avoid setuptools if possible"}, {"filename": "python/triton/compiler/__init__.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,4 +1,4 @@\n-from .compiler import CompiledKernel, compile\n+from .compiler import CompiledKernel, compile, instance_descriptor\n from .errors import CompilationError\n \n-__all__ = [\"compile\", \"CompiledKernel\", \"CompilationError\"]\n+__all__ = [\"compile\", \"instance_descriptor\", \"CompiledKernel\", \"CompilationError\"]"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 66, "deletions": 32, "changes": 98, "file_content_changes": "@@ -1255,15 +1255,21 @@ def abs(x, _builder=None):\n # Reductions\n # -----------------------\n \n-def _add_reduction_docstr(name: str) -> Callable[[T], T]:\n+def _add_reduction_docstr(name: str, return_indices_arg: str = None, tie_break_arg: str = None) -> Callable[[T], T]:\n \n     def _decorator(func: T) -> T:\n         docstr = \"\"\"\n     Returns the {name} of all elements in the :code:`input` tensor along the provided :code:`axis`\n \n     :param input: the input values\n-    :param axis: the dimension along which the reduction should be done\n-    \"\"\"\n+    :param axis: the dimension along which the reduction should be done\"\"\"\n+        if return_indices_arg is not None:\n+            docstr += f\"\"\"\n+    :param {return_indices_arg}: if true, return index corresponding to the {name} value\"\"\"\n+        if tie_break_arg is not None:\n+            docstr += f\"\"\"\n+    :param {tie_break_arg}: if true, return the left-most indices in case of ties for values that aren't NaN\"\"\"\n+\n         func.__doc__ = docstr.format(name=name)\n         return func\n \n@@ -1374,65 +1380,93 @@ def maximum(x, y):\n \n \n @jit\n-def _max_combine(a, b):\n-    return maximum(a, b)\n+def _argmax_combine(value1, index1, value2, index2, tie_break_left):\n+    if tie_break_left:\n+        tie = value1 == value2 and index1 < index2\n+    else:\n+        tie = False\n+    gt = value1 > value2 or tie\n+    v_ret = where(gt, value1, value2)\n+    i_ret = where(gt, index1, index2)\n+    return v_ret, i_ret\n \n \n @jit\n-def _argmax_combine(value1, index1, value2, index2):\n-    gt = value1 > value2\n-    value_ret = where(gt, value1, value2)\n-    index_ret = where(gt, index1, index2)\n-    return value_ret, index_ret\n+def _argmax_combine_tie_break_left(value1, index1, value2, index2):\n+    return _argmax_combine(value1, index1, value2, index2, True)\n \n \n @jit\n-@_add_reduction_docstr(\"maximum\")\n-def max(input, axis=None, return_indices=False):\n+def _argmax_combine_tie_break_fast(value1, index1, value2, index2):\n+    return _argmax_combine(value1, index1, value2, index2, False)\n+\n+\n+@jit\n+@_add_reduction_docstr(\"maximum\",\n+                       return_indices_arg=\"return_indices\",\n+                       tie_break_arg=\"return_indices_tie_break_left\")\n+def max(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n     input = _promote_reduction_input(input)\n     if return_indices:\n-        return _reduce_with_indices(input, axis, _argmax_combine)\n+        if return_indices_tie_break_left:\n+            return _reduce_with_indices(input, axis, _argmax_combine_tie_break_left)\n+        else:\n+            return _reduce_with_indices(input, axis, _argmax_combine_tie_break_fast)\n     else:\n-        return reduce(input, axis, _max_combine)\n+        return reduce(input, axis, maximum)\n \n \n @jit\n-@_add_reduction_docstr(\"maximum index\")\n-def argmax(input, axis):\n-    (_, ret) = max(input, axis, return_indices=True)\n+@_add_reduction_docstr(\"maximum index\", tie_break_arg=\"tie_break_left\")\n+def argmax(input, axis, tie_break_left=True):\n+    (_, ret) = max(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n     return ret\n \n # min and argmin\n \n \n @jit\n-def _min_combine(a, b):\n-    # TODO: minimum/maximum doesn't get lowered to fmin/fmax...\n-    return minimum(a, b)\n-\n-\n-@jit\n-def _argmin_combine(value1, index1, value2, index2):\n-    lt = value1 < value2\n+def _argmin_combine(value1, index1, value2, index2, tie_break_left):\n+    if tie_break_left:\n+        tie = value1 == value2 and index1 < index2\n+    else:\n+        tie = False\n+    lt = value1 < value2 or tie\n     value_ret = where(lt, value1, value2)\n     index_ret = where(lt, index1, index2)\n     return value_ret, index_ret\n \n \n @jit\n-@_add_reduction_docstr(\"minimum\")\n-def min(input, axis=None, return_indices=False):\n+def _argmin_combine_tie_break_left(value1, index1, value2, index2):\n+    return _argmin_combine(value1, index1, value2, index2, True)\n+\n+\n+@jit\n+def _argmin_combine_tie_break_fast(value1, index1, value2, index2):\n+    return _argmin_combine(value1, index1, value2, index2, False)\n+\n+\n+@jit\n+@_add_reduction_docstr(\"minimum\",\n+                       return_indices_arg=\"return_indices\",\n+                       tie_break_arg=\"return_indices_tie_break_left\")\n+def min(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n     input = _promote_reduction_input(input)\n     if return_indices:\n-        return _reduce_with_indices(input, axis, _argmin_combine)\n+        if return_indices_tie_break_left:\n+            return _reduce_with_indices(input, axis, _argmin_combine_tie_break_left)\n+        else:\n+            return _reduce_with_indices(input, axis, _argmin_combine_tie_break_fast)\n     else:\n-        return reduce(input, axis, _min_combine)\n+        return reduce(input, axis, minimum)\n \n \n @jit\n-@_add_reduction_docstr(\"minimum index\")\n-def argmin(input, axis):\n-    _, ret = min(input, axis, return_indices=True)\n+@_add_reduction_docstr(\"minimum index\",\n+                       tie_break_arg=\"tie_break_left\")\n+def argmin(input, axis, tie_break_left=True):\n+    _, ret = min(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n     return ret\n \n "}, {"filename": "python/triton/tools/aot.py", "status": "removed", "additions": 0, "deletions": 117, "changes": 117, "file_content_changes": "@@ -1,117 +0,0 @@\n-import argparse\n-import sys\n-\n-from .._C.libtriton.triton import ir\n-# import triton.compiler.compiler as tc\n-from ..compiler.compiler import (get_amdgpu_arch_fulldetails, llir_to_amdgcn_and_hsaco,\n-                                 llir_to_ptx, optimize_ttgir, optimize_ttir,\n-                                 ttgir_to_llir, ttir_to_ttgir)\n-\n-if __name__ == '__main__':\n-\n-    # valid source and target formats\n-    VALID_FORMATS = ['triton-ir', 'triton-gpu-ir', 'llvm-ir', 'ptx', 'amdgcn']\n-\n-    # set up the argument parser\n-    # TODO: conditional requirements\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument('src', help=\"Source file to compile\")\n-    parser.add_argument('--target', required=True,\n-                        help=\"Target format, one of: \" + ', '.join(VALID_FORMATS))\n-    parser.add_argument('--sm', type=int, help=\"Compute capability to compile for\")\n-    parser.add_argument('--ptx-version', type=int, help=\"PTX version to compile for\")\n-    parser.add_argument('--gfx', type=str, help=\"AMDGPU target to compile for\")\n-    parser.add_argument('--triple', type=str, help=\"target triple, for example: amdgcn-amd-amdhsa\")\n-    parser.add_argument('--features', type=str, help=\"target features, for example: +sramecc,-xnack\")\n-    parser.add_argument('--num_warps', type=int, help=\"number of warps to compile ttgir for\")\n-\n-    # parse the args\n-    args = parser.parse_args()\n-\n-    # TODO: clean-up and re-use triton.compiler primitive functions\n-    # check for validity of format arguments\n-    if args.target not in VALID_FORMATS:\n-        print(\"Invalid target format: \" + args.target)\n-        sys.exit(0)\n-\n-    # parse source file to MLIR module\n-    context = ir.context()\n-    module = ir.parse_mlir_module(args.src, context)\n-    module.context = context\n-\n-    # optimizer triton-ir\n-    module = optimize_ttir(module, arch=args.sm)\n-    if args.target == 'triton-ir':\n-        print(module.str())\n-        sys.exit(0)\n-\n-    if not args.num_warps:\n-        args.num_warps = 4\n-\n-    # llvm-ir -> amdgcn\n-    if args.target == 'amdgcn':\n-        # auto detect available architecture and features\n-        # if nothing detected, set with default values\n-        arch_details = get_amdgpu_arch_fulldetails()\n-        if not arch_details:\n-            arch_name = \"\"\n-            arch_triple = \"amdgcn-amd-amdhsa\"\n-            arch_features = \"\"\n-        else:\n-            arch_triple, arch_name, arch_features = arch_details\n-\n-        # stop processing if architecture name is not automatically detected and is not set manually\n-        if not args.gfx and not arch_name:\n-            raise argparse.ArgumentError(None, \"Must specify --gfx for AMDGCN compilation\")\n-\n-        # rewrite default and automatically detected values with manually provided data\n-        if args.gfx:\n-            arch_name = args.gfx\n-        if args.triple:\n-            arch_triple = args.triple\n-        if args.features:\n-            arch_features = args.features\n-\n-        # triton-ir -> triton-gpu-ir\n-        # use compute_capability == 80\n-        module = ttir_to_ttgir(module, num_warps=args.num_warps)  # num_stages=3, compute_capability=80)\n-        module = optimize_ttgir(module, num_stages=3, arch=80)\n-        # triton-gpu-ir -> llvm-ir\n-        # use compute_capability == 80\n-        module = ttgir_to_llir(module, extern_libs=None, arch=80)\n-        # llvm-ir -> amdgcn asm, hsaco binary\n-        module, hsaco_path = llir_to_amdgcn_and_hsaco(module, arch_name, arch_triple, arch_features)\n-\n-        print(hsaco_path)\n-        print(module)\n-        sys.exit(0)\n-\n-    if not args.sm:\n-        raise argparse.ArgumentError(None, \"Must specify --sm for PTX compilation\")\n-\n-    # triton-ir -> triton-gpu-ir\n-    module = ttir_to_ttgir(module, num_warps=args.num_warps)\n-    module = optimize_ttgir(module, num_stages=3, arch=args.sm)\n-    if args.target == 'triton-gpu-ir':\n-        print(module.str())\n-        sys.exit(0)\n-\n-    # triton-gpu-ir -> llvm-ir\n-    module = ttgir_to_llir(module, extern_libs=None, arch=args.sm)\n-    if args.target == 'llvm-ir':\n-        print(module)\n-        sys.exit(0)\n-\n-    # llvm-ir -> ptx\n-    if args.target == 'ptx':\n-        if not args.ptx_version:\n-            raise argparse.ArgumentError(None, \"Must specify --ptx-version for PTX compilation\")\n-        module = llir_to_ptx(module, arch=args.sm, ptx_version=args.ptx_version)\n-\n-    # llvm-ir -> amdgcn\n-    if args.target == 'amdgcn':\n-        if not args.gfx:\n-            raise argparse.ArgumentError(None, \"Must specify --gfx for AMDGCN compilation\")\n-        module, hsaco_path = llir_to_amdgcn_and_hsaco(module, args.gfx)\n-\n-    print(module)"}, {"filename": "python/triton/tools/compile.c", "status": "added", "additions": 64, "deletions": 0, "changes": 64, "file_content_changes": "@@ -0,0 +1,64 @@\n+/* clang-format off */\n+#include <stdio.h>\n+#include <stdint.h>\n+#include <inttypes.h>\n+#include <string.h>\n+#include <cuda.h>\n+\n+\n+// helpers to check for cuda errors\n+#define CUDA_CHECK(ans) {{\\\n+    gpuAssert((ans), __FILE__, __LINE__);\\\n+  }}\\\n+\n+static inline void gpuAssert(CUresult code, const char *file, int line) {{\n+  if (code != CUDA_SUCCESS) {{\n+    const char *prefix = \"Triton Error [CUDA]: \";\n+    const char *str;\n+    cuGetErrorString(code, &str);\n+    char err[1024] = {{0}};\n+    strcat(err, prefix);\n+    strcat(err, str);\n+    printf(\"%s\\\\n\", err);\n+    exit(code);\n+  }}\n+}}\n+\n+// globals\n+#define CUBIN_NAME {kernel_name}_cubin\n+CUmodule {kernel_name}_mod = NULL;\n+CUfunction {kernel_name}_func = NULL;\n+unsigned char CUBIN_NAME[{bin_size}] = {{ {bin_data} }};\n+\n+\n+void unload_{kernel_name}(void) {{\n+    CUDA_CHECK(cuModuleUnload({kernel_name}_mod));\n+}}\n+\n+// TODO: some code duplication with `runtime/backend/cuda.c`\n+void load_{kernel_name}() {{\n+    int dev = 0;\n+    void *bin = (void *)&CUBIN_NAME;\n+    int shared = {shared};\n+    CUDA_CHECK(cuModuleLoadData(&{kernel_name}_mod, bin));\n+    CUDA_CHECK(cuModuleGetFunction(&{kernel_name}_func, {kernel_name}_mod, \"{kernel_name}\"));\n+    // set dynamic shared memory if necessary\n+    int shared_optin;\n+    CUDA_CHECK(cuDeviceGetAttribute(&shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, dev));\n+    if (shared > 49152 && shared_optin > 49152) {{\n+      CUDA_CHECK(cuFuncSetCacheConfig({kernel_name}_func, CU_FUNC_CACHE_PREFER_SHARED));\n+      CUDA_CHECK(cuFuncSetAttribute({kernel_name}_func, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, shared_optin))\n+    }}\n+}}\n+\n+/*\n+{kernel_docstring}\n+*/\n+CUresult {kernel_name}(CUstream stream, unsigned int gX,unsigned int gY,unsigned int gZ,unsigned int numWarps, {signature}) {{\n+    if ({kernel_name}_func == NULL)\n+       load_{kernel_name}();\n+    void *args[{num_args}] = {{ {arg_pointers} }};\n+    // TODO: shared memory\n+    if(gX * gY * gZ > 0)\n+      return cuLaunchKernel({kernel_name}_func, gX, gY, gZ, numWarps * 32, 1, 1, {shared}, stream, args, NULL);\n+}}"}, {"filename": "python/triton/tools/compile.h", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -0,0 +1,15 @@\n+#ifndef TT_KERNEL_INCLUDES\n+#define TT_KERNEL_INCLUDES\n+\n+#include <cuda.h>\n+#include <inttypes.h>\n+#include <stdint.h>\n+#include <stdio.h>\n+\n+#endif\n+\n+void unload_{kernel_name}(void);\n+void load_{kernel_name}(void);\n+// tt-linker: {kernel_name}:{signature}\n+CUresult{kernel_name}(CUstream stream, unsigned int gX, unsigned int gY,\n+                      unsigned int gZ, unsigned int numWarps, {signature});"}, {"filename": "python/triton/tools/compile.py", "status": "added", "additions": 103, "deletions": 0, "changes": 103, "file_content_changes": "@@ -0,0 +1,103 @@\n+import binascii\n+import importlib.util\n+import sys\n+from argparse import ArgumentParser\n+from pathlib import Path\n+\n+import triton\n+from triton.compiler.code_generator import kernel_suffix\n+from triton.compiler.make_launcher import ty_to_cpp\n+\n+desc = \"\"\"\n+Triton ahead-of-time compiler:\n+\n+This program compiles the kernel with name `kernel-name` in the file at the\n+provided `path` into self-contained C source-code that embeds the `cubin`\n+data along with utilities to load, unload and launch the kernel.\n+\n+signature is provided as a list of (optionally divisibility-hinted) types\n+or constexpr values, e.g.\n+\n+`compile.py --kernel-name kernel --signature \"*f32:16, i32:16, 1024, i32\" --out-name kernel /path/to/kernel.py`\n+\n+will compile triton.JITFunction of name `kernel` inside the file `/path/to/kernel.py`.\n+Said kernel will be specialized such that argument 0, 1 are assumed to be multiple of 16,\n+and argument 2 is assumed to be a compile-time constant of value 1024, i.e. it won't be part of the generated prototype.\n+\n+The resulting entry point will have signature\n+\n+CUresult kernel_{specialization_suffix}(CUstream stream, unsigned gX, unsigned gY, unsigned gZ, unsigned numWarps, float* arg0, int32_t arg1, int32_t arg2)\n+\n+Different such specialized entry points can be combined using the `linker.py` script.\n+\n+NOTE: when resolving the scope of /path/to/kernel.py, the file will be executed from within its parent directory with the python interpreter\n+used to run this `compile.py` script\n+\"\"\"\n+\n+if __name__ == \"__main__\":\n+\n+    # command-line arguments\n+    parser = ArgumentParser(description=desc)\n+    parser.add_argument(\"path\", help=\"Path to Python source containing desired kernel in its scope. File will be executed.\")\n+    parser.add_argument(\"--kernel-name\", \"-n\", type=str, default=\"\")\n+    parser.add_argument(\"--out-path\", \"-o\", type=Path, help=\"Out filename\")\n+    parser.add_argument(\"--out-name\", \"-on\", type=str, default=None, help=\"Out name for the compiled kernel\")\n+    parser.add_argument(\"--signature\", \"-s\", type=str, help=\"Signature of the kernel\")\n+    args = parser.parse_args()\n+\n+    # execute python sources and extract functions wrapped in JITFunction\n+    arg_path = Path(args.path)\n+    sys.path.insert(0, str(arg_path.parent))\n+    spec = importlib.util.spec_from_file_location(arg_path.stem, arg_path)\n+    mod = importlib.util.module_from_spec(spec)\n+    spec.loader.exec_module(mod)\n+    kernel = getattr(mod, args.kernel_name)\n+\n+    # validate and parse signature\n+    signature = list(map(lambda s: s.strip(\" \"), args.signature.split(\",\")))\n+\n+    def constexpr(s):\n+        try:\n+            ret = int(s)\n+            return ret\n+        except ValueError:\n+            pass\n+        try:\n+            ret = float(s)\n+            return ret\n+        except ValueError:\n+            pass\n+        return None\n+    hints = {i: constexpr(s.split(\":\")[1]) for i, s in enumerate(signature) if \":\" in s}\n+    hints = {k: v for k, v in hints.items() if v is not None}\n+    constexprs = {i: constexpr(s) for i, s in enumerate(signature)}\n+    constexprs = {k: v for k, v in constexprs.items() if v is not None}\n+    signature = {i: s.split(\":\")[0] for i, s in enumerate(signature) if i not in constexprs}\n+\n+    # compile ast into cubin\n+    for h in hints.values():\n+        assert h in [1, 16], f\"Only 1 and 16 are valid hints, got {h}\"\n+    divisible_by_16 = [i for i, h in hints.items() if h == 16]\n+    equal_to_1 = [i for i, h in hints.items() if h == 1]\n+    config = triton.compiler.instance_descriptor(divisible_by_16=divisible_by_16, equal_to_1=equal_to_1)\n+    ccinfo = triton.compile(kernel, signature=signature, constants=constexprs, configs=[config], num_warps=1)\n+    arg_names = [kernel.arg_names[i] for i in signature.keys()]\n+\n+    # dump C stub code\n+    suffix = kernel_suffix(signature.values(), config)\n+    func_name = '_'.join([kernel.__name__, suffix])\n+    hex_ = str(binascii.hexlify(ccinfo.asm[\"cubin\"]))[2:-1]\n+    params = {\n+        \"kernel_name\": func_name,\n+        \"bin_size\": len(hex_),\n+        \"bin_data\": \", \".join([f\"0x{x}{y}\" for x, y in zip(hex_[::2], hex_[1::2])]),\n+        \"signature\": \", \".join([f\"{ty_to_cpp(ty)} {name}\" for name, ty in zip(arg_names, signature.values())]),\n+        \"arg_pointers\": \", \".join([f\"&{arg}\" for arg in arg_names]),\n+        \"num_args\": len(arg_names),\n+        \"kernel_docstring\": \"\",\n+        \"shared\": ccinfo.shared,\n+    }\n+    for ext in ['h', 'c']:\n+        template_path = Path(__file__).parent / f\"compile.{ext}\"\n+        with args.out_path.with_suffix(f\".{suffix}.{ext}\").open(\"w\") as fp:\n+            fp.write(Path(template_path).read_text().format(**params))"}, {"filename": "python/triton/tools/link.py", "status": "added", "additions": 192, "deletions": 0, "changes": 192, "file_content_changes": "@@ -0,0 +1,192 @@\n+from collections import defaultdict\n+from pathlib import Path\n+from typing import Sequence, Union\n+\n+from dataclasses import dataclass\n+\n+\n+def _exists(x):\n+    return x is not None\n+\n+\n+class LinkerError(Exception):\n+    pass\n+\n+\n+@dataclass\n+class KernelLinkerMeta:\n+    arg_names: Sequence[str]\n+    arg_ctypes: Sequence[str]\n+    sizes: Sequence[Union[int, None]]\n+    suffix: str\n+    num_specs: int\n+    \"\"\" number of specialized arguments \"\"\"\n+\n+\n+class HeaderParser:\n+    def __init__(self) -> None:\n+        import re\n+\n+        # [kernel_name, c signature]\n+        self.linker_directives = re.compile(\"//[\\\\s]*tt-linker:[\\\\s]*([\\\\w]+):(.+)\")\n+        # [name, suffix]\n+        self.kernel_name = re.compile(\"([\\\\w]+)_([\\\\w]+)\")\n+        # [(argnum, d|c)]\n+        self.kernel_suffix = re.compile(\"([0-9]+)([c,d])\")\n+        # [(type, name)]\n+        self.c_sig = re.compile(\"[\\\\s]*(\\\\w+)\\\\s(\\\\w+)[,]?\")\n+\n+        self.kernels = defaultdict(list)\n+\n+    def extract_linker_meta(self, header: str):\n+\n+        for ln in header.splitlines():\n+            if ln.startswith(\"//\"):\n+                m = self.linker_directives.match(ln)\n+                if _exists(m):\n+                    ker_name, c_sig = m.group(1), m.group(2)\n+                    name, suffix = self._match_name(ker_name)\n+                    c_types, arg_names = self._match_c_sig(c_sig)\n+                    num_specs, sizes = self._match_suffix(suffix)\n+                    self._add_kernel(\n+                        name,\n+                        KernelLinkerMeta(\n+                            arg_names=arg_names,\n+                            arg_ctypes=c_types,\n+                            sizes=sizes,\n+                            suffix=suffix,\n+                            num_specs=num_specs,\n+                        ),\n+                    )\n+\n+    def _match_name(self, ker_name: str):\n+        m = self.kernel_name.match(ker_name)\n+        if _exists(m):\n+            name, suffix = m.group(1), m.group(2)\n+            return name, suffix\n+        raise LinkerError(f\"{ker_name} is not a valid kernel name\")\n+\n+    def _match_c_sig(self, c_sig: str):\n+        m = self.c_sig.findall(c_sig)\n+        if len(m):\n+            tys, args = [], []\n+            for (ty, arg_name) in m:\n+                tys.append(ty)\n+                args.append(arg_name)\n+            return tys, args\n+\n+        raise LinkerError(f\"{c_sig} is not a valid argument signature\")\n+\n+    def _match_suffix(self, suffix: str):\n+        m = self.kernel_suffix.findall(suffix)\n+        if not len(m):\n+            raise LinkerError(f\"{suffix} is not a valid kernel suffix\")\n+        sizes = []\n+        num_specs = len(m)\n+        s2i = {\"c\": 1, \"d\": 16}\n+        for (argnum, arg_size_ann) in m:\n+            while len(sizes) < int(argnum):\n+                sizes.append(None)\n+\n+            sizes.append(s2i[arg_size_ann])\n+        return num_specs, sizes\n+\n+    def _add_kernel(self, name: str, ker: KernelLinkerMeta):\n+        if name in self.kernels:\n+            last: KernelLinkerMeta = self.kernels[name][-1]\n+\n+            for (cur, new_) in zip(last.arg_ctypes, ker.arg_ctypes):\n+                if cur != new_:\n+                    raise LinkerError(\n+                        f\"Mismatched signature for kernel {name}: \\n\\texisting sig is: {','.join(last.arg_ctypes)}\\n\\tcurrent is: {','.join(ker.arg_ctypes)}\"\n+                    )\n+\n+        self.kernels[name].append(ker)\n+\n+\n+def gen_signature(m):\n+    return \", \".join([f\"{ty} {arg}\" for ty, arg in zip(m.arg_ctypes, m.arg_names)])\n+\n+\n+def make_decls(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n+    return f\"\"\"\n+CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, unsigned int numWarps, {gen_signature(metas[-1])});\n+void load_{name}();\n+void unload_{name}();\n+    \"\"\"\n+\n+\n+def make_kernel_dispatcher(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n+    src = f\"// launcher for: {name}\\n\"\n+    for meta in sorted(metas, key=lambda m: -m.num_specs):\n+        src += f\"CUresult {name}_{meta.suffix}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, unsigned int numWarps, {gen_signature(meta)});\\n\"\n+    src += \"\\n\"\n+\n+    src += f\"CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, unsigned int numWarps, {gen_signature(metas[-1])}){{\"\n+    src += \"\\n\"\n+    for meta in sorted(metas, key=lambda m: -m.num_specs):\n+        cond_fn = lambda val, hint: f\"({val} % {hint} == 0)\" if hint == 16 else f\"({val} == {hint})\" if hint == 1 else None\n+        conds = \" && \".join([cond_fn(val, hint) for val, hint in zip(meta.arg_names, meta.sizes) if hint is not None])\n+        src += f\"  if ({conds})\\n\"\n+        src += f\"    return {name}_{meta.suffix}(stream, gX, gY, gZ, numWarps, {', '.join(meta.arg_names)});\\n\"\n+    src += \"}\\n\"\n+\n+    for mode in [\"load\", \"unload\"]:\n+        src += f\"\\n// {mode} for: {name}\\n\"\n+        for meta in sorted(metas, key=lambda m: -m.num_specs):\n+            src += f\"void {mode}_{name}_{meta.suffix}();\\n\"\n+        src += f\"void {mode}_{name}() {{\"\n+        src += \"\\n\"\n+        for meta in sorted(metas, key=lambda m: -m.num_specs):\n+            src += f\"  {mode}_{name}_{meta.suffix}();\\n\"\n+        src += \"}\\n\"\n+    return src\n+\n+\n+desc = \"\"\"\n+Triton ahead-of-time linker:\n+\n+This program takes in header files generated by compile.py, and generates a\n+single entry-point responsible for dispatching the user's input to the right\n+kernel given the specializations that were compiled.\n+\n+Example usage:\n+python link.py /path/to/headers/*.h -o kernel_name\n+\"\"\"\n+\n+if __name__ == \"__main__\":\n+    from argparse import ArgumentParser\n+\n+    parser = ArgumentParser(description=desc)\n+    parser.add_argument(\n+        \"headers\",\n+        nargs=\"+\",\n+        help=\"Paths to header files to link. Must include linker directive annotations (autogenerated by ttc)\",\n+    )\n+    parser.add_argument(\"--out\", \"-o\", type=Path, help=\"Out filename\")\n+    parser.add_argument(\"--prefix\", type=str, default=\"\", help=\"String to prefix kernel dispatcher names\")\n+    args = parser.parse_args()\n+\n+    # metadata\n+    parser = HeaderParser()\n+    includes = []\n+    for header in args.headers:\n+        h_path = Path(header)\n+        h_str = h_path.read_text()\n+        includes.append(h_path.name)\n+        parser.extract_linker_meta(h_str)\n+\n+    # generate headers\n+    decls = [make_decls(name, meta) for name, meta in parser.kernels.items()]\n+    with args.out.with_suffix(\".h\").open(\"w\") as fp:\n+        fp.write(\"#include <cuda.h>\\n\" + \"\\n\".join(decls))\n+\n+    # generate source\n+    defs = [make_kernel_dispatcher(name, meta) for name, meta in parser.kernels.items()]\n+    with args.out.with_suffix(\".c\").open(\"w\") as fp:\n+        out = \"\"\n+        out += \"#include <cuda.h>\\n\"\n+        out += \"#include <stdint.h>\\n\"\n+        out += \"\\n\"\n+        out += \"\\n\".join(defs)\n+        fp.write(out)"}, {"filename": "test/Target/tritongpu_to_llvmir.mlir", "status": "removed", "additions": 0, "deletions": 16, "changes": 16, "file_content_changes": "@@ -1,16 +0,0 @@\n-// RUN: %PYTHON -m triton.tools.aot %s --target=llvm-ir --sm=80 | FileCheck %s\n-\n-// == LLVM IR check begin ==\n-// CHECK-LABEL: ; ModuleID = 'LLVMDialectModule'\n-// CHECK: define void @test_empty_kernel\n-// CHECK: !nvvm.annotations\n-// CHECK: !{ptr @test_empty_kernel, !\"maxntidx\", i32 128}\n-\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-\n-tt.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n-\n-  tt.return\n-}\n-\n-}"}, {"filename": "test/Target/tritongpu_to_llvmir_noinline.mlir", "status": "removed", "additions": 0, "deletions": 22, "changes": 22, "file_content_changes": "@@ -1,22 +0,0 @@\n-// RUN: %PYTHON -m triton.tools.aot %s --target=llvm-ir --sm=80 | FileCheck %s\n-\n-// == LLVM IR check begin ==\n-// CHECK-LABEL: ; ModuleID = 'LLVMDialectModule'\n-// CHECK: define void @test_func\n-// CHECK: define void @test_kernel\n-// CHECK: tail call void @test_func\n-\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-\n-tt.func @test_func(%lb : index, %A : !tt.ptr<f16>) attributes { noinline = true } {\n-  %0 = arith.constant 1.0 : f16\n-  tt.store %A, %0 : f16\n-  tt.return\n-}\n-\n-tt.func @test_kernel(%lb : index, %A : !tt.ptr<f16>) {\n-  tt.call @test_func(%lb, %A) : (index, !tt.ptr<f16>) -> ()\n-  tt.return\n-}\n-\n-}"}, {"filename": "test/Target/tritongpu_to_ptx.mlir", "status": "removed", "additions": 0, "deletions": 14, "changes": 14, "file_content_changes": "@@ -1,14 +0,0 @@\n-// RUN: %PYTHON -m triton.tools.aot %s --target=ptx --sm=80 --ptx-version=63 | FileCheck %s\n-// CHECK-LABEL: // Generated by LLVM NVPTX Back-End\n-// CHECK: .version 6.3\n-// CHECK: .target sm_80\n-// CHECK: .address_size 64\n-\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-\n-tt.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n-\n-  tt.return\n-}\n-\n-}"}]