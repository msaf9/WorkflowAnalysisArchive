[{"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/RewriteTensorPointer.cpp", "status": "modified", "additions": 77, "deletions": 3, "changes": 80, "file_content_changes": "@@ -366,6 +366,9 @@ class TritonGPURewriteTensorPointerPass\n       : computeCapability(computeCapability) {}\n \n   static bool needRewrite(Operation *op, const DenseSet<Value> &valueToRemove) {\n+    if (auto ifOp = dyn_cast<scf::IfOp>(op)) {\n+      op = ifOp.thenYield().getOperation();\n+    }\n     return std::any_of(op->getOperands().begin(), op->getOperands().end(),\n                        [&valueToRemove](Value operand) {\n                          return tt::isTensorPointerType(operand.getType()) &&\n@@ -615,6 +618,77 @@ class TritonGPURewriteTensorPointerPass\n     return nullptr;\n   }\n \n+  Operation *rewriteIfOp(OpBuilder &builder, scf::IfOp op,\n+                         std::stack<Operation *> &eraser,\n+                         DenseSet<Value> &valueToRemove) {\n+    auto thenYieldOp = op.thenYield();\n+    assert(op.getNumResults() == thenYieldOp.getNumOperands());\n+    SmallVector<Value> results = thenYieldOp.getOperands();\n+\n+    // get new result types for IfOp\n+    SmallVector<Type> newRetTypes;\n+    for (unsigned i = 0; i < results.size(); ++i) {\n+      if (!tt::isTensorPointerType(results[i].getType()) ||\n+          !valueToRemove.count(results[i])) {\n+        newRetTypes.push_back(results[i].getType());\n+        continue;\n+      }\n+      auto makeTensorPtrOp = getMakeTensorPtrOp(results[i]);\n+      assert(rewritedInfo.count(makeTensorPtrOp.getResult()));\n+      auto info = rewritedInfo[makeTensorPtrOp.getResult()];\n+      for (unsigned j = 0; j < info.length(); ++j) {\n+        newRetTypes.push_back(builder.getI64Type());\n+      }\n+    }\n+\n+    // create and clone new IfOp\n+    bool hasElse = !op.getElseRegion().empty();\n+    scf::IfOp newOp = builder.create<scf::IfOp>(op.getLoc(), newRetTypes,\n+                                                op.getCondition(), hasElse);\n+    IRMapping mapping;\n+    for (unsigned i = 0; i < op->getNumOperands(); ++i) {\n+      mapping.map(op->getOperand(i), newOp->getOperand(i));\n+    }\n+    auto rematerialize = [&](Block *block) {\n+      for (Operation &opInIf : block->getOperations()) {\n+        auto newOp = builder.clone(opInIf, mapping);\n+        for (unsigned i = 0; i < opInIf.getNumResults(); ++i) {\n+          if (valueToRemove.count(opInIf.getResult(i)))\n+            valueToRemove.insert(newOp->getResult(i));\n+          mapping.map(opInIf.getResult(i), newOp->getResult(i));\n+        }\n+      }\n+    };\n+    builder.setInsertionPointToStart(newOp.thenBlock());\n+    rematerialize(op.thenBlock());\n+    if (hasElse) {\n+      builder.setInsertionPointToStart(newOp.elseBlock());\n+      rematerialize(op.elseBlock());\n+    }\n+\n+    // update rewritedInfo for IfOp\n+    unsigned oldResIdx = 0, newResIdx = 0;\n+    while (oldResIdx < results.size()) {\n+      if (!tt::isTensorPointerType(results[oldResIdx].getType()) ||\n+          !valueToRemove.count(results[oldResIdx])) {\n+        oldResIdx++;\n+        newResIdx++;\n+      } else {\n+        auto makeTensorPtrOp = getMakeTensorPtrOp(results[oldResIdx]);\n+        assert(rewritedInfo.count(makeTensorPtrOp.getResult()));\n+        auto info = rewritedInfo[makeTensorPtrOp.getResult()];\n+        for (unsigned j = 0; j < info.length(); ++j) {\n+          info.setOffset(j, newOp->getResult(newResIdx++));\n+        }\n+        rewritedInfo[op.getResult(oldResIdx)] = info;\n+        oldResIdx++;\n+      }\n+    }\n+\n+    eraser.push(op);\n+    return newOp;\n+  }\n+\n   Operation *rewriteOp(Operation *op, std::stack<Operation *> &eraser,\n                        DenseSet<Value> &valueToRemove) {\n     OpBuilder builder(op);\n@@ -630,8 +704,6 @@ class TritonGPURewriteTensorPointerPass\n       return rewriteAdvanceOp(builder, advanceOp, eraser, valueToRemove);\n     } else if (isa<tt::LoadOp>(op) || isa<tt::StoreOp>(op)) {\n       return rewriteLoadStoreOp(builder, op, eraser, valueToRemove);\n-    } else if (auto storeOp = dyn_cast<tt::StoreOp>(op)) {\n-      return rewriteLoadStoreOp(builder, op, eraser, valueToRemove);\n     } else if (op->getDialect()->getNamespace() == \"scf\" ||\n                op->getDialect()->getNamespace() == \"cf\") {\n       if (!needRewrite(op, valueToRemove))\n@@ -641,9 +713,11 @@ class TritonGPURewriteTensorPointerPass\n         return rewriteForOp(builder, forOp, eraser, valueToRemove);\n       } else if (auto yieldOp = dyn_cast<scf::YieldOp>(op)) {\n         return rewriteYieldOp(builder, yieldOp, eraser, valueToRemove);\n+      } else if (auto ifOp = dyn_cast<scf::IfOp>(op)) {\n+        return rewriteIfOp(builder, ifOp, eraser, valueToRemove);\n       } else {\n         llvm_unreachable(\"Currently we only support tensor pointer usages \"\n-                         \"inside a `scf::ForOp`, others such as `scf::IfOp`,\"\n+                         \"inside a `scf::ForOp` or `scf::IfOp`, others such as \"\n                          \"`scf::WhileOp`, `cf::BranchOp` or `cf::CondBranchOp` \"\n                          \"are not supported yet\");\n       }"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 78, "deletions": 116, "changes": 194, "file_content_changes": "@@ -114,20 +114,21 @@ def static_persistent_tma_matmul_kernel(\n         pre_pid_n = pid_n\n \n \n-@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,TRANS_A,TRANS_B', [\n-    [4096, 4096, 64, 64, 64, 16, 4, 1, False, True],\n-    [4096, 4096, 64, 64, 64, 32, 4, 1, False, True],\n-    [4096, 4096, 64, 256, 64, 16, 4, 1, False, True],\n-    [4096, 4096, 64, 128, 128, 16, 4, 1, False, True],\n-    # TODO: fix issue for 8-warp persistent kernel\n-    # [4096, 4096, 64, 128, 128, 16, 8, 1, False, True],\n-    # [4096, 4096, 64, 128, 256, 16, 8, 1, False, True],\n-])\n+@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,TRANS_A,TRANS_B,KERN',\n+                         [(*shape, kern)\n+                          for shape in [\n+                             [4096, 4096, 64, 64, 64, 16, 4, 1, False, True],\n+                             [4096, 4096, 64, 64, 64, 32, 4, 1, False, True],\n+                             [4096, 4096, 64, 256, 64, 16, 4, 1, False, True],\n+                             [4096, 4096, 64, 128, 128, 16, 4, 1, False, True],\n+                             # TODO: fix issue for 8-warp persistent kernel\n+                             # [4096, 4096, 64, 128, 128, 16, 8, 1, False, True],\n+                             # [4096, 4096, 64, 128, 256, 16, 8, 1, False, True],\n+                         ]\n+                             for kern in ['vintage', 'stylish']\n+                         ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n-def test_user_defined_persistent_non_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, TRANS_A, TRANS_B):\n-    # TODO: fix RewriteTensorPtrPass\n-    pytest.skip('RewriteTensorPtrPass issue')\n-\n+def test_user_defined_persistent_non_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, TRANS_A, TRANS_B, KERN):\n     if (TRANS_A):\n         a = .1 * torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -141,27 +142,13 @@ def test_user_defined_persistent_non_warp_specialized_gemm(M, N, K, BLOCK_M, BLO\n \n     num_SMs = torch.cuda.get_device_properties('cuda').multi_processor_count\n     grid = lambda META: (num_SMs,)\n-\n-    def call_vintage():\n+    if KERN == 'vintage':\n         static_persistent_matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c, M=M, N=N, K=K, stride_am=a.stride(0), stride_ak=a.stride(1), stride_bk=b.stride(0), stride_bn=b.stride(1), stride_cm=c.stride(0), stride_cn=c.stride(1), BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K, NUM_SM=num_SMs, num_warps=NUM_WARPS, num_ctas=NUM_CTAS)\n-        return c\n-\n-    def call_stylish():\n+    elif KERN == 'stylish':\n         static_persistent_tma_matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c, M=M, N=N, K=K, stride_am=a.stride(0), stride_ak=a.stride(1), stride_bk=b.stride(0), stride_bn=b.stride(1), stride_cm=c.stride(0), stride_cn=c.stride(1), BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K, NUM_SM=num_SMs, num_warps=NUM_WARPS, num_ctas=NUM_CTAS)\n-        return c\n \n     th_c = torch.matmul(a, b)\n-\n-    # Test using old style of ptr calculation\n-    tt_c = call_vintage()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n-\n-    # Cealr c\n-    c = torch.randn((M, N), device=a.device, dtype=torch.float32)\n-\n-    # Test using make_block_ptr\n-    tt_c = call_stylish()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n+    torch.testing.assert_allclose(th_c, c, atol=1e-2, rtol=0)\n \n \n @triton.jit\n@@ -243,40 +230,43 @@ def tma_warp_specialized_matmul_kernel(\n     tl.store(c_ptrs, accumulator, mask=mask)\n \n \n-@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_CTAS,TRANS_A,TRANS_B', [\n-    [2048, 2048, 64, 64, 64, 16, 1, False, True],\n-    [4096, 4096, 64, 64, 64, 16, 1, False, True],\n-    [128, 4096, 64, 64, 64, 16, 1, False, True],\n-    [4096, 128, 64, 64, 64, 16, 1, False, True],\n-    [4096, 4096, 64, 64, 64, 32, 1, False, True],\n-    [4096, 4096, 256, 128, 128, 16, 1, False, True],\n-    [4096, 4096, 320, 128, 64, 64, 1, False, True],\n-    [4096, 4096, 320, 64, 128, 64, 1, False, True],\n-    [4096, 4096, 320, 128, 128, 64, 1, False, True],\n-    [4096, 4096, 256, 256, 64, 16, 1, False, True],\n-    [4096, 4096, 256, 256, 64, 64, 1, False, True],\n-    [4096, 4096, 256, 64, 256, 16, 1, False, True],\n-    [4096, 4096, 256, 64, 256, 64, 1, False, True],\n-    [4096, 4096, 256, 256, 128, 16, 1, False, True],\n-    [4096, 4096, 256, 256, 128, 64, 1, False, True],\n-    [4096, 4096, 256, 128, 256, 16, 1, False, True],\n-    [4096, 4096, 256, 128, 256, 64, 1, False, True],\n-    # numCTAs > 1\n-    [2048, 2048, 64, 128, 128, 64, 2, False, True],\n-    [2048, 2048, 64, 128, 128, 64, 2, False, True],\n-    [2048, 2048, 128, 256, 128, 64, 4, False, True],\n-    [4096, 4096, 128, 256, 128, 64, 4, False, True],\n-    [4096, 4096, 256, 128, 256, 64, 4, False, True],\n-    [4096, 4096, 256, 256, 256, 64, 4, False, True],\n-])\n+@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_CTAS,TRANS_A,TRANS_B,KERN',\n+                         [(*shape, kern)\n+                          for shape in [\n+                             [2048, 2048, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 4096, 64, 64, 64, 16, 1, False, True],\n+                             [128, 4096, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 128, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 4096, 64, 64, 64, 32, 1, False, True],\n+                             [4096, 4096, 256, 128, 128, 16, 1, False, True],\n+                             [4096, 4096, 320, 128, 64, 64, 1, False, True],\n+                             [4096, 4096, 320, 64, 128, 64, 1, False, True],\n+                             [4096, 4096, 320, 128, 128, 64, 1, False, True],\n+                             [4096, 4096, 256, 256, 64, 16, 1, False, True],\n+                             [4096, 4096, 256, 256, 64, 64, 1, False, True],\n+                             [4096, 4096, 256, 64, 256, 16, 1, False, True],\n+                             [4096, 4096, 256, 64, 256, 64, 1, False, True],\n+                             [4096, 4096, 256, 256, 128, 16, 1, False, True],\n+                             [4096, 4096, 256, 256, 128, 64, 1, False, True],\n+                             [4096, 4096, 256, 128, 256, 16, 1, False, True],\n+                             [4096, 4096, 256, 128, 256, 64, 1, False, True],\n+                             # numCTAs > 1\n+                             [2048, 2048, 64, 128, 128, 64, 2, False, True],\n+                             [2048, 2048, 64, 128, 128, 64, 2, False, True],\n+                             [2048, 2048, 128, 256, 128, 64, 4, False, True],\n+                             [4096, 4096, 128, 256, 128, 64, 4, False, True],\n+                             [4096, 4096, 256, 128, 256, 64, 4, False, True],\n+                             [4096, 4096, 256, 256, 256, 64, 4, False, True],\n+                         ]\n+                             for kern in ['vintage', 'stylish']\n+                         ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n-def test_non_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B):\n+def test_non_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B, KERN):\n     if '-'.join(map(str, [M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B])) in [\n         '4096-4096-256-128-256-16-1-False-True',\n         '4096-4096-256-128-256-64-1-False-True'\n     ]:\n         pytest.skip('Insufficient register resources')\n-\n     if (TRANS_A):\n         a = .1 * torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -290,8 +280,7 @@ def test_non_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K\n     c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n \n     grid = lambda META: (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n-\n-    def call_vintage():\n+    if KERN == 'vintage':\n         warp_specialized_matmul_kernel[grid](\n             a, b, c,\n             M, N, K,\n@@ -302,9 +291,7 @@ def call_vintage():\n             num_warps=4,\n             num_ctas=NUM_CTAS,\n             enable_warp_specialization=True)\n-        return c\n-\n-    def call_stylish():\n+    elif KERN == 'stylish':\n         tma_warp_specialized_matmul_kernel[grid](\n             a, b, c,\n             M, N, K,\n@@ -315,21 +302,9 @@ def call_stylish():\n             num_warps=4,\n             num_ctas=NUM_CTAS,\n             enable_warp_specialization=True)\n-        return c\n \n     th_c = torch.matmul(a, b)\n-\n-    # Test using old style of ptr calculation\n-    tt_c = call_vintage()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n-\n-    # Cealr c\n-    c = torch.randn((M, N), device=a.device, dtype=torch.float32)\n-\n-    # # Test using make_block_ptr\n-    tt_c = call_stylish()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n-\n+    torch.testing.assert_allclose(th_c, c, atol=1e-2, rtol=0)\n     # # #############################################Performance Evaluation#############################################\n     # fn = lambda: call_vintage()\n     # ms = triton.testing.do_bench(fn, warmup=25, rep=100)\n@@ -437,29 +412,31 @@ def static_persistent_tma_warp_specialized_matmul_kernel(\n         pre_pid_n = pid_n\n \n \n-@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_CTAS,TRANS_A,TRANS_B', [\n-    [2048, 2048, 64, 64, 64, 16, 1, False, True],\n-    [4096, 4096, 64, 64, 64, 16, 1, False, True],\n-    [128, 4096, 64, 64, 64, 16, 1, False, True],\n-    [4096, 128, 64, 64, 64, 16, 1, False, True],\n-    [4096, 4096, 64, 64, 64, 32, 1, False, True],\n-    [4096, 4096, 256, 128, 128, 16, 1, False, True],\n-    [4096, 4096, 320, 128, 64, 64, 1, False, True],\n-    [4096, 4096, 320, 64, 128, 64, 1, False, True],\n-    [4096, 4096, 320, 128, 128, 64, 1, False, True],\n-    [4096, 4096, 256, 256, 64, 16, 1, False, True],\n-    [4096, 4096, 256, 256, 64, 64, 1, False, True],\n-    [4096, 4096, 256, 64, 256, 16, 1, False, True],\n-    [4096, 4096, 256, 64, 256, 64, 1, False, True],\n-    [4096, 4096, 256, 256, 128, 16, 1, False, True],\n-    [4096, 4096, 256, 256, 128, 64, 1, False, True],\n-    [4096, 4096, 256, 128, 256, 16, 1, False, True],\n-    [4096, 4096, 256, 128, 256, 64, 1, False, True],\n-])\n+@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_CTAS,TRANS_A,TRANS_B,KERN',\n+                         [(*shape, kern)\n+                          for shape in [\n+                             [2048, 2048, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 4096, 64, 64, 64, 16, 1, False, True],\n+                             [128, 4096, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 128, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 4096, 64, 64, 64, 32, 1, False, True],\n+                             [4096, 4096, 256, 128, 128, 16, 1, False, True],\n+                             [4096, 4096, 320, 128, 64, 64, 1, False, True],\n+                             [4096, 4096, 320, 64, 128, 64, 1, False, True],\n+                             [4096, 4096, 320, 128, 128, 64, 1, False, True],\n+                             [4096, 4096, 256, 256, 64, 16, 1, False, True],\n+                             [4096, 4096, 256, 256, 64, 64, 1, False, True],\n+                             [4096, 4096, 256, 64, 256, 16, 1, False, True],\n+                             [4096, 4096, 256, 64, 256, 64, 1, False, True],\n+                             [4096, 4096, 256, 256, 128, 16, 1, False, True],\n+                             [4096, 4096, 256, 256, 128, 64, 1, False, True],\n+                             [4096, 4096, 256, 128, 256, 16, 1, False, True],\n+                             [4096, 4096, 256, 128, 256, 64, 1, False, True],\n+                         ]\n+                             for kern in ['vintage', 'stylish']\n+                         ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n-def test_user_defined_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B):\n-    # TODO: fix RewriteTensorPtrPass\n-    pytest.skip('RewriteTensorPtrPass issue')\n+def test_user_defined_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B, KERN):\n     if (TRANS_A):\n         a = .1 * torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -473,40 +450,25 @@ def test_user_defined_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N\n \n     num_SMs = torch.cuda.get_device_properties('cuda').multi_processor_count\n     grid = lambda META: (num_SMs,)\n-\n-    def call_vintage():\n+    if KERN == 'vintage':\n         static_persistent_warp_specialized_matmul_kernel[grid](\n             a, b, c,\n             M, N, K,\n             a.stride(0), a.stride(1),\n             b.stride(0), b.stride(1),\n             c.stride(0), c.stride(1),\n             BLOCK_M, BLOCK_N, BLOCK_K, num_SMs)\n-        return c\n-\n-    def call_stylish():\n+    elif KERN == 'stylish':\n         static_persistent_tma_warp_specialized_matmul_kernel[grid](\n             a, b, c,\n             M, N, K,\n             a.stride(0), a.stride(1),\n             b.stride(0), b.stride(1),\n             c.stride(0), c.stride(1),\n             BLOCK_M, BLOCK_N, BLOCK_K, num_SMs)\n-        return c\n \n     th_c = torch.matmul(a, b)\n-\n-    # Test using old style of ptr calculation\n-    tt_c = call_vintage()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n-\n-    # Cealr c\n-    c = torch.randn((M, N), device=a.device, dtype=torch.float32)\n-\n-    # Test using make_block_ptr\n-    tt_c = call_stylish()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n-\n+    torch.testing.assert_allclose(th_c, c, atol=1e-2, rtol=0)\n     # #############################################Performance Evaluation#############################################\n     # fn = lambda: call_stylish()\n     # ms = triton.testing.do_bench(fn, warmup=25, rep=100)"}, {"filename": "test/TritonGPU/rewrite-tensor-pointer.mlir", "status": "modified", "additions": 58, "deletions": 1, "changes": 59, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -tritongpu-rewrite-tensor-pointer | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -tritongpu-rewrite-tensor-pointer | FileCheck %s\n \n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [2, 2], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n@@ -62,3 +62,60 @@ module attributes {\"triton_gpu.compute-capability\" = 90 : i32, \"triton_gpu.num-c\n     tt.return\n   }\n }\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [2, 2], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0]}>\n+module attributes {\"triton_gpu.compute-capability\" = 90 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @if_for_if(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg4: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg5: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg6: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg7: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}) attributes {noinline = false} {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<64x1xf32, #blocked>\n+    %c63_i32 = arith.constant 63 : i32\n+    %c-16_i32 = arith.constant -16 : i32\n+    %c132_i32 = arith.constant 132 : i32\n+    %c0_i32 = arith.constant 0 : i32\n+    %c1_i64 = arith.constant 1 : i64\n+    %c64_i32 = arith.constant 64 : i32\n+    %0 = tt.get_program_id x : i32\n+    %1 = arith.addi %arg3, %c63_i32 : i32\n+    %2 = arith.divsi %1, %c64_i32 : i32\n+    %3 = arith.muli %0, %c64_i32 : i32\n+    %4 = arith.extsi %arg3 : i32 to i64\n+    %5 = arith.extsi %arg4 : i32 to i64\n+    %6 = arith.extsi %arg5 : i32 to i64\n+    // CHECK-NOT: tt.make_tensor_ptr\n+    %7 = tt.make_tensor_ptr %arg0, [%4, %5], [%6, %c1_i64], [%3, %c0_i32] {order = array<i32: 1, 0>} : <tensor<64x16xf16, #blocked>, 1>\n+    %8 = \"triton_gpu.cmpi\"(%2, %c132_i32) <{predicate = 5 : i64}> : (i32, i32) -> i1\n+    scf.if %8 {\n+      %9 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked1>\n+      %10 = tt.splat %arg7 : (i32) -> tensor<64x1xi32, #blocked>\n+      %11 = tt.splat %arg2 : (!tt.ptr<f32, 1>) -> tensor<64x1x!tt.ptr<f32, 1>, #blocked>\n+      %12 = scf.for %arg8 = %0 to %2 step %c132_i32 iter_args(%arg9 = %7) -> (!tt.ptr<tensor<64x16xf16, #blocked>, 1>)  : i32 {\n+        %13 = \"triton_gpu.cmpi\"(%arg8, %c132_i32) <{predicate = 5 : i64}> : (i32, i32) -> i1\n+        %14 = scf.if %13 -> (!tt.ptr<tensor<64x16xf16, #blocked>, 1>) {\n+          %25 = arith.subi %arg8, %0 : i32\n+          %26 = arith.muli %25, %c64_i32 : i32\n+          // CHECK-NOT: tt.advance\n+          %27 = tt.advance %arg9, [%26, %c-16_i32] : <tensor<64x16xf16, #blocked>, 1>\n+          scf.yield %27 : !tt.ptr<tensor<64x16xf16, #blocked>, 1>\n+        } else {\n+          scf.yield %arg9 : !tt.ptr<tensor<64x16xf16, #blocked>, 1>\n+        }\n+        %15 = arith.muli %arg8, %c64_i32 : i32\n+        %16 = tt.splat %15 : (i32) -> tensor<64xi32, #blocked1>\n+        %17 = arith.addi %9, %16 : tensor<64xi32, #blocked1>\n+        %18 = triton_gpu.convert_layout %17 : (tensor<64xi32, #blocked1>) -> tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+        %19 = tt.expand_dims %18 {axis = 1 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<64x1xi32, #blocked2>\n+        %20 = triton_gpu.convert_layout %19 : (tensor<64x1xi32, #blocked2>) -> tensor<64x1xi32, #blocked>\n+        %21 = arith.muli %20, %10 : tensor<64x1xi32, #blocked>\n+        %22 = tt.addptr %11, %21 : tensor<64x1x!tt.ptr<f32, 1>, #blocked>, tensor<64x1xi32, #blocked>\n+        %23 = triton_gpu.convert_layout %22 : (tensor<64x1x!tt.ptr<f32, 1>, #blocked>) -> tensor<64x1x!tt.ptr<f32, 1>, #blocked>\n+        %24 = triton_gpu.convert_layout %cst : (tensor<64x1xf32, #blocked>) -> tensor<64x1xf32, #blocked>\n+        tt.store %23, %24 {cache = 1 : i32, evict = 1 : i32} : tensor<64x1xf32, #blocked>\n+        scf.yield %14 : !tt.ptr<tensor<64x16xf16, #blocked>, 1>\n+      }\n+    }\n+    tt.return\n+  }\n+}"}]