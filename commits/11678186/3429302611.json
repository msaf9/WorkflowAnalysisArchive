[{"filename": "CMakeLists.txt", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -15,6 +15,10 @@ endif()\n option(TRITON_BUILD_TUTORIALS \"Build C++ Triton tutorials\" ON)\n option(TRITON_BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n \n+# Ensure Python3 vars are set correctly\n+#  used conditionally in this file and by lit tests\n+find_package(Python3 REQUIRED COMPONENTS Development Interpreter)\n+\n # Default build type\n if(NOT CMAKE_BUILD_TYPE)\n   message(STATUS \"Default build type: Release\")\n@@ -133,24 +137,22 @@ endif()\n if(TRITON_BUILD_PYTHON_MODULE)\n     message(STATUS \"Adding Python module\")\n     set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n+    set(PYTHON_SRC ${PYTHON_SRC_PATH}/main.cc ${PYTHON_SRC_PATH}/triton.cc)\n     include_directories(\".\" ${PYTHON_SRC_PATH})\n     if (PYTHON_INCLUDE_DIRS)\n       include_directories(${PYTHON_INCLUDE_DIRS})\n     else()\n-      find_package(Python3 REQUIRED COMPONENTS Development)\n       include_directories(${Python3_INCLUDE_DIRS})\n       link_directories(${Python3_LIBRARY_DIRS})\n       link_libraries(${Python3_LIBRARIES})\n       add_link_options(${Python3_LINK_OPTIONS})\n     endif()\n-    set(PYTHON_SRC ${PYTHON_SRC_PATH}/main.cc ${PYTHON_SRC_PATH}/triton.cc)\n endif()\n \n \n # # Triton\n # file(GLOB_RECURSE LIBTRITON_SRC lib/*.cc)\n # if (WIN32 AND TRITON_BUILD_PYTHON_MODULE)\n-#     find_package(Python3 REQUIRED COMPONENTS Development)\n #     Python3_add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n #     set_target_properties(triton PROPERTIES SUFFIX \".pyd\")\n #     set_target_properties(triton PROPERTIES PREFIX \"lib\")"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -21,11 +21,11 @@ template <typename Int> Int product(llvm::ArrayRef<Int> arr) {\n template <typename Int> Int ceil(Int m, Int n) { return (m + n - 1) / n; }\n \n // output[i] = input[order[i]]\n-template <typename T>\n-SmallVector<T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n+template <typename T, typename RES_T = T>\n+SmallVector<RES_T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n   size_t rank = order.size();\n   assert(input.size() == rank);\n-  SmallVector<T> result(rank);\n+  SmallVector<RES_T> result(rank);\n   for (auto it : llvm::enumerate(order)) {\n     result[it.index()] = input[it.value()];\n   }"}, {"filename": "include/triton/Conversion/Passes.td", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -38,6 +38,7 @@ def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"\n                              \"mlir::gpu::GPUDialect\",\n                              \"mlir::scf::SCFDialect\",\n                              \"mlir::LLVM::LLVMDialect\",\n+                             \"mlir::tensor::TensorDialect\",\n                              \"mlir::triton::TritonDialect\",\n                              \"mlir::triton::gpu::TritonGPUDialect\",\n                              \"mlir::NVVM::NVVMDialect\","}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -320,7 +320,7 @@ struct PTXInstrExecution {\n   // Prefix a !predicate to the instruction.\n   PTXInstrExecution &predicateNot(mlir::Value value, StringRef constraint) {\n     pred = instr->builder->newOperand(value, constraint);\n-    pred->repr = [](int idx) { return \"@!%\" + std::to_string(idx); };\n+    pred->repr = [](int idx) { return \"@!$\" + std::to_string(idx); };\n     return *this;\n   }\n "}, {"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,7 +1,6 @@\n #ifndef TRITON_DIALECT_TRITON_IR_DIALECT_H_\n #define TRITON_DIALECT_TRITON_IR_DIALECT_H_\n \n-#include \"mlir/Dialect/GPU/GPUDialect.h\"\n #include \"mlir/Dialect/Math/IR/Math.h\"\n #include \"mlir/Dialect/SCF/SCF.h\"\n #include \"mlir/Dialect/StandardOps/IR/Ops.h\""}, {"filename": "include/triton/Dialect/Triton/IR/TritonDialect.td", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -27,7 +27,6 @@ def Triton_Dialect : Dialect {\n     \"math::MathDialect\",\n     \"StandardOpsDialect\",\n     \"scf::SCFDialect\",\n-    \"gpu::GPUDialect\",\n \n     // Since LLVM 15\n     // \"cf::ControlFlowDialect\","}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 12, "deletions": 6, "changes": 18, "file_content_changes": "@@ -186,7 +186,14 @@ def TT_StoreOp : TT_Op<\"store\",\n // Atomic Op\n //\n def TT_AtomicRMWOp : TT_Op<\"atomic_rmw\", [SameOperandsAndResultShape,\n-                                          SameOperandsAndResultEncoding]> {\n+                                          SameOperandsAndResultEncoding,\n+                                          MemoryEffects<[MemWrite]>,\n+                                          TypesMatchWith<\"infer ptr type from value type\",\n+                                                         \"val\", \"ptr\",\n+                                                         \"getPointerTypeSameShape($_self)\">,\n+                                          TypesMatchWith<\"infer mask type from value type\",\n+                                                         \"val\", \"mask\", \"getI1SameShape($_self)\",\n+                                                       \"($_op.getOperands().size() <= 2) || std::equal_to<>()\">]> {\n     let summary = \"atomic rmw\";\n \n     let description = [{\n@@ -195,8 +202,8 @@ def TT_AtomicRMWOp : TT_Op<\"atomic_rmw\", [SameOperandsAndResultShape,\n         return old value at $ptr\n     }];\n \n-    let arguments = (ins TT_AtomicRMWAttr:$atomic_rmw_op, TT_PtrTensor:$ptr,\n-                         TT_Type:$val, I1Tensor:$mask);\n+    let arguments = (ins TT_AtomicRMWAttr:$atomic_rmw_op, TT_PtrLike:$ptr,\n+                         TT_Type:$val, Optional<TT_BoolLike>:$mask);\n \n     let results = (outs TT_Type:$result);\n }\n@@ -357,12 +364,11 @@ def TT_ExtElemwiseOp : TT_Op<\"ext_elemwise\", [NoSideEffect, Elementwise, SameOpe\n         return $libpath/$libname:$symbol($args...)\n     }];\n \n-    let arguments = (ins Variadic<TT_Tensor>:$args, StrAttr:$libname, StrAttr:$libpath, StrAttr:$symbol);\n+    let arguments = (ins Variadic<TT_Type>:$args, StrAttr:$libname, StrAttr:$libpath, StrAttr:$symbol);\n \n-    let results = (outs TT_Tensor:$result);\n+    let results = (outs TT_Type:$result);\n \n     let assemblyFormat = \"operands attr-dict `:` type(operands) `->` type($result)\";\n-\n }\n \n //"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -2,13 +2,15 @@\n #define TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_\n \n #include \"mlir/Dialect/GPU/GPUDialect.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/Dialect.h\"\n \n // TritonGPU depends on Triton\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n \n #include \"triton/Dialect/TritonGPU/IR/Dialect.h.inc\"\n+#include \"triton/Dialect/TritonGPU/IR/Traits.h\"\n \n #define GET_ATTRDEF_CLASSES\n #include \"triton/Dialect/Triton/IR/AttrInterfaces.h.inc\""}, {"filename": "include/triton/Dialect/TritonGPU/IR/Traits.h", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "file_content_changes": "@@ -0,0 +1,31 @@\n+#ifndef TRITON_GPU_IR_TRAITS_H_\n+#define TRITON_GPU_IR_TRAITS_H_\n+\n+#include \"mlir/IR/OpDefinition.h\"\n+\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+\n+namespace mlir {\n+namespace OpTrait {\n+\n+// These functions are out-of-line implementations of the methods in the\n+// corresponding trait classes.  This avoids them being template\n+// instantiated/duplicated.\n+namespace impl {\n+LogicalResult verifyResultsAreSharedEncoding(Operation *op);\n+} // namespace impl\n+\n+template <typename ConcreteType>\n+class ResultsAreSharedEncoding\n+    : public TraitBase<ConcreteType, ResultsAreSharedEncoding> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifyResultsAreSharedEncoding(op);\n+  }\n+};\n+\n+} // namespace OpTrait\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 10, "deletions": 9, "changes": 19, "file_content_changes": "@@ -163,18 +163,19 @@ for\n                      \"ArrayRef<unsigned>\":$order,\n                      \"unsigned\":$numWarps), [{\n       int rank = sizePerThread.size();\n-      int remainingWarps = numWarps;\n-      int remainingLanes = 32;\n+      unsigned remainingLanes = 32;\n+      unsigned remainingThreads = numWarps*32;\n+      unsigned remainingWarps = numWarps;\n       SmallVector<unsigned, 4> threadsPerWarp(rank);\n       SmallVector<unsigned, 4> warpsPerCTA(rank);\n       for (int _dim = 0; _dim < rank; ++_dim) {\n-        int dim = order[_dim];\n-        int maxNumThreads = int(shape[dim]) / sizePerThread[dim];\n-        warpsPerCTA[dim] = std::clamp(remainingWarps, 1, maxNumThreads);\n-        maxNumThreads = maxNumThreads / warpsPerCTA[dim];\n-        threadsPerWarp[dim] = std::clamp(remainingLanes, 1, maxNumThreads);\n-        remainingWarps /= warpsPerCTA[dim];\n-        remainingLanes /= threadsPerWarp[dim];\n+        int i = order[_dim];\n+        unsigned threadsPerCTA = std::clamp<unsigned>(remainingThreads, 1, shape[i] / sizePerThread[i]);\n+        threadsPerWarp[i] = std::clamp<unsigned>(threadsPerCTA, 1, remainingLanes);\n+        warpsPerCTA[i] = std::clamp<unsigned>(threadsPerCTA / threadsPerWarp[i], 1, remainingWarps);\n+        remainingWarps /= warpsPerCTA[i];\n+        remainingLanes /= threadsPerWarp[i];\n+        remainingThreads /= threadsPerCTA;\n       }\n \n       return $_get(context, sizePerThread, threadsPerWarp, warpsPerCTA, order);"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUDialect.td", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -16,7 +16,8 @@ def TritonGPU_Dialect : Dialect {\n \n   let dependentDialects = [\n     \"triton::TritonDialect\",\n-    \"mlir::gpu::GPUDialect\"\n+    \"mlir::gpu::GPUDialect\",\n+    \"tensor::TensorDialect\",\n   ];\n \n   let extraClassDeclaration = [{"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 9, "deletions": 44, "changes": 53, "file_content_changes": "@@ -10,6 +10,8 @@ include \"mlir/IR/OpBase.td\"\n include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n \n+def ResultsAreSharedEncoding: NativeOpTrait<\"ResultsAreSharedEncoding\">;\n+\n class TTG_Op<string mnemonic, list<Trait> traits = []> :\n     Op<TritonGPU_Dialect, mnemonic, traits>;\n \n@@ -75,7 +77,8 @@ def TTG_SelectOp : TTG_Op<\"select\", [NoSideEffect]> {\n \n \n def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n-                                    [SameVariadicOperandSize,\n+                                    [AttrSizedOperandSegments,\n+                                     ResultsAreSharedEncoding,\n                                      // MemoryEffects<[MemRead]>, doesn't work with CSE but seems like it should?\n                                      NoSideEffect,\n                                      TypesMatchWith<\"infer mask type from src type\",\n@@ -93,6 +96,10 @@ def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n       It returns a copy of `$dst` with the proper slice updated asynchronously with the value of `$src`.\n       This operation is non-blocking, and `$results` will have the updated value after the corresponding async_wait.\n \n+      When converting from `tt.load` to `triton_gpu.insert_slice_async`, the `$evict`, `$cache`, and `$isVolatile` fields\n+      might be ignored on certain hardware. For example, on NVIDIA GPUs, the cache policy is determined by the backend,\n+      and `$evict` and `$isVolatile` are ignored because they apply to L1 cache only.\n+\n       The insert_slice_async operation supports the following arguments:\n \n       * src: the tensor that is inserted.\n@@ -149,48 +156,9 @@ def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n   let parser = [{ return parseInsertSliceAsyncOp(parser, result); }];\n \n   let printer = [{ return printInsertSliceAsyncOp(p, *this); }];\n-\n-  // result needs to be of shared layout\n-  let verifier = [{ return ::verify(*this); }];\n }\n \n-def TTG_ExtractSliceOp : TTG_Op<\"extract_slice\", [NoSideEffect, InferTypeOpInterface]> {\n-  let summary = \"extract slice\";\n-  let description = [{\n-    The \"extract_slice\" operation extracts a `$result` tensor from a `$src` tensor as\n-    specified by the operation's `$index` and `$axis` arguments.\n-\n-    The extract_slice operation supports the following arguments:\n-\n-    * src: the tensor that is extracted from.\n-    * index: the index at the given `$axis` from which the `$src` tensor is extracted\n-\n-    Example:\n-\n-    ```\n-    // Rank-reducing extract_slice.\n-    %1 = tensor.extract_slice %0, %index {axis = 0} : tensor<8x16x4xf32> -> tensor<1x16x4xf32>\n-    ```\n-  }];\n-\n-  let arguments = (ins TT_Tensor:$src, I32:$index, I32Attr:$axis);\n-\n-  let results = (outs TT_Tensor:$result);\n-\n-  let assemblyFormat = [{$src `,` $index attr-dict `:` type($src) `->` type($result)}];\n-\n-  let extraClassDeclaration = [{\n-    static ::mlir::LogicalResult inferReturnTypes(::mlir::MLIRContext *context,\n-          ::llvm::Optional<::mlir::Location> location, ::mlir::ValueRange operands,\n-          ::mlir::DictionaryAttr attributes, ::mlir::RegionRange regions,\n-          ::llvm::SmallVectorImpl<::mlir::Type> &inferredReturnTypes);\n-  }];\n-\n-  // result needs to be of shared layout\n-  let verifier = [{ return ::verify(*this); }];\n-}\n-\n-def TTG_AllocTensorOp : TTG_Op<\"alloc_tensor\", [NoSideEffect]> {\n+def TTG_AllocTensorOp : TTG_Op<\"alloc_tensor\", [NoSideEffect, ResultsAreSharedEncoding]> {\n   let summary = \"allocate tensor\";\n \n   let description = [{\n@@ -203,9 +171,6 @@ def TTG_AllocTensorOp : TTG_Op<\"alloc_tensor\", [NoSideEffect]> {\n   let assemblyFormat = [{attr-dict `:` type($result)}];\n \n   let results = (outs TT_Tensor:$result);\n-\n-  // result needs to be of shared layout\n-  let verifier = [{ return ::verify(*this); }];\n }\n \n #endif"}, {"filename": "include/triton/Target/LLVMIR/LLVMIRTranslation.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -1,5 +1,6 @@\n #ifndef TRITON_TARGET_LLVMIRTRANSLATION_H\n #define TRITON_TARGET_LLVMIRTRANSLATION_H\n+#include \"llvm/ADT/StringRef.h\"\n #include <memory>\n #include <vector>\n \n@@ -29,6 +30,8 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n std::unique_ptr<llvm::Module>\n translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module);\n \n+bool linkExternLib(llvm::Module &module, llvm::StringRef path);\n+\n } // namespace triton\n } // namespace mlir\n "}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 13, "deletions": 12, "changes": 25, "file_content_changes": "@@ -1,4 +1,5 @@\n #include \"triton/Analysis/Alias.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n@@ -24,18 +25,18 @@ ChangeResult SharedMemoryAliasAnalysis::visitOperation(\n   if (maybeSharedAllocationOp(op)) {\n     // These ops may allocate a new shared memory buffer.\n     auto result = op->getResult(0);\n-    if (isSharedEncoding(result)) {\n-      // FIXME(Keren): extract and insert are always alias for now\n-      if (auto extractSliceOp = dyn_cast<triton::gpu::ExtractSliceOp>(op)) {\n-        // extract_slice %src, %index\n-        aliasInfo = AliasInfo(operands[0]->getValue());\n-      } else if (auto insertSliceOp =\n-                     dyn_cast<triton::gpu::InsertSliceAsyncOp>(op)) {\n-        // insert_slice_async %src, %dst, %index\n-        aliasInfo = AliasInfo(operands[1]->getValue());\n-      } else {\n-        aliasInfo.insert(result);\n-      }\n+    // FIXME(Keren): extract and insert are always alias for now\n+    if (auto extractSliceOp = dyn_cast<tensor::ExtractSliceOp>(op)) {\n+      // extract_slice %src\n+      aliasInfo = AliasInfo(operands[0]->getValue());\n+      pessimistic = false;\n+    } else if (auto insertSliceOp =\n+                   dyn_cast<triton::gpu::InsertSliceAsyncOp>(op)) {\n+      // insert_slice_async %src, %dst, %index\n+      aliasInfo = AliasInfo(operands[1]->getValue());\n+      pessimistic = false;\n+    } else if (isSharedEncoding(result)) {\n+      aliasInfo.insert(result);\n       pessimistic = false;\n     }\n   }"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -1,6 +1,7 @@\n #include \"triton/Analysis/Allocation.h\"\n #include \"mlir/Analysis/Liveness.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"triton/Analysis/Alias.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n@@ -76,13 +77,13 @@ SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op) {\n   auto srcShape = srcTy.getShape();\n   auto axis = op.axis();\n \n-  bool fast_reduce = axis == 1; // FIXME(Qingyi): The fastest-changing dimension\n+  bool fastReduce = axis == srcLayout.getOrder()[0];\n \n   SmallVector<unsigned> smemShape;\n   for (auto d : srcShape)\n     smemShape.push_back(d);\n \n-  if (fast_reduce) {\n+  if (fastReduce) {\n     unsigned sizeInterWarps = srcLayout.getWarpsPerCTA()[axis];\n     smemShape[axis] = sizeInterWarps;\n   } else {\n@@ -123,7 +124,7 @@ class AllocationAnalysis {\n     // For example: %a = scf.if -> yield\n     // %a must be allocated elsewhere by other operations.\n     // FIXME(Keren): extract and insert are always alias for now\n-    if (!maybeSharedAllocationOp(op) || isa<triton::gpu::ExtractSliceOp>(op) ||\n+    if (!maybeSharedAllocationOp(op) || isa<tensor::ExtractSliceOp>(op) ||\n         isa<triton::gpu::InsertSliceAsyncOp>(op)) {\n       return;\n     }"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -2,6 +2,7 @@\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n #include \"mlir/Dialect/GPU/GPUDialect.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n \n namespace mlir {\n \n@@ -43,8 +44,7 @@ void MembarAnalysis::dfsOperation(Operation *operation,\n void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n                               OpBuilder *builder) {\n   if (isa<scf::ForOp>(op) || isa<scf::IfOp>(op) || isa<scf::YieldOp>(op) ||\n-      isa<triton::gpu::ExtractSliceOp>(op) ||\n-      isa<triton::gpu::AllocTensorOp>(op)) {\n+      isa<tensor::ExtractSliceOp>(op) || isa<triton::gpu::AllocTensorOp>(op)) {\n     // Do not insert barriers before control flow operations and\n     // alloc/extract/insert\n     // alloc is an allocation op without memory write."}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -24,7 +24,8 @@ bool maybeSharedAllocationOp(Operation *op) {\n               mlir::TypeID::get<triton::gpu::TritonGPUDialect>() ||\n           dialect->getTypeID() == mlir::TypeID::get<triton::TritonDialect>() ||\n           dialect->getTypeID() ==\n-              mlir::TypeID::get<arith::ArithmeticDialect>());\n+              mlir::TypeID::get<arith::ArithmeticDialect>() ||\n+          dialect->getTypeID() == mlir::TypeID::get<tensor::TensorDialect>());\n }\n \n std::string getValueOperandName(Value value, AsmState &state) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/PtxAsmFormat.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -125,7 +125,7 @@ std::string PTXBuilder::dump() const {\n     lines.push_back(exec->dump());\n   }\n \n-  return strJoin(lines, \"\\r\\n\");\n+  return strJoin(lines, \"\\n\\t\");\n }\n \n PTXInstrExecution &PTXInstrCommon::call(ArrayRef<Operand *> oprs) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 809, "deletions": 414, "changes": 1223, "file_content_changes": "@@ -11,6 +11,7 @@\n #include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n #include \"mlir/Dialect/GPU/GPUDialect.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"mlir/Transforms/DialectConversion.h\"\n@@ -50,7 +51,8 @@ static StringRef getStructAttrsAttrName() { return \"llvm.struct_attrs\"; }\n namespace {\n \n // Create a 32-bit integer constant.\n-Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v) {\n+static Value createConstantI32(Location loc, PatternRewriter &rewriter,\n+                               int32_t v) {\n   auto i32ty = rewriter.getIntegerType(32);\n   return rewriter.create<LLVM::ConstantOp>(loc, i32ty,\n                                            IntegerAttr::get(i32ty, v));\n@@ -63,24 +65,27 @@ Value createConstantF32(Location loc, PatternRewriter &rewriter, float v) {\n }\n \n // Create an index type constant.\n-Value createIndexConstant(OpBuilder &builder, Location loc,\n-\n-                          TypeConverter *converter, int64_t value) {\n+static Value createIndexConstant(OpBuilder &builder, Location loc,\n+                                 TypeConverter *converter, int64_t value) {\n   Type ty = converter->convertType(builder.getIndexType());\n   return builder.create<LLVM::ConstantOp>(loc, ty,\n                                           builder.getIntegerAttr(ty, value));\n }\n \n // Create an integer constant of \\param width bits.\n-Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n-                                int64_t value) {\n+static Value createLLVMIntegerConstant(OpBuilder &builder, Location loc,\n+                                       short width, int64_t value) {\n   Type ty = builder.getIntegerType(width);\n   return builder.create<LLVM::ConstantOp>(loc, ty,\n                                           builder.getIntegerAttr(ty, value));\n }\n \n } // namespace\n \n+// A helper function for using printf in LLVM conversion.\n+void llPrintf(StringRef msg, ValueRange args,\n+              ConversionPatternRewriter &rewriter);\n+\n // Shortcuts for some commonly used LLVM ops to keep code simple and intuitive\n #define zext(...) rewriter.create<LLVM::ZExtOp>(loc, __VA_ARGS__)\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n@@ -138,6 +143,10 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n   LLVM::createIndexConstant(rewriter, loc, this->getTypeConverter(),           \\\n                             __VA_ARGS__)\n \n+// Helper function\n+#define tid_val() getThreadId(rewriter, loc)\n+#define llprintf(fmt, ...) LLVM::llPrintf(fmt, {__VA_ARGS__}, rewriter)\n+\n } // namespace LLVM\n } // namespace mlir\n \n@@ -338,6 +347,7 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n   return llvmStruct;\n }\n \n+// Delinearize on compile-time consts, assuming the order is [n, .. 2, 1, 0]\n template <typename T>\n static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n   // shape: {a, b, c, d}  ->  accMul: {b*c*d, c*d, d, 1}\n@@ -355,6 +365,7 @@ static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n   return multiDimIndex;\n }\n \n+// Linearize on compile-time consts, assuming the order is [n, .. 2, 1, 0]\n template <typename T>\n static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   assert(multiDimIndex.size() == shape.size());\n@@ -371,8 +382,8 @@ static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   return linearIndex;\n }\n \n-Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n-                  Value val, Value pred) {\n+static Value storeShared(ConversionPatternRewriter &rewriter, Location loc,\n+                         Value ptr, Value val, Value pred) {\n   MLIRContext *ctx = rewriter.getContext();\n   unsigned bits = val.getType().getIntOrFloatBitWidth();\n   const char *c = bits == 64 ? \"l\" : (bits == 16 ? \"h\" : \"r\");\n@@ -385,6 +396,50 @@ Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n   return builder.launch(rewriter, loc, void_ty(ctx));\n }\n \n+struct SharedMemoryObject {\n+  Value base; // i32 ptr. The start address of the shared memory object.\n+  // We need to store strides as Values but not integers because the\n+  // extract_slice instruction can take a slice at artibary offsets.\n+  // Take $a[16:32, 16:32] as an example, though we know the stride of $a[0] is\n+  // 32, we need to let the instruction that uses $a to be aware of that.\n+  // Otherwise, when we use $a, we only know that the shape of $a is 16x16. If\n+  // we store strides into an attribute array of integers, the information\n+  // cannot pass through block argument assignment because attributes are\n+  // associated with operations but not Values.\n+  // TODO(Keren): We may need to figure out a way to store strides as integers\n+  // if we want to support more optimizations.\n+  SmallVector<Value>\n+      strides; // i32 int. The strides of the shared memory object.\n+\n+  SharedMemoryObject(Value base, ArrayRef<Value> strides)\n+      : base(base), strides(strides.begin(), strides.end()) {}\n+\n+  SharedMemoryObject(Value base, ArrayRef<int64_t> shape, Location loc,\n+                     ConversionPatternRewriter &rewriter)\n+      : base(base) {\n+    auto stride = 1;\n+    for (auto dim : llvm::reverse(shape)) {\n+      this->strides.emplace_back(i32_val(stride));\n+      stride *= dim;\n+    }\n+    this->strides = llvm::to_vector<4>(llvm::reverse(this->strides));\n+  }\n+\n+  SmallVector<Value> getElems() const {\n+    SmallVector<Value> elems;\n+    elems.push_back(base);\n+    elems.append(strides.begin(), strides.end());\n+    return elems;\n+  }\n+\n+  SmallVector<Type> getTypes() const {\n+    SmallVector<Type> types;\n+    types.push_back(base.getType());\n+    types.append(strides.size(), IntegerType::get(base.getContext(), 32));\n+    return types;\n+  }\n+};\n+\n struct ConvertTritonGPUOpToLLVMPatternBase {\n   static SmallVector<Value>\n   getElementsFromStruct(Location loc, Value llvmStruct,\n@@ -466,12 +521,12 @@ class ConvertTritonGPUOpToLLVMPattern\n       multiDim[0] = linear;\n     } else {\n       Value remained = linear;\n-      for (auto &&en : llvm::enumerate(llvm::reverse(shape.drop_front()))) {\n+      for (auto &&en : llvm::enumerate(shape.drop_back())) {\n         Value dimSize = idx_val(en.value());\n-        multiDim[rank - 1 - en.index()] = urem(remained, dimSize);\n+        multiDim[en.index()] = urem(remained, dimSize);\n         remained = udiv(remained, dimSize);\n       }\n-      multiDim[0] = remained;\n+      multiDim[rank - 1] = remained;\n     }\n     return multiDim;\n   }\n@@ -481,16 +536,26 @@ class ConvertTritonGPUOpToLLVMPattern\n     int rank = multiDim.size();\n     Value linear = idx_val(0);\n     if (rank > 0) {\n-      linear = multiDim.front();\n+      linear = multiDim.back();\n       for (auto [dim, shape] :\n-           llvm::zip(multiDim.drop_front(), shape.drop_front())) {\n+           llvm::reverse(llvm::zip(multiDim.drop_back(), shape.drop_back()))) {\n         Value dimSize = idx_val(shape);\n         linear = add(mul(linear, dimSize), dim);\n       }\n     }\n     return linear;\n   }\n \n+  Value dot(ConversionPatternRewriter &rewriter, Location loc,\n+            ArrayRef<Value> offsets, ArrayRef<Value> strides) const {\n+    assert(offsets.size() == strides.size());\n+    Value ret = idx_val(0);\n+    for (auto [offset, stride] : llvm::zip(offsets, strides)) {\n+      ret = add(ret, mul(offset, stride));\n+    }\n+    return ret;\n+  }\n+\n   // Get an index-base for each dimension for a \\param blocked_layout.\n   SmallVector<Value>\n   emitBaseIndexForBlockedLayout(Location loc,\n@@ -512,6 +577,7 @@ class ConvertTritonGPUOpToLLVMPattern\n         delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n     SmallVector<Value> multiDimThreadId =\n         delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n+\n     SmallVector<Value> multiDimBase(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n       // Wrap around multiDimWarpId/multiDimThreadId incase\n@@ -673,6 +739,25 @@ class ConvertTritonGPUOpToLLVMPattern\n     return base;\n   }\n \n+  static SharedMemoryObject\n+  getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n+                                  ConversionPatternRewriter &rewriter) {\n+    auto elems = getElementsFromStruct(loc, llvmStruct, rewriter);\n+    return SharedMemoryObject(/*base=*/elems[0],\n+                              /*strides=*/{elems.begin() + 1, elems.end()});\n+  }\n+\n+  static Value\n+  getStructFromSharedMemoryObject(Location loc,\n+                                  const SharedMemoryObject &smemObj,\n+                                  ConversionPatternRewriter &rewriter) {\n+    auto elems = smemObj.getElems();\n+    auto types = smemObj.getTypes();\n+    auto structTy =\n+        LLVM::LLVMStructType::getLiteral(rewriter.getContext(), types);\n+    return getStructFromElements(loc, elems, rewriter, structTy);\n+  }\n+\n protected:\n   const Allocation *allocation;\n   Value smem;\n@@ -967,7 +1052,7 @@ struct LoadOpConversion\n       if (other) {\n         for (size_t ii = 0; ii < nWords; ++ii) {\n           PTXInstr &mov = *ptxBuilder.create<>(\"mov\");\n-          mov.o(\"u\", width);\n+          mov.o(\"u\" + std::to_string(width));\n \n           size_t size = width / valueElemNbits;\n \n@@ -1291,7 +1376,9 @@ struct ReduceOpConversion\n LogicalResult\n ReduceOpConversion::matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter) const {\n-  if (op.axis() == 1) // FIXME(Qingyi): The fastest-changing dimension\n+  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+  auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  if (op.axis() == srcLayout.getOrder()[0])\n     return matchAndRewriteFast(op, adaptor, rewriter);\n   return matchAndRewriteBasic(op, adaptor, rewriter);\n }\n@@ -1373,6 +1460,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n \n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n   auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto srcOrd = srcLayout.getOrder();\n   auto srcShape = srcTy.getShape();\n \n   auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n@@ -1416,16 +1504,21 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     SmallVector<Value> writeIdx = indices[key];\n \n     writeIdx[axis] = udiv(writeIdx[axis], sizePerThread);\n-    Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n+    Value writeOffset =\n+        linearize(rewriter, loc, reorder<Value>(writeIdx, srcOrd),\n+                  reorder<unsigned>(smemShape, srcOrd));\n     Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n     store(acc, writePtr);\n \n     SmallVector<Value> readIdx(writeIdx.size(), ints[0]);\n     for (int N = smemShape[axis] / 2; N > 0; N >>= 1) {\n       readIdx[axis] = ints[N];\n       Value readMask = icmp_slt(writeIdx[axis], ints[N]);\n-      Value readOffset = select(\n-          readMask, linearize(rewriter, loc, readIdx, smemShape), ints[0]);\n+      Value readOffset =\n+          select(readMask,\n+                 linearize(rewriter, loc, reorder<Value>(readIdx, srcOrd),\n+                           reorder<unsigned>(smemShape, srcOrd)),\n+                 ints[0]);\n       Value readPtr = gep(elemPtrTy, writePtr, readOffset);\n       barrier();\n       accumulate(rewriter, loc, op.redOp(), acc, load(readPtr), false);\n@@ -1448,7 +1541,9 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     for (unsigned i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       readIdx.insert(readIdx.begin() + axis, ints[0]);\n-      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n+      Value readOffset =\n+          linearize(rewriter, loc, reorder<Value>(readIdx, srcOrd),\n+                    reorder<unsigned>(smemShape, srcOrd));\n       Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n       resultVals[i] = load(readPtr);\n     }\n@@ -1477,6 +1572,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n   auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n   auto srcShape = srcTy.getShape();\n+  auto srcRank = srcTy.getRank();\n \n   auto threadsPerWarp = srcLayout.getThreadsPerWarp();\n   auto warpsPerCTA = srcLayout.getWarpsPerCTA();\n@@ -1521,6 +1617,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n       delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n   SmallVector<Value> multiDimWarpId =\n       delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+\n   Value laneIdAxis = multiDimLaneId[axis];\n   Value warpIdAxis = multiDimWarpId[axis];\n \n@@ -1538,56 +1635,77 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n       accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n     }\n \n-    if (sizeInterWarps == 1) {\n-      SmallVector<Value> writeIdx = indices[key];\n-      writeIdx[axis] = zero;\n-      Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n-      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n-      storeShared(rewriter, loc, writePtr, acc, laneZero);\n-    } else {\n-      SmallVector<Value> writeIdx = indices[key];\n-      writeIdx[axis] =\n-          warpIdAxis; // axis must be the fastest-changing dimension\n-      Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n-      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n-      storeShared(rewriter, loc, writePtr, acc, laneZero);\n-      barrier();\n+    SmallVector<Value> writeIdx = indices[key];\n+    writeIdx[axis] = (sizeInterWarps == 1) ? zero : warpIdAxis;\n+    Value writeOffset =\n+        linearize(rewriter, loc, reorder<Value>(writeIdx, order),\n+                  reorder<unsigned>(smemShape, order));\n+    Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+    storeShared(rewriter, loc, writePtr, acc, laneZero);\n+  }\n \n-      SmallVector<Value> readIdx = writeIdx;\n-      readIdx[axis] = urem(laneId, i32_val(sizeInterWarps));\n-      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n-      Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n-      acc = load(readPtr);\n+  barrier();\n \n-      // reduce across warps\n-      for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n-        Value shfl = shflSync(rewriter, loc, acc, N);\n-        accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n-      }\n+  // the second round of shuffle reduction\n+  //   now the problem size: sizeInterWarps, s1, s2, .. , sn  =>\n+  //                                      1, s1, s2, .. , sn\n+  //   where sizeInterWarps is 2^m\n+  //\n+  // each thread needs to process:\n+  //   elemsPerThread = sizeInterWarps * s1 * s2 .. Sn / numThreads\n+  unsigned elems = product<unsigned>(smemShape);\n+  unsigned numThreads = product<unsigned>(srcLayout.getWarpsPerCTA()) * 32;\n+  unsigned elemsPerThread = std::max<unsigned>(elems / numThreads, 1);\n+  Value readOffset = threadId;\n+  for (unsigned round = 0; round < elemsPerThread; ++round) {\n+    Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n+    Value acc = load(readPtr);\n+\n+    for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n+      Value shfl = shflSync(rewriter, loc, acc, N);\n+      accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n+    }\n \n-      writeIdx[axis] = zero;\n-      writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n-      writePtr = gep(elemPtrTy, smemBase, writeOffset);\n-      storeShared(rewriter, loc, writePtr, acc, and_(laneZero, warpZero));\n+    Value writeOffset = udiv(readOffset, i32_val(sizeInterWarps));\n+    Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+    Value threadIsNeeded = icmp_slt(threadId, i32_val(elems));\n+    Value laneIdModSizeInterWarps = urem(laneId, i32_val(sizeInterWarps));\n+    Value laneIdModSizeInterWarpsIsZero =\n+        icmp_eq(laneIdModSizeInterWarps, zero);\n+    storeShared(rewriter, loc, writePtr, acc,\n+                and_(threadIsNeeded, laneIdModSizeInterWarpsIsZero));\n+\n+    if (round != elemsPerThread - 1) {\n+      readOffset = add(readOffset, i32_val(numThreads));\n     }\n   }\n \n+  // We could avoid this barrier in some of the layouts, however this is not\n+  // the general case. TODO: optimize the barrier incase the layouts are\n+  // accepted.\n+  barrier();\n+\n   // set output values\n   if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n     // nd-tensor where n >= 1\n     auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n     auto resultShape = resultTy.getShape();\n+    SmallVector<unsigned> resultOrd;\n+    for (auto ord : order) {\n+      if (ord != 0)\n+        resultOrd.push_back(ord - 1);\n+    }\n \n     unsigned resultElems = getElemsPerThread(resultTy);\n     auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultShape);\n     assert(resultIndices.size() == resultElems);\n \n-    barrier();\n     SmallVector<Value> resultVals(resultElems);\n     for (size_t i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n-      readIdx.insert(readIdx.begin() + axis, i32_val(0));\n-      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n+      Value readOffset =\n+          linearize(rewriter, loc, reorder<Value>(readIdx, resultOrd),\n+                    reorder<int64_t, unsigned>(resultShape, resultOrd));\n       Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n       resultVals[i] = load(readPtr);\n     }\n@@ -1599,7 +1717,6 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n     rewriter.replaceOp(op, ret);\n   } else {\n     // 0d-tensor -> scalar\n-    barrier();\n     Value resultVal = load(smemBase);\n     rewriter.replaceOp(op, resultVal);\n   }\n@@ -1636,6 +1753,191 @@ struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n   }\n };\n \n+struct PrintfOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintfOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::PrintfOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::PrintfOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op->getLoc();\n+    SmallVector<Value, 16> operands;\n+    for (auto operand : adaptor.getOperands()) {\n+      auto sub_operands = this->getElementsFromStruct(loc, operand, rewriter);\n+      for (auto elem : sub_operands) {\n+        operands.push_back(elem);\n+      }\n+    }\n+    std::string formatStr;\n+    llvm::raw_string_ostream os(formatStr);\n+    os << op.prefix();\n+    if (operands.size() > 0) {\n+      os << getFormatSubstr(operands[0]);\n+    }\n+\n+    for (size_t i = 1; i < operands.size(); ++i) {\n+      os << \", \" << getFormatSubstr(operands[i]);\n+    }\n+    llPrintf(formatStr, operands, rewriter);\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+  // get format specific for each input value\n+  // currently support pointer, i8, i16, i32, i64, f16, bf16, f32, f64\n+  std::string getFormatSubstr(Value value) const {\n+    Type type = value.getType();\n+    unsigned width = type.getIntOrFloatBitWidth();\n+\n+    if (type.isa<LLVM::LLVMPointerType>()) {\n+      return \"%p\";\n+    } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n+      return \"%f\";\n+    } else if (type.isSignedInteger()) {\n+      return \"%i\";\n+    } else if (type.isUnsignedInteger() || type.isSignlessInteger()) {\n+      return \"%u\";\n+    }\n+    assert(false && \"not supported type\");\n+  }\n+\n+  // declare vprintf(i8*, i8*) as external function\n+  static LLVM::LLVMFuncOp\n+  getVprintfDeclaration(ConversionPatternRewriter &rewriter) {\n+    auto moduleOp =\n+        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    StringRef funcName(\"vprintf\");\n+    Operation *funcOp = moduleOp.lookupSymbol(funcName);\n+    if (funcOp)\n+      return cast<LLVM::LLVMFuncOp>(*funcOp);\n+\n+    auto *context = rewriter.getContext();\n+\n+    SmallVector<Type> argsType{ptr_ty(IntegerType::get(context, 8)),\n+                               ptr_ty(IntegerType::get(context, 8))};\n+    auto funcType = LLVM::LLVMFunctionType::get(i32_ty, argsType);\n+\n+    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+    rewriter.setInsertionPointToStart(moduleOp.getBody());\n+\n+    return rewriter.create<LLVM::LLVMFuncOp>(UnknownLoc::get(context), funcName,\n+                                             funcType);\n+  }\n+\n+  // extend integer to int32, extend float to float64\n+  // this comes from vprintf alignment requirements.\n+  static std::pair<Type, Value>\n+  promoteValue(ConversionPatternRewriter &rewriter, Value value) {\n+    auto *context = rewriter.getContext();\n+    auto type = value.getType();\n+    type.dump();\n+    unsigned width = type.getIntOrFloatBitWidth();\n+    Value newOp = value;\n+    Type newType = type;\n+\n+    bool bUnsigned = type.isUnsignedInteger();\n+    if (type.isIntOrIndex() && width < 32) {\n+      if (bUnsigned) {\n+        newType = ui32_ty;\n+        newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n+                                              value);\n+      } else {\n+        newType = i32_ty;\n+        newOp = rewriter.create<LLVM::SExtOp>(UnknownLoc::get(context), newType,\n+                                              value);\n+      }\n+    } else if (type.isBF16() || type.isF16() || type.isF32()) {\n+      newType = f64_ty;\n+      newOp = rewriter.create<LLVM::FPExtOp>(UnknownLoc::get(context), newType,\n+                                             value);\n+    }\n+\n+    return {newType, newOp};\n+  }\n+\n+  static void llPrintf(StringRef msg, ValueRange args,\n+                       ConversionPatternRewriter &rewriter) {\n+    static const char formatStringPrefix[] = \"printfFormat_\";\n+    assert(!msg.empty() && \"printf with empty string not support\");\n+    Type int8Ptr = ptr_ty(i8_ty);\n+\n+    auto *context = rewriter.getContext();\n+    auto moduleOp =\n+        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    auto funcOp = getVprintfDeclaration(rewriter);\n+\n+    Value one = rewriter.create<LLVM::ConstantOp>(\n+        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(1));\n+    Value zero = rewriter.create<LLVM::ConstantOp>(\n+        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(0));\n+\n+    unsigned stringNumber = 0;\n+    SmallString<16> stringConstName;\n+    do {\n+      stringConstName.clear();\n+      (formatStringPrefix + Twine(stringNumber++)).toStringRef(stringConstName);\n+    } while (moduleOp.lookupSymbol(stringConstName));\n+\n+    llvm::SmallString<64> formatString(msg);\n+    formatString.push_back('\\n');\n+    formatString.push_back('\\0');\n+    size_t formatStringSize = formatString.size_in_bytes();\n+    auto globalType = LLVM::LLVMArrayType::get(i8_ty, formatStringSize);\n+\n+    LLVM::GlobalOp global;\n+    {\n+      ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+      rewriter.setInsertionPointToStart(moduleOp.getBody());\n+      global = rewriter.create<LLVM::GlobalOp>(\n+          UnknownLoc::get(context), globalType,\n+          /*isConstant=*/true, LLVM::Linkage::Internal, stringConstName,\n+          rewriter.getStringAttr(formatString));\n+    }\n+\n+    Value globalPtr =\n+        rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(context), global);\n+    Value stringStart =\n+        rewriter.create<LLVM::GEPOp>(UnknownLoc::get(context), int8Ptr,\n+                                     globalPtr, mlir::ValueRange({zero, zero}));\n+\n+    Value bufferPtr =\n+        rewriter.create<LLVM::NullOp>(UnknownLoc::get(context), int8Ptr);\n+\n+    SmallVector<Value, 16> newArgs;\n+    if (args.size() >= 1) {\n+      SmallVector<Type> argTypes;\n+      for (auto arg : args) {\n+        Type newType;\n+        Value newArg;\n+        std::tie(newType, newArg) = promoteValue(rewriter, arg);\n+        argTypes.push_back(newType);\n+        newArgs.push_back(newArg);\n+      }\n+\n+      Type structTy = LLVM::LLVMStructType::getLiteral(context, argTypes);\n+      auto allocated = rewriter.create<LLVM::AllocaOp>(UnknownLoc::get(context),\n+                                                       ptr_ty(structTy), one,\n+                                                       /*alignment=*/0);\n+\n+      for (const auto &entry : llvm::enumerate(newArgs)) {\n+        auto index = rewriter.create<LLVM::ConstantOp>(\n+            UnknownLoc::get(context), i32_ty,\n+            rewriter.getI32IntegerAttr(entry.index()));\n+        auto fieldPtr = rewriter.create<LLVM::GEPOp>(\n+            UnknownLoc::get(context), ptr_ty(argTypes[entry.index()]),\n+            allocated, ArrayRef<Value>{zero, index});\n+        rewriter.create<LLVM::StoreOp>(UnknownLoc::get(context), entry.value(),\n+                                       fieldPtr);\n+      }\n+      bufferPtr = rewriter.create<LLVM::BitcastOp>(UnknownLoc::get(context),\n+                                                   int8Ptr, allocated);\n+    }\n+\n+    ValueRange operands{stringStart, bufferPtr};\n+    rewriter.create<LLVM::CallOp>(UnknownLoc::get(context), funcOp, operands);\n+  }\n+};\n+\n struct MakeRangeOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp> {\n \n@@ -1699,9 +2001,6 @@ struct AddPtrOpConversion\n     auto resultTy = op.getType();\n     auto resultTensorTy = resultTy.dyn_cast<RankedTensorType>();\n     if (resultTensorTy) {\n-      auto resultLayout =\n-          resultTensorTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n-      assert(resultLayout && \"Unexpected resultLayout in AddPtrOpConversion\");\n       unsigned elems = getElemsPerThread(resultTy);\n       Type elemTy =\n           getTypeConverter()->convertType(resultTensorTy.getElementType());\n@@ -1738,46 +2037,63 @@ struct AllocTensorOpConversion\n     auto resultTy = op.getType().dyn_cast<RankedTensorType>();\n     auto llvmElemTy =\n         getTypeConverter()->convertType(resultTy.getElementType());\n-    auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n-    Value resultVal =\n-        rewriter.create<LLVM::BitcastOp>(loc, elemPtrTy, smemBase);\n-    rewriter.replaceOp(op, resultVal);\n+    auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n+    smemBase = bitcast(smemBase, elemPtrTy);\n+    auto smemObj =\n+        SharedMemoryObject(smemBase, resultTy.getShape(), loc, rewriter);\n+    auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n+    rewriter.replaceOp(op, retVal);\n     return success();\n   }\n };\n \n struct ExtractSliceOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ExtractSliceOp> {\n+    : public ConvertTritonGPUOpToLLVMPattern<tensor::ExtractSliceOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n-      triton::gpu::ExtractSliceOp>::ConvertTritonGPUOpToLLVMPattern;\n+      tensor::ExtractSliceOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   LogicalResult\n-  matchAndRewrite(triton::gpu::ExtractSliceOp op, OpAdaptor adaptor,\n+  matchAndRewrite(tensor::ExtractSliceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n+    // %dst = extract_slice %src[%offsets]\n     Location loc = op->getLoc();\n-    auto srcTy = op.src().getType().dyn_cast<RankedTensorType>();\n+    auto srcTy = op.source().getType().dyn_cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n     assert(srcLayout && \"Unexpected resultLayout in ExtractSliceOpConversion\");\n-\n-    // axis > 0 will result in non-contiguous memory access if the result\n-    // tensor is an alias of the source tensor.\n-    auto axis = op->getAttrOfType<IntegerAttr>(\"axis\").getInt();\n-    assert(axis == 0 && \"extract_slice: Only axis=0 is supported for now\");\n-\n-    // Example:\n-    // %dst = extract_slice %src, %index {axis = 0}\n-    // src.shape = [11, 2, 3, 4, 1]\n-    // offset = %index * 2 * 3 * 4 * 1\n-    auto dstTy = op.getType().dyn_cast<RankedTensorType>();\n-    auto base = product<int64_t>(dstTy.getShape());\n-    auto baseVal = createIndexAttrConstant(\n-        rewriter, loc, getTypeConverter()->getIndexType(), base);\n-    Value offset = mul(adaptor.index(), baseVal);\n-\n-    auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n-    auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n-    Value resultVal = gep(elemPtrTy, adaptor.src(), offset);\n-    rewriter.replaceOp(op, resultVal);\n+    assert(op.hasUnitStride() &&\n+           \"Only unit stride supported by ExtractSliceOpConversion\");\n+\n+    // newBase = base + offset\n+    // Triton support either static and dynamic offsets\n+    auto smemObj =\n+        getSharedMemoryObjectFromStruct(loc, adaptor.source(), rewriter);\n+    SmallVector<Value, 4> offsetVals;\n+    auto mixedOffsets = op.getMixedOffsets();\n+    for (auto i = 0; i < mixedOffsets.size(); ++i) {\n+      if (op.isDynamicOffset(i))\n+        offsetVals.emplace_back(adaptor.offsets()[i]);\n+      else\n+        offsetVals.emplace_back(i32_val(op.getStaticOffset(i)));\n+    }\n+    // Compute the offset based on the original strides of the shared memory\n+    // object\n+    auto offset = dot(rewriter, loc, offsetVals, smemObj.strides);\n+    // newShape = rank_reduce(shape)\n+    // Triton only supports static tensor sizes\n+    SmallVector<Value, 4> strideVals;\n+    auto staticSizes = op.static_sizes();\n+    for (auto i = 0; i < op.static_sizes().size(); ++i) {\n+      if (op.getStaticSize(i) != 1) {\n+        strideVals.emplace_back(smemObj.strides[i]);\n+      }\n+    }\n+    auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n+    auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n+    auto resTy = op.getType().dyn_cast<RankedTensorType>();\n+    smemObj =\n+        SharedMemoryObject(gep(elemPtrTy, smemObj.base, offset), strideVals);\n+    auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n+    rewriter.replaceOp(op, retVal);\n     return success();\n   }\n };\n@@ -1977,9 +2293,9 @@ struct FpToFpOpConversion\n         \"max.u32 nosign_1_1, nosign_1_1, 0x3800;      \\n\"\n         \"min.u32 nosign_1_1, nosign_1_1, 0x3ff0;      \\n\"\n         \"or.b32 nosign1, nosign_1_0, nosign_1_1;      \\n\"\n-        \"add.u32 nosign0, nosign0, rn_;               \\n\"  // round to nearest zero\n+        \"add.u32 nosign0, nosign0, rn_;               \\n\"\n         \"add.u32 nosign1, nosign1, rn_;               \\n\"\n-        \"sub.u32 nosign0, nosign0, 0x38003800;        \\n\"  // compensate offset\n+        \"sub.u32 nosign0, nosign0, 0x38003800;        \\n\"\n         \"sub.u32 nosign1, nosign1, 0x38003800;        \\n\"\n         \"shr.u32 nosign0, nosign0, 4;                 \\n\"\n         \"shr.u32 nosign1, nosign1, 4;                 \\n\"\n@@ -2309,17 +2625,6 @@ struct ConvertLayoutOpConversion\n   }\n \n private:\n-  template <typename T>\n-  SmallVector<T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) const {\n-    size_t rank = order.size();\n-    assert(input.size() == rank);\n-    SmallVector<T> result(rank);\n-    for (auto it : llvm::enumerate(order)) {\n-      result[rank - 1 - it.value()] = input[it.index()];\n-    }\n-    return result;\n-  };\n-\n   // shared memory rd/st for blocked or mma layout with data padding\n   void processReplica(Location loc, ConversionPatternRewriter &rewriter,\n                       bool stNotRd, RankedTensorType type,\n@@ -2590,8 +2895,9 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n   Value src = op.src();\n   Value dst = op.result();\n   auto srcTy = src.getType().cast<RankedTensorType>();\n-  auto dstTy = dst.getType().cast<RankedTensorType>();\n   auto srcShape = srcTy.getShape();\n+  auto dstTy = dst.getType().cast<RankedTensorType>();\n+  auto dstShape = dstTy.getShape();\n   assert(srcShape.size() == 2 &&\n          \"Unexpected rank of ConvertLayout(blocked->shared)\");\n   auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n@@ -2637,6 +2943,8 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n   Value smemBase = getSharedMemoryBase(loc, rewriter, dst);\n   auto elemPtrTy = ptr_ty(getTypeConverter()->convertType(elemTy), 3);\n   smemBase = bitcast(smemBase, elemPtrTy);\n+  auto smemObj = SharedMemoryObject(smemBase, dstShape, loc, rewriter);\n+  auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n   unsigned numWordsEachRep = product<unsigned>(wordsInEachRep);\n   SmallVector<Value> wordVecs(numWordsEachRep);\n   // TODO: We should get less barriers if it is handled by membar pass\n@@ -2697,8 +3005,10 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n       }\n     }\n   }\n-  barrier();\n-  rewriter.replaceOp(op, smemBase);\n+  // Barrier is not necessary.\n+  // The membar pass knows that it writes to shared memory and will handle it\n+  // properly.\n+  rewriter.replaceOp(op, retVal);\n   return success();\n }\n \n@@ -2708,9 +3018,10 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n class MMA16816SmemLoader {\n public:\n   MMA16816SmemLoader(int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n-                     ArrayRef<int64_t> tileShape, ArrayRef<int> instrShape,\n-                     ArrayRef<int> matShape, int perPhase, int maxPhase,\n-                     int elemBytes, ConversionPatternRewriter &rewriter,\n+                     ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n+                     ArrayRef<int> instrShape, ArrayRef<int> matShape,\n+                     int perPhase, int maxPhase, int elemBytes,\n+                     ConversionPatternRewriter &rewriter,\n                      TypeConverter *typeConverter, const Location &loc)\n       : order(order.begin(), order.end()), kOrder(kOrder),\n         tileShape(tileShape.begin(), tileShape.end()),\n@@ -2721,8 +3032,8 @@ class MMA16816SmemLoader {\n     cMatShape = matShape[order[0]];\n     sMatShape = matShape[order[1]];\n \n-    cTileStride = tileShape[order[1]];\n-    sTileStride = tileShape[order[0]];\n+    cTileStride = smemStrides[order[0]];\n+    sTileStride = smemStrides[order[1]];\n \n     // rule: k must be the fast-changing axis.\n     needTrans = kOrder != order[0];\n@@ -2825,8 +3136,7 @@ class MMA16816SmemLoader {\n     for (int i = 0; i < numPtr; ++i) {\n       Value cMatOffI = add(cMatOff, i32_val(i * pLoadStrideInMat));\n       cMatOffI = xor_(cMatOffI, phase);\n-      offs[i] = add(mul(cMatOffI, i32_val(cMatShape)),\n-                    mul(sOff, i32_val(sTileStride)));\n+      offs[i] = add(mul(cMatOffI, i32_val(cMatShape)), mul(sOff, sTileStride));\n     }\n \n     return offs;\n@@ -2862,7 +3172,7 @@ class MMA16816SmemLoader {\n         Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n         cOff = urem(cOff, i32_val(tileShape[order[0]]));\n         sOff = urem(sOff, i32_val(tileShape[order[1]]));\n-        offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, i32_val(sTileStride)));\n+        offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, sTileStride));\n       }\n     }\n     return offs;\n@@ -2902,7 +3212,7 @@ class MMA16816SmemLoader {\n           // To prevent out-of-bound access when tile is too small.\n           cOff = urem(cOff, i32_val(tileShape[order[0]]));\n           sOff = urem(sOff, i32_val(tileShape[order[1]]));\n-          offs[ptrOff] = add(cOff, mul(sOff, i32_val(sTileStride)));\n+          offs[ptrOff] = add(cOff, mul(sOff, sTileStride));\n         }\n       }\n     }\n@@ -2936,14 +3246,15 @@ class MMA16816SmemLoader {\n     Value ptr = getPtr(ptrIdx);\n \n     if (canUseLdmatrix) {\n-      int sOffset =\n-          matIdx[order[1]] * sMatStride * sMatShape * sTileStride * elemBytes;\n-      PTXBuilder builder;\n+      Value sOffset =\n+          mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sTileStride);\n+      Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n \n+      PTXBuilder builder;\n       // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n       // thread.\n       auto resArgs = builder.newListOperand(4, \"=r\");\n-      auto addrArg = builder.newAddrOperand(ptr, \"r\", sOffset);\n+      auto addrArg = builder.newAddrOperand(sOffsetPtr, \"r\");\n \n       auto ldmatrix = builder.create(\"ldmatrix.sync.aligned.m8n8.x4\")\n                           ->o(\"trans\", needTrans /*predicate*/)\n@@ -2958,40 +3269,44 @@ class MMA16816SmemLoader {\n         return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n       };\n \n-      Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n+      // The struct should have exactly the same element types.\n+      Type elemType = resV4.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n \n-      return {extract_val(fp16x2Ty, resV4, getIntAttr(0)),\n-              extract_val(fp16x2Ty, resV4, getIntAttr(1)),\n-              extract_val(fp16x2Ty, resV4, getIntAttr(2)),\n-              extract_val(fp16x2Ty, resV4, getIntAttr(3))};\n+      return {extract_val(elemType, resV4, getIntAttr(0)),\n+              extract_val(elemType, resV4, getIntAttr(1)),\n+              extract_val(elemType, resV4, getIntAttr(2)),\n+              extract_val(elemType, resV4, getIntAttr(3))};\n     } else if (elemBytes == 4 &&\n                needTrans) { // Use lds.32 to load tf32 matrices\n       Value ptr2 = getPtr(ptrIdx + 1);\n       assert(sMatStride == 1);\n-      int sOffsetElem =\n-          matIdx[order[1]] * (sMatStride * sMatShape) * sTileStride;\n-      int sOffsetArrElem = 1 * (sMatStride * sMatShape) * sTileStride;\n+      int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n+      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sTileStride);\n+      int sOffsetArrElem = sMatStride * sMatShape;\n+      Value sOffsetArrElemVal =\n+          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sTileStride));\n \n       Value elems[4];\n       Type elemTy = type::f32Ty(ctx);\n+      Type elemPtrTy = ptr_ty(elemTy);\n       if (kOrder == 1) {\n-        elems[0] = load(gep(elemTy, ptr, i32_val(sOffsetElem)));\n-        elems[1] = load(gep(elemTy, ptr2, i32_val(sOffsetElem)));\n+        elems[0] = load(gep(elemPtrTy, ptr, i32_val(sOffsetElem)));\n+        elems[1] = load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem)));\n         elems[2] =\n-            load(gep(elemTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+            load(gep(elemPtrTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n         elems[3] =\n-            load(gep(elemTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n+            load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n       } else {\n-        elems[0] = load(gep(elemTy, ptr, i32_val(sOffsetElem)));\n-        elems[2] = load(gep(elemTy, ptr2, i32_val(sOffsetElem)));\n+        elems[0] = load(gep(elemPtrTy, ptr, i32_val(sOffsetElem)));\n+        elems[2] = load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem)));\n         elems[1] =\n-            load(gep(elemTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+            load(gep(elemPtrTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n         elems[3] =\n-            load(gep(elemTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n+            load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n       }\n-\n       return {elems[0], elems[1], elems[2], elems[3]};\n-    } else if (elemBytes == 1 && needTrans) {\n+\n+    } else if (elemBytes == 1 && needTrans) { // work with int8\n       std::array<std::array<Value, 4>, 2> ptrs;\n       ptrs[0] = {\n           getPtr(ptrIdx),\n@@ -3008,9 +3323,11 @@ class MMA16816SmemLoader {\n       };\n \n       assert(sMatStride == 1);\n-      int sOffsetElem =\n-          matIdx[order[1]] * (sMatStride * sMatShape) * sTileStride;\n-      int sOffsetArrElem = 1 * (sMatStride * sMatShape) * sTileStride;\n+      int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n+      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sTileStride);\n+      int sOffsetArrElem = 1 * (sMatStride * sMatShape);\n+      Value sOffsetArrElemVal =\n+          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sTileStride));\n \n       std::array<Value, 4> i8v4Elems;\n       std::array<Value, 4> i32Elems;\n@@ -3019,17 +3336,16 @@ class MMA16816SmemLoader {\n \n       Value i8Elems[4][4];\n       Type elemTy = type::i8Ty(ctx);\n+      Type elemPtrTy = ptr_ty(elemTy);\n       if (kOrder == 1) {\n-        Value offset = i32_val(sOffsetElem);\n-\n         for (int i = 0; i < 2; ++i)\n           for (int j = 0; j < 4; ++j)\n-            i8Elems[i][j] = load(gep(elemTy, ptrs[i][j], offset));\n+            i8Elems[i][j] = load(gep(elemPtrTy, ptrs[i][j], sOffsetElemVal));\n \n-        offset = i32_val(sOffsetElem + sOffsetArrElem);\n         for (int i = 2; i < 4; ++i)\n           for (int j = 0; j < 4; ++j)\n-            i8Elems[i][j] = load(gep(elemTy, ptrs[i - 2][j], offset));\n+            i8Elems[i][j] =\n+                load(gep(elemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n \n         for (int m = 0; m < 4; ++m) {\n           for (int e = 0; e < 4; ++e)\n@@ -3038,16 +3354,14 @@ class MMA16816SmemLoader {\n           i32Elems[m] = bitcast(i8v4Elems[m], i32_ty);\n         }\n       } else { // k first\n-        Value offset = i32_val(sOffsetElem);\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[0][j] = load(gep(elemTy, ptrs[0][j], offset));\n+          i8Elems[0][j] = load(gep(elemPtrTy, ptrs[0][j], sOffsetElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[2][j] = load(gep(elemTy, ptrs[1][j], offset));\n-        offset = i32_val(sOffsetElem + sOffsetArrElem);\n+          i8Elems[2][j] = load(gep(elemPtrTy, ptrs[1][j], sOffsetElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[1][j] = load(gep(elemTy, ptrs[0][j], offset));\n+          i8Elems[1][j] = load(gep(elemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[3][j] = load(gep(elemTy, ptrs[1][j], offset));\n+          i8Elems[3][j] = load(gep(elemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n \n         for (int m = 0; m < 4; ++m) {\n           for (int e = 0; e < 4; ++e)\n@@ -3080,8 +3394,8 @@ class MMA16816SmemLoader {\n   int cMatShape;\n   int sMatShape;\n \n-  int cTileStride;\n-  int sTileStride;\n+  Value cTileStride;\n+  Value sTileStride;\n \n   bool needTrans;\n   bool canUseLdmatrix;\n@@ -3131,6 +3445,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     size_t reduceAxis = 1;\n     unsigned K = AShape[reduceAxis];\n     bool isOuter = K == 1;\n+\n     bool isMMA = D.getType()\n                      .cast<RankedTensorType>()\n                      .getEncoding()\n@@ -3142,11 +3457,13 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                       .getEncoding()\n                       .cast<MmaEncodingAttr>();\n \n-    if (!isOuter && isMMA) {\n+    bool isHMMA = isDotHMMA(op);\n+    if (!isOuter && isMMA && isHMMA) {\n       if (mmaLayout.getVersion() == 1)\n         return convertMMA884(op, adaptor, rewriter);\n       if (mmaLayout.getVersion() == 2)\n         return convertMMA16816(op, adaptor, rewriter);\n+\n       llvm::report_fatal_error(\n           \"Unsupported MMA kind found when converting DotOp to LLVM.\");\n     }\n@@ -3159,6 +3476,49 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n         \"Unsupported DotOp found when converting TritonGPU to LLVM.\");\n   }\n \n+  // Tell whether a DotOp support HMMA.\n+  // This is port from the master branch, the original logic is retained.\n+  static bool isDotHMMA(DotOp op) {\n+    auto a = op.a();\n+    auto b = op.b();\n+    auto c = op.c();\n+    auto d = op.getResult();\n+    auto aTensorTy = a.getType().cast<RankedTensorType>();\n+    auto bTensorTy = b.getType().cast<RankedTensorType>();\n+    auto cTensorTy = c.getType().cast<RankedTensorType>();\n+    auto dTensorTy = d.getType().cast<RankedTensorType>();\n+\n+    if (!dTensorTy.getEncoding().isa<MmaEncodingAttr>())\n+      return false;\n+\n+    auto mmaLayout = dTensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto aElemTy = aTensorTy.getElementType();\n+    auto bElemTy = bTensorTy.getElementType();\n+\n+    assert((mmaLayout.getVersion() == 1 || mmaLayout.getVersion() == 2) &&\n+           \"Unexpected MMA layout version found\");\n+    // Refer to mma section for the data type supported by Volta and Hopper\n+    // Tensor Core in\n+    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-fragment-mma-884-f16\n+    return (aElemTy.isF16() && bElemTy.isF16()) ||\n+           (aElemTy.isBF16() && bElemTy.isBF16()) ||\n+           (aElemTy.isF32() && bElemTy.isF32() && op.allowTF32() &&\n+            mmaLayout.getVersion() >= 2) ||\n+           (aElemTy.isInteger(8) && bElemTy.isInteger(8) &&\n+            mmaLayout.getVersion() >= 2);\n+  }\n+\n+  // Tell whether a DotOp support HMMA by the operand type(either $a or $b).\n+  // We cannot get both the operand types(in TypeConverter), here we assume the\n+  // types of both the operands are identical here.\n+  // TODO[Superjomn]: Find a better way to implement it.\n+  static bool isDotHMMA(TensorType operand, bool allowTF32, int mmaVersion) {\n+    auto elemTy = operand.getElementType();\n+    return elemTy.isF16() || elemTy.isBF16() ||\n+           (elemTy.isF32() && allowTF32 && mmaVersion >= 2) ||\n+           (elemTy.isInteger(8) && mmaVersion >= 2);\n+  }\n+\n private:\n   // Convert to mma.m16n8k16\n   LogicalResult convertMMA16816(triton::DotOp a, OpAdaptor adaptor,\n@@ -3168,10 +3528,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                               ConversionPatternRewriter &rewriter) const;\n \n   LogicalResult convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n-                              ConversionPatternRewriter &rewriter) const {\n-    assert(false && \"Not implemented yet.\");\n-    return failure();\n-  }\n+                              ConversionPatternRewriter &rewriter) const;\n };\n \n // Helper for conversion of DotOp with mma<version=1>, that is sm<80\n@@ -3250,12 +3607,12 @@ struct DotOpMmaV1ConversionHelper {\n   }\n \n   // Loading $a from smem to registers, returns a LLVM::Struct.\n-  Value loadA(Value A, Value llA, Value thread, Value smem, Location loc,\n-              ConversionPatternRewriter &rewriter) const;\n+  Value loadA(Value A, const SharedMemoryObject &smemObj, Value thread,\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n \n   // Loading $b from smem to registers, returns a LLVM::Struct.\n-  Value loadB(Value B, Value llB, Value thread, Value smem, Location loc,\n-              ConversionPatternRewriter &rewriter) const;\n+  Value loadB(Value B, const SharedMemoryObject &smemObj, Value thread,\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n \n   // Loading $c to registers, returns a LLVM::Struct.\n   Value loadC(Value C, Value llC, ConversionPatternRewriter &rewriter) const;\n@@ -3662,21 +4019,22 @@ struct MMA16816ConversionHelper {\n   }\n \n   // Loading $a from smem to registers, returns a LLVM::Struct.\n-  Value loadA(Value tensor, Value llTensor) const {\n+  Value loadA(Value tensor, const SharedMemoryObject &smemObj) const {\n     auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n     auto shape = aTensorTy.getShape();\n \n     ValueTable ha;\n     std::function<void(int, int)> loadFn;\n     auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n     auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(aTensorTy);\n+\n     int numRepM = getNumRepM(aTensorTy, shape[0]);\n     int numRepK = getNumRepK(aTensorTy, shape[1]);\n \n     if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n       // load from smem\n       loadFn = getLoadMatrixFn(\n-          tensor, llTensor, mmaLayout, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n+          tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n           1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShape*/,\n           {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/);\n     } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n@@ -3698,7 +4056,7 @@ struct MMA16816ConversionHelper {\n   }\n \n   // Loading $b from smem to registers, returns a LLVM::Struct.\n-  Value loadB(Value tensor, Value llTensor) {\n+  Value loadB(Value tensor, const SharedMemoryObject &smemObj) {\n     ValueTable hb;\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n     auto shape = tensorTy.getShape();\n@@ -3708,7 +4066,7 @@ struct MMA16816ConversionHelper {\n     int numRepN = getNumRepN(tensorTy, shape[1]);\n \n     auto loadFn = getLoadMatrixFn(\n-        tensor, llTensor, mmaLayout, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n+        tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n         0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShape*/,\n         {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/);\n \n@@ -3785,26 +4143,34 @@ struct MMA16816ConversionHelper {\n                                              std::to_string(i)));\n         // reuse the output registers\n       }\n+\n       mma(retArgs, aArgs, bArgs, cArgs);\n       Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n \n       auto getIntAttr = [&](int v) {\n         return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n       };\n \n+      Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n       for (int i = 0; i < 4; ++i)\n         fc[m * colsPerThread + 4 * n + i] =\n-            extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(i));\n+            extract_val(elemTy, mmaOut, getIntAttr(i));\n     };\n \n     for (int k = 0; k < numRepK; ++k)\n       for (int m = 0; m < numRepM; ++m)\n         for (int n = 0; n < numRepN; ++n)\n           callMma(2 * m, n, 2 * k);\n \n+    Type resElemTy = dTensorTy.getElementType();\n+\n+    for (auto &elem : fc) {\n+      elem = bitcast(elem, resElemTy);\n+    }\n+\n     // replace with new packed result\n     Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(fc.size(), type::f32Ty(ctx)));\n+        ctx, SmallVector<Type>(fc.size(), resElemTy));\n     Value res = getStructFromElements(loc, fc, rewriter, structTy);\n     rewriter.replaceOp(op, res);\n \n@@ -3813,10 +4179,10 @@ struct MMA16816ConversionHelper {\n \n private:\n   std::function<void(int, int)>\n-  getLoadMatrixFn(Value tensor, Value llTensor, MmaEncodingAttr mmaLayout,\n-                  int wpt, uint32_t kOrder, ArrayRef<int> instrShape,\n-                  ArrayRef<int> matShape, Value warpId,\n-                  ValueTable &vals) const {\n+  getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n+                  MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder,\n+                  ArrayRef<int> instrShape, ArrayRef<int> matShape,\n+                  Value warpId, ValueTable &vals) const {\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n     // We assumes that the input operand of Dot should be from shared layout.\n     // TODO(Superjomn) Consider other layouts if needed later.\n@@ -3835,25 +4201,24 @@ struct MMA16816ConversionHelper {\n \n     // (a, b) is the coordinate.\n     auto load = [=, &vals, &ld2](int a, int b) {\n-      MMA16816SmemLoader loader(wpt, sharedLayout.getOrder(), kOrder,\n-                                tensorTy.getShape() /*tileShape*/, instrShape,\n-                                matShape, perPhase, maxPhase, elemBytes,\n-                                rewriter, typeConverter, loc);\n+      MMA16816SmemLoader loader(\n+          wpt, sharedLayout.getOrder(), kOrder, smemObj.strides,\n+          tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n+          maxPhase, elemBytes, rewriter, typeConverter, loc);\n       SmallVector<Value> offs = loader.computeOffsets(warpId, lane);\n-\n       const int numPtrs = loader.getNumPtr();\n-\n       SmallVector<Value> ptrs(numPtrs);\n \n       Type smemPtrTy = helper.getShemPtrTy();\n       for (int i = 0; i < numPtrs; ++i) {\n-        ptrs[i] =\n-            bitcast(gep(smemPtrTy, llTensor, ValueRange({offs[i]})), smemPtrTy);\n+        ptrs[i] = bitcast(gep(smemPtrTy, smemObj.base, ValueRange({offs[i]})),\n+                          smemPtrTy);\n       }\n \n       auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n           (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n           ptrs, helper.getMatType(), helper.getShemPtrTy());\n+\n       if (!needTrans) {\n         ld2(vals, a, b, ha0);\n         ld2(vals, a + 1, b, ha1);\n@@ -3898,10 +4263,9 @@ struct MMA16816ConversionHelper {\n \n     assert(!elems.empty());\n \n-    Type fp16Ty = type::f16Ty(ctx);\n-    Type fp16x2Ty = vec_ty(fp16Ty, 2);\n+    Type elemTy = elems[0].getType();\n     Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(elems.size(), fp16x2Ty));\n+        ctx, SmallVector<Type>(elems.size(), elemTy));\n     auto result = getStructFromElements(loc, elems, rewriter, structTy);\n     return result;\n   }\n@@ -3940,29 +4304,47 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n       dotOperandLayout.getParent().dyn_cast_or_null<MmaEncodingAttr>();\n   assert(mmaLayout);\n \n+  bool isOuter{};\n+  {\n+    int K{};\n+    if (dotOperandLayout.getOpIdx() == 0) // $a\n+      K = dstTensorTy.getShape()[1];\n+    else // $b\n+      K = dstTensorTy.getShape()[0];\n+    isOuter = K == 1;\n+  }\n+\n+  // TODO[Superjomn]: the allowTF32 is not available in ConvertLayoutOp for it\n+  // is an attribute of DotOp.\n+  bool allowTF32 = false;\n+  bool isHMMA = DotOpConversion::isDotHMMA(dstTensorTy, allowTF32,\n+                                           mmaLayout.getVersion());\n+\n+  auto smemObj = getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n   Value res;\n-  if (mmaLayout.getVersion() == 2) {\n+  if (!isOuter && mmaLayout.getVersion() == 2 && isHMMA) { // tensor core v2\n     MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n                                        rewriter, getTypeConverter(),\n                                        op.getLoc());\n \n     if (dotOperandLayout.getOpIdx() == 0) {\n       // operand $a\n-      res = mmaHelper.loadA(src, adaptor.src());\n+      res = mmaHelper.loadA(src, smemObj);\n     } else if (dotOperandLayout.getOpIdx() == 1) {\n       // operand $b\n-      res = mmaHelper.loadB(src, adaptor.src());\n+      res = mmaHelper.loadB(src, smemObj);\n     }\n-  } else if (mmaLayout.getVersion() == 1) {\n+  } else if (!isOuter && mmaLayout.getVersion() == 1 &&\n+             isHMMA) { // tensor core v1\n     DotOpMmaV1ConversionHelper helper(mmaLayout);\n     if (dotOperandLayout.getOpIdx() == 0) {\n       // operand $a\n-      res = helper.loadA(src, adaptor.src(), getThreadId(rewriter, loc),\n-                         adaptor.src(), loc, rewriter);\n+      res =\n+          helper.loadA(src, smemObj, getThreadId(rewriter, loc), loc, rewriter);\n     } else if (dotOperandLayout.getOpIdx() == 1) {\n       // operand $b\n-      res = helper.loadB(src, adaptor.src(), getThreadId(rewriter, loc),\n-                         adaptor.src(), loc, rewriter);\n+      res =\n+          helper.loadB(src, smemObj, getThreadId(rewriter, loc), loc, rewriter);\n     }\n   } else {\n     assert(false && \"Unsupported mma layout found\");\n@@ -3999,8 +4381,12 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adaptor,\n     loadedA = adaptor.a();\n     loadedB = adaptor.b();\n   } else {\n-    loadedA = mmaHelper.loadA(op.a(), adaptor.a());\n-    loadedB = mmaHelper.loadB(op.b(), adaptor.b());\n+    SharedMemoryObject smemA =\n+        getSharedMemoryObjectFromStruct(loc, adaptor.a(), rewriter);\n+    SharedMemoryObject smemB =\n+        getSharedMemoryObjectFromStruct(loc, adaptor.b(), rewriter);\n+    loadedA = mmaHelper.loadA(op.a(), smemA);\n+    loadedB = mmaHelper.loadB(op.b(), smemB);\n   }\n \n   loadedC = mmaHelper.loadC(op.c(), adaptor.c());\n@@ -4125,8 +4511,12 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n }\n \n Value DotOpMmaV1ConversionHelper::loadA(\n-    Value tensor, Value llTensor, Value thread, Value smem, Location loc,\n+    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n     ConversionPatternRewriter &rewriter) const {\n+  // smem\n+  Value smem = smemObj.base;\n+  auto strides = smemObj.strides;\n+\n   auto *ctx = rewriter.getContext();\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n   auto shape = tensorTy.getShape();\n@@ -4146,10 +4536,10 @@ Value DotOpMmaV1ConversionHelper::loadA(\n \n   int vecA = sharedLayout.getVec();\n \n-  int strideAM = isARow ? shape[1] : 1;\n-  int strideAK = isARow ? 1 : shape[0];\n-  int strideA0 = isARow ? strideAK : strideAM;\n-  int strideA1 = isARow ? strideAM : strideAK;\n+  Value strideAM = isARow ? strides[0] : i32_val(1);\n+  Value strideAK = isARow ? i32_val(1) : strides[1];\n+  Value strideA0 = isARow ? strideAK : strideAM;\n+  Value strideA1 = isARow ? strideAM : strideAK;\n \n   int strideRepM = wpt[0] * fpw[0] * 8;\n   int strideRepK = 1;\n@@ -4175,8 +4565,7 @@ Value DotOpMmaV1ConversionHelper::loadA(\n     offA0I = udiv(offA0I, i32_val(vecA));\n     offA0I = xor_(offA0I, phaseA);\n     offA0I = xor_(offA0I, i32_val(vecA));\n-    offA[i] =\n-        add(mul(offA0I, i32_val(strideA0)), mul(offA1, i32_val(strideA1)));\n+    offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n   }\n \n   Type f16x2Ty = vec_ty(f16_ty, 2);\n@@ -4205,8 +4594,9 @@ Value DotOpMmaV1ConversionHelper::loadA(\n \n     int stepAM = isARow ? m : m / numPtrA * numPtrA;\n     int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n-    Value pa = gep(f16PtrTy, thePtrA,\n-                   i32_val(stepAM * strideRepM * strideAM + stepAK * strideAK));\n+    Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),\n+                       mul(i32_val(stepAK), strideAK));\n+    Value pa = gep(f16PtrTy, thePtrA, offset);\n     Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n     Value ha = load(bitcast(pa, aPtrTy));\n     // record lds that needs to be moved\n@@ -4243,8 +4633,12 @@ Value DotOpMmaV1ConversionHelper::loadA(\n }\n \n Value DotOpMmaV1ConversionHelper::loadB(\n-    Value tensor, Value llTensor, Value thread, Value smem, Location loc,\n+    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n     ConversionPatternRewriter &rewriter) const {\n+  // smem\n+  Value smem = smemObj.base;\n+  auto strides = smemObj.strides;\n+\n   auto *ctx = rewriter.getContext();\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n   auto shape = tensorTy.getShape();\n@@ -4257,10 +4651,10 @@ Value DotOpMmaV1ConversionHelper::loadB(\n   SmallVector<int> rep({0, 2 * packSize1, 1});       // pad M with 0\n   SmallVector<int> spw({0, fpw[1] * 4 * rep[1], 1}); // pad M with 0\n   int vecB = sharedLayout.getVec();\n-  int strideBN = isBRow ? 1 : shape[0];\n-  int strideBK = isBRow ? shape[1] : 1;\n-  int strideB0 = isBRow ? strideBN : strideBK;\n-  int strideB1 = isBRow ? strideBK : strideBN;\n+  Value strideBN = isBRow ? i32_val(1) : strides[1];\n+  Value strideBK = isBRow ? strides[0] : i32_val(1);\n+  Value strideB0 = isBRow ? strideBN : strideBK;\n+  Value strideB1 = isBRow ? strideBK : strideBN;\n   int strideRepN = wpt[1] * fpw[1] * 8;\n   int strideRepK = 1;\n \n@@ -4285,8 +4679,7 @@ Value DotOpMmaV1ConversionHelper::loadB(\n     offB0I = udiv(offB0I, i32_val(vecB));\n     offB0I = xor_(offB0I, phaseB);\n     offB0I = mul(offB0I, i32_val(vecB));\n-    offB[i] =\n-        add(mul(offB0I, i32_val(strideB0)), mul(offB1, i32_val(strideB1)));\n+    offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));\n   }\n \n   Type f16PtrTy = ptr_ty(f16_ty);\n@@ -4307,8 +4700,9 @@ Value DotOpMmaV1ConversionHelper::loadB(\n \n     int stepBN = isBRow ? n / numPtrB * numPtrB : n;\n     int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n-    Value pb = gep(f16PtrTy, thePtrB,\n-                   i32_val(stepBN * strideRepN * strideBN + stepBK * strideBK));\n+    Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n+                       mul(i32_val(stepBK), strideBK));\n+    Value pb = gep(f16PtrTy, thePtrB, offset);\n     Value hb =\n         load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n     // record lds that needs to be moved\n@@ -4427,6 +4821,155 @@ DotOpMmaV1ConversionHelper::extractLoadedOperand(\n   return rcds;\n }\n \n+LogicalResult\n+DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n+                               ConversionPatternRewriter &rewriter) const {\n+  auto *ctx = rewriter.getContext();\n+  auto loc = op.getLoc();\n+  auto threadId = getThreadId(rewriter, loc);\n+\n+  using ValueTable = std::map<std::pair<int, int>, Value>;\n+\n+  auto A = op.a();\n+  auto B = op.b();\n+  auto C = op.c();\n+  auto D = op.getResult();\n+\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto cTensorTy = C.getType().cast<RankedTensorType>();\n+  auto dTensorTy = D.getType().cast<RankedTensorType>();\n+\n+  auto aShape = aTensorTy.getShape();\n+  auto bShape = bTensorTy.getShape();\n+  auto cShape = cTensorTy.getShape();\n+\n+  auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto cLayout = cTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto dLayout = dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+\n+  auto aOrder = aLayout.getOrder();\n+  auto bOrder = bLayout.getOrder();\n+\n+  auto order = dLayout.getOrder();\n+\n+  bool isARow = aOrder[0] == 1;\n+  bool isBRow = bOrder[0] == 1;\n+\n+  int strideAM = isARow ? aShape[1] : 1;\n+  int strideAK = isARow ? 1 : aShape[0];\n+  int strideBN = isBRow ? 1 : bShape[0];\n+  int strideBK = isBRow ? bShape[1] : 1;\n+  int strideA0 = isARow ? strideAK : strideAM;\n+  int strideA1 = isARow ? strideAM : strideAK;\n+  int strideB0 = isBRow ? strideBN : strideBK;\n+  int strideB1 = isBRow ? strideBK : strideBN;\n+  int lda = isARow ? strideAM : strideAK;\n+  int ldb = isBRow ? strideBK : strideBN;\n+  int aPerPhase = aLayout.getPerPhase();\n+  int aMaxPhase = aLayout.getMaxPhase();\n+  int bPerPhase = bLayout.getPerPhase();\n+  int bMaxPhase = bLayout.getMaxPhase();\n+  int aNumPtr = 8;\n+  int bNumPtr = 8;\n+  int NK = aShape[1];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value mContig = i32_val(sizePerThread[order[1]]);\n+  Value nContig = i32_val(sizePerThread[order[0]]);\n+\n+  // threadId in blocked layout\n+  SmallVector<Value> threadIds;\n+  {\n+    int dim = cShape.size();\n+    threadIds.resize(dim);\n+    for (unsigned k = 0; k < dim - 1; k++) {\n+      Value dimK = i32_val(shapePerCTA[order[k]]);\n+      Value rem = urem(threadId, dimK);\n+      threadId = udiv(threadId, dimK);\n+      threadIds[order[k]] = rem;\n+    }\n+    Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n+    threadIds[order[dim - 1]] = urem(threadId, dimK);\n+  }\n+\n+  Value threadIdM = threadIds[0];\n+  Value threadIdN = threadIds[1];\n+\n+  Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n+  Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n+  SmallVector<Value> aOff(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i) {\n+    aOff[i] = add(mul(offA0, i32_val(strideA0)), mul(offA1, i32_val(strideA1)));\n+  }\n+\n+  Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n+  Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n+  SmallVector<Value> bOff(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i) {\n+    bOff[i] = add(mul(offB0, i32_val(strideB0)), mul(offB1, i32_val(strideB1)));\n+  }\n+\n+  auto aSmem = getSharedMemoryObjectFromStruct(loc, adaptor.a(), rewriter);\n+  auto bSmem = getSharedMemoryObjectFromStruct(loc, adaptor.b(), rewriter);\n+\n+  Type f32PtrTy = ptr_ty(f32_ty);\n+  SmallVector<Value> aPtrs(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i)\n+    aPtrs[i] = gep(f32PtrTy, aSmem.base, aOff[i]);\n+\n+  SmallVector<Value> bPtrs(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i)\n+    bPtrs[i] = gep(f32PtrTy, bSmem.base, bOff[i]);\n+\n+  ValueTable has, hbs;\n+  auto cc = getElementsFromStruct(loc, adaptor.c(), rewriter);\n+  SmallVector<Value> ret = cc;\n+  // is this compatible with blocked layout?\n+\n+  for (unsigned k = 0; k < NK; k++) {\n+    int z = 0;\n+    for (unsigned i = 0; i < cShape[order[1]]; i += shapePerCTA[order[1]])\n+      for (unsigned j = 0; j < cShape[order[0]]; j += shapePerCTA[order[0]])\n+        for (unsigned ii = 0; ii < sizePerThread[order[1]]; ++ii)\n+          for (unsigned jj = 0; jj < sizePerThread[order[0]]; ++jj) {\n+            unsigned m = order[0] == 1 ? i : j;\n+            unsigned n = order[0] == 1 ? j : i;\n+            unsigned mm = order[0] == 1 ? ii : jj;\n+            unsigned nn = order[0] == 1 ? jj : ii;\n+            if (!has.count({m + mm, k})) {\n+              Value pa = gep(f32PtrTy, aPtrs[0],\n+                             i32_val((m + mm) * strideAM + k * strideAK));\n+              Value va = load(pa);\n+              has[{m + mm, k}] = va;\n+            }\n+            if (!hbs.count({n + nn, k})) {\n+              Value pb = gep(f32PtrTy, bPtrs[0],\n+                             i32_val((n + nn) * strideBN + k * strideBK));\n+              Value vb = load(pb);\n+              hbs[{n + nn, k}] = vb;\n+            }\n+\n+            ret[z] = rewriter.create<LLVM::FMulAddOp>(loc, has[{m + mm, k}],\n+                                                      hbs[{n + nn, k}], ret[z]);\n+            ++z;\n+          }\n+  }\n+\n+  auto res = getStructFromElements(\n+      loc, ret, rewriter,\n+      struct_ty(SmallVector<Type>(ret.size(), ret[0].getType())));\n+  rewriter.replaceOp(op, res);\n+\n+  return success();\n+}\n+\n /// ====================== mma codegen end ============================\n \n Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,\n@@ -4500,7 +5043,17 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n       return LLVM::LLVMStructType::getLiteral(ctx, types);\n     } else if (auto shared_layout =\n                    layout.dyn_cast_or_null<SharedEncodingAttr>()) {\n-      return LLVM::LLVMPointerType::get(convertType(type.getElementType()), 3);\n+      SmallVector<Type, 4> types;\n+      // base ptr\n+      auto ptrType =\n+          LLVM::LLVMPointerType::get(convertType(type.getElementType()), 3);\n+      types.push_back(ptrType);\n+      // shape dims\n+      auto rank = type.getRank();\n+      for (auto i = 0; i < rank; i++) {\n+        types.push_back(IntegerType::get(ctx, 32));\n+      }\n+      return LLVM::LLVMStructType::getLiteral(ctx, types);\n     } else if (auto mmaLayout = layout.dyn_cast_or_null<MmaEncodingAttr>()) {\n       if (mmaLayout.getVersion() == 2) {\n         auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(type);\n@@ -4637,15 +5190,26 @@ struct InsertSliceAsyncOpConversion\n     auto srcElems = getLLVMElems(src, llSrc, rewriter, loc);\n \n     // %dst\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto dstShape = dstTy.getShape();\n+    auto smemObj = getSharedMemoryObjectFromStruct(loc, llDst, rewriter);\n     auto axis = op->getAttrOfType<IntegerAttr>(\"axis\").getInt();\n-    assert(axis == 0 && \"insert_slice_async: Only axis=0 is supported for now\");\n-    auto dstBase = createIndexAttrConstant(rewriter, loc,\n-                                           getTypeConverter()->getIndexType(),\n-                                           product<int64_t>(srcTy.getShape()));\n-    Value offset = mul(llIndex, dstBase);\n-    auto dstPtrTy = LLVM::LLVMPointerType::get(\n-        getTypeConverter()->convertType(resTy.getElementType()), 3);\n-    Value dstPtrBase = gep(dstPtrTy, llDst, offset);\n+    SmallVector<Value, 4> offsetVals;\n+    SmallVector<Value, 4> srcStrides;\n+    for (auto i = 0; i < dstShape.size(); ++i) {\n+      if (i == axis) {\n+        offsetVals.emplace_back(llIndex);\n+      } else {\n+        offsetVals.emplace_back(i32_val(0));\n+        srcStrides.emplace_back(smemObj.strides[i]);\n+      }\n+    }\n+    // Compute the offset based on the original dimensions of the shared memory\n+    // object\n+    auto dstOffset = dot(rewriter, loc, offsetVals, smemObj.strides);\n+    auto dstPtrTy =\n+        ptr_ty(getTypeConverter()->convertType(resTy.getElementType()), 3);\n+    Value dstPtrBase = gep(dstPtrTy, smemObj.base, dstOffset);\n \n     // %mask\n     SmallVector<Value> maskElems;\n@@ -4673,12 +5237,11 @@ struct InsertSliceAsyncOpConversion\n     unsigned maxPhase = resSharedLayout.getMaxPhase();\n     auto sizePerThread = srcBlockedLayout.getSizePerThread();\n     auto threadsPerCTA = getThreadsPerCTA(srcBlockedLayout);\n-\n     auto inOrder = srcBlockedLayout.getOrder();\n \n-    // If perPhase * maxPhase > threadsPerCTA, we need to swizzle over\n-    // elements across phases. If perPhase * maxPhase == threadsPerCTA,\n-    // swizzle is not allowed\n+    // If perPhase * maxPhase > threadsPerCTA, we will have elements\n+    // that share the same tile indices. The index calculation will\n+    // be cached.\n     auto numSwizzleRows = std::max<unsigned>(\n         (perPhase * maxPhase) / threadsPerCTA[inOrder[1]], 1);\n     // A sharedLayout encoding has a \"vec\" parameter.\n@@ -4687,7 +5250,7 @@ struct InsertSliceAsyncOpConversion\n     auto numVecCols = std::max<unsigned>(inVec / outVec, 1);\n \n     auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcShape);\n-    // <<tileVecIdxRow, tileVecIdxCol>, TileOffset>\n+    //  <<tileVecIdxRow, tileVecIdxCol>, TileOffset>\n     DenseMap<std::pair<unsigned, unsigned>, Value> tileOffsetMap;\n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n       // minVec = 2, inVec = 4, outVec = 2\n@@ -4705,7 +5268,6 @@ struct InsertSliceAsyncOpConversion\n           vecIdxCol / numVecCols * numVecCols * threadsPerCTA[inOrder[0]];\n       auto baseOffsetRow = vecIdxRow / numSwizzleRows * numSwizzleRows *\n                            threadsPerCTA[inOrder[1]];\n-      auto baseOffset = (baseOffsetRow * srcShape[inOrder[0]] + baseOffsetCol);\n       auto tileVecIdxCol = vecIdxCol % numVecCols;\n       auto tileVecIdxRow = vecIdxRow % numSwizzleRows;\n \n@@ -4718,17 +5280,21 @@ struct InsertSliceAsyncOpConversion\n         // Example1:\n         // outVec = 2, inVec = 2, minVec = 2\n         // outVec = 2, inVec = 4, minVec = 2\n-        //     | [1 2] [3 4]  ... [15 16] |\n-        //     | [3 4] [5 6]  ... [1 2]   |\n+        //     | [1 2] [3 4] [5 6] ... |\n+        //     | [3 4] [1 2] [7 8] ... |\n+        //     | [5 6] [7 8] [1 2] ... |\n         // Example2:\n         // outVec = 4, inVec = 2, minVec = 2\n-        //     | [1 2 3 4] [5 6 7 8] ... [13 14 15 16] |\n-        //     | [5 6 7 8] [9 10 11 12] ... [1 2 3 4]  |\n+        //     | [1 2 3 4] [5 6 7 8] [9 10 11 12] ... |\n+        //     | [5 6 7 8] [1 2 3 4] [13 14 15 16] ... |\n+        //     | [9 10 11 12] [13 14 15 16] [1 2 3 4] ... |\n         auto srcIdx = srcIndices[tileVecIdxRow * sizePerThread[inOrder[0]]];\n         Value phase = urem(udiv(srcIdx[inOrder[1]], i32_val(perPhase)),\n                            i32_val(maxPhase));\n-        Value rowOffset =\n-            mul(srcIdx[inOrder[1]], i32_val(srcShape[inOrder[0]]));\n+        // srcShape and smemObj.shape maybe different if smemObj is a\n+        // slice of the original shared memory object.\n+        // So we need to use the original shape to compute the offset\n+        Value rowOffset = mul(srcIdx[inOrder[1]], srcStrides[inOrder[1]]);\n         Value colOffset =\n             add(srcIdx[inOrder[0]], i32_val(tileVecIdxCol * minVec));\n         Value swizzleIdx = udiv(colOffset, i32_val(outVec));\n@@ -4748,21 +5314,25 @@ struct InsertSliceAsyncOpConversion\n       auto numWords = vecBitWidth / bitWidth;\n       auto numWordElems = bitWidth / resElemTy.getIntOrFloatBitWidth();\n \n-      // XXX(Keren): Tune CG and CA here.\n+      // Tune CG and CA here.\n       auto byteWidth = bitWidth / 8;\n       CacheModifier srcCacheModifier =\n           byteWidth == 16 ? CacheModifier::CG : CacheModifier::CA;\n       assert(byteWidth == 16 || byteWidth == 8 || byteWidth == 4);\n       auto resByteWidth = resElemTy.getIntOrFloatBitWidth() / 8;\n \n-      auto tileOffset = tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}];\n+      Value tileOffset = tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}];\n+      Value baseOffset =\n+          add(mul(i32_val(baseOffsetRow), srcStrides[inOrder[1]]),\n+              i32_val(baseOffsetCol));\n+      Value basePtr = gep(dstPtrTy, tileOffset, baseOffset);\n       for (size_t wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n         PTXBuilder ptxBuilder;\n         auto wordElemIdx = wordIdx * numWordElems;\n         auto &copyAsyncOp =\n             *ptxBuilder.create<PTXCpAsyncLoadInstr>(srcCacheModifier);\n-        auto *dstOperand = ptxBuilder.newAddrOperand(\n-            tileOffset, \"r\", (wordElemIdx + baseOffset) * resByteWidth);\n+        auto *dstOperand =\n+            ptxBuilder.newAddrOperand(basePtr, \"r\", wordElemIdx * resByteWidth);\n         auto *srcOperand =\n             ptxBuilder.newAddrOperand(srcElems[elemIdx + wordElemIdx], \"l\");\n         auto *copySize = ptxBuilder.newConstantOperand(byteWidth);\n@@ -4873,190 +5443,6 @@ struct FDivOpConversion\n   }\n };\n \n-struct PrintfOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintfOp> {\n-  using ConvertTritonGPUOpToLLVMPattern<\n-      triton::PrintfOp>::ConvertTritonGPUOpToLLVMPattern;\n-\n-  LogicalResult\n-  matchAndRewrite(triton::PrintfOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    auto loc = op->getLoc();\n-    SmallVector<Value, 16> operands;\n-    for (auto operand : adaptor.getOperands()) {\n-      auto sub_operands = this->getElementsFromStruct(loc, operand, rewriter);\n-      for (auto elem : sub_operands) {\n-        operands.push_back(elem);\n-      }\n-    }\n-    std::string formatStr;\n-    llvm::raw_string_ostream os(formatStr);\n-    os << op.prefix();\n-    if (operands.size() > 0) {\n-      os << getFormatSubstr(operands[0]);\n-    }\n-\n-    for (size_t i = 1; i < operands.size(); ++i) {\n-      os << \", \" << getFormatSubstr(operands[i]);\n-    }\n-    llPrintf(formatStr, operands, rewriter);\n-    rewriter.eraseOp(op);\n-    return success();\n-  }\n-  // get format specific for each input value\n-  // currently support pointer, i8, i16, i32, i64, f16, bf16, f32, f64\n-  std::string getFormatSubstr(Value value) const {\n-    Type type = value.getType();\n-    unsigned width = type.getIntOrFloatBitWidth();\n-\n-    if (type.isa<LLVM::LLVMPointerType>()) {\n-      return \"%p\";\n-    } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n-      return \"%f\";\n-    } else if (type.isSignedInteger()) {\n-      return \"%i\";\n-    } else if (type.isUnsignedInteger() || type.isSignlessInteger()) {\n-      return \"%u\";\n-    }\n-    assert(false && \"not supported type\");\n-  }\n-\n-  // declare vprintf(i8*, i8*) as external function\n-  LLVM::LLVMFuncOp\n-  getVprintfDeclaration(ConversionPatternRewriter &rewriter) const {\n-    auto moduleOp =\n-        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n-    StringRef funcName(\"vprintf\");\n-    Operation *funcOp = moduleOp.lookupSymbol(funcName);\n-    if (funcOp)\n-      return cast<LLVM::LLVMFuncOp>(*funcOp);\n-\n-    auto *context = rewriter.getContext();\n-\n-    SmallVector<Type> argsType{ptr_ty(IntegerType::get(context, 8)),\n-                               ptr_ty(IntegerType::get(context, 8))};\n-    auto funcType = LLVM::LLVMFunctionType::get(i32_ty, argsType);\n-\n-    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n-    rewriter.setInsertionPointToStart(moduleOp.getBody());\n-\n-    return rewriter.create<LLVM::LLVMFuncOp>(UnknownLoc::get(context), funcName,\n-                                             funcType);\n-  }\n-\n-  // extend integer to int32, extend float to float64\n-  // this comes from vprintf alignment requirements.\n-  std::pair<Type, Value> promoteValue(ConversionPatternRewriter &rewriter,\n-                                      Value value) const {\n-    auto *context = rewriter.getContext();\n-    auto type = value.getType();\n-    unsigned width = type.getIntOrFloatBitWidth();\n-    Value newOp = value;\n-    Type newType = type;\n-\n-    bool bUnsigned = type.isUnsignedInteger();\n-    if (type.isIntOrIndex() && width < 32) {\n-      if (bUnsigned) {\n-        newType = ui32_ty;\n-        newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n-                                              value);\n-      } else {\n-        newType = i32_ty;\n-        newOp = rewriter.create<LLVM::SExtOp>(UnknownLoc::get(context), newType,\n-                                              value);\n-      }\n-    } else if (type.isBF16() || type.isF16() || type.isF32()) {\n-      newType = f64_ty;\n-      newOp = rewriter.create<LLVM::FPExtOp>(UnknownLoc::get(context), newType,\n-                                             value);\n-    }\n-\n-    return {newType, newOp};\n-  }\n-\n-  void llPrintf(StringRef msg, ValueRange args,\n-                ConversionPatternRewriter &rewriter) const {\n-    static const char formatStringPrefix[] = \"printfFormat_\";\n-    assert(!msg.empty() && \"printf with empty string not support\");\n-    Type int8Ptr = ptr_ty(i8_ty);\n-\n-    auto *context = rewriter.getContext();\n-    auto moduleOp =\n-        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n-    auto funcOp = getVprintfDeclaration(rewriter);\n-\n-    Value one = rewriter.create<LLVM::ConstantOp>(\n-        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(1));\n-    Value zero = rewriter.create<LLVM::ConstantOp>(\n-        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(0));\n-\n-    unsigned stringNumber = 0;\n-    SmallString<16> stringConstName;\n-    do {\n-      stringConstName.clear();\n-      (formatStringPrefix + Twine(stringNumber++)).toStringRef(stringConstName);\n-    } while (moduleOp.lookupSymbol(stringConstName));\n-\n-    llvm::SmallString<64> formatString(msg);\n-    formatString.push_back('\\n');\n-    formatString.push_back('\\0');\n-    size_t formatStringSize = formatString.size_in_bytes();\n-    auto globalType = LLVM::LLVMArrayType::get(i8_ty, formatStringSize);\n-\n-    LLVM::GlobalOp global;\n-    {\n-      ConversionPatternRewriter::InsertionGuard guard(rewriter);\n-      rewriter.setInsertionPointToStart(moduleOp.getBody());\n-      global = rewriter.create<LLVM::GlobalOp>(\n-          UnknownLoc::get(context), globalType,\n-          /*isConstant=*/true, LLVM::Linkage::Internal, stringConstName,\n-          rewriter.getStringAttr(formatString));\n-    }\n-\n-    Value globalPtr =\n-        rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(context), global);\n-    Value stringStart =\n-        rewriter.create<LLVM::GEPOp>(UnknownLoc::get(context), int8Ptr,\n-                                     globalPtr, mlir::ValueRange({zero, zero}));\n-\n-    Value bufferPtr =\n-        rewriter.create<LLVM::NullOp>(UnknownLoc::get(context), int8Ptr);\n-\n-    SmallVector<Value, 16> newArgs;\n-    if (args.size() >= 1) {\n-      SmallVector<Type> argTypes;\n-      for (auto arg : args) {\n-        Type newType;\n-        Value newArg;\n-        std::tie(newType, newArg) = promoteValue(rewriter, arg);\n-        argTypes.push_back(newType);\n-        newArgs.push_back(newArg);\n-      }\n-\n-      Type structTy = LLVM::LLVMStructType::getLiteral(context, argTypes);\n-      auto allocated = rewriter.create<LLVM::AllocaOp>(UnknownLoc::get(context),\n-                                                       ptr_ty(structTy), one,\n-                                                       /*alignment=*/0);\n-\n-      for (const auto &entry : llvm::enumerate(newArgs)) {\n-        auto index = rewriter.create<LLVM::ConstantOp>(\n-            UnknownLoc::get(context), i32_ty,\n-            rewriter.getI32IntegerAttr(entry.index()));\n-        auto fieldPtr = rewriter.create<LLVM::GEPOp>(\n-            UnknownLoc::get(context), ptr_ty(argTypes[entry.index()]),\n-            allocated, ArrayRef<Value>{zero, index});\n-        rewriter.create<LLVM::StoreOp>(UnknownLoc::get(context), entry.value(),\n-                                       fieldPtr);\n-      }\n-      bufferPtr = rewriter.create<LLVM::BitcastOp>(UnknownLoc::get(context),\n-                                                   int8Ptr, allocated);\n-    }\n-\n-    ValueRange operands{stringStart, bufferPtr};\n-    rewriter.create<LLVM::CallOp>(UnknownLoc::get(context), funcOp, operands);\n-  }\n-};\n-\n void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n                                   AxisInfoAnalysis &axisInfoAnalysis,\n@@ -5241,8 +5627,8 @@ void ConvertTritonGPUToLLVM::initSharedMemory(\n   OpBuilder b(mod.getBodyRegion());\n   auto loc = mod.getLoc();\n   auto elemTy = typeConverter.convertType(b.getIntegerType(8));\n-  // Set array size 0 and external linkage indicates that we use dynamic shared\n-  // allocation to allow a larger shared memory size for each kernel.\n+  // Set array size 0 and external linkage indicates that we use dynamic\n+  // shared allocation to allow a larger shared memory size for each kernel.\n   auto arrayTy = LLVM::LLVMArrayType::get(elemTy, 0);\n   auto global = b.create<LLVM::GlobalOp>(\n       loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::External,\n@@ -5263,6 +5649,15 @@ void ConvertTritonGPUToLLVM::initSharedMemory(\n \n namespace mlir {\n \n+namespace LLVM {\n+\n+void llPrintf(StringRef msg, ValueRange args,\n+              ConversionPatternRewriter &rewriter) {\n+  PrintfOpConversion::llPrintf(msg, args, rewriter);\n+}\n+\n+} // namespace LLVM\n+\n TritonLLVMConversionTarget::TritonLLVMConversionTarget(\n     MLIRContext &ctx, mlir::LLVMTypeConverter &typeConverter)\n     : ConversionTarget(ctx) {"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -278,6 +278,20 @@ struct TritonStorePattern : public OpConversionPattern<triton::StoreOp> {\n   }\n };\n \n+struct TritonAtomicRMWPattern\n+    : public OpConversionPattern<triton::AtomicRMWOp> {\n+  using OpConversionPattern<triton::AtomicRMWOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::AtomicRMWOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    rewriter.replaceOpWithNewOp<triton::AtomicRMWOp>(\n+        op, typeConverter->convertType(op.getType()), adaptor.atomic_rmw_op(),\n+        adaptor.ptr(), adaptor.val(), adaptor.mask());\n+    return success();\n+  }\n+};\n+\n struct TritonExtElemwisePattern\n     : public OpConversionPattern<triton::ExtElemwiseOp> {\n   using OpConversionPattern<triton::ExtElemwiseOp>::OpConversionPattern;"}, {"filename": "lib/Dialect/Triton/IR/Traits.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -66,4 +66,4 @@ mlir::LogicalResult mlir::OpTrait::impl::verifyTensorSize(Operation *op) {\n     }\n   }\n   return success();\n-}\n\\ No newline at end of file\n+}"}, {"filename": "lib/Dialect/TritonGPU/IR/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,5 +1,6 @@\n add_mlir_dialect_library(TritonGPUIR\n   Dialect.cpp\n+  Traits.cpp\n \n   DEPENDS\n   TritonGPUTableGen"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 27, "deletions": 60, "changes": 87, "file_content_changes": "@@ -119,10 +119,13 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n                   \"BlockedEncodingAttr not implemented\");\n     }\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    assert(mmaLayout.getVersion() == 2 &&\n-           \"mmaLayout version = 1 is not implemented yet\");\n-    return {16 * mmaLayout.getWarpsPerCTA()[0],\n-            8 * mmaLayout.getWarpsPerCTA()[1]};\n+    if (mmaLayout.getVersion() == 2)\n+      return {16 * mmaLayout.getWarpsPerCTA()[0],\n+              8 * mmaLayout.getWarpsPerCTA()[1]};\n+    if (mmaLayout.getVersion() == 1)\n+      return {16 * mmaLayout.getWarpsPerCTA()[0],\n+              16 * mmaLayout.getWarpsPerCTA()[1]};\n+    assert(0 && \"Unexpected MMA layout version found\");\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n   }\n@@ -476,7 +479,7 @@ void SharedEncodingAttr::print(AsmPrinter &printer) const {\n \n ParseResult parseInsertSliceAsyncOp(OpAsmParser &parser,\n                                     OperationState &result) {\n-  SmallVector<OpAsmParser::OperandType, 4> allOperands;\n+  SmallVector<OpAsmParser::OperandType, 8> allOperands;\n   Type srcType, dstType;\n   SMLoc allOperandLoc = parser.getCurrentLocation();\n   if (parser.parseOperandList(allOperands) ||\n@@ -491,54 +494,44 @@ ParseResult parseInsertSliceAsyncOp(OpAsmParser &parser,\n   operandTypes.push_back(dstType); // dst\n   operandTypes.push_back(\n       IntegerType::get(parser.getBuilder().getContext(), 32)); // index\n-  if (allOperands.size() >= 4)\n+\n+  int hasMask = 0, hasOther = 0;\n+  if (allOperands.size() >= 4) {\n     operandTypes.push_back(triton::getI1SameShape(srcType)); // mask\n-  if (allOperands.size() >= 5)\n+    hasMask = 1;\n+  }\n+  if (allOperands.size() >= 5) {\n     operandTypes.push_back(triton::getPointeeType(srcType)); // other\n+    hasOther = 1;\n+  }\n \n   if (parser.resolveOperands(allOperands, operandTypes, allOperandLoc,\n                              result.operands))\n     return failure();\n+\n+  // Deduce operand_segment_sizes from the number of the operands.\n+  auto operand_segment_sizesAttrName =\n+      InsertSliceAsyncOp::operand_segment_sizesAttrName(result.name);\n+  result.addAttribute(\n+      operand_segment_sizesAttrName,\n+      parser.getBuilder().getI32VectorAttr({1, 1, 1, hasMask, hasOther}));\n   return success();\n }\n \n void printInsertSliceAsyncOp(OpAsmPrinter &printer,\n                              InsertSliceAsyncOp insertSliceAsyncOp) {\n   printer << \" \";\n   printer << insertSliceAsyncOp.getOperation()->getOperands();\n-  printer.printOptionalAttrDict(insertSliceAsyncOp->getAttrs(),\n-                                /*elidedAttrs=*/{});\n+  // \"operand_segment_sizes\" can be deduced, so we don't print it.\n+  printer.printOptionalAttrDict(\n+      insertSliceAsyncOp->getAttrs(),\n+      {insertSliceAsyncOp.operand_segment_sizesAttrName()});\n   printer << \" : \";\n   printer.printStrippedAttrOrType(insertSliceAsyncOp.src().getType());\n   printer << \" -> \";\n   printer.printStrippedAttrOrType(insertSliceAsyncOp.result().getType());\n }\n \n-//===----------------------------------------------------------------------===//\n-// ExtractSliceOp\n-//===----------------------------------------------------------------------===//\n-\n-mlir::LogicalResult ExtractSliceOp::inferReturnTypes(\n-    ::mlir::MLIRContext *context, llvm::Optional<::mlir::Location> location,\n-    ::mlir::ValueRange operands, mlir::DictionaryAttr attributes,\n-    ::mlir::RegionRange regions,\n-    llvm::SmallVectorImpl<::mlir::Type> &inferredReturnTypes) {\n-  auto srcType = operands[0].getType().cast<RankedTensorType>();\n-  auto encoding = srcType.getEncoding();\n-  auto srcShape = srcType.getShape();\n-  auto axis = attributes.get(\"axis\").cast<IntegerAttr>().getInt();\n-  if (axis < 0 || (size_t)axis > srcShape.size())\n-    return failure();\n-  SmallVector<int64_t, 4> dstShape;\n-  for (size_t i = 0; i < srcShape.size(); i++)\n-    if (i != (size_t)axis)\n-      dstShape.push_back(srcShape[i]);\n-  auto returnType =\n-      RankedTensorType::get(dstShape, srcType.getElementType(), encoding);\n-  inferredReturnTypes.assign({returnType});\n-  return success();\n-}\n-\n //===----------------------------------------------------------------------===//\n // DotOperand Encoding\n //===----------------------------------------------------------------------===//\n@@ -633,32 +626,6 @@ void TritonGPUDialect::initialize() {\n   addInterfaces<TritonGPUInferLayoutInterface>();\n }\n \n-//===----------------------------------------------------------------------===//\n-// Verification\n-//===----------------------------------------------------------------------===//\n-\n-static LogicalResult verify(InsertSliceAsyncOp op) {\n-  if (!isSharedEncoding(op.getResult())) {\n-    return op.emitOpError(\n-        \"insert_slice_async should return a shared memory tensor\");\n-  }\n-  return success();\n-}\n-\n-static LogicalResult verify(ExtractSliceOp op) {\n-  if (!isSharedEncoding(op.getResult())) {\n-    return op.emitOpError(\"extract_slice should return a shared memory tensor\");\n-  }\n-  return success();\n-}\n-\n-static LogicalResult verify(AllocTensorOp op) {\n-  if (!isSharedEncoding(op.getResult())) {\n-    return op.emitOpError(\"alloc_tensor should return a shared memory tensor\");\n-  }\n-  return success();\n-}\n-\n #define GET_OP_CLASSES\n #include \"triton/Dialect/TritonGPU/IR/Ops.cpp.inc\"\n "}, {"filename": "lib/Dialect/TritonGPU/IR/Traits.cpp", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -0,0 +1,14 @@\n+#include \"triton/Dialect/TritonGPU/IR/Traits.h\"\n+#include \"triton/Analysis/Utility.h\"\n+\n+mlir::LogicalResult\n+mlir::OpTrait::impl::verifyResultsAreSharedEncoding(Operation *op) {\n+  if (failed(verifyAtLeastNResults(op, 1)))\n+    return failure();\n+\n+  for (auto result : op->getResults())\n+    if (!isSharedEncoding(result))\n+      return op->emitOpError() << \"requires all results to be shared encoding\";\n+\n+  return success();\n+};"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -118,6 +118,10 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n       builder.setInsertionPoint(curr);\n       if (auto load = dyn_cast<triton::LoadOp>(curr))\n         coalesceOp<triton::LoadOp>(axisInfo, curr, load.ptr(), builder);\n+      if (auto op = dyn_cast<triton::AtomicRMWOp>(curr))\n+        coalesceOp<triton::AtomicRMWOp>(axisInfo, curr, op.ptr(), builder);\n+      if (auto op = dyn_cast<triton::AtomicCASOp>(curr))\n+        coalesceOp<triton::AtomicCASOp>(axisInfo, curr, op.ptr(), builder);\n       if (auto load = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr))\n         coalesceOp<triton::gpu::InsertSliceAsyncOp>(axisInfo, curr, load.src(),\n                                                     builder);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 33, "deletions": 19, "changes": 52, "file_content_changes": "@@ -111,37 +111,41 @@ class SimplifyConversion : public mlir::RewritePattern {\n     // cvt(insert_slice(x), type2) -> insert_slice(cvt(x, type2))\n     auto insert_slice = dyn_cast<triton::gpu::InsertSliceAsyncOp>(arg);\n     if (insert_slice) {\n-      auto newType = op->getResult(0).getType();\n+      auto newType = op->getResult(0).getType().cast<RankedTensorType>();\n       // Ensure that the new insert_slice op is placed in the same place as the\n       // old insert_slice op. Otherwise, the new insert_slice op may be placed\n       // after the async_wait op, which is not allowed.\n       OpBuilder::InsertionGuard guard(rewriter);\n       rewriter.setInsertionPoint(insert_slice);\n-      auto new_arg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+      auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n           op->getLoc(), newType, insert_slice.dst());\n       rewriter.replaceOpWithNewOp<triton::gpu::InsertSliceAsyncOp>(\n-          op, newType, insert_slice.src(), new_arg.getResult(),\n+          op, newType, insert_slice.src(), newArg.getResult(),\n           insert_slice.index(), insert_slice.mask(), insert_slice.other(),\n-          insert_slice.cache(), insert_slice.evict(), insert_slice.isVolatile(),\n-          insert_slice.axis());\n+          insert_slice.cache(), insert_slice.evict(),\n+          insert_slice.isVolatile(), insert_slice.axis());\n       return mlir::success();\n     }\n-    // cvt(extract_slice(x), type2) ->extract_slice(cvt(x, type2))\n-    auto extract_slice = dyn_cast<triton::gpu::ExtractSliceOp>(arg);\n+    // cvt(extract_slice(x), type2) -> extract_slice(cvt(x, type2))\n+    auto extract_slice = dyn_cast<tensor::ExtractSliceOp>(arg);\n     if (extract_slice) {\n-      auto origType = extract_slice.src().getType().cast<RankedTensorType>();\n+      auto origType = extract_slice.source().getType().cast<RankedTensorType>();\n+      auto newType = RankedTensorType::get(\n+          origType.getShape(), origType.getElementType(),\n+          op->getResult(0).getType().cast<RankedTensorType>().getEncoding());\n+      auto resType = op->getResult(0).getType().cast<RankedTensorType>();\n       // Ensure that the new extract_slice op is placed in the same place as the\n       // old extract_slice op. Otherwise, the new extract_slice op may be placed\n       // after the async_wait op, which is not allowed.\n       OpBuilder::InsertionGuard guard(rewriter);\n       rewriter.setInsertionPoint(extract_slice);\n-      auto newType = RankedTensorType::get(\n-          origType.getShape(), origType.getElementType(),\n-          op->getResult(0).getType().cast<RankedTensorType>().getEncoding());\n-      auto new_arg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          op->getLoc(), newType, extract_slice.src());\n-      rewriter.replaceOpWithNewOp<triton::gpu::ExtractSliceOp>(\n-          op, new_arg.getResult(), extract_slice.index(), extract_slice.axis());\n+      auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+          op->getLoc(), newType, extract_slice.source());\n+      rewriter.replaceOpWithNewOp<tensor::ExtractSliceOp>(\n+          op, resType, newArg.getResult(), extract_slice.offsets(),\n+          extract_slice.sizes(), extract_slice.strides(),\n+          extract_slice.static_offsets(), extract_slice.static_sizes(),\n+          extract_slice.static_strides());\n       return mlir::success();\n     }\n     // cvt(type2, x)\n@@ -198,9 +202,9 @@ static LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n inline bool expensive_to_remat(Operation *op) {\n   if (!op)\n     return true;\n-  if (isa<triton::gpu::ExtractSliceOp, triton::gpu::AllocTensorOp,\n+  if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n           triton::gpu::InsertSliceAsyncOp, triton::LoadOp, triton::StoreOp,\n-          triton::DotOp>(op))\n+          triton::AtomicRMWOp, triton::AtomicCASOp, triton::DotOp>(op))\n     return true;\n   if (isa<scf::YieldOp, scf::ForOp>(op))\n     return true;\n@@ -478,7 +482,9 @@ class RematerializeForward : public mlir::RewritePattern {\n \n     SetVector<Operation *> cvtSlices;\n     auto filter = [&](Operation *op) {\n-      return isInLoop(op) && !isa<triton::LoadOp>(op) &&\n+      return isInLoop(op) &&\n+             !isa<triton::LoadOp, triton::StoreOp, triton::AtomicRMWOp,\n+                  triton::AtomicCASOp>(op) &&\n              !isa<triton::DotOp>(op) && !isa<scf::YieldOp>(op) &&\n              !isa<triton::gpu::ConvertLayoutOp>(op);\n     };\n@@ -570,6 +576,14 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto oldRetType = dotOp.getResult().getType().cast<RankedTensorType>();\n     if (oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n       return failure();\n+\n+    auto A = dotOp.getOperand(0).getType().cast<RankedTensorType>();\n+    auto B = dotOp.getOperand(1).getType().cast<RankedTensorType>();\n+    // for FMA, should retain the blocked layout.\n+    if (A.getElementType().isF32() && B.getElementType().isF32() &&\n+        !dotOp.allowTF32())\n+      return failure();\n+\n     // get MMA encoding for the given number of warps\n     auto retShape = oldRetType.getShape();\n     auto mod = op->getParentOfType<mlir::ModuleOp>();\n@@ -623,4 +637,4 @@ class TritonGPUCombineOpsPass\n \n std::unique_ptr<Pass> mlir::createTritonGPUCombineOpsPass() {\n   return std::make_unique<TritonGPUCombineOpsPass>();\n-}\n\\ No newline at end of file\n+}"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 50, "deletions": 41, "changes": 91, "file_content_changes": "@@ -15,6 +15,14 @@ using namespace mlir;\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n+static Type getI1SameShape(Value v) {\n+  Type vType = v.getType();\n+  auto i1Type = IntegerType::get(vType.getContext(), 1);\n+  auto tensorType = vType.cast<RankedTensorType>();\n+  return RankedTensorType::get(tensorType.getShape(), i1Type,\n+                               tensorType.getEncoding());\n+}\n+\n namespace {\n class LoopPipeliner {\n   /// cache forOp we are working on\n@@ -262,13 +270,23 @@ void LoopPipeliner::emitPrologue() {\n           loadStageBuffer[op->getResult(0)] = {loadsBuffer[op->getResult(0)]};\n         }\n         // load => copy async\n-        // TODO: check if the hardware supports async copy\n         if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n+          Value mask = lookupOrDefault(loadOp.mask(), stage);\n+          Value newMask;\n+          if (mask) {\n+            Value splatCond = builder.create<triton::SplatOp>(\n+                mask.getLoc(), mask.getType(), loopCond);\n+            newMask =\n+                builder.create<arith::AndIOp>(mask.getLoc(), mask, splatCond);\n+          } else {\n+            newMask = builder.create<triton::SplatOp>(\n+                loopCond.getLoc(), getI1SameShape(loadOp), loopCond);\n+          }\n+          // TODO: check if the hardware supports async copy\n           newOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n               op->getLoc(), loadsBuffer[loadOp].getType(),\n               lookupOrDefault(loadOp.ptr(), stage),\n-              loadStageBuffer[loadOp][stage], pipelineIterIdx,\n-              lookupOrDefault(loadOp.mask(), stage),\n+              loadStageBuffer[loadOp][stage], pipelineIterIdx, newMask,\n               lookupOrDefault(loadOp.other(), stage), loadOp.cache(),\n               loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n           loadStageBuffer[loadOp].push_back(newOp->getResult(0));\n@@ -287,33 +305,6 @@ void LoopPipeliner::emitPrologue() {\n         }\n       }\n \n-      // If this is a load/async_copy, we need to update the mask\n-      if (Value mask = [&]() {\n-            if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(newOp)) {\n-              return loadOp.mask();\n-            } else if (auto insertSliceAsyncOp =\n-                           llvm::dyn_cast<triton::gpu::InsertSliceAsyncOp>(\n-                               newOp)) {\n-              return insertSliceAsyncOp.mask();\n-            } else {\n-              return mlir::Value();\n-            }\n-          }()) {\n-        // assert(I1 or TensorOf<[I1]>);\n-        OpBuilder::InsertionGuard g(builder);\n-        // TODO: move this out of the loop\n-        builder.setInsertionPoint(newOp);\n-        Value splatCond = builder.create<triton::SplatOp>(\n-            mask.getLoc(), mask.getType(), loopCond);\n-        Value newMask =\n-            builder.create<arith::AndIOp>(mask.getLoc(), mask, splatCond);\n-        // TODO: better way to do this?\n-        if (llvm::isa<triton::LoadOp>(newOp))\n-          newOp->setOperand(1, newMask);\n-        else // InsertSliceAsyncOp\n-          newOp->setOperand(3, newMask);\n-      }\n-\n       // update mapping of results\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n         Value originalResult = op->getResult(dstIdx);\n@@ -332,21 +323,27 @@ void LoopPipeliner::emitPrologue() {\n                 newOp->getResult(dstIdx), stage + 1);\n         }\n       }\n-    }\n+    } // for (Operation *op : orderedDeps)\n \n     pipelineIterIdx = builder.create<arith::AddIOp>(\n         iv.getLoc(), pipelineIterIdx,\n         builder.create<arith::ConstantIntOp>(iv.getLoc(), 1, 32));\n   } // for (int stage = 0; stage < numStages - 1; ++stage)\n \n+  auto intAttr = [&](int64_t v) { return builder.getI64IntegerAttr(v); };\n+\n   // async.wait & extract_slice\n   builder.create<triton::gpu::AsyncWaitOp>(loads[0].getLoc(),\n                                            loads.size() * (numStages - 2));\n   loopIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n   for (Value loadOp : loads) {\n-    Value extractSlice = builder.create<triton::gpu::ExtractSliceOp>(\n-        loadOp.getLoc(), loadsMapping[loadOp].getType(),\n-        loadStageBuffer[loadOp][numStages - 1], loopIterIdx, /*axis*/ 0);\n+    auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n+    Value extractSlice = builder.create<tensor::ExtractSliceOp>(\n+        loadOp.getLoc(), sliceType, loadStageBuffer[loadOp][numStages - 1],\n+        SmallVector<OpFoldResult>{intAttr(0), intAttr(0), intAttr(0)},\n+        SmallVector<OpFoldResult>{intAttr(1), intAttr(sliceType.getShape()[0]),\n+                                  intAttr(sliceType.getShape()[1])},\n+        SmallVector<OpFoldResult>{intAttr(1), intAttr(1), intAttr(1)});\n     loadsExtract[loadOp] = extractSlice;\n   }\n   // bump up loopIterIdx, this is used for getting the correct slice for the\n@@ -477,35 +474,47 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   Value extractSliceIndex = builder.create<arith::RemSIOp>(\n       nextIV.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n+  extractSliceIndex = builder.create<arith::IndexCastOp>(\n+      extractSliceIndex.getLoc(), builder.getIndexType(), extractSliceIndex);\n+\n+  auto intAttr = [&](int64_t v) { return builder.getI64IntegerAttr(v); };\n \n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n-    // TODO(da): does this work if loadOp has no mask?\n     // update loading mask\n     if (loads.contains(op->getResult(0))) {\n       auto loadOp = llvm::cast<triton::LoadOp>(op);\n       Value mask = loadOp.mask();\n+      Value newMask;\n       if (mask) {\n         Value splatCond = builder.create<triton::SplatOp>(\n             mask.getLoc(), mask.getType(), nextLoopCond);\n-        Value newMask = builder.create<arith::AndIOp>(\n+        newMask = builder.create<arith::AndIOp>(\n             mask.getLoc(), splatCond, nextMapping.lookupOrDefault(mask));\n         // if mask is defined outside the loop, don't update the map more than\n         // once\n         if (!(forOp.isDefinedOutsideOfLoop(mask) && nextMapping.contains(mask)))\n           nextMapping.map(mask, newMask);\n-      }\n+        newMask = nextMapping.lookupOrDefault(loadOp.mask());\n+      } else\n+        newMask = builder.create<triton::SplatOp>(\n+            loadOp.getLoc(), getI1SameShape(loadOp), nextLoopCond);\n       Value insertAsyncOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n           op->getLoc(), loadsBuffer[loadOp].getType(),\n           nextMapping.lookupOrDefault(loadOp.ptr()),\n           newForOp.getRegionIterArgs()[bufferIdx + nextBuffers.size()],\n-          insertSliceIndex, nextMapping.lookupOrDefault(loadOp.mask()),\n+          insertSliceIndex, newMask,\n           nextMapping.lookupOrDefault(loadOp.other()), loadOp.cache(),\n           loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n       nextBuffers.push_back(insertAsyncOp);\n-      nextOp = builder.create<triton::gpu::ExtractSliceOp>(\n-          op->getLoc(), loadsMapping[loadOp].getType(), insertAsyncOp,\n-          extractSliceIndex, /*axis*/ 0);\n+      auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n+      nextOp = builder.create<tensor::ExtractSliceOp>(\n+          op->getLoc(), sliceType, insertAsyncOp,\n+          SmallVector<OpFoldResult>{extractSliceIndex, intAttr(0), intAttr(0)},\n+          SmallVector<OpFoldResult>{intAttr(1),\n+                                    intAttr(sliceType.getShape()[0]),\n+                                    intAttr(sliceType.getShape()[1])},\n+          SmallVector<OpFoldResult>{intAttr(1), intAttr(1), intAttr(1)});\n       extractSlices.push_back(nextOp->getResult(0));\n     } else\n       nextOp = builder.clone(*op, nextMapping);"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 27, "deletions": 15, "changes": 42, "file_content_changes": "@@ -151,7 +151,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n     return nullptr;\n   }\n \n-  std::map<std::string, std::string> extern_libs;\n+  std::map<std::string, std::string> externLibs;\n   SmallVector<LLVM::LLVMFuncOp> funcs;\n   module.walk([&](LLVM::LLVMFuncOp func) {\n     if (func.isExternal())\n@@ -166,7 +166,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n           func.getOperation()->getAttr(\"libpath\").dyn_cast<StringAttr>();\n       if (name) {\n         std::string lib_name = name.str();\n-        extern_libs[lib_name] = path.str();\n+        externLibs[lib_name] = path.str();\n       }\n     }\n   }\n@@ -176,7 +176,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n                     ->getAttr(\"triton_gpu.externs\")\n                     .dyn_cast<DictionaryAttr>();\n     for (auto &attr : dict) {\n-      extern_libs[attr.getName().strref().trim().str()] =\n+      externLibs[attr.getName().strref().trim().str()] =\n           attr.getValue().dyn_cast<StringAttr>().strref().trim().str();\n     }\n   }\n@@ -188,19 +188,9 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n   }\n \n   llvm::SMDiagnostic err;\n-  for (auto &lib : extern_libs) {\n-    auto ext_mod = llvm::parseIRFile(lib.second, err, *llvmContext);\n-    if (!ext_mod) {\n-      llvm::errs() << \"Failed to load extern lib \" << lib.first;\n+  for (auto &lib : externLibs) {\n+    if (linkExternLib(*llvmir, lib.second))\n       return nullptr;\n-    }\n-    ext_mod->setTargetTriple(llvmir->getTargetTriple());\n-    ext_mod->setDataLayout(llvmir->getDataLayout());\n-\n-    if (llvm::Linker::linkModules(*llvmir, std::move(ext_mod))) {\n-      llvm::errs() << \"Failed to link extern lib \" << lib.first;\n-      return nullptr;\n-    }\n   }\n \n   return llvmir;\n@@ -226,5 +216,27 @@ void addExternalLibs(mlir::ModuleOp &module,\n   return;\n }\n \n+bool linkExternLib(llvm::Module &module, llvm::StringRef path) {\n+  llvm::SMDiagnostic err;\n+  auto &ctx = module.getContext();\n+\n+  auto extMod = llvm::parseIRFile(path, err, ctx);\n+  if (!extMod) {\n+    llvm::errs() << \"Failed to load \" << path;\n+    return true;\n+  }\n+\n+  extMod->setTargetTriple(module.getTargetTriple());\n+  extMod->setDataLayout(module.getDataLayout());\n+\n+  if (llvm::Linker::linkModules(module, std::move(extMod),\n+                                llvm::Linker::Flags::LinkOnlyNeeded)) {\n+    llvm::errs() << \"Failed to link \" << path;\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 38, "deletions": 0, "changes": 38, "file_content_changes": "@@ -29,6 +29,7 @@\n #include \"llvm/Target/TargetOptions.h\"\n #include \"llvm/Transforms/Scalar.h\"\n #include \"llvm/Transforms/Utils/Cloning.h\"\n+#include <filesystem>\n #include <regex>\n \n namespace triton {\n@@ -61,6 +62,43 @@ static bool find_and_replace(std::string &str, const std::string &begin,\n }\n \n static std::string llir_to_ptx(llvm::Module *module, int capability, int ptx) {\n+  bool hasExternal = false;\n+  for (auto &func : *module) {\n+    if (func.hasExternalLinkage()) {\n+      hasExternal = true;\n+      break;\n+    }\n+  }\n+\n+  if (hasExternal) {\n+    namespace fs = std::filesystem;\n+    // [triton root dir]/python/triton/language/libdevice.10.bc\n+    static const fs::path libdevice = fs::path(__FILE__)\n+                                          .parent_path()\n+                                          .parent_path()\n+                                          .parent_path()\n+                                          .parent_path() /\n+                                      \"python\" / \"triton\" / \"language\" /\n+                                      \"libdevice.10.bc\";\n+    if (mlir::triton::linkExternLib(*module, libdevice.string()))\n+      llvm::errs() << \"link failed for: \" << libdevice.string();\n+  }\n+\n+  // please check https://llvm.org/docs/NVPTXUsage.html#reflection-parameters\n+  // this will enable fast math path in libdevice\n+  // for example, when enable nvvm-reflect-ftz, sqrt.approx.f32 will change to\n+  // sqrt.approx.ftz.f32\n+  {\n+    auto &ctx = module->getContext();\n+    llvm::Type *I32 = llvm::Type::getInt32Ty(ctx);\n+    llvm::Metadata *mdFour =\n+        llvm::ConstantAsMetadata::get(llvm::ConstantInt::getSigned(I32, 4));\n+    llvm::Metadata *mdName = llvm::MDString::get(ctx, \"nvvm-reflect-ftz\");\n+    llvm::Metadata *mdOne =\n+        llvm::ConstantAsMetadata::get(llvm::ConstantInt::getSigned(I32, 1));\n+    llvm::MDNode *reflect = llvm::MDNode::get(ctx, {mdFour, mdName, mdOne});\n+    module->addModuleFlag(reflect);\n+  }\n   // LLVM version in use may not officially support target hardware\n   int max_nvvm_cc = 75;\n   // int max_nvvm_ptx = 74;"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 12, "deletions": 16, "changes": 28, "file_content_changes": "@@ -261,7 +261,7 @@ void init_triton_ir(py::module &&m) {\n           },\n           ret::reference)\n       .def(\"dump\", [](mlir::OpState &self) { self->dump(); })\n-      .def(\"str\",\n+      .def(\"__str__\",\n            [](mlir::OpState &self) -> std::string {\n              std::string str;\n              llvm::raw_string_ostream os(str);\n@@ -319,28 +319,22 @@ void init_triton_ir(py::module &&m) {\n   m.def(\n       \"parse_mlir_module\",\n       [](const std::string &inputFilename, mlir::MLIRContext &context) {\n-        // open file\n-        std::string errorMessage;\n-        auto input = mlir::openInputFile(inputFilename, &errorMessage);\n-        if (!input)\n-          throw std::runtime_error(errorMessage);\n-\n         // initialize registry\n         mlir::DialectRegistry registry;\n         registry.insert<mlir::triton::TritonDialect,\n                         mlir::triton::gpu::TritonGPUDialect,\n                         mlir::math::MathDialect, mlir::arith::ArithmeticDialect,\n                         mlir::StandardOpsDialect, mlir::scf::SCFDialect>();\n-\n         context.appendDialectRegistry(registry);\n         context.loadAllAvailableDialects();\n-        context.allowUnregisteredDialects();\n \n         // parse module\n-        llvm::SourceMgr sourceMgr;\n-        sourceMgr.AddNewSourceBuffer(std::move(input), llvm::SMLoc());\n         mlir::OwningOpRef<mlir::ModuleOp> module(\n-            mlir::parseSourceFile(sourceMgr, &context));\n+            mlir::parseSourceFile(inputFilename, &context));\n+        // locations are incompatible with ptx < 7.5 !\n+        module->walk([](mlir::Operation *op) {\n+          op->setLoc(mlir::UnknownLoc::get(op->getContext()));\n+        });\n         if (!module)\n           throw std::runtime_error(\"Parse MLIR file failed.\");\n \n@@ -1081,7 +1075,8 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::OpBuilder &self, mlir::Value &ptr, mlir::Value &cmp,\n               mlir::Value &val) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             auto ptrType = ptr.getType().dyn_cast<mlir::triton::PointerType>();\n+             auto ptrType = mlir::getElementTypeOrSelf(ptr)\n+                                .cast<mlir::triton::PointerType>();\n              mlir::Type dstType = ptrType.getPointeeType();\n              return self.create<mlir::triton::AtomicCASOp>(loc, dstType, ptr,\n                                                            cmp, val);\n@@ -1091,7 +1086,8 @@ void init_triton_ir(py::module &&m) {\n               mlir::Value &ptr, mlir::Value &val,\n               mlir::Value &mask) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             auto ptrType = ptr.getType().dyn_cast<mlir::triton::PointerType>();\n+             auto ptrType = mlir::getElementTypeOrSelf(ptr)\n+                                .cast<mlir::triton::PointerType>();\n              mlir::Type dstType = ptrType.getPointeeType();\n              return self.create<mlir::triton::AtomicRMWOp>(loc, dstType, rmwOp,\n                                                            ptr, val, mask);\n@@ -1281,8 +1277,8 @@ void init_triton_translation(py::module &m) {\n   using ret = py::return_value_policy;\n \n   m.def(\"get_shared_memory_size\", [](mlir::ModuleOp module) {\n-    return module->getAttrOfType<mlir::IntegerAttr>(\"triton_gpu.shared\")\n-        .getInt();\n+    auto shared = module->getAttrOfType<mlir::IntegerAttr>(\"triton_gpu.shared\");\n+    return shared.getInt();\n   });\n \n   m.def("}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 34, "deletions": 3, "changes": 37, "file_content_changes": "@@ -144,7 +144,7 @@ def kernel(Z, X, SIZE: tl.constexpr):\n     # triton result\n     x_tri = to_triton(x, device=device, dst_type=dtype_x)\n     z_tri = to_triton(np.empty_like(z_ref), device=device, dst_type=dtype_x)\n-    kernel[(1, )](z_tri, x_tri, SIZE=SIZE, num_warps=4, extern_libs={\"libdevice\": \"/usr/local/cuda/nvvm/libdevice/libdevice.10.bc\"})\n+    kernel[(1, )](z_tri, x_tri, SIZE=SIZE, num_warps=4)\n     # compare\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n \n@@ -940,7 +940,9 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n # shape (128, 256) and (32, 1024) are not enabled on sm86 because the required shared memory\n # exceeds the limit of 99KB\n-reduce2d_shapes = [(2, 32), (4, 32), (4, 128), (32, 64), (64, 128)]\n+reduce2d_shapes = [(2, 32), (4, 32), (4, 128)]\n+# TODO: fix and uncomment\n+#, (32, 64), (64, 128)]\n if 'V100' in torch.cuda.get_device_name(0):\n     reduce2d_shapes += [(128, 256) and (32, 1024)]\n \n@@ -1544,7 +1546,7 @@ def _kernel(dst):\n                          [('int32', 'libdevice.ffs', ''),\n                           ('float32', 'libdevice.pow', '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'),\n                           ('float64', 'libdevice.norm4d', '')])\n-def test_libdevice(dtype_str, expr, lib_path):\n+def test_libdevice_tensor(dtype_str, expr, lib_path):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -1580,3 +1582,32 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n         np.testing.assert_equal(y_ref, to_numpy(y_tri))\n     else:\n         np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n+\n+\n+@pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n+                         [('float32', 'libdevice.pow', '')])\n+def test_libdevice_scalar(dtype_str, expr, lib_path):\n+\n+    @triton.jit\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        x = X\n+        y = GENERATE_TEST_HERE\n+        tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+    shape = (128, )\n+    rs = RandomState(17)\n+    # limit the range of integers so that the sum does not overflow\n+    x = numpy_random((1,), dtype_str=dtype_str, rs=rs)\n+    y_ref = np.zeros(shape, dtype=x.dtype)\n+\n+    # numpy does not allow negative factors in power, so we use abs()\n+    x = np.abs(x)\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+    y_ref[:] = np.power(x, x)\n+\n+    # triton result\n+    x_tri = to_triton(x)[0].item()\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n+    # compare\n+    np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)"}, {"filename": "python/tests/test_elementwise.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -137,7 +137,7 @@ def kernel(X0, X1, Y, BLOCK: tl.constexpr):\n     # reference result\n \n     if expr == \"cdiv\":\n-        y_ref = (x0 + x1 - 1) // x1\n+        y_ref = torch.div(x0 + x1 - 1, x1, rounding_mode='trunc')\n     elif expr == \"umulhi\":\n         y_ref = ((x0.to(torch.int64) * x1) >> 32).to(torch.int32)\n     else:"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 89, "deletions": 2, "changes": 91, "file_content_changes": "@@ -55,6 +55,33 @@ def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n     assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n \n \n+@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n+    [64, 128, 128, 1],\n+    [128, 128, 128, 4],\n+    [16, 8, 32, 1],\n+    [32, 16, 64, 2],\n+    [32, 16, 64, 4],\n+])\n+def test_gemm_no_scf_int8(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n+    a = torch.randint(-5, 5, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n+    b = torch.randint(-5, 5, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n+    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.int32)\n+\n+    grid = lambda META: (1, )\n+    matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n+                               stride_am=a.stride(0), stride_ak=a.stride(1),\n+                               stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                               stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                               M=SIZE_M, N=SIZE_N, K=SIZE_K,\n+                               num_warps=NUM_WARPS)\n+\n+    aa = a.cpu()\n+    bb = b.cpu()\n+    golden = torch.matmul(aa.float(), bb.float()).int()\n+    torch.set_printoptions(profile=\"full\")\n+    torch.testing.assert_close(c.cpu(), golden, check_dtype=False)\n+\n+\n @triton.jit\n def matmul_kernel(\n     a_ptr, b_ptr, c_ptr,\n@@ -80,8 +107,6 @@ def matmul_kernel(\n     c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n     tl.store(c_ptrs, accumulator)\n \n-# TODO: DotConversion in TritonGPUToLLVM cannot support non-splat C for the moment\n-\n \n def get_variant_golden(a, b):\n     SIZE_M = a.shape[0]\n@@ -144,3 +169,65 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n \n     torch.set_printoptions(profile=\"full\")\n     assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n+\n+\n+@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n+    [32, 32, 16, 4, 32, 32, 16],\n+    [32, 16, 16, 4, 32, 32, 16],\n+    [128, 8, 8, 4, 32, 32, 16],\n+    [127, 41, 43, 4, 32, 32, 16],\n+])\n+def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n+    @triton.jit\n+    def matmul_kernel(\n+        a_ptr, b_ptr, c_ptr,\n+        M, N, K,\n+        stride_am, stride_ak,\n+        stride_bk, stride_bn,\n+        stride_cm, stride_cn,\n+        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+    ):\n+        pid = tl.program_id(axis=0)\n+        # num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+        pid_m = pid // num_pid_n\n+        pid_n = pid % num_pid_n\n+\n+        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        offs_k = tl.arange(0, BLOCK_SIZE_K)\n+        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+\n+        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        for k in range(0, K, BLOCK_SIZE_K):\n+            a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n+            b_mask = (offs_k[:, None] < K) & (offs_bn[None, :] < N)\n+            a = tl.load(a_ptrs, a_mask)\n+            b = tl.load(b_ptrs, b_mask)\n+            # NOTE the allow_tf32 should be false to force the dot op to do fmadot lowering\n+            accumulator += tl.dot(a, b, allow_tf32=False)\n+            a_ptrs += BLOCK_SIZE_K * stride_ak\n+            b_ptrs += BLOCK_SIZE_K * stride_bk\n+            offs_k += BLOCK_SIZE_K\n+\n+        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n+        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n+        tl.store(c_ptrs, accumulator, c_mask)\n+\n+    a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n+    b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n+    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n+\n+    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n+    matmul_kernel[grid](a, b, c,\n+                        M, N, K,\n+                        stride_am=a.stride(0), stride_ak=a.stride(1),\n+                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K)\n+\n+    golden = torch.matmul(a, b)\n+    torch.testing.assert_close(c, golden)"}, {"filename": "python/tests/test_math_ops.py", "status": "removed", "additions": 0, "deletions": 33, "changes": 33, "file_content_changes": "@@ -1,33 +0,0 @@\n-\n-import triton\n-import triton.language as tl\n-\n-\n-@triton.jit\n-def math_kernel(x1_ptr, x2_ptr, x3_ptr, x4_ptr, n, BLOCK_SIZE: tl.constexpr):\n-    offsets = tl.arange(0, BLOCK_SIZE)\n-    x1 = tl.load(x1_ptr + offsets, mask=offsets < n)\n-    x2 = tl.load(x2_ptr + offsets, mask=offsets < n)\n-    x3 = tl.load(x3_ptr + offsets, mask=offsets < n)\n-    x4 = tl.load(x4_ptr + offsets, mask=offsets < n)\n-\n-    y1 = tl.sin(x1)\n-    y2 = tl.libdevice.sin(x2)\n-    y3 = tl.libdevice.div_rn(x3, x3)\n-    y4 = tl.libdevice.fma_rd(x4, x4, x4)\n-\n-    tl.store(x1_ptr + offsets, y1, mask=offsets < n)\n-    tl.store(x2_ptr + offsets, y2, mask=offsets < n)\n-    tl.store(x3_ptr + offsets, y3, mask=offsets < n)\n-    tl.store(x4_ptr + offsets, y4, mask=offsets < n)\n-\n-\n-def test_empty_kernel_cubin_compile():\n-    kernel = triton.compiler._compile(math_kernel,\n-                                      \"*fp32,*fp32,*fp32,*fp32,i32\",\n-                                      device=0,\n-                                      constants={\"BLOCK_SIZE\": 256},\n-                                      output=\"ttgir\")  # \"cubin\"\n-    assert kernel\n-    # TODO: Check if the values are correct.\n-    # TODO: Cover all the math operators"}, {"filename": "python/tests/test_reduce.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -126,7 +126,6 @@ def test_reduce2d(op, dtype, shape, axis):\n         golden_z = torch.min(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n     else:\n         golden_z = torch.max(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n-\n     if dtype.is_floating_point and op == 'sum':\n         if shape[axis] >= 256:\n             assert_close(z, golden_z, rtol=0.05, atol=0.1)"}, {"filename": "python/tests/test_type.py", "status": "removed", "additions": 0, "deletions": 80, "changes": 80, "file_content_changes": "@@ -1,80 +0,0 @@\n-import triton\n-import triton.language as tl\n-\n-\n-# TODO: function with no arguments don't work\n-@triton.jit\n-def binop_type_check(X):\n-    # 0d-tensor is not allowed.\n-    # zero_0d = tl.zeros([], dtype=tl.float32)\n-    zero_1d = tl.zeros([2], dtype=tl.float32)\n-    zero_2d_21 = tl.zeros([2, 1], dtype=tl.float32)\n-    zero_2d_22 = tl.zeros([2, 2], dtype=tl.float32)\n-\n-    # scalar + scalar -> scalar\n-    a0 = 0.0 + 0.0\n-    # # scalar + 0D -> 0D\n-    # a1 = 0.0 + zero_0d\n-    # a2 = zero_0d + 0.0\n-    # scalar + 1D -> 1D\n-    a3 = 0.0 + zero_1d\n-    a4 = zero_1d + 0.0\n-    # scalar + 2D -> 2D\n-    a5 = 0.0 + zero_2d_22\n-    a6 = zero_2d_22 + 0.0\n-\n-    # # 0D + 0D -> 0D\n-    # b1 = zero_0d + zero_0d\n-    # # 0D + 1D -> 1D\n-    # b2 = zero_0d + zero_1d\n-    # b3 = zero_1d + zero_0d\n-    # # 0D + 2D -> 2D\n-    # b4 = zero_0d + zero_2d_22\n-    # b5 = zero_2d_22 + zero_0d\n-\n-    # 1D + 1D -> 1D\n-    c1 = zero_1d + zero_1d\n-    # 1D + 2D -> 2D\n-    c2 = zero_1d + zero_2d_21\n-    c3 = zero_1d + zero_2d_22\n-    c4 = zero_2d_21 + zero_1d\n-    c5 = zero_2d_22 + zero_1d\n-\n-    # 2D + 2D -> 2D\n-    d1 = zero_2d_21 + zero_2d_21\n-    d2 = zero_2d_22 + zero_2d_22\n-    d3 = zero_2d_21 + zero_2d_22\n-    d4 = zero_2d_22 + zero_2d_21\n-\n-    # return a0, a1, a2, a3, a4, a5, a6, b1, b2, b3, b4, b5, c1, c2, c3, c4, c5, d1, d2, d3, d4\n-    return a0, a3, a4, a5, a6, c1, c2, c3, c4, c5, d1, d2, d3, d4\n-\n-\n-def test_binop_type_check():\n-    kernel = triton.compiler._compile(binop_type_check,\n-                                      signature=\"*fp32\",\n-                                      device=0,\n-                                      output=\"ttir\")\n-    assert (kernel)\n-    # TODO: Check types of the results\n-\n-\n-@triton.jit\n-def reduce_type_check(ptr):\n-    v_32 = tl.load(ptr + tl.arange(0, 32))\n-    v_scalar = tl.min(v_32, axis=0)\n-    tl.store(ptr, v_scalar)\n-    v_64x128 = tl.load(ptr + tl.arange(0, 64)[:, None] + tl.arange(0, 128)[None, :])\n-    v_64 = tl.max(v_64x128, axis=1)\n-    tl.store(ptr + tl.arange(0, 64), v_64)\n-    v_128 = tl.max(v_64x128, axis=0)\n-    tl.store(ptr + tl.arange(0, 128), v_128)\n-\n-\n-def test_reduce_type_check():\n-    kernel = triton.compiler._compile(reduce_type_check,\n-                                      signature=\"*fp32\",\n-                                      device=0,\n-                                      output=\"ttir\")\n-    assert (kernel)\n-    # TODO: Check types of the results"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 101, "deletions": 79, "changes": 180, "file_content_changes": "@@ -16,14 +16,16 @@\n import warnings\n from collections import namedtuple\n from sysconfig import get_paths\n-from typing import Any, Dict, Tuple, Union\n+from typing import Any, Callable, Dict, Tuple, Union\n \n+from pathlib import Path\n import setuptools\n import torch\n from filelock import FileLock\n \n import triton\n import triton._C.libtriton.triton as _triton\n+from .tools.disasm import extract\n \n \n def str_to_ty(name):\n@@ -822,7 +824,10 @@ def kernel_suffix(signature, specialization):\n # ------------------------------------------------------------------------------\n \n \n-def make_triton_ir(fn, signature, specialization, constants):\n+def build_triton_ir(fn, signature, specialization, constants):\n+    # canonicalize signature\n+    if isinstance(signature, str):\n+      signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n     context = _triton.ir.context()\n     context.load_triton()\n     # create kernel prototype\n@@ -852,7 +857,6 @@ def make_triton_ir(fn, signature, specialization, constants):\n     ret.context = context\n     return ret, generator\n \n-\n def optimize_triton_ir(mod):\n     pm = _triton.ir.pass_manager(mod.context)\n     pm.enable_debug()\n@@ -864,19 +868,14 @@ def optimize_triton_ir(mod):\n     pm.run(mod)\n     return mod\n \n+def ast_to_ttir(fn, signature, specialization, constants):\n+    mod, _ = build_triton_ir(fn, signature, specialization, constants)\n+    return optimize_triton_ir(mod)\n \n-def make_tritongpu_ir(mod, num_warps):\n+def ttir_to_ttgir(mod, num_warps, num_stages):\n     pm = _triton.ir.pass_manager(mod.context)\n     pm.add_convert_triton_to_tritongpu_pass(num_warps)\n-    pm.run(mod)\n-    return mod\n-\n-\n-def optimize_tritongpu_ir(mod, num_stages):\n-    pm = _triton.ir.pass_manager(mod.context)\n     pm.enable_debug()\n-    # Get error in backend due to wrong conversion in expanding async-related instruction.\n-    # TODO[Superjomn]: Open it when fixed.\n     pm.add_tritongpu_pipeline_pass(num_stages)\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n@@ -897,28 +896,41 @@ def add_external_libs(mod, libs):\n     _triton.add_external_libs(mod, list(libs.keys()), list(libs.values()))\n \n \n-def make_llvm_ir(mod):\n+def ttgir_to_llir(mod, extern_libs):\n+    if extern_libs:\n+        add_external_libs(mod, extern_libs)\n     return _triton.translate_triton_gpu_to_llvmir(mod)\n \n \n-def make_ptx(mod: Any, compute_capability: int, ptx_version: int) -> Tuple[str, int]:\n+def llir_to_ptx(mod: Any, compute_capability: int = None, ptx_version: int = None) -> Tuple[str, int]:\n     '''\n     Translate TritonGPU module to PTX code.\n     :param mod: a TritonGPU dialect module\n     :return:\n         - PTX code\n         - shared memory alloaction size\n     '''\n+    if compute_capability is None:\n+        device = torch.cuda.current_device()\n+        compute_capability = torch.cuda.get_device_capability(device)\n+        compute_capability = compute_capability[0] * 10 + compute_capability[1]\n+    if ptx_version is None:\n+        _, cuda_version = path_to_ptxas()\n+        ptx_version = ptx_get_version(cuda_version)\n     return _triton.translate_llvmir_to_ptx(mod, compute_capability, ptx_version)\n \n \n-def make_cubin(ptx: str, ptxas: str, compute_capability: int):\n+\n+def ptx_to_cubin(ptx: str, device: int):\n     '''\n     Compile TritonGPU module to cubin.\n     :param ptx: ptx code\n     :param device: CUDA device\n     :return: str\n     '''\n+    ptxas, _ = path_to_ptxas()\n+    compute_capability = torch.cuda.get_device_capability(device)\n+    compute_capability = compute_capability[0] * 10 + compute_capability[1]\n     return _triton.compile_ptx_to_cubin(ptx, ptxas, compute_capability)\n \n \n@@ -978,46 +990,6 @@ def path_to_ptxas():\n instance_descriptor = namedtuple(\"instance_descriptor\", [\"divisible_by_16\", \"equal_to_1\"], defaults=[set(), set()])\n \n \n-def _compile(fn, signature: str, device: int = -1, constants=dict(), specialization=instance_descriptor(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, output: str = \"ttgir\") -> Tuple[str, int, str]:\n-    if isinstance(signature, str):\n-        signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n-    valid_outputs = (\"ttir\", \"ttgir\", \"ptx\", \"cubin\")\n-    assert output in valid_outputs, \"output should be one of [%s], but get \\\"%s\\\"\" % (','.join(valid_outputs), output)\n-\n-    # triton-ir\n-    module, _ = make_triton_ir(fn, signature, specialization, constants)\n-    module = optimize_triton_ir(module)\n-    if output == \"ttir\":\n-        return module.str()\n-\n-    # tritongpu-ir\n-    module = make_tritongpu_ir(module, num_warps)\n-    module = optimize_tritongpu_ir(module, num_stages)\n-    if output == \"ttgir\":\n-        return module.str()\n-\n-    if extern_libs:\n-        add_external_libs(module, extern_libs)\n-\n-    # llvm-ir\n-    llvm_ir = make_llvm_ir(module)\n-\n-    assert device >= 0, \"device should be provided.\"\n-    ptxas, cuda_version = path_to_ptxas()\n-    compute_capability = torch.cuda.get_device_capability(device)\n-    compute_capability = compute_capability[0] * 10 + compute_capability[1]\n-    ptx_version = ptx_get_version(cuda_version)\n-    ptx = make_ptx(llvm_ir, compute_capability, ptx_version)\n-    shem_size = _triton.get_shared_memory_size(module)\n-    kernel_name = ptx_get_kernel_name(ptx)\n-    if output == \"ptx\":\n-        return ptx, shem_size, kernel_name\n-\n-    cubin = make_cubin(ptx, ptxas, compute_capability)\n-    if output == \"cubin\":\n-        return cubin, ptx, shem_size, kernel_name\n-\n-    assert False\n \n \n # ------------------------------------------------------------------------------\n@@ -1306,6 +1278,23 @@ def make_fn_cache_key(fn_hash, signature, configs, constants, num_warps, num_sta\n     return key\n \n \n+def read_or_execute(cache_manager, force_compile, file_name, metadata,\n+                    run_if_found: Callable[[str], bytes] = None,\n+                    run_if_not_found: Callable = None):\n+    suffix = file_name.split(\".\")[1]\n+    if not force_compile and cache_manager.has_file(file_name):\n+      module = run_if_found(cache_manager._make_path(file_name))\n+      data = module if isinstance(module, bytes) else str(module).encode(\"utf-8\")\n+      md5 = hashlib.md5(data).hexdigest()\n+      has_changed = metadata and md5 != metadata[\"md5\"][suffix]\n+      return module, md5, has_changed, True\n+    module = run_if_not_found()\n+    data = module if isinstance(module, bytes) else str(module).encode(\"utf-8\")\n+    md5 = hashlib.md5(data).hexdigest()\n+    cache_manager.put(data, file_name, True if isinstance(data, bytes) else data)\n+    return module, md5, True, False\n+\n+\n def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, configs=None):\n     if isinstance(signature, str):\n         signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n@@ -1329,48 +1318,68 @@ def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: i\n             so = _build(fn.__name__, src_path, tmpdir)\n             with open(so, \"rb\") as f:\n                 so_cache_manager.put(f.read(), so_name, binary=True)\n-\n-    # retrieve cached shared object if it exists\n+    so_path = so_cache_manager._make_path(so_name)\n+    # create cache manager\n     fn_cache_key = make_fn_cache_key(fn.cache_key, signature, configs, constants, num_warps, num_stages)\n     fn_cache_manager = CacheManager(fn_cache_key)\n-    ptx_name = f\"{name}.ptx\"\n-    cubin_name = f\"{name}.cubin\"\n-    data_name = f\"{name}.json\"\n-    if not fn_cache_manager.has_file(cubin_name) or \\\n-       not fn_cache_manager.has_file(data_name) or \\\n-       not fn_cache_manager.has_file(ptx_name):\n-        cubin, ptx, shared, kernel_name = _compile(fn, signature, device, constants, configs[0], num_warps, num_stages, extern_libs, \"cubin\")\n-        metadata = {\"name\": kernel_name, \"shared\": shared, \"num_warps\": num_warps, \"num_stages\": num_stages}\n-        fn_cache_manager.put(cubin, cubin_name)\n-        fn_cache_manager.put(ptx, ptx_name, binary=False)\n-        fn_cache_manager.put(json.dumps(metadata), data_name, binary=False)\n+    # load metadata if any\n+    metadata = None\n+    if fn_cache_manager.has_file(f'{name}.json'):\n+      with open(fn_cache_manager._make_path(f\"{name}.json\")) as f:\n+            metadata = json.load(f)\n+    context = _triton.ir.context()\n+    force_compile = False\n+    # ast -> triton-ir (or read from cache)\n+    ttir, ttir_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.ttir\", metadata,\n+                           run_if_found = lambda path: _triton.ir.parse_mlir_module(path, context),\n+                           run_if_not_found = lambda: ast_to_ttir(fn, signature, configs[0], constants))\n+    # triton-ir -> triton-gpu-ir (or read from cache)\n+    ttgir, ttgir_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.ttgir\", metadata,\n+                            run_if_found = lambda path: _triton.ir.parse_mlir_module(path, context),\n+                            run_if_not_found = lambda: ttir_to_ttgir(ttir, num_warps, num_stages))\n+    # triton-gpu-ir -> llvm-ir (or read from cache)\n+    llir, llir_md5, force_compile, llvm_cached = read_or_execute(fn_cache_manager, force_compile, f\"{name}.llir\", metadata,\n+                           run_if_found = lambda path: Path(path).read_bytes(),\n+                           run_if_not_found = lambda: ttgir_to_llir(ttgir, extern_libs))\n+    if llvm_cached:\n+        shmem_size = metadata[\"shared\"]\n+    else:\n+        shmem_size = _triton.get_shared_memory_size(ttgir)\n+    # llvm-ir -> ptx (or read from cache)\n+    ptx, ptx_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.ptx\", metadata,\n+                          run_if_found = lambda path: Path(path).read_text(),\n+                          run_if_not_found = lambda: llir_to_ptx(llir))\n+    # ptx -> cubin (or read from cache)\n+    cubin, cubin_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.cubin\", metadata,\n+                            run_if_found = lambda path: Path(path).read_bytes(),      \n+                            run_if_not_found= lambda: ptx_to_cubin(ptx, device))\n+    # dump new metadata\n+    kernel_name = ptx_get_kernel_name(ptx)\n+    metadata = {\"name\": kernel_name, \"shared\": shmem_size, \"num_warps\": num_warps, \"num_stages\": num_stages,\n+                \"md5\": {  \"cubin\": cubin_md5,  \"ptx\": ptx_md5,  \n+                          \"llir\": llir_md5,\n+                          \"ttir\": ttir_md5, \"ttgir\": ttgir_md5 }}\n+    fn_cache_manager.put(json.dumps(metadata), f\"{name}.json\", binary=False)\n \n-    return CompiledKernel(name, so_cache_manager._make_path(so_name), fn_cache_manager.cache_dir)\n+    asm = {\"ttir\": ttir, \"ttgir\": ttgir, \"llir\": llir, \"ptx\": ptx, \"cubin\": cubin}\n+    return CompiledKernel(so_path, metadata, asm)\n \n \n class CompiledKernel:\n \n-    def __init__(self, fn_name, so_path, cache_dir):\n-\n+    def __init__(self, so_path, metadata, asm):\n         # initialize launcher\n         import importlib.util\n         spec = importlib.util.spec_from_file_location(\"launcher\", so_path)\n         mod = importlib.util.module_from_spec(spec)\n         spec.loader.exec_module(mod)\n         self.c_wrapper = getattr(mod, \"launch\")\n         # initialize metadata\n-        with open(os.path.join(cache_dir, f\"{fn_name}.json\")) as f:\n-            metadata = json.load(f)\n         self.shared = metadata[\"shared\"]\n         self.num_warps = metadata[\"num_warps\"]\n         self.num_stages = metadata[\"num_stages\"]\n         # initialize asm dict\n-        self.asm = dict()\n-        with open(os.path.join(cache_dir, f\"{fn_name}.cubin\"), \"rb\") as f:\n-            self.asm[\"cubin\"] = f.read()\n-        with open(os.path.join(cache_dir, f\"{fn_name}.ptx\"), \"r\") as f:\n-            self.asm[\"ptx\"] = f.read()\n-\n+        self.asm = asm\n         device = torch.cuda.current_device()\n         global cuda_utils\n         if cuda_utils is None:\n@@ -1386,6 +1395,19 @@ def runner(*args, stream=None):\n             self.c_wrapper(grid[0], grid[1], grid[2], self.num_warps, self.shared, stream, self.cu_function, *args)\n         return\n \n+    def get_sass(self, fun=None):\n+        if 'sass' in self.asm:\n+            return self.asm['sass']\n+        fd, path = tempfile.mkstemp()\n+        try:\n+            with open(fd, 'wb') as cubin:\n+                cubin.write(self.asm['cubin'])\n+            self.sass = extract(path, fun)\n+        finally:\n+            os.remove(path)\n+        self.asm['sass'] = self.sass\n+        return self.sass\n+\n \n class CudaUtils(object):\n "}, {"filename": "python/triton/language/extern.py", "status": "modified", "additions": 28, "deletions": 22, "changes": 50, "file_content_changes": "@@ -56,28 +56,34 @@ def elementwise(lib_name: str, lib_path: str, args: list, arg_type_symbol_dict:\n         :return: the return value of the function\n     '''\n     dispatch_args = args.copy()\n-    if len(args) == 1:\n-        dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n-        ret_shape = dispatch_args[0].shape\n-    elif len(args) == 2:\n-        dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n-        dispatch_args[1] = core._to_tensor(dispatch_args[1], _builder)\n-        dispatch_args[0], dispatch_args[1] = semantic.binary_op_type_checking_impl(\n-            dispatch_args[0], dispatch_args[1], _builder)\n-        ret_shape = dispatch_args[0].shape\n-    else:\n-        for i in range(len(dispatch_args)):\n-            dispatch_args[i] = core._to_tensor(dispatch_args[i], _builder)\n-        broadcast_arg = dispatch_args[0]\n-        # Get the broadcast shape over all the arguments\n-        for i in range(len(dispatch_args)):\n-            _, broadcast_arg = semantic.binary_op_type_checking_impl(\n-                dispatch_args[i], broadcast_arg, _builder)\n-        # Change the shape of each argument based on the broadcast shape\n-        for i in range(len(dispatch_args)):\n-            dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n-                dispatch_args[i], broadcast_arg, _builder)\n-        ret_shape = broadcast_arg.shape\n+    all_scalar = True\n+    ret_shape = None\n+    for dispatch_arg in dispatch_args:\n+        if dispatch_arg.type.is_block():\n+            all_scalar = False\n+    if not all_scalar:\n+        if len(args) == 1:\n+            dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n+            ret_shape = dispatch_args[0].shape\n+        elif len(args) == 2:\n+            dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n+            dispatch_args[1] = core._to_tensor(dispatch_args[1], _builder)\n+            dispatch_args[0], dispatch_args[1] = semantic.binary_op_type_checking_impl(\n+                dispatch_args[0], dispatch_args[1], _builder)\n+            ret_shape = dispatch_args[0].shape\n+        else:\n+            for i in range(len(dispatch_args)):\n+                dispatch_args[i] = core._to_tensor(dispatch_args[i], _builder)\n+            broadcast_arg = dispatch_args[0]\n+            # Get the broadcast shape over all the arguments\n+            for i in range(len(dispatch_args)):\n+                _, broadcast_arg = semantic.binary_op_type_checking_impl(\n+                    dispatch_args[i], broadcast_arg, _builder)\n+            # Change the shape of each argument based on the broadcast shape\n+            for i in range(len(dispatch_args)):\n+                dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n+                    dispatch_args[i], broadcast_arg, _builder)\n+            ret_shape = broadcast_arg.shape\n     func = getattr(_builder, \"create_external_elementwise\")\n     return dispatch(func, lib_name, lib_path, dispatch_args, arg_type_symbol_dict, ret_shape, _builder)\n "}, {"filename": "python/triton/tools/aot.py", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -38,14 +38,13 @@\n         exit(0)\n \n     # triton-ir -> triton-gpu-ir\n-    module = triton.compiler.make_tritongpu_ir(module, num_warps=4)\n-    module = triton.compiler.optimize_tritongpu_ir(module, num_stages=3)\n+    module = triton.compiler.ttir_to_ttgir(module, num_warps=4, num_stages=3)\n     if args.target == 'triton-gpu-ir':\n         print(module.str())\n         exit(0)\n \n     # triton-gpu-ir -> llvm-ir\n-    module = triton.compiler.make_llvm_ir(module)\n+    module = triton.compiler.ttgir_to_llir(module, extern_libs=None)\n     if args.target == 'llvm-ir':\n         print(module)\n         exit(0)\n@@ -56,6 +55,6 @@\n         raise argparse.ArgumentError(None, \"Must specify --ptx-version for PTX compilation\")\n \n     # llvm-ir -> ptx\n-    module = triton.compiler.make_ptx(module, compute_capability=args.sm, ptx_version=args.ptx_version)\n+    module = triton.compiler.llir_to_ptx(module, compute_capability=args.sm, ptx_version=args.ptx_version)\n     assert args.target == 'ptx'\n     print(module)"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -156,7 +156,7 @@\n \n @triton.autotune(\n     configs=[\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n     ],\n     key=['M', 'N', 'K'],\n )"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -18,10 +18,10 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   %a_off = arith.constant dense<4> : tensor<128x32xi32, #AL>\n   %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n   scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n-    %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<128x32xf16, #AL>\n+    %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n     // CHECK: %4 -> %4\n     %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n-    %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<32x128xf16, #BL>\n+    %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n     // CHECK-NEXT: %6 -> %6 \n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n@@ -60,17 +60,17 @@ func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n   %index = arith.constant 0 : i32\n   // CHECK: %2 -> %cst_0\n-  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A>\n+  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A>\n   return\n }\n \n // CHECK-LABEL: extract_slice\n func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n-  %index = arith.constant 0 : i32\n+  %index = arith.constant 0 : index\n   // CHECK-NEXT: %0 -> %cst\n-  %cst1 = triton_gpu.extract_slice %cst0, %index { axis = 0 : i32 } : tensor<1x16x16xf16, #A> -> tensor<16x16xf16, #A>\n+  %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A> to tensor<16x16xf16, #A>\n   return\n }\n \n@@ -144,9 +144,9 @@ func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !t\n   // CHECK-NEXT: %0#2 -> %cst,%cst_0\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n     scf.if %i1 {\n-      %index = arith.constant 8 : i32\n+      %index = arith.constant 8 : index\n       // CHECK-NEXT: %1 -> %cst,%cst_0\n-      %cst0 = triton_gpu.extract_slice %a_shared, %index { axis = 0 : i32 } : tensor<128x32xf16, #A> -> tensor<32xf16, #A>\n+      %cst0 = tensor.extract_slice %a_shared[%index, 0][1, 32][1, 1] : tensor<128x32xf16, #A> to tensor<32xf16, #A>\n       scf.yield\n     }\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -178,7 +178,7 @@ func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: offset = 0, size = 512\n   %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n   %index = arith.constant 0 : i32\n-  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A>\n+  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A>\n   return\n   // CHECK-NEXT: size = 512\n }\n@@ -187,8 +187,8 @@ func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n-  %index = arith.constant 0 : i32\n-  %cst1 = triton_gpu.extract_slice %cst0, %index { axis = 0 : i32 } : tensor<1x16x16xf16, #A> -> tensor<16x16xf16, #A>\n+  %index = arith.constant 0 : index\n+  %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1,1,1] : tensor<1x16x16xf16, #A> to tensor<16x16xf16, #A>\n   return\n   // CHECK-NEXT: size = 512\n }\n@@ -271,8 +271,8 @@ func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n     scf.if %i1 {\n-      %index = arith.constant 8 : i32\n-      %cst0 = triton_gpu.extract_slice %a_shared, %index { axis = 0 : i32 } : tensor<128x32xf16, #A> -> tensor<32xf16, #A>\n+      %index = arith.constant 8 : index\n+      %cst0 = tensor.extract_slice %a_shared[%index, 0][1, 32][1, 1] : tensor<128x32xf16, #A> to tensor<32xf16, #A>\n       scf.yield\n     }\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -22,9 +22,9 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n \n   scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n-    %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<128x32xf16, #AL>\n+    %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n     %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n-    %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<32x128xf16, #BL>\n+    %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n     // CHECK: Membar 13\n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n@@ -41,7 +41,7 @@ func @raw_single_block(%A : !tt.ptr<f16>) {\n   %cst1 = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n-  %a1_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<128x32xf16, #AL>\n+  %a1_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n   %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n   // CHECK: Membar 5\n   %a2 = triton_gpu.convert_layout %a1 : (tensor<128x32xf16, #A>) -> tensor<128x32xf16, #A>\n@@ -53,7 +53,7 @@ func @war_single_block(%A : !tt.ptr<f16>) {\n   %cst1 = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n-  %a1_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<128x32xf16, #AL>\n+  %a1_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n   %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n   // CHECK: Membar 5\n   %a2 = triton_gpu.convert_layout %a1 : (tensor<128x32xf16, #A>) -> tensor<128x32xf16, #AL>\n@@ -98,8 +98,8 @@ func @alloc() {\n // CHECK-LABEL: extract_slice\n func @extract_slice() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n-  %index = arith.constant 0 : i32\n-  %cst1 = triton_gpu.extract_slice %cst0, %index { axis = 0 : i32 } : tensor<1x16x16xf16, #A> -> tensor<16x16xf16, #A>\n+  %index = arith.constant 0 : index\n+  %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A> to tensor<16x16xf16, #A>\n   // CHECK: Membar 3\n   %cst2 = triton_gpu.convert_layout %cst1 : (tensor<16x16xf16, #A>) -> tensor<16x16xf16, #AL>\n   // CHECK-NEXT: Membar 5\n@@ -114,7 +114,7 @@ func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   %tensor = triton_gpu.alloc_tensor : tensor<1x16x16xf16, #A>\n   %index = arith.constant 0 : i32\n-  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A>\n+  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A>\n   %b = tt.cat %a, %a {axis = 0} : (tensor<1x16x16xf16, #A>, tensor<1x16x16xf16, #A>) -> tensor<2x16x16xf16, #A>\n   // CHECK: Membar 7\n   %c = tt.cat %b, %b {axis = 0} : (tensor<2x16x16xf16, #A>, tensor<2x16x16xf16, #A>) -> tensor<4x16x16xf16, #A>"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 56, "deletions": 15, "changes": 71, "file_content_changes": "@@ -346,18 +346,24 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem\n   // CHECK-LABEL: basic_extract_slice\n   func @basic_extract_slice() {\n-    // CHECK: %[[BASE0:.*]] = llvm.mlir.addressof @global_smem\n-    // CHECK-NEXT: %[[BASE1:.*]] = llvm.bitcast %[[BASE0]]\n-    // CHECK-NEXT: %[[OFFSET0:.*]] = llvm.mlir.constant\n-    // CHECK-NEXT: %[[OFFSET1:.*]] = llvm.mlir.constant\n-    // CHECK-NEXT: llvm.getelementptr %[[BASE1]][%[[OFFSET1]]]\n-    // CHECK-NEXT: %[[BASE2:.*]] = llvm.bitcast\n-    // CHECK-NEXT: %[[OFFSET2:.*]] = llvm.mlir.constant\n-    // CHECK-NEXT: %[[OFFSET3:.*]] = llvm.mul %[[OFFSET0]], %[[OFFSET2]]\n-    // CHECK-NEXT: llvm.getelementptr %[[BASE2]][%[[OFFSET3]]]\n-    %index = arith.constant 1 : i32\n+    // CHECK: llvm.mlir.addressof @global_smem\n+    // CHECK: llvm.extractvalue\n+    // CHECK-NEXT: llvm.extractvalue\n+    // CHECK-NEXT: llvm.extractvalue\n+    // CHECK-NEXT: llvm.extractvalue\n+    // CHECK-NEXT: llvm.mlir.constant(0 : i32) : i32\n+    // CHECK-NEXT: llvm.mlir.constant(0 : i32) : i32\n+    // CHECK-NEXT: llvm.mlir.constant(0 : i32) : i32\n+    // CHECK-NEXT: llvm.mul\n+    // CHECK-NEXT: llvm.add\n+    // CHECK-NEXT: llvm.mul\n+    // CHECK-NEXT: llvm.add\n+    // CHECK-NEXT: llvm.mul\n+    // CHECK-NEXT: llvm.add\n+    // CHECK-NEXT: llvm.getelementptr\n+    %index = arith.constant 1 : index\n     %0 = triton_gpu.alloc_tensor : tensor<128x16x32xf32, #shared0>\n-    %1 = triton_gpu.extract_slice %0, %index {axis = 0: i32} : tensor<128x16x32xf32, #shared0> -> tensor<16x32xf32, #shared0>\n+    %1 = tensor.extract_slice %0[%index, 0, 0][1, 16, 32][1, 1, 1] : tensor<128x16x32xf32, #shared0> to tensor<16x32xf32, #shared0>\n     return\n   }\n }\n@@ -488,22 +494,38 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %tensor = triton_gpu.alloc_tensor : tensor<2x32x32xf32, #A>\n     %index = arith.constant 1 : i32\n \n+    // CHECK: llvm.mlir.constant(0 : i32) : i32\n+    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n+    // CHECK: llvm.mlir.constant(0 : i32) : i32\n+    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n+    // CHECK: llvm.mlir.constant(0 : i32) : i32\n+    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n+    // CHECK: llvm.mlir.constant(0 : i32) : i32\n+    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n+    // CHECK: llvm.mlir.constant(16 : i32) : i32\n+    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 2048 ], [ ${{.*}} + 0 ], 0x4, 0x4\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n+    // CHECK: llvm.mlir.constant(16 : i32) : i32\n+    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 2048 ], [ ${{.*}} + 0 ], 0x4, 0x4\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n+    // CHECK: llvm.mlir.constant(16 : i32) : i32\n+    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 2048 ], [ ${{.*}} + 0 ], 0x4, 0x4\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n+    // CHECK: llvm.mlir.constant(16 : i32) : i32\n+    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 2048 ], [ ${{.*}} + 0 ], 0x4, 0x4\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32x!tt.ptr<f32>, #AL> -> tensor<2x32x32xf32, #A>\n@@ -789,3 +811,22 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     return\n   }\n }\n+\n+// -----\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n+#mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  func @matmul_fmadot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #blocked>\n+    // CHECK: llvm.intr.fmuladd\n+    %28 = tt.dot %a, %b, %cst {allowTF32 = false, transA = false, transB = false} : tensor<32x16xf32, #shared> * tensor<16x32xf32, #shared> -> tensor<32x32xf32, #blocked>\n+    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n+    %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n+    tt.store %36, %28 : tensor<32x32xf32, #blocked>\n+    return\n+  }\n+}"}, {"filename": "test/Target/tritongpu_to_llvmir.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: python3 -m triton.tools.aot %s --target=llvm-ir | FileCheck %s\n+// RUN: %PYTHON -m triton.tools.aot %s --target=llvm-ir | FileCheck %s\n \n // == LLVM IR check begin ==\n // CHECK-LABEL: ; ModuleID = 'LLVMDialectModule'"}, {"filename": "test/Target/tritongpu_to_ptx.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: python3 -m triton.tools.aot %s --target=ptx --sm=80 --ptx-version=63 | FileCheck %s\n+// RUN: %PYTHON -m triton.tools.aot %s --target=ptx --sm=80 --ptx-version=63 | FileCheck %s\n // CHECK-LABEL: // Generated by LLVM NVPTX Back-End\n // CHECK: .version 6.3\n // CHECK: .target sm_80"}, {"filename": "test/TritonGPU/coalesce.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -9,8 +9,8 @@\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n \n-// CHECK: [[row_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [1, 0]}>\n-// CHECK: [[col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [0, 1]}>\n+// CHECK: [[row_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>\n+// CHECK: [[col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [1, 4], order = [0, 1]}>\n // CHECK: [[load_ptr:%.*]] = triton_gpu.convert_layout {{.*}} -> tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n // CHECK: [[load_mask:%.*]] = triton_gpu.convert_layout {{.*}} -> tensor<64x64xi1, [[row_layout]]>\n // CHECK: [[load_other:%.*]] = triton_gpu.convert_layout {{.*}} -> tensor<64x64xf32, [[row_layout]]>"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -62,7 +62,7 @@ func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n // CHECK-LABEL: transpose\n func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<64x64xf32, [[row_layout]]>\n+  // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, [[row_layout]]>\n   // CHECK: [[cvt_val:%.*]] = triton_gpu.convert_layout [[loaded_val]] : (tensor<64x64xf32, [[row_layout]]>) -> tensor<64x64xf32, [[col_layout]]>\n   // CHECK: tt.store {{.*}}, [[cvt_val]], %cst_1 : tensor<64x64xf32, [[col_layout]]>\n   // CHECK: return\n@@ -91,7 +91,7 @@ func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt\n   %19 = triton_gpu.convert_layout %10 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n   %20 = triton_gpu.convert_layout %cst_0 : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n   %21 = triton_gpu.convert_layout %cst : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n-  %22 = tt.load %19, %20, %21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false, isOtherUnspecified = false} : tensor<64x64xf32, #blocked3>\n+  %22 = tt.load %19, %20, %21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked3>\n   %23 = triton_gpu.convert_layout %22 : (tensor<64x64xf32, #blocked3>) -> tensor<64x64xf32, #blocked1>\n   %24 = triton_gpu.convert_layout %18 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked4>\n   %25 = triton_gpu.convert_layout %23 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked4>\n@@ -133,7 +133,7 @@ func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %ar\n       %23 = triton_gpu.convert_layout %arg7 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n       %24 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n       %25 = triton_gpu.convert_layout %cst_1 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n-      %26 = tt.load %23, %24, %25 {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<64x64xf32, #blocked3>\n+      %26 = tt.load %23, %24, %25 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked3>\n       %27 = triton_gpu.convert_layout %26 : (tensor<64x64xf32, #blocked3>) -> tensor<64x64xf32, #blocked1>\n       %28 = arith.addf %arg6, %27 : tensor<64x64xf32, #blocked1>\n       %29 = tt.addptr %arg7, %cst_0 : tensor<64x64x!tt.ptr<f32>, #blocked1>"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 28, "deletions": 18, "changes": 46, "file_content_changes": "@@ -13,24 +13,32 @@\n // CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n // CHECK-DAG: %[[CONSTANT_2:.*]] = arith.constant 2 : i32\n // CHECK-DAG: %[[CONSTANT_3:.*]] = arith.constant 3 : i32\n+// CHECK-DAG: %[[LOOP_COND_0:.*]] = arith.cmpi slt, %[[LB:.*]], %[[UB:.*]]\n // CHECK: %[[ABUFFER:.*]] = triton_gpu.alloc_tensor\n-// CHECK: %[[A0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]]\n+// CHECK-DAG: %[[LOOP_COND_0_SPLAT_A:.*]] = tt.splat %[[LOOP_COND_0]]\n+// CHECK: %[[A0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]], %[[LOOP_COND_0_SPLAT_A]]\n // CHECK: %[[BBUFFER:.*]] = triton_gpu.alloc_tensor\n-// CHECK: %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]]\n-// CHECK: %[[A1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n-// CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n+// CHECK-DAG: %[[LOOP_COND_0_SPLAT_B:.*]] = tt.splat %[[LOOP_COND_0]]\n+// CHECK: %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]], %[[LOOP_COND_0_SPLAT_B]]\n+// CHECK-DAG: %[[IV_1:.*]] = arith.addi %[[LB]], %[[STEP:.*]]\n+// CHECK-DAG: %[[LOOP_COND_1:.*]] = arith.cmpi slt, %[[IV_1]], %[[UB]]\n+// CHECK-DAG: %[[LOOP_COND_1_SPLAT_A:.*]] = tt.splat %[[LOOP_COND_1]]\n+// CHECK: %[[A1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]], %[[LOOP_COND_1_SPLAT_A]]\n+// CHECK-DAG: %[[LOOP_COND_1_SPLAT_B:.*]] = tt.splat %[[LOOP_COND_1]]\n+// CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]], %[[LOOP_COND_1_SPLAT_B]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n-// CHECK: %[[A0:.*]] = triton_gpu.extract_slice %[[A1BUFFER]], %[[CONSTANT_0]]\n-// CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]], %[[CONSTANT_0]]\n+// CHECK: %[[A0:.*]] = tensor.extract_slice %[[A1BUFFER]][0, 0, 0]\n+// CHECK: %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:   tt.dot %[[arg_a0]], %[[arg_b0]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_INT:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.index_cast %[[EXTRACT_INT]] : i32 to index\n // CHECK:   %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n-// CHECK:   %[[NEXT_A:.*]] = triton_gpu.extract_slice %[[NEXT_A_BUFFER]], %[[EXTRACT_IDX]]\n-// CHECK:   %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]], %[[EXTRACT_IDX]]\n+// CHECK:   %[[NEXT_A:.*]] = tensor.extract_slice %[[NEXT_A_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n+// CHECK:   %[[NEXT_B:.*]] = tensor.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:   scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n@@ -48,7 +56,7 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n \n   scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n-    %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n+    %a_ = tt.load %a_ptr {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n     %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n@@ -76,17 +84,18 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n // CHECK:   %[[A1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n // CHECK:   %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n-// CHECK:   %[[A0:.*]] = triton_gpu.extract_slice %[[A1BUFFER]], %[[CONSTANT_0]]\n-// CHECK:   %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]], %[[CONSTANT_0]]\n+// CHECK:   %[[A0:.*]] = tensor.extract_slice %[[A1BUFFER]][0, 0, 0]\n+// CHECK:   %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK:   scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:     tt.dot %[[arg_a0]], %[[arg_b0]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_INT:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.index_cast %[[EXTRACT_INT]] : i32 to index\n // CHECK:     %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:     %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:     triton_gpu.async_wait {num = 2 : i32}\n-// CHECK:     %[[NEXT_A:.*]] = triton_gpu.extract_slice %[[NEXT_A_BUFFER]], %[[EXTRACT_IDX]]\n-// CHECK:     %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]], %[[EXTRACT_IDX]]\n+// CHECK:     %[[NEXT_A:.*]] = tensor.extract_slice %[[NEXT_A_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n+// CHECK:     %[[NEXT_B:.*]] = tensor.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n@@ -130,14 +139,15 @@ func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f\n // CHECK: %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]]\n // CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n // CHECK: triton_gpu.async_wait {num = 1 : i32}\n-// CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]], %[[CONSTANT_0]]\n+// CHECK: %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:   tt.dot {{.*}}, %[[arg_b0]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_INT:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.index_cast %[[EXTRACT_INT]] : i32 to index\n // CHECK:   %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   triton_gpu.async_wait {num = 1 : i32}\n-// CHECK:   %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]]\n+// CHECK:   %[[NEXT_B:.*]] = tensor.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:   scf.yield {{.*}}, {{.*}}, %[[NEXT_B_BUFFER]], %[[NEXT_B]], {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/PtxAsmFormatTest.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -112,8 +112,8 @@ TEST_F(PtxAsmFormatTest, MultiLinePTX) {\n   mov(valVal1, constVal);\n   mov(valVal1, valVal0);\n \n-  EXPECT_EQ(builder.dump(), \"mov $0, 0x1;\\r\\n\"\n-                            \"mov $1, 0x1;\\r\\n\"\n+  EXPECT_EQ(builder.dump(), \"mov $0, 0x1;\\n\\t\"\n+                            \"mov $1, 0x1;\\n\\t\"\n                             \"mov $1, $0;\");\n \n   auto values = builder.getAllMLIRArgs();"}]