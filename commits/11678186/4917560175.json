[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -25,9 +25,9 @@ jobs:\n         id: set-matrix\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n-            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], [\"self-hosted\", \"gfx908\"], \"macos-10.15\"]'\n+            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], [\"self-hosted\", \"gfx908\"]]'\n           else\n-            echo '::set-output name=matrix::[\"ubuntu-latest\", \"macos-10.15\"]'\n+            echo '::set-output name=matrix::[\"ubuntu-latest\"]'\n           fi\n \n   Integration-Tests:"}, {"filename": "docs/python-api/triton.language.rst", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -34,6 +34,7 @@ Shape Manipulation Ops\n     :nosignatures:\n \n     broadcast_to\n+    expand_dims\n     reshape\n     ravel\n "}, {"filename": "include/triton/Analysis/Allocation.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -245,8 +245,6 @@ class ModuleAllocation : public CallGraph<Allocation> {\n   FuncOffsetMapT sharedMemoryValue;\n };\n \n-template <typename T> Interval(T, T) -> Interval<T>;\n-\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_ALLOCATION_H"}, {"filename": "include/triton/Analysis/Membar.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -72,7 +72,7 @@ class MembarAnalysis {\n   /// in the following circumstances:\n   /// - RAW: If a shared memory write is followed by a shared memory read, and\n   /// their addresses are intersected, a barrier is inserted.\n-  /// - WAR: If a shared memory read is followed by a shared memory read, and\n+  /// - WAR: If a shared memory read is followed by a shared memory write, and\n   /// their addresses are intersected, a barrier is inserted.\n   /// The following circumstances do not require a barrier:\n   /// - WAW: not possible because overlapped memory allocation is not allowed."}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -81,9 +81,8 @@ void MembarAnalysis::visitTerminator(Operation *op,\n                                      SmallVector<Block *> &successors) {\n   if (auto branchInterface = dyn_cast<BranchOpInterface>(op)) {\n     Block *parentBlock = branchInterface->getBlock();\n-    for (Block *successor : parentBlock->getSuccessors()) {\n-      successors.push_back(successor);\n-    }\n+    successors.append(std::begin(parentBlock->getSuccessors()),\n+                      std::end(parentBlock->getSuccessors()));\n     return;\n   }\n   // Otherwise, it could be a return op"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 9, "deletions": 4, "changes": 13, "file_content_changes": "@@ -107,14 +107,19 @@ struct ConvertLayoutOpConversion\n       unsigned dim = sliceLayout.getDim();\n       auto parentEncoding = sliceLayout.getParent();\n       auto parentSizePerThread = getSizePerThread(parentEncoding);\n-      unsigned stride = 1;\n-      if (getOrder(parentEncoding)[0] == dim)\n-        stride = parentSizePerThread[dim];\n       auto parentShape = sliceLayout.paddedShape(shape);\n       auto parentTy = RankedTensorType::get(parentShape, type.getElementType(),\n                                             parentEncoding);\n+      auto offsets = emitOffsetForLayout(layout, type);\n+      auto parentOffset = emitOffsetForLayout(parentEncoding, parentTy);\n+      SmallVector<int> idxs;\n+      for (SmallVector<unsigned> off : offsets) {\n+        off.insert(off.begin() + dim, 0);\n+        auto it = std::find(parentOffset.begin(), parentOffset.end(), off);\n+        idxs.push_back(std::distance(parentOffset.begin(), it));\n+      }\n       auto multiDimOffsetParent = getMultiDimOffset(\n-          parentEncoding, loc, rewriter, elemId * stride, parentTy,\n+          parentEncoding, loc, rewriter, idxs[elemId], parentTy,\n           sliceLayout.paddedShape(multiDimCTAInRepId),\n           sliceLayout.paddedShape(shapePerCTA));\n       SmallVector<Value> multiDimOffset(rank);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 11, "deletions": 21, "changes": 32, "file_content_changes": "@@ -298,14 +298,7 @@ struct StoreOpConversion\n       vec = std::min(vec, maskAlign);\n     }\n \n-    // numElements = 1 for scalar\n-    auto tensorTy = valueTy.dyn_cast<RankedTensorType>();\n-    auto numElems = tensorTy ? tensorTy.getNumElements() : 1;\n-    Value mask = int_val(1, 1);\n-    auto tid = tid_val();\n-    mask = and_(mask,\n-                icmp_slt(mul(tid, i32_val(elemsPerThread)), i32_val(numElems)));\n-\n+    Value mask = getMask(valueTy, rewriter, loc);\n     const size_t dtsize =\n         std::max<int>(1, valueElemTy.getIntOrFloatBitWidth() / 8);\n     const size_t valueElemNBits = dtsize * 8;\n@@ -406,13 +399,13 @@ struct AtomicCASOpConversion\n     auto valElements = getTypeConverter()->unpackLLElements(\n         loc, llVal, rewriter, op.getVal().getType());\n \n-    auto TensorTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    auto valueTy = op.getResult().getType();\n+    auto TensorTy = valueTy.dyn_cast<RankedTensorType>();\n     Type valueElemTy =\n         TensorTy ? getTypeConverter()->convertType(TensorTy.getElementType())\n-                 : op.getResult().getType();\n+                 : valueTy;\n     auto valueElemNBits = valueElemTy.getIntOrFloatBitWidth();\n-    auto tid = tid_val();\n-    Value pred = icmp_eq(tid, i32_val(0));\n+    Value mask = getMask(valueTy, rewriter, loc);\n     PTXBuilder ptxBuilderMemfence;\n     auto memfence = ptxBuilderMemfence.create<PTXInstr>(\"membar\")->o(\"gl\");\n     memfence();\n@@ -432,7 +425,7 @@ struct AtomicCASOpConversion\n     auto *valOpr = ptxBuilderAtomicCAS.newOperand(casVal, \"r\");\n     auto &atom = *ptxBuilderAtomicCAS.create<PTXInstr>(\"atom\");\n     atom.global().o(\"cas\").o(\"b32\");\n-    atom(dstOpr, ptrOpr, cmpOpr, valOpr).predicate(pred);\n+    atom(dstOpr, ptrOpr, cmpOpr, valOpr).predicate(mask);\n     auto old = ptxBuilderAtomicCAS.launch(rewriter, loc, valueElemTy);\n     barrier();\n \n@@ -441,7 +434,7 @@ struct AtomicCASOpConversion\n     auto *valOprStore = ptxBuilderStore.newOperand(old, \"r\");\n     auto &st = *ptxBuilderStore.create<PTXInstr>(\"st\");\n     st.shared().o(\"b32\");\n-    st(dstOprStore, valOprStore).predicate(pred);\n+    st(dstOprStore, valOprStore).predicate(mask);\n     ptxBuilderStore.launch(rewriter, loc, ASMReturnTy);\n     ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n     barrier();\n@@ -490,10 +483,11 @@ struct AtomicRMWOpConversion\n       maskElements = getTypeConverter()->unpackLLElements(\n           loc, llMask, rewriter, op.getMask().getType());\n \n-    auto tensorTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    auto valueTy = op.getResult().getType();\n+    auto tensorTy = valueTy.dyn_cast<RankedTensorType>();\n     Type valueElemTy =\n         tensorTy ? getTypeConverter()->convertType(tensorTy.getElementType())\n-                 : op.getResult().getType();\n+                 : valueTy;\n     const size_t valueElemNBits = valueElemTy.getIntOrFloatBitWidth();\n     auto elemsPerThread = getTotalElemsPerThread(val.getType());\n     // vec = 1, numElements = 1 for scalar\n@@ -506,10 +500,7 @@ struct AtomicRMWOpConversion\n       // mask\n       numElems = tensorTy.getNumElements();\n     }\n-    Value mask = int_val(1, 1);\n-    auto tid = tid_val();\n-    mask = and_(mask,\n-                icmp_slt(mul(tid, i32_val(elemsPerThread)), i32_val(numElems)));\n+    Value mask = getMask(valueTy, rewriter, loc);\n \n     auto vecTy = vec_ty(valueElemTy, vec);\n     SmallVector<Value> resultVals(elemsPerThread);\n@@ -589,7 +580,6 @@ struct AtomicRMWOpConversion\n         memfenc();\n         auto ASMReturnTy = void_ty(ctx);\n         ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n-        rmwMask = and_(rmwMask, icmp_eq(tid, i32_val(0)));\n         atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n         auto old = ptxBuilderAtomicRMW.launch(rewriter, loc, valueElemTy);\n         Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 40, "deletions": 0, "changes": 40, "file_content_changes": "@@ -421,6 +421,46 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // -----------------------------------------------------------------------\n   // Utilities\n   // -----------------------------------------------------------------------\n+  Value getMask(Type valueTy, ConversionPatternRewriter &rewriter,\n+                Location loc) const {\n+    auto tensorTy = valueTy.dyn_cast<RankedTensorType>();\n+    Value mask = int_val(1, 1);\n+    auto tid = tid_val();\n+    if (tensorTy) {\n+      auto layout = tensorTy.getEncoding();\n+      auto shape = tensorTy.getShape();\n+      unsigned rank = shape.size();\n+      auto sizePerThread = triton::gpu::getSizePerThread(layout);\n+      auto threadsPerWarp = triton::gpu::getThreadsPerWarp(layout);\n+      auto warpsPerCTA = triton::gpu::getWarpsPerCTA(layout);\n+      auto order = triton::gpu::getOrder(layout);\n+      auto shapePerCTA = triton::gpu::getShapePerCTA(layout, shape);\n+      Value warpSize = i32_val(32);\n+      Value laneId = urem(tid, warpSize);\n+      Value warpId = udiv(tid, warpSize);\n+      SmallVector<Value> multiDimWarpId =\n+          delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+      SmallVector<Value> multiDimThreadId =\n+          delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n+      for (unsigned dim = 0; dim < rank; ++dim) {\n+        // if there is no data replication across threads on this dimension\n+        if (shape[dim] >= shapePerCTA[dim])\n+          continue;\n+        // Otherwise, we need to mask threads that will replicate data on this\n+        // dimension. Calculate the thread index on this dimension for the CTA\n+        Value threadDim =\n+            add(mul(multiDimWarpId[dim], i32_val(threadsPerWarp[dim])),\n+                multiDimThreadId[dim]);\n+        mask = and_(mask, icmp_slt(mul(threadDim, i32_val(sizePerThread[dim])),\n+                                   i32_val(shape[dim])));\n+      }\n+    } else {\n+      // If the tensor is not ranked, then it is a scalar and only thread 0 can\n+      // write\n+      mask = and_(mask, icmp_eq(tid, i32_val(0)));\n+    }\n+    return mask;\n+  }\n \n   // Convert an \\param index to a multi-dim coordinate given \\param shape and\n   // \\param order."}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 18, "deletions": 2, "changes": 20, "file_content_changes": "@@ -72,8 +72,24 @@ struct ReturnOpConversion : public ConvertOpToLLVMPattern<triton::ReturnOp> {\n                                                   op->getAttrs());\n     } else {\n       // A device function\n-      auto newOp =\n-          rewriter.create<LLVM::ReturnOp>(op.getLoc(), adaptor.getOperands());\n+      LLVM::ReturnOp newOp;\n+      if (adaptor.getOperands().size() < 2) {\n+        // Single or no return value.\n+        newOp =\n+            rewriter.create<LLVM::ReturnOp>(op.getLoc(), adaptor.getOperands());\n+      } else {\n+        // Pack the results into a struct.\n+        auto packedResultsTy = this->getTypeConverter()->packFunctionResults(\n+            funcOp.getResultTypes());\n+        Value packedResults =\n+            rewriter.create<LLVM::UndefOp>(op.getLoc(), packedResultsTy);\n+        auto loc = op.getLoc();\n+        for (auto it : llvm::enumerate(adaptor.getOperands())) {\n+          packedResults = insert_val(packedResultsTy, packedResults, it.value(),\n+                                     it.index());\n+        }\n+        newOp = rewriter.create<LLVM::ReturnOp>(op.getLoc(), packedResults);\n+      }\n       newOp->setAttrs(op->getAttrs());\n       rewriter.replaceOp(op, newOp->getResults());\n     }"}, {"filename": "python/setup.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -294,7 +294,6 @@ def build_extension(self, ext):\n             \"numpy\",\n             \"pytest\",\n             \"scipy>=1.7.1\",\n-            \"torch\",\n         ],\n         \"tutorials\": [\n             \"matplotlib\","}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 187, "deletions": 2, "changes": 189, "file_content_changes": "@@ -457,6 +457,86 @@ def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.con\n     assert (y_broadcasted_np == to_numpy(y_broadcasted_tri)).all()\n \n \n+# ----------------\n+# test expand_dims\n+# ----------------\n+def test_expand_dims():\n+    @triton.jit\n+    def expand_dims_kernel(dummy, N: tl.constexpr):\n+        offset1 = tl.arange(0, N)\n+\n+        t = tl.expand_dims(offset1, 0)\n+        tl.static_assert(t.shape == [1, N])\n+\n+        t = tl.expand_dims(offset1, 1)\n+        tl.static_assert(t.shape == [N, 1])\n+\n+        t = tl.expand_dims(offset1, -1)\n+        tl.static_assert(t.shape == [N, 1])\n+\n+        t = tl.expand_dims(offset1, -2)\n+        tl.static_assert(t.shape == [1, N])\n+\n+        t = tl.expand_dims(offset1, (0, -1))\n+        tl.static_assert(t.shape == [1, N, 1])\n+\n+        t = tl.expand_dims(offset1, (0, 1, 3))\n+        tl.static_assert(t.shape == [1, 1, N, 1])\n+\n+        t = tl.expand_dims(offset1, (-4, 2, -1))\n+        tl.static_assert(t.shape == [1, N, 1, 1])\n+\n+        t = tl.expand_dims(offset1, (3, 1, 2))\n+        tl.static_assert(t.shape == [N, 1, 1, 1])\n+\n+    N = 32\n+    dummy_tensor = torch.empty((), device=\"cuda\")\n+    expand_dims_kernel[(1,)](dummy_tensor, N)\n+\n+\n+def test_expand_dims_error_cases():\n+    @triton.jit\n+    def dim_out_of_range1(dummy, N: tl.constexpr):\n+        offset1 = tl.arange(0, N)\n+\n+        t = tl.expand_dims(offset1, -2)\n+        t = tl.expand_dims(offset1, -3)\n+\n+    @triton.jit\n+    def dim_out_of_range2(dummy, N: tl.constexpr):\n+        offset1 = tl.arange(0, N)\n+\n+        t = tl.expand_dims(offset1, 1)\n+        t = tl.expand_dims(offset1, 2)\n+\n+    @triton.jit\n+    def duplicate_dim1(dummy, N: tl.constexpr):\n+        offset1 = tl.arange(0, N)\n+\n+        t = tl.expand_dims(offset1, (0, 0))\n+\n+    @triton.jit\n+    def duplicate_dim2(dummy, N: tl.constexpr):\n+        offset1 = tl.arange(0, N)\n+\n+        t = tl.expand_dims(offset1, (0, -3))\n+\n+    N = 32\n+    dummy_tensor = torch.empty((), device=\"cuda\")\n+\n+    with pytest.raises(triton.CompilationError, match=\"invalid axis -3\"):\n+        dim_out_of_range1[(1,)](dummy_tensor, N)\n+\n+    with pytest.raises(triton.CompilationError, match=\"invalid axis 2\"):\n+        dim_out_of_range2[(1,)](dummy_tensor, N)\n+\n+    with pytest.raises(triton.CompilationError, match=r\"duplicate axes, normalized axes = \\[0, 0\\]\"):\n+        duplicate_dim1[(1,)](dummy_tensor, N)\n+\n+    with pytest.raises(triton.CompilationError, match=r\"duplicate axes, normalized axes = \\[0, 0\\]\"):\n+        duplicate_dim2[(1,)](dummy_tensor, N)\n+\n+\n # ---------------\n # test where\n # ---------------\n@@ -774,7 +854,19 @@ def noinline_dynamic_fn(x, y, Z):\n     tl.store(Z, z)\n \n \n-@pytest.mark.parametrize(\"mode\", [\"simple\", \"call_graph\", \"shared\", \"dynamic\"])\n+@triton.jit(noinline=True)\n+def noinline_call_multi_values_fn(x, y):\n+    return x + 1, y + 2\n+\n+\n+@triton.jit(noinline=True)\n+def noinline_multi_values_fn(x, y, Z):\n+    x, y = noinline_call_multi_values_fn(x, y)\n+    z = x + y\n+    tl.store(Z, z)\n+\n+\n+@pytest.mark.parametrize(\"mode\", [\"simple\", \"call_graph\", \"shared\", \"dynamic\", \"multi_values\"])\n def test_noinline(mode):\n     device = 'cuda'\n \n@@ -795,7 +887,7 @@ def kernel(X, Y, Z):\n     kernel[(1,)](x, y, z, num_warps=1)\n     if mode == \"simple\":\n         assert torch.equal(z, x + y)\n-    elif mode == \"call_graph\" or mode == \"dynamic\":\n+    elif mode == \"call_graph\" or mode == \"dynamic\" or mode == \"multi_values\":\n         assert torch.equal(z, x + 1 + y + 2)\n     elif mode == \"shared\":\n         ref = torch.full((16, 16), 16, device=device, dtype=torch.float32)\n@@ -1408,6 +1500,99 @@ def test_reduce_layouts(M, N, src_layout, axis, device='cuda'):\n     np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n \n \n+layouts = [\n+    BlockedLayout([1, 4], [1, 32], [4, 1], [1, 0]),\n+    BlockedLayout([1, 4], [1, 32], [2, 2], [1, 0]),\n+    MmaLayout(version=(2, 0), warps_per_cta=[4, 1])\n+]\n+\n+\n+@pytest.mark.parametrize(\"M\", [32, 64, 128, 256])\n+@pytest.mark.parametrize(\"src_layout\", layouts)\n+def test_store_op(M, src_layout, device='cuda'):\n+    ir = f\"\"\"\n+    #src = {src_layout}\n+    module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n+        tt.func public @kernel(%arg0: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}) {{\n+            %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+            %1 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<{M}x!tt.ptr<f32>, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+            %2 = tt.addptr %1, %0 : tensor<{M}x!tt.ptr<f32>, #triton_gpu.slice<{{dim = 1, parent = #src}}>>, tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+            %3 = tt.load %2 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}xf32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+            %4 = tt.expand_dims %3 {{axis = 1 : i32}} : (tensor<{M}xf32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xf32, #src>\n+            %5 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+            %6 = tt.expand_dims %5 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n+            %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<{M}x1x!tt.ptr<f32>, #src>\n+            %8 = tt.addptr %7, %6 : tensor<{M}x1x!tt.ptr<f32>, #src>, tensor<{M}x1xi32, #src>\n+            tt.store %8, %4 : tensor<{M}x1xf32, #src>\n+            tt.return\n+        }}\n+    }}\n+    \"\"\"\n+\n+    import tempfile\n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n+        f.write(ir)\n+        f.flush()\n+        store_kernel = triton.compile(f.name)\n+\n+    rs = RandomState(17)\n+    x = rs.randint(0, 4, (M, 1)).astype('float32')\n+    y = np.zeros((M, 1), dtype='float32')\n+    x_tri = torch.tensor(x, device=device)\n+    y_tri = torch.tensor(y, device=device)\n+\n+    pgm = store_kernel[(1, 1, 1)](x_tri, y_tri)\n+    y_ref = x\n+    np.testing.assert_allclose(y_ref, y_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+\n+\n+layouts = [\n+    BlockedLayout([1, 4], [1, 32], [4, 1], [1, 0]),\n+    BlockedLayout([1, 4], [1, 32], [2, 2], [1, 0]),\n+    MmaLayout(version=(2, 0), warps_per_cta=[4, 1])\n+]\n+\n+\n+@pytest.mark.parametrize(\"M\", [64, 128, 256])\n+@pytest.mark.parametrize(\"src_layout\", layouts)\n+@pytest.mark.parametrize(\"dst_layout\", layouts)\n+@pytest.mark.parametrize(\"src_dim\", [0, 1])\n+@pytest.mark.parametrize(\"dst_dim\", [0, 1])\n+def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device='cuda'):\n+    ir = f\"\"\"\n+    #dst = {dst_layout}\n+    #src = {src_layout}\n+    module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n+        tt.func public @kernel(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n+            %0 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %1 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %2 = tt.addptr %0, %1 : tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>, tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %3 = tt.load %2 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %4 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %5 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %6 = tt.addptr %4, %5 : tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>, tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %7 = triton_gpu.convert_layout %3 : (tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>) -> tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            tt.store %6, %7 : tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            tt.return\n+        }}\n+    }}\n+    \"\"\"\n+    import tempfile\n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n+        f.write(ir)\n+        f.flush()\n+        kernel = triton.compile(f.name)\n+\n+    rs = RandomState(17)\n+    x = rs.randint(0, 4, (M, )).astype('int32')\n+    y = np.zeros((M, ), dtype='int32')\n+    x_tri = torch.tensor(x, device=device)\n+    y_tri = torch.tensor(y, device=device)\n+    pgm = kernel[(1, 1, 1)](x_tri, y_tri)\n+    y_ref = x\n+    np.testing.assert_allclose(y_ref, y_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+\n+\n @triton.jit\n def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n     delta = mean_2 - mean_1"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -39,6 +39,7 @@\n     dot,\n     dtype,\n     exp,\n+    expand_dims,\n     full,\n     fdiv,\n     float16,\n@@ -130,6 +131,7 @@\n     \"dot\",\n     \"dtype\",\n     \"exp\",\n+    \"expand_dims\",\n     \"extra\",\n     \"fdiv\",\n     \"float16\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 39, "deletions": 4, "changes": 43, "file_content_changes": "@@ -3,7 +3,7 @@\n from contextlib import contextmanager\n from enum import Enum\n from functools import wraps\n-from typing import Callable, List, TypeVar\n+from typing import Callable, List, Sequence, TypeVar\n \n import triton\n from . import semantic\n@@ -883,6 +883,41 @@ def reshape(input, shape, _builder=None):\n     shape = _shape_check_impl(shape)\n     return semantic.reshape(input, shape, _builder)\n \n+\n+def _wrap_axis(axis, ndim):\n+    if not (-ndim <= axis < ndim):\n+        raise ValueError(f\"invalid axis {axis}. Expected {-ndim} <= axis < {ndim}\")\n+\n+    return axis if axis >= 0 else axis + ndim\n+\n+\n+@builtin\n+def expand_dims(input, axis, _builder=None):\n+    \"\"\"\n+    Expand the shape of a tensor, by inserting new length-1 dimensions.\n+\n+    Axis indices are with respect to the resulting tensor, so\n+    ``result.shape[axis]`` will be 1 for each axis.\n+\n+    :param input: The input tensor.\n+    :type input: tl.tensor\n+    :param axis: The indices to add new axes\n+    :type axis: int | Sequence[int]\n+\n+    \"\"\"\n+    axis = _constexpr_to_value(axis)\n+    axes = list(axis) if isinstance(axis, Sequence) else [axis]\n+    new_ndim = len(input.shape) + len(axes)\n+    axes = [_wrap_axis(_constexpr_to_value(d), new_ndim) for d in axes]\n+\n+    if len(set(axes)) != len(axes):\n+        raise ValueError(f\"expand_dims recieved duplicate axes, normalized axes = {axes}\")\n+\n+    ret = input\n+    for a in sorted(axes):\n+        ret = semantic.expand_dims(ret, a, _builder)\n+    return ret\n+\n # -----------------------\n # Linear Algebra\n # -----------------------\n@@ -1281,9 +1316,9 @@ def _argreduce(input, axis, combine_fn, _builder=None, _generator=None):\n \n     if len(input.shape) > 1:\n         # Broadcast index across the non-reduced axes\n-        expand_dims_index = [constexpr(None)] * len(input.shape)\n-        expand_dims_index[axis] = slice(None)\n-        index = index.__getitem__(expand_dims_index, _builder=_builder)\n+        axes_to_expand = [constexpr(d) for d in range(len(input.shape))]\n+        del axes_to_expand[axis]\n+        index = expand_dims(index, axes_to_expand, _builder=_builder)\n         index = broadcast_to(index, input.shape, _builder=_builder)\n \n     rvalue, rindices = reduce((input, index), axis, combine_fn,"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -142,6 +142,7 @@ def warmup(self, *args, **kwargs):\n class Config:\n     \"\"\"\n     An object that represents a possible kernel configuration for the auto-tuner to try.\n+\n     :ivar meta: a dictionary of meta-parameters to pass to the kernel as keyword arguments.\n     :type meta: dict[Str, Any]\n     :ivar num_warps: the number of warps to use for the kernel when compiled for GPUs. For example, if\n@@ -173,8 +174,10 @@ def __str__(self):\n def autotune(configs, key, prune_configs_by=None, reset_to_zero=None):\n     \"\"\"\n     Decorator for auto-tuning a :code:`triton.jit`'d function.\n+\n     .. highlight:: python\n     .. code-block:: python\n+\n         @triton.autotune(configs=[\n             triton.Config(meta={'BLOCK_SIZE': 128}, num_warps=4),\n             triton.Config(meta={'BLOCK_SIZE': 1024}, num_warps=8),\n@@ -223,8 +226,10 @@ def heuristics(values):\n     \"\"\"\n     Decorator for specifying how the values of certain meta-parameters may be computed.\n     This is useful for cases where auto-tuning is prohibitevely expensive, or just not applicable.\n+\n     .. highlight:: python\n     .. code-block:: python\n+\n         @triton.heuristics(values={'BLOCK_SIZE': lambda args: 2 ** int(math.ceil(math.log2(args[1])))})\n         @triton.jit\n         def kernel(x_ptr, x_size, **META):"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -1009,7 +1009,6 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32\n   tt.func @atomic_add_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xi1, #blocked0>, %arg2 : tensor<256xf32, #blocked0>) {\n-    // CHECK: llvm.icmp \"slt\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$3 atom.global.gpu.add.f32\n     // CHECK: llvm.inline_asm\n@@ -1026,6 +1025,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   tt.func @atomic_add_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : i1, %arg2 : f32) {\n     // CHECK: llvm.icmp \"eq\"\n     // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$3 atom.global.gpu.add.f32\n     %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (!tt.ptr<f32>, f32, i1) -> f32\n     tt.return\n@@ -1038,7 +1038,6 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: store_f32\n   tt.func @store_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xf32, #blocked0>) {\n-    // CHECK: llvm.icmp \"slt\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$2 st.global.b32\n     // CHECK: llvm.inline_asm\n@@ -1053,7 +1052,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: store_f32_scalar\n   tt.func @store_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : f32) {\n-    // CHECK: llvm.icmp \"slt\"\n+    // CHECK: llvm.icmp \"eq\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$2 st.global.b32\n     tt.store %arg0, %arg1 : f32"}]