[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 92, "deletions": 1, "changes": 93, "file_content_changes": "@@ -199,6 +199,89 @@ struct ConvertLayoutOpConversion\n     llvm_unreachable(\"unexpected layout in getMultiDimOffset\");\n   }\n \n+  // shared memory rd/st for blocked or mma layout with data padding\n+  void processReplica1D(Location loc, ConversionPatternRewriter &rewriter,\n+                        bool stNotRd, RankedTensorType type,\n+                        ArrayRef<unsigned> numCTAsEachRep,\n+                        ArrayRef<unsigned> multiDimRepId, unsigned vec,\n+                        ArrayRef<unsigned> paddedRepShape,\n+                        ArrayRef<unsigned> outOrd, SmallVector<Value> &vals,\n+                        Value smemBase) const {\n+    auto accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n+    auto layout = type.getEncoding();\n+    auto rank = type.getRank();\n+    auto sizePerThread = getSizePerThread(layout);\n+    auto accumSizePerThread = product<unsigned>(sizePerThread);\n+    SmallVector<unsigned> numCTAs(rank);\n+    auto shapePerCTA = getShapePerCTA(layout, type.getShape());\n+    auto order = getOrder(layout);\n+    for (unsigned d = 0; d < rank; ++d) {\n+      numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n+    }\n+    auto elemTy = type.getElementType();\n+    bool isInt1 = elemTy.isInteger(1);\n+    bool isPtr = elemTy.isa<triton::PointerType>();\n+    auto llvmElemTyOrig = getTypeConverter()->convertType(elemTy);\n+    if (isInt1)\n+      elemTy = IntegerType::get(elemTy.getContext(), 8);\n+    else if (isPtr)\n+      elemTy = IntegerType::get(elemTy.getContext(), 64);\n+\n+    auto llvmElemTy = getTypeConverter()->convertType(elemTy);\n+\n+    for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n+      auto multiDimCTAInRepId =\n+          getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep, order);\n+      SmallVector<unsigned> multiDimCTAId(rank);\n+      for (const auto &it : llvm::enumerate(multiDimCTAInRepId)) {\n+        auto d = it.index();\n+        multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n+      }\n+\n+      auto linearCTAId =\n+          getLinearIndex<unsigned>(multiDimCTAId, numCTAs, order);\n+      // TODO: This is actually redundant index calculation, we should\n+      //       consider of caching the index calculation result in case\n+      //       of performance issue observed.\n+      for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n+        SmallVector<Value> multiDimOffset =\n+            getMultiDimOffset(layout, loc, rewriter, elemId, type,\n+                              multiDimCTAInRepId, shapePerCTA);\n+        Value offset =\n+            linearize(rewriter, loc, multiDimOffset, paddedRepShape, outOrd);\n+\n+        auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n+        Value ptr = gep(elemPtrTy, smemBase, offset);\n+        auto vecTy = vec_ty(llvmElemTy, vec);\n+        ptr = bitcast(ptr, ptr_ty(vecTy, 3));\n+        if (stNotRd) {\n+          Value valVec = undef(vecTy);\n+          for (unsigned v = 0; v < vec; ++v) {\n+            auto currVal = vals[elemId + linearCTAId * accumSizePerThread + v];\n+            if (isInt1)\n+              currVal = zext(llvmElemTy, currVal);\n+            else if (isPtr)\n+              currVal = ptrtoint(llvmElemTy, currVal);\n+            valVec = insert_element(vecTy, valVec, currVal, i32_val(v));\n+          }\n+          store(valVec, ptr);\n+        } else {\n+          Value valVec = load(ptr);\n+          for (unsigned v = 0; v < vec; ++v) {\n+            Value currVal = extract_element(llvmElemTy, valVec, i32_val(v));\n+            if (isInt1)\n+              currVal = icmp_ne(currVal,\n+                                rewriter.create<LLVM::ConstantOp>(\n+                                    loc, i8_ty, rewriter.getI8IntegerAttr(0)));\n+            else if (isPtr)\n+              currVal = inttoptr(llvmElemTyOrig, currVal);\n+            vals[elemId + linearCTAId * accumSizePerThread + v] = currVal;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n   // shared memory rd/st for blocked or mma layout with data padding\n   void processReplica(Location loc, ConversionPatternRewriter &rewriter,\n                       bool stNotRd, RankedTensorType type,\n@@ -481,7 +564,7 @@ struct ConvertLayoutOpConversion\n     triton::gpu::SharedEncodingAttr sharedLayout;\n     // If either src or dst is mmav1, we need to use padding to avoid bank\n     // conflict.\n-    if (isSrcMmaV1 || isDstMmaV1) {\n+    if (isSrcMmaV1 || isDstMmaV1 || rank != 2) {\n       repShape = getScratchConfigForCvtLayout(op, inVec, outVec, true);\n       auto maxVec = std::max(inVec, outVec);\n       sharedLayout = triton::gpu::SharedEncodingAttr::get(\n@@ -516,6 +599,10 @@ struct ConvertLayoutOpConversion\n         processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ true, srcTy,\n                                multiDimRepId, inVec, repShape, outOrd, vals,\n                                smemBase, shape);\n+      } else if (rank != 2) {\n+        processReplica1D(loc, rewriter, /*stNotRd*/ true, srcTy,\n+                         inNumCTAsEachRep, multiDimRepId, inVec, repShape,\n+                         outOrd, vals, smemBase);\n       } else {\n         processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,\n                        multiDimRepId, inVec, repShape, outOrd, vals, smemBase,\n@@ -528,6 +615,10 @@ struct ConvertLayoutOpConversion\n                                multiDimRepId, outVec, repShape, outOrd, outVals,\n                                smemBase, shape,\n                                /*isDestMma=*/true);\n+      } else if (rank != 2) {\n+        processReplica1D(loc, rewriter, /*stNotRd*/ false, dstTy,\n+                         outNumCTAsEachRep, multiDimRepId, outVec, repShape,\n+                         outOrd, outVals, smemBase);\n       } else {\n         processReplica(loc, rewriter, /*stNotRd*/ false, dstTy,\n                        outNumCTAsEachRep, multiDimRepId, outVec, repShape,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -295,6 +295,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto inOrder = triton::gpu::getOrder(srcEncoding);\n     auto outOrder = triton::gpu::getOrder(resSharedLayout);\n     auto rank = outOrder.size();\n+    assert(rank == 2);\n     // tensor indices held by the current thread, as LLVM values\n     auto srcIndices = emitIndices(loc, rewriter, srcEncoding, srcTy);\n     // return values"}]