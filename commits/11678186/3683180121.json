[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -96,9 +96,6 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n           bool is_row = order[0] != 0;\n           bool is_vec4 = opIdx == 0 ? !is_row && (shape[order[0]] <= 16) :\n               is_row && (shape[order[0]] <= 16);\n-          // TODO[Superjomn]: Support the case when is_vec4=false later\n-          // Currently, we only support ld.v2, for the mma layout varies with different ld vector width.\n-          is_vec4 = true;\n           int pack_size = opIdx == 0 ? ((is_row || is_vec4) ? 1 : 2) :\n                                        ((is_row && !is_vec4) ? 2 : 1);\n           int rep = 2 * pack_size;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1423,6 +1423,7 @@ Value DotOpMmaV1ConversionHelper::loadA(\n     ld(has, m, k, ha00, ha01);\n \n     if (vecA > 4) {\n+      assert(false); // DEBUG\n       Value ha10 = bitcast(extract_element(ha, i32_val(2)), f16x2Ty);\n       Value ha11 = bitcast(extract_element(ha, i32_val(3)), f16x2Ty);\n       if (isARow)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 29, "deletions": 0, "changes": 29, "file_content_changes": "@@ -59,6 +59,8 @@ using ::mlir::triton::gpu::SliceEncodingAttr;\n namespace mlir {\n namespace LLVM {\n \n+Value gThreadId; // DEBUG\n+\n static StringRef getStructAttrsAttrName() { return \"llvm.struct_attrs\"; }\n \n // A helper function for using printf in LLVM conversion.\n@@ -346,6 +348,7 @@ class ConvertTritonGPUOpToLLVMPattern\n         ValueRange{rewriter.create<::mlir::gpu::ThreadIdOp>(\n             loc, rewriter.getIndexType(), ::mlir::gpu::Dimension::x)});\n     Value threadId = cast.getResult(0);\n+    LLVM::gThreadId = threadId; // DEBUG\n     return threadId;\n   }\n \n@@ -3639,6 +3642,9 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n         {hb.first, \"r\"},\n         {hb.second, \"r\"},\n     });\n+\n+    std::vector<Value> args = {ha.first, ha.second, hb.first, hb.second};\n+\n     auto *COprs = builder.newListOperand();\n     for (int i = 0; i < 8; ++i)\n       COprs->listAppend(builder.newOperand(acc[idx[i]], std::to_string(i)));\n@@ -3661,6 +3667,27 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n       acc[idx[i]] = elem;\n       resVals[(m * numN / 2 + n) * 8 + i] = elem;\n     }\n+\n+#define SHOW_MMA_V1 1\n+#if SHOW_MMA_V1\n+    auto get_f16 = [&](Value value, int idx) {\n+      return extract_element(f16_ty, value, idx_val(idx));\n+    };\n+\n+    std::vector<Value> pargs({LLVM::gThreadId});\n+    for (int i = 0; i < 4; i++) {\n+      pargs.push_back(get_f16(args[i], 0));\n+      pargs.push_back(get_f16(args[i], 1));\n+    }\n+    for (int i = 0; i < 8; i++) {\n+      pargs.push_back(extract_val(f32_ty, res, getIntAttr(i)));\n+    }\n+\n+    LLVM::vprintf(\"mma t-%d A:(%f,%f) (%f,%f) B:(%f,%f) (%f,%f) \"\n+                  \"D:(%f,%f,%f,%f,%f,%f,%f,%f)\",\n+                  pargs, rewriter);\n+\n+#endif\n   };\n \n   for (unsigned k = 0; k < NK; k += 4)\n@@ -3669,6 +3696,8 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n         callMMA(m, n, k);\n       }\n \n+  LLVM::vprintf_array(LLVM::gThreadId, acc, \"acc\", \"%f\", rewriter);\n+\n   Type structTy = LLVM::LLVMStructType::getLiteral(\n       ctx, SmallVector<Type>(resSize, type::f32Ty(ctx)));\n   Value res = getStructFromElements(loc, resVals, rewriter, structTy);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -802,6 +802,7 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto mod = op->getParentOfType<mlir::ModuleOp>();\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n     int version = computeCapabilityToMMAVersion(computeCapability);\n+    version = 1;\n \n     auto newRetType = RankedTensorType::get(\n         retShape, oldRetType.getElementType(),"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -311,4 +311,9 @@ def guard_for_volta(is_int8=False, is_tf32=False):\n \n     if is_on_Volta:\n         if (not is_feature_supported):\n-            pytest.skip(\"Not valid on Volta\")\n\\ No newline at end of file\n+            pytest.skip(\"Not valid on Volta\")\n+\n+\n+#test_gemm(*[32, 128, 64, 1, 32, 128, 64, False, False])\n+#test_gemm(*[32, 32, 32, 1, 32, 32, 32, False, False])\n+test_gemm(*[32, 16, 16, 1, 32, 16, 16, False, False])"}]