[{"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 24, "deletions": 5, "changes": 29, "file_content_changes": "@@ -9,6 +9,8 @@\n // CHECK: [[$col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK: [[$col_layout_novec:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK-LABEL: cst\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n tt.func @cst() -> tensor<1024xi32, #layout1> {\n   %cst = arith.constant dense<0> : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %cst : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n@@ -67,8 +69,6 @@ tt.func @remat_single_value(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   tt.return\n }\n \n-\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @remat_fast_load(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   %0 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<16x!tt.ptr<i32>, #layout1>\n   %1 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #layout1>\n@@ -80,7 +80,6 @@ tt.func @remat_fast_load(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   tt.store %5, %4 : tensor<16xi32, #layout0>\n   tt.return\n }\n-}\n \n // CHECK-LABEL: if\n tt.func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n@@ -164,6 +163,8 @@ tt.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility =\n   tt.return\n }\n \n+}\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n #slice1dim1 = #triton_gpu.slice<{dim = 1, parent = #blocked1}>\n@@ -173,6 +174,7 @@ tt.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility =\n #blocked4 = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n \n // CHECK-LABEL: transpose\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, {{%cst.*}}, {{%cst.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, [[$row_layout]]>\n@@ -212,8 +214,10 @@ tt.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32\n   tt.store %24, %25, %26 : tensor<64x64xf32, #blocked4>\n   tt.return\n }\n+}\n \n // CHECK-LABEL: loop\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   // CHECK: [[loop_ret:%.*]]:2 = scf.for {{.*}} -> (tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>)\n@@ -266,8 +270,10 @@ tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32,\n   tt.store %20, %21, %22 : tensor<64x64xf32, #blocked1>\n   tt.return\n }\n+}\n \n // CHECK-LABEL: loop_if\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @loop_if(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n   %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n   %cst_0 = arith.constant dense<64> : tensor<64x64xi32, #blocked1>\n@@ -318,8 +324,10 @@ tt.func @loop_if(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i3\n   tt.store %20, %21, %22 : tensor<64x64xf32, #blocked1>\n   tt.return\n }\n+}\n \n // CHECK-LABEL: vecadd\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %c256_i32 = arith.constant 256 : i32\n@@ -349,9 +357,11 @@ tt.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr\n   tt.store %21, %22 : tensor<256xf32, #layout1>\n   tt.return\n }\n+}\n \n // Select has args with different element types\n // CHECK-LABEL: select\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @select(%arg0: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %cst = arith.constant dense<30000> : tensor<1x1xi32, #blocked2>\n@@ -400,9 +410,11 @@ tt.func @select(%arg0: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr\n   }\n   tt.return\n }\n+}\n \n // Make sure the following IR doesn't hang the compiler.\n // CHECK-LABEL: long_func\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func public @long_func(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg5: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg7: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg9: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg10: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg11: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg12: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg13: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg14: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg15: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}) {\n   %cst = arith.constant dense<1.000000e+00> : tensor<1024xf32, #blocked0>\n   %cst_0 = arith.constant dense<5.000000e-04> : tensor<1024xf32, #blocked0>\n@@ -796,10 +808,12 @@ tt.func public @long_func(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg\n   tt.store %365, %366 : tensor<1024xf64, #blocked0>\n   tt.return\n }\n+}\n \n // A mnist model from torch inductor.\n // Check if topological sort is working correct and there's no unnecessary convert\n // CHECK-LABEL: mnist\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %cst = arith.constant dense<10> : tensor<16x1xi32, #blocked2>\n@@ -884,17 +898,19 @@ tt.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !\n   tt.store %61, %62, %63 : tensor<16x16xf32, #blocked4>\n   tt.return\n }\n+}\n \n // -----\n \n+// cmpf and cmpi have different operands and result types\n+// CHECK-LABEL: cmp\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [8, 1], order = [0, 1]}>\n #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [2, 4], order = [0, 1]}>\n #blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 8], order = [0, 1]}>\n #blocked4 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [8, 1], order = [1, 0]}>\n #blocked5 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [8, 1], order = [1, 0]}>\n-// cmpf and cmpi have different operands and result types\n-// CHECK-LABEL: cmp\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) {\n   %c64 = arith.constant 64 : index\n   %c2048 = arith.constant 2048 : index\n@@ -1034,11 +1050,13 @@ tt.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt\n   }\n   tt.return\n }\n+}\n \n // -----\n \n // Just make sure it doesn't crash on non-tensor types.\n // CHECK-LABEL: if_no_tensor\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %c-1_i64 = arith.constant -1 : i64\n@@ -1062,6 +1080,7 @@ tt.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %\n   tt.store %9, %8 {cache = 1 : i32, evict = 1 : i32} : f32\n   tt.return\n }\n+}\n \n // -----\n "}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -11,6 +11,8 @@\n #BLR = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #BLC = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [0, 1]}>\n \n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n // CHECK: tt.func @push_elementwise1\n // CHECK: %[[ALOAD:.*]] = tt.load %arg0\n // CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]]\n@@ -122,3 +124,5 @@ tt.func @push_elementwise5(\n   %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av1> * tensor<16x16xf16, #Bv1> -> tensor<16x16xf32, #Cv1>\n   tt.return %newc : tensor<16x16xf32, #Cv1>\n }\n+\n+}"}]