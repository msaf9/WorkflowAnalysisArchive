[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 25, "deletions": 40, "changes": 65, "file_content_changes": "@@ -385,6 +385,8 @@ def test_index1d(expr, dtype_str, device='cuda'):\n     rank_y = expr.count(',') + 1\n     shape_x = [32 for _ in range(rank_x)]\n     shape_z = [32 for _ in range(rank_y)]\n+    shape_z_rank_mismatch = [32 for _ in range(rank_y)]\n+    shape_z_dim_mismatch = [64 for _ in range(rank_y)]\n \n     # Triton kernel\n     @triton.jit\n@@ -395,12 +397,17 @@ def kernel(Z, X, SIZE: tl.constexpr):\n         z = GENERATE_TEST_HERE\n         tl.store(Z_PTR_EXPR, z)\n \n-    to_replace = {\n-        'X_PTR_EXPR': make_ptr_str('X', shape_x),\n-        'Z_PTR_EXPR': make_ptr_str('Z', shape_z),\n-        'GENERATE_TEST_HERE': expr,\n-    }\n-    kernel = patch_kernel(kernel, to_replace)\n+    def generate_kernel(shape_x, shape_z):\n+        to_replace = {\n+            'X_PTR_EXPR': make_ptr_str('X', shape_x),\n+            'Z_PTR_EXPR': make_ptr_str('Z', shape_z),\n+            'GENERATE_TEST_HERE': expr,\n+        }\n+        return patch_kernel(kernel, to_replace)\n+\n+    kernel_match = generate_kernel(shape_x, shape_z)\n+    kernel_dim_mismatch = generate_kernel(shape_x, shape_z_dim_mismatch)\n+    kernel_rank_mismatch = generate_kernel(shape_x, shape_z_rank_mismatch)\n \n     # torch result\n     x = numpy_random(shape_x, dtype_str=dtype_str)\n@@ -409,10 +416,21 @@ def kernel(Z, X, SIZE: tl.constexpr):\n     # triton result\n     z_tri = to_triton(np.empty_like(z_ref), device=device)\n     x_tri = to_triton(x)\n-    kernel[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n+    kernel_match[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n     # compare\n     assert (z_ref == to_numpy(z_tri)).all()\n \n+    def catch_compilation_error(kernel):\n+        try:\n+            kernel[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n+        except triton.code_gen.CompilationError as e:\n+            np.testing.assert_(True)\n+        except BaseException:\n+            np.testing.assert_(False)\n+\n+    catch_compilation_error(kernel_dim_mismatch)\n+    catch_compilation_error(kernel_rank_mismatch)\n+\n \n # ---------------\n # test tuples\n@@ -1216,36 +1234,3 @@ def _kernel(dst):\n     _kernel[(1,)](dst=dst, num_warps=1)\n     _kernel[(1,)](dst=dst, num_warps=2)\n     _kernel[(1,)](dst=dst, num_warps=4)\n-\n-\n-@pytest.mark.parametrize(\"dtype_str\", dtypes)\n-def test_broadcast(dtype_str):\n-    SIZE = 128\n-\n-    @triton.jit\n-    def kernel(X, Y, SIZE1: tl.constexpr, SIZE2: tl.constexpr, SIZE: tl.constexpr):\n-        x_offs = tl.arange(0, SIZE1)[:, None] * SIZE2 + tl.arange(0, SIZE2)[None, :]\n-        y_offs = tl.arange(0, SIZE)[:, None] * SIZE + tl.arange(0, SIZE)[None, :]\n-        # broadcast value\n-        x = tl.load(X + x_offs)\n-        # broadcast shape\n-        tl.store(Y + y_offs, x)\n-\n-    # inputs\n-    rs = RandomState(17)\n-    x = numpy_random((SIZE, SIZE), dtype_str=dtype_str, rs=rs)\n-    y = numpy_random((SIZE, SIZE), dtype_str=dtype_str)\n-    x_tri = to_triton(x, device='cuda')\n-    y_tri = to_triton(y, device='cuda')\n-    try:\n-        kernel[(1,)](x_tri, y_tri, SIZE // 2, SIZE * 2, SIZE)\n-    except triton.code_gen.CompilationError as e:\n-        np.testing.assert_(True)\n-    except BaseException:\n-        np.testing.assert_(False)\n-    # compute reference\n-    y[:] = x[0, :]\n-    # compute triton reference\n-    kernel[(1,)](x_tri, y_tri, 1, SIZE, SIZE)\n-    y_tri = to_numpy(y_tri)\n-    np.testing.assert_equal(y, y_tri)"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -485,7 +485,7 @@ def broadcast_impl_shape(input: tl.tensor,\n     for i in range(len(src_shape)):\n         if shape[i] != src_shape[i] and src_shape[i] != 1:\n             raise ValueError(f\"Cannot broadcast, the expanded size of the tensor ({shape[i]})\"\n-                             f\"must match the existing size ({src_shape[1]}) at non-singleton dimension\"\n+                             f\" must match the existing size ({src_shape[1]}) at non-singleton dimension\"\n                              f\" {i}: {src_shape}, {shape}\")\n     ret_ty = tl.block_type(input.type.scalar, shape)\n     return tl.tensor(builder.create_broadcast(input.handle, shape), ret_ty)"}]