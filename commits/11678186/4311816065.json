[{"filename": "include/triton/Conversion/Passes.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -2,6 +2,7 @@\n #define TRITON_CONVERSION_PASSES_H\n \n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n "}, {"filename": "include/triton/Conversion/Passes.td", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -49,4 +49,18 @@ def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"\n     ];\n }\n \n+def TritonConvertArithToIndex : Pass<\"triton-convert-arith-to-index\", \"mlir::ModuleOp\"> {\n+\n+    let summary = \"Convert arith to index\";\n+    \n+    let constructor = \"mlir::triton::createTritonConvertArithToIndexPass()\";\n+\n+    let description = [{\n+      Convert arith operation on index values to corresponding ops in the index dialect.\n+      We need this because SCFToCF conversion currently generates arith ops on indices.\n+    }];\n+\n+    let dependentDialects = [\"mlir::arith::ArithDialect\"];\n+}\n+\n #endif"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h", "status": "added", "additions": 20, "deletions": 0, "changes": 20, "file_content_changes": "@@ -0,0 +1,20 @@\n+#ifndef TRITON_CONVERSION_ARITH_TO_INDEX_H\n+#define TRITON_CONVERSION_ARITH_TO_INDEX_H\n+\n+#include \"mlir/Conversion/LLVMCommon/TypeConverter.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include <memory>\n+\n+namespace mlir {\n+\n+class ModuleOp;\n+template <typename T> class OperationPass;\n+\n+namespace triton {\n+\n+std::unique_ptr<OperationPass<ModuleOp>> createTritonConvertArithToIndexPass();\n+\n+}\n+} // namespace mlir\n+\n+#endif\n\\ No newline at end of file"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 65, "deletions": 0, "changes": 65, "file_content_changes": "@@ -9,6 +9,8 @@ include \"triton/Dialect/Triton/IR/TritonAttrDefs.td\"\n include \"mlir/IR/OpBase.td\"\n include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n+include \"mlir/Interfaces/DestinationStyleOpInterface.td\"\n+include \"mlir/Interfaces/ViewLikeInterface.td\"\n \n def ResultsAreSharedEncoding: NativeOpTrait<\"ResultsAreSharedEncoding\">;\n \n@@ -105,6 +107,69 @@ def TTG_SelectOp : TTG_Op<\"select\", [Pure, Elementwise,\n }\n \n \n+\n+def TTG_ExtractSliceOp : TTG_Op<\"extract_slice\",\n+                                [AttrSizedOperandSegments,\n+                                 ResultsAreSharedEncoding,\n+                                 Pure,\n+                                 OffsetSizeAndStrideOpInterface\n+                                 ]> {\n+  let summary = \"extract slice operation\";\n+  let description = [{\n+    same as tensor.extract_slice, but with int32 index. The motivations for re-implementing it are:\n+    We reimplement ExtractSliceOp with int32 index, because:\n+    - we want to enforce int32 indexing on GPUs since Triton tensors fit in SRAM\n+    - we still want to use indexWidth = 64 when lowering to LLVM because our loops can have\n+      64-bit induction variables and scf.for uses indexType for bounds/ivs\n+  }];\n+\n+  let arguments = (ins\n+    AnyRankedTensor:$source,\n+    Variadic<I32>:$offsets,\n+    Variadic<I32>:$sizes,\n+    Variadic<I32>:$strides,\n+    DenseI64ArrayAttr:$static_offsets,\n+    DenseI64ArrayAttr:$static_sizes,\n+    DenseI64ArrayAttr:$static_strides\n+  );\n+  let results = (outs AnyRankedTensor:$result);\n+\n+  let builders = [\n+    // Build an ExtractSliceOp with mixed static and dynamic entries and custom\n+    // result type. If the type passed is nullptr, it is inferred.\n+    OpBuilder<(ins \"RankedTensorType\":$resultType, \"Value\":$source,\n+      \"ArrayRef<OpFoldResult>\":$offsets, \"ArrayRef<OpFoldResult>\":$sizes,\n+      \"ArrayRef<OpFoldResult>\":$strides,\n+      CArg<\"ArrayRef<NamedAttribute>\", \"{}\">:$attrs)>,\n+  ];\n+\n+  let extraClassDeclaration = [{\n+    /// Return the number of leading operands before the `offsets`, `sizes` and\n+    /// and `strides` operands.\n+    static unsigned getOffsetSizeAndStrideStartOperandIndex() { return 1; }\n+\n+    /// Returns the type of the base tensor operand.\n+    RankedTensorType getSourceType() {\n+      return getSource().getType().cast<RankedTensorType>();\n+    }\n+\n+    std::array<unsigned, 3> getArrayAttrMaxRanks() {\n+      unsigned rank = getSourceType().getRank();\n+      return {rank, rank, rank};\n+    }\n+  }];\n+\n+  let assemblyFormat = [{\n+    $source ``\n+    custom<DynamicIndexList>($offsets, $static_offsets)\n+    custom<DynamicIndexList>($sizes, $static_sizes)\n+    custom<DynamicIndexList>($strides, $static_strides)\n+    attr-dict `:` type($source) `to` type($result)\n+  }];\n+}\n+\n+//\n+\n def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n                                     [AttrSizedOperandSegments,\n                                      ResultsAreSharedEncoding,"}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -27,7 +27,7 @@ void SharedMemoryAliasAnalysis::visitOperation(\n     // These ops may allocate a new shared memory buffer.\n     auto result = op->getResult(0);\n     // XXX(Keren): the following ops are always aliasing for now\n-    if (isa<tensor::ExtractSliceOp, triton::TransOp>(op)) {\n+    if (isa<triton::gpu::ExtractSliceOp, triton::TransOp>(op)) {\n       // extract_slice %src\n       // trans %src\n       aliasInfo = AliasInfo(operands[0]->getValue());"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -78,8 +78,8 @@ void MembarAnalysis::visitTerminator(Operation *op,\n \n void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n                             OpBuilder *builder) {\n-  if (isa<tensor::ExtractSliceOp>(op) || isa<triton::gpu::AllocTensorOp>(op) ||\n-      isa<triton::TransOp>(op)) {\n+  if (isa<triton::gpu::ExtractSliceOp>(op) ||\n+      isa<triton::gpu::AllocTensorOp>(op) || isa<triton::TransOp>(op)) {\n     // alloc is an allocation op without memory write.\n     // FIXME(Keren): extract_slice is always alias for now\n     return;"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -114,7 +114,7 @@ bool maybeSharedAllocationOp(Operation *op) {\n }\n \n bool maybeAliasOp(Operation *op) {\n-  return isa<tensor::ExtractSliceOp>(op) || isa<triton::TransOp>(op) ||\n+  return isa<triton::gpu::ExtractSliceOp>(op) || isa<triton::TransOp>(op) ||\n          isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n          isa<tensor::InsertSliceOp>(op);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ArithToIndexPass.cpp", "status": "added", "additions": 90, "deletions": 0, "changes": 90, "file_content_changes": "@@ -0,0 +1,90 @@\n+#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n+#include \"mlir/Analysis/DataFlowFramework.h\"\n+#include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n+#include \"mlir/Conversion/ControlFlowToLLVM//ControlFlowToLLVM.h\"\n+#include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n+#include \"mlir/Conversion/LLVMCommon/VectorPattern.h\"\n+#include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n+#include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n+#include \"mlir/Dialect/Index/IR/IndexDialect.h\"\n+#include \"mlir/Dialect/Index/IR/IndexOps.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Conversion/Passes.h.inc\"\n+\n+namespace {\n+class TritonArithToIndexConversionTarget : public mlir::ConversionTarget {\n+public:\n+  static bool hasIndexResultOrOperand(Operation *op) {\n+    if (!op)\n+      return false;\n+    bool hasRetIndex = llvm::find_if(op->getResultTypes(), [](Type type) {\n+                         return type.isIndex();\n+                       }) != op->getResultTypes().end();\n+    bool hasArgIndex = llvm::find_if(op->getOperandTypes(), [](Type type) {\n+                         return type.isIndex();\n+                       }) != op->getOperandTypes().end();\n+    return !hasRetIndex && !hasArgIndex;\n+  }\n+\n+  explicit TritonArithToIndexConversionTarget(MLIRContext &ctx)\n+      : ConversionTarget(ctx) {\n+    addLegalDialect<index::IndexDialect>();\n+    addDynamicallyLegalDialect<arith::ArithDialect>(hasIndexResultOrOperand);\n+  }\n+};\n+\n+template <class SrcOp, class DstOp>\n+LogicalResult replaceArithWithIndex(SrcOp op, PatternRewriter &rewriter) {\n+  // if (!hasIndexResultOrOperand(&*op))\n+  //   return failure();\n+  rewriter.replaceOpWithNewOp<DstOp>(op, op->getResultTypes(),\n+                                     op->getOperands(), op->getAttrs());\n+  return success();\n+}\n+\n+LogicalResult replaceArithCmpWithIndexCmp(arith::CmpIOp op,\n+                                          PatternRewriter &rewriter) {\n+  // if (!hasIndexResultOrOperand(&*op))\n+  //   return failure();\n+  rewriter.replaceOpWithNewOp<index::CmpOp>(\n+      op, op.getResult().getType(), (index::IndexCmpPredicate)op.getPredicate(),\n+      op.getOperand(0), op.getOperand(1));\n+  return success();\n+}\n+\n+class ArithToIndex : public TritonConvertArithToIndexBase<ArithToIndex> {\n+public:\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp mod = getOperation();\n+    TritonArithToIndexConversionTarget target(*context);\n+    RewritePatternSet patterns(context);\n+    patterns.add(replaceArithWithIndex<arith::IndexCastOp, index::CastSOp>);\n+    patterns.add(replaceArithWithIndex<arith::ConstantOp, index::ConstantOp>);\n+    patterns.add(replaceArithWithIndex<arith::AddIOp, index::AddOp>);\n+    patterns.add(replaceArithCmpWithIndexCmp);\n+    if (failed(applyPartialConversion(mod, target, std::move(patterns)))) {\n+      return signalPassFailure();\n+    }\n+  }\n+};\n+} // namespace\n+\n+namespace mlir {\n+namespace triton {\n+\n+std::unique_ptr<OperationPass<ModuleOp>> createTritonConvertArithToIndexPass() {\n+  return std::make_unique<::ArithToIndex>();\n+}\n+\n+} // namespace triton\n+} // namespace mlir\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,4 +1,5 @@\n add_mlir_conversion_library(TritonGPUToLLVM\n+    ArithToIndexPass.cpp\n     ConvertLayoutOpToLLVM.cpp\n     DotOpToLLVM.cpp\n     ElementwiseOpToLLVM.cpp"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 17, "deletions": 17, "changes": 34, "file_content_changes": "@@ -68,7 +68,7 @@ struct ConvertLayoutOpConversion\n           elemId, getSizePerThread(layout), getOrder(layout));\n       for (unsigned d = 0; d < rank; ++d) {\n         multiDimOffset[d] = add(multiDimOffsetFirstElem[d],\n-                                idx_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n+                                i32_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n                                         multiDimElemId[d]));\n       }\n       return multiDimOffset;\n@@ -93,21 +93,21 @@ struct ConvertLayoutOpConversion\n       SmallVector<Value> mmaColIdx(4);\n       SmallVector<Value> mmaRowIdx(2);\n       Value threadId = getThreadId(rewriter, loc);\n-      Value warpSize = idx_val(32);\n+      Value warpSize = i32_val(32);\n       Value laneId = urem(threadId, warpSize);\n       Value warpId = udiv(threadId, warpSize);\n       // TODO: fix the bug in MMAEncodingAttr document\n       SmallVector<Value> multiDimWarpId(2);\n-      multiDimWarpId[0] = urem(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n-      multiDimWarpId[1] = udiv(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n-      Value _1 = idx_val(1);\n-      Value _2 = idx_val(2);\n-      Value _4 = idx_val(4);\n-      Value _8 = idx_val(8);\n-      Value _16 = idx_val(16);\n+      multiDimWarpId[0] = urem(warpId, i32_val(mmaLayout.getWarpsPerCTA()[0]));\n+      multiDimWarpId[1] = udiv(warpId, i32_val(mmaLayout.getWarpsPerCTA()[0]));\n+      Value _1 = i32_val(1);\n+      Value _2 = i32_val(2);\n+      Value _4 = i32_val(4);\n+      Value _8 = i32_val(8);\n+      Value _16 = i32_val(16);\n       if (mmaLayout.isAmpere()) {\n-        multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n-        multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 8));\n+        multiDimWarpId[0] = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n+        multiDimWarpId[1] = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n         Value mmaGrpId = udiv(laneId, _4);\n         Value mmaGrpIdP8 = add(mmaGrpId, _8);\n         Value mmaThreadIdInGrp = urem(laneId, _4);\n@@ -131,9 +131,9 @@ struct ConvertLayoutOpConversion\n         multiDimOffset[0] = elemId < 2 ? mmaRowIdx[0] : mmaRowIdx[1];\n         multiDimOffset[1] = elemId % 2 == 0 ? mmaColIdx[0] : mmaColIdx[1];\n         multiDimOffset[0] = add(\n-            multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n+            multiDimOffset[0], i32_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n         multiDimOffset[1] = add(\n-            multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n+            multiDimOffset[1], i32_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n       } else if (mmaLayout.isVolta()) {\n         auto [isARow, isBRow, isAVec4, isBVec4, mmaId] =\n             mmaLayout.decodeVoltaLayoutStates();\n@@ -212,13 +212,13 @@ struct ConvertLayoutOpConversion\n               currVal = zext(llvmElemTy, currVal);\n             else if (isPtr)\n               currVal = ptrtoint(llvmElemTy, currVal);\n-            valVec = insert_element(vecTy, valVec, currVal, idx_val(v));\n+            valVec = insert_element(vecTy, valVec, currVal, i32_val(v));\n           }\n           store(valVec, ptr);\n         } else {\n           Value valVec = load(ptr);\n           for (unsigned v = 0; v < vec; ++v) {\n-            Value currVal = extract_element(llvmElemTy, valVec, idx_val(v));\n+            Value currVal = extract_element(llvmElemTy, valVec, i32_val(v));\n             if (isInt1)\n               currVal = icmp_ne(currVal,\n                                 rewriter.create<LLVM::ConstantOp>(\n@@ -322,13 +322,13 @@ struct ConvertLayoutOpConversion\n         Value valVec = undef(vecTy);\n         for (unsigned v = 0; v < vec; ++v) {\n           auto currVal = coord2valT[elemId + v].second;\n-          valVec = insert_element(vecTy, valVec, currVal, idx_val(v));\n+          valVec = insert_element(vecTy, valVec, currVal, i32_val(v));\n         }\n         store(valVec, ptr);\n       } else {\n         Value valVec = load(ptr);\n         for (unsigned v = 0; v < vec; ++v) {\n-          Value currVal = extract_element(elemTy, valVec, idx_val(v));\n+          Value currVal = extract_element(elemTy, valVec, i32_val(v));\n           vals[elemId + v] = currVal;\n         }\n       }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -565,7 +565,7 @@ struct AtomicRMWOpConversion\n         auto ret = ptxBuilderAtomicRMW.launch(rewriter, loc, retType);\n         for (int ii = 0; ii < vec; ++ii) {\n           resultVals[i + ii] =\n-              vec == 1 ? ret : extract_element(valueElemTy, ret, idx_val(ii));\n+              vec == 1 ? ret : extract_element(valueElemTy, ret, i32_val(ii));\n         }\n       } else {\n         PTXBuilder ptxBuilderMemfence;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 10, "deletions": 13, "changes": 23, "file_content_changes": "@@ -325,11 +325,9 @@ struct GetProgramIdOpConversion\n     Location loc = op->getLoc();\n     assert(op.getAxis() < 3);\n \n-    Value blockId = rewriter.create<::mlir::gpu::BlockIdOp>(\n-        loc, rewriter.getIndexType(), dims[op.getAxis()]);\n-    auto llvmIndexTy = getTypeConverter()->getIndexType();\n-    rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>(\n-        op, TypeRange{llvmIndexTy}, ValueRange{blockId});\n+    Value blockId =\n+        rewriter.create<::mlir::gpu::BlockIdOp>(loc, dims[op.getAxis()]);\n+    rewriter.replaceOpWithNewOp<arith::TruncIOp>(op, i32_ty, blockId);\n     return success();\n   }\n \n@@ -349,11 +347,10 @@ struct GetNumProgramsOpConversion\n     Location loc = op->getLoc();\n     assert(op.getAxis() < 3);\n \n-    Value blockId = rewriter.create<::mlir::gpu::GridDimOp>(\n-        loc, rewriter.getIndexType(), dims[op.getAxis()]);\n-    auto llvmIndexTy = getTypeConverter()->getIndexType();\n-    rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>(\n-        op, TypeRange{llvmIndexTy}, ValueRange{blockId});\n+    Value blockId =\n+        rewriter.create<::mlir::gpu::GridDimOp>(loc, dims[op.getAxis()]);\n+    rewriter.replaceOpWithNewOp<arith::TruncIOp>(op, i32_ty, blockId);\n+\n     return success();\n   }\n \n@@ -431,12 +428,12 @@ struct AllocTensorOpConversion\n };\n \n struct ExtractSliceOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<tensor::ExtractSliceOp> {\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ExtractSliceOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n-      tensor::ExtractSliceOp>::ConvertTritonGPUOpToLLVMPattern;\n+      triton::gpu::ExtractSliceOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   LogicalResult\n-  matchAndRewrite(tensor::ExtractSliceOp op, OpAdaptor adaptor,\n+  matchAndRewrite(triton::gpu::ExtractSliceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     // %dst = extract_slice %src[%offsets]\n     Location loc = op->getLoc();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 27, "deletions": 32, "changes": 59, "file_content_changes": "@@ -208,13 +208,9 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n   Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n     auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n-    auto cast = rewriter.create<UnrealizedConversionCastOp>(\n-        loc, TypeRange{llvmIndexTy},\n-        ValueRange{rewriter.create<::mlir::gpu::ThreadIdOp>(\n-            loc, rewriter.getIndexType(), ::mlir::gpu::Dimension::x)});\n-    Value threadId = cast.getResult(0);\n-\n-    return threadId;\n+    auto tid = rewriter.create<::mlir::gpu::ThreadIdOp>(\n+        loc, rewriter.getIndexType(), ::mlir::gpu::Dimension::x);\n+    return rewriter.create<arith::TruncIOp>(loc, i32_ty, tid);\n   }\n \n   // -----------------------------------------------------------------------\n@@ -223,13 +219,12 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   template <typename T>\n   Value getSharedMemoryBase(Location loc, ConversionPatternRewriter &rewriter,\n                             T value) const {\n-\n     auto ptrTy = LLVM::LLVMPointerType::get(\n         this->getTypeConverter()->convertType(rewriter.getI8Type()), 3);\n     auto bufferId = allocation->getBufferId(value);\n     assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n     size_t offset = allocation->getOffset(bufferId);\n-    Value offVal = idx_val(offset);\n+    Value offVal = i32_val(offset);\n     Value base = gep(ptrTy, smem, offVal);\n     return base;\n   }\n@@ -244,8 +239,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     // This utililty computes the pointers for accessing the provided swizzled\n     // shared memory layout `resSharedLayout`. More specifically, it computes,\n     // for all indices (row, col) of `srcEncoding` such that idx % inVec = 0,\n-    // the pointer: ptr[(row, col)] = base + (rowOff * strides[ord[1]] + colOff)\n-    // where :\n+    // the pointer: ptr[(row, col)] = base + (rowOff * strides[ord[1]] +\n+    // colOff) where :\n     //   compute phase = (row // perPhase) % maxPhase\n     //   rowOff = row\n     //   colOff = colOffSwizzled + colOffOrdered\n@@ -255,8 +250,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     // Note 1:\n     // -------\n     // Because swizzling happens at a granularity of outVec, we need to\n-    // decompose the offset into a swizzled factor and a non-swizzled (ordered)\n-    // factor\n+    // decompose the offset into a swizzled factor and a non-swizzled\n+    // (ordered) factor\n     //\n     // Note 2:\n     // -------\n@@ -435,7 +430,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     } else {\n       Value remained = linear;\n       for (auto &&en : llvm::enumerate(shape.drop_back())) {\n-        Value dimSize = idx_val(en.value());\n+        Value dimSize = i32_val(en.value());\n         multiDim[en.index()] = urem(remained, dimSize);\n         remained = udiv(remained, dimSize);\n       }\n@@ -454,12 +449,12 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n                   ArrayRef<Value> multiDim, ArrayRef<unsigned> shape) const {\n     auto rank = multiDim.size();\n-    Value linear = idx_val(0);\n+    Value linear = i32_val(0);\n     if (rank > 0) {\n       linear = multiDim.back();\n       for (auto [dim, dimShape] :\n            llvm::reverse(llvm::zip(multiDim.drop_back(), shape.drop_back()))) {\n-        Value dimSize = idx_val(dimShape);\n+        Value dimSize = i32_val(dimShape);\n         linear = add(mul(linear, dimSize), dim);\n       }\n     }\n@@ -469,7 +464,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   Value dot(ConversionPatternRewriter &rewriter, Location loc,\n             ArrayRef<Value> offsets, ArrayRef<Value> strides) const {\n     assert(offsets.size() == strides.size());\n-    Value ret = idx_val(0);\n+    Value ret = i32_val(0);\n     for (auto [offset, stride] : llvm::zip(offsets, strides)) {\n       ret = add(ret, mul(offset, stride));\n     }\n@@ -597,7 +592,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                                 const BlockedEncodingAttr &blocked_layout,\n                                 ArrayRef<int64_t> shape) const {\n     Value threadId = getThreadId(rewriter, loc);\n-    Value warpSize = idx_val(32);\n+    Value warpSize = i32_val(32);\n     Value laneId = urem(threadId, warpSize);\n     Value warpId = udiv(threadId, warpSize);\n     auto sizePerThread = blocked_layout.getSizePerThread();\n@@ -619,13 +614,13 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       auto maxWarps =\n           ceil<unsigned>(shape[k], sizePerThread[k] * threadsPerWarp[k]);\n       auto maxThreads = ceil<unsigned>(shape[k], sizePerThread[k]);\n-      multiDimWarpId[k] = urem(multiDimWarpId[k], idx_val(maxWarps));\n-      multiDimThreadId[k] = urem(multiDimThreadId[k], idx_val(maxThreads));\n+      multiDimWarpId[k] = urem(multiDimWarpId[k], i32_val(maxWarps));\n+      multiDimThreadId[k] = urem(multiDimThreadId[k], i32_val(maxThreads));\n       // multiDimBase[k] = (multiDimThreadId[k] +\n       //                    multiDimWarpId[k] * threadsPerWarp[k]) *\n       //                   sizePerThread[k];\n-      Value threadsPerWarpK = idx_val(threadsPerWarp[k]);\n-      Value sizePerThreadK = idx_val(sizePerThread[k]);\n+      Value threadsPerWarpK = i32_val(threadsPerWarp[k]);\n+      Value sizePerThreadK = i32_val(sizePerThread[k]);\n       multiDimBase[k] =\n           mul(sizePerThreadK, add(multiDimThreadId[k],\n                                   mul(multiDimWarpId[k], threadsPerWarpK)));\n@@ -799,21 +794,21 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                               ArrayRef<int64_t> shape) const {\n     auto _warpsPerCTA = mmaLayout.getWarpsPerCTA();\n     assert(_warpsPerCTA.size() == 2);\n-    SmallVector<Value> warpsPerCTA = {idx_val(_warpsPerCTA[0]),\n-                                      idx_val(_warpsPerCTA[1])};\n+    SmallVector<Value> warpsPerCTA = {i32_val(_warpsPerCTA[0]),\n+                                      i32_val(_warpsPerCTA[1])};\n     Value threadId = getThreadId(rewriter, loc);\n-    Value warpSize = idx_val(32);\n+    Value warpSize = i32_val(32);\n     Value laneId = urem(threadId, warpSize);\n     Value warpId = udiv(threadId, warpSize);\n-    Value warpId0 = urem(urem(warpId, warpsPerCTA[0]), idx_val(shape[0] / 16));\n+    Value warpId0 = urem(urem(warpId, warpsPerCTA[0]), i32_val(shape[0] / 16));\n     Value warpId1 = urem(urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]),\n-                         idx_val(shape[1] / 8));\n-    Value offWarp0 = mul(warpId0, idx_val(16));\n-    Value offWarp1 = mul(warpId1, idx_val(8));\n+                         i32_val(shape[1] / 8));\n+    Value offWarp0 = mul(warpId0, i32_val(16));\n+    Value offWarp1 = mul(warpId1, i32_val(8));\n \n     SmallVector<Value> multiDimBase(2);\n-    multiDimBase[0] = add(udiv(laneId, idx_val(4)), offWarp0);\n-    multiDimBase[1] = add(mul(idx_val(2), urem(laneId, idx_val(4))), offWarp1);\n+    multiDimBase[0] = add(udiv(laneId, i32_val(4)), offWarp0);\n+    multiDimBase[1] = add(mul(i32_val(2), urem(laneId, i32_val(4))), offWarp1);\n     return multiDimBase;\n   }\n \n@@ -850,7 +845,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                                                 SmallVector<Value>(rank));\n     for (unsigned n = 0; n < elemsPerThread; ++n)\n       for (unsigned k = 0; k < rank; ++k)\n-        multiDimIdx[n][k] = add(multiDimBase[k], idx_val(offset[n][k]));\n+        multiDimIdx[n][k] = add(multiDimBase[k], i32_val(offset[n][k]));\n     return multiDimIdx;\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 80, "deletions": 85, "changes": 165, "file_content_changes": "@@ -4,8 +4,11 @@\n #include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n #include \"mlir/Conversion/ControlFlowToLLVM//ControlFlowToLLVM.h\"\n #include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n+#include \"mlir/Conversion/LLVMCommon/VectorPattern.h\"\n #include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n #include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n+#include \"mlir/Dialect/Index/IR/IndexDialect.h\"\n+#include \"mlir/Dialect/Index/IR/IndexOps.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n #include \"mlir/Pass/Pass.h\"\n@@ -24,31 +27,21 @@\n #include \"TypeConverter.h\"\n #include \"ViewOpToLLVM.h\"\n \n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n+\n using namespace mlir;\n using namespace mlir::triton;\n \n #define GEN_PASS_CLASSES\n #include \"triton/Conversion/Passes.h.inc\"\n \n-namespace mlir {\n-\n-class TritonLLVMConversionTarget : public ConversionTarget {\n-public:\n-  explicit TritonLLVMConversionTarget(MLIRContext &ctx)\n-      : ConversionTarget(ctx) {\n-    addLegalDialect<LLVM::LLVMDialect>();\n-    addLegalDialect<NVVM::NVVMDialect>();\n-    addIllegalDialect<triton::TritonDialect>();\n-    addIllegalDialect<triton::gpu::TritonGPUDialect>();\n-    addIllegalDialect<mlir::gpu::GPUDialect>();\n-    addLegalOp<mlir::UnrealizedConversionCastOp>();\n-  }\n-};\n+namespace {\n \n class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n public:\n   explicit TritonLLVMFunctionConversionTarget(MLIRContext &ctx)\n       : ConversionTarget(ctx) {\n+    addLegalDialect<index::IndexDialect>();\n     addLegalDialect<LLVM::LLVMDialect>();\n     addLegalDialect<NVVM::NVVMDialect>();\n     addIllegalOp<mlir::func::FuncOp>();\n@@ -66,9 +59,26 @@ class TritonPTXConversionTarget : public ConversionTarget {\n   }\n };\n \n-} // namespace mlir\n+struct ReturnOpConversion : public ConvertOpToLLVMPattern<func::ReturnOp> {\n+  using ConvertOpToLLVMPattern<func::ReturnOp>::ConvertOpToLLVMPattern;\n \n-namespace {\n+  LogicalResult\n+  matchAndRewrite(func::ReturnOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    unsigned numArguments = op.getNumOperands();\n+\n+    // Currently, Triton kernel function always return nothing.\n+    // TODO(Superjomn) add support for non-inline device function\n+    if (numArguments > 0) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"Only kernel function with nothing returned is supported.\");\n+    }\n+\n+    rewriter.replaceOpWithNewOp<LLVM::ReturnOp>(op, TypeRange(), ValueRange(),\n+                                                op->getAttrs());\n+    return success();\n+  }\n+};\n \n /// FuncOp legalization pattern that converts MemRef arguments to pointers to\n /// MemRef descriptors (LLVM struct data types) containing all the MemRef type\n@@ -82,8 +92,9 @@ struct FuncOpConversion : public FuncOpConversionBase {\n   matchAndRewrite(func::FuncOp funcOp, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     auto newFuncOp = convertFuncOpToLLVMFuncOp(funcOp, rewriter);\n-    if (!newFuncOp)\n+    if (!newFuncOp) {\n       return failure();\n+    }\n \n     auto ctx = funcOp->getContext();\n \n@@ -103,6 +114,22 @@ struct FuncOpConversion : public FuncOpConversionBase {\n   int numWarps{0};\n };\n \n+class TritonLLVMConversionTarget : public ConversionTarget {\n+public:\n+  explicit TritonLLVMConversionTarget(MLIRContext &ctx)\n+      : ConversionTarget(ctx) {\n+    addLegalDialect<LLVM::LLVMDialect>();\n+    addLegalDialect<NVVM::NVVMDialect>();\n+    addIllegalDialect<triton::TritonDialect>();\n+    addIllegalDialect<triton::gpu::TritonGPUDialect>();\n+    addIllegalDialect<mlir::gpu::GPUDialect>();\n+    addLegalOp<mlir::UnrealizedConversionCastOp>();\n+  }\n+};\n+\n+using FPTruncLowering =\n+    VectorConvertToLLVMPattern<LLVM::FPTruncOp, arith::TruncFOp>;\n+\n class ConvertTritonGPUToLLVM\n     : public ConvertTritonGPUToLLVMBase<ConvertTritonGPUToLLVM> {\n \n@@ -113,48 +140,39 @@ class ConvertTritonGPUToLLVM\n   void runOnOperation() override {\n     MLIRContext *context = &getContext();\n     ModuleOp mod = getOperation();\n-\n     mlir::LowerToLLVMOptions option(context);\n     option.overrideIndexBitwidth(32);\n     TritonGPUToLLVMTypeConverter typeConverter(context, option);\n-    TritonLLVMFunctionConversionTarget funcTarget(*context);\n     TritonLLVMConversionTarget target(*context);\n-\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n \n-    // Step 1: Decompose unoptimized layout conversions to use shared memory\n-    // Step 2: Decompose insert_slice_async to use load + insert_slice for\n-    //   pre-Ampere architectures or unsupported vectorized load sizes\n-    // Step 3: Allocate shared memories and insert barriers\n-    // Step 4: Convert FuncOp to LLVMFuncOp via partial conversion\n-    // Step 5: Get axis and shared memory info\n-    // Step 6: Convert the rest of ops via partial conversion\n-    //\n-    // The reason for a separation between 4/6 is that, step 5 is out of the\n-    // scope of Dialect Conversion, thus we need to make sure the smem is not\n-    // revised during the conversion of step 6.\n-\n-    // Step 1\n+    /* preprocess */\n     decomposeMmaToDotOperand(mod, numWarps);\n     decomposeBlockedToDotOperand(mod);\n-\n-    // Step 2\n     if (failed(decomposeInsertSliceAsyncOp(mod)))\n       return signalPassFailure();\n \n-    // Step 3\n+    /* allocate shared memory and set barrier */\n     Allocation allocation(mod);\n     MembarAnalysis membarPass(&allocation);\n     membarPass.run();\n \n-    // Step 4\n-    RewritePatternSet funcPatterns(context);\n-    funcPatterns.add<FuncOpConversion>(typeConverter, numWarps, /*benefit=*/1);\n-    if (failed(\n-            applyPartialConversion(mod, funcTarget, std::move(funcPatterns))))\n-      return signalPassFailure();\n+    /* lower functions */\n+    {\n+      mlir::LowerToLLVMOptions option(context);\n+      TritonGPUToLLVMTypeConverter typeConverter(context, option);\n+      TritonLLVMFunctionConversionTarget funcTarget(*context);\n+      RewritePatternSet funcPatterns(context);\n+      funcPatterns.add<FuncOpConversion>(typeConverter, numWarps,\n+                                         /*benefit=*/1);\n+      funcPatterns.add<ReturnOpConversion>(typeConverter);\n+      mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n+                                                            funcPatterns);\n+      if (failed(\n+              applyPartialConversion(mod, funcTarget, std::move(funcPatterns))))\n+        return signalPassFailure();\n+    }\n \n-    // Step 5 - get axis and shared memory info\n     std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n     AxisInfoAnalysis *axisInfoAnalysis = solver->load<AxisInfoAnalysis>();\n     if (failed(solver->initializeAndRun(mod)))\n@@ -164,52 +182,29 @@ class ConvertTritonGPUToLLVM\n                  mlir::IntegerAttr::get(mlir::IntegerType::get(context, 32),\n                                         allocation.getSharedMemorySize()));\n \n-    // Step 6 - rewrite rest of ops\n-    // We set a higher benefit here to ensure triton's patterns runs before\n-    // arith patterns for some encoding not supported by the community\n-    // patterns.\n+    /* rewrite ops */\n+    RewritePatternSet patterns(context);\n+    // TritonGPU lowering patterns\n     OpBuilder::InsertPoint indexInsertPoint;\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo indexCacheInfo{\n         &baseIndexCache, &indexCache, &indexInsertPoint};\n-\n-    RewritePatternSet patterns(context);\n-\n-    // Normal conversions\n-    populateTritonGPUToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                    *axisInfoAnalysis, &allocation, smem,\n-                                    indexCacheInfo, /*benefit=*/10);\n-    // ConvertLayoutOp\n-    populateConvertLayoutOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                          *axisInfoAnalysis, &allocation, smem,\n-                                          indexCacheInfo, /*benefit=*/10);\n-    // DotOp\n-    populateDotOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                *axisInfoAnalysis, &allocation, smem,\n-                                /*benefit=*/10);\n-    // ElementwiseOp\n-    populateElementwiseOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                        *axisInfoAnalysis, &allocation, smem,\n-                                        /*benefit=*/10);\n-    // LoadStoreOp\n-    populateLoadStoreOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                      *axisInfoAnalysis, &allocation, smem,\n-                                      indexCacheInfo, /*benefit=*/10);\n-    // ReduceOp\n-    populateReduceOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                   *axisInfoAnalysis, &allocation, smem,\n-                                   indexCacheInfo, /*benefit=*/10);\n-    // ViewOp\n-    populateViewOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                 *axisInfoAnalysis, &allocation, smem,\n-                                 /*benefit=*/10);\n-\n-    // Add arith/math's patterns to help convert scalar expression to LLVM.\n-    mlir::arith::populateArithToLLVMConversionPatterns(typeConverter, patterns);\n-    mlir::populateMathToLLVMConversionPatterns(typeConverter, patterns);\n-    mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n-                                                          patterns);\n+    auto populatePatterns1 = [&](auto populateFunc) {\n+      populateFunc(typeConverter, patterns, numWarps, *axisInfoAnalysis,\n+                   &allocation, smem, indexCacheInfo, /*benefit*/ 1);\n+    };\n+    auto populatePatterns2 = [&](auto populateFunc) {\n+      populateFunc(typeConverter, patterns, numWarps, *axisInfoAnalysis,\n+                   &allocation, smem, /*benefit*/ 1);\n+    };\n+    populatePatterns1(populateTritonGPUToLLVMPatterns);\n+    populatePatterns1(populateConvertLayoutOpToLLVMPatterns);\n+    populatePatterns2(populateDotOpToLLVMPatterns);\n+    populatePatterns2(populateElementwiseOpToLLVMPatterns);\n+    populatePatterns1(populateLoadStoreOpToLLVMPatterns);\n+    populatePatterns1(populateReduceOpToLLVMPatterns);\n+    populatePatterns2(populateViewOpToLLVMPatterns);\n+    // Native lowering patterns\n     mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n-\n     if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n       return signalPassFailure();\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -71,6 +71,7 @@\n #define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n \n // Types\n+#define i64_ty rewriter.getIntegerType(64)\n #define i32_ty rewriter.getIntegerType(32)\n #define i16_ty rewriter.getIntegerType(16)\n #define ui32_ty rewriter.getIntegerType(32, false)\n@@ -90,9 +91,6 @@\n #define i32_val(...) LLVM::createConstantI32(loc, rewriter, __VA_ARGS__)\n #define int_val(width, val)                                                    \\\n   LLVM::createLLVMIntegerConstant(rewriter, loc, width, val)\n-#define idx_val(...)                                                           \\\n-  LLVM::createIndexConstant(rewriter, loc, this->getTypeConverter(),           \\\n-                            __VA_ARGS__)\n #define tid_val() getThreadId(rewriter, loc)\n \n // Attributes"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 25, "deletions": 2, "changes": 27, "file_content_changes": "@@ -851,7 +851,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n     return mlir::success();\n   }\n   // cvt(extract_slice(x), type2) -> extract_slice(cvt(x, type2))\n-  auto extract_slice = dyn_cast<tensor::ExtractSliceOp>(arg);\n+  auto extract_slice = dyn_cast<triton::gpu::ExtractSliceOp>(arg);\n   if (extract_slice) {\n     if (!isSharedEncoding(op->getResult(0))) {\n       return mlir::failure();\n@@ -872,7 +872,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n     rewriter.setInsertionPoint(extract_slice);\n     auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         op->getLoc(), newType, extract_slice.getSource());\n-    rewriter.replaceOpWithNewOp<tensor::ExtractSliceOp>(\n+    rewriter.replaceOpWithNewOp<triton::gpu::ExtractSliceOp>(\n         op, resType, newArg.getResult(), extract_slice.offsets(),\n         extract_slice.sizes(), extract_slice.strides(),\n         extract_slice.static_offsets(), extract_slice.static_sizes(),\n@@ -925,6 +925,29 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n \n //===----------------------------------------------------------------------===//\n \n+/// Build an ExtractSliceOp with mixed static and dynamic entries and custom\n+/// result type. If the type passed is nullptr, it is inferred.\n+void ExtractSliceOp::build(OpBuilder &b, OperationState &result,\n+                           RankedTensorType resultType, Value source,\n+                           ArrayRef<OpFoldResult> offsets,\n+                           ArrayRef<OpFoldResult> sizes,\n+                           ArrayRef<OpFoldResult> strides,\n+                           ArrayRef<NamedAttribute> attrs) {\n+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;\n+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;\n+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);\n+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);\n+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);\n+  auto sourceRankedTensorType = source.getType().cast<RankedTensorType>();\n+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,\n+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),\n+        b.getDenseI64ArrayAttr(staticSizes),\n+        b.getDenseI64ArrayAttr(staticStrides));\n+  result.addAttributes(attrs);\n+}\n+\n+//===----------------------------------------------------------------------===//\n+\n void TritonGPUDialect::initialize() {\n   addAttributes<\n #define GET_ATTRDEF_LIST"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "file_content_changes": "@@ -394,7 +394,7 @@ void LoopPipeliner::emitPrologue() {\n     sliceType = RankedTensorType::get({bufferShape[1], bufferShape[2]},\n                                       sliceType.getElementType(),\n                                       loadsBufferType[loadOp].getEncoding());\n-    Value extractSlice = builder.create<tensor::ExtractSliceOp>(\n+    Value extractSlice = builder.create<triton::gpu::ExtractSliceOp>(\n         loadOp.getLoc(), sliceType, loadStageBuffer[loadOp][numStages - 1],\n         SmallVector<OpFoldResult>{int_attr(0), int_attr(0), int_attr(0)},\n         SmallVector<OpFoldResult>{int_attr(1),\n@@ -532,8 +532,6 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   Value extractSliceIndex = builder.create<arith::RemSIOp>(\n       nextIV.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n-  extractSliceIndex = builder.create<arith::IndexCastOp>(\n-      extractSliceIndex.getLoc(), builder.getIndexType(), extractSliceIndex);\n \n   for (Operation *op : orderedDeps)\n     if (!loads.contains(op->getResult(0))) {\n@@ -591,7 +589,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n                                         sliceType.getElementType(),\n                                         loadsBufferType[loadOp].getEncoding());\n \n-      nextOp = builder.create<tensor::ExtractSliceOp>(\n+      nextOp = builder.create<triton::gpu::ExtractSliceOp>(\n           op->getLoc(), sliceType, insertAsyncOp,\n           SmallVector<OpFoldResult>{extractSliceIndex, int_attr(0),\n                                     int_attr(0)},"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "file_content_changes": "@@ -103,11 +103,9 @@ Value Prefetcher::generatePrefetch(Value v, unsigned opIdx, bool isPrologue,\n   if (offsetK)\n     offset[kIdx] = *offsetK;\n \n-  Value newSmem = builder.create<tensor::ExtractSliceOp>(\n-      v.getLoc(),\n-      // TODO: encoding?\n-      RankedTensorType::get(shape, elementType, type.getEncoding()), v,\n-      SmallVector<OpFoldResult>{intAttr(offset[0]), intAttr(offset[1])},\n+  Value newSmem = builder.create<triton::gpu::ExtractSliceOp>(\n+      v.getLoc(), RankedTensorType::get(shape, elementType, type.getEncoding()),\n+      v, SmallVector<OpFoldResult>{intAttr(offset[0]), intAttr(offset[1])},\n       SmallVector<OpFoldResult>{intAttr(shape[0]), intAttr(shape[1])},\n       SmallVector<OpFoldResult>{intAttr(1), intAttr(1)});\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -202,7 +202,7 @@ inline bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n     return true;\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n     return expensiveLoadOrStore(op, targetEncoding);\n-  if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n+  if (isa<triton::gpu::ExtractSliceOp, triton::gpu::AllocTensorOp,\n           triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n           triton::AtomicCASOp, triton::DotOp>(op))\n     return true;"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "file_content_changes": "@@ -12,6 +12,7 @@\n #include \"mlir/Target/LLVMIR/Export.h\"\n #include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n #include \"mlir/Transforms/Passes.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Tools/Sys/GetEnv.hpp\"\n #include \"llvm/ADT/APInt.h\"\n@@ -295,12 +296,14 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n       /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n   pm.addPass(mlir::createConvertSCFToCFPass());\n+  pm.addPass(createTritonConvertArithToIndexPass());\n+  pm.addPass(mlir::createConvertIndexToLLVMPass());\n   pm.addPass(createConvertTritonGPUToLLVMPass(computeCapability));\n-  // Canonicalize to eliminate the remaining UnrealizedConversionCastOp\n+  pm.addPass(mlir::createArithToLLVMConversionPass());\n   pm.addPass(mlir::createCanonicalizerPass());\n-  pm.addPass(mlir::createCSEPass()); // Simplify the IR to improve readability.\n+  // Simplify the IR\n+  pm.addPass(mlir::createCSEPass());\n   pm.addPass(mlir::createSymbolDCEPass());\n-  pm.addPass(mlir::createCanonicalizerPass());\n \n   if (failed(pm.run(module))) {\n     llvm::errs() << \"Pass execution failed\";"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "file_content_changes": "@@ -13,6 +13,8 @@\n \n #include \"mlir/Dialect/ControlFlow/IR/ControlFlow.h\"\n #include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n+#include \"mlir/Dialect/Index/IR/IndexDialect.h\"\n+#include \"mlir/Dialect/Index/IR/IndexOps.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"triton/Analysis/Allocation.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n@@ -119,6 +121,7 @@ void init_triton_ir(py::module &&m) {\n       .def(py::init<>())\n       .def(\"load_triton\", [](mlir::MLIRContext &self) {\n         self.getOrLoadDialect<mlir::triton::TritonDialect>();\n+        self.getOrLoadDialect<mlir::index::IndexDialect>();\n         // we load LLVM because the frontend uses LLVM.undef for\n         // some placeholders\n         self.getOrLoadDialect<mlir::triton::TritonDialect>();\n@@ -393,8 +396,8 @@ void init_triton_ir(py::module &&m) {\n         registry.insert<mlir::triton::TritonDialect,\n                         mlir::triton::gpu::TritonGPUDialect,\n                         mlir::math::MathDialect, mlir::arith::ArithDialect,\n-                        mlir::func::FuncDialect, mlir::scf::SCFDialect,\n-                        mlir::cf::ControlFlowDialect>();\n+                        mlir::index::IndexDialect, mlir::func::FuncDialect,\n+                        mlir::scf::SCFDialect, mlir::cf::ControlFlowDialect>();\n         context.appendDialectRegistry(registry);\n         context.loadAllAvailableDialects();\n \n@@ -807,7 +810,7 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::OpBuilder &self, mlir::Value &input) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::IndexCastOp>(\n-                 loc, self.getI32Type(), input);\n+                 loc, self.getI64Type(), input);\n            })\n       .def(\"create_fmul\",\n            [](mlir::OpBuilder &self, mlir::Value &lhs,"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 18, "deletions": 1, "changes": 19, "file_content_changes": "@@ -1501,7 +1501,7 @@ def kernel(x):\n \n @pytest.mark.parametrize(\"value, value_type\", [\n     (-1, 'i32'), (0, 'i32'), (-2**31, 'i32'), (2**31 - 1, 'i32'),\n-    (2**31, 'u32'), (2**32 - 1, 'u32'), (2**32, 'i64'), (2**63 - 1, 'i64'),\n+    (2**31, 'i64'), (2**32 - 1, 'i64'), (2**32, 'i64'), (2**63 - 1, 'i64'),\n     (-2**63, 'i64'), (2**63, 'u64'), (2**64 - 1, 'u64')\n ])\n def test_value_specialization(value: int, value_type: str, device='cuda') -> None:\n@@ -1761,6 +1761,23 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n # -----------------------\n \n \n+def test_for_iv_int64():\n+\n+    @triton.jit\n+    def kernel(Out, lo, hi):\n+        acc = 0\n+        acc = acc.to(tl.int64)\n+        for i in range(lo, hi):\n+            acc += i\n+        tl.store(Out, acc)\n+\n+    lo = 2**35\n+    hi = 2**35 + 20\n+    out = to_triton(np.zeros((1,), dtype=np.int64), device='cuda')\n+    kernel[(1,)](out, lo, hi)\n+    assert out[0] == sum(range(lo, hi))\n+\n+\n def test_if_else():\n \n     @triton.jit"}, {"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 0, "deletions": 28, "changes": 28, "file_content_changes": "@@ -1,6 +1,5 @@\n import multiprocessing\n import os\n-import re\n import shutil\n from collections import namedtuple\n \n@@ -107,33 +106,6 @@ def inc_counter(*args, **kwargs):\n     assert counter == target\n \n \n-@pytest.mark.parametrize(\"value, value_type\", [\n-    (-1, 'i32'), (0, 'i32'), (1, 'i32'), (-2**31, 'i32'), (2**31 - 1, 'i32'),\n-    (2**32, 'i64'), (2**63 - 1, 'i64'), (-2**63, 'i64'),\n-    (2**31, 'u32'), (2**32 - 1, 'u32'), (2**63, 'u64'), (2**64 - 1, 'u64')\n-])\n-def test_value_specialization(value: int, value_type: str, device='cuda') -> None:\n-\n-    @triton.jit\n-    def kernel(VALUE, X):\n-        pass\n-\n-    cache_str = None\n-\n-    def get_cache_str(*args, **kwargs):\n-        nonlocal cache_str\n-        cache_str = kwargs[\"repr\"]\n-    triton.JITFunction.cache_hook = get_cache_str\n-    reset_tmp_dir()\n-    x = torch.tensor([3.14159], device='cuda')\n-    kernel[(1, )](value, x)\n-    triton.JITFunction.cache_hook = None\n-\n-    cache_str_match = re.match(r\".*VALUE: (\\w+).*\", cache_str)\n-    spec_type = None if cache_str_match is None else cache_str_match.group(1)\n-    assert spec_type == value_type\n-\n-\n def test_constexpr_not_callable() -> None:\n     @triton.jit\n     def kernel(X, c: tl.constexpr):"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 15, "deletions": 6, "changes": 21, "file_content_changes": "@@ -665,17 +665,24 @@ def visit_For(self, node):\n             step = triton.language.constexpr(-step.value)\n             negative_step = True\n             lb, ub = ub, lb\n+        lb = triton.language.core._to_tensor(lb, self.builder)\n+        ub = triton.language.core._to_tensor(ub, self.builder)\n+        step = triton.language.core._to_tensor(step, self.builder)\n+        # induction variable type\n+        iv_type = triton.language.semantic.integer_promote_impl(lb.dtype, ub.dtype)\n+        iv_type = triton.language.semantic.integer_promote_impl(iv_type, step.dtype)\n+        iv_ir_type = iv_type.to_ir(self.builder)\n         # lb/ub/step might be constexpr, we need to cast them to tensor\n-        lb = triton.language.core._to_tensor(lb, self.builder).handle\n-        ub = triton.language.core._to_tensor(ub, self.builder).handle\n-        step = triton.language.core._to_tensor(step, self.builder).handle\n+        lb = lb.handle\n+        ub = ub.handle\n+        step = step.handle\n         # ForOp can only accept IndexType as lb/ub/step. Cast integer to Index\n         lb = self.builder.create_to_index(lb)\n         ub = self.builder.create_to_index(ub)\n         step = self.builder.create_to_index(step)\n         # Create placeholder for the loop induction variable\n-        iv = self.builder.create_undef(self.builder.get_int32_ty())\n-        self.set_value(node.target.id, triton.language.core.tensor(iv, triton.language.core.int32))\n+        iv = self.builder.create_undef(iv_ir_type)\n+        self.set_value(node.target.id, triton.language.core.tensor(iv, iv_type))\n \n         with enter_sub_region(self) as sr:\n             liveins, insert_block = sr\n@@ -732,11 +739,13 @@ def visit_For(self, node):\n             # update induction variable with actual value, and replace all uses\n             self.builder.set_insertion_point_to_start(for_op.get_body(0))\n             iv = self.builder.create_index_to_si(for_op.get_induction_var())\n+            iv = self.builder.create_int_cast(iv, iv_ir_type, True)\n             if negative_step:\n                 ub_si = self.builder.create_index_to_si(ub)\n+                ub_si = self.builder.create_int_cast(ub_si, iv_ir_type, True)\n                 iv = self.builder.create_sub(ub_si, iv)\n             self.lscope[node.target.id].handle.replace_all_uses_with(iv)\n-            self.set_value(node.target.id, triton.language.core.tensor(iv, triton.language.core.int32))\n+            self.set_value(node.target.id, triton.language.core.tensor(iv, iv_type))\n \n         # update lscope & local_defs (ForOp defines new values)\n         for i, name in enumerate(names):"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -125,8 +125,6 @@ def _key_of(arg):\n         elif isinstance(arg, int):\n             if -2**31 <= arg and arg <= 2**31 - 1:\n                 return \"i32\"\n-            elif 2**31 <= arg and arg <= 2**32 - 1:\n-                return \"u32\"\n             elif 2**63 <= arg and arg <= 2**64 - 1:\n                 return \"u64\"\n             else:"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -156,7 +156,7 @@\n \n @triton.autotune(\n     configs=[\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n     ],\n     key=['M', 'N', 'K'],\n )"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -93,9 +93,9 @@ func.func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n func.func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n-  %index = arith.constant 0 : index\n-  // CHECK-NEXT: %extracted_slice -> %cst\n-  %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n+  %index = arith.constant 0 : i32\n+  // CHECK-NEXT: %0 -> %cst\n+  %cst1 = triton_gpu.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n   return\n }\n \n@@ -169,9 +169,9 @@ func.func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   // CHECK-NEXT: %0#2 -> %cst,%cst_0\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     scf.if %i1 {\n-      %index = arith.constant 8 : index\n-      // CHECK-NEXT: %extracted_slice -> %cst,%cst_0\n-      %cst0 = tensor.extract_slice %a_shared[%index, 0][1, 32][1, 1] : tensor<128x32xf16, #A_SHARED> to tensor<32xf16, #A_SHARED>\n+      %index = arith.constant 8 : i32\n+      // CHECK-NEXT: %1 -> %cst,%cst_0\n+      %cst0 = triton_gpu.extract_slice %a_shared[%index, 0][1, 32][1, 1] : tensor<128x32xf16, #A_SHARED> to tensor<32xf16, #A_SHARED>\n       scf.yield\n     }\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -200,8 +200,8 @@ func.func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n func.func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n-  %index = arith.constant 0 : index\n-  %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1,1,1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n+  %index = arith.constant 0 : i32\n+  %cst1 = triton_gpu.extract_slice %cst0[%index, 0, 0][1, 16, 16][1,1,1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 512\n }\n@@ -284,8 +284,8 @@ func.func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f1\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     scf.if %i1 {\n-      %index = arith.constant 8 : index\n-      %cst0 = tensor.extract_slice %a_shared[%index, 0][1, 32][1, 1] : tensor<128x32xf16, #A_SHARED> to tensor<32xf16, #A_SHARED>\n+      %index = arith.constant 8 : i32\n+      %cst0 = triton_gpu.extract_slice %a_shared[%index, 0][1, 32][1, 1] : tensor<128x32xf16, #A_SHARED> to tensor<32xf16, #A_SHARED>\n       scf.yield\n     }\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -109,8 +109,8 @@ func.func @alloc() {\n // CHECK-LABEL: extract_slice\n func.func @extract_slice() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n-  %index = arith.constant 0 : index\n-  %0 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n+  %index = arith.constant 0 : i32\n+  %0 = triton_gpu.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %1 = triton_gpu.convert_layout %0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -1,7 +1,7 @@\n // RUN: triton-opt %s -split-input-file --convert-triton-gpu-to-llvm | FileCheck %s\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  // CHECK: llvm.func @test_empty_kernel(%arg0: i32, %arg1: !llvm.ptr<f16, 1>)\n+  // CHECK: llvm.func @test_empty_kernel(%arg0: i64, %arg1: !llvm.ptr<f16, 1>)\n   // Here the 128 comes from the 4 in module attribute multiples 32\n   // CHECK:  attributes {nvvm.kernel = 1 : ui1, nvvm.maxntid = [128 : i32]} {{.*}}\n   func.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n@@ -412,9 +412,9 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK-NEXT: llvm.mul\n     // CHECK-NEXT: llvm.add\n     // CHECK-NEXT: llvm.getelementptr\n-    %index = arith.constant 1 : index\n+    %index = arith.constant 1 : i32\n     %0 = triton_gpu.alloc_tensor : tensor<128x16x32xf32, #shared0>\n-    %1 = tensor.extract_slice %0[%index, 0, 0][1, 16, 32][1, 1, 1] : tensor<128x16x32xf32, #shared0> to tensor<16x32xf32, #shared0>\n+    %1 = triton_gpu.extract_slice %0[%index, 0, 0][1, 16, 32][1, 1, 1] : tensor<128x16x32xf32, #shared0> to tensor<16x32xf32, #shared0>\n     return\n   }\n }"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 13, "deletions": 16, "changes": 29, "file_content_changes": "@@ -29,20 +29,19 @@\n // CHECK-DAG: %[[LOOP_COND_1_SPLAT_B:.*]] = tt.splat %[[LOOP_COND_1]]\n // CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]], %[[LOOP_COND_1_SPLAT_B]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n-// CHECK: %[[A0:.*]] = tensor.extract_slice %[[A1BUFFER]][0, 0, 0]\n-// CHECK: %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n+// CHECK: %[[A0:.*]] = triton_gpu.extract_slice %[[A1BUFFER]][0, 0, 0]\n+// CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:   %[[arg_a0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_a0]]\n // CHECK:   %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n // CHECK:   tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_INT:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.index_cast %[[EXTRACT_INT]] : i32 to index\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:   %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n-// CHECK:   %[[NEXT_A:.*]] = tensor.extract_slice %[[NEXT_A_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n-// CHECK:   %[[NEXT_B:.*]] = tensor.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n+// CHECK:   %[[NEXT_A:.*]] = triton_gpu.extract_slice %[[NEXT_A_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n+// CHECK:   %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:   scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n@@ -101,20 +100,19 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index,\n // CHECK:   %[[A1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n // CHECK:   %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n-// CHECK:   %[[A0:.*]] = tensor.extract_slice %[[A1BUFFER]][0, 0, 0]\n-// CHECK:   %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n+// CHECK:   %[[A0:.*]] = triton_gpu.extract_slice %[[A1BUFFER]][0, 0, 0]\n+// CHECK:   %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK:   scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:     %[[arg_a0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_a0]]\n // CHECK:     %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n // CHECK:     tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_INT:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.index_cast %[[EXTRACT_INT]] : i32 to index\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:     %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:     %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:     triton_gpu.async_wait {num = 2 : i32}\n-// CHECK:   %[[NEXT_A:.*]] = tensor.extract_slice %[[NEXT_A_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n-// CHECK:   %[[NEXT_B:.*]] = tensor.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n+// CHECK:   %[[NEXT_A:.*]] = triton_gpu.extract_slice %[[NEXT_A_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n+// CHECK:   %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n@@ -170,16 +168,15 @@ func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n // CHECK: %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]]\n // CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n // CHECK: triton_gpu.async_wait {num = 1 : i32}\n-// CHECK: %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n+// CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:   %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n // CHECK:   tt.dot {{.*}}, %[[arg_b0_dot_op]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_INT:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.index_cast %[[EXTRACT_INT]] : i32 to index\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:   %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   triton_gpu.async_wait {num = 1 : i32}\n-// CHECK:   %[[NEXT_B:.*]] = tensor.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n+// CHECK:   %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:   scf.yield {{.*}}, {{.*}}, %[[NEXT_B_BUFFER]], %[[NEXT_B]], {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]"}, {"filename": "test/TritonGPU/prefetch.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -12,20 +12,20 @@\n \n \n // CHECK: func.func @matmul_loop\n-// CHECK-DAG: %[[A0_PREFETCH_SMEM:.*]] = tensor.extract_slice %[[A0:.*]][0, 0] [128, 16]\n+// CHECK-DAG: %[[A0_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice %[[A0:.*]][0, 0] [128, 16]\n // CHECK-DAG: %[[A0_PREFETCH:.*]] = triton_gpu.convert_layout %[[A0_PREFETCH_SMEM]]\n-// CHECK-DAG: %[[B0_PREFETCH_SMEM:.*]] = tensor.extract_slice %[[B0:.*]][0, 0] [16, 128]\n+// CHECK-DAG: %[[B0_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice %[[B0:.*]][0, 0] [16, 128]\n // CHECK-DAG: %[[B0_PREFETCH:.*]] = triton_gpu.convert_layout %[[B0_PREFETCH_SMEM]]\n // CHECK:     scf.for {{.*}} iter_args({{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, %[[a0_prefetch:.*]] = %[[A0_PREFETCH]], %[[b0_prefetch:.*]] = %[[B0_PREFETCH]]\n-// CHECK-DAG:   %[[A_REM_SMEM:.*]] = tensor.extract_slice %[[arg_a0]][0, 16] [128, 16]\n+// CHECK-DAG:   %[[A_REM_SMEM:.*]] = triton_gpu.extract_slice %[[arg_a0]][0, 16] [128, 16]\n // CHECK-DAG:   %[[A_REM:.*]] = triton_gpu.convert_layout %[[A_REM_SMEM]]\n-// CHECK-DAG:   %[[B_REM_SMEM:.*]] = tensor.extract_slice %[[arg_b0]][16, 0] [16, 128]\n+// CHECK-DAG:   %[[B_REM_SMEM:.*]] = triton_gpu.extract_slice %[[arg_b0]][16, 0] [16, 128]\n // CHECK-DAG:   %[[B_REM:.*]] = triton_gpu.convert_layout %[[B_REM_SMEM]]\n // CHECK:       %[[D_FIRST:.*]] = tt.dot %[[a0_prefetch]], %[[b0_prefetch:.*]], {{.*}}\n // CHECK:       tt.dot %[[A_REM]], %[[B_REM]], %[[D_FIRST:.*]]\n-// CHECK-DAG:   %[[NEXT_A_PREFETCH_SMEM:.*]] = tensor.extract_slice {{.*}}[0, 0] [128, 16]\n+// CHECK-DAG:   %[[NEXT_A_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice {{.*}}[0, 0] [128, 16]\n // CHECK-DAG:   %[[NEXT_A_PREFETCH:.*]] = triton_gpu.convert_layout %[[NEXT_A_PREFETCH_SMEM]]\n-// CHECK-DAG:   %[[NEXT_B_PREFETCH_SMEM:.*]] = tensor.extract_slice {{.*}}[0, 0] [16, 128]\n+// CHECK-DAG:   %[[NEXT_B_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice {{.*}}[0, 0] [16, 128]\n // CHECK-DAG:   %[[NEXT_B_PREFETCH:.*]] = triton_gpu.convert_layout %[[NEXT_B_PREFETCH_SMEM]]\n // CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_PREFETCH]], %[[NEXT_B_PREFETCH]]\n func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {"}]