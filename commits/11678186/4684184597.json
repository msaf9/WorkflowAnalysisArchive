[{"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -102,7 +102,7 @@ def test_matmul(M, N, K, dtype_str):\n     ms = triton.testing.do_bench(fn, warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    print()\n+    print_perf(ms, cur_gpu_util, ref_gpu_util)\n     triton.testing.assert_close(cur_gpu_util, ref_gpu_util, return_mode=\"min\", atol=0.01, rtol=0.05)\n \n \n@@ -159,6 +159,7 @@ def test_elementwise(N):\n     ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=500)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n+    print_perf(ms, cur_gpu_util, ref_gpu_util)\n     triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n #######################\n@@ -207,4 +208,5 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n+    print_perf(ms, cur_gpu_util, ref_gpu_util)\n     triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)"}]