[{"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 15, "deletions": 2, "changes": 17, "file_content_changes": "@@ -238,7 +238,9 @@ updateDotEncodingLayout(SmallVector<ConvertLayoutOp> &convertsToDotEncoding,\n                                 convertsToDotEncoding.end());\n   // Collect all the operations where the type needs to be propagated.\n   for (auto cvt : convertsToDotEncoding) {\n-    auto filter = [&](Operation *op) {\n+    auto forwardFilter = [&](Operation *op) {\n+      if (op == cvt.getOperation())\n+        return true;\n       for (Value operand : op->getOperands()) {\n         auto tensorType = operand.getType().dyn_cast<RankedTensorType>();\n         if (tensorType &&\n@@ -247,7 +249,18 @@ updateDotEncodingLayout(SmallVector<ConvertLayoutOp> &convertsToDotEncoding,\n       }\n       return false;\n     };\n-    mlir::getForwardSlice(cvt.getResult(), &slices, {filter});\n+    auto backwardFilter = [&](Operation *op) {\n+      for (Value results : op->getResults()) {\n+        auto tensorType = results.getType().dyn_cast<RankedTensorType>();\n+        if (tensorType &&\n+            tensorType.getEncoding().isa<DotOperandEncodingAttr>())\n+          return true;\n+      }\n+      return false;\n+    };\n+    SetVector<Operation *> opSlice =\n+        getSlice(cvt.getOperation(), {backwardFilter}, {forwardFilter});\n+    slices.insert(opSlice.begin(), opSlice.end());\n   }\n   // Apply the type change by walking ops in topological order.\n   slices = mlir::topologicalSort(slices);"}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 39, "deletions": 0, "changes": 39, "file_content_changes": "@@ -184,3 +184,42 @@ tt.func @push_convert_both_operands(\n }\n \n }\n+\n+// -----\n+\n+#blockedA = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#blockedB = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n+// CHECK: #[[BA:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+// CHECK: #[[BB:.*]] = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n+// CHECK: #[[MMA:.*]] = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n+\n+// CHECK: tt.func @update_kwidth_slice\n+// CHECK: %[[CST:.+]] = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[ALOAD:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #[[BA]]>\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]] : (tensor<16x16xf16, #[[BA]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[BLOAD:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #[[BB]]>\n+// CHECK: %[[BCVT:.*]] = triton_gpu.convert_layout %[[BLOAD]] : (tensor<16x16xf16, #[[BB]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[AEXT:.*]] = arith.extf %[[ACVT]] : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>> to tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[BEXT:.*]] = arith.extf %[[BCVT]] : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>> to tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[ADD:.+]] = arith.addf %[[BEXT]], %[[CST]] : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: tt.dot %[[AEXT]], %[[ADD]], %{{.*}} {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>> -> tensor<16x16xf32, #mma>\n+tt.func @update_kwidth_slice(\n+                   %pa: tensor<16x16x!tt.ptr<f16>, #blockedA> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #blockedB> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %c: tensor<16x16xf32, #mma>) -> tensor<16x16xf32, #mma>{\n+  %cst = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #blockedB>\n+  %a = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blockedA>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blockedB>\n+  %ae = arith.extf %a : tensor<16x16xf16, #blockedA> to tensor<16x16xf32, #blockedA>\n+  %be = arith.extf %b : tensor<16x16xf16, #blockedB> to tensor<16x16xf32, #blockedB>\n+  %add = arith.addf %be, %cst : tensor<16x16xf32, #blockedB>\n+  %al = triton_gpu.convert_layout %ae : (tensor<16x16xf32, #blockedA>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n+  %bl = triton_gpu.convert_layout %add : (tensor<16x16xf32, #blockedB>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+  %r = tt.dot %al, %bl, %c {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>> -> tensor<16x16xf32, #mma>\n+  tt.return %r : tensor<16x16xf32, #mma>\n+}\n+\n+}"}]