[{"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 3, "deletions": 11, "changes": 14, "file_content_changes": "@@ -43,7 +43,6 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n       return 0;\n     }\n   };\n-  // blocked -> blocked\n   if (srcLayout.isa<BlockedEncodingAttr>() &&\n       dstLayout.isa<BlockedEncodingAttr>()) {\n     auto srcBlockedLayout = srcLayout.cast<BlockedEncodingAttr>();\n@@ -66,14 +65,6 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n     }\n     paddedRepShape[outOrd[0]] += pad;\n   }\n-  // blocked -> shared\n-  if (srcLayout.isa<BlockedEncodingAttr>() &&\n-      dstLayout.isa<SharedEncodingAttr>()) {\n-    auto sharedLayout = dstLayout.cast<SharedEncodingAttr>();\n-    for (int v : dstTy.getShape())\n-      paddedRepShape.push_back(v);\n-  }\n-\n   return paddedRepShape;\n }\n \n@@ -140,8 +131,9 @@ class AllocationAnalysis {\n       auto dstTy = cvtLayout.result().getType().cast<RankedTensorType>();\n       auto srcEncoding = srcTy.getEncoding();\n       auto dstEncoding = dstTy.getEncoding();\n-      if (srcEncoding.isa<SharedEncodingAttr>()) {\n-        // only block->block and block->shared is supported now\n+      if (srcEncoding.isa<SharedEncodingAttr>() ||\n+          dstEncoding.isa<SharedEncodingAttr>()) {\n+        // Only blocked -> blocked conversion requires for scratch allocation\n         return;\n       }\n       // ConvertLayoutOp with both input/output non-shared_layout"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 20, "changes": 22, "file_content_changes": "@@ -1449,13 +1449,6 @@ struct ConvertLayoutOpConversion\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::gpu::ConvertLayoutOp>::ConvertTritonGPUOpToLLVMPattern;\n \n-  ConvertLayoutOpConversion(LLVMTypeConverter &converter,\n-                            const Allocation *allocation, Value smem,\n-                            PatternBenefit benefit)\n-      : ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp>(converter,\n-                                                                      benefit),\n-        allocation(allocation), smem(smem) {}\n-\n   LogicalResult\n   matchAndRewrite(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n@@ -1469,8 +1462,6 @@ struct ConvertLayoutOpConversion\n     if ((!srcLayout.isa<BlockedEncodingAttr>()) ||\n         (!dstLayout.isa<BlockedEncodingAttr>())) {\n       // TODO: not implemented\n-      llvm::errs()\n-          << \"convert_layout except for blocked -> blocked is not implemented\";\n       return failure();\n     }\n     auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n@@ -1656,9 +1647,6 @@ struct ConvertLayoutOpConversion\n       }\n     }\n   }\n-\n-  const Allocation *allocation;\n-  Value smem;\n };\n \n /// ====================== dot codegen begin ==========================\n@@ -1995,11 +1983,8 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     NOT_APPLICABLE,\n   };\n \n-  explicit DotOpConversion(LLVMTypeConverter &typeConverter,\n-                           const Allocation *allocation, Value smem,\n-                           PatternBenefit benefit = 1)\n-      : ConvertTritonGPUOpToLLVMPattern(typeConverter, benefit),\n-        allocation(allocation), smem(smem) {}\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::DotOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   LogicalResult\n   matchAndRewrite(triton::DotOp op, OpAdaptor adaptor,\n@@ -2064,9 +2049,6 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     assert(false && \"Not implemented yet.\");\n     return failure();\n   }\n-\n-  const Allocation *allocation;\n-  Value smem;\n };\n \n struct DotOpConversionHelper {"}]