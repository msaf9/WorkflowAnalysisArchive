[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -21,7 +21,7 @@ jobs:\n \n       - name: Clear cache\n         run: |\n-          rm -r /tmp/triton/\n+          rm -r ~/.triton/\n         continue-on-error: true\n \n       - name: Install Triton"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 5, "changes": 5, "file_content_changes": "@@ -3,11 +3,6 @@ include(ExternalProject)\n \n set(CMAKE_CXX_STANDARD 17)\n \n-if(NOT TRITON_LLVM_BUILD_DIR)\n-    set(TRITON_LLVM_BUILD_DIR ${CMAKE_BINARY_DIR})\n-endif()\n-\n-\n project(triton)\n include(CTest)\n if(NOT WIN32)"}, {"filename": "docs/python-api/triton.language.rst", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -106,9 +106,13 @@ Atomic Ops\n     :nosignatures:\n \n     atomic_cas\n+    atomic_xchg\n     atomic_add\n     atomic_max\n     atomic_min\n+    atomic_and\n+    atomic_or\n+    atomic_xor\n \n \n Comparison ops"}, {"filename": "include/triton/codegen/analysis/layout.h", "status": "modified", "additions": 25, "deletions": 2, "changes": 27, "file_content_changes": "@@ -258,7 +258,8 @@ class shared_layout: public data_layout {\n                 const std::vector<unsigned>& shapes,\n                 const std::vector<ir::value *> &values_,\n                 ir::type *ty,\n-                analysis::align* align, target *tgt);\n+                analysis::align* align, target *tgt,\n+                bool is_tmp = false);\n   void accept(layout_visitor* vst) { vst->visit_layout_shared(this); }\n   // accessors\n   size_t get_size()                         { return size_; }\n@@ -276,6 +277,7 @@ class shared_layout: public data_layout {\n   int  get_mma_strided()                    { return mma_strided_; }\n   bool allow_swizzle() const                { return allow_swizzle_; }\n   data_layout* get_arg_layout()             { return arg_layout_; }\n+  bool is_tmp() const                       { return is_tmp_; }\n \n private:\n   size_t size_;\n@@ -290,6 +292,7 @@ class shared_layout: public data_layout {\n   int mma_strided_;\n   bool allow_swizzle_ = true;\n   target *tgt_;\n+  bool is_tmp_;\n };\n \n \n@@ -308,21 +311,40 @@ class layouts {\n \n   void create(size_t id, const std::vector<ir::value*>& values);\n \n-public:\n+  void create_tmp_layout(size_t id, data_layout* arg,\n+                         const std::vector<int>& axes,\n+                         const std::vector<unsigned>& shape,\n+                         ir::instruction* i,\n+                         bool is_index = false);\n+\n+ public:\n   // constructor\n   layouts(analysis::axes *axes, analysis::align *align, size_t num_warps, target* tgt);\n \n   // accessors\n   unsigned layout_of(ir::value *value) const                  { return groups_.at(value); }\n   bool has(ir::value* value) const { return groups_.find(value) != groups_.end(); }\n+  bool has(size_t id)                                         { return layouts_.find(id) != layouts_.end(); }\n   const std::vector<ir::value*>& values_of(unsigned id) const { return values_.at(id); }\n   size_t num_layouts() const                                  { return values_.size();}\n   data_layout* get(size_t id)                                 { return layouts_.at(id); }\n   data_layout* get(ir::value *v)                              { return get(layout_of(v));}\n   std::map<size_t, data_layout*> &get_all()                   { return layouts_; }\n   bool has_tmp(ir::value* i)                                  { return tmp_.find(i) != tmp_.end(); }\n   int tmp(ir::value* i)                                       { return tmp_.at(i);}\n+  int has_tmp_index(ir::value* i)                             { return tmp_index_.find(i) != tmp_index_.end(); }\n+  int tmp_index(ir::value* i)                                 { return tmp_index_.at(i);}\n   void copy(ir::value* dst, ir::value* src)                   { groups_[dst] = groups_[src]; }\n+\n+  // layout checkers\n+  bool is_scanline(ir::instruction* i);\n+\n+  bool is_coalesced_scanline(ir::instruction* i);\n+\n+  bool is_mma(ir::instruction* i);\n+\n+  bool is_a100_mma(ir::instruction* i);\n+\n   // execution\n   void run(ir::module &mod);\n \n@@ -336,6 +358,7 @@ class layouts {\n   std::map<size_t, std::vector<ir::value*>> values_;\n   std::map<size_t, data_layout*> layouts_;\n   std::map<ir::value*, size_t> tmp_;\n+  std::map<ir::value*, size_t> tmp_index_;\n };\n \n }"}, {"filename": "include/triton/codegen/selection/generator.h", "status": "modified", "additions": 10, "deletions": 4, "changes": 14, "file_content_changes": "@@ -119,8 +119,15 @@ class generator: public ir::visitor, public analysis::layout_visitor {\n   llvm::Attribute cvt(ir::attribute attr);\n   void packed_type(ir::value* i);\n   void forward_declare(ir::function* fn);\n+  Value *cast_shared_layout_ptr(analysis::data_layout *layout, Type *ty);\n \n-public:\n+ private:\n+  typedef std::function<void(\n+      std::pair<Value *, Value *> &acc, std::function<Value *()> load_value_fn,\n+      std::function<Value *()> load_index_fn, bool is_first)>\n+      acc_fn_t;\n+\n+ public:\n   generator(analysis::axes *a_axes,\n             analysis::layouts *layouts,\n             analysis::align *alignment,\n@@ -177,9 +184,8 @@ class generator: public ir::visitor, public analysis::layout_visitor {\n   void visit_trans_inst(ir::trans_inst*);\n   void visit_sqrt_inst(ir::sqrt_inst*);\n   Value* shfl_sync(Value* acc, int32_t i);\n-  void visit_reduce1d_inst(ir::reduce_inst*, std::function<Value*(Value*,Value*)>, Value*);\n-  void visit_reducend_inst_fast(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral);\n-  void visit_reducend_inst(ir::reduce_inst*, std::function<Value*(Value*,Value*)>, Value*);\n+  void visit_reducend_inst_fast(ir::reduce_inst* x, acc_fn_t do_acc, Value *neutral);\n+  void visit_reducend_inst(ir::reduce_inst* x, acc_fn_t do_acc, Value *neutral);\n   void visit_reduce_inst(ir::reduce_inst*);\n   void visit_select_inst(ir::select_inst*);\n   void visit_layout_convert(ir::value *out, ir::value *in);"}, {"filename": "include/triton/codegen/transform/coalesce.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -32,11 +32,12 @@ class coalesce {\n   ir::value* rematerialize(ir::value *v, ir::builder& builder, std::map<ir::value*, ir::value*>& seen);\n \n public:\n-  coalesce(analysis::align* align, triton::codegen::analysis::layouts *layouts);\n+  coalesce(analysis::align* align, triton::codegen::analysis::layouts *layouts, bool has_sm80);\n   triton::ir::value *simplify(ir::instruction* i, triton::ir::builder &builder);\n   void run(ir::module &mod);\n \n private:\n+  bool has_sm80_;\n   analysis::align* align_;\n   analysis::layouts* layout_;\n };"}, {"filename": "include/triton/codegen/transform/cts.h", "status": "modified", "additions": 11, "deletions": 3, "changes": 14, "file_content_changes": "@@ -15,18 +15,26 @@ namespace ir {\n }\n \n namespace codegen{\n+\n+namespace analysis{\n+class layouts;\n+}\n+\n namespace transform{\n \n class cts {\n private:\n-  void add_copy(ir::instruction *parent, ir::value *x, ir::builder &builder, bool to_shared);\n+  bool is_shmem_op(ir::instruction* i, int op);\n+  bool is_shmem_res(ir::value* i);\n+void add_copy(ir::instruction *parent, ir::value *x, ir::builder &builder, bool to_shared, std::map<ir::value*,ir::value*>& copies);\n \n public:\n-  cts(bool use_async = false): use_async_(use_async) {}\n+  cts(analysis::layouts* layouts, bool has_sm80 = false): layouts_(layouts), has_sm80_(has_sm80) {}\n   void run(ir::module &mod);\n \n private:\n-  bool use_async_;\n+  bool has_sm80_;\n+  analysis::layouts* layouts_;\n };\n \n }"}, {"filename": "include/triton/ir/builder.h", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -142,9 +142,9 @@ class builder{\n   value *create_or(value *lhs, value *rhs);\n   // Input/Output\n   value *create_load(value *arg, load_inst::CACHE_MODIFIER cache, load_inst::EVICTION_POLICY eviction, bool is_volatile);\n-  value *create_store(value *ptr, value *val);\n+  value *create_store(value *ptr, value *val, store_inst::EVICTION_POLICY eviction);\n   value *create_masked_load(value *arg, value *mask, value *false_value, load_inst::CACHE_MODIFIER cache, load_inst::EVICTION_POLICY eviction, bool is_volatile);\n-  value *create_masked_store(value *ptr, value *val, value *mask);\n+  value *create_masked_store(value *ptr, value *val, value *mask, store_inst::EVICTION_POLICY eviction);\n   // Struct instructions\n   value *create_insert_value(value* val, value *elt, size_t idx);\n   value *create_extract_value(value* val, size_t idx);\n@@ -182,7 +182,7 @@ class builder{\n   value *create_cos(value* arg);\n   value *create_sin(value* arg);\n   value *create_log(value* arg);\n-  value *create_dot(value *A, value *B, value *C, bool allow_tf32);\n+  value *create_dot(value *A, value *B, value *C, bool trans_a, bool trans_b, bool allow_tf32);\n   value *create_trans(value *A, const std::vector<int> &perm = {});\n   value *create_sqrt(value *A);\n   value *create_reduce(value *A, reduce_inst::op_t op, unsigned axis);"}, {"filename": "include/triton/ir/function.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -112,7 +112,7 @@ class function: public global_object{\n   static function *create(function_type *ty, linkage_types_t linkage,\n                           const std::string &name, module *mod);\n   // blocks\n-  const blocks_t &blocks() { return blocks_; }\n+        blocks_t &blocks() { return blocks_; }\n   const blocks_t &blocks() const { return blocks_; }\n   void insert_block(basic_block* block, basic_block *next = nullptr);\n "}, {"filename": "include/triton/ir/instructions.h", "status": "modified", "additions": 41, "deletions": 20, "changes": 61, "file_content_changes": "@@ -435,13 +435,31 @@ class getelementptr_inst: public instruction {\n //===----------------------------------------------------------------------===//\n \n class io_inst: public instruction {\n+public:\n+\n+  enum EVICTION_POLICY : uint32_t {\n+    NORMAL=0,\n+    EVICT_FIRST,\n+    EVICT_LAST,\n+  };\n+\n protected:\n-  io_inst(type *ty, value_id_t id, unsigned num_ops,\n+  io_inst(type *ty, value_id_t id, unsigned num_ops, EVICTION_POLICY eviction,\n           const std::string &name = \"\", instruction *next = nullptr);\n \n+  std::string get_eviction_policy_repr() const {\n+    if (eviction_ == EVICT_FIRST) return \".L1::evict_first\";\n+    if (eviction_ == EVICT_LAST) return \".L2::evict_last\";\n+    return \"\";\n+  }\n+\n public:\n   // accessors\n   value *get_pointer_operand() { return get_operand(0); }\n+  EVICTION_POLICY get_eviction_policy() const { return eviction_; }\n+\n+protected:\n+  EVICTION_POLICY eviction_;\n };\n \n // load\n@@ -453,14 +471,8 @@ class load_inst: public io_inst {\n     CG,\n   };\n \n-  enum EVICTION_POLICY : uint32_t {\n-    NORMAL=0,\n-    EVICT_FIRST,\n-    EVICT_LAST,\n-  };\n \n   CACHE_MODIFIER get_cache_modifier() const { return cache_; }\n-  EVICTION_POLICY get_eviction_policy() const { return eviction_; }\n   bool get_is_volatile() const { return is_volatile_; }\n \n protected:\n@@ -472,12 +484,6 @@ class load_inst: public io_inst {\n     if (cache_ == CG) return \".cg\";\n     return \"\"; \n   }\n-  std::string get_eviction_policy_repr() const {\n-    if (eviction_ == EVICT_FIRST) return \".L1::evict_first\";\n-    if (eviction_ == EVICT_LAST) return \".L2::evict_last\";\n-    return \"\";\n-  }\n-  EVICTION_POLICY eviction_;\n   CACHE_MODIFIER cache_;\n \n   std::string get_volatile_repr() {\n@@ -553,7 +559,7 @@ class masked_load_async_inst: public load_inst {\n // store\n class store_inst: public io_inst {\n protected:\n-  store_inst(value *ptr, value_id_t id, unsigned num_ops,\n+  store_inst(value *ptr, value_id_t id, unsigned num_ops, EVICTION_POLICY eviction,\n             const std::string &name = \"\", instruction *next = nullptr);\n \n public:\n@@ -564,11 +570,11 @@ class store_inst: public io_inst {\n class unmasked_store_inst: public store_inst{\n private:\n   std::string repr_impl() const { return \"unmasked_store\"; }\n-  unmasked_store_inst(value *ptr, value *v, const std::string &name, instruction *next);\n+  unmasked_store_inst(value *ptr, value *v, EVICTION_POLICY eviction, const std::string &name, instruction *next);\n \n public:\n   // factory method\n-  static unmasked_store_inst* create(value* ptr, value *v,\n+  static unmasked_store_inst* create(value* ptr, value *v, EVICTION_POLICY eviction,\n                                     const std::string &name = \"\",\n                                     instruction *next = nullptr);\n   _TRITON_DEFINE_CLONE(unmasked_store_inst)\n@@ -578,14 +584,14 @@ class unmasked_store_inst: public store_inst{\n class masked_store_inst: public store_inst{\n private:\n   std::string repr_impl() const { return \"masked_store\"; }\n-  masked_store_inst(value *ptr, value *v, value *mask,\n+  masked_store_inst(value *ptr, value *v, value *mask, EVICTION_POLICY eviction,\n                     const std::string &name, instruction *next);\n \n public:\n   // accessors\n   value *get_mask_operand() { return get_operand(2); }\n   // factory method\n-  static masked_store_inst* create(value *ptr, value *v, value *mask,\n+  static masked_store_inst* create(value *ptr, value *v, value *mask, EVICTION_POLICY eviction,\n                                    const std::string &name = \"\",\n                                    instruction *next = nullptr);\n   _TRITON_DEFINE_CLONE(masked_store_inst)\n@@ -755,6 +761,8 @@ class get_num_programs_inst: public builtin_inst {\n class atomic_inst: public io_inst {\n public:\n   using io_inst::io_inst;\n+  atomic_inst(type *ty, value_id_t id, unsigned num_ops, const std::string &name, instruction *next):\n+    io_inst(ty, id, num_ops, NORMAL, name, next) {}\n };\n \n class atomic_rmw_inst: public atomic_inst {\n@@ -856,6 +864,8 @@ class dot_inst: public builtin_inst {\n   bool is_prefetched() const { return is_prefetched_; }\n   void set_prefetched(bool is_prefetched) { is_prefetched_ = is_prefetched; }\n   bool allow_tf32() const { return allow_tf32_; }\n+  bool is_trans_a() const { return AT_ == Trans; }\n+  bool is_trans_b() const { return BT_ == Trans; }\n \n public:\n   static instruction *create(value *A, value *B, value *C, bool AT, bool BT, bool allow_tf32, const std::string &name = \"\", instruction *next = nullptr);\n@@ -872,6 +882,8 @@ class dot_inst: public builtin_inst {\n   DataType C_type_ = DataType::FP32;\n   DataType A_type_ = DataType::FP16;\n   DataType B_type_ = DataType::FP16;\n+  TransT AT_;\n+  TransT BT_;\n };\n \n //class outer_inst: public builtin_inst {\n@@ -914,7 +926,9 @@ class reduce_inst: public builtin_inst {\n public:\n   enum op_t{\n     ADD, SUB, MAX, MIN, UMAX, UMIN,\n+    ARGMAX, ARGMIN, ARGUMAX, ARGUMIN,\n     FADD, FSUB, FMAX, FMIN,\n+    ARGFMAX, ARGFMIN,\n     XOR\n   };\n \n@@ -932,12 +946,19 @@ class reduce_inst: public builtin_inst {\n   static instruction* create(value *arg, op_t op, unsigned axis, const std::string &name = \"\", instruction *next = nullptr);\n   unsigned get_axis() const { return axis_; }\n   op_t get_op() const { return op_; }\n+  bool with_index() const {\n+    return with_index_ops_.find(op_) != with_index_ops_.end();\n+  }\n \n private:\n-  unsigned axis_;\n-  op_t op_;\n+ const static inline std::set<op_t> with_index_ops_ = {\n+     op_t::ARGMAX,  op_t::ARGMIN,  op_t::ARGUMAX,\n+     op_t::ARGUMIN, op_t::ARGFMAX, op_t::ARGFMIN};\n+ unsigned axis_;\n+ op_t op_;\n };\n \n+\n class select_inst: public builtin_inst {\n private:\n   select_inst(value *pred, value *if_value, value *else_value, const std::string& name, instruction* next);"}, {"filename": "include/triton/ir/utils.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -22,6 +22,7 @@ class cfg {\n };\n \n void for_each_instruction(ir::module& mod, const std::function<void(triton::ir::instruction*)> &fn);\n+void for_each_instruction_backward(module &mod, const std::function<void (instruction *)> &do_work);\n void for_each_value(ir::module& mod, const std::function<void(triton::ir::value *)> &fn);\n \n }"}, {"filename": "lib/codegen/analysis/allocation.cc", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -92,8 +92,10 @@ void allocation::run(ir::module &mod) {\n   }\n   // Save maximum size of induced memory space\n   allocated_size_ = 0;\n-  for(shared_layout* x: V)\n+  for(shared_layout* x: V){\n     allocated_size_ = std::max<size_t>(allocated_size_, starts[x] + x->get_size());\n+    // std::cout << \"start: \" << starts[x] << \" | end: \" << starts[x] + x->get_size() << std::endl;\n+  }\n }\n \n }"}, {"filename": "lib/codegen/analysis/layout.cc", "status": "modified", "additions": 92, "deletions": 31, "changes": 123, "file_content_changes": "@@ -212,11 +212,9 @@ mma_layout::mma_layout(size_t num_warps,\n     order_ = {0, 1};\n   }\n   else{\n-    // fpw_ = {1, 1, 1};\n     spw_ = mma_instr_shape_.at(tensor_core_type_); // e.g., {16, 8, 16} for f32.f16.f16.f32\n     contig_per_thread_ = {1, 2};\n     order_ = {1, 0};\n-    // rep_ = {2,  2, 1};\n   }\n \n   /* warps per tile */\n@@ -233,24 +231,45 @@ mma_layout::mma_layout(size_t num_warps,\n     }while(wpt_nm1 != wpt_);\n   } else {\n     bool changed = false;\n-    do {\n-      changed = false;\n-      if (wpt_[0] * wpt_[1] * wpt_[2] >= num_warps)\n-        break;\n-      if (shape_[0] / spw_[0] / wpt_[0] >= shape_[1] / (spw_[1]*2) / wpt_[1]) {\n-        if (wpt_[0] < shape_[0] / spw_[0]) {\n-          wpt_[0] *= 2;\n-          changed = true;\n-        }\n-      } else {\n-        if (wpt_[1] < shape_[1] / (spw_[1]*2)) {\n-          wpt_[1] *= 2;\n-          changed = true;\n+    // try to have a warp own entire rows of the output\n+    // this makes it easier to fuse multiple mmas by fusing\n+    // registers\n+    bool one_warp_per_row = false;\n+    for(ir::value* v: values)\n+    for(ir::user* u: v->get_users()){\n+      auto* dot = dynamic_cast<ir::dot_inst*>(u);\n+      auto* cts = dynamic_cast<ir::copy_to_shared_inst*>(u);\n+      if((dot && dot->get_operand(2)!=v) || !layout_a->to_shared() || cts)\n+        one_warp_per_row = shape[0] / spw_[0] >= num_warps;\n+    }\n+    // std::cout << one_warp_per_row << std::endl;\n+\n+    if(one_warp_per_row){\n+      wpt_[1] = 1;\n+      wpt_[0] = num_warps;\n+    }\n+    else{\n+      do {\n+        changed = false;\n+        if (wpt_[0] * wpt_[1] * wpt_[2] >= num_warps)\n+          break;\n+        if (shape_[0] / spw_[0] / wpt_[0] >= shape_[1] / (spw_[1]*2) / wpt_[1]) {\n+          if (wpt_[0] < shape_[0] / spw_[0]) {\n+            wpt_[0] *= 2;\n+            changed = true;\n+          }\n+        } else {\n+          if (wpt_[1] < shape_[1] / (spw_[1]*2)) {\n+            wpt_[1] *= 2;\n+            changed = true;\n+          }\n         }\n-      }\n-    } while (changed);\n+      } while(changed);\n+    }\n   }\n \n+  // std::cout << wpt_[0] << \" \" << wpt_[1] << std::endl;\n+\n   /* shape per block */\n   shape_per_cta_ = {spw_[0]*wpt_[0], spw_[1]*wpt_[1], 1};\n }\n@@ -430,8 +449,8 @@ shared_layout::shared_layout(data_layout *arg,\n                                  const std::vector<unsigned>& shape,\n                                  const std::vector<ir::value *> &values,\n                                  ir::type *ty,\n-                                 analysis::align* align, target *tgt)\n-    : data_layout(SHARED, axes, shape, values, align), ty_(ty), tgt_(tgt) {\n+                                 analysis::align* align, target *tgt, bool is_tmp)\n+    : data_layout(SHARED, axes, shape, values, align), ty_(ty), tgt_(tgt), is_tmp_(is_tmp){\n \n   size_ = 0;\n   arg_layout_ = arg;\n@@ -588,6 +607,45 @@ void layouts::create(size_t id, const std::vector<ir::value*>& values) {\n   }\n }\n \n+// layout checkers\n+bool layouts::is_scanline(ir::instruction *i) {\n+  return this->get(i->get_operand(0))->to_scanline() != nullptr;\n+}\n+\n+bool layouts::is_coalesced_scanline(ir::instruction *i) {\n+  if (auto *red = dynamic_cast<ir::reduce_inst *>(i)) {\n+    auto *scanline = this->get(i->get_operand(0))->to_scanline();\n+    return scanline && scanline->get_order()[0] == red->get_axis();\n+  }\n+  return false;\n+}\n+\n+bool layouts::is_mma(ir::instruction *i) {\n+  return this->get(i->get_operand(0))->to_mma() != nullptr;\n+}\n+\n+bool layouts::is_a100_mma(ir::instruction *i) {\n+  if (auto *red = dynamic_cast<ir::reduce_inst *>(i)) {\n+    return is_mma(red) && (tgt_->as_nvidia()->sm() >= 80) &&\n+           (red->get_axis() == 1);\n+  }\n+  return false;\n+}\n+\n+void layouts::create_tmp_layout(size_t id, data_layout *arg,\n+                                const std::vector<int> &axes,\n+                                const std::vector<unsigned> &shape,\n+                                ir::instruction *i, bool is_index) {\n+  ir::type *ty = is_index ? ir::type::get_int32_ty(i->get_type()->get_context())\n+                          : i->get_type()->get_scalar_ty();\n+  layouts_[id] = new shared_layout(arg, axes, shape, {i}, ty, align_, tgt_, true);\n+  if (is_index) {\n+    tmp_index_[i] = id;\n+  } else {\n+    tmp_[i] = id;\n+  }\n+}\n+\n void layouts::run(ir::module &mod) {\n   // make graph\n   graph_.clear();\n@@ -612,22 +670,26 @@ void layouts::run(ir::module &mod) {\n //    std::cout << \"layout: \" << std::endl;\n //    i->print(std::cout);\n     if(auto *red = dynamic_cast<ir::reduce_inst*>(i)) {\n-      id++;\n       ir::value *arg = red->get_operand(0);\n-      unsigned axis = red->get_axis();\n+      distributed_layout *layout =\n+          dynamic_cast<analysis::distributed_layout *>(get(arg));\n       // shape\n       auto shapes = arg->get_type()->get_block_shapes();\n-      distributed_layout* layout = dynamic_cast<analysis::distributed_layout*>(get(arg));\n-      shapes[axis] = layout->shape_per_cta(axis) / layout->contig_per_thread(axis);\n-      \n+      unsigned axis = red->get_axis();\n+      shapes[axis] =\n+          layout->shape_per_cta(axis) / layout->contig_per_thread(axis);\n       // create layout\n-      layouts_[id] = new shared_layout(layout, axes_->get(arg), shapes, {red}, red->get_type()->get_scalar_ty(), align_, tgt_);\n-      tmp_[red] = id;\n+      id++;\n+      create_tmp_layout(id, layout, axes_->get(arg), shapes, red);\n+\n+      if (red->with_index()) {\n+        id++;\n+        create_tmp_layout(id, layout, axes_->get(arg), shapes, red, true);\n+      }\n     }\n     if(auto *val = dynamic_cast<ir::cvt_layout_inst*>(i)){\n       distributed_layout* out_layout = dynamic_cast<distributed_layout*>(get(val));\n       distributed_layout* in_layout = dynamic_cast<distributed_layout*>(get(i->get_operand(0)));\n-      id++;\n       size_t dim = val->get_type()->get_tile_rank();\n       ir::type::block_shapes_t shape(dim);\n       for(size_t k = 0; k < dim; k++){\n@@ -640,13 +702,12 @@ void layouts::run(ir::module &mod) {\n       int out_vec = out_layout->contig_per_thread(out_ord[0]);\n       int pad = std::max(in_vec, out_vec);\n       shape[out_ord[0]] += pad;\n-      layouts_[id] = new shared_layout(out_layout, axes_->get(val), shape, {val}, val->get_type()->get_scalar_ty(), align_, tgt_);\n-      tmp_[val] = id;\n+      id++;\n+      create_tmp_layout(id, out_layout, axes_->get(val), shape, val);\n     }\n     if(auto *atom = dynamic_cast<ir::atomic_inst*>(i)){\n       id++;\n-      layouts_[id] = new shared_layout(nullptr, {}, {1}, {atom}, atom->get_type()->get_scalar_ty(), align_, tgt_);\n-      tmp_[atom] = id;\n+      create_tmp_layout(id, nullptr, {}, {1}, atom);\n     }\n   });\n "}, {"filename": "lib/codegen/analysis/liveness.cc", "status": "modified", "additions": 90, "deletions": 25, "changes": 115, "file_content_changes": "@@ -14,43 +14,108 @@ namespace analysis{\n void liveness::run(ir::module &mod) {\n   intervals_.clear();\n \n+  std::map<ir::value*, std::set<shared_layout*>> layouts_map;\n+  for(auto &x: layouts_->get_all()){\n+    shared_layout* layout = x.second->to_shared();\n+    if(!layout || layout->is_tmp())\n+      continue;\n+    for(ir::value* v:layout->get_values()){\n+      layouts_map[v].insert(layout);\n+    }\n+  }\n+\n+\n+\n+  std::map<ir::user*, std::set<shared_layout*>> live_in;\n+  while(true){\n+    bool changed = false;\n+    ir::instruction* last_inst = nullptr;\n+    ir::for_each_instruction_backward(mod, [&](ir::instruction* i){\n+      // gen\n+      std::set<shared_layout*> gen;\n+      for(ir::value* v: i->ops())\n+      for(shared_layout* layout: layouts_map[v])\n+        gen.insert(layout);\n+      // kill\n+      std::set<shared_layout*> kill;\n+      for(shared_layout* layout: layouts_map[i])\n+        kill.insert(layout);\n+      // temporaries are handled separately\n+      if(layouts_->has_tmp(i)){\n+        gen.insert(layouts_->get(layouts_->tmp(i))->to_shared());\n+        kill.insert(layouts_->get(layouts_->tmp(i))->to_shared());\n+      }\n+      if(layouts_->has_tmp_index(i)){\n+        gen.insert(layouts_->get(layouts_->tmp_index(i))->to_shared());\n+        kill.insert(layouts_->get(layouts_->tmp_index(i))->to_shared());\n+      }\n+      // live-out\n+      std::set<shared_layout*> live_out;\n+      std::vector<ir::instruction*> succs = {last_inst};\n+      if(i == i->get_parent()->get_inst_list().back())\n+        for(ir::basic_block* succ: i->get_parent()->get_successors())\n+          succs.push_back(succ->get_inst_list().front());\n+      for(ir::instruction* succ: succs)\n+      for(shared_layout* layout: live_in[succ])\n+      if(!layout->is_tmp())\n+        live_out.insert(layout);\n+\n+      // new sets\n+      std::set<shared_layout*> live_out_minus_kill;\n+      std::set_difference(live_out.begin(), live_out.end(), kill.begin(), kill.end(), \n+                          std::inserter(live_out_minus_kill, live_out_minus_kill.end()));\n+      std::set<shared_layout*> new_live_in;\n+      std::set_union(gen.begin(), gen.end(), live_out_minus_kill.begin(), live_out_minus_kill.end(),\n+                      std::inserter(new_live_in, new_live_in.end()));\n+      \n+      changed = changed || (new_live_in != live_in[i]);\n+      live_in[i] = new_live_in;\n+      last_inst = i;\n+    });\n+    if(!changed)\n+      break;\n+  }\n+    \n+  // ir::for_each_instruction(mod, [&](ir::instruction* i){\n+  //   i->print(std::cout);\n+  //   std::cout << \" live_in: \" << live_in[i].size() << std::endl;\n+  // });\n+\n+\n+\n   // Assigns index to each instruction\n   std::map<ir::value*, slot_index> indices;\n-  for(ir::function *fn: mod.get_function_list()){\n-    slot_index index = 0;\n-    for(ir::basic_block *block: fn->blocks())\n-    for(ir::instruction *instr: block->get_inst_list()){\n+  slot_index index = 0;\n+  ir::for_each_instruction(mod, [&](ir::instruction* instr){\n       index += 1;\n       indices.insert({instr, index});\n-    }\n+  });\n+  \n+\n+  for(auto &x: layouts_->get_all()){\n+    shared_layout* layout = x.second->to_shared();\n+    if(layout)\n+      intervals_[layout] = segment{INT32_MAX, 0};\n   }\n \n-  // create live intervals\n+  for(auto& x: live_in)\n+  for(shared_layout* layout: x.second)\n+    intervals_[layout].start = std::min<int>(intervals_[layout].start, indices[x.first]);\n+\n+  for(auto& x: live_in)\n+  for(shared_layout* layout: x.second){\n+    intervals_[layout].end = std::max<int>(intervals_[layout].end, indices[x.first] + 1);\n+  }\n+\n+  \n   for(auto &x: layouts_->get_all()) {\n     shared_layout* layout = x.second->to_shared();\n     if(!layout)\n       continue;\n-    // users\n-    std::set<ir::user*> users;\n-    for(ir::value *v: layout->get_values()){\n-      for(ir::user *u: v->get_users())\n-        users.insert(u);\n-    }\n-    // compute intervals\n-    unsigned start = INT32_MAX;\n-    for(ir::value *v: layout->get_values())\n-      if(indices.find(v) != indices.end())\n-        start = std::min(start, indices.at(v));\n-    unsigned end = 0;\n-    for(ir::user *u: users)\n-      if(indices.find(u) != indices.end())\n-        end = std::max(end, indices.at(u));\n-    if(end == 0)\n-      end = start + 1;\n-    intervals_[layout] = segment{start, end};\n+    // std::cout << intervals_[layout].start << \" \" << intervals_[layout].end << std::endl;\n   }\n \n-\n+  \n \n }\n "}, {"filename": "lib/codegen/analysis/swizzle.cc", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -28,12 +28,15 @@ void swizzle::run(ir::module &) {\n       }\n       auto ord = layout->get_order();\n       scanline_layout* in_layout = dynamic_cast<scanline_layout*>(layout->get_arg_layout());\n-      if(!in_layout)\n-        continue;\n+      int per_phase = 1;\n       int dtsize = layout->get_type()->get_scalar_ty()->get_primitive_size_in_bits() / 8;\n+      if(in_layout)\n+        per_phase = std::max<int>(128 / (in_layout->mts(ord[0])*in_layout->nts(ord[0])*dtsize), 1);\n+      else\n+        per_phase = 1;\n       if(tgt_->as_nvidia() && tgt_->as_nvidia()->sm() < 80){\n         int inner = mma_dot_a ? 0 : 1;\n-        per_phase_[layout] = std::max<int>(128 / (in_layout->mts(ord[0])*in_layout->nts(ord[0])*dtsize), 1);\n+        per_phase_[layout] = per_phase;\n         max_phase_[layout] = (ord[inner] == 1 ? 8 : 4) / per_phase_[layout];\n         if(mma_dot_a)\n           vec_[layout] = 2*layouts_->get(mma_dot_a)->to_mma()->rep(0);\n@@ -46,7 +49,7 @@ void swizzle::run(ir::module &) {\n           max_phase_[layout] = 1;\n           vec_[layout] = 1;\n         } else {\n-          per_phase_[layout] = std::max<int>(128 / (in_layout->mts(ord[0])*in_layout->nts(ord[0])*dtsize), 1);\n+          per_phase_[layout] = per_phase;\n           max_phase_[layout] = layout->get_mma_strided() / per_phase_[layout];\n           vec_[layout]       = layout->get_mma_vec();\n         }"}, {"filename": "lib/codegen/pass.cc", "status": "modified", "additions": 10, "deletions": 4, "changes": 14, "file_content_changes": "@@ -74,21 +74,21 @@ std::unique_ptr<llvm::Module> add_passes_to_emit_bin(\n   std::string name = ir.get_function_list()[0]->get_name();\n   std::unique_ptr<llvm::Module> llvm(new llvm::Module(name, ctx));\n   // optimizations\n-  bool cts_use_async = target->as_nvidia() && target->as_nvidia()->sm() >= 80;\n+  bool has_sm80 = target->as_nvidia() && target->as_nvidia()->sm() >= 80;\n   // create passes\n   codegen::analysis::align align;\n   codegen::transform::inliner inliner;\n   codegen::analysis::axes axes;\n-  codegen::transform::cts cts(cts_use_async);\n-  codegen::transform::pipeline pipeline(cts_use_async, num_stages);\n+  codegen::transform::pipeline pipeline(has_sm80, num_stages);\n   codegen::transform::disassociate disassociate;\n   codegen::analysis::layouts layouts(&axes, &align, num_warps, target);\n+  codegen::transform::cts cts(&layouts, has_sm80);\n   codegen::analysis::liveness liveness(&layouts);\n   codegen::analysis::swizzle swizzle(&layouts, target);\n   codegen::analysis::allocation allocation(&liveness);\n   codegen::transform::dce dce;\n   codegen::transform::peephole peephole(target, &layouts);\n-  codegen::transform::coalesce coalesce(&align, &layouts);\n+  codegen::transform::coalesce coalesce(&align, &layouts, has_sm80);\n   codegen::transform::prefetch prefetch_s(target);\n   codegen::transform::membar barriers(&liveness, &layouts, &allocation,\n                                       &prefetch_s, target);\n@@ -97,6 +97,7 @@ std::unique_ptr<llvm::Module> add_passes_to_emit_bin(\n   // run passes\n   inliner.run(ir);\n   dce.run(ir);\n+  // ir.print(std::cout);\n   peephole.run(ir);\n   dce.run(ir);\n   pipeline.run(ir);\n@@ -129,10 +130,15 @@ std::unique_ptr<llvm::Module> add_passes_to_emit_bin(\n   axes.run(ir);\n   layouts.run(ir);\n   swizzle.run(ir);\n+  // std::cout << \"---\" << std::endl;\n+  // ir.print(std::cout);\n+  // std::cout << \"---\" << std::endl;\n+  // ir.print(std::cout);\n   liveness.run(ir);\n   allocation.run(ir);\n   prefetch_s.run(ir);\n   barriers.run(ir);\n+  // exit(1);\n   // ir.print(std::cout);\n   isel.visit(ir, *llvm);\n   shared_static = allocation.allocated_size();"}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 486, "deletions": 178, "changes": 664, "file_content_changes": "@@ -112,6 +112,8 @@ Value* geper::operator()(Value *ptr, Value* off, const std::string& name){\n #define extract_val(...)     builder_->CreateExtractValue(__VA_ARGS__)\n #define fadd(...)            builder_->CreateFAdd(__VA_ARGS__)\n #define fcmp(...)            builder_->CreateFCmp(__VA_ARGS__)\n+#define fcmp_oge(...)        builder_->CreateFCmpOGE(__VA_ARGS__)\n+#define fcmp_ole(...)        builder_->CreateFCmpOLE(__VA_ARGS__)\n #define fmul(...)            builder_->CreateFMul(__VA_ARGS__)\n #define fpcast(...)          builder_->CreateFPCast(__VA_ARGS__)\n #define fsub(...)            builder_->CreateFSub(__VA_ARGS__)\n@@ -742,11 +744,13 @@ void generator::visit_load_inst(ir::load_inst* x){\n   BasicBlock *current = builder_->GetInsertBlock();\n   Module *module = current->getModule();\n   Value *tid = tgt_->get_local_id(module, *builder_, 0);\n+  Value *lane = urem(tid, i32(32));\n   ir::value *op = x->get_pointer_operand();\n   ir::masked_load_inst *mx = dynamic_cast<ir::masked_load_inst*>(x);\n   Type* ty  = cvt(op->get_type()->get_scalar_ty()->get_pointer_element_ty());\n   // compute vector width\n   size_t vec = 1;\n+  bool is_mma_first_row = false;\n   if(op->get_type()->is_block_ty()){\n     auto   ord = ords_.at(op);\n     size_t aln = alignment_->get(op, ord[0]);\n@@ -755,11 +759,15 @@ void generator::visit_load_inst(ir::load_inst* x){\n       max_eq = std::max<size_t>(max_eq, 1);\n       aln = std::min(aln, max_eq);\n     }\n-    auto layout = layouts_->get(x)->to_scanline();\n-    if(layout){\n-      size_t nts = layout->nts(ord[0]);\n-      vec = std::min(nts, aln);\n-    }\n+    analysis::distributed_layout* layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(x));\n+    assert(layout);\n+\n+    vec = std::min<size_t>(layout->contig_per_thread(ord[0]), aln);\n+    // TODO: generalize\n+    is_mma_first_row = (ord.size() >= 1) && layout->to_mma() && \n+                       (a_axes_->get(x, ord[0]) == layouts_->get(x)->get_axis(1));\n+    if(is_mma_first_row)\n+      vec = std::min<size_t>(2, aln);\n   }\n   // code generation\n   auto idxs = idxs_.at(x);\n@@ -793,8 +801,9 @@ void generator::visit_load_inst(ir::load_inst* x){\n     int tot_width = nbits*vec;\n     int width = std::min(tot_width, max_word_width);\n     int n_words = std::max(1, tot_width / width);\n-    bool has_evict_policy = (x->get_eviction_policy() != ir::load_inst::NORMAL) && tgt_->as_nvidia()->sm() >= 80;\n-    has_evict_policy = false; // currently disable until supported in `store`\n+    bool has_l2_evict_policy = (x->get_eviction_policy() != ir::load_inst::NORMAL) && tgt_->as_nvidia()->sm() >= 80;\n+    has_l2_evict_policy = false;\n+    // has_evict_policy = false; // currently disable until supported in `store`\n     // -----\n     // create inline asm string\n     // -----\n@@ -808,7 +817,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n     if (x->get_cache_modifier() == ir::load_inst::CG) asm_oss << \".cg\";\n     if (x->get_eviction_policy() == ir::load_inst::EVICT_FIRST) asm_oss << \".L1::evict_first\";\n     if (x->get_eviction_policy() == ir::load_inst::EVICT_LAST) asm_oss << \".L1::evict_last\";\n-    if (has_evict_policy) asm_oss << \".L2::cache_hint\";\n+    if (has_l2_evict_policy) asm_oss << \".L2::cache_hint\";\n     if(n_words > 1)\n       asm_oss << \".v\" << n_words; // vector width\n     asm_oss << \".b\" << width; // word size\n@@ -820,7 +829,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n     asm_oss << \"}\";\n     asm_oss << \", [ $\" << n_words + 1; // load\n     asm_oss << \" + \" << in_off << \"]\"; // constant offset\n-    if (has_evict_policy) asm_oss << \", $\" << n_words + 2;\n+    if (has_l2_evict_policy) asm_oss << \", $\" << n_words + 2;\n     asm_oss << \";\";\n     bool has_other = other && (other != UndefValue::get(other->getType()));\n     std::vector<Value *> others;\n@@ -842,7 +851,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n       if(ConstantInt* cst = dyn_cast<ConstantInt>(v))\n         asm_oss << \"0x\" << std::hex << cst->getSExtValue();\n       else{\n-        asm_oss << \"$\" << n_words + has_evict_policy +  2 + ii;\n+        asm_oss << \"$\" << n_words + has_l2_evict_policy +  2 + ii;\n         others.push_back(v);\n       }\n       asm_oss.flags(flags);\n@@ -857,7 +866,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n     std::vector<Type*> arg_tys = {pred->getType(), ptr->getType()};\n     for(Value *v: others)\n         arg_tys.push_back(v->getType());\n-    if (has_evict_policy) \n+    if (has_l2_evict_policy) \n       arg_tys.push_back(i64_ty);\n     FunctionType *asm_ty = FunctionType::get(ret_ty, arg_tys, false);\n     // ---\n@@ -873,7 +882,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n       asm_cstrt += \",\";\n       asm_cstrt += (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n     }\n-    if (has_evict_policy) \n+    if (has_l2_evict_policy) \n       asm_cstrt += \",l\";\n     // ---\n     // finally call inline ASM\n@@ -882,7 +891,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n     std::vector<Value*> args = {pred, ptr};\n     for(Value *v: others)\n         args.push_back(v);\n-    if (has_evict_policy)\n+    if (has_l2_evict_policy)\n       args.push_back(policies_.at(x->get_eviction_policy()));\n   \n     \n@@ -933,6 +942,9 @@ void generator::visit_store_inst(ir::store_inst * x){\n   // operands\n   ir::value *ptr_op = x->get_pointer_operand();\n   ir::value *val_op = x->get_value_operand();\n+  ir::value *msk_op = nullptr;\n+  if(auto* msk_st = dynamic_cast<ir::masked_store_inst*>(x))\n+    msk_op = msk_st->get_mask_operand();\n   // vector size\n   size_t vec = 1;\n   if(val_op->get_type()->is_block_ty()){\n@@ -944,36 +956,108 @@ void generator::visit_store_inst(ir::store_inst * x){\n       max_eq = std::max<size_t>(max_eq, 1);\n       aln = std::min(aln, max_eq);\n     }\n-    vec  = std::min(nts, aln);\n-  }\n+    analysis::distributed_layout* layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(ptr_op));\n+    assert(layout);\n+    // vec  = std::min(nts, aln);\n+    vec = std::min<size_t>(layout->contig_per_thread(ord[0]), aln);\n+    // TODO: generalize\n+    bool is_mma_first_row = (ord.size() >= 1) && layout->to_mma() && \n+                       (a_axes_->get(ptr_op, ord[0]) == layouts_->get(ptr_op)->get_axis(1));\n+    if(is_mma_first_row)\n+      vec = std::min<size_t>(2, aln);\n+  }\n+  bool has_l2_evict_policy = (x->get_eviction_policy() != ir::load_inst::NORMAL) && tgt_->as_nvidia()->sm() >= 80;\n+  has_l2_evict_policy = false;\n   auto idxs    = idxs_.at(val_op);\n   Type *ty = cvt(val_op->get_type()->get_scalar_ty());\n   if (ty->isBFloatTy()) // llvm11-nvptx cannot select bf16 store\n     ty = f16_ty;\n+  if(ty->isIntegerTy(1))\n+    ty = builder_->getInt8Ty();\n   for(size_t i = 0; i < idxs.size(); i += vec){\n-    auto idx = idxs[i];\n-    // pointer\n+    indices_t idx = idxs[i];\n+    // pointers\n     Value *ptr = vals_[ptr_op][idx];\n-    // vectorize\n-    Type *v_ty = vec_ty(ty, vec);\n-    ptr = bit_cast(ptr, v_ty->getPointerTo(1));\n-    // value\n-    Value* val = UndefValue::get(v_ty);\n-    for(size_t ii = 0; ii < vec; ii++)\n-      val = insert_elt(val, bit_cast(vals_.at(val_op)[idxs[i + ii]], ty), ii);\n-    if(mx){\n-      Value *msk = vals_[mx->get_mask_operand()][idx];\n-      Instruction *no_op = intrinsic(Intrinsic::donothing, {}, {});\n-      builder_->SetInsertPoint(no_op->getParent());\n-      Instruction* dummy = builder_->CreateRet(nullptr);\n-      Instruction *term = llvm::SplitBlockAndInsertIfThen(msk, no_op, false);\n-      dummy->removeFromParent();\n-      builder_->SetInsertPoint(term);\n-      store(val, ptr);\n-      builder_->SetInsertPoint(no_op);\n+    size_t dtsize = std::max<int>(1, val_op->get_type()->get_scalar_ty()->get_primitive_size_in_bits() / 8);\n+    GetElementPtrInst *in_gep = dyn_cast<GetElementPtrInst>(ptr);\n+    size_t in_off;\n+    if(in_gep){\n+        ConstantInt* cst = dyn_cast<ConstantInt>(in_gep->idx_begin());\n+        in_off = cst ? cst->getValue().getSExtValue()*dtsize : 0;\n+        ptr = cst ? in_gep->getPointerOperand() : in_gep;\n     }\n-    else\n-      store(val, ptr);\n+    else{\n+        in_off = 0;\n+    }\n+    // mask\n+    Value *pred = msk_op ? vals_[msk_op][idx] : builder_->getTrue();\n+    size_t nbits = dtsize*8;\n+    // pack sub-words (< 32/64bits) into words\n+    // each load has width min(nbits*vec, 32/64)\n+    // and there are (nbits * vec)/width of them\n+    int max_word_width = std::max<int>(32, nbits);\n+    int tot_width = nbits*vec;\n+    int width = std::min(tot_width, max_word_width);\n+    int n_words = std::max(1, tot_width / width);\n+    // -----\n+    // create inline asm string\n+    // -----\n+    std::ostringstream asm_oss;\n+    asm_oss << \"@$0\"; // predicate\n+    asm_oss << \" st.global\";\n+    if (has_l2_evict_policy) asm_oss << \".L2::cache_hint\";\n+    if(n_words > 1)\n+      asm_oss << \".v\" << n_words; // vector width\n+    asm_oss << \".b\" << width; // word size\n+    asm_oss << \" [ $1 + \" << in_off << \"]\";\n+    asm_oss << \" , {\";\n+    for(int i = 0; i < n_words; i++){ // return values\n+      if(i > 0) asm_oss << \",\";\n+      asm_oss << \"$\" << 2 + i;\n+    }\n+    asm_oss << \"}\";\n+    if (has_l2_evict_policy) asm_oss << \", $\" << n_words + 2;\n+    asm_oss << \";\";\n+    // ----\n+    // create inline ASM signature\n+    // ---\n+    Type* val_arg_ty = IntegerType::get(*ctx_, width);\n+    std::vector<Type*> arg_tys = {pred->getType(), ptr->getType()};\n+    for(int ii = 0; ii < n_words; ii++)\n+      arg_tys.push_back(val_arg_ty);\n+    if (has_l2_evict_policy) \n+      arg_tys.push_back(i64_ty);\n+    FunctionType *asm_ty = FunctionType::get(builder_->getVoidTy(), arg_tys, false);\n+    // ---\n+    // create inline ASM constraints\n+    // ---\n+    std::string asm_cstrt = \"b,l\";\n+    for(int ii = 0; ii < n_words; ii++){\n+      asm_cstrt += \",\";\n+      asm_cstrt += (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n+    }\n+    if (has_l2_evict_policy) \n+      asm_cstrt += \",l\";\n+    // ---\n+    // finally call inline ASM\n+    // ---\n+    InlineAsm *_asm = InlineAsm::get(asm_ty, asm_oss.str(), asm_cstrt, true);\n+    std::vector<Value*> args = {pred, ptr};\n+    for(unsigned int ii = 0; ii < n_words; ii++){\n+      size_t n_subw = width / nbits;\n+      Value* curr = UndefValue::get(vec_ty(ty, n_subw));\n+      for(unsigned int jj = 0; jj < n_subw; jj++){\n+        Value* new_elt = vals_[val_op][idxs[i + ii*n_subw + jj]];\n+        if(new_elt->getType()->isIntegerTy(1))\n+          new_elt = builder_->CreateSExt(new_elt, builder_->getInt8Ty());\n+        new_elt = bit_cast(new_elt, ty);\n+        curr = builder_->CreateInsertElement(curr, new_elt, jj);\n+      }\n+      args.push_back(bit_cast(curr, val_arg_ty));\n+    }\n+    if (has_l2_evict_policy)\n+      args.push_back(policies_.at(x->get_eviction_policy()));\n+    call(_asm, args);\n   }\n }\n void generator::visit_unmasked_store_inst(ir::unmasked_store_inst* x) {\n@@ -1096,6 +1180,7 @@ void generator::visit_exp_inst(ir::exp_inst* x){\n   InlineAsm *ex2 = InlineAsm::get(fn_ty, \"ex2.approx.f32 $0, $0;\", \"=f,0\", false);\n   for(auto idx: idxs_.at(x)){\n     Value *ex2arg = fmul(vals_[x->get_operand(0)][idx], log2e);\n+    // Value *ex2arg = vals_[x->get_operand(0)][idx];\n     vals_[x][idx] = call(ex2, std::vector<llvm::Value*>{ex2arg});\n   }\n }\n@@ -1200,20 +1285,43 @@ void generator::visit_atomic_rmw_inst(ir::atomic_rmw_inst *atom) {\n \n   // vector size\n   int vec = 1;\n+  Value *mask = builder_->getInt1(true);\n   if(atom->get_type()->is_block_ty()){\n+    auto shape = atom->get_type()->get_block_shapes();\n     int ld = ords_.at(ptr)[0];\n     unsigned alignment = alignment_->get(ptr, ld);\n     vec = std::min<int>(layouts_->get(ptr)->to_scanline()->nts(ld), alignment);\n     vec = std::min(vec, val->get_type()->get_tile_element_ty()->is_fp16_ty() ? 2 : 1);\n+    // mask out inactive threads\n+    analysis::data_layout* layout = layouts_->get(val);\n+    auto curr_axes = a_axes_->get(val);\n+    auto layt_axes = layout->get_axes();\n+    for(unsigned k = 0; k < layt_axes.size(); k++){\n+      unsigned ax = layt_axes.at(k);\n+      distributed_axis dax = axes_.at(ax);\n+      // axis is part of the original layout: thread id should be 0\n+      // but not the current layout\n+      if(std::find(curr_axes.begin(), curr_axes.end(), ax) == curr_axes.end())\n+        mask = and_(mask, icmp_eq(dax.thread_id, i32(0)));\n+    }\n+    // last axis may spillover\n+    Value *thread_id = tgt_->get_local_id(mod_, *builder_, 0);\n+    int per_thread = 1;\n+    for(int ax: layt_axes) { per_thread *= axes_.at(ax).contiguous; }\n+    int numel = 1;\n+    for(int s: layout->get_shape()) { numel *= s; }\n+    mask = and_(mask, icmp_ult(mul(thread_id, i32(per_thread)), i32(numel)));\n   }\n \n+\n   for(int i = 0; i < idxs_.at(val).size(); i += vec){\n     auto idx = idxs_[val][i];\n     Value *rmw_val = UndefValue::get(vec_ty(vals_[val][idx]->getType(), vec));\n     for(int ii = 0; ii < vec; ii++)\n       rmw_val = insert_elt(rmw_val, vals_[val][idxs_[val][i+ii]], ii);\n     Value *rmw_ptr = vals_[ptr][idx];\n     Value *rmw_msk = vals_[msk][idx];\n+    rmw_msk = and_(rmw_msk, mask);\n     if(vec == 1)\n       rmw_val = extract_elt(rmw_val, i32(0));\n     Type* ty = rmw_val->getType();\n@@ -1289,6 +1397,18 @@ void generator::visit_mma884(ir::dot_inst* C, ir::value *A, ir::value *B, ir::va\n   // order\n   auto ord_a = layouts_->get(A)->get_order();\n   auto ord_b = layouts_->get(B)->get_order();\n+  bool is_a_trans = C->is_trans_a();\n+  // is_a_trans = false;\n+  if(C->is_trans_a()){\n+    std::swap(ord_a[0], ord_a[1]);\n+    std::swap(shape_a[0], shape_a[1]);\n+    std::swap(offset_a_m_, offset_a_k_);\n+  }\n+  // std::cout << \"visiting\" << std::endl;\n+  // if(C->is_trans_b()){\n+  //   std::swap(ord_b[0], ord_b[1]);\n+    // std::swap(shape_b[0], shape_b[1]);\n+  // }\n   // layouts\n   analysis::mma_layout*    layout_c = layouts_->get(C)->to_mma();\n   analysis::shared_layout* layout_a = layouts_->get(A)->to_shared();\n@@ -1320,6 +1440,12 @@ void generator::visit_mma884(ir::dot_inst* C, ir::value *A, ir::value *B, ir::va\n   int step_b0   = is_b_row ? stride_rep_n : stride_rep_k;\n   int num_ptr_b = std::max(2 * per_phase_b * max_phase_b / step_b0, 1);\n \n+\n+  // max_phase_a = 4;\n+  // vec_a = 8;\n+  // std::cout << per_phase_a << \" \" << max_phase_a << \" \" << step_a0 << \" \" << num_ptr_a << \" \" << stride_am << \" \" << stride_ak << \" \" << stride_a0 << \" \" << stride_a1 << std::endl;\n+  // std::cout << vec_a << \" \" << vec_b << std::endl;\n+\n   /* --------------------------------- */\n   /* --- pre-compute pointer lanes --- */\n   /* --------------------------------- */\n@@ -1914,12 +2040,17 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n   auto shape_a = A->get_type()->get_block_shapes();\n   auto shape_b = B->get_type()->get_block_shapes();\n   auto ord_a = layouts_->get(A)->get_order();\n+  if(C->is_trans_a()){\n+    std::swap(ord_a[0], ord_a[1]);\n+    std::swap(shape_a[0], shape_a[1]);\n+  }\n   auto ord_b = layouts_->get(B)->get_order();\n+  if(C->is_trans_b()){\n+    std::swap(ord_b[0], ord_b[1]);\n+    std::swap(shape_b[0], shape_b[1]);\n+  }\n+  NK = shape_a[1];\n   analysis::mma_layout* layout = layouts_->get(C)->to_mma();\n-  analysis::shared_layout* layout_a = (analysis::shared_layout*)layouts_->get(C->get_operand(0));\n-  analysis::shared_layout* layout_b = (analysis::shared_layout*)layouts_->get(C->get_operand(1));\n-  bool is_a_row = ord_a[0] == 1;\n-  bool is_b_row = ord_b[0] == 1;\n \n   std::vector<int> mma_instr_shape = layout->get_mma_instr_shape();\n   const int mma_instr_m = mma_instr_shape[0];\n@@ -1931,10 +2062,6 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n   const int mat_shape_n = mat_shape[1];\n   const int mat_shape_k = mat_shape[2];\n \n-  const int per_phase_a = swizzle_->get_per_phase(layout_a);\n-  const int max_phase_a = swizzle_->get_max_phase(layout_a);\n-  const int per_phase_b = swizzle_->get_per_phase(layout_b);\n-  const int max_phase_b = swizzle_->get_max_phase(layout_b);\n \n   const int num_rep_m = shapes[0] / layout->shape_per_cta(0);\n   const int num_rep_n = shapes[1] / layout->shape_per_cta(1);\n@@ -1999,7 +2126,12 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n \n   BasicBlock* CurrBB = builder_->GetInsertBlock();\n   BasicBlock* FirstBB = &CurrBB->getParent()->getEntryBlock();\n-  if(FirstBB != CurrBB)\n+\n+  // if true, this will move pointer declarations to the entry basic block\n+  // not prefetched cases tend to be more limited in resource usage\n+  // so we don't pre-compute ptrs to save registers\n+  bool licm_ptrs = C->is_prefetched() && (FirstBB != CurrBB);\n+  if(licm_ptrs)\n     builder_->SetInsertPoint(FirstBB->getTerminator());\n \n   Value* thread = tgt_->get_local_id(mod_, *builder_, 0);\n@@ -2013,47 +2145,137 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n   size_t dtsize_a = A->get_type()->get_scalar_ty()->get_primitive_size_in_bits() / 8;\n   size_t dtsize_b = B->get_type()->get_scalar_ty()->get_primitive_size_in_bits() / 8;\n \n+  ir::phi_node* phiA = dynamic_cast<ir::phi_node*>(A);\n+  ir::phi_node* phiB = dynamic_cast<ir::phi_node*>(B);\n+  auto register_lds2 =\n+    [&](std::map<std::pair<unsigned, unsigned>, Value*>& vals, int mn, int k, int inc, Value* val, bool is_prefetch) {\n+      if (k < 2 && is_prefetch) {\n+        ir::basic_block* inc_block = phiA->get_incoming_block(inc);\n+        lazy_phi_incs_.push_back(std::make_tuple((PHINode*)vals[{mn, k}], val, inc_block));\n+      } else\n+        vals[{mn, k}] = val;\n+  };\n+\n   // | -> k (row-major), since we have ldmatrix.trans, we only need to change stride\n   // v (s0_0(0), s1_0(2), | *num_rep_k\n   // m  s0_1(1), s1_1(3)) |  (stride in num of matrices(mat_stride_ak): 2)\n   // -----------\n   //   *num_rep_m (stride in num of matrices(mat_stride_am): 2*layout->wpt(0))\n-  mma16816_smem_loader a_loader(layout->wpt(0), ord_a, /*k_order*/1, shape_a, \n-                                {mma_instr_m, mma_instr_k}, {mat_shape_m, mat_shape_k}, \n-                                per_phase_a, max_phase_a, dtsize_a, builder_, add, mul, gep);\n-  std::vector<Value*> off_a = a_loader.compute_offs(warp_m, lane);\n-  int num_ptr_a = a_loader.get_num_ptr();\n+  std::function<void(int,int,int,bool)> load_a;\n+  analysis::shared_layout* layout_a = layouts_->get(C->get_operand(0))->to_shared();\n+  bool is_a_shared = layout_a != nullptr;\n+  if(is_a_shared) {\n+    const int per_phase_a = swizzle_->get_per_phase(layout_a);\n+    const int max_phase_a = swizzle_->get_max_phase(layout_a);\n+    mma16816_smem_loader a_loader(layout->wpt(0), ord_a, /*k_order*/1, shape_a, \n+                                  {mma_instr_m, mma_instr_k}, {mat_shape_m, mat_shape_k}, \n+                                  per_phase_a, max_phase_a, dtsize_a, builder_, add, mul, gep);\n+    std::vector<Value*> off_a = a_loader.compute_offs(warp_m, lane);\n+    int num_ptr_a = a_loader.get_num_ptr();\n+    // pointers\n+    std::vector<Value*> ptrs_a(num_ptr_a);\n+    if(licm_ptrs)\n+      builder_->SetInsertPoint(CurrBB);\n+    for(int i = 0; i < num_ptr_a; i++)\n+      ptrs_a[i] = bit_cast(gep(shmems_[A], {off_a[i]}), smem_ptr_ty);\n+    if(licm_ptrs)\n+      builder_->SetInsertPoint(FirstBB->getTerminator());\n+    // loading function\n+    load_a = [&,a_loader,ptrs_a,off_a](int m, int k, int inc, bool is_prefetch) mutable {\n+      auto [ha0, ha1, ha2, ha3] = a_loader.load_x4(m, k, inc, is_prefetch, phiA, shared_pre_ptr_[layout_a],\n+                                                  shared_next_ptr_[layout_a], off_a, ptrs_a, \n+                                                  ldmatrix_ty, smem_ptr_ty, prefetch_latch_to_bb_);\n+      register_lds2(ha, m,   k,   inc, ha0, is_prefetch);\n+      register_lds2(ha, m+1, k,   inc, ha1, is_prefetch);\n+      register_lds2(ha, m,   k+1, inc, ha2, is_prefetch);\n+      register_lds2(ha, m+1, k+1, inc, ha3, is_prefetch);\n+    };\n+  }\n+  else {\n+    load_a = [&](int m, int k, int inc, bool is_prefetch) {\n+      distributed_axis ax_n = axes_.at(a_axes_->get(A, 1));\n+      int ldm = ax_n.values.size();\n+      if(ldm != num_rep_k*4)\n+        throw std::runtime_error(\"Internal compiler error when trying to fuse matmuls!\");\n+      // std::cout << m << \" \" << k << std::endl;\n+      // std::cout << idxs_[A].size() << std::endl;\n+      // std::cout << (m+1)*ldm + k*2 + 3 << std::endl;\n+      // int ldm = num_rep_k*4;\n+      Value* ha0 = UndefValue::get(fp16x2_ty);\n+      Value* ha1 = UndefValue::get(fp16x2_ty);\n+      Value* ha2 = UndefValue::get(fp16x2_ty);\n+      Value* ha3 = UndefValue::get(fp16x2_ty);\n+      ha0 = builder_->CreateInsertElement(ha0, vals_[A][idxs_[A][(m+0)*ldm + k*2 + 0]], i32(0));\n+      ha0 = builder_->CreateInsertElement(ha0, vals_[A][idxs_[A][(m+0)*ldm + k*2 + 1]], i32(1));\n+      ha1 = builder_->CreateInsertElement(ha1, vals_[A][idxs_[A][(m+1)*ldm + k*2 + 0]], i32(0));\n+      ha1 = builder_->CreateInsertElement(ha1, vals_[A][idxs_[A][(m+1)*ldm + k*2 + 1]], i32(1));\n+      ha2 = builder_->CreateInsertElement(ha2, vals_[A][idxs_[A][(m+0)*ldm + k*2 + 2]], i32(0));\n+      ha2 = builder_->CreateInsertElement(ha2, vals_[A][idxs_[A][(m+0)*ldm + k*2 + 3]], i32(1));\n+      ha3 = builder_->CreateInsertElement(ha3, vals_[A][idxs_[A][(m+1)*ldm + k*2 + 2]], i32(0));\n+      ha3 = builder_->CreateInsertElement(ha3, vals_[A][idxs_[A][(m+1)*ldm + k*2 + 3]], i32(1));\n+      ha[{m, k}] = ha0;\n+      ha[{m+1, k}] = ha1;\n+      ha[{m, k+1}] = ha2;\n+      ha[{m+1, k+1}] = ha3;\n+    };\n+  }\n+\n \n   // | -> n (col-major)\n   // v (s0_0(0), | (stride: wpt(1)) | s1_0(2)  | *num_rep_n\n   // k  s0_1(1), |                  | s1_1(3)) | (stride in num of matrices(mat_stride_bn): wpt(1))\n   // -----------\n   //   *num_rep_k (stride in num of matrices(mat_stride_bk): 2)\n-  mma16816_smem_loader b_loader(layout->wpt(1), ord_b, /*k_order*/0, shape_b,\n-                                {mma_instr_k, mma_instr_n}, {mat_shape_k, mat_shape_n},\n+  analysis::shared_layout* layout_b = layouts_->get(C->get_operand(1))->to_shared();\n+  const int per_phase_b = swizzle_->get_per_phase(layout_b);\n+  const int max_phase_b = swizzle_->get_max_phase(layout_b);\n+  std::vector<int> mma_instr_b{mma_instr_k, mma_instr_n};\n+  std::vector<int> mat_shape_b{mat_shape_k, mat_shape_n};\n+  int k_order_b = 0;\n+  // if(C->is_trans_b()){\n+    // std::swap(mma_instr_b[0], mma_instr_b[1]);\n+    // std::swap(mat_shape_b[0], mat_shape_b[1]);\n+    // k_order_b = k_order_b ^ 1;\n+    // std::swap(ord_b[0], ord_b[1]);\n+    // std::swap(shape_b[0], shape_b[1]);\n+  // }\n+\n+  mma16816_smem_loader b_loader(layout->wpt(1), ord_b, k_order_b, shape_b,\n+                                mma_instr_b, mat_shape_b,\n                                 per_phase_b, max_phase_b, dtsize_b, builder_, add, mul, gep);\n   std::vector<Value*> off_b = b_loader.compute_offs(warp_n, lane);\n-  int num_ptr_b = b_loader.get_num_ptr();\n \n-  builder_->SetInsertPoint(CurrBB);\n-  // A pointer\n-  std::vector<Value*> ptrs_a(num_ptr_a);\n-  for(int i = 0; i < num_ptr_a; i++)\n-    ptrs_a[i] = bit_cast(gep(shmems_[A], {off_a[i]}), smem_ptr_ty);\n-  // B pointer\n+  if(licm_ptrs)\n+    builder_->SetInsertPoint(CurrBB);\n+  // pointers\n+  int num_ptr_b = b_loader.get_num_ptr();\n   std::vector<Value*> ptrs_b(num_ptr_b);\n   for(int i = 0; i < num_ptr_b; i++)\n     ptrs_b[i] = bit_cast(gep(shmems_[B], {off_b[i]}), smem_ptr_ty);\n \n-  InlineAsm *mma_fn = InlineAsm::get(mma_ty, layout->get_ptx_instr() +\n+    \n+  // loading function\n+  std::function<void(int,int,int,bool)> load_b;\n+  load_b = [&](int n, int k, int inc, bool is_prefetch) {\n+      auto [hb0, hb1, hb2, hb3] = b_loader.load_x4(k, n, inc, is_prefetch, phiB, shared_pre_ptr_[layout_b],\n+                                                   shared_next_ptr_[layout_b], off_b, ptrs_b, \n+                                                   ldmatrix_ty, smem_ptr_ty, prefetch_latch_to_bb_);\n+      register_lds2(hb, n,   k,   inc, hb0, is_prefetch);\n+      register_lds2(hb, n+1, k,   inc, hb2, is_prefetch);\n+      register_lds2(hb, n,   k+1, inc, hb1, is_prefetch);\n+      register_lds2(hb, n+1, k+1, inc, hb3, is_prefetch);\n+  };\n+\n+\n+\n+  // create mma & unpack result, m, n, k are offsets in mat\n+  auto call_mma = [&](unsigned m, unsigned n, unsigned k) {\n+      InlineAsm *mma_fn = InlineAsm::get(mma_ty, layout->get_ptx_instr() +\n                                              \" {$0, $1, $2, $3},\"\n                                              \" {$4, $5, $6, $7},\"\n                                              \" {$8, $9},\"\n                                              \" {$10, $11, $12, $13};\",\n                                              \"=r,=r,=r,=r,r,r,r,r,r,r,0,1,2,3\", true);\n-\n-  // create mma & unpack result, m, n, k are offsets in mat\n-  auto call_mma = [&](unsigned m, unsigned n, unsigned k) {\n       unsigned cols_per_thread = num_rep_n * 2;\n       std::vector<size_t> idx = {\n         (m + 0)*cols_per_thread + (n*2 + 0),\n@@ -2070,39 +2292,6 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n       fc[idx[2]] = extract_val(nc, std::vector<unsigned>{2});\n       fc[idx[3]] = extract_val(nc, std::vector<unsigned>{3});\n   };\n-\n-  ir::phi_node* phiA = dynamic_cast<ir::phi_node*>(A);\n-  ir::phi_node* phiB = dynamic_cast<ir::phi_node*>(B);\n-\n-  auto register_lds2 =\n-    [&](std::map<std::pair<unsigned, unsigned>, Value*>& vals, int mn, int k, int inc, Value* val, bool is_prefetch) {\n-      if (k < 2 && is_prefetch) {\n-        ir::basic_block* inc_block = phiA->get_incoming_block(inc);\n-        lazy_phi_incs_.push_back(std::make_tuple((PHINode*)vals[{mn, k}], val, inc_block));\n-      } else\n-        vals[{mn, k}] = val;\n-  };\n-\n-  auto load_a = [&](int m, int k, int inc, bool is_prefetch) {\n-      auto [ha0, ha1, ha2, ha3] = a_loader.load_x4(m, k, inc, is_prefetch, phiA, shared_pre_ptr_[layout_a],\n-                                                   shared_next_ptr_[layout_a], off_a, ptrs_a, \n-                                                   ldmatrix_ty, smem_ptr_ty, prefetch_latch_to_bb_);\n-      register_lds2(ha, m,   k,   inc, ha0, is_prefetch);\n-      register_lds2(ha, m+1, k,   inc, ha1, is_prefetch);\n-      register_lds2(ha, m,   k+1, inc, ha2, is_prefetch);\n-      register_lds2(ha, m+1, k+1, inc, ha3, is_prefetch);\n-  };\n-\n-  auto load_b = [&](int n, int k, int inc, bool is_prefetch) {\n-      auto [hb0, hb1, hb2, hb3] = b_loader.load_x4(k, n, inc, is_prefetch, phiB, shared_pre_ptr_[layout_b],\n-                                                   shared_next_ptr_[layout_b], off_b, ptrs_b, \n-                                                   ldmatrix_ty, smem_ptr_ty, prefetch_latch_to_bb_);\n-      register_lds2(hb, n,   k,   inc, hb0, is_prefetch);\n-      register_lds2(hb, n+1, k,   inc, hb2, is_prefetch);\n-      register_lds2(hb, n,   k+1, inc, hb1, is_prefetch);\n-      register_lds2(hb, n+1, k+1, inc, hb3, is_prefetch);\n-  };\n-\n   if (C->is_prefetched()) {\n       // create phis\n       builder_->SetInsertPoint(CurrBB->getFirstNonPHI());\n@@ -2161,6 +2350,7 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n       i = 0;\n     vals_[C][idx] = fcs.at(key)[i++];\n   };\n+\n }\n \n /**\n@@ -2334,15 +2524,15 @@ inline Value* generator::shfl_sync(Value* acc, int32_t i){\n /**\n  * \\brief Code Generation for `reduce` (ND case)\n  */\n-void generator::visit_reducend_inst_fast(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral){\n-  //\n+void generator::visit_reducend_inst_fast(ir::reduce_inst* x, acc_fn_t do_acc, Value *neutral){\n   ir::value *arg = x->get_operand(0);\n+  const auto with_index = x->with_index();\n+  unsigned axis = x->get_axis();\n   analysis::distributed_layout* layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(arg));\n-  std::vector<unsigned> shapes = layout->get_shape();\n+  const auto &shapes = layout->get_shape();\n \n   Type* sca_ty = cvt(arg->get_type()->get_scalar_ty());\n   size_t n_bits = sca_ty->getPrimitiveSizeInBits();\n-\n   std::string n_bits_str = std::to_string(n_bits);\n   std::string cst = (n_bits == 64) ? \"l\" : \"r\";\n \n@@ -2351,6 +2541,15 @@ void generator::visit_reducend_inst_fast(ir::reduce_inst* x, std::function<Value\n   FunctionType *ld_shared_ty = FunctionType::get(sca_ty, {i1_ty, ptr_ty(sca_ty, 3)}, false);\n   InlineAsm *ld_shared = InlineAsm::get(ld_shared_ty, \"@$1 ld.shared.b\" + n_bits_str + \" $0, [$2];\", \"=\" + cst + \",b,\" + cst, true);\n \n+  Type *index_ty = IntegerType::get(*ctx_, 32);\n+  FunctionType *st_shared_index_ty =\n+      FunctionType::get(void_ty, {i1_ty, ptr_ty(index_ty, 3), index_ty}, false);\n+  InlineAsm *st_shared_index = InlineAsm::get(\n+      st_shared_index_ty, \"@$0 st.shared.b32 [$1], $2;\", \"b,r,r\", true);\n+  FunctionType *ld_shared_index_ty =\n+      FunctionType::get(index_ty, {i1_ty, ptr_ty(index_ty, 3)}, false);\n+  InlineAsm *ld_shared_index = InlineAsm::get(\n+      ld_shared_index_ty, \"@$1 ld.shared.b32 $0, [$2];\", \"=r,b,r\", true);\n \n   Value* thread = tgt_->get_local_id(mod_, *builder_, 0);\n   Value* warp = udiv(thread, i32(32));\n@@ -2362,104 +2561,141 @@ void generator::visit_reducend_inst_fast(ir::reduce_inst* x, std::function<Value\n   std::vector<indices_t> arg_idxs = idxs_.at(arg);\n   size_t n_elts = arg_idxs.size();\n   unsigned col_per_thread = 0;\n-  Value* warp_i;\n-  Value* warp_j;\n-  if(analysis::scanline_layout* scanline = layout->to_scanline()){\n+  Value* warp_j = nullptr;\n+  if (analysis::scanline_layout *scanline = layout->to_scanline()) {\n     std::vector<int> order = layout->get_order();\n     unsigned mts = scanline->mts(order[0]);\n     shuffle_width = std::min<int>(mts, 32);\n-    warps_per_inner = std::max<int>(mts/32, 1);\n+    warps_per_inner = std::max<int>(mts / 32, 1);\n     col_per_thread = shapes[order[0]] / mts;\n-    warp_i = udiv(warp, i32(warps_per_inner));\n     warp_j = urem(warp, i32(warps_per_inner));\n-  }\n-  else if(layout->to_mma()){\n-    shuffle_width = 4; \n+  } else if (layout->to_mma()) {\n+    shuffle_width = 4;\n     warps_per_inner = layout->to_mma()->wpt(1);\n-    col_per_thread = 16;\n-    warp_i = axes_.at(a_axes_->get(arg, 0)).thread_id;\n+    col_per_thread = axes_.at(a_axes_->get(arg, 1)).values.size();\n     warp_j = axes_.at(a_axes_->get(arg, 1)).thread_id;\n-  }\n+  } \n+  assert(warp_j != nullptr);\n \n   // unsigned col_per_thread = 2 * shapes[order[0]] / layout->shape_per_cta(order[0]);\n   //\n-  Type *ret_ty = cvt(x->get_type()->get_scalar_ty());\n-  unsigned addr_space = shmem_->getType()->getPointerAddressSpace();\n-  Value *base = bit_cast(shmem_, ptr_ty(ret_ty, addr_space));\n+  Value *base = cast_shared_layout_ptr(layouts_->get(layouts_->tmp(x)),\n+                                       cvt(x->get_type()->get_scalar_ty()));\n+  Value *index_base =\n+      with_index ? cast_shared_layout_ptr(layouts_->get(layouts_->tmp_index(x)),\n+                                          IntegerType::get(*ctx_, 32))\n+                 : nullptr;\n+\n   // preds\n   Value* is_lane0 = icmp_eq(lane, i32(0));\n   Value* is_warp0 = icmp_eq(warp, i32(0));\n   Value* is_thread0 = icmp_eq(thread, i32(0));\n   Value* lane_j = urem(lane, i32(shuffle_width));\n-  Value* first_lane_in_col = icmp_eq(lane_j, i32(0));\n-  add_barrier();\n+  if(warps_per_inner > 1)\n+    add_barrier();\n   // compute partial sum for each warp, and store to shared memory\n   for(size_t i = 0; i < n_elts/col_per_thread; i++){\n-    Value* acc;\n+    std::pair<Value*, Value*> acc;\n     // reduce within thread\n     for(size_t j = 0; j < col_per_thread; j++){\n-      Value* val = arg_vals[arg_idxs[i*col_per_thread + j]];\n-      // acc = (j == 0) ? val : do_acc(acc, val);\n-      acc = (j == 0) ? val : do_acc(acc, val);\n+      auto arg_idx = arg_idxs[i*col_per_thread + j];\n+      bool is_first = j == 0;\n+      do_acc(\n+          acc, [&]() -> Value * { return arg_vals[arg_idx]; },\n+          [&]() -> Value * { return arg_idx[axis]; }, is_first);\n     }\n+\n     // reduce within warp\n-    for(int k = shuffle_width/2 ; k > 0; k >>= 1)\n-      acc = do_acc(acc, shfl_sync(acc, k));\n+    for(int k = shuffle_width/2 ; k > 0; k >>= 1) {\n+      do_acc(\n+          acc, [&]() -> Value * { return shfl_sync(acc.first, k); },\n+          [&]() -> Value * { return shfl_sync(acc.second, k); }, false);\n+    }\n     // store partial result to shared memory\n     auto x_idxs = idxs_[x][i];\n     Value* x_idx = x_idxs.empty() ? builder_->getInt32(0) : x_idxs[0];\n-    Value* st_off = add(mul(x_idx, i32(warps_per_inner)), warp_j);\n-    call(st_shared, {icmp_eq(lane_j, i32(0)), gep(base, st_off), acc});\n+    // single warp on the reduce dimension -- no need to use shmem\n+    if(warps_per_inner==1){\n+      vals_[x][idxs_[x][i]] = with_index ? acc.second : acc.first;\n+    }\n+    else{\n+      Value* st_off = add(mul(x_idx, i32(warps_per_inner)), warp_j);\n+      call(st_shared, {icmp_eq(lane_j, i32(0)), gep(base, st_off), acc.first});\n+      if (with_index) {\n+        call(st_shared_index,\n+            {icmp_eq(lane_j, i32(0)), gep(index_base, st_off), acc.second});\n+      }\n+    }\n   }\n+  if(warps_per_inner==1)\n+    return;\n   add_barrier();\n   // at this point, partial accumulator synchronized in shared memory\n   // Just need to reduce `warp_per_inner` numbers in shared memory\n   for(size_t i = 0; i < n_elts/col_per_thread; i++){\n     auto x_idxs = idxs_[x][i];\n     Value* x_idx = x_idxs.empty() ? builder_->getInt32(0) : x_idxs[0];\n     Value* ld_off = add(mul(x_idx, i32(warps_per_inner)), urem(lane_j, i32(warps_per_inner)));\n-    Value* acc = call(ld_shared, {builder_->getInt1(true), gep(base, ld_off)});\n-    for(int k = warps_per_inner/2; k > 0; k >>= 1)\n-      acc = do_acc(acc, shfl_sync(acc, k));\n-    vals_[x][idxs_[x][i]] = acc;\n+    std::pair<Value*, Value*> acc;\n+    acc.first = call(ld_shared, {builder_->getInt1(true), gep(base, ld_off)});\n+    acc.second = with_index ? call(ld_shared_index, {builder_->getInt1(true),\n+                                                     gep(index_base, ld_off)})\n+                            : nullptr;\n+    for (int k = warps_per_inner / 2; k > 0; k >>= 1) {\n+      do_acc(\n+          acc, [&]() -> Value * { return shfl_sync(acc.first, k); },\n+          [&]() -> Value * { return shfl_sync(acc.second, k); }, false);\n+    }\n+    vals_[x][idxs_[x][i]] = with_index ? acc.second : acc.first;\n   }\n   // add_barrier();\n }\n \n-void generator::visit_reducend_inst(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral) {\n+\n+void generator::visit_reducend_inst(ir::reduce_inst* x, acc_fn_t do_acc, Value *neutral) {\n   ir::value *arg = x->get_operand(0);\n-  Type *ty = cvt(x->get_type()->get_scalar_ty());\n   unsigned axis = x->get_axis();\n+  auto with_index = x->with_index();\n \n   // reduce within thread\n-  std::map<indices_t, Value*> accs;\n+  // index-><current reduced value, current min/max index (optional)>\n+  std::map<indices_t, std::pair<Value*, Value*>> accs;\n   for(indices_t idx: idxs_.at(arg)){\n     indices_t pidx = idx;\n     pidx[axis] = i32(0);\n-    Value *current = vals_[arg][idx];\n     bool is_first = accs.find(pidx) == accs.end();\n-    accs[pidx] = is_first ? current : do_acc(accs[pidx], current);\n+    do_acc(\n+        accs[pidx], [&]() -> Value * { return vals_[arg][idx]; },\n+        [&]() -> Value * { return idx[axis]; }, is_first);\n   };\n \n   // reduce within blocks\n-  analysis::data_layout* layout = layouts_->get(layouts_->tmp(x));\n-  Value *base = shared_ptr_.at(layout);\n-  auto shape  = layout->get_shape();\n-  auto order  = layout->get_order();\n-  int  space = base->getType()->getPointerAddressSpace();\n-  Value *ptr = bit_cast(base, ptr_ty(ty, space));\n+  auto *data_layout = layouts_->get(layouts_->tmp(x));\n+  auto *data_ptr =\n+      cast_shared_layout_ptr(data_layout, cvt(x->get_type()->get_scalar_ty()));\n+  auto *index_ptr =\n+      with_index ? cast_shared_layout_ptr(layouts_->get(layouts_->tmp_index(x)),\n+                                          IntegerType::get(*ctx_, 32))\n+                 : data_ptr;\n+\n+  auto shape  = data_layout->get_shape();\n+  auto order  = data_layout->get_order();\n   Value *lane = axes_.at(a_axes_->get(arg, axis)).thread_id;\n   for(auto& x: accs) {\n     // current element being computed\n-    Value *&acc = x.second;\n+    std::pair<Value *, Value *> acc = x.second;\n     indices_t write_idx = x.first;\n     write_idx[axis] = lane;\n     // shared memory write  pointer\n     Value *write_off = shared_off(shape, order, write_idx);\n-    Value *write_ptr = gep(ptr, write_off);\n+    Value *write_ptr = gep(data_ptr, write_off);\n+    Value *index_write_ptr = gep(index_ptr, write_off);\n     // initialize shared memory\n     add_barrier();\n-    store(acc, write_ptr);\n+    store(acc.first, write_ptr);\n+    if (with_index) {\n+      store(acc.second, index_write_ptr);\n+    }\n     // build result\n     indices_t idx(write_idx.size(), i32(0));\n     for(size_t i = shape[axis]/2; i > 0; i >>= 1){\n@@ -2468,11 +2704,17 @@ void generator::visit_reducend_inst(ir::reduce_inst* x, std::function<Value*(Val\n       Value *read_msk = icmp_ult(lane, i32(i));\n       Value *read_off = select(read_msk, shared_off(shape, order, idx), i32(0));\n       Value *read_ptr = gep(write_ptr, read_off);\n+      Value *index_read_ptr = gep(index_write_ptr, read_off);\n       add_barrier();\n       // update accumulator\n-      acc = do_acc(acc, load(read_ptr));\n+      do_acc(\n+          acc, [&]() -> Value * { return load(read_ptr); },\n+          [&]() -> Value * { return load(index_read_ptr); }, false);\n       add_barrier();\n-      store(acc, write_ptr);\n+      store(acc.first, write_ptr);\n+      if (with_index) {\n+        store(acc.second, index_write_ptr);\n+      }\n     }\n   }\n   add_barrier();\n@@ -2482,7 +2724,8 @@ void generator::visit_reducend_inst(ir::reduce_inst* x, std::function<Value*(Val\n     indices_t read_idx = idx;\n     read_idx.insert(read_idx.begin() + axis, i32(0));\n     Value *read_off = shared_off(shape, order, read_idx);\n-    Value *read_ptr = gep(ptr, read_off);\n+    Value *read_ptr =\n+        with_index ? gep(index_ptr, read_off) : gep(data_ptr, read_off);\n     vals_[x][idx] = load(read_ptr);\n   };\n }\n@@ -2494,45 +2737,76 @@ void generator::visit_reduce_inst(ir::reduce_inst* x) {\n   Type *ty = cvt(x->get_type()->get_scalar_ty());\n   // accumulation function\n   ir::reduce_inst::op_t op = x->get_op();\n-  auto do_acc = [&](Value *x, Value *y) -> Value* {\n+  auto do_acc_op = [&](Value *x, Value *y) -> Value* {\n     switch(op){\n     case ir::reduce_inst::ADD: return add(x, y);\n     case ir::reduce_inst::SUB: return sub(x, y);\n-    case ir::reduce_inst::MAX: return select(icmp_sge(x, y), x, y);\n-    case ir::reduce_inst::MIN: return select(icmp_sle(x, y), x, y);\n+    case ir::reduce_inst::ARGUMAX: return icmp_uge(x, y);\n+    case ir::reduce_inst::ARGUMIN: return icmp_ule(x, y);\n+    case ir::reduce_inst::ARGMAX: return icmp_sge(x, y);\n+    case ir::reduce_inst::ARGMIN: return icmp_sle(x, y);\n     case ir::reduce_inst::UMAX: return select(icmp_uge(x, y), x, y);\n     case ir::reduce_inst::UMIN: return select(icmp_ule(x, y), x, y);\n+    case ir::reduce_inst::MAX: return select(icmp_sge(x, y), x, y);\n+    case ir::reduce_inst::MIN: return select(icmp_sle(x, y), x, y);\n     case ir::reduce_inst::FADD: return fadd(x, y);\n     case ir::reduce_inst::FSUB: return fsub(x, y);\n+    case ir::reduce_inst::ARGFMAX: return fcmp_oge(x, y);\n+    case ir::reduce_inst::ARGFMIN: return fcmp_ole(x, y);\n     case ir::reduce_inst::FMAX: return max_num(x, y);\n     case ir::reduce_inst::FMIN: return min_num(x, y);\n     case ir::reduce_inst::XOR: return xor_(x, y);\n+\n     default: throw std::runtime_error(\"unreachable\");\n     }\n   };\n+\n+  auto do_acc = [&](std::pair<Value *, Value *> &acc,\n+                    std::function<Value *()> load_value_fn,\n+                    std::function<Value *()> load_index_fn,\n+                    bool is_first) -> void {\n+    auto *val = load_value_fn();\n+    if (x->with_index()) {\n+      auto *index = load_index_fn();\n+      if (is_first) {\n+        acc.first = val;\n+        acc.second = index;\n+      } else {\n+        Value *ret = do_acc_op(acc.first, val);\n+        acc.first = select(ret, acc.first, val);\n+        acc.second = select(ret, acc.second, index);\n+      }\n+    } else {\n+      acc.first = is_first ? val : do_acc_op(acc.first, val);\n+    }\n+  };\n+\n   // neutral element\n   Value *neutral;\n   switch(op) {\n     case ir::reduce_inst::ADD: neutral = ConstantInt::get(ty, 0); break;\n     case ir::reduce_inst::SUB: neutral = ConstantInt::get(ty, 0); break;\n-    case ir::reduce_inst::MAX: neutral = ConstantInt::get(ty, INT32_MIN); break;\n-    case ir::reduce_inst::MIN: neutral = ConstantInt::get(ty, INT32_MAX); break;\n+    case ir::reduce_inst::ARGUMAX: neutral = ConstantInt::get(ty, INT32_MIN); break;\n+    case ir::reduce_inst::ARGUMIN: neutral = ConstantInt::get(ty, INT32_MAX); break;\n+    case ir::reduce_inst::ARGMAX: neutral = ConstantInt::get(ty, INT32_MIN); break;\n+    case ir::reduce_inst::ARGMIN: neutral = ConstantInt::get(ty, INT32_MAX); break;\n     case ir::reduce_inst::UMAX: neutral = ConstantInt::get(ty, 0); break;\n     case ir::reduce_inst::UMIN: neutral = ConstantInt::get(ty, UINT32_MAX); break;\n+    case ir::reduce_inst::MAX: neutral = ConstantInt::get(ty, INT32_MIN); break;\n+    case ir::reduce_inst::MIN: neutral = ConstantInt::get(ty, INT32_MAX); break;\n     case ir::reduce_inst::FADD: neutral = ConstantFP::get(ty, 0); break;\n     case ir::reduce_inst::FSUB: neutral = ConstantFP::get(ty, 0); break;\n+    case ir::reduce_inst::ARGFMAX: neutral = ConstantFP::get(ty, -INFINITY); break;\n+    case ir::reduce_inst::ARGFMIN: neutral = ConstantFP::get(ty, INFINITY); break;\n     case ir::reduce_inst::FMAX: neutral = ConstantFP::get(ty, -INFINITY); break;\n     case ir::reduce_inst::FMIN: neutral = ConstantFP::get(ty, INFINITY); break;\n-    case ir::reduce_inst::XOR: neutral = neutral = ConstantInt::get(ty, 0); break;\n+    case ir::reduce_inst::XOR: neutral = ConstantInt::get(ty, 0); break;\n     default: throw std::runtime_error(\"unreachable\");\n   }\n   ir::value *arg = x->get_operand(0);\n-  int cc = tgt_->as_nvidia()->sm();\n-  analysis::scanline_layout* scanline = layouts_->get(x->get_operand(0))->to_scanline();\n-  analysis::mma_layout* mma = layouts_->get(x->get_operand(0))->to_mma();\n-  bool is_coalesced_scanline = scanline && (scanline->get_order()[0] == x->get_axis());\n-  bool is_a100_mma = mma && (cc >= 80) && (x->get_axis() == 1);\n-  if(is_coalesced_scanline || is_a100_mma)\n+  bool is_coalesced_scanline = layouts_->is_coalesced_scanline(x);\n+  bool is_a100_mma = layouts_->is_a100_mma(x);\n+  if (is_coalesced_scanline || is_a100_mma)\n     visit_reducend_inst_fast(x, do_acc, neutral);\n   else\n     visit_reducend_inst(x, do_acc, neutral);\n@@ -2562,10 +2836,10 @@ void generator::visit_layout_convert(ir::value *out, ir::value *in){\n   // Orders\n   analysis::distributed_layout* in_layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(in));\n   analysis::distributed_layout* out_layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(out));\n-  auto in_ord = in_layout->get_order();\n-  auto out_ord = out_layout->get_order();\n   Value *base;\n-  base = gep(shmem_, i32(alloc_->offset(layouts_->get(layouts_->tmp(out)))));\n+  int off = alloc_->offset(layouts_->get(layouts_->tmp(out)));\n+   // std::cout << off << std::endl;\n+  base = gep(shmem_, i32(off));\n   base = bit_cast(base, ptr_ty(ty, 3));\n   std::vector<int> n_reps;\n   for(int i = 0; i < shape.size(); i++){\n@@ -2580,9 +2854,16 @@ void generator::visit_layout_convert(ir::value *out, ir::value *in){\n     in_ax.push_back(axes_.at(a_axes_->get(in, d)).values);\n     out_ax.push_back(axes_.at(a_axes_->get(out, d)).values);\n   }\n-  in_ord = in_layout->to_mma() ? out_ord : in_ord;\n-  out_ord = out_layout->to_mma() ? in_ord : out_ord;\n-  int in_vec = out_ord[0] == 0 ? 1 : in_layout->contig_per_thread(in_ord[0]);\n+  auto in_ord =\n+      in_layout->to_mma() ? out_layout->get_order() : in_layout->get_order();\n+  auto out_ord =\n+      out_layout->to_mma() ? in_layout->get_order() : out_layout->get_order();\n+  // out_ord[0] == 0 or in_order[0] == 0 means the first dimension is\n+  // non-contiguous. in_vec can be greater than 0 only if both out_ord[0] and\n+  // and in_ord[0] are contiguous.\n+  int in_vec = out_ord[0] == 0  ? 1\n+               : in_ord[0] == 0 ? 1\n+                                : in_layout->contig_per_thread(in_ord[0]);\n   int out_vec = out_ord[0] == 0 ? 1 : out_layout->contig_per_thread(out_ord[0]);\n   int pad = std::max(in_vec, out_vec);\n   Value *in_ld = i32(shape[in_ord[0]] + pad);\n@@ -2740,15 +3021,26 @@ void generator::visit_copy_to_shared_inst(ir::copy_to_shared_inst* cts) {\n   //\n   int mts_0 = in_layout->shape_per_cta(in_order[0]) / in_layout->contig_per_thread(in_order[0]);\n   int mts_1 = in_layout->shape_per_cta(in_order[1]) / in_layout->contig_per_thread(in_order[1]);\n+  if(in_layout->to_mma()){\n+    mts_0 = 4 * in_layout->to_mma()->wpt(in_order[0]);\n+    mts_1 = 8 * in_layout->to_mma()->wpt(in_order[1]);\n+    per_phase = 1;\n+    max_phase = 8;\n+  }\n \n   int in_ld = in_layout->get_shape()[in_order[0]] / mts_0;\n-  int n_shared_1 = std::max<int>(per_phase*max_phase / mts_1, 1);\n   int n_shared_0 = std::max<int>(in_vec    / out_vec, 1);\n+  int n_shared_1 = std::max<int>(per_phase*max_phase / mts_1, 1);\n+  if(in_layout->to_mma()){\n+    n_shared_0 = 8;\n+    n_shared_1 = 1;\n+  }\n \n   BasicBlock* CurrBB = builder_->GetInsertBlock();\n   BasicBlock* FirstBB = &CurrBB->getParent()->getEntryBlock();\n   auto shapes = cts->get_type()->get_block_shapes();\n \n+\n   // store to shared\n   Value *current = nullptr;\n   std::map<std::pair<int, int>, Value*> ptrs;\n@@ -2763,9 +3055,7 @@ void generator::visit_copy_to_shared_inst(ir::copy_to_shared_inst* cts) {\n       // input ptr info\n       int id_0 = id % (in_ld/min_vec);\n       int id_1 = id / (in_ld/min_vec);\n-      int off_0 = id_0 / n_shared_0 * n_shared_0 * mts_0;\n-      int off_1 = id_1 / n_shared_1 * n_shared_1 * mts_1;\n-      int off = (off_1*shapes[in_order[0]] + off_0);\n+      // std::cout << id_0 << \" \" << id_1 << \" \" << in_ld << \" \" << std::endl;\n       std::pair<int, int> key = {id_1  % n_shared_1, id_0 % n_shared_0};\n       if(ptrs.find(key) == ptrs.end()){\n         if(FirstBB->getTerminator())\n@@ -2784,6 +3074,13 @@ void generator::visit_copy_to_shared_inst(ir::copy_to_shared_inst* cts) {\n         builder_->SetInsertPoint(CurrBB);\n         ptrs[key] = gep(shmems_.at(cts), {off});\n       }\n+      int off_0 = id_0 / n_shared_0 * n_shared_0 * mts_0;\n+      int off_1 = id_1 / n_shared_1 * n_shared_1 * mts_1;\n+      if(in_layout->to_mma()){\n+        off_0 = id_0/n_shared_0*n_shared_0*8;\n+        off_1 = id_1/n_shared_1*n_shared_1*8;\n+      }\n+      int off = (off_1*shapes[in_order[0]] + off_0);\n       Value* ptr = gep(ptrs[key], {i32(off)});\n       ptr = bit_cast(ptr, current->getType()->getPointerTo(3));\n       // asm\n@@ -2962,6 +3259,13 @@ void generator::forward_declare(ir::function* fn){\n   fns_[fn] = ret;\n }\n \n+Value *generator::cast_shared_layout_ptr(analysis::data_layout *layout,\n+                                         Type *ty) {\n+  unsigned addr_space = shmem_->getType()->getPointerAddressSpace();\n+  Value *base = bit_cast(shared_ptr_.at(layout), ptr_ty(ty, addr_space));\n+  return base;\n+}\n+\n void generator::visit_function(ir::function* fn) {\n   idxs_.clear();\n   vals_.clear();\n@@ -3005,7 +3309,7 @@ void generator::visit_function(ir::function* fn) {\n   if(tgt_->as_nvidia()->sm() >= 80)\n   for(ir::load_inst::EVICTION_POLICY evict: {ir::load_inst::EVICT_FIRST, ir::load_inst::EVICT_LAST}){\n     std::string policy = (evict == ir::load_inst::EVICT_FIRST) ? \"evict_first\" : \"evict_last\";\n-    std::string asm_str = \"createpolicy.fractional.L2::\" + policy + \".b64 $0;\";\n+    std::string asm_str = \"createpolicy.fractional.L2::\" + policy + \".b64 $0, 1.0;\";\n     InlineAsm* iasm = InlineAsm::get(FunctionType::get(i64_ty, {}), asm_str, \"=l\", false);\n     policies_[evict] = call(iasm);\n   }\n@@ -3143,36 +3447,36 @@ void generator::visit_layout_mma(analysis::mma_layout* layout) {\n }\n \n void generator::visit_layout_scanline(analysis::scanline_layout* layout) {\n-  Value* u_thread_id = tgt_->get_local_id(mod_, *builder_, 0);\n+  Value* thread_id = tgt_->get_local_id(mod_, *builder_, 0);\n   auto order = layout->get_order();\n   const auto& shape = layout->get_shape();\n   // Delinearize\n   size_t dim = shape.size();\n-  std::vector<Value*> thread_id(dim);\n+  std::vector<Value*> thread_ids(dim);\n   for(unsigned k = 0; k < dim - 1; k++){\n     Constant *dim_k = i32(layout->mts(order[k]));\n-    Value *rem = urem(u_thread_id, dim_k);\n-    u_thread_id = udiv(u_thread_id, dim_k);\n-    thread_id[order[k]] = rem;\n+    Value *rem = urem(thread_id, dim_k);\n+    thread_id = udiv(thread_id, dim_k);\n+    thread_ids[order[k]] = rem;\n   }\n   Constant *dim_k = i32(layout->mts(order[dim - 1]));\n-  thread_id[order[dim - 1]] = urem(u_thread_id, dim_k);\n+  thread_ids[order[dim - 1]] = urem(thread_id, dim_k);\n \n   // Create axes\n   for(unsigned k = 0; k < dim; k++) {\n     int nts = layout->nts(k);\n     int mts = layout->mts(k);\n     std::string str_k = std::to_string(k);\n     Value *contiguous_k = i32(nts);\n-    Value *scaled_thread_id = mul(thread_id[k], contiguous_k);\n+    Value *scaled_thread_ids = mul(thread_ids[k], contiguous_k);\n     unsigned per_cta  = layout->shape_per_cta(k);\n     unsigned per_thread = nts * shape[k] / per_cta;\n     std::vector<Value*> idx_list(per_thread);\n     for(unsigned n = 0 ; n < per_thread; n++){\n       unsigned offset = n / nts * per_cta + n % nts;\n-      idx_list[n] = add(scaled_thread_id, i32(offset), \"idx_\" + str_k + \"_\" + std::to_string(n));\n+      idx_list[n] = add(scaled_thread_ids, i32(offset), \"idx_\" + str_k + \"_\" + std::to_string(n));\n     }\n-    axes_[layout->get_axis(k)] = distributed_axis{nts, idx_list, thread_id[k]};\n+    axes_[layout->get_axis(k)] = distributed_axis{nts, idx_list, thread_ids[k]};\n   }\n }\n \n@@ -3235,7 +3539,6 @@ void generator::visit_basic_block(ir::basic_block * block) {\n   BasicBlock *parent = bbs_[block];\n   builder_->SetInsertPoint(parent);\n   for(ir::instruction *i: block->get_inst_list()){\n-    // i->print(std::cout);\n     visit_value(i);\n     // std::cout << \"done\" << std::endl;\n   }\n@@ -3260,7 +3563,10 @@ void generator::init_idx(ir::value *v) {\n   std::vector<distributed_axis> axes(rank);\n   std::vector<int> ord(rank);\n   // compute axes\n+  // std::cout << \"axes\" << std::endl;\n   for(size_t d = 0; d < shapes.size(); d++){\n+    // std::cout << d << \" \" << shapes[d] << std::endl;\n+    // std::cout << a_axes_->get(v, d) << std::endl;\n     if(shapes[d] > 1){\n       unsigned x = a_axes_->get(v, d);\n       axes[d] = axes_.at(x);\n@@ -3270,6 +3576,7 @@ void generator::init_idx(ir::value *v) {\n       axes[d].values = {i32(0)};\n     }\n   }\n+  // std::cout << \"axes ok\" << std::endl;\n   // compute order\n   analysis::data_layout* layout = layouts_->get(v);\n   std::iota(ord.begin(), ord.end(), 0);\n@@ -3416,6 +3723,7 @@ void generator::finalize_phi_node(ir::phi_node *x) {\n     for(indices_t idx: idxs_.at(x)){\n       PHINode *phi = (PHINode*)vals_[x][idx];\n       Value *inc = vals_[x->get_incoming_value(n)][idx];\n+      // x->print(std::cout);\n       phi->addIncoming(inc, block);\n     }\n   }"}, {"filename": "lib/codegen/transform/coalesce.cc", "status": "modified", "additions": 17, "deletions": 47, "changes": 64, "file_content_changes": "@@ -12,44 +12,8 @@ namespace triton {\n namespace codegen{\n namespace transform{\n \n-coalesce::coalesce(analysis::align* align, analysis::layouts *layouts)\n-  : align_(align), layout_(layouts) { }\n-\n-\n-// simplify layout conversions using the following simple rules:\n-//   - cvt_1(cvt_2(x)) if convert1 is the inverse of convert2\n-//   - cvt_1(elementwise(x, y)) = elementwise(convert(x), convert(y))\n-//ir::value* coalesce::simplify(ir::instruction *inst, ir::builder& builder){\n-//  ir::value* _op = inst->get_operand(0);\n-//  ir::instruction* op = dynamic_cast<ir::instruction*>(_op);\n-//  analysis::mma_layout* mma_in  = layout_->get(op)  ->to_mma();\n-//  analysis::mma_layout* mma_out = layout_->get(inst)->to_mma();\n-//  std::cout << 1 << std::endl;\n-//  // i must be layout conversion instruction\n-//  if(!mma_in && !mma_out)\n-//    return inst;\n-//  //   - cvt_1(cvt_2(x)) if convert1 is the inverse of convert2\n-//  bool is_op_cvt = op->get_id() == ir::INST_CVT_LAYOUT;\n-//  if((mma_in || mma_out) && is_op_cvt &&\n-//     (layout_->get(inst) == layout_->get(op->get_operand(0))))\n-//    return op->get_operand(0);\n-//  //   - cvt_1(elementwise(x, y)) = elementwise(cvt_1(x), cvt_2(y))\n-//  if(op->get_id() != ir::INST_BINOP && op->get_id() != ir::INST_GETELEMENTPTR)\n-//    return inst;\n-//  std::cout << 1 << std::endl;\n-//  for(size_t i = 0; i < op->get_num_operands(); i++){\n-//    ir::value* arg_i = op->get_operand(i);\n-//    builder.set_insert_point(op);\n-//    // create new layout transform\n-//    ir::instruction* new_arg_i = inst->clone();\n-//    builder.insert(new_arg_i);\n-//    // set the right args\n-//    new_arg_i->replace_uses_of_with(new_arg_i->get_operand(0), arg_i);\n-//    op->replace_uses_of_with(arg_i, simplify(new_arg_i, builder));\n-//  }\n-//  std::cout << 2 << std::endl;\n-//  return op;\n-//}\n+coalesce::coalesce(analysis::align* align, analysis::layouts *layouts, bool has_sm80)\n+  : align_(align), layout_(layouts), has_sm80_(has_sm80) { }\n \n void coalesce::run(ir::module &mod) {\n   std::set<analysis::data_layout*> invalidated;\n@@ -62,20 +26,23 @@ void coalesce::run(ir::module &mod) {\n     if(dynamic_cast<ir::store_inst*>(i) || dynamic_cast<ir::atomic_rmw_inst*>(i))\n     if(ir::value* op = i->get_operand(1))\n     if(op->get_type()->is_block_ty())\n-    if(op->get_type()->get_tile_rank() == 2)\n+    if(op->get_type()->get_tile_ranks1() == 2)\n     if(invalidated.find(layout_->get(op)) == invalidated.end())\n-    if(layout_->get(op)->to_mma()){\n+    if(layout_->get(op)->to_mma())\n+    if(dynamic_cast<ir::io_inst*>(i)->get_eviction_policy()==ir::io_inst::NORMAL){\n       ir::instruction* new_op = ir::cvt_layout_inst::create(op);\n       builder.set_insert_point(i);\n       builder.insert(new_op);\n       i->replace_uses_of_with(op, new_op);\n     }\n     // coalesce before copy_to_shared\n-    // It's dirty, but the backend is being rewritten from scratch. :)\n-    if(dynamic_cast<ir::copy_to_shared_inst*>(i))\n+    // only necessary for sm < 80 as Ampere+ can handle reduction\n+    // on MMA layout\n+    if(!has_sm80_)\n+    if(dynamic_cast<ir::copy_to_shared_inst*>(i) || dynamic_cast<ir::reduce_inst*>(i))\n     if(ir::value* op = i->get_operand(0))\n     if(op->get_type()->is_block_ty())\n-    if(op->get_type()->get_tile_rank() == 2)\n+    if(op->get_type()->get_tile_ranks1() == 2)\n     if(invalidated.find(layout_->get(op)) == invalidated.end())\n     if(layout_->get(op)->to_mma()){\n       ir::instruction* new_op = ir::cvt_layout_inst::create(op);\n@@ -88,8 +55,9 @@ void coalesce::run(ir::module &mod) {\n     // uncoalesce after load\n     if(auto x = dynamic_cast<ir::load_inst*>(i))\n     if(x->get_type()->is_block_ty())\n-    if(x->get_type()->get_tile_rank()==2)\n-    if(layout_->get(x)->to_mma()){\n+    if(x->get_type()->get_tile_ranks1()==2)\n+    if(layout_->get(x)->to_mma())\n+    if(!has_sm80_ || dynamic_cast<ir::io_inst*>(i)->get_eviction_policy()==ir::io_inst::NORMAL){\n         builder.set_insert_point_after(x);\n         ir::instruction* new_x = ir::cvt_layout_inst::create(x);\n         builder.insert(new_x);\n@@ -107,9 +75,11 @@ void coalesce::run(ir::module &mod) {\n       auto out_contig = align_->contiguous(ptr);\n       auto val_inst = dynamic_cast<ir::instruction*>(val);\n       if(!val_inst)\n-        break;\n+        continue;\n       if(dynamic_cast<ir::cvt_layout_inst*>(val))\n-        break;\n+        continue;\n+      if(!val->get_type()->is_block_ty() || val->get_type()->get_tile_ranks1()==1)\n+        continue;\n       std::vector<unsigned> in_contig;\n       std::vector<ir::instruction*> queue = {val_inst};\n       std::set<ir::instruction*> seen;"}, {"filename": "lib/codegen/transform/cts.cc", "status": "modified", "additions": 43, "deletions": 22, "changes": 65, "file_content_changes": "@@ -1,26 +1,28 @@\n+#include \"triton/codegen/analysis/layout.h\"\n #include \"triton/codegen/transform/cts.h\"\n #include \"triton/ir/module.h\"\n #include \"triton/ir/function.h\"\n #include \"triton/ir/basic_block.h\"\n #include \"triton/ir/instructions.h\"\n+#include \"triton/ir/utils.h\"\n #include <iostream>\n \n namespace triton {\n namespace codegen{\n namespace transform{\n \n \n-inline bool is_shmem_op(ir::instruction* i, int op) {\n+bool cts::is_shmem_op(ir::instruction* i, int op) {\n   if(i->get_id() == ir::INST_DOT)\n-    return op==0 || op==1;\n+    return op == 0 || op == 1;\n   if(i->get_id() == ir::INST_COPY_FROM_SHARED)\n     return op==0;\n   if(i->get_id() == ir::INST_TRANS)\n     return op==0;\n   return false;\n }\n \n-inline bool is_shmem_res(ir::value* v){\n+bool cts::is_shmem_res(ir::value* v){\n   ir::instruction* i = dynamic_cast<ir::instruction*>(v);\n   if(!i)\n     return false;\n@@ -35,7 +37,7 @@ inline bool is_shmem_res(ir::value* v){\n \n \n // run pass on module\n-void cts::add_copy(ir::instruction *parent, ir::value *x, ir::builder &builder, bool to_shared) {\n+void cts::add_copy(ir::instruction *parent, ir::value *x, ir::builder &builder, bool to_shared, std::map<ir::value*, ir::value*>& copies) {\n   auto *i = dynamic_cast<ir::instruction*>(x);\n   // not an instruction\n   if(!i) {\n@@ -51,7 +53,7 @@ void cts::add_copy(ir::instruction *parent, ir::value *x, ir::builder &builder,\n   // phi node\n   if(auto* phi = dynamic_cast<ir::phi_node*>(x)) {\n     for(unsigned i = 0; i < phi->get_num_incoming(); ++i)\n-      add_copy(phi, phi->get_incoming_value(i), builder, to_shared);\n+      add_copy(phi, phi->get_incoming_value(i), builder, to_shared, copies);\n     return;\n   }\n   // already in shared memory\n@@ -65,30 +67,49 @@ void cts::add_copy(ir::instruction *parent, ir::value *x, ir::builder &builder,\n   }\n   else\n     copy = builder.create_copy_from_shared(x);\n-  parent->replace_uses_of_with(x, copy);\n+  copies.insert({x, copy});\n+  parent->replace_uses_of_with(x, copies.at(x));\n }\n \n void cts::run(ir::module &mod) {\n+  // Precompute where copies should be added\n+  std::set<ir::value*> shmem_ops;\n+  std::set<ir::value*> shmem_res;\n+  ir::for_each_instruction(mod, [&](ir::instruction* i) {\n+    if(i->get_id() == ir::INST_DOT){\n+      ir::dot_inst* dot = dynamic_cast<ir::dot_inst*>(i);\n+      ir::value* lhs = i->get_operand(0);\n+      ir::type* ty = lhs->get_type()->get_scalar_ty();\n+      analysis::mma_layout* mma_lhs = layouts_->get(lhs)->to_mma();\n+      // TODO: V100\n+      bool is_lhs_shmem = !(mma_lhs && has_sm80_ && ty->get_primitive_size_in_bits() == 16 && !dot->is_trans_a());\n+      if(is_lhs_shmem)\n+        shmem_ops.insert(lhs);\n+      shmem_ops.insert(i->get_operand(1));\n+    }\n+    if(i->get_id() == ir::INST_COPY_FROM_SHARED)\n+      shmem_ops.insert(i->get_operand(0));\n+    if(i->get_id() == ir::INST_TRANS)\n+      shmem_ops.insert(i->get_operand(0));\n+    if(i->get_id() == ir::INST_TRANS ||\n+       i->get_id() == ir::INST_COPY_TO_SHARED ||\n+       i->get_id() == ir::INST_MASKED_LOAD_ASYNC)\n+      shmem_res.insert(i);\n+  });\n+\n   // Add shared copies\n+  std::map<ir::value*, ir::value*> copies;\n   ir::builder &builder = mod.get_builder();\n-  for(ir::function* fn: mod.get_function_list()){\n-    for(ir::basic_block* block: fn->blocks())\n-    for(ir::instruction* i: block->get_inst_list()){\n-      size_t num_op = i->get_num_operands();\n+  ir::for_each_instruction(mod, [&](ir::instruction* i) {\n+    size_t num_op = i->get_num_operands();\n+    for(size_t k = 0; k < num_op; k++){\n+      ir::value* op = i->get_operand(k);\n       // copy to shared operands\n-      for(size_t k = 0; k < num_op; k++)\n-        if(is_shmem_op(i, k)){\n-          add_copy(i, i->get_operand(k), builder, true);\n-        }\n-      // copy from shared operands\n-      for(size_t k = 0; k < num_op; k++)\n-        if(!dynamic_cast<ir::phi_node*>(i) &&\n-           !is_shmem_op(i,k) &&\n-           is_shmem_res(i->get_operand(k))){\n-          add_copy(i, i->get_operand(k), builder, false);\n-        }\n+      bool is_shmem_op = shmem_ops.find(op) != shmem_ops.end();\n+      if(is_shmem_op)\n+        add_copy(i, op, builder, true, copies);\n     }\n-  }\n+  });\n }\n \n "}, {"filename": "lib/codegen/transform/membar.cc", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "file_content_changes": "@@ -60,15 +60,22 @@ membar::val_set_t membar::intersect_with(const val_set_t& as, const val_set_t& b\n       continue;\n     analysis::shared_layout* a_layout = layouts_->get(a)->to_shared();\n     analysis::shared_layout* a_tmp = layouts_->has_tmp(a) ? layouts_->get(layouts_->tmp(a))->to_shared() : nullptr;\n+    analysis::shared_layout* a_tmp_index = layouts_->has_tmp_index(a) ? layouts_->get(layouts_->tmp_index(a))->to_shared() : nullptr;\n     for(ir::value* b: bs){\n       if(!b->get_type()->is_block_ty())\n         continue;\n       analysis::shared_layout* b_layout = layouts_->get(b)->to_shared();\n       analysis::shared_layout* b_tmp = layouts_->has_tmp(b) ? layouts_->get(layouts_->tmp(b))->to_shared() : nullptr;\n+      analysis::shared_layout* b_tmp_index = layouts_->has_tmp_index(b) ? layouts_->get(layouts_->tmp_index(b))->to_shared() : nullptr;\n       if(intersect_with(a_layout, b_layout) ||\n          intersect_with(a_layout, b_tmp) ||\n+         intersect_with(a_layout, b_tmp_index) ||\n          intersect_with(a_tmp, b_layout) ||\n-         intersect_with(a_tmp, b_tmp))\n+         intersect_with(a_tmp, b_tmp) ||\n+         intersect_with(a_tmp, b_tmp_index) ||\n+         intersect_with(a_tmp_index, b_layout) ||\n+         intersect_with(a_tmp_index, b_tmp) ||\n+         intersect_with(a_tmp_index, b_tmp_index))\n         ret.insert(b);\n     }\n   }"}, {"filename": "lib/codegen/transform/peephole.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -87,7 +87,7 @@ bool peephole::rewrite_dot(ir::instruction *value, ir::builder& builder){\n     ir::value *a = dot->get_operand(0);\n     ir::value *b = dot->get_operand(1);\n     builder.set_insert_point(add);\n-    ir::value * new_dot = builder.insert(ir::dot_inst::create_nn(a, b, other, dot->allow_tf32(), dot->get_name()));\n+    ir::value * new_dot = builder.insert(ir::dot_inst::create(a, b, other, dot->is_trans_a(), dot->is_trans_b(), dot->allow_tf32(), dot->get_name()));\n     add->replace_all_uses_with(new_dot);\n     return true;\n   }"}, {"filename": "lib/ir/basic_block.cc", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -26,7 +26,10 @@ void basic_block::replace_phi_uses_with(basic_block* before, basic_block* after)\n     auto* curr_phi = dynamic_cast<ir::phi_node*>(i);\n     if(!curr_phi)\n       break;\n-    curr_phi->replace_uses_of_with(before, after);\n+    // curr_phi->replace_uses_of_with(before, after);\n+    for (size_t idx = 0; idx < curr_phi->get_num_incoming(); ++idx)\n+    if (curr_phi->get_incoming_block(idx) == before)\n+      curr_phi->set_incoming_block(idx, after);\n   }\n }\n "}, {"filename": "lib/ir/builder.cc", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -299,16 +299,16 @@ value *builder::create_load(value *ptr, load_inst::CACHE_MODIFIER cache, load_in\n   return insert(unmasked_load_inst::create(ptr, cache, eviction, is_volatile));\n }\n \n-value *builder::create_store(value *ptr, value *val){\n-  return insert(unmasked_store_inst::create(ptr, val));\n+value *builder::create_store(value *ptr, value *val, store_inst::EVICTION_POLICY eviction){\n+  return insert(unmasked_store_inst::create(ptr, val, eviction));\n }\n \n value *builder::create_masked_load(value *ptr, value *mask, value *false_value, load_inst::CACHE_MODIFIER cache, load_inst::EVICTION_POLICY eviction, bool is_volatile){\n   return insert(masked_load_inst::create(ptr, mask, false_value, cache, eviction, is_volatile));\n }\n \n-value *builder::create_masked_store(value *ptr, value *val, value *mask){\n-  return insert(masked_store_inst::create(ptr, val, mask));\n+value *builder::create_masked_store(value *ptr, value *val, value *mask, store_inst::EVICTION_POLICY eviction){\n+  return insert(masked_store_inst::create(ptr, val, mask, eviction));\n }\n \n //===----------------------------------------------------------------------===//\n@@ -425,8 +425,8 @@ value *builder::create_log(value *arg){\n   return insert(log_inst::create(arg));\n }\n \n-value *builder::create_dot(value *A, value *B, value *C, bool allow_tf32) {\n-  return insert(dot_inst::create_nn(A, B, C, allow_tf32));\n+value *builder::create_dot(value *A, value *B, value *C, bool trans_a, bool trans_b, bool allow_tf32) {\n+  return insert(dot_inst::create(A, B, C, trans_a, trans_b, allow_tf32));\n }\n \n value *builder::create_trans(value *A, const std::vector<int>& perm) {"}, {"filename": "lib/ir/instructions.cc", "status": "modified", "additions": 16, "deletions": 14, "changes": 30, "file_content_changes": "@@ -69,6 +69,7 @@ void phi_node::set_incoming_block(unsigned i, basic_block *block){\n \n // Add incoming\n void phi_node::add_incoming(value *v, basic_block *block){\n+  assert(v && \"PHI node got a null value!!\");\n   resize_ops(get_num_operands() + 1);\n   blocks_.resize(get_num_operands() + 1);\n   set_incoming_value(get_num_operands() - 1, v);\n@@ -494,13 +495,13 @@ getelementptr_inst *getelementptr_inst::create(value *ptr, const std::vector<val\n //===----------------------------------------------------------------------===//\n \n // io_inst\n-io_inst::io_inst(type *ty, value_id_t id, unsigned num_ops, const std::string &name, instruction *next)\n-  : instruction(ty, id, num_ops, name, next)\n+io_inst::io_inst(type *ty, value_id_t id, unsigned num_ops, EVICTION_POLICY eviction, const std::string &name, instruction *next)\n+  : instruction(ty, id, num_ops, name, next), eviction_(eviction)\n { }\n \n // load_inst\n load_inst::load_inst(value *ptr, value_id_t id, unsigned num_ops, load_inst::CACHE_MODIFIER cache, EVICTION_POLICY eviction, bool is_volatile, const std::string &name, instruction *next)\n-  : io_inst(get_pointee_type(ptr->get_type()), id, num_ops, name, next), cache_(cache), eviction_(eviction), is_volatile_(is_volatile)\n+  : io_inst(get_pointee_type(ptr->get_type()), id, num_ops, eviction, name, next), cache_(cache), is_volatile_(is_volatile)\n { }\n \n // load\n@@ -557,34 +558,35 @@ masked_load_async_inst* masked_load_async_inst::create(value *ptr, value *mask,\n \n // store\n \n-store_inst::store_inst(value *ptr, value_id_t id, unsigned num_ops, const std::string &name, instruction *next)\n-  : io_inst(type::get_void_ty(ptr->get_type()->get_context()), id, num_ops, name, next)\n+store_inst::store_inst(value *ptr, value_id_t id, unsigned num_ops, EVICTION_POLICY eviction, const std::string &name, instruction *next)\n+  : io_inst(type::get_void_ty(ptr->get_type()->get_context()), id, num_ops, eviction, name, next)\n { }\n \n // unmasked_store\n-unmasked_store_inst::unmasked_store_inst(value *ptr, value *val,\n+unmasked_store_inst::unmasked_store_inst(value *ptr, value *val, EVICTION_POLICY eviction,\n                                          const std::string &name, instruction *next)\n-    : store_inst(ptr, INST_UNMASKED_STORE, 2, name, next)  {\n+    : store_inst(ptr, INST_UNMASKED_STORE, 2, eviction, name, next)  {\n   set_operand(0, ptr);\n   set_operand(1, val);\n }\n \n-unmasked_store_inst* unmasked_store_inst::create(value *ptr, value *val,\n+unmasked_store_inst* unmasked_store_inst::create(value *ptr, value *val, EVICTION_POLICY eviction,\n                                                  const std::string &name, instruction *next) {\n-  return new unmasked_store_inst(ptr, val, name, next);\n+  return new unmasked_store_inst(ptr, val, eviction, name, next);\n }\n \n // masked store\n-masked_store_inst::masked_store_inst(value *ptr, value *val, value *mask,\n+masked_store_inst::masked_store_inst(value *ptr, value *val, value *mask, EVICTION_POLICY eviction,\n                                      const std::string &name, instruction *next)\n-  : store_inst(ptr, INST_MASKED_STORE, 3, name, next) {\n+  : store_inst(ptr, INST_MASKED_STORE, 3, eviction, name, next) {\n   set_operand(0, ptr);\n   set_operand(1, val);\n   set_operand(2, mask);\n }\n \n-masked_store_inst* masked_store_inst::create(value *ptr, value *val, value *mask, const std::string &name, instruction *next)  {\n-  return new masked_store_inst(ptr, val, mask, name, next);\n+masked_store_inst* masked_store_inst::create(value *ptr, value *val, value *mask, EVICTION_POLICY eviction, \n+                                             const std::string &name, instruction *next)  {\n+  return new masked_store_inst(ptr, val, mask, eviction, name, next);\n }\n \n //===----------------------------------------------------------------------===//\n@@ -679,7 +681,7 @@ instruction* downcast_inst::create(value *arg, const std::string &name, instruct\n \n dot_inst::dot_inst(value *A, value *B, value *C, TransT AT, TransT BT, bool allow_tf32,\n                          const std::string &name, instruction *next)\n-    : builtin_inst(C->get_type(), INST_DOT, 3, name, next) {\n+    : builtin_inst(C->get_type(), INST_DOT, 3, name, next), AT_(AT), BT_(BT){\n   set_operand(0, A);\n   set_operand(1, B);\n   set_operand(2, C);"}, {"filename": "lib/ir/utils.cc", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -43,6 +43,15 @@ std::vector<basic_block*> cfg::reverse_post_order(function* fn) {\n   return result;\n }\n \n+void for_each_instruction_backward(module &mod, const std::function<void (instruction *)> &do_work) {\n+  for(ir::function *fn: mod.get_function_list())\n+  for(ir::basic_block *block: cfg::post_order(fn)){\n+    auto inst_list = block->get_inst_list();\n+    for(auto it = inst_list.rbegin(); it != inst_list.rend() ; it++)\n+      do_work(*it);\n+  }\n+}\n+\n void for_each_instruction(module &mod, const std::function<void (instruction *)> &do_work) {\n   for(ir::function *fn: mod.get_function_list())\n   for(ir::basic_block *block: cfg::reverse_post_order(fn))"}, {"filename": "python/bench/bench_matmul.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -5,7 +5,7 @@\n \n def rounded_linspace(low, high, steps, div):\n     ret = torch.linspace(low, high, steps)\n-    ret = (ret.int() + div - 1) // div * div\n+    ret = torch.div(ret.int() + div - 1, div, rounding_mode='trunc') * div\n     ret = torch.unique(ret)\n     return list(map(int, ret))\n "}, {"filename": "python/setup.py", "status": "modified", "additions": 15, "deletions": 8, "changes": 23, "file_content_changes": "@@ -7,14 +7,27 @@\n import subprocess\n import sys\n import tarfile\n-import tempfile\n import urllib.request\n from distutils.version import LooseVersion\n \n from setuptools import Extension, setup\n from setuptools.command.build_ext import build_ext\n \n \n+# Taken from https://github.com/pytorch/pytorch/blob/master/tools/setup_helpers/env.py\n+def check_env_flag(name: str, default: str = \"\") -> bool:\n+    return os.getenv(name, default).upper() in [\"ON\", \"1\", \"YES\", \"TRUE\", \"Y\"]\n+\n+\n+def get_build_type():\n+    if check_env_flag(\"DEBUG\"):\n+        return \"Debug\"\n+    elif check_env_flag(\"REL_WITH_DEB_INFO\"):\n+        return \"RelWithDebInfo\"\n+    else:\n+        return \"Release\"\n+\n+\n def get_llvm():\n     # tries to find system LLVM\n     versions = ['-11.0', '-11', '-11-64']\n@@ -80,15 +93,10 @@ def run(self):\n \n     def build_extension(self, ext):\n         llvm_include_dir, llvm_library_dir = get_llvm()\n-        self.debug = True\n         extdir = os.path.abspath(os.path.dirname(self.get_ext_fullpath(ext.path)))\n         # create build directories\n-        build_suffix = 'debug' if self.debug else 'release'\n-        llvm_build_dir = os.path.join(tempfile.gettempdir(), \"llvm-\" + build_suffix)\n         if not os.path.exists(self.build_temp):\n             os.makedirs(self.build_temp)\n-        if not os.path.exists(llvm_build_dir):\n-            os.makedirs(llvm_build_dir)\n         # python directories\n         python_include_dirs = [distutils.sysconfig.get_python_inc()] + ['/usr/local/cuda/include']\n         cmake_args = [\n@@ -99,11 +107,10 @@ def build_extension(self, ext):\n             \"-DLLVM_LIBRARY_DIR=\" + llvm_library_dir,\n             # '-DPYTHON_EXECUTABLE=' + sys.executable,\n             # '-DCMAKE_VERBOSE_MAKEFILE:BOOL=ON',\n-            \"-DTRITON_LLVM_BUILD_DIR=\" + llvm_build_dir,\n             \"-DPYTHON_INCLUDE_DIRS=\" + \";\".join(python_include_dirs)\n         ]\n         # configuration\n-        cfg = \"Debug\" if self.debug else \"Release\"\n+        cfg = get_build_type()\n         build_args = [\"--config\", cfg]\n \n         if platform.system() == \"Windows\":"}, {"filename": "python/src/functions.h", "status": "modified", "additions": 23, "deletions": 3, "changes": 26, "file_content_changes": "@@ -353,9 +353,6 @@ ir::value *sqrt(ir::value *input, ir::builder *builder) {\n   return builder->create_sqrt(input);\n };\n \n-/*----------------------------------------------\n- definition of triton.min\n- ----------------------------------------------*/\n ir::value *reduce_impl(ir::value *input, unsigned int axis, ir::builder *builder, const std::string &name,\n                        ir::reduce_inst::op_t FLOAT_OP, ir::reduce_inst::op_t INT_OP) {\n   ir::type *scalar_ty = input->get_type()->get_scalar_ty();\n@@ -367,13 +364,26 @@ ir::value *reduce_impl(ir::value *input, unsigned int axis, ir::builder *builder\n     throw_not_int_or_float(name);\n }\n \n+/*----------------------------------------------\n+ definition of triton.min\n+ ----------------------------------------------*/\n std::string min_docstr = R\"pbdoc(\n     Returns the minimum value of `input`.\n  )pbdoc\";\n ir::value *min(ir::value *input, unsigned int axis, ir::builder *builder) {\n   return reduce_impl(input, axis, builder, \"min\", ir::reduce_inst::FMIN, ir::reduce_inst::MIN);\n };\n \n+/*----------------------------------------------\n+ definition of triton.arg_min\n+ ----------------------------------------------*/\n+std::string min_docstr = R\"pbdoc(\n+    Returns the minimum value's index of `input`.\n+ )pbdoc\";\n+ir::value *argmin(ir::value *input, unsigned int axis, ir::builder *builder) {\n+  return reduce_impl(input, axis, builder, \"argmin\", ir::reduce_inst::ARGFMIN, ir::reduce_inst::ARGMIN);\n+};\n+\n /*----------------------------------------------\n  definition of triton.max\n  ----------------------------------------------*/\n@@ -384,6 +394,16 @@ ir::value *max(ir::value *input, unsigned int axis, ir::builder *builder) {\n   return reduce_impl(input, axis, builder, \"max\", ir::reduce_inst::FMAX, ir::reduce_inst::MAX);\n };\n \n+/*----------------------------------------------\n+ definition of triton.arg_max\n+ ----------------------------------------------*/\n+std::string max_docstr = R\"pbdoc(\n+    Returns the maximum value's index of `input`.\n+ )pbdoc\";\n+ir::value *argmax(ir::value *input, unsigned int axis, ir::builder *builder) {\n+  return reduce_impl(input, axis, builder, \"argmax\", ir::reduce_inst::ARGFMAX, ir::reduce_inst::ARGMAX);\n+};\n+\n /*----------------------------------------------\n  definition of triton.sum\n  ----------------------------------------------*/"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -573,8 +573,14 @@ void init_triton_ir(py::module &&m) {\n       .value(\"MAX\", ir::reduce_inst::MAX)\n       .value(\"UMIN\", ir::reduce_inst::UMIN)\n       .value(\"UMAX\", ir::reduce_inst::UMAX)\n+      .value(\"ARGMIN\", ir::reduce_inst::ARGMIN)\n+      .value(\"ARGMAX\", ir::reduce_inst::ARGMAX)\n+      .value(\"ARGUMIN\", ir::reduce_inst::ARGUMIN)\n+      .value(\"ARGUMAX\", ir::reduce_inst::ARGUMAX)\n       .value(\"FMIN\", ir::reduce_inst::FMIN)\n       .value(\"FMAX\", ir::reduce_inst::FMAX)\n+      .value(\"ARGFMIN\", ir::reduce_inst::ARGFMIN)\n+      .value(\"ARGFMAX\", ir::reduce_inst::ARGFMAX)\n       .value(\"XOR\", ir::reduce_inst::XOR);\n   \n   py::enum_<ir::atomic_rmw_op_t>(m, \"ATOMIC_OP\")"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 148, "deletions": 34, "changes": 182, "file_content_changes": "@@ -386,6 +386,8 @@ def test_index1d(expr, dtype_str, device='cuda'):\n     rank_y = expr.count(',') + 1\n     shape_x = [32 for _ in range(rank_x)]\n     shape_z = [32 for _ in range(rank_y)]\n+    shape_z_rank_mismatch = [32 for _ in range(rank_y + 1)]\n+    shape_z_dim_mismatch = [64 for _ in range(rank_y)]\n \n     # Triton kernel\n     @triton.jit\n@@ -396,12 +398,17 @@ def kernel(Z, X, SIZE: tl.constexpr):\n         z = GENERATE_TEST_HERE\n         tl.store(Z_PTR_EXPR, z)\n \n-    to_replace = {\n-        'X_PTR_EXPR': make_ptr_str('X', shape_x),\n-        'Z_PTR_EXPR': make_ptr_str('Z', shape_z),\n-        'GENERATE_TEST_HERE': expr,\n-    }\n-    kernel = patch_kernel(kernel, to_replace)\n+    def generate_kernel(shape_x, shape_z):\n+        to_replace = {\n+            'X_PTR_EXPR': make_ptr_str('X', shape_x),\n+            'Z_PTR_EXPR': make_ptr_str('Z', shape_z),\n+            'GENERATE_TEST_HERE': expr,\n+        }\n+        return patch_kernel(kernel, to_replace)\n+\n+    kernel_match = generate_kernel(shape_x, shape_z)\n+    kernel_dim_mismatch = generate_kernel(shape_x, shape_z_dim_mismatch)\n+    kernel_rank_mismatch = generate_kernel(shape_x, shape_z_rank_mismatch)\n \n     # torch result\n     x = numpy_random(shape_x, dtype_str=dtype_str)\n@@ -410,10 +417,21 @@ def kernel(Z, X, SIZE: tl.constexpr):\n     # triton result\n     z_tri = to_triton(np.empty_like(z_ref), device=device)\n     x_tri = to_triton(x)\n-    kernel[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n+    kernel_match[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n     # compare\n     assert (z_ref == to_numpy(z_tri)).all()\n \n+    def catch_compilation_error(kernel):\n+        try:\n+            kernel[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n+        except triton.code_gen.CompilationError as e:\n+            np.testing.assert_(True)\n+        except BaseException:\n+            np.testing.assert_(False)\n+\n+    catch_compilation_error(kernel_dim_mismatch)\n+    catch_compilation_error(kernel_rank_mismatch)\n+\n \n # ---------------\n # test tuples\n@@ -515,6 +533,29 @@ def kernel(X, Z):\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n \n \n+@pytest.mark.parametrize(\"axis\", [0, 1])\n+def test_tensor_atomic_rmw(axis, device=\"cuda\"):\n+    shape0, shape1 = 8, 8\n+    # triton kernel\n+\n+    @triton.jit\n+    def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n+        off0 = tl.arange(0, SHAPE0)\n+        off1 = tl.arange(0, SHAPE1)\n+        x = tl.load(X + off0[:, None] * SHAPE1 + off1[None, :])\n+        z = tl.sum(x, axis=AXIS)\n+        tl.atomic_add(Z + off0, z)\n+    rs = RandomState(17)\n+    x = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n+    # reference result\n+    z_ref = np.sum(x, axis=axis)\n+    # triton result\n+    x_tri = to_triton(x, device=device)\n+    z_tri = to_triton(np.zeros((shape0,), dtype=\"float32\"), device=device)\n+    kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n+\n+\n def test_atomic_cas():\n     # 1. make sure that atomic_cas changes the original value (Lock)\n     @triton.jit\n@@ -691,7 +732,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n \n @pytest.mark.parametrize(\"op, dtype_str, shape\",\n                          [(op, dtype, shape)\n-                          for op in ['min', 'max', 'sum']\n+                          for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n                           for dtype in dtypes\n                           for shape in [32, 64, 128, 512]])\n def test_reduce1d(op, dtype_str, shape, device='cuda'):\n@@ -708,28 +749,37 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random((shape,), dtype_str=dtype_str, rs=rs)\n     x_tri = to_triton(x, device=device)\n-    numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min}[op]\n+    numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n+                'argmin': np.argmin, 'argmax': np.argmax}[op]\n     # numpy result\n-    z_ref = numpy_op(x).astype(getattr(np, dtype_str))\n+    z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n+    z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n     # triton result\n-    z_tri = to_triton(numpy_random((1,), dtype_str=dtype_str, rs=rs), device=device)\n+    z_tri = to_triton(numpy_random((1,), dtype_str=z_dtype_str, rs=rs), device=device)\n     kernel[(1,)](x_tri, z_tri, BLOCK=shape)\n+    z_tri = to_numpy(z_tri)\n     # compare\n     if op == 'sum':\n-        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+        np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n     else:\n-        np.testing.assert_equal(z_ref, to_numpy(z_tri))\n+        if op == 'argmin' or op == 'argmax':\n+            # argmin and argmax can have multiple valid indices.\n+            # so instead we compare the values pointed by indices\n+            np.testing.assert_equal(x[z_ref], x[z_tri])\n+        else:\n+            np.testing.assert_equal(z_ref, z_tri)\n \n \n reduce_configs1 = [\n     (op, dtype, (1, 1024), axis) for dtype in dtypes\n-    for op in ['min', 'max', 'sum']\n+    for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n     for axis in [1]\n ]\n reduce_configs2 = [\n-    (op, 'float32', shape, 1)\n-    for op in ['min', 'max', 'sum']\n+    (op, 'float32', shape, axis)\n+    for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n     for shape in [(2, 32), (4, 32), (4, 128), (32, 64), (64, 128), (128, 256), (32, 1024)]\n+    for axis in [0, 1]\n ]\n \n \n@@ -742,25 +792,46 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n         range_n = tl.arange(0, BLOCK_N)\n         x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n         z = GENERATE_TEST_HERE\n-        tl.store(Z + range_m, z)\n+        if AXIS == 1:\n+            tl.store(Z + range_m, z)\n+        else:\n+            tl.store(Z + range_n, z)\n \n     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=AXIS)'})\n     # input\n     rs = RandomState(17)\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n     x_tri = to_triton(x)\n-    numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min}[op]\n+    numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n+                'argmin': np.argmin, 'argmax': np.argmax}[op]\n+    z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n     # numpy result\n-    z_ref = numpy_op(x, axis=axis).astype(getattr(np, dtype_str))\n+    z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n     # triton result\n+<<<<<<< HEAD\n     z_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device=device)\n     kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n+=======\n+    z_tri = to_triton(numpy_random((shape[1 - axis],), dtype_str=z_dtype_str, rs=rs),\n+                      device=device)\n+    kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n+    z_tri = to_numpy(z_tri)\n+>>>>>>> origin/master\n     # compare\n     if op == 'sum':\n-        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+        np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n     else:\n-        np.testing.assert_equal(z_ref, to_numpy(z_tri))\n+        if op == 'argmin' or op == 'argmax':\n+            # argmin and argmax can have multiple valid indices.\n+            # so instead we compare the values pointed by indices\n+            z_ref_index = np.expand_dims(z_ref, axis=axis)\n+            z_tri_index = np.expand_dims(z_tri, axis=axis)\n+            z_ref_value = np.take_along_axis(x, z_ref_index, axis=axis)\n+            z_tri_value = np.take_along_axis(x, z_tri_index, axis=axis)\n+            np.testing.assert_equal(z_ref_value, z_tri_value)\n+        else:\n+            np.testing.assert_equal(z_ref, z_tri)\n \n # ---------------\n # test permute\n@@ -769,8 +840,8 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n \n @pytest.mark.parametrize(\"dtype_str, shape, perm\",\n                          [(dtype, shape, perm)\n-                          for dtype in ['float32']\n-                             for shape in [(128, 128)]\n+                          for dtype in ['float16', 'float32']\n+                             for shape in [(64, 64), (128, 128)]\n                              for perm in [(1, 0)]])\n def test_permute(dtype_str, shape, perm, device='cuda'):\n \n@@ -788,18 +859,26 @@ def kernel(X, stride_xm, stride_xn,\n     x = numpy_random(shape, dtype_str=dtype_str)\n     # triton result\n     z_tri = to_triton(np.empty_like(x), device=device)\n+    z_tri_contiguous = to_triton(np.empty_like(x), device=device)\n     x_tri = to_triton(x, device=device)\n     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n                          z_tri, z_tri.stride(1), z_tri.stride(0),\n                          BLOCK_M=shape[0], BLOCK_N=shape[1])\n+    pgm_contiguous = kernel[(1, 1)](x_tri, x_tri.stride(1), x_tri.stride(0),\n+                                    z_tri_contiguous, z_tri_contiguous.stride(0), z_tri_contiguous.stride(1),\n+                                    BLOCK_M=shape[0], BLOCK_N=shape[1])\n     # torch result\n     z_ref = x.transpose(*perm)\n     # compare\n     triton.testing.assert_almost_equal(z_tri, z_ref)\n+    triton.testing.assert_almost_equal(z_tri_contiguous, z_ref)\n     # parse ptx to make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']\n     assert 'ld.global.v4' in ptx\n     assert 'st.global.v4' in ptx\n+    ptx = pgm_contiguous.asm['ptx']\n+    assert 'ld.global.v4' in ptx\n+    assert 'st.global.v4' in ptx\n \n # ---------------\n # test dot\n@@ -808,10 +887,10 @@ def kernel(X, stride_xm, stride_xn,\n \n @pytest.mark.parametrize(\"epilogue, allow_tf32, dtype\",\n                          [(epilogue, allow_tf32, dtype)\n-                          for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols']\n+                          for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n                           for allow_tf32 in [True, False]\n-                          for dtype in ['float32', 'int8']\n-                          if not (allow_tf32 and (dtype == 'int8'))])\n+                          for dtype in ['float16']\n+                          if not (allow_tf32 and (dtype in ['float16']))])\n def test_dot(epilogue, allow_tf32, dtype, device='cuda'):\n     cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n     if cc < 80:\n@@ -820,21 +899,30 @@ def test_dot(epilogue, allow_tf32, dtype, device='cuda'):\n         elif dtype == 'float32' and allow_tf32:\n             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n \n+    M, N, K = 128, 128, 64\n+    num_warps = 8\n+    trans_a, trans_b = False, False\n+\n     # triton kernel\n     @triton.jit\n     def kernel(X, stride_xm, stride_xk,\n                Y, stride_yk, stride_yn,\n+               W, stride_wn, stride_wl,\n                Z, stride_zm, stride_zn,\n                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n-               ALLOW_TF32: tl.constexpr):\n+               ALLOW_TF32: tl.constexpr,\n+               DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n+               TRANS_A: tl.constexpr, TRANS_B: tl.constexpr):\n         off_m = tl.arange(0, BLOCK_M)\n         off_n = tl.arange(0, BLOCK_N)\n+        off_l = tl.arange(0, BLOCK_N)\n         off_k = tl.arange(0, BLOCK_K)\n         Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk\n         Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn\n+        Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl\n         Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n-        z = tl.dot(tl.load(Xs), tl.load(Ys), allow_tf32=ALLOW_TF32)\n+        z = tl.dot(tl.load(Xs), tl.load(Ys), trans_a=TRANS_A, trans_b=TRANS_B, allow_tf32=ALLOW_TF32)\n         if ADD_MATRIX:\n             z += tl.load(Zs)\n         if ADD_ROWS:\n@@ -843,39 +931,65 @@ def kernel(X, stride_xm, stride_xk,\n         if ADD_COLS:\n             ZCs = Z + off_n * stride_zn\n             z += tl.load(ZCs)[None, :]\n+        if DO_SOFTMAX:\n+            max = tl.max(z, 1)\n+            z = z - max[:, None]\n+            num = tl.exp(z)\n+            den = tl.sum(num, 1)\n+            z = num / den[:, None]\n+        if CHAIN_DOT:\n+            # tl.store(Zs, z)\n+            # tl.debug_barrier()\n+            z = tl.dot(z.to(tl.float16), tl.load(Ws), trans_a=TRANS_A)\n         tl.store(Zs, z)\n     # input\n-    M, N, K = 64, 64, 32\n     rs = RandomState(17)\n-    x = numpy_random((M, K), dtype_str=dtype, rs=rs)\n-    y = numpy_random((K, N), dtype_str=dtype, rs=rs)\n+    x = numpy_random((K, M) if trans_a else (M, K), dtype_str=dtype, rs=rs) * .1\n+    y = numpy_random((N, K) if trans_b else (K, N), dtype_str=dtype, rs=rs) * .1\n+    w = numpy_random((N, N), dtype_str=dtype, rs=rs) * .1\n     if allow_tf32:\n         x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n         y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+        w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n     x_tri = to_triton(x, device=device)\n     y_tri = to_triton(y, device=device)\n+    w_tri = to_triton(w, device=device)\n     # triton result\n-    z = numpy_random((M, N), dtype_str=dtype, rs=rs)\n+    z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs) * .1\n     z_tri = to_triton(z, device=device)\n     if epilogue == 'trans':\n         z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])\n     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n                          y_tri, y_tri.stride(0), y_tri.stride(1),\n+                         w_tri, w_tri.stride(0), w_tri.stride(1),\n                          z_tri, z_tri.stride(0), z_tri.stride(1),\n+                         TRANS_A=trans_a, TRANS_B=trans_b,\n                          BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n                          ADD_MATRIX=epilogue == 'add-matrix',\n                          ADD_ROWS=epilogue == 'add-rows',\n                          ADD_COLS=epilogue == 'add-cols',\n-                         ALLOW_TF32=allow_tf32)\n+                         DO_SOFTMAX=epilogue == 'softmax',\n+                         CHAIN_DOT=epilogue == 'chain-dot',\n+                         ALLOW_TF32=allow_tf32,\n+                         num_warps=num_warps)\n     # torch result\n-    z_ref = np.matmul(x, y)\n+    x_ref = x.T if trans_a else x\n+    y_ref = y.T if trans_b else y\n+    z_ref = np.matmul(x_ref, y_ref)\n     if epilogue == 'add-matrix':\n         z_ref += z\n     if epilogue == 'add-rows':\n         z_ref += z[:, 0][:, None]\n     if epilogue == 'add-cols':\n         z_ref += z[0, :][None, :]\n+    if epilogue == 'softmax':\n+        num = np.exp(z_ref - np.max(z_ref, axis=-1, keepdims=True))\n+        denom = np.sum(num, axis=-1, keepdims=True)\n+        z_ref = num / denom\n+    if epilogue == 'chain-dot':\n+        z_ref = np.matmul(z_ref.T if trans_a else z_ref, w)\n     # compare\n+    # print(z_ref[:,0], z_tri[:,0])\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n     # make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']"}, {"filename": "python/triton/code_gen.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -211,7 +211,7 @@ def _try_remove_trivial_phi(self, phi: triton.language.tensor) -> triton.languag\n             return phi\n         v = unique_handles.pop()\n         phi.handle.replace_all_uses_with(v)\n-        phi.handle.erase_from_parent()\n+        # phi.handle.erase_from_parent()\n         # TODO: remove trivial phis recursively\n         return triton.language.tensor(v, phi.type)\n \n@@ -955,7 +955,7 @@ def add_to_cache(self, key, wargs, device_idx, num_warps, num_stages):\n         return self.fn._warmup(key, arg_types=arg_types, device=device_idx, attributes=attributes, constants=constants, num_warps=num_warps, num_stages=num_stages, is_manual_warmup=False)\n \n     def __call__(self, *wargs, grid, num_warps=4, num_stages=2, **kwargs):\n-        assert num_warps != 0 and (num_warps & (num_warps - 1)) == 0, f\"{num_warps=} must be a power of 2.\"\n+        assert num_warps != 0 and (num_warps & (num_warps - 1)) == 0, f\"num_warps={num_warps} must be a power of 2.\"\n         # handle arguments passed by name\n         kwargs = {self.fn.arg_names.index(name): value for name, value in kwargs.items()}\n         wargs = list(wargs)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 52, "deletions": 16, "changes": 68, "file_content_changes": "@@ -372,6 +372,17 @@ def __eq__(self, other):\n     def __call__(self, *args, **kwds):\n         return self.value(*args, **kwds)\n \n+    def to(self, dtype, bitcast=False, _builder=None):\n+        if dtype in [float8, float16, bfloat16]:\n+            raise ValueError(\"floating point constexpr must be float64\")\n+        if dtype.is_int():\n+            ret_ty = int\n+        elif dtype.is_bool():\n+            ret_ty = bool\n+        elif dtype.is_floating():\n+            ret_ty = float\n+        return constexpr(ret_ty(self.value))\n+\n \n class tensor:\n     # infer dtype from ir type\n@@ -734,7 +745,7 @@ def reshape(input, shape, _builder=None):\n \n \n @builtin\n-def dot(input, other, allow_tf32=True, _builder=None):\n+def dot(input, other, trans_a=False, trans_b=False, allow_tf32=True, _builder=None):\n     \"\"\"\n     Returns the matrix product of two blocks.\n \n@@ -746,7 +757,7 @@ def dot(input, other, allow_tf32=True, _builder=None):\n     :type other: 2D tensor of scalar-type in {:code:`float16`, :code:`bfloat16`, :code:`float32`}\n     \"\"\"\n     allow_tf32 = _constexpr_to_value(allow_tf32)\n-    return semantic.dot(input, other, allow_tf32, _builder)\n+    return semantic.dot(input, other, trans_a, trans_b, allow_tf32, _builder)\n \n \n # -----------------------\n@@ -784,7 +795,7 @@ def load(pointer, mask=None, other=None, cache_modifier=\"\", eviction_policy=\"\",\n \n \n @builtin\n-def store(pointer, value, mask=None, _builder=None):\n+def store(pointer, value, mask=None, eviction_policy=\"\", _builder=None):\n     \"\"\"\n     Stores :code:`value` tensor of elements in memory, element-wise, at the memory locations specified by :code:`pointer`.\n \n@@ -801,13 +812,32 @@ def store(pointer, value, mask=None, _builder=None):\n     value = _to_tensor(value, _builder)\n     if mask is not None:\n         mask = _to_tensor(mask, _builder)\n-    return semantic.store(pointer, value, mask, _builder)\n+    return semantic.store(pointer, value, mask, eviction_policy, _builder)\n \n \n # -----------------------\n # Atomic Memory Operations\n # -----------------------\n \n+@builtin\n+def atomic_cas(pointer, cmp, val, _builder=None):\n+    \"\"\"\n+        Performs an atomic compare-and-swap at the memory location specified by :code:`pointer`.\n+\n+        Return the data stored at :code:`pointer` before the atomic operation.\n+\n+        :param pointer: The memory locations to compare-and-swap.\n+        :type pointer: Block of dtype=triton.PointerDType\n+        :param cmp: The values expected to be found in the atomic object\n+        :type cmp: Block of dtype=`pointer.dtype.element_ty`\n+        :param val: The values to copy in case the expected value matches the contained value.\n+        :type val: Block of dtype=`pointer.dtype.element_ty`\n+    \"\"\"\n+    cmp = _to_tensor(cmp, _builder)\n+    val = _to_tensor(val, _builder)\n+    return semantic.atomic_cas(pointer, cmp, val, _builder)\n+\n+\n def _add_atomic_docstr(name):\n \n     def _decorator(func):\n@@ -816,27 +846,19 @@ def _decorator(func):\n \n     Return the data stored at :code:`pointer` before the atomic operation.\n \n-    :param pointer: The memory locations to compare-and-swap.\n+    :param pointer: The memory locations to apply {name}.\n     :type pointer: Block of dtype=triton.PointerDType\n-    :param cmp: The values expected to be found in the atomic object\n-    :type cmp: Block of dtype=`pointer.dtype.element_ty`\n-    :param val: The values to copy in case the expected value matches the contained value.\n+    :param val: The values to {name} in the atomic object.\n     :type val: Block of dtype=`pointer.dtype.element_ty`\n+    :param mask: If mask[idx] is false, do not apply {name}.\n+    :type mask: Block of triton.int1, optional\n     \"\"\"\n         func.__doc__ = docstr.format(name=name)\n         return func\n \n     return _decorator\n \n \n-@builtin\n-@_add_atomic_docstr(\"compare-and-swap\")\n-def atomic_cas(pointer, cmp, val, _builder=None):\n-    cmp = _to_tensor(cmp, _builder)\n-    val = _to_tensor(val, _builder)\n-    return semantic.atomic_cas(pointer, cmp, val, _builder)\n-\n-\n @builtin\n @_add_atomic_docstr(\"exchange\")\n def atomic_xchg(pointer, val, mask=None, _builder=None):\n@@ -1002,13 +1024,27 @@ def max(input, axis, _builder=None):\n     return semantic.max(input, axis, _builder)\n \n \n+@builtin\n+@_add_reduction_docstr(\"maximum index\")\n+def argmax(input, axis, _builder=None):\n+    axis = _constexpr_to_value(axis)\n+    return semantic.argmax(input, axis, _builder)\n+\n+\n @builtin\n @_add_reduction_docstr(\"minimum\")\n def min(input, axis, _builder=None):\n     axis = _constexpr_to_value(axis)\n     return semantic.min(input, axis, _builder)\n \n \n+@builtin\n+@_add_reduction_docstr(\"minimum index\")\n+def argmin(input, axis, _builder=None):\n+    axis = _constexpr_to_value(axis)\n+    return semantic.argmin(input, axis, _builder)\n+\n+\n @builtin\n @_add_reduction_docstr(\"sum\")\n def sum(input, axis, _builder=None):"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 57, "deletions": 26, "changes": 83, "file_content_changes": "@@ -436,6 +436,9 @@ def not_equal(input: tl.tensor,\n \n \n def arange(start: int, end: int, builder: ir.builder) -> tl.tensor:\n+    if not isinstance(start, int) or not isinstance(end, int):\n+        raise ValueError(\"arange's arguments must be of type tl.constexpr\")\n+\n     shape = [end - start]\n     ret_ty = tl.block_type(tl.int32, shape)\n     return tl.tensor(builder.get_range(start, end), ret_ty)\n@@ -482,6 +485,11 @@ def broadcast_impl_shape(input: tl.tensor,\n         raise ValueError(f\"Cannot broadcast, rank mismatch: {src_shape}, {shape}\")\n     if shape == src_shape:\n         return input\n+    for i in range(len(src_shape)):\n+        if shape[i] != src_shape[i] and src_shape[i] != 1:\n+            raise ValueError(f\"Cannot broadcast, the expanded size of the tensor ({shape[i]})\"\n+                             f\" must match the existing size ({src_shape[1]}) at non-singleton dimension\"\n+                             f\" {i}: {src_shape}, {shape}\")\n     ret_ty = tl.block_type(input.type.scalar, shape)\n     return tl.tensor(builder.create_broadcast(input.handle, shape), ret_ty)\n \n@@ -648,6 +656,18 @@ def cast(input: tl.tensor,\n # ===----------------------------------------------------------------------===//\n \n \n+def _parse_eviction_policy(eviction_policy):\n+    eviction = ir.EVICTION_POLICY.NORMAL  # default\n+    if eviction_policy:\n+        if eviction_policy == \"evict_last\":\n+            eviction = ir.EVICTION_POLICY.EVICT_LAST\n+        elif eviction_policy == \"evict_first\":\n+            eviction = ir.EVICTION_POLICY.EVICT_FIRST\n+        else:\n+            raise ValueError(f\"Eviction policy {eviction_policy} not supported\")\n+    return eviction\n+\n+\n def load(ptr: tl.tensor,\n          mask: Optional[tl.tensor],\n          other: Optional[tl.tensor],\n@@ -684,14 +704,7 @@ def load(ptr: tl.tensor,\n             raise ValueError(f\"Cache modifier {cache_modifier} not supported\")\n \n     # eviction policy\n-    eviction = ir.EVICTION_POLICY.NORMAL  # default\n-    if eviction_policy:\n-        if eviction_policy == \"evict_last\":\n-            eviction = ir.EVICTION_POLICY.EVICT_LAST\n-        elif eviction_policy == \"evict_first\":\n-            eviction = ir.EVICTION_POLICY.EVICT_FIRST\n-        else:\n-            raise ValueError(f\"Eviction policy {eviction_policy} not supported\")\n+    eviction = _parse_eviction_policy(eviction_policy)\n \n     if ptr.type.is_block():\n         shape = ptr.type.get_block_shapes()\n@@ -721,6 +734,7 @@ def load(ptr: tl.tensor,\n def store(ptr: tl.tensor,\n           val: tl.tensor,\n           mask: Optional[tl.tensor],\n+          eviction_policy: str,\n           builder: ir.builder) -> tl.tensor:\n     if not ptr.type.scalar.is_ptr():\n         raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n@@ -735,14 +749,15 @@ def store(ptr: tl.tensor,\n         elt_ty_ptr = tl.int8\n         ptr_ty = tl.pointer_type(elt_ty_ptr, ptr_ty.address_space)\n         ptr = cast(ptr, ptr_ty, builder)\n-\n+    # eviction policy\n+    eviction = _parse_eviction_policy(eviction_policy)\n     # cast to target data-type\n     val = cast(val, elt_ty, builder)\n     if not mask:\n-        return tl.tensor(builder.create_store(ptr.handle, val.handle), tl.void)\n+        return tl.tensor(builder.create_store(ptr.handle, val.handle, eviction), tl.void)\n     if not mask.type.scalar.is_bool():\n         raise ValueError(\"Mask must have boolean scalar type\")\n-    return tl.tensor(builder.create_masked_store(ptr.handle, val.handle, mask.handle), tl.void)\n+    return tl.tensor(builder.create_masked_store(ptr.handle, val.handle, mask.handle, eviction), tl.void)\n \n #########\n # atomic\n@@ -897,27 +912,31 @@ def atomic_xchg(ptr: tl.tensor,\n # ===----------------------------------------------------------------------===//\n \n \n-def dot(lhs: tl.tensor,\n-        rhs: tl.tensor,\n+def dot(a: tl.tensor,\n+        b: tl.tensor,\n+        trans_a: bool,\n+        trans_b: bool,\n         allow_tf32: bool,\n         builder: ir.builder) -> tl.tensor:\n-    assert lhs.type.is_block() and rhs.type.is_block()\n-    assert len(lhs.shape) == 2 and len(rhs.shape) == 2\n-    assert lhs.shape[-1] == rhs.shape[0]\n-    assert lhs.shape[0] >= 16 and lhs.shape[1] >= 16 and rhs.shape[1] >= 16,\\\n+    in_a = 1 if not trans_a else 0\n+    in_b = 1 if trans_b else 0\n+    assert a.type.is_block() and b.type.is_block()\n+    assert len(a.shape) == 2 and len(b.shape) == 2\n+    assert a.shape[in_a] == b.shape[in_b]\n+    assert a.shape[0] >= 16 and a.shape[1] >= 16 and b.shape[1] >= 16,\\\n         \"small blocks not supported!\"\n-    if lhs.type.scalar.is_int():\n+    if a.type.scalar.is_int():\n         _0 = builder.get_int32(0)\n         ret_scalar_ty = tl.int32\n     else:\n         _0 = builder.get_float32(0)\n         ret_scalar_ty = tl.float32\n-    M = lhs.type.shape[0]\n-    N = rhs.type.shape[1]\n+    M = a.type.shape[in_a ^ 1]\n+    N = b.type.shape[in_b ^ 1]\n     _0 = builder.create_splat(_0, [M, N])\n     ret_ty = tl.block_type(ret_scalar_ty, [M, N])\n-    return tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32),\n-                     ret_ty)\n+    ret = builder.create_dot(a.handle, b.handle, _0, trans_a, trans_b, allow_tf32)\n+    return tl.tensor(ret, ret_ty)\n \n \n # ===----------------------------------------------------------------------===//\n@@ -961,10 +980,14 @@ def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n \n     # choose the right unsigned operation\n     if scalar_ty.is_int_unsigned():\n-        if INT_OP is ir.REDUCE_OP.MIN:\n-            INT_OP = ir.REDUCE_OP.UMIN\n-        elif INT_OP is ir.REDUCE_OP.MAX:\n-            INT_OP = ir.REDUCE_OP.UMAX\n+        int_op_to_unit = {\n+            ir.REDUCE_OP.MIN: ir.REDUCE_OP.UMIN,\n+            ir.REDUCE_OP.MAX: ir.REDUCE_OP.UMAX,\n+            ir.REDUCE_OP.ARGMIN: ir.REDUCE_OP.ARGUMIN,\n+            ir.REDUCE_OP.ARGMAX: ir.REDUCE_OP.ARGUMAX,\n+        }\n+        if INT_OP in int_op_to_unit:\n+            INT_OP = int_op_to_unit[INT_OP]\n \n     # get result type\n     shape = input.type.shape\n@@ -988,10 +1011,18 @@ def min(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     return reduce_impl(input, axis, builder, \"min\", ir.REDUCE_OP.FMIN, ir.REDUCE_OP.MIN)\n \n \n+def argmin(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n+    return reduce_impl(input, axis, builder, \"argmin\", ir.REDUCE_OP.ARGFMIN, ir.REDUCE_OP.ARGMIN)\n+\n+\n def max(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     return reduce_impl(input, axis, builder, \"max\", ir.REDUCE_OP.FMAX, ir.REDUCE_OP.MAX)\n \n \n+def argmax(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n+    return reduce_impl(input, axis, builder, \"argmax\", ir.REDUCE_OP.ARGFMAX, ir.REDUCE_OP.ARGMAX)\n+\n+\n def sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     return reduce_impl(input, axis, builder, \"sum\", ir.REDUCE_OP.FADD, ir.REDUCE_OP.ADD)\n "}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 26, "deletions": 22, "changes": 48, "file_content_changes": "@@ -128,17 +128,19 @@ def _layer_norm_bwd_dwdb(\n     cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n     dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n     db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-    for i in range(0, M, BLOCK_SIZE_M):\n-        rows = i + tl.arange(0, BLOCK_SIZE_M)\n-        mask = (rows[:, None] < M) & (cols[None, :] < N)\n-        offs = rows[:, None] * N + cols[None, :]\n-        a = tl.load(A + offs, mask=mask, other=0.).to(tl.float32)\n-        dout = tl.load(DOut + offs, mask=mask, other=0.).to(tl.float32)\n-        mean = tl.load(Mean + rows, mask=rows < M, other=0.)\n-        rstd = tl.load(Var + rows, mask=rows < M, other=0.)\n-        a_hat = (a - mean[:, None]) * rstd[:, None]\n-        dw += dout * a_hat\n-        db += dout\n+    UNROLL: tl.constexpr = 4\n+    for i in range(0, M, BLOCK_SIZE_M * UNROLL):\n+        for j in range(UNROLL):\n+            rows = i + j * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+            mask = (rows[:, None] < M) & (cols[None, :] < N)\n+            offs = rows[:, None] * N + cols[None, :]\n+            a = tl.load(A + offs, mask=mask, other=0.).to(tl.float32)\n+            dout = tl.load(DOut + offs, mask=mask, other=0.).to(tl.float32)\n+            mean = tl.load(Mean + rows, mask=rows < M, other=0.)\n+            rstd = tl.load(Var + rows, mask=rows < M, other=0.)\n+            a_hat = (a - mean[:, None]) * rstd[:, None]\n+            dw += dout * a_hat\n+            db += dout\n     sum_dw = tl.sum(dw, axis=0)\n     sum_db = tl.sum(db, axis=0)\n     tl.store(DW + cols, sum_dw, mask=cols < N)\n@@ -211,7 +213,15 @@ def backward(ctx, dout):\n             BLOCK_SIZE_N=ctx.BLOCK_SIZE,\n             num_warps=ctx.num_warps,\n         )\n-        # accumulate partial sums in separate kernel\n+        if N > 10240:\n+            BLOCK_SIZE_N = 128\n+            BLOCK_SIZE_M = 32\n+            num_warps = 4\n+        else:\n+            # maximize occupancy for small N\n+            BLOCK_SIZE_N = 16\n+            BLOCK_SIZE_M = 16\n+            num_warps = 8\n         grid = lambda meta: [triton.cdiv(N, meta[\"BLOCK_SIZE_N\"])]\n         _layer_norm_bwd_dwdb[grid](\n             a, dout,\n@@ -220,17 +230,11 @@ def backward(ctx, dout):\n             dbias,\n             M,\n             N,\n-            BLOCK_SIZE_M=32,\n-            BLOCK_SIZE_N=128,\n+            BLOCK_SIZE_M=BLOCK_SIZE_M,\n+            BLOCK_SIZE_N=BLOCK_SIZE_N,\n+            num_warps=num_warps\n         )\n-        return (da, None, dweight, dbias, None, None,\n-                None, None, None, None,\n-                None,\n-                None, None, None,\n-                None,\n-                None, None, None,\n-                None, None, None,\n-                None, None, None)\n+        return (da, None, dweight, dbias, None)\n \n \n def layer_norm(a, normalized_shape, weight, bias, eps):"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "added", "additions": 198, "deletions": 0, "changes": 198, "file_content_changes": "@@ -0,0 +1,198 @@\n+import pytest\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def _fwd_kernel(\n+    Q, K, V,\n+    TMP, L, M,  # NOTE: TMP is a scratchpad buffer to workaround a compiler bug\n+    Out,\n+    stride_qz, stride_qh, stride_qm, stride_qk,\n+    stride_kz, stride_kh, stride_kk, stride_kn,\n+    stride_vz, stride_vh, stride_vk, stride_vn,\n+    stride_oz, stride_oh, stride_om, stride_on,\n+    Z, H, N_CTX,\n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+    BLOCK_N: tl.constexpr,\n+):\n+    start_qm = tl.program_id(0)\n+    off_hz = tl.program_id(1)\n+    # initialize offsets\n+    offs_m = start_qm * BLOCK_M + tl.arange(0, BLOCK_M)\n+    offs_n = tl.arange(0, BLOCK_N)\n+    offs_d = tl.arange(0, BLOCK_DMODEL)\n+    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n+    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    # Initialize pointers to Q, K, V\n+    q_ptrs = Q + off_q\n+    k_ptrs = K + off_k\n+    v_ptrs = V + off_v\n+    # initialize pointer to m and l\n+    t_ptrs = TMP + off_hz * N_CTX + offs_m\n+\n+    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n+\n+    q = tl.load(q_ptrs)\n+    for start_n in range(0, start_qm + 1):\n+        # -- compute qk ----\n+        k = tl.load(k_ptrs)\n+        qk = tl.dot(q, k)\n+        qk += tl.where(offs_m[:, None] >= (start_n * BLOCK_N + offs_n[None, :]), 0, float(\"-inf\"))\n+        # -- compute m_ij, p, l_ij\n+        m_ij = tl.max(qk, 1)\n+        p = tl.exp(qk - m_ij[:, None])\n+        l_ij = tl.sum(p, 1)\n+        # -- update m_i and l_i\n+        m_i_new = tl.maximum(m_i, m_ij)\n+        alpha = tl.exp(m_i - m_i_new)\n+        beta = tl.exp(m_ij - m_i_new)\n+        l_i_new = alpha * l_i + beta * l_ij\n+        # -- update output accumulator --\n+        # scale p\n+        p_scale = beta / l_i_new\n+        p = p * p_scale[:, None]\n+        p = p.to(tl.float16)\n+        # scale acc\n+        acc_scale = l_i / l_i_new * alpha\n+        tl.store(t_ptrs, acc_scale)\n+        acc_scale = tl.load(t_ptrs)  # BUG: have to store and immediately load\n+        acc = acc * acc_scale[:, None]\n+        # update acc\n+        v = tl.load(v_ptrs)\n+        acc += tl.dot(p, v)\n+        k_ptrs += BLOCK_N * stride_kn\n+        v_ptrs += BLOCK_N * stride_vk\n+        # r_ptrs += BLOCK_N\n+        l_i = l_i_new\n+        m_i = m_i_new\n+\n+    start_qm = tl.program_id(0)\n+    offs_m = start_qm * BLOCK_M + tl.arange(0, BLOCK_M)\n+    # write back l and m\n+    l_ptrs = L + off_hz * N_CTX + offs_m\n+    m_ptrs = M + off_hz * N_CTX + offs_m\n+    tl.store(l_ptrs, l_i)\n+    tl.store(m_ptrs, m_i)\n+    # initialize pointers to output\n+    offs_n = tl.arange(0, BLOCK_DMODEL)\n+    off_out = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n+    out_ptrs = Out + off_out\n+    tl.store(out_ptrs, acc)\n+\n+\n+class _attention(torch.autograd.Function):\n+\n+    @staticmethod\n+    def forward(ctx, q, k, v):\n+        BLOCK = 128\n+        # shape constraints\n+        Lq, Lk = q.shape[-1], k.shape[-2]\n+        assert Lq == Lk\n+        o = torch.empty_like(q)\n+        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1])\n+        tmp = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        _fwd_kernel[grid](\n+            q, k, v,\n+            tmp, L, m,\n+            o,\n+            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+            q.shape[0], q.shape[1], q.shape[2],\n+            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+            BLOCK_DMODEL=64, num_warps=4,\n+            num_stages=1,\n+        )\n+        ctx.save_for_backward(q, k, v, o, L, m)\n+        ctx.BLOCK = BLOCK\n+        ctx.grid = grid\n+        return o\n+\n+\n+attention = _attention.apply\n+\n+\n+@pytest.mark.parametrize('Z, H, N_CTX, D_MODEL', [(2, 3, 1024, 64)])\n+def test_op(Z, H, N_CTX, D_MODEL, dtype=torch.float16):\n+    torch.manual_seed(20)\n+    q = .5 * torch.randn((Z, H, N_CTX, D_MODEL), dtype=dtype, device=\"cuda\", requires_grad=True)\n+    k = .5 * torch.randn((Z, H, D_MODEL, N_CTX), dtype=dtype, device=\"cuda\", requires_grad=True)\n+    v = .5 * torch.randn((Z, H, N_CTX, D_MODEL), dtype=dtype, device=\"cuda\", requires_grad=True)\n+    # triton implementation\n+    tri_out = attention(q, k, v)\n+    # reference implementation\n+    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n+    ref_qk = torch.matmul(q, k)\n+    for z in range(Z):\n+        for h in range(H):\n+            ref_qk[:, :, M == 0] = float(\"-inf\")\n+    ref_qk = torch.softmax(ref_qk, dim=-1)\n+    ref_out = torch.matmul(ref_qk, v)\n+    # compare\n+    triton.testing.assert_almost_equal(ref_out, tri_out)\n+\n+\n+try:\n+    from flash_attn.flash_attn_interface import flash_attn_func\n+    HAS_FLASH = True\n+except BaseException:\n+    HAS_FLASH = False\n+\n+BATCH, N_HEADS, N_CTX, D_HEAD = 4, 64, 2048, 64\n+# vary batch size for fixed heads / seq\n+batch_bench = triton.testing.Benchmark(\n+    x_names=['BATCH'],\n+    x_vals=[2**i for i in range(0, 8)],\n+    line_arg='provider',\n+    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n+    line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n+    styles=[('red', '-'), ('blue', '-')],\n+    ylabel='ms',\n+    plot_name=f'fused-attention-seq{N_CTX}-head{N_HEADS}-d{D_HEAD}',\n+    args={'H': N_HEADS, 'N_CTX': N_CTX, 'D_MODEL': D_HEAD, 'dtype': torch.float16}\n+)\n+# vary seq length for fixed head and batch=4\n+seq_bench = triton.testing.Benchmark(\n+    x_names=['N_CTX'],\n+    x_vals=[2**i for i in range(10, 16)],\n+    line_arg='provider',\n+    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n+    line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n+    styles=[('red', '-'), ('blue', '-')],\n+    ylabel='ms',\n+    plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}',\n+    args={'H': D_HEAD, 'BATCH': BATCH, 'D_MODEL': D_HEAD, 'dtype': torch.float16}\n+)\n+\n+\n+@triton.testing.perf_report([batch_bench, seq_bench])\n+def bench_flash_attention(BATCH, H, N_CTX, D_MODEL, provider, dtype=torch.float16, device=\"cuda\"):\n+    warmup = 25\n+    rep = 500\n+    if provider == \"triton\":\n+        q = torch.randn((BATCH, H, N_CTX, D_MODEL), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        k = torch.randn((BATCH, H, D_MODEL, N_CTX), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        v = torch.randn((BATCH, H, N_CTX, D_MODEL), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        fn = lambda: attention(q, k, v)\n+        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n+        return ms\n+    if provider == \"flash\":\n+        lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n+        cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n+        cu_seqlens[1:] = lengths.cumsum(0)\n+        qkv = torch.randn((BATCH * N_CTX, 3, H, D_MODEL), dtype=dtype, device=device, requires_grad=True)\n+        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\n+        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n+        return ms\n+\n+\n+bench_flash_attention.run(save_path='.', print_data=True)"}]