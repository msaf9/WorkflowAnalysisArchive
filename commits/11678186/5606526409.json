[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "file_content_changes": "@@ -123,7 +123,7 @@ def _fwd_kernel(\n @triton.jit\n def _bwd_preprocess(\n     Out, DO,\n-    NewDO, Delta,\n+    Delta,\n     BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n ):\n     off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n@@ -134,7 +134,6 @@ def _bwd_preprocess(\n     # compute\n     delta = tl.sum(o * do, axis=1)\n     # write-back\n-    tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\n     tl.store(Delta + off_m, delta)\n \n \n@@ -277,16 +276,15 @@ def backward(ctx, do):\n         dq = torch.zeros_like(q, dtype=torch.float32)\n         dk = torch.empty_like(k)\n         dv = torch.empty_like(v)\n-        do_scaled = torch.empty_like(do)\n         delta = torch.empty_like(L)\n         _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n             o, do,\n-            do_scaled, delta,\n+            delta,\n             BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n         )\n         _bwd_kernel[(ctx.grid[1],)](\n             q, k, v, ctx.sm_scale,\n-            o, do_scaled,\n+            o, do,\n             dq, dk, dv,\n             L, delta,\n             q.stride(0), q.stride(1), q.stride(2), q.stride(3),"}]