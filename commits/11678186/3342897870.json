[{"filename": "include/triton/Analysis/Allocation.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -6,6 +6,7 @@\n #include \"llvm/ADT/SetVector.h\"\n #include \"llvm/Support/raw_ostream.h\"\n \n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include <atomic>\n #include <limits>\n@@ -19,6 +20,8 @@ SmallVector<unsigned>\n getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n                              unsigned &outVec);\n \n+SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op);\n+\n } // namespace triton\n \n /// Modified from llvm-15.0: llvm/ADT/AddressRanges.h"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -250,6 +250,12 @@ struct PTXIOInstr : public PTXInstrBase<PTXIOInstr> {\n     return *this;\n   }\n \n+  // Add \".shared\" suffix to instruction\n+  PTXIOInstr &shared(bool predicate = true) {\n+    o(\"shared\", predicate);\n+    return *this;\n+  }\n+\n   // Add \".v\" suffix to instruction\n   PTXIOInstr &v(int vecWidth, bool predicate = true) {\n     if (vecWidth > 1) {"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -324,7 +324,9 @@ def SliceEncodingAttr : DistributedEncoding<\"SliceEncoding\"> {\n     \"Attribute\":$parent\n   );\n \n-  let extraClassDeclaration = extraBaseClassDeclaration;\n+  let extraClassDeclaration = extraBaseClassDeclaration # [{\n+    SmallVector<int64_t> paddedShape(ArrayRef<int64_t> shape) const;\n+  }];\n }\n \n "}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 40, "deletions": 3, "changes": 43, "file_content_changes": "@@ -14,6 +14,7 @@ using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n+using ::mlir::triton::gpu::SliceEncodingAttr;\n \n namespace mlir {\n \n@@ -33,6 +34,10 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n          \"Unexpect layout in getScratchConfigForCvtLayout()\");\n   unsigned rank = dstTy.getRank();\n   SmallVector<unsigned> paddedRepShape(rank);\n+  if (auto srcSliceLayout = srcLayout.dyn_cast<SliceEncodingAttr>())\n+    srcLayout = srcSliceLayout.getParent();\n+  if (auto dstSliceLayout = dstLayout.dyn_cast<SliceEncodingAttr>())\n+    dstLayout = dstSliceLayout.getParent();\n   auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>();\n   auto srcMmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n   auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>();\n@@ -73,6 +78,31 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   return paddedRepShape;\n }\n \n+SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op) {\n+  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+  auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto srcShape = srcTy.getShape();\n+  auto rank = srcShape.size();\n+  auto axis = op.axis();\n+\n+  bool fast_reduce = axis == 1; // FIXME(Qingyi): The fastest-changing dimension\n+\n+  SmallVector<unsigned> smemShape;\n+  for (auto d : srcShape)\n+    smemShape.push_back(d);\n+\n+  if (fast_reduce) {\n+    unsigned sizeInterWarps = srcLayout.getWarpsPerCTA()[axis];\n+    smemShape[axis] = sizeInterWarps;\n+  } else {\n+    unsigned threadsPerCTAAxis =\n+        srcLayout.getThreadsPerWarp()[axis] * srcLayout.getWarpsPerCTA()[axis];\n+    smemShape[axis] = threadsPerCTAAxis;\n+  }\n+\n+  return smemShape;\n+}\n+\n class AllocationAnalysis {\n public:\n   AllocationAnalysis(Operation *operation, Allocation *allocation)\n@@ -127,9 +157,16 @@ class AllocationAnalysis {\n       // TODO(Keren): Reduce with index is not supported yet.\n       auto value = op->getOperand(0);\n       if (auto tensorType = value.getType().dyn_cast<RankedTensorType>()) {\n-        auto bytes = tensorType.getNumElements() *\n-                     tensorType.getElementTypeBitWidth() / 8;\n-        allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+        if (tensorType.getEncoding().isa<BlockedEncodingAttr>()) {\n+          auto smemShape = getScratchConfigForReduce(reduceOp);\n+          unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(),\n+                                           1, std::multiplies{});\n+          auto bytes = elems * tensorType.getElementTypeBitWidth() / 8;\n+          allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+        } else {\n+          assert(0 && \"ReduceOp with input layout other than blocked layout is \"\n+                      \"not implemented yet\");\n+        }\n       }\n     } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n       auto srcTy = cvtLayout.src().getType().cast<RankedTensorType>();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 468, "deletions": 43, "changes": 511, "file_content_changes": "@@ -76,7 +76,15 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n #define urem(...) rewriter.create<LLVM::URemOp>(loc, __VA_ARGS__)\n #define add(...) rewriter.create<LLVM::AddOp>(loc, __VA_ARGS__)\n+#define fadd(...) rewriter.create<LLVM::FAddOp>(loc, __VA_ARGS__)\n #define mul(...) rewriter.create<LLVM::MulOp>(loc, __VA_ARGS__)\n+#define smax(...) rewriter.create<LLVM::SMaxOp>(loc, __VA_ARGS__)\n+#define umax(...) rewriter.create<LLVM::UMaxOp>(loc, __VA_ARGS__)\n+#define fmax(...) rewriter.create<LLVM::MaxNumOp>(loc, __VA_ARGS__)\n+#define smin(...) rewriter.create<LLVM::SMinOp>(loc, __VA_ARGS__)\n+#define umin(...) rewriter.create<LLVM::UMinOp>(loc, __VA_ARGS__)\n+#define fmin(...) rewriter.create<LLVM::MinNumOp>(loc, __VA_ARGS__)\n+#define and_(...) rewriter.create<LLVM::AndOp>(loc, __VA_ARGS__)\n #define xor_(...) rewriter.create<LLVM::XOrOp>(loc, __VA_ARGS__)\n #define bitcast(...) rewriter.create<LLVM::BitcastOp>(loc, __VA_ARGS__)\n #define gep(...) rewriter.create<LLVM::GEPOp>(loc, __VA_ARGS__)\n@@ -89,11 +97,16 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n   rewriter.create<LLVM::ExtractElementOp>(loc, __VA_ARGS__)\n #define load(...) rewriter.create<LLVM::LoadOp>(loc, __VA_ARGS__)\n #define store(val, ptr) rewriter.create<LLVM::StoreOp>(loc, val, ptr)\n+#define icmp_eq(...)                                                           \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::eq, __VA_ARGS__)\n+#define icmp_slt(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::slt, __VA_ARGS__)\n #define select(...) rewriter.create<LLVM::SelectOp>(loc, __VA_ARGS__)\n #define address_of(...) rewriter.create<LLVM::AddressOfOp>(loc, __VA_ARGS__)\n #define barrier() rewriter.create<mlir::gpu::BarrierOp>(loc)\n #define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n #define i32_ty rewriter.getIntegerType(32)\n+#define f32_ty rewriter.getF32Type()\n #define vec_ty(type, num) VectorType::get(num, type)\n #define void_ty LLVM::LLVMVoidType::get(ctx)\n #define struct_ty(...) LLVM::LLVMStructType::getLiteral(__VA_ARGS__)\n@@ -336,6 +349,20 @@ static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   return linearIndex;\n }\n \n+Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n+                  Value val, Value pred) {\n+  MLIRContext *ctx = rewriter.getContext();\n+  unsigned bits = val.getType().getIntOrFloatBitWidth();\n+  const char *c = bits == 64 ? \"l\" : (bits == 16 ? \"h\" : \"r\");\n+\n+  PTXBuilder builder;\n+  auto &st = builder.create<PTXIOInstr>(\"st\")->shared().b(bits);\n+  auto *ptrOpr = builder.newAddrOperand(ptr, \"r\");\n+  auto *valOpr = builder.newOperand(val, c);\n+  st(ptrOpr, valOpr).predicate(pred, \"b\");\n+  return builder.launch(rewriter, loc, void_ty);\n+}\n+\n struct ConvertTritonGPUOpToLLVMPatternBase {\n   static SmallVector<Value>\n   getElementsFromStruct(Location loc, Value llvmStruct,\n@@ -504,17 +531,8 @@ class ConvertTritonGPUOpToLLVMPattern\n     unsigned dim = sliceLayout.getDim();\n     size_t rank = shape.size();\n     if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n-      SmallVector<int64_t> paddedShape(rank + 1);\n-      for (unsigned d = 0; d < rank + 1; ++d) {\n-        if (d < dim)\n-          paddedShape[d] = shape[d];\n-        else if (d == dim)\n-          paddedShape[d] = 1;\n-        else\n-          paddedShape[d] = shape[d - 1];\n-      }\n       auto paddedIndices = emitIndicesForBlockedLayout(\n-          loc, rewriter, blockedParent, paddedShape);\n+          loc, rewriter, blockedParent, sliceLayout.paddedShape(shape));\n       unsigned numIndices = paddedIndices.size();\n       SmallVector<SmallVector<Value>> resultIndices(numIndices);\n       for (unsigned i = 0; i < numIndices; ++i)\n@@ -536,31 +554,19 @@ class ConvertTritonGPUOpToLLVMPattern\n     }\n   }\n \n-  // Emit indices calculation within each ConversionPattern, and returns a\n-  // [elemsPerThread X rank] index matrix.\n-  // TODO: [goostavz] Double confirm the redundant indices calculations will\n-  //       be eliminated in the consequent MLIR/LLVM optimization. We might\n-  //       implement a indiceCache if necessary.\n-  SmallVector<SmallVector<Value>>\n-  emitIndicesForBlockedLayout(Location loc, ConversionPatternRewriter &rewriter,\n-                              const BlockedEncodingAttr &blockedLayout,\n-                              ArrayRef<int64_t> shape) const {\n-    auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForBlockedLayout(const BlockedEncodingAttr &blockedLayout,\n+                             ArrayRef<int64_t> shape) const {\n     auto sizePerThread = blockedLayout.getSizePerThread();\n     auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n     auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n+\n     unsigned rank = shape.size();\n     SmallVector<unsigned> shapePerCTA = getShapePerCTA(blockedLayout);\n     SmallVector<unsigned> tilesPerDim(rank);\n     for (unsigned k = 0; k < rank; ++k)\n       tilesPerDim[k] = ceil<unsigned>(shape[k], shapePerCTA[k]);\n \n-    // step 1, delinearize threadId to get the base index\n-    auto multiDimBase =\n-        emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n-\n-    // step 2, get offset of each element\n-    unsigned elemsPerThread = blockedLayout.getElemsPerThread(shape);\n     SmallVector<SmallVector<unsigned>> offset(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n       // 1 block in minimum if shape[k] is less than shapePerCTA[k]\n@@ -577,12 +583,10 @@ class ConvertTritonGPUOpToLLVMPattern\n                                       threadsPerWarp[k] +\n                                   threadOffset * sizePerThread[k] + elemOffset);\n     }\n-    // step 3, add offset to base, and reorder the sequence of indices to\n-    // guarantee that elems in the same sizePerThread are adjacent in order\n-    SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread,\n-                                                SmallVector<Value>(rank));\n-    unsigned totalSizePerThread = product<unsigned>(sizePerThread);\n \n+    unsigned elemsPerThread = blockedLayout.getElemsPerThread(shape);\n+    unsigned totalSizePerThread = product<unsigned>(sizePerThread);\n+    SmallVector<SmallVector<unsigned>> reorderedOffset(elemsPerThread);\n     for (unsigned n = 0; n < elemsPerThread; ++n) {\n       unsigned linearNanoTileId = n / totalSizePerThread;\n       unsigned linearNanoTileElemId = n % totalSizePerThread;\n@@ -595,10 +599,38 @@ class ConvertTritonGPUOpToLLVMPattern\n             multiDimNanoTileId[k] *\n                 (sizePerThread[k] * threadsPerWarp[k] * warpsPerCTA[k]) +\n             multiDimNanoTileElemId[k];\n-        multiDimIdx[n][k] =\n-            add(multiDimBase[k], idx_val(offset[k][reorderedMultiDimId]));\n+        reorderedOffset[n].push_back(offset[k][reorderedMultiDimId]);\n       }\n     }\n+    return reorderedOffset;\n+  }\n+\n+  // Emit indices calculation within each ConversionPattern, and returns a\n+  // [elemsPerThread X rank] index matrix.\n+  // TODO: [goostavz] Double confirm the redundant indices calculations will\n+  //       be eliminated in the consequent MLIR/LLVM optimization. We might\n+  //       implement a indiceCache if necessary.\n+  SmallVector<SmallVector<Value>>\n+  emitIndicesForBlockedLayout(Location loc, ConversionPatternRewriter &rewriter,\n+                              const BlockedEncodingAttr &blockedLayout,\n+                              ArrayRef<int64_t> shape) const {\n+    // step 1, delinearize threadId to get the base index\n+    auto multiDimBase =\n+        emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n+\n+    // step 2, get offset of each element\n+    SmallVector<SmallVector<unsigned>> offset =\n+        emitOffsetForBlockedLayout(blockedLayout, shape);\n+\n+    // step 3, add offset to base, and reorder the sequence of indices to\n+    // guarantee that elems in the same sizePerThread are adjacent in order\n+    unsigned rank = shape.size();\n+    unsigned elemsPerThread = offset.size();\n+    SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread,\n+                                                SmallVector<Value>(rank));\n+    for (unsigned n = 0; n < elemsPerThread; ++n)\n+      for (unsigned k = 0; k < rank; ++k)\n+        multiDimIdx[n][k] = add(multiDimBase[k], idx_val(offset[n][k]));\n \n     return multiDimIdx;\n   }\n@@ -1027,8 +1059,12 @@ struct StoreOpConversion\n     MLIRContext *ctx = rewriter.getContext();\n \n     auto valueTy = value.getType().dyn_cast<RankedTensorType>();\n-    if (!valueTy)\n-      return failure();\n+    if (!valueTy) {\n+      store(llValue, llPtr);\n+      rewriter.eraseOp(op);\n+      return success();\n+    }\n+\n     Type valueElemTy =\n         getTypeConverter()->convertType(valueTy.getElementType());\n \n@@ -1225,6 +1261,360 @@ struct BroadcastOpConversion\n   }\n };\n \n+/// ====================== reduce codegen begin ==========================\n+\n+struct ReduceOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::ReduceOp> {\n+public:\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::ReduceOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override;\n+\n+private:\n+  void accumulate(ConversionPatternRewriter &rewriter, Location loc,\n+                  RedOp redOp, Value &acc, Value cur, bool isFirst) const;\n+\n+  Value shflSync(ConversionPatternRewriter &rewriter, Location loc, Value val,\n+                 int i) const;\n+\n+  // Use shared memory for reduction within warps and across warps\n+  LogicalResult matchAndRewriteBasic(triton::ReduceOp op, OpAdaptor adaptor,\n+                                     ConversionPatternRewriter &rewriter) const;\n+\n+  // Use warp shuffle for reduction within warps and shared memory for data\n+  // exchange across warps\n+  LogicalResult matchAndRewriteFast(triton::ReduceOp op, OpAdaptor adaptor,\n+                                    ConversionPatternRewriter &rewriter) const;\n+};\n+\n+LogicalResult\n+ReduceOpConversion::matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n+                                    ConversionPatternRewriter &rewriter) const {\n+  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+  auto rank = srcTy.getShape().size();\n+  if (op.axis() == 1) // FIXME(Qingyi): The fastest-changing dimension\n+    return matchAndRewriteFast(op, adaptor, rewriter);\n+  return matchAndRewriteBasic(op, adaptor, rewriter);\n+}\n+\n+void ReduceOpConversion::accumulate(ConversionPatternRewriter &rewriter,\n+                                    Location loc, RedOp redOp, Value &acc,\n+                                    Value cur, bool isFirst) const {\n+  if (isFirst) {\n+    acc = cur;\n+    return;\n+  }\n+  auto type = cur.getType();\n+  switch (redOp) {\n+  case RedOp::ADD:\n+    acc = add(acc, cur);\n+    break;\n+  case RedOp::MAX:\n+    if (type.isUnsignedInteger())\n+      acc = umax(acc, cur);\n+    else\n+      acc = smax(acc, cur);\n+    break;\n+  case RedOp::MIN:\n+    if (type.isUnsignedInteger())\n+      acc = umin(acc, cur);\n+    else\n+      acc = smin(acc, cur);\n+    break;\n+  case RedOp::FADD:\n+    acc = fadd(acc.getType(), acc, cur);\n+    break;\n+  case RedOp::FMAX:\n+    acc = fmax(acc, cur);\n+    break;\n+  case RedOp::FMIN:\n+    acc = fmin(acc, cur);\n+    break;\n+  case RedOp::XOR:\n+    acc = xor_(acc, cur);\n+    break;\n+  default:\n+    llvm::report_fatal_error(\"Unsupported reduce op\");\n+  }\n+};\n+\n+Value ReduceOpConversion::shflSync(ConversionPatternRewriter &rewriter,\n+                                   Location loc, Value val, int i) const {\n+  MLIRContext *ctx = rewriter.getContext();\n+  unsigned bits = val.getType().getIntOrFloatBitWidth();\n+\n+  if (bits == 64) {\n+    Type vecTy = vec_ty(f32_ty, 2);\n+    Value vec = bitcast(vecTy, val);\n+    Value val0 = extract_element(f32_ty, vec, i32_val(0));\n+    Value val1 = extract_element(f32_ty, vec, i32_val(1));\n+    val0 = shflSync(rewriter, loc, val0, i);\n+    val1 = shflSync(rewriter, loc, val1, i);\n+    vec = undef(vecTy);\n+    vec = insert_element(vecTy, vec, val0, i32_val(0));\n+    vec = insert_element(vecTy, vec, val1, i32_val(1));\n+    return bitcast(val.getType(), vec);\n+  }\n+\n+  PTXBuilder builder;\n+  auto &shfl = builder.create(\"shfl.sync\")->o(\"bfly\").o(\"b32\");\n+  auto *dOpr = builder.newOperand(\"=r\");\n+  auto *aOpr = builder.newOperand(val, \"r\");\n+  auto *bOpr = builder.newConstantOperand(i);\n+  auto *cOpr = builder.newConstantOperand(\"0x1f\");\n+  auto *maskOpr = builder.newConstantOperand(\"0xffffffff\");\n+  shfl(dOpr, aOpr, bOpr, cOpr, maskOpr);\n+  return builder.launch(rewriter, loc, val.getType(), false);\n+}\n+\n+LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n+    triton::ReduceOp op, OpAdaptor adaptor,\n+    ConversionPatternRewriter &rewriter) const {\n+  Location loc = op->getLoc();\n+  unsigned axis = op.axis();\n+\n+  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+  auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto srcShape = srcTy.getShape();\n+\n+  auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n+  auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+  Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+  smemBase = bitcast(elemPtrTy, smemBase);\n+\n+  auto smemShape = getScratchConfigForReduce(op);\n+\n+  unsigned srcElems = getElemsPerThread(srcLayout, srcShape);\n+  auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n+  auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n+\n+  SmallVector<SmallVector<unsigned>> offset =\n+      emitOffsetForBlockedLayout(srcLayout, srcShape);\n+\n+  std::map<SmallVector<unsigned>, Value> accs;\n+  std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n+\n+  // reduce within threads\n+  for (unsigned i = 0; i < srcElems; ++i) {\n+    SmallVector<unsigned> key = offset[i];\n+    key[axis] = 0;\n+    bool isFirst = accs.find(key) == accs.end();\n+    accumulate(rewriter, loc, op.redOp(), accs[key], srcValues[i], isFirst);\n+    if (isFirst)\n+      indices[key] = srcIndices[i];\n+  }\n+\n+  // cached int32 constants\n+  std::map<int, Value> ints;\n+  ints[0] = i32_val(0);\n+  for (int N = smemShape[axis] / 2; N > 0; N >>= 1)\n+    ints[N] = i32_val(N);\n+  Value sizePerThread = i32_val(srcLayout.getSizePerThread()[axis]);\n+\n+  // reduce across threads\n+  for (auto it : accs) {\n+    const SmallVector<unsigned> &key = it.first;\n+    Value acc = it.second;\n+    SmallVector<Value> writeIdx = indices[key];\n+\n+    writeIdx[axis] = udiv(writeIdx[axis], sizePerThread);\n+    Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n+    Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+    store(acc, writePtr);\n+\n+    SmallVector<Value> readIdx(writeIdx.size(), ints[0]);\n+    for (int N = smemShape[axis] / 2; N > 0; N >>= 1) {\n+      readIdx[axis] = ints[N];\n+      Value readMask = icmp_slt(writeIdx[axis], ints[N]);\n+      Value readOffset = select(\n+          readMask, linearize(rewriter, loc, readIdx, smemShape), ints[0]);\n+      Value readPtr = gep(elemPtrTy, writePtr, readOffset);\n+      barrier();\n+      accumulate(rewriter, loc, op.redOp(), acc, load(readPtr), false);\n+      store(acc, writePtr);\n+    }\n+  }\n+\n+  // set output values\n+  if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n+    // nd-tensor where n >= 1\n+    auto resultLayout = resultTy.getEncoding();\n+    auto resultShape = resultTy.getShape();\n+\n+    unsigned resultElems = getElemsPerThread(resultLayout, resultShape);\n+    auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultShape);\n+    assert(resultIndices.size() == resultElems);\n+\n+    barrier();\n+    SmallVector<Value> resultVals(resultElems);\n+    for (int i = 0; i < resultElems; i++) {\n+      SmallVector<Value> readIdx = resultIndices[i];\n+      readIdx.insert(readIdx.begin() + axis, ints[0]);\n+      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n+      Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n+      resultVals[i] = load(readPtr);\n+    }\n+\n+    SmallVector<Type> resultTypes(resultElems, llvmElemTy);\n+    Type structTy =\n+        LLVM::LLVMStructType::getLiteral(this->getContext(), resultTypes);\n+    Value ret = getStructFromElements(loc, resultVals, rewriter, structTy);\n+    rewriter.replaceOp(op, ret);\n+  } else {\n+    // 0d-tensor -> scalar\n+    barrier();\n+    Value resultVal = load(smemBase);\n+    rewriter.replaceOp(op, resultVal);\n+  }\n+\n+  return success();\n+}\n+\n+LogicalResult ReduceOpConversion::matchAndRewriteFast(\n+    triton::ReduceOp op, OpAdaptor adaptor,\n+    ConversionPatternRewriter &rewriter) const {\n+  Location loc = op->getLoc();\n+  unsigned axis = adaptor.axis();\n+\n+  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+  auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto srcShape = srcTy.getShape();\n+  auto srcOrder = srcLayout.getOrder();\n+\n+  auto threadsPerWarp = srcLayout.getThreadsPerWarp();\n+  auto warpsPerCTA = srcLayout.getWarpsPerCTA();\n+\n+  auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n+  auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+  Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+  smemBase = bitcast(elemPtrTy, smemBase);\n+\n+  auto order = srcLayout.getOrder();\n+  unsigned sizeIntraWarps = threadsPerWarp[axis];\n+  unsigned sizeInterWarps = warpsPerCTA[axis];\n+\n+  unsigned srcElems = getElemsPerThread(srcLayout, srcShape);\n+  auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n+  auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n+\n+  SmallVector<SmallVector<unsigned>> offset =\n+      emitOffsetForBlockedLayout(srcLayout, srcShape);\n+\n+  std::map<SmallVector<unsigned>, Value> accs;\n+  std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n+\n+  auto smemShape = getScratchConfigForReduce(op);\n+\n+  // reduce within threads\n+  for (unsigned i = 0; i < srcElems; ++i) {\n+    SmallVector<unsigned> key = offset[i];\n+    key[axis] = 0;\n+    bool isFirst = accs.find(key) == accs.end();\n+    accumulate(rewriter, loc, op.redOp(), accs[key], srcValues[i], isFirst);\n+    if (isFirst)\n+      indices[key] = srcIndices[i];\n+  }\n+\n+  Value threadId = getThreadId(rewriter, loc);\n+  Value warpSize = i32_val(32);\n+  Value warpId = udiv(threadId, warpSize);\n+  Value laneId = urem(threadId, warpSize);\n+\n+  SmallVector<Value> multiDimLaneId =\n+      delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n+  SmallVector<Value> multiDimWarpId =\n+      delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+  Value laneIdAxis = multiDimLaneId[axis];\n+  Value warpIdAxis = multiDimWarpId[axis];\n+\n+  Value zero = i32_val(0);\n+  Value laneZero = icmp_eq(laneIdAxis, zero);\n+  Value warpZero = icmp_eq(warpIdAxis, zero);\n+\n+  for (auto it : accs) {\n+    const SmallVector<unsigned> &key = it.first;\n+    Value acc = it.second;\n+\n+    // reduce within warps\n+    for (unsigned N = sizeIntraWarps / 2; N > 0; N >>= 1) {\n+      Value shfl = shflSync(rewriter, loc, acc, N);\n+      accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n+    }\n+\n+    if (sizeInterWarps == 1) {\n+      SmallVector<Value> writeIdx = indices[key];\n+      writeIdx[axis] = zero;\n+      Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n+      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+      storeShared(rewriter, loc, writePtr, acc, laneZero);\n+    } else {\n+      SmallVector<Value> writeIdx = indices[key];\n+      writeIdx[axis] =\n+          warpIdAxis; // axis must be the fastest-changing dimension\n+      Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n+      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+      storeShared(rewriter, loc, writePtr, acc, laneZero);\n+      barrier();\n+\n+      SmallVector<Value> readIdx = writeIdx;\n+      readIdx[axis] = urem(laneId, i32_val(sizeInterWarps));\n+      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n+      Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n+      acc = load(readPtr);\n+\n+      // reduce across warps\n+      for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n+        Value shfl = shflSync(rewriter, loc, acc, N);\n+        accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n+      }\n+\n+      writeIdx[axis] = zero;\n+      writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n+      writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+      storeShared(rewriter, loc, writePtr, acc, and_(laneZero, warpZero));\n+    }\n+  }\n+\n+  // set output values\n+  if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n+    // nd-tensor where n >= 1\n+    auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n+    auto resultShape = resultTy.getShape();\n+\n+    unsigned resultElems = getElemsPerThread(resultLayout, resultShape);\n+    auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultShape);\n+    assert(resultIndices.size() == resultElems);\n+\n+    barrier();\n+    SmallVector<Value> resultVals(resultElems);\n+    for (int i = 0; i < resultElems; i++) {\n+      SmallVector<Value> readIdx = resultIndices[i];\n+      readIdx.insert(readIdx.begin() + axis, i32_val(0));\n+      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n+      Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n+      resultVals[i] = load(readPtr);\n+    }\n+\n+    SmallVector<Type> resultTypes(resultElems, llvmElemTy);\n+    Type structTy =\n+        LLVM::LLVMStructType::getLiteral(this->getContext(), resultTypes);\n+    Value ret = getStructFromElements(loc, resultVals, rewriter, structTy);\n+    rewriter.replaceOp(op, ret);\n+  } else {\n+    // 0d-tensor -> scalar\n+    barrier();\n+    Value resultVal = load(smemBase);\n+    rewriter.replaceOp(op, resultVal);\n+  }\n+\n+  return success();\n+}\n+\n+/// ====================== reduce codegen end ==========================\n+\n template <typename SourceOp>\n struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n   using OpAdaptor = typename SourceOp::Adaptor;\n@@ -1742,15 +2132,16 @@ struct ConvertLayoutOpConversion\n         dstLayout.isa<DotOperandEncodingAttr>()) {\n       return lowerSharedToDotOperand(op, adaptor, rewriter);\n     }\n-    if ((!srcLayout.isa<BlockedEncodingAttr>() &&\n-         !srcLayout.isa<MmaEncodingAttr>()) ||\n-        (!dstLayout.isa<BlockedEncodingAttr>() &&\n-         !dstLayout.isa<MmaEncodingAttr>())) {\n-      // TODO: to be implemented\n-      return failure();\n+    if ((srcLayout.isa<BlockedEncodingAttr>() ||\n+         srcLayout.isa<MmaEncodingAttr>() ||\n+         srcLayout.isa<SliceEncodingAttr>()) &&\n+        (dstLayout.isa<BlockedEncodingAttr>() ||\n+         dstLayout.isa<MmaEncodingAttr>() ||\n+         dstLayout.isa<SliceEncodingAttr>())) {\n+      return lowerDistributedToDistributed(op, adaptor, rewriter);\n     }\n-\n-    return lowerDistributedToDistributed(op, adaptor, rewriter);\n+    // TODO: to be implemented\n+    return failure();\n   }\n \n private:\n@@ -1803,6 +2194,7 @@ void ConvertLayoutOpConversion::processReplica(\n   unsigned accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n   auto layout = type.getEncoding();\n   auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>();\n+  auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>();\n   auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>();\n   auto rank = type.getRank();\n   auto sizePerThread = getSizePerThread(layout);\n@@ -1820,6 +2212,18 @@ void ConvertLayoutOpConversion::processReplica(\n   if (blockedLayout) {\n     multiDimOffsetFirstElem = emitBaseIndexForBlockedLayout(\n         loc, rewriter, blockedLayout, type.getShape());\n+  } else if (sliceLayout) {\n+    unsigned dim = sliceLayout.getDim();\n+    auto parent = sliceLayout.getParent();\n+    if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n+      SmallVector<int64_t> paddedShape =\n+          sliceLayout.paddedShape(type.getShape());\n+      multiDimOffsetFirstElem = emitBaseIndexForBlockedLayout(\n+          loc, rewriter, blockedParent, paddedShape);\n+    } else {\n+      assert(0 && \"SliceEncodingAttr with parent other than \"\n+                  \"BlockedEncodingAttr not implemented\");\n+    }\n   } else if (mmaLayout) {\n     Value threadId = getThreadId(rewriter, loc);\n     Value warpSize = idx_val(32);\n@@ -1867,6 +2271,25 @@ void ConvertLayoutOpConversion::processReplica(\n                   idx_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n                           multiDimElemId[d]));\n         }\n+      } else if (sliceLayout) {\n+        unsigned dim = sliceLayout.getDim();\n+        auto parent = sliceLayout.getParent();\n+        if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n+          SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n+              elemId, blockedParent.getSizePerThread());\n+          for (unsigned d = 0; d < rank + 1; ++d) {\n+            if (d == dim)\n+              continue;\n+            unsigned slicedD = d < dim ? d : (d - 1);\n+            multiDimOffset[slicedD] =\n+                add(multiDimOffsetFirstElem[d],\n+                    idx_val(multiDimCTAInRepId[slicedD] * shapePerCTA[slicedD] +\n+                            multiDimElemId[d]));\n+          }\n+        } else {\n+          assert(0 && \"SliceEncodingAttr with parent other than \"\n+                      \"BlockedEncodingAttr not implemented\");\n+        }\n       } else if (mmaLayout) {\n         assert(rank == 2);\n         assert(mmaLayout.getVersion() == 2 &&\n@@ -1956,6 +2379,7 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n     auto multiDimRepId = getMultiDimIndex<unsigned>(repId, numReplicates);\n     barrier();\n     if (srcLayout.isa<BlockedEncodingAttr>() ||\n+        srcLayout.isa<SliceEncodingAttr>() ||\n         srcLayout.isa<MmaEncodingAttr>()) {\n       processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,\n                      multiDimRepId, inVec, paddedRepShape, outOrd, vals,\n@@ -3808,6 +4232,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<ExtElemwiseOpConversion>(typeConverter, benefit);\n \n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n+  patterns.add<ReduceOpConversion>(typeConverter, allocation, smem, benefit);\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n                                           benefit);\n "}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 45, "deletions": 10, "changes": 55, "file_content_changes": "@@ -63,6 +63,19 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n                                  blockedLayout.getSizePerThread().end());\n+  } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    unsigned dim = sliceLayout.getDim();\n+    auto parent = sliceLayout.getParent();\n+    if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n+      SmallVector<unsigned> sizePerThread(\n+          blockedParent.getSizePerThread().begin(),\n+          blockedParent.getSizePerThread().end());\n+      sizePerThread.erase(sizePerThread.begin() + dim);\n+      return sizePerThread;\n+    } else {\n+      assert(0 && \"SliceEncodingAttr with parent other than \"\n+                  \"BlockedEncodingAttr not implemented\");\n+    }\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     assert(mmaLayout.getVersion() == 2 &&\n            \"mmaLayout version = 1 is not implemented yet\");\n@@ -95,6 +108,21 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n       shape.push_back(blockedLayout.getSizePerThread()[d] *\n                       blockedLayout.getThreadsPerWarp()[d] *\n                       blockedLayout.getWarpsPerCTA()[d]);\n+  } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    unsigned dim = sliceLayout.getDim();\n+    auto parent = sliceLayout.getParent();\n+    if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n+      for (int d = 0, n = blockedParent.getOrder().size(); d < n; ++d) {\n+        if (d == dim)\n+          continue;\n+        shape.push_back(blockedParent.getSizePerThread()[d] *\n+                        blockedParent.getThreadsPerWarp()[d] *\n+                        blockedParent.getWarpsPerCTA()[d]);\n+      }\n+    } else {\n+      assert(0 && \"SliceEncodingAttr with parent other than \"\n+                  \"BlockedEncodingAttr not implemented\");\n+    }\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     assert(mmaLayout.getVersion() == 2 &&\n            \"mmaLayout version = 1 is not implemented yet\");\n@@ -206,23 +234,30 @@ unsigned BlockedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   return product<unsigned>(elemsPerThread);\n }\n \n+SmallVector<int64_t>\n+SliceEncodingAttr::paddedShape(ArrayRef<int64_t> shape) const {\n+  size_t rank = shape.size();\n+  unsigned dim = getDim();\n+  SmallVector<int64_t> retShape(rank + 1);\n+  for (unsigned d = 0; d < rank + 1; ++d) {\n+    if (d < dim)\n+      retShape[d] = shape[d];\n+    else if (d == dim)\n+      retShape[d] = 1;\n+    else\n+      retShape[d] = shape[d - 1];\n+  }\n+  return retShape;\n+}\n+\n unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   size_t rank = shape.size();\n   auto parent = getParent();\n   unsigned dim = getDim();\n   if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n     assert(rank == blockedParent.getSizePerThread().size() - 1 &&\n            \"unexpected rank in SliceEncodingAttr::getElemsPerThread\");\n-    SmallVector<int64_t> paddedShape(rank + 1);\n-    for (unsigned d = 0; d < rank + 1; ++d) {\n-      if (d < dim)\n-        paddedShape[d] = shape[d];\n-      else if (d == dim)\n-        paddedShape[d] = 1;\n-      else\n-        paddedShape[d] = shape[d - 1];\n-    }\n-    return blockedParent.getElemsPerThread(paddedShape);\n+    return blockedParent.getElemsPerThread(paddedShape(shape));\n   } else {\n     assert(0 && \"getElemsPerThread not implemented\");\n     return 0;"}, {"filename": "python/tests/test_reduce.py", "status": "added", "additions": 115, "deletions": 0, "changes": 115, "file_content_changes": "@@ -0,0 +1,115 @@\n+import pytest\n+import torch\n+from torch.testing import assert_close\n+\n+import triton\n+import triton.language as tl\n+\n+dtype_mapping = {\n+    'float16': torch.float16,\n+    'float32': torch.float32,\n+    'float64': torch.float64,\n+}\n+\n+\n+def patch_kernel(template, to_replace):\n+    kernel = triton.JITFunction(template.fn)\n+    for key, value in to_replace.items():\n+        kernel.src = kernel.src.replace(key, value)\n+    return kernel\n+\n+\n+@triton.jit\n+def reduce1d_kernel(x_ptr, z_ptr, block: tl.constexpr):\n+    x = tl.load(x_ptr + tl.arange(0, block))\n+    tl.store(z_ptr, tl.OP(x, axis=0))\n+\n+\n+@triton.jit\n+def reduce2d_kernel(x_ptr, z_ptr, axis: tl.constexpr, block_m: tl.constexpr, block_n: tl.constexpr):\n+    range_m = tl.arange(0, block_m)\n+    range_n = tl.arange(0, block_n)\n+    x = tl.load(x_ptr + range_m[:, None] * block_n + range_n[None, :])\n+    z = tl.OP(x, axis=axis)\n+    if axis == 0:\n+        tl.store(z_ptr + range_n, z)\n+    else:\n+        tl.store(z_ptr + range_m, z)\n+\n+\n+reduce1d_configs = [\n+    (op, dtype, shape)\n+    for op in ['sum', 'min', 'max']\n+    for dtype in ['float16', 'float32', 'float64']\n+    for shape in [4, 8, 16, 32, 64, 128, 512, 1024]\n+]\n+\n+\n+@pytest.mark.parametrize('op, dtype, shape', reduce1d_configs)\n+def test_reduce1d(op, dtype, shape):\n+    dtype = dtype_mapping[dtype]\n+    x = torch.randn((shape,), device='cuda', dtype=dtype)\n+    z = torch.empty(\n+        tuple(),\n+        device=x.device,\n+        dtype=dtype,\n+    )\n+\n+    kernel = patch_kernel(reduce1d_kernel, {'OP': op})\n+    grid = (1,)\n+    kernel[grid](x_ptr=x, z_ptr=z, block=shape)\n+\n+    if op == 'sum':\n+        golden_z = torch.sum(x, dtype=dtype)\n+    elif op == 'min':\n+        golden_z = torch.min(x)\n+    else:\n+        golden_z = torch.max(x)\n+\n+    if op == 'sum':\n+        if shape >= 256:\n+            assert_close(z, golden_z, rtol=0.05, atol=0.1)\n+        elif shape >= 32:\n+            assert_close(z, golden_z, rtol=0.05, atol=0.02)\n+        else:\n+            assert_close(z, golden_z, rtol=0.01, atol=0.01)\n+    else:\n+        assert_close(z, golden_z, rtol=0.001, atol=0.001)\n+\n+\n+reduce2d_configs = [\n+    (op, dtype, shape, axis)\n+    for op in ['sum', 'min', 'max']\n+    for dtype in ['float16', 'float32', 'float64']\n+    for shape in [(1, 4), (1, 8), (1, 16), (1, 32), (2, 32), (4, 32), (4, 128), (32, 64)]\n+    for axis in [0, 1]\n+]\n+\n+\n+@pytest.mark.parametrize('op, dtype, shape, axis', reduce2d_configs)\n+def test_reduce2d(op, dtype, shape, axis):\n+    dtype = dtype_mapping[dtype]\n+    x = torch.randn(shape, device='cuda', dtype=dtype)\n+    reduced_shape = (shape[1 - axis],)\n+    z = torch.empty(reduced_shape, device=x.device, dtype=dtype)\n+\n+    kernel = patch_kernel(reduce2d_kernel, {'OP': op})\n+    grid = (1,)\n+    kernel[grid](x_ptr=x, z_ptr=z, axis=axis, block_m=shape[0], block_n=shape[1])\n+\n+    if op == 'sum':\n+        golden_z = torch.sum(x, dim=axis, keepdim=False, dtype=dtype)\n+    elif op == 'min':\n+        golden_z = torch.min(x, dim=axis, keepdim=False)[0]\n+    else:\n+        golden_z = torch.max(x, dim=axis, keepdim=False)[0]\n+\n+    if op == 'sum':\n+        if shape[axis] >= 256:\n+            assert_close(z, golden_z, rtol=0.05, atol=0.1)\n+        elif shape[axis] >= 32:\n+            assert_close(z, golden_z, rtol=0.05, atol=0.02)\n+        else:\n+            assert_close(z, golden_z, rtol=0.01, atol=0.01)\n+    else:\n+        assert_close(z, golden_z, rtol=0.001, atol=0.001)"}]