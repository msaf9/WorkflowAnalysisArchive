[{"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 6, "changes": 7, "file_content_changes": "@@ -571,8 +571,6 @@ def visit_For(self, node):\n                         ast.NodeVisitor.generic_visit(self, stmt)\n                 return\n \n-        \n-\n         # collect lower bound (lb), upper bound (ub), and step\n         lb = self.visit(node.iter.args[0] if len(node.iter.args) > 1 else ast.Num(0))\n         ub = self.visit(node.iter.args[1] if len(node.iter.args) > 1 else node.iter.args[0])\n@@ -630,15 +628,14 @@ def visit_For(self, node):\n             # create YieldOp\n             self.builder.set_insertion_point_to_end(for_op.get_body(0))\n             if len(yields) > 0:\n-              self.builder.create_yield_op([y.handle for y in yields])\n+                self.builder.create_yield_op([y.handle for y in yields])\n             for_op_region = for_op.get_body(0).get_parent()\n             assert for_op_region.size() == 1, \"We use SCF, so the loop body should only have one block\"\n             # replace global uses with block arguments\n             for i, name in enumerate(names):\n                 # arg0 is the induction variable\n                 for_op.get_body(0).replace_use_in_block_with(init_args[i].handle, for_op.get_body(0).arg(i + 1))\n \n-\n         # update lscope & local_defs (ForOp defines new values)\n         for i, name in enumerate(names):\n             self.set_value(name, triton.language.core.tensor(for_op.get_result(i), yields[i].type))\n@@ -976,7 +973,6 @@ def _compile(fn, signature: str, device: int = -1, constants=dict(), specializat\n     if output == \"ttir\":\n         return module.str()\n \n-\n     # tritongpu-ir\n     module = make_tritongpu_ir(module, num_warps)\n     module = optimize_tritongpu_ir(module, num_stages)\n@@ -986,7 +982,6 @@ def _compile(fn, signature: str, device: int = -1, constants=dict(), specializat\n     print(module.str())\n     exit(1)\n \n-\n     # llvm-ir\n     llvm_ir = make_llvm_ir(module)\n "}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -299,4 +299,4 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     triton.testing.assert_almost_equal(ref_out, tri_out)\n     triton.testing.assert_almost_equal(ref_dv, tri_dv)\n     triton.testing.assert_almost_equal(ref_dk, tri_dk)\n-    triton.testing.assert_almost_equal(ref_dq, tri_dq)\n\\ No newline at end of file\n+    triton.testing.assert_almost_equal(ref_dq, tri_dq)"}]