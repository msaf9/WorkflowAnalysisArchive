[{"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -755,7 +755,9 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %26 = triton_gpu.convert_layout %23 : (tensor<128x32xf16, #blocked0>) -> tensor<128x32xf16, #shared>\n     %27 = triton_gpu.convert_layout %25 : (tensor<32x256xf16, #blocked1>) -> tensor<32x256xf16, #shared>\n \n-    // add convert_layout for dot's operands $a and $b.\n+    // add convert_layout[shared->dot_op] for dot's operands $a and $b.\n+\n+    // CHECK: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n     %a_mat = triton_gpu.convert_layout %26 : (tensor<128x32xf16, #shared>) -> tensor<128x32xf16, #dot_operand_a>\n     %b_mat = triton_gpu.convert_layout %27 : (tensor<32x256xf16, #shared>) -> tensor<32x256xf16, #dot_operand_b>\n "}]