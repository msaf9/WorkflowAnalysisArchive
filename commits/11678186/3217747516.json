[{"filename": "test/TritonGPU/swizzle.mlir", "status": "modified", "additions": 20, "deletions": 6, "changes": 26, "file_content_changes": "@@ -6,24 +6,32 @@\n #mma4w = #triton_gpu.mma<{version=2, warpsPerCTA=[2, 2]}>\n #mma8w = #triton_gpu.mma<{version=2, warpsPerCTA=[2, 4]}>\n \n-// A w8 128x64 [vec, per, max] 8 1 8\n-// B w8 64x256 [vec, per, max] 8 1 8\n+// CHECK: [[shared_v8p1m8:#.*]] = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 8, order = [1, 0]}>\n+// CHECK: [[shared_v8p2m4:#.*]] = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+// CHECK: [[shared_v8p4m2:#.*]] = #triton_gpu.shared<{vec = 8, perPhase = 4, maxPhase = 2, order = [1, 0]}>\n+\n+#shared2 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#shared3 = #triton_gpu.shared<{vec = 8, perPhase = 4, maxPhase = 2, order = [1, 0]}>\n+\n+\n module attributes {\"triton_gpu.num-warps\" = 8 : i32} {\n   // CHECK-LABEL: swizzle_mma_f16_128x256x64_w8\n   func @swizzle_mma_f16_128x256x64_w8(%A: tensor<128x64xf16, #shared>, %B: tensor<64x256xf16, #shared>) {\n     %cst0 = arith.constant dense<0.000000e+00> : tensor<128x256xf32, #mma8w>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<128x64xf16, {{.*}}>) -> tensor<128x64xf16, [[shared_v8p1m8]]>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<64x256xf16, {{.*}}>) -> tensor<64x256xf16, [[shared_v8p1m8]]>\n     %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<128x64xf16, #shared> * tensor<64x256xf16, #shared> -> tensor<128x256xf32, #mma8w>\n     return\n   }\n }\n \n \n-// A w4 128x64 [vec, per, max] 8 1 8\n-// B w4 64x128 [vec, per, max] 8 1 8\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: swizzle_mma_f16_128x128x64_w4\n   func @swizzle_mma_f16_128x128x64_w4(%A: tensor<128x64xf16, #shared>, %B: tensor<64x128xf16, #shared>) {\n     %cst0 = arith.constant dense<0.000000e+00> : tensor<128x128xf32, #mma4w>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<128x64xf16, {{.*}}>) -> tensor<128x64xf16, [[shared_v8p1m8]]>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<64x128xf16, {{.*}}>) -> tensor<64x128xf16, [[shared_v8p1m8]]>\n     %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<128x64xf16, #shared> * tensor<64x128xf16, #shared> -> tensor<128x128xf32, #mma4w>\n     return\n   }\n@@ -35,6 +43,8 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: swizzle_mma_f16_128x128x32_w4\n   func @swizzle_mma_f16_128x128x32_w4(%A: tensor<128x32xf16, #shared>, %B: tensor<32x128xf16, #shared>) {\n     %cst0 = arith.constant dense<0.000000e+00> : tensor<128x128xf32, #mma4w>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<128x32xf16, {{.*}}>) -> tensor<128x32xf16, [[shared_v8p2m4]]>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<32x128xf16, {{.*}}>) -> tensor<32x128xf16, [[shared_v8p1m8]]>\n     %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<128x32xf16, #shared> * tensor<32x128xf16, #shared> -> tensor<128x128xf32, #mma4w>\n     return\n   }\n@@ -46,6 +56,8 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   // CHECK-LABEL: swizzle_mma_f16_32x32x32_w2\n   func @swizzle_mma_f16_32x32x32_w2(%A: tensor<32x32xf16, #shared>, %B: tensor<32x32xf16, #shared>) {\n     %cst0 = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma2w>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<32x32xf16, {{.*}}>) -> tensor<32x32xf16, [[shared_v8p2m4]]>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<32x32xf16, {{.*}}>) -> tensor<32x32xf16, [[shared_v8p2m4]]>\n     %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<32x32xf16, #shared> * tensor<32x32xf16, #shared> -> tensor<32x32xf32, #mma2w>\n     return\n   }\n@@ -54,9 +66,11 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n // A w1 16x16 [vec, per, max] 8 4 2\n // B w1 16x16 [vec, per, max] 8 4 2\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK-LABEL: swizzle_mma_f16_16x16x32_w1\n-  func @swizzle_mma_f16_16x16x32_w1(%A: tensor<16x16xf16, #shared>, %B: tensor<16x16xf16, #shared>) {\n+  // CHECK-LABEL: swizzle_mma_f16_16x16x16_w1\n+  func @swizzle_mma_f16_16x16x16_w1(%A: tensor<16x16xf16, #shared>, %B: tensor<16x16xf16, #shared>) {\n     %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma1w>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<16x16xf16, {{.*}}>) -> tensor<16x16xf16, [[shared_v8p4m2]]>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<16x16xf16, {{.*}}>) -> tensor<16x16xf16, [[shared_v8p4m2]]>\n     %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<16x16xf16, #shared> * tensor<16x16xf16, #shared> -> tensor<16x16xf32, #mma1w>\n     return\n   }"}]