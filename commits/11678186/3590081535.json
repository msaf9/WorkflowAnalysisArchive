[{"filename": "include/triton/Analysis/AxisInfo.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -131,6 +131,12 @@ class AxisInfoAnalysis : public ForwardDataFlowAnalysis<AxisInfo> {\n   ChangeResult\n   visitOperation(Operation *op,\n                  ArrayRef<LatticeElement<AxisInfo> *> operands) override;\n+\n+  unsigned getPtrVectorSize(Value ptr);\n+\n+  unsigned getPtrAlignment(Value ptr);\n+\n+  unsigned getMaskAlignment(Value mask);\n };\n \n } // namespace mlir"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -25,11 +25,11 @@ namespace gpu {\n \n unsigned getElemsPerThread(Type type);\n \n-SmallVector<unsigned> getThreadsPerWarp(Attribute layout);\n+SmallVector<unsigned> getThreadsPerWarp(const Attribute &layout);\n \n-SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n+SmallVector<unsigned> getWarpsPerCTA(const Attribute &layout);\n \n-SmallVector<unsigned> getSizePerThread(Attribute layout);\n+SmallVector<unsigned> getSizePerThread(const Attribute &layout);\n \n SmallVector<unsigned> getContigPerThread(Attribute layout);\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -151,6 +151,10 @@ def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n   //  attr-dict `:` type($src) `->` type($dst)\n   //}];\n \n+  let extraClassDeclaration = [{\n+      static DenseSet<unsigned> getEligibleLoadByteWidth(int computeCapability);\n+  }];\n+\n   // The custom parser could be replaced with oilist in LLVM-16\n   let parser = [{ return parseInsertSliceAsyncOp(parser, result); }];\n "}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 42, "deletions": 0, "changes": 42, "file_content_changes": "@@ -276,4 +276,46 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n   return result;\n }\n \n+unsigned AxisInfoAnalysis::getPtrVectorSize(Value ptr) {\n+  auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+  if (!tensorTy)\n+    return 1;\n+  auto layout = tensorTy.getEncoding();\n+  auto shape = tensorTy.getShape();\n+\n+  // Here order should be ordered by contiguous first, so the first element\n+  // should have the largest contiguous.\n+  auto order = triton::gpu::getOrder(layout);\n+  unsigned align = getPtrAlignment(ptr);\n+\n+  unsigned contigPerThread = triton::gpu::getSizePerThread(layout)[order[0]];\n+  unsigned vec = std::min(align, contigPerThread);\n+  vec = std::min<unsigned>(shape[order[0]], vec);\n+\n+  return vec;\n+}\n+\n+unsigned AxisInfoAnalysis::getPtrAlignment(Value ptr) {\n+  auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+  if (!tensorTy)\n+    return 1;\n+  auto axisInfo = lookupLatticeElement(ptr)->getValue();\n+  auto layout = tensorTy.getEncoding();\n+  auto order = triton::gpu::getOrder(layout);\n+  unsigned maxMultiple = axisInfo.getDivisibility(order[0]);\n+  unsigned maxContig = axisInfo.getContiguity(order[0]);\n+  unsigned alignment = std::min(maxMultiple, maxContig);\n+  return alignment;\n+}\n+\n+unsigned AxisInfoAnalysis::getMaskAlignment(Value mask) {\n+  auto tensorTy = mask.getType().dyn_cast<RankedTensorType>();\n+  if (!tensorTy)\n+    return 1;\n+  auto maskOrder = triton::gpu::getOrder(tensorTy.getEncoding());\n+  auto maskAxis = lookupLatticeElement(mask)->getValue();\n+  auto alignment = std::max<unsigned>(maskAxis.getConstancy(maskOrder[0]), 1);\n+  return alignment;\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 66, "deletions": 55, "changes": 121, "file_content_changes": "@@ -821,7 +821,7 @@ struct ArithConstantSplatOpConversion\n // Contains some helper functions for both Load and Store conversions.\n struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n   explicit LoadStoreConversionBase(AxisInfoAnalysis &axisAnalysisPass)\n-      : AxisAnalysisPass(axisAnalysisPass) {}\n+      : axisAnalysisPass(axisAnalysisPass) {}\n \n   // Get corresponding LLVM element values of \\param value.\n   static SmallVector<Value> getLLVMElems(Value value, Value llValue,\n@@ -837,51 +837,15 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n   }\n \n   unsigned getVectorSize(Value ptr) const {\n-    auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n-    if (!tensorTy)\n-      return 1;\n-    auto layout = tensorTy.getEncoding();\n-    auto shape = tensorTy.getShape();\n-\n-    auto axisInfo = getAxisInfo(ptr);\n-    // Here order should be ordered by contiguous first, so the first element\n-    // should have the largest contiguous.\n-    auto order = getOrder(layout);\n-    unsigned align = getAlignment(ptr, layout);\n-\n-    unsigned contigPerThread = getSizePerThread(layout)[order[0]];\n-    unsigned vec = std::min(align, contigPerThread);\n-    vec = std::min<unsigned>(shape[order[0]], vec);\n-\n-    return vec;\n-  }\n-\n-  unsigned getAlignment(Value val, const Attribute &layout) const {\n-    auto axisInfo = getAxisInfo(val);\n-    auto order = getOrder(layout);\n-    unsigned maxMultiple = axisInfo->getDivisibility(order[0]);\n-    unsigned maxContig = axisInfo->getContiguity(order[0]);\n-    unsigned alignment = std::min(maxMultiple, maxContig);\n-    return alignment;\n+    return axisAnalysisPass.getPtrVectorSize(ptr);\n   }\n \n   unsigned getMaskAlignment(Value mask) const {\n-    auto tensorTy = mask.getType().cast<RankedTensorType>();\n-    auto maskOrder = getOrder(tensorTy.getEncoding());\n-    auto maskAxis = getAxisInfo(mask);\n-    return std::max<int>(maskAxis->getConstancy(maskOrder[0]), 1);\n-  }\n-\n-  llvm::Optional<AxisInfo> getAxisInfo(Value val) const {\n-    if (auto it = AxisAnalysisPass.lookupLatticeElement(val)) {\n-      return it->getValue();\n-    }\n-\n-    return llvm::Optional<AxisInfo>{};\n+    return axisAnalysisPass.getMaskAlignment(mask);\n   }\n \n protected:\n-  AxisInfoAnalysis &AxisAnalysisPass;\n+  AxisInfoAnalysis &axisAnalysisPass;\n };\n \n struct LoadOpConversion\n@@ -4601,30 +4565,68 @@ class ConvertTritonGPUToLLVM\n     });\n   }\n \n-  void decomposeInsertSliceAsyncOp(ModuleOp mod,\n-                                   TritonGPUToLLVMTypeConverter &converter) {\n-    // cp.async is supported in Ampere and later\n-    if (computeCapability >= 80)\n-      return;\n-\n+  void decomposeInsertSliceAsyncOp(ModuleOp mod) {\n+    AxisInfoAnalysis axisInfoAnalysis(mod.getContext());\n+    axisInfoAnalysis.run(mod);\n+    // TODO(Keren): This is a hacky knob that may cause performance regression\n+    // when decomposition has been performed. We should remove this knob once we\n+    // have thorough analysis on async wait. Currently, we decompose\n+    // `insert_slice_async` into `load` and `insert_slice` without knowing which\n+    // `async_wait` is responsible for the `insert_slice_async`. To guarantee\n+    // correctness, we blindly set the `async_wait` to wait for all async ops.\n+    //\n+    // There are two options to improve this:\n+    // 1. We can perform a dataflow analysis to find the `async_wait` that is\n+    // responsible for the `insert_slice_async` in the backend.\n+    // 2. We can modify the pipeline to perform the decomposition before the\n+    // `async_wait` is inserted. However, it is also risky because we don't know\n+    // the correct vectorized shape yet in the pipeline pass. Making the\n+    // pipeline pass aware of the vectorization could introduce additional\n+    // dependencies on the AxisInfoAnalysis and the Coalesce analysis.\n+    bool decomposed = false;\n     // insert_slice_async %src, %dst, %idx, %mask, %other\n     // =>\n     // %tmp = load %src, %mask, %other\n     // %res = insert_slice %tmp into %dst[%idx]\n     mod.walk([&](triton::gpu::InsertSliceAsyncOp insertSliceAsyncOp) -> void {\n       OpBuilder builder(insertSliceAsyncOp);\n-      // load\n-      auto srcTy = insertSliceAsyncOp.src().getType().cast<RankedTensorType>();\n-      auto dstTy = insertSliceAsyncOp.getType().cast<RankedTensorType>();\n+\n+      // Get the vectorized load size\n+      auto src = insertSliceAsyncOp.src();\n+      auto dst = insertSliceAsyncOp.dst();\n+      auto srcTy = src.getType().cast<RankedTensorType>();\n+      auto dstTy = dst.getType().cast<RankedTensorType>();\n       auto srcBlocked =\n           srcTy.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>();\n-      auto elemTy = converter.convertType(dstTy.getElementType());\n-      auto tmpTy = RankedTensorType::get(srcTy.getShape(), elemTy, srcBlocked);\n+      auto resSharedLayout =\n+          dstTy.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n+      auto resElemTy = dstTy.getElementType();\n+      unsigned inVec = axisInfoAnalysis.getPtrVectorSize(src);\n+      unsigned outVec = resSharedLayout.getVec();\n+      unsigned minVec = std::min(outVec, inVec);\n+      auto maxBitWidth =\n+          std::max<unsigned>(128, resElemTy.getIntOrFloatBitWidth());\n+      auto vecBitWidth = resElemTy.getIntOrFloatBitWidth() * minVec;\n+      auto bitWidth = std::min<unsigned>(maxBitWidth, vecBitWidth);\n+      auto byteWidth = bitWidth / 8;\n+\n+      // If the load byte width is not eligible or the current compute\n+      // capability does not support async copy, then we do decompose\n+      if (triton::gpu::InsertSliceAsyncOp::getEligibleLoadByteWidth(\n+              computeCapability)\n+              .contains(byteWidth) &&\n+          computeCapability >= 80)\n+        return;\n+\n+      // load\n+      auto tmpTy =\n+          RankedTensorType::get(srcTy.getShape(), resElemTy, srcBlocked);\n       auto loadOp = builder.create<triton::LoadOp>(\n           insertSliceAsyncOp.getLoc(), tmpTy, insertSliceAsyncOp.src(),\n           insertSliceAsyncOp.mask(), insertSliceAsyncOp.other(),\n           insertSliceAsyncOp.cache(), insertSliceAsyncOp.evict(),\n           insertSliceAsyncOp.isVolatile());\n+\n       // insert_slice\n       auto axis = insertSliceAsyncOp.axis();\n       auto intAttr = [&](int64_t v) { return builder.getI64IntegerAttr(v); };\n@@ -4642,10 +4644,20 @@ class ConvertTritonGPUToLLVM\n       // Replace\n       insertSliceAsyncOp.replaceAllUsesWith(insertSliceOp.getResult());\n       insertSliceAsyncOp.erase();\n+      decomposed = true;\n     });\n \n+    // async wait is supported in Ampere and later\n     mod.walk([&](triton::gpu::AsyncWaitOp asyncWaitOp) -> void {\n-      asyncWaitOp.erase();\n+      if (computeCapability < 80) {\n+        asyncWaitOp.erase();\n+      } else if (decomposed) {\n+        OpBuilder builder(asyncWaitOp);\n+        // Wait for all previous async ops\n+        auto newAsyncWaitOp = builder.create<triton::gpu::AsyncWaitOp>(\n+            asyncWaitOp.getLoc(), builder.getI64IntegerAttr(0));\n+        asyncWaitOp.erase();\n+      }\n     });\n   }\n \n@@ -4668,7 +4680,7 @@ class ConvertTritonGPUToLLVM\n \n     // step 1: Decompose unoptimized layout conversions to use shared memory\n     // step 2: Decompose insert_slice_async to use load + insert_slice for\n-    // pre-Ampere architectures\n+    // pre-Ampere architectures or unsupported vectorized load sizes\n     // step 3: Allocate shared memories and insert barriers\n     // step 4: Convert SCF to CFG\n     // step 5: Convert FuncOp to LLVMFuncOp via partial conversion\n@@ -4678,10 +4690,9 @@ class ConvertTritonGPUToLLVM\n     // separation between 1/4 is that, step 3 is out of the scope of Dialect\n     // Conversion, thus we need to make sure the smem is not revised during the\n     // conversion of step 4.\n-\n     decomposeBlockedToDotOperand(mod);\n \n-    decomposeInsertSliceAsyncOp(mod, typeConverter);\n+    decomposeInsertSliceAsyncOp(mod);\n \n     Allocation allocation(mod);\n     MembarAnalysis membarPass(&allocation);"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 12, "deletions": 3, "changes": 15, "file_content_changes": "@@ -71,7 +71,7 @@ unsigned getElemsPerThread(Type type) {\n   return getElemsPerThread(tensorType.getEncoding(), tensorType.getShape());\n }\n \n-SmallVector<unsigned> getThreadsPerWarp(Attribute layout) {\n+SmallVector<unsigned> getThreadsPerWarp(const Attribute &layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getThreadsPerWarp().begin(),\n                                  blockedLayout.getThreadsPerWarp().end());\n@@ -86,7 +86,7 @@ SmallVector<unsigned> getThreadsPerWarp(Attribute layout) {\n   return {};\n }\n \n-SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n+SmallVector<unsigned> getWarpsPerCTA(const Attribute &layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getWarpsPerCTA().begin(),\n                                  blockedLayout.getWarpsPerCTA().end());\n@@ -99,7 +99,7 @@ SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n   return {};\n }\n \n-SmallVector<unsigned> getSizePerThread(Attribute layout) {\n+SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n                                  blockedLayout.getSizePerThread().end());\n@@ -659,6 +659,15 @@ void printInsertSliceAsyncOp(OpAsmPrinter &printer,\n   printer.printStrippedAttrOrType(insertSliceAsyncOp.result().getType());\n }\n \n+DenseSet<unsigned>\n+InsertSliceAsyncOp::getEligibleLoadByteWidth(int computeCapability) {\n+  DenseSet<unsigned> validLoadBytes;\n+  if (computeCapability >= 80) {\n+    validLoadBytes = {4, 8, 16};\n+  }\n+  return validLoadBytes;\n+}\n+\n //===----------------------------------------------------------------------===//\n // ASM Interface (i.e.: alias)\n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 37, "deletions": 39, "changes": 76, "file_content_changes": "@@ -25,18 +25,20 @@ static Type getI1SameShape(Value v) {\n                                tensorType.getEncoding());\n }\n \n+#define int_attr(num) builder.getI64IntegerAttr(num)\n+\n namespace {\n \n class LoopPipeliner {\n-  /// cache forOp we are working on\n+  /// Cache forOp we are working on\n   scf::ForOp forOp;\n \n-  /// cache YieldOp for this forOp\n+  /// Cache YieldOp for this forOp\n   scf::YieldOp yieldOp;\n \n-  /// loads to be pipelined\n+  /// Loads to be pipelined\n   SetVector<Value> loads;\n-  /// the value that each load will be mapped to (after layout conversion)\n+  /// The value that each load will be mapped to (after layout conversion)\n   DenseMap<Value, Value> loadsMapping;\n   /// load => buffer\n   DenseMap<Value, Value> loadsBuffer;\n@@ -51,7 +53,7 @@ class LoopPipeliner {\n   ///\n   Value loopIterIdx;\n \n-  /// comments on numStages:\n+  /// Comments on numStages:\n   ///   [0, numStages-1) are in the prologue\n   ///   numStages-1 is appended after the loop body\n   int numStages;\n@@ -61,6 +63,7 @@ class LoopPipeliner {\n \n   /// Block arguments that loads depend on\n   DenseSet<BlockArgument> depArgs;\n+\n   /// Operations (inside the loop body) that loads depend on\n   DenseSet<Operation *> depOps;\n \n@@ -71,7 +74,7 @@ class LoopPipeliner {\n \n   Value lookupOrDefault(Value origin, int stage);\n \n-  /// returns a empty buffer of size <numStages, ...>\n+  /// Returns a empty buffer of size <numStages, ...>\n   ttg::AllocTensorOp allocateEmptyBuffer(Operation *op, OpBuilder &builder);\n \n public:\n@@ -84,7 +87,7 @@ class LoopPipeliner {\n   /// Collect loads to pipeline. Return success if we can pipeline this loop\n   LogicalResult initialize();\n \n-  /// emit pipelined loads (before loop body)\n+  /// Emit pipelined loads (before loop body)\n   void emitPrologue();\n \n   /// emit pipelined loads (after loop body)\n@@ -134,7 +137,7 @@ void LoopPipeliner::collectDeps(Value v, int stages, DenseSet<Value> &deps) {\n \n ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(Operation *op,\n                                                       OpBuilder &builder) {\n-  // allocate a buffer for each pipelined tensor\n+  // Allocate a buffer for each pipelined tensor\n   // shape: e.g. (numStages==4), <32x64xbf16> -> <4x32x64xbf16>\n   Value convertLayout = loadsMapping[op->getResult(0)];\n   if (auto tensorType = convertLayout.getType().dyn_cast<RankedTensorType>()) {\n@@ -215,9 +218,9 @@ LogicalResult LoopPipeliner::initialize() {\n       loads.insert(loadOp);\n   }\n \n-  // we have some loads to pipeline\n+  // We have some loads to pipeline\n   if (!loads.empty()) {\n-    // update depArgs & depOps\n+    // Update depArgs & depOps\n     for (Value loadOp : loads) {\n       for (Value dep : loadDeps[loadOp]) {\n         // TODO: we should record the stage that the value is depended on\n@@ -244,23 +247,20 @@ void LoopPipeliner::emitPrologue() {\n     setValueMapping(arg, operand.get(), 0);\n   }\n \n-  // helper to construct int attribute\n-  auto intAttr = [&](int64_t val) { return builder.getI64IntegerAttr(val); };\n-\n   // prologue from [0, numStage-1)\n   Value iv = forOp.getLowerBound();\n   pipelineIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n   for (int stage = 0; stage < numStages - 1; ++stage) {\n-    // special handling for induction variable as the increment is implicit\n+    // Special handling for induction variable as the increment is implicit\n     if (stage != 0)\n       iv = builder.create<arith::AddIOp>(iv.getLoc(), iv, forOp.getStep());\n     setValueMapping(forOp.getInductionVar(), iv, stage);\n \n-    // special handling for loop condition as there is no condition in ForOp\n+    // Special handling for loop condition as there is no condition in ForOp\n     Value loopCond = builder.create<arith::CmpIOp>(\n         iv.getLoc(), arith::CmpIPredicate::slt, iv, forOp.getUpperBound());\n \n-    // rematerialize peeled values\n+    // Rematerialize peeled values\n     SmallVector<Operation *> orderedDeps;\n     for (Operation &op : forOp.getLoopBody().front()) {\n       if (depOps.contains(&op))\n@@ -314,7 +314,7 @@ void LoopPipeliner::emitPrologue() {\n         }\n       }\n \n-      // update mapping of results\n+      // Update mapping of results\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n         Value originalResult = op->getResult(dstIdx);\n         // copy_async will update the value of its only use\n@@ -350,13 +350,14 @@ void LoopPipeliner::emitPrologue() {\n                               loadsBufferType[loadOp].getEncoding());\n     Value extractSlice = builder.create<tensor::ExtractSliceOp>(\n         loadOp.getLoc(), sliceType, loadStageBuffer[loadOp][numStages - 1],\n-        SmallVector<OpFoldResult>{intAttr(0), intAttr(0), intAttr(0)},\n-        SmallVector<OpFoldResult>{intAttr(1), intAttr(sliceType.getShape()[0]),\n-                                  intAttr(sliceType.getShape()[1])},\n-        SmallVector<OpFoldResult>{intAttr(1), intAttr(1), intAttr(1)});\n+        SmallVector<OpFoldResult>{int_attr(0), int_attr(0), int_attr(0)},\n+        SmallVector<OpFoldResult>{int_attr(1),\n+                                  int_attr(sliceType.getShape()[0]),\n+                                  int_attr(sliceType.getShape()[1])},\n+        SmallVector<OpFoldResult>{int_attr(1), int_attr(1), int_attr(1)});\n     loadsExtract[loadOp] = extractSlice;\n   }\n-  // bump up loopIterIdx, this is used for getting the correct slice for the\n+  // Bump up loopIterIdx, this is used for getting the correct slice for the\n   // *next* iteration\n   loopIterIdx = builder.create<arith::AddIOp>(\n       loopIterIdx.getLoc(), loopIterIdx,\n@@ -365,9 +366,6 @@ void LoopPipeliner::emitPrologue() {\n \n void LoopPipeliner::emitEpilogue() {\n   // If there's any outstanding async copies, we need to wait for them.\n-  // TODO(Keren): We may want to completely avoid the async copies in the last\n-  // few iterations by setting is_masked attribute to true. We don't want to use\n-  // the mask operand because it's a tensor but not a scalar.\n   OpBuilder builder(forOp);\n   OpBuilder::InsertionGuard g(builder);\n   builder.setInsertionPointAfter(forOp);\n@@ -376,9 +374,8 @@ void LoopPipeliner::emitEpilogue() {\n \n scf::ForOp LoopPipeliner::createNewForOp() {\n   OpBuilder builder(forOp);\n-  auto intAttr = [&](int64_t v) { return builder.getI64IntegerAttr(v); };\n \n-  // order of new args:\n+  // Order of new args:\n   //   (original args),\n   //   (insertSliceAsync buffer at stage numStages - 1)  for each load\n   //   (extracted tensor)  for each load\n@@ -465,15 +462,15 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n                     newForOp.getRegionIterArgs()[argIdx + depArgsBeginIdx]);\n     ++argIdx;\n   }\n-  // special handling for iv & loop condition\n+  // Special handling for iv & loop condition\n   Value nextIV = builder.create<arith::AddIOp>(\n       newForOp.getInductionVar().getLoc(),\n       newForOp.getRegionIterArgs()[nextIVIdx], newForOp.getStep());\n   Value nextLoopCond =\n       builder.create<arith::CmpIOp>(nextIV.getLoc(), arith::CmpIPredicate::slt,\n                                     nextIV, newForOp.getUpperBound());\n \n-  // slice index\n+  // Slice index\n   SmallVector<Value> nextBuffers;\n   SmallVector<Value> extractSlices;\n \n@@ -490,7 +487,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n-    // update loading mask\n+    // Update loading mask\n     if (loads.contains(op->getResult(0))) {\n       auto loadOp = llvm::cast<triton::LoadOp>(op);\n       Value mask = loadOp.mask();\n@@ -500,7 +497,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n             mask.getLoc(), mask.getType(), nextLoopCond);\n         newMask = builder.create<arith::AndIOp>(\n             mask.getLoc(), splatCond, nextMapping.lookupOrDefault(mask));\n-        // if mask is defined outside the loop, don't update the map more than\n+        // If mask is defined outside the loop, don't update the map more than\n         // once\n         if (!(forOp.isDefinedOutsideOfLoop(mask) && nextMapping.contains(mask)))\n           nextMapping.map(mask, newMask);\n@@ -522,18 +519,19 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n                                         loadsBufferType[loadOp].getEncoding());\n       nextOp = builder.create<tensor::ExtractSliceOp>(\n           op->getLoc(), sliceType, insertAsyncOp,\n-          SmallVector<OpFoldResult>{extractSliceIndex, intAttr(0), intAttr(0)},\n-          SmallVector<OpFoldResult>{intAttr(1),\n-                                    intAttr(sliceType.getShape()[0]),\n-                                    intAttr(sliceType.getShape()[1])},\n-          SmallVector<OpFoldResult>{intAttr(1), intAttr(1), intAttr(1)});\n+          SmallVector<OpFoldResult>{extractSliceIndex, int_attr(0),\n+                                    int_attr(0)},\n+          SmallVector<OpFoldResult>{int_attr(1),\n+                                    int_attr(sliceType.getShape()[0]),\n+                                    int_attr(sliceType.getShape()[1])},\n+          SmallVector<OpFoldResult>{int_attr(1), int_attr(1), int_attr(1)});\n       extractSlices.push_back(nextOp->getResult(0));\n     } else\n       nextOp = builder.clone(*op, nextMapping);\n-    // update mapping of results\n+    // Update mapping of results\n     for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n       nextMapping.map(op->getResult(dstIdx), nextOp->getResult(dstIdx));\n-      // if this is a loop-carried value, update the mapping for yield\n+      // If this is a loop-carried value, update the mapping for yield\n       auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n       for (OpOperand &operand : originYield->getOpOperands()) {\n         if (operand.get() == op->getResult(dstIdx)) {\n@@ -583,7 +581,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n     it->getDefiningOp()->moveAfter(asyncWait);\n   }\n \n-  // bump iteration count\n+  // Bump iteration count\n   pipelineIterIdx = builder.create<arith::AddIOp>(\n       nextIV.getLoc(), pipelineIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), 1, 32));"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -172,8 +172,9 @@ def get_proper_err(a, b, golden):\n     [128, 64, 128, 4, 128, 64, 128, False, False],\n     [16, 16, 16, 16, 16, 16, 16, False, False],  # wpt overflow issue\n     # K-Forloop\n-    [32, 32, 64, 4, 32, 32, 32, False, False], # Single shared encoding\n-    [16, 16, 128, 4, 16, 16, 16, False, False], # Single shared encoding and small k\n+    #[16, 16, 64, 4, 8, 8, 8, False, False],  # Wrap threads\n+    [32, 32, 64, 4, 32, 32, 32, False, False],  # Single shared encoding\n+    [16, 16, 128, 4, 16, 16, 16, False, False],  # Single shared encoding and small k\n     [64, 32, 128, 4, 64, 32, 64, False, False],\n     [128, 16, 128, 4, 128, 16, 32, False, False],\n     [32, 16, 128, 4, 32, 16, 32, False, False],"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 39, "deletions": 0, "changes": 39, "file_content_changes": "@@ -387,6 +387,45 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // -----\n \n+#block0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [4], warpsPerCTA = [4], order = [0]}>\n+#block1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [8], warpsPerCTA = [4], order = [0]}>\n+#block2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 1], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#block3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 8], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#slice2d1 = #triton_gpu.slice<{dim = 1, parent=#block2}>\n+#slice3d0 = #triton_gpu.slice<{dim = 0, parent=#block3}>\n+#AL = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#A = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: basic_insert_slice_async_fallback\n+  func @basic_insert_slice_async_fallback(%arg0: !tt.ptr<f16> {tt.divisibility = 1 : i32}) {\n+    %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #slice2d1>\n+    %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<64xi32, #slice3d0>\n+    %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #slice2d1>) -> tensor<16x1xi32, #block2>\n+    %off1 = tt.expand_dims %off1_ {axis = 0 : i32} : (tensor<64xi32, #slice3d0>) -> tensor<1x64xi32, #block3>\n+    %broadcast_off0_scalar = tt.broadcast %off0 : (tensor<16x1xi32, #block2>) -> tensor<16x64xi32, #block2>\n+    %cst_scalar = arith.constant 64 : i32\n+    %cst = tt.splat %cst_scalar : (i32) -> tensor<16x64xi32, #block2>\n+    %broadcast_off0_ = arith.muli %broadcast_off0_scalar, %cst : tensor<16x64xi32, #block2>\n+    %broadcast_off1_ = tt.broadcast %off1 : (tensor<1x64xi32, #block3>) -> tensor<16x64xi32, #block3>\n+    %broadcast_off0 = triton_gpu.convert_layout %broadcast_off0_ : (tensor<16x64xi32, #block2>) -> tensor<16x64xi32, #AL>\n+    %broadcast_off1 = triton_gpu.convert_layout %broadcast_off1_ : (tensor<16x64xi32, #block3>) -> tensor<16x64xi32, #AL>\n+    %off = arith.addi %broadcast_off0, %broadcast_off1 : tensor<16x64xi32, #AL>\n+    %a_init = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<16x64x!tt.ptr<f16>, #AL>\n+    %a_ptr = tt.addptr %a_init, %off : tensor<16x64x!tt.ptr<f16>, #AL>\n+    %tensor = triton_gpu.alloc_tensor : tensor<2x16x64xf16, #A>\n+    %index = arith.constant 1 : i32\n+\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<8xi32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<8xi32>, 3>\n+    %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x64x!tt.ptr<f16>, #AL> -> tensor<2x16x64xf16, #A>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #block0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [4], warpsPerCTA = [4], order = [0]}>\n #block1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [8], warpsPerCTA = [4], order = [0]}>\n #block2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 1], warpsPerCTA = [4, 1], order = [1, 0]}>"}]