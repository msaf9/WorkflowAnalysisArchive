[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 21, "deletions": 7, "changes": 28, "file_content_changes": "@@ -2147,6 +2147,7 @@ def _kernel(dst):\n @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n                          [('int32', 'math.ffs', ''),\n                           ('float32', 'math.log2', ''),\n+                          ('float32', 'math.scalbn', ''),\n                           ('float32', 'math.pow', tl.math.libdevice_path()),\n                           ('float64', 'math.pow_dtype', tl.math.libdevice_path()),\n                           ('float64', 'math.norm4d', '')])\n@@ -2164,24 +2165,31 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n \n     if expr == 'math.log2':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.broadcast_to(tl.math.log2(5.0), x.shape)'})\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.broadcast_to(tl.{expr}(5.0), x.shape)'})\n         y_ref = np.log2(5.0)\n     elif expr == 'math.ffs':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.ffs(x)'})\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{expr}(x)'})\n         y_ref = np.zeros(shape, dtype=x.dtype)\n         for i in range(shape[0]):\n             y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n+    elif expr == 'math.scalbn':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{expr}(x, 2)'})\n+        y_ref = x * pow(2, 2)\n+    elif expr == 'math.pow_dtype':\n+        x = np.abs(x)\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.math.pow(x, 0.5)'})\n+        y_ref = np.power(x, 0.5)\n     elif expr == 'math.pow':\n         # numpy does not allow negative factors in power, so we use abs()\n         x = np.abs(x)\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.pow(x, x)'})\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{expr}(x, x)'})\n         y_ref = np.power(x, x)\n     elif expr == 'math.pow_dtype':\n         x = np.abs(x)\n         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.pow(x, 0.5)'})\n         y_ref = np.power(x, 0.5)\n     elif expr == 'math.norm4d':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.norm4d(x, x, x, x)'})\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{expr}(x, x, x, x)'})\n         y_ref = np.sqrt(4 * np.power(x, 2))\n \n     x_tri = to_triton(x)\n@@ -2197,6 +2205,7 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n \n @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n                          [('float32', 'math.pow', ''),\n+                          ('float64', 'math.pow_dtype', ''),\n                           ('float64', 'math.pow', tl.math.libdevice_path())])\n def test_math_scalar(dtype_str, expr, lib_path):\n \n@@ -2213,9 +2222,14 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     y_ref = np.zeros(shape, dtype=x.dtype)\n \n     # numpy does not allow negative factors in power, so we use abs()\n-    x = np.abs(x)\n-    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.pow(x, x)'})\n-    y_ref[:] = np.power(x, x)\n+    if expr == 'math.pow':\n+        x = np.abs(x)\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.pow(x, x)'})\n+        y_ref[:] = np.power(x, x)\n+    elif expr == 'math.pow_dtype':\n+        x = np.abs(x)\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.pow(x, 0.5)'})\n+        y_ref[:] = np.power(x, 0.5)\n \n     # triton result\n     x_tri = to_triton(x)[0].item()"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 13, "deletions": 4, "changes": 17, "file_content_changes": "@@ -1582,27 +1582,36 @@ def extern_elementwise(lib_name: str, lib_path: str, args: list, arg_type_symbol\n         :param lib_path: the path of the library\n         :param args: the arguments of the function\n         :param arg_type_symbol_dict: the type of the arguments\n+        :param is_pure: whether the function is pure\n         :param _builder: the builder\n         :return: the return value of the function\n     '''\n     dispatch_args = args.copy()\n     all_scalar = True\n     ret_shape = None\n+    arg_types = []\n     for i in range(len(dispatch_args)):\n         dispatch_args[i] = _to_tensor(dispatch_args[i], _builder)\n+        arg_types.append(dispatch_args[i].dtype)\n         if dispatch_args[i].type.is_block():\n             all_scalar = False\n-    if not all_scalar:\n+    if len(arg_types) > 0:\n+        arg_types = tuple(arg_types)\n+        arithmetic_check = True\n+        # If there's a type tuple that is not supported by the library, we will do arithmetic check\n+        if arg_types in arg_type_symbol_dict:\n+            arithmetic_check = False\n         broadcast_arg = dispatch_args[0]\n         # Get the broadcast shape over all the arguments\n         for i, item in enumerate(dispatch_args):\n             _, broadcast_arg = semantic.binary_op_type_checking_impl(\n-                item, broadcast_arg, _builder)\n+                item, broadcast_arg, _builder, arithmetic_check=arithmetic_check)\n         # Change the shape of each argument based on the broadcast shape\n         for i in range(len(dispatch_args)):\n             dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n-                dispatch_args[i], broadcast_arg, _builder)\n-        ret_shape = broadcast_arg.shape\n+                dispatch_args[i], broadcast_arg, _builder, arithmetic_check=arithmetic_check)\n+        if not all_scalar:\n+            ret_shape = broadcast_arg.shape\n     func = getattr(_builder, \"create_extern_elementwise\")\n     return dispatch(func, lib_name, lib_path, dispatch_args, arg_type_symbol_dict, ret_shape, is_pure, _builder)\n "}, {"filename": "python/triton/language/math.py", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "file_content_changes": "@@ -1,13 +1,18 @@\n import functools\n import os\n \n-from ..runtime import driver\n from . import core\n \n \n @functools.lru_cache()\n def libdevice_path():\n-    return os.getenv(\"TRITON_LIBDEVICE_PATH\", driver.libdevice_path)\n+    import torch\n+    third_party_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\")\n+    if torch.version.hip is None:\n+        default = os.path.join(third_party_dir, \"cuda\", \"lib\", \"libdevice.10.bc\")\n+    else:\n+        default = ''\n+    return os.getenv(\"TRITON_LIBDEVICE_PATH\", default)\n \n \n @core.extern"}, {"filename": "python/triton/runtime/driver.py", "status": "modified", "additions": 0, "deletions": 13, "changes": 13, "file_content_changes": "@@ -61,17 +61,9 @@ def __new__(cls):\n             cls.instance = super(CudaDriver, cls).__new__(cls)\n         return cls.instance\n \n-    def get_extern_path(self):\n-        return os.path.join(self.third_party_dir(), \"cuda\", \"lib\")\n-\n-    def get_libdevice_path(self):\n-        return os.path.join(self.third_party_dir(), \"cuda\", \"lib\", \"libdevice.10.bc\")\n-\n     def __init__(self):\n         self.utils = CudaUtils()\n         self.backend = self.CUDA\n-        self.libdevice_path = self.get_libdevice_path()\n-        self.extern_path = self.get_extern_path()\n \n # -----------------------------\n # HIP\n@@ -114,13 +106,9 @@ def __new__(cls):\n             cls.instance = super(HIPDriver, cls).__new__(cls)\n         return cls.instance\n \n-    def get_libdevice_path(self):\n-        return os.path.join(self.third_party_dir(), \"third_party\", \"rocm\", \"lib\", \"libdevice.10.bc\")\n-\n     def __init__(self):\n         self.utils = HIPUtils()\n         self.backend = self.HIP\n-        self.libdevice_path = self.get_libdevice_path()\n \n \n class UnsupportedDriver(DriverBase):\n@@ -133,7 +121,6 @@ def __new__(cls):\n     def __init__(self):\n         self.utils = None\n         self.backend = None\n-        self.libdevice_path = ''\n \n # -----------------------------\n # Driver"}, {"filename": "python/triton/tools/build_extern.py", "status": "modified", "additions": 8, "deletions": 3, "changes": 11, "file_content_changes": "@@ -287,16 +287,21 @@ def _output_stubs(self) -> str:\n         # @extern.extern\n         # def <op_name>(<args>, _builder=None):\n         #   arg_type_symbol_dict = {[arg_type]: {(symbol, ret_type)}}\n-        #   return extern.dispatch(\"libdevice\", <path>, <args>, <arg_type_symbol_dict>, _builder)\n+        #   return core.extern_elementwise(\"libdevice\", <path>, <args>, <arg_type_symbol_dict>, _builder)\n         import_str = \"from . import core\\n\"\n-        import_str += \"from ..runtime import driver\\n\"\n         import_str += \"import os\\n\"\n         import_str += \"import functools\\n\"\n \n         header_str = \"\"\n         header_str += \"@functools.lru_cache()\\n\"\n         header_str += \"def libdevice_path():\\n\"\n-        header_str += \"    return os.getenv(\\\"TRITON_LIBDEVICE_PATH\\\", driver.libdevice_path)\\n\"\n+        header_str += \"    import torch\\n\"\n+        header_str += \"    third_party_dir =  os.path.join(os.path.dirname(os.path.abspath(__file__)), \\\"..\\\", \\\"third_party\\\")\\n\"\n+        header_str += \"    if torch.version.hip is None:\\n\"\n+        header_str += \"        default = os.path.join(third_party_dir, \\\"cuda\\\", \\\"lib\\\", \\\"libdevice.10.bc\\\")\\n\"\n+        header_str += \"    else:\\n\"\n+        header_str += \"        default = ''\\n\"\n+        header_str += \"    return os.getenv(\\\"TRITON_LIBDEVICE_PATH\\\", default)\\n\"\n         func_str = \"\"\n         for symbols in self._symbol_groups.values():\n             func_str += \"@core.extern\\n\""}]