[{"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -475,17 +475,17 @@ Attribute MmaEncodingAttr::parse(AsmParser &parser, Type type) {\n   if (parser.parseGreater().failed())\n     return {};\n \n-  unsigned majorVersion = 0;\n-  unsigned minorVersion = 0;\n+  unsigned versionMajor = 0;\n+  unsigned versionMinor = 0;\n   SmallVector<unsigned, 2> warpsPerCTA;\n \n   for (const NamedAttribute &attr : dict) {\n-    if (attr.getName() == \"majorVersion\") {\n-      if (parseUInt(parser, attr, majorVersion, \"majorVersion\").failed())\n+    if (attr.getName() == \"versionMajor\") {\n+      if (parseUInt(parser, attr, versionMajor, \"versionMajor\").failed())\n         return {};\n     }\n-    if (attr.getName() == \"minorVersion\") {\n-      if (parseUInt(parser, attr, minorVersion, \"minorVersion\").failed())\n+    if (attr.getName() == \"versionMinor\") {\n+      if (parseUInt(parser, attr, versionMinor, \"versionMinor\").failed())\n         return {};\n     }\n     if (attr.getName() == \"warpsPerCTA\") {\n@@ -494,8 +494,8 @@ Attribute MmaEncodingAttr::parse(AsmParser &parser, Type type) {\n     }\n   }\n \n-  return parser.getChecked<MmaEncodingAttr>(parser.getContext(), majorVersion,\n-                                            minorVersion, warpsPerCTA);\n+  return parser.getChecked<MmaEncodingAttr>(parser.getContext(), versionMajor,\n+                                            versionMinor, warpsPerCTA);\n }\n \n void MmaEncodingAttr::print(AsmPrinter &printer) const {"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -4,7 +4,7 @@\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n-#C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n+#C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n "}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -5,7 +5,7 @@\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n-#C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n+#C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n "}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -5,7 +5,7 @@\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n-#C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n+#C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n "}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -748,7 +748,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n // -----\n \n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [1, 0]}>\n-#mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [2, 2]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mmav2_block\n@@ -768,7 +768,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n // -----\n \n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [1, 0]}>\n-#mma = #triton_gpu.mma<{version = 1, warpsPerCTA = [2, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 1, warpsPerCTA = [2, 1]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mmav1_block\n@@ -853,7 +853,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n \n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n-#mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [2, 2]}>\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n@@ -878,7 +878,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n-#mma = #triton_gpu.mma<{version = 1, warpsPerCTA = [2, 2]}>\n+#mma = #triton_gpu.mma<{versionMajor = 1, warpsPerCTA = [2, 2]}>\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma, isMMAv1Row=true}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma, isMMAv1Row=true}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -4,7 +4,7 @@\n // matmul: 128x32 @ 32x128 -> 128x128\n #AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n-#C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n+#C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n "}]