[{"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -1,3 +1,30 @@\n+//===----------------------------------------------------------------------===//\n+//\n+// This pass tries to prefetch operands (a and b) of tt.dot.\n+// Those ConvertLayoutOps will be lowered to shared memory loads.\n+//\n+// For example:\n+// %a: tensor<128x32xf16, #enc>\n+// scf.for %iv = ... iter_args(%a_arg = %a, ...) {\n+//   %d = tt.dot %a_arg, %b, %c\n+//   ...\n+//   scf.yield %a_next, ...\n+// }\n+//\n+// will be translated to\n+//\n+// %a: tensor<128x32xf16, #enc>\n+// %a_tmp = tensor.extract_slice %a[0, 0] [128, 16]\n+// %a_prefetch = triton_gpu.convert_layout %a_tmp\n+// scf.for %iv = ... iter_args(%a_buf = %a, ..., %a_prefetch_arg = %a_prefetch) {\n+//   %x = tt.dot %a_arg, %b, %c\n+//   %a_tmp_rem = tensor.extract_slice %a_buf[0, 16] [128, 16]\n+//   %a_prefetch_next = triton_gpu.convert_layout %a_tmp_rem\n+//   ...\n+//   scf.yield %next_a, ..., %a_prefetch_next\n+// }\n+//===----------------------------------------------------------------------===//\n+\n #include \"mlir/IR/BlockAndValueMapping.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\""}]