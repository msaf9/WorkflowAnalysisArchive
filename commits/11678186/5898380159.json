[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -97,7 +97,6 @@ class ScanLoweringHelper {\n   explicit ScanLoweringHelper(triton::ScanOp op) : scanOp(op) {\n     auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n     srcEncoding = type.getEncoding();\n-    srcShapePerCTA = triton::gpu::getShapePerCTA(type);\n   }\n   // Return true if the lowering of the scan op is supported.\n   bool isSupported();\n@@ -135,7 +134,6 @@ class ScanLoweringHelper {\n private:\n   triton::ScanOp scanOp;\n   Attribute srcEncoding;\n-  ArrayRef<int64_t> srcShapePerCTA;\n };\n \n bool maybeSharedAllocationOp(Operation *op);\n@@ -362,17 +360,19 @@ std::unique_ptr<DataFlowSolver> createDataFlowSolver();\n \n triton::MakeTensorPtrOp getMakeTensorPtrOp(Value v);\n \n-SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n-                                                ArrayRef<int64_t> shape,\n-                                                StringRef inputType);\n SmallVector<unsigned, 3>\n mmaVersionToInstrShape(int version, ArrayRef<int64_t> shape, Type type);\n-SmallVector<unsigned, 3>\n-mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n-                       ArrayRef<int64_t> shape);\n \n Value getParentValueWithSameEncoding(Attribute layout, Value value);\n \n+inline Type getInput0EltType(Value value) {\n+  return value.getDefiningOp()\n+      ->getOperands()[0]\n+      .getType()\n+      .cast<RankedTensorType>()\n+      .getElementType();\n+}\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_UTILITY_H"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 1, "deletions": 5, "changes": 6, "file_content_changes": "@@ -39,9 +39,6 @@ SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n SmallVector<unsigned> getSizePerThread(BlockedEncodingAttr layout);\n SmallVector<unsigned> getSizePerThread(Value value);\n \n-SmallVector<unsigned> getSizePerThread(Attribute layout,\n-                                       ArrayRef<int64_t> shapePerCTA);\n-\n // Returns the number of contiguous elements that each thread\n // has access to, on each dimension of the tensor. E.g.\n // for a blocked layout with sizePerThread = [1, 4], returns [1, 4],\n@@ -71,8 +68,7 @@ getThreadsPerWarpWithUniqueData(Attribute layout,\n // 1], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4] and tensor shape = [2, 2],\n // returns [1, 1], since the first warp has access to the full tensor, whereas\n // the other warps have access to replicated elements.\n-SmallVector<unsigned>\n-getWarpsPerCTAWithUniqueData(Attribute layout, ArrayRef<int64_t> tensorShape);\n+SmallVector<unsigned> getWarpsPerCTAWithUniqueData(Value value);\n \n SmallVector<unsigned> getThreadsPerCTA(Attribute layout);\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 2, "deletions": 5, "changes": 7, "file_content_changes": "@@ -515,8 +515,7 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n     \"unsigned\":$versionMajor,\n     \"unsigned\":$versionMinor,\n     ArrayRefParameter<\"unsigned\">:$warpsPerCTA,\n-    \"CTALayoutAttr\":$CTALayout,\n-    StringRefParameter<\"\">:$inputType\n+    \"CTALayoutAttr\":$CTALayout\n   );\n \n   let builders = [\n@@ -561,7 +560,7 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n           wpt[1] = std::clamp<int>(wpt[1] * 2, 1, shapeC[1] / spw[1]);\n       } while (wpt_nm1 != wpt);\n \n-      return $_get(context, versionMajor, versionMinor, wpt, CTALayout, \"\");\n+      return $_get(context, versionMajor, versionMinor, wpt, CTALayout);\n     }]>,\n \n \n@@ -586,8 +585,6 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n     bool isAmpere() const;\n     bool isHopper() const;\n \n-    unsigned getElemsPerThreadOfOperand(int opIdx, ArrayRef<int64_t> shape) const;\n-\n     // Get [isARow, isBRow, isAVec4, isBVec4, id] from versionMinor\n     std::tuple<bool, bool, bool, bool, int> decodeVoltaLayoutStates() const;\n "}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 7, "deletions": 21, "changes": 28, "file_content_changes": "@@ -102,9 +102,9 @@ unsigned ReduceOpHelper::getIntraWarpSize() {\n unsigned ReduceOpHelper::getInterWarpSizeWithUniqueData() {\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n   unsigned sizeIntraWarps = getIntraWarpSizeWithUniqueData();\n-  return std::min(srcReduceDimSize / sizeIntraWarps,\n-                  triton::gpu::getWarpsPerCTAWithUniqueData(\n-                      getSrcLayout(), getSrcShape())[axis]);\n+  return std::min(\n+      srcReduceDimSize / sizeIntraWarps,\n+      triton::gpu::getWarpsPerCTAWithUniqueData(getSrcValue())[axis]);\n }\n \n unsigned ReduceOpHelper::getIntraWarpSizeWithUniqueData() {\n@@ -121,7 +121,7 @@ unsigned ReduceOpHelper::getThreadsReductionAxis() {\n   auto srcShape = getSrcShape();\n   return triton::gpu::getThreadsPerWarpWithUniqueData(srcLayout,\n                                                       srcShape)[axis] *\n-         triton::gpu::getWarpsPerCTAWithUniqueData(srcLayout, srcShape)[axis];\n+         triton::gpu::getWarpsPerCTAWithUniqueData(getSrcValue())[axis];\n }\n \n SmallVector<unsigned> ReduceOpHelper::getScratchConfigBasic() {\n@@ -232,8 +232,7 @@ unsigned ScanLoweringHelper::getAxisNumWarps() {\n \n unsigned ScanLoweringHelper::getAxisNumBlocks() {\n   auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n-  auto sizePerThreads =\n-      triton::gpu::getSizePerThread(srcEncoding, srcShapePerCTA);\n+  auto sizePerThreads = triton::gpu::getSizePerThread(scanOp.getOperand(0));\n   auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n   auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n   unsigned axis = getAxis();\n@@ -244,8 +243,7 @@ unsigned ScanLoweringHelper::getAxisNumBlocks() {\n \n unsigned ScanLoweringHelper::getNonAxisNumBlocks() {\n   auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n-  auto sizePerThreads =\n-      triton::gpu::getSizePerThread(srcEncoding, srcShapePerCTA);\n+  auto sizePerThreads = triton::gpu::getSizePerThread(scanOp.getOperand(0));\n   auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n   auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n   unsigned axis = getAxis();\n@@ -313,8 +311,7 @@ unsigned ScanLoweringHelper::getAxisBlockStride() {\n   auto order = triton::gpu::getOrder(srcEncoding);\n   unsigned stride = 1;\n   auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n-  auto sizePerThreads =\n-      triton::gpu::getSizePerThread(srcEncoding, srcShapePerCTA);\n+  auto sizePerThreads = triton::gpu::getSizePerThread(scanOp.getOperand(0));\n   auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n   auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n   for (unsigned dim : order) {\n@@ -739,17 +736,6 @@ mmaVersionToInstrShape(int version, ArrayRef<int64_t> shape, Type eltType) {\n   }\n }\n \n-SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n-                                                ArrayRef<int64_t> shape,\n-                                                StringRef inputType) {\n-  return {0};\n-}\n-\n-SmallVector<unsigned, 3>\n-mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n-                       ArrayRef<int64_t> shape) {\n-  return {0};\n-}\n Value getParentValueWithSameEncoding(Attribute layout, Value value) {\n   return {};\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -116,7 +116,7 @@ struct ConvertLayoutOpConversion\n           emitBaseIndexForLayout(loc, rewriter, value, false);\n       SmallVector<Value> multiDimOffset(rank);\n       SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n-          elemId, getSizePerThread(layout, {}), getOrder(layout));\n+          elemId, getSizePerThread(value), getOrder(layout));\n       for (unsigned d = 0; d < rank; ++d) {\n         multiDimOffset[d] =\n             add(multiDimOffsetFirstElem[d],\n@@ -152,7 +152,8 @@ struct ConvertLayoutOpConversion\n     }\n     if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n       auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n-      auto instrShape = mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n+      auto instrShape = mmaVersionToInstrShape(\n+          mmaLayout.getVersionMajor(), shapePerCTA, getInput0EltType(value));\n       SmallVector<Value> mmaColIdx(4);\n       SmallVector<Value> mmaRowIdx(2);\n       Value threadId = getThreadId(rewriter, loc);\n@@ -248,7 +249,7 @@ struct ConvertLayoutOpConversion\n     auto layout = type.getEncoding();\n     auto rank = type.getRank();\n     auto shapePerCTA = getShapePerCTA(layout, type.getShape());\n-    auto sizePerThread = getSizePerThread(layout, shapePerCTA);\n+    auto sizePerThread = getSizePerThread(value);\n     auto accumSizePerThread = product<unsigned>(sizePerThread);\n     SmallVector<unsigned> numCTATiles(rank);\n     auto shapePerCTATile = getShapePerCTATile(value);\n@@ -738,7 +739,8 @@ struct ConvertLayoutOpConversion\n                                                          rewriter, srcTy);\n \n       auto srcShapePerCTA = getShapePerCTA(mmaLayout, srcShape);\n-      auto instrShape = mmaVersionToInstrShape(mmaLayout, srcShapePerCTA);\n+      auto instrShape = mmaVersionToInstrShape(\n+          mmaLayout.getVersionMajor(), srcShapePerCTA, getInput0EltType(src));\n       auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n       uint32_t repM =\n           ceil<unsigned>(srcShapePerCTA[0], instrShape[0] * warpsPerCTA[0]);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -47,7 +47,7 @@ int getShapePerCTATileForMN(BlockedEncodingAttr layout, bool isM) {\n // Get sizePerThread for M or N axis.\n int getSizePerThreadForMN(BlockedEncodingAttr layout, bool isM) {\n   auto order = layout.getOrder();\n-  auto sizePerThread = getSizePerThread(layout, {});\n+  auto sizePerThread = getSizePerThread(layout);\n \n   int mSizePerThread =\n       order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n@@ -111,7 +111,7 @@ Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n   int M = aShapePerCTA[0];\n \n   auto shapePerCTATile = getShapePerCTATile(dLayout);\n-  auto sizePerThread = getSizePerThread(dLayout, {});\n+  auto sizePerThread = getSizePerThread(dLayout);\n \n   Value _0 = i32_val(0);\n \n@@ -175,7 +175,7 @@ Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n   int N = bShapePerCTA[1];\n \n   auto shapePerCTATile = getShapePerCTATile(dLayout);\n-  auto sizePerThread = getSizePerThread(dLayout, {});\n+  auto sizePerThread = getSizePerThread(dLayout);\n \n   Value _0 = i32_val(0);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/FMA.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -53,7 +53,7 @@ LogicalResult convertFMADot(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n   Value llA = adaptor.getA();\n   Value llB = adaptor.getB();\n \n-  auto sizePerThread = getSizePerThread(dLayout, {});\n+  auto sizePerThread = getSizePerThread(dLayout);\n   auto shapePerCTATile = mlir::triton::gpu::getShapePerCTATile(D);\n \n   int K = aShapePerCTA[1];"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/WGMMA.cpp", "status": "modified", "additions": 8, "deletions": 4, "changes": 12, "file_content_changes": "@@ -161,7 +161,8 @@ DotOpMmaV3SmemLoader loadA(TritonGPUToLLVMTypeConverter *typeConverter,\n   auto aSharedLayout = aTensorTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n   assert(aSharedLayout && \"only support load dot operand from shared.\");\n   auto shapePerCTA = getShapePerCTA(aTensorTy);\n-  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA);\n+  auto instrShape = mmaVersionToInstrShape(\n+      mmaEncoding.getVersionMajor(), shapePerCTA, aTensorTy.getElementType());\n   auto wpt = mmaEncoding.getWarpsPerCTA();\n   auto aOrd = aSharedLayout.getOrder();\n   bool transA = aOrd[0] == 0;\n@@ -192,7 +193,8 @@ DotOpMmaV3SmemLoader loadB(TritonGPUToLLVMTypeConverter *typeConverter,\n   auto bSharedLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n   assert(bSharedLayout && \"only support load B from shared.\");\n   auto shapePerCTA = triton::gpu::getShapePerCTA(bTensorTy);\n-  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA);\n+  auto instrShape = mmaVersionToInstrShape(\n+      mmaEncoding.getVersionMajor(), shapePerCTA, bTensorTy.getElementType());\n \n   auto wpt = mmaEncoding.getWarpsPerCTA();\n   auto bOrd = bSharedLayout.getOrder();\n@@ -279,7 +281,8 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n   bool transA = aOrd[0] == 0;\n   bool transB = bOrd[0] == 1;\n   auto dShapePerCTA = getShapePerCTA(dTensorTy);\n-  auto instrShape = mmaVersionToInstrShape(mmaEncoding, dShapePerCTA);\n+  auto instrShape = mmaVersionToInstrShape(\n+      mmaEncoding.getVersionMajor(), dShapePerCTA, aTensorTy.getElementType());\n \n   auto accSize = 2 * (instrShape[1] / 4);\n   int M = 4 * instrShape[0];\n@@ -362,7 +365,8 @@ Value loadC(Value dTensor, Value tensor, Value llTensor) {\n   auto mmaEncoding = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n   assert(mmaEncoding && \"Currently, we only support $c with a mma layout.\");\n   auto shapePerCTA = getShapePerCTA(tensorTy);\n-  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA);\n+  auto instrShape = mmaVersionToInstrShape(\n+      mmaEncoding.getVersionMajor(), shapePerCTA, getInput0EltType(dTensor));\n   auto wpt = mmaEncoding.getWarpsPerCTA();\n   // benzh@need from ACC\n   auto shapePerCTATile = getShapePerCTATile(dTensor);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "file_content_changes": "@@ -151,8 +151,7 @@ struct ReduceOpConversion\n     }\n \n     writeIdx = index;\n-    auto sizePerThread = triton::gpu::getSizePerThread(\n-        layout, triton::gpu::getShapePerCTA(value.getType()));\n+    auto sizePerThread = triton::gpu::getSizePerThread(value);\n     Value axisSizePerThread = ints[sizePerThread[axis]];\n     Value _8 = ints[8];\n     Value _16 = ints[16];\n@@ -199,8 +198,7 @@ struct ReduceOpConversion\n     // The order of the axes for the the threads within the warp\n     auto srcOrd = triton::gpu::getOrder(srcLayout);\n     auto srcShape = helper.getSrcShape();\n-    auto sizePerThread = triton::gpu::getSizePerThread(\n-        srcLayout, triton::gpu::getShapePerCTA(srcLayout, srcShape));\n+    auto sizePerThread = triton::gpu::getSizePerThread(helper.getSrcValue());\n \n     SmallVector<Type> elemPtrTys(srcTys.size());\n     for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n@@ -511,7 +509,7 @@ struct ReduceOpConversion\n     auto threadsPerWarp =\n         triton::gpu::getThreadsPerWarpWithUniqueData(srcLayout, srcShape);\n     auto warpsPerCTA =\n-        triton::gpu::getWarpsPerCTAWithUniqueData(srcLayout, srcShape);\n+        triton::gpu::getWarpsPerCTAWithUniqueData(helper.getSrcValue());\n     auto order = getOrder(srcLayout);\n     SmallVector<Value> multiDimLaneId =\n         delinearize(rewriter, loc, laneId, threadsPerWarp, order);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 6, "deletions": 11, "changes": 17, "file_content_changes": "@@ -546,7 +546,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       auto shape = tensorTy.getShape();\n       unsigned rank = shape.size();\n       auto shapePerCTA = triton::gpu::getShapePerCTA(tensorTy);\n-      auto sizePerThread = triton::gpu::getSizePerThread(layout, shapePerCTA);\n+      auto sizePerThread = triton::gpu::getSizePerThread(value);\n       auto threadsPerWarp = triton::gpu::getThreadsPerWarp(layout);\n       auto warpsPerCTA = triton::gpu::getWarpsPerCTA(layout);\n       auto order = triton::gpu::getOrder(layout);\n@@ -666,7 +666,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n           result = emitBaseIndexWithinCTAForMmaLayoutV1(loc, rewriter,\n                                                         mmaLayout, type);\n         if (mmaLayout.isAmpere() || mmaLayout.isHopper())\n-          result = emitBaseIndexWithinCTAForMmaLayoutV2V3(loc, rewriter,\n+          result = emitBaseIndexWithinCTAForMmaLayoutV2V3(loc, rewriter, value,\n                                                           mmaLayout, type);\n       } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n         auto parentLayout = sliceLayout.getParent();\n@@ -1009,15 +1009,15 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   }\n \n   SmallVector<Value> emitBaseIndexWithinCTAForMmaLayoutV2V3(\n-      Location loc, ConversionPatternRewriter &rewriter,\n+      Location loc, ConversionPatternRewriter &rewriter, Value value,\n       const MmaEncodingAttr &mmaLayout, RankedTensorType type) const {\n     auto shape = type.getShape();\n     auto _warpsPerCTA = mmaLayout.getWarpsPerCTA();\n     assert(_warpsPerCTA.size() == 2);\n     auto order = triton::gpu::getOrder(mmaLayout);\n     auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n-    ArrayRef<unsigned int> instrShape =\n-        mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n+    ArrayRef<unsigned int> instrShape = mmaVersionToInstrShape(\n+        mmaLayout.getVersionMajor(), shapePerCTA, getInput0EltType(value));\n     SmallVector<Value> warpsPerCTA = {i32_val(_warpsPerCTA[0]),\n                                       i32_val(_warpsPerCTA[1])};\n \n@@ -1071,13 +1071,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto shape = type.getShape();\n     auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n     SmallVector<SmallVector<unsigned>> ret;\n-    auto inputType = value.getDefiningOp()\n-                         ->getOperands()[0]\n-                         .getType()\n-                         .cast<RankedTensorType>()\n-                         .getElementType();\n     ArrayRef<unsigned int> instrShape = mmaVersionToInstrShape(\n-        mmaLayout.getVersionMajor(), shapePerCTA, inputType);\n+        mmaLayout.getVersionMajor(), shapePerCTA, getInput0EltType(value));\n \n     for (unsigned i = 0; i < shapePerCTA[0];\n          i += triton::gpu::getShapePerCTATile(value)[0]) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -655,7 +655,7 @@ class ConvertTritonGPUToLLVM\n             dstType.getShape(), dstType.getElementType(),\n             triton::gpu::BlockedEncodingAttr::get(\n                 mod.getContext(), srcType.getShape(),\n-                getSizePerThread(srcMma, triton::gpu::getShapePerCTA(srcType)),\n+                triton::gpu::getSizePerThread(cvtOp.getOperand()),\n                 getOrder(srcMma), numWarps, threadsPerWarp, numCTAs));\n         auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n             cvtOp.getLoc(), tmpType, cvtOp.getOperand());"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 50, "deletions": 80, "changes": 130, "file_content_changes": "@@ -159,38 +159,45 @@ SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n   return {};\n }\n \n-SmallVector<unsigned>\n-getWarpsPerCTAWithUniqueData(Attribute layout, ArrayRef<int64_t> tensorShape) {\n+SmallVector<unsigned> getWarpsPerCTAWithUniqueData(Value value) {\n+  auto type = value.getType().cast<RankedTensorType>();\n+  auto layout = type.getEncoding();\n+  auto tensorShape = type.getShape();\n   if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parentLayout = sliceLayout.getParent();\n-    auto parentShape = sliceLayout.paddedShape(tensorShape);\n-    auto parentWarpsPerCTA =\n-        getWarpsPerCTAWithUniqueData(parentLayout, parentShape);\n-    SmallVector<unsigned> warpsPerCTA = parentWarpsPerCTA;\n-    warpsPerCTA.erase(warpsPerCTA.begin() + sliceLayout.getDim());\n-    return warpsPerCTA;\n+    auto parentValue = getParentValueWithSameEncoding(parentLayout, value);\n+    auto parentWarpsPerCTA = getWarpsPerCTAWithUniqueData(parentValue);\n+    parentWarpsPerCTA.erase(parentWarpsPerCTA.begin() + sliceLayout.getDim());\n+    return parentWarpsPerCTA;\n   }\n   auto warpsPerCTA = getWarpsPerCTA(layout);\n   assert(warpsPerCTA.size() == tensorShape.size() &&\n          \"layout and tensor shape must have the same rank\");\n   for (unsigned i = 0; i < warpsPerCTA.size(); i++) {\n     auto sizePerWarp =\n-        getSizePerThread(layout, {})[i] * getThreadsPerWarp(layout)[i];\n+        getSizePerThread(value)[i] * getThreadsPerWarp(layout)[i];\n     auto maxWarpsPerDim = ceil<unsigned>(tensorShape[i], sizePerWarp);\n     warpsPerCTA[i] = std::min<unsigned>(warpsPerCTA[i], maxWarpsPerDim);\n   }\n \n   return warpsPerCTA;\n }\n \n-SmallVector<unsigned> getSizePerThread(Attribute layout,\n-                                       ArrayRef<int64_t> shapePerCTA) {\n+SmallVector<unsigned> getSizePerThread(BlockedEncodingAttr layout) {\n+  return SmallVector<unsigned>(layout.getSizePerThread().begin(),\n+                               layout.getSizePerThread().end());\n+}\n+\n+SmallVector<unsigned> getSizePerThread(Value value) {\n+  auto type = value.getType().cast<RankedTensorType>();\n+  auto layout = type.getEncoding();\n+\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-    return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n-                                 blockedLayout.getSizePerThread().end());\n+    return getSizePerThread(blockedLayout);\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-    // [benzh] should here provide parent shape???\n-    auto sizePerThread = getSizePerThread(sliceLayout.getParent(), shapePerCTA);\n+    auto parentLayout = sliceLayout.getParent();\n+    auto parentValue = getParentValueWithSameEncoding(parentLayout, value);\n+    auto sizePerThread = getSizePerThread(parentValue);\n     sizePerThread.erase(sizePerThread.begin() + sliceLayout.getDim());\n     return sizePerThread;\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n@@ -199,7 +206,9 @@ SmallVector<unsigned> getSizePerThread(Attribute layout,\n     } else if (mmaLayout.isVolta()) {\n       return {1, 2};\n     } else if (mmaLayout.isHopper()) {\n-      auto instrShape = mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n+      auto shapePerCTA = getShapePerCTA(type);\n+      auto instrShape = mmaVersionToInstrShape(\n+          mmaLayout.getVersionMajor(), shapePerCTA, getInput0EltType(value));\n       // TODO(thomas): what are those magic numbers?\n       return SmallVector<unsigned>{instrShape[0] * 4 / 32, instrShape[1] / 4};\n     } else {\n@@ -209,26 +218,28 @@ SmallVector<unsigned> getSizePerThread(Attribute layout,\n     auto parentLayout = dotLayout.getParent();\n     assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n     if (auto parentMmaLayout = parentLayout.dyn_cast<MmaEncodingAttr>()) {\n-      assert(parentMmaLayout.isAmpere() &&\n-             \"mmaLayout version = 1 is not implemented yet\");\n-      auto opIdx = dotLayout.getOpIdx();\n-      if (opIdx == 0) {\n+      if (parentMmaLayout.isAmpere()) {\n+        auto opIdx = dotLayout.getOpIdx();\n+        if (opIdx == 0) {\n+          return {2, 4};\n+        } else if (opIdx == 1) {\n+          return {4, 1};\n+        }\n+      } else if (parentMmaLayout.isHopper()) {\n+        auto opIdx = dotLayout.getOpIdx();\n+        assert(opIdx == 0 && \"hopper mma only support A from register\");\n         return {2, 4};\n-      } else if (opIdx == 1) {\n-        return {4, 1};\n       } else {\n-        assert(0 && \"DotOperandEncodingAttr opIdx must be 0 or 1\");\n-        return {};\n+        assert(0 && \"mmaLayout version = 1 is not implemented yet\");\n       }\n     } else {\n       assert(0 && \"DotOperandEncodingAttr non-MmaEncodingAttr parent not \"\n                   \"supported yet\");\n-      return {};\n     }\n   } else {\n     assert(0 && \"getSizePerThread not implemented\");\n-    return {};\n   }\n+  return {};\n }\n \n SmallVector<unsigned> getContigPerThread(Attribute layout) {\n@@ -237,9 +248,11 @@ SmallVector<unsigned> getContigPerThread(Attribute layout) {\n     return {1, 2};\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parentLayout = sliceLayout.getParent();\n+    // [benzh] should we consideer sliced dim???\n     return getContigPerThread(parentLayout);\n   } else {\n-    return getSizePerThread(layout, {});\n+    auto blocked = layout.cast<BlockedEncodingAttr>();\n+    return getSizePerThread(blocked);\n   }\n }\n \n@@ -301,7 +314,6 @@ static SmallVector<unsigned> getMMAShapePerCTATile_(MmaEncodingAttr mmaLayout,\n                                                     bool fromInput = false) {\n   SmallVector<unsigned> shape;\n   auto tensorShape = getShapePerCTA(value.getType());\n-  Type inputType;\n \n   if (mmaLayout.isAmpere())\n     return {16 * mmaLayout.getWarpsPerCTA()[0],\n@@ -315,8 +327,14 @@ static SmallVector<unsigned> getMMAShapePerCTATile_(MmaEncodingAttr mmaLayout,\n             static_cast<unsigned>(tensorShape[1])};\n   }\n   if (mmaLayout.isHopper()) {\n+    Type inputType;\n+    if (fromInput)\n+      inputType = value.getType().cast<RankedTensorType>().getElementType();\n+    else\n+      inputType = getInput0EltType(value);\n     auto instrShape = mmaVersionToInstrShape(mmaLayout.getVersionMajor(),\n                                              tensorShape, inputType);\n+\n     return {16 * mmaLayout.getWarpsPerCTA()[0],\n             instrShape[1] * mmaLayout.getWarpsPerCTA()[1]};\n   }\n@@ -687,17 +705,6 @@ static LogicalResult parseBool(AsmParser &parser, const NamedAttribute &attr,\n   return parseBoolAttrValue(parser, attr.getValue(), value, desc);\n };\n \n-static LogicalResult parseStrAttr(AsmParser &parser, const NamedAttribute &attr,\n-                                  StringRef &res, StringRef desc) {\n-  auto strAttr = attr.getValue().dyn_cast<StringAttr>();\n-  if (!strAttr) {\n-    parser.emitError(parser.getNameLoc(), \"expected an string for \") << desc;\n-    return failure();\n-  }\n-  res = strAttr.strref();\n-  return success();\n-};\n-\n //===----------------------------------------------------------------------===//\n // Attribute methods\n //===----------------------------------------------------------------------===//\n@@ -798,7 +805,7 @@ MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape, Type eltTy) const {\n   } else if (isHopper()) {\n     auto wpt = getWarpsPerCTA();\n     auto instrMNK =\n-        mmaVersionToInstrShape(getVersionMajor(), shape, getInputType());\n+        mmaVersionToInstrShape(getVersionMajor(), shapePerCTA, eltTy);\n     int repM = ceil<unsigned>(shapePerCTA[0], instrMNK[0] * wpt[0]);\n     int repN = ceil<unsigned>(shapePerCTA[1], instrMNK[1] * wpt[1]);\n     elemsPerThread[0] = 2 * repM;\n@@ -810,37 +817,6 @@ MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape, Type eltTy) const {\n   return elemsPerThread;\n }\n \n-unsigned\n-MmaEncodingAttr::getElemsPerThreadOfOperand(int opIdx,\n-                                            ArrayRef<int64_t> shape) const {\n-  size_t rank = shape.size();\n-  assert(rank == 2 && \"Unexpected rank of mma layout\");\n-  auto shapePerCTA = getShapePerCTA(*this, shape);\n-  int res = 0;\n-  if (isVolta()) {\n-    llvm_unreachable(\n-        \"getElemsPerThreadOfOperand() not supported for version 1\");\n-  } else if (isAmpere()) {\n-    llvm_unreachable(\n-        \"getElemsPerThreadOfOperand() not supported for version 2\");\n-  } else if (isHopper()) {\n-    auto wpt = getWarpsPerCTA();\n-    auto instrMNK =\n-        mmaVersionToInstrShape(getVersionMajor(), shapePerCTA, getInputType());\n-    if (opIdx == 0) {\n-      int repM = ceil<unsigned>(shapePerCTA[0], instrMNK[0] * wpt[0]);\n-      int repK = ceil<unsigned>(shapePerCTA[1], instrMNK[2]);\n-      return 8 * repM * repK;\n-    } else if (opIdx == 1) {\n-      int repK = ceil<unsigned>(shapePerCTA[0], instrMNK[2]);\n-      int repN = ceil<unsigned>(shapePerCTA[1], instrMNK[1] * wpt[1]);\n-      // benzh@ here need more check\n-      return 4 * std::max<int>(instrMNK[1] / 32, 1) * repK * repN;\n-    }\n-  }\n-  return res;\n-}\n-\n unsigned MmaEncodingAttr::getTotalElemsPerThread(ArrayRef<int64_t> shape,\n                                                  Type eltTy) const {\n   return product<unsigned>(getElemsPerThread(shape, eltTy));\n@@ -959,7 +935,7 @@ unsigned DotOperandEncodingAttr::getTotalElemsPerThread(ArrayRef<int64_t> shape,\n   if (auto blockedLayout = getParent().dyn_cast<BlockedEncodingAttr>()) {\n     auto shapePerCTATile = getShapePerCTATile(blockedLayout);\n     auto order = blockedLayout.getOrder();\n-    auto sizePerThread = getSizePerThread(blockedLayout, {});\n+    auto sizePerThread = getSizePerThread(blockedLayout);\n \n     int K = getOpIdx() == 0 ? shape[1] : shape[0];\n     int otherDim = getOpIdx() == 1 ? shape[1] : shape[0];\n@@ -1108,19 +1084,13 @@ Attribute MmaEncodingAttr::parse(AsmParser &parser, Type type) {\n       if (parseIntArrayAttr(parser, attr, CTAOrder, \"CTAOrder\").failed())\n         return {};\n     }\n-    if (attr.getName() == \"inputType\") {\n-      if (parseStrAttr(parser, attr, inputType, \"inputType\").failed()) {\n-        return {};\n-      }\n-    }\n   }\n \n   auto CTALayout = CTALayoutAttr::get(parser.getContext(), CTAsPerCGA,\n                                       CTASplitNum, CTAOrder);\n \n-  return parser.getChecked<MmaEncodingAttr>(parser.getContext(), versionMajor,\n-                                            versionMinor, warpsPerCTA,\n-                                            CTALayout, inputType);\n+  return parser.getChecked<MmaEncodingAttr>(\n+      parser.getContext(), versionMajor, versionMinor, warpsPerCTA, CTALayout);\n }\n \n void MmaEncodingAttr::print(AsmPrinter &printer) const {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -259,7 +259,7 @@ class BlockedToMMA : public mlir::RewritePattern {\n                                           numWarps, instrShape);\n       mmaEnc = ttg::MmaEncodingAttr::get(oldRetType.getContext(), versionMajor,\n                                          0 /*versionMinor*/, warpsPerTile,\n-                                         CTALayout, \"\");\n+                                         CTALayout);\n     }\n     auto newRetType = RankedTensorType::get(\n         oldRetType.getShape(), oldRetType.getElementType(), mmaEnc);"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/PlanCTA.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -301,8 +301,7 @@ bool CTAPlanner::processReduce(triton::FuncOp &funcOp) {\n \n     auto rank = srcShape.size();\n     auto order = ttg::getOrder(srcLayout);\n-    auto sizePerThread =\n-        ttg::getSizePerThread(srcLayout, ttg::getShapePerCTA(srcTy));\n+    auto sizePerThread = ttg::getSizePerThread(src);\n     auto CTAOrder = ttg::getCTAOrder(srcLayout);\n \n     llvm::SmallVector<unsigned> CTAsPerCGA(rank, 0);"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/EmitIndicesTest.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -121,7 +121,7 @@ class EmitIndicesTest : public ::testing::Test {\n                        llvm::ArrayRef<unsigned> instrShape,\n                        const std::string &refStr) {\n     auto layout = MmaEncodingAttr::get(&context, versionMajor, versionMinor,\n-                                       warpsPerCTA, getSingleCTALayout2d(), \"\");\n+                                       warpsPerCTA, getSingleCTALayout2d());\n     runDistributed2d(row, col, layout, /*multiCTA=*/false, refStr);\n   }\n \n@@ -131,7 +131,7 @@ class EmitIndicesTest : public ::testing::Test {\n                          llvm::ArrayRef<unsigned> instrShape, unsigned opIdx,\n                          const std::string &refStr) {\n     auto parent = MmaEncodingAttr::get(&context, versionMajor, versionMinor,\n-                                       warpsPerCTA, getSingleCTALayout2d(), \"\");\n+                                       warpsPerCTA, getSingleCTALayout2d());\n     auto layout = DotOperandEncodingAttr::get(&context, opIdx, parent, 0);\n     runDistributed2d(row, col, layout, /*multiCTA=*/false, refStr);\n   }\n@@ -636,7 +636,7 @@ TEST_F(EmitIndicesTest, LayoutVisualizer_Mma) {\n \n   Attribute mmaLayout = MmaEncodingAttr::get(\n       /*context=*/&context, /*versionMajor=*/2, /*versionMinor=*/1,\n-      /*warpsPerCTA=*/{1, 1}, /*CTALayout=*/CTALayout, \"\");\n+      /*warpsPerCTA=*/{1, 1}, /*CTALayout=*/CTALayout);\n \n   llvm::SmallVector<int64_t> shape = {/*row=*/16, /*col=*/8};\n "}, {"filename": "unittest/Dialect/TritonGPU/SwizzleTest.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -33,7 +33,7 @@ TEST_P(SwizzleDotOperandTestFixture, DotOperands) {\n \n   // create encoding\n   auto parent =\n-      triton::gpu::MmaEncodingAttr::get(&ctx, 2, 0, {1, 1}, CTALayout, \"\");\n+      triton::gpu::MmaEncodingAttr::get(&ctx, 2, 0, {1, 1}, CTALayout);\n   auto encoding = triton::gpu::DotOperandEncodingAttr::get(\n       &ctx, params.opIdx, parent, 32 / params.typeWidth);\n "}]