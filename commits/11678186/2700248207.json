[{"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 11, "deletions": 4, "changes": 15, "file_content_changes": "@@ -1,9 +1,11 @@\n+import functools\n import os\n import re\n import shutil\n \n import pytest\n import torch\n+from matplotlib.pyplot import get\n \n import triton\n import triton.language as tl\n@@ -39,31 +41,36 @@ def kernel_nospec(X, i, BLOCK: tl.constexpr):\n     tl.store(X, i)\n \n \n+@functools.cache\n+def get_args():\n+    return [torch.empty(1, dtype=torch.int32, device='cuda'), 1, 1024]\n+\n+\n def apply_src_change(target, old, new):\n     kernel.hash = None\n     function_1.hash = None\n     function_2.hash = None\n     function_1.src = function_1.src.replace(old, new)\n     target.src = target.src.replace(old, new)\n-    ret = target.cache_key\n+    ret = target.cache_key(get_args())\n     target.src = target.src.replace(new, old)\n     return ret\n \n \n def test_nochange():\n-    baseline = kernel.cache_key\n+    baseline = kernel.cache_key(get_args())\n     updated = apply_src_change(kernel, 'i + 1', 'i + 1')\n     assert baseline == updated\n \n \n def test_toplevel_change():\n-    baseline = kernel.cache_key\n+    baseline = kernel.cache_key(get_args())\n     updated = apply_src_change(kernel, 'i + 1', 'i + 2')\n     assert baseline != updated\n \n \n def test_nested1_change():\n-    baseline = kernel.cache_key\n+    baseline = kernel.cache_key(get_args())\n     updated = apply_src_change(function_1, 'i + 1', 'i + 2')\n     assert baseline != updated\n "}, {"filename": "python/triton/code_gen.py", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -964,10 +964,12 @@ def __call__(self, *wargs, grid, num_warps=4, num_stages=2, extern_libs={}, **kw\n             wargs.insert(pos + i, kwargs[pos])\n         if len(wargs) != len(self.fn.arg_names):\n             raise TypeError(f\"Function takes {len(self.fn.arg_names)} positional arguments but {len(wargs)} were given\")\n+        constexpr_str = ''\n         # handle annotations\n         for pos, _type in self.fn.annotations.items():\n             assert _type == triton.language.constexpr, \"only constexpr annotations are supported for now\"\n             wargs[pos] = _type(wargs[pos])\n+            constexpr_str = constexpr_str + f'-{wargs[pos].value}'\n         # check that tensors are on GPU.\n         # for arg in wargs:\n         #     if hasattr(arg, 'data_ptr'):\n@@ -979,10 +981,12 @@ def __call__(self, *wargs, grid, num_warps=4, num_stages=2, extern_libs={}, **kw\n         # so we're being conservative here\n         torch.cuda.set_device(device)\n         if device not in self.cache_key:\n+            self.cache_key[device] = {}\n+        if constexpr_str not in self.cache_key[device]:\n             cc = torch.cuda.get_device_capability(device)\n             cc = str(cc[0]) + '-' + str(cc[1])\n-            self.cache_key[device] = self.fn.cache_key(wargs) + cc\n-        cache_key = self.cache_key[device]\n+            self.cache_key[device][constexpr_str] = self.fn.cache_key(wargs) + cc\n+        cache_key = self.cache_key[device][constexpr_str]\n         stream = current_cuda_stream(device)\n         return _triton.runtime.launch(wargs, self.fn.do_not_specialize, cache_key, self.fn.arg_names,\n                                       device, stream, self.fn.bin_cache, num_warps, num_stages, extern_libs, self.add_to_cache,"}]