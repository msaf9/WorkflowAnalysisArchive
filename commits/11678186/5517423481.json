[{"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -94,11 +94,14 @@ def _fwd_kernel(\n     # load q: it will stay in SRAM throughout\n     q = tl.load(Q_block_ptr)\n     q = (q * qk_scale).to(K.dtype.element_ty)\n+    # advance block pointers to first iteration of the loop\n+    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n+    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n     # loop over k, v and update accumulator\n     for start_n in range(lo, hi, BLOCK_N):\n         start_n = tl.multiple_of(start_n, BLOCK_N)\n         # -- compute qk ----\n-        k = tl.load(tl.advance(K_block_ptr, (0, start_n)))\n+        k = tl.load(K_block_ptr)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         qk += tl.dot(q, k, allow_tf32=True)\n         if MODE == 1 or MODE == 3:\n@@ -120,12 +123,15 @@ def _fwd_kernel(\n         acc_scale = l_i / l_i_new\n         acc = acc * acc_scale[:, None]\n         # update acc\n-        v = tl.load(tl.advance(V_block_ptr, (start_n, 0)))\n+        v = tl.load(V_block_ptr)\n         p = p.to(V.dtype.element_ty)\n         acc += tl.dot(p, v, allow_tf32=True)\n         # update m_i and l_i\n         l_i = l_i_new\n         m_i = m_i_new\n+        # update pointers\n+        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n+        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n     # write back l and m\n     l_ptrs = L + off_hz * N_CTX + offs_m\n     m_ptrs = M + off_hz * N_CTX + offs_m"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -93,11 +93,14 @@ def _fwd_kernel(\n     # load q: it will stay in SRAM throughout\n     q = tl.load(Q_block_ptr)\n     q = (q * qk_scale).to(tl.float16)\n+    # advance block pointers to first iteration of the loop\n+    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n+    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n     # loop over k, v and update accumulator\n     for start_n in range(lo, hi, BLOCK_N):\n         start_n = tl.multiple_of(start_n, BLOCK_N)\n         # -- compute qk ----\n-        k = tl.load(tl.advance(K_block_ptr, (0, start_n)))\n+        k = tl.load(K_block_ptr)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         qk += tl.dot(q, k)\n         if MODE == 1 or MODE == 3:\n@@ -119,12 +122,15 @@ def _fwd_kernel(\n         acc_scale = l_i / l_i_new\n         acc = acc * acc_scale[:, None]\n         # update acc\n-        v = tl.load(tl.advance(V_block_ptr, (start_n, 0)))\n+        v = tl.load(V_block_ptr)\n         p = p.to(tl.float16)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n         l_i = l_i_new\n         m_i = m_i_new\n+        # update pointers\n+        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n+        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n     # write back l and m\n     l_ptrs = L + off_hz * N_CTX + offs_m\n     m_ptrs = M + off_hz * N_CTX + offs_m"}]