[{"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -39,10 +39,6 @@ class DialectInferLayoutInterface\n                             Attribute &resultEncoding,\n                             std::optional<Location> location) const = 0;\n \n-  virtual LogicalResult\n-  inferSqueezeDimsOpEncoding(Attribute operandEncoding, unsigned axis,\n-                             Attribute &resultEncoding,\n-                             std::optional<Location> location) const = 0;\n   // Note: this function only verify operand encoding but doesn't infer result\n   // encoding\n   virtual LogicalResult"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 0, "deletions": 14, "changes": 14, "file_content_changes": "@@ -294,20 +294,6 @@ def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [Pure,\n     let hasFolder = 1;\n }\n \n-def TT_SqueezeDimsOp : TT_Op<\"squeeze_dims\", [Pure,\n-                                            DeclareOpInterfaceMethods<InferTypeOpInterface>,\n-                                            SameOperandsAndResultElementType]> {\n-    let summary = \"squeeze_dims\";\n-\n-    let arguments = (ins TT_Tensor:$src, I32Attr:$axis);\n-\n-    let results = (outs TT_Tensor:$result);\n-\n-    let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n-    let hasCanonicalizeMethod = 1;\n-}\n-\n-\n // view is not `pure` because it may reorder elements\n def TT_ViewOp : TT_Op<\"view\", [NoMemoryEffect,\n                                SameOperandsAndResultElementType]> {"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 2, "deletions": 23, "changes": 25, "file_content_changes": "@@ -245,27 +245,6 @@ struct TritonExpandDimsPattern\n   }\n };\n \n-struct TritonSqueezeDimsPattern\n-    : public OpConversionPattern<triton::SqueezeDimsOp> {\n-\n-  using OpConversionPattern<triton::SqueezeDimsOp>::OpConversionPattern;\n-\n-  LogicalResult\n-  matchAndRewrite(triton::SqueezeDimsOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-\n-    RankedTensorType argType =\n-        adaptor.getSrc().getType().cast<RankedTensorType>();\n-    Attribute _argEncoding = argType.getEncoding();\n-    if (!_argEncoding)\n-      return failure();\n-    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::SqueezeDimsOp>(\n-                      op, adaptor.getSrc(), adaptor.getAxis()),\n-                  adaptor.getAttributes());\n-    return success();\n-  }\n-};\n-\n struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n   using OpConversionPattern<triton::DotOp>::OpConversionPattern;\n \n@@ -678,8 +657,8 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n           TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n           TritonReducePattern, TritonReduceReturnPattern, TritonScanPattern,\n           TritonScanReturnPattern, TritonTransPattern, TritonExpandDimsPattern,\n-          TritonSqueezeDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n-          TritonLoadPattern, TritonStorePattern,\n+          TritonMakeRangePattern, TritonDotPattern, TritonLoadPattern,\n+          TritonStorePattern,\n           TritonExternElementwisePattern<triton::PureExternElementwiseOp>,\n           TritonExternElementwisePattern<triton::ImpureExternElementwiseOp>,\n           TritonPrintPattern, TritonAssertPattern, TritonAtomicRMWPattern,"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 0, "deletions": 45, "changes": 45, "file_content_changes": "@@ -664,51 +664,6 @@ OpFoldResult ExpandDimsOp::fold(FoldAdaptor adaptor) {\n   return foldViewLikeOp(*this, adaptor.getSrc());\n }\n \n-//-- SqueezeDimsOp --\n-mlir::LogicalResult mlir::triton::SqueezeDimsOp::inferReturnTypes(\n-    MLIRContext *context, std::optional<Location> loc, ValueRange operands,\n-    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,\n-    SmallVectorImpl<Type> &inferredReturnTypes) {\n-  // infer shape\n-  auto arg = operands[0];\n-  auto argTy = arg.getType().cast<RankedTensorType>();\n-  auto retShape = argTy.getShape().vec();\n-  Properties *prop = properties.as<Properties *>();\n-  int axis = prop->axis.getInt();\n-  retShape.erase(retShape.begin() + axis);\n-  // infer encoding\n-  Attribute argEncoding = argTy.getEncoding();\n-  Attribute retEncoding;\n-  if (argEncoding) {\n-    Dialect &dialect = argEncoding.getDialect();\n-    auto inferLayoutInterface = dyn_cast<DialectInferLayoutInterface>(&dialect);\n-    if (inferLayoutInterface\n-            ->inferSqueezeDimsOpEncoding(argEncoding, axis, retEncoding, loc)\n-            .failed())\n-      return emitOptionalError(loc, \"failed to infer layout for SqueezeDimsOp\");\n-  }\n-  // create type\n-  auto argEltTy = argTy.getElementType();\n-  inferredReturnTypes.push_back(\n-      RankedTensorType::get(retShape, argEltTy, retEncoding));\n-  return mlir::success();\n-}\n-\n-LogicalResult SqueezeDimsOp::canonicalize(SqueezeDimsOp op,\n-                                          PatternRewriter &rewriter) {\n-  auto definingOp = op.getOperand().getDefiningOp();\n-  if (!definingOp) {\n-    return mlir::failure();\n-  }\n-  // squeeze_dims(expand_dims(x, dim), dim) -> x\n-  if (auto expand = dyn_cast<triton::ExpandDimsOp>(definingOp))\n-    if (expand.getAxis() == op.getAxis()) {\n-      rewriter.replaceOp(op, expand.getOperand());\n-      return mlir::success();\n-    }\n-  return mlir::failure();\n-}\n-\n //-- ViewOp --\n template <typename OpType>\n LogicalResult canonicalizeViewOrBroadcast(OpType op,"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 26, "deletions": 36, "changes": 62, "file_content_changes": "@@ -110,7 +110,7 @@ class CombineBroadcastMulReducePattern : public mlir::RewritePattern {\n private:\n   static bool isAddF32(const Operation *op) {\n     if (auto addf = dyn_cast_or_null<arith::AddFOp>(op))\n-      return addf.getType().isa<mlir::Float32Type>();\n+      return addf.getType().getIntOrFloatBitWidth() <= 32;\n     return false;\n   }\n \n@@ -154,49 +154,39 @@ class CombineBroadcastMulReducePattern : public mlir::RewritePattern {\n         mulOp.getOperand(1).getDefiningOp());\n     if (!broadcastRhsOp)\n       return mlir::failure();\n+    // broadcast operand is expand dims\n+    auto expandLhsOp = llvm::dyn_cast_or_null<triton::ExpandDimsOp>(\n+        broadcastLhsOp.getOperand().getDefiningOp());\n+    if (!expandLhsOp)\n+      return mlir::failure();\n+    auto expandRhsOp = llvm::dyn_cast_or_null<triton::ExpandDimsOp>(\n+        broadcastRhsOp.getOperand().getDefiningOp());\n+    if (!expandRhsOp)\n+      return mlir::failure();\n     // get not-broadcast dimensions\n-    auto lhsInShape =\n-        broadcastLhsOp.getOperand().getType().cast<ShapedType>().getShape();\n-    auto lhsOutShape = broadcastLhsOp.getType().cast<ShapedType>().getShape();\n-    auto rhsInShape =\n-        broadcastRhsOp.getOperand().getType().cast<ShapedType>().getShape();\n-    auto rhsOutShape = broadcastRhsOp.getType().cast<ShapedType>().getShape();\n-    SmallVector<int> keptDimLHS = getEqualIndices(lhsInShape, lhsOutShape);\n-    SmallVector<int> keptDimRHS = getEqualIndices(rhsInShape, rhsOutShape);\n-    if (keptDimLHS.size() != 2 || keptDimRHS.size() != 2)\n+    int expandLhsAxis = expandLhsOp.getAxis();\n+    int expandRhsAxis = expandRhsOp.getAxis();\n+    if (expandLhsAxis != 2 || expandRhsAxis != 0)\n       return mlir::failure();\n-    if (keptDimLHS[1] != keptDimRHS[0] || reduceOp.getAxis() != keptDimLHS[1])\n+    auto broadcastLhsShape =\n+        broadcastLhsOp.getType().cast<ShapedType>().getShape();\n+    auto broadcastRhsShape =\n+        broadcastLhsOp.getType().cast<ShapedType>().getShape();\n+    if (broadcastLhsShape[2] < 16 || broadcastRhsShape[0] < 16)\n       return mlir::failure();\n-    // replace with dot\n-    Type newLhsType = RankedTensorType::get(\n-        {lhsInShape[keptDimLHS[0]], lhsOutShape[keptDimLHS[1]]},\n-        broadcastLhsOp.getOperand()\n-            .getType()\n-            .cast<ShapedType>()\n-            .getElementType());\n-    Type newRhsType = RankedTensorType::get(\n-        {rhsOutShape[keptDimRHS[0]], rhsInShape[keptDimRHS[1]]},\n-        broadcastRhsOp.getOperand()\n-            .getType()\n-            .cast<ShapedType>()\n-            .getElementType());\n-    Type newAccType = RankedTensorType::get(\n-        {lhsInShape[keptDimLHS[0]], rhsInShape[keptDimRHS[1]]},\n-        broadcastLhsOp.getOperand()\n-            .getType()\n-            .cast<ShapedType>()\n-            .getElementType());\n+    Type newAccType =\n+        RankedTensorType::get({broadcastLhsShape[0], broadcastRhsShape[2]},\n+                              broadcastLhsOp.getOperand()\n+                                  .getType()\n+                                  .cast<ShapedType>()\n+                                  .getElementType());\n     rewriter.setInsertionPoint(op);\n-    auto newLhs = rewriter.create<triton::SqueezeDimsOp>(\n-        op->getLoc(), broadcastLhsOp.getOperand(), 2);\n-    auto newRhs = rewriter.create<triton::SqueezeDimsOp>(\n-        op->getLoc(), broadcastRhsOp.getOperand(), 0);\n     auto newAcc = rewriter.create<triton::SplatOp>(\n         op->getLoc(), newAccType,\n         rewriter.create<arith::ConstantOp>(op->getLoc(),\n                                            rewriter.getF32FloatAttr(0)));\n-    rewriter.replaceOpWithNewOp<triton::DotOp>(op, newLhs, newRhs, newAcc,\n-                                               true);\n+    rewriter.replaceOpWithNewOp<triton::DotOp>(\n+        op, expandLhsOp.getOperand(), expandRhsOp.getOperand(), newAcc, true);\n     return mlir::success();\n   }\n };"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 0, "deletions": 9, "changes": 9, "file_content_changes": "@@ -1086,15 +1086,6 @@ struct TritonGPUInferLayoutInterface\n     return success();\n   }\n \n-  LogicalResult\n-  inferSqueezeDimsOpEncoding(Attribute operandEncoding, unsigned axis,\n-                             Attribute &resultEncoding,\n-                             std::optional<Location> location) const override {\n-    resultEncoding = SliceEncodingAttr::get(getDialect()->getContext(), axis,\n-                                            operandEncoding);\n-    return success();\n-  }\n-\n   LogicalResult\n   inferDotOpEncoding(Attribute operandEncoding, unsigned opIdx,\n                      Attribute retEncoding,"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 0, "deletions": 12, "changes": 12, "file_content_changes": "@@ -1226,18 +1226,6 @@ void init_triton_ir(py::module &&m) {\n                 loc, mlir::RankedTensorType::get(retShape, argEltType), arg,\n                 axis);\n           })\n-      .def(\n-          \"create_squeeze_dims\",\n-          [](mlir::OpBuilder &self, mlir::Value &arg, int axis) -> mlir::Value {\n-            auto loc = self.getUnknownLoc();\n-            auto argType = arg.getType().dyn_cast<mlir::RankedTensorType>();\n-            auto argEltType = argType.getElementType();\n-            std::vector<int64_t> retShape = argType.getShape();\n-            retShape.erase(retShape.begin() + axis);\n-            return self.create<mlir::triton::SqueezeDimsOp>(\n-                loc, mlir::RankedTensorType::get(retShape, argEltType), arg,\n-                axis);\n-          })\n       .def(\"create_cat\",\n            [](mlir::OpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -13,6 +13,7 @@\n     zeros_like,\n )\n from .core import (\n+    TRITON_MAX_TENSOR_NUMEL,\n     abs,\n     advance,\n     arange,\n@@ -72,7 +73,6 @@\n     reshape,\n     sin,\n     sqrt,\n-    squeeze_dims,\n     static_assert,\n     static_print,\n     store,\n@@ -106,6 +106,7 @@\n \n \n __all__ = [\n+    \"TRITON_MAX_TENSOR_NUMEL\",\n     \"abs\",\n     \"advance\",\n     \"arange\",\n@@ -182,7 +183,6 @@\n     \"sin\",\n     \"softmax\",\n     \"sqrt\",\n-    \"squeeze_dims\",\n     \"static_range\",\n     \"static_assert\",\n     \"static_print\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 2, "deletions": 23, "changes": 25, "file_content_changes": "@@ -919,34 +919,13 @@ def expand_dims(input, axis, _builder=None):\n     axes = list(axis) if isinstance(axis, Sequence) else [axis]\n     new_ndim = len(input.shape) + len(axes)\n     axes = [_wrap_axis(_constexpr_to_value(d), new_ndim) for d in axes]\n+\n     if len(set(axes)) != len(axes):\n         raise ValueError(f\"expand_dims recieved duplicate axes, normalized axes = {axes}\")\n-    ret = input\n-    for a in sorted(axes):\n-        ret = semantic.expand_dims(ret, a, _builder)\n-    return ret\n-\n-\n-@builtin\n-def squeeze_dims(input, axis, _builder=None):\n-    \"\"\"\n-    Squeeze the shape of a tensor, by removing length 1 dimension\n-\n-    :param input: The input tensor.\n-    :type input: tl.tensor\n-    :param axis: The indices to add new axes in the input tensor\n-    :type axis: int | Sequence[int]\n \n-    \"\"\"\n-    axis = _constexpr_to_value(axis)\n-    axes = list(axis) if isinstance(axis, Sequence) else [axis]\n-    new_ndim = len(input.shape) + len(axes)\n-    axes = [_wrap_axis(_constexpr_to_value(d), new_ndim) for d in axes]\n-    if len(set(axes)) != len(axes):\n-        raise ValueError(f\"squeeze_dims recieved duplicate axes, normalized axes = {axes}\")\n     ret = input\n     for a in sorted(axes):\n-        ret = semantic.squeeze_dims(ret, a, _builder)\n+        ret = semantic.expand_dims(ret, a, _builder)\n     return ret\n \n # -----------------------"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 0, "deletions": 7, "changes": 7, "file_content_changes": "@@ -547,13 +547,6 @@ def expand_dims(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_expand_dims(input.handle, axis), ret_ty)\n \n \n-def squeeze_dims(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n-    dst_shape = list(input.type.shape)\n-    del dst_shape[axis]\n-    ret_ty = tl.block_type(input.type.scalar, dst_shape)\n-    return tl.tensor(builder.create_squeeze_dims(input.handle, axis), ret_ty)\n-\n-\n def cat(lhs: tl.tensor, rhs: tl.tensor, can_reorder: bool, builder: ir.builder) -> tl.tensor:\n     assert can_reorder, \"current implementation of `cat` always may reorder elements\"\n     assert len(lhs.shape) == 1"}]