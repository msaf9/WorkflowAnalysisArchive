[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 10, "deletions": 8, "changes": 18, "file_content_changes": "@@ -12,19 +12,19 @@ namespace mlir {\n \n class ReduceOpHelper {\n public:\n-  explicit ReduceOpHelper(triton::ReduceOp rop)\n-      : op(rop.getOperation()), axis(rop.getAxis()) {\n-    auto firstTy = rop.getOperands()[0].getType().cast<RankedTensorType>();\n+  explicit ReduceOpHelper(triton::ReduceOp op)\n+      : op(op.getOperation()), axis(op.getAxis()) {\n+    auto firstTy = op.getOperands()[0].getType().cast<RankedTensorType>();\n     srcShape = firstTy.getShape();\n     srcEncoding = firstTy.getEncoding();\n-    srcElementTypes = rop.getElementTypes();\n+    srcElementTypes = op.getElementTypes();\n \n-    for (const auto &t : rop.getInputTypes()) {\n+    for (const auto &t : op.getInputTypes()) {\n       if (t.getShape() != srcShape) {\n-        rop.emitError() << \"shape mismatch\";\n+        op.emitError() << \"shape mismatch\";\n       }\n       if (t.getEncoding() != srcEncoding) {\n-        rop.emitError() << \"encoding mismatch\";\n+        op.emitError() << \"encoding mismatch\";\n       }\n     }\n   }\n@@ -33,6 +33,8 @@ class ReduceOpHelper {\n \n   Attribute getSrcLayout() { return srcEncoding; }\n \n+  triton::ReduceOp getOperation() { return op; }\n+\n   bool isFastReduction();\n \n   unsigned getInterWarpSize();\n@@ -54,7 +56,7 @@ class ReduceOpHelper {\n   bool isSupportedLayout();\n \n private:\n-  Operation *op;\n+  triton::ReduceOp op;\n   ArrayRef<int64_t> srcShape;\n   Attribute srcEncoding;\n   SmallVector<Type> srcElementTypes;"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -28,8 +28,13 @@ unsigned getTotalElemsPerThread(Attribute layout, ArrayRef<int64_t> shape,\n \n SmallVector<unsigned> getElemsPerThread(Type type);\n \n+// Returns the number of threads per warp that may have access to replicated\n+// elements. If you want non-replicated threads, use\n+// getThreadsPerWarpWithUniqueData.\n SmallVector<unsigned> getThreadsPerWarp(Attribute layout);\n \n+// Returns the number of warps per CTA that may have access to replicated\n+// elements. If you want non-replicated warps, use getWarpsPerCTAWithUniqueData.\n SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n \n SmallVector<unsigned> getSizePerThread(Attribute layout);"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -5,11 +5,15 @@\n #include \"mlir/IR/Matchers.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Tools/Sys/GetEnv.hpp\"\n #include <deque>\n \n namespace mlir {\n \n bool ReduceOpHelper::isFastReduction() {\n+  // Disable fast reduction only for debugging purpose\n+  if (::triton::tools::getBoolEnv(\"DISABLE_FAST_REDUCTION\"))\n+    return false;\n   return axis == triton::gpu::getOrder(getSrcLayout())[0];\n }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 8, "deletions": 3, "changes": 11, "file_content_changes": "@@ -367,8 +367,10 @@ struct ReduceOpConversion\n     Value warpId = udiv(threadId, warpSize);\n     Value laneId = urem(threadId, warpSize);\n \n-    auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcLayout);\n-    auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcLayout);\n+    auto threadsPerWarp =\n+        triton::gpu::getThreadsPerWarpWithUniqueData(srcLayout, srcShape);\n+    auto warpsPerCTA =\n+        triton::gpu::getWarpsPerCTAWithUniqueData(srcLayout, srcShape);\n     auto order = getOrder(srcLayout);\n     SmallVector<Value> multiDimLaneId =\n         delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n@@ -412,8 +414,11 @@ struct ReduceOpConversion\n     //\n     // Each thread needs to process:\n     //   elemsPerThread = sizeInterWarps * s1 * s2 .. Sn / numThreads\n+\n+    auto mod = op.getOperation()->getParentOfType<ModuleOp>();\n     unsigned numThreads =\n-        product<unsigned>(triton::gpu::getWarpsPerCTA(srcLayout)) * 32;\n+        product<unsigned>(triton::gpu::getWarpsPerCTA(srcLayout)) *\n+        triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n     unsigned elemsPerThread = std::max<unsigned>(elems / numThreads, 1);\n     Value readOffset = threadId;\n     for (unsigned round = 0; round < elemsPerThread; ++round) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -529,7 +529,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     assert(rank > 0);\n     SmallVector<Value> multiDim(rank);\n     if (rank == 1) {\n-      multiDim[0] = linear;\n+      multiDim[0] = urem(linear, i32_val(shape[0]));\n     } else {\n       Value remained = linear;\n       for (auto &&en : llvm::enumerate(shape.drop_back())) {"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -85,6 +85,8 @@ SmallVector<unsigned> getThreadsPerWarp(Attribute layout) {\n   if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parent = sliceLayout.getParent();\n     auto parentThreadsPerWarp = getThreadsPerWarp(parent);\n+    assert(parentThreadsPerWarp.size() == 2 &&\n+           \"getThreadsPerWarp only implemented for 2D slice layout\");\n     SmallVector<unsigned> threadsPerWarp = parentThreadsPerWarp;\n     threadsPerWarp.erase(threadsPerWarp.begin() + sliceLayout.getDim());\n     for (unsigned i = 0; i < threadsPerWarp.size(); i++)\n@@ -129,6 +131,8 @@ SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n   if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parent = sliceLayout.getParent();\n     auto parentWarpsPerCTA = getWarpsPerCTA(parent);\n+    assert(parentWarpsPerCTA.size() == 2 &&\n+           \"getWarpsPerCTA only implemented for 2D slice layout\");\n     SmallVector<unsigned> warpsPerCTA = parentWarpsPerCTA;\n     warpsPerCTA.erase(warpsPerCTA.begin() + sliceLayout.getDim());\n     for (unsigned i = 0; i < warpsPerCTA.size(); i++)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 18, "deletions": 6, "changes": 24, "file_content_changes": "@@ -1701,7 +1701,18 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n \n @pytest.mark.parametrize(\"M, N\", [[128, 128], [256, 128], [256, 256], [128, 256]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n-def test_chain_reduce(M, N, src_layout, device='cuda'):\n+@pytest.mark.parametrize(\"op\", [\"sum\", \"max\"])\n+def test_chain_reduce(M, N, src_layout, op, device='cuda'):\n+    op_str = \"\"\n+    if op == \"sum\":\n+        op_str = f\"\"\"\n+        %13 = arith.addi %arg2, %arg3 : i32\n+        tt.reduce.return %13 : i32\"\"\"\n+    elif op == \"max\":\n+        op_str = f\"\"\"\n+        %13 = \"triton_gpu.cmpi\"(%arg2, %arg3) <{{predicate = 4 : i64}}> : (i32, i32) -> i1\n+        %14 = arith.select %13, %arg2, %arg3 : i32\n+        tt.reduce.return %14 : i32\"\"\"\n     ir = f\"\"\"\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n@@ -1720,13 +1731,11 @@ def test_chain_reduce(M, N, src_layout, device='cuda'):\n         %10 = tt.load %9 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #src>\n         %11 = \"tt.reduce\"(%10) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n-        %13 = arith.addi %arg2, %arg3 : i32\n-        tt.reduce.return %13 : i32\n+        {op_str}\n         }}) {{axis = 1 : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n         %12 = \"tt.reduce\"(%11) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n-        %13 = arith.addi %arg2, %arg3 : i32\n-        tt.reduce.return %13 : i32\n+        {op_str}\n         }}) {{axis = 0 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> i32\n         tt.store %arg1, %12 {{cache = 1 : i32, evict = 1 : i32}} : i32\n         tt.return\n@@ -1748,7 +1757,10 @@ def test_chain_reduce(M, N, src_layout, device='cuda'):\n     z_tri = torch.tensor(z, device=device)\n \n     pgm = kernel[(1, 1, 1)](x_tri, z_tri)\n-    z_ref = np.sum(x)\n+    if op == \"sum\":\n+        z_ref = np.sum(x)\n+    elif op == \"max\":\n+        z_ref = np.max(x)\n \n     np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n "}]