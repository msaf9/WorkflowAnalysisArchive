[{"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 13, "deletions": 14, "changes": 27, "file_content_changes": "@@ -11,10 +11,9 @@\n from pathlib import Path\n from typing import Any, Tuple\n \n-import triton\n-import triton._C.libtriton.triton as _triton\n-from .._C.libtriton.triton import (add_external_libs, compile_ptx_to_cubin,\n-                                   get_num_warps, get_shared_memory_size, ir,\n+from .._C.libtriton.triton import (ClusterInfo, TMAInfos, add_external_libs,\n+                                   compile_ptx_to_cubin, get_env_vars, get_num_warps,\n+                                   get_shared_memory_size, ir,\n                                    translate_llvmir_to_hsaco, translate_llvmir_to_ptx,\n                                    translate_triton_gpu_to_llvmir)\n from ..common.backend import get_backend, path_to_ptxas\n@@ -101,8 +100,8 @@ def optimize_ttgir(mod, num_stages, num_warps, num_ctas, arch,\n     if arch // 10 >= 9 and enable_warp_specialization and num_warps == 4:\n         pm.add_tritongpu_ws_feasibility_checking_pass(arch)\n         pm.run(mod)\n-        ws_enabled = _triton.ir.is_ws_supported(mod)\n-        pm = _triton.ir.pass_manager(mod.context)\n+        ws_enabled = ir.is_ws_supported(mod)\n+        pm = ir.pass_manager(mod.context)\n         pm.enable_debug()\n     if ws_enabled:\n         pm.add_tritongpu_wsdecomposing_pass(arch)\n@@ -426,12 +425,12 @@ def compile(fn, **kwargs):\n     if os.environ.get('OPTIMIZE_EPILOGUE', '') == '1':\n         optimize_epilogue = True\n     #\n-    cluster_info = _triton.ClusterInfo()\n+    cluster_info = ClusterInfo()\n     if \"clusterDims\" in kwargs:\n         cluster_info.clusterDimX = kwargs[\"clusterDims\"][0]\n         cluster_info.clusterDimY = kwargs[\"clusterDims\"][1]\n         cluster_info.clusterDimZ = kwargs[\"clusterDims\"][2]\n-    tma_infos = _triton.TMAInfos()\n+    tma_infos = TMAInfos()\n     # build compilation stages\n     stages = dict()\n     stages[\"ast\"] = (lambda path: fn, None)\n@@ -479,7 +478,7 @@ def compile(fn, **kwargs):\n         first_stage = list(stages.keys()).index(ir_name)\n \n     # create cache manager\n-    fn_cache_manager = get_cache_manager(make_hash(fn, arch, _triton.get_env_vars(), **kwargs))\n+    fn_cache_manager = get_cache_manager(make_hash(fn, arch, get_env_vars(), **kwargs))\n     # determine name and extension type of provided function\n     if isinstance(fn, JITFunction):\n         name, ext = fn.__name__, \"ast\"\n@@ -512,7 +511,7 @@ def compile(fn, **kwargs):\n                     \"constants\": _get_jsonable_constants(constants),\n                     \"debug\": debug,\n                     \"arch\": arch, }\n-        metadata.update(_triton.get_env_vars())\n+        metadata.update(get_env_vars())\n         if ext == \"ptx\":\n             assert \"shared\" in kwargs, \"ptx compilation must provide shared memory size\"\n             metadata[\"shared\"] = kwargs[\"shared\"]\n@@ -559,7 +558,7 @@ def compile(fn, **kwargs):\n         if ir_name == \"llir\" and \"shared\" not in metadata:\n             metadata[\"shared\"] = get_shared_memory_size(module)\n         if ir_name == \"ttgir\":\n-            metadata[\"enable_warp_specialization\"] = _triton.ir.is_ws_supported(next_module)\n+            metadata[\"enable_warp_specialization\"] = ir.is_ws_supported(next_module)\n             if metadata[\"enable_warp_specialization\"]:\n                 metadata[\"num_warps\"] = get_num_warps(next_module)\n         if ir_name == \"ptx\":\n@@ -571,7 +570,7 @@ def compile(fn, **kwargs):\n             _device_backend.add_meta_info(ir_name, module, next_module, metadata, asm)\n         module = next_module\n \n-    ids_of_folded_args = tuple([int(k) for k in configs[0].ids_of_folded_args]) if isinstance(fn, triton.runtime.JITFunction) else ()\n+    ids_of_folded_args = tuple([int(k) for k in configs[0].ids_of_folded_args]) if isinstance(fn, JITFunction) else ()\n     if \"clusterDims\" not in metadata:\n         metadata[\"clusterDims\"] = [\n             cluster_info.clusterDimX,\n@@ -586,10 +585,10 @@ def compile(fn, **kwargs):\n             metadata[\"tensormaps_info\"][i].ids_of_folded_args = ids_of_folded_args\n \n     ids_of_tensormaps = get_ids_of_tensormaps(metadata.get(\"tensormaps_info\", None))\n-    if isinstance(fn, triton.runtime.JITFunction) and \"tensormaps_info\" in metadata:\n+    if isinstance(fn, JITFunction) and \"tensormaps_info\" in metadata:\n         fn.tensormaps_info = metadata[\"tensormaps_info\"]\n \n-    ids_of_const_exprs = tuple(fn.constexprs) if isinstance(fn, triton.runtime.JITFunction) else ()\n+    ids_of_const_exprs = tuple(fn.constexprs) if isinstance(fn, JITFunction) else ()\n     ids = {\"ids_of_tensormaps\": ids_of_tensormaps, \"ids_of_folded_args\": ids_of_folded_args, \"ids_of_const_exprs\": ids_of_const_exprs}\n     # cache manager\n     if is_cuda or is_hip:"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -11,7 +11,7 @@\n from typing import (Callable, Generic, Iterable, List, Optional, TypeVar, Union, cast,\n                     overload)\n \n-import triton\n+from .._C.libtriton.triton import TMAInfos\n from ..common.backend import get_backend, path_to_ptxas\n \n TRITON_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n@@ -479,7 +479,7 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         # index of constexprs\n         self.constexprs = [self.arg_names.index(name) for name, ty in self.__annotations__.items() if 'constexpr' in ty]\n         # tma info\n-        self.tensormaps_info = triton._C.libtriton.triton.TMAInfos()\n+        self.tensormaps_info = TMAInfos()\n         # launcher\n         self.run = self._make_launcher()\n         # re-use docs of wrapped function"}]