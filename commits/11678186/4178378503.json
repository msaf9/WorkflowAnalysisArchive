[{"filename": "lib/Dialect/TritonGPU/Transforms/UpdateMmaForVolta.cpp", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "file_content_changes": "@@ -133,13 +133,17 @@ Type updateStaleType(\n   // mma encoding\n   if (auto mma = encoding.dyn_cast<MmaEncodingAttr>()) {\n     auto newMma = layoutMap.lookup(mma);\n+    if (!newMma)\n+      return Type();\n     return RankedTensorType::get(type.getShape(), type.getElementType(),\n                                  newMma);\n   }\n   // slice encoding\n   else if (auto slice = encoding.dyn_cast<SliceEncodingAttr>()) {\n     if (auto mma = slice.getParent().dyn_cast<MmaEncodingAttr>()) {\n       auto newMma = layoutMap.lookup(mma);\n+      if (!newMma)\n+        return Type();\n       auto newSlice =\n           SliceEncodingAttr::get(slice.getContext(), slice.getDim(), newMma);\n       return RankedTensorType::get(type.getShape(), type.getElementType(),\n@@ -150,6 +154,8 @@ Type updateStaleType(\n   else if (auto dotOp = encoding.dyn_cast<DotOperandEncodingAttr>()) {\n     if (auto mma = dotOp.getParent().dyn_cast<MmaEncodingAttr>()) {\n       auto newMma = layoutMap.lookup(mma);\n+      if (!newMma)\n+        return Type();\n       auto newDotOp = DotOperandEncodingAttr::get(\n           dotOp.getContext(), dotOp.getOpIdx(), newMma, dotOp.getIsMMAv1Row());\n       return RankedTensorType::get(type.getShape(), type.getElementType(),\n@@ -186,9 +192,6 @@ class UpdateMmaForVoltaPass\n       auto type = op->getResult(0).getType().dyn_cast<RankedTensorType>();\n       if (!type)\n         return;\n-      auto encoding = type.getEncoding().dyn_cast<MmaEncodingAttr>();\n-      if (layoutMap.find(encoding) == layoutMap.end())\n-        return;\n       Type newType = updateStaleType(layoutMap, type);\n       if (!newType)\n         return;"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1104,7 +1104,6 @@ def kernel(X, stride_xm, stride_xn,\n                           for dtype in ['int8', 'float16', 'float32']])\n def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype, device='cuda'):\n     capability = torch.cuda.get_device_capability()\n-    capability = (7, 0)\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if capability[0] < 8:"}]