[{"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 167, "deletions": 24, "changes": 191, "file_content_changes": "@@ -1245,7 +1245,7 @@ class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n         ctx, mmaLayout.getVersionMajor(), mmaLayout.getWarpsPerCTA(),\n         AT.getShape(), BT.getShape(), isARow, isBRow);\n \n-    // need to update\n+    // Collect the wrong MMA Layouts, and mark need to update.\n     mmaToUpdate.try_emplace(mmaLayout, newMmaLayout);\n \n     return failure();\n@@ -1254,6 +1254,15 @@ class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n \n class UpdateMMAVersionMinorForVolta : public mlir::RewritePattern {\n   const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n+  enum class Kind {\n+    kUnk,\n+    kCvtMma,\n+    kCvtToMma,\n+    kCvtToDotOp,\n+    kDot,\n+    kConstant,\n+  };\n+  mutable Kind rewriteKind{Kind::kUnk};\n \n public:\n   UpdateMMAVersionMinorForVolta(\n@@ -1263,46 +1272,83 @@ class UpdateMMAVersionMinorForVolta : public mlir::RewritePattern {\n         mmaToUpdate(mmaToUpdate) {}\n \n   LogicalResult match(Operation *op) const override {\n+    MmaEncodingAttr mma;\n+    if (mmaToUpdate.empty())\n+      return failure();\n     if (op->getNumResults() != 1)\n       return failure();\n     auto tensorTy = op->getResult(0).getType().dyn_cast<RankedTensorType>();\n     if (!tensorTy)\n       return failure();\n     bool hit{};\n+    // ConvertLayoutOp\n     if (auto cvt = llvm::dyn_cast<ConvertLayoutOp>(op)) {\n+      auto srcTensorTy =\n+          cvt.getOperand().getType().dyn_cast<RankedTensorType>();\n+      // cvt X -> dot_operand\n       if (auto dotOperand =\n               tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>()) {\n-        if (auto mma = dotOperand.getParent().dyn_cast<MmaEncodingAttr>())\n-          hit = mmaToUpdate.count(mma);\n-      } else if (auto mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>())\n-        hit = mmaToUpdate.count(mma);\n+        mma = dotOperand.getParent().dyn_cast<MmaEncodingAttr>();\n+        rewriteKind = Kind::kCvtToDotOp;\n+        if (mma && mmaToUpdate.count(mma))\n+          return success();\n+      }\n+      if ((mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>())) {\n+        // cvt X -> mma\n+        rewriteKind = Kind::kCvtToMma;\n+        if (mma && mmaToUpdate.count(mma))\n+          return success();\n+      }\n+      if (srcTensorTy &&\n+          (mma = srcTensorTy.getEncoding().dyn_cast<MmaEncodingAttr>())) {\n+        // cvt mma -> X\n+        rewriteKind = Kind::kCvtMma;\n+        llvm::outs() << \"cvt op: \" << *op << \"\\n\";\n+        auto retTypes = op->getOperand(0).getDefiningOp()->getResultTypes();\n+        for (auto t : retTypes) {\n+          llvm::outs() << \"- \" << t << \"\\n\";\n+        }\n+        return failure();\n+        if (mma && mmaToUpdate.count(mma))\n+          return success();\n+      }\n     } else if (auto dot = llvm::dyn_cast<DotOp>(op)) {\n-      auto mma = dot.d()\n-                     .getType()\n-                     .cast<RankedTensorType>()\n-                     .getEncoding()\n-                     .dyn_cast<MmaEncodingAttr>();\n-      if (mma)\n-        hit = mmaToUpdate.count(mma);\n+      // DotOp\n+      mma = dot.d()\n+                .getType()\n+                .cast<RankedTensorType>()\n+                .getEncoding()\n+                .dyn_cast<MmaEncodingAttr>();\n+      rewriteKind = Kind::kDot;\n     } else if (auto constant = llvm::dyn_cast<arith::ConstantOp>(op)) {\n-      if (auto mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>())\n-        hit = mmaToUpdate.count(mma);\n+      // ConstantOp\n+      mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n+      rewriteKind = Kind::kConstant;\n     }\n \n-    return failure(!hit);\n+    return success(mma && mmaToUpdate.count(mma));\n   }\n \n   void rewrite(Operation *op, PatternRewriter &rewriter) const override {\n-    if (auto cvt = llvm::dyn_cast<ConvertLayoutOp>(op)) {\n-      auto tensorTy = cvt.result().getType().cast<RankedTensorType>();\n-      if (tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>())\n-        rewriteCvtDotOp(op, rewriter);\n-      else\n-        rewriteCvtMma(op, rewriter);\n-    } else if (auto dot = llvm::dyn_cast<DotOp>(op))\n+    switch (rewriteKind) {\n+    case Kind::kDot:\n       rewriteDot(op, rewriter);\n-    else if (llvm::dyn_cast<arith::ConstantOp>(op))\n+      break;\n+    case Kind::kConstant:\n       rewriteConstant(op, rewriter);\n+      break;\n+    case Kind::kCvtMma:\n+      rewriteCvtMma(op, rewriter);\n+      break;\n+    case Kind::kCvtToDotOp:\n+      rewriteCvtDotOp(op, rewriter);\n+      break;\n+    case Kind::kCvtToMma:\n+      rewriteCvtToMma(op, rewriter);\n+      break;\n+    default:\n+      assert(false && \"Not supported\");\n+    }\n   }\n \n private:\n@@ -1333,7 +1379,7 @@ class UpdateMMAVersionMinorForVolta : public mlir::RewritePattern {\n                                        dot.c(), dot.allowTF32());\n   }\n \n-  void rewriteCvtMma(Operation *op, PatternRewriter &rewriter) const {\n+  void rewriteCvtToMma(Operation *op, PatternRewriter &rewriter) const {\n     auto *ctx = op->getContext();\n     auto cvt = llvm::cast<ConvertLayoutOp>(op);\n     auto tensorTy = cvt.result().getType().cast<RankedTensorType>();\n@@ -1345,6 +1391,28 @@ class UpdateMMAVersionMinorForVolta : public mlir::RewritePattern {\n                                                  cvt.getOperand());\n   }\n \n+  void rewriteCvtMma(Operation *op, PatternRewriter &rewriter) const {\n+    auto *ctx = op->getContext();\n+    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n+    auto tensorTy = cvt.getOperand().getType().cast<RankedTensorType>();\n+    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto newMma = mmaToUpdate.lookup(mma);\n+    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n+                                             tensorTy.getElementType(), newMma);\n+    llvm::outs() << \"for.types: \";\n+    for (auto t : cvt.getOperand().getDefiningOp()->getResultTypes())\n+      llvm::outs() << \"- \" << t << \"\\n\";\n+    llvm::outs() << \"cvt.operand: \" << cvt.getOperand().getType() << \"\\n\";\n+    auto forOp = cast<scf::ForOp>(cvt.getOperand().getDefiningOp());\n+    llvm::outs() << \"cvt.operand forop.types : \"\n+                 << \"\\n\";\n+    for (auto t : forOp.getBody()->getTerminator()->getResultTypes())\n+      llvm::outs() << \"- \" << t << \"\\n\";\n+    // TODO\n+    rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n+                                                 cvt.getOperand());\n+  }\n+\n   void rewriteConstant(Operation *op, PatternRewriter &rewriter) const {\n     auto *ctx = op->getContext();\n     auto constant = llvm::cast<arith::ConstantOp>(op);\n@@ -1364,6 +1432,70 @@ class UpdateMMAVersionMinorForVolta : public mlir::RewritePattern {\n   }\n };\n \n+class RematerializeForloop : public RewritePattern {\n+\n+  const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n+\n+public:\n+  RematerializeForloop(\n+      mlir::MLIRContext *ctx,\n+      const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n+      : RewritePattern(scf::ForOp::getOperationName(), 1 /*benefit*/, ctx),\n+        mmaToUpdate(mmaToUpdate) {}\n+\n+  LogicalResult matchAndRewrite(Operation *op,\n+                                PatternRewriter &rewriter) const override {\n+    auto forOp = cast<scf::ForOp>(op);\n+    auto iterOps = forOp.getIterOperands();\n+    auto resTypes = forOp->getResultTypes();\n+    bool needRematerialize{};\n+    for (auto type : resTypes)\n+      if (auto tensorTy = type.dyn_cast<RankedTensorType>())\n+        if (auto mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>()) {\n+          auto it = mmaToUpdate.find(mma);\n+          if (it == mmaToUpdate.end())\n+            return failure();\n+          needRematerialize = true;\n+          break;\n+        }\n+\n+    if (needRematerialize) {\n+      auto res = rematerializeForLoop(rewriter, forOp);\n+      rewriter.replaceOp(op, res);\n+      return success();\n+    }\n+    return failure();\n+  }\n+\n+  SmallVector<Value, 4> rematerializeForLoop(mlir::PatternRewriter &rewriter,\n+                                             scf::ForOp &forOp) const {\n+    llvm::outs() << \"initArgs:\" << forOp.getInitArgs()[0].getType() << \"\\n\";\n+    auto newForOp = rewriter.create<scf::ForOp>(\n+        forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n+        forOp.getStep(), forOp.getInitArgs());\n+\n+    newForOp->moveBefore(forOp);\n+    rewriter.setInsertionPointToStart(newForOp.getBody());\n+    BlockAndValueMapping mapping;\n+    for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n+      mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n+    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n+\n+    for (auto &op : forOp.getBody()->without_terminator()) {\n+      rewriter.clone(op, mapping);\n+    }\n+\n+    SmallVector<Value, 4> newYieldArgs;\n+    auto yieldOp = forOp.getBody()->getTerminator();\n+    for (Value arg : yieldOp->getOperands())\n+      newYieldArgs.push_back(mapping.lookup(arg));\n+    rewriter.create<scf::YieldOp>(forOp.getLoc(), newYieldArgs);\n+\n+    auto newResults = newForOp->getResults();\n+    return newResults;\n+  }\n+};\n+\n } // namespace\n \n #define GEN_PASS_CLASSES\n@@ -1398,6 +1530,7 @@ class TritonGPUCombineOpsPass\n       signalPassFailure();\n     }\n \n+    llvm::outs() << \"before:\\n\" << m << \"\\n\";\n     llvm::DenseMap<MmaEncodingAttr, MmaEncodingAttr> mmaToUpdate;\n     {\n       mlir::RewritePatternSet patterns(context);\n@@ -1408,12 +1541,22 @@ class TritonGPUCombineOpsPass\n     {\n       mlir::RewritePatternSet patterns(context);\n       patterns.add<UpdateMMAVersionMinorForVolta>(context, mmaToUpdate);\n+      patterns.add<SimplifyConversion>(context);\n       mlir::GreedyRewriteConfig config;\n       config.useTopDownTraversal = true;\n \n       if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n         signalPassFailure();\n     }\n+    llvm::outs() << \"after:\\n\" << m << \"\\n\";\n+\n+    {\n+      mlir::RewritePatternSet patterns(context);\n+      patterns.add<RematerializeForloop>(context, mmaToUpdate);\n+      if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n+        signalPassFailure();\n+    }\n+    llvm::outs() << \"final:\\n\" << m << \"\\n\";\n \n     mlir::RewritePatternSet loopFixup(context);\n     loopFixup.add<FixupLoop>(context);"}]