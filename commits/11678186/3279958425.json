[{"filename": "lib/Dialect/TritonGPU/Transforms/Swizzle.cpp", "status": "modified", "additions": 20, "deletions": 15, "changes": 35, "file_content_changes": "@@ -79,21 +79,26 @@ struct SwizzlePass : public TritonGPUSwizzleBase<SwizzlePass> {\n         return;\n       for (int opIdx : {0, 1}) {\n         Value op = dotOp.getOperand(opIdx);\n-        auto ty = op.getType().template cast<RankedTensorType>();\n-        // compute new swizzled encoding\n-        SwizzleInfo swizzle = getSwizzleMMA(opIdx, retEncoding, ty);\n-        auto newEncoding = triton::gpu::SharedEncodingAttr::get(\n-            &getContext(), swizzle.vec, swizzle.perPhase, swizzle.maxPhase,\n-            ty.getEncoding()\n-                .cast<triton::gpu::SharedEncodingAttr>()\n-                .getOrder());\n-        // create conversion\n-        auto newType = RankedTensorType::get(ty.getShape(), ty.getElementType(),\n-                                             newEncoding);\n-        Operation *newOp = builder.create<triton::gpu::ConvertLayoutOp>(\n-            op.getLoc(), newType, op);\n-        // bind new op to dot operand\n-        dotOp->replaceUsesOfWith(op, newOp->getResult(0));\n+        // if the dot operand is of dot_op layout which is converted from\n+        // shared layout\n+        if (auto cvt = op.getDefiningOp<triton::gpu::ConvertLayoutOp>()) {\n+          auto ty = cvt.src().getType().template cast<RankedTensorType>();\n+          // compute new swizzled encoding\n+          SwizzleInfo swizzle = getSwizzleMMA(opIdx, retEncoding, ty);\n+          auto newEncoding = triton::gpu::SharedEncodingAttr::get(\n+              &getContext(), swizzle.vec, swizzle.perPhase, swizzle.maxPhase,\n+              ty.getEncoding()\n+                  .cast<triton::gpu::SharedEncodingAttr>()\n+                  .getOrder());\n+          // create conversion\n+          auto newType = RankedTensorType::get(ty.getShape(),\n+                                               ty.getElementType(),\n+                                               newEncoding);\n+          Operation *newOp = builder.create<triton::gpu::ConvertLayoutOp>(\n+              op.getLoc(), newType, op);\n+          // bind new op to cvt operand\n+          cvt->replaceUsesOfWith(op, newOp->getResult(0));\n+        }\n       }\n     });\n   }"}, {"filename": "test/TritonGPU/swizzle.mlir", "status": "modified", "additions": 29, "deletions": 10, "changes": 39, "file_content_changes": "@@ -13,59 +13,78 @@\n #shared2 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #shared3 = #triton_gpu.shared<{vec = 8, perPhase = 4, maxPhase = 2, order = [1, 0]}>\n \n+#mma1w_op0 = #triton_gpu.dot_op<{opIdx = 0, parent = #mma1w}>\n+#mma1w_op1 = #triton_gpu.dot_op<{opIdx = 1, parent = #mma1w}>\n+#mma2w_op0 = #triton_gpu.dot_op<{opIdx = 0, parent = #mma2w}>\n+#mma2w_op1 = #triton_gpu.dot_op<{opIdx = 1, parent = #mma2w}>\n+#mma4w_op0 = #triton_gpu.dot_op<{opIdx = 0, parent = #mma4w}>\n+#mma4w_op1 = #triton_gpu.dot_op<{opIdx = 1, parent = #mma4w}>\n+#mma8w_op0 = #triton_gpu.dot_op<{opIdx = 0, parent = #mma8w}>\n+#mma8w_op1 = #triton_gpu.dot_op<{opIdx = 1, parent = #mma8w}>\n+\n \n module attributes {\"triton_gpu.num-warps\" = 8 : i32} {\n   // CHECK-LABEL: swizzle_mma_f16_128x256x64_w8\n-  func @swizzle_mma_f16_128x256x64_w8(%A: tensor<128x64xf16, #shared>, %B: tensor<64x256xf16, #shared>) {\n+  func @swizzle_mma_f16_128x256x64_w8(%A_SMEM: tensor<128x64xf16, #shared>, %B_SMEM: tensor<64x256xf16, #shared>) {\n     %cst0 = arith.constant dense<0.000000e+00> : tensor<128x256xf32, #mma8w>\n     // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<128x64xf16, {{.*}}>) -> tensor<128x64xf16, [[shared_v8p1m8]]>\n     // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<64x256xf16, {{.*}}>) -> tensor<64x256xf16, [[shared_v8p1m8]]>\n-    %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<128x64xf16, #shared> * tensor<64x256xf16, #shared> -> tensor<128x256xf32, #mma8w>\n+    %A = triton_gpu.convert_layout %A_SMEM : (tensor<128x64xf16, #shared>) -> tensor<128x64xf16, #mma8w_op0>\n+    %B = triton_gpu.convert_layout %B_SMEM : (tensor<64x256xf16, #shared>) -> tensor<64x256xf16, #mma8w_op1>\n+    %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<128x64xf16, #mma8w_op0> * tensor<64x256xf16, #mma8w_op1> -> tensor<128x256xf32, #mma8w>\n     return\n   }\n }\n \n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: swizzle_mma_f16_128x128x64_w4\n-  func @swizzle_mma_f16_128x128x64_w4(%A: tensor<128x64xf16, #shared>, %B: tensor<64x128xf16, #shared>) {\n+  func @swizzle_mma_f16_128x128x64_w4(%A_SMEM: tensor<128x64xf16, #shared>, %B_SMEM: tensor<64x128xf16, #shared>) {\n     %cst0 = arith.constant dense<0.000000e+00> : tensor<128x128xf32, #mma4w>\n     // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<128x64xf16, {{.*}}>) -> tensor<128x64xf16, [[shared_v8p1m8]]>\n     // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<64x128xf16, {{.*}}>) -> tensor<64x128xf16, [[shared_v8p1m8]]>\n-    %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<128x64xf16, #shared> * tensor<64x128xf16, #shared> -> tensor<128x128xf32, #mma4w>\n+    %A = triton_gpu.convert_layout %A_SMEM : (tensor<128x64xf16, #shared>) -> tensor<128x64xf16, #mma4w_op0>\n+    %B = triton_gpu.convert_layout %B_SMEM : (tensor<64x128xf16, #shared>) -> tensor<64x128xf16, #mma4w_op1>\n+    %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<128x64xf16, #mma4w_op0> * tensor<64x128xf16, #mma4w_op1> -> tensor<128x128xf32, #mma4w>\n     return\n   }\n }\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: swizzle_mma_f16_128x128x32_w4\n-  func @swizzle_mma_f16_128x128x32_w4(%A: tensor<128x32xf16, #shared>, %B: tensor<32x128xf16, #shared>) {\n+  func @swizzle_mma_f16_128x128x32_w4(%A_SMEM: tensor<128x32xf16, #shared>, %B_SMEM: tensor<32x128xf16, #shared>) {\n     %cst0 = arith.constant dense<0.000000e+00> : tensor<128x128xf32, #mma4w>\n     // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<128x32xf16, {{.*}}>) -> tensor<128x32xf16, [[shared_v8p2m4]]>\n     // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<32x128xf16, {{.*}}>) -> tensor<32x128xf16, [[shared_v8p1m8]]>\n-    %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<128x32xf16, #shared> * tensor<32x128xf16, #shared> -> tensor<128x128xf32, #mma4w>\n+    %A = triton_gpu.convert_layout %A_SMEM : (tensor<128x32xf16, #shared>) -> tensor<128x32xf16, #mma4w_op0>\n+    %B = triton_gpu.convert_layout %B_SMEM : (tensor<32x128xf16, #shared>) -> tensor<32x128xf16, #mma4w_op1>\n+    %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<128x32xf16, #mma4w_op0> * tensor<32x128xf16, #mma4w_op1> -> tensor<128x128xf32, #mma4w>\n     return\n   }\n }\n \n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   // CHECK-LABEL: swizzle_mma_f16_32x32x32_w2\n-  func @swizzle_mma_f16_32x32x32_w2(%A: tensor<32x32xf16, #shared>, %B: tensor<32x32xf16, #shared>) {\n+  func @swizzle_mma_f16_32x32x32_w2(%A_SMEM: tensor<32x32xf16, #shared>, %B_SMEM: tensor<32x32xf16, #shared>) {\n     %cst0 = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma2w>\n     // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<32x32xf16, {{.*}}>) -> tensor<32x32xf16, [[shared_v8p2m4]]>\n     // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<32x32xf16, {{.*}}>) -> tensor<32x32xf16, [[shared_v8p2m4]]>\n-    %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<32x32xf16, #shared> * tensor<32x32xf16, #shared> -> tensor<32x32xf32, #mma2w>\n+    %A = triton_gpu.convert_layout %A_SMEM : (tensor<32x32xf16, #shared>) -> tensor<32x32xf16, #mma2w_op0>\n+    %B = triton_gpu.convert_layout %B_SMEM : (tensor<32x32xf16, #shared>) -> tensor<32x32xf16, #mma2w_op1>\n+    %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<32x32xf16, #mma2w_op0> * tensor<32x32xf16, #mma2w_op1> -> tensor<32x32xf32, #mma2w>\n     return\n   }\n }\n \n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: swizzle_mma_f16_16x16x16_w1\n-  func @swizzle_mma_f16_16x16x16_w1(%A: tensor<16x16xf16, #shared>, %B: tensor<16x16xf16, #shared>) {\n+  func @swizzle_mma_f16_16x16x16_w1(%A_SMEM: tensor<16x16xf16, #shared>, %B_SMEM: tensor<16x16xf16, #shared>) {\n     %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma1w>\n     // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<16x16xf16, {{.*}}>) -> tensor<16x16xf16, [[shared_v8p4m2]]>\n     // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<16x16xf16, {{.*}}>) -> tensor<16x16xf16, [[shared_v8p4m2]]>\n-    %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<16x16xf16, #shared> * tensor<16x16xf16, #shared> -> tensor<16x16xf32, #mma1w>\n+    %A = triton_gpu.convert_layout %A_SMEM : (tensor<16x16xf16, #shared>) -> tensor<16x16xf16, #mma1w_op0>\n+    %B = triton_gpu.convert_layout %B_SMEM : (tensor<16x16xf16, #shared>) -> tensor<16x16xf16, #mma1w_op1>\n+    %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<16x16xf16, #mma1w_op0> * tensor<16x16xf16, #mma1w_op1> -> tensor<16x16xf32, #mma1w>\n     return\n   }\n }"}]