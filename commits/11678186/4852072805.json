[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -620,10 +620,10 @@ struct ConvertLayoutOpConversion\n       // is implemented\n       SmallVector<Value> reorderedVals;\n       for (unsigned i = 0; i < vecVals.size(); i += 4) {\n-        reorderedVals.push_back(vecVals[i]);\n-        reorderedVals.push_back(vecVals[i + 2]);\n-        reorderedVals.push_back(vecVals[i + 1]);\n-        reorderedVals.push_back(vecVals[i + 3]);\n+        reorderedVals.push_back(bitcast(vecVals[i], i32_ty));\n+        reorderedVals.push_back(bitcast(vecVals[i + 2], i32_ty));\n+        reorderedVals.push_back(bitcast(vecVals[i + 1], i32_ty));\n+        reorderedVals.push_back(bitcast(vecVals[i + 3], i32_ty));\n       }\n \n       Value view = getTypeConverter()->packLLElements(loc, reorderedVals,\n@@ -642,19 +642,19 @@ struct ConvertLayoutOpConversion\n     auto loc = op.getLoc();\n     Value src = op.getSrc();\n     Value dst = op.getResult();\n-    bool isHMMA = supportMMA(dst, mmaLayout.getVersionMajor());\n \n     auto smemObj =\n         getSharedMemoryObjectFromStruct(loc, adaptor.getSrc(), rewriter);\n     Value res;\n \n-    if (!isOuter && mmaLayout.isAmpere() && isHMMA) { // tensor core v2\n+    if (!isOuter && mmaLayout.isAmpere()) { // tensor core v2\n \n       res = SharedToDotOperandMMAv2::convertLayout(\n           dotOperandLayout.getOpIdx(), rewriter, loc, src, dotOperandLayout,\n           smemObj, getTypeConverter(), tid_val());\n \n-    } else if (!isOuter && mmaLayout.isVolta() && isHMMA) { // tensor core v1\n+    } else if (!isOuter && mmaLayout.isVolta() &&\n+               supportMMA(dst, mmaLayout.getVersionMajor())) { // tensor core v1\n       bool isMMAv1Row = dotOperandLayout.getMMAv1IsRow();\n       auto srcSharedLayout = src.getType()\n                                  .cast<RankedTensorType>()"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 112, "deletions": 170, "changes": 282, "file_content_changes": "@@ -19,10 +19,10 @@ using ::mlir::triton::gpu::SharedEncodingAttr;\n class MMA16816SmemLoader {\n public:\n   MMA16816SmemLoader(int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n-                     ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n-                     ArrayRef<int> instrShape, ArrayRef<int> matShape,\n-                     int perPhase, int maxPhase, int elemBytes,\n-                     ConversionPatternRewriter &rewriter,\n+                     int kWidth, ArrayRef<Value> smemStrides,\n+                     ArrayRef<int64_t> tileShape, ArrayRef<int> instrShape,\n+                     ArrayRef<int> matShape, int perPhase, int maxPhase,\n+                     int elemBytes, ConversionPatternRewriter &rewriter,\n                      TritonGPUToLLVMTypeConverter *typeConverter,\n                      const Location &loc);\n \n@@ -33,7 +33,7 @@ class MMA16816SmemLoader {\n     if (canUseLdmatrix)\n       return computeLdmatrixMatOffs(warpOff, lane, cSwizzleOffset);\n     else\n-      return computeLdsMatOffs(warpOff, lane, cSwizzleOffset, elemBytes);\n+      return computeLdsMatOffs(warpOff, lane, cSwizzleOffset);\n     return {};\n   }\n \n@@ -45,7 +45,7 @@ class MMA16816SmemLoader {\n                                             Value cSwizzleOffset);\n   // compute 8-bit matrix offset.\n   SmallVector<Value> computeLdsMatOffs(Value warpOff, Value lane,\n-                                       Value cSwizzleOffset, int elemBytes);\n+                                       Value cSwizzleOffset);\n \n   // Load 4 matrices and returns 4 vec<2> elements.\n   std::tuple<Value, Value, Value, Value>\n@@ -55,6 +55,7 @@ class MMA16816SmemLoader {\n private:\n   SmallVector<uint32_t> order;\n   int kOrder;\n+  int kWidth;\n   SmallVector<int64_t> tileShape;\n   SmallVector<int> instrShape;\n   SmallVector<int> matShape;\n@@ -176,9 +177,7 @@ MMA16816SmemLoader::computeLdmatrixMatOffs(Value warpId, Value lane,\n \n SmallVector<Value> MMA16816SmemLoader::computeLdsMatOffs(Value warpOff,\n                                                          Value lane,\n-                                                         Value cSwizzleOffset,\n-                                                         int elemBytes) {\n-  assert(elemBytes <= 4);\n+                                                         Value cSwizzleOffset) {\n   int cTileShape = tileShape[order[0]];\n   int sTileShape = tileShape[order[1]];\n   if (!needTrans) {\n@@ -187,10 +186,10 @@ SmallVector<Value> MMA16816SmemLoader::computeLdsMatOffs(Value warpOff,\n \n   SmallVector<Value> offs(numPtrs);\n \n+  int vecWidth = kWidth;\n   int threadsPerQuad[2] = {8, 4};\n   int laneWidth = 4;\n   int laneHeight = 8;\n-  int vecWidth = 4 / elemBytes;\n   int quadWidth = laneWidth * vecWidth;\n   int quadHeight = laneHeight;\n   int numQuadI = 2;\n@@ -232,8 +231,8 @@ SmallVector<Value> MMA16816SmemLoader::computeLdsMatOffs(Value warpOff,\n         Value i = add(iBase, mul(iOff, i32_val(quadHeight)));\n         Value j = add(jBase, mul(jOff, i32_val(quadWidth)));\n         // wrap around the bounds\n-        i = urem(i, i32_val(cTileShape));\n-        j = urem(j, i32_val(sTileShape));\n+        // i = urem(i, i32_val(cTileShape));\n+        // j = urem(j, i32_val(sTileShape));\n         if (needTrans) {\n           offs[idx] = add(i, mul(j, sStride));\n         } else {\n@@ -304,7 +303,6 @@ MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> offs,\n     return {extract_val(elemTy, resV4, 0), extract_val(elemTy, resV4, 1),\n             extract_val(elemTy, resV4, 2), extract_val(elemTy, resV4, 3)};\n   } else {\n-    elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n     // base pointers\n     std::array<std::array<Value, 4>, 2> ptrs;\n     int vecWidth = 4 / elemBytes;\n@@ -324,39 +322,50 @@ MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> offs,\n     std::array<Value, 2> ii = {i0, i1};\n     // load 4 32-bit values from shared memory\n     // (equivalent to ldmatrix.x4)\n-    SmallVector<SmallVector<Value>> vals(4, SmallVector<Value>(vecWidth));\n+    SmallVector<SmallVector<Value>> vptrs(4, SmallVector<Value>(vecWidth));\n     for (int i = 0; i < 4; ++i)\n       for (int j = 0; j < vecWidth; ++j)\n-        vals[i][j] = load(gep(shemPtrTy, ptrs[i / 2][j], ii[i % 2]));\n+        vptrs[i][j] = gep(shemPtrTy, ptrs[i / 2][j], ii[i % 2]);\n     // row + trans and col + no-trans are equivalent\n-    if ((needTrans && kOrder == 1) || (!needTrans && kOrder == 0))\n-      std::swap(vals[1], vals[2]);\n+    bool isActualTrans =\n+        (needTrans && kOrder == 1) || (!needTrans && kOrder == 0);\n+    if (isActualTrans)\n+      std::swap(vptrs[1], vptrs[2]);\n     // pack loaded vectors into 4 32-bit values\n+    int inc = needTrans ? 1 : kWidth;\n+    VectorType packedTy = vec_ty(int_ty(8 * elemBytes), inc);\n+    int canonBits = std::min(32, 8 * elemBytes * inc);\n+    int canonWidth = (8 * elemBytes * inc) / canonBits;\n+    Type canonInt = int_ty(canonBits);\n     std::array<Value, 4> retElems;\n-    retElems.fill(undef(elemTy));\n-    for (int m = 0; m < 4; ++m) {\n-      for (int e = 0; e < vecWidth; ++e)\n-        retElems[m] = insert_element(retElems[m].getType(), retElems[m],\n-                                     vals[m][e], i32_val(e));\n+    retElems.fill(undef(vec_ty(canonInt, 32 / canonBits)));\n+    for (int r = 0; r < 2; ++r) {\n+      for (int em = 0; em < 2 * vecWidth; em += inc) {\n+        int e = em % vecWidth;\n+        int m = em / vecWidth;\n+        int idx = m * 2 + r;\n+        Value ptr = bitcast(vptrs[idx][e], ptr_ty(packedTy, 3));\n+        Value val = load(ptr);\n+        Value canonval = bitcast(val, vec_ty(canonInt, canonWidth));\n+        for (int w = 0; w < canonWidth; ++w) {\n+          retElems[idx + w * kWidth / vecWidth] =\n+              insert_element(retElems[idx + w * kWidth / vecWidth],\n+                             extract_element(canonval, i32_val(w)), i32_val(e));\n+        }\n+      }\n     }\n-    if (elemBytes == 1)\n-      return {bitcast(retElems[0], i32_ty), bitcast(retElems[1], i32_ty),\n-              bitcast(retElems[2], i32_ty), bitcast(retElems[3], i32_ty)};\n-    else\n-      return {retElems[0], retElems[1], retElems[2], retElems[3]};\n+    return {bitcast(retElems[0], i32_ty), bitcast(retElems[1], i32_ty),\n+            bitcast(retElems[2], i32_ty), bitcast(retElems[3], i32_ty)};\n   }\n-\n-  assert(false && \"Invalid smem load\");\n-  return {Value{}, Value{}, Value{}, Value{}};\n }\n \n MMA16816SmemLoader::MMA16816SmemLoader(\n-    int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n+    int wpt, ArrayRef<uint32_t> order, uint32_t kOrder, int kWidth,\n     ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n     ArrayRef<int> instrShape, ArrayRef<int> matShape, int perPhase,\n     int maxPhase, int elemBytes, ConversionPatternRewriter &rewriter,\n     TritonGPUToLLVMTypeConverter *typeConverter, const Location &loc)\n-    : order(order.begin(), order.end()), kOrder(kOrder),\n+    : order(order.begin(), order.end()), kOrder(kOrder), kWidth(kWidth),\n       tileShape(tileShape.begin(), tileShape.end()),\n       instrShape(instrShape.begin(), instrShape.end()),\n       matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n@@ -369,7 +378,8 @@ MMA16816SmemLoader::MMA16816SmemLoader(\n \n   // rule: k must be the fast-changing axis.\n   needTrans = kOrder != order[0];\n-  canUseLdmatrix = elemBytes == 2 || (!needTrans); // b16\n+  canUseLdmatrix = elemBytes == 2 || (!needTrans);\n+  canUseLdmatrix = canUseLdmatrix && (kWidth == 4 / elemBytes);\n \n   if (canUseLdmatrix) {\n     // Each CTA, the warps is arranged as [1xwpt] if not transposed,\n@@ -409,42 +419,12 @@ Type getShemPtrTy(Type argType) {\n     return ptr_ty(type::i16Ty(ctx), 3);\n   else if (argType.isF32())\n     return ptr_ty(type::f32Ty(ctx), 3);\n-  else if (argType.isInteger(8))\n+  else if (argType.getIntOrFloatBitWidth() == 8)\n     return ptr_ty(type::i8Ty(ctx), 3);\n   else\n     llvm::report_fatal_error(\"mma16816 data type not supported\");\n }\n \n-Type getMatType(Type argType) {\n-  MLIRContext *ctx = argType.getContext();\n-  // floating point types\n-  Type fp32x1Ty = vec_ty(type::f32Ty(ctx), 1);\n-  Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n-  Type i16x2Ty = vec_ty(type::i16Ty(ctx), 2);\n-  Type fp16x2Pack4Ty =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n-  // LLVM 14.0 does not support bf16 type, so we use i16 instead.\n-  Type bf16x2Pack4Ty =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i16x2Ty));\n-  Type fp32Pack4Ty =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32x1Ty));\n-  // integer types\n-  Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n-  Type i8x4Pack4Ty =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i8x4Ty));\n-\n-  if (argType.isF16())\n-    return fp16x2Pack4Ty;\n-  else if (argType.isBF16())\n-    return bf16x2Pack4Ty;\n-  else if (argType.isF32())\n-    return fp32Pack4Ty;\n-  else if (argType.isInteger(8))\n-    return i8x4Pack4Ty;\n-  else\n-    llvm::report_fatal_error(\"mma16816 data type not supported\");\n-}\n-\n Value composeValuesToDotOperandLayoutStruct(\n     const ValueTable &vals, int n0, int n1,\n     TritonGPUToLLVMTypeConverter *typeConverter, Location loc,\n@@ -470,7 +450,7 @@ Value composeValuesToDotOperandLayoutStruct(\n \n std::function<void(int, int)>\n getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n-                MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder,\n+                MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder, int kWidth,\n                 SmallVector<int> instrShape, SmallVector<int> matShape,\n                 Value warpId, Value lane, ValueTable &vals, bool isA,\n                 TritonGPUToLLVMTypeConverter *typeConverter,\n@@ -485,143 +465,105 @@ getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n   const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n   auto order = sharedLayout.getOrder();\n \n-  // the original register_lds2, but discard the prefetch logic.\n-  auto ld2 = [](ValueTable &vals, int mn, int k, Value val) {\n-    vals[{mn, k}] = val;\n-  };\n-\n   // (a, b) is the coordinate.\n-  auto load = [=, &rewriter, &vals, &ld2](int a, int b) {\n+  auto load = [=, &rewriter, &vals](int a, int b) {\n     MMA16816SmemLoader loader(\n-        wpt, sharedLayout.getOrder(), kOrder, smemObj.strides,\n+        wpt, sharedLayout.getOrder(), kOrder, kWidth, smemObj.strides,\n         tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n         maxPhase, elemBytes, rewriter, typeConverter, loc);\n     Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n     SmallVector<Value> offs =\n         loader.computeOffsets(warpId, lane, cSwizzleOffset);\n+    // initialize pointers\n     const int numPtrs = loader.getNumPtrs();\n     SmallVector<Value> ptrs(numPtrs);\n-\n     Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-\n     Type smemPtrTy = getShemPtrTy(eltTy);\n-    for (int i = 0; i < numPtrs; ++i) {\n-      ptrs[i] =\n-          bitcast(gep(smemPtrTy, smemBase, ValueRange({offs[i]})), smemPtrTy);\n-    }\n-\n+    for (int i = 0; i < numPtrs; ++i)\n+      ptrs[i] = bitcast(gep(smemPtrTy, smemBase, offs[i]), smemPtrTy);\n+    // actually load from shared memory\n+    auto matTy = LLVM::LLVMStructType::getLiteral(eltTy.getContext(),\n+                                                  SmallVector<Type>(4, i32_ty));\n     auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n         (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n-        ptrs, getMatType(eltTy), getShemPtrTy(eltTy));\n-\n-    if (isA) {\n-      ld2(vals, a, b, ha0);\n-      ld2(vals, a + 1, b, ha1);\n-      ld2(vals, a, b + 1, ha2);\n-      ld2(vals, a + 1, b + 1, ha3);\n-    } else {\n-      ld2(vals, a, b, ha0);\n-      ld2(vals, a + 1, b, ha2);\n-      ld2(vals, a, b + 1, ha1);\n-      ld2(vals, a + 1, b + 1, ha3);\n-    }\n+        ptrs, matTy, getShemPtrTy(eltTy));\n+    if (!isA)\n+      std::swap(ha1, ha2);\n+    // the following is incorrect\n+    // but causes dramatically better performance in ptxas\n+    // although it only changes the order of operands in\n+    // `mma.sync`\n+    // if(isA)\n+    //   std::swap(ha1, ha2);\n+    // update user-provided values in-place\n+    vals[{a, b}] = ha0;\n+    vals[{a + 1, b}] = ha1;\n+    vals[{a, b + 1}] = ha2;\n+    vals[{a + 1, b + 1}] = ha3;\n   };\n \n   return load;\n }\n \n-Value loadA(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n-            DotOperandEncodingAttr aEncoding, const SharedMemoryObject &smemObj,\n-            TritonGPUToLLVMTypeConverter *typeConverter, Value thread) {\n-  auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n-  int bitwidth = aTensorTy.getElementTypeBitWidth();\n-  auto mmaLayout = aEncoding.getParent().cast<MmaEncodingAttr>();\n-\n-  SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n-                             aTensorTy.getShape().end());\n-\n-  ValueTable ha;\n-  std::function<void(int, int)> loadFn;\n-  int mmaInstrM = 16, mmaInstrN = 8, mmaInstrK = 4 * 64 / bitwidth;\n-  int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n-\n-  auto numRep = aEncoding.getMMAv2Rep(aTensorTy.getShape(), bitwidth);\n-  int numRepM = numRep[0];\n-  int numRepK = numRep[1];\n-\n-  if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n-    int wpt0 = mmaLayout.getWarpsPerCTA()[0];\n-    Value warp = udiv(thread, i32_val(32));\n-    Value lane = urem(thread, i32_val(32));\n-    Value warpM = urem(urem(warp, i32_val(wpt0)), i32_val(shape[0] / 16));\n-    // load from smem\n-    // we use ldmatrix.x4 so each warp processes 16x16 elements.\n-    int wpt = std::min<int>(wpt0, shape[0] / 16);\n-    loadFn = getLoadMatrixFn(\n-        tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/,\n-        {mmaInstrM, mmaInstrK} /*instrShape*/,\n-        {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, lane /*laneId*/,\n-        ha /*vals*/, true /*isA*/, typeConverter /* typeConverter */,\n-        rewriter /*rewriter*/, loc /*loc*/);\n-  } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n-    // load from registers, used in gemm fuse\n-    // TODO(Superjomn) Port the logic.\n-    assert(false && \"Loading A from register is not supported yet.\");\n-  } else {\n-    assert(false && \"A's layout is not supported.\");\n-  }\n-\n-  // step1. Perform loading.\n-  for (int m = 0; m < numRepM; ++m)\n-    for (int k = 0; k < numRepK; ++k)\n-      loadFn(2 * m, 2 * k);\n-\n-  // step2. Format the values to LLVM::Struct to passing to mma codegen.\n-  return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK,\n-                                               typeConverter, loc, rewriter);\n-}\n-\n-Value loadB(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n-            DotOperandEncodingAttr bEncoding, const SharedMemoryObject &smemObj,\n-            TritonGPUToLLVMTypeConverter *typeConverter, Value thread) {\n-  ValueTable hb;\n+Value loadArg(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n+              DotOperandEncodingAttr encoding,\n+              const SharedMemoryObject &smemObj,\n+              TritonGPUToLLVMTypeConverter *typeConverter, Value thread,\n+              bool isA) {\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n   int bitwidth = tensorTy.getElementTypeBitWidth();\n-  auto mmaLayout = bEncoding.getParent().cast<MmaEncodingAttr>();\n+  auto mmaLayout = encoding.getParent().cast<MmaEncodingAttr>();\n \n   SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n                              tensorTy.getShape().end());\n \n+  ValueTable vals;\n   int mmaInstrM = 16, mmaInstrN = 8, mmaInstrK = 4 * 64 / bitwidth;\n   int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n \n-  auto numRep = bEncoding.getMMAv2Rep(tensorTy.getShape(), bitwidth);\n-  int numRepK = numRep[0];\n-  int numRepN = numRep[1];\n+  auto numRep = encoding.getMMAv2Rep(tensorTy.getShape(), bitwidth);\n+  int kWidth = encoding.getMMAv2kWidth();\n \n   int wpt0 = mmaLayout.getWarpsPerCTA()[0];\n   int wpt1 = mmaLayout.getWarpsPerCTA()[1];\n   Value warp = udiv(thread, i32_val(32));\n   Value lane = urem(thread, i32_val(32));\n+  Value warpM = urem(urem(warp, i32_val(wpt0)), i32_val(shape[0] / 16));\n   Value warpMN = udiv(warp, i32_val(wpt0));\n   Value warpN = urem(urem(warpMN, i32_val(wpt1)), i32_val(shape[1] / 8));\n-  // we use ldmatrix.x4 so each warp processes 16x16 elements.\n-  int wpt = std::min<int>(wpt1, shape[1] / 16);\n-  auto loadFn = getLoadMatrixFn(\n-      tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/,\n-      {mmaInstrK, mmaInstrN} /*instrShape*/,\n-      {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, lane /*laneId*/,\n-      hb /*vals*/, false /*isA*/, typeConverter /* typeConverter */,\n-      rewriter /*rewriter*/, loc /*loc*/);\n-\n-  for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n+\n+  int wpt;\n+  if (isA)\n+    wpt = std::min<int>(wpt0, shape[0] / 16);\n+  else\n+    wpt = std::min<int>(wpt1, shape[1] / 16);\n+\n+  std::function<void(int, int)> loadFn;\n+  if (isA)\n+    loadFn = getLoadMatrixFn(\n+        tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/, kWidth,\n+        {mmaInstrM, mmaInstrK} /*instrShape*/,\n+        {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, lane /*laneId*/,\n+        vals /*vals*/, isA /*isA*/, typeConverter /* typeConverter */,\n+        rewriter /*rewriter*/, loc /*loc*/);\n+  else\n+    loadFn = getLoadMatrixFn(\n+        tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/, kWidth,\n+        {mmaInstrK, mmaInstrN} /*instrShape*/,\n+        {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, lane /*laneId*/,\n+        vals /*vals*/, isA /*isA*/, typeConverter /* typeConverter */,\n+        rewriter /*rewriter*/, loc /*loc*/);\n+\n+  // Perform loading.\n+  int numRepOuter = isA ? numRep[0] : std::max<int>(numRep[1] / 2, 1);\n+  int numRepK = isA ? numRep[1] : numRep[0];\n+  for (int m = 0; m < numRepOuter; ++m)\n     for (int k = 0; k < numRepK; ++k)\n-      loadFn(2 * n, 2 * k);\n-  }\n+      loadFn(2 * m, 2 * k);\n \n-  Value result = composeValuesToDotOperandLayoutStruct(\n-      hb, std::max(numRepN / 2, 1), numRepK, typeConverter, loc, rewriter);\n-  return result;\n+  // Format the values to LLVM::Struct to passing to mma codegen.\n+  return composeValuesToDotOperandLayoutStruct(vals, numRepOuter, numRepK,\n+                                               typeConverter, loc, rewriter);\n }\n \n namespace SharedToDotOperandMMAv2 {\n@@ -630,12 +572,12 @@ Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,\n                     const SharedMemoryObject &smemObj,\n                     TritonGPUToLLVMTypeConverter *typeConverter, Value thread) {\n   if (opIdx == 0)\n-    return loadA(rewriter, loc, tensor, encoding, smemObj, typeConverter,\n-                 thread);\n+    return loadArg(rewriter, loc, tensor, encoding, smemObj, typeConverter,\n+                   thread, true);\n   else {\n     assert(opIdx == 1);\n-    return loadB(rewriter, loc, tensor, encoding, smemObj, typeConverter,\n-                 thread);\n+    return loadArg(rewriter, loc, tensor, encoding, smemObj, typeConverter,\n+                   thread, false);\n   }\n }\n } // namespace SharedToDotOperandMMAv2"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 193, "deletions": 66, "changes": 259, "file_content_changes": "@@ -4,11 +4,140 @@ using namespace mlir;\n using namespace mlir::triton;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n \n+static SmallVector<Value> reorderValues(const SmallVector<Value> &values,\n+                                        Type inType, Type ouType) {\n+  auto inTensorTy = inType.dyn_cast<RankedTensorType>();\n+  auto ouTensorTy = ouType.dyn_cast<RankedTensorType>();\n+  if (!inTensorTy || !ouTensorTy)\n+    return values;\n+  auto inEncoding =\n+      dyn_cast<triton::gpu::DotOperandEncodingAttr>(inTensorTy.getEncoding());\n+  auto ouEncoding =\n+      dyn_cast<triton::gpu::DotOperandEncodingAttr>(ouTensorTy.getEncoding());\n+  assert(inEncoding == ouEncoding);\n+  if (!inEncoding)\n+    return values;\n+  size_t inBitWidth = inTensorTy.getElementType().getIntOrFloatBitWidth();\n+  size_t ouBitWidth = ouTensorTy.getElementType().getIntOrFloatBitWidth();\n+  auto ouEltTy = ouTensorTy.getElementType();\n+  if (inBitWidth == ouBitWidth)\n+    return values;\n+  if (inBitWidth == 16 && ouBitWidth == 32) {\n+    SmallVector<Value> ret;\n+    for (unsigned i = 0; i < values.size(); i += 8) {\n+      ret.push_back(values[i]);\n+      ret.push_back(values[i + 1]);\n+      ret.push_back(values[i + 4]);\n+      ret.push_back(values[i + 5]);\n+      ret.push_back(values[i + 2]);\n+      ret.push_back(values[i + 3]);\n+      ret.push_back(values[i + 6]);\n+      ret.push_back(values[i + 7]);\n+    }\n+    return ret;\n+  }\n+  if (inBitWidth == 8 && ouBitWidth == 16) {\n+    SmallVector<Value> ret;\n+    for (unsigned i = 0; i < values.size(); i += 16) {\n+      ret.push_back(values[i + 0]);\n+      ret.push_back(values[i + 1]);\n+      ret.push_back(values[i + 2]);\n+      ret.push_back(values[i + 3]);\n+      ret.push_back(values[i + 8]);\n+      ret.push_back(values[i + 9]);\n+      ret.push_back(values[i + 10]);\n+      ret.push_back(values[i + 11]);\n+      ret.push_back(values[i + 4]);\n+      ret.push_back(values[i + 5]);\n+      ret.push_back(values[i + 6]);\n+      ret.push_back(values[i + 7]);\n+      ret.push_back(values[i + 12]);\n+      ret.push_back(values[i + 13]);\n+      ret.push_back(values[i + 14]);\n+      ret.push_back(values[i + 15]);\n+    }\n+    return ret;\n+    // for (unsigned i = 0; i < values.size(); i += 16) {\n+    //   ret.push_back(values[i]);\n+    //   ret.push_back(values[i + 1]);\n+    //   ret.push_back(values[i + 4]);\n+    //   ret.push_back(values[i + 5]);\n+    //   ret.push_back(values[i + 8]);\n+    //   ret.push_back(values[i + 9]);\n+    //   ret.push_back(values[i + 12]);\n+    //   ret.push_back(values[i + 13]);\n+\n+    //   ret.push_back(values[i + 2]);\n+    //   ret.push_back(values[i + 3]);\n+    //   ret.push_back(values[i + 6]);\n+    //   ret.push_back(values[i + 7]);\n+    //   ret.push_back(values[i + 10]);\n+    //   ret.push_back(values[i + 11]);\n+    //   ret.push_back(values[i + 14]);\n+    //   ret.push_back(values[i + 15]);\n+    // }\n+    return values;\n+  }\n+  llvm_unreachable(\"unimplemented code path\");\n+}\n+\n+inline SmallVector<Value> unpackI32(const SmallVector<Value> &inValues,\n+                                    Type srcTy,\n+                                    ConversionPatternRewriter &rewriter,\n+                                    Location loc,\n+                                    TypeConverter *typeConverter) {\n+  auto tensorTy = srcTy.dyn_cast<RankedTensorType>();\n+  if (!tensorTy)\n+    return inValues;\n+  auto encoding = tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>();\n+  if (!(encoding && encoding.getParent().isa<MmaEncodingAttr>()))\n+    return inValues;\n+  SmallVector<Value> outValues;\n+  for (auto v : inValues) {\n+    // cast i32 to appropriate eltType vector and extract elements\n+    auto eltType = typeConverter->convertType(tensorTy.getElementType());\n+    auto vecType = vec_ty(eltType, 32 / eltType.getIntOrFloatBitWidth());\n+    auto vec = bitcast(v, vecType);\n+    for (int i = 0; i < 32 / eltType.getIntOrFloatBitWidth(); i++) {\n+      outValues.push_back(extract_element(vec, i32_val(i)));\n+    }\n+  }\n+  return outValues;\n+}\n+\n+inline SmallVector<Value> packI32(const SmallVector<Value> &inValues,\n+                                  Type srcTy,\n+                                  ConversionPatternRewriter &rewriter,\n+                                  Location loc, TypeConverter *typeConverter) {\n+  auto tensorTy = srcTy.dyn_cast<RankedTensorType>();\n+  if (!tensorTy)\n+    return inValues;\n+  auto encoding = tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>();\n+  if (!(encoding && encoding.getParent().isa<MmaEncodingAttr>()))\n+    return inValues;\n+  SmallVector<Value> outValues;\n+  auto eltType = typeConverter->convertType(tensorTy.getElementType());\n+  int vecWidth = 32 / eltType.getIntOrFloatBitWidth();\n+  auto vecType = vec_ty(eltType, vecWidth);\n+  for (int i = 0; i < inValues.size(); i += vecWidth) {\n+    Value vec = undef(vecType);\n+    for (int j = 0; j < vecWidth; j++) {\n+      vec = insert_element(vec, inValues[i + j], i32_val(j));\n+    }\n+    outValues.push_back(bitcast(vec, i32_ty));\n+  }\n+  return outValues;\n+}\n+\n struct FpToFpOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::FpToFpOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::FpToFpOp>::ConvertTritonGPUOpToLLVMPattern;\n \n+  typedef std::function<SmallVector<Value>(\n+      Location, ConversionPatternRewriter &, const Value &, const Value &,\n+      const Value &, const Value &)>\n+      ConvertorT;\n   /* ------------------ */\n   // FP8 -> FP16\n   /* ------------------ */\n@@ -490,35 +619,14 @@ struct FpToFpOpConversion\n     return builder.launch(rewriter, loc, f16_ty, false);\n   }\n \n-  LogicalResult\n-  matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    auto srcTensorType = op.getFrom().getType().cast<mlir::RankedTensorType>();\n-    auto dstTensorType =\n-        op.getResult().getType().cast<mlir::RankedTensorType>();\n-    auto srcEltType = srcTensorType.getElementType();\n-    auto dstEltType = dstTensorType.getElementType();\n-    auto loc = op->getLoc();\n-    auto elems = getTotalElemsPerThread(dstTensorType);\n-    SmallVector<Value> resultVals;\n-    bool isSrcFP8 =\n-        srcEltType.isa<mlir::Float8E4M3FNType, mlir::Float8E5M2Type>();\n-    bool isDstFP8 =\n-        dstEltType.isa<mlir::Float8E4M3FNType, mlir::Float8E5M2Type>();\n-\n-    // Select convertor\n-    typedef std::function<SmallVector<Value>(\n-        Location, ConversionPatternRewriter &, const Value &, const Value &,\n-        const Value &, const Value &)>\n-        ConvertorT;\n-\n+  ConvertorT getConversionFunc(Type srcTy, Type dstTy) const {\n     auto F8E4M3TyID = TypeID::get<mlir::Float8E4M3FNType>();\n     auto F8E5M2TyID = TypeID::get<mlir::Float8E5M2Type>();\n     auto F16TyID = TypeID::get<mlir::Float16Type>();\n     auto BF16TyID = TypeID::get<mlir::BFloat16Type>();\n     auto F32TyID = TypeID::get<mlir::Float32Type>();\n     auto F64TyID = TypeID::get<mlir::Float64Type>();\n-    DenseMap<std::pair<TypeID, TypeID>, ConvertorT> convertorMap = {\n+    static DenseMap<std::pair<TypeID, TypeID>, ConvertorT> convertorMap = {\n         // F8 -> F16\n         {{F8E4M3TyID, F16TyID}, convertFp8E4M3x4ToFp16x4},\n         {{F8E5M2TyID, F16TyID}, convertFp8E5M2x4ToFp16x4},\n@@ -539,28 +647,46 @@ struct FpToFpOpConversion\n         {{F32TyID, F8E5M2TyID}, convertFp32x4ToFp8E5M2x4},\n     };\n \n-    std::pair<TypeID, TypeID> key = {srcEltType.getTypeID(),\n-                                     dstEltType.getTypeID()};\n+    std::pair<TypeID, TypeID> key = {srcTy.getTypeID(), dstTy.getTypeID()};\n     if (convertorMap.count(key) == 0) {\n-      llvm::errs() << \"Unsupported conversion from \" << srcEltType << \" to \"\n-                   << dstEltType << \"\\n\";\n+      llvm::errs() << \"Unsupported conversion from \" << srcTy << \" to \" << dstTy\n+                   << \"\\n\";\n       llvm_unreachable(\"\");\n     }\n-    auto convertor = convertorMap.lookup(key);\n+    return convertorMap.lookup(key);\n+  }\n \n-    // Vectorized casting\n+  LogicalResult\n+  matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    // llvm::outs() << 0 << \"\\n\";\n+    auto srcTensorType = op.getFrom().getType().cast<mlir::RankedTensorType>();\n+    auto dstTensorType =\n+        op.getResult().getType().cast<mlir::RankedTensorType>();\n+    auto loc = op->getLoc();\n+    // check that the number of elements is divisible by 4\n+    // Get convertor\n+    auto cvtFunc = getConversionFunc(srcTensorType.getElementType(),\n+                                     dstTensorType.getElementType());\n+    // Unpack value\n+    auto inVals = getTypeConverter()->unpackLLElements(loc, adaptor.getFrom(),\n+                                                       rewriter, srcTensorType);\n+    inVals =\n+        unpackI32(inVals, srcTensorType, rewriter, loc, getTypeConverter());\n+    // Cast\n+    SmallVector<Value> outVals;\n+    auto elems = inVals.size();\n     assert(elems % 4 == 0 &&\n            \"FP8 casting only support tensors with 4-aligned sizes\");\n-    auto elements = getTypeConverter()->unpackLLElements(\n-        loc, adaptor.getFrom(), rewriter, srcTensorType);\n-    for (size_t i = 0; i < elems; i += 4) {\n-      auto converted = convertor(loc, rewriter, elements[i], elements[i + 1],\n-                                 elements[i + 2], elements[i + 3]);\n-      resultVals.append(converted);\n-    }\n-\n-    assert(resultVals.size() == elems);\n-    auto result = getTypeConverter()->packLLElements(loc, resultVals, rewriter,\n+    for (size_t i = 0; i < elems; i += 4)\n+      outVals.append(cvtFunc(loc, rewriter, inVals[i], inVals[i + 1],\n+                             inVals[i + 2], inVals[i + 3]));\n+    // Pack values\n+    assert(outVals.size() == elems);\n+    outVals = reorderValues(outVals, srcTensorType, dstTensorType);\n+    outVals =\n+        packI32(outVals, dstTensorType, rewriter, loc, getTypeConverter());\n+    auto result = getTypeConverter()->packLLElements(loc, outVals, rewriter,\n                                                      dstTensorType);\n     rewriter.replaceOp(op, result);\n     return success();\n@@ -582,43 +708,44 @@ class ElementwiseOpConversionBase\n                   ConversionPatternRewriter &rewriter) const override {\n     auto resultTy = op.getType();\n     Location loc = op->getLoc();\n-\n-    unsigned elems = getTotalElemsPerThread(resultTy);\n+    // element type\n     auto resultElementTy = getElementTypeOrSelf(resultTy);\n     Type elemTy = this->getTypeConverter()->convertType(resultElementTy);\n-    SmallVector<Type> types(elems, elemTy);\n-    Type structTy = this->getTypeConverter()->convertType(resultTy);\n-\n-    auto *concreteThis = static_cast<const ConcreteT *>(this);\n-    auto operands = getOperands(rewriter, adaptor, resultTy, elems, loc);\n-    SmallVector<Value> resultVals(elems);\n-    for (unsigned i = 0; i < elems; ++i) {\n-      resultVals[i] = concreteThis->createDestOp(op, adaptor, rewriter, elemTy,\n-                                                 operands[i], loc);\n-      if (!bool(resultVals[i]))\n+    SmallVector<Value> resultVals;\n+    //\n+    SmallVector<SmallVector<Value>> allOperands;\n+    for (auto operand : adaptor.getOperands()) {\n+      auto argTy = op->getOperand(0).getType();\n+      auto sub_operands = this->getTypeConverter()->unpackLLElements(\n+          loc, operand, rewriter, argTy);\n+      sub_operands = unpackI32(sub_operands, argTy, rewriter, loc,\n+                               this->getTypeConverter());\n+      allOperands.resize(sub_operands.size());\n+      for (auto v : llvm::enumerate(sub_operands))\n+        allOperands[v.index()].push_back(v.value());\n+    }\n+    if (allOperands.size() == 0)\n+      allOperands.push_back({});\n+    for (const SmallVector<Value> &operands : allOperands) {\n+      Value curr =\n+          ((ConcreteT *)(this))\n+              ->createDestOp(op, adaptor, rewriter, elemTy, operands, loc);\n+      if (!bool(curr))\n         return failure();\n+      resultVals.push_back(curr);\n+    }\n+    if (op->getNumOperands() > 0) {\n+      auto argTy = op->getOperand(0).getType();\n+      resultVals = reorderValues(resultVals, argTy, resultTy);\n     }\n+    resultVals =\n+        packI32(resultVals, resultTy, rewriter, loc, this->getTypeConverter());\n     Value view = this->getTypeConverter()->packLLElements(loc, resultVals,\n                                                           rewriter, resultTy);\n     rewriter.replaceOp(op, view);\n \n     return success();\n   }\n-\n-protected:\n-  SmallVector<SmallVector<Value>>\n-  getOperands(ConversionPatternRewriter &rewriter, OpAdaptor adaptor,\n-              Type operandTy, const unsigned elems, Location loc) const {\n-    SmallVector<SmallVector<Value>> operands(elems);\n-    for (auto operand : adaptor.getOperands()) {\n-      auto sub_operands = this->getTypeConverter()->unpackLLElements(\n-          loc, operand, rewriter, operandTy);\n-      for (size_t i = 0; i < elems; ++i) {\n-        operands[i].push_back(sub_operands[i]);\n-      }\n-    }\n-    return operands;\n-  }\n };\n \n template <typename SourceOp, typename DestOp>"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "modified", "additions": 2, "deletions": 11, "changes": 13, "file_content_changes": "@@ -106,17 +106,8 @@ Type TritonGPUToLLVMTypeConverter::getElementTypeForStruct(\n     return elemTy;\n   if (mmaParent.isAmpere()) {\n     int bitwidth = elemTy.getIntOrFloatBitWidth();\n-    // sub-word integer types need to be packed for perf reasons\n-    if (elemTy.isa<IntegerType>() && bitwidth < 32)\n-      return IntegerType::get(ctx, 32);\n-    // TODO: unify everything to use packed integer-types\n-    // otherwise, vector types are ok\n-    const llvm::DenseMap<int, Type> elemTyMap = {\n-        {32, vec_ty(elemTy, 1)},\n-        {16, vec_ty(elemTy, 2)},\n-        {8, vec_ty(elemTy, 4)},\n-    };\n-    return elemTyMap.lookup(bitwidth);\n+    assert(bitwidth <= 32);\n+    return IntegerType::get(ctx, 32);\n   } else {\n     assert(mmaParent.isVolta());\n     return vec_ty(elemTy, 2);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -80,6 +80,7 @@\n #define call(...) rewriter.create<LLVM::CallOp>(loc, __VA_ARGS__)\n \n // Types\n+#define int_ty(width) rewriter.getIntegerType(width)\n #define i64_ty rewriter.getIntegerType(64)\n #define i32_ty rewriter.getIntegerType(32)\n #define i16_ty rewriter.getIntegerType(16)"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 71, "deletions": 0, "changes": 71, "file_content_changes": "@@ -72,6 +72,76 @@ class ConvertTransConvert : public mlir::RewritePattern {\n   }\n };\n \n+//\n+\n+class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n+\n+public:\n+  MoveOpAfterLayoutConversion(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n+                             1, context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto srcTy = cvt.getOperand().getType().cast<RankedTensorType>();\n+    auto retTy = cvt.getResult().getType().dyn_cast<RankedTensorType>();\n+    auto retEncoding =\n+        retTy.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+    auto srcEncoding =\n+        srcTy.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>();\n+    if (!retTy)\n+      return failure();\n+    if (!retEncoding)\n+      return failure();\n+    auto retEncodingParent =\n+        retEncoding.getParent().dyn_cast<triton::gpu::MmaEncodingAttr>();\n+    if (!retEncodingParent || retEncodingParent.isVolta())\n+      return failure();\n+    if (!srcEncoding)\n+      return failure();\n+    // don't move things around when cvt operand is a block arg\n+    Operation *argOp = cvt.getOperand().getDefiningOp();\n+    if (!argOp)\n+      return failure();\n+    //\n+    SetVector<Operation *> processed;\n+    SetVector<Attribute> layout;\n+    llvm::MapVector<Value, Attribute> toConvert;\n+    int numCvts = simulateBackwardRematerialization(cvt, processed, layout,\n+                                                    toConvert, retEncoding);\n+    if (numCvts > 1 || toConvert.size() == 1)\n+      return failure();\n+    for (Operation *op : processed) {\n+      if (op->getNumOperands() != 1)\n+        continue;\n+      auto srcTy = op->getOperand(0).getType().cast<RankedTensorType>();\n+      auto dstTy = op->getResult(0).getType().cast<RankedTensorType>();\n+      // we don't want to push conversions backward if there is a downcast\n+      // since it would result in more shared memory traffic\n+      if (srcTy.getElementType().getIntOrFloatBitWidth() >\n+          dstTy.getElementType().getIntOrFloatBitWidth())\n+        return failure();\n+      // we only push back when the first op in the chain has a load operand\n+      if ((op == processed.back()) &&\n+          !isa<triton::LoadOp>(op->getOperand(0).getDefiningOp()))\n+        return failure();\n+      // we don't want to use ldmatrix for 8-bit data that requires trans\n+      // since Nvidia GPUs can't do it efficiently\n+      bool isTrans =\n+          (retEncoding.getOpIdx() == 1) ^ (srcEncoding.getOrder()[0] == 0);\n+      bool isInt8 = srcTy.getElementType().getIntOrFloatBitWidth() == 8;\n+      if (isTrans && isInt8)\n+        return failure();\n+    }\n+    IRMapping mapping;\n+    rematerializeConversionChain(toConvert, rewriter, processed, mapping);\n+    rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n+    return mlir::success();\n+  }\n+};\n+\n } // namespace\n \n #define GEN_PASS_CLASSES\n@@ -93,6 +163,7 @@ class TritonGPUOptimizeDotOperandsPass\n \n     mlir::RewritePatternSet patterns(context);\n     patterns.add<ConvertTransConvert>(context);\n+    patterns.add<MoveOpAfterLayoutConversion>(context);\n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n       signalPassFailure();\n     if (fixupLoops(m).failed())"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -571,12 +571,15 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   // 2. clone the loop body, replace original args with args of the new ForOp\n   // Insert async wait if necessary.\n+  DenseSet<Value> isModified;\n   for (Operation &op : forOp.getBody()->without_terminator()) {\n+    // is modified\n     auto it = std::find(loads.begin(), loads.end(), op.getOperand(0));\n     if (it == loads.end()) {\n-      Operation *newOp = builder.clone(op, mapping);\n+      Operation *newOp = cloneWithInferType(builder, &op, mapping);\n       continue;\n     }\n+\n     // we replace the use new load use with a convert layout\n     size_t i = std::distance(loads.begin(), it);\n     auto cvtDstTy = op.getResult(0).getType().cast<RankedTensorType>();\n@@ -590,6 +593,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n         op.getResult(0).getLoc(), newDstTy,\n         newForOp.getRegionIterArgs()[loadIdx + i]);\n     mapping.map(op.getResult(0), cvt.getResult());\n+    isModified.insert(op.getResult(0));\n   }\n \n   // 3. prefetch the next iteration"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 74, "deletions": 15, "changes": 89, "file_content_changes": "@@ -45,7 +45,7 @@ class Prefetcher {\n   scf::YieldOp yieldOp;\n   ///\n   // TODO: add a hook to infer prefetchWidth\n-  unsigned prefetchWidth = 16;\n+  unsigned prefetchWidth = 32;\n \n   /// dots to be prefetched\n   SetVector<Value> dots;\n@@ -56,6 +56,8 @@ class Prefetcher {\n   DenseMap<Value, Value> dot2bHeaderDef;\n   DenseMap<Value, Value> dot2aYield;\n   DenseMap<Value, Value> dot2bYield;\n+  DenseMap<Value, SmallVector<Value>> dot2aVals;\n+  DenseMap<Value, SmallVector<Value>> dot2bVals;\n   /// operand => defining\n   DenseMap<Value, Value> operand2headPrefetch;\n \n@@ -66,6 +68,9 @@ class Prefetcher {\n                          std::optional<int64_t> offsetK = std::nullopt,\n                          std::optional<int64_t> shapeK = std::nullopt);\n \n+  void cloneElementwiseOps(Value &bRem, const SmallVector<Value> &vals,\n+                           OpBuilder &builder);\n+\n public:\n   Prefetcher() = delete;\n \n@@ -80,6 +85,24 @@ class Prefetcher {\n   scf::ForOp createNewForOp();\n };\n \n+void Prefetcher::cloneElementwiseOps(Value &ret, const SmallVector<Value> &vals,\n+                                     OpBuilder &builder) {\n+  IRMapping mapping;\n+  mapping.map(vals[0], ret);\n+  for (int i = 1; i < vals.size(); i++) {\n+    Value v = vals[i];\n+    Value curr = builder.clone(*v.getDefiningOp(), mapping)->getResult(0);\n+    auto retType = RankedTensorType::get(\n+        ret.getType().cast<RankedTensorType>().getShape(),\n+        curr.getType().cast<RankedTensorType>().getElementType(),\n+        curr.getType().cast<RankedTensorType>().getEncoding());\n+    curr.setType(retType);\n+    mapping.map(v, curr);\n+  }\n+  if (vals.size() > 1)\n+    ret = mapping.lookup(vals.back());\n+}\n+\n Value Prefetcher::generatePrefetch(Value v, unsigned opIdx, bool isPrologue,\n                                    Attribute dotEncoding, OpBuilder &builder,\n                                    std::optional<int64_t> offsetK,\n@@ -135,11 +158,32 @@ LogicalResult Prefetcher::initialize() {\n     return failure();\n \n   // returns source of cvt\n-  auto getPrefetchSrc = [](Value v) -> Value {\n-    if (auto cvt = v.getDefiningOp<triton::gpu::ConvertLayoutOp>())\n-      if (isSharedEncoding(cvt.getOperand()))\n-        return cvt.getSrc();\n-    return Value();\n+\n+  // returns source of cvt\n+  auto getPrefetchSrc = [](Value v) -> SmallVector<Value> {\n+    // walk back to conversion\n+    Operation *op = v.getDefiningOp();\n+    bool foundConvertFromShared = false;\n+    SmallVector<Value> rets;\n+    rets.push_back(op->getResult(0));\n+    while (op) {\n+      if (op->getNumOperands() != 1)\n+        break;\n+      if (!op->getResult(0).hasOneUse())\n+        break;\n+      rets.push_back(op->getOperand(0));\n+      if (auto cvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(op))\n+        if (isSharedEncoding(cvt.getOperand())) {\n+          foundConvertFromShared = true;\n+          break;\n+        }\n+      op = op->getOperand(0).getDefiningOp();\n+    }\n+    std::reverse(rets.begin(), rets.end());\n+\n+    if (foundConvertFromShared)\n+      return rets;\n+    return {};\n   };\n \n   auto getIncomingOp = [this](Value v) -> Value {\n@@ -176,14 +220,19 @@ LogicalResult Prefetcher::initialize() {\n     // Skip prefetching if kSize is less than prefetchWidth\n     if (kSize < prefetchWidth)\n       continue;\n-    Value aSmem = getPrefetchSrc(dot.getA());\n-    Value bSmem = getPrefetchSrc(dot.getB());\n-    if (aSmem && bSmem) {\n+    auto aVals = getPrefetchSrc(dot.getA());\n+    auto bVals = getPrefetchSrc(dot.getB());\n+\n+    if (aVals.size() && bVals.size()) {\n+      Value aSmem = aVals.front();\n+      Value bSmem = bVals.front();\n       Value aHeaderDef = getIncomingOp(aSmem);\n       Value bHeaderDef = getIncomingOp(bSmem);\n       // Only prefetch loop arg\n       if (aHeaderDef && bHeaderDef) {\n         dots.insert(dot);\n+        dot2aVals[dot] = aVals;\n+        dot2bVals[dot] = bVals;\n         dot2aHeaderDef[dot] = aHeaderDef;\n         dot2bHeaderDef[dot] = bHeaderDef;\n         dot2aLoopArg[dot] = aSmem;\n@@ -205,10 +254,13 @@ void Prefetcher::emitPrologue() {\n         dot.getType().cast<RankedTensorType>().getEncoding();\n     Value aPrefetched =\n         generatePrefetch(dot2aHeaderDef[dot], 0, true, dotEncoding, builder);\n-    operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().getA()] =\n-        aPrefetched;\n+    cloneElementwiseOps(aPrefetched, dot2aVals[dot], builder);\n     Value bPrefetched =\n         generatePrefetch(dot2bHeaderDef[dot], 1, true, dotEncoding, builder);\n+    cloneElementwiseOps(bPrefetched, dot2bVals[dot], builder);\n+\n+    operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().getA()] =\n+        aPrefetched;\n     operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().getB()] =\n         bPrefetched;\n   }\n@@ -266,9 +318,11 @@ scf::ForOp Prefetcher::createNewForOp() {\n         Value aRem =\n             generatePrefetch(mapping.lookup(dot2aLoopArg[dot]), 0, false,\n                              dotEncoding, builder, kOff, kShape);\n+        cloneElementwiseOps(aRem, dot2aVals[dot], builder);\n         Value bRem =\n             generatePrefetch(mapping.lookup(dot2bLoopArg[dot]), 1, false,\n                              dotEncoding, builder, kOff, kShape);\n+        cloneElementwiseOps(bRem, dot2bVals[dot], builder);\n         builder.restoreInsertionPoint(insertionPoint);\n         newOp = builder.clone(*dot, mapping);\n         newOp->setOperand(0, aRem);\n@@ -291,10 +345,15 @@ scf::ForOp Prefetcher::createNewForOp() {\n   for (Value dot : dots) {\n     Attribute dotEncoding =\n         dot.getType().cast<RankedTensorType>().getEncoding();\n-    yieldValues.push_back(generatePrefetch(mapping.lookup(dot2aYield[dot]), 0,\n-                                           true, dotEncoding, builder));\n-    yieldValues.push_back(generatePrefetch(mapping.lookup(dot2bYield[dot]), 1,\n-                                           true, dotEncoding, builder));\n+    Value aToYield = generatePrefetch(mapping.lookup(dot2aYield[dot]), 0, true,\n+                                      dotEncoding, builder);\n+    cloneElementwiseOps(aToYield, dot2aVals[dot], builder);\n+    yieldValues.push_back(aToYield);\n+    // bToYield\n+    Value bToYield = generatePrefetch(mapping.lookup(dot2bYield[dot]), 1, true,\n+                                      dotEncoding, builder);\n+    cloneElementwiseOps(bToYield, dot2bVals[dot], builder);\n+    yieldValues.push_back(bToYield);\n   }\n   // Update ops of yield\n   if (!yieldValues.empty())"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -209,6 +209,15 @@ int simulateBackwardRematerialization(\n Operation *cloneWithInferType(mlir::OpBuilder &rewriter, Operation *op,\n                               IRMapping &mapping) {\n   Operation *newOp = rewriter.clone(*op, mapping);\n+  // if input types haven't changed, we're done\n+  bool preserveTypes =\n+      std::all_of(op->operand_begin(), op->operand_end(), [&](Value v) {\n+        return !mapping.contains(v) ||\n+               v.getType() == mapping.lookup(v).getType();\n+      });\n+  if (preserveTypes)\n+    return newOp;\n+\n   if (newOp->getNumResults() == 0)\n     return newOp;\n   auto origType = op->getResult(0).getType().dyn_cast<RankedTensorType>();"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -2322,6 +2322,33 @@ def kernel(ExitEarly, Out):\n     assert to_numpy(out)[0] == 1\n \n \n+@triton.jit\n+def add_fn(x):\n+    return x + 1\n+\n+\n+@pytest.mark.parametrize(\"call_type\", [\"attribute\", \"jit_function\"])\n+def test_if_call(call_type):\n+    @triton.jit\n+    def kernel(Out, call_type: tl.constexpr):\n+        pid = tl.program_id(0)\n+        o = tl.load(Out)\n+        if pid == 0:\n+            if call_type == \"attribute\":\n+                a = o + 1\n+                a = a.to(tl.int32)\n+                o = a\n+            else:\n+                a = o\n+                a = add_fn(a)\n+                o = a\n+        tl.store(Out, o)\n+\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    kernel[(1,)](out, call_type)\n+    assert to_numpy(out)[0] == 1\n+\n+\n @pytest.mark.parametrize(\"_cond1\", [True, False])\n @pytest.mark.parametrize(\"_cond2\", [True, False])\n @pytest.mark.parametrize(\"_cond3\", [True, False])"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -168,6 +168,8 @@ def contains_return_op(self, node):\n             pred = lambda s: self.contains_return_op(s)\n             return any(pred(s) for s in node.body)\n         elif isinstance(node, ast.Call):\n+            if isinstance(node.func, ast.Attribute):\n+                return False\n             fn = self.visit(node.func)\n             if isinstance(fn, JITFunction):\n                 old_gscope = self.gscope"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -251,7 +251,7 @@ def convert_type_repr(x):\n     return x\n \n \n-def make_hash(fn, **kwargs):\n+def make_hash(fn, arch, **kwargs):\n     if isinstance(fn, triton.runtime.JITFunction):\n         configs = kwargs[\"configs\"]\n         signature = kwargs[\"signature\"]\n@@ -262,7 +262,7 @@ def make_hash(fn, **kwargs):\n         # Get unique key for the compiled code\n         get_conf_key = lambda conf: (sorted(conf.divisible_by_16), sorted(conf.equal_to_1))\n         configs_key = [get_conf_key(conf) for conf in configs]\n-        key = f\"{fn.cache_key}-{''.join(signature.values())}-{configs_key}-{constants}-{num_warps}-{num_stages}-{debug}\"\n+        key = f\"{fn.cache_key}-{''.join(signature.values())}-{configs_key}-{constants}-{num_warps}-{num_stages}-{debug}-{arch}\"\n         return hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n     assert isinstance(fn, str)\n     return hashlib.md5((Path(fn).read_text() + triton.runtime.jit.version_key()).encode(\"utf-8\")).hexdigest()\n@@ -418,7 +418,7 @@ def compile(fn, **kwargs):\n     # cache manager\n     so_path = make_stub(name, signature, constants)\n     # create cache manager\n-    fn_cache_manager = get_cache_manager(make_hash(fn, **kwargs))\n+    fn_cache_manager = get_cache_manager(make_hash(fn, arch, **kwargs))\n     # determine name and extension type of provided function\n     if isinstance(fn, triton.runtime.JITFunction):\n         name, ext = fn.__name__, \"ast\""}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -978,10 +978,10 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    // CHECK-SAME: (vector<1xf32>, vector<1xf32>, vector<1xf32>, vector<1xf32>)\n+    // CHECK-SAME: (i32, i32, i32, i32)\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    // CHECK-SAME: (vector<1xf32>, vector<1xf32>, vector<1xf32>, vector<1xf32>)\n+    // CHECK-SAME: (i32, i32, i32, i32)\n     %a_mat = triton_gpu.convert_layout %a : (tensor<32x16xf32, #shared>) -> tensor<32x16xf32, #dot_operand_a>\n     %b_mat = triton_gpu.convert_layout %b : (tensor<16x32xf32, #shared>) -> tensor<16x32xf32, #dot_operand_b>\n "}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "added", "additions": 52, "deletions": 0, "changes": 52, "file_content_changes": "@@ -0,0 +1,52 @@\n+// RUN: triton-opt %s -split-input-file -tritongpu-optimize-dot-operands -tritongpu-remove-layout-conversions -canonicalize | FileCheck %s\n+\n+#Cv2 = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n+#Av2 = #triton_gpu.dot_op<{opIdx = 0, parent = #Cv2, kWidth=2}>\n+#Bv2 = #triton_gpu.dot_op<{opIdx = 1, parent = #Cv2, kWidth=2}>\n+#Cv1 = #triton_gpu.mma<{versionMajor = 1, warpsPerCTA = [4, 1]}>\n+#Av1 = #triton_gpu.dot_op<{opIdx = 0, parent = #Cv1}>\n+#Bv1 = #triton_gpu.dot_op<{opIdx = 1, parent = #Cv1}>\n+#AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n+\n+// CHECK: tt.func @push_elementwise1\n+// CHECK: %[[ALOAD:.*]] = tt.load %arg0\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]]\n+// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ACVT]]\n+// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n+// CHECK: %[[C:.*]] = tt.dot %[[AF16]]\n+// CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma>\n+tt.func @push_elementwise1(\n+                   %pa: tensor<16x16x!tt.ptr<i8>, #AL> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #BL> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %c: tensor<16x16xf32, #Cv2>) -> tensor<16x16xf32, #Cv2>{\n+  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #AL>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BL>\n+  %af8 = tt.bitcast %ai8: tensor<16x16xi8, #AL> -> tensor<16x16xf8E5M2, #AL>\n+  %a = tt.fp_to_fp %af8: tensor<16x16xf8E5M2, #AL> -> tensor<16x16xf16, #AL>\n+  %dota = triton_gpu.convert_layout %a : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #Av2>\n+  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BL>) -> tensor<16x16xf16, #Bv2>\n+  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av2> * tensor<16x16xf16, #Bv2> -> tensor<16x16xf32, #Cv2>\n+  tt.return %newc : tensor<16x16xf32, #Cv2>\n+}\n+\n+// CHECK: tt.func @push_elementwise2\n+// CHECK: %[[ALOAD:.*]] = tt.load %arg0\n+// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ALOAD]]\n+// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[AF16]]\n+// CHECK: %[[C:.*]] = tt.dot %[[ACVT]]\n+// CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma1>\n+tt.func @push_elementwise2(\n+                   %pa: tensor<16x16x!tt.ptr<i8>, #AL> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #BL> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %c: tensor<16x16xf32, #Cv1>) -> tensor<16x16xf32, #Cv1>{\n+  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #AL>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BL>\n+  %af8 = tt.bitcast %ai8: tensor<16x16xi8, #AL> -> tensor<16x16xf8E5M2, #AL>\n+  %a = tt.fp_to_fp %af8: tensor<16x16xf8E5M2, #AL> -> tensor<16x16xf16, #AL>\n+  %dota = triton_gpu.convert_layout %a : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #Av1>\n+  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BL>) -> tensor<16x16xf16, #Bv1>\n+  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av1> * tensor<16x16xf16, #Bv1> -> tensor<16x16xf32, #Cv1>\n+  tt.return %newc : tensor<16x16xf32, #Cv1>\n+}"}, {"filename": "test/TritonGPU/prefetch.mlir", "status": "modified", "additions": 21, "deletions": 17, "changes": 38, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file -tritongpu-prefetch | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -tritongpu-prefetch -canonicalize | FileCheck %s\n \n // 4 warps\n // matmul: 128x32 @ 32x128 -> 128x128\n@@ -11,54 +11,58 @@\n #B_OP = #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 2}>\n \n \n-// CHECK: tt.func @matmul_loop\n+// CHECK: tt.func @matmul_loop_mixed\n // CHECK-DAG: %[[A0_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice %[[A0:.*]][0, 0] [128, 16]\n // CHECK-DAG: %[[A0_PREFETCH:.*]] = triton_gpu.convert_layout %[[A0_PREFETCH_SMEM]]\n+// CHECK-DAG: %[[A0_CVT:.*]] = tt.fp_to_fp %[[A0_PREFETCH]]\n // CHECK-DAG: %[[B0_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice %[[B0:.*]][0, 0] [16, 128]\n // CHECK-DAG: %[[B0_PREFETCH:.*]] = triton_gpu.convert_layout %[[B0_PREFETCH_SMEM]]\n-// CHECK:     scf.for {{.*}} iter_args({{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, %[[a0_prefetch:.*]] = %[[A0_PREFETCH]], %[[b0_prefetch:.*]] = %[[B0_PREFETCH]]\n+// CHECK:     scf.for {{.*}} iter_args({{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, %[[a0_prefetch:.*]] = %[[A0_CVT]], %[[b0_prefetch:.*]] = %[[B0_PREFETCH]]\n // CHECK-DAG:   %[[A_REM_SMEM:.*]] = triton_gpu.extract_slice %[[arg_a0]][0, 16] [128, 16]\n // CHECK-DAG:   %[[A_REM:.*]] = triton_gpu.convert_layout %[[A_REM_SMEM]]\n+// CHECK-DAG:   %[[A_REM_CVT:.*]] = tt.fp_to_fp %[[A_REM]]\n // CHECK-DAG:   %[[B_REM_SMEM:.*]] = triton_gpu.extract_slice %[[arg_b0]][16, 0] [16, 128]\n // CHECK-DAG:   %[[B_REM:.*]] = triton_gpu.convert_layout %[[B_REM_SMEM]]\n // CHECK:       %[[D_FIRST:.*]] = tt.dot %[[a0_prefetch]], %[[b0_prefetch:.*]], {{.*}}\n-// CHECK:       tt.dot %[[A_REM]], %[[B_REM]], %[[D_FIRST:.*]]\n+// CHECK:       tt.dot %[[A_REM_CVT]], %[[B_REM]], %[[D_FIRST:.*]]\n // CHECK-DAG:   %[[NEXT_A_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice {{.*}}[0, 0] [128, 16]\n // CHECK-DAG:   %[[NEXT_A_PREFETCH:.*]] = triton_gpu.convert_layout %[[NEXT_A_PREFETCH_SMEM]]\n+// CHECK-DAG:   %[[NEXT_A_PREFETCH_CVT:.*]] = tt.fp_to_fp %[[NEXT_A_PREFETCH]]\n // CHECK-DAG:   %[[NEXT_B_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice {{.*}}[0, 0] [16, 128]\n // CHECK-DAG:   %[[NEXT_B_PREFETCH:.*]] = triton_gpu.convert_layout %[[NEXT_B_PREFETCH_SMEM]]\n-// CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_PREFETCH]], %[[NEXT_B_PREFETCH]]\n-tt.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n-  %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n+// CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_PREFETCH_CVT]], %[[NEXT_B_PREFETCH]]\n+tt.func @matmul_loop_mixed(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f8E5M2>, %B : !tt.ptr<f16>) -> tensor<128x128xf32, #C>{\n+  %a_ptr_init = tt.broadcast %A : (!tt.ptr<f8E5M2>) -> tensor<128x32x!tt.ptr<f8E5M2>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n \n   %a_mask = arith.constant dense<true> : tensor<128x32xi1, #AL>\n-  %a_other = arith.constant dense<0.00e+00> : tensor<128x32xf16, #AL>\n+  %a_other = arith.constant dense<0.00e+00> : tensor<128x32xf8E5M2, #AL>\n   %b_mask = arith.constant dense<true> : tensor<32x128xi1, #BL>\n   %b_other = arith.constant dense<0.00e+00> : tensor<32x128xf16, #BL>\n   %c_init = arith.constant dense<0.00e+00> : tensor<128x128xf32, #C>\n \n   %a_off = arith.constant dense<4> : tensor<128x32xi32, #AL>\n   %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n \n-  %a_ = tt.load %a_ptr_init, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-  %a_init = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+  %a_ = tt.load %a_ptr_init, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf8E5M2, #AL>\n+  %a_init = triton_gpu.convert_layout %a_ : (tensor<128x32xf8E5M2, #AL>) -> tensor<128x32xf8E5M2, #A>\n   %b_ = tt.load %b_ptr_init, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n   %b_init = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n \n-  scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %a = %a_init, %b = %b_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x32xf16, #A>, tensor<32x128xf16, #B>, tensor<128x128xf32, #C>) {\n-    %a_op = triton_gpu.convert_layout %a : (tensor<128x32xf16, #A>) -> tensor<128x32xf16, #A_OP>\n+  %loop:5 = scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %a = %a_init, %b = %b_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f8E5M2>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x32xf8E5M2, #A>, tensor<32x128xf16, #B>, tensor<128x128xf32, #C>) {\n+    %a_op_ = triton_gpu.convert_layout %a : (tensor<128x32xf8E5M2, #A>) -> tensor<128x32xf8E5M2, #A_OP>\n+    %a_op = tt.fp_to_fp %a_op_ : tensor<128x32xf8E5M2, #A_OP> -> tensor<128x32xf16, #A_OP>\n     %b_op = triton_gpu.convert_layout %b : (tensor<32x128xf16, #B>) -> tensor<32x128xf16, #B_OP>\n     %c = tt.dot %a_op, %b_op, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_OP> * tensor<32x128xf16, #B_OP> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f8E5M2>, #AL>, tensor<128x32xi32, #AL>\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n-    %next_a_ = tt.load %next_a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-    %next_a = triton_gpu.convert_layout %next_a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+    %next_a_ = tt.load %next_a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf8E5M2, #AL>\n+    %next_a = triton_gpu.convert_layout %next_a_ : (tensor<128x32xf8E5M2, #AL>) -> tensor<128x32xf8E5M2, #A>\n     %next_b_ = tt.load %next_b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n     %next_b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n \n-    scf.yield %next_a_ptr, %next_b_ptr, %next_a, %next_b, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x32xf16, #A>, tensor<32x128xf16, #B>, tensor<128x128xf32, #C>\n+    scf.yield %next_a_ptr, %next_b_ptr, %next_a, %next_b, %c : tensor<128x32x!tt.ptr<f8E5M2>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x32xf8E5M2, #A>, tensor<32x128xf16, #B>, tensor<128x128xf32, #C>\n   }\n-  tt.return\n+  tt.return %loop#4 : tensor<128x128xf32, #C>\n }"}]