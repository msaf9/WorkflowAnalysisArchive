[{"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -57,18 +57,18 @@ def nvsmi(attrs):\n         (512, 512, 512): {'float16': 0.08, 'float32': 0.13, 'int8': 0.05},\n         (1024, 1024, 1024): {'float16': 0.33, 'float32': 0.35, 'int8': 0.169},\n         (2048, 2048, 2048): {'float16': 0.64, 'float32': 0.57, 'int8': 0.34},\n-        (4096, 4096, 4096): {'float16': 0.79, 'float32': 0.75, 'int8': 0.46},\n-        (8192, 8192, 8192): {'float16': 0.80, 'float32': 0.85, 'int8': 0.51},\n+        (4096, 4096, 4096): {'float16': 0.82, 'float32': 0.75, 'int8': 0.46},\n+        (8192, 8192, 8192): {'float16': 0.77, 'float32': 0.85, 'int8': 0.51},\n         # tall-skinny\n         (16, 1024, 1024): {'float16': 0.0077, 'float32': 0.0127, 'int8': 0.005},\n         (16, 4096, 4096): {'float16': 0.0363, 'float32': 0.0457, 'int8': 0.0259},\n-        (16, 8192, 8192): {'float16': 0.0564, 'float32': 0.0648, 'int8': 0.0431},\n+        (16, 8192, 8192): {'float16': 0.07, 'float32': 0.0648, 'int8': 0.0431},\n         (64, 1024, 1024): {'float16': 0.0271, 'float32': 0.0509, 'int8': 0.0169},\n-        (64, 4096, 4096): {'float16': 0.141, 'float32': 0.162, 'int8': 0.097},\n-        (64, 8192, 8192): {'float16': 0.244, 'float32': 0.257, 'int8': 0.174},\n+        (64, 4096, 4096): {'float16': 0.16, 'float32': 0.162, 'int8': 0.097},\n+        (64, 8192, 8192): {'float16': 0.30, 'float32': 0.257, 'int8': 0.174},\n         (1024, 64, 1024): {'float16': 0.0263, 'float32': 0.0458, 'int8': 0.017},\n-        (4096, 64, 4096): {'float16': 0.135, 'float32': 0.177, 'int8': 0.102},\n-        (8192, 64, 8192): {'float16': 0.216, 'float32': 0.230, 'int8': 0.177},\n+        (4096, 64, 4096): {'float16': 0.16, 'float32': 0.177, 'int8': 0.102},\n+        (8192, 64, 8192): {'float16': 0.25, 'float32': 0.230, 'int8': 0.177},\n     }\n }\n \n@@ -93,7 +93,7 @@ def test_matmul(M, N, K, dtype_str):\n         a = torch.randn((M, K), dtype=dtype, device='cuda')\n         b = torch.randn((K, N), dtype=dtype, device='cuda')\n     fn = lambda: triton.ops.matmul(a, b)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=1000)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)\n@@ -149,7 +149,7 @@ def test_elementwise(N):\n     y = torch.randn_like(z)\n     grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n     fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=1000)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)"}]