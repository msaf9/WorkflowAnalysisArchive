[{"filename": "bin/RegisterTritonDialects.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -6,6 +6,7 @@\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n \n #include \"triton/Conversion/TritonGPUToLLVM/Passes.h\"\n+#include \"triton/Conversion/TritonToLinalg/Passes.h\"\n #include \"triton/Conversion/TritonToTritonGPU/Passes.h\"\n \n #include \"mlir/InitAllPasses.h\"\n@@ -27,6 +28,7 @@ inline void registerTritonDialects(mlir::DialectRegistry &registry) {\n   mlir::test::registerTestAlignmentPass();\n   mlir::test::registerTestAllocationPass();\n   mlir::test::registerTestMembarPass();\n+  mlir::triton::registerTritonToLinalgPass();\n   mlir::triton::registerConvertTritonToTritonGPUPass();\n   mlir::triton::registerConvertTritonGPUToLLVMPass();\n "}, {"filename": "include/triton/Analysis/MaskAnalysis.h", "status": "added", "additions": 135, "deletions": 0, "changes": 135, "file_content_changes": "@@ -0,0 +1,135 @@\n+//===----------------------------------------------------------------------===//\n+//\n+// Copyright (c) Triton Project Contributors.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#ifndef TRITON_ANALYSIS_MASKANALYSIS_H\n+#define TRITON_ANALYSIS_MASKANALYSIS_H\n+\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+\n+namespace mlir {\n+\n+class ConversionPatternRewriter;\n+\n+namespace triton {\n+// Data structure used to decode the pattern in a mask used for load and store.\n+// start and end field represent the start and end index of a range (produced\n+// by make_range, addi, etc.). While multi-dimensional data is possible, we\n+// assume range comparison can only be done on 1 dimension at a time (and\n+// results of range comparions across dimensions can be combined), hence start\n+// and end are not vectors. dims represents the real access size for ld/st\n+// (instead of the tensor/memref size specified by the IR). scalar is a shortcut\n+// used when the entire state contains a single scalar value.\n+//\n+// The general lifetime of this data structure is roughly:\n+// 1. A range is created by make_range and optionally operated on by addi w/\n+// result of splat, expand_dims, etc. During this phase, either (1) both start\n+// and end are populated, or (2) scalar is populated. Only one of the dimensions\n+// (that contains the range) can have dim > 1.\n+// 2. Result from step 1 is compared with a another MaskState that represents a\n+// scalar value. The resulting state only has dims populated.\n+// 3. Optionally, result from step 2 can be broadcasted and anded with other\n+// results from step 2. The resulting state only has dims populated.\n+//\n+// Example of creating 2D mask:\n+//  mask = (rows[:, None] < M) & (cols[None, :] < N)\n+struct MaskState {\n+  OpFoldResult start;\n+  OpFoldResult end;\n+  SmallVector<OpFoldResult> dims;\n+  OpFoldResult scalar;\n+\n+  int64_t getRank() const { return dims.size(); }\n+\n+  bool isEmpty() const { return getRank() == 0 && !scalar && !start && !end; }\n+\n+  bool isMask() const { return !start && !end && !scalar && dims.size() != 0; }\n+\n+  // Recursively parse a Value; call the coresponding function based on the\n+  // defining operation and Value type\n+  LogicalResult parse(Value operand, const Location loc,\n+                      ConversionPatternRewriter &rewriter);\n+\n+  tensor::ExtractSliceOp\n+  getExtractSlice(Value source, const Location loc,\n+                  ConversionPatternRewriter &rewriter) const;\n+\n+  memref::SubViewOp getSubview(Value source, const Location loc,\n+                               ConversionPatternRewriter &rewriter) const;\n+\n+private:\n+  // -------\n+  // Utility functions to operate on MaskState\n+  // -------\n+  LogicalResult addStateScalar(const MaskState &state,\n+                               const OpFoldResult scalar, Location loc,\n+                               ConversionPatternRewriter &rewriter);\n+\n+  LogicalResult addStates(const MaskState &lhsState, const MaskState &rhsState,\n+                          Location loc, ConversionPatternRewriter &rewriter);\n+\n+  LogicalResult minStates(const MaskState &lhsState, const MaskState &rhsState,\n+                          Location loc, ConversionPatternRewriter &rewriter);\n+  // -------\n+  // Helper functions to parse values to populate MaskState\n+  // -------\n+\n+  // Operand is the result of a constant\n+  // Get the value of the constant and assign it to scalar.\n+  LogicalResult parseConstant(arith::ConstantOp constOp, const Location loc,\n+                              ConversionPatternRewriter &rewriter);\n+\n+  // Operand is an integer scalar\n+  LogicalResult parseIntScalar(Value scalar, const Location loc,\n+                               ConversionPatternRewriter &rewriter);\n+\n+  // Operand is the result of addi\n+  // One and only one of the operands should be a scalar. Increment both start\n+  // and end, dims remains unchanged, and scalar is empty.\n+  LogicalResult parseAdd(arith::AddIOp addOp, const Location loc,\n+                         ConversionPatternRewriter &rewriter);\n+  // Operand is the result of andi\n+  // Each of the result state dims is smaller of the two operands' dims.\n+  // Insert instruction if needed to get new dims.\n+  LogicalResult parseAnd(arith::AndIOp andOp, const Location loc,\n+                         ConversionPatternRewriter &rewriter);\n+\n+  // Operand is the result of cmpi\n+  // Assume only of the dimensions have size > 1. Only support slt for now.\n+  // For that dimension, calculate this new dim as: dim = min(end, value) -\n+  // start\n+  LogicalResult parseCmp(arith::CmpIOp cmpOp, const Location loc,\n+                         ConversionPatternRewriter &rewriter);\n+  // Operand is the result of make_range\n+  // Set start and end accordingly; step size must be 1.\n+  LogicalResult parseMakeRange(triton::MakeRangeOp rangeOp, const Location loc,\n+                               ConversionPatternRewriter &rewriter);\n+  // Operand is the result of broadcast\n+  // Change dims only; assume only applies to tensors.\n+  LogicalResult parseBroadcast(triton::BroadcastOp broadcastOp,\n+                               const Location loc,\n+                               ConversionPatternRewriter &rewriter);\n+  // Operand is the result of splat\n+  // Assume only applies to scalar. start and end are left empty; scalar will\n+  // be assigned, and dims will be updated.\n+  LogicalResult parseSplat(triton::SplatOp splatOp, const Location loc,\n+                           ConversionPatternRewriter &rewriter);\n+  // Operand is the result of expand_dims\n+  // Insert additional dims; start and end do not change and correspond to the\n+  // dimension that contains the range.\n+  LogicalResult parseExpandDims(triton::ExpandDimsOp expandDimsOp,\n+                                const Location loc,\n+                                ConversionPatternRewriter &rewriter);\n+};\n+\n+} // namespace triton\n+\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Analysis/OpFoldResultUtils.h", "status": "added", "additions": 48, "deletions": 0, "changes": 48, "file_content_changes": "@@ -0,0 +1,48 @@\n+//===----------------------------------------------------------------------===//\n+//\n+// Copyright (c) Triton Project Contributors.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#ifndef TRITON_ANALYSIS_OPFOLDRESULT_UTILS_H\n+#define TRITON_ANALYSIS_OPFOLDRESULT_UTILS_H\n+\n+#include \"mlir/IR/Location.h\"\n+#include \"mlir/IR/OpDefinition.h\"\n+\n+#include <optional>\n+\n+namespace mlir {\n+\n+class ConversionPatternRewriter;\n+\n+// Return integer if ofr is an IntegerAttr. Note that this function differs\n+// from getConstantIntValue, which returns an integer if ofr is the constant\n+// result of an operation too.\n+std::optional<int64_t> getIntAttr(const OpFoldResult ofr);\n+\n+// Process addition of two OFRs. If both OFRs are Integer Attributes, result\n+// is an Integer Attribute. Otherwise, insert the arith.addi instruction if\n+// needed and use its result Value.\n+OpFoldResult addOFRs(const OpFoldResult lhs, const OpFoldResult rhs,\n+                     const Location loc, ConversionPatternRewriter &rewriter);\n+\n+// Produce result = lhs - rhs. If both OFRs are Integer Attributes, result\n+// is an Integer Attribute. Otherwise, insert the arith.addi instruction if\n+// needed and use its result Value.\n+OpFoldResult subOFRs(const OpFoldResult lhs, const OpFoldResult rhs,\n+                     const Location loc, ConversionPatternRewriter &rewriter);\n+\n+// Process multiplication of two OFRs. If both OFRs are Integer Attributes,\n+// result is an Integer Attribtue. Otherwise, insert the arith.muli\n+// instruction if needed and use its result Value.\n+OpFoldResult mulOFRValue(const OpFoldResult lhs, const Value rhs,\n+                         const Location loc,\n+                         ConversionPatternRewriter &rewriter);\n+\n+OpFoldResult minOFRs(const OpFoldResult lhs, const OpFoldResult rhs,\n+                     const Location loc, ConversionPatternRewriter &rewriter);\n+\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Analysis/PtrAnalysis.h", "status": "added", "additions": 206, "deletions": 0, "changes": 206, "file_content_changes": "@@ -0,0 +1,206 @@\n+//===----------------------------------------------------------------------===//\n+//\n+// Copyright (c) Triton Project Contributors.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#ifndef TRITON_ANALYSIS_PTRANALYSIS_H\n+#define TRITON_ANALYSIS_PTRANALYSIS_H\n+\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n+\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+\n+#include <set>\n+\n+namespace mlir {\n+\n+class ConversionPatternRewriter;\n+\n+namespace triton {\n+\n+// Data structure used to decode pointer arithmetics and potentially to be\n+// translate it into memref. offsets, sizes, and strides are in unit of elements\n+// in a linearly laid-out memory, which is the same as pointer arithmetic\n+// operations in Triton language. scalar is a shortcut used when the entire\n+// state describes a single scalar value. source is the base pointer.\n+struct PtrState {\n+  SmallVector<OpFoldResult> offsets;\n+  SmallVector<OpFoldResult> sizes;\n+  SmallVector<OpFoldResult> strides;\n+  Value source;\n+  Value scalar;\n+\n+  int64_t getRank() const;\n+\n+  bool isEmpty() const;\n+\n+  // Process addition of two PtrStates.\n+  void addState(const PtrState &lhsState, const PtrState &rhsState,\n+                Location loc, ConversionPatternRewriter &rewriter);\n+\n+  // Process multiplication of two PtrStates\n+  void mulState(const PtrState &lhsState, const PtrState &rhsState,\n+                const Location loc, ConversionPatternRewriter &rewriter);\n+\n+  // Produce a reinterpret cast based on the current PtrState. Additional\n+  // instructions may be inserted in calculating the final offset.\n+  memref::ReinterpretCastOp createCastOp(ArrayRef<int64_t> resultShape,\n+                                         const Location loc,\n+                                         ConversionPatternRewriter &rewriter);\n+};\n+\n+class PtrAnalysis {\n+public:\n+  using IndexMapSet = std::map<int, std::set<int>>;\n+\n+  // Recursively parse a Value; call the corresponding\n+  // function based on the defining operation and argument type.\n+  static void\n+  visitOperand(Value operand, PtrState &state, const Location loc,\n+               ConversionPatternRewriter &rewriter,\n+               const llvm::SmallDenseMap<Value, PtrState> &knownPtrs);\n+\n+  // Operand is the result of arith.addi. Process both arguments and insert any\n+  // arith.addi instruction as needed.\n+  // Main assumptions:\n+  //  Only one of lhsState and rhsState has source field set\n+  //  Current PtrState should be empty\n+  // Expected result:\n+  //  source = lhsState.source ? lhsState.source : rhsState.source\n+  //  sizes[i] = lhsState.sizes[i] (which should match rhsState.sizes[i])\n+  //  offsets[i] = lhsState.offsets[i] + rhsState.offsets[i]\n+  //  strides[i] = lhsState.strides[i] + rhsState.strides[i]\n+  static void\n+  visitOperandAdd(arith::AddIOp addOp, PtrState &state, const Location loc,\n+                  ConversionPatternRewriter &rewriter,\n+                  const llvm::SmallDenseMap<Value, PtrState> &knownPtrs);\n+\n+  // Operand is the result of arith.muli. Process both arguments and insert any\n+  // arith.muli instruction as needed.\n+  // Main assumptions:\n+  //  Neither lhsState nor rhsState has source field set\n+  //  Current PtrState should be empty\n+  //  Currently only support one of the operand is a scalar index\n+  // Expected result (scalar and tensorState represent the two operands):\n+  //  source = null\n+  //  sizes[i] = tensorState.sizes[i]\n+  //  offsets[i] = tensorState.offsets[i] * scalar\n+  //  strides[i] = tensorState.strides[i] * scalar\n+  static void\n+  visitOperandMul(arith::MulIOp mulOp, PtrState &state, const Location loc,\n+                  ConversionPatternRewriter &rewriter,\n+                  const llvm::SmallDenseMap<Value, PtrState> &knownPtrs);\n+\n+  // Operand is the result of make_range.\n+  // Main assumptions:\n+  //  start, end, and shape are all statically known\n+  //  The output of make_range is 1-dimensional\n+  //  Does not check validity of inputs (e.g., stride > 0)\n+  // Expected result:\n+  //  source = null\n+  //  sizes[0] = shape[0]\n+  //  offset[0] = start\n+  //  strides[0] = ceiling( (end - start) / shape[0] )\n+  static void\n+  visitOperandMakeRange(triton::MakeRangeOp rangeOp, PtrState &state,\n+                        Location loc, ConversionPatternRewriter &rewriter,\n+                        const llvm::SmallDenseMap<Value, PtrState> &knownPtrs);\n+\n+  // Operand is the result of expand_dims\n+  // Main assumptions:\n+  //  Only 1 dimension changes for each invocation of reshape\n+  //  The changed dimension must have size of 1\n+  // Expected result:\n+  //  Insert a dimension of size 1, stride 0, and offset 0\n+  static void\n+  visitOperandExpandDims(triton::ExpandDimsOp expandDimsOp, PtrState &state,\n+                         const Location loc,\n+                         ConversionPatternRewriter &rewriter,\n+                         const llvm::SmallDenseMap<Value, PtrState> &knownPtrs);\n+\n+  // Operand is the result of broadcast\n+  // Main assumptions:\n+  //  Rank of soure and result is the same\n+  // Expected result:\n+  //  Update sizes[i] only, no changes to other fields\n+  static void\n+  visitOperandBroadcast(triton::BroadcastOp broadcastOp, PtrState &state,\n+                        const Location loc, ConversionPatternRewriter &rewriter,\n+                        const llvm::SmallDenseMap<Value, PtrState> &knownPtrs);\n+\n+  // Operand is the result of splat\n+  // Main assumptions:\n+  //  Source is a scalar value (i.e., an integer or a pointer, not a tensor)\n+  // Expected result:\n+  //  sizes[i] reflect the shape of the result, strides[i] = 0,  offsets[i] = 0\n+  //  if source is an integer, offset[0] = scalar = source\n+  static void\n+  visitOperandSplat(triton::SplatOp splatOp, PtrState &state,\n+                    const Location loc, ConversionPatternRewriter &rewriter,\n+                    const llvm::SmallDenseMap<Value, PtrState> &knownPtrs);\n+\n+  // Operand is the result of arith.constant that is a splat\n+  // Main assumptions:\n+  //  Source is a constant op that produces a constant dense tensor where all\n+  //  elements are the same (i.e.: a constant that is splatted)\n+  // Expected result:\n+  //  sizes[i] reflect the shape of the result, strides[i] = 0,  offsets[i] =\n+  //  splat value if i == 0, otherwise 0\n+  static void\n+  visitOperandConstSplat(arith::ConstantOp op, PtrState &state,\n+                         const Location loc,\n+                         ConversionPatternRewriter &rewriter,\n+                         const llvm::SmallDenseMap<Value, PtrState> &knownPtrs);\n+\n+  // Operand is the result of addptr.\n+  // Main assumptions:\n+  //  The ptr field should populate the source field\n+  //  ptr and offset fields should result in same rank\n+  // Expected result:\n+  //  The resulting state for ptr and offset wil be added\n+  static void\n+  visitOperandAddptr(triton::AddPtrOp addptrOp, PtrState &state,\n+                     const Location loc, ConversionPatternRewriter &rewriter,\n+                     const llvm::SmallDenseMap<Value, PtrState> &knownPtrs);\n+\n+  // Operand is the result of reinterpret_cast.\n+  // Main assumptions:\n+  //  None\n+  // Expected result:\n+  //  Directly grab all corresponding fields from reinterpret_cast.\n+  static void\n+  visitOperandReintCast(memref::ReinterpretCastOp reintCastOp, PtrState &state,\n+                        const Location loc, ConversionPatternRewriter &rewriter,\n+                        const llvm::SmallDenseMap<Value, PtrState> &knownPtrs);\n+\n+  // Parse the state of AddPtrOp, insert any instruction needed to\n+  // calculate strides and offsets, build PtrState for this operand, and record\n+  // PtrState for knownPtrs.\n+  static void rewriteAddptrOp(triton::AddPtrOp op,\n+                              ConversionPatternRewriter &rewriter,\n+                              llvm::SmallDenseMap<Value, PtrState> &knownPtrs);\n+\n+  // Parse the state of YieldOp, insert any instruction needed to calculate\n+  // strides and offsets, build PtrState for this operand, and record PtrState\n+  // in knownPtrs.\n+  static void\n+  rewriteYieldOp(scf::YieldOp op, ConversionPatternRewriter &rewriter,\n+                 const IndexMapSet &levelToBlockArgIndex, const int level,\n+                 const llvm::SmallDenseMap<Value, PtrState> &knownPtrs);\n+\n+  static void rewriteForOp(scf::ForOp op, ConversionPatternRewriter &rewriter,\n+                           IndexMapSet &levelToBlockArgIndex, const int level,\n+                           llvm::SmallDenseMap<Value, PtrState> &knownPtrs);\n+\n+  static Value getScalarMemRef(Value ptr, Value memRef, const Location loc,\n+                               ConversionPatternRewriter &rewriter);\n+};\n+\n+} // namespace triton\n+\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Analysis/UseAnalysis.h", "status": "added", "additions": 114, "deletions": 0, "changes": 114, "file_content_changes": "@@ -0,0 +1,114 @@\n+//===----------------------------------------------------------------------===//\n+//\n+// Copyright (c) Triton Project Contributors.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#ifndef TRITON_ANALYSIS_USEANALYSIS_H\n+#define TRITON_ANALYSIS_USEANALYSIS_H\n+\n+#include \"mlir/Analysis/DataFlow/SparseAnalysis.h\"\n+#include \"mlir/Pass/Pass.h\"\n+\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+\n+namespace mlir {\n+namespace triton {\n+\n+std::unique_ptr<Pass> createTritonUseAnalysisPass();\n+\n+enum class UseType {\n+  Undefined, // Initial state\n+  DataUse,   // value used for tensor computation only\n+  MetaUse,   // value used for metadata only\n+  MixUse     // value used for both tensor computation and metadata\n+};\n+\n+struct UseInfo : public dataflow::AbstractSparseLattice {\n+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(UseInfo)\n+  using AbstractSparseLattice::AbstractSparseLattice;\n+\n+  // Lattice state transfer function\n+  ChangeResult meetUseType(const UseType &other) {\n+    if (other == UseType::Undefined)\n+      return ChangeResult::NoChange;\n+\n+    switch (type) {\n+    case UseType::Undefined:\n+      type = other;\n+      return ChangeResult::Change;\n+    case UseType::DataUse:\n+    case UseType::MetaUse:\n+      if (type == other) {\n+        return ChangeResult::NoChange;\n+      } else {\n+        type = UseType::MixUse;\n+        return ChangeResult::Change;\n+      }\n+    case UseType::MixUse:\n+      return ChangeResult::NoChange;\n+    }\n+  }\n+\n+  ChangeResult meet(const AbstractSparseLattice &other) override {\n+    auto rhs = reinterpret_cast<const UseInfo *>(&other);\n+    return meetUseType(rhs->type);\n+  }\n+\n+  void print(raw_ostream &os) const override {\n+    switch (type) {\n+    case UseType::DataUse:\n+      os << \"DataUse\";\n+      break;\n+    case UseType::MetaUse:\n+      os << \"MetaUse\";\n+      break;\n+    case UseType::MixUse:\n+      os << \"MixUse\";\n+      break;\n+    default:\n+      os << \"Undefined\";\n+    }\n+  }\n+\n+  UseType type = UseType::Undefined;\n+};\n+\n+class UseAnalysis : public dataflow::SparseBackwardDataFlowAnalysis<UseInfo> {\n+public:\n+  using SparseBackwardDataFlowAnalysis::SparseBackwardDataFlowAnalysis;\n+  void visitOperation(Operation *op, ArrayRef<UseInfo *> operands,\n+                      ArrayRef<const UseInfo *> results) override;\n+\n+  void visitBranchOperand(OpOperand &operand) override { return; }\n+\n+  void setToExitState(UseInfo *lattice) override {\n+    lattice->type = UseType::Undefined;\n+  }\n+\n+private:\n+  void propagateUse(UseInfo *lattice, const UseType &type) {\n+    auto changed = lattice->meetUseType(type);\n+    propagateIfChanged(lattice, changed);\n+  }\n+\n+  void propagateResults(UseInfo *lattice, ArrayRef<const UseInfo *> results) {\n+    auto changed = ChangeResult::NoChange;\n+    for (auto result : results)\n+      changed |= lattice->meet(*result);\n+    propagateIfChanged(lattice, changed);\n+  }\n+};\n+\n+// Use SparseBackwardDataAnalysis to identify operations whose results are used\n+// as data tensor operations, meta operations (address calculation,\n+// broadcasting/splating constant, etc.), or both. For operations used as both\n+// purposes, clone them so that the remaining pass built on\n+// ConversionPatternRewriter can replace all tensor producers cleanly and simply\n+// delete meta data producers.\n+LogicalResult runUseAnalysis(triton::FuncOp &funcOp);\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif // TRITON_CONVERSION_TRITONTOAFFINE_TRITONUSEANALYSIS_H"}, {"filename": "include/triton/Conversion/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,2 +1,3 @@\n add_subdirectory(TritonToTritonGPU)\n add_subdirectory(TritonGPUToLLVM)\n+add_subdirectory(TritonToLinalg)"}, {"filename": "include/triton/Conversion/TritonToLinalg/CMakeLists.txt", "status": "added", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -0,0 +1,9 @@\n+#===------------------------------------------------------------------------===#\n+#\n+# Copyright (c) Triton Project Contributors.\n+#\n+#===------------------------------------------------------------------------===#\n+\n+set(LLVM_TARGET_DEFINITIONS Passes.td)\n+mlir_tablegen(Passes.h.inc -gen-pass-decls --name TritonToLinalg)\n+add_public_tablegen_target(TritonToLinalgConversionPassIncGen)"}, {"filename": "include/triton/Conversion/TritonToLinalg/Passes.h", "status": "added", "additions": 21, "deletions": 0, "changes": 21, "file_content_changes": "@@ -0,0 +1,21 @@\n+//===----------------------------------------------------------------------===//\n+//\n+// Copyright (c) Triton Project Contributors.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#ifndef TRITON_TO_LINALG_CONVERSION_PASSES_H\n+#define TRITON_TO_LINALG_CONVERSION_PASSES_H\n+\n+#include \"triton/Conversion/TritonToLinalg/TritonToLinalg.h\"\n+\n+namespace mlir {\n+namespace triton {\n+\n+#define GEN_PASS_REGISTRATION\n+#include \"triton/Conversion/TritonToLinalg/Passes.h.inc\"\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Conversion/TritonToLinalg/Passes.td", "status": "added", "additions": 17, "deletions": 0, "changes": 17, "file_content_changes": "@@ -0,0 +1,17 @@\n+//===----------------------------------------------------------------------===//\n+//\n+// Copyright (c) Triton Project Contributors.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#ifndef TRITON_TO_LINALG_CONVERSION_PASSES\n+#define TRITON_TO_LINALG_CONVERSION_PASSES\n+\n+include \"mlir/Pass/PassBase.td\"\n+\n+def TritonToLinalg : Pass<\"triton-to-linalg\", \"mlir::ModuleOp\"> {\n+  let summary = \"Convert Triton to Linalg dialect\";\n+  let constructor = \"triton::createTritonToLinalgPass()\";\n+}\n+\n+#endif"}, {"filename": "include/triton/Conversion/TritonToLinalg/TritonToLinalg.h", "status": "added", "additions": 32, "deletions": 0, "changes": 32, "file_content_changes": "@@ -0,0 +1,32 @@\n+//===----------------------------------------------------------------------===//\n+//\n+// Copyright (c) Triton Project Contributors.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#ifndef TRITON_CONVERSION_TRITONTOLINALG_TRITONTOLINALG_H\n+#define TRITON_CONVERSION_TRITONTOLINALG_TRITONTOLINALG_H\n+\n+#include \"mlir/Dialect/Bufferization/IR/Bufferization.h\"\n+#include \"mlir/Dialect/Linalg/IR/Linalg.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+\n+namespace mlir {\n+namespace triton {\n+\n+std::unique_ptr<OperationPass<ModuleOp>> createTritonToLinalgPass();\n+\n+void populateTritonToLinalgCanonicalizationPatterns(\n+    RewritePatternSet &patterns);\n+\n+void populateTritonToLinalgConversionPatterns(TypeConverter &typeConverter,\n+                                              RewritePatternSet &patterns,\n+                                              unsigned int launchGridRank);\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif // TRITON_CONVERSION_TRITONTOLINALG_TRITONTOLINALG_H"}, {"filename": "lib/Analysis/CMakeLists.txt", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -4,6 +4,10 @@ add_mlir_library(TritonAnalysis\n   Membar.cpp\n   Alias.cpp\n   Utility.cpp\n+  UseAnalysis.cpp\n+  MaskAnalysis.cpp\n+  PtrAnalysis.cpp\n+  OpFoldResultUtils.cpp\n \n   DEPENDS\n   TritonTableGen"}, {"filename": "lib/Analysis/MaskAnalysis.cpp", "status": "added", "additions": 329, "deletions": 0, "changes": 329, "file_content_changes": "@@ -0,0 +1,329 @@\n+//===----------------------------------------------------------------------===//\n+//\n+// Copyright (c) Triton Project Contributors.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#include \"triton/Analysis/MaskAnalysis.h\"\n+#include \"triton/Analysis/OpFoldResultUtils.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+\n+#include \"mlir/Transforms/DialectConversion.h\"\n+\n+namespace mlir {\n+\n+namespace triton {\n+\n+LogicalResult MaskState::parse(Value operand, const Location loc,\n+                               ConversionPatternRewriter &rewriter) {\n+  if (auto op = operand.getDefiningOp<arith::ConstantOp>()) {\n+    return this->parseConstant(op, loc, rewriter);\n+  } else if (operand.getType().isa<IntegerType>()) {\n+    return this->parseIntScalar(operand, loc, rewriter);\n+  } else if (auto op = operand.getDefiningOp<arith::AddIOp>()) {\n+    return this->parseAdd(op, loc, rewriter);\n+  } else if (auto op = operand.getDefiningOp<arith::AndIOp>()) {\n+    return this->parseAnd(op, loc, rewriter);\n+  } else if (auto op = operand.getDefiningOp<arith::CmpIOp>()) {\n+    return this->parseCmp(op, loc, rewriter);\n+  } else if (auto op = operand.getDefiningOp<triton::MakeRangeOp>()) {\n+    return this->parseMakeRange(op, loc, rewriter);\n+  } else if (auto op = operand.getDefiningOp<triton::BroadcastOp>()) {\n+    return this->parseBroadcast(op, loc, rewriter);\n+  } else if (auto op = operand.getDefiningOp<triton::SplatOp>()) {\n+    return this->parseSplat(op, loc, rewriter);\n+  } else if (auto op = operand.getDefiningOp<triton::ExpandDimsOp>()) {\n+    return this->parseExpandDims(op, loc, rewriter);\n+  } else {\n+    return failure();\n+  }\n+}\n+\n+tensor::ExtractSliceOp\n+MaskState::getExtractSlice(Value source, const Location loc,\n+                           ConversionPatternRewriter &rewriter) const {\n+  auto sourceType = source.getType().cast<RankedTensorType>();\n+  SmallVector<OpFoldResult> offsets(getRank(), rewriter.getIndexAttr(0));\n+  SmallVector<OpFoldResult> strides(getRank(), rewriter.getIndexAttr(1));\n+\n+  auto dstType = tensor::ExtractSliceOp::inferResultType(sourceType, offsets,\n+                                                         dims, strides);\n+\n+  return rewriter.create<tensor::ExtractSliceOp>(loc, dstType, source, offsets,\n+                                                 dims, strides);\n+}\n+\n+memref::SubViewOp\n+MaskState::getSubview(Value source, const Location loc,\n+                      ConversionPatternRewriter &rewriter) const {\n+  auto sourceType = source.getType().cast<MemRefType>();\n+  SmallVector<OpFoldResult> offsets(getRank(), rewriter.getIndexAttr(0));\n+  SmallVector<OpFoldResult> strides(getRank(), rewriter.getIndexAttr(1));\n+  auto dstType =\n+      memref::SubViewOp::inferResultType(sourceType, offsets, dims, strides);\n+\n+  return rewriter.create<memref::SubViewOp>(loc, dstType.cast<MemRefType>(),\n+                                            source, offsets, dims, strides);\n+}\n+\n+LogicalResult MaskState::addStateScalar(const MaskState &state,\n+                                        const OpFoldResult scalar, Location loc,\n+                                        ConversionPatternRewriter &rewriter) {\n+  start = addOFRs(state.start, scalar, loc, rewriter);\n+  end = addOFRs(state.end, scalar, loc, rewriter);\n+  dims = state.dims;\n+  return success();\n+}\n+\n+LogicalResult MaskState::addStates(const MaskState &lhsState,\n+                                   const MaskState &rhsState, Location loc,\n+                                   ConversionPatternRewriter &rewriter) {\n+  if (lhsState.scalar && rhsState.scalar) {\n+    InFlightDiagnostic diag =\n+        emitError(loc) << \"Unexpected case where both lhs and rhs are scalars\";\n+    return failure();\n+  }\n+\n+  if (!lhsState.scalar && !rhsState.scalar) {\n+    InFlightDiagnostic diag =\n+        emitError(loc)\n+        << \"Unsupported scenario where neither lhs nor rhs is a scalar\";\n+    return failure();\n+  }\n+\n+  if (lhsState.scalar)\n+    return addStateScalar(rhsState, lhsState.scalar, loc, rewriter);\n+  else\n+    return addStateScalar(lhsState, rhsState.scalar, loc, rewriter);\n+}\n+\n+LogicalResult MaskState::minStates(const MaskState &lhsState,\n+                                   const MaskState &rhsState, Location loc,\n+                                   ConversionPatternRewriter &rewriter) {\n+  if (lhsState.getRank() != rhsState.getRank()) {\n+    InFlightDiagnostic diag =\n+        emitError(loc)\n+        << \"Unexpected case where lhs and rhs have different ranks\";\n+    return failure();\n+  }\n+\n+  for (uint32_t i = 0; i < lhsState.getRank(); i++) {\n+    auto lhsDim = lhsState.dims[i];\n+    auto rhsDim = rhsState.dims[i];\n+    dims.push_back(minOFRs(lhsDim, rhsDim, loc, rewriter));\n+  }\n+  return success();\n+}\n+\n+LogicalResult MaskState::parseConstant(arith::ConstantOp constOp,\n+                                       const Location loc,\n+                                       ConversionPatternRewriter &rewriter) {\n+  assert(this->isEmpty());\n+\n+  if (isa<DenseElementsAttr>(constOp.getValue())) {\n+    auto attr = cast<DenseElementsAttr>(constOp.getValue());\n+    auto elementType = attr.getElementType();\n+    assert(attr.isSplat() && elementType.isa<IntegerType>() &&\n+           \"All elements must share a single integer constant value\");\n+    auto values = attr.getValues<IntegerAttr>();\n+    auto value = values[0].getValue();\n+    auto constAttr = rewriter.getIndexAttr(value.getSExtValue());\n+    auto op = rewriter.create<arith::ConstantOp>(loc, constAttr,\n+                                                 rewriter.getIndexType());\n+    this->scalar = op.getValue();\n+  } else {\n+    auto value = constOp.getValue().cast<IntegerAttr>().getInt();\n+    this->scalar = rewriter.getIndexAttr(value);\n+  }\n+\n+  return success();\n+}\n+\n+LogicalResult MaskState::parseIntScalar(Value scalar, const Location loc,\n+                                        ConversionPatternRewriter &rewriter) {\n+  assert(this->isEmpty());\n+  auto castOp =\n+      rewriter.create<arith::IndexCastOp>(loc, rewriter.getIndexType(), scalar);\n+  this->scalar = castOp.getResult();\n+  return success();\n+}\n+\n+LogicalResult MaskState::parseAdd(arith::AddIOp addOp, const Location loc,\n+                                  ConversionPatternRewriter &rewriter) {\n+  assert(this->isEmpty());\n+\n+  MaskState lhsState;\n+  if (failed(lhsState.parse(addOp.getLhs(), loc, rewriter)))\n+    return failure();\n+\n+  MaskState rhsState;\n+  if (failed(rhsState.parse(addOp.getRhs(), loc, rewriter)))\n+    return failure();\n+\n+  return this->addStates(lhsState, rhsState, loc, rewriter);\n+}\n+\n+LogicalResult MaskState::parseAnd(arith::AndIOp andOp, const Location loc,\n+                                  ConversionPatternRewriter &rewriter) {\n+  assert(this->isEmpty());\n+\n+  MaskState lhsState;\n+  if (failed(lhsState.parse(andOp.getLhs(), loc, rewriter)) ||\n+      !lhsState.isMask())\n+    return failure();\n+\n+  MaskState rhsState;\n+  if (failed(rhsState.parse(andOp.getRhs(), loc, rewriter)) ||\n+      !rhsState.isMask())\n+    return failure();\n+\n+  return this->minStates(lhsState, rhsState, loc, rewriter);\n+}\n+\n+LogicalResult MaskState::parseCmp(arith::CmpIOp cmpOp, const Location loc,\n+                                  ConversionPatternRewriter &rewriter) {\n+  assert(this->isEmpty());\n+\n+  if (cmpOp.getPredicate() != arith::CmpIPredicate::slt) {\n+    InFlightDiagnostic diag = emitError(loc) << \"Unsupported cmpi predicate\";\n+    return failure();\n+  }\n+\n+  MaskState lhsState;\n+  if (failed(lhsState.parse(cmpOp.getLhs(), loc, rewriter)))\n+    return failure();\n+\n+  MaskState rhsState;\n+  if (failed(rhsState.parse(cmpOp.getRhs(), loc, rewriter)))\n+    return failure();\n+\n+  assert((!lhsState.scalar && rhsState.scalar) && \"Unsupported cmpi scenario\");\n+\n+  int32_t cmpDim = -1;\n+  for (int32_t i = 0; i < lhsState.getRank(); i++) {\n+    auto dimIntAttr = getIntAttr(lhsState.dims[i]);\n+    if (!dimIntAttr || dimIntAttr.value() != 1) {\n+      if (cmpDim != -1) {\n+        InFlightDiagnostic diag = emitError(loc)\n+                                  << \"Unsupported cmpi with more than one \"\n+                                     \"dimension with size larger than 1\";\n+        return failure();\n+      }\n+      cmpDim = i;\n+    }\n+  }\n+  assert(cmpDim != -1 &&\n+         \"Unexpected case where no dimension has size larger than 1\");\n+\n+  auto newEnd = minOFRs(lhsState.end, rhsState.scalar, loc, rewriter);\n+  auto newDim = subOFRs(newEnd, lhsState.start, loc, rewriter);\n+\n+  for (int32_t i = 0; i < lhsState.getRank(); i++) {\n+    if (i == cmpDim)\n+      this->dims.push_back(newDim);\n+    else\n+      this->dims.push_back(lhsState.dims[i]);\n+  }\n+\n+  return success();\n+}\n+\n+LogicalResult MaskState::parseMakeRange(triton::MakeRangeOp rangeOp,\n+                                        const Location loc,\n+                                        ConversionPatternRewriter &rewriter) {\n+  assert(this->isEmpty());\n+\n+  auto shape = rangeOp.getType().cast<ShapedType>().getShape();\n+  auto start = rangeOp.getStart();\n+  auto end = rangeOp.getEnd();\n+  auto stride = (end - start + shape[0] - 1) / shape[0];\n+\n+  if (stride != 1) {\n+    InFlightDiagnostic diag =\n+        emitError(loc)\n+        << \"stride must be 1 for make_range whose result is used \"\n+           \"as load or store masks\";\n+    return failure();\n+  }\n+\n+  this->start = rewriter.getIndexAttr(start);\n+  this->end = rewriter.getIndexAttr(end);\n+  this->dims.push_back(rewriter.getIndexAttr(shape[0]));\n+\n+  return success();\n+}\n+\n+LogicalResult MaskState::parseBroadcast(triton::BroadcastOp broadcastOp,\n+                                        const Location loc,\n+                                        ConversionPatternRewriter &rewriter) {\n+  assert(this->isEmpty());\n+\n+  auto src = broadcastOp.getSrc();\n+  auto dst = broadcastOp.getResult();\n+  assert(src.getType().isa<ShapedType>() &&\n+         \"input to tt.broadcast should be a tensor\");\n+\n+  auto srcShape = src.getType().cast<ShapedType>().getShape();\n+  auto dstShape = dst.getType().cast<ShapedType>().getShape();\n+  assert(srcShape.size() == dstShape.size() &&\n+         \"rank of source and destination should match\");\n+\n+  if (failed(parse(src, loc, rewriter)))\n+    return failure();\n+\n+  for (size_t i = 0; i < srcShape.size(); i++) {\n+    if (srcShape[i] == dstShape[i])\n+      continue;\n+    else if (srcShape[i] < dstShape[i])\n+      this->dims[i] = rewriter.getIndexAttr(dstShape[i]);\n+    else\n+      llvm_unreachable(\"unexpected dimensions used in broadcast\");\n+  }\n+\n+  return success();\n+}\n+\n+LogicalResult MaskState::parseSplat(triton::SplatOp splatOp, const Location loc,\n+                                    ConversionPatternRewriter &rewriter) {\n+  assert(this->isEmpty());\n+\n+  auto src = splatOp.getSrc();\n+  auto dst = splatOp.getResult();\n+  auto dstShape = dst.getType().cast<ShapedType>().getShape();\n+\n+  if (!src.getType().isa<IntegerType>()) {\n+    InFlightDiagnostic diag =\n+        emitError(loc)\n+        << \"splat source must be an integer scalar for load/store masks\";\n+    return failure();\n+  }\n+\n+  if (failed(this->parse(src, loc, rewriter)))\n+    return failure();\n+\n+  for (auto s : dstShape)\n+    this->dims.push_back(rewriter.getIndexAttr(s));\n+\n+  return success();\n+}\n+\n+LogicalResult MaskState::parseExpandDims(triton::ExpandDimsOp expandDimsOp,\n+                                         const Location loc,\n+                                         ConversionPatternRewriter &rewriter) {\n+  assert(this->isEmpty());\n+\n+  if (failed(this->parse(expandDimsOp.getSrc(), loc, rewriter)))\n+    return failure();\n+\n+  auto dstShape =\n+      expandDimsOp.getResult().getType().cast<ShapedType>().getShape();\n+  auto axis = expandDimsOp.getAxis();\n+  assert(dstShape[axis] == 1 &&\n+         \"expect changed dimension to be 1 in expand_dims\");\n+  this->dims.insert(this->dims.begin() + axis, rewriter.getIndexAttr(1));\n+\n+  return success();\n+}\n+\n+} // namespace triton\n+} // namespace mlir"}, {"filename": "lib/Analysis/OpFoldResultUtils.cpp", "status": "added", "additions": 179, "deletions": 0, "changes": 179, "file_content_changes": "@@ -0,0 +1,179 @@\n+//===----------------------------------------------------------------------===//\n+//\n+// Copyright (c) Triton Project Contributors.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#include \"triton/Analysis/OpFoldResultUtils.h\"\n+\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+\n+namespace mlir {\n+\n+// Return integer if ofr is an IntegerAttr. Note that this function differs\n+// from getConstantIntValue, which returns an integer if ofr is the constant\n+// result of an operation too.\n+std::optional<int64_t> getIntAttr(const OpFoldResult ofr) {\n+  if (ofr.is<Attribute>() && ofr.get<Attribute>().isa<IntegerAttr>())\n+    return ofr.get<Attribute>().dyn_cast<IntegerAttr>().getInt();\n+\n+  return std::nullopt;\n+}\n+\n+// Process addition of two OFRs. If both OFRs are Integer Attributes, result\n+// is an Integer Attribute. Otherwise, insert the arith.addi instruction if\n+// needed and use its result Value.\n+OpFoldResult addOFRs(const OpFoldResult lhs, const OpFoldResult rhs,\n+                     const Location loc, ConversionPatternRewriter &rewriter) {\n+  auto lhsIntAttr = getIntAttr(lhs);\n+  auto rhsIntAttr = getIntAttr(rhs);\n+\n+  // shortcut for special cases\n+  if (!lhsIntAttr && rhsIntAttr && rhsIntAttr.value() == 0)\n+    return lhs;\n+  if (!rhsIntAttr && lhsIntAttr && lhsIntAttr.value() == 0)\n+    return rhs;\n+\n+  // both lhs and rhs are constants, return result directly\n+  if (lhsIntAttr && rhsIntAttr)\n+    return rewriter.getIndexAttr(lhsIntAttr.value() + rhsIntAttr.value());\n+\n+  // otherwise, need to create instructions to calculate new attribute value\n+  auto lhsValue = lhs.dyn_cast<Value>();\n+  if (lhsIntAttr) {\n+    auto lhsOp = rewriter.create<arith::ConstantOp>(\n+        loc, rewriter.getIndexAttr(lhsIntAttr.value()));\n+    lhsValue = lhsOp.getResult();\n+  } else {\n+    assert(lhsValue.getType().isa<IndexType>());\n+  }\n+\n+  auto rhsValue = rhs.dyn_cast<Value>();\n+  if (rhsIntAttr) {\n+    auto rhsOp = rewriter.create<arith::ConstantOp>(\n+        loc, rewriter.getIndexAttr(rhsIntAttr.value()));\n+    rhsValue = rhsOp.getResult();\n+  } else {\n+    assert(lhsValue.getType().isa<IndexType>());\n+  }\n+\n+  return rewriter.create<arith::AddIOp>(loc, lhsValue, rhsValue).getResult();\n+}\n+\n+// Produce result = lhs - rhs. If both OFRs are Integer Attributes, result\n+// is an Integer Attribute. Otherwise, insert the arith.addi instruction if\n+// needed and use its result Value.\n+OpFoldResult subOFRs(const OpFoldResult lhs, const OpFoldResult rhs,\n+                     const Location loc, ConversionPatternRewriter &rewriter) {\n+  auto lhsIntAttr = getIntAttr(lhs);\n+  auto rhsIntAttr = getIntAttr(rhs);\n+\n+  // shortcut for special cases\n+  if (!lhsIntAttr && rhsIntAttr && rhsIntAttr.value() == 0)\n+    return lhs;\n+\n+  // both lhs and rhs are constants, return result directly\n+  if (lhsIntAttr && rhsIntAttr)\n+    return rewriter.getIndexAttr(lhsIntAttr.value() - rhsIntAttr.value());\n+\n+  // otherwise, need to create instructions to calculate new attribute value\n+  auto lhsValue = lhs.dyn_cast<Value>();\n+  if (lhsIntAttr) {\n+    auto lhsOp = rewriter.create<arith::ConstantOp>(\n+        loc, rewriter.getIndexAttr(lhsIntAttr.value()));\n+    lhsValue = lhsOp.getResult();\n+  }\n+\n+  auto rhsValue = rhs.dyn_cast<Value>();\n+  if (rhsIntAttr) {\n+    auto rhsOp = rewriter.create<arith::ConstantOp>(\n+        loc, rewriter.getIndexAttr(rhsIntAttr.value()));\n+    rhsValue = rhsOp.getResult();\n+  }\n+\n+  auto sumOp = rewriter.create<arith::SubIOp>(loc, lhsValue, rhsValue);\n+  return sumOp.getResult();\n+}\n+\n+// Process multiplication of two OFRs. If both OFRs are Integer Attributes,\n+// result is an Integer Attribtue. Otherwise, insert the arith.muli\n+// instruction if needed and use its result Value.\n+OpFoldResult mulOFRValue(const OpFoldResult lhs, const Value rhs,\n+                         const Location loc,\n+                         ConversionPatternRewriter &rewriter) {\n+  auto lhsIntAttr = getIntAttr(lhs);\n+\n+  auto rhsIsConst = false;\n+  // if rhs is not a const, use max value since min is used to represent\n+  // dynamic size or stride\n+  auto rhsConstValue = std::numeric_limits<int64_t>::max();\n+  auto rhsOp = rhs.getDefiningOp<arith::ConstantOp>();\n+  if (rhsOp) {\n+    rhsIsConst = true;\n+    rhsConstValue = rhsOp.getValue().cast<IntegerAttr>().getInt();\n+  }\n+\n+  // shortcuts for special cases\n+  if (lhsIntAttr) {\n+    if (lhsIntAttr.value() == 0)\n+      return lhs;\n+    if (lhsIntAttr.value() == 1)\n+      return rhs;\n+  }\n+  if (rhsIsConst) {\n+    if (rhsConstValue == 0)\n+      return rhsOp.getResult();\n+    if (rhsConstValue == 1)\n+      return lhs;\n+  }\n+\n+  // 0. both lhs and rhs are constants\n+  if (lhsIntAttr && rhsIsConst)\n+    return rewriter.getIndexAttr(lhsIntAttr.value() * rhsConstValue);\n+\n+  // 1. if lhs is constant but rhs is not\n+  if (lhsIntAttr && !rhsIsConst) {\n+    auto lhsConstOp = rewriter.create<arith::ConstantOp>(\n+        loc, rewriter.getIndexAttr(lhsIntAttr.value()));\n+    auto mulOp =\n+        rewriter.create<arith::MulIOp>(loc, lhsConstOp.getResult(), rhs);\n+    return mulOp.getResult();\n+  }\n+\n+  // 2. if lhs is not constant\n+  assert(!lhsIntAttr);\n+  auto mulOp = rewriter.create<arith::MulIOp>(loc, lhs.get<Value>(), rhs);\n+  return mulOp.getResult();\n+}\n+\n+OpFoldResult minOFRs(const OpFoldResult lhs, const OpFoldResult rhs,\n+                     const Location loc, ConversionPatternRewriter &rewriter) {\n+  auto lhsIntAttr = getIntAttr(lhs);\n+  auto rhsIntAttr = getIntAttr(rhs);\n+\n+  // both lhs and rhs are constants, return result directly\n+  if (lhsIntAttr && rhsIntAttr)\n+    return rewriter.getIndexAttr(\n+        std::min(lhsIntAttr.value(), rhsIntAttr.value()));\n+\n+  // otherwise, need to create instructions to calculate new attribute value\n+  auto lhsValue = lhs.dyn_cast<Value>();\n+  if (lhsIntAttr) {\n+    auto lhsOp = rewriter.create<arith::ConstantOp>(\n+        loc, rewriter.getIndexAttr(lhsIntAttr.value()));\n+    lhsValue = lhsOp.getResult();\n+  }\n+\n+  auto rhsValue = rhs.dyn_cast<Value>();\n+  if (rhsIntAttr) {\n+    auto rhsOp = rewriter.create<arith::ConstantOp>(\n+        loc, rewriter.getIndexAttr(rhsIntAttr.value()));\n+    rhsValue = rhsOp.getResult();\n+  }\n+\n+  auto minOp = rewriter.create<arith::MinSIOp>(loc, lhsValue, rhsValue);\n+  return minOp.getResult();\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Analysis/PtrAnalysis.cpp", "status": "added", "additions": 837, "deletions": 0, "changes": 837, "file_content_changes": "@@ -0,0 +1,837 @@\n+//===----------------------------------------------------------------------===//\n+//\n+// Copyright (c) Triton Project Contributors.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#include \"triton/Analysis/PtrAnalysis.h\"\n+#include \"triton/Analysis/OpFoldResultUtils.h\"\n+\n+#include \"mlir/IR/IRMapping.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+\n+#include \"llvm/Support/Debug.h\"\n+#include <set>\n+\n+#define DEBUG_TYPE \"triton-ptr-analysis\"\n+\n+namespace mlir {\n+\n+namespace triton {\n+\n+int64_t PtrState::getRank() const {\n+  assert(offsets.size() == sizes.size() && offsets.size() == strides.size());\n+  return offsets.size();\n+}\n+\n+bool PtrState::isEmpty() const {\n+  return (getRank() == 0 && !source && !scalar);\n+}\n+\n+void PtrState::addState(const PtrState &lhsState, const PtrState &rhsState,\n+                        Location loc, ConversionPatternRewriter &rewriter) {\n+  assert(isEmpty() && lhsState.getRank() == rhsState.getRank());\n+\n+  // at most one of lhs and rhs should have valid source, since otherwise we\n+  // will be losing information\n+  assert(!(lhsState.source && rhsState.source));\n+  source = lhsState.source ? lhsState.source : rhsState.source;\n+\n+  if (lhsState.scalar && rhsState.scalar) {\n+    auto addOp =\n+        rewriter.create<arith::AddIOp>(loc, lhsState.scalar, rhsState.scalar);\n+    scalar = addOp.getResult();\n+  } else if (lhsState.getRank() == 0) { // both lhs and rhs are scalars\n+    scalar = lhsState.scalar ? lhsState.scalar : rhsState.scalar;\n+  }\n+\n+  for (uint64_t i = 0; i < lhsState.sizes.size(); i++) {\n+    auto newOffset =\n+        addOFRs(lhsState.offsets[i], rhsState.offsets[i], loc, rewriter);\n+    offsets.push_back(newOffset);\n+\n+    auto newStride =\n+        addOFRs(lhsState.strides[i], rhsState.strides[i], loc, rewriter);\n+    strides.push_back(newStride);\n+\n+    sizes.push_back(lhsState.sizes[i]);\n+  }\n+}\n+\n+void PtrState::mulState(const PtrState &lhsState, const PtrState &rhsState,\n+                        const Location loc,\n+                        ConversionPatternRewriter &rewriter) {\n+  bool rhsScalar = true;\n+  assert(isEmpty() && lhsState.getRank() == rhsState.getRank());\n+\n+  // neither lhs nor rhs should have source, since multiplying base pointer\n+  // does not make sense\n+  assert(!(lhsState.source && rhsState.source));\n+\n+  source = lhsState.source ? lhsState.source : rhsState.source;\n+\n+  assert((lhsState.scalar || rhsState.scalar) &&\n+         !(lhsState.scalar && rhsState.scalar) &&\n+         \"currently does not support both tensors are effectively non-scalar\");\n+\n+  if (!rhsState.scalar && lhsState.scalar)\n+    rhsScalar = false;\n+\n+  for (uint64_t i = 0; i < lhsState.sizes.size(); i++) {\n+    OpFoldResult newOffset;\n+    OpFoldResult newStride;\n+    if (rhsScalar) {\n+      newOffset =\n+          mulOFRValue(lhsState.offsets[i], rhsState.scalar, loc, rewriter);\n+      newStride =\n+          mulOFRValue(lhsState.strides[i], rhsState.scalar, loc, rewriter);\n+    } else {\n+      newOffset =\n+          mulOFRValue(rhsState.offsets[i], lhsState.scalar, loc, rewriter);\n+      newStride =\n+          mulOFRValue(rhsState.strides[i], lhsState.scalar, loc, rewriter);\n+    }\n+    offsets.push_back(newOffset);\n+    strides.push_back(newStride);\n+    sizes.push_back(lhsState.sizes[i]);\n+  }\n+}\n+\n+memref::ReinterpretCastOp\n+PtrState::createCastOp(ArrayRef<int64_t> resultShape, const Location loc,\n+                       ConversionPatternRewriter &rewriter) {\n+  // Accumulate final offset\n+  OpFoldResult targetOffset = rewriter.getIndexAttr(0);\n+  for (auto o : offsets)\n+    targetOffset = addOFRs(targetOffset, o, loc, rewriter);\n+\n+  // Create result MemRefType\n+  SmallVector<int64_t> staticOffset;\n+  SmallVector<Value> dynamicOffset;\n+  SmallVector<int64_t> staticStrides;\n+  SmallVector<Value> dynamicStrides;\n+  dispatchIndexOpFoldResult(targetOffset, dynamicOffset, staticOffset);\n+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);\n+\n+  auto elementType = source.getType().cast<BaseMemRefType>().getElementType();\n+  auto layout = StridedLayoutAttr::get(source.getContext(), staticOffset[0],\n+                                       staticStrides);\n+  auto resultType = MemRefType::get(resultShape, elementType, layout);\n+\n+  // Create reinterpret cast\n+  return rewriter.create<memref::ReinterpretCastOp>(\n+      loc, resultType, source, targetOffset, sizes, strides);\n+}\n+\n+void PtrAnalysis::visitOperandAdd(\n+    arith::AddIOp addOp, PtrState &state, const Location loc,\n+    ConversionPatternRewriter &rewriter,\n+    const llvm::SmallDenseMap<Value, PtrState> &knownPtrs) {\n+  PtrState lhsState;\n+  visitOperand(addOp.getLhs(), lhsState, loc, rewriter, knownPtrs);\n+\n+  PtrState rhsState;\n+  visitOperand(addOp.getRhs(), rhsState, loc, rewriter, knownPtrs);\n+\n+  state.addState(lhsState, rhsState, loc, rewriter);\n+}\n+\n+void PtrAnalysis::visitOperandMul(\n+    arith::MulIOp mulOp, PtrState &state, const Location loc,\n+    ConversionPatternRewriter &rewriter,\n+    const llvm::SmallDenseMap<Value, PtrState> &knownPtrs) {\n+  PtrState lhsState;\n+  visitOperand(mulOp.getLhs(), lhsState, loc, rewriter, knownPtrs);\n+\n+  PtrState rhsState;\n+  visitOperand(mulOp.getRhs(), rhsState, loc, rewriter, knownPtrs);\n+\n+  state.mulState(lhsState, rhsState, loc, rewriter);\n+}\n+\n+void PtrAnalysis::visitOperandMakeRange(\n+    triton::MakeRangeOp rangeOp, PtrState &state, Location loc,\n+    ConversionPatternRewriter &rewriter,\n+    const llvm::SmallDenseMap<Value, PtrState> &knownPtrs) {\n+  assert(state.isEmpty());\n+\n+  auto shape = rangeOp.getType().cast<ShapedType>().getShape();\n+\n+  auto start = rangeOp.getStart();\n+  auto end = rangeOp.getEnd();\n+  auto stride = (end - start + shape[0] - 1) / shape[0];\n+\n+  state.offsets.push_back(rewriter.getIndexAttr(start));\n+  state.sizes.push_back(rewriter.getIndexAttr(shape[0]));\n+  state.strides.push_back(rewriter.getIndexAttr(stride));\n+}\n+\n+void PtrAnalysis::visitOperandExpandDims(\n+    triton::ExpandDimsOp expandDimsOp, PtrState &state, const Location loc,\n+    ConversionPatternRewriter &rewriter,\n+    const llvm::SmallDenseMap<Value, PtrState> &knownPtrs) {\n+  assert(state.isEmpty());\n+\n+  visitOperand(expandDimsOp.getSrc(), state, loc, rewriter, knownPtrs);\n+\n+  auto dstShape =\n+      expandDimsOp.getResult().getType().cast<ShapedType>().getShape();\n+  auto axis = expandDimsOp.getAxis();\n+\n+  assert(dstShape[axis] == 1 &&\n+         \"expect changed dimension to be 1 in expand_dims\");\n+\n+  // insert dimension info\n+  state.offsets.insert(state.offsets.begin() + axis, rewriter.getIndexAttr(0));\n+  state.sizes.insert(state.sizes.begin() + axis, rewriter.getIndexAttr(1));\n+  state.strides.insert(state.strides.begin() + axis, rewriter.getIndexAttr(0));\n+}\n+\n+void PtrAnalysis::visitOperandBroadcast(\n+    triton::BroadcastOp broadcastOp, PtrState &state, const Location loc,\n+    ConversionPatternRewriter &rewriter,\n+    const llvm::SmallDenseMap<Value, PtrState> &knownPtrs) {\n+  assert(state.isEmpty());\n+\n+  auto src = broadcastOp.getSrc();\n+  auto dst = broadcastOp.getResult();\n+  assert(src.getType().isa<ShapedType>() &&\n+         \"input to tt.broadcast should be a tensor\");\n+\n+  auto srcShape = src.getType().cast<ShapedType>().getShape();\n+  auto dstShape = dst.getType().cast<ShapedType>().getShape();\n+  assert(srcShape.size() == dstShape.size() &&\n+         \"rank of source and destination should match\");\n+\n+  visitOperand(src, state, loc, rewriter, knownPtrs);\n+\n+  for (size_t i = 0; i < srcShape.size(); i++) {\n+    if (srcShape[i] == dstShape[i])\n+      continue;\n+    else if (srcShape[i] < dstShape[i])\n+      state.sizes[i] = rewriter.getIndexAttr(dstShape[i]);\n+    else\n+      llvm_unreachable(\"unexpected dimensions used in broadcast\");\n+  }\n+\n+  return;\n+}\n+\n+void PtrAnalysis::visitOperandSplat(\n+    triton::SplatOp splatOp, PtrState &state, const Location loc,\n+    ConversionPatternRewriter &rewriter,\n+    const llvm::SmallDenseMap<Value, PtrState> &knownPtrs) {\n+  assert(state.isEmpty());\n+\n+  auto src = splatOp.getSrc();\n+  auto dst = splatOp.getResult();\n+  auto dstShape = dst.getType().cast<ShapedType>().getShape();\n+\n+  visitOperand(src, state, loc, rewriter, knownPtrs);\n+\n+  if (src.getType().isa<IntegerType>() ||\n+      src.getType().isa<triton::PointerType>()) {\n+    for (auto s : dstShape) {\n+      state.offsets.push_back(rewriter.getIndexAttr(0));\n+      state.sizes.push_back(rewriter.getIndexAttr(s));\n+      state.strides.push_back(rewriter.getIndexAttr(0));\n+    }\n+  } else {\n+    // src is a memref that represent a scalar pointer; it should have one\n+    // dimension of size 1. This happens inside a for loop that originally has\n+    // an init arg that is a tensor of pointers; this arg would have been\n+    // replaced by rewriteForOp.\n+    auto srcType = src.getType().cast<MemRefType>();\n+    assert(srcType.getRank() == 1 && state.getRank() == 1 &&\n+           \"splat MemRef source should have rank 1\");\n+    assert(srcType.getShape()[0] == 1 &&\n+           getIntAttr(state.sizes[0]).value() == 1 &&\n+           \"splat MemRef source should have size 1\");\n+\n+    // Stride[0] will have value of 1 set in visitOperandAddPtr. This value will\n+    // be represented by a constOp. Clear this value.\n+    state.strides[0] = rewriter.getIndexAttr(0);\n+\n+    for (auto [i, s] : llvm::enumerate(dstShape)) {\n+      if (i == 0) {\n+        state.sizes[i] = rewriter.getIndexAttr(s);\n+        continue;\n+      }\n+      state.offsets.push_back(rewriter.getIndexAttr(0));\n+      state.sizes.push_back(rewriter.getIndexAttr(s));\n+      state.strides.push_back(rewriter.getIndexAttr(0));\n+    }\n+  }\n+\n+  // If we splat a integer value, scalar should become the offset of the outer\n+  // most dimension\n+  if (state.scalar)\n+    state.offsets[0] = state.scalar;\n+\n+  return;\n+}\n+\n+void PtrAnalysis::visitOperandAddptr(\n+    triton::AddPtrOp addptrOp, PtrState &state, const Location loc,\n+    ConversionPatternRewriter &rewriter,\n+    const llvm::SmallDenseMap<Value, PtrState> &knownPtrs) {\n+  assert(state.isEmpty());\n+\n+  PtrState ptrState;\n+  visitOperand(addptrOp.getPtr(), ptrState, addptrOp.getLoc(), rewriter,\n+               knownPtrs);\n+\n+  PtrState offsetState;\n+  visitOperand(addptrOp.getOffset(), offsetState, addptrOp.getLoc(), rewriter,\n+               knownPtrs);\n+\n+  assert(ptrState.source && \"ptr field should provide source / base pointer\");\n+\n+  // Handle the special case when we are in a for loop, ptr is originally a\n+  // scalar pointer but replaced with a memref. In this case, ptrState will have\n+  // rank 1 and offsetState will have rank 0.\n+  // TODO:\n+  //  Passing a block argument pointer directly into a for loop not supported\n+  if (ptrState.getRank() == 1 && offsetState.getRank() == 0) {\n+    offsetState.sizes.push_back(rewriter.getIndexAttr(1));\n+    offsetState.offsets.push_back(offsetState.scalar);\n+    offsetState.strides.push_back(rewriter.getIndexAttr(0));\n+  }\n+\n+  assert(ptrState.getRank() == offsetState.getRank() &&\n+         \"ptr and offset field should have the same rank\");\n+\n+  state.addState(ptrState, offsetState, addptrOp.getLoc(), rewriter);\n+}\n+\n+void PtrAnalysis::visitOperandReintCast(\n+    memref::ReinterpretCastOp reintCastOp, PtrState &state, const Location loc,\n+    ConversionPatternRewriter &rewriter,\n+    const llvm::SmallDenseMap<Value, PtrState> &knownPtrs) {\n+  assert(state.isEmpty());\n+\n+  state.offsets = reintCastOp.getMixedOffsets();\n+  state.sizes = reintCastOp.getMixedSizes();\n+  state.strides = reintCastOp.getMixedStrides();\n+  state.source = reintCastOp.getSource();\n+\n+  // getMixedOffsets produces staticOffsets (which is the result of collapsing\n+  // multiple dimensions). Populate the rest of the dimensions with zeroes.\n+  assert(state.offsets.size() == 1);\n+  for (size_t i = 1; i < state.sizes.size(); i++) {\n+    state.offsets.push_back(rewriter.getIndexAttr(0));\n+  }\n+}\n+\n+void PtrAnalysis::visitOperand(\n+    Value operand, PtrState &state, const Location loc,\n+    ConversionPatternRewriter &rewriter,\n+    const llvm::SmallDenseMap<Value, PtrState> &knownPtrs) {\n+\n+  if (knownPtrs.find(operand) != knownPtrs.end()) {\n+    state = knownPtrs.lookup(operand);\n+    return;\n+  }\n+\n+  if (operand.getType().isa<IntegerType>()) {\n+    auto castOp = rewriter.create<arith::IndexCastOp>(\n+        loc, rewriter.getIndexType(), operand);\n+    state.scalar = castOp.getResult();\n+    return;\n+  }\n+\n+  if (operand.getType().isa<triton::PointerType>()) {\n+    auto remappedPtr = rewriter.getRemappedValue(operand);\n+    assert(remappedPtr);\n+\n+    // A scalar pointer can either be produced by AddPtrOp or a block argument\n+    if (auto op = operand.getDefiningOp()) {\n+      assert(operand.getDefiningOp<triton::AddPtrOp>() &&\n+             \"Assume only addptr can produce a scalar pointer\");\n+      visitOperandAddptr(cast<triton::AddPtrOp>(op), state, loc, rewriter,\n+                         knownPtrs);\n+    } else {\n+      state.source = remappedPtr;\n+    }\n+    return;\n+  }\n+\n+  if (auto op = operand.getDefiningOp<arith::AddIOp>()) {\n+    visitOperandAdd(op, state, loc, rewriter, knownPtrs);\n+  } else if (auto op = operand.getDefiningOp<arith::MulIOp>()) {\n+    visitOperandMul(op, state, loc, rewriter, knownPtrs);\n+  } else if (auto op = operand.getDefiningOp<triton::MakeRangeOp>()) {\n+    visitOperandMakeRange(op, state, loc, rewriter, knownPtrs);\n+  } else if (auto op = operand.getDefiningOp<triton::BroadcastOp>()) {\n+    visitOperandBroadcast(op, state, loc, rewriter, knownPtrs);\n+  } else if (auto op = operand.getDefiningOp<triton::SplatOp>()) {\n+    visitOperandSplat(op, state, loc, rewriter, knownPtrs);\n+  } else if (auto op = operand.getDefiningOp<triton::ExpandDimsOp>()) {\n+    visitOperandExpandDims(op, state, loc, rewriter, knownPtrs);\n+  } else if (auto op = operand.getDefiningOp<triton::AddPtrOp>()) {\n+    visitOperandAddptr(op, state, loc, rewriter, knownPtrs);\n+  } else if (auto op = operand.getDefiningOp<arith::ConstantOp>()) {\n+    visitOperandConstSplat(op, state, loc, rewriter, knownPtrs);\n+  } else {\n+    operand.getDefiningOp()->dump();\n+    llvm_unreachable(\"encountered addptr operand produced by an \"\n+                     \"unsupported operation\");\n+  }\n+}\n+\n+void PtrAnalysis::visitOperandConstSplat(\n+    arith::ConstantOp op, PtrState &state, const Location loc,\n+    ConversionPatternRewriter &rewriter,\n+    const llvm::SmallDenseMap<Value, PtrState> &knownPtrs) {\n+  assert(state.isEmpty());\n+  // this condition is to handle cases where tt.broadcast and tt.splat are\n+  // folded\n+  auto attr = cast<DenseElementsAttr>(op.getValue());\n+  auto elementType = attr.getElementType();\n+  assert(attr.isSplat() && elementType.isa<IntegerType>());\n+  auto values = attr.getValues<IntegerAttr>();\n+  auto value = values[0].getValue();\n+  auto constAttr = rewriter.getIndexAttr(value.getSExtValue());\n+  auto constOp = rewriter.create<arith::ConstantOp>(loc, constAttr,\n+                                                    rewriter.getIndexType());\n+\n+  state.scalar = constOp;\n+\n+  auto resultType = cast<ShapedType>(op.getResult().getType());\n+  for (auto i = 0; i < resultType.getShape().size(); i++) {\n+    if (i == 0) {\n+      state.offsets.push_back(constOp.getResult());\n+    } else {\n+      state.offsets.push_back(rewriter.getIndexAttr(0));\n+    }\n+\n+    state.sizes.push_back(rewriter.getIndexAttr(resultType.getShape()[i]));\n+    state.strides.push_back(rewriter.getIndexAttr(0));\n+  }\n+}\n+\n+void PtrAnalysis::rewriteAddptrOp(\n+    triton::AddPtrOp op, ConversionPatternRewriter &rewriter,\n+    llvm::SmallDenseMap<Value, PtrState> &knownPtrs) {\n+  // any inserted instruction should be before this addptr\n+  auto origIp = rewriter.saveInsertionPoint();\n+  rewriter.setInsertionPoint(op);\n+\n+  PtrState state;\n+  visitOperandAddptr(op, state, op.getLoc(), rewriter, knownPtrs);\n+\n+  // If the result is a scalar pointer, visitOperandAddptr will not populate\n+  // sizes, strides, and offsets. We need to do it here.\n+  if (state.sizes.size() == 0) {\n+    state.sizes.push_back(rewriter.getIndexAttr(1));\n+    state.strides.push_back(rewriter.getIndexAttr(0));\n+    state.offsets.push_back(state.scalar);\n+  }\n+\n+  SmallVector<int64_t> scalarShape(1, 1);\n+  ArrayRef<int64_t> resultShape;\n+  if (auto shapedType = op.getResult().getType().dyn_cast<ShapedType>()) {\n+    resultShape = shapedType.getShape();\n+  } else {\n+    // scalar pointer, should produce a one dimensional memref\n+    resultShape = scalarShape;\n+    assert(state.getRank() == 1);\n+  }\n+\n+  // If there are dimensions with size 1 and stride 0, replace stride 0 with 1\n+  // so inferResultType below works as expected.\n+  for (size_t i = 0; i < state.sizes.size(); i++) {\n+    auto strideIntAttr = getIntAttr(state.strides[i]);\n+    auto sizeIntAttr = getIntAttr(state.sizes[i]);\n+\n+    if (!strideIntAttr || strideIntAttr != 0)\n+      continue;\n+\n+    if (sizeIntAttr && sizeIntAttr.value() == 1)\n+      state.strides[i] = rewriter.getIndexAttr(1);\n+  }\n+\n+  auto castOp = state.createCastOp(resultShape, op.getLoc(), rewriter);\n+  LLVM_DEBUG({\n+    llvm::dbgs() << \"cast MemRefType:\\n\";\n+    castOp.getOperation()->print(llvm::dbgs(),\n+                                 OpPrintingFlags().printGenericOpForm());\n+    llvm::dbgs() << \"\\n\";\n+  });\n+\n+  state.source = castOp.getResult();\n+  rewriter.replaceOp(op, castOp.getResult());\n+\n+  knownPtrs[op.getResult()] = state;\n+\n+  rewriter.restoreInsertionPoint(origIp);\n+}\n+\n+void PtrAnalysis::rewriteYieldOp(\n+    scf::YieldOp op, ConversionPatternRewriter &rewriter,\n+    const IndexMapSet &levelToBlockArgIndex, const int level,\n+    const llvm::SmallDenseMap<Value, PtrState> &knownPtrs) {\n+  // any inserted instruction should be before this yield\n+  OpBuilder::InsertionGuard insertionGuard{rewriter};\n+  rewriter.setInsertionPoint(op);\n+\n+  auto adaptor = scf::YieldOp::Adaptor(op);\n+\n+  SmallVector<PtrState> initArgState;\n+  SmallVector<Value> operands(adaptor.getOperands());\n+\n+  // For each of the init arg that we added additional Values in for loop, we\n+  // need to add corresponding Values as yield operands. The loop below gathers\n+  // PtrState for those values.\n+  for (auto [i, v] : llvm::enumerate(adaptor.getOperands())) {\n+    if (auto mappedV = rewriter.getRemappedValue(v)) {\n+      // If this value is a tensor of pointers produced by AddPtrOp,\n+      // TritonTypeConverter should have converted to MemRefType without layout\n+      // information. Since it doesn't match with the MemRefType that we\n+      // produced in rewriteAddptrOp (which is in canonical form with layout\n+      // information), an unrealized_conversion_cast should have been added. We\n+      // need to trace it back through this unrealized_conversion_cast to get\n+      // the original reinterpret_cast. Also see comments in\n+      // TritonTypeConverter::addConversion.\n+      //\n+      // For TritonToLinalg, we do not use any TypeConverters, hence we can\n+      // access the reinterpret_cast directly.\n+      if (v.getDefiningOp<triton::AddPtrOp>()) {\n+        if (auto castOp = mappedV.getDefiningOp<UnrealizedConversionCastOp>()) {\n+          auto castInputs = castOp.getInputs();\n+          assert(castInputs.size() == 1 &&\n+                 \"only expect 1:1 mapping for unrealized_conversion_cast that \"\n+                 \"were \"\n+                 \"automatically inserted during legalizing\");\n+          v = castInputs[0];\n+        } else if (auto castOp =\n+                       mappedV.getDefiningOp<memref::ReinterpretCastOp>()) {\n+          v = castOp;\n+        } else {\n+          llvm_unreachable(\"mapped value defined by an unexpected op\");\n+        }\n+      } else {\n+        // If this value is not a tensor of pointers, we will use the mapped\n+        // value, and rely on the conversion will happen later automatically\n+        // when we legalize loop body.\n+\n+        // TODO:\n+        //  The scenario where a value is a tensor of pointers but not produced\n+        //  by AddPtrOp is not supported\n+        if (mappedV.getType().isa<TensorType>() &&\n+            mappedV.getType()\n+                .dyn_cast<TensorType>()\n+                .getElementType()\n+                .isa<triton::PointerType>())\n+          llvm_unreachable(\"unsupported scenario where a value is a tensor of \"\n+                           \"pointers but not produced by AddPtrOp\");\n+        v = mappedV;\n+      }\n+    }\n+\n+    if (levelToBlockArgIndex.find(level) == levelToBlockArgIndex.end())\n+      continue;\n+    auto thisSet = levelToBlockArgIndex.find(level)->second;\n+    if (thisSet.find(i) == thisSet.end())\n+      continue;\n+\n+    auto reintCastOp = v.getDefiningOp<memref::ReinterpretCastOp>();\n+    assert(\n+        reintCastOp ||\n+        (v.getType().isa<TensorType>() &&\n+         v.getType().dyn_cast<TensorType>().getElementType().isa<IndexType>()));\n+\n+    PtrState state;\n+    if (reintCastOp) {\n+      visitOperandReintCast(reintCastOp, state, op.getLoc(), rewriter,\n+                            knownPtrs);\n+    } else {\n+      visitOperand(v, state, op.getLoc(), rewriter, knownPtrs);\n+    }\n+    initArgState.push_back(state);\n+  }\n+\n+  // For each of the PtrState recorded in the last step, extract value\n+  // that correspond to offset and stride for each dimension and append\n+  // them to yield operands.\n+  for (auto state : initArgState) {\n+    for (auto s : state.offsets) {\n+      // offsets can be IntAttr zeroes, since reinterpret_cast collapses them\n+      // for the input memref, and the for loop may not update offsets other\n+      // than offsets[0]. Create constants Values for those zeroes.\n+      if (auto sIntAttr = getIntAttr(s)) {\n+        assert(sIntAttr.value() == 0 && \"attribute offsets should be zeroes\");\n+        auto constOp = rewriter.create<arith::ConstantOp>(\n+            op.getLoc(), rewriter.getIndexAttr(0));\n+        operands.push_back(constOp.getResult());\n+      } else {\n+        operands.push_back(s.get<Value>());\n+      }\n+    }\n+\n+    for (auto s : state.strides) {\n+      assert(!getIntAttr(s) &&\n+             \"PtrState strides for yield within for loop not expected to be \"\n+             \"attribute.\");\n+      operands.push_back(s.get<Value>());\n+    }\n+  }\n+\n+  // Yield is a terminator op that must be at the end of the function\n+  rewriter.setInsertionPointAfter(op);\n+  auto newOp = rewriter.replaceOpWithNewOp<scf::YieldOp>(op, operands);\n+  assert(op->getNumResults() == 0);\n+\n+  LLVM_DEBUG({\n+    llvm::dbgs() << \"new yield:\";\n+    newOp.getOperation()->print(llvm::dbgs(),\n+                                OpPrintingFlags().printGenericOpForm());\n+    llvm::dbgs() << \"\\n\";\n+  });\n+}\n+\n+void PtrAnalysis::rewriteForOp(\n+    scf::ForOp op, ConversionPatternRewriter &rewriter,\n+    IndexMapSet &levelToBlockArgIndex, const int level,\n+    llvm::SmallDenseMap<Value, PtrState> &knownPtrs) {\n+  SmallVector<Value> newInitArgs;\n+\n+  SmallVector<std::pair<int, PtrState>> initArgIndexState;\n+  SmallVector<std::pair<int, PtrState>> knownPtrsTmp;\n+\n+  // Create a new list of init args\n+  for (auto [i, arg] : llvm::enumerate(op.getInitArgs())) {\n+    auto mappedV = rewriter.getRemappedValue(arg);\n+\n+    // Trace back the original value. See comments in rewriteYieldOp.\n+    // This block is unreachable for TritonToLinalg because we don't use\n+    // TypeConverters.\n+    if (mappedV && mappedV.getDefiningOp<UnrealizedConversionCastOp>()) {\n+      auto castOp = mappedV.getDefiningOp<UnrealizedConversionCastOp>();\n+      assert(castOp && \"expected unrealized_conversion_cast\");\n+      auto castInputs = castOp.getInputs();\n+      assert(castInputs.size() == 1 &&\n+             \"only expect 1:1 mapping for unrealized_conversion_cast that were \"\n+             \"automatically inserted during legalizing\");\n+      mappedV = castInputs[0];\n+    }\n+\n+    memref::ReinterpretCastOp reintCastOp;\n+\n+    // If this init arg is supposed to be remapped, use the remapped value\n+    // instead. In addition, if this init arg is a memref created by a\n+    // reinterpret_cast or a tensor of index, there is a chance that it will be\n+    // used in addptr. Create PtrState for each such init arg.\n+    if (mappedV) {\n+      // TODO:\n+      //  Passing a block argument pointer directly into a for loop not\n+      assert(!(mappedV.dyn_cast<BlockArgument>() &&\n+               mappedV.getType().isa<UnrankedMemRefType>()) &&\n+             \"cannot take pointer block argument as init arg for for loop\");\n+      reintCastOp = mappedV.getDefiningOp<memref::ReinterpretCastOp>();\n+      newInitArgs.push_back(mappedV);\n+    } else {\n+      newInitArgs.push_back(arg);\n+    }\n+\n+    auto indexTensor =\n+        arg.getType().isa<TensorType>() &&\n+        arg.getType().dyn_cast<TensorType>().getElementType().isa<IndexType>();\n+\n+    if (!reintCastOp && !indexTensor)\n+      continue;\n+\n+    PtrState state;\n+    if (reintCastOp) {\n+      visitOperandReintCast(reintCastOp, state, op.getLoc(), rewriter,\n+                            llvm::SmallDenseMap<Value, PtrState>(0));\n+    } else {\n+      // TODO:\n+      visitOperand(arg, state, op.getLoc(), rewriter,\n+                   llvm::SmallDenseMap<Value, PtrState>(0));\n+    }\n+\n+    // Record the PtrState for later processing\n+    initArgIndexState.push_back(std::make_pair(i, state));\n+  }\n+\n+  // Set insertion point to be before the for loop for new variables passed\n+  // into the new loop.\n+  auto origIp = rewriter.saveInsertionPoint();\n+  rewriter.setInsertionPoint(op);\n+\n+  // For each of the PtrState recorded in the last step, insert new\n+  // instructions to describe offset and stride for each dimension and append\n+  // them to init args\n+  for (auto [i, state] : initArgIndexState) {\n+    // For each dimension, if the corresponding offset and stride is an\n+    // integer attribute, create a constant value and append them at the end\n+    // of init arg list.\n+    for (auto [j, s] : llvm::enumerate(state.offsets)) {\n+      auto sIntAttr = getIntAttr(s);\n+      if (sIntAttr) {\n+        auto constOp = rewriter.create<arith::ConstantOp>(\n+            op.getLoc(), rewriter.getIndexAttr(sIntAttr.value()));\n+        newInitArgs.push_back(constOp.getResult());\n+        state.offsets[j] = constOp.getResult();\n+      } else {\n+        newInitArgs.push_back(s.get<Value>());\n+      }\n+    }\n+\n+    for (auto [j, s] : llvm::enumerate(state.strides)) {\n+      auto sIntAttr = getIntAttr(s);\n+      if (sIntAttr) {\n+        auto constOp = rewriter.create<arith::ConstantOp>(\n+            op.getLoc(), rewriter.getIndexAttr(sIntAttr.value()));\n+        newInitArgs.push_back(constOp.getResult());\n+        state.strides[j] = constOp.getResult();\n+      } else {\n+        newInitArgs.push_back(s.get<Value>());\n+      }\n+    }\n+\n+    // Note that we want the knownPtrs to be indexed by block arg, but we only\n+    // have index for now. Also, the state we record is the init arg, but want\n+    // to to use newly created block arg. These block args are not created yet.\n+    // We will translate this mapping later.\n+    knownPtrsTmp.push_back(std::make_pair(i, state));\n+    levelToBlockArgIndex[level].insert(i);\n+\n+    // If the original init arg is a memref produced by reinterpret_cast, create\n+    // a new memref using new strides and offsets created above. This produces a\n+    // canonicalized memref, which will match what the for loop generates if it\n+    // modifies the memref. E.g., original reinterpret_cast can produce a memref\n+    // with const stride:\n+    //  - memref<4x256xbf16, affine_map<(d0, d1)[s0, s1] -> (d0 * 256 + s0 + d1\n+    //  * s1)>>\n+    // The new reinterpret_cast will always have dynamic stride and offset:\n+    //  - memref<4x256xbf16, affine_map<(d0, d1)[s0, s1, s2] -> (d0 * s1 + s0 +\n+    //  d1 * s2)>>\n+    if (auto reintCastOp =\n+            newInitArgs[i].getDefiningOp<memref::ReinterpretCastOp>()) {\n+      SmallVector<int64_t> resultShape;\n+      for (auto s : state.sizes) {\n+        auto sIntAttr = getIntAttr(s);\n+        assert(sIntAttr && \"expected constant size\");\n+        resultShape.push_back(sIntAttr.value());\n+      }\n+      auto castOp = state.createCastOp(resultShape, op.getLoc(), rewriter);\n+\n+      LLVM_DEBUG({\n+        llvm::dbgs() << \"new reinterpret_cast with dynamic sizes \"\n+                        \"and offsets:\";\n+        castOp->print(llvm::dbgs(), OpPrintingFlags().printGenericOpForm());\n+        llvm::dbgs() << \"\\n\";\n+      });\n+\n+      newInitArgs[i] = castOp.getResult();\n+    }\n+  }\n+\n+  rewriter.restoreInsertionPoint(origIp);\n+\n+  // create a new scf::ForOp that uses updated init args and same loop body\n+  auto newOp = rewriter.create<scf::ForOp>(\n+      op.getLoc(), op.getLowerBound(), op.getUpperBound(), op.getStep(),\n+      newInitArgs, [&](OpBuilder &b, Location loc, Value iv, ValueRange args) {\n+        IRMapping mapping;\n+        mapping.map(op.getInductionVar(), iv);\n+        mapping.map(op.getInitArgs(), newInitArgs);\n+        mapping.map(op.getRegionIterArgs(), args);\n+        for (auto &bodyOp : op.getLoopBody().getOps()) {\n+          b.clone(bodyOp, mapping);\n+        }\n+      });\n+\n+  // Convert the book-keeping data structure to use the correct key and value.\n+  // Key is converted from init arg index to newly created block arg, and\n+  // Value's PtrState fields are converted from init arg to newly created block\n+  // arg\n+  int cnt = op.getRegionIterArgs().size();\n+  for (auto [i, state] : knownPtrsTmp) {\n+    for (auto it = state.offsets.begin(); it != state.offsets.end(); it++) {\n+      *it = newOp.getRegionIterArgs()[cnt];\n+      cnt++;\n+    }\n+\n+    for (auto it = state.strides.begin(); it != state.strides.end(); it++) {\n+      *it = newOp.getRegionIterArgs()[cnt];\n+      cnt++;\n+    }\n+\n+    auto key = newOp.getRegionIterArgs()[i];\n+    knownPtrs.insert(std::make_pair(key, state));\n+  }\n+  assert(static_cast<size_t>(cnt) == newOp.getRegionIterArgs().size() &&\n+         \"expect to remap all new block args\");\n+\n+  // replace only the results that correspond to the original scf.for\n+  auto resultsToReplaceWith = ResultRange(\n+      newOp.result_begin(), newOp.result_begin() + op.getNumResults());\n+  rewriter.replaceOp(op, resultsToReplaceWith);\n+\n+  // Update the loop body. Manually invoke the rewrite logic on addptr and yield\n+  // in the loop body, so we can take advantage of the states we built up\n+  for (auto &bodyOp : newOp.getLoopBody().getOps()) {\n+    if (auto addptrOp = dyn_cast<triton::AddPtrOp>(bodyOp)) {\n+      rewriteAddptrOp(addptrOp, rewriter, knownPtrs);\n+    } else if (auto forOp = dyn_cast<scf::ForOp>(bodyOp)) {\n+      // TODO:\n+      //  Nested for loops are not supported at the moment\n+      assert(0 && \"nested loops currently not supported\");\n+      // rewriteForOp(forOp, rewriter, levelToBlockArgIndex, level+1,\n+      // knownPtrs); levelToBlockArgIndex.erase(level+1);\n+    }\n+  }\n+\n+  if (op.getNumRegionIterArgs()) {\n+    auto yieldOp = cast<scf::YieldOp>(newOp.getBody()->getTerminator());\n+    rewriteYieldOp(yieldOp, rewriter, levelToBlockArgIndex, level, knownPtrs);\n+  }\n+\n+  LLVM_DEBUG({\n+    llvm::dbgs() << \"new for\\n\";\n+    newOp.getOperation()->print(llvm::dbgs(),\n+                                OpPrintingFlags().printGenericOpForm());\n+    llvm::dbgs() << \"\\n\";\n+  });\n+}\n+\n+Value PtrAnalysis::getScalarMemRef(Value ptr, Value memRef, const Location loc,\n+                                   ConversionPatternRewriter &rewriter) {\n+  assert(ptr.getType().cast<triton::PointerType>() &&\n+         \"expected scalar pointer\");\n+\n+  // If pointer is generated by tt.addptr, TypeConverter will have inserted an\n+  // unrealized conversion cast for ptr to cast its type from tt.ptr to unranked\n+  // memref. Input of this cast is the actual source memref.\n+  //\n+  // For TritonToLinalg, we can access the reinterpret_cast directly due to no\n+  // usages of TypeConverters.\n+  if (ptr.getDefiningOp<triton::AddPtrOp>()) {\n+    if (auto uCast = memRef.getDefiningOp<UnrealizedConversionCastOp>()) {\n+      assert(uCast && \"expected unrealized conversion inserted by type \"\n+                      \"converter not found\");\n+      return uCast.getInputs()[0];\n+    } else if (auto castOp =\n+                   memRef.getDefiningOp<memref::ReinterpretCastOp>()) {\n+      return castOp.getResult();\n+    } else {\n+      llvm_unreachable(\"pointer value is defined by an unexpected op\");\n+    }\n+  }\n+\n+  assert(isa<BlockArgument>(ptr) &&\n+         \"pointer is neither produced by addptr nor a block argument\");\n+  PtrState state;\n+  state.source = memRef;\n+  state.offsets.push_back(rewriter.getIndexAttr(0));\n+  state.sizes.push_back(rewriter.getIndexAttr(1));\n+  state.strides.push_back(rewriter.getIndexAttr(1));\n+  auto castOp = state.createCastOp(SmallVector<int64_t>(1, 1), loc, rewriter);\n+  return castOp.getResult();\n+}\n+\n+} // namespace triton\n+} // namespace mlir"}, {"filename": "lib/Analysis/UseAnalysis.cpp", "status": "added", "additions": 217, "deletions": 0, "changes": 217, "file_content_changes": "@@ -0,0 +1,217 @@\n+//===----------------------------------------------------------------------===//\n+//\n+// Copyright (c) Triton Project Contributors.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#include \"triton/Analysis/UseAnalysis.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+\n+#include \"mlir/Analysis/DataFlow/ConstantPropagationAnalysis.h\"\n+#include \"mlir/Analysis/DataFlow/DeadCodeAnalysis.h\"\n+\n+#include \"llvm/ADT/TypeSwitch.h\"\n+#include \"llvm/Support/Debug.h\"\n+\n+using namespace mlir;\n+using namespace triton;\n+using namespace dataflow;\n+\n+#define DEBUG_TYPE \"triton-use-analysis\"\n+\n+//===----------------------------------------------------------------------===//\n+// Use Analysis\n+// Note that logic below should evolve with triton-to-affine pass\n+//===----------------------------------------------------------------------===//\n+void triton::UseAnalysis::visitOperation(Operation *op,\n+                                         ArrayRef<UseInfo *> operands,\n+                                         ArrayRef<const UseInfo *> results) {\n+  // If an op only produces pointer, all its operands are used as meta data.\n+  // This accounts for scenarios such as addptr in a loop whose result is\n+  // yielded. In this case, if the loop returns data tensors, addptr will be\n+  // marked correctly as meta use.\n+  if (op->getResults().size() == 1) {\n+    auto resultType = op->getResult(0).getType().dyn_cast<ShapedType>();\n+    if (resultType && isa<triton::PointerType>(resultType.getElementType())) {\n+      for (auto opnd : operands)\n+        propagateUse(opnd, UseType::MetaUse);\n+    }\n+  }\n+\n+  TypeSwitch<Operation *>(op)\n+      .Case<triton::LoadOp>([&](auto load) {\n+        propagateUse(operands[0], UseType::MetaUse);\n+        auto mask = load.getMask();\n+        auto other = load.getOther();\n+        if (mask) {\n+          assert(mask != other && \"mask and other cannot be the same\");\n+          propagateUse(operands[1], UseType::MetaUse);\n+        }\n+        if (other) {\n+          // TODO:\n+          // More complicated patterns that generate other is unsupported.\n+          propagateUse(operands[2], UseType::MetaUse);\n+        }\n+      })\n+      .Case<triton::StoreOp>([&](auto store) {\n+        propagateUse(operands[0], UseType::MetaUse);\n+        propagateUse(operands[1], UseType::DataUse);\n+        auto value = store.getValue();\n+        auto mask = store.getMask();\n+        if (mask) {\n+          assert(mask != value && \"mask and data cannot be the same\");\n+          propagateUse(operands[2], UseType::MetaUse);\n+        }\n+      })\n+      .Case<triton::DotOp>([&](auto dot) {\n+        propagateResults(operands[0], results);\n+        propagateResults(operands[1], results);\n+\n+        auto opc = dot.getC();\n+        triton::SplatOp splat;\n+        if (opc)\n+          splat = opc.template getDefiningOp<triton::SplatOp>();\n+\n+        if (opc && splat && splat.getSrc().getDefiningOp<arith::ConstantOp>())\n+          propagateUse(operands[2], UseType::MetaUse);\n+        else\n+          propagateUse(operands[2], UseType::DataUse);\n+      })\n+      .Default([&](Operation *op) {\n+        // this condition account for tt.addptr\n+        for (auto operand : operands) {\n+          propagateResults(operand, results);\n+        }\n+      });\n+}\n+\n+LogicalResult triton::runUseAnalysis(triton::FuncOp &funcOp) {\n+  MLIRContext *context = funcOp.getContext();\n+  SymbolTableCollection symbolTable;\n+\n+  DataFlowSolver solver;\n+  solver.load<DeadCodeAnalysis>();\n+  solver.load<SparseConstantPropagation>();\n+  solver.load<UseAnalysis>(symbolTable);\n+  if (failed(solver.initializeAndRun(funcOp)))\n+    return failure();\n+\n+  // Walk the func op, convert tags on operands to tags on operations\n+  funcOp.walk([&](Operation *op) {\n+    UseType useType = UseType::Undefined;\n+    for (auto result : op->getResults()) {\n+      auto use = solver.lookupState<UseInfo>(result);\n+      assert(use && \"Lattice value not found\");\n+      auto thisUseType = use->type;\n+      if (thisUseType == UseType::Undefined)\n+        continue;\n+      if (useType == UseType::Undefined)\n+        useType = thisUseType;\n+      if (thisUseType == UseType::MixUse || thisUseType != useType) {\n+        useType = UseType::MixUse;\n+        break;\n+      }\n+    }\n+\n+    if (useType == UseType::Undefined) {\n+      LLVM_DEBUG({ op->setAttr(\"Undefined\", UnitAttr::get(context)); });\n+      return;\n+    } else if (useType == UseType::MetaUse) {\n+      assert(op->getNumResults() == 1 &&\n+             \"Ops used for meta computation are expected to have one result\");\n+      // Only set the tag if the operation uses tensors\n+      if (op->getResult(0).getType().isa<ShapedType>()) {\n+        // Setting tag for erasing op later\n+        op->setAttr(\"MetaUse\", UnitAttr::get(context));\n+      }\n+      return;\n+    } else if (useType == UseType::DataUse) {\n+      LLVM_DEBUG({ op->setAttr(\"DataUse\", UnitAttr::get(context)); });\n+      return;\n+    }\n+\n+    assert(useType == UseType::MixUse);\n+\n+    // If the operation only produces scalars, no need to clone it\n+    bool shapedResult = true;\n+    for (auto result : op->getResults())\n+      shapedResult &= result.getType().isa<ShapedType>();\n+    if (!shapedResult) {\n+      LLVM_DEBUG({ op->setAttr(\"MixUse\", UnitAttr::get(context)); });\n+      return;\n+    }\n+\n+    // Value has MixUse. However, the operation may or may not have direct\n+    // MetaUse. E.g., it may only have MixUse, or only have MixUse and\n+    // DataUse.\n+    // - If the operation has direct MetaUse, clone it, tag the clone as\n+    // MetaUse only and point meta users to use the clone.\n+    // - If not, do nothing; this operation will still be materlized.\n+    llvm::SetVector<Operation *> metaUsers;\n+    for (auto result : op->getResults()) {\n+      for (auto user : result.getUsers()) {\n+        TypeSwitch<Operation *>(user)\n+            .Case<triton::LoadOp>([&](auto load) {\n+              auto ptr = load.getPtr();\n+              auto mask = load.getMask();\n+              auto other = load.getOther();\n+              if (result == ptr || result == mask || result == other)\n+                metaUsers.insert(user);\n+            })\n+            .Case<triton::StoreOp>([&](auto store) {\n+              auto ptr = store.getPtr();\n+              auto mask = store.getMask();\n+              if (result == ptr || result == mask)\n+                metaUsers.insert(user);\n+            })\n+            .Case<triton::DotOp>([&](auto dot) {\n+              auto opc = dot.getC();\n+              triton::SplatOp splat;\n+              if (opc)\n+                splat = opc.template getDefiningOp<triton::SplatOp>();\n+\n+              if (opc && splat &&\n+                  splat.getSrc().getDefiningOp<arith::ConstantOp>())\n+                metaUsers.insert(user);\n+            })\n+            .Default([&](Operation *op) {\n+              // if all output of user are used as meta data, user is a meta\n+              // user. This condition account for addptr, or an addi whose\n+              // output only feeds into addptr\n+              bool allMeta = true;\n+              for (auto res : op->getResults()) {\n+                auto resUse = solver.lookupState<UseInfo>(res);\n+                if (resUse->type != UseType::MetaUse) {\n+                  allMeta = false;\n+                  break;\n+                }\n+              }\n+              if (allMeta)\n+                metaUsers.insert(user);\n+            });\n+      }\n+    }\n+\n+    // If the operation doesn't have direct meta users, no need to clone it\n+    if (metaUsers.empty()) {\n+      LLVM_DEBUG({ op->setAttr(\"MixUse\", UnitAttr::get(context)); });\n+      return;\n+    }\n+\n+    // Clone the operation; switch all meta users to use the clone\n+    OpBuilder builder(op);\n+    auto clone = builder.clone(*op);\n+    LLVM_DEBUG({ op->setAttr(\"MixUse\", UnitAttr::get(context)); });\n+\n+    // Setting tag for erasing op later\n+    clone->setAttr(\"MetaUse\", UnitAttr::get(context));\n+\n+    for (auto [res_i, result] : llvm::enumerate(op->getResults()))\n+      for (auto user : metaUsers)\n+        for (auto &operand : user->getOpOperands())\n+          if (operand.get() == result)\n+            operand.set(clone->getResult(res_i));\n+  });\n+\n+  return success();\n+}"}, {"filename": "lib/Conversion/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,2 +1,3 @@\n add_subdirectory(TritonToTritonGPU)\n add_subdirectory(TritonGPUToLLVM)\n+add_subdirectory(TritonToLinalg)\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonToLinalg/CMakeLists.txt", "status": "added", "additions": 33, "deletions": 0, "changes": 33, "file_content_changes": "@@ -0,0 +1,33 @@\n+#===------------------------------------------------------------------------===#\n+#\n+# Copyright (c) Triton Project Contributors.\n+#\n+#===------------------------------------------------------------------------===#\n+\n+add_mlir_conversion_library(TritonToLinalg\n+  TritonToLinalg.cpp\n+  TritonToLinalgPass.cpp\n+\n+  ADDITIONAL_HEADER_DIRS\n+  ${PROJECT_SOURCE_DIR}/include/triton/Conversion/TritonToLinalg\n+  ${PROJECT_BINARY_DIR}/include/triton/Conversion/TritonToLinalg\n+\n+  DEPENDS\n+  TritonToLinalgConversionPassIncGen\n+\n+  LINK_COMPONENTS\n+  Core\n+\n+  LINK_LIBS PUBLIC\n+  MLIRArithDialect\n+  MLIRDialectUtils\n+  MLIRIR\n+  MLIRMathDialect\n+  MLIRPass\n+  MLIRTensorDialect\n+  MLIRTransforms\n+  MLIRSupport\n+  TritonAnalysis\n+  TritonIR\n+  TritonTransforms\n+)"}, {"filename": "lib/Conversion/TritonToLinalg/TritonToLinalg.cpp", "status": "added", "additions": 923, "deletions": 0, "changes": 923, "file_content_changes": "@@ -0,0 +1,923 @@\n+//===----------------------------------------------------------------------===//\n+//\n+// Copyright (c) Triton Project Contributors.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#include \"triton/Conversion/TritonToLinalg/TritonToLinalg.h\"\n+#include \"triton/Analysis/MaskAnalysis.h\"\n+#include \"triton/Analysis/PtrAnalysis.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+\n+#include \"mlir/Dialect/Affine/IR/AffineOps.h\"\n+#include \"mlir/Dialect/Bufferization/IR/Bufferization.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n+#include \"mlir/Dialect/Linalg/IR/Linalg.h\"\n+#include \"mlir/Dialect/Linalg/Passes.h\"\n+\n+#include \"llvm/ADT/TypeSwitch.h\"\n+#include \"llvm/Support/FormatVariadic.h\"\n+\n+#include <numeric>\n+\n+using namespace mlir;\n+using namespace triton;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Conversion/TritonToLinalg/Passes.h.inc\"\n+\n+//===----------------------------------------------------------------------===//\n+// Utilities\n+//===----------------------------------------------------------------------===//\n+\n+// Extract a scalar value from v.\n+// If v is a scalar, return that directly. Otherwise, parse through operations\n+// (currently only support splat and sitofp) that produce it and to extract they\n+// underlying scalar value . If no scalar value can be extracted, a nullptr is\n+// returned.\n+static std::optional<Value>\n+getScalarValue(Value v, Location loc, ConversionPatternRewriter &rewriter) {\n+  // Record if an sitofp op was in the chain of ops that produce the scalar\n+  Operation *siToFp = nullptr;\n+\n+  while (true) {\n+    if (!v.getType().dyn_cast<ShapedType>()) {\n+      break;\n+    } else if (auto op = v.getDefiningOp<arith::ConstantOp>()) {\n+      if (auto attr = op.getValue().dyn_cast<DenseElementsAttr>()) {\n+        if (!attr.isSplat()) {\n+          InFlightDiagnostic diag = emitError(loc)\n+                                    << \"other value used in masked load \"\n+                                       \"produced by unsupported instruction\";\n+          return nullptr;\n+        }\n+        auto elemValue = attr.getSplatValue<Attribute>();\n+        auto constOp =\n+            rewriter.create<arith::ConstantOp>(op.getLoc(), elemValue);\n+        v = constOp.getResult();\n+      }\n+    } else if (auto op = v.getDefiningOp<triton::SplatOp>()) {\n+      v = op.getSrc();\n+    } else if (auto op = v.getDefiningOp<arith::SIToFPOp>()) {\n+      siToFp = op;\n+      v = op.getIn();\n+    } else {\n+      InFlightDiagnostic diag = emitError(loc)\n+                                << \"other value used in masked load produced \"\n+                                   \"by unsupported instruction\";\n+      return nullptr;\n+    }\n+  }\n+\n+  if (siToFp) {\n+    auto resType = siToFp->getResult(0).getType();\n+    if (auto shapedType = dyn_cast<ShapedType>(resType)) {\n+      resType = shapedType.getElementType();\n+    }\n+    return rewriter.create<arith::SIToFPOp>(loc, resType, v);\n+  }\n+  return v;\n+}\n+\n+static SmallVector<utils::IteratorType> getNParallelLoopsAttrs(unsigned n) {\n+  return SmallVector<utils::IteratorType>(n, utils::IteratorType::parallel);\n+}\n+\n+static Value getTransposedValue(Value source, const Location loc,\n+                                ConversionPatternRewriter &rewriter) {\n+\n+  auto sourceType = source.getType().cast<RankedTensorType>();\n+  auto sourceRank = sourceType.getRank();\n+\n+  SmallVector<int64_t> perm(sourceRank);\n+  std::iota(std::begin(perm), std::end(perm), 0);\n+  std::swap(perm[sourceRank - 1], perm[sourceRank - 2]);\n+\n+  SmallVector<int64_t> transposedShape(sourceType.getShape());\n+  std::swap(transposedShape[sourceRank - 1], transposedShape[sourceRank - 2]);\n+\n+  Value transposeInit = rewriter.create<tensor::EmptyOp>(\n+      loc, transposedShape, sourceType.getElementType());\n+\n+  Value transpose =\n+      rewriter.create<linalg::TransposeOp>(loc, source, transposeInit, perm)\n+          .getResults()[0];\n+\n+  return transpose;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// Op Lowering Patterns\n+//===----------------------------------------------------------------------===//\n+\n+namespace {\n+\n+struct SplatConverter : public OpConversionPattern<triton::SplatOp> {\n+  using OpConversionPattern<triton::SplatOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::SplatOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto opType = op.getType().cast<TensorType>();\n+    auto loc = op.getLoc();\n+\n+    auto init = rewriter.create<tensor::EmptyOp>(loc, opType.getShape(),\n+                                                 opType.getElementType());\n+\n+    auto filledTensor =\n+        rewriter\n+            .create<linalg::FillOp>(loc, ValueRange{adaptor.getSrc()},\n+                                    ValueRange{init})\n+            .result();\n+\n+    rewriter.replaceOp(op, filledTensor);\n+    return success();\n+  }\n+};\n+\n+struct BroadcastConverter : public OpConversionPattern<triton::BroadcastOp> {\n+private:\n+  using OpConversionPattern<triton::BroadcastOp>::OpConversionPattern;\n+\n+  SmallVector<int64_t> getBroadcastDims(RankedTensorType src,\n+                                        RankedTensorType dst) const {\n+    SmallVector<int64_t> broadcastDims;\n+    auto srcShape = src.getShape();\n+    auto dstShape = dst.getShape();\n+\n+    for (size_t i = 0; i < srcShape.size(); i++) {\n+      if (dstShape[i] != srcShape[i]) {\n+        assert(srcShape[i] == 1);\n+        broadcastDims.push_back(i);\n+      }\n+    }\n+    assert(!broadcastDims.empty() && \"cannot identify broadcast dimension\");\n+    return broadcastDims;\n+  }\n+\n+  // Broadcasts input tensor based on TosaToLinalg's broadcastToShape\n+  AffineMap getBroadcastAffineMap(MLIRContext *context,\n+                                  ArrayRef<int64_t> inputShape,\n+                                  ArrayRef<int64_t> broadcastToShape) const {\n+\n+    assert(broadcastToShape.size() >= inputShape.size());\n+\n+    // Create affine map and shapes for tensor initialization.\n+    SmallVector<AffineExpr> outExpr;\n+\n+    size_t diff = broadcastToShape.size() - inputShape.size();\n+    for (size_t i = 0; i < broadcastToShape.size(); i++) {\n+      if (i < diff) {\n+        continue;\n+      }\n+      size_t j = i - diff;\n+      if (inputShape[j] == 1) {\n+        // Broadcast singleton dimension\n+        outExpr.push_back(mlir::getAffineConstantExpr(0, context));\n+        continue;\n+      }\n+      // Non-broadcast case\n+      outExpr.push_back(mlir::getAffineDimExpr(i, context));\n+    }\n+    return AffineMap::get(broadcastToShape.size(), 0, outExpr, context);\n+  }\n+\n+public:\n+  LogicalResult\n+  matchAndRewrite(triton::BroadcastOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op.getLoc();\n+\n+    assert(op->getNumResults() == 1 && \"code assumes single result!\");\n+    RankedTensorType sourceType =\n+        cast<RankedTensorType>(adaptor.getSrc().getType());\n+    RankedTensorType resultType = cast<RankedTensorType>(op.getType());\n+    auto elementType = resultType.getElementType();\n+    size_t resultRank = resultType.getRank();\n+\n+    SmallVector<AffineMap> indexingMaps;\n+    indexingMaps.reserve(op->getNumOperands() + op->getNumResults());\n+\n+    indexingMaps.push_back(getBroadcastAffineMap(\n+        op->getContext(), sourceType.getShape(), resultType.getShape()));\n+    indexingMaps.append(op->getNumResults(),\n+                        rewriter.getMultiDimIdentityMap(resultRank));\n+\n+    assert(op->getNumResults() == 1 && \"code assumes single result!\");\n+    auto init = rewriter.create<tensor::EmptyOp>(loc, resultType.getShape(),\n+                                                 elementType);\n+\n+    auto linalgOp = rewriter.create<linalg::GenericOp>(\n+        loc, op->getResultTypes(), ValueRange{adaptor.getSrc()},\n+        ValueRange{init}, indexingMaps, getNParallelLoopsAttrs(resultRank),\n+        [&](OpBuilder &nestedBuilder, Location nestedLoc,\n+            ValueRange blockArgs) {\n+          Value opResult = blockArgs[0];\n+          nestedBuilder.create<linalg::YieldOp>(loc, opResult);\n+        });\n+\n+    linalgOp->setAttr(\"broadcastDims\",\n+                      rewriter.getDenseI64ArrayAttr(\n+                          getBroadcastDims(sourceType, resultType)));\n+\n+    rewriter.replaceOp(op, linalgOp->getResults());\n+    return success();\n+  }\n+};\n+\n+struct ExpandDimsConverter : public OpConversionPattern<triton::ExpandDimsOp> {\n+  using OpConversionPattern<triton::ExpandDimsOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ExpandDimsOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto src = adaptor.getSrc();\n+    auto srcRank = src.getType().cast<RankedTensorType>().getRank();\n+    auto resType = op->getResultTypes()[0].cast<RankedTensorType>();\n+    SmallVector<ReassociationIndices> reassoc;\n+    int64_t c = 0;\n+    for (int64_t i = 0; i < srcRank; i++) {\n+      ReassociationIndices g;\n+      g.push_back(c++);\n+      if (op.getAxis() == i)\n+        g.push_back(c++);\n+      else if (op.getAxis() == i + 1 && i == srcRank - 1)\n+        g.push_back(c++);\n+      reassoc.push_back(g);\n+    }\n+\n+    auto expandShapeOp = rewriter.create<tensor::ExpandShapeOp>(\n+        op.getLoc(), resType, src, reassoc);\n+\n+    rewriter.replaceOp(op, expandShapeOp.getResult());\n+    return success();\n+  }\n+};\n+\n+struct TransposeConverter : public OpConversionPattern<triton::TransOp> {\n+  using OpConversionPattern<triton::TransOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::TransOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto src = adaptor.getSrc();\n+    auto srcRank = src.getType().cast<ShapedType>().getRank();\n+    assert(srcRank == 2 && \"only expect transposing 2D data\");\n+\n+    auto res = getTransposedValue(src, op.getLoc(), rewriter);\n+    rewriter.replaceOp(op, res);\n+    return success();\n+  }\n+};\n+\n+struct MakeRangeConverter : public OpConversionPattern<triton::MakeRangeOp> {\n+  using OpConversionPattern<triton::MakeRangeOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::MakeRangeOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op.getLoc();\n+    auto type = op.getResult().getType().cast<TensorType>();\n+    auto shape = type.getShape();\n+    auto elementType = type.getElementType();\n+    auto context = rewriter.getContext();\n+\n+    assert(type.getShape().size() == 1 &&\n+           type.getElementType().getIntOrFloatBitWidth() == 32 &&\n+           \"make range can only return 1D int32 tensor\");\n+\n+    SmallVector<AffineMap> indexingMaps{AffineMap::get(\n+        /* dimCount */ 1, /* symbolCount */ 0,\n+        SmallVector<AffineExpr>{mlir::getAffineDimExpr(0, context)}, context)};\n+\n+    auto init = rewriter.create<tensor::EmptyOp>(loc, shape, elementType);\n+    auto linalgOp = rewriter.create<linalg::GenericOp>(\n+        loc, op->getResultTypes(), /* operands */ ValueRange{},\n+        ValueRange{init}, indexingMaps, getNParallelLoopsAttrs(1),\n+        [&](OpBuilder &nestedBuilder, Location nestedLoc,\n+            ValueRange blockArgs) {\n+          Value index = nestedBuilder.create<linalg::IndexOp>(loc, 0);\n+          Value res = nestedBuilder.create<arith::IndexCastOp>(\n+              loc, type.getElementType(), index);\n+          nestedBuilder.create<linalg::YieldOp>(loc, res);\n+        });\n+\n+    rewriter.replaceOp(op, linalgOp->getResults());\n+    return success();\n+  }\n+};\n+\n+struct AddPtrConverter : public OpConversionPattern<triton::AddPtrOp> {\n+  using OpConversionPattern<triton::AddPtrOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::AddPtrOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    llvm::SmallDenseMap<Value, PtrState> knwonPtrs;\n+    PtrAnalysis::rewriteAddptrOp(op, rewriter, knwonPtrs);\n+    return success();\n+  }\n+};\n+\n+struct AssertConverter : public OpConversionPattern<triton::AssertOp> {\n+  using OpConversionPattern<triton::AssertOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::AssertOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto condVal = op.getCondition();\n+\n+    if (condVal.getType().isa<mlir::TensorType>()) {\n+      auto scalarVal = getScalarValue(op.getCondition(), op.getLoc(), rewriter);\n+      condVal = scalarVal.value_or(condVal);\n+    }\n+    assert(condVal && condVal.getType().isa<mlir::IntegerType>() &&\n+           \"Only asserts on scalars are currently supported\");\n+\n+    if (!condVal.getType().isInteger(1)) {\n+      auto zero =\n+          rewriter.create<mlir::arith::ConstantIntOp>(op.getLoc(), 0, 32);\n+      auto newCond = rewriter.create<mlir::arith::CmpIOp>(\n+          op.getLoc(), arith::CmpIPredicate::ne, condVal, zero);\n+      condVal = newCond.getResult();\n+    }\n+\n+    auto assertMessage =\n+        llvm::formatv(\"{0}.py:{1}: {2} Assertion `{3}` failed\", op.getFile(),\n+                      op.getLine(), op.getFunc(), op.getMessage());\n+    auto assertOp = rewriter.create<mlir::cf::AssertOp>(op.getLoc(), condVal,\n+                                                        assertMessage.str());\n+\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+};\n+\n+struct BitcastConverter : public OpConversionPattern<triton::BitcastOp> {\n+  using OpConversionPattern<triton::BitcastOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::BitcastOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto arithBitcast = rewriter.create<arith::BitcastOp>(\n+        op.getLoc(), op.getType(), op.getOperand());\n+\n+    rewriter.replaceOp(op, arithBitcast.getResult());\n+    return success();\n+  }\n+};\n+\n+struct LoadConverter : public OpConversionPattern<triton::LoadOp> {\n+private:\n+  using OpConversionPattern<triton::LoadOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto ptr = adaptor.getPtr();\n+    auto mask = op.getMask();\n+    auto other = op.getOther();\n+    auto loc = op.getLoc();\n+\n+    // 0. Shortcut for scalar loads\n+    if (!op.getResult().getType().isa<ShapedType>()) {\n+      // Temporarily disbale scalar load until later passes support it\n+      op.emitError(\"Scalar load is currently not supported\");\n+      return failure();\n+\n+      auto sMemRef = PtrAnalysis::getScalarMemRef(op.getPtr(), adaptor.getPtr(),\n+                                                  loc, rewriter);\n+      auto zeroMap = AffineMap::getConstantMap(0, rewriter.getContext());\n+      auto loadOp = rewriter.create<AffineLoadOp>(op.getLoc(), sMemRef, zeroMap,\n+                                                  std::nullopt);\n+      rewriter.replaceOp(op, loadOp.getResult());\n+      return success();\n+    }\n+\n+    // 1. Simple case where no mask is used.\n+    auto type = ptr.getType().cast<MemRefType>();\n+    auto tensorType =\n+        RankedTensorType::get(type.getShape(), type.getElementType());\n+    auto alloc = rewriter.create<memref::AllocOp>(\n+        loc, MemRefType::get(type.getShape(), type.getElementType()));\n+\n+    if (!mask) {\n+      assert(!other && \"other value used in non-masked load\");\n+      rewriter.create<memref::CopyOp>(loc, ptr, alloc);\n+      Value tensor = rewriter.create<bufferization::ToTensorOp>(\n+          loc, tensorType, alloc, true /* restrict */, true /* writable */);\n+      rewriter.replaceOp(op, tensor);\n+      return success();\n+    }\n+\n+    // 2. Continuous masked loads.\n+    // Analyze the mask operand to determine at runtime the size of the data we\n+    // are moving.\n+    MaskState mstate;\n+    auto isContMask = mstate.parse(mask, loc, rewriter);\n+\n+    if (isContMask.failed())\n+      return failure();\n+\n+    auto castOp = ptr.getDefiningOp<memref::ReinterpretCastOp>();\n+    assert(castOp);\n+    ptr = castOp.getResult();\n+\n+    auto srcSubview = mstate.getSubview(ptr, loc, rewriter);\n+    auto dstSubview = mstate.getSubview(alloc, loc, rewriter);\n+\n+    // fill load destination with other value\n+    if (other) {\n+      auto scalarOther = getScalarValue(other, loc, rewriter);\n+      assert(scalarOther.has_value() &&\n+             \"other value used in masked load produced by \"\n+             \"unsupported instruction\");\n+\n+      // For each dimension check if mstate.dims[i] < shape[i], or-accumulate\n+      // the result\n+      auto shape = type.getShape();\n+      auto accBase =\n+          rewriter.create<arith::ConstantOp>(loc, rewriter.getBoolAttr(false))\n+              .getResult();\n+      for (size_t i = 0; i < type.getShape().size(); i++) {\n+        auto shapei = rewriter.create<arith::ConstantOp>(\n+            loc, rewriter.getIndexAttr(shape[i]));\n+\n+        Value dimi = mstate.dims[i].dyn_cast<Value>();\n+        if (!dimi) {\n+          dimi = rewriter.create<arith::ConstantOp>(\n+              loc, mstate.dims[i].get<Attribute>().cast<IntegerAttr>());\n+        }\n+\n+        auto cmpOp = rewriter.create<arith::CmpIOp>(\n+            loc, arith::CmpIPredicate::slt, dimi, shapei);\n+        accBase = rewriter.create<arith::OrIOp>(loc, accBase, cmpOp.getResult())\n+                      .getResult();\n+      }\n+\n+      // condition the memset on the or-accumulation\n+      // initialize with padding prior to CopyOp\n+      rewriter.create<scf::IfOp>(\n+          loc, accBase, [&](OpBuilder &builder, Location loc) {\n+            builder.create<linalg::FillOp>(loc, ValueRange{scalarOther.value()},\n+                                           ValueRange{alloc});\n+            builder.create<scf::YieldOp>(loc);\n+          });\n+    }\n+\n+    rewriter.create<memref::CopyOp>(loc, srcSubview, dstSubview);\n+    Value tensor = rewriter.create<bufferization::ToTensorOp>(\n+        loc, tensorType, alloc, true /* restrict */, true /* writable */);\n+    rewriter.replaceOp(op, tensor);\n+\n+    return success();\n+  }\n+};\n+\n+struct StoreConverter : public OpConversionPattern<triton::StoreOp> {\n+  using OpConversionPattern<triton::StoreOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::StoreOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto ptr = adaptor.getPtr();\n+    auto val = adaptor.getValue();\n+    auto mask = op.getMask();\n+    auto loc = op.getLoc();\n+\n+    // 0. Shortcut for scalar stores\n+    if (!val.getType().isa<ShapedType>()) {\n+      auto sMemRef =\n+          PtrAnalysis::getScalarMemRef(op.getPtr(), ptr, loc, rewriter);\n+      auto zeroMap = AffineMap::getConstantMap(0, rewriter.getContext());\n+      rewriter.create<AffineStoreOp>(loc, val, sMemRef, zeroMap, std::nullopt);\n+      rewriter.eraseOp(op);\n+      return success();\n+    }\n+\n+    // 1. Simple case where no mask is used.\n+    if (!mask) {\n+      rewriter.create<memref::TensorStoreOp>(loc, val, ptr);\n+      rewriter.eraseOp(op);\n+      return success();\n+    }\n+\n+    // 2. Continuous masked stores.\n+    // Analyze the mask operand to determine at runtime the size of the data we\n+    // are moving.\n+    MaskState mstate;\n+    auto isContMask = mstate.parse(mask, loc, rewriter);\n+\n+    if (isContMask.failed())\n+      return failure();\n+\n+    auto srcSlice = mstate.getExtractSlice(val, loc, rewriter);\n+    auto dstSubview = mstate.getSubview(ptr, loc, rewriter);\n+\n+    rewriter.create<memref::TensorStoreOp>(loc, srcSlice, dstSubview);\n+    rewriter.eraseOp(op);\n+\n+    return success();\n+  }\n+};\n+\n+struct LoopConverter : public OpConversionPattern<scf::ForOp> {\n+  using OpConversionPattern<scf::ForOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(scf::ForOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    llvm::SmallDenseMap<Value, PtrState> knownPtrs;\n+    PtrAnalysis::IndexMapSet\n+        levelToBlockArgIndex; // level -> set of block arg index to be replaced\n+\n+    PtrAnalysis::rewriteForOp(op, rewriter, levelToBlockArgIndex, 0, knownPtrs);\n+    return success();\n+  }\n+};\n+\n+struct YieldConverter : public OpConversionPattern<scf::YieldOp> {\n+  using OpConversionPattern<scf::YieldOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(scf::YieldOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    rewriter.replaceOpWithNewOp<scf::YieldOp>(op, adaptor.getOperands());\n+    return success();\n+  }\n+};\n+\n+struct MatmulConverter : public OpConversionPattern<triton::DotOp> {\n+  using OpConversionPattern<triton::DotOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::DotOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto dstType = op.getType().cast<RankedTensorType>();\n+    auto loc = op.getLoc();\n+\n+    auto opa = adaptor.getA();\n+    auto opb = adaptor.getB();\n+    auto opc = adaptor.getC();\n+    auto opcOrig = op.getC();\n+\n+    bool skipC = false;\n+    if (auto splatOp = opcOrig.getDefiningOp<triton::SplatOp>()) {\n+      if (auto val = splatOp.getSrc().getDefiningOp<arith::ConstantOp>()) {\n+        if (val.getValue().cast<FloatAttr>().getValueAsDouble() == 0.) {\n+          skipC = true;\n+        }\n+      }\n+    } else if (auto constOp = opcOrig.getDefiningOp<arith::ConstantOp>()) {\n+      if (auto denseAttr = dyn_cast<DenseElementsAttr>(constOp.getValue())) {\n+        if (denseAttr.isSplat() &&\n+            denseAttr.getSplatValue<FloatAttr>().getValueAsDouble() == 0.) {\n+          skipC = true;\n+        }\n+      }\n+    }\n+\n+    auto init = rewriter.create<tensor::EmptyOp>(loc, dstType.getShape(),\n+                                                 dstType.getElementType());\n+\n+    auto res = rewriter\n+                   .create<linalg::MatmulOp>(loc, ValueRange{opa, opb},\n+                                             ValueRange{init})\n+                   .getResult(0);\n+\n+    if (!skipC) {\n+      res = rewriter.create<arith::AddFOp>(loc, res, opc);\n+    }\n+\n+    rewriter.replaceOp(op, res);\n+    return success();\n+  }\n+};\n+\n+struct ReduceConverter : public OpConversionPattern<triton::ReduceOp> {\n+  using OpConversionPattern<triton::ReduceOp>::OpConversionPattern;\n+\n+private:\n+  llvm::SmallVector<Operation *> getRedOps(triton::ReduceOp redOp) const {\n+    auto reduceBlock = redOp.getBody();\n+    llvm::SmallVector<Operation *> ops;\n+    for (auto &op : reduceBlock->without_terminator()) {\n+      ops.push_back(&op);\n+    }\n+    return ops;\n+  }\n+\n+  bool isReductionOpSupported(Operation *redOp) const {\n+    return isa<arith::AddFOp, arith::MaxFOp>(redOp);\n+  }\n+\n+  float getRedBaseVal(Operation *redOp) const {\n+    return llvm::TypeSwitch<Operation *, float>(redOp)\n+        .Case([](arith::AddFOp) { return 0; })\n+        .Case([](arith::MaxFOp) {\n+          return -std::numeric_limits<float>::infinity();\n+        })\n+        .Default([](Operation *op) {\n+          op->dump();\n+          llvm_unreachable(\"Reduction op not yet supported\");\n+          return -1;\n+        });\n+  }\n+\n+  bool requiresF32Conversion(const Type elemType, Operation *redOp) const {\n+    return elemType.isa<FloatType>() &&\n+           elemType.getIntOrFloatBitWidth() <\n+               Float32Type::get(elemType.getContext()).getWidth() &&\n+           isa<arith::AddFOp>(redOp);\n+  }\n+\n+  Value getRedElement(Value lhs, Value rhs, const Location loc,\n+                      Operation *redOp, OpBuilder &b,\n+                      const bool convertLhsToF32Precision) const {\n+    return llvm::TypeSwitch<Operation *, Value>(redOp)\n+        .Case([&](arith::AddFOp) {\n+          if (convertLhsToF32Precision) {\n+            lhs = b.create<arith::ExtFOp>(loc, Float32Type::get(b.getContext()),\n+                                          lhs);\n+          }\n+          return b.create<arith::AddFOp>(loc, lhs, rhs);\n+        })\n+        .Case([&](arith::MaxFOp) {\n+          return b.create<arith::MaxFOp>(loc, lhs, rhs);\n+        })\n+        .Default([](Operation *op) {\n+          op->dump();\n+          llvm_unreachable(\"Reduction op not yet supported\");\n+          return nullptr;\n+        });\n+  }\n+\n+  LogicalResult\n+  convertToLinalgReduce(triton::ReduceOp op,\n+                        typename triton::ReduceOp::Adaptor adaptor,\n+                        ConversionPatternRewriter &rewriter) const {\n+    auto source = adaptor.getOperands().front();\n+    auto sourceType = cast<RankedTensorType>(source.getType());\n+    auto elemType = sourceType.getElementType();\n+    auto resType = op.getResult().front().getType();\n+    auto loc = op.getLoc();\n+    auto reductionOps = getRedOps(op);\n+\n+    // Reduction of arbitrary operations isn't supported because using the first\n+    // element across the reduction dimension requires us to iterate over a\n+    // subview that skips over each first element.\n+    if (reductionOps.size() != 1 ||\n+        !isReductionOpSupported(reductionOps.front())) {\n+      return op.emitError(\"Only support lowering reduction with body \"\n+                          \"containing 1 maxf or addf.\");\n+    }\n+\n+    auto rop = reductionOps.front();\n+    auto axis = op.getAxis();\n+    auto isVectorReduce = sourceType.getRank() == 1;\n+\n+    if (axis == sourceType.getRank() - 1 && !isVectorReduce) {\n+      source = getTransposedValue(source, op.getLoc(), rewriter);\n+      axis = sourceType.getRank() - 2;\n+    }\n+\n+    bool convertToF32Precision = requiresF32Conversion(resType, rop);\n+\n+    auto constantType = convertToF32Precision\n+                            ? Float32Type::get(rewriter.getContext())\n+                            : elemType;\n+    float accBaseVal = getRedBaseVal(rop);\n+    auto accBase = rewriter.create<arith::ConstantOp>(\n+        loc, constantType, rewriter.getFloatAttr(constantType, accBaseVal));\n+    Value initTensor;\n+\n+    if (isVectorReduce) {\n+      // The affine vectorizer cannot vectorize affine loops generated from\n+      // linalg.reduce for the vector reduce case, so we must rewrite the\n+      // linalg.reduce to affine loops manually. Here we lower to AllocTensor\n+      // directly instead of EmptyOp so that the subsequent pass can recognize\n+      // the patterns (EmptyOp is susceptible to being CSE'd away, making it\n+      // harder to match the patterns correctly).\n+      initTensor = rewriter.create<bufferization::AllocTensorOp>(\n+          loc, RankedTensorType::get({}, constantType), ValueRange{});\n+      initTensor = rewriter.create<tensor::InsertOp>(loc, accBase, initTensor,\n+                                                     ValueRange{});\n+    } else {\n+      Value init = rewriter.create<tensor::EmptyOp>(\n+          loc, cast<RankedTensorType>(resType).getShape(), constantType);\n+      initTensor = rewriter\n+                       .create<linalg::FillOp>(loc, ValueRange{accBase},\n+                                               ValueRange{init})\n+                       .result();\n+    }\n+\n+    Value finalResult =\n+        rewriter\n+            .create<linalg::ReduceOp>(\n+                loc, ValueRange{source}, ValueRange{initTensor},\n+                SmallVector<int64_t>{axis},\n+                [&](OpBuilder &opBuilder, Location loc, ValueRange inputs) {\n+                  assert(inputs.size() == 2);\n+                  Value result =\n+                      getRedElement(inputs[0], inputs[1], loc, rop, opBuilder,\n+                                    convertToF32Precision);\n+                  opBuilder.create<linalg::YieldOp>(loc, result);\n+                })\n+            .getResult(0);\n+\n+    if (sourceType.getRank() == 1) {\n+      finalResult =\n+          rewriter.create<tensor::ExtractOp>(loc, constantType, finalResult);\n+    }\n+\n+    if (convertToF32Precision) {\n+      finalResult = rewriter.create<arith::TruncFOp>(\n+          loc, BFloat16Type::get(rewriter.getContext()), finalResult);\n+    }\n+\n+    rewriter.replaceOp(op, finalResult);\n+    return success();\n+  }\n+\n+public:\n+  LogicalResult\n+  matchAndRewrite(triton::ReduceOp op,\n+                  typename triton::ReduceOp::Adaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto sourceType =\n+        adaptor.getOperands().front().getType().cast<RankedTensorType>();\n+    assert(sourceType.hasRank() && \"Expected input is \"\n+                                   \"ranked\");\n+\n+    int64_t axis = op.getAxis();\n+    assert(axis >= 0 && axis < sourceType.getRank() &&\n+           \"Expected reduction \"\n+           \"axis is within \"\n+           \"operand's rank\");\n+\n+    return convertToLinalgReduce(op, adaptor, rewriter);\n+  }\n+};\n+\n+struct GetProgramIDConverter\n+    : public OpConversionPattern<triton::GetProgramIdOp> {\n+  using OpConversionPattern<triton::GetProgramIdOp>::OpConversionPattern;\n+\n+private:\n+  const unsigned int LAUNCH_GRID_RANK;\n+\n+public:\n+  GetProgramIDConverter(MLIRContext *context, unsigned int launchGridRank)\n+      : OpConversionPattern(context), LAUNCH_GRID_RANK(launchGridRank) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::GetProgramIdOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto axis = op.getAxis();\n+    assert(axis < LAUNCH_GRID_RANK && \"program_id expects \"\n+                                      \"axis to be either 0, \"\n+                                      \"1, or 2\");\n+\n+    auto func = op->getParentOfType<FunctionOpInterface>();\n+    auto numArgs = func.getNumArguments();\n+    auto id = func.getArgument(numArgs - LAUNCH_GRID_RANK + axis);\n+\n+    rewriter.replaceOp(op, id);\n+    return success();\n+  }\n+};\n+\n+// Remove all Meta ops except for AddPtr which is handled by AddPtrConverter.\n+// Use benefit == 10 to ensure that this pattern always takes precedence over\n+// other patterns.\n+struct MetaOpConverter : public RewritePattern {\n+private:\n+  // UseAnalysis will tag operations whose results are used only as meta-data\n+  // with \"MetaUse\" tag.\n+  bool isMetaUse(Operation *op) const { return op->hasAttr(\"MetaUse\"); }\n+\n+public:\n+  MetaOpConverter(MLIRContext *context)\n+      : RewritePattern(MatchAnyOpTypeTag(), /*benefit=*/10, context) {}\n+\n+  LogicalResult matchAndRewrite(Operation *op,\n+                                PatternRewriter &rewriter) const final {\n+\n+    if (isa<triton::AddPtrOp>(op)) {\n+      return rewriter.notifyMatchFailure(op,\n+                                         \"AddPtrOp will be handled separately\");\n+    }\n+\n+    if (isMetaUse(op)) {\n+      rewriter.eraseOp(op);\n+      return success();\n+    }\n+\n+    return rewriter.notifyMatchFailure(op, \"requires meta ops\");\n+  }\n+};\n+\n+// Convert a pair of cmpf and select to either min or max.\n+// Leave the pattern as simple as possible because triton has plans to emit\n+// min and max directly.\n+struct MinMaxConverter : public OpRewritePattern<arith::CmpFOp> {\n+  using OpRewritePattern<arith::CmpFOp>::OpRewritePattern;\n+\n+  MinMaxConverter(MLIRContext *context)\n+      : OpRewritePattern<arith::CmpFOp>(context, /*benefit=*/10) {}\n+\n+  LogicalResult matchAndRewrite(arith::CmpFOp cmpOp,\n+                                PatternRewriter &rewriter) const final {\n+    if (!cmpOp.getResult().hasOneUse()) {\n+      return failure();\n+    }\n+    auto selectOp =\n+        dyn_cast<arith::SelectOp>(*cmpOp.getResult().getUsers().begin());\n+    if (!selectOp) {\n+      return failure();\n+    }\n+\n+    if (!(cmpOp.getResult() == selectOp.getCondition() &&\n+          cmpOp.getLhs() == selectOp.getTrueValue() &&\n+          cmpOp.getRhs() == selectOp.getFalseValue())) {\n+      return failure();\n+    }\n+\n+    auto pred = cmpOp.getPredicate();\n+    auto loc = cmpOp.getLoc();\n+    if (pred == arith::CmpFPredicate::OGT) {\n+      rewriter.replaceOpWithNewOp<arith::MaxFOp>(selectOp, cmpOp.getLhs(),\n+                                                 cmpOp.getRhs());\n+    } else if (pred == arith::CmpFPredicate::OLT) {\n+      rewriter.replaceOpWithNewOp<arith::MinFOp>(selectOp, cmpOp.getLhs(),\n+                                                 cmpOp.getRhs());\n+    } else {\n+      llvm_unreachable(\"Unhandled predicate\");\n+    }\n+\n+    rewriter.eraseOp(cmpOp);\n+\n+    return success();\n+  }\n+};\n+\n+struct DenseConstantConverter : public OpConversionPattern<arith::ConstantOp> {\n+  using OpConversionPattern<arith::ConstantOp>::OpConversionPattern;\n+  LogicalResult\n+  matchAndRewrite(arith::ConstantOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto attr = cast<DenseElementsAttr>(op.getValue());\n+    auto loc = op.getLoc();\n+\n+    auto splatConst = rewriter.create<arith::ConstantOp>(\n+        loc, attr.getSplatValue<Attribute>(), attr.getElementType());\n+\n+    auto init = rewriter.create<tensor::EmptyOp>(\n+        loc, cast<RankedTensorType>(op.getResult().getType()).getShape(),\n+        attr.getElementType());\n+\n+    rewriter.replaceOpWithNewOp<linalg::FillOp>(op, ValueRange{splatConst},\n+                                                ValueRange{init});\n+\n+    return success();\n+  }\n+};\n+\n+} // namespace\n+\n+void mlir::triton::populateTritonToLinalgCanonicalizationPatterns(\n+    RewritePatternSet &patterns) {\n+  patterns.add<MinMaxConverter>(patterns.getContext());\n+}\n+\n+void mlir::triton::populateTritonToLinalgConversionPatterns(\n+    TypeConverter &typeConverter, RewritePatternSet &patterns,\n+    unsigned int launchGridRank) {\n+  populateFunctionOpInterfaceTypeConversionPattern<triton::FuncOp>(\n+      patterns, typeConverter);\n+  patterns.add<MetaOpConverter>(patterns.getContext());\n+  patterns.add<StoreConverter>(patterns.getContext());\n+  patterns.add<AddPtrConverter>(patterns.getContext());\n+  patterns.add<GetProgramIDConverter>(patterns.getContext(), launchGridRank);\n+  patterns.add<YieldConverter>(patterns.getContext());\n+  patterns.add<LoadConverter>(patterns.getContext());\n+  patterns.add<LoopConverter>(patterns.getContext());\n+  patterns.add<BroadcastConverter>(patterns.getContext());\n+  patterns.add<TransposeConverter>(patterns.getContext());\n+  patterns.add<MakeRangeConverter>(patterns.getContext());\n+  patterns.add<ExpandDimsConverter>(patterns.getContext());\n+  patterns.add<BitcastConverter>(patterns.getContext());\n+  patterns.add<AssertConverter>(patterns.getContext());\n+  patterns.add<MatmulConverter>(patterns.getContext());\n+  patterns.add<SplatConverter>(patterns.getContext());\n+  patterns.add<ReduceConverter>(patterns.getContext());\n+  patterns.add<DenseConstantConverter>(patterns.getContext());\n+\n+  // Note: the ordering here matters!\n+  // MetaOpConverter has PatternBenefit == 10 which should take precedence over\n+  // these linalg patterns, but to be safe, add these patterns last so that they\n+  // will be tried last. Incorrect ordering or having MetaOpConverter has lower\n+  // PatternBenefit will result in element-wise meta ops being converted to\n+  // linalg.generic ops.\n+  linalg::populateElementwiseToLinalgConversionPatterns(patterns);\n+}"}, {"filename": "lib/Conversion/TritonToLinalg/TritonToLinalgPass.cpp", "status": "added", "additions": 232, "deletions": 0, "changes": 232, "file_content_changes": "@@ -0,0 +1,232 @@\n+//===----------------------------------------------------------------------===//\n+//\n+// Copyright (c) Triton Project Contributors.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#include \"triton/Analysis/UseAnalysis.h\"\n+#include \"triton/Conversion/TritonToLinalg/TritonToLinalg.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+\n+#include \"mlir/Dialect/Affine/IR/AffineOps.h\"\n+#include \"mlir/Dialect/Bufferization/IR/Bufferization.h\"\n+#include \"mlir/Dialect/Linalg/IR/Linalg.h\"\n+#include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n+#include \"mlir/Pass/PassManager.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"mlir/Transforms/Passes.h\"\n+\n+#include \"llvm/Support/Debug.h\"\n+\n+#define DEBUG_TYPE \"triton-to-linalg\"\n+\n+using namespace mlir;\n+using namespace triton;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Conversion/TritonToLinalg/Passes.h.inc\"\n+\n+namespace {\n+\n+class TritonTypeConverter : public TypeConverter {\n+public:\n+  TritonTypeConverter() {\n+    // The order of type conversion is important: later ones are tried earlier.\n+    addConversion([](Type type) { return type; });\n+    addConversion([](triton::PointerType ptrType) {\n+      return UnrankedMemRefType::get(ptrType.getPointeeType(), 0);\n+    });\n+    addConversion([](TensorType tensorType) -> Type {\n+      auto elemType = tensorType.getElementType();\n+      if (auto ptrType = elemType.dyn_cast<triton::PointerType>()) {\n+        elemType = ptrType.getPointeeType();\n+      }\n+      return MemRefType::get(tensorType.getShape(), elemType);\n+    });\n+  }\n+};\n+\n+struct TritonToLinalgPass : public TritonToLinalgBase<TritonToLinalgPass> {\n+\n+  static unsigned int constexpr LAUNCH_GRID_RANK = 3;\n+\n+  // Add additional I32 arguments to represent program\n+  // ID, one for each dimension of the launch grid\n+  static void addProgramId(triton::FuncOp func) {\n+    OpBuilder b(func);\n+\n+    auto origFuncType = func.getFunctionType();\n+    auto origInputTypes = origFuncType.getInputs();\n+    SmallVector<Type> newInputTypes(origInputTypes);\n+    newInputTypes.append(LAUNCH_GRID_RANK, b.getI32Type());\n+\n+    auto newFuncType =\n+        b.getFunctionType(newInputTypes, origFuncType.getResults());\n+\n+    func.setFunctionType(newFuncType);\n+\n+    // Add empty attributes for each new argument if needed\n+    if (func.getAllArgAttrs()) {\n+      SmallVector<DictionaryAttr> newArgAttrs;\n+      func.getAllArgAttrs(newArgAttrs);\n+      newArgAttrs.append(LAUNCH_GRID_RANK, DictionaryAttr());\n+      func.setAllArgAttrs(newArgAttrs);\n+    }\n+\n+    // Add the corresponding arguments to function body\n+    for (unsigned int i = 0; i < LAUNCH_GRID_RANK; i++) {\n+      func.getBody().front().addArgument(b.getI32Type(), func.getLoc());\n+    }\n+  }\n+\n+public:\n+  void getDependentDialects(DialectRegistry &registry) const override {\n+    registry.insert<func::FuncDialect, arith::ArithDialect, math::MathDialect,\n+                    linalg::LinalgDialect, AffineDialect, scf::SCFDialect,\n+                    tensor::TensorDialect, bufferization::BufferizationDialect,\n+                    memref::MemRefDialect>();\n+  }\n+\n+  void runOnOperation() override {\n+    auto moduleOp = getOperation();\n+\n+    {\n+      RewritePatternSet patterns(&getContext());\n+      populateTritonToLinalgCanonicalizationPatterns(patterns);\n+      if (failed(applyPatternsAndFoldGreedily(moduleOp, std::move(patterns)))) {\n+        signalPassFailure();\n+      }\n+    }\n+\n+    moduleOp.walk([this](triton::FuncOp op) {\n+      if (failed(runUseAnalysis(op))) {\n+        signalPassFailure();\n+      }\n+    });\n+\n+    RewritePatternSet patterns(&getContext());\n+    ConversionTarget target(getContext());\n+    TritonTypeConverter tritonTypeConverter;\n+\n+    target.addLegalDialect<\n+        func::FuncDialect, arith::ArithDialect, math::MathDialect,\n+        linalg::LinalgDialect, AffineDialect, scf::SCFDialect,\n+        cf::ControlFlowDialect, tensor::TensorDialect,\n+        bufferization::BufferizationDialect, memref::MemRefDialect>();\n+\n+    target.addLegalOp<ModuleOp>();\n+\n+    target.addIllegalDialect<triton::TritonDialect>();\n+\n+    // triton.reduce will be lowered to linalg.reduce. Unfortunately, mlir\n+    // inserts the ops inside triton.reduce's region BEFORE triton.reduce\n+    // itself, so the conversion algorithm will visit triton.reduce_return\n+    // first. Without marking this op as legal, the conversion process will fail\n+    // because there's no legalization pattern for triton.reduce_return.\n+    target.addLegalOp<triton::ReduceReturnOp>();\n+\n+    target.addLegalOp<triton::ReturnOp>();\n+\n+    // Update function signature to use memrefs\n+    target.addDynamicallyLegalOp<triton::FuncOp>([&](triton::FuncOp op) {\n+      return tritonTypeConverter.isSignatureLegal(op.getFunctionType());\n+    });\n+\n+    // Lower dense constant to linalg.fill\n+    target.addDynamicallyLegalOp<arith::ConstantOp>([](arith::ConstantOp op) {\n+      if (!isa<RankedTensorType>(op.getResult().getType())) {\n+        return true;\n+      }\n+\n+      if (auto denseAttr = dyn_cast<DenseElementsAttr>(op.getValue())) {\n+        if (denseAttr.isSplat() &&\n+            isa<FloatType, IntegerType>(denseAttr.getElementType())) {\n+          return false;\n+        }\n+      }\n+      return true;\n+    });\n+\n+    target.addDynamicallyLegalOp<scf::ForOp, scf::YieldOp>([](Operation *op) {\n+      return llvm::all_of(op->getOperandTypes(), [](Type t) {\n+        if (isa<triton::PointerType>(t)) {\n+          return false;\n+        }\n+        if (auto shapedType = dyn_cast<ShapedType>(t)) {\n+          return shapedType.getElementType().isIntOrFloat();\n+        }\n+        assert(t.isIntOrIndexOrFloat());\n+        return true;\n+      });\n+    });\n+\n+    target.addDynamicallyLegalDialect<arith::ArithDialect, math::MathDialect>(\n+        [](Operation *op) {\n+          if (op->hasAttr(\"MetaUse\")) {\n+            return false;\n+          }\n+\n+          if (isa<arith::ConstantOp>(op)) {\n+            return true;\n+          }\n+\n+          bool operateOnTensors =\n+              llvm::all_of(op->getOperandTypes(), [](Type type) {\n+                return type.isa<RankedTensorType>();\n+              });\n+\n+          return !operateOnTensors;\n+        });\n+\n+    triton::populateTritonToLinalgConversionPatterns(\n+        tritonTypeConverter, patterns, LAUNCH_GRID_RANK);\n+\n+    for (auto func : getOperation().getOps<triton::FuncOp>())\n+      addProgramId(func);\n+\n+    if (failed(applyFullConversion(moduleOp, target, std::move(patterns))))\n+      signalPassFailure();\n+\n+    // Convert tt.func and tt.return into func's counterparts\n+    moduleOp.walk([&](triton::FuncOp func) {\n+      OpBuilder builder(func);\n+\n+      auto name = func.getName();\n+      auto type = func.getFunctionType();\n+\n+      SmallVector<DictionaryAttr> argAttrs, resAttrs;\n+      func.getAllArgAttrs(argAttrs);\n+      func.getAllResultAttrs(resAttrs);\n+\n+      auto funcFunc = builder.create<func::FuncOp>(func.getLoc(), name, type);\n+      funcFunc.setAllArgAttrs(argAttrs);\n+      funcFunc.setAllResultAttrs(resAttrs);\n+\n+      auto &funcFuncBody = funcFunc.getBody();\n+      auto &funcBody = func.getBody();\n+\n+      IRMapping map;\n+      funcBody.cloneInto(&funcFuncBody, map);\n+\n+      for (Block &block : funcFuncBody.getBlocks()) {\n+        auto term = block.getTerminator();\n+        builder.setInsertionPoint(term);\n+        builder.create<func::ReturnOp>(func.getLoc(), term->getOperands());\n+        term->erase();\n+      }\n+      func.erase();\n+    });\n+\n+    // Erase dead code and fold constants created during lowering\n+    PassManager pm(&getContext(), moduleOp.getOperationName());\n+    pm.addPass(createCanonicalizerPass());\n+    if (failed(runPipeline(pm, getOperation()))) {\n+      signalPassFailure();\n+    }\n+  }\n+};\n+} // namespace\n+\n+std::unique_ptr<OperationPass<ModuleOp>> triton::createTritonToLinalgPass() {\n+  return std::make_unique<TritonToLinalgPass>();\n+}"}, {"filename": "test/Conversion/TritonToLinalg/addptr_2d_example.mlir", "status": "added", "additions": 69, "deletions": 0, "changes": 69, "file_content_changes": "@@ -0,0 +1,69 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %arg0 : !tt.ptr<bf16>,\n+    %arg1 : !tt.ptr<bf16>,\n+    %arg2 : !tt.ptr<bf16>,\n+    %arg3 : i32\n+  )\n+  {\n+    %0 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32>\n+    // offset = 0, size = 4, stride = 1\n+    %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<4xi32>) -> tensor<4x1xi32>\n+    // offset = [0,0], size = [4,1], stride = [1,0]\n+    %2 = tt.broadcast %1 : (tensor<4x1xi32>) -> tensor<4x256xi32>\n+    // offset = [0,0], size = [4,256], stride = [1,0]\n+    %arg3splat = tt.splat %arg3 : (i32) -> tensor<4x256xi32>\n+    %offset3 = arith.addi %2, %arg3splat : tensor<4x256xi32>\n+    // offset = [%arg3,0], size = [4,256], stride = [1,0]\n+    %3 = tt.make_range {end = 256 : i32, start = 0 : i32}: tensor<256xi32>\n+    // offset = 0, size = 256, stride = 1\n+    %4 = tt.expand_dims %3 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+    // offset = [0,0], size = [1,256], stride = [0,1]\n+    %5 = tt.broadcast %4 : (tensor<1x256xi32>) -> tensor<4x256xi32>\n+    // offset = [0,0], size = [4,256], stride = [0,1]\n+    %6 = arith.constant 5 : i32\n+    %splat6 = tt.splat %6 : (i32) -> tensor<4x256xi32>\n+    %scale5 = arith.muli %5, %splat6 : tensor<4x256xi32>\n+    // offset = [0,0], size = [4,256], stride = [0,5]\n+    %7 = arith.addi %offset3, %scale5: tensor<4x256xi32>\n+    // offset = [%arg3, 0], size = [4, 256], stride = [1, 5]\n+    %8 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<4x256x!tt.ptr<bf16>>\n+    %9 = tt.addptr %8, %7 : tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xi32>\n+    // source: %arg0, offset = [%arg3, 0], size = [4, 256], stride = [1, 5]\n+    %10 = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<4x256xbf16>\n+    %11 = tt.splat %arg1 : (!tt.ptr<bf16>) -> tensor<4x256x!tt.ptr<bf16>>\n+    %12 = tt.addptr %11, %7 : tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xi32>\n+    // source: %arg1, offset = [%arg3, 0], size = [4, 256], stride = [1, 5]\n+    %13 = tt.load %12 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<4x256xbf16>\n+    %14 = arith.addf %10, %13 : tensor<4x256xbf16>\n+    %15 = tt.splat %arg2 : (!tt.ptr<bf16>) -> tensor<4x256x!tt.ptr<bf16>>\n+    %16 = tt.addptr %15, %7 : tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xi32>\n+    // source: %arg2, offset = [%arg3, 0], size = [4, 256], stride = [1, 5]\n+    tt.store %16, %14 : tensor<4x256xbf16>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: memref<*xbf16>, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32, %[[VAL_6:.*]]: i32) { \n+// CHECK:           %[[VAL_7:.*]] = arith.constant 5 : index\n+// CHECK:           %[[VAL_8:.*]] = arith.index_cast %[[VAL_3]] : i32 to index\n+// CHECK:           %[[VAL_9:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_8]]], sizes: [4, 256], strides: [1, %[[VAL_7]]] : memref<*xbf16> to memref<4x256xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           %[[VAL_10:.*]] = memref.alloc() : memref<4x256xbf16>\n+// CHECK:           memref.copy %[[VAL_9]], %[[VAL_10]] : memref<4x256xbf16, strided<[1, ?], offset: ?>> to memref<4x256xbf16>\n+// CHECK:           %[[VAL_11:.*]] = bufferization.to_tensor %[[VAL_10]] restrict writable : memref<4x256xbf16>\n+// CHECK:           %[[VAL_12:.*]] = arith.index_cast %[[VAL_3]] : i32 to index\n+// CHECK:           %[[VAL_13:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}%[[VAL_12]]], sizes: [4, 256], strides: [1, %[[VAL_7]]] : memref<*xbf16> to memref<4x256xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           %[[VAL_14:.*]] = memref.alloc() : memref<4x256xbf16>\n+// CHECK:           memref.copy %[[VAL_13]], %[[VAL_14]] : memref<4x256xbf16, strided<[1, ?], offset: ?>> to memref<4x256xbf16>\n+// CHECK:           %[[VAL_15:.*]] = bufferization.to_tensor %[[VAL_14]] restrict writable : memref<4x256xbf16>\n+// CHECK:           %[[VAL_16:.*]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_11]], %[[VAL_15]] : tensor<4x256xbf16>, tensor<4x256xbf16>) outs(%[[VAL_11]] : tensor<4x256xbf16>) {\n+// CHECK:           ^bb0(%[[VAL_17:.*]]: bf16, %[[VAL_18:.*]]: bf16, %[[VAL_19:.*]]: bf16):\n+// CHECK:             %[[VAL_20:.*]] = arith.addf %[[VAL_17]], %[[VAL_18]] : bf16\n+// CHECK:             linalg.yield %[[VAL_20]] : bf16\n+// CHECK:           } -> tensor<4x256xbf16>\n+// CHECK:           %[[VAL_21:.*]] = arith.index_cast %[[VAL_3]] : i32 to index\n+// CHECK:           %[[VAL_22:.*]] = memref.reinterpret_cast %[[VAL_2]] to offset: {{\\[}}%[[VAL_21]]], sizes: [4, 256], strides: [1, %[[VAL_7]]] : memref<*xbf16> to memref<4x256xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           memref.tensor_store %[[VAL_23:.*]], %[[VAL_22]] : memref<4x256xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_add_value.mlir", "status": "added", "additions": 68, "deletions": 0, "changes": 68, "file_content_changes": "@@ -0,0 +1,68 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+  %arg0 : !tt.ptr<bf16>,\n+  %arg1 : !tt.ptr<bf16>,\n+  %arg2 : i32,\n+  %arg3 : i32\n+  )\n+  {\n+  %0 = tt.make_range {end = 4 : i32, start = 0 : i32}:tensor<4xi32>\n+  // offset = 0, size = 4, stride = 1\n+  %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<4xi32>) -> tensor<4x1xi32>\n+  // offset = [0,0], size = [4,1], stride = [1,0]\n+  %2 = tt.broadcast %1 : (tensor<4x1xi32>) -> tensor<4x256xi32>\n+  // offset = [0,0], size = [4,256], stride = [1,0]\n+  %arg2splat = tt.splat %arg2 : (i32) -> tensor<4x256xi32>\n+  %offset2 = arith.addi %2, %arg2splat : tensor<4x256xi32>\n+  // offset = [%arg2,0], size = [4,256], stride = [1,0]\n+  %arg3splat = tt.splat %arg3 : (i32) -> tensor<4x256xi32>\n+  %offset3 = arith.addi %offset2, %arg3splat : tensor<4x256xi32>\n+  // offset = [%arg2+%arg3,0], size = [4,256], stride = [1,0]\n+  %c10 = arith.constant 10 : i32\n+  %c10splat = tt.splat %c10 : (i32) -> tensor<4x256xi32>\n+  %offset4 = arith.addi %offset3, %c10splat : tensor<4x256xi32>\n+  // offset = [%arg2+%arg3+10,0], size = [4,256], stride = [1,0]\n+  %3 = tt.make_range {end = 256 : i32, start = 0 : i32}:tensor<256xi32>\n+  // offset = 0, size = 256, stride = 1\n+  %4 = tt.expand_dims %3 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+  // offset = [0,0], size = [1,256], stride = [0,1]\n+  %5 = tt.broadcast %4 : (tensor<1x256xi32>) -> tensor<4x256xi32>\n+  // offset = [0,0], size = [4,256], stride = [0,1]\n+  %c6 = arith.constant 6 : i32\n+  %splat6 = tt.splat %c6 : (i32) -> tensor<4x256xi32>\n+  %scale5 = arith.muli %5, %splat6 : tensor<4x256xi32>\n+  // offset = [0,0], size = [4,256], stride = [0,6]\n+  %7 = arith.addi %offset4, %scale5: tensor<4x256xi32>\n+  // offset = [%arg2+%arg3+10, 0], size = [4, 256], stride = [1, 6]\n+  %8 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<4x256x!tt.ptr<bf16>>\n+  %9 = tt.addptr %8, %7 : tensor<4x256x!tt.ptr<bf16>>,tensor<4x256xi32>\n+  // source = %arg0, offset = [%arg2+%arg3+10, 0], size = [4, 256], stride = [1, 6]\n+  %10 = tt.splat %arg1 : (!tt.ptr<bf16>) -> tensor<4x256x!tt.ptr<bf16>>\n+  %11 = tt.addptr %10, %7 : tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xi32>\n+  // source = %arg1, offset = [%arg2+%arg3+10, 0], size = [4, 256], stride = [1, 6]\n+  %12 = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<4x256xbf16>\n+  tt.store %11, %12 : tensor<4x256xbf16>\n+  tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32, %[[VAL_6:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_7:.*]] = arith.constant 6 : index\n+// CHECK-DAG:           %[[VAL_8:.*]] = arith.constant 10 : index\n+// CHECK:           %[[VAL_9:.*]] = arith.index_cast %[[VAL_2]] : i32 to index\n+// CHECK:           %[[VAL_10:.*]] = arith.index_cast %[[VAL_3]] : i32 to index\n+// CHECK:           %[[VAL_11:.*]] = arith.addi %[[VAL_9]], %[[VAL_10]] : index\n+// CHECK:           %[[VAL_12:.*]] = arith.addi %[[VAL_11]], %[[VAL_8]] : index\n+// CHECK:           %[[VAL_13:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_12]]], sizes: [4, 256], strides: [1, %[[VAL_7]]] : memref<*xbf16> to memref<4x256xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           %[[VAL_14:.*]] = arith.index_cast %[[VAL_2]] : i32 to index\n+// CHECK:           %[[VAL_15:.*]] = arith.index_cast %[[VAL_3]] : i32 to index\n+// CHECK:           %[[VAL_16:.*]] = arith.addi %[[VAL_14]], %[[VAL_15]] : index\n+// CHECK:           %[[VAL_17:.*]] = arith.addi %[[VAL_16]], %[[VAL_8]] : index\n+// CHECK:           %[[VAL_18:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}%[[VAL_17]]], sizes: [4, 256], strides: [1, %[[VAL_7]]] : memref<*xbf16> to memref<4x256xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           %[[VAL_19:.*]] = memref.alloc() : memref<4x256xbf16>\n+// CHECK:           memref.copy %[[VAL_13]], %[[VAL_19]] : memref<4x256xbf16, strided<[1, ?], offset: ?>> to memref<4x256xbf16>\n+// CHECK:           %[[VAL_20:.*]] = bufferization.to_tensor %[[VAL_19]] restrict writable : memref<4x256xbf16>\n+// CHECK:           memref.tensor_store %[[VAL_20]], %[[VAL_18]] : memref<4x256xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_for_accumulation.mlir", "status": "added", "additions": 93, "deletions": 0, "changes": 93, "file_content_changes": "@@ -0,0 +1,93 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %arg0 : !tt.ptr<bf16>,\n+    %arg1 : !tt.ptr<bf16>,\n+    %arg2 : !tt.ptr<bf16>,\n+    %arg3 : i32,\n+    %arg4 : i32\n+  )\n+  {\n+    %0 = tt.make_range {end = 4 : i32, start = 0 : i32}:tensor<4xi32>\n+    // offset = 0, size = 4, stride = 1\n+    %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<4xi32>) -> tensor<4x1xi32>\n+    // offset = [0,0], size = [4,1], stride = [1,0]\n+    %2 = tt.broadcast %1 : (tensor<4x1xi32>) -> tensor<4x256xi32>\n+    // offset = [0,0], size = [4,256], stride = [1,0]\n+    %arg3splat = tt.splat %arg3 : (i32) -> tensor<4x256xi32>\n+    %offset3 = arith.addi %2, %arg3splat : tensor<4x256xi32>\n+    // offset = [%arg3,0], size = [4,256], stride = [1,0]\n+    %3 = tt.make_range {end = 256 : i32, start = 0 : i32}:tensor<256xi32>\n+    // offset = 0, size = 256, stride = 1\n+    %4 = tt.expand_dims %3 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+    // offset = [0,0], size = [1,256], stride = [0,1]\n+    %5 = tt.broadcast %4 : (tensor<1x256xi32>) -> tensor<4x256xi32>\n+    // offset = [0,0], size = [4,256], stride = [0,1]\n+    %c5 = arith.constant 5 : i32\n+    %splat6 = tt.splat %c5 : (i32) -> tensor<4x256xi32>\n+    // scalar = 5\n+    %scale5 = arith.muli %5, %splat6 : tensor<4x256xi32> // Why we never called the conversion function for the inputs here?\n+    // offset = [0,0], size = [4,256], stride = [0,5]\n+    %7 = arith.addi %offset3, %scale5: tensor<4x256xi32> // Why we never called the conversion function for the inputs here?\n+    // offset = [%arg3, 0], size = [4, 256], stride = [1, 5]\n+    %8 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<4x256x!tt.ptr<bf16>> // Why is the input unknown\n+    %9 = tt.addptr %8, %7 : tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xi32>\n+    // source: %arg0, offset = [%arg3, 0], size = [4, 256], stride = [1, 5]\n+    %19 = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<4x256xbf16> // this will be replaced with a memref.copy\n+    %11 = tt.splat %arg1 : (!tt.ptr<bf16>) -> tensor<4x256x!tt.ptr<bf16>>\n+    %12 = tt.addptr %11, %7 : tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xi32>\n+    // source: %arg1, offset = [%arg3, 0], size = [4, 256], stride = [1, 5]\n+    %c0 = arith.constant 0 : index\n+    %c12 = arith.constant 12 : index\n+    %c3 = arith.constant 3 : index\n+    %i_c3 = arith.constant 3 : i32\n+    %sum_out, %_ptr = scf.for %i = %c0 to %c12 step %c3 iter_args(%sum_iter = %19, %ptr_iter = %12) -> (tensor<4x256xbf16>, tensor<4x256x!tt.ptr<bf16>>) {\n+        %20 = tt.load %ptr_iter {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<4x256xbf16>\n+        %sum = arith.addf %sum_iter, %20 : tensor<4x256xbf16>\n+        // pointer updates\n+        %17 = tt.splat %i_c3 : (i32) -> tensor<4x256xi32>\n+        // offset: [3, 0], size = [4, 256], stride [0, 0]\n+        %ptr = tt.addptr %ptr_iter, %17 : tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xi32>\n+        // source: %arg1, offset = [%arg3+%i, 0], size = [4, 256], stride = [1, 5]\n+        scf.yield %sum, %ptr : tensor<4x256xbf16>, tensor<4x256x!tt.ptr<bf16>>\n+    }\n+    %15 = tt.splat %arg2 : (!tt.ptr<bf16>) -> tensor<4x256x!tt.ptr<bf16>>\n+    %16 = tt.addptr %15, %7 : tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xi32>\n+    // source: %arg2, offset = [%arg3, 0], size = [4, 256], stride = [1, 5]\n+    tt.store %16, %sum_out : tensor<4x256xbf16>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: memref<*xbf16>, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32, %[[VAL_6:.*]]: i32, %[[VAL_7:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_8:.*]] = arith.constant 5 : index\n+// CHECK-DAG:           %[[VAL_9:.*]] = arith.constant 1 : index\n+// CHECK-DAG:           %[[VAL_10:.*]] = arith.constant 3 : index\n+// CHECK-DAG:           %[[VAL_11:.*]] = arith.constant 12 : index\n+// CHECK-DAG:           %[[VAL_12:.*]] = arith.constant 0 : index\n+// CHECK:           %[[VAL_13:.*]] = arith.index_cast %[[VAL_3]] : i32 to index\n+// CHECK:           %[[VAL_14:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_13]]], sizes: [4, 256], strides: [1, %[[VAL_8]]] : memref<*xbf16> to memref<4x256xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           %[[VAL_15:.*]] = memref.alloc() : memref<4x256xbf16>\n+// CHECK:           memref.copy %[[VAL_14]], %[[VAL_15]] : memref<4x256xbf16, strided<[1, ?], offset: ?>> to memref<4x256xbf16>\n+// CHECK:           %[[VAL_16:.*]] = bufferization.to_tensor %[[VAL_15]] restrict writable : memref<4x256xbf16>\n+// CHECK:           %[[VAL_17:.*]] = arith.index_cast %[[VAL_3]] : i32 to index\n+// CHECK:           %[[VAL_18:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}%[[VAL_17]]], sizes: [4, 256], strides: {{\\[}}%[[VAL_9]], %[[VAL_8]]] : memref<*xbf16> to memref<4x256xbf16, strided<[?, ?], offset: ?>>\n+// CHECK:           %[[VAL_19:.*]]:4 = scf.for %[[VAL_20:.*]] = %[[VAL_12]] to %[[VAL_11]] step %[[VAL_10]] iter_args(%[[VAL_21:.*]] = %[[VAL_16]], %[[VAL_22:.*]] = %[[VAL_18]], %[[VAL_23:.*]] = %[[VAL_17]], %[[VAL_24:.*]] = %[[VAL_12]]) -> (tensor<4x256xbf16>, memref<4x256xbf16, strided<[?, ?], offset: ?>>, index, index) {\n+// CHECK:             %[[VAL_25:.*]] = memref.alloc() : memref<4x256xbf16>\n+// CHECK:             memref.copy %[[VAL_22]], %[[VAL_25]] : memref<4x256xbf16, strided<[?, ?], offset: ?>> to memref<4x256xbf16>\n+// CHECK:             %[[VAL_26:.*]] = bufferization.to_tensor %[[VAL_25]] restrict writable : memref<4x256xbf16>\n+// CHECK:             %[[VAL_27:.*]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_21]], %[[VAL_26]] : tensor<4x256xbf16>, tensor<4x256xbf16>) outs(%[[VAL_21]] : tensor<4x256xbf16>) {\n+// CHECK:             ^bb0(%[[VAL_28:.*]]: bf16, %[[VAL_29:.*]]: bf16, %[[VAL_30:.*]]: bf16):\n+// CHECK:               %[[VAL_31:.*]] = arith.addf %[[VAL_28]], %[[VAL_29]] : bf16\n+// CHECK:               linalg.yield %[[VAL_31]] : bf16\n+// CHECK:             } -> tensor<4x256xbf16>\n+// CHECK:             %[[VAL_32:.*]] = arith.addi %[[VAL_23]], %[[VAL_10]] : index\n+// CHECK:             %[[VAL_33:.*]] = arith.addi %[[VAL_32]], %[[VAL_24]] : index\n+// CHECK:             %[[VAL_34:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}%[[VAL_33]]], sizes: [4, 256], strides: {{\\[}}%[[VAL_9]], %[[VAL_8]]] : memref<*xbf16> to memref<4x256xbf16, strided<[?, ?], offset: ?>>\n+// CHECK:             scf.yield %[[VAL_35:.*]], %[[VAL_34]], %[[VAL_33]], %[[VAL_12]] : tensor<4x256xbf16>, memref<4x256xbf16, strided<[?, ?], offset: ?>>, index, index\n+// CHECK:           }\n+// CHECK:           %[[VAL_36:.*]] = arith.index_cast %[[VAL_3]] : i32 to index\n+// CHECK:           %[[VAL_37:.*]] = memref.reinterpret_cast %[[VAL_2]] to offset: {{\\[}}%[[VAL_36]]], sizes: [4, 256], strides: [1, %[[VAL_8]]] : memref<*xbf16> to memref<4x256xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           memref.tensor_store %[[VAL_38:.*]]#0, %[[VAL_37]] : memref<4x256xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_for_expand_ptr.mlir", "status": "added", "additions": 64, "deletions": 0, "changes": 64, "file_content_changes": "@@ -0,0 +1,64 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %arg0 : !tt.ptr<bf16>\n+  )\n+  {\n+    %c0 = arith.constant 0 : index\n+    %c12 = arith.constant 12 : index\n+    %c3 = arith.constant 3 : index\n+    %i_c3 = arith.constant 3 : i32\n+    %0 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<256x!tt.ptr<bf16>>\n+    %1 = tt.make_range {end = 2048 : i32, start = 1024 : i32}:tensor<256xi32>\n+    // source: null, sizes: 256, offsets: 1024, strides: 4\n+    %2 = tt.addptr %0, %1 : tensor<256x!tt.ptr<bf16>>, tensor<256xi32>\n+    // source: arg0, sizes: 256, offsets: 1024, strides: 4\n+    // gep operand is another gep' output, which is passed into the loop as varible, used after update\n+    %_ptr = scf.for %i = %c0 to %c12 step %c3 iter_args(%ptr = %2) -> (tensor<256x!tt.ptr<bf16>>) {\n+      %6 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32>\n+      %7 = tt.expand_dims %6 {axis = 1 : i32} : (tensor<256xi32>) -> tensor<256x1xi32>\n+      %8 = tt.broadcast %7 : (tensor<256x1xi32>) -> tensor<256x256xi32>\n+      // sizes: [256, 256], offsets: [0, 0], strides: [1, 0]\n+      %9 = tt.make_range {end = 512 : i32, start = 256 : i32} : tensor<256xi32>\n+      %10 = tt.expand_dims %9 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+      %11 = tt.broadcast %10 : (tensor<1x256xi32>) -> tensor<256x256xi32>\n+      // sizes: [256, 256], offsets: [0, 256], strides: [0, 1]\n+      %12 = arith.addi %8, %11 : tensor<256x256xi32>\n+      // sizes: [256, 256], offsets: [0, 256], strides: [1, 1]\n+      %13 = tt.expand_dims %ptr {axis = 1 : i32} : (tensor<256x!tt.ptr<bf16>>) -> tensor<256x1x!tt.ptr<bf16>>\n+      %14 = tt.broadcast %13 : (tensor<256x1x!tt.ptr<bf16>>) -> tensor<256x256x!tt.ptr<bf16>>\n+      %15 = tt.addptr %14, %12 : tensor<256x256x!tt.ptr<bf16>>, tensor<256x256xi32>\n+      // source: arg0, sizes: [256, 256], offsets: [1024 + i, 256], strides: [5, 1]\n+      // perform load\n+      %16 = tt.load %15 {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<256x256xbf16>\n+      tt.store %15, %16 : tensor<256x256xbf16>\n+      // pointer updates\n+      %17 = tt.splat %i_c3 : (i32) -> tensor<256xi32>\n+      // sizes: 256, offsets: 3, strides: 0\n+      %ptr_iter = tt.addptr %ptr, %17 : tensor<256x!tt.ptr<bf16>>, tensor<256xi32>\n+      // source: arg0, sizes: 256, offsets: 1024 + i, strides: 4\n+      scf.yield %ptr_iter : tensor<256x!tt.ptr<bf16>>\n+    }\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: i32, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_4:.*]] = arith.constant 5 : index\n+// CHECK-DAG:           %[[VAL_5:.*]] = arith.constant 256 : index\n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 1024 : index\n+// CHECK-DAG:           %[[VAL_7:.*]] = arith.constant 0 : index\n+// CHECK-DAG:           %[[VAL_8:.*]] = arith.constant 12 : index\n+// CHECK-DAG:           %[[VAL_9:.*]] = arith.constant 3 : index\n+// CHECK:           %[[VAL_10:.*]] = scf.for %[[VAL_11:.*]] = %[[VAL_7]] to %[[VAL_8]] step %[[VAL_9]] iter_args(%[[VAL_12:.*]] = %[[VAL_6]]) -> (index) {\n+// CHECK:             %[[VAL_13:.*]] = arith.addi %[[VAL_12]], %[[VAL_5]] : index\n+// CHECK:             %[[VAL_14:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_13]]], sizes: [256, 256], strides: {{\\[}}%[[VAL_4]], 1] : memref<*xbf16> to memref<256x256xbf16, strided<[?, 1], offset: ?>>\n+// CHECK:             %[[VAL_15:.*]] = memref.alloc() : memref<256x256xbf16>\n+// CHECK:             memref.copy %[[VAL_14]], %[[VAL_15]] : memref<256x256xbf16, strided<[?, 1], offset: ?>> to memref<256x256xbf16>\n+// CHECK:             %[[VAL_16:.*]] = bufferization.to_tensor %[[VAL_15]] restrict writable : memref<256x256xbf16>\n+// CHECK:             memref.tensor_store %[[VAL_16]], %[[VAL_14]] : memref<256x256xbf16, strided<[?, 1], offset: ?>>\n+// CHECK:             %[[VAL_17:.*]] = arith.addi %[[VAL_12]], %[[VAL_9]] : index\n+// CHECK:             scf.yield %[[VAL_17]] : index\n+// CHECK:           }\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_for_more_init_args.mlir", "status": "added", "additions": 72, "deletions": 0, "changes": 72, "file_content_changes": "@@ -0,0 +1,72 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %arg0 : !tt.ptr<bf16>,\n+    %arg1 : !tt.ptr<bf16>\n+  )\n+  {\n+    %c0 = arith.constant 0 : index\n+    %c1 = arith.constant 1 : index\n+    %c2 = arith.constant 2 : index\n+    %c3 = arith.constant 3 : index\n+    %c12 = arith.constant 12 : index\n+    %0 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<256x!tt.ptr<bf16>>\n+    %1 = tt.make_range {end = 2048 : i32, start = 1024 : i32}:tensor<256xi32>\n+    // source: null, sizes: 256, offsets: 1024, strides: 4\n+    %2 = tt.addptr %0, %1 : tensor<256x!tt.ptr<bf16>>, tensor<256xi32>\n+    // source: arg0, sizes: 256, offsets: 1024, strides: 4\n+    %3 = tt.splat %arg1 : (!tt.ptr<bf16>) -> tensor<256x!tt.ptr<bf16>>\n+    %4 = tt.addptr %3, %1 : tensor<256x!tt.ptr<bf16>>, tensor<256xi32>\n+    // source: arg1, sizes: 256, offsets: 1024, strides: 4\n+    %_arg2, %_ptr_ld, %_arg3, %_ptr_st, %_arg4 = scf.for %i = %c0 to %c12 step %c3 iter_args(%arg2 = %c1, %ptr_ld = %2, %arg3 = %c2, %ptr_st = %4, %arg4 = %c3) -> (index, tensor<256x!tt.ptr<bf16>>, index, tensor<256x!tt.ptr<bf16>>, index) {\n+        // perform load\n+        %5 = tt.load %ptr_ld {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<256xbf16>\n+        tt.store %ptr_st, %5 : tensor<256xbf16>\n+        // pointer updates\n+        %cast3 = arith.index_cast %c3 : index to i32\n+        %6 = tt.splat %cast3 : (i32) -> tensor<256xi32>\n+        %ptr_ld_iter = tt.addptr %ptr_ld, %6 : tensor<256x!tt.ptr<bf16>>, tensor<256xi32>\n+        // source: arg0, sizes: 256, offsets: 1024 + i*3, strides: 4\n+        %arg2_iter = arith.addi %arg2, %c3 : index\n+        %arg3_iter = arith.addi %arg3, %c3 : index\n+        %arg4_iter = arith.addi %arg4, %c3 : index\n+        %7 = arith.addi %arg2_iter, %arg3_iter : index\n+        %8 = arith.addi %7, %arg4_iter : index\n+        %cast8 = arith.index_cast %8 : index to i32\n+        %9 = tt.splat %cast8 : (i32) -> tensor<256xi32>\n+        %ptr_st_iter = tt.addptr %ptr_st, %9 : tensor<256x!tt.ptr<bf16>>, tensor<256xi32>\n+        // source: arg1, sizes: 256, offsets: 1024 + loop-carry variable*i, strides: 4\n+        scf.yield %arg2_iter, %ptr_ld_iter, %arg3_iter, %ptr_st_iter, %arg4_iter : index, tensor<256x!tt.ptr<bf16>>, index, tensor<256x!tt.ptr<bf16>>, index\n+    }\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_5:.*]] = arith.constant 4 : index\n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 1024 : index\n+// CHECK-DAG:           %[[VAL_7:.*]] = arith.constant 0 : index\n+// CHECK-DAG:           %[[VAL_8:.*]] = arith.constant 1 : index\n+// CHECK-DAG:           %[[VAL_9:.*]] = arith.constant 2 : index\n+// CHECK-DAG:           %[[VAL_10:.*]] = arith.constant 3 : index\n+// CHECK-DAG:           %[[VAL_11:.*]] = arith.constant 12 : index\n+// CHECK:           %[[VAL_12:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_6]]], sizes: [256], strides: {{\\[}}%[[VAL_5]]] : memref<*xbf16> to memref<256xbf16, strided<[?], offset: ?>>\n+// CHECK:           %[[VAL_13:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}%[[VAL_6]]], sizes: [256], strides: {{\\[}}%[[VAL_5]]] : memref<*xbf16> to memref<256xbf16, strided<[?], offset: ?>>\n+// CHECK:           %[[VAL_14:.*]]:7 = scf.for %[[VAL_15:.*]] = %[[VAL_7]] to %[[VAL_11]] step %[[VAL_10]] iter_args(%[[VAL_16:.*]] = %[[VAL_8]], %[[VAL_17:.*]] = %[[VAL_12]], %[[VAL_18:.*]] = %[[VAL_9]], %[[VAL_19:.*]] = %[[VAL_13]], %[[VAL_20:.*]] = %[[VAL_10]], %[[VAL_21:.*]] = %[[VAL_6]], %[[VAL_22:.*]] = %[[VAL_6]]) -> (index, memref<256xbf16, strided<[?], offset: ?>>, index, memref<256xbf16, strided<[?], offset: ?>>, index, index, index) {\n+// CHECK:             %[[VAL_23:.*]] = memref.alloc() : memref<256xbf16>\n+// CHECK:             memref.copy %[[VAL_17]], %[[VAL_23]] : memref<256xbf16, strided<[?], offset: ?>> to memref<256xbf16>\n+// CHECK:             %[[VAL_24:.*]] = bufferization.to_tensor %[[VAL_23]] restrict writable : memref<256xbf16>\n+// CHECK:             memref.tensor_store %[[VAL_24]], %[[VAL_19]] : memref<256xbf16, strided<[?], offset: ?>>\n+// CHECK:             %[[VAL_25:.*]] = arith.addi %[[VAL_21]], %[[VAL_10]] : index\n+// CHECK:             %[[VAL_26:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_25]]], sizes: [256], strides: {{\\[}}%[[VAL_5]]] : memref<*xbf16> to memref<256xbf16, strided<[?], offset: ?>>\n+// CHECK:             %[[VAL_27:.*]] = arith.addi %[[VAL_16]], %[[VAL_10]] : index\n+// CHECK:             %[[VAL_28:.*]] = arith.addi %[[VAL_18]], %[[VAL_10]] : index\n+// CHECK:             %[[VAL_29:.*]] = arith.addi %[[VAL_20]], %[[VAL_10]] : index\n+// CHECK:             %[[VAL_30:.*]] = arith.addi %[[VAL_27]], %[[VAL_28]] : index\n+// CHECK:             %[[VAL_31:.*]] = arith.addi %[[VAL_30]], %[[VAL_29]] : index\n+// CHECK:             %[[VAL_32:.*]] = arith.addi %[[VAL_22]], %[[VAL_31]] : index\n+// CHECK:             %[[VAL_33:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}%[[VAL_32]]], sizes: [256], strides: {{\\[}}%[[VAL_5]]] : memref<*xbf16> to memref<256xbf16, strided<[?], offset: ?>>\n+// CHECK:             scf.yield %[[VAL_27]], %[[VAL_26]], %[[VAL_28]], %[[VAL_33]], %[[VAL_29]], %[[VAL_25]], %[[VAL_32]] : index, memref<256xbf16, strided<[?], offset: ?>>, index, memref<256xbf16, strided<[?], offset: ?>>, index, index, index\n+// CHECK:           }\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_for_used_after_update.mlir", "status": "added", "additions": 98, "deletions": 0, "changes": 98, "file_content_changes": "@@ -0,0 +1,98 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %arg0 : !tt.ptr<bf16>\n+  )\n+  {\n+    %c0 = arith.constant 0 : index\n+    %c12 = arith.constant 12 : index\n+    %c3 = arith.constant 3 : index\n+    %i_c3 = arith.constant 3 : i32\n+    %0 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<256x!tt.ptr<bf16>>\n+    %1 = tt.make_range {end = 2048 : i32, start = 1024 : i32}:tensor<256xi32>\n+    // source: null, sizes: 256, offsets: 1024, strides: 4\n+    %2 = tt.addptr %0, %1 : tensor<256x!tt.ptr<bf16>>, tensor<256xi32>\n+    // source: arg0, sizes: 256, offsets: 1024, strides: 4\n+    // gep operand is another gep' output, which is passed into the loop as varible, used after update\n+    %_ptr = scf.for %i = %c0 to %c12 step %c3 iter_args(%ptr = %2) -> (tensor<256x!tt.ptr<bf16>>) {\n+        // pointer updates\n+        %4 = tt.splat %i_c3 : (i32) -> tensor<256xi32>\n+        // sizes: 256, offsets: 3, strides: 0\n+        %ptr_iter = tt.addptr %ptr, %4 : tensor<256x!tt.ptr<bf16>>, tensor<256xi32>\n+        // source: arg0, sizes: 256, offsets: 1024 + i, strides: 4\n+        // perform load\n+        %3 = tt.load %ptr_iter {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<256xbf16>\n+        tt.store %ptr_iter, %3 : tensor<256xbf16>\n+        scf.yield %ptr_iter : tensor<256x!tt.ptr<bf16>>\n+    }\n+    // Expected output\n+    // %offset_dim0 = arith.constant 1024                                                       <- insert instructions to initialize init arg(new)\n+    // for iter_args (%offset_dim0_iter = %offset_dim0) {                                       <- replace varibles passed in as init arg (new)\n+    //  %4 = %offset_dim0_iter + %c3                                                            <- replace gep of splat with add (already done)\n+    //  %subview = memref.subview %arg0, [%4][256][4] : memref<> -> memref<>                    <- generate subview on getelementptr (already done)\n+    //  ...\n+    //  scf.yield %4                                                                            <- replace yielding an gep output with the corresponding dim variable (new)\n+    // }\n+    // TODO: examples below are not supported since scf.for does not support returning a tensor type\n+    // Example 3, gep operand is a vector of i32, which is passed into the loop as variable, pointer updated using step, used after update\n+    //%_ptr3 = scf.for %i = %c0 to %c12 step %c3 iter_args(%ptr = %1) -> (tensor<256xi32>) {\n+    //    // offset update\n+    //    %3 = tt.splat %c3 : (i32) -> tensor<256xi32>\n+    //    %ptr_iter = arith.addi %3, %ptr : tensor<256xi32>\n+    //    // generate pointer\n+    //    %gep_ptr = tt.addptr %0, %ptr_iter : tensor<256x!tt.ptr<bf16>>\n+    //    // perform load\n+    //    %4 = tt.load %gep_ptr {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<256xbf16>\n+    //    tt.store %gep_ptr, %4 : tensor<256xbf16>\n+    //    scf.yield %ptr_iter : tensor<256xi32>\n+    //}\n+    // Expected output\n+    // %offset_dim0 = arith.constant 1024                                                       <- insert instructions to initialize init arg(new)\n+    // for iter_args (%offset_dim0_iter = %offset_dim0) {                                       <- replace varibles passed in as init arg (new)\n+    //  %4 = %offset_dim0_iter + %c3                                                            <- replace gep of splat with add (already done)\n+    //  %subview = memref.subview %arg0, [%offset_dim0_iter][256][4] : memref<> -> memref<>     <- generate subview on load (new)\n+    //  ...\n+    //  scf.yield %4                                                                            <- replace yielding an gep output with the corresponding dim variable (new)\n+    // }\n+    //// Example 4, gep operand is a vector of i32, which is passed into the loop as variable, pointer updated using step, used before update\n+    //%_ptr4 = scf.for %i = %c0 to %c12 step %c3 iter_args(%ptr = %1) -> (tensor<256xi32>) {\n+    //    // generate pointer\n+    //    %gep_ptr = tt.addptr %0, %ptr : tensor<256x!tt.ptr<bf16>>\n+    //\n+    //    // perform load\n+    //    %4 = tt.load %gep_ptr {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<256xbf16>\n+    //    tt.store %gep_ptr, %4 : tensor<256xbf16>\n+    //    // offset update\n+    //    %3 = tt.splat %c3 : (i32) -> tensor<256xi32>\n+    //    %ptr_iter = arith.addi %3, %ptr : tensor<256xi32>\n+    //    scf.yield %ptr_iter : tensor<256xi32>\n+    //}\n+    // Expected output\n+    // %offset_dim0 = arith.constant 1024                                                       <- insert instructions to initialize init arg(new)\n+    // for iter_args (%offset_dim0_iter = %offset_dim0) {                                       <- replace varibles passed in as init arg (new)\n+    //  %subview = memref.subview %arg0, [%offset_dim0_iter][256][4] : memref<> -> memref<>     <- generate subview on load (new)\n+    //  ...\n+    //  %4 = %offset_dim0_iter + %c3                                                            <- replace gep of splat with add (already done)\n+    //  scf.yield %4                                                                            <- replace yielding an gep output with the corresponding dim variable (new)\n+    // }\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: i32, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_4:.*]] = arith.constant 4 : index\n+// CHECK-DAG:           %[[VAL_5:.*]] = arith.constant 1024 : index\n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 0 : index\n+// CHECK-DAG:           %[[VAL_7:.*]] = arith.constant 12 : index\n+// CHECK-DAG:           %[[VAL_8:.*]] = arith.constant 3 : index\n+// CHECK:           %[[VAL_9:.*]] = scf.for %[[VAL_10:.*]] = %[[VAL_6]] to %[[VAL_7]] step %[[VAL_8]] iter_args(%[[VAL_11:.*]] = %[[VAL_5]]) -> (index) {\n+// CHECK:             %[[VAL_12:.*]] = arith.addi %[[VAL_11]], %[[VAL_8]] : index\n+// CHECK:             %[[VAL_13:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_12]]], sizes: [256], strides: {{\\[}}%[[VAL_4]]] : memref<*xbf16> to memref<256xbf16, strided<[?], offset: ?>>\n+// CHECK:             %[[VAL_14:.*]] = memref.alloc() : memref<256xbf16>\n+// CHECK:             memref.copy %[[VAL_13]], %[[VAL_14]] : memref<256xbf16, strided<[?], offset: ?>> to memref<256xbf16>\n+// CHECK:             %[[VAL_15:.*]] = bufferization.to_tensor %[[VAL_14]] restrict writable : memref<256xbf16>\n+// CHECK:             memref.tensor_store %[[VAL_15]], %[[VAL_13]] : memref<256xbf16, strided<[?], offset: ?>>\n+// CHECK:             scf.yield %[[VAL_12]] : index\n+// CHECK:           }\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_for_used_before_update.mlir", "status": "added", "additions": 55, "deletions": 0, "changes": 55, "file_content_changes": "@@ -0,0 +1,55 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %arg0 : !tt.ptr<bf16>\n+  )\n+  {\n+    %c0 = arith.constant 0 : index\n+    %c12 = arith.constant 12 : index\n+    %c3 = arith.constant 3 : index\n+    %i_c3 = arith.constant 3 : i32\n+    %0 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<256x!tt.ptr<bf16>>\n+    %1 = tt.make_range {end = 2048 : i32, start = 1024 : i32}:tensor<256xi32>\n+    // source: null, sizes: 256, offsets: 1024, strides: 4\n+    %2 = tt.addptr %0, %1 : tensor<256x!tt.ptr<bf16>>, tensor<256xi32>\n+    // source: arg0, sizes: 256, offsets: 1024, strides: 4\n+    // Example 2, gep operand is another gep's output, which is passed into the loop as varible, used before update\n+    %_ptr2 = scf.for %i = %c0 to %c12 step %c3 iter_args(%ptr = %2) -> (tensor<256x!tt.ptr<bf16>>) {\n+        // perform load\n+        %3 = tt.load %ptr {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<256xbf16>\n+        tt.store %ptr, %3 : tensor<256xbf16>\n+        // pointer updates\n+        %4 = tt.splat %i_c3 : (i32) -> tensor<256xi32>\n+        %ptr_iter = tt.addptr %ptr, %4 : tensor<256x!tt.ptr<bf16>>, tensor<256xi32>\n+        scf.yield %ptr_iter : tensor<256x!tt.ptr<bf16>>\n+    }\n+    // Expected output\n+    // %offset_dim0 = arith.constant 1024                                                       <- insert instructions to initialize init arg(new)\n+    // for iter_args (%offset_dim0_iter = %offset_dim0) {                                       <- replace varibles passed in as init arg (new)\n+    //  %subview = memref.subview %arg0, [%offset_dim0_iter][256][4] : memref<> -> memref<>     <- generate subview on load (new)\n+    //  ...\n+    //  %4 = %offset_dim0_iter + %c3                                                            <- replace gep of splat with add (already done)\n+    //  scf.yield %4                                                                            <- replace yielding an gep output with the corresponding dim variable (new)\n+    // }\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: i32, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_4:.*]] = arith.constant 4 : index\n+// CHECK-DAG:           %[[VAL_5:.*]] = arith.constant 1024 : index\n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 0 : index\n+// CHECK-DAG:           %[[VAL_7:.*]] = arith.constant 12 : index\n+// CHECK-DAG:           %[[VAL_8:.*]] = arith.constant 3 : index\n+// CHECK:           %[[VAL_9:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_5]]], sizes: [256], strides: {{\\[}}%[[VAL_4]]] : memref<*xbf16> to memref<256xbf16, strided<[?], offset: ?>>\n+// CHECK:           %[[VAL_10:.*]]:2 = scf.for %[[VAL_11:.*]] = %[[VAL_6]] to %[[VAL_7]] step %[[VAL_8]] iter_args(%[[VAL_12:.*]] = %[[VAL_9]], %[[VAL_13:.*]] = %[[VAL_5]]) -> (memref<256xbf16, strided<[?], offset: ?>>, index) {\n+// CHECK:             %[[VAL_14:.*]] = memref.alloc() : memref<256xbf16>\n+// CHECK:             memref.copy %[[VAL_12]], %[[VAL_14]] : memref<256xbf16, strided<[?], offset: ?>> to memref<256xbf16>\n+// CHECK:             %[[VAL_15:.*]] = bufferization.to_tensor %[[VAL_14]] restrict writable : memref<256xbf16>\n+// CHECK:             memref.tensor_store %[[VAL_15]], %[[VAL_12]] : memref<256xbf16, strided<[?], offset: ?>>\n+// CHECK:             %[[VAL_16:.*]] = arith.addi %[[VAL_13]], %[[VAL_8]] : index\n+// CHECK:             %[[VAL_17:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_16]]], sizes: [256], strides: {{\\[}}%[[VAL_4]]] : memref<*xbf16> to memref<256xbf16, strided<[?], offset: ?>>\n+// CHECK:             scf.yield %[[VAL_17]], %[[VAL_16]] : memref<256xbf16, strided<[?], offset: ?>>, index\n+// CHECK:           }\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_loopback.mlir", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -0,0 +1,53 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+  %arg0 : !tt.ptr<bf16>,\n+  %arg1 : !tt.ptr<bf16>,\n+  %arg2 : i32\n+  )\n+  {\n+  %0 = tt.make_range {end = 4 : i32, start = 0 : i32}:tensor<4xi32>\n+  // offset = 0, size = 4, stride = 1\n+  %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<4xi32>) -> tensor<4x1xi32>\n+  // offset = [0,0], size = [4,1], stride = [1,0]\n+  %2 = tt.broadcast %1 : (tensor<4x1xi32>) -> tensor<4x256xi32>\n+  // offset = [0,0], size = [4,256], stride = [1,0]\n+  %arg2splat = tt.splat %arg2 : (i32) -> tensor<4x256xi32>\n+  %offset2 = arith.addi %2, %arg2splat : tensor<4x256xi32>\n+  // offset = [%arg2,0], size = [4,256], stride = [1,0]\n+  %3 = tt.make_range {end = 256 : i32, start = 0 : i32}:tensor<256xi32>\n+  // offset = 0, size = 256, stride = 1\n+  %4 = tt.expand_dims %3 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+  // offset = [0,0], size = [1,256], stride = [0,1]\n+  %5 = tt.broadcast %4 : (tensor<1x256xi32>) -> tensor<4x256xi32>\n+  // offset = [0,0], size = [4,256], stride = [0,1]\n+  %c6 = arith.constant 6 : i32\n+  %splat6 = tt.splat %c6 : (i32) -> tensor<4x256xi32>\n+  %scale5 = arith.muli %5, %splat6 : tensor<4x256xi32>\n+  // offset = [0,0], size = [4,256], stride = [0,6]\n+  %7 = arith.addi %offset2, %scale5: tensor<4x256xi32>\n+  // offset = [%arg2, 0], size = [4, 256], stride = [1, 6]\n+  %8 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<4x256x!tt.ptr<bf16>>\n+  %9 = tt.addptr %8, %7 : tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xi32>\n+  // source: arg0, offset = [%arg2, 0], size = [4, 256], stride = [1, 6]\n+  %10 = tt.splat %arg1 : (!tt.ptr<bf16>) -> tensor<4x256x!tt.ptr<bf16>>\n+  %11 = tt.addptr %10, %7 : tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xi32>\n+  // source: arg1, offset = [%arg2, 0], size = [4, 256], stride = [1, 6]\n+  %12 = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<4x256xbf16>\n+  tt.store %11, %12 : tensor<4x256xbf16>\n+  tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32) { \n+// CHECK:           %[[VAL_6:.*]] = arith.constant 6 : index\n+// CHECK:           %[[VAL_7:.*]] = arith.index_cast %[[VAL_2]] : i32 to index\n+// CHECK:           %[[VAL_8:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_7]]], sizes: [4, 256], strides: [1, %[[VAL_6]]] : memref<*xbf16> to memref<4x256xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           %[[VAL_9:.*]] = arith.index_cast %[[VAL_2]] : i32 to index\n+// CHECK:           %[[VAL_10:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}%[[VAL_9]]], sizes: [4, 256], strides: [1, %[[VAL_6]]] : memref<*xbf16> to memref<4x256xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           %[[VAL_11:.*]] = memref.alloc() : memref<4x256xbf16>\n+// CHECK:           memref.copy %[[VAL_8]], %[[VAL_11]] : memref<4x256xbf16, strided<[1, ?], offset: ?>> to memref<4x256xbf16>\n+// CHECK:           %[[VAL_12:.*]] = bufferization.to_tensor %[[VAL_11]] restrict writable : memref<4x256xbf16>\n+// CHECK:           memref.tensor_store %[[VAL_12]], %[[VAL_10]] : memref<4x256xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_mul_const_const.mlir", "status": "added", "additions": 48, "deletions": 0, "changes": 48, "file_content_changes": "@@ -0,0 +1,48 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %arg0 : !tt.ptr<bf16>,\n+    %arg1 : !tt.ptr<bf16>,\n+    %arg2 : i32\n+  )\n+  {\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = tt.make_range {end = 1024 : i32, start = 0 : i32}:tensor<1024xi32>\n+    %2 = tt.splat %0 : (i32) -> tensor<1024xi32>\n+    %3 = arith.addi %2, %1 : tensor<1024xi32>\n+    //%3: splat(%0) + range(0, 1024)\n+    //%3: offset = %0, size = 1024, stride = 1\n+    // vector and scalar are both constant\n+    %4 = tt.make_range {end = 4096 : i32, start = 2048 : i32}:tensor<1024xi32>\n+    %c10 = arith.constant 10 : i32\n+    %5 = tt.splat %c10 : (i32) -> tensor<1024xi32>\n+    %6 = arith.muli %5, %4 : tensor<1024xi32>\n+    //%6: splat(%c10)*range(2048, 4096);\n+    //%6: offset = %c10*2048, size = 1024, stride = %c10*2\n+    %7 = arith.addi %3, %6 : tensor<1024xi32>\n+    //%7: offset = %c10*2048 + %0, size = 1024, stride = %c10*2+1\n+    %8 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<1024x!tt.ptr<bf16>>\n+    %9 = tt.addptr %8, %7 : tensor<1024x!tt.ptr<bf16>>, tensor<1024xi32>\n+    //source=%arg0 offset = %c10*2048 + pid0, size = 1024, stride = %c10*2+1\n+    %10 = tt.splat %arg1 : (!tt.ptr<bf16>) -> tensor<1024x!tt.ptr<bf16>>\n+    %11 = tt.addptr %10, %3 : tensor<1024x!tt.ptr<bf16>>, tensor<1024xi32>\n+    //source=%arg1, offset = pid0, size = 1024, stride = 1\n+    %16 = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xbf16>\n+    tt.store %11, %16 : tensor<1024xbf16>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32) { \n+// CHECK:           %[[VAL_7:.*]] = arith.constant 20480 : index\n+// CHECK:           %[[VAL_8:.*]] = arith.index_cast %[[VAL_3]] : i32 to index\n+// CHECK:           %[[VAL_9:.*]] = arith.addi %[[VAL_8]], %[[VAL_7]] : index\n+// CHECK:           %[[VAL_10:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_9]]], sizes: [1024], strides: {{\\[}}21] : memref<*xbf16> to memref<1024xbf16, strided<[21], offset: ?>>\n+// CHECK:           %[[VAL_11:.*]] = arith.index_cast %[[VAL_3]] : i32 to index\n+// CHECK:           %[[VAL_12:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}%[[VAL_11]]], sizes: [1024], strides: [1] : memref<*xbf16> to memref<1024xbf16, strided<[1], offset: ?>>\n+// CHECK:           %[[VAL_13:.*]] = memref.alloc() : memref<1024xbf16>\n+// CHECK:           memref.copy %[[VAL_10]], %[[VAL_13]] : memref<1024xbf16, strided<[21], offset: ?>> to memref<1024xbf16>\n+// CHECK:           %[[VAL_14:.*]] = bufferization.to_tensor %[[VAL_13]] restrict writable : memref<1024xbf16>\n+// CHECK:           memref.tensor_store %[[VAL_14]], %[[VAL_12]] : memref<1024xbf16, strided<[1], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_mul_value_const.mlir", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -0,0 +1,53 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %arg0 : !tt.ptr<bf16>,\n+    %arg1 : !tt.ptr<bf16>,\n+    %arg2 : i32\n+  )\n+  {\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = tt.make_range {end = 1024 : i32, start = 0 : i32}:tensor<1024xi32>\n+    %2 = tt.splat %0 : (i32) -> tensor<1024xi32>\n+    %3 = arith.addi %2, %1 : tensor<1024xi32>\n+    //%3: splat(%0) + range(0, 1024)\n+    //%3: offset = %0, size = 1024, stride = 1\n+    // vector is constant, scalar is value\n+    %4 = tt.make_range {end = 4096 : i32, start = 2048 : i32}:tensor<1024xi32>\n+    %5 = tt.splat %arg2 : (i32) -> tensor<1024xi32>\n+    %6 = arith.muli %5, %4 : tensor<1024xi32>\n+    //%6: splat(%arg2)*range(2048, 4096);\n+    //%6: offset = %arg2*2048, size = 1024, stride = %arg2*2\n+    %7 = arith.addi %3, %6 : tensor<1024xi32>\n+    //%7: offset = %arg2*2048 + %0, size = 1024, stride = %arg2*2+1\n+    %8 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<1024x!tt.ptr<bf16>>\n+    %9 = tt.addptr %8, %7 : tensor<1024x!tt.ptr<bf16>>, tensor<1024xi32>\n+    //source=%arg0: offset = %arg2*2048 + pid0, size = 1024, stride = %arg2*2+1\n+    %10 = tt.splat %arg1 : (!tt.ptr<bf16>) -> tensor<1024x!tt.ptr<bf16>>\n+    %11 = tt.addptr %10, %3 : tensor<1024x!tt.ptr<bf16>>, tensor<1024xi32>\n+    //source=%arg1: offset = pid0, size = 1024, stride = 1\n+    %16 = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xbf16>\n+    tt.store %11, %16 : tensor<1024xbf16>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 1 : index\n+// CHECK-DAG:           %[[VAL_7:.*]] = arith.constant 2 : index\n+// CHECK-DAG:           %[[VAL_8:.*]] = arith.constant 2048 : index\n+// CHECK:           %[[VAL_9:.*]] = arith.index_cast %[[VAL_3]] : i32 to index\n+// CHECK:           %[[VAL_10:.*]] = arith.index_cast %[[VAL_2]] : i32 to index\n+// CHECK:           %[[VAL_11:.*]] = arith.muli %[[VAL_10]], %[[VAL_8]] : index\n+// CHECK:           %[[VAL_12:.*]] = arith.muli %[[VAL_10]], %[[VAL_7]] : index\n+// CHECK:           %[[VAL_13:.*]] = arith.addi %[[VAL_9]], %[[VAL_11]] : index\n+// CHECK:           %[[VAL_14:.*]] = arith.addi %[[VAL_12]], %[[VAL_6]] : index\n+// CHECK:           %[[VAL_15:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_13]]], sizes: [1024], strides: {{\\[}}%[[VAL_14]]] : memref<*xbf16> to memref<1024xbf16, strided<[?], offset: ?>>\n+// CHECK:           %[[VAL_16:.*]] = arith.index_cast %[[VAL_3]] : i32 to index\n+// CHECK:           %[[VAL_17:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}%[[VAL_16]]], sizes: [1024], strides: [1] : memref<*xbf16> to memref<1024xbf16, strided<[1], offset: ?>>\n+// CHECK:           %[[VAL_18:.*]] = memref.alloc() : memref<1024xbf16>\n+// CHECK:           memref.copy %[[VAL_15]], %[[VAL_18]] : memref<1024xbf16, strided<[?], offset: ?>> to memref<1024xbf16>\n+// CHECK:           %[[VAL_19:.*]] = bufferization.to_tensor %[[VAL_18]] restrict writable : memref<1024xbf16>\n+// CHECK:           memref.tensor_store %[[VAL_19]], %[[VAL_17]] : memref<1024xbf16, strided<[1], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_nested.mlir", "status": "added", "additions": 73, "deletions": 0, "changes": 73, "file_content_changes": "@@ -0,0 +1,73 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %arg0 : !tt.ptr<bf16>,\n+    %arg1 : i32\n+  )\n+  {\n+    %0 = tt.make_range {end = 4 : i32, start = 0 : i32}:tensor<4xi32>\n+    // offset = 0, size = 4, stride = 1\n+    %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<4xi32>) -> tensor<4x1xi32>\n+    // offset = [0,0], size = [4,1], stride = [1,0]\n+    %2 = tt.broadcast %1 : (tensor<4x1xi32>) -> tensor<4x256xi32>\n+    // offset = [0,0], size = [4,256], stride = [1,0]\n+    %arg1splat = tt.splat %arg1 : (i32) -> tensor<4x256xi32>\n+    %offset3 = arith.addi %2, %arg1splat : tensor<4x256xi32>\n+    // offset = [%arg1,0], size = [4,256], stride = [1,0]\n+    %3 = tt.make_range {end = 256 : i32, start = 0 : i32}:tensor<256xi32>\n+    // offset = 0, size = 256, stride = 1\n+    %4 = tt.expand_dims %3 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+    // offset = [0,0], size = [1,256], stride = [0,1]\n+    %5 = tt.broadcast %4 : (tensor<1x256xi32>) -> tensor<4x256xi32>\n+    // offset = [0,0], size = [4,256], stride = [0,1]\n+    %6 = arith.constant 5 : i32\n+    %splat6 = tt.splat %6 : (i32) -> tensor<4x256xi32>\n+    %scale5 = arith.muli %5, %splat6 : tensor<4x256xi32>\n+    // offset = [0,0], size = [4,256], stride = [0,5]\n+    %7 = arith.addi %offset3, %scale5: tensor<4x256xi32>\n+    // offset = [%arg1, 0], size = [4, 256], stride = [1, 5]\n+    %8 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<4x256x!tt.ptr<bf16>>\n+    %9 = tt.addptr %8, %7 : tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xi32>\n+    // source: %arg0, offset = [%arg1, 0], size = [4, 256], stride = [1, 5]\n+    %10 = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<4x256xbf16>\n+    %12 = tt.addptr %9, %7 : tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xi32>\n+    // source: %arg0, offset = [%arg1+%arg1, 0], size = [4, 256], stride = [2, 10]\n+    %13 = tt.load %12 {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<4x256xbf16>\n+    %14 = arith.addf %10, %13 : tensor<4x256xbf16>\n+    %16 = tt.addptr %12, %7 : tensor<4x256x!tt.ptr<bf16>>, tensor<4x256xi32>\n+    // source: %arg0, offset = [%arg1+%arg1+%arg1, 0], size = [4, 256], stride = [3, 15]\n+    tt.store %16, %14 : tensor<4x256xbf16>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: i32, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_5:.*]] = arith.constant 15 : index\n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 5 : index\n+// CHECK-DAG:           %[[VAL_7:.*]] = arith.constant 10 : index\n+// CHECK:           %[[VAL_8:.*]] = arith.index_cast %[[VAL_1]] : i32 to index\n+// CHECK:           %[[VAL_9:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_8]]], sizes: [4, 256], strides: [1, %[[VAL_6]]] : memref<*xbf16> to memref<4x256xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           %[[VAL_10:.*]] = memref.alloc() : memref<4x256xbf16>\n+// CHECK:           memref.copy %[[VAL_9]], %[[VAL_10]] : memref<4x256xbf16, strided<[1, ?], offset: ?>> to memref<4x256xbf16>\n+// CHECK:           %[[VAL_11:.*]] = bufferization.to_tensor %[[VAL_10]] restrict writable : memref<4x256xbf16>\n+// CHECK:           %[[VAL_12:.*]] = arith.index_cast %[[VAL_1]] : i32 to index\n+// CHECK:           %[[VAL_13:.*]] = arith.index_cast %[[VAL_1]] : i32 to index\n+// CHECK:           %[[VAL_14:.*]] = arith.addi %[[VAL_12]], %[[VAL_13]] : index\n+// CHECK:           %[[VAL_15:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_14]]], sizes: [4, 256], strides: [2, %[[VAL_7]]] : memref<*xbf16> to memref<4x256xbf16, strided<[2, ?], offset: ?>>\n+// CHECK:           %[[VAL_16:.*]] = memref.alloc() : memref<4x256xbf16>\n+// CHECK:           memref.copy %[[VAL_15]], %[[VAL_16]] : memref<4x256xbf16, strided<[2, ?], offset: ?>> to memref<4x256xbf16>\n+// CHECK:           %[[VAL_17:.*]] = bufferization.to_tensor %[[VAL_16]] restrict writable : memref<4x256xbf16>\n+// CHECK:           %[[VAL_18:.*]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_11]], %[[VAL_17]] : tensor<4x256xbf16>, tensor<4x256xbf16>) outs(%[[VAL_11]] : tensor<4x256xbf16>) {\n+// CHECK:           ^bb0(%[[VAL_19:.*]]: bf16, %[[VAL_20:.*]]: bf16, %[[VAL_21:.*]]: bf16):\n+// CHECK:             %[[VAL_22:.*]] = arith.addf %[[VAL_19]], %[[VAL_20]] : bf16\n+// CHECK:             linalg.yield %[[VAL_22]] : bf16\n+// CHECK:           } -> tensor<4x256xbf16>\n+// CHECK:           %[[VAL_23:.*]] = arith.index_cast %[[VAL_1]] : i32 to index\n+// CHECK:           %[[VAL_24:.*]] = arith.index_cast %[[VAL_1]] : i32 to index\n+// CHECK:           %[[VAL_25:.*]] = arith.addi %[[VAL_23]], %[[VAL_24]] : index\n+// CHECK:           %[[VAL_26:.*]] = arith.index_cast %[[VAL_1]] : i32 to index\n+// CHECK:           %[[VAL_27:.*]] = arith.addi %[[VAL_25]], %[[VAL_26]] : index\n+// CHECK:           %[[VAL_28:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_27]]], sizes: [4, 256], strides: [3, %[[VAL_5]]] : memref<*xbf16> to memref<4x256xbf16, strided<[3, ?], offset: ?>>\n+// CHECK:           memref.tensor_store %[[VAL_29:.*]], %[[VAL_28]] : memref<4x256xbf16, strided<[3, ?], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_reshape_broadcast.mlir", "status": "added", "additions": 42, "deletions": 0, "changes": 42, "file_content_changes": "@@ -0,0 +1,42 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+// TODO: expand this example to 3D\n+module {\n+  tt.func @kernel(\n+  %arg0 : !tt.ptr<bf16>,\n+  %arg1 : !tt.ptr<bf16>\n+  )\n+  {\n+  %0 = tt.make_range {end = 1024 : i32, start = 512 : i32}:tensor<256xi32>\n+  // offset = [512] size = 256, stride = 2\n+  %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<256xi32>) -> tensor<256x1xi32>\n+  // offset = [512,0], size = [256,1], stride = [2,0]\n+  %2 = tt.broadcast %1 : (tensor<256x1xi32>) -> tensor<256x128xi32>\n+  // offset = [512,0], size = [256,128], stride = [2,0]\n+  %5 = tt.make_range {end = 1408 : i32, start = 1024 : i32}:tensor<128xi32>\n+  // offset = 1024, size = 128, stride = 3\n+  %6 = tt.expand_dims %5 {axis = 0 : i32} : (tensor<128xi32>) -> tensor<1x128xi32>\n+  // offset = [0,1024], size = [1,128], stride = [0,3]\n+  %7 = tt.broadcast %6 : (tensor<1x128xi32>) -> tensor<256x128xi32>\n+  // offset = [0,1024], size = [256,128], stride = [0,3]\n+  %c6 = arith.constant 6 : i32\n+  %splat6 = tt.splat %c6 : (i32) -> tensor<256x128xi32>\n+  %scale7 = arith.muli %7, %splat6 : tensor<256x128xi32>\n+  // offset = [0,6144], size = [256,128], stride = [0,18]\n+  %14 = arith.addi %2, %scale7 : tensor<256x128xi32>\n+  // offset = [512,6144], size = [256,128], stride = [2,18]\n+  %17 = tt.splat %arg1 : (!tt.ptr<bf16>) -> tensor<256x128x!tt.ptr<bf16>>\n+  %18 = tt.addptr %17, %14 : tensor<256x128x!tt.ptr<bf16>>, tensor<256x128xi32>\n+  %19 = tt.load %18 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x128xbf16>\n+  tt.store %18, %19 : tensor<256x128xbf16>\n+  tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32) {\n+// CHECK:           %[[VAL_7:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}6656], sizes: [256, 128], strides: [2, 18] : memref<*xbf16> to memref<256x128xbf16, strided<[2, 18], offset: 6656>>\n+// CHECK:           %[[VAL_8:.*]] = memref.alloc() : memref<256x128xbf16>\n+// CHECK:           memref.copy %[[VAL_7]], %[[VAL_8]] : memref<256x128xbf16, strided<[2, 18], offset: 6656>> to memref<256x128xbf16>\n+// CHECK:           %[[VAL_9:.*]] = bufferization.to_tensor %[[VAL_8]] restrict writable : memref<256x128xbf16>\n+// CHECK:           memref.tensor_store %[[VAL_9]], %[[VAL_7]] : memref<256x128xbf16, strided<[2, 18], offset: 6656>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_scalar_broadcast.mlir", "status": "added", "additions": 65, "deletions": 0, "changes": 65, "file_content_changes": "@@ -0,0 +1,65 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel (%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32, %arg3: i32, %arg4: i32) {\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %arg2 : i32\n+    %2 = tt.addptr %arg1, %1 : !tt.ptr<f32>, i32\n+    // source = arg1, offset = %1, size = 1, strides = 0\n+    %3 = tt.splat %2 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+    // source = arg1, offset = %1, size = 1024, strides = 0\n+    %4 = tt.expand_dims %3 {axis = 1 : i32} : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024x1x!tt.ptr<f32>>\n+    // source = arg1, offset = [%1, 0], size = [1024, 1], strides = [0, 0]\n+    %5 = tt.broadcast %4 : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x1024x!tt.ptr<f32>>\n+    // source = arg1, offset = [%1, 0], size = [1024, 1024], strides = [0, 0]\n+    %6 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>\n+    // offset = 0, size = 1024, strides = 1\n+    %7 = tt.expand_dims %6 {axis = 0 : i32} : (tensor<1024xi32>) -> tensor<1x1024xi32>\n+    // offset = [0, 0], size = [1, 1024], strides = [0, 1]\n+    %8 = tt.broadcast %7 : (tensor<1x1024xi32>) -> tensor<1024x1024xi32>\n+    // offset = [0, 0], size = [1024, 1024], strides = [0, 1]\n+    %9 = tt.make_range {end = 2048 : i32, start = 0 : i32} : tensor<1024xi32>\n+    // offset = 0, size = 1024, strides = 2\n+    %10 = tt.expand_dims %9 {axis = 1 : i32} : (tensor<1024xi32>) -> tensor<1024x1xi32>\n+    // offset = [0, 0], size = [1024, 1], strides = [2, 0]\n+    %11 = tt.broadcast %10 : (tensor<1024x1xi32>) -> tensor<1024x1024xi32>\n+    // offset = [0, 0], size = [1024, 1024], strides = [2, 0]\n+    %12 = arith.addi %8, %11 : tensor<1024x1024xi32>\n+    // offset = [0, 0], size = [1024, 1024], strides = [2, 1]\n+    %13 = tt.addptr %5, %12 : tensor<1024x1024x!tt.ptr<f32>>, tensor<1024x1024xi32>\n+    // source = arg1, offset = [pid * %arg2, 0], size = [1024, 1024], strides = [2, 1]\n+    %14 = tt.load %13 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024x1024xf32>\n+    %17 = math.exp %14 : tensor<1024x1024xf32>\n+    %18 = arith.muli %0, %arg3 : i32\n+    %19 = tt.addptr %arg0, %18 : !tt.ptr<f32>, i32\n+    // source = arg0, offset = pid+arg3, size = 1, strides = 0\n+    %20 = tt.splat %19 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+    // source = arg0, offset = pid+arg3, size = 1024, strides = 0\n+    %21 = tt.expand_dims %20 {axis = 1 : i32} : (tensor<1024x!tt.ptr<f32>>) -> tensor<1024x1x!tt.ptr<f32>>\n+    // source = arg0, offset = [pid+arg3, 0], size = [1024, 1], strides = [0, 0]\n+    %22 = tt.broadcast %21 : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x1024x!tt.ptr<f32>>\n+    // source = arg0, offset = [pid+arg3, 0], size = [1024, 1024], strides = [0, 0]\n+    %23 = tt.addptr %22, %12 : tensor<1024x1024x!tt.ptr<f32>>, tensor<1024x1024xi32>\n+    // source = arg0, offset = [pid+arg3, 0], size = [1024, 1024], strides = [2, 1]\n+    tt.store %23, %17 : tensor<1024x1024xf32>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xf32> {tt.divisibility = 16 : i32}, %[[VAL_1:.*]]: memref<*xf32> {tt.divisibility = 16 : i32}, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32, %[[VAL_6:.*]]: i32, %[[VAL_7:.*]]: i32) { \n+// CHECK:           %[[VAL_8:.*]] = arith.muli %[[VAL_5]], %[[VAL_2]] : i32\n+// CHECK:           %[[VAL_9:.*]] = arith.index_cast %[[VAL_8]] : i32 to index\n+// CHECK:           %[[VAL_10:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}%[[VAL_9]]], sizes: [1024, 1024], strides: [2, 1] : memref<*xf32> to memref<1024x1024xf32, strided<[2, 1], offset: ?>>\n+// CHECK:           %[[VAL_11:.*]] = memref.alloc() : memref<1024x1024xf32>\n+// CHECK:           memref.copy %[[VAL_10]], %[[VAL_11]] : memref<1024x1024xf32, strided<[2, 1], offset: ?>> to memref<1024x1024xf32>\n+// CHECK:           %[[VAL_12:.*]] = bufferization.to_tensor %[[VAL_11]] restrict writable : memref<1024x1024xf32>\n+// CHECK:           %[[VAL_13:.*]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_12]] : tensor<1024x1024xf32>) outs(%[[VAL_12]] : tensor<1024x1024xf32>) {\n+// CHECK:           ^bb0(%[[VAL_14:.*]]: f32, %[[VAL_15:.*]]: f32):\n+// CHECK:             %[[VAL_16:.*]] = math.exp %[[VAL_14]] : f32\n+// CHECK:             linalg.yield %[[VAL_16]] : f32\n+// CHECK:           } -> tensor<1024x1024xf32>\n+// CHECK:           %[[VAL_17:.*]] = arith.muli %[[VAL_5]], %[[VAL_3]] : i32\n+// CHECK:           %[[VAL_18:.*]] = arith.index_cast %[[VAL_17]] : i32 to index\n+// CHECK:           %[[VAL_19:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_18]]], sizes: [1024, 1024], strides: [2, 1] : memref<*xf32> to memref<1024x1024xf32, strided<[2, 1], offset: ?>>\n+// CHECK:           memref.tensor_store %[[VAL_20:.*]], %[[VAL_19]] : memref<1024x1024xf32, strided<[2, 1], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_scalar_for.mlir", "status": "added", "additions": 70, "deletions": 0, "changes": 70, "file_content_changes": "@@ -0,0 +1,70 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel (%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32, %arg3: i32, %arg4: i32) {\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %arg2 : i32\n+    %2 = tt.addptr %arg1, %1 : !tt.ptr<f32>, i32\n+    // source = %arg1, offset = %1, size = 1, strides = 0\n+    %cf0 = arith.constant 0.000000e+00 : f32\n+    %tensor_cf0 = tt.splat %cf0 : (f32) -> tensor<1024xf32>\n+    %c0 = arith.constant 0 : index\n+    %c12 = arith.constant 12 : index\n+    %c3 = arith.constant 3 : index\n+    %_ptr, %sum_out = scf.for %i = %c0 to %c12 step %c3 iter_args(%ptr_iter = %2, %sum_iter = %tensor_cf0) ->  (!tt.ptr<f32>, tensor<1024xf32>) {\n+      %3 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>\n+      // offset = 0, size = 1024, strides = 1\n+      %4 = tt.splat %ptr_iter : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+      // source = %arg1, offset = %1, size = 1024, strides = 0\n+      %5 = tt.addptr %4, %3 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+      // source = %arg1, offset = %1, size = 1024, strides = 1\n+      %8 = tt.load %5 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n+      %9 = math.exp %8 : tensor<1024xf32>\n+      %sum_next = arith.addf %sum_iter, %9 : tensor<1024xf32>\n+      %cast_i = arith.index_cast %i : index to i32\n+      %ptr_next = tt.addptr %ptr_iter, %cast_i : !tt.ptr<f32>, i32\n+      // source = %arg1, offset = %1 + %i, size = 1, strides = 0\n+      scf.yield %ptr_next, %sum_next : !tt.ptr<f32>, tensor<1024xf32>\n+    }\n+    %10 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>\n+    %18 = arith.muli %0, %arg3 : i32\n+    %19 = tt.addptr %arg0, %18 : !tt.ptr<f32>, i32\n+    %20 = tt.splat %19 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+    %21 = tt.addptr %20, %10 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+    tt.store %21, %sum_out : tensor<1024xf32>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xf32> {tt.divisibility = 16 : i32}, %[[VAL_1:.*]]: memref<*xf32> {tt.divisibility = 16 : i32}, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32, %[[VAL_6:.*]]: i32, %[[VAL_7:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_8:.*]] = arith.constant 3 : index\n+// CHECK-DAG:           %[[VAL_9:.*]] = arith.constant 12 : index\n+// CHECK-DAG:           %[[VAL_10:.*]] = arith.constant 0 : index\n+// CHECK-DAG:           %[[VAL_11:.*]] = arith.constant 0.000000e+00 : f32\n+// CHECK:           %[[VAL_14:.*]] = tensor.empty() : tensor<1024xf32>\n+// CHECK:           %[[VAL_15:.*]] = linalg.fill ins(%[[VAL_11]] : f32) outs(%[[VAL_14]] : tensor<1024xf32>) -> tensor<1024xf32>\n+// CHECK:           %[[VAL_12:.*]] = arith.muli %[[VAL_5]], %[[VAL_2]] : i32\n+// CHECK:           %[[VAL_13:.*]] = arith.index_cast %[[VAL_12]] : i32 to index\n+// CHECK:           %[[VAL_16:.*]]:2 = scf.for %[[VAL_17:.*]] = %[[VAL_10]] to %[[VAL_9]] step %[[VAL_8]] iter_args(%[[VAL_18:.*]] = %[[VAL_15]], %[[VAL_19:.*]] = %[[VAL_13]]) -> (tensor<1024xf32>, index) {\n+// CHECK:             %[[VAL_20:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}%[[VAL_19]]], sizes: [1024], strides: [1] : memref<*xf32> to memref<1024xf32, strided<[1], offset: ?>>\n+// CHECK:             %[[VAL_21:.*]] = memref.alloc() : memref<1024xf32>\n+// CHECK:             memref.copy %[[VAL_20]], %[[VAL_21]] : memref<1024xf32, strided<[1], offset: ?>> to memref<1024xf32>\n+// CHECK:             %[[VAL_22:.*]] = bufferization.to_tensor %[[VAL_21]] restrict writable : memref<1024xf32>\n+// CHECK:             %[[VAL_23:.*]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_22]] : tensor<1024xf32>) outs(%[[VAL_22]] : tensor<1024xf32>) {\n+// CHECK:             ^bb0(%[[VAL_24:.*]]: f32, %[[VAL_25:.*]]: f32):\n+// CHECK:               %[[VAL_26:.*]] = math.exp %[[VAL_24]] : f32\n+// CHECK:               linalg.yield %[[VAL_26]] : f32\n+// CHECK:             } -> tensor<1024xf32>\n+// CHECK:             %[[VAL_27:.*]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_18]], %[[VAL_28:.*]] : tensor<1024xf32>, tensor<1024xf32>) outs(%[[VAL_18]] : tensor<1024xf32>) {\n+// CHECK:             ^bb0(%[[VAL_29:.*]]: f32, %[[VAL_30:.*]]: f32, %[[VAL_31:.*]]: f32):\n+// CHECK:               %[[VAL_32:.*]] = arith.addf %[[VAL_29]], %[[VAL_30]] : f32\n+// CHECK:               linalg.yield %[[VAL_32]] : f32\n+// CHECK:             } -> tensor<1024xf32>\n+// CHECK:             %[[VAL_33:.*]] = arith.addi %[[VAL_19]], %[[VAL_17]] : index\n+// CHECK:             scf.yield %[[VAL_34:.*]], %[[VAL_33]] : tensor<1024xf32>, index\n+// CHECK:           }\n+// CHECK:           %[[VAL_35:.*]] = arith.muli %[[VAL_5]], %[[VAL_3]] : i32\n+// CHECK:           %[[VAL_36:.*]] = arith.index_cast %[[VAL_35]] : i32 to index\n+// CHECK:           %[[VAL_37:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_36]]], sizes: [1024], strides: [1] : memref<*xf32> to memref<1024xf32, strided<[1], offset: ?>>\n+// CHECK:           memref.tensor_store %[[VAL_38:.*]]#0, %[[VAL_37]] : memref<1024xf32, strided<[1], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_scalar_for_2d.mlir", "status": "added", "additions": 92, "deletions": 0, "changes": 92, "file_content_changes": "@@ -0,0 +1,92 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel (%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32, %arg3: i32, %arg4: i32) {\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %arg2 : i32\n+    %2 = tt.addptr %arg1, %1 : !tt.ptr<f32>, i32\n+    %cf0 = arith.constant 0.000000e+00 : f32\n+    %tensor_cf0 = tt.splat %cf0 : (f32) -> tensor<128x128xf32>\n+    %c0 = arith.constant 0 : index\n+    %c12 = arith.constant 12 : index\n+    %c3 = arith.constant 3 : index\n+    %sum_out, %_ptr = scf.for %i = %c0 to %c12 step %c3 iter_args(%sum_iter = %tensor_cf0,  %ptr_iter = %2) ->  (tensor<128x128xf32>, !tt.ptr<f32> ) {\n+      %3 = tt.splat %ptr_iter : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n+      // source = %arg1, offset = [%1, 0], size = [128, 128], strides = [0, 0]\n+      %4 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+      %5 = tt.expand_dims %4 {axis = 0 : i32} : (tensor<128xi32>) -> tensor<1x128xi32>\n+      %6 = tt.broadcast %5 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n+      // offset = [0, 0], size = [128, 128], strides = [0, 1]\n+      %7 = tt.make_range {end = 384 : i32, start = 128 : i32} : tensor<128xi32>\n+      %8 = tt.expand_dims %7 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n+      %9 = tt.broadcast %8 : (tensor<128x1xi32>) -> tensor<128x128xi32>\n+      // offset = [128, 0], size = [128, 128], strides = [2, 0]\n+      %10 = arith.addi %6, %9 : tensor<128x128xi32>\n+      // offset = [128, 0], size = [128, 128], strides = [2, 1]\n+      %11 = tt.addptr %3, %10 : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n+      // source = %arg1, offset = [%1 + 128, 0], size = [128, 128], strides = [2, 1]\n+      %12 = tt.load %11 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf32>\n+      %17 = math.exp %12 : tensor<128x128xf32>\n+      %sum_next = arith.addf %sum_iter, %17 : tensor<128x128xf32>\n+      %cast_i = arith.index_cast %i : index to i32\n+      %ptr_next = tt.addptr %ptr_iter, %cast_i : !tt.ptr<f32>, i32\n+      // source = %arg1, offset = %1 + %i, size = 1, strides = 0\n+      scf.yield %sum_next, %ptr_next : tensor<128x128xf32>, !tt.ptr<f32>\n+    }\n+    %4 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+    %5 = tt.expand_dims %4 {axis = 0 : i32} : (tensor<128xi32>) -> tensor<1x128xi32>\n+    %6 = tt.broadcast %5 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n+    // offset = [0, 0], size = [128, 128], strides = [0, 1]\n+    %7 = tt.make_range {end = 384 : i32, start = 128 : i32} : tensor<128xi32>\n+    %8 = tt.expand_dims %7 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n+    %9 = tt.broadcast %8 : (tensor<128x1xi32>) -> tensor<128x128xi32>\n+    // offset = [128, 0], size = [128, 128], strides = [2, 0]\n+    %10 = arith.addi %6, %9 : tensor<128x128xi32>\n+    // offset = [128, 0], size = [128, 128], strides = [2, 1]\n+    %18 = arith.muli %0, %arg3 : i32\n+    %19 = tt.addptr %arg0, %18 : !tt.ptr<f32>, i32\n+    // source = arg0, offset = %18, size = 1, strides = 0\n+    %20 = tt.splat %19 : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n+    // source = arg0, offset = [%18, 0], size = [128, 128], strides = [0, 0]\n+    %21 = tt.addptr %20, %10 : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n+    // source = %arg0, offset = [%18 + 128, 0], size = [128, 128], strides = [2, 1]\n+    tt.store %21, %sum_out : tensor<128x128xf32>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xf32> {tt.divisibility = 16 : i32}, %[[VAL_1:.*]]: memref<*xf32> {tt.divisibility = 16 : i32}, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32, %[[VAL_6:.*]]: i32, %[[VAL_7:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_8:.*]] = arith.constant 128 : index\n+// CHECK-DAG:           %[[VAL_9:.*]] = arith.constant 3 : index\n+// CHECK-DAG:           %[[VAL_10:.*]] = arith.constant 12 : index\n+// CHECK-DAG:           %[[VAL_11:.*]] = arith.constant 0 : index\n+// CHECK-DAG:           %[[VAL_12:.*]] = arith.constant 0.000000e+00 : f32\n+// CHECK:           %[[VAL_15:.*]] = tensor.empty() : tensor<128x128xf32>\n+// CHECK:           %[[VAL_16:.*]] = linalg.fill ins(%[[VAL_12]] : f32) outs(%[[VAL_15]] : tensor<128x128xf32>) -> tensor<128x128xf32>\n+// CHECK:           %[[VAL_13:.*]] = arith.muli %[[VAL_5]], %[[VAL_2]] : i32\n+// CHECK:           %[[VAL_14:.*]] = arith.index_cast %[[VAL_13]] : i32 to index\n+// CHECK:           %[[VAL_17:.*]]:2 = scf.for %[[VAL_18:.*]] = %[[VAL_11]] to %[[VAL_10]] step %[[VAL_9]] iter_args(%[[VAL_19:.*]] = %[[VAL_16]], %[[VAL_20:.*]] = %[[VAL_14]]) -> (tensor<128x128xf32>, index) {\n+// CHECK:             %[[VAL_21:.*]] = arith.addi %[[VAL_20]], %[[VAL_8]] : index\n+// CHECK:             %[[VAL_22:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}%[[VAL_21]]], sizes: [128, 128], strides: [2, 1] : memref<*xf32> to memref<128x128xf32, strided<[2, 1], offset: ?>>\n+// CHECK:             %[[VAL_23:.*]] = memref.alloc() : memref<128x128xf32>\n+// CHECK:             memref.copy %[[VAL_22]], %[[VAL_23]] : memref<128x128xf32, strided<[2, 1], offset: ?>> to memref<128x128xf32>\n+// CHECK:             %[[VAL_24:.*]] = bufferization.to_tensor %[[VAL_23]] restrict writable : memref<128x128xf32>\n+// CHECK:             %[[VAL_25:.*]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_24]] : tensor<128x128xf32>) outs(%[[VAL_24]] : tensor<128x128xf32>) {\n+// CHECK:             ^bb0(%[[VAL_26:.*]]: f32, %[[VAL_27:.*]]: f32):\n+// CHECK:               %[[VAL_28:.*]] = math.exp %[[VAL_26]] : f32\n+// CHECK:               linalg.yield %[[VAL_28]] : f32\n+// CHECK:             } -> tensor<128x128xf32>\n+// CHECK:             %[[VAL_29:.*]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_19]], %[[VAL_30:.*]] : tensor<128x128xf32>, tensor<128x128xf32>) outs(%[[VAL_19]] : tensor<128x128xf32>) {\n+// CHECK:             ^bb0(%[[VAL_31:.*]]: f32, %[[VAL_32:.*]]: f32, %[[VAL_33:.*]]: f32):\n+// CHECK:               %[[VAL_34:.*]] = arith.addf %[[VAL_31]], %[[VAL_32]] : f32\n+// CHECK:               linalg.yield %[[VAL_34]] : f32\n+// CHECK:             } -> tensor<128x128xf32>\n+// CHECK:             %[[VAL_35:.*]] = arith.addi %[[VAL_20]], %[[VAL_18]] : index\n+// CHECK:             scf.yield %[[VAL_36:.*]], %[[VAL_35]] : tensor<128x128xf32>, index\n+// CHECK:           }\n+// CHECK:           %[[VAL_37:.*]] = arith.muli %[[VAL_5]], %[[VAL_3]] : i32\n+// CHECK:           %[[VAL_38:.*]] = arith.index_cast %[[VAL_37]] : i32 to index\n+// CHECK:           %[[VAL_39:.*]] = arith.addi %[[VAL_38]], %[[VAL_8]] : index\n+// CHECK:           %[[VAL_40:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_39]]], sizes: [128, 128], strides: [2, 1] : memref<*xf32> to memref<128x128xf32, strided<[2, 1], offset: ?>>\n+// CHECK:           memref.tensor_store %[[VAL_41:.*]]#0, %[[VAL_40]] : memref<128x128xf32, strided<[2, 1], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_scalar_loopback.mlir", "status": "added", "additions": 21, "deletions": 0, "changes": 21, "file_content_changes": "@@ -0,0 +1,21 @@\n+// RUN: triton-opt --triton-to-linalg %s\n+// XFAIL: *\n+// Disable this test since we do not support scalar loads at the moment.\n+\n+module {\n+  tt.func @kernel(\n+  %arg0 : !tt.ptr<bf16>,\n+  %arg1 : !tt.ptr<bf16>,\n+  %arg2 : i32\n+  )\n+  {\n+    %0 = tt.addptr %arg0, %arg2 : !tt.ptr<bf16>, i32\n+    %1 = tt.addptr %arg1, %arg2 : !tt.ptr<bf16>, i32\n+\n+    // expected-error @below {{Scalar load is currently not supported}}\n+    %10 = tt.load %0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: bf16\n+    tt.store %1, %10 : bf16\n+  tt.return\n+  }\n+}\n+"}, {"filename": "test/Conversion/TritonToLinalg/addptr_scalar_nested.mlir", "status": "added", "additions": 57, "deletions": 0, "changes": 57, "file_content_changes": "@@ -0,0 +1,57 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel (%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32, %arg3: i32, %arg4: i32) {\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %arg2 : i32\n+    %2 = tt.addptr %arg1, %1 : !tt.ptr<f32>, i32\n+    // source = arg1, offset = %1, size = 1, strides = 0\n+    %3 = arith.muli %0, %arg3 : i32\n+    %4 = tt.addptr %2, %3 : !tt.ptr<f32>, i32\n+    // source = arg1, offset = %1+%3, size = 1, strides = 0\n+    %5 = arith.muli %0, %arg4 : i32\n+    %6 = tt.addptr %4, %5 : !tt.ptr<f32>, i32\n+    // source = arg1, offset = %1+%3+%5, size = 1, strides = 0\n+    %7 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>\n+    // offset = 0, size = 1024, strides = 1\n+    %8 = tt.splat %6 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+    // source = arg1, offset = %1, size = 1024, strides = 0\n+    %9 = tt.addptr %8, %7 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+    // source = arg1, offset = %1+%3+%5, size = 1024, strides = 1\n+    %10 = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n+    %17 = math.exp %10 : tensor<1024xf32>\n+    %18 = arith.muli %0, %arg3 : i32\n+    %19 = tt.addptr %arg0, %18 : !tt.ptr<f32>, i32\n+    // source = arg0, offset = %18, size = 1, strides = 0\n+    %20 = tt.splat %19 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+    // source = arg0, offset = %18, size = 1024, strides = 0\n+    %21 = tt.addptr %20, %7 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+    // source = arg0, offset = %18, size = 1024, strides = 1\n+    tt.store %21, %17 : tensor<1024xf32>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xf32> {tt.divisibility = 16 : i32}, %[[VAL_1:.*]]: memref<*xf32> {tt.divisibility = 16 : i32}, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32, %[[VAL_6:.*]]: i32, %[[VAL_7:.*]]: i32) { \n+// CHECK:           %[[VAL_8:.*]] = arith.muli %[[VAL_5]], %[[VAL_2]] : i32\n+// CHECK:           %[[VAL_9:.*]] = arith.muli %[[VAL_5]], %[[VAL_3]] : i32\n+// CHECK:           %[[VAL_10:.*]] = arith.muli %[[VAL_5]], %[[VAL_4]] : i32\n+// CHECK:           %[[VAL_11:.*]] = arith.index_cast %[[VAL_8]] : i32 to index\n+// CHECK:           %[[VAL_12:.*]] = arith.index_cast %[[VAL_9]] : i32 to index\n+// CHECK:           %[[VAL_13:.*]] = arith.addi %[[VAL_11]], %[[VAL_12]] : index\n+// CHECK:           %[[VAL_14:.*]] = arith.index_cast %[[VAL_10]] : i32 to index\n+// CHECK:           %[[VAL_15:.*]] = arith.addi %[[VAL_13]], %[[VAL_14]] : index\n+// CHECK:           %[[VAL_16:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}%[[VAL_15]]], sizes: [1024], strides: [1] : memref<*xf32> to memref<1024xf32, strided<[1], offset: ?>>\n+// CHECK:           %[[VAL_17:.*]] = memref.alloc() : memref<1024xf32>\n+// CHECK:           memref.copy %[[VAL_16]], %[[VAL_17]] : memref<1024xf32, strided<[1], offset: ?>> to memref<1024xf32>\n+// CHECK:           %[[VAL_18:.*]] = bufferization.to_tensor %[[VAL_17]] restrict writable : memref<1024xf32>\n+// CHECK:           %[[VAL_19:.*]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_18]] : tensor<1024xf32>) outs(%[[VAL_18]] : tensor<1024xf32>) {\n+// CHECK:           ^bb0(%[[VAL_20:.*]]: f32, %[[VAL_21:.*]]: f32):\n+// CHECK:             %[[VAL_22:.*]] = math.exp %[[VAL_20]] : f32\n+// CHECK:             linalg.yield %[[VAL_22]] : f32\n+// CHECK:           } -> tensor<1024xf32>\n+// CHECK:           %[[VAL_23:.*]] = arith.muli %[[VAL_5]], %[[VAL_3]] : i32\n+// CHECK:           %[[VAL_24:.*]] = arith.index_cast %[[VAL_23]] : i32 to index\n+// CHECK:           %[[VAL_25:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_24]]], sizes: [1024], strides: [1] : memref<*xf32> to memref<1024xf32, strided<[1], offset: ?>>\n+// CHECK:           memref.tensor_store %[[VAL_26:.*]], %[[VAL_25]] : memref<1024xf32, strided<[1], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_scalar_splat.mlir", "status": "added", "additions": 45, "deletions": 0, "changes": 45, "file_content_changes": "@@ -0,0 +1,45 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel (%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32, %arg3: i32, %arg4: i32) {\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %arg2 : i32\n+    %2 = tt.addptr %arg1, %1 : !tt.ptr<f32>, i32\n+    // source = %arg1, offset = %1, size = 1, strides = 0\n+    %3 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>\n+    // offset = 0, size = 1024, strides = 1\n+    %4 = tt.splat %2 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+    // source = %arg1, offset = %1, size = 1024, strides = 0\n+    %5 = tt.addptr %4, %3 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+    // source = %arg1, offset = %1, size = 1024, strides = 1\n+    %8 = tt.load %5 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n+    %17 = math.exp %8 : tensor<1024xf32>\n+    %18 = arith.muli %0, %arg3 : i32\n+    %19 = tt.addptr %arg0, %18 : !tt.ptr<f32>, i32\n+    // source = %arg0, offset = %18, size = 1, strides = 0\n+    %20 = tt.splat %19 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+    // source = %arg0, offset = %18, size = 1024, strides = 0\n+    %21 = tt.addptr %20, %3 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+    // source = %arg0, offset = %18, size = 1024, strides = 1\n+    tt.store %21, %17 : tensor<1024xf32>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xf32> {tt.divisibility = 16 : i32}, %[[VAL_1:.*]]: memref<*xf32> {tt.divisibility = 16 : i32}, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32, %[[VAL_6:.*]]: i32, %[[VAL_7:.*]]: i32) { \n+// CHECK:           %[[VAL_8:.*]] = arith.muli %[[VAL_5]], %[[VAL_2]] : i32\n+// CHECK:           %[[VAL_9:.*]] = arith.index_cast %[[VAL_8]] : i32 to index\n+// CHECK:           %[[VAL_10:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}%[[VAL_9]]], sizes: [1024], strides: [1] : memref<*xf32> to memref<1024xf32, strided<[1], offset: ?>>\n+// CHECK:           %[[VAL_11:.*]] = memref.alloc() : memref<1024xf32>\n+// CHECK:           memref.copy %[[VAL_10]], %[[VAL_11]] : memref<1024xf32, strided<[1], offset: ?>> to memref<1024xf32>\n+// CHECK:           %[[VAL_12:.*]] = bufferization.to_tensor %[[VAL_11]] restrict writable : memref<1024xf32>\n+// CHECK:           %[[VAL_13:.*]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_12]] : tensor<1024xf32>) outs(%[[VAL_12]] : tensor<1024xf32>) {\n+// CHECK:           ^bb0(%[[VAL_14:.*]]: f32, %[[VAL_15:.*]]: f32):\n+// CHECK:             %[[VAL_16:.*]] = math.exp %[[VAL_14]] : f32\n+// CHECK:             linalg.yield %[[VAL_16]] : f32\n+// CHECK:           } -> tensor<1024xf32>\n+// CHECK:           %[[VAL_17:.*]] = arith.muli %[[VAL_5]], %[[VAL_3]] : i32\n+// CHECK:           %[[VAL_18:.*]] = arith.index_cast %[[VAL_17]] : i32 to index\n+// CHECK:           %[[VAL_19:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_18]]], sizes: [1024], strides: [1] : memref<*xf32> to memref<1024xf32, strided<[1], offset: ?>>\n+// CHECK:           memref.tensor_store %[[VAL_20:.*]], %[[VAL_19]] : memref<1024xf32, strided<[1], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/addptr_scalar_splat_2d.mlir", "status": "added", "additions": 56, "deletions": 0, "changes": 56, "file_content_changes": "@@ -0,0 +1,56 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel (%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32, %arg3: i32, %arg4: i32) {\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %arg2 : i32\n+    %2 = tt.addptr %arg1, %1 : !tt.ptr<f32>, i32\n+    %3 = tt.splat %2 : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n+    // source = %arg1, offset = [%1, 0], size = [128, 128], strides = [0, 0]\n+    %4 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+    %5 = tt.expand_dims %4 {axis = 0 : i32} : (tensor<128xi32>) -> tensor<1x128xi32>\n+    %6 = tt.broadcast %5 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n+    // offset = [0, 0], size = [128, 128], strides = [0, 1]\n+    %7 = tt.make_range {end = 384 : i32, start = 128 : i32} : tensor<128xi32>\n+    // offset = 128, size = 128, strides = 1\n+    %8 = tt.expand_dims %7 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n+    %9 = tt.broadcast %8 : (tensor<128x1xi32>) -> tensor<128x128xi32>\n+    // offset = [128, 0], size = [128, 128], strides = [2, 0]\n+    %10 = arith.addi %6, %9 : tensor<128x128xi32>\n+    // offset = [128, 0], size = [128, 128], strides = [2, 1]\n+    %11 = tt.addptr %3, %10 : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n+    // source = %arg1, offset = [%1 + 128, 0], size = [128, 128], strides = [2, 1]\n+    %12 = tt.load %11 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf32>\n+    %17 = math.exp %12 : tensor<128x128xf32>\n+    %18 = arith.muli %0, %arg3 : i32\n+    %19 = tt.addptr %arg0, %18 : !tt.ptr<f32>, i32\n+    // source = arg0, offset = %18, size = 1, strides = 0\n+    %20 = tt.splat %19 : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n+    // source = arg0, offset = [%18, 0], size = [128, 128], strides = [0, 0]\n+    %21 = tt.addptr %20, %10 : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n+    // source = %arg0, offset = [%18 + 128, 0], size = [128, 128], strides = [2, 1]\n+    tt.store %21, %17 : tensor<128x128xf32>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xf32> {tt.divisibility = 16 : i32}, %[[VAL_1:.*]]: memref<*xf32> {tt.divisibility = 16 : i32}, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32, %[[VAL_6:.*]]: i32, %[[VAL_7:.*]]: i32) { \n+// CHECK:           %[[VAL_8:.*]] = arith.constant 128 : index\n+// CHECK:           %[[VAL_9:.*]] = arith.muli %[[VAL_5]], %[[VAL_2]] : i32\n+// CHECK:           %[[VAL_10:.*]] = arith.index_cast %[[VAL_9]] : i32 to index\n+// CHECK:           %[[VAL_11:.*]] = arith.addi %[[VAL_10]], %[[VAL_8]] : index\n+// CHECK:           %[[VAL_12:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}%[[VAL_11]]], sizes: [128, 128], strides: [2, 1] : memref<*xf32> to memref<128x128xf32, strided<[2, 1], offset: ?>>\n+// CHECK:           %[[VAL_13:.*]] = memref.alloc() : memref<128x128xf32>\n+// CHECK:           memref.copy %[[VAL_12]], %[[VAL_13]] : memref<128x128xf32, strided<[2, 1], offset: ?>> to memref<128x128xf32>\n+// CHECK:           %[[VAL_14:.*]] = bufferization.to_tensor %[[VAL_13]] restrict writable : memref<128x128xf32>\n+// CHECK:           %[[VAL_15:.*]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_14]] : tensor<128x128xf32>) outs(%[[VAL_14]] : tensor<128x128xf32>) {\n+// CHECK:           ^bb0(%[[VAL_16:.*]]: f32, %[[VAL_17:.*]]: f32):\n+// CHECK:             %[[VAL_18:.*]] = math.exp %[[VAL_16]] : f32\n+// CHECK:             linalg.yield %[[VAL_18]] : f32\n+// CHECK:           } -> tensor<128x128xf32>\n+// CHECK:           %[[VAL_19:.*]] = arith.muli %[[VAL_5]], %[[VAL_3]] : i32\n+// CHECK:           %[[VAL_20:.*]] = arith.index_cast %[[VAL_19]] : i32 to index\n+// CHECK:           %[[VAL_21:.*]] = arith.addi %[[VAL_20]], %[[VAL_8]] : index\n+// CHECK:           %[[VAL_22:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_21]]], sizes: [128, 128], strides: [2, 1] : memref<*xf32> to memref<128x128xf32, strided<[2, 1], offset: ?>>\n+// CHECK:           memref.tensor_store %[[VAL_23:.*]], %[[VAL_22]] : memref<128x128xf32, strided<[2, 1], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/arith_not_ptr_arith.mlir", "status": "added", "additions": 39, "deletions": 0, "changes": 39, "file_content_changes": "@@ -0,0 +1,39 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %a : !tt.ptr<i32>,\n+    %b : !tt.ptr<i32>\n+  ) -> () {\n+        // offset calculations\n+        %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>\n+        // a pointer\n+        %8 = tt.splat %a : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>\n+        %9 = tt.addptr %8, %0 : tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>\n+        // b pointer\n+        %18 = tt.splat %b : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>\n+        %19 = tt.addptr %18, %0 : tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>\n+        %am = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xi32>\n+        %bm = tt.load %19 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xi32>\n+        %5 = arith.addi %am, %bm : tensor<1024xi32>\n+        tt.store %19, %5 : tensor<1024xi32>\n+        tt.return\n+    }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xi32>, %[[VAL_1:.*]]: memref<*xi32>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32) { \n+// CHECK:           %[[VAL_5:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [1024], strides: [1] : memref<*xi32> to memref<1024xi32, strided<[1]>>\n+// CHECK:           %[[VAL_6:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: [0], sizes: [1024], strides: [1] : memref<*xi32> to memref<1024xi32, strided<[1]>>\n+// CHECK:           %[[VAL_7:.*]] = memref.alloc() : memref<1024xi32>\n+// CHECK:           memref.copy %[[VAL_5]], %[[VAL_7]] : memref<1024xi32, strided<[1]>> to memref<1024xi32>\n+// CHECK:           %[[VAL_8:.*]] = bufferization.to_tensor %[[VAL_7]] restrict writable : memref<1024xi32>\n+// CHECK:           %[[VAL_9:.*]] = memref.alloc() : memref<1024xi32>\n+// CHECK:           memref.copy %[[VAL_6]], %[[VAL_9]] : memref<1024xi32, strided<[1]>> to memref<1024xi32>\n+// CHECK:           %[[VAL_10:.*]] = bufferization.to_tensor %[[VAL_9]] restrict writable : memref<1024xi32>\n+// CHECK:           %[[VAL_11:.*]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_8]], %[[VAL_10]] : tensor<1024xi32>, tensor<1024xi32>) outs(%[[VAL_8]] : tensor<1024xi32>) {\n+// CHECK:           ^bb0(%[[VAL_12:.*]]: i32, %[[VAL_13:.*]]: i32, %[[VAL_14:.*]]: i32):\n+// CHECK:             %[[VAL_15:.*]] = arith.addi %[[VAL_12]], %[[VAL_13]] : i32\n+// CHECK:             linalg.yield %[[VAL_15]] : i32\n+// CHECK:           } -> tensor<1024xi32>\n+// CHECK:           memref.tensor_store %[[VAL_16:.*]], %[[VAL_6]] : memref<1024xi32, strided<[1]>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/bitcast.mlir", "status": "added", "additions": 44, "deletions": 0, "changes": 44, "file_content_changes": "@@ -0,0 +1,44 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+\n+module {\n+  tt.func @kernel(%a : !tt.ptr<i32>, %b : !tt.ptr<f32>) -> () {\n+    // offset calculations\n+    %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>\n+\n+    // a pointer\n+    %8 = tt.splat %a : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>\n+    %9 = tt.addptr %8, %0 : tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>\n+\n+    // b pointer\n+    %18 = tt.splat %b : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+    %19 = tt.addptr %18, %0 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+\n+    %am = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xi32>\n+\n+    // cast result before doing float add\n+    %am_bitcast = tt.bitcast %am : tensor<1024xi32> -> tensor<1024xf32>\n+\n+\n+    tt.store %19, %am_bitcast : tensor<1024xf32>\n+    tt.return\n+  }\n+}\n+\n+// CHECK: module {\n+// CHECK:   func.func @kernel(%arg0: memref<*xi32>, %arg1: memref<*xf32>, %arg2: i32, %arg3: i32, %arg4: i32) {\n+// CHECK:   [[RC_:%.+]] = memref.reinterpret_cast %arg0 to offset: [0], sizes: [1024], strides: [1]{{.*}} : memref<*xi32> to memref<1024xi32, strided<[1]>>\n+// CHECK:   [[RC_0_:%.+]] = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1024], strides: [1]{{.*}} : memref<*xf32> to memref<1024xf32, strided<[1]>>\n+// CHECK:   [[ALLOC_:%.+]] = memref.alloc() : memref<1024xi32>\n+// CHECK:   memref.copy [[RC_]], [[ALLOC_]] : memref<1024xi32, strided<[1]>> to memref<1024xi32>\n+// CHECK:   [[VAR_0_:%.+]] = bufferization.to_tensor [[ALLOC_]] restrict writable : memref<1024xi32>\n+// CHECK:   [[VAR_1_:%.+]] = tensor.empty() : tensor<1024xf32>\n+// CHECK:   [[VAR_2_:%.+]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\"]} ins([[VAR_0_]] : tensor<1024xi32>) outs([[VAR_1_]] : tensor<1024xf32>) {\n+// CHECK:   ^bb0(%in: i32, %out: f32):\n+// CHECK:     [[VAR_5_:%.+]] = arith.bitcast %in : i32 to f32\n+// CHECK:     linalg.yield [[VAR_5_]] : f32\n+// CHECK:   } -> tensor<1024xf32>\n+// CHECK:   memref.tensor_store [[VAR_2_]], [[RC_0_]] : memref<1024xf32, strided<[1]>>\n+// CHECK:     return\n+// CHECK:   }\n+// CHECK: }\n+"}, {"filename": "test/Conversion/TritonToLinalg/convert_1d_elemwise_arith_binary.mlir", "status": "added", "additions": 72, "deletions": 0, "changes": 72, "file_content_changes": "@@ -0,0 +1,72 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %a : !tt.ptr<f32>,\n+    %b : !tt.ptr<f32>,\n+    %c : tensor<1024x!tt.ptr<f32>>\n+  ) -> () {\n+        %cst = arith.constant dense<true> : tensor<1024xi1>\n+        // offset calculations\n+        %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>\n+        // a pointer\n+        %8 = tt.splat %a : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+        %9 = tt.addptr %8, %0 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+        // b pointer\n+        %18 = tt.splat %b : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+        %19 = tt.addptr %18, %0 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+        %am = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n+        %bm = tt.load %19 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n+        %1 = arith.addf %am, %bm : tensor<1024xf32>\n+        %2 = arith.subf %1, %bm : tensor<1024xf32>\n+        %3 = arith.mulf %2, %bm : tensor<1024xf32>\n+        %4 = arith.divf %3, %bm : tensor<1024xf32>\n+        %5 = arith.cmpf \"oeq\", %4, %bm : tensor<1024xf32>\n+        %6 = arith.select %5, %am, %bm : tensor<1024xi1>, tensor<1024xf32>\n+        tt.store %c, %6 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32>\n+        tt.return\n+    }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xf32>, %[[VAL_1:.*]]: memref<*xf32>, %[[VAL_2:.*]]: memref<1024xf32>, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32) { \n+// CHECK:           %[[VAL_6:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [1024], strides: [1] : memref<*xf32> to memref<1024xf32, strided<[1]>>\n+// CHECK:           %[[VAL_7:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: [0], sizes: [1024], strides: [1] : memref<*xf32> to memref<1024xf32, strided<[1]>>\n+// CHECK:           %[[VAL_8:.*]] = memref.alloc() : memref<1024xf32>\n+// CHECK:           memref.copy %[[VAL_6]], %[[VAL_8]] : memref<1024xf32, strided<[1]>> to memref<1024xf32>\n+// CHECK:           %[[VAL_9:.*]] = bufferization.to_tensor %[[VAL_8]] restrict writable : memref<1024xf32>\n+// CHECK:           %[[VAL_10:.*]] = memref.alloc() : memref<1024xf32>\n+// CHECK:           memref.copy %[[VAL_7]], %[[VAL_10]] : memref<1024xf32, strided<[1]>> to memref<1024xf32>\n+// CHECK:           %[[VAL_11:.*]] = bufferization.to_tensor %[[VAL_10]] restrict writable : memref<1024xf32>\n+// CHECK:           %[[VAL_12:.*]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_9]], %[[VAL_11]] : tensor<1024xf32>, tensor<1024xf32>) outs(%[[VAL_9]] : tensor<1024xf32>) {\n+// CHECK:           ^bb0(%[[VAL_13:.*]]: f32, %[[VAL_14:.*]]: f32, %[[VAL_15:.*]]: f32):\n+// CHECK:             %[[VAL_16:.*]] = arith.addf %[[VAL_13]], %[[VAL_14]] : f32\n+// CHECK:             linalg.yield %[[VAL_16]] : f32\n+// CHECK:           } -> tensor<1024xf32>\n+// CHECK:           %[[VAL_17:.*]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_18:.*]], %[[VAL_11]] : tensor<1024xf32>, tensor<1024xf32>) outs(%[[VAL_18]] : tensor<1024xf32>) {\n+// CHECK:           ^bb0(%[[VAL_19:.*]]: f32, %[[VAL_20:.*]]: f32, %[[VAL_21:.*]]: f32):\n+// CHECK:             %[[VAL_22:.*]] = arith.subf %[[VAL_19]], %[[VAL_20]] : f32\n+// CHECK:             linalg.yield %[[VAL_22]] : f32\n+// CHECK:           } -> tensor<1024xf32>\n+// CHECK:           %[[VAL_23:.*]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_24:.*]], %[[VAL_11]] : tensor<1024xf32>, tensor<1024xf32>) outs(%[[VAL_24]] : tensor<1024xf32>) {\n+// CHECK:           ^bb0(%[[VAL_25:.*]]: f32, %[[VAL_26:.*]]: f32, %[[VAL_27:.*]]: f32):\n+// CHECK:             %[[VAL_28:.*]] = arith.mulf %[[VAL_25]], %[[VAL_26]] : f32\n+// CHECK:             linalg.yield %[[VAL_28]] : f32\n+// CHECK:           } -> tensor<1024xf32>\n+// CHECK:           %[[VAL_29:.*]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_30:.*]], %[[VAL_11]] : tensor<1024xf32>, tensor<1024xf32>) outs(%[[VAL_30]] : tensor<1024xf32>) {\n+// CHECK:           ^bb0(%[[VAL_31:.*]]: f32, %[[VAL_32:.*]]: f32, %[[VAL_33:.*]]: f32):\n+// CHECK:             %[[VAL_34:.*]] = arith.divf %[[VAL_31]], %[[VAL_32]] : f32\n+// CHECK:             linalg.yield %[[VAL_34]] : f32\n+// CHECK:           } -> tensor<1024xf32>\n+// CHECK:           %[[VAL_35:.*]] = tensor.empty() : tensor<1024xi1>\n+// CHECK:           %[[VAL_36:.*]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_37:.*]], %[[VAL_11]] : tensor<1024xf32>, tensor<1024xf32>) outs(%[[VAL_35]] : tensor<1024xi1>) {\n+// CHECK:           ^bb0(%[[VAL_38:.*]]: f32, %[[VAL_39:.*]]: f32, %[[VAL_40:.*]]: i1):\n+// CHECK:             %[[VAL_41:.*]] = arith.cmpf oeq, %[[VAL_38]], %[[VAL_39]] : f32\n+// CHECK:             linalg.yield %[[VAL_41]] : i1\n+// CHECK:           } -> tensor<1024xi1>\n+// CHECK:           %[[VAL_42:.*]] = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_43:.*]], %[[VAL_9]], %[[VAL_11]] : tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) outs(%[[VAL_9]] : tensor<1024xf32>) {\n+// CHECK:           ^bb0(%[[VAL_44:.*]]: i1, %[[VAL_45:.*]]: f32, %[[VAL_46:.*]]: f32, %[[VAL_47:.*]]: f32):\n+// CHECK:             %[[VAL_48:.*]] = arith.select %[[VAL_44]], %[[VAL_45]], %[[VAL_46]] : f32\n+// CHECK:             linalg.yield %[[VAL_48]] : f32\n+// CHECK:           } -> tensor<1024xf32>\n+// CHECK:           memref.tensor_store %[[VAL_49:.*]], %[[VAL_2]] : memref<1024xf32>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/convert_1d_elemwise_arith_ternary.mlir", "status": "added", "additions": 49, "deletions": 0, "changes": 49, "file_content_changes": "@@ -0,0 +1,49 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %a : !tt.ptr<i1>,\n+    %b : !tt.ptr<f32>,\n+    %c : !tt.ptr<f32>,\n+    %d : tensor<1024x!tt.ptr<f32>>\n+  ) -> () {\n+    // offset calculations\n+    %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>\n+    // a pointer\n+    %8 = tt.splat %a : (!tt.ptr<i1>) -> tensor<1024x!tt.ptr<i1>>\n+    %9 = tt.addptr %8, %0 : tensor<1024x!tt.ptr<i1>>, tensor<1024xi32>\n+    // b pointer\n+    %18 = tt.splat %b : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+    %19 = tt.addptr %18, %0 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+    // c pointer\n+    %28 = tt.splat %c : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+    %29 = tt.addptr %28, %0 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+    %am = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xi1>\n+    %bm = tt.load %19 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n+    %cm = tt.load %29 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n+    %10 = arith.select %am, %bm, %cm : tensor<1024xi1>, tensor<1024xf32>\n+    tt.store %d, %10 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xi1>, %[[VAL_1:.*]]: memref<*xf32>, %[[VAL_2:.*]]: memref<*xf32>, %[[VAL_3:.*]]: memref<1024xf32>, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32, %[[VAL_6:.*]]: i32) { \n+// CHECK:           %[[VAL_7:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [1024], strides: [1] : memref<*xi1> to memref<1024xi1, strided<[1]>>\n+// CHECK:           %[[VAL_8:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: [0], sizes: [1024], strides: [1] : memref<*xf32> to memref<1024xf32, strided<[1]>>\n+// CHECK:           %[[VAL_9:.*]] = memref.reinterpret_cast %[[VAL_2]] to offset: [0], sizes: [1024], strides: [1] : memref<*xf32> to memref<1024xf32, strided<[1]>>\n+// CHECK:           %[[VAL_10:.*]] = memref.alloc() : memref<1024xi1>\n+// CHECK:           memref.copy %[[VAL_7]], %[[VAL_10]] : memref<1024xi1, strided<[1]>> to memref<1024xi1>\n+// CHECK:           %[[VAL_11:.*]] = bufferization.to_tensor %[[VAL_10]] restrict writable : memref<1024xi1>\n+// CHECK:           %[[VAL_12:.*]] = memref.alloc() : memref<1024xf32>\n+// CHECK:           memref.copy %[[VAL_8]], %[[VAL_12]] : memref<1024xf32, strided<[1]>> to memref<1024xf32>\n+// CHECK:           %[[VAL_13:.*]] = bufferization.to_tensor %[[VAL_12]] restrict writable : memref<1024xf32>\n+// CHECK:           %[[VAL_14:.*]] = memref.alloc() : memref<1024xf32>\n+// CHECK:           memref.copy %[[VAL_9]], %[[VAL_14]] : memref<1024xf32, strided<[1]>> to memref<1024xf32>\n+// CHECK:           %[[VAL_15:.*]] = bufferization.to_tensor %[[VAL_14]] restrict writable : memref<1024xf32>\n+// CHECK:           %[[VAL_16:.*]] = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_11]], %[[VAL_13]], %[[VAL_15]] : tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) outs(%[[VAL_13]] : tensor<1024xf32>) {\n+// CHECK:           ^bb0(%[[VAL_17:.*]]: i1, %[[VAL_18:.*]]: f32, %[[VAL_19:.*]]: f32, %[[VAL_20:.*]]: f32):\n+// CHECK:             %[[VAL_21:.*]] = arith.select %[[VAL_17]], %[[VAL_18]], %[[VAL_19]] : f32\n+// CHECK:             linalg.yield %[[VAL_21]] : f32\n+// CHECK:           } -> tensor<1024xf32>\n+// CHECK:           memref.tensor_store %[[VAL_22:.*]], %[[VAL_3]] : memref<1024xf32>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/convert_1d_elemwise_arith_unary.mlir", "status": "added", "additions": 88, "deletions": 0, "changes": 88, "file_content_changes": "@@ -0,0 +1,88 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %f32ptr : !tt.ptr<f32>,\n+    %intptr : !tt.ptr<i32>,\n+    %f16ptr : !tt.ptr<f16>,\n+    %save0 : tensor<1024x!tt.ptr<bf16>>,\n+    %save1 : tensor<1024x!tt.ptr<f32>>,\n+    %save2 : tensor<1024x!tt.ptr<f32>>,\n+    %save3 : tensor<1024x!tt.ptr<f32>>,\n+    %save4 : tensor<1024x!tt.ptr<f32>>\n+  ) -> () {\n+    // offset calculations\n+    %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>\n+    // f32ptr pointer\n+    %8 = tt.splat %f32ptr : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+    %9 = tt.addptr %8, %0 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+    // intptr pointer\n+    %18 = tt.splat %intptr : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>\n+    %19 = tt.addptr %18, %0 : tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>\n+    // f32ptr pointer\n+    %28 = tt.splat %f16ptr : (!tt.ptr<f16>) -> tensor<1024x!tt.ptr<f16>>\n+    %29 = tt.addptr %28, %0 : tensor<1024x!tt.ptr<f16>>, tensor<1024xi32>\n+    %afm = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n+    %aim = tt.load %19 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xi32>\n+    %bfm = tt.load %29 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf16>\n+    %5 = arith.truncf %afm : tensor<1024xf32> to tensor<1024xbf16>\n+    %6 = math.exp %afm : tensor<1024xf32>\n+    %7 = arith.sitofp %aim : tensor<1024xi32> to tensor<1024xf32>\n+    %10 = arith.extf %bfm : tensor<1024xf16> to tensor<1024xf32>\n+    %11 = math.sqrt %afm : tensor<1024xf32>\n+    tt.store %save0, %5 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xbf16>\n+    tt.store %save1, %6 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32>\n+    tt.store %save2, %7 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32>\n+    tt.store %save3, %10 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32>\n+    tt.store %save4, %11 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xf32>, %[[VAL_1:.*]]: memref<*xi32>, %[[VAL_2:.*]]: memref<*xf16>, %[[VAL_3:.*]]: memref<1024xbf16>, %[[VAL_4:.*]]: memref<1024xf32>, %[[VAL_5:.*]]: memref<1024xf32>, %[[VAL_6:.*]]: memref<1024xf32>, %[[VAL_7:.*]]: memref<1024xf32>, %[[VAL_8:.*]]: i32, %[[VAL_9:.*]]: i32, %[[VAL_10:.*]]: i32) { \n+// CHECK:           %[[VAL_11:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [1024], strides: [1] : memref<*xf32> to memref<1024xf32, strided<[1]>>\n+// CHECK:           %[[VAL_12:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: [0], sizes: [1024], strides: [1] : memref<*xi32> to memref<1024xi32, strided<[1]>>\n+// CHECK:           %[[VAL_13:.*]] = memref.reinterpret_cast %[[VAL_2]] to offset: [0], sizes: [1024], strides: [1] : memref<*xf16> to memref<1024xf16, strided<[1]>>\n+// CHECK:           %[[VAL_14:.*]] = memref.alloc() : memref<1024xf32>\n+// CHECK:           memref.copy %[[VAL_11]], %[[VAL_14]] : memref<1024xf32, strided<[1]>> to memref<1024xf32>\n+// CHECK:           %[[VAL_15:.*]] = bufferization.to_tensor %[[VAL_14]] restrict writable : memref<1024xf32>\n+// CHECK:           %[[VAL_16:.*]] = memref.alloc() : memref<1024xi32>\n+// CHECK:           memref.copy %[[VAL_12]], %[[VAL_16]] : memref<1024xi32, strided<[1]>> to memref<1024xi32>\n+// CHECK:           %[[VAL_17:.*]] = bufferization.to_tensor %[[VAL_16]] restrict writable : memref<1024xi32>\n+// CHECK:           %[[VAL_18:.*]] = memref.alloc() : memref<1024xf16>\n+// CHECK:           memref.copy %[[VAL_13]], %[[VAL_18]] : memref<1024xf16, strided<[1]>> to memref<1024xf16>\n+// CHECK:           %[[VAL_19:.*]] = bufferization.to_tensor %[[VAL_18]] restrict writable : memref<1024xf16>\n+// CHECK:           %[[VAL_20:.*]] = tensor.empty() : tensor<1024xbf16>\n+// CHECK:           %[[VAL_21:.*]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_15]] : tensor<1024xf32>) outs(%[[VAL_20]] : tensor<1024xbf16>) {\n+// CHECK:           ^bb0(%[[VAL_22:.*]]: f32, %[[VAL_23:.*]]: bf16):\n+// CHECK:             %[[VAL_24:.*]] = arith.truncf %[[VAL_22]] : f32 to bf16\n+// CHECK:             linalg.yield %[[VAL_24]] : bf16\n+// CHECK:           } -> tensor<1024xbf16>\n+// CHECK:           %[[VAL_25:.*]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_15]] : tensor<1024xf32>) outs(%[[VAL_15]] : tensor<1024xf32>) {\n+// CHECK:           ^bb0(%[[VAL_26:.*]]: f32, %[[VAL_27:.*]]: f32):\n+// CHECK:             %[[VAL_28:.*]] = math.exp %[[VAL_26]] : f32\n+// CHECK:             linalg.yield %[[VAL_28]] : f32\n+// CHECK:           } -> tensor<1024xf32>\n+// CHECK:           %[[VAL_29:.*]] = tensor.empty() : tensor<1024xf32>\n+// CHECK:           %[[VAL_30:.*]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_17]] : tensor<1024xi32>) outs(%[[VAL_29]] : tensor<1024xf32>) {\n+// CHECK:           ^bb0(%[[VAL_31:.*]]: i32, %[[VAL_32:.*]]: f32):\n+// CHECK:             %[[VAL_33:.*]] = arith.sitofp %[[VAL_31]] : i32 to f32\n+// CHECK:             linalg.yield %[[VAL_33]] : f32\n+// CHECK:           } -> tensor<1024xf32>\n+// CHECK:           %[[VAL_34:.*]] = tensor.empty() : tensor<1024xf32>\n+// CHECK:           %[[VAL_35:.*]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_19]] : tensor<1024xf16>) outs(%[[VAL_34]] : tensor<1024xf32>) {\n+// CHECK:           ^bb0(%[[VAL_36:.*]]: f16, %[[VAL_37:.*]]: f32):\n+// CHECK:             %[[VAL_38:.*]] = arith.extf %[[VAL_36]] : f16 to f32\n+// CHECK:             linalg.yield %[[VAL_38]] : f32\n+// CHECK:           } -> tensor<1024xf32>\n+// CHECK:           %[[VAL_39:.*]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\"]} ins(%[[VAL_15]] : tensor<1024xf32>) outs(%[[VAL_15]] : tensor<1024xf32>) {\n+// CHECK:           ^bb0(%[[VAL_40:.*]]: f32, %[[VAL_41:.*]]: f32):\n+// CHECK:             %[[VAL_42:.*]] = math.sqrt %[[VAL_40]] : f32\n+// CHECK:             linalg.yield %[[VAL_42]] : f32\n+// CHECK:           } -> tensor<1024xf32>\n+// CHECK:           memref.tensor_store %[[VAL_43:.*]], %[[VAL_3]] : memref<1024xbf16>\n+// CHECK:           memref.tensor_store %[[VAL_44:.*]], %[[VAL_4]] : memref<1024xf32>\n+// CHECK:           memref.tensor_store %[[VAL_45:.*]], %[[VAL_5]] : memref<1024xf32>\n+// CHECK:           memref.tensor_store %[[VAL_46:.*]], %[[VAL_6]] : memref<1024xf32>\n+// CHECK:           memref.tensor_store %[[VAL_47:.*]], %[[VAL_7]] : memref<1024xf32>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/convert_2d_elemwise_arith_binary.mlir", "status": "added", "additions": 55, "deletions": 0, "changes": 55, "file_content_changes": "@@ -0,0 +1,55 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %a : !tt.ptr<f32>,\n+    %b : !tt.ptr<f32>,\n+    %c : tensor<128x128x!tt.ptr<f32>>,\n+    %d : tensor<128x128x!tt.ptr<f32>>\n+  ) -> () {\n+        // offset calculations\n+        %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+        %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n+        %moff = tt.broadcast %1 : (tensor<128x1xi32>) -> tensor<128x128xi32>\n+        %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+        %4 = tt.expand_dims %3 {axis = 0 : i32} : (tensor<128xi32>) -> tensor<1x128xi32>\n+        %koff = tt.broadcast %4 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n+        %mkoff = arith.addi %moff, %koff : tensor<128x128xi32>\n+        // a pointer\n+        %8 = tt.splat %a : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n+        %9 = tt.addptr %8, %mkoff : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n+        // b pointer\n+        %18 = tt.splat %b : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n+        %19 = tt.addptr %18, %mkoff : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n+        %af = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf32>\n+        %bf = tt.load %19 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf32>\n+        %res0 = arith.addf %af, %bf : tensor<128x128xf32>\n+        %res1 = arith.subf %af, %bf : tensor<128x128xf32>\n+        tt.store %c, %res0 {cache = 1 : i32, evict = 1 : i32} : tensor<128x128xf32>\n+        tt.store %d, %res1 {cache = 1 : i32, evict = 1 : i32} : tensor<128x128xf32>\n+        tt.return\n+    }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xf32>, %[[VAL_1:.*]]: memref<*xf32>, %[[VAL_2:.*]]: memref<128x128xf32>, %[[VAL_3:.*]]: memref<128x128xf32>, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32, %[[VAL_6:.*]]: i32) { \n+// CHECK:           %[[VAL_7:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [128, 128], strides: [1, 1] : memref<*xf32> to memref<128x128xf32, strided<[1, 1]>>\n+// CHECK:           %[[VAL_8:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: [0], sizes: [128, 128], strides: [1, 1] : memref<*xf32> to memref<128x128xf32, strided<[1, 1]>>\n+// CHECK:           %[[VAL_9:.*]] = memref.alloc() : memref<128x128xf32>\n+// CHECK:           memref.copy %[[VAL_7]], %[[VAL_9]] : memref<128x128xf32, strided<[1, 1]>> to memref<128x128xf32>\n+// CHECK:           %[[VAL_10:.*]] = bufferization.to_tensor %[[VAL_9]] restrict writable : memref<128x128xf32>\n+// CHECK:           %[[VAL_11:.*]] = memref.alloc() : memref<128x128xf32>\n+// CHECK:           memref.copy %[[VAL_8]], %[[VAL_11]] : memref<128x128xf32, strided<[1, 1]>> to memref<128x128xf32>\n+// CHECK:           %[[VAL_12:.*]] = bufferization.to_tensor %[[VAL_11]] restrict writable : memref<128x128xf32>\n+// CHECK:           %[[VAL_13:.*]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_10]], %[[VAL_12]] : tensor<128x128xf32>, tensor<128x128xf32>) outs(%[[VAL_10]] : tensor<128x128xf32>) {\n+// CHECK:           ^bb0(%[[VAL_14:.*]]: f32, %[[VAL_15:.*]]: f32, %[[VAL_16:.*]]: f32):\n+// CHECK:             %[[VAL_17:.*]] = arith.addf %[[VAL_14]], %[[VAL_15]] : f32\n+// CHECK:             linalg.yield %[[VAL_17]] : f32\n+// CHECK:           } -> tensor<128x128xf32>\n+// CHECK:           %[[VAL_18:.*]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_10]], %[[VAL_12]] : tensor<128x128xf32>, tensor<128x128xf32>) outs(%[[VAL_10]] : tensor<128x128xf32>) {\n+// CHECK:           ^bb0(%[[VAL_19:.*]]: f32, %[[VAL_20:.*]]: f32, %[[VAL_21:.*]]: f32):\n+// CHECK:             %[[VAL_22:.*]] = arith.subf %[[VAL_19]], %[[VAL_20]] : f32\n+// CHECK:             linalg.yield %[[VAL_22]] : f32\n+// CHECK:           } -> tensor<128x128xf32>\n+// CHECK:           memref.tensor_store %[[VAL_23:.*]], %[[VAL_2]] : memref<128x128xf32>\n+// CHECK:           memref.tensor_store %[[VAL_24:.*]], %[[VAL_3]] : memref<128x128xf32>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/convert_2d_elemwise_arith_ternary.mlir", "status": "added", "additions": 55, "deletions": 0, "changes": 55, "file_content_changes": "@@ -0,0 +1,55 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+    tt.func @kernel(\n+                        %a : !tt.ptr<i1>,\n+                        %b : !tt.ptr<f32>,\n+                        %c : !tt.ptr<f32>,\n+                        %d : tensor<128x128x!tt.ptr<f32>>\n+  ) -> () {\n+        // offset calculations\n+        %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+        %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n+        %moff = tt.broadcast %1 : (tensor<128x1xi32>) -> tensor<128x128xi32>\n+        %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+        %4 = tt.expand_dims %3 {axis = 0 : i32} : (tensor<128xi32>) -> tensor<1x128xi32>\n+        %koff = tt.broadcast %4 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n+        %mkoff = arith.addi %moff, %koff : tensor<128x128xi32>\n+        // a pointer\n+        %8 = tt.splat %a : (!tt.ptr<i1>) -> tensor<128x128x!tt.ptr<i1>>\n+        %9 = tt.addptr %8, %mkoff : tensor<128x128x!tt.ptr<i1>>, tensor<128x128xi32>\n+        // b pointer\n+        %18 = tt.splat %b : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n+        %19 = tt.addptr %18, %mkoff : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n+        // c pointer\n+        %28 = tt.splat %c : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n+        %29 = tt.addptr %28, %mkoff : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n+        %am = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xi1>\n+        %bm = tt.load %19 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf32>\n+        %cm = tt.load %29 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf32>\n+        %100 = arith.select %am, %bm, %cm : tensor<128x128xi1>, tensor<128x128xf32>\n+        tt.store %d, %100 {cache = 1 : i32, evict = 1 : i32} : tensor<128x128xf32>\n+        tt.return\n+    }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xi1>, %[[VAL_1:.*]]: memref<*xf32>, %[[VAL_2:.*]]: memref<*xf32>, %[[VAL_3:.*]]: memref<128x128xf32>, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32, %[[VAL_6:.*]]: i32) { \n+// CHECK:           %[[VAL_7:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [128, 128], strides: [1, 1] : memref<*xi1> to memref<128x128xi1, strided<[1, 1]>>\n+// CHECK:           %[[VAL_8:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: [0], sizes: [128, 128], strides: [1, 1] : memref<*xf32> to memref<128x128xf32, strided<[1, 1]>>\n+// CHECK:           %[[VAL_9:.*]] = memref.reinterpret_cast %[[VAL_2]] to offset: [0], sizes: [128, 128], strides: [1, 1] : memref<*xf32> to memref<128x128xf32, strided<[1, 1]>>\n+// CHECK:           %[[VAL_10:.*]] = memref.alloc() : memref<128x128xi1>\n+// CHECK:           memref.copy %[[VAL_7]], %[[VAL_10]] : memref<128x128xi1, strided<[1, 1]>> to memref<128x128xi1>\n+// CHECK:           %[[VAL_11:.*]] = bufferization.to_tensor %[[VAL_10]] restrict writable : memref<128x128xi1>\n+// CHECK:           %[[VAL_12:.*]] = memref.alloc() : memref<128x128xf32>\n+// CHECK:           memref.copy %[[VAL_8]], %[[VAL_12]] : memref<128x128xf32, strided<[1, 1]>> to memref<128x128xf32>\n+// CHECK:           %[[VAL_13:.*]] = bufferization.to_tensor %[[VAL_12]] restrict writable : memref<128x128xf32>\n+// CHECK:           %[[VAL_14:.*]] = memref.alloc() : memref<128x128xf32>\n+// CHECK:           memref.copy %[[VAL_9]], %[[VAL_14]] : memref<128x128xf32, strided<[1, 1]>> to memref<128x128xf32>\n+// CHECK:           %[[VAL_15:.*]] = bufferization.to_tensor %[[VAL_14]] restrict writable : memref<128x128xf32>\n+// CHECK:           %[[VAL_16:.*]] = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_11]], %[[VAL_13]], %[[VAL_15]] : tensor<128x128xi1>, tensor<128x128xf32>, tensor<128x128xf32>) outs(%[[VAL_13]] : tensor<128x128xf32>) {\n+// CHECK:           ^bb0(%[[VAL_17:.*]]: i1, %[[VAL_18:.*]]: f32, %[[VAL_19:.*]]: f32, %[[VAL_20:.*]]: f32):\n+// CHECK:             %[[VAL_21:.*]] = arith.select %[[VAL_17]], %[[VAL_18]], %[[VAL_19]] : f32\n+// CHECK:             linalg.yield %[[VAL_21]] : f32\n+// CHECK:           } -> tensor<128x128xf32>\n+// CHECK:           memref.tensor_store %[[VAL_22:.*]], %[[VAL_3]] : memref<128x128xf32>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/convert_2d_elemwise_arith_unary.mlir", "status": "added", "additions": 94, "deletions": 0, "changes": 94, "file_content_changes": "@@ -0,0 +1,94 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %f32ptr : !tt.ptr<f32>,\n+    %intptr : !tt.ptr<i32>,\n+    %f16ptr : !tt.ptr<f16>,\n+    %save0 : tensor<128x128x!tt.ptr<bf16>>,\n+    %save1 : tensor<128x128x!tt.ptr<f32>>,\n+    %save2 : tensor<128x128x!tt.ptr<f32>>,\n+    %save3 : tensor<128x128x!tt.ptr<f32>>,\n+    %save4 : tensor<128x128x!tt.ptr<f32>>\n+  ) -> () {\n+    // offset calculations\n+    %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+    %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n+    %moff = tt.broadcast %1 : (tensor<128x1xi32>) -> tensor<128x128xi32>\n+    %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+    %4 = tt.expand_dims %3 {axis = 0 : i32} : (tensor<128xi32>) -> tensor<1x128xi32>\n+    %koff = tt.broadcast %4 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n+    %mkoff = arith.addi %moff, %koff : tensor<128x128xi32>\n+    // f32ptr pointer\n+    %8 = tt.splat %f32ptr : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n+    %9 = tt.addptr %8, %mkoff : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n+    // intptr pointer\n+    %18 = tt.splat %intptr : (!tt.ptr<i32>) -> tensor<128x128x!tt.ptr<i32>>\n+    %19 = tt.addptr %18, %mkoff : tensor<128x128x!tt.ptr<i32>>, tensor<128x128xi32>\n+    // f16ptr pointer\n+    %28 = tt.splat %f16ptr : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>>\n+    %29 = tt.addptr %28, %mkoff : tensor<128x128x!tt.ptr<f16>>, tensor<128x128xi32>\n+    %afm = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf32>\n+    %aim = tt.load %19 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xi32>\n+    %bfm = tt.load %29 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf16>\n+    %5 = arith.truncf %afm : tensor<128x128xf32> to tensor<128x128xbf16>\n+    %6 = math.exp %afm : tensor<128x128xf32>\n+    %7 = arith.sitofp %aim : tensor<128x128xi32> to tensor<128x128xf32>\n+    %10 = arith.extf %bfm : tensor<128x128xf16> to tensor<128x128xf32>\n+    %11 = math.sqrt %afm : tensor<128x128xf32>\n+    tt.store %save0, %5 {cache = 1 : i32, evict = 1 : i32} : tensor<128x128xbf16>\n+    tt.store %save1, %6 {cache = 1 : i32, evict = 1 : i32} : tensor<128x128xf32>\n+    tt.store %save2, %7 {cache = 1 : i32, evict = 1 : i32} : tensor<128x128xf32>\n+    tt.store %save3, %10 {cache = 1 : i32, evict = 1 : i32} : tensor<128x128xf32>\n+    tt.store %save4, %11 {cache = 1 : i32, evict = 1 : i32} : tensor<128x128xf32>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xf32>, %[[VAL_1:.*]]: memref<*xi32>, %[[VAL_2:.*]]: memref<*xf16>, %[[VAL_3:.*]]: memref<128x128xbf16>, %[[VAL_4:.*]]: memref<128x128xf32>, %[[VAL_5:.*]]: memref<128x128xf32>, %[[VAL_6:.*]]: memref<128x128xf32>, %[[VAL_7:.*]]: memref<128x128xf32>, %[[VAL_8:.*]]: i32, %[[VAL_9:.*]]: i32, %[[VAL_10:.*]]: i32) { \n+// CHECK:           %[[VAL_11:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [128, 128], strides: [1, 1] : memref<*xf32> to memref<128x128xf32, strided<[1, 1]>>\n+// CHECK:           %[[VAL_12:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: [0], sizes: [128, 128], strides: [1, 1] : memref<*xi32> to memref<128x128xi32, strided<[1, 1]>>\n+// CHECK:           %[[VAL_13:.*]] = memref.reinterpret_cast %[[VAL_2]] to offset: [0], sizes: [128, 128], strides: [1, 1] : memref<*xf16> to memref<128x128xf16, strided<[1, 1]>>\n+// CHECK:           %[[VAL_14:.*]] = memref.alloc() : memref<128x128xf32>\n+// CHECK:           memref.copy %[[VAL_11]], %[[VAL_14]] : memref<128x128xf32, strided<[1, 1]>> to memref<128x128xf32>\n+// CHECK:           %[[VAL_15:.*]] = bufferization.to_tensor %[[VAL_14]] restrict writable : memref<128x128xf32>\n+// CHECK:           %[[VAL_16:.*]] = memref.alloc() : memref<128x128xi32>\n+// CHECK:           memref.copy %[[VAL_12]], %[[VAL_16]] : memref<128x128xi32, strided<[1, 1]>> to memref<128x128xi32>\n+// CHECK:           %[[VAL_17:.*]] = bufferization.to_tensor %[[VAL_16]] restrict writable : memref<128x128xi32>\n+// CHECK:           %[[VAL_18:.*]] = memref.alloc() : memref<128x128xf16>\n+// CHECK:           memref.copy %[[VAL_13]], %[[VAL_18]] : memref<128x128xf16, strided<[1, 1]>> to memref<128x128xf16>\n+// CHECK:           %[[VAL_19:.*]] = bufferization.to_tensor %[[VAL_18]] restrict writable : memref<128x128xf16>\n+// CHECK:           %[[VAL_20:.*]] = tensor.empty() : tensor<128x128xbf16>\n+// CHECK:           %[[VAL_21:.*]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_15]] : tensor<128x128xf32>) outs(%[[VAL_20]] : tensor<128x128xbf16>) {\n+// CHECK:           ^bb0(%[[VAL_22:.*]]: f32, %[[VAL_23:.*]]: bf16):\n+// CHECK:             %[[VAL_24:.*]] = arith.truncf %[[VAL_22]] : f32 to bf16\n+// CHECK:             linalg.yield %[[VAL_24]] : bf16\n+// CHECK:           } -> tensor<128x128xbf16>\n+// CHECK:           %[[VAL_25:.*]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_15]] : tensor<128x128xf32>) outs(%[[VAL_15]] : tensor<128x128xf32>) {\n+// CHECK:           ^bb0(%[[VAL_26:.*]]: f32, %[[VAL_27:.*]]: f32):\n+// CHECK:             %[[VAL_28:.*]] = math.exp %[[VAL_26]] : f32\n+// CHECK:             linalg.yield %[[VAL_28]] : f32\n+// CHECK:           } -> tensor<128x128xf32>\n+// CHECK:           %[[VAL_29:.*]] = tensor.empty() : tensor<128x128xf32>\n+// CHECK:           %[[VAL_30:.*]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_17]] : tensor<128x128xi32>) outs(%[[VAL_29]] : tensor<128x128xf32>) {\n+// CHECK:           ^bb0(%[[VAL_31:.*]]: i32, %[[VAL_32:.*]]: f32):\n+// CHECK:             %[[VAL_33:.*]] = arith.sitofp %[[VAL_31]] : i32 to f32\n+// CHECK:             linalg.yield %[[VAL_33]] : f32\n+// CHECK:           } -> tensor<128x128xf32>\n+// CHECK:           %[[VAL_34:.*]] = tensor.empty() : tensor<128x128xf32>\n+// CHECK:           %[[VAL_35:.*]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_19]] : tensor<128x128xf16>) outs(%[[VAL_34]] : tensor<128x128xf32>) {\n+// CHECK:           ^bb0(%[[VAL_36:.*]]: f16, %[[VAL_37:.*]]: f32):\n+// CHECK:             %[[VAL_38:.*]] = arith.extf %[[VAL_36]] : f16 to f32\n+// CHECK:             linalg.yield %[[VAL_38]] : f32\n+// CHECK:           } -> tensor<128x128xf32>\n+// CHECK:           %[[VAL_39:.*]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_15]] : tensor<128x128xf32>) outs(%[[VAL_15]] : tensor<128x128xf32>) {\n+// CHECK:           ^bb0(%[[VAL_40:.*]]: f32, %[[VAL_41:.*]]: f32):\n+// CHECK:             %[[VAL_42:.*]] = math.sqrt %[[VAL_40]] : f32\n+// CHECK:             linalg.yield %[[VAL_42]] : f32\n+// CHECK:           } -> tensor<128x128xf32>\n+// CHECK:           memref.tensor_store %[[VAL_43:.*]], %[[VAL_3]] : memref<128x128xbf16>\n+// CHECK:           memref.tensor_store %[[VAL_44:.*]], %[[VAL_4]] : memref<128x128xf32>\n+// CHECK:           memref.tensor_store %[[VAL_45:.*]], %[[VAL_5]] : memref<128x128xf32>\n+// CHECK:           memref.tensor_store %[[VAL_46:.*]], %[[VAL_6]] : memref<128x128xf32>\n+// CHECK:           memref.tensor_store %[[VAL_47:.*]], %[[VAL_7]] : memref<128x128xf32>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/convert_splat_float.mlir", "status": "added", "additions": 23, "deletions": 0, "changes": 23, "file_content_changes": "@@ -0,0 +1,23 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+    tt.func @kernel(%fin : f32,\n+                    %bin : bf16,\n+                    %save0 : tensor<1024x!tt.ptr<f32>>,\n+                    %save1 : tensor<128x256x!tt.ptr<bf16>>) -> () {\n+        %0 = tt.splat %fin : (f32) -> (tensor<1024xf32>)\n+        %1 = tt.splat %bin : (bf16) -> (tensor<128x256xbf16>)\n+        tt.store %save0, %0 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32>\n+        tt.store %save1, %1 {cache = 1 : i32, evict = 1 : i32} : tensor<128x256xbf16>\n+        tt.return\n+    }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: f32, %[[VAL_1:.*]]: bf16, %[[VAL_2:.*]]: memref<1024xf32>, %[[VAL_3:.*]]: memref<128x256xbf16>, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32, %[[VAL_6:.*]]: i32) { \n+// CHECK:           %[[VAL_7:.*]] = tensor.empty() : tensor<1024xf32>\n+// CHECK:           %[[VAL_8:.*]] = linalg.fill ins(%[[VAL_0]] : f32) outs(%[[VAL_7]] : tensor<1024xf32>) -> tensor<1024xf32>\n+// CHECK:           %[[VAL_9:.*]] = tensor.empty() : tensor<128x256xbf16>\n+// CHECK:           %[[VAL_10:.*]] = linalg.fill ins(%[[VAL_1]] : bf16) outs(%[[VAL_9]] : tensor<128x256xbf16>) -> tensor<128x256xbf16>\n+// CHECK:           memref.tensor_store %[[VAL_8]], %[[VAL_2]] : memref<1024xf32>\n+// CHECK:           memref.tensor_store %[[VAL_10]], %[[VAL_3]] : memref<128x256xbf16>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/dot.mlir", "status": "added", "additions": 78, "deletions": 0, "changes": 78, "file_content_changes": "@@ -0,0 +1,78 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %arg0 : !tt.ptr<bf16>,\n+    %arg1 : !tt.ptr<bf16>,\n+    %arg2 : !tt.ptr<bf16>\n+  )\n+  {\n+    %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+    %c64 = arith.constant 128 : i32\n+    %1 = tt.splat %c64 : (i32) -> tensor<128xi32>\n+    %2 = arith.muli %0, %1 : tensor<128xi32>\n+    %3 = tt.expand_dims %2 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n+    %4 = tt.broadcast %3 : (tensor<128x1xi32>) -> tensor<128x64xi32>\n+    %5 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %6 = tt.expand_dims %5 {axis = 0 : i32} : (tensor<64xi32>) -> tensor<1x64xi32>\n+    %7 = tt.broadcast %6 : (tensor<1x64xi32>) -> tensor<128x64xi32>\n+    %8 = arith.addi %4, %7 : tensor<128x64xi32>\n+    %10 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32>\n+    %11 = tt.expand_dims %10 {axis = 1 : i32} : (tensor<256xi32>) -> tensor<256x1xi32>\n+    %12 = tt.broadcast %11 : (tensor<256x1xi32>) -> tensor<256x64xi32>\n+    %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %c256 = arith.constant 256 : i32\n+    %14 = tt.splat %c256 : (i32) -> tensor<64xi32>\n+    %15 = arith.muli %13, %14 : tensor<64xi32>\n+    %16 = tt.expand_dims %15 {axis = 0 : i32} : (tensor<64xi32>) -> tensor<1x64xi32>\n+    %17 = tt.broadcast %16 : (tensor<1x64xi32>) -> tensor<256x64xi32>\n+    %18 = arith.addi %12, %17 : tensor<256x64xi32>\n+    %20 = tt.splat %c256 : (i32) -> tensor<128xi32>\n+    %21 = arith.muli %0, %20 : tensor<128xi32>\n+    %22 = tt.expand_dims %21 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n+    %23 = tt.broadcast %22 : (tensor<128x1xi32>) -> tensor<128x256xi32>\n+    %24 = tt.expand_dims %10 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+    %25 = tt.broadcast %24 {axis = 0 : i32} : (tensor<1x256xi32>) -> tensor<128x256xi32>\n+    %26 = arith.addi %23, %25 : tensor<128x256xi32>\n+    %30 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<128x64x!tt.ptr<bf16>>\n+    %31 = tt.addptr %30, %8 : tensor<128x64x!tt.ptr<bf16>>, tensor<128x64xi32>\n+    %32 = tt.load %31 {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<128x64xbf16>\n+    %40 = tt.splat %arg1 : (!tt.ptr<bf16>) -> tensor<256x64x!tt.ptr<bf16>>\n+    %41 = tt.addptr %40, %18 : tensor<256x64x!tt.ptr<bf16>>, tensor<256x64xi32>\n+    %42 = tt.load %41 {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<256x64xbf16>\n+    %43 = tt.trans %42 : (tensor<256x64xbf16>) -> tensor<64x256xbf16>\n+    %50 = tt.splat %arg2 : (!tt.ptr<bf16>) -> tensor<128x256x!tt.ptr<bf16>>\n+    %51 = tt.addptr %50, %26 : tensor<128x256x!tt.ptr<bf16>>, tensor<128x256xi32>\n+    %52 = tt.load %51 {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<128x256xbf16>\n+    %60 = tt.dot %32, %43, %52 {allowTF32 = false} : tensor<128x64xbf16> * tensor<64x256xbf16> -> tensor<128x256xbf16>\n+    tt.store %51, %60 : tensor<128x256xbf16>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: memref<*xbf16>, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 256 : index\n+// CHECK-DAG:           %[[VAL_7:.*]] = arith.constant 128 : index\n+// CHECK:           %[[VAL_8:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [128, 64], strides: {{\\[}}%[[VAL_7]], 1] : memref<*xbf16> to memref<128x64xbf16, strided<[?, 1]>>\n+// CHECK:           %[[VAL_9:.*]] = memref.alloc() : memref<128x64xbf16>\n+// CHECK:           memref.copy %[[VAL_8]], %[[VAL_9]] : memref<128x64xbf16, strided<[?, 1]>> to memref<128x64xbf16>\n+// CHECK:           %[[VAL_10:.*]] = bufferization.to_tensor %[[VAL_9]] restrict writable : memref<128x64xbf16>\n+// CHECK:           %[[VAL_11:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: [0], sizes: [256, 64], strides: [1, %[[VAL_6]]] : memref<*xbf16> to memref<256x64xbf16, strided<[1, ?]>>\n+// CHECK:           %[[VAL_12:.*]] = memref.alloc() : memref<256x64xbf16>\n+// CHECK:           memref.copy %[[VAL_11]], %[[VAL_12]] : memref<256x64xbf16, strided<[1, ?]>> to memref<256x64xbf16>\n+// CHECK:           %[[VAL_13:.*]] = bufferization.to_tensor %[[VAL_12]] restrict writable : memref<256x64xbf16>\n+// CHECK:           %[[VAL_14:.*]] = tensor.empty() : tensor<64x256xbf16>\n+// CHECK:           %[[VAL_15:.*]] = linalg.transpose ins(%[[VAL_13]] : tensor<256x64xbf16>) outs(%[[VAL_14]] : tensor<64x256xbf16>) permutation = [1, 0]\n+// CHECK:           %[[VAL_16:.*]] = memref.reinterpret_cast %[[VAL_2]] to offset: [0], sizes: [128, 256], strides: {{\\[}}%[[VAL_6]], 1] : memref<*xbf16> to memref<128x256xbf16, strided<[?, 1]>>\n+// CHECK:           %[[VAL_17:.*]] = memref.alloc() : memref<128x256xbf16>\n+// CHECK:           memref.copy %[[VAL_16]], %[[VAL_17]] : memref<128x256xbf16, strided<[?, 1]>> to memref<128x256xbf16>\n+// CHECK:           %[[VAL_18:.*]] = bufferization.to_tensor %[[VAL_17]] restrict writable : memref<128x256xbf16>\n+// CHECK:           %[[VAL_19:.*]] = tensor.empty() : tensor<128x256xbf16>\n+// CHECK:           %[[VAL_20:.*]] = linalg.matmul ins(%[[VAL_10]], %[[VAL_15]] : tensor<128x64xbf16>, tensor<64x256xbf16>) outs(%[[VAL_19]] : tensor<128x256xbf16>) -> tensor<128x256xbf16>\n+// CHECK:           %[[VAL_21:.*]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_20]], %[[VAL_18]] : tensor<128x256xbf16>, tensor<128x256xbf16>) outs(%[[VAL_20]] : tensor<128x256xbf16>) {\n+// CHECK:           ^bb0(%[[VAL_22:.*]]: bf16, %[[VAL_23:.*]]: bf16, %[[VAL_24:.*]]: bf16):\n+// CHECK:             %[[VAL_25:.*]] = arith.addf %[[VAL_22]], %[[VAL_23]] : bf16\n+// CHECK:             linalg.yield %[[VAL_25]] : bf16\n+// CHECK:           } -> tensor<128x256xbf16>\n+// CHECK:           memref.tensor_store %[[VAL_26:.*]], %[[VAL_16]] : memref<128x256xbf16, strided<[?, 1]>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/kernel-01-vector-add.mlir", "status": "added", "additions": 77, "deletions": 0, "changes": 77, "file_content_changes": "@@ -0,0 +1,77 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+\n+module {\n+  tt.func public @add_kernel_01234(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32) {\n+    %c1024_i32 = arith.constant 1024 : i32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %c1024_i32 : i32\n+    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>\n+    %3 = tt.splat %1 : (i32) -> tensor<1024xi32>\n+    %4 = arith.addi %3, %2 : tensor<1024xi32>\n+    %5 = tt.splat %arg3 : (i32) -> tensor<1024xi32>\n+    %6 = arith.cmpi slt, %4, %5 : tensor<1024xi32>\n+    %7 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+    %8 = tt.addptr %7, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+    %9 = tt.load %8, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n+    %10 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+    %11 = tt.addptr %10, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+    %12 = tt.load %11, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n+    %13 = arith.addf %9, %12 : tensor<1024xf32>\n+    %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+    %15 = tt.addptr %14, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+    tt.store %15, %13, %6 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32>\n+    tt.return\n+  }\n+}\n+\n+// CHECK-DAG:   [[MAP_0_:#.+]] = affine_map<(d0) -> (d0)>\n+// CHECK-LABEL:  func.func @add_kernel_01234\n+// CHECK-SAME:   ([[PARAM_0_:%.+]]: memref<*xf32>, [[PARAM_1_:%.+]]: memref<*xf32>, [[PARAM_2_:%.+]]: memref<*xf32>, [[PARAM_3_:%.+]]: i32, [[PARAM_4_:%.+]]: i32, [[PARAM_5_:%.+]]: i32, [[PARAM_6_:%.+]]: i32) {\n+// CHECK-DAG:       [[CST_1024_:%.+]] = arith.constant 1024 : index\n+// CHECK-DAG:       [[CST_1024_1_:%.+]] = arith.constant 1024 : i32\n+// CHECK:           [[VAR_0_:%.+]] = arith.muli [[PARAM_4_]], [[CST_1024_1_]] : i32\n+// CHECK:           [[VAR_1_:%.+]] = arith.index_cast [[VAR_0_]] : i32 to index\n+// CHECK-DAG:       [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[VAR_1_]]{{.}}, sizes: [1024], strides: [1]{{.*}} : memref<*xf32> to memref<1024xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:       [[RES_:%.+]] = memref.alloc() : memref<1024xf32>\n+// CHECK-DAG:       [[VAR_2_:%.+]] = arith.index_cast [[VAR_0_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_3_:%.+]] = arith.addi [[VAR_2_]], [[CST_1024_]] : index\n+// CHECK-DAG:       [[VAR_4_:%.+]] = arith.index_cast [[PARAM_3_]] : i32 to index\n+// CHECK:           [[VAR_5_:%.+]] = arith.minsi [[VAR_3_]], [[VAR_4_]] : index\n+// CHECK:           [[VAR_6_:%.+]] = arith.subi [[VAR_5_]], [[VAR_2_]] : index\n+// CHECK-DAG:       [[VAR_subview_:%.+]] = memref.subview [[VAR_reinterpret_cast_]][0] {{.}}[[VAR_6_]]{{.}} [1]{{.*}} : memref<1024xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:       [[VAR_subview_0_:%.+]] = memref.subview [[RES_]][0] {{.}}[[VAR_6_]]{{.}} [1] : memref<1024xf32> to memref<?xf32, strided<[1]>>\n+// CHECK:           memref.copy [[VAR_subview_]], [[VAR_subview_]]_0 : memref<?xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1]>>\n+// CHECK-DAG:       [[VAR_7_:%.+]] = bufferization.to_tensor [[RES_]] restrict writable : memref<1024xf32>\n+// CHECK-DAG:       [[VAR_8_:%.+]] = arith.index_cast [[VAR_0_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_reinterpret_cast_1_:%.+]] = memref.reinterpret_cast [[PARAM_1_]] to offset: {{.}}[[VAR_8_]]{{.}}, sizes: [1024], strides: [1]{{.*}} : memref<*xf32> to memref<1024xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:       [[RES_1_:%.+]] = memref.alloc() : memref<1024xf32>\n+// CHECK-DAG:       [[VAR_9_:%.+]] = arith.index_cast [[VAR_0_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_10_:%.+]] = arith.addi [[VAR_9_]], [[CST_1024_]] : index\n+// CHECK-DAG:       [[VAR_11_:%.+]] = arith.index_cast [[PARAM_3_]] : i32 to index\n+// CHECK:           [[VAR_12_:%.+]] = arith.minsi [[VAR_10_]], [[VAR_11_]] : index\n+// CHECK:           [[VAR_13_:%.+]] = arith.subi [[VAR_12_]], [[VAR_9_]] : index\n+// CHECK-DAG:       [[VAR_subview_3_:%.+]] = memref.subview [[VAR_reinterpret_cast_1_]][0] {{.}}[[VAR_13_]]{{.}} [1]{{.*}} : memref<1024xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:       [[VAR_subview_4_:%.+]] = memref.subview [[RES_1_]][0] {{.}}[[VAR_13_]]{{.}} [1] : memref<1024xf32> to memref<?xf32, strided<[1]>>\n+// CHECK:           memref.copy [[VAR_subview_3_]], [[VAR_subview_4_]] : memref<?xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1]>>\n+// CHECK:           [[VAR_14_:%.+]] = bufferization.to_tensor [[RES_1_]] restrict writable : memref<1024xf32>\n+// CHECK:           [[VAR_15_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins([[VAR_7_]], [[VAR_14_]] : tensor<1024xf32>, tensor<1024xf32>) outs([[VAR_7_]] : tensor<1024xf32>) {\n+// CHECK:           ^bb0([[in_1:%.+]]: f32, [[in_2:%.+]]: f32, [[out:%.+]]: f32):\n+// CHECK:             [[VAR_22_:%.+]] = arith.addf [[in_1]], [[in_2]] : f32\n+// CHECK:             linalg.yield [[VAR_22_]] : f32\n+// CHECK:           } -> tensor<1024xf32>\n+// CHECK:           [[VAR_16_:%.+]] = arith.index_cast [[VAR_0_]] : i32 to index\n+// CHECK-DAG:       [[VAR_reinterpret_cast_5_:%.+]] = memref.reinterpret_cast [[PARAM_2_]] to offset: {{.}}[[VAR_16_]]{{.}}, sizes: [1024], strides: [1]{{.*}} : memref<*xf32> to memref<1024xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:       [[VAR_17_:%.+]] = arith.index_cast [[VAR_0_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_18_:%.+]] = arith.addi [[VAR_17_]], [[CST_1024_]] : index\n+// CHECK-DAG:       [[VAR_19_:%.+]] = arith.index_cast [[PARAM_3_]] : i32 to index\n+// CHECK:           [[VAR_20_:%.+]] = arith.minsi [[VAR_18_]], [[VAR_19_]] : index\n+// CHECK:           [[VAR_21_:%.+]] = arith.subi [[VAR_20_]], [[VAR_17_]] : index\n+// CHECK-DAG:       [[VAR_extracted_slice_:%.+]] = tensor.extract_slice [[VAR_15_]][0] {{.}}[[VAR_21_]]{{.}} [1] : tensor<1024xf32> to tensor<?xf32>\n+// CHECK-DAG:       [[VAR_subview_6_:%.+]] = memref.subview [[VAR_reinterpret_cast_5_]][0] {{.}}[[VAR_21_]]{{.}} [1]{{.*}} : memref<1024xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1], offset: ?>>\n+// CHECK:           memref.tensor_store [[VAR_extracted_slice_]], [[VAR_subview_6_]] : memref<?xf32, strided<[1], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/kernel-02-fused-softmax.mlir", "status": "added", "additions": 105, "deletions": 0, "changes": 105, "file_content_changes": "@@ -0,0 +1,105 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+\n+module {\n+  tt.func public @softmax_kernel_012345(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32) {\n+    %cst = arith.constant 0xFF800000 : f32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %arg2 : i32\n+    %2 = tt.addptr %arg1, %1 : !tt.ptr<f32>, i32\n+    %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+    %4 = tt.splat %2 : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n+    %5 = tt.addptr %4, %3 : tensor<128x!tt.ptr<f32>>, tensor<128xi32>\n+    %6 = tt.splat %arg4 : (i32) -> tensor<128xi32>\n+    %7 = arith.cmpi slt, %3, %6 : tensor<128xi32>\n+    %8 = tt.splat %cst : (f32) -> tensor<128xf32>\n+    %9 = tt.load %5, %7, %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128xf32>\n+    %10 = \"tt.reduce\"(%9) ({\n+    ^bb0(%arg5: f32, %arg6: f32):\n+      %21 = arith.cmpf ogt, %arg5, %arg6 : f32\n+      %22 = arith.select %21, %arg5, %arg6 : f32\n+      tt.reduce.return %22 : f32\n+    }) {axis = 0 : i32} : (tensor<128xf32>) -> f32\n+    %11 = tt.splat %10 : (f32) -> tensor<128xf32>\n+    %12 = arith.subf %9, %11 : tensor<128xf32>\n+    %13 = math.exp %12 : tensor<128xf32>\n+    %14 = \"tt.reduce\"(%13) ({\n+    ^bb0(%arg5: f32, %arg6: f32):\n+      %21 = arith.addf %arg5, %arg6 : f32\n+      tt.reduce.return %21 : f32\n+    }) {axis = 0 : i32} : (tensor<128xf32>) -> f32\n+    %15 = tt.splat %14 : (f32) -> tensor<128xf32>\n+    %16 = arith.divf %13, %15 : tensor<128xf32>\n+    %17 = arith.muli %0, %arg3 : i32\n+    %18 = tt.addptr %arg0, %17 : !tt.ptr<f32>, i32\n+    %19 = tt.splat %18 : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n+    %20 = tt.addptr %19, %3 : tensor<128x!tt.ptr<f32>>, tensor<128xi32>\n+    tt.store %20, %16, %7 {cache = 1 : i32, evict = 1 : i32} : tensor<128xf32>\n+    tt.return\n+  }\n+}\n+\n+// CHECK-DAG:   [[MAP_0_:#.+]] = affine_map<(d0) -> (d0)>\n+// CHECK-LABEL:  func.func @softmax_kernel_012345\n+// CHECK-SAME:   ([[PARAM_0_:%.+]]: memref<*xf32>, [[PARAM_1_:%.+]]: memref<*xf32>, [[PARAM_2_:%.+]]: i32, [[PARAM_3_:%.+]]: i32, [[PARAM_4_:%.+]]: i32, [[PARAM_5_:%.+]]: i32, [[PARAM_6_:%.+]]: i32, [[PARAM_7_:%.+]]: i32) {\n+// CHECK-DAG:       [[CST_0_dot_000000_:%.+]] = arith.constant 0.000000e+00 : f32\n+// CHECK-DAG:       [[CST_128_:%.+]] = arith.constant 128 : index\n+// CHECK-DAG:       [[CST_0_:%.+]] = arith.constant 0xFF800000 : f32\n+// CHECK-DAG:       [[VAR_0_:%.+]] = arith.muli [[PARAM_5_]], [[PARAM_2_]] : i32\n+// CHECK:           [[VAR_1_:%.+]] = arith.index_cast [[VAR_0_]] : i32 to index\n+// CHECK-DAG:       [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_1_]] to offset: {{.}}[[VAR_1_]]{{.}}, sizes: [128], strides: [1]{{.*}} : memref<*xf32> to memref<128xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:       [[RES_:%.+]] = memref.alloc() : memref<128xf32>\n+// CHECK-DAG:       [[VAR_2_:%.+]] = arith.index_cast [[PARAM_4_]] : i32 to index\n+// CHECK:           [[VAR_3_:%.+]] = arith.minsi [[VAR_2_]], [[CST_128_]] : index\n+// CHECK-DAG:       [[VAR_subview_:%.+]] = memref.subview [[VAR_reinterpret_cast_]][0] {{.}}[[VAR_3_]]{{.}} [1]{{.*}} : memref<128xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:       [[VAR_subview_1_:%.+]] = memref.subview [[RES_]][0] {{.}}[[VAR_3_]]{{.}} [1] : memref<128xf32> to memref<?xf32, strided<[1]>>\n+// CHECK-DAG:       [[VAR_4_:%.+]] = arith.cmpi slt, [[VAR_3_]], [[CST_128_]] : index\n+// CHECK:           scf.if [[VAR_4_]] {\n+// CHECK:             linalg.fill ins([[CST_0_]] : f32) outs([[RES_]] : memref<128xf32>)\n+// CHECK:           }\n+// CHECK:           memref.copy [[VAR_subview_]], [[VAR_subview_]]_1 : memref<?xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1]>>\n+// CHECK-DAG:       [[VAR_5_:%.+]] = bufferization.to_tensor [[RES_]] restrict writable : memref<128xf32>\n+// CHECK-DAG:       [[VAR_6_:%.+]] = bufferization.alloc_tensor() : tensor<f32>\n+// CHECK:           [[VAR_inserted_:%.+]] = tensor.insert [[CST_0_]] into [[VAR_6_]][] : tensor<f32>\n+// CHECK:           [[VAR_reduced_:%.+]] = linalg.reduce ins([[VAR_5_]] : tensor<128xf32>) outs([[VAR_inserted_]] : tensor<f32>) dimensions = [0]\n+// CHECK:             ([[in_1:%.+]]: f32, [[init_1:%.+]]: f32) {\n+// CHECK:               [[VAR_19_:%.+]] = arith.maxf [[in_1]], [[init_1]] : f32\n+// CHECK:               linalg.yield [[VAR_19_]] : f32\n+// CHECK:             }\n+// CHECK-DAG:       [[VAR_extracted_:%.+]] = tensor.extract [[VAR_reduced_]][] : tensor<f32>\n+// CHECK-DAG:       [[VAR_7_:%.+]] = tensor.empty() : tensor<128xf32>\n+// CHECK:           [[VAR_8_:%.+]] = linalg.fill ins([[VAR_extracted_]] : f32) outs([[VAR_7_]] : tensor<128xf32>) -> tensor<128xf32>\n+// CHECK:           [[VAR_9_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins([[VAR_5_]], [[VAR_8_]] : tensor<128xf32>, tensor<128xf32>) outs([[VAR_5_]] : tensor<128xf32>) {\n+// CHECK:           ^bb0([[in_1:%.+]]: f32, [[in_2:%.+]]: f32, [[out:%.+]]: f32):\n+// CHECK:             [[VAR_19_1_:%.+]] = arith.subf [[in_1]], [[in_2]] : f32\n+// CHECK:             linalg.yield [[VAR_19_1_]] : f32\n+// CHECK:           } -> tensor<128xf32>\n+// CHECK:           [[VAR_10_:%.+]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\"]} ins([[VAR_9_]] : tensor<128xf32>) outs([[VAR_9_]] : tensor<128xf32>) {\n+// CHECK:           ^bb0([[in_1:%.+]]: f32, [[out_1:%.+]]: f32):\n+// CHECK:             [[VAR_19_2_:%.+]] = math.exp [[in_1]] : f32\n+// CHECK:             linalg.yield [[VAR_19_2_]] : f32\n+// CHECK:           } -> tensor<128xf32>\n+// CHECK:           [[VAR_11_:%.+]] = bufferization.alloc_tensor() : tensor<f32>\n+// CHECK:           [[VAR_inserted_2_:%.+]] = tensor.insert [[CST_0_dot_000000_]] into [[VAR_11_]][] : tensor<f32>\n+// CHECK:           [[VAR_reduced_3_:%.+]] = linalg.reduce ins([[VAR_10_]] : tensor<128xf32>) outs([[VAR_inserted_2_]] : tensor<f32>) dimensions = [0]\n+// CHECK:             ([[in_1:%.+]]: f32, [[init_1:%.+]]: f32) {\n+// CHECK:               [[VAR_19_3_:%.+]] = arith.addf [[in_1]], [[init_1]] : f32\n+// CHECK:               linalg.yield [[VAR_19_3_]] : f32\n+// CHECK:             }\n+// CHECK-DAG:       [[VAR_extracted_4_:%.+]] = tensor.extract [[VAR_reduced_3_]][] : tensor<f32>\n+// CHECK-DAG:       [[VAR_12_:%.+]] = tensor.empty() : tensor<128xf32>\n+// CHECK:           [[VAR_13_:%.+]] = linalg.fill ins([[VAR_extracted_4_]] : f32) outs([[VAR_12_]] : tensor<128xf32>) -> tensor<128xf32>\n+// CHECK:           [[VAR_14_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins([[VAR_10_]], [[VAR_13_]] : tensor<128xf32>, tensor<128xf32>) outs([[VAR_10_]] : tensor<128xf32>) {\n+// CHECK:           ^bb0([[in_1:%.+]]: f32, [[in_2:%.+]]: f32, [[out_1:%.+]]: f32):\n+// CHECK:             [[VAR_19_4_:%.+]] = arith.divf [[in_1]], [[in_2]] : f32\n+// CHECK:             linalg.yield [[VAR_19_4_]] : f32\n+// CHECK:           } -> tensor<128xf32>\n+// CHECK:           [[VAR_15_:%.+]] = arith.muli [[PARAM_5_]], [[PARAM_3_]] : i32\n+// CHECK:           [[VAR_16_:%.+]] = arith.index_cast [[VAR_15_]] : i32 to index\n+// CHECK-DAG:       [[VAR_reinterpret_cast_5_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[VAR_16_]]{{.}}, sizes: [128], strides: [1]{{.*}} : memref<*xf32> to memref<128xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:       [[VAR_17_:%.+]] = arith.index_cast [[PARAM_4_]] : i32 to index\n+// CHECK:           [[VAR_18_:%.+]] = arith.minsi [[VAR_17_]], [[CST_128_]] : index\n+// CHECK-DAG:       [[VAR_extracted_slice_:%.+]] = tensor.extract_slice [[VAR_14_]][0] {{.}}[[VAR_18_]]{{.}} [1] : tensor<128xf32> to tensor<?xf32>\n+// CHECK-DAG:       [[VAR_subview_6_:%.+]] = memref.subview [[VAR_reinterpret_cast_5_]][0] {{.}}[[VAR_18_]]{{.}} [1]{{.*}} : memref<128xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1], offset: ?>>\n+// CHECK:           memref.tensor_store [[VAR_extracted_slice_]], [[VAR_subview_6_]] : memref<?xf32, strided<[1], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/kernel-03-matrix-multiplication.mlir", "status": "added", "additions": 217, "deletions": 0, "changes": 217, "file_content_changes": "@@ -0,0 +1,217 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+\n+module {\n+  tt.func public @matmul_kernel_0123456789101112131415(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: !tt.ptr<bf16>, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32) {\n+    %c63_i32 = arith.constant 63 : i32\n+    %c255_i32 = arith.constant 255 : i32\n+    %c127_i32 = arith.constant 127 : i32\n+    %c1_i32 = arith.constant 1 : i32\n+    %c0_i32 = arith.constant 0 : i32\n+    %c64_i32 = arith.constant 64 : i32\n+    %cst = arith.constant 0.000000e+00 : f32\n+    %c256_i32 = arith.constant 256 : i32\n+    %c128_i32 = arith.constant 128 : i32\n+    %c8_i32 = arith.constant 8 : i32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.addi %arg3, %c127_i32 : i32\n+    %2 = arith.divsi %1, %c128_i32 : i32\n+    %3 = arith.addi %arg4, %c255_i32 : i32\n+    %4 = arith.divsi %3, %c256_i32 : i32\n+    %5 = arith.addi %arg5, %c63_i32 : i32\n+    %6 = arith.divsi %5, %c64_i32 : i32\n+    %7 = arith.muli %4, %c8_i32 : i32\n+    %8 = arith.divsi %0, %7 : i32\n+    %9 = arith.muli %8, %c8_i32 : i32\n+    %10 = arith.subi %2, %9 : i32\n+    %11 = arith.cmpi slt, %10, %c8_i32 : i32\n+    %12 = arith.select %11, %10, %c8_i32 : i32\n+    %13 = arith.remsi %0, %12 : i32\n+    %14 = arith.addi %9, %13 : i32\n+    %15 = arith.remsi %0, %7 : i32\n+    %16 = arith.divsi %15, %12 : i32\n+    %17 = arith.muli %14, %c128_i32 : i32\n+    %18 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+    %19 = tt.splat %17 : (i32) -> tensor<128xi32>\n+    %20 = arith.addi %19, %18 : tensor<128xi32>\n+    %21 = arith.muli %16, %c256_i32 : i32\n+    %22 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32>\n+    %23 = tt.splat %21 : (i32) -> tensor<256xi32>\n+    %24 = arith.addi %23, %22 : tensor<256xi32>\n+    %25 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %26 = tt.expand_dims %20 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n+    %27 = tt.splat %arg6 : (i32) -> tensor<128x1xi32>\n+    %28 = arith.muli %26, %27 : tensor<128x1xi32>\n+    %29 = tt.expand_dims %25 {axis = 0 : i32} : (tensor<64xi32>) -> tensor<1x64xi32>\n+    %30 = tt.splat %arg7 : (i32) -> tensor<1x64xi32>\n+    %31 = arith.muli %29, %30 : tensor<1x64xi32>\n+    %32 = tt.broadcast %28 : (tensor<128x1xi32>) -> tensor<128x64xi32>\n+    %33 = tt.broadcast %31 : (tensor<1x64xi32>) -> tensor<128x64xi32>\n+    %34 = arith.addi %32, %33 : tensor<128x64xi32>\n+    %35 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<128x64x!tt.ptr<bf16>>\n+    %36 = tt.addptr %35, %34 : tensor<128x64x!tt.ptr<bf16>>, tensor<128x64xi32>\n+    %37 = tt.expand_dims %25 {axis = 1 : i32} : (tensor<64xi32>) -> tensor<64x1xi32>\n+    %38 = tt.splat %arg8 : (i32) -> tensor<64x1xi32>\n+    %39 = arith.muli %37, %38 : tensor<64x1xi32>\n+    %40 = tt.expand_dims %24 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+    %41 = tt.splat %arg9 : (i32) -> tensor<1x256xi32>\n+    %42 = arith.muli %40, %41 : tensor<1x256xi32>\n+    %43 = tt.broadcast %39 : (tensor<64x1xi32>) -> tensor<64x256xi32>\n+    %44 = tt.broadcast %42 : (tensor<1x256xi32>) -> tensor<64x256xi32>\n+    %45 = arith.addi %43, %44 : tensor<64x256xi32>\n+    %46 = tt.splat %arg1 : (!tt.ptr<bf16>) -> tensor<64x256x!tt.ptr<bf16>>\n+    %47 = tt.addptr %46, %45 : tensor<64x256x!tt.ptr<bf16>>, tensor<64x256xi32>\n+    %48 = tt.splat %cst : (f32) -> tensor<128x256xf32>\n+    %49 = arith.muli %arg7, %c64_i32 : i32\n+    %50 = tt.splat %49 : (i32) -> tensor<128x64xi32>\n+    %51 = arith.muli %arg8, %c64_i32 : i32\n+    %52 = tt.splat %51 : (i32) -> tensor<64x256xi32>\n+    %53:3 = scf.for %arg12 = %c0_i32 to %6 step %c1_i32 iter_args(%arg13 = %48, %arg14 = %36, %arg15 = %47) -> (tensor<128x256xf32>, tensor<128x64x!tt.ptr<bf16>>, tensor<64x256x!tt.ptr<bf16>>)  : i32 {\n+      %71 = tt.load %arg14 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x64xbf16>\n+      %72 = tt.load %arg15 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x256xbf16>\n+      %73 = tt.dot %71, %72, %48 {allowTF32 = true} : tensor<128x64xbf16> * tensor<64x256xbf16> -> tensor<128x256xf32>\n+      %74 = arith.addf %arg13, %73 : tensor<128x256xf32>\n+      %75 = tt.addptr %arg14, %50 : tensor<128x64x!tt.ptr<bf16>>, tensor<128x64xi32>\n+      %76 = tt.addptr %arg15, %52 : tensor<64x256x!tt.ptr<bf16>>, tensor<64x256xi32>\n+      scf.yield %74, %75, %76 : tensor<128x256xf32>, tensor<128x64x!tt.ptr<bf16>>, tensor<64x256x!tt.ptr<bf16>>\n+    }\n+    %54 = arith.truncf %53#0 : tensor<128x256xf32> to tensor<128x256xbf16>\n+    %55 = tt.splat %arg10 : (i32) -> tensor<128x1xi32>\n+    %56 = arith.muli %55, %26 : tensor<128x1xi32>\n+    %57 = tt.splat %arg2 : (!tt.ptr<bf16>) -> tensor<128x1x!tt.ptr<bf16>>\n+    %58 = tt.addptr %57, %56 : tensor<128x1x!tt.ptr<bf16>>, tensor<128x1xi32>\n+    %59 = tt.splat %arg11 : (i32) -> tensor<1x256xi32>\n+    %60 = arith.muli %59, %40 : tensor<1x256xi32>\n+    %61 = tt.broadcast %58 : (tensor<128x1x!tt.ptr<bf16>>) -> tensor<128x256x!tt.ptr<bf16>>\n+    %62 = tt.broadcast %60 : (tensor<1x256xi32>) -> tensor<128x256xi32>\n+    %63 = tt.addptr %61, %62 : tensor<128x256x!tt.ptr<bf16>>, tensor<128x256xi32>\n+    %64 = tt.splat %arg3 : (i32) -> tensor<128x1xi32>\n+    %65 = arith.cmpi slt, %26, %64 : tensor<128x1xi32>\n+    %66 = tt.splat %arg4 : (i32) -> tensor<1x256xi32>\n+    %67 = arith.cmpi slt, %40, %66 : tensor<1x256xi32>\n+    %68 = tt.broadcast %65 : (tensor<128x1xi1>) -> tensor<128x256xi1>\n+    %69 = tt.broadcast %67 : (tensor<1x256xi1>) -> tensor<128x256xi1>\n+    %70 = arith.andi %68, %69 : tensor<128x256xi1>\n+    tt.store %63, %54, %70 {cache = 1 : i32, evict = 1 : i32} : tensor<128x256xbf16>\n+    tt.return\n+  }\n+}\n+\n+// CHECK-DAG:   [[MAP_0_:#.+]] = affine_map<(d0, d1) -> (d0, d1)>\n+// CHECK-LABEL:  func.func @matmul_kernel_0123456789101112131415\n+// CHECK-SAME:   ([[PARAM_0_:%.+]]: memref<*xbf16>, [[PARAM_1_:%.+]]: memref<*xbf16>, [[PARAM_2_:%.+]]: memref<*xbf16>, [[PARAM_3_:%.+]]: i32, [[PARAM_4_:%.+]]: i32, [[PARAM_5_:%.+]]: i32, [[PARAM_6_:%.+]]: i32, [[PARAM_7_:%.+]]: i32, [[PARAM_8_:%.+]]: i32, [[PARAM_9_:%.+]]: i32, [[PARAM_10_:%.+]]: i32, [[PARAM_11_:%.+]]: i32, [[PARAM_12_:%.+]]: i32, [[PARAM_13_:%.+]]: i32, [[PARAM_14_:%.+]]: i32) {\n+// CHECK-DAG:       [[CST_256_:%.+]] = arith.constant 256 : index\n+// CHECK-DAG:       [[CST_128_:%.+]] = arith.constant 128 : index\n+// CHECK-DAG:       [[CST_0_:%.+]] = arith.constant 0 : index\n+// CHECK-DAG:       [[CST_8_:%.+]] = arith.constant 8 : i32\n+// CHECK-DAG:       [[CST_128_1_:%.+]] = arith.constant 128 : i32\n+// CHECK-DAG:       [[CST_256_1_:%.+]] = arith.constant 256 : i32\n+// CHECK-DAG:       [[CST_64_:%.+]] = arith.constant 64 : i32\n+// CHECK-DAG:       [[CST_0_1_:%.+]] = arith.constant 0 : i32\n+// CHECK-DAG:       [[CST_1_:%.+]] = arith.constant 1 : i32\n+// CHECK-DAG:       [[CST_127_:%.+]] = arith.constant 127 : i32\n+// CHECK-DAG:       [[CST_255_:%.+]] = arith.constant 255 : i32\n+// CHECK-DAG:       [[CST_63_:%.+]] = arith.constant 63 : i32\n+// CHECK-DAG:       [[CST_0_dot_000000_:%.+]] = arith.constant 0.000000e+00 : f32\n+// CHECK-DAG:       [[VAR_0_:%.+]] = tensor.empty() : tensor<128x256xf32>\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_1_:%.+]] = linalg.fill ins([[CST_0_dot_000000_]] : f32) outs([[VAR_0_]] : tensor<128x256xf32>) -> tensor<128x256xf32>\n+// CHECK-DAG:       [[VAR_2_:%.+]] = arith.addi [[PARAM_3_]], [[CST_127_]] : i32\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_3_:%.+]] = arith.divsi [[VAR_2_]], [[CST_128_1_]] : i32\n+// CHECK-DAG:       [[VAR_4_:%.+]] = arith.addi [[PARAM_4_]], [[CST_255_]] : i32\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_5_:%.+]] = arith.divsi [[VAR_4_]], [[CST_256_1_]] : i32\n+// CHECK-DAG:       [[VAR_6_:%.+]] = arith.addi [[PARAM_5_]], [[CST_63_]] : i32\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_7_:%.+]] = arith.divsi [[VAR_6_]], [[CST_64_]] : i32\n+// CHECK-DAG:       [[VAR_8_:%.+]] = arith.muli [[VAR_5_]], [[CST_8_]] : i32\n+// CHECK:           [[VAR_9_:%.+]] = arith.divsi [[PARAM_12_]], [[VAR_8_]] : i32\n+// CHECK:           [[VAR_10_:%.+]] = arith.muli [[VAR_9_]], [[CST_8_]] : i32\n+// CHECK:           [[VAR_11_:%.+]] = arith.subi [[VAR_3_]], [[VAR_10_]] : i32\n+// CHECK:           [[VAR_12_:%.+]] = arith.cmpi slt, [[VAR_11_]], [[CST_8_]] : i32\n+// CHECK:           [[VAR_13_:%.+]] = arith.select [[VAR_12_]], [[VAR_11_]], [[CST_8_]] : i32\n+// CHECK:           [[VAR_14_:%.+]] = arith.remsi [[PARAM_12_]], [[VAR_13_]] : i32\n+// CHECK-DAG:       [[VAR_15_:%.+]] = arith.addi [[VAR_10_]], [[VAR_14_]] : i32\n+// CHECK-DAG:       [[VAR_16_:%.+]] = arith.remsi [[PARAM_12_]], [[VAR_8_]] : i32\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_17_:%.+]] = arith.divsi [[VAR_16_]], [[VAR_13_]] : i32\n+// CHECK-DAG:       [[VAR_18_:%.+]] = arith.muli [[VAR_15_]], [[CST_128_1_]] : i32\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_19_:%.+]] = arith.muli [[VAR_17_]], [[CST_256_1_]] : i32\n+// CHECK-DAG:       [[VAR_20_:%.+]] = arith.index_cast [[VAR_18_]] : i32 to index\n+// CHECK-DAG:       [[VAR_21_:%.+]] = arith.index_cast [[PARAM_6_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_22_:%.+]] = arith.muli [[VAR_20_]], [[VAR_21_]] : index\n+// CHECK-DAG:       [[VAR_23_:%.+]] = arith.index_cast [[PARAM_7_]] : i32 to index\n+// CHECK-DAG:       [[VAR_24_:%.+]] = arith.index_cast [[PARAM_8_]] : i32 to index\n+// CHECK-DAG:       [[VAR_25_:%.+]] = arith.index_cast [[VAR_19_]] : i32 to index\n+// CHECK-DAG:       [[VAR_26_:%.+]] = arith.index_cast [[PARAM_9_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_27_:%.+]] = arith.muli [[VAR_25_]], [[VAR_26_]] : index\n+// CHECK-DAG:       [[VAR_28_:%.+]] = arith.muli [[PARAM_7_]], [[CST_64_]] : i32\n+// CHECK-DAG:       [[VAR_29_:%.+]] = arith.muli [[PARAM_8_]], [[CST_64_]] : i32\n+// CHECK-DAG:       [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[VAR_22_]]{{.}}, sizes: [128, 64], strides: {{.}}[[VAR_21_]], [[VAR_23_]]{{.}} : memref<*xbf16> to memref<128x64xbf16, strided<[?, ?], offset: ?>>\n+// CHECK:           [[VAR_reinterpret_cast_0_:%.+]] = memref.reinterpret_cast [[PARAM_1_]] to offset: {{.}}[[VAR_27_]]{{.}}, sizes: [64, 256], strides: {{.}}[[VAR_24_]], [[VAR_26_]]{{.}} : memref<*xbf16> to memref<64x256xbf16, strided<[?, ?], offset: ?>>\n+// CHECK-DAG:       [[VAR_30_:%.+]]:7 = scf.for [[VAR_arg15_:%.+]] = [[CST_0_1_]] to [[VAR_7_]] step [[CST_1_]] iter_args([[VAR_arg16_:%.+]] = [[VAR_1_]], [[VAR_arg17_:%.+]] = [[VAR_reinterpret_cast_]], [[VAR_arg18_:%.+]] = [[VAR_reinterpret_cast_]]_0, [[VAR_arg19_:%.+]] = [[VAR_22_]], [[VAR_arg20_:%.+]] = [[CST_0_]], [[VAR_arg21_:%.+]] = [[VAR_27_]], [[VAR_arg22_:%.+]] = [[CST_0_]]) -> (tensor<128x256xf32>, memref<128x64xbf16, strided<[?, ?], offset: ?>>, memref<64x256xbf16, strided<[?, ?], offset: ?>>, index, index, index, index)  : i32 {\n+// CHECK-DAG:         [[RES_:%.+]] = memref.alloc() : memref<128x64xbf16>\n+// CHECK:             memref.copy [[VAR_arg17_]], [[RES_]] : memref<128x64xbf16, strided<[?, ?], offset: ?>> to memref<128x64xbf16>\n+// CHECK-DAG:         [[VAR_52_:%.+]] = bufferization.to_tensor [[RES_]] restrict writable : memref<128x64xbf16>\n+// CHECK-DAG:         [[RES_1_:%.+]] = memref.alloc() : memref<64x256xbf16>\n+// CHECK:             memref.copy [[VAR_arg18_]], [[RES_1_]] : memref<64x256xbf16, strided<[?, ?], offset: ?>> to memref<64x256xbf16>\n+// CHECK-DAG:         [[VAR_53_:%.+]] = bufferization.to_tensor [[RES_1_]] restrict writable : memref<64x256xbf16>\n+// CHECK-DAG:         [[VAR_54_:%.+]] = tensor.empty() : tensor<128x256xf32>\n+// CHECK:             [[VAR_55_:%.+]] = linalg.matmul ins([[VAR_52_]], [[VAR_53_]] : tensor<128x64xbf16>, tensor<64x256xbf16>) outs([[VAR_54_]] : tensor<128x256xf32>) -> tensor<128x256xf32>\n+// CHECK:             [[VAR_56_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins([[VAR_55_]], [[VAR_1_]] : tensor<128x256xf32>, tensor<128x256xf32>) outs([[VAR_55_]] : tensor<128x256xf32>) {\n+// CHECK:             ^bb0([[in_0:.+]]: f32, [[in_1:.+]]: f32, [[out:.+]]: f32):\n+// CHECK:               [[VAR_64_:%.+]] = arith.addf [[in_0]], [[in_1]] : f32\n+// CHECK:               linalg.yield [[VAR_64_]] : f32\n+// CHECK:             } -> tensor<128x256xf32>\n+// CHECK:             [[VAR_57_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins([[VAR_arg16_]], [[VAR_56_]] : tensor<128x256xf32>, tensor<128x256xf32>) outs([[VAR_arg16_]] : tensor<128x256xf32>) {\n+// CHECK:             ^bb0([[in_0:.+]]: f32, [[in_1:.+]]: f32, [[out:.+]]: f32):\n+// CHECK:               [[VAR_64_1_:%.+]] = arith.addf [[in_0]], [[in_1]] : f32\n+// CHECK:               linalg.yield [[VAR_64_1_]] : f32\n+// CHECK:             } -> tensor<128x256xf32>\n+// CHECK:             [[VAR_58_:%.+]] = arith.index_cast [[VAR_28_]] : i32 to index\n+// CHECK:             [[VAR_59_:%.+]] = arith.addi [[VAR_arg19_]], [[VAR_58_]] : index\n+// CHECK:             [[VAR_60_:%.+]] = arith.addi [[VAR_59_]], [[VAR_arg20_]] : index\n+// CHECK-DAG:         [[VAR_reinterpret_cast_3_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[VAR_60_]]{{.}}, sizes: [128, 64], strides: {{.}}[[VAR_21_]], [[VAR_23_]]{{.}} : memref<*xbf16> to memref<128x64xbf16, strided<[?, ?], offset: ?>>\n+// CHECK-DAG:         [[VAR_61_:%.+]] = arith.index_cast [[VAR_29_]] : i32 to index\n+// CHECK:             [[VAR_62_:%.+]] = arith.addi [[VAR_arg21_]], [[VAR_61_]] : index\n+// CHECK:             [[VAR_63_:%.+]] = arith.addi [[VAR_62_]], [[VAR_arg22_]] : index\n+// CHECK:             [[VAR_reinterpret_cast_4_:%.+]] = memref.reinterpret_cast [[PARAM_1_]] to offset: {{.}}[[VAR_63_]]{{.}}, sizes: [64, 256], strides: {{.}}[[VAR_24_]], [[VAR_26_]]{{.}} : memref<*xbf16> to memref<64x256xbf16, strided<[?, ?], offset: ?>>\n+// CHECK:             scf.yield [[VAR_57_]], [[VAR_reinterpret_cast_3_]], [[VAR_reinterpret_cast_4_]], [[VAR_60_]], [[CST_0_]], [[VAR_63_]], [[CST_0_]] : tensor<128x256xf32>, memref<128x64xbf16, strided<[?, ?], offset: ?>>, memref<64x256xbf16, strided<[?, ?], offset: ?>>, index, index, index, index\n+// CHECK:           }\n+// CHECK:           [[VAR_31_:%.+]] = tensor.empty() : tensor<128x256xbf16>\n+// CHECK:           [[VAR_32_:%.+]] = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins([[VAR_30_]]#0 : tensor<128x256xf32>) outs([[VAR_31_]] : tensor<128x256xbf16>) {\n+// CHECK:           ^bb0([[in:.+]]: f32, [[out:.+]]: bf16):\n+// CHECK:             [[VAR_52_1_:%.+]] = arith.truncf [[in]] : f32 to bf16\n+// CHECK:             linalg.yield [[VAR_52_1_]] : bf16\n+// CHECK:           } -> tensor<128x256xbf16>\n+// CHECK-DAG:       [[VAR_33_:%.+]] = arith.index_cast [[PARAM_10_]] : i32 to index\n+// CHECK-DAG:       [[VAR_34_:%.+]] = arith.index_cast [[VAR_18_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_35_:%.+]] = arith.muli [[VAR_34_]], [[VAR_33_]] : index\n+// CHECK-DAG:       [[VAR_36_:%.+]] = arith.index_cast [[PARAM_11_]] : i32 to index\n+// CHECK-DAG:       [[VAR_37_:%.+]] = arith.index_cast [[VAR_19_]] : i32 to index\n+// CHECK:           [[VAR_38_:%.+]] = arith.muli [[VAR_37_]], [[VAR_36_]] : index\n+// CHECK:           [[VAR_39_:%.+]] = arith.addi [[VAR_35_]], [[VAR_38_]] : index\n+// CHECK-DAG:       [[VAR_reinterpret_cast_1_:%.+]] = memref.reinterpret_cast [[PARAM_2_]] to offset: {{.}}[[VAR_39_]]{{.}}, sizes: [128, 256], strides: {{.}}[[VAR_33_]], [[VAR_36_]]{{.}} : memref<*xbf16> to memref<128x256xbf16, strided<[?, ?], offset: ?>>\n+// CHECK-DAG:       [[VAR_40_:%.+]] = arith.index_cast [[VAR_18_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_41_:%.+]] = arith.addi [[VAR_40_]], [[CST_128_]] : index\n+// CHECK-DAG:       [[VAR_42_:%.+]] = arith.index_cast [[PARAM_3_]] : i32 to index\n+// CHECK:           [[VAR_43_:%.+]] = arith.minsi [[VAR_41_]], [[VAR_42_]] : index\n+// CHECK-DAG:       [[VAR_44_:%.+]] = arith.subi [[VAR_43_]], [[VAR_40_]] : index\n+// CHECK-DAG:       [[VAR_45_:%.+]] = arith.index_cast [[VAR_19_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_46_:%.+]] = arith.addi [[VAR_45_]], [[CST_256_]] : index\n+// CHECK-DAG:       [[VAR_47_:%.+]] = arith.index_cast [[PARAM_4_]] : i32 to index\n+// CHECK:           [[VAR_48_:%.+]] = arith.minsi [[VAR_46_]], [[VAR_47_]] : index\n+// CHECK-DAG:       [[VAR_49_:%.+]] = arith.subi [[VAR_48_]], [[VAR_45_]] : index\n+// CHECK-DAG:       [[VAR_50_:%.+]] = arith.minsi [[VAR_44_]], [[CST_128_]] : index\n+// CHECK:           [[VAR_51_:%.+]] = arith.minsi [[VAR_49_]], [[CST_256_]] : index\n+// CHECK-DAG:       [[VAR_extracted_slice_:%.+]] = tensor.extract_slice [[VAR_32_]][0, 0] {{.}}[[VAR_50_]], [[VAR_51_]]{{.}} [1, 1] : tensor<128x256xbf16> to tensor<?x?xbf16>\n+// CHECK-DAG:       [[VAR_subview_:%.+]] = memref.subview [[VAR_reinterpret_cast_1_]][0, 0] {{.}}[[VAR_50_]], [[VAR_51_]]{{.}} [1, 1] : memref<128x256xbf16, strided<[?, ?], offset: ?>> to memref<?x?xbf16, strided<[?, ?], offset: ?>>\n+// CHECK:           memref.tensor_store [[VAR_extracted_slice_]], [[VAR_subview_]] : memref<?x?xbf16, strided<[?, ?], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/kernel-05-layer-norm-dwdb.mlir", "status": "added", "additions": 189, "deletions": 0, "changes": 189, "file_content_changes": "@@ -0,0 +1,189 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+\n+module {\n+  tt.func public @_layer_norm_bwd_dwdb_0123456(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: !tt.ptr<f32>, %arg4: i32, %arg5: i32) {\n+    %c0_i32 = arith.constant 0 : i32\n+    %c256_i32 = arith.constant 256 : i32\n+    %cst = arith.constant 0.000000e+00 : f32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %c256_i32 : i32\n+    %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32>\n+    %3 = tt.splat %1 : (i32) -> tensor<256xi32>\n+    %4 = arith.addi %3, %2 : tensor<256xi32>\n+    %5 = tt.splat %cst : (f32) -> tensor<256x256xf32>\n+    %6 = tt.splat %arg4 : (i32) -> tensor<256x1xi32>\n+    %7 = tt.expand_dims %4 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+    %8 = tt.splat %arg5 : (i32) -> tensor<1x256xi32>\n+    %9 = arith.cmpi slt, %7, %8 : tensor<1x256xi32>\n+    %10 = tt.broadcast %9 : (tensor<1x256xi1>) -> tensor<256x256xi1>\n+    %11 = tt.splat %arg5 : (i32) -> tensor<256x1xi32>\n+    %12 = tt.broadcast %7 : (tensor<1x256xi32>) -> tensor<256x256xi32>\n+    %13 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x256x!tt.ptr<f32>>\n+    %14 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x256x!tt.ptr<f32>>\n+    %15:2 = scf.for %arg6 = %c0_i32 to %arg4 step %c256_i32 iter_args(%arg7 = %5, %arg8 = %5) -> (tensor<256x256xf32>, tensor<256x256xf32>)  : i32 {\n+      %24 = tt.splat %arg6 : (i32) -> tensor<256xi32>\n+      %25 = arith.addi %24, %2 : tensor<256xi32>\n+      %26 = tt.expand_dims %25 {axis = 1 : i32} : (tensor<256xi32>) -> tensor<256x1xi32>\n+      %27 = arith.cmpi slt, %26, %6 : tensor<256x1xi32>\n+      %28 = tt.broadcast %27 : (tensor<256x1xi1>) -> tensor<256x256xi1>\n+      %29 = arith.andi %28, %10 : tensor<256x256xi1>\n+      %30 = arith.muli %26, %11 : tensor<256x1xi32>\n+      %31 = tt.broadcast %30 : (tensor<256x1xi32>) -> tensor<256x256xi32>\n+      %32 = arith.addi %31, %12 : tensor<256x256xi32>\n+      %33 = tt.addptr %13, %32 : tensor<256x256x!tt.ptr<f32>>, tensor<256x256xi32>\n+      %34 = tt.load %33, %29, %5 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x256xf32>\n+      %35 = arith.addf %arg7, %34 : tensor<256x256xf32>\n+      %36 = tt.addptr %14, %32 : tensor<256x256x!tt.ptr<f32>>, tensor<256x256xi32>\n+      %37 = tt.load %36, %29, %5 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x256xf32>\n+      %38 = arith.addf %arg8, %37 : tensor<256x256xf32>\n+      scf.yield %35, %38 : tensor<256x256xf32>, tensor<256x256xf32>\n+    }\n+    %16 = \"tt.reduce\"(%15#0) ({\n+    ^bb0(%arg6: f32, %arg7: f32):\n+      %24 = arith.addf %arg6, %arg7 : f32\n+      tt.reduce.return %24 : f32\n+    }) {axis = 0 : i32} : (tensor<256x256xf32>) -> tensor<256xf32>\n+    %17 = \"tt.reduce\"(%15#1) ({\n+    ^bb0(%arg6: f32, %arg7: f32):\n+      %24 = arith.addf %arg6, %arg7 : f32\n+      tt.reduce.return %24 : f32\n+    }) {axis = 0 : i32} : (tensor<256x256xf32>) -> tensor<256xf32>\n+    %18 = tt.splat %arg5 : (i32) -> tensor<256xi32>\n+    %19 = arith.cmpi slt, %4, %18 : tensor<256xi32>\n+    %20 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n+    %21 = tt.addptr %20, %4 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n+    tt.store %21, %16, %19 {cache = 1 : i32, evict = 1 : i32} : tensor<256xf32>\n+    %22 = tt.splat %arg3 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n+    %23 = tt.addptr %22, %4 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n+    tt.store %23, %17, %19 {cache = 1 : i32, evict = 1 : i32} : tensor<256xf32>\n+    tt.return\n+  }\n+}\n+\n+// CHECK-DAG:   [[MAP_0_:#.+]] = affine_map<(d0, d1) -> (d0, d1)>\n+// CHECK-LABEL:  func.func @_layer_norm_bwd_dwdb_0123456\n+// CHECK-SAME:   ([[PARAM_0_:%.+]]: memref<*xf32>, [[PARAM_1_:%.+]]: memref<*xf32>, [[PARAM_2_:%.+]]: memref<*xf32>, [[PARAM_3_:%.+]]: memref<*xf32>, [[PARAM_4_:%.+]]: i32, [[PARAM_5_:%.+]]: i32, [[PARAM_6_:%.+]]: i32, [[PARAM_7_:%.+]]: i32, [[PARAM_8_:%.+]]: i32) {\n+// CHECK-DAG:       [[CST_256_:%.+]] = arith.constant 256 : index\n+// CHECK-DAG:       [[CST_256_1_:%.+]] = arith.constant 256 : i32\n+// CHECK-DAG:       [[CST_0_:%.+]] = arith.constant 0 : i32\n+// CHECK-DAG:       [[CST_0_dot_000000_:%.+]] = arith.constant 0.000000e+00 : f32\n+// CHECK-DAG:       [[VAR_0_:%.+]] = tensor.empty() : tensor<256x256xf32>\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_1_:%.+]] = linalg.fill ins([[CST_0_dot_000000_]] : f32) outs([[VAR_0_]] : tensor<256x256xf32>) -> tensor<256x256xf32>\n+// CHECK-DAG:       [[VAR_2_:%.+]] = arith.muli [[PARAM_6_]], [[CST_256_1_]] : i32\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_3_:%.+]]:2 = scf.for [[VAR_arg9_:%.+]] = [[CST_0_]] to [[PARAM_4_]] step [[CST_256_1_]] iter_args([[VAR_arg10_:%.+]] = [[VAR_1_]], [[VAR_arg11_:%.+]] = [[VAR_1_]]) -> (tensor<256x256xf32>, tensor<256x256xf32>)  : i32 {\n+// CHECK-DAG:         [[VAR_20_:%.+]] = arith.index_cast [[VAR_arg9_]] : i32 to index\n+// CHECK-DAG:         [[VAR_21_:%.+]] = arith.index_cast [[PARAM_5_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:         [[VAR_22_:%.+]] = arith.muli [[VAR_20_]], [[VAR_21_]] : index\n+// CHECK-DAG:         [[VAR_23_:%.+]] = arith.index_cast [[VAR_2_]] : i32 to index\n+// CHECK:             [[VAR_24_:%.+]] = arith.addi [[VAR_22_]], [[VAR_23_]] : index\n+// CHECK-DAG:         [[VAR_reinterpret_cast_4_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[VAR_24_]]{{.}}, sizes: [256, 256], strides: {{.}}[[VAR_21_]], 1] : memref<*xf32> to memref<256x256xf32, strided<[?, 1], offset: ?>>\n+// CHECK-DAG:         [[RES_:%.+]] = memref.alloc() : memref<256x256xf32>\n+// CHECK-DAG:         [[VAR_25_:%.+]] = arith.index_cast [[VAR_arg9_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:         [[VAR_26_:%.+]] = arith.addi [[VAR_25_]], [[CST_256_]] : index\n+// CHECK-DAG:         [[VAR_27_:%.+]] = arith.index_cast [[PARAM_4_]] : i32 to index\n+// CHECK:             [[VAR_28_:%.+]] = arith.minsi [[VAR_26_]], [[VAR_27_]] : index\n+// CHECK-DAG:         [[VAR_29_:%.+]] = arith.subi [[VAR_28_]], [[VAR_25_]] : index\n+// CHECK-DAG:         [[VAR_30_:%.+]] = arith.index_cast [[VAR_2_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:         [[VAR_31_:%.+]] = arith.addi [[VAR_30_]], [[CST_256_]] : index\n+// CHECK-DAG:         [[VAR_32_:%.+]] = arith.index_cast [[PARAM_5_]] : i32 to index\n+// CHECK:             [[VAR_33_:%.+]] = arith.minsi [[VAR_31_]], [[VAR_32_]] : index\n+// CHECK-DAG:         [[VAR_34_:%.+]] = arith.subi [[VAR_33_]], [[VAR_30_]] : index\n+// CHECK-DAG:         [[VAR_35_:%.+]] = arith.minsi [[VAR_29_]], [[CST_256_]] : index\n+// CHECK:             [[VAR_36_:%.+]] = arith.minsi [[VAR_34_]], [[CST_256_]] : index\n+// CHECK-DAG:         [[VAR_subview_5_:%.+]] = memref.subview [[VAR_reinterpret_cast_4_]][0, 0] {{.}}[[VAR_35_]], [[VAR_36_]]{{.}} [1, 1] : memref<256x256xf32, strided<[?, 1], offset: ?>> to memref<?x?xf32, strided<[?, 1], offset: ?>>\n+// CHECK-DAG:         [[VAR_subview_6_:%.+]] = memref.subview [[RES_]][0, 0] {{.}}[[VAR_35_]], [[VAR_36_]]{{.}} [1, 1] : memref<256x256xf32> to memref<?x?xf32, strided<[256, 1]>>\n+// CHECK-DAG:         [[VAR_37_:%.+]] = arith.cmpi slt, [[VAR_35_]], [[CST_256_]] : index\n+// CHECK-DAG:         [[VAR_38_:%.+]] = arith.cmpi slt, [[VAR_36_]], [[CST_256_]] : index\n+// CHECK:             [[VAR_39_:%.+]] = arith.ori [[VAR_37_]], [[VAR_38_]] : i1\n+// CHECK:             scf.if [[VAR_39_]] {\n+// CHECK:               linalg.fill ins([[CST_0_dot_000000_]] : f32) outs([[RES_]] : memref<256x256xf32>)\n+// CHECK:             }\n+// CHECK:             memref.copy [[VAR_subview_5_]], [[VAR_subview_6_]] : memref<?x?xf32, strided<[?, 1], offset: ?>> to memref<?x?xf32, strided<[256, 1]>>\n+// CHECK:             [[VAR_40_:%.+]] = bufferization.to_tensor [[RES_]] restrict writable : memref<256x256xf32>\n+// CHECK:             [[VAR_41_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins([[VAR_arg10_]], [[VAR_40_]] : tensor<256x256xf32>, tensor<256x256xf32>) outs([[VAR_arg10_]] : tensor<256x256xf32>) {\n+// CHECK:             ^bb0([[in_0:.+]]: f32, [[in_1:.+]]: f32, [[out:.+]]: f32):\n+// CHECK:               [[VAR_64_:%.+]] = arith.addf [[in_0]], [[in_1]] : f32\n+// CHECK:               linalg.yield [[VAR_64_]] : f32\n+// CHECK:             } -> tensor<256x256xf32>\n+// CHECK-DAG:         [[VAR_42_:%.+]] = arith.index_cast [[VAR_arg9_]] : i32 to index\n+// CHECK-DAG:         [[VAR_43_:%.+]] = arith.index_cast [[PARAM_5_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:         [[VAR_44_:%.+]] = arith.muli [[VAR_42_]], [[VAR_43_]] : index\n+// CHECK-DAG:         [[VAR_45_:%.+]] = arith.index_cast [[VAR_2_]] : i32 to index\n+// CHECK:             [[VAR_46_:%.+]] = arith.addi [[VAR_44_]], [[VAR_45_]] : index\n+// CHECK-DAG:         [[VAR_reinterpret_cast_7_:%.+]] = memref.reinterpret_cast [[PARAM_1_]] to offset: {{.}}[[VAR_46_]]{{.}}, sizes: [256, 256], strides: {{.}}[[VAR_43_]], 1] : memref<*xf32> to memref<256x256xf32, strided<[?, 1], offset: ?>>\n+// CHECK-DAG:         [[RES_1_:%.+]] = memref.alloc() : memref<256x256xf32>\n+// CHECK-DAG:         [[VAR_47_:%.+]] = arith.index_cast [[VAR_arg9_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:         [[VAR_48_:%.+]] = arith.addi [[VAR_47_]], [[CST_256_]] : index\n+// CHECK-DAG:         [[VAR_49_:%.+]] = arith.index_cast [[PARAM_4_]] : i32 to index\n+// CHECK:             [[VAR_50_:%.+]] = arith.minsi [[VAR_48_]], [[VAR_49_]] : index\n+// CHECK-DAG:         [[VAR_51_:%.+]] = arith.subi [[VAR_50_]], [[VAR_47_]] : index\n+// CHECK-DAG:         [[VAR_52_:%.+]] = arith.index_cast [[VAR_2_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:         [[VAR_53_:%.+]] = arith.addi [[VAR_52_]], [[CST_256_]] : index\n+// CHECK-DAG:         [[VAR_54_:%.+]] = arith.index_cast [[PARAM_5_]] : i32 to index\n+// CHECK:             [[VAR_55_:%.+]] = arith.minsi [[VAR_53_]], [[VAR_54_]] : index\n+// CHECK-DAG:         [[VAR_56_:%.+]] = arith.subi [[VAR_55_]], [[VAR_52_]] : index\n+// CHECK-DAG:         [[VAR_57_:%.+]] = arith.minsi [[VAR_51_]], [[CST_256_]] : index\n+// CHECK:             [[VAR_58_:%.+]] = arith.minsi [[VAR_56_]], [[CST_256_]] : index\n+// CHECK-DAG:         [[VAR_subview_9_:%.+]] = memref.subview [[VAR_reinterpret_cast_7_]][0, 0] {{.}}[[VAR_57_]], [[VAR_58_]]{{.}} [1, 1] : memref<256x256xf32, strided<[?, 1], offset: ?>> to memref<?x?xf32, strided<[?, 1], offset: ?>>\n+// CHECK-DAG:         [[VAR_subview_10_:%.+]] = memref.subview [[RES_1_]][0, 0] {{.}}[[VAR_57_]], [[VAR_58_]]{{.}} [1, 1] : memref<256x256xf32> to memref<?x?xf32, strided<[256, 1]>>\n+// CHECK-DAG:         [[VAR_59_:%.+]] = arith.cmpi slt, [[VAR_57_]], [[CST_256_]] : index\n+// CHECK-DAG:         [[VAR_60_:%.+]] = arith.cmpi slt, [[VAR_58_]], [[CST_256_]] : index\n+// CHECK:             [[VAR_61_:%.+]] = arith.ori [[VAR_59_]], [[VAR_60_]] : i1\n+// CHECK:             scf.if [[VAR_61_]] {\n+// CHECK:               linalg.fill ins([[CST_0_dot_000000_]] : f32) outs([[RES_1_]] : memref<256x256xf32>)\n+// CHECK:             }\n+// CHECK:             memref.copy [[VAR_subview_9_]], [[VAR_subview_10_]] : memref<?x?xf32, strided<[?, 1], offset: ?>> to memref<?x?xf32, strided<[256, 1]>>\n+// CHECK:             [[VAR_62_:%.+]] = bufferization.to_tensor [[RES_1_]] restrict writable : memref<256x256xf32>\n+// CHECK:             [[VAR_63_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins([[VAR_arg11_]], [[VAR_62_]] : tensor<256x256xf32>, tensor<256x256xf32>) outs([[VAR_arg11_]] : tensor<256x256xf32>) {\n+// CHECK:             ^bb0([[in_0:.+]]: f32, [[in_1:.+]]: f32, [[out:.+]]: f32):\n+// CHECK:               [[VAR_64_1_:%.+]] = arith.addf [[in_0]], [[in_1]] : f32\n+// CHECK:               linalg.yield [[VAR_64_1_]] : f32\n+// CHECK:             } -> tensor<256x256xf32>\n+// CHECK:             scf.yield [[VAR_41_]], [[VAR_63_]] : tensor<256x256xf32>, tensor<256x256xf32>\n+// CHECK:           }\n+// CHECK:           [[VAR_4_:%.+]] = tensor.empty() : tensor<256xf32>\n+// CHECK:           [[VAR_5_:%.+]] = linalg.fill ins([[CST_0_dot_000000_]] : f32) outs([[VAR_4_]] : tensor<256xf32>) -> tensor<256xf32>\n+// CHECK:           [[VAR_reduced_:%.+]] = linalg.reduce ins([[VAR_3_]]#0 : tensor<256x256xf32>) outs([[VAR_5_]] : tensor<256xf32>) dimensions = [0]\n+// CHECK:             ([[in_0:.+]]: f32, [[in_1:.+]]: f32) {\n+// CHECK:               [[VAR_20_1_:%.+]] = arith.addf [[in_0]], [[in_1]] : f32\n+// CHECK:               linalg.yield [[VAR_20_1_]] : f32\n+// CHECK:             }\n+// CHECK:           [[VAR_6_:%.+]] = tensor.empty() : tensor<256xf32>\n+// CHECK:           [[VAR_7_:%.+]] = linalg.fill ins([[CST_0_dot_000000_]] : f32) outs([[VAR_6_]] : tensor<256xf32>) -> tensor<256xf32>\n+// CHECK:           [[VAR_reduced_0_:%.+]] = linalg.reduce ins([[VAR_3_]]#1 : tensor<256x256xf32>) outs([[VAR_7_]] : tensor<256xf32>) dimensions = [0]\n+// CHECK:             ([[in_0:.+]]: f32, [[in_1:.+]]: f32) {\n+// CHECK:               [[VAR_20_2_:%.+]] = arith.addf [[in_0]], [[in_1]] : f32\n+// CHECK:               linalg.yield [[VAR_20_2_]] : f32\n+// CHECK:             }\n+// CHECK:           [[VAR_8_:%.+]] = arith.index_cast [[VAR_2_]] : i32 to index\n+// CHECK-DAG:       [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_2_]] to offset: {{.}}[[VAR_8_]]{{.}}, sizes: [256], strides: [1] : memref<*xf32> to memref<256xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:       [[VAR_9_:%.+]] = arith.index_cast [[VAR_2_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_10_:%.+]] = arith.addi [[VAR_9_]], [[CST_256_]] : index\n+// CHECK-DAG:       [[VAR_11_:%.+]] = arith.index_cast [[PARAM_5_]] : i32 to index\n+// CHECK:           [[VAR_12_:%.+]] = arith.minsi [[VAR_10_]], [[VAR_11_]] : index\n+// CHECK:           [[VAR_13_:%.+]] = arith.subi [[VAR_12_]], [[VAR_9_]] : index\n+// CHECK-DAG:       [[VAR_extracted_slice_:%.+]] = tensor.extract_slice [[VAR_reduced_]][0] {{.}}[[VAR_13_]]{{.}} [1] : tensor<256xf32> to tensor<?xf32>\n+// CHECK-DAG:       [[VAR_subview_:%.+]] = memref.subview [[VAR_reinterpret_cast_]][0] {{.}}[[VAR_13_]]{{.}} [1] : memref<256xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1], offset: ?>>\n+// CHECK:           memref.tensor_store [[VAR_extracted_slice_]], [[VAR_subview_]] : memref<?xf32, strided<[1], offset: ?>>\n+// CHECK:           [[VAR_14_:%.+]] = arith.index_cast [[VAR_2_]] : i32 to index\n+// CHECK-DAG:       [[VAR_reinterpret_cast_1_:%.+]] = memref.reinterpret_cast [[PARAM_3_]] to offset: {{.}}[[VAR_14_]]{{.}}, sizes: [256], strides: [1] : memref<*xf32> to memref<256xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:       [[VAR_15_:%.+]] = arith.index_cast [[VAR_2_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_16_:%.+]] = arith.addi [[VAR_15_]], [[CST_256_]] : index\n+// CHECK-DAG:       [[VAR_17_:%.+]] = arith.index_cast [[PARAM_5_]] : i32 to index\n+// CHECK:           [[VAR_18_:%.+]] = arith.minsi [[VAR_16_]], [[VAR_17_]] : index\n+// CHECK:           [[VAR_19_:%.+]] = arith.subi [[VAR_18_]], [[VAR_15_]] : index\n+// CHECK-DAG:       [[VAR_extracted_slice_2_:%.+]] = tensor.extract_slice [[VAR_reduced_0_]][0] {{.}}[[VAR_19_]]{{.}} [1] : tensor<256xf32> to tensor<?xf32>\n+// CHECK-DAG:       [[VAR_subview_3_:%.+]] = memref.subview [[VAR_reinterpret_cast_1_]][0] {{.}}[[VAR_19_]]{{.}} [1] : memref<256xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1], offset: ?>>\n+// CHECK:           memref.tensor_store [[VAR_extracted_slice_2_]], [[VAR_subview_3_]] : memref<?xf32, strided<[1], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/kernel-05-layer-norm-fwd.mlir", "status": "added", "additions": 313, "deletions": 0, "changes": 313, "file_content_changes": "@@ -0,0 +1,313 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+\n+module {\n+  tt.func public @_layer_norm_fwd_fused_0123456789(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: !tt.ptr<f32>, %arg4: !tt.ptr<f32>, %arg5: !tt.ptr<f32>, %arg6: i32, %arg7: i32, %arg8: f32) {\n+    %c256_i32 = arith.constant 256 : i32\n+    %c0_i32 = arith.constant 0 : i32\n+    %cst = arith.constant 1.000000e+00 : f32\n+    %cst_0 = arith.constant 0.000000e+00 : f32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %arg6 : i32\n+    %2 = tt.addptr %arg1, %1 : !tt.ptr<f32>, i32\n+    %3 = tt.addptr %arg0, %1 : !tt.ptr<f32>, i32\n+    %4 = tt.splat %cst_0 : (f32) -> tensor<256xf32>\n+    %5 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32>\n+    %6 = tt.splat %arg7 : (i32) -> tensor<256xi32>\n+    %7 = tt.splat %3 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n+    %8 = scf.for %arg9 = %c0_i32 to %arg7 step %c256_i32 iter_args(%arg10 = %4) -> (tensor<256xf32>)  : i32 {\n+      %32 = tt.splat %arg9 : (i32) -> tensor<256xi32>\n+      %33 = arith.addi %32, %5 : tensor<256xi32>\n+      %34 = arith.cmpi slt, %33, %6 : tensor<256xi32>\n+      %35 = tt.addptr %7, %33 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n+      %36 = tt.load %35, %34, %4 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32>\n+      %37 = arith.addf %arg10, %36 : tensor<256xf32>\n+      scf.yield %37 : tensor<256xf32>\n+    }\n+    %9 = \"tt.reduce\"(%8) ({\n+    ^bb0(%arg9: f32, %arg10: f32):\n+      %32 = arith.addf %arg9, %arg10 : f32\n+      tt.reduce.return %32 : f32\n+    }) {axis = 0 : i32} : (tensor<256xf32>) -> f32\n+    %10 = arith.sitofp %arg7 : i32 to f32\n+    %11 = arith.divf %9, %10 : f32\n+    %12 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32>\n+    %13 = tt.splat %arg7 : (i32) -> tensor<256xi32>\n+    %14 = tt.splat %3 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n+    %15 = tt.splat %11 : (f32) -> tensor<256xf32>\n+    %16 = scf.for %arg9 = %c0_i32 to %arg7 step %c256_i32 iter_args(%arg10 = %4) -> (tensor<256xf32>)  : i32 {\n+      %32 = tt.splat %arg9 : (i32) -> tensor<256xi32>\n+      %33 = arith.addi %32, %12 : tensor<256xi32>\n+      %34 = arith.cmpi slt, %33, %13 : tensor<256xi32>\n+      %35 = tt.addptr %14, %33 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n+      %36 = tt.load %35, %34, %4 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32>\n+      %37 = arith.subf %36, %15 : tensor<256xf32>\n+      %38 = arith.select %34, %37, %4 : tensor<256xi1>, tensor<256xf32>\n+      %39 = arith.mulf %38, %38 : tensor<256xf32>\n+      %40 = arith.addf %arg10, %39 : tensor<256xf32>\n+      scf.yield %40 : tensor<256xf32>\n+    }\n+    %17 = \"tt.reduce\"(%16) ({\n+    ^bb0(%arg9: f32, %arg10: f32):\n+      %32 = arith.addf %arg9, %arg10 : f32\n+      tt.reduce.return %32 : f32\n+    }) {axis = 0 : i32} : (tensor<256xf32>) -> f32\n+    %18 = arith.divf %17, %10 : f32\n+    %19 = arith.addf %18, %arg8 : f32\n+    %20 = math.sqrt %19 : f32\n+    %21 = arith.divf %cst, %20 : f32\n+    %22 = tt.addptr %arg4, %0 : !tt.ptr<f32>, i32\n+    tt.store %22, %11 {cache = 1 : i32, evict = 1 : i32} : f32\n+    %23 = tt.addptr %arg5, %0 : !tt.ptr<f32>, i32\n+    tt.store %23, %21 {cache = 1 : i32, evict = 1 : i32} : f32\n+    %24 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32>\n+    %25 = tt.splat %arg7 : (i32) -> tensor<256xi32>\n+    %26 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n+    %27 = tt.splat %arg3 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n+    %28 = tt.splat %3 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n+    %29 = tt.splat %11 : (f32) -> tensor<256xf32>\n+    %30 = tt.splat %21 : (f32) -> tensor<256xf32>\n+    %31 = tt.splat %2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n+    scf.for %arg9 = %c0_i32 to %arg7 step %c256_i32  : i32 {\n+      %32 = tt.splat %arg9 : (i32) -> tensor<256xi32>\n+      %33 = arith.addi %32, %24 : tensor<256xi32>\n+      %34 = arith.cmpi slt, %33, %25 : tensor<256xi32>\n+      %35 = tt.addptr %26, %33 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n+      %36 = tt.load %35, %34 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32>\n+      %37 = tt.addptr %27, %33 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n+      %38 = tt.load %37, %34 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32>\n+      %39 = tt.addptr %28, %33 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n+      %40 = tt.load %39, %34, %4 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32>\n+      %41 = arith.subf %40, %29 : tensor<256xf32>\n+      %42 = arith.mulf %41, %30 : tensor<256xf32>\n+      %43 = arith.mulf %42, %36 : tensor<256xf32>\n+      %44 = arith.addf %43, %38 : tensor<256xf32>\n+      %45 = tt.addptr %31, %33 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n+      tt.store %45, %44, %34 {cache = 1 : i32, evict = 1 : i32} : tensor<256xf32>\n+    }\n+    tt.return\n+  }\n+}\n+\n+// CHECK-DAG:   [[MAP_0_:#.+]] = affine_map<(d0) -> (d0)>\n+// CHECK-LABEL:  func.func @_layer_norm_fwd_fused_0123456789\n+// CHECK-SAME:   ([[PARAM_0_:%.+]]: memref<*xf32>, [[PARAM_1_:%.+]]: memref<*xf32>, [[PARAM_2_:%.+]]: memref<*xf32>, [[PARAM_3_:%.+]]: memref<*xf32>, [[PARAM_4_:%.+]]: memref<*xf32>, [[PARAM_5_:%.+]]: memref<*xf32>, [[PARAM_6_:%.+]]: i32, [[PARAM_7_:%.+]]: i32, [[PARAM_8_:%.+]]: f32, [[PARAM_9_:%.+]]: i32, [[PARAM_10_:%.+]]: i32, [[PARAM_11_:%.+]]: i32) {\n+// CHECK-DAG:       [[CST_256_:%.+]] = arith.constant 256 : index\n+// CHECK-DAG:       [[CST_1_dot_000000_:%.+]] = arith.constant 1.000000e+00 : f32\n+// CHECK-DAG:       [[CST_0_:%.+]] = arith.constant 0 : i32\n+// CHECK-DAG:       [[CST_256_1_:%.+]] = arith.constant 256 : i32\n+// CHECK-DAG:       [[CST_0_dot_000000_:%.+]] = arith.constant 0.000000e+00 : f32\n+// CHECK-DAG:       [[VAR_0_:%.+]] = tensor.empty() : tensor<256xf32>\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_1_:%.+]] = linalg.fill ins([[CST_0_dot_000000_]] : f32) outs([[VAR_0_]] : tensor<256xf32>) -> tensor<256xf32>\n+// CHECK-DAG:       [[VAR_2_:%.+]] = arith.muli [[PARAM_9_]], [[PARAM_6_]] : i32\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_3_:%.+]] = scf.for [[VAR_arg12_:%.+]] = [[CST_0_]] to [[PARAM_7_]] step [[CST_256_1_]] iter_args([[VAR_arg13_:%.+]] = [[VAR_1_]]) -> (tensor<256xf32>)  : i32 {\n+// CHECK-DAG:         [[VAR_25_:%.+]] = arith.index_cast [[VAR_2_]] : i32 to index\n+// CHECK-DAG:         [[VAR_26_:%.+]] = arith.index_cast [[VAR_arg12_]] : i32 to index\n+// CHECK:             [[VAR_27_:%.+]] = arith.addi [[VAR_25_]], [[VAR_26_]] : index\n+// CHECK-DAG:         [[VAR_reinterpret_cast_5_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[VAR_27_]]{{.}}, sizes: [256], strides: [1] : memref<*xf32> to memref<256xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:         [[RES_:%.+]] = memref.alloc() : memref<256xf32>\n+// CHECK-DAG:         [[VAR_28_:%.+]] = arith.index_cast [[VAR_arg12_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:         [[VAR_29_:%.+]] = arith.addi [[VAR_28_]], [[CST_256_]] : index\n+// CHECK-DAG:         [[VAR_30_:%.+]] = arith.index_cast [[PARAM_7_]] : i32 to index\n+// CHECK:             [[VAR_31_:%.+]] = arith.minsi [[VAR_29_]], [[VAR_30_]] : index\n+// CHECK:             [[VAR_32_:%.+]] = arith.subi [[VAR_31_]], [[VAR_28_]] : index\n+// CHECK-DAG:         [[VAR_subview_:%.+]] = memref.subview [[VAR_reinterpret_cast_5_]][0] {{.}}[[VAR_32_]]{{.}} [1] : memref<256xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:         [[VAR_subview_6_:%.+]] = memref.subview [[RES_]][0] {{.}}[[VAR_32_]]{{.}} [1] : memref<256xf32> to memref<?xf32, strided<[1]>>\n+// CHECK-DAG:         [[VAR_33_:%.+]] = arith.cmpi slt, [[VAR_32_]], [[CST_256_]] : index\n+// CHECK:             scf.if [[VAR_33_]] {\n+// CHECK:               linalg.fill ins([[CST_0_dot_000000_]] : f32) outs([[RES_]] : memref<256xf32>)\n+// CHECK:             }\n+// CHECK:             memref.copy [[VAR_subview_]], [[VAR_subview_]]_6 : memref<?xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1]>>\n+// CHECK:             [[VAR_34_:%.+]] = bufferization.to_tensor [[RES_]] restrict writable : memref<256xf32>\n+// CHECK:             [[VAR_35_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins([[VAR_arg13_]], [[VAR_34_]] : tensor<256xf32>, tensor<256xf32>) outs([[VAR_arg13_]] : tensor<256xf32>) {\n+// CHECK:             ^bb0([[in_0:.+]]: f32, [[in_1:.+]]: f32, [[out:.+]]: f32):\n+// CHECK:               [[VAR_36_:%.+]] = arith.addf [[in_0]], [[in_1]] : f32\n+// CHECK:               linalg.yield [[VAR_36_]] : f32\n+// CHECK:             } -> tensor<256xf32>\n+// CHECK:             scf.yield [[VAR_35_]] : tensor<256xf32>\n+// CHECK:           }\n+// CHECK:           [[VAR_4_:%.+]] = bufferization.alloc_tensor() : tensor<f32>\n+// CHECK:           [[VAR_inserted_:%.+]] = tensor.insert [[CST_0_dot_000000_]] into [[VAR_4_]][] : tensor<f32>\n+// CHECK:           [[VAR_reduced_:%.+]] = linalg.reduce ins([[VAR_3_]] : tensor<256xf32>) outs([[VAR_inserted_]] : tensor<f32>) dimensions = [0]\n+// CHECK:             ([[in_0:.+]]: f32, [[in_1:.+]]: f32) {\n+// CHECK:               [[VAR_25_1_:%.+]] = arith.addf [[in_0]], [[in_1]] : f32\n+// CHECK:               linalg.yield [[VAR_25_1_]] : f32\n+// CHECK:             }\n+// CHECK-DAG:       [[VAR_extracted_:%.+]] = tensor.extract [[VAR_reduced_]][] : tensor<f32>\n+// CHECK-DAG:       [[VAR_5_:%.+]] = arith.sitofp [[PARAM_7_]] : i32 to f32\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_6_:%.+]] = arith.divf [[VAR_extracted_]], [[VAR_5_]] : f32\n+// CHECK-DAG:       [[VAR_7_:%.+]] = tensor.empty() : tensor<256xi32>\n+// CHECK:           [[VAR_8_:%.+]] = linalg.generic {indexing_maps = [#map], iterator_types = [\"parallel\"]} outs([[VAR_7_]] : tensor<256xi32>) {\n+// CHECK:           ^bb0([[out:.+]]: i32):\n+// CHECK:             [[VAR_25_2_:%.+]] = linalg.index 0 : index\n+// CHECK:             [[VAR_26_1_:%.+]] = arith.index_cast [[VAR_25_2_]] : index to i32\n+// CHECK:             linalg.yield [[VAR_26_1_]] : i32\n+// CHECK:           } -> tensor<256xi32>\n+// CHECK:           [[VAR_9_:%.+]] = tensor.empty() : tensor<256xi32>\n+// CHECK-DAG:       [[VAR_10_:%.+]] = linalg.fill ins([[PARAM_7_]] : i32) outs([[VAR_9_]] : tensor<256xi32>) -> tensor<256xi32>\n+// CHECK-DAG:       [[VAR_11_:%.+]] = tensor.empty() : tensor<256xf32>\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:       [[VAR_12_:%.+]] = linalg.fill ins([[VAR_6_]] : f32) outs([[VAR_11_]] : tensor<256xf32>) -> tensor<256xf32>\n+// CHECK-DAG:       [[VAR_13_:%.+]] = scf.for [[VAR_arg12_1_:%.+]] = [[CST_0_]] to [[PARAM_7_]] step [[CST_256_1_]] iter_args([[VAR_arg13_1_:%.+]] = [[VAR_1_]]) -> (tensor<256xf32>)  : i32 {\n+// CHECK-DAG:         [[VAR_25_3_:%.+]] = tensor.empty() : tensor<256xi32>\n+// CHECK:             [[VAR_26_2_:%.+]] = linalg.fill ins([[VAR_arg12_1_]] : i32) outs([[VAR_25_3_]] : tensor<256xi32>) -> tensor<256xi32>\n+// CHECK:             [[VAR_27_1_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins([[VAR_26_2_]], [[VAR_8_]] : tensor<256xi32>, tensor<256xi32>) outs([[VAR_26_2_]] : tensor<256xi32>) {\n+// CHECK:             ^bb0([[in_0:.+]]: i32, [[in_1:.+]]: i32, [[out:.+]]: i32):\n+// CHECK:               [[VAR_44_:%.+]] = arith.addi [[in_0]], [[in_1]] : i32\n+// CHECK:               linalg.yield [[VAR_44_]] : i32\n+// CHECK:             } -> tensor<256xi32>\n+// CHECK:             [[VAR_28_1_:%.+]] = tensor.empty() : tensor<256xi1>\n+// CHECK:             [[VAR_29_1_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins([[VAR_27_1_]], [[VAR_10_]] : tensor<256xi32>, tensor<256xi32>) outs([[VAR_28_1_]] : tensor<256xi1>) {\n+// CHECK:             ^bb0([[in_0:.+]]: i32, [[in_1:.+]]: i32, [[out:.+]]: i1):\n+// CHECK:               [[VAR_44_1_:%.+]] = arith.cmpi slt, [[in_0]], [[in_1]] : i32\n+// CHECK:               linalg.yield [[VAR_44_1_]] : i1\n+// CHECK:             } -> tensor<256xi1>\n+// CHECK-DAG:         [[VAR_30_1_:%.+]] = arith.index_cast [[VAR_2_]] : i32 to index\n+// CHECK-DAG:         [[VAR_31_1_:%.+]] = arith.index_cast [[VAR_arg12_1_]] : i32 to index\n+// CHECK:             [[VAR_32_1_:%.+]] = arith.addi [[VAR_30_1_]], [[VAR_31_1_]] : index\n+// CHECK-DAG:         [[VAR_reinterpret_cast_5_1_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[VAR_32_1_]]{{.}}, sizes: [256], strides: [1] : memref<*xf32> to memref<256xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:         [[RES_1_:%.+]] = memref.alloc() : memref<256xf32>\n+// CHECK-DAG:         [[VAR_33_1_:%.+]] = arith.index_cast [[VAR_arg12_1_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:         [[VAR_34_1_:%.+]] = arith.addi [[VAR_33_1_]], [[CST_256_]] : index\n+// CHECK-DAG:         [[VAR_35_1_:%.+]] = arith.index_cast [[PARAM_7_]] : i32 to index\n+// CHECK:             [[VAR_36_1_:%.+]] = arith.minsi [[VAR_34_1_]], [[VAR_35_1_]] : index\n+// CHECK:             [[VAR_37_:%.+]] = arith.subi [[VAR_36_1_]], [[VAR_33_1_]] : index\n+// CHECK-DAG:         [[VAR_subview_1_:%.+]] = memref.subview [[VAR_reinterpret_cast_5_1_]][0] {{.}}[[VAR_37_]]{{.}} [1] : memref<256xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:         [[VAR_subview_6_1_:%.+]] = memref.subview [[RES_1_]][0] {{.}}[[VAR_37_]]{{.}} [1] : memref<256xf32> to memref<?xf32, strided<[1]>>\n+// CHECK-DAG:         [[VAR_38_:%.+]] = arith.cmpi slt, [[VAR_37_]], [[CST_256_]] : index\n+// CHECK:             scf.if [[VAR_38_]] {\n+// CHECK:               linalg.fill ins([[CST_0_dot_000000_]] : f32) outs([[RES_1_]] : memref<256xf32>)\n+// CHECK:             }\n+// CHECK:             memref.copy [[VAR_subview_1_]], [[VAR_subview_1_]]_6 : memref<?xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1]>>\n+// CHECK:             [[VAR_39_:%.+]] = bufferization.to_tensor [[RES_1_]] restrict writable : memref<256xf32>\n+// CHECK:             [[VAR_40_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins([[VAR_39_]], [[VAR_12_]] : tensor<256xf32>, tensor<256xf32>) outs([[VAR_39_]] : tensor<256xf32>) {\n+// CHECK:             ^bb0([[in_0:.+]]: f32, [[in_1:.+]]: f32, [[out:.+]]: f32):\n+// CHECK:               [[VAR_44_2_:%.+]] = arith.subf [[in_0]], [[in_1]] : f32\n+// CHECK:               linalg.yield [[VAR_44_2_]] : f32\n+// CHECK:             } -> tensor<256xf32>\n+// CHECK:             [[VAR_41_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = [\"parallel\"]} ins([[VAR_29_1_]], [[VAR_40_]], [[VAR_1_]] : tensor<256xi1>, tensor<256xf32>, tensor<256xf32>) outs([[VAR_40_]] : tensor<256xf32>) {\n+// CHECK:             ^bb0([[in_0:.+]]: i1, [[in_1:.+]]: f32, [[in_2:.+]]: f32, [[out:.+]]: f32):\n+// CHECK:               [[VAR_44_3_:%.+]] = arith.select [[in_0]], [[in_1]], [[in_2]] : f32\n+// CHECK:               linalg.yield [[VAR_44_3_]] : f32\n+// CHECK:             } -> tensor<256xf32>\n+// CHECK:             [[VAR_42_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins([[VAR_41_]], [[VAR_41_]] : tensor<256xf32>, tensor<256xf32>) outs([[VAR_41_]] : tensor<256xf32>) {\n+// CHECK:             ^bb0([[in_0:.+]]: f32, [[in_1:.+]]: f32, [[out:.+]]: f32):\n+// CHECK:               [[VAR_44_4_:%.+]] = arith.mulf [[in_0]], [[in_1]] : f32\n+// CHECK:               linalg.yield [[VAR_44_4_]] : f32\n+// CHECK:             } -> tensor<256xf32>\n+// CHECK:             [[VAR_43_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins([[VAR_arg13_1_]], [[VAR_42_]] : tensor<256xf32>, tensor<256xf32>) outs([[VAR_arg13_1_]] : tensor<256xf32>) {\n+// CHECK:             ^bb0([[in_0:.+]]: f32, [[in_1:.+]]: f32, [[out:.+]]: f32):\n+// CHECK:               [[VAR_44_5_:%.+]] = arith.addf [[in_0]], [[in_1]] : f32\n+// CHECK:               linalg.yield [[VAR_44_5_]] : f32\n+// CHECK:             } -> tensor<256xf32>\n+// CHECK:             scf.yield [[VAR_43_]] : tensor<256xf32>\n+// CHECK:           }\n+// CHECK:           [[VAR_14_:%.+]] = bufferization.alloc_tensor() : tensor<f32>\n+// CHECK:           [[VAR_inserted_1_:%.+]] = tensor.insert [[CST_0_dot_000000_]] into [[VAR_14_]][] : tensor<f32>\n+// CHECK:           [[VAR_reduced_2_:%.+]] = linalg.reduce ins([[VAR_13_]] : tensor<256xf32>) outs([[VAR_inserted_1_]] : tensor<f32>) dimensions = [0]\n+// CHECK:             ([[in_0:.+]]: f32, [[in_1:.+]]: f32) {\n+// CHECK:               [[VAR_25_4_:%.+]] = arith.addf [[in_0]], [[in_1]] : f32\n+// CHECK:               linalg.yield [[VAR_25_4_]] : f32\n+// CHECK:             }\n+// CHECK:           [[VAR_extracted_3_:%.+]] = tensor.extract [[VAR_reduced_2_]][] : tensor<f32>\n+// CHECK:           [[VAR_15_:%.+]] = arith.divf [[VAR_extracted_3_]], [[VAR_5_]] : f32\n+// CHECK:           [[VAR_16_:%.+]] = arith.addf [[VAR_15_]], [[PARAM_8_]] : f32\n+// CHECK:           [[VAR_17_:%.+]] = math.sqrt [[VAR_16_]] : f32\n+// CHECK-DAG:       [[VAR_18_:%.+]] = arith.divf [[CST_1_dot_000000_]], [[VAR_17_]] : f32\n+// CHECK-DAG:       [[VAR_19_:%.+]] = arith.index_cast [[PARAM_9_]] : i32 to index\n+// CHECK:           [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_4_]] to offset: {{.}}[[VAR_19_]]{{.}}, sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>>\n+// CHECK:           affine.store [[VAR_6_]], [[VAR_reinterpret_cast_]][0] : memref<1xf32, strided<[1], offset: ?>>\n+// CHECK:           [[VAR_20_:%.+]] = arith.index_cast [[PARAM_9_]] : i32 to index\n+// CHECK:           [[VAR_reinterpret_cast_4_:%.+]] = memref.reinterpret_cast [[PARAM_5_]] to offset: {{.}}[[VAR_20_]]{{.}}, sizes: [1], strides: [1] : memref<*xf32> to memref<1xf32, strided<[1], offset: ?>>\n+// CHECK:           affine.store [[VAR_18_]], [[VAR_reinterpret_cast_4_]][0] : memref<1xf32, strided<[1], offset: ?>>\n+// CHECK:           [[VAR_21_:%.+]] = tensor.empty() : tensor<256xf32>\n+// CHECK-DAG:       [[VAR_22_:%.+]] = linalg.fill ins([[VAR_6_]] : f32) outs([[VAR_21_]] : tensor<256xf32>) -> tensor<256xf32>\n+// CHECK-DAG:       [[VAR_23_:%.+]] = tensor.empty() : tensor<256xf32>\n+// CHECK:           [[VAR_24_:%.+]] = linalg.fill ins([[VAR_18_]] : f32) outs([[VAR_23_]] : tensor<256xf32>) -> tensor<256xf32>\n+// CHECK:           scf.for [[VAR_arg12_1_:%.+]] = [[CST_0_]] to [[PARAM_7_]] step [[CST_256_1_]]  : i32 {\n+// CHECK:             [[VAR_25_5_:%.+]] = arith.index_cast [[VAR_arg12_1_]] : i32 to index\n+// CHECK-DAG:         [[VAR_reinterpret_cast_5_2_:%.+]] = memref.reinterpret_cast [[PARAM_2_]] to offset: {{.}}[[VAR_25_5_]]{{.}}, sizes: [256], strides: [1] : memref<*xf32> to memref<256xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:         [[RES_2_:%.+]] = memref.alloc() : memref<256xf32>\n+// CHECK-DAG:         [[VAR_26_3_:%.+]] = arith.index_cast [[VAR_arg12_1_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:         [[VAR_27_2_:%.+]] = arith.addi [[VAR_26_3_]], [[CST_256_]] : index\n+// CHECK-DAG:         [[VAR_28_2_:%.+]] = arith.index_cast [[PARAM_7_]] : i32 to index\n+// CHECK:             [[VAR_29_2_:%.+]] = arith.minsi [[VAR_27_2_]], [[VAR_28_2_]] : index\n+// CHECK:             [[VAR_30_2_:%.+]] = arith.subi [[VAR_29_2_]], [[VAR_26_3_]] : index\n+// CHECK-DAG:         [[VAR_subview_2_:%.+]] = memref.subview [[VAR_reinterpret_cast_5_2_]][0] {{.}}[[VAR_30_2_]]{{.}} [1] : memref<256xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:         [[VAR_subview_6_2_:%.+]] = memref.subview [[RES_2_]][0] {{.}}[[VAR_30_2_]]{{.}} [1] : memref<256xf32> to memref<?xf32, strided<[1]>>\n+// CHECK:             memref.copy [[VAR_subview_2_]], [[VAR_subview_2_]]_6 : memref<?xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1]>>\n+// CHECK-DAG:         [[VAR_31_2_:%.+]] = bufferization.to_tensor [[RES_2_]] restrict writable : memref<256xf32>\n+// CHECK-DAG:         [[VAR_32_2_:%.+]] = arith.index_cast [[VAR_arg12_1_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:         [[VAR_reinterpret_cast_7_:%.+]] = memref.reinterpret_cast [[PARAM_3_]] to offset: {{.}}[[VAR_32_2_]]{{.}}, sizes: [256], strides: [1] : memref<*xf32> to memref<256xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:         [[RES_3_:%.+]] = memref.alloc() : memref<256xf32>\n+// CHECK-DAG:         [[VAR_33_2_:%.+]] = arith.index_cast [[VAR_arg12_1_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:         [[VAR_34_2_:%.+]] = arith.addi [[VAR_33_2_]], [[CST_256_]] : index\n+// CHECK-DAG:         [[VAR_35_2_:%.+]] = arith.index_cast [[PARAM_7_]] : i32 to index\n+// CHECK:             [[VAR_36_2_:%.+]] = arith.minsi [[VAR_34_2_]], [[VAR_35_2_]] : index\n+// CHECK:             [[VAR_37_1_:%.+]] = arith.subi [[VAR_36_2_]], [[VAR_33_2_]] : index\n+// CHECK-DAG:         [[VAR_subview_9_:%.+]] = memref.subview [[VAR_reinterpret_cast_7_]][0] {{.}}[[VAR_37_1_]]{{.}} [1] : memref<256xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:         [[VAR_subview_10_:%.+]] = memref.subview [[RES_3_]][0] {{.}}[[VAR_37_1_]]{{.}} [1] : memref<256xf32> to memref<?xf32, strided<[1]>>\n+// CHECK:             memref.copy [[VAR_subview_9_]], [[VAR_subview_10_]] : memref<?xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1]>>\n+// CHECK-DAG:         [[VAR_38_1_:%.+]] = bufferization.to_tensor [[RES_3_]] restrict writable : memref<256xf32>\n+// CHECK-DAG:         [[VAR_39_1_:%.+]] = arith.index_cast [[VAR_2_]] : i32 to index\n+// CHECK-DAG:         [[VAR_40_1_:%.+]] = arith.index_cast [[VAR_arg12_1_]] : i32 to index\n+// CHECK:             [[VAR_41_1_:%.+]] = arith.addi [[VAR_39_1_]], [[VAR_40_1_]] : index\n+// CHECK-DAG:         [[VAR_reinterpret_cast_11_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[VAR_41_1_]]{{.}}, sizes: [256], strides: [1] : memref<*xf32> to memref<256xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:         [[RES_4_:%.+]] = memref.alloc() : memref<256xf32>\n+// CHECK-DAG:         [[VAR_42_1_:%.+]] = arith.index_cast [[VAR_arg12_1_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:         [[VAR_43_1_:%.+]] = arith.addi [[VAR_42_1_]], [[CST_256_]] : index\n+// CHECK-DAG:         [[VAR_44_6_:%.+]] = arith.index_cast [[PARAM_7_]] : i32 to index\n+// CHECK:             [[VAR_45_:%.+]] = arith.minsi [[VAR_43_1_]], [[VAR_44_6_]] : index\n+// CHECK:             [[VAR_46_:%.+]] = arith.subi [[VAR_45_]], [[VAR_42_1_]] : index\n+// CHECK-DAG:         [[VAR_subview_13_:%.+]] = memref.subview [[VAR_reinterpret_cast_11_]][0] {{.}}[[VAR_46_]]{{.}} [1] : memref<256xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:         [[VAR_subview_14_:%.+]] = memref.subview [[RES_4_]][0] {{.}}[[VAR_46_]]{{.}} [1] : memref<256xf32> to memref<?xf32, strided<[1]>>\n+// CHECK-DAG:         [[VAR_47_:%.+]] = arith.cmpi slt, [[VAR_46_]], [[CST_256_]] : index\n+// CHECK:             scf.if [[VAR_47_]] {\n+// CHECK:               linalg.fill ins([[CST_0_dot_000000_]] : f32) outs([[RES_4_]] : memref<256xf32>)\n+// CHECK:             }\n+// CHECK:             memref.copy [[VAR_subview_13_]], [[VAR_subview_14_]] : memref<?xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1]>>\n+// CHECK:             [[VAR_48_:%.+]] = bufferization.to_tensor [[RES_4_]] restrict writable : memref<256xf32>\n+// CHECK:             [[VAR_49_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins([[VAR_48_]], [[VAR_22_]] : tensor<256xf32>, tensor<256xf32>) outs([[VAR_48_]] : tensor<256xf32>) {\n+// CHECK:             ^bb0([[in_0:.+]]: f32, [[in_1:.+]]: f32, [[out:.+]]: f32):\n+// CHECK:               [[VAR_61_:%.+]] = arith.subf [[in_0]], [[in_1]] : f32\n+// CHECK:               linalg.yield [[VAR_61_]] : f32\n+// CHECK:             } -> tensor<256xf32>\n+// CHECK:             [[VAR_50_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins([[VAR_49_]], [[VAR_24_]] : tensor<256xf32>, tensor<256xf32>) outs([[VAR_49_]] : tensor<256xf32>) {\n+// CHECK:             ^bb0([[in_0:.+]]: f32, [[in_1:.+]]: f32, [[out:.+]]: f32):\n+// CHECK:               [[VAR_61_1_:%.+]] = arith.mulf [[in_0]], [[in_1]] : f32\n+// CHECK:               linalg.yield [[VAR_61_1_]] : f32\n+// CHECK:             } -> tensor<256xf32>\n+// CHECK:             [[VAR_51_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins([[VAR_50_]], [[VAR_31_2_]] : tensor<256xf32>, tensor<256xf32>) outs([[VAR_50_]] : tensor<256xf32>) {\n+// CHECK:             ^bb0([[in_0:.+]]: f32, [[in_1:.+]]: f32, [[out:.+]]: f32):\n+// CHECK:               [[VAR_61_2_:%.+]] = arith.mulf [[in_0]], [[in_1]] : f32\n+// CHECK:               linalg.yield [[VAR_61_2_]] : f32\n+// CHECK:             } -> tensor<256xf32>\n+// CHECK:             [[VAR_52_:%.+]] = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\"]} ins([[VAR_51_]], [[VAR_38_1_]] : tensor<256xf32>, tensor<256xf32>) outs([[VAR_51_]] : tensor<256xf32>) {\n+// CHECK:             ^bb0([[in_0:.+]]: f32, [[in_1:.+]]: f32, [[out:.+]]: f32):\n+// CHECK:               [[VAR_61_3_:%.+]] = arith.addf [[in_0]], [[in_1]] : f32\n+// CHECK:               linalg.yield [[VAR_61_3_]] : f32\n+// CHECK:             } -> tensor<256xf32>\n+// CHECK-DAG:         [[VAR_53_:%.+]] = arith.index_cast [[VAR_2_]] : i32 to index\n+// CHECK-DAG:         [[VAR_54_:%.+]] = arith.index_cast [[VAR_arg12_1_]] : i32 to index\n+// CHECK:             [[VAR_55_:%.+]] = arith.addi [[VAR_53_]], [[VAR_54_]] : index\n+// CHECK-DAG:         [[VAR_reinterpret_cast_15_:%.+]] = memref.reinterpret_cast [[PARAM_1_]] to offset: {{.}}[[VAR_55_]]{{.}}, sizes: [256], strides: [1] : memref<*xf32> to memref<256xf32, strided<[1], offset: ?>>\n+// CHECK-DAG:         [[VAR_56_:%.+]] = arith.index_cast [[VAR_arg12_1_]] : i32 to index\n+// CHECK-NOT: separator of consecutive DAGs\n+// CHECK-DAG:         [[VAR_57_:%.+]] = arith.addi [[VAR_56_]], [[CST_256_]] : index\n+// CHECK-DAG:         [[VAR_58_:%.+]] = arith.index_cast [[PARAM_7_]] : i32 to index\n+// CHECK:             [[VAR_59_:%.+]] = arith.minsi [[VAR_57_]], [[VAR_58_]] : index\n+// CHECK:             [[VAR_60_:%.+]] = arith.subi [[VAR_59_]], [[VAR_56_]] : index\n+// CHECK-DAG:         [[VAR_extracted_slice_:%.+]] = tensor.extract_slice [[VAR_52_]][0] {{.}}[[VAR_60_]]{{.}} [1] : tensor<256xf32> to tensor<?xf32>\n+// CHECK-DAG:         [[VAR_subview_16_:%.+]] = memref.subview [[VAR_reinterpret_cast_15_]][0] {{.}}[[VAR_60_]]{{.}} [1] : memref<256xf32, strided<[1], offset: ?>> to memref<?xf32, strided<[1], offset: ?>>\n+// CHECK:             memref.tensor_store [[VAR_extracted_slice_]], [[VAR_subview_16_]] : memref<?xf32, strided<[1], offset: ?>>\n+// CHECK:           }\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/masked_ldst_1d.mlir", "status": "added", "additions": 45, "deletions": 0, "changes": 45, "file_content_changes": "@@ -0,0 +1,45 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+  %arg0 : !tt.ptr<bf16>,\n+  %arg1 : !tt.ptr<bf16>,\n+  %arg2 : i32\n+  )\n+  {\n+    %0 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<128x!tt.ptr<bf16>>\n+    %1 = tt.splat %arg1 : (!tt.ptr<bf16>) -> tensor<128x!tt.ptr<bf16>>\n+    %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+    %ldptr = tt.addptr %0, %2 : tensor<128x!tt.ptr<bf16>>, tensor<128xi32>\n+    %stptr = tt.addptr %1, %2 : tensor<128x!tt.ptr<bf16>>, tensor<128xi32>\n+    %nans = arith.constant dense<0xFF80> : tensor<128xbf16>\n+    %5 = tt.splat %arg2 : (i32) -> tensor<128xi32>\n+    %mask = arith.cmpi slt, %2, %5 : tensor<128xi32>\n+    %buff = tt.load %ldptr, %mask, %nans {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128xbf16>\n+    tt.store %stptr, %buff, %mask : tensor<128xbf16>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 0xFF80 : bf16\n+// CHECK-DAG:           %[[VAL_7:.*]] = arith.constant 128 : index\n+// CHECK:           %[[VAL_8:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [128], strides: [1] : memref<*xbf16> to memref<128xbf16, strided<[1]>>\n+// CHECK:           %[[VAL_9:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: [0], sizes: [128], strides: [1] : memref<*xbf16> to memref<128xbf16, strided<[1]>>\n+// CHECK:           %[[VAL_10:.*]] = memref.alloc() : memref<128xbf16>\n+// CHECK:           %[[VAL_11:.*]] = arith.index_cast %[[VAL_2]] : i32 to index\n+// CHECK:           %[[VAL_12:.*]] = arith.minsi %[[VAL_11]], %[[VAL_7]] : index\n+// CHECK:           %[[VAL_13:.*]] = memref.subview %[[VAL_8]][0] {{\\[}}%[[VAL_12]]] [1] : memref<128xbf16, strided<[1]>> to memref<?xbf16, strided<[1]>>\n+// CHECK:           %[[VAL_14:.*]] = memref.subview %[[VAL_10]][0] {{\\[}}%[[VAL_12]]] [1] : memref<128xbf16> to memref<?xbf16, strided<[1]>>\n+// CHECK:           %[[VAL_15:.*]] = arith.cmpi slt, %[[VAL_12]], %[[VAL_7]] : index\n+// CHECK:           scf.if %[[VAL_15]] {\n+// CHECK:             linalg.fill ins(%[[VAL_6]] : bf16) outs(%[[VAL_10]] : memref<128xbf16>)\n+// CHECK:           }\n+// CHECK:           memref.copy %[[VAL_13]], %[[VAL_14]] : memref<?xbf16, strided<[1]>> to memref<?xbf16, strided<[1]>>\n+// CHECK:           %[[VAL_16:.*]] = bufferization.to_tensor %[[VAL_10]] restrict writable : memref<128xbf16>\n+// CHECK:           %[[VAL_17:.*]] = arith.index_cast %[[VAL_2]] : i32 to index\n+// CHECK:           %[[VAL_18:.*]] = arith.minsi %[[VAL_17]], %[[VAL_7]] : index\n+// CHECK:           %[[VAL_19:.*]] = tensor.extract_slice %[[VAL_16]][0] {{\\[}}%[[VAL_18]]] [1] : tensor<128xbf16> to tensor<?xbf16>\n+// CHECK:           %[[VAL_20:.*]] = memref.subview %[[VAL_9]][0] {{\\[}}%[[VAL_18]]] [1] : memref<128xbf16, strided<[1]>> to memref<?xbf16, strided<[1]>>\n+// CHECK:           memref.tensor_store %[[VAL_19]], %[[VAL_20]] : memref<?xbf16, strided<[1]>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/masked_ldst_2d.mlir", "status": "added", "additions": 108, "deletions": 0, "changes": 108, "file_content_changes": "@@ -0,0 +1,108 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+  %arg0 : !tt.ptr<bf16>,\n+  %arg1 : !tt.ptr<bf16>,\n+  %arg2 : i32,\n+  %arg3 : i32\n+  )\n+  {\n+    // Mimic a scenario where the raw pointer points to a buffer with dimension (1024, 1024)\n+    // in row-major, but the actual tensor size is (arg2, arg3).\n+    // We are trying to load a 128x256 sub-buffer starting at (2, 3).\n+    // The resulting memref:\n+    //  offset = 3074\n+    //  size[1] = 128\n+    //  size[0] = 256\n+    //  stride[0] = 1024\n+    //  stride[1] = 1\n+    %0 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<128x256x!tt.ptr<bf16>>\n+    %1 = tt.splat %arg1 : (!tt.ptr<bf16>) -> tensor<128x256x!tt.ptr<bf16>>\n+    // horizontal index\n+    %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+    %c2 = arith.constant 2 : i32\n+    %c2tensor = tt.splat %c2 : (i32) -> tensor<128xi32>\n+    %offset2 = arith.addi %2, %c2tensor : tensor<128xi32>\n+    %3 = tt.expand_dims %offset2 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n+    %4 = tt.broadcast %3 : (tensor<128x1xi32>) -> tensor<128x256xi32>\n+    // vertical index\n+    %5 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32>\n+    %c3 = arith.constant 3 : i32\n+    %c3tensor = tt.splat %c3 : (i32) -> tensor<256xi32>\n+    %offset5 = arith.addi %5, %c3tensor : tensor<256xi32>\n+    %c1024 = arith.constant 1024 : i32\n+    %c1024tensor = tt.splat %c1024 : (i32) -> tensor<256xi32>\n+    %scale5 = arith.muli %offset5, %c1024tensor : tensor<256xi32>\n+    %6 = tt.expand_dims %scale5 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+    %7 = tt.broadcast %6 : (tensor<1x256xi32>) -> tensor<128x256xi32>\n+    // combined index\n+    %index = arith.addi %4, %7 : tensor<128x256xi32>\n+    %ldptr = tt.addptr %0, %index : tensor<128x256x!tt.ptr<bf16>>, tensor<128x256xi32>\n+    %stptr = tt.addptr %1, %index : tensor<128x256x!tt.ptr<bf16>>, tensor<128x256xi32>\n+    // other value for masked load\n+    %cnan = arith.constant 0xFF80 : bf16\n+    %nans = tt.splat %cnan : (bf16) -> tensor<128x256xbf16>\n+    // horizontal mask\n+    %8 = tt.splat %arg2 : (i32) -> tensor<128xi32>\n+    %9 = arith.cmpi slt, %offset2, %8 : tensor<128xi32>\n+    %10 = tt.expand_dims %9 {axis = 1 : i32} : (tensor<128xi1>) -> tensor<128x1xi1>\n+    %11 = tt.broadcast %10 : (tensor<128x1xi1>) -> tensor<128x256xi1>\n+    // vertical mask\n+    %12 = tt.splat %arg3 : (i32) -> tensor<256xi32>\n+    %13 = arith.cmpi slt, %offset5, %12 : tensor<256xi32>\n+    %14 = tt.expand_dims %13 {axis = 0 : i32} : (tensor<256xi1>) -> tensor<1x256xi1>\n+    %15 = tt.broadcast %14 : (tensor<1x256xi1>) -> tensor<128x256xi1>\n+    // combined mask\n+    %mask = arith.andi %11, %15 : tensor<128x256xi1>\n+    // dim0 = min(%arg2, 128), dim1 = min(%arg3, 256)\n+    // TODO: need reinterpret cast\n+    %buff = tt.load %ldptr, %mask, %nans {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x256xbf16>\n+    tt.store %stptr, %buff, %mask : tensor<128x256xbf16>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32, %[[VAL_6:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_7:.*]] = arith.constant 3074 : index\n+// CHECK-DAG:           %[[VAL_8:.*]] = arith.constant 1024 : index\n+// CHECK-DAG:           %[[VAL_9:.*]] = arith.constant 3 : index\n+// CHECK-DAG:           %[[VAL_10:.*]] = arith.constant 2 : index\n+// CHECK-DAG:           %[[VAL_11:.*]] = arith.constant 256 : index\n+// CHECK-DAG:           %[[VAL_12:.*]] = arith.constant 128 : index\n+// CHECK-DAG:           %[[VAL_13:.*]] = arith.constant 259 : index\n+// CHECK-DAG:           %[[VAL_14:.*]] = arith.constant 130 : index\n+// CHECK-DAG:           %[[VAL_15:.*]] = arith.constant 0xFF80 : bf16\n+// CHECK:           %[[VAL_16:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: {{\\[}}%[[VAL_7]]], sizes: [128, 256], strides: [1, %[[VAL_8]]] : memref<*xbf16> to memref<128x256xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           %[[VAL_17:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}%[[VAL_7]]], sizes: [128, 256], strides: [1, %[[VAL_8]]] : memref<*xbf16> to memref<128x256xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           %[[VAL_18:.*]] = memref.alloc() : memref<128x256xbf16>\n+// CHECK:           %[[VAL_19:.*]] = arith.index_cast %[[VAL_2]] : i32 to index\n+// CHECK:           %[[VAL_20:.*]] = arith.minsi %[[VAL_19]], %[[VAL_14]] : index\n+// CHECK:           %[[VAL_21:.*]] = arith.subi %[[VAL_20]], %[[VAL_10]] : index\n+// CHECK:           %[[VAL_22:.*]] = arith.index_cast %[[VAL_3]] : i32 to index\n+// CHECK:           %[[VAL_23:.*]] = arith.minsi %[[VAL_22]], %[[VAL_13]] : index\n+// CHECK:           %[[VAL_24:.*]] = arith.subi %[[VAL_23]], %[[VAL_9]] : index\n+// CHECK:           %[[VAL_25:.*]] = arith.minsi %[[VAL_21]], %[[VAL_12]] : index\n+// CHECK:           %[[VAL_26:.*]] = arith.minsi %[[VAL_24]], %[[VAL_11]] : index\n+// CHECK:           %[[VAL_27:.*]] = memref.subview %[[VAL_16]][0, 0] {{\\[}}%[[VAL_25]], %[[VAL_26]]] [1, 1] : memref<128x256xbf16, strided<[1, ?], offset: ?>> to memref<?x?xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           %[[VAL_28:.*]] = memref.subview %[[VAL_18]][0, 0] {{\\[}}%[[VAL_25]], %[[VAL_26]]] [1, 1] : memref<128x256xbf16> to memref<?x?xbf16, strided<[256, 1]>>\n+// CHECK:           %[[VAL_29:.*]] = arith.cmpi slt, %[[VAL_25]], %[[VAL_12]] : index\n+// CHECK:           %[[VAL_30:.*]] = arith.cmpi slt, %[[VAL_26]], %[[VAL_11]] : index\n+// CHECK:           %[[VAL_31:.*]] = arith.ori %[[VAL_29]], %[[VAL_30]] : i1\n+// CHECK:           scf.if %[[VAL_31]] {\n+// CHECK:             linalg.fill ins(%[[VAL_15]] : bf16) outs(%[[VAL_18]] : memref<128x256xbf16>)\n+// CHECK:           }\n+// CHECK:           memref.copy %[[VAL_27]], %[[VAL_28]] : memref<?x?xbf16, strided<[1, ?], offset: ?>> to memref<?x?xbf16, strided<[256, 1]>>\n+// CHECK:           %[[VAL_32:.*]] = bufferization.to_tensor %[[VAL_18]] restrict writable : memref<128x256xbf16>\n+// CHECK:           %[[VAL_33:.*]] = arith.index_cast %[[VAL_2]] : i32 to index\n+// CHECK:           %[[VAL_34:.*]] = arith.minsi %[[VAL_33]], %[[VAL_14]] : index\n+// CHECK:           %[[VAL_35:.*]] = arith.subi %[[VAL_34]], %[[VAL_10]] : index\n+// CHECK:           %[[VAL_36:.*]] = arith.index_cast %[[VAL_3]] : i32 to index\n+// CHECK:           %[[VAL_37:.*]] = arith.minsi %[[VAL_36]], %[[VAL_13]] : index\n+// CHECK:           %[[VAL_38:.*]] = arith.subi %[[VAL_37]], %[[VAL_9]] : index\n+// CHECK:           %[[VAL_39:.*]] = arith.minsi %[[VAL_35]], %[[VAL_12]] : index\n+// CHECK:           %[[VAL_40:.*]] = arith.minsi %[[VAL_38]], %[[VAL_11]] : index\n+// CHECK:           %[[VAL_41:.*]] = tensor.extract_slice %[[VAL_32]][0, 0] {{\\[}}%[[VAL_39]], %[[VAL_40]]] [1, 1] : tensor<128x256xbf16> to tensor<?x?xbf16>\n+// CHECK:           %[[VAL_42:.*]] = memref.subview %[[VAL_17]][0, 0] {{\\[}}%[[VAL_39]], %[[VAL_40]]] [1, 1] : memref<128x256xbf16, strided<[1, ?], offset: ?>> to memref<?x?xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           memref.tensor_store %[[VAL_41]], %[[VAL_42]] : memref<?x?xbf16, strided<[1, ?], offset: ?>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/masked_ldst_sitofp_other.mlir", "status": "added", "additions": 47, "deletions": 0, "changes": 47, "file_content_changes": "@@ -0,0 +1,47 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+  %arg0 : !tt.ptr<bf16>,\n+  %arg1 : !tt.ptr<bf16>,\n+  %arg2 : i32\n+  )\n+  {\n+    %0 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<128x!tt.ptr<bf16>>\n+    %1 = tt.splat %arg1 : (!tt.ptr<bf16>) -> tensor<128x!tt.ptr<bf16>>\n+    %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+    %ldptr = tt.addptr %0, %2 : tensor<128x!tt.ptr<bf16>>, tensor<128xi32>\n+    %stptr = tt.addptr %1, %2 : tensor<128x!tt.ptr<bf16>>, tensor<128xi32>\n+    %c7_i32 = arith.constant 7 : i32\n+    %splat_c7_i32 = tt.splat %c7_i32 : (i32) -> tensor<128xi32>\n+    %splat_c7_bf16 = arith.sitofp %splat_c7_i32 : tensor<128xi32> to tensor<128xbf16>\n+    %5 = tt.splat %arg2 : (i32) -> tensor<128xi32>\n+    %mask = arith.cmpi slt, %2, %5 : tensor<128xi32>\n+    %buff = tt.load %ldptr, %mask, %splat_c7_bf16 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128xbf16>\n+    tt.store %stptr, %buff, %mask : tensor<128xbf16>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 7.000000e+00 : bf16\n+// CHECK-DAG:           %[[VAL_7:.*]] = arith.constant 128 : index\n+// CHECK:           %[[VAL_8:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [128], strides: [1] : memref<*xbf16> to memref<128xbf16, strided<[1]>>\n+// CHECK:           %[[VAL_9:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: [0], sizes: [128], strides: [1] : memref<*xbf16> to memref<128xbf16, strided<[1]>>\n+// CHECK:           %[[VAL_10:.*]] = memref.alloc() : memref<128xbf16>\n+// CHECK:           %[[VAL_11:.*]] = arith.index_cast %[[VAL_2]] : i32 to index\n+// CHECK:           %[[VAL_12:.*]] = arith.minsi %[[VAL_11]], %[[VAL_7]] : index\n+// CHECK:           %[[VAL_13:.*]] = memref.subview %[[VAL_8]][0] {{\\[}}%[[VAL_12]]] [1] : memref<128xbf16, strided<[1]>> to memref<?xbf16, strided<[1]>>\n+// CHECK:           %[[VAL_14:.*]] = memref.subview %[[VAL_10]][0] {{\\[}}%[[VAL_12]]] [1] : memref<128xbf16> to memref<?xbf16, strided<[1]>>\n+// CHECK:           %[[VAL_15:.*]] = arith.cmpi slt, %[[VAL_12]], %[[VAL_7]] : index\n+// CHECK:           scf.if %[[VAL_15]] {\n+// CHECK:             linalg.fill ins(%[[VAL_6]] : bf16) outs(%[[VAL_10]] : memref<128xbf16>)\n+// CHECK:           }\n+// CHECK:           memref.copy %[[VAL_13]], %[[VAL_14]] : memref<?xbf16, strided<[1]>> to memref<?xbf16, strided<[1]>>\n+// CHECK:           %[[VAL_16:.*]] = bufferization.to_tensor %[[VAL_10]] restrict writable : memref<128xbf16>\n+// CHECK:           %[[VAL_17:.*]] = arith.index_cast %[[VAL_2]] : i32 to index\n+// CHECK:           %[[VAL_18:.*]] = arith.minsi %[[VAL_17]], %[[VAL_7]] : index\n+// CHECK:           %[[VAL_19:.*]] = tensor.extract_slice %[[VAL_16]][0] {{\\[}}%[[VAL_18]]] [1] : tensor<128xbf16> to tensor<?xbf16>\n+// CHECK:           %[[VAL_20:.*]] = memref.subview %[[VAL_9]][0] {{\\[}}%[[VAL_18]]] [1] : memref<128xbf16, strided<[1]>> to memref<?xbf16, strided<[1]>>\n+// CHECK:           memref.tensor_store %[[VAL_19]], %[[VAL_20]] : memref<?xbf16, strided<[1]>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/reducemax_32_256_bf16.mlir", "status": "added", "additions": 58, "deletions": 0, "changes": 58, "file_content_changes": "@@ -0,0 +1,58 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+    tt.func @kernel(%afloat : !tt.ptr<bf16>,\n+        %res : tensor<256x16x!tt.ptr<bf16>>\n+    ) -> () {\n+    // offset calculations\n+    %0 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>\n+    %c256 = arith.constant 256 : i32\n+    %ct256 = tt.splat %c256 : (i32) -> tensor<32xi32>\n+    %ws = arith.muli %ct256, %0 : tensor<32xi32>\n+    %1 = tt.expand_dims %ws {axis = 1 : i32} : (tensor<32xi32>) -> tensor<32x1xi32>\n+    %m2 = tt.broadcast %1 : (tensor<32x1xi32>) -> tensor<32x256xi32>\n+    %100 = tt.expand_dims %m2 {axis = 2 : i32} : (tensor<32x256xi32>) -> tensor<32x256x1xi32>\n+    %moff = tt.broadcast %100 : (tensor<32x256x1xi32>) -> tensor<32x256x16xi32>\n+    %33 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32>\n+    %34 = tt.expand_dims %33 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+    %k2 = tt.broadcast %34 : (tensor<1x256xi32>) -> tensor<32x256xi32>\n+    %200 = tt.expand_dims %k2 {axis = 2 : i32} : (tensor<32x256xi32>) -> tensor<32x256x1xi32>\n+    %koff = tt.broadcast %200 : (tensor<32x256x1xi32>) -> tensor<32x256x16xi32>\n+    %23 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32>\n+    %24 = tt.expand_dims %23 {axis = 0 : i32} : (tensor<16xi32>) -> tensor<1x16xi32>\n+    %n2 = tt.broadcast %24 : (tensor<1x16xi32>) -> tensor<256x16xi32>\n+    %300 = tt.expand_dims %n2 {axis = 0 : i32} : (tensor<256x16xi32>) -> tensor<1x256x16xi32>\n+    %noff = tt.broadcast %300 : (tensor<1x256x16xi32>) -> tensor<32x256x16xi32>\n+    %mkoff = arith.addi %moff, %koff : tensor<32x256x16xi32>\n+    %mknoff = arith.addi %mkoff, %noff : tensor<32x256x16xi32>\n+    // afloat pointer\n+    %8 = tt.splat %afloat : (!tt.ptr<bf16>) -> tensor<32x256x16x!tt.ptr<bf16>>\n+    %9 = tt.addptr %8, %mknoff : tensor<32x256x16x!tt.ptr<bf16>>, tensor<32x256x16xi32>\n+    %afm = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x256x16xbf16>\n+    %6 = \"tt.reduce\"(%afm) ({\n+    ^bb0(%arg5: bf16, %arg6: bf16):\n+      %21 = arith.cmpf ogt, %arg5, %arg6 : bf16\n+      %22 = arith.select %21, %arg5, %arg6 : bf16\n+      tt.reduce.return %22 : bf16\n+    }) {axis = 0 : i32} : (tensor<32x256x16xbf16>) -> tensor<256x16xbf16>\n+    tt.store %res, %6 {cache = 1 : i32, evict = 1 : i32} : tensor<256x16xbf16>\n+    tt.return\n+    }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<256x16xbf16>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_5:.*]] = arith.constant 256 : index\n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 0xFF80 : bf16\n+// CHECK:           %[[VAL_7:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [32, 256, 16], strides: {{\\[}}%[[VAL_5]], 1, 1] : memref<*xbf16> to memref<32x256x16xbf16, strided<[?, 1, 1]>>\n+// CHECK:           %[[VAL_8:.*]] = memref.alloc() : memref<32x256x16xbf16>\n+// CHECK:           memref.copy %[[VAL_7]], %[[VAL_8]] : memref<32x256x16xbf16, strided<[?, 1, 1]>> to memref<32x256x16xbf16>\n+// CHECK:           %[[VAL_9:.*]] = bufferization.to_tensor %[[VAL_8]] restrict writable : memref<32x256x16xbf16>\n+// CHECK:           %[[VAL_10:.*]] = tensor.empty() : tensor<256x16xbf16>\n+// CHECK:           %[[VAL_11:.*]] = linalg.fill ins(%[[VAL_6]] : bf16) outs(%[[VAL_10]] : tensor<256x16xbf16>) -> tensor<256x16xbf16>\n+// CHECK:           %[[VAL_12:.*]] = linalg.reduce ins(%[[VAL_9]] : tensor<32x256x16xbf16>) outs(%[[VAL_11]] : tensor<256x16xbf16>) dimensions = [0]\n+// CHECK:             (%[[VAL_13:.*]]: bf16, %[[VAL_14:.*]]: bf16) {\n+// CHECK:               %[[VAL_15:.*]] = arith.maxf %[[VAL_13]], %[[VAL_14]] : bf16\n+// CHECK:               linalg.yield %[[VAL_15]] : bf16\n+// CHECK:             }\n+// CHECK:           memref.tensor_store %[[VAL_12]], %[[VAL_1]] : memref<256x16xbf16>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/reducesum_512_256_bf16_axis0.mlir", "status": "added", "additions": 51, "deletions": 0, "changes": 51, "file_content_changes": "@@ -0,0 +1,51 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+    tt.func @kernel(%afloat : !tt.ptr<bf16>,\n+        %res : !tt.ptr<bf16>\n+    ) -> () {\n+    // offset calculations\n+    %0 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32>\n+    %c256 = arith.constant 256 : i32\n+    %ct256 = tt.splat %c256 : (i32) -> tensor<512xi32>\n+    %ws = arith.muli %ct256, %0 : tensor<512xi32>\n+    %1 = tt.expand_dims %ws {axis = 1 : i32} : (tensor<512xi32>) -> tensor<512x1xi32>\n+    %moff = tt.broadcast %1 : (tensor<512x1xi32>) -> tensor<512x256xi32>\n+    %3 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32>\n+    %4 = tt.expand_dims %3 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+    %koff = tt.broadcast %4 : (tensor<1x256xi32>) -> tensor<512x256xi32>\n+    %mkoff = arith.addi %moff, %koff : tensor<512x256xi32>\n+    // afloat pointer\n+    %8 = tt.splat %afloat : (!tt.ptr<bf16>) -> tensor<512x256x!tt.ptr<bf16>>\n+    %9 = tt.addptr %8, %mkoff : tensor<512x256x!tt.ptr<bf16>>, tensor<512x256xi32>\n+    // res pointer\n+    %18 = tt.splat %res : (!tt.ptr<bf16>) -> tensor<256x!tt.ptr<bf16>>\n+    %19 = tt.addptr %18, %3 : tensor<256x!tt.ptr<bf16>>, tensor<256xi32>\n+    %afm = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x256xbf16>\n+    %5 = \"tt.reduce\"(%afm) ({\n+    ^bb0(%arg5: bf16, %arg6: bf16):\n+      %21 = arith.addf %arg5, %arg6 : bf16\n+      tt.reduce.return %21 : bf16\n+    }) {axis = 0 : i32} : (tensor<512x256xbf16>) -> tensor<256xbf16>\n+    tt.store %19, %5 : tensor<256xbf16>\n+    tt.return\n+    }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_5:.*]] = arith.constant 256 : index\n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 0.000000e+00 : bf16\n+// CHECK:           %[[VAL_7:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [512, 256], strides: {{\\[}}%[[VAL_5]], 1] : memref<*xbf16> to memref<512x256xbf16, strided<[?, 1]>>\n+// CHECK:           %[[VAL_8:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: [0], sizes: [256], strides: [1] : memref<*xbf16> to memref<256xbf16, strided<[1]>>\n+// CHECK:           %[[VAL_9:.*]] = memref.alloc() : memref<512x256xbf16>\n+// CHECK:           memref.copy %[[VAL_7]], %[[VAL_9]] : memref<512x256xbf16, strided<[?, 1]>> to memref<512x256xbf16>\n+// CHECK:           %[[VAL_10:.*]] = bufferization.to_tensor %[[VAL_9]] restrict writable : memref<512x256xbf16>\n+// CHECK:           %[[VAL_11:.*]] = tensor.empty() : tensor<256xbf16>\n+// CHECK:           %[[VAL_12:.*]] = linalg.fill ins(%[[VAL_6]] : bf16) outs(%[[VAL_11]] : tensor<256xbf16>) -> tensor<256xbf16>\n+// CHECK:           %[[VAL_13:.*]] = linalg.reduce ins(%[[VAL_10]] : tensor<512x256xbf16>) outs(%[[VAL_12]] : tensor<256xbf16>) dimensions = [0]\n+// CHECK:             (%[[VAL_14:.*]]: bf16, %[[VAL_15:.*]]: bf16) {\n+// CHECK:               %[[VAL_16:.*]] = arith.addf %[[VAL_14]], %[[VAL_15]] : bf16\n+// CHECK:               linalg.yield %[[VAL_16]] : bf16\n+// CHECK:             }\n+// CHECK:           memref.tensor_store %[[VAL_13]], %[[VAL_8]] : memref<256xbf16, strided<[1]>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/reducesum_512_256_bf16_axis1.mlir", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -0,0 +1,53 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+    tt.func @kernel(%afloat : !tt.ptr<bf16>,\n+        %res : !tt.ptr<bf16>\n+    ) -> () {\n+    // offset calculations\n+    %0 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32>\n+    %c256 = arith.constant 256 : i32\n+    %ct256 = tt.splat %c256 : (i32) -> tensor<512xi32>\n+    %ws = arith.muli %ct256, %0 : tensor<512xi32>\n+    %1 = tt.expand_dims %ws {axis = 1 : i32} : (tensor<512xi32>) -> tensor<512x1xi32>\n+    %moff = tt.broadcast %1 : (tensor<512x1xi32>) -> tensor<512x256xi32>\n+    %3 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32>\n+    %4 = tt.expand_dims %3 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+    %koff = tt.broadcast %4 : (tensor<1x256xi32>) -> tensor<512x256xi32>\n+    %mkoff = arith.addi %moff, %koff : tensor<512x256xi32>\n+    // afloat pointer\n+    %8 = tt.splat %afloat : (!tt.ptr<bf16>) -> tensor<512x256x!tt.ptr<bf16>>\n+    %9 = tt.addptr %8, %mkoff : tensor<512x256x!tt.ptr<bf16>>, tensor<512x256xi32>\n+    // res pointer\n+    %18 = tt.splat %res : (!tt.ptr<bf16>) -> tensor<512x!tt.ptr<bf16>>\n+    %19 = tt.addptr %18, %0 : tensor<512x!tt.ptr<bf16>>, tensor<512xi32>\n+    %afm = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x256xbf16>\n+    %5 = \"tt.reduce\"(%afm) ({\n+    ^bb0(%arg5: bf16, %arg6: bf16):\n+      %21 = arith.addf %arg5, %arg6 : bf16\n+      tt.reduce.return %21 : bf16\n+    }) {axis = 1 : i32} : (tensor<512x256xbf16>) -> tensor<512xbf16>\n+    tt.store %19, %5 : tensor<512xbf16>\n+    tt.return\n+    }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_5:.*]] = arith.constant 256 : index\n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 0.000000e+00 : bf16\n+// CHECK:           %[[VAL_7:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [512, 256], strides: {{\\[}}%[[VAL_5]], 1] : memref<*xbf16> to memref<512x256xbf16, strided<[?, 1]>>\n+// CHECK:           %[[VAL_8:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: [0], sizes: [512], strides: [1] : memref<*xbf16> to memref<512xbf16, strided<[1]>>\n+// CHECK:           %[[VAL_9:.*]] = memref.alloc() : memref<512x256xbf16>\n+// CHECK:           memref.copy %[[VAL_7]], %[[VAL_9]] : memref<512x256xbf16, strided<[?, 1]>> to memref<512x256xbf16>\n+// CHECK:           %[[VAL_10:.*]] = bufferization.to_tensor %[[VAL_9]] restrict writable : memref<512x256xbf16>\n+// CHECK:           %[[VAL_11:.*]] = tensor.empty() : tensor<256x512xbf16>\n+// CHECK:           %[[VAL_12:.*]] = linalg.transpose ins(%[[VAL_10]] : tensor<512x256xbf16>) outs(%[[VAL_11]] : tensor<256x512xbf16>) permutation = [1, 0]\n+// CHECK:           %[[VAL_13:.*]] = tensor.empty() : tensor<512xbf16>\n+// CHECK:           %[[VAL_14:.*]] = linalg.fill ins(%[[VAL_6]] : bf16) outs(%[[VAL_13]] : tensor<512xbf16>) -> tensor<512xbf16>\n+// CHECK:           %[[VAL_15:.*]] = linalg.reduce ins(%[[VAL_12]] : tensor<256x512xbf16>) outs(%[[VAL_14]] : tensor<512xbf16>) dimensions = [0]\n+// CHECK:             (%[[VAL_16:.*]]: bf16, %[[VAL_17:.*]]: bf16) {\n+// CHECK:               %[[VAL_18:.*]] = arith.addf %[[VAL_16]], %[[VAL_17]] : bf16\n+// CHECK:               linalg.yield %[[VAL_18]] : bf16\n+// CHECK:             }\n+// CHECK:           memref.tensor_store %[[VAL_15]], %[[VAL_8]] : memref<512xbf16, strided<[1]>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/reducesum_512_256_f32_axis0.mlir", "status": "added", "additions": 51, "deletions": 0, "changes": 51, "file_content_changes": "@@ -0,0 +1,51 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+    tt.func @kernel(%afloat : !tt.ptr<f32>,\n+        %res : !tt.ptr<f32>\n+    ) -> () {\n+    // offset calculations\n+    %0 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32>\n+    %c256 = arith.constant 256 : i32\n+    %ct256 = tt.splat %c256 : (i32) -> tensor<512xi32>\n+    %ws = arith.muli %ct256, %0 : tensor<512xi32>\n+    %1 = tt.expand_dims %ws {axis = 1 : i32} : (tensor<512xi32>) -> tensor<512x1xi32>\n+    %moff = tt.broadcast %1 : (tensor<512x1xi32>) -> tensor<512x256xi32>\n+    %3 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32>\n+    %4 = tt.expand_dims %3 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+    %koff = tt.broadcast %4 : (tensor<1x256xi32>) -> tensor<512x256xi32>\n+    %mkoff = arith.addi %moff, %koff : tensor<512x256xi32>\n+    // afloat pointer\n+    %8 = tt.splat %afloat : (!tt.ptr<f32>) -> tensor<512x256x!tt.ptr<f32>>\n+    %9 = tt.addptr %8, %mkoff : tensor<512x256x!tt.ptr<f32>>, tensor<512x256xi32>\n+    // res pointer\n+    %18 = tt.splat %res : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n+    %19 = tt.addptr %18, %3 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n+    %afm = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x256xf32>\n+    %5 = \"tt.reduce\"(%afm) ({\n+    ^bb0(%arg5: f32, %arg6: f32):\n+      %21 = arith.addf %arg5, %arg6 : f32\n+      tt.reduce.return %21 : f32\n+    }) {axis = 0 : i32} : (tensor<512x256xf32>) -> tensor<256xf32>\n+    tt.store %19, %5 : tensor<256xf32>\n+    tt.return\n+    }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xf32>, %[[VAL_1:.*]]: memref<*xf32>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_5:.*]] = arith.constant 256 : index\n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 0.000000e+00 : f32\n+// CHECK:           %[[VAL_7:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [512, 256], strides: {{\\[}}%[[VAL_5]], 1] : memref<*xf32> to memref<512x256xf32, strided<[?, 1]>>\n+// CHECK:           %[[VAL_8:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: [0], sizes: [256], strides: [1] : memref<*xf32> to memref<256xf32, strided<[1]>>\n+// CHECK:           %[[VAL_9:.*]] = memref.alloc() : memref<512x256xf32>\n+// CHECK:           memref.copy %[[VAL_7]], %[[VAL_9]] : memref<512x256xf32, strided<[?, 1]>> to memref<512x256xf32>\n+// CHECK:           %[[VAL_10:.*]] = bufferization.to_tensor %[[VAL_9]] restrict writable : memref<512x256xf32>\n+// CHECK:           %[[VAL_11:.*]] = tensor.empty() : tensor<256xf32>\n+// CHECK:           %[[VAL_12:.*]] = linalg.fill ins(%[[VAL_6]] : f32) outs(%[[VAL_11]] : tensor<256xf32>) -> tensor<256xf32>\n+// CHECK:           %[[VAL_13:.*]] = linalg.reduce ins(%[[VAL_10]] : tensor<512x256xf32>) outs(%[[VAL_12]] : tensor<256xf32>) dimensions = [0]\n+// CHECK:             (%[[VAL_14:.*]]: f32, %[[VAL_15:.*]]: f32) {\n+// CHECK:               %[[VAL_16:.*]] = arith.addf %[[VAL_14]], %[[VAL_15]] : f32\n+// CHECK:               linalg.yield %[[VAL_16]] : f32\n+// CHECK:             }\n+// CHECK:           memref.tensor_store %[[VAL_13]], %[[VAL_8]] : memref<256xf32, strided<[1]>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/reducesum_512_256_f32_axis1.mlir", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -0,0 +1,53 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+    tt.func @kernel(%afloat : !tt.ptr<f32>,\n+        %res : !tt.ptr<f32>\n+    ) -> () {\n+    // offset calculations\n+    %0 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32>\n+    %c256 = arith.constant 256 : i32\n+    %ct256 = tt.splat %c256 : (i32) -> tensor<512xi32>\n+    %ws = arith.muli %ct256, %0 : tensor<512xi32>\n+    %1 = tt.expand_dims %ws {axis = 1 : i32} : (tensor<512xi32>) -> tensor<512x1xi32>\n+    %moff = tt.broadcast %1 : (tensor<512x1xi32>) -> tensor<512x256xi32>\n+    %3 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32>\n+    %4 = tt.expand_dims %3 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+    %koff = tt.broadcast %4 : (tensor<1x256xi32>) -> tensor<512x256xi32>\n+    %mkoff = arith.addi %moff, %koff : tensor<512x256xi32>\n+    // afloat pointer\n+    %8 = tt.splat %afloat : (!tt.ptr<f32>) -> tensor<512x256x!tt.ptr<f32>>\n+    %9 = tt.addptr %8, %mkoff : tensor<512x256x!tt.ptr<f32>>, tensor<512x256xi32>\n+    // res pointer\n+    %18 = tt.splat %res : (!tt.ptr<f32>) -> tensor<512x!tt.ptr<f32>>\n+    %19 = tt.addptr %18, %0 : tensor<512x!tt.ptr<f32>>, tensor<512xi32>\n+    %afm = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512x256xf32>\n+    %5 = \"tt.reduce\"(%afm) ({\n+    ^bb0(%arg5: f32, %arg6: f32):\n+      %21 = arith.addf %arg5, %arg6 : f32\n+      tt.reduce.return %21 : f32\n+    }) {axis = 1 : i32} : (tensor<512x256xf32>) -> tensor<512xf32>\n+    tt.store %19, %5 : tensor<512xf32>\n+    tt.return\n+    }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xf32>, %[[VAL_1:.*]]: memref<*xf32>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_5:.*]] = arith.constant 256 : index\n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 0.000000e+00 : f32\n+// CHECK:           %[[VAL_7:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [512, 256], strides: {{\\[}}%[[VAL_5]], 1] : memref<*xf32> to memref<512x256xf32, strided<[?, 1]>>\n+// CHECK:           %[[VAL_8:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: [0], sizes: [512], strides: [1] : memref<*xf32> to memref<512xf32, strided<[1]>>\n+// CHECK:           %[[VAL_9:.*]] = memref.alloc() : memref<512x256xf32>\n+// CHECK:           memref.copy %[[VAL_7]], %[[VAL_9]] : memref<512x256xf32, strided<[?, 1]>> to memref<512x256xf32>\n+// CHECK:           %[[VAL_10:.*]] = bufferization.to_tensor %[[VAL_9]] restrict writable : memref<512x256xf32>\n+// CHECK:           %[[VAL_11:.*]] = tensor.empty() : tensor<256x512xf32>\n+// CHECK:           %[[VAL_12:.*]] = linalg.transpose ins(%[[VAL_10]] : tensor<512x256xf32>) outs(%[[VAL_11]] : tensor<256x512xf32>) permutation = [1, 0]\n+// CHECK:           %[[VAL_13:.*]] = tensor.empty() : tensor<512xf32>\n+// CHECK:           %[[VAL_14:.*]] = linalg.fill ins(%[[VAL_6]] : f32) outs(%[[VAL_13]] : tensor<512xf32>) -> tensor<512xf32>\n+// CHECK:           %[[VAL_15:.*]] = linalg.reduce ins(%[[VAL_12]] : tensor<256x512xf32>) outs(%[[VAL_14]] : tensor<512xf32>) dimensions = [0]\n+// CHECK:             (%[[VAL_16:.*]]: f32, %[[VAL_17:.*]]: f32) {\n+// CHECK:               %[[VAL_18:.*]] = arith.addf %[[VAL_16]], %[[VAL_17]] : f32\n+// CHECK:               linalg.yield %[[VAL_18]] : f32\n+// CHECK:             }\n+// CHECK:           memref.tensor_store %[[VAL_15]], %[[VAL_8]] : memref<512xf32, strided<[1]>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/reducesum_middle_dim.mlir", "status": "added", "additions": 58, "deletions": 0, "changes": 58, "file_content_changes": "@@ -0,0 +1,58 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+    tt.func @kernel(%afloat : !tt.ptr<bf16>,\n+        %res : !tt.ptr<bf16>,\n+        %out: tensor<32x16x!tt.ptr<bf16>>\n+    ) -> () {\n+    // offset calculations\n+    %0 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>\n+    %c256 = arith.constant 256 : i32\n+    %ct256 = tt.splat %c256 : (i32) -> tensor<32xi32>\n+    %ws = arith.muli %ct256, %0 : tensor<32xi32>\n+    %1 = tt.expand_dims %ws {axis = 1 : i32} : (tensor<32xi32>) -> tensor<32x1xi32>\n+    %m2 = tt.broadcast %1 : (tensor<32x1xi32>) -> tensor<32x256xi32>\n+    %100 = tt.expand_dims %m2 {axis = 2 : i32} : (tensor<32x256xi32>) -> tensor<32x256x1xi32>\n+    %moff = tt.broadcast %100 : (tensor<32x256x1xi32>) -> tensor<32x256x16xi32>\n+    %33 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32>\n+    %34 = tt.expand_dims %33 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+    %k2 = tt.broadcast %34 : (tensor<1x256xi32>) -> tensor<32x256xi32>\n+    %200 = tt.expand_dims %k2 {axis = 2 : i32} : (tensor<32x256xi32>) -> tensor<32x256x1xi32>\n+    %koff = tt.broadcast %200 : (tensor<32x256x1xi32>) -> tensor<32x256x16xi32>\n+    %23 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32>\n+    %24 = tt.expand_dims %23 {axis = 0 : i32} : (tensor<16xi32>) -> tensor<1x16xi32>\n+    %n2 = tt.broadcast %24 : (tensor<1x16xi32>) -> tensor<256x16xi32>\n+    %300 = tt.expand_dims %n2 {axis = 0 : i32} : (tensor<256x16xi32>) -> tensor<1x256x16xi32>\n+    %noff = tt.broadcast %300 : (tensor<1x256x16xi32>) -> tensor<32x256x16xi32>\n+    %mkoff = arith.addi %moff, %koff : tensor<32x256x16xi32>\n+    %mknoff = arith.addi %mkoff, %noff : tensor<32x256x16xi32>\n+    // afloat pointer\n+    %8 = tt.splat %afloat : (!tt.ptr<bf16>) -> tensor<32x256x16x!tt.ptr<bf16>>\n+    %9 = tt.addptr %8, %mknoff : tensor<32x256x16x!tt.ptr<bf16>>, tensor<32x256x16xi32>\n+    %afm = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x256x16xbf16>\n+    %5 = \"tt.reduce\"(%afm) ({\n+    ^bb0(%arg5: bf16, %arg6: bf16):\n+      %21 = arith.addf %arg5, %arg6 : bf16\n+      tt.reduce.return %21 : bf16\n+    }) {axis = 1 : i32} : (tensor<32x256x16xbf16>) -> tensor<32x16xbf16>\n+    tt.store %out, %5 : tensor<32x16xbf16>\n+    tt.return\n+    }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: memref<32x16xbf16>, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 256 : index\n+// CHECK-DAG:           %[[VAL_7:.*]] = arith.constant 0.000000e+00 : bf16\n+// CHECK:           %[[VAL_8:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [32, 256, 16], strides: {{\\[}}%[[VAL_6]], 1, 1] : memref<*xbf16> to memref<32x256x16xbf16, strided<[?, 1, 1]>>\n+// CHECK:           %[[VAL_9:.*]] = memref.alloc() : memref<32x256x16xbf16>\n+// CHECK:           memref.copy %[[VAL_8]], %[[VAL_9]] : memref<32x256x16xbf16, strided<[?, 1, 1]>> to memref<32x256x16xbf16>\n+// CHECK:           %[[VAL_10:.*]] = bufferization.to_tensor %[[VAL_9]] restrict writable : memref<32x256x16xbf16>\n+// CHECK:           %[[VAL_11:.*]] = tensor.empty() : tensor<32x16xbf16>\n+// CHECK:           %[[VAL_12:.*]] = linalg.fill ins(%[[VAL_7]] : bf16) outs(%[[VAL_11]] : tensor<32x16xbf16>) -> tensor<32x16xbf16>\n+// CHECK:           %[[VAL_13:.*]] = linalg.reduce ins(%[[VAL_10]] : tensor<32x256x16xbf16>) outs(%[[VAL_12]] : tensor<32x16xbf16>) dimensions = [1]\n+// CHECK:             (%[[VAL_14:.*]]: bf16, %[[VAL_15:.*]]: bf16) {\n+// CHECK:               %[[VAL_16:.*]] = arith.addf %[[VAL_14]], %[[VAL_15]] : bf16\n+// CHECK:               linalg.yield %[[VAL_16]] : bf16\n+// CHECK:             }\n+// CHECK:           memref.tensor_store %[[VAL_13]], %[[VAL_2]] : memref<32x16xbf16>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/reducesum_scalar.mlir", "status": "added", "additions": 38, "deletions": 0, "changes": 38, "file_content_changes": "@@ -0,0 +1,38 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(%afloat : !tt.ptr<bf16>, %res : !tt.ptr<bf16>)\n+  {\n+    %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+    %1 = tt.splat %afloat : (!tt.ptr<bf16>) -> tensor<128x!tt.ptr<bf16>>\n+    %2 = tt.addptr %1, %0 : tensor<128x!tt.ptr<bf16>>, tensor<128xi32>\n+    %afm = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128xbf16>\n+    %3 = \"tt.reduce\"(%afm) ({\n+    ^bb0(%arg5: bf16, %arg6: bf16):\n+      %21 = arith.addf %arg5, %arg6 : bf16\n+      tt.reduce.return %21 : bf16\n+    }) {axis = 0 : i32} : (tensor<128xbf16>) -> bf16\n+    tt.store %res, %3 : bf16\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32) { \n+// CHECK:           %[[VAL_5:.*]] = arith.constant 0.000000e+00 : f32\n+// CHECK:           %[[VAL_6:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [128], strides: [1] : memref<*xbf16> to memref<128xbf16, strided<[1]>>\n+// CHECK:           %[[VAL_7:.*]] = memref.alloc() : memref<128xbf16>\n+// CHECK:           memref.copy %[[VAL_6]], %[[VAL_7]] : memref<128xbf16, strided<[1]>> to memref<128xbf16>\n+// CHECK:           %[[VAL_8:.*]] = bufferization.to_tensor %[[VAL_7]] restrict writable : memref<128xbf16>\n+// CHECK:           %[[VAL_9:.*]] = bufferization.alloc_tensor() : tensor<f32>\n+// CHECK:           %[[VAL_10:.*]] = tensor.insert %[[VAL_5]] into %[[VAL_9]][] : tensor<f32>\n+// CHECK:           %[[VAL_11:.*]] = linalg.reduce ins(%[[VAL_8]] : tensor<128xbf16>) outs(%[[VAL_10]] : tensor<f32>) dimensions = [0]\n+// CHECK:             (%[[VAL_12:.*]]: bf16, %[[VAL_13:.*]]: f32) {\n+// CHECK:               %[[VAL_14:.*]] = arith.extf %[[VAL_12]] : bf16 to f32\n+// CHECK:               %[[VAL_15:.*]] = arith.addf %[[VAL_14]], %[[VAL_13]] : f32\n+// CHECK:               linalg.yield %[[VAL_15]] : f32\n+// CHECK:             }\n+// CHECK:           %[[VAL_16:.*]] = tensor.extract %[[VAL_11]][] : tensor<f32>\n+// CHECK:           %[[VAL_17:.*]] = arith.truncf %[[VAL_16]] : f32 to bf16\n+// CHECK:           %[[VAL_18:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: [0], sizes: [1], strides: [1] : memref<*xbf16> to memref<1xbf16, strided<[1]>>\n+// CHECK:           affine.store %[[VAL_17]], %[[VAL_18]][0] : memref<1xbf16, strided<[1]>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/use_dot_opc.mlir", "status": "added", "additions": 73, "deletions": 0, "changes": 73, "file_content_changes": "@@ -0,0 +1,73 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+    %arg0 : !tt.ptr<bf16>,\n+    %arg1 : !tt.ptr<bf16>,\n+    %arg2 : !tt.ptr<bf16>\n+  )\n+  {\n+    %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+    %c64 = arith.constant 128 : i32\n+    %1 = tt.splat %c64 : (i32) -> tensor<128xi32>\n+    %2 = arith.muli %0, %1 : tensor<128xi32>\n+    %3 = tt.expand_dims %2 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n+    %4 = tt.broadcast %3 : (tensor<128x1xi32>) -> tensor<128x64xi32>\n+    %5 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %6 = tt.expand_dims %5 {axis = 0 : i32} : (tensor<64xi32>) -> tensor<1x64xi32>\n+    %7 = tt.broadcast %6 : (tensor<1x64xi32>) -> tensor<128x64xi32>\n+    %8 = arith.addi %4, %7 : tensor<128x64xi32>\n+    %10 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32>\n+    %11 = tt.expand_dims %10 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+    %12 = tt.broadcast %11 : (tensor<1x256xi32>) -> tensor<64x256xi32>\n+    %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %c256 = arith.constant 256 : i32\n+    %14 = tt.splat %c256 : (i32) -> tensor<64xi32>\n+    %15 = arith.muli %13, %14 : tensor<64xi32>\n+    %16 = tt.expand_dims %15 {axis = 1 : i32} : (tensor<64xi32>) -> tensor<64x1xi32>\n+    %17 = tt.broadcast %16 : (tensor<64x1xi32>) -> tensor<64x256xi32>\n+    %18 = arith.addi %12, %17 : tensor<64x256xi32>\n+    %20 = tt.splat %c256 : (i32) -> tensor<128xi32>\n+    %21 = arith.muli %0, %20 : tensor<128xi32>\n+    %22 = tt.expand_dims %21 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n+    %23 = tt.broadcast %22 : (tensor<128x1xi32>) -> tensor<128x256xi32>\n+    %24 = tt.expand_dims %10 {axis = 0 : i32} : (tensor<256xi32>) -> tensor<1x256xi32>\n+    %25 = tt.broadcast %24 {axis = 0 : i32} : (tensor<1x256xi32>) -> tensor<128x256xi32>\n+    %26 = arith.addi %23, %25 : tensor<128x256xi32>\n+    %30 = tt.splat %arg0 : (!tt.ptr<bf16>) -> tensor<128x64x!tt.ptr<bf16>>\n+    %31 = tt.addptr %30, %8 : tensor<128x64x!tt.ptr<bf16>>, tensor<128x64xi32>\n+    %32 = tt.load %31 {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<128x64xbf16>\n+    %40 = tt.splat %arg1 : (!tt.ptr<bf16>) -> tensor<64x256x!tt.ptr<bf16>>\n+    %41 = tt.addptr %40, %18 : tensor<64x256x!tt.ptr<bf16>>, tensor<64x256xi32>\n+    %42 = tt.load %41 {cache = 1 : i32, evict = 1 : i32, isVolatile = false}: tensor<64x256xbf16>\n+    %50 = tt.splat %arg2 : (!tt.ptr<bf16>) -> tensor<128x256x!tt.ptr<bf16>>\n+    %51 = tt.addptr %50, %26 : tensor<128x256x!tt.ptr<bf16>>, tensor<128x256xi32>\n+    %cf0 = arith.constant 0.0 : bf16\n+    %71 = tt.splat %cf0 : (bf16) -> (tensor<128x256xbf16>)\n+    %60 = tt.dot %32, %42, %71 {allowTF32 = false} : tensor<128x64xbf16> * tensor<64x256xbf16> -> tensor<128x256xbf16>\n+    tt.store %51, %60 : tensor<128x256xbf16>\n+    tt.store %51, %71 : tensor<128x256xbf16>\n+    tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: memref<*xbf16>, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 256 : index\n+// CHECK-DAG:           %[[VAL_7:.*]] = arith.constant 128 : index\n+// CHECK-DAG:           %[[VAL_8:.*]] = arith.constant 0.000000e+00 : bf16\n+// CHECK:           %[[VAL_16:.*]] = tensor.empty() : tensor<128x256xbf16>\n+// CHECK:           %[[VAL_17:.*]] = linalg.fill ins(%[[VAL_8]] : bf16) outs(%[[VAL_16]] : tensor<128x256xbf16>) -> tensor<128x256xbf16>\n+// CHECK:           %[[VAL_9:.*]] = memref.reinterpret_cast %[[VAL_0]] to offset: [0], sizes: [128, 64], strides: {{\\[}}%[[VAL_7]], 1] : memref<*xbf16> to memref<128x64xbf16, strided<[?, 1]>>\n+// CHECK:           %[[VAL_10:.*]] = memref.alloc() : memref<128x64xbf16>\n+// CHECK:           memref.copy %[[VAL_9]], %[[VAL_10]] : memref<128x64xbf16, strided<[?, 1]>> to memref<128x64xbf16>\n+// CHECK:           %[[VAL_11:.*]] = bufferization.to_tensor %[[VAL_10]] restrict writable : memref<128x64xbf16>\n+// CHECK:           %[[VAL_12:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: [0], sizes: [64, 256], strides: {{\\[}}%[[VAL_6]], 1] : memref<*xbf16> to memref<64x256xbf16, strided<[?, 1]>>\n+// CHECK:           %[[VAL_13:.*]] = memref.alloc() : memref<64x256xbf16>\n+// CHECK:           memref.copy %[[VAL_12]], %[[VAL_13]] : memref<64x256xbf16, strided<[?, 1]>> to memref<64x256xbf16>\n+// CHECK:           %[[VAL_14:.*]] = bufferization.to_tensor %[[VAL_13]] restrict writable : memref<64x256xbf16>\n+// CHECK:           %[[VAL_15:.*]] = memref.reinterpret_cast %[[VAL_2]] to offset: [0], sizes: [128, 256], strides: {{\\[}}%[[VAL_6]], 1] : memref<*xbf16> to memref<128x256xbf16, strided<[?, 1]>>\n+// CHECK:           %[[VAL_18:.*]] = tensor.empty() : tensor<128x256xbf16>\n+// CHECK:           %[[VAL_19:.*]] = linalg.matmul ins(%[[VAL_11]], %[[VAL_14]] : tensor<128x64xbf16>, tensor<64x256xbf16>) outs(%[[VAL_18]] : tensor<128x256xbf16>) -> tensor<128x256xbf16>\n+// CHECK:           memref.tensor_store %[[VAL_19]], %[[VAL_15]] : memref<128x256xbf16, strided<[?, 1]>>\n+// CHECK:           memref.tensor_store %[[VAL_17]], %[[VAL_15]] : memref<128x256xbf16, strided<[?, 1]>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/use_end_chain.mlir", "status": "added", "additions": 91, "deletions": 0, "changes": 91, "file_content_changes": "@@ -0,0 +1,91 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+  %arg0 : !tt.ptr<bf16>,\n+  %arg1 : !tt.ptr<bf16>\n+  )\n+  {\n+  %0 = tt.make_range {end = 768 : i32, start = 512 : i32}:tensor<256xi32>\n+  // offset = [512] size = 256, stride = 1\n+  %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<256xi32>) -> tensor<256x1xi32>\n+  // offset = [512,0], size = [256,1], stride = [1,0]\n+  %2 = tt.broadcast %1 : (tensor<256x1xi32>) -> tensor<256x128xi32>\n+  // offset = [512,0], size = [256,128], stride = [1,0]\n+  %5 = tt.make_range {end = 1152 : i32, start = 1024 : i32}:tensor<128xi32>\n+  // offset = 1024, size = 128, stride = 1\n+  %6 = tt.expand_dims %5 {axis = 0 : i32} : (tensor<128xi32>) -> tensor<1x128xi32>\n+  // offset = [0,1024], size = [1,128], stride = [0,1]\n+  %7 = tt.broadcast %6 : (tensor<1x128xi32>) -> tensor<256x128xi32>\n+  // offset = [0,1024], size = [256,128], stride = [0,1]\n+  %c6 = arith.constant 6 : i32\n+  %splat6 = tt.splat %c6 : (i32) -> tensor<256x128xi32>\n+  %scale7 = arith.muli %7, %splat6 : tensor<256x128xi32>\n+  // offset = [0,6144], size = [256,128], stride = [0,6]\n+  %14 = arith.addi %2, %scale7 : tensor<256x128xi32>\n+  // offset = [512,6144], size = [256,128], stride = [1,6]\n+  // mixed use\n+  %17 = tt.splat %arg1 : (!tt.ptr<bf16>) -> tensor<256x128x!tt.ptr<bf16>>\n+  %18 = tt.addptr %17, %14 : tensor<256x128x!tt.ptr<bf16>>, tensor<256x128xi32>\n+  %19 = tt.load %18 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x128xbf16>\n+  tt.store %18, %19 : tensor<256x128xbf16>\n+  %20 = arith.sitofp %14 : tensor<256x128xi32> to tensor<256x128xbf16>\n+  tt.store %18, %20 : tensor<256x128xbf16>\n+  tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: i32, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_6:.*]] = arith.constant 6 : index\n+// CHECK-DAG:           %[[VAL_7:.*]] = arith.constant 6 : i32\n+// CHECK:           %[[VAL_30:.*]] = tensor.empty() : tensor<256x128xi32>\n+// CHECK:           %[[VAL_31:.*]] = linalg.fill ins(%[[VAL_7]] : i32) outs(%[[VAL_30]] : tensor<256x128xi32>) -> tensor<256x128xi32>\n+// CHECK:           %[[VAL_8:.*]] = tensor.empty() : tensor<256xi32>\n+// CHECK:           %[[VAL_9:.*]] = linalg.generic {indexing_maps = [#map], iterator_types = [\"parallel\"]} outs(%[[VAL_8]] : tensor<256xi32>) {\n+// CHECK:           ^bb0(%[[VAL_10:.*]]: i32):\n+// CHECK:             %[[VAL_11:.*]] = linalg.index 0 : index\n+// CHECK:             %[[VAL_12:.*]] = arith.index_cast %[[VAL_11]] : index to i32\n+// CHECK:             linalg.yield %[[VAL_12]] : i32\n+// CHECK:           } -> tensor<256xi32>\n+// CHECK:           %[[VAL_13:.*]] = tensor.expand_shape %[[VAL_14:.*]] {{\\[\\[}}0, 1]] : tensor<256xi32> into tensor<256x1xi32>\n+// CHECK:           %[[VAL_15:.*]] = tensor.empty() : tensor<256x128xi32>\n+// CHECK:           %[[VAL_16:.*]] = linalg.generic {indexing_maps = [#map1, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_13]] : tensor<256x1xi32>) outs(%[[VAL_15]] : tensor<256x128xi32>) attrs =  {broadcastDims = array<i64: 1>} {\n+// CHECK:           ^bb0(%[[VAL_17:.*]]: i32, %[[VAL_18:.*]]: i32):\n+// CHECK:             linalg.yield %[[VAL_17]] : i32\n+// CHECK:           } -> tensor<256x128xi32>\n+// CHECK:           %[[VAL_19:.*]] = tensor.empty() : tensor<128xi32>\n+// CHECK:           %[[VAL_20:.*]] = linalg.generic {indexing_maps = [#map], iterator_types = [\"parallel\"]} outs(%[[VAL_19]] : tensor<128xi32>) {\n+// CHECK:           ^bb0(%[[VAL_21:.*]]: i32):\n+// CHECK:             %[[VAL_22:.*]] = linalg.index 0 : index\n+// CHECK:             %[[VAL_23:.*]] = arith.index_cast %[[VAL_22]] : index to i32\n+// CHECK:             linalg.yield %[[VAL_23]] : i32\n+// CHECK:           } -> tensor<128xi32>\n+// CHECK:           %[[VAL_24:.*]] = tensor.expand_shape %[[VAL_25:.*]] {{\\[\\[}}0, 1]] : tensor<128xi32> into tensor<1x128xi32>\n+// CHECK:           %[[VAL_26:.*]] = tensor.empty() : tensor<256x128xi32>\n+// CHECK:           %[[VAL_27:.*]] = linalg.generic {indexing_maps = [#map3, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_24]] : tensor<1x128xi32>) outs(%[[VAL_26]] : tensor<256x128xi32>) attrs =  {broadcastDims = array<i64: 0>} {\n+// CHECK:           ^bb0(%[[VAL_28:.*]]: i32, %[[VAL_29:.*]]: i32):\n+// CHECK:             linalg.yield %[[VAL_28]] : i32\n+// CHECK:           } -> tensor<256x128xi32>\n+// CHECK:           %[[VAL_32:.*]] = linalg.generic {indexing_maps = [#map2, #map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_33:.*]], %[[VAL_31]] : tensor<256x128xi32>, tensor<256x128xi32>) outs(%[[VAL_33]] : tensor<256x128xi32>) {\n+// CHECK:           ^bb0(%[[VAL_34:.*]]: i32, %[[VAL_35:.*]]: i32, %[[VAL_36:.*]]: i32):\n+// CHECK:             %[[VAL_37:.*]] = arith.muli %[[VAL_34]], %[[VAL_35]] : i32\n+// CHECK:             linalg.yield %[[VAL_37]] : i32\n+// CHECK:           } -> tensor<256x128xi32>\n+// CHECK:           %[[VAL_38:.*]] = linalg.generic {indexing_maps = [#map2, #map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_39:.*]], %[[VAL_40:.*]] : tensor<256x128xi32>, tensor<256x128xi32>) outs(%[[VAL_39]] : tensor<256x128xi32>) {\n+// CHECK:           ^bb0(%[[VAL_41:.*]]: i32, %[[VAL_42:.*]]: i32, %[[VAL_43:.*]]: i32):\n+// CHECK:             %[[VAL_44:.*]] = arith.addi %[[VAL_41]], %[[VAL_42]] : i32\n+// CHECK:             linalg.yield %[[VAL_44]] : i32\n+// CHECK:           } -> tensor<256x128xi32>\n+// CHECK:           %[[VAL_45:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}6656], sizes: [256, 128], strides: [1, %[[VAL_6]]] : memref<*xbf16> to memref<256x128xbf16, strided<[1, ?], offset: 6656>>\n+// CHECK:           %[[VAL_46:.*]] = memref.alloc() : memref<256x128xbf16>\n+// CHECK:           memref.copy %[[VAL_45]], %[[VAL_46]] : memref<256x128xbf16, strided<[1, ?], offset: 6656>> to memref<256x128xbf16>\n+// CHECK:           %[[VAL_47:.*]] = bufferization.to_tensor %[[VAL_46]] restrict writable : memref<256x128xbf16>\n+// CHECK:           memref.tensor_store %[[VAL_47]], %[[VAL_45]] : memref<256x128xbf16, strided<[1, ?], offset: 6656>>\n+// CHECK:           %[[VAL_48:.*]] = tensor.empty() : tensor<256x128xbf16>\n+// CHECK:           %[[VAL_49:.*]] = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_50:.*]] : tensor<256x128xi32>) outs(%[[VAL_48]] : tensor<256x128xbf16>) {\n+// CHECK:           ^bb0(%[[VAL_51:.*]]: i32, %[[VAL_52:.*]]: bf16):\n+// CHECK:             %[[VAL_53:.*]] = arith.sitofp %[[VAL_51]] : i32 to bf16\n+// CHECK:             linalg.yield %[[VAL_53]] : bf16\n+// CHECK:           } -> tensor<256x128xbf16>\n+// CHECK:           memref.tensor_store %[[VAL_54:.*]], %[[VAL_45]] : memref<256x128xbf16, strided<[1, ?], offset: 6656>>\n+// CHECK:           return\n+// CHECK:         }"}, {"filename": "test/Conversion/TritonToLinalg/use_mid_chain.mlir", "status": "added", "additions": 62, "deletions": 0, "changes": 62, "file_content_changes": "@@ -0,0 +1,62 @@\n+// RUN: triton-opt --triton-to-linalg %s | FileCheck %s\n+module {\n+  tt.func @kernel(\n+  %arg0 : !tt.ptr<bf16>,\n+  %arg1 : !tt.ptr<bf16>,\n+  %arg2 : !tt.ptr<i32>\n+  )\n+  {\n+  %0 = tt.make_range {end = 768 : i32, start = 512 : i32}:tensor<256xi32>\n+  // offset = [512] size = 256, stride = 1\n+  %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<256xi32>) -> tensor<256x1xi32>\n+  // offset = [512,0], size = [256,1], stride = [1,0]\n+  %2 = tt.broadcast %1 : (tensor<256x1xi32>) -> tensor<256x128xi32>\n+  // offset = [512,0], size = [256,128], stride = [1,0]\n+  // mixed use\n+  %5 = tt.make_range {end = 1152 : i32, start = 1024 : i32}:tensor<128xi32>\n+  // offset = 1024, size = 128, stride = 1\n+  %6 = tt.expand_dims %5 {axis = 0 : i32} : (tensor<128xi32>) -> tensor<1x128xi32>\n+  // offset = [0,1024], size = [1,128], stride = [0,1]\n+  %7 = tt.broadcast %6 : (tensor<1x128xi32>) -> tensor<256x128xi32>\n+  // offset = [0,1024], size = [256,128], stride = [0,1]\n+  %c6 = arith.constant 6 : i32\n+  %splat6 = tt.splat %c6 : (i32) -> tensor<256x128xi32>\n+  %scale7 = arith.muli %7, %splat6 : tensor<256x128xi32>\n+  // offset = [0,6144], size = [256,128], stride = [0,6]\n+  %14 = arith.addi %2, %scale7 : tensor<256x128xi32>\n+  // offset = [512,6144], size = [256,128], stride = [1,6]\n+  %17 = tt.splat %arg1 : (!tt.ptr<bf16>) -> tensor<256x128x!tt.ptr<bf16>>\n+  %18 = tt.addptr %17, %14 : tensor<256x128x!tt.ptr<bf16>>, tensor<256x128xi32>\n+  %19 = tt.load %18 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x128xbf16>\n+  tt.store %18, %19 : tensor<256x128xbf16>\n+  %20 = tt.splat %arg2 : (!tt.ptr<i32>) -> tensor<256x128x!tt.ptr<i32>>\n+  %21 = tt.addptr %20, %14 : tensor<256x128x!tt.ptr<i32>>, tensor<256x128xi32>\n+  tt.store %21, %2 : tensor<256x128xi32>\n+  tt.return\n+  }\n+}\n+// CHECK-LABEL:   func.func @kernel(\n+// CHECK-SAME:                      %[[VAL_0:.*]]: memref<*xbf16>, %[[VAL_1:.*]]: memref<*xbf16>, %[[VAL_2:.*]]: memref<*xi32>, %[[VAL_3:.*]]: i32, %[[VAL_4:.*]]: i32, %[[VAL_5:.*]]: i32) { \n+// CHECK-DAG:           %[[VAL_7:.*]] = arith.constant 6 : index\n+// CHECK:           %[[VAL_8:.*]] = tensor.empty() : tensor<256xi32>\n+// CHECK:           %[[VAL_9:.*]] = linalg.generic {indexing_maps = [#map], iterator_types = [\"parallel\"]} outs(%[[VAL_8]] : tensor<256xi32>) {\n+// CHECK:           ^bb0(%[[VAL_10:.*]]: i32):\n+// CHECK:             %[[VAL_11:.*]] = linalg.index 0 : index\n+// CHECK:             %[[VAL_12:.*]] = arith.index_cast %[[VAL_11]] : index to i32\n+// CHECK:             linalg.yield %[[VAL_12]] : i32\n+// CHECK:           } -> tensor<256xi32>\n+// CHECK:           %[[VAL_13:.*]] = tensor.expand_shape %[[VAL_14:.*]] {{\\[\\[}}0, 1]] : tensor<256xi32> into tensor<256x1xi32>\n+// CHECK:           %[[VAL_15:.*]] = tensor.empty() : tensor<256x128xi32>\n+// CHECK:           %[[VAL_16:.*]] = linalg.generic {indexing_maps = [#map1, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%[[VAL_13]] : tensor<256x1xi32>) outs(%[[VAL_15]] : tensor<256x128xi32>) attrs =  {broadcastDims = array<i64: 1>} {\n+// CHECK:           ^bb0(%[[VAL_17:.*]]: i32, %[[VAL_18:.*]]: i32):\n+// CHECK:             linalg.yield %[[VAL_17]] : i32\n+// CHECK:           } -> tensor<256x128xi32>\n+// CHECK:           %[[VAL_19:.*]] = memref.reinterpret_cast %[[VAL_1]] to offset: {{\\[}}6656], sizes: [256, 128], strides: [1, %[[VAL_7]]] : memref<*xbf16> to memref<256x128xbf16, strided<[1, ?], offset: 6656>>\n+// CHECK:           %[[VAL_20:.*]] = memref.alloc() : memref<256x128xbf16>\n+// CHECK:           memref.copy %[[VAL_19]], %[[VAL_20]] : memref<256x128xbf16, strided<[1, ?], offset: 6656>> to memref<256x128xbf16>\n+// CHECK:           %[[VAL_21:.*]] = bufferization.to_tensor %[[VAL_20]] restrict writable : memref<256x128xbf16>\n+// CHECK:           memref.tensor_store %[[VAL_21]], %[[VAL_19]] : memref<256x128xbf16, strided<[1, ?], offset: 6656>>\n+// CHECK:           %[[VAL_22:.*]] = memref.reinterpret_cast %[[VAL_2]] to offset: {{\\[}}6656], sizes: [256, 128], strides: [1, %[[VAL_7]]] : memref<*xi32> to memref<256x128xi32, strided<[1, ?], offset: 6656>>\n+// CHECK:           memref.tensor_store %[[VAL_23:.*]], %[[VAL_22]] : memref<256x128xi32, strided<[1, ?], offset: 6656>>\n+// CHECK:           return\n+// CHECK:         }"}]