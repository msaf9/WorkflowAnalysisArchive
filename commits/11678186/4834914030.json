[{"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -33,6 +33,8 @@ SmallVector<unsigned> getSizePerThread(Attribute layout);\n \n SmallVector<unsigned> getContigPerThread(Attribute layout);\n \n+SmallVector<unsigned> getUniqueContigPerThread(Type type);\n+\n SmallVector<unsigned> getThreadsPerCTA(Attribute layout);\n \n SmallVector<unsigned>"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -919,11 +919,13 @@ unsigned ModuleAxisInfoAnalysis::getPtrContiguity(Value ptr) {\n   auto order = triton::gpu::getOrder(layout);\n   unsigned align = getPtrAlignment(ptr);\n \n-  unsigned contigPerThread = triton::gpu::getSizePerThread(layout)[order[0]];\n-  contigPerThread = std::min(align, contigPerThread);\n-  contigPerThread = std::min<unsigned>(shape[order[0]], contigPerThread);\n+  auto uniqueContigPerThread = triton::gpu::getUniqueContigPerThread(tensorTy);\n+  assert(order[0] < uniqueContigPerThread.size() &&\n+         \"Unxpected uniqueContigPerThread size\");\n+  unsigned contiguity = uniqueContigPerThread[order[0]];\n+  contiguity = std::min(align, contiguity);\n \n-  return contigPerThread;\n+  return contiguity;\n }\n \n unsigned ModuleAxisInfoAnalysis::getPtrAlignment(Value ptr) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 8, "deletions": 4, "changes": 12, "file_content_changes": "@@ -106,13 +106,17 @@ struct ConvertLayoutOpConversion\n     if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n       unsigned dim = sliceLayout.getDim();\n       auto parentEncoding = sliceLayout.getParent();\n+      auto parentSizePerThread = getSizePerThread(parentEncoding);\n+      unsigned stride = 1;\n+      if (getOrder(parentEncoding)[0] == dim)\n+        stride = parentSizePerThread[dim];\n       auto parentShape = sliceLayout.paddedShape(shape);\n       auto parentTy = RankedTensorType::get(parentShape, type.getElementType(),\n                                             parentEncoding);\n-      auto multiDimOffsetParent =\n-          getMultiDimOffset(parentEncoding, loc, rewriter, elemId, parentTy,\n-                            sliceLayout.paddedShape(multiDimCTAInRepId),\n-                            sliceLayout.paddedShape(shapePerCTA));\n+      auto multiDimOffsetParent = getMultiDimOffset(\n+          parentEncoding, loc, rewriter, elemId * stride, parentTy,\n+          sliceLayout.paddedShape(multiDimCTAInRepId),\n+          sliceLayout.paddedShape(shapePerCTA));\n       SmallVector<Value> multiDimOffset(rank);\n       for (unsigned d = 0; d < rank + 1; ++d) {\n         if (d == dim)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 28, "deletions": 14, "changes": 42, "file_content_changes": "@@ -11,7 +11,7 @@\n #include \"Utility.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n-\n+#include <set>\n using namespace mlir;\n using namespace mlir::triton;\n \n@@ -533,6 +533,13 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n           result = emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, type);\n         if (mmaLayout.isAmpere())\n           result = emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, type);\n+      } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+        auto parentLayout = sliceLayout.getParent();\n+        auto parentShape = sliceLayout.paddedShape(type.getShape());\n+        RankedTensorType parentTy = RankedTensorType::get(\n+            parentShape, type.getElementType(), parentLayout);\n+        result = emitBaseIndexForLayout(loc, rewriter, parentLayout, parentTy);\n+        result.erase(result.begin() + sliceLayout.getDim());\n       } else {\n         llvm_unreachable(\"unsupported emitBaseIndexForLayout\");\n       }\n@@ -554,6 +561,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       if (mmaLayout.isAmpere())\n         return emitOffsetForMmaLayoutV2(mmaLayout, type);\n     }\n+    if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>())\n+      return emitOffsetForSliceLayout(sliceLayout, type);\n     llvm_unreachable(\"unsupported emitOffsetForLayout\");\n   }\n \n@@ -579,7 +588,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       } else if (auto mma = layout.dyn_cast<MmaEncodingAttr>()) {\n         result = emitIndicesForDistributedLayout(loc, b, mma, type);\n       } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n-        result = emitIndicesForSliceLayout(loc, b, slice, type);\n+        result = emitIndicesForDistributedLayout(loc, b, slice, type);\n       } else {\n         llvm_unreachable(\n             \"emitIndices for layouts other than blocked & slice not \"\n@@ -895,24 +904,29 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return multiDimIdx;\n   }\n \n-  SmallVector<SmallVector<Value>>\n-  emitIndicesForSliceLayout(Location loc, ConversionPatternRewriter &rewriter,\n-                            const SliceEncodingAttr &sliceLayout,\n-                            RankedTensorType type) const {\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForSliceLayout(const SliceEncodingAttr &sliceLayout,\n+                           RankedTensorType type) const {\n     auto parentEncoding = sliceLayout.getParent();\n     unsigned dim = sliceLayout.getDim();\n     auto parentShape = sliceLayout.paddedShape(type.getShape());\n     RankedTensorType parentTy = RankedTensorType::get(\n         parentShape, type.getElementType(), parentEncoding);\n-    auto parentIndices = emitIndices(loc, rewriter, parentEncoding, parentTy);\n-    unsigned numIndices = parentIndices.size();\n-    SmallVector<SmallVector<Value>> resultIndices;\n-    for (unsigned i = 0; i < numIndices; ++i) {\n-      SmallVector<Value> indices = parentIndices[i];\n-      indices.erase(indices.begin() + dim);\n-      resultIndices.push_back(indices);\n+    auto parentOffsets = emitOffsetForLayout(parentEncoding, parentTy);\n+\n+    unsigned numOffsets = parentOffsets.size();\n+    SmallVector<SmallVector<unsigned>> resultOffsets;\n+    std::set<SmallVector<unsigned>> uniqueOffsets;\n+\n+    for (unsigned i = 0; i < numOffsets; ++i) {\n+      SmallVector<unsigned> offsets = parentOffsets[i];\n+      offsets.erase(offsets.begin() + dim);\n+      if (uniqueOffsets.find(offsets) == uniqueOffsets.end()) {\n+        resultOffsets.push_back(offsets);\n+        uniqueOffsets.insert(offsets);\n+      }\n     }\n-    return resultIndices;\n+    return resultOffsets;\n   }\n \n protected:"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 52, "deletions": 12, "changes": 64, "file_content_changes": "@@ -116,23 +116,64 @@ struct CatOpConversion : public ConvertTritonGPUOpToLLVMPattern<CatOp> {\n   }\n };\n \n-template <typename SourceOp>\n-struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n-  using OpAdaptor = typename SourceOp::Adaptor;\n-  explicit ViewLikeOpConversion(TritonGPUToLLVMTypeConverter &typeConverter,\n-                                PatternBenefit benefit = 1)\n-      : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n+struct ViewOpConversion : public ConvertTritonGPUOpToLLVMPattern<ViewOp> {\n+  using OpAdaptor = typename ViewOp::Adaptor;\n+  explicit ViewOpConversion(TritonGPUToLLVMTypeConverter &typeConverter,\n+                            PatternBenefit benefit = 1)\n+      : ConvertTritonGPUOpToLLVMPattern<ViewOp>(typeConverter, benefit) {}\n \n   LogicalResult\n-  matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n+  matchAndRewrite(ViewOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n     auto resultTy = op.getType().template cast<RankedTensorType>();\n     auto vals = this->getTypeConverter()->unpackLLElements(\n         loc, adaptor.getSrc(), rewriter, op.getOperand().getType());\n-    Value view =\n+    Value ret =\n         this->getTypeConverter()->packLLElements(loc, vals, rewriter, resultTy);\n-    rewriter.replaceOp(op, view);\n+    rewriter.replaceOp(op, ret);\n+    return success();\n+  }\n+};\n+\n+struct ExpandDimsOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<ExpandDimsOp> {\n+  using OpAdaptor = typename ExpandDimsOp::Adaptor;\n+  explicit ExpandDimsOpConversion(TritonGPUToLLVMTypeConverter &typeConverter,\n+                                  PatternBenefit benefit = 1)\n+      : ConvertTritonGPUOpToLLVMPattern<ExpandDimsOp>(typeConverter, benefit) {}\n+\n+  LogicalResult\n+  matchAndRewrite(ExpandDimsOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    auto srcVals = this->getTypeConverter()->unpackLLElements(\n+        loc, adaptor.getSrc(), rewriter, op.getOperand().getType());\n+\n+    auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n+    auto resultTy = op.getType().template cast<RankedTensorType>();\n+\n+    assert(srcTy.getEncoding().isa<SliceEncodingAttr>() &&\n+           \"ExpandDimsOp only support SliceEncodingAttr\");\n+    auto srcLayout = srcTy.getEncoding().dyn_cast<SliceEncodingAttr>();\n+    auto resultLayout = resultTy.getEncoding();\n+\n+    auto srcOffsets = emitOffsetForLayout(srcLayout, srcTy);\n+    auto resultOffsets = emitOffsetForLayout(resultLayout, resultTy);\n+    DenseMap<SmallVector<unsigned>, Value, SmallVectorKeyInfo> srcValues;\n+    for (size_t i = 0; i < srcOffsets.size(); i++) {\n+      srcValues[srcOffsets[i]] = srcVals[i];\n+    }\n+\n+    SmallVector<Value> resultVals;\n+    for (size_t i = 0; i < resultOffsets.size(); i++) {\n+      auto offset = resultOffsets[i];\n+      offset.erase(offset.begin() + srcLayout.getDim());\n+      resultVals.push_back(srcValues.lookup(offset));\n+    }\n+    Value ret = this->getTypeConverter()->packLLElements(loc, resultVals,\n+                                                         rewriter, resultTy);\n+    rewriter.replaceOp(op, ret);\n     return success();\n   }\n };\n@@ -163,9 +204,8 @@ struct TransOpConversion\n void populateViewOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns,\n                                   PatternBenefit benefit) {\n-  patterns.add<ViewLikeOpConversion<triton::ViewOp>>(typeConverter, benefit);\n-  patterns.add<ViewLikeOpConversion<triton::ExpandDimsOp>>(typeConverter,\n-                                                           benefit);\n+  patterns.add<ViewOpConversion>(typeConverter, benefit);\n+  patterns.add<ExpandDimsOpConversion>(typeConverter, benefit);\n   patterns.add<SplatOpConversion>(typeConverter, benefit);\n   patterns.add<ArithConstantSplatOpConversion>(typeConverter, benefit);\n   patterns.add<CatOpConversion>(typeConverter, benefit);"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 36, "deletions": 4, "changes": 40, "file_content_changes": "@@ -103,10 +103,9 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n     return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n                                  blockedLayout.getSizePerThread().end());\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-    auto ret = getSizePerThread(sliceLayout.getParent());\n-    return ret;\n-    // ret.erase(ret.begin() + sliceLayout.getDim());\n-    return ret;\n+    auto sizePerThread = getSizePerThread(sliceLayout.getParent());\n+    sizePerThread.erase(sizePerThread.begin() + sliceLayout.getDim());\n+    return sizePerThread;\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     if (mmaLayout.isAmpere()) {\n       return {2, 2};\n@@ -146,11 +145,43 @@ SmallVector<unsigned> getContigPerThread(Attribute layout) {\n   if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     assert(mmaLayout.isVolta() || mmaLayout.isAmpere());\n     return {1, 2};\n+  } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    auto parentLayout = sliceLayout.getParent();\n+    return getContigPerThread(parentLayout);\n   } else {\n     return getSizePerThread(layout);\n   }\n }\n \n+SmallVector<unsigned> getUniqueContigPerThread(Type type) {\n+  if (type.isIntOrIndexOrFloat() || type.isa<triton::PointerType>())\n+    return SmallVector<unsigned>(1, 1);\n+  auto tensorType = type.cast<RankedTensorType>();\n+  auto shape = tensorType.getShape();\n+  // If slice layout, call recursively on parent layout, and drop\n+  // sliced dim\n+  if (auto sliceLayout =\n+          tensorType.getEncoding().dyn_cast<SliceEncodingAttr>()) {\n+    auto parentLayout = sliceLayout.getParent();\n+    auto parentShape = sliceLayout.paddedShape(shape);\n+    auto parentTy = RankedTensorType::get(\n+        parentShape, tensorType.getElementType(), parentLayout);\n+    auto parentUniqueContigPerThread = getUniqueContigPerThread(parentTy);\n+    parentUniqueContigPerThread.erase(parentUniqueContigPerThread.begin() +\n+                                      sliceLayout.getDim());\n+    return parentUniqueContigPerThread;\n+  }\n+  // Base case\n+  auto rank = shape.size();\n+  SmallVector<unsigned> ret(rank);\n+  auto contigPerThread = getContigPerThread(tensorType.getEncoding());\n+  assert(contigPerThread.size() == rank && \"Unexpected contigPerThread size\");\n+  for (int d = 0; d < rank; ++d) {\n+    ret[d] = std::min<unsigned>(shape[d], contigPerThread[d]);\n+  }\n+  return ret;\n+}\n+\n SmallVector<unsigned> getThreadsPerCTA(Attribute layout) {\n   SmallVector<unsigned> threads;\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n@@ -375,6 +406,7 @@ SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n   auto parent = getParent();\n   auto parentElemsPerThread =\n       ::getElemsPerThread(parent, paddedShape(shape), eltTy);\n+  parentElemsPerThread.erase(parentElemsPerThread.begin() + getDim());\n   return parentElemsPerThread;\n }\n unsigned SliceEncodingAttr::getTotalElemsPerThread(ArrayRef<int64_t> shape,"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -869,7 +869,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_blocked1d_to_slice1\n   tt.func @convert_blocked1d_to_slice1(%src:tensor<32xi32, #blocked0>) {\n-    // CHECK-COUNT-32: llvm.load {{.*}} : !llvm.ptr<vector<1xi32>, 3>\n+    // CHECK-COUNT-8: llvm.load {{.*}} : !llvm.ptr<vector<1xi32>, 3>\n     %cvt = triton_gpu.convert_layout %src : (tensor<32xi32, #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n     tt.return\n   }"}]