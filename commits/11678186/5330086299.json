[{"filename": ".github/workflows/Dockerfile", "status": "added", "additions": 37, "deletions": 0, "changes": 37, "file_content_changes": "@@ -0,0 +1,37 @@\n+FROM centos:7\n+ARG llvm_dir=llvm-project\n+\n+# Add the cache artifacts and the LLVM source tree to the container\n+ADD sccache /sccache\n+ADD \"${llvm_dir}\" /source/llvm-project\n+ENV SCCACHE_DIR=\"/sccache\"\n+ENV SCCACHE_CACHE_SIZE=\"2G\"\n+\n+# Install build dependencies\n+RUN yum install --assumeyes centos-release-scl\n+RUN yum install --assumeyes devtoolset-9-gcc* python3-devel python3-pip\n+SHELL [ \"/usr/bin/scl\", \"enable\", \"devtoolset-9\" ]\n+\n+RUN python3 -m pip install --upgrade pip\n+RUN python3 -m pip install --upgrade cmake ninja sccache\n+\n+# Install MLIR's Python Dependencies\n+RUN python3 -m pip install -r /source/llvm-project/mlir/python/requirements.txt\n+\n+# Configure, Build, Test, and Install LLVM\n+RUN cmake -GNinja -Bbuild \\\n+  -DCMAKE_BUILD_TYPE=Release \\\n+  -DCMAKE_C_COMPILER=gcc \\\n+  -DCMAKE_CXX_COMPILER=g++ \\\n+  -DCMAKE_C_COMPILER_LAUNCHER=sccache \\\n+  -DCMAKE_CXX_COMPILER_LAUNCHER=sccache \\\n+  -DCMAKE_INSTALL_PREFIX=\"/install\" \\\n+  -DLLVM_BUILD_UTILS=ON \\\n+  -DLLVM_ENABLE_ASSERTIONS=ON \\\n+  -DMLIR_ENABLE_BINDINGS_PYTHON=ON \\\n+  -DLLVM_ENABLE_PROJECTS=mlir \\\n+  -DLLVM_INSTALL_UTILS=ON \\\n+  -DLLVM_TARGETS_TO_BUILD=\"host;NVPTX;AMDGPU\" \\\n+  /source/llvm-project/llvm\n+\n+RUN ninja -C build check-mlir install"}, {"filename": ".github/workflows/llvm-build.yml", "status": "added", "additions": 195, "deletions": 0, "changes": 195, "file_content_changes": "@@ -0,0 +1,195 @@\n+name: LLVM Build\n+\n+on:\n+  push:\n+    branches:\n+      - llvm-head\n+    paths:\n+      - llvm-hash.txt\n+  workflow_dispatch:\n+\n+env:\n+  SCCACHE_DIR: ${{ github.workspace }}/sccache\n+\n+permissions:\n+  contents: read\n+  id-token: write\n+\n+jobs:\n+\n+  build:\n+\n+    strategy:\n+      fail-fast: true\n+\n+      matrix:\n+        platform: [\n+          ubuntu-20.04-x64,\n+          ubuntu-22.04-x64,\n+          centos-7-x64,\n+          macos-x64,\n+          macos-arm64\n+        ]\n+\n+        include:\n+          # Specify OS versions\n+          - platform: ubuntu-20.04-x64\n+            host-os: ubuntu-20.04\n+            target-os: ubuntu\n+            arch: x64\n+          - platform: ubuntu-22.04-x64\n+            host-os: ubuntu-22.04\n+            target-os: ubuntu\n+            arch: x64\n+          - platform: centos-7-x64\n+            host-os: ubuntu-22.04\n+            target-os: centos\n+            arch: x64\n+          - platform: macos-x64\n+            host-os: macos-12\n+            target-os: macos\n+            arch: x64\n+          - platform: macos-arm64\n+            host-os: macos-12\n+            target-os: macos\n+            arch: arm64\n+\n+    runs-on: ${{ matrix.host-os }}\n+\n+    steps:\n+\n+    - name: Checkout Repo\n+      uses: actions/checkout@v3\n+      with:\n+        path: llvm-build\n+\n+    - name: Fetch LLVM Commit Hash\n+      run: |\n+        LLVM_COMMIT_HASH=\"$(cat llvm-build/llvm-hash.txt)\"\n+        echo \"Found LLVM commit hash: ${LLVM_COMMIT_HASH}\"\n+        echo \"llvm_commit_hash=${LLVM_COMMIT_HASH}\" >> ${GITHUB_ENV}\n+\n+        SHORT_LLVM_COMMIT_HASH=\"${LLVM_COMMIT_HASH:0:8}\"\n+        echo \"Short LLVM commit hash: ${SHORT_LLVM_COMMIT_HASH}\"\n+        echo \"short_llvm_commit_hash=${SHORT_LLVM_COMMIT_HASH}\" >> ${GITHUB_ENV}\n+\n+        INSTALL_DIR=\"llvm-${SHORT_LLVM_COMMIT_HASH}-${{ matrix.platform }}\"\n+        echo \"LLVM installation directory name: ${INSTALL_DIR}\"\n+        echo \"llvm_install_dir=${INSTALL_DIR}\" >> ${GITHUB_ENV}\n+\n+    - name: Checkout LLVM\n+      uses: actions/checkout@v3\n+      with:\n+        repository: llvm/llvm-project\n+        path: llvm-project\n+        ref: ${{ env.llvm_commit_hash }}\n+\n+    - name: Set up Python\n+      uses: actions/setup-python@v4\n+      with:\n+        python-version: 3.11\n+\n+    - name: Install Prerequisites\n+      run: |\n+        python3 -m pip install cmake ninja sccache\n+        mkdir -p ${{ env.SCCACHE_DIR }}\n+        rm -rf ${{ env.SCCACHE_DIR }}/*\n+\n+    - name: Enable Cache\n+      uses: actions/cache@v3\n+      with:\n+        path: ${{ env.SCCACHE_DIR }}\n+        key: ${{ matrix.platform }}-${{ env.short_llvm_commit_hash }}\n+        restore-keys: ${{ matrix.platform }}-\n+\n+    - name: Configure, Build, Test, and Install LLVM (Ubuntu and macOS x64)\n+      if: matrix.arch == 'x64' && contains(fromJSON('[\"ubuntu\", \"macos\"]'), matrix.target-os)\n+      run: >\n+        python3 -m pip install -r llvm-project/mlir/python/requirements.txt\n+\n+        cmake -GNinja -Bllvm-project/build\n+        -DCMAKE_BUILD_TYPE=Release\n+        -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++\n+        -DCMAKE_C_COMPILER_LAUNCHER=sccache -DCMAKE_CXX_COMPILER_LAUNCHER=sccache\n+        -DCMAKE_INSTALL_PREFIX=\"${{ env.llvm_install_dir }}\"\n+        -DCMAKE_LINKER=lld\n+        -DLLVM_BUILD_UTILS=ON\n+        -DLLVM_ENABLE_ASSERTIONS=ON\n+        -DMLIR_ENABLE_BINDINGS_PYTHON=ON\n+        -DLLVM_ENABLE_PROJECTS=mlir\n+        -DLLVM_INSTALL_UTILS=ON\n+        -DLLVM_TARGETS_TO_BUILD=\"host;NVPTX;AMDGPU\"\n+        llvm-project/llvm\n+\n+        ninja -C llvm-project/build check-mlir install\n+\n+        tar czf \"${{ env.llvm_install_dir }}.tar.gz\" \"${{ env.llvm_install_dir }}\"\n+\n+    - name: Configure, Build, and Install LLVM (macOS arm64)\n+      if: matrix.arch == 'arm64' && matrix.target-os == 'macos'\n+      run: >\n+        python3 -m pip install -r llvm-project/mlir/python/requirements.txt\n+\n+        cmake -GNinja -Bllvm-project/build\n+        -DCMAKE_BUILD_TYPE=Release\n+        -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++\n+        -DCMAKE_C_COMPILER_LAUNCHER=sccache -DCMAKE_CXX_COMPILER_LAUNCHER=sccache\n+        -DCMAKE_INSTALL_PREFIX=\"${{ env.llvm_install_dir }}\"\n+        -DCMAKE_LINKER=lld\n+        -DCMAKE_OSX_ARCHITECTURES=arm64\n+        -DLLVM_BUILD_UTILS=ON\n+        -DLLVM_ENABLE_ASSERTIONS=ON\n+        -DMLIR_ENABLE_BINDINGS_PYTHON=ON\n+        -DLLVM_ENABLE_PROJECTS=mlir\n+        -DLLVM_ENABLE_ZSTD=OFF\n+        -DLLVM_INSTALL_UTILS=ON\n+        -DLLVM_TARGETS_TO_BUILD=\"AArch64\"\n+        -DLLVM_USE_HOST_TOOLS=ON\n+        llvm-project/llvm\n+\n+        ninja -C llvm-project/build install\n+\n+        tar czf \"${{ env.llvm_install_dir }}.tar.gz\" \"${{ env.llvm_install_dir }}\"\n+\n+    - name: Configure, Build, Test, and Install LLVM (CentOS)\n+      if: matrix.target-os == 'centos'\n+      run: |\n+        docker build --tag llvm-build --build-arg llvm_dir=llvm-project \\\n+          -f llvm-build/.github/workflows/Dockerfile .\n+\n+        # Create temporary container to copy cache and installed artifacts.\n+        CONTAINER_ID=$(docker create llvm-build)\n+        docker cp \"${CONTAINER_ID}:/install\" \"${{ env.llvm_install_dir }}\"\n+        tar czf \"${{ env.llvm_install_dir }}.tar.gz\" \"${{ env.llvm_install_dir }}\"\n+\n+        # We remove the existing directory, otherwise docker will\n+        # create a subdirectory inside the existing directory.\n+        rm -rf \"${{ env.SCCACHE_DIR }}\"\n+        docker cp \"${CONTAINER_ID}:/sccache\" \"${{ env.SCCACHE_DIR }}\"\n+        sudo chown -R \"$(id -u -n):$(id -g -n)\" \"${{ env.SCCACHE_DIR }}\"\n+\n+        docker rm \"${CONTAINER_ID}\"\n+\n+    - name: Azure Login\n+      uses: azure/login@v1\n+      with:\n+        client-id: ${{ secrets.AZURE_CLIENT_ID }}\n+        tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n+        subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n+\n+    - name: Upload LLVM Artifacts to Azure\n+      run: |\n+        az storage blob upload --account-name tritonlang --auth-mode login --container-name llvm-builds --file \"${{ env.llvm_install_dir }}.tar.gz\" --name \"${{ env.llvm_install_dir }}.tar.gz\" --overwrite\n+\n+        URL=$(az storage blob url --account-name tritonlang --auth-mode login --container-name llvm-builds --name \"${{ env.llvm_install_dir }}.tar.gz\")\n+        echo \"Blob URL: ${URL}\"\n+\n+    - name: Azure Logout\n+      run: |\n+        az logout\n+        az cache purge\n+        az account clear\n+      if: always()\n+\n+    - name: Dump Sccache Statistics\n+      run: sccache --show-stats"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 9, "deletions": 4, "changes": 13, "file_content_changes": "@@ -2484,11 +2484,11 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # -------------\n \n \n-@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and\"])\n+@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and_dynamic\", \"if_and_static\"])\n def test_if(if_type):\n \n     @triton.jit\n-    def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr):\n+    def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr, StaticVaue: tl.constexpr):\n         pid = tl.program_id(0)\n         cond = tl.load(Cond)\n         if IfType == \"if\":\n@@ -2498,17 +2498,22 @@ def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr\n                 tl.store(Ret, tl.load(XFalse))\n         elif IfType == \"if_exp\":\n             tl.store(Ret, tl.load(XTrue)) if pid % 2 else tl.store(Ret, tl.load(XFalse))\n-        elif IfType == \"if_and\":\n+        elif IfType == \"if_and_dynamic\":\n             if BoolVar and pid % 2 == 0:\n                 tl.store(Ret, tl.load(XTrue))\n             else:\n                 tl.store(Ret, tl.load(XFalse))\n+        elif IfType == \"if_and_static\":\n+            if StaticVaue != 0 and StaticVaue != 0:\n+                tl.store(Ret, tl.load(XTrue))\n+            else:\n+                tl.store(Ret, tl.load(XFalse))\n \n     cond = torch.ones(1, dtype=torch.int32, device='cuda')\n     x_true = torch.tensor([3.14], dtype=torch.float32, device='cuda')\n     x_false = torch.tensor([1.51], dtype=torch.float32, device='cuda')\n     ret = torch.empty(1, dtype=torch.float32, device='cuda')\n-    kernel[(1,)](cond, x_true, x_false, ret, if_type, True)\n+    kernel[(1,)](cond, x_true, x_false, ret, if_type, True, 1)\n     assert torch.equal(ret, x_true)\n \n "}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "file_content_changes": "@@ -595,12 +595,14 @@ def visit_Pass(self, node):\n     def visit_Compare(self, node):\n         if not (len(node.comparators) == 1 and len(node.ops) == 1):\n             raise UnsupportedLanguageConstruct(None, node, \"simultaneous multiple comparison is not supported\")\n-        lhs = _unwrap_if_constexpr(self.visit(node.left))\n-        rhs = _unwrap_if_constexpr(self.visit(node.comparators[0]))\n+        lhs = self.visit(node.left)\n+        rhs = self.visit(node.comparators[0])\n+        lhs_value = _unwrap_if_constexpr(lhs)\n+        rhs_value = _unwrap_if_constexpr(rhs)\n         if type(node.ops[0]) == ast.Is:\n-            return constexpr(lhs is rhs)\n+            return constexpr(lhs_value is rhs_value)\n         if type(node.ops[0]) == ast.IsNot:\n-            return constexpr(lhs is not rhs)\n+            return constexpr(lhs_value is not rhs_value)\n         method_name = self._method_name_for_comp_op.get(type(node.ops[0]))\n         if method_name is None:\n             raise UnsupportedLanguageConstruct(None, node, \"AST comparison operator '{}' is not (currently) implemented.\".format(node.ops[0].__name__))\n@@ -988,7 +990,7 @@ def execute_static_assert(self, node: ast.Call) -> None:\n         if not (0 < arg_count <= 2) or len(node.keywords):\n             raise TypeError(\"`static_assert` requires one or two positional arguments only\")\n \n-        passed = self.visit(node.args[0])\n+        passed = _unwrap_if_constexpr(self.visit(node.args[0]))\n         if not isinstance(passed, bool):\n             raise NotImplementedError(\"Assertion condition could not be determined at compile-time. Make sure that it depends only on `constexpr` values\")\n         if not passed:"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -65,6 +65,7 @@ def optimize_ttir(mod, arch):\n \n def ttir_to_ttgir(mod, num_warps):\n     pm = ir.pass_manager(mod.context)\n+    pm.enable_debug()\n     pm.add_convert_triton_to_tritongpu_pass(num_warps)\n     pm.run(mod)\n     return mod\n@@ -384,7 +385,6 @@ def compile(fn, **kwargs):\n     is_cuda = device_type == \"cuda\" and _is_cuda(arch)\n     is_hip = device_type in [\"cuda\", \"hip\"] and not is_cuda\n     context = ir.context()\n-    asm = dict()\n     constants = kwargs.get(\"constants\", dict())\n     num_warps = kwargs.get(\"num_warps\", 4)\n     num_stages = kwargs.get(\"num_stages\", 3 if is_cuda and arch >= 75 else 2)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 35, "deletions": 4, "changes": 39, "file_content_changes": "@@ -1521,23 +1521,43 @@ def max_contiguous(input, values, _builder=None):\n @builtin\n def static_print(*values, sep: str = \" \", end: str = \"\\n\", file=None, flush=False, _builder=None):\n     '''\n-    Print the values at compile time. The parameters are the same as the builtin :code:`print`.\n+    Print the values at compile time. The parameters are the same as the Python builtin :code:`print`.\n+\n+    Calling the Python builtin :code:`print` inside your kernel is the same as calling this.\n+\n+    .. highlight:: python\n+    .. code-block:: python\n+\n+        tl.static_print(f\"{BLOCK_SIZE=}\")\n+        print(f\"{BLOCK_SIZE=}\")\n     '''\n     pass\n \n \n @builtin\n def static_assert(cond, msg=\"\", _builder=None):\n     '''\n-    Assert the condition at compile time. The parameters are the same as the builtin :code:`assert`.\n+    Assert the condition at compile time.  Does not require that the :code:`TRITON_DEBUG` environment variable\n+    is set.\n+\n+    .. highlight:: python\n+    .. code-block:: python\n+\n+        tl.static_assert(BLOCK_SIZE == 1024)\n     '''\n     pass\n \n \n @builtin\n def device_print(prefix, *args, _builder=None):\n     '''\n-    Print the values at runtime from the device.\n+    Print the values at runtime from the device.  String formatting does not work, so you should\n+    provide the values you want to print as arguments.\n+\n+    .. highlight:: python\n+    .. code-block:: python\n+\n+        tl.device_print(\"pid\", pid)\n \n     :param prefix: a prefix to print before the values. This is required to be a string literal.\n     :param args: the values to print. They can be any tensor or scalar.\n@@ -1560,7 +1580,18 @@ def device_print(prefix, *args, _builder=None):\n @builtin\n def device_assert(cond, msg=\"\", _builder=None):\n     '''\n-    Assert the condition at runtime from the device.\n+    Assert the condition at runtime from the device.  Requires that the environment variable :code:`TRITON_DEBUG`\n+    is set to a value besides :code:`0` in order for this to have any effect.\n+\n+    Using the Python :code:`assert` statement is the same as calling this function, except that the second argument\n+    must be provided and must be a string, e.g. :code:`assert pid == 0, \"pid != 0\"`.  The environment variable must\n+    be set for this :code:`assert` statement to have any effect.\n+\n+    .. highlight:: python\n+    .. code-block:: python\n+\n+        tl.device_assert(pid == 0)\n+        assert pid == 0, f\"pid != 0\"\n \n     :param cond: the condition to assert. This is required to be a boolean tensor.\n     :param msg: the message to print if the assertion fails. This is required to be a string literal."}]