[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 57, "deletions": 71, "changes": 128, "file_content_changes": "@@ -354,25 +354,6 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n   return llvmStruct;\n }\n \n-// TODO[goostavz]: to be deprecated\n-// delinearize supposing order is [n, .. , 2, 1, 0]\n-template <typename T>\n-static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n-  // shape: {a, b, c, d}  ->  accMul: {b*c*d, c*d, d, 1}\n-  size_t rank = shape.size();\n-  T accMul = product(shape.drop_front());\n-  T linearRemain = linearIndex;\n-  SmallVector<T> multiDimIndex(rank);\n-  for (size_t i = 0; i < rank; ++i) {\n-    multiDimIndex[i] = linearRemain / accMul;\n-    linearRemain = linearRemain % accMul;\n-    if (i != (rank - 1)) {\n-      accMul = accMul / shape[i + 1];\n-    }\n-  }\n-  return multiDimIndex;\n-}\n-\n // delinearize supposing order is [0, 1, .. , n]\n template <typename T>\n static SmallVector<T> getMultiDimIndexImpl(T linearIndex, ArrayRef<T> shape) {\n@@ -405,24 +386,7 @@ static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape,\n   return multiDim;\n }\n \n-// TODO[goostavz]: to be deprecated\n-// linearize supposing order is [n, .. , 2, 1, 0]\n-template <typename T>\n-static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n-  assert(multiDimIndex.size() == shape.size());\n-  // shape: {a, b, c, d}  ->  accMul: {b*c*d, c*d, d, 1}\n-  size_t rank = shape.size();\n-  T accMul = product(shape.drop_front());\n-  T linearIndex = 0;\n-  for (size_t i = 0; i < rank; ++i) {\n-    linearIndex += multiDimIndex[i] * accMul;\n-    if (i != (rank - 1)) {\n-      accMul = accMul / shape[i + 1];\n-    }\n-  }\n-  return linearIndex;\n-}\n-\n+// linearize supposing order is [0, 1, .. , n]\n template <typename T>\n static T getLinearIndexImpl(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   assert(multiDimIndex.size() == shape.size());\n@@ -619,6 +583,13 @@ class ConvertTritonGPUOpToLLVMPattern\n     return multiDim;\n   }\n \n+  Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n+                  ArrayRef<Value> multiDim, ArrayRef<unsigned> shape,\n+                  ArrayRef<unsigned> order) const {\n+    return linearize(rewriter, loc, reorder<Value>(multiDim, order),\n+                     reorder<unsigned>(shape, order));\n+  }\n+\n   Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n                   ArrayRef<Value> multiDim, ArrayRef<unsigned> shape) const {\n     int rank = multiDim.size();\n@@ -1434,10 +1405,12 @@ struct BroadcastOpConversion\n     auto resultShape = resultTy.getShape();\n     unsigned rank = srcTy.getRank();\n     assert(rank == resultTy.getRank());\n+    auto order = srcLayout.getOrder();\n \n-    SmallVector<int64_t, 4> srcLogicalShape(2 * rank);\n-    SmallVector<int64_t, 4> resultLogicalShape(2 * rank);\n-    SmallVector<unsigned, 2> broadcastDims;\n+    SmallVector<int64_t> srcLogicalShape(2 * rank);\n+    SmallVector<unsigned> srcLogicalOrder(2 * rank);\n+    SmallVector<int64_t> resultLogicalShape(2 * rank);\n+    SmallVector<unsigned> broadcastDims;\n     for (unsigned d = 0; d < rank; ++d) {\n       unsigned resultShapePerCTA = resultLayout.getSizePerThread()[d] *\n                                    resultLayout.getThreadsPerWarp()[d] *\n@@ -1455,9 +1428,13 @@ struct BroadcastOpConversion\n       }\n       resultLogicalShape[d] = numCtas;\n       resultLogicalShape[d + rank] = resultLayout.getSizePerThread()[d];\n+\n+      srcLogicalOrder[d] = order[d] + rank;\n+      srcLogicalOrder[d + rank] = order[d];\n     }\n     int64_t duplicates = 1;\n-    SmallVector<int64_t, 2> broadcastSizes(broadcastDims.size() * 2);\n+    SmallVector<int64_t> broadcastSizes(broadcastDims.size() * 2);\n+    SmallVector<unsigned> broadcastOrder(broadcastDims.size() * 2);\n     for (auto it : llvm::enumerate(broadcastDims)) {\n       // Incase there are multiple indices in the src that is actually\n       // calculating the same element, srcLogicalShape may not need to be 1.\n@@ -1466,30 +1443,44 @@ struct BroadcastOpConversion\n       // [1, 2]\n       int64_t d = resultLogicalShape[it.value()] / srcLogicalShape[it.value()];\n       broadcastSizes[it.index()] = d;\n+      broadcastOrder[it.index()] = srcLogicalOrder[it.value()];\n       duplicates *= d;\n       d = resultLogicalShape[it.value() + rank] /\n           srcLogicalShape[it.value() + rank];\n       broadcastSizes[it.index() + broadcastDims.size()] = d;\n+      broadcastOrder[it.index() + broadcastDims.size()] =\n+          srcLogicalOrder[it.value() + rank];\n       duplicates *= d;\n     }\n+    auto argsort = [](SmallVector<unsigned> input) {\n+      SmallVector<unsigned> idx(input.size());\n+      std::iota(idx.begin(), idx.end(), 0);\n+      std::sort(idx.begin(), idx.end(), [&input](unsigned a, unsigned b) {\n+        return input[a] < input[b];\n+      });\n+      return idx;\n+    };\n+    broadcastOrder = argsort(broadcastOrder);\n \n     unsigned srcElems = getElemsPerThread(srcTy);\n     auto srcVals = getElementsFromStruct(loc, src, rewriter);\n     unsigned resultElems = getElemsPerThread(resultTy);\n     SmallVector<Value> resultVals(resultElems);\n     for (unsigned i = 0; i < srcElems; ++i) {\n-      auto srcMultiDim = getMultiDimIndex<int64_t>(i, srcLogicalShape);\n+      auto srcMultiDim =\n+          getMultiDimIndex<int64_t>(i, srcLogicalShape, srcLogicalOrder);\n       for (int64_t j = 0; j < duplicates; ++j) {\n         auto resultMultiDim = srcMultiDim;\n-        auto bcastMultiDim = getMultiDimIndex<int64_t>(j, broadcastSizes);\n+        auto bcastMultiDim =\n+            getMultiDimIndex<int64_t>(j, broadcastSizes, broadcastOrder);\n         for (auto bcastDim : llvm::enumerate(broadcastDims)) {\n           resultMultiDim[bcastDim.value()] += bcastMultiDim[bcastDim.index()];\n           resultMultiDim[bcastDim.value() + rank] +=\n               bcastMultiDim[bcastDim.index() + broadcastDims.size()] *\n               srcLogicalShape[bcastDim.index() + broadcastDims.size()];\n         }\n-        auto resultLinearIndex =\n-            getLinearIndex<int64_t>(resultMultiDim, resultLogicalShape);\n+        auto resultLinearIndex = getLinearIndex<int64_t>(\n+            resultMultiDim, resultLogicalShape, srcLogicalOrder);\n         resultVals[resultLinearIndex] = srcVals[i];\n       }\n     }\n@@ -1663,9 +1654,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     SmallVector<Value> writeIdx = indices[key];\n \n     writeIdx[axis] = udiv(writeIdx[axis], sizePerThread);\n-    Value writeOffset =\n-        linearize(rewriter, loc, reorder<Value>(writeIdx, srcOrd),\n-                  reorder<unsigned>(smemShape, srcOrd));\n+    Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, srcOrd);\n     Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n     store(acc, writePtr);\n \n@@ -1674,9 +1663,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n       readIdx[axis] = ints[N];\n       Value readMask = icmp_slt(writeIdx[axis], ints[N]);\n       Value readOffset =\n-          select(readMask,\n-                 linearize(rewriter, loc, reorder<Value>(readIdx, srcOrd),\n-                           reorder<unsigned>(smemShape, srcOrd)),\n+          select(readMask, linearize(rewriter, loc, readIdx, smemShape, srcOrd),\n                  ints[0]);\n       Value readPtr = gep(elemPtrTy, writePtr, readOffset);\n       barrier();\n@@ -1700,9 +1687,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     for (unsigned i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       readIdx.insert(readIdx.begin() + axis, ints[0]);\n-      Value readOffset =\n-          linearize(rewriter, loc, reorder<Value>(readIdx, srcOrd),\n-                    reorder<unsigned>(smemShape, srcOrd));\n+      Value readOffset = linearize(rewriter, loc, readIdx, smemShape, srcOrd);\n       Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n       resultVals[i] = load(readPtr);\n     }\n@@ -1796,9 +1781,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n \n     SmallVector<Value> writeIdx = indices[key];\n     writeIdx[axis] = (sizeInterWarps == 1) ? zero : warpIdAxis;\n-    Value writeOffset =\n-        linearize(rewriter, loc, reorder<Value>(writeIdx, order),\n-                  reorder<unsigned>(smemShape, order));\n+    Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, order);\n     Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n     storeShared(rewriter, loc, writePtr, acc, laneZero);\n   }\n@@ -1849,23 +1832,25 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n     // nd-tensor where n >= 1\n     auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n-    auto resultShape = resultTy.getShape();\n     SmallVector<unsigned> resultOrd;\n     for (auto ord : order) {\n       if (ord != 0)\n         resultOrd.push_back(ord - 1);\n     }\n \n     unsigned resultElems = getElemsPerThread(resultTy);\n-    auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultShape);\n+    auto resultIndices =\n+        emitIndices(loc, rewriter, resultLayout, resultTy.getShape());\n     assert(resultIndices.size() == resultElems);\n \n     SmallVector<Value> resultVals(resultElems);\n+    SmallVector<unsigned> resultShape;\n+    std::copy(resultTy.getShape().begin(), resultTy.getShape().end(),\n+              std::back_inserter(resultShape));\n     for (size_t i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       Value readOffset =\n-          linearize(rewriter, loc, reorder<Value>(readIdx, resultOrd),\n-                    reorder<int64_t, unsigned>(resultShape, resultOrd));\n+          linearize(rewriter, loc, readIdx, resultShape, resultOrd);\n       Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n       resultVals[i] = load(readPtr);\n     }\n@@ -2816,8 +2801,8 @@ struct ConvertLayoutOpConversion\n       auto multiDimOffsetFirstElem =\n           emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n       SmallVector<Value> multiDimOffset(rank);\n-      SmallVector<unsigned> multiDimElemId =\n-          getMultiDimIndex<unsigned>(elemId, blockedLayout.getSizePerThread());\n+      SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n+          elemId, blockedLayout.getSizePerThread(), blockedLayout.getOrder());\n       for (unsigned d = 0; d < rank; ++d) {\n         multiDimOffset[d] = add(multiDimOffsetFirstElem[d],\n                                 idx_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n@@ -2848,9 +2833,7 @@ struct ConvertLayoutOpConversion\n       Value warpSize = idx_val(32);\n       Value laneId = urem(threadId, warpSize);\n       Value warpId = udiv(threadId, warpSize);\n-      // auto multiDimWarpId =\n-      //     delinearize(rewriter, loc, warpId, mmaLayout.getWarpsPerCTA());\n-      // TODO: double confirm if its document bug or DotConversion's Bug\n+      // TODO: fix the bug in MMAEncodingAttr document\n       SmallVector<Value> multiDimWarpId(2);\n       multiDimWarpId[0] = urem(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n       multiDimWarpId[1] = udiv(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n@@ -2940,6 +2923,7 @@ void ConvertLayoutOpConversion::processReplica(\n   auto accumSizePerThread = product<unsigned>(sizePerThread);\n   SmallVector<unsigned> numCTAs(rank);\n   auto shapePerCTA = getShapePerCTA(layout);\n+  auto order = getOrder(layout);\n   for (unsigned d = 0; d < rank; ++d) {\n     numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n   }\n@@ -2950,14 +2934,16 @@ void ConvertLayoutOpConversion::processReplica(\n   auto llvmElemTy = getTypeConverter()->convertType(elemTy);\n \n   for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n-    auto multiDimCTAInRepId = getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep);\n+    auto multiDimCTAInRepId =\n+        getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep, order);\n     SmallVector<unsigned> multiDimCTAId(rank);\n     for (auto it : llvm::enumerate(multiDimCTAInRepId)) {\n       auto d = it.index();\n       multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n     }\n \n-    unsigned linearCTAId = getLinearIndex<unsigned>(multiDimCTAId, numCTAs);\n+    unsigned linearCTAId =\n+        getLinearIndex<unsigned>(multiDimCTAId, numCTAs, order);\n     // TODO: This is actually redundant index calculation, we should\n     //       consider of caching the index calculation result in case\n     //       of performance issue observed.\n@@ -2966,8 +2952,7 @@ void ConvertLayoutOpConversion::processReplica(\n           getMultiDimOffset(layout, loc, rewriter, elemId, type.getShape(),\n                             multiDimCTAInRepId, shapePerCTA);\n       Value offset =\n-          linearize(rewriter, loc, reorder<Value>(multiDimOffset, outOrd),\n-                    reorder<unsigned>(paddedRepShape, outOrd));\n+          linearize(rewriter, loc, multiDimOffset, paddedRepShape, outOrd);\n       auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n       Value ptr = gep(elemPtrTy, smemBase, offset);\n       auto vecTy = vec_ty(llvmElemTy, vec);\n@@ -3044,7 +3029,8 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n   SmallVector<Value> outVals(outElems);\n \n   for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n-    auto multiDimRepId = getMultiDimIndex<unsigned>(repId, numReplicates);\n+    auto multiDimRepId =\n+        getMultiDimIndex<unsigned>(repId, numReplicates, outOrd);\n     barrier();\n     if (srcLayout.isa<BlockedEncodingAttr>() ||\n         srcLayout.isa<SliceEncodingAttr>() ||"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 16, "deletions": 11, "changes": 27, "file_content_changes": "@@ -154,6 +154,19 @@ def get_variant_golden(a, b):\n     c_padded = torch.matmul(a_padded, b_padded)\n     return c_padded[:SIZE_M, :SIZE_N]\n \n+# It's not easy to get a proper error threshold in different size\n+# Here the gemm calculation is padded to a different size in order to get\n+# a variant version of the golden result. And the error between golden and\n+# golden_variant provide reference on selecting the proper rtol / atol.\n+\n+\n+def get_proper_err(a, b, golden):\n+    golden_variant = get_variant_golden(a, b)\n+    golden_diff = golden - golden_variant\n+    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n+    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n+    return (golden_abs_err, golden_rel_err)\n+\n \n @pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,TRANS_A,TRANS_B', [\n     # Non-forloop\n@@ -198,16 +211,7 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n                         BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n                         num_warps=NUM_WARPS)\n     golden = torch.matmul(a, b)\n-\n-    # It's not easy to get a proper error threshold in different size\n-    # Here the gemm calculation is padded to a different size in order to get\n-    # a variant version of the golden result. And the error between golden and\n-    # golden_variant provide reference on selecting the proper rtol / atol.\n-    golden_variant = get_variant_golden(a, b)\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n-\n+    golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n     torch.set_printoptions(profile=\"full\")\n     assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n \n@@ -272,4 +276,5 @@ def matmul_kernel(\n                         BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K)\n \n     golden = torch.matmul(a, b)\n-    torch.testing.assert_close(c, golden)\n+    golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n+    torch.testing.assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err))"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -245,12 +245,12 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %0 = tt.view %arg : (tensor<256xf32, #blocked0>) -> tensor<256x1xf32,#blocked2>\n     // CHECK: llvm.mlir.undef\n     // CHECK: llvm.insertvalue %[[T0]]\n-    // CHECK: llvm.insertvalue %[[T0]]\n-    // CHECK: llvm.insertvalue %[[T0]]\n-    // CHECK: llvm.insertvalue %[[T0]]\n     // CHECK: llvm.insertvalue %[[T1]]\n+    // CHECK: llvm.insertvalue %[[T0]]\n     // CHECK: llvm.insertvalue %[[T1]]\n+    // CHECK: llvm.insertvalue %[[T0]]\n     // CHECK: llvm.insertvalue %[[T1]]\n+    // CHECK: llvm.insertvalue %[[T0]]\n     // CHECK: llvm.insertvalue %[[T1]]\n     %1 = tt.broadcast %0 : (tensor<256x1xf32,#blocked2>) -> tensor<256x4xf32, #blocked2>\n     return"}]