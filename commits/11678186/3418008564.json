[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 239, "deletions": 16, "changes": 255, "file_content_changes": "@@ -2708,8 +2708,8 @@ class MMA16816SmemLoader {\n       Value sOffset =\n           mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sTileStride);\n       Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n-      PTXBuilder builder;\n \n+      PTXBuilder builder;\n       // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n       // thread.\n       auto resArgs = builder.newListOperand(4, \"=r\");\n@@ -2799,12 +2799,12 @@ class MMA16816SmemLoader {\n       if (kOrder == 1) {\n         for (int i = 0; i < 2; ++i)\n           for (int j = 0; j < 4; ++j)\n-            i8Elems[i][j] = load(gep(elemTy, ptrs[i][j], sOffsetElemVal));\n+            i8Elems[i][j] = load(gep(elemPtrTy, ptrs[i][j], sOffsetElemVal));\n \n         for (int i = 2; i < 4; ++i)\n           for (int j = 0; j < 4; ++j)\n             i8Elems[i][j] =\n-                load(gep(elemPtrTy, ptrs[i - 2][j], sOffsetElemVal));\n+                load(gep(elemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n \n         for (int m = 0; m < 4; ++m) {\n           for (int e = 0; e < 4; ++e)\n@@ -2904,6 +2904,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     size_t reduceAxis = 1;\n     unsigned K = AShape[reduceAxis];\n     bool isOuter = K == 1;\n+\n     bool isMMA = D.getType()\n                      .cast<RankedTensorType>()\n                      .getEncoding()\n@@ -2915,11 +2916,13 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                       .getEncoding()\n                       .cast<MmaEncodingAttr>();\n \n-    if (!isOuter && isMMA) {\n+    bool isHMMA = isDotHMMA(op);\n+    if (!isOuter && isMMA && isHMMA) {\n       if (mmaLayout.getVersion() == 1)\n         return convertMMA884(op, adaptor, rewriter);\n       if (mmaLayout.getVersion() == 2)\n         return convertMMA16816(op, adaptor, rewriter);\n+\n       llvm::report_fatal_error(\n           \"Unsupported MMA kind found when converting DotOp to LLVM.\");\n     }\n@@ -2932,6 +2935,46 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n         \"Unsupported DotOp found when converting TritonGPU to LLVM.\");\n   }\n \n+  // Tell whether a DotOp support HMMA.\n+  // This is port from the master branch, the original logic is retained.\n+  static bool isDotHMMA(DotOp op) {\n+    auto a = op.a();\n+    auto b = op.b();\n+    auto c = op.c();\n+    auto d = op.getResult();\n+    auto aTensorTy = a.getType().cast<RankedTensorType>();\n+    auto bTensorTy = b.getType().cast<RankedTensorType>();\n+    auto cTensorTy = c.getType().cast<RankedTensorType>();\n+    auto dTensorTy = d.getType().cast<RankedTensorType>();\n+\n+    if (!dTensorTy.getEncoding().isa<MmaEncodingAttr>())\n+      return false;\n+\n+    auto mmaLayout = dTensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto aElemTy = aTensorTy.getElementType();\n+    auto bElemTy = bTensorTy.getElementType();\n+    // Refer to mma section for the data type supported by Volta and Hopper\n+    // Tensor Core in\n+    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-fragment-mma-884-f16\n+    return (aElemTy.isF16() && bElemTy.isF16()) ||\n+           (aElemTy.isBF16() && bElemTy.isBF16()) ||\n+           (aElemTy.isF32() && bElemTy.isF32() && op.allowTF32() &&\n+            mmaLayout.getVersion() >= 2) ||\n+           (aElemTy.isInteger(8) && bElemTy.isInteger(8) &&\n+            mmaLayout.getVersion() >= 2);\n+  }\n+\n+  // Tell whether a DotOp support HMMA by the operand type(either $a or $b).\n+  // We cannot get both the operand types(in TypeConverter), here we assume the\n+  // types of both the operands are identical here.\n+  // TODO[Superjomn]: Find a better way to implement it.\n+  static bool isDotHMMA(TensorType operand, bool allowTF32, int mmaVersion) {\n+    auto elemTy = operand.getElementType();\n+    return elemTy.isF16() || elemTy.isBF16() ||\n+           (elemTy.isF32() && allowTF32 && mmaVersion >= 2) ||\n+           (elemTy.isInteger(8) && mmaVersion >= 2);\n+  }\n+\n private:\n   // Convert to mma.m16n8k16\n   LogicalResult convertMMA16816(triton::DotOp a, OpAdaptor adaptor,\n@@ -2941,10 +2984,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                               ConversionPatternRewriter &rewriter) const;\n \n   LogicalResult convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n-                              ConversionPatternRewriter &rewriter) const {\n-    assert(false && \"Not implemented yet.\");\n-    return failure();\n-  }\n+                              ConversionPatternRewriter &rewriter) const;\n };\n \n // Helper for conversion of DotOp with mma<version=1>, that is sm<80\n@@ -3644,7 +3684,6 @@ struct MMA16816ConversionHelper {\n         return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n       };\n \n-      llvm::outs() << \"mmaOut.getType: \" << mmaOut.getType() << \"\\n\";\n       Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n       for (int i = 0; i < 4; ++i)\n         fc[m * colsPerThread + 4 * n + i] =\n@@ -3656,14 +3695,15 @@ struct MMA16816ConversionHelper {\n         for (int n = 0; n < numRepN; ++n)\n           callMma(2 * m, n, 2 * k);\n \n-    // bitcast to fp32 in bulk\n+    Type resElemTy = dTensorTy.getElementType();\n+\n     for (auto &elem : fc) {\n-      elem = bitcast(elem, type::i32Ty(ctx));\n+      elem = bitcast(elem, resElemTy);\n     }\n \n     // replace with new packed result\n     Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(fc.size(), type::i32Ty(ctx)));\n+        ctx, SmallVector<Type>(fc.size(), resElemTy));\n     Value res = getStructFromElements(loc, fc, rewriter, structTy);\n     rewriter.replaceOp(op, res);\n \n@@ -3797,9 +3837,25 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n       dotOperandLayout.getParent().dyn_cast_or_null<MmaEncodingAttr>();\n   assert(mmaLayout);\n \n+  bool isOuter{};\n+  {\n+    int K{};\n+    if (dotOperandLayout.getOpIdx() == 0) // $a\n+      K = dstTensorTy.getShape()[1];\n+    else // $b\n+      K = dstTensorTy.getShape()[0];\n+    isOuter = K == 1;\n+  }\n+\n+  // TODO[Superjomn]: the allowTF32 is not available in ConvertLayoutOp for it\n+  // is an attribute of DotOp.\n+  bool allowTF32 = false;\n+  bool isHMMA = DotOpConversion::isDotHMMA(dstTensorTy, allowTF32,\n+                                           mmaLayout.getVersion());\n+\n   auto smemObj = getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n   Value res;\n-  if (mmaLayout.getVersion() == 2) {\n+  if (!isOuter && mmaLayout.getVersion() == 2 && isHMMA) { // tensor core v2\n     MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n                                        rewriter, getTypeConverter(),\n                                        op.getLoc());\n@@ -3811,7 +3867,8 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n       // operand $b\n       res = mmaHelper.loadB(src, smemObj);\n     }\n-  } else if (mmaLayout.getVersion() == 1) {\n+  } else if (!isOuter && mmaLayout.getVersion() == 1 &&\n+             isHMMA) { // tensor core v1\n     DotOpMmaV1ConversionHelper helper(mmaLayout);\n     if (dotOperandLayout.getOpIdx() == 0) {\n       // operand $a\n@@ -4297,6 +4354,172 @@ DotOpMmaV1ConversionHelper::extractLoadedOperand(\n   return rcds;\n }\n \n+template <typename T> void print_vec(ArrayRef<T> vec) {\n+  for (int v : vec)\n+    llvm::outs() << v << \" \";\n+  llvm::outs() << \"\\n\";\n+}\n+\n+LogicalResult\n+DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n+                               ConversionPatternRewriter &rewriter) const {\n+  auto *ctx = rewriter.getContext();\n+  auto loc = op.getLoc();\n+  auto threadId = getThreadId(rewriter, loc);\n+\n+  using ValueTable = std::map<std::pair<int, int>, Value>;\n+\n+  auto A = op.a();\n+  auto B = op.b();\n+  auto C = op.c();\n+  auto D = op.getResult();\n+\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto cTensorTy = C.getType().cast<RankedTensorType>();\n+  auto dTensorTy = D.getType().cast<RankedTensorType>();\n+\n+  auto aShape = aTensorTy.getShape();\n+  auto bShape = bTensorTy.getShape();\n+  auto cShape = cTensorTy.getShape();\n+\n+  auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto cLayout = cTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto dLayout = dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+\n+  auto aOrder = aLayout.getOrder();\n+  auto bOrder = bLayout.getOrder();\n+\n+  auto order = dLayout.getOrder();\n+\n+  bool isARow = aOrder[0] == 1;\n+  bool isBRow = bOrder[0] == 1;\n+\n+  int strideAM = isARow ? aShape[1] : 1;\n+  int strideAK = isARow ? 1 : aShape[0];\n+  int strideBN = isBRow ? 1 : bShape[0];\n+  int strideBK = isBRow ? bShape[1] : 1;\n+  int strideA0 = isARow ? strideAK : strideAM;\n+  int strideA1 = isARow ? strideAM : strideAK;\n+  int strideB0 = isBRow ? strideBN : strideBK;\n+  int strideB1 = isBRow ? strideBK : strideBN;\n+  int lda = isARow ? strideAM : strideAK;\n+  int ldb = isBRow ? strideBK : strideBN;\n+  int aPerPhase = aLayout.getPerPhase();\n+  int aMaxPhase = aLayout.getMaxPhase();\n+  int bPerPhase = bLayout.getPerPhase();\n+  int bMaxPhase = bLayout.getMaxPhase();\n+  int aNumPtr = 8;\n+  int bNumPtr = 8;\n+  int NK = aShape[1];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  llvm::outs() << \"strideA: \" << strideAM << \" \" << strideAK << \"\\n\";\n+  llvm::outs() << \"strideB: \" << strideBN << \" \" << strideBK << \"\\n\";\n+  llvm::outs() << \"shapePerCTA: \";\n+  print_vec<unsigned>(shapePerCTA);\n+  llvm::outs() << \"\\n\";\n+\n+  llvm::outs() << \"sizePerThread: \";\n+  print_vec<unsigned>(sizePerThread);\n+  llvm::outs() << \"\\n\";\n+\n+  Value _0 = i32_val(0);\n+\n+  Value mContig = i32_val(sizePerThread[order[1]]);\n+  Value nContig = i32_val(sizePerThread[order[0]]);\n+\n+  // threadId in blocked layout\n+  SmallVector<Value> threadIds;\n+  {\n+    int dim = cShape.size();\n+    threadIds.resize(dim);\n+    for (unsigned k = 0; k < dim - 1; k++) {\n+      Value dimK = i32_val(shapePerCTA[order[k]]);\n+      Value rem = urem(threadId, dimK);\n+      threadId = udiv(threadId, dimK);\n+      threadIds[order[k]] = rem;\n+    }\n+    Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n+    threadIds[order[dim - 1]] = urem(threadId, dimK);\n+  }\n+\n+  Value threadIdM = threadIds[0];\n+  Value threadIdN = threadIds[1];\n+\n+  Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n+  Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n+  SmallVector<Value> aOff(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i) {\n+    aOff[i] = add(mul(offA0, i32_val(strideA0)), mul(offA1, i32_val(strideA1)));\n+  }\n+\n+  Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n+  Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n+  SmallVector<Value> bOff(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i) {\n+    bOff[i] = add(mul(offB0, i32_val(strideB0)), mul(offB1, i32_val(strideB1)));\n+  }\n+\n+  auto aSmem = getSharedMemoryObjectFromStruct(loc, adaptor.a(), rewriter);\n+  auto bSmem = getSharedMemoryObjectFromStruct(loc, adaptor.b(), rewriter);\n+\n+  Type f32PtrTy = ptr_ty(f32_ty);\n+  SmallVector<Value> aPtrs(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i)\n+    aPtrs[i] = gep(f32PtrTy, aSmem.base, aOff[i]);\n+\n+  SmallVector<Value> bPtrs(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i)\n+    bPtrs[i] = gep(f32PtrTy, bSmem.base, bOff[i]);\n+\n+  ValueTable has, hbs;\n+  auto cc = getElementsFromStruct(loc, adaptor.c(), rewriter);\n+  SmallVector<Value> ret = cc;\n+  // is this compatible with blocked layout?\n+\n+  for (unsigned k = 0; k < NK; k++) {\n+    int z = 0;\n+    for (unsigned i = 0; i < cShape[order[1]]; i += shapePerCTA[order[1]])\n+      for (unsigned j = 0; j < cShape[order[0]]; j += shapePerCTA[order[0]])\n+        for (unsigned ii = 0; ii < sizePerThread[order[1]]; ++ii)\n+          for (unsigned jj = 0; jj < sizePerThread[order[0]]; ++jj) {\n+            unsigned m = order[0] == 1 ? i : j;\n+            unsigned n = order[0] == 1 ? j : i;\n+            unsigned mm = order[0] == 1 ? ii : jj;\n+            unsigned nn = order[0] == 1 ? jj : ii;\n+            if (!has.count({m + mm, k})) {\n+              Value pa = gep(f32PtrTy, aPtrs[0],\n+                             i32_val((m + mm) * strideAM + k * strideAK));\n+              Value va = load(pa);\n+              has[{m + mm, k}] = va;\n+            }\n+            if (!hbs.count({n + nn, k})) {\n+              Value pb = gep(f32PtrTy, bPtrs[0],\n+                             i32_val((n + nn) * strideBN + k * strideBK));\n+              Value vb = load(pb);\n+              hbs[{n + nn, k}] = vb;\n+            }\n+\n+            llvm::outs() << z << \": \" << m + mm << \" \" << n + nn << \"\\n\";\n+            ret[z] = rewriter.create<LLVM::FMulAddOp>(loc, has[{m + mm, k}],\n+                                                      hbs[{n + nn, k}], ret[z]);\n+            ++z;\n+          }\n+  }\n+\n+  auto res = getStructFromElements(\n+      loc, ret, rewriter,\n+      struct_ty(SmallVector<Type>(ret.size(), ret[0].getType())));\n+  rewriter.replaceOp(op, res);\n+\n+  return success();\n+}\n+\n /// ====================== mma codegen end ============================\n \n Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,\n@@ -5134,8 +5357,8 @@ void ConvertTritonGPUToLLVM::initSharedMemory(\n   OpBuilder b(mod.getBodyRegion());\n   auto loc = mod.getLoc();\n   auto elemTy = typeConverter.convertType(b.getIntegerType(8));\n-  // Set array size 0 and external linkage indicates that we use dynamic shared\n-  // allocation to allow a larger shared memory size for each kernel.\n+  // Set array size 0 and external linkage indicates that we use dynamic\n+  // shared allocation to allow a larger shared memory size for each kernel.\n   auto arrayTy = LLVM::LLVMArrayType::get(elemTy, 0);\n   auto global = b.create<LLVM::GlobalOp>(\n       loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::External,"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -117,10 +117,12 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n                   \"BlockedEncodingAttr not implemented\");\n     }\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    assert(mmaLayout.getVersion() == 2 &&\n-           \"mmaLayout version = 1 is not implemented yet\");\n-    return {16 * mmaLayout.getWarpsPerCTA()[0],\n-            8 * mmaLayout.getWarpsPerCTA()[1]};\n+    if (mmaLayout.getVersion() == 2)\n+      return {16 * mmaLayout.getWarpsPerCTA()[0],\n+              8 * mmaLayout.getWarpsPerCTA()[1]};\n+    if (mmaLayout.getVersion() == 1)\n+      return {16 * mmaLayout.getWarpsPerCTA()[0],\n+              16 * mmaLayout.getWarpsPerCTA()[1]};\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 9, "deletions": 1, "changes": 10, "file_content_changes": "@@ -576,6 +576,14 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto oldRetType = dotOp.getResult().getType().cast<RankedTensorType>();\n     if (oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n       return failure();\n+\n+    auto A = dotOp.getOperand(0).getType().cast<RankedTensorType>();\n+    auto B = dotOp.getOperand(1).getType().cast<RankedTensorType>();\n+    // for FMA, should retain the blocked layout.\n+    if (A.getElementType().isF32() && B.getElementType().isF32() &&\n+        !dotOp.allowTF32())\n+      return failure();\n+\n     // get MMA encoding for the given number of warps\n     auto retShape = oldRetType.getShape();\n     auto mod = op->getParentOfType<mlir::ModuleOp>();\n@@ -629,4 +637,4 @@ class TritonGPUCombineOpsPass\n \n std::unique_ptr<Pass> mlir::createTritonGPUCombineOpsPass() {\n   return std::make_unique<TritonGPUCombineOpsPass>();\n-}\n\\ No newline at end of file\n+}"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -168,7 +168,3 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n \n     torch.set_printoptions(profile=\"full\")\n     assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n-\n-\n-test_gemm_no_scf_int8(SIZE_M=16, SIZE_N=32, SIZE_K=64, NUM_WARPS=2)\n-#test_gemm(*[128, 64, 128, 4, 128, 64, 32])"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -811,3 +811,22 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     return\n   }\n }\n+\n+// -----\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n+#mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  func @matmul_fmadot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #blocked>\n+    // CHECK: llvm.intr.fmuladd\n+    %28 = tt.dot %a, %b, %cst {allowTF32 = false, transA = false, transB = false} : tensor<32x16xf32, #shared> * tensor<16x32xf32, #shared> -> tensor<32x32xf32, #blocked>\n+    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n+    %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n+    tt.store %36, %28 : tensor<32x32xf32, #blocked>\n+    return\n+  }\n+}"}]