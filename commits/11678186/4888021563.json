[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -457,7 +457,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     } else {\n       // If the tensor is not ranked, then it is a scalar and only thread 0 can\n       // write\n-      mask = and_(mask, icmp_slt(tid, i32_val(1)));\n+      mask = and_(mask, icmp_eq(tid, i32_val(0)));\n     }\n     return mask;\n   }"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1009,7 +1009,6 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32\n   tt.func @atomic_add_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xi1, #blocked0>, %arg2 : tensor<256xf32, #blocked0>) {\n-    // CHECK: llvm.icmp \"slt\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$3 atom.global.gpu.add.f32\n     // CHECK: llvm.inline_asm\n@@ -1026,6 +1025,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   tt.func @atomic_add_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : i1, %arg2 : f32) {\n     // CHECK: llvm.icmp \"eq\"\n     // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$3 atom.global.gpu.add.f32\n     %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (!tt.ptr<f32>, f32, i1) -> f32\n     tt.return\n@@ -1052,7 +1052,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: store_f32_scalar\n   tt.func @store_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : f32) {\n-    // CHECK: llvm.icmp \"slt\"\n+    // CHECK: llvm.icmp \"eq\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$2 st.global.b32\n     tt.store %arg0, %arg1 : f32"}]