[{"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 10, "deletions": 9, "changes": 19, "file_content_changes": "@@ -31,27 +31,28 @@ static bool findAndReplace(std::string &str, const std::string &begin,\n }\n \n std::string translateLLVMIRToPTX(llvm::Module &module, int cc, int version) {\n-  // LLVM version in use may not officially support target hardware\n-  int maxNNVMCC = 75;\n+  // LLVM version in use may not officially support target hardware.\n+  // Supported versions for LLVM 14 are here:\n+  // https://github.com/llvm/llvm-project/blob/f28c006a5895fc0e329fe15fead81e37457cb1d1/clang/include/clang/Basic/BuiltinsNVPTX.def\n+  int maxPTX = std::min(75, version);\n+  int maxCC = std::min(86, cc);\n   // options\n   auto options = llvm::cl::getRegisteredOptions();\n   auto *shortPtr =\n       static_cast<llvm::cl::opt<bool> *>(options[\"nvptx-short-ptr\"]);\n   assert(shortPtr);\n   shortPtr->setValue(true);\n-  // compute capability\n-  std::string sm = \"sm_\" + std::to_string(cc);\n+  std::string sm = \"sm_\" + std::to_string(maxCC);\n   // max PTX version\n-  int ptxMajor = version / 10;\n-  int ptxMinor = version % 10;\n+  int ptxMajor = maxPTX / 10;\n+  int ptxMinor = maxPTX % 10;\n   // create\n   llvm::SmallVector<char, 0> buffer;\n   std::string triple = \"nvptx64-nvidia-cuda\";\n-  std::string proc = \"sm_\" + std::to_string(std::min(cc, maxNNVMCC));\n+  std::string proc = \"sm_\" + std::to_string(maxCC);\n   std::string layout = \"\";\n   std::string features = \"\";\n-  // std::string features = \"+ptx\" + std::to_string(std::min(ptx,\n-  // max_nvvm_ptx));\n+  // std::string features = \"+ptx\" + std::to_string(maxPTX);\n   initLLVM();\n   // verify and store llvm\n   llvm::legacy::PassManager pm;"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 6, "deletions": 17, "changes": 23, "file_content_changes": "@@ -963,23 +963,12 @@ def ptx_get_version(cuda_version) -> int:\n     '''\n     assert isinstance(cuda_version, str)\n     major, minor = map(int, cuda_version.split('.'))\n-    version = major * 1000 + minor * 10\n-    if version >= 11040:\n-        return 74\n-    if version >= 11030:\n-        return 73\n-    if version >= 11020:\n-        return 72\n-    if version >= 11010:\n-        return 71\n-    if version >= 11000:\n-        return 70\n-    if version >= 10020:\n-        return 65\n-    if version >= 10010:\n-        return 64\n-    if version >= 10000:\n-        return 63\n+    if major == 12:\n+        return 80 + minor\n+    if major == 11:\n+        return 70 + minor\n+    if major == 10:\n+        return 63 + minor\n     raise RuntimeError(\"Triton only support CUDA 10.0 or higher\")\n \n "}]