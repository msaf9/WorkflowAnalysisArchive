[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -307,8 +307,7 @@ class ConvertTritonGPUToLLVM\n     // Preprocess\n     decomposeMmaToDotOperand(mod, numWarps);\n     decomposeBlockedToDotOperand(mod);\n-    if (failed(decomposeInsertSliceAsyncOp(mod)))\n-      return signalPassFailure();\n+    decomposeInsertSliceAsyncOp(mod);\n \n     // Allocate shared memory and set barrier\n     ModuleAllocation allocation(mod);\n@@ -487,7 +486,7 @@ class ConvertTritonGPUToLLVM\n     });\n   }\n \n-  LogicalResult decomposeInsertSliceAsyncOp(ModuleOp mod) const {\n+  void decomposeInsertSliceAsyncOp(ModuleOp mod) const {\n     ModuleAxisInfoAnalysis axisInfoAnalysis(mod);\n     // TODO(Keren): This is a hacky knob that may cause performance regression\n     // when decomposition has been performed. We should remove this knob once we\n@@ -515,6 +514,7 @@ class ConvertTritonGPUToLLVM\n       // Get the vectorized load size\n       auto src = insertSliceAsyncOp.getSrc();\n       auto dst = insertSliceAsyncOp.getDst();\n+      auto mask = insertSliceAsyncOp.getMask();\n       auto srcTy = src.getType().cast<RankedTensorType>();\n       auto dstTy = dst.getType().cast<RankedTensorType>();\n       auto srcBlocked =\n@@ -523,6 +523,9 @@ class ConvertTritonGPUToLLVM\n           dstTy.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n       auto resElemTy = dstTy.getElementType();\n       unsigned inVec = axisInfoAnalysis.getPtrContiguity(src);\n+      if (mask)\n+        inVec =\n+            std::min<unsigned>(axisInfoAnalysis.getMaskAlignment(mask), inVec);\n       unsigned outVec = resSharedLayout.getVec();\n       unsigned minVec = std::min(outVec, inVec);\n       auto maxBitWidth =\n@@ -586,7 +589,6 @@ class ConvertTritonGPUToLLVM\n         asyncWaitOp.erase();\n       }\n     });\n-    return success();\n   }\n };\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 192, "deletions": 107, "changes": 299, "file_content_changes": "@@ -8,6 +8,7 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"llvm/ADT/MapVector.h\"\n \n //===----------------------------------------------------------------------===//\n //\n@@ -17,6 +18,7 @@\n //\n //===----------------------------------------------------------------------===//\n \n+using llvm::MapVector;\n using namespace mlir;\n namespace ttg = triton::gpu;\n \n@@ -79,31 +81,46 @@ class LoopPipeliner {\n   /// value (in loop) => value at stage N\n   DenseMap<Value, SmallVector<Value>> valueMapping;\n \n-  /// Block arguments that loads depend on\n-  SetVector<BlockArgument> depArgs;\n-\n+  /// For each argument, we need to record at which stage it is defined.\n   /// If we have a load that immediately depends on a block argument in the\n   /// current iteration, it is an immediate dependency. Otherwise, it is a\n   /// non-immediate dependency, which means the load depends on a block argument\n   /// in the previous iterations.\n   /// For example:\n   /// scf.for (%arg0, %arg1, %arg2) {\n-  ///   %0 = load %arg0  <--- immediate dep, this address is initialized at\n-  ///   numStages-2\n+  ///   %0 = load %arg0  <--- immediate dep, this address is initialized before\n+  ///   numStages-1\n   ///   %1 = load %arg1\n   ///   %2 = add %1, %arg2\n   ///   %3 = load %2  <--- non-immediate dep, %arg1 must be an update-to-date\n   ///   value\n   /// }\n-  SetVector<BlockArgument> immedidateDepArgs;\n+  /// Collect values that v depends on and are defined inside the loop\n+  LogicalResult collectDeps(Value v, int stage,\n+                            MapVector<Value, int> &depStage);\n+\n+  /// Associate each variable with a unique stage. If a variable is defined\n+  /// at multiple stages, we don't pipeline it.\n+  LogicalResult addDep(Value v, int stage, MapVector<Value, int> &depStage);\n+\n+  int getArgDefStage(Value v, int stage);\n+\n+  /// Block arguments that loads depend on\n+  MapVector<BlockArgument, int> depArgUseStage;\n \n-  SetVector<BlockArgument> nonImmedidateDepArgs;\n+  /// Block arguments that loads depend on (defined in the loop body)\n+  MapVector<BlockArgument, int> depArgDefStage;\n \n   /// Operations (inside the loop body) that loads depend on\n-  SetVector<Operation *> depOps;\n+  MapVector<Operation *, int> depOpDefStage;\n \n-  /// collect values that v depends on and are defined inside the loop\n-  void collectDeps(Value v, int stages, SetVector<Value> &deps);\n+  /// Operations (inside the loop body) that loads depend on (defined in the\n+  /// loop body)\n+  SetVector<BlockArgument> immediateDepArgs;\n+\n+  /// Operations (inside the loop body) that loads depend on (defined in the\n+  /// previous iterations)\n+  SetVector<BlockArgument> nonImmediateDepArgs;\n \n   void setValueMapping(Value origin, Value newValue, int stage);\n \n@@ -150,31 +167,60 @@ Value LoopPipeliner::lookupOrDefault(Value origin, int stage) {\n   return valueMapping[origin][stage];\n }\n \n-void LoopPipeliner::collectDeps(Value v, int stages, SetVector<Value> &deps) {\n+LogicalResult LoopPipeliner::addDep(Value v, int stage,\n+                                    MapVector<Value, int> &depStage) {\n+  if (!depStage.contains(v)) {\n+    depStage.insert(std::make_pair(v, stage));\n+  } else if (depStage[v] != stage)\n+    return failure();\n+  return success();\n+}\n+\n+LogicalResult LoopPipeliner::collectDeps(Value v, int stage,\n+                                         MapVector<Value, int> &depStage) {\n   // Loop-invariant value, skip\n   if (v.getParentRegion() != &forOp.getLoopBody())\n-    return;\n+    return success();\n \n   // Since we only need to peel the loop numStages-1 times, don't worry about\n   // depends that are too far away\n-  if (stages < 0)\n-    return;\n+  if (stage < 0)\n+    return success();\n \n   if (auto arg = v.dyn_cast<BlockArgument>()) {\n+    // Skip the first arg (loop induction variable)\n+    // Otherwise the op idx is arg.getArgNumber()-1\n     if (arg.getArgNumber() > 0) {\n-      // Skip the first arg (loop induction variable)\n-      // Otherwise the op idx is arg.getArgNumber()-1\n-      deps.insert(v);\n-      collectDeps(yieldOp->getOperand(arg.getArgNumber() - 1), stages - 1,\n-                  deps);\n+      // If we've found the first definition of this arg, we're done, don't\n+      // recurse\n+      if (addDep(v, stage, depStage).succeeded())\n+        if (collectDeps(yieldOp->getOperand(arg.getArgNumber() - 1), stage - 1,\n+                        depStage)\n+                .failed())\n+          return failure();\n     }\n   } else { // value\n-    // v might be in deps, but we still need to visit v.\n-    // This is because v might depend on value in previous iterations\n-    deps.insert(v);\n+    // An operation cannot be dependent on different stages\n+    if (addDep(v, stage, depStage).failed())\n+      return failure();\n     for (Value op : v.getDefiningOp()->getOperands())\n-      collectDeps(op, stages, deps);\n+      if (collectDeps(op, stage, depStage).failed())\n+        return failure();\n   }\n+  return success();\n+}\n+\n+int LoopPipeliner::getArgDefStage(Value v, int stage) {\n+  if (stage < 0)\n+    return -1;\n+  if (auto arg = v.dyn_cast<BlockArgument>()) {\n+    if (arg.getArgNumber() > 0) {\n+      return getArgDefStage(yieldOp->getOperand(arg.getArgNumber() - 1),\n+                            stage - 1);\n+    }\n+    llvm_unreachable(\"Loop induction variable should not be a dependency\");\n+  } else\n+    return stage;\n }\n \n ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(Operation *op,\n@@ -198,7 +244,8 @@ LogicalResult LoopPipeliner::initialize() {\n   ModuleOp moduleOp = forOp->getParentOfType<ModuleOp>();\n   ModuleAxisInfoAnalysis axisInfoAnalysis(moduleOp);\n \n-  // can we use forOp.walk(...) here?\n+  // We cannot use forOp.walk(...) here because we only want to visit the\n+  // operations in the loop body block. Nested blocks are handled separately.\n   SmallVector<triton::LoadOp, 2> validLoads;\n   for (Operation &op : *loop)\n     if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n@@ -215,7 +262,11 @@ LogicalResult LoopPipeliner::initialize() {\n                     .cast<triton::PointerType>()\n                     .getPointeeType();\n       unsigned width = vec * ty.getIntOrFloatBitWidth();\n-      // cp.async's cp-size can only be 4, 8 and 16.\n+      // We do not pipeline all loads for the following reasons:\n+      // 1. On nvidia GPUs, cp.async's cp-size can only be 4, 8 and 16.\n+      // 2. It's likely that pipling small load won't offer much performance\n+      //    improvement and may even hurt performance by increasing register\n+      //    pressure.\n       if (width >= 32)\n         validLoads.push_back(loadOp);\n     }\n@@ -225,12 +276,28 @@ LogicalResult LoopPipeliner::initialize() {\n     return failure();\n \n   // load => values that it depends on\n+  // Don't pipeline if any load's operands\n   DenseMap<Value, SetVector<Value>> loadDeps;\n+  MapVector<Value, int> depStage;\n   for (triton::LoadOp loadOp : validLoads) {\n-    SetVector<Value> deps;\n-    for (Value op : loadOp->getOperands())\n-      collectDeps(op, numStages - 1, deps);\n-    loadDeps[loadOp] = deps;\n+    for (Value op : loadOp->getOperands()) {\n+      MapVector<Value, int> operandDepStage;\n+      if (collectDeps(op, numStages - 1, operandDepStage).failed())\n+        return failure();\n+      for (auto [v, stage] : operandDepStage) {\n+        auto immedidate = operandDepStage.front().first.isa<BlockArgument>();\n+        if (v.isa<BlockArgument>()) {\n+          auto arg = v.cast<BlockArgument>();\n+          if (immedidate)\n+            immediateDepArgs.insert(arg);\n+          else\n+            nonImmediateDepArgs.insert(arg);\n+        }\n+        loadDeps[loadOp].insert(v);\n+        if (addDep(v, stage, depStage).failed())\n+          return failure();\n+      }\n+    }\n   }\n \n   // Don't pipeline valid loads that depend on other valid loads\n@@ -278,9 +345,7 @@ LogicalResult LoopPipeliner::initialize() {\n         continue;\n       isCandidate = true;\n       loadsMapping[loadOp] = convertLayout;\n-    }\n-\n-    else\n+    } else\n       isCandidate = false;\n \n     if (isCandidate)\n@@ -316,35 +381,27 @@ LogicalResult LoopPipeliner::initialize() {\n     bufferShape.insert(bufferShape.begin(), numStages);\n     auto sharedEnc = ttg::SharedEncodingAttr::get(\n         ty.getContext(), dotOpEnc, ty.getShape(),\n-        triton::gpu::getOrder(ty.getEncoding()), loadsSmallestType[loadOp]);\n+        ttg::getOrder(ty.getEncoding()), loadsSmallestType[loadOp]);\n     loadsBufferType[loadOp] =\n         RankedTensorType::get(bufferShape, ty.getElementType(), sharedEnc);\n   }\n \n   // We have some loads to pipeline\n   if (!loads.empty()) {\n     // Update depArgs & depOps\n-    for (Value loadOp : loads) {\n-      auto &deps = loadDeps[loadOp];\n-      for (auto &dep : deps) {\n-        if (auto arg = dep.dyn_cast<BlockArgument>()) {\n-          depArgs.insert(arg);\n-          if (deps.front().isa<BlockArgument>()) {\n-            immedidateDepArgs.insert(arg);\n-          } else {\n-            nonImmedidateDepArgs.insert(arg);\n-          }\n-        } else\n-          depOps.insert(dep.getDefiningOp());\n-      }\n+    for (auto [dep, stage] : depStage) {\n+      if (auto arg = dep.dyn_cast<BlockArgument>())\n+        depArgUseStage.insert({arg, stage});\n+      else\n+        depOpDefStage.insert({dep.getDefiningOp(), stage});\n     }\n     return success();\n   }\n \n   // Check if immedidateDepArgs and nonImmedidateDepArgs are disjoint\n   // If yes, we cannot pipeline the loop for now\n-  for (BlockArgument arg : immedidateDepArgs)\n-    if (nonImmedidateDepArgs.contains(arg)) {\n+  for (BlockArgument arg : immediateDepArgs)\n+    if (nonImmediateDepArgs.contains(arg)) {\n       return failure();\n     }\n \n@@ -375,12 +432,13 @@ Value LoopPipeliner::getLoadMask(triton::LoadOp loadOp, Value mappedMask,\n \n void LoopPipeliner::emitPrologue() {\n   OpBuilder builder(forOp);\n+  // Get init operands for loop carried values\n   for (BlockArgument &arg : forOp.getRegionIterArgs()) {\n     OpOperand &operand = forOp.getOpOperandForRegionIterArg(arg);\n     setValueMapping(arg, operand.get(), 0);\n   }\n \n-  // prologue from [0, numStage-1)\n+  // Emit prologue from [0, numStage-1)\n   Value iv = forOp.getLowerBound();\n   pipelineIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n   for (int stage = 0; stage < numStages - 1; ++stage) {\n@@ -396,12 +454,12 @@ void LoopPipeliner::emitPrologue() {\n     // Rematerialize peeled values\n     SmallVector<Operation *> orderedDeps;\n     for (Operation &op : forOp.getLoopBody().front()) {\n-      if (depOps.contains(&op))\n+      if (depOpDefStage.contains(&op))\n         orderedDeps.push_back(&op);\n       else if (op.getNumResults() > 0 && loads.contains(op.getResult(0)))\n         orderedDeps.push_back(&op);\n     }\n-    assert(depOps.size() + loads.size() == orderedDeps.size() &&\n+    assert(depOpDefStage.size() + loads.size() == orderedDeps.size() &&\n            \"depOps contains invalid values\");\n     for (Operation *op : orderedDeps) {\n       Operation *newOp = nullptr;\n@@ -416,14 +474,13 @@ void LoopPipeliner::emitPrologue() {\n           Value newMask =\n               getLoadMask(loadOp, lookupOrDefault(loadOp.getMask(), stage),\n                           loopCond, builder);\n-          // TODO: check if the hardware supports async copy\n-          newOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n+          newOp = builder.create<ttg::InsertSliceAsyncOp>(\n               op->getLoc(), loadsBuffer[loadOp].getType(),\n               lookupOrDefault(loadOp.getPtr(), stage),\n               loadStageBuffer[loadOp][stage], pipelineIterIdx, newMask,\n               lookupOrDefault(loadOp.getOther(), stage), loadOp.getCache(),\n               loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n-          builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n+          builder.create<ttg::AsyncCommitGroupOp>(op->getLoc());\n           loadStageBuffer[loadOp].push_back(newOp->getResult(0));\n         } else\n           llvm_unreachable(\"This should be LoadOp\");\n@@ -439,9 +496,8 @@ void LoopPipeliner::emitPrologue() {\n               loadOp.getBoundaryCheckAttr(), loadOp.getPaddingAttr(),\n               loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n           addNamedAttrs(newOp, op->getDiscardableAttrDictionary());\n-        } else {\n+        } else\n           newOp = builder.clone(*op);\n-        }\n         // Update loop-carried uses\n         for (unsigned opIdx = 0; opIdx < op->getNumOperands(); ++opIdx) {\n           auto it = valueMapping.find(op->getOperand(opIdx));\n@@ -453,32 +509,40 @@ void LoopPipeliner::emitPrologue() {\n         }\n       }\n \n-      // Update mapping of results\n-      // if (stage == numStages - 2)\n-      //   continue;\n-\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n-        Value originalResult = op->getResult(dstIdx);\n+        Value originResult = op->getResult(dstIdx);\n         // copy_async will update the value of its only use\n-        // TODO: load should not be used in the preheader?\n-        if (loads.contains(originalResult)) {\n+        if (loads.contains(originResult))\n           break;\n-          // originalResult = loadsMapping[originalResult];\n-        }\n-        setValueMapping(originalResult, newOp->getResult(dstIdx), stage);\n+        setValueMapping(originResult, newOp->getResult(dstIdx), stage);\n         // update mapping for loop-carried values (args)\n         for (OpOperand &operand : yieldOp->getOpOperands()) {\n-          if (operand.get() == op->getResult(dstIdx))\n-            setValueMapping(\n-                forOp.getRegionIterArgs()[operand.getOperandNumber()],\n-                newOp->getResult(dstIdx), stage + 1);\n+          if (operand.get() == op->getResult(dstIdx)) {\n+            auto yieldIdx = operand.getOperandNumber();\n+            auto value = forOp.getRegionIterArgs()[yieldIdx];\n+            setValueMapping(value, newOp->getResult(dstIdx), stage + 1);\n+          }\n         }\n       }\n     } // for (Operation *op : orderedDeps)\n \n+    // Update pipeline index\n     pipelineIterIdx = builder.create<arith::AddIOp>(\n         iv.getLoc(), pipelineIterIdx,\n         builder.create<arith::ConstantIntOp>(iv.getLoc(), 1, 32));\n+\n+    // Some values have not been used by any ops in the loop body\n+    for (BlockArgument arg : forOp.getRegionIterArgs()) {\n+      // Check if arg has a yieldOp use\n+      for (OpOperand &operand : arg.getUses()) {\n+        if (operand.getOwner() == yieldOp) {\n+          auto yieldIdx = operand.getOperandNumber();\n+          auto value = forOp.getRegionIterArgs()[yieldIdx];\n+          if (!valueMapping[value][stage + 1])\n+            setValueMapping(value, valueMapping[arg][stage], stage + 1);\n+        }\n+      }\n+    }\n   } // for (int stage = 0; stage < numStages - 1; ++stage)\n \n   // async.wait & extract_slice\n@@ -494,7 +558,7 @@ void LoopPipeliner::emitPrologue() {\n     sliceType = RankedTensorType::get({bufferShape[1], bufferShape[2]},\n                                       sliceType.getElementType(),\n                                       loadsBufferType[loadOp].getEncoding());\n-    Value extractSlice = builder.create<triton::gpu::ExtractSliceOp>(\n+    Value extractSlice = builder.create<ttg::ExtractSliceOp>(\n         loadOp.getLoc(), sliceType, loadStageBuffer[loadOp][numStages - 1],\n         SmallVector<OpFoldResult>{int_attr(0), int_attr(0), int_attr(0)},\n         SmallVector<OpFoldResult>{int_attr(1),\n@@ -515,7 +579,7 @@ void LoopPipeliner::emitEpilogue() {\n   OpBuilder builder(forOp);\n   OpBuilder::InsertionGuard g(builder);\n   builder.setInsertionPointAfter(forOp);\n-  builder.create<triton::gpu::AsyncWaitOp>(forOp.getLoc(), 0);\n+  builder.create<ttg::AsyncWaitOp>(forOp.getLoc(), 0);\n }\n \n scf::ForOp LoopPipeliner::createNewForOp() {\n@@ -547,22 +611,24 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n     newLoopArgs.push_back(loadsExtract[loadOp]);\n \n   size_t depArgsBeginIdx = newLoopArgs.size();\n-  for (BlockArgument depArg : depArgs) {\n+  for (auto [depArg, useStage] : depArgUseStage) {\n     depArgsIdx[depArg] = newLoopArgs.size();\n-    if (immedidateDepArgs.contains(depArg)) {\n+    auto defStage = getArgDefStage(depArg, useStage);\n+    assert(defStage >= 0 &&\n+           \"newLoopArgs has null args without a define op. Consider either \"\n+           \"rewrite the loop to reduce cross iteration dependencies or \"\n+           \"increase the num_stages value.\");\n+    if (immediateDepArgs.contains(depArg) && defStage == numStages - 2) {\n       newLoopArgs.push_back(valueMapping[depArg][numStages - 2]);\n     } else\n       newLoopArgs.push_back(valueMapping[depArg][numStages - 1]);\n   }\n \n-  size_t nextIVIdx = newLoopArgs.size();\n+  size_t ivIndex = newLoopArgs.size();\n   newLoopArgs.push_back(valueMapping[forOp.getInductionVar()][numStages - 2]);\n   newLoopArgs.push_back(pipelineIterIdx);\n   newLoopArgs.push_back(loopIterIdx);\n \n-  for (size_t i = 0; i < newLoopArgs.size(); ++i)\n-    assert(newLoopArgs[i]);\n-\n   // 1. signature of the new ForOp\n   auto newForOp = builder.create<scf::ForOp>(\n       forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n@@ -575,7 +641,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n   mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n \n-  // 2. clone the loop body, replace original args with args of the new ForOp\n+  // 3. clone the loop body, replace original args with args of the new ForOp\n   // Insert async wait if necessary.\n   DenseSet<Value> isModified;\n   for (Operation &op : forOp.getBody()->without_terminator()) {\n@@ -607,50 +673,54 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n     isModified.insert(op.getResult(0));\n   }\n \n-  // 3. prefetch the next iteration\n+  // 4. prefetch the next iteration\n   SmallVector<Operation *> orderedDeps;\n   for (Operation &op : forOp.getLoopBody().front()) {\n-    if (depOps.contains(&op))\n+    if (depOpDefStage.contains(&op))\n       orderedDeps.push_back(&op);\n     else if (op.getNumResults() > 0 && loads.contains(op.getResult(0)))\n       orderedDeps.push_back(&op);\n   }\n-  assert(depOps.size() + loads.size() == orderedDeps.size() &&\n+  assert(depOpDefStage.size() + loads.size() == orderedDeps.size() &&\n          \"depOps contains invalid values\");\n   IRMapping nextMapping;\n   DenseMap<BlockArgument, Value> depArgsMapping;\n   size_t argIdx = 0;\n-  for (BlockArgument arg : depArgs) {\n+  for (auto [depArg, useStage] : depArgUseStage) {\n     BlockArgument nextArg =\n         newForOp.getRegionIterArgs()[argIdx + depArgsBeginIdx];\n-    nextMapping.map(arg, nextArg);\n+    nextMapping.map(depArg, nextArg);\n     ++argIdx;\n   }\n \n   // Special handling for iv & loop condition\n+  Value curIV = newForOp.getRegionIterArgs()[ivIndex];\n   Value nextIV = builder.create<arith::AddIOp>(\n-      newForOp.getInductionVar().getLoc(),\n-      newForOp.getRegionIterArgs()[nextIVIdx], newForOp.getStep());\n+      newForOp.getInductionVar().getLoc(), curIV, newForOp.getStep());\n   Value nextLoopCond =\n       builder.create<arith::CmpIOp>(nextIV.getLoc(), arith::CmpIPredicate::slt,\n                                     nextIV, newForOp.getUpperBound());\n-  nextMapping.map(forOp.getInductionVar(), nextIV);\n \n   // Slice index\n   SmallVector<Value> nextBuffers;\n   SmallVector<Value> extractSlices;\n \n-  pipelineIterIdx = newForOp.getRegionIterArgs()[nextIVIdx + 1];\n+  pipelineIterIdx = newForOp.getRegionIterArgs()[ivIndex + 1];\n   Value insertSliceIndex = builder.create<arith::RemSIOp>(\n       nextIV.getLoc(), pipelineIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n-  loopIterIdx = newForOp.getRegionIterArgs()[nextIVIdx + 2];\n+  loopIterIdx = newForOp.getRegionIterArgs()[ivIndex + 2];\n   Value extractSliceIndex = builder.create<arith::RemSIOp>(\n       nextIV.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n \n+  // Prefetch load deps\n   for (Operation *op : orderedDeps)\n     if (!loads.contains(op->getResult(0))) {\n+      if (depOpDefStage[op] == numStages - 2)\n+        nextMapping.map(forOp.getInductionVar(), curIV);\n+      else\n+        nextMapping.map(forOp.getInductionVar(), nextIV);\n       Operation *nextOp;\n       if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n         auto newMask =\n@@ -668,21 +738,22 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n         nextOp = builder.clone(*op, nextMapping);\n       }\n \n-      auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n-        for (OpOperand &operand : originYield->getOpOperands()) {\n+        for (OpOperand &operand : yieldOp->getOpOperands()) {\n           if (operand.get() == op->getResult(dstIdx)) {\n-            size_t originIdx = operand.getOperandNumber();\n-            size_t newArgIdx = depArgsIdx[forOp.getRegionIterArgs()[originIdx]];\n-            BlockArgument newArg = newForOp.getRegionIterArgs()[newArgIdx];\n-            nextMapping.map(forOp.getRegionIterArgs()[originIdx],\n+            size_t yieldIdx = operand.getOperandNumber();\n+            size_t depYieldIdx =\n+                depArgsIdx[forOp.getRegionIterArgs()[yieldIdx]];\n+            BlockArgument newArg = newForOp.getRegionIterArgs()[depYieldIdx];\n+            nextMapping.map(forOp.getRegionIterArgs()[yieldIdx],\n                             nextOp->getResult(dstIdx));\n             depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n           }\n         }\n       }\n     }\n \n+  // loads -> async loads\n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n     // Update loading mask\n@@ -699,14 +770,14 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n           nextMapping.map(loadOp.getMask(), newMask);\n         newMask = nextMapping.lookupOrDefault(mask);\n       }\n-      Value insertAsyncOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n+      Value insertAsyncOp = builder.create<ttg::InsertSliceAsyncOp>(\n           op->getLoc(), loadsBuffer[loadOp].getType(),\n           nextMapping.lookupOrDefault(loadOp.getPtr()),\n           newForOp.getRegionIterArgs()[bufferIdx + nextBuffers.size()],\n           insertSliceIndex, newMask,\n           nextMapping.lookupOrDefault(loadOp.getOther()), loadOp.getCache(),\n           loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n-      builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n+      builder.create<ttg::AsyncCommitGroupOp>(op->getLoc());\n       nextBuffers.push_back(insertAsyncOp);\n       // ExtractSlice\n       auto bufferType = insertAsyncOp.getType().cast<RankedTensorType>();\n@@ -716,7 +787,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n                                         sliceType.getElementType(),\n                                         loadsBufferType[loadOp].getEncoding());\n \n-      nextOp = builder.create<triton::gpu::ExtractSliceOp>(\n+      nextOp = builder.create<ttg::ExtractSliceOp>(\n           op->getLoc(), sliceType, insertAsyncOp,\n           SmallVector<OpFoldResult>{extractSliceIndex, int_attr(0),\n                                     int_attr(0)},\n@@ -730,19 +801,34 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n         nextMapping.map(op->getResult(dstIdx), nextOp->getResult(dstIdx));\n         // If this is a loop-carried value, update the mapping for yield\n-        auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n-        for (OpOperand &operand : originYield->getOpOperands()) {\n+        for (OpOperand &operand : yieldOp->getOpOperands()) {\n           if (operand.get() == op->getResult(dstIdx)) {\n-            size_t originIdx = operand.getOperandNumber();\n-            size_t newArgIdx = depArgsIdx[forOp.getRegionIterArgs()[originIdx]];\n-            BlockArgument newArg = newForOp.getRegionIterArgs()[newArgIdx];\n+            auto yieldIdx = operand.getOperandNumber();\n+            auto depYieldIdx = depArgsIdx[forOp.getRegionIterArgs()[yieldIdx]];\n+            auto newArg = newForOp.getRegionIterArgs()[depYieldIdx];\n             depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n           }\n         }\n       }\n     }\n   }\n \n+  // Some values have not been used by any ops in the loop body\n+  for (BlockArgument arg : forOp.getRegionIterArgs()) {\n+    // Check if arg has a yieldOp use\n+    for (OpOperand &operand : arg.getUses()) {\n+      if (operand.getOwner() == yieldOp) {\n+        auto yieldIdx = operand.getOperandNumber();\n+        auto depYieldIdx = depArgsIdx[forOp.getRegionIterArgs()[yieldIdx]];\n+        auto newArg = newForOp.getRegionIterArgs()[depYieldIdx];\n+        if (!depArgsMapping.contains(newArg)) {\n+          auto argIdx = depArgsIdx[arg];\n+          depArgsMapping[newArg] = newForOp.getRegionIterArgs()[argIdx];\n+        }\n+      }\n+    }\n+  }\n+\n   // async.wait & extract_slice\n   Operation *asyncWait = builder.create<ttg::AsyncWaitOp>(\n       loads[0].getLoc(), loads.size() * (numStages - 2));\n@@ -761,14 +847,14 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   // Finally, the YieldOp, need to sync with the order of newLoopArgs\n   SmallVector<Value> yieldValues;\n-  for (Value v : forOp.getBody()->getTerminator()->getOperands())\n+  for (Value v : yieldOp->getOperands())\n     yieldValues.push_back(mapping.lookup(v));\n   for (Value nextBuffer : nextBuffers)\n     yieldValues.push_back(nextBuffer);\n   for (Value nextSlice : extractSlices)\n     yieldValues.push_back(nextSlice);\n \n-  for (size_t i = depArgsBeginIdx; i < nextIVIdx; ++i) {\n+  for (size_t i = depArgsBeginIdx; i < ivIndex; ++i) {\n     auto arg = newForOp.getRegionIterArgs()[i];\n     assert(depArgsMapping.count(arg) && \"Missing loop-carried value\");\n     yieldValues.push_back(depArgsMapping[arg]);\n@@ -778,8 +864,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   yieldValues.push_back(loopIterIdx);\n \n   builder.setInsertionPointToEnd(newForOp.getBody());\n-  builder.create<scf::YieldOp>(forOp.getBody()->getTerminator()->getLoc(),\n-                               yieldValues);\n+  builder.create<scf::YieldOp>(yieldOp->getLoc(), yieldValues);\n   return newForOp;\n }\n "}, {"filename": "python/test/regression/test_functional_regressions.py", "status": "modified", "additions": 94, "deletions": 0, "changes": 94, "file_content_changes": "@@ -1,4 +1,5 @@\n import numpy as np\n+import pytest\n import torch\n from numpy.random import RandomState\n \n@@ -134,3 +135,96 @@ def batched_vecmat(\n     C_ref = np.sum(AB, axis=2)\n \n     np.testing.assert_allclose(C_ref, C_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+\n+\n+@pytest.mark.parametrize(\"type\", [\"pre_load\", \"post_load\", \"post_pre_mixed\", \"post_load_two_iters\", \"post_load_three_iters\"])\n+def test_iv_dependent_matmul(type):\n+    @triton.jit\n+    def kernel(\n+        a_ptr, b_ptr, c_ptr,\n+        M, N, K,\n+        stride_am, stride_ak,\n+        stride_bk, stride_bn,\n+        stride_cm, stride_cn,\n+        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+        type: tl.constexpr\n+    ):\n+        pid = tl.program_id(axis=0)\n+        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+        pid_m = pid // num_pid_n\n+        pid_n = pid % num_pid_n\n+\n+        offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n+        offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n+        offs_k = tl.arange(0, BLOCK_SIZE_K)\n+        a_ptr = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+        b_ptr = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+        a_ptrs = a_ptr\n+        b_ptrs = b_ptr\n+        if type == \"post_load_two_iters\":\n+            a_ptrs_next = a_ptr + BLOCK_SIZE_K * stride_ak\n+            b_ptrs_next = b_ptr + BLOCK_SIZE_K * stride_bk\n+        elif type == \"post_load_three_iters\":\n+            a_ptrs_next = a_ptr + BLOCK_SIZE_K * stride_ak\n+            b_ptrs_next = b_ptr + BLOCK_SIZE_K * stride_bk\n+            a_ptrs_next_next = a_ptr + 2 * BLOCK_SIZE_K * stride_ak\n+            b_ptrs_next_next = b_ptr + 2 * BLOCK_SIZE_K * stride_bk\n+\n+        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n+            if type == \"pre_load\":\n+                a_ptrs = a_ptr + k * BLOCK_SIZE_K * stride_ak\n+                b_ptrs = b_ptr + k * BLOCK_SIZE_K * stride_bk\n+            elif type == \"post_pre_mixed\":\n+                a_ptrs = a_ptr + k * BLOCK_SIZE_K * stride_ak\n+            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n+            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n+            accumulator += tl.dot(a, b)\n+            if type == \"post_load\":\n+                a_ptrs = a_ptr + (k + 1) * BLOCK_SIZE_K * stride_ak\n+                b_ptrs = b_ptr + (k + 1) * BLOCK_SIZE_K * stride_bk\n+            elif type == \"post_pre_mixed\":\n+                b_ptrs = b_ptr + (k + 1) * BLOCK_SIZE_K * stride_bk\n+            elif type == \"post_load_two_iters\":\n+                a_ptrs = a_ptrs_next\n+                b_ptrs = b_ptrs_next\n+                a_ptrs_next = a_ptr + (k + 2) * BLOCK_SIZE_K * stride_ak\n+                b_ptrs_next = b_ptr + (k + 2) * BLOCK_SIZE_K * stride_bk\n+            elif type == \"post_load_three_iters\":\n+                a_ptrs = a_ptrs_next\n+                b_ptrs = b_ptrs_next\n+                a_ptrs_next = a_ptrs_next_next\n+                b_ptrs_next = b_ptrs_next_next\n+                a_ptrs_next_next = a_ptr + (k + 3) * BLOCK_SIZE_K * stride_ak\n+                b_ptrs_next_next = b_ptr + (k + 3) * BLOCK_SIZE_K * stride_bk\n+        c = accumulator.to(tl.float16)\n+\n+        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n+        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n+        tl.store(c_ptrs, c, mask=c_mask)\n+\n+    M = 256\n+    K = 256\n+    N = 256\n+    BLOCK_SIZE_K = 32\n+    BLOCK_SIZE_N = 32\n+    BLOCK_SIZE_M = 32\n+\n+    a = torch.rand((M, K), device='cuda')\n+    b = torch.rand((K, N), device='cuda')\n+\n+    torch_output = torch.mm(a, b)\n+    triton_output = torch.empty_like(\n+        torch_output, device=torch_output.device)\n+\n+    def grid(META):\n+        return (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n+\n+    num_stages = 4 if type == \"post_load_three_iters\" else 3\n+    kernel[grid](a, b, triton_output, M, N, K, a.stride(0), a.stride(1),\n+                 b.stride(0), b.stride(1), triton_output.stride(0), triton_output.stride(1),\n+                 BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n+                 type=type, num_stages=num_stages)\n+    torch.testing.assert_allclose(torch_output, triton_output, rtol=1e-2, atol=1e-2)"}]