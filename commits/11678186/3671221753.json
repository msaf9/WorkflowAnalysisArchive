[{"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -593,6 +593,8 @@ Attribute DotOperandEncodingAttr::parse(AsmParser &parser, Type type) {\n   if(parent.isa<MmaEncodingAttr>() &&\n      parent.cast<MmaEncodingAttr>().getVersion() == 1){\n     isMMAv1Row = attrs.get(\"isMMAv1Row\");\n+    if(!isMMAv1Row)\n+      llvm::report_fatal_error(\"isMMAv1Row attribute is missing\");\n   }\n   return parser.getChecked<DotOperandEncodingAttr>(parser.getContext(), opIdx,\n                                                    parent, isMMAv1Row);"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -879,8 +879,8 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n #mma = #triton_gpu.mma<{version = 1, warpsPerCTA = [2, 2]}>\n-#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n-#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma, isMMAv1Row=true}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma, isMMAv1Row=true}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   func @matmul884_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<128x32xf16, #shared>, %b:tensor<32x256xf16, #shared>) {"}]