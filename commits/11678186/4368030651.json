[{"filename": "bin/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 7, "changes": 7, "file_content_changes": "@@ -1,10 +1,3 @@\n-add_custom_target(copy-FileCheck ALL\n-    COMMAND ${CMAKE_COMMAND} -E copy\n-        \"${LLVM_LIBRARY_DIR}/../bin/FileCheck\"\n-        \"${CMAKE_CURRENT_BINARY_DIR}/FileCheck\"\n-    COMMENT \"Copying FileCheck to the current directory\"\n-)\n-\n get_property(dialect_libs GLOBAL PROPERTY MLIR_DIALECT_LIBS)\n get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -35,7 +35,7 @@ Right now, Triton implements two classes of layouts: shared, and distributed.\n   }];\n \n   code extraBaseClassDeclaration = [{\n-    unsigned getElemsPerThread(ArrayRef<int64_t> shape) const;\n+    unsigned getElemsPerThread(ArrayRef<int64_t> shape, Type eltTy) const;\n     ::mlir::LogicalResult verifyLayoutForArg(::mlir::Operation* op, unsigned argNo) const;\n   }];\n }"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -23,7 +23,7 @@ std::unique_ptr<Pass> createTritonGPURemoveLayoutConversionsPass();\n \n std::unique_ptr<Pass> createTritonGPUVerifier();\n \n-std::unique_ptr<Pass> createTritonGPUFuseTranspositionsPass();\n+std::unique_ptr<Pass> createTritonGPUOptimizeDotOperandsPass();\n \n std::unique_ptr<Pass> createTritonGPUUpdateMmaForVoltaPass();\n "}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -60,15 +60,15 @@ def TritonGPUAccelerateMatmul : Pass<\"tritongpu-accelerate-matmul\", \"mlir::Modul\n \n }\n \n-def TritonGPUFuseTranspositions : Pass<\"tritongpu-fuse-transposition\", \"mlir::ModuleOp\"> {\n+def TritonGPUOptimizeDotOperands : Pass<\"tritongpu-optimize-dot-operands\", \"mlir::ModuleOp\"> {\n   let summary = \"fuse transpositions\";\n \n   let description = [{\n     Re-arranged layouts of tensors used as matrix multiplication operands so as to promote the use of\n     hardware-accelerated transpositions.\n   }];\n \n-  let constructor = \"mlir::createTritonGPUFuseTranspositionsPass()\";\n+  let constructor = \"mlir::createTritonGPUOptimizeDotOperandsPass()\";\n \n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n                            \"mlir::triton::TritonDialect\"];\n@@ -86,6 +86,7 @@ def TritonGPUCoalesce: Pass<\"tritongpu-coalesce\", \"mlir::ModuleOp\"> {\n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n }\n \n+\n def TritonGPURemoveLayoutConversions : Pass<\"tritongpu-remove-layout-conversions\", \"mlir::ModuleOp\"> {\n   let summary = \"remove superfluous layout conversions\";\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 11, "deletions": 7, "changes": 18, "file_content_changes": "@@ -54,13 +54,14 @@ struct ConvertLayoutOpConversion\n private:\n   SmallVector<Value> getMultiDimOffset(Attribute layout, Location loc,\n                                        ConversionPatternRewriter &rewriter,\n-                                       unsigned elemId, ArrayRef<int64_t> shape,\n+                                       unsigned elemId, RankedTensorType type,\n                                        ArrayRef<unsigned> multiDimCTAInRepId,\n                                        ArrayRef<unsigned> shapePerCTA) const {\n+    auto shape = type.getShape();\n     unsigned rank = shape.size();\n     if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n       auto multiDimOffsetFirstElem =\n-          emitBaseIndexForLayout(loc, rewriter, blockedLayout, shape);\n+          emitBaseIndexForLayout(loc, rewriter, blockedLayout, type);\n       SmallVector<Value> multiDimOffset(rank);\n       SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n           elemId, getSizePerThread(layout), getOrder(layout));\n@@ -73,9 +74,12 @@ struct ConvertLayoutOpConversion\n     }\n     if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n       unsigned dim = sliceLayout.getDim();\n+      auto parentEncoding = sliceLayout.getParent();\n+      auto parentShape = sliceLayout.paddedShape(shape);\n+      auto parentTy = RankedTensorType::get(parentShape, type.getElementType(),\n+                                            parentEncoding);\n       auto multiDimOffsetParent =\n-          getMultiDimOffset(sliceLayout.getParent(), loc, rewriter, elemId,\n-                            sliceLayout.paddedShape(shape),\n+          getMultiDimOffset(parentEncoding, loc, rewriter, elemId, parentTy,\n                             sliceLayout.paddedShape(multiDimCTAInRepId),\n                             sliceLayout.paddedShape(shapePerCTA));\n       SmallVector<Value> multiDimOffset(rank);\n@@ -193,7 +197,7 @@ struct ConvertLayoutOpConversion\n       //       of performance issue observed.\n       for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n         SmallVector<Value> multiDimOffset =\n-            getMultiDimOffset(layout, loc, rewriter, elemId, type.getShape(),\n+            getMultiDimOffset(layout, loc, rewriter, elemId, type,\n                               multiDimCTAInRepId, shapePerCTA);\n         Value offset =\n             linearize(rewriter, loc, multiDimOffset, paddedRepShape, outOrd);\n@@ -283,7 +287,7 @@ struct ConvertLayoutOpConversion\n         // TODO[Superjomn]: Move the coordinate computation out of loop, it is\n         // duplicate in Volta.\n         SmallVector<Value> multiDimOffset =\n-            getMultiDimOffset(layout, loc, rewriter, elemId, type.getShape(),\n+            getMultiDimOffset(layout, loc, rewriter, elemId, type,\n                               multiDimCTAInRepId, shapePerCTA);\n         coord2val[elemId] = std::make_pair(multiDimOffset, vals[elemId]);\n       }\n@@ -476,7 +480,7 @@ struct ConvertLayoutOpConversion\n \n     auto dstStrides =\n         getStridesFromShapeAndOrder(dstShape, outOrd, loc, rewriter);\n-    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy);\n     storeDistributedToShared(src, adaptor.getSrc(), dstStrides, srcIndices, dst,\n                              smemBase, elemTy, loc, rewriter);\n     auto smemObj ="}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -660,7 +660,7 @@ struct InsertSliceOpConversion\n     auto smemBase = gep(elemPtrTy, smemObj.base, offset);\n \n     auto llSrc = adaptor.getSource();\n-    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy);\n     storeDistributedToShared(src, llSrc, srcStrides, srcIndices, dst, smemBase,\n                              elemTy, loc, rewriter);\n     // Barrier is not necessary.\n@@ -785,7 +785,7 @@ struct InsertSliceAsyncOpConversion\n     // single vector read into multiple ones\n     auto numVecCols = std::max<unsigned>(inVec / outVec, 1);\n \n-    auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcShape);\n+    auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcTy);\n \n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n       // 16 * 8 = 128bits"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 8, "changes": 14, "file_content_changes": "@@ -158,12 +158,12 @@ struct ReduceOpConversion\n     indexSmemBase = bitcast(indexSmemBase, indexPtrTy);\n \n     unsigned srcElems = getElemsPerThread(srcTy);\n-    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy);\n     auto srcValues = getTypeConverter()->unpackLLElements(\n         loc, adaptor.getOperand(), rewriter, srcTy);\n \n     SmallVector<SmallVector<unsigned>> offset =\n-        emitOffsetForLayout(srcLayout, srcShape);\n+        emitOffsetForLayout(srcLayout, srcTy);\n \n     std::map<SmallVector<unsigned>, Value> accs;\n     std::map<SmallVector<unsigned>, Value> accIndices;\n@@ -246,8 +246,7 @@ struct ReduceOpConversion\n       auto resultShape = resultTy.getShape();\n \n       unsigned resultElems = getElemsPerThread(resultTy);\n-      auto resultIndices =\n-          emitIndices(loc, rewriter, resultLayout, resultShape);\n+      auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultTy);\n       assert(resultIndices.size() == resultElems);\n \n       SmallVector<Value> resultVals(resultElems);\n@@ -305,12 +304,12 @@ struct ReduceOpConversion\n     unsigned sizeInterWarps = helper.getInterWarpSize();\n \n     unsigned srcElems = getElemsPerThread(srcTy);\n-    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy);\n     auto srcValues = getTypeConverter()->unpackLLElements(\n         loc, adaptor.getOperand(), rewriter, srcTy);\n \n     SmallVector<SmallVector<unsigned>> offset =\n-        emitOffsetForLayout(srcLayout, srcShape);\n+        emitOffsetForLayout(srcLayout, srcTy);\n \n     std::map<SmallVector<unsigned>, Value> accs;\n     std::map<SmallVector<unsigned>, Value> accIndices;\n@@ -444,8 +443,7 @@ struct ReduceOpConversion\n       auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n       auto resultShape = resultTy.getShape();\n       unsigned resultElems = getElemsPerThread(resultTy);\n-      auto resultIndices =\n-          emitIndices(loc, rewriter, resultLayout, resultShape);\n+      auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultTy);\n       assert(resultIndices.size() == resultElems);\n \n       SmallVector<Value> resultVals(resultElems);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -65,8 +65,8 @@ struct BroadcastOpConversion\n \n     assert(rank == resultTy.getRank());\n     auto order = triton::gpu::getOrder(srcLayout);\n-    auto srcOffsets = emitOffsetForLayout(srcLayout, srcShape);\n-    auto resultOffsets = emitOffsetForLayout(resultLayout, resultShape);\n+    auto srcOffsets = emitOffsetForLayout(srcLayout, srcTy);\n+    auto resultOffsets = emitOffsetForLayout(resultLayout, resultTy);\n     SmallVector<Value> srcVals =\n         getTypeConverter()->unpackLLElements(loc, src, rewriter, srcTy);\n \n@@ -353,14 +353,14 @@ struct MakeRangeOpConversion\n   matchAndRewrite(triton::MakeRangeOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n-    auto rankedTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    auto rankedTy = op.getResult().getType().cast<RankedTensorType>();\n     auto shape = rankedTy.getShape();\n     auto layout = rankedTy.getEncoding();\n \n     auto elemTy = rankedTy.getElementType();\n     assert(elemTy.isInteger(32));\n     Value start = createIndexAttrConstant(rewriter, loc, elemTy, op.getStart());\n-    auto idxs = emitIndices(loc, rewriter, layout, shape);\n+    auto idxs = emitIndices(loc, rewriter, layout, rankedTy);\n     unsigned elems = idxs.size();\n     SmallVector<Value> retVals(elems);\n     // TODO: slice layout has more elements than expected."}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 47, "deletions": 39, "changes": 86, "file_content_changes": "@@ -142,25 +142,26 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<func::FuncOp> {\n   }\n };\n \n-using IndexCacheKeyT = std::pair<Attribute, SmallVector<int64_t>>;\n+using IndexCacheKeyT = std::pair<Attribute, RankedTensorType>;\n \n struct CacheKeyDenseMapInfo {\n   static IndexCacheKeyT getEmptyKey() {\n     auto *pointer = llvm::DenseMapInfo<void *>::getEmptyKey();\n     return std::make_pair(\n         mlir::Attribute(static_cast<mlir::Attribute::ImplType *>(pointer)),\n-        SmallVector<int64_t>{});\n+        RankedTensorType{});\n   }\n   static IndexCacheKeyT getTombstoneKey() {\n     auto *pointer = llvm::DenseMapInfo<void *>::getTombstoneKey();\n+    auto tombstone = llvm::DenseMapInfo<RankedTensorType>::getTombstoneKey();\n     return std::make_pair(\n         mlir::Attribute(static_cast<mlir::Attribute::ImplType *>(pointer)),\n-        SmallVector<int64_t>{std::numeric_limits<int64_t>::max()});\n+        tombstone);\n   }\n   static unsigned getHashValue(IndexCacheKeyT key) {\n-    return llvm::hash_combine(\n-        mlir::hash_value(key.first),\n-        llvm::hash_combine_range(key.second.begin(), key.second.end()));\n+    auto shape = key.second.getShape();\n+    return llvm::hash_combine(mlir::hash_value(key.first),\n+                              mlir::hash_value(key.second));\n   }\n   static bool isEqual(IndexCacheKeyT LHS, IndexCacheKeyT RHS) {\n     return LHS == RHS;\n@@ -284,7 +285,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto inOrder = triton::gpu::getOrder(srcEncoding);\n     auto outOrder = triton::gpu::getOrder(resSharedLayout);\n     // tensor indices held by the current thread, as LLVM values\n-    auto srcIndices = emitIndices(loc, rewriter, srcEncoding, srcShape);\n+    auto srcIndices = emitIndices(loc, rewriter, srcEncoding, srcTy);\n     // return values\n     DenseMap<unsigned, Value> ret;\n     // cache for non-immediate offsets\n@@ -502,8 +503,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   SmallVector<Value> emitBaseIndexForLayout(Location loc,\n                                             ConversionPatternRewriter &rewriter,\n                                             const Attribute &layout,\n-                                            ArrayRef<int64_t> shape) const {\n-    IndexCacheKeyT key = std::make_pair(layout, llvm::to_vector(shape));\n+                                            RankedTensorType type) const {\n+    IndexCacheKeyT key = std::make_pair(layout, type);\n     auto cache = indexCacheInfo.baseIndexCache;\n     assert(cache && \"baseIndexCache is nullptr\");\n     auto insertPt = indexCacheInfo.indexInsertPoint;\n@@ -515,12 +516,12 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       SmallVector<Value> result;\n       if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n         result =\n-            emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n+            emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, type);\n       } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n         if (mmaLayout.isVolta())\n-          result = emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, shape);\n+          result = emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, type);\n         if (mmaLayout.isAmpere())\n-          result = emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, shape);\n+          result = emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, type);\n       } else {\n         llvm_unreachable(\"unsupported emitBaseIndexForLayout\");\n       }\n@@ -531,14 +532,14 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   }\n \n   SmallVector<SmallVector<unsigned>>\n-  emitOffsetForLayout(const Attribute &layout, ArrayRef<int64_t> shape) const {\n+  emitOffsetForLayout(const Attribute &layout, RankedTensorType type) const {\n     if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n-      return emitOffsetForBlockedLayout(blockedLayout, shape);\n+      return emitOffsetForBlockedLayout(blockedLayout, type);\n     if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n       if (mmaLayout.isVolta())\n-        return emitOffsetForMmaLayoutV1(mmaLayout, shape);\n+        return emitOffsetForMmaLayoutV1(mmaLayout, type);\n       if (mmaLayout.isAmpere())\n-        return emitOffsetForMmaLayoutV2(mmaLayout, shape);\n+        return emitOffsetForMmaLayoutV2(mmaLayout, type);\n     }\n     llvm_unreachable(\"unsupported emitOffsetForLayout\");\n   }\n@@ -549,8 +550,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   SmallVector<SmallVector<Value>> emitIndices(Location loc,\n                                               ConversionPatternRewriter &b,\n                                               const Attribute &layout,\n-                                              ArrayRef<int64_t> shape) const {\n-    IndexCacheKeyT key(layout, llvm::to_vector(shape));\n+                                              RankedTensorType type) const {\n+    IndexCacheKeyT key(layout, type);\n     auto cache = indexCacheInfo.indexCache;\n     assert(cache && \"indexCache is nullptr\");\n     auto insertPt = indexCacheInfo.indexInsertPoint;\n@@ -561,11 +562,11 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       restoreInsertionPointIfSet(insertPt, b);\n       SmallVector<SmallVector<Value>> result;\n       if (auto blocked = layout.dyn_cast<BlockedEncodingAttr>()) {\n-        result = emitIndicesForDistributedLayout(loc, b, blocked, shape);\n+        result = emitIndicesForDistributedLayout(loc, b, blocked, type);\n       } else if (auto mma = layout.dyn_cast<MmaEncodingAttr>()) {\n-        result = emitIndicesForDistributedLayout(loc, b, mma, shape);\n+        result = emitIndicesForDistributedLayout(loc, b, mma, type);\n       } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n-        result = emitIndicesForSliceLayout(loc, b, slice, shape);\n+        result = emitIndicesForSliceLayout(loc, b, slice, type);\n       } else {\n         llvm_unreachable(\n             \"emitIndices for layouts other than blocked & slice not \"\n@@ -594,11 +595,10 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // -----------------------------------------------------------------------\n \n   // Get an index-base for each dimension for a \\param blocked_layout.\n-  SmallVector<Value>\n-  emitBaseIndexForBlockedLayout(Location loc,\n-                                ConversionPatternRewriter &rewriter,\n-                                const BlockedEncodingAttr &blocked_layout,\n-                                ArrayRef<int64_t> shape) const {\n+  SmallVector<Value> emitBaseIndexForBlockedLayout(\n+      Location loc, ConversionPatternRewriter &rewriter,\n+      const BlockedEncodingAttr &blocked_layout, RankedTensorType type) const {\n+    auto shape = type.getShape();\n     Value threadId = getThreadId(rewriter, loc);\n     Value warpSize = i32_val(32);\n     Value laneId = urem(threadId, warpSize);\n@@ -638,7 +638,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n   SmallVector<SmallVector<unsigned>>\n   emitOffsetForBlockedLayout(const BlockedEncodingAttr &blockedLayout,\n-                             ArrayRef<int64_t> shape) const {\n+                             RankedTensorType type) const {\n+    auto shape = type.getShape();\n     auto sizePerThread = blockedLayout.getSizePerThread();\n     auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n     auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n@@ -667,7 +668,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                                   threadOffset * sizePerThread[k] + elemOffset);\n     }\n \n-    unsigned elemsPerThread = blockedLayout.getElemsPerThread(shape);\n+    unsigned elemsPerThread = triton::gpu::getElemsPerThread(type);\n     unsigned totalSizePerThread = product<unsigned>(sizePerThread);\n     SmallVector<SmallVector<unsigned>> reorderedOffset(elemsPerThread);\n     for (unsigned n = 0; n < elemsPerThread; ++n) {\n@@ -695,7 +696,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   SmallVector<Value>\n   emitBaseIndexForMmaLayoutV1(Location loc, ConversionPatternRewriter &rewriter,\n                               const MmaEncodingAttr &mmaLayout,\n-                              ArrayRef<int64_t> shape) const {\n+                              RankedTensorType type) const {\n+    auto shape = type.getShape();\n \n     auto wpt = mmaLayout.getWarpsPerCTA();\n     auto fpw = LLVM::DotOpMmaV1ConversionHelper::fpw;\n@@ -759,7 +761,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n   SmallVector<SmallVector<unsigned>>\n   emitOffsetForMmaLayoutV1(const MmaEncodingAttr &mmaLayout,\n-                           ArrayRef<int64_t> shape) const {\n+                           RankedTensorType type) const {\n+    auto shape = type.getShape();\n \n     auto [isARow, isBRow, isAVec4, isBVec4, id] =\n         mmaLayout.decodeVoltaLayoutStates();\n@@ -799,7 +802,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   SmallVector<Value>\n   emitBaseIndexForMmaLayoutV2(Location loc, ConversionPatternRewriter &rewriter,\n                               const MmaEncodingAttr &mmaLayout,\n-                              ArrayRef<int64_t> shape) const {\n+                              RankedTensorType type) const {\n+    auto shape = type.getShape();\n     auto _warpsPerCTA = mmaLayout.getWarpsPerCTA();\n     assert(_warpsPerCTA.size() == 2);\n     SmallVector<Value> warpsPerCTA = {i32_val(_warpsPerCTA[0]),\n@@ -822,7 +826,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n   SmallVector<SmallVector<unsigned>>\n   emitOffsetForMmaLayoutV2(const MmaEncodingAttr &mmaLayout,\n-                           ArrayRef<int64_t> shape) const {\n+                           RankedTensorType type) const {\n+    auto shape = type.getShape();\n     SmallVector<SmallVector<unsigned>> ret;\n \n     for (unsigned i = 0; i < shape[0]; i += getShapePerCTA(mmaLayout)[0]) {\n@@ -840,13 +845,14 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // [elemsPerThread X rank] index matrix.\n   SmallVector<SmallVector<Value>> emitIndicesForDistributedLayout(\n       Location loc, ConversionPatternRewriter &rewriter,\n-      const Attribute &layout, ArrayRef<int64_t> shape) const {\n+      const Attribute &layout, RankedTensorType type) const {\n     // step 1, delinearize threadId to get the base index\n-    auto multiDimBase = emitBaseIndexForLayout(loc, rewriter, layout, shape);\n+    auto multiDimBase = emitBaseIndexForLayout(loc, rewriter, layout, type);\n     // step 2, get offset of each element\n-    auto offset = emitOffsetForLayout(layout, shape);\n+    auto offset = emitOffsetForLayout(layout, type);\n     // step 3, add offset to base, and reorder the sequence of indices to\n     // guarantee that elems in the same sizePerThread are adjacent in order\n+    auto shape = type.getShape();\n     unsigned rank = shape.size();\n     unsigned elemsPerThread = offset.size();\n     SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread,\n@@ -860,11 +866,13 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   SmallVector<SmallVector<Value>>\n   emitIndicesForSliceLayout(Location loc, ConversionPatternRewriter &rewriter,\n                             const SliceEncodingAttr &sliceLayout,\n-                            ArrayRef<int64_t> shape) const {\n-    auto parent = sliceLayout.getParent();\n+                            RankedTensorType type) const {\n+    auto parentEncoding = sliceLayout.getParent();\n     unsigned dim = sliceLayout.getDim();\n-    auto parentIndices =\n-        emitIndices(loc, rewriter, parent, sliceLayout.paddedShape(shape));\n+    auto parentShape = sliceLayout.paddedShape(type.getShape());\n+    RankedTensorType parentTy = RankedTensorType::get(\n+        parentShape, type.getElementType(), parentEncoding);\n+    auto parentIndices = emitIndices(loc, rewriter, parentEncoding, parentTy);\n     unsigned numIndices = parentIndices.size();\n     SmallVector<SmallVector<Value>> resultIndices;\n     for (unsigned i = 0; i < numIndices; ++i) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -224,7 +224,7 @@ class ConvertTritonGPUToLLVM\n private:\n   Value smem;\n \n-  using IndexCacheKeyT = std::pair<Attribute, SmallVector<int64_t>>;\n+  using IndexCacheKeyT = std::pair<Attribute, RankedTensorType>;\n   DenseMap<IndexCacheKeyT, SmallVector<Value>, CacheKeyDenseMapInfo>\n       baseIndexCache;\n   DenseMap<IndexCacheKeyT, SmallVector<SmallVector<Value>>,"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 45, "deletions": 21, "changes": 66, "file_content_changes": "@@ -46,29 +46,30 @@ namespace gpu {\n // so that all distributed layouts implement\n // these utilities\n \n-unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape) {\n+unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape,\n+                           Type eltTy) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-    return blockedLayout.getElemsPerThread(shape);\n+    return blockedLayout.getElemsPerThread(shape, eltTy);\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-    return sliceLayout.getElemsPerThread(shape);\n+    return sliceLayout.getElemsPerThread(shape, eltTy);\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    return mmaLayout.getElemsPerThread(shape);\n+    return mmaLayout.getElemsPerThread(shape, eltTy);\n   } else if (auto sharedLayout = layout.dyn_cast<SharedEncodingAttr>()) {\n-    return sharedLayout.getElemsPerThread(shape);\n+    return sharedLayout.getElemsPerThread(shape, eltTy);\n   } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n-    return dotLayout.getElemsPerThread(shape);\n+    return dotLayout.getElemsPerThread(shape, eltTy);\n   } else {\n     assert(0 && \"getElemsPerThread not implemented\");\n     return 0;\n   }\n }\n \n unsigned getElemsPerThread(Type type) {\n-  if (type.isIntOrIndexOrFloat() || type.isa<triton::Float8Type>() ||\n-      type.isa<triton::PointerType>())\n+  if (type.isIntOrIndexOrFloat() || type.isa<triton::PointerType>())\n     return 1;\n   auto tensorType = type.cast<RankedTensorType>();\n-  return getElemsPerThread(tensorType.getEncoding(), tensorType.getShape());\n+  return getElemsPerThread(tensorType.getEncoding(), tensorType.getShape(),\n+                           tensorType.getElementType());\n }\n \n SmallVector<unsigned> getThreadsPerWarp(const Attribute &layout) {\n@@ -330,7 +331,8 @@ SliceEncodingAttr BlockedEncodingAttr::squeeze(int axis) {\n   return SliceEncodingAttr::get(getContext(), axis, *this);\n }\n \n-unsigned BlockedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n+unsigned BlockedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n+                                                Type eltTy) const {\n   size_t rank = shape.size();\n   auto sizePerThread = getSizePerThread();\n   auto warpsPerCTA = getWarpsPerCTA();\n@@ -365,12 +367,14 @@ SliceEncodingAttr::paddedShape<unsigned>(ArrayRef<unsigned> shape) const;\n template SmallVector<int64_t>\n SliceEncodingAttr::paddedShape<int64_t>(ArrayRef<int64_t> shape) const;\n \n-unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n+unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n+                                              Type eltTy) const {\n   auto parent = getParent();\n-  return ::getElemsPerThread(parent, paddedShape(shape));\n+  return ::getElemsPerThread(parent, paddedShape(shape), eltTy);\n }\n \n-unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n+unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n+                                            Type eltTy) const {\n   size_t rank = shape.size();\n   assert(rank == 2 && \"Unexpected rank of mma layout\");\n   assert((isVolta() || isAmpere()) && \"Only version 1 and 2 is supported\");\n@@ -401,18 +405,38 @@ unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   return res;\n }\n \n-unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n-  // TODO:\n-  assert(0 && \"SharedEncodingAttr::getElemsPerThread not implemented\");\n+unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n+                                               Type eltTy) const {\n+  llvm_unreachable(\"Unexpected shared layout\");\n   return 0;\n }\n \n-unsigned\n-DotOperandEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n-  if (auto blockedLayout = getParent().dyn_cast<BlockedEncodingAttr>()) {\n-    return blockedLayout.getElemsPerThread(shape);\n+unsigned DotOperandEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n+                                                   Type eltTy) const {\n+  if (auto mmaParent = getParent().dyn_cast<MmaEncodingAttr>()) {\n+    int warpsPerCTAM = mmaParent.getWarpsPerCTA()[0];\n+    int warpsPerCTAN = mmaParent.getWarpsPerCTA()[1];\n+    // TODO: always assume sub-register typed are packed into int32\n+    if (mmaParent.isAmpere()) {\n+      int bitwidth = eltTy.getIntOrFloatBitWidth();\n+      int shapePerWarpM = 16;\n+      int shapePerWarpN = 8;\n+      int shapePerWarpK = 4 * 64 / bitwidth;\n+      int shapePerCTAM = shapePerWarpM * warpsPerCTAM;\n+      int shapePerCTAN = shapePerWarpN * warpsPerCTAN;\n+      if (getOpIdx() == 0) {\n+        int repM = std::max<int>(1, shape[0] / shapePerCTAM);\n+        int repK = std::max<int>(1, shape[1] / shapePerWarpK);\n+        return 4 * repM * repK * (32 / bitwidth);\n+      }\n+      if (getOpIdx() == 1) {\n+        int repN = std::max<int>(1, shape[1] / shapePerCTAN);\n+        int repK = std::max<int>(1, shape[0] / shapePerWarpK);\n+        return 4 * std::max(repN / 2, 1) * repK * (32 / bitwidth);\n+      }\n+    }\n   }\n-  assert(0 && \"DotOperandEncodingAttr::getElemsPerThread not implemented\");\n+  llvm_unreachable(\"unknown mma version\");\n   return 0;\n }\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -2,7 +2,7 @@ add_mlir_dialect_library(TritonGPUTransforms\n   AccelerateMatmul.cpp\n   Coalesce.cpp\n   DecomposeConversions.cpp\n-  FuseTranspositions.cpp\n+  OptimizeDotOperands.cpp\n   Pipeline.cpp\n   Prefetch.cpp\n   RemoveLayoutConversions.cpp"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "renamed", "additions": 47, "deletions": 5, "changes": 52, "file_content_changes": "@@ -120,15 +120,56 @@ class ConvertTransConvert : public mlir::RewritePattern {\n   }\n };\n \n+class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n+\n+public:\n+  MoveOpAfterLayoutConversion(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n+                             1, context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto srcTy = cvt.getOperand().getType().cast<RankedTensorType>();\n+    auto retTy = cvt.getResult().getType().dyn_cast<RankedTensorType>();\n+    if (!retTy)\n+      return failure();\n+    if (!isa<triton::gpu::DotOperandEncodingAttr>(retTy.getEncoding()))\n+      return failure();\n+    if (isa<triton::gpu::SharedEncodingAttr>(srcTy.getEncoding()))\n+      return failure();\n+    //\n+    Operation *argOp = cvt.getOperand().getDefiningOp();\n+    //\n+    if (!argOp)\n+      return failure();\n+    // we only handle loads since the goal of this pass is to\n+    SetVector<Operation *> processed;\n+    SetVector<Attribute> layout;\n+    llvm::MapVector<Value, Attribute> toConvert;\n+    int numCvts = simulateBackwardRematerialization(\n+        cvt, processed, layout, toConvert, retTy.getEncoding());\n+    if (numCvts > 1 || toConvert.size() == 1)\n+      return failure();\n+\n+    IRMapping mapping;\n+    rematerializeConversionChain(toConvert, rewriter, processed, mapping);\n+    rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n+    return mlir::success();\n+  }\n+};\n+\n } // namespace\n \n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n-class TritonGPUFuseTranspositionsPass\n-    : public TritonGPUFuseTranspositionsBase<TritonGPUFuseTranspositionsPass> {\n+class TritonGPUOptimizeDotOperandsPass\n+    : public TritonGPUOptimizeDotOperandsBase<\n+          TritonGPUOptimizeDotOperandsPass> {\n public:\n-  TritonGPUFuseTranspositionsPass() = default;\n+  TritonGPUOptimizeDotOperandsPass() = default;\n \n   void runOnOperation() override {\n     MLIRContext *context = &getContext();\n@@ -141,13 +182,14 @@ class TritonGPUFuseTranspositionsPass\n     mlir::RewritePatternSet patterns(context);\n     patterns.add<OptimizeConvertToDotOperand>(context);\n     patterns.add<ConvertTransConvert>(context);\n+    // patterns.add<MoveOpAfterLayoutConversion>(context);\n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n       signalPassFailure();\n     if (fixupLoops(m).failed())\n       signalPassFailure();\n   }\n };\n \n-std::unique_ptr<Pass> mlir::createTritonGPUFuseTranspositionsPass() {\n-  return std::make_unique<TritonGPUFuseTranspositionsPass>();\n+std::unique_ptr<Pass> mlir::createTritonGPUOptimizeDotOperandsPass() {\n+  return std::make_unique<TritonGPUOptimizeDotOperandsPass>();\n }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 28, "deletions": 33, "changes": 61, "file_content_changes": "@@ -1,6 +1,8 @@\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/IRMapping.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n+#include \"mlir/Interfaces/SideEffectInterfaces.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n@@ -173,6 +175,8 @@ LogicalResult LoopPipeliner::initialize() {\n     if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n       auto ptr = loadOp.getPtr();\n       unsigned vec = axisInfoAnalysis->getPtrContiguity(ptr);\n+      if (auto mask = loadOp.getMask())\n+        vec = std::min<unsigned>(vec, axisInfoAnalysis->getMaskAlignment(mask));\n       auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n       if (!tensorTy)\n         continue;\n@@ -480,15 +484,25 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   // 3. replace loads with block args (from prologue)\n   for (size_t idx = 0; idx < loads.size(); ++idx) {\n+    OpBuilder::InsertionGuard guard(builder);\n     Value load = loads[idx];\n     assert(load.hasOneUse() &&\n            \"we assume that this load has one use (ConvertLayout)\");\n     Value loadUse = load.getUsers().begin()->getResult(0);\n-    mapping.lookup(loadUse).replaceAllUsesWith(\n+    // set insertion point\n+    Value newLoad = mapping.lookup(load);\n+    Value newLoadUse = mapping.lookup(loadUse);\n+    builder.setInsertionPoint(newLoadUse.getDefiningOp());\n+    // create conversion\n+    auto cvt = builder.create<ttg::ConvertLayoutOp>(\n+        loadUse.getLoc(), loadUse.getType(),\n         newForOp.getRegionIterArgs()[loadIdx + idx]);\n+\n+    // replace uses\n+    newLoadUse.replaceAllUsesWith(cvt.getResult());\n     // delete old load and layout conversion\n-    mapping.lookup(loadUse).getDefiningOp()->erase();\n-    mapping.lookup(load).getDefiningOp()->erase();\n+    newLoadUse.getDefiningOp()->erase();\n+    newLoad.getDefiningOp()->erase();\n   }\n \n   // 4. prefetch the next iteration\n@@ -616,35 +630,6 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n     }\n   }\n \n-  {\n-    OpBuilder::InsertionGuard guard(builder);\n-    for (Operation &op : *newForOp.getBody()) {\n-      if (auto dotOp = llvm::dyn_cast<triton::DotOp>(&op)) {\n-        builder.setInsertionPoint(&op);\n-        auto dotType = dotOp.getType().cast<RankedTensorType>();\n-        Value a = dotOp.getA();\n-        Value b = dotOp.getB();\n-        auto layoutCast = [&](Value dotOperand, int opIdx) -> Value {\n-          auto tensorType = dotOperand.getType().cast<RankedTensorType>();\n-          if (!tensorType.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {\n-            auto newEncoding = ttg::DotOperandEncodingAttr::get(\n-                tensorType.getContext(), opIdx, dotType.getEncoding());\n-            auto newType =\n-                RankedTensorType::get(tensorType.getShape(),\n-                                      tensorType.getElementType(), newEncoding);\n-            return builder.create<ttg::ConvertLayoutOp>(dotOperand.getLoc(),\n-                                                        newType, dotOperand);\n-          }\n-          return dotOperand;\n-        };\n-        a = layoutCast(a, 0);\n-        b = layoutCast(b, 1);\n-        dotOp->setOperand(0, a);\n-        dotOp->setOperand(1, b);\n-      }\n-    }\n-  }\n-\n   // async.wait & extract_slice\n   Operation *asyncWait = builder.create<ttg::AsyncWaitOp>(\n       loads[0].getLoc(), loads.size() * (numStages - 2));\n@@ -696,6 +681,17 @@ struct PipelinePass : public TritonGPUPipelineBase<PipelinePass> {\n     if (numStages <= 1)\n       return;\n \n+    // Pre-processing\n+    // we make sure element-wise ops are done *after* the conversion\n+    // to dot operands\n+    // we can achieve this with simple recursive pattern matching\n+    // MLIRContext *context = &getContext();\n+    // mlir::RewritePatternSet patterns(context);\n+    // patterns.add<MoveOpAfterLayoutConversion>(context);\n+    // auto didPreprocess =\n+    //     applyPatternsAndFoldGreedily(getOperation(), std::move(patterns));\n+\n+    // Do the pipelining\n     getOperation()->walk([&](scf::ForOp forOp) -> void {\n       LoopPipeliner pipeliner(forOp, numStages);\n \n@@ -705,7 +701,6 @@ struct PipelinePass : public TritonGPUPipelineBase<PipelinePass> {\n       pipeliner.emitPrologue();\n \n       scf::ForOp newForOp = pipeliner.createNewForOp();\n-\n       pipeliner.emitEpilogue();\n \n       // replace the original loop"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 9, "deletions": 197, "changes": 206, "file_content_changes": "@@ -108,7 +108,7 @@ class SimplifyReduceCvt : public mlir::RewritePattern {\n     auto newReduce = rewriter.create<triton::ReduceOp>(\n         op->getLoc(), reduce.getRedOp(), reduceArg.getOperand(),\n         reduce.getAxis());\n-    if (isa<triton::gpu::ConvertLayoutOp>(\n+    if (isa_and_nonnull<triton::gpu::ConvertLayoutOp>(\n             *reduceArg.getOperand().getDefiningOp()))\n       return mlir::failure();\n     Value newRet = newReduce.getResult();\n@@ -146,157 +146,6 @@ class SimplifyConversion : public mlir::RewritePattern {\n //\n // -----------------------------------------------------------------------------\n \n-// TODO: Interface\n-LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n-                             Attribute &ret) {\n-  ret = targetEncoding;\n-  if (auto expand_dims = dyn_cast<triton::ExpandDimsOp>(op)) {\n-    ret = triton::gpu::SliceEncodingAttr::get(\n-        op->getContext(), expand_dims.getAxis(), targetEncoding);\n-  }\n-  if (auto reduce = dyn_cast<triton::ReduceOp>(op)) {\n-    auto sliceEncoding =\n-        targetEncoding.dyn_cast<triton::gpu::SliceEncodingAttr>();\n-    if (!sliceEncoding)\n-      return failure();\n-    if (sliceEncoding.getDim() != reduce.getAxis())\n-      return failure();\n-    ret = sliceEncoding.getParent();\n-  }\n-  if (auto view = dyn_cast<triton::ViewOp>(op)) {\n-    return failure();\n-  }\n-  return success();\n-}\n-\n-inline bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n-  // Case 1: A size 1 tensor is not expensive since all threads will load the\n-  // same\n-  if (isSingleValue(op->getOperand(0)))\n-    return false;\n-  // auto ptr = op->getOperand(0);\n-  //// Case 2: We assume that `evict_last` loads/stores have high hit rate\n-  // if (auto load = dyn_cast<triton::LoadOp>(op))\n-  //   if (load.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n-  //     return false;\n-  // if (auto store = dyn_cast<triton::StoreOp>(op))\n-  //   if (store.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n-  //     return false;\n-  // if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n-  //   auto encoding = tensorTy.getEncoding();\n-  //   // Case 3: Different type conversion is expensive (e.g., mma <-> block)\n-  //   if (encoding.getTypeID() != targetEncoding.getTypeID())\n-  //     return true;\n-  //   auto sizePerThread = triton::gpu::getSizePerThread(encoding);\n-  //   auto targetSizePerThread = triton::gpu::getSizePerThread(targetEncoding);\n-  //   auto order = triton::gpu::getOrder(encoding);\n-  //   auto targetOrder = triton::gpu::getOrder(targetEncoding);\n-  //   // Case 4: The targeEncoding may expose more vectorization opportunities\n-  //   return sizePerThread[order[0]] >= targetSizePerThread[targetOrder[0]];\n-  // }\n-  return true;\n-}\n-\n-inline bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n-  if (!op)\n-    return true;\n-  if (isa<triton::LoadOp, triton::StoreOp>(op))\n-    return expensiveLoadOrStore(op, targetEncoding);\n-  if (isa<triton::gpu::ExtractSliceOp, triton::gpu::AllocTensorOp,\n-          triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n-          triton::AtomicCASOp, triton::DotOp>(op))\n-    return true;\n-  if (isa<scf::YieldOp, scf::ForOp, scf::IfOp, scf::WhileOp, scf::ConditionOp>(\n-          op))\n-    return true;\n-  return false;\n-}\n-\n-LogicalResult simulateBackwardRematerialization(\n-    Operation *initOp, SetVector<Operation *> &processed,\n-    SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n-    const Attribute &targetEncoding) {\n-  // DFS\n-  std::vector<std::pair<Operation *, Attribute>> queue;\n-  queue.emplace_back(initOp, targetEncoding);\n-  // We want to see the effect of converting `initOp` to a new layout\n-  // so we initialize `numCvts = 1`.\n-  int numCvts = 1;\n-  while (!queue.empty()) {\n-    Operation *currOp;\n-    Attribute currLayout;\n-    std::tie(currOp, currLayout) = queue.back();\n-    queue.pop_back();\n-    // If the current operation is expensive to rematerialize,\n-    // we stop everything\n-    if (expensiveToRemat(currOp, currLayout))\n-      break;\n-    // A conversion will be removed here (i.e. transferred to operands)\n-    numCvts -= 1;\n-    // Done processing\n-    processed.insert(currOp);\n-    layout.insert(currLayout);\n-    // Add all operands to the queue\n-    for (Value argI : currOp->getOperands()) {\n-      Attribute newEncoding;\n-      // Cannot invert the current encoding for this operand\n-      // we stop everything\n-      if (failed(invertEncoding(currLayout, currOp, newEncoding)))\n-        return mlir::failure();\n-      if (toConvert.count(argI) && toConvert[argI] != newEncoding)\n-        return mlir::failure();\n-      Operation *opArgI = argI.getDefiningOp();\n-      toConvert.insert({argI, newEncoding});\n-      // 1. Only convert RankedTensorType\n-      // 2. Skip if there's no defining op\n-      // 3. Skip if the defining op has already been processed\n-      // 4. Skip or the defining op is in a different block\n-      if (!argI.getType().isa<RankedTensorType>() || !opArgI ||\n-          processed.contains(opArgI) ||\n-          opArgI->getBlock() != currOp->getBlock())\n-        continue;\n-      // If the conversion can be folded into opArgI then\n-      // we don't count this conversion as expensive\n-      if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n-              triton::MakeRangeOp, triton::SplatOp>(*opArgI))\n-        continue;\n-      if (auto view = dyn_cast<triton::ViewOp>(opArgI))\n-        continue;\n-\n-      // We add one expensive conversion for the current operand\n-      numCvts += 1;\n-      queue.emplace_back(opArgI, newEncoding);\n-    }\n-  }\n-  // if rematerialization would add more conversions than it removes\n-  // then we don't do it\n-  if (numCvts > 0)\n-    return mlir::failure();\n-  return mlir::success();\n-}\n-\n-//\n-\n-Operation *cloneWithInferType(mlir::PatternRewriter &rewriter, Operation *op,\n-                              IRMapping &mapping) {\n-  Operation *newOp = rewriter.clone(*op, mapping);\n-  auto origType = op->getResult(0).getType().cast<RankedTensorType>();\n-  auto argType = newOp->getOperand(0).getType().cast<RankedTensorType>();\n-  auto newType = RankedTensorType::get(\n-      origType.getShape(), origType.getElementType(), argType.getEncoding());\n-  newOp->getResult(0).setType(newType);\n-  auto typeInfer = dyn_cast<InferTypeOpInterface>(newOp);\n-  if (typeInfer) {\n-    SmallVector<Type, 1> newTypes;\n-    auto success = typeInfer.inferReturnTypes(\n-        newOp->getContext(), newOp->getLoc(), newOp->getOperands(),\n-        newOp->getAttrDictionary(), newOp->getRegions(), newTypes);\n-    if (succeeded(success))\n-      newOp->getResult(0).setType(newTypes.front());\n-  }\n-  return newOp;\n-}\n-\n // op(cvt(arg_0), arg_1, ..., arg_n)\n // -> cvt(op(arg_0, cvt(arg_1), ..., cvt(arg_n)))\n void pushConversionForward(triton::gpu::ConvertLayoutOp cvt,\n@@ -368,11 +217,11 @@ class MoveConvertOutOfIf : public mlir::RewritePattern {\n \n     IRMapping mapping;\n     for (size_t i = 0; i < numOps; i++) {\n-      auto thenCvt = dyn_cast<triton::gpu::ConvertLayoutOp>(\n+      auto thenCvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n           thenYield.getOperand(i).getDefiningOp());\n       if (hasElse) {\n         auto elseYield = ifOp.elseYield();\n-        auto elseCvt = dyn_cast<triton::gpu::ConvertLayoutOp>(\n+        auto elseCvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n             elseYield.getOperand(i).getDefiningOp());\n         if (thenCvt && elseCvt &&\n             std::distance(elseCvt->user_begin(), elseCvt->user_end()) == 1 &&\n@@ -492,8 +341,8 @@ class RematerializeForward : public mlir::RewritePattern {\n         SetVector<Attribute> layout;\n         llvm::MapVector<Value, Attribute> toConvert;\n         if (argOp && (argOp != cvt) && cvtSlices.count(argOp) == 0 &&\n-            failed(simulateBackwardRematerialization(argOp, processed, layout,\n-                                                     toConvert, srcEncoding))) {\n+            simulateBackwardRematerialization(argOp, processed, layout,\n+                                              toConvert, srcEncoding) > 0) {\n           return failure();\n         }\n       }\n@@ -539,50 +388,13 @@ class RematerializeBackward : public mlir::RewritePattern {\n     SetVector<Attribute> layout;\n     llvm::MapVector<Value, Attribute> toConvert;\n     std::vector<std::pair<Operation *, Attribute>> queue;\n-    if (failed(simulateBackwardRematerialization(\n-            cvt, processed, layout, toConvert, targetType.getEncoding())))\n+    if (simulateBackwardRematerialization(cvt, processed, layout, toConvert,\n+                                          targetType.getEncoding()) > 0)\n       return mlir::failure();\n \n-    SmallVector<Value, 4> sortedValues;\n-    SetVector<Operation *> tmp;\n-    for (auto &item : toConvert) {\n-      Value v = item.first;\n-      if (v.getDefiningOp())\n-        tmp.insert(v.getDefiningOp());\n-      else\n-        sortedValues.push_back(v);\n-    }\n-    tmp = mlir::multiRootTopologicalSort(tmp);\n-    for (Operation *op : tmp)\n-      sortedValues.push_back(op->getResult(0));\n-\n     IRMapping mapping;\n-    for (Value currOperand : sortedValues) {\n-      // unpack information\n-      Attribute targetLayout = toConvert.lookup(currOperand);\n-      // rematerialize the operand if necessary\n-      Operation *currOperation = currOperand.getDefiningOp();\n-      if (processed.contains(currOperation)) {\n-        Operation *newOperation =\n-            cloneWithInferType(rewriter, currOperation, mapping);\n-        newOperation->moveAfter(currOperation);\n-        currOperation = newOperation;\n-        currOperand = currOperation->getResult(0);\n-      }\n-      // compute target type for the layout cast\n-      auto currType = currOperand.getType().cast<RankedTensorType>();\n-      auto newType = RankedTensorType::get(\n-          currType.getShape(), currType.getElementType(), targetLayout);\n-      auto newOperand = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          currOperand.getLoc(), newType, currOperand);\n-      if (currOperation)\n-        newOperand->moveAfter(currOperation);\n-      else {\n-        Block *block = currOperand.cast<BlockArgument>().getOwner();\n-        newOperand->moveAfter(block, block->begin());\n-      }\n-      mapping.map(currOperand, newOperand);\n-    }\n+    rematerializeConversionChain(toConvert, rewriter, processed, mapping);\n+\n     rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n     return mlir::success();\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 198, "deletions": 0, "changes": 198, "file_content_changes": "@@ -1,7 +1,10 @@\n #include \"Utility.h\"\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/IR/IRMapping.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n namespace mlir {\n \n@@ -60,4 +63,199 @@ LogicalResult fixupLoops(ModuleOp mod) {\n   return success();\n }\n \n+// -------------------------------------------------------------------------- //\n+\n+// TODO: Interface\n+LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n+                             Attribute &ret) {\n+  ret = targetEncoding;\n+  if (auto expand_dims = dyn_cast<triton::ExpandDimsOp>(op)) {\n+    ret = triton::gpu::SliceEncodingAttr::get(\n+        op->getContext(), expand_dims.getAxis(), targetEncoding);\n+  }\n+  if (auto reduce = dyn_cast<triton::ReduceOp>(op)) {\n+    auto sliceEncoding =\n+        targetEncoding.dyn_cast<triton::gpu::SliceEncodingAttr>();\n+    if (!sliceEncoding)\n+      return failure();\n+    if (sliceEncoding.getDim() != reduce.getAxis())\n+      return failure();\n+    ret = sliceEncoding.getParent();\n+  }\n+  if (auto view = dyn_cast<triton::ViewOp>(op)) {\n+    return failure();\n+  }\n+  return success();\n+}\n+\n+bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n+  // Case 1: A size 1 tensor is not expensive since all threads will load the\n+  // same\n+  if (isSingleValue(op->getOperand(0)))\n+    return false;\n+  auto ptr = op->getOperand(0);\n+  // Case 2: We assume that `evict_last` loads/stores have high hit rate\n+  if (auto load = dyn_cast<triton::LoadOp>(op))\n+    if (load.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n+      return false;\n+  if (auto store = dyn_cast<triton::StoreOp>(op))\n+    if (store.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n+      return false;\n+  if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n+    auto encoding = tensorTy.getEncoding();\n+    // Case 3: Different type conversion is expensive (e.g., mma <-> block)\n+    if (encoding.getTypeID() != targetEncoding.getTypeID())\n+      return true;\n+    auto sizePerThread = triton::gpu::getSizePerThread(encoding);\n+    auto targetSizePerThread = triton::gpu::getSizePerThread(targetEncoding);\n+    auto order = triton::gpu::getOrder(encoding);\n+    auto targetOrder = triton::gpu::getOrder(targetEncoding);\n+    // Case 4: The targeEncoding may expose more vectorization opportunities\n+    return sizePerThread[order[0]] >= targetSizePerThread[targetOrder[0]];\n+  }\n+  return false;\n+}\n+\n+bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n+  if (!op)\n+    return true;\n+  if (isa<triton::LoadOp, triton::StoreOp>(op))\n+    return expensiveLoadOrStore(op, targetEncoding);\n+  if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n+          triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n+          triton::AtomicCASOp, triton::DotOp>(op))\n+    return true;\n+  if (isa<scf::YieldOp, scf::ForOp, scf::IfOp, scf::WhileOp, scf::ConditionOp>(\n+          op))\n+    return true;\n+  return false;\n+}\n+\n+int simulateBackwardRematerialization(\n+    Operation *initOp, SetVector<Operation *> &processed,\n+    SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n+    const Attribute &targetEncoding) {\n+  // DFS\n+  std::vector<std::pair<Operation *, Attribute>> queue;\n+  queue.emplace_back(initOp, targetEncoding);\n+  // We want to see the effect of converting `initOp` to a new layout\n+  // so we initialize `numCvts = 1`.\n+  int numCvts = 1;\n+  while (!queue.empty()) {\n+    Operation *currOp;\n+    Attribute currLayout;\n+    std::tie(currOp, currLayout) = queue.back();\n+    queue.pop_back();\n+    // If the current operation is expensive to rematerialize,\n+    // we stop everything\n+    if (expensiveToRemat(currOp, currLayout))\n+      break;\n+    // A conversion will be removed here (i.e. transferred to operands)\n+    numCvts -= 1;\n+    // Done processing\n+    processed.insert(currOp);\n+    layout.insert(currLayout);\n+    // Add all operands to the queue\n+    for (Value argI : currOp->getOperands()) {\n+      Attribute newEncoding;\n+      // Cannot invert the current encoding for this operand\n+      // we stop everything\n+      if (failed(invertEncoding(currLayout, currOp, newEncoding)))\n+        return INT_MAX;\n+      if (toConvert.count(argI) && toConvert[argI] != newEncoding)\n+        return INT_MAX;\n+      Operation *opArgI = argI.getDefiningOp();\n+      toConvert.insert({argI, newEncoding});\n+      // 1. Only convert RankedTensorType\n+      // 2. Skip if there's no defining op\n+      // 3. Skip if the defining op has already been processed\n+      // 4. Skip or the defining op is in a different block\n+      if (!argI.getType().isa<RankedTensorType>() || !opArgI ||\n+          processed.contains(opArgI) ||\n+          opArgI->getBlock() != currOp->getBlock())\n+        continue;\n+      // If the conversion can be folded into opArgI then\n+      // we don't count this conversion as expensive\n+      if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n+              triton::MakeRangeOp, triton::SplatOp>(*opArgI))\n+        continue;\n+      if (auto view = dyn_cast<triton::ViewOp>(opArgI))\n+        continue;\n+\n+      // We add one expensive conversion for the current operand\n+      numCvts += 1;\n+      queue.emplace_back(opArgI, newEncoding);\n+    }\n+  }\n+  // return net number of conversions\n+  return numCvts;\n+}\n+\n+//\n+\n+Operation *cloneWithInferType(mlir::PatternRewriter &rewriter, Operation *op,\n+                              IRMapping &mapping) {\n+  Operation *newOp = rewriter.clone(*op, mapping);\n+  auto origType = op->getResult(0).getType().cast<RankedTensorType>();\n+  auto argType = newOp->getOperand(0).getType().cast<RankedTensorType>();\n+  auto newType = RankedTensorType::get(\n+      origType.getShape(), origType.getElementType(), argType.getEncoding());\n+  newOp->getResult(0).setType(newType);\n+  auto typeInfer = dyn_cast<InferTypeOpInterface>(newOp);\n+  if (typeInfer) {\n+    SmallVector<Type, 1> newTypes;\n+    auto success = typeInfer.inferReturnTypes(\n+        newOp->getContext(), newOp->getLoc(), newOp->getOperands(),\n+        newOp->getAttrDictionary(), newOp->getRegions(), newTypes);\n+    if (succeeded(success))\n+      newOp->getResult(0).setType(newTypes.front());\n+  }\n+  return newOp;\n+}\n+\n+void rematerializeConversionChain(\n+    const llvm::MapVector<Value, Attribute> &toConvert,\n+    mlir::PatternRewriter &rewriter, SetVector<Operation *> &processed,\n+    IRMapping &mapping) {\n+  SmallVector<Value, 4> sortedValues;\n+  SetVector<Operation *> tmp;\n+  for (auto &item : toConvert) {\n+    Value v = item.first;\n+    if (v.getDefiningOp())\n+      tmp.insert(v.getDefiningOp());\n+    else\n+      sortedValues.push_back(v);\n+  }\n+  tmp = mlir::multiRootTopologicalSort(tmp);\n+  for (Operation *op : tmp)\n+    sortedValues.push_back(op->getResult(0));\n+\n+  for (Value currOperand : sortedValues) {\n+    // unpack information\n+    Attribute targetLayout = toConvert.lookup(currOperand);\n+    // rematerialize the operand if necessary\n+    Operation *currOperation = currOperand.getDefiningOp();\n+    if (processed.contains(currOperation)) {\n+      Operation *newOperation =\n+          cloneWithInferType(rewriter, currOperation, mapping);\n+      newOperation->moveAfter(currOperation);\n+      currOperation = newOperation;\n+      currOperand = currOperation->getResult(0);\n+    }\n+    // compute target type for the layout cast\n+    auto currType = currOperand.getType().cast<RankedTensorType>();\n+    auto newType = RankedTensorType::get(\n+        currType.getShape(), currType.getElementType(), targetLayout);\n+    auto newOperand = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        currOperand.getLoc(), newType, currOperand);\n+    if (currOperation)\n+      newOperand->moveAfter(currOperation);\n+    else {\n+      Block *block = currOperand.cast<BlockArgument>().getOwner();\n+      newOperand->moveAfter(block, block->begin());\n+    }\n+    mapping.map(currOperand, newOperand);\n+  }\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "file_content_changes": "@@ -2,11 +2,32 @@\n #define TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/IR/PatternMatch.h\"\n+#include \"llvm/ADT/MapVector.h\"\n \n namespace mlir {\n \n LogicalResult fixupLoops(ModuleOp mod);\n \n+// TODO: Interface\n+LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n+                             Attribute &ret);\n+\n+bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n+\n+bool expensiveToRemat(Operation *op, Attribute &targetEncoding);\n+\n+int simulateBackwardRematerialization(\n+    Operation *initOp, SetVector<Operation *> &processed,\n+    SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n+    const Attribute &targetEncoding);\n+\n+Operation *cloneWithInferType(mlir::PatternRewriter &rewriter, Operation *op,\n+                              IRMapping &mapping);\n+\n+void rematerializeConversionChain(\n+    const llvm::MapVector<Value, Attribute> &toConvert,\n+    mlir::PatternRewriter &rewriter, SetVector<Operation *> &processed,\n+    IRMapping &mapping);\n } // namespace mlir\n \n #endif // TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -310,6 +310,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n     return nullptr;\n   }\n \n+  // llvm::outs() << module << \"\\n\";\n   auto llvmIR = translateLLVMToLLVMIR(llvmContext, module);\n   if (!llvmIR) {\n     llvm::errs() << \"Translate to LLVM IR failed\";"}, {"filename": "python/setup.py", "status": "modified", "additions": 12, "deletions": 7, "changes": 19, "file_content_changes": "@@ -9,6 +9,7 @@\n import tempfile\n import urllib.request\n from distutils.version import LooseVersion\n+from pathlib import Path\n from typing import NamedTuple\n \n from setuptools import Extension, setup\n@@ -38,7 +39,6 @@ class Package(NamedTuple):\n     package: str\n     name: str\n     url: str\n-    test_file: str\n     include_flag: str\n     lib_flag: str\n     syspath_var_name: str\n@@ -49,7 +49,7 @@ class Package(NamedTuple):\n def get_pybind11_package_info():\n     name = \"pybind11-2.10.0\"\n     url = \"https://github.com/pybind/pybind11/archive/refs/tags/v2.10.0.tar.gz\"\n-    return Package(\"pybind11\", name, url, \"include/pybind11/pybind11.h\", \"PYBIND11_INCLUDE_DIR\", \"\", \"PYBIND11_SYSPATH\")\n+    return Package(\"pybind11\", name, url, \"PYBIND11_INCLUDE_DIR\", \"\", \"PYBIND11_SYSPATH\")\n \n # llvm\n \n@@ -65,12 +65,13 @@ def get_llvm_package_info():\n         linux_suffix = 'ubuntu-18.04' if vglibc > 217 else 'centos-7'\n         system_suffix = f\"linux-gnu-{linux_suffix}\"\n     else:\n-        return Package(\"llvm\", \"LLVM-C.lib\", \"\", \"lib\", \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n+        return Package(\"llvm\", \"LLVM-C.lib\", \"\", \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n     use_assert_enabled_llvm = check_env_flag(\"TRITON_USE_ASSERT_ENABLED_LLVM\", \"False\")\n     release_suffix = \"assert\" if use_assert_enabled_llvm else \"release\"\n     name = f'llvm+mlir-17.0.0-x86_64-{system_suffix}-{release_suffix}'\n-    url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/llvm-17.0.0-8e5a41e8271f/{name}.tar.xz\"\n-    return Package(\"llvm\", name, url, \"lib\", \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n+    version = \"llvm-17.0.0-8e5a41e8271f\"\n+    url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n+    return Package(\"llvm\", name, url, \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n \n \n def get_thirdparty_packages(triton_cache_path):\n@@ -81,8 +82,9 @@ def get_thirdparty_packages(triton_cache_path):\n         package_dir = os.path.join(package_root_dir, p.name)\n         if p.syspath_var_name in os.environ:\n             package_dir = os.environ[p.syspath_var_name]\n-        test_file_path = os.path.join(package_dir, p.test_file)\n-        if not os.path.exists(test_file_path):\n+        version_file_path = os.path.join(package_dir, \"version.txt\")\n+        if not os.path.exists(version_file_path) or\\\n+           Path(version_file_path).read_text() != p.url:\n             try:\n                 shutil.rmtree(package_root_dir)\n             except Exception:\n@@ -92,6 +94,9 @@ def get_thirdparty_packages(triton_cache_path):\n             ftpstream = urllib.request.urlopen(p.url)\n             file = tarfile.open(fileobj=ftpstream, mode=\"r|*\")\n             file.extractall(path=package_root_dir)\n+            # write version url to package_dir\n+            with open(os.path.join(package_dir, \"version.txt\"), \"w\") as f:\n+                f.write(p.url)\n         if p.include_flag:\n             thirdparty_cmake_args.append(f\"-D{p.include_flag}={package_dir}/include\")\n         if p.lib_flag:"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1450,9 +1450,9 @@ void init_triton_ir(py::module &&m) {\n              self.addPass(\n                  mlir::createTritonGPUAccelerateMatmulPass(computeCapability));\n            })\n-      .def(\"add_tritongpu_fuse_transpositions_pass\",\n+      .def(\"add_tritongpu_optimize_dot_operands_pass\",\n            [](mlir::PassManager &self) {\n-             self.addPass(mlir::createTritonGPUFuseTranspositionsPass());\n+             self.addPass(mlir::createTritonGPUOptimizeDotOperandsPass());\n            })\n       .def(\"add_tritongpu_remove_layout_conversions_pass\",\n            [](mlir::PassManager &self) {"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -683,6 +683,22 @@ def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n \n+def test_tensor_atomic_rmw_block(device=\"cuda\"):\n+    shape = (8, 8)\n+\n+    @triton.jit\n+    def kernel(X, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n+        off0 = tl.arange(0, SHAPE0)\n+        off1 = tl.arange(0, SHAPE1)\n+        offs = off0[:, None] * SHAPE1 + off1[None, :]\n+        val = offs.to(tl.float32)\n+        x = X + offs\n+        tl.atomic_min(x, val)\n+    x = torch.ones((8, 8), device=device, dtype=torch.float32)\n+    kernel[(2,)](x, shape[0], shape[1])\n+    assert torch.min(x).item() == 0.0\n+\n+\n def test_atomic_cas():\n     # 1. make sure that atomic_cas changes the original value (Lock)\n     @triton.jit"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -1021,10 +1021,10 @@ def optimize_ttgir(mod, num_stages, compute_capability):\n     pm.add_tritongpu_coalesce_pass()\n     pm.add_tritongpu_accelerate_matmul_pass(compute_capability)\n     pm.add_tritongpu_remove_layout_conversions_pass()\n-    pm.add_tritongpu_fuse_transpositions_pass()\n+    pm.add_tritongpu_optimize_dot_operands_pass()\n     pm.add_tritongpu_pipeline_pass(num_stages)\n     pm.add_tritongpu_prefetch_pass()\n-    pm.add_tritongpu_fuse_transpositions_pass()\n+    pm.add_tritongpu_optimize_dot_operands_pass()\n     pm.add_tritongpu_remove_layout_conversions_pass()\n     pm.add_tritongpu_decompose_conversions_pass()\n     if compute_capability // 10 == 7:\n@@ -1725,7 +1725,8 @@ def _init_handles(self):\n         if self.shared > max_shared:\n             raise OutOfResources(self.shared, max_shared, \"shared memory\")\n         mod, func, n_regs, n_spills = cuda_utils.load_binary(self.metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n-        # print(self.shared, n_regs, n_spills)\n+        self.n_spills = n_spills\n+        self.n_regs = n_regs\n         self.cu_module = mod\n         self.cu_function = func\n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -641,7 +641,7 @@ def bitcast(input: tl.tensor,\n             builder: ir.builder) -> tl.tensor:\n     src_ty = input.type\n     if src_ty.is_block():\n-        dst_ty = tl.block_type(dst_ty, input.type.get_block_shapes())\n+        dst_ty = tl.block_type(dst_ty.scalar, input.type.get_block_shapes())\n     if src_ty == dst_ty:\n         return input\n     src_sca_ty = src_ty.scalar\n@@ -665,7 +665,7 @@ def cast(input: tl.tensor,\n     if isinstance(dst_ty, tl.constexpr):\n         dst_ty = dst_ty.value\n     if src_ty.is_block():\n-        dst_ty = tl.block_type(dst_ty, input.type.get_block_shapes())\n+        dst_ty = tl.block_type(dst_ty.scalar, input.type.get_block_shapes())\n     if src_ty == dst_ty:\n         return input\n \n@@ -1133,6 +1133,10 @@ def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n \n     # get result type\n     shape = input.type.shape\n+\n+    rank = len(shape)\n+    assert 0 <= axis < rank, f\"axis (v={axis}) is out of range, should be within [0, {rank})\"\n+\n     ret_shape = []\n     for i, s in enumerate(shape):\n         if i != axis:"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -156,7 +156,7 @@\n \n @triton.autotune(\n     configs=[\n-        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n     ],\n     key=['M', 'N', 'K'],\n )"}, {"filename": "test/CMakeLists.txt", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -13,11 +13,13 @@ configure_lit_site_cfg(\n \n set(TRITON_TEST_DEPENDS\n   triton-opt\n-  copy-FileCheck\n )\n \n+set(FILECHECK_PATH \"${LLVM_LIBRARY_DIR}/../bin/FileCheck\")\n+set(LIT_ARGS \"-Dfilecheck=${FILECHECK_PATH}\")\n add_lit_testsuite(check-triton-lit-tests \"Running the triton regression tests\"\n   ${CMAKE_CURRENT_BINARY_DIR}\n+  ARGS ${LIT_ARGS}\n   DEPENDS ${TRITON_TEST_DEPENDS}\n   )\n "}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -955,3 +955,30 @@ func.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !\n   }\n   return\n }\n+\n+\n+// Just make sure it doesn't crash on non-tensor types.\n+// CHECK-LABEL: if_no_tensor\n+func.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}) {\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %c-1_i64 = arith.constant -1 : i64\n+  %cst = arith.constant 0.000000e+00 : f32\n+  %c-1_i32 = arith.constant -1 : i32\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = tt.addptr %arg3, %0 : !tt.ptr<i64>, i32\n+  %2 = tt.load %1 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64\n+  %3 = arith.cmpi eq, %2, %c-1_i64 : i64\n+  %4 = arith.select %3, %c-1_i32, %arg2 : i32\n+  %5 = scf.if %3 -> (!tt.ptr<f32>) {\n+    scf.yield %arg0 : !tt.ptr<f32>\n+  } else {\n+    %10 = tt.addptr %arg0, %2 : !tt.ptr<f32>, i64\n+    scf.yield %10 : !tt.ptr<f32>\n+  }\n+  %6 = arith.extsi %4 : i32 to i64\n+  %7 = arith.cmpi slt, %2, %6 : i64\n+  %8 = tt.load %5, %7, %cst {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : f32\n+  %9 = tt.addptr %arg1, %0 : !tt.ptr<f32>, i32\n+  tt.store %9, %8 {cache = 1 : i32, evict = 1 : i32} : f32\n+  return\n+}"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 24, "deletions": 18, "changes": 42, "file_content_changes": "@@ -33,8 +33,9 @@\n // CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:   %[[arg_a0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_a0]]\n-// CHECK:   %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n-// CHECK:   tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op]], {{.*}}\n+// CHECK:   %[[arg_b0_dot_op_0:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n+// CHECK:   %[[arg_b0_dot_op_1:.*]] = arith.mulf %[[arg_b0_dot_op_0]]\n+// CHECK:   tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op_1]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:   %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n@@ -47,7 +48,7 @@\n // CHECK:   scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n func.func @matmul_loop(%lb : index, %ub : index, %step : index,\n                   %A : !tt.ptr<f16> {tt.divisibility = 16 : i32},\n-                  %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n+                  %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) -> tensor<128x128xf32, #C> {\n   // A ptrs\n   %a_ptr_splat = tt.splat %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %a_tmp0 = tt.make_range {end = 32: i32, start = 0: i32} : tensor<32xi32, #ALs0>\n@@ -70,23 +71,25 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index,\n \n   %a_off = arith.constant dense<4> : tensor<128x32xi32, #AL>\n   %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n+  \n+  %b_scale = arith.constant dense<4.> : tensor<32x128xf16, #B>\n \n-  scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n+  %loop:3 = scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n     %a_ = tt.load %a_ptr {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n     %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n-    %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n-    %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n+    %b__ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n+    %b_ = triton_gpu.convert_layout %b__ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n+    %b = arith.mulf %b_, %b_scale: tensor<32x128xf16, #B>\n \n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n \n     %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n-  return\n+  return %loop#2: tensor<128x128xf32, #C>\n }\n \n-\n // CHECK: func.func @matmul_loop_nested\n // CHECK-DAG: %[[CONSTANT_0:.*]] = arith.constant 0 : i32\n // CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n@@ -118,8 +121,10 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index,\n // CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n                          %A : !tt.ptr<f16> {tt.divisibility = 16 : i32},\n-                         %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n-  scf.for %iv0 = %lb to %ub step %step {\n+                         %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) -> tensor<128x128xf32, #C>{\n+\n+  %c_start = arith.constant dense<0.00e+00> : tensor<128x128xf32, #C>\n+  %loop1:1 = scf.for %iv0 = %lb to %ub step %step iter_args(%c_init = %c_start) -> (tensor<128x128xf32, #C>) {\n     // A ptrs\n     %a_ptr_splat = tt.splat %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n     %a_tmp0 = tt.make_range {end = 32: i32, start = 0: i32} : tensor<32xi32, #ALs0>\n@@ -137,12 +142,11 @@ func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n     %a_other = arith.constant dense<0.00e+00> : tensor<128x32xf16, #AL>\n     %b_mask = arith.constant dense<true> : tensor<32x128xi1, #BL>\n     %b_other = arith.constant dense<0.00e+00> : tensor<32x128xf16, #BL>\n-    %c_init = arith.constant dense<0.00e+00> : tensor<128x128xf32, #C>\n \n     %a_off = arith.constant dense<4> : tensor<128x32xi32, #AL>\n     %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n \n-    scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n+    %loop2:3 = scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n       %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n       %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n       %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n@@ -154,9 +158,11 @@ func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n       %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n       scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n     }\n+\n+    scf.yield %loop2#2 : tensor<128x128xf32, #C>\n   }\n-  return\n-}\n+  return %loop1#0 : tensor<128x128xf32, #C>\n+} \n \n \n // CHECK: func.func @matmul_loop_single_pipeline\n@@ -182,7 +188,7 @@ func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n // CHECK:   scf.yield {{.*}}, {{.*}}, %[[NEXT_B_BUFFER]], %[[NEXT_B]], {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n                                   %A : !tt.ptr<f16> {tt.divisibility = 16 : i32},\n-                                  %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n+                                  %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) -> tensor<128x128xf32, #C> {\n   // A ptrs\n   %a_ptr_splat = tt.splat %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %a_tmp0 = tt.make_range {end = 32: i32, start = 0: i32} : tensor<32xi32, #ALs0>\n@@ -208,12 +214,12 @@ func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n \n   %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n \n-  scf.for %iv = %lb to %ub step %step iter_args(%b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n+  %loop:2 = scf.for %iv = %lb to %ub step %step iter_args(%b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_b_ptr, %c : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n-  return\n-}\n+  return %loop#1 : tensor<128x128xf32, #C>\n+}\n\\ No newline at end of file"}, {"filename": "test/TritonGPU/update-mma-for-volta.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file -tritongpu-fuse-transposition -tritongpu-update-mma-for-volta 2>&1 | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -tritongpu-optimize-dot-operands -tritongpu-update-mma-for-volta 2>&1 | FileCheck %s\n \n // -----\n "}]