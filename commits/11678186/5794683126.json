[{"filename": ".gitignore", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -24,3 +24,6 @@ venv.bak/\n # JetBrains project files\n .idea\n cmake-build-*\n+\n+# Third-party binaries\n+ptxas"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -575,7 +575,7 @@ struct ExtractSliceOpConversion\n     // newShape = rank_reduce(shape)\n     // Triton only supports static tensor sizes\n     SmallVector<Value, 4> strideVals;\n-    for (auto i = 0; i < op.static_sizes().size(); ++i) {\n+    for (auto i = 0; i < op.getStaticSizes().size(); ++i) {\n       if (op.getStaticSize(i) == 1) {\n         offsetVals.erase(offsetVals.begin() + i);\n       } else {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 30, "deletions": 30, "changes": 60, "file_content_changes": "@@ -11,6 +11,7 @@\n #include \"Utility.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n #include \"triton/Target/PTX/TmaMetadata.h\"\n #include <set>\n \n@@ -31,6 +32,7 @@ using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;\n using ::mlir::triton::gpu::TMAMetadataTy;\n+namespace ttng = ::mlir::triton::nvidia_gpu;\n \n typedef DenseMap<Operation *, triton::MakeTensorPtrOp> TensorPtrMapT;\n \n@@ -238,12 +240,26 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return llvmStruct;\n   }\n \n-  Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n-    auto tid = rewriter.create<::mlir::gpu::ThreadIdOp>(\n+  // Returns CTA level thread idx\n+  Value getThreadIdInCTA(ConversionPatternRewriter &rewriter,\n+                         Location loc) const {\n+    Value tid = rewriter.create<::mlir::gpu::ThreadIdOp>(\n         loc, ::mlir::gpu::Dimension::x);\n     return rewriter.create<arith::IndexCastOp>(loc, i32_ty, tid);\n   }\n \n+  // Returns CTA level thread idx for not ws mode.\n+  // Returns agent level thread idx for ws mode.\n+  Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n+    Value tid = getThreadIdInCTA(rewriter, loc);\n+    auto mod = rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    if (ttng::TritonNvidiaGPUDialect::getWSSupportedAttr(mod)) {\n+      Value _128 = rewriter.create<arith::ConstantIntOp>(loc, 128, 32);\n+      tid = rewriter.create<arith::RemSIOp>(loc, tid, _128);\n+    }\n+    return tid;\n+  }\n+\n   static Value getSRegValue(OpBuilder &b, Location loc,\n                             const std::string &sRegStr) {\n     PTXBuilder builder;\n@@ -984,32 +1000,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return ret;\n   }\n \n-  SmallVector<Value>\n-  emitBaseIndexForMmaLayoutV2(Location loc, ConversionPatternRewriter &rewriter,\n-                              const MmaEncodingAttr &mmaLayout,\n-                              RankedTensorType type) const {\n-    auto shape = type.getShape();\n-    auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n-    assert(warpsPerCTA.size() == 2);\n-    auto order = triton::gpu::getOrder(mmaLayout);\n-    Value threadId = getThreadId(rewriter, loc);\n-    Value warpSize = i32_val(32);\n-    Value laneId = urem(threadId, warpSize);\n-    Value warpId = udiv(threadId, warpSize);\n-\n-    SmallVector<Value> multiDimWarpId =\n-        delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n-    multiDimWarpId[0] = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n-    multiDimWarpId[1] = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n-    Value offWarp0 = mul(multiDimWarpId[0], i32_val(16));\n-    Value offWarp1 = mul(multiDimWarpId[1], i32_val(8));\n-\n-    SmallVector<Value> multiDimBase(2);\n-    multiDimBase[0] = add(udiv(laneId, i32_val(4)), offWarp0);\n-    multiDimBase[1] = add(mul(i32_val(2), urem(laneId, i32_val(4))), offWarp1);\n-    return multiDimBase;\n-  }\n-\n   SmallVector<SmallVector<unsigned>>\n   emitOffsetForMmaLayoutV2(const MmaEncodingAttr &mmaLayout,\n                            RankedTensorType type) const {\n@@ -1062,8 +1052,18 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     else\n       warpsN = shape[1] / instrShape[1];\n \n-    SmallVector<Value> multiDimWarpId =\n-        delinearize(rewriter, loc, warpId, _warpsPerCTA, order);\n+    SmallVector<Value> multiDimWarpId(2);\n+    if (mmaLayout.isHopper()) {\n+      // TODO[goostavz]: the tiling order from CTA->warp level is different for\n+      // MMAv2/3. This is a workaround since we don't explicitly have warpGrp\n+      // level in the layout definition, and the tiling order of warpGrp->warp\n+      // must be fixed to meet the HW's needs. We may need to consider to\n+      // explicitly define warpGrpPerCTA for MMAv3 layout.\n+      multiDimWarpId[0] = urem(warpId, warpsPerCTA[0]);\n+      multiDimWarpId[1] = urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]);\n+    } else {\n+      multiDimWarpId = delinearize(rewriter, loc, warpId, _warpsPerCTA, order);\n+    }\n     Value warpId0 = urem(multiDimWarpId[0], i32_val(warpsM));\n     Value warpId1 = urem(multiDimWarpId[1], i32_val(warpsN));\n "}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1606,10 +1606,10 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n     auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         op->getLoc(), newType, extract_slice.getSource());\n     rewriter.replaceOpWithNewOp<triton::gpu::ExtractSliceOp>(\n-        op, resType, newArg.getResult(), extract_slice.offsets(),\n-        extract_slice.sizes(), extract_slice.strides(),\n-        extract_slice.static_offsets(), extract_slice.static_sizes(),\n-        extract_slice.static_strides());\n+        op, resType, newArg.getResult(), extract_slice.getOffsets(),\n+        extract_slice.getSizes(), extract_slice.getStrides(),\n+        extract_slice.getStaticOffsets(), extract_slice.getStaticSizes(),\n+        extract_slice.getStaticStrides());\n     return mlir::success();\n   }\n "}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSMaterialization.cpp", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -380,8 +380,10 @@ void materializeTokenOperations(Operation *parentOp, int numCTAs) {\n     Value bufferEmptyArray =\n         builder.create<ttng::AllocMBarrierOp>(tokenLoc, mBarriersTy, numCTAs);\n \n-    // Make sure that MBarriers are initialized in all CTAs\n-    if (numCTAs > 1) {\n+    if (numCTAs == 1) {\n+      builder.create<mlir::gpu::BarrierOp>(tokenLoc);\n+    } else {\n+      // Make sure that MBarriers are initialized in all CTAs\n       builder.create<triton::nvidia_gpu::ClusterArriveOp>(tokenLoc, false);\n       builder.create<triton::nvidia_gpu::ClusterWaitOp>(tokenLoc);\n     }"}, {"filename": "lib/Hopper/HopperHelpers.c", "status": "modified", "additions": 0, "deletions": 32, "changes": 32, "file_content_changes": "@@ -104,38 +104,6 @@ __DEVICE__ __attribute__((__always_inline__)) void __nv_wgmma_wait_group() {\n   asm volatile(\"wgmma.wait_group.sync.aligned 0;\\n\");\n }\n \n-// GMMA expects data to be in TN format. if A is column major, transa should be\n-// set GMMA expects data to be in TN format. if B is row major, transb should be\n-// set\n-__DEVICE__ __attribute__((__always_inline__)) float32\n-__nv_wgmma_m64n64k16_f32_f16_f16_row_col(const uint64_t desc_a,\n-                                         const uint64_t desc_b, float32 acc) {\n-  const uint32_t scale_d = 1;\n-  asm volatile(\"{\\n\"\n-               \".reg .pred p;\\n\\t\"\n-               \"setp.eq.u32 p, %34, 1;\\n\\t\"\n-               \"wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16\\n\"\n-               \"{%0, %1, %2, %3, %4, %5, %6, %7, \\n\"\n-               \" %8, %9, %10, %11, %12, %13, %14, %15,\\n\"\n-               \" %16, %17, %18, %19, %20, %21, %22, %23,\\n\"\n-               \" %24, %25, %26, %27, %28, %29, %30, %31},\\n\"\n-               \"%32, \\n\"\n-               \"%33, \\n\"\n-               \"p, 1, 1, 0, 0;\\n\"\n-               \"}\"\n-               : \"+f\"(acc.d0), \"+f\"(acc.d1), \"+f\"(acc.d2), \"+f\"(acc.d3),\n-                 \"+f\"(acc.d4), \"+f\"(acc.d5), \"+f\"(acc.d6), \"+f\"(acc.d7),\n-                 \"+f\"(acc.d8), \"+f\"(acc.d9), \"+f\"(acc.d10), \"+f\"(acc.d11),\n-                 \"+f\"(acc.d12), \"+f\"(acc.d13), \"+f\"(acc.d14), \"+f\"(acc.d15),\n-                 \"+f\"(acc.d16), \"+f\"(acc.d17), \"+f\"(acc.d18), \"+f\"(acc.d19),\n-                 \"+f\"(acc.d20), \"+f\"(acc.d21), \"+f\"(acc.d22), \"+f\"(acc.d23),\n-                 \"+f\"(acc.d24), \"+f\"(acc.d25), \"+f\"(acc.d26), \"+f\"(acc.d27),\n-                 \"+f\"(acc.d28), \"+f\"(acc.d29), \"+f\"(acc.d30), \"+f\"(acc.d31)\n-               : \"l\"(desc_a), \"l\"(desc_b), \"r\"(scale_d));\n-\n-  return acc;\n-}\n-\n __DEVICE__ __attribute__((__always_inline__)) void\n __nv_mbarrier_init(uint32_t bar, uint32_t expected, uint32_t pred) {\n   if (pred) {"}, {"filename": "python/setup.py", "status": "modified", "additions": 10, "deletions": 2, "changes": 12, "file_content_changes": "@@ -68,7 +68,9 @@ def get_pybind11_package_info():\n def get_llvm_package_info():\n     # added statement for Apple Silicon\n     system = platform.system()\n-    arch = 'x86_64'\n+    arch = platform.machine()\n+    if arch == 'aarch64':\n+        arch = 'arm64'\n     if system == \"Darwin\":\n         system_suffix = \"apple-darwin\"\n         arch = platform.machine()\n@@ -84,6 +86,9 @@ def get_llvm_package_info():\n     name = f'llvm+mlir-17.0.0-{arch}-{system_suffix}-{release_suffix}'\n     version = \"llvm-17.0.0-c5dede880d17\"\n     url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n+    # FIXME: remove the following once github.com/ptillet/triton-llvm-releases has arm64 llvm releases\n+    if arch == 'arm64' and 'linux' in system_suffix:\n+        url = f\"https://github.com/acollins3/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n     return Package(\"llvm\", name, url, \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n \n \n@@ -124,7 +129,10 @@ def download_and_copy_ptxas():\n     base_dir = os.path.dirname(__file__)\n     src_path = \"bin/ptxas\"\n     version = \"12.1.105\"\n-    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.1.1/linux-64/cuda-nvcc-{version}-0.tar.bz2\"\n+    arch = platform.machine()\n+    if arch == \"x86_64\":\n+        arch = \"64\"\n+    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.1.1/linux-{arch}/cuda-nvcc-{version}-0.tar.bz2\"\n     dst_prefix = os.path.join(base_dir, \"triton\")\n     dst_suffix = os.path.join(\"third_party\", \"cuda\", src_path)\n     dst_path = os.path.join(dst_prefix, dst_suffix)"}, {"filename": "python/test/unit/hopper/test_gemm.py", "status": "modified", "additions": 28, "deletions": 41, "changes": 69, "file_content_changes": "@@ -149,6 +149,7 @@ def matmul_kernel(\n     DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n     A_ORDER_0: tl.constexpr, A_ORDER_1: tl.constexpr,\n     B_ORDER_0: tl.constexpr, B_ORDER_1: tl.constexpr,\n+    W_ORDER_0: tl.constexpr, W_ORDER_1: tl.constexpr,\n     Z_ORDER_0: tl.constexpr, Z_ORDER_1: tl.constexpr\n ):\n     pid = tl.program_id(axis=0)\n@@ -167,8 +168,9 @@ def matmul_kernel(\n                                    offsets=(block_offset_m, 0), block_shape=(BLOCK_M, BLOCK_K), order=(A_ORDER_0, A_ORDER_1))\n     b_tile_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn),\n                                    offsets=(0, block_offset_n), block_shape=(BLOCK_K, BLOCK_N), order=(B_ORDER_0, B_ORDER_1))\n+    # for chain-dot, BLOCK_N must always be equal to N, and each program loads the whole W matrix\n     w_tile_ptr = tl.make_block_ptr(base=w_ptr, shape=(N, N), strides=(stride_wm, stride_wn),\n-                                   offsets=(0, block_offset_n), block_shape=(BLOCK_N, BLOCK_N), order=(Z_ORDER_1, Z_ORDER_0))\n+                                   offsets=(0, 0), block_shape=(BLOCK_N, BLOCK_N), order=(W_ORDER_0, W_ORDER_1))\n     z = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n \n     offs_m = block_offset_m + tl.arange(0, BLOCK_M)\n@@ -213,7 +215,7 @@ def matmul_kernel(\n         tl.store(z_ptrs, z, mask=mask)\n \n \n-@pytest.mark.parametrize('BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,M,N,K,TRANS_A,TRANS_B,TRANS_C,epilogue,out_dtype,USE_TMA_STORE,NUM_STAGES,ENABLE_WS',\n+@pytest.mark.parametrize('BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,M,N,K,TRANS_A,TRANS_B,TRANS_OUTPUT,epilogue,out_dtype,USE_TMA_STORE,NUM_STAGES,ENABLE_WS',\n                          [(128, 128, 64, 4, 1, *shape_w_c, 'none', out_dtype, use_tma_store, 3, enable_ws)\n                           for shape_w_c in [\n                              # badcase from cublas-important-layers\n@@ -225,7 +227,7 @@ def matmul_kernel(\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n                              for enable_ws in [False, True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_c, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n                               # softmax works for one CTA\n                               for shape_w_c in [\n                              [64, 64, 16, 4, 1, 64, 64, 64],\n@@ -239,10 +241,10 @@ def matmul_kernel(\n                              for use_tma_store in [False, True]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n-                             for trans_c in [False, True]\n+                             for trans_output in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_c, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n                               for shape_w_c in [\n                              [64, 64, 16, 4, 1, 128, 128, 64],\n                              *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4, 8] for num_ctas in [1, 2, 4]],\n@@ -264,11 +266,11 @@ def matmul_kernel(\n                              for use_tma_store in [False, True]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n-                             for trans_c in [False, True]\n+                             for trans_output in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                             if not (epilogue == 'chain-dot' and (shape_w_c[5] is not None or shape_w_c[0] != shape_w_c[1]))\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_c, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                             if not (epilogue == 'chain-dot' and (shape_w_c[6] is not None or shape_w_c[1] != shape_w_c[6]))\n+                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                               for shape_w_c in [\n                              [64, 64, 32, 4, 1, 128, 256, 64],\n                              [128, 128, 16, 4, 4, 512, 256, 64],\n@@ -287,34 +289,34 @@ def matmul_kernel(\n                              for use_tma_store in [False, True]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n-                             for trans_c in [False, True]\n+                             for trans_output in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                         ] + [(64, n, 16, 4, 1, 512, 256, 256, False, True, trans_c, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                         ] + [(64, n, 16, 4, 1, 512, 256, 256, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                               # loop over instr shapes\n                               for n in [16, 32, 64, 128, 256]\n-                              for trans_c in [False, True]\n+                              for trans_output in [False, True]\n                               for out_dtype in ['float16', 'float32']\n                               for use_tma_store in [False, True]\n                               for num_stages in [2, 4, 5, 7]\n                               for enable_ws in [False, True]\n-                              ] + [(*shape_w_c, *shape, False, True, trans_c, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                              ] + [(*shape_w_c, *shape, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                                    # irregular shapes\n                                    for shape_w_c in [\n                                        [128, 128, 64, 4, 1],\n                                        [256, 128, 64, 4, 2],\n                                        [128, 128, 128, 4, 2],\n                               ]\n                              for shape in list(itertools.product([*range(512, 4096, 360)], [*range(512, 4096, 360)], [512, 1024]))\n-                             for trans_c in [False, True]\n+                             for trans_output in [False, True]\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n                              for num_stages in [2, 3, 4]\n                              for enable_ws in [False, True]\n                          ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()\n                     [0] < 9, reason=\"Requires compute capability >= 9\")\n-def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A, TRANS_B, TRANS_C, epilogue, out_dtype, USE_TMA_STORE, NUM_STAGES, ENABLE_WS):\n+def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A, TRANS_B, TRANS_OUTPUT, epilogue, out_dtype, USE_TMA_STORE, NUM_STAGES, ENABLE_WS):\n     if '-'.join(map(str, [BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A, TRANS_B])) in [\n         '16-32-64-4-4-512-256-64-True-False',\n         '16-32-64-4-4-512-256-64-True-True',\n@@ -331,13 +333,7 @@ def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A,\n         '16-32-64-8-2-256-256-256-False',\n         '16-32-64-8-2-256-256-256-True',\n     ]:\n-        pytest.skip('illegal memory access.')\n-\n-    # with ENABLE_TMA=1 and ENABLE_MMA_V3=1\n-    if '-'.join(map(str, [BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K])) in [\n-        '64-64-32-8-1-128-256-64',\n-    ]:\n-        pytest.skip('Tensor-likes are not close!')\n+        pytest.skip('Known legacy issue, ldmatrix can only support x4')\n \n     # with ENABLE_TMA=1 and ENABLE_MMA_V3=1\n     if ENABLE_WS:\n@@ -375,27 +371,23 @@ def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A,\n \n     # avoid out of memory\n     if epilogue in ['add-matrix', 'add-rows', 'add-cols']:\n-        if (TRANS_C):\n-            bias = torch.randn((M, N), device='cuda', dtype=torch_out_dtype)\n-        else:\n+        if (TRANS_OUTPUT):\n             bias = torch.randn((N, M), device='cuda', dtype=torch_out_dtype).T\n+        else:\n+            bias = torch.randn((M, N), device='cuda', dtype=torch_out_dtype)\n     else:\n         bias = torch.randn((1, 1), device='cuda', dtype=torch_out_dtype)\n \n-    if epilogue == 'chain-dot':\n-        if (TRANS_C):\n-            w = torch.randn((N, N), device='cuda', dtype=torch.float16).T\n-        else:\n-            w = torch.randn((M, M), device='cuda', dtype=torch.float16)\n-    else:\n-        w = torch.randn((1, 1), device='cuda', dtype=torch.float16).T\n+    # for chain-dot only\n+    w = torch.randn((N, N), device='cuda', dtype=torch.float16).T\n+    w_order = [0, 1]\n \n-    if (TRANS_C):\n-        z = torch.full((M, N), 1., device='cuda', dtype=torch_out_dtype)\n-        z_order = [1, 0]\n-    else:\n+    if (TRANS_OUTPUT):\n         z = torch.full((N, M), 1., device='cuda', dtype=torch_out_dtype).T\n         z_order = [0, 1]\n+    else:\n+        z = torch.full((M, N), 1., device='cuda', dtype=torch_out_dtype)\n+        z_order = [1, 0]\n \n     # torch result\n     a_f32 = a.to(torch.float32)\n@@ -439,17 +431,12 @@ def grid(META):\n                               CHAIN_DOT=epilogue == 'chain-dot',\n                               A_ORDER_0=a_order[0], A_ORDER_1=a_order[1],\n                               B_ORDER_0=b_order[0], B_ORDER_1=b_order[1],\n+                              W_ORDER_0=w_order[0], W_ORDER_1=w_order[1],\n                               Z_ORDER_0=z_order[0], Z_ORDER_1=z_order[1],\n                               num_warps=NUM_WARPS, num_ctas=NUM_CTAS, num_stages=NUM_STAGES,\n                               enable_warp_specialization=ENABLE_WS)\n \n     torch.set_printoptions(profile=\"full\")\n-    # print(\"abs_err: {}, rel_err: {}\".format(golden_abs_err, golden_rel_err))\n-    # print(\"golden: \")\n-    # print(golden)\n-    # print(\"result: \")\n-    # print(z)\n-    # print(\"max_gap: {}\".format(torch.max(torch.abs(z - golden))))\n     golden = torch.nn.functional.normalize(golden)\n     z = torch.nn.functional.normalize(z)\n     assert_close(z, golden,"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 7, "deletions": 16, "changes": 23, "file_content_changes": "@@ -26,15 +26,6 @@\n \n import triton\n import triton.language as tl\n-from .utils import get_proper_err, get_variant_golden\n-\n-\n-def isMMAV3OrTMAEnabled():\n-    import os\n-    for k in ('ENABLE_MMA_V3', 'ENABLE_TMA'):\n-        if os.environ.get(k, '0').lower() in ['1', 'on', 'true']:\n-            return True\n-    return False\n \n \n @triton.jit\n@@ -280,7 +271,11 @@ def tma_warp_specialized_matmul_kernel(\n ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n def test_non_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B):\n-    pytest.skip('hang')\n+    if '-'.join(map(str, [M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B])) in [\n+        '4096-4096-256-128-256-16-1-False-True',\n+        '4096-4096-256-128-256-64-1-False-True'\n+    ]:\n+        pytest.skip('Insufficient register resources')\n \n     if (TRANS_A):\n         a = .1 * torch.randn((K, M), device='cuda', dtype=torch.float16).T\n@@ -607,8 +602,6 @@ def static_persistent_matmul_no_scf_kernel(\n                              ]))\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n def test_static_persistent_matmul_no_scf_kernel(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_TYPE, USE_TMA_EPILOGUE, USE_TMA_LOAD):\n-    if isMMAV3OrTMAEnabled():\n-        pytest.skip(\"known failure\")\n     if (TRANS_A):\n         a = torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -641,14 +634,12 @@ def test_static_persistent_matmul_no_scf_kernel(M, N, K, NUM_CTAS, NUM_WARPS, TR\n     a_f32 = a.to(torch.float32)\n     b_f32 = b.to(torch.float32)\n     golden = torch.matmul(a_f32, b_f32)\n-    golden_variant = get_variant_golden(a_f32, b_f32)\n-    golden_abs_err, golden_rel_err = get_proper_err(golden, golden_variant)\n     torch.set_printoptions(profile=\"full\")\n     assert_close(\n         c,\n         golden,\n-        rtol=max(1e-2, 1.1 * golden_rel_err),\n-        atol=max(1e-3, 1.1 * golden_abs_err),\n+        rtol=1e-2,\n+        atol=1e-3,\n         check_dtype=False)\n \n "}, {"filename": "python/test/unit/hopper/test_tma_store_gemm.py", "status": "modified", "additions": 1, "deletions": 32, "changes": 33, "file_content_changes": "@@ -28,36 +28,6 @@\n import triton.language as tl\n \n \n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K)).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K)).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N)).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N)).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-# It's not easy to get a proper error threshold in different size\n-# Here the gemm calculation is padded to a different size in order to get\n-# a variant version of the golden result. And the error between golden and\n-# golden_variant provide reference on selecting the proper rtol / atol.\n-\n-\n-def get_proper_err(a, b, golden):\n-    golden_variant = get_variant_golden(a, b)\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n-    return (golden_abs_err, golden_rel_err)\n-\n-\n @triton.jit\n def matmul_tma_load_store(\n     a_ptr, b_ptr, c_ptr,\n@@ -118,6 +88,5 @@ def test_tma_load_store(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_F\n                                   num_ctas=NUM_CTAS,\n                                   OUTPUT_F16=OUTPUT_F16)\n     golden = torch.matmul(a, b)\n-    golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n     torch.set_printoptions(profile=\"full\")\n-    assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n+    assert_close(c, golden, rtol=1e-2, atol=1e-3, check_dtype=False)"}, {"filename": "python/test/unit/hopper/ttgir_tests/test_tma.py", "status": "modified", "additions": 2, "deletions": 7, "changes": 9, "file_content_changes": "@@ -23,7 +23,6 @@\n \n import pytest\n import torch\n-from test_util import get_proper_err\n from torch.testing import assert_close\n \n import triton\n@@ -63,13 +62,9 @@ def test_tma_wgmma_64_64_16_f16(TTGIR, TRANS_A, TRANS_B):\n \n     golden = torch.matmul(a, b)\n     torch.set_printoptions(profile=\"full\", sci_mode=False)\n-    golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n     assert_close(\n         c,\n         golden,\n-        rtol=max(1e-4,\n-                 1.5 * golden_rel_err),\n-        atol=max(\n-            1e-4,\n-            1.5 * golden_abs_err),\n+        rtol=1e-2,\n+        atol=1e-3,\n         check_dtype=False)"}, {"filename": "python/test/unit/hopper/ttgir_tests/test_util.py", "status": "removed", "additions": 0, "deletions": 52, "changes": 52, "file_content_changes": "@@ -1,52 +0,0 @@\n-# Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n-#\n-# Permission is hereby granted, free of charge, to any person obtaining\n-# a copy of this software and associated documentation files\n-# (the \"Software\"), to deal in the Software without restriction,\n-# including without limitation the rights to use, copy, modify, merge,\n-# publish, distribute, sublicense, and/or sell copies of the Software,\n-# and to permit persons to whom the Software is furnished to do so,\n-# subject to the following conditions:\n-#\n-# The above copyright notice and this permission notice shall be\n-# included in all copies or substantial portions of the Software.\n-#\n-# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n-# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n-# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n-# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n-# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n-# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n-# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n-\n-import torch\n-\n-\n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K)).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K)).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N)).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N)).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-# It's not easy to get a proper error threshold in different size\n-# Here the gemm calculation is padded to a different size in order to get\n-# a variant version of the golden result. And the error between golden and\n-# golden_variant provide reference on selecting the proper rtol / atol.\n-\n-\n-def get_proper_err(a, b, golden):\n-    golden_variant = get_variant_golden(a, b)\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n-    return (golden_abs_err, golden_rel_err)"}, {"filename": "python/test/unit/hopper/utils.py", "status": "removed", "additions": 0, "deletions": 32, "changes": 32, "file_content_changes": "@@ -1,32 +0,0 @@\n-import torch\n-\n-\n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K), dtype=a.dtype).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K), dtype=a.dtype).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N), dtype=b.dtype).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N), dtype=b.dtype).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-# It's not easy to get a proper error threshold in different size\n-# Here the gemm calculation is padded to a different size in order to get\n-# a variant version of the golden result. And the error between golden and\n-# golden_variant provide reference on selecting the proper rtol / atol.\n-\n-\n-def get_proper_err(golden, golden_variant):\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    # avoid problems when golden_rel_err is 'inf'\n-    abs_golden = torch.abs(golden) + torch.full_like(golden, torch.finfo(golden.dtype).smallest_normal)\n-    golden_rel_err = torch.max(torch.abs(golden_diff) / abs_golden).item()\n-    return (golden_abs_err, golden_rel_err)"}, {"filename": "python/triton/hopper_lib/libhopper_helpers.bc", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -979,7 +979,7 @@ def _store_block_pointer(ptr, val, mask, boundary_check, cache, eviction, builde\n     if not val.type.is_block():\n         val = broadcast_impl_shape(val, block_shape, builder)\n     assert val.type.is_block(), \"Value argument must be block type or a scalar\"\n-    assert block_shape == val.type.get_block_shapes(), \"Block shape and value shape mismatch\"\n+    assert block_shape == val.type.get_block_shapes(), f\"Block shape({block_shape}) and value shape({val.type.get_block_shapes()}) mismatch\"\n     assert ptr.type.element_ty.element_ty == val.type.element_ty, f\"Block element type({ptr.type.element_ty.element_ty}) and value element type({val.type.element_ty}) mismatch\"\n \n     elt_ty = ptr.type.element_ty.element_ty"}, {"filename": "python/triton/third_party/cuda/bin/ptxas", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/tools/compile.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -43,6 +43,7 @@\n     parser.add_argument(\"path\", help=\"Path to Python source containing desired kernel in its scope. File will be executed.\")\n     parser.add_argument(\"--kernel-name\", \"-n\", type=str, default=\"\", help=\"Name of the kernel to compile\", required=True)\n     parser.add_argument(\"--num-warps\", \"-w\", type=int, default=1, help=\"Number of warps to launch the kernel\")\n+    parser.add_argument(\"--num-stages\", \"-ns\", type=int, default=3, help=\"Number of stages meta-parameter for the kernel\")\n     parser.add_argument(\"--out-name\", \"-on\", type=str, default=None, help=\"Out name for the compiled kernel\")\n     parser.add_argument(\"--out-path\", \"-o\", type=Path, default=None, help=\"Out filename\")\n     parser.add_argument(\"--signature\", \"-s\", type=str, help=\"Signature of the kernel\", required=True)\n@@ -96,7 +97,7 @@ def constexpr(s):\n     config = triton.compiler.instance_descriptor(divisible_by_16=divisible_by_16, equal_to_1=equal_to_1)\n     for i in equal_to_1:\n         constexprs.update({i: 1})\n-    ccinfo = triton.compile(kernel, signature=signature, constants=constexprs, configs=[config], num_warps=args.num_warps)\n+    ccinfo = triton.compile(kernel, signature=signature, constants=constexprs, configs=[config], num_warps=args.num_warps, num_stages=args.num_stages)\n     arg_names = []\n     arg_types = []\n     for i in signature.keys():"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 36, "deletions": 34, "changes": 70, "file_content_changes": "@@ -32,33 +32,34 @@ def _fwd_kernel(\n     stride_kz, stride_kh, stride_kn, stride_kk,\n     stride_vz, stride_vh, stride_vk, stride_vn,\n     stride_oz, stride_oh, stride_om, stride_on,\n-    Z, H, N_CTX,\n+    Z, H, N_CTX, P_SEQ,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n     IS_CAUSAL: tl.constexpr,\n ):\n     start_m = tl.program_id(0)\n     off_hz = tl.program_id(1)\n-    qvk_offset = off_hz * stride_qh\n+    q_offset = off_hz * stride_qh\n+    kv_offset = off_hz * stride_kh\n     Q_block_ptr = tl.make_block_ptr(\n-        base=Q + qvk_offset,\n+        base=Q + q_offset,\n         shape=(N_CTX, BLOCK_DMODEL),\n         strides=(stride_qm, stride_qk),\n         offsets=(start_m * BLOCK_M, 0),\n         block_shape=(BLOCK_M, BLOCK_DMODEL),\n         order=(1, 0)\n     )\n     K_block_ptr = tl.make_block_ptr(\n-        base=K + qvk_offset,\n-        shape=(BLOCK_DMODEL, N_CTX),\n+        base=K + kv_offset,\n+        shape=(BLOCK_DMODEL, N_CTX + P_SEQ),\n         strides=(stride_kk, stride_kn),\n         offsets=(0, 0),\n         block_shape=(BLOCK_DMODEL, BLOCK_N),\n         order=(0, 1)\n     )\n     V_block_ptr = tl.make_block_ptr(\n-        base=V + qvk_offset,\n-        shape=(N_CTX, BLOCK_DMODEL),\n+        base=V + kv_offset,\n+        shape=(N_CTX + P_SEQ, BLOCK_DMODEL),\n         strides=(stride_vk, stride_vn),\n         offsets=(0, 0),\n         block_shape=(BLOCK_N, BLOCK_DMODEL),\n@@ -80,15 +81,15 @@ def _fwd_kernel(\n     q = (q * qk_scale).to(tl.float16)\n     # loop over k, v and update accumulator\n     lo = 0\n-    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX\n+    hi = P_SEQ + (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX + P_SEQ\n     for start_n in range(lo, hi, BLOCK_N):\n         # -- load k, v --\n         k = tl.load(K_block_ptr)\n         v = tl.load(V_block_ptr)\n         # -- compute qk ---\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         if IS_CAUSAL:\n-            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+            qk = tl.where(P_SEQ + offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n         qk += tl.dot(q, k)\n         # -- compute scaling constant ---\n         m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n@@ -110,7 +111,7 @@ def _fwd_kernel(\n     tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n     # write back O\n     O_block_ptr = tl.make_block_ptr(\n-        base=Out + qvk_offset,\n+        base=Out + q_offset,\n         shape=(N_CTX, BLOCK_DMODEL),\n         strides=(stride_om, stride_on),\n         offsets=(start_m * BLOCK_M, 0),\n@@ -146,8 +147,8 @@ def _bwd_kernel(\n     stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n     stride_vz, stride_vh, stride_vk, stride_vn,\n-    Z, H, N_CTX,\n-    num_block,\n+    Z, H, N_CTX, P_SEQ,\n+    num_block_q, num_block_kv,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n     CAUSAL: tl.constexpr,\n@@ -158,20 +159,20 @@ def _bwd_kernel(\n     qk_scale = sm_scale * 1.44269504\n     # offset pointers for batch/head\n     Q += off_z * stride_qz + off_h * stride_qh\n-    K += off_z * stride_qz + off_h * stride_qh\n-    V += off_z * stride_qz + off_h * stride_qh\n+    K += off_z * stride_kz + off_h * stride_kh\n+    V += off_z * stride_vz + off_h * stride_vh\n     DO += off_z * stride_qz + off_h * stride_qh\n     DQ += off_z * stride_qz + off_h * stride_qh\n-    DK += off_z * stride_qz + off_h * stride_qh\n-    DV += off_z * stride_qz + off_h * stride_qh\n-    for start_n in range(0, num_block):\n+    DK += off_z * stride_kz + off_h * stride_kh\n+    DV += off_z * stride_vz + off_h * stride_vh\n+    for start_n in range(0, num_block_kv):\n         if CAUSAL:\n-            lo = start_n * BLOCK_M\n+            lo = tl.math.max(start_n * BLOCK_M - P_SEQ, 0)\n         else:\n             lo = 0\n         # initialize row/col offsets\n         offs_qm = lo + tl.arange(0, BLOCK_M)\n-        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n+        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M) \n         offs_m = tl.arange(0, BLOCK_N)\n         offs_k = tl.arange(0, BLOCK_DMODEL)\n         # initialize pointers to value-like data\n@@ -183,20 +184,20 @@ def _bwd_kernel(\n         # pointer to row-wise quantities in value-like data\n         D_ptrs = D + off_hz * N_CTX\n         l_ptrs = L + off_hz * N_CTX\n-        # initialize dv amd dk\n-        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        # initialize dk amd dv\n         dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n         # k and v stay in SRAM throughout\n         k = tl.load(k_ptrs)\n         v = tl.load(v_ptrs)\n         # loop over rows\n-        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n+        for start_m in range(lo, num_block_q * BLOCK_M, BLOCK_M):\n             offs_m_curr = start_m + offs_m\n             # load q, k, v, do on-chip\n             q = tl.load(q_ptrs)\n             # recompute p = softmax(qk, dim=-1).T\n             if CAUSAL:\n-                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n+                qk = tl.where(P_SEQ + offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n             else:\n                 qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n             qk += tl.dot(q, tl.trans(k))\n@@ -223,10 +224,10 @@ def _bwd_kernel(\n             q_ptrs += BLOCK_M * stride_qm\n             do_ptrs += BLOCK_M * stride_qm\n         # write-back\n-        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n         dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n-        tl.store(dv_ptrs, dv)\n+        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n         tl.store(dk_ptrs, dk)\n+        tl.store(dv_ptrs, dv)\n \n \n empty = torch.empty(128, device=\"cuda\")\n@@ -245,7 +246,7 @@ def forward(ctx, q, k, v, causal, sm_scale):\n         BLOCK_N = 64\n         grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n-\n+        P_SEQ = 0 if q.shape[-2] == k.shape[-2] else k.shape[-2] - q.shape[-2]\n         num_warps = 4 if Lk <= 64 else 8\n         _fwd_kernel[grid](\n             q, k, v, sm_scale,\n@@ -255,7 +256,7 @@ def forward(ctx, q, k, v, causal, sm_scale):\n             k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n             v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n             o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n-            q.shape[0], q.shape[1], q.shape[2],\n+            q.shape[0], q.shape[1], q.shape[2], P_SEQ,\n             BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,\n             IS_CAUSAL=causal,\n             num_warps=num_warps,\n@@ -266,6 +267,7 @@ def forward(ctx, q, k, v, causal, sm_scale):\n         ctx.sm_scale = sm_scale\n         ctx.BLOCK_DMODEL = Lk\n         ctx.causal = causal\n+        ctx.P_SEQ = P_SEQ\n         return o\n \n     @staticmethod\n@@ -290,8 +292,8 @@ def backward(ctx, do):\n             q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n             k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n             v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-            q.shape[0], q.shape[1], q.shape[2],\n-            ctx.grid[0],\n+            q.shape[0], q.shape[1], q.shape[2], ctx.P_SEQ,\n+            ctx.grid[0], triton.cdiv(k.shape[2], BLOCK),\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n             BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n             CAUSAL=ctx.causal,\n@@ -303,17 +305,17 @@ def backward(ctx, do):\n attention = _attention.apply\n \n \n-@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(6, 9, 1024, 64)])\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD, P_SEQ', [(6, 9, 1024, 64, 128)])\n @pytest.mark.parametrize('causal', [False, True])\n-def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n+def test_op(Z, H, N_CTX, D_HEAD, P_SEQ, causal, dtype=torch.float16):\n     torch.manual_seed(20)\n     q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n-    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n-    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX + P_SEQ, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX + P_SEQ, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n     sm_scale = 0.5\n     dout = torch.randn_like(q)\n     # reference implementation\n-    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n+    M = torch.tril(torch.ones((N_CTX, N_CTX + P_SEQ), device=\"cuda\"), diagonal=P_SEQ)\n     p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n     if causal:\n         p[:, :, M == 0] = float(\"-inf\")"}, {"filename": "python/tutorials/10-experimental-tmastg-matrix-multiplication.py", "status": "modified", "additions": 5, "deletions": 29, "changes": 34, "file_content_changes": "@@ -84,31 +84,6 @@ def matmul_kernel(\n     tl.store(c_block_ptr, accumulator)\n \n \n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K)).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K)).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N)).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N)).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-\n-def get_proper_err(a, b, golden):\n-    golden_variant = get_variant_golden(a, b)\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n-    return (golden_abs_err, golden_rel_err)\n-\n-\n def matmul(a, b):\n     # checks constraints\n     assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n@@ -134,15 +109,16 @@ def grid(META):\n a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n b = torch.randn((512, 512), device='cuda', dtype=torch.float16).T\n c = matmul(a, b)\n+c = torch.nn.functional.normalize(c)\n+\n+golden = torch.nn.functional.normalize(torch.matmul(a, b))\n \n-golden = torch.matmul(a, b)\n-golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n torch.set_printoptions(profile=\"full\")\n assert_close(\n     c,\n     golden,\n-    rtol=max(1e-4, 1.5 * golden_rel_err),\n-    atol=max(1e-4, 1.5 * golden_abs_err),\n+    rtol=1e-2,\n+    atol=1e-3,\n     check_dtype=False)\n \n "}]