[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 109, "deletions": 2, "changes": 111, "file_content_changes": "@@ -25,7 +25,7 @@ jobs:\n         id: set-matrix\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n-            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], [\"self-hosted\", \"H100\"], [\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]'\n+            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"H100\"], [\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]'\n           else\n             echo '::set-output name=matrix::[\"ubuntu-latest\"]'\n           fi\n@@ -140,7 +140,7 @@ jobs:\n         if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n         uses: actions/upload-artifact@v2\n         with:\n-          name: artifacts\n+          name: artifacts ${{ matrix.runner[1] }}\n           path: ~/.triton/artifacts.tar.gz\n \n       - name: Run CXX unittests\n@@ -173,3 +173,110 @@ jobs:\n           sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n           python3 -m pytest -vs .\n           sudo nvidia-smi -i 0 -rgc\n+\n+  Compare-artifacts:\n+    needs: Integration-Tests\n+\n+    runs-on: ubuntu-latest\n+\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v2\n+\n+      - name: Install gh CLI\n+        run: |\n+          sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-key 23F3D4EA75716059\n+          echo \"deb [arch=$(dpkg --print-architecture)] https://cli.github.com/packages focal main\" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null\n+          sudo apt update\n+          sudo apt install gh\n+\n+      - name: Download latest main artifacts\n+        env:\n+          ARTIFACT_NAME: artifacts A100\n+          ARTIFACT_JOB_NAME: Integration-Tests\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+        run: |\n+          OWNER_REPO=\"${{ github.repository }}\"\n+          PR_NUMBER=$(gh api --method GET repos/$OWNER_REPO/pulls -f state=closed | jq \".[] | select(.merged_at != null) | .number\" | head -1)\n+          echo \"Last merged PR number: $PR_NUMBER\"\n+\n+          BRANCH_NAME=$(gh api repos/$OWNER_REPO/pulls/$PR_NUMBER --jq '.head.ref')\n+          echo \"BRANCH_NAME: $BRANCH_NAME\"\n+          WORKFLOW_RUN_ID=$(gh api --method GET repos/$OWNER_REPO/actions/runs | jq --arg branch_name \"$BRANCH_NAME\" '.workflow_runs[] | select(.head_branch == $branch_name)' | jq '.id' | head -1)\n+          echo \"WORKFLOW_RUN_ID: $WORKFLOW_RUN_ID\"\n+          ARTIFACT_URL=$(gh api repos/$OWNER_REPO/actions/runs/$WORKFLOW_RUN_ID/artifacts | jq --arg artifact_name \"$ARTIFACT_NAME\" '.artifacts[] | select(.name == $artifact_name).archive_download_url' --raw-output)\n+          echo \"ARTIFACT_URL: $ARTIFACT_URL\"\n+\n+          if [ -n \"$ARTIFACT_URL\" ]; then\n+            echo \"Downloading artifact: $ARTIFACT_URL\"\n+            curl --location --remote-header-name -H \"Authorization: token $GH_TOKEN\" -o reference.zip \"$ARTIFACT_URL\"\n+            # Print the size of the downloaded artifact\n+            echo \"Artifact size (stat): $(stat --printf=\"%s bytes\" reference.zip)\"\n+            echo \"Artifact size (du): $(du -sh reference.zip)\"\n+            unzip reference.zip\n+            tar -xzf artifacts.tar.gz\n+            rm reference.zip\n+            rm artifacts.tar.gz\n+            mv cache reference\n+          else\n+            echo \"No artifact found with the name: $ARTIFACT_NAME\"\n+          fi\n+      - name: Download current job artifacts\n+        uses: actions/download-artifact@v2\n+        with:\n+          name: artifacts A100\n+      - name: Unzip current job artifacts\n+        run: |\n+          # Print the size of the downloaded artifact\n+          echo \"Artifact size (stat): $(stat --printf=\"%s bytes\" artifacts.tar.gz)\"\n+          echo \"Artifact size (du): $(du -sh artifacts.tar.gz)\"\n+          tar -xzf artifacts.tar.gz\n+          rm artifacts.tar.gz\n+          mv cache current\n+      - name: Compare artifacts\n+        run: |\n+          set +e\n+          python3 python/test/tools/compare_files.py --path1 reference --path2 current --kernels python/test/kernel_comparison/kernels.yml\n+          exit_code=$?\n+          set -e\n+          echo $exit_code\n+          if [ $exit_code -eq 0 ]; then\n+            echo \"Artifacts are identical\"\n+            echo \"COMPARISON_RESULT=true\" >> $GITHUB_ENV\n+          elif [ $exit_code -eq 1 ]; then\n+            echo \"Artifacts are different\"\n+            echo \"COMPARISON_RESULT=false\" >> $GITHUB_ENV\n+          else\n+            echo \"Error while comparing artifacts\"\n+            echo \"COMPARISON_RESULT=error\" >> $GITHUB_ENV\n+          fi\n+          echo \"COMPARISON_RESULT=env.COMPARISON_RESULT\"\n+      - name: Check exit code and handle failure\n+        if: ${{ env.COMPARISON_RESULT == 'error' }}\n+        run: |\n+          echo \"Error while comparing artifacts\"\n+          exit 1\n+      - name: Fetch Run ID\n+        id: get_run_id\n+        run: echo \"RUN_ID=${{ github.run_id }}\" >> $GITHUB_ENV\n+\n+      - name: Upload results as artifact\n+        uses: actions/upload-artifact@v2\n+        with:\n+          name: kernels-reference-check\n+          path: kernels_reference_check.txt\n+\n+      - name: Check output and comment on PR\n+        if: ${{ env.COMPARISON_RESULT == 'false' }}\n+        uses: actions/github-script@v5\n+        with:\n+          script: |\n+            const run_id = ${{ env.RUN_ID }};\n+            const issue_number = context.payload.pull_request.number;\n+            const message = `:warning: **This PR does not produce bitwise identical kernels as the branch it's merged against.** Please check artifacts for details. [Download the output file here](https://github.com/${{ github.repository }}/actions/runs/${run_id}).`;\n+            await github.rest.issues.createComment({\n+                owner: context.repo.owner,\n+                repo: context.repo.repo,\n+                issue_number: issue_number,\n+                body: message\n+            });"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -392,6 +392,7 @@ def TT_DotOp : TT_Op<\"dot\", [Pure,\n     let results = (outs TT_FpIntTensor:$d);\n \n     let assemblyFormat = \"$a`,` $b`,` $c attr-dict `:` type($a) `*` type($b) `->` type($d)\";\n+    let hasVerifier = 1;\n }\n \n //"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "file_content_changes": "@@ -6,6 +6,7 @@\n #include \"mlir/IR/OperationSupport.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n namespace mlir {\n namespace triton {\n@@ -398,6 +399,25 @@ mlir::LogicalResult mlir::triton::DotOp::inferReturnTypes(\n   return mlir::success();\n }\n \n+LogicalResult mlir::triton::DotOp::verify() {\n+  auto aTy = getOperand(0).getType().cast<RankedTensorType>();\n+  auto bTy = getOperand(1).getType().cast<RankedTensorType>();\n+  if (aTy.getElementType() != bTy.getElementType())\n+    return emitError(\"element types of operands A and B must match\");\n+  auto aEncoding =\n+      aTy.getEncoding().dyn_cast_or_null<triton::gpu::DotOperandEncodingAttr>();\n+  auto bEncoding =\n+      bTy.getEncoding().dyn_cast_or_null<triton::gpu::DotOperandEncodingAttr>();\n+  if (!aEncoding && !bEncoding)\n+    return mlir::success();\n+  // Verify that the encodings are valid.\n+  if (!aEncoding || !bEncoding)\n+    return emitError(\"mismatching encoding between A and B operands\");\n+  if (aEncoding.getMMAv2kWidth() != bEncoding.getMMAv2kWidth())\n+    return emitError(\"mismatching kWidth between A and B operands\");\n+  return mlir::success();\n+}\n+\n //-- ReduceOp --\n static mlir::LogicalResult\n inferReduceReturnShape(const RankedTensorType &argTy, const Type &retEltTy,"}, {"filename": "python/setup.py", "status": "modified", "additions": 7, "deletions": 1, "changes": 8, "file_content_changes": "@@ -13,6 +13,7 @@\n \n from setuptools import Extension, setup\n from setuptools.command.build_ext import build_ext\n+from setuptools.command.build_py import build_py\n \n \n # Taken from https://github.com/pytorch/pytorch/blob/master/tools/setup_helpers/env.py\n@@ -146,6 +147,11 @@ def download_and_copy_ptxas():\n \n # ---- cmake extension ----\n \n+class CMakeBuildPy(build_py):\n+    def run(self) -> None:\n+        self.run_command('build_ext')\n+        return super().run()\n+\n \n class CMakeExtension(Extension):\n     def __init__(self, name, path, sourcedir=\"\"):\n@@ -280,7 +286,7 @@ def build_extension(self, ext):\n     ],\n     include_package_data=True,\n     ext_modules=[CMakeExtension(\"triton\", \"triton/_C/\")],\n-    cmdclass={\"build_ext\": CMakeBuild},\n+    cmdclass={\"build_ext\": CMakeBuild, \"build_py\": CMakeBuildPy},\n     zip_safe=False,\n     # for PyPI\n     keywords=[\"Compiler\", \"Deep Learning\"],"}, {"filename": "python/test/kernel_comparison/kernels.yml", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "file_content_changes": "@@ -0,0 +1,31 @@\n+name_and_extension:\n+  - name: _kernel_0d1d2d34567c89c1011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3d4d5d6d7c8d9c10d11c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3d4d5d6d7c8c9d10d11c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3456c789c1011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d345d6d7c8c9d1011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d34567c8c91011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3456c78c91011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d345d6c78c9d1011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d345d6c789c1011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3d4d5d6c7d8d9c10d11c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3d4d5d6c7d8c9d10d11c\n+    extension: ptx\n+  - name: _kernel_0d1d2d345d6d7c89c1011c\n+    extension: ptx\n+  - name: _bwd_kernel_0d1d2d34d5d6d7d8d9d10d11d12d13d14d15c16d17d18d19c20d21d22d23c2425d26d27\n+    extension: ptx\n+  - name: _fwd_kernel_0d1d2d34d5d6d7d8d9d10c11d12d13d14c15d16d17d18c19d20d21d22c2324d25d\n+    extension: ptx\n+  - name: _bwd_preprocess_0d1d2d3d4d\n+    extension: ptx"}, {"filename": "python/test/tools/compare_files.py", "status": "added", "additions": 261, "deletions": 0, "changes": 261, "file_content_changes": "@@ -0,0 +1,261 @@\n+import argparse\n+import difflib\n+import glob\n+import os\n+import sys\n+from typing import Dict, List, Optional, Tuple\n+\n+import yaml\n+\n+\n+class ComparisonResult:\n+    def __init__(self, name: str, extension: str, numComparisons: int, diffs: List[str] = None, errors: List[str] = None):\n+        self.name = name\n+        self.extension = extension\n+        self.numComparisons = numComparisons\n+        self.diffs = [] if diffs is None else diffs\n+        self.errors = [] if errors is None else errors\n+\n+    def isSuccess(self) -> bool:\n+        return len(self.diffs) == 0 and len(self.errors) == 0\n+\n+    def __str__(self) -> str:\n+        return f\"name={self.name}, extension={self.extension}, numComparisons={self.numComparisons}, success={self.isSuccess()}\"\n+\n+\n+def listFilesWithExtension(path: str, extension: str) -> List[str]:\n+    \"\"\"\n+        Returns a list of files in the given path with the given extension\n+        The files are returned with their full path\n+    \"\"\"\n+    files = glob.glob(os.path.join(path, f'*.{extension}'))\n+    return files\n+\n+\n+def getFileWithExtension(path: str, ext: str) -> Optional[str]:\n+    \"\"\"\n+        Returns a single file in the given path with the given extension\n+    \"\"\"\n+    # get all files in directory with extension\n+    files = listFilesWithExtension(path, ext)\n+    if len(files) == 0:\n+        return None\n+    # filter out files with grp in their name\n+    files = [f for f in files if \"__grp__\" not in f]\n+    if len(files) != 1:\n+        print(f\"Found {len(files)} files in {path} with extension {ext}!\")\n+        sys.exit(2)\n+    return files[0]\n+\n+\n+def loadYamlFile(filePath: str) -> List[Dict[str, str]]:\n+    \"\"\"\n+        Loads a yaml file and returns its content as a list of dictionaries\n+    \"\"\"\n+    with open(filePath, 'r') as file:\n+        content = yaml.safe_load(file)\n+    return content\n+\n+\n+def compareFiles(file1: str, file2: str) -> bool:\n+    \"\"\"\n+        Compares two files and returns True if they are the same, False otherwise\n+    \"\"\"\n+    with open(file1, 'rb') as f1, open(file2, 'rb') as f2:\n+        content1 = f1.read()\n+        content2 = f2.read()\n+\n+    return content1 == content2\n+\n+\n+def diffFiles(file1, file2):\n+    with open(file1, 'r') as f1:\n+        file1_lines = f1.readlines()\n+    with open(file2, 'r') as f2:\n+        file2_lines = f2.readlines()\n+\n+    diff = list(difflib.unified_diff(file1_lines, file2_lines, file1, file2))\n+    return diff\n+\n+\n+def getFileVec(path: str) -> List[Tuple[str, str]]:\n+    \"\"\"\n+        Returns a list of tuples (extension, file) for the given path (note: the path includes the hash)\n+        The returned list must have extensions (json, ttir, ttgir)\n+        in this particular order, unless a file with a certain extension does not exist\n+    \"\"\"\n+    vec = []\n+    for ext in [\"json\", \"ttir\", \"ttgir\"]:\n+        file = getFileWithExtension(path, ext)\n+        if file is not None:\n+            vec.append((ext, file))\n+    return vec\n+\n+\n+def getNameToHashesDict(path: str) -> Dict[str, List[str]]:\n+    \"\"\"\n+        Returns a dictionary that maps kernel names to a list of hashes that have the same kernel name\n+        in the given path\n+        Note: the hashes must have a json file and either a ttir or ttgir file, otherwise they are ignored\n+    \"\"\"\n+    nameToHashes = {}\n+    for hash in os.listdir(path):\n+        fullPath = os.path.join(path, hash)\n+        if not os.path.isdir(fullPath):\n+            print(f\"Path {fullPath} is not a directory!\")\n+            sys.exit(2)\n+        fileVec = getFileVec(fullPath)\n+        if len(fileVec) < 2 or fileVec[0][0] != \"json\":\n+            continue\n+        jsonFile = fileVec[0][1]\n+        # load json file\n+        with open(jsonFile, 'r') as file:\n+            content = yaml.safe_load(file)\n+            # get name\n+            name = content[\"name\"]\n+            nameToHashes.setdefault(name, []).append(hash)\n+    return nameToHashes\n+\n+\n+def doFilesMatch(path1: str, path2: str) -> bool:\n+    \"\"\"\n+        Returns True if the files in the given paths match, False otherwise\n+        The files are considered to match if:\n+        1. The number of files in both paths match\n+        2. The json files match\n+        3. Both paths have a ttir that match, if a ttir does not exist, the ttgir file must exist and match\n+    \"\"\"\n+    filesVec1 = getFileVec(path1)\n+    filesVec2 = getFileVec(path2)\n+    # The number of files must match\n+    if len(filesVec1) != len(filesVec2):\n+        return False\n+\n+    for (ext1, file1), (ext2, file2) in zip(filesVec1, filesVec2):\n+        if ext1 != ext2:\n+            return False\n+        if not compareFiles(file1, file2):\n+            return False\n+        else:\n+            # once we actually compared a ttir or ttgir file, we can break\n+            if ext1 in (\"ttir\", \"ttgir\"):\n+                break\n+    return True\n+\n+\n+def compareMatchingFiles(name: str, extension: str, nameToHashes1: Dict[str, List[str]], nameToHashes2: Dict[str, List[str]], args) -> ComparisonResult:\n+    \"\"\"\n+        Compare files with the given name/extension in all hashes in both paths\n+        Return the first mismatching files as a tuple (file1, file2), otherwise, return an empty tuple\n+    \"\"\"\n+    hashes1 = nameToHashes1.get(name, [])\n+    hashes2 = nameToHashes2.get(name, [])\n+    diffs = []\n+    errors = []\n+    numComparisons = 0\n+    for hash1 in hashes1:\n+        path1 = os.path.join(args.path1, hash1)\n+        for hash2 in hashes2:\n+            path2 = os.path.join(args.path2, hash2)\n+            # check whether both paths have:\n+            # 1. json files that match\n+            # 2. ttir files that match (if they exist), otherwise ttgir files that match (if they exist)\n+            # if any of these contraints is not met, then we can skip this pair of hashes since they are not a match\n+            if not doFilesMatch(path1, path2):\n+                continue\n+            numComparisons += 1\n+            extFile1 = listFilesWithExtension(path1, extension)[0]\n+            extFile2 = listFilesWithExtension(path2, extension)[0]\n+            diff = diffFiles(extFile1, extFile2)\n+            if len(diff) > 0:\n+                diffs.append(diffFiles(extFile2, extFile1))\n+    if numComparisons == 0:\n+        errors.append(f\"Did not find any matching files for {name}\")\n+    return ComparisonResult(name=name, extension=extension, numComparisons=numComparisons, diffs=diffs, errors=errors)\n+\n+\n+def dumpResults(results: List[ComparisonResult], fileName: str):\n+    \"\"\"\n+        Dumps the results to the given file\n+    \"\"\"\n+    with open(fileName, 'w') as file:\n+        for result in results:\n+            file.write(str(result) + \"\\n\")\n+            file.write(\"Diffs:\\n\")\n+            for diff in result.diffs:\n+                for line in diff:\n+                    file.write(line)\n+            file.write(\"Errors:\\n\")\n+            for error in result.errors:\n+                file.write(error)\n+            file.write(\"\\n\\n\")\n+\n+\n+def main(args) -> bool:\n+    \"\"\"\n+        Iterates over all kernels in the given yaml file and compares them\n+        in the given paths\n+    \"\"\"\n+    if args.path1 == args.path2:\n+        print(\"Cannot compare files in the same directory!\")\n+        sys.exit(2)\n+    # Get kernel name to hashes dict, these hashes would have the same kernel name\n+    nameToHashes1 = getNameToHashesDict(args.path1)\n+    nameToHashes2 = getNameToHashesDict(args.path2)\n+\n+    yamlFilePath = args.kernels\n+    if not os.path.exists(yamlFilePath):\n+        print(f\"Path {yamlFilePath} does not exist!\")\n+        sys.exit(2)\n+    nameAndExtension = loadYamlFile(yamlFilePath)[\"name_and_extension\"]\n+\n+    results = []\n+    # iterate over the kernels that need to be checked\n+    for d in nameAndExtension:\n+        name = d[\"name\"]  # kernel name\n+        extension = d[\"extension\"]  # extension of the file to be compared (e.g. ptx)\n+        # Compare all hashes on path 1 with all hashes on path 2\n+        # result is either the mismatching (file1, file2) with \"extension\" or empty tuple if no mismatch\n+        result = compareMatchingFiles(name, extension, nameToHashes1, nameToHashes2, args)\n+        print(result)\n+        # Otherwise, add it to the mismatches\n+        results.append(result)\n+\n+    # Dump results\n+    dumpResults(results, \"kernels_reference_check.txt\")\n+\n+    success = all(result.isSuccess() for result in results)\n+\n+    if not success:\n+        print(\"Failed!\")\n+        sys.exit(1)\n+\n+    print(\"Passed!\")\n+    sys.exit(0)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--path1\",\n+        type=str,\n+        default=None,\n+        required=True,\n+        help=(\"Path to first cache directory\"),\n+    )\n+    parser.add_argument(\n+        \"--path2\",\n+        type=str,\n+        default=None,\n+        required=True,\n+        help=(\"Path to second cache directory\"),\n+    )\n+    parser.add_argument(\n+        \"--kernels\",\n+        type=str,\n+        default=None,\n+        required=True,\n+        help=(\"Path to kernels yaml file\"),\n+    )\n+    args = parser.parse_args()\n+    main(args)"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -67,6 +67,7 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (128, 128, 32, 1, 4, 2, 384, 128, 640, AT, BT, DTYPE, DTYPE),\n                 (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE, DTYPE),\n                 (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE, DTYPE),\n+                (128, 256, 64, 1, 8, 3, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n         # n-stage\n@@ -131,6 +132,9 @@ def get_input(n, m, t, dtype):\n     th_c = torch.matmul(a, b)\n     try:\n         tt_c = triton.ops.matmul(a, b)\n-        torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n+        atol, rtol = 1e-2, 0\n+        if ADTYPE == torch.bfloat16 or BDTYPE == torch.bfloat16:\n+            atol, rtol = 3.5e-2, 0\n+        torch.testing.assert_allclose(th_c, tt_c, atol=atol, rtol=rtol)\n     except triton.OutOfResources as e:\n         pytest.skip(str(e))"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -71,7 +71,7 @@ def visit_Attribute(self, node):\n         lhs = self.visit(node.value)\n         while isinstance(lhs, ast.Attribute):\n             lhs = self.visit(lhs.value)\n-        if lhs is None or lhs.__name__ == \"triton\":\n+        if lhs is None or getattr(lhs, \"__name__\", \"\") == \"triton\":\n             return None\n         return getattr(lhs, node.attr)\n "}, {"filename": "test/Conversion/invalid.mlir", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "file_content_changes": "@@ -0,0 +1,41 @@\n+// RUN: triton-opt %s -split-input-file -verify-diagnostics\n+\n+#mma0 = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[1,1]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, kWidth=2}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  tt.func @convert_dot(%A: tensor<16x16xf32, #dot_operand_a>, %B: tensor<16x16xf16, #dot_operand_b>, %C: tensor<16x16xf32, #mma0>) {\n+    // expected-error@+1 {{element types of operands A and B must match}}\n+    %D = tt.dot %A, %B, %C {allowTF32 = true, transA = false, transB = false} :\n+        tensor<16x16xf32, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#mma0 = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[1,1]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, kWidth=1}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  tt.func @convert_dot(%A: tensor<16x16xf16>, %B: tensor<16x16xf16, #dot_operand_b>, %C: tensor<16x16xf32, #mma0>) {\n+    // expected-error@+1 {{mismatching encoding between A and B operands}}\n+    %D = tt.dot %A, %B, %C {allowTF32 = true, transA = false, transB = false} :\n+        tensor<16x16xf16> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#mma0 = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[1,1]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, kWidth=1}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  tt.func @convert_dot(%A: tensor<16x16xf16, #dot_operand_a>, %B: tensor<16x16xf16, #dot_operand_b>, %C: tensor<16x16xf32, #mma0>) {\n+    // expected-error@+1 {{mismatching kWidth between A and B operands}}\n+    %D = tt.dot %A, %B, %C {allowTF32 = true, transA = false, transB = false} :\n+        tensor<16x16xf16, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n+    tt.return\n+  }\n+}"}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "file_content_changes": "@@ -125,6 +125,27 @@ tt.func @push_elementwise5(\n   tt.return %newc : tensor<16x16xf32, #Cv1>\n }\n \n+// CHECK: tt.func @succeeds_if_arg_is_not_convert_layout\n+// CHECK: %[[ALOAD:.*]] = tt.load %arg0\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]]\n+// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ACVT]]\n+// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n+// CHECK: %[[C:.*]] = tt.dot %[[AF16]]\n+// CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma>\n+tt.func @succeeds_if_arg_is_not_convert_layout(\n+                   %pa: tensor<16x16x!tt.ptr<i8>, #ALR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #BLC> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %c: tensor<16x16xf32, #Cv2>) -> tensor<16x16xf32, #Cv2>{\n+  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #ALR>\n+  %dotai8 = triton_gpu.convert_layout %ai8 : (tensor<16x16xi8, #ALR>) -> tensor<16x16xi8, #Av2>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BLC>\n+  %dotaf8 = tt.bitcast %dotai8 : tensor<16x16xi8, #Av2> -> tensor<16x16xf8E5M2, #Av2>\n+  %dota = tt.fp_to_fp %dotaf8 : tensor<16x16xf8E5M2, #Av2> -> tensor<16x16xf16, #Av2>\n+  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLC>) -> tensor<16x16xf16, #Bv2>\n+  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av2> * tensor<16x16xf16, #Bv2> -> tensor<16x16xf32, #Cv2>\n+  tt.return %newc : tensor<16x16xf32, #Cv2>\n+}\n+\n }\n \n // -----"}]