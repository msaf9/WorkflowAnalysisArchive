[{"filename": "CMakeLists.txt", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -240,9 +240,12 @@ if(TRITON_BUILD_PYTHON_MODULE)\n       ${TRITON_LIBRARIES}\n     )\n   else()\n-    target_link_libraries(triton ${LLVM_LIBRARIES} z stdc++fs\n+    target_link_libraries(triton ${LLVM_LIBRARIES} z\n       ${TRITON_LIBRARIES}\n     )\n+    # TODO: Figure out which target is sufficient to fix errors; triton is\n+    # apparently not enough\n+    link_libraries(stdc++fs)\n   endif()\n \n   target_link_options(triton PRIVATE ${LLVM_LDFLAGS})"}, {"filename": "README.md", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -38,24 +38,25 @@ pip install -U --pre triton\n ```\n git clone https://github.com/openai/triton.git;\n cd triton/python;\n-pip install cmake; # build time dependency\n+pip install cmake; # build-time dependency\n pip install -e .\n ```\n \n # Changelog\n \n Version 2.0 is out! New features include:\n-- Many, many bugfixes\n+- Many, many bug fixes\n - Performance improvements\n - Backend rewritten to use MLIR\n - Support for kernels that contain back-to-back matmuls (e.g., flash attention)\n \n # Contributing\n \n-Community contributions are more than welcome, whether it be to fix bugs or to add new features. Feel free to open GitHub issues about your contribution ideas, and we will review them. A contributor's guide containing general guidelines is coming soon!\n+Community contributions are more than welcome, whether it be to fix bugs or to add new features. Feel free to open GitHub issues about your contribution ideas, and we will review them. Please do not submit PRs that simply fix simple typos in our documentation -- unless they can lead to confusion.\n \n-If you\u2019re interested in joining our team and working on Triton & GPU kernels, [we\u2019re hiring](https://openai.com/jobs/#acceleration)!\n+Note 1: A more detailed contributor's guide containing general guidelines is coming soon!\n \n+Note 2: If you\u2019re interested in joining our team and working on Triton & GPU kernels, [we\u2019re hiring](https://openai.com/jobs/#acceleration)!\n \n # Compatibility\n "}, {"filename": "cmake/FindLLVM.cmake", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,4 +1,3 @@\n-\n # - Find LLVM headers and libraries.\n # This module locates LLVM and adapts the llvm-config output for use with\n # CMake."}, {"filename": "docs/getting-started/installation.rst", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -1,10 +1,10 @@\n-==============\n+============\n Installation\n-==============\n+============\n \n----------------------\n+--------------------\n Binary Distributions\n----------------------\n+--------------------\n \n You can install the latest stable release of Triton from pip:\n \n@@ -21,21 +21,21 @@ And the latest nightly release:\n       pip install -U --pre triton\n \n \n---------------\n+-----------\n From Source\n---------------\n+-----------\n \n-+++++++++++++++\n+++++++++++++++\n Python Package\n-+++++++++++++++\n+++++++++++++++\n \n You can install the Python package from source by running the following commands:\n \n .. code-block:: bash\n \n       git clone https://github.com/openai/triton.git;\n       cd triton/python;\n-      pip install cmake; # build time dependency\n+      pip install cmake; # build-time dependency\n       pip install -e .\n \n Note that, if llvm-11 is not present on your system, the setup.py script will download the official LLVM11 static libraries link against that.\n@@ -51,5 +51,5 @@ and the benchmarks\n \n .. code-block:: bash\n       \n-      cd bench/\n+      cd bench\n       python -m run --with-plots --result-dir /tmp/triton-bench"}, {"filename": "docs/index.rst", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -3,6 +3,7 @@ Welcome to Triton's documentation!\n \n Triton is a language and compiler for parallel programming. It aims to provide a Python-based programming environment for productively writing custom DNN compute kernels capable of running at maximal throughput on modern GPU hardware.\n \n+\n Getting Started\n ---------------\n \n@@ -17,8 +18,9 @@ Getting Started\n    getting-started/installation\n    getting-started/tutorials/index\n \n+\n Python API\n--------------------\n+----------\n \n - :doc:`triton <python-api/triton>`\n - :doc:`triton.language <python-api/triton.language>`\n@@ -36,7 +38,7 @@ Python API\n \n    \n Going Further\n-------------------\n+-------------\n \n Check out the following documents to learn more about Triton and how it compares against other DSLs for DNNs:\n "}, {"filename": "docs/programming-guide/chapter-1/introduction.rst", "status": "modified", "additions": 16, "deletions": 14, "changes": 30, "file_content_changes": "@@ -1,18 +1,18 @@\n-==============\n+============\n Introduction\n-==============\n+============\n \n---------------\n+-----------\n Motivations\n---------------\n+-----------\n \n-Over the past decade, Deep Neural Networks (DNNs) have emerged as an important class of Machine Learning (ML) models, capable of  achieving state-of-the-art performance across many domains ranging from natural language processing [SUTSKEVER2014]_ to computer vision [REDMON2016]_ to computational neuroscience [LEE2017]_. The strength of these models lies in their hierarchical structure, composed of a sequence of parametric (e.g., convolutional) and non-parametric (e.g., rectified linearity) *layers*. This pattern, though notoriously computationally expensive, also generates a large amount of highly parallelizable work particularly well suited for multi- and many- core processors.\n+Over the past decade, Deep Neural Networks (DNNs) have emerged as an important class of Machine Learning (ML) models, capable of achieving state-of-the-art performance across many domains ranging from natural language processing [SUTSKEVER2014]_ to computer vision [REDMON2016]_ to computational neuroscience [LEE2017]_. The strength of these models lies in their hierarchical structure, composed of a sequence of parametric (e.g., convolutional) and non-parametric (e.g., rectified linearity) *layers*. This pattern, though notoriously computationally expensive, also generates a large amount of highly parallelizable work particularly well suited for multi- and many- core processors.\n \n As a consequence, Graphics Processing Units (GPUs) have become a cheap and accessible resource for exploring and/or deploying novel research ideas in the field. This trend has been accelerated by the release of several frameworks for General-Purpose GPU (GPGPU) computing, such as CUDA and OpenCL, which have made the development of high-performance programs easier. Yet, GPUs remain incredibly challenging to optimize for locality and parallelism, especially for computations that cannot be efficiently implemented using a combination of pre-existing optimized primitives. To make matters worse, GPU architectures are also rapidly evolving and specializing, as evidenced by the addition of tensor cores to NVIDIA (and more recently AMD) micro-architectures.\n \n-This tension between the computational opportunities offered by DNNs and the practical difficulty of GPU programming has created substantial academic and industrial interest for Domain-Specific Languages (DSLs) and compilers. Regrettably, these systems -- whether they be based on  polyhedral machinery (*e.g.*, Tiramisu [BAGHDADI2021]_, Tensor Comprehensions [VASILACHE2018]_) or scheduling languages (*e.g.*, Halide [JRK2013]_, TVM [CHEN2018]_) -- remain less flexible and (for the same algorithm) markedly slower than the best handwritten compute kernels available in libraries like `cuBLAS <https://docs.nvidia.com/cuda/cublas/index.html>`_, `cuDNN <https://docs.nvidia.com/deeplearning/cudnn/api/index.html>`_ or `TensorRT <https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html>`_.\n+This tension between the computational opportunities offered by DNNs and the practical difficulty of GPU programming has created substantial academic and industrial interest for Domain-Specific Languages (DSLs) and compilers. Regrettably, these systems -- whether they be based on polyhedral machinery (e.g., Tiramisu [BAGHDADI2021]_, Tensor Comprehensions [VASILACHE2018]_) or scheduling languages (e.g., Halide [JRK2013]_, TVM [CHEN2018]_) -- remain less flexible and (for the same algorithm) markedly slower than the best handwritten compute kernels available in libraries like `cuBLAS <https://docs.nvidia.com/cuda/cublas/index.html>`_, `cuDNN <https://docs.nvidia.com/deeplearning/cudnn/api/index.html>`_ or `TensorRT <https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html>`_.\n \n-The main premise of this project is the following: programming paradigms based on blocked algorithms [LAM1991]_ can facilitate the construction of high-performance compute kernels for neural networks.  We specifically revisit traditional \"Single Program, Multiple Data\" (SPMD [AUGUIN1983]_) execution models for GPUs, and propose a variant in which programs -- rather than threads -- are blocked. For example, in the case of matrix multiplication, CUDA and Triton differ as follows:\n+The main premise of this project is the following: programming paradigms based on blocked algorithms [LAM1991]_ can facilitate the construction of high-performance compute kernels for neural networks. We specifically revisit traditional \"Single Program, Multiple Data\" (SPMD [AUGUIN1983]_) execution models for GPUs, and propose a variant in which programs -- rather than threads -- are blocked. For example, in the case of matrix multiplication, CUDA and Triton differ as follows:\n \n .. table::\n     :widths: 50 50\n@@ -31,8 +31,8 @@ The main premise of this project is the following: programming paradigms based o\n     |   #pragma parallel                                  |   #pragma parallel                                  |\n     |   for(int n = 0; j < N; n++){                       |   for(int n = 0; n < N; n += NB){                   |\n     |     float acc = 0;                                  |     float acc[MB, NB] = 0;                          |\n-    |     for(int k = 0; k < K;k ++)                      |     for(int k = 0; k < K; k += KB)                  |\n-    |       acc += A[i, k]* B[k, j];                      |       acc +=  A[m:m+MB, k:k+KB]                     |\n+    |     for(int k = 0; k < K; k++)                      |     for(int k = 0; k < K; k += KB)                  |\n+    |       acc += A[i, k] * B[k, j];                     |       acc +=  A[m:m+MB, k:k+KB]                     |\n     |                                                     |             @ B[k:k+KB, n:n+NB];                    |\n     |     C[i, j] = acc;                                  |     C[m:m+MB, n:n+NB] = acc;                        |\n     |   }                                                 |   }                                                 |\n@@ -48,15 +48,17 @@ The main premise of this project is the following: programming paradigms based o\n \n A key benefit of this approach is that it leads to block-structured iteration spaces that offer programmers more flexibility than existing DSLs when implementing sparse operations, all while allowing compilers to aggressively optimize programs for data locality and parallelism.\n \n---------------\n+\n+----------\n Challenges\n---------------\n+----------\n \n The main challenge posed by our proposed paradigm is that of work scheduling, i.e., how the work done by each program instance should be partitioned for efficient execution on modern GPUs. To address this issue, the Triton compiler makes heavy use of *block-level data-flow analysis*, a technique for scheduling iteration blocks statically based on the control- and data-flow structure of the target program. The resulting system actually works surprisingly well: our compiler manages to apply a broad range of interesting optimization automatically (e.g., automatic coalescing, thread swizzling, pre-fetching, automatic vectorization, tensor core-aware instruction selection, shared memory allocation/synchronization, asynchronous copy scheduling). Of course doing all this is not trivial; one of the purposes of this guide is to give you a sense of how it works.\n \n---------------\n+\n+----------\n References\n---------------\n+----------\n \n .. [SUTSKEVER2014] I. Sutskever et al., \"Sequence to Sequence Learning with Neural Networks\", NIPS 2014\n .. [REDMON2016] J. Redmon et al., \"You Only Look Once: Unified, Real-Time Object Detection\", CVPR 2016\n@@ -66,4 +68,4 @@ References\n .. [JRK2013] J. Ragan-Kelley et al., \"Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines\", PLDI 2013\n .. [CHEN2018] T. Chen et al., \"TVM: An Automated End-to-End Optimizing Compiler for Deep Learning\", OSDI 2018\n .. [LAM1991] M. Lam et al., \"The Cache Performance and Optimizations of Blocked Algorithms\", ASPLOS 1991\n-.. [AUGUIN1983] M. Auguin et al., \"Opsila: an advanced SIMD for numerical analysis and signal processing\", EUROMICRO 1983\n\\ No newline at end of file\n+.. [AUGUIN1983] M. Auguin et al., \"Opsila: an advanced SIMD for numerical analysis and signal processing\", EUROMICRO 1983"}, {"filename": "docs/programming-guide/chapter-2/related-work.rst", "status": "modified", "additions": 22, "deletions": 19, "changes": 41, "file_content_changes": "@@ -1,18 +1,19 @@\n-==============\n+============\n Related Work\n-==============\n+============\n \n At first sight, Triton may seem like just yet another DSL for DNNs. The purpose of this section is to contextualize Triton and highlight its differences with the two leading approaches in this domain: polyhedral compilation and scheduling languages.\n \n------------------------\n+\n+----------------------\n Polyhedral Compilation\n------------------------\n+----------------------\n \n Traditional compilers typically rely on intermediate representations, such as LLVM-IR [LATTNER2004]_, that encode control flow information using (un)conditional branches. This relatively low-level format makes it difficult to statically analyze the runtime behavior (e.g., cache misses) of input programs, and to  automatically optimize loops accordingly through the use of tiling [WOLFE1989]_, fusion [DARTE1999]_ and interchange [ALLEN1984]_. To solve this issue, polyhedral compilers [ANCOURT1991]_ rely on program representations that have statically predictable control flow, thereby enabling aggressive compile-time program transformations for data locality and parallelism. Though this strategy has been adopted by many languages and compilers for DNNs such as Tiramisu [BAGHDADI2021]_, Tensor Comprehensions [VASILACHE2018]_, Diesel [ELANGO2018]_ and the Affine dialect in MLIR [LATTNER2019]_, it also comes with a number of limitations that will be described later in this section.\n \n-+++++++++++++++++++++++\n+++++++++++++++++++++++\n Program Representation\n-+++++++++++++++++++++++\n+++++++++++++++++++++++\n \n Polyhedral compilation is a vast area of research. In this section we only outline the most basic aspects of this topic, but readers interested in the solid mathematical foundations underneath may refer to the ample literature on linear and integer programming.\n \n@@ -105,19 +106,19 @@ Where :math:`\\Theta_S(\\mathbf{x})` is a p-dimensional vector representing the sl\n \n where :math:`i` and :math:`j` are respectively the slowest and fastest growing loop indices in the nest. If :math:`T_S` is a vector (resp. tensor), then :math:`\\Theta_S` is a said to be one-dimensional (resp. multi-dimensional).\n \n-+++++++++++\n+++++++++++\n Advantages\n-+++++++++++\n+++++++++++\n \n Programs amenable to polyhedral compilation can be aggressively transformed and optimized. Most of these transformations actually boil down to the production of  schedules and iteration domains that enable loop transformations promoting parallelism and spatial/temporal data locality (e.g., fusion, interchange, tiling, parallelization).\n \n Polyhedral compilers can also automatically go through complex verification processes to ensure that the semantics of their input program is preserved throughout this optimization phase. Note that polyhedral optimizers are not incompatible with more standard optimization techniques. In fact, it is not uncommon for these systems to be implemented as a set of LLVM passes that can be run ahead of more traditional compilation techniques [GROSSER2012]_.\n \n All in all, polyhedral machinery is extremely powerful, when applicable. It has been shown to support most common loop transformations, and has indeed achieved performance comparable to state-of-the-art GPU libraries for dense matrix multiplication [ELANGO2018]_. Additionally, it is also fully automatic and doesn't require any hint from programmers apart from source-code in a C-like format. \n \n-++++++++++++\n++++++++++++\n Limitations\n-++++++++++++\n++++++++++++\n \n Unfortunately, polyhedral compilers suffer from two major limitations that have prevented its adoption as a universal method for code generation in neural networks.\n \n@@ -127,9 +128,10 @@ Second, the polyhedral framework is not very generally applicable; SCoPs are rel\n \n On the other hand, blocked program representations advocated by this dissertation are less restricted in scope and can achieve close to peak performance using standard dataflow analysis.\n \n------------------------\n+\n+--------------------\n Scheduling Languages\n------------------------\n+--------------------\n \n Separation of concerns [DIJKSTRA82]_ is a well-known design principle in computer science: programs should be decomposed into modular layers of abstraction that separate the semantics of their algorithms from the details of their implementation. Systems like Halide and TVM push this philosophy one step further, and enforce this separation at the grammatical level through the use of a  **scheduling language**. The benefits of this methodology are particularly visible in the case of matrix multiplication, where, as one can see below, the definition of the algorithm (Line 1-7) is completely disjoint from its implementation (Line 8-16), meaning that both can be maintained, optimized and distributed independently. \n \n@@ -156,17 +158,17 @@ Separation of concerns [DIJKSTRA82]_ is a well-known design principle in compute\n \n The resulting code may however not be completely portable, as schedules can sometimes rely on execution models (e.g., SPMD) or hardware intrinsics (e.g., matrix-multiply-accumulate) that are not widely available. This issue can be mitigated by auto-scheduling mechanisms [MULLAPUDI2016]_.\n \n-+++++++++++\n+++++++++++\n Advantages\n-+++++++++++\n+++++++++++\n \n The main advantage of this approach is that it allows programmers to write an algorithm *only once*, and focus on performance optimization separately. It makes it possible to manually specify optimizations that a polyhedral compiler wouldn't be able to figure out automatically using static data-flow analysis.\n \n Scheduling languages are, without a doubt, one of the most popular approaches for neural network code generation. The most popular system for this purpose is probably TVM, which provides good performance across a wide range of platforms as well as built-in automatic scheduling mechanisms.\n \n-++++++++++++\n++++++++++++\n Limitations\n-++++++++++++\n++++++++++++\n \n This ease-of-development comes at a cost. First of all, existing systems that follow this paradigm tend to be noticeably slower than Triton on modern hardware when applicable (e.g., V100/A100 tensor cores w/ equal tile sizes). I do believe that this is not a fundamental issue of scheduling languages -- in the sense that it could probably be solved with more efforts -- but it could mean that these systems are harder to engineer. More importantly, existing scheduling languages generate loops whose bounds and increments cannot depend on surrounding loop indices without at least imposing severe constraints on possible schedules -- if not breaking the system entirely. This is problematic for sparse computations, whose iteration spaces may be irregular.\n \n@@ -181,7 +183,7 @@ This ease-of-development comes at a cost. First of all, existing systems that fo\n     |   for(int j = 0; j < 4; j++)                        |                                                     |\n     |     float acc = 0;                                  |                                                     |\n     |     for(int k = 0; k < K[i]; k++)                   |                                                     |\n-    |       acc += A[i][col[i,k]]*B[k][j]                 |                                                     |\n+    |       acc += A[i][col[i, k]] * B[k][j]              |                                                     |\n     |     C[i][j] = acc;                                  |                                                     |\n     +-----------------------------------------------------+-----------------------------------------------------+\n \n@@ -190,9 +192,10 @@ This ease-of-development comes at a cost. First of all, existing systems that fo\n \n On the other hand, the block-based program representation that we advocate for through this work allows for block-structured iteration spaces and allows programmers to manually handle load-balancing as they wish.\n \n---------------\n+\n+----------\n References\n---------------\n+----------\n \n .. [LATTNER2004] C. Lattner et al., \"LLVM: a compilation framework for lifelong program analysis transformation\", CGO 2004\n .. [WOLFE1989] M. Wolfe, \"More Iteration Space Tiling\", SC 1989"}, {"filename": "docs/python-api/triton.language.rst", "status": "modified", "additions": 17, "deletions": 14, "changes": 31, "file_content_changes": "@@ -1,11 +1,11 @@\n triton.language\n-================\n+===============\n \n .. currentmodule:: triton.language\n \n \n Programming Model\n--------------------\n+-----------------\n \n .. autosummary::\n     :toctree: generated\n@@ -16,7 +16,7 @@ Programming Model\n \n \n Creation Ops\n--------------\n+------------\n \n .. autosummary::\n     :toctree: generated\n@@ -27,7 +27,7 @@ Creation Ops\n \n \n Shape Manipulation Ops\n------------------------\n+----------------------\n \n .. autosummary::\n     :toctree: generated\n@@ -40,16 +40,17 @@ Shape Manipulation Ops\n \n \n Linear Algebra Ops\n--------------------\n+------------------\n \n .. autosummary::\n     :toctree: generated\n     :nosignatures:\n \n     dot\n \n+\n Memory Ops\n---------------------\n+----------\n \n .. autosummary::\n     :toctree: generated\n@@ -62,7 +63,7 @@ Memory Ops\n \n \n Indexing Ops\n---------------\n+------------\n \n .. autosummary::\n     :toctree: generated\n@@ -72,7 +73,7 @@ Indexing Ops\n \n \n Math Ops\n-----------\n+--------\n \n .. autosummary::\n     :toctree: generated\n@@ -88,7 +89,7 @@ Math Ops\n \n \n Reduction Ops\n----------------\n+-------------\n \n .. autosummary::\n     :toctree: generated\n@@ -98,8 +99,9 @@ Reduction Ops\n     min\n     sum\n \n+\n Atomic Ops\n----------------\n+----------\n \n .. autosummary::\n     :toctree: generated\n@@ -112,7 +114,7 @@ Atomic Ops\n \n \n Comparison ops\n----------------\n+--------------\n \n .. autosummary::\n     :toctree: generated\n@@ -124,7 +126,7 @@ Comparison ops\n .. _Random Number Generation:\n \n Random Number Generation\n--------------------------\n+------------------------\n \n .. autosummary::\n     :toctree: generated\n@@ -135,11 +137,12 @@ Random Number Generation\n     rand\n     randn\n \n+\n Compiler Hint Ops\n--------------------\n+-----------------\n \n .. autosummary::\n     :toctree: generated\n     :nosignatures:\n \n-    multiple_of\n\\ No newline at end of file\n+    multiple_of"}, {"filename": "docs/python-api/triton.rst", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n triton\n-========\n+======\n \n .. currentmodule:: triton\n \n@@ -10,4 +10,4 @@ triton\n     jit\n     autotune\n     heuristics\n-    Config\n\\ No newline at end of file\n+    Config"}, {"filename": "docs/python-api/triton.testing.rst", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n triton.testing\n-================\n+==============\n \n .. currentmodule:: triton.testing\n \n@@ -9,4 +9,4 @@ triton.testing\n \n     do_bench\n     Benchmark\n-    perf_report\n\\ No newline at end of file\n+    perf_report"}, {"filename": "include/triton/Analysis/AxisInfo.h", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -112,7 +112,8 @@ class AxisInfo {\n \n   /// The _divisibility_ information maps the `d`-th\n   /// dimension to the largest power-of-two that\n-  /// divides the first element of all the values along it\n+  /// divides the first element of all groups of\n+  // _contiguity_ values along it\n   /// For example:\n   /// [10, 11, 12, 13, 18, 19, 20, 21]\n   /// [20, 21, 22, 23, 28, 29, 30, 31]\n@@ -123,6 +124,10 @@ class AxisInfo {\n   /// [14, 18, 22, 26]\n   /// [15, 19, 23, 27]\n   //  would have divisibility [4, 1]\n+  //  On the other hand:\n+  //  [0, 1, 2, 0, 4, 5, 6, 7]\n+  //  would have divisibility 1 because\n+  //  _contiguity_=1\n   DimVectorT divisibility;\n \n   /// The _constancy_ information maps the `d`-th"}, {"filename": "include/triton/Conversion/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -1,4 +1,3 @@\n-\n set(LLVM_TARGET_DEFINITIONS Passes.td)\n mlir_tablegen(Passes.h.inc -gen-pass-decls)\n-add_public_tablegen_target(TritonConversionPassIncGen)\n\\ No newline at end of file\n+add_public_tablegen_target(TritonConversionPassIncGen)"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 7, "deletions": 1, "changes": 8, "file_content_changes": "@@ -72,7 +72,7 @@ def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n def TT_FpToFpOp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n                                      SameOperandsAndResultEncoding,\n                                      Pure,\n-                                     DeclareOpInterfaceMethods<CastOpInterface>]> {\n+                                     /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Floating point casting for custom types\";\n \n     let description = [{\n@@ -246,6 +246,9 @@ def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [Pure,\n     let results = (outs TT_Tensor:$result);\n \n     let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n+\n+    let hasCanonicalizeMethod = 1;\n+    let hasFolder = 1;\n }\n \n // view is not `pure` because it may reorder elements\n@@ -259,6 +262,8 @@ def TT_ViewOp : TT_Op<\"view\", [NoMemoryEffect,\n \n     let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n \n+    let hasCanonicalizeMethod = 1;\n+    let hasFolder = 1;\n }\n \n def TT_BroadcastOp : TT_Op<\"broadcast\", [Pure,\n@@ -272,6 +277,7 @@ def TT_BroadcastOp : TT_Op<\"broadcast\", [Pure,\n \n     let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n \n+    let hasCanonicalizeMethod = 1;\n     let hasFolder = 1;\n }\n "}, {"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -14,9 +14,7 @@ class TritonTypeDef<string name, string _mnemonic>\n }\n \n // Floating-point Type\n-def F8 : TritonTypeDef<\"Float8\", \"f8\">;\n-\n-def TT_Float : AnyTypeOf<[F8, F16, BF16, F32, F64], \"floating-point\">;\n+def TT_Float : AnyTypeOf<[F8E4M3FN, F8E5M2, F16, BF16, F32, F64], \"floating-point\">;\n def TT_FloatTensor : TensorOf<[TT_Float]>;\n def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -9,4 +9,3 @@ set(LLVM_TARGET_DEFINITIONS TritonGPUAttrDefs.td)\n mlir_tablegen(TritonGPUAttrDefs.h.inc -gen-attrdef-decls)\n mlir_tablegen(TritonGPUAttrDefs.cpp.inc -gen-attrdef-defs)\n add_public_tablegen_target(TritonGPUAttrDefsIncGen)\n-"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 48, "deletions": 33, "changes": 81, "file_content_changes": "@@ -374,44 +374,63 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n   );\n \n   let builders = [\n-     // Specially for MMAV1(Volta)\n+    // Specially for MMAV1(Volta)\n     AttrBuilder<(ins \"int\":$versionMajor,\n                      \"int\":$numWarps,\n+                     \"ArrayRef<int64_t>\":$shapeC,\n+                     \"bool\":$isARow,\n+                     \"bool\":$isBRow,\n+                     \"bool\":$isAVec4,\n+                     \"bool\":$isBVec4,\n                      \"int\":$id), [{\n       assert(versionMajor == 1 && \"This builder is specially for versionMajor==1\");\n-      SmallVector<unsigned> wpt({static_cast<unsigned>(numWarps), 1});\n-      int versionMinor = 0;\n+      // 4-bits to encode 4 booleans: [isARow, isBRow, isAVec4, isBVec4]\n+      int versionMinor = (isARow * (1<<0)) |\\\n+                         (isBRow * (1<<1)) |\\\n+                         (isAVec4 * (1<<2)) |\\\n+                         (isBVec4 * (1<<3));\n \n-      assert(id < (1<<numBitsToHoldMmaV1ID) && \"MMAv1 ID exceeds the maximum\");\n-      for (int i = 0; i < numBitsToHoldMmaV1ID; ++i)\n-        versionMinor |= static_cast<bool>((1<<i) & id) * (1<<(4+i));\n \n+      // TODO: Share code with\n+      // DotOpMmaV1ConversionHelper::AParam/BParam, since same code to compute the\n+      // rep,spw and fpw.\n+      SmallVector<unsigned> wpt({1, 1});\n+      SmallVector<unsigned> wpt_nm1;\n+\n+      SmallVector<int, 2> rep(2), spw(2);\n+      std::array<int, 3> fpw{{2, 2, 1}};\n+      int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+      rep[0] = 2 * packSize0;\n+      spw[0] = fpw[0] * 4 * rep[0];\n+\n+      int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+      rep[1] = 2 * packSize1;\n+      spw[1] = fpw[1] * 4 * rep[1];\n+\n+      do {\n+        wpt_nm1 = wpt;\n+        if (wpt[0] * wpt[1] < numWarps)\n+          wpt[0] = std::clamp<int>(wpt[0] * 2, 1, shapeC[0] / spw[0]);\n+        if (wpt[0] * wpt[1] < numWarps)\n+          wpt[1] = std::clamp<int>(wpt[1] * 2, 1, shapeC[1] / spw[1]);\n+      } while (wpt_nm1 != wpt);\n+      \n       return $_get(context, versionMajor, versionMinor, wpt);\n     }]>,\n \n-    // Specially for MMAV1(Volta)\n+\n     AttrBuilder<(ins \"int\":$versionMajor,\n-                     \"ArrayRef<unsigned>\":$warpsPerCTA,\n+                     \"int\":$numWarps,\n                      \"ArrayRef<int64_t>\":$shapeA,\n                      \"ArrayRef<int64_t>\":$shapeB,\n+                     \"ArrayRef<int64_t>\":$shapeC,\n                      \"bool\":$isARow,\n                      \"bool\":$isBRow,\n                      \"int\":$id), [{\n       assert(versionMajor == 1 && \"This builder is specially for versionMajor==1\");\n       bool isAVec4 = !isARow && (shapeA[isARow] <= 16);\n       bool isBVec4 = isBRow && (shapeB[isBRow] <= 16);\n-      // 4-bits to encode 4 booleans: [isARow, isBRow, isAVec4, isBVec4]\n-      // 3-bits to encode the MMA ID to make each unique\n-      int versionMinor = (isARow * (1<<0)) |\\\n-                         (isBRow * (1<<1)) |\\\n-                         (isAVec4 * (1<<2)) |\\\n-                         (isBVec4 * (1<<3));\n-\n-      assert(id < (1<<numBitsToHoldMmaV1ID) && \"MMAv1 ID exceeds the maximum\");\n-      for (int i = 0; i < numBitsToHoldMmaV1ID; ++i)\n-        versionMinor |= static_cast<bool>((1<<i) & id) * (1<<(4+i));\n-\n-      return $_get(context, versionMajor, versionMinor, warpsPerCTA);\n+      return get(context, versionMajor, numWarps, shapeC, isARow, isBRow, isAVec4, isBVec4, id);\n     }]>\n   ];\n \n@@ -481,25 +500,21 @@ section 9.7.13.4.1 for more details.\n   let parameters = (\n     ins\n     \"unsigned\":$opIdx,\n-    \"Attribute\":$parent,\n-    \"Attribute\":$isMMAv1Row\n+    \"Attribute\":$parent\n   );\n \n   let builders = [\n-    AttrBuilder<(ins \"unsigned\":$opIdx,\n-                     \"Attribute\":$parent), [{\n-      Attribute isMMAv1Row;\n-      if(parent.isa<MmaEncodingAttr>() &&\n-         parent.cast<MmaEncodingAttr>().isVolta()){\n-        isMMAv1Row = BoolAttr::get(context, true);\n-      }\n-      return $_get(context, opIdx, parent, isMMAv1Row);\n-    }]>\n-\n   ];\n \n   let hasCustomAssemblyFormat = 1;\n-  let extraClassDeclaration = extraBaseClassDeclaration;\n+  let extraClassDeclaration = extraBaseClassDeclaration # [{\n+    bool getMMAv1IsRow() const;\n+    bool getMMAv1IsVec4() const;\n+    SmallVector<int> getMMAv1Rep() const;\n+    SmallVector<int> getMMAv1ShapePerWarp() const;\n+    int getMMAv1Vec() const;\n+    int getMMAv1NumOuter(ArrayRef<int64_t> shape) const;\n+  }];\n }\n \n "}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 0, "deletions": 12, "changes": 12, "file_content_changes": "@@ -123,16 +123,4 @@ def TritonGPUDecomposeConversions: Pass<\"tritongpu-decompose-conversions\", \"mlir\n                            \"mlir::triton::TritonDialect\"];\n }\n \n-def UpdateMmaForVolta : Pass<\"tritongpu-update-mma-for-volta\", \"mlir::ModuleOp\"> {\n-  let summary = \"Update mma encodings for Volta\";\n-\n-  let description = [{\n-    This helps to update the mma encodings for Volta.\n-  }];\n-\n-  let constructor = \"mlir::createTritonGPUUpdateMmaForVoltaPass()\";\n-\n-  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n-}\n-\n #endif"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -65,7 +65,7 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n         return {};\n \n   assert(srcLayout && dstLayout &&\n-         \"Unexpect layout in getScratchConfigForCvtLayout()\");\n+         \"Unexpected layout in getScratchConfigForCvtLayout()\");\n   auto [inOrd, outOrd] = getCvtOrder(srcLayout, dstLayout);\n   unsigned srcContigPerThread = getContigPerThread(srcLayout)[inOrd[0]];\n   unsigned dstContigPerThread = getContigPerThread(dstLayout)[outOrd[0]];"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 22, "deletions": 20, "changes": 42, "file_content_changes": "@@ -164,16 +164,16 @@ class MakeRangeOpAxisInfoVisitor final\n   }\n };\n \n-class ConstantOpAxisInfoVisitor final\n-    : public AxisInfoVisitorImpl<arith::ConstantOp> {\n+template <typename OpTy>\n+class ConstantOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n public:\n-  using AxisInfoVisitorImpl<arith::ConstantOp>::AxisInfoVisitorImpl;\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n \n   AxisInfo\n-  getAxisInfo(arith::ConstantOp op,\n+  getAxisInfo(OpTy op,\n               ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n-    auto intAttr = op.getValue().dyn_cast<IntegerAttr>();\n-    auto boolAttr = op.getValue().dyn_cast<BoolAttr>();\n+    auto intAttr = op.getValue().template dyn_cast<IntegerAttr>();\n+    auto boolAttr = op.getValue().template dyn_cast<BoolAttr>();\n     if (intAttr || boolAttr) {\n       int64_t value{};\n       if (intAttr)\n@@ -186,10 +186,10 @@ class ConstantOpAxisInfoVisitor final\n                       /*knownConstantValue=*/{value});\n     }\n     // TODO: generalize to dense attr\n-    auto splatAttr = op.getValue().dyn_cast<SplatElementsAttr>();\n+    auto splatAttr = op.getValue().template dyn_cast<SplatElementsAttr>();\n     if (splatAttr && splatAttr.getElementType().isIntOrIndex()) {\n-      int64_t value = splatAttr.getSplatValue<APInt>().getZExtValue();\n-      TensorType ty = splatAttr.getType().cast<TensorType>();\n+      int64_t value = splatAttr.template getSplatValue<APInt>().getZExtValue();\n+      TensorType ty = splatAttr.getType().template cast<TensorType>();\n       return AxisInfo(\n           /*contiguity=*/AxisInfo::DimVectorT(ty.getRank(), 1),\n           /*divisibility=*/\n@@ -233,7 +233,8 @@ class AddSubOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n     if (lhs.getConstantValue().has_value() &&\n         rhs.getConstantValue().has_value()) {\n       if constexpr (std::is_same_v<OpTy, arith::AddIOp> ||\n-                    std::is_same_v<OpTy, triton::AddPtrOp>) {\n+                    std::is_same_v<OpTy, triton::AddPtrOp> ||\n+                    std::is_same_v<OpTy, LLVM::AddOp>) {\n         return {lhs.getConstantValue().value() +\n                 rhs.getConstantValue().value()};\n       } else if constexpr (std::is_same_v<OpTy, arith::SubIOp>) {\n@@ -334,14 +335,11 @@ class DivOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n     if (lhs.getConstantValue().has_value() &&\n         lhs.getConstantValue().value() == 0)\n       return lhs.getDivisibility(dim);\n-    // Case 2: rhs is constant\n-    if (rhs.getConstantValue().has_value()) {\n-      auto lhsDivisibility = lhs.getDivisibility(dim);\n-      auto rhsValue = rhs.getConstantValue().value();\n-      if (lhsDivisibility % rhsValue == 0)\n-        return lhsDivisibility / rhsValue;\n-    }\n-    // Case 3: both are not constant\n+    // Case 2: rhs is 1\n+    if (rhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().value() == 1)\n+      return lhs.getDivisibility(dim);\n+    // otherwise: return 1\n     return 1;\n   }\n \n@@ -815,11 +813,15 @@ AxisInfoAnalysis::AxisInfoAnalysis(DataFlowSolver &solver)\n                   CastOpAxisInfoVisitor<triton::gpu::ConvertLayoutOp>,\n                   CastOpAxisInfoVisitor<mlir::UnrealizedConversionCastOp>,\n                   CastOpAxisInfoVisitor<triton::BitcastOp>>();\n+  // TODO: Remove rules for LLVM::ConstantOp, LLVM::AddOp\n+  // when scf.for supports integers induction variable\n   visitors.append<MakeRangeOpAxisInfoVisitor>();\n-  visitors.append<ConstantOpAxisInfoVisitor>();\n+  visitors.append<ConstantOpAxisInfoVisitor<arith::ConstantOp>,\n+                  ConstantOpAxisInfoVisitor<LLVM::ConstantOp>>();\n   visitors.append<AddSubOpAxisInfoVisitor<triton::AddPtrOp>,\n                   AddSubOpAxisInfoVisitor<arith::AddIOp>,\n-                  AddSubOpAxisInfoVisitor<arith::SubIOp>>();\n+                  AddSubOpAxisInfoVisitor<arith::SubIOp>,\n+                  AddSubOpAxisInfoVisitor<LLVM::AddOp>>();\n   visitors.append<MulIOpAxisInfoVisitor>();\n   visitors.append<DivOpAxisInfoVisitor<arith::DivSIOp>,\n                   DivOpAxisInfoVisitor<arith::DivUIOp>>();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 9, "deletions": 13, "changes": 22, "file_content_changes": "@@ -137,11 +137,11 @@ struct ConvertLayoutOpConversion\n         multiDimOffset[1] = add(\n             multiDimOffset[1], i32_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n       } else if (mmaLayout.isVolta()) {\n-        auto [isARow, isBRow, isAVec4, isBVec4, mmaId] =\n+        auto [isARow, isBRow, isAVec4, isBVec4, _] =\n             mmaLayout.decodeVoltaLayoutStates();\n         auto coords = DotOpMmaV1ConversionHelper::getMNCoords(\n-            threadId, rewriter, mmaLayout.getWarpsPerCTA(), shape, isARow,\n-            isBRow, isAVec4, isBVec4);\n+            threadId, rewriter, mmaLayout.getWarpsPerCTA(), mmaLayout, shape,\n+            isARow, isBRow, isAVec4, isBVec4);\n         return DotOpMmaV1ConversionHelper::getCoord(elemId, coords);\n       } else {\n         llvm_unreachable(\"Unexpected MMALayout version\");\n@@ -234,9 +234,9 @@ struct ConvertLayoutOpConversion\n     }\n   }\n \n-  // The MMAV1's result is quite different from the exising \"Replica\" structure,\n-  // add a new simple but clear implementation for it to avoid modificating the\n-  // logic of the exising one.\n+  // The MMAV1's result is quite different from the existing \"Replica\"\n+  // structure, add a new simple but clear implementation for it to avoid\n+  // modifying the logic of the existing one.\n   void processReplicaForMMAV1(Location loc, ConversionPatternRewriter &rewriter,\n                               bool stNotRd, RankedTensorType type,\n                               ArrayRef<unsigned> multiDimRepId, unsigned vec,\n@@ -293,12 +293,9 @@ struct ConvertLayoutOpConversion\n       }\n \n       if (needTrans) {\n-        auto [isARow, isBRow, isAVec4, isBVec4, mmaId] =\n-            mma.decodeVoltaLayoutStates();\n-        DotOpMmaV1ConversionHelper helper(mma);\n         // do transpose\n-        int numM = helper.getElemsM(mma.getWarpsPerCTA()[0], shape[0], isARow,\n-                                    isAVec4);\n+        auto aEncoding = DotOperandEncodingAttr::get(mma.getContext(), 0, mma);\n+        int numM = aEncoding.getMMAv1NumOuter(shape);\n         int numN = accumSizePerThread / numM;\n \n         for (int r = 0; r < numM; r++) {\n@@ -638,8 +635,7 @@ struct ConvertLayoutOpConversion\n       }\n     } else if (!isOuter && mmaLayout.isVolta() && isHMMA) { // tensor core v1\n       DotOpMmaV1ConversionHelper helper(mmaLayout);\n-      bool isMMAv1Row =\n-          dotOperandLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+      bool isMMAv1Row = dotOperandLayout.getMMAv1IsRow();\n       auto srcSharedLayout = src.getType()\n                                  .cast<RankedTensorType>()\n                                  .getEncoding()"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.cpp", "status": "modified", "additions": 31, "deletions": 89, "changes": 120, "file_content_changes": "@@ -4,60 +4,6 @@\n namespace mlir {\n namespace LLVM {\n \n-int DotOpMmaV1ConversionHelper::numElemsPerThreadA(ArrayRef<int64_t> shape,\n-                                                   bool isARow, bool isAVec4,\n-                                                   int vec) const {\n-  int numM = getNumM(shape[0], isARow, isAVec4);\n-  int NK = shape[1];\n-  // Here we mimic the logic in loadA, the result cannot be calculated\n-  // directly.\n-  llvm::DenseSet<std::pair<int, int>> visited;\n-  auto ld = [&](int m, int k) {\n-    visited.insert({m, k});\n-    if (vec > 4) {\n-      if (isARow)\n-        visited.insert({m, k + 4});\n-      else\n-        visited.insert({m + 1, k});\n-    }\n-  };\n-\n-  for (unsigned k = 0; k < NK; k += 4)\n-    for (unsigned m = 0; m < numM / 2; ++m)\n-      if (!visited.count({m, k}))\n-        ld(m, k);\n-\n-  return visited.size() * 2;\n-}\n-\n-int DotOpMmaV1ConversionHelper::numElemsPerThreadB(ArrayRef<int64_t> shape,\n-                                                   bool isBRow, bool isBVec4,\n-                                                   int vec) const {\n-  unsigned numN = getNumN(shape[1], isBRow, isBVec4);\n-  int NK = shape[0];\n-  // Here we mimic the logic in loadA, the result cannot be calculated\n-  // directly.\n-  llvm::DenseSet<std::pair<int, int>> visited;\n-  int elemsPerLd = vec > 4 ? 4 : 2;\n-  auto ld = [&](int n, int k) {\n-    visited.insert({n, k});\n-    if (vec > 4) {\n-      if (isBRow)\n-        visited.insert({n + 1, k});\n-      else\n-        visited.insert({n, k + 4});\n-    }\n-  };\n-\n-  for (unsigned k = 0; k < NK; k += 4)\n-    for (unsigned n = 0; n < numN / 2; ++n) {\n-      if (!visited.count({n, k}))\n-        ld(n, k);\n-    }\n-\n-  return visited.size() * 2;\n-}\n-\n Value DotOpMmaV1ConversionHelper::loadA(\n     Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n     TritonGPUToLLVMTypeConverter *typeConverter,\n@@ -72,12 +18,12 @@ Value DotOpMmaV1ConversionHelper::loadA(\n   Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n \n   bool isARow = order[0] != 0;\n-  auto [isARow_, _0, isAVec4, _1, _2] = mmaLayout.decodeVoltaLayoutStates();\n-\n-  AParam param(isARow_, isAVec4);\n-\n+  auto resultEncoding = resultTy.cast<RankedTensorType>()\n+                            .getEncoding()\n+                            .cast<DotOperandEncodingAttr>();\n   auto [offsetAM, offsetAK, _3, _4] = computeOffsets(\n-      thread, isARow, false, fpw, param.spw, param.rep, rewriter, loc);\n+      thread, isARow, false, fpw, resultEncoding.getMMAv1ShapePerWarp(),\n+      resultEncoding.getMMAv1Rep(), rewriter, loc);\n \n   int vecA = sharedLayout.getVec();\n \n@@ -154,7 +100,9 @@ Value DotOpMmaV1ConversionHelper::loadA(\n     }\n   };\n \n-  unsigned numM = getNumM(shape[0], isARow, isAVec4);\n+  bool isARow_ = resultEncoding.getMMAv1IsRow();\n+  bool isAVec4 = resultEncoding.getMMAv1IsVec4();\n+  unsigned numM = resultEncoding.getMMAv1NumOuter(shape);\n   for (unsigned k = 0; k < NK; k += 4)\n     for (unsigned m = 0; m < numM / 2; ++m)\n       if (!has.count({m, k}))\n@@ -188,10 +136,9 @@ Value DotOpMmaV1ConversionHelper::loadB(\n   Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n   bool isBRow = order[0] != 0; // is row-major in shared memory layout\n   // isBRow_ indicates whether B is row-major in DotOperand layout\n-  auto [_0, isBRow_, _1, isBVec4, _2] = mmaLayout.decodeVoltaLayoutStates();\n-  assert(isBRow == isBRow_ && \"B need smem isRow\");\n-\n-  BParam param(isBRow_, isBVec4);\n+  auto resultEncoding = resultTy.cast<RankedTensorType>()\n+                            .getEncoding()\n+                            .cast<DotOperandEncodingAttr>();\n \n   int vecB = sharedLayout.getVec();\n   Value strideBN = isBRow ? i32_val(1) : strides[1];\n@@ -202,7 +149,8 @@ Value DotOpMmaV1ConversionHelper::loadB(\n   int strideRepK = 1;\n \n   auto [_3, _4, offsetBN, offsetBK] = computeOffsets(\n-      thread, false, isBRow, fpw, param.spw, param.rep, rewriter, loc);\n+      thread, false, isBRow, fpw, resultEncoding.getMMAv1ShapePerWarp(),\n+      resultEncoding.getMMAv1Rep(), rewriter, loc);\n \n   // swizzling\n   int perPhaseB = sharedLayout.getPerPhase();\n@@ -268,7 +216,10 @@ Value DotOpMmaV1ConversionHelper::loadB(\n     }\n   };\n \n-  unsigned numN = getNumN(shape[1], isBRow, isBVec4);\n+  bool isBRow_ = resultEncoding.getMMAv1IsRow();\n+  assert(isBRow == isBRow_ && \"B need smem isRow\");\n+  bool isBVec4 = resultEncoding.getMMAv1IsVec4();\n+  unsigned numN = resultEncoding.getMMAv1NumOuter(shape);\n   for (unsigned k = 0; k < NK; k += 4)\n     for (unsigned n = 0; n < numN / 2; ++n) {\n       if (!hbs.count({n, k}))\n@@ -365,10 +316,12 @@ DotOpMmaV1ConversionHelper::extractLoadedOperand(\n   return rcds;\n }\n \n+// TODO: Mostly a duplicate of TritonGPUToLLVMBase::emitBaseIndexforMMaLayoutV1\n SmallVector<DotOpMmaV1ConversionHelper::CoordTy>\n DotOpMmaV1ConversionHelper::getMNCoords(Value thread,\n                                         ConversionPatternRewriter &rewriter,\n                                         ArrayRef<unsigned int> wpt,\n+                                        const MmaEncodingAttr &mmaLayout,\n                                         ArrayRef<int64_t> shape, bool isARow,\n                                         bool isBRow, bool isAVec4,\n                                         bool isBVec4) {\n@@ -383,11 +336,17 @@ DotOpMmaV1ConversionHelper::getMNCoords(Value thread,\n   Value _fpw0 = i32_val(fpw[0]);\n   Value _fpw1 = i32_val(fpw[1]);\n \n-  DotOpMmaV1ConversionHelper::AParam aParam(isARow, isAVec4);\n-  DotOpMmaV1ConversionHelper::BParam bParam(isBRow, isBVec4);\n-\n-  SmallVector<int, 2> rep({aParam.rep[0], bParam.rep[1]});\n-  SmallVector<int, 2> spw({aParam.spw[0], bParam.spw[1]});\n+  // A info\n+  auto aEncoding = DotOperandEncodingAttr::get(ctx, 0, mmaLayout);\n+  auto aRep = aEncoding.getMMAv1Rep();\n+  auto aSpw = aEncoding.getMMAv1ShapePerWarp();\n+  // B info\n+  auto bEncoding = DotOperandEncodingAttr::get(ctx, 1, mmaLayout);\n+  auto bSpw = bEncoding.getMMAv1ShapePerWarp();\n+  auto bRep = bEncoding.getMMAv1Rep();\n+\n+  SmallVector<int, 2> rep({aRep[0], bRep[1]});\n+  SmallVector<int, 2> spw({aSpw[0], bSpw[1]});\n   SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n \n   Value lane = urem(thread, _32);\n@@ -463,23 +422,6 @@ DotOpMmaV1ConversionHelper::getMNCoords(Value thread,\n   return coords; // {M,N} in row-major\n }\n \n-void DotOpMmaV1ConversionHelper::AParam::build(bool isARow) {\n-  int packSize0 = (isARow || isAVec4) ? 1 : 2;\n-  int repM = 2 * packSize0;\n-  int repK = 1;\n-  int spwM = fpw[0] * 4 * repM;\n-  rep.assign({repM, 0, repK});\n-  spw.assign({spwM, 0, 1});\n-  vec = 2 * rep[0];\n-}\n-\n-void DotOpMmaV1ConversionHelper::BParam::build(bool isBRow) {\n-  int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n-  rep.assign({0, 2 * packSize1, 1});\n-  spw.assign({0, fpw[1] * 4 * rep[1], 1});\n-  vec = 2 * rep[1];\n-}\n-\n std::tuple<int, int>\n DotOpMmaV2ConversionHelper::getRepMN(const RankedTensorType &tensorTy) {\n   auto mmaLayout = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n@@ -1414,7 +1356,7 @@ int DotOpFMAConversionHelper::getNumElemsPerThread(\n   auto shapePerCTA = getShapePerCTA(blockedLayout);\n   auto sizePerThread = getSizePerThread(blockedLayout);\n \n-  // TODO[Superjomn]: we assume the k aixs is fixed for $a and $b here, fix it\n+  // TODO[Superjomn]: we assume the k axis is fixed for $a and $b here, fix it\n   // if not.\n   int K = dotOpLayout.getOpIdx() == 0 ? shape[1] : shape[0];\n   int otherDim = dotOpLayout.getOpIdx() == 1 ? shape[1] : shape[0];"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 4, "deletions": 73, "changes": 77, "file_content_changes": "@@ -46,45 +46,6 @@ struct DotOpMmaV1ConversionHelper {\n   explicit DotOpMmaV1ConversionHelper(MmaEncodingAttr mmaLayout)\n       : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()) {}\n \n-  // Help to share some variables across multiple functions for A.\n-  // TODO[Superjomn]: refactor and restrict this to only use in DotOp\n-  // conversion.\n-  struct AParam {\n-    SmallVector<int> rep;\n-    SmallVector<int> spw;\n-    bool isAVec4{};\n-    int vec{}; // This could only used in DotOp, not in\n-               // loadA/loadB/TypeConverter\n-\n-    AParam(bool isARow, bool isAVec4) : isAVec4(isAVec4) { build(isARow); }\n-\n-  private:\n-    void build(bool isARow);\n-  };\n-\n-  // Help to share some variables across multiple functions for A.\n-  // TODO[Superjomn]: refactor and restrict this to only use in DotOp\n-  // conversion.\n-  struct BParam {\n-    SmallVector<int> rep;\n-    SmallVector<int> spw;\n-    bool isBVec4{};\n-    int vec{}; // This could only used in DotOp, not in\n-               // loadA/loadB/TypeConverter\n-\n-    BParam(bool isBRow, bool isBVec4) : isBVec4(isBVec4) { build(isBRow); }\n-\n-  private:\n-    void build(bool isBRow);\n-  };\n-\n-  int getRepM(int M) const {\n-    return std::max<int>(M / (wpt[0] * instrShape[0]), 1);\n-  }\n-  int getRepN(int N) const {\n-    return std::max<int>(N / (wpt[1] * instrShape[1]), 1);\n-  }\n-\n   static ArrayRef<unsigned> getMmaInstrShape() { return instrShape; }\n \n   static Type getMmaRetType(TensorType operand) {\n@@ -101,28 +62,6 @@ struct DotOpMmaV1ConversionHelper {\n     return struct_ty(SmallVector<Type>{vecTy});\n   }\n \n-  // Get the number of fp16x2 elements for $a.\n-  unsigned getNumM(int M, bool isARow, bool isAVec4) const {\n-    AParam param(isARow, isAVec4);\n-\n-    unsigned numM = param.rep[0] * M / (param.spw[0] * wpt[0]);\n-    return numM;\n-  }\n-\n-  // Get the number of fp16x2 elements for $b.\n-  unsigned getNumN(int N, bool isBRow, bool isBVec4) const {\n-    BParam param(isBRow, isBVec4);\n-\n-    unsigned numN = param.rep[1] * N / (param.spw[1] * wpt[1]);\n-    return numN;\n-  }\n-\n-  int numElemsPerThreadA(ArrayRef<int64_t> shape, bool isARow, bool isAVec4,\n-                         int vec) const;\n-\n-  int numElemsPerThreadB(ArrayRef<int64_t> shape, bool isBRow, bool isBVec4,\n-                         int vec) const;\n-\n   // Loading $a from smem to registers, returns a LLVM::Struct.\n   Value loadA(Value tensor, const SharedMemoryObject &smemObj, Value thread,\n               Location loc, TritonGPUToLLVMTypeConverter *converter,\n@@ -152,21 +91,13 @@ struct DotOpMmaV1ConversionHelper {\n       Value llStruct, int NK, ConversionPatternRewriter &rewriter,\n       TritonGPUToLLVMTypeConverter *typeConverter, Type type) const;\n \n-  // Get the number of elements of this thread in M axis. The N axis could be\n-  // further deduced with the accSize / elemsM. \\param wpt: the wpt in M axis\n-  // \\param M: the shape in M axis\n-  int getElemsM(int wpt, int M, bool isARow, bool isAVec4) {\n-    DotOpMmaV1ConversionHelper::AParam param(isARow, isAVec4);\n-    int shapePerCTAM = param.spw[0] * wpt;\n-    return M / shapePerCTAM * param.rep[0];\n-  }\n-\n   using CoordTy = SmallVector<Value>;\n   // Get the coordinates(m,n) of the elements emit by a thread in accumulator.\n   static SmallVector<CoordTy>\n   getMNCoords(Value thread, ConversionPatternRewriter &rewriter,\n-              ArrayRef<unsigned> wpt, ArrayRef<int64_t> shape, bool isARow,\n-              bool isBRow, bool isAVec4, bool isBVec4);\n+              ArrayRef<unsigned> wpt, const MmaEncodingAttr &mmaLayout,\n+              ArrayRef<int64_t> shape, bool isARow, bool isBRow, bool isAVec4,\n+              bool isBVec4);\n \n   // \\param elemId the offset of the element in a thread\n   static CoordTy getCoord(int elemId, ArrayRef<CoordTy> coords) {\n@@ -401,7 +332,7 @@ class MMA16816SmemLoader {\n };\n \n // This class helps to adapt the existing DotOpConversion to the latest\n-// DotOpOperand layout design. It decouples the exising implementation to two\n+// DotOpOperand layout design. It decouples the existing implementation to two\n // parts:\n // 1. loading the specific operand matrix(for $a, $b, $c) from smem\n // 2. passing the loaded value and perform the mma codegen"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -114,17 +114,17 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     auto AShape = ATensorTy.getShape();\n     auto BShape = BTensorTy.getShape();\n \n-    bool isARow = ALayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    bool isBRow = BLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    auto [isARow_, isBRow_, isAVec4_, isBVec4_, mmaId] =\n+    bool isARow = ALayout.getMMAv1IsRow();\n+    bool isBRow = BLayout.getMMAv1IsRow();\n+    auto [isARow_, isBRow_, isAVec4_, isBVec4_, _] =\n         mmaLayout.decodeVoltaLayoutStates();\n     assert(isARow == isARow_);\n     assert(isBRow == isBRow_);\n \n     DotOpMmaV1ConversionHelper helper(mmaLayout);\n \n-    unsigned numM = helper.getNumM(AShape[0], isARow, isAVec4_);\n-    unsigned numN = helper.getNumN(BShape[1], isBRow, isBVec4_);\n+    unsigned numM = ALayout.getMMAv1NumOuter(AShape);\n+    unsigned numN = BLayout.getMMAv1NumOuter(BShape);\n     unsigned NK = AShape[1];\n \n     auto has = helper.extractLoadedOperand(adaptor.getA(), NK, rewriter,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 278, "deletions": 150, "changes": 428, "file_content_changes": "@@ -9,10 +9,14 @@ struct FpToFpOpConversion\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::FpToFpOp>::ConvertTritonGPUOpToLLVMPattern;\n \n+  /* ------------------ */\n+  // FP8 -> FP16\n+  /* ------------------ */\n+\n   static SmallVector<Value>\n   convertFp8x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n+                       const char *ptxAsm, const Value &v0, const Value &v1,\n+                       const Value &v2, const Value &v3) {\n     auto ctx = rewriter.getContext();\n     auto fp8x4VecTy = vec_ty(i8_ty, 4);\n     Value fp8x4Vec = undef(fp8x4VecTy);\n@@ -23,17 +27,6 @@ struct FpToFpOpConversion\n     fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n \n     PTXBuilder builder;\n-    auto *ptxAsm = \"{                                      \\n\"\n-                   \".reg .b32 a<2>, b<2>;                  \\n\"\n-                   \"prmt.b32 a0, 0, $2, 0x5040;            \\n\"\n-                   \"prmt.b32 a1, 0, $2, 0x7060;            \\n\"\n-                   \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"shr.b32  b0, b0, 1;                    \\n\"\n-                   \"shr.b32  b1, b1, 1;                    \\n\"\n-                   \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\"\n-                   \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\"\n-                   \"}\";\n     auto &ptxOp = *builder.create(ptxAsm);\n \n     auto *o0 = builder.newOperand(\"=r\");\n@@ -55,51 +48,41 @@ struct FpToFpOpConversion\n   }\n \n   static SmallVector<Value>\n-  convertFp16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n-    Value fp16x2Vec0 = undef(fp16x2VecTy);\n-    Value fp16x2Vec1 = undef(fp16x2VecTy);\n-    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v0, i32_val(0));\n-    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v1, i32_val(1));\n-    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v2, i32_val(0));\n-    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v3, i32_val(1));\n-    fp16x2Vec0 = bitcast(fp16x2Vec0, i32_ty);\n-    fp16x2Vec1 = bitcast(fp16x2Vec1, i32_ty);\n-\n-    PTXBuilder builder;\n+  convertFp8E4M3x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n     auto *ptxAsm = \"{                                      \\n\"\n                    \".reg .b32 a<2>, b<2>;                  \\n\"\n-                   \"shl.b32 a0, $1, 1;                     \\n\"\n-                   \"shl.b32 a1, $2, 1;                     \\n\"\n-                   \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"add.u32 a0, a0, 0x00800080;            \\n\"\n-                   \"add.u32 a1, a1, 0x00800080;            \\n\"\n-                   \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n-                   \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n-                   \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n+                   \"prmt.b32 a0, 0, $2, 0x5040;            \\n\"\n+                   \"prmt.b32 a1, 0, $2, 0x7060;            \\n\"\n+                   \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n+                   \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n+                   \"shr.b32  b0, b0, 1;                    \\n\"\n+                   \"shr.b32  b1, b1, 1;                    \\n\"\n+                   \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\"\n+                   \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\"\n                    \"}\";\n-    auto &ptxOp = *builder.create(ptxAsm);\n-\n-    auto *o = builder.newOperand(\"=r\");\n-    auto *i0 = builder.newOperand(fp16x2Vec0, \"r\");\n-    auto *i1 = builder.newOperand(fp16x2Vec1, \"r\");\n-    ptxOp({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n+    return convertFp8x4ToFp16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n \n-    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n-    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n+  static SmallVector<Value>\n+  convertFp8E5M2x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = \"{                           \\n\"\n+                   \"prmt.b32 $0, 0, $2, 0x5140; \\n\\t\"\n+                   \"prmt.b32 $1, 0, $2, 0x7362; \\n\\t\"\n+                   \"}\";\n+    return convertFp8x4ToFp16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n   }\n \n+  /* ------------------ */\n+  // FP8 -> BF16\n+  /* ------------------ */\n   static SmallVector<Value>\n   convertFp8x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n+                       const char *ptxAsm, const Value &v0, const Value &v1,\n+                       const Value &v2, const Value &v3) {\n     auto ctx = rewriter.getContext();\n     auto fp8x4VecTy = vec_ty(i8_ty, 4);\n     Value fp8x4Vec = undef(fp8x4VecTy);\n@@ -110,21 +93,6 @@ struct FpToFpOpConversion\n     fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n \n     PTXBuilder builder;\n-    auto *ptxAsm = \"{                                          \\n\"\n-                   \".reg .b32 a<2>, sign<2>, nosign<2>, b<2>;  \\n\"\n-                   \"prmt.b32 a0, 0, $2, 0x5040;                \\n\"\n-                   \"prmt.b32 a1, 0, $2, 0x7060;                \\n\"\n-                   \"and.b32 sign0, a0, 0x80008000;             \\n\"\n-                   \"and.b32 sign1, a1, 0x80008000;             \\n\"\n-                   \"and.b32 nosign0, a0, 0x7fff7fff;           \\n\"\n-                   \"and.b32 nosign1, a1, 0x7fff7fff;           \\n\"\n-                   \"shr.b32 nosign0, nosign0, 4;               \\n\"\n-                   \"shr.b32 nosign1, nosign1, 4;               \\n\"\n-                   \"add.u32 nosign0, nosign0, 0x38003800;      \\n\"\n-                   \"add.u32 nosign1, nosign1, 0x38003800;      \\n\"\n-                   \"or.b32 $0, sign0, nosign0;                 \\n\"\n-                   \"or.b32 $1, sign1, nosign1;                 \\n\"\n-                   \"}\";\n     auto &ptxOp = *builder.create(ptxAsm);\n \n     auto *o0 = builder.newOperand(\"=r\");\n@@ -145,10 +113,145 @@ struct FpToFpOpConversion\n             extract_element(i16_ty, bf16x2Vec1, i32_val(1))};\n   }\n \n+  static SmallVector<Value>\n+  convertFp8E4M3x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = \"{                                          \\n\"\n+                   \".reg .b32 a<2>, sign<2>, nosign<2>, b<2>;  \\n\"\n+                   \"prmt.b32 a0, 0, $2, 0x5040;                \\n\"\n+                   \"prmt.b32 a1, 0, $2, 0x7060;                \\n\"\n+                   \"and.b32 sign0, a0, 0x80008000;             \\n\"\n+                   \"and.b32 sign1, a1, 0x80008000;             \\n\"\n+                   \"and.b32 nosign0, a0, 0x7fff7fff;           \\n\"\n+                   \"and.b32 nosign1, a1, 0x7fff7fff;           \\n\"\n+                   \"shr.b32 nosign0, nosign0, 4;               \\n\"\n+                   \"shr.b32 nosign1, nosign1, 4;               \\n\"\n+                   \"add.u32 nosign0, nosign0, 0x38003800;      \\n\"\n+                   \"add.u32 nosign1, nosign1, 0x38003800;      \\n\"\n+                   \"or.b32 $0, sign0, nosign0;                 \\n\"\n+                   \"or.b32 $1, sign1, nosign1;                 \\n\"\n+                   \"}\";\n+    return convertFp8x4ToBf16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  };\n+\n+  static SmallVector<Value>\n+  convertFp8E5M2x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = \"{                                      \\n\"\n+                   \".reg .b32 a<2>, b<2>;                  \\n\"\n+                   \"prmt.b32 a0, 0, $2, 0x5140;            \\n\"\n+                   \"prmt.b32 a1, 0, $2, 0x7362;            \\n\"\n+                   \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n+                   \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n+                   \"shr.b32  b0, b0, 3;                    \\n\"\n+                   \"shr.b32  b1, b1, 3;                    \\n\"\n+                   \"add.u32  b0, b0, 0x30003000;           \\n\"\n+                   \"add.u32  b1, b1, 0x30003000;           \\n\"\n+                   \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\"\n+                   \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\"\n+                   \"}\";\n+    return convertFp8x4ToBf16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  };\n+\n+  /* ------------------ */\n+  // FP16 -> FP8\n+  /* ------------------ */\n+\n+  static SmallVector<Value>\n+  convertFp16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const char *ptxAsm, const Value &v0, const Value &v1,\n+                       const Value &v2, const Value &v3) {\n+    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n+    Value fp16x2Vec0 = undef(fp16x2VecTy);\n+    Value fp16x2Vec1 = undef(fp16x2VecTy);\n+    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v0, i32_val(0));\n+    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v1, i32_val(1));\n+    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v2, i32_val(0));\n+    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v3, i32_val(1));\n+    fp16x2Vec0 = bitcast(fp16x2Vec0, i32_ty);\n+    fp16x2Vec1 = bitcast(fp16x2Vec1, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto &ptxOp = *builder.create(ptxAsm);\n+\n+    auto *o = builder.newOperand(\"=r\");\n+    auto *i0 = builder.newOperand(fp16x2Vec0, \"r\");\n+    auto *i1 = builder.newOperand(fp16x2Vec1, \"r\");\n+    ptxOp({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n+\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n+    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp16x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = \"{                                      \\n\"\n+                   \".reg .b32 a<2>, b<2>;                  \\n\"\n+                   \"shl.b32 a0, $1, 1;                     \\n\"\n+                   \"shl.b32 a1, $2, 1;                     \\n\"\n+                   \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n+                   \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n+                   \"add.u32 a0, a0, 0x00800080;            \\n\"\n+                   \"add.u32 a1, a1, 0x00800080;            \\n\"\n+                   \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n+                   \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n+                   \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n+                   \"}\";\n+    return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = \"{                                      \\n\"\n+                   \"prmt.b32 $0, $1, $2, 0x7531; \\n\\t\"\n+                   \"}\";\n+    return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n+\n+  /* ------------------ */\n+  // FP32 -> FP8\n+  /* ------------------ */\n+\n+  static SmallVector<Value>\n+  convertFp32x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8E4M3x4(loc, rewriter, c0, c1, c2, c3);\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp32x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8E5M2x4(loc, rewriter, c0, c1, c2, c3);\n+  }\n+\n+  /* ------------------ */\n+  // BF16 -> FP8\n+  /* ------------------ */\n+\n   static SmallVector<Value>\n   convertBf16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n+                       const char *ptxAsm, const Value &v0, const Value &v1,\n+                       const Value &v2, const Value &v3) {\n     auto bf16x2VecTy = vec_ty(i16_ty, 2);\n     Value bf16x2Vec0 = undef(bf16x2VecTy);\n     Value bf16x2Vec1 = undef(bf16x2VecTy);\n@@ -160,6 +263,25 @@ struct FpToFpOpConversion\n     bf16x2Vec1 = bitcast(bf16x2Vec1, i32_ty);\n \n     PTXBuilder builder;\n+    auto &ptxOp = *builder.create(ptxAsm);\n+\n+    auto *o = builder.newOperand(\"=r\");\n+    auto *i0 = builder.newOperand(bf16x2Vec0, \"r\");\n+    auto *i1 = builder.newOperand(bf16x2Vec1, \"r\");\n+    ptxOp({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n+\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n+    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertBf16x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n     auto *ptxAsm = \"{                                            \\n\"\n                    \".reg .u32 sign, sign<2>, nosign, nosign<2>;  \\n\"\n                    \".reg .u32 fp8_min, fp8_max, rn_, zero;       \\n\"\n@@ -196,63 +318,64 @@ struct FpToFpOpConversion\n                    \"prmt.b32 nosign, nosign0, nosign1, 0x6420;   \\n\"\n                    \"or.b32 $0, nosign, sign;                     \\n\"\n                    \"}\";\n-    auto &ptxOp = *builder.create(ptxAsm);\n+    return convertBf16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  };\n \n-    auto *o = builder.newOperand(\"=r\");\n-    auto *i0 = builder.newOperand(bf16x2Vec0, \"r\");\n-    auto *i1 = builder.newOperand(bf16x2Vec1, \"r\");\n-    ptxOp({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n+  // TODO:\n+  // static SmallVector<Value>\n+  // convertBf16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n+  //                          const Value &v0, const Value &v1, const Value &v2,\n+  //                          const Value &v3) {\n+  // }\n \n-    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n-    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n+  /* ------------------ */\n+  // FP8 -> FP32\n+  /* ------------------ */\n+\n+  static SmallVector<Value>\n+  convertFp8E4M3x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto fp16Values = convertFp8E4M3x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+    return {convertFp16ToFp32(loc, rewriter, fp16Values[0]),\n+            convertFp16ToFp32(loc, rewriter, fp16Values[1]),\n+            convertFp16ToFp32(loc, rewriter, fp16Values[2]),\n+            convertFp16ToFp32(loc, rewriter, fp16Values[3])};\n   }\n \n   static SmallVector<Value>\n-  convertFp8x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto fp16Values = convertFp8x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+  convertFp8E5M2x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto fp16Values = convertFp8E5M2x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n     return {rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[0]),\n             rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[1]),\n             rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[2]),\n             rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[3])};\n   }\n \n-  static SmallVector<Value>\n-  convertFp32x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n-    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n-    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n-    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n-    return convertFp16x4ToFp8x4(loc, rewriter, c0, c1, c2, c3);\n-  }\n+  //\n \n   static SmallVector<Value>\n-  convertFp8x4ToFp64x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto fp16Values = convertFp8x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+  convertFp8E4M3x4ToFp64x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto fp16Values = convertFp8E4M3x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n     return {rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[0]),\n             rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[1]),\n             rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[2]),\n             rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[3])};\n   }\n \n   static SmallVector<Value>\n-  convertFp64x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n+  convertFp64x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n     auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n     auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n     auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n     auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n-    return convertFp16x4ToFp8x4(loc, rewriter, c0, c1, c2, c3);\n+    return convertFp16x4ToFp8E4M3x4(loc, rewriter, c0, c1, c2, c3);\n   }\n \n   static Value convertBf16ToFp32(Location loc,\n@@ -312,58 +435,63 @@ struct FpToFpOpConversion\n     auto loc = op->getLoc();\n     auto elems = getElemsPerThread(dstTensorType);\n     SmallVector<Value> resultVals;\n+    bool isSrcFP8 =\n+        srcEltType.isa<mlir::Float8E4M3FNType, mlir::Float8E5M2Type>();\n+    bool isDstFP8 =\n+        dstEltType.isa<mlir::Float8E4M3FNType, mlir::Float8E5M2Type>();\n \n     // Select convertor\n-    if (srcEltType.isa<triton::Float8Type>() ||\n-        dstEltType.isa<triton::Float8Type>()) {\n-      std::function<SmallVector<Value>(Location, ConversionPatternRewriter &,\n-                                       const Value &, const Value &,\n-                                       const Value &, const Value &)>\n-          convertor;\n-      if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF16()) {\n-        convertor = convertFp8x4ToFp16x4;\n-      } else if (srcEltType.isF16() && dstEltType.isa<triton::Float8Type>()) {\n-        convertor = convertFp16x4ToFp8x4;\n-      } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isBF16()) {\n-        convertor = convertFp8x4ToBf16x4;\n-      } else if (srcEltType.isBF16() && dstEltType.isa<triton::Float8Type>()) {\n-        convertor = convertBf16x4ToFp8x4;\n-      } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF32()) {\n-        convertor = convertFp8x4ToFp32x4;\n-      } else if (srcEltType.isF32() && dstEltType.isa<triton::Float8Type>()) {\n-        convertor = convertFp32x4ToFp8x4;\n-      } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF64()) {\n-        convertor = convertFp8x4ToFp64x4;\n-      } else if (srcEltType.isF64() && dstEltType.isa<triton::Float8Type>()) {\n-        convertor = convertFp64x4ToFp8x4;\n-      } else {\n-        assert(false && \"unsupported fp8 casting\");\n-      }\n-\n-      // Vectorized casting\n-      assert(elems % 4 == 0 &&\n-             \"FP8 casting only support tensors with 4-aligned sizes\");\n-      auto elements = getTypeConverter()->unpackLLElements(\n-          loc, adaptor.getFrom(), rewriter, srcTensorType);\n-      for (size_t i = 0; i < elems; i += 4) {\n-        auto converted = convertor(loc, rewriter, elements[i], elements[i + 1],\n-                                   elements[i + 2], elements[i + 3]);\n-        resultVals.append(converted);\n-      }\n-    } else if (srcEltType.isBF16() && dstEltType.isF32()) {\n-      resultVals.emplace_back(\n-          convertBf16ToFp32(loc, rewriter, adaptor.getFrom()));\n-    } else if (srcEltType.isF32() && dstEltType.isBF16()) {\n-      resultVals.emplace_back(\n-          convertFp32ToBf16(loc, rewriter, adaptor.getFrom()));\n-    } else if (srcEltType.isF16() && dstEltType.isF32()) {\n-      resultVals.emplace_back(\n-          convertFp16ToFp32(loc, rewriter, adaptor.getFrom()));\n-    } else if (srcEltType.isF32() && dstEltType.isF16()) {\n-      resultVals.emplace_back(\n-          convertFp32ToFp16(loc, rewriter, adaptor.getFrom()));\n-    } else {\n-      assert(false && \"unsupported type casting\");\n+    typedef std::function<SmallVector<Value>(\n+        Location, ConversionPatternRewriter &, const Value &, const Value &,\n+        const Value &, const Value &)>\n+        ConvertorT;\n+\n+    auto F8E4M3TyID = TypeID::get<mlir::Float8E4M3FNType>();\n+    auto F8E5M2TyID = TypeID::get<mlir::Float8E5M2Type>();\n+    auto F16TyID = TypeID::get<mlir::Float16Type>();\n+    auto BF16TyID = TypeID::get<mlir::BFloat16Type>();\n+    auto F32TyID = TypeID::get<mlir::Float32Type>();\n+    auto F64TyID = TypeID::get<mlir::Float64Type>();\n+    DenseMap<std::pair<TypeID, TypeID>, ConvertorT> convertorMap = {\n+        // F8 -> F16\n+        {{F8E4M3TyID, F16TyID}, convertFp8E4M3x4ToFp16x4},\n+        {{F8E5M2TyID, F16TyID}, convertFp8E5M2x4ToFp16x4},\n+        // F16 -> F8\n+        {{F16TyID, F8E4M3TyID}, convertFp16x4ToFp8E4M3x4},\n+        {{F16TyID, F8E5M2TyID}, convertFp16x4ToFp8E5M2x4},\n+        // F8 -> BF16\n+        {{F8E4M3TyID, BF16TyID}, convertFp8E4M3x4ToBf16x4},\n+        {{F8E5M2TyID, BF16TyID}, convertFp8E5M2x4ToBf16x4},\n+        // BF16 -> F8\n+        {{BF16TyID, F8E4M3TyID}, convertBf16x4ToFp8E4M3x4},\n+        // TODO:\n+        // {{BF16TyID, F8E5M2TyID}, convertBf16x4ToFp8E5M2x4},\n+        // F8 -> F32\n+        {{F8E4M3TyID, F32TyID}, convertFp8E4M3x4ToFp32x4},\n+        {{F8E5M2TyID, F32TyID}, convertFp8E5M2x4ToFp32x4},\n+        // F32 -> F8\n+        {{F32TyID, F8E4M3TyID}, convertFp32x4ToFp8E4M3x4},\n+        {{F32TyID, F8E5M2TyID}, convertFp32x4ToFp8E5M2x4},\n+    };\n+\n+    std::pair<TypeID, TypeID> key = {srcEltType.getTypeID(),\n+                                     dstEltType.getTypeID()};\n+    if (convertorMap.count(key) == 0) {\n+      llvm::errs() << \"Unsupported conversion from \" << srcEltType << \" to \"\n+                   << dstEltType << \"\\n\";\n+      llvm_unreachable(\"\");\n+    }\n+    auto convertor = convertorMap.lookup(key);\n+\n+    // Vectorized casting\n+    assert(elems % 4 == 0 &&\n+           \"FP8 casting only support tensors with 4-aligned sizes\");\n+    auto elements = getTypeConverter()->unpackLLElements(\n+        loc, adaptor.getFrom(), rewriter, srcTensorType);\n+    for (size_t i = 0; i < elems; i += 4) {\n+      auto converted = convertor(loc, rewriter, elements[i], elements[i + 1],\n+                                 elements[i + 2], elements[i + 3]);\n+      resultVals.append(converted);\n     }\n \n     assert(resultVals.size() == elems);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -434,7 +434,7 @@ struct ReduceOpConversion\n \n     // We could avoid this barrier in some of the layouts, however this is not\n     // the general case.\n-    // TODO: optimize the barrier incase the layouts are accepted.\n+    // TODO: optimize the barrier in case the layouts are accepted.\n     barrier();\n \n     // set output values"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 33, "deletions": 15, "changes": 48, "file_content_changes": "@@ -19,6 +19,7 @@ using namespace mlir::triton;\n \n using ::mlir::LLVM::SharedMemoryObject;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;\n \n@@ -298,7 +299,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       Value idxRow = idx[outOrder[1]]; // discontiguous dimension\n       Value strideCol = srcStrides[outOrder[0]];\n       Value strideRow = srcStrides[outOrder[1]];\n-      // extract dynamic/static offset for immediate offseting\n+      // extract dynamic/static offset for immediate offsetting\n       unsigned immedateOffCol = 0;\n       if (auto add = dyn_cast_or_null<LLVM::AddOp>(idxCol.getDefiningOp()))\n         if (auto _cst = dyn_cast_or_null<LLVM::ConstantOp>(\n@@ -310,7 +311,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n           idxCol = cacheCol[key];\n           immedateOffCol = cst / (outVec * maxPhase) * (outVec * maxPhase);\n         }\n-      // extract dynamic/static offset for immediate offseting\n+      // extract dynamic/static offset for immediate offsetting\n       unsigned immedateOffRow = 0;\n       if (auto add = dyn_cast_or_null<LLVM::AddOp>(idxRow.getDefiningOp()))\n         if (auto _cst = dyn_cast_or_null<LLVM::ConstantOp>(\n@@ -362,7 +363,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto srcDistributedLayout = srcTy.getEncoding();\n     if (auto mmaLayout = srcDistributedLayout.dyn_cast<MmaEncodingAttr>()) {\n       assert((!mmaLayout.isVolta()) &&\n-             \"ConvertLayout MMAv1->Shared is not suppported yet\");\n+             \"ConvertLayout MMAv1->Shared is not supported yet\");\n     }\n     auto dstSharedLayout =\n         dstTy.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n@@ -617,7 +618,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n     SmallVector<Value> multiDimBase(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n-      // Wrap around multiDimWarpId/multiDimThreadId incase\n+      // Wrap around multiDimWarpId/multiDimThreadId in case\n       // shape[k] > shapePerCTA[k]\n       auto maxWarps =\n           ceil<unsigned>(shape[k], sizePerThread[k] * threadsPerWarp[k]);\n@@ -701,7 +702,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n     auto wpt = mmaLayout.getWarpsPerCTA();\n     auto fpw = LLVM::DotOpMmaV1ConversionHelper::fpw;\n-    auto [isARow, isBRow, isAVec4, isBVec4, id] =\n+    auto [isARow, isBRow, isAVec4, isBVec4, _] =\n         mmaLayout.decodeVoltaLayoutStates();\n \n     Value thread = getThreadId(rewriter, loc);\n@@ -714,11 +715,17 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     Value _fpw0 = i32_val(fpw[0]);\n     Value _fpw1 = i32_val(fpw[1]);\n \n-    LLVM::DotOpMmaV1ConversionHelper::AParam aParam(isARow, isAVec4);\n-    LLVM::DotOpMmaV1ConversionHelper::BParam bParam(isBRow, isBVec4);\n-\n-    SmallVector<int, 2> rep({aParam.rep[0], bParam.rep[1]});\n-    SmallVector<int, 2> spw({aParam.spw[0], bParam.spw[1]});\n+    // A info\n+    auto aEncoding = DotOperandEncodingAttr::get(ctx, 0, mmaLayout);\n+    auto aRep = aEncoding.getMMAv1Rep();\n+    auto aSpw = aEncoding.getMMAv1ShapePerWarp();\n+    // B info\n+    auto bEncoding = DotOperandEncodingAttr::get(ctx, 1, mmaLayout);\n+    auto bSpw = bEncoding.getMMAv1ShapePerWarp();\n+    auto bRep = bEncoding.getMMAv1Rep();\n+\n+    SmallVector<int, 2> rep({aRep[0], bRep[1]});\n+    SmallVector<int, 2> spw({aSpw[0], bSpw[1]});\n     SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n \n     Value lane = urem(thread, _32);\n@@ -764,14 +771,25 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                            RankedTensorType type) const {\n     auto shape = type.getShape();\n \n-    auto [isARow, isBRow, isAVec4, isBVec4, id] =\n+    auto [isARow, isBRow, isAVec4, isBVec4, _] =\n         mmaLayout.decodeVoltaLayoutStates();\n-    LLVM::DotOpMmaV1ConversionHelper::AParam aParam(isARow, isAVec4);\n-    LLVM::DotOpMmaV1ConversionHelper::BParam bParam(isBRow, isBVec4);\n+\n+    // TODO: seems like the apttern below to get `rep`/`spw` appears quite often\n+    // A info\n+    auto aEncoding =\n+        DotOperandEncodingAttr::get(type.getContext(), 0, mmaLayout);\n+    auto aRep = aEncoding.getMMAv1Rep();\n+    auto aSpw = aEncoding.getMMAv1ShapePerWarp();\n+    // B info\n+    auto bEncoding =\n+        DotOperandEncodingAttr::get(type.getContext(), 1, mmaLayout);\n+    auto bSpw = bEncoding.getMMAv1ShapePerWarp();\n+    auto bRep = bEncoding.getMMAv1Rep();\n+\n     auto wpt = mmaLayout.getWarpsPerCTA();\n     auto fpw = LLVM::DotOpMmaV1ConversionHelper::fpw;\n-    SmallVector<int, 2> rep({aParam.rep[0], bParam.rep[1]});\n-    SmallVector<int, 2> spw({aParam.spw[0], bParam.spw[1]});\n+    SmallVector<int, 2> rep({aRep[0], bRep[1]});\n+    SmallVector<int, 2> spw({aSpw[0], bSpw[1]});\n     SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n \n     SmallVector<unsigned> idxM;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "modified", "additions": 9, "deletions": 28, "changes": 37, "file_content_changes": "@@ -28,7 +28,10 @@ TritonGPUToLLVMTypeConverter::TritonGPUToLLVMTypeConverter(\n     return convertTritonTensorType(type);\n   });\n   // Internally store float8 as int8\n-  addConversion([&](triton::Float8Type type) -> llvm::Optional<Type> {\n+  addConversion([&](mlir::Float8E4M3FNType type) -> llvm::Optional<Type> {\n+    return IntegerType::get(type.getContext(), 8);\n+  });\n+  addConversion([&](mlir::Float8E5M2Type type) -> llvm::Optional<Type> {\n     return IntegerType::get(type.getContext(), 8);\n   });\n   // Internally store bfloat16 as int16\n@@ -140,36 +143,14 @@ TritonGPUToLLVMTypeConverter::convertTritonTensorType(RankedTensorType type) {\n         } else {\n           assert(false && \"Unsupported element type\");\n         }\n-        if (dotOpLayout.getOpIdx() == 0) { // $a\n-          auto elems =\n-              MMA16816ConversionHelper::getANumElemsPerThread(type, wpt[0]);\n-          return struct_ty(SmallVector<Type>(elems, targetTy));\n-        }\n-        if (dotOpLayout.getOpIdx() == 1) { // $b\n-          auto elems =\n-              MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt[1]);\n-          return struct_ty(SmallVector<Type>(elems, targetTy));\n-        }\n+        auto elems = getElemsPerThread(type);\n+        return struct_ty(SmallVector<Type>(elems, targetTy));\n       }\n \n       if (mmaLayout.isVolta()) {\n-        auto [isARow, isBRow, isAVec4, isBVec4, mmaId] =\n-            mmaLayout.decodeVoltaLayoutStates();\n-        DotOpMmaV1ConversionHelper helper(mmaLayout);\n-        if (dotOpLayout.getOpIdx() == 0) { // $a\n-          DotOpMmaV1ConversionHelper::AParam param(isARow, isAVec4);\n-          int elems =\n-              helper.numElemsPerThreadA(shape, isARow, isAVec4, param.vec);\n-          Type x2Ty = vec_ty(elemTy, 2);\n-          return struct_ty(SmallVector<Type>(elems, x2Ty));\n-        }\n-        if (dotOpLayout.getOpIdx() == 1) { // $b\n-          DotOpMmaV1ConversionHelper::BParam param(isBRow, isBVec4);\n-          int elems =\n-              helper.numElemsPerThreadB(shape, isBRow, isBVec4, param.vec);\n-          Type x2Ty = vec_ty(elemTy, 2);\n-          return struct_ty(SmallVector<Type>(elems, x2Ty));\n-        }\n+        int elems = getElemsPerThread(type);\n+        Type x2Ty = vec_ty(elemTy, 2);\n+        return struct_ty(SmallVector<Type>(elems, x2Ty));\n       }\n     }\n "}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 30, "deletions": 4, "changes": 34, "file_content_changes": "@@ -3,6 +3,7 @@\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n #include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n+#include \"mlir/Dialect/Index/IR/IndexDialect.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n #include \"mlir/Pass/Pass.h\"\n@@ -685,9 +686,12 @@ class CFBranchPattern : public OpConversionPattern<cf::BranchOp> {\n   LogicalResult\n   matchAndRewrite(cf::BranchOp op, cf::BranchOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    addNamedAttrs(rewriter.replaceOpWithNewOp<cf::BranchOp>(\n-                      op, op.getSuccessor(), adaptor.getOperands()),\n-                  adaptor.getAttributes());\n+    auto converter = getTypeConverter();\n+    auto newOp = rewriter.replaceOpWithNewOp<cf::BranchOp>(\n+        op, op.getSuccessor(), adaptor.getOperands());\n+    if (failed(rewriter.convertRegionTypes(newOp.getSuccessor()->getParent(),\n+                                           *converter)))\n+      return failure();\n     return success();\n   }\n };\n@@ -712,14 +716,36 @@ class CFCondBranchPattern : public OpConversionPattern<cf::CondBranchOp> {\n     if (failed(rewriter.convertRegionTypes(newOp.getFalseDest()->getParent(),\n                                            *converter)))\n       return failure();\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+};\n+\n+class FuncOpPattern : public OpConversionPattern<func::FuncOp> {\n+public:\n+  using OpConversionPattern<func::FuncOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(func::FuncOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto converter = getTypeConverter();\n+    auto newOp = rewriter.replaceOpWithNewOp<func::FuncOp>(\n+        op, op.getName(), op.getFunctionType());\n+    addNamedAttrs(newOp, adaptor.getAttributes());\n+    rewriter.inlineRegionBefore(op.getBody(), newOp.getBody(),\n+                                newOp.getBody().end());\n+    if (failed(rewriter.convertRegionTypes(&newOp.getBody(), *converter)))\n+      return failure();\n+\n     return success();\n   }\n };\n \n void populateCFPatterns(TritonGPUTypeConverter &typeConverter,\n                         RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n-  patterns.add<CFBranchPattern, CFCondBranchPattern>(typeConverter, context);\n+  patterns.add<FuncOpPattern, CFCondBranchPattern, CFBranchPattern>(\n+      typeConverter, context);\n }\n //\n "}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 92, "deletions": 40, "changes": 132, "file_content_changes": "@@ -123,29 +123,6 @@ void StoreOp::print(OpAsmPrinter &printer) {\n namespace mlir {\n namespace triton {\n \n-//-- FpToFpOp --\n-bool FpToFpOp::areCastCompatible(::mlir::TypeRange inputs,\n-                                 ::mlir::TypeRange outputs) {\n-  if (inputs.size() != 1 || outputs.size() != 1)\n-    return false;\n-  auto srcEltType = inputs.front();\n-  auto dstEltType = outputs.front();\n-  auto srcTensorType = srcEltType.dyn_cast<mlir::RankedTensorType>();\n-  auto dstTensorType = dstEltType.dyn_cast<mlir::RankedTensorType>();\n-  if (srcTensorType && dstTensorType) {\n-    srcEltType = srcTensorType.getElementType();\n-    dstEltType = dstTensorType.getElementType();\n-  }\n-  // Check whether fp8 <=> fp16, bf16, f32, f64\n-  // Make `srcEltType` always the fp8 side\n-  if (dstEltType.dyn_cast<mlir::triton::Float8Type>())\n-    std::swap(srcEltType, dstEltType);\n-  if (!srcEltType.dyn_cast<mlir::triton::Float8Type>())\n-    return false;\n-  return dstEltType.isF16() || dstEltType.isBF16() || dstEltType.isF32() ||\n-         dstEltType.isF64();\n-}\n-\n //-- StoreOp --\n void StoreOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n                     ::mlir::Value ptr, ::mlir::Value value,\n@@ -314,12 +291,11 @@ bool mlir::triton::ReduceOp::withIndex(mlir::triton::RedOp redOp) {\n \n //-- SplatOp --\n OpFoldResult SplatOp::fold(FoldAdaptor adaptor) {\n-  auto constOperand = getSrc().getDefiningOp<arith::ConstantOp>();\n-  if (!constOperand)\n+  auto value = adaptor.getSrc();\n+  if (!value)\n     return {};\n   auto shapedType = getType().cast<ShapedType>();\n-  auto ret = SplatElementsAttr::get(\n-      shapedType, ArrayRef<Attribute>(constOperand.getValue()));\n+  auto ret = SplatElementsAttr::get(shapedType, ArrayRef<Attribute>(value));\n   return ret;\n }\n \n@@ -352,24 +328,100 @@ mlir::LogicalResult mlir::triton::ExpandDimsOp::inferReturnTypes(\n   return mlir::success();\n }\n \n-//-- BroadcastOp --\n-OpFoldResult BroadcastOp::fold(FoldAdaptor adaptor) {\n-  auto constOperand = getSrc().getDefiningOp<arith::ConstantOp>();\n-  if (!constOperand)\n+LogicalResult ExpandDimsOp::canonicalize(ExpandDimsOp op,\n+                                         PatternRewriter &rewriter) {\n+  auto definingOp = op.getOperand().getDefiningOp();\n+  if (!definingOp) {\n+    return mlir::failure();\n+  }\n+  // expand_dims(splat) -> splat\n+  if (auto splat = dyn_cast<triton::SplatOp>(definingOp)) {\n+    rewriter.replaceOpWithNewOp<triton::SplatOp>(op, op.getType(),\n+                                                 splat.getOperand());\n+    return mlir::success();\n+  }\n+  return mlir::failure();\n+}\n+\n+template <typename ViewLikeOp>\n+static OpFoldResult foldViewLikeOp(ViewLikeOp op, Attribute value) {\n+  if (!value)\n     return {};\n \n-  auto shapedType = getType().cast<ShapedType>();\n-  auto value = constOperand.getValue();\n+  auto shapedType = op.getType().template cast<mlir::ShapedType>();\n   if (auto denseElemsAttr = value.dyn_cast<DenseElementsAttr>()) {\n-    if (!denseElemsAttr.isSplat())\n-      return {};\n-    return SplatElementsAttr::get(shapedType,\n-                                  denseElemsAttr.getSplatValue<Attribute>());\n-  } else if (value.getType().isIntOrIndexOrFloat()) {\n-    return SplatElementsAttr::get(shapedType, value);\n-  } else {\n+    if (denseElemsAttr.isSplat()) {\n+      return denseElemsAttr.resizeSplat(shapedType);\n+    } else {\n+      return denseElemsAttr.reshape(shapedType);\n+    }\n+  }\n+  return {};\n+}\n+\n+OpFoldResult ExpandDimsOp::fold(FoldAdaptor adaptor) {\n+  return foldViewLikeOp(*this, adaptor.getSrc());\n+}\n+\n+//-- ViewOp --\n+template <typename OpType>\n+LogicalResult canonicalizeViewOrBroadcast(OpType op,\n+                                          PatternRewriter &rewriter) {\n+  auto definingOp = op.getOperand().getDefiningOp();\n+  if (!definingOp) {\n+    return mlir::failure();\n+  }\n+\n+  // view(view) -> view\n+  if (auto parent_view = dyn_cast<OpType>(definingOp)) {\n+    rewriter.replaceOpWithNewOp<OpType>(op, op.getType(),\n+                                        parent_view.getOperand());\n+    return mlir::success();\n+  }\n+\n+  // view(splat) -> splat\n+  if (auto splat = dyn_cast<triton::SplatOp>(definingOp)) {\n+    rewriter.replaceOpWithNewOp<triton::SplatOp>(op, op.getType(),\n+                                                 splat.getOperand());\n+    return mlir::success();\n+  }\n+\n+  return mlir::failure();\n+}\n+LogicalResult ViewOp::canonicalize(ViewOp op, PatternRewriter &rewriter) {\n+  return canonicalizeViewOrBroadcast(op, rewriter);\n+}\n+\n+OpFoldResult ViewOp::fold(FoldAdaptor adaptor) {\n+  if (getType() == getOperand().getType()) {\n+    // no-op\n+    return getOperand();\n+  }\n+\n+  return foldViewLikeOp(*this, adaptor.getSrc());\n+}\n+\n+//-- BroadcastOp --\n+LogicalResult BroadcastOp::canonicalize(BroadcastOp op,\n+                                        PatternRewriter &rewriter) {\n+  return canonicalizeViewOrBroadcast(op, rewriter);\n+}\n+\n+OpFoldResult BroadcastOp::fold(FoldAdaptor adaptor) {\n+  if (getType() == getOperand().getType()) {\n+    // no-op\n+    return getOperand();\n+  }\n+\n+  auto value = adaptor.getSrc();\n+  if (!value)\n     return {};\n+\n+  if (auto denseElemsAttr = value.dyn_cast<SplatElementsAttr>()) {\n+    auto shapedType = getType().cast<ShapedType>();\n+    return denseElemsAttr.resizeSplat(shapedType);\n   }\n+  return {};\n }\n \n } // namespace triton"}, {"filename": "lib/Dialect/Triton/IR/Types.cpp", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -43,9 +43,7 @@ namespace mlir {\n unsigned getPointeeBitWidth(RankedTensorType tensorTy) {\n   auto ptrTy = tensorTy.getElementType().cast<triton::PointerType>();\n   auto pointeeType = ptrTy.getPointeeType();\n-  return pointeeType.isa<triton::Float8Type>()\n-             ? 8\n-             : pointeeType.getIntOrFloatBitWidth();\n+  return pointeeType.getIntOrFloatBitWidth();\n }\n \n } // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 117, "deletions": 13, "changes": 130, "file_content_changes": "@@ -416,23 +416,84 @@ unsigned DotOperandEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n   if (auto mmaParent = getParent().dyn_cast<MmaEncodingAttr>()) {\n     int warpsPerCTAM = mmaParent.getWarpsPerCTA()[0];\n     int warpsPerCTAN = mmaParent.getWarpsPerCTA()[1];\n-    // TODO: always assume sub-register typed are packed into int32\n+    // A100\n     if (mmaParent.isAmpere()) {\n       int bitwidth = eltTy.getIntOrFloatBitWidth();\n       int shapePerWarpM = 16;\n       int shapePerWarpN = 8;\n       int shapePerWarpK = 4 * 64 / bitwidth;\n       int shapePerCTAM = shapePerWarpM * warpsPerCTAM;\n       int shapePerCTAN = shapePerWarpN * warpsPerCTAN;\n+\n       if (getOpIdx() == 0) {\n         int repM = std::max<int>(1, shape[0] / shapePerCTAM);\n         int repK = std::max<int>(1, shape[1] / shapePerWarpK);\n-        return 4 * repM * repK * (32 / bitwidth);\n+        return 4 * repM * repK;\n       }\n       if (getOpIdx() == 1) {\n         int repN = std::max<int>(1, shape[1] / shapePerCTAN);\n         int repK = std::max<int>(1, shape[0] / shapePerWarpK);\n-        return 4 * std::max(repN / 2, 1) * repK * (32 / bitwidth);\n+        return 4 * std::max(repN / 2, 1) * repK;\n+      }\n+    }\n+    // V100\n+    if (mmaParent.isVolta()) {\n+      bool isRow = getMMAv1IsRow();\n+      bool isVec4 = getMMAv1IsVec4();\n+      if (getOpIdx() == 0) {\n+        int packSizeM = (isRow || isVec4) ? 1 : 2;\n+        int repM = 2 * packSizeM;\n+        int spwM = 2 * 4 * repM;\n+        int numM = getMMAv1NumOuter(shape);\n+        int NK = shape[1];\n+        int vec = 2 * repM;\n+        // Here we mimic the logic in loadA, the result cannot be calculated\n+        // directly.\n+        llvm::DenseSet<std::pair<int, int>> visited;\n+        auto ld = [&](int m, int k) {\n+          visited.insert({m, k});\n+          if (vec > 4) {\n+            if (isRow)\n+              visited.insert({m, k + 4});\n+            else\n+              visited.insert({m + 1, k});\n+          }\n+        };\n+        for (unsigned k = 0; k < NK; k += 4)\n+          for (unsigned m = 0; m < numM / 2; ++m)\n+            if (!visited.count({m, k}))\n+              ld(m, k);\n+        return visited.size() * 2;\n+      }\n+      if (getOpIdx() == 1) {\n+        int packSizeN = (isRow && !isVec4) ? 2 : 1;\n+        int repN = 2 * packSizeN;\n+        int spwN = 2 * 4 * repN;\n+        int numN = getMMAv1NumOuter(shape);\n+        int vec = 2 * repN;\n+\n+        int NK = shape[0];\n+        // Here we mimic the logic in loadA, the result cannot be calculated\n+        // directly.\n+        llvm::DenseSet<std::pair<int, int>> visited;\n+        int elemsPerLd = vec > 4 ? 4 : 2;\n+        auto ld = [&](int n, int k) {\n+          visited.insert({n, k});\n+          if (vec > 4) {\n+            if (isRow)\n+              visited.insert({n + 1, k});\n+            else\n+              visited.insert({n, k + 4});\n+          }\n+        };\n+\n+        for (unsigned k = 0; k < NK; k += 4)\n+          for (unsigned n = 0; n < numN / 2; ++n) {\n+            if (!visited.count({n, k}))\n+              ld(n, k);\n+          }\n+\n+        return visited.size() * 2;\n       }\n     }\n   }\n@@ -654,26 +715,69 @@ Attribute DotOperandEncodingAttr::parse(AsmParser &parser, Type type) {\n     return {};\n   unsigned opIdx = attrs.get(\"opIdx\").cast<IntegerAttr>().getInt();\n   Attribute parent = attrs.get(\"parent\");\n-  Attribute isMMAv1Row;\n-  if (parent.isa<MmaEncodingAttr>() &&\n-      parent.cast<MmaEncodingAttr>().isVolta()) {\n-    isMMAv1Row = attrs.get(\"isMMAv1Row\");\n-    if (!isMMAv1Row)\n-      llvm::report_fatal_error(\"isMMAv1Row attribute is missing\");\n-  }\n   return parser.getChecked<DotOperandEncodingAttr>(parser.getContext(), opIdx,\n-                                                   parent, isMMAv1Row);\n+                                                   parent);\n }\n \n void DotOperandEncodingAttr::print(mlir::AsmPrinter &printer) const {\n   printer << \"<{\"\n           << \"opIdx = \" << getOpIdx() << \", \"\n           << \"parent = \" << getParent();\n-  if (getIsMMAv1Row())\n-    printer << \", isMMAv1Row = \" << getIsMMAv1Row();\n   printer << \"}>\";\n }\n \n+bool DotOperandEncodingAttr::getMMAv1IsRow() const {\n+  auto [isARow, isBRow, _0, _1, _2] =\n+      getParent().cast<MmaEncodingAttr>().decodeVoltaLayoutStates();\n+  return getOpIdx() == 0 ? isARow : isBRow;\n+}\n+\n+bool DotOperandEncodingAttr::getMMAv1IsVec4() const {\n+  auto [_0, _1, isAVec4, isBVec4, _2] =\n+      getParent().cast<MmaEncodingAttr>().decodeVoltaLayoutStates();\n+  return getOpIdx() == 0 ? isAVec4 : isBVec4;\n+}\n+\n+SmallVector<int> DotOperandEncodingAttr::getMMAv1Rep() const {\n+  auto [isARow, isBRow, isAVec4, isBVec4, _] =\n+      getParent().cast<MmaEncodingAttr>().decodeVoltaLayoutStates();\n+  // A\n+  if (getOpIdx() == 0) {\n+    int packSize = (isARow || isAVec4) ? 1 : 2;\n+    return {2 * packSize, 0, 1};\n+  }\n+  // B\n+  else {\n+    int packSize = (isBRow && !isBVec4) ? 2 : 1;\n+    return {0, 2 * packSize, 1};\n+  }\n+}\n+\n+SmallVector<int> DotOperandEncodingAttr::getMMAv1ShapePerWarp() const {\n+  auto rep = getMMAv1Rep();\n+  if (getOpIdx() == 0) {\n+    return {8 * rep[0], 0, 1};\n+  } else {\n+    return {0, 8 * rep[1], 1};\n+  }\n+}\n+\n+int DotOperandEncodingAttr::getMMAv1Vec() const {\n+  size_t opIdx = getOpIdx();\n+  return 2 * getMMAv1Rep()[opIdx];\n+}\n+\n+int DotOperandEncodingAttr::getMMAv1NumOuter(ArrayRef<int64_t> shape) const {\n+  auto spw = getMMAv1ShapePerWarp();\n+  auto rep = getMMAv1Rep();\n+  auto warpsPerCTA = getParent().cast<MmaEncodingAttr>().getWarpsPerCTA();\n+  if (getOpIdx() == 0) {\n+    return rep[0] * shape[0] / (spw[0] * warpsPerCTA[0]);\n+  } else {\n+    return rep[1] * shape[1] / (spw[1] * warpsPerCTA[1]);\n+  }\n+}\n+\n //===----------------------------------------------------------------------===//\n // InsertSliceAsyncOp\n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 41, "deletions": 36, "changes": 77, "file_content_changes": "@@ -8,6 +8,7 @@\n using namespace mlir;\n namespace {\n using triton::DotOp;\n+using triton::gpu::BlockedEncodingAttr;\n using triton::gpu::ConvertLayoutOp;\n using triton::gpu::DotOperandEncodingAttr;\n using triton::gpu::MmaEncodingAttr;\n@@ -40,12 +41,6 @@ SmallVector<int64_t, 2> mmaVersionToShapePerWarp(int version) {\n   }\n }\n \n-SmallVector<unsigned, 2> warpsPerTileV1(const ArrayRef<int64_t> shape,\n-                                        int numWarps) {\n-  // Set a default value that ensures product of wpt equals numWarps\n-  return {static_cast<unsigned>(numWarps), 1};\n-}\n-\n SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n                                         const ArrayRef<int64_t> shape,\n                                         int numWarps) {\n@@ -89,19 +84,6 @@ class BlockedToMMA : public mlir::RewritePattern {\n       : mlir::RewritePattern(triton::DotOp::getOperationName(), 2, context),\n         computeCapability(computeCapability) {}\n \n-  static SmallVector<unsigned, 2> getWarpsPerTile(triton::DotOp dotOp,\n-                                                  const ArrayRef<int64_t> shape,\n-                                                  int version, int numWarps) {\n-    switch (version) {\n-    case 1:\n-      return warpsPerTileV1(shape, numWarps);\n-    case 2:\n-      return warpsPerTileV2(dotOp, shape, numWarps);\n-    default:\n-      llvm_unreachable(\"unsupported MMA version\");\n-    }\n-  }\n-\n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n@@ -122,13 +104,46 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto mod = op->getParentOfType<mlir::ModuleOp>();\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n \n-    auto warpsPerTile =\n-        getWarpsPerTile(dotOp, retShape, versionMajor, numWarps);\n+    // operands\n+    Value a = dotOp.getA();\n+    Value b = dotOp.getB();\n+    auto oldAType = a.getType().cast<RankedTensorType>();\n+    auto oldBType = b.getType().cast<RankedTensorType>();\n+\n     triton::gpu::MmaEncodingAttr mmaEnc;\n     if (versionMajor == 1) {\n+      SetVector<Operation *> aBwdSlices, bBwdSlices;\n+      auto isCvt = [](Operation *op) { return isa<ConvertLayoutOp>(op); };\n+      getBackwardSlice(a, &aBwdSlices, isCvt);\n+      getBackwardSlice(b, &bBwdSlices, isCvt);\n+      // get the source of the first conversion found in slices\n+      auto getCvtArgOrder = [](Operation *op) {\n+        return cast<ConvertLayoutOp>(op)\n+            .getOperand()\n+            .getType()\n+            .cast<RankedTensorType>()\n+            .getEncoding()\n+            .cast<BlockedEncodingAttr>()\n+            .getOrder();\n+      };\n+      bool isARow = true;\n+      bool isBRow = true;\n+      Operation *aOp = a.getDefiningOp();\n+      Operation *bOp = b.getDefiningOp();\n+      if (!aBwdSlices.empty())\n+        aOp = aBwdSlices[0];\n+      if (!bBwdSlices.empty())\n+        bOp = bBwdSlices[0];\n+      if (aOp)\n+        isARow = getCvtArgOrder(aOp)[0] == 1;\n+      if (bOp)\n+        isBRow = getCvtArgOrder(bOp)[0] == 1;\n+\n       mmaEnc = triton::gpu::MmaEncodingAttr::get(\n-          oldRetType.getContext(), versionMajor, numWarps, mmaV1Counter++);\n+          oldRetType.getContext(), versionMajor, numWarps, oldAType.getShape(),\n+          oldBType.getShape(), retShape, isARow, isBRow, mmaV1Counter++);\n     } else if (versionMajor == 2) {\n+      auto warpsPerTile = warpsPerTileV2(dotOp, retShape, numWarps);\n       mmaEnc = triton::gpu::MmaEncodingAttr::get(\n           oldRetType.getContext(), versionMajor, 0 /*versionMinor*/,\n           warpsPerTile);\n@@ -142,10 +157,6 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto oldAcc = dotOp.getOperand(2);\n     auto newAcc = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         oldAcc.getLoc(), newRetType, oldAcc);\n-    Value a = dotOp.getA();\n-    Value b = dotOp.getB();\n-    auto oldAType = a.getType().cast<RankedTensorType>();\n-    auto oldBType = b.getType().cast<RankedTensorType>();\n     auto oldAOrder = oldAType.getEncoding()\n                          .cast<triton::gpu::DotOperandEncodingAttr>()\n                          .getParent()\n@@ -156,21 +167,15 @@ class BlockedToMMA : public mlir::RewritePattern {\n                          .getParent()\n                          .cast<triton::gpu::BlockedEncodingAttr>()\n                          .getOrder();\n-    Attribute isMMAv1RowA;\n-    Attribute isMMAv1RowB;\n-    if (versionMajor == 1) {\n-      isMMAv1RowA = BoolAttr::get(getContext(), oldAOrder[0] == 1);\n-      isMMAv1RowB = BoolAttr::get(getContext(), oldBOrder[0] == 1);\n-    }\n \n     auto newAType = RankedTensorType::get(\n         oldAType.getShape(), oldAType.getElementType(),\n-        triton::gpu::DotOperandEncodingAttr::get(\n-            oldAType.getContext(), 0, newRetType.getEncoding(), isMMAv1RowA));\n+        triton::gpu::DotOperandEncodingAttr::get(oldAType.getContext(), 0,\n+                                                 newRetType.getEncoding()));\n     auto newBType = RankedTensorType::get(\n         oldBType.getShape(), oldBType.getElementType(),\n-        triton::gpu::DotOperandEncodingAttr::get(\n-            oldBType.getContext(), 1, newRetType.getEncoding(), isMMAv1RowB));\n+        triton::gpu::DotOperandEncodingAttr::get(oldBType.getContext(), 1,\n+                                                 newRetType.getEncoding()));\n \n     a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), newAType, a);\n     b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), newBType, b);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -8,7 +8,6 @@ add_mlir_dialect_library(TritonGPUTransforms\n   RemoveLayoutConversions.cpp\n   ReorderInstructions.cpp\n   TritonGPUConversion.cpp\n-  UpdateMmaForVolta.cpp\n   Utility.cpp\n \n   DEPENDS"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 0, "deletions": 90, "changes": 90, "file_content_changes": "@@ -16,54 +16,6 @@ using triton::gpu::DotOperandEncodingAttr;\n using triton::gpu::MmaEncodingAttr;\n using triton::gpu::SliceEncodingAttr;\n \n-class OptimizeConvertToDotOperand : public mlir::RewritePattern {\n-public:\n-  explicit OptimizeConvertToDotOperand(mlir::MLIRContext *context)\n-      : RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(), 1,\n-                       context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto srcType = cvt.getOperand().getType().cast<RankedTensorType>();\n-    auto dstType = cvt.getResult().getType().cast<RankedTensorType>();\n-    // order\n-    ArrayRef<unsigned> order;\n-    if (auto srcBlockedLayout =\n-            srcType.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>())\n-      order = srcBlockedLayout.getOrder();\n-    else if (auto srcSharedLayout =\n-                 srcType.getEncoding()\n-                     .dyn_cast<triton::gpu::SharedEncodingAttr>())\n-      order = srcSharedLayout.getOrder();\n-    else\n-      return failure();\n-    // dot operand output\n-    auto dstDotOperandLayout =\n-        dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n-    if (!dstDotOperandLayout)\n-      return failure();\n-    if (!dstDotOperandLayout.getIsMMAv1Row())\n-      return failure();\n-    bool isMMAv1Row =\n-        dstDotOperandLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    if ((order[0] == 1 && isMMAv1Row) || (order[0] == 0 && !isMMAv1Row))\n-      return failure();\n-\n-    auto newIsRow = BoolAttr::get(op->getContext(), !isMMAv1Row);\n-    auto newDstEncoding = triton::gpu::DotOperandEncodingAttr::get(\n-        op->getContext(), dstDotOperandLayout.getOpIdx(),\n-        dstDotOperandLayout.getParent(), newIsRow);\n-    auto newDstType = RankedTensorType::get(\n-        dstType.getShape(), dstType.getElementType(), newDstEncoding);\n-    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        op->getLoc(), newDstType, cvt.getOperand());\n-    rewriter.replaceOp(op, newCvt.getResult());\n-    return success();\n-  }\n-};\n-\n // convert(trans(convert(arg)))\n // x = convert_layout arg: #distributed -> #shared_x\n // y = trans x: #shared_x -> #shared_y\n@@ -120,46 +72,6 @@ class ConvertTransConvert : public mlir::RewritePattern {\n   }\n };\n \n-class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n-\n-public:\n-  MoveOpAfterLayoutConversion(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             1, context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto srcTy = cvt.getOperand().getType().cast<RankedTensorType>();\n-    auto retTy = cvt.getResult().getType().dyn_cast<RankedTensorType>();\n-    if (!retTy)\n-      return failure();\n-    if (!isa<triton::gpu::DotOperandEncodingAttr>(retTy.getEncoding()))\n-      return failure();\n-    if (isa<triton::gpu::SharedEncodingAttr>(srcTy.getEncoding()))\n-      return failure();\n-    //\n-    Operation *argOp = cvt.getOperand().getDefiningOp();\n-    //\n-    if (!argOp)\n-      return failure();\n-    // we only handle loads since the goal of this pass is to\n-    SetVector<Operation *> processed;\n-    SetVector<Attribute> layout;\n-    llvm::MapVector<Value, Attribute> toConvert;\n-    int numCvts = simulateBackwardRematerialization(\n-        cvt, processed, layout, toConvert, retTy.getEncoding());\n-    if (numCvts > 1 || toConvert.size() == 1)\n-      return failure();\n-\n-    IRMapping mapping;\n-    rematerializeConversionChain(toConvert, rewriter, processed, mapping);\n-    rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n-    return mlir::success();\n-  }\n-};\n-\n } // namespace\n \n #define GEN_PASS_CLASSES\n@@ -180,9 +92,7 @@ class TritonGPUOptimizeDotOperandsPass\n     auto ret = pm.run(m);\n \n     mlir::RewritePatternSet patterns(context);\n-    patterns.add<OptimizeConvertToDotOperand>(context);\n     patterns.add<ConvertTransConvert>(context);\n-    // patterns.add<MoveOpAfterLayoutConversion>(context);\n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n       signalPassFailure();\n     if (fixupLoops(m).failed())"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -595,7 +595,7 @@ class TritonGPURemoveLayoutConversionsPass\n     patterns.add<DecomposeDotOperand>(context);\n     patterns.add<ConvertDotConvert>(context);\n \n-    if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n+    if (mlir::applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n       signalPassFailure();\n     }\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 16, "deletions": 11, "changes": 27, "file_content_changes": "@@ -14,11 +14,10 @@ using namespace mlir::triton::gpu;\n TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n                                                int numWarps)\n     : context(context), numWarps(numWarps) {\n-  // TODO: how does MLIR pick the right conversion?\n   addConversion([](Type type) { return type; });\n   addConversion([this](RankedTensorType tensorType) -> RankedTensorType {\n     // types with encoding are already in the right format\n-    // TODO: check for layout encodings specifically\n+    // TODO: check for layout encodings more specifically\n     if (tensorType.getEncoding())\n       return tensorType;\n     // pessimistic values for attributes:\n@@ -41,16 +40,19 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n   // This will create newArg, and map(origArg, newArg)\n   addArgumentMaterialization([&](OpBuilder &builder,\n                                  RankedTensorType tensorType, ValueRange inputs,\n-                                 Location loc) {\n-    llvm_unreachable(\"Argument rematerialization not implemented\");\n+                                 Location loc) -> llvm::Optional<Value> {\n+    llvm_unreachable(\"Argument rematerialization should not happen in Triton \"\n+                     \"-> TritonGPU conversion\");\n     return std::nullopt;\n   });\n \n   // If the origValue still has live user(s), use this to\n   // convert origValue to newValue\n   addSourceMaterialization([&](OpBuilder &builder, RankedTensorType tensorType,\n-                               ValueRange inputs, Location loc) {\n-    llvm_unreachable(\"Source rematerialization not implemented\");\n+                               ValueRange inputs,\n+                               Location loc) -> llvm::Optional<Value> {\n+    llvm_unreachable(\"Source rematerialization should not happen in Triton -> \"\n+                     \"TritonGPU Conversion\");\n     return std::nullopt;\n   });\n \n@@ -62,9 +64,6 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n     auto cast =\n         builder.create<triton::gpu::ConvertLayoutOp>(loc, tensorType, inputs);\n     return Optional<Value>(cast.getResult());\n-    // return Optional<Value>(cast.getResult(0));\n-    // llvm_unreachable(\"Not implemented\");\n-    // return std::nullopt;\n   });\n }\n \n@@ -82,10 +81,16 @@ TritonGPUConversionTarget::TritonGPUConversionTarget(\n                scf::ReduceReturnOp>();\n \n   addDynamicallyLegalDialect<arith::ArithDialect, math::MathDialect,\n-                             triton::TritonDialect, scf::SCFDialect>(\n+                             func::FuncDialect, triton::TritonDialect,\n+                             cf::ControlFlowDialect, scf::SCFDialect>(\n       [&](Operation *op) {\n-        if (typeConverter.isLegal(op))\n+        bool hasLegalRegions = true;\n+        for (auto &region : op->getRegions()) {\n+          hasLegalRegions = hasLegalRegions && typeConverter.isLegal(&region);\n+        }\n+        if (hasLegalRegions && typeConverter.isLegal(op)) {\n           return true;\n+        }\n         return false;\n       });\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/UpdateMmaForVolta.cpp", "status": "removed", "additions": 0, "deletions": 212, "changes": 212, "file_content_changes": "@@ -1,212 +0,0 @@\n-#include \"Utility.h\"\n-#include \"mlir/Dialect/SCF/IR/SCF.h\"\n-#include \"mlir/IR/Matchers.h\"\n-#include \"mlir/IR/PatternMatch.h\"\n-#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n-#include \"triton/Analysis/Utility.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n-#define GEN_PASS_CLASSES\n-#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n-\n-namespace mlir {\n-namespace {\n-using triton::DotOp;\n-using triton::gpu::ConvertLayoutOp;\n-using triton::gpu::DotOperandEncodingAttr;\n-using triton::gpu::MmaEncodingAttr;\n-using triton::gpu::SharedEncodingAttr;\n-using triton::gpu::SliceEncodingAttr;\n-\n-// Get the wpt for MMAv1 using more information.\n-// Reference the original logic here\n-// https://github.com/openai/triton/blob/0e4691e6dd91e001a8d33b71badf8b3314325459/lib/codegen/analysis/layout.cc#L223\n-SmallVector<unsigned> getWarpsPerCTA(ArrayRef<int64_t> shape, bool isARow,\n-                                     bool isBRow, bool isAVec4, bool isBVec4,\n-                                     int numWarps) {\n-  // TODO[Superjomn]: Share code with\n-  // DotOpMmaV1ConversionHelper::AParam/BParam, since same code to compute the\n-  // rep,spw and fpw.\n-  SmallVector<unsigned> wpt({1, 1});\n-  SmallVector<unsigned> wpt_nm1;\n-\n-  SmallVector<int, 2> rep(2), spw(2);\n-  std::array<int, 3> fpw{{2, 2, 1}};\n-  int packSize0 = (isARow || isAVec4) ? 1 : 2;\n-  rep[0] = 2 * packSize0;\n-  spw[0] = fpw[0] * 4 * rep[0];\n-\n-  int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n-  rep[1] = 2 * packSize1;\n-  spw[1] = fpw[1] * 4 * rep[1];\n-\n-  do {\n-    wpt_nm1 = wpt;\n-    if (wpt[0] * wpt[1] < numWarps)\n-      wpt[0] = std::clamp<int>(wpt[0] * 2, 1, shape[0] / spw[0]);\n-    if (wpt[0] * wpt[1] < numWarps)\n-      wpt[1] = std::clamp<int>(wpt[1] * 2, 1, shape[1] / spw[1]);\n-  } while (wpt_nm1 != wpt);\n-\n-  return wpt;\n-}\n-\n-// Given a (potentially malformed) DotOp, determines the optimal\n-// MMAEncoding to use on V100\n-LogicalResult getOptimizedV100MMaLayout(triton::DotOp dotOp,\n-                                        MmaEncodingAttr &old,\n-                                        MmaEncodingAttr &ret) {\n-  auto *ctx = dotOp->getContext();\n-  auto AT = dotOp.getA().getType().cast<RankedTensorType>();\n-  auto BT = dotOp.getB().getType().cast<RankedTensorType>();\n-  auto DT = dotOp.getD().getType().cast<RankedTensorType>();\n-  auto shapeA = AT.getShape();\n-  auto shapeB = BT.getShape();\n-  if (!DT.getEncoding())\n-    return mlir::failure();\n-  auto mmaLayout = DT.getEncoding().dyn_cast<MmaEncodingAttr>();\n-  if (!(mmaLayout && mmaLayout.isVolta()))\n-    return mlir::failure();\n-  // We have an MmaEncodingAttr here. Find the correct layout for it.\n-  auto dotOperandA = AT.getEncoding().cast<DotOperandEncodingAttr>();\n-  auto dotOperandB = BT.getEncoding().cast<DotOperandEncodingAttr>();\n-  bool isARow = dotOperandA.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-  bool isBRow = dotOperandB.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-  auto [isARow_, isBRow_, isAVec4_, isBVec4_, mmaId] =\n-      mmaLayout.decodeVoltaLayoutStates();\n-  bool isAVec4 = !isARow && (shapeA[isARow] <= 16);\n-  bool isBVec4 = isBRow && (shapeB[isBRow] <= 16);\n-  // The wpt of MMAv1 is also determined by isARow, isBRow and shape, and it\n-  // could only be set here for those states might be updated by previous\n-  // patterns in the Combine Pass.\n-  auto tgtWpt = getWarpsPerCTA(DT.getShape(), isARow, isBRow, isAVec4, isBVec4,\n-                               product(mmaLayout.getWarpsPerCTA()));\n-  if (isARow == isARow_ && isBRow == isBRow_ && isAVec4 == isAVec4_ &&\n-      isBVec4 == isBVec4_) {\n-    if (tgtWpt == mmaLayout.getWarpsPerCTA())\n-      return mlir::failure();\n-  }\n-  // Recalculate the wpt, for here we could get the latest information, the\n-  // wpt should be updated.\n-  auto updatedWpt =\n-      getWarpsPerCTA(DT.getShape(), isARow, isBRow, isAVec4, isBVec4,\n-                     product(mmaLayout.getWarpsPerCTA()));\n-  // return results\n-  old = mmaLayout;\n-  ret =\n-      MmaEncodingAttr::get(ctx, mmaLayout.getVersionMajor(), updatedWpt,\n-                           AT.getShape(), BT.getShape(), isARow, isBRow, mmaId);\n-  return mlir::success();\n-}\n-\n-// Replace result op type\n-void setOpResultType(Operation *op, ArrayRef<Type> newTypes) {\n-  if (op->getNumResults() != newTypes.size())\n-    llvm_unreachable(\"number of types different from number of results\");\n-  // nothing to do\n-  if (op->getNumResults() == 0)\n-    return;\n-  // replace types\n-  for (unsigned i = 0; i < op->getNumResults(); i++) {\n-    Type newType = newTypes[i];\n-    op->getResult(i).setType(newType);\n-  }\n-  // special case: arith.constant: we need to change the value attr\n-  if (isa<arith::ConstantOp>(op)) {\n-    Type newType = newTypes[0];\n-    auto attr = op->getAttrDictionary()\n-                    .get(\"value\")\n-                    .dyn_cast<mlir::DenseElementsAttr>();\n-    if (attr) {\n-      auto newAttr =\n-          mlir::DenseElementsAttr::getFromRawBuffer(newType, attr.getRawData());\n-      op->setAttr(\"value\", newAttr);\n-    }\n-  }\n-}\n-\n-// update style type given the provided layoutMap\n-Type updateStaleType(\n-    const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &layoutMap,\n-    RankedTensorType type) {\n-  auto encoding = type.getEncoding();\n-  // mma encoding\n-  if (auto mma = encoding.dyn_cast<MmaEncodingAttr>()) {\n-    auto newMma = layoutMap.lookup(mma);\n-    if (!newMma)\n-      return Type();\n-    return RankedTensorType::get(type.getShape(), type.getElementType(),\n-                                 newMma);\n-  }\n-  // slice encoding\n-  else if (auto slice = encoding.dyn_cast<SliceEncodingAttr>()) {\n-    if (auto mma = slice.getParent().dyn_cast<MmaEncodingAttr>()) {\n-      auto newMma = layoutMap.lookup(mma);\n-      if (!newMma)\n-        return Type();\n-      auto newSlice =\n-          SliceEncodingAttr::get(slice.getContext(), slice.getDim(), newMma);\n-      return RankedTensorType::get(type.getShape(), type.getElementType(),\n-                                   newSlice);\n-    }\n-  }\n-  // dot operand encoding\n-  else if (auto dotOp = encoding.dyn_cast<DotOperandEncodingAttr>()) {\n-    if (auto mma = dotOp.getParent().dyn_cast<MmaEncodingAttr>()) {\n-      auto newMma = layoutMap.lookup(mma);\n-      if (!newMma)\n-        return Type();\n-      auto newDotOp = DotOperandEncodingAttr::get(\n-          dotOp.getContext(), dotOp.getOpIdx(), newMma, dotOp.getIsMMAv1Row());\n-      return RankedTensorType::get(type.getShape(), type.getElementType(),\n-                                   newDotOp);\n-    }\n-  }\n-  return Type();\n-}\n-\n-} // namespace\n-\n-class UpdateMmaForVoltaPass\n-    : public UpdateMmaForVoltaBase<UpdateMmaForVoltaPass> {\n-public:\n-  UpdateMmaForVoltaPass() = default;\n-  void runOnOperation() override {\n-    MLIRContext *context = &getContext();\n-    ModuleOp m = getOperation();\n-    // Step 1:\n-    // Build a map from old MMA encoding to new MMA encoding.\n-    DenseMap<MmaEncodingAttr, MmaEncodingAttr> layoutMap;\n-    m.walk([&layoutMap](triton::DotOp dotOp) {\n-      MmaEncodingAttr newLayout;\n-      MmaEncodingAttr oldLayout;\n-      if (failed(getOptimizedV100MMaLayout(dotOp, oldLayout, newLayout)))\n-        return;\n-      layoutMap[oldLayout] = newLayout;\n-    });\n-    // Step 2:\n-    // Replace all wrong layouts with the right one\n-    m.walk([&layoutMap](Operation *op) {\n-      if (op->getNumResults() != 1)\n-        return;\n-      auto type = op->getResult(0).getType().dyn_cast<RankedTensorType>();\n-      if (!type)\n-        return;\n-      Type newType = updateStaleType(layoutMap, type);\n-      if (!newType)\n-        return;\n-      setOpResultType(op, {newType});\n-    });\n-    // Step 3:\n-    // We may have messed up some loops in the process.\n-    // Fix them up\n-    if (fixupLoops(m).failed())\n-      signalPassFailure();\n-  }\n-};\n-\n-std::unique_ptr<Pass> createTritonGPUUpdateMmaForVoltaPass() {\n-  return std::make_unique<UpdateMmaForVoltaPass>();\n-}\n-\n-} // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -82,7 +82,7 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n       return failure();\n     ret = sliceEncoding.getParent();\n   }\n-  if (auto view = dyn_cast<triton::ViewOp>(op)) {\n+  if (isa<triton::ViewOp, triton::CatOp>(op)) {\n     return failure();\n   }\n   return success();\n@@ -179,8 +179,6 @@ int simulateBackwardRematerialization(\n       if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n               triton::MakeRangeOp, triton::SplatOp>(*opArgI))\n         continue;\n-      if (auto view = dyn_cast<triton::ViewOp>(opArgI))\n-        continue;\n \n       // We add one expensive conversion for the current operand\n       numCvts += 1;"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -308,7 +308,6 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n     return nullptr;\n   }\n \n-  // llvm::outs() << module << \"\\n\";\n   auto llvmIR = translateLLVMToLLVMIR(llvmContext, module);\n   if (!llvmIR) {\n     llvm::errs() << \"Translate to LLVM IR failed\";"}, {"filename": "python/examples/copy_strided.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,4 +1,3 @@\n-\n import triton\n import triton.language as tl\n "}, {"filename": "python/setup.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -83,8 +83,8 @@ def get_thirdparty_packages(triton_cache_path):\n         if p.syspath_var_name in os.environ:\n             package_dir = os.environ[p.syspath_var_name]\n         version_file_path = os.path.join(package_dir, \"version.txt\")\n-        if not os.path.exists(version_file_path) or\\\n-           Path(version_file_path).read_text() != p.url:\n+        if p.syspath_var_name not in os.environ and\\\n+           (not os.path.exists(version_file_path) or Path(version_file_path).read_text() != p.url):\n             try:\n                 shutil.rmtree(package_root_dir)\n             except Exception:"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -602,9 +602,13 @@ void init_triton_ir(py::module &&m) {\n       .def(\n           \"get_int64_ty\",\n           [](mlir::OpBuilder &self) -> mlir::Type { return self.getI64Type(); })\n-      .def(\"get_fp8_ty\",\n+      .def(\"get_fp8e4_ty\",\n            [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getType<mlir::triton::Float8Type>();\n+             return self.getType<mlir::Float8E4M3FNType>();\n+           })\n+      .def(\"get_fp8e5_ty\",\n+           [](mlir::OpBuilder &self) -> mlir::Type {\n+             return self.getType<mlir::Float8E5M2Type>();\n            })\n       .def(\n           \"get_half_ty\",\n@@ -1458,10 +1462,6 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::createTritonGPURemoveLayoutConversionsPass());\n            })\n-      .def(\"add_tritongpu_update_mma_for_volta_pass\",\n-           [](mlir::PassManager &self) {\n-             self.addPass(mlir::createTritonGPUUpdateMmaForVoltaPass());\n-           })\n       .def(\"add_tritongpu_reorder_instructions_pass\",\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::createTritonGPUReorderInstructionsPass());"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -56,7 +56,7 @@ def nvsmi(attrs):\n     'a100': {\n         (512, 512, 512): {'float16': 0.08, 'float32': 0.13, 'int8': 0.05},\n         (1024, 1024, 1024): {'float16': 0.33, 'float32': 0.35, 'int8': 0.169},\n-        (2048, 2048, 2048): {'float16': 0.64, 'float32': 0.57, 'int8': 0.34},\n+        (2048, 2048, 2048): {'float16': 0.62, 'float32': 0.57, 'int8': 0.34},\n         (4096, 4096, 4096): {'float16': 0.81, 'float32': 0.75, 'int8': 0.46},\n         (8192, 8192, 8192): {'float16': 0.77, 'float32': 0.85, 'int8': 0.51},\n         # tall-skinny"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 83, "deletions": 29, "changes": 112, "file_content_changes": "@@ -295,6 +295,43 @@ def test_floordiv(dtype_x, dtype_y, device='cuda'):\n     _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n \n \n+def test_unsigned_name_mangling(device='cuda'):\n+    # Test that uint32 and int32 are mangled differently by the compiler\n+    SIZE = 128\n+    # define the kernel / launch-grid\n+\n+    @triton.jit\n+    def kernel(O1, O2, X, Y, SIZE: tl.constexpr):\n+        off = tl.arange(0, SIZE)\n+        x = tl.load(X + off)\n+        y = tl.load(Y + off)\n+        out1 = tl.abs(x)  # uint32 -> nop\n+        out2 = tl.abs(-y)  # int32 -> should have an effect\n+        tl.store(O1 + off, out1)\n+        tl.store(O2 + off, out2)\n+\n+    dtype_x = 'uint32'\n+    dtype_y = 'int32'\n+    # inputs\n+    rs = RandomState(17)\n+    x = numpy_random(SIZE, dtype_str=dtype_x, rs=rs)\n+    y = numpy_random(SIZE, dtype_str=dtype_y, rs=rs)\n+    # reference result\n+    expect = (np.abs(x), np.abs(-y))\n+    # triton result\n+    x_tri = to_triton(x, device=device, dst_type=dtype_x)\n+    y_tri = to_triton(y, device=device, dst_type=dtype_y)\n+    actual = tuple(\n+        to_triton(np.empty_like(e), device=device)\n+        for e in expect\n+    )\n+    kernel[(1, )](actual[0], actual[1], x_tri, y_tri, SIZE=SIZE, num_warps=4)\n+\n+    # Bitwise op, so expect exact equality\n+    assert (expect[0] == to_numpy(actual[0])).all()\n+    assert (expect[1] == to_numpy(actual[1])).all()\n+\n+\n # ---------------\n # test bitwise ops\n # ---------------\n@@ -483,6 +520,17 @@ def test_unary_op(dtype_x, expr, device='cuda'):\n def test_math_op(expr, device='cuda'):\n     _test_unary('float32', f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n \n+# ----------------\n+# test abs\n+# ----------------\n+\n+\n+@pytest.mark.parametrize(\"dtype_x\", [\n+    (dtype_x)\n+    for dtype_x in dtypes_with_bfloat16\n+])\n+def test_abs(dtype_x, device='cuda'):\n+    _test_unary(dtype_x, 'tl.abs(x)', 'np.abs(x) ', device=device)\n \n # ----------------\n # test indexing\n@@ -837,10 +885,11 @@ def kernel(in_out_ptr):\n         assert torch.all(x == 2)\n \n \n-@pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n-def test_f8_xf16_roundtrip(dtype):\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n+@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.float32])\n+def test_f8_xf16_roundtrip(in_dtype, out_dtype):\n     \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n-    check_type_supported(dtype)\n+    check_type_supported(out_dtype)\n \n     @triton.jit\n     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n@@ -851,20 +900,24 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n     f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n-    f8 = triton.reinterpret(f8_tensor, tl.float8)\n+    # f32_to_f8 doesn't handle nan, so we make sure f8_tensor doesn't contain any nan\n+    all_exp_ones = (f8_tensor & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n+    f8_tensor[all_exp_ones] = 0\n+    f8 = triton.reinterpret(f8_tensor, in_dtype)\n     n_elements = f8_tensor.numel()\n-    xf16 = torch.empty_like(f8_tensor, dtype=dtype)\n+    xf16 = torch.empty_like(f8_tensor, dtype=out_dtype)\n     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     copy_kernel[grid](f8, xf16, n_elements, BLOCK_SIZE=1024)\n \n     f8_output_tensor = torch.empty_like(xf16, dtype=torch.int8)\n-    f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n+    f8_output = triton.reinterpret(f8_output_tensor, in_dtype)\n     copy_kernel[grid](xf16, f8_output, n_elements, BLOCK_SIZE=1024)\n \n     assert torch.all(f8_tensor == f8_output_tensor)\n \n \n-def test_f16_to_f8_rounding():\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4])\n+def test_f16_to_f8_rounding(in_dtype):\n     \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n     error is the minimum over all float8.\n     Or the same explanation a bit mathier:\n@@ -887,7 +940,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     f16_input = torch.tensor(f16_input_np, dtype=torch.float16, device='cuda')\n     n_elements = f16_input.numel()\n     f8_output_tensor = torch.empty_like(f16_input, dtype=torch.int8)\n-    f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n+    f8_output = triton.reinterpret(f8_output_tensor, in_dtype)\n     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     copy_kernel[grid](f16_input, f8_output, n_elements, BLOCK_SIZE=1024)\n \n@@ -897,7 +950,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     abs_error = torch.abs(f16_input - f16_output)\n \n     all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device='cuda')\n-    all_f8_vals = triton.reinterpret(all_f8_vals_tensor, tl.float8)\n+    all_f8_vals = triton.reinterpret(all_f8_vals_tensor, in_dtype)\n     all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=torch.float16)\n     copy_kernel[grid](all_f8_vals, all_f8_vals_in_f16, n_elements=256, BLOCK_SIZE=1024)\n \n@@ -1135,7 +1188,8 @@ def kernel(X, stride_xm, stride_xn,\n                                            [128, 128, 64, 4],\n                                            [64, 128, 128, 4],\n                                            [32, 128, 64, 2],\n-                                           [128, 128, 64, 2],\n+                                           # triggers nvptx/ptxas bug on V100 currently\n+                                           # [128, 128, 64, 2],\n                                            [64, 128, 128, 2]]\n                           for allow_tf32 in [True]\n                           for col_a in [True, False]\n@@ -1748,11 +1802,11 @@ def _kernel(dst):\n \n \n @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n-                         [('int32', 'libdevice.ffs', ''),\n-                          ('float32', 'libdevice.log2', ''),\n-                          ('float32', 'libdevice.pow', tl.libdevice.LIBDEVICE_PATH),\n-                          ('float64', 'libdevice.norm4d', '')])\n-def test_libdevice_tensor(dtype_str, expr, lib_path):\n+                         [('int32', 'math.ffs', ''),\n+                          ('float32', 'math.log2', ''),\n+                          ('float32', 'math.pow', tl.math.LIBDEVICE_PATH),\n+                          ('float64', 'math.norm4d', '')])\n+def test_math_tensor(dtype_str, expr, lib_path):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -1765,37 +1819,37 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n \n-    if expr == 'libdevice.log2':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.broadcast_to(tl.libdevice.log2(5.0), x.shape)'})\n+    if expr == 'math.log2':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.broadcast_to(tl.math.log2(5.0), x.shape)'})\n         y_ref = np.log2(5.0)\n-    elif expr == 'libdevice.ffs':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.ffs(x)'})\n+    elif expr == 'math.ffs':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.ffs(x)'})\n         y_ref = np.zeros(shape, dtype=x.dtype)\n         for i in range(shape[0]):\n             y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n-    elif expr == 'libdevice.pow':\n+    elif expr == 'math.pow':\n         # numpy does not allow negative factors in power, so we use abs()\n         x = np.abs(x)\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.pow(x, x)'})\n         y_ref = np.power(x, x)\n-    elif expr == 'libdevice.norm4d':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.norm4d(x, x, x, x)'})\n+    elif expr == 'math.norm4d':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.norm4d(x, x, x, x)'})\n         y_ref = np.sqrt(4 * np.power(x, 2))\n \n     x_tri = to_triton(x)\n     # triton result\n     y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n     # compare\n-    if expr == 'libdevice.ffs':\n+    if expr == 'math.ffs':\n         np.testing.assert_equal(y_ref, to_numpy(y_tri))\n     else:\n         np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n \n \n @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n-                         [('float32', 'libdevice.pow', '')])\n-def test_libdevice_scalar(dtype_str, expr, lib_path):\n+                         [('float32', 'math.pow', '')])\n+def test_math_scalar(dtype_str, expr, lib_path):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -1811,13 +1865,13 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n \n     # numpy does not allow negative factors in power, so we use abs()\n     x = np.abs(x)\n-    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.pow(x, x)'})\n     y_ref[:] = np.power(x, x)\n \n     # triton result\n     x_tri = to_triton(x)[0].item()\n     y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n-    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'math': lib_path})\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n \n@@ -1975,7 +2029,7 @@ def kernel(InitI, Bound, CutOff, OutI, OutJ):\n # -----------------------\n # test layout conversions\n # -----------------------\n-# TODO: backend hsould be tested separately\n+# TODO: backend should be tested separately\n \n \n class MmaLayout:"}, {"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -196,3 +196,15 @@ def kernel_sub(a, b, o, N: tl.constexpr):\n     proc.start()\n     proc.join()\n     assert proc.exitcode == 0\n+\n+\n+def test_memory_leak() -> None:\n+    @triton.jit\n+    def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n+        xnumel = 10\n+        xoffset = tl.program_id(0) * XBLOCK\n+        xindex = xoffset + tl.arange(0, XBLOCK)[:]\n+        xmask = xindex < xnumel\n+        x0 = xindex\n+        tmp0 = tl.load(in_ptr0 + (x0), xmask)\n+        tl.store(out_ptr0 + (x0 + tl.zeros([XBLOCK], tl.int32)), tmp0, xmask)"}, {"filename": "python/test/unit/runtime/test_launch.py", "status": "added", "additions": 35, "deletions": 0, "changes": 35, "file_content_changes": "@@ -0,0 +1,35 @@\n+import gc\n+import tracemalloc\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+def test_memory_leak() -> None:\n+\n+    @triton.jit\n+    def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n+        xnumel = 10\n+        xoffset = tl.program_id(0) * XBLOCK\n+        xindex = xoffset + tl.arange(0, XBLOCK)[:]\n+        xmask = xindex < xnumel\n+        x0 = xindex\n+        tmp0 = tl.load(in_ptr0 + (x0), xmask)\n+        tl.store(out_ptr0 + (x0 + tl.zeros([XBLOCK], tl.int32)), tmp0, xmask)\n+\n+    tracemalloc.start()\n+    try:\n+        inp = torch.randn(10, device='cuda')\n+        out = torch.randn(10, device='cuda')\n+        kernel[(10,)](inp, out, 10, XBLOCK=16)\n+        gc.collect()\n+        begin, _ = tracemalloc.get_traced_memory()\n+        for _ in range(100):\n+            kernel[(10,)](inp, out, 10, XBLOCK=16)\n+        gc.collect()\n+        end, _ = tracemalloc.get_traced_memory()\n+        assert end - begin < 1000\n+    finally:\n+        tracemalloc.stop()"}, {"filename": "python/triton/__init__.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,5 +1,5 @@\n \"\"\"isort:skip_file\"\"\"\n-__version__ = '2.0.0'\n+__version__ = '2.1.0'\n \n # ---------------------------------------\n # Note: import order is significant here."}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 172, "deletions": 78, "changes": 250, "file_content_changes": "@@ -16,7 +16,7 @@\n import warnings\n from collections import namedtuple\n from pathlib import Path\n-from typing import Any, Callable, Dict, Tuple, Union\n+from typing import Any, Callable, Dict, Optional, Tuple, Union\n \n import setuptools\n import torch\n@@ -33,7 +33,8 @@ def str_to_ty(name):\n         ty = str_to_ty(name[1:])\n         return triton.language.pointer_type(ty)\n     tys = {\n-        \"fp8\": triton.language.float8,\n+        \"fp8e5\": triton.language.float8e5,\n+        \"fp8e4\": triton.language.float8e4,\n         \"fp16\": triton.language.float16,\n         \"bf16\": triton.language.bfloat16,\n         \"fp32\": triton.language.float32,\n@@ -56,7 +57,9 @@ def mangle_ty(ty):\n     if ty.is_ptr():\n         return 'P' + mangle_ty(ty.element_ty)\n     if ty.is_int():\n-        return 'i' + str(ty.int_bitwidth)\n+        SIGNED = triton.language.dtype.SIGNEDNESS.SIGNED\n+        prefix = 'i' if ty.int_signedness == SIGNED else 'u'\n+        return prefix + str(ty.int_bitwidth)\n     if ty.is_fp8():\n         return 'fp8'\n     if ty.is_fp16():\n@@ -106,10 +109,11 @@ def __exit__(self, *args, **kwargs):\n \n \n class CodeGenerator(ast.NodeVisitor):\n-    def __init__(self, context, prototype, gscope, attributes, constants, function_name, module=None, is_kernel=False, function_types=dict(), debug=False):\n+    def __init__(self, context, prototype, gscope, attributes, constants, function_name,\n+                 module=None, is_kernel=False, function_types: Optional[Dict] = None, debug=False):\n         self.builder = _triton.ir.builder(context)\n         self.module = self.builder.create_module() if module is None else module\n-        self.function_ret_types = function_types\n+        self.function_ret_types = {} if function_types is None else function_types\n         self.prototype = prototype\n         self.gscope = gscope\n         self.lscope = dict()\n@@ -119,45 +123,45 @@ def __init__(self, context, prototype, gscope, attributes, constants, function_n\n         self.is_kernel = is_kernel\n         self.last_node = None\n         self.debug = debug\n-        self.builtins = {\n-            'range': range,\n-            'min': triton.language.minimum,\n-            'float': float,\n-            'int': int,\n-            'print': triton.language.core.device_print,\n-            'isinstance': isinstance,\n-            'getattr': getattr,\n-        }\n-        self.static_functions = [\n-            'static_print', 'static_assert'\n-        ]\n         self.scf_stack = []\n         # SSA-construction\n         # name => triton.language.tensor\n         self.local_defs: Dict[str, triton.language.tensor] = {}\n         self.global_uses: Dict[str, triton.language.tensor] = {}\n-\n-    def get_value(self, name):\n-        ''' This function:\n-        1. make sure `name` is defined\n-        2. if `name` is triton.language.tensor, get stored tensor by calling\n-           `self._get_tensor()`\n-        '''\n-        # search node.id in local scope\n-        ret = None\n-        if name in self.lscope:\n-            ret = self.lscope[name]\n-            if name not in self.local_defs:\n-                self.global_uses[name] = ret\n-        # search node.id in global scope\n-        elif name in self.gscope:\n-            ret = self.gscope[name]\n-        # search node.id in builtins\n-        elif name in self.builtins:\n-            ret = self.builtins[name]\n-        else:\n-            raise ValueError(f'{name} is not defined')\n-        return ret\n+        self.dereference_name: Callable[[str], Any] = self._define_name_lookup()\n+\n+    builtin_namespace: Dict[str, Any] = {_.__name__: _ for _ in (range, float, int, isinstance, getattr)}\n+\n+    def _define_name_lookup(self):\n+        # TODO: this needs to be moved to class scope when cyclic imports untangled and `triton.language` can be imported at module level\n+        self.builtin_namespace.update((\n+            ('print', triton.language.core.device_print),\n+            ('min', triton.language.minimum),  # TODO: why `min`? if `min`, why not `max`? `sum`? `all`?\n+        ))\n+        # TODO: this needs to be moved to class scope when cyclic imports untangled and `triton.language` can be imported at module level\n+        self.statically_implemented_functions.update((\n+            (triton.language.core.static_assert, self.execute_static_assert),\n+            (triton.language.core.static_print, self.execute_static_print),\n+        ))\n+\n+        def local_lookup(name: str, absent):\n+            value = self.lscope.get(name, absent)  # this needs to be re-fetched from `self` every time, because it gets switched occasionally\n+            if value is not absent and name not in self.local_defs:\n+                self.global_uses[name] = value\n+            return value\n+\n+        absent_marker = object()\n+\n+        def name_lookup(name: str) -> Any:\n+            lookup_order = local_lookup, self.gscope.get, self.builtin_namespace.get\n+            absent = absent_marker\n+            for lookup_function in lookup_order:\n+                value = lookup_function(name, absent)\n+                if value is not absent:\n+                    return value\n+            raise NameError(f'{name} is not defined')\n+\n+        return name_lookup\n \n     def set_value(self, name: str,\n                   value: Union[triton.language.tensor, triton.language.constexpr]) -> None:\n@@ -182,9 +186,27 @@ def visit_compound_statement(self, stmts):\n                 break\n         return stmts and isinstance(stmt, ast.Return)\n \n+    # TODO: should be its own AST visitor\n     def contains_return_op(self, node):\n         if isinstance(node, ast.Return):\n             return True\n+        elif isinstance(node, ast.Assign):\n+            return self.contains_return_op(node.value)\n+        elif isinstance(node, ast.Module):\n+            pred = lambda s: self.contains_return_op(s)\n+            return any(pred(s) for s in node.body)\n+        elif isinstance(node, ast.FunctionDef):\n+            pred = lambda s: self.contains_return_op(s)\n+            return any(pred(s) for s in node.body)\n+        elif isinstance(node, ast.Call):\n+            fn = self.visit(node.func)\n+            if isinstance(fn, triton.JITFunction):\n+                old_gscope = self.gscope\n+                self.gscope = sys.modules[fn.fn.__module__].__dict__\n+                ret = self.contains_return_op(fn.parse())\n+                self.gscope = old_gscope\n+                return ret\n+            return False\n         elif isinstance(node, ast.If):\n             pred = lambda s: self.contains_return_op(s)\n             ret = any(pred(s) for s in node.body)\n@@ -311,18 +333,21 @@ def visit_Assign(self, node):\n         _names = []\n         for target in node.targets:\n             _names += [self.visit(target)]\n-        assert len(_names) == 1\n+        if len(_names) > 1:\n+            raise NotImplementedError(\"Multiple assignment is not supported.\")\n         names = _names[0]\n         values = self.visit(node.value)\n         if not isinstance(names, tuple):\n             names = [names]\n         if not isinstance(values, tuple):\n             values = [values]\n+        native_nontensor_types = (triton.language.dtype, )\n         for name, value in zip(names, values):\n             # by default, constexpr are assigned into python variable\n             if isinstance(value, triton.language.constexpr):\n                 value = value.value\n-            if not isinstance(value, triton.language.tensor):\n+            if not isinstance(value, triton.language.tensor) and \\\n+               not isinstance(value, native_nontensor_types):\n                 value = triton.language.core._to_tensor(value, self.builder)\n             self.set_value(name, value)\n \n@@ -332,12 +357,12 @@ def visit_AugAssign(self, node):\n         rhs = ast.BinOp(lhs, node.op, node.value)\n         assign = ast.Assign(targets=[node.target], value=rhs)\n         self.visit(assign)\n-        return self.get_value(name)\n+        return self.dereference_name(name)\n \n     def visit_Name(self, node):\n         if type(node.ctx) == ast.Store:\n             return node.id\n-        return self.get_value(node.id)\n+        return self.dereference_name(node.id)\n \n     def visit_Store(self, node):\n         ast.NodeVisitor.generic_visit(self, node)\n@@ -654,7 +679,7 @@ def visit_For(self, node):\n                     ast.NodeVisitor.generic_visit(self, stmt)\n             return\n \n-        if IteratorClass != self.builtins['range']:\n+        if IteratorClass is not range:\n             raise RuntimeError('Only `range` and `static_range` iterators are currently supported')\n \n         # visit iterator arguments\n@@ -766,8 +791,8 @@ def visit_Slice(self, node):\n     def visit_Index(self, node):\n         return self.visit(node.value)\n \n-    def visit_keyword(self, node):\n-        return {node.arg: self.visit(node.value)}\n+    def visit_keyword(self, node) -> Tuple[str, Any]:\n+        return node.arg, self.visit(node.value)\n \n     def visit_Assert(self, node) -> Any:\n         if not self.debug:\n@@ -781,22 +806,16 @@ def visit_Call(self, node):\n         fn = self.visit(node.func)\n         if isinstance(fn, triton.language.constexpr):\n             fn = fn.value\n-        kws = dict()\n-        for keyword in node.keywords:\n-            kws.update(self.visit(keyword))\n+\n+        static_implementation = self.statically_implemented_functions.get(fn)\n+        if static_implementation is not None:\n+            return static_implementation(node)\n+\n+        kws = dict(self.visit(keyword) for keyword in node.keywords)\n         args = [self.visit(arg) for arg in node.args]\n-        if fn.__name__ == \"print\":\n-            fn = self.builtins[\"print\"]\n-        elif fn.__name__ == \"device_assert\":\n+        if fn is triton.language.core.device_assert:   # TODO: this should not be so hardcoded\n             if not self.debug:\n                 return\n-        elif fn.__name__ in self.static_functions:\n-            if fn.__name__ == \"static_print\":\n-                print(*args, **kws)\n-                return\n-            elif fn.__name__ == \"static_assert\":\n-                assert args[0], args[1]\n-                return\n         if isinstance(fn, triton.runtime.JITFunction):\n             from inspect import getcallargs\n             args = getcallargs(fn.fn, *args, **kws)\n@@ -834,12 +853,10 @@ def visit_Call(self, node):\n                 for i in range(call_op.get_num_results()):\n                     results.append(triton.language.tensor(call_op.get_result(i), callee_ret_type[i]))\n                 return tuple(results)\n-        if (hasattr(fn, '__self__') and self.is_triton_tensor(fn.__self__)) \\\n-                or impl.is_builtin(fn):\n+        if (hasattr(fn, '__self__') and self.is_triton_tensor(fn.__self__)) or impl.is_builtin(fn):\n             return fn(*args, _builder=self.builder, **kws)\n-        if fn in self.builtins.values():\n-            args = [arg.value if isinstance(arg, triton.language.constexpr) else arg\n-                    for arg in args]\n+        if fn in self.builtin_namespace.values():\n+            args = [arg.value if isinstance(arg, triton.language.constexpr) else arg for arg in args]\n         return fn(*args, **kws)\n \n     def visit_Constant(self, node):\n@@ -886,6 +903,22 @@ def visit_Expr(self, node):\n     def visit_NoneType(self, node):\n         return None\n \n+    def visit_JoinedStr(self, node):\n+        values = list(node.values)\n+        for i, value in enumerate(values):\n+            if isinstance(value, ast.Constant):\n+                values[i] = str(value.value)\n+            elif isinstance(value, ast.FormattedValue):\n+                conversion_code = value.conversion\n+                evaluated = self.visit(value.value)\n+                if not isinstance(evaluated, triton.language.constexpr):\n+                    raise NotImplementedError(\"Cannot evaluate f-string containing non-constexpr conversion values,\"\n+                                              \" found conversion of type \" + str(type(evaluated)))\n+                values[i] = (\"{}\" if conversion_code < 0 else \"{!\" + chr(conversion_code) + \"}\").format(evaluated.value)\n+            else:\n+                raise AssertionError(\"encountered unexpected node of type {} in a JoinedStr node\".format(type(value)))\n+        return ''.join(values)\n+\n     def visit(self, node):\n         if node is not None:\n             self.last_node = node\n@@ -900,19 +933,77 @@ def generic_visit(self, node):\n         typename = type(node).__name__\n         raise NotImplementedError(\"Unsupported node: {}\".format(typename))\n \n+    # TODO: populate this here (rather than inside `_define_name_lookup`) once cyclic imports resolved\n+    statically_implemented_functions: Dict[object, Callable[[ast.Call], Any]] = {}\n+\n+    def execute_static_print(self, node: ast.Call) -> None:\n+        # TODO: too simplistic? Perhaps do something else with non-constexpr\n+        def unwrap(_):\n+            return _.value if isinstance(_, triton.language.constexpr) else _\n+\n+        kws = {name: unwrap(value) for name, value in (self.visit(keyword) for keyword in node.keywords)}\n+        args = [unwrap(self.visit(arg)) for arg in node.args]\n+        print(*args, **kws)\n+\n+    def execute_static_assert(self, node: ast.Call) -> None:\n+        arg_count = len(node.args)\n+        if not (0 < arg_count <= 2) or len(node.keywords):\n+            raise TypeError(\"`static_assert` requires one or two positional arguments only\")\n+\n+        passed = self.visit(node.args[0])\n+        if not isinstance(passed, bool):\n+            raise NotImplementedError(\"Assertion condition could not be determined at compile-time. Make sure that it depends only on `constexpr` values\")\n+        if not passed:\n+            if arg_count == 1:\n+                message = \"\"\n+            else:\n+                try:\n+                    message = self.visit(node.args[1])\n+                except Exception as e:\n+                    message = \"<failed to evaluate assertion message: \" + repr(e) + \">\"\n+\n+            raise CompileTimeAssertionFailure(None, node, message)\n+        return None\n+\n \n class CompilationError(Exception):\n-    def __init__(self, src, node):\n-        self.message = f'at {node.lineno}:{node.col_offset}:\\n'\n-        self.message += '\\n'.join(src.split('\\n')[:node.lineno])\n-        self.message += '\\n' + ' ' * node.col_offset + '^'\n+    source_line_count_max_in_message = 12\n+\n+    def _format_message(self) -> str:\n+        node = self.node\n+        message = f'at {node.lineno}:{node.col_offset}:'\n+        if self.src is None:\n+            message += \" <source unavailable>\"\n+        else:\n+            message += '\\n'.join(self.src.split('\\n')[:node.lineno][-self.source_line_count_max_in_message:])\n+            message += '\\n' + ' ' * node.col_offset + '^'\n+        if self.error_message:\n+            message += '\\n' + self.error_message\n+        return message\n+\n+    def __init__(self, src: Optional[str], node: ast.AST, error_message: Optional[str]):\n         self.src = src\n         self.node = node\n-        super().__init__(self.message)\n+        self.error_message = error_message\n+        self.message = self._format_message()\n+\n+    def __str__(self):\n+        return self.message\n+\n+    def __repr__(self):\n+        return \"{}({!r})\".format(type(self).__name__, self.message)\n \n     def __reduce__(self):\n         # this is necessary to make CompilationError picklable\n-        return (type(self), (self.src, self.node))\n+        return type(self), (self.src, self.node, self.error_message)\n+\n+\n+class CompileTimeAssertionFailure(CompilationError):\n+    \"\"\"Specific exception for failed tests in `static_assert` invocations\"\"\"\n+\n+    def set_source_code(self, src: Optional[str]):\n+        self.src = src\n+        self.message = self._format_message()\n \n \n class OutOfResources(Exception):\n@@ -977,11 +1068,16 @@ def build_triton_ir(fn, signature, specialization, constants, debug=False):\n     generator = CodeGenerator(context, prototype, gscope=gscope, constants=all_constants, function_name=function_name, attributes=new_attrs, is_kernel=True, debug=debug)\n     try:\n         generator.visit(fn.parse())\n+    except CompileTimeAssertionFailure as e:\n+        e.set_source_code(fn.src)\n+        raise\n+    except CompilationError:  # (can this ever happen? nobody has access to fn.src except here)\n+        raise  # unchanged\n     except Exception as e:\n         node = generator.last_node\n-        if node is None or isinstance(e, (NotImplementedError, CompilationError)):\n-            raise e\n-        raise CompilationError(fn.src, node) from e\n+        if node is None:\n+            raise\n+        raise CompilationError(fn.src, node, repr(e)) from e\n     ret = generator.module\n     # module takes ownership of the context\n     ret.context = context\n@@ -1017,6 +1113,7 @@ def optimize_ttgir(mod, num_stages, compute_capability):\n     pm = _triton.ir.pass_manager(mod.context)\n     pm.enable_debug()\n     pm.add_tritongpu_coalesce_pass()\n+    pm.add_tritongpu_remove_layout_conversions_pass()\n     pm.add_tritongpu_accelerate_matmul_pass(compute_capability)\n     pm.add_tritongpu_remove_layout_conversions_pass()\n     pm.add_tritongpu_optimize_dot_operands_pass()\n@@ -1025,10 +1122,6 @@ def optimize_ttgir(mod, num_stages, compute_capability):\n     pm.add_tritongpu_optimize_dot_operands_pass()\n     pm.add_tritongpu_remove_layout_conversions_pass()\n     pm.add_tritongpu_decompose_conversions_pass()\n-    if compute_capability // 10 == 7:\n-        # The update_mma_for_volta pass helps to compute some information for MMA encoding specifically for MMAv1\n-        # NOTE this pass should be placed after all the passes those modifies mma layout\n-        pm.add_tritongpu_update_mma_for_volta_pass()\n     pm.add_tritongpu_reorder_instructions_pass()\n     pm.add_cse_pass()\n     pm.add_symbol_dce_pass()\n@@ -1262,6 +1355,7 @@ def format_of(ty):\n         ptr_info.valid = false;\n     }}\n     ptr_info.dev_ptr = dev_ptr;\n+    Py_DECREF(ret);  // Thanks ChatGPT!\n     return ptr_info;\n   }}\n   PyErr_SetString(PyExc_TypeError, \"Pointer argument must be either uint64 or have data_ptr method\");\n@@ -1330,7 +1424,7 @@ def format_of(ty):\n \n \n def default_cache_dir():\n-    return os.path.join(os.environ[\"HOME\"], \".triton\", \"cache\")\n+    return os.path.join(Path.home(), \".triton\", \"cache\")\n \n \n class CacheManager:"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -5,7 +5,7 @@\n     ir,\n     builtin,\n )\n-from . import libdevice\n+from . import math\n from .core import (\n     abs,\n     arange,\n@@ -38,7 +38,8 @@\n     float16,\n     float32,\n     float64,\n-    float8,\n+    float8e4,\n+    float8e5,\n     function_type,\n     int1,\n     int16,\n@@ -130,7 +131,8 @@\n     \"float16\",\n     \"float32\",\n     \"float64\",\n-    \"float8\",\n+    \"float8e4\",\n+    \"float8e5\",\n     \"full\",\n     \"function_type\",\n     \"int1\",\n@@ -139,7 +141,7 @@\n     \"int64\",\n     \"int8\",\n     \"ir\",\n-    \"libdevice\",\n+    \"math\",\n     \"load\",\n     \"log\",\n     \"max\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 29, "deletions": 16, "changes": 45, "file_content_changes": "@@ -37,10 +37,9 @@ def _to_tensor(x, builder):\n \n \n class dtype:\n-    SINT_TYPES = ['int1', 'int8', 'int16', 'int32', 'int64']\n-    UINT_TYPES = ['uint8', 'uint16', 'uint32', 'uint64']\n-    FP_TYPES = ['fp8', 'fp16', 'bf16', 'fp32', 'fp64']\n-    CUSTOMIZED_FP_TYPES = ['fp8']\n+    SINT_TYPES = ['int8', 'int16', 'int32', 'int64']\n+    UINT_TYPES = ['int1', 'uint8', 'uint16', 'uint32', 'uint64']\n+    FP_TYPES = ['fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n     STANDARD_FP_TYPES = ['fp16', 'bf16', 'fp32', 'fp64']\n     OTHER_TYPES = ['void']\n \n@@ -60,9 +59,12 @@ def __init__(self, name):\n             self.int_bitwidth = int(name.split('int')[-1])\n             self.primitive_bitwidth = self.int_bitwidth\n         elif name in dtype.FP_TYPES:\n-            if name == 'fp8':\n+            if name == 'fp8e4':\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n+            elif name == 'fp8e5':\n+                self.fp_mantissa_width = 2\n+                self.primitive_bitwidth = 8\n             elif name == 'fp16':\n                 self.fp_mantissa_width = 10\n                 self.primitive_bitwidth = 16\n@@ -75,11 +77,13 @@ def __init__(self, name):\n             elif name == 'fp64':\n                 self.fp_mantissa_width = 53\n                 self.primitive_bitwidth = 64\n+            else:\n+                raise RuntimeError(f'Unsupported floating-point type {name}')\n         elif name == 'void':\n             self.primitive_bitwidth = 0\n \n     def is_fp8(self):\n-        return self.name == 'fp8'\n+        return 'fp8' in self.name\n \n     def is_fp16(self):\n         return self.name == 'fp16'\n@@ -123,9 +127,6 @@ def is_uint64(self):\n     def is_floating(self):\n         return self.name in dtype.FP_TYPES\n \n-    def is_customized_floating(self):\n-        return self.name in dtype.CUSTOMIZED_FP_TYPES\n-\n     def is_standard_floating(self):\n         return self.name in dtype.STANDARD_FP_TYPES\n \n@@ -181,8 +182,10 @@ def to_ir(self, builder: ir.builder) -> ir.type:\n             return builder.get_int32_ty()\n         elif self.name in ('int64', 'uint64'):\n             return builder.get_int64_ty()\n-        elif self.name == 'fp8':\n-            return builder.get_fp8_ty()\n+        elif self.name == 'fp8e5':\n+            return builder.get_fp8e5_ty()\n+        elif self.name == 'fp8e4':\n+            return builder.get_fp8e4_ty()\n         elif self.name == 'fp16':\n             return builder.get_half_ty()\n         elif self.name == 'bf16':\n@@ -314,7 +317,8 @@ def to_ir(self, builder: ir.builder):\n uint16 = dtype('uint16')\n uint32 = dtype('uint32')\n uint64 = dtype('uint64')\n-float8 = dtype('fp8')\n+float8e5 = dtype('fp8e5')\n+float8e4 = dtype('fp8e4')\n float16 = dtype('fp16')\n bfloat16 = dtype('bf16')\n float32 = dtype('fp32')\n@@ -894,7 +898,7 @@ def load(pointer, mask=None, other=None, cache_modifier=\"\", eviction_policy=\"\",\n     :param other: if mask[idx] is false, return other[idx]\n     :type other: Block, optional\n     :param cache_modifier: changes cache option in nvidia ptx\n-    'type cache_modifier: str, optional\n+    :type cache_modifier: str, optional\n     \"\"\"\n     # mask, other can be constexpr\n     if _constexpr_to_value(mask) is not None:\n@@ -1061,7 +1065,7 @@ def _add_math_1arg_docstr(name: str) -> Callable[[T], T]:\n \n     def _decorator(func: T) -> T:\n         docstr = \"\"\"\n-    Computes the element-wise {name} of :code:`x`\n+    Computes the element-wise {name} of :code:`x`.\n \n     :param x: the input values\n     :type x: Block\n@@ -1211,7 +1215,16 @@ def max_contiguous(input, values, _builder=None):\n \n @triton.jit\n def abs(x):\n-    return where(x >= 0, x, -x)\n+    x_dtype = x.dtype\n+    if x_dtype.is_floating():\n+        num_bits: constexpr = x.dtype.primitive_bitwidth\n+        int_dtype = dtype(f'int{num_bits}')\n+        mask = 2 ** (num_bits - 1) - 1\n+        ret = x.to(int_dtype, bitcast=True) & mask.to(int_dtype)\n+        ret = ret.to(x_dtype, bitcast=True)\n+    else:\n+        ret = where(x >= 0, x, -x)\n+    return ret\n \n \n @triton.jit\n@@ -1271,7 +1284,7 @@ def softmax(x, ieee_rounding=False):\n @triton.jit\n def ravel(x):\n     \"\"\"\n-    Returns a contiguous flattened view of :code:`x`\n+    Returns a contiguous flattened view of :code:`x`.\n \n     :param x: the input tensor\n     :type x: Block"}, {"filename": "python/triton/language/math.py", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "python/triton/language/libdevice.py"}, {"filename": "python/triton/language/random.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -106,7 +106,7 @@ def uint32_to_uniform_float(x):\n def rand(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offset` block,\n-    returns a block of random :code:`float32` in :math:`U(0, 1)`\n+    returns a block of random :code:`float32` in :math:`U(0, 1)`.\n \n     :param seed: The seed for generating random numbers.\n     :param offsets: The offsets to generate random numbers for.\n@@ -120,7 +120,7 @@ def rand(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n def rand4x(seed, offsets, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offsets` block,\n-    returns a 4 blocks of random :code:`float32` in :math:`U(0, 1)`\n+    returns a 4 blocks of random :code:`float32` in :math:`U(0, 1)`.\n \n     :param seed: The seed for generating random numbers.\n     :param offsets: The offsets to generate random numbers for.\n@@ -151,7 +151,7 @@ def pair_uniform_to_normal(u1, u2):\n def randn(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offset` block,\n-    returns a block of random :code:`float32` in :math:`\\\\mathcal{N}(0, 1)`\n+    returns a block of random :code:`float32` in :math:`\\\\mathcal{N}(0, 1)`.\n \n     :param seed: The seed for generating random numbers.\n     :param offsets: The offsets to generate random numbers for.\n@@ -167,7 +167,7 @@ def randn(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n def randn4x(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offset` block,\n-    returns a 4 blocks of random :code:`float32` in :math:`\\\\mathcal{N}(0, 1)`\n+    returns a 4 blocks of random :code:`float32` in :math:`\\\\mathcal{N}(0, 1)`.\n \n     :param seed: The seed for generating random numbers.\n     :param offsets: The offsets to generate random numbers for."}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -673,8 +673,8 @@ def cast(input: tl.tensor,\n     dst_sca_ty = dst_ty.scalar\n \n     # Casting with customized floating types involved: fp8 <=> bf16, fp16, fp32, fp64\n-    if (src_sca_ty.is_customized_floating() and dst_sca_ty.is_floating()) or \\\n-       (src_sca_ty.is_floating() and dst_sca_ty.is_customized_floating()):\n+    if (src_sca_ty.is_fp8() and dst_sca_ty.is_floating()) or \\\n+       (src_sca_ty.is_floating() and dst_sca_ty.is_fp8()):\n         return tl.tensor(builder.create_fp_to_fp(input.handle, dst_ty.to_ir(builder)),\n                          dst_ty)\n \n@@ -1188,14 +1188,14 @@ def xor_sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n def umulhi(x: tl.tensor, y: tl.tensor, builder: ir.builder) -> tl.tensor:\n     x, y = binary_op_type_checking_impl(x, y, builder)\n     # FIXME(Keren): not portable, should be fixed\n-    from . import libdevice\n-    return libdevice.mulhi(x, y, _builder=builder)\n+    from . import math\n+    return math.mulhi(x, y, _builder=builder)\n \n \n def floor(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     # FIXME(Keren): not portable, should be fixed\n-    from . import libdevice\n-    return libdevice.floor(x, _builder=builder)\n+    from . import math\n+    return math.floor(x, _builder=builder)\n \n \n def exp(x: tl.tensor, builder: ir.builder) -> tl.tensor:"}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -181,8 +181,8 @@ def _dsd_kernel(\n     inc_b = tl.load(pinc)\n     inc_b = tl.multiple_of(inc_b, 8)\n     for k in range(K, 0, -TILE_K):\n-        a = tl.load(pa, mask=True)\n-        b = tl.load(pb, mask=offs_bn[None, :] < DS0)\n+        a = tl.load(pa)\n+        b = tl.load(pb)\n         acc += tl.dot(a, b)\n         pa += inc_a\n         pb += inc_b * stride_bk"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -15,7 +15,7 @@ def __init__(self, fn, arg_names, configs, key, reset_to_zero, prune_configs_by:\n         :param prune_configs_by: a dict of functions that are used to prune configs, fields:\n             'perf_model': performance model used to predicate running time with different configs, returns running time\n             'top_k': number of configs to bench\n-            'prune_num_stages_by'(optional): a function used to prune num_stages. It take configs:List[Config] as its input, and returns pruned configs.\n+            'prune_num_stages_by'(optional): a function used to prune num_stages. It takes configs:List[Config] as its input, and returns pruned configs.\n         '''\n         if not configs:\n             self.configs = [Config({}, num_warps=4, num_stages=2)]\n@@ -165,18 +165,18 @@ def autotune(configs, key, prune_configs_by=None, reset_to_zero=None):\n         @triton.jit\n         def kernel(x_ptr, x_size, **META):\n             BLOCK_SIZE = META['BLOCK_SIZE']\n-    :note: When all the configurations are evaluated, the kernel will run multiple time.\n+    :note: When all the configurations are evaluated, the kernel will run multiple times.\n            This means that whatever value the kernel updates will be updated multiple times.\n            To avoid this undesired behavior, you can use the `reset_to_zero` argument, which\n-           reset the value of the provided tensor to `zero` before running any configuration.\n+           resets the value of the provided tensor to `zero` before running any configuration.\n     :param configs: a list of :code:`triton.Config` objects\n     :type configs: list[triton.Config]\n     :param key: a list of argument names whose change in value will trigger the evaluation of all provided configs.\n     :type key: list[str]\n     :param prune_configs_by: a dict of functions that are used to prune configs, fields:\n         'perf_model': performance model used to predicate running time with different configs, returns running time\n         'top_k': number of configs to bench\n-        'early_config_prune'(optional): a function used to do early prune (eg, num_stages). It take configs:List[Config] as its input, and returns pruned configs.\n+        'early_config_prune'(optional): a function used to do early prune (eg, num_stages). It takes configs:List[Config] as its input, and returns pruned configs.\n     :param reset_to_zero: a list of argument names whose value will be reset to zero before evaluating any configs.\n     :type reset_to_zero: list[str]\n     \"\"\"\n@@ -209,9 +209,9 @@ def heuristics(values):\n         @triton.jit\n         def kernel(x_ptr, x_size, **META):\n             BLOCK_SIZE = META['BLOCK_SIZE'] # smallest power-of-two >= x_size\n-    .param values: a dictionary of meta-parameter names and functions that compute the value of the meta-parameter.\n+    :param values: a dictionary of meta-parameter names and functions that compute the value of the meta-parameter.\n                    each such function takes a list of positional arguments as input.\n-    .type values: dict[str, Callable[[list[Any]], Any]]\n+    :type values: dict[str, Callable[[list[Any]], Any]]\n     \"\"\"\n     def decorator(fn):\n         return Heuristics(fn, fn.arg_names, values)"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -177,7 +177,8 @@ def _type_of(key):\n                 triton.language.uint16: 'u16',\n                 triton.language.uint32: 'u32',\n                 triton.language.uint64: 'u64',\n-                triton.language.float8: 'fp8',\n+                triton.language.float8e5: 'fp8e5',\n+                triton.language.float8e4: 'fp8e4',\n                 triton.language.float16: 'fp16',\n                 triton.language.bfloat16: 'bf16',\n                 triton.language.float32: 'fp32',\n@@ -315,7 +316,9 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None):\n         self.annotations = {self.arg_names.index(name): ty for name, ty in fn.__annotations__.items()}\n         self.__annotations__ = fn.__annotations__\n         # index of constexprs\n-        self.constexprs = [self.arg_names.index(ann) for ann in self.__annotations__.keys()]\n+        from triton.language.core import \\\n+            constexpr  # import here rather than at module level due to circular import tangle\n+        self.constexprs = [index for index, ty in self.annotations.items() if isinstance(ty, type) and issubclass(ty, constexpr)]\n         # launcher\n         self.run = self._make_launcher()\n         # re-use docs of wrapped function"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -106,10 +106,8 @@ def allclose(x, y, atol=0, rtol=1e-2):\n         return torch.sum(x ^ y) == 0\n     if x.dtype in [torch.int8, torch.int16, torch.int32, torch.int64]:\n         rtol = 0\n-    diff = abs(x - y)\n-    x_max = torch.max(x)\n-    y_max = torch.max(y)\n-    return torch.max(diff) <= atol + rtol * torch.max(x_max, y_max)\n+        atol = 0\n+    return torch.allclose(x, y, rtol=rtol, atol=atol)\n \n \n def nvsmi(attrs):\n@@ -454,10 +452,12 @@ def get_max_simd_tflops(dtype: torch.dtype, backend=None, device=None):\n         backend = _triton.runtime.backend.CUDA\n     if not device:\n         device = torch.cuda.current_device()\n-    num_subcores = _triton.runtime.num_sm(backend, device) * 4  # on recent GPUs\n-    clock_rate = _triton.runtime.clock_rate(backend, device)  # in kHz\n-    cc = _triton.runtime.cc(backend, device)\n-    if cc < 80:\n+\n+    triton.compiler.init_cuda_utils()\n+    num_subcores = triton.compiler.cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4\n+    clock_rate = triton.compiler.cuda_utils.get_device_properties(device)[\"sm_clock_rate\"]  # in kHz\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] < 8:\n         if dtype == torch.float32:\n             ops_per_sub_core = 32  # 2*16\n         elif dtype == torch.float16:"}, {"filename": "python/tutorials/01-vector-add.py", "status": "modified", "additions": 8, "deletions": 5, "changes": 13, "file_content_changes": "@@ -1,16 +1,18 @@\n \"\"\"\n Vector Addition\n-=================\n-In this tutorial, you will write a simple vector addition using Triton and learn about:\n+===============\n \n+In this tutorial, you will write a simple vector addition using Triton.\n+\n+In doing so, you will learn about:\n - The basic programming model of Triton.\n - The `triton.jit` decorator, which is used to define Triton kernels.\n - The best practices for validating and benchmarking your custom ops against native reference implementations.\n \"\"\"\n \n # %%\n # Compute Kernel\n-# --------------------------\n+# --------------\n \n import torch\n \n@@ -92,9 +94,10 @@ def add(x: torch.Tensor, y: torch.Tensor):\n \n # %%\n # Benchmark\n-# -----------\n+# ---------\n+#\n # We can now benchmark our custom op on vectors of increasing sizes to get a sense of how it does relative to PyTorch.\n-# To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of your custom ops.\n+# To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of our custom ops.\n # for different problem sizes.\n \n "}, {"filename": "python/tutorials/02-fused-softmax.py", "status": "modified", "additions": 12, "deletions": 7, "changes": 19, "file_content_changes": "@@ -1,18 +1,20 @@\n \"\"\"\n Fused Softmax\n-=================\n+=============\n+\n In this tutorial, you will write a fused softmax operation that is significantly faster\n than PyTorch's native op for a particular class of matrices: those whose rows can fit in\n the GPU's SRAM.\n-You will learn about:\n \n+In doing so, you will learn about:\n - The benefits of kernel fusion for bandwidth-bound operations.\n - Reduction operators in Triton.\n \"\"\"\n \n # %%\n # Motivations\n-# ------------\n+# -----------\n+#\n # Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.\n # Let us consider instead the case of a simple (numerically stabilized) softmax operation:\n \n@@ -55,9 +57,11 @@ def naive_softmax(x):\n \n # %%\n # Compute Kernel\n-# ----------------\n+# --------------\n+#\n # Our softmax kernel works as follows: each program loads a row of the input matrix X,\n # normalizes it and writes back the result to the output Y.\n+#\n # Note that one important limitation of Triton is that each block must have a\n # power-of-two number of elements, so we need to internally \"pad\" each row and guard the\n # memory operations properly if we want to handle any possible input shapes:\n@@ -93,6 +97,7 @@ def softmax_kernel(\n # %%\n # We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.\n \n+\n def softmax(x):\n     n_rows, n_cols = x.shape\n     # The block size is the smallest power of two greater than the number of columns in `x`\n@@ -124,7 +129,7 @@ def softmax(x):\n \n # %%\n # Unit Test\n-# ----------\n+# ---------\n \n # %%\n # We make sure that we test our kernel on a matrix with an irregular number of rows and columns.\n@@ -141,7 +146,8 @@ def softmax(x):\n \n # %%\n # Benchmark\n-# -------------\n+# ---------\n+#\n # Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.\n # We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.\n \n@@ -185,7 +191,6 @@ def benchmark(M, N, provider):\n \n # %%\n # In the above plot, we can see that:\n-#\n #  - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.\n #  - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.\n #    Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape."}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 15, "deletions": 13, "changes": 28, "file_content_changes": "@@ -1,10 +1,10 @@\n \"\"\"\n Matrix Multiplication\n-======================\n+=====================\n In this tutorial, you will write a 25-lines high-performance FP16 matrix multiplication\n kernel that achieves performance on par with cuBLAS.\n-You will specifically learn about:\n \n+In doing so, you will learn about:\n - Block-level matrix multiplications\n - Multi-dimensional pointer arithmetic\n - Program re-ordering for improved L2 cache hit rate\n@@ -13,7 +13,8 @@\n \n # %%\n # Motivations\n-# -------------\n+# -----------\n+#\n # Matrix multiplications are a key building block of most modern high-performance computing systems.\n # They are notoriously hard to optimize, hence their implementation is generally done by\n # hardware vendors themselves as part of so-called \"kernel libraries\" (e.g., cuBLAS).\n@@ -42,15 +43,16 @@\n \n # %%\n # Compute Kernel\n-# ----------------\n+# --------------\n #\n # The above algorithm is, actually, fairly straightforward to implement in Triton.\n # The main difficulty comes from the computation of the memory locations at which blocks\n # of :code:`A` and :code:`B` must be read in the inner loop. For that, we need\n-# multi-dimensional pointer arithmetics.\n+# multi-dimensional pointer arithmetic.\n+#\n #\n-# Pointer Arithmetics\n-# ~~~~~~~~~~~~~~~~~~~~\n+# Pointer Arithmetic\n+# ~~~~~~~~~~~~~~~~~~\n #\n # For a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given b\n # y :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\n@@ -81,7 +83,7 @@\n #\n #\n # L2 Cache Optimizations\n-# ~~~~~~~~~~~~~~~~~~~~~~~~\n+# ~~~~~~~~~~~~~~~~~~~~~~\n #\n # As mentioned above, each program instance computes a :code:`[BLOCK_SIZE_M, BLOCK_SIZE_N]`\n # block of :code:`C`.\n@@ -137,8 +139,7 @@\n \n # %%\n # Final Result\n-# -------------\n-#\n+# ------------\n \n import torch\n \n@@ -201,7 +202,7 @@ def matmul_kernel(\n     # and accumulate\n     # a_ptrs is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n     # b_ptrs is a block of [BLOCK_SIZE_K, BLOCK_SIZE_n] pointers\n-    # see above `Pointer Arithmetics` section for details\n+    # see above `Pointer Arithmetic` section for details\n     offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n     offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n     offs_k = tl.arange(0, BLOCK_SIZE_K)\n@@ -281,7 +282,7 @@ def matmul(a, b, activation=None):\n \n # %%\n # Unit Test\n-# -----------\n+# ---------\n #\n # We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS)\n \n@@ -299,10 +300,11 @@ def matmul(a, b, activation=None):\n \n # %%\n # Benchmark\n-# --------------\n+# ---------\n #\n # Square Matrix Performance\n # ~~~~~~~~~~~~~~~~~~~~~~~~~~\n+#\n # We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices, but feel free to arrange this script as you wish to benchmark any other matrix shape.\n \n "}, {"filename": "python/tutorials/04-low-memory-dropout.py", "status": "modified", "additions": 24, "deletions": 20, "changes": 44, "file_content_changes": "@@ -1,18 +1,20 @@\n \"\"\"\n Low-Memory Dropout\n-=================\n+==================\n \n In this tutorial, you will write a memory-efficient implementation of dropout whose state\n will be composed of a single int32 seed. This differs from more traditional implementations of dropout,\n-whose state is generally composed of a bit mask tensor of the same shape as the input. You will learn about:\n+whose state is generally composed of a bit mask tensor of the same shape as the input.\n \n+In doing so, you will learn about:\n - The limitations of naive implementations of Dropout with PyTorch\n - Parallel pseudo-random number generation in Triton\n \"\"\"\n \n # %%\n # Baseline\n-# -------------\n+# --------\n+#\n # The *dropout* operator was first introduced in [SRIVASTAVA2014]_ as a way to improve the performance\n # of deep neural networks in low-data regime (i.e. regularization).\n #\n@@ -37,12 +39,12 @@\n \n @triton.jit\n def _dropout(\n-        x_ptr,  # pointer to the input\n-        x_keep_ptr,  # pointer to a mask of 0s and 1s\n-        output_ptr,  # pointer to the output\n-        n_elements,  # number of elements in the `x` tensor\n-        p,  # probability that an element of `x` is changed to zero\n-        BLOCK_SIZE: tl.constexpr,\n+    x_ptr,  # pointer to the input\n+    x_keep_ptr,  # pointer to a mask of 0s and 1s\n+    output_ptr,  # pointer to the output\n+    n_elements,  # number of elements in the `x` tensor\n+    p,  # probability that an element of `x` is changed to zero\n+    BLOCK_SIZE: tl.constexpr,\n ):\n     pid = tl.program_id(axis=0)\n     block_start = pid * BLOCK_SIZE\n@@ -81,15 +83,16 @@ def dropout(x, x_keep, p):\n \n # %%\n # Seeded dropout\n-# -------------\n-# Above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\n+# --------------\n+#\n+# The above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\n # we need to store the dropout mask for backpropagation. Secondly, dropout state management can get\n # very tricky when using recompute/checkpointing (e.g. see all the notes about `preserve_rng_state` in\n # https://pytorch.org/docs/1.9.0/checkpoint.html). In this tutorial we'll describe an alternative implementation\n # that (1) has a smaller memory footprint; (2) requires less data movement; and (3) simplifies the management\n # of persisting randomness across multiple invocations of the kernel.\n #\n-# Pseudorandom number generation in Triton is simple! In this tutorial we will use the\n+# Pseudo-random number generation in Triton is simple! In this tutorial we will use the\n # :code:`triton.language.rand` function which generates a block of uniformly distributed :code:`float32`\n # values in [0, 1), given a seed and a block of :code:`int32` offsets. But if you need it, Triton also provides\n # other :ref:`random number generation strategies <Random Number Generation>`.\n@@ -102,12 +105,12 @@ def dropout(x, x_keep, p):\n \n @triton.jit\n def _seeded_dropout(\n-        x_ptr,\n-        output_ptr,\n-        n_elements,\n-        p,\n-        seed,\n-        BLOCK_SIZE: tl.constexpr,\n+    x_ptr,\n+    output_ptr,\n+    n_elements,\n+    p,\n+    seed,\n+    BLOCK_SIZE: tl.constexpr,\n ):\n     # compute memory offsets of elements handled by this instance\n     pid = tl.program_id(axis=0)\n@@ -153,14 +156,15 @@ def seeded_dropout(x, p, seed):\n \n # %%\n # Exercises\n-# -------------\n+# ---------\n+#\n # 1. Extend the kernel to operate over a matrix and use a vector of seeds - one per row.\n # 2. Add support for striding.\n # 3. (challenge) Implement a kernel for sparse Johnson-Lindenstrauss transform which generates the projection matrix one the fly each time using a seed.\n \n # %%\n # References\n-# --------------\n+# ----------\n #\n # .. [SALMON2011] John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, \"Parallel Random Numbers: As Easy as 1, 2, 3\", 2011\n # .. [SRIVASTAVA2014] Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", JMLR 2014"}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 11, "deletions": 8, "changes": 19, "file_content_changes": "@@ -3,15 +3,16 @@\n ====================\n In this tutorial, you will write a high-performance layer normalization\n kernel that runs faster than the PyTorch implementation.\n-You will specifically learn about:\n \n-- How to implement backward pass in Triton\n-- How to implement parallel reduction in Triton\n+In doing so, you will learn about:\n+- Implementing backward pass in Triton\n+- Implementing parallel reduction in Triton\n \"\"\"\n \n # %%\n # Motivations\n-# -------------\n+# -----------\n+#\n # The *LayerNorm* operator was first introduced in [BA2016]_ as a way to improve the performance\n # of sequential models (e.g., Transformers) or neural networks with small batch size.\n # It takes a vector :math:`x` as input and produces a vector :math:`y` of the same shape as output.\n@@ -23,7 +24,7 @@\n #    y = \\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} } * w + b\n #\n # where :math:`\\epsilon` is a small constant added to the denominator for numerical stability.\n-# Let\u2019s first take a look at the foward pass implementation.\n+# Let\u2019s first take a look at the forward pass implementation.\n \n import torch\n \n@@ -91,7 +92,8 @@ def _layer_norm_fwd_fused(\n \n # %%\n # Backward pass\n-# ---------------------------------\n+# -------------\n+#\n # The backward pass for the layer normalization operator is a bit more involved than the forward pass.\n # Let :math:`\\hat{x}` be the normalized inputs :math:`\\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} }` before the linear transformation,\n # the Vector-Jacobian Products (VJP) :math:`\\nabla_{x}` of :math:`x` are given by:\n@@ -218,7 +220,8 @@ def _layer_norm_bwd_dwdb(\n \n # %%\n # Benchmark\n-# ---------------------------------\n+# ---------\n+#\n # We can now compare the performance of our kernel against that of PyTorch.\n # Here we focus on inputs that have Less than 64KB per feature.\n # Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward pass.\n@@ -362,6 +365,6 @@ def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='c\n \n # %%\n # References\n-# --------------\n+# ----------\n #\n # .. [BA2016] Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \"Layer Normalization\", Arxiv 2016"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,6 +1,7 @@\n \"\"\"\n Fused Attention\n ===============\n+\n This is a Triton implementation of the Flash Attention algorithm\n (see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n \"\"\""}, {"filename": "python/tutorials/README.rst", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,5 +1,5 @@\n Tutorials\n-==================\n+=========\n \n Below is a gallery of tutorials for writing various basic operations with Triton. It is recommended that you read through the tutorials in order, starting with the simplest one.\n "}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -82,7 +82,7 @@ func.func @div() {\n   %3 = arith.divui %1, %0 : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [64], constancy = [128], constant_value = 64\n   %4 = arith.constant dense<64> : tensor<128xi32>\n-  // CHECK-NEXT: contiguity = [1], divisibility = [16777216], constancy = [64], constant_value = <none>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [64], constant_value = <none>\n   %5 = arith.divsi %0, %4 : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n   %6 = arith.divsi %4, %0 : tensor<128xi32>\n@@ -94,11 +94,12 @@ func.func @div() {\n   %9 = arith.divui %0, %8 : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [128], divisibility = [8192], constancy = [1], constant_value = <none>\n   %10 = tt.make_range {end = 8320 : i32, start = 8192 : i32} : tensor<128xi32>\n-  // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [64], constant_value = <none>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [64], constant_value = <none>\n   %11 = arith.divsi %10, %4 : tensor<128xi32>\n   return\n }\n \n+\n // -----\n \n // CHECK-LABEL: @rem\n@@ -179,11 +180,11 @@ func.func @logic() {\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [64], constancy = [128], constant_value = 64\n   %1 = arith.constant dense<64> : tensor<128xi32>\n-  // CHECK-NEXT: contiguity = [1], divisibility = [16777216], constancy = [64], constant_value = <none>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [64], constant_value = <none>\n   %2 = arith.divsi %0, %1 : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [8], constancy = [128], constant_value = 8\n   %3 = arith.constant dense<8> : tensor<128xi32>\n-  // CHECK-NEXT: contiguity = [1], divisibility = [134217728], constancy = [8], constant_value = <none>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [8], constant_value = <none>\n   %4 = arith.divsi %0, %3 : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n   %5 = arith.andi %0, %1 : tensor<128xi32>"}, {"filename": "test/Triton/combine.mlir", "status": "modified", "additions": 60, "deletions": 0, "changes": 60, "file_content_changes": "@@ -150,3 +150,63 @@ func.func @test_canonicalize_masked_store_fail_pattern(%ptr: tensor<8x!tt.ptr<f3\n     tt.store %ptr, %val, %mask : tensor<8xf32>\n     return\n }\n+\n+// CHECK-LABEL: @test_canonicalize_expand_dims\n+func.func @test_canonicalize_expand_dims(%arg0: tensor<f32>) -> (tensor<1x8xf32>) {\n+    %splat = tt.splat %arg0 : (tensor<f32>) -> tensor<8xf32>\n+    // CHECK: %{{.*}} = tt.splat %arg0 : (tensor<f32>) -> tensor<1x8xf32>\n+    %ed = tt.expand_dims %splat {axis = 0 : i32} : (tensor<8xf32>) -> tensor<1x8xf32>\n+\n+    return %ed : tensor<1x8xf32>\n+}\n+\n+\n+// CHECK-LABEL: @test_canonicalize_view\n+func.func @test_canonicalize_view(%arg0: tensor<8xf32>, %arg1: tensor<f32>) -> (tensor<4x2xf32>, tensor<2x2x2xf32>, tensor<8xf32>) {\n+    %view0 = tt.view %arg0 : (tensor<8xf32>) -> tensor<2x4xf32>\n+    // CHECK: %{{.*}} = tt.view %arg0 : (tensor<8xf32>) -> tensor<4x2xf32>\n+    %view1 = tt.view %view0 : (tensor<2x4xf32>) -> tensor<4x2xf32>\n+\n+    %splat = tt.splat %arg1 : (tensor<f32>) -> tensor<8xf32>\n+    // CHECK: %{{.*}} = tt.splat %arg1 : (tensor<f32>) -> tensor<2x2x2xf32>\n+    %view2 = tt.view %splat : (tensor<8xf32>) -> tensor<2x2x2xf32>\n+\n+    %view3 = tt.view %arg0 : (tensor<8xf32>) -> tensor<8xf32>\n+    // CHECK: %{{.*}} = arith.addf %arg0, %arg0 : tensor<8xf32>\n+    %add = arith.addf %view3, %arg0 : tensor<8xf32>\n+\n+    return %view1, %view2, %add : tensor<4x2xf32>, tensor<2x2x2xf32>, tensor<8xf32>\n+}\n+\n+// CHECK-LABEL: @test_canonicalize_broadcast\n+func.func @test_canonicalize_broadcast(%arg0: tensor<1x1x8xf32>, %arg1: tensor<f32>) -> (tensor<4x2x8xf32>, tensor<8x8xf32>, tensor<1x1x8xf32>) {\n+    %broadcast0 = tt.broadcast %arg0 : (tensor<1x1x8xf32>) -> tensor<1x2x8xf32>\n+    // CHECK: %{{.*}} = tt.broadcast %arg0 : (tensor<1x1x8xf32>) -> tensor<4x2x8xf32>\n+    %broadcast1 = tt.broadcast %broadcast0 : (tensor<1x2x8xf32>) -> tensor<4x2x8xf32>\n+\n+    %splat = tt.splat %arg1 : (tensor<f32>) -> tensor<1x8xf32>\n+    // CHECK: %{{.*}} = tt.splat %arg1 : (tensor<f32>) -> tensor<8x8xf32>\n+    %broadcast2 = tt.broadcast %splat : (tensor<1x8xf32>) -> tensor<8x8xf32>\n+\n+    %broadcast3 = tt.broadcast %arg0 : (tensor<1x1x8xf32>) -> tensor<1x1x8xf32>\n+    // CHECK: %{{.*}} = arith.addf %arg0, %arg0 : tensor<1x1x8xf32>\n+    %add = arith.addf %broadcast3, %arg0 : tensor<1x1x8xf32>\n+\n+    return %broadcast1, %broadcast2, %add : tensor<4x2x8xf32>, tensor<8x8xf32>, tensor<1x1x8xf32>\n+}\n+\n+// CHECK-LABEL: @test_fold_views\n+func.func @test_fold_views() -> (tensor<16x8xf32>, tensor<16x128xf32>, tensor<1x1x128xf32>) {\n+    %a = arith.constant dense<1.0> : tensor<1x128xf32>\n+\n+    // CHECK-DAG: %{{.*}} = arith.constant dense<1.{{.*}}> : tensor<16x8xf32>\n+    %b = tt.view %a : (tensor<1x128xf32>) -> tensor<16x8xf32>\n+\n+    // CHECK-DAG: %{{.*}} = arith.constant dense<1.{{.*}}> : tensor<16x128xf32>\n+    %c = tt.broadcast %a : (tensor<1x128xf32>) -> tensor<16x128xf32>\n+\n+    // CHECK-DAG: %{{.*}} = arith.constant dense<1.{{.*}}> : tensor<1x1x128xf32>\n+    %d = tt.expand_dims %a {axis = 0: i32} : (tensor<1x128xf32>) -> tensor<1x1x128xf32>\n+\n+    return %b, %c, %d : tensor<16x8xf32>, tensor<16x128xf32>, tensor<1x1x128xf32>\n+}"}, {"filename": "test/TritonGPU/prefetch.mlir", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -62,4 +62,3 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16\n   }\n   return\n }\n-"}, {"filename": "test/TritonGPU/update-mma-for-volta.mlir", "status": "removed", "additions": 0, "deletions": 73, "changes": 73, "file_content_changes": "@@ -1,73 +0,0 @@\n-// RUN: triton-opt %s -split-input-file -tritongpu-optimize-dot-operands -tritongpu-update-mma-for-volta 2>&1 | FileCheck %s\n-\n-// -----\n-\n-// check the UpdateMMAVersionMinorForVolta pattern\n-#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [4, 4], order = [1, 0]}>\n-#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n-#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[4,4]}>\n-// Here, the isMMAv1Row of a and b's dot_operands mismatch #mma0's versionMinor,\n-// and the pattern should update the versionMinor.\n-#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n-#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n-// It creates a new MMA layout to fit with $a and $b's dot_operand, and get the right warpsPerCTA\n-// The ID of this MMA instance should be 0.\n-// CHECK: [[$new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 2]}>\n-module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n-  // CHECK-LABEL: dot_mmav1\n-  func.func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n-    %C = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked0>\n-    %AA = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a>\n-    %BB = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b>\n-    %CC = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma0>\n-\n-    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, %cst {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[$new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[$new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[$new_mma]]>\n-    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<64x64xf32, #mma0>\n-    %res = triton_gpu.convert_layout %D : (tensor<64x64xf32, #mma0>) -> tensor<64x64xf32, #blocked0>\n-\n-    return %res : tensor<64x64xf32, #blocked0>\n-  }\n-}\n-\n-\n-// -----\n-// Check id in multiple MMA layout instances\n-\n-#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [4, 4], order = [1, 0]}>\n-#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n-#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[4,4]}>\n-// mma id=1, with all other boolean flags be false, should get a versionMinor of 16(= 1 * 1<<4)\n-#mma1 = #triton_gpu.mma<{versionMajor=1, versionMinor=16, warpsPerCTA=[4,4]}>\n-\n-// Will still get two MMA layouts\n-// CHECK-DAG: [[$new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 2]}>\n-// CHECK-DAG: [[$new_mma1:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 19, warpsPerCTA = [4, 2]}>\n-\n-#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n-#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n-#dot_operand_a1 = #triton_gpu.dot_op<{opIdx=0, parent=#mma1, isMMAv1Row=true}>\n-#dot_operand_b1 = #triton_gpu.dot_op<{opIdx=1, parent=#mma1, isMMAv1Row=false}>\n-\n-module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n-  // CHECK-LABEL: dot_mmav1\n-  func.func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n-    %C = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked0>\n-    %AA = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a>\n-    %BB = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b>\n-    %CC = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma0>\n-\n-    %AA1 = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a1>\n-    %BB1 = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b1>\n-    %CC1 = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma1>\n-\n-    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[$new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[$new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[$new_mma]]>\n-    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[$new_mma1]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[$new_mma1]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[$new_mma1]]>\n-    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<64x64xf32, #mma0>\n-    %D1 = tt.dot %AA1, %BB1, %CC1 {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a1> * tensor<64x64xf16, #dot_operand_b1> -> tensor<64x64xf32, #mma1>\n-    %res = triton_gpu.convert_layout %D : (tensor<64x64xf32, #mma0>) -> tensor<64x64xf32, #blocked0>\n-    %res1 = triton_gpu.convert_layout %D1 : (tensor<64x64xf32, #mma1>) -> tensor<64x64xf32, #blocked0>\n-    %sum = arith.addf %res, %res1 : tensor<64x64xf32, #blocked0>\n-\n-    return %sum : tensor<64x64xf32, #blocked0>\n-  }\n-}"}, {"filename": "unittest/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,4 +1,3 @@\n-\n include (${CMAKE_CURRENT_SOURCE_DIR}/googletest.cmake)\n \n include(GoogleTest)"}]