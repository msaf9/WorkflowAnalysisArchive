[{"filename": "python/src/triton.cc", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "file_content_changes": "@@ -120,6 +120,7 @@ void init_triton_ir(py::module &&m) {\n         // some placeholders\n         self.getOrLoadDialect<mlir::triton::TritonDialect>();\n         self.getOrLoadDialect<mlir::LLVM::LLVMDialect>();\n+        self.getOrLoadDialect<mlir::gpu::GPUDialect>();\n       });\n   // .def(py::init([](){\n   //   mlir::MLIRContext context;\n@@ -1265,7 +1266,13 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::OpBuilder &self, mlir::Type &type) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              return self.create<::mlir::LLVM::UndefOp>(loc, type);\n-           });\n+           })\n+     // Force GPU barrier\n+     .def(\"create_barrier\",\n+          [](mlir::OpBuilder &self) {\n+            auto loc = self.getUnknownLoc();\n+            self.create<mlir::gpu::BarrierOp>(loc);\n+          });\n \n   py::class_<mlir::PassManager>(m, \"pass_manager\")\n       .def(py::init<mlir::MLIRContext *>())"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -72,7 +72,7 @@ def test_gemm_no_scf(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n     for shape in [\n         [64, 128, 128],\n         [128, 128, 128],\n-        [16, 8, 32],\n+        [16, 16, 32],\n         [32, 16, 64],\n         [32, 16, 64],\n     ]"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -983,6 +983,11 @@ def dot(lhs: tl.tensor,\n         allow_tf32: bool,\n         builder: ir.builder) -> tl.tensor:\n     assert lhs.type.is_block() and rhs.type.is_block()\n+    assert len(lhs.shape) == 2 and len(rhs.shape) == 2\n+    assert lhs.shape[1].value == rhs.shape[0].value\n+    assert lhs.shape[0].value >= 16 and lhs.shape[1].value >= 16 \\\n+           and rhs.shape[1].value >= 16,\\\n+        \"small blocks not supported!\"\n     if lhs.type.scalar.is_int():\n         _0 = builder.get_int32(0)\n         ret_scalar_ty = tl.int32\n@@ -1139,7 +1144,7 @@ def max_contiguous(x: tl.tensor, values: List[int]) -> tl.tensor:\n \n \n def debug_barrier(builder: ir.builder) -> tl.tensor:\n-    return tl.tensor(builder.create_barrier(''), tl.void)\n+    return tl.tensor(builder.create_barrier(), tl.void)\n \n \n def printf(prefix: str, args: List[tl.tensor], builder: ir.builder) -> tl.tensor:"}]