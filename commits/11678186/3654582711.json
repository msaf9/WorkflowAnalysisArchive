[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -434,11 +434,11 @@ section 9.7.13.4.1 for more details.\n                      \"Attribute\":$parent), [{\n       if(parent.isa<MmaEncodingAttr>() &&\n          parent.cast<MmaEncodingAttr>().getVersion() == 1){\n-        llvm::errs() << \"DotOperand for MMAv1 must have isMMAv1Row field\\n\";\n+        llvm::report_fatal_error(\"DotOperand for MMAv1 must have isMMAv1Row field\");\n         return {};\n       }\n       Attribute none;\n-      return $_get(context, opIdx, parent, none); \n+      return $_get(context, opIdx, parent, none);\n     }]>\n \n   ];"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 18, "deletions": 10, "changes": 28, "file_content_changes": "@@ -3428,12 +3428,16 @@ Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n   } else if (!isOuter && mmaLayout.getVersion() == 1 &&\n              isHMMA) { // tensor core v1\n     DotOpMmaV1ConversionHelper helper(mmaLayout);\n-    bool isMMAv1Row = dotOperandLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    auto srcSharedLayout = src.getType().cast<RankedTensorType>().getEncoding().cast<SharedEncodingAttr>();\n+    bool isMMAv1Row =\n+        dotOperandLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    auto srcSharedLayout = src.getType()\n+                               .cast<RankedTensorType>()\n+                               .getEncoding()\n+                               .cast<SharedEncodingAttr>();\n \n     // Can only convert [1, 0] to row or [0, 1] to col for now\n-    if((srcSharedLayout.getOrder()[0] == 1 && !isMMAv1Row) ||\n-       (srcSharedLayout.getOrder()[0] == 0 && isMMAv1Row)){\n+    if ((srcSharedLayout.getOrder()[0] == 1 && !isMMAv1Row) ||\n+        (srcSharedLayout.getOrder()[0] == 0 && isMMAv1Row)) {\n       llvm::errs() << \"Unsupported Shared -> DotOperand[MMAv1] conversion\\n\";\n       return Value();\n     }\n@@ -3550,6 +3554,14 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n                        .cast<RankedTensorType>()\n                        .getEncoding()\n                        .cast<MmaEncodingAttr>();\n+  auto ALayout = A.getType()\n+                     .cast<RankedTensorType>()\n+                     .getEncoding()\n+                     .cast<DotOperandEncodingAttr>();\n+  auto BLayout = B.getType()\n+                     .cast<RankedTensorType>()\n+                     .getEncoding()\n+                     .cast<DotOperandEncodingAttr>();\n \n   auto ATensorTy = A.getType().cast<RankedTensorType>();\n   auto BTensorTy = B.getType().cast<RankedTensorType>();\n@@ -3561,12 +3573,8 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n   auto DShape = DTensorTy.getShape();\n   auto wpt = mmaLayout.getWarpsPerCTA();\n \n-  // TODO[Superjomn]: order cannot accessed in DotOp.\n-  SmallVector<unsigned> AOrder({1, 0});\n-  SmallVector<unsigned> BOrder({1, 0});\n-\n-  bool isARow = AOrder[0] != 0;\n-  bool isBRow = BOrder[0] != 0;\n+  bool isARow = ALayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+  bool isBRow = BLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n   bool isAVec4 = !isARow && AShape[isARow] <= 16; // fp16*4 = 16bytes\n   bool isBVec4 = isBRow && BShape[isBRow] <= 16;\n   // TODO[Superjomn]: ld.v4 is not supported."}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -303,9 +303,17 @@ def matmul_kernel(\n     [32, 32, 32, 1, 32, 32, 32, False, False],\n     [128, 32, 32, 1, 128, 32, 32, False, False],\n \n+    [128, 32, 32, 1, 128, 32, 32, True, False],\n+    [128, 32, 32, 1, 128, 32, 32, True, True],\n+\n     # split-K\n     [16, 16, 32, 1, 16, 16, 16, False, False],\n     [64, 64, 128, 1, 64, 64, 32, False, False],\n+\n+    [16, 16, 32, 1, 16, 16, 16, True, False],\n+    [16, 16, 32, 1, 16, 16, 16, True, True],\n+    [64, 64, 128, 1, 64, 64, 32, True, True],\n ])\n def test_gemm_for_mmav1(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n     test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B)\n+"}]