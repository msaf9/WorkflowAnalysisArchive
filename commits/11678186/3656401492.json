[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -2886,11 +2886,15 @@ struct ConvertLayoutOpConversion\n         Value laneIdRem16Div4 = udiv(laneIdRem16, _4);\n         Value laneIdRem16Div4Rem2 = urem(laneIdRem16Div4, _2);\n         Value laneIdRem4Div2 = udiv(urem(laneId, _4), _2);\n+        Value rowWarpOffset = mul(multiDimWarpId[0], _16);\n+        Value colWarpOffset = mul(multiDimWarpId[1], _16);\n         mmaRowIdx[0] =\n             add(add(mul(laneIdDiv16, _8), mul(laneIdRem16Div4Rem2, _4)),\n                 laneIdRem2);\n+        mmaRowIdx[0] = add(mmaRowIdx[0], rowWarpOffset);\n         mmaRowIdx[1] = add(mmaRowIdx[0], _2);\n         mmaColIdx[0] = add(mul(laneIdRem16Div8, _4), mul(laneIdRem4Div2, _2));\n+        mmaColIdx[0] = add(mmaColIdx[0], colWarpOffset);\n         mmaColIdx[1] = add(mmaColIdx[0], _1);\n         mmaColIdx[2] = add(mmaColIdx[0], _8);\n         mmaColIdx[3] = add(mmaColIdx[0], idx_val(9));"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 28, "deletions": 2, "changes": 30, "file_content_changes": "@@ -302,10 +302,36 @@ def matmul_kernel(\n     [32, 16, 32, 1, 32, 16, 32, False, False],\n     [32, 32, 32, 1, 32, 32, 32, False, False],\n     [128, 32, 32, 1, 128, 32, 32, False, False],\n-\n-    # split-K\n+    # # split-K\n     [16, 16, 32, 1, 16, 16, 16, False, False],\n     [64, 64, 128, 1, 64, 64, 32, False, False],\n+    # numWarps > 1\n+    [32, 32, 64, 2, 32, 32, 32, False, False]\n+    [64, 32, 64, 4, 64, 32, 64, False, False],\n+    [128, 64, 128, 4, 128, 64, 128, False, False],\n+    # [16, 16, 16, 16, 16, 16, 16, False, False],  # wpt overflow issue, hang on Volta\n+    # K-Forloop\n+    # [16, 16, 64, 4, 8, 8, 8, False, False],  # Wrap threads\n+    [32, 32, 64, 4, 32, 32, 32, False, False],  # Single shared encoding\n+    # [16, 16, 128, 4, 16, 16, 16, False, False],  # Single shared encoding and small k, hang on Volta\n+    [64, 32, 128, 4, 64, 32, 64, False, False],\n+    [128, 16, 128, 4, 128, 16, 32, False, False],\n+    # [32, 16, 128, 4, 32, 16, 32, False, False], # hang on Volta\n+    [32, 64, 128, 4, 32, 64, 32, False, False],\n+    [32, 128, 256, 4, 32, 128, 64, False, False],\n+    [64, 128, 64, 4, 64, 128, 32, False, False],\n+    [64, 64, 128, 4, 64, 64, 32, False, False],\n+    [128, 128, 64, 4, 128, 128, 32, False, False],\n+    [128, 128, 128, 4, 128, 128, 32, False, False],\n+    [128, 128, 256, 4, 128, 128, 64, False, False],\n+    [128, 256, 128, 4, 128, 256, 32, False, False],\n+    [256, 128, 64, 4, 256, 128, 16, False, False],\n+    [128, 64, 128, 4, 128, 64, 32, False, False],\n+    # [16, 16, 64, 4, 16, 16, 16, False, False], # hang on Volta\n+    [32, 32, 64, 4, 32, 32, 32, False, False],\n+    # # trans\n+    # [128, 64, 128, 4, 128, 64, 32, True, False],\n+    # [128, 64, 128, 4, 128, 64, 32, False, True],\n ])\n def test_gemm_for_mmav1(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n     test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B)"}]