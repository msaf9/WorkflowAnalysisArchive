[{"filename": "python/triton/compiler.py", "status": "modified", "additions": 16, "deletions": 7, "changes": 23, "file_content_changes": "@@ -1202,12 +1202,16 @@ def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: i\n     # retrieve cached shared object if it exists\n     fn_cache_key = make_fn_cache_key(fn.cache_key, signature, configs, constants, num_warps, num_stages)\n     fn_cache_manager = CacheManager(fn_cache_key)\n+    ptx_name = f\"{name}.ptx\"\n     cubin_name = f\"{name}.cubin\"\n     data_name = f\"{name}.json\"\n-    if not fn_cache_manager.has_file(cubin_name) or not fn_cache_manager.has_file(data_name):\n+    if not fn_cache_manager.has_file(cubin_name) or \\\n+       not fn_cache_manager.has_file(data_name) or \\\n+       not fn_cache_manager.has_file(ptx_name):\n         asm, shared, kernel_name = _compile(fn, signature, device, constants, configs[0], num_warps, num_stages, extern_libs, \"cubin\")\n         metadata = {\"name\": kernel_name, \"shared\": shared, \"num_warps\": num_warps, \"num_stages\": num_stages}\n         fn_cache_manager.put(asm[\"cubin\"], cubin_name)\n+        fn_cache_manager.put(asm[\"ptx\"], ptx_name, binary=False)\n         fn_cache_manager.put(json.dumps(metadata), data_name, binary=False)\n \n     return CompiledKernel(name, so_cache_manager._make_path(so_name), fn_cache_manager.cache_dir)\n@@ -1222,18 +1226,23 @@ def __init__(self, fn_name, so_path, cache_dir):\n         mod = importlib.util.module_from_spec(spec)\n         spec.loader.exec_module(mod)\n         self.c_wrapper = getattr(mod, \"launch\")\n-        # initialize cuModule and data\n+        # initialize metadata\n         with open(os.path.join(cache_dir, f\"{fn_name}.json\")) as f:\n             metadata = json.load(f)\n         self.shared = metadata[\"shared\"]\n         self.num_warps = metadata[\"num_warps\"]\n         self.num_stages = metadata[\"num_stages\"]\n-        cu_path = os.path.join(cache_dir, f\"{fn_name}.cubin\")\n+        # initialize asm dict\n+        self.asm = dict()\n+        with open(os.path.join(cache_dir, f\"{fn_name}.cubin\"), \"rb\") as f:\n+            self.asm[\"cubin\"] = f.read()\n+        with open(os.path.join(cache_dir, f\"{fn_name}.ptx\"), \"r\") as f:\n+            self.asm[\"ptx\"] = f.read()\n+        \n         device = torch.cuda.current_device()\n-        with open(cu_path, \"rb\") as cubin:\n-            mod, func, n_regs, n_spills = _triton.code_gen.load_binary(metadata[\"name\"], cubin.read(), self.shared, device)\n-            self.cu_module = mod\n-            self.cu_function = func\n+        mod, func, n_regs, n_spills = _triton.code_gen.load_binary(metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n+        self.cu_module = mod\n+        self.cu_function = func\n \n     def __getitem__(self, grid):\n         def runner(*args, stream=None):"}]