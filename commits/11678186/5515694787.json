[{"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 12, "deletions": 13, "changes": 25, "file_content_changes": "@@ -71,7 +71,7 @@ def nvsmi(attrs):\n         (1024, 1024, 1024): {'float16': 0.281, 'float32': 0.316, 'int8': 0.169},\n         (2048, 2048, 2048): {'float16': 0.598, 'float32': 0.534, 'int8': 0.34},\n         (4096, 4096, 4096): {'float16': 0.741, 'float32': 0.752, 'int8': 0.46},\n-        (8192, 8192, 8192): {'float16': 0.772, 'float32': 0.934, 'int8': 0.51},\n+        (8192, 8192, 8192): {'float16': 0.772, 'float32': 0.816, 'int8': 0.51},\n         # tall-skinny\n         (16, 1024, 1024): {'float16': 0.006, 'float32': 0.009, 'int8': 0.005},\n         (16, 4096, 4096): {'float16': 0.055, 'float32': 0.045, 'int8': 0.026},\n@@ -110,7 +110,7 @@ def test_matmul(M, N, K, dtype_str):\n         a = torch.randn((M, K), dtype=dtype, device='cuda')\n         b = torch.randn((K, N), dtype=dtype, device='cuda')\n     fn = lambda: triton.ops.matmul(a, b)\n-    ms = triton.testing.benchmark(fn)\n+    ms = triton.testing.do_bench_cudagraph(fn)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     print_perf(ms, cur_gpu_util, ref_gpu_util)\n@@ -181,7 +181,7 @@ def test_elementwise(N, dtype_str):\n     y = torch.randn_like(z)\n     grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n     fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n-    ms = triton.testing.benchmark(fn)\n+    ms = triton.testing.do_bench_cudagraph(fn)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     print_perf(ms, cur_gpu_util, ref_gpu_util)\n@@ -195,29 +195,28 @@ def test_elementwise(N, dtype_str):\n flash_attention_data = {\n     \"a100\": {\n         (4, 48, 4096, 64, True, True, 'forward', 'float16'): 0.424,\n-        (4, 48, 4096, 64, True, True, 'backward', 'float16'): 0.201,\n         (4, 48, 4096, 64, True, True, 'forward', 'bfloat16'): 0.379,\n-        (4, 48, 4096, 64, True, True, 'backward', 'bfloat16'): 0.199,\n         (4, 48, 1024, 16, True, True, 'forward', 'float32'): 0.098,\n+        (4, 48, 4096, 64, True, True, 'backward', 'float16'): 0.201,\n+        (4, 48, 4096, 64, True, True, 'backward', 'bfloat16'): 0.199,\n         (4, 48, 1024, 16, True, True, 'backward', 'float32'): 0.087,\n-\n         (4, 48, 4096, 64, True, False, 'forward', 'float16'): 0.240,\n-        (4, 48, 4096, 64, True, False, 'backward', 'float16'): 0.135,\n         (4, 48, 4096, 64, True, False, 'forward', 'bfloat16'): 0.210,\n-        (4, 48, 4096, 64, True, False, 'backward', 'bfloat16'): 0.135,\n         (4, 48, 1024, 16, True, False, 'forward', 'float32'): 0.061,\n+        (4, 48, 4096, 64, True, False, 'backward', 'float16'): 0.135,\n+        (4, 48, 4096, 64, True, False, 'backward', 'bfloat16'): 0.135,\n         (4, 48, 1024, 16, True, False, 'backward', 'float32'): 0.052,\n         (4, 48, 4096, 64, False, True, 'forward', 'float16'): 0.424,\n-        (4, 48, 4096, 64, False, True, 'backward', 'float16'): 0.262,\n         (4, 48, 4096, 64, False, True, 'forward', 'bfloat16'): 0.378,\n-        (4, 48, 4096, 64, False, True, 'backward', 'bfloat16'): 0.254,\n         (4, 48, 1024, 16, False, True, 'forward', 'float32'): 0.099,\n+        (4, 48, 4096, 64, False, True, 'backward', 'float16'): 0.262,\n+        (4, 48, 4096, 64, False, True, 'backward', 'bfloat16'): 0.254,\n         (4, 48, 1024, 16, False, True, 'backward', 'float32'): 0.125,\n         (4, 48, 4096, 64, False, False, 'forward', 'float16'): 0.238,\n-        (4, 48, 4096, 64, False, False, 'backward', 'float16'): 0.158,\n         (4, 48, 4096, 64, False, False, 'forward', 'bfloat16'): 0.211,\n-        (4, 48, 4096, 64, False, False, 'backward', 'bfloat16'): 0.134,\n         (4, 48, 1024, 16, False, False, 'forward', 'float32'): 0.062,\n+        (4, 48, 4096, 64, False, False, 'backward', 'float16'): 0.158,\n+        (4, 48, 4096, 64, False, False, 'backward', 'bfloat16'): 0.134,\n         (4, 48, 1024, 16, False, False, 'backward', 'float32'): 0.075,\n     }\n }\n@@ -251,7 +250,7 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str):\n         o = fn()\n         do = torch.randn_like(o)\n         fn = lambda: o.backward(do, retain_graph=True)\n-    ms = triton.testing.benchmark(fn)\n+    ms = triton.testing.do_bench_cudagraph(fn)\n     # compute flops\n     flops_per_matmul = 2. * Z * H * N_CTX * N_CTX * D_HEAD * 0.5\n     total_flops = 2 * flops_per_matmul"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 4, "deletions": 12, "changes": 16, "file_content_changes": "@@ -16,7 +16,7 @@ def nvsmi(attrs):\n     return ret\n \n \n-def benchmark(fn, rep=20, grad_to_none=None):\n+def do_bench_cudagraph(fn, n_warmup=10, n_repeat=20, grad_to_none=None):\n     import torch\n     \"\"\"\n     Benchmark the runtime of the provided function.\n@@ -32,7 +32,6 @@ def benchmark(fn, rep=20, grad_to_none=None):\n     :param quantiles: Performance percentile to return in addition to the median.\n     :type quantiles: list[float]\n     \"\"\"\n-    n_retries = 10\n     if torch.cuda.current_stream() == torch.cuda.default_stream():\n         raise RuntimeError(\"Cannot capture graph in default stream. Please use side stream in benchmark code.\")\n     # record CUDAGraph\n@@ -47,23 +46,16 @@ def benchmark(fn, rep=20, grad_to_none=None):\n         fn()\n     torch.cuda.synchronize()\n     fn = lambda: g.replay()\n-    # Allocate events for benchmarking\n-    # Estimate the runtime of the function\n-    start_event = torch.cuda.Event(enable_timing=True)\n-    end_event = torch.cuda.Event(enable_timing=True)\n-    start_event.record()\n-    fn()\n-    end_event.record()\n-    torch.cuda.synchronize()\n-    estimate_ms = start_event.elapsed_time(end_event)\n     # compute number of repetition to last `rep` ms\n-    n_repeat = max(1, int(rep / estimate_ms))\n     start_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]\n     end_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]\n     ret = []\n+    n_retries = 1\n     for _ in range(n_retries):\n         # Benchmark\n         torch.cuda.synchronize()\n+        for i in range(n_warmup):\n+            fn()\n         for i in range(n_repeat):\n             # we don't want `fn` to accumulate gradient values\n             # if it contains a backward pass. So we clear the"}]