[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -66,6 +66,7 @@ jobs:\n           python3 -m pip install --upgrade pip\n           python3 -m pip install cmake==3.24\n           python3 -m pip install --no-build-isolation -vvv '.[tests]'\n+          python3 -m pip install pytest-xdist\n \n       - name: Run lit tests\n         if: ${{ env.BACKEND == 'CUDA'}}\n@@ -82,7 +83,9 @@ jobs:\n         if: ${{ env.BACKEND == 'CUDA'}}\n         run: |\n           cd python/test/unit\n-          python3 -m pytest\n+          python3 -m pytest -n 8 --ignore=runtime\n+          # run runtime tests serially to avoid race condition with cache handling.\n+          python3 -m pytest runtime/\n \n       - name: Create artifacts archive\n         if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}"}, {"filename": "python/test/unit/operators/test_cross_entropy.py", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -29,11 +29,13 @@ def test_op(M, N, dtype, mode):\n     # backward pass\n     elif mode == 'backward':\n         dy = torch.randn_like(tt_y)\n-        # triton backward\n-        tt_y.backward(dy)\n-        tt_dx = x.grad.clone()\n+        # run torch first to make sure cuda context is initialized\n         # torch backward\n-        x.grad.zero_()\n         th_y.backward(dy)\n         th_dx = x.grad.clone()\n+        # triton backward\n+        x.grad.zero_()\n+        tt_y.backward(dy)\n+        tt_dx = x.grad.clone()\n+\n         torch.testing.assert_allclose(th_dx, tt_dx)"}]