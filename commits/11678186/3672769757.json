[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 21, "deletions": 1, "changes": 22, "file_content_changes": "@@ -416,15 +416,35 @@ In TritonGPU dialect, considering `d = tt.dot a, b, c`\n tt.dot's operands a and b must be of DotOperandEncodingAttr layout.\n a's opIdx is 0, b's opIdx is 1.\n The parend field in DotOperandEncodingAttr is the layout of d.\n+\n+For MMA v1, an additional attribute `isMMAv1Row` determines whether e.g. the a operand is used\n+in the context of an mma.884.row.col or an mma.884.col.col operation. See the PTX ISA documentation\n+section 9.7.13.4.1 for more details.\n   }];\n \n   let parameters = (\n     ins\n     \"unsigned\":$opIdx,\n-    \"Attribute\":$parent\n+    \"Attribute\":$parent,\n+    \"Attribute\":$isMMAv1Row\n   );\n \n+  let builders = [\n+    AttrBuilder<(ins \"unsigned\":$opIdx,\n+                     \"Attribute\":$parent), [{\n+      Attribute isMMAv1Row;\n+      if(parent.isa<MmaEncodingAttr>() &&\n+         parent.cast<MmaEncodingAttr>().getVersion() == 1){\n+        isMMAv1Row = BoolAttr::get(context, true);\n+      }\n+      return $_get(context, opIdx, parent, isMMAv1Row);\n+    }]>\n+\n+  ];\n+\n   let extraClassDeclaration = extraBaseClassDeclaration;\n }\n \n+\n+\n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 24, "deletions": 6, "changes": 30, "file_content_changes": "@@ -3432,6 +3432,20 @@ Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n   } else if (!isOuter && mmaLayout.getVersion() == 1 &&\n              isHMMA) { // tensor core v1\n     DotOpMmaV1ConversionHelper helper(mmaLayout);\n+    bool isMMAv1Row =\n+        dotOperandLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    auto srcSharedLayout = src.getType()\n+                               .cast<RankedTensorType>()\n+                               .getEncoding()\n+                               .cast<SharedEncodingAttr>();\n+\n+    // Can only convert [1, 0] to row or [0, 1] to col for now\n+    if ((srcSharedLayout.getOrder()[0] == 1 && !isMMAv1Row) ||\n+        (srcSharedLayout.getOrder()[0] == 0 && isMMAv1Row)) {\n+      llvm::errs() << \"Unsupported Shared -> DotOperand[MMAv1] conversion\\n\";\n+      return Value();\n+    }\n+\n     if (dotOperandLayout.getOpIdx() == 0) { // operand $a\n       // TODO[Superjomn]: transA is not available here.\n       bool transA = false;\n@@ -3544,6 +3558,14 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n                        .cast<RankedTensorType>()\n                        .getEncoding()\n                        .cast<MmaEncodingAttr>();\n+  auto ALayout = A.getType()\n+                     .cast<RankedTensorType>()\n+                     .getEncoding()\n+                     .cast<DotOperandEncodingAttr>();\n+  auto BLayout = B.getType()\n+                     .cast<RankedTensorType>()\n+                     .getEncoding()\n+                     .cast<DotOperandEncodingAttr>();\n \n   auto ATensorTy = A.getType().cast<RankedTensorType>();\n   auto BTensorTy = B.getType().cast<RankedTensorType>();\n@@ -3555,12 +3577,8 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n   auto DShape = DTensorTy.getShape();\n   auto wpt = mmaLayout.getWarpsPerCTA();\n \n-  // TODO[Superjomn]: order cannot accessed in DotOp.\n-  SmallVector<unsigned> AOrder({1, 0});\n-  SmallVector<unsigned> BOrder({1, 0});\n-\n-  bool isARow = AOrder[0] != 0;\n-  bool isBRow = BOrder[0] != 0;\n+  bool isARow = ALayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+  bool isBRow = BLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n   bool isAVec4 = !isARow && AShape[isARow] <= 16; // fp16*4 = 16bytes\n   bool isBVec4 = isBRow && BShape[isBRow] <= 16;\n   // TODO[Superjomn]: ld.v4 is not supported."}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 12, "deletions": 3, "changes": 15, "file_content_changes": "@@ -589,15 +589,24 @@ Attribute DotOperandEncodingAttr::parse(AsmParser &parser, Type type) {\n     return {};\n   unsigned opIdx = attrs.get(\"opIdx\").cast<IntegerAttr>().getInt();\n   Attribute parent = attrs.get(\"parent\");\n-\n+  Attribute isMMAv1Row;\n+  if(parent.isa<MmaEncodingAttr>() &&\n+     parent.cast<MmaEncodingAttr>().getVersion() == 1){\n+    isMMAv1Row = attrs.get(\"isMMAv1Row\");\n+    if(!isMMAv1Row)\n+      llvm::report_fatal_error(\"isMMAv1Row attribute is missing\");\n+  }\n   return parser.getChecked<DotOperandEncodingAttr>(parser.getContext(), opIdx,\n-                                                   parent);\n+                                                   parent, isMMAv1Row);\n }\n \n void DotOperandEncodingAttr::print(mlir::AsmPrinter &printer) const {\n   printer << \"<{\"\n           << \"opIdx = \" << getOpIdx() << \", \"\n-          << \"parent = \" << getParent() << \"}>\";\n+          << \"parent = \" << getParent();\n+  if(getIsMMAv1Row())\n+    printer << \", isMMAv1Row = \" << getIsMMAv1Row();      \n+  printer << \"}>\";\n }\n \n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 118, "deletions": 2, "changes": 120, "file_content_changes": "@@ -715,6 +715,55 @@ class OptimizeBlockedToShared : public mlir::RewritePattern {\n   }\n };\n \n+class OptimizeConvertToDotOperand : public mlir::RewritePattern {\n+public:\n+  OptimizeConvertToDotOperand(mlir::MLIRContext *context)\n+      : RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(), 1,\n+                       context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto srcType = cvt.getOperand().getType().cast<RankedTensorType>();\n+    auto dstType = cvt.getResult().getType().cast<RankedTensorType>();\n+    // order\n+    ArrayRef<unsigned> order;\n+    if(auto srcBlockedLayout =\n+        srcType.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>())\n+      order = srcBlockedLayout.getOrder();\n+    else if(auto srcSharedLayout =\n+        srcType.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>())\n+      order = srcSharedLayout.getOrder();\n+    else\n+      return failure();\n+    // dot operand output\n+    auto dstDotOperandLayout =\n+        dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+    if (!dstDotOperandLayout)\n+      return failure();\n+    unsigned opIdx = dstDotOperandLayout.getOpIdx();\n+    if(!dstDotOperandLayout.getIsMMAv1Row())\n+      return failure();\n+    bool isMMAv1Row = dstDotOperandLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    if((order[0] == 1 && isMMAv1Row) ||\n+       (order[0] == 0 && !isMMAv1Row))\n+      return failure();\n+    auto newIsRow = BoolAttr::get(op->getContext(), !isMMAv1Row);\n+    auto newDstEncoding = triton::gpu::DotOperandEncodingAttr::get(\n+        op->getContext(), dstDotOperandLayout.getOpIdx(), dstDotOperandLayout.getParent(),\n+        newIsRow);\n+    auto newDstType = RankedTensorType::get(\n+        dstType.getShape(),\n+        dstType.getElementType(), newDstEncoding);\n+    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        op->getLoc(), newDstType, cvt.getOperand());\n+    rewriter.replaceOp(op, newCvt.getResult());\n+    return success();\n+  }\n+};\n+\n+\n class BlockedToMMA : public mlir::RewritePattern {\n   int computeCapability;\n \n@@ -772,14 +821,28 @@ class BlockedToMMA : public mlir::RewritePattern {\n     Value b = dotOp.b();\n     auto oldAType = a.getType().cast<RankedTensorType>();\n     auto oldBType = b.getType().cast<RankedTensorType>();\n+    auto oldAOrder = oldAType.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>()\n+                              .getParent().cast<triton::gpu::BlockedEncodingAttr>().getOrder();\n+    auto oldBOrder = oldBType.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>()\n+                              .getParent().cast<triton::gpu::BlockedEncodingAttr>().getOrder();\n+    Attribute isMMAv1RowA;\n+    Attribute isMMAv1RowB;\n+    if(version == 1){\n+      isMMAv1RowA = BoolAttr::get(getContext(), oldAOrder[0] == 1);\n+      isMMAv1RowB = BoolAttr::get(getContext(), oldBOrder[0] == 1);\n+    }\n+\n     auto newAType = RankedTensorType::get(\n         oldAType.getShape(), oldAType.getElementType(),\n         triton::gpu::DotOperandEncodingAttr::get(oldAType.getContext(), 0,\n-                                                 newRetType.getEncoding()));\n+                                                 newRetType.getEncoding(),\n+                                                 isMMAv1RowA));\n     auto newBType = RankedTensorType::get(\n         oldBType.getShape(), oldBType.getElementType(),\n         triton::gpu::DotOperandEncodingAttr::get(oldBType.getContext(), 1,\n-                                                 newRetType.getEncoding()));\n+                                                 newRetType.getEncoding(),\n+                                                 isMMAv1RowB));\n+\n     a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), newAType, a);\n     b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), newBType, b);\n     auto newDot = rewriter.create<triton::DotOp>(dotOp.getLoc(), newRetType, a,\n@@ -791,6 +854,51 @@ class BlockedToMMA : public mlir::RewritePattern {\n   }\n };\n \n+class FixupLoop : public mlir::RewritePattern {\n+\n+public:\n+  FixupLoop(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(scf::ForOp::getOperationName(), 2,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto forOp = cast<scf::ForOp>(op);\n+\n+    // Rewrite init argument\n+    SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n+    bool shouldRematerialize = false;\n+    for(size_t i = 0; i < newInitArgs.size(); i++){\n+      auto initArg = newInitArgs[i];\n+      auto regionArg = forOp.getRegionIterArgs()[i];\n+      if(newInitArgs[i].getType() != forOp.getRegionIterArgs()[i].getType()){\n+        shouldRematerialize = true;\n+        break;\n+      }\n+    }\n+    if(!shouldRematerialize)\n+      return failure();\n+    \n+    scf::ForOp newForOp = rewriter.create<scf::ForOp>(\n+        forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n+        forOp.getStep(), newInitArgs);\n+    newForOp->moveBefore(forOp);\n+    rewriter.setInsertionPointToStart(newForOp.getBody());\n+    BlockAndValueMapping mapping;\n+    for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n+      mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n+\n+    for (Operation &op : forOp.getBody()->getOperations()) {\n+      Operation *newOp = rewriter.clone(op, mapping);\n+    }\n+    rewriter.replaceOp(forOp, newForOp.getResults());\n+    return success();\n+\n+\n+  }\n+};\n+\n } // namespace\n \n #define GEN_PASS_CLASSES\n@@ -810,6 +918,7 @@ class TritonGPUCombineOpsPass\n     mlir::RewritePatternSet patterns(context);\n \n     patterns.add<OptimizeBlockedToShared>(context);\n+    patterns.add<OptimizeConvertToDotOperand>(context);\n     patterns.add<SimplifyConversion>(context);\n     patterns.add<DecomposeDotOperand>(context);\n     patterns.add<RematerializeBackward>(context);\n@@ -820,6 +929,13 @@ class TritonGPUCombineOpsPass\n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n       signalPassFailure();\n     }\n+\n+    // llvm::outs() << m << \"\\n\";\n+    mlir::RewritePatternSet loopFixup(context);\n+    loopFixup.add<FixupLoop>(context);\n+    if (applyPatternsAndFoldGreedily(m, std::move(loopFixup)).failed()) {\n+      signalPassFailure();\n+    }\n   }\n };\n "}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 6, "deletions": 10, "changes": 16, "file_content_changes": "@@ -32,7 +32,7 @@ def matmul_no_scf_kernel(\n     (shape, num_warps, trans_a, trans_b)\n     for shape in [\n         [128, 256, 32],\n-        [256, 128, 16],\n+        # [256, 128, 16],\n         [128, 16, 32],\n         [32, 128, 64],\n         [128, 128, 64],\n@@ -43,8 +43,6 @@ def matmul_no_scf_kernel(\n     for trans_b in [False, True]\n ])\n def test_gemm_no_scf(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n-    guard_for_volta(NUM_WARPS, TRANS_A, TRANS_B)\n-\n     SIZE_M, SIZE_N, SIZE_K = SHAPE\n     if (TRANS_A):\n         a = torch.randn((SIZE_K, SIZE_M), device='cuda', dtype=torch.float16).T\n@@ -83,7 +81,7 @@ def test_gemm_no_scf(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n     for trans_b in [False, True]\n ])\n def test_gemm_no_scf_int8(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n-    guard_for_volta(NUM_WARPS, TRANS_A, TRANS_B, is_int8=True)\n+    guard_for_volta(is_int8=True)\n \n     SIZE_M, SIZE_N, SIZE_K = SHAPE\n \n@@ -199,7 +197,6 @@ def get_proper_err(a, b, golden):\n     [128, 64, 128, 4, 128, 64, 32, False, True],\n ])\n def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n-    guard_for_volta(NUM_WARPS, TRANS_A, TRANS_B)\n \n     if (TRANS_A):\n         a = torch.randn((SIZE_K, SIZE_M), device='cuda', dtype=torch.float16).T\n@@ -276,7 +273,7 @@ def matmul_kernel(\n         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n         tl.store(c_ptrs, accumulator, c_mask)\n \n-    guard_for_volta(num_warps, trans_a=False, trans_b=False, is_tf32=allow_tf32)\n+    guard_for_volta(is_tf32=allow_tf32)\n \n     # Configure the pytorch counterpart\n     torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n@@ -302,7 +299,7 @@ def matmul_kernel(\n         torch.testing.assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err))\n \n \n-def guard_for_volta(num_warps, trans_a, trans_b, is_int8=False, is_tf32=False):\n+def guard_for_volta(is_int8=False, is_tf32=False):\n     '''\n     Tell whether the test case is valid on Volta GPU.\n     Some features are WIP, so the corresponding support are missing.\n@@ -311,8 +308,7 @@ def guard_for_volta(num_warps, trans_a, trans_b, is_int8=False, is_tf32=False):\n     is_on_Volta = capability[0] < 8\n     # TODO[Superjomn]: Remove the constraints below when features are ready\n     is_feature_supported = not (is_int8 or is_tf32)\n-    is_feature_ready = not (trans_a or trans_b)\n \n     if is_on_Volta:\n-        if (not is_feature_supported) or (not is_feature_ready):\n-            pytest.skip(\"Not valid on Volta\")\n+        if (not is_feature_supported):\n+            pytest.skip(\"Not valid on Volta\")\n\\ No newline at end of file"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -1385,18 +1385,18 @@ def make_hash(fn, **kwargs):\n \n # def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, configs=None):\n def compile(fn, **kwargs):\n+    capability = torch.cuda.get_device_capability()\n+    capability = capability[0] * 10 + capability[1]\n     # we get the kernel, i.e. the first function generated in the module\n     # if fn is not a JITFunction, then it\n     # has to be a path to a file\n     context = _triton.ir.context()\n     asm = dict()\n     constants = kwargs.get(\"constants\", dict())\n     num_warps = kwargs.get(\"num_warps\", 4)\n-    num_stages = kwargs.get(\"num_stages\", 3)\n+    num_stages = kwargs.get(\"num_stages\", 3 if capability >= 75 else 2)\n     extern_libs = kwargs.get(\"extern_libs\", dict())\n     device = kwargs.get(\"device\", torch.cuda.current_device())\n-    capability = torch.cuda.get_device_capability()\n-    capability = capability[0] * 10 + capability[1]\n     # build compilation stages\n     stages = {\n         \"ast\": (lambda path: fn, None),"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -879,8 +879,8 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n #mma = #triton_gpu.mma<{version = 1, warpsPerCTA = [2, 2]}>\n-#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n-#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma, isMMAv1Row=true}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma, isMMAv1Row=true}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   func @matmul884_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<128x32xf16, #shared>, %b:tensor<32x256xf16, #shared>) {"}]