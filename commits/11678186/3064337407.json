[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 19, "deletions": 13, "changes": 32, "file_content_changes": "@@ -29,22 +29,26 @@ def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape, NoSideEffec\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Cast int64 to pointer\";\n \n-    let arguments = (ins I64Tensor:$from);\n+    let arguments = (ins TT_I64Like:$from);\n \n-    let results = (outs TT_PtrTensor:$result);\n+    let results = (outs TT_PtrLike:$result);\n+\n+    let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n }\n \n def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape, NoSideEffect,\n-                                      /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n+                                         /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Cast pointer to int64\";\n \n-    let arguments = (ins TT_PtrTensor:$from);\n+    let arguments = (ins TT_PtrLike:$from);\n \n-    let results = (outs I64Tensor:$result);\n+    let results = (outs TT_I64Like:$result);\n+\n+    let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n }\n \n def TT_FpToFp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape, NoSideEffect,\n-                                /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n+                                   /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Floating point casting for custom types\";\n \n     let description = [{\n@@ -54,9 +58,11 @@ def TT_FpToFp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape, NoSideEffect,\n         BF8 <-> F8, FP16, FP32\n     }];\n \n-    let arguments = (ins TT_FloatTensor:$from);\n+    let arguments = (ins TT_FloatLike:$from);\n+\n+    let results = (outs TT_FloatLike:$result);\n \n-    let results = (outs TT_FloatTensor:$result);\n+    let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n \n     // TODO: We need a verifier here.\n }\n@@ -127,16 +133,16 @@ def TT_StoreOp : TT_Op<\"store\",\n     let hasCanonicalizer = 1;\n }\n \n-def TT_GEPOp : TT_Op<\"getelementptr\",\n+def TT_AddPtrOp : TT_Op<\"addptr\",\n                      [NoSideEffect, SameOperandsAndResultShape,\n                       TypesMatchWith<\"result type matches ptr type\",\n                                      \"result\", \"ptr\", \"$_self\">,\n                       TypesMatchWith<\"result shape matches offset shape\",\n                                      \"result\", \"offset\",\n                                      \"getI32SameShape($_self)\">]> {\n-    let arguments = (ins TT_PtrTensor:$ptr, I32Tensor:$offset);\n+    let arguments = (ins TT_PtrLike:$ptr, TT_I32Like:$offset);\n \n-    let results = (outs TT_PtrTensor:$result);\n+    let results = (outs TT_PtrLike:$result);\n \n     let assemblyFormat = \"$ptr `,` $offset attr-dict `:` type($result)\";\n }\n@@ -278,7 +284,7 @@ def TT_AtomicCASOp : TT_Op<\"atomic_cas\"> {\n         return $old\n     }];\n \n-    let arguments = (ins TT_Pointer:$ptr, TT_Type:$cmp, TT_Type:$val);\n+    let arguments = (ins TT_Ptr:$ptr, TT_Type:$cmp, TT_Type:$val);\n \n     let results = (outs TT_Type:$result);\n }\n@@ -318,7 +324,7 @@ def TT_MakeRangeOp : TT_Op<\"make_range\", [NoSideEffect]> {\n \n     let arguments = (ins I32Attr:$start, I32Attr:$end);\n \n-    let results = (outs TT_IntegerTensor:$result);\n+    let results = (outs TT_IntTensor:$result);\n \n     let assemblyFormat = \"attr-dict `:` type($result)\";\n }"}, {"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "modified", "additions": 26, "deletions": 8, "changes": 34, "file_content_changes": "@@ -12,18 +12,36 @@ class TritonTypeDef<string name, string _mnemonic>\n     let mnemonic = _mnemonic;\n }\n \n+// Floating-point Type\n def F8 : TritonTypeDef<\"Float8\", \"f8\">;\n def BF8 : TritonTypeDef<\"BFloat8\", \"bf8\">;\n \n def TT_Float : AnyTypeOf<[F16, BF16, F32, F64], \"floating-point\">;\n def TT_FloatTensor : TensorOf<[TT_Float]>;\n+def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;\n \n-// IntegerType\n+// Boolean Type\n+// TT_Bool -> I1\n+def TT_BoolTensor : TensorOf<[I1]>;\n+def TT_BoolLike : AnyTypeOf<[I1, TT_BoolTensor]>;\n+\n+// Integer Type\n def TT_Int : AnyTypeOf<[I1, I8, I16, I32, I64], \"integer\">;\n-def TT_IntegerTensor : TensorOf<[TT_Int]>;\n+def TT_IntTensor : TensorOf<[TT_Int]>;\n+def TT_IntLike : AnyTypeOf<[TT_Int, TT_IntTensor]>;\n+\n+// I32 Type\n+// TT_I32 -> I32\n+// TT_I32Tensor -> I32Tensor\n+def TT_I32Like: AnyTypeOf<[I32, I32Tensor]>;\n+\n+// I64 Type\n+// TT_I64 -> I64\n+// TT_I64Tensor -> I64Tensor\n+def TT_I64Like: AnyTypeOf<[I64, I64Tensor]>;\n \n-// PointerType\n-def TT_Pointer : TritonTypeDef<\"Pointer\", \"ptr\"> {\n+// Pointer Type\n+def TT_Ptr : TritonTypeDef<\"Pointer\", \"ptr\"> {\n     let summary = \"pointer type\";\n \n     let description = [{\n@@ -43,12 +61,12 @@ def TT_Pointer : TritonTypeDef<\"Pointer\", \"ptr\"> {\n \n     let skipDefaultBuilders = 1;\n }\n-def TT_PtrTensor : TensorOf<[TT_Pointer]>;\n+def TT_PtrTensor : TensorOf<[TT_Ptr]>;\n+def TT_PtrLike : AnyTypeOf<[TT_Ptr, TT_PtrTensor]>;\n \n-def TT_FpIntTensor : AnyTypeOf<[TT_FloatTensor, TT_IntegerTensor]>;\n+def TT_FpIntTensor : AnyTypeOf<[TT_FloatTensor, TT_IntTensor]>;\n def TT_Tensor : AnyTypeOf<[TT_FpIntTensor, TT_PtrTensor]>;\n \n-def TT_Type : AnyTypeOf<[TT_Float, TT_FloatTensor, TT_Int, TT_IntegerTensor, \n-                         TT_Pointer, TT_PtrTensor]>;\n+def TT_Type : AnyTypeOf<[TT_FloatLike, TT_IntLike, TT_PtrLike]>;\n \n #endif"}, {"filename": "include/triton/Dialect/Triton/Transforms/Passes.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -8,7 +8,7 @@ def TritonCombineOps : Pass</*cli-arg*/\"triton-combine\", /*Op*/\"mlir::ModuleOp\">\n   let description = [{\n     dot(a, b, 0) + c => dot(a, b, c)\n \n-    gep(gep(ptr, idx0), idx1) => gep(ptr, AddI(idx0, idx1))\n+    addptr(addptr(ptr, idx0), idx1) => addptr(ptr, AddI(idx0, idx1))\n \n     select(cond, load(ptrs, broadcast(cond), ???), other) =>\n         load(ptrs, broadcast(cond), other)"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 5, "deletions": 11, "changes": 16, "file_content_changes": "@@ -10,12 +10,6 @@ include \"mlir/IR/OpBase.td\"\n include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n \n-def TT_BoolTensor : TensorOf<[I1]>;\n-\n-def TT_BoolLike : AnyTypeOf<[I1, TT_BoolTensor]>;\n-def TT_IntegerLike : AnyTypeOf<[TT_Int, TT_IntegerTensor]>;\n-def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;\n-\n class TTG_Op<string mnemonic, list<Trait> traits = []> :\n     Op<TritonGPU_Dialect, mnemonic, traits>;\n \n@@ -48,8 +42,8 @@ def TTG_CmpIOp : TTG_Op<\"cmpi\", [NoSideEffect]> {\n   let description = [{}];\n \n   let arguments = (ins Arith_CmpIPredicateAttr:$predicate,\n-                       TT_IntegerLike:$lhs,\n-                       TT_IntegerLike:$rhs);\n+                       TT_IntLike:$lhs,\n+                       TT_IntLike:$rhs);\n \n   let results = (outs TT_BoolLike:$result);\n }\n@@ -66,7 +60,7 @@ def TTG_CmpFOp : TTG_Op<\"cmpf\"> {\n   let results = (outs TT_BoolLike:$result);\n }\n \n-def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\", \n+def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n                                     [SameVariadicOperandSize,\n                                      MemoryEffects<[MemRead, MemWrite]>,\n                                      TypesMatchWith<\"infer mask type from src type\",\n@@ -94,7 +88,7 @@ def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n       * other: optional tensor-rank number of other tensors which specify what\n               values are inserted into the `$dst` tensor if the corresponding\n               element of the `$mask` tensor is false.\n-      \n+\n       In the future, we may decompose this operation into a sequence of:\n \n       * `async` operation to specify a sequence of asynchronous operations\n@@ -191,7 +185,7 @@ def TTG_AllocTensorOp : TTG_Op<\"alloc_tensor\", [NoSideEffect]> {\n     Note: This op can be repalced to a `bufferization.alloc_tensor` in LLVM 16.\n   }];\n \n-  let assemblyFormat = [{attr-dict `:` type($result)}]; \n+  let assemblyFormat = [{attr-dict `:` type($result)}];\n \n   let results = (outs TT_Tensor:$result);\n "}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -126,7 +126,7 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     }\n   }\n   // Addition\n-  if (llvm::isa<arith::AddIOp, triton::GEPOp>(op)) {\n+  if (llvm::isa<arith::AddIOp, triton::AddPtrOp>(op)) {\n     auto newContiguity = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n       return std::max(gcd(lhs.getContiguity(d), rhs.getConstancy(d)),\n                       gcd(lhs.getConstancy(d), rhs.getContiguity(d)));"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -701,7 +701,7 @@ struct StoreOpConversion\n \n     const int numVecs = numElems / vec;\n     for (size_t vecStart = 0; vecStart < numElems; vecStart += vec) {\n-      // TODO: optimization when ptr is GEP with constant offset\n+      // TODO: optimization when ptr is AddPtr with constant offset\n       size_t in_off = 0;\n \n       const int maxWordWidth = std::max<int>(32, valueElemNbits);\n@@ -1173,12 +1173,13 @@ struct GetProgramIdOpConversion\n   }\n };\n \n-struct GEPOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::GEPOp> {\n+struct AddPtrOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::AddPtrOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n-      triton::GEPOp>::ConvertTritonGPUOpToLLVMPattern;\n+      triton::AddPtrOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   LogicalResult\n-  matchAndRewrite(triton::GEPOp op, OpAdaptor adaptor,\n+  matchAndRewrite(triton::AddPtrOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n     auto resultTy = op.getType().dyn_cast<RankedTensorType>();\n@@ -1298,7 +1299,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n \n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n   patterns.add<FuncOpConversion>(typeConverter, numWarps, benefit);\n-  patterns.add<GEPOpConversion>(typeConverter, benefit);\n+  patterns.add<AddPtrOpConversion>(typeConverter, benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n   patterns.add<LoadOpConversion>(typeConverter, analysis, benefit);\n   patterns.add<MakeRangeOpConversion>(typeConverter, benefit);"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -323,7 +323,7 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n   patterns.add< // TODO: view should have custom pattern that views the layout\n       TritonGenericPattern<triton::ViewOp>,\n       TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n-      TritonGenericPattern<triton::GEPOp>, TritonReducePattern,\n+      TritonGenericPattern<triton::AddPtrOp>, TritonReducePattern,\n       TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n       TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern>(\n       typeConverter, context);"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -15,15 +15,15 @@ static Type getI1SameShape(Type type) {\n   if (auto tensorType = type.dyn_cast<RankedTensorType>())\n     return RankedTensorType::get(tensorType.getShape(), i1Type,\n                                  tensorType.getEncoding());\n-  return Type();\n+  return i1Type;\n }\n \n static Type getI32SameShape(Type type) {\n   auto i32Type = IntegerType::get(type.getContext(), 32);\n   if (auto tensorType = type.dyn_cast<RankedTensorType>())\n     return RankedTensorType::get(tensorType.getShape(), i32Type,\n                                  tensorType.getEncoding());\n-  return Type();\n+  return i32Type;\n }\n \n static Type getPointerTypeFromTensor(Type type) {"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -194,7 +194,7 @@ class CombineOpsPass : public TritonCombineOpsBase<CombineOpsPass> {\n     patterns.add<CombineDotAddFRevPattern>(context);\n     // %}\n     patterns.add<CombineSelectMaskedLoadPattern>(context);\n-    patterns.add<CombineGEPPattern>(context);\n+    patterns.add<CombineAddPtrPattern>(context);\n     patterns.add<CombineBroadcastConstantPattern>(context);\n \n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.td", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -30,12 +30,12 @@ def CombineDotAddFRevPattern : Pat<\n         [(Constraint<CPred<\"isZero($0)\">> $c)]>;\n \n \n-// gep(gep(%ptr, %idx0), %idx1) => gep(%ptr, AddI(%idx0, %idx1))\n+// addptr(addptr(%ptr, %idx0), %idx1) => addptr(%ptr, AddI(%idx0, %idx1))\n //   Note: leave (sub %c0, %c0) canceling to ArithmeticDialect\n //         (ref: ArithmeticCanonicalization.td)\n-def CombineGEPPattern : Pat<\n-        (TT_GEPOp (TT_GEPOp $ptr, $idx0), $idx1),\n-        (TT_GEPOp $ptr, (Arith_AddIOp $idx0, $idx1))>;\n+def CombineAddPtrPattern : Pat<\n+        (TT_AddPtrOp (TT_AddPtrOp $ptr, $idx0), $idx1),\n+        (TT_AddPtrOp $ptr, (Arith_AddIOp $idx0, $idx1))>;\n \n // broadcast(cst) => cst\n def getConstantValue : NativeCodeCall<\"getConstantValue($_builder, $0, $1)\">;"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -203,7 +203,7 @@ bool tryLegalizeOp(Operation *op, DenseSet<Value> toPreserve,\n                                  targetType.getEncoding());\n   };\n   bool hasSameTypes = op->getDialect()->getNamespace() == \"arith\" ||\n-                      isa<triton::SplatOp, triton::GEPOp>(op);\n+                      isa<triton::SplatOp, triton::AddPtrOp>(op);\n   if (hasSameTypes) {\n     // replace argument types\n     for (auto arg : llvm::enumerate(op->getOperands())) {\n@@ -440,4 +440,4 @@ class TritonGPUCombineOpsPass\n \n std::unique_ptr<Pass> mlir::createTritonGPUCombineOpsPass() {\n   return std::make_unique<TritonGPUCombineOpsPass>();\n-}\n\\ No newline at end of file\n+}"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Verifier.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -70,7 +70,7 @@ class TritonGPUVerifier : public TritonGPUVerifierBase<TritonGPUVerifier> {\n     if (auto storeOp = llvm::dyn_cast<triton::StoreOp>(op)) {\n       // TODO: fill this\n     }\n-    if (auto gepOp = llvm::dyn_cast<triton::GEPOp>(op)) {\n+    if (auto addptrOp = llvm::dyn_cast<triton::AddPtrOp>(op)) {\n       // TODO: fill this\n     }\n     // Triton builtin Ops"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1245,13 +1245,13 @@ void init_triton_ir(py::module &&m) {\n              return mlir::Value(\n                  self.create<mlir::arith::ShRSIOp>(loc, lhs, rhs));\n            })\n-      // GEP\n-      .def(\"create_gep\",\n+      // AddPtr (similar to GEP)\n+      .def(\"create_addptr\",\n            [](mlir::OpBuilder &self, mlir::Value &ptr,\n               mlir::Value &offset) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::GEPOp>(loc, ptr.getType(), ptr,\n-                                                     offset);\n+             return self.create<mlir::triton::AddPtrOp>(loc, ptr.getType(), ptr,\n+                                                        offset);\n            })\n       // Comparison (int)\n       .def(\"create_icmpSLE\","}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -121,7 +121,7 @@ def add(input: tl.tensor,\n     if other_scalar_ty.is_ptr() and not input_scalar_ty.is_ptr():\n         input, other = other, input\n     if input_scalar_ty.is_ptr():\n-        return tl.tensor(builder.create_gep(input.handle, other.handle), input.type)\n+        return tl.tensor(builder.create_addptr(input.handle, other.handle), input.type)\n     # float + float\n     elif input_scalar_ty.is_floating():\n         return tl.tensor(builder.create_fadd(input.handle, other.handle), input.type)\n@@ -138,7 +138,7 @@ def sub(input: tl.tensor,\n     scalar_ty = input.type.scalar\n     # ptr - offset\n     if scalar_ty.is_ptr():\n-        return tl.tensor(builder.create_gep(input.handle, minus(other, builder).handle),\n+        return tl.tensor(builder.create_addptr(input.handle, minus(other, builder).handle),\n                          input.type)\n     # float - float\n     if scalar_ty.is_floating():"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -26,8 +26,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.getelementptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.getelementptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return"}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -18,21 +18,21 @@ func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {t\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n   %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 1]\n-  %6 = tt.getelementptr %5, %4 : tensor<128x1x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %4 : tensor<128x1x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [1, 1]\n   %7 = tt.expand_dims %1 {axis = 0 : i32}: (tensor<128xi32>) -> tensor<1x128xi32>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128]\n   %8 = tt.broadcast %6 : (tensor<128x1x!tt.ptr<f32>>) -> tensor<128x128x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [128, 1]\n   %9 = tt.broadcast %7 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n   // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 16] ; Constancy: [1, 1]\n-  %10 = tt.getelementptr %8, %9 : tensor<128x128x!tt.ptr<f32>>\n+  %10 = tt.addptr %8, %9 : tensor<128x128x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [65536, 1] ; Constancy: [1, 1]\n   %11 = tt.expand_dims %0 {axis = 1 : i32}: (tensor<128xi32>) -> tensor<128x1xi32>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n   %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1]\n-  %13 = tt.getelementptr %12, %11 : tensor<128x1x!tt.ptr<f32>>\n+  %13 = tt.addptr %12, %11 : tensor<128x1x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [1, 1]\n   %14 = tt.expand_dims %1 {axis = 0 : i32} : (tensor<128xi32>) -> tensor<1x128xi32>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128]\n@@ -44,7 +44,7 @@ func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {t\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 1048576] ; Constancy: [128, 1]\n   %18 = tt.broadcast %16 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n   // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1]\n-  %19 = tt.getelementptr %17, %18 : tensor<128x128x!tt.ptr<f32>>\n+  %19 = tt.addptr %17, %18 : tensor<128x128x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1]\n   %20 = tt.load %10, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf32>\n   tt.store %19, %20, %cst : tensor<128x128xf32>"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -30,8 +30,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n \n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.getelementptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.getelementptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -28,8 +28,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n     // CHECK: Membar 13\n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.getelementptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.getelementptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "added", "additions": 55, "deletions": 0, "changes": 55, "file_content_changes": "@@ -0,0 +1,55 @@\n+// RUN: triton-opt %s | FileCheck %s\n+\n+func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n+  // scalar -> scalar\n+  // CHECK:  i64 -> !tt.ptr<f32>\n+  %0 = tt.int_to_ptr %scalar_i64 : i64 -> !tt.ptr<f32>\n+  // CHECK: !tt.ptr<f32> -> i64\n+  %1 = tt.ptr_to_int %scalar_ptr : !tt.ptr<f32> -> i64\n+  // CHECK: f32 -> f16\n+  %2 = tt.fp_to_fp %scalar_f32 : f32 -> f16\n+\n+  // 0D tensor -> 0D tensor\n+  %tensor_ptr_0d = tt.splat %scalar_ptr : (!tt.ptr<f32>) -> tensor<!tt.ptr<f32>>\n+  %tensor_f32_0d = tt.splat %scalar_f32 : (f32) -> tensor<f32>\n+  %tensor_i64_0d = tt.splat %scalar_i64 : (i64) -> tensor<i64>\n+\n+  // CHECK: tensor<i64> -> tensor<!tt.ptr<f32>>\n+  %3 = tt.int_to_ptr %tensor_i64_0d : tensor<i64> -> tensor<!tt.ptr<f32>>\n+  // CHECK: tensor<!tt.ptr<f32>> -> tensor<i64>\n+  %4 = tt.ptr_to_int %tensor_ptr_0d : tensor<!tt.ptr<f32>> -> tensor<i64>\n+  // CHECK: tensor<f32> -> tensor<f16>\n+  %5 = tt.fp_to_fp %tensor_f32_0d : tensor<f32> -> tensor<f16>\n+\n+  // 1D tensor -> 1D tensor\n+  %tensor_ptr_1d = tt.splat %scalar_ptr : (!tt.ptr<f32>) -> tensor<16x!tt.ptr<f32>>\n+  %tensor_f32_1d = tt.splat %scalar_f32 : (f32) -> tensor<16xf32>\n+  %tensor_i64_1d = tt.splat %scalar_i64 : (i64) -> tensor<16xi64>\n+\n+  // CHECK: tensor<16xi64> -> tensor<16x!tt.ptr<f32>>\n+  %6 = tt.int_to_ptr %tensor_i64_1d : tensor<16xi64> -> tensor<16x!tt.ptr<f32>>\n+  // CHECK: tensor<16x!tt.ptr<f32>> -> tensor<16xi64>\n+  %7 = tt.ptr_to_int %tensor_ptr_1d : tensor<16x!tt.ptr<f32>> -> tensor<16xi64>\n+  // CHECK: tensor<16xf32> -> tensor<16xf16>\n+  %8 = tt.fp_to_fp %tensor_f32_1d : tensor<16xf32> -> tensor<16xf16>\n+  return\n+}\n+\n+func @addptr_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_i32: i32) {\n+  // scalar -> scalar\n+  // CHECK: !tt.ptr<f32>\n+  %0 = tt.addptr %scalar_ptr, %scalar_i32 : !tt.ptr<f32>\n+\n+  // 0D tensor -> 0D tensor\n+  %tensor_ptr_0d = tt.splat %scalar_ptr : (!tt.ptr<f32>) -> tensor<!tt.ptr<f32>>\n+  %tensor_i32_0d = tt.splat %scalar_i32 : (i32) -> tensor<i32>\n+  // CHECK: tensor<!tt.ptr<f32>>\n+  %1 = tt.addptr %tensor_ptr_0d, %tensor_i32_0d : tensor<!tt.ptr<f32>>\n+\n+  // 1D tensor -> 1D tensor\n+  %tensor_ptr_1d = tt.splat %scalar_ptr : (!tt.ptr<f32>) -> tensor<16x!tt.ptr<f32>>\n+  %tensor_i32_1d = tt.splat %scalar_i32 : (i32) -> tensor<16xi32>\n+  // CHECK: tensor<16x!tt.ptr<f32>>\n+  %2 = tt.addptr %tensor_ptr_1d, %tensor_i32_1d : tensor<16x!tt.ptr<f32>>\n+  return\n+}"}, {"filename": "test/Conversion/triton_to_tritongpu.mlir", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "test/Conversion/ops.mlir"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -82,17 +82,17 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n     %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n     %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %6 = tt.getelementptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n     %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %8 = tt.getelementptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n \n     // CHECK: ld.global.v4.b32\n     %9 = tt.load %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n     // CHECK: ld.global.v4.b32\n     %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n     %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n     %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %13 = tt.getelementptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n \n     // Store 4 elements to global\n     // CHECK: st.global.b32.v4\n@@ -104,7 +104,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n \n \n // TODO: Add a testcase to verify the optimization when ptr of the LoadOp\n-//       is from a GEP with const idx\n+//       is from an addptr with const idx\n \n // -----\n \n@@ -187,11 +187,11 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  // CHECK-LABEL: basic_gep\n-  func @basic_gep(%arg0 : tensor<256x!tt.ptr<f32>,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n+  // CHECK-LABEL: basic_addptr\n+  func @basic_addptr(%arg0 : tensor<256x!tt.ptr<f32>,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n     // CHECK: llvm.getelementptr\n     // CHECK: llvm.getelementptr\n-    %0 = tt.getelementptr %arg0, %arg1 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %0 = tt.addptr %arg0, %arg1 : tensor<256x!tt.ptr<f32>, #blocked0>\n     return\n   }\n }"}, {"filename": "test/Triton/combine.mlir", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -22,8 +22,8 @@ func @test_combine_dot_add_pattern() -> (tensor<128x128xf32>, tensor<128x128xf32\n     return %res0, %res1 : tensor<128x128xf32>, tensor<128x128xf32>\n }\n \n-// CHECK-LABEL: @test_combine_gep_pattern\n-func @test_combine_gep_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>> {\n+// CHECK-LABEL: @test_combine_addptr_pattern\n+func @test_combine_addptr_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>> {\n     %off0 = arith.constant 10 : i32\n     %off1 = arith.constant 15 : i32\n \n@@ -37,9 +37,9 @@ func @test_combine_gep_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>> {\n     %idx0 = tt.broadcast %off0 : (i32) -> tensor<8xi32>\n     %idx1 = tt.broadcast %off1 : (i32) -> tensor<8xi32>\n \n-    // CHECK-NEXT: %1 = tt.getelementptr %[[tmp0]], %[[cst]] : tensor<8x!tt.ptr<f32>>\n-    %ptr0 = tt.getelementptr %base_, %idx0 : tensor<8x!tt.ptr<f32>>\n-    %ptr1 = tt.getelementptr %ptr0, %idx1 : tensor<8x!tt.ptr<f32>>\n+    // CHECK-NEXT: %1 = tt.addptr %[[tmp0]], %[[cst]] : tensor<8x!tt.ptr<f32>>\n+    %ptr0 = tt.addptr %base_, %idx0 : tensor<8x!tt.ptr<f32>>\n+    %ptr1 = tt.addptr %ptr0, %idx1 : tensor<8x!tt.ptr<f32>>\n \n     return %ptr1 : tensor<8x!tt.ptr<f32>>\n }"}, {"filename": "test/Triton/vecadd.mlir", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "file_content_changes": "@@ -11,9 +11,9 @@ module {\n     %5 = tt.broadcast %arg3 : (i32) -> tensor<256xi32>\n     %6 = arith.cmpi slt, %4, %5 : tensor<256xi32>\n     %7 = tt.broadcast %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n-    %8 = tt.getelementptr %7, %4 : tensor<256x!tt.ptr<f32>>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>>\n     %9 = tt.broadcast %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n-    %10 = tt.getelementptr %9, %4 : tensor<256x!tt.ptr<f32>>\n+    %10 = tt.addptr %9, %4 : tensor<256x!tt.ptr<f32>>\n     %cst = arith.constant 0.000000e+00 : f32\n     %11 = tt.broadcast %cst : (f32) -> tensor<256xf32>\n     %c0_i32 = arith.constant 0 : i32\n@@ -31,13 +31,13 @@ module {\n       %22 = arith.addf %19, %21 : tensor<256xf32>\n       %23 = arith.addf %arg7, %22 : tensor<256xf32>\n       %24 = tt.broadcast %arg5 : (i32) -> tensor<256xi32>\n-      %25 = tt.getelementptr %arg8, %24 : tensor<256x!tt.ptr<f32>>\n+      %25 = tt.addptr %arg8, %24 : tensor<256x!tt.ptr<f32>>\n       %26 = tt.broadcast %arg5 : (i32) -> tensor<256xi32>\n-      %27 = tt.getelementptr %arg9, %26 : tensor<256x!tt.ptr<f32>>\n+      %27 = tt.addptr %arg9, %26 : tensor<256x!tt.ptr<f32>>\n       scf.yield %23, %25, %27 : tensor<256xf32>, tensor<256x!tt.ptr<f32>>, tensor<256x!tt.ptr<f32>>\n     }\n     %16 = tt.broadcast %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n-    %17 = tt.getelementptr %16, %4 : tensor<256x!tt.ptr<f32>>\n+    %17 = tt.addptr %16, %4 : tensor<256x!tt.ptr<f32>>\n     tt.store %17, %15#0, %6 : tensor<256xf32>\n     return\n   }\n@@ -57,9 +57,9 @@ module {\n //     %5 = tt.broadcast %arg3 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %6 = \"triton_gpu.cmpi\"(%4, %5) {predicate = 2 : i64} : (tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>) -> tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %7 = tt.broadcast %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %8 = tt.getelementptr %7, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %8 = tt.addptr %7, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %9 = tt.broadcast %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %10 = tt.getelementptr %9, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %10 = tt.addptr %9, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %11 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %12 = arith.index_cast %arg4 : i32 to index\n //     %13 = arith.cmpi slt, %c0, %12 : index\n@@ -72,9 +72,9 @@ module {\n //     %20 = arith.andi %6, %19 : tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %21 = triton_gpu.copy_async %10, %20, %18 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">> -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %22 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %23 = tt.getelementptr %8, %22, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %23 = tt.addptr %8, %22, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %24 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %25 = tt.getelementptr %10, %24, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %25 = tt.addptr %10, %24, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %26 = arith.cmpi slt, %c32, %12 : index\n //     %27 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %28 = tt.broadcast %26 : (i1) -> tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n@@ -85,9 +85,9 @@ module {\n //     %33 = arith.andi %6, %32 : tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %34 = triton_gpu.copy_async %25, %33, %31 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">> -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %35 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %36 = tt.getelementptr %23, %35, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %36 = tt.addptr %23, %35, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %37 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %38 = tt.getelementptr %25, %37, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %38 = tt.addptr %25, %37, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %39 = arith.cmpi slt, %c64, %12 : index\n //     %40 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %41 = tt.broadcast %39 : (i1) -> tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n@@ -98,16 +98,16 @@ module {\n //     %46 = arith.andi %6, %45 : tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %47 = triton_gpu.copy_async %38, %46, %44 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">> -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %48 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %49 = tt.getelementptr %36, %48, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %49 = tt.addptr %36, %48, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %50 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %51 = tt.getelementptr %38, %50, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %51 = tt.addptr %38, %50, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %52:12 = scf.for %arg6 = %c0 to %12 step %c32 iter_args(%arg7 = %11, %arg8 = %8, %arg9 = %10, %arg10 = %17, %arg11 = %30, %arg12 = %43, %arg13 = %21, %arg14 = %34, %arg15 = %47, %arg16 = %51, %arg17 = %49, %arg18 = %c64) -> (tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, index) {\n //       %55 = arith.addf %arg10, %arg13 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %56 = arith.addf %arg7, %55 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %57 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//       %58 = tt.getelementptr %arg8, %57, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//       %58 = tt.addptr %arg8, %57, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %59 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//       %60 = tt.getelementptr %arg9, %59, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//       %60 = tt.addptr %arg9, %59, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %61 = arith.addi %arg18, %c32 : index\n //       %62 = arith.cmpi slt, %61, %12 : index\n //       %63 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n@@ -117,13 +117,13 @@ module {\n //       %67 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %68 = triton_gpu.copy_async %arg16, %65, %67 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">> -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %69 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//       %70 = tt.getelementptr %arg17, %69, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//       %70 = tt.addptr %arg17, %69, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %71 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//       %72 = tt.getelementptr %arg16, %71, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//       %72 = tt.addptr %arg16, %71, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       scf.yield %56, %58, %60, %arg11, %arg12, %66, %arg14, %arg15, %68, %72, %70, %61 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, index\n //     }\n //     %53 = tt.broadcast %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %54 = tt.getelementptr %53, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %54 = tt.addptr %53, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     tt.store %54, %52#0, %6 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     return\n //   }"}, {"filename": "test/TritonGPU/coalesce.mlir", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -28,20 +28,20 @@ func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n   %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n   %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-  %5 = tt.getelementptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n   %6 = tt.expand_dims %0 {axis = 0 : i32} : (tensor<64xi32, #blocked0>) -> tensor<1x64xi32, #blocked2>\n   %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n   %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-  %10 = tt.getelementptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %11 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-  %12 = tt.getelementptr %11, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %12 = tt.addptr %11, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n   %13 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n   %14 = arith.muli %6, %13 : tensor<1x64xi32, #blocked2>\n   %15 = tt.broadcast %12 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %16 = tt.broadcast %14 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n   %17 = triton_gpu.convert_layout %16 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-  %18 = tt.getelementptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %18 = tt.addptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %19 = tt.load %10, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked1>\n   tt.store %18, %19, %cst : tensor<64x64xf32, #blocked1>\n   return"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "file_content_changes": "@@ -70,21 +70,21 @@ func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt\n   // CHECK: %5 = arith.muli %2, %3 : tensor<64x1xi32, [[row_layout]]>\n   // CHECK: %6 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = [[col_layout]]}>>\n   // CHECK: %7 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = [[row_layout]]}>>\n-  // CHECK: %8 = tt.getelementptr %4, %5 : tensor<64x1x!tt.ptr<f32>, [[row_layout]]>\n+  // CHECK: %8 = tt.addptr %4, %5 : tensor<64x1x!tt.ptr<f32>, [[row_layout]]>\n   // CHECK: %9 = tt.expand_dims %7 {axis = 0 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = [[row_layout]]}>>) -> tensor<1x64xi32, [[row_layout]]>\n   // CHECK: %10 = tt.broadcast %8 : (tensor<64x1x!tt.ptr<f32>, [[row_layout]]>) -> tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n   // CHECK: %11 = tt.broadcast %9 : (tensor<1x64xi32, [[row_layout]]>) -> tensor<64x64xi32, [[row_layout]]>\n   // CHECK: %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, [[col_layout]]>\n   // CHECK: %13 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = [[col_layout]]}>>) -> tensor<64x1xi32, [[col_layout]]>\n   // CHECK: %14 = tt.expand_dims %6 {axis = 0 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = [[col_layout]]}>>) -> tensor<1x64xi32, [[col_layout]]>\n   // CHECK: %15 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, [[col_layout]]>\n-  // CHECK: %16 = tt.getelementptr %12, %13 : tensor<64x1x!tt.ptr<f32>, [[col_layout]]>\n+  // CHECK: %16 = tt.addptr %12, %13 : tensor<64x1x!tt.ptr<f32>, [[col_layout]]>\n   // CHECK: %17 = arith.muli %14, %15 : tensor<1x64xi32, [[col_layout]]>\n   // CHECK: %18 = tt.broadcast %16 : (tensor<64x1x!tt.ptr<f32>, [[col_layout]]>) -> tensor<64x64x!tt.ptr<f32>, [[col_layout]]>\n   // CHECK: %19 = tt.broadcast %17 : (tensor<1x64xi32, [[col_layout]]>) -> tensor<64x64xi32, [[col_layout]]>\n-  // CHECK: %20 = tt.getelementptr %10, %11 : tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n+  // CHECK: %20 = tt.addptr %10, %11 : tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n   // CHECK: %21 = tt.load %20, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<64x64xf32, [[row_layout]]>\n-  // CHECK: %22 = tt.getelementptr %18, %19 : tensor<64x64x!tt.ptr<f32>, [[col_layout]]>\n+  // CHECK: %22 = tt.addptr %18, %19 : tensor<64x64x!tt.ptr<f32>, [[col_layout]]>\n   // CHECK: %23 = triton_gpu.convert_layout %21 : (tensor<64x64xf32, [[row_layout]]>) -> tensor<64x64xf32, [[col_layout]]>\n   // CHECK: tt.store %22, %23, %cst_1 : tensor<64x64xf32, [[col_layout]]>\n   // CHECK: return\n@@ -95,20 +95,20 @@ func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt\n   %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n   %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n   %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-  %5 = tt.getelementptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n   %6 = tt.expand_dims %0 {axis = 0 : i32} : (tensor<64xi32, #blocked0>) -> tensor<1x64xi32, #blocked2>\n   %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n   %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-  %10 = tt.getelementptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %11 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-  %12 = tt.getelementptr %11, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %12 = tt.addptr %11, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n   %13 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n   %14 = arith.muli %6, %13 : tensor<1x64xi32, #blocked2>\n   %15 = tt.broadcast %12 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %16 = tt.broadcast %14 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n   %17 = triton_gpu.convert_layout %16 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-  %18 = tt.getelementptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %18 = tt.addptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %19 = triton_gpu.convert_layout %10 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n   %20 = triton_gpu.convert_layout %cst_0 : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n   %21 = triton_gpu.convert_layout %cst : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n@@ -127,7 +127,7 @@ func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %ar\n     // CHECK: [[loop_ret:%.*]]:2 = scf.for {{.*}} -> (tensor<64x64xf32, [[row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[row_layout]]>)\n     // CHECK-NEXT: {{.*}} = tt.load {{.*}} : tensor<64x64xf32, [[row_layout]]>\n     // CHECK-NEXT: {{.*}} = arith.addf {{.*}} : tensor<64x64xf32, [[row_layout]]>\n-    // CHECK-NEXT: {{.*}} = tt.getelementptr {{.*}} : tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n+    // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n     // CHECK-NEXT: scf.yield {{.*}} : tensor<64x64xf32, [[row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n     // CHECK-NEXT: }\n     // CHECK-NEXT: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[row_layout]]>) -> tensor<64x64xf32, [[col_layout_novec]]>\n@@ -143,30 +143,30 @@ func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %ar\n     %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n     %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n     %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-    %5 = tt.getelementptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+    %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n     %6 = tt.expand_dims %0 {axis = 0 : i32} : (tensor<64xi32, #blocked0>) -> tensor<1x64xi32, #blocked2>\n     %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n     %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n     %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-    %10 = tt.getelementptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+    %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n     %11:2 = scf.for %arg5 = %c0 to %c32 step %c1 iter_args(%arg6 = %cst_1, %arg7 = %10) -> (tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>) {\n       %23 = triton_gpu.convert_layout %arg7 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n       %24 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n       %25 = triton_gpu.convert_layout %cst_1 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n       %26 = tt.load %23, %24, %25 {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<64x64xf32, #blocked3>\n       %27 = triton_gpu.convert_layout %26 : (tensor<64x64xf32, #blocked3>) -> tensor<64x64xf32, #blocked1>\n       %28 = arith.addf %arg6, %27 : tensor<64x64xf32, #blocked1>\n-      %29 = tt.getelementptr %arg7, %cst_0 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+      %29 = tt.addptr %arg7, %cst_0 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n       scf.yield %28, %29 : tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>\n     }\n     %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-    %13 = tt.getelementptr %12, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+    %13 = tt.addptr %12, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n     %14 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n     %15 = arith.muli %6, %14 : tensor<1x64xi32, #blocked2>\n     %16 = tt.broadcast %13 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n     %17 = tt.broadcast %15 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n     %18 = triton_gpu.convert_layout %17 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-    %19 = tt.getelementptr %16, %18 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+    %19 = tt.addptr %16, %18 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n     %20 = triton_gpu.convert_layout %19 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n     %21 = triton_gpu.convert_layout %11#0 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked1>\n     %22 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked1>\n@@ -190,17 +190,17 @@ func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f3\n   %9 = arith.addi %6, %7 : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   %10 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   %11 = arith.addi %4, %5 : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %12 = tt.getelementptr %8, %9 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n+  %12 = tt.addptr %8, %9 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   %13 = tt.load %12 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   %14 = triton_gpu.convert_layout %13 : (tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>\n-  %15 = tt.getelementptr %10, %11 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n+  %15 = tt.addptr %10, %11 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   %16 = tt.load %15 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   %17 = triton_gpu.convert_layout %16 : (tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>\n   %18 = arith.addf %14, %17 : tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>\n   %19 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   %20 = arith.addi %2, %3 : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %21 = tt.getelementptr %19, %20 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n+  %21 = tt.addptr %19, %20 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   %22 = triton_gpu.convert_layout %18 : (tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>) -> tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   tt.store %21, %22 : tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   return\n-}\n\\ No newline at end of file\n+}"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -55,8 +55,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n \n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.getelementptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.getelementptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return\n@@ -112,8 +112,8 @@ func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f\n \n       %c = tt.dot %a, %b, %prev_c {allowTF32 = true} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n \n-      %next_a_ptr = tt.getelementptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-      %next_b_ptr = tt.getelementptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+      %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n+      %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n       scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n     }\n   }\n@@ -161,7 +161,7 @@ func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index, %A :\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n-    %next_b_ptr = tt.getelementptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n     scf.yield %next_b_ptr, %c : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return"}, {"filename": "test/TritonGPU/matmul.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -46,7 +46,7 @@ func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c6\n     %31 = tt.broadcast %29 : (tensor<1x64xi32>) -> tensor<64x64xi32>\n     %32 = arith.addi %30, %31 : tensor<64x64xi32>\n     %33 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>>\n-    %34 = tt.getelementptr %33, %32 : tensor<64x64x!tt.ptr<f32>>\n+    %34 = tt.addptr %33, %32 : tensor<64x64x!tt.ptr<f32>>\n     %35 = tt.expand_dims %23 {axis = 1 : i32} : (tensor<64xi32>) -> tensor<64x1xi32>\n     %36 = tt.splat %arg8 : (i32) -> tensor<64x1xi32>\n     %37 = arith.muli %35, %36 : tensor<64x1xi32>\n@@ -57,7 +57,7 @@ func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c6\n     %42 = tt.broadcast %40 : (tensor<1x64xi32>) -> tensor<64x64xi32>\n     %43 = arith.addi %41, %42 : tensor<64x64xi32>\n     %44 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>>\n-    %45 = tt.getelementptr %44, %43 : tensor<64x64x!tt.ptr<f32>>\n+    %45 = tt.addptr %44, %43 : tensor<64x64x!tt.ptr<f32>>\n     %46 = arith.index_cast %arg5 : i32 to index\n     %47:3 = scf.for %arg12 = %c0 to %46 step %c64 iter_args(%arg13 = %cst_0, %arg14 = %34, %arg15 = %45) -> (tensor<64x64xf32>, tensor<64x64x!tt.ptr<f32>>, tensor<64x64x!tt.ptr<f32>>) {\n       %76 = tt.load %arg14, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32>\n@@ -66,10 +66,10 @@ func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c6\n       %79 = arith.addf %arg13, %78 : tensor<64x64xf32>\n       %80 = arith.muli %arg7, %c64_i32 : i32\n       %81 = tt.splat %80 : (i32) -> tensor<64x64xi32>\n-      %82 = tt.getelementptr %arg14, %81 : tensor<64x64x!tt.ptr<f32>>\n+      %82 = tt.addptr %arg14, %81 : tensor<64x64x!tt.ptr<f32>>\n       %83 = arith.muli %arg8, %c64_i32 : i32\n       %84 = tt.splat %83 : (i32) -> tensor<64x64xi32>\n-      %85 = tt.getelementptr %arg15, %84 : tensor<64x64x!tt.ptr<f32>>\n+      %85 = tt.addptr %arg15, %84 : tensor<64x64x!tt.ptr<f32>>\n       scf.yield %79, %82, %85 : tensor<64x64xf32>, tensor<64x64x!tt.ptr<f32>>, tensor<64x64x!tt.ptr<f32>>\n     }\n     %48 = arith.muli %12, %c64_i32 : i32\n@@ -90,7 +90,7 @@ func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c6\n     %63 = tt.broadcast %61 : (tensor<1x64xi32>) -> tensor<64x64xi32>\n     %64 = arith.addi %62, %63 : tensor<64x64xi32>\n     %65 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>>\n-    %66 = tt.getelementptr %65, %64 : tensor<64x64x!tt.ptr<f32>>\n+    %66 = tt.addptr %65, %64 : tensor<64x64x!tt.ptr<f32>>\n     %67 = tt.expand_dims %51 {axis = 1 : i32} : (tensor<64xi32>) -> tensor<64x1xi32>\n     %68 = tt.splat %arg3 : (i32) -> tensor<64x1xi32>\n     %69 = arith.cmpi slt, %67, %68 : tensor<64x1xi32>\n@@ -103,4 +103,4 @@ func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c6\n     tt.store %66, %47#0, %75 : tensor<64x64xf32>\n     return\n   }\n-}\n\\ No newline at end of file\n+}"}]