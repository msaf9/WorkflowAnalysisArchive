[{"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -77,7 +77,7 @@ SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op) {\n   auto srcShape = srcTy.getShape();\n   auto axis = op.axis();\n \n-  bool fastReduce = axis == srcLayout.getOrder()[0];\n+  bool fastReduce = axis == getOrder(srcLayout)[0];\n \n   SmallVector<unsigned> smemShape;\n   for (auto d : srcShape)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -1742,7 +1742,8 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   // each thread needs to process:\n   //   elemsPerThread = sizeInterWarps * s1 * s2 .. Sn / numThreads\n   unsigned elems = product<unsigned>(smemShape);\n-  unsigned numThreads = product<unsigned>(srcLayout.getWarpsPerCTA()) * 32;\n+  unsigned numThreads =\n+      product<unsigned>(triton::gpu::getWarpsPerCTA(srcLayout)) * 32;\n   unsigned elemsPerThread = std::max<unsigned>(elems / numThreads, 1);\n   Value readOffset = threadId;\n   for (unsigned round = 0; round < elemsPerThread; ++round) {\n@@ -2389,7 +2390,6 @@ struct ConvertLayoutOpConversion\n   }\n \n private:\n-\n   template <typename T>\n   SmallVector<T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) const {\n     size_t rank = order.size();\n@@ -2477,7 +2477,6 @@ struct ConvertLayoutOpConversion\n     llvm_unreachable(\"unexpected layout in getMultiDimOffset\");\n   }\n \n-\n   // shared memory rd/st for blocked or mma layout with data padding\n   void processReplica(Location loc, ConversionPatternRewriter &rewriter,\n                       bool stNotRd, RankedTensorType type,"}, {"filename": "python/tests/test_reduce.py", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -117,8 +117,7 @@ def test_reduce2d(op, dtype, shape, axis):\n     z = torch.empty(reduced_shape, device=x.device, dtype=reduced_dtype)\n \n     kernel = patch_kernel(reduce2d_kernel, {'OP': op})\n-    grid = (1,)\n-    kernel[grid](x_ptr=x, z_ptr=z, axis=axis, block_m=shape[0], block_n=shape[1])\n+    kernel[(1,)](x_ptr=x, z_ptr=z, axis=axis, block_m=shape[0], block_n=shape[1])\n \n     if op == 'sum':\n         golden_z = torch.sum(x, dim=axis, keepdim=False, dtype=reduced_dtype)"}]