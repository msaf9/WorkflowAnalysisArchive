[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 45, "deletions": 44, "changes": 89, "file_content_changes": "@@ -4,19 +4,18 @@\n import triton\n import triton.language as tl\n \n-# fmt: off\n \n @triton.jit\n def _fwd_kernel(\n     Q, K, V,\n-    TMP, L, M, #NOTE: TMP is a scratchpad buffer to workaround a compiler bug\n-    Out, \n+    TMP, L, M,  # NOTE: TMP is a scratchpad buffer to workaround a compiler bug\n+    Out,\n     stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kk, stride_kn,\n     stride_vz, stride_vh, stride_vk, stride_vn,\n     stride_oz, stride_oh, stride_om, stride_on,\n     Z, H, N_CTX,\n-    BLOCK_M: tl.constexpr, BLOCK_DMODEL : tl.constexpr, \n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n ):\n     off_hz = tl.program_id(0)\n@@ -25,40 +24,40 @@ def _fwd_kernel(\n     start_qm = tl.num_programs(1) - 1 - tl.program_id(1)\n     start_kn = 0\n     # initialize pointers to Q\n-    offs_qm = start_qm * BLOCK_M + tl.arange(0, BLOCK_M) \n+    offs_qm = start_qm * BLOCK_M + tl.arange(0, BLOCK_M)\n     offs_qk = tl.arange(0, BLOCK_DMODEL)\n-    q_ptrs = Q + (off_z * stride_qz \\\n-                + off_h * stride_qh \\\n-                + offs_qm[:, None] * stride_qm \\\n-                + offs_qk[None,:] * stride_qk)\n+    q_ptrs = Q + (off_z * stride_qz\n+                  + off_h * stride_qh\n+                  + offs_qm[:, None] * stride_qm\n+                  + offs_qk[None, :] * stride_qk)\n     # initialize pointers to K\n     offs_kk = tl.arange(0, BLOCK_DMODEL)\n     offs_kn = start_kn * BLOCK_N + tl.arange(0, BLOCK_N)\n-    k_ptrs = K + (off_z * stride_kz \\\n-                + off_h * stride_kh \\\n-                + offs_kn[None,:] * stride_kn \\\n-                + offs_kk[:, None] * stride_kk)\n+    k_ptrs = K + (off_z * stride_kz\n+                  + off_h * stride_kh\n+                  + offs_kn[None, :] * stride_kn\n+                  + offs_kk[:, None] * stride_kk)\n     # initialize pointers to V\n     off_vk = tl.arange(0, BLOCK_N)\n     off_vn = tl.arange(0, BLOCK_DMODEL)\n-    v_ptrs = V +  off_z * stride_vz \\\n-                + off_h * stride_vh \\\n-                + off_vk[:, None] * stride_vk \\\n-                + off_vn[None,:] * stride_vn\n+    v_ptrs = V + off_z * stride_vz \\\n+        + off_h * stride_vh \\\n+        + off_vk[:, None] * stride_vk \\\n+        + off_vn[None, :] * stride_vn\n     # initialize pointer to m and l\n-    t_ptrs = TMP + off_hz*N_CTX + offs_qm\n+    t_ptrs = TMP + off_hz * N_CTX + offs_qm\n \n     acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n     m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n     l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n \n     num_blocks_for_row = start_qm + 1\n     for start_n in range(0, num_blocks_for_row):\n-        q = tl.load(q_ptrs) # BUG: fails when moved out of the loop\n+        q = tl.load(q_ptrs)  # BUG: fails when moved out of the loop\n         # -- compute qk ----\n         k = tl.load(k_ptrs)\n         qk = tl.dot(q, k)\n-        qk = tl.where(offs_qm[:, None] >= (start_n*BLOCK_N + offs_kn[None,:]), qk, float(\"-inf\"))\n+        qk = tl.where(offs_qm[:, None] >= (start_n * BLOCK_N + offs_kn[None, :]), qk, float(\"-inf\"))\n         # -- compute m_ij, p, l_ij\n         m_ij = tl.max(qk, 1)\n         p = tl.exp(qk - m_ij[:, None])\n@@ -74,28 +73,28 @@ def _fwd_kernel(\n         # scale acc\n         acc_scale = l_i / l_i_new * tl.exp(m_i - m_i_new)\n         tl.store(t_ptrs, acc_scale)\n-        acc_scale = tl.load(t_ptrs) # BUG: have to store and immediately load\n+        acc_scale = tl.load(t_ptrs)  # BUG: have to store and immediately load\n         acc = acc * acc_scale[:, None]\n         # update acc\n         v = tl.load(v_ptrs)\n         acc += tl.dot(p, v)\n-        k_ptrs += BLOCK_N*stride_kn\n-        v_ptrs += BLOCK_N*stride_vk\n+        k_ptrs += BLOCK_N * stride_kn\n+        v_ptrs += BLOCK_N * stride_vk\n         l_i = l_i_new\n         m_i = m_i_new\n \n     # write back l and m\n-    l_ptrs = L   + off_hz*N_CTX + offs_qm\n-    m_ptrs = M   + off_hz*N_CTX + offs_qm\n+    l_ptrs = L + off_hz * N_CTX + offs_qm\n+    m_ptrs = M + off_hz * N_CTX + offs_qm\n     tl.store(l_ptrs, l_i)\n     tl.store(m_ptrs, m_i)\n     # initialize pointers to output\n     offs_om = offs_qm\n     offs_on = tl.arange(0, BLOCK_DMODEL)\n     out_ptrs = Out + off_z * stride_oz \\\n-                 + off_h * stride_oh \\\n-                 + offs_om[:, None] * stride_om \\\n-                 + offs_on[None,:] * stride_on\n+        + off_h * stride_oh \\\n+        + offs_om[:, None] * stride_om \\\n+        + offs_on[None, :] * stride_on\n     tl.store(out_ptrs, acc)\n \n \n@@ -108,55 +107,56 @@ def forward(ctx, q, k, v):\n         Lq, Lk = q.shape[-1], k.shape[-2]\n         assert Lq == Lk\n         o = torch.empty_like(q)\n-        grid = (q.shape[0]*q.shape[1], triton.cdiv(q.shape[2], BLOCK))\n-        tmp = torch.empty((q.shape[0]*q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n-        l   = torch.empty((q.shape[0]*q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n-        m   = torch.empty((q.shape[0]*q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        grid = (q.shape[0] * q.shape[1], triton.cdiv(q.shape[2], BLOCK))\n+        tmp = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        l = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         pgm = _fwd_kernel[grid](\n             q, k, v,\n-            tmp, l, m, \n-            o, \n+            tmp, l, m,\n+            o,\n             q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n             k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n             v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n             o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n             q.shape[0], q.shape[1], q.shape[2],\n-            BLOCK_M=BLOCK, BLOCK_N=BLOCK, \n-            BLOCK_DMODEL = 64, num_warps=4,\n+            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+            BLOCK_DMODEL=64, num_warps=4,\n             num_stages=1,\n         )\n         ctx.save_for_backward(q, k, v, o, l, m)\n         ctx.BLOCK = BLOCK\n         ctx.grid = grid\n         return o\n \n+\n attention = _attention.apply\n \n+\n @pytest.mark.parametrize('Z, H, N_CTX, D_MODEL', [(2, 3, 1024, 64)])\n def test_op(Z, H, N_CTX, D_MODEL, dtype=torch.float16):\n     torch.manual_seed(20)\n-    q = .5*torch.randn((Z, H, N_CTX, D_MODEL), dtype=dtype, device=\"cuda\", requires_grad=True)\n-    k = .5*torch.randn((Z, H, D_MODEL, N_CTX), dtype=dtype, device=\"cuda\", requires_grad=True)\n-    v = .5*torch.randn((Z, H, N_CTX, D_MODEL), dtype=dtype, device=\"cuda\", requires_grad=True)\n+    q = .5 * torch.randn((Z, H, N_CTX, D_MODEL), dtype=dtype, device=\"cuda\", requires_grad=True)\n+    k = .5 * torch.randn((Z, H, D_MODEL, N_CTX), dtype=dtype, device=\"cuda\", requires_grad=True)\n+    v = .5 * torch.randn((Z, H, N_CTX, D_MODEL), dtype=dtype, device=\"cuda\", requires_grad=True)\n     # triton implementation\n     tri_out = attention(q, k, v)\n     # reference implementation\n     M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n     ref_qk = torch.matmul(q, k)\n     for z in range(Z):\n         for h in range(H):\n-            ref_qk[:,:, M==0] = float(\"-inf\")\n+            ref_qk[:, :, M == 0] = float(\"-inf\")\n     ref_qk = torch.softmax(ref_qk, dim=-1)\n     ref_out = torch.matmul(ref_qk, v)\n     # compare\n     triton.testing.assert_almost_equal(ref_out, tri_out)\n \n \n-\n try:\n     from src.flash_attn_interface import flash_attn_func\n     HAS_FLASH = True\n-except:\n+except BaseException:\n     HAS_FLASH = False\n \n BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n@@ -185,6 +185,7 @@ def test_op(Z, H, N_CTX, D_MODEL, dtype=torch.float16):\n     args={'H': D_HEAD, 'BATCH': BATCH, 'D_MODEL': D_HEAD, 'dtype': torch.float16}\n )\n \n+\n @triton.testing.perf_report([batch_bench, seq_bench])\n def bench_flash_attention(BATCH, H, N_CTX, D_MODEL, provider, dtype=torch.float16, device=\"cuda\"):\n     warmup = 25\n@@ -200,10 +201,10 @@ def bench_flash_attention(BATCH, H, N_CTX, D_MODEL, provider, dtype=torch.float1\n         lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n         cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n         cu_seqlens[1:] = lengths.cumsum(0)\n-        qkv = torch.randn((BATCH*N_CTX, 3, H, D_MODEL), dtype=dtype, device=device, requires_grad=True)\n+        qkv = torch.randn((BATCH * N_CTX, 3, H, D_MODEL), dtype=dtype, device=device, requires_grad=True)\n         fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\n         ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n         return ms\n \n \n-bench_flash_attention.run(save_path='.', print_data=True)\n\\ No newline at end of file\n+bench_flash_attention.run(save_path='.', print_data=True)"}]