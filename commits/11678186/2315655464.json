[{"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 37, "deletions": 35, "changes": 72, "file_content_changes": "@@ -16,14 +16,13 @@\n except ModuleNotFoundError:\n     HAS_APEX = False\n \n-# fmt: off\n \n @triton.jit\n def _layer_norm_fwd_fused(\n-    Out, \n-    A, \n-    Weight, \n-    Bias, \n+    Out,\n+    A,\n+    Weight,\n+    Bias,\n     Mean, Rstd,\n     stride, N, eps,\n     BLOCK_SIZE: tl.constexpr,\n@@ -37,17 +36,17 @@ def _layer_norm_fwd_fused(\n     _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n     for off in range(0, N, BLOCK_SIZE):\n         cols = off + tl.arange(0, BLOCK_SIZE)\n-        a = tl.load(A + cols, mask=cols<N, other=0., eviction_policy=\"evict_last\").to(tl.float32)\n+        a = tl.load(A + cols, mask=cols < N, other=0., eviction_policy=\"evict_last\").to(tl.float32)\n         _mean += a\n-    mean = tl.sum(_mean, axis = 0) / N\n+    mean = tl.sum(_mean, axis=0) / N\n     # compute variance\n     _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n     for off in range(0, N, BLOCK_SIZE):\n         cols = off + tl.arange(0, BLOCK_SIZE)\n-        a = tl.load(A + cols, mask=cols<N, other=0., eviction_policy=\"evict_last\").to(tl.float32)\n-        a = tl.where(cols<N, a - mean, 0.)\n+        a = tl.load(A + cols, mask=cols < N, other=0., eviction_policy=\"evict_last\").to(tl.float32)\n+        a = tl.where(cols < N, a - mean, 0.)\n         _var += a * a\n-    var = tl.sum(_var, axis = 0) / N\n+    var = tl.sum(_var, axis=0) / N\n     rstd = 1 / tl.sqrt(var + eps)\n     # write-back mean/rstd\n     tl.store(Mean + row, mean)\n@@ -65,22 +64,24 @@ def _layer_norm_fwd_fused(\n         tl.store(Out + cols, out, mask=mask)\n \n # Backward pass (DA + partial DW + partial DB)\n+\n+\n @triton.jit\n def _layer_norm_bwd_dx_fused(\n-    _DA, \n-    _DOut, \n+    _DA,\n+    _DOut,\n     _A,\n     Weight,\n     Mean, Rstd,\n-    stride, NumRows, NumCols, eps, \n+    stride, NumRows, NumCols, eps,\n     BLOCK_SIZE_N: tl.constexpr,\n ):\n     # position of elements processed by this program\n     pid = tl.program_id(0)\n     row = pid\n-    A = _A + row*stride\n-    DOut = _DOut + row*stride\n-    DA = _DA + row*stride\n+    A = _A + row * stride\n+    DOut = _DOut + row * stride\n+    DA = _DA + row * stride\n     mean = tl.load(Mean + row)\n     rstd = tl.load(Rstd + row)\n     # load data to SRAM\n@@ -117,8 +118,8 @@ def _layer_norm_bwd_dx_fused(\n def _layer_norm_bwd_dwdb(\n     A, DOut,\n     Mean, Var,\n-    DW, \n-    DB, \n+    DW,\n+    DB,\n     M, N,\n     BLOCK_SIZE_M: tl.constexpr,\n     BLOCK_SIZE_N: tl.constexpr,\n@@ -129,12 +130,12 @@ def _layer_norm_bwd_dwdb(\n     db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n     for i in range(0, M, BLOCK_SIZE_M):\n         rows = i + tl.arange(0, BLOCK_SIZE_M)\n-        mask = (rows[:, None] < M) & (cols[None,:] < N)\n-        offs = rows[:, None] * N + cols[None,:]\n-        a    = tl.load(A + offs, mask=mask, other=0.).to(tl.float32)\n+        mask = (rows[:, None] < M) & (cols[None, :] < N)\n+        offs = rows[:, None] * N + cols[None, :]\n+        a = tl.load(A + offs, mask=mask, other=0.).to(tl.float32)\n         dout = tl.load(DOut + offs, mask=mask, other=0.).to(tl.float32)\n-        mean = tl.load(Mean + rows, mask=rows<M, other=0.)\n-        rstd = tl.load(Var + rows, mask=rows<M, other=0.)\n+        mean = tl.load(Mean + rows, mask=rows < M, other=0.)\n+        rstd = tl.load(Var + rows, mask=rows < M, other=0.)\n         a_hat = (a - mean[:, None]) * rstd[:, None]\n         dw += dout * a_hat\n         db += dout\n@@ -162,10 +163,10 @@ def forward(ctx, a, normalized_shape, weight, bias, eps):\n         # heuristics for number of warps\n         num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n         _layer_norm_fwd_fused[(M,)](\n-            out, \n-            a_arg, \n-            weight, \n-            bias, \n+            out,\n+            a_arg,\n+            weight,\n+            bias,\n             mean, rstd,\n             a_arg.stride(0), N, eps,\n             BLOCK_SIZE=BLOCK_SIZE,\n@@ -192,18 +193,18 @@ def backward(ctx, dout):\n         # heuristics for amount of parallel reduction stream for DG/DB\n         N = weight.shape[0]\n         # allocate output\n-        da       = torch.empty_like(dout)\n+        da = torch.empty_like(dout)\n         # enqueue kernel using forward pass heuristics\n         # also compute partial sums for DW and DB\n         x_arg = a.reshape(-1, a.shape[-1])\n         M, N = x_arg.shape\n-        dweight  = torch.empty((weight.shape[0],), dtype=weight.dtype, device=weight.device)\n-        dbias    = torch.empty((weight.shape[0],), dtype=weight.dtype, device=weight.device)\n+        dweight = torch.empty((weight.shape[0],), dtype=weight.dtype, device=weight.device)\n+        dbias = torch.empty((weight.shape[0],), dtype=weight.dtype, device=weight.device)\n         _layer_norm_bwd_dx_fused[(M,)](\n-            da, \n-            dout, \n-            a, \n-            weight, \n+            da,\n+            dout,\n+            a,\n+            weight,\n             mean, var,\n             x_arg.stride(0), M, N,\n             ctx.eps,\n@@ -216,7 +217,7 @@ def backward(ctx, dout):\n             a, dout,\n             mean, var,\n             dweight,\n-            dbias, \n+            dbias,\n             M,\n             N,\n             BLOCK_SIZE_M=32,\n@@ -235,6 +236,7 @@ def backward(ctx, dout):\n def layer_norm(a, normalized_shape, weight, bias, eps):\n     return LayerNorm.apply(a, normalized_shape, weight, bias, eps)\n \n+\n def test_layer_norm(M, N, dtype, eps=1e-5, device='cuda'):\n     torch.manual_seed(0)\n     # create data"}]