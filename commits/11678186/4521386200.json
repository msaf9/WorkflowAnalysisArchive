[{"filename": ".pre-commit-config.yaml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -24,7 +24,7 @@ repos:\n     rev: v1.6.0\n     hooks:\n       - id: autopep8\n-        args: [\"-a\", \"-i\"]\n+        args: [\"-a\", \"-i\", \"--max-line-length\", \"88\"]\n         stages: [commit, push, manual]\n   - repo: https://github.com/pycqa/flake8\n     rev: 6.0.0"}, {"filename": "python/examples/copy_strided.py", "status": "modified", "additions": 1, "deletions": 6, "changes": 7, "file_content_changes": "@@ -14,10 +14,5 @@ def kernel(X, stride_xm,\n     tl.store(Zs, tl.load(Xs))\n \n \n-ret = triton.compile(\n-    kernel,\n-    signature=\"*fp32,i32,*fp32,i32\",\n-    constants={\n-        \"BLOCK_M\": 64,\n-        \"BLOCK_N\": 64})\n+ret = triton.compile(kernel, signature=\"*fp32,i32,*fp32,i32\", constants={\"BLOCK_M\": 64, \"BLOCK_N\": 64})\n print(ret.asm[\"ttgir\"])"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 18, "deletions": 61, "changes": 79, "file_content_changes": "@@ -18,12 +18,7 @@\n \n def nvsmi(attrs):\n     attrs = ','.join(attrs)\n-    cmd = [\n-        'nvidia-smi',\n-        '-i',\n-        '0',\n-        '--query-gpu=' + attrs,\n-        '--format=csv,noheader,nounits']\n+    cmd = ['nvidia-smi', '-i', '0', '--query-gpu=' + attrs, '--format=csv,noheader,nounits']\n     out = subprocess.check_output(cmd)\n     ret = out.decode(sys.stdout.encoding).split(',')\n     ret = [int(x) for x in ret]\n@@ -86,33 +81,23 @@ def nvsmi(attrs):\n def test_matmul(M, N, K, dtype_str):\n     if dtype_str in ['float32', 'int8'] and DEVICE_NAME != 'a100':\n         pytest.skip('Only test float32 & int8 on a100')\n-    dtype = {\n-        'float16': torch.float16,\n-        'float32': torch.float32,\n-        'int8': torch.int8}[dtype_str]\n+    dtype = {'float16': torch.float16, 'float32': torch.float32, 'int8': torch.int8}[dtype_str]\n     torch.manual_seed(0)\n     ref_gpu_util = matmul_data[DEVICE_NAME][(M, N, K)][dtype_str]\n     cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n-    max_gpu_perf = get_max_tensorcore_tflops(\n-        dtype, clock_rate=cur_sm_clock * 1e3)\n+    max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n     if dtype == torch.int8:\n         a = torch.randint(-128, 127, (M, K), dtype=dtype, device='cuda')\n         b = torch.randint(-128, 127, (N, K), dtype=dtype, device='cuda')\n         b = b.t()  # only test row-col layout\n     else:\n         a = torch.randn((M, K), dtype=dtype, device='cuda')\n         b = torch.randn((K, N), dtype=dtype, device='cuda')\n-\n-    def fn():\n-        return triton.ops.matmul(a, b)\n+    fn = lambda: triton.ops.matmul(a, b)\n     ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    torch.testing.assert_allclose(\n-        cur_gpu_util,\n-        ref_gpu_util,\n-        atol=0.01,\n-        rtol=0.05)\n+    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n \n #######################\n@@ -163,20 +148,12 @@ def test_elementwise(N):\n     z = torch.empty((N, ), dtype=torch.float16, device='cuda')\n     x = torch.randn_like(z)\n     y = torch.randn_like(z)\n-\n-    def grid(args):\n-        return (triton.cdiv(N, args['BLOCK_SIZE']), )\n-\n-    def fn():\n-        return _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n+    grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n+    fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n     ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=500)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    torch.testing.assert_allclose(\n-        cur_gpu_util,\n-        ref_gpu_util,\n-        atol=0.01,\n-        rtol=0.05)\n+    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n #######################\n # Flash-Attention\n@@ -198,34 +175,20 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n     is_backward = mode == 'backward'\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8:\n-        pytest.skip(\n-            \"Flash attention only supported for compute capability < 80\")\n+        pytest.skip(\"Flash attention only supported for compute capability < 80\")\n     torch.manual_seed(20)\n-    dtype = {\n-        'float16': torch.float16,\n-        'float32': torch.float32,\n-        'int8': torch.int8}[dtype_str]\n+    dtype = {'float16': torch.float16, 'float32': torch.float32, 'int8': torch.int8}[dtype_str]\n     # init data\n-    q = torch.empty(\n-        (Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(\n-        mean=0.1, std=0.2).requires_grad_()\n-    k = torch.empty(\n-        (Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(\n-        mean=0.4, std=0.2).requires_grad_()\n-    v = torch.empty(\n-        (Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(\n-        mean=0.3, std=0.2).requires_grad_()\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n     sm_scale = 0.2\n     # benchmark\n-\n-    def fn():\n-        return triton.ops.attention(q, k, v, sm_scale)\n+    fn = lambda: triton.ops.attention(q, k, v, sm_scale)\n     if is_backward:\n         o = fn()\n         do = torch.randn_like(o)\n-\n-        def fn():\n-            return o.backward(do, retain_graph=True)\n+        fn = lambda: o.backward(do, retain_graph=True)\n     ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=500)\n     # compute flops\n     flops_per_matmul = 2. * Z * H * N_CTX * N_CTX * D_HEAD * 0.5\n@@ -235,13 +198,7 @@ def fn():\n     cur_gpu_perf = total_flops / ms * 1e-9\n     # maximum flops\n     cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n-    max_gpu_perf = get_max_tensorcore_tflops(\n-        dtype, clock_rate=cur_sm_clock * 1e3)\n+    max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    ref_gpu_util = flash_attention_data[DEVICE_NAME][(\n-        Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n-    torch.testing.assert_allclose(\n-        cur_gpu_util,\n-        ref_gpu_util,\n-        atol=0.01,\n-        rtol=0.05)\n+    ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n+    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 93, "deletions": 272, "changes": 365, "file_content_changes": "@@ -27,8 +27,7 @@ def _bitwidth(dtype: str) -> int:\n     return int(re.search(r'(\\d+)$', dtype).group(1))\n \n \n-def numpy_random(shape, dtype_str,\n-                 rs: Optional[RandomState] = None, low=None, high=None):\n+def numpy_random(shape, dtype_str, rs: Optional[RandomState] = None, low=None, high=None):\n     \"\"\"\n     Override `rs` if you're calling this function twice and don't want the same\n     result for both calls.\n@@ -43,8 +42,7 @@ def numpy_random(shape, dtype_str,\n         high = iinfo.max if high is None else min(high, iinfo.max)\n         dtype = getattr(np, dtype_str)\n         x = rs.randint(low, high, shape, dtype=dtype)\n-        # Hack. Never return zero so tests of division don't error out.\n-        x[x == 0] = 1\n+        x[x == 0] = 1  # Hack. Never return zero so tests of division don't error out.\n         return x\n     elif dtype_str in float_dtypes:\n         return rs.normal(0, 1, shape).astype(dtype_str)\n@@ -57,8 +55,7 @@ def numpy_random(shape, dtype_str,\n         raise RuntimeError(f'Unknown dtype {dtype_str}')\n \n \n-def to_triton(x: np.ndarray, device='cuda',\n-              dst_type=None) -> Union[TensorWrapper, torch.Tensor]:\n+def to_triton(x: np.ndarray, device='cuda', dst_type=None) -> Union[TensorWrapper, torch.Tensor]:\n     '''\n     Note: We need dst_type because the type of x can be different from dst_type.\n           For example: x is of type `float32`, dst_type is `bfloat16`.\n@@ -68,8 +65,7 @@ def to_triton(x: np.ndarray, device='cuda',\n     if t in uint_dtypes:\n         signed_type_name = t.lstrip('u')  # e.g. \"uint16\" -> \"int16\"\n         x_signed = x.astype(getattr(np, signed_type_name))\n-        return reinterpret(torch.tensor(\n-            x_signed, device=device), getattr(tl, t))\n+        return reinterpret(torch.tensor(x_signed, device=device), getattr(tl, t))\n     else:\n         if t == 'float32' and dst_type == 'bfloat16':\n             return torch.tensor(x, device=device).bfloat16()\n@@ -110,8 +106,7 @@ def check_type_supported(dtype):\n     skip test if dtype is not supported on the current device\n     '''\n     cc = torch.cuda.get_device_capability()\n-    if cc[0] < 8 and (dtype is tl.bfloat16 or dtype ==\n-                      \"bfloat16\" or dtype is torch.bfloat16):\n+    if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n         pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n \n \n@@ -123,12 +118,7 @@ def test_empty_kernel(dtype_x, device='cuda'):\n     def kernel(X, SIZE: tl.constexpr):\n         pass\n     check_type_supported(dtype_x)\n-    x = to_triton(\n-        numpy_random(\n-            SIZE,\n-            dtype_str=dtype_x),\n-        device=device,\n-        dst_type=dtype_x)\n+    x = to_triton(numpy_random(SIZE, dtype_str=dtype_x), device=device, dst_type=dtype_x)\n     kernel[(1, )](x, SIZE=SIZE, num_warps=4)\n \n \n@@ -191,8 +181,7 @@ def _binary_op_dtype_override(a: str, b: str) -> Optional[np.dtype]:\n     return overrides.get(key)\n \n \n-def _test_binary(dtype_x, dtype_y, expr, numpy_expr=None, mode_x='real',\n-                 mode_y='real', device='cuda', y_low=None, y_high=None):\n+def _test_binary(dtype_x, dtype_y, expr, numpy_expr=None, mode_x='real', mode_y='real', device='cuda', y_low=None, y_high=None):\n     check_type_supported(dtype_x)  # early return if dtype_x is not supported\n     check_type_supported(dtype_y)\n     SIZE = 128\n@@ -266,10 +255,8 @@ def _mod_operation_ill_conditioned(dtype_x, dtype_y) -> bool:\n ])\n def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n     expr = f' x {op} y'\n-    if op == '%' and dtype_x in int_dtypes + \\\n-            uint_dtypes and dtype_y in int_dtypes + uint_dtypes:\n-        # LLVM has 'numpy.fmod', not 'numpy.remainder', semantics on integer\n-        # remainders.\n+    if op == '%' and dtype_x in int_dtypes + uint_dtypes and dtype_y in int_dtypes + uint_dtypes:\n+        # LLVM has 'numpy.fmod', not 'numpy.remainder', semantics on integer remainders.\n         numpy_expr = 'np.fmod(x, y)'\n     elif op in ('/', '%') and dtype_x in ('int16', 'float16', 'bfloat16') and dtype_y in ('int16', 'float16', 'bfloat16'):\n         # Triton promotes 16-bit floating-point / and % to 32-bit because there\n@@ -290,16 +277,14 @@ def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n            (dtype_x in uint_dtypes and dtype_y in int_dtypes))):\n         with pytest.raises(triton.CompilationError) as exc_info:\n             _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n-        assert re.match(\n-            'Cannot use .* because they have different signedness', str(exc_info.value.__cause__))\n+        assert re.match('Cannot use .* because they have different signedness', str(exc_info.value.__cause__))\n     else:\n         _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n \n \n @pytest.mark.parametrize(\"dtype_x, dtype_y\",\n                          [(dtype_x, dtype_y) for dtype_x in int_dtypes for dtype_y in int_dtypes] +\n-                         [(dtype_x, dtype_y)\n-                          for dtype_x in uint_dtypes for dtype_y in uint_dtypes]\n+                         [(dtype_x, dtype_y) for dtype_x in uint_dtypes for dtype_y in uint_dtypes]\n                          )\n def test_floordiv(dtype_x, dtype_y, device='cuda'):\n     # Triton has IEEE, not numpy/torch, semantics for %, and those carry\n@@ -358,26 +343,17 @@ def kernel(O1, O2, X, Y, SIZE: tl.constexpr):\n ])\n def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n     expr = f'x {op} y'\n-    if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(\n-            dtype_x) >= _bitwidth(dtype_y)):\n+    if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n         numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n     elif (dtype_y in uint_dtypes and dtype_x in int_dtypes and _bitwidth(dtype_y) >= _bitwidth(dtype_x)):\n         numpy_expr = f'x.astype(np.{dtype_y}) {op} y.astype(np.{dtype_y})'\n     else:\n         numpy_expr = None\n     if 'float' in dtype_x + dtype_y:\n         with pytest.raises(triton.CompilationError) as exc_info:\n-            _test_binary(\n-                dtype_x,\n-                dtype_y,\n-                expr,\n-                numpy_expr='np.array([])',\n-                device=device)\n-        # The CompilationError must have been caused by a C++ exception with\n-        # this text.\n-        assert re.match(\n-            'invalid operands of type', str(\n-                exc_info.value.__cause__))\n+            _test_binary(dtype_x, dtype_y, expr, numpy_expr='np.array([])', device=device)\n+        # The CompilationError must have been caused by a C++ exception with this text.\n+        assert re.match('invalid operands of type', str(exc_info.value.__cause__))\n     else:\n         _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n \n@@ -396,14 +372,7 @@ def test_shift_op(dtype_x, dtype_y, op, device='cuda'):\n     else:\n         dtype_z = f'uint{bw}'\n     numpy_expr = f'x.astype(np.{dtype_z}) {op} y.astype(np.{dtype_z})'\n-    _test_binary(\n-        dtype_x,\n-        dtype_y,\n-        expr,\n-        numpy_expr,\n-        device=device,\n-        y_low=0,\n-        y_high=65)\n+    _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device, y_low=0, y_high=65)\n \n \n # ---------------\n@@ -430,21 +399,13 @@ def test_shift_op(dtype_x, dtype_y, op, device='cuda'):\n                           ])\n def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n     expr = f'x {op} y'\n-    if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(\n-            dtype_x) >= _bitwidth(dtype_y)):\n+    if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n         numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n     elif (dtype_y in uint_dtypes and dtype_x in int_dtypes and _bitwidth(dtype_y) >= _bitwidth(dtype_x)):\n         numpy_expr = f'x.astype(np.{dtype_y}) {op} y.astype(np.{dtype_y})'\n     else:\n         numpy_expr = None\n-    _test_binary(\n-        dtype_x,\n-        dtype_y,\n-        expr,\n-        numpy_expr,\n-        mode_x=mode_x,\n-        mode_y=mode_y,\n-        device=device)\n+    _test_binary(dtype_x, dtype_y, expr, numpy_expr, mode_x=mode_x, mode_y=mode_y, device=device)\n \n \n # ---------------\n@@ -489,34 +450,13 @@ def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n     cond_tri = to_triton(cond, device='cuda')\n     x_tri = to_triton(x, device='cuda', dst_type=dtype)\n     y_tri = to_triton(y, device='cuda', dst_type=dtype)\n-    z_tri = to_triton(\n-        np.empty(\n-            SIZE,\n-            dtype=z.dtype),\n-        device='cuda',\n-        dst_type=dtype)\n-\n-    def grid(meta): return (triton.cdiv(SIZE, meta['BLOCK_SIZE']),)\n-    where_kernel[grid](\n-        cond_tri,\n-        x_tri,\n-        y_tri,\n-        z_tri,\n-        SIZE,\n-        BLOCK_SIZE=1024,\n-        TEST_POINTERS=select_ptrs,\n-        TEST_SCALAR_POINTERS=False)\n+    z_tri = to_triton(np.empty(SIZE, dtype=z.dtype), device='cuda', dst_type=dtype)\n+\n+    grid = lambda meta: (triton.cdiv(SIZE, meta['BLOCK_SIZE']),)\n+    where_kernel[grid](cond_tri, x_tri, y_tri, z_tri, SIZE, BLOCK_SIZE=1024, TEST_POINTERS=select_ptrs, TEST_SCALAR_POINTERS=False)\n     assert (z == to_numpy(z_tri)).all()\n     if select_ptrs:\n-        where_kernel[grid](\n-            cond_tri,\n-            x_tri,\n-            y_tri,\n-            z_tri,\n-            SIZE,\n-            BLOCK_SIZE=1024,\n-            TEST_POINTERS=select_ptrs,\n-            TEST_SCALAR_POINTERS=True)\n+        where_kernel[grid](cond_tri, x_tri, y_tri, z_tri, SIZE, BLOCK_SIZE=1024, TEST_POINTERS=select_ptrs, TEST_SCALAR_POINTERS=True)\n         z = np.where(cond[0], x, y)\n         assert (z == to_numpy(z_tri)).all()\n \n@@ -549,8 +489,7 @@ def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n     z = np.where(mask, x, 0)\n     cond_tri = to_triton(mask, device=\"cuda\")\n     x_tri = to_triton(x, device='cuda', dst_type=dtype)\n-    z_tri = to_triton(np.empty((SIZE, SIZE), dtype=z.dtype),\n-                      device='cuda', dst_type=dtype)\n+    z_tri = to_triton(np.empty((SIZE, SIZE), dtype=z.dtype), device='cuda', dst_type=dtype)\n     where_kernel[(1,)](cond_tri, x_tri, z_tri, SIZE)\n     assert (z == to_numpy(z_tri)).all()\n     where_scalar_condition[(1,)](x_tri, z_tri, SIZE)\n@@ -605,15 +544,13 @@ def abs_kernel(Z, X, SIZE: tl.constexpr):\n         tl.store(Z + off, z)\n \n     f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n-    # f32_to_f8 doesn't handle nan, so we make sure f8_tensor doesn't contain\n-    # any nan\n-    all_exp_ones = (f8_tensor & 0b01111100) == 128 - \\\n-        2**in_dtype.fp_mantissa_width\n+    # f32_to_f8 doesn't handle nan, so we make sure f8_tensor doesn't contain any nan\n+    all_exp_ones = (f8_tensor & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n     f8_tensor[all_exp_ones] = 0\n     f8 = triton.reinterpret(f8_tensor, in_dtype)\n     n_elements = f8_tensor.numel()\n     out_f8 = torch.empty_like(f8_tensor)\n-    def grid(meta): return (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     abs_kernel[(1,)](f8, triton.reinterpret(out_f8, in_dtype), n_elements)\n \n     f32_tensor = convert_float_to_float32(f8_tensor, in_dtype)\n@@ -638,9 +575,7 @@ def make_ptr_str(name, shape):\n     return f\"{name} + {' + '.join(offsets)}\"\n \n \n-# TODO: handle `%4 = triton_gpu.convert_layout %3 : (tensor<32xi32,\n-# #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent =\n-# #blocked1}>>``\n+# TODO: handle `%4 = triton_gpu.convert_layout %3 : (tensor<32xi32, #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>``\n @pytest.mark.parametrize(\"expr, dtype_str\", [\n     (f'x[{s}]', d)\n     for s in ['None, :', ':, None',\n@@ -761,8 +696,7 @@ def test_atomic_rmw(op, dtype_x_str, mode, device='cuda'):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         if dtype_x_str == 'float16':\n-            pytest.skip(\n-                \"Only test atomic float16 ops on devices with sm >= 70\")\n+            pytest.skip(\"Only test atomic float16 ops on devices with sm >= 70\")\n     n_programs = 5\n \n     # triton kernel\n@@ -772,20 +706,15 @@ def kernel(X, Z):\n         x = tl.load(X + pid)\n         old = GENERATE_TEST_HERE\n \n-    kernel = patch_kernel(\n-        kernel, {\n-            'GENERATE_TEST_HERE': f'tl.atomic_{op}(Z, x)'})\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.atomic_{op}(Z, x)'})\n     numpy_op = {'add': np.sum, 'max': np.max, 'min': np.min}[op]\n-    max_neutral = float(\n-        '-inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).min\n-    min_neutral = float('inf') if dtype_x_str in float_dtypes else np.iinfo(\n-        getattr(np, dtype_x_str)).max\n+    max_neutral = float('-inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).min\n+    min_neutral = float('inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).max\n     neutral = {'add': 0, 'max': max_neutral, 'min': min_neutral}[op]\n \n     # triton result\n     rs = RandomState(17)\n-    x = np.array([2**i for i in range(n_programs)],\n-                 dtype=getattr(np, dtype_x_str))\n+    x = np.array([2**i for i in range(n_programs)], dtype=getattr(np, dtype_x_str))\n     if mode == 'all_neg':\n         x = -np.abs(x)\n     if mode == 'all_pos':\n@@ -798,13 +727,7 @@ def kernel(X, Z):\n         x[idx] = np.max(np.abs(x)) + 1\n     x_tri = to_triton(x, device=device)\n \n-    z_tri = to_triton(\n-        np.array(\n-            [neutral],\n-            dtype=getattr(\n-                np,\n-                dtype_x_str)),\n-        device=device)\n+    z_tri = to_triton(np.array([neutral], dtype=getattr(np, dtype_x_str)), device=device)\n     kernel[(n_programs, )](x_tri, z_tri)\n     # torch result\n     z_ref = numpy_op(x).astype(getattr(np, dtype_x_str))\n@@ -823,8 +746,7 @@ def test_tensor_atomic_rmw(shape, axis, device=\"cuda\"):\n     # triton kernel\n \n     @triton.jit\n-    def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr,\n-               SHAPE1: tl.constexpr):\n+    def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n         off0 = tl.arange(0, SHAPE0)\n         off1 = tl.arange(0, SHAPE1)\n         x = tl.load(X + off0[:, None] * SHAPE1 + off1[None, :])\n@@ -915,15 +837,12 @@ def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n     check_type_supported(dtype_x)\n     check_type_supported(dtype_z)\n \n-    # This is tricky because numpy doesn't have bfloat, and torch doesn't have\n-    # uints.\n+    # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n     x0 = 43 if dtype_x in int_dtypes else 43.5\n     if dtype_x in float_dtypes and dtype_z == 'int1':\n         x0 = 0.5\n     if dtype_x.startswith('bfloat'):\n-        x_tri = torch.tensor(\n-            [x0], dtype=getattr(\n-                torch, dtype_x), device=device)\n+        x_tri = torch.tensor([x0], dtype=getattr(torch, dtype_x), device=device)\n     else:\n         x = np.array([x0], dtype=getattr(np, dtype_x))\n         x_tri = to_triton(x)\n@@ -942,10 +861,7 @@ def kernel(X, Z, BITCAST: tl.constexpr):\n     if dtype_z.startswith('bfloat'):\n         z_tri = torch.empty((1,), dtype=getattr(torch, dtype_z), device=device)\n     else:\n-        z_tri = to_triton(\n-            np.empty(\n-                (1, ), dtype=getattr(\n-                    np, dtype_z_np)), device=device)\n+        z_tri = to_triton(np.empty((1, ), dtype=getattr(np, dtype_z_np)), device=device)\n     kernel[(1, )](x_tri, z_tri, BITCAST=bitcast)\n     # torch result\n     if dtype_z.startswith('bfloat') or dtype_x.startswith('bfloat'):\n@@ -973,22 +889,10 @@ def kernel(output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n     triton_dtype_str = 'uint8' if dtype_str == 'bool' else dtype_str\n-    kernel = patch_kernel(\n-        kernel, {\n-            'GENERATE_TEST_HERE': f'tl.zeros([BLOCK_SIZE], dtype=tl.{triton_dtype_str}) + 1'})\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.zeros([BLOCK_SIZE], dtype=tl.{triton_dtype_str}) + 1'})\n     block_size = 128\n-    ref = torch.ones(\n-        [block_size],\n-        dtype=getattr(\n-            torch,\n-            dtype_str),\n-        device='cuda')\n-    output = torch.zeros(\n-        [block_size],\n-        dtype=getattr(\n-            torch,\n-            dtype_str),\n-        device='cuda')\n+    ref = torch.ones([block_size], dtype=getattr(torch, dtype_str), device='cuda')\n+    output = torch.zeros([block_size], dtype=getattr(torch, dtype_str), device='cuda')\n     kernel[(1,)](output, block_size, BLOCK_SIZE=block_size)\n \n     assert torch.all(output == ref)\n@@ -1021,27 +925,19 @@ def convert_float_to_float32(fp: torch.tensor, dtype=None):\n \n     output = torch.where(exp == 0,\n                          # subnormal\n-                         ((-1.0) ** sign) * (2.0 ** (1 - exp_bias)) * \\\n-                         (frac / (2.0 ** dtype.fp_mantissa_width)),\n+                         ((-1.0) ** sign) * (2.0 ** (1 - exp_bias)) * (frac / (2.0 ** dtype.fp_mantissa_width)),\n                          # normal\n                          ((-1.0) ** sign) * (2.0 ** (exp - exp_bias)) * (1.0 + frac / (2.0 ** dtype.fp_mantissa_width))).float()\n \n-    extended_exp = (\n-        (1 << (\n-            tl.float32.primitive_bitwidth -\n-            tl.float32.fp_mantissa_width -\n-            1)) -\n-        1) << tl.float32.fp_mantissa_width\n+    extended_exp = ((1 << (tl.float32.primitive_bitwidth - tl.float32.fp_mantissa_width - 1)) - 1) << tl.float32.fp_mantissa_width\n     # special cases, exp is 0b11..1\n     if dtype == tl.float8e4:\n         # float8e4m3 does not have infinities\n         output[fp == torch.tensor(0b01111111, dtype=torch.int8)] = torch.nan\n         output[fp == torch.tensor(0b11111111, dtype=torch.int8)] = torch.nan\n     else:\n         output = torch.where(exp == (1 << exp_width) - 1,\n-                             ((sign << (tl.float32.primitive_bitwidth -\n-                                        1)) | extended_exp | (frac << (tl.float32.fp_mantissa_width -\n-                                                                       dtype.fp_mantissa_width))).view(torch.float32),\n+                             ((sign << (tl.float32.primitive_bitwidth - 1)) | extended_exp | (frac << (tl.float32.fp_mantissa_width - dtype.fp_mantissa_width))).view(torch.float32),\n                              output)\n     return output\n \n@@ -1051,8 +947,7 @@ def test_convert_float16_to_float32(in_dtype):\n     \"\"\"Tests that check convert_float_to_float32 function\"\"\"\n     check_type_supported(in_dtype)\n \n-    f16_input = torch.tensor(\n-        range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16).view(in_dtype)\n+    f16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16).view(in_dtype)\n     f32_output = convert_float_to_float32(f16_input)\n \n     nan = f16_input.isnan()\n@@ -1064,39 +959,33 @@ def test_convert_float16_to_float32(in_dtype):\n \n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n-@pytest.mark.parametrize(\"out_dtype\",\n-                         [torch.float16, torch.bfloat16, torch.float32])\n+@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16, torch.float32])\n def test_f8_xf16_roundtrip(in_dtype, out_dtype):\n     \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n     check_type_supported(out_dtype)\n \n     @triton.jit\n-    def copy_kernel(input_ptr, output_ptr, n_elements,\n-                    BLOCK_SIZE: tl.constexpr):\n+    def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n         mask = offsets < n_elements\n         input = tl.load(input_ptr + offsets, mask=mask)\n         output = input\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n     f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n-    # f32_to_f8 doesn't handle nan, so we make sure f8_tensor doesn't contain\n-    # any nan\n-    all_exp_ones = (f8_tensor & 0b01111100) == 128 - \\\n-        2**in_dtype.fp_mantissa_width\n+    # f32_to_f8 doesn't handle nan, so we make sure f8_tensor doesn't contain any nan\n+    all_exp_ones = (f8_tensor & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n     f8_tensor[all_exp_ones] = 0\n     f8 = triton.reinterpret(f8_tensor, in_dtype)\n     n_elements = f8_tensor.numel()\n     xf16 = torch.empty_like(f8_tensor, dtype=out_dtype)\n-    def grid(meta): return (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     copy_kernel[grid](f8, xf16, n_elements, BLOCK_SIZE=1024)\n \n     # exponent_mask = 0b01111100 for float8e5\n     # exponent_mask = 0b01111000 for float8e4\n     exponent_mask = 0b01111111 ^ ((1 << in_dtype.fp_mantissa_width) - 1)\n-    normal = torch.logical_and(\n-        (f8_tensor & exponent_mask) != 0,\n-        (f8_tensor & exponent_mask) != exponent_mask)\n+    normal = torch.logical_and((f8_tensor & exponent_mask) != 0, (f8_tensor & exponent_mask) != exponent_mask)\n     ref16 = convert_float_to_float32(f8_tensor, in_dtype)\n     # WARN: currently only normal float8s are handled\n     assert torch.all(xf16[normal] == ref16[normal])\n@@ -1116,37 +1005,30 @@ def test_f16_to_f8_rounding(in_dtype, out_dtype):\n     Or the same explanation a bit mathier:\n     for all f16 |f16 - fromf8(tof8(f16))| == min over all f8 |f16 - fromf8(f8)|\"\"\"\n     @triton.jit\n-    def copy_kernel(input_ptr, output_ptr, n_elements,\n-                    BLOCK_SIZE: tl.constexpr):\n+    def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n         mask = offsets < n_elements\n         input = tl.load(input_ptr + offsets, mask=mask)\n         output = input\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n-    i16_input = torch.tensor(\n-        range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16, device='cuda')\n+    i16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16, device='cuda')\n     f16_input = i16_input.view(out_dtype)\n     n_elements = f16_input.numel()\n     f8_output_tensor = torch.empty_like(f16_input, dtype=torch.int8)\n     f8_output = triton.reinterpret(f8_output_tensor, in_dtype)\n-    def grid(meta): return (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     copy_kernel[grid](f16_input, f8_output, n_elements, BLOCK_SIZE=1024)\n \n     f16_output = torch.empty_like(f16_input, dtype=out_dtype)\n     copy_kernel[grid](f8_output, f16_output, n_elements, BLOCK_SIZE=1024)\n \n     abs_error = torch.abs(f16_input - f16_output)\n \n-    all_f8_vals_tensor = torch.tensor(\n-        range(2 ** 8), dtype=torch.uint8, device='cuda')\n+    all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device='cuda')\n     all_f8_vals = triton.reinterpret(all_f8_vals_tensor, in_dtype)\n     all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=out_dtype)\n-    copy_kernel[grid](\n-        all_f8_vals,\n-        all_f8_vals_in_f16,\n-        n_elements=256,\n-        BLOCK_SIZE=1024)\n+    copy_kernel[grid](all_f8_vals, all_f8_vals_in_f16, n_elements=256, BLOCK_SIZE=1024)\n \n     all_finite_f8_vals_in_f16 = all_f8_vals_in_f16[\n         torch.isfinite(all_f8_vals_in_f16)\n@@ -1163,16 +1045,12 @@ def grid(meta): return (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     # WARN: only normalized numbers are handled\n     f8_normal_min = 1 << in_dtype.fp_mantissa_width  # 0b00001000 for float8e4\n     f8_normal_max = 0b01111110 if in_dtype == tl.float8e4 else 0b01111011\n-    f16_min, f16_max, f16_max_minus_1 = convert_float_to_float32(torch.tensor(\n-        [f8_normal_min, f8_normal_max, f8_normal_max - 1], dtype=torch.int8), in_dtype)\n+    f16_min, f16_max, f16_max_minus_1 = convert_float_to_float32(torch.tensor([f8_normal_min, f8_normal_max, f8_normal_max - 1], dtype=torch.int8), in_dtype)\n     assert torch.all(torch.isfinite(f16_min))\n     assert torch.all(torch.isfinite(f16_max))\n     thres_error = f16_max - f16_max_minus_1\n     mismatch = torch.logical_and(\n-        torch.logical_or(\n-            abs_error != min_error, abs_error > thres_error), torch.logical_and(\n-            torch.isfinite(f16_input), torch.logical_and(\n-                torch.abs(f16_input) <= f16_max, torch.abs(f16_input) >= f16_min))\n+        torch.logical_or(abs_error != min_error, abs_error > thres_error), torch.logical_and(torch.isfinite(f16_input), torch.logical_and(torch.abs(f16_input) <= f16_max, torch.abs(f16_input) >= f16_min))\n     )\n     assert torch.all(\n         torch.logical_not(mismatch)\n@@ -1208,9 +1086,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))\n         tl.store(Z, GENERATE_TEST_HERE)\n \n-    kernel = patch_kernel(\n-        kernel, {\n-            'GENERATE_TEST_HERE': f'tl.{op}(x, axis=0)'})\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=0)'})\n     # input\n     rs = RandomState(17)\n     # limit the range of integers so that the sum does not overflow\n@@ -1271,15 +1147,13 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n ]\n \n \n-@pytest.mark.parametrize(\"op, dtype_str, shape, axis\",\n-                         reduce_configs1 + reduce_configs2)\n+@pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n     check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n-    def kernel(X, Z, BLOCK_M: tl.constexpr,\n-               BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n+    def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n         range_m = tl.arange(0, BLOCK_M)\n         range_n = tl.arange(0, BLOCK_N)\n         x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n@@ -1289,9 +1163,7 @@ def kernel(X, Z, BLOCK_M: tl.constexpr,\n         else:\n             tl.store(Z + range_n, z)\n \n-    kernel = patch_kernel(\n-        kernel, {\n-            'GENERATE_TEST_HERE': f'tl.{op}(x, axis=AXIS)'})\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=AXIS)'})\n     # input\n     rs = RandomState(17)\n     # limit the range of integers so that the sum does not overflow\n@@ -1358,17 +1230,13 @@ def kernel(X, stride_xm, stride_xn,\n     x = numpy_random(shape, dtype_str=dtype_str)\n     # triton result\n     z_tri = to_triton(np.empty_like(x), device=device, dst_type=dtype_str)\n-    z_tri_contiguous = to_triton(\n-        np.empty_like(x),\n-        device=device,\n-        dst_type=dtype_str)\n+    z_tri_contiguous = to_triton(np.empty_like(x), device=device, dst_type=dtype_str)\n     x_tri = to_triton(x, device=device, dst_type=dtype_str)\n     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n                          z_tri, z_tri.stride(1), z_tri.stride(0),\n                          BLOCK_M=shape[0], BLOCK_N=shape[1])\n     pgm_contiguous = kernel[(1, 1)](x_tri, x_tri.stride(1), x_tri.stride(0),\n-                                    z_tri_contiguous, z_tri_contiguous.stride(\n-                                        0), z_tri_contiguous.stride(1),\n+                                    z_tri_contiguous, z_tri_contiguous.stride(0), z_tri_contiguous.stride(1),\n                                     BLOCK_M=shape[0], BLOCK_N=shape[1])\n     # numpy result\n     z_ref = x.transpose(*perm)\n@@ -1415,8 +1283,7 @@ def kernel(X, stride_xm, stride_xn,\n                                                       ('float16', 'float16'),\n                                                       ('float16', 'float32'),\n                                                       ('float32', 'float32')]])\n-def test_dot(M, N, K, num_warps, col_a, col_b, epilogue,\n-             allow_tf32, in_dtype, out_dtype, device='cuda'):\n+def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype, device='cuda'):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n@@ -1553,11 +1420,9 @@ def kernel(X, stride_xm, stride_xk,\n     # print(z_ref[:,0], z_tri[:,0])\n     if in_dtype == 'float32':\n         # XXX: Somehow there's a larger difference when we use float32\n-        np.testing.assert_allclose(\n-            z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n     elif out_dtype == tl.float16:\n-        np.testing.assert_allclose(\n-            z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n     else:\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n     # make sure ld/st are vectorized\n@@ -1593,9 +1458,7 @@ def kernel_dynamic(out, val, dtype: tl.constexpr):\n         out_ptr = out + tl.arange(0, 128)[:]\n         tl.store(out_ptr, a)\n \n-    kernel_static_patched = patch_kernel(\n-        kernel_static, {\n-            'GENERATE_TEST_HERE': f\"tl.full((128,), 2, tl.{dtype_str})\"})\n+    kernel_static_patched = patch_kernel(kernel_static, {'GENERATE_TEST_HERE': f\"tl.full((128,), 2, tl.{dtype_str})\"})\n     out_static = torch.zeros((128), dtype=dtype, device=\"cuda\")\n     kernel_static_patched[(1,)](out_static)\n     out_dynamic = torch.zeros((128), dtype=dtype, device=\"cuda\")\n@@ -1640,20 +1503,15 @@ def _kernel(z, BLOCK: tl.constexpr,\n         val = tl.arange(START, END)\n         tl.store(z + off, val)\n     _kernel[(1,)](z_tri, START=start, END=start + BLOCK, BLOCK=BLOCK)\n-    z_ref = torch.arange(\n-        start,\n-        BLOCK + start,\n-        dtype=torch.int32,\n-        device=device)\n+    z_ref = torch.arange(start, BLOCK + start, dtype=torch.int32, device=device)\n     np.testing.assert_allclose(to_numpy(z_tri), to_numpy(z_ref))\n \n # ---------------\n # test load\n # ---------------\n \n \n-@pytest.mark.parametrize(\"dtype_str, size, size_diff\", [(dtype_str, size, size_diff)\n-                         for dtype_str in torch_dtypes for size in [128, 512] for size_diff in [0, 1, 2, 3, 4]])\n+@pytest.mark.parametrize(\"dtype_str, size, size_diff\", [(dtype_str, size, size_diff) for dtype_str in torch_dtypes for size in [128, 512] for size_diff in [0, 1, 2, 3, 4]])\n def test_masked_load(dtype_str, size, size_diff, device='cuda'):\n     dtype = getattr(torch, dtype_str)\n     check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n@@ -1663,15 +1521,13 @@ def test_masked_load(dtype_str, size, size_diff, device='cuda'):\n     if dtype_str == 'bool':\n         input = torch.randint(0, 2, (input_size,), dtype=dtype, device=device)\n     elif dtype_str in int_dtypes or dtype_str in uint_dtypes:\n-        input = torch.randint(\n-            0, 127, (input_size,), dtype=dtype, device=device)\n+        input = torch.randint(0, 127, (input_size,), dtype=dtype, device=device)\n     else:\n         input = torch.rand(input_size, dtype=dtype, device=device)\n     output = torch.zeros((output_size,), dtype=dtype, device=device)\n \n     @triton.jit\n-    def _kernel(in_ptr, out_ptr, in_size: tl.constexpr,\n-                out_size: tl.constexpr):\n+    def _kernel(in_ptr, out_ptr, in_size: tl.constexpr, out_size: tl.constexpr):\n         in_offsets = tl.arange(0, out_size)\n         # Load inputs.\n         x = GENERATE_TEST_HERE\n@@ -1680,21 +1536,17 @@ def _kernel(in_ptr, out_ptr, in_size: tl.constexpr,\n         tl.store(out_ptr + output_offsets, x)\n \n     mask_str = \"mask=in_offsets < in_size, other=1\" if size_diff > 0 else \"None\"\n-    kernel = patch_kernel(\n-        _kernel, {\n-            'GENERATE_TEST_HERE': f\"tl.load(in_ptr + in_offsets, {mask_str})\"})\n+    kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.load(in_ptr + in_offsets, {mask_str})\"})\n     kernel[(1,)](input, output, input_size, output_size)\n \n-    reference_out = torch.cat(\n-        (input, torch.ones((size_diff,), dtype=dtype, device=device)))\n+    reference_out = torch.cat((input, torch.ones((size_diff,), dtype=dtype, device=device)))\n     # print((output - reference_out).nonzero())\n     torch.testing.assert_allclose(output, reference_out)\n \n # Testing masked loads with an intermate copy to shared memory run.\n \n \n-@pytest.mark.parametrize(\"dtype\",\n-                         [torch.bfloat16, torch.float16, torch.float32])\n+@pytest.mark.parametrize(\"dtype\", [torch.bfloat16, torch.float16, torch.float32])\n def test_masked_load_shared_memory(dtype, device='cuda'):\n     check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n \n@@ -1793,16 +1645,14 @@ def test_vectorization_hints(has_hints):\n     off = torch.zeros(1, device='cuda', dtype=torch.int32)\n \n     @triton.jit\n-    def _kernel(dst, src, off, N, BLOCK_SIZE: tl.constexpr,\n-                HINT: tl.constexpr):\n+    def _kernel(dst, src, off, N, BLOCK_SIZE: tl.constexpr, HINT: tl.constexpr):\n         offsets = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n         offsets = offsets + tl.load(off)\n         if HINT:\n             tl.max_contiguous(tl.multiple_of(offsets, 1024), 1024)\n         x = tl.load(src + offsets, mask=offsets < N)\n         tl.store(dst + offsets, x, mask=offsets < N)\n-    pgm = _kernel[(1,)](dst, src, off, N=1024,\n-                        BLOCK_SIZE=src.shape[0], HINT=has_hints)\n+    pgm = _kernel[(1,)](dst, src, off, N=1024, BLOCK_SIZE=src.shape[0], HINT=has_hints)\n     ptx = pgm.asm[\"ptx\"]\n     if has_hints:\n         assert \"ld.global.v4.b32\" in ptx\n@@ -1882,8 +1732,7 @@ def kernel(x):\n     (2**31, 'i64'), (2**32 - 1, 'i64'), (2**32, 'i64'), (2**63 - 1, 'i64'),\n     (-2**63, 'i64'), (2**63, 'u64'), (2**64 - 1, 'u64')\n ])\n-def test_value_specialization(\n-        value: int, value_type: str, device='cuda') -> None:\n+def test_value_specialization(value: int, value_type: str, device='cuda') -> None:\n     spec_type = None\n \n     def cache_hook(*args, **kwargs):\n@@ -1910,8 +1759,7 @@ def kernel(VALUE, X):\n     \"value, overflow\",\n     [(2**64 - 1, False), (2**64, True), (-2**63, False), (-2**63 - 1, True)]\n )\n-def test_value_specialization_overflow(\n-        value: int, overflow: bool, device='cuda') -> None:\n+def test_value_specialization_overflow(value: int, overflow: bool, device='cuda') -> None:\n \n     @triton.jit\n     def kernel(VALUE, X):\n@@ -1930,8 +1778,7 @@ def kernel(VALUE, X):\n # test constexpr\n # ----------------\n \n-@pytest.mark.parametrize(\"op\", ['+', '-', '*',\n-                         '/', '%', '<', '>', '<<', '>>', '&', '^', '|'])\n+@pytest.mark.parametrize(\"op\", ['+', '-', '*', '/', '%', '<', '>', '<<', '>>', '&', '^', '|'])\n @pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n @pytest.mark.parametrize(\"is_rhs_constexpr\", [True, False])\n def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr):\n@@ -1953,9 +1800,7 @@ def kernel(Z, X, Y):\n         y_str = \"4.13\" if is_rhs_constexpr else \"y\"\n         x = numpy_random((1,), dtype_str=\"float32\")\n         y = numpy_random((1,), dtype_str=\"float32\")\n-    kernel = patch_kernel(\n-        kernel, {\n-            'GENERATE_TEST_HERE': f\"{x_str} {op} {y_str}\"})\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{x_str} {op} {y_str}\"})\n     z = np.array(eval(f\"{x_str} {op} {y_str}\"))\n     x_tri = to_triton(x)\n     y_tri = to_triton(y)\n@@ -2084,9 +1929,7 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n \n     if expr == 'math.log2':\n-        kernel = patch_kernel(\n-            kernel, {\n-                'GENERATE_TEST_HERE': 'tl.broadcast_to(tl.math.log2(5.0), x.shape)'})\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.broadcast_to(tl.math.log2(5.0), x.shape)'})\n         y_ref = np.log2(5.0)\n     elif expr == 'math.ffs':\n         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.ffs(x)'})\n@@ -2096,26 +1939,16 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     elif expr == 'math.pow':\n         # numpy does not allow negative factors in power, so we use abs()\n         x = np.abs(x)\n-        kernel = patch_kernel(kernel,\n-                              {'GENERATE_TEST_HERE': 'tl.math.pow(x, x)'})\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.pow(x, x)'})\n         y_ref = np.power(x, x)\n     elif expr == 'math.norm4d':\n-        kernel = patch_kernel(\n-            kernel, {\n-                'GENERATE_TEST_HERE': 'tl.math.norm4d(x, x, x, x)'})\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.norm4d(x, x, x, x)'})\n         y_ref = np.sqrt(4 * np.power(x, 2))\n \n     x_tri = to_triton(x)\n     # triton result\n-    y_tri = to_triton(\n-        numpy_random(\n-            (shape[0],\n-             ),\n-            dtype_str=dtype_str,\n-            rs=rs),\n-        device='cuda')\n-    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0],\n-                 extern_libs={'libdevice': lib_path})\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n     # compare\n     if expr == 'math.ffs':\n         np.testing.assert_equal(y_ref, to_numpy(y_tri))\n@@ -2146,13 +1979,7 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n \n     # triton result\n     x_tri = to_triton(x)[0].item()\n-    y_tri = to_triton(\n-        numpy_random(\n-            (shape[0],\n-             ),\n-            dtype_str=dtype_str,\n-            rs=rs),\n-        device='cuda')\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'math': lib_path})\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n@@ -2163,8 +1990,7 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"lo, hi, iv\", [(2**35, 2**35 + 20, 1), (2**35, 2**35 + 20, 2), (2**35, 2**35 + 20, 3),\n-                                        (15, -16, -1), (15, -\n-                                                        16, -2), (15, -16, -3),\n+                                        (15, -16, -1), (15, -16, -2), (15, -16, -3),\n                                         (-18, -22, -1), (22, 18, -1)])\n def test_for_iv(lo, hi, iv):\n \n@@ -2328,8 +2154,7 @@ def __str__(self):\n \n \n class BlockedLayout:\n-    def __init__(self, size_per_thread,\n-                 threads_per_warp, warps_per_cta, order):\n+    def __init__(self, size_per_thread, threads_per_warp, warps_per_cta, order):\n         self.sz_per_thread = str(size_per_thread)\n         self.threads_per_warp = str(threads_per_warp)\n         self.warps_per_cta = str(warps_per_cta)\n@@ -2424,8 +2249,7 @@ def kernel(Input, Index, Out, N: int):\n @pytest.mark.parametrize(\"dtype_str\", ['float16', 'int16'])\n def test_ptx_cast(dtype_str):\n     @triton.jit\n-    def kernel(in_ptr0, out_ptr2, xnumel, rnumel, dtype: tl.constexpr,\n-               XBLOCK: tl.constexpr, RBLOCK: tl.constexpr):\n+    def kernel(in_ptr0, out_ptr2, xnumel, rnumel, dtype: tl.constexpr, XBLOCK: tl.constexpr, RBLOCK: tl.constexpr):\n         xoffset = tl.program_id(0) * XBLOCK\n         xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n         xmask = xindex < xnumel\n@@ -2436,15 +2260,13 @@ def kernel(in_ptr0, out_ptr2, xnumel, rnumel, dtype: tl.constexpr,\n             rindex = roffset + rbase\n             rmask = rindex < rnumel\n             r1 = rindex\n-            tmp0 = tl.load(in_ptr0 + (r1 + (197 * x0)),\n-                           rmask & xmask).to(dtype)\n+            tmp0 = tl.load(in_ptr0 + (r1 + (197 * x0)), rmask & xmask).to(dtype)\n             tmp1 = 2\n             tmp2 = tmp0 * tmp1\n             tmp3 = tmp2.to(dtype)\n             tmp5 = _tmp4 < tmp3\n             _tmp4 = tl.where(rmask & xmask & tmp5, tmp3, _tmp4)\n-            tl.store(out_ptr2 + (r1 + (197 * x0) +\n-                     tl.zeros([XBLOCK, RBLOCK], tl.int32)), _tmp4, rmask & xmask)\n+            tl.store(out_ptr2 + (r1 + (197 * x0) + tl.zeros([XBLOCK, RBLOCK], tl.int32)), _tmp4, rmask & xmask)\n \n     torch.manual_seed(123)\n     if dtype_str == 'int16':\n@@ -2457,6 +2279,5 @@ def kernel(in_ptr0, out_ptr2, xnumel, rnumel, dtype: tl.constexpr,\n     s0 = 4\n     buf11 = -torch.ones((6 * s0, 197, 197), device='cuda', dtype=torch_dtype)\n     buf14 = -torch.ones((s0, 6, 197, 197), device='cuda', dtype=torch_dtype)\n-    kernel[(4728,)](buf11, buf14, 1182 * s0, 197,\n-                    triton_dtype, 1, 256, num_warps=2)\n+    kernel[(4728,)](buf11, buf14, 1182 * s0, 197, triton_dtype, 1, 256, num_warps=2)\n     assert buf14.to(torch.float32).mean() == -2.0"}, {"filename": "python/test/unit/language/test_random.py", "status": "modified", "additions": 5, "deletions": 12, "changes": 17, "file_content_changes": "@@ -12,8 +12,7 @@\n \n \n class PhiloxConfig:\n-    def __init__(self, PHILOX_ROUND_A, PHILOX_ROUND_B,\n-                 PHILOX_KEY_A, PHILOX_KEY_B, DTYPE):\n+    def __init__(self, PHILOX_ROUND_A, PHILOX_ROUND_B, PHILOX_KEY_A, PHILOX_KEY_B, DTYPE):\n         self.PHILOX_ROUND_A = np.array(PHILOX_ROUND_A, dtype=DTYPE)\n         self.PHILOX_ROUND_B = np.array(PHILOX_ROUND_B, dtype=DTYPE)\n         self.PHILOX_KEY_A = np.array(PHILOX_KEY_A, dtype=DTYPE)\n@@ -62,16 +61,12 @@ def _into_pieces(self, n, pad=4):\n     def _multiply_low_high(self, a, b):\n         low = a * b\n         high = int(a) * int(b)\n-        high = np.array(\n-            high >> (np.dtype(self._dtype).itemsize * 8),\n-            dtype=self._dtype)\n+        high = np.array(high >> (np.dtype(self._dtype).itemsize * 8), dtype=self._dtype)\n         return low, high\n \n     def _single_round(self, counter, key):\n-        lo0, hi0 = self._multiply_low_high(\n-            self._config.PHILOX_ROUND_A, counter[0])\n-        lo1, hi1 = self._multiply_low_high(\n-            self._config.PHILOX_ROUND_B, counter[2])\n+        lo0, hi0 = self._multiply_low_high(self._config.PHILOX_ROUND_A, counter[0])\n+        lo1, hi1 = self._multiply_low_high(self._config.PHILOX_ROUND_B, counter[2])\n         ret0 = hi1 ^ counter[1] ^ key[0]\n         ret1 = lo1\n         ret2 = hi0 ^ counter[3] ^ key[1]\n@@ -158,9 +153,7 @@ def kernel(X, N, seed):\n     grid = (triton.cdiv(N, BLOCK),)\n     kernel[grid](x, N, seed)\n     assert all((x >= 0) & (x <= 1))\n-    assert scipy.stats.kstest(\n-        x.tolist(), 'uniform', args=(\n-            0, 1)).statistic < 0.01\n+    assert scipy.stats.kstest(x.tolist(), 'uniform', args=(0, 1)).statistic < 0.01\n \n # test normal PRNG\n "}, {"filename": "python/test/unit/language/test_subprocess.py", "status": "modified", "additions": 3, "deletions": 21, "changes": 24, "file_content_changes": "@@ -10,26 +10,13 @@\n \n # TODO: bfloat16 after LLVM-15\n func_types = [\"device_assert\", \"assert\", \"static_assert\"]\n-torch_types = [\n-    \"int8\",\n-    \"uint8\",\n-    \"int16\",\n-    \"int32\",\n-    \"long\",\n-    \"float16\",\n-    \"float32\",\n-    \"float64\"]\n+torch_types = [\"int8\", \"uint8\", \"int16\", \"int32\", \"long\", \"float16\", \"float32\", \"float64\"]\n \n \n @pytest.mark.parametrize(\"func_type, data_type\",\n                          [(\"device_print\", data_type) for data_type in torch_types] + [(\"print\", \"int32\"), (\"static_print\", \"int32\")])\n def test_print(func_type: str, data_type: str):\n-    proc = subprocess.Popen([sys.executable,\n-                             print_path,\n-                             func_type,\n-                             data_type],\n-                            stdout=subprocess.PIPE,\n-                            shell=False)\n+    proc = subprocess.Popen([sys.executable, print_path, func_type, data_type], stdout=subprocess.PIPE, shell=False)\n     outs, _ = proc.communicate()\n     outs = outs.split()\n     new_lines = set()\n@@ -52,12 +39,7 @@ def test_print(func_type: str, data_type: str):\n @pytest.mark.parametrize(\"func_type\", func_types)\n def test_assert(func_type: str):\n     os.environ[\"TRITON_DEBUG\"] = \"1\"\n-    proc = subprocess.Popen([sys.executable,\n-                             assert_path,\n-                             func_type],\n-                            stdout=subprocess.PIPE,\n-                            stderr=subprocess.PIPE,\n-                            shell=False)\n+    proc = subprocess.Popen([sys.executable, assert_path, func_type], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)\n     _, errs = proc.communicate()\n     errs = errs.splitlines()\n     num_errs = 0"}, {"filename": "python/test/unit/operators/test_blocksparse.py", "status": "modified", "additions": 13, "deletions": 41, "changes": 54, "file_content_changes": "@@ -6,21 +6,15 @@\n \n \n def sparsify_tensor(x, mask, block):\n-    ret = torch.empty((x.size(0), mask.sum(), block, block),\n-                      dtype=x.dtype, device=x.device)\n+    ret = torch.empty((x.size(0), mask.sum(), block, block), dtype=x.dtype, device=x.device)\n     for idx, (h, i, j) in enumerate(zip(*mask.nonzero(as_tuple=True))):\n         ret[:, idx, :, :] = x[:, h, i * block:(i + 1) * block, j * block:(j + 1) * block]\n     return ret\n \n \n-def make_pair(shape, device=\"cuda\", alpha=1e-2, beta=0.,\n-              trans=False, data=None, dtype=torch.float32):\n+def make_pair(shape, device=\"cuda\", alpha=1e-2, beta=0., trans=False, data=None, dtype=torch.float32):\n     if data is None:\n-        data = torch.randn(\n-            shape,\n-            dtype=torch.float32,\n-            requires_grad=True,\n-            device=device)\n+        data = torch.randn(shape, dtype=torch.float32, requires_grad=True, device=device)\n     ref_ret = data\n     ref_ret = ref_ret * alpha + beta\n     ref_ret = ref_ret.half().to(dtype)\n@@ -44,19 +38,14 @@ def mask_tensor(x, mask, block, value=0):\n @pytest.mark.parametrize(\"BLOCK\", [16, 32, 64])\n # TODO: float32 fails\n @pytest.mark.parametrize(\"DTYPE\", [torch.float16])\n-def test_matmul(MODE, TRANS_A, TRANS_B, BLOCK, DTYPE,\n-                Z=3, H=2, M=512, N=384, K=256):\n+def test_matmul(MODE, TRANS_A, TRANS_B, BLOCK, DTYPE, Z=3, H=2, M=512, N=384, K=256):\n     seed = 0\n     torch.manual_seed(seed)\n     is_sdd = MODE == \"sdd\"\n     is_dsd = MODE == \"dsd\"\n     is_dds = MODE == \"dds\"\n-\n-    def do_sparsify(x):\n-        return sparsify_tensor(x, layout, BLOCK)\n-\n-    def do_mask(x):\n-        return mask_tensor(x, layout, BLOCK)\n+    do_sparsify = lambda x: sparsify_tensor(x, layout, BLOCK)\n+    do_mask = lambda x: mask_tensor(x, layout, BLOCK)\n     # create inputs\n     # create op\n     a_shape = (Z, H, K, M) if TRANS_A else (Z, H, M, K)\n@@ -92,13 +81,7 @@ def do_mask(x):\n     b_tri = do_sparsify(b_tri) if is_dds else b_tri\n     a_tri.retain_grad()\n     b_tri.retain_grad()\n-    op = triton.ops.blocksparse.matmul(\n-        layout,\n-        BLOCK,\n-        MODE,\n-        trans_a=TRANS_A,\n-        trans_b=TRANS_B,\n-        device=\"cuda\")\n+    op = triton.ops.blocksparse.matmul(layout, BLOCK, MODE, trans_a=TRANS_A, trans_b=TRANS_B, device=\"cuda\")\n     c_tri = triton.testing.catch_oor(lambda: op(a_tri, b_tri), pytest)\n     triton.testing.catch_oor(lambda: c_tri.backward(dc_tri), pytest)\n     da_tri = a_tri.grad\n@@ -151,8 +134,7 @@ def test_softmax(BLOCK, WIDTH, is_dense, Z=2, H=2, is_causal=True, scale=0.4):\n     a_tri = sparsify_tensor(a_tri, layout, BLOCK)\n     a_tri.retain_grad()\n     dout_tri = sparsify_tensor(dout_tri, layout, BLOCK)\n-    op = triton.ops.blocksparse.softmax(\n-        layout, BLOCK, device=\"cuda\", is_dense=is_dense)\n+    op = triton.ops.blocksparse.softmax(layout, BLOCK, device=\"cuda\", is_dense=is_dense)\n     out_tri = op(a_tri, scale=scale, is_causal=is_causal)\n     out_tri.backward(dout_tri)\n     da_tri = a_tri.grad\n@@ -184,19 +166,12 @@ def test_attention_fwd_bwd(\n \n     # Triton:\n     n_blocks = n_ctx // block\n-    layout = torch.tril(torch.ones(\n-        [n_heads, n_blocks, n_blocks], dtype=torch.long))\n+    layout = torch.tril(torch.ones([n_heads, n_blocks, n_blocks], dtype=torch.long))\n     query, key, value = [x.clone() for x in qkvs]\n     query.retain_grad()\n     key.retain_grad()\n     value.retain_grad()\n-    attn_out = triton_attention(\n-        layout,\n-        block,\n-        query=query,\n-        key=key,\n-        value=value,\n-        scale=scale)\n+    attn_out = triton_attention(layout, block, query=query, key=key, value=value, scale=scale)\n     # ad hoc loss\n     loss = (attn_out ** 2).mean()\n     loss.backward()\n@@ -235,12 +210,9 @@ def triton_attention(\n     value: torch.Tensor,\n     scale: float,\n ):\n-    sparse_dot_sdd_nt = triton.ops.blocksparse.matmul(\n-        layout, block, \"sdd\", trans_a=False, trans_b=True, device=value.device)\n-    sparse_dot_dsd_nn = triton.ops.blocksparse.matmul(\n-        layout, block, \"dsd\", trans_a=False, trans_b=False, device=value.device)\n-    sparse_softmax = triton.ops.blocksparse.softmax(\n-        layout, block, device=value.device)\n+    sparse_dot_sdd_nt = triton.ops.blocksparse.matmul(layout, block, \"sdd\", trans_a=False, trans_b=True, device=value.device)\n+    sparse_dot_dsd_nn = triton.ops.blocksparse.matmul(layout, block, \"dsd\", trans_a=False, trans_b=False, device=value.device)\n+    sparse_softmax = triton.ops.blocksparse.softmax(layout, block, device=value.device)\n \n     w = sparse_dot_sdd_nt(query, key)\n     w = sparse_softmax(w, scale=scale, is_causal=True)"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 4, "deletions": 17, "changes": 21, "file_content_changes": "@@ -66,8 +66,7 @@\n         ]\n     ),\n )\n-def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K,\n-            NWARP, NSTAGE, M, N, K, AT, BT, DTYPE):\n+def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, DTYPE):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n@@ -77,18 +76,9 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K,\n         pytest.skip(\"bfloat16 matmuls don't allow split_k for now\")\n     torch.manual_seed(0)\n     # nuke kernel decorators -- will set meta-parameters manually\n-    kwargs = {\n-        'BLOCK_M': BLOCK_M,\n-        'BLOCK_N': BLOCK_N,\n-        'BLOCK_K': BLOCK_K,\n-        'SPLIT_K': SPLIT_K}\n+    kwargs = {'BLOCK_M': BLOCK_M, 'BLOCK_N': BLOCK_N, 'BLOCK_K': BLOCK_K, 'SPLIT_K': SPLIT_K}\n     pre_hook = None if SPLIT_K == 1 else lambda nargs: nargs['C'].zero_()\n-    configs = [\n-        triton.Config(\n-            kwargs=kwargs,\n-            num_warps=NWARP,\n-            num_stages=NSTAGE,\n-            pre_hook=pre_hook)]\n+    configs = [triton.Config(kwargs=kwargs, num_warps=NWARP, num_stages=NSTAGE, pre_hook=pre_hook)]\n     kernel = triton.ops._matmul.kernel\n     kernel.configs = configs\n     # kernel.run = kernel.run.run.run\n@@ -98,10 +88,7 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K,\n     N = BLOCK_N if N is None else N\n     K = BLOCK_K * SPLIT_K if K is None else K\n     # allocate/transpose inputs\n-    DTYPE = {\n-        \"float16\": torch.float16,\n-        \"bfloat16\": torch.bfloat16,\n-        \"float32\": torch.float32}[DTYPE]\n+    DTYPE = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[DTYPE]\n     a = .1 * torch.randn((K, M) if AT else (M, K), device=\"cuda\", dtype=DTYPE)\n     b = .1 * torch.randn((N, K) if BT else (K, N), device=\"cuda\", dtype=DTYPE)\n     a = a.t() if AT else a"}, {"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 4, "deletions": 32, "changes": 36, "file_content_changes": "@@ -140,14 +140,7 @@ def kernel_add(a, b, o, N: tl.constexpr):\n         32,\n     ]\n     assert len(kernel_add.cache) == 0\n-    kernel_add.warmup(\n-        torch.float32,\n-        torch.float32,\n-        torch.float32,\n-        32,\n-        grid=(\n-            1,\n-        ))\n+    kernel_add.warmup(torch.float32, torch.float32, torch.float32, 32, grid=(1,))\n     assert len(kernel_add.cache) == 1\n     kernel_add.warmup(*args, grid=(1,))\n     assert len(kernel_add.cache) == 1\n@@ -165,34 +158,13 @@ def kernel_add(a, b, o, N: tl.constexpr):\n \n     device = torch.cuda.current_device()\n     assert len(kernel_add.cache[device]) == 0\n-    kernel_add.warmup(\n-        torch.float32,\n-        torch.float32,\n-        torch.float32,\n-        32,\n-        grid=(\n-            1,\n-        ))\n+    kernel_add.warmup(torch.float32, torch.float32, torch.float32, 32, grid=(1,))\n     assert len(kernel_add.cache[device]) == 1\n     kernel_add.debug = False\n-    kernel_add.warmup(\n-        torch.float32,\n-        torch.float32,\n-        torch.float32,\n-        32,\n-        grid=(\n-            1,\n-        ))\n+    kernel_add.warmup(torch.float32, torch.float32, torch.float32, 32, grid=(1,))\n     assert len(kernel_add.cache[device]) == 1\n     kernel_add.debug = True\n-    kernel_add.warmup(\n-        torch.float32,\n-        torch.float32,\n-        torch.float32,\n-        32,\n-        grid=(\n-            1,\n-        ))\n+    kernel_add.warmup(torch.float32, torch.float32, torch.float32, 32, grid=(1,))\n     assert len(kernel_add.cache[device]) == 2\n \n "}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 15, "deletions": 30, "changes": 45, "file_content_changes": "@@ -49,8 +49,7 @@ class SIGNEDNESS(Enum):\n \n     def __init__(self, name):\n         self.name = name\n-        assert name in dtype.SINT_TYPES + dtype.UINT_TYPES + \\\n-            dtype.FP_TYPES + dtype.OTHER_TYPES, name\n+        assert name in dtype.SINT_TYPES + dtype.UINT_TYPES + dtype.FP_TYPES + dtype.OTHER_TYPES, name\n         if name in dtype.SINT_TYPES:\n             self.int_signedness = dtype.SIGNEDNESS.SIGNED\n             self.int_bitwidth = int(name.split('int')[-1])\n@@ -261,8 +260,7 @@ def __init__(self, element_ty: dtype, shape: List):\n         for s in self.shape:\n             self.numel *= s\n         if self.numel > TRITON_MAX_TENSOR_NUMEL:\n-            raise ValueError(\n-                f\"numel ({self.numel}) exceeds triton maximum tensor numel ({TRITON_MAX_TENSOR_NUMEL})\")\n+            raise ValueError(f\"numel ({self.numel}) exceeds triton maximum tensor numel ({TRITON_MAX_TENSOR_NUMEL})\")\n \n         self.name = self.__str__()\n \n@@ -295,8 +293,7 @@ def scalar(self):\n \n \n class function_type(dtype):\n-    def __init__(self, ret_types: List[dtype],\n-                 param_types: List[dtype]) -> None:\n+    def __init__(self, ret_types: List[dtype], param_types: List[dtype]) -> None:\n         self.ret_types = ret_types\n         self.param_types = param_types\n \n@@ -473,8 +470,7 @@ def __init__(self, handle, type: dtype):\n \n     def __str__(self) -> str:\n         # ex. \"float32[3,4]\"\n-        return str(self.dtype) + '[' + ','.join(str(s)\n-                                                for s in self.shape) + ']'\n+        return str(self.dtype) + '[' + ','.join(str(s) for s in self.shape) + ']'\n \n     @builtin\n     def __add__(self, other, _builder=None):\n@@ -764,8 +760,7 @@ def _shape_check_impl(shape):\n         if not isinstance(d, constexpr):\n             raise TypeError(f\"Shape element {i} must have type `constexpr`\")\n         if not isinstance(d.value, int):\n-            raise TypeError(\n-                f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n+            raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n     return [_constexpr_to_value(x) for x in shape]\n \n \n@@ -889,8 +884,7 @@ def dot(input, other, allow_tf32=True, out_dtype=float32, _builder=None):\n \n \n @builtin\n-def load(pointer, mask=None, other=None, cache_modifier=\"\",\n-         eviction_policy=\"\", volatile=False, _builder=None):\n+def load(pointer, mask=None, other=None, cache_modifier=\"\", eviction_policy=\"\", volatile=False, _builder=None):\n     \"\"\"\n     Return a tensor of data whose values are, elementwise, loaded from memory at location defined by :code:`pointer`.\n \n@@ -915,13 +909,11 @@ def load(pointer, mask=None, other=None, cache_modifier=\"\",\n     cache_modifier = _constexpr_to_value(cache_modifier)\n     eviction_policy = _constexpr_to_value(eviction_policy)\n     volatile = _constexpr_to_value(volatile)\n-    return semantic.load(pointer, mask, other, cache_modifier,\n-                         eviction_policy, volatile, _builder)\n+    return semantic.load(pointer, mask, other, cache_modifier, eviction_policy, volatile, _builder)\n \n \n @builtin\n-def store(pointer, value, mask=None, cache_modifier=\"\",\n-          eviction_policy=\"\", _builder=None):\n+def store(pointer, value, mask=None, cache_modifier=\"\", eviction_policy=\"\", _builder=None):\n     \"\"\"\n     Stores :code:`value` tensor of elements in memory, element-wise, at the memory locations specified by :code:`pointer`.\n \n@@ -940,8 +932,7 @@ def store(pointer, value, mask=None, cache_modifier=\"\",\n         mask = _to_tensor(mask, _builder)\n     cache_modifier = _constexpr_to_value(cache_modifier)\n     eviction_policy = _constexpr_to_value(eviction_policy)\n-    return semantic.store(pointer, value, mask,\n-                          cache_modifier, eviction_policy, _builder)\n+    return semantic.store(pointer, value, mask, cache_modifier, eviction_policy, _builder)\n \n \n # -----------------------\n@@ -1204,8 +1195,7 @@ def multiple_of(input, values, _builder=None):\n         if not isinstance(d, constexpr):\n             raise TypeError(f\"values element {i} must have type `constexpr`\")\n         if not isinstance(d.value, int):\n-            raise TypeError(\n-                f\"values element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n+            raise TypeError(f\"values element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n     values = [x.value for x in values]\n     return semantic.multiple_of(input, values)\n \n@@ -1221,8 +1211,7 @@ def max_contiguous(input, values, _builder=None):\n         if not isinstance(d, constexpr):\n             raise TypeError(f\"values element {i} must have type `constexpr`\")\n         if not isinstance(d.value, int):\n-            raise TypeError(\n-                f\"values element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n+            raise TypeError(f\"values element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n     values = [x.value for x in values]\n     return semantic.max_contiguous(input, values)\n \n@@ -1353,8 +1342,7 @@ def zeros_like(input):\n \n \n @builtin\n-def static_print(*values, sep: str = \" \", end: str = \"\\n\",\n-                 file=None, flush=False, _builder=None):\n+def static_print(*values, sep: str = \" \", end: str = \"\\n\", file=None, flush=False, _builder=None):\n     pass\n \n \n@@ -1397,8 +1385,7 @@ def device_assert(cond, msg=\"\", _builder=None):\n     # where the triton function is called but not where the\n     # device_assert is called. Need to enhance this.\n     lineno = frame.f_back.f_lineno\n-    return semantic.device_assert(_to_tensor(\n-        cond, _builder), msg, file_name, func_name, lineno, _builder)\n+    return semantic.device_assert(_to_tensor(cond, _builder), msg, file_name, func_name, lineno, _builder)\n \n # -----------------------\n # Iterators\n@@ -1425,9 +1412,7 @@ def __init__(self, arg1, arg2=None, step=None):\n             self.end = arg2\n \n     def __iter__(self):\n-        raise RuntimeError(\n-            \"static_range can only be used in @triton.jit'd functions\")\n+        raise RuntimeError(\"static_range can only be used in @triton.jit'd functions\")\n \n     def __next__(self):\n-        raise RuntimeError(\n-            \"static_range can only be used in @triton.jit'd functions\")\n+        raise RuntimeError(\"static_range can only be used in @triton.jit'd functions\")"}, {"filename": "python/triton/language/extern.py", "status": "modified", "additions": 4, "deletions": 8, "changes": 12, "file_content_changes": "@@ -3,8 +3,7 @@\n from . import core, semantic\n \n \n-def dispatch(func, lib_name: str, lib_path: str, args: list,\n-             arg_type_symbol_dict: dict, ret_shape: tuple, _builder=None):\n+def dispatch(func, lib_name: str, lib_path: str, args: list, arg_type_symbol_dict: dict, ret_shape: tuple, _builder=None):\n     '''\n         Dispatch a function to a library\n         :param func: the function to dispatch\n@@ -43,12 +42,10 @@ def dispatch(func, lib_name: str, lib_path: str, args: list,\n         ret_type = arg_type_symbol_dict[arg_types][1]\n         if ret_shape:\n             ret_type = core.block_type(ret_type, ret_shape)\n-        return core.tensor(func(lib_name, lib_path, symbol,\n-                           arg_list, ret_type.to_ir(_builder)), ret_type)\n+        return core.tensor(func(lib_name, lib_path, symbol, arg_list, ret_type.to_ir(_builder)), ret_type)\n \n \n-def elementwise(lib_name: str, lib_path: str, args: list,\n-                arg_type_symbol_dict: dict, _builder=None):\n+def elementwise(lib_name: str, lib_path: str, args: list, arg_type_symbol_dict: dict, _builder=None):\n     '''\n         Dispatch an elementwise function to a library\n         :param lib_name: the name of the library\n@@ -77,5 +74,4 @@ def elementwise(lib_name: str, lib_path: str, args: list,\n                 dispatch_args[i], broadcast_arg, _builder)\n         ret_shape = broadcast_arg.shape\n     func = getattr(_builder, \"create_external_elementwise\")\n-    return dispatch(func, lib_name, lib_path, dispatch_args,\n-                    arg_type_symbol_dict, ret_shape, _builder)\n+    return dispatch(func, lib_name, lib_path, dispatch_args, arg_type_symbol_dict, ret_shape, _builder)"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 110, "deletions": 259, "changes": 369, "file_content_changes": "@@ -11,8 +11,7 @@ class IncompatibleTypeErrorImpl(Exception):\n     def __init__(self, type_a, type_b):\n         self.type_a = type_a\n         self.type_b = type_b\n-        self.message = \"invalid operands of type \" + \\\n-            self.type_a.__repr__() + \" and \" + self.type_b.__repr__()\n+        self.message = \"invalid operands of type \" + self.type_a.__repr__() + \" and \" + self.type_b.__repr__()\n         super(IncompatibleTypeErrorImpl, self).__init__(self.message)\n \n \n@@ -48,8 +47,7 @@ def integer_promote_impl(a_ty: tl.dtype, b_ty: tl.dtype) -> tl.dtype:\n     assert False\n \n \n-def computation_type_impl(a_ty: tl.dtype, b_ty: tl.dtype,\n-                          div_or_mod: bool) -> tl.dtype:\n+def computation_type_impl(a_ty: tl.dtype, b_ty: tl.dtype, div_or_mod: bool) -> tl.dtype:\n     # 1) if one operand is double, the other is implicitly\n     #    converted to double\n     if a_ty.is_fp64() or b_ty.is_fp64():\n@@ -60,8 +58,7 @@ def computation_type_impl(a_ty: tl.dtype, b_ty: tl.dtype,\n         return tl.float32\n     # 3 ) if one operand is half, the other is implicitly converted to half\n     #     unless we're doing / or %, which do not exist natively in PTX for fp16.\n-    # Supported PTX op: add, sub, mul, fma, neg, abs, min, max, tanh, ex2,\n-    # setp\n+    #     Supported PTX op: add, sub, mul, fma, neg, abs, min, max, tanh, ex2, setp\n     if a_ty.is_fp16() or b_ty.is_fp16():\n         if div_or_mod:\n             return tl.float32\n@@ -88,8 +85,7 @@ def computation_type_impl(a_ty: tl.dtype, b_ty: tl.dtype,\n # ===----------------------------------------------------------------------===//\n \n \n-def check_ptr_type_impl(type_a: tl.dtype, type_b: tl.dtype,\n-                        allow_ptr_a: bool) -> None:\n+def check_ptr_type_impl(type_a: tl.dtype, type_b: tl.dtype, allow_ptr_a: bool) -> None:\n     if type_a.is_ptr():\n         if not allow_ptr_a:\n             raise IncompatibleTypeErrorImpl(type_a, type_b)\n@@ -124,8 +120,7 @@ def binary_op_type_checking_impl(lhs: tl.tensor,\n def add(input: tl.tensor,\n         other: tl.tensor,\n         builder: ir.builder) -> tl.tensor:\n-    input, other = binary_op_type_checking_impl(\n-        input, other, builder, True, True)\n+    input, other = binary_op_type_checking_impl(input, other, builder, True, True)\n     input_scalar_ty = input.type.scalar\n     other_scalar_ty = other.type.scalar\n \n@@ -134,37 +129,31 @@ def add(input: tl.tensor,\n     if other_scalar_ty.is_ptr() and not input_scalar_ty.is_ptr():\n         input, other = other, input\n     if input_scalar_ty.is_ptr():\n-        return tl.tensor(builder.create_addptr(\n-            input.handle, other.handle), input.type)\n+        return tl.tensor(builder.create_addptr(input.handle, other.handle), input.type)\n     # float + float\n     elif input_scalar_ty.is_floating():\n-        return tl.tensor(builder.create_fadd(\n-            input.handle, other.handle), input.type)\n+        return tl.tensor(builder.create_fadd(input.handle, other.handle), input.type)\n     # int + int\n     elif input_scalar_ty.is_int():\n-        return tl.tensor(builder.create_add(\n-            input.handle, other.handle), input.type)\n+        return tl.tensor(builder.create_add(input.handle, other.handle), input.type)\n     assert False\n \n \n def sub(input: tl.tensor,\n         other: tl.tensor,\n         builder: ir.builder) -> tl.tensor:\n-    input, other = binary_op_type_checking_impl(\n-        input, other, builder, True, False)\n+    input, other = binary_op_type_checking_impl(input, other, builder, True, False)\n     scalar_ty = input.type.scalar\n     # ptr - offset\n     if scalar_ty.is_ptr():\n         return tl.tensor(builder.create_addptr(input.handle, minus(other, builder).handle),\n                          input.type)\n     # float - float\n     if scalar_ty.is_floating():\n-        return tl.tensor(builder.create_fsub(\n-            input.handle, other.handle), input.type)\n+        return tl.tensor(builder.create_fsub(input.handle, other.handle), input.type)\n     # int - int\n     elif scalar_ty.is_int():\n-        return tl.tensor(builder.create_sub(\n-            input.handle, other.handle), input.type)\n+        return tl.tensor(builder.create_sub(input.handle, other.handle), input.type)\n     assert False\n \n \n@@ -175,20 +164,17 @@ def mul(input: tl.tensor,\n     scalar_ty = input.type.scalar\n     # float * float\n     if scalar_ty.is_floating():\n-        return tl.tensor(builder.create_fmul(\n-            input.handle, other.handle), input.type)\n+        return tl.tensor(builder.create_fmul(input.handle, other.handle), input.type)\n     # * int\n     elif scalar_ty.is_int():\n-        return tl.tensor(builder.create_mul(\n-            input.handle, other.handle), input.type)\n+        return tl.tensor(builder.create_mul(input.handle, other.handle), input.type)\n     assert False\n \n \n def truediv(input: tl.tensor,\n             other: tl.tensor,\n             builder: ir.builder) -> tl.tensor:\n-    input, other = binary_op_type_checking_impl(\n-        input, other, builder, False, False, True, True)\n+    input, other = binary_op_type_checking_impl(input, other, builder, False, False, True, True)\n     input_scalar_ty = input.type.scalar\n     other_scalar_ty = other.type.scalar\n     # float / int\n@@ -210,27 +196,23 @@ def truediv(input: tl.tensor,\n     # unreachable\n     else:\n         assert False\n-    return tl.tensor(builder.create_fdiv(\n-        input.handle, other.handle), input.type)\n+    return tl.tensor(builder.create_fdiv(input.handle, other.handle), input.type)\n \n \n def floordiv(input: tl.tensor,\n              other: tl.tensor,\n              builder: ir.builder) -> tl.tensor:\n-    input, other = binary_op_type_checking_impl(\n-        input, other, builder, False, False, True, True)\n+    input, other = binary_op_type_checking_impl(input, other, builder, False, False, True, True)\n     input_scalar_ty = input.type.scalar\n     other_scalar_ty = other.type.scalar\n     if input_scalar_ty.is_int() and other_scalar_ty.is_int():\n         ret_ty = integer_promote_impl(input_scalar_ty, other_scalar_ty)\n         input = cast(input, ret_ty, builder)\n         other = cast(other, ret_ty, builder)\n         if ret_ty.is_int_signed():\n-            return tl.tensor(builder.create_sdiv(\n-                input.handle, other.handle), input.type)\n+            return tl.tensor(builder.create_sdiv(input.handle, other.handle), input.type)\n         else:\n-            return tl.tensor(builder.create_udiv(\n-                input.handle, other.handle), input.type)\n+            return tl.tensor(builder.create_udiv(input.handle, other.handle), input.type)\n     assert False\n \n \n@@ -241,19 +223,16 @@ def fdiv(input: tl.tensor,\n     input_scalar_ty = input.type.scalar\n     other_scalar_ty = other.type.scalar\n     if not input_scalar_ty.is_floating() or not other_scalar_ty.is_floating():\n-        raise ValueError(\n-            \"both operands of fdiv must have floating scalar type\")\n-    input, other = binary_op_type_checking_impl(\n-        input, other, builder, False, False, False, True)\n+        raise ValueError(\"both operands of fdiv must have floating scalar type\")\n+    input, other = binary_op_type_checking_impl(input, other, builder, False, False, False, True)\n     ret = builder.create_fdiv(input.handle, other.handle)\n     return tl.tensor(ret, input.type)\n \n \n def mod(input: tl.tensor,\n         other: tl.tensor,\n         builder: ir.builder) -> tl.tensor:\n-    input, other = binary_op_type_checking_impl(\n-        input, other, builder, False, False, True, True)\n+    input, other = binary_op_type_checking_impl(input, other, builder, False, False, True, True)\n     scalar_ty = input.type.scalar\n     other_scalar_ty = other.type.scalar\n     # float % float\n@@ -270,11 +249,9 @@ def mod(input: tl.tensor,\n                              \"because they have different signedness;\"\n                              \"this is unlikely to result in a useful answer. Cast them to the same signedness.\")\n         if scalar_ty.is_int_signed():\n-            return tl.tensor(builder.create_srem(\n-                input.handle, other.handle), input.type)\n+            return tl.tensor(builder.create_srem(input.handle, other.handle), input.type)\n         else:\n-            return tl.tensor(builder.create_urem(\n-                input.handle, other.handle), input.type)\n+            return tl.tensor(builder.create_urem(input.handle, other.handle), input.type)\n     assert False\n \n ##############\n@@ -285,8 +262,7 @@ def mod(input: tl.tensor,\n def bitwise_op_type_checking_impl(input: tl.tensor,\n                                   other: tl.tensor,\n                                   builder: ir.builder) -> Tuple[tl.tensor, tl.tensor]:\n-    input, other = binary_op_type_checking_impl(\n-        input, other, builder, False, False, False)\n+    input, other = binary_op_type_checking_impl(input, other, builder, False, False, False)\n     input_sca_ty = input.type.scalar\n     other_sca_ty = other.type.scalar\n     if not input_sca_ty.is_int() or not other_sca_ty.is_int():\n@@ -303,8 +279,7 @@ def and_(input: tl.tensor,\n          other: tl.tensor,\n          builder: ir.builder) -> tl.tensor:\n     input, other = bitwise_op_type_checking_impl(input, other, builder)\n-    return tl.tensor(builder.create_and(\n-        input.handle, other.handle), input.type)\n+    return tl.tensor(builder.create_and(input.handle, other.handle), input.type)\n \n \n def or_(input: tl.tensor,\n@@ -318,21 +293,18 @@ def xor_(input: tl.tensor,\n          other: tl.tensor,\n          builder: ir.builder) -> tl.tensor:\n     input, other = bitwise_op_type_checking_impl(input, other, builder)\n-    return tl.tensor(builder.create_xor(\n-        input.handle, other.handle), input.type)\n+    return tl.tensor(builder.create_xor(input.handle, other.handle), input.type)\n \n \n-def logical_and(input: tl.tensor, other: tl.tensor,\n-                builder: ir.builder) -> tl.tensor:\n+def logical_and(input: tl.tensor, other: tl.tensor, builder: ir.builder) -> tl.tensor:\n     if not input.type.is_int1():\n         input = bitcast(input, tl.dtype(\"int1\"), builder)\n     if not other.type.is_int1():\n         other = bitcast(other, tl.dtype(\"int1\"), builder)\n     return and_(input, other, builder)\n \n \n-def logical_or(input: tl.tensor, other: tl.tensor,\n-               builder: ir.builder) -> tl.tensor:\n+def logical_or(input: tl.tensor, other: tl.tensor, builder: ir.builder) -> tl.tensor:\n     if not input.type.is_int1():\n         input = bitcast(input, tl.dtype(\"int1\"), builder)\n     if not other.type.is_int1():\n@@ -350,24 +322,21 @@ def lshr(input: tl.tensor,\n          other: tl.tensor,\n          builder: ir.builder) -> tl.tensor:\n     input, other = bitwise_op_type_checking_impl(input, other, builder)\n-    return tl.tensor(builder.create_lshr(\n-        input.handle, other.handle), input.type)\n+    return tl.tensor(builder.create_lshr(input.handle, other.handle), input.type)\n \n \n def ashr(input: tl.tensor,\n          other: tl.tensor,\n          builder: ir.builder) -> tl.tensor:\n     input, other = bitwise_op_type_checking_impl(input, other, builder)\n-    return tl.tensor(builder.create_ashr(\n-        input.handle, other.handle), input.type)\n+    return tl.tensor(builder.create_ashr(input.handle, other.handle), input.type)\n \n \n def shl(input: tl.tensor,\n         other: tl.tensor,\n         builder: ir.builder) -> tl.tensor:\n     input, other = bitwise_op_type_checking_impl(input, other, builder)\n-    return tl.tensor(builder.create_shl(\n-        input.handle, other.handle), input.type)\n+    return tl.tensor(builder.create_shl(input.handle, other.handle), input.type)\n \n # ===----------------------------------------------------------------------===//\n #                               Unary Operators\n@@ -382,25 +351,17 @@ def minus(input: tl.tensor,\n           builder: ir.builder) -> tl.tensor:\n     input_sca_ty = input.type.scalar\n     if input_sca_ty.is_ptr():\n-        raise ValueError(\n-            f\"wrong type argument to unary minus ({input_sca_ty!r})\")\n-    _0 = tl.tensor(\n-        builder.get_null_value(\n-            input_sca_ty.to_ir(builder)),\n-        input_sca_ty)\n+        raise ValueError(\"wrong type argument to unary minus (\" + input_sca_ty.__repr__() + \")\")\n+    _0 = tl.tensor(builder.get_null_value(input_sca_ty.to_ir(builder)), input_sca_ty)\n     return sub(_0, input, builder)\n \n \n def invert(input: tl.tensor,\n            builder: tl.tensor) -> tl.tensor:\n     input_sca_ty = input.type.scalar\n     if input_sca_ty.is_ptr() or input_sca_ty.is_floating():\n-        raise ValueError(\n-            f\"wrong type argument to unary invert ({input_sca_ty!r})\")\n-    _1 = tl.tensor(\n-        builder.get_all_ones_value(\n-            input_sca_ty.to_ir(builder)),\n-        input_sca_ty)\n+        raise ValueError(\"wrong type argument to unary invert (\" + input_sca_ty.__repr__() + \")\")\n+    _1 = tl.tensor(builder.get_all_ones_value(input_sca_ty.to_ir(builder)), input_sca_ty)\n     return xor_(input, _1, builder)\n \n \n@@ -421,16 +382,13 @@ def greater_than(input: tl.tensor,\n     scalar_ty = input.type.scalar\n     # float > float\n     if scalar_ty.is_floating():\n-        return tl.tensor(builder.create_fcmpOGT(\n-            input.handle, other.handle), _bool_like(input))\n+        return tl.tensor(builder.create_fcmpOGT(input.handle, other.handle), _bool_like(input))\n     # > int\n     elif scalar_ty.is_int():\n         if scalar_ty.is_int_signed():\n-            return tl.tensor(builder.create_icmpSGT(\n-                input.handle, other.handle), _bool_like(input))\n+            return tl.tensor(builder.create_icmpSGT(input.handle, other.handle), _bool_like(input))\n         else:\n-            return tl.tensor(builder.create_icmpUGT(\n-                input.handle, other.handle), _bool_like(input))\n+            return tl.tensor(builder.create_icmpUGT(input.handle, other.handle), _bool_like(input))\n     assert False\n \n \n@@ -441,16 +399,13 @@ def greater_equal(input: tl.tensor,\n     scalar_ty = input.type.scalar\n     # float >= float\n     if scalar_ty.is_floating():\n-        return tl.tensor(builder.create_fcmpOGE(\n-            input.handle, other.handle), _bool_like(input))\n+        return tl.tensor(builder.create_fcmpOGE(input.handle, other.handle), _bool_like(input))\n     # >= int\n     elif scalar_ty.is_int():\n         if scalar_ty.is_int_signed():\n-            return tl.tensor(builder.create_icmpSGE(\n-                input.handle, other.handle), _bool_like(input))\n+            return tl.tensor(builder.create_icmpSGE(input.handle, other.handle), _bool_like(input))\n         else:\n-            return tl.tensor(builder.create_icmpUGE(\n-                input.handle, other.handle), _bool_like(input))\n+            return tl.tensor(builder.create_icmpUGE(input.handle, other.handle), _bool_like(input))\n     assert False\n \n \n@@ -461,16 +416,13 @@ def less_than(input: tl.tensor,\n     scalar_ty = input.type.scalar\n     # float < float\n     if scalar_ty.is_floating():\n-        return tl.tensor(builder.create_fcmpOLT(\n-            input.handle, other.handle), _bool_like(input))\n+        return tl.tensor(builder.create_fcmpOLT(input.handle, other.handle), _bool_like(input))\n     # < int\n     elif scalar_ty.is_int():\n         if scalar_ty.is_int_signed():\n-            return tl.tensor(builder.create_icmpSLT(\n-                input.handle, other.handle), _bool_like(input))\n+            return tl.tensor(builder.create_icmpSLT(input.handle, other.handle), _bool_like(input))\n         else:\n-            return tl.tensor(builder.create_icmpULT(\n-                input.handle, other.handle), _bool_like(input))\n+            return tl.tensor(builder.create_icmpULT(input.handle, other.handle), _bool_like(input))\n     assert False\n \n \n@@ -481,16 +433,13 @@ def less_equal(input: tl.tensor,\n     scalar_ty = input.type.scalar\n     # float < float\n     if scalar_ty.is_floating():\n-        return tl.tensor(builder.create_fcmpOLE(\n-            input.handle, other.handle), _bool_like(input))\n+        return tl.tensor(builder.create_fcmpOLE(input.handle, other.handle), _bool_like(input))\n     # < int\n     elif scalar_ty.is_int():\n         if scalar_ty.is_int_signed():\n-            return tl.tensor(builder.create_icmpSLE(\n-                input.handle, other.handle), _bool_like(input))\n+            return tl.tensor(builder.create_icmpSLE(input.handle, other.handle), _bool_like(input))\n         else:\n-            return tl.tensor(builder.create_icmpULE(\n-                input.handle, other.handle), _bool_like(input))\n+            return tl.tensor(builder.create_icmpULE(input.handle, other.handle), _bool_like(input))\n     assert False\n \n \n@@ -501,12 +450,10 @@ def equal(input: tl.tensor,\n     scalar_ty = input.type.scalar\n     # float == float\n     if scalar_ty.is_floating():\n-        return tl.tensor(builder.create_fcmpOEQ(\n-            input.handle, other.handle), _bool_like(input))\n+        return tl.tensor(builder.create_fcmpOEQ(input.handle, other.handle), _bool_like(input))\n     # == int\n     elif scalar_ty.is_int():\n-        return tl.tensor(builder.create_icmpEQ(\n-            input.handle, other.handle), _bool_like(input))\n+        return tl.tensor(builder.create_icmpEQ(input.handle, other.handle), _bool_like(input))\n     assert False\n \n \n@@ -517,12 +464,10 @@ def not_equal(input: tl.tensor,\n     scalar_ty = input.type.scalar\n     # float == float\n     if scalar_ty.is_floating():\n-        return tl.tensor(builder.create_fcmpUNE(\n-            input.handle, other.handle), _bool_like(input))\n+        return tl.tensor(builder.create_fcmpUNE(input.handle, other.handle), _bool_like(input))\n     # == int\n     elif scalar_ty.is_int():\n-        return tl.tensor(builder.create_icmpNE(\n-            input.handle, other.handle), _bool_like(input))\n+        return tl.tensor(builder.create_icmpNE(input.handle, other.handle), _bool_like(input))\n     assert False\n \n # ===----------------------------------------------------------------------===//\n@@ -538,16 +483,14 @@ def arange(start: int, end: int, builder: ir.builder) -> tl.tensor:\n     if is_start_int64 or is_end_int64:\n         raise ValueError(\"arange must fit in int32\")\n     if end <= start:\n-        raise ValueError(\n-            \"arange's end argument must be greater than the start argument\")\n+        raise ValueError(\"arange's end argument must be greater than the start argument\")\n \n     shape = [end - start]\n     ret_ty = tl.block_type(tl.int32, shape)\n     return tl.tensor(builder.create_make_range(start, end), ret_ty)\n \n \n-def full(shape: List[int], value, dtype: tl.dtype,\n-         builder: ir.builder) -> tl.tensor:\n+def full(shape: List[int], value, dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n     if isinstance(value, tl.tensor):\n         assert value.numel.value == 1, \"only accepts size-1 tensor\"\n         value = cast(value, dtype, builder)\n@@ -561,8 +504,7 @@ def full(shape: List[int], value, dtype: tl.dtype,\n             get_value_fn = getattr(builder, f\"get_{dtype.name}\")\n             value = get_value_fn(value)\n         if dtype is None:\n-            raise ValueError(\n-                \"dtype must be specified when value is not a tensor\")\n+            raise ValueError(\"dtype must be specified when value is not a tensor\")\n         ret_ty = tl.block_type(dtype, shape)\n         return tl.tensor(builder.create_splat(value, shape), ret_ty)\n \n@@ -601,8 +543,7 @@ def expand_dims(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_expand_dims(input.handle, axis), ret_ty)\n \n \n-def cat(lhs: tl.tensor, rhs: tl.tensor, can_reorder: bool,\n-        builder: ir.builder) -> tl.tensor:\n+def cat(lhs: tl.tensor, rhs: tl.tensor, can_reorder: bool, builder: ir.builder) -> tl.tensor:\n     assert can_reorder, \"current implementation of `cat` always may reorder elements\"\n     assert len(lhs.shape) == 1\n     ret_type = tl.block_type(lhs.type.scalar, [lhs.shape[0] + rhs.shape[0]])\n@@ -612,9 +553,7 @@ def cat(lhs: tl.tensor, rhs: tl.tensor, can_reorder: bool,\n def trans(input: tl.tensor, builder: ir.builder) -> tl.tensor:\n     if len(input.shape) != 2:\n         raise ValueError(\"Only 2D tensors can be transposed\")\n-    ret_type = tl.block_type(\n-        input.type.scalar, [\n-            input.shape[1], input.shape[0]])\n+    ret_type = tl.block_type(input.type.scalar, [input.shape[1], input.shape[0]])\n     return tl.tensor(builder.create_trans(input.handle), ret_type)\n \n \n@@ -626,8 +565,7 @@ def broadcast_impl_shape(input: tl.tensor,\n         return tl.tensor(builder.create_splat(input.handle, shape), ret_ty)\n     src_shape = input.type.get_block_shapes()\n     if len(src_shape) != len(shape):\n-        raise ValueError(\n-            f\"Cannot broadcast, rank mismatch: {src_shape}, {shape}\")\n+        raise ValueError(f\"Cannot broadcast, rank mismatch: {src_shape}, {shape}\")\n     if shape == src_shape:\n         return input\n     for i, item in enumerate(src_shape):\n@@ -648,19 +586,11 @@ def broadcast_impl_value(lhs: tl.tensor,\n     # make_shape_compatible(block, scalar)\n     if lhs_ty.is_block() and not rhs_ty.is_block():\n         rhs_ty = tl.block_type(rhs_ty.scalar, lhs_ty.shape)\n-        rhs = tl.tensor(\n-            builder.create_splat(\n-                rhs.handle,\n-                lhs_ty.get_block_shapes()),\n-            rhs_ty)\n+        rhs = tl.tensor(builder.create_splat(rhs.handle, lhs_ty.get_block_shapes()), rhs_ty)\n     # make_shape_compatible(scalar, block)\n     elif not lhs_ty.is_block() and rhs_ty.is_block():\n         lhs_ty = tl.block_type(lhs_ty.scalar, rhs_ty.shape)\n-        lhs = tl.tensor(\n-            builder.create_splat(\n-                lhs.handle,\n-                rhs_ty.get_block_shapes()),\n-            lhs_ty)\n+        lhs = tl.tensor(builder.create_splat(lhs.handle, rhs_ty.get_block_shapes()), lhs_ty)\n     # make_shape_compatible(block, block)\n     elif lhs_ty.is_block() and rhs_ty.is_block():\n         lhs_shape = lhs_ty.get_block_shapes()\n@@ -669,19 +599,13 @@ def broadcast_impl_value(lhs: tl.tensor,\n         if len(lhs_shape) < len(rhs_shape):\n             # Add new axes to lhs\n             for dim in range(len(lhs_shape), len(rhs_shape)):\n-                lhs = tl.tensor(\n-                    builder.create_expand_dims(\n-                        lhs.handle, dim), tl.block_type(\n-                        lhs_ty.scalar, lhs_shape + [1]))\n+                lhs = tl.tensor(builder.create_expand_dims(lhs.handle, dim), tl.block_type(lhs_ty.scalar, lhs_shape + [1]))\n                 lhs_ty = lhs.type\n                 lhs_shape = lhs_ty.get_block_shapes()\n         elif len(rhs_shape) < len(lhs_shape):\n             # Add new axes to rhs\n             for dim in range(len(rhs_shape), len(lhs_shape)):\n-                rhs = tl.tensor(\n-                    builder.create_expand_dims(\n-                        rhs.handle, dim), tl.block_type(\n-                        rhs_ty.scalar, rhs_shape + [1]))\n+                rhs = tl.tensor(builder.create_expand_dims(rhs.handle, dim), tl.block_type(rhs_ty.scalar, rhs_shape + [1]))\n                 rhs_ty = rhs.type\n                 rhs_shape = rhs_ty.get_block_shapes()\n         assert len(rhs_shape) == len(lhs_shape)\n@@ -700,14 +624,10 @@ def broadcast_impl_value(lhs: tl.tensor,\n                                  \"at index \" + str(i) + \": \" + str(left) + \" and \" + str(right))\n         if lhs_shape != ret_shape:\n             ret_ty = tl.block_type(lhs_ty.scalar, ret_shape)\n-            lhs = tl.tensor(\n-                builder.create_broadcast(\n-                    lhs.handle, ret_shape), ret_ty)\n+            lhs = tl.tensor(builder.create_broadcast(lhs.handle, ret_shape), ret_ty)\n         if rhs_shape != ret_shape:\n             ret_ty = tl.block_type(rhs_ty.scalar, ret_shape)\n-            rhs = tl.tensor(\n-                builder.create_broadcast(\n-                    rhs.handle, ret_shape), ret_ty)\n+            rhs = tl.tensor(builder.create_broadcast(rhs.handle, ret_shape), ret_ty)\n     # (scalar, scalar) => returns original blocks\n     return lhs, rhs\n \n@@ -752,8 +672,7 @@ def cast(input: tl.tensor,\n     src_sca_ty = src_ty.scalar\n     dst_sca_ty = dst_ty.scalar\n \n-    # Casting with customized floating types involved: fp8 <=> bf16, fp16,\n-    # fp32, fp64\n+    # Casting with customized floating types involved: fp8 <=> bf16, fp16, fp32, fp64\n     if (src_sca_ty.is_fp8() and dst_sca_ty.is_floating()) or \\\n        (src_sca_ty.is_floating() and dst_sca_ty.is_fp8()):\n         return tl.tensor(builder.create_fp_to_fp(input.handle, dst_ty.to_ir(builder)),\n@@ -839,13 +758,11 @@ def cast(input: tl.tensor,\n \n     # Casting integer types to pointer types\n     if src_sca_ty.is_int() and dst_sca_ty.is_ptr():\n-        return tl.tensor(builder.create_int_to_ptr(\n-            input.handle, dst_ty.to_ir(builder)), dst_ty)\n+        return tl.tensor(builder.create_int_to_ptr(input.handle, dst_ty.to_ir(builder)), dst_ty)\n \n     # Casting pointer types to pointer types\n     if src_sca_ty.is_ptr() and dst_sca_ty.is_ptr():\n-        return tl.tensor(builder.create_bitcast(\n-            input.handle, dst_ty.to_ir(builder)), dst_ty)\n+        return tl.tensor(builder.create_bitcast(input.handle, dst_ty.to_ir(builder)), dst_ty)\n \n     assert False, f'cannot cast {input} to {dst_ty}'\n \n@@ -874,8 +791,7 @@ def str_to_eviction_policy(eviction_policy):\n         elif eviction_policy == \"evict_first\":\n             eviction = ir.EVICTION_POLICY.EVICT_FIRST\n         else:\n-            raise ValueError(\n-                f\"Eviction policy {eviction_policy} not supported\")\n+            raise ValueError(f\"Eviction policy {eviction_policy} not supported\")\n     return eviction\n \n \n@@ -887,23 +803,17 @@ def load(ptr: tl.tensor,\n          is_volatile: bool,\n          builder: ir.builder) -> tl.tensor:\n     if not ptr.type.scalar.is_ptr():\n-        raise ValueError(\n-            f\"Pointer argument of load instruction is {ptr.type!r}\"\n-        )\n+        raise ValueError(\"Pointer argument of load instruction is \" + ptr.type.__repr__())\n     if not ptr.type.is_block():\n         if mask and mask.type.is_block():\n-            raise ValueError(\n-                \"Mask argument cannot be block type if pointer argument is not a block\")\n+            raise ValueError(\"Mask argument cannot be block type if pointer argument is not a block\")\n         if other and other.type.is_block():\n-            raise ValueError(\n-                \"Other argument cannot be block type if pointer argument is not a block\")\n+            raise ValueError(\"Other argument cannot be block type if pointer argument is not a block\")\n     if ptr.type.is_block():\n         if mask:\n-            mask = broadcast_impl_shape(\n-                mask, ptr.type.get_block_shapes(), builder)\n+            mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n         if other:\n-            other = broadcast_impl_shape(\n-                other, ptr.type.get_block_shapes(), builder)\n+            other = broadcast_impl_shape(other, ptr.type.get_block_shapes(), builder)\n \n     ptr_ty = ptr.type.scalar\n     elt_ty = ptr_ty.element_ty\n@@ -948,15 +858,12 @@ def store(ptr: tl.tensor,\n           eviction_policy: str,\n           builder: ir.builder) -> tl.tensor:\n     if not ptr.type.scalar.is_ptr():\n-        raise ValueError(\n-            f\"Pointer argument of store instruction is {ptr.type!r}\")\n+        raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n     if not ptr.type.is_block():\n         if val.type.is_block():\n-            raise ValueError(\n-                \"Value argument cannot be block type if pointer argument is not a block\")\n+            raise ValueError(\"Value argument cannot be block type if pointer argument is not a block\")\n         if mask and mask.type.is_block():\n-            raise ValueError(\n-                \"Mask argument cannot be block type if pointer argument is not a block\")\n+            raise ValueError(\"Mask argument cannot be block type if pointer argument is not a block\")\n     if ptr.type.is_block():\n         val = broadcast_impl_shape(val, ptr.type.get_block_shapes(), builder)\n     if mask and ptr.type.is_block():\n@@ -974,12 +881,10 @@ def store(ptr: tl.tensor,\n     # cast to target data-type\n     val = cast(val, elt_ty, builder)\n     if not mask:\n-        return tl.tensor(builder.create_store(\n-            ptr.handle, val.handle, cache, eviction), tl.void)\n+        return tl.tensor(builder.create_store(ptr.handle, val.handle, cache, eviction), tl.void)\n     if not mask.type.scalar.is_bool():\n         raise ValueError(\"Mask must have boolean scalar type\")\n-    return tl.tensor(builder.create_masked_store(\n-        ptr.handle, val.handle, mask.handle, cache, eviction), tl.void)\n+    return tl.tensor(builder.create_masked_store(ptr.handle, val.handle, mask.handle, cache, eviction), tl.void)\n \n #########\n # atomic\n@@ -992,10 +897,8 @@ def atomic_cas(ptr: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n     element_ty = ptr.type.scalar.element_ty\n     if element_ty.primitive_bitwidth not in [16, 32, 64]:\n-        raise ValueError(\n-            \"atomic_cas only supports elements with width {16, 32, 64}\")\n-    return tl.tensor(builder.create_atomic_cas(\n-        ptr.handle, cmp.handle, val.handle), val.type)\n+        raise ValueError(\"atomic_cas only supports elements with width {16, 32, 64}\")\n+    return tl.tensor(builder.create_atomic_cas(ptr.handle, cmp.handle, val.handle), val.type)\n \n \n def atom_red_typechecking_impl(ptr: tl.tensor,\n@@ -1004,29 +907,24 @@ def atom_red_typechecking_impl(ptr: tl.tensor,\n                                op: str,\n                                builder: ir.builder) -> Tuple[tl.tensor, tl.tensor, tl.tensor]:\n     if not ptr.type.scalar.is_ptr():\n-        raise ValueError(\n-            f\"Pointer argument of store instruction is {ptr.type!r}\")\n+        raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n \n     element_ty = ptr.type.scalar.element_ty\n     if element_ty is tl.float16 and op != 'add':\n-        raise ValueError(f\"atomic_{op} does not support fp16\")\n+        raise ValueError(\"atomic_\" + op + \" does not support fp16\")\n     if element_ty in [tl.int1, tl.int8, tl.int16, tl.bfloat16]:\n-        raise ValueError(\n-            f\"atomic_{op} does not support {element_ty}\")\n+        raise ValueError(\"atomic_\" + op + \" does not support \" + str(element_ty))\n     if ptr.type.is_block():\n         if mask:\n-            mask = broadcast_impl_shape(\n-                mask, ptr.type.get_block_shapes(), builder)\n+            mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n         if val:\n-            val = broadcast_impl_shape(\n-                val, ptr.type.get_block_shapes(), builder)\n+            val = broadcast_impl_shape(val, ptr.type.get_block_shapes(), builder)\n     val = cast(val, ptr.type.scalar.element_ty, builder)\n     if not mask:\n         mask_ir = builder.get_int1(True)\n         mask_ty = tl.int1\n         if ptr.type.is_block():\n-            mask_ir = builder.create_splat(\n-                mask_ir, ptr.type.get_block_shapes())\n+            mask_ir = builder.create_splat(mask_ir, ptr.type.get_block_shapes())\n             mask_ty = tl.block_type(tl.int1, ptr.type.get_block_shapes())\n         mask = tl.tensor(mask_ir, mask_ty)\n     return ptr, val, mask\n@@ -1059,26 +957,8 @@ def atomic_max(ptr: tl.tensor,\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n     pos = greater_equal(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n     neg = less_than(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n-    pos_ret = tl.tensor(\n-        builder.create_atomic_rmw(\n-            ir.ATOMIC_OP.MAX,\n-            i_ptr.handle,\n-            i_val.handle,\n-            and_(\n-                mask,\n-                pos,\n-                builder).handle),\n-        i_val.type)\n-    neg_ret = tl.tensor(\n-        builder.create_atomic_rmw(\n-            ir.ATOMIC_OP.UMIN,\n-            i_ptr.handle,\n-            i_val.handle,\n-            and_(\n-                mask,\n-                neg,\n-                builder).handle),\n-        i_val.type)\n+    pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX, i_ptr.handle, i_val.handle, and_(mask, pos, builder).handle), i_val.type)\n+    neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN, i_ptr.handle, i_val.handle, and_(mask, neg, builder).handle), i_val.type)\n     return where(pos, pos_ret, neg_ret, builder)\n \n \n@@ -1129,45 +1009,39 @@ def atomic_add(ptr: tl.tensor,\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'add', builder)\n     sca_ty = val.type.scalar\n     op = ir.ATOMIC_OP.FADD if sca_ty.is_floating() else ir.ATOMIC_OP.ADD\n-    return tl.tensor(builder.create_atomic_rmw(\n-        op, ptr.handle, val.handle, mask.handle), val.type)\n+    return tl.tensor(builder.create_atomic_rmw(op, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_and(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'and', builder)\n-    return tl.tensor(builder.create_atomic_rmw(\n-        ir.ATOMIC_OP.AND, ptr.handle, val.handle, mask.handle), val.type)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.AND, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_or(ptr: tl.tensor,\n               val: tl.tensor,\n               mask: tl.tensor,\n               builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'or', builder)\n-    return tl.tensor(builder.create_atomic_rmw(\n-        ir.ATOMIC_OP.OR, ptr.handle, val.handle, mask.handle), val.type)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.OR, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_xor(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'xor', builder)\n-    return tl.tensor(builder.create_atomic_rmw(\n-        ir.ATOMIC_OP.XOR, ptr.handle, val.handle, mask.handle), val.type)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XOR, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_xchg(ptr: tl.tensor,\n                 val: tl.tensor,\n                 mask: tl.tensor,\n                 builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(\n-        ptr, val, mask, 'xchg', builder)\n-    return tl.tensor(builder.create_atomic_rmw(\n-        ir.ATOMIC_OP.XCHG, ptr.handle, val.handle, mask.handle), val.type)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'xchg', builder)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XCHG, ptr.handle, val.handle, mask.handle), val.type)\n \n # ===----------------------------------------------------------------------===//\n #                               Linear Algebra\n@@ -1193,8 +1067,7 @@ def dot(lhs: tl.tensor,\n         _0 = builder.get_fp32(0)\n         ret_scalar_ty = tl.float32\n     else:\n-        _0 = builder.get_fp16(\n-            0) if out_dtype.is_fp16() else builder.get_fp32(0)\n+        _0 = builder.get_fp16(0) if out_dtype.is_fp16() else builder.get_fp32(0)\n         ret_scalar_ty = out_dtype\n \n     M = lhs.type.shape[0]\n@@ -1223,8 +1096,7 @@ def where(condition: tl.tensor,\n     if not condition.type.is_block():\n         condition, _ = broadcast_impl_value(condition, x, builder)\n     ret_ty = x.type\n-    return tl.tensor(builder.create_select(\n-        condition.handle, x.handle, y.handle), ret_ty)\n+    return tl.tensor(builder.create_select(condition.handle, x.handle, y.handle), ret_ty)\n \n # ===----------------------------------------------------------------------===//\n #                               Reductions\n@@ -1281,45 +1153,37 @@ def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n         res_ty = out_scalar_ty\n \n     if scalar_ty.is_floating():\n-        return tl.tensor(builder.create_reduce(\n-            input.handle, FLOAT_OP, axis), res_ty)\n+        return tl.tensor(builder.create_reduce(input.handle, FLOAT_OP, axis), res_ty)\n     elif scalar_ty.is_int():\n-        return tl.tensor(builder.create_reduce(\n-            input.handle, INT_OP, axis), res_ty)\n+        return tl.tensor(builder.create_reduce(input.handle, INT_OP, axis), res_ty)\n     assert False\n \n \n def min(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n-    return reduce_impl(input, axis, builder, \"min\",\n-                       ir.REDUCE_OP.FMIN, ir.REDUCE_OP.MIN)\n+    return reduce_impl(input, axis, builder, \"min\", ir.REDUCE_OP.FMIN, ir.REDUCE_OP.MIN)\n \n \n def argmin(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n-    return reduce_impl(input, axis, builder, \"argmin\",\n-                       ir.REDUCE_OP.ARGFMIN, ir.REDUCE_OP.ARGMIN)\n+    return reduce_impl(input, axis, builder, \"argmin\", ir.REDUCE_OP.ARGFMIN, ir.REDUCE_OP.ARGMIN)\n \n \n def max(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n-    return reduce_impl(input, axis, builder, \"max\",\n-                       ir.REDUCE_OP.FMAX, ir.REDUCE_OP.MAX)\n+    return reduce_impl(input, axis, builder, \"max\", ir.REDUCE_OP.FMAX, ir.REDUCE_OP.MAX)\n \n \n def argmax(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n-    return reduce_impl(input, axis, builder, \"argmax\",\n-                       ir.REDUCE_OP.ARGFMAX, ir.REDUCE_OP.ARGMAX)\n+    return reduce_impl(input, axis, builder, \"argmax\", ir.REDUCE_OP.ARGFMAX, ir.REDUCE_OP.ARGMAX)\n \n \n def sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n-    return reduce_impl(input, axis, builder, \"sum\",\n-                       ir.REDUCE_OP.FADD, ir.REDUCE_OP.ADD)\n+    return reduce_impl(input, axis, builder, \"sum\", ir.REDUCE_OP.FADD, ir.REDUCE_OP.ADD)\n \n \n def xor_sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     scalar_ty = input.type.scalar\n     if not scalar_ty.is_int():\n         raise ValueError(\"xor_sum only supported for integers\")\n-    return reduce_impl(input, axis, builder, \"sum\",\n-                       ir.REDUCE_OP.XOR, ir.REDUCE_OP.XOR)\n+    return reduce_impl(input, axis, builder, \"sum\", ir.REDUCE_OP.XOR, ir.REDUCE_OP.XOR)\n \n \n # ===----------------------------------------------------------------------===\n@@ -1376,41 +1240,28 @@ def abs(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n \n def multiple_of(x: tl.tensor, values: List[int]) -> tl.tensor:\n     if len(x.shape) != len(values):\n-        raise ValueError(\n-            \"Shape of input to multiple_of does not match the length of values\")\n-    x.handle.set_attr(\n-        \"tt.divisibility\",\n-        ir.make_attr(\n-            values,\n-            x.handle.get_context()))\n+        raise ValueError(\"Shape of input to multiple_of does not match the length of values\")\n+    x.handle.set_attr(\"tt.divisibility\", ir.make_attr(values, x.handle.get_context()))\n     return x\n \n \n def max_contiguous(x: tl.tensor, values: List[int]) -> tl.tensor:\n     if len(x.shape) != len(values):\n-        raise ValueError(\n-            \"Shape of input to max_contiguous does not match the length of values\")\n-    x.handle.set_attr(\n-        \"tt.contiguity\",\n-        ir.make_attr(\n-            values,\n-            x.handle.get_context()))\n+        raise ValueError(\"Shape of input to max_contiguous does not match the length of values\")\n+    x.handle.set_attr(\"tt.contiguity\", ir.make_attr(values, x.handle.get_context()))\n     return x\n \n \n def debug_barrier(builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_barrier(), tl.void)\n \n \n-def device_print(\n-        prefix: str, args: List[tl.tensor], builder: ir.builder) -> tl.tensor:\n+def device_print(prefix: str, args: List[tl.tensor], builder: ir.builder) -> tl.tensor:\n     new_args = []\n     for arg in args:\n         new_args.append(arg.handle)\n     return tl.tensor(builder.create_print(prefix, new_args), tl.void)\n \n \n-def device_assert(cond: tl.tensor, msg: str, file_name: str,\n-                  func_name, lineno: int, builder: ir.builder) -> tl.tensor:\n-    return tl.tensor(builder.create_assert(cond.handle, msg,\n-                     file_name, func_name, lineno), tl.void)\n+def device_assert(cond: tl.tensor, msg: str, file_name: str, func_name, lineno: int, builder: ir.builder) -> tl.tensor:\n+    return tl.tensor(builder.create_assert(cond.handle, msg, file_name, func_name, lineno), tl.void)"}, {"filename": "python/triton/ops/blocksparse/softmax.py", "status": "modified", "additions": 7, "deletions": 18, "changes": 25, "file_content_changes": "@@ -44,12 +44,8 @@ def _blocksparse_softmax_fwd(\n     if IS_DENSE:\n         ns = tl.arange(0, ROW_SIZE)\n     else:\n-        off_lut = offset + 2 * \\\n-            tl.num_programs(0) * tl.num_programs(1) // BLOCK_SIZE\n-        start_n = tl.load(\n-            LUT + off_lut + block_n,\n-            mask=block_n < size,\n-            other=0)\n+        off_lut = offset + 2 * tl.num_programs(0) * tl.num_programs(1) // BLOCK_SIZE\n+        start_n = tl.load(LUT + off_lut + block_n, mask=block_n < size, other=0)\n         ns = start_n * BLOCK_SIZE + lane_n\n     # load X\n     mask = block_n < size\n@@ -110,8 +106,7 @@ def _blocksparse_softmax_bwd(\n     if IS_DENSE:\n         ns = tl.arange(0, ROW_SIZE)\n     else:\n-        off_lut = offset + 2 * \\\n-            tl.num_programs(0) * tl.num_programs(1) // BLOCK_SIZE\n+        off_lut = offset + 2 * tl.num_programs(0) * tl.num_programs(1) // BLOCK_SIZE\n         start_n = tl.load(LUT + off_lut + block_n, mask=mask, other=0)\n         ns = start_n * BLOCK_SIZE + lane_n\n     # load data\n@@ -165,14 +160,12 @@ def forward(\n         M = a.shape[0]\n         grid = [spdims[0], spdims[1] * block, M]\n         rel_shape = (1, 1, 1, 1) if rel_logits is None else rel_logits.shape\n-        rel_strides = (\n-            1, 1, 1, 1) if rel_logits is None else rel_logits.stride()\n+        rel_strides = (1, 1, 1, 1) if rel_logits is None else rel_logits.stride()\n         # enqueue kernel\n         out = torch.empty_like(a)\n         _blocksparse_softmax_fwd[grid](\n             out, a, a.stride(0), lut,\n-            # relative attn\n-            rel_logits, rel_shape[-1], rel_strides[0], rel_strides[1],\n+            rel_logits, rel_shape[-1], rel_strides[0], rel_strides[1],  # relative attn\n             scale,\n             is_causal,\n             BLOCK_SIZE=block,\n@@ -201,10 +194,7 @@ def backward(ctx, dout):\n         # relative logits gradients\n         dr = None\n         if ctx.needs_input_grad[3]:\n-            dr = torch.zeros(\n-                ctx.rel_shape,\n-                dtype=ctx.rel_dtype,\n-                device=out.device)\n+            dr = torch.zeros(ctx.rel_shape, dtype=ctx.rel_dtype, device=out.device)\n         # run kernel\n         M = out.shape[0]\n         grid = (ctx.spdims[0], ctx.spdims[1] * ctx.block, M)\n@@ -236,8 +226,7 @@ def __init__(self, layout, block, device, is_dense=False):\n         self.spdims = layout.shape\n         self.layout = layout\n         self.block = block\n-        self.lut, self.maxlut = _softmax.make_lut(\n-            self.layout, self.block, device)\n+        self.lut, self.maxlut = _softmax.make_lut(self.layout, self.block, device)\n         self.is_dense = is_dense\n \n     def __call__(self, a, *, scale=1.0, rel_logits=None, is_causal=False):"}, {"filename": "python/triton/ops/cross_entropy.py", "status": "modified", "additions": 3, "deletions": 7, "changes": 10, "file_content_changes": "@@ -72,16 +72,14 @@ class _cross_entropy(torch.autograd.Function):\n     @classmethod\n     def forward(cls, ctx, logits, indices):\n         # make sure we can use triton\n-        assert indices.dtype == torch.int64, \"Indices are expected to be of type long.\"\n+        assert (indices.dtype == torch.int64), \"Indices are expected to be of type long.\"\n         # make kernel\n         device, dtype = logits.device, logits.dtype\n         n_cols = logits.shape[-1]\n         # run the kernel\n         result = torch.empty_like(indices, dtype=dtype, device=device)\n         neg_logprobs = torch.empty_like(logits, dtype=dtype, device=device)\n-\n-        def grid(opt):\n-            return (logits.numel() // n_cols, )\n+        grid = lambda opt: (logits.numel() // n_cols, )\n         _forward[grid](logits, neg_logprobs, indices, result, n_cols)\n         # save for backward\n         ctx.save_for_backward(neg_logprobs, indices)\n@@ -99,9 +97,7 @@ def backward(cls, ctx, dneg_logprobs):\n         # run the kernel\n         # neg_logprobs will be modified in place to become our gradient:\n         n_cols = neg_logprobs.shape[-1]\n-\n-        def grid(opt):\n-            return (neg_logprobs.numel() // n_cols, )\n+        grid = lambda opt: (neg_logprobs.numel() // n_cols, )\n         _backward[grid](neg_logprobs, indices, dneg_logprobs, n_cols)\n         return neg_logprobs, None\n "}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 20, "deletions": 114, "changes": 134, "file_content_changes": "@@ -29,107 +29,25 @@ def get_configs_io_bound():\n @triton.autotune(\n     configs=[\n         # basic configs for compute-bound matmuls\n-        triton.Config({'BLOCK_M': 128,\n-                       'BLOCK_N': 256,\n-                       'BLOCK_K': 32,\n-                       'SPLIT_K': 1},\n-                      num_stages=3,\n-                      num_warps=8),\n-        triton.Config({'BLOCK_M': 256,\n-                       'BLOCK_N': 128,\n-                       'BLOCK_K': 32,\n-                       'SPLIT_K': 1},\n-                      num_stages=3,\n-                      num_warps=8),\n-        triton.Config({'BLOCK_M': 256,\n-                       'BLOCK_N': 64,\n-                       'BLOCK_K': 32,\n-                       'SPLIT_K': 1},\n-                      num_stages=4,\n-                      num_warps=4),\n-        triton.Config({'BLOCK_M': 64,\n-                       'BLOCK_N': 256,\n-                       'BLOCK_K': 32,\n-                       'SPLIT_K': 1},\n-                      num_stages=4,\n-                      num_warps=4),\n-        triton.Config({'BLOCK_M': 128,\n-                       'BLOCK_N': 128,\n-                       'BLOCK_K': 32,\n-                       'SPLIT_K': 1},\n-                      num_stages=4,\n-                      num_warps=4),\n-        triton.Config({'BLOCK_M': 128,\n-                       'BLOCK_N': 64,\n-                       'BLOCK_K': 32,\n-                       'SPLIT_K': 1},\n-                      num_stages=4,\n-                      num_warps=4),\n-        triton.Config({'BLOCK_M': 64,\n-                       'BLOCK_N': 128,\n-                       'BLOCK_K': 32,\n-                       'SPLIT_K': 1},\n-                      num_stages=4,\n-                      num_warps=4),\n-        triton.Config({'BLOCK_M': 128,\n-                       'BLOCK_N': 32,\n-                       'BLOCK_K': 32,\n-                       'SPLIT_K': 1},\n-                      num_stages=4,\n-                      num_warps=4),\n-        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32,\n-                      'SPLIT_K': 1}, num_stages=5, num_warps=2),\n+        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n+        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n+        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2),\n         # good for int8\n-        triton.Config({'BLOCK_M': 128,\n-                       'BLOCK_N': 256,\n-                       'BLOCK_K': 128,\n-                       'SPLIT_K': 1},\n-                      num_stages=3,\n-                      num_warps=8),\n-        triton.Config({'BLOCK_M': 256,\n-                       'BLOCK_N': 128,\n-                       'BLOCK_K': 128,\n-                       'SPLIT_K': 1},\n-                      num_stages=3,\n-                      num_warps=8),\n-        triton.Config({'BLOCK_M': 256,\n-                       'BLOCK_N': 64,\n-                       'BLOCK_K': 128,\n-                       'SPLIT_K': 1},\n-                      num_stages=4,\n-                      num_warps=4),\n-        triton.Config({'BLOCK_M': 64,\n-                       'BLOCK_N': 256,\n-                       'BLOCK_K': 128,\n-                       'SPLIT_K': 1},\n-                      num_stages=4,\n-                      num_warps=4),\n-        triton.Config({'BLOCK_M': 128,\n-                       'BLOCK_N': 128,\n-                       'BLOCK_K': 128,\n-                       'SPLIT_K': 1},\n-                      num_stages=4,\n-                      num_warps=4),\n-        triton.Config({'BLOCK_M': 128,\n-                       'BLOCK_N': 64,\n-                       'BLOCK_K': 64,\n-                       'SPLIT_K': 1},\n-                      num_stages=4,\n-                      num_warps=4),\n-        triton.Config({'BLOCK_M': 64,\n-                       'BLOCK_N': 128,\n-                       'BLOCK_K': 64,\n-                       'SPLIT_K': 1},\n-                      num_stages=4,\n-                      num_warps=4),\n-        triton.Config({'BLOCK_M': 128,\n-                       'BLOCK_N': 32,\n-                       'BLOCK_K': 64,\n-                       'SPLIT_K': 1},\n-                      num_stages=4,\n-                      num_warps=4),\n-        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64,\n-                      'SPLIT_K': 1}, num_stages=5, num_warps=2),\n+        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n+        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n+        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=5, num_warps=2),\n     ] + get_configs_io_bound(),\n     key=['M', 'N', 'K'],\n     prune_configs_by={\n@@ -220,27 +138,15 @@ def _call(a, b, dot_out_dtype):\n             else:\n                 dot_out_dtype = tl.int32\n         else:\n-            assert isinstance(\n-                dot_out_dtype, torch.dtype), \"dot_out_dtype must be a torch.dtype\"\n+            assert isinstance(dot_out_dtype, torch.dtype), \"dot_out_dtype must be a torch.dtype\"\n             if dot_out_dtype == torch.float16:\n                 dot_out_dtype = tl.float16\n             elif dot_out_dtype in [torch.float32, torch.bfloat16]:\n                 dot_out_dtype = tl.float32\n             else:\n                 dot_out_dtype = tl.int32\n         # launch kernel\n-\n-        def grid(META):\n-            return (\n-                triton.cdiv(\n-                    M,\n-                    META['BLOCK_M']\n-                ) * triton.cdiv(\n-                    N,\n-                    META['BLOCK_N']\n-                ),\n-                META['SPLIT_K']\n-            )\n+        grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']), META['SPLIT_K'])\n         _kernel[grid](a, b, c, M, N, K,\n                       a.stride(0), a.stride(1),\n                       b.stride(0), b.stride(1),"}, {"filename": "python/triton/tools/aot.py", "status": "modified", "additions": 19, "deletions": 49, "changes": 68, "file_content_changes": "@@ -16,27 +16,12 @@\n     parser.add_argument('src', help=\"Source file to compile\")\n     parser.add_argument('--target', required=True,\n                         help=\"Target format, one of: \" + ', '.join(VALID_FORMATS))\n-    parser.add_argument(\n-        '--sm',\n-        type=int,\n-        help=\"Compute capability to compile for\")\n-    parser.add_argument(\n-        '--ptx-version',\n-        type=int,\n-        help=\"PTX version to compile for\")\n+    parser.add_argument('--sm', type=int, help=\"Compute capability to compile for\")\n+    parser.add_argument('--ptx-version', type=int, help=\"PTX version to compile for\")\n     parser.add_argument('--gfx', type=str, help=\"AMDGPU target to compile for\")\n-    parser.add_argument(\n-        '--triple',\n-        type=str,\n-        help=\"target triple, for example: amdgcn-amd-amdhsa\")\n-    parser.add_argument(\n-        '--features',\n-        type=str,\n-        help=\"target features, for example: +sramecc,-xnack\")\n-    parser.add_argument(\n-        '--num_warps',\n-        type=int,\n-        help=\"number of warps to compile ttgir for\")\n+    parser.add_argument('--triple', type=str, help=\"target triple, for example: amdgcn-amd-amdhsa\")\n+    parser.add_argument('--features', type=str, help=\"target features, for example: +sramecc,-xnack\")\n+    parser.add_argument('--num_warps', type=int, help=\"number of warps to compile ttgir for\")\n \n     # parse the args\n     args = parser.parse_args()\n@@ -73,14 +58,11 @@\n         else:\n             arch_triple, arch_name, arch_features = arch_details\n \n-        # stop processing if architecture name is not automatically detected\n-        # and is not set manually\n+        # stop processing if architecture name is not automatically detected and is not set manually\n         if not args.gfx and not arch_name:\n-            raise argparse.ArgumentError(\n-                None, \"Must specify --gfx for AMDGCN compilation\")\n+            raise argparse.ArgumentError(None, \"Must specify --gfx for AMDGCN compilation\")\n \n-        # rewrite default and automatically detected values with manually\n-        # provided data\n+        # rewrite default and automatically detected values with manually provided data\n         if args.gfx:\n             arch_name = args.gfx\n         if args.triple:\n@@ -90,56 +72,44 @@\n \n         # triton-ir -> triton-gpu-ir\n         # use compute_capability == 80\n-        # num_stages=3, compute_capability=80)\n-        module = triton.compiler.ttir_to_ttgir(\n-            module, num_warps=args.num_warps)\n-        module = triton.compiler.optimize_ttgir(\n-            module, num_stages=3, compute_capability=80)\n+        module = triton.compiler.ttir_to_ttgir(module, num_warps=args.num_warps)  # num_stages=3, compute_capability=80)\n+        module = triton.compiler.optimize_ttgir(module, num_stages=3, compute_capability=80)\n         # triton-gpu-ir -> llvm-ir\n         # use compute_capability == 80\n-        module = triton.compiler.ttgir_to_llir(\n-            module, extern_libs=None, compute_capability=80)\n+        module = triton.compiler.ttgir_to_llir(module, extern_libs=None, compute_capability=80)\n         # llvm-ir -> amdgcn asm, hsaco binary\n-        module, hsaco_path = triton.compiler.llir_to_amdgcn_and_hsaco(\n-            module, arch_name, arch_triple, arch_features)\n+        module, hsaco_path = triton.compiler.llir_to_amdgcn_and_hsaco(module, arch_name, arch_triple, arch_features)\n \n         print(hsaco_path)\n         print(module)\n         sys.exit(0)\n \n     if not args.sm:\n-        raise argparse.ArgumentError(\n-            None, \"Must specify --sm for PTX compilation\")\n+        raise argparse.ArgumentError(None, \"Must specify --sm for PTX compilation\")\n \n     # triton-ir -> triton-gpu-ir\n     module = triton.compiler.ttir_to_ttgir(module, num_warps=args.num_warps)\n-    module = triton.compiler.optimize_ttgir(\n-        module, num_stages=3, compute_capability=args.sm)\n+    module = triton.compiler.optimize_ttgir(module, num_stages=3, compute_capability=args.sm)\n     if args.target == 'triton-gpu-ir':\n         print(module.str())\n         sys.exit(0)\n \n     # triton-gpu-ir -> llvm-ir\n-    module = triton.compiler.ttgir_to_llir(\n-        module, extern_libs=None, compute_capability=args.sm)\n+    module = triton.compiler.ttgir_to_llir(module, extern_libs=None, compute_capability=args.sm)\n     if args.target == 'llvm-ir':\n         print(module)\n         sys.exit(0)\n \n     # llvm-ir -> ptx\n     if args.target == 'ptx':\n         if not args.ptx_version:\n-            raise argparse.ArgumentError(\n-                None, \"Must specify --ptx-version for PTX compilation\")\n-        module = triton.compiler.llir_to_ptx(\n-            module, compute_capability=args.sm, ptx_version=args.ptx_version)\n+            raise argparse.ArgumentError(None, \"Must specify --ptx-version for PTX compilation\")\n+        module = triton.compiler.llir_to_ptx(module, compute_capability=args.sm, ptx_version=args.ptx_version)\n \n     # llvm-ir -> amdgcn\n     if args.target == 'amdgcn':\n         if not args.gfx:\n-            raise argparse.ArgumentError(\n-                None, \"Must specify --gfx for AMDGCN compilation\")\n-        module, hsaco_path = triton.compiler.llir_to_amdgcn_and_hsaco(\n-            module, args.gfx)\n+            raise argparse.ArgumentError(None, \"Must specify --gfx for AMDGCN compilation\")\n+        module, hsaco_path = triton.compiler.llir_to_amdgcn_and_hsaco(module, args.gfx)\n \n     print(module)"}, {"filename": "python/triton/tools/disasm.py", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -64,8 +64,7 @@ def extract(file_path, fun):\n     if fun is None:\n         sass_str = subprocess.check_output([\"cuobjdump\", \"-sass\", file_path])\n     else:\n-        sass_str = subprocess.check_output(\n-            [\"cuobjdump\", \"-fun\", fun, \"-sass\", file_path])\n+        sass_str = subprocess.check_output([\"cuobjdump\", \"-fun\", fun, \"-sass\", file_path])\n     sass_lines = sass_str.splitlines()\n     line_idx = 0\n     while line_idx < len(sass_lines):"}, {"filename": "python/tutorials/02-fused-softmax.py", "status": "modified", "additions": 13, "deletions": 27, "changes": 40, "file_content_changes": "@@ -16,8 +16,7 @@\n # -----------\n #\n # Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.\n-# Let us consider instead the case of a simple (numerically stabilized)\n-# softmax operation:\n+# Let us consider instead the case of a simple (numerically stabilized) softmax operation:\n \n import torch\n \n@@ -75,20 +74,17 @@ def softmax_kernel(\n ):\n     # The rows of the softmax are independent, so we parallelize across those\n     row_idx = tl.program_id(0)\n-    # The stride represents how much we need to increase the pointer to\n-    # advance 1 row\n+    # The stride represents how much we need to increase the pointer to advance 1 row\n     row_start_ptr = input_ptr + row_idx * input_row_stride\n     # The block size is the next power of two greater than n_cols, so we can fit each\n     # row in a single block\n     col_offsets = tl.arange(0, BLOCK_SIZE)\n     input_ptrs = row_start_ptr + col_offsets\n-    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than\n-    # n_cols\n+    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n     row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n     # Subtract maximum for numerical stability\n     row_minus_max = row - tl.max(row, axis=0)\n-    # Note that exponentiation in Triton is fast but approximate (i.e., think\n-    # __expf in CUDA)\n+    # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n     numerator = tl.exp(row_minus_max)\n     denominator = tl.sum(numerator, axis=0)\n     softmax_output = numerator / denominator\n@@ -99,14 +95,12 @@ def softmax_kernel(\n \n \n # %%\n-# We can create a helper function that enqueues the kernel and its\n-# (meta-)arguments for any given input tensor.\n+# We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.\n \n \n def softmax(x):\n     n_rows, n_cols = x.shape\n-    # The block size is the smallest power of two greater than the number of\n-    # columns in `x`\n+    # The block size is the smallest power of two greater than the number of columns in `x`\n     BLOCK_SIZE = triton.next_power_of_2(n_cols)\n     # Another trick we can use is to ask the compiler to use more threads per row by\n     # increasing the number of warps (`num_warps`) over which each row is distributed.\n@@ -155,8 +149,7 @@ def softmax(x):\n # ---------\n #\n # Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.\n-# We will then compare its performance against (1) :code:`torch.softmax`\n-# and (2) the :code:`naive_softmax` defined above.\n+# We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.\n \n \n @triton.testing.perf_report(\n@@ -165,8 +158,7 @@ def softmax(x):\n         x_vals=[\n             128 * i for i in range(2, 100)\n         ],  # different possible values for `x_name`\n-        line_arg='provider',\n-        # argument name whose value corresponds to a different line in the plot\n+        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n         line_vals=[\n             'triton',\n             'torch-native',\n@@ -179,24 +171,19 @@ def softmax(x):\n         ],  # label name for the lines\n         styles=[('blue', '-'), ('green', '-'), ('green', '--')],  # line styles\n         ylabel=\"GB/s\",  # label name for the y-axis\n-        plot_name=\"softmax-performance\",\n-        # name for the plot. Used also as a file name for saving the plot.\n-        args={'M': 4096},\n-        # values for function arguments not in `x_names` and `y_name`\n+        plot_name=\"softmax-performance\",  # name for the plot. Used also as a file name for saving the plot.\n+        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n     )\n )\n def benchmark(M, N, provider):\n     x = torch.randn(M, N, device='cuda', dtype=torch.float32)\n     if provider == 'torch-native':\n-        ms, min_ms, max_ms = triton.testing.do_bench(\n-            lambda: torch.softmax(x, axis=-1))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n     if provider == 'triton':\n         ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x))\n     if provider == 'torch-jit':\n         ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x))\n-\n-    def gbps(ms):\n-        return 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n+    gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n \n \n@@ -206,5 +193,4 @@ def gbps(ms):\n # In the above plot, we can see that:\n #  - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.\n #  - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.\n-# Note however that the PyTorch `softmax` operation is more general and\n-# will work on tensors of any shape.\n+#    Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape."}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 17, "deletions": 39, "changes": 56, "file_content_changes": "@@ -39,8 +39,7 @@\n #          acc += dot(a, b)\n #        C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc;\n #\n-# where each iteration of the doubly-nested for-loop is performed by a\n-# dedicated Triton program instance.\n+# where each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance.\n \n # %%\n # Compute Kernel\n@@ -158,12 +157,7 @@\n \n @triton.autotune(\n     configs=[\n-        triton.Config({'BLOCK_SIZE_M': 128,\n-                       'BLOCK_SIZE_N': 256,\n-                       'BLOCK_SIZE_K': 64,\n-                       'GROUP_SIZE_M': 8},\n-                      num_stages=3,\n-                      num_warps=8),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n     ],\n     key=['M', 'N', 'K'],\n )\n@@ -243,23 +237,20 @@ def matmul_kernel(\n     # Write back the block of the output matrix C\n     offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n     offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-    c_ptrs = c_ptr + stride_cm * \\\n-        offs_cm[:, None] + stride_cn * offs_cn[None, :]\n+    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n     c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n     tl.store(c_ptrs, c, mask=c_mask)\n \n \n-# we can fuse `leaky_relu` by providing it as an `ACTIVATION`\n-# meta-parameter in `_matmul`\n+# we can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`\n @triton.jit\n def leaky_relu(x):\n     return tl.where(x >= 0, x, 0.01 * x)\n \n \n # %%\n # We can now create a convenience wrapper function that only takes two input tensors\n-# and (1) checks any shape constraint; (2) allocates the output; (3)\n-# launches the above kernel\n+# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel\n \n \n def matmul(a, b, activation=None):\n@@ -275,11 +266,9 @@ def matmul(a, b, activation=None):\n     # allocates output\n     c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n     # 1D launch kernel where each block gets its own program.\n-\n-    def grid(META):\n-        return (\n-            triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n-        )\n+    grid = lambda META: (\n+        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n+    )\n     matmul_kernel[grid](\n         a, b, c,\n         M, N, K,\n@@ -295,8 +284,7 @@ def grid(META):\n # Unit Test\n # ---------\n #\n-# We can test our custom matrix multiplication operation against a native\n-# torch implementation (i.e., cuBLAS)\n+# We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS)\n \n torch.manual_seed(0)\n a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n@@ -317,45 +305,35 @@ def grid(META):\n # Square Matrix Performance\n # ~~~~~~~~~~~~~~~~~~~~~~~~~~\n #\n-# We can now compare the performance of our kernel against that of cuBLAS.\n-# Here we focus on square matrices, but feel free to arrange this script\n-# as you wish to benchmark any other matrix shape.\n+# We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices, but feel free to arrange this script as you wish to benchmark any other matrix shape.\n \n \n @triton.testing.perf_report(\n     triton.testing.Benchmark(\n-        # argument names to use as an x-axis for the plot\n-        x_names=['M', 'N', 'K'],\n+        x_names=['M', 'N', 'K'],  # argument names to use as an x-axis for the plot\n         x_vals=[\n             8192\n         ],  # different possible values for `x_name`\n-        line_arg='provider',\n-        # argument name whose value corresponds to a different line in the plot\n+        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n         # possible values for `line_arg``\n         line_vals=['cublas', 'triton'],\n         # label name for the lines\n         line_names=[\"cuBLAS\", \"Triton\"],\n         # line styles\n-        styles=[('green', '-'), ('green', '--'),\n-                ('blue', '-'), ('blue', '--')],\n+        styles=[('green', '-'), ('green', '--'), ('blue', '-'), ('blue', '--')],\n         ylabel=\"TFLOPS\",  # label name for the y-axis\n-        plot_name=\"matmul-performance\",\n-        # name for the plot. Used also as a file name for saving the plot.\n+        plot_name=\"matmul-performance\",  # name for the plot. Used also as a file name for saving the plot.\n         args={},\n     )\n )\n def benchmark(M, N, K, provider):\n     a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n     b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n     if provider == 'cublas':\n-        ms, min_ms, max_ms = triton.testing.do_bench(\n-            lambda: torch.matmul(a, b), rep=100)\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), rep=100)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(\n-            lambda: matmul(a, b), rep=100)\n-\n-    def perf(ms):\n-        return 2 * M * N * K * 1e-12 / (ms * 1e-3)\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), rep=100)\n+    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n     return perf(ms), perf(max_ms), perf(min_ms)\n \n "}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 23, "deletions": 61, "changes": 84, "file_content_changes": "@@ -32,12 +32,9 @@ def _fwd_kernel(\n     offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n     offs_n = tl.arange(0, BLOCK_N)\n     offs_d = tl.arange(0, BLOCK_DMODEL)\n-    off_q = off_hz * stride_qh + \\\n-        offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + \\\n-        offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\n-    off_v = off_hz * stride_qh + \\\n-        offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\n+    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n     # Initialize pointers to Q, K, V\n     q_ptrs = Q + off_q\n     k_ptrs = K + off_k\n@@ -55,8 +52,7 @@ def _fwd_kernel(\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         qk += tl.dot(q, tl.trans(k))\n         qk *= sm_scale\n-        qk = tl.where(offs_m[:, None] >= (\n-            start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n         # compute new m\n         m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n         # correct old l\n@@ -88,8 +84,7 @@ def _fwd_kernel(\n     tl.store(m_ptrs, m_prev)\n     # initialize pointers to output\n     offs_n = tl.arange(0, BLOCK_DMODEL)\n-    off_o = off_hz * stride_oh + \\\n-        offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n+    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n     out_ptrs = Out + off_o\n     tl.store(out_ptrs, acc)\n \n@@ -169,8 +164,7 @@ def _bwd_kernel(\n             # recompute p = softmax(qk, dim=-1).T\n             # NOTE: `do` is pre-divided by `l`; no normalization here\n             qk = tl.dot(q, tl.trans(k))\n-            qk = tl.where(offs_m_curr[:, None] >= (\n-                offs_n[None, :]), qk, float(\"-inf\"))\n+            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n             m = tl.load(m_ptrs + offs_m_curr)\n             p = tl.exp(qk * sm_scale - m[:, None])\n             # compute dv\n@@ -213,14 +207,8 @@ def forward(ctx, q, k, v, sm_scale):\n         assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n         grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n-        L = torch.empty(\n-            (q.shape[0] * q.shape[1], q.shape[2]),\n-            device=q.device,\n-            dtype=torch.float32)\n-        m = torch.empty(\n-            (q.shape[0] * q.shape[1], q.shape[2]),\n-            device=q.device,\n-            dtype=torch.float32)\n+        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         num_warps = 4 if Lk <= 64 else 8\n \n         _fwd_kernel[grid](\n@@ -284,15 +272,9 @@ def backward(ctx, do):\n @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     torch.manual_seed(20)\n-    q = torch.empty(\n-        (Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(\n-        mean=0.1, std=0.2).requires_grad_()\n-    k = torch.empty(\n-        (Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(\n-        mean=0.4, std=0.2).requires_grad_()\n-    v = torch.empty(\n-        (Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(\n-        mean=0.3, std=0.2).requires_grad_()\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n     sm_scale = 0.2\n     dout = torch.randn_like(q)\n     # reference implementation\n@@ -340,58 +322,38 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     styles=[('red', '-'), ('blue', '-')],\n     ylabel='ms',\n     plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n-    args={\n-        'H': N_HEADS,\n-        'BATCH': BATCH,\n-        'D_HEAD': D_HEAD,\n-        'dtype': torch.float16,\n-        'mode': mode}\n+    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\n ) for mode in ['fwd', 'bwd']]\n \n \n @triton.testing.perf_report(configs)\n-def bench_flash_attention(BATCH, H, N_CTX, D_HEAD,\n-                          mode, provider, dtype=torch.float16, device=\"cuda\"):\n+def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.float16, device=\"cuda\"):\n     assert mode in ['fwd', 'bwd']\n     warmup = 25\n     rep = 100\n     if provider == \"triton\":\n-        q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype,\n-                        device=\"cuda\", requires_grad=True)\n-        k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype,\n-                        device=\"cuda\", requires_grad=True)\n-        v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype,\n-                        device=\"cuda\", requires_grad=True)\n+        q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n         sm_scale = 1.3\n-\n-        def fn():\n-            return attention(q, k, v, sm_scale)\n+        fn = lambda: attention(q, k, v, sm_scale)\n         if mode == 'bwd':\n             o = fn()\n             do = torch.randn_like(o)\n-\n-            def fn():\n-                return o.backward(do, retain_graph=True)\n-        ms = triton.testing.do_bench(\n-            fn, percentiles=None, warmup=warmup, rep=rep)\n+            fn = lambda: o.backward(do, retain_graph=True)\n+        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n         return ms\n     if provider == \"flash\":\n         lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n         cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n         cu_seqlens[1:] = lengths.cumsum(0)\n-        qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD),\n-                          dtype=dtype, device=device, requires_grad=True)\n-\n-        def fn():\n-            return flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\n+        qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n+        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\n         if mode == 'bwd':\n             o = fn()\n             do = torch.randn_like(o)\n-\n-            def fn():\n-                return o.backward(do, retain_graph=True)\n-        ms = triton.testing.do_bench(\n-            fn, percentiles=None, warmup=warmup, rep=rep)\n+            fn = lambda: o.backward(do, retain_graph=True)\n+        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n         return ms\n \n "}]