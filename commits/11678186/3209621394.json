[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -20,6 +20,18 @@ template <typename Int> Int product(llvm::ArrayRef<Int> arr) {\n \n template <typename Int> Int ceil(Int m, Int n) { return (m + n - 1) / n; }\n \n+// output[i] = input[order[i]]\n+template <typename T>\n+SmallVector<T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n+  size_t rank = order.size();\n+  assert(input.size() == rank);\n+  SmallVector<T> result(rank);\n+  for (auto it : llvm::enumerate(order)) {\n+    result[it.index()] = input[it.value()];\n+  }\n+  return result;\n+}\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_UTILITY_H"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h", "status": "modified", "additions": 74, "deletions": 7, "changes": 81, "file_content_changes": "@@ -2,12 +2,16 @@\n #define TRITON_CONVERSION_TRITON_GPU_TO_LLVM_ASM_FORMAT_H_\n \n #include \"mlir/IR/Value.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/ADT/StringRef.h\"\n #include <memory>\n #include <string>\n \n namespace mlir {\n+class ConversionPatternRewriter;\n+class Location;\n+\n namespace triton {\n using llvm::StringRef;\n \n@@ -96,14 +100,40 @@ struct PTXBuilder {\n     std::string dump() const;\n   };\n \n-  template <typename INSTR = PTXInstr> INSTR *create(const std::string &name) {\n-    instrs.emplace_back(std::make_unique<INSTR>(this, name));\n+  template <typename INSTR = PTXInstr, typename... Args>\n+  INSTR *create(Args &&...args) {\n+    instrs.emplace_back(std::make_unique<INSTR>(this, args...));\n     return static_cast<INSTR *>(instrs.back().get());\n   }\n \n   // Create a list of operands.\n   Operand *newListOperand() { return newOperand(); }\n \n+  Operand *newListOperand(ArrayRef<std::pair<mlir::Value, std::string>> items) {\n+    auto *list = newOperand();\n+    for (auto &item : items) {\n+      list->listAppend(newOperand(item.first, item.second));\n+    }\n+    return list;\n+  }\n+\n+  Operand *newListOperand(unsigned count, mlir::Value val,\n+                          const std::string &constraint) {\n+    auto *list = newOperand();\n+    for (int i = 0; i < count; ++i) {\n+      list->listAppend(newOperand(val, constraint));\n+    }\n+    return list;\n+  }\n+\n+  Operand *newListOperand(unsigned count, const std::string &constraint) {\n+    auto *list = newOperand();\n+    for (int i = 0; i < count; ++i) {\n+      list->listAppend(newOperand(constraint));\n+    }\n+    return list;\n+  }\n+\n   // Create a new operand. It will not add to operand list.\n   // @value: the MLIR value bind to this operand.\n   // @constraint: ASM operand constraint, .e.g. \"=r\"\n@@ -131,6 +161,11 @@ struct PTXBuilder {\n \n   std::string dump() const;\n \n+  mlir::Value launch(ConversionPatternRewriter &rewriter, Location loc,\n+                     Type resTy, bool hasSideEffect = true,\n+                     bool isAlignStack = false,\n+                     ArrayRef<Attribute> attrs = {}) const;\n+\n private:\n   Operand *newOperand() {\n     argArchive.emplace_back(std::make_unique<Operand>());\n@@ -155,6 +190,7 @@ struct PTXInstrCommon {\n   using Operand = PTXBuilder::Operand;\n \n   // clang-format off\n+  PTXInstrExecution& operator()() { return call({}); }\n   PTXInstrExecution& operator()(Operand* a) { return call({a}); }\n   PTXInstrExecution& operator()(Operand* a, Operand* b) { return call({a, b}); }\n   PTXInstrExecution& operator()(Operand* a, Operand* b, Operand* c) { return call({a, b, c}); }\n@@ -205,30 +241,61 @@ struct PTXInstr : public PTXInstrBase<PTXInstr> {\n // PtxIOInstr store(\"st\");\n // store.predicate(pValue).global().v(32).b(1); // @%0 st.global.v32.b1\n // store.addAddr(addrValue, \"l\", off);\n-struct PtxIOInstr : public PTXInstrBase<PtxIOInstr> {\n-  using PTXInstrBase<PtxIOInstr>::PTXInstrBase;\n+struct PTXIOInstr : public PTXInstrBase<PTXIOInstr> {\n+  using PTXInstrBase<PTXIOInstr>::PTXInstrBase;\n \n   // Add \".global\" suffix to instruction\n-  PtxIOInstr &global(bool predicate = true) {\n+  PTXIOInstr &global(bool predicate = true) {\n     o(\"global\", predicate);\n     return *this;\n   }\n \n   // Add \".v\" suffix to instruction\n-  PtxIOInstr &v(int vecWidth, bool predicate = true) {\n+  PTXIOInstr &v(int vecWidth, bool predicate = true) {\n     if (vecWidth > 1) {\n       o(\"v\" + std::to_string(vecWidth), predicate);\n     }\n     return *this;\n   }\n \n   // Add \".b\" suffix to instruction\n-  PtxIOInstr &b(int width) {\n+  PTXIOInstr &b(int width) {\n     o(\"b\" + std::to_string(width));\n     return *this;\n   }\n };\n \n+struct PTXCpAsyncInstrBase : public PTXInstrBase<PTXCpAsyncInstrBase> {\n+  explicit PTXCpAsyncInstrBase(PTXBuilder *builder)\n+      : PTXInstrBase(builder, \"cp.async\") {}\n+};\n+\n+struct PTXCpAsyncCommitGroupInstr : public PTXCpAsyncInstrBase {\n+  explicit PTXCpAsyncCommitGroupInstr(PTXBuilder *builder)\n+      : PTXCpAsyncInstrBase(builder) {\n+    o(\"commit_group\");\n+  }\n+};\n+\n+struct PTXCpAsyncWaitGroupInstr : public PTXCpAsyncInstrBase {\n+  explicit PTXCpAsyncWaitGroupInstr(PTXBuilder *builder)\n+      : PTXCpAsyncInstrBase(builder) {\n+    o(\"wait_group\");\n+  }\n+};\n+\n+struct PTXCpAsyncLoadInstr : public PTXCpAsyncInstrBase {\n+  explicit PTXCpAsyncLoadInstr(PTXBuilder *builder,\n+                               triton::CacheModifier modifier,\n+                               triton::EvictionPolicy policy)\n+      : PTXCpAsyncInstrBase(builder) {\n+    o(triton::stringifyCacheModifier(modifier).str());\n+    o(\"shared\");\n+    o(\"global\");\n+    o(\"L2::\" + triton::stringifyEvictionPolicy(policy).str());\n+  }\n+};\n+\n // Record the operands and context for \"launching\" a PtxInstr.\n struct PTXInstrExecution {\n   using Operand = PTXBuilder::Operand;"}, {"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -16,7 +16,7 @@ def TT_CacheModifierAttr : I32EnumAttr<\n def TT_EvictionPolicyAttr : I32EnumAttr<\n     \"EvictionPolicy\", \"\",\n     [\n-        I32EnumAttrCase<\"NORMAL\", 1, \"normal\">,\n+        I32EnumAttrCase<\"NORMAL\", 1, \"evict_normal\">,\n         I32EnumAttrCase<\"EVICT_FIRST\", 2, \"evict_first\">,\n         I32EnumAttrCase<\"EVICT_LAST\", 3, \"evict_last\">\n     ]> {"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 7, "deletions": 1, "changes": 8, "file_content_changes": "@@ -22,7 +22,13 @@ namespace gpu {\n \n unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape);\n \n-unsigned getShapePerCTA(const Attribute &layout, unsigned d);\n+SmallVector<unsigned> getSizePerThread(Attribute layout);\n+\n+SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout);\n+\n+SmallVector<unsigned> getShapePerCTA(const Attribute &layout);\n+\n+SmallVector<unsigned> getOrder(const Attribute &layout);\n \n } // namespace gpu\n } // namespace triton"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -54,7 +54,7 @@ in memory. For example, a swizzled row-major layout could store its data\n as follows:\n \n A_{0, 0}  A_{0, 1}  A_{0, 2}  A_{0, 3} ...   [phase 0] \\ per_phase = 2\n-A_{1, 0}  A_{0, 1}  A_{1, 2}  A_{1, 3} ...   [phase 0] /\n+A_{1, 0}  A_{1, 1}  A_{1, 2}  A_{1, 3} ...   [phase 0] /\n groups of vec=2 elements\n are stored contiguously\n _ _ _ _ /\\_ _ _ _\n@@ -169,6 +169,7 @@ for\n         int dim = order[_dim];\n         int maxNumThreads = int(shape[dim]) / sizePerThread[dim];\n         warpsPerCTA[dim] = std::clamp(remainingWarps, 1, maxNumThreads);\n+        maxNumThreads = maxNumThreads / warpsPerCTA[dim];\n         threadsPerWarp[dim] = std::clamp(remainingLanes, 1, maxNumThreads);\n         remainingWarps /= warpsPerCTA[dim];\n         remainingLanes /= threadsPerWarp[dim];"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 37, "deletions": 32, "changes": 69, "file_content_changes": "@@ -11,6 +11,7 @@\n #include <numeric>\n \n using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n@@ -32,39 +33,43 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n          \"Unexpect layout in getScratchConfigForCvtLayout()\");\n   unsigned rank = dstTy.getRank();\n   SmallVector<unsigned> paddedRepShape(rank);\n-  // TODO: move to TritonGPUAttrDefs.h.inc\n-  auto getShapePerCTA = [&](const Attribute &layout, unsigned d) -> unsigned {\n-    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-      return blockedLayout.getSizePerThread()[d] *\n-             blockedLayout.getThreadsPerWarp()[d] *\n-             blockedLayout.getWarpsPerCTA()[d];\n-    } else {\n-      assert(0 && \"Unimplemented usage of getShapePerCTA\");\n-      return 0;\n-    }\n-  };\n-  if (srcLayout.isa<BlockedEncodingAttr>() &&\n-      dstLayout.isa<BlockedEncodingAttr>()) {\n-    auto srcBlockedLayout = srcLayout.cast<BlockedEncodingAttr>();\n-    auto dstBlockedLayout = dstLayout.cast<BlockedEncodingAttr>();\n-    auto inOrd = srcBlockedLayout.getOrder();\n-    auto outOrd = dstBlockedLayout.getOrder();\n-    // TODO: Fix the legacy issue that ourOrd[0] == 0 always means\n-    //       that we cannot do vectorization.\n-    inVec = outOrd[0] == 0  ? 1\n-            : inOrd[0] == 0 ? 1\n-                            : srcBlockedLayout.getSizePerThread()[inOrd[0]];\n-    outVec =\n-        outOrd[0] == 0 ? 1 : dstBlockedLayout.getSizePerThread()[outOrd[0]];\n-    unsigned pad = std::max(inVec, outVec);\n-    for (unsigned d = 0; d < rank; ++d) {\n-      paddedRepShape[d] = std::max(\n-          std::min<unsigned>(srcTy.getShape()[d], getShapePerCTA(srcLayout, d)),\n-          std::min<unsigned>(dstTy.getShape()[d],\n-                             getShapePerCTA(dstLayout, d)));\n-    }\n-    paddedRepShape[outOrd[0]] += pad;\n+  auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>();\n+  auto srcMmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n+  auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>();\n+  auto dstMmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>();\n+  assert((srcBlockedLayout || srcMmaLayout) &&\n+         \"Unexpected srcLayout in getScratchConfigForCvtLayout\");\n+  assert((dstBlockedLayout || dstMmaLayout) &&\n+         \"Unexpected dstLayout in getScratchConfigForCvtLayout\");\n+  assert(!(srcMmaLayout && dstMmaLayout) &&\n+         \"Unexpected mma -> mma layout conversion\");\n+  auto inOrd =\n+      srcMmaLayout ? dstBlockedLayout.getOrder() : srcBlockedLayout.getOrder();\n+  auto outOrd =\n+      dstMmaLayout ? srcBlockedLayout.getOrder() : dstBlockedLayout.getOrder();\n+  unsigned srcContigPerThread =\n+      srcBlockedLayout ? srcBlockedLayout.getSizePerThread()[inOrd[0]] : 2;\n+  unsigned dstContigPerThread =\n+      dstBlockedLayout ? dstBlockedLayout.getSizePerThread()[outOrd[0]] : 2;\n+  // TODO: Fix the legacy issue that ourOrd[0] == 0 always means\n+  //       that we cannot do vectorization.\n+  inVec = outOrd[0] == 0 ? 1 : inOrd[0] == 0 ? 1 : srcContigPerThread;\n+  outVec = outOrd[0] == 0 ? 1 : dstContigPerThread;\n+\n+  auto srcShapePerCTA = getShapePerCTA(srcLayout);\n+  auto dstShapePerCTA = getShapePerCTA(dstLayout);\n+\n+  unsigned pad = std::max(inVec, outVec);\n+  for (unsigned d = 0; d < rank; ++d) {\n+    paddedRepShape[d] =\n+        std::max(std::min<unsigned>(srcTy.getShape()[d], srcShapePerCTA[d]),\n+                 std::min<unsigned>(dstTy.getShape()[d], dstShapePerCTA[d]));\n+  }\n+  unsigned paddedDim = 1;\n+  if (auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>()) {\n+    paddedDim = dstBlockedLayout.getOrder()[0];\n   }\n+  paddedRepShape[paddedDim] += pad;\n   return paddedRepShape;\n }\n "}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -65,7 +65,7 @@ AxisInfo AxisInfo::join(const AxisInfo &lhs, const AxisInfo &rhs) {\n   DimVectorT retContiguity;\n   DimVectorT retDivisibility;\n   DimVectorT retConstancy;\n-  for (size_t d = 0; d < lhs.getRank(); d++) {\n+  for (size_t d = 0; d < lhs.getRank(); ++d) {\n     retContiguity.push_back(gcd(lhs.getContiguity(d), rhs.getContiguity(d)));\n     retDivisibility.push_back(\n         gcd(lhs.getDivisibility(d), rhs.getDivisibility(d)));\n@@ -87,7 +87,7 @@ AxisInfo AxisInfoAnalysis::visitBinaryOp(\n   AxisInfo::DimVectorT newContiguity;\n   AxisInfo::DimVectorT newDivisibility;\n   AxisInfo::DimVectorT newConstancy;\n-  for (size_t d = 0; d < rank; d++) {\n+  for (size_t d = 0; d < rank; ++d) {\n     newContiguity.push_back(getContiguity(lhsInfo, rhsInfo, d));\n     newDivisibility.push_back(getDivisibility(lhsInfo, rhsInfo, d));\n     newConstancy.push_back(getConstancy(lhsInfo, rhsInfo, d));\n@@ -166,7 +166,7 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     AxisInfo::DimVectorT contiguity;\n     AxisInfo::DimVectorT divisibility;\n     AxisInfo::DimVectorT constancy;\n-    for (size_t d = 0; d < retTy.getRank(); d++) {\n+    for (size_t d = 0; d < retTy.getRank(); ++d) {\n       contiguity.push_back(1);\n       divisibility.push_back(opInfo.getDivisibility(0));\n       constancy.push_back(retTy.getShape()[d]);\n@@ -202,7 +202,7 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     AxisInfo::DimVectorT contiguity;\n     AxisInfo::DimVectorT divisibility;\n     AxisInfo::DimVectorT constancy;\n-    for (size_t d = 0; d < retTy.getRank(); d++) {\n+    for (size_t d = 0; d < retTy.getRank(); ++d) {\n       contiguity.push_back(opShape[d] == 1 ? 1 : opInfo.getContiguity(d));\n       divisibility.push_back(opInfo.getDivisibility(d));\n       constancy.push_back(opShape[d] == 1 ? retShape[d] : 1);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/PtxAsmFormat.cpp", "status": "modified", "additions": 25, "deletions": 2, "changes": 27, "file_content_changes": "@@ -1,4 +1,6 @@\n #include \"triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n #include \"llvm/Support/raw_ostream.h\"\n #include <sstream> // unify to llvm::raw_string_ostream ?\n \n@@ -10,7 +12,7 @@ std::string strJoin(llvm::ArrayRef<std::string> strs,\n                     llvm::StringRef delimiter) {\n   std::string osStr;\n   llvm::raw_string_ostream os(osStr);\n-  for (size_t i = 0; !strs.empty() && i < strs.size() - 1; i++)\n+  for (size_t i = 0; !strs.empty() && i < strs.size() - 1; ++i)\n     os << strs[i] << delimiter;\n   if (!strs.empty())\n     os << strs.back();\n@@ -74,6 +76,25 @@ SmallVector<PTXBuilder::Operand *, 4> PTXBuilder::getAllArgs() const {\n   return res;\n }\n \n+mlir::Value PTXBuilder::launch(ConversionPatternRewriter &rewriter,\n+                               Location loc, Type resTy, bool hasSideEffect,\n+                               bool isAlignStack,\n+                               ArrayRef<Attribute> attrs) const {\n+  auto *ctx = rewriter.getContext();\n+  auto inlineAsm = rewriter.create<LLVM::InlineAsmOp>(\n+      loc, resTy, getAllMLIRArgs(), // operands\n+      dump(),                       // asm_string\n+      getConstraints(),             // constraints\n+      hasSideEffect,                // has_side_effects\n+      isAlignStack,                 // is_align_stack\n+      LLVM::AsmDialectAttr::get(ctx,\n+                                LLVM::AsmDialect::AD_ATT), // asm_dialect\n+      ArrayAttr::get(ctx, attrs)                           // operand_attrs\n+  );\n+\n+  return inlineAsm.getRes();\n+}\n+\n std::string PTXInstr::Operand::dump() const {\n   if (repr)\n     return repr(idx);\n@@ -120,11 +141,12 @@ PTXInstrExecution &PTXInstrCommon::operator()(ArrayRef<Operand *> oprs) {\n std::string PTXInstrExecution::dump() const {\n   std::string osStr;\n   llvm::raw_string_ostream os(osStr);\n-  if (pred)\n+  if (pred) {\n     if (!pred->repr)\n       os << \"@\" << pred->dump() << \" \";\n     else\n       os << pred->repr(pred->idx) << \" \";\n+  }\n \n   std::string instrRepr = strJoin(instr->instrParts, \".\");\n \n@@ -151,5 +173,6 @@ PTXInstrExecution::getArgList() const {\n   }\n   return args;\n }\n+\n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 934, "deletions": 480, "changes": 1414, "file_content_changes": "@@ -29,7 +29,10 @@ using namespace mlir;\n using namespace mlir::triton;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::getThreadsPerCTA;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;\n@@ -48,16 +51,32 @@ Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v) {\n                                            IntegerAttr::get(i32ty, v));\n }\n \n-// Add other specification if needed...\n+// Create a index type constant.\n+Value createIndexConstant(OpBuilder &builder, Location loc,\n+\n+                          TypeConverter *converter, int64_t value) {\n+  Type ty = converter->convertType(builder.getIndexType());\n+  return builder.create<LLVM::ConstantOp>(loc, ty,\n+                                          builder.getIntegerAttr(ty, value));\n+}\n+\n+// Create an integer constant of \\param width bits.\n+Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n+                                int64_t value) {\n+  Type ty = builder.getIntegerType(width);\n+  return builder.create<LLVM::ConstantOp>(loc, ty,\n+                                          builder.getIntegerAttr(ty, value));\n+}\n \n } // namespace\n \n+// Shortcuts for some commonly used LLVM ops to keep code simple and intuitive\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n #define urem(...) rewriter.create<LLVM::URemOp>(loc, __VA_ARGS__)\n #define add(...) rewriter.create<LLVM::AddOp>(loc, __VA_ARGS__)\n #define mul(...) rewriter.create<LLVM::MulOp>(loc, __VA_ARGS__)\n #define xor_(...) rewriter.create<LLVM::XOrOp>(loc, __VA_ARGS__)\n-#define bit_cast(...) rewriter.create<LLVM::BitcastOp>(loc, __VA_ARGS__)\n+#define bitcast(...) rewriter.create<LLVM::BitcastOp>(loc, __VA_ARGS__)\n #define gep(...) rewriter.create<LLVM::GEPOp>(loc, __VA_ARGS__)\n #define ptr_ty(...) LLVM::LLVMPointerType::get(__VA_ARGS__)\n #define insert_val(...) rewriter.create<LLVM::InsertValueOp>(loc, __VA_ARGS__)\n@@ -66,10 +85,24 @@ Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v) {\n   rewriter.create<LLVM::InsertElementOp>(loc, __VA_ARGS__)\n #define extract_element(...)                                                   \\\n   rewriter.create<LLVM::ExtractElementOp>(loc, __VA_ARGS__)\n+#define load(...) rewriter.create<LLVM::LoadOp>(loc, __VA_ARGS__)\n+#define store(val, ptr) rewriter.create<LLVM::StoreOp>(loc, val, ptr)\n+#define select(...) rewriter.create<LLVM::SelectOp>(loc, __VA_ARGS__)\n #define address_of(...) rewriter.create<LLVM::AddressOfOp>(loc, __VA_ARGS__)\n-\n+#define barrier rewriter.create<mlir::gpu::BarrierOp>(loc)\n+#define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n+#define i32_ty rewriter.getIntegerType(32)\n+#define vec_ty(type, num) VectorType::get(num, type)\n+#define void_ty LLVM::LLVMVoidType::get(ctx)\n+#define struct_ty(...) LLVM::LLVMStructType::getLiteral(__VA_ARGS__)\n+\n+// Creator for constant\n #define i32_val(...) LLVM::createConstantI32(loc, rewriter, __VA_ARGS__)\n-#define i32_ty() rewriter.getIntegerType(32)\n+#define int_val(width, val)                                                    \\\n+  LLVM::createLLVMIntegerConstant(rewriter, loc, width, val)\n+#define idx_val(...)                                                           \\\n+  LLVM::createIndexConstant(rewriter, loc, this->getTypeConverter(),           \\\n+                            __VA_ARGS__)\n \n } // namespace LLVM\n } // namespace mlir\n@@ -80,6 +113,18 @@ namespace type = mlir::triton::type;\n \n class TritonGPUToLLVMTypeConverter;\n \n+// TODO: keep these before we have better debug log utilities\n+template <typename T>\n+void print_array(ArrayRef<T> array, const std::string &str) {\n+  std::cout << str << \": \";\n+  for (const T &e : array)\n+    std::cout << e << \",\";\n+  std::cout << std::endl;\n+}\n+template <typename T> void print_scalar(const T &e, const std::string &str) {\n+  std::cout << str << \": \" << e << std::endl;\n+}\n+\n // FuncOpConversion/FuncOpConversionBase is borrowed from\n // https://github.com/llvm/llvm-project/blob/fae656b2dd80246c3c6f01e9c77c49560368752c/mlir/lib/Conversion/FuncToLLVM/FuncToLLVM.cpp#L276\n // since it is not exposed on header files in mlir v14\n@@ -213,7 +258,7 @@ struct FuncOpConversion : public FuncOpConversionBase {\n     // Set an attribute for maxntidx, it could be used in latter LLVM codegen\n     // for `nvvm.annotation` metadata.\n     newFuncOp->setAttr(NVVMMetadataField::MaxNTid,\n-                       rewriter.getIntegerAttr(i32_ty(), 32 * NumWarps));\n+                       rewriter.getIntegerAttr(i32_ty, 32 * NumWarps));\n \n     rewriter.eraseOp(funcOp);\n     return success();\n@@ -245,19 +290,6 @@ struct ReturnOpConversion : public ConvertOpToLLVMPattern<::mlir::ReturnOp> {\n   }\n };\n \n-static Value createIndexAttrConstant(OpBuilder &builder, Location loc,\n-                                     Type resultType, int64_t value) {\n-  return builder.create<LLVM::ConstantOp>(\n-      loc, resultType, builder.getIntegerAttr(builder.getIndexType(), value));\n-}\n-\n-static Value createLLVMIntegerConstant(OpBuilder &builder, Location loc,\n-                                       LLVMTypeConverter *converter, Type ty,\n-                                       int64_t value) {\n-  return builder.create<LLVM::ConstantOp>(loc, converter->convertType(ty),\n-                                          builder.getIntegerAttr(ty, value));\n-}\n-\n Value getStructFromElements(Location loc, ValueRange resultVals,\n                             ConversionPatternRewriter &rewriter,\n                             Type structType) {\n@@ -270,42 +302,36 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n }\n \n template <typename T>\n-static SmallVector<T> getMultiDimIndex(T linear_index, ArrayRef<T> shape) {\n-  // sizes {a, b, c, d}  ->  acc_mul {b*c*d, c*d, d, 1}\n+static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n+  // shape: {a, b, c, d}  ->  accMul: {b*c*d, c*d, d, 1}\n   size_t rank = shape.size();\n-  T acc_mul = 1;\n-  for (size_t i = 1; i < rank; ++i) {\n-    acc_mul *= shape[i];\n-  }\n-  T linear_remain = linear_index;\n-  SmallVector<T> multidim_index(rank);\n+  T accMul = product(shape.drop_front());\n+  T linearRemain = linearIndex;\n+  SmallVector<T> multiDimIndex(rank);\n   for (size_t i = 0; i < rank; ++i) {\n-    multidim_index[i] = linear_remain / acc_mul;\n-    linear_remain = linear_remain % acc_mul;\n+    multiDimIndex[i] = linearRemain / accMul;\n+    linearRemain = linearRemain % accMul;\n     if (i != (rank - 1)) {\n-      acc_mul = acc_mul / shape[i + 1];\n+      accMul = accMul / shape[i + 1];\n     }\n   }\n-  return multidim_index;\n+  return multiDimIndex;\n }\n \n template <typename T>\n-static T getLinearIndex(ArrayRef<T> multidim_index, ArrayRef<T> shape) {\n-  assert(multidim_index.size() == shape.size());\n-  // sizes {a, b, c, d}  ->  acc_mul {b*c*d, c*d, d, 1}\n+static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n+  assert(multiDimIndex.size() == shape.size());\n+  // shape: {a, b, c, d}  ->  accMul: {b*c*d, c*d, d, 1}\n   size_t rank = shape.size();\n-  T acc_mul = 1;\n-  for (size_t i = 1; i < rank; ++i) {\n-    acc_mul *= shape[i];\n-  }\n-  T linear_index = 0;\n+  T accMul = product(shape.drop_front());\n+  T linearIndex = 0;\n   for (size_t i = 0; i < rank; ++i) {\n-    linear_index += multidim_index[i] * acc_mul;\n+    linearIndex += multiDimIndex[i] * accMul;\n     if (i != (rank - 1)) {\n-      acc_mul = acc_mul / shape[i + 1];\n+      accMul = accMul / shape[i + 1];\n     }\n   }\n-  return linear_index;\n+  return linearIndex;\n }\n \n struct ConvertTritonGPUOpToLLVMPatternBase {\n@@ -350,16 +376,22 @@ class ConvertTritonGPUOpToLLVMPattern\n     return threadId;\n   }\n \n+  Value createIndexConst(ConversionPatternRewriter &rewriter, Location loc,\n+                         int64_t value) const {\n+    return rewriter.create<LLVM::ConstantOp>(\n+        loc, this->getTypeConverter()->getIndexType(),\n+        rewriter.getIntegerAttr(rewriter.getIndexType(), value));\n+  }\n+\n+  // Convert an \\param index to a multi-dim coordinate given \\param shape and\n+  // \\param order.\n   SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n                                  Location loc, Value linear,\n                                  ArrayRef<unsigned> shape,\n                                  ArrayRef<unsigned> order) const {\n     unsigned rank = shape.size();\n     assert(rank == order.size());\n-    SmallVector<unsigned> reordered(rank);\n-    for (unsigned i = 0; i < rank; ++i) {\n-      reordered[i] = shape[order[i]];\n-    }\n+    auto reordered = reorder(shape, order);\n     auto reorderedMultiDim = delinearize(rewriter, loc, linear, reordered);\n     SmallVector<Value> multiDim(rank);\n     for (unsigned i = 0; i < rank; ++i) {\n@@ -379,9 +411,7 @@ class ConvertTritonGPUOpToLLVMPattern\n     } else {\n       Value remained = linear;\n       for (auto &&en : llvm::enumerate(llvm::reverse(shape.drop_front()))) {\n-        Value dimSize = createIndexAttrConstant(\n-            rewriter, loc, this->getTypeConverter()->getIndexType(),\n-            en.value());\n+        Value dimSize = idx_val(en.value());\n         multiDim[rank - 1 - en.index()] = urem(remained, dimSize);\n         remained = udiv(remained, dimSize);\n       }\n@@ -393,28 +423,27 @@ class ConvertTritonGPUOpToLLVMPattern\n   Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n                   ArrayRef<Value> multiDim, ArrayRef<unsigned> shape) const {\n     int rank = multiDim.size();\n-    Value linear = createIndexAttrConstant(\n-        rewriter, loc, this->getTypeConverter()->getIndexType(), 0);\n+    Value linear = idx_val(0);\n     if (rank > 0) {\n       linear = multiDim.front();\n-      for (auto &&z : llvm::zip(multiDim.drop_front(), shape.drop_front())) {\n-        Value dimSize = createIndexAttrConstant(\n-            rewriter, loc, this->getTypeConverter()->getIndexType(),\n-            std::get<1>(z));\n-        linear = add(mul(linear, dimSize), std::get<0>(z));\n+      for (auto [dim, shape] :\n+           llvm::zip(multiDim.drop_front(), shape.drop_front())) {\n+        Value dimSize = idx_val(shape);\n+        linear = add(mul(linear, dimSize), dim);\n       }\n     }\n     return linear;\n   }\n \n+  // Get an index-base for each dimension for a \\param blocked_layout.\n   SmallVector<Value>\n   emitBaseIndexForBlockedLayout(Location loc,\n                                 ConversionPatternRewriter &rewriter,\n                                 const BlockedEncodingAttr &blocked_layout,\n                                 ArrayRef<int64_t> shape) const {\n     auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n     Value threadId = getThreadId(rewriter, loc);\n-    Value warpSize = createIndexAttrConstant(rewriter, loc, llvmIndexTy, 32);\n+    Value warpSize = idx_val(32);\n     Value laneId = urem(threadId, warpSize);\n     Value warpId = udiv(threadId, warpSize);\n     auto sizePerThread = blocked_layout.getSizePerThread();\n@@ -423,7 +452,7 @@ class ConvertTritonGPUOpToLLVMPattern\n     auto order = blocked_layout.getOrder();\n     unsigned rank = shape.size();\n \n-    // step 1, delinearise threadId to get the base index\n+    // delinearize threadId to get the base index\n     SmallVector<Value> multiDimWarpId =\n         delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n     SmallVector<Value> multiDimThreadId =\n@@ -435,19 +464,13 @@ class ConvertTritonGPUOpToLLVMPattern\n       unsigned maxWarps =\n           ceil<unsigned>(shape[k], sizePerThread[k] * threadsPerWarp[k]);\n       unsigned maxThreads = ceil<unsigned>(shape[k], sizePerThread[k]);\n-      multiDimWarpId[k] =\n-          urem(multiDimWarpId[k],\n-               createIndexAttrConstant(rewriter, loc, llvmIndexTy, maxWarps));\n-      multiDimThreadId[k] =\n-          urem(multiDimThreadId[k],\n-               createIndexAttrConstant(rewriter, loc, llvmIndexTy, maxThreads));\n+      multiDimWarpId[k] = urem(multiDimWarpId[k], idx_val(maxWarps));\n+      multiDimThreadId[k] = urem(multiDimThreadId[k], idx_val(maxThreads));\n       // multiDimBase[k] = (multiDimThreadId[k] +\n       //                    multiDimWarpId[k] * threadsPerWarp[k]) *\n       //                   sizePerThread[k];\n-      Value threadsPerWarpK = createIndexAttrConstant(\n-          rewriter, loc, llvmIndexTy, threadsPerWarp[k]);\n-      Value sizePerThreadK =\n-          createIndexAttrConstant(rewriter, loc, llvmIndexTy, sizePerThread[k]);\n+      Value threadsPerWarpK = idx_val(threadsPerWarp[k]);\n+      Value sizePerThreadK = idx_val(sizePerThread[k]);\n       multiDimBase[k] =\n           mul(sizePerThreadK, add(multiDimThreadId[k],\n                                   mul(multiDimWarpId[k], threadsPerWarpK)));\n@@ -480,25 +503,22 @@ class ConvertTritonGPUOpToLLVMPattern\n     if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n       SmallVector<int64_t> paddedShape(rank + 1);\n       for (unsigned d = 0; d < rank + 1; ++d) {\n-        if (d < dim) {\n+        if (d < dim)\n           paddedShape[d] = shape[d];\n-        } else if (d == dim) {\n+        else if (d == dim)\n           paddedShape[d] = 1;\n-        } else {\n+        else\n           paddedShape[d] = shape[d - 1];\n-        }\n       }\n       auto paddedIndices = emitIndicesForBlockedLayout(\n           loc, rewriter, blockedParent, paddedShape);\n       unsigned numIndices = paddedIndices.size();\n       SmallVector<SmallVector<Value>> resultIndices(numIndices);\n-      for (unsigned i = 0; i < numIndices; ++i) {\n-        for (unsigned d = 0; d < rank + 1; ++d) {\n-          if (d != dim) {\n+      for (unsigned i = 0; i < numIndices; ++i)\n+        for (unsigned d = 0; d < rank + 1; ++d)\n+          if (d != dim)\n             resultIndices[i].push_back(paddedIndices[i][d]);\n-          }\n-        }\n-      }\n+\n       return resultIndices;\n \n     } else if (auto sliceParent = parent.dyn_cast<SliceEncodingAttr>()) {\n@@ -513,7 +533,8 @@ class ConvertTritonGPUOpToLLVMPattern\n     }\n   }\n \n-  // Emit indices calculation within each ConversionPattern\n+  // Emit indices calculation within each ConversionPattern, and returns a\n+  // [elemsPerThread X rank] index matrix.\n   // TODO: [goostavz] Double confirm the redundant indices calculations will\n   //       be eliminated in the consequent MLIR/LLVM optimization. We might\n   //       implement a indiceCache if necessary.\n@@ -526,26 +547,21 @@ class ConvertTritonGPUOpToLLVMPattern\n     auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n     auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n     unsigned rank = shape.size();\n-    SmallVector<unsigned> shapePerCTA(rank);\n-    for (unsigned k = 0; k < rank; ++k) {\n-      shapePerCTA[k] = sizePerThread[k] * threadsPerWarp[k] * warpsPerCTA[k];\n-    }\n+    SmallVector<unsigned> shapePerCTA = getShapePerCTA(blockedLayout);\n+    SmallVector<unsigned> tilesPerDim(rank);\n+    for (unsigned k = 0; k < rank; ++k)\n+      tilesPerDim[k] = ceil<unsigned>(shape[k], shapePerCTA[k]);\n \n     // step 1, delinearize threadId to get the base index\n     auto multiDimBase =\n         emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n \n     // step 2, get offset of each element\n-    unsigned elemsPerThread = 1;\n+    unsigned elemsPerThread = blockedLayout.getElemsPerThread(shape);\n     SmallVector<SmallVector<unsigned>> offset(rank);\n-    SmallVector<unsigned> multiDimElemsPerThread(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n-      multiDimElemsPerThread[k] =\n-          ceil<unsigned>(shape[k], shapePerCTA[k]) * sizePerThread[k];\n-      elemsPerThread *= multiDimElemsPerThread[k];\n       // 1 block in minimum if shape[k] is less than shapePerCTA[k]\n-      for (unsigned blockOffset = 0;\n-           blockOffset < ceil<unsigned>(shape[k], shapePerCTA[k]);\n+      for (unsigned blockOffset = 0; blockOffset < tilesPerDim[k];\n            ++blockOffset)\n         for (unsigned warpOffset = 0; warpOffset < warpsPerCTA[k]; ++warpOffset)\n           for (unsigned threadOffset = 0; threadOffset < threadsPerWarp[k];\n@@ -558,34 +574,26 @@ class ConvertTritonGPUOpToLLVMPattern\n                                       threadsPerWarp[k] +\n                                   threadOffset * sizePerThread[k] + elemOffset);\n     }\n-    // step 3, add offset to base, and reorder the sequence of indices,\n-    //         to guarantee that elems in a same sizePerThread are adjacent in\n-    //         order\n-    SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread);\n-    unsigned accumSizePerThread =\n-        std::accumulate(sizePerThread.begin(), sizePerThread.end(), 1,\n-                        std::multiplies<unsigned>());\n-    SmallVector<unsigned> threadsPerDim(rank);\n-    for (unsigned k = 0; k < rank; ++k) {\n-      threadsPerDim[k] = ceil<unsigned>(shape[k], sizePerThread[k]);\n-    }\n+    // step 3, add offset to base, and reorder the sequence of indices to\n+    // guarantee that elems in the same sizePerThread are adjacent in order\n+    SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread,\n+                                                SmallVector<Value>(rank));\n+    unsigned totalSizePerThread = product<unsigned>(sizePerThread);\n+\n     for (unsigned n = 0; n < elemsPerThread; ++n) {\n-      unsigned linearNanoTileId = n / accumSizePerThread;\n-      unsigned linearElemsInNanoTileId = n % accumSizePerThread;\n+      unsigned linearNanoTileId = n / totalSizePerThread;\n+      unsigned linearNanoTileElemId = n % totalSizePerThread;\n       SmallVector<unsigned> multiDimNanoTileId =\n-          getMultiDimIndex<unsigned>(linearNanoTileId, threadsPerDim);\n-      SmallVector<unsigned> multiElemsInNanoTileId =\n-          getMultiDimIndex<unsigned>(linearElemsInNanoTileId, sizePerThread);\n-      multiDimIdx[n].resize(rank);\n+          getMultiDimIndex<unsigned>(linearNanoTileId, tilesPerDim);\n+      SmallVector<unsigned> multiDimNanoTileElemId =\n+          getMultiDimIndex<unsigned>(linearNanoTileElemId, sizePerThread);\n       for (unsigned k = 0; k < rank; ++k) {\n         unsigned reorderedMultiDimId =\n             multiDimNanoTileId[k] *\n                 (sizePerThread[k] * threadsPerWarp[k] * warpsPerCTA[k]) +\n-            multiElemsInNanoTileId[k];\n+            multiDimNanoTileElemId[k];\n         multiDimIdx[n][k] =\n-            add(multiDimBase[k],\n-                createIndexAttrConstant(rewriter, loc, llvmIndexTy,\n-                                        offset[k][reorderedMultiDimId]));\n+            add(multiDimBase[k], idx_val(offset[k][reorderedMultiDimId]));\n       }\n     }\n \n@@ -601,7 +609,7 @@ class ConvertTritonGPUOpToLLVMPattern\n     assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n     size_t offset = allocation->getOffset(bufferId);\n     auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n-    Value offVal = createIndexAttrConstant(rewriter, loc, llvmIndexTy, offset);\n+    Value offVal = idx_val(offset);\n     Value base = gep(ptrTy, smem, offVal);\n     return base;\n   }\n@@ -620,19 +628,17 @@ class ConvertTritonGPUOpToLLVMPattern\n Value convertSplatLikeOp(Type elemType, Type resType, Value constVal,\n                          TypeConverter *typeConverter,\n                          ConversionPatternRewriter &rewriter, Location loc) {\n-\n   auto tensorTy = resType.cast<RankedTensorType>();\n   auto layout = tensorTy.getEncoding();\n   auto srcType = typeConverter->convertType(elemType);\n-  auto llSrc = bit_cast(srcType, constVal);\n-  size_t numElemsPerThread = getElemsPerThread(layout, tensorTy.getShape());\n-  llvm::SmallVector<Value, 4> elems(numElemsPerThread, llSrc);\n+  auto llSrc = bitcast(srcType, constVal);\n+  size_t elemsPerThread = getElemsPerThread(layout, tensorTy.getShape());\n+  llvm::SmallVector<Value, 4> elems(elemsPerThread, llSrc);\n   llvm::SmallVector<Type, 4> elemTypes(elems.size(), srcType);\n   auto structTy =\n       LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);\n \n-  auto llStruct = getStructFromElements(loc, elems, rewriter, structTy);\n-  return llStruct;\n+  return getStructFromElements(loc, elems, rewriter, structTy);\n }\n \n struct SplatOpConversion\n@@ -701,22 +707,14 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n   // Get corresponding LLVM element values of \\param value.\n   SmallVector<Value> getLLVMElems(Value value, Value llValue,\n                                   const BlockedEncodingAttr &layout,\n-                                  TypeConverter *typeConverter,\n                                   ConversionPatternRewriter &rewriter,\n                                   Location loc) const {\n     if (!value)\n       return {};\n \n-    auto ty = value.getType().cast<RankedTensorType>();\n-    auto shape = ty.getShape();\n+    auto shape = value.getType().cast<RankedTensorType>().getShape();\n     // Here, we assume that all inputs should have a blockedLayout\n-\n     unsigned valueElems = layout.getElemsPerThread(shape);\n-\n-    auto llvmElemTy = typeConverter->convertType(ty.getElementType());\n-    auto llvmElemPtrPtrTy =\n-        LLVM::LLVMPointerType::get(LLVM::LLVMPointerType::get(llvmElemTy));\n-\n     auto valueVals = getElementsFromStruct(loc, llValue, valueElems, rewriter);\n     return valueVals;\n   }\n@@ -729,7 +727,7 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n     assert(layout && \"unexpected layout in getLayout\");\n     auto shape = ty.getShape();\n     unsigned valueElems = layout.getElemsPerThread(shape);\n-    return std::make_tuple(layout, valueElems);\n+    return {layout, valueElems};\n   }\n \n   unsigned getAlignment(Value val, const BlockedEncodingAttr &layout) const {\n@@ -805,16 +803,13 @@ struct StoreOpConversion\n \n     auto [layout, numElems] = getLayout(ptr);\n \n-    auto ptrElems =\n-        getLLVMElems(ptr, llPtr, layout, getTypeConverter(), rewriter, loc);\n-    auto valueElems =\n-        getLLVMElems(value, llValue, layout, getTypeConverter(), rewriter, loc);\n+    auto ptrElems = getLLVMElems(ptr, llPtr, layout, rewriter, loc);\n+    auto valueElems = getLLVMElems(value, llValue, layout, rewriter, loc);\n     assert(ptrElems.size() == valueElems.size());\n \n     SmallVector<Value> maskElems;\n     if (llMask) {\n-      maskElems =\n-          getLLVMElems(mask, llMask, layout, getTypeConverter(), rewriter, loc);\n+      maskElems = getLLVMElems(mask, llMask, layout, rewriter, loc);\n       assert(valueElems.size() == maskElems.size());\n     }\n \n@@ -843,45 +838,42 @@ struct StoreOpConversion\n       const bool hasL2EvictPolicy = false;\n \n       PTXBuilder ptxBuilder;\n-      auto &ptxStoreInstr = *ptxBuilder.create<PtxIOInstr>(\"st\");\n+      auto &ptxStoreInstr = *ptxBuilder.create<PTXIOInstr>(\"st\");\n \n       llvm::SmallVector<std::string> asmArgs;\n \n       Type valArgTy = IntegerType::get(ctx, width);\n-      auto wordTy = VectorType::get(wordNElems, valueElemTy);\n+      auto wordTy = vec_ty(valueElemTy, wordNElems);\n \n       auto *asmArgList = ptxBuilder.newListOperand();\n-      for (int wordIdx = 0; wordIdx < nWords; wordIdx++) {\n+      for (int wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n         // llWord is a width-len composition\n         Value llWord = rewriter.create<LLVM::UndefOp>(loc, wordTy);\n         // Insert each value element to the composition\n-        for (int elemIdx = 0; elemIdx < wordNElems; elemIdx++) {\n+        for (int elemIdx = 0; elemIdx < wordNElems; ++elemIdx) {\n           const size_t elemOffset = vecStart + wordIdx * wordNElems + elemIdx;\n           assert(elemOffset < valueElems.size());\n           Value elem = valueElems[elemOffset];\n           if (elem.getType().isInteger(1))\n             elem = rewriter.create<LLVM::SExtOp>(loc, type::i8Ty(ctx), elem);\n-          elem = bit_cast(valueElemTy, elem);\n+          elem = bitcast(valueElemTy, elem);\n \n           Type u32Ty = typeConverter->convertType(type::u32Ty(ctx));\n           llWord =\n               insert_element(wordTy, llWord, elem,\n                              rewriter.create<LLVM::ConstantOp>(\n                                  loc, u32Ty, IntegerAttr::get(u32Ty, elemIdx)));\n         }\n-        llWord = bit_cast(valArgTy, llWord);\n+        llWord = bitcast(valArgTy, llWord);\n         std::string constraint =\n             (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n         asmArgList->listAppend(ptxBuilder.newOperand(llWord, constraint));\n       }\n \n-      // TODO(Superjomn) Need to check masks before vectorize the load for all\n+      // TODO(Superjomn) Need to check masks before vectorize the load for\n       // the values share one predicate? Here assume all the mask values are\n       // the same.\n-      Value maskVal =\n-          llMask ? maskElems[vecStart]\n-                 : createLLVMIntegerConstant(rewriter, loc, getTypeConverter(),\n-                                             rewriter.getIntegerType(1), 1);\n+      Value maskVal = llMask ? maskElems[vecStart] : int_val(1, 1);\n       ptxStoreInstr.global().b(width).v(nWords);\n \n       auto *asmAddr =\n@@ -890,22 +882,12 @@ struct StoreOpConversion\n       ptxStoreInstr(asmAddr, asmArgList).predicate(maskVal, \"b\");\n       Type boolTy = getTypeConverter()->convertType(rewriter.getIntegerType(1));\n       llvm::SmallVector<Type> argTys({boolTy, ptr.getType()});\n-      for (int i = 0; i < nWords; i++)\n+      for (int i = 0; i < nWords; ++i)\n         argTys.push_back(valArgTy);\n \n       auto ASMReturnTy = LLVM::LLVMVoidType::get(ctx);\n \n-      auto inlineAsm = rewriter.create<LLVM::InlineAsmOp>(\n-          loc, ASMReturnTy, ptxBuilder.getAllMLIRArgs(), // operands\n-          ptxBuilder.dump(),                             // asm_string\n-          ptxBuilder.getConstraints(),                   // constraints\n-          // TODO(Superjomn) determine the side effect.\n-          true,  // has_side_effects\n-          false, // is_align_stack\n-          LLVM::AsmDialectAttr::get(ctx,\n-                                    LLVM::AsmDialect::AD_ATT), // asm_dialect\n-          ArrayAttr::get(ctx, {})                              // operand_attrs\n-      );\n+      ptxBuilder.launch(rewriter, loc, ASMReturnTy);\n     }\n     rewriter.eraseOp(op);\n     return success();\n@@ -1089,7 +1071,6 @@ struct LoadOpConversion\n   LogicalResult\n   matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-\n     Value ptr = op.ptr();\n     Value mask = op.mask();\n     Value other = op.other();\n@@ -1109,14 +1090,12 @@ struct LoadOpConversion\n \n     auto [layout, numElems] = getLayout(ptr);\n \n-    auto ptrElems =\n-        getLLVMElems(ptr, llPtr, layout, getTypeConverter(), rewriter, loc);\n+    auto ptrElems = getLLVMElems(ptr, llPtr, layout, rewriter, loc);\n     assert(ptrElems.size() == numElems);\n \n     SmallVector<Value> maskElems;\n     if (llMask) {\n-      maskElems =\n-          getLLVMElems(mask, llMask, layout, getTypeConverter(), rewriter, loc);\n+      maskElems = getLLVMElems(mask, llMask, layout, rewriter, loc);\n       assert(ptrElems.size() == maskElems.size());\n     }\n \n@@ -1141,8 +1120,7 @@ struct LoadOpConversion\n       splatVal = constAttr.getSplatValue<APInt>().getSExtValue();\n     }\n \n-    auto otherElems =\n-        getLLVMElems(other, llOther, layout, getTypeConverter(), rewriter, loc);\n+    auto otherElems = getLLVMElems(other, llOther, layout, rewriter, loc);\n \n     SmallVector<Value> loadedVals;\n     for (size_t vecStart = 0; vecStart < numElems; vecStart += vec) {\n@@ -1162,15 +1140,12 @@ struct LoadOpConversion\n       const bool hasL2EvictPolicy = false;\n \n       PTXBuilder ptxBuilder;\n-      auto &ld = *ptxBuilder.create<PtxIOInstr>(\"ld\");\n+      auto &ld = *ptxBuilder.create<PTXIOInstr>(\"ld\");\n \n       // TODO(Superjomn) Need to check masks before vectorize the load for all\n       // the values share one predicate? Here assume all the mask values are\n       // the same.\n-      Value pred =\n-          mask ? maskElems[vecStart]\n-               : createLLVMIntegerConstant(rewriter, loc, getTypeConverter(),\n-                                           rewriter.getIntegerType(1), 1);\n+      Value pred = mask ? maskElems[vecStart] : int_val(1, 1);\n \n       const std::string readConstraint =\n           (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n@@ -1179,7 +1154,7 @@ struct LoadOpConversion\n \n       // prepare asm operands\n       auto *dstsOpr = ptxBuilder.newListOperand();\n-      for (int wordIdx = 0; wordIdx < nWords; wordIdx++) {\n+      for (int wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n         auto *opr = ptxBuilder.newOperand(writeConstraint); // =r operations\n         dstsOpr->listAppend(opr);\n       }\n@@ -1210,30 +1185,28 @@ struct LoadOpConversion\n       else\n         ld(dstsOpr, addrOpr, evictOpr).predicate(pred, \"b\");\n \n-      SmallVector<Value> others;\n       if (other) {\n-        for (size_t ii = 0; ii < nWords; ii++) {\n+        for (size_t ii = 0; ii < nWords; ++ii) {\n           PTXInstr &mov = *ptxBuilder.create<>(\"mov\");\n           mov.o(\"u\", width);\n \n           size_t size = width / valueElemNbits;\n \n           auto vecTy = LLVM::getFixedVectorType(valueElemTy, size);\n           Value v = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n-          for (size_t s = 0; s < size; s++) {\n+          for (size_t s = 0; s < size; ++s) {\n             Value falseVal = otherElems[vecStart + ii * size + s];\n             Value sVal = createIndexAttrConstant(\n                 rewriter, loc, this->getTypeConverter()->getIndexType(), s);\n             v = insert_element(vecTy, v, falseVal, sVal);\n           }\n-          v = bit_cast(IntegerType::get(getContext(), width), v);\n+          v = bitcast(IntegerType::get(getContext(), width), v);\n \n           PTXInstr::Operand *opr{};\n           if (otherIsSplatConstInt) {\n             opr = ptxBuilder.newConstantOperand(splatVal);\n           } else {\n             opr = ptxBuilder.newOperand(v, readConstraint);\n-            others.push_back(v);\n           }\n \n           mov(dstsOpr->listGet(ii), opr).predicateNot(pred, \"b\");\n@@ -1251,34 +1224,27 @@ struct LoadOpConversion\n       // TODO: if (has_l2_evict_policy)\n       auto asmDialectAttr = LLVM::AsmDialectAttr::get(rewriter.getContext(),\n                                                       LLVM::AsmDialect::AD_ATT);\n-      auto inlineAsmOp = rewriter.create<LLVM::InlineAsmOp>(\n-          loc, retTy, /*operands=*/ptxBuilder.getAllMLIRArgs(),\n-          /*asm_string=*/ptxBuilder.dump(),\n-          /*constraints=*/ptxBuilder.getConstraints(),\n-          /*has_side_effects=*/true,\n-          /*is_align_stack=*/false, /*asm_dialect=*/asmDialectAttr,\n-          /*operand_attrs=*/ArrayAttr());\n-      Value ret = inlineAsmOp.getResult(0);\n+      Value ret = ptxBuilder.launch(rewriter, loc, retTy);\n \n       // ---\n       // extract and store return values\n       // ---\n       SmallVector<Value> rets;\n-      for (unsigned int ii = 0; ii < nWords; ii++) {\n+      for (unsigned int ii = 0; ii < nWords; ++ii) {\n         Value curr;\n         if (retTy.isa<LLVM::LLVMStructType>()) {\n           curr = extract_val(IntegerType::get(getContext(), width), ret,\n                              rewriter.getI64ArrayAttr(ii));\n         } else {\n           curr = ret;\n         }\n-        curr = bit_cast(\n+        curr = bitcast(\n             LLVM::getFixedVectorType(valueElemTy, width / valueElemNbits),\n             curr);\n         rets.push_back(curr);\n       }\n-      int tmp = (width / valueElemNbits);\n-      for (size_t ii = 0; ii < vec; ii++) {\n+      int tmp = width / valueElemNbits;\n+      for (size_t ii = 0; ii < vec; ++ii) {\n         Value vecIdx = createIndexAttrConstant(\n             rewriter, loc, this->getTypeConverter()->getIndexType(), ii % tmp);\n         Value loaded = extract_element(valueElemTy, rets[ii / tmp], vecIdx);\n@@ -1379,9 +1345,8 @@ struct ExtractSliceOpConversion\n \n     // axis > 0 will result in non-contiguous memory access if the result tensor\n     // is an alias of the source tensor.\n-    auto axis =\n-        op->getAttrOfType<IntegerAttr>(\"axis\").cast<IntegerAttr>().getInt();\n-    assert(axis == 0 && \"Only axis=0 is supported for now\");\n+    auto axis = op->getAttrOfType<IntegerAttr>(\"axis\").getInt();\n+    assert(axis == 0 && \"extract_slice: Only axis=0 is supported for now\");\n \n     // Example:\n     // %dst = extract_slice %src, %index {axis = 0}\n@@ -1391,12 +1356,11 @@ struct ExtractSliceOpConversion\n     auto base = product<int64_t>(dstTy.getShape());\n     auto baseVal = createIndexAttrConstant(\n         rewriter, loc, getTypeConverter()->getIndexType(), base);\n-    Value offset = rewriter.create<LLVM::MulOp>(loc, adaptor.index(), baseVal);\n+    Value offset = mul(adaptor.index(), baseVal);\n \n     auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n     auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n-    Value resultVal =\n-        rewriter.create<LLVM::GEPOp>(loc, elemPtrTy, adaptor.src(), offset);\n+    Value resultVal = gep(elemPtrTy, adaptor.src(), offset);\n     rewriter.replaceOp(op, resultVal);\n     return success();\n   }\n@@ -1452,108 +1416,24 @@ struct ConvertLayoutOpConversion\n   LogicalResult\n   matchAndRewrite(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto loc = op.getLoc();\n     Value src = op.src();\n     Value dst = op.result();\n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n     Attribute dstLayout = dstTy.getEncoding();\n-    if ((!srcLayout.isa<BlockedEncodingAttr>()) ||\n-        (!dstLayout.isa<BlockedEncodingAttr>())) {\n-      // TODO: not implemented\n-      return failure();\n-    }\n-    auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n-    Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n-    auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n-    smemBase = bit_cast(elemPtrTy, smemBase);\n-\n-    auto shape = dstTy.getShape();\n-    unsigned rank = dstTy.getRank();\n-    auto getContigPerThread = [&](const Attribute &layout,\n-                                  unsigned d) -> unsigned {\n-      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-        return blockedLayout.getSizePerThread()[d];\n-      } else {\n-        assert(0 && \"Unimplemented usage of getContigPerThread\");\n-        return 0;\n-      }\n-    };\n-    auto getAccumElemsPerThread = [&](const Attribute &layout) -> unsigned {\n-      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-        return product<unsigned>(blockedLayout.getSizePerThread());\n-      } else {\n-        assert(0 && \"Unimplemented usage of getAccumElemsPerThread\");\n-        return 0;\n-      }\n-    };\n-    auto getOrder = [&](const Attribute &layout) -> ArrayRef<unsigned> {\n-      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-        return blockedLayout.getOrder();\n-      } else {\n-        assert(0 && \"Unimplemented usage of getAccumElemsPerThread\");\n-        return {};\n-      }\n-    };\n-    SmallVector<unsigned> numReplicates(rank);\n-    SmallVector<unsigned> inNumCTAsEachRep(rank);\n-    SmallVector<unsigned> outNumCTAsEachRep(rank);\n-    SmallVector<unsigned> inNumCTAs(rank);\n-    SmallVector<unsigned> outNumCTAs(rank);\n-    for (unsigned d = 0; d < rank; ++d) {\n-      unsigned inPerCTA =\n-          std::min(unsigned(shape[d]), getShapePerCTA(srcLayout, d));\n-      unsigned outPerCTA =\n-          std::min(unsigned(shape[d]), getShapePerCTA(dstLayout, d));\n-      unsigned maxPerCTA = std::max(inPerCTA, outPerCTA);\n-      numReplicates[d] = ceil<unsigned>(shape[d], maxPerCTA);\n-      inNumCTAsEachRep[d] = maxPerCTA / inPerCTA;\n-      outNumCTAsEachRep[d] = maxPerCTA / outPerCTA;\n-      // TODO: confirm this\n-      assert(maxPerCTA % inPerCTA == 0 && maxPerCTA % outPerCTA == 0);\n-      inNumCTAs[d] = ceil<unsigned>(shape[d], inPerCTA);\n-      outNumCTAs[d] = ceil<unsigned>(shape[d], outPerCTA);\n+    if (srcLayout.isa<BlockedEncodingAttr>() &&\n+        dstLayout.isa<SharedEncodingAttr>()) {\n+      return lowerBlockedToShared(op, adaptor, rewriter);\n     }\n-    // Potentially we need to store for multiple CTAs in this replication\n-    unsigned accumNumReplicates = product<unsigned>(numReplicates);\n-    unsigned accumInSizePerThread = getAccumElemsPerThread(srcLayout);\n-    unsigned elems = getElemsPerThread(srcLayout, srcTy.getShape());\n-    auto vals = getElementsFromStruct(loc, adaptor.src(), elems, rewriter);\n-    unsigned inVec = 0;\n-    unsigned outVec = 0;\n-    auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n-\n-    unsigned outElems = getElemsPerThread(dstLayout, shape);\n-    auto outOrd = getOrder(dstLayout);\n-    SmallVector<Value> outVals(outElems);\n-    for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n-      auto multiDimRepId = getMultiDimIndex<unsigned>(repId, numReplicates);\n-      rewriter.create<mlir::gpu::BarrierOp>(loc);\n-      if (auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>()) {\n-        processReplicaBlocked(loc, rewriter, /*stNotRd*/ true, srcTy,\n-                              inNumCTAsEachRep, multiDimRepId, inVec,\n-                              paddedRepShape, outOrd, vals, smemBase);\n-      } else {\n-        assert(0 && \"ConvertLayout with input layout not implemented\");\n-        return failure();\n-      }\n-      rewriter.create<mlir::gpu::BarrierOp>(loc);\n-      if (auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>()) {\n-        processReplicaBlocked(loc, rewriter, /*stNotRd*/ false, dstTy,\n-                              outNumCTAsEachRep, multiDimRepId, outVec,\n-                              paddedRepShape, outOrd, outVals, smemBase);\n-      } else {\n-        assert(0 && \"ConvertLayout with output layout not implemented\");\n-        return failure();\n-      }\n+    if ((!srcLayout.isa<BlockedEncodingAttr>() &&\n+         !srcLayout.isa<MmaEncodingAttr>()) ||\n+        (!dstLayout.isa<BlockedEncodingAttr>() &&\n+         !dstLayout.isa<MmaEncodingAttr>())) {\n+      // TODO: to be implemented\n+      return failure();\n     }\n-\n-    SmallVector<Type> types(outElems, llvmElemTy);\n-    Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n-    Value result = getStructFromElements(loc, outVals, rewriter, structTy);\n-    rewriter.replaceOp(op, result);\n-    return success();\n+    return lowerDistributedToDistributed(op, adaptor, rewriter);\n   }\n \n private:\n@@ -1568,98 +1448,348 @@ struct ConvertLayoutOpConversion\n     return result;\n   };\n \n-  void processReplicaBlocked(Location loc, ConversionPatternRewriter &rewriter,\n-                             bool stNotRd, RankedTensorType type,\n-                             ArrayRef<unsigned> numCTAsEachRep,\n-                             ArrayRef<unsigned> multiDimRepId, unsigned vec,\n-                             ArrayRef<unsigned> paddedRepShape,\n-                             ArrayRef<unsigned> outOrd,\n-                             SmallVector<Value> &vals, Value smemBase) const {\n-    unsigned accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n-    auto layout = type.getEncoding().cast<BlockedEncodingAttr>();\n-    auto rank = type.getRank();\n-    auto sizePerThread = layout.getSizePerThread();\n-    auto accumSizePerThread = product<unsigned>(sizePerThread);\n-    auto llvmIndexTy = getTypeConverter()->getIndexType();\n-    SmallVector<unsigned> numCTAs(rank);\n-    SmallVector<unsigned> shapePerCTA(rank);\n-    for (unsigned d = 0; d < rank; ++d) {\n-      shapePerCTA[d] = layout.getSizePerThread()[d] *\n-                       layout.getThreadsPerWarp()[d] *\n-                       layout.getWarpsPerCTA()[d];\n-      numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n+  // shared memory rd/st for blocked or mma layout with data padding\n+  void processReplica(Location loc, ConversionPatternRewriter &rewriter,\n+                      bool stNotRd, RankedTensorType type,\n+                      ArrayRef<unsigned> numCTAsEachRep,\n+                      ArrayRef<unsigned> multiDimRepId, unsigned vec,\n+                      ArrayRef<unsigned> paddedRepShape,\n+                      ArrayRef<unsigned> outOrd, SmallVector<Value> &vals,\n+                      Value smemBase) const;\n+\n+  // blocked/mma -> blocked/mma.\n+  // Data padding in shared memory to avoid bank confict.\n+  LogicalResult\n+  lowerDistributedToDistributed(triton::gpu::ConvertLayoutOp op,\n+                                OpAdaptor adaptor,\n+                                ConversionPatternRewriter &rewriter) const;\n+\n+  // blocked -> shared.\n+  // Swizzling in shared memory to avoid bank conflict. Normally used for\n+  // A/B operands of dots.\n+  LogicalResult lowerBlockedToShared(triton::gpu::ConvertLayoutOp op,\n+                                     OpAdaptor adaptor,\n+                                     ConversionPatternRewriter &rewriter) const;\n+};\n+\n+void ConvertLayoutOpConversion::processReplica(\n+    Location loc, ConversionPatternRewriter &rewriter, bool stNotRd,\n+    RankedTensorType type, ArrayRef<unsigned> numCTAsEachRep,\n+    ArrayRef<unsigned> multiDimRepId, unsigned vec,\n+    ArrayRef<unsigned> paddedRepShape, ArrayRef<unsigned> outOrd,\n+    SmallVector<Value> &vals, Value smemBase) const {\n+  unsigned accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n+  auto layout = type.getEncoding();\n+  auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>();\n+  auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>();\n+  auto rank = type.getRank();\n+  auto sizePerThread = getSizePerThread(layout);\n+  auto accumSizePerThread = product<unsigned>(sizePerThread);\n+  auto llvmIndexTy = getTypeConverter()->getIndexType();\n+  SmallVector<unsigned> numCTAs(rank);\n+  auto shapePerCTA = getShapePerCTA(layout);\n+  for (unsigned d = 0; d < rank; ++d) {\n+    numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n+  }\n+  auto llvmElemTy = getTypeConverter()->convertType(type.getElementType());\n+  SmallVector<Value> multiDimOffsetFirstElem;\n+  SmallVector<Value> mmaColIdx(2);\n+  SmallVector<Value> mmaRowIdx(2);\n+  if (blockedLayout) {\n+    multiDimOffsetFirstElem = emitBaseIndexForBlockedLayout(\n+        loc, rewriter, blockedLayout, type.getShape());\n+  } else if (mmaLayout) {\n+    Value threadId = getThreadId(rewriter, loc);\n+    Value warpSize = idx_val(32);\n+    Value laneId = urem(threadId, warpSize);\n+    Value warpId = udiv(threadId, warpSize);\n+    // auto multiDimWarpId =\n+    //     delinearize(rewriter, loc, warpId, mmaLayout.getWarpsPerCTA());\n+    // TODO: double confirm if its document bug or DotConversion's Bug\n+    SmallVector<Value> multiDimWarpId(2);\n+    multiDimWarpId[0] = urem(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n+    multiDimWarpId[1] = udiv(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n+    Value four = idx_val(4);\n+    Value mmaGrpId = udiv(laneId, four);\n+    Value mmaGrpIdP8 = add(mmaGrpId, idx_val(8));\n+    Value mmaThreadIdInGrp = urem(laneId, four);\n+    Value mmaThreadIdInGrpM2 = mul(mmaThreadIdInGrp, idx_val(2));\n+    Value mmaThreadIdInGrpM2P1 = add(mmaThreadIdInGrpM2, idx_val(1));\n+    Value colWarpOffset = mul(multiDimWarpId[0], idx_val(16));\n+    mmaColIdx[0] = add(mmaGrpId, colWarpOffset);\n+    mmaColIdx[1] = add(mmaGrpIdP8, colWarpOffset);\n+    Value rowWarpOffset = mul(multiDimWarpId[1], idx_val(8));\n+    mmaRowIdx[0] = add(mmaThreadIdInGrpM2, rowWarpOffset);\n+    mmaRowIdx[1] = add(mmaThreadIdInGrpM2P1, rowWarpOffset);\n+  }\n+  for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n+    auto multiDimCTAInRepId = getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep);\n+    SmallVector<unsigned> multiDimCTAId(rank);\n+    for (auto it : llvm::enumerate(multiDimCTAInRepId)) {\n+      auto d = it.index();\n+      multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n     }\n-    auto llvmElemTy = getTypeConverter()->convertType(type.getElementType());\n-    auto multiDimOffsetFirstElem =\n-        emitBaseIndexForBlockedLayout(loc, rewriter, layout, type.getShape());\n-    for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n-      auto multiDimCTAInRepId =\n-          getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep);\n-      SmallVector<unsigned> multiDimCTAId(rank);\n-      for (auto it : llvm::enumerate(multiDimCTAInRepId)) {\n-        auto d = it.index();\n-        multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n-      }\n \n-      unsigned linearCTAId = getLinearIndex<unsigned>(multiDimCTAId, numCTAs);\n-      // TODO: This is actually redundant index calculation, we should\n-      //       consider of caching the index calculation result in case\n-      //       of performance issue observed.\n-      // for (unsigned elemId = linearCTAId * accumSizePerThread;\n-      //      elemId < (linearCTAId + 1) * accumSizePerThread; elemId += vec) {\n-      for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n-        auto multiDimElemId =\n-            getMultiDimIndex<unsigned>(elemId, layout.getSizePerThread());\n-        SmallVector<Value> multiDimOffset(rank);\n+    unsigned linearCTAId = getLinearIndex<unsigned>(multiDimCTAId, numCTAs);\n+    // TODO: This is actually redundant index calculation, we should\n+    //       consider of caching the index calculation result in case\n+    //       of performance issue observed.\n+    for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n+      SmallVector<Value> multiDimOffset(rank);\n+      if (blockedLayout) {\n+        SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n+            elemId, blockedLayout.getSizePerThread());\n         for (unsigned d = 0; d < rank; ++d) {\n-          multiDimOffset[d] = add(\n-              multiDimOffsetFirstElem[d],\n-              createIndexAttrConstant(rewriter, loc, llvmIndexTy,\n-                                      multiDimCTAInRepId[d] * shapePerCTA[d] +\n-                                          multiDimElemId[d]));\n+          multiDimOffset[d] =\n+              add(multiDimOffsetFirstElem[d],\n+                  idx_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n+                          multiDimElemId[d]));\n         }\n-        Value offset =\n-            linearize(rewriter, loc, reorder<Value>(multiDimOffset, outOrd),\n-                      reorder<unsigned>(paddedRepShape, outOrd));\n-        auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n-        Value ptr = gep(elemPtrTy, smemBase, offset);\n-        auto vecTy = VectorType::get(vec, llvmElemTy);\n-        ptr = bit_cast(LLVM::LLVMPointerType::get(vecTy, 3), ptr);\n-        if (stNotRd) {\n-          Value valVec = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n-          for (unsigned v = 0; v < vec; ++v) {\n-            Value vVal = createIndexAttrConstant(\n-                rewriter, loc, getTypeConverter()->getIndexType(), v);\n-            valVec = insert_element(\n-                vecTy, valVec,\n-                vals[elemId + linearCTAId * accumSizePerThread + v], vVal);\n-          }\n-          rewriter.create<LLVM::StoreOp>(loc, valVec, ptr);\n-        } else {\n-          Value valVec = rewriter.create<LLVM::LoadOp>(loc, ptr);\n-          for (unsigned v = 0; v < vec; ++v) {\n-            Value vVal = createIndexAttrConstant(\n-                rewriter, loc, getTypeConverter()->getIndexType(), v);\n-            vals[elemId + linearCTAId * accumSizePerThread + v] =\n-                extract_element(llvmElemTy, valVec, vVal);\n-          }\n+      } else if (mmaLayout) {\n+        assert(rank == 2);\n+        assert(mmaLayout.getVersion() == 2 &&\n+               \"mmaLayout ver1 not implemented yet\");\n+        multiDimOffset[0] = elemId < 2 ? mmaColIdx[0] : mmaColIdx[1];\n+        multiDimOffset[1] = elemId % 2 == 0 ? mmaRowIdx[0] : mmaRowIdx[1];\n+        multiDimOffset[0] = add(\n+            multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n+        multiDimOffset[1] = add(\n+            multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n+      } else {\n+        assert(0 && \"unexpected layout in processReplica\");\n+      }\n+      Value offset =\n+          linearize(rewriter, loc, reorder<Value>(multiDimOffset, outOrd),\n+                    reorder<unsigned>(paddedRepShape, outOrd));\n+      auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n+      Value ptr = gep(elemPtrTy, smemBase, offset);\n+      auto vecTy = vec_ty(llvmElemTy, vec);\n+      ptr = bitcast(ptr_ty(vecTy, 3), ptr);\n+      if (stNotRd) {\n+        Value valVec = undef(vecTy);\n+        for (unsigned v = 0; v < vec; ++v) {\n+          valVec = insert_element(\n+              vecTy, valVec,\n+              vals[elemId + linearCTAId * accumSizePerThread + v], idx_val(v));\n+        }\n+        store(valVec, ptr);\n+      } else {\n+        Value valVec = load(ptr);\n+        for (unsigned v = 0; v < vec; ++v) {\n+          vals[elemId + linearCTAId * accumSizePerThread + v] =\n+              extract_element(llvmElemTy, valVec, idx_val(v));\n         }\n       }\n     }\n   }\n+}\n+\n+LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n+    triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto loc = op.getLoc();\n+  Value src = op.src();\n+  Value dst = op.result();\n+  auto srcTy = src.getType().cast<RankedTensorType>();\n+  auto dstTy = dst.getType().cast<RankedTensorType>();\n+  Attribute srcLayout = srcTy.getEncoding();\n+  Attribute dstLayout = dstTy.getEncoding();\n+  auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n+  Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+  auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n+  smemBase = bitcast(elemPtrTy, smemBase);\n+  auto shape = dstTy.getShape();\n+  unsigned rank = dstTy.getRank();\n+  SmallVector<unsigned> numReplicates(rank);\n+  SmallVector<unsigned> inNumCTAsEachRep(rank);\n+  SmallVector<unsigned> outNumCTAsEachRep(rank);\n+  SmallVector<unsigned> inNumCTAs(rank);\n+  SmallVector<unsigned> outNumCTAs(rank);\n+  auto srcShapePerCTA = getShapePerCTA(srcLayout);\n+  auto dstShapePerCTA = getShapePerCTA(dstLayout);\n+  for (unsigned d = 0; d < rank; ++d) {\n+    unsigned inPerCTA = std::min<unsigned>(shape[d], srcShapePerCTA[d]);\n+    unsigned outPerCTA = std::min<unsigned>(shape[d], dstShapePerCTA[d]);\n+    unsigned maxPerCTA = std::max(inPerCTA, outPerCTA);\n+    numReplicates[d] = ceil<unsigned>(shape[d], maxPerCTA);\n+    inNumCTAsEachRep[d] = maxPerCTA / inPerCTA;\n+    outNumCTAsEachRep[d] = maxPerCTA / outPerCTA;\n+    assert(maxPerCTA % inPerCTA == 0 && maxPerCTA % outPerCTA == 0);\n+    inNumCTAs[d] = ceil<unsigned>(shape[d], inPerCTA);\n+    outNumCTAs[d] = ceil<unsigned>(shape[d], outPerCTA);\n+  }\n+  // Potentially we need to store for multiple CTAs in this replication\n+  unsigned accumNumReplicates = product<unsigned>(numReplicates);\n+  unsigned elems = getElemsPerThread(srcLayout, srcTy.getShape());\n+  auto vals = getElementsFromStruct(loc, adaptor.src(), elems, rewriter);\n+  unsigned inVec = 0;\n+  unsigned outVec = 0;\n+  auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n+\n+  unsigned outElems = getElemsPerThread(dstLayout, shape);\n+  auto outOrd = getOrder(dstLayout);\n+  SmallVector<Value> outVals(outElems);\n+\n+  for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n+    auto multiDimRepId = getMultiDimIndex<unsigned>(repId, numReplicates);\n+    barrier;\n+    if (srcLayout.isa<BlockedEncodingAttr>() ||\n+        srcLayout.isa<MmaEncodingAttr>()) {\n+      processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,\n+                     multiDimRepId, inVec, paddedRepShape, outOrd, vals,\n+                     smemBase);\n+    } else {\n+      assert(0 && \"ConvertLayout with input layout not implemented\");\n+      return failure();\n+    }\n+    barrier;\n+    if (dstLayout.isa<BlockedEncodingAttr>() ||\n+        dstLayout.isa<MmaEncodingAttr>()) {\n+      processReplica(loc, rewriter, /*stNotRd*/ false, dstTy, outNumCTAsEachRep,\n+                     multiDimRepId, outVec, paddedRepShape, outOrd, outVals,\n+                     smemBase);\n+    } else {\n+      assert(0 && \"ConvertLayout with output layout not implemented\");\n+      return failure();\n+    }\n+  }\n+\n+  SmallVector<Type> types(outElems, llvmElemTy);\n+  Type structTy = struct_ty(getContext(), types);\n+  Value result = getStructFromElements(loc, outVals, rewriter, structTy);\n+  rewriter.replaceOp(op, result);\n+\n+  return success();\n };\n \n+LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n+    triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto loc = op.getLoc();\n+  Value src = op.src();\n+  Value dst = op.result();\n+  auto srcTy = src.getType().cast<RankedTensorType>();\n+  auto dstTy = dst.getType().cast<RankedTensorType>();\n+  auto srcShape = srcTy.getShape();\n+  assert(srcShape.size() == 2 &&\n+         \"Unexpected rank of ConvertLayout(blocked->shared)\");\n+  auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto inOrd = srcBlockedLayout.getOrder();\n+  auto outOrd = dstSharedLayout.getOrder();\n+  unsigned inVec =\n+      inOrd == outOrd ? srcBlockedLayout.getSizePerThread()[inOrd[0]] : 1;\n+  unsigned outVec = dstSharedLayout.getVec();\n+  unsigned minVec = std::min(outVec, inVec);\n+  unsigned perPhase = dstSharedLayout.getPerPhase();\n+  unsigned maxPhase = dstSharedLayout.getMaxPhase();\n+  unsigned numElems = getElemsPerThread(srcBlockedLayout, srcShape);\n+  auto inVals = getElementsFromStruct(loc, adaptor.src(), numElems, rewriter);\n+  unsigned srcAccumSizeInThreads =\n+      product<unsigned>(srcBlockedLayout.getSizePerThread());\n+  auto elemTy = srcTy.getElementType();\n+  auto wordTy = vec_ty(elemTy, minVec);\n+\n+  // TODO: [goostavz] We should make a cache for the calculation of\n+  // emitBaseIndexForBlockedLayout in case backend compiler not being able to\n+  // optimize that\n+  SmallVector<Value> multiDimOffsetFirstElem =\n+      emitBaseIndexForBlockedLayout(loc, rewriter, srcBlockedLayout, srcShape);\n+  SmallVector<unsigned> srcShapePerCTA = getShapePerCTA(srcBlockedLayout);\n+  SmallVector<unsigned> reps{ceil<unsigned>(srcShape[0], srcShapePerCTA[0]),\n+                             ceil<unsigned>(srcShape[1], srcShapePerCTA[1])};\n+\n+  // Visit each input value in the order they are placed in inVals\n+  //\n+  // Please note that the order was not awaring of blockLayout.getOrder(),\n+  // thus the adjacent elems may not belong to a same word. This could be\n+  // improved if we update the elements order by emitIndicesForBlockedLayout()\n+  SmallVector<unsigned> wordsInEachRep(2);\n+  wordsInEachRep[0] = inOrd[0] == 0\n+                          ? srcBlockedLayout.getSizePerThread()[0] / minVec\n+                          : srcBlockedLayout.getSizePerThread()[0];\n+  wordsInEachRep[1] = inOrd[0] == 0\n+                          ? srcBlockedLayout.getSizePerThread()[1]\n+                          : srcBlockedLayout.getSizePerThread()[1] / minVec;\n+  Value outVecVal = idx_val(outVec);\n+  Value minVecVal = idx_val(minVec);\n+  Value smemBase = getSharedMemoryBase(loc, rewriter, dst);\n+  auto elemPtrTy = ptr_ty(getTypeConverter()->convertType(elemTy), 3);\n+  smemBase = bitcast(elemPtrTy, smemBase);\n+  unsigned numWordsEachRep = product<unsigned>(wordsInEachRep);\n+  SmallVector<Value> wordVecs(numWordsEachRep);\n+  for (unsigned i = 0; i < numElems; ++i) {\n+    if (i % srcAccumSizeInThreads == 0) {\n+      // start of a replication\n+      for (unsigned w = 0; w < numWordsEachRep; ++w) {\n+        wordVecs[w] = undef(wordTy);\n+      }\n+    }\n+    unsigned linearIdxInNanoTile = i % srcAccumSizeInThreads;\n+    auto multiDimIdxInNanoTile = getMultiDimIndex<unsigned>(\n+        linearIdxInNanoTile, srcBlockedLayout.getSizePerThread());\n+    multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n+    unsigned pos = multiDimIdxInNanoTile[inOrd[0]] % minVec;\n+    unsigned wordVecIdx =\n+        getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep);\n+    wordVecs[wordVecIdx] =\n+        insert_element(wordTy, wordVecs[wordVecIdx], inVals[i], idx_val(pos));\n+\n+    if (i % srcAccumSizeInThreads == srcAccumSizeInThreads - 1) {\n+      // end of replication, store the vectors into shared memory\n+      unsigned linearRepIdx = i / srcAccumSizeInThreads;\n+      auto multiDimRepIdx = getMultiDimIndex<unsigned>(linearRepIdx, reps);\n+      for (unsigned linearWordIdx = 0; linearWordIdx < numWordsEachRep;\n+           ++linearWordIdx) {\n+        // step 1: recover the multidim_index from the index of input_elements\n+        auto multiDimWordIdx =\n+            getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep);\n+        SmallVector<Value> multiDimIdx(2);\n+        auto wordOffset0 = multiDimRepIdx[0] * srcShapePerCTA[0] +\n+                           multiDimWordIdx[0] * (inOrd[0] == 0 ? minVec : 1);\n+        auto wordOffset1 = multiDimRepIdx[1] * srcShapePerCTA[1] +\n+                           multiDimWordIdx[1] * (inOrd[0] == 1 ? minVec : 1);\n+        multiDimIdx[0] = add(multiDimOffsetFirstElem[0], idx_val(wordOffset0));\n+        multiDimIdx[1] = add(multiDimOffsetFirstElem[1], idx_val(wordOffset1));\n+\n+        // step 2: do swizzling\n+        Value remained = urem(multiDimIdx[inOrd[0]], outVecVal);\n+        multiDimIdx[inOrd[0]] = udiv(multiDimIdx[inOrd[0]], outVecVal);\n+        Value off_1 = mul(multiDimIdx[inOrd[1]], idx_val(srcShape[inOrd[0]]));\n+        Value phaseId = udiv(multiDimIdx[inOrd[1]], idx_val(perPhase));\n+        phaseId = urem(phaseId, idx_val(maxPhase));\n+        Value off_0 = xor_(multiDimIdx[inOrd[0]], phaseId);\n+        off_0 = mul(off_0, outVecVal);\n+        remained = udiv(remained, minVecVal);\n+        off_0 = add(off_0, mul(remained, minVecVal));\n+        Value offset = add(off_1, off_0);\n+\n+        // step 3: store\n+        Value smemAddr = gep(elemPtrTy, smemBase, offset);\n+        smemAddr = bitcast(ptr_ty(wordTy, 3), smemAddr);\n+        store(wordVecs[linearWordIdx], smemAddr);\n+      }\n+    }\n+  }\n+  // TODO: double confirm if the Barrier is necessary here\n+  barrier;\n+  rewriter.replaceOp(op, smemBase);\n+  return success();\n+}\n /// ====================== dot codegen begin ==========================\n \n+// Data loader for mma.16816 instruction.\n class MMA16816SmemLoader {\n public:\n   MMA16816SmemLoader(int wpt, ArrayRef<uint32_t> order, int kOrder,\n                      ArrayRef<int64_t> tileShape, ArrayRef<int> instrShape,\n                      ArrayRef<int> matShape, int perPhase, int maxPhase,\n                      int elemBytes, ConversionPatternRewriter &rewriter,\n                      TypeConverter *typeConverter, const Location &loc)\n-      : wpt(wpt), order(order), kOrder(kOrder), tileShape(tileShape),\n-        instrShape(instrShape), matShape(matShape), perPhase(perPhase),\n+      : wpt(wpt), order(order.begin(), order.end()), kOrder(kOrder),\n+        tileShape(tileShape.begin(), tileShape.end()),\n+        instrShape(instrShape.begin(), instrShape.end()),\n+        matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n         maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter),\n         typeConverter(typeConverter), loc(loc), ctx(rewriter.getContext()) {\n     cMatShape = matShape[order[0]];\n@@ -1691,7 +1821,7 @@ class MMA16816SmemLoader {\n     loadStrideInMat[kOrder] =\n         2; // instrShape[kOrder] / matShape[kOrder], always 2\n     loadStrideInMat[kOrder ^ 1] =\n-        wpt * (instrShape[order[1]] / matShape[order[1]]);\n+        wpt * (instrShape[kOrder ^ 1] / matShape[kOrder ^ 1]);\n \n     pLoadStrideInMat = loadStrideInMat[order[0]];\n     sMatStride =\n@@ -1722,8 +1852,6 @@ class MMA16816SmemLoader {\n   // Compute the offset to the matrix this thread(indexed by warpOff and lane)\n   // mapped to.\n   SmallVector<Value> computeLdmatrixMatOffs(Value warpId, Value lane) {\n-    MLIRContext *ctx = warpId.getContext();\n-\n     // 4x4 matrices\n     Value c = urem(lane, i32_val(8));\n     Value s = udiv(lane, i32_val(8)); // sub-warp-id\n@@ -1864,16 +1992,19 @@ class MMA16816SmemLoader {\n     int k = matIdx[kOrder];\n \n     int ptrIdx{-1};\n+\n     if (canUseLdmatrix)\n       ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n-    else if (elemBytes == 4 && needTrans) // tf32 & trans\n+    else if (elemBytes == 4 && needTrans)\n       ptrIdx = matIdx[order[0]];\n     else if (elemBytes == 1 && needTrans)\n       ptrIdx = matIdx[order[0]] * 4;\n     else\n       llvm::report_fatal_error(\"unsupported mma type found\");\n \n-    // prefetch logic removed here.\n+    // The main difference with the original triton code is we removed the\n+    // prefetch-related logic here for the upstream optimizer phase should take\n+    // care with it, and that is transparent in dot conversion.\n     auto getPtr = [&](int idx) { return ptrs[idx]; };\n \n     Value ptr = getPtr(ptrIdx);\n@@ -1884,58 +2015,136 @@ class MMA16816SmemLoader {\n           matIdx[order[1]] * sMatStride * sMatShape * sTileStride * elemBytes;\n       PTXBuilder builder;\n \n-      auto resArgs = builder.newListOperand();\n-\n       // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a thread.\n-      for (int i = 0; i < 4; i++)\n-        resArgs->listAppend(builder.newOperand(\"=r\"));\n+      auto resArgs = builder.newListOperand(4, \"=r\");\n       auto addrArg = builder.newAddrOperand(ptr, \"r\", sOffset);\n \n       auto ldmatrix = builder.create(\"ldmatrix.sync.aligned.m8n8.x4\")\n                           ->o(\"trans\", needTrans /*predicate*/)\n                           .o(\"shared.b16\");\n       ldmatrix(resArgs, addrArg);\n \n-      auto inlineAsm = rewriter.create<LLVM::InlineAsmOp>(\n-          loc, ldmatrixRetTy, builder.getAllMLIRArgs(), // operands\n-          builder.dump(),                               // asm_string\n-          builder.getConstraints(),                     // constraints\n-          true,                                         // has_side_effects\n-          false,                                        // is_align_stack\n-          LLVM::AsmDialectAttr::get(ctx,\n-                                    LLVM::AsmDialect::AD_ATT), // asm_dialect\n-          ArrayAttr::get(ctx, {})                              // operand_attrs\n-      );\n+      // The result type is 4xi32, each i32 is composed of 2xf16\n+      // elements(adjacent two columns in a row)\n+      Value resV4 = builder.launch(rewriter, loc, ldmatrixRetTy);\n \n       auto getIntAttr = [&](int v) {\n-        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty(), v)});\n+        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n       };\n \n-      Value resV4 = inlineAsm.getRes(); // 4xi32, each is composed of 2xf16\n-                                        // elements(adjacent columns in a row)\n-\n-      Type fp16x2Ty = VectorType::get({2}, type::f16Ty(ctx));\n+      Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n \n-      return std::make_tuple(extract_val(fp16x2Ty, resV4, getIntAttr(0)),\n-                             extract_val(fp16x2Ty, resV4, getIntAttr(1)),\n-                             extract_val(fp16x2Ty, resV4, getIntAttr(2)),\n-                             extract_val(fp16x2Ty, resV4, getIntAttr(3)));\n+      return {extract_val(fp16x2Ty, resV4, getIntAttr(0)),\n+              extract_val(fp16x2Ty, resV4, getIntAttr(1)),\n+              extract_val(fp16x2Ty, resV4, getIntAttr(2)),\n+              extract_val(fp16x2Ty, resV4, getIntAttr(3))};\n     } else if (elemBytes == 4 &&\n                needTrans) { // Use lds.32 to load tf32 matrices\n-      assert(false && \"Not implemented yet\");\n+      Value ptr2 = getPtr(ptrIdx + 1);\n+      assert(sMatStride == 1);\n+      int sOffsetElem =\n+          matIdx[order[1]] * (sMatStride * sMatShape) * sTileStride;\n+      int sOffsetArrElem = 1 * (sMatStride * sMatShape) * sTileStride;\n+\n+      Value elems[4];\n+      Type elemTy = type::f32Ty(ctx);\n+      if (kOrder == 1) {\n+        elems[0] = load(gep(elemTy, ptr, i32_val(sOffsetElem)));\n+        elems[1] = load(gep(elemTy, ptr2, i32_val(sOffsetElem)));\n+        elems[2] =\n+            load(gep(elemTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+        elems[3] =\n+            load(gep(elemTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n+      } else {\n+        elems[0] = load(gep(elemTy, ptr, i32_val(sOffsetElem)));\n+        elems[2] = load(gep(elemTy, ptr2, i32_val(sOffsetElem)));\n+        elems[1] =\n+            load(gep(elemTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+        elems[3] =\n+            load(gep(elemTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n+      }\n+\n+      return {elems[0], elems[1], elems[2], elems[3]};\n     } else if (elemBytes == 1 && needTrans) {\n-      assert(false && \"Not implemented yet\");\n+      std::array<std::array<Value, 4>, 2> ptrs;\n+      ptrs[0] = {\n+          getPtr(ptrIdx),\n+          getPtr(ptrIdx + 1),\n+          getPtr(ptrIdx + 2),\n+          getPtr(ptrIdx + 3),\n+      };\n+\n+      ptrs[1] = {\n+          getPtr(ptrIdx + 4),\n+          getPtr(ptrIdx + 5),\n+          getPtr(ptrIdx + 6),\n+          getPtr(ptrIdx + 7),\n+      };\n+\n+      assert(sMatStride == 1);\n+      int sOffsetElem =\n+          matIdx[order[1]] * (sMatStride * sMatShape) * sTileStride;\n+      int sOffsetArrElem = 1 * (sMatStride * sMatShape) * sTileStride;\n+\n+      std::array<Value, 4> i8v4Elems;\n+      std::array<Value, 4> i32Elems;\n+      i8v4Elems.fill(\n+          rewriter.create<LLVM::UndefOp>(loc, vec_ty(type::i8Ty(ctx), 4)));\n+\n+      Value i8Elems[4][4];\n+      Type elemTy = type::i8Ty(ctx);\n+      if (kOrder == 1) {\n+        Value offset = i32_val(sOffsetElem);\n+\n+        for (int i = 0; i < 2; ++i)\n+          for (int j = 0; j < 4; ++j)\n+            i8Elems[i][j] = load(gep(elemTy, ptrs[i][j], offset));\n+\n+        offset = i32_val(sOffsetElem + sOffsetArrElem);\n+        for (int i = 2; i < 4; ++i)\n+          for (int j = 0; j < 4; ++j)\n+            i8Elems[i][j] = load(gep(elemTy, ptrs[i - 2][j], offset));\n+\n+        for (int m = 0; m < 4; ++m) {\n+          for (int e = 0; e < 4; ++e)\n+            i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n+                                          i8Elems[m][e], i32_val(e));\n+          i32Elems[m] = bitcast(i32_ty, i8v4Elems[m]);\n+        }\n+      } else { // k first\n+        Value offset = i32_val(sOffsetElem);\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[0][j] = load(gep(elemTy, ptrs[0][j], offset));\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[2][j] = load(gep(elemTy, ptrs[1][j], offset));\n+        offset = i32_val(sOffsetElem + sOffsetArrElem);\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[1][j] = load(gep(elemTy, ptrs[0][j], offset));\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[3][j] = load(gep(elemTy, ptrs[1][j], offset));\n+\n+        for (int m = 0; m < 4; ++m) {\n+          for (int e = 0; e < 4; ++e)\n+            i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n+                                          i8Elems[m][e], i32_val(e));\n+          i32Elems[m] = bitcast(i32_ty, i8v4Elems[m]);\n+        }\n+      }\n+\n+      return {i32Elems[0], i32Elems[1], i32Elems[2], i32Elems[3]};\n     }\n-    return std::make_tuple(Value{}, Value{}, Value{}, Value{});\n+\n+    assert(false && \"Invalid smem load\");\n+    return {Value{}, Value{}, Value{}, Value{}};\n   }\n \n private:\n   int wpt;\n-  ArrayRef<uint32_t> order;\n+  SmallVector<uint32_t> order;\n   int kOrder;\n-  ArrayRef<int64_t> tileShape;\n-  ArrayRef<int> instrShape;\n-  ArrayRef<int> matShape;\n+  SmallVector<int64_t> tileShape;\n+  SmallVector<int> instrShape;\n+  SmallVector<int> matShape;\n   int perPhase;\n   int maxPhase;\n   int elemBytes;\n@@ -2070,10 +2279,6 @@ struct DotOpConversionHelper {\n                     .cast<RankedTensorType>()\n                     .getEncoding()\n                     .cast<MmaEncodingAttr>();\n-\n-    ATensorTy = A.getType().cast<RankedTensorType>();\n-    BTensorTy = B.getType().cast<RankedTensorType>();\n-    DTensorTy = D.getType().cast<RankedTensorType>();\n   }\n \n   // Load SplatLike C which contains a constVal. It simply returns 4 fp32\n@@ -2126,8 +2331,8 @@ struct DotOpConversionHelper {\n   // The type of a matrix that loaded by either a ldmatrix or composed lds.\n   Type getMatType() const {\n     Type fp32Ty = type::f32Ty(ctx);\n-    Type fp16x2Ty = VectorType::get({2}, type::f16Ty(ctx));\n-    Type bf16x2Ty = VectorType::get({2}, type::bf16Ty(ctx));\n+    Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n+    Type bf16x2Ty = vec_ty(type::bf16Ty(ctx), 2);\n     // floating point types\n     Type fp16x2Pack4Ty =\n         LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n@@ -2136,7 +2341,7 @@ struct DotOpConversionHelper {\n     Type fp32Pack4Ty =\n         LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n     // integer types\n-    Type i8x4Ty = VectorType::get({4}, type::i8Ty(ctx));\n+    Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n     Type i8x4Pack4Ty =\n         LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i8x4Ty));\n     Type i32Pack4Ty = LLVM::LLVMStructType::getLiteral(\n@@ -2158,6 +2363,23 @@ struct DotOpConversionHelper {\n     return Type{};\n   }\n \n+  Type getLoadElemTy() {\n+    switch (mmaType) {\n+    case TensorCoreType::FP32_FP16_FP16_FP32:\n+      return vec_ty(type::f16Ty(ctx), 2);\n+    case TensorCoreType::FP32_BF16_BF16_FP32:\n+      return vec_ty(type::bf16Ty(ctx), 2);\n+    case TensorCoreType::FP32_TF32_TF32_FP32:\n+      return type::f32Ty(ctx);\n+    case TensorCoreType::INT32_INT8_INT8_INT32:\n+      return type::i32Ty(ctx);\n+    default:\n+      llvm::report_fatal_error(\"Unsupported mma type found\");\n+    }\n+\n+    return Type{};\n+  }\n+\n   Type getMmaRetType() const {\n     Type fp32Ty = type::f32Ty(ctx);\n     Type i32Ty = type::i32Ty(ctx);\n@@ -2344,9 +2566,10 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n   const int numRepN = std::max<int>(dShape[1] / (wpt[1] * mmaInstrN), 1);\n   const int numRepK = std::max<int>(NK / mmaInstrK, 1);\n \n-  Value head = getThreadId(rewriter, loc);\n-  Value lane = urem(head, i32_val(32));\n-  Value warp = udiv(head, i32_val(32));\n+  Value _32 = i32_val(32);\n+  Value thread = getThreadId(rewriter, loc);\n+  Value lane = urem(thread, _32);\n+  Value warp = udiv(thread, _32);\n   Value warpMN = udiv(warp, i32_val(wpt[0]));\n   Value warpM = urem(warp, i32_val(wpt[0]));\n   Value warpN = urem(warpMN, i32_val(wpt[1]));\n@@ -2358,14 +2581,14 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n   std::map<std::pair<unsigned, unsigned>, Value> hb;\n \n   // the original register_lds2, but discard the prefetch logic.\n-  auto ld2 = [&](decltype(ha) &vals, int mn, int k, Value val) {\n+  auto ld2 = [](decltype(ha) &vals, int mn, int k, Value val) {\n     vals[{mn, k}] = val;\n   };\n \n   // Load A or B matrix.\n   auto getLoadMatrixFn =\n-      [&](Value tensor, int wpt, int kOrder, ArrayRef<int> instrShape,\n-          ArrayRef<int> matShape, Value warpId,\n+      [&](Value tensor, Value llTensor, int wpt, int kOrder,\n+          ArrayRef<int> instrShape, ArrayRef<int> matShape, Value warpId,\n           decltype(ha) &vals) -> std::function<void(int, int)> {\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n     // We assumes that the input operand of Dot should be from shared layout.\n@@ -2374,6 +2597,7 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n     const int perPhase = sharedLayout.getPerPhase();\n     const int maxPhase = sharedLayout.getMaxPhase();\n     const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n+    auto order = sharedLayout.getOrder();\n \n     MMA16816SmemLoader loader(wpt, sharedLayout.getOrder(), kOrder,\n                               tensorTy.getShape() /*tileShape*/, instrShape,\n@@ -2385,101 +2609,104 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n     SmallVector<Value> ptrs(numPtrs);\n \n     Type smemPtrTy = helper.getShemPtrTy();\n-    auto smemBase = getSharedMemoryBase(loc, rewriter, tensor);\n-    for (int i = 0; i < numPtrs; i++) {\n-      ptrs[i] = bit_cast(\n-          smemPtrTy, gep(smemBase.getType(), smemBase, ValueRange({offs[i]})));\n+    for (int i = 0; i < numPtrs; ++i) {\n+      ptrs[i] =\n+          bitcast(smemPtrTy, gep(smemPtrTy, llTensor, ValueRange({offs[i]})));\n     }\n \n+    bool needTrans = kOrder != order[0];\n+\n     // (a, b) is the coordinate.\n-    auto load = [&, loader, ptrs, offs](int a, int b) {\n+    auto load = [=, &vals, &helper, &ld2](int a, int b) {\n       auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n           (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n           ptrs, helper.getMatType(), helper.getShemPtrTy());\n-      ld2(vals, a, b, ha0);\n-      ld2(vals, a + 1, b, ha1);\n-      ld2(vals, a, b + 1, ha2);\n-      ld2(vals, a + 1, b + 1, ha3);\n+      if (!needTrans) {\n+        ld2(vals, a, b, ha0);\n+        ld2(vals, a + 1, b, ha1);\n+        ld2(vals, a, b + 1, ha2);\n+        ld2(vals, a + 1, b + 1, ha3);\n+      } else {\n+        ld2(vals, a, b, ha0);\n+        ld2(vals, a + 1, b, ha2);\n+        ld2(vals, a, b + 1, ha1);\n+        ld2(vals, a + 1, b + 1, ha3);\n+      }\n     };\n \n     return load;\n   };\n \n-  std::function<void(int, int)> loadA = getLoadMatrixFn(\n-      A, mmaLayout.getWarpsPerCTA()[0] /*wpt*/, 1 /*kOrder*/,\n-      {mmaInstrM, mmaInstrK} /*instrShpae*/,\n-      {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/);\n+  std::function<void(int, int)> loadA;\n+  if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n+    // load from smem\n+    loadA = getLoadMatrixFn(\n+        A, adapter.a() /*llTensor*/, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n+        1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShpae*/,\n+        {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/);\n+  } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n+    // load from registers, used in gemm fuse\n+    // TODO(Superjomn) Port the logic.\n+    assert(false && \"Loading A from register is not supported yet.\");\n+  } else {\n+    assert(false && \"A's layout is not supported.\");\n+  }\n+\n   std::function<void(int, int)> loadB = getLoadMatrixFn(\n-      B, mmaLayout.getWarpsPerCTA()[1] /*wpt*/, 0 /*kOrder*/,\n-      {mmaInstrK, mmaInstrN} /*instrShpae*/,\n+      B, adapter.b() /*llTensor*/, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n+      0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShpae*/,\n       {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/);\n \n-  const unsigned mStride = numRepN * 2;\n-  SmallVector<Value> fc(numRepM * mStride + numRepN * 2);\n+  const int fcSize = 4 * numRepM * numRepN;\n+  SmallVector<Value> fc(fcSize);\n+\n+  // Currently, we only support a SplatLike C. For the other cases, e.g., C in\n+  // shared layout or blocked layout, we will support them by expanding\n+  // convert_layout.\n+  auto hc = helper.loadSplatLikeC(C, loc, rewriter);\n+  assert(hc.size() == 4UL && \"Only splat-like C is supported now\");\n+  for (int i = 0; i < fc.size(); i++)\n+    fc[i] = hc[0];\n+\n   auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n+    unsigned colsPerThread = numRepN * 2;\n     PTXBuilder builder;\n-\n     auto &mma = *builder.create(helper.getMmaInstr().str());\n+    auto retArgs = builder.newListOperand(4, \"=r\");\n+    auto aArgs = builder.newListOperand({\n+        {ha[{m, k}], \"r\"},\n+        {ha[{m + 1, k}], \"r\"},\n+        {ha[{m, k + 1}], \"r\"},\n+        {ha[{m + 1, k + 1}], \"r\"},\n+    });\n+    auto bArgs =\n+        builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n+    auto cArgs = builder.newListOperand();\n+    for (int i = 0; i < 4; ++i) {\n+      cArgs->listAppend(builder.newOperand(fc[m * colsPerThread + 4 * n + i],\n+                                           std::to_string(i)));\n+      // reuse the output registers\n+    }\n+    mma(retArgs, aArgs, bArgs, cArgs);\n+    Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n \n-    auto retArgs = builder.newListOperand();\n-    for (int i = 0; i < 4; ++i)\n-      retArgs->listAppend(builder.newOperand(\"=r\"));\n-    auto aArg0 = builder.newOperand(ha[{m, k}], \"r\");\n-    auto aArg1 = builder.newOperand(ha[{m + 1, k}], \"r\");\n-    auto aArg2 = builder.newOperand(ha[{m, k + 1}], \"r\");\n-    auto aArg3 = builder.newOperand(ha[{m + 1, k}], \"r\");\n-\n-    auto bArg0 = builder.newOperand(ha[{n, k}], \"r\");\n-    auto bArg1 = builder.newOperand(ha[{n, k + 1}], \"r\");\n-\n-    // Currently, we only support a SplatLike C. For the other cases, e.g., C in\n-    // shared layout or blocked layout, we will support them by expanding\n-    // convert_layout.\n-    auto hc = helper.loadSplatLikeC(C, loc, rewriter);\n-    assert(hc.size() == 4UL && \"Only splat-like C is supported now\");\n-    auto cArg0 = builder.newOperand(hc[0], \"0\"); // reuse the output registers\n-    auto cArg1 = builder.newOperand(hc[1], \"1\");\n-    auto cArg2 = builder.newOperand(hc[2], \"2\");\n-    auto cArg3 = builder.newOperand(hc[3], \"3\");\n-\n-    mma({retArgs, aArg0, aArg1, aArg2, aArg3, bArg0, bArg1, cArg0, cArg1, cArg2,\n-         cArg3});\n-\n-    auto inlineAsm = rewriter.create<LLVM::InlineAsmOp>(\n-        loc, helper.getMmaRetType(), builder.getAllMLIRArgs(), // operands\n-        builder.dump(),                                        // asm_string\n-        builder.getConstraints(),                              // constraints\n-        true,  // has_side_effects\n-        false, // is_align_stack\n-        LLVM::AsmDialectAttr::get(ctx,\n-                                  LLVM::AsmDialect::AD_ATT), // asm_dialect\n-        ArrayAttr::get(ctx, {})                              // operand_attrs\n-    );\n-\n-    auto mmaOut = inlineAsm.getRes();\n     auto getIntAttr = [&](int v) {\n-      return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty(), v)});\n+      return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n     };\n \n-    fc[(m + 0) * mStride + (n * 2 + 0)] =\n-        extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(0));\n-    fc[(m + 0) * mStride + (n * 2 + 1)] =\n-        extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(1));\n-    fc[(m + 1) * mStride + (n * 2 + 0)] =\n-        extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(2));\n-    fc[(m + 1) * mStride + (n * 2 + 1)] =\n-        extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(3));\n+    for (int i = 0; i < 4; i++)\n+      fc[m * colsPerThread + 4 * n + i] =\n+          extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(i));\n   };\n \n   // Main program\n-\n-  for (unsigned k = 0; k < numRepK; k++) {\n-    for (unsigned m = 0; m < numRepM; m++)\n+  for (unsigned k = 0; k < numRepK; ++k) {\n+    for (unsigned m = 0; m < numRepM; ++m)\n       loadA(2 * m, 2 * k);\n     for (unsigned n = 0; n < numRepN; n += 2)\n       loadB(n, 2 * k);\n-    for (unsigned m = 0; m < numRepM; m++)\n-      for (unsigned n = 0; n < numRepN; n++) {\n+    for (unsigned m = 0; m < numRepM; ++m)\n+      for (unsigned n = 0; n < numRepN; ++n) {\n         callMma(2 * m, n, 2 * k);\n       }\n   }\n@@ -2517,16 +2744,14 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n \n   llvm::Optional<Type> convertTritonTensorType(RankedTensorType type) {\n     Attribute layout = type.getEncoding();\n-    if (layout && (layout.isa<BlockedEncodingAttr>() ||\n-                   layout.isa<SliceEncodingAttr>())) {\n+    if (layout &&\n+        (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n+         layout.isa<MmaEncodingAttr>())) {\n       unsigned numElementsPerThread =\n           getElemsPerThread(layout, type.getShape());\n       SmallVector<Type, 4> types(numElementsPerThread,\n                                  convertType(type.getElementType()));\n       return LLVM::LLVMStructType::getLiteral(&getContext(), types);\n-    } else if (auto mma_layout = layout.dyn_cast_or_null<MmaEncodingAttr>()) {\n-      // TODO: Not implemented\n-      return type;\n     } else if (auto shared_layout =\n                    layout.dyn_cast_or_null<SharedEncodingAttr>()) {\n       return LLVM::LLVMPointerType::get(convertType(type.getElementType()), 3);\n@@ -2535,6 +2760,229 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n   }\n };\n \n+struct AsyncWaitOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::AsyncWaitOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::gpu::AsyncWaitOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::gpu::AsyncWaitOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    PTXBuilder ptxBuilder;\n+    auto &asyncWaitOp = *ptxBuilder.create<PTXCpAsyncWaitGroupInstr>();\n+    auto num = op->getAttrOfType<IntegerAttr>(\"num\").getInt();\n+    asyncWaitOp(ptxBuilder.newConstantOperand(num));\n+\n+    auto ctx = op.getContext();\n+    auto loc = op.getLoc();\n+    auto voidTy = LLVM::LLVMVoidType::get(ctx);\n+    auto ret = ptxBuilder.launch(rewriter, loc, voidTy);\n+\n+    // Safe to remove the op since it doesn't have any return value.\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+};\n+\n+struct InsertSliceAsyncOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::InsertSliceAsyncOp>,\n+      public LoadStoreConversionBase {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::gpu::InsertSliceAsyncOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  InsertSliceAsyncOpConversion(LLVMTypeConverter &converter,\n+                               const Allocation *allocation, Value smem,\n+                               AxisInfoAnalysis &axisAnalysisPass,\n+                               PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::gpu::InsertSliceAsyncOp>(\n+            converter, allocation, smem, benefit),\n+        LoadStoreConversionBase(axisAnalysisPass) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::gpu::InsertSliceAsyncOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    // insert_slice_async %src, %dst, %index, %mask, %other\n+    auto loc = op.getLoc();\n+    Value src = op.src();\n+    Value dst = op.dst();\n+    Value res = op.result();\n+    Value mask = op.mask();\n+    Value other = op.other();\n+    assert(allocation->getBufferId(res) == Allocation::InvalidBufferId &&\n+           \"Only support in-place insert_slice_async for now\");\n+\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto resTy = dst.getType().cast<RankedTensorType>();\n+    auto resElemTy = resTy.getElementType();\n+    auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+    auto resSharedLayout = resTy.getEncoding().cast<SharedEncodingAttr>();\n+    auto srcShape = srcTy.getShape();\n+    assert(srcShape.size() == 2 &&\n+           \"insert_slice_async: Unexpected rank of %src\");\n+\n+    Value llDst = adaptor.dst();\n+    Value llSrc = adaptor.src();\n+    Value llMask = adaptor.mask();\n+    Value llOther = adaptor.other();\n+    Value llIndex = adaptor.index();\n+\n+    // %src\n+    auto srcElems = getLLVMElems(src, llSrc, srcBlockedLayout, rewriter, loc);\n+\n+    // %dst\n+    auto axis = op->getAttrOfType<IntegerAttr>(\"axis\").getInt();\n+    assert(axis == 0 && \"insert_slice_async: Only axis=0 is supported for now\");\n+    auto dstBase = createIndexAttrConstant(rewriter, loc,\n+                                           getTypeConverter()->getIndexType(),\n+                                           product<int64_t>(resTy.getShape()));\n+    Value offset = mul(llIndex, dstBase);\n+    auto dstPtrTy = LLVM::LLVMPointerType::get(\n+        getTypeConverter()->convertType(resTy.getElementType()), 3);\n+    Value dstPtrBase = gep(dstPtrTy, llDst, offset);\n+\n+    // %mask\n+    SmallVector<Value> maskElems;\n+    if (llMask) {\n+      maskElems = getLLVMElems(mask, llMask, srcBlockedLayout, rewriter, loc);\n+      assert(srcElems.size() == maskElems.size());\n+    }\n+\n+    // %other\n+    SmallVector<Value> otherElems;\n+    if (llOther) {\n+      // TODO(Keren): support \"other\" tensor.\n+      // It's not necessary for now because the pipeline pass will skip\n+      // generating insert_slice_async if the load op has any \"other\" tensor.\n+      assert(false && \"insert_slice_async: Other value not supported yet\");\n+      otherElems =\n+          getLLVMElems(other, llOther, srcBlockedLayout, rewriter, loc);\n+      assert(srcElems.size() == otherElems.size());\n+    }\n+\n+    unsigned inVec = getVectorizeSize(src, srcBlockedLayout);\n+    unsigned outVec = resSharedLayout.getVec();\n+    unsigned minVec = std::min(outVec, inVec);\n+    unsigned numElems = getElemsPerThread(srcBlockedLayout, srcShape);\n+    unsigned perPhase = resSharedLayout.getPerPhase();\n+    unsigned maxPhase = resSharedLayout.getMaxPhase();\n+    auto sizePerThread = srcBlockedLayout.getSizePerThread();\n+    auto threadsPerWarp = srcBlockedLayout.getThreadsPerWarp();\n+    auto warpsPerCTA = srcBlockedLayout.getWarpsPerCTA();\n+    auto threadsPerCTA = getThreadsPerCTA(srcBlockedLayout);\n+\n+    auto inOrder = srcBlockedLayout.getOrder();\n+    auto outOrder = resSharedLayout.getOrder();\n+    // If perPhase * maxPhase > threadsPerCTA, we need to swizzle over elements\n+    // across phases.\n+    // If perPhase * maxPhase == threadsPerCTA, swizzle is not allowd\n+    auto numSwizzleRows = std::max<unsigned>(\n+        (perPhase * maxPhase) / threadsPerCTA[inOrder[1]], 1);\n+    // A sharedLayout encoding has a \"vec\" parameter.\n+    // On the column dimension, if inVec > outVec, it means we have to divide\n+    // single vector read into multiple ones\n+    auto numVecCols = std::max<unsigned>(inVec / outVec, 1);\n+\n+    auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcShape);\n+    // <<tileVecIdxRow, tileVecIdxCol>, TileOffset>\n+    DenseMap<std::pair<unsigned, unsigned>, Value> tileOffsetMap;\n+    for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n+      // minVec = 2, inVec = 4, outVec = 2\n+      //   baseOffsetCol = 0   baseOffsetCol = 0\n+      //   tileVecIdxCol = 0   tileVecIdxCol = 1\n+      //                -/\\-   -/\\-\n+      //               [|x x| |x x| x x x x x]\n+      //               [|x x| |x x| x x x x x]\n+      // baseOffsetRow [|x x| |x x| x x x x x]\n+      //               [|x x| |x x| x x x x x]\n+      auto vecIdx = elemIdx / minVec;\n+      auto vecIdxCol = vecIdx % (sizePerThread[inOrder[0]] / minVec);\n+      auto vecIdxRow = vecIdx / (sizePerThread[inOrder[0]] / minVec);\n+      auto baseOffsetCol =\n+          vecIdxCol / numVecCols * numVecCols * threadsPerCTA[inOrder[0]];\n+      auto baseOffsetRow = vecIdxRow / numSwizzleRows * numSwizzleRows *\n+                           threadsPerCTA[inOrder[1]];\n+      auto baseOffset = (baseOffsetRow * srcShape[inOrder[0]] + baseOffsetCol);\n+      auto tileVecIdxCol = vecIdxCol % numVecCols;\n+      auto tileVecIdxRow = vecIdxRow % numSwizzleRows;\n+\n+      if (!tileOffsetMap.count({tileVecIdxRow, tileVecIdxCol})) {\n+        // Swizzling\n+        // Since the swizzling index is related to outVec, and we know minVec\n+        // already, inVec doesn't matter\n+        //\n+        // (Numbers represent row indices)\n+        // Example1:\n+        // outVec = 2, inVec = 2, minVec = 2\n+        // outVec = 2, inVec = 4, minVec = 2\n+        //     | [1 2] [3 4]  ... [15 16] |\n+        //     | [3 4] [5 6]  ... [1 2]   |\n+        // Example2:\n+        // outVec = 4, inVec = 2, minVec = 2\n+        //     | [1 2 3 4] [5 6 7 8] ... [13 14 15 16] |\n+        //     | [5 6 7 8] [9 10 11 12] ... [1 2 3 4]  |\n+        auto srcIdx = srcIndices[tileVecIdxRow * sizePerThread[inOrder[0]]];\n+        Value phase = urem(udiv(srcIdx[inOrder[1]], i32_val(perPhase)),\n+                           i32_val(maxPhase));\n+        Value rowOffset =\n+            mul(srcIdx[inOrder[1]], i32_val(srcShape[inOrder[0]]));\n+        Value colOffset =\n+            add(srcIdx[inOrder[0]], i32_val(tileVecIdxCol * minVec));\n+        Value swizzleIdx = udiv(colOffset, i32_val(outVec));\n+        Value swizzleColOffset =\n+            add(mul(xor_(swizzleIdx, phase), i32_val(outVec)),\n+                urem(colOffset, i32_val(outVec)));\n+        Value tileOffset = add(rowOffset, swizzleColOffset);\n+        tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}] =\n+            gep(dstPtrTy, dstPtrBase, tileOffset);\n+      }\n+\n+      // 16 * 8 = 128bits\n+      auto maxBitWidth =\n+          std::max<unsigned>(128, resElemTy.getIntOrFloatBitWidth());\n+      auto vecBitWidth = resElemTy.getIntOrFloatBitWidth() * minVec;\n+      auto bitWidth = std::min<unsigned>(maxBitWidth, vecBitWidth);\n+      auto numWords = vecBitWidth / bitWidth;\n+      auto numWordElems = bitWidth / resElemTy.getIntOrFloatBitWidth();\n+\n+      // XXX(Keren): Tune CG and CA here.\n+      CacheModifier srcCacheModifier =\n+          bitWidth == 128 ? CacheModifier::CG : CacheModifier::CA;\n+      assert(bitWidth == 128 || bitWidth == 64 || bitWidth == 32);\n+\n+      for (int wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n+        PTXBuilder ptxBuilder;\n+        auto &copyAsyncOp = *ptxBuilder.create<PTXCpAsyncLoadInstr>(\n+            srcCacheModifier, op.evict());\n+\n+        auto tileOffset = tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}];\n+        auto *dstOperand =\n+            ptxBuilder.newAddrOperand(tileOffset, \"r\", baseOffset);\n+        auto *srcOperand = ptxBuilder.newAddrOperand(srcElems[vecIdx], \"l\");\n+        auto *copySize = ptxBuilder.newConstantOperand(bitWidth);\n+        auto *srcSize = copySize;\n+        if (op.mask()) {\n+          // We don't use predicate in this case, setting src-size to 0\n+          // if there's any mask. cp.async will automatically fill the\n+          // remaining slots with 0 if cp-size > src-size.\n+          // XXX(Keren): Always assume other = 0 for now.\n+          auto selectOp = select(maskElems[vecIdx + wordIdx * numWordElems],\n+                                 i32_val(bitWidth), i32_val(0));\n+          srcSize = ptxBuilder.newOperand(selectOp, \"r\");\n+        }\n+        copyAsyncOp(dstOperand, srcOperand, copySize, srcSize);\n+        ptxBuilder.launch(rewriter, loc, LLVM::LLVMVoidType::get(getContext()));\n+      }\n+    }\n+\n+    PTXBuilder ptxBuilder;\n+    ptxBuilder.create<PTXCpAsyncCommitGroupInstr>()->operator()();\n+    auto ret =\n+        ptxBuilder.launch(rewriter, loc, LLVM::LLVMVoidType::get(getContext()));\n+    rewriter.replaceOp(op, ret);\n+    return success();\n+  }\n+};\n+\n void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n                                   AxisInfoAnalysis &axisInfoAnalysis,\n@@ -2544,6 +2992,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<AllocTensorOpConversion>(typeConverter, allocation, smem,\n                                         benefit);\n   patterns.add<ArithConstantSplatOpConversion>(typeConverter, benefit);\n+  patterns.add<AsyncWaitOpConversion>(typeConverter, benefit);\n   patterns.add<BinaryOpConversion<arith::AddIOp, LLVM::AddOp>>(typeConverter,\n                                                                benefit);\n   patterns.add<BinaryOpConversion<arith::AddFOp, LLVM::FAddOp>>(typeConverter,\n@@ -2558,6 +3007,8 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<ExtractSliceOpConversion>(typeConverter, allocation, smem,\n                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n+  patterns.add<InsertSliceAsyncOpConversion>(typeConverter, allocation, smem,\n+                                             axisInfoAnalysis, benefit);\n   patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<MakeRangeOpConversion>(typeConverter, benefit);\n   patterns.add<ReturnOpConversion>(typeConverter, benefit);\n@@ -2654,6 +3105,9 @@ void ConvertTritonGPUToLLVM::initSharedMemory(\n          \"Inliner pass is expected before TritonGPUToLLVM\");\n   b.setInsertionPointToStart(&funcs[0].getBody().front());\n   smem = b.create<LLVM::AddressOfOp>(loc, global);\n+  auto ptrTy =\n+      LLVM::LLVMPointerType::get(typeConverter.convertType(b.getI8Type()), 3);\n+  smem = b.create<LLVM::BitcastOp>(loc, ptrTy, smem);\n }\n \n } // namespace"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 70, "deletions": 15, "changes": 85, "file_content_changes": "@@ -58,14 +58,65 @@ unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape) {\n   }\n }\n \n-unsigned getShapePerCTA(const Attribute &layout, unsigned d) {\n+SmallVector<unsigned> getSizePerThread(Attribute layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-    return blockedLayout.getSizePerThread()[d] *\n-           blockedLayout.getThreadsPerWarp()[d] *\n-           blockedLayout.getWarpsPerCTA()[d];\n+    return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n+                                 blockedLayout.getSizePerThread().end());\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    assert(mmaLayout.getVersion() == 2 &&\n+           \"mmaLayout version = 1 is not implemented yet\");\n+    return SmallVector<unsigned>{2, 2};\n+  } else {\n+    assert(0 && \"getSizePerThread not implemented\");\n+    return {};\n+  }\n+}\n+\n+SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout) {\n+  SmallVector<unsigned> threads;\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    for (int d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n+      threads.push_back(blockedLayout.getThreadsPerWarp()[d] *\n+                        blockedLayout.getWarpsPerCTA()[d]);\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    assert(0 && \"Unimplemented usage of MmaEncodingAttr\");\n+  } else {\n+    assert(0 && \"Unimplemented usage of getShapePerCTA\");\n+  }\n+\n+  return threads;\n+}\n+\n+SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n+  SmallVector<unsigned> shape;\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    for (int d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n+      shape.push_back(blockedLayout.getSizePerThread()[d] *\n+                      blockedLayout.getThreadsPerWarp()[d] *\n+                      blockedLayout.getWarpsPerCTA()[d]);\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    assert(mmaLayout.getVersion() == 2 &&\n+           \"mmaLayout version = 1 is not implemented yet\");\n+    return {16 * mmaLayout.getWarpsPerCTA()[0],\n+            8 * mmaLayout.getWarpsPerCTA()[1]};\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n-    return 0;\n+  }\n+  return shape;\n+}\n+\n+SmallVector<unsigned> getOrder(const Attribute &layout) {\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    return SmallVector<unsigned>(blockedLayout.getOrder().begin(),\n+                                 blockedLayout.getOrder().end());\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    return SmallVector<unsigned>{1, 0};\n+  } else if (auto sharedLayout = layout.dyn_cast<SharedEncodingAttr>()) {\n+    return SmallVector<unsigned>(sharedLayout.getOrder().begin(),\n+                                 sharedLayout.getOrder().end());\n+  } else {\n+    assert(0 && \"Unimplemented usage of getOrder\");\n+    return {};\n   }\n };\n \n@@ -141,16 +192,17 @@ SliceEncodingAttr BlockedEncodingAttr::squeeze(int axis) {\n \n unsigned BlockedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   size_t rank = shape.size();\n-  assert(rank == getSizePerThread().size() &&\n+  auto sizePerThread = getSizePerThread();\n+  auto warpsPerCTA = getWarpsPerCTA();\n+  auto threadsPerWarp = getThreadsPerWarp();\n+  assert(rank == sizePerThread.size() &&\n          \"unexpected rank in BlockedEncodingAttr::getElemsPerThread\");\n-  SmallVector<unsigned> elemsPerThreadPerDim(rank);\n+  SmallVector<unsigned> elemsPerThread(rank);\n   for (size_t i = 0; i < rank; ++i) {\n-    unsigned t =\n-        getSizePerThread()[i] * getThreadsPerWarp()[i] * getWarpsPerCTA()[i];\n-    elemsPerThreadPerDim[i] =\n-        ceil<unsigned>(shape[i], t) * getSizePerThread()[i];\n+    unsigned t = sizePerThread[i] * threadsPerWarp[i] * warpsPerCTA[i];\n+    elemsPerThread[i] = ceil<unsigned>(shape[i], t) * sizePerThread[i];\n   }\n-  return product<unsigned>(elemsPerThreadPerDim);\n+  return product<unsigned>(elemsPerThread);\n }\n \n unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n@@ -177,9 +229,12 @@ unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n }\n \n unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n-  int threads = product(getWarpsPerCTA());\n-  int numElem = product(shape);\n-  return numElem / threads;\n+  size_t rank = shape.size();\n+  assert(rank == 2 && \"Unexpected rank of mma layout\");\n+  assert(getVersion() == 2 && \"mmaLayout version = 1 is not implemented yet\");\n+  unsigned elemsCol = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]) * 2;\n+  unsigned elemsRow = ceil<unsigned>(shape[1], 8 * getWarpsPerCTA()[1]) * 2;\n+  return elemsCol * elemsRow;\n }\n \n unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -120,8 +120,8 @@ class PullConversionToSource : public mlir::RewritePattern {\n \n     auto blacklist = [](Operation *op) {\n       if (isa<triton::gpu::ExtractSliceOp, triton::gpu::AllocTensorOp,\n-              triton::gpu::InsertSliceAsyncOp, triton::LoadOp, triton::StoreOp>(\n-              op))\n+              triton::gpu::InsertSliceAsyncOp, triton::LoadOp, triton::StoreOp,\n+              triton::DotOp>(op))\n         return true;\n       if (isa<scf::YieldOp, scf::ForOp>(op))\n         return true;\n@@ -306,6 +306,8 @@ class MoveArgConvertOutOfLoop : public mlir::RewritePattern {\n         auto fwdCvtIt = std::find_if(opIt, fwdEndIt, isCvt);\n         auto bwdCvtIt = std::find_if(bwdBeginIt, opIt, isCvt);\n \n+        if (!iterArg.value().getType().isa<RankedTensorType>())\n+          continue;\n         if (fwdCvtIt != fwdEndIt) {\n           auto newFor = tryConvertIterArg(forOp, rewriter, iterArg.index(),\n                                           (*fwdCvtIt)->getResult(0).getType());"}, {"filename": "python/tests/test_gemm.py", "status": "added", "additions": 52, "deletions": 0, "changes": 52, "file_content_changes": "@@ -0,0 +1,52 @@\n+import pytest\n+import torch\n+from torch.testing import assert_allclose\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def matmul_kernel(\n+    a_ptr, b_ptr, c_ptr,\n+    stride_am, stride_ak,\n+    stride_bk, stride_bn,\n+    stride_cm, stride_cn,\n+    M: tl.constexpr, N: tl.constexpr, K: tl.constexpr\n+):\n+    offs_m = tl.arange(0, M)\n+    offs_n = tl.arange(0, N)\n+    offs_k = tl.arange(0, K)\n+    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n+    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n+    a = tl.load(a_ptrs)\n+    b = tl.load(b_ptrs)\n+\n+    c = tl.dot(a, b)\n+\n+    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n+    tl.store(c_ptrs, c)\n+\n+# TODO: num_warps could only be 4 for now\n+\n+\n+@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n+    [128, 256, 32, 4],\n+    [256, 128, 16, 4],\n+    [128, 16, 32, 4],\n+    [32, 128, 64, 4],\n+])\n+def test_gemm_impl(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n+    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n+    grid = lambda META: (1, )\n+    matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n+                        stride_am=a.stride(0), stride_ak=a.stride(1),\n+                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                        M=SIZE_M, N=SIZE_N, K=SIZE_K,\n+                        num_warps=NUM_WARPS)\n+    golden = torch.matmul(a, b)\n+    torch.set_printoptions(profile=\"full\")\n+    assert_allclose(c, golden, rtol=1e-3, atol=1e-3)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "file_content_changes": "@@ -910,7 +910,7 @@ def ptx_get_version(cuda_version) -> int:\n \n \n def path_to_ptxas():\n-    prefixes = [os.environ.get(\"TRITON_PTXAS_PATH\", \"\"), \"\", \"/usr/local/cuda/\"]\n+    prefixes = [os.environ.get(\"TRITON_PTXAS_PATH\", \"\"), \"\", os.environ.get('CUDA_PATH', default_cuda_dir())]\n     for prefix in prefixes:\n         ptxas = os.path.join(prefix, \"bin\", \"ptxas\")\n         if os.path.exists(ptxas):\n@@ -1127,6 +1127,10 @@ def default_cache_dir():\n     return os.path.join(os.environ[\"HOME\"], \".triton\", \"cache\")\n \n \n+def default_cuda_dir():\n+    return os.path.join(\"/usr\", \"local\", \"cuda\")\n+\n+\n class CacheManager:\n \n     def __init__(self, key):\n@@ -1181,7 +1185,8 @@ def quiet():\n \n def _build(name, src, srcdir):\n     cuda_lib_dir = libcuda_dir()\n-    cu_include_dir = \"/usr/local/cuda/include\"\n+    cuda_path = os.environ.get('CUDA_PATH', default_cuda_dir())\n+    cu_include_dir = os.path.join(cuda_path, \"include\")\n     suffix = sysconfig.get_config_var('EXT_SUFFIX')\n     so = os.path.join(srcdir, '{name}{suffix}'.format(name=name, suffix=suffix))\n     # try to avoid setuptools if possible"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 188, "deletions": 6, "changes": 194, "file_content_changes": "@@ -374,6 +374,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_alloc_tensor\n   func @basic_alloc_tensor() {\n     // CHECK: llvm.mlir.addressof @global_smem\n+    // CHECK-NEXT: llvm.bitcast\n     // CHECK-NEXT: llvm.mlir.constant\n     // CHECK-NEXT: llvm.getelementptr\n     // CHECK-NEXT: llvm.bitcast\n@@ -390,13 +391,14 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_extract_slice\n   func @basic_extract_slice() {\n     // CHECK: %[[BASE0:.*]] = llvm.mlir.addressof @global_smem\n+    // CHECK-NEXT: %[[BASE1:.*]] = llvm.bitcast %[[BASE0]]\n     // CHECK-NEXT: %[[OFFSET0:.*]] = llvm.mlir.constant\n     // CHECK-NEXT: %[[OFFSET1:.*]] = llvm.mlir.constant\n-    // CHECK-NEXT: llvm.getelementptr %[[BASE0]][%[[OFFSET1]]]\n-    // CHECK-NEXT: %[[BASE1:.*]] = llvm.bitcast\n+    // CHECK-NEXT: llvm.getelementptr %[[BASE1]][%[[OFFSET1]]]\n+    // CHECK-NEXT: %[[BASE2:.*]] = llvm.bitcast\n     // CHECK-NEXT: %[[OFFSET2:.*]] = llvm.mlir.constant\n     // CHECK-NEXT: %[[OFFSET3:.*]] = llvm.mul %[[OFFSET0]], %[[OFFSET2]]\n-    // CHECK-NEXT: llvm.getelementptr %[[BASE1]][%[[OFFSET3]]]\n+    // CHECK-NEXT: llvm.getelementptr %[[BASE2]][%[[OFFSET3]]]\n     %index = arith.constant 1 : i32\n     %0 = triton_gpu.alloc_tensor : tensor<128x16x32xf32, #shared0>\n     %1 = triton_gpu.extract_slice %0, %index {axis = 0: i32} : tensor<128x16x32xf32, #shared0> -> tensor<16x32xf32, #shared0>\n@@ -406,6 +408,149 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // -----\n \n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: basic_async_wait\n+  func @basic_async_wait() {\n+    // CHECK: cp.async.wait_group 0x4\n+    triton_gpu.async_wait {num = 4: i32}\n+    return\n+  }\n+}\n+\n+// -----\n+\n+#block0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [4], warpsPerCTA = [4], order = [0]}>\n+#block1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [8], warpsPerCTA = [4], order = [0]}>\n+#block2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 1], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#block3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 8], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#AL = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#A = #triton_gpu.shared<{vec = 4, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: basic_insert_slice_async_v4\n+  func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n+    %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #block0>\n+    %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<64xi32, #block1>\n+    %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #block0>) -> tensor<16x1xi32, #block2>\n+    %off1 = tt.expand_dims %off1_ {axis = 0 : i32} : (tensor<64xi32, #block1>) -> tensor<1x64xi32, #block3>\n+    %broadcast_off0_scalar = tt.broadcast %off0 : (tensor<16x1xi32, #block2>) -> tensor<16x64xi32, #block2>\n+    %cst_scalar = arith.constant 64 : i32\n+    %cst = tt.splat %cst_scalar : (i32) -> tensor<16x64xi32, #block2>\n+    %broadcast_off0_ = arith.muli %broadcast_off0_scalar, %cst : tensor<16x64xi32, #block2>\n+    %broadcast_off1_ = tt.broadcast %off1 : (tensor<1x64xi32, #block3>) -> tensor<16x64xi32, #block3>\n+    %broadcast_off0 = triton_gpu.convert_layout %broadcast_off0_ : (tensor<16x64xi32, #block2>) -> tensor<16x64xi32, #AL>\n+    %broadcast_off1 = triton_gpu.convert_layout %broadcast_off1_ : (tensor<16x64xi32, #block3>) -> tensor<16x64xi32, #AL>\n+    %off = arith.addi %broadcast_off0, %broadcast_off1 : tensor<16x64xi32, #AL>\n+    %a_init = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<16x64x!tt.ptr<f32>, #AL>\n+    %a_ptr = tt.addptr %a_init, %off : tensor<16x64x!tt.ptr<f32>, #AL>\n+    %tensor = triton_gpu.alloc_tensor : tensor<2x16x64xf32, #A>\n+    %index = arith.constant 1 : i32\n+\n+    // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n+    // CHECK-SAME: cp.async.cg.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x80, 0x80\n+    // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n+    // CHECK-SAME: cp.async.cg.shared.global.L2::evict_normal [ ${{.*}} + 8 ], [ ${{.*}} + 0 ], 0x80, 0x80\n+    // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n+    // CHECK-SAME: cp.async.commit_group\n+    %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x64x!tt.ptr<f32>, #AL> -> tensor<2x16x64xf32, #A>\n+    return\n+  }\n+}\n+\n+// -----\n+\n+#block0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [4], warpsPerCTA = [4], order = [0]}>\n+#block1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [8], warpsPerCTA = [4], order = [0]}>\n+#block2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 1], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#block3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 8], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#A = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: basic_insert_slice_async_v1\n+  func @basic_insert_slice_async_v1(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n+    %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #block0>\n+    %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #block1>\n+    %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #block0>) -> tensor<16x1xi32, #block2>\n+    %off1 = tt.expand_dims %off1_ {axis = 0 : i32} : (tensor<32xi32, #block1>) -> tensor<1x32xi32, #block3>\n+    %broadcast_off0_scalar = tt.broadcast %off0 : (tensor<16x1xi32, #block2>) -> tensor<16x32xi32, #block2>\n+    %cst_scalar = arith.constant 32 : i32\n+    %cst = tt.splat %cst_scalar : (i32) -> tensor<16x32xi32, #block2>\n+    %broadcast_off0_ = arith.muli %broadcast_off0_scalar, %cst : tensor<16x32xi32, #block2>\n+    %broadcast_off1_ = tt.broadcast %off1 : (tensor<1x32xi32, #block3>) -> tensor<16x32xi32, #block3>\n+    %broadcast_off0 = triton_gpu.convert_layout %broadcast_off0_ : (tensor<16x32xi32, #block2>) -> tensor<16x32xi32, #AL>\n+    %broadcast_off1 = triton_gpu.convert_layout %broadcast_off1_ : (tensor<16x32xi32, #block3>) -> tensor<16x32xi32, #AL>\n+    %off = arith.addi %broadcast_off0, %broadcast_off1 : tensor<16x32xi32, #AL>\n+    %a_init = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<16x32x!tt.ptr<f32>, #AL>\n+    %a_ptr = tt.addptr %a_init, %off : tensor<16x32x!tt.ptr<f32>, #AL>\n+    %tensor = triton_gpu.alloc_tensor : tensor<2x16x32xf32, #A>\n+    %index = arith.constant 1 : i32\n+\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.commit_group\n+    %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x32x!tt.ptr<f32>, #AL> -> tensor<2x16x32xf32, #A>\n+    return\n+  }\n+}\n+\n+// -----\n+\n+#block0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [8], warpsPerCTA = [4], order = [0]}>\n+#block2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 1], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#block3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 8], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#A = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: basic_insert_slice_async_v1_multictas\n+  func @basic_insert_slice_async_v1_multictas(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n+    %off0_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #block0>\n+    %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #block0>\n+    %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<32xi32, #block0>) -> tensor<32x1xi32, #block2>\n+    %off1 = tt.expand_dims %off1_ {axis = 0 : i32} : (tensor<32xi32, #block0>) -> tensor<1x32xi32, #block3>\n+    %broadcast_off0_scalar = tt.broadcast %off0 : (tensor<32x1xi32, #block2>) -> tensor<32x32xi32, #block2>\n+    %cst_scalar = arith.constant 32 : i32\n+    %cst = tt.splat %cst_scalar : (i32) -> tensor<32x32xi32, #block2>\n+    %broadcast_off0_ = arith.muli %broadcast_off0_scalar, %cst : tensor<32x32xi32, #block2>\n+    %broadcast_off1_ = tt.broadcast %off1 : (tensor<1x32xi32, #block3>) -> tensor<32x32xi32, #block3>\n+    %broadcast_off0 = triton_gpu.convert_layout %broadcast_off0_ : (tensor<32x32xi32, #block2>) -> tensor<32x32xi32, #AL>\n+    %broadcast_off1 = triton_gpu.convert_layout %broadcast_off1_ : (tensor<32x32xi32, #block3>) -> tensor<32x32xi32, #AL>\n+    %off = arith.addi %broadcast_off0, %broadcast_off1 : tensor<32x32xi32, #AL>\n+    %a_init = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+    %a_ptr = tt.addptr %a_init, %off : tensor<32x32x!tt.ptr<f32>, #AL>\n+    %tensor = triton_gpu.alloc_tensor : tensor<2x32x32xf32, #A>\n+    %index = arith.constant 1 : i32\n+\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.commit_group\n+    %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32x!tt.ptr<f32>, #AL> -> tensor<2x32x32xf32, #A>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: basic_splat\n@@ -424,9 +569,9 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_store\n   func @basic_store(%ptrs: tensor<256x!tt.ptr<f32>, #blocked0>, %vals: tensor<256xf32, #blocked0>, %mask: tensor<256xi1, #blocked0>) {\n-    // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n+    // CHECK: llvm.inline_asm\n     // CHECK-SAME: st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n-    // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n+    // CHECK: llvm.inline_asm\n     // CHECK-SAME: st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     tt.store %ptrs, %vals, %mask : tensor<256xf32, #blocked0>\n     return\n@@ -561,7 +706,6 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   }\n }\n \n-\n // TODO: problems in MLIR's parser on slice layout\n // #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n // module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n@@ -570,3 +714,41 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n //     return\n //   }\n // }\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<2560 x i8>\n+  // CHECK-LABEL: convert_layout_mma_block\n+  func @convert_layout_mma_blocked(%arg0: tensor<32x16xf32, #mma>) {\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<32x16xf32, #mma>) -> tensor<32x16xf32, #blocked0>\n+    return\n+  }\n+}\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<16384 x i8>\n+  // CHECK-LABEL: convert_layout_blocked_shared\n+  func @convert_layout_blocked_shared(%arg0: tensor<128x32xf32, #blocked0>) {\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<8xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<8xf32>, 3>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n+    return\n+  }\n+}"}, {"filename": "test/TritonGPU/coalesce.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -7,8 +7,8 @@\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n \n-// CHECK: [[row_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n-// CHECK: [[col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n+// CHECK: [[row_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [1, 0]}>\n+// CHECK: [[col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK: [[load_ptr:%.*]] = triton_gpu.convert_layout {{.*}} -> tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n // CHECK: [[load_mask:%.*]] = triton_gpu.convert_layout {{.*}} -> tensor<64x64xi1, [[row_layout]]>\n // CHECK: [[load_other:%.*]] = triton_gpu.convert_layout {{.*}} -> tensor<64x64xf32, [[row_layout]]>"}, {"filename": "unittest/Analysis/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,5 +1,5 @@\n add_triton_ut(\n-  NAME TritonAnalysisTests\n+  NAME TestTritonAnalysis\n   SRCS UtilityTest.cpp\n   LIBS TritonAnalysis\n )"}, {"filename": "unittest/Analysis/UtilityTest.cpp", "status": "modified", "additions": 17, "deletions": 2, "changes": 19, "file_content_changes": "@@ -4,11 +4,26 @@\n //===----------------------------------------------------------------------===//\n \n #include \"triton/Analysis/Utility.h\"\n-#include <gmock/gmock.h>\n #include <gtest/gtest.h>\n \n namespace mlir {\n \n-TEST(UtilityTest, DummyTest) { EXPECT_EQ(true, true); }\n+TEST(Analysis, reorder) {\n+  SmallVector<int> shape({10, 20, 30});\n+  {\n+    SmallVector<unsigned> order({2, 1, 0});\n+    auto reordered = reorder<int>(shape, order);\n+    EXPECT_EQ(reordered[0], 30);\n+    EXPECT_EQ(reordered[1], 20);\n+    EXPECT_EQ(reordered[2], 10);\n+  }\n+  {\n+    SmallVector<unsigned> order({1, 0, 2});\n+    auto reordered = reorder<int>(shape, order);\n+    EXPECT_EQ(reordered[0], 20);\n+    EXPECT_EQ(reordered[1], 10);\n+    EXPECT_EQ(reordered[2], 30);\n+  }\n+}\n \n } // namespace mlir"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,5 +1,5 @@\n add_triton_ut(\n-\tNAME PtxAsmFormatTest\n+\tNAME TestPtxAsmFormat\n \tSRCS PtxAsmFormatTest.cpp\n \tLIBS TritonGPUToLLVM\n )"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/PtxAsmFormatTest.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -76,7 +76,7 @@ TEST_F(PtxAsmFormatTest, complexInstruction) {\n \n   auto &ld =\n       builder\n-          .create<PtxIOInstr>(\"ld\") //\n+          .create<PTXIOInstr>(\"ld\") //\n           ->o(\"volatile\", isVolatile)\n           .global()\n           .o(\"ca\", cache == CacheModifier::CA)"}]