[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -1957,7 +1957,8 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n-                         [('float32', 'math.pow', '')])\n+                         [('float32', 'math.pow', ''),\n+                          ('float64', 'math.pow', tl.math.LIBDEVICE_PATH)])\n def test_math_scalar(dtype_str, expr, lib_path):\n \n     @triton.jit\n@@ -1980,7 +1981,7 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     # triton result\n     x_tri = to_triton(x)[0].item()\n     y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n-    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'math': lib_path})\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n "}, {"filename": "python/tutorials/07-libdevice-function.py", "status": "added", "additions": 73, "deletions": 0, "changes": 73, "file_content_changes": "@@ -0,0 +1,73 @@\n+\"\"\"\n+Libdevice (`tl.math`) function\n+===============\n+Triton can invoke a custom function from an external library.\n+In this example, we will use the `libdevice` library (a.k.a `math` in triton) to apply `asin` on a tensor.\n+Please refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\n+In `triton/language/math.py`, we try to aggregate functions with the same computation but different data types together.\n+For example, both `__nv_asin` and `__nvasinf` calculate the principal value of the arc sine of the input, but `__nv_asin` operates on `double` and `__nv_asinf` operates on `float`.\n+Using triton, you can simply call `tl.math.asin`.\n+Triton automatically selects the correct underlying device function to invoke based on input and output types.\n+\"\"\"\n+\n+# %%\n+#  asin Kernel\n+# --------------------------\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def asin_kernel(\n+        x_ptr,\n+        y_ptr,\n+        n_elements,\n+        BLOCK_SIZE: tl.constexpr,\n+):\n+    pid = tl.program_id(axis=0)\n+    block_start = pid * BLOCK_SIZE\n+    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+    mask = offsets < n_elements\n+    x = tl.load(x_ptr + offsets, mask=mask)\n+    x = tl.math.asin(x)\n+    tl.store(y_ptr + offsets, x, mask=mask)\n+\n+# %%\n+#  Using the default libdevice library path\n+# --------------------------\n+# We can use the default libdevice library path encoded in `triton/language/math.py`\n+\n+\n+torch.manual_seed(0)\n+size = 98432\n+x = torch.rand(size, device='cuda')\n+output_triton = torch.zeros(size, device='cuda')\n+output_torch = torch.asin(x)\n+assert x.is_cuda and output_triton.is_cuda\n+n_elements = output_torch.numel()\n+grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024)\n+print(output_torch)\n+print(output_triton)\n+print(\n+    f'The maximum difference between torch and triton is '\n+    f'{torch.max(torch.abs(output_torch - output_triton))}'\n+)\n+\n+# %%\n+#  Customize the libdevice library path\n+# --------------------------\n+# We can also customize the libdevice library path by passing the path to the `libdevice` library to the `asin` kernel.\n+\n+output_triton = torch.empty_like(x)\n+asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024,\n+                  extern_libs={'libdevice': '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'})\n+print(output_torch)\n+print(output_triton)\n+print(\n+    f'The maximum difference between torch and triton is '\n+    f'{torch.max(torch.abs(output_torch - output_triton))}'\n+)"}]