[{"filename": "python/tests/test_backend.py", "status": "modified", "additions": 24, "deletions": 22, "changes": 46, "file_content_changes": "@@ -1,16 +1,20 @@\n+import pytest\n+import torch\n+\n import triton\n import triton.language as tl\n-import torch\n-import pytest\n-from .test_core import  numpy_random, to_triton\n+from .test_core import numpy_random, to_triton\n+\n \n class MmaLayout:\n-    def __init__(self, version, warps_per_cta):\n-        self.version = version\n+    def __init__(self, version_major, warps_per_cta, version_minor=0):\n+        self.version_major = version_major\n+        self.version_minor = version_minor\n         self.warps_per_cta = str(warps_per_cta)\n \n     def __str__(self):\n-        return f\"#triton_gpu.mma<{{version={self.version}, warpsPerCTA={self.warps_per_cta}}}>\"\n+        return f\"#triton_gpu.mma<{{versionMajor={self.version_major}, versionMinor={self.version_minor}, warpsPerCTA={self.warps_per_cta}}}>\"\n+\n \n class BlockedLayout:\n     def __init__(self, size_per_thread, threads_per_warp, warps_per_cta, order):\n@@ -22,17 +26,18 @@ def __init__(self, size_per_thread, threads_per_warp, warps_per_cta, order):\n     def __str__(self):\n         return f\"#triton_gpu.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}}}>\"\n \n+\n layouts = [\n-  # MmaLayout(version=1, warps_per_cta=[1, 4]),\n-  MmaLayout(version=2, warps_per_cta=[1, 4]),\n-  # MmaLayout(version=1, warps_per_cta=[4, 1]),\n-  MmaLayout(version=2, warps_per_cta=[4, 1]),\n-  BlockedLayout([1, 8], [2, 16], [4, 1], [1, 0]),\n-  BlockedLayout([1, 4], [4, 8], [2, 2], [1, 0]),\n-  BlockedLayout([1, 1], [1, 32], [2, 2], [1, 0]),\n-  BlockedLayout([8, 1], [16, 2], [1, 4], [0, 1]),\n-  BlockedLayout([4, 1], [8, 4], [2, 2], [0, 1]),\n-  BlockedLayout([1, 1], [32, 1], [2, 2], [0, 1])\n+    # MmaLayout(version=1, warps_per_cta=[1, 4]),\n+    MmaLayout(version_major=2, warps_per_cta=[1, 4]),\n+    # MmaLayout(version=1, warps_per_cta=[4, 1]),\n+    MmaLayout(version_major=2, warps_per_cta=[4, 1]),\n+    BlockedLayout([1, 8], [2, 16], [4, 1], [1, 0]),\n+    BlockedLayout([1, 4], [4, 8], [2, 2], [1, 0]),\n+    BlockedLayout([1, 1], [1, 32], [2, 2], [1, 0]),\n+    BlockedLayout([8, 1], [16, 2], [1, 4], [0, 1]),\n+    BlockedLayout([4, 1], [8, 4], [2, 2], [0, 1]),\n+    BlockedLayout([1, 1], [32, 1], [2, 2], [0, 1])\n ]\n \n \n@@ -45,13 +50,11 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n         pytest.skip()\n     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):\n         pytest.skip()\n-    \n-    \n \n     ir = f\"\"\"\n #src = {src_layout}\n #dst = {dst_layout}\n-\"\"\"  + \"\"\"\n+\"\"\" + \"\"\"\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   func public @kernel_0d1d(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n     %cst = arith.constant dense<128> : tensor<128x1xi32, #src>\n@@ -73,7 +76,7 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n     tt.store %14, %13 : tensor<128x128xf16, #dst>\n     return\n   }\n-}    \n+}\n \"\"\"\n \n     x = to_triton(numpy_random(shape, dtype_str=dtype))\n@@ -85,7 +88,6 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n         f.write(ir)\n         f.flush()\n         kernel = triton.compile(f.name)\n-    kernel[(1,1,1)](x.data_ptr(), z.data_ptr())\n+    kernel[(1, 1, 1)](x.data_ptr(), z.data_ptr())\n \n     assert torch.equal(z, x)\n-"}]