[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -601,6 +601,11 @@ def CallOp : TT_Op<\"call\", [CallOpInterface, /*MemRefsNormalizable, */DeclareOpI\n     CallInterfaceCallable getCallableForCallee() {\n       return (*this)->getAttrOfType<SymbolRefAttr>(\"callee\");\n     }\n+\n+    /// Set the callee for this operation.\n+    void setCalleeFromCallable(CallInterfaceCallable callee) {\n+      (*this)->setAttr(\"callee\", callee.get<SymbolRefAttr>());\n+    }\n   }];\n \n   let assemblyFormat = [{"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 8, "deletions": 6, "changes": 14, "file_content_changes": "@@ -67,19 +67,21 @@ class ArithConstantPattern : public OpConversionPattern<arith::ConstantOp> {\n   matchAndRewrite(arith::ConstantOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = getTypeConverter()->convertType(op.getType());\n+    auto retShapedType = retType.cast<ShapedType>();\n     auto value = adaptor.getValue().dyn_cast<DenseElementsAttr>();\n-    if (dyn_cast<RankedTensorType>(retType)) {\n+    if (dyn_cast<RankedTensorType>(retShapedType)) {\n       assert(value);\n       if (value.getElementType().isInteger(1) && value.isSplat())\n         // Workaround until https://reviews.llvm.org/D133743 is included.\n-        value = DenseElementsAttr::get(retType, value.getSplatValue<bool>());\n+        value =\n+            DenseElementsAttr::get(retShapedType, value.getSplatValue<bool>());\n       else\n         // This is a hack. We just want to add encoding\n-        value = value.reshape(retType);\n+        value = value.reshape(retShapedType);\n     }\n-    addNamedAttrs(\n-        rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, retType, value),\n-        adaptor.getAttributes());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<arith::ConstantOp>(\n+                      op, retShapedType, value),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };"}, {"filename": "lib/Dialect/Triton/IR/Dialect.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -91,5 +91,5 @@ void TritonDialect::initialize() {\n Operation *TritonDialect::materializeConstant(OpBuilder &builder,\n                                               Attribute value, Type type,\n                                               Location loc) {\n-  return builder.create<arith::ConstantOp>(loc, type, value);\n+  return arith::ConstantOp::materialize(builder, value, type, loc);\n }"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -349,7 +349,7 @@ void triton::StoreOp::getCanonicalizationPatterns(RewritePatternSet &results,\n //-- TransOp --\n mlir::LogicalResult mlir::triton::TransOp::inferReturnTypes(\n     MLIRContext *context, std::optional<Location> location, ValueRange operands,\n-    DictionaryAttr attributes, RegionRange regions,\n+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,\n     SmallVectorImpl<Type> &inferredReturnTypes) {\n   // type is the same as the input\n   auto argTy = operands[0].getType().cast<RankedTensorType>();\n@@ -376,7 +376,7 @@ mlir::LogicalResult mlir::triton::TransOp::inferReturnTypes(\n //-- DotOp --\n mlir::LogicalResult mlir::triton::DotOp::inferReturnTypes(\n     MLIRContext *context, std::optional<Location> location, ValueRange operands,\n-    DictionaryAttr attributes, RegionRange regions,\n+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,\n     SmallVectorImpl<Type> &inferredReturnTypes) {\n   // type is the same as the accumulator\n   auto accTy = operands[2].getType().cast<RankedTensorType>();\n@@ -444,7 +444,7 @@ void ReduceOp::build(mlir::OpBuilder &builder, mlir::OperationState &state,\n \n mlir::LogicalResult mlir::triton::ReduceOp::inferReturnTypes(\n     MLIRContext *context, std::optional<Location> location, ValueRange operands,\n-    DictionaryAttr attributes, RegionRange regions,\n+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,\n     SmallVectorImpl<Type> &inferredReturnTypes) {\n   for (auto arg : operands) {\n     auto argTy = arg.getType().cast<RankedTensorType>();\n@@ -550,7 +550,7 @@ OpFoldResult SplatOp::fold(FoldAdaptor adaptor) {\n //-- ExpandDimsOp --\n mlir::LogicalResult mlir::triton::ExpandDimsOp::inferReturnTypes(\n     MLIRContext *context, std::optional<Location> loc, ValueRange operands,\n-    DictionaryAttr attributes, RegionRange regions,\n+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,\n     SmallVectorImpl<Type> &inferredReturnTypes) {\n   // infer shape\n   auto arg = operands[0];"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -37,7 +37,7 @@ bool isBroadcastConstantCombinable(Attribute value) {\n DenseElementsAttr getConstantValue(Builder &builder, Attribute value,\n                                    Value bcast_res) {\n \n-  Type resType = bcast_res.getType();\n+  auto resType = bcast_res.getType().cast<ShapedType>();\n   DenseElementsAttr res;\n   if (auto denseValue = value.dyn_cast<DenseElementsAttr>()) {\n     res ="}, {"filename": "lib/Dialect/Triton/Transforms/RewriteTensorPointer.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -165,10 +165,10 @@ struct RewritedInfo {\n     auto otherTensorType = RankedTensorType::get(tensorShape, elementType);\n \n     // Set zero padding value\n-    Attribute attr =\n+    TypedAttr attr =\n         elementType.isIntOrIndex()\n-            ? builder.getIntegerAttr(elementType, 0).cast<Attribute>()\n-            : builder.getFloatAttr(elementType, 0).cast<Attribute>();\n+            ? builder.getIntegerAttr(elementType, 0).cast<TypedAttr>()\n+            : builder.getFloatAttr(elementType, 0).cast<TypedAttr>();\n \n     // Float NaN padding case\n     if (padding.value() == triton::PaddingOption::PAD_NAN) {"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1224,8 +1224,8 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   // cvt(type, constant) -> constant\n   if (auto cst = llvm::dyn_cast<arith::ConstantOp>(arg))\n     if (auto ret = cst.getValue().dyn_cast<SplatElementsAttr>()) {\n-      auto newRet = SplatElementsAttr::get(op->getResultTypes().front(),\n-                                           ret.getSplatValue<Attribute>());\n+      auto ty = op->getResultTypes().front().cast<ShapedType>();\n+      auto newRet = SplatElementsAttr::get(ty, ret.getSplatValue<Attribute>());\n       rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newRet);\n       return mlir::success();\n     }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -212,7 +212,8 @@ Operation *cloneWithInferType(mlir::OpBuilder &rewriter, Operation *op,\n     SmallVector<Type, 1> newTypes;\n     auto success = typeInfer.inferReturnTypes(\n         newOp->getContext(), newOp->getLoc(), newOp->getOperands(),\n-        newOp->getAttrDictionary(), newOp->getRegions(), newTypes);\n+        newOp->getAttrDictionary(), newOp->getPropertiesStorage(),\n+        newOp->getRegions(), newTypes);\n     if (succeeded(success)) {\n       for (size_t i = 0; i < newTypes.size(); i++)\n         newOp->getResult(i).setType(newTypes[i]);"}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -68,7 +68,7 @@ def get_llvm_package_info():\n     use_assert_enabled_llvm = check_env_flag(\"TRITON_USE_ASSERT_ENABLED_LLVM\", \"False\")\n     release_suffix = \"assert\" if use_assert_enabled_llvm else \"release\"\n     name = f'llvm+mlir-17.0.0-x86_64-{system_suffix}-{release_suffix}'\n-    version = \"llvm-17.0.0-f733b4fb9b8b\"\n+    version = \"llvm-17.0.0-c5dede880d17\"\n     url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n     return Package(\"llvm\", name, url, \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n "}]