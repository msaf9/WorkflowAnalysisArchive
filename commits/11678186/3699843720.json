[{"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -992,8 +992,9 @@ struct MMA16816ConversionHelper {\n     if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n       Value warpM = getWarpM(shape[0]);\n       // load from smem\n+    // we use ldmatrix.x4 so each warp processes 16x16 elements.\n       int wpt =\n-          std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / matShapeM);\n+          std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / 16);\n       loadFn =\n           getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/,\n                           {mmaInstrM, mmaInstrK} /*instrShape*/,\n@@ -1036,8 +1037,9 @@ struct MMA16816ConversionHelper {\n     int numRepN = getNumRepN(tensorTy, shape[1]);\n \n     Value warpN = getWarpN(shape[1]);\n+    // we use ldmatrix.x4 so each warp processes 16x16 elements.\n     int wpt =\n-        std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / matShapeN);\n+        std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / 16);\n     auto loadFn =\n         getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/,\n                         {mmaInstrK, mmaInstrN} /*instrShape*/,"}, {"filename": "python/test/unit/operators/test_blocksparse.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -10,7 +10,7 @@\n @pytest.mark.parametrize(\"TRANS_B\", [False, True])\n @pytest.mark.parametrize(\"BLOCK\", [16, 32, 64])\n # TODO: float32 fails\n-@pytest.mark.parametrize(\"DTYPE\", [torch.float32])\n+@pytest.mark.parametrize(\"DTYPE\", [torch.float16])\n def test_matmul(MODE, TRANS_A, TRANS_B, BLOCK, DTYPE, Z=3, H=2, M=512, N=384, K=256):\n     seed = 0\n     torch.manual_seed(seed)"}]