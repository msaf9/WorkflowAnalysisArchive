[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 131, "deletions": 9, "changes": 140, "file_content_changes": "@@ -1366,13 +1366,15 @@ struct ExtractSliceOpConversion\n   }\n };\n \n-template <typename SourceOp, typename DestOp>\n-class BinaryOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n+// A CRTP style of base class.\n+template <typename SourceOp, typename DestOp, typename ConcreteT>\n+class BinaryOpConversionBase\n+    : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n public:\n   using OpAdaptor = typename SourceOp::Adaptor;\n \n-  explicit BinaryOpConversion(LLVMTypeConverter &typeConverter,\n-                              PatternBenefit benefit = 1)\n+  explicit BinaryOpConversionBase(LLVMTypeConverter &typeConverter,\n+                                  PatternBenefit benefit = 1)\n       : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n \n   LogicalResult\n@@ -1393,20 +1395,138 @@ class BinaryOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n     SmallVector<Type> types(elems, elemTy);\n     Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n-    auto lhss =\n-        this->getElementsFromStruct(loc, adaptor.getLhs(), elems, rewriter);\n-    auto rhss =\n-        this->getElementsFromStruct(loc, adaptor.getRhs(), elems, rewriter);\n+\n+    auto *concreteThis = static_cast<const ConcreteT *>(this);\n+    auto lhss = this->getElementsFromStruct(loc, concreteThis->getLhs(adaptor),\n+                                            elems, rewriter);\n+    auto rhss = this->getElementsFromStruct(loc, concreteThis->getRhs(adaptor),\n+                                            elems, rewriter);\n     SmallVector<Value> resultVals(elems);\n     for (unsigned i = 0; i < elems; ++i) {\n-      resultVals[i] = rewriter.create<DestOp>(loc, elemTy, lhss[i], rhss[i]);\n+      resultVals[i] = concreteThis->createDestOp(op, rewriter, elemTy, lhss[i],\n+                                                 rhss[i], loc);\n     }\n     Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n     return success();\n   }\n };\n \n+template <typename SourceOp, typename DestOp>\n+struct BinaryOpConversion\n+    : public BinaryOpConversionBase<SourceOp, DestOp,\n+                                    BinaryOpConversion<SourceOp, DestOp>> {\n+\n+  explicit BinaryOpConversion(LLVMTypeConverter &typeConverter,\n+                              PatternBenefit benefit = 1)\n+      : BinaryOpConversionBase<SourceOp, DestOp,\n+                               BinaryOpConversion<SourceOp, DestOp>>(\n+            typeConverter, benefit) {}\n+\n+  using OpAdaptor = typename SourceOp::Adaptor;\n+  // An interface to support variant DestOp builder.\n+  DestOp createDestOp(SourceOp op, ConversionPatternRewriter &rewriter,\n+                      Type elemTy, Value lhs, Value rhs, Location loc) const {\n+    return rewriter.create<DestOp>(loc, elemTy, lhs, rhs);\n+  }\n+\n+  // Get the left operand of the op.\n+  Value getLhs(OpAdaptor adaptor) const { return adaptor.getLhs(); }\n+  // Get the right operand of the op.\n+  Value getRhs(OpAdaptor adaptor) const { return adaptor.getRhs(); }\n+};\n+\n+struct CmpIOpConversion\n+    : public BinaryOpConversionBase<triton::gpu::CmpIOp, LLVM::ICmpOp,\n+                                    CmpIOpConversion> {\n+  explicit CmpIOpConversion(LLVMTypeConverter &typeConverter,\n+                            PatternBenefit benefit = 1)\n+      : BinaryOpConversionBase(typeConverter, benefit) {}\n+\n+  // An interface to support variant DestOp builder.\n+  LLVM::ICmpOp createDestOp(triton::gpu::CmpIOp op,\n+                            ConversionPatternRewriter &rewriter, Type elemTy,\n+                            Value lhs, Value rhs, Location loc) const {\n+    return rewriter.create<LLVM::ICmpOp>(\n+        loc, elemTy, ArithCmpIPredicteToLLVM(op.predicate()), lhs, rhs);\n+  }\n+\n+  // Get the left operand of the op.\n+  Value getLhs(OpAdaptor adaptor) const { return adaptor.lhs(); }\n+  // Get the right operand of the op.\n+  Value getRhs(OpAdaptor adaptor) const { return adaptor.rhs(); }\n+\n+  static LLVM::ICmpPredicate\n+  ArithCmpIPredicteToLLVM(arith::CmpIPredicate predicate) {\n+    switch (predicate) {\n+#define __PRED_ENUM(item__)                                                    \\\n+  case arith::CmpIPredicate::item__:                                           \\\n+    return LLVM::ICmpPredicate::item__\n+\n+      __PRED_ENUM(eq);\n+      __PRED_ENUM(ne);\n+      __PRED_ENUM(sgt);\n+      __PRED_ENUM(sge);\n+      __PRED_ENUM(slt);\n+      __PRED_ENUM(sle);\n+      __PRED_ENUM(ugt);\n+      __PRED_ENUM(uge);\n+      __PRED_ENUM(ult);\n+      __PRED_ENUM(ule);\n+\n+#undef __PRED_ENUM\n+    }\n+  }\n+};\n+\n+struct CmpFOpConversion\n+    : public BinaryOpConversionBase<triton::gpu::CmpFOp, LLVM::ICmpOp,\n+                                    CmpFOpConversion> {\n+  explicit CmpFOpConversion(LLVMTypeConverter &typeConverter,\n+                            PatternBenefit benefit = 1)\n+      : BinaryOpConversionBase(typeConverter, benefit) {}\n+\n+  // An interface to support variant DestOp builder.\n+  LLVM::FCmpOp createDestOp(triton::gpu::CmpFOp op,\n+                            ConversionPatternRewriter &rewriter, Type elemTy,\n+                            Value lhs, Value rhs, Location loc) const {\n+    return rewriter.create<LLVM::FCmpOp>(\n+        loc, elemTy, ArithCmpFPredicteToLLVM(op.predicate()), lhs, rhs);\n+  }\n+\n+  // Get the left operand of the op.\n+  Value getLhs(OpAdaptor adaptor) const { return adaptor.lhs(); }\n+  // Get the right operand of the op.\n+  Value getRhs(OpAdaptor adaptor) const { return adaptor.rhs(); }\n+\n+  static LLVM::FCmpPredicate\n+  ArithCmpFPredicteToLLVM(arith::CmpFPredicate predicate) {\n+    switch (predicate) {\n+#define __PRED_ENUM(item__, item1__)                                           \\\n+  case arith::CmpFPredicate::item__:                                           \\\n+    return LLVM::FCmpPredicate::item1__\n+\n+      __PRED_ENUM(OEQ, oeq);\n+      __PRED_ENUM(ONE, one);\n+      __PRED_ENUM(OGT, ogt);\n+      __PRED_ENUM(OGE, oge);\n+      __PRED_ENUM(OLT, olt);\n+      __PRED_ENUM(OLE, ole);\n+      __PRED_ENUM(ORD, ord);\n+      __PRED_ENUM(UEQ, ueq);\n+      __PRED_ENUM(UGT, ugt);\n+      __PRED_ENUM(ULT, ult);\n+      __PRED_ENUM(ULE, ule);\n+      __PRED_ENUM(UNE, une);\n+      __PRED_ENUM(UNO, uno);\n+      __PRED_ENUM(AlwaysTrue, _true);\n+      __PRED_ENUM(AlwaysFalse, _false);\n+\n+#undef __PRED_ENUM\n+    }\n+  }\n+};\n+\n struct ConvertLayoutOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {\n public:\n@@ -3001,6 +3121,8 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                                                benefit);\n   patterns.add<BinaryOpConversion<arith::MulFOp, LLVM::FMulOp>>(typeConverter,\n                                                                 benefit);\n+  patterns.add<CmpIOpConversion>(typeConverter, benefit);\n+  patterns.add<CmpFOpConversion>(typeConverter, benefit);\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n                                           benefit);"}, {"filename": "python/tests/test_vecadd_no_scf.py", "status": "modified", "additions": 36, "deletions": 17, "changes": 53, "file_content_changes": "@@ -1,43 +1,62 @@\n+import math\n+\n+import pytest\n import torch\n from torch.testing import assert_close\n \n import triton\n import triton.language as tl\n \n \n-def vecadd_no_scf_tester(num_warps, block_size, tensor_shape):\n+def vecadd_no_scf_tester(num_warps, block_size, shape):\n     @triton.jit\n     def kernel(x_ptr,\n                y_ptr,\n                z_ptr,\n+               n_elements,\n                BLOCK_SIZE_N: tl.constexpr):\n         pid = tl.program_id(axis=0)\n \n         offset = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n         x_ptrs = x_ptr + offset\n         y_ptrs = y_ptr + offset\n-        x = tl.load(x_ptrs)\n-        y = tl.load(y_ptrs)\n+\n+        mask = offset < n_elements\n+\n+        x = tl.load(x_ptrs, mask=mask)\n+        y = tl.load(y_ptrs, mask=mask)\n         z = x + y\n         z_ptrs = z_ptr + offset\n-        tl.store(z_ptrs, z)\n+        tl.store(z_ptrs, z, mask=mask)\n \n-    x = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n-    y = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n-    z = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n+    x = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    y = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    z = torch.empty(shape, device=x.device, dtype=x.dtype)\n \n-    grid = lambda EA: (x.shape.numel() // block_size,)\n-    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, BLOCK_SIZE_N=block_size, num_warps=num_warps)\n+    grid = lambda EA: (math.ceil(x.shape.numel() / block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, n_elements=x.shape.numel(), BLOCK_SIZE_N=block_size, num_warps=num_warps)\n \n     golden_z = x + y\n     assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n \n \n-def test_vecadd_no_scf():\n-    vecadd_no_scf_tester(num_warps=4, block_size=256)\n-    vecadd_no_scf_tester(num_warps=2, block_size=256)\n-    vecadd_no_scf_tester(num_warps=1, block_size=256)\n-\n-\n-if __name__ == '__main__':\n-    test_vecadd_no_scf()\n+@pytest.mark.parametrize('num_warps, block_size, shape', [\n+    [4, 256, (256,)],\n+    [2, 256, (256,)],\n+    [1, 256, (256,)],\n+    [4, 16, (256,)],\n+    [2, 64, (256,)],\n+    [1, 128, (256,)],\n+])\n+def test_vecadd_no_scf(num_warps, block_size, shape):\n+    vecadd_no_scf_tester(num_warps, block_size, shape)\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, shape', [\n+    [1, 128, (256 + 1,)],\n+    [1, 256, (256 + 1,)],\n+    [2, 256, (3, 256 + 7)],\n+    [4, 256, (3, 256 + 7)],\n+])\n+def test_vecadd__no_scf_masked(num_warps, block_size, shape):\n+    vecadd_no_scf_tester(num_warps, block_size, shape)"}]