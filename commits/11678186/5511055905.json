[{"filename": "python/src/triton.cc", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "file_content_changes": "@@ -643,6 +643,26 @@ void init_triton_ir(py::module &&m) {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n                  v, self.getBuilder().getI64Type()));\n            })\n+      .def(\"get_uint8\",\n+           [](TritonOpBuilder &self, uint64_t v) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 v, self.getBuilder().getI8Type()));\n+           })\n+      .def(\"get_uint16\",\n+           [](TritonOpBuilder &self, uint64_t v) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 v, self.getBuilder().getI16Type()));\n+           })\n+      .def(\"get_uint32\",\n+           [](TritonOpBuilder &self, uint64_t v) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 v, self.getBuilder().getI32Type()));\n+           })\n+      .def(\"get_uint64\",\n+           [](TritonOpBuilder &self, uint64_t v) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 v, self.getBuilder().getI64Type()));\n+           })\n       .def(\"get_bf16\",\n            [](TritonOpBuilder &self, float v) -> mlir::Value {\n              auto type = self.getBuilder().getBF16Type();"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -2171,9 +2171,13 @@ def kernel(Z, X, Y,\n     assert \"triton_gpu.async_wait {num = 2 : i32}\" in h.asm['ttgir']\n \n \n-@pytest.mark.parametrize(\"dtype_str\", int_dtypes + float_dtypes + ['bfloat16'])\n+@pytest.mark.parametrize(\"dtype_str\", int_dtypes + uint_dtypes + float_dtypes + ['bfloat16'])\n def test_full(dtype_str, device):\n-    dtype = getattr(torch, dtype_str)\n+    if dtype_str in uint_dtypes and not hasattr(torch, dtype_str):\n+        # PyTorch only has unsigned 8, but not 16, 32, or 64\n+        dtype = getattr(torch, dtype_str[1:])  # uintx -> intx\n+    else:\n+        dtype = getattr(torch, dtype_str)\n     check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     @triton.jit"}]