[{"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 44, "deletions": 5, "changes": 49, "file_content_changes": "@@ -131,6 +131,11 @@ struct ReduceOpConversion\n     }\n   }\n \n+  // Calculates the write index in the shared memory where we would be writing\n+  // the within-thread accumulations before we start doing across-threads\n+  // accumulations. `index` is the index of the within-thread accumulations in\n+  // the full tensor, whereas `writeIdx` is the mapped-to index in the shared\n+  // memory\n   void getWriteIndexBasic(ConversionPatternRewriter &rewriter, Location loc,\n                           Attribute layout, SmallVector<Value> &index,\n                           SmallVector<Value> &writeIdx,\n@@ -141,6 +146,12 @@ struct ReduceOpConversion\n     Value _8 = ints[8];\n     Value _16 = ints[16];\n     if (layout.isa<BlockedEncodingAttr>()) {\n+      // A single thread owns axisSizePerThread contiguous values\n+      // on the reduction axis. After within thread reduction,\n+      // we would have a single accumulation every `axisSizePerThread`\n+      // contiguous values in the original tensor, so we would need\n+      // to map every `axisSizePerThread` to 1 value in smem as:\n+      // writeIdx[axis] = index[axis] / axisSizePerThread\n       writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n     }\n     auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>();\n@@ -152,9 +163,7 @@ struct ReduceOpConversion\n         writeIdx[axis] =\n             add(mul(udiv(index[axis], _16), _8), urem(index[axis], _8));\n       } else {\n-        // A single thread owns axisSizePerThread contiguous values\n-        // on the reduction axis, so after within thread reduction,\n-        // writeIdx[axis] = index[axis] / axisSizePerThread\n+        // Same as BlockedEncodingAttr case\n         writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n       }\n     }\n@@ -170,37 +179,47 @@ struct ReduceOpConversion\n     ReduceOpHelper helper(op);\n     Location loc = op->getLoc();\n     unsigned axis = op.getAxis();\n+    // Specifies whether the reduce operation returns an index\n+    // rather than a value, e.g. argmax, argmin, .. etc\n     bool withIndex = triton::ReduceOp::withIndex(op.getRedOp());\n \n     auto srcTy = op.getOperand().getType().cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding();\n     if (!helper.isSupportedLayout()) {\n       assert(false && \"Unexpected srcLayout in ReduceOpConversion\");\n     }\n+    // The order of the axes for the the threads within the warp\n     auto srcOrd = triton::gpu::getOrder(srcLayout);\n+    // The elements owned by each thread, e.g. [2, 2] means that each thread\n+    // in a warp would own 2x2 elements of the original tensor\n     auto sizePerThread = triton::gpu::getSizePerThread(srcLayout);\n     auto srcShape = srcTy.getShape();\n \n     auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n     auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n     auto indexPtrTy = LLVM::LLVMPointerType::get(llvmIndexTy, 3);\n+    // The shared memory base address\n     Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n     smemBase = bitcast(smemBase, elemPtrTy);\n \n+    // The shape of the shared memory\n     auto smemShape = helper.getScratchConfigBasic();\n     unsigned elems = product<unsigned>(smemShape);\n     Value indexSmemBase = gep(elemPtrTy, smemBase, i32_val(elems));\n     indexSmemBase = bitcast(indexSmemBase, indexPtrTy);\n \n     unsigned srcElems = getElemsPerThread(srcTy);\n+    // Emits indices of the original tensor that each thread\n+    // would own\n     auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy);\n     auto srcValues = getTypeConverter()->unpackLLElements(\n         loc, adaptor.getOperand(), rewriter, srcTy);\n-\n+    // Emits offsets (the offset from the base index)\n+    // of the original tensor that each thread would own\n     SmallVector<SmallVector<unsigned>> offset =\n         emitOffsetForLayout(srcLayout, srcTy);\n-\n+    // Keep track of accumulations and their indices\n     std::map<SmallVector<unsigned>, Value> accs;\n     std::map<SmallVector<unsigned>, Value> accIndices;\n     std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n@@ -238,29 +257,49 @@ struct ReduceOpConversion\n       Value accIndex;\n       if (withIndex)\n         accIndex = accIndices[key];\n+      // get the writeIdx at which to write in smem\n       SmallVector<Value> writeIdx;\n       getWriteIndexBasic(rewriter, loc, srcLayout, indices[key], writeIdx, ints,\n                          axis);\n+      // calculate the offset in smem for that writeIdx\n       Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, srcOrd);\n+      // Get element pointers for the value and index\n       Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n       Value indexWritePtr = gep(indexPtrTy, indexSmemBase, writeOffset);\n+      // Store the within-thread accumulated value at writePtr\n       store(acc, writePtr);\n+      // Store the index of within-thread accumulation at indexWritePtr\n       if (withIndex)\n         store(accIndex, indexWritePtr);\n \n       SmallVector<Value> readIdx(writeIdx.size(), ints[0]);\n+      // Perform parallel reduction with sequential addressing\n+      // E.g. We reduce `smemShape[axis]` elements into `smemShape[axis]/2`\n+      // elements using `smemShape[axis]/2` threads where each thread\n+      // would accumalte values that are `smemShape[axis]/2` apart\n+      // to avoid bank conflicts. Then we repeat with `smemShape[axis]/4`\n+      // threads, .. etc.\n       for (int N = smemShape[axis] / 2; N > 0; N >>= 1) {\n+        // The readIdx will be N elements away on the reduction axis\n         readIdx[axis] = ints[N];\n+        // If the writeIdx is greater or equal to N, do nothing\n         Value readMask = icmp_slt(writeIdx[axis], ints[N]);\n+        // Calculate the readOffset, if readMask is False, readOffset=0\n+        // meaning we reduce the value at writeIdx with itself\n         Value readOffset = select(\n             readMask, linearize(rewriter, loc, readIdx, smemShape, srcOrd),\n             ints[0]);\n+        // The readPtr is readOffset away from writePtr\n         Value readPtr = gep(elemPtrTy, writePtr, readOffset);\n         barrier();\n+        // If we do not care about the index, i.e. this is not an argmax,\n+        // argmin, .. etc\n         if (!withIndex) {\n+          // The value at the readPtr, whereas acc is the value at writePtr\n           Value cur = load(readPtr);\n           accumulate(rewriter, loc, op.getRedOp(), acc, cur, false);\n           barrier();\n+          // Update writePtr value\n           store(acc, writePtr);\n         } else {\n           Value cur = load(readPtr);"}]