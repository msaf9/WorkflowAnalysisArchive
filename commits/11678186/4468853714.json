[{"filename": "include/triton/Conversion/Passes.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -2,7 +2,6 @@\n #define TRITON_CONVERSION_PASSES_H\n \n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n "}, {"filename": "include/triton/Conversion/Passes.td", "status": "modified", "additions": 0, "deletions": 14, "changes": 14, "file_content_changes": "@@ -49,18 +49,4 @@ def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"\n     ];\n }\n \n-def TritonConvertArithToIndex : Pass<\"triton-convert-arith-to-index\", \"mlir::ModuleOp\"> {\n-\n-    let summary = \"Convert arith to index\";\n-    \n-    let constructor = \"mlir::triton::createTritonConvertArithToIndexPass()\";\n-\n-    let description = [{\n-      Convert arith operation on index values to corresponding ops in the index dialect.\n-      We need this because SCFToCF conversion currently generates arith ops on indices.\n-    }];\n-\n-    let dependentDialects = [\"mlir::index::IndexDialect\"];\n-}\n-\n #endif"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h", "status": "removed", "additions": 0, "deletions": 20, "changes": 20, "file_content_changes": "@@ -1,20 +0,0 @@\n-#ifndef TRITON_CONVERSION_ARITH_TO_INDEX_H\n-#define TRITON_CONVERSION_ARITH_TO_INDEX_H\n-\n-#include \"mlir/Conversion/LLVMCommon/TypeConverter.h\"\n-#include \"mlir/Transforms/DialectConversion.h\"\n-#include <memory>\n-\n-namespace mlir {\n-\n-class ModuleOp;\n-template <typename T> class OperationPass;\n-\n-namespace triton {\n-\n-std::unique_ptr<OperationPass<ModuleOp>> createTritonConvertArithToIndexPass();\n-\n-}\n-} // namespace mlir\n-\n-#endif\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ArithToIndexPass.cpp", "status": "removed", "additions": 0, "deletions": 90, "changes": 90, "file_content_changes": "@@ -1,90 +0,0 @@\n-#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n-#include \"mlir/Analysis/DataFlowFramework.h\"\n-#include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n-#include \"mlir/Conversion/ControlFlowToLLVM//ControlFlowToLLVM.h\"\n-#include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n-#include \"mlir/Conversion/LLVMCommon/VectorPattern.h\"\n-#include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n-#include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n-#include \"mlir/Dialect/Index/IR/IndexDialect.h\"\n-#include \"mlir/Dialect/Index/IR/IndexOps.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n-#include \"mlir/Pass/Pass.h\"\n-#include \"triton/Dialect/Triton/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-\n-using namespace mlir;\n-using namespace mlir::triton;\n-\n-#define GEN_PASS_CLASSES\n-#include \"triton/Conversion/Passes.h.inc\"\n-\n-namespace {\n-class TritonArithToIndexConversionTarget : public mlir::ConversionTarget {\n-public:\n-  static bool hasIndexResultOrOperand(Operation *op) {\n-    if (!op)\n-      return false;\n-    bool hasRetIndex = llvm::find_if(op->getResultTypes(), [](Type type) {\n-                         return type.isIndex();\n-                       }) != op->getResultTypes().end();\n-    bool hasArgIndex = llvm::find_if(op->getOperandTypes(), [](Type type) {\n-                         return type.isIndex();\n-                       }) != op->getOperandTypes().end();\n-    return !hasRetIndex && !hasArgIndex;\n-  }\n-\n-  explicit TritonArithToIndexConversionTarget(MLIRContext &ctx)\n-      : ConversionTarget(ctx) {\n-    addLegalDialect<index::IndexDialect>();\n-    addDynamicallyLegalDialect<arith::ArithDialect>(hasIndexResultOrOperand);\n-  }\n-};\n-\n-template <class SrcOp, class DstOp>\n-LogicalResult replaceArithWithIndex(SrcOp op, PatternRewriter &rewriter) {\n-  // if (!hasIndexResultOrOperand(&*op))\n-  //   return failure();\n-  rewriter.replaceOpWithNewOp<DstOp>(op, op->getResultTypes(),\n-                                     op->getOperands(), op->getAttrs());\n-  return success();\n-}\n-\n-LogicalResult replaceArithCmpWithIndexCmp(arith::CmpIOp op,\n-                                          PatternRewriter &rewriter) {\n-  // if (!hasIndexResultOrOperand(&*op))\n-  //   return failure();\n-  rewriter.replaceOpWithNewOp<index::CmpOp>(\n-      op, op.getResult().getType(), (index::IndexCmpPredicate)op.getPredicate(),\n-      op.getOperand(0), op.getOperand(1));\n-  return success();\n-}\n-\n-class ArithToIndex : public TritonConvertArithToIndexBase<ArithToIndex> {\n-public:\n-  void runOnOperation() override {\n-    MLIRContext *context = &getContext();\n-    ModuleOp mod = getOperation();\n-    TritonArithToIndexConversionTarget target(*context);\n-    RewritePatternSet patterns(context);\n-    patterns.add(replaceArithWithIndex<arith::IndexCastOp, index::CastSOp>);\n-    patterns.add(replaceArithWithIndex<arith::ConstantOp, index::ConstantOp>);\n-    patterns.add(replaceArithWithIndex<arith::AddIOp, index::AddOp>);\n-    patterns.add(replaceArithCmpWithIndexCmp);\n-    if (failed(applyPartialConversion(mod, target, std::move(patterns)))) {\n-      return signalPassFailure();\n-    }\n-  }\n-};\n-} // namespace\n-\n-namespace mlir {\n-namespace triton {\n-\n-std::unique_ptr<OperationPass<ModuleOp>> createTritonConvertArithToIndexPass() {\n-  return std::make_unique<::ArithToIndex>();\n-}\n-\n-} // namespace triton\n-} // namespace mlir\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,5 +1,4 @@\n add_mlir_conversion_library(TritonGPUToLLVM\n-    ArithToIndexPass.cpp\n     ConvertLayoutOpToLLVM.cpp\n     DotOpToLLVM.cpp\n     ElementwiseOpToLLVM.cpp"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.cpp", "status": "modified", "additions": 82, "deletions": 12, "changes": 94, "file_content_changes": "@@ -443,6 +443,8 @@ Type DotOpMmaV2ConversionHelper::getShemPtrTy() const {\n     return ptr_ty(type::i16Ty(ctx), 3);\n   case TensorCoreType::FP32_TF32_TF32_FP32:\n     return ptr_ty(type::f32Ty(ctx), 3);\n+  case TensorCoreType::FP16_FP16_FP16_FP16:\n+    return ptr_ty(type::f16Ty(ctx), 3);\n   case TensorCoreType::INT32_INT8_INT8_INT32:\n     return ptr_ty(type::i8Ty(ctx), 3);\n   default:\n@@ -471,6 +473,8 @@ Type DotOpMmaV2ConversionHelper::getMatType() const {\n   switch (mmaType) {\n   case TensorCoreType::FP32_FP16_FP16_FP32:\n     return fp16x2Pack4Ty;\n+  case TensorCoreType::FP16_FP16_FP16_FP16:\n+    return fp16x2Pack4Ty;\n   case TensorCoreType::FP32_BF16_BF16_FP32:\n     return bf16x2Pack4Ty;\n   case TensorCoreType::FP32_TF32_TF32_FP32:\n@@ -488,6 +492,8 @@ Type DotOpMmaV2ConversionHelper::getLoadElemTy() {\n   switch (mmaType) {\n   case TensorCoreType::FP32_FP16_FP16_FP32:\n     return vec_ty(type::f16Ty(ctx), 2);\n+  case TensorCoreType::FP16_FP16_FP16_FP16:\n+    return vec_ty(type::f16Ty(ctx), 2);\n   case TensorCoreType::FP32_BF16_BF16_FP32:\n     return vec_ty(type::bf16Ty(ctx), 2);\n   case TensorCoreType::FP32_TF32_TF32_FP32:\n@@ -503,14 +509,19 @@ Type DotOpMmaV2ConversionHelper::getLoadElemTy() {\n \n Type DotOpMmaV2ConversionHelper::getMmaRetType() const {\n   Type fp32Ty = type::f32Ty(ctx);\n+  Type fp16Ty = type::f16Ty(ctx);\n   Type i32Ty = type::i32Ty(ctx);\n   Type fp32x4Ty =\n       LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n   Type i32x4Ty =\n       LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i32Ty));\n+  Type fp16x2Pack2Ty = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(2, vec_ty(fp16Ty, 2)));\n   switch (mmaType) {\n   case TensorCoreType::FP32_FP16_FP16_FP32:\n     return fp32x4Ty;\n+  case TensorCoreType::FP16_FP16_FP16_FP16:\n+    return fp16x2Pack2Ty;\n   case TensorCoreType::FP32_BF16_BF16_FP32:\n     return fp32x4Ty;\n   case TensorCoreType::FP32_TF32_TF32_FP32:\n@@ -524,6 +535,25 @@ Type DotOpMmaV2ConversionHelper::getMmaRetType() const {\n   return Type{};\n }\n \n+int DotOpMmaV2ConversionHelper::getMmaRetSize() const {\n+  switch (mmaType) {\n+  case TensorCoreType::FP32_FP16_FP16_FP32:\n+    return 4;\n+  case TensorCoreType::FP16_FP16_FP16_FP16:\n+    return 2;\n+  case TensorCoreType::FP32_BF16_BF16_FP32:\n+    return 4;\n+  case TensorCoreType::FP32_TF32_TF32_FP32:\n+    return 4;\n+  case TensorCoreType::INT32_INT8_INT8_INT32:\n+    return 4;\n+  default:\n+    llvm::report_fatal_error(\"Unsupported mma type found\");\n+  }\n+\n+  return 4;\n+}\n+\n DotOpMmaV2ConversionHelper::TensorCoreType\n DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(Type operandTy) {\n   auto tensorTy = operandTy.cast<RankedTensorType>();\n@@ -559,6 +589,9 @@ DotOpMmaV2ConversionHelper::getMmaType(triton::DotOp op) {\n   } else if (dTy.getElementType().isInteger(32)) {\n     if (aTy.getElementType().isInteger(8) && bTy.getElementType().isInteger(8))\n       return TensorCoreType::INT32_INT8_INT8_INT32;\n+  } else if (dTy.getElementType().isF16()) {\n+    if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n+      return TensorCoreType::FP16_FP16_FP16_FP16;\n   }\n \n   return TensorCoreType::NOT_APPLICABLE;\n@@ -1004,6 +1037,31 @@ Value MMA16816ConversionHelper::loadC(Value tensor, Value llTensor) const {\n   assert(structTy.getBody().size() == fcSize &&\n          \"DotOp's $c operand should pass the same number of values as $d in \"\n          \"mma layout.\");\n+\n+  auto numMmaRets = helper.getMmaRetSize();\n+  assert(numMmaRets == 4 || numMmaRets == 2);\n+  if (numMmaRets == 4) {\n+    return llTensor;\n+  } else if (numMmaRets == 2) {\n+    auto cPack = SmallVector<Value>();\n+    auto cElemTy = tensorTy.getElementType();\n+    int numCPackedElem = 4 / numMmaRets;\n+    Type cPackTy = vec_ty(cElemTy, numCPackedElem);\n+    for (int i = 0; i < fcSize; i += numCPackedElem) {\n+      Value pack = rewriter.create<LLVM::UndefOp>(loc, cPackTy);\n+      for (int j = 0; j < numCPackedElem; ++j) {\n+        pack = insert_element(\n+            cPackTy, pack, extract_val(cElemTy, llTensor, i + j), i32_val(j));\n+      }\n+      cPack.push_back(pack);\n+    }\n+\n+    Type structTy = LLVM::LLVMStructType::getLiteral(\n+        ctx, SmallVector<Type>(cPack.size(), cPackTy));\n+    Value result =\n+        typeConverter->packLLElements(loc, cPack, rewriter, structTy);\n+    return result;\n+  }\n   return llTensor;\n }\n LogicalResult MMA16816ConversionHelper::convertDot(Value a, Value b, Value c,\n@@ -1032,14 +1090,18 @@ LogicalResult MMA16816ConversionHelper::convertDot(Value a, Value b, Value c,\n   ValueTable hb = getValuesFromDotOperandLayoutStruct(\n       loadedB, std::max(numRepN / 2, 1), numRepK, bTensorTy);\n   auto fc = typeConverter->unpackLLElements(loc, loadedC, rewriter, dTensorTy);\n+  auto numMmaRets = helper.getMmaRetSize();\n+  int numCPackedElem = 4 / numMmaRets;\n \n   auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n     unsigned colsPerThread = numRepN * 2;\n     PTXBuilder builder;\n     auto &mma = *builder.create(helper.getMmaInstr().str());\n     // using =r for float32 works but leads to less readable ptx.\n     bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n-    auto retArgs = builder.newListOperand(4, isIntMMA ? \"=r\" : \"=f\");\n+    bool isAccF16 = dTensorTy.getElementType().isF16();\n+    auto retArgs =\n+        builder.newListOperand(numMmaRets, isIntMMA || isAccF16 ? \"=r\" : \"=f\");\n     auto aArgs = builder.newListOperand({\n         {ha[{m, k}], \"r\"},\n         {ha[{m + 1, k}], \"r\"},\n@@ -1049,18 +1111,21 @@ LogicalResult MMA16816ConversionHelper::convertDot(Value a, Value b, Value c,\n     auto bArgs =\n         builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n     auto cArgs = builder.newListOperand();\n-    for (int i = 0; i < 4; ++i) {\n-      cArgs->listAppend(builder.newOperand(fc[m * colsPerThread + 4 * n + i],\n-                                           std::to_string(i)));\n+    for (int i = 0; i < numMmaRets; ++i) {\n+      cArgs->listAppend(builder.newOperand(\n+          fc[(m * colsPerThread + 4 * n) / numCPackedElem + i],\n+          std::to_string(i)));\n       // reuse the output registers\n     }\n \n     mma(retArgs, aArgs, bArgs, cArgs);\n     Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n \n     Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n-    for (int i = 0; i < 4; ++i)\n-      fc[m * colsPerThread + 4 * n + i] = extract_val(elemTy, mmaOut, i);\n+    for (int i = 0; i < numMmaRets; ++i) {\n+      fc[(m * colsPerThread + 4 * n) / numCPackedElem + i] =\n+          extract_val(elemTy, mmaOut, i);\n+    }\n   };\n \n   for (int k = 0; k < numRepK; ++k)\n@@ -1070,14 +1135,19 @@ LogicalResult MMA16816ConversionHelper::convertDot(Value a, Value b, Value c,\n \n   Type resElemTy = dTensorTy.getElementType();\n \n-  for (auto &elem : fc) {\n-    elem = bitcast(elem, resElemTy);\n-  }\n-\n   // replace with new packed result\n   Type structTy = LLVM::LLVMStructType::getLiteral(\n-      ctx, SmallVector<Type>(fc.size(), resElemTy));\n-  Value res = typeConverter->packLLElements(loc, fc, rewriter, structTy);\n+      ctx, SmallVector<Type>(fc.size() * numCPackedElem, resElemTy));\n+  SmallVector<Value> results(fc.size() * numCPackedElem);\n+  for (int i = 0; i < fc.size(); ++i) {\n+    for (int j = 0; j < numCPackedElem; ++j) {\n+      results[i * numCPackedElem + j] =\n+          numCPackedElem > 1\n+              ? bitcast(extract_element(fc[i], i32_val(j)), resElemTy)\n+              : bitcast(fc[i], resElemTy);\n+    }\n+  }\n+  Value res = typeConverter->packLLElements(loc, results, rewriter, structTy);\n   rewriter.replaceOp(op, res);\n \n   return success();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 25, "deletions": 0, "changes": 25, "file_content_changes": "@@ -116,6 +116,7 @@ struct DotOpMmaV2ConversionHelper {\n     FP32_FP16_FP16_FP32 = 0, // default\n     FP32_BF16_BF16_FP32,\n     FP32_TF32_TF32_FP32,\n+    FP16_FP16_FP16_FP16,\n     // integer tensor core instr\n     INT32_INT1_INT1_INT32, // Not implemented\n     INT32_INT4_INT4_INT32, // Not implemented\n@@ -155,6 +156,8 @@ struct DotOpMmaV2ConversionHelper {\n \n   Type getMmaRetType() const;\n \n+  int getMmaRetSize() const;\n+\n   ArrayRef<int> getMmaInstrShape() const {\n     assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n            \"Unknown mma type found.\");\n@@ -174,6 +177,9 @@ struct DotOpMmaV2ConversionHelper {\n   }\n \n   // Deduce the TensorCoreType from either $a or $b's type.\n+  // TODO: both the input type ($a or $b) and output type ($c or $d) are\n+  // needed to differentiate TensorCoreType::FP32_FP16_FP16_FP32 from\n+  // TensorCoreType::FP16_FP16_FP16_FP16\n   static TensorCoreType getTensorCoreTypeFromOperand(Type operandTy);\n \n   int getVec() const {\n@@ -202,6 +208,7 @@ struct DotOpMmaV2ConversionHelper {\n           {TensorCoreType::FP32_FP16_FP16_FP32, {16, 8, 16}},\n           {TensorCoreType::FP32_BF16_BF16_FP32, {16, 8, 16}},\n           {TensorCoreType::FP32_TF32_TF32_FP32, {16, 8, 8}},\n+          {TensorCoreType::FP16_FP16_FP16_FP16, {16, 8, 16}},\n \n           {TensorCoreType::INT32_INT1_INT1_INT32, {16, 8, 256}},\n           {TensorCoreType::INT32_INT4_INT4_INT32, {16, 8, 64}},\n@@ -217,6 +224,7 @@ struct DotOpMmaV2ConversionHelper {\n           {TensorCoreType::FP32_FP16_FP16_FP32, {8, 8, 8}},\n           {TensorCoreType::FP32_BF16_BF16_FP32, {8, 8, 8}},\n           {TensorCoreType::FP32_TF32_TF32_FP32, {8, 8, 4}},\n+          {TensorCoreType::FP16_FP16_FP16_FP16, {8, 8, 8}},\n \n           {TensorCoreType::INT32_INT1_INT1_INT32, {8, 8, 64}},\n           {TensorCoreType::INT32_INT4_INT4_INT32, {8, 8, 32}},\n@@ -234,6 +242,8 @@ struct DotOpMmaV2ConversionHelper {\n        \"mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\"},\n       {TensorCoreType::FP32_TF32_TF32_FP32,\n        \"mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\"},\n+      {TensorCoreType::FP16_FP16_FP16_FP16,\n+       \"mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\"},\n \n       {TensorCoreType::INT32_INT1_INT1_INT32,\n        \"mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc\"},\n@@ -248,6 +258,7 @@ struct DotOpMmaV2ConversionHelper {\n       {TensorCoreType::FP32_FP16_FP16_FP32, 8},\n       {TensorCoreType::FP32_BF16_BF16_FP32, 8},\n       {TensorCoreType::FP32_TF32_TF32_FP32, 4},\n+      {TensorCoreType::FP16_FP16_FP16_FP16, 8},\n \n       {TensorCoreType::INT32_INT1_INT1_INT32, 128},\n       {TensorCoreType::INT32_INT4_INT4_INT32, 32},\n@@ -366,6 +377,20 @@ struct MMA16816ConversionHelper {\n     warp = udiv(thread, _32);\n   }\n \n+  MMA16816ConversionHelper(DotOp op, MmaEncodingAttr mmaLayout, Value thread,\n+                           ConversionPatternRewriter &rewriter,\n+                           TritonGPUToLLVMTypeConverter *typeConverter,\n+                           Location loc)\n+      : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()), thread(thread),\n+        helper(mmaLayout), rewriter(rewriter), typeConverter(typeConverter),\n+        loc(loc), ctx(mmaLayout.getContext()) {\n+    helper.deduceMmaType(op);\n+\n+    Value _32 = i32_val(32);\n+    lane = urem(thread, _32);\n+    warp = udiv(thread, _32);\n+  }\n+\n   // Get a warpId for M axis.\n   Value getWarpM(int M) const {\n     auto matInstrShape = helper.getMmaInstrShape();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "file_content_changes": "@@ -33,8 +33,13 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                                     .getEncoding()\n                                     .dyn_cast<MmaEncodingAttr>();\n     if (!isOuter && mmaLayout && supportMMA(op, mmaLayout.getVersionMajor())) {\n-      if (mmaLayout.isVolta())\n+      if (mmaLayout.isVolta()) {\n+        if (D.getType().cast<RankedTensorType>().getElementType().isF16()) {\n+          llvm_unreachable(\n+              \"out_dtype=float16 for dot is not implemented on V100 yet\");\n+        }\n         return convertMMA884(op, adaptor, rewriter);\n+      }\n       if (mmaLayout.isAmpere())\n         return convertMMA16816(op, adaptor, rewriter);\n \n@@ -67,7 +72,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     Value B = op.getB();\n     Value C = op.getC();\n \n-    MMA16816ConversionHelper mmaHelper(A.getType(), mmaLayout,\n+    MMA16816ConversionHelper mmaHelper(op, mmaLayout,\n                                        getThreadId(rewriter, loc), rewriter,\n                                        getTypeConverter(), loc);\n "}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -946,6 +946,12 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n                                                 view.getResult());\n     return mlir::success();\n   }\n+  // cvt(cat) -> cat\n+  if (auto cat = dyn_cast<triton::CatOp>(arg)) {\n+    rewriter.replaceOpWithNewOp<triton::CatOp>(op, op->getResult(0).getType(),\n+                                               cat.getOperands());\n+    return mlir::success();\n+  }\n   // cvt(alloc_tensor(x), type2) -> alloc_tensor(x, type2)\n   auto alloc_tensor = dyn_cast<triton::gpu::AllocTensorOp>(arg);\n   if (alloc_tensor) {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 13, "deletions": 2, "changes": 15, "file_content_changes": "@@ -555,8 +555,19 @@ class ConvertDotConvert : public mlir::RewritePattern {\n       return mlir::failure();\n \n     // TODO: int tensor cores\n-    auto _0f = rewriter.create<arith::ConstantFloatOp>(\n-        op->getLoc(), APFloat(0.0f), dstTy.getElementType().cast<FloatType>());\n+    auto out_dtype = dstTy.getElementType().cast<FloatType>();\n+    APFloat value(0.0f);\n+    if (out_dtype.isBF16())\n+      value = APFloat(APFloat::IEEEhalf(), APInt(16, 0));\n+    else if (out_dtype.isF16())\n+      value = APFloat(APFloat::IEEEhalf(), APInt(16, 0));\n+    else if (out_dtype.isF32())\n+      value = APFloat(0.0f);\n+    else\n+      llvm_unreachable(\"unsupported data type\");\n+\n+    auto _0f =\n+        rewriter.create<arith::ConstantFloatOp>(op->getLoc(), value, out_dtype);\n     auto _0 = rewriter.create<triton::SplatOp>(\n         op->getLoc(), dotOp.getResult().getType(), _0f);\n     auto newDot = rewriter.create<triton::DotOp>("}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -12,7 +12,6 @@\n #include \"mlir/Target/LLVMIR/Export.h\"\n #include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n #include \"mlir/Transforms/Passes.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Tools/Sys/GetEnv.hpp\"\n #include \"llvm/ADT/APInt.h\"\n@@ -296,7 +295,6 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n       /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n   pm.addPass(mlir::createConvertSCFToCFPass());\n-  pm.addPass(createTritonConvertArithToIndexPass());\n   pm.addPass(mlir::createConvertIndexToLLVMPass());\n   pm.addPass(createConvertTritonGPUToLLVMPass(computeCapability));\n   pm.addPass(mlir::createArithToLLVMConversionPass());"}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 10, "deletions": 5, "changes": 15, "file_content_changes": "@@ -1,6 +1,5 @@\n #include \"triton/Target/PTX/PTXTranslation.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n-#include <optional>\n \n #include \"llvm/IR/IRBuilder.h\"\n #include \"llvm/IR/LegacyPassManager.h\"\n@@ -12,13 +11,19 @@\n #include \"llvm/Support/TargetSelect.h\"\n #include \"llvm/Target/TargetMachine.h\"\n \n+#include <mutex>\n+#include <optional>\n+\n namespace triton {\n \n static void initLLVM() {\n-  LLVMInitializeNVPTXTargetInfo();\n-  LLVMInitializeNVPTXTarget();\n-  LLVMInitializeNVPTXTargetMC();\n-  LLVMInitializeNVPTXAsmPrinter();\n+  static std::once_flag init_flag;\n+  std::call_once(init_flag, []() {\n+    LLVMInitializeNVPTXTargetInfo();\n+    LLVMInitializeNVPTXTarget();\n+    LLVMInitializeNVPTXTargetMC();\n+    LLVMInitializeNVPTXAsmPrinter();\n+  });\n }\n \n static bool findAndReplace(std::string &str, const std::string &begin,"}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -69,7 +69,7 @@ def get_llvm_package_info():\n     use_assert_enabled_llvm = check_env_flag(\"TRITON_USE_ASSERT_ENABLED_LLVM\", \"False\")\n     release_suffix = \"assert\" if use_assert_enabled_llvm else \"release\"\n     name = f'llvm+mlir-17.0.0-x86_64-{system_suffix}-{release_suffix}'\n-    version = \"llvm-17.0.0-8e5a41e8271f\"\n+    version = \"llvm-17.0.0-2538e550420f\"\n     url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n     return Package(\"llvm\", name, url, \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n "}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 51, "deletions": 26, "changes": 77, "file_content_changes": "@@ -1173,15 +1173,17 @@ def kernel(X, stride_xm, stride_xn,\n # ---------------\n \n \n-@pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype\",\n-                         [(*shape, 4, False, False, epilogue, allow_tf32, dtype)\n+@pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype\",\n+                         [(*shape, 4, False, False, epilogue, allow_tf32, in_dtype, out_dtype)\n                           for shape in [(64, 64, 64), (16, 16, 16)]\n                           for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n                           for allow_tf32 in [True, False]\n-                          for dtype in ['float16', 'float32']\n-                          if not (allow_tf32 and (dtype in ['float16']))] +\n+                          for in_dtype, out_dtype in [('float16', 'float16'),\n+                                                      ('float16', 'float32'),\n+                                                      ('float32', 'float32')]\n+                          if not (allow_tf32 and (in_dtype in ['float16']))] +\n \n-                         [(*shape_nw, col_a, col_b, 'none', allow_tf32, dtype)\n+                         [(*shape_nw, col_a, col_b, 'none', allow_tf32, in_dtype, out_dtype)\n                           for shape_nw in [[128, 256, 32, 8],\n                                            [128, 16, 32, 4],\n                                            [32, 128, 64, 4],\n@@ -1194,19 +1196,25 @@ def kernel(X, stride_xm, stride_xn,\n                           for allow_tf32 in [True]\n                           for col_a in [True, False]\n                           for col_b in [True, False]\n-                          for dtype in ['int8', 'float16', 'float32']])\n-def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype, device='cuda'):\n+                          for in_dtype, out_dtype in [('int8', 'int8'),\n+                                                      ('float16', 'float16'),\n+                                                      ('float16', 'float32'),\n+                                                      ('float32', 'float32')]])\n+def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype, device='cuda'):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if capability[0] < 8:\n-        if dtype == 'int8':\n+        if in_dtype == 'int8':\n             pytest.skip(\"Only test int8 on devices with sm >= 80\")\n-        elif dtype == 'float32' and allow_tf32:\n+        elif in_dtype == 'float32' and allow_tf32:\n             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n     if capability[0] == 7:\n         if (M, N, K, num_warps) == (128, 256, 32, 8):\n             pytest.skip(\"shared memory out of resource\")\n+        if out_dtype == 'float16':\n+            # TODO: support out_dtype=float16 for tl.dot on V100\n+            pytest.skip(\"Only test out_dtype=float16 on devices with sm >=80\")\n \n     torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n \n@@ -1216,6 +1224,7 @@ def kernel(X, stride_xm, stride_xk,\n                Y, stride_yk, stride_yn,\n                W, stride_wn, stride_wl,\n                Z, stride_zm, stride_zn,\n+               out_dtype: tl.constexpr,\n                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n                ALLOW_TF32: tl.constexpr,\n@@ -1231,7 +1240,7 @@ def kernel(X, stride_xm, stride_xk,\n         Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n         x = tl.load(Xs)\n         y = tl.load(Ys)\n-        z = tl.dot(x, y, allow_tf32=ALLOW_TF32)\n+        z = tl.dot(x, y, allow_tf32=ALLOW_TF32, out_dtype=out_dtype)\n         if ADD_MATRIX:\n             z += tl.load(Zs)\n         if ADD_ROWS:\n@@ -1248,42 +1257,54 @@ def kernel(X, stride_xm, stride_xk,\n             z = num / den[:, None]\n         if CHAIN_DOT:\n             w = tl.load(Ws)\n-            z = tl.dot(z.to(w.dtype), w)\n+            z = tl.dot(z.to(w.dtype), w, out_dtype=out_dtype)\n         tl.store(Zs, z)\n     # input\n     rs = RandomState(17)\n     if col_a:\n-        x = numpy_random((K, M), dtype_str=dtype, rs=rs).T\n+        x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T\n     else:\n-        x = numpy_random((M, K), dtype_str=dtype, rs=rs)\n+        x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)\n     if col_b:\n-        y = numpy_random((N, K), dtype_str=dtype, rs=rs).T\n+        y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T\n     else:\n-        y = numpy_random((K, N), dtype_str=dtype, rs=rs)\n-    w = numpy_random((N, N), dtype_str=dtype, rs=rs)\n-    if 'int' not in dtype:\n+        y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)\n+    w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)\n+    if 'int' not in in_dtype:\n         x *= .1\n         y *= .1\n-    if dtype == 'float32' and allow_tf32:\n+    if in_dtype == 'float32' and allow_tf32:\n         x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n         y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n         w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n     x_tri = to_triton(x, device=device)\n     y_tri = to_triton(y, device=device)\n     w_tri = to_triton(w, device=device)\n     # triton result\n-    if dtype == 'int8':\n+    if out_dtype == 'int8':\n         z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)\n     else:\n-        z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs) * .1\n+        z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1\n \n     z_tri = to_triton(z, device=device)\n     if epilogue == 'trans':\n         z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])\n+\n+    if out_dtype == 'int8':\n+        out_dtype = tl.int8\n+    elif out_dtype == 'float16' and epilogue != 'softmax':\n+        # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will\n+        # fail with the following error: 'llvm.fmul' op requires the same type\n+        # for all operands and results\n+        out_dtype = tl.float16\n+    else:\n+        out_dtype = tl.float32\n+\n     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n                          y_tri, y_tri.stride(0), y_tri.stride(1),\n                          w_tri, w_tri.stride(0), w_tri.stride(1),\n                          z_tri, z_tri.stride(0), z_tri.stride(1),\n+                         out_dtype,\n                          COL_A=col_a, COL_B=col_b,\n                          BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n                          ADD_MATRIX=epilogue == 'add-matrix',\n@@ -1294,7 +1315,7 @@ def kernel(X, stride_xm, stride_xk,\n                          ALLOW_TF32=allow_tf32,\n                          num_warps=num_warps)\n     # torch result\n-    if dtype == 'int8':\n+    if in_dtype == 'int8':\n         z_ref = np.matmul(x.astype(np.float32),\n                           y.astype(np.float32())).astype(np.int32)\n     else:\n@@ -1314,9 +1335,11 @@ def kernel(X, stride_xm, stride_xk,\n         z_ref = np.matmul(z_ref, w)\n     # compare\n     # print(z_ref[:,0], z_tri[:,0])\n-    if dtype == 'float32':\n+    if in_dtype == 'float32':\n         # XXX: Somehow there's a larger difference when we use float32\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n+    elif out_dtype == tl.float16:\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n     else:\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n     # make sure ld/st are vectorized\n@@ -1325,12 +1348,14 @@ def kernel(X, stride_xm, stride_xk,\n         # XXX: skip small sizes because they are not vectorized\n         assert 'ld.global.v4' in ptx\n         assert 'st.global.v4' in ptx\n-    if dtype == 'float32' and allow_tf32:\n+    if in_dtype == 'float32' and allow_tf32:\n         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n-    elif dtype == 'float32' and allow_tf32:\n+    elif in_dtype == 'float32' and allow_tf32:\n         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' not in ptx\n-    elif dtype == 'int8':\n+    elif in_dtype == 'int8':\n         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n+    elif out_dtype == tl.float16:\n+        assert 'mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16' in ptx\n \n \n @pytest.mark.parametrize(\"dtype_str\", int_dtypes + float_dtypes + ['bfloat16'])\n@@ -1467,7 +1492,7 @@ def _kernel(in1_ptr, in2_ptr, output_ptr,\n         w = tl.load(in2_ptr + in2_offsets, mask=in2_offsets < in2_numel)\n \n         # Without a dot product the memory doesn't get promoted to shared.\n-        o = tl.dot(x, w)\n+        o = tl.dot(x, w, out_dtype=tl.float32)\n \n         # Store output\n         output_offsets = M_offsets[:, None] * out_stride + N_offsets[None, :]"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 6, "deletions": 8, "changes": 14, "file_content_changes": "@@ -701,14 +701,15 @@ def visit_For(self, node):\n         iv_type = triton.language.semantic.integer_promote_impl(lb.dtype, ub.dtype)\n         iv_type = triton.language.semantic.integer_promote_impl(iv_type, step.dtype)\n         iv_ir_type = iv_type.to_ir(self.builder)\n+        iv_is_signed = iv_type.int_signedness == triton.language.core.dtype.SIGNEDNESS.SIGNED\n         # lb/ub/step might be constexpr, we need to cast them to tensor\n         lb = lb.handle\n         ub = ub.handle\n         step = step.handle\n         # ForOp can only accept IndexType as lb/ub/step. Cast integer to Index\n-        lb = self.builder.create_to_index(lb)\n-        ub = self.builder.create_to_index(ub)\n-        step = self.builder.create_to_index(step)\n+        lb = self.builder.create_int_cast(lb, iv_ir_type, iv_is_signed)\n+        ub = self.builder.create_int_cast(ub, iv_ir_type, iv_is_signed)\n+        step = self.builder.create_int_cast(step, iv_ir_type, iv_is_signed)\n         # Create placeholder for the loop induction variable\n         iv = self.builder.create_undef(iv_ir_type)\n         self.set_value(node.target.id, triton.language.core.tensor(iv, iv_type))\n@@ -767,12 +768,9 @@ def visit_For(self, node):\n \n             # update induction variable with actual value, and replace all uses\n             self.builder.set_insertion_point_to_start(for_op.get_body(0))\n-            iv = self.builder.create_index_to_si(for_op.get_induction_var())\n-            iv = self.builder.create_int_cast(iv, iv_ir_type, True)\n+            iv = for_op.get_induction_var()\n             if negative_step:\n-                ub_si = self.builder.create_index_to_si(ub)\n-                ub_si = self.builder.create_int_cast(ub_si, iv_ir_type, True)\n-                iv = self.builder.create_sub(ub_si, iv)\n+                iv = self.builder.create_sub(ub, iv)\n             self.lscope[node.target.id].handle.replace_all_uses_with(iv)\n             self.set_value(node.target.id, triton.language.core.tensor(iv, iv_type))\n "}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -862,7 +862,7 @@ def reshape(input, shape, _builder=None):\n \n \n @builtin\n-def dot(input, other, allow_tf32=True, _builder=None):\n+def dot(input, other, allow_tf32=True, out_dtype=float32, _builder=None):\n     \"\"\"\n     Returns the matrix product of two blocks.\n \n@@ -874,7 +874,8 @@ def dot(input, other, allow_tf32=True, _builder=None):\n     :type other: 2D tensor of scalar-type in {:code:`float16`, :code:`bfloat16`, :code:`float32`}\n     \"\"\"\n     allow_tf32 = _constexpr_to_value(allow_tf32)\n-    return semantic.dot(input, other, allow_tf32, _builder)\n+    out_dtype = _constexpr_to_value(out_dtype)\n+    return semantic.dot(input, other, allow_tf32, out_dtype, _builder)\n \n \n # -----------------------"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -1051,6 +1051,7 @@ def atomic_xchg(ptr: tl.tensor,\n def dot(lhs: tl.tensor,\n         rhs: tl.tensor,\n         allow_tf32: bool,\n+        out_dtype: tl.dtype,\n         builder: ir.builder) -> tl.tensor:\n     assert lhs.type.is_block() and rhs.type.is_block()\n     assert lhs.dtype == rhs.dtype, \"lhs and rhs must have the same dtype!\"\n@@ -1062,9 +1063,13 @@ def dot(lhs: tl.tensor,\n     if lhs.type.scalar.is_int():\n         _0 = builder.get_int32(0)\n         ret_scalar_ty = tl.int32\n-    else:\n+    elif lhs.type.scalar.is_fp32() or lhs.type.scalar.is_bf16():\n         _0 = builder.get_fp32(0)\n         ret_scalar_ty = tl.float32\n+    else:\n+        _0 = builder.get_fp16(0) if out_dtype.is_fp16() else builder.get_fp32(0)\n+        ret_scalar_ty = out_dtype\n+\n     M = lhs.type.shape[0]\n     N = rhs.type.shape[1]\n     _0 = builder.create_splat(_0, [M, N])"}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -64,7 +64,7 @@ def _sdd_kernel(\n         else:\n             a = tl.load(a_ptrs, mask=offs_ak[None, :] < k, other=0.)\n             b = tl.load(b_ptrs, mask=offs_bk[:, None] < k, other=0.)\n-        acc += tl.dot(a, b)\n+        acc += tl.dot(a, b, out_dtype=tl.float32)\n         a_ptrs += TILE_K * stride_ak\n         b_ptrs += TILE_K * stride_bk\n     c = acc.to(C.dtype.element_ty)\n@@ -183,7 +183,7 @@ def _dsd_kernel(\n     for k in range(K, 0, -TILE_K):\n         a = tl.load(pa)\n         b = tl.load(pb)\n-        acc += tl.dot(a, b)\n+        acc += tl.dot(a, b, out_dtype=tl.float32)\n         pa += inc_a\n         pb += inc_b * stride_bk\n         pinc += 2"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 21, "deletions": 9, "changes": 30, "file_content_changes": "@@ -64,9 +64,9 @@ def _kernel(A, B, C, M, N, K,\n             stride_am, stride_ak,\n             stride_bk, stride_bn,\n             stride_cm, stride_cn,\n+            dot_out_dtype: tl.constexpr,\n             BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n             GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr,\n-            ACC_TYPE: tl.constexpr\n             ):\n     # matrix multiplication\n     pid = tl.program_id(0)\n@@ -88,7 +88,7 @@ def _kernel(A, B, C, M, N, K,\n     # pointers\n     A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n     B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n-    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n+    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=dot_out_dtype)\n     for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n         if EVEN_K:\n             a = tl.load(A)\n@@ -97,7 +97,7 @@ def _kernel(A, B, C, M, N, K,\n             k_remaining = K - k * (BLOCK_K * SPLIT_K)\n             a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.)\n             b = tl.load(B, mask=rk[:, None] < k_remaining, other=0.)\n-        acc += tl.dot(a, b)\n+        acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n         A += BLOCK_K * SPLIT_K * stride_ak\n         B += BLOCK_K * SPLIT_K * stride_bk\n     acc = acc.to(C.dtype.element_ty)\n@@ -119,7 +119,7 @@ class _matmul(torch.autograd.Function):\n     _locks = {}\n \n     @staticmethod\n-    def _call(a, b):\n+    def _call(a, b, dot_out_dtype):\n         device = a.device\n         # handle non-contiguous inputs if necessary\n         if a.stride(0) > 1 and a.stride(1) > 1:\n@@ -132,20 +132,32 @@ def _call(a, b):\n         _, N = b.shape\n         # allocates output\n         c = torch.empty((M, N), device=device, dtype=a.dtype)\n-        # accumulator types\n-        ACC_TYPE = tl.float32 if a.dtype in [torch.float16, torch.bfloat16, torch.float32] else tl.int32\n+        if dot_out_dtype is None:\n+            if a.dtype in [torch.float16, torch.float32, torch.bfloat16]:\n+                dot_out_dtype = tl.float32\n+            else:\n+                dot_out_dtype = tl.int32\n+        else:\n+            assert isinstance(dot_out_dtype, torch.dtype), \"dot_out_dtype must be a torch.dtype\"\n+            if dot_out_dtype == torch.float16:\n+                dot_out_dtype = tl.float16\n+            elif dot_out_dtype in [torch.float32, torch.bfloat16]:\n+                dot_out_dtype = tl.float32\n+            else:\n+                dot_out_dtype = tl.int32\n         # launch kernel\n         grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']), META['SPLIT_K'])\n         _kernel[grid](a, b, c, M, N, K,\n                       a.stride(0), a.stride(1),\n                       b.stride(0), b.stride(1),\n                       c.stride(0), c.stride(1),\n-                      GROUP_M=8, ACC_TYPE=ACC_TYPE)\n+                      dot_out_dtype=dot_out_dtype,\n+                      GROUP_M=8)\n         return c\n \n     @staticmethod\n-    def forward(ctx, a, b):\n-        return _matmul._call(a, b)\n+    def forward(ctx, a, b, dot_out_dtype=None):\n+        return _matmul._call(a, b, dot_out_dtype=dot_out_dtype)\n \n \n matmul = _matmul.apply"}]