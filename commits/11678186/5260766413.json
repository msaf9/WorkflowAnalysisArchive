[{"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -6,12 +6,12 @@\n from typing import Any, Callable, Dict, Optional, Tuple, Type, Union\n \n from .. import language\n+from .._C.libtriton.triton import ir\n from ..language import constexpr, tensor\n # ideally we wouldn't need any runtime component\n from ..runtime import JITFunction\n from .errors import (CompilationError, CompileTimeAssertionFailure,\n                      UnsupportedLanguageConstruct)\n-from .._C.libtriton.triton import ir\n \n \n def mangle_ty(ty):"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -12,14 +12,18 @@\n from typing import Any, Tuple\n \n # import triton\n-from .._C.libtriton.triton import ir, add_external_libs, translate_triton_gpu_to_llvmir, translate_llvmir_to_ptx, compile_ptx_to_cubin, translate_llvmir_to_hsaco, get_shared_memory_size\n+from .._C.libtriton.triton import (add_external_libs, compile_ptx_to_cubin,\n+                                   get_shared_memory_size, ir,\n+                                   translate_llvmir_to_hsaco, translate_llvmir_to_ptx,\n+                                   translate_triton_gpu_to_llvmir)\n from ..common.backend import get_backend\n-from ..runtime.driver import driver\n-from ..runtime.jit import jit, JITFunction, get_current_device, get_device_capability, version_key, get_cuda_stream\n # from ..runtime import driver, jit, JITFunction\n # TODO: runtime.errors\n from ..runtime.autotuner import OutOfResources\n from ..runtime.cache import get_cache_manager\n+from ..runtime.driver import driver\n+from ..runtime.jit import (JITFunction, get_cuda_stream, get_current_device,\n+                           get_device_capability, version_key)\n from ..tools.disasm import extract\n from .code_generator import ast_to_ttir\n from .make_launcher import make_stub"}, {"filename": "python/triton/debugger/debugger.py", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -2,16 +2,17 @@\n import random\n from typing import Tuple\n \n-# import triton\n-\n+from .. import language as tl\n # import .language.core as lcore\n from ..language import core as lcore\n-from .. import language as tl\n+from . import torch_wrapper\n from .core import ExecutionContext\n from .memory_map import MemoryMap\n from .tl_lang import (TritonLangProxy, WrappedTensor, _primitive_to_tensor,\n                       debugger_constexpr)\n-from . import torch_wrapper\n+\n+# import triton\n+\n \n torch = torch_wrapper.torch\n tl_method_backup = {}"}, {"filename": "python/triton/debugger/tl_lang.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -2,9 +2,9 @@\n \n # import triton\n from ..language import core as lcore\n+from . import torch_wrapper\n from .core import ExecutionContext\n from .memory_map import MemoryMap\n-from . import torch_wrapper\n \n torch = torch_wrapper.torch\n "}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -5,11 +5,10 @@\n from functools import wraps\n from typing import Callable, List, Sequence, TypeVar\n \n+from .._C.libtriton.triton import ir\n # import triton\n from ..runtime.jit import jit\n-\n from . import semantic\n-from .._C.libtriton.triton import ir\n \n T = TypeVar('T')\n "}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -1,10 +1,11 @@\n import torch\n \n+from ... import cdiv, heuristics, jit\n+from ... import language as tl\n+\n # import triton\n # import language as tl\n \n-from ... import jit, next_power_of_2, heuristics, cdiv\n-from ... import language as tl\n # ********************************************************\n # --------------------------------------------------------\n # Sparse = Dense x Dense (SDD)"}, {"filename": "python/triton/ops/blocksparse/softmax.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -2,8 +2,10 @@\n \n # import triton\n # import language as tl\n-from ... import jit, next_power_of_2\n+from ... import jit\n from ... import language as tl\n+from ... import next_power_of_2\n+\n \n def num_warps(n):\n     if n <= 128:"}, {"filename": "python/triton/ops/cross_entropy.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -2,8 +2,9 @@\n \n # import triton\n # import language as tl\n-from .. import jit, heuristics, next_power_of_2\n+from .. import heuristics, jit\n from .. import language as tl\n+from .. import next_power_of_2\n \n \n def num_warps(N):"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -7,11 +7,12 @@\n \n import torch\n \n+from .. import cdiv, jit\n+from .. import language as tl\n+\n # import triton\n # import language as tl\n \n-from .. import jit, heuristics, next_power_of_2, cdiv\n-from .. import language as tl\n \n @jit\n def _fwd_kernel("}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -1,12 +1,12 @@\n import torch\n \n-# import triton\n-# import language as tl\n-\n-from .. import jit, heuristics, next_power_of_2, Config, cdiv, autotune\n+from .. import Config, autotune, cdiv, heuristics, jit\n from .. import language as tl\n from .matmul_perf_model import early_config_prune, estimate_matmul_time\n \n+# import triton\n+# import language as tl\n+\n \n def init_to_zero(name):\n     return lambda nargs: nargs[name].zero_()\n@@ -21,11 +21,11 @@ def get_configs_io_bound():\n                     num_warps = 2 if block_n <= 64 else 4\n                     configs.append(\n                         Config({'BLOCK_M': block_m, 'BLOCK_N': block_n, 'BLOCK_K': block_k, 'SPLIT_K': 1},\n-                                      num_stages=num_stages, num_warps=num_warps))\n+                               num_stages=num_stages, num_warps=num_warps))\n                     # split_k\n                     for split_k in [2, 4, 8, 16]:\n                         configs.append(Config({'BLOCK_M': block_m, 'BLOCK_N': block_n, 'BLOCK_K': block_k, 'SPLIT_K': split_k},\n-                                                     num_stages=num_stages, num_warps=num_warps, pre_hook=init_to_zero('C')))\n+                                              num_stages=num_stages, num_warps=num_warps, pre_hook=init_to_zero('C')))\n     return configs\n \n "}, {"filename": "python/triton/ops/matmul_perf_model.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -4,9 +4,9 @@\n \n # import triton\n from .. import cdiv\n+from .._C.libtriton.triton import runtime\n from ..runtime import driver\n from ..testing import get_dram_gbps, get_max_simd_tflops, get_max_tensorcore_tflops\n-from .._C.libtriton.triton import runtime\n \n \n def get_tensorcore_tflops(backend, device, num_ctas, num_warps, dtype):"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -20,6 +20,7 @@\n TRITON_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n TRITON_VERSION = \"2.1.0\"\n \n+\n def get_cuda_stream(idx=None):\n     if idx is None:\n         idx = get_current_device()"}, {"filename": "python/triton/tools/aot.py", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -3,7 +3,10 @@\n \n from .._C.libtriton.triton import ir\n # import triton.compiler.compiler as tc\n-from ..compiler.compile import optimize_ttir, ttir_to_ttgir, optimize_ttgir, ttgir_to_llir, llir_to_amdgcn_and_hsaco, get_amdgpu_arch_fulldetails, llir_to_ptx\n+from ..compiler.compile import (get_amdgpu_arch_fulldetails, llir_to_amdgcn_and_hsaco,\n+                                llir_to_ptx, optimize_ttgir, optimize_ttir,\n+                                ttgir_to_llir, ttir_to_ttgir)\n+\n if __name__ == '__main__':\n \n     # valid source and target formats"}]