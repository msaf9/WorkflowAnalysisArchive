[{"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -478,8 +478,8 @@ struct TritonReducePattern : public OpConversionPattern<triton::ReduceOp> {\n     addNamedAttrs(newReduce, adaptor.getAttributes());\n \n     auto &newCombineOp = newReduce.getCombineOp();\n-    rewriter.inlineRegionBefore(op.getCombineOp(), newCombineOp,\n-                                newCombineOp.end());\n+    rewriter.cloneRegionBefore(op.getCombineOp(), newCombineOp,\n+                               newCombineOp.end());\n     rewriter.replaceOp(op, newReduce.getResult());\n     return success();\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -137,11 +137,12 @@ class SimplifyReduceCvt : public mlir::RewritePattern {\n           op->getLoc(), newTy, newOperands[i]);\n     }\n \n+    rewriter.setInsertionPoint(reduce);\n     auto newReduce = rewriter.create<triton::ReduceOp>(\n         op->getLoc(), newOperands, reduce.getAxis());\n     auto &newCombineOp = newReduce.getCombineOp();\n-    rewriter.inlineRegionBefore(reduce.getCombineOp(), newCombineOp,\n-                                newCombineOp.end());\n+    rewriter.cloneRegionBefore(reduce.getCombineOp(), newCombineOp,\n+                               newCombineOp.end());\n \n     SmallVector<Value> newRet = newReduce.getResult();\n     auto oldTypes = reduce.getResult().getType();"}, {"filename": "python/setup.py", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -203,6 +203,14 @@ def build_extension(self, ext):\n             max_jobs = os.getenv(\"MAX_JOBS\", str(2 * os.cpu_count()))\n             build_args += ['-j' + max_jobs]\n \n+        if check_env_flag(\"TRITON_BUILD_WITH_CLANG_LLD\"):\n+            cmake_args += [\"-DCMAKE_C_COMPILER=clang\",\n+                           \"-DCMAKE_CXX_COMPILER=clang++\",\n+                           \"-DCMAKE_LINKER=lld\",\n+                           \"-DCMAKE_EXE_LINKER_FLAGS=-fuse-ld=lld\",\n+                           \"-DCMAKE_MODULE_LINKER_FLAGS=-fuse-ld=lld\",\n+                           \"-DCMAKE_SHARED_LINKER_FLAGS=-fuse-ld=lld\"]\n+\n         env = os.environ.copy()\n         subprocess.check_call([\"cmake\", self.base_dir] + cmake_args, cwd=self.build_temp, env=env)\n         subprocess.check_call([\"cmake\", \"--build\", \".\"] + build_args, cwd=self.build_temp)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1183,7 +1183,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n # TODO: [Qingyi] Fix argmin / argmax\n reduce_configs1 = [\n     (op, dtype, (1, 1024), axis) for dtype in dtypes_with_bfloat16\n-    for op in ['min', 'max', 'sum']\n+    for op in ['min', 'max', 'sum', 'argmin', 'argmax']\n     for axis in [1]\n ]\n \n@@ -1199,7 +1199,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n reduce_configs2 = [\n     (op, 'float32', shape, axis)\n-    for op in ['min', 'max', 'sum']\n+    for op in ['min', 'max', 'sum', 'argmin', 'argmax']\n     for shape in reduce2d_shapes\n     for axis in [0, 1]\n ]"}, {"filename": "python/test/unit/operators/test_inductor.py", "status": "modified", "additions": 100, "deletions": 0, "changes": 100, "file_content_changes": "@@ -53,3 +53,103 @@ def triton_(in_out_ptr0, in_out_ptr1, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel\n     arg9_1 = torch.rand(64, device=\"cuda\")\n     triton_[(512,)](buf14, buf16, arg114_1, arg115_1, arg8_1, arg9_1, 512, 4096, 1, 2048)\n     torch.testing.assert_allclose(buf16.mean().item(), buf14.mean().item(), atol=1e-7, rtol=0)\n+\n+\n+def test_avg_pool_bw():\n+\n+    @triton.jit\n+    def triton_(in_ptr0, out_ptr0, XBLOCK: tl.constexpr):\n+        xoffset = tl.program_id(0) * XBLOCK\n+        xindex = xoffset + tl.arange(0, XBLOCK)[:]\n+        x1 = (xindex // 8) % 8\n+        x0 = xindex % 8\n+        x2 = (xindex // 64)\n+        x5 = xindex\n+        tmp0 = (-1) + x1\n+        tmp1 = (-1) + x0\n+        tmp2 = 2 + x1\n+        tmp3 = 2 + x0\n+        tmp4 = 0\n+        tmp5 = tl.where(tmp0 != tmp0, tmp0, tl.where(tmp0 > tmp4, tmp0, tmp4))\n+        tmp6 = tl.where(tmp1 != tmp1, tmp1, tl.where(tmp1 > tmp4, tmp1, tmp4))\n+        tmp7 = 8\n+        tmp8 = tl.where(tmp2 != tmp2, tmp2, tl.where(tmp2 < tmp7, tmp2, tmp7))\n+        tmp9 = tl.where(tmp3 != tmp3, tmp3, tl.where(tmp3 < tmp7, tmp3, tmp7))\n+        tmp10 = tmp5 + tmp4\n+        tmp11 = tmp6 + tmp4\n+        tmp12 = 1\n+        tmp13 = tmp8 - tmp12\n+        tmp14 = tl.where(tmp10 != tmp10, tmp10, tl.where(tmp10 < tmp13, tmp10, tmp13))\n+        tmp15 = tmp9 - tmp12\n+        tmp16 = tl.where(tmp11 != tmp11, tmp11, tl.where(tmp11 < tmp15, tmp11, tmp15))\n+        tmp17 = tl.load(in_ptr0 + (tmp16 + (8 * tmp14) + (64 * x2)), None).to(tl.float32)\n+        tmp18 = tmp17 / 9\n+        tmp19 = tmp10 < tmp8\n+        tmp20 = tmp11 < tmp9\n+        tmp21 = tmp19 & tmp20\n+        tmp22 = 0.0\n+        tmp23 = tl.where(tmp21, tmp18, tmp22)\n+        tmp24 = tmp6 + tmp12\n+        tmp25 = tl.where(tmp24 != tmp24, tmp24, tl.where(tmp24 < tmp15, tmp24, tmp15))\n+        tmp26 = tl.load(in_ptr0 + (tmp25 + (8 * tmp14) + (64 * x2)), None).to(tl.float32)\n+        tmp27 = tmp26 / 9\n+        tmp28 = tmp24 < tmp9\n+        tmp29 = tmp19 & tmp28\n+        tmp30 = tmp23 + tmp27\n+        tmp31 = tl.where(tmp29, tmp30, tmp23)\n+        tmp32 = 2\n+        tmp33 = tmp6 + tmp32\n+        tmp34 = tl.where(tmp33 != tmp33, tmp33, tl.where(tmp33 < tmp15, tmp33, tmp15))\n+        tmp35 = tl.load(in_ptr0 + (tmp34 + (8 * tmp14) + (64 * x2)), None).to(tl.float32)\n+        tmp36 = tmp35 / 9\n+        tmp37 = tmp33 < tmp9\n+        tmp38 = tmp19 & tmp37\n+        tmp39 = tmp31 + tmp36\n+        tmp40 = tl.where(tmp38, tmp39, tmp31)\n+        tmp41 = tmp5 + tmp12\n+        tmp42 = tl.where(tmp41 != tmp41, tmp41, tl.where(tmp41 < tmp13, tmp41, tmp13))\n+        tmp43 = tl.load(in_ptr0 + (tmp16 + (8 * tmp42) + (64 * x2)), None).to(tl.float32)\n+        tmp44 = tmp43 / 9\n+        tmp45 = tmp41 < tmp8\n+        tmp46 = tmp45 & tmp20\n+        tmp47 = tmp40 + tmp44\n+        tmp48 = tl.where(tmp46, tmp47, tmp40)\n+        tmp49 = tl.load(in_ptr0 + (tmp25 + (8 * tmp42) + (64 * x2)), None).to(tl.float32)\n+        tmp50 = tmp49 / 9\n+        tmp51 = tmp45 & tmp28\n+        tmp52 = tmp48 + tmp50\n+        tmp53 = tl.where(tmp51, tmp52, tmp48)\n+        tmp54 = tl.load(in_ptr0 + (tmp34 + (8 * tmp42) + (64 * x2)), None).to(tl.float32)\n+        tmp55 = tmp54 / 9\n+        tmp56 = tmp45 & tmp37\n+        tmp57 = tmp53 + tmp55\n+        tmp58 = tl.where(tmp56, tmp57, tmp53)\n+        tmp59 = tmp5 + tmp32\n+        tmp60 = tl.where(tmp59 != tmp59, tmp59, tl.where(tmp59 < tmp13, tmp59, tmp13))\n+        tmp61 = tl.load(in_ptr0 + (tmp16 + (8 * tmp60) + (64 * x2)), None).to(tl.float32)\n+        tmp62 = tmp61 / 9\n+        tmp63 = tmp59 < tmp8\n+        tmp64 = tmp63 & tmp20\n+        tmp65 = tmp58 + tmp62\n+        tmp66 = tl.where(tmp64, tmp65, tmp58)\n+        tmp67 = tl.load(in_ptr0 + (tmp25 + (8 * tmp60) + (64 * x2)), None).to(tl.float32)\n+        tmp68 = tmp67 / 9\n+        tmp69 = tmp63 & tmp28\n+        tmp70 = tmp66 + tmp68\n+        tmp71 = tl.where(tmp69, tmp70, tmp66)\n+        tmp72 = tl.load(in_ptr0 + (tmp34 + (8 * tmp60) + (64 * x2)), None).to(tl.float32)\n+        tmp73 = tmp72 / 9\n+        tmp74 = tmp63 & tmp37\n+        tmp75 = tmp71 + tmp73\n+        tmp76 = tl.where(tmp74, tmp75, tmp71)\n+        tl.store(out_ptr0 + (x5 + tl.zeros([XBLOCK], tl.int32)), tmp76, None)\n+\n+    inp = torch.ones(8, 2048, 8, 8, device=\"cuda\", dtype=torch.half)\n+    out = torch.ones_like(inp) * 3\n+    numel = inp.numel()\n+    triton_[(numel // 1024,)](inp, out, 1024)\n+    out_ref = torch.ones_like(inp)\n+    out_ref[:, :, 1:7, 0::7] = 2 / 3\n+    out_ref[:, :, 0::7, 1:7] = 2 / 3\n+    out_ref[:, :, 0::7, 0::7] = 4 / 9\n+    torch.testing.assert_allclose(out, out_ref)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -704,7 +704,7 @@ def __getitem__(self, slices, _builder=None):\n             elif sl == slice(None, None, None):\n                 pass\n             else:\n-                assert False, \"unsupported\"\n+                assert False, f\"unsupported tensor index: {sl}\"\n         return ret\n \n     @property\n@@ -1281,7 +1281,7 @@ def _argreduce(input, axis, combine_fn, _builder=None, _generator=None):\n \n     if len(input.shape) > 1:\n         # Broadcast index across the non-reduced axes\n-        expand_dims_index = [None] * len(input.shape)\n+        expand_dims_index = [constexpr(None)] * len(input.shape)\n         expand_dims_index[axis] = slice(None)\n         index = index.__getitem__(expand_dims_index, _builder=_builder)\n         index = broadcast_to(index, input.shape, _builder=_builder)\n@@ -1597,11 +1597,11 @@ def extern_elementwise(lib_name: str, lib_path: str, args: list, arg_type_symbol\n         # Get the broadcast shape over all the arguments\n         for i, item in enumerate(dispatch_args):\n             _, broadcast_arg = semantic.binary_op_type_checking_impl(\n-                item, broadcast_arg, _builder)\n+                item, broadcast_arg, _builder, arithmetic_check=False)\n         # Change the shape of each argument based on the broadcast shape\n         for i in range(len(dispatch_args)):\n             dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n-                dispatch_args[i], broadcast_arg, _builder)\n+                dispatch_args[i], broadcast_arg, _builder, arithmetic_check=False)\n         ret_shape = broadcast_arg.shape\n     func = getattr(_builder, \"create_extern_elementwise\")\n     return dispatch(func, lib_name, lib_path, dispatch_args, arg_type_symbol_dict, ret_shape, is_pure, _builder)"}]