[{"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -931,12 +931,13 @@ def _store_block_pointer(ptr, val, mask, boundary_check, cache, eviction, builde\n     if mask:\n         raise ValueError(\"`mask` and `other` arguments cannot be specified for loading block pointers\")\n \n-    # Check same shape\n+    # Check same shape and element type\n     block_shape = ptr.type.element_ty.get_block_shapes()\n     if not val.type.is_block():\n         val = broadcast_impl_shape(val, block_shape, builder)\n     assert val.type.is_block(), \"Value argument must be block type or a scalar\"\n     assert block_shape == val.type.get_block_shapes(), \"Block shape and value shape mismatch\"\n+    assert ptr.type.element_ty.element_ty == val.type.element_ty, \"Block element type and value element type mismatch\"\n \n     elt_ty = ptr.type.element_ty.element_ty\n     assert elt_ty != tl.int1, \"`tl.int1` should be rewrited in `tl.make_block_ptr`\""}, {"filename": "python/tutorials/01-vector-add.py", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -5,9 +5,13 @@\n In this tutorial, you will write a simple vector addition using Triton.\n \n In doing so, you will learn about:\n-- The basic programming model of Triton.\n-- The `triton.jit` decorator, which is used to define Triton kernels.\n-- The best practices for validating and benchmarking your custom ops against native reference implementations.\n+\n+* The basic programming model of Triton.\n+\n+* The `triton.jit` decorator, which is used to define Triton kernels.\n+\n+* The best practices for validating and benchmarking your custom ops against native reference implementations.\n+\n \"\"\"\n \n # %%"}, {"filename": "python/tutorials/02-fused-softmax.py", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -7,8 +7,11 @@\n the GPU's SRAM.\n \n In doing so, you will learn about:\n-- The benefits of kernel fusion for bandwidth-bound operations.\n-- Reduction operators in Triton.\n+\n+* The benefits of kernel fusion for bandwidth-bound operations.\n+\n+* Reduction operators in Triton.\n+\n \"\"\"\n \n # %%"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 86, "deletions": 84, "changes": 170, "file_content_changes": "@@ -1,14 +1,19 @@\n \"\"\"\n Matrix Multiplication\n =====================\n-In this tutorial, you will write a 25-lines high-performance FP16 matrix multiplication\n-kernel that achieves performance on par with cuBLAS.\n+In this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\n+performance on parallel with cuBLAS.\n+\n+You will specifically learn about:\n+\n+* Block-level matrix multiplications.\n+\n+* Multi-dimensional pointer arithmetics.\n+\n+* Program re-ordering for improved L2 cache hit rate.\n+\n+* Automatic performance tuning.\n \n-In doing so, you will learn about:\n-- Block-level matrix multiplications\n-- Multi-dimensional pointer arithmetic\n-- Program re-ordering for improved L2 cache hit rate\n-- Automatic performance tuning\n \"\"\"\n \n # %%\n@@ -28,16 +33,16 @@\n #\n #  .. code-block:: python\n #\n-#    # do in parallel\n+#    # Do in parallel\n #    for m in range(0, M, BLOCK_SIZE_M):\n-#      # do in parallel\n+#      # Do in parallel\n #      for n in range(0, N, BLOCK_SIZE_N):\n #        acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32)\n #        for k in range(0, K, BLOCK_SIZE_K):\n #          a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K]\n #          b = B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]\n #          acc += dot(a, b)\n-#        C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc;\n+#        C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc\n #\n # where each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance.\n \n@@ -48,11 +53,10 @@\n # The above algorithm is, actually, fairly straightforward to implement in Triton.\n # The main difficulty comes from the computation of the memory locations at which blocks\n # of :code:`A` and :code:`B` must be read in the inner loop. For that, we need\n-# multi-dimensional pointer arithmetic.\n+# multi-dimensional pointer arithmetics.\n #\n-#\n-# Pointer Arithmetic\n-# ~~~~~~~~~~~~~~~~~~\n+# Pointer Arithmetics\n+# ~~~~~~~~~~~~~~~~~~~\n #\n # For a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given b\n # y :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\n@@ -64,12 +68,16 @@\n #    &A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K] =  a_ptr + (m : m+BLOCK_SIZE_M)[:, None]*A.stride(0) + (k : k+BLOCK_SIZE_K)[None, :]*A.stride(1);\n #    &B[k : k+BLOCK_SIZE_K, n:n+BLOCK_SIZE_N] =  b_ptr + (k : k+BLOCK_SIZE_K)[:, None]*B.stride(0) + (n : n+BLOCK_SIZE_N)[None, :]*B.stride(1);\n #\n-# Which means that pointers for blocks of A and B can be initialized (i.e., :code:`k=0`) in Triton as:\n+# Which means that pointers for blocks of A and B can be initialized (i.e., :code:`k=0`) in Triton as the following\n+# code. Also note that we need an extra modulo to handle the case where :code:`M` is not a multiple of\n+# :code:`BLOCK_SIZE_M` or :code:`N` is not a multiple of :code:`BLOCK_SIZE_N`, in which case we can pad the data with\n+# some useless values, which will not contribute to the results. For the :code:`K` dimension, we will handle that later\n+# using masking load semantics.\n #\n #  .. code-block:: python\n #\n-#    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-#    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+#    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n+#    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n #    offs_k = tl.arange(0, BLOCK_SIZE_K)\n #    a_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k [None, :]*stride_ak)\n #    b_ptrs = b_ptr + (offs_k [:, None]*stride_bk + offs_bn[None, :]*stride_bn)\n@@ -107,30 +115,31 @@\n #\n #  .. code-block:: python\n #\n-#    # program ID\n+#    # Program ID\n #    pid = tl.program_id(axis=0)\n-#    # number of program ids along the M axis\n+#    # Number of program ids along the M axis\n #    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n-#    # number of programs ids along the N axis\n+#    # Number of programs ids along the N axis\n #    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-#    # number of programs in group\n+#    # Number of programs in group\n #    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n-#    # id of the group this program is in\n+#    # Id of the group this program is in\n #    group_id = pid // num_pid_in_group\n-#    # row-id of the first program in the group\n+#    # Row-id of the first program in the group\n #    first_pid_m = group_id * GROUP_SIZE_M\n-#    # if `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\n+#    # If `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\n #    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n-#    # *within groups*, programs are ordered in a column-major order\n-#    # row-id of the program in the *launch grid*\n+#    # *Within groups*, programs are ordered in a column-major order\n+#    # Row-id of the program in the *launch grid*\n #    pid_m = first_pid_m + (pid % group_size_m)\n-#    # col-id of the program in the *launch grid*\n+#    # Col-id of the program in the *launch grid*\n #    pid_n = (pid % num_pid_in_group) // group_size_m\n #\n # For example, in the following matmul where each matrix is 9 blocks by 9 blocks,\n # we can see that if we compute the output in row-major ordering, we need to load 90\n # blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped\n # ordering, we only need to load 54 blocks.\n+#\n #   .. image:: grouped_vs_row_major_ordering.png\n #\n # In practice, this can improve the performance of our matrix multiplication kernel by\n@@ -146,15 +155,12 @@\n import triton\n import triton.language as tl\n \n-# %\n-# :code:`triton.jit`'ed functions can be auto-tuned by using the `triton.autotune`\n-# decorator, which consumes:\n-#   - A list of :code:`triton.Config` objects that define different configurations of\n-#       meta-parameters (e.g., BLOCK_SIZE_M) and compilation options (e.g., num_warps) to try\n-#   - An autotuning *key* whose change in values will trigger evaluation of all the\n-#       provided configs\n-\n \n+# `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n+#   - A list of `triton.Config` objects that define different configurations of\n+#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n+#   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n+#       provided configs\n @triton.autotune(\n     configs=[\n         triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n@@ -175,8 +181,8 @@ def matmul_kernel(\n     # Matrix dimensions\n     M, N, K,\n     # The stride variables represent how much to increase the ptr by when moving by 1\n-    # element in a particular dimension. E.g. stride_am is how much to increase a_ptr\n-    # by to get the element one row down (A has M rows)\n+    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n+    # by to get the element one row down (A has M rows).\n     stride_am, stride_ak,\n     stride_bk, stride_bn,\n     stride_cm, stride_cn,\n@@ -190,12 +196,11 @@ def matmul_kernel(\n     \"\"\"\n     # -----------------------------------------------------------\n     # Map program ids `pid` to the block of C it should compute.\n-    # This is done in a grouped ordering to promote L2 data reuse\n-    # See above `L2 Cache Optimizations` section for details\n+    # This is done in a grouped ordering to promote L2 data reuse.\n+    # See above `L2 Cache Optimizations` section for details.\n     pid = tl.program_id(axis=0)\n     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n     num_pid_in_group = GROUP_SIZE_M * num_pid_n\n     group_id = pid // num_pid_in_group\n     first_pid_m = group_id * GROUP_SIZE_M\n@@ -207,70 +212,66 @@ def matmul_kernel(\n     # Create pointers for the first blocks of A and B.\n     # We will advance this pointer as we move in the K direction\n     # and accumulate\n-    # a_ptrs is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n-    # b_ptrs is a block of [BLOCK_SIZE_K, BLOCK_SIZE_n] pointers\n-    # see above `Pointer Arithmetic` section for details\n-    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n+    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n+    # See above `Pointer Arithmetics` section for details\n+    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n+    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n     offs_k = tl.arange(0, BLOCK_SIZE_K)\n     a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n     b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n \n     # -----------------------------------------------------------\n-    # Iterate to compute a block of the C matrix\n+    # Iterate to compute a block of the C matrix.\n     # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n     # of fp32 values for higher accuracy.\n-    # `accumulator` will be converted back to fp16 after the loop\n+    # `accumulator` will be converted back to fp16 after the loop.\n     accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-    for k in range(0, num_pid_k):\n-        # Note that for simplicity, we don't apply a mask here.\n-        # This means that if K is not a multiple of BLOCK_SIZE_K,\n-        # this will access out-of-bounds memory and produce an\n-        # error or (worse!) incorrect results.\n-        a = tl.load(a_ptrs)\n-        b = tl.load(b_ptrs)\n-        # We accumulate along the K dimension\n+    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n+        # Load the next block of A and B, generate a mask by checking the K dimension.\n+        # If it is out of bounds, set it to 0.\n+        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n+        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n+        # We accumulate along the K dimension.\n         accumulator += tl.dot(a, b)\n-        # Advance the ptrs to the next K block\n+        # Advance the ptrs to the next K block.\n         a_ptrs += BLOCK_SIZE_K * stride_ak\n         b_ptrs += BLOCK_SIZE_K * stride_bk\n-    # you can fuse arbitrary activation functions here\n+    # You can fuse arbitrary activation functions here\n     # while the accumulator is still in FP32!\n-    if ACTIVATION:\n-        accumulator = ACTIVATION(accumulator)\n+    if ACTIVATION == \"leaky_relu\":\n+        accumulator = leaky_relu(accumulator)\n     c = accumulator.to(tl.float16)\n \n     # -----------------------------------------------------------\n-    # Write back the block of the output matrix C\n+    # Write back the block of the output matrix C with masks.\n     offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n     offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n     c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n     c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n     tl.store(c_ptrs, c, mask=c_mask)\n \n \n-# we can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`\n+# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n @triton.jit\n def leaky_relu(x):\n+    x = x + 1\n     return tl.where(x >= 0, x, 0.01 * x)\n \n \n # %%\n-# We can now create a convenience wrapper function that only takes two input tensors\n-# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel\n+# We can now create a convenience wrapper function that only takes two input tensors,\n+# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n \n \n-def matmul(a, b, activation=None):\n-    # checks constraints\n-    assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n-    assert a.is_contiguous(), \"matrix A must be contiguous\"\n-    assert b.is_contiguous(), \"matrix B must be contiguous\"\n+def matmul(a, b, activation=\"\"):\n+    # Check constraints.\n+    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n+    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n+    assert b.is_contiguous(), \"Matrix B must be contiguous\"\n     M, K = a.shape\n     K, N = b.shape\n-    assert (\n-        K % 32 == 0\n-    ), \"We don't check memory-out-of-bounds with K so K must be divisible by BLOCK_SIZE_K\"\n-    # allocates output\n+    # Allocates output.\n     c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n     # 1D launch kernel where each block gets its own program.\n     grid = lambda META: (\n@@ -282,7 +283,7 @@ def matmul(a, b, activation=None):\n         a.stride(0), a.stride(1),\n         b.stride(0), b.stride(1),\n         c.stride(0), c.stride(1),\n-        ACTIVATION=activation,\n+        ACTIVATION=activation\n     )\n     return c\n \n@@ -291,12 +292,12 @@ def matmul(a, b, activation=None):\n # Unit Test\n # ---------\n #\n-# We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS)\n+# We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS).\n \n torch.manual_seed(0)\n a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-triton_output = matmul(a, b, activation=None)\n+triton_output = matmul(a, b)\n torch_output = torch.matmul(a, b)\n print(f\"triton_output={triton_output}\")\n print(f\"torch_output={torch_output}\")\n@@ -312,24 +313,25 @@ def matmul(a, b, activation=None):\n # Square Matrix Performance\n # ~~~~~~~~~~~~~~~~~~~~~~~~~~\n #\n-# We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices, but feel free to arrange this script as you wish to benchmark any other matrix shape.\n+# We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices,\n+# but feel free to arrange this script as you wish to benchmark any other matrix shape.\n \n \n @triton.testing.perf_report(\n     triton.testing.Benchmark(\n-        x_names=['M', 'N', 'K'],  # argument names to use as an x-axis for the plot\n+        x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot\n         x_vals=[\n             128 * i for i in range(2, 33)\n-        ],  # different possible values for `x_name`\n-        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n-        # possible values for `line_arg``\n+        ],  # Different possible values for `x_name`\n+        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\n+        # Possible values for `line_arg`\n         line_vals=['cublas', 'triton'],\n-        # label name for the lines\n+        # Label name for the lines\n         line_names=[\"cuBLAS\", \"Triton\"],\n-        # line styles\n+        # Line styles\n         styles=[('green', '-'), ('blue', '-')],\n-        ylabel=\"TFLOPS\",  # label name for the y-axis\n-        plot_name=\"matmul-performance\",  # name for the plot. Used also as a file name for saving the plot.\n+        ylabel=\"TFLOPS\",  # Label name for the y-axis\n+        plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n         args={},\n     )\n )"}, {"filename": "python/tutorials/04-low-memory-dropout.py", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -7,8 +7,11 @@\n whose state is generally composed of a bit mask tensor of the same shape as the input.\n \n In doing so, you will learn about:\n-- The limitations of naive implementations of Dropout with PyTorch\n-- Parallel pseudo-random number generation in Triton\n+\n+* The limitations of naive implementations of Dropout with PyTorch.\n+\n+* Parallel pseudo-random number generation in Triton.\n+\n \"\"\"\n \n # %%"}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -5,8 +5,11 @@\n kernel that runs faster than the PyTorch implementation.\n \n In doing so, you will learn about:\n-- Implementing backward pass in Triton\n-- Implementing parallel reduction in Triton\n+\n+* Implementing backward pass in Triton.\n+\n+* Implementing parallel reduction in Triton.\n+\n \"\"\"\n \n # %%"}, {"filename": "python/tutorials/07-math-functions.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1,6 +1,6 @@\n \"\"\"\n Libdevice (`tl.math`) function\n-===============\n+==============================\n Triton can invoke a custom function from an external library.\n In this example, we will use the `libdevice` library (a.k.a `math` in triton) to apply `asin` on a tensor.\n Please refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\n@@ -12,7 +12,7 @@\n \n # %%\n #  asin Kernel\n-# --------------------------\n+# ------------\n \n import torch\n \n@@ -37,7 +37,7 @@ def asin_kernel(\n \n # %%\n #  Using the default libdevice library path\n-# --------------------------\n+# -----------------------------------------\n # We can use the default libdevice library path encoded in `triton/language/math.py`\n \n \n@@ -59,7 +59,7 @@ def asin_kernel(\n \n # %%\n #  Customize the libdevice library path\n-# --------------------------\n+# -------------------------------------\n # We can also customize the libdevice library path by passing the path to the `libdevice` library to the `asin` kernel.\n \n output_triton = torch.empty_like(x)"}, {"filename": "python/tutorials/08-experimental-block-pointer.py", "status": "added", "additions": 220, "deletions": 0, "changes": 220, "file_content_changes": "@@ -0,0 +1,220 @@\n+\"\"\"\n+Block Pointer (Experimental)\n+============================\n+This tutorial will guide you through writing a matrix multiplication algorithm that utilizes block pointer semantics.\n+These semantics are more friendly for Triton to optimize and can result in better performance on specific hardware.\n+Note that this feature is still experimental and may change in the future.\n+\n+\"\"\"\n+\n+# %%\n+# Motivations\n+# -----------\n+# In the previous matrix multiplication tutorial, we constructed blocks of values by de-referencing blocks of pointers,\n+# i.e., :code:`load(block<pointer_type<element_type>>) -> block<element_type>`, which involved loading blocks of\n+# elements from memory. This approach allowed for flexibility in using hardware-managed cache and implementing complex\n+# data structures, such as tensors of trees or unstructured look-up tables.\n+#\n+# However, the drawback of this approach is that it relies heavily on complex optimization passes by the compiler to\n+# optimize memory access patterns. This can result in brittle code that may suffer from performance degradation when the\n+# optimizer fails to perform adequately. Additionally, as memory controllers specialize to accommodate dense spatial\n+# data structures commonly used in machine learning workloads, this problem is likely to worsen.\n+#\n+# To address this issue, we will use block pointers :code:`pointer_type<block<element_type>>` and load them into\n+# :code:`block<element_type>`, in which way gives better friendliness for the compiler to optimize memory access\n+# patterns.\n+#\n+# Let's start with the previous matrix multiplication example and demonstrate how to rewrite it to utilize block pointer\n+# semantics.\n+\n+# %%\n+# Make a Block Pointer\n+# --------------------\n+# A block pointer pointers to a block in a parent tensor and is constructed by :code:`make_block_ptr` function,\n+# which takes the following information as arguments:\n+# - :code:`base`: the base pointer to the parent tensor;\n+# - :code:`shape`: the shape of the parent tensor;\n+# - :code:`strides`: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\n+# - :code:`offsets`: the offsets of the block;\n+# - :code:`block_shape`: the shape of the block;\n+# - :code:`order`: the order of the block, which means how the block is laid out in memory.\n+#\n+# For example, to a block pointer to a :code:`BLOCK_SIZE_M`x:code:`BLOCK_SIZE_K` block in a row-major 2D matrix A by\n+# offsets :code:`(pid_m * BLOCK_SIZE_M, 0)` and strides :code:`(stride_am, stride_ak)`, we can use the following code\n+# (exactly the same as the previous matrix multiplication tutorial):\n+#\n+# .. code-block:: python\n+#\n+#     a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\n+#                                     offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\n+#                                     order=(1, 0))\n+#\n+# Note that the :code:`order` argument is set to :code:`(1, 0)`, which means the second axis is the inner dimension in\n+# terms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary\n+# for some hardware backends to optimize for better performance.\n+\n+# %%\n+# Load/Store a Block Pointer\n+# --------------------------\n+# To load/store a block pointer, we can use :code:`load/store` function, which takes a block pointer as an argument,\n+# de-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\n+# :code:`boundary_check` to specify whether to check the boundary of each axis for the block pointer. With check on and\n+# out-of-bound values will be masked according to the :code:`padding_option` argument (load only), which can be\n+# :code:`zero` or :code:`nan`. Temporarily, we do not support other values due to some hardware limitations. In this\n+# mode of block pointer load/store does not support :code:`mask` or :code:`other` arguments in the legacy mode.\n+#\n+# So to load a block in A, we can simply write :code:`a = tl.load(a_block_ptr, boundary_check=(0, 1))`. Boundary check\n+# may cost extra performance, so if you can guarantee that the block pointer is always in-bound in some axis, you can\n+# turn off the check by not passing the index into the :code:`boundary_check` argument. For example, if we know that\n+# :code:`M` is a multiple of :code:`BLOCK_SIZE_M`, we can write :code:`a = tl.load(a_block_ptr, boundary_check=(1, ))`.\n+\n+# %%\n+# Advance a Block Pointer\n+# -----------------------\n+# To advance a block pointer, we can use :code:`advance` function, which takes a block pointer and the increment for\n+# each axis as arguments and returns a new block pointer with the same shape and strides as the original one,\n+# but with the offsets advanced by the specified amount.\n+#\n+# For example, to advance the block pointer by :code:`BLOCK_SIZE_K` in the second axis\n+# (no need to multiply with stride), we can write :code:`a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))`.\n+\n+# %%\n+# Final Result\n+# ------------\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.autotune(\n+    configs=[\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+    ],\n+    key=['M', 'N', 'K'],\n+)\n+@triton.jit\n+def matmul_kernel_with_block_pointers(\n+        # Pointers to matrices\n+        a_ptr, b_ptr, c_ptr,\n+        # Matrix dimensions\n+        M, N, K,\n+        # The stride variables represent how much to increase the ptr by when moving by 1\n+        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n+        # by to get the element one row down (A has M rows).\n+        stride_am, stride_ak,\n+        stride_bk, stride_bn,\n+        stride_cm, stride_cn,\n+        # Meta-parameters\n+        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+        GROUP_SIZE_M: tl.constexpr\n+):\n+    \"\"\"Kernel for computing the matmul C = A x B.\n+    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n+    \"\"\"\n+    # -----------------------------------------------------------\n+    # Map program ids `pid` to the block of C it should compute.\n+    # This is done in a grouped ordering to promote L2 data reuse.\n+    # See the matrix multiplication tutorial for details.\n+    pid = tl.program_id(axis=0)\n+    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n+    group_id = pid // num_pid_in_group\n+    first_pid_m = group_id * GROUP_SIZE_M\n+    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n+    pid_m = first_pid_m + (pid % group_size_m)\n+    pid_n = (pid % num_pid_in_group) // group_size_m\n+\n+    # ----------------------------------------------------------\n+    # Create block pointers for the first blocks of A and B.\n+    # We will advance this pointer as we move in the K direction and accumulate.\n+    # See above `Make a Block Pointer` section for details.\n+    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\n+                                    offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\n+                                    order=(1, 0))\n+    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn),\n+                                    offsets=(0, pid_n * BLOCK_SIZE_N), block_shape=(BLOCK_SIZE_K, BLOCK_SIZE_N),\n+                                    order=(1, 0))\n+\n+    # -----------------------------------------------------------\n+    # Iterate to compute a block of the C matrix.\n+    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block.\n+    # of fp32 values for higher accuracy.\n+    # `accumulator` will be converted back to fp16 after the loop.\n+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+    for k in range(0, K, BLOCK_SIZE_K):\n+        # Load with boundary checks, no need to calculate the mask manually.\n+        # For better performance, you may remove some axis from the boundary\n+        # check, if you can guarantee that the access is always in-bound in\n+        # that axis.\n+        # See above `Load/Store a Block Pointer` section for details.\n+        a = tl.load(a_block_ptr, boundary_check=(0, 1))\n+        b = tl.load(b_block_ptr, boundary_check=(0, 1))\n+        # We accumulate along the K dimension.\n+        accumulator += tl.dot(a, b)\n+        # Advance the block pointer to the next K block.\n+        # See above `Advance a Block Pointer` section for details.\n+        a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))\n+        b_block_ptr = tl.advance(b_block_ptr, (BLOCK_SIZE_K, 0))\n+    c = accumulator.to(tl.float16)\n+\n+    # ----------------------------------------------------------------\n+    # Write back the block of the output matrix C with boundary checks.\n+    # See above `Load/Store a Block Pointer` section for details.\n+    c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn),\n+                                    offsets=(pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N),\n+                                    block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_N), order=(1, 0))\n+    tl.store(c_block_ptr, c, boundary_check=(0, 1))\n+\n+\n+# We can now create a convenience wrapper function that only takes two input tensors,\n+# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n+def matmul(a, b):\n+    # Check constraints.\n+    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n+    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n+    assert b.is_contiguous(), \"Matrix B must be contiguous\"\n+    M, K = a.shape\n+    K, N = b.shape\n+    # Allocates output.\n+    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n+    # 1D launch kernel where each block gets its own program.\n+    grid = lambda META: (\n+        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n+    )\n+    matmul_kernel_with_block_pointers[grid](\n+        a, b, c,\n+        M, N, K,\n+        a.stride(0), a.stride(1),\n+        b.stride(0), b.stride(1),\n+        c.stride(0), c.stride(1),\n+    )\n+    return c\n+\n+\n+# %%\n+# Unit Test\n+# ---------\n+#\n+# Still we can test our matrix multiplication with block pointers against a native torch implementation (i.e., cuBLAS).\n+\n+torch.manual_seed(0)\n+a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+triton_output = matmul(a, b)\n+torch_output = torch.matmul(a, b)\n+print(f\"triton_output={triton_output}\")\n+print(f\"torch_output={torch_output}\")\n+if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n+    print(\"\u2705 Triton and Torch match\")\n+else:\n+    print(\"\u274c Triton and Torch differ\")"}]