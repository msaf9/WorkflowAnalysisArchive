[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -62,6 +62,11 @@ jobs:\n         run: |\n           echo \"PATH=${HOME}/.local/bin:${PATH}\" >> \"${GITHUB_ENV}\"\n \n+      - name: Check pre-commit\n+        run: |\n+          python3 -m pip install --upgrade pre-commit\n+          python3 -m pre_commit run --all-files --verbose\n+\n       - name: Install Triton\n         if: ${{ env.BACKEND == 'CUDA'}}\n         run: |"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -758,9 +758,8 @@ struct ConvertLayoutOpConversion\n                            i32_val(srcShape[0] / instrShape[0]));\n \n       unsigned inVec =\n-          inOrd == outOrd\n-              ? triton::gpu::getContigPerThread(mmaLayout)[inOrd[0]]\n-              : 1;\n+          inOrd == outOrd ? triton::gpu::getContigPerThread(mmaLayout)[inOrd[0]]\n+                          : 1;\n       unsigned outVec = dstSharedLayout.getVec();\n       unsigned minVec = std::min(outVec, inVec);\n       assert(minVec == 2);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv2.cpp", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "file_content_changes": "@@ -237,9 +237,7 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n           {ha[{m, k + 1}], \"r\"},\n           {ha[{m + 1, k + 1}], \"r\"},\n       });\n-      auto bArgs2 = builder.newListOperand({\n-          {hb[{n, k + 1}], \"r\"}\n-      });\n+      auto bArgs2 = builder.newListOperand({{hb[{n, k + 1}], \"r\"}});\n       mma(retArgs, aArgs1, bArgs1, cArgs);\n       mma(retArgs, aArgs2, bArgs2, cArgs);\n     } else {\n@@ -333,4 +331,3 @@ LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n                               ConversionPatternRewriter &rewriter) {\n   return convertMMA(op, adaptor, typeConverter, rewriter, false /*isTuring*/);\n }\n-"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -246,8 +246,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n       auto warpsPerTile = getWarpsPerTile(dotOp, retShapePerCTA, versionMajor,\n                                           numWarps, instrShape);\n       mmaEnc = ttg::MmaEncodingAttr::get(oldRetType.getContext(), versionMajor,\n-                                         versionMinor, warpsPerTile,\n-                                         CTALayout, instrShape);\n+                                         versionMinor, warpsPerTile, CTALayout,\n+                                         instrShape);\n     }\n     auto newRetType = RankedTensorType::get(\n         oldRetType.getShape(), oldRetType.getElementType(), mmaEnc);"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 36, "deletions": 0, "changes": 36, "file_content_changes": "@@ -54,6 +54,7 @@\n #include <fstream>\n #include <optional>\n #include <pybind11/buffer_info.h>\n+#include <pybind11/embed.h>\n #include <pybind11/functional.h>\n #include <pybind11/pybind11.h>\n #include <pybind11/stl.h>\n@@ -172,6 +173,30 @@ class TritonOpBuilder {\n   bool lineInfoEnabled = !triton::tools::getBoolEnv(\"TRITON_DISABLE_LINE_INFO\");\n };\n \n+static std::string locationToString(mlir::Location loc) {\n+  std::string str;\n+  llvm::raw_string_ostream os(str);\n+  loc.print(os);\n+  os.flush(); // Make sure all the content is dumped into the 'str' string\n+  return str;\n+}\n+\n+static void outputWarning(mlir::Location loc, const std::string &msg) {\n+  std::string locStr = locationToString(loc);\n+\n+  py::exec(\n+      R\"(\n+import warnings\n+\n+def custom_showwarning(message, category, filename, lineno, file=None, line=None):\n+    print(f\"UserWarning: {message}\")\n+\n+warnings.showwarning = custom_showwarning\n+warnings.warn(f\"{loc}: {msg}\")\n+)\",\n+      py::globals(), py::dict(py::arg(\"loc\") = locStr, py::arg(\"msg\") = msg));\n+}\n+\n /*****************************************************************************/\n /* Python bindings for triton::ir                                            */\n /*****************************************************************************/\n@@ -596,6 +621,17 @@ void init_triton_ir(py::module &&m) {\n                  newBlock->erase();\n                }\n              });\n+             // 2. Check if the result of tl.advance is used\n+             self.walk([&](mlir::Operation *op) {\n+               if (mlir::isa<mlir::triton::AdvanceOp>(op) &&\n+                   op->getResult(0).use_empty())\n+                 outputWarning(op->getLoc(), \"The result of tl.advance is not \"\n+                                             \"being used. Note that tl.advance \"\n+                                             \"does not have any side effects. \"\n+                                             \"To move the block pointer, you \"\n+                                             \"need to assign the result of \"\n+                                             \"tl.advance to a variable.\");\n+             });\n            })\n       .def_property_readonly(\"type\", &mlir::triton::FuncOp::getFunctionType)\n       .def(\"reset_type\", &mlir::triton::FuncOp::setType);"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -2313,9 +2313,9 @@ def kernel(X, stride_xm, stride_xk,\n         assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f32.tf32.tf32', ptx)\n     elif in_dtype == 'float16' and out_dtype == tl.float32:\n         if capability[0] == 7 and capability[1] == 5:  # Turing\n-          assert re.search(r'mma.sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f32.f16.f16', ptx)\n+            assert re.search(r'mma.sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f32.f16.f16', ptx)\n         else:\n-          assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k16(?:.row.col)?.f32.f16.f16', ptx)\n+            assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k16(?:.row.col)?.f32.f16.f16', ptx)\n     elif in_dtype == 'float16' and out_dtype == tl.float16:\n         if capability[0] == 7 and capability[1] == 5:  # Turing\n             assert re.search(r'mma.sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f16.f16.f16', ptx)\n@@ -2331,6 +2331,7 @@ def test_dot_mulbroadcastred(in_dtype, device):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8:\n         pytest.skip(\"Requires sm >= 80 to run\")\n+\n     @triton.jit\n     def kernel(Z, X, Y,\n                M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n@@ -2430,6 +2431,7 @@ def kernel(out_ptr):\n def test_dot_without_load(dtype_str, device):\n     capability = torch.cuda.get_device_capability()\n     allow_tf32 = capability[0] > 7\n+\n     @triton.jit\n     def _kernel(out, ALLOW_TF32: tl.constexpr):\n         a = GENERATE_TEST_HERE\n@@ -2900,6 +2902,8 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # -------------\n \n # TODO(Keren): if_exp_dynamic\n+\n+\n @pytest.mark.parametrize(\"if_type\", [\"if\", \"if_and_dynamic\", \"if_exp_static\", \"if_and_static\"])\n def test_if(if_type, device):\n \n@@ -3084,7 +3088,7 @@ def kernel(X, Y, Z, n: tl.constexpr, BLOCK: tl.constexpr):\n @pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n def test_inline_asm_packed(num_ctas, device):\n     check_cuda_only(device)\n-    \n+\n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))"}, {"filename": "python/tutorials/09-experimental-tma-matrix-multiplication.py", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -191,5 +191,10 @@ def perf(ms):\n     return perf(ms), perf(max_ms), perf(min_ms)\n \n \n+if torch.cuda.get_device_capability()[0] < 9:\n+    import sys\n+    print(\"Skipping TMA benchmark for GPU with compute capability < 9\")\n+    sys.exit(0)\n+\n test_matmul()\n benchmark.run(show_plots=False, print_data=True)"}, {"filename": "python/tutorials/10-experimental-tma-store-matrix-multiplication.py", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -171,4 +171,9 @@ def perf(ms):\n     return perf(ms), perf(max_ms), perf(min_ms)\n \n \n+if torch.cuda.get_device_capability()[0] < 9:\n+    import sys\n+    print(\"Skipping TMA benchmark for GPU with compute capability < 9\")\n+    sys.exit(0)\n+\n benchmark.run(show_plots=False, print_data=True)"}]