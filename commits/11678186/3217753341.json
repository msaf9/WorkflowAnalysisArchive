[{"filename": "test/TritonGPU/swizzle.mlir", "status": "modified", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -37,8 +37,6 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   }\n }\n \n-// A w4 128x32 [vec, per, max] 8 2 4\n-// B w4 32x128 [vec, per, max] 8 1 8\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: swizzle_mma_f16_128x128x32_w4\n   func @swizzle_mma_f16_128x128x32_w4(%A: tensor<128x32xf16, #shared>, %B: tensor<32x128xf16, #shared>) {\n@@ -50,8 +48,6 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   }\n }\n \n-// A w2 32x32 [vec, per, max] 8 2 4\n-// B w2 32x32 [vec, per, max] 8 2 4\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   // CHECK-LABEL: swizzle_mma_f16_32x32x32_w2\n   func @swizzle_mma_f16_32x32x32_w2(%A: tensor<32x32xf16, #shared>, %B: tensor<32x32xf16, #shared>) {\n@@ -63,8 +59,6 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   }\n }\n \n-// A w1 16x16 [vec, per, max] 8 4 2\n-// B w1 16x16 [vec, per, max] 8 4 2\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: swizzle_mma_f16_16x16x16_w1\n   func @swizzle_mma_f16_16x16x16_w1(%A: tensor<16x16xf16, #shared>, %B: tensor<16x16xf16, #shared>) {"}]