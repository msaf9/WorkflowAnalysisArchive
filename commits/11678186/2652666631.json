[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 6, "deletions": 7, "changes": 13, "file_content_changes": "@@ -94,18 +94,18 @@ def _fwd_kernel(\n \n @triton.jit\n def _bwd_preprocess(\n-    O, DO, L,\n+    Out, DO, L,\n     NewDO, Delta,\n     BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n ):\n     off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n     off_n = tl.arange(0, D_HEAD)\n     # load\n-    o = tl.load(O + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n     do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n-    l = tl.load(L + off_m).to(tl.float32)\n+    denom = tl.load(L + off_m).to(tl.float32)\n     # compute\n-    do = do / l[:, None]\n+    do = do / denom[:, None]\n     delta = tl.sum(o * do, axis=1)\n     # write-back\n     tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\n@@ -114,7 +114,7 @@ def _bwd_preprocess(\n \n @triton.jit\n def _bwd_kernel(\n-    Q, K, V, sm_scale, O, DO,\n+    Q, K, V, sm_scale, Out, DO,\n     DQ, DK, DV,\n     L, M,\n     D,\n@@ -243,7 +243,7 @@ def backward(ctx, do):\n             do_scaled, delta,\n             BLOCK_M=ctx.BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n         )\n-        pgm = _bwd_kernel[(ctx.grid[1],)](\n+        _bwd_kernel[(ctx.grid[1],)](\n             q, k, v, ctx.sm_scale,\n             o, do_scaled,\n             dq, dk, dv,\n@@ -267,7 +267,6 @@ def backward(ctx, do):\n @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(3, 2, 2048, 64)])\n def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     torch.manual_seed(20)\n-    BLOCK_M, BLOCK_N = 128, 128\n     q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0, std=.5).requires_grad_()\n     k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0, std=.5).requires_grad_()\n     v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0, std=.5).requires_grad_()"}]