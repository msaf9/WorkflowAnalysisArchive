[{"filename": ".github/workflows/documentation.yml", "status": "added", "additions": 52, "deletions": 0, "changes": 52, "file_content_changes": "@@ -0,0 +1,52 @@\n+name: Documentation\n+on:\n+  workflow_dispatch:\n+  schedule:\n+    - cron: \"0 0 * * *\"\n+\n+jobs:\n+\n+  Build-Documentation:\n+\n+    runs-on: [self-hosted, V100]\n+\n+    steps:\n+\n+      - name: Checkout gh-pages\n+        uses: actions/checkout@v2\n+        with:\n+          ref: 'gh-pages'\n+\n+      - name: Clear docs\n+        run: |\n+          rm -r /tmp/triton-docs\n+        continue-on-error: true\n+\n+      - name: Checkout branch\n+        uses: actions/checkout@v2\n+\n+      - name: Build docs\n+        run: |\n+          git fetch origin main\n+          cd docs\n+          sphinx-multiversion . _build/html/\n+\n+      - name: Publish docs\n+        run: |\n+          git branch\n+          # update docs\n+          mkdir /tmp/triton-docs;\n+          mv docs/_build/html/* /tmp/triton-docs/\n+          git checkout gh-pages\n+          cp -r CNAME /tmp/triton-docs/\n+          cp -r index.html /tmp/triton-docs/\n+          cp -r .nojekyll /tmp/triton-docs/\n+          rm -r *\n+          cp -r /tmp/triton-docs/* .\n+          git add .\n+          git commit -am \"[GH-PAGES] Updated website\"\n+          # publish docs\n+          eval `ssh-agent -s`\n+          DISPLAY=:0 SSH_ASKPASS=~/.ssh/give_pass.sh ssh-add ${{ secrets.SSH_KEY }} <<< ${{ secrets.SSH_PASS }}\n+          git remote set-url origin git@github.com:openai/triton.git\n+          git push"}, {"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 14, "deletions": 12, "changes": 26, "file_content_changes": "@@ -13,7 +13,7 @@ concurrency:\n   cancel-in-progress: ${{ github.ref != 'refs/heads/master' }}\n \n env:\n-  TRITON_USE_ASSERT_ENABLED_LLVM: 'TRUE'\n+  TRITON_USE_ASSERT_ENABLED_LLVM: \"TRUE\"\n \n jobs:\n   Runner-Preparation:\n@@ -71,25 +71,27 @@ jobs:\n         if: ${{ env.BACKEND != 'ROCM'}}\n         run: |\n           cd python\n+          python3 -m pip install --upgrade pip\n           python3 -m pip install cmake==3.24\n-          python3 -m pip install -vvv -e '.[tests]'\n+          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n       - name: Install Triton on ROCM\n         if: ${{ env.BACKEND == 'ROCM'}}\n         run: |\n           cd python\n+          python3 -m pip install --upgrade pip\n           python3 -m pip install cmake==3.24\n           python3 -m pip install torch==1.13.1 --index-url https://download.pytorch.org/whl/rocm5.2\n-          python3 -m pip install -vvv -e '.[tests]'\n+          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n       - name: Run lit tests\n         if: ${{ env.BACKEND != 'ROCM'}}\n         run: |\n           python3 -m pip install lit\n           cd python\n-          LIT_TEST_DIR=\"build/$(ls build)/test\"\n+          LIT_TEST_DIR=\"build/$(ls build | grep -i temp)/test\"\n           if [ ! -d \"${LIT_TEST_DIR}\" ]; then\n-            echo \"Not found '${LIT_TEST_DIR}'.  Did you change an installation method?\" ; exit -1\n+            echo \"Coult not find '${LIT_TEST_DIR}'\" ; exit -1\n           fi\n           lit -v \"${LIT_TEST_DIR}\"\n \n@@ -99,19 +101,19 @@ jobs:\n           cd python/test/unit\n           python3 -m pytest\n \n-      - name: Run python tests on ROCM\n-        if: ${{ env.BACKEND == 'ROCM'}}\n-        run: |\n-          cd python/test/unit/language\n-          python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py::test_empty_kernel\"\n-\n       - name: Run CXX unittests\n         if: ${{ env.BACKEND != 'ROCM'}}\n         run: |\n           cd python\n-          cd \"build/$(ls build)\"\n+          cd \"build/$(ls build | grep -i temp)\"\n           ctest\n \n+      - name: Run python tests on ROCM\n+        if: ${{ env.BACKEND == 'ROCM'}}\n+        run: |\n+          cd python/test/unit/language\n+          python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py::test_empty_kernel\"\n+\n       - name: Regression tests\n         if: ${{ contains(matrix.runner, 'A100') }}\n         run: |"}, {"filename": "README.md", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -2,7 +2,7 @@\n   <img src=\"https://cdn.openai.com/triton/assets/triton-logo.png\" alt=\"Triton logo\" width=\"88\" height=\"100\">\n </div>\n \n-[![Wheels](https://github.com/openai/triton/actions/workflows/wheels.yml/badge.svg)](https://github.com/openai/triton/actions/workflows/wheels.yml)\n+[![Wheels](https://github.com/openai/triton/actions/workflows/wheels.yml/badge.svg?branch=release/2.0.x)](https://github.com/openai/triton/actions/workflows/wheels.yml)\n \n \n **`Documentation`** |"}, {"filename": "docs/conf.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -50,7 +50,7 @@ def forward_jit_fn(func):\n \n         def wrapped(obj, **kwargs):\n             import triton\n-            if isinstance(obj, triton.code_gen.JITFunction):\n+            if isinstance(obj, triton.runtime.JITFunction):\n                 obj = obj.fn\n             return old(obj)\n \n@@ -60,7 +60,7 @@ def wrapped(obj, **kwargs):\n \n     def documenter(app, obj, parent):\n         import triton\n-        if isinstance(obj, triton.code_gen.JITFunction):\n+        if isinstance(obj, triton.runtime.JITFunction):\n             obj = obj.fn\n         return old_documenter(app, obj, parent)\n "}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -34,6 +34,8 @@ class ReduceOpHelper {\n \n   unsigned getScratchSizeInBytes();\n \n+  bool isSupportedLayout();\n+\n private:\n   triton::ReduceOp op;\n   RankedTensorType srcTy{};\n@@ -88,8 +90,7 @@ template <typename T> T highestPowOf2Divisor(T n) {\n \n bool isSingleValue(Value value);\n \n-bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n-                        triton::gpu::DotOperandEncodingAttr &dotOperandLayout);\n+bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy);\n \n /// Multi-root DAG topological sort.\n /// Performs a topological sort of the Operation in the `toSort` SetVector."}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -59,10 +59,10 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   Attribute dstLayout = dstTy.getEncoding();\n \n   // MmaToDotShortcut doesn't use shared mem\n-  if (auto mmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>())\n-    if (auto dotOperandLayout = dstLayout.dyn_cast<DotOperandEncodingAttr>())\n-      if (isMmaToDotShortcut(mmaLayout, dotOperandLayout))\n-        return {};\n+  if (srcLayout.isa<MmaEncodingAttr>() &&\n+      dstLayout.isa<DotOperandEncodingAttr>())\n+    if (isMmaToDotShortcut(srcTy, dstTy))\n+      return {};\n \n   assert(srcLayout && dstLayout &&\n          \"Unexpected layout in getScratchConfigForCvtLayout()\");"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 20, "deletions": 3, "changes": 23, "file_content_changes": "@@ -91,6 +91,19 @@ unsigned ReduceOpHelper::getScratchSizeInBytes() {\n   return bytes;\n }\n \n+bool ReduceOpHelper::isSupportedLayout() {\n+  auto srcLayout = srcTy.getEncoding();\n+  if (srcLayout.isa<triton::gpu::BlockedEncodingAttr>()) {\n+    return true;\n+  }\n+  if (auto mmaLayout = srcLayout.dyn_cast<triton::gpu::MmaEncodingAttr>()) {\n+    if (mmaLayout.isAmpere()) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n bool isSharedEncoding(Value value) {\n   auto type = value.getType();\n   if (auto tensorType = type.dyn_cast<RankedTensorType>()) {\n@@ -157,14 +170,18 @@ std::string getValueOperandName(Value value, AsmState &state) {\n   return opName;\n }\n \n-bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n-                        triton::gpu::DotOperandEncodingAttr &dotOperandLayout) {\n+bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy) {\n   // dot_op<opIdx=0, parent=#mma> = #mma\n   // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n+  auto srcLayout = srcTy.getEncoding();\n+  auto dstLayout = dstTy.getEncoding();\n+  auto mmaLayout = srcLayout.cast<triton::gpu::MmaEncodingAttr>();\n+  auto dotOperandLayout = dstLayout.cast<triton::gpu::DotOperandEncodingAttr>();\n   return mmaLayout.getVersionMajor() == 2 &&\n          mmaLayout.getWarpsPerCTA()[1] == 1 &&\n          dotOperandLayout.getOpIdx() == 0 &&\n-         dotOperandLayout.getParent() == mmaLayout;\n+         dotOperandLayout.getParent() == mmaLayout &&\n+         !srcTy.getElementType().isF32();\n }\n \n bool isSingleValue(Value value) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 5, "changes": 6, "file_content_changes": "@@ -568,11 +568,7 @@ struct ConvertLayoutOpConversion\n     auto loc = op.getLoc();\n     auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n     auto dstTy = op.getResult().getType().cast<RankedTensorType>();\n-    auto srcLayout = srcTy.getEncoding();\n-    auto dstLayout = dstTy.getEncoding();\n-    auto srcMmaLayout = srcLayout.cast<MmaEncodingAttr>();\n-    auto dstDotLayout = dstLayout.cast<DotOperandEncodingAttr>();\n-    if (isMmaToDotShortcut(srcMmaLayout, dstDotLayout)) {\n+    if (isMmaToDotShortcut(srcTy, dstTy)) {\n       // get source values\n       auto vals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n                                                        rewriter, srcTy);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 87, "deletions": 10, "changes": 97, "file_content_changes": "@@ -131,40 +131,92 @@ struct ReduceOpConversion\n     }\n   }\n \n+  // Calculates the write index in the shared memory where we would be writing\n+  // the within-thread accumulations before we start doing across-threads\n+  // accumulations. `index` is the index of the within-thread accumulations in\n+  // the full tensor, whereas `writeIdx` is the mapped-to index in the shared\n+  // memory\n+  void getWriteIndexBasic(ConversionPatternRewriter &rewriter, Location loc,\n+                          Attribute layout, SmallVector<Value> &index,\n+                          SmallVector<Value> &writeIdx,\n+                          std::map<int, Value> &ints, unsigned axis) const {\n+    writeIdx = index;\n+    auto sizePerThread = triton::gpu::getSizePerThread(layout);\n+    Value axisSizePerThread = ints[sizePerThread[axis]];\n+    Value _8 = ints[8];\n+    Value _16 = ints[16];\n+    if (layout.isa<BlockedEncodingAttr>()) {\n+      // A single thread owns axisSizePerThread contiguous values\n+      // on the reduction axis. After within thread reduction,\n+      // we would have a single accumulation every `axisSizePerThread`\n+      // contiguous values in the original tensor, so we would need\n+      // to map every `axisSizePerThread` to 1 value in smem as:\n+      // writeIdx[axis] = index[axis] / axisSizePerThread\n+      writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n+    }\n+    auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>();\n+    if (mmaLayout && mmaLayout.isAmpere()) {\n+      if (axis == 0) {\n+        // Because warpTileSize = [16, 8] and threadsPerWarp = [8, 4], each 8\n+        // rows in smem would correspond to a warp. The mapping\n+        // is: (warp_index) x 8 + (row index within warp)\n+        writeIdx[axis] =\n+            add(mul(udiv(index[axis], _16), _8), urem(index[axis], _8));\n+      } else {\n+        // Same as BlockedEncodingAttr case\n+        writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n+      }\n+    }\n+    if (mmaLayout && !mmaLayout.isAmpere()) {\n+      llvm::report_fatal_error(\"Unsupported layout\");\n+    }\n+  }\n+\n   // Use shared memory for reduction within warps and across warps\n   LogicalResult\n   matchAndRewriteBasic(triton::ReduceOp op, OpAdaptor adaptor,\n                        ConversionPatternRewriter &rewriter) const {\n+    ReduceOpHelper helper(op);\n     Location loc = op->getLoc();\n     unsigned axis = op.getAxis();\n+    // Specifies whether the reduce operation returns an index\n+    // rather than a value, e.g. argmax, argmin, .. etc\n     bool withIndex = triton::ReduceOp::withIndex(op.getRedOp());\n \n     auto srcTy = op.getOperand().getType().cast<RankedTensorType>();\n-    auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n-    auto srcOrd = srcLayout.getOrder();\n+    auto srcLayout = srcTy.getEncoding();\n+    if (!helper.isSupportedLayout()) {\n+      assert(false && \"Unexpected srcLayout in ReduceOpConversion\");\n+    }\n+    // The order of the axes for the the threads within the warp\n+    auto srcOrd = triton::gpu::getOrder(srcLayout);\n+    auto sizePerThread = triton::gpu::getSizePerThread(srcLayout);\n     auto srcShape = srcTy.getShape();\n \n     auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n     auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n     auto indexPtrTy = LLVM::LLVMPointerType::get(llvmIndexTy, 3);\n+\n     Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n     smemBase = bitcast(smemBase, elemPtrTy);\n \n-    ReduceOpHelper helper(op);\n     auto smemShape = helper.getScratchConfigBasic();\n     unsigned elems = product<unsigned>(smemShape);\n     Value indexSmemBase = gep(elemPtrTy, smemBase, i32_val(elems));\n     indexSmemBase = bitcast(indexSmemBase, indexPtrTy);\n \n     unsigned srcElems = getElemsPerThread(srcTy);\n+    // Emits indices of the original tensor that each thread\n+    // would own\n     auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy);\n     auto srcValues = getTypeConverter()->unpackLLElements(\n         loc, adaptor.getOperand(), rewriter, srcTy);\n-\n+    // Emits offsets (the offset from the base index)\n+    // of the original tensor that each thread would own\n     SmallVector<SmallVector<unsigned>> offset =\n         emitOffsetForLayout(srcLayout, srcTy);\n-\n+    // Keep track of accumulations and their indices\n     std::map<SmallVector<unsigned>, Value> accs;\n     std::map<SmallVector<unsigned>, Value> accIndices;\n     std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n@@ -191,7 +243,9 @@ struct ReduceOpConversion\n     ints[0] = i32_val(0);\n     for (int N = smemShape[axis] / 2; N > 0; N >>= 1)\n       ints[N] = i32_val(N);\n-    Value sizePerThread = i32_val(srcLayout.getSizePerThread()[axis]);\n+    ints[sizePerThread[axis]] = i32_val(sizePerThread[axis]);\n+    ints[8] = i32_val(8);\n+    ints[16] = i32_val(16);\n \n     // reduce across threads\n     for (auto it : accs) {\n@@ -200,29 +254,49 @@ struct ReduceOpConversion\n       Value accIndex;\n       if (withIndex)\n         accIndex = accIndices[key];\n-      SmallVector<Value> writeIdx = indices[key];\n-\n-      writeIdx[axis] = udiv(writeIdx[axis], sizePerThread);\n+      // get the writeIdx at which to write in smem\n+      SmallVector<Value> writeIdx;\n+      getWriteIndexBasic(rewriter, loc, srcLayout, indices[key], writeIdx, ints,\n+                         axis);\n+      // calculate the offset in smem for that writeIdx\n       Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, srcOrd);\n+      // Get element pointers for the value and index\n       Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n       Value indexWritePtr = gep(indexPtrTy, indexSmemBase, writeOffset);\n+      // Store the within-thread accumulated value at writePtr\n       store(acc, writePtr);\n+      // Store the index of within-thread accumulation at indexWritePtr\n       if (withIndex)\n         store(accIndex, indexWritePtr);\n \n       SmallVector<Value> readIdx(writeIdx.size(), ints[0]);\n+      // Perform parallel reduction with sequential addressing\n+      // E.g. We reduce `smemShape[axis]` elements into `smemShape[axis]/2`\n+      // elements using `smemShape[axis]/2` threads where each thread\n+      // would accumalte values that are `smemShape[axis]/2` apart\n+      // to avoid bank conflicts. Then we repeat with `smemShape[axis]/4`\n+      // threads, .. etc.\n       for (int N = smemShape[axis] / 2; N > 0; N >>= 1) {\n+        // The readIdx will be N elements away on the reduction axis\n         readIdx[axis] = ints[N];\n+        // If the writeIdx is greater or equal to N, do nothing\n         Value readMask = icmp_slt(writeIdx[axis], ints[N]);\n+        // Calculate the readOffset, if readMask is False, readOffset=0\n+        // meaning we reduce the value at writeIdx with itself\n         Value readOffset = select(\n             readMask, linearize(rewriter, loc, readIdx, smemShape, srcOrd),\n             ints[0]);\n+        // The readPtr is readOffset away from writePtr\n         Value readPtr = gep(elemPtrTy, writePtr, readOffset);\n         barrier();\n+        // If we do not care about the index, i.e. this is not an argmax,\n+        // argmin, .. etc\n         if (!withIndex) {\n+          // The value at the readPtr, whereas acc is the value at writePtr\n           Value cur = load(readPtr);\n           accumulate(rewriter, loc, op.getRedOp(), acc, cur, false);\n           barrier();\n+          // Update writePtr value\n           store(acc, writePtr);\n         } else {\n           Value cur = load(readPtr);\n@@ -274,12 +348,16 @@ struct ReduceOpConversion\n   // exchange across warps\n   LogicalResult matchAndRewriteFast(triton::ReduceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter) const {\n+    ReduceOpHelper helper(op);\n     Location loc = op->getLoc();\n     unsigned axis = adaptor.getAxis();\n     bool withIndex = triton::ReduceOp::withIndex(op.getRedOp());\n \n     auto srcTy = op.getOperand().getType().cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding();\n+    if (!helper.isSupportedLayout()) {\n+      assert(false && \"Unexpected srcLayout in ReduceOpConversion\");\n+    }\n     auto srcShape = srcTy.getShape();\n     auto order = getOrder(srcLayout);\n \n@@ -293,7 +371,6 @@ struct ReduceOpConversion\n     Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n     smemBase = bitcast(smemBase, elemPtrTy);\n \n-    ReduceOpHelper helper(op);\n     auto smemShapes = helper.getScratchConfigsFast();\n     unsigned elems = product<unsigned>(smemShapes[0]);\n     unsigned maxElems = std::max(elems, product<unsigned>(smemShapes[1]));"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -264,7 +264,7 @@ class ConvertTritonGPUToLLVM\n           srcType.getEncoding().dyn_cast<triton::gpu::MmaEncodingAttr>();\n       auto dstDotOp =\n           dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n-      if (srcMma && dstDotOp && !isMmaToDotShortcut(srcMma, dstDotOp)) {\n+      if (srcMma && dstDotOp && !isMmaToDotShortcut(srcType, dstType)) {\n         auto tmpType = RankedTensorType::get(\n             dstType.getShape(), dstType.getElementType(),\n             triton::gpu::BlockedEncodingAttr::get("}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 31, "deletions": 23, "changes": 54, "file_content_changes": "@@ -16,6 +16,11 @@\n #######################\n \n \n+def print_perf(cur_ms, cur_util, ref_util):\n+    # print on the same line cur_ms, cur_util and ref_util with 3 decimal places\n+    print(f'{cur_ms:.3f} ms \\t cur: {cur_util:.3f} \\t ref: {ref_util:.3f} \\t dif={cur_util - ref_util:.3f}', end='\\t')\n+\n+\n def nvsmi(attrs):\n     attrs = ','.join(attrs)\n     cmd = ['nvidia-smi', '-i', '0', '--query-gpu=' + attrs, '--format=csv,noheader,nounits']\n@@ -55,21 +60,21 @@ def nvsmi(attrs):\n     # A100 in the CI server is slow-ish for some reason.\n     # On some other servers, we are getting about 90% peak for 8kx8x8k float16\n     'a100': {\n-        (512, 512, 512): {'float16': 0.08, 'float32': 0.13, 'int8': 0.05},\n-        (1024, 1024, 1024): {'float16': 0.33, 'float32': 0.35, 'int8': 0.169},\n-        (2048, 2048, 2048): {'float16': 0.62, 'float32': 0.57, 'int8': 0.34},\n-        (4096, 4096, 4096): {'float16': 0.81, 'float32': 0.75, 'int8': 0.46},\n-        (8192, 8192, 8192): {'float16': 0.77, 'float32': 0.85, 'int8': 0.51},\n+        (512, 512, 512): {'float16': 0.084, 'float32': 0.13, 'int8': 0.05},\n+        (1024, 1024, 1024): {'float16': 0.332, 'float32': 0.35, 'int8': 0.169},\n+        (2048, 2048, 2048): {'float16': 0.641, 'float32': 0.57, 'int8': 0.34},\n+        (4096, 4096, 4096): {'float16': 0.785, 'float32': 0.75, 'int8': 0.46},\n+        (8192, 8192, 8192): {'float16': 0.805, 'float32': 0.85, 'int8': 0.51},\n         # tall-skinny\n         (16, 1024, 1024): {'float16': 0.0077, 'float32': 0.0127, 'int8': 0.005},\n-        (16, 4096, 4096): {'float16': 0.0363, 'float32': 0.0457, 'int8': 0.0259},\n+        (16, 4096, 4096): {'float16': 0.044, 'float32': 0.0457, 'int8': 0.0259},\n         (16, 8192, 8192): {'float16': 0.07, 'float32': 0.0648, 'int8': 0.0431},\n-        (64, 1024, 1024): {'float16': 0.0271, 'float32': 0.0509, 'int8': 0.0169},\n-        (64, 4096, 4096): {'float16': 0.16, 'float32': 0.162, 'int8': 0.097},\n-        (64, 8192, 8192): {'float16': 0.30, 'float32': 0.257, 'int8': 0.174},\n-        (1024, 64, 1024): {'float16': 0.0263, 'float32': 0.0458, 'int8': 0.017},\n+        (64, 1024, 1024): {'float16': 0.030, 'float32': 0.0509, 'int8': 0.0169},\n+        (64, 4096, 4096): {'float16': 0.163, 'float32': 0.162, 'int8': 0.097},\n+        (64, 8192, 8192): {'float16': 0.285, 'float32': 0.257, 'int8': 0.174},\n+        (1024, 64, 1024): {'float16': 0.033, 'float32': 0.0458, 'int8': 0.017},\n         (4096, 64, 4096): {'float16': 0.16, 'float32': 0.177, 'int8': 0.102},\n-        (8192, 64, 8192): {'float16': 0.25, 'float32': 0.230, 'int8': 0.177},\n+        (8192, 64, 8192): {'float16': 0.254, 'float32': 0.230, 'int8': 0.177},\n     }\n }\n \n@@ -94,10 +99,11 @@ def test_matmul(M, N, K, dtype_str):\n         a = torch.randn((M, K), dtype=dtype, device='cuda')\n         b = torch.randn((K, N), dtype=dtype, device='cuda')\n     fn = lambda: triton.ops.matmul(a, b)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n+    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    print_perf(ms, cur_gpu_util, ref_gpu_util)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n \n #######################\n@@ -129,12 +135,12 @@ def _add(x_ptr, y_ptr, output_ptr, n_elements,\n         1024 * 65536: 0.939,\n     },\n     'a100': {\n-        1024 * 16: 0.008,\n-        1024 * 64: 0.034,\n-        1024 * 256: 0.114,\n-        1024 * 1024: 0.315,\n-        1024 * 4096: 0.580,\n-        1024 * 16384: 0.782,\n+        1024 * 16: 0.010,\n+        1024 * 64: 0.040,\n+        1024 * 256: 0.132,\n+        1024 * 1024: 0.353,\n+        1024 * 4096: 0.605,\n+        1024 * 16384: 0.758,\n         1024 * 65536: 0.850,\n     }\n }\n@@ -150,10 +156,11 @@ def test_elementwise(N):\n     y = torch.randn_like(z)\n     grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n     fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=500)\n+    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=500)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    print_perf(ms, cur_gpu_util, ref_gpu_util)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n #######################\n # Flash-Attention\n@@ -189,7 +196,7 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n         o = fn()\n         do = torch.randn_like(o)\n         fn = lambda: o.backward(do, retain_graph=True)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=500)\n+    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=500)\n     # compute flops\n     flops_per_matmul = 2. * Z * H * N_CTX * N_CTX * D_HEAD * 0.5\n     total_flops = 2 * flops_per_matmul\n@@ -201,4 +208,5 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n-    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    print_perf(ms, cur_gpu_util, ref_gpu_util)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 89, "deletions": 21, "changes": 110, "file_content_changes": "@@ -110,6 +110,26 @@ def check_type_supported(dtype):\n         pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n \n \n+class MmaLayout:\n+    def __init__(self, version, warps_per_cta):\n+        self.version = version\n+        self.warps_per_cta = str(warps_per_cta)\n+\n+    def __str__(self):\n+        return f\"#triton_gpu.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}}}>\"\n+\n+\n+class BlockedLayout:\n+    def __init__(self, size_per_thread, threads_per_warp, warps_per_cta, order):\n+        self.sz_per_thread = str(size_per_thread)\n+        self.threads_per_warp = str(threads_per_warp)\n+        self.warps_per_cta = str(warps_per_cta)\n+        self.order = str(order)\n+\n+    def __str__(self):\n+        return f\"#triton_gpu.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}}}>\"\n+\n+\n @pytest.mark.parametrize(\"dtype_x\", list(dtypes) + [\"bfloat16\"])\n def test_empty_kernel(dtype_x, device='cuda'):\n     SIZE = 128\n@@ -1240,6 +1260,75 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n         else:\n             np.testing.assert_equal(z_ref, z_tri)\n \n+\n+layouts = [\n+    BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n+    BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1]),\n+    MmaLayout(version=(2, 0), warps_per_cta=[4, 1])\n+]\n+\n+\n+@pytest.mark.parametrize(\"M, N\", [[128, 16], [128, 128], [32, 128]])\n+@pytest.mark.parametrize(\"src_layout\", layouts)\n+@pytest.mark.parametrize(\"axis\", [0, 1])\n+def test_reduce_layouts(M, N, src_layout, axis, device='cuda'):\n+    rdims_2d = f\"1x{N}\" if axis == 0 else f\"{M}x1\"\n+    rdims_1d = f\"{N}\" if axis == 0 else f\"{M}\"\n+    store_range = \"%7\" if axis == 0 else \"%1\"\n+    ir = f\"\"\"\n+    #blocked = #triton_gpu.blocked<{{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}}>\n+    #src = {src_layout}\n+    module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n+    tt.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}, %arg1: i32 {{tt.divisibility = 16 : i32}}, %arg2: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}) {{\n+        %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>\n+        %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>\n+        %2 = tt.splat %arg1 : (i32) -> tensor<{M}x1xi32, #blocked>\n+        %3 = arith.muli %1, %2 : tensor<{M}x1xi32, #blocked>\n+        %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<{M}x1x!tt.ptr<f32>, #blocked>\n+        %5 = tt.addptr %4, %3 : tensor<{M}x1x!tt.ptr<f32>, #blocked>, tensor<{M}x1xi32, #blocked>\n+        %6 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>\n+        %7 = tt.expand_dims %6 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x{N}xi32, #blocked>\n+        %8 = tt.broadcast %5 : (tensor<{M}x1x!tt.ptr<f32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<f32>, #blocked>\n+        %9 = tt.broadcast %7 : (tensor<1x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #blocked>\n+        %10 = tt.addptr %8, %9 : tensor<{M}x{N}x!tt.ptr<f32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n+        %11 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<{rdims_2d}x!tt.ptr<f32>, #blocked>\n+        %12 = tt.addptr %11, {store_range} : tensor<{rdims_2d}x!tt.ptr<f32>, #blocked>, tensor<{rdims_2d}xi32, #blocked>\n+        %13 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xf32, #blocked>\n+        %14 = triton_gpu.convert_layout %13 : (tensor<{M}x{N}xf32, #blocked>) -> tensor<{M}x{N}xf32, #src>\n+        %15 = tt.reduce %14 {{axis = {axis} : i32, redOp = 12 : i32}} : tensor<{M}x{N}xf32, #src> -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n+        %16 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n+        %17 = tt.expand_dims %16 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xf32, #blocked>\n+        tt.store %12, %17 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xf32, #blocked>\n+        tt.return\n+    }}\n+    }}\n+    \"\"\"\n+\n+    import tempfile\n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n+        f.write(ir)\n+        f.flush()\n+        kernel = triton.compile(f.name)\n+\n+    rs = RandomState(17)\n+    x = rs.randint(0, 4, (M, N)).astype('float32')\n+    x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+\n+    if axis == 0:\n+        z = np.zeros((1, N)).astype('float32')\n+    else:\n+        z = np.zeros((M, 1)).astype('float32')\n+\n+    x_tri = torch.tensor(x, device=device)\n+    z_tri = torch.tensor(z, device=device)\n+\n+    pgm = kernel[(1, 1, 4)](x_tri, x_tri.stride(0), z_tri)\n+\n+    z_ref = np.max(x, axis=axis, keepdims=True)\n+\n+    np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+\n+\n # ---------------\n # test permute\n # ---------------\n@@ -2187,27 +2276,6 @@ def kernel(InitI, Bound, CutOff, OutI, OutJ):\n # -----------------------\n # TODO: backend should be tested separately\n \n-\n-class MmaLayout:\n-    def __init__(self, version, warps_per_cta):\n-        self.version = version\n-        self.warps_per_cta = str(warps_per_cta)\n-\n-    def __str__(self):\n-        return f\"#triton_gpu.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}}}>\"\n-\n-\n-class BlockedLayout:\n-    def __init__(self, size_per_thread, threads_per_warp, warps_per_cta, order):\n-        self.sz_per_thread = str(size_per_thread)\n-        self.threads_per_warp = str(threads_per_warp)\n-        self.warps_per_cta = str(warps_per_cta)\n-        self.order = str(order)\n-\n-    def __str__(self):\n-        return f\"#triton_gpu.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}}}>\"\n-\n-\n layouts = [\n     # MmaLayout(version=1, warps_per_cta=[1, 4]),\n     MmaLayout(version=(2, 0), warps_per_cta=[1, 4]),"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -77,7 +77,7 @@ def kernel_call():\n             self.hook(args)\n             self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **current)\n         try:\n-            return do_bench(kernel_call, percentiles=(0.5, 0.2, 0.8))\n+            return do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n         except OutOfResources:\n             return (float('inf'), float('inf'), float('inf'))\n "}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 12, "deletions": 9, "changes": 21, "file_content_changes": "@@ -18,8 +18,10 @@ def nvsmi(attrs):\n \n \n def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n-             percentiles=(0.5, 0.2, 0.8),\n-             record_clocks=False, fast_flush=False):\n+             quantiles=None,\n+             fast_flush=True,\n+             return_mode=\"mean\"):\n+    assert return_mode in [\"min\", \"max\", \"mean\", \"median\"]\n     import torch\n     \"\"\"\n     Benchmark the runtime of the provided function. By default, return the median runtime of :code:`fn` along with\n@@ -33,8 +35,8 @@ def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n     :type rep: int\n     :param grad_to_none: Reset the gradient of the provided tensor to None\n     :type grad_to_none: torch.tensor, optional\n-    :param percentiles: Performance percentile to return in addition to the median.\n-    :type percentiles: list[float]\n+    :param quantiles: Performance percentile to return in addition to the median.\n+    :type quantiles: list[float]\n     :param fast_flush: Use faster kernel to flush L2 between measurements\n     :type fast_flush: bool\n     \"\"\"\n@@ -82,11 +84,12 @@ def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n     # Record clocks\n     torch.cuda.synchronize()\n     times = torch.tensor([s.elapsed_time(e) for s, e in zip(start_event, end_event)])\n-    if percentiles:\n-        percentiles = torch.quantile(times, torch.tensor(percentiles)).tolist()\n-        return tuple(percentiles)\n-    else:\n-        return torch.mean(times).item()\n+    if quantiles is not None:\n+        ret = torch.quantile(times, torch.tensor(quantiles)).tolist()\n+        if len(ret) == 1:\n+            ret = ret[0]\n+        return ret\n+    return getattr(torch, return_mode)(times).item()\n \n \n def assert_close(x, y, atol=None, rtol=None, err_msg=''):"}, {"filename": "python/tutorials/01-vector-add.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -120,10 +120,11 @@ def add(x: torch.Tensor, y: torch.Tensor):\n def benchmark(size, provider):\n     x = torch.rand(size, device='cuda', dtype=torch.float32)\n     y = torch.rand(size, device='cuda', dtype=torch.float32)\n+    quantiles = [0.5, 0.2, 0.8]\n     if provider == 'torch':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y)\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y), quantiles=quantiles)\n     gbps = lambda ms: 12 * size / ms * 1e-6\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n "}, {"filename": "python/tutorials/02-fused-softmax.py", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -177,12 +177,13 @@ def softmax(x):\n )\n def benchmark(M, N, provider):\n     x = torch.randn(M, N, device='cuda', dtype=torch.float32)\n+    quantiles = [0.5, 0.2, 0.8]\n     if provider == 'torch-native':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1), quantiles=quantiles)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x), quantiles=quantiles)\n     if provider == 'torch-jit':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x), quantiles=quantiles)\n     gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n "}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -329,10 +329,11 @@ def matmul(a, b, activation=None):\n def benchmark(M, N, K, provider):\n     a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n     b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n+    quantiles = [0.5, 0.2, 0.8]\n     if provider == 'cublas':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), rep=100)\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), rep=100)\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n     perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n     return perf(ms), perf(max_ms), perf(min_ms)\n "}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -339,6 +339,7 @@ def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='c\n     x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\n     dy = .1 * torch.randn_like(x)\n     x.requires_grad_(True)\n+    quantiles = [0.5, 0.2, 0.8]\n     # utility functions\n     if provider == 'triton':\n         y_fwd = lambda: layer_norm(x, w_shape, weight, bias, eps)\n@@ -350,13 +351,13 @@ def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='c\n     # forward pass\n     if mode == 'forward':\n         gbps = lambda ms: 2 * x.numel() * x.element_size() / ms * 1e-6\n-        ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, rep=500)\n+        ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, quantiles=quantiles, rep=500)\n     # backward pass\n     if mode == 'backward':\n         gbps = lambda ms: 3 * x.numel() * x.element_size() / ms * 1e-6\n         y = y_fwd()\n         ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True),\n-                                                     grad_to_none=[x], rep=500)\n+                                                     quantiles=quantiles, grad_to_none=[x], rep=500)\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n \n "}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -341,7 +341,7 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n             o = fn()\n             do = torch.randn_like(o)\n             fn = lambda: o.backward(do, retain_graph=True)\n-        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n+        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n         return ms\n     if provider == \"flash\":\n         lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n@@ -353,7 +353,7 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n             o = fn()\n             do = torch.randn_like(o)\n             fn = lambda: o.backward(do, retain_graph=True)\n-        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n+        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n         return ms\n \n "}]