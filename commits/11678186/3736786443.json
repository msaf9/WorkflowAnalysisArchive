[{"filename": "README.md", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -33,6 +33,15 @@ And the latest nightly release:\n pip install -U --pre triton\n ```\n \n+# Install from source\n+\n+```\n+git clone https://github.com/openai/triton.git;\n+cd triton/python;\n+pip install cmake; # build time dependency\n+pip install -e .\n+```\n+\n # Changelog\n \n Version 1.1 is out! New features include:"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpConversions.cpp", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -218,13 +218,15 @@ ReduceOpConversion::matchAndRewriteBasic(\n       if (!withIndex) {\n         Value cur = load(readPtr);\n         accumulate(rewriter, loc, op.redOp(), acc, cur, false);\n+        barrier();\n         store(acc, writePtr);\n       } else {\n         Value cur = load(readPtr);\n         Value indexReadPtr = gep(indexPtrTy, indexWritePtr, readOffset);\n         Value curIndex = load(indexReadPtr);\n         accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, cur,\n                             curIndex, false);\n+        barrier();\n         store(acc, writePtr);\n         store(accIndex, indexWritePtr);\n       }"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -610,6 +610,10 @@ def without_fn(X, Y, A, B, C):\n     ]\n     for mode in ['all_neg', 'all_pos', 'min_neg', 'max_pos']]))\n def test_atomic_rmw(op, dtype_x_str, mode, device='cuda'):\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] < 7:\n+        if dtype_x_str == 'float16':\n+            pytest.skip(\"Only test atomic float16 ops on devices with sm >= 70\")\n     n_programs = 5\n \n     # triton kernel\n@@ -1098,6 +1102,8 @@ def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype, devi\n     # if dtype == 'float32' and not allow_tf32:\n     #     pytest.skip(\"Seems to have bugs\")\n     capability = torch.cuda.get_device_capability()\n+    if capability[0] < 7:\n+        pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if capability[0] < 8:\n         if dtype == 'int8':\n             pytest.skip(\"Only test int8 on devices with sm >= 80\")"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 55, "deletions": 14, "changes": 69, "file_content_changes": "@@ -849,7 +849,7 @@ def build_triton_ir(fn, signature, specialization, constants):\n     gscope = fn.__globals__.copy()\n     function_name = '_'.join([fn.__name__, kernel_suffix(signature.values(), specialization)])\n     tys = list(signature.values())\n-    new_constants = {k: True if tys[k] == \"i1\" else 1 for k in specialization.equal_to_1}\n+    new_constants = {k: True if k in tys and tys[k] == \"i1\" else 1 for k in specialization.equal_to_1}\n     new_attrs = {k: (\"multiple_of\", 16) for k in specialization.divisible_by_16}\n     all_constants = constants.copy()\n     all_constants.update(new_constants)\n@@ -1024,8 +1024,11 @@ def ty_to_cpp(ty):\n         \"i64\": \"int64_t\",\n         \"u32\": \"uint32_t\",\n         \"u64\": \"uint64_t\",\n+        \"fp16\": \"float\",\n+        \"bf16\": \"float\",\n         \"fp32\": \"float\",\n         \"f32\": \"float\",\n+        \"fp64\": \"double\",\n     }[ty]\n \n \n@@ -1055,6 +1058,8 @@ def _extracted_type(ty):\n             'i64': 'int64_t',\n             'u32': 'uint32_t',\n             'u64': 'uint64_t',\n+            'fp16': 'float',\n+            'bf16': 'float',\n             'fp32': 'float',\n             'f32': 'float',\n             'fp64': 'double',\n@@ -1072,7 +1077,7 @@ def format_of(ty):\n             \"int64_t\": \"L\",\n         }[ty]\n \n-    format = \"iiiiiKK\" + ''.join([format_of(_extracted_type(ty)) for ty in signature.values()])\n+    format = \"iiiiiKKOOO\" + ''.join([format_of(_extracted_type(ty)) for ty in signature.values()])\n \n     # generate glue code\n     src = f\"\"\"\n@@ -1130,11 +1135,37 @@ def format_of(ty):\n   uint64_t _function;\n   int num_warps;\n   int shared_memory;\n+  PyObject *launch_enter_hook = NULL;\n+  PyObject *launch_exit_hook = NULL;\n+  PyObject *compiled_kernel = NULL;\n+  PyObject *hook_ret = NULL;\n   {' '.join([f\"{_extracted_type(ty)} _arg{i}; \" for i, ty in signature.items()])}\n-  if(!PyArg_ParseTuple(args, \\\"{format}\\\", &gridX, &gridY, &gridZ, &num_warps, &shared_memory, &_stream, &_function, {', '.join(f\"&_arg{i}\" for i, ty in signature.items())})) {{\n+  if(!PyArg_ParseTuple(args, \\\"{format}\\\", &gridX, &gridY, &gridZ, &num_warps, &shared_memory, &_stream, &_function, &launch_enter_hook, &launch_exit_hook, &compiled_kernel, {', '.join(f\"&_arg{i}\" for i, ty in signature.items())})) {{\n     return NULL;\n   }}\n+\n+  if (launch_enter_hook != Py_None) {{\n+    PyObject *new_args = PyTuple_Pack(1, compiled_kernel);\n+    hook_ret = PyObject_CallObject(launch_enter_hook, new_args);\n+    Py_DECREF(new_args);\n+  }}\n+\n   _launch(gridX, gridY, gridZ, num_warps, shared_memory, (CUstream)_stream, (CUfunction)_function, {', '.join(f\"getPointer(_arg{i},{i})\" if ty[0]==\"*\" else f\"_arg{i}\"for i, ty in signature.items())});\n+\n+  if (launch_exit_hook != Py_None) {{\n+    PyObject *new_args = NULL;\n+    if (hook_ret) {{\n+        new_args = PyTuple_Pack(2, compiled_kernel, hook_ret);\n+    }} else {{\n+        new_args = PyTuple_Pack(1, compiled_kernel);\n+    }}\n+    hook_ret = PyObject_CallObject(launch_exit_hook, new_args);\n+    Py_DECREF(new_args);\n+  }}\n+\n+  if (hook_ret) {{\n+      Py_DECREF(hook_ret);\n+  }}\n   if(PyErr_Occurred()) {{\n     return NULL;\n   }}\n@@ -1174,7 +1205,8 @@ def default_cache_dir():\n \n \n def default_cuda_dir():\n-    return os.path.join(\"/usr\", \"local\", \"cuda\")\n+    default_dir = \"/usr/local/cuda\"\n+    return os.getenv(\"CUDA_HOME\", default=default_dir)\n \n \n class CacheManager:\n@@ -1217,9 +1249,9 @@ def put(self, data, filename, binary=True):\n \n \n @functools.lru_cache()\n-def libcuda_dir():\n-    loc = subprocess.check_output([\"whereis\", \"libcuda.so\"]).decode().strip().split()[-1]\n-    return os.path.dirname(loc)\n+def libcuda_dirs():\n+    locs = subprocess.check_output([\"whereis\", \"libcuda.so\"]).decode().strip().split()[1:]\n+    return [os.path.dirname(loc) for loc in locs]\n \n \n @contextlib.contextmanager\n@@ -1233,7 +1265,7 @@ def quiet():\n \n \n def _build(name, src, srcdir):\n-    cuda_lib_dir = libcuda_dir()\n+    cuda_lib_dirs = libcuda_dirs()\n     cuda_path = os.environ.get('CUDA_PATH', default_cuda_dir())\n     cu_include_dir = os.path.join(cuda_path, \"include\")\n     suffix = sysconfig.get_config_var('EXT_SUFFIX')\n@@ -1246,12 +1278,16 @@ def _build(name, src, srcdir):\n         gcc = shutil.which(\"gcc\")\n         cc = gcc if gcc is not None else clang\n     py_include_dir = get_paths()[\"include\"]\n-    ret = subprocess.check_call([cc, src, \"-O3\", f\"-I{cu_include_dir}\", f\"-I{py_include_dir}\", f\"-I{srcdir}\", \"-shared\", \"-fPIC\", f\"-L{cuda_lib_dir}\", \"-lcuda\", \"-o\", so])\n+\n+    cc_cmd = [cc, src, \"-O3\", f\"-I{cu_include_dir}\", f\"-I{py_include_dir}\", f\"-I{srcdir}\", \"-shared\", \"-fPIC\", \"-lcuda\", \"-o\", so]\n+    cc_cmd += [f\"-L{dir}\" for dir in cuda_lib_dirs]\n+    ret = subprocess.check_call(cc_cmd)\n+\n     if ret == 0:\n         return so\n     # fallback on setuptools\n     extra_compile_args = []\n-    library_dirs = [cuda_lib_dir]\n+    library_dirs = cuda_lib_dirs\n     include_dirs = [srcdir, cu_include_dir]\n     libraries = ['cuda']\n     # extra arguments\n@@ -1282,10 +1318,10 @@ def _build(name, src, srcdir):\n     return so\n \n \n-def make_so_cache_key(signature, constants):\n+def make_so_cache_key(version_hash, signature, constants):\n     # Get unique key for the compiled code\n     signature = {k: 'ptr' if v[0] == '*' else v for k, v in signature.items()}\n-    key = f\"{''.join(signature.values())}{constants}\"\n+    key = f\"{version_hash}-{''.join(signature.values())}{constants}\"\n     key = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n     return key\n \n@@ -1320,7 +1356,7 @@ def read_or_execute(cache_manager, force_compile, file_name, metadata,\n \n def make_stub(name, signature, constants):\n     # name of files that are cached\n-    so_cache_key = make_so_cache_key(signature, constants)\n+    so_cache_key = make_so_cache_key(triton.runtime.jit.version_key(), signature, constants)\n     so_cache_manager = CacheManager(so_cache_key)\n     so_name = f\"{name}.so\"\n     # retrieve stub from cache if it exists\n@@ -1492,6 +1528,10 @@ def compile(fn, **kwargs):\n \n class CompiledKernel:\n \n+    # Hooks for external tools to monitor the execution of triton kernels\n+    launch_enter_hook = None\n+    launch_exit_hook = None\n+\n     def __init__(self, so_path, metadata, asm):\n         # initialize launcher\n         import importlib.util\n@@ -1533,7 +1573,8 @@ def __getitem__(self, grid):\n         def runner(*args, stream=None):\n             if stream is None:\n                 stream = torch.cuda.current_stream().cuda_stream\n-            self.c_wrapper(grid[0], grid[1], grid[2], self.num_warps, self.shared, stream, self.cu_function, *args)\n+            self.c_wrapper(grid[0], grid[1], grid[2], self.num_warps, self.shared, stream, self.cu_function,\n+                           CompiledKernel.launch_enter_hook, CompiledKernel.launch_exit_hook, self, *args)\n         return runner\n \n     def get_sass(self, fun=None):"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "file_content_changes": "@@ -7,7 +7,7 @@\n import os\n import subprocess\n import textwrap\n-from collections import namedtuple\n+from collections import defaultdict, namedtuple\n from typing import TypeVar, Generic, cast, Callable, overload, Optional, Iterable, Union\n \n import torch\n@@ -112,6 +112,7 @@ def __getitem__(self, grid) -> T:\n \n class JITFunction(KernelInterface[T]):\n \n+    # Hook for inspecting compiled functions and modules\n     cache_hook = None\n     divisibility = 16\n \n@@ -257,31 +258,30 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     if stream is None and not warmup:\n       stream = get_cuda_stream(device)\n     try:\n-      bin = cache[key]\n+      bin = cache[device][key]\n       if not warmup:\n-          bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, {args})\n+          bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, triton.compiler.CompiledKernel.launch_enter_hook, triton.compiler.CompiledKernel.launch_exit_hook, bin, {args})\n       return bin\n     # kernel not cached -- compile\n     except KeyError:\n       # build dict of constant values\n       args = [{args}]\n-      configs = self._get_config(*args),\n+      all_args = {', '.join([f'{arg}' for arg in self.arg_names])},\n+      configs = self._get_config(*all_args),\n       constants = self._make_constants(constexpr_key)\n-      constants.update({{i: None for i, arg in enumerate(args) if arg is None}})\n+      constants.update({{i: None for i, arg in enumerate(all_args) if arg is None}})\n       constants.update({{i: 1 for i in configs[0].equal_to_1}})\n       # build kernel signature -- doesn't include specialized arguments\n-      all_args = {', '.join([f'{arg}' for arg in self.arg_names])},\n       signature = {{ i: self._type_of(_key_of(arg)) for i, arg in enumerate(all_args) if i not in self.constexprs }}\n       # build stub signature -- includes arguments that are specialized\n       for i, arg in constants.items():\n         if callable(arg):\n-          raise TypeError(f\"Callable constexpr at index {i} is not supported\")\n-      device = 0\n+          raise TypeError(f\"Callable constexpr at index {{i}} is not supported\")\n       if not self._call_hook(key, signature, device, constants, num_warps, num_stages, extern_libs, configs):\n         bin = triton.compile(self, signature=signature, device=device, constants=constants, num_warps=num_warps, num_stages=num_stages, extern_libs=extern_libs, configs=configs)\n         if not warmup:\n-            bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, *args)\n-        self.cache[key] = bin\n+            bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, triton.compiler.CompiledKernel.launch_enter_hook, triton.compiler.CompiledKernel.launch_exit_hook, bin, *args)\n+        self.cache[device][key] = bin\n         return bin\n       return None\n \"\"\"\n@@ -306,7 +306,7 @@ def __init__(self, fn, version=None, do_not_specialize=None):\n         self.src = textwrap.dedent(inspect.getsource(fn))\n         self.src = self.src[self.src.find(\"def\"):]\n         # cache of just-in-time compiled kernels\n-        self.cache = dict()\n+        self.cache = defaultdict(dict)\n         self.hash = None\n         # JITFunction can be instantiated as kernel\n         # when called with a grid using __getitem__"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 9, "deletions": 3, "changes": 12, "file_content_changes": "@@ -105,7 +105,6 @@ def allclose(x, y, tol=1e-2):\n     diff = abs(x - y)\n     x_max = torch.max(x)\n     y_max = torch.max(y)\n-    tol = 1e-2\n     err = torch.max(diff) / torch.max(x_max, y_max)\n     return err <= tol\n \n@@ -119,7 +118,9 @@ def nvsmi(attrs):\n     return ret\n \n \n-def do_bench(fn, warmup=25, rep=100, grad_to_none=None, percentiles=[0.5, 0.2, 0.8], record_clocks=False):\n+def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n+             percentiles=(0.5, 0.2, 0.8),\n+             record_clocks=False, fast_flush=False):\n     \"\"\"\n     Benchmark the runtime of the provided function. By default, return the median runtime of :code:`fn` along with\n     the 20-th and 80-th performance percentile.\n@@ -134,6 +135,8 @@ def do_bench(fn, warmup=25, rep=100, grad_to_none=None, percentiles=[0.5, 0.2, 0\n     :type grad_to_none: torch.tensor, optional\n     :param percentiles: Performance percentile to return in addition to the median.\n     :type percentiles: list[float]\n+    :param fast_flush: Use faster kernel to flush L2 between measurements\n+    :type fast_flush: bool\n     \"\"\"\n \n     # Estimate the runtime of the function\n@@ -155,7 +158,10 @@ def do_bench(fn, warmup=25, rep=100, grad_to_none=None, percentiles=[0.5, 0.2, 0\n     # doesn't contain any input data before the run\n     start_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]\n     end_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]\n-    cache = torch.empty(int(256e6), dtype=torch.int8, device='cuda')\n+    if fast_flush:\n+        cache = torch.empty(int(256e6 // 4), dtype=torch.int, device='cuda')\n+    else:\n+        cache = torch.empty(int(256e6), dtype=torch.int8, device='cuda')\n     # Warm-up\n     for _ in range(n_warmup):\n         fn()"}, {"filename": "python/triton/tools/build_extern.py", "status": "modified", "additions": 63, "deletions": 27, "changes": 90, "file_content_changes": "@@ -1,10 +1,24 @@\n import argparse\n import subprocess\n from abc import ABC, abstractmethod\n+from typing import Dict, List, Optional\n \n \n class Symbol:\n-    def __init__(self, name: str, op_name: str, ret_type: str, arg_names: list, arg_types: list) -> None:\n+    _name: str\n+    _op_name: str\n+    _ret_type: str\n+    _arg_names: List[str]\n+    _arg_types: List[str]\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        op_name: str,\n+        ret_type: str,\n+        arg_names: List[str],\n+        arg_types: List[str],\n+    ) -> None:\n         '''\n         A symbol is a function declaration.\n         :param name: name of the symbol\n@@ -16,31 +30,31 @@ def __init__(self, name: str, op_name: str, ret_type: str, arg_names: list, arg_\n         self._name = name\n         self._op_name = op_name\n         self._ret_type = ret_type\n-        self._arg_names = arg_names\n-        self._arg_types = arg_types\n+        self._arg_names = list(arg_names)\n+        self._arg_types = list(arg_types)\n \n     @property\n-    def name(self):\n+    def name(self) -> str:\n         return self._name\n \n     @property\n-    def op_name(self):\n+    def op_name(self) -> str:\n         return self._op_name\n \n     @property\n-    def ret_type(self):\n+    def ret_type(self) -> str:\n         return self._ret_type\n \n     @property\n-    def arg_names(self):\n+    def arg_names(self) -> List[str]:\n         return self._arg_names\n \n     @property\n-    def arg_types(self):\n+    def arg_types(self) -> List[str]:\n         return self._arg_types\n \n \n-def convert_type(type_str):\n+def convert_type(type_str) -> Optional[str]:\n     if type_str == \"i32\":\n         return \"int32\"\n     elif type_str == \"u32\":\n@@ -58,7 +72,7 @@ def convert_type(type_str):\n         return None\n \n \n-def to_unsigned(type_str):\n+def to_unsigned(type_str) -> str:\n     if type_str == \"int32\":\n         return \"uint32\"\n     elif type_str == \"int64\":\n@@ -68,7 +82,19 @@ def to_unsigned(type_str):\n \n \n class ExternLibrary(ABC):\n-    def __init__(self, name: str, path: str, format: bool = True, grouping: bool = True) -> None:\n+    _name: str\n+    _path: str\n+    _symbols: Dict[str, Symbol]\n+    _format: bool\n+    _grouping: bool\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        path: str,\n+        format: bool = True,\n+        grouping: bool = True,\n+    ) -> None:\n         '''\n         Abstract class for extern library.\n         :param name: name of the library\n@@ -78,34 +104,34 @@ def __init__(self, name: str, path: str, format: bool = True, grouping: bool = T\n         self._name = name\n         self._path = path\n         self._symbols = {}\n-        self._format = True\n+        self._format = format\n         self._grouping = grouping\n \n     @property\n-    def name(self):\n+    def name(self) -> str:\n         return self._name\n \n     @property\n-    def path(self):\n+    def path(self) -> str:\n         return self._path\n \n     @property\n-    def symbols(self):\n+    def symbols(self) -> Dict[str, Symbol]:\n         return self._symbols\n \n     @property\n-    def grouping(self):\n+    def grouping(self) -> bool:\n         return self._grouping\n \n     @abstractmethod\n-    def parse_symbols(self, input_file):\n+    def parse_symbols(self, input_file) -> None:\n         pass\n \n     @abstractmethod\n     def _output_stubs(self) -> str:\n         pass\n \n-    def generate_stub_file(self, output_dir):\n+    def generate_stub_file(self, output_dir) -> None:\n         file_str = self._output_stubs()\n         if file_str is None or len(file_str) == 0:\n             raise Exception(\"file_str is empty\")\n@@ -121,6 +147,8 @@ def generate_stub_file(self, output_dir):\n \n \n class Libdevice(ExternLibrary):\n+    _symbol_groups: Dict[str, List[Symbol]]\n+\n     def __init__(self, path) -> None:\n         '''\n         Constructor for Libdevice.\n@@ -129,7 +157,7 @@ def __init__(self, path) -> None:\n         super().__init__(\"libdevice\", path)\n         self._symbol_groups = {}\n \n-    def _extract_symbol(self, line):\n+    def _extract_symbol(self, line) -> Optional[Symbol]:\n         # Extract symbols from line in the following format:\n         # \"define [internal] <ret_type> @<name>(<arg_types>,)\"\n         entries = line.split(\"@\")\n@@ -170,7 +198,7 @@ def _extract_symbol(self, line):\n                 arg_types[i] = to_unsigned(arg_type)\n         return Symbol(func_name, op_name, ret_type, arg_names, arg_types)\n \n-    def _group_symbols(self):\n+    def _group_symbols(self) -> None:\n         symbol_set = {}\n         for symbol in self._symbols.values():\n             op_name = symbol.op_name\n@@ -240,7 +268,7 @@ def _group_symbols(self):\n             else:\n                 self._symbol_groups[op_name] = [symbol]\n \n-    def parse_symbols(self, input_file):\n+    def parse_symbols(self, input_file) -> None:\n         if len(self.symbols) > 0:\n             return\n         output = subprocess.check_output([\"grep\", \"define\", input_file]).decode().splitlines()\n@@ -252,7 +280,7 @@ def parse_symbols(self, input_file):\n \n         self._group_symbols()\n \n-    def _output_stubs(self):\n+    def _output_stubs(self) -> str:\n         # Generate python functions in the following format:\n         # @extern.extern\n         # def <op_name>(<args>, _builder=None):\n@@ -293,31 +321,39 @@ def _output_stubs(self):\n \n \n class LLVMDisassembler:\n-    def __init__(self, path):\n+    _path: str\n+    _ll_file: str\n+\n+    def __init__(self, path) -> None:\n         '''\n         Invoke llvm-dis to disassemble the given file.\n         :param path: path to llvm-dis\n         '''\n         self._path = path\n         self._ll_file = \"/tmp/extern_lib.ll\"\n \n-    def disasm(self, lib_path):\n+    def disasm(self, lib_path: str) -> None:\n         subprocess.Popen([self._path, lib_path, \"-o\", self.ll_file],\n                          stdout=subprocess.PIPE).communicate()\n \n     @property\n-    def ll_file(self):\n+    def ll_file(self) -> str:\n         return self._ll_file\n \n     @property\n-    def path(self):\n+    def path(self) -> str:\n         return self._path\n \n \n extern_libs = [\"libdevice\"]\n \n \n-def build(llvm_dis_path, lib_path, lib_name, output_dir):\n+def build(\n+    llvm_dis_path: str,\n+    lib_path: str,\n+    lib_name: str,\n+    output_dir: str,\n+) -> None:\n     '''\n       Interface function to build the library file.\n       :param llvm_dis_path: path to the llvm-dis binary"}, {"filename": "python/tutorials/01-vector-add.py", "status": "modified", "additions": 36, "deletions": 36, "changes": 72, "file_content_changes": "@@ -3,9 +3,9 @@\n =================\n In this tutorial, you will write a simple vector addition using Triton and learn about:\n \n-- The basic programming model of Triton\n+- The basic programming model of Triton.\n - The `triton.jit` decorator, which is used to define Triton kernels.\n-- The best practices for validating and benchmarking your custom ops against native reference implementations\n+- The best practices for validating and benchmarking your custom ops against native reference implementations.\n \"\"\"\n \n # %%\n@@ -20,51 +20,51 @@\n \n @triton.jit\n def add_kernel(\n-    x_ptr,  # *Pointer* to first input vector\n-    y_ptr,  # *Pointer* to second input vector\n-    output_ptr,  # *Pointer* to output vector\n-    n_elements,  # Size of the vector\n-    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n-                 # NOTE: `constexpr` so it can be used as a shape value\n+    x_ptr,  # *Pointer* to first input vector.\n+    y_ptr,  # *Pointer* to second input vector.\n+    output_ptr,  # *Pointer* to output vector.\n+    n_elements,  # Size of the vector.\n+    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n+                 # NOTE: `constexpr` so it can be used as a shape value.\n ):\n-    # There are multiple 'program's processing different data. We identify which program\n-    # we are here\n-    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0\n+    # There are multiple 'programs' processing different data. We identify which program\n+    # we are here:\n+    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n     # This program will process inputs that are offset from the initial data.\n-    # for instance, if you had a vector of length 256 and block_size of 64, the programs\n+    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n     # would each access the elements [0:64, 64:128, 128:192, 192:256].\n-    # Note that offsets is a list of pointers\n+    # Note that offsets is a list of pointers:\n     block_start = pid * BLOCK_SIZE\n     offsets = block_start + tl.arange(0, BLOCK_SIZE)\n-    # Create a mask to guard memory operations against out-of-bounds accesses\n+    # Create a mask to guard memory operations against out-of-bounds accesses.\n     mask = offsets < n_elements\n     # Load x and y from DRAM, masking out any extra elements in case the input is not a\n-    # multiple of the block size\n+    # multiple of the block size.\n     x = tl.load(x_ptr + offsets, mask=mask)\n     y = tl.load(y_ptr + offsets, mask=mask)\n     output = x + y\n-    # Write x + y back to DRAM\n+    # Write x + y back to DRAM.\n     tl.store(output_ptr + offsets, output, mask=mask)\n \n \n # %%\n # Let's also declare a helper function to (1) allocate the `z` tensor\n-# and (2) enqueue the above kernel with appropriate grid/block sizes.\n+# and (2) enqueue the above kernel with appropriate grid/block sizes:\n \n \n def add(x: torch.Tensor, y: torch.Tensor):\n-    # We need to preallocate the output\n+    # We need to preallocate the output.\n     output = torch.empty_like(x)\n     assert x.is_cuda and y.is_cuda and output.is_cuda\n     n_elements = output.numel()\n     # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n-    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int]\n-    # In this case, we use a 1D grid where the size is the number of blocks\n+    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n+    # In this case, we use a 1D grid where the size is the number of blocks:\n     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     # NOTE:\n-    #  - each torch.tensor object is implicitly converted into a pointer to its first element.\n-    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel\n-    #  - don't forget to pass meta-parameters as keywords arguments\n+    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n+    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n+    #  - Don't forget to pass meta-parameters as keywords arguments.\n     add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n     # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n     # running asynchronously at this point.\n@@ -94,24 +94,24 @@ def add(x: torch.Tensor, y: torch.Tensor):\n # Benchmark\n # -----------\n # We can now benchmark our custom op on vectors of increasing sizes to get a sense of how it does relative to PyTorch.\n-# To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of your custom ops\n+# To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of your custom ops.\n # for different problem sizes.\n \n \n @triton.testing.perf_report(\n     triton.testing.Benchmark(\n-        x_names=['size'],  # argument names to use as an x-axis for the plot\n+        x_names=['size'],  # Argument names to use as an x-axis for the plot.\n         x_vals=[\n             2 ** i for i in range(12, 28, 1)\n-        ],  # different possible values for `x_name`\n-        x_log=True,  # x axis is logarithmic\n-        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n-        line_vals=['triton', 'torch'],  # possible values for `line_arg`\n-        line_names=['Triton', 'Torch'],  # label name for the lines\n-        styles=[('blue', '-'), ('green', '-')],  # line styles\n-        ylabel='GB/s',  # label name for the y-axis\n-        plot_name='vector-add-performance',  # name for the plot. Used also as a file name for saving the plot.\n-        args={},  # values for function arguments not in `x_names` and `y_name`\n+        ],  # Different possible values for `x_name`.\n+        x_log=True,  # x axis is logarithmic.\n+        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n+        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\n+        line_names=['Triton', 'Torch'],  # Label name for the lines.\n+        styles=[('blue', '-'), ('green', '-')],  # Line styles.\n+        ylabel='GB/s',  # Label name for the y-axis.\n+        plot_name='vector-add-performance',  # Name for the plot. Used also as a file name for saving the plot.\n+        args={},  # Values for function arguments not in `x_names` and `y_name`.\n     )\n )\n def benchmark(size, provider):\n@@ -127,5 +127,5 @@ def benchmark(size, provider):\n \n # %%\n # We can now run the decorated function above. Pass `print_data=True` to see the performance number, `show_plots=True` to plot them, and/or\n-# `save_path='/path/to/results/' to save them to disk along with raw CSV data\n-# benchmark.run(print_data=True, show_plots=True)\n+# `save_path='/path/to/results/' to save them to disk along with raw CSV data:\n+benchmark.run(print_data=True, show_plots=True)"}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 41, "deletions": 24, "changes": 65, "file_content_changes": "@@ -17,35 +17,52 @@\n     HAS_APEX = False\n \n \n-# Forward Pass\n @triton.jit\n-def _layer_norm_fwd_fused(X, Y, W, B, M, V, stride, N, eps,\n-                          BLOCK_SIZE: tl.constexpr):\n+def _layer_norm_fwd_fused(\n+    A,\n+    Out,\n+    Weight,\n+    Bias,\n+    Mean, Rstd,\n+    stride, N, eps,\n+    BLOCK_SIZE: tl.constexpr,\n+):\n     # position of elements processed by this program\n     row = tl.program_id(0)\n-    cols = tl.arange(0, BLOCK_SIZE)\n-    mask = cols < N\n-    # offset data pointers to start at the row of interest\n-    X += row * stride\n-    Y += row * stride\n-    # load data and cast to float32\n-    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n+    Out += row * stride\n+    A += row * stride\n     # compute mean\n-    mean = tl.sum(x, axis=0) / N\n-    # compute std\n-    xmean = tl.where(mask, x - mean, 0.)\n-    var = tl.sum(xmean * xmean, axis=0) / N\n+    mean = 0\n+    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n+    for off in range(0, N, BLOCK_SIZE):\n+        cols = off + tl.arange(0, BLOCK_SIZE)\n+        a = tl.load(A + cols, mask=cols < N, other=0.).to(tl.float32)\n+        _mean += a\n+    mean = tl.sum(_mean, axis=0) / N\n+    # compute variance\n+    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n+    for off in range(0, N, BLOCK_SIZE):\n+        cols = off + tl.arange(0, BLOCK_SIZE)\n+        a = tl.load(A + cols, mask=cols < N, other=0.).to(tl.float32)\n+        a = tl.where(cols < N, a - mean, 0.)\n+        _var += a * a\n+    var = tl.sum(_var, axis=0) / N\n     rstd = 1 / tl.sqrt(var + eps)\n-    xhat = xmean * rstd\n     # write-back mean/rstd\n-    tl.store(M + row, mean)\n-    tl.store(V + row, rstd)\n+    tl.store(Mean + row, mean)\n+    tl.store(Rstd + row, rstd)\n     # multiply by weight and add bias\n-    w = tl.load(W + cols, mask=mask)\n-    b = tl.load(B + cols, mask=mask)\n-    y = xhat * w + b\n-    # write-back\n-    tl.store(Y + cols, y, mask=mask)\n+    for off in range(0, N, BLOCK_SIZE):\n+        cols = off + tl.arange(0, BLOCK_SIZE)\n+        mask = cols < N\n+        weight = tl.load(Weight + cols, mask=mask)\n+        bias = tl.load(Bias + cols, mask=mask)\n+        a = tl.load(A + cols, mask=mask, other=0.).to(tl.float32)\n+        a_hat = (a - mean) * rstd\n+        out = a_hat * weight + bias\n+        # # write-back\n+        tl.store(Out + cols, out, mask=mask)\n+\n \n \n # Backward pass (DX + partial DW + partial DB)\n@@ -258,5 +275,5 @@ def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='c\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n \n \n-# test_layer_norm(1151, 8192, torch.float16)\n-bench_layer_norm.run(save_path='.', print_data=True)\n+test_layer_norm(1151, 8192, torch.float16)\n+# bench_layer_norm.run(save_path='.', print_data=True)"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -250,7 +250,8 @@ def backward(ctx, do):\n             BLOCK_M=ctx.BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n         )\n \n-        num_warps = 4 if ctx.BLOCK_DMODEL <= 64 else 8\n+        # NOTE: kernel currently buggy for other values of `num_warps`\n+        num_warps = 8\n         _bwd_kernel[(ctx.grid[1],)](\n             q, k, v, ctx.sm_scale,\n             o, do_scaled,"}]