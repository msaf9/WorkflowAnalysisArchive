[{"filename": "python/triton/testing.py", "status": "modified", "additions": 12, "deletions": 8, "changes": 20, "file_content_changes": "@@ -40,29 +40,33 @@ def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n     :type fast_flush: bool\n     \"\"\"\n \n-    # Estimate the runtime of the function\n     fn()\n     torch.cuda.synchronize()\n+\n+    # We maintain a buffer of 256 MB that we clear\n+    # before each kernel call to make sure that the L2\n+    # doesn't contain any input data before the run\n+    if fast_flush:\n+        cache = torch.empty(int(256e6 // 4), dtype=torch.int, device='cuda')\n+    else:\n+        cache = torch.empty(int(256e6), dtype=torch.int8, device='cuda')\n+\n+    # Estimate the runtime of the function\n     start_event = torch.cuda.Event(enable_timing=True)\n     end_event = torch.cuda.Event(enable_timing=True)\n     start_event.record()\n     for _ in range(5):\n+        cache.zero_()\n         fn()\n     end_event.record()\n     torch.cuda.synchronize()\n     estimate_ms = start_event.elapsed_time(end_event) / 5\n+\n     # compute number of warmup and repeat\n     n_warmup = max(1, int(warmup / estimate_ms))\n     n_repeat = max(1, int(rep / estimate_ms))\n-    # We maintain a buffer of 256 MB that we clear\n-    # before each kernel call to make sure that the L2\n-    # doesn't contain any input data before the run\n     start_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]\n     end_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]\n-    if fast_flush:\n-        cache = torch.empty(int(256e6 // 4), dtype=torch.int, device='cuda')\n-    else:\n-        cache = torch.empty(int(256e6), dtype=torch.int8, device='cuda')\n     # Warm-up\n     for _ in range(n_warmup):\n         fn()"}]