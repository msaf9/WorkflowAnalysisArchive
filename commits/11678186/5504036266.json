[{"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "file_content_changes": "@@ -89,9 +89,9 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n                 (128, 128, 32, 8, 4, 2, 256, 256, 128, AT, BT, ADTYPE, BDTYPE),\n-            ] for ADTYPE, BDTYPE in [(\"float8e5\", \"float8e5\"),\n+            ] for ADTYPE, BDTYPE in [(\"float8e4b15\", \"float8e5\"),\n                                      (\"float8e5\", \"float16\"),\n-                                     (\"float16\", \"float8e5\"),\n+                                     (\"float16\", \"float8e4b15\"),\n                                      (\"float16\", \"float32\"),\n                                      (\"float32\", \"float16\"),\n                                      (\"bfloat16\", \"float32\"),\n@@ -123,27 +123,27 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n     a_fp8 = \"float8\" in ADTYPE\n     b_fp8 = \"float8\" in BDTYPE\n \n-    def maybe_upcast(x, dtype):\n-        if a_fp8:\n+    def maybe_upcast(x, is_float8):\n+        if is_float8:\n             return f8_to_f16(x)\n         return x\n \n-    def init_input(n, m, t, dtype):\n+    def init_input(n, m, t, dtype, is_float8):\n         if t:\n-            return init_input(m, n, False, dtype).t()\n-        if \"float8\" in dtype:\n+            return init_input(m, n, False, dtype, is_float8).t()\n+        if is_float8:\n             return torch.randint(20, 60, (n, m), device=\"cuda\", dtype=torch.int8)\n         dtype = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[dtype]\n         return .1 * torch.randn((n, m), device=\"cuda\", dtype=dtype)\n \n     # allocate/transpose inputs\n-    a = init_input(M, K, AT, ADTYPE)\n-    b = init_input(K, N, BT, BDTYPE)\n+    a = init_input(M, K, AT, ADTYPE, a_fp8)\n+    b = init_input(K, N, BT, BDTYPE, b_fp8)\n     # run test\n-    th_a = maybe_upcast(a, ADTYPE).to(torch.float32)\n+    th_a = maybe_upcast(a, a_fp8).to(torch.float32)\n     if AT and a_fp8:\n         th_a = th_a.view(th_a.shape[::-1]).T\n-    th_b = maybe_upcast(b, BDTYPE).to(torch.float32)\n+    th_b = maybe_upcast(b, b_fp8).to(torch.float32)\n     if BT and b_fp8:\n         th_b = th_b.view(th_b.shape[::-1]).T\n     th_c = torch.matmul(th_a, th_b)"}]