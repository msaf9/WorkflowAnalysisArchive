[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -391,8 +391,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n         getSwizzledSharedPtrs(loc, inVec, srcTy, dstSharedLayout, dstElemTy,\n                               smemObj, rewriter, offsetVals, srcStrides);\n \n-    std::map<unsigned, Value> cache0;\n-    std::map<unsigned, Value> cache1;\n     for (unsigned i = 0; i < numElems; ++i) {\n       if (i % minVec == 0)\n         word = undef(wordTy);\n@@ -733,8 +731,9 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     Value warpSize = idx_val(32);\n     Value laneId = urem(threadId, warpSize);\n     Value warpId = udiv(threadId, warpSize);\n-    Value warpId0 = urem(warpId, warpsPerCTA[0]);\n-    Value warpId1 = urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]);\n+    Value warpId0 = urem(urem(warpId, warpsPerCTA[0]), idx_val(shape[0] / 16));\n+    Value warpId1 = urem(urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]),\n+                         idx_val(shape[1] / 8));\n     Value offWarp0 = mul(warpId0, idx_val(16));\n     Value offWarp1 = mul(warpId1, idx_val(8));\n "}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -1083,7 +1083,7 @@ def kernel(X, stride_xm, stride_xn,\n \n @pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype\",\n                          [(*shape, 4, False, False, epilogue, allow_tf32, dtype)\n-                          for shape in [(64, 64, 64)]\n+                          for shape in [(64, 64, 64), (16, 16, 16)]\n                           for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n                           for allow_tf32 in [True, False]\n                           for dtype in ['float16', 'float32']\n@@ -1228,8 +1228,10 @@ def kernel(X, stride_xm, stride_xk,\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n     # make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']\n-    assert 'ld.global.v4' in ptx\n-    assert 'st.global.v4' in ptx\n+    if K > 16 or N > 16 or M > 16:\n+        # XXX: skip small sizes because they are not vectorized    \n+        assert 'ld.global.v4' in ptx\n+        assert 'st.global.v4' in ptx\n     if dtype == 'float32' and allow_tf32:\n         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n     elif dtype == 'float32' and allow_tf32:"}]