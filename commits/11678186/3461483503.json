[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 75, "deletions": 19, "changes": 94, "file_content_changes": "@@ -347,7 +347,8 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n   return llvmStruct;\n }\n \n-// Delinearize on compile-time consts, assuming the order is [n, .. 2, 1, 0]\n+// TODO[goostavz]: to be deprecated\n+// delinearize supposing order is [n, .. , 2, 1, 0]\n template <typename T>\n static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n   // shape: {a, b, c, d}  ->  accMul: {b*c*d, c*d, d, 1}\n@@ -365,7 +366,40 @@ static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n   return multiDimIndex;\n }\n \n-// Linearize on compile-time consts, assuming the order is [n, .. 2, 1, 0]\n+// delinearize supposing order is [0, 1, .. , n]\n+template <typename T>\n+static SmallVector<T> getMultiDimIndexImpl(T linearIndex, ArrayRef<T> shape) {\n+  // shape: {a, b, c, d}  ->  accMul: {1, a, a*b, a*b*c}\n+  size_t rank = shape.size();\n+  T accMul = product(shape.drop_back());\n+  T linearRemain = linearIndex;\n+  SmallVector<T> multiDimIndex(rank);\n+  for (int i = rank - 1; i >= 0; --i) {\n+    multiDimIndex[i] = linearRemain / accMul;\n+    linearRemain = linearRemain % accMul;\n+    if (i != 0) {\n+      accMul = accMul / shape[i - 1];\n+    }\n+  }\n+  return multiDimIndex;\n+}\n+\n+template <typename T>\n+static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape,\n+                                       ArrayRef<unsigned> order) {\n+  size_t rank = shape.size();\n+  assert(rank == order.size());\n+  auto reordered = reorder(shape, order);\n+  auto reorderedMultiDim = getMultiDimIndexImpl<T>(linearIndex, reordered);\n+  SmallVector<T> multiDim(rank);\n+  for (unsigned i = 0; i < rank; ++i) {\n+    multiDim[order[i]] = reorderedMultiDim[i];\n+  }\n+  return multiDim;\n+}\n+\n+// TODO[goostavz]: to be deprecated\n+// linearize supposing order is [n, .. , 2, 1, 0]\n template <typename T>\n static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   assert(multiDimIndex.size() == shape.size());\n@@ -382,6 +416,30 @@ static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   return linearIndex;\n }\n \n+template <typename T>\n+static T getLinearIndexImpl(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n+  assert(multiDimIndex.size() == shape.size());\n+  // shape: {a, b, c, d}  ->  accMul: {1, a, a*b, a*b*c}\n+  size_t rank = shape.size();\n+  T accMul = product(shape.drop_back());\n+  T linearIndex = 0;\n+  for (int i = rank - 1; i >= 0; --i) {\n+    linearIndex += multiDimIndex[i] * accMul;\n+    if (i != 0) {\n+      accMul = accMul / shape[i - 1];\n+    }\n+  }\n+  return linearIndex;\n+}\n+\n+template <typename T>\n+static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape,\n+                        ArrayRef<unsigned> order) {\n+  assert(shape.size() == order.size());\n+  return getLinearIndexImpl<T>(reorder(multiDimIndex, order),\n+                               reorder(shape, order));\n+}\n+\n static Value storeShared(ConversionPatternRewriter &rewriter, Location loc,\n                          Value ptr, Value val, Value pred) {\n   MLIRContext *ctx = rewriter.getContext();\n@@ -632,6 +690,7 @@ class ConvertTritonGPUOpToLLVMPattern\n     auto sizePerThread = blockedLayout.getSizePerThread();\n     auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n     auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n+    auto order = blockedLayout.getOrder();\n \n     unsigned rank = shape.size();\n     SmallVector<unsigned> shapePerCTA = getShapePerCTA(blockedLayout);\n@@ -663,9 +722,9 @@ class ConvertTritonGPUOpToLLVMPattern\n       unsigned linearNanoTileId = n / totalSizePerThread;\n       unsigned linearNanoTileElemId = n % totalSizePerThread;\n       SmallVector<unsigned> multiDimNanoTileId =\n-          getMultiDimIndex<unsigned>(linearNanoTileId, tilesPerDim);\n-      SmallVector<unsigned> multiDimNanoTileElemId =\n-          getMultiDimIndex<unsigned>(linearNanoTileElemId, sizePerThread);\n+          getMultiDimIndex<unsigned>(linearNanoTileId, tilesPerDim, order);\n+      SmallVector<unsigned> multiDimNanoTileElemId = getMultiDimIndex<unsigned>(\n+          linearNanoTileElemId, sizePerThread, order);\n       for (unsigned k = 0; k < rank; ++k) {\n         unsigned reorderedMultiDimId =\n             multiDimNanoTileId[k] *\n@@ -1881,8 +1940,6 @@ struct PrintfOpConversion\n   // currently support pointer, i8, i16, i32, i64, f16, bf16, f32, f64\n   std::string getFormatSubstr(Value value) const {\n     Type type = value.getType();\n-    unsigned width = type.getIntOrFloatBitWidth();\n-\n     if (type.isa<LLVM::LLVMPointerType>()) {\n       return \"%p\";\n     } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n@@ -1924,13 +1981,11 @@ struct PrintfOpConversion\n   promoteValue(ConversionPatternRewriter &rewriter, Value value) {\n     auto *context = rewriter.getContext();\n     auto type = value.getType();\n-    type.dump();\n-    unsigned width = type.getIntOrFloatBitWidth();\n     Value newOp = value;\n     Type newType = type;\n \n     bool bUnsigned = type.isUnsignedInteger();\n-    if (type.isIntOrIndex() && width < 32) {\n+    if (type.isIntOrIndex() && type.getIntOrFloatBitWidth() < 32) {\n       if (bUnsigned) {\n         newType = ui32_ty;\n         newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n@@ -3057,23 +3112,24 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n     }\n     unsigned linearIdxInNanoTile = i % srcAccumSizeInThreads;\n     auto multiDimIdxInNanoTile = getMultiDimIndex<unsigned>(\n-        linearIdxInNanoTile, srcBlockedLayout.getSizePerThread());\n+        linearIdxInNanoTile, srcBlockedLayout.getSizePerThread(), inOrd);\n     unsigned pos = multiDimIdxInNanoTile[inOrd[0]] % minVec;\n     multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n     unsigned wordVecIdx =\n-        getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep);\n+        getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep, inOrd);\n     wordVecs[wordVecIdx] =\n         insert_element(wordTy, wordVecs[wordVecIdx], inVals[i], idx_val(pos));\n \n     if (i % srcAccumSizeInThreads == srcAccumSizeInThreads - 1) {\n       // end of replication, store the vectors into shared memory\n       unsigned linearRepIdx = i / srcAccumSizeInThreads;\n-      auto multiDimRepIdx = getMultiDimIndex<unsigned>(linearRepIdx, reps);\n+      auto multiDimRepIdx =\n+          getMultiDimIndex<unsigned>(linearRepIdx, reps, inOrd);\n       for (unsigned linearWordIdx = 0; linearWordIdx < numWordsEachRep;\n            ++linearWordIdx) {\n         // step 1: recover the multidim_index from the index of input_elements\n         auto multiDimWordIdx =\n-            getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep);\n+            getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep, inOrd);\n         SmallVector<Value> multiDimIdx(2);\n         auto wordOffset0 = multiDimRepIdx[0] * srcShapePerCTA[0] +\n                            multiDimWordIdx[0] * (inOrd[0] == 0 ? minVec : 1);\n@@ -3083,12 +3139,12 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n         multiDimIdx[1] = add(multiDimOffsetFirstElem[1], idx_val(wordOffset1));\n \n         // step 2: do swizzling\n-        Value remained = urem(multiDimIdx[inOrd[0]], outVecVal);\n-        multiDimIdx[inOrd[0]] = udiv(multiDimIdx[inOrd[0]], outVecVal);\n-        Value off_1 = mul(multiDimIdx[inOrd[1]], idx_val(srcShape[inOrd[0]]));\n-        Value phaseId = udiv(multiDimIdx[inOrd[1]], idx_val(perPhase));\n+        Value remained = urem(multiDimIdx[outOrd[0]], outVecVal);\n+        multiDimIdx[outOrd[0]] = udiv(multiDimIdx[outOrd[0]], outVecVal);\n+        Value off_1 = mul(multiDimIdx[outOrd[1]], idx_val(srcShape[outOrd[0]]));\n+        Value phaseId = udiv(multiDimIdx[outOrd[1]], idx_val(perPhase));\n         phaseId = urem(phaseId, idx_val(maxPhase));\n-        Value off_0 = xor_(multiDimIdx[inOrd[0]], phaseId);\n+        Value off_0 = xor_(multiDimIdx[outOrd[0]], phaseId);\n         off_0 = mul(off_0, outVecVal);\n         remained = udiv(remained, minVecVal);\n         off_0 = add(off_0, mul(remained, minVecVal));"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 80, "deletions": 39, "changes": 119, "file_content_changes": "@@ -30,18 +30,32 @@ def matmul_no_scf_kernel(\n # TODO: num_warps could only be 4 for now\n \n \n-@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n-    [128, 256, 32, 4],\n-    [256, 128, 16, 4],\n-    [128, 16, 32, 4],\n-    [32, 128, 64, 4],\n-    [128, 128, 64, 4],\n-    [64, 128, 128, 4],\n-    [64, 128, 128, 2],\n+@pytest.mark.parametrize('SHAPE,NUM_WARPS,TRANS_A,TRANS_B', [\n+    (shape, num_warps, trans_a, trans_b)\n+    for shape in [\n+        [128, 256, 32],\n+        [256, 128, 16],\n+        [128, 16, 32],\n+        [32, 128, 64],\n+        [128, 128, 64],\n+        [64, 128, 128],\n+    ]\n+    for num_warps in [2, 4]\n+    for trans_a in [False, True]\n+    for trans_b in [False, True]\n ])\n-def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n-    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n-    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+def test_gemm_no_scf(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n+    SIZE_M, SIZE_N, SIZE_K = SHAPE\n+    if (TRANS_A):\n+        a = torch.randn((SIZE_K, SIZE_M), device='cuda', dtype=torch.float16).T\n+    else:\n+        a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+\n+    if (TRANS_B):\n+        b = torch.randn((SIZE_N, SIZE_K), device='cuda', dtype=torch.float16).T\n+    else:\n+        b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+\n     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n     grid = lambda META: (1, )\n     matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n@@ -55,16 +69,32 @@ def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n     assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n \n \n-@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n-    [64, 128, 128, 1],\n-    [128, 128, 128, 4],\n-    [16, 8, 32, 1],\n-    [32, 16, 64, 2],\n-    [32, 16, 64, 4],\n+@pytest.mark.parametrize('SHAPE,NUM_WARPS,TRANS_A,TRANS_B', [\n+    (shape, num_warps, trans_a, trans_b)\n+    for shape in [\n+        [64, 128, 128],\n+        [128, 128, 128],\n+        [16, 8, 32],\n+        [32, 16, 64],\n+        [32, 16, 64],\n+    ]\n+    for num_warps in [1, 2, 4]\n+    for trans_a in [False, True]\n+    for trans_b in [False, True]\n ])\n-def test_gemm_no_scf_int8(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n-    a = torch.randint(-5, 5, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n-    b = torch.randint(-5, 5, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n+def test_gemm_no_scf_int8(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n+    SIZE_M, SIZE_N, SIZE_K = SHAPE\n+\n+    if (TRANS_A):\n+        a = torch.randint(-5, 5, (SIZE_K, SIZE_M), device='cuda', dtype=torch.int8).T\n+    else:\n+        a = torch.randint(-5, 5, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n+\n+    if (TRANS_B):\n+        b = torch.randint(-5, 5, (SIZE_N, SIZE_K), device='cuda', dtype=torch.int8).T\n+    else:\n+        b = torch.randint(-5, 5, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n+\n     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.int32)\n \n     grid = lambda META: (1, )\n@@ -125,28 +155,39 @@ def get_variant_golden(a, b):\n     return c_padded[:SIZE_M, :SIZE_N]\n \n \n-@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K', [\n+@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,TRANS_A,TRANS_B', [\n     # Non-forloop\n-    [64, 32, 64, 4, 64, 32, 64],\n-    [128, 64, 128, 4, 128, 64, 128],\n+    [64, 32, 64, 4, 64, 32, 64, False, False],\n+    [128, 64, 128, 4, 128, 64, 128, False, False],\n     # K-Forloop\n-    [64, 32, 128, 4, 64, 32, 64],\n-    [128, 16, 128, 4, 128, 16, 32],\n-    [32, 16, 128, 4, 32, 16, 32],\n-    [32, 64, 128, 4, 32, 64, 32],\n-    [32, 128, 256, 4, 32, 128, 64],\n-    [64, 128, 64, 4, 64, 128, 32],\n-    [64, 64, 128, 4, 64, 64, 32],\n-    [128, 128, 64, 4, 128, 128, 32],\n-    [128, 128, 128, 4, 128, 128, 32],\n-    [128, 128, 256, 4, 128, 128, 64],\n-    [128, 256, 128, 4, 128, 256, 32],\n-    [256, 128, 64, 4, 256, 128, 16],\n-    [128, 64, 128, 4, 128, 64, 32],\n+    [64, 32, 128, 4, 64, 32, 64, False, False],\n+    [128, 16, 128, 4, 128, 16, 32, False, False],\n+    [32, 16, 128, 4, 32, 16, 32, False, False],\n+    [32, 64, 128, 4, 32, 64, 32, False, False],\n+    [32, 128, 256, 4, 32, 128, 64, False, False],\n+    [64, 128, 64, 4, 64, 128, 32, False, False],\n+    [64, 64, 128, 4, 64, 64, 32, False, False],\n+    [128, 128, 64, 4, 128, 128, 32, False, False],\n+    [128, 128, 128, 4, 128, 128, 32, False, False],\n+    [128, 128, 256, 4, 128, 128, 64, False, False],\n+    [128, 256, 128, 4, 128, 256, 32, False, False],\n+    [256, 128, 64, 4, 256, 128, 16, False, False],\n+    [128, 64, 128, 4, 128, 64, 32, False, False],\n+    # TODO[goostavz]: fix these cases\n+    #[128, 64, 128, 4, 128, 64, 32, True, False],\n+    #[128, 64, 128, 4, 128, 64, 32, False, True],\n ])\n-def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K):\n-    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n-    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n+    if (TRANS_A):\n+        a = torch.randn((SIZE_K, SIZE_M), device='cuda', dtype=torch.float16).T\n+    else:\n+        a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+\n+    if (TRANS_B):\n+        b = torch.randn((SIZE_N, SIZE_K), device='cuda', dtype=torch.float16).T\n+    else:\n+        b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+\n     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n     grid = lambda META: (1, )\n     matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,"}]