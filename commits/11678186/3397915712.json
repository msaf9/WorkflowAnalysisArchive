[{"filename": "lib/codegen/pass.cc", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -149,6 +149,13 @@ std::unique_ptr<llvm::Module> add_passes_to_emit_bin(\n   // ir.print(std::cout);\n   isel.visit(ir, *llvm);\n   shared_static = allocation.allocated_size();\n+  if (target->as_nvidia() && target->as_nvidia()->sm() < 70) {\n+    // sm < 70 (Pascal) has little shared memory resource.\n+    // Instead of having \"Error: Invalid argument\" on launching a kernel, let's throw an error here.\n+    if (shared_static >= 65536) {\n+      throw std::runtime_error(\"Device does not support shared memory of \" + std::to_string(shared_static) + \"bytes\");\n+    }\n+  }\n \n   if (isel.get_extern_lib_map().size() > 0) {\n     // If there's any extern lib calls,"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -605,6 +605,10 @@ def without_fn(X, Y, A, B, C):\n     ]\n     for mode in ['all_neg', 'all_pos', 'min_neg', 'max_pos']]))\n def test_atomic_rmw(op, dtype_x_str, mode, device='cuda'):\n+    cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n+    if cc < 70:\n+        if dtype_x_str == 'float16':\n+            pytest.skip(\"Only test atomic float16 ops on devices with sm >= 70\")\n     n_programs = 5\n \n     # triton kernel\n@@ -1042,6 +1046,8 @@ def kernel(X, stride_xm, stride_xn,\n                           if not (allow_tf32 and (dtype in ['float16']))])\n def test_dot(epilogue, allow_tf32, dtype, device='cuda'):\n     cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n+    if cc < 70:\n+        pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if cc < 80:\n         if dtype == 'int8':\n             pytest.skip(\"Only test int8 on devices with sm >= 80\")\n@@ -1227,6 +1233,10 @@ def _kernel(in_ptr, out_ptr, in_size: tl.constexpr, out_size: tl.constexpr):\n \n @pytest.mark.parametrize(\"dtype\", [torch.bfloat16, torch.float16, torch.float32])\n def test_masked_load_shared_memory(dtype, device='cuda'):\n+    cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n+    if cc < 70:\n+        pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n+\n     check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n \n     M = 32"}, {"filename": "python/test/unit/operators/test_blocksparse.py", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -2,6 +2,7 @@\n import torch\n \n import triton\n+import triton._C.libtriton.triton as _triton\n \n \n @pytest.mark.parametrize(\"MODE\", [\"sdd\", \"dds\", \"dsd\"])\n@@ -125,6 +126,10 @@ def test_attention_fwd_bwd(\n     batch_size=2,\n     n_heads=2,\n ):\n+    cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n+    if cc < 70:\n+        pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n+\n     # inputs\n     qkv_shape = (batch_size, n_heads, n_ctx, 64)\n     qkvs = ["}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -68,6 +68,8 @@\n )\n def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, DTYPE):\n     cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n+    if cc < 70:\n+        pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if cc < 80 and DTYPE == \"bfloat16\":\n         pytest.skip(\"Only test bfloat16 on devices with sm >= 80\")\n     if DTYPE == \"bfloat16\" and SPLIT_K != 1:"}]