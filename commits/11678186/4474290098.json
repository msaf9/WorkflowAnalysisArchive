[{"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 26, "deletions": 22, "changes": 48, "file_content_changes": "@@ -51,16 +51,18 @@ struct FpToFpOpConversion\n   convertFp8E4M3x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n                            const Value &v0, const Value &v1, const Value &v2,\n                            const Value &v3) {\n-    auto *ptxAsm = \"{                                      \\n\"\n-                   \".reg .b32 a<2>, b<2>;                  \\n\"\n-                   \"prmt.b32 a0, 0, $2, 0x5040;            \\n\"\n-                   \"prmt.b32 a1, 0, $2, 0x7060;            \\n\"\n-                   \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"shr.b32  b0, b0, 1;                    \\n\"\n-                   \"shr.b32  b1, b1, 1;                    \\n\"\n-                   \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\"\n-                   \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\"\n+    auto *ptxAsm = \"{                                      \\n\" // WARN: subnormal (0bs0000xxx) are not handled\n+                   \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+                   \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n+                   \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n+                   \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff (strip sign)\n+                   \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // b1 = a1 & 0x7fff7fff (strip sign)\n+                   \"shr.b32  b0, b0, 1;                    \\n\" // b0 >>= 1 (shift into fp16 position)\n+                   \"shr.b32  b1, b1, 1;                    \\n\" // b1 >>= 1 (shift into fp16 position)\n+                   \"add.u32  b0, b0, 0x20002000;           \\n\" // b0.exp += 2**4-2**3 (exponent compensate is 8)\n+                   \"add.u32  b1, b1, 0x20002000;           \\n\" // b1 += 8 << 10 | 8 << 10 << 16 (0x20002000)\n+                   \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0 | (0x80008000 & a0) (restore sign)\n+                   \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // out1 = b1 | (0x80008000 & a1) (restore sign)\n                    \"}\";\n     return convertFp8x4ToFp16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n   }\n@@ -69,7 +71,7 @@ struct FpToFpOpConversion\n   convertFp8E5M2x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n                            const Value &v0, const Value &v1, const Value &v2,\n                            const Value &v3) {\n-    auto *ptxAsm = \"{                           \\n\"\n+    auto *ptxAsm = \"{                           \\n\"   // exponent bias of Fp8E5M2 and Fp16 are the same\n                    \"prmt.b32 $0, 0, $2, 0x5140; \\n\\t\"\n                    \"prmt.b32 $1, 0, $2, 0x7362; \\n\\t\"\n                    \"}\";\n@@ -193,17 +195,19 @@ struct FpToFpOpConversion\n   convertFp16x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n                            const Value &v0, const Value &v1, const Value &v2,\n                            const Value &v3) {\n-    auto *ptxAsm = \"{                                      \\n\"\n-                   \".reg .b32 a<2>, b<2>;                  \\n\"\n-                   \"shl.b32 a0, $1, 1;                     \\n\"\n-                   \"shl.b32 a1, $2, 1;                     \\n\"\n-                   \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"add.u32 a0, a0, 0x00800080;            \\n\"\n-                   \"add.u32 a1, a1, 0x00800080;            \\n\"\n-                   \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n-                   \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n-                   \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n+    auto *ptxAsm = \"{                                      \\n\" // WARN: subnormal Fp8s are not handled\n+                   \".reg .b32 a<2>, b<2>;                  \\n\" // see convertFp8E4M3x4ToFp16x4\n+                   \"sub.u32 a0, $1, 0x20002000;            \\n\" // a0 = input0 - 0x20002000 (compensate offset)\n+                   \"sub.u32 a1, $2, 0x20002000;            \\n\" // a1 = input1 - 0x20002000 (8 << 10 | 8 << 10 << 16)\n+                   \"shl.b32 a0, a0, 1;                     \\n\" // a0 <<= 1 (shift into fp8e4 position)\n+                   \"shl.b32 a1, a1, 1;                     \\n\" // a1 <<= 1 (shift into fp8e4 position)\n+                   \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // a0 &= 0x7fff7fff (strip sign)\n+                   \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // a1 &= 0x7fff7fff (strip sign)\n+                   \"add.u32 a0, a0, 0x00800080;            \\n\" // a0 += 0x00800080 (round to nearest)\n+                   \"add.u32 a1, a1, 0x00800080;            \\n\" // a1 += 0x00800080 (round to nearest)\n+                   \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\" // b0 = a0 | (0x80008000 & input0) (restore sign)\n+                   \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\" // b1 = a1 | (0x80008000 & input1) (restore sign)\n+                   \"prmt.b32 $0, b0, b1, 0x7531;           \\n\" // output = b1b0\n                    \"}\";\n     return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n   }"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 60, "deletions": 2, "changes": 62, "file_content_changes": "@@ -885,6 +885,52 @@ def kernel(in_out_ptr):\n         assert torch.all(x == 2)\n \n \n+def convert_float_to_float32(fp: torch.tensor, dtype=None):\n+    if not dtype:\n+        dtype = getattr(tl, torch_dtype_name(fp.dtype))\n+\n+    fp = fp.view(getattr(torch, f\"int{dtype.primitive_bitwidth}\"))\n+    exp_width = dtype.primitive_bitwidth - dtype.fp_mantissa_width - 1\n+    exp_bias = 2 ** (exp_width - 1) - 1\n+    sign = ((fp >> (dtype.primitive_bitwidth - 1)) & 0x01).int()\n+    exp = ((fp >> dtype.fp_mantissa_width) & ((1 << exp_width) - 1)).int()\n+    frac = (fp & ((1 << dtype.fp_mantissa_width) - 1)).int()\n+\n+    output = torch.where(exp == 0,\n+                         # subnormal\n+                         ((-1.0) ** sign) * (2.0 ** (1 - exp_bias)) * (frac / (2.0 ** dtype.fp_mantissa_width)),\n+                         # normal\n+                         ((-1.0) ** sign) * (2.0 ** (exp - exp_bias)) * (1.0 + frac / (2.0 ** dtype.fp_mantissa_width))).float()\n+\n+    extended_exp = ((1 << (tl.float32.primitive_bitwidth - tl.float32.fp_mantissa_width - 1)) - 1) << tl.float32.fp_mantissa_width\n+    # special cases, exp is 0b11..1\n+    if dtype == tl.float8e4:\n+        # float8e4m3 does not have infinities\n+        output[fp == torch.tensor(0b01111111, dtype=torch.int8)] = torch.nan\n+        output[fp == torch.tensor(0b11111111, dtype=torch.int8)] = torch.nan\n+    else:\n+        output = torch.where(exp == (1 << exp_width) - 1,\n+                             ((sign << (tl.float32.primitive_bitwidth - 1)) | extended_exp | (frac << (tl.float32.fp_mantissa_width - dtype.fp_mantissa_width))).view(torch.float32),\n+                             output)\n+    return output\n+\n+\n+@pytest.mark.parametrize(\"in_dtype\", [torch.float16, torch.bfloat16])\n+def test_convert_float16_to_float32(in_dtype):\n+    \"\"\"Tests that check convert_float_to_float32 function\"\"\"\n+    check_type_supported(in_dtype)\n+\n+    f16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16).view(in_dtype)\n+    f32_output = convert_float_to_float32(f16_input)\n+\n+    nan = f16_input.isnan()\n+    assert torch.all(f32_output[nan].isnan())\n+    inf = f16_input.isinf()\n+    assert torch.all(f32_output[inf].isinf())\n+    other = torch.logical_not(torch.logical_or(nan, inf))\n+    assert torch.all(f16_input[other] == f32_output[other])\n+\n+\n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.float32])\n def test_f8_xf16_roundtrip(in_dtype, out_dtype):\n@@ -909,6 +955,14 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     copy_kernel[grid](f8, xf16, n_elements, BLOCK_SIZE=1024)\n \n+    # exponent_mask = 0b01111100 for float8e5\n+    # exponent_mask = 0b01111000 for float8e4\n+    exponent_mask = 0b01111111 ^ ((1 << in_dtype.fp_mantissa_width) - 1)\n+    normal = torch.logical_and((f8_tensor & exponent_mask) != 0, (f8_tensor & exponent_mask) != exponent_mask)\n+    ref16 = convert_float_to_float32(f8_tensor, in_dtype)\n+    # WARN: currently only normal float8s are handled\n+    assert torch.all(xf16[normal] == ref16[normal])\n+\n     f8_output_tensor = torch.empty_like(xf16, dtype=torch.int8)\n     f8_output = triton.reinterpret(f8_output_tensor, in_dtype)\n     copy_kernel[grid](xf16, f8_output, n_elements, BLOCK_SIZE=1024)\n@@ -965,9 +1019,13 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         ),\n         dim=1,\n     )[0]\n-    # 1.9375 is float8 max\n+\n+    # WARN: only normalized numbers are handled\n+    f8_normal_min = 1 << in_dtype.fp_mantissa_width  # 0b00001000 for float8e4\n+    f8_normal_max = 0b01111110\n+    f16_min, f16_max = convert_float_to_float32(torch.tensor([f8_normal_min, f8_normal_max], dtype=torch.int8), in_dtype)\n     mismatch = torch.logical_and(\n-        abs_error != min_error, torch.logical_and(torch.isfinite(f16_input), torch.abs(f16_input) < 1.9375)\n+        abs_error != min_error, torch.logical_and(torch.isfinite(f16_input), torch.logical_and(torch.abs(f16_input) <= f16_max, torch.abs(f16_input) >= f16_min))\n     )\n     assert torch.all(\n         torch.logical_not(mismatch)"}]