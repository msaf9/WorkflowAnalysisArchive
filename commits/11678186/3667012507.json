[{"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 9, "deletions": 10, "changes": 19, "file_content_changes": "@@ -43,8 +43,7 @@ def matmul_no_scf_kernel(\n     for trans_b in [False, True]\n ])\n def test_gemm_no_scf(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n-    if not valid_on_Volta(NUM_WARPS, TRANS_A, TRANS_B):\n-        pytest.skip(\"Not valid on Volta\")\n+    guard_for_volta(NUM_WARPS, TRANS_A, TRANS_B)\n \n     SIZE_M, SIZE_N, SIZE_K = SHAPE\n     if (TRANS_A):\n@@ -84,8 +83,7 @@ def test_gemm_no_scf(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n     for trans_b in [False, True]\n ])\n def test_gemm_no_scf_int8(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n-    if not valid_on_Volta(NUM_WARPS, TRANS_A, TRANS_B, is_int8=True):\n-        pytest.skip(\"Not valid on Volta\")\n+    guard_for_volta(NUM_WARPS, TRANS_A, TRANS_B, is_int8=True)\n \n     SIZE_M, SIZE_N, SIZE_K = SHAPE\n \n@@ -201,8 +199,7 @@ def get_proper_err(a, b, golden):\n     [128, 64, 128, 4, 128, 64, 32, False, True],\n ])\n def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n-    if not valid_on_Volta(NUM_WARPS, TRANS_A, TRANS_B):\n-        pytest.skip(\"Not valid on Volta\")\n+    guard_for_volta(NUM_WARPS, TRANS_A, TRANS_B):\n \n     if (TRANS_A):\n         a = torch.randn((SIZE_K, SIZE_M), device='cuda', dtype=torch.float16).T\n@@ -279,8 +276,7 @@ def matmul_kernel(\n         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n         tl.store(c_ptrs, accumulator, c_mask)\n \n-    if not valid_on_Volta(num_warps, trans_a=False, trans_b=False, is_tf32=allow_tf32):\n-        pytest.skip(\"Not valid on Volta\")\n+    guard_for_volta(num_warps, trans_a=False, trans_b=False, is_tf32=allow_tf32)\n \n     # Configure the pytorch counterpart\n     torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n@@ -306,7 +302,7 @@ def matmul_kernel(\n         torch.testing.assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err))\n \n \n-def valid_on_Volta(num_warps, trans_a, trans_b, is_int8=False, is_tf32=False):\n+def guard_for_volta(num_warps, trans_a, trans_b, is_int8=False, is_tf32=False):\n     '''\n     Tell whether the test case is valid on Volta GPU.\n     Some features are WIP, so the corresponding support are missing.\n@@ -318,4 +314,7 @@ def valid_on_Volta(num_warps, trans_a, trans_b, is_int8=False, is_tf32=False):\n     is_on_Volta = capability[0] < 8\n     # TODO[Superjomn]: Remove the constraints below when features are ready\n     is_feature_ready = not (trans_a or trans_b)\n-    return is_on_Volta and is_feature_ready\n+\n+    if is_on_Volta:\n+        if not is_feature_ready:\n+            pytest.skip(\"Not valid on Volta\")"}]