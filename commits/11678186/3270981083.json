[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 78, "deletions": 17, "changes": 95, "file_content_changes": "@@ -2448,6 +2448,9 @@ struct DotOpConversionHelper {\n   }\n \n   void deduceMmaType(DotOp op) const { mmaType = getMmaType(op); }\n+  void deduceMmaType(Type operandTy) const {\n+    mmaType = getTensorCoreTypeFromOperand(operandTy);\n+  }\n \n   Type getShemPtrTy() const {\n     switch (mmaType) {\n@@ -2552,6 +2555,22 @@ struct DotOpConversionHelper {\n     return mmaMatShape.at(mmaType);\n   }\n \n+  // Deduce the TensorCoreType from either $a or $b's type. This method is not\n+  // safe, but we cannot get the DotOp in some getmaMatShape usage case.\n+  TensorCoreType getTensorCoreTypeFromOperand(Type operandTy) const {\n+    auto tensorTy = operandTy.cast<RankedTensorType>();\n+    auto elemTy = tensorTy.getElementType();\n+    if (elemTy.isF16())\n+      return TensorCoreType::FP32_FP16_FP16_FP32;\n+    if (elemTy.isF32())\n+      return TensorCoreType::FP32_TF32_TF32_FP32;\n+    if (elemTy.isBF16())\n+      return TensorCoreType::FP32_BF16_BF16_FP32;\n+    if (elemTy.isInteger(8))\n+      return TensorCoreType::INT32_INT8_INT8_INT32;\n+    return TensorCoreType::NOT_APPLICABLE;\n+  }\n+\n   int getVec() const {\n     assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n            \"Unknown mma type found.\");\n@@ -2664,9 +2683,9 @@ struct MMA16816ConversionHelper {\n   MmaEncodingAttr mmaLayout;\n   ArrayRef<unsigned int> wpt;\n \n-  int mmaInstrM{-1}, mmaInstrN{-1}, mmaInstrK{-1};\n-  int matShapeM{-1}, matShapeN{-1}, matShapeK{-1};\n-  int numRepM{-1}, numRepN{-1}, numRepK{-1};\n+  // int mmaInstrM{-1}, mmaInstrN{-1}, mmaInstrK{-1};\n+  // int matShapeM{-1}, matShapeN{-1}, matShapeK{-1};\n+  // int numRepM{-1}, numRepN{-1}, numRepK{-1};\n   Value thread, lane, warp, warpMN, warpN, warpM;\n   // size_t aElemBytes{}, bElemBytes{};\n \n@@ -2686,16 +2705,6 @@ struct MMA16816ConversionHelper {\n         thread(thread) {\n     wpt = mmaLayout.getWarpsPerCTA();\n \n-    auto mmaInstrShape = helper.getMmaInstrShape();\n-    mmaInstrM = mmaInstrShape[0];\n-    mmaInstrN = mmaInstrShape[1];\n-    mmaInstrK = mmaInstrShape[2];\n-\n-    auto matShape = helper.getMmaMatShape();\n-    matShapeM = matShape[0];\n-    matShapeN = matShape[1];\n-    matShapeK = matShape[2];\n-\n     // int NK = aShape[1];\n     // // shape / shape_per_cta\n     // numRepM = std::max<int>(dShape[0] / (wpt[0] * mmaInstrM), 1);\n@@ -2713,12 +2722,55 @@ struct MMA16816ConversionHelper {\n     // bElemBytes = bTensorTy.getElementTypeBitWidth() / 8;\n   }\n \n+  // Get the mmaInstrShape from either $a or $b.\n+  std::tuple<int, int, int> getMmaInstrShape(Type operand) const {\n+    helper.deduceMmaType(operand);\n+    auto mmaInstrShape = helper.getMmaInstrShape();\n+    int mmaInstrM = mmaInstrShape[0];\n+    int mmaInstrN = mmaInstrShape[1];\n+    int mmaInstrK = mmaInstrShape[2];\n+    return std::make_tuple(mmaInstrM, mmaInstrN, mmaInstrK);\n+  }\n+\n+  std::tuple<int, int, int> getMmaMatShape(Type operand) const {\n+    helper.deduceMmaType(operand);\n+    auto matShape = helper.getMmaMatShape();\n+    int matShapeM = matShape[0];\n+    int matShapeN = matShape[1];\n+    int matShapeK = matShape[2];\n+    return std::make_tuple(matShapeM, matShapeN, matShapeK);\n+  }\n+\n+  // \\param operand is either $a or $b's type.\n+  inline int getNumRepM(Type operand, int M) const {\n+    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(operand);\n+    return std::max<int>(M / (wpt[0] * mmaInstrM), 1);\n+  }\n+\n+  // \\param operand is either $a or $b's type.\n+  inline int getNumRepN(Type operand, int N) const {\n+    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(operand);\n+    return std::max<int>(N / (wpt[1] * mmaInstrN), 1);\n+  }\n+\n+  // \\param operand is either $a or $b's type.\n+  inline int getNumRepK(Type operand, int K) const {\n+    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(operand);\n+    return std::max<int>(K / mmaInstrK, 1);\n+  }\n+\n   // Loading $a from smem to registers, returns a LLVM::Struct.\n   Value loadA(Value tensor, Value llTensor) const {\n     auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n+    auto shape = aTensorTy.getShape();\n \n     ValueTable ha;\n     std::function<void(int, int)> loadFn;\n+    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n+    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaMatShape(aTensorTy);\n+    int numRepM = getNumRepM(aTensorTy, shape[0]);\n+    int numRepK = getNumRepK(aTensorTy, shape[1]);\n+\n     if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n       // load from smem\n       loadFn = getLoadMatrixFn(\n@@ -2749,6 +2801,13 @@ struct MMA16816ConversionHelper {\n   // Loading $b from smem to registers, returns a LLVM::Struct.\n   Value loadB(Value tensor, Value llTensor) {\n     ValueTable hb;\n+    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+    auto shape = tensorTy.getShape();\n+    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(tensorTy);\n+    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaMatShape(tensorTy);\n+    int numRepK = getNumRepK(tensorTy, shape[0]);\n+    int numRepN = getNumRepN(tensorTy, shape[1]);\n+\n     auto loadFn = getLoadMatrixFn(\n         tensor, llTensor, mmaLayout, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n         0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShpae*/,\n@@ -2777,7 +2836,7 @@ struct MMA16816ConversionHelper {\n \n   // Conduct the Dot conversion.\n   // Input the \\param a, \\param b, \\param c, all of them are result of loading.\n-  LogicalResult convertDot(DotOp op) const {\n+  LogicalResult convertDot(DotOp op, DotOpAdaptor adapter) const {\n     Value a = op.a();\n     Value b = op.b();\n     Value c = op.c();\n@@ -2795,9 +2854,11 @@ struct MMA16816ConversionHelper {\n \n     int NK = aShape[1];\n     // shape / shape_per_cta\n-    int numRepM = std::max<int>(dShape[0] / (wpt[0] * mmaInstrM), 1);\n-    int numRepN = std::max<int>(dShape[1] / (wpt[1] * mmaInstrN), 1);\n-    int numRepK = std::max<int>(NK / mmaInstrK, 1);\n+    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n+    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaMatShape(aTensorTy);\n+    int numRepM = getNumRepM(aTensorTy, dShape[0]);\n+    int numRepN = getNumRepN(aTensorTy, dShape[1]);\n+    int numRepK = getNumRepK(aTensorTy, aShape[1]);\n \n     ValueTable ha = getValuesFromDotOperandLayoutStruct(a, numRepM, numRepK);\n     ValueTable hb = getValuesFromDotOperandLayoutStruct("}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 25, "deletions": 1, "changes": 26, "file_content_changes": "@@ -415,6 +415,30 @@ void SharedEncodingAttr::print(AsmPrinter &printer) const {\n           << \"}>\";\n }\n \n+//===----------------------------------------------------------------------===//\n+// DotOperand Encoding\n+//===----------------------------------------------------------------------===//\n+Attribute DotOperandEncodingAttr::parse(AsmParser &parser, Type type) {\n+  if (parser.parseLess().failed())\n+    return {};\n+  NamedAttrList attrs;\n+  if (parser.parseOptionalAttrDict(attrs).failed())\n+    return {};\n+  if (parser.parseGreater().failed())\n+    return {};\n+  unsigned opIdx = attrs.get(\"opIdx\").cast<IntegerAttr>().getInt();\n+  Attribute parent = attrs.get(\"parent\");\n+\n+  return parser.getChecked<DotOperandEncodingAttr>(parser.getContext(), opIdx,\n+                                                   parent);\n+}\n+\n+void DotOperandEncodingAttr::print(mlir::AsmPrinter &printer) const {\n+  printer << \"<{\"\n+          << \"opIdx = \" << getOpIdx() << \", \"\n+          << \"parent = \" << getParent() << \"}>\";\n+}\n+\n //===----------------------------------------------------------------------===//\n // InsertSliceAsyncOp\n //===----------------------------------------------------------------------===//\n@@ -586,4 +610,4 @@ LogicalResult TritonGPUDialect::verifyOperationAttribute(Operation *op,\n                                                          NamedAttribute attr) {\n   // TODO: fill this.\n   return success();\n-}\n\\ No newline at end of file\n+}"}]