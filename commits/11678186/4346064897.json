[{"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -131,11 +131,14 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n     auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto srcTy = cvt.getOperand().getType().cast<RankedTensorType>();\n     auto retTy = cvt.getResult().getType().dyn_cast<RankedTensorType>();\n     if (!retTy)\n       return failure();\n     if (!isa<triton::gpu::DotOperandEncodingAttr>(retTy.getEncoding()))\n       return failure();\n+    if (isa<triton::gpu::SharedEncodingAttr>(srcTy.getEncoding()))\n+      return failure();\n     //\n     Operation *argOp = cvt.getOperand().getDefiningOp();\n     //"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -182,7 +182,6 @@ LogicalResult LoopPipeliner::initialize() {\n                     .cast<triton::PointerType>()\n                     .getPointeeType();\n       unsigned width = vec * ty.getIntOrFloatBitWidth();\n-      llvm::outs() << vec << \" \" << ty.getIntOrFloatBitWidth() << \"\\n\";\n       if (width >= 32)\n         allLoads.push_back(loadOp);\n     }"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1725,7 +1725,6 @@ def _init_handles(self):\n         if self.shared > max_shared:\n             raise OutOfResources(self.shared, max_shared, \"shared memory\")\n         mod, func, n_regs, n_spills = cuda_utils.load_binary(self.metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n-        print(self.shared, n_regs, n_spills)\n         self.cu_module = mod\n         self.cu_function = func\n "}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 5, "deletions": 9, "changes": 14, "file_content_changes": "@@ -156,7 +156,7 @@\n \n @triton.autotune(\n     configs=[\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n     ],\n     key=['M', 'N', 'K'],\n )\n@@ -221,7 +221,6 @@ def matmul_kernel(\n         # error or (worse!) incorrect results.\n         a = tl.load(a_ptrs)\n         b = tl.load(b_ptrs)\n-        b = b.to(tl.float8, bitcast=True).to(tl.float16)\n         # We accumulate along the K dimension\n         accumulator += tl.dot(a, b)\n         # Advance the ptrs to the next K block\n@@ -256,6 +255,8 @@ def leaky_relu(x):\n def matmul(a, b, activation=None):\n     # checks constraints\n     assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n+    assert a.is_contiguous(), \"matrix A must be contiguous\"\n+    assert b.is_contiguous(), \"matrix B must be contiguous\"\n     M, K = a.shape\n     K, N = b.shape\n     assert (\n@@ -269,14 +270,12 @@ def matmul(a, b, activation=None):\n     )\n     matmul_kernel[grid](\n         a, b, c,\n-        # a, b, c,\n         M, N, K,\n         a.stride(0), a.stride(1),\n         b.stride(0), b.stride(1),\n         c.stride(0), c.stride(1),\n         ACTIVATION=activation,\n     )\n-    # print(h.asm[\"ttir\"])\n     return c\n \n \n@@ -288,10 +287,7 @@ def matmul(a, b, activation=None):\n \n torch.manual_seed(0)\n a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-b = torch.randint(-128, 127, (512, 512), dtype=torch.int8, device='cuda')\n-# b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-\n-b = b.t()\n+b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n triton_output = matmul(a, b, activation=None)\n torch_output = torch.matmul(a, b)\n print(f\"triton_output={triton_output}\")\n@@ -339,4 +335,4 @@ def benchmark(M, N, K, provider):\n     return perf(ms), perf(max_ms), perf(min_ms)\n \n \n-# benchmark.run(show_plots=True, print_data=True)\n+benchmark.run(show_plots=True, print_data=True)"}]