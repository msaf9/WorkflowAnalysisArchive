[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 9, "deletions": 2, "changes": 11, "file_content_changes": "@@ -17,7 +17,7 @@ jobs:\n         id: set-matrix\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n-            echo '::set-output name=matrix::[[\"self-hosted\", \"A10\"], \"macos-10.15\"]'\n+            echo '::set-output name=matrix::[[\"self-hosted\", \"A10\"], [\"self-hosted\", \"V100\"], \"macos-10.15\"]'\n           else\n             echo '::set-output name=matrix::[\"ubuntu-latest\", \"macos-10.15\"]'\n           fi\n@@ -79,11 +79,18 @@ jobs:\n           lit -v \"$LIT_TEST_DIR\"\n \n       - name: Run python tests\n-        if: ${{matrix.runner[0] == 'self-hosted'}}\n+        if: ${{matrix.runner[0] == 'self-hosted' && matrix.runner[1] == 'A10'}}\n         run: |\n           cd python/tests\n           pytest\n \n+      # TODO[Superjomn] Enable all the tests on V100 if available\n+      - name: Run python tests on V100\n+        if: ${{matrix.runner[0] == 'self-hosted' && matrix.runner[1] == 'V100'}}\n+        run: |\n+          cd python/tests\n+          pytest test_gemm.py::test_gemm_no_scf_for_mmav1\n+\n       - name: Run CXX unittests\n         run: |\n           cd python/"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 12, "deletions": 8, "changes": 20, "file_content_changes": "@@ -87,20 +87,24 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n         // number of rows per phase\n         int perPhase = 128 / (shape[order[0]] * (eltTy.getIntOrFloatBitWidth() / 8));\n         perPhase = std::max<int>(perPhase, 1);\n-        \n+\n         // index of the inner dimension in `order`\n         unsigned inner = (opIdx == 0) ? 0 : 1;\n \n         // ---- begin version 1 ----\n         if (version == 1) {\n           bool is_row = order[0] != 0;\n-          bool is_vec4 = opIdx == 0 ? is_row && (shape[order[0]] <= 16) :\n-                                      !is_row && (shape[order[0]] <= 16);\n+          bool is_vec4 = opIdx == 0 ? !is_row && (shape[order[0]] <= 16) :\n+              is_row && (shape[order[0]] <= 16);\n+          // TODO[Superjomn]: Support the case when is_vec4=false later\n+          // Currently, we only support ld.v2, for the mma layout varies with different ld vector width.\n+          is_vec4 = true;\n           int pack_size = opIdx == 0 ? ((is_row || is_vec4) ? 1 : 2) :\n                                        ((is_row && !is_vec4) ? 2 : 1);\n           int rep = 2 * pack_size;\n           int maxPhase = (order[inner] == 1 ? 8 : 4) / perPhase;\n-          return $_get(context, 2 * rep, perPhase, maxPhase, order);\n+          int vec = 2 * rep;\n+          return $_get(context, vec, perPhase, maxPhase, order);\n         }\n \n         // ---- begin version 2 ----\n@@ -110,23 +114,23 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n           // for now, disable swizzle when using transposed int8 tensor cores\n           if (eltTy.isInteger(8) && order[0] == inner)\n             return $_get(context, 1, 1, 1, order);\n-            \n+\n           // --- handle A operand ---\n           if (opIdx == 0) { // compute swizzling for A operand\n               int vec = (order[0] == 1) ? matShape[2] : matShape[0]; // k : m\n               int mmaStride = (order[0] == 1) ? matShape[0] : matShape[2];\n               int maxPhase = mmaStride / perPhase;\n               return $_get(context, vec, perPhase, maxPhase, order);\n-          } \n+          }\n \n           // --- handle B operand ---\n           if (opIdx == 1) {\n               int vec = (order[0] == 1) ? matShape[1] : matShape[2]; // n : k\n               int mmaStride = (order[0] == 1) ? matShape[2] : matShape[1];\n               int maxPhase = mmaStride / perPhase;\n               return $_get(context, vec, perPhase, maxPhase, order);\n-          } \n-            \n+          }\n+\n           llvm_unreachable(\"invalid operand index\");\n         }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 62, "deletions": 76, "changes": 138, "file_content_changes": "@@ -39,15 +39,6 @@ using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n-// Forward declaration necessary functions locates in TritonGPUToLLVM.cpp .\n-llvm::SmallVector<mlir::Value>\n-getElementsFromStruct(mlir::Location loc, mlir::Value llvmStruct,\n-                      mlir::ConversionPatternRewriter &rewriter);\n-\n-mlir::LLVM::SharedMemoryObject\n-getSharedMemoryObjectFromStruct(mlir::Location loc, mlir::Value llvmStruct,\n-                                mlir::ConversionPatternRewriter &rewriter);\n-\n // Helper for conversion of DotOp with mma<version=1>, that is sm<80\n struct DotOpMmaV1ConversionHelper {\n   MmaEncodingAttr mmaLayout;\n@@ -710,17 +701,13 @@ class MMA16816SmemLoader {\n       if (kOrder == 1) {\n         elems[0] = load(gep(elemPtrTy, ptr, sOffsetElemVal));\n         elems[1] = load(gep(elemPtrTy, ptr2, sOffsetElemVal));\n-        elems[2] =\n-            load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n-        elems[3] =\n-            load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n+        elems[2] = load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n+        elems[3] = load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n       } else {\n         elems[0] = load(gep(elemPtrTy, ptr, sOffsetElemVal));\n         elems[2] = load(gep(elemPtrTy, ptr2, sOffsetElemVal));\n-        elems[1] =\n-            load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n-        elems[3] =\n-            load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n+        elems[1] = load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n+        elems[3] = load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n       }\n       return {elems[0], elems[1], elems[2], elems[3]};\n \n@@ -952,7 +939,6 @@ struct MMA16816ConversionHelper {\n   // Loading $a from smem to registers, returns a LLVM::Struct.\n   Value loadA(Value tensor, const SharedMemoryObject &smemObj) const {\n     auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto layout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n \n     SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n                                aTensorTy.getShape().end());\n@@ -973,12 +959,13 @@ struct MMA16816ConversionHelper {\n     if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n       Value warpM = getWarpM(shape[0]);\n       // load from smem\n-      int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / matShapeM);\n-      loadFn = getLoadMatrixFn(\n-          tensor, smemObj, mmaLayout, wpt /*wpt*/,\n-          1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShape*/,\n-          {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/,\n-          true /*isA*/);\n+      int wpt =\n+          std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / matShapeM);\n+      loadFn =\n+          getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/,\n+                          {mmaInstrM, mmaInstrK} /*instrShape*/,\n+                          {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/,\n+                          ha /*vals*/, true /*isA*/);\n     } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n       // load from registers, used in gemm fuse\n       // TODO(Superjomn) Port the logic.\n@@ -1000,7 +987,6 @@ struct MMA16816ConversionHelper {\n   Value loadB(Value tensor, const SharedMemoryObject &smemObj) {\n     ValueTable hb;\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto layout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n \n     SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n                                tensorTy.getShape().end());\n@@ -1017,12 +1003,13 @@ struct MMA16816ConversionHelper {\n     int numRepN = getNumRepN(tensorTy, shape[1]);\n \n     Value warpN = getWarpN(shape[1]);\n-    int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / matShapeN);\n-    auto loadFn = getLoadMatrixFn(\n-        tensor, smemObj, mmaLayout,  wpt /*wpt*/,\n-        0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShape*/,\n-        {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/,\n-        false /*isA*/);\n+    int wpt =\n+        std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / matShapeN);\n+    auto loadFn =\n+        getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/,\n+                        {mmaInstrK, mmaInstrN} /*instrShape*/,\n+                        {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/,\n+                        hb /*vals*/, false /*isA*/);\n \n     for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n       for (int k = 0; k < numRepK; ++k)\n@@ -1167,6 +1154,7 @@ struct MMA16816ConversionHelper {\n       SmallVector<Value> ptrs(numPtrs);\n \n       Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+\n       Type smemPtrTy = helper.getShemPtrTy();\n       for (int i = 0; i < numPtrs; ++i) {\n         ptrs[i] =\n@@ -1292,7 +1280,6 @@ struct DotOpFMAConversionHelper {\n     auto blockedLayout = dotOpLayout.getParent().cast<BlockedEncodingAttr>();\n     auto shapePerCTA = getShapePerCTA(blockedLayout);\n     auto sizePerThread = getSizePerThread(blockedLayout);\n-    auto order = blockedLayout.getOrder();\n \n     // TODO[Superjomn]: we assume the k aixs is fixed for $a and $b here, fix it\n     // if not.\n@@ -1342,17 +1329,15 @@ Value DotOpMmaV1ConversionHelper::loadA(\n   SmallVector<unsigned> order(sharedLayout.getOrder().begin(),\n                               sharedLayout.getOrder().end());\n \n-  // TODO [Superjomn]: transA cannot be accessed in ConvertLayoutOp.\n-  bool transA = false;\n-  if (transA) {\n-    std::swap(shape[0], shape[1]);\n-    std::swap(order[0], order[1]);\n-  }\n-\n   Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+  Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n \n   bool isARow = order[0] != 0;\n   bool isAVec4 = !isARow && shape[order[0]] <= 16; // fp16*4 = 16bytes\n+  // TODO[Superjomn]: Support the case when isAVec4=false later\n+  // Currently, we only support ld.v2, for the mma layout varies with different\n+  // ld vector width.\n+  isAVec4 = true;\n   int packSize0 = (isARow || isAVec4) ? 1 : 2;\n \n   SmallVector<int> fpw({2, 2, 1});\n@@ -1362,6 +1347,16 @@ Value DotOpMmaV1ConversionHelper::loadA(\n   SmallVector<int> rep({repM, 0, repK}); // pad N with 0\n   SmallVector<int> spw({spwM, 0, 1});    // pad N with 0\n \n+  auto [offsetAM, offsetAK, _0, _1] =\n+      computeOffsets(thread, isARow, false, fpw, spw, rep, rewriter, loc);\n+  // TODO [Superjomn]: transA cannot be accessed in ConvertLayoutOp.\n+  bool transA = false;\n+  if (transA) {\n+    std::swap(shape[0], shape[1]);\n+    std::swap(offsetAM, offsetAK);\n+    std::swap(order[0], order[1]);\n+  }\n+\n   int vecA = sharedLayout.getVec();\n \n   auto strides = smemObj.strides;\n@@ -1373,9 +1368,6 @@ Value DotOpMmaV1ConversionHelper::loadA(\n   int strideRepM = wpt[0] * fpw[0] * 8;\n   int strideRepK = 1;\n \n-  auto [offsetAM, offsetAK, _0, _1] =\n-      computeOffsets(thread, isARow, false, fpw, spw, rep, rewriter, loc);\n-\n   // swizzling\n   int perPhaseA = sharedLayout.getPerPhase();\n   int maxPhaseA = sharedLayout.getMaxPhase();\n@@ -1398,19 +1390,14 @@ Value DotOpMmaV1ConversionHelper::loadA(\n   }\n \n   Type f16x2Ty = vec_ty(f16_ty, 2);\n-  // One thread get 8 elements as result\n-  Type retTy =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector(8, type::f32Ty(ctx)));\n \n   // prepare arguments\n   SmallVector<Value> ptrA(numPtrA);\n \n   std::map<std::pair<int, int>, std::pair<Value, Value>> has;\n-  auto smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n   for (int i = 0; i < numPtrA; i++)\n-    ptrA[i] = gep(ptr_ty(f16_ty), smem, offA[i]);\n+    ptrA[i] = gep(ptr_ty(f16_ty), smemBase, offA[i]);\n \n-  auto instrShape = getMmaInstrShape();\n   unsigned numM = std::max<int>(rep[0] * shape[0] / (spw[0] * wpt[0]), 1);\n \n   Type f16PtrTy = ptr_ty(f16_ty);\n@@ -1420,7 +1407,7 @@ Value DotOpMmaV1ConversionHelper::loadA(\n   };\n   auto loadA = [&](int m, int k) {\n     int offidx = (isARow ? k / 4 : m) % numPtrA;\n-    Value thePtrA = gep(f16PtrTy, smem, offA[offidx]);\n+    Value thePtrA = gep(f16PtrTy, smemBase, offA[offidx]);\n \n     int stepAM = isARow ? m : m / numPtrA * numPtrA;\n     int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n@@ -1446,12 +1433,10 @@ Value DotOpMmaV1ConversionHelper::loadA(\n \n   for (unsigned k = 0; k < NK; k += 4)\n     for (unsigned m = 0; m < numM / 2; ++m)\n-      if (!has.count({m, k}))\n-        loadA(m, k);\n+      loadA(m, k);\n \n   SmallVector<Value> elems;\n   elems.reserve(has.size() * 2);\n-  auto vecTy = vec_ty(f16_ty, 2);\n   for (auto item : has) { // has is a map, the key should be ordered.\n     elems.push_back(item.second.first);\n     elems.push_back(item.second.second);\n@@ -1466,7 +1451,6 @@ Value DotOpMmaV1ConversionHelper::loadB(\n     Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n     ConversionPatternRewriter &rewriter) const {\n   // smem\n-  Value smem = smemObj.base;\n   auto strides = smemObj.strides;\n \n   auto *ctx = rewriter.getContext();\n@@ -1478,46 +1462,50 @@ Value DotOpMmaV1ConversionHelper::loadB(\n   SmallVector<unsigned> order(sharedLayout.getOrder().begin(),\n                               sharedLayout.getOrder().end());\n \n-  // TODO [Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n-  bool transB = false;\n-\n-  if (transB) {\n-    std::swap(order[0], order[1]);\n-    std::swap(shape[0], shape[1]);\n-  }\n+  Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n \n   bool isBRow = order[0] != 0;\n   bool isBVec4 = isBRow && shape[order[0]] <= 16;\n+  // TODO[Superjomn]: Support the case when isBVec4=false later\n+  // Currently, we only support ld.v2, for the mma layout varies with different\n+  // ld vector width.\n+  isBVec4 = true;\n   int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n   SmallVector<int> fpw({2, 2, 1});\n   SmallVector<int> rep({0, 2 * packSize1, 1});       // pad M with 0\n   SmallVector<int> spw({0, fpw[1] * 4 * rep[1], 1}); // pad M with 0\n   int vecB = sharedLayout.getVec();\n+\n   Value strideBN = isBRow ? i32_val(1) : strides[1];\n   Value strideBK = isBRow ? strides[0] : i32_val(1);\n   Value strideB0 = isBRow ? strideBN : strideBK;\n   Value strideB1 = isBRow ? strideBK : strideBN;\n   int strideRepN = wpt[1] * fpw[1] * 8;\n   int strideRepK = 1;\n \n+  // TODO [Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n+  bool transB = false;\n+\n+  auto [_0, _1, offsetBN, offsetBK] =\n+      computeOffsets(thread, false, isBRow, fpw, spw, rep, rewriter, loc);\n+  if (transB) {\n+    std::swap(order[0], order[1]);\n+    std::swap(shape[0], shape[1]);\n+    std::swap(offsetBK, offsetBN);\n+  }\n+\n   // swizzling\n-  int perPhaseA = sharedLayout.getPerPhase();\n-  int maxPhaseA = sharedLayout.getMaxPhase();\n   int perPhaseB = sharedLayout.getPerPhase();\n   int maxPhaseB = sharedLayout.getMaxPhase();\n   int stepB0 = isBRow ? strideRepN : strideRepK;\n   int numPtrB = std::max(2 * perPhaseB * maxPhaseB / stepB0, 1);\n   int NK = shape[0];\n \n-  auto [_0, _1, offsetBN, offsetBK] =\n-      computeOffsets(thread, false, isBRow, fpw, spw, rep, rewriter, loc);\n-  if (transB)\n-    std::swap(offsetBK, offsetBN);\n-\n   Value offB0 = isBRow ? offsetBN : offsetBK;\n   Value offB1 = isBRow ? offsetBK : offsetBN;\n   Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n   Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+\n   offB0 = add(offB0, cSwizzleOffset);\n   SmallVector<Value> offB(numPtrB);\n   for (int i = 0; i < numPtrB; ++i) {\n@@ -1549,6 +1537,7 @@ Value DotOpMmaV1ConversionHelper::loadB(\n     Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n                        mul(i32_val(stepBK), strideBK));\n     Value pb = gep(f16PtrTy, thePtrB, offset);\n+\n     Value hb =\n         load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n     // record lds that needs to be moved\n@@ -1651,9 +1640,12 @@ DotOpMmaV1ConversionHelper::extractLoadedOperand(\n   SmallVector<Value> elems =\n       getElementsFromStruct(llStruct.getLoc(), llStruct, rewriter);\n \n-  for (int k = 0, offset = 0, i = 0; k < NK && offset < elems.size();\n-       k += 4, i++, offset += 2) {\n-    rcds[{i, k}] = std::make_pair(elems[offset], elems[offset + 1]);\n+  int offset = 0;\n+  for (int i = 0; offset < elems.size(); ++i) {\n+    for (int k = 0; k < NK; k += 4) {\n+      rcds[{i, k}] = std::make_pair(elems[offset], elems[offset + 1]);\n+      offset += 2;\n+    }\n   }\n \n   return rcds;\n@@ -1675,9 +1667,7 @@ Value DotOpFMAConversionHelper::loadA(\n   int strideAK = isARow ? 1 : aShape[0];\n   int strideA0 = isARow ? strideAK : strideAM;\n   int strideA1 = isARow ? strideAM : strideAK;\n-  int lda = isARow ? strideAM : strideAK;\n   int aNumPtr = 8;\n-  int bNumPtr = 8;\n   int NK = aShape[1];\n \n   auto shapePerCTA = getShapePerCTA(dLayout);\n@@ -1686,13 +1676,11 @@ Value DotOpFMAConversionHelper::loadA(\n   Value _0 = i32_val(0);\n \n   Value mContig = i32_val(sizePerThread[order[1]]);\n-  Value nContig = i32_val(sizePerThread[order[0]]);\n \n   // threadId in blocked layout\n   auto threadIds = getThreadIds(thread, shapePerCTA, order, rewriter, loc);\n \n   Value threadIdM = threadIds[0];\n-  Value threadIdN = threadIds[1];\n \n   Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n   Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n@@ -1745,7 +1733,6 @@ Value DotOpFMAConversionHelper::loadB(\n   int strideBK = isBRow ? bShape[1] : 1;\n   int strideB0 = isBRow ? strideBN : strideBK;\n   int strideB1 = isBRow ? strideBK : strideBN;\n-  int ldb = isBRow ? strideBK : strideBN;\n   int bNumPtr = 8;\n   int NK = bShape[0];\n \n@@ -1754,7 +1741,6 @@ Value DotOpFMAConversionHelper::loadB(\n \n   Value _0 = i32_val(0);\n \n-  Value mContig = i32_val(sizePerThread[order[1]]);\n   Value nContig = i32_val(sizePerThread[order[0]]);\n \n   // threadId in blocked layout"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 24, "deletions": 13, "changes": 37, "file_content_changes": "@@ -62,12 +62,11 @@ namespace LLVM {\n static StringRef getStructAttrsAttrName() { return \"llvm.struct_attrs\"; }\n \n // A helper function for using printf in LLVM conversion.\n-void llPrintf(StringRef msg, ValueRange args,\n-              ConversionPatternRewriter &rewriter);\n+void vprintf(StringRef msg, ValueRange args,\n+             ConversionPatternRewriter &rewriter);\n \n-// Helper function\n-#define tid_val() getThreadId(rewriter, loc)\n-#define llprintf(fmt, ...) LLVM::llPrintf(fmt, {__VA_ARGS__}, rewriter)\n+void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n+                   std::string elem_repr, ConversionPatternRewriter &builder);\n \n } // namespace LLVM\n } // namespace mlir\n@@ -3537,8 +3536,8 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n   SmallVector<Value> resVals(resSize);\n \n   auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n-    auto ha = has[{m, k}];\n-    auto hb = hbs[{n, k}];\n+    auto ha = has.at({m, k});\n+    auto hb = hbs.at({n, k});\n     std::vector<size_t> idx{{\n         (m * 2 + 0) + (n * 4 + 0) * numM, // row0\n         (m * 2 + 0) + (n * 4 + 1) * numM,\n@@ -3554,13 +3553,13 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n \n     auto *resOprs = builder.newListOperand(8, \"=f\");\n     auto *AOprs = builder.newListOperand({\n-        {ha.first, \"f\"},\n-        {ha.second, \"f\"},\n+        {ha.first, \"r\"},\n+        {ha.second, \"r\"},\n     });\n \n     auto *BOprs = builder.newListOperand({\n-        {hb.first, \"f\"},\n-        {hb.second, \"f\"},\n+        {hb.first, \"r\"},\n+        {hb.second, \"r\"},\n     });\n     auto *COprs = builder.newListOperand();\n     for (int i = 0; i < 8; ++i)\n@@ -4806,11 +4805,23 @@ namespace mlir {\n \n namespace LLVM {\n \n-void llPrintf(StringRef msg, ValueRange args,\n-              ConversionPatternRewriter &rewriter) {\n+void vprintf(StringRef msg, ValueRange args,\n+             ConversionPatternRewriter &rewriter) {\n   PrintfOpConversion::llPrintf(msg, args, rewriter);\n }\n \n+void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n+                   std::string elem_repr, ConversionPatternRewriter &builder) {\n+  std::string fmt = info + \" t-%d \";\n+  std::vector<Value> new_arr({thread});\n+  for (int i = 0; i < arr.size(); ++i) {\n+    fmt += elem_repr + ((i == arr.size() - 1) ? \"\" : \", \");\n+    new_arr.push_back(arr[i]);\n+  }\n+\n+  vprintf(fmt, new_arr, builder);\n+}\n+\n } // namespace LLVM\n \n TritonLLVMConversionTarget::TritonLLVMConversionTarget("}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -111,6 +111,8 @@\n   LLVM::createIndexConstant(rewriter, loc, this->getTypeConverter(),           \\\n                             __VA_ARGS__)\n \n+#define tid_val() getThreadId(rewriter, loc)\n+\n namespace mlir {\n namespace LLVM {\n using namespace mlir::triton;"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -756,6 +756,7 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto mod = op->getParentOfType<mlir::ModuleOp>();\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n     int version = computeCapabilityToMMAVersion(computeCapability);\n+\n     auto newRetType = RankedTensorType::get(\n         retShape, oldRetType.getElementType(),\n         triton::gpu::MmaEncodingAttr::get("}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -1383,6 +1383,11 @@ void init_triton_translation(py::module &m) {\n         llvm::SMDiagnostic error;\n         std::unique_ptr<llvm::Module> module =\n             llvm::parseIR(buffer->getMemBufferRef(), error, context);\n+        if (!module)\n+          llvm::report_fatal_error(\n+              \"failed to parse IR: \" + error.getMessage() +\n+              \"lineno: \" + std::to_string(error.getLineNo()));\n+\n         // translate module to PTX\n         auto ptxCode =\n             triton::translateLLVMIRToPTX(*module, capability, version);"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 19, "deletions": 1, "changes": 20, "file_content_changes": "@@ -172,7 +172,7 @@ def get_proper_err(a, b, golden):\n     [128, 64, 128, 4, 128, 64, 128, False, False],\n     [16, 16, 16, 16, 16, 16, 16, False, False],  # wpt overflow issue\n     # K-Forloop\n-    #[16, 16, 64, 4, 8, 8, 8, False, False],  # Wrap threads\n+    # [16, 16, 64, 4, 8, 8, 8, False, False],  # Wrap threads\n     [32, 32, 64, 4, 32, 32, 32, False, False],  # Single shared encoding\n     [16, 16, 128, 4, 16, 16, 16, False, False],  # Single shared encoding and small k\n     [64, 32, 128, 4, 64, 32, 64, False, False],\n@@ -292,3 +292,21 @@ def matmul_kernel(\n         torch.testing.assert_close(c, golden, rtol=max(1e-2, 1.5 * golden_rel_err), atol=max(1e-2, 1.5 * golden_abs_err))\n     else:\n         torch.testing.assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err))\n+\n+\n+# NOTE this is useful only on Volta GPU.\n+@pytest.mark.parametrize('SHAPE,NUM_WARPS,TRANS_A,TRANS_B', [\n+    (shape, num_warps, trans_a, trans_b)\n+    for shape in [\n+        [16, 16, 16],\n+        [16, 16, 32],\n+        [32, 16, 16],\n+        [32, 32, 32],\n+        [128, 16, 16],\n+    ]\n+    for num_warps in [1]\n+    for trans_a in [False]\n+    for trans_b in [False]\n+])\n+def test_gemm_no_scf_for_mmav1(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n+    test_gemm_no_scf(SHAPE, NUM_WARPS, TRANS_A, TRANS_B)"}]