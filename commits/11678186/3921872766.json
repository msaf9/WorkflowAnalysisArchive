[{"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -25,6 +25,10 @@ class DialectInferLayoutInterface\n public:\n   DialectInferLayoutInterface(Dialect *dialect) : Base(dialect) {}\n \n+  virtual LogicalResult\n+  inferTransOpEncoding(Attribute operandEncoding,\n+                       Attribute &resultEncoding) const = 0;\n+\n   virtual LogicalResult\n   inferReduceOpEncoding(Attribute operandEncoding, unsigned axis,\n                         Attribute &resultEncoding) const = 0;"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -289,7 +289,7 @@ def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n }\n \n def TT_TransOp : TT_Op<\"trans\", [NoSideEffect,\n-                                 SameOperandsAndResultElementType]> {\n+                                 DeclareOpInterfaceMethods<InferTypeOpInterface>]> {\n \n     let summary = \"transpose a tensor\";\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 34, "deletions": 6, "changes": 40, "file_content_changes": "@@ -376,31 +376,59 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n   );\n \n   let builders = [\n-    // specific for MMAV1(Volta)\n+     // Specially for MMAV1(Volta)\n+    AttrBuilder<(ins \"int\":$versionMajor,\n+                     \"int\":$numWarps,\n+                     \"int\":$id), [{\n+      assert(versionMajor == 1 && \"This builder is specially for versionMajor==1\");\n+      SmallVector<unsigned> wpt({static_cast<unsigned>(numWarps), 1});\n+      int versionMinor = 0;\n+\n+      assert(id < (1<<numBitsToHoldMmaV1ID) && \"MMAv1 ID exceeds the maximum\");\n+      for (int i = 0; i < numBitsToHoldMmaV1ID; ++i)\n+        versionMinor |= static_cast<bool>((1<<i) & id) * (1<<(4+i));\n+\n+      return $_get(context, versionMajor, versionMinor, wpt);\n+    }]>,\n+\n+    // Specially for MMAV1(Volta)\n     AttrBuilder<(ins \"int\":$versionMajor,\n                      \"ArrayRef<unsigned>\":$warpsPerCTA,\n                      \"ArrayRef<int64_t>\":$shapeA,\n                      \"ArrayRef<int64_t>\":$shapeB,\n                      \"bool\":$isARow,\n-                     \"bool\":$isBRow), [{\n-      assert(versionMajor == 1 && \"Only MMAv1 has multiple versionMinor.\");\n+                     \"bool\":$isBRow,\n+                     \"int\":$id), [{\n+      assert(versionMajor == 1 && \"This builder is specially for versionMajor==1\");\n       bool isAVec4 = !isARow && (shapeA[isARow] <= 16);\n       bool isBVec4 = isBRow && (shapeB[isBRow] <= 16);\n       // 4-bits to encode 4 booleans: [isARow, isBRow, isAVec4, isBVec4]\n       int versionMinor = (isARow * (1<<0)) |\\\n                          (isBRow * (1<<1)) |\\\n                          (isAVec4 * (1<<2)) |\\\n                          (isBVec4 * (1<<3));\n+\n+      assert(id < (1<<numBitsToHoldMmaV1ID) && \"MMAv1 ID exceeds the maximum\");\n+      for (int i = 0; i < numBitsToHoldMmaV1ID; ++i)\n+        versionMinor |= static_cast<bool>((1<<i) & id) * (1<<(4+i));\n+\n       return $_get(context, versionMajor, versionMinor, warpsPerCTA);\n     }]>\n-\n   ];\n \n   let extraClassDeclaration = extraBaseClassDeclaration # [{\n     bool isVolta() const;\n     bool isAmpere() const;\n-    // Get [isARow, isBRow, isAVec4, isBVec4] from versionMinor\n-    std::tuple<bool, bool, bool, bool> decodeVoltaLayoutStates() const;\n+    // Get [isARow, isBRow, isAVec4, isBVec4, id] from versionMinor\n+    std::tuple<bool, bool, bool, bool, int> decodeVoltaLayoutStates() const;\n+    // Number of bits in versionMinor to hold the ID of the MMA encoding instance.\n+    // Here 5 bits can hold 32 IDs in a single module.\n+    static constexpr int numBitsToHoldMmaV1ID{5};\n+\n+    // Here is a temporary flag that indicates whether we need to update the warpsPerCTA for MMAv1, since the current backend cannot support the updated wpt.\n+    // The mmav1's wpt-related logic is separated into multiple files, so a global flag is added here for universal coordination.\n+    // TODO[Superjomn]: Remove this flag once the MMAv1 backend is ready.\n+    static constexpr bool _mmaV1UpdateWpt{false};\n   }];\n \n }"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -13,10 +13,16 @@ std::unique_ptr<Pass> createTritonGPUCanonicalizeLoopsPass();\n \n std::unique_ptr<Pass> createTritonGPUCoalescePass();\n \n+std::unique_ptr<Pass> createTritonGPUReorderInstructionsPass();\n+\n+std::unique_ptr<Pass> createTritonGPUDecomposeConversionsPass();\n+\n std::unique_ptr<Pass> createTritonGPUCombineOpsPass(int computeCapability = 80);\n \n std::unique_ptr<Pass> createTritonGPUVerifier();\n \n+std::unique_ptr<Pass> createTritonGPUUpdateMmaForVoltaPass();\n+\n /// Generate the code for registering passes.\n #define GEN_PASS_REGISTRATION\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\""}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 36, "deletions": 0, "changes": 36, "file_content_changes": "@@ -71,6 +71,30 @@ def TritonGPUCombineOps : Pass<\"tritongpu-combine\", \"mlir::ModuleOp\"> {\n   ];\n }\n \n+def TritonGPUReorderInstructions: Pass<\"tritongpu-reorder-instructions\", \"mlir::ModuleOp\"> {\n+  let summary = \"Reorder instructions\";\n+\n+  let description = \"This pass reorder instructions so as to (1) decrease register pressure (e.g., by moving \"\n+                    \"conversions from shared memory before their first use) and (2) promote LLVM instruction \"\n+                    \"order more friendly to `ptxas`.\";\n+\n+  let constructor = \"mlir::createTritonGPUReorderInstructionsPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::TritonDialect\"];\n+}\n+\n+def TritonGPUDecomposeConversions: Pass<\"tritongpu-decompose-conversions\", \"mlir::ModuleOp\"> {\n+  let summary = \"Decompose convert[distributed -> dotOperand] into convert[distributed -> shared -> dotOperand]\";\n+\n+  let description = \"Decomposing conversions this way makes it possible to use CSE and re-use #shared tensors\";\n+\n+  let constructor = \"mlir::createTritonGPUDecomposeConversionsPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::TritonDialect\"];\n+}\n+\n def TritonGPUCanonicalizeLoops: Pass<\"tritongpu-canonicalize-loops\", \"mlir::ModuleOp\"> {\n   let summary = \"canonicalize scf.ForOp ops\";\n \n@@ -84,4 +108,16 @@ def TritonGPUCanonicalizeLoops: Pass<\"tritongpu-canonicalize-loops\", \"mlir::Modu\n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n }\n \n+def UpdateMmaForVolta : Pass<\"tritongpu-update-mma-for-volta\", \"mlir::ModuleOp\"> {\n+  let summary = \"Update mma encodings for Volta\";\n+\n+  let description = [{\n+    This helps to update the mma encodings for Volta.\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPUUpdateMmaForVoltaPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n+}\n+\n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 0, "deletions": 61, "changes": 61, "file_content_changes": "@@ -26,67 +26,6 @@ bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n          dotOperandLayout.getParent() == mmaLayout;\n }\n \n-void storeDistributedToShared(Value src, Value llSrc,\n-                              ArrayRef<Value> dstStrides,\n-                              ArrayRef<SmallVector<Value>> srcIndices,\n-                              Value dst, Value smemBase, Type elemTy,\n-                              Location loc,\n-                              ConversionPatternRewriter &rewriter) {\n-  auto srcTy = src.getType().cast<RankedTensorType>();\n-  auto srcShape = srcTy.getShape();\n-  assert(srcShape.size() == 2 && \"Unexpected rank of storeDistributedToShared\");\n-  auto dstTy = dst.getType().cast<RankedTensorType>();\n-  auto srcDistributedLayout = srcTy.getEncoding();\n-  if (auto mmaLayout = srcDistributedLayout.dyn_cast<MmaEncodingAttr>()) {\n-    assert((!mmaLayout.isVolta()) &&\n-           \"ConvertLayout MMAv1->Shared is not suppported yet\");\n-  }\n-  auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto inOrd = getOrder(srcDistributedLayout);\n-  auto outOrd = dstSharedLayout.getOrder();\n-  unsigned inVec =\n-      inOrd == outOrd ? getContigPerThread(srcDistributedLayout)[inOrd[0]] : 1;\n-  unsigned outVec = dstSharedLayout.getVec();\n-  unsigned minVec = std::min(outVec, inVec);\n-  unsigned perPhase = dstSharedLayout.getPerPhase();\n-  unsigned maxPhase = dstSharedLayout.getMaxPhase();\n-  unsigned numElems = getElemsPerThread(srcTy);\n-  assert(numElems == srcIndices.size());\n-  auto inVals = getElementsFromStruct(loc, llSrc, rewriter);\n-  auto wordTy = vec_ty(elemTy, minVec);\n-  auto elemPtrTy = ptr_ty(elemTy);\n-  Value outVecVal = i32_val(outVec);\n-  Value minVecVal = i32_val(minVec);\n-  Value word;\n-  for (unsigned i = 0; i < numElems; ++i) {\n-    if (i % minVec == 0)\n-      word = undef(wordTy);\n-    word = insert_element(wordTy, word, inVals[i], i32_val(i % minVec));\n-    if (i % minVec == minVec - 1) {\n-      // step 1: recover the multidim_index from the index of\n-      SmallVector<Value> multiDimIdx = srcIndices[i];\n-      SmallVector<Value> dbgVal = srcIndices[i];\n-\n-      // step 2: do swizzling\n-      Value remained = urem(multiDimIdx[outOrd[0]], outVecVal);\n-      multiDimIdx[outOrd[0]] = udiv(multiDimIdx[outOrd[0]], outVecVal);\n-      Value off_1 = mul(multiDimIdx[outOrd[1]], dstStrides[outOrd[1]]);\n-      Value phaseId = udiv(multiDimIdx[outOrd[1]], i32_val(perPhase));\n-      phaseId = urem(phaseId, i32_val(maxPhase));\n-      Value off_0 = xor_(multiDimIdx[outOrd[0]], phaseId);\n-      off_0 = mul(off_0, outVecVal);\n-      remained = udiv(remained, minVecVal);\n-      off_0 = add(off_0, mul(remained, minVecVal));\n-      Value offset = add(off_1, mul(off_0, dstStrides[outOrd[0]]));\n-\n-      // step 3: store\n-      Value smemAddr = gep(elemPtrTy, smemBase, offset);\n-      smemAddr = bitcast(smemAddr, ptr_ty(wordTy, 3));\n-      store(word, smemAddr);\n-    }\n-  }\n-}\n-\n struct ConvertLayoutOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {\n public:"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.h", "status": "modified", "additions": 0, "deletions": 7, "changes": 7, "file_content_changes": "@@ -11,13 +11,6 @@ using ::mlir::triton::gpu::DotOperandEncodingAttr;\n bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n                         DotOperandEncodingAttr &dotOperandLayout);\n \n-void storeDistributedToShared(Value src, Value llSrc,\n-                              ArrayRef<Value> srcStrides,\n-                              ArrayRef<SmallVector<Value>> srcIndices,\n-                              Value dst, Value smemBase, Type elemPtrTy,\n-                              Location loc,\n-                              ConversionPatternRewriter &rewriter);\n-\n void populateConvertLayoutOpToLLVMPatterns(\n     mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 5, "deletions": 59, "changes": 64, "file_content_changes": "@@ -743,6 +743,9 @@ struct InsertSliceAsyncOpConversion\n     auto sizePerThread = srcBlockedLayout.getSizePerThread();\n     auto threadsPerCTA = getThreadsPerCTA(srcBlockedLayout);\n     auto inOrder = srcBlockedLayout.getOrder();\n+    DenseMap<unsigned, Value> sharedPtrs =\n+        getSwizzledSharedPtrs(loc, inVec, srcTy, resSharedLayout, resElemTy,\n+                              smemObj, rewriter, offsetVals, srcStrides);\n \n     // If perPhase * maxPhase > threadsPerCTA, we will have elements\n     // that share the same tile indices. The index calculation will\n@@ -755,61 +758,8 @@ struct InsertSliceAsyncOpConversion\n     auto numVecCols = std::max<unsigned>(inVec / outVec, 1);\n \n     auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcShape);\n-    //  <<tileVecIdxRow, tileVecIdxCol>, TileOffset>\n-    DenseMap<std::pair<unsigned, unsigned>, Value> tileOffsetMap;\n+\n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n-      // minVec = 2, inVec = 4, outVec = 2\n-      //   baseOffsetCol = 0   baseOffsetCol = 0\n-      //   tileVecIdxCol = 0   tileVecIdxCol = 1\n-      //                -/\\-   -/\\-\n-      //               [|x x| |x x| x x x x x]\n-      //               [|x x| |x x| x x x x x]\n-      // baseOffsetRow [|x x| |x x| x x x x x]\n-      //               [|x x| |x x| x x x x x]\n-      auto vecIdx = elemIdx / minVec;\n-      auto vecIdxCol = vecIdx % (sizePerThread[inOrder[0]] / minVec);\n-      auto vecIdxRow = vecIdx / (sizePerThread[inOrder[0]] / minVec);\n-      auto baseOffsetCol =\n-          vecIdxCol / numVecCols * numVecCols * threadsPerCTA[inOrder[0]];\n-      auto baseOffsetRow = vecIdxRow / numSwizzleRows * numSwizzleRows *\n-                           threadsPerCTA[inOrder[1]];\n-      auto tileVecIdxCol = vecIdxCol % numVecCols;\n-      auto tileVecIdxRow = vecIdxRow % numSwizzleRows;\n-\n-      if (!tileOffsetMap.count({tileVecIdxRow, tileVecIdxCol})) {\n-        // Swizzling\n-        // Since the swizzling index is related to outVec, and we know minVec\n-        // already, inVec doesn't matter\n-        //\n-        // (Numbers represent row indices)\n-        // Example1:\n-        // outVec = 2, inVec = 2, minVec = 2\n-        // outVec = 2, inVec = 4, minVec = 2\n-        //     | [1 2] [3 4] [5 6] ... |\n-        //     | [3 4] [1 2] [7 8] ... |\n-        //     | [5 6] [7 8] [1 2] ... |\n-        // Example2:\n-        // outVec = 4, inVec = 2, minVec = 2\n-        //     | [1 2 3 4] [5 6 7 8] [9 10 11 12] ... |\n-        //     | [5 6 7 8] [1 2 3 4] [13 14 15 16] ... |\n-        //     | [9 10 11 12] [13 14 15 16] [1 2 3 4] ... |\n-        auto srcIdx = srcIndices[tileVecIdxRow * sizePerThread[inOrder[0]]];\n-        Value phase = urem(udiv(srcIdx[inOrder[1]], i32_val(perPhase)),\n-                           i32_val(maxPhase));\n-        // srcShape and smemObj.shape maybe different if smemObj is a\n-        // slice of the original shared memory object.\n-        // So we need to use the original shape to compute the offset\n-        Value rowOffset = mul(srcIdx[inOrder[1]], srcStrides[inOrder[1]]);\n-        Value colOffset =\n-            add(srcIdx[inOrder[0]], i32_val(tileVecIdxCol * minVec));\n-        Value swizzleIdx = udiv(colOffset, i32_val(outVec));\n-        Value swizzleColOffset =\n-            add(mul(xor_(swizzleIdx, phase), i32_val(outVec)),\n-                urem(colOffset, i32_val(outVec)));\n-        Value tileOffset = add(rowOffset, swizzleColOffset);\n-        tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}] =\n-            gep(dstPtrTy, dstPtrBase, tileOffset);\n-      }\n \n       // 16 * 8 = 128bits\n       auto maxBitWidth =\n@@ -826,11 +776,7 @@ struct InsertSliceAsyncOpConversion\n       assert(byteWidth == 16 || byteWidth == 8 || byteWidth == 4);\n       auto resByteWidth = resElemTy.getIntOrFloatBitWidth() / 8;\n \n-      Value tileOffset = tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}];\n-      Value baseOffset =\n-          add(mul(i32_val(baseOffsetRow), srcStrides[inOrder[1]]),\n-              i32_val(baseOffsetCol));\n-      Value basePtr = gep(dstPtrTy, tileOffset, baseOffset);\n+      Value basePtr = sharedPtrs[elemIdx];\n       for (size_t wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n         PTXBuilder ptxBuilder;\n         auto wordElemIdx = wordIdx * numWordElems;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 181, "deletions": 0, "changes": 181, "file_content_changes": "@@ -219,6 +219,187 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return base;\n   }\n \n+  DenseMap<unsigned, Value>\n+  getSwizzledSharedPtrs(Location loc, unsigned inVec, RankedTensorType srcTy,\n+                        triton::gpu::SharedEncodingAttr resSharedLayout,\n+                        Type resElemTy, SharedMemoryObject smemObj,\n+                        ConversionPatternRewriter &rewriter,\n+                        SmallVectorImpl<Value> &offsetVals,\n+                        SmallVectorImpl<Value> &srcStrides) const {\n+    // This utililty computes the pointers for accessing the provided swizzled\n+    // shared memory layout `resSharedLayout`. More specifically, it computes,\n+    // for all indices (row, col) of `srcEncoding` such that idx % inVec = 0,\n+    // the pointer: ptr[(row, col)] = base + (rowOff * strides[ord[1]] + colOff)\n+    // where :\n+    //   compute phase = (row // perPhase) % maxPhase\n+    //   rowOff = row\n+    //   colOff = colOffSwizzled + colOffOrdered\n+    //     colOffSwizzled = ((col // outVec) ^ phase) * outVec\n+    //     colOffOrdered = (col % outVec) // minVec * minVec\n+    //\n+    // Note 1:\n+    // -------\n+    // Because swizzling happens at a granularity of outVec, we need to\n+    // decompose the offset into a swizzled factor and a non-swizzled (ordered)\n+    // factor\n+    //\n+    // Note 2:\n+    // -------\n+    // If we have x, y, z of the form:\n+    // x = 0b00000xxxx\n+    // y = 0byyyyy0000\n+    // z = 0b00000zzzz\n+    // then (x + y) XOR z = 0byyyyxxxx XOR 0b00000zzzz = (x XOR z) + y\n+    // This means that we can use some immediate offsets for shared memory\n+    // operations.\n+    auto dstPtrTy = ptr_ty(resElemTy, 3);\n+    auto dstOffset = dot(rewriter, loc, offsetVals, smemObj.strides);\n+    Value dstPtrBase = gep(dstPtrTy, smemObj.base, dstOffset);\n+\n+    auto srcEncoding = srcTy.getEncoding();\n+    auto srcShape = srcTy.getShape();\n+    unsigned numElems = triton::gpu::getElemsPerThread(srcTy);\n+    // swizzling params as described in TritonGPUAttrDefs.td\n+    unsigned outVec = resSharedLayout.getVec();\n+    unsigned perPhase = resSharedLayout.getPerPhase();\n+    unsigned maxPhase = resSharedLayout.getMaxPhase();\n+    // order\n+    auto inOrder = triton::gpu::getOrder(srcEncoding);\n+    auto outOrder = triton::gpu::getOrder(resSharedLayout);\n+    // tensor indices held by the current thread, as LLVM values\n+    auto srcIndices = emitIndices(loc, rewriter, srcEncoding, srcShape);\n+    // return values\n+    DenseMap<unsigned, Value> ret;\n+    // cache for non-immediate offsets\n+    DenseMap<unsigned, Value> cacheCol, cacheRow;\n+    unsigned minVec = std::min(outVec, inVec);\n+    for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n+      // extract multi dimensional index for current element\n+      auto idx = srcIndices[elemIdx];\n+      Value idxCol = idx[inOrder[0]]; // contiguous dimension\n+      Value idxRow = idx[inOrder[1]]; // discontiguous dimension\n+      Value strideCol = srcStrides[inOrder[0]];\n+      Value strideRow = srcStrides[inOrder[1]];\n+      // extract dynamic/static offset for immediate offseting\n+      unsigned immedateOffCol = 0;\n+      if (auto add = dyn_cast_or_null<LLVM::AddOp>(idxCol.getDefiningOp()))\n+        if (auto _cst = dyn_cast_or_null<LLVM::ConstantOp>(\n+                add.getRhs().getDefiningOp())) {\n+          unsigned cst =\n+              _cst.getValue().cast<IntegerAttr>().getValue().getSExtValue();\n+          unsigned key = cst % (outVec * maxPhase);\n+          cacheCol.insert({key, idxCol});\n+          idxCol = cacheCol[key];\n+          immedateOffCol = cst / (outVec * maxPhase) * (outVec * maxPhase);\n+        }\n+      // extract dynamic/static offset for immediate offseting\n+      unsigned immedateOffRow = 0;\n+      if (auto add = dyn_cast_or_null<LLVM::AddOp>(idxRow.getDefiningOp()))\n+        if (auto _cst = dyn_cast_or_null<LLVM::ConstantOp>(\n+                add.getRhs().getDefiningOp())) {\n+          unsigned cst =\n+              _cst.getValue().cast<IntegerAttr>().getValue().getSExtValue();\n+          unsigned key = cst % (perPhase * maxPhase);\n+          cacheRow.insert({key, idxRow});\n+          idxRow = cacheRow[key];\n+          immedateOffRow = cst / (perPhase * maxPhase) * (perPhase * maxPhase);\n+        }\n+      // compute phase = (row // perPhase) % maxPhase\n+      Value phase = urem(udiv(idxRow, i32_val(perPhase)), i32_val(maxPhase));\n+      // row offset is simply row index\n+      Value rowOff = mul(idxRow, strideRow);\n+      // because swizzling happens at a granularity of outVec, we need to\n+      // decompose the offset into a swizzled factor and a non-swizzled\n+      // (ordered) factor: colOffSwizzled = ((col // outVec) ^ phase) * outVec\n+      // colOffOrdered = (col % outVec) // minVec * minVec\n+      Value colOffSwizzled = xor_(udiv(idxCol, i32_val(outVec)), phase);\n+      colOffSwizzled = mul(colOffSwizzled, i32_val(outVec));\n+      Value colOffOrdered = urem(idxCol, i32_val(outVec));\n+      colOffOrdered = udiv(colOffOrdered, i32_val(minVec));\n+      colOffOrdered = mul(colOffOrdered, i32_val(minVec));\n+      Value colOff = add(colOffSwizzled, colOffOrdered);\n+      // compute non-immediate offset\n+      Value offset = add(rowOff, mul(colOff, strideCol));\n+      Value currPtr = gep(dstPtrTy, dstPtrBase, offset);\n+      // compute immediate offset\n+      Value immedateOff =\n+          add(mul(i32_val(immedateOffRow), srcStrides[inOrder[1]]),\n+              i32_val(immedateOffCol));\n+      ret[elemIdx] = gep(dstPtrTy, currPtr, immedateOff);\n+    }\n+    return ret;\n+  }\n+\n+  bool isMmaToDotShortcut(\n+      MmaEncodingAttr &mmaLayout,\n+      triton::gpu::DotOperandEncodingAttr &dotOperandLayout) const {\n+    // dot_op<opIdx=0, parent=#mma> = #mma\n+    // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n+    return mmaLayout.getWarpsPerCTA()[1] == 1 &&\n+           dotOperandLayout.getOpIdx() == 0 &&\n+           dotOperandLayout.getParent() == mmaLayout;\n+  }\n+\n+  void storeDistributedToShared(Value src, Value llSrc,\n+                                ArrayRef<Value> dstStrides,\n+                                ArrayRef<SmallVector<Value>> srcIndices,\n+                                Value dst, Value smemBase, Type elemTy,\n+                                Location loc,\n+                                ConversionPatternRewriter &rewriter) const {\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto srcShape = srcTy.getShape();\n+    assert(srcShape.size() == 2 &&\n+           \"Unexpected rank of storeDistributedToShared\");\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto srcDistributedLayout = srcTy.getEncoding();\n+    if (auto mmaLayout = srcDistributedLayout.dyn_cast<MmaEncodingAttr>()) {\n+      assert((!mmaLayout.isVolta()) &&\n+             \"ConvertLayout MMAv1->Shared is not suppported yet\");\n+    }\n+    auto dstSharedLayout =\n+        dstTy.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n+    auto dstElemTy = dstTy.getElementType();\n+    auto inOrd = triton::gpu::getOrder(srcDistributedLayout);\n+    auto outOrd = dstSharedLayout.getOrder();\n+    unsigned inVec =\n+        inOrd == outOrd\n+            ? triton::gpu::getContigPerThread(srcDistributedLayout)[inOrd[0]]\n+            : 1;\n+    unsigned outVec = dstSharedLayout.getVec();\n+    unsigned minVec = std::min(outVec, inVec);\n+    unsigned perPhase = dstSharedLayout.getPerPhase();\n+    unsigned maxPhase = dstSharedLayout.getMaxPhase();\n+    unsigned numElems = triton::gpu::getElemsPerThread(srcTy);\n+    assert(numElems == srcIndices.size());\n+    auto inVals = LLVM::getElementsFromStruct(loc, llSrc, rewriter);\n+    auto wordTy = vec_ty(elemTy, minVec);\n+    auto elemPtrTy = ptr_ty(elemTy);\n+    Value outVecVal = i32_val(outVec);\n+    Value minVecVal = i32_val(minVec);\n+    Value word;\n+\n+    SmallVector<Value> srcStrides = {dstStrides[0], dstStrides[1]};\n+    SmallVector<Value> offsetVals = {i32_val(0), i32_val(0)};\n+    SharedMemoryObject smemObj(smemBase, srcStrides, offsetVals);\n+\n+    DenseMap<unsigned, Value> sharedPtrs =\n+        getSwizzledSharedPtrs(loc, inVec, srcTy, dstSharedLayout, dstElemTy,\n+                              smemObj, rewriter, offsetVals, srcStrides);\n+\n+    std::map<unsigned, Value> cache0;\n+    std::map<unsigned, Value> cache1;\n+    for (unsigned i = 0; i < numElems; ++i) {\n+      if (i % minVec == 0)\n+        word = undef(wordTy);\n+      word = insert_element(wordTy, word, inVals[i], i32_val(i % minVec));\n+      if (i % minVec == minVec - 1) {\n+        Value smemAddr = sharedPtrs[i / minVec * minVec];\n+        smemAddr = bitcast(smemAddr, ptr_ty(wordTy, 3));\n+        store(word, smemAddr);\n+      }\n+    }\n+  }\n+\n   // -----------------------------------------------------------------------\n   // Utilities\n   // -----------------------------------------------------------------------"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -319,20 +319,7 @@ struct TritonTransPattern : public OpConversionPattern<triton::TransOp> {\n       src = rewriter.create<triton::gpu::ConvertLayoutOp>(src.getLoc(), srcType,\n                                                           src);\n     }\n-    auto srcSharedEncoding =\n-        srcEncoding.cast<triton::gpu::SharedEncodingAttr>();\n-    SmallVector<unsigned> retOrder(srcSharedEncoding.getOrder().begin(),\n-                                   srcSharedEncoding.getOrder().end());\n-    SmallVector<int64_t> retShapes(srcType.getShape().begin(),\n-                                   srcType.getShape().end());\n-    std::reverse(retOrder.begin(), retOrder.end());\n-    std::reverse(retShapes.begin(), retShapes.end());\n-    auto retEncoding =\n-        triton::gpu::SharedEncodingAttr::get(getContext(), 1, 1, 1, retOrder);\n-    auto retType =\n-        RankedTensorType::get(retShapes, srcType.getElementType(), retEncoding);\n-\n-    rewriter.replaceOpWithNewOp<triton::TransOp>(op, retType, src);\n+    rewriter.replaceOpWithNewOp<triton::TransOp>(op, src);\n     return success();\n   }\n };"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -206,6 +206,33 @@ void LoadOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n   state.addTypes({resultType});\n }\n \n+//-- TransOp --\n+mlir::LogicalResult mlir::triton::TransOp::inferReturnTypes(\n+    MLIRContext *context, Optional<Location> location, ValueRange operands,\n+    DictionaryAttr attributes, RegionRange regions,\n+    SmallVectorImpl<Type> &inferredReturnTypes) {\n+  // type is the same as the input\n+  auto argTy = operands[0].getType().cast<RankedTensorType>();\n+  SmallVector<int64_t> retShape(argTy.getShape().begin(),\n+                                argTy.getShape().end());\n+  std::reverse(retShape.begin(), retShape.end());\n+  auto retEltTy = argTy.getElementType();\n+  Attribute argEncoding = argTy.getEncoding();\n+  Attribute retEncoding;\n+  if (argEncoding) {\n+    Dialect &dialect = argEncoding.getDialect();\n+    auto inferLayoutInterface = dyn_cast<DialectInferLayoutInterface>(&dialect);\n+    if (inferLayoutInterface->inferTransOpEncoding(argEncoding, retEncoding)\n+            .failed()) {\n+      llvm::report_fatal_error(\"failed to infer layout for ReduceOp\");\n+      return mlir::failure();\n+    }\n+  }\n+  inferredReturnTypes.push_back(\n+      RankedTensorType::get(retShape, retEltTy, retEncoding));\n+  return mlir::success();\n+}\n+\n //-- DotOp --\n mlir::LogicalResult mlir::triton::DotOp::inferReturnTypes(\n     MLIRContext *context, Optional<Location> location, ValueRange operands,"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 32, "deletions": 5, "changes": 37, "file_content_changes": "@@ -104,7 +104,10 @@ SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n     return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n                                  blockedLayout.getSizePerThread().end());\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-    return getSizePerThread(sliceLayout.getParent());\n+    auto ret = getSizePerThread(sliceLayout.getParent());\n+    return ret;\n+    // ret.erase(ret.begin() + sliceLayout.getDim());\n+    return ret;\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     if (mmaLayout.isAmpere()) {\n       return {2, 2};\n@@ -158,7 +161,11 @@ SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout) {\n       threads.push_back(blockedLayout.getThreadsPerWarp()[d] *\n                         blockedLayout.getWarpsPerCTA()[d]);\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    assert(0 && \"Unimplemented usage of MmaEncodingAttr\");\n+    if (mmaLayout.getVersionMajor() == 2) {\n+      threads = {8 * mmaLayout.getWarpsPerCTA()[0],\n+                 4 * mmaLayout.getWarpsPerCTA()[1]};\n+    } else\n+      assert(0 && \"Unimplemented usage of MmaEncodingAttr\");\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n   }\n@@ -593,15 +600,20 @@ bool MmaEncodingAttr::isVolta() const { return getVersionMajor() == 1; }\n \n bool MmaEncodingAttr::isAmpere() const { return getVersionMajor() == 2; }\n \n-// Get [isARow, isBRow, isAVec4, isBVec4] from versionMinor\n-std::tuple<bool, bool, bool, bool>\n+// Get [isARow, isBRow, isAVec4, isBVec4, id] from versionMinor\n+std::tuple<bool, bool, bool, bool, int>\n MmaEncodingAttr::decodeVoltaLayoutStates() const {\n   unsigned versionMinor = getVersionMinor();\n   bool isARow = versionMinor & (1 << 0);\n   bool isBRow = versionMinor & (1 << 1);\n   bool isAVec4 = versionMinor & (1 << 2);\n   bool isBVec4 = versionMinor & (1 << 3);\n-  return std::make_tuple(isARow, isBRow, isAVec4, isBVec4);\n+\n+  int id = 0;\n+  for (int i = numBitsToHoldMmaV1ID - 1; i >= 0; --i)\n+    id = (id << 1) + static_cast<bool>(versionMinor & (1 << (4 + i)));\n+\n+  return std::make_tuple(isARow, isBRow, isAVec4, isBVec4, id);\n }\n \n //===----------------------------------------------------------------------===//\n@@ -734,6 +746,21 @@ struct TritonGPUInferLayoutInterface\n     return success();\n   }\n \n+  LogicalResult inferTransOpEncoding(Attribute operandEncoding,\n+                                     Attribute &resultEncoding) const override {\n+    SharedEncodingAttr sharedEncoding =\n+        operandEncoding.dyn_cast<SharedEncodingAttr>();\n+    if (!sharedEncoding)\n+      return failure();\n+    SmallVector<unsigned> retOrder(sharedEncoding.getOrder().begin(),\n+                                   sharedEncoding.getOrder().end());\n+    std::reverse(retOrder.begin(), retOrder.end());\n+    resultEncoding = SharedEncodingAttr::get(\n+        getDialect()->getContext(), sharedEncoding.getVec(),\n+        sharedEncoding.getPerPhase(), sharedEncoding.getMaxPhase(), retOrder);\n+    return mlir::success();\n+  }\n+\n   LogicalResult\n   inferExpandDimsOpEncoding(Attribute operandEncoding, unsigned axis,\n                             Attribute &resultEncoding,"}, {"filename": "lib/Dialect/TritonGPU/Transforms/CMakeLists.txt", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -8,7 +8,11 @@ add_mlir_dialect_library(TritonGPUTransforms\n   Combine.cpp\n   Pipeline.cpp\n   Prefetch.cpp\n+  ReorderInstructions.cpp\n+  DecomposeConversions.cpp\n   TritonGPUConversion.cpp\n+  UpdateMmaForVolta.cpp\n+  Utility.cpp\n \n   DEPENDS\n   TritonGPUTransformsIncGen"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 126, "deletions": 269, "changes": 395, "file_content_changes": "@@ -1,3 +1,4 @@\n+#include \"Utility.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/SCF/SCF.h\"\n #include \"mlir/IR/BlockAndValueMapping.h\"\n@@ -26,6 +27,7 @@ using triton::DotOp;\n using triton::gpu::ConvertLayoutOp;\n using triton::gpu::DotOperandEncodingAttr;\n using triton::gpu::MmaEncodingAttr;\n+using triton::gpu::SliceEncodingAttr;\n \n // -----------------------------------------------------------------------------\n //\n@@ -482,7 +484,7 @@ class FoldConvertAndReduce : public mlir::RewritePattern {\n       return op->getBlock() == cvt->getBlock() &&\n              !(isa<triton::ReduceOp>(op) &&\n                !op->getResult(0).getType().isa<RankedTensorType>()) &&\n-             !isa<scf::YieldOp>(op);\n+             !isa<triton::gpu::ConvertLayoutOp>(op) && !isa<scf::YieldOp>(op);\n     };\n     mlir::getForwardSlice(cvt.getResult(), &cvtSlices, filter);\n     if (cvtSlices.empty())\n@@ -885,24 +887,31 @@ SmallVector<int64_t, 2> mmaVersionToShapePerWarp(int version) {\n \n SmallVector<unsigned, 2> warpsPerTileV1(const ArrayRef<int64_t> shape,\n                                         int numWarps) {\n-  SmallVector<unsigned, 2> ret = {1, 1};\n-  SmallVector<int64_t, 2> shapePerWarp =\n-      mmaVersionToShapePerWarp(1 /*version*/);\n-  bool changed = false;\n-  do {\n-    changed = false;\n-    int pre = ret[0];\n-    if (ret[0] * ret[1] < numWarps) {\n-      ret[0] = std::clamp<unsigned>(ret[0] * 2, 1, shape[0] / shapePerWarp[0]);\n-      changed = pre != ret[0];\n-    }\n-    if (ret[0] * ret[1] < numWarps) {\n-      pre = ret[1];\n-      ret[1] = std::clamp<unsigned>(ret[1] * 2, 1, shape[1] / shapePerWarp[1]);\n-      changed = pre != ret[1];\n-    }\n-  } while (changed);\n-  return ret;\n+  if (!MmaEncodingAttr::_mmaV1UpdateWpt) {\n+    SmallVector<unsigned, 2> ret = {1, 1};\n+    SmallVector<int64_t, 2> shapePerWarp =\n+        mmaVersionToShapePerWarp(1 /*version*/);\n+    bool changed = false;\n+    do {\n+      changed = false;\n+      int pre = ret[0];\n+      if (ret[0] * ret[1] < numWarps) {\n+        ret[0] =\n+            std::clamp<unsigned>(ret[0] * 2, 1, shape[0] / shapePerWarp[0]);\n+        changed = pre != ret[0];\n+      }\n+      if (ret[0] * ret[1] < numWarps) {\n+        pre = ret[1];\n+        ret[1] =\n+            std::clamp<unsigned>(ret[1] * 2, 1, shape[1] / shapePerWarp[1]);\n+        changed = pre != ret[1];\n+      }\n+    } while (changed);\n+    return ret;\n+  } else {\n+    // Set a default value and ensure product of wpt equals numWarps\n+    return {static_cast<unsigned>(numWarps), 1};\n+  }\n }\n \n SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n@@ -1039,6 +1048,7 @@ class OptimizeConvertToDotOperand : public mlir::RewritePattern {\n \n class BlockedToMMA : public mlir::RewritePattern {\n   int computeCapability;\n+  mutable int mmaV1Counter{}; // used to generate ID for MMAv1 encoding\n \n public:\n   BlockedToMMA(mlir::MLIRContext *context, int computeCapability)\n@@ -1097,13 +1107,13 @@ class BlockedToMMA : public mlir::RewritePattern {\n         getWarpsPerTile(dotOp, retShape, versionMajor, numWarps);\n     triton::gpu::MmaEncodingAttr mmaEnc;\n     if (versionMajor == 1) {\n-      auto shapeA = AType.getShape();\n-      auto shapeB = BType.getShape();\n-      bool isARow = AOrder[0] != 0;\n-      bool isBRow = BOrder[0] != 0;\n-      mmaEnc = triton::gpu::MmaEncodingAttr::get(\n-          oldRetType.getContext(), versionMajor, warpsPerTile, shapeA, shapeB,\n-          isARow, isBRow);\n+      if (MmaEncodingAttr::_mmaV1UpdateWpt)\n+        mmaEnc = triton::gpu::MmaEncodingAttr::get(\n+            oldRetType.getContext(), versionMajor, numWarps, mmaV1Counter++);\n+      else\n+        mmaEnc = triton::gpu::MmaEncodingAttr::get(\n+            dotOp->getContext(), versionMajor, 0 /*versionMinor*/,\n+            warpsPerTileV1(retShape, numWarps));\n     } else if (versionMajor == 2) {\n       mmaEnc = triton::gpu::MmaEncodingAttr::get(\n           oldRetType.getContext(), versionMajor, 0 /*versionMinor*/,\n@@ -1159,238 +1169,107 @@ class BlockedToMMA : public mlir::RewritePattern {\n   }\n };\n \n-class FixupLoop : public mlir::RewritePattern {\n+// Convert + trans + convert\n+// x = convert_layout distributed -> #shared_x\n+// y = trans x -> #shared_y\n+// z = convert_layout y -> #dot_operand\n+class ConvertTransConvert : public mlir::RewritePattern {\n \n public:\n-  explicit FixupLoop(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(scf::ForOp::getOperationName(), 2, context) {}\n+  ConvertTransConvert(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n+                             1, context) {}\n \n-  mlir::LogicalResult\n+  LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n-    auto forOp = cast<scf::ForOp>(op);\n-\n-    // Rewrite init argument\n-    SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n-    bool shouldRematerialize = false;\n-    for (size_t i = 0; i < newInitArgs.size(); i++) {\n-      auto initArg = newInitArgs[i];\n-      auto regionArg = forOp.getRegionIterArgs()[i];\n-      if (newInitArgs[i].getType() != forOp.getRegionIterArgs()[i].getType() ||\n-          newInitArgs[i].getType() != forOp.getResultTypes()[i]) {\n-        shouldRematerialize = true;\n-        break;\n-      }\n-    }\n-    if (!shouldRematerialize)\n-      return failure();\n-\n-    scf::ForOp newForOp = rewriter.create<scf::ForOp>(\n-        forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n-        forOp.getStep(), newInitArgs);\n-    newForOp->moveBefore(forOp);\n-    rewriter.setInsertionPointToStart(newForOp.getBody());\n-    BlockAndValueMapping mapping;\n-    for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n-      mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n-    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n+    auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto tmpOp = dyn_cast_or_null<triton::TransOp>(dstOp.src().getDefiningOp());\n+    if (!tmpOp)\n+      return mlir::failure();\n+    auto srcOp = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n+        tmpOp.src().getDefiningOp());\n+    if (!srcOp)\n+      return mlir::failure();\n+    auto arg = srcOp.src();\n+    auto X = tmpOp.src();\n+    auto Y = dstOp.src();\n+    // types\n+    auto argType = arg.getType().cast<RankedTensorType>();\n+    auto XType = X.getType().cast<RankedTensorType>();\n+    auto YType = Y.getType().cast<RankedTensorType>();\n+    auto ZType = dstOp.getResult().getType().cast<RankedTensorType>();\n+    // encodings\n+    auto argEncoding = argType.getEncoding();\n+    auto XEncoding =\n+        XType.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n+    auto YEncoding =\n+        YType.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n+    auto ZEncoding =\n+        ZType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+    if (!ZEncoding)\n+      return mlir::failure();\n+    // new X encoding\n+    auto newXOrder = triton::gpu::getOrder(argEncoding);\n+    auto newXEncoding = triton::gpu::SharedEncodingAttr::get(\n+        getContext(), ZEncoding, XType.getShape(), newXOrder,\n+        XType.getElementType());\n+    auto newXType = RankedTensorType::get(XType.getShape(),\n+                                          XType.getElementType(), newXEncoding);\n+    if (XEncoding == newXEncoding)\n+      return mlir::failure();\n \n-    for (Operation &op : forOp.getBody()->getOperations()) {\n-      rewriter.clone(op, mapping);\n-    }\n-    rewriter.replaceOp(forOp, newForOp.getResults());\n-    return success();\n+    auto newX = rewriter.create<triton::gpu::ConvertLayoutOp>(srcOp.getLoc(),\n+                                                              newXType, arg);\n+    auto newY = rewriter.create<triton::TransOp>(tmpOp.getLoc(), newX);\n+    rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(dstOp, ZType,\n+                                                              newY);\n+    return mlir::success();\n   }\n };\n \n-// This pattern collects the wrong Mma those need to update and create the right\n-// ones for each.\n-class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n-  DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n-\n+//\n+class ConvertDotConvert : public mlir::RewritePattern {\n public:\n-  CollectMmaToUpdateForVolta(\n-      mlir::MLIRContext *ctx,\n-      DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n-      : mlir::RewritePattern(triton::DotOp::getOperationName(), 1, ctx),\n-        mmaToUpdate(mmaToUpdate) {}\n+  ConvertDotConvert(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n+                             1, context) {}\n \n-  mlir::LogicalResult\n+  LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n+    auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto dotOp = dyn_cast_or_null<triton::DotOp>(dstOp.src().getDefiningOp());\n+    if (!dotOp)\n+      return mlir::failure();\n+    if (std::distance(dstOp->user_begin(), dstOp->user_end()) != 1 ||\n+        std::distance(dotOp->user_begin(), dotOp->user_end()) != 1)\n+      return mlir::failure();\n+    auto cvtOp = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n+        dotOp.getOperand(2).getDefiningOp());\n+    if (!cvtOp)\n+      return mlir::failure();\n+    auto loadOp = dyn_cast_or_null<triton::LoadOp>(cvtOp.src().getDefiningOp());\n+    if (!loadOp)\n+      return mlir::failure();\n+    auto dstTy = dstOp.getResult().getType().cast<RankedTensorType>();\n+    auto srcTy = cvtOp.getOperand().getType().cast<RankedTensorType>();\n+    if (dstTy != srcTy)\n+      return mlir::failure();\n \n-    auto dotOp = cast<triton::DotOp>(op);\n-    auto *ctx = dotOp->getContext();\n-    auto AT = dotOp.a().getType().cast<RankedTensorType>();\n-    auto BT = dotOp.b().getType().cast<RankedTensorType>();\n-    auto DT = dotOp.d().getType().cast<RankedTensorType>();\n-    if (!DT.getEncoding())\n-      return failure();\n-    auto mmaLayout = DT.getEncoding().dyn_cast<MmaEncodingAttr>();\n-    if (!(mmaLayout && mmaLayout.isVolta()))\n-      return failure();\n-\n-    // Has processed.\n-    if (mmaToUpdate.count(mmaLayout))\n-      return failure();\n-\n-    auto dotOperandA = AT.getEncoding().cast<DotOperandEncodingAttr>();\n-    auto dotOperandB = BT.getEncoding().cast<DotOperandEncodingAttr>();\n-    bool isARow = dotOperandA.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    bool isBRow = dotOperandB.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    auto [isARow_, isBRow_, isAVec4, isBVec4] =\n-        mmaLayout.decodeVoltaLayoutStates();\n-    if (isARow_ == isARow && isBRow_ == isBRow) {\n-      return failure(); // No need to update\n-    }\n-\n-    auto newMmaLayout = MmaEncodingAttr::get(\n-        ctx, mmaLayout.getVersionMajor(), mmaLayout.getWarpsPerCTA(),\n-        AT.getShape(), BT.getShape(), isARow, isBRow);\n-\n-    // Collect the wrong MMA Layouts, and mark need to update.\n-    mmaToUpdate.try_emplace(mmaLayout, newMmaLayout);\n-\n-    return failure();\n-  }\n-};\n-\n-// Correct the versionMinor field in MmaEncodingAttr for Volta.\n-class UpdateMMAVersionMinorForVolta : public mlir::RewritePattern {\n-  const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n-  enum class Kind {\n-    kUnk,\n-    kCvtToMma,\n-    kCvtToDotOp,\n-    kDot,\n-    kConstant,\n-  };\n-  mutable Kind rewriteKind{Kind::kUnk};\n-\n-public:\n-  UpdateMMAVersionMinorForVolta(\n-      mlir::MLIRContext *ctx, llvm::StringRef opName,\n-      const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n-      : RewritePattern(opName, 1 /*benefit*/, ctx), mmaToUpdate(mmaToUpdate) {}\n-\n-  LogicalResult match(Operation *op) const override {\n-    MmaEncodingAttr mma;\n-    if (mmaToUpdate.empty())\n-      return failure();\n-    if (op->getNumResults() != 1)\n-      return failure();\n-    auto tensorTy = op->getResult(0).getType().dyn_cast<RankedTensorType>();\n-    if (!tensorTy)\n-      return failure();\n-\n-    // ConvertLayoutOp\n-    if (auto cvt = llvm::dyn_cast<ConvertLayoutOp>(op)) {\n-      // cvt X -> dot_operand\n-      if (auto dotOperand =\n-              tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>()) {\n-        mma = dotOperand.getParent().dyn_cast<MmaEncodingAttr>();\n-        rewriteKind = Kind::kCvtToDotOp;\n-        if (mma && mmaToUpdate.count(mma))\n-          return success();\n-      }\n-      if ((mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>())) {\n-        // cvt X -> mma\n-        rewriteKind = Kind::kCvtToMma;\n-        if (mma && mmaToUpdate.count(mma))\n-          return success();\n-      }\n-    } else if (auto dot = llvm::dyn_cast<DotOp>(op)) {\n-      // DotOp\n-      mma = dot.d()\n-                .getType()\n-                .cast<RankedTensorType>()\n-                .getEncoding()\n-                .dyn_cast<MmaEncodingAttr>();\n-      rewriteKind = Kind::kDot;\n-    } else if (auto constant = llvm::dyn_cast<arith::ConstantOp>(op)) {\n-      // ConstantOp\n-      mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n-      rewriteKind = Kind::kConstant;\n-    }\n-\n-    return success(mma && mmaToUpdate.count(mma));\n-  }\n-\n-  void rewrite(Operation *op, PatternRewriter &rewriter) const override {\n-    switch (rewriteKind) {\n-    case Kind::kDot:\n-      rewriteDot(op, rewriter);\n-      break;\n-    case Kind::kConstant:\n-      rewriteConstant(op, rewriter);\n-      break;\n-    case Kind::kCvtToDotOp:\n-      rewriteCvtDotOp(op, rewriter);\n-      break;\n-    case Kind::kCvtToMma:\n-      rewriteCvtToMma(op, rewriter);\n-      break;\n-    default:\n-      llvm::report_fatal_error(\"Not supported rewrite kind\");\n-    }\n-  }\n-\n-private:\n-  void rewriteCvtDotOp(Operation *op, PatternRewriter &rewriter) const {\n-    auto *ctx = op->getContext();\n-    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n-    auto tensorTy = cvt.result().getType().cast<RankedTensorType>();\n-    auto dotOperand = tensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n-    MmaEncodingAttr newMma =\n-        mmaToUpdate.lookup(dotOperand.getParent().cast<MmaEncodingAttr>());\n-    auto newDotOperand = DotOperandEncodingAttr::get(\n-        ctx, dotOperand.getOpIdx(), newMma, dotOperand.getIsMMAv1Row());\n-    auto newTensorTy = RankedTensorType::get(\n-        tensorTy.getShape(), tensorTy.getElementType(), newDotOperand);\n-    rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n-                                                 cvt.getOperand());\n-  }\n-\n-  void rewriteDot(Operation *op, PatternRewriter &rewriter) const {\n-    auto *ctx = op->getContext();\n-    auto dot = llvm::cast<DotOp>(op);\n-    auto tensorTy = dot.d().getType().cast<RankedTensorType>();\n-    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n-    auto newMma = mmaToUpdate.lookup(mma);\n-    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n-                                             tensorTy.getElementType(), newMma);\n-    rewriter.replaceOpWithNewOp<DotOp>(op, newTensorTy, dot.a(), dot.b(),\n-                                       dot.c(), dot.allowTF32());\n-  }\n-\n-  void rewriteCvtToMma(Operation *op, PatternRewriter &rewriter) const {\n-    auto *ctx = op->getContext();\n-    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n-    auto tensorTy = cvt.result().getType().cast<RankedTensorType>();\n-    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n-    auto newMma = mmaToUpdate.lookup(mma);\n-    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n-                                             tensorTy.getElementType(), newMma);\n-    rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n-                                                 cvt.getOperand());\n-  }\n-\n-  void rewriteConstant(Operation *op, PatternRewriter &rewriter) const {\n-    auto *ctx = op->getContext();\n-    auto constant = llvm::cast<arith::ConstantOp>(op);\n-    auto tensorTy = constant.getResult().getType().dyn_cast<RankedTensorType>();\n-    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n-    auto newMma = mmaToUpdate.lookup(mma);\n-    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n-                                             tensorTy.getElementType(), newMma);\n-    if (auto attr = constant.getValue().dyn_cast<SplatElementsAttr>()) {\n-      auto newRet =\n-          SplatElementsAttr::get(newTensorTy, attr.getSplatValue<Attribute>());\n-      rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newTensorTy, newRet);\n-      return;\n-    }\n-\n-    assert(false && \"Not supported ConstantOp value type\");\n+    // TODO: int tensor cores\n+    auto _0f = rewriter.create<arith::ConstantFloatOp>(\n+        op->getLoc(), APFloat(0.0f), dstTy.getElementType().cast<FloatType>());\n+    auto _0 = rewriter.create<triton::SplatOp>(\n+        op->getLoc(), dotOp.getResult().getType(), _0f);\n+    auto newDot = rewriter.create<triton::DotOp>(\n+        op->getLoc(), dotOp.getResult().getType(), dotOp.getOperand(0),\n+        dotOp.getOperand(1), _0, dotOp.allowTF32());\n+    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        op->getLoc(), dstTy, newDot.getResult());\n+    auto newAdd = rewriter.replaceOpWithNewOp<arith::AddFOp>(\n+        op, newCvt, cvtOp.getOperand());\n+    return mlir::success();\n   }\n };\n \n@@ -1423,36 +1302,14 @@ class TritonGPUCombineOpsPass\n     patterns.add<MoveConvertOutOfLoop>(context);\n     patterns.add<MoveConvertOutOfIf>(context);\n     patterns.add<BlockedToMMA>(context, computeCapability);\n+    patterns.add<ConvertTransConvert>(context);\n+    patterns.add<ConvertDotConvert>(context);\n \n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n       signalPassFailure();\n     }\n \n-    llvm::DenseMap<MmaEncodingAttr, MmaEncodingAttr> mmaToUpdate;\n-    {\n-      mlir::RewritePatternSet patterns(context);\n-      patterns.add<CollectMmaToUpdateForVolta>(context, mmaToUpdate);\n-      if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n-        signalPassFailure();\n-    }\n-    {\n-      mlir::RewritePatternSet patterns(context);\n-      patterns.add<UpdateMMAVersionMinorForVolta>(\n-          context, DotOp::getOperationName(), mmaToUpdate);\n-      patterns.add<UpdateMMAVersionMinorForVolta>(\n-          context, ConvertLayoutOp::getOperationName(), mmaToUpdate);\n-      patterns.add<UpdateMMAVersionMinorForVolta>(\n-          context, arith::ConstantOp::getOperationName(), mmaToUpdate);\n-      mlir::GreedyRewriteConfig config;\n-      config.useTopDownTraversal = true;\n-\n-      if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n-        signalPassFailure();\n-    }\n-\n-    mlir::RewritePatternSet loopFixup(context);\n-    loopFixup.add<FixupLoop>(context);\n-    if (applyPatternsAndFoldGreedily(m, std::move(loopFixup)).failed()) {\n+    if (fixupLoops(m).failed()) {\n       signalPassFailure();\n     }\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/DecomposeConversions.cpp", "status": "added", "additions": 69, "deletions": 0, "changes": 69, "file_content_changes": "@@ -0,0 +1,69 @@\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n+#include \"mlir/Dialect/SCF/SCF.h\"\n+#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Verifier.h\"\n+#include \"mlir/Interfaces/InferTypeOpInterface.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Pass/PassManager.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"mlir/Transforms/Passes.h\"\n+#include \"mlir/Transforms/RegionUtils.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n+\n+using namespace mlir;\n+\n+class TritonGPUDecomposeConversionsPass\n+    : public TritonGPUDecomposeConversionsBase<\n+          TritonGPUDecomposeConversionsPass> {\n+public:\n+  TritonGPUDecomposeConversionsPass() = default;\n+\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp mod = getOperation();\n+    mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n+      OpBuilder builder(cvtOp);\n+      auto srcType = cvtOp.getOperand().getType().cast<RankedTensorType>();\n+      auto dstType = cvtOp.getType().cast<RankedTensorType>();\n+      auto srcEncoding = srcType.getEncoding();\n+      if (srcEncoding.isa<triton::gpu::SharedEncodingAttr>())\n+        return;\n+      auto dstDotOp =\n+          dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+      if (!dstDotOp)\n+        return;\n+      if (auto srcMmaEncoding =\n+              srcEncoding.dyn_cast<triton::gpu::MmaEncodingAttr>()) {\n+\n+        if (srcMmaEncoding.getVersionMajor() == 1 ||\n+            (srcMmaEncoding.getWarpsPerCTA()[1] == 1 &&\n+             dstDotOp.getParent() == srcMmaEncoding))\n+          return;\n+      }\n+      auto tmpType = RankedTensorType::get(\n+          dstType.getShape(), dstType.getElementType(),\n+          triton::gpu::SharedEncodingAttr::get(\n+              mod.getContext(), dstDotOp, srcType.getShape(),\n+              triton::gpu::getOrder(srcEncoding), srcType.getElementType()));\n+      auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n+          cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n+      auto newConvert = builder.create<triton::gpu::ConvertLayoutOp>(\n+          cvtOp.getLoc(), dstType, tmp);\n+      cvtOp.replaceAllUsesWith(newConvert.getResult());\n+      cvtOp.erase();\n+    });\n+  }\n+};\n+\n+std::unique_ptr<Pass> mlir::createTritonGPUDecomposeConversionsPass() {\n+  return std::make_unique<TritonGPUDecomposeConversionsPass>();\n+}"}, {"filename": "lib/Dialect/TritonGPU/Transforms/ReorderInstructions.cpp", "status": "added", "additions": 101, "deletions": 0, "changes": 101, "file_content_changes": "@@ -0,0 +1,101 @@\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n+#include \"mlir/Dialect/SCF/SCF.h\"\n+#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Verifier.h\"\n+#include \"mlir/Interfaces/InferTypeOpInterface.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Pass/PassManager.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"mlir/Transforms/Passes.h\"\n+#include \"mlir/Transforms/RegionUtils.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n+\n+using namespace mlir;\n+\n+static inline bool\n+willIncreaseRegisterPressure(triton::gpu::ConvertLayoutOp op) {\n+  auto srcType = op.getOperand().getType().cast<RankedTensorType>();\n+  auto dstType = op.getResult().getType().cast<RankedTensorType>();\n+  auto srcEncoding = srcType.getEncoding();\n+  auto dstEncoding = dstType.getEncoding();\n+  if (srcEncoding.isa<triton::gpu::SharedEncodingAttr>())\n+    return true;\n+  if (dstEncoding.isa<triton::gpu::DotOperandEncodingAttr>())\n+    return true;\n+  return false;\n+}\n+\n+class TritonGPUReorderInstructionsPass\n+    : public TritonGPUReorderInstructionsBase<\n+          TritonGPUReorderInstructionsPass> {\n+public:\n+  TritonGPUReorderInstructionsPass() = default;\n+\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp m = getOperation();\n+    // Sink conversions into loops when they will increase\n+    // register pressure\n+    DenseMap<Operation *, Operation *> opToMove;\n+    m.walk([&](triton::gpu::ConvertLayoutOp op) {\n+      if (!willIncreaseRegisterPressure(op))\n+        return;\n+      auto user_begin = op->user_begin();\n+      auto user_end = op->user_end();\n+      if (std::distance(user_begin, user_end) != 1)\n+        return;\n+      opToMove.insert({op, *user_begin});\n+    });\n+    for (auto &kv : opToMove)\n+      kv.first->moveBefore(kv.second);\n+    // Move convert(load) immediately after dependent load\n+    m.walk([&](triton::gpu::ConvertLayoutOp op) {\n+      auto dstType = op.getResult().getType().cast<RankedTensorType>();\n+      auto dstEncoding = dstType.getEncoding();\n+      if (!dstEncoding.isa<triton::gpu::SharedEncodingAttr>())\n+        return;\n+      Operation *argOp = op.getOperand().getDefiningOp();\n+      if (!argOp)\n+        return;\n+      op->moveAfter(argOp);\n+    });\n+    // Move transpositions just after their definition\n+    opToMove.clear();\n+    m.walk([&](triton::TransOp op) {\n+      Operation *argOp = op.getOperand().getDefiningOp();\n+      if (!argOp)\n+        return;\n+      op->moveAfter(argOp);\n+    });\n+    // Move `dot` operand so that conversions to opIdx=0 happens before\n+    // conversions to opIdx=1\n+    m.walk([&](triton::gpu::ConvertLayoutOp op) {\n+      auto dstType = op.getResult().getType().cast<RankedTensorType>();\n+      auto dstEncoding =\n+          dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+      if (!dstEncoding)\n+        return;\n+      int opIdx = dstEncoding.getOpIdx();\n+      if (opIdx != 1)\n+        return;\n+      if (op->getUsers().empty())\n+        return;\n+      auto user_begin = op->user_begin();\n+      op->moveBefore(*user_begin);\n+    });\n+    return;\n+  }\n+};\n+\n+std::unique_ptr<Pass> mlir::createTritonGPUReorderInstructionsPass() {\n+  return std::make_unique<TritonGPUReorderInstructionsPass>();\n+}"}, {"filename": "lib/Dialect/TritonGPU/Transforms/UpdateMmaForVolta.cpp", "status": "added", "additions": 354, "deletions": 0, "changes": 354, "file_content_changes": "@@ -0,0 +1,354 @@\n+#include \"Utility.h\"\n+#include \"mlir/Dialect/SCF/SCF.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+\n+namespace mlir {\n+namespace {\n+using triton::DotOp;\n+using triton::gpu::ConvertLayoutOp;\n+using triton::gpu::DotOperandEncodingAttr;\n+using triton::gpu::MmaEncodingAttr;\n+using triton::gpu::SliceEncodingAttr;\n+\n+// This pattern collects the wrong Mma those need to update and create the right\n+// ones for each.\n+// TODO[Superjomn]: RewirtePattern is not needed here, Rewrite this to a method\n+class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n+  // Holds the mapping from old(wrong) mmaEncodingAttr to the new(correct)\n+  // mmaEncodingAttr.\n+  DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n+\n+public:\n+  CollectMmaToUpdateForVolta(\n+      mlir::MLIRContext *ctx,\n+      DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n+      : mlir::RewritePattern(triton::DotOp::getOperationName(), 1, ctx),\n+        mmaToUpdate(mmaToUpdate) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+\n+    auto dotOp = cast<triton::DotOp>(op);\n+    auto *ctx = dotOp->getContext();\n+    auto AT = dotOp.a().getType().cast<RankedTensorType>();\n+    auto BT = dotOp.b().getType().cast<RankedTensorType>();\n+    auto DT = dotOp.d().getType().cast<RankedTensorType>();\n+    if (!DT.getEncoding())\n+      return failure();\n+    auto mmaLayout = DT.getEncoding().dyn_cast<MmaEncodingAttr>();\n+    if (!(mmaLayout && mmaLayout.isVolta()))\n+      return failure();\n+\n+    // Has processed.\n+    if (mmaToUpdate.count(mmaLayout))\n+      return failure();\n+\n+    auto dotOperandA = AT.getEncoding().cast<DotOperandEncodingAttr>();\n+    auto dotOperandB = BT.getEncoding().cast<DotOperandEncodingAttr>();\n+    bool isARow = dotOperandA.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    bool isBRow = dotOperandB.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    auto [isARow_, isBRow_, isAVec4, isBVec4, mmaId] =\n+        mmaLayout.decodeVoltaLayoutStates();\n+\n+    // The wpt of MMAv1 is also determined by isARow, isBRow and shape, and it\n+    // could only be set here for those states might be updated by previous\n+    // patterns in the Combine Pass.\n+    if (isARow_ == isARow && isBRow_ == isBRow) {\n+      auto tgtWpt =\n+          getWarpsPerCTA(DT.getShape(), isARow, isBRow, isAVec4, isBVec4,\n+                         product(mmaLayout.getWarpsPerCTA()));\n+      // Check if the wpt should be updated.\n+      if (tgtWpt == mmaLayout.getWarpsPerCTA() ||\n+          !MmaEncodingAttr::_mmaV1UpdateWpt)\n+        return failure();\n+    }\n+\n+    MmaEncodingAttr newMmaLayout;\n+    {\n+      // Create a temporary MMA layout to obtain the isAVec4 and isBVec4\n+      auto tmpMmaLayout = MmaEncodingAttr::get(\n+          ctx, mmaLayout.getVersionMajor(), mmaLayout.getWarpsPerCTA(),\n+          AT.getShape(), BT.getShape(), isARow, isBRow, mmaId);\n+      auto [isARow_, isBRow_, isAVec4_, isBVec4_, _] =\n+          tmpMmaLayout.decodeVoltaLayoutStates();\n+\n+      // Recalculate the wpt, for here we could get the latest information, the\n+      // wpt should be updated.\n+      auto updatedWpt =\n+          getWarpsPerCTA(DT.getShape(), isARow_, isBRow_, isAVec4_, isBVec4_,\n+                         product(mmaLayout.getWarpsPerCTA()));\n+      auto newWpt = MmaEncodingAttr::_mmaV1UpdateWpt\n+                        ? updatedWpt\n+                        : mmaLayout.getWarpsPerCTA();\n+      newMmaLayout = MmaEncodingAttr::get(ctx, mmaLayout.getVersionMajor(),\n+                                          newWpt, AT.getShape(), BT.getShape(),\n+                                          isARow, isBRow, mmaId);\n+    }\n+\n+    // Collect the wrong MMA Layouts, and mark need to update.\n+    mmaToUpdate.try_emplace(mmaLayout, newMmaLayout);\n+\n+    return failure();\n+  }\n+\n+  // Get the wpt for MMAv1 using more information.\n+  // Reference the original logic here\n+  // https://github.com/openai/triton/blob/0e4691e6dd91e001a8d33b71badf8b3314325459/lib/codegen/analysis/layout.cc#L223\n+  SmallVector<unsigned, 2> getWarpsPerCTA(ArrayRef<int64_t> shape, bool isARow,\n+                                          bool isBRow, bool isAVec4,\n+                                          bool isBVec4, int numWarps) const {\n+    // TODO[Superjomn]: Share code with\n+    // DotOpMmaV1ConversionHelper::AParam/BParam, since same code to compute the\n+    // rep,spw and fpw.\n+    SmallVector<unsigned, 2> wpt({1, 1});\n+    SmallVector<unsigned, 2> wpt_nm1;\n+\n+    SmallVector<int, 2> rep(2), spw(2);\n+    std::array<int, 3> fpw{{2, 2, 1}};\n+    int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+    rep[0] = 2 * packSize0;\n+    spw[0] = fpw[0] * 4 * rep[0];\n+\n+    int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+    rep[1] = 2 * packSize1;\n+    spw[1] = fpw[1] * 4 * rep[1];\n+\n+    do {\n+      wpt_nm1 = wpt;\n+      if (wpt[0] * wpt[1] < numWarps)\n+        wpt[0] = std::clamp<int>(wpt[0] * 2, 1, shape[0] / spw[0]);\n+      if (wpt[0] * wpt[1] < numWarps)\n+        wpt[1] = std::clamp<int>(wpt[1] * 2, 1, shape[1] / spw[1]);\n+    } while (wpt_nm1 != wpt);\n+\n+    return wpt;\n+  }\n+};\n+\n+class UpdateMMAForMMAv1 : public mlir::RewritePattern {\n+  const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n+\n+public:\n+  UpdateMMAForMMAv1(\n+      MLIRContext *context,\n+      const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n+      : RewritePattern(MatchAnyOpTypeTag{}, 1, context),\n+        mmaToUpdate(mmaToUpdate) {}\n+\n+  LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    // Nothing to update\n+    if (mmaToUpdate.empty())\n+      return failure();\n+\n+    if (auto dotOp = llvm::dyn_cast<DotOp>(op))\n+      return rewriteDotOp(op, rewriter);\n+    else if (auto cvtOp = llvm::dyn_cast<ConvertLayoutOp>(op))\n+      return rewriteCvtOp(op, rewriter);\n+    else if (auto expandDimsOp = llvm::dyn_cast<triton::ExpandDimsOp>(op))\n+      return rewriteExpandDimsOp(op, rewriter);\n+    else if (auto constOp = llvm::dyn_cast<arith::ConstantOp>(op))\n+      return rewriteConstantOp(op, rewriter);\n+    else\n+      return rewriteElementwiseOp(op, rewriter);\n+    return failure();\n+  }\n+\n+  LogicalResult rewriteDotOp(Operation *op,\n+                             mlir::PatternRewriter &rewriter) const {\n+    auto dotOp = llvm::cast<DotOp>(op);\n+    auto tensorTy = dotOp->getResult(0).getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return failure();\n+\n+    auto mma = dotOp.d()\n+                   .getType()\n+                   .cast<RankedTensorType>()\n+                   .getEncoding()\n+                   .dyn_cast<MmaEncodingAttr>();\n+    if (!mma || !mmaToUpdate.count(mma))\n+      return failure();\n+\n+    auto newTensorTy = getUpdatedType(tensorTy);\n+    rewriter.replaceOpWithNewOp<DotOp>(op, newTensorTy, dotOp.a(), dotOp.b(),\n+                                       dotOp.c(), dotOp.allowTF32());\n+    return success();\n+  }\n+\n+  LogicalResult rewriteCvtOp(Operation *op,\n+                             mlir::PatternRewriter &rewriter) const {\n+    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n+    if (!needUpdate(cvt.getResult().getType()))\n+      return failure();\n+    auto tensorTy = cvt.result().getType().dyn_cast<RankedTensorType>();\n+\n+    auto newTensorTy = getUpdatedType(tensorTy);\n+    auto newOp = rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n+                                                              cvt.getOperand());\n+    return success();\n+  }\n+\n+  LogicalResult rewriteExpandDimsOp(Operation *op,\n+                                    mlir::PatternRewriter &rewriter) const {\n+    auto expandDims = llvm::cast<triton::ExpandDimsOp>(op);\n+    auto srcTy = expandDims.src().getType();\n+    auto resTy = expandDims.getResult().getType();\n+\n+    // the result type need to update\n+    if (!needUpdate(srcTy) && needUpdate(resTy)) {\n+      rewriter.replaceOpWithNewOp<triton::ExpandDimsOp>(op, expandDims.src(),\n+                                                        expandDims.axis());\n+      return success();\n+    }\n+\n+    return failure();\n+  }\n+\n+  LogicalResult rewriteConstantOp(Operation *op,\n+                                  mlir::PatternRewriter &rewriter) const {\n+    auto constant = llvm::cast<arith::ConstantOp>(op);\n+    auto resTy = constant.getResult().getType();\n+    if (!needUpdate(resTy))\n+      return failure();\n+\n+    auto tensorTy = constant.getResult().getType().cast<RankedTensorType>();\n+    auto mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n+    if ((!mma))\n+      return failure();\n+\n+    auto newTensorTy = getUpdatedType(tensorTy);\n+    if (auto attr = constant.getValue().dyn_cast<SplatElementsAttr>()) {\n+      auto newRet =\n+          SplatElementsAttr::get(newTensorTy, attr.getSplatValue<Attribute>());\n+      rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newTensorTy, newRet);\n+      return success();\n+    }\n+\n+    return failure();\n+  }\n+\n+  LogicalResult rewriteElementwiseOp(Operation *op,\n+                                     mlir::PatternRewriter &rewriter) const {\n+    if (op->getNumOperands() != 1 || op->getNumResults() != 1)\n+      return failure();\n+\n+    auto srcTy = op->getOperand(0).getType();\n+    auto resTy = op->getResult(0).getType();\n+    if (!needUpdate(srcTy) && needUpdate(resTy)) {\n+      op->getResult(0).setType(\n+          getUpdatedType(resTy.dyn_cast<RankedTensorType>()));\n+      return success();\n+    }\n+    return failure();\n+  }\n+\n+  RankedTensorType getUpdatedType(RankedTensorType type) const {\n+    if (!needUpdate(type))\n+      return type;\n+    auto encoding = type.getEncoding();\n+    if (auto mma = encoding.dyn_cast<MmaEncodingAttr>()) {\n+      auto newMma = mmaToUpdate.lookup(mma);\n+      return RankedTensorType::get(type.getShape(), type.getElementType(),\n+                                   newMma);\n+    } else if (auto slice = encoding.dyn_cast<SliceEncodingAttr>()) {\n+      if (auto mma = slice.getParent().dyn_cast<MmaEncodingAttr>()) {\n+        auto newMma = mmaToUpdate.lookup(mma);\n+        auto newSlice =\n+            SliceEncodingAttr::get(slice.getContext(), slice.getDim(), newMma);\n+        return RankedTensorType::get(type.getShape(), type.getElementType(),\n+                                     newSlice);\n+      }\n+    } else if (auto dotOp = encoding.dyn_cast<DotOperandEncodingAttr>()) {\n+      if (auto mma = dotOp.getParent().dyn_cast<MmaEncodingAttr>()) {\n+        auto newMma = mmaToUpdate.lookup(mma);\n+        auto newDotOp =\n+            DotOperandEncodingAttr::get(dotOp.getContext(), dotOp.getOpIdx(),\n+                                        newMma, dotOp.getIsMMAv1Row());\n+        return RankedTensorType::get(type.getShape(), type.getElementType(),\n+                                     newDotOp);\n+      }\n+    }\n+    return type;\n+  }\n+\n+  // Tell if this type contains a wrong MMA encoding and need to update.\n+  bool needUpdate(Type type) const {\n+    auto tensorTy = type.dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return false;\n+    return needUpdate(tensorTy);\n+  }\n+\n+  // Tell if this type contains a wrong MMA encoding and need to update.\n+  bool needUpdate(RankedTensorType type) const {\n+    auto encoding = type.getEncoding();\n+    if (!encoding)\n+      return false;\n+\n+    MmaEncodingAttr mma;\n+    if ((mma = encoding.dyn_cast<MmaEncodingAttr>())) {\n+    } else if (auto slice = encoding.dyn_cast<SliceEncodingAttr>()) {\n+      mma = slice.getParent().dyn_cast<MmaEncodingAttr>();\n+    } else if (auto dotOp = encoding.dyn_cast<DotOperandEncodingAttr>()) {\n+      mma = dotOp.getParent().dyn_cast<MmaEncodingAttr>();\n+    }\n+\n+    return mma && mmaToUpdate.count(mma);\n+  }\n+};\n+\n+} // namespace\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n+\n+class UpdateMmaForVoltaPass\n+    : public UpdateMmaForVoltaBase<UpdateMmaForVoltaPass> {\n+public:\n+  UpdateMmaForVoltaPass() = default;\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp m = getOperation();\n+\n+    llvm::DenseMap<MmaEncodingAttr, MmaEncodingAttr> mmaToUpdate;\n+    {\n+      mlir::RewritePatternSet patterns(context);\n+      patterns.add<CollectMmaToUpdateForVolta>(context, mmaToUpdate);\n+\n+      GreedyRewriteConfig config;\n+      config.enableRegionSimplification =\n+          false; // The pattern doesn't modify the IR\n+      if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n+        signalPassFailure();\n+    }\n+\n+    if (!mmaToUpdate.empty()) {\n+      mlir::RewritePatternSet patterns(context);\n+      patterns.add<UpdateMMAForMMAv1>(context, mmaToUpdate);\n+\n+      mlir::GreedyRewriteConfig config;\n+      // Make sure the slice and dot_operand layouts' parent mma are updated\n+      // before updating DotOp or it will get a mismatch parent-encoding.\n+      config.useTopDownTraversal = true;\n+\n+      if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n+        signalPassFailure();\n+\n+      if (fixupLoops(m).failed())\n+        signalPassFailure();\n+    }\n+  }\n+};\n+\n+std::unique_ptr<Pass> createTritonGPUUpdateMmaForVoltaPass() {\n+  return std::make_unique<UpdateMmaForVoltaPass>();\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "added", "additions": 63, "deletions": 0, "changes": 63, "file_content_changes": "@@ -0,0 +1,63 @@\n+#include \"Utility.h\"\n+#include \"mlir/Dialect/SCF/SCF.h\"\n+#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+\n+namespace mlir {\n+\n+namespace {\n+\n+class FixupLoop : public mlir::RewritePattern {\n+\n+public:\n+  explicit FixupLoop(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(scf::ForOp::getOperationName(), 2, context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto forOp = cast<scf::ForOp>(op);\n+\n+    // Rewrite init argument\n+    SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n+    bool shouldRematerialize = false;\n+    for (size_t i = 0; i < newInitArgs.size(); i++) {\n+      if (newInitArgs[i].getType() != forOp.getRegionIterArgs()[i].getType() ||\n+          newInitArgs[i].getType() != forOp.getResultTypes()[i]) {\n+        shouldRematerialize = true;\n+        break;\n+      }\n+    }\n+    if (!shouldRematerialize)\n+      return failure();\n+\n+    scf::ForOp newForOp = rewriter.create<scf::ForOp>(\n+        forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n+        forOp.getStep(), newInitArgs);\n+    newForOp->moveBefore(forOp);\n+    rewriter.setInsertionPointToStart(newForOp.getBody());\n+    BlockAndValueMapping mapping;\n+    for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n+      mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n+    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n+\n+    for (Operation &op : forOp.getBody()->getOperations()) {\n+      rewriter.clone(op, mapping);\n+    }\n+    rewriter.replaceOp(forOp, newForOp.getResults());\n+    return success();\n+  }\n+};\n+\n+} // namespace\n+\n+LogicalResult fixupLoops(ModuleOp mod) {\n+  auto *ctx = mod.getContext();\n+  mlir::RewritePatternSet patterns(ctx);\n+  patterns.add<FixupLoop>(ctx);\n+  if (applyPatternsAndFoldGreedily(mod, std::move(patterns)).failed())\n+    return failure();\n+  return success();\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.h", "status": "added", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -0,0 +1,12 @@\n+#ifndef TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#define TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+\n+namespace mlir {\n+\n+LogicalResult fixupLoops(ModuleOp mod);\n+\n+} // namespace mlir\n+\n+#endif // TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 13, "deletions": 1, "changes": 14, "file_content_changes": "@@ -1362,11 +1362,23 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::createTritonGPUPrefetchPass());\n            })\n-      .def(\"add_triton_gpu_combine_pass\",\n+      .def(\"add_tritongpu_combine_pass\",\n            [](mlir::PassManager &self, int computeCapability) {\n              self.addPass(\n                  mlir::createTritonGPUCombineOpsPass(computeCapability));\n            })\n+      .def(\"add_tritongpu_update_mma_for_volta_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::createTritonGPUUpdateMmaForVoltaPass());\n+           })\n+      .def(\"add_tritongpu_reorder_instructions_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::createTritonGPUReorderInstructionsPass());\n+           })\n+      .def(\"add_tritongpu_decompose_conversions_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::createTritonGPUDecomposeConversionsPass());\n+           })\n       .def(\"add_triton_gpu_to_llvm\",\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::triton::createConvertTritonGPUToLLVMPass());"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -1453,6 +1453,20 @@ def kernel(x):\n     kernel[(1, )](x)\n \n \n+@pytest.mark.parametrize(\"device\", ['cuda', 'cpu'])\n+def test_pointer_arguments(device):\n+    @triton.jit\n+    def kernel(x):\n+        pass\n+    x = torch.empty(1024, device=device)\n+    result = True\n+    try:\n+        kernel[(1,)](x)\n+    except ValueError:\n+        result = True if device == 'cpu' else False\n+    assert result\n+\n+\n @pytest.mark.parametrize(\"value, value_type\", [\n     (-1, 'i32'), (0, 'i32'), (-2**31, 'i32'), (2**31 - 1, 'i32'),\n     (2**31, 'u32'), (2**32 - 1, 'u32'), (2**32, 'i64'), (2**63 - 1, 'i64'),"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 42, "deletions": 9, "changes": 51, "file_content_changes": "@@ -890,17 +890,24 @@ def ttir_to_ttgir(mod, num_warps, num_stages, compute_capability):\n     pm.add_coalesce_pass()\n     # The combine pass converts blocked layout to mma layout\n     # for dot ops so that pipeline can get shared memory swizzled correctly.\n-    pm.add_triton_gpu_combine_pass(compute_capability)\n+    pm.add_tritongpu_combine_pass(compute_capability)\n     pm.add_tritongpu_pipeline_pass(num_stages)\n     # Prefetch must be done after pipeline pass because pipeline pass\n     # extracts slices from the original tensor.\n     pm.add_tritongpu_prefetch_pass()\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n-    pm.add_triton_gpu_combine_pass(compute_capability)\n+    pm.add_tritongpu_combine_pass(compute_capability)\n     pm.add_licm_pass()\n-    pm.add_triton_gpu_combine_pass(compute_capability)\n+    pm.add_tritongpu_combine_pass(compute_capability)\n+    if compute_capability // 10 == 7:\n+        # The update_mma_for_volta pass helps to compute some information for MMA encoding specifically for MMAv1\n+        pm.add_tritongpu_update_mma_for_volta_pass()\n     pm.add_cse_pass()\n+    pm.add_tritongpu_decompose_conversions_pass()\n+    pm.add_cse_pass()\n+    pm.add_symbol_dce_pass()\n+    pm.add_tritongpu_reorder_instructions_pass()\n     pm.run(mod)\n     return mod\n \n@@ -1067,6 +1074,7 @@ def format_of(ty):\n     # generate glue code\n     src = f\"\"\"\n #include \\\"cuda.h\\\"\n+#include <stdbool.h>\n #include <Python.h>\n \n static inline void gpuAssert(CUresult code, const char *file, int line)\n@@ -1092,12 +1100,22 @@ def format_of(ty):\n   }}\n }}\n \n-static inline CUdeviceptr getPointer(PyObject *obj, int idx) {{\n+typedef struct _DevicePtrInfo {{\n+    CUdeviceptr dev_ptr;\n+    bool valid;\n+}} DevicePtrInfo;\n+\n+static inline DevicePtrInfo getPointer(PyObject *obj, int idx) {{\n+  DevicePtrInfo ptr_info;\n+  ptr_info.dev_ptr = 0;\n+  ptr_info.valid = true;\n   if (PyLong_Check(obj)) {{\n-    return (CUdeviceptr)PyLong_AsUnsignedLongLong(obj);\n+    ptr_info.dev_ptr = PyLong_AsUnsignedLongLong(obj);\n+    return ptr_info;\n   }}\n   if (obj == Py_None) {{\n-    return (CUdeviceptr)0;\n+    // valid nullptr\n+    return ptr_info;\n   }}\n   PyObject *ptr = PyObject_GetAttrString(obj, \"data_ptr\");\n   if(ptr){{\n@@ -1107,11 +1125,23 @@ def format_of(ty):\n     Py_DECREF(ptr);\n     if (!PyLong_Check(ret)) {{\n       PyErr_SetString(PyExc_TypeError, \"data_ptr method of Pointer object must return 64-bit int\");\n+      ptr_info.valid = false;\n+      return ptr_info;\n     }}\n-    return (CUdeviceptr)PyLong_AsUnsignedLongLong(ret);\n+    ptr_info.dev_ptr = PyLong_AsUnsignedLongLong(ret);\n+    unsigned attr;\n+    CUresult status =\n+        cuPointerGetAttribute(&attr, CU_POINTER_ATTRIBUTE_MEMORY_TYPE, ptr_info.dev_ptr);\n+    if (!(attr == CU_MEMORYTYPE_DEVICE || attr == CU_MEMORYTYPE_UNIFIED) ||\n+        !(status == CUDA_SUCCESS)) {{\n+        PyErr_Format(PyExc_ValueError,\n+                     \"Pointer argument (at %d) cannot be accessed from Triton (cpu tensor?)\", idx);\n+        ptr_info.valid = false;\n+    }}\n+    return ptr_info;\n   }}\n   PyErr_SetString(PyExc_TypeError, \"Pointer argument must be either uint64 or have data_ptr method\");\n-  return (CUdeviceptr)0;\n+  return ptr_info;\n }}\n \n static PyObject* launch(PyObject* self, PyObject* args) {{\n@@ -1135,7 +1165,10 @@ def format_of(ty):\n     Py_DECREF(new_args);\n   }}\n \n-  _launch(gridX, gridY, gridZ, num_warps, shared_memory, (CUstream)_stream, (CUfunction)_function, {', '.join(f\"getPointer(_arg{i},{i})\" if ty[0]==\"*\" else f\"_arg{i}\"for i, ty in signature.items())});\n+\n+  // raise exception asap\n+  {\"; \".join([f\"DevicePtrInfo ptr_info{i} = getPointer(_arg{i}, {i}); if (!ptr_info{i}.valid) return NULL;\" if ty[0] == \"*\" else \"\" for i, ty in signature.items()])};\n+  _launch(gridX, gridY, gridZ, num_warps, shared_memory, (CUstream)_stream, (CUfunction)_function, {', '.join(f\"ptr_info{i}.dev_ptr\" if ty[0]==\"*\" else f\"_arg{i}\"for i, ty in signature.items())});\n \n   if (launch_exit_hook != Py_None) {{\n     PyObject *new_args = NULL;"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 6, "deletions": 9, "changes": 15, "file_content_changes": "@@ -225,18 +225,18 @@ def forward(ctx, q, k, v, sm_scale):\n             q.shape[0], q.shape[1], q.shape[2],\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n             BLOCK_DMODEL=Lk, num_warps=num_warps,\n-            num_stages=1,\n+            num_stages=2,\n         )\n \n         ctx.save_for_backward(q, k, v, o, L, m)\n-        ctx.BLOCK = BLOCK\n         ctx.grid = grid\n         ctx.sm_scale = sm_scale\n         ctx.BLOCK_DMODEL = Lk\n         return o\n \n     @staticmethod\n     def backward(ctx, do):\n+        BLOCK = 128\n         q, k, v, o, l, m = ctx.saved_tensors\n         do = do.contiguous()\n         dq = torch.zeros_like(q, dtype=torch.float32)\n@@ -247,11 +247,8 @@ def backward(ctx, do):\n         _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n             o, do, l,\n             do_scaled, delta,\n-            BLOCK_M=ctx.BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n+            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n         )\n-\n-        # NOTE: kernel currently buggy for other values of `num_warps`\n-        num_warps = 8\n         _bwd_kernel[(ctx.grid[1],)](\n             q, k, v, ctx.sm_scale,\n             o, do_scaled,\n@@ -263,8 +260,8 @@ def backward(ctx, do):\n             v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n             q.shape[0], q.shape[1], q.shape[2],\n             ctx.grid[0],\n-            BLOCK_M=ctx.BLOCK, BLOCK_N=ctx.BLOCK,\n-            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=num_warps,\n+            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n             num_stages=1,\n         )\n         return dq, dk, dv, None\n@@ -319,7 +316,7 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n # vary seq length for fixed head and batch=4\n configs = [triton.testing.Benchmark(\n     x_names=['N_CTX'],\n-    x_vals=[2**i for i in range(10, 16)],\n+    x_vals=[2**i for i in range(10, 15)],\n     line_arg='provider',\n     line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n     line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -3,6 +3,7 @@\n #AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#A_SHARED_T = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [0, 1]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n@@ -57,7 +58,7 @@ func @trans(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %tensor = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n   // CHECK: %0 -> %cst\n-  %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n   return\n }\n "}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -4,6 +4,7 @@\n #sliceAd0 = #triton_gpu.slice<{dim = 0, parent = #AL}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#A_SHARED_T = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [0, 1]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n@@ -178,7 +179,7 @@ func @scratch() {\n func @trans(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 1024\n   %tensor = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n-  %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n   return\n }\n \n@@ -303,7 +304,7 @@ func @for_use_ancestor(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16\n   // CHECK-NEXT: offset = 16384, size = 8192\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %a_shared, %b_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n-    %c0 = tt.trans %c_shared_init : (tensor<128x32xf16, #A_SHARED>) -> tensor<32x128xf16, #A_SHARED>\n+    %c0 = tt.trans %c_shared_init : (tensor<128x32xf16, #A_SHARED>) -> tensor<32x128xf16, #A_SHARED_T>\n     // CHECK-NEXT: offset = 24576, size = 8192\n     %c1 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n     scf.yield %b_shared, %a_shared: tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -4,6 +4,7 @@\n #sliceAd0 = #triton_gpu.slice<{dim = 0, parent = #AL}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#A_SHARED_T = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [0, 1]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n@@ -114,7 +115,7 @@ func @extract_slice() {\n // CHECK-LABEL: trans\n func @trans() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n-  %b = tt.trans %cst0 : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  %b = tt.trans %cst0 : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n   return\n }\n "}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 2, "deletions": 14, "changes": 16, "file_content_changes": "@@ -540,35 +540,23 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %index = arith.constant 1 : i32\n \n     // CHECK: llvm.mlir.constant(0 : i32) : i32\n+    // CHECK: llvm.mlir.constant(16 : i32) : i32\n+    // CHECK: llvm.mul\n     // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n-    // CHECK: llvm.mlir.constant(0 : i32) : i32\n-    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n-    // CHECK: llvm.mlir.constant(0 : i32) : i32\n-    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n-    // CHECK: llvm.mlir.constant(0 : i32) : i32\n-    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n-    // CHECK: llvm.mlir.constant(16 : i32) : i32\n-    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n-    // CHECK: llvm.mlir.constant(16 : i32) : i32\n-    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n-    // CHECK: llvm.mlir.constant(16 : i32) : i32\n-    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n-    // CHECK: llvm.mlir.constant(16 : i32) : i32\n-    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 0, "deletions": 29, "changes": 29, "file_content_changes": "@@ -183,32 +183,3 @@ func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f3\n   tt.store %21, %22 : tensor<256xf32, #layout1>\n   return\n }\n-\n-\n-// -----\n-\n-// check the UpdateMMAVersionMinorForVolta pattern\n-#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n-#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n-#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[1,1]}>\n-// Here, the isMMAv1Row of a and b's dot_operands mismatch #mma0's versionMinor,\n-// and the pattern should update the versionMinor.\n-#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n-#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n-// It creates a new MMA layout to fit with $a and $b's dot_operand\n-// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 11, warpsPerCTA = [1, 1]}>\n-module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK-LABEL: dot_mmav1\n-  func @dot_mmav1(%A: tensor<16x16xf16, #blocked0>, %B: tensor<16x16xf16, #blocked0>) -> tensor<16x16xf32, #blocked0> {\n-    %C = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked0>\n-    %AA = triton_gpu.convert_layout %A : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #dot_operand_a>\n-    %BB = triton_gpu.convert_layout %B : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #dot_operand_b>\n-    %CC = triton_gpu.convert_layout %C : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #mma0>\n-\n-    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, %cst {allowTF32 = true} : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<16x16xf32, [[new_mma]]>\n-    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<16x16xf16, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n-    %res = triton_gpu.convert_layout %D : (tensor<16x16xf32, #mma0>) -> tensor<16x16xf32, #blocked0>\n-\n-    return %res : tensor<16x16xf32, #blocked0>\n-  }\n-}"}, {"filename": "test/TritonGPU/update-mma-for-volta.mlir", "status": "added", "additions": 73, "deletions": 0, "changes": 73, "file_content_changes": "@@ -0,0 +1,73 @@\n+// RUN: triton-opt %s -split-input-file -tritongpu-combine -tritongpu-update-mma-for-volta 2>&1 | FileCheck %s\n+\n+// -----\n+\n+// check the UpdateMMAVersionMinorForVolta pattern\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [4, 4], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n+#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[4,4]}>\n+// Here, the isMMAv1Row of a and b's dot_operands mismatch #mma0's versionMinor,\n+// and the pattern should update the versionMinor.\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n+// It creates a new MMA layout to fit with $a and $b's dot_operand, and get the right warpsPerCTA\n+// The ID of this MMA instance should be 0.\n+// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 4]}>\n+module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n+  // CHECK-LABEL: dot_mmav1\n+  func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n+    %C = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked0>\n+    %AA = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a>\n+    %BB = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b>\n+    %CC = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma0>\n+\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, %cst {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma]]>\n+    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<64x64xf32, #mma0>\n+    %res = triton_gpu.convert_layout %D : (tensor<64x64xf32, #mma0>) -> tensor<64x64xf32, #blocked0>\n+\n+    return %res : tensor<64x64xf32, #blocked0>\n+  }\n+}\n+\n+\n+// -----\n+// Check id in multiple MMA layout instances\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [4, 4], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n+#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[4,4]}>\n+// mma id=1, with all other boolean flags be false, should get a versionMinor of 16(= 1 * 1<<4)\n+#mma1 = #triton_gpu.mma<{versionMajor=1, versionMinor=16, warpsPerCTA=[4,4]}>\n+\n+// Will still get two MMA layouts\n+// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 4]}>\n+// CHECK: [[new_mma1:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 19, warpsPerCTA = [4, 4]}>\n+\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n+#dot_operand_a1 = #triton_gpu.dot_op<{opIdx=0, parent=#mma1, isMMAv1Row=true}>\n+#dot_operand_b1 = #triton_gpu.dot_op<{opIdx=1, parent=#mma1, isMMAv1Row=false}>\n+\n+module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n+  // CHECK-LABEL: dot_mmav1\n+  func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n+    %C = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked0>\n+    %AA = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a>\n+    %BB = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b>\n+    %CC = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma0>\n+\n+    %AA1 = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a1>\n+    %BB1 = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b1>\n+    %CC1 = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma1>\n+\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma]]>\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma1]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma1]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma1]]>\n+    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<64x64xf32, #mma0>\n+    %D1 = tt.dot %AA1, %BB1, %CC1 {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a1> * tensor<64x64xf16, #dot_operand_b1> -> tensor<64x64xf32, #mma1>\n+    %res = triton_gpu.convert_layout %D : (tensor<64x64xf32, #mma0>) -> tensor<64x64xf32, #blocked0>\n+    %res1 = triton_gpu.convert_layout %D1 : (tensor<64x64xf32, #mma1>) -> tensor<64x64xf32, #blocked0>\n+    %sum = arith.addf %res, %res1 : tensor<64x64xf32, #blocked0>\n+\n+    return %sum : tensor<64x64xf32, #blocked0>\n+  }\n+}"}, {"filename": "unittest/Dialect/TritonGPU/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -1,6 +1,5 @@\n-\n add_triton_ut(\n \tNAME TestSwizzling\n \tSRCS SwizzleTest.cpp\n \tLIBS TritonGPUIR  ${dialect_libs} ${conversion_libs}\n-)\n\\ No newline at end of file\n+)"}]