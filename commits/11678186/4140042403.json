[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 0, "deletions": 63, "changes": 63, "file_content_changes": "@@ -1952,69 +1952,6 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n     assert torch.equal(z, x)\n \n \n-# layouts = [\n-#     BlockedLayout([1, 8], [2, 16], [4, 1], [1, 0]),\n-#     BlockedLayout([1, 4], [4, 8], [2, 2], [1, 0]),\n-#     BlockedLayout([1, 1], [1, 32], [2, 2], [1, 0]),\n-#     BlockedLayout([8, 1], [16, 2], [1, 4], [0, 1]),\n-#     BlockedLayout([4, 1], [8, 4], [2, 2], [0, 1]),\n-#     BlockedLayout([1, 1], [32, 1], [2, 2], [0, 1]),\n-#     BlockedLayout([4, 4], [1, 32], [4, 1], [1, 0])\n-# ]\n-\n-\n-# @pytest.mark.parametrize(\"shape\", [(128, 128)])\n-# @pytest.mark.parametrize(\"dtype\", ['float16'])\n-# @pytest.mark.parametrize(\"src_layout\", layouts)\n-# @pytest.mark.parametrize(\"dst_layout\", layouts)\n-# def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n-#     if str(src_layout) == str(dst_layout):\n-#         pytest.skip()\n-#     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):\n-#         pytest.skip()\n-\n-#     ir = f\"\"\"\n-# #src = {src_layout}\n-# #dst = {dst_layout}\n-# \"\"\" + \"\"\"\n-# module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-#   func public @kernel_0d1d(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n-#     %cst = arith.constant dense<128> : tensor<128x1xi32, #src>\n-#     %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #src}>>\n-#     %1 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #src}>>\n-#     %2 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>, #src>\n-#     %4 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #src}>>) -> tensor<128x1xi32, #src>\n-#     %5 = arith.muli %4, %cst : tensor<128x1xi32, #src>\n-#     %6 = tt.expand_dims %1 {axis = 0 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #src}>>) -> tensor<1x128xi32, #src>\n-#     %7 = tt.broadcast %6 : (tensor<1x128xi32, #src>) -> tensor<128x128xi32, #src>\n-#     %8 = tt.broadcast %5 : (tensor<128x1xi32, #src>) -> tensor<128x128xi32, #src>\n-#     %9 = arith.addi %8, %7 : tensor<128x128xi32, #src>\n-#     %10 = tt.addptr %2, %9 : tensor<128x128x!tt.ptr<f16>, #src>, tensor<128x128xi32, #src>\n-#     %11 = tt.load %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf16, #src>\n-#     %3 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>, #dst>\n-#     %12 = triton_gpu.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n-#     %13 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n-#     %14 = tt.addptr %3, %12 : tensor<128x128x!tt.ptr<f16>, #dst>, tensor<128x128xi32, #dst>\n-#     tt.store %14, %13 : tensor<128x128xf16, #dst>\n-#     return\n-#   }\n-# }\n-# \"\"\"\n-\n-#     x = to_triton(numpy_random(shape, dtype_str=dtype))\n-#     z = torch.empty_like(x)\n-\n-#     # write the IR to a temporary file using mkstemp\n-#     import tempfile\n-#     with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n-#         f.write(ir)\n-#         f.flush()\n-#         kernel = triton.compile(f.name)\n-#     kernel[(1, 1, 1)](x.data_ptr(), z.data_ptr())\n-\n-#     assert torch.equal(z, x)\n-\n-\n def test_load_scalar_with_mask():\n     @triton.jit\n     def kernel(Input, Index, Out, N: int):"}]