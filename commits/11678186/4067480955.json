[{"filename": "python/test/unit/operators/test_flash_attention.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -1,7 +1,9 @@\n-import torch\n import pytest\n+import torch\n+\n import triton\n \n+\n @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     capability = torch.cuda.get_device_capability()"}, {"filename": "python/triton/ops/__init__.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,8 +1,8 @@\n # from .conv import _conv, conv\n from . import blocksparse\n from .cross_entropy import _cross_entropy, cross_entropy\n-from .matmul import _matmul, matmul\n from .flash_attention import attention\n+from .matmul import _matmul, matmul\n \n __all__ = [\n     \"blocksparse\","}]