[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -56,6 +56,22 @@ def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n     let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n }\n \n+// arith.bitcast doesn't support pointers\n+def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape, \n+                                     SameOperandsAndResultEncoding,\n+                                     NoSideEffect,\n+                                     /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n+    let summary = \"Cast between types of the same bitwidth\";\n+\n+    let arguments = (ins TT_Type:$from);\n+\n+    let results = (outs TT_Type:$result);\n+\n+    let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n+\n+    // TODO: Add verifier\n+}\n+\n def TT_FpToFp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape, \n                                    SameOperandsAndResultEncoding,\n                                    NoSideEffect,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 115, "deletions": 72, "changes": 187, "file_content_changes": "@@ -91,7 +91,7 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n #define store(val, ptr) rewriter.create<LLVM::StoreOp>(loc, val, ptr)\n #define select(...) rewriter.create<LLVM::SelectOp>(loc, __VA_ARGS__)\n #define address_of(...) rewriter.create<LLVM::AddressOfOp>(loc, __VA_ARGS__)\n-#define barrier rewriter.create<mlir::gpu::BarrierOp>(loc)\n+#define barrier() rewriter.create<mlir::gpu::BarrierOp>(loc)\n #define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n #define i32_ty rewriter.getIntegerType(32)\n #define vec_ty(type, num) VectorType::get(num, type)\n@@ -1400,6 +1400,8 @@ struct ExtractSliceOpConversion\n   }\n };\n \n+// TODO: rewrite Ternary/Binary/Unary as Elementwise\n+\n // A CRTP style of base class.\n template <typename SourceOp, typename DestOp, typename ConcreteT>\n class ElementwiseOpConversionBase\n@@ -1482,6 +1484,77 @@ struct ElementwiseOpConversion\n   }\n };\n \n+//\n+// Ternary\n+//\n+\n+template <typename SourceOp, typename DestOp, typename ConcreteT>\n+class TernaryOpConversionBase\n+    : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n+public:\n+  using OpAdaptor = typename SourceOp::Adaptor;\n+\n+  explicit TernaryOpConversionBase(LLVMTypeConverter &typeConverter,\n+                                   PatternBenefit benefit = 1)\n+      : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n+\n+  LogicalResult\n+  matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto resultTy = op.getType().template dyn_cast<RankedTensorType>();\n+    // ArithmeticToLLVM will handle the lowering of scalar ArithOps\n+    if (!resultTy)\n+      return failure();\n+\n+    Location loc = op->getLoc();\n+    auto resultLayout =\n+        resultTy.getEncoding().template dyn_cast<BlockedEncodingAttr>();\n+    auto resultShape = resultTy.getShape();\n+    assert(resultLayout && \"Unexpected resultLayout in TernaryOpConversion\");\n+    unsigned elems = resultLayout.getElemsPerThread(resultShape);\n+    Type elemTy =\n+        this->getTypeConverter()->convertType(resultTy.getElementType());\n+    SmallVector<Type> types(elems, elemTy);\n+    Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n+\n+    auto *concreteThis = static_cast<const ConcreteT *>(this);\n+    auto lhss =\n+        this->getElementsFromStruct(loc, adaptor.getOperands()[0], rewriter);\n+    auto rhss =\n+        this->getElementsFromStruct(loc, adaptor.getOperands()[1], rewriter);\n+    auto thss =\n+        this->getElementsFromStruct(loc, adaptor.getOperands()[2], rewriter);\n+    SmallVector<Value> resultVals(elems);\n+    for (unsigned i = 0; i < elems; ++i) {\n+      resultVals[i] = concreteThis->createDestOp(op, rewriter, elemTy, lhss[i],\n+                                                 rhss[i], thss[i], loc);\n+    }\n+    Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n+    rewriter.replaceOp(op, view);\n+    return success();\n+  }\n+};\n+\n+template <typename SourceOp, typename DestOp>\n+struct TernaryOpConversion\n+    : public TernaryOpConversionBase<SourceOp, DestOp,\n+                                     TernaryOpConversion<SourceOp, DestOp>> {\n+\n+  explicit TernaryOpConversion(LLVMTypeConverter &typeConverter,\n+                               PatternBenefit benefit = 1)\n+      : TernaryOpConversionBase<SourceOp, DestOp,\n+                                TernaryOpConversion<SourceOp, DestOp>>(\n+            typeConverter, benefit) {}\n+\n+  using OpAdaptor = typename SourceOp::Adaptor;\n+  // An interface to support variant DestOp builder.\n+  DestOp createDestOp(SourceOp op, ConversionPatternRewriter &rewriter,\n+                      Type elemTy, Value lhs, Value rhs, Value th,\n+                      Location loc) const {\n+    return rewriter.create<DestOp>(loc, elemTy, lhs, rhs, th);\n+  }\n+};\n+\n //\n // Unary\n //\n@@ -1883,7 +1956,7 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n \n   for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n     auto multiDimRepId = getMultiDimIndex<unsigned>(repId, numReplicates);\n-    barrier;\n+    barrier();\n     if (srcLayout.isa<BlockedEncodingAttr>() ||\n         srcLayout.isa<MmaEncodingAttr>()) {\n       processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,\n@@ -1893,7 +1966,7 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n       assert(0 && \"ConvertLayout with input layout not implemented\");\n       return failure();\n     }\n-    barrier;\n+    barrier();\n     if (dstLayout.isa<BlockedEncodingAttr>() ||\n         dstLayout.isa<MmaEncodingAttr>()) {\n       processReplica(loc, rewriter, /*stNotRd*/ false, dstTy, outNumCTAsEachRep,\n@@ -1969,6 +2042,11 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n   smemBase = bitcast(elemPtrTy, smemBase);\n   unsigned numWordsEachRep = product<unsigned>(wordsInEachRep);\n   SmallVector<Value> wordVecs(numWordsEachRep);\n+  // TODO: We should get less barriers if it is handled by membar pass\n+  //       instead of the backend, since the later can only handle it in\n+  //       the most conservative way. However just keep for now and revisit\n+  //       in the future in case necessary.\n+  barrier();\n   for (unsigned i = 0; i < numElems; ++i) {\n     if (i % srcAccumSizeInThreads == 0) {\n       // start of a replication\n@@ -2022,8 +2100,7 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n       }\n     }\n   }\n-  // TODO: double confirm if the Barrier is necessary here\n-  barrier;\n+  barrier();\n   rewriter.replaceOp(op, smemBase);\n   return success();\n }\n@@ -3063,11 +3140,6 @@ struct MMA16816ConversionHelper {\n         for (unsigned n = 0; n < numRepN; ++n)\n           callMma(2 * m, n, 2 * k);\n \n-    // NOTE, the barrier here is a temporary trick making the gemm with a\n-    // k-forloop pass the precision test, or it will fail.\n-    // TODO[Superjomn]: Fix with a more general and performance-friendly way.\n-    barrier;\n-\n     // replace with new packed result\n     Type structTy = LLVM::LLVMStructType::getLiteral(\n         ctx, SmallVector<Type>(fc.size(), type::f32Ty(ctx)));\n@@ -3645,23 +3717,12 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                         benefit);\n   patterns.add<ArithConstantSplatOpConversion>(typeConverter, benefit);\n   patterns.add<AsyncWaitOpConversion>(typeConverter, benefit);\n-  patterns.add<ElementwiseOpConversion<arith::AddIOp, LLVM::AddOp>>(\n-      typeConverter, benefit);\n-  patterns.add<ElementwiseOpConversion<arith::SubIOp, LLVM::SubOp>>(\n-      typeConverter, benefit);\n-  patterns.add<ElementwiseOpConversion<arith::AddFOp, LLVM::FAddOp>>(\n-      typeConverter, benefit);\n-  patterns.add<ElementwiseOpConversion<arith::SubFOp, LLVM::FSubOp>>(\n-      typeConverter, benefit);\n-  patterns.add<ElementwiseOpConversion<arith::MulIOp, LLVM::MulOp>>(\n-      typeConverter, benefit);\n-  patterns.add<ElementwiseOpConversion<arith::MulFOp, LLVM::FMulOp>>(\n-      typeConverter, benefit);\n-\n-  patterns.add<ElementwiseOpConversion<arith::AndIOp, LLVM::AndOp>>(\n-      typeConverter, benefit);\n-  patterns.add<ElementwiseOpConversion<arith::OrIOp, LLVM::OrOp>>(typeConverter,\n-                                                                  benefit);\n+\n+#define POPULATE_TERNARY_OP(SRC_OP, DST_OP)                                    \\\n+  patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n+  POPULATE_TERNARY_OP(triton::gpu::SelectOp, LLVM::SelectOp);\n+#undef POPULATE_TERNARY_OP\n+\n #define POPULATE_BINARY_OP(SRC_OP, DST_OP)                                     \\\n   patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n \n@@ -3677,24 +3738,38 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   POPULATE_BINARY_OP(arith::RemFOp, LLVM::FRemOp) // %\n   POPULATE_BINARY_OP(arith::RemSIOp, LLVM::SRemOp)\n   POPULATE_BINARY_OP(arith::RemUIOp, LLVM::URemOp)\n-  POPULATE_BINARY_OP(arith::AndIOp, LLVM::AndOp) // &\n-  POPULATE_BINARY_OP(arith::OrIOp, LLVM::OrOp)   // |\n+  POPULATE_BINARY_OP(arith::AndIOp, LLVM::AndOp)   // &\n+  POPULATE_BINARY_OP(arith::OrIOp, LLVM::OrOp)     // |\n+  POPULATE_BINARY_OP(arith::XOrIOp, LLVM::XOrOp)   // ^\n+  POPULATE_BINARY_OP(arith::ShLIOp, LLVM::ShlOp)   // <<\n+  POPULATE_BINARY_OP(arith::ShRSIOp, LLVM::AShrOp) // >>\n+  POPULATE_BINARY_OP(arith::ShRUIOp, LLVM::LShrOp) // >>\n #undef POPULATE_BINARY_OP\n \n   patterns.add<CmpIOpConversion>(typeConverter, benefit);\n   patterns.add<CmpFOpConversion>(typeConverter, benefit);\n-#define POPULATE_CAST_OP(SRC_OP, DST_OP)                                       \\\n+#define POPULATE_UNARY_OP(SRC_OP, DST_OP)                                      \\\n   patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n-  POPULATE_CAST_OP(arith::TruncIOp, LLVM::TruncOp)\n-  POPULATE_CAST_OP(arith::TruncFOp, LLVM::FPTruncOp)\n-  POPULATE_CAST_OP(arith::ExtSIOp, LLVM::SExtOp)\n-  POPULATE_CAST_OP(arith::ExtUIOp, LLVM::ZExtOp)\n-  POPULATE_CAST_OP(arith::FPToUIOp, LLVM::FPToUIOp)\n-  POPULATE_CAST_OP(arith::FPToSIOp, LLVM::FPToSIOp)\n-  POPULATE_CAST_OP(arith::UIToFPOp, LLVM::UIToFPOp)\n-  POPULATE_CAST_OP(arith::SIToFPOp, LLVM::SIToFPOp)\n-  POPULATE_CAST_OP(arith::ExtFOp, LLVM::FPExtOp)\n-#undef POPULATE_CAST_OP\n+  POPULATE_UNARY_OP(arith::TruncIOp, LLVM::TruncOp)\n+  POPULATE_UNARY_OP(arith::TruncFOp, LLVM::FPTruncOp)\n+  POPULATE_UNARY_OP(arith::ExtSIOp, LLVM::SExtOp)\n+  POPULATE_UNARY_OP(arith::ExtUIOp, LLVM::ZExtOp)\n+  POPULATE_UNARY_OP(arith::FPToUIOp, LLVM::FPToUIOp)\n+  POPULATE_UNARY_OP(arith::FPToSIOp, LLVM::FPToSIOp)\n+  POPULATE_UNARY_OP(arith::UIToFPOp, LLVM::UIToFPOp)\n+  POPULATE_UNARY_OP(arith::SIToFPOp, LLVM::SIToFPOp)\n+  POPULATE_UNARY_OP(arith::ExtFOp, LLVM::FPExtOp)\n+  POPULATE_UNARY_OP(triton::BitcastOp, LLVM::BitcastOp)\n+  POPULATE_UNARY_OP(triton::IntToPtrOp, LLVM::IntToPtrOp)\n+  POPULATE_UNARY_OP(triton::PtrToIntOp, LLVM::PtrToIntOp)\n+  POPULATE_UNARY_OP(math::LogOp, math::LogOp)\n+  POPULATE_UNARY_OP(math::CosOp, math::CosOp)\n+  POPULATE_UNARY_OP(math::SinOp, math::SinOp)\n+  POPULATE_UNARY_OP(math::SqrtOp, math::SqrtOp)\n+  POPULATE_UNARY_OP(math::ExpOp, math::ExpOp)\n+#undef POPULATE_UNARY_OP\n+\n+  patterns.add<ExtElemwiseOpConversion>(typeConverter, benefit);\n \n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n@@ -3714,38 +3789,6 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<ViewLikeOpConversion<triton::ExpandDimsOp>>(typeConverter,\n                                                            benefit);\n   patterns.add<DotOpConversion>(typeConverter, allocation, smem, benefit);\n-\n-  patterns.add<ExtElemwiseOpConversion>(typeConverter, benefit);\n-\n-  patterns.add<ElementwiseOpConversion<math::LogOp, math::LogOp>>(typeConverter,\n-                                                                  benefit);\n-  patterns.add<ElementwiseOpConversion<math::CosOp, math::CosOp>>(typeConverter,\n-                                                                  benefit);\n-  patterns.add<ElementwiseOpConversion<math::SinOp, math::SinOp>>(typeConverter,\n-                                                                  benefit);\n-  patterns.add<ElementwiseOpConversion<math::SqrtOp, math::SqrtOp>>(\n-      typeConverter, benefit);\n-  patterns.add<ElementwiseOpConversion<math::ExpOp, math::ExpOp>>(typeConverter,\n-                                                                  benefit);\n-\n-  patterns.add<\n-      ElementwiseOpConversion<mlir::arith::SIToFPOp, mlir::arith::SIToFPOp>>(\n-      typeConverter, benefit);\n-  patterns.add<\n-      ElementwiseOpConversion<mlir::arith::UIToFPOp, mlir::arith::UIToFPOp>>(\n-      typeConverter, benefit);\n-  patterns.add<\n-      ElementwiseOpConversion<mlir::arith::FPToSIOp, mlir::arith::FPToSIOp>>(\n-      typeConverter, benefit);\n-  patterns\n-      .add<ElementwiseOpConversion<mlir::arith::ExtFOp, mlir::arith::ExtFOp>>(\n-          typeConverter, benefit);\n-  patterns.add<\n-      ElementwiseOpConversion<mlir::arith::TruncFOp, mlir::arith::TruncFOp>>(\n-      typeConverter, benefit);\n-  patterns.add<\n-      ElementwiseOpConversion<triton::gpu::SelectOp, mlir::LLVM::SelectOp>>(\n-      typeConverter, benefit);\n }\n \n class ConvertTritonGPUToLLVM"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -351,6 +351,9 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n   MLIRContext *context = patterns.getContext();\n   patterns.add< // TODO: view should have custom pattern that views the layout\n       TritonGenericPattern<triton::ViewOp>,\n+      TritonGenericPattern<triton::BitcastOp>,\n+      TritonGenericPattern<triton::IntToPtrOp>,\n+      TritonGenericPattern<triton::PtrToIntOp>,\n       TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n       TritonGenericPattern<triton::AddPtrOp>, TritonReducePattern,\n       TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -150,9 +150,9 @@ void LoadOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n                    ::mlir::Value ptr, ::mlir::Value mask, ::mlir::Value other,\n                    ::mlir::triton::CacheModifier cache,\n                    ::mlir::triton::EvictionPolicy evict, bool isVolatile) {\n-  TensorType ptrType = ptr.getType().dyn_cast<TensorType>();\n+  TensorType ptrType = ptr.getType().cast<TensorType>();\n   Type elementType =\n-      ptrType.getElementType().dyn_cast<PointerType>().getPointeeType();\n+      ptrType.getElementType().cast<PointerType>().getPointeeType();\n   auto shape = ptrType.getShape();\n   Type resultType = RankedTensorType::get(shape, elementType);\n   state.addOperands(ptr);"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 28, "deletions": 6, "changes": 34, "file_content_changes": "@@ -441,11 +441,22 @@ void init_triton_ir(py::module &&m) {\n                  loc, self.getF32FloatAttr(v));\n            })\n       .def(\"get_null_value\",\n-           [](mlir::OpBuilder &self, mlir::Type &type) -> mlir::Value {\n+           [](mlir::OpBuilder &self, mlir::Type type) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             if (type.isa<mlir::FloatType>())\n-               return self.create<mlir::arith::ConstantOp>(\n-                   loc, self.getF32FloatAttr(0.0));\n+             if (auto floatTy = type.dyn_cast<mlir::FloatType>())\n+               return self.create<mlir::arith::ConstantFloatOp>(\n+                   loc, mlir::APFloat(floatTy.getFloatSemantics(), 0), floatTy);\n+             else if (auto intTy = type.dyn_cast<mlir::IntegerType>())\n+               return self.create<mlir::arith::ConstantIntOp>(loc, 0, intTy);\n+             else\n+               throw std::runtime_error(\"Not implemented\");\n+           })\n+      .def(\"get_all_ones_value\",\n+           [](mlir::OpBuilder &self, mlir::Type type) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             uint64_t val = 0xFFFFFFFFFFFFFFFF;\n+             if (auto intTy = type.dyn_cast<mlir::IntegerType>())\n+               return self.create<mlir::arith::ConstantIntOp>(loc, val, intTy);\n              else\n                throw std::runtime_error(\"Not implemented\");\n            })\n@@ -602,7 +613,7 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::OpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::BitcastOp>(loc, dstType, src);\n+             return self.create<mlir::triton::BitcastOp>(loc, dstType, src);\n            })\n       // .def(\"create_cast\", &ir::builder::create_cast)\n       // .def(\"create_ptr_to_int\", &ir::builder::create_ptr_to_int)\n@@ -1143,6 +1154,18 @@ void init_triton_ir(py::module &&m) {\n              return self.create<mlir::triton::ReduceOp>(loc, resType, redOp,\n                                                         operand, axis);\n            })\n+      .def(\"create_ptr_to_int\",\n+           [](mlir::OpBuilder &self, mlir::Value &val,\n+              mlir::Type &type) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<mlir::triton::PtrToIntOp>(loc, type, val);\n+           })\n+      .def(\"create_int_to_ptr\",\n+           [](mlir::OpBuilder &self, mlir::Value &val,\n+              mlir::Type &type) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<mlir::triton::IntToPtrOp>(loc, type, val);\n+           })\n       .def(\"create_select\",\n            [](mlir::OpBuilder &self, mlir::Value &condition,\n               mlir::Value &trueValue, mlir::Value &falseValue) -> mlir::Value {\n@@ -1231,7 +1254,6 @@ void init_triton_ir(py::module &&m) {\n }\n \n void init_triton_translation(py::module &m) {\n-\n   using ret = py::return_value_policy;\n \n   m.def(\"get_shared_memory_size\", [](mlir::ModuleOp module) {"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 141, "deletions": 137, "changes": 278, "file_content_changes": "@@ -281,141 +281,142 @@ def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n         _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n \n \n-# @pytest.mark.parametrize(\"dtype_x, dtype_y\",\n-#                          [(dtype_x, dtype_y) for dtype_x in int_dtypes for dtype_y in int_dtypes] +\n-#                          [(dtype_x, dtype_y) for dtype_x in uint_dtypes for dtype_y in uint_dtypes]\n-#                          )\n-# def test_floordiv(dtype_x, dtype_y, device='cuda'):\n-#     # Triton has IEEE, not numpy/torch, semantics for %, and those carry\n-#     # through to //, so we have to use a nonstandard expression to get a\n-#     # reference result for //.\n-#     expr = 'x // y'\n-#     numpy_expr = '((x - np.fmod(x, y)) / y)'\n-#     _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n+@pytest.mark.parametrize(\"dtype_x, dtype_y\",\n+                         [(dtype_x, dtype_y) for dtype_x in int_dtypes for dtype_y in int_dtypes] +\n+                         [(dtype_x, dtype_y) for dtype_x in uint_dtypes for dtype_y in uint_dtypes]\n+                         )\n+def test_floordiv(dtype_x, dtype_y, device='cuda'):\n+    # Triton has IEEE, not numpy/torch, semantics for %, and those carry\n+    # through to //, so we have to use a nonstandard expression to get a\n+    # reference result for //.\n+    expr = 'x // y'\n+    numpy_expr = '((x - np.fmod(x, y)) / y)'\n+    _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n \n \n-# # ---------------\n-# # test bitwise ops\n-# # ---------------\n-# @pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n-#     (dtype_x, dtype_y, op)\n-#     for op in ['&', '|', '^']\n-#     for dtype_x in dtypes + dtypes_with_bfloat16\n-#     for dtype_y in dtypes + dtypes_with_bfloat16\n-# ])\n-# def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n-#     expr = f'x {op} y'\n-#     if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n-#         numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n-#     elif (dtype_y in uint_dtypes and dtype_x in int_dtypes and _bitwidth(dtype_y) >= _bitwidth(dtype_x)):\n-#         numpy_expr = f'x.astype(np.{dtype_y}) {op} y.astype(np.{dtype_y})'\n-#     else:\n-#         numpy_expr = None\n-#     if 'float' in dtype_x + dtype_y:\n-#         with pytest.raises(triton.CompilationError) as exc_info:\n-#             _test_binary(dtype_x, dtype_y, expr, numpy_expr='np.array([])', device=device)\n-#         # The CompilationError must have been caused by a C++ exception with this text.\n-#         assert re.match('invalid operands of type', str(exc_info.value.__cause__))\n-#     else:\n-#         _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n-\n+# ---------------\n+# test bitwise ops\n+# ---------------\n+@pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n+    (dtype_x, dtype_y, op)\n+    for op in ['&', '|', '^']\n+    for dtype_x in dtypes + dtypes_with_bfloat16\n+    for dtype_y in dtypes + dtypes_with_bfloat16\n+])\n+def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n+    expr = f'x {op} y'\n+    if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n+        numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n+    elif (dtype_y in uint_dtypes and dtype_x in int_dtypes and _bitwidth(dtype_y) >= _bitwidth(dtype_x)):\n+        numpy_expr = f'x.astype(np.{dtype_y}) {op} y.astype(np.{dtype_y})'\n+    else:\n+        numpy_expr = None\n+    if 'float' in dtype_x + dtype_y:\n+        with pytest.raises(triton.CompilationError) as exc_info:\n+            _test_binary(dtype_x, dtype_y, expr, numpy_expr='np.array([])', device=device)\n+        # The CompilationError must have been caused by a C++ exception with this text.\n+        assert re.match('invalid operands of type', str(exc_info.value.__cause__))\n+    else:\n+        _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n \n-# @pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n-#     (dtype_x, dtype_y, op)\n-#     for op in ['<<', '>>']\n-#     for dtype_x in int_dtypes + uint_dtypes\n-#     for dtype_y in int_dtypes + uint_dtypes\n-# ])\n-# def test_shift_op(dtype_x, dtype_y, op, device='cuda'):\n-#     expr = f'x {op} y'\n-#     bw = max(_bitwidth(dtype_x), _bitwidth(dtype_y))\n-#     dtype_z = f'uint{bw}'\n-#     numpy_expr = f'x.astype(np.{dtype_z}) {op} y.astype(np.{dtype_z})'\n-#     _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device, y_low=0, y_high=65)\n \n+@pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n+    (dtype_x, dtype_y, op)\n+    for op in ['<<', '>>']\n+    for dtype_x in int_dtypes + uint_dtypes\n+    for dtype_y in int_dtypes + uint_dtypes\n+])\n+def test_shift_op(dtype_x, dtype_y, op, device='cuda'):\n+    expr = f'x {op} y'\n+    bw = max(_bitwidth(dtype_x), _bitwidth(dtype_y))\n+    dtype_z = f'uint{bw}'\n+    numpy_expr = f'x.astype(np.{dtype_z}) {op} y.astype(np.{dtype_z})'\n+    _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device, y_low=0, y_high=65)\n \n-# # ---------------\n-# # test compare ops\n-# # ---------------\n-# ops = ['==', '!=', '>', '<', '>=', '<=']\n-\n-\n-# @pytest.mark.parametrize(\"dtype_x, dtype_y, op, mode_x, mode_y\",\n-#                          # real\n-#                          [\n-#                              (dtype_x, dtype_y, op, 'real', 'real')\n-#                              for op in ops\n-#                              for dtype_x in dtypes\n-#                              for dtype_y in dtypes\n-#                          ] +\n-#                          # NaNs\n-#                          [('float32', 'float32', op, mode_x, mode_y)\n-#                              for op in ops\n-#                              for mode_x, mode_y in [('nan', 'real'),\n-#                                                     ('real', 'nan'),\n-#                                                     ('nan', 'nan')]\n-\n-#                           ])\n-# def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n-#     expr = f'x {op} y'\n-#     if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n-#         numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n-#     elif (dtype_y in uint_dtypes and dtype_x in int_dtypes and _bitwidth(dtype_y) >= _bitwidth(dtype_x)):\n-#         numpy_expr = f'x.astype(np.{dtype_y}) {op} y.astype(np.{dtype_y})'\n-#     else:\n-#         numpy_expr = None\n-#     _test_binary(dtype_x, dtype_y, expr, numpy_expr, mode_x=mode_x, mode_y=mode_y, device=device)\n \n+# ---------------\n+# test compare ops\n+# ---------------\n+ops = ['==', '!=', '>', '<', '>=', '<=']\n+\n+\n+@pytest.mark.parametrize(\"dtype_x, dtype_y, op, mode_x, mode_y\",\n+                         # real\n+                         [\n+                             (dtype_x, dtype_y, op, 'real', 'real')\n+                             for op in ops\n+                             for dtype_x in dtypes\n+                             for dtype_y in dtypes\n+                         ] +\n+                         # NaNs\n+                         [('float32', 'float32', op, mode_x, mode_y)\n+                             for op in ops\n+                             for mode_x, mode_y in [('nan', 'real'),\n+                                                    ('real', 'nan'),\n+                                                    ('nan', 'nan')]\n+\n+                          ])\n+def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n+    expr = f'x {op} y'\n+    if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n+        numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n+    elif (dtype_y in uint_dtypes and dtype_x in int_dtypes and _bitwidth(dtype_y) >= _bitwidth(dtype_x)):\n+        numpy_expr = f'x.astype(np.{dtype_y}) {op} y.astype(np.{dtype_y})'\n+    else:\n+        numpy_expr = None\n+    _test_binary(dtype_x, dtype_y, expr, numpy_expr, mode_x=mode_x, mode_y=mode_y, device=device)\n \n-# # ---------------\n-# # test where\n-# # ---------------\n-# @pytest.mark.parametrize(\"dtype\", dtypes_with_bfloat16 + [\"*int32\"])\n-# def test_where(dtype):\n-#     select_ptrs = False\n-#     if dtype == \"*int32\":\n-#         dtype = \"int64\"\n-#         select_ptrs = True\n-#     check_type_supported(dtype)\n \n-#     @triton.jit\n-#     def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n-#                      BLOCK_SIZE: tl.constexpr,\n-#                      TEST_POINTERS: tl.constexpr):\n-#         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n-#         mask = offsets < n_elements\n-#         decide = tl.load(cond_ptr + offsets, mask=mask)\n-#         if TEST_POINTERS:\n-#             a = tl.load(a_ptr + offsets, mask=mask).to(tl.pi32_t)\n-#             b = tl.load(b_ptr + offsets, mask=mask).to(tl.pi32_t)\n-#         else:\n-#             a = tl.load(a_ptr + offsets, mask=mask)\n-#             b = tl.load(b_ptr + offsets, mask=mask)\n-#         output = tl.where(decide, a, b)\n-#         tl.store(output_ptr + offsets, output, mask=mask)\n+# ---------------\n+# test where\n+# ---------------\n+@pytest.mark.parametrize(\"dtype\", dtypes_with_bfloat16 + [\"*int32\"])\n+def test_where(dtype):\n+    select_ptrs = False\n+    if dtype == \"*int32\":\n+        dtype = \"int64\"\n+        select_ptrs = True\n+    check_type_supported(dtype)\n \n-#     SIZE = 1_000\n-#     rs = RandomState(17)\n-#     cond = numpy_random(SIZE, 'bool', rs)\n-#     x = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n-#     y = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n-#     z = np.where(cond, x, y)\n+    @triton.jit\n+    def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n+                     BLOCK_SIZE: tl.constexpr,\n+                     TEST_POINTERS: tl.constexpr):\n+        offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+        mask = offsets < n_elements\n+        decide = tl.load(cond_ptr + offsets, mask=mask)\n+        if TEST_POINTERS:\n+            a = tl.load(a_ptr + offsets, mask=mask).to(tl.pi32_t)\n+            b = tl.load(b_ptr + offsets, mask=mask).to(tl.pi32_t)\n+        else:\n+            a = tl.load(a_ptr + offsets, mask=mask)\n+            b = tl.load(b_ptr + offsets, mask=mask)\n+        output = tl.where(decide, a, b)\n+        tl.store(output_ptr + offsets, output, mask=mask)\n+\n+    SIZE = 1_000\n+    rs = RandomState(17)\n+    cond = numpy_random(SIZE, 'bool', rs)\n+    x = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n+    y = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n+    z = np.where(cond, x, y)\n \n-#     cond_tri = to_triton(cond, device='cuda')\n-#     x_tri = to_triton(x, device='cuda', dst_type=dtype)\n-#     y_tri = to_triton(y, device='cuda', dst_type=dtype)\n-#     z_tri = to_triton(np.empty(SIZE, dtype=z.dtype), device='cuda', dst_type=dtype)\n+    cond_tri = to_triton(cond, device='cuda')\n+    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n+    y_tri = to_triton(y, device='cuda', dst_type=dtype)\n+    z_tri = to_triton(np.empty(SIZE, dtype=z.dtype), device='cuda', dst_type=dtype)\n \n-#     grid = lambda meta: (triton.cdiv(SIZE, meta['BLOCK_SIZE']),)\n-#     where_kernel[grid](cond_tri, x_tri, y_tri, z_tri, SIZE, BLOCK_SIZE=1024, TEST_POINTERS=select_ptrs)\n-#     assert (z == to_numpy(z_tri)).all()\n+    grid = lambda meta: (triton.cdiv(SIZE, meta['BLOCK_SIZE']),)\n+    where_kernel[grid](cond_tri, x_tri, y_tri, z_tri, SIZE, BLOCK_SIZE=1024, TEST_POINTERS=select_ptrs)\n+    assert (z == to_numpy(z_tri)).all()\n \n \n+# TODO: wrong result\n # def test_where_broadcast():\n #     @triton.jit\n #     def where_kernel(cond_ptr, a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n-#         xoffsets = tl.reshape(tl.arange(0, BLOCK_SIZE), [BLOCK_SIZE, 1])\n-#         yoffsets = tl.reshape(tl.arange(0, BLOCK_SIZE), [1, BLOCK_SIZE])\n+#         xoffsets = tl.arange(0, BLOCK_SIZE)[:, None]\n+#         yoffsets = tl.arange(0, BLOCK_SIZE)[None, :]\n \n #         mask = tl.load(cond_ptr + yoffsets)\n #         vals = tl.load(a_ptr + yoffsets + BLOCK_SIZE * xoffsets)\n@@ -424,8 +425,8 @@ def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n \n #     @triton.jit\n #     def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n-#         xoffsets = tl.reshape(tl.arange(0, BLOCK_SIZE), [BLOCK_SIZE, 1])\n-#         yoffsets = tl.reshape(tl.arange(0, BLOCK_SIZE), [1, BLOCK_SIZE])\n+#         xoffsets = tl.arange(0, BLOCK_SIZE)[:, None]\n+#         yoffsets = tl.arange(0, BLOCK_SIZE)[None, :]\n #         mask = 0\n #         vals = tl.load(a_ptr + yoffsets + BLOCK_SIZE * xoffsets)\n #         res = tl.where(mask, vals, 0.)\n@@ -451,17 +452,19 @@ def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n # # ---------------\n \n \n-# @pytest.mark.parametrize(\"dtype_x, expr\", [\n-#     (dtype_x, ' -x') for dtype_x in dtypes_with_bfloat16\n-# ] + [\n-#     (dtype_x, ' ~x') for dtype_x in int_dtypes\n-# ])\n-# def test_unary_op(dtype_x, expr, device='cuda'):\n-#     _test_unary(dtype_x, expr, device=device)\n+@pytest.mark.parametrize(\"dtype_x, expr\", [\n+    (dtype_x, ' -x') for dtype_x in dtypes_with_bfloat16\n+] + [\n+    (dtype_x, ' ~x') for dtype_x in int_dtypes\n+])\n+def test_unary_op(dtype_x, expr, device='cuda'):\n+    _test_unary(dtype_x, expr, device=device)\n \n # # ----------------\n # # test math ops\n # # ----------------\n+\n+# TODO: Math module\n # # @pytest.mark.parametrize(\"expr\", [\n # #     'exp', 'log', 'cos', 'sin'\n # # ])\n@@ -479,17 +482,18 @@ def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n # # ----------------\n \n \n-# def make_ptr_str(name, shape):\n-#     rank = len(shape)\n-#     offsets = []\n-#     stride = 1\n-#     for i in reversed(range(rank)):\n-#         idx = ', '.join([':' if ii == i else 'None' for ii in range(rank)])\n-#         offsets += [f'tl.arange(0, {shape[i]})[{idx}]*{stride}']\n-#         stride *= shape[i]\n-#     return f\"{name} + {' + '.join(offsets)}\"\n+def make_ptr_str(name, shape):\n+    rank = len(shape)\n+    offsets = []\n+    stride = 1\n+    for i in reversed(range(rank)):\n+        idx = ', '.join([':' if ii == i else 'None' for ii in range(rank)])\n+        offsets += [f'tl.arange(0, {shape[i]})[{idx}]*{stride}']\n+        stride *= shape[i]\n+    return f\"{name} + {' + '.join(offsets)}\"\n \n \n+# TODO: handle `%4 = triton_gpu.convert_layout %3 : (tensor<32xi32, #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>``\n # @pytest.mark.parametrize(\"expr, dtype_str\", [\n #     (f'x[{s}]', d)\n #     for s in ['None, :', ':, None', 'None, :, :', ':, :, None']"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 29, "deletions": 2, "changes": 31, "file_content_changes": "@@ -83,6 +83,23 @@ def matmul_kernel(\n # TODO: DotConversion in TritonGPUToLLVM cannot support non-splat C for the moment\n \n \n+def get_variant_golden(a, b):\n+    SIZE_M = a.shape[0]\n+    SIZE_K = a.shape[1]\n+    SIZE_N = b.shape[1]\n+    assert a.shape[1] == b.shape[0]\n+    zero_M_K = torch.zeros((SIZE_M, SIZE_K)).cuda()\n+    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K)).cuda()\n+    zero_K_N = torch.zeros((SIZE_K, SIZE_N)).cuda()\n+    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N)).cuda()\n+    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n+    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n+    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n+    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n+    c_padded = torch.matmul(a_padded, b_padded)\n+    return c_padded[:SIZE_M, :SIZE_N]\n+\n+\n @pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K', [\n     # Non-forloop\n     [64, 32, 64, 4, 64, 32, 64],\n@@ -94,8 +111,8 @@ def matmul_kernel(\n     [32, 64, 128, 4, 32, 64, 32],\n     [32, 128, 256, 4, 32, 128, 64],\n     [64, 128, 64, 4, 64, 128, 32],\n-    [128, 128, 64, 4, 128, 128, 32],\n     [64, 64, 128, 4, 64, 64, 32],\n+    [128, 128, 64, 4, 128, 128, 32],\n     [128, 128, 128, 4, 128, 128, 32],\n     [128, 128, 256, 4, 128, 128, 64],\n     [128, 256, 128, 4, 128, 256, 32],\n@@ -115,5 +132,15 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n                         BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n                         num_warps=NUM_WARPS)\n     golden = torch.matmul(a, b)\n+\n+    # It's not easy to get a proper error threshold in different size\n+    # Here the gemm calculation is padded to a different size in order to get\n+    # a variant version of the golden result. And the error between golden and\n+    # golden_variant provide reference on selecting the proper rtol / atol.\n+    golden_variant = get_variant_golden(a, b)\n+    golden_diff = golden - golden_variant\n+    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n+    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n+\n     torch.set_printoptions(profile=\"full\")\n-    assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n+    assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -36,7 +36,7 @@ def str_to_ty(name):\n         \"bf16\": triton.language.bfloat16,\n         \"fp32\": triton.language.float32,\n         \"fp64\": triton.language.float64,\n-        \"i1\": triton.language.int8,\n+        \"i1\": triton.language.int1,\n         \"i8\": triton.language.int8,\n         \"i16\": triton.language.int16,\n         \"i32\": triton.language.int32,"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -729,9 +729,10 @@ def cat(input, other, _builder=None):\n \n \n @builtin\n-def reshape(input, shape, _builder=None):\n+def view(input, shape, _builder=None):\n     \"\"\"\n-    Tries to reshape the given tensor to a new shape.\n+    Returns a tensor with the same elements as `input` but a different shape.\n+    The order of the elements may not be preserved.\n \n     :param input: The input tensor.\n     :type input:\n@@ -740,7 +741,7 @@ def reshape(input, shape, _builder=None):\n \n     \"\"\"\n     shape = [x.value for x in shape]\n-    return semantic.reshape(input, shape, _builder)\n+    return semantic.view(input, shape, _builder)\n \n \n # -----------------------\n@@ -1151,7 +1152,7 @@ def ravel(x):\n     :param x: the input tensor\n     :type x: Block\n     \"\"\"\n-    return triton.language.reshape(x, [x.numel])\n+    return triton.language.view(x, [x.numel])\n \n \n @triton.jit"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 12, "deletions": 7, "changes": 19, "file_content_changes": "@@ -344,7 +344,7 @@ def invert(input: tl.tensor,\n     input_sca_ty = input.type.scalar\n     if input_sca_ty.is_ptr() or input_sca_ty.is_floating():\n         raise ValueError(\"wrong type argument to unary invert (\" + input_sca_ty.__repr__() + \")\")\n-    _1 = tl.tensor(ir.constant.get_all_ones_value(input_sca_ty.to_ir(builder)), input_sca_ty)\n+    _1 = tl.tensor(builder.get_all_ones_value(input_sca_ty.to_ir(builder)), input_sca_ty)\n     return xor_(input, _1, builder)\n \n \n@@ -480,11 +480,13 @@ def zeros(shape: List[int], dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n def view(input: tl.tensor,\n          dst_shape: List[int],\n          builder: ir.builder) -> tl.tensor:\n+    # TODO: disable when TritonToTritonGPU handles views properly\n+    assert len(input.shape) == len(dst_shape)\n     numel = 1\n     for s in dst_shape:\n         numel *= s\n     if input.type.numel != numel:\n-        raise ValueError(\"cannot reshape block of different shape\")\n+        raise ValueError(\"cannot view block of different shape\")\n     ret_ty = tl.block_type(input.type.scalar, dst_shape)\n     return tl.tensor(builder.create_view(input.handle, dst_shape), ret_ty)\n \n@@ -515,7 +517,7 @@ def broadcast_impl_shape(input: tl.tensor,\n     for i in range(len(src_shape)):\n         if shape[i] != src_shape[i] and src_shape[i] != 1:\n             raise ValueError(f\"Cannot broadcast, the expanded size of the tensor ({shape[i]})\"\n-                             f\" must match the existing size ({src_shape[1]}) at non-singleton dimension\"\n+                             f\" must match the existing size ({src_shape[i]}) at non-singleton dimension\"\n                              f\" {i}: {src_shape}, {shape}\")\n     ret_ty = tl.block_type(input.type.scalar, shape)\n     return tl.tensor(builder.create_broadcast(input.handle, shape), ret_ty)\n@@ -678,7 +680,7 @@ def cast(input: tl.tensor,\n     if src_sca_ty.is_ptr() and dst_sca_ty.is_int():\n         bitwidth = dst_sca_ty.int_bitwidth\n         if bitwidth == 64:\n-            return tl.tensor(builder.create_cast(ir.PtrToInt, input.handle, dst_ty.to_ir(builder)),\n+            return tl.tensor(builder.create_ptr_to_int(input.handle, dst_ty.to_ir(builder)),\n                              dst_ty)\n         if bitwidth == 1:\n             return not_equal(cast(input, tl.int64, builder),\n@@ -988,18 +990,21 @@ def where(condition: tl.tensor,\n           builder: ir.builder) -> tl.tensor:\n     condition = cast(condition, tl.int1, builder)\n     if condition.type.is_block():\n-        x = broadcast_impl_shape(x, condition.type.get_block_shapes(), builder)\n-        y = broadcast_impl_shape(y, condition.type.get_block_shapes(), builder)\n+        condition, x = broadcast_impl_value(condition, x, builder)\n+        x, y = broadcast_impl_value(x, y, builder)\n+        condition, x = broadcast_impl_value(condition, x, builder)\n \n     x, y = binary_op_type_checking_impl(x, y, builder, True, True)\n+    if not condition.type.is_block():\n+        condition, _ = broadcast_impl_value(condition, x, builder)\n     ret_ty = x.type\n     return tl.tensor(builder.create_select(condition.handle, x.handle, y.handle), ret_ty)\n \n-\n # ===----------------------------------------------------------------------===//\n #                               Reductions\n # ===----------------------------------------------------------------------===\n \n+\n def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n                 FLOAT_OP: ir.REDUCE_OP, INT_OP: ir.REDUCE_OP) -> tl.tensor:\n     scalar_ty = input.type.scalar"}]