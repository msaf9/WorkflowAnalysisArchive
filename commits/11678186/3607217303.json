[{"filename": "include/triton/Analysis/Membar.h", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -29,7 +29,11 @@ class MembarAnalysis {\n   /// The following circumstances are not considered yet:\n   /// - Double buffers\n   /// - N buffers\n-  MembarAnalysis(Allocation *allocation) : allocation(allocation) { run(); }\n+  MembarAnalysis(Allocation *allocation) : allocation(allocation) {}\n+\n+  /// Runs the membar analysis to the given operation, inserts a barrier if\n+  /// necessary.\n+  void run();\n \n private:\n   struct RegionInfo {\n@@ -82,10 +86,6 @@ class MembarAnalysis {\n     }\n   };\n \n-  /// Runs the membar analysis to the given operation, inserts a barrier if\n-  /// necessary.\n-  void run();\n-\n   /// Applies the barrier analysis based on the SCF dialect, in which each\n   /// region has a single basic block only.\n   /// Example:"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 35, "deletions": 14, "changes": 49, "file_content_changes": "@@ -24,21 +24,43 @@ void MembarAnalysis::dfsOperation(Operation *operation,\n     // scf.if only: two regions\n     // scf.for: one region\n     RegionInfo curRegionInfo;\n-    for (auto &region : operation->getRegions()) {\n-      // Copy the parent info as the current info.\n-      RegionInfo regionInfo = *parentRegionInfo;\n-      for (auto &block : region.getBlocks()) {\n-        assert(region.getBlocks().size() == 1 &&\n-               \"Multiple blocks in a region is not supported\");\n-        for (auto &op : block.getOperations()) {\n-          // Traverse the nested operation.\n-          dfsOperation(&op, &regionInfo, builder);\n+    auto traverseRegions = [&]() -> auto{\n+      for (auto &region : operation->getRegions()) {\n+        // Copy the parent info as the current info.\n+        RegionInfo regionInfo = *parentRegionInfo;\n+        for (auto &block : region.getBlocks()) {\n+          assert(region.getBlocks().size() == 1 &&\n+                 \"Multiple blocks in a region is not supported\");\n+          for (auto &op : block.getOperations()) {\n+            // Traverse the nested operation.\n+            dfsOperation(&op, &regionInfo, builder);\n+          }\n         }\n+        curRegionInfo.join(regionInfo);\n       }\n-      curRegionInfo.join(regionInfo);\n+      // Set the parent region info as the union of the nested region info.\n+      *parentRegionInfo = curRegionInfo;\n+    };\n+\n+    traverseRegions();\n+    if (isa<scf::ForOp>(operation)) {\n+      // scf.for can have two possible inputs: the init value and the\n+      // previous iteration's result. Although we've applied alias analysis,\n+      // there could be unsynced memory accesses on reused memories.\n+      // For example, consider the following code:\n+      // %1 = convert_layout %0: blocked -> shared\n+      // ...\n+      // gpu.barrier\n+      // ...\n+      // %5 = convert_layout %4 : shared -> dot\n+      // %6 = tt.dot %2, %5\n+      // scf.yield\n+      //\n+      // Though %5 could be released before scf.yield, it may shared the same\n+      // memory with %1. So we actually have to insert a barrier before %1 to\n+      // make sure the memory is synced.\n+      traverseRegions();\n     }\n-    // Set the parent region info as the union of the nested region info.\n-    *parentRegionInfo = curRegionInfo;\n   }\n }\n \n@@ -49,8 +71,7 @@ void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n     // Do not insert barriers before control flow operations and\n     // alloc/extract/insert\n     // alloc is an allocation op without memory write.\n-    // In contrast, arith.constant is an allocation op with memory write.\n-    // FIXME(Keren): extract is always alias for now\n+    // FIXME(Keren): extract_slice is always alias for now\n     return;\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 5, "changes": 7, "file_content_changes": "@@ -3151,10 +3151,6 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n     ConversionPatternRewriter &rewriter) const {\n   auto loc = op.getLoc();\n \n-  // TODO[Keren]: A temporary workaround for an issue from membar pass.\n-  // https://triton-lang.slack.com/archives/C042VBSQWNS/p1669796615860699?thread_ts=1669779203.526739&cid=C042VBSQWNS\n-  barrier();\n-\n   Value src = op.src();\n   Value dst = op.result();\n   auto srcTy = src.getType().cast<RankedTensorType>();\n@@ -4716,7 +4712,8 @@ class ConvertTritonGPUToLLVM\n     decomposeInsertSliceAsyncOp(mod);\n \n     Allocation allocation(mod);\n-    MembarAnalysis membar(&allocation);\n+    MembarAnalysis membarPass(&allocation);\n+    membarPass.run();\n \n     RewritePatternSet scf_patterns(context);\n     mlir::populateLoopToStdConversionPatterns(scf_patterns);"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 46, "deletions": 1, "changes": 47, "file_content_changes": "@@ -456,10 +456,55 @@ struct SCFYieldPattern : public OpConversionPattern<scf::YieldOp> {\n   }\n };\n \n+// This is borrowed from ConvertFIfOpTypes in\n+//    SCF/Transforms/StructuralTypeConversions.cpp\n+class SCFIfPattern : public OpConversionPattern<scf::IfOp> {\n+public:\n+  using OpConversionPattern<scf::IfOp>::OpConversionPattern;\n+  LogicalResult\n+  matchAndRewrite(scf::IfOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    // TODO: Generalize this to any type conversion, not just 1:1.\n+    //\n+    // We need to implement something more sophisticated here that tracks which\n+    // types convert to which other types and does the appropriate\n+    // materialization logic.\n+    // For example, it's possible that one result type converts to 0 types and\n+    // another to 2 types, so newResultTypes would at least be the right size to\n+    // not crash in the llvm::zip call below, but then we would set the the\n+    // wrong type on the SSA values! These edge cases are also why we cannot\n+    // safely use the TypeConverter::convertTypes helper here.\n+    SmallVector<Type> newResultTypes;\n+    for (auto type : op.getResultTypes()) {\n+      Type newType = typeConverter->convertType(type);\n+      if (!newType)\n+        return rewriter.notifyMatchFailure(op, \"not a 1:1 type conversion\");\n+      newResultTypes.push_back(newType);\n+    }\n+\n+    // See comments in the ForOp pattern for why we clone without regions and\n+    // then inline.\n+    scf::IfOp newOp =\n+        cast<scf::IfOp>(rewriter.cloneWithoutRegions(*op.getOperation()));\n+    rewriter.inlineRegionBefore(op.getThenRegion(), newOp.getThenRegion(),\n+                                newOp.getThenRegion().end());\n+    rewriter.inlineRegionBefore(op.getElseRegion(), newOp.getElseRegion(),\n+                                newOp.getElseRegion().end());\n+\n+    // Update the operands and types.\n+    newOp->setOperands(adaptor.getOperands());\n+    for (auto t : llvm::zip(newOp.getResults(), newResultTypes))\n+      std::get<0>(t).setType(std::get<1>(t));\n+    rewriter.replaceOp(op, newOp.getResults());\n+    return success();\n+  }\n+};\n+\n void populateSCFPatterns(TritonGPUTypeConverter &typeConverter,\n                          RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n-  patterns.add<SCFYieldPattern, SCFForPattern>(typeConverter, context);\n+  patterns.add<SCFYieldPattern, SCFForPattern, SCFIfPattern>(typeConverter,\n+                                                             context);\n }\n \n class ConvertTritonToTritonGPU"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 45, "deletions": 41, "changes": 86, "file_content_changes": "@@ -50,22 +50,25 @@ class DecomposeDotOperand : public mlir::RewritePattern {\n     auto dstType = convert.getType().cast<RankedTensorType>();\n     if (srcType.getEncoding().isa<triton::gpu::BlockedEncodingAttr>() &&\n         dstType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>()) {\n-      auto dstDotOperand = dstType.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>();\n+      auto dstDotOperand =\n+          dstType.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>();\n       auto dstParent = dstDotOperand.getParent();\n-      if(dstDotOperand.getOpIdx()==1 || \n-         !dstParent.isa<triton::gpu::MmaEncodingAttr>())\n+      if (dstDotOperand.getOpIdx() == 1 ||\n+          !dstParent.isa<triton::gpu::MmaEncodingAttr>())\n         return mlir::failure();\n       auto dstParentMma = dstParent.cast<triton::gpu::MmaEncodingAttr>();\n-      if(dstParentMma.getVersion() == 1 ||\n-         dstParentMma.getWarpsPerCTA()[1] > 1)\n+      if (dstParentMma.getVersion() == 1 ||\n+          dstParentMma.getWarpsPerCTA()[1] > 1)\n         return mlir::failure();\n-      SetVector<Operation*> bwdSlices;\n+      SetVector<Operation *> bwdSlices;\n       mlir::getBackwardSlice(convert.getResult(), &bwdSlices);\n-      if(llvm::find_if(bwdSlices, [](Operation *op) { return isa<triton::DotOp>(op); }) == bwdSlices.end())\n+      if (llvm::find_if(bwdSlices, [](Operation *op) {\n+            return isa<triton::DotOp>(op);\n+          }) == bwdSlices.end())\n         return mlir::failure();\n-      \n-      auto tmpType =\n-          RankedTensorType::get(dstType.getShape(), dstType.getElementType(), dstParentMma);\n+\n+      auto tmpType = RankedTensorType::get(\n+          dstType.getShape(), dstType.getElementType(), dstParentMma);\n       auto tmp = rewriter.create<triton::gpu::ConvertLayoutOp>(\n           convert.getLoc(), tmpType, convert.getOperand());\n       auto newConvert = rewriter.create<triton::gpu::ConvertLayoutOp>(\n@@ -601,10 +604,9 @@ mmaVersionToShapePerWarp(int version, const ArrayRef<int64_t> &shape,\n   }\n }\n \n-\n SmallVector<unsigned, 2> warpsPerTileV1(triton::DotOp dotOp,\n-                                         const ArrayRef<int64_t> shape,\n-                                         int numWarps) {\n+                                        const ArrayRef<int64_t> shape,\n+                                        int numWarps) {\n   SmallVector<unsigned, 2> ret = {1, 1};\n   SmallVector<int64_t, 2> shapePerWarp =\n       mmaVersionToShapePerWarp(1, shape, numWarps);\n@@ -624,35 +626,37 @@ SmallVector<unsigned, 2> warpsPerTileV1(triton::DotOp dotOp,\n }\n \n SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n-                                         const ArrayRef<int64_t> shape,\n-                                         int numWarps) {\n-    SetVector<Operation*> slices;\n-    mlir::getForwardSlice(dotOp.getResult(), &slices);\n-    if(llvm::find_if(slices, [](Operation *op) { return isa<triton::DotOp>(op); }) != slices.end())\n-      return {(unsigned)numWarps, 1};\n-    \n-    SmallVector<unsigned, 2> ret = {1, 1};\n-    SmallVector<int64_t, 2> shapePerWarp = {16, 8};\n-    bool changed = false;\n-    // TODO (@daadaada): double-check.\n-    // original logic in\n-    // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L252\n-    // seems buggy for shape = [32, 16] ?\n-    do {\n-      changed = false;\n-      if (ret[0] * ret[1] >= numWarps)\n-        break;\n-      if (shape[0] / shapePerWarp[0] / ret[0] >=\n-          shape[1] / (shapePerWarp[1] * 2) / ret[1]) {\n-        if (ret[0] < shape[0] / shapePerWarp[0]) {\n-          ret[0] *= 2;\n-        } else\n-          ret[1] *= 2;\n-      } else {\n+                                        const ArrayRef<int64_t> shape,\n+                                        int numWarps) {\n+  SetVector<Operation *> slices;\n+  mlir::getForwardSlice(dotOp.getResult(), &slices);\n+  if (llvm::find_if(slices, [](Operation *op) {\n+        return isa<triton::DotOp>(op);\n+      }) != slices.end())\n+    return {(unsigned)numWarps, 1};\n+\n+  SmallVector<unsigned, 2> ret = {1, 1};\n+  SmallVector<int64_t, 2> shapePerWarp = {16, 8};\n+  bool changed = false;\n+  // TODO (@daadaada): double-check.\n+  // original logic in\n+  // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L252\n+  // seems buggy for shape = [32, 16] ?\n+  do {\n+    changed = false;\n+    if (ret[0] * ret[1] >= numWarps)\n+      break;\n+    if (shape[0] / shapePerWarp[0] / ret[0] >=\n+        shape[1] / (shapePerWarp[1] * 2) / ret[1]) {\n+      if (ret[0] < shape[0] / shapePerWarp[0]) {\n+        ret[0] *= 2;\n+      } else\n         ret[1] *= 2;\n-      }\n-    } while (true);\n-    return ret;\n+    } else {\n+      ret[1] *= 2;\n+    }\n+  } while (true);\n+  return ret;\n }\n \n } // namespace"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -130,10 +130,10 @@ LogicalResult Prefetcher::initialize() {\n \n   if (dotsInFor.empty())\n     return failure();\n-  \n+\n   // TODO: segfault (original for still has uses)\n   // when used in flash attention that has 2 dots in the loop\n-  if(dotsInFor.size() > 1)\n+  if (dotsInFor.size() > 1)\n     return failure();\n \n   // returns source of cvt"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 6, "deletions": 7, "changes": 13, "file_content_changes": "@@ -1251,13 +1251,12 @@ void init_triton_ir(py::module &&m) {\n                                        llvm::StringRef(prefix)),\n                  values);\n            })\n-       // Undef\n-          .def(\"create_undef\",\n-               [](mlir::OpBuilder &self, mlir::Type &type) -> mlir::Value {\n-               auto loc = self.getUnknownLoc();\n-               return self.create<::mlir::LLVM::UndefOp>(loc, type);\n-          })    \n-       ;\n+      // Undef\n+      .def(\"create_undef\",\n+           [](mlir::OpBuilder &self, mlir::Type &type) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<::mlir::LLVM::UndefOp>(loc, type);\n+           });\n \n   py::class_<mlir::PassManager>(m, \"pass_manager\")\n       .def(py::init<mlir::MLIRContext *>())"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 20, "deletions": 30, "changes": 50, "file_content_changes": "@@ -677,36 +677,7 @@ def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr)\n     kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n-# def test_atomic_cas():\n-#     # 1. make sure that atomic_cas changes the original value (Lock)\n-#     @triton.jit\n-#     def change_value(Lock):\n-#         tl.atomic_cas(Lock, 0, 1)\n-\n-#     Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n-#     change_value[(1,)](Lock)\n-\n-#     assert (Lock[0] == 1)\n-\n-#     # 2. only one block enters the critical section\n-#     @triton.jit\n-#     def serialized_add(data, Lock):\n-#         ptrs = data + tl.arange(0, 128)\n-#         while tl.atomic_cas(Lock, 0, 1) == 1:\n-#             pass\n-\n-#         tl.store(ptrs, tl.load(ptrs) + 1.0)\n-\n-#         # release lock\n-#         tl.atomic_xchg(Lock, 0)\n-\n-#     Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n-#     data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n-#     ref = torch.full((128,), 64.0)\n-#     serialized_add[(64,)](data, Lock)\n-#     triton.testing.assert_almost_equal(data, ref)\n-\n-def test_simple_atomic_cas():\n+def test_atomic_cas():\n     # 1. make sure that atomic_cas changes the original value (Lock)\n     @triton.jit\n     def change_value(Lock):\n@@ -717,6 +688,25 @@ def change_value(Lock):\n \n     assert (Lock[0] == 1)\n \n+    # 2. only one block enters the critical section\n+    @triton.jit\n+    def serialized_add(data, Lock):\n+        ptrs = data + tl.arange(0, 128)\n+        while tl.atomic_cas(Lock, 0, 1) == 1:\n+            pass\n+\n+        tl.store(ptrs, tl.load(ptrs) + 1.0)\n+\n+        # release lock\n+        tl.atomic_xchg(Lock, 0)\n+\n+    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+    data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n+    ref = torch.full((128,), 64.0)\n+    serialized_add[(64,)](data, Lock)\n+    triton.testing.assert_almost_equal(data, ref)\n+\n+\n # # ---------------\n # # test cast\n # # ---------------"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 11, "deletions": 4, "changes": 15, "file_content_changes": "@@ -359,7 +359,7 @@ def visit_If(self, node):\n             cond = cond.to(triton.language.int1, _builder=self.builder)\n             with enter_sub_region(self) as sr:\n                 liveins, ip_block = sr\n-\n+                liveins_copy = liveins.copy()\n                 then_block = self.builder.create_block()\n                 self.builder.set_insertion_point_to_start(then_block)\n                 self.visit_compound_statement(node.body)\n@@ -394,7 +394,15 @@ def visit_If(self, node):\n                             if then_defs[then_name].type == else_defs[else_name].type:\n                                 names.append(then_name)\n                                 ret_types.append(then_defs[then_name].type)\n-\n+                \n+                # defined in else block but not in then block\n+                # to find in parent scope and yield them\n+                for else_name in else_defs:\n+                    if else_name in liveins and else_name not in then_defs:\n+                        if else_defs[else_name].type == liveins[else_name].type:\n+                            names.append(else_name)\n+                            ret_types.append(else_defs[else_name].type)\n+                            then_defs[else_name] = liveins_copy[else_name]\n                 self.builder.set_insertion_point_to_end(ip_block)\n \n                 if then_defs or node.orelse:  # with else block\n@@ -528,8 +536,7 @@ def visit_While(self, node):\n                                                                 [ty.to_ir(self.builder) for ty in ret_types])\n             loop_block.merge_block_before(after_block)\n             self.builder.set_insertion_point_to_end(after_block)\n-            if len(yields) > 0:\n-                self.builder.create_yield_op([y.handle for y in yields])\n+            self.builder.create_yield_op([y.handle for y in yields])\n \n         # update global uses in while_op\n         for i, name in enumerate(names):"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 46, "deletions": 2, "changes": 48, "file_content_changes": "@@ -261,14 +261,58 @@ func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B :\n   %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n-    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #AL>\n     // CHECK-NEXT: Membar 6\n-    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #AL>\n     scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   // CHECK-NEXT: Membar 9\n   %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n   return\n }\n \n+// Although cst2 is not an argument of scf.yield, its memory is reused by cst1.\n+// So we need a barrier both before and after cst1\n+// CHECK-LABEL: for_reuse\n+func @for_reuse(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK-NEXT: Membar 2\n+  %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    // CHECK-NEXT: Membar 5\n+    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    // CHECK-NEXT: Membar 7\n+    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+  }\n+  // CHECK-NEXT: Membar 10\n+  %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n+  return\n+}\n+\n+\n+// CHECK-LABEL: for_reuse_nested\n+func @for_reuse_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK-NEXT: Membar 2\n+  %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    // CHECK-NEXT: Membar 5\n+    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    %a_shared_next, %b_shared_next, %c_shared_next = scf.for %ivv = %lb to %ub step %step iter_args(%a_shared_nested = %a_shared_init, %b_shared_nested = %b_shared_init, %c_shared_nested = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+      // CHECK-NEXT: Membar 7\n+      %cst2 = tt.cat %a_shared_nested, %b_shared_nested {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+      scf.yield %c_shared_nested, %a_shared_nested, %b_shared_nested : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+    }\n+    scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+  }\n+  // CHECK-NEXT: Membar 11\n+  %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n+  return\n+}\n+\n }"}, {"filename": "test/lib/Analysis/TestMembar.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -26,7 +26,9 @@ struct TestMembarPass\n     auto op_name = SymbolTable::getSymbolName(operation).getValue().str();\n     os << op_name << \"\\n\";\n     Allocation allocation(operation);\n-    MembarAnalysis analysis(&allocation);\n+    MembarAnalysis membarPass(&allocation);\n+    membarPass.run();\n+\n     size_t operationId = 0;\n     operation->walk<WalkOrder::PreOrder>([&](Operation *op) {\n       if (isa<gpu::BarrierOp>(op)) {"}]