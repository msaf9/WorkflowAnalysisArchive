[{"filename": "include/triton/Dialect/Triton/IR/TritonDialect.td", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -37,6 +37,7 @@ def Triton_Dialect : Dialect {\n \n   let hasConstantMaterializer = 1;\n   let useDefaultTypePrinterParser = 1;\n+  let usePropertiesForAttributes = 1;\n }\n \n include \"triton/Dialect/Triton/IR/TritonTypes.td\""}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -446,10 +446,11 @@ mlir::LogicalResult mlir::triton::ReduceOp::inferReturnTypes(\n     MLIRContext *context, std::optional<Location> location, ValueRange operands,\n     DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,\n     SmallVectorImpl<Type> &inferredReturnTypes) {\n+  Properties *prop = properties.as<Properties *>();\n+  int axis = prop->axis.getInt();\n   for (auto arg : operands) {\n     auto argTy = arg.getType().cast<RankedTensorType>();\n     auto retEltTy = argTy.getElementType();\n-    int axis = attributes.get(\"axis\").cast<IntegerAttr>().getInt();\n     if (inferReduceReturnShape(argTy, retEltTy, axis, inferredReturnTypes)\n             .failed()) {\n       return failure();\n@@ -557,7 +558,8 @@ mlir::LogicalResult mlir::triton::ExpandDimsOp::inferReturnTypes(\n   auto arg = operands[0];\n   auto argTy = arg.getType().cast<RankedTensorType>();\n   auto retShape = argTy.getShape().vec();\n-  int axis = attributes.get(\"axis\").cast<IntegerAttr>().getInt();\n+  Properties *prop = properties.as<Properties *>();\n+  int axis = prop->axis.getInt();\n   retShape.insert(retShape.begin() + axis, 1);\n   // infer encoding\n   Attribute argEncoding = argTy.getEncoding();\n@@ -740,7 +742,7 @@ void triton::FuncOp::print(OpAsmPrinter &printer) {\n LogicalResult\n triton::CallOp::verifySymbolUses(mlir::SymbolTableCollection &symbolTable) {\n   // Check that the callee attribute was specified.\n-  auto fnAttr = (*this)->getAttrOfType<FlatSymbolRefAttr>(\"callee\");\n+  auto fnAttr = (*this).getProperties().callee;\n   if (!fnAttr)\n     return emitOpError(\"requires a 'callee' symbol reference attribute\");\n   FuncOp fn = symbolTable.lookupNearestSymbolFrom<FuncOp>(*this, fnAttr);"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 30, "deletions": 6, "changes": 36, "file_content_changes": "@@ -76,40 +76,64 @@ tt.func @load_store_ops_scalar(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   tt.return\n }\n \n+// CHECK-LABEL: reduce_ops_infer\n tt.func @reduce_ops_infer(%ptr: !tt.ptr<f32>, %v : tensor<1x2x4xf32>) {\n   // Test if reduce ops infer types correctly\n \n-  // CHECK: }) {axis = 0 : i32} : (tensor<1x2x4xf32>) -> tensor<2x4xf32>\n+  // CHECK: tt.reduce\n+  // CHECK-SAME: axis = 0\n+  // CHECK: tt.reduce.return\n+  // CHECK-NEXT: (tensor<1x2x4xf32>) -> tensor<2x4xf32>\n   %a = \"tt.reduce\" (%v) ({\n   ^bb0(%arg0: f32, %arg1: f32):\n     %add = arith.addf %arg0, %arg1 : f32\n     tt.reduce.return %add : f32\n   }) {axis = 0 : i32}  : (tensor<1x2x4xf32>) -> tensor<2x4xf32>\n-  // CHECK: }) {axis = 1 : i32}  : (tensor<1x2x4xf32>) -> tensor<1x4xf32>\n+\n+  // CHECK: tt.reduce\n+  // CHECK-SAME: axis = 1\n+  // CHECK: tt.reduce.return\n+  // CHECK-NEXT: (tensor<1x2x4xf32>) -> tensor<1x4xf32>\n   %b = \"tt.reduce\" (%v) ({\n   ^bb0(%arg0: f32, %arg1: f32):\n     %add = arith.addf %arg0, %arg1 : f32\n     tt.reduce.return %add : f32\n   }) {axis = 1 : i32}  : (tensor<1x2x4xf32>) -> tensor<1x4xf32>\n-  // CHECK: }) {axis = 2 : i32}  : (tensor<1x2x4xf32>) -> tensor<1x2xf32>\n+\n+  // CHECK: tt.reduce\n+  // CHECK-SAME: axis = 2\n+  // CHECK: tt.reduce.return\n+  // CHECK-NEXT: (tensor<1x2x4xf32>) -> tensor<1x2xf32>\n   %c = \"tt.reduce\" (%v) ({\n   ^bb0(%arg0: f32, %arg1: f32):\n     %add = arith.addf %arg0, %arg1 : f32\n     tt.reduce.return %add : f32\n   }) {axis = 2 : i32}  : (tensor<1x2x4xf32>) -> tensor<1x2xf32>\n-  // CHECK: }) {axis = 1 : i32}  : (tensor<1x4xf32>) -> tensor<1xf32>\n+\n+  // CHECK: tt.reduce\n+  // CHECK-SAME: axis = 1\n+  // CHECK: tt.reduce.return\n+  // CHECK-NEXT: (tensor<1x4xf32>) -> tensor<1xf32>\n   %e = \"tt.reduce\" (%b) ({\n   ^bb0(%arg0: f32, %arg1: f32):\n     %add = arith.addf %arg0, %arg1 : f32\n     tt.reduce.return %add : f32\n   }) {axis = 1 : i32}  : (tensor<1x4xf32>) -> tensor<1xf32>\n-  // CHECK: }) {axis = 0 : i32}  : (tensor<2x4xf32>) -> tensor<4xf32>\n+\n+  // CHECK: tt.reduce\n+  // CHECK-SAME: axis = 0\n+  // CHECK: tt.reduce.return\n+  // CHECK-NEXT: (tensor<2x4xf32>) -> tensor<4xf32>\n   %f = \"tt.reduce\" (%a) ({\n   ^bb0(%arg0: f32, %arg1: f32):\n     %add = arith.addf %arg0, %arg1 : f32\n     tt.reduce.return %add : f32\n   }) {axis = 0 : i32}  : (tensor<2x4xf32>) -> tensor<4xf32>\n-  // CHECK: }) {axis = 0 : i32}  : (tensor<4xf32>) -> f32\n+\n+  // CHECK: tt.reduce\n+  // CHECK-SAME: axis = 0\n+  // CHECK: tt.reduce.return\n+  // CHECK-NEXT: (tensor<4xf32>) -> f32\n   %g = \"tt.reduce\" (%f) ({\n   ^bb0(%arg0: f32, %arg1: f32):\n     %add = arith.addf %arg0, %arg1 : f32"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -1054,7 +1054,9 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n // Check if the SimplifyReduceCvt handles convert_layout lifted from the for loop.\n // CHECK-LABEL: reduce_cvt2\n // Match the reduction\n-// CHECK: }) {axis = 1 : i32} : (tensor<1x256xf32, #blocked>) -> tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+// CHECK: tt.reduce\n+// CHECK-SAME: axis = 1\n+// CHECK: (tensor<1x256xf32, #blocked>) -> tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n // CHECK-NEXT: triton_gpu.convert_layout\n // CHECK-NOT: triton_gpu.convert_layout\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>"}]