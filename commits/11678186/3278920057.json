[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -330,7 +330,7 @@ def SliceEncodingAttr : DistributedEncoding<\"SliceEncoding\"> {\n // This is copied from a WIP PR.\n // TODO[Superjomn]: Remove it in the final PR.\n def DotOperandEncodingAttr : DistributedEncoding<\"DotOperandEncoding\"> {\n-  let mnemonic = \"dot_op\";\n+  let mnemonic = \"dot_operand\";\n \n   let description = [{}];\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 25, "deletions": 6, "changes": 31, "file_content_changes": "@@ -3094,12 +3094,31 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adaptor,\n   MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n                                      rewriter, getTypeConverter(), loc);\n \n-  auto A = mmaHelper.loadA(op.a(), adaptor.a());\n-  auto B = mmaHelper.loadB(op.b(), adaptor.b());\n-  auto C = mmaHelper.loadC(op.c());\n-\n-  return mmaHelper.convertDot(op.a(), op.b(), op.c(), op.d(), A, B, C, op,\n-                              adaptor);\n+  Value A = op.a();\n+  Value B = op.b();\n+  auto ATensorTy = A.getType().cast<RankedTensorType>();\n+  auto BTensorTy = B.getType().cast<RankedTensorType>();\n+\n+  Value loadedA, loadedB, loadedC;\n+  // We support two kinds of operand layouts: 1. both $a, $b are dot_operand\n+  // layout, 2. both of them are shared layout.\n+  if (ATensorTy.getEncoding().isa<DotOperandEncodingAttr>()) {\n+    assert(BTensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n+           \"Both $a and %b should be DotOperand layout.\");\n+    loadedA = adaptor.a();\n+    loadedB = adaptor.b();\n+  } else {\n+    loadedA = mmaHelper.loadA(op.a(), adaptor.a());\n+    loadedB = mmaHelper.loadB(op.b(), adaptor.b());\n+  }\n+\n+  // TODO[Superjomn]: Process C as a mma layout.\n+  // Currently, C is simply treated as a Splat Op, and the data layout is not\n+  // mattered.\n+  loadedC = mmaHelper.loadC(op.c());\n+\n+  return mmaHelper.convertDot(op.a(), op.b(), op.c(), op.d(), loadedA, loadedB,\n+                              loadedC, op, adaptor);\n }\n \n /// ====================== mma codegen end ============================"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 60, "deletions": 0, "changes": 60, "file_content_changes": "@@ -714,3 +714,63 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     return\n   }\n }\n+\n+// -----\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n+#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n+#dot_operand_a = #triton_gpu.dot_operand<{opIdx=0, parent=#mma}>\n+#dot_operand_b = #triton_gpu.dot_operand<{opIdx=1, parent=#mma}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  func @matmul_kernel_dot_operand_layout(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<128x256xf32, #mma>\n+    %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked0}>>\n+    %1 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+    %2 = tt.splat %arg3 : (i32) -> tensor<128x1xi32, #blocked0>\n+    %3 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<128x1x!tt.ptr<f16>, #blocked0>\n+    %4 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked0}>>\n+    %5 = tt.expand_dims %4 {axis = 0 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked0}>>) -> tensor<1x32xi32, #blocked0>\n+    %6 = tt.broadcast %5 : (tensor<1x32xi32, #blocked0>) -> tensor<128x32xi32, #blocked0>\n+    %7 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+    %8 = tt.splat %arg4 : (i32) -> tensor<32x1xi32, #blocked1>\n+    %9 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<32x1x!tt.ptr<f16>, #blocked1>\n+    %10 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>\n+    %11 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>\n+    %12 = tt.expand_dims %7 {axis = 1 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<32x1xi32, #blocked1>\n+    %13 = arith.muli %12, %8 : tensor<32x1xi32, #blocked1>\n+    %14 = tt.addptr %9, %13 : tensor<32x1x!tt.ptr<f16>, #blocked1>\n+    %15 = tt.broadcast %14 : (tensor<32x1x!tt.ptr<f16>, #blocked1>) -> tensor<32x256x!tt.ptr<f16>, #blocked1>\n+    %16 = tt.expand_dims %10 {axis = 0 : i32} : (tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>) -> tensor<1x256xi32, #blocked1>\n+    %17 = tt.broadcast %16 : (tensor<1x256xi32, #blocked1>) -> tensor<32x256xi32, #blocked1>\n+    %18 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked0}>>) -> tensor<128x1xi32, #blocked0>\n+    %19 = arith.muli %18, %2 : tensor<128x1xi32, #blocked0>\n+    %20 = tt.addptr %3, %19 : tensor<128x1x!tt.ptr<f16>, #blocked0>\n+    %21 = tt.broadcast %20 : (tensor<128x1x!tt.ptr<f16>, #blocked0>) -> tensor<128x32x!tt.ptr<f16>, #blocked0>\n+    %22 = tt.addptr %21, %6 : tensor<128x32x!tt.ptr<f16>, #blocked0>\n+    %23 = tt.load %22 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #blocked0>\n+    %24 = tt.addptr %15, %17 : tensor<32x256x!tt.ptr<f16>, #blocked1>\n+    %25 = tt.load %24 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x256xf16, #blocked1>\n+    %26 = triton_gpu.convert_layout %23 : (tensor<128x32xf16, #blocked0>) -> tensor<128x32xf16, #shared>\n+    %27 = triton_gpu.convert_layout %25 : (tensor<32x256xf16, #blocked1>) -> tensor<32x256xf16, #shared>\n+\n+    // add convert_layout for dot's operands $a and $b.\n+    %a_mat = triton_gpu.convert_layout %26 : (tensor<128x32xf16, #shared>) -> tensor<128x32xf16, #dot_operand_a>\n+    %b_mat = triton_gpu.convert_layout %27 : (tensor<32x256xf16, #shared>) -> tensor<32x256xf16, #dot_operand_b>\n+\n+    %28 = tt.dot %a_mat, %b_mat, %cst {allowTF32 = true} : tensor<128x32xf16, #dot_operand_a> * tensor<32x256xf16, #dot_operand_b> -> tensor<128x256xf32, #mma>\n+    %29 = tt.splat %arg5 : (i32) -> tensor<128x1xi32, #blocked2>\n+    %30 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #blocked2>\n+    %31 = tt.expand_dims %11 {axis = 0 : i32} : (tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>) -> tensor<1x256xi32, #blocked2>\n+    %32 = tt.broadcast %31 : (tensor<1x256xi32, #blocked2>) -> tensor<128x256xi32, #blocked2>\n+    %33 = tt.expand_dims %1 {axis = 1 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<128x1xi32, #blocked2>\n+    %34 = arith.muli %33, %29 : tensor<128x1xi32, #blocked2>\n+    %35 = tt.addptr %30, %34 : tensor<128x1x!tt.ptr<f32>, #blocked2>\n+    %36 = tt.broadcast %35 : (tensor<128x1x!tt.ptr<f32>, #blocked2>) -> tensor<128x256x!tt.ptr<f32>, #blocked2>\n+    %37 = tt.addptr %36, %32 : tensor<128x256x!tt.ptr<f32>, #blocked2>\n+    %38 = triton_gpu.convert_layout %28 : (tensor<128x256xf32, #mma>) -> tensor<128x256xf32, #blocked2>\n+    tt.store %37, %38 : tensor<128x256xf32, #blocked2>\n+    return\n+  }\n+}"}]