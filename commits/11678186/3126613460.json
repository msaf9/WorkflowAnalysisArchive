[{"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -372,7 +372,6 @@ void SharedEncodingAttr::print(AsmPrinter &printer) const {\n           << \"}>\";\n }\n \n-\n //===----------------------------------------------------------------------===//\n // DotOperand Encoding\n //===----------------------------------------------------------------------===//\n@@ -410,7 +409,6 @@ void DotOperandEncodingAttr::print(mlir::AsmPrinter &printer) const {\n           << \"parent = \" << getParent() << \"}>\";\n }\n \n-\n //===----------------------------------------------------------------------===//\n // InsertSliceAsyncOp\n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 5, "deletions": 9, "changes": 14, "file_content_changes": "@@ -364,14 +364,11 @@ void LoopPipeliner::emitPrologue() {\n   for (Value loadOp : loads) {\n     auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n     Value extractSlice = builder.create<tensor::ExtractSliceOp>(\n-        loadOp.getLoc(), sliceType,\n-        loadStageBuffer[loadOp][numStages - 1],\n+        loadOp.getLoc(), sliceType, loadStageBuffer[loadOp][numStages - 1],\n         SmallVector<OpFoldResult>{intAttr(0), intAttr(0), intAttr(0)},\n-        SmallVector<OpFoldResult>{intAttr(1),\n-                                  intAttr(sliceType.getShape()[0]),\n+        SmallVector<OpFoldResult>{intAttr(1), intAttr(sliceType.getShape()[0]),\n                                   intAttr(sliceType.getShape()[1])},\n-        SmallVector<OpFoldResult>{intAttr(1), intAttr(1), intAttr(1)}\n-    );\n+        SmallVector<OpFoldResult>{intAttr(1), intAttr(1), intAttr(1)});\n     loadsExtract[loadOp] = extractSlice;\n   }\n }\n@@ -488,8 +485,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n       nextIV.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n   extractSliceIndex = builder.create<arith::IndexCastOp>(\n-    extractSliceIndex.getLoc(), builder.getIndexType(), extractSliceIndex\n-  );\n+      extractSliceIndex.getLoc(), builder.getIndexType(), extractSliceIndex);\n \n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n@@ -518,7 +514,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n       nextBuffers.push_back(insertAsyncOp);\n       auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n       nextOp = builder.create<tensor::ExtractSliceOp>(\n-        op->getLoc(), sliceType, insertAsyncOp,\n+          op->getLoc(), sliceType, insertAsyncOp,\n           SmallVector<OpFoldResult>{extractSliceIndex, intAttr(0), intAttr(0)},\n           SmallVector<OpFoldResult>{intAttr(1),\n                                     intAttr(sliceType.getShape()[0]),"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 42, "deletions": 37, "changes": 79, "file_content_changes": "@@ -59,27 +59,22 @@ Value Prefetcher::generatePrefetch(Value v, unsigned opIdx, bool isPrefetch,\n \n   // k => (prefetchWidth, k - prefetchWidth)\n   int64_t kIdx = opIdx == 0 ? 1 : 0;\n-  offset[kIdx] = isPrefetch? 0 : prefetchWidth;\n-  shape[kIdx] = isPrefetch? prefetchWidth : (shape[kIdx] - prefetchWidth);\n-\n+  offset[kIdx] = isPrefetch ? 0 : prefetchWidth;\n+  shape[kIdx] = isPrefetch ? prefetchWidth : (shape[kIdx] - prefetchWidth);\n \n   Value newSmem = builder.create<tensor::ExtractSliceOp>(\n-    v.getLoc(),\n-    // TODO: encoding?\n-    RankedTensorType::get(shape, elementType, type.getEncoding()),\n-    v,\n-    SmallVector<OpFoldResult>{intAttr(offset[0]), intAttr(offset[1])},\n-    SmallVector<OpFoldResult>{intAttr(shape[0]), intAttr(shape[1])},\n-    SmallVector<OpFoldResult>{intAttr(1), intAttr(1)}\n-  );\n-\n-  auto dotOperandEnc = \n-    triton::gpu::DotOperandEncodingAttr::get(builder.getContext(), opIdx,\n-                                              mmaEncoding);\n+      v.getLoc(),\n+      // TODO: encoding?\n+      RankedTensorType::get(shape, elementType, type.getEncoding()), v,\n+      SmallVector<OpFoldResult>{intAttr(offset[0]), intAttr(offset[1])},\n+      SmallVector<OpFoldResult>{intAttr(shape[0]), intAttr(shape[1])},\n+      SmallVector<OpFoldResult>{intAttr(1), intAttr(1)});\n+\n+  auto dotOperandEnc = triton::gpu::DotOperandEncodingAttr::get(\n+      builder.getContext(), opIdx, mmaEncoding);\n   Value prefetchSlice = builder.create<triton::gpu::ConvertLayoutOp>(\n-    v.getLoc(),\n-    RankedTensorType::get(shape, elementType, dotOperandEnc), newSmem\n-  );\n+      v.getLoc(), RankedTensorType::get(shape, elementType, dotOperandEnc),\n+      newSmem);\n \n   return prefetchSlice;\n }\n@@ -131,16 +126,18 @@ void Prefetcher::emitPrologue() {\n   OpBuilder builder(forOp);\n \n   for (Value dot : dots) {\n-    auto mmaEncoding = dot.getType().cast<RankedTensorType>().getEncoding()\n-                          .cast<triton::gpu::MmaEncodingAttr>();\n+    auto mmaEncoding = dot.getType()\n+                           .cast<RankedTensorType>()\n+                           .getEncoding()\n+                           .cast<triton::gpu::MmaEncodingAttr>();\n     if (Value aDef = dot2aDef.lookup(dot)) {\n-      Value newA = generatePrefetch(aDef, 0, /*isPrefetch*/true, mmaEncoding,\n-                                    builder);\n+      Value newA =\n+          generatePrefetch(aDef, 0, /*isPrefetch*/ true, mmaEncoding, builder);\n       operand2headDef[dot.getDefiningOp<triton::DotOp>().a()] = newA;\n     }\n     if (Value bDef = dot2bDef.lookup(dot)) {\n-      Value newB = generatePrefetch(bDef, 1, /*isPrefetch*/true, mmaEncoding,\n-                                    builder);\n+      Value newB =\n+          generatePrefetch(bDef, 1, /*isPrefetch*/ true, mmaEncoding, builder);\n       operand2headDef[dot.getDefiningOp<triton::DotOp>().b()] = newB;\n     }\n   }\n@@ -158,7 +155,7 @@ scf::ForOp Prefetcher::createNewForOp() {\n     if (Value b = dot2bArg.lookup(dot))\n       loopArgs.push_back(operand2headDef[b]);\n   }\n-  \n+\n   auto newForOp = builder.create<scf::ForOp>(\n       forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n       forOp.getStep(), loopArgs);\n@@ -171,22 +168,28 @@ scf::ForOp Prefetcher::createNewForOp() {\n   for (Operation &op : forOp.getBody()->without_terminator()) {\n     Operation *newOp = nullptr;\n     if (auto dot = dyn_cast<triton::DotOp>(&op)) {\n-      auto mmaEncoding = dot.getType().cast<RankedTensorType>().getEncoding()\n-                            .cast<triton::gpu::MmaEncodingAttr>();\n+      auto mmaEncoding = dot.getType()\n+                             .cast<RankedTensorType>()\n+                             .getEncoding()\n+                             .cast<triton::gpu::MmaEncodingAttr>();\n       // prefetched dot\n       Operation *firstDot = builder.clone(*dot, mapping);\n       if (Value a = operand2headDef.lookup(dot.a()))\n-        firstDot->setOperand(0, newForOp.getRegionIterArgForOpOperand(*a.use_begin()));\n+        firstDot->setOperand(\n+            0, newForOp.getRegionIterArgForOpOperand(*a.use_begin()));\n       if (Value b = operand2headDef.lookup(dot.b()))\n-        firstDot->setOperand(1, newForOp.getRegionIterArgForOpOperand(*b.use_begin()));\n+        firstDot->setOperand(\n+            1, newForOp.getRegionIterArgForOpOperand(*b.use_begin()));\n \n       // remaining part (Note it's possible that dot.a() is not in mapping)\n       Value aRem = mapping.lookupOrNull(dot.a());\n       Value bRem = mapping.lookupOrNull(dot.b());\n       if (Value a = dot2aArg.lookup(dot))\n-        aRem = generatePrefetch(mapping.lookup(a), 0, false, mmaEncoding, builder);\n+        aRem =\n+            generatePrefetch(mapping.lookup(a), 0, false, mmaEncoding, builder);\n       if (Value b = dot2bArg.lookup(dot))\n-        bRem = generatePrefetch(mapping.lookup(b), 1, false, mmaEncoding, builder);\n+        bRem =\n+            generatePrefetch(mapping.lookup(b), 1, false, mmaEncoding, builder);\n       newOp = builder.clone(*dot, mapping);\n       // Use sliced a & b\n       if (aRem && aRem != mapping.lookup(dot.a()))\n@@ -207,13 +210,16 @@ scf::ForOp Prefetcher::createNewForOp() {\n   for (Value v : forOp.getBody()->getTerminator()->getOperands())\n     yieldValues.push_back(mapping.lookup(v));\n   for (Value dot : dots) {\n-    auto mmaEncoding = dot.getType().cast<RankedTensorType>().getEncoding()\n-                          .cast<triton::gpu::MmaEncodingAttr>();\n-    // TODO: This is wrong. Should be define of yield.\n+    auto mmaEncoding = dot.getType()\n+                           .cast<RankedTensorType>()\n+                           .getEncoding()\n+                           .cast<triton::gpu::MmaEncodingAttr>();\n     if (Value a = dot2aYield.lookup(dot))\n-      yieldValues.push_back(generatePrefetch(mapping.lookup(a), 0, true, mmaEncoding, builder));\n+      yieldValues.push_back(\n+          generatePrefetch(mapping.lookup(a), 0, true, mmaEncoding, builder));\n     if (Value b = dot2bYield.lookup(dot))\n-      yieldValues.push_back(generatePrefetch(mapping.lookup(b), 1, true, mmaEncoding, builder));\n+      yieldValues.push_back(\n+          generatePrefetch(mapping.lookup(b), 1, true, mmaEncoding, builder));\n   }\n   // Update ops of yield\n   builder.create<scf::YieldOp>(yieldOp.getLoc(), yieldValues);\n@@ -247,7 +253,6 @@ struct PrefetchPass : public TritonGPUPrefetchBase<PrefetchPass> {\n \n } // anonymous namespace\n \n-\n std::unique_ptr<Pass> mlir::createTritonGPUPrefetchPass() {\n   return std::make_unique<PrefetchPass>();\n }\n\\ No newline at end of file"}]