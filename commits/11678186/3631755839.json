[{"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 19, "deletions": 11, "changes": 30, "file_content_changes": "@@ -123,9 +123,13 @@ void LoopPipeliner::collectDeps(Value v, int stages, DenseSet<Value> &deps) {\n     return;\n \n   if (auto arg = v.dyn_cast<BlockArgument>()) {\n-    deps.insert(v);\n-    // Note: we have iv as the first arg, so the op idx is arg.getArgNumber()-1\n-    collectDeps(yieldOp->getOperand(arg.getArgNumber() - 1), stages - 1, deps);\n+    if (arg.getArgNumber() > 0) {\n+      // Skip the first arg (loop induction variable)\n+      // Otherwise the op idx is arg.getArgNumber()-1\n+      deps.insert(v);\n+      collectDeps(yieldOp->getOperand(arg.getArgNumber() - 1), stages - 1,\n+                  deps);\n+    }\n   } else { // value\n     // v might be in deps, but we still need to visit v.\n     // This is because v might depend on value in previous iterations\n@@ -376,11 +380,11 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   OpBuilder builder(forOp);\n \n   // Order of new args:\n-  //   (original args),\n-  //   (insertSliceAsync buffer at stage numStages - 1)  for each load\n-  //   (extracted tensor)  for each load\n-  //   (depArgs at stage numStages-1)\n-  //   (iv at stage numStages-1)\n+  //   (original args)\n+  //   (insertSliceAsync buffer at stage numStages - 1) for each load\n+  //   (extracted tensor) for each load\n+  //   (depArgs at stage numStages - 1)\n+  //   (iv at stage numStages - 2)\n   //   (pipeline iteration index)\n   //   (loop iteration index)\n   SmallVector<Value> newLoopArgs;\n@@ -421,6 +425,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   BlockAndValueMapping mapping;\n   for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n+  mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n \n   // 2.1 clone the loop body, replace original args with args of the new ForOp\n   // Insert async wait if necessary.\n@@ -469,6 +474,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   Value nextLoopCond =\n       builder.create<arith::CmpIOp>(nextIV.getLoc(), arith::CmpIPredicate::slt,\n                                     nextIV, newForOp.getUpperBound());\n+  nextMapping.map(forOp.getInductionVar(), nextIV);\n \n   // Slice index\n   SmallVector<Value> nextBuffers;\n@@ -598,9 +604,11 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   for (Value nextSlice : extractSlices)\n     yieldValues.push_back(nextSlice);\n \n-  for (size_t i = depArgsBeginIdx; i < nextIVIdx; ++i)\n-    yieldValues.push_back(\n-        depArgsMapping.lookup(newForOp.getRegionIterArgs()[i]));\n+  for (size_t i = depArgsBeginIdx; i < nextIVIdx; ++i) {\n+    auto arg = newForOp.getRegionIterArgs()[i];\n+    assert(depArgsMapping.count(arg) && \"Missing loop-carried value\");\n+    yieldValues.push_back(depArgsMapping[arg]);\n+  }\n   yieldValues.push_back(nextIV);\n   yieldValues.push_back(pipelineIterIdx);\n   yieldValues.push_back(loopIterIdx);"}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -257,5 +257,5 @@ def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='c\n                                                      grad_to_none=[x], rep=500)\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n \n-\n+# test_layer_norm(1151, 8192, torch.float16)\n bench_layer_norm.run(save_path='.', print_data=True)"}]