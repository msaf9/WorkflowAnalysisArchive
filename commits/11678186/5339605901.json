[{"filename": ".editorconfig", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "file_content_changes": "@@ -0,0 +1,41 @@\n+# https://editorconfig.org/\n+\n+root = true\n+\n+[*]\n+charset = utf-8\n+end_of_line = lf\n+indent_style = space\n+indent_size = 4\n+trim_trailing_whitespace = true\n+insert_final_newline = true\n+\n+[*.py]\n+indent_size = 4\n+src_paths=python\n+\n+[*.{yaml,yml}]\n+indent_size = 2\n+\n+[*.md]\n+indent_size = 2\n+x-soft-wrap-text = true\n+\n+[*.rst]\n+indent_size = 4\n+x-soft-wrap-text = true\n+\n+[CMakeLists.txt,*.cmake]\n+indent_size = 2\n+\n+[Makefile]\n+indent_style = tab\n+\n+[*.{c,cc,cpp,h,hpp,cu,cuh}]\n+indent_size = 2\n+\n+[*.mlir]\n+indent_size = 2\n+\n+[*.td]\n+indent_size = 4"}, {"filename": ".flake8", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -0,0 +1,2 @@\n+[flake8]\n+ignore = E501,E701,E731"}, {"filename": ".github/workflows/Dockerfile", "status": "added", "additions": 37, "deletions": 0, "changes": 37, "file_content_changes": "@@ -0,0 +1,37 @@\n+FROM centos:7\n+ARG llvm_dir=llvm-project\n+\n+# Add the cache artifacts and the LLVM source tree to the container\n+ADD sccache /sccache\n+ADD \"${llvm_dir}\" /source/llvm-project\n+ENV SCCACHE_DIR=\"/sccache\"\n+ENV SCCACHE_CACHE_SIZE=\"2G\"\n+\n+# Install build dependencies\n+RUN yum install --assumeyes centos-release-scl\n+RUN yum install --assumeyes devtoolset-9-gcc* python3-devel python3-pip\n+SHELL [ \"/usr/bin/scl\", \"enable\", \"devtoolset-9\" ]\n+\n+RUN python3 -m pip install --upgrade pip\n+RUN python3 -m pip install --upgrade cmake ninja sccache\n+\n+# Install MLIR's Python Dependencies\n+RUN python3 -m pip install -r /source/llvm-project/mlir/python/requirements.txt\n+\n+# Configure, Build, Test, and Install LLVM\n+RUN cmake -GNinja -Bbuild \\\n+  -DCMAKE_BUILD_TYPE=Release \\\n+  -DCMAKE_C_COMPILER=gcc \\\n+  -DCMAKE_CXX_COMPILER=g++ \\\n+  -DCMAKE_C_COMPILER_LAUNCHER=sccache \\\n+  -DCMAKE_CXX_COMPILER_LAUNCHER=sccache \\\n+  -DCMAKE_INSTALL_PREFIX=\"/install\" \\\n+  -DLLVM_BUILD_UTILS=ON \\\n+  -DLLVM_ENABLE_ASSERTIONS=ON \\\n+  -DMLIR_ENABLE_BINDINGS_PYTHON=ON \\\n+  -DLLVM_ENABLE_PROJECTS=mlir \\\n+  -DLLVM_INSTALL_UTILS=ON \\\n+  -DLLVM_TARGETS_TO_BUILD=\"host;NVPTX;AMDGPU\" \\\n+  /source/llvm-project/llvm\n+\n+RUN ninja -C build check-mlir install"}, {"filename": ".github/workflows/documentation.yml", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -0,0 +1,53 @@\n+name: Documentation\n+on:\n+  workflow_dispatch:\n+  schedule:\n+    - cron: \"0 0 * * *\"\n+\n+jobs:\n+  Build-Documentation:\n+    runs-on: [self-hosted, A100]\n+\n+    steps:\n+      - name: Checkout branch\n+        uses: actions/checkout@v2\n+        with:\n+          token: ${{ secrets.CI_PAT }}\n+          fetch-depth: 0\n+\n+      - name: Clear docs\n+        run: |\n+          rm -r /tmp/triton-docs\n+        continue-on-error: true\n+\n+      - name: Install dependent packages\n+        run: |\n+          pip3 install tabulate\n+          pip3 install cmake\n+\n+      #- name: Fetch dependent branches\n+      #  run: |\n+      #    git fetch origin main:main\n+\n+      - name: Build docs\n+        run: |\n+          cd docs\n+          export PATH=$(python3 -c \"import cmake; print(cmake.CMAKE_BIN_DIR)\"):$PATH\n+          python3 -m sphinx_multiversion . _build/html/\n+\n+      - name: Update docs\n+        run: |\n+          mkdir /tmp/triton-docs\n+          mv docs/_build/html/* /tmp/triton-docs/\n+          git checkout gh-pages\n+          cp -r CNAME /tmp/triton-docs/\n+          cp -r index.html /tmp/triton-docs/\n+          cp -r .nojekyll /tmp/triton-docs/\n+          rm -r *\n+          cp -r /tmp/triton-docs/* .\n+          git add .\n+          git commit -am \"[GH-PAGES] Updated website\"\n+\n+      - name: Publish docs\n+        run: |\n+          git push origin gh-pages"}, {"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 118, "deletions": 35, "changes": 153, "file_content_changes": "@@ -3,9 +3,17 @@ name: Integration Tests\n on:\n   workflow_dispatch:\n   pull_request:\n-    branches:\n-      - main\n-      - triton-mlir\n+    branches: [main]\n+  merge_group:\n+    branches: [main]\n+    types: [checks_requested]\n+\n+concurrency:\n+  group: ${{ github.ref }}\n+  cancel-in-progress: ${{ github.ref != 'refs/heads/master' }}\n+\n+env:\n+  TRITON_USE_ASSERT_ENABLED_LLVM: \"TRUE\"\n \n jobs:\n   Runner-Preparation:\n@@ -17,9 +25,9 @@ jobs:\n         id: set-matrix\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n-            echo '::set-output name=matrix::[[\"self-hosted\", \"A10\"], [\"self-hosted\", \"V100\"], \"macos-10.15\"]' >> $GITHUB_OUTPUT\n+            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], [\"self-hosted\", \"H100\"], [\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]' >> $GITHUB_OUTPUT\n           else\n-            echo '::set-output name=matrix::[\"ubuntu-latest\", \"macos-10.15\"]' >> $GITHUB_OUTPUT\n+            echo '::set-output name=matrix::[\"ubuntu-latest\"]' >> $GITHUB_OUTPUT\n           fi\n \n   Integration-Tests:\n@@ -35,58 +43,133 @@ jobs:\n       - name: Checkout\n         uses: actions/checkout@v3\n \n-      - name: Clear cache\n+      - name: Set CUDA ENV\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n         run: |\n-          rm -rf ~/.triton/cache/\n+          echo \"BACKEND=CUDA\" >> \"${GITHUB_ENV}\"\n \n-      - name: Check imports\n-        if: ${{ matrix.runner != 'macos-10.15' }}\n+      - name: Set ROCM ENV\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'gfx908')}}\n         run: |\n-          pip install isort\n-          isort -c ./python || ( echo '::error title=Imports not sorted::Please run \\\"isort ./python\\\"' ; exit 1 )\n+          echo \"BACKEND=ROCM\" >> \"${GITHUB_ENV}\"\n \n-      - name: Check python style\n-        if: ${{ matrix.runner != 'macos-10.15' }}\n+      - name: Set XPU ENV\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'arc770')}}\n+        run: |\n+          echo \"BACKEND=XPU\" >> \"${GITHUB_ENV}\"\n+\n+      - name: Clear cache\n         run: |\n-          pip install autopep8\n-          autopep8 -a -r -d --exit-code ./python || ( echo '::error title=Style issues::Please run \\\"autopep8 -a -r -i ./python\\\"' ; exit 1 )\n+          rm -rf ~/.triton\n \n-      - name: Check cpp style\n-        if: ${{ matrix.runner != 'macos-10.15' }}\n+      - name: Update PATH\n         run: |\n-          pip install clang-format\n-          find . -regex '.*\\.\\(cpp\\|hpp\\|h\\|cc\\)' -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file --dry-run -Werror -i ||\n-          (echo '::error title=Style issues:: Please run `find . -regex \".*\\.\\(cpp\\|hpp\\|h\\|cc\\)\" -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file -i`' ; exit 1)\n+          echo \"PATH=${HOME}/.local/bin:${PATH}\" >> \"${GITHUB_ENV}\"\n \n-      - name: Flake8\n-        if: ${{ matrix.runner != 'macos-10.15' }}\n+      - name: Check pre-commit\n+        if: ${{ matrix.runner != 'macos-10.15' && (matrix.runner[1] != 'arc770') }}\n         run: |\n-          pip install flake8\n-          flake8 --config ./python/setup.cfg ./python || ( echo '::error::Flake8 failed; see logs for errors.' ; exit 1 )\n+          python3 -m pip install --upgrade pre-commit\n+          python3 -m pre_commit run --all-files\n+\n+      - name: Check pre-commit arc770\n+        if: ${{ matrix.runner != 'macos-10.15' && (matrix.runner[1] == 'arc770') }}\n+        run: |\n+          source ${HOME}/triton_vars.sh\n+          source ${HOME}/miniconda3/bin/activate\n+          conda activate triton-xpu-ci\n+          python3 -m pip install --upgrade pre-commit\n+          python3 -m pre_commit run --all-files\n \n       - name: Install Triton\n+        if: ${{ env.BACKEND == 'CUDA'}}\n+        run: |\n+          cd python\n+          python3 -m pip install --upgrade pip\n+          python3 -m pip install cmake==3.24\n+          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n+\n+      - name: Install Triton on ROCM\n+        if: ${{ env.BACKEND == 'ROCM'}}\n         run: |\n           cd python\n-          TRITON_USE_ASSERT_ENABLED_LLVM=TRUE pip3 install -e '.[tests]'\n+          python3 -m pip install --upgrade pip\n+          python3 -m pip install cmake==3.24\n+          python3 -m pip install torch==1.13.1 --index-url https://download.pytorch.org/whl/rocm5.2\n+          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n+\n+      - name: Install Triton on XPU\n+        if: ${{ env.BACKEND == 'XPU'}}\n+        run: |\n+          source ${HOME}/triton_vars.sh\n+          source ${HOME}/miniconda3/bin/activate\n+          conda activate triton-xpu-ci\n+          git submodule update --init --recursive\n+          cd python\n+          python3 -m pip install --upgrade pip\n+          python3 -m pip install cmake==3.24\n+          export TRITON_CODEGEN_INTEL_XPU_BACKEND=1\n+          python3 -m pip uninstall -y triton\n+          python3 setup.py build\n+          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n       - name: Run lit tests\n+        if: ${{ env.BACKEND == 'CUDA'}}\n         run: |\n+          python3 -m pip install lit\n           cd python\n-          LIT_TEST_DIR=\"build/$(ls build)/test\"\n-          if [ ! -d \"$LIT_TEST_DIR\" ]; then\n-            echo \"Not found `$LIT_TEST_DIR`.  Did you change an installation method?\" ; exit -1\n+          LIT_TEST_DIR=\"build/$(ls build | grep -i cmake)/test\"\n+          if [ ! -d \"${LIT_TEST_DIR}\" ]; then\n+            echo \"Coult not find '${LIT_TEST_DIR}'\" ; exit -1\n           fi\n-          lit -v \"$LIT_TEST_DIR\"\n+          lit -v \"${LIT_TEST_DIR}\"\n \n-      - name: Run python tests\n-        if: ${{matrix.runner[0] == 'self-hosted'}}\n+      - name: Run python tests on CUDA\n+        if: ${{ env.BACKEND == 'CUDA'}}\n         run: |\n-          cd python/test/unit/\n-          pytest\n+          cd python/test/unit\n+          python3 -m pytest\n \n+      - name: Create artifacts archive\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n+        run: |\n+          cd ~/.triton\n+          tar -czvf artifacts.tar.gz cache\n+\n+      - name: Upload artifacts archive\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n+        uses: actions/upload-artifact@v2\n+        with:\n+          name: artifacts\n+          path: ~/.triton/artifacts.tar.gz\n \n       - name: Run CXX unittests\n+        if: ${{ env.BACKEND == 'CUDA'}}\n         run: |\n-          cd python/\n-          cd \"build/$(ls build)\"\n+          cd python\n+          cd \"build/$(ls build | grep -i cmake)\"\n           ctest\n+\n+      - name: Run python tests on ROCM\n+        if: ${{ env.BACKEND == 'ROCM'}}\n+        run: |\n+          cd python/test/unit/language\n+          python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py::test_empty_kernel\"\n+\n+      - name: Run python tests on XPU\n+        if: ${{ env.BACKEND == 'XPU'}}\n+        run: |\n+          source ${HOME}/triton_vars.sh\n+          source ${HOME}/miniconda3/bin/activate\n+          conda activate triton-xpu-ci\n+          cd python/test/backend/third_party_backends\n+          python3 -m pytest --capture=tee-sys -rfs --verbose --backend xpu\n+\n+      - name: Regression tests\n+        if: ${{ contains(matrix.runner, 'A100') }}\n+        run: |\n+          cd python/test/regression\n+          sudo nvidia-smi -i 0 -pm 1\n+          sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n+          python3 -m pytest -vs .\n+          sudo nvidia-smi -i 0 -rgc"}, {"filename": ".github/workflows/llvm-build.yml", "status": "added", "additions": 195, "deletions": 0, "changes": 195, "file_content_changes": "@@ -0,0 +1,195 @@\n+name: LLVM Build\n+\n+on:\n+  push:\n+    branches:\n+      - llvm-head\n+    paths:\n+      - llvm-hash.txt\n+  workflow_dispatch:\n+\n+env:\n+  SCCACHE_DIR: ${{ github.workspace }}/sccache\n+\n+permissions:\n+  contents: read\n+  id-token: write\n+\n+jobs:\n+\n+  build:\n+\n+    strategy:\n+      fail-fast: true\n+\n+      matrix:\n+        platform: [\n+          ubuntu-20.04-x64,\n+          ubuntu-22.04-x64,\n+          centos-7-x64,\n+          macos-x64,\n+          macos-arm64\n+        ]\n+\n+        include:\n+          # Specify OS versions\n+          - platform: ubuntu-20.04-x64\n+            host-os: ubuntu-20.04\n+            target-os: ubuntu\n+            arch: x64\n+          - platform: ubuntu-22.04-x64\n+            host-os: ubuntu-22.04\n+            target-os: ubuntu\n+            arch: x64\n+          - platform: centos-7-x64\n+            host-os: ubuntu-22.04\n+            target-os: centos\n+            arch: x64\n+          - platform: macos-x64\n+            host-os: macos-12\n+            target-os: macos\n+            arch: x64\n+          - platform: macos-arm64\n+            host-os: macos-12\n+            target-os: macos\n+            arch: arm64\n+\n+    runs-on: ${{ matrix.host-os }}\n+\n+    steps:\n+\n+    - name: Checkout Repo\n+      uses: actions/checkout@v3\n+      with:\n+        path: llvm-build\n+\n+    - name: Fetch LLVM Commit Hash\n+      run: |\n+        LLVM_COMMIT_HASH=\"$(cat llvm-build/llvm-hash.txt)\"\n+        echo \"Found LLVM commit hash: ${LLVM_COMMIT_HASH}\"\n+        echo \"llvm_commit_hash=${LLVM_COMMIT_HASH}\" >> ${GITHUB_ENV}\n+\n+        SHORT_LLVM_COMMIT_HASH=\"${LLVM_COMMIT_HASH:0:8}\"\n+        echo \"Short LLVM commit hash: ${SHORT_LLVM_COMMIT_HASH}\"\n+        echo \"short_llvm_commit_hash=${SHORT_LLVM_COMMIT_HASH}\" >> ${GITHUB_ENV}\n+\n+        INSTALL_DIR=\"llvm-${SHORT_LLVM_COMMIT_HASH}-${{ matrix.platform }}\"\n+        echo \"LLVM installation directory name: ${INSTALL_DIR}\"\n+        echo \"llvm_install_dir=${INSTALL_DIR}\" >> ${GITHUB_ENV}\n+\n+    - name: Checkout LLVM\n+      uses: actions/checkout@v3\n+      with:\n+        repository: llvm/llvm-project\n+        path: llvm-project\n+        ref: ${{ env.llvm_commit_hash }}\n+\n+    - name: Set up Python\n+      uses: actions/setup-python@v4\n+      with:\n+        python-version: 3.11\n+\n+    - name: Install Prerequisites\n+      run: |\n+        python3 -m pip install cmake ninja sccache\n+        mkdir -p ${{ env.SCCACHE_DIR }}\n+        rm -rf ${{ env.SCCACHE_DIR }}/*\n+\n+    - name: Enable Cache\n+      uses: actions/cache@v3\n+      with:\n+        path: ${{ env.SCCACHE_DIR }}\n+        key: ${{ matrix.platform }}-${{ env.short_llvm_commit_hash }}\n+        restore-keys: ${{ matrix.platform }}-\n+\n+    - name: Configure, Build, Test, and Install LLVM (Ubuntu and macOS x64)\n+      if: matrix.arch == 'x64' && contains(fromJSON('[\"ubuntu\", \"macos\"]'), matrix.target-os)\n+      run: >\n+        python3 -m pip install -r llvm-project/mlir/python/requirements.txt\n+\n+        cmake -GNinja -Bllvm-project/build\n+        -DCMAKE_BUILD_TYPE=Release\n+        -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++\n+        -DCMAKE_C_COMPILER_LAUNCHER=sccache -DCMAKE_CXX_COMPILER_LAUNCHER=sccache\n+        -DCMAKE_INSTALL_PREFIX=\"${{ env.llvm_install_dir }}\"\n+        -DCMAKE_LINKER=lld\n+        -DLLVM_BUILD_UTILS=ON\n+        -DLLVM_ENABLE_ASSERTIONS=ON\n+        -DMLIR_ENABLE_BINDINGS_PYTHON=ON\n+        -DLLVM_ENABLE_PROJECTS=mlir\n+        -DLLVM_INSTALL_UTILS=ON\n+        -DLLVM_TARGETS_TO_BUILD=\"host;NVPTX;AMDGPU\"\n+        llvm-project/llvm\n+\n+        ninja -C llvm-project/build check-mlir install\n+\n+        tar czf \"${{ env.llvm_install_dir }}.tar.gz\" \"${{ env.llvm_install_dir }}\"\n+\n+    - name: Configure, Build, and Install LLVM (macOS arm64)\n+      if: matrix.arch == 'arm64' && matrix.target-os == 'macos'\n+      run: >\n+        python3 -m pip install -r llvm-project/mlir/python/requirements.txt\n+\n+        cmake -GNinja -Bllvm-project/build\n+        -DCMAKE_BUILD_TYPE=Release\n+        -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++\n+        -DCMAKE_C_COMPILER_LAUNCHER=sccache -DCMAKE_CXX_COMPILER_LAUNCHER=sccache\n+        -DCMAKE_INSTALL_PREFIX=\"${{ env.llvm_install_dir }}\"\n+        -DCMAKE_LINKER=lld\n+        -DCMAKE_OSX_ARCHITECTURES=arm64\n+        -DLLVM_BUILD_UTILS=ON\n+        -DLLVM_ENABLE_ASSERTIONS=ON\n+        -DMLIR_ENABLE_BINDINGS_PYTHON=ON\n+        -DLLVM_ENABLE_PROJECTS=mlir\n+        -DLLVM_ENABLE_ZSTD=OFF\n+        -DLLVM_INSTALL_UTILS=ON\n+        -DLLVM_TARGETS_TO_BUILD=\"AArch64\"\n+        -DLLVM_USE_HOST_TOOLS=ON\n+        llvm-project/llvm\n+\n+        ninja -C llvm-project/build install\n+\n+        tar czf \"${{ env.llvm_install_dir }}.tar.gz\" \"${{ env.llvm_install_dir }}\"\n+\n+    - name: Configure, Build, Test, and Install LLVM (CentOS)\n+      if: matrix.target-os == 'centos'\n+      run: |\n+        docker build --tag llvm-build --build-arg llvm_dir=llvm-project \\\n+          -f llvm-build/.github/workflows/Dockerfile .\n+\n+        # Create temporary container to copy cache and installed artifacts.\n+        CONTAINER_ID=$(docker create llvm-build)\n+        docker cp \"${CONTAINER_ID}:/install\" \"${{ env.llvm_install_dir }}\"\n+        tar czf \"${{ env.llvm_install_dir }}.tar.gz\" \"${{ env.llvm_install_dir }}\"\n+\n+        # We remove the existing directory, otherwise docker will\n+        # create a subdirectory inside the existing directory.\n+        rm -rf \"${{ env.SCCACHE_DIR }}\"\n+        docker cp \"${CONTAINER_ID}:/sccache\" \"${{ env.SCCACHE_DIR }}\"\n+        sudo chown -R \"$(id -u -n):$(id -g -n)\" \"${{ env.SCCACHE_DIR }}\"\n+\n+        docker rm \"${CONTAINER_ID}\"\n+\n+    - name: Azure Login\n+      uses: azure/login@v1\n+      with:\n+        client-id: ${{ secrets.AZURE_CLIENT_ID }}\n+        tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n+        subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n+\n+    - name: Upload LLVM Artifacts to Azure\n+      run: |\n+        az storage blob upload --account-name tritonlang --auth-mode login --container-name llvm-builds --file \"${{ env.llvm_install_dir }}.tar.gz\" --name \"${{ env.llvm_install_dir }}.tar.gz\" --overwrite\n+\n+        URL=$(az storage blob url --account-name tritonlang --auth-mode login --container-name llvm-builds --name \"${{ env.llvm_install_dir }}.tar.gz\")\n+        echo \"Blob URL: ${URL}\"\n+\n+    - name: Azure Logout\n+      run: |\n+        az logout\n+        az cache purge\n+        az account clear\n+      if: always()\n+\n+    - name: Dump Sccache Statistics\n+      run: sccache --show-stats"}, {"filename": ".github/workflows/torch-inductor-tests.yml", "status": "added", "additions": 39, "deletions": 0, "changes": 39, "file_content_changes": "@@ -0,0 +1,39 @@\n+name: Torchinductor\n+\n+on:\n+  workflow_dispatch:\n+\n+jobs:\n+  Runner-Preparation:\n+    runs-on: ubuntu-latest\n+    outputs:\n+      matrix: ${{ steps.set-matrix.outputs.matrix }}\n+    steps:\n+      - name: Prepare runner matrix\n+        id: set-matrix\n+        run: |\n+          echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"]]'\n+\n+  Integration-Tests:\n+    needs: Runner-Preparation\n+    runs-on: ${{ matrix.runner }}\n+    strategy:\n+      matrix:\n+        runner: ${{fromJson(needs.Runner-Preparation.outputs.matrix)}}\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v2\n+      #- name: Packages\n+      #  run: |\n+      #    ./.github/workflows/torchinductor/scripts/install_torchinductor.sh\n+      - name: Environment\n+        run: |\n+          source /opt/torchinductor_venv/bin/activate\n+          ./.github/workflows/torchinductor/scripts/install_triton.sh\n+      - name: Performance\n+        run: |\n+          ./.github/workflows/torchinductor/scripts/run_torchinductor_perf.sh\n+      # Runs too long time\n+      #- name: Accuracy\n+      #  run: |\n+      #    ./.github/workflows/torchinductor/scripts/run_torchinductor_acc.sh"}, {"filename": ".github/workflows/torchinductor/data/huggingface.csv", "status": "added", "additions": 37, "deletions": 0, "changes": 37, "file_content_changes": "@@ -0,0 +1,37 @@\n+dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio\n+cuda,AlbertForMaskedLM,4,1.5511,164.3373,26.8523,1.2647\n+cuda,AlbertForQuestionAnswering,4,1.5501,163.5580,25.7983,1.3145\n+cuda,BartForCausalLM,4,1.5080,71.7230,32.8907,0.9749\n+cuda,BertForMaskedLM,16,1.5350,67.9451,35.3286,1.0494\n+cuda,BertForQuestionAnswering,16,1.6735,53.2963,34.3754,1.1710\n+cuda,BlenderbotSmallForCausalLM,64,1.2106,46.6466,23.8058,0.9120\n+cuda,BlenderbotSmallForConditionalGeneration,64,1.3616,77.3013,55.3546,0.9803\n+cuda,CamemBert,16,1.4779,76.1809,35.3883,1.0469\n+cuda,DebertaForMaskedLM,4,0.8415,62.3395,35.9657,1.0418\n+cuda,DebertaForQuestionAnswering,8,1.0609,67.5151,35.7728,1.1528\n+cuda,DebertaV2ForMaskedLM,1,0.6026,134.6517,66.1783,0.9773\n+cuda,DistilBertForMaskedLM,128,1.2460,66.9382,18.3089,0.9624\n+cuda,DistilBertForQuestionAnswering,256,1.3997,72.4126,18.1956,1.1486\n+cuda,DistillGPT2,16,1.6656,60.5455,17.2280,1.0641\n+cuda,ElectraForCausalLM,32,1.8299,45.4841,37.0944,0.9717\n+cuda,ElectraForQuestionAnswering,64,2.0289,52.6890,35.9632,1.1928\n+cuda,GPT2ForSequenceClassification,4,2.2567,38.2969,30.0527,1.2323\n+cuda,LayoutLMForMaskedLM,16,1.5423,68.8018,36.5562,1.0495\n+cuda,LayoutLMForSequenceClassification,16,1.7058,53.9355,35.2225,1.1659\n+cuda,MBartForCausalLM,4,1.4945,71.4649,32.8653,0.9830\n+cuda,MegatronBertForCausalLM,4,1.4328,58.4404,70.6226,1.0951\n+cuda,MegatronBertForQuestionAnswering,8,1.5886,85.2533,69.1219,1.1152\n+cuda,MobileBertForMaskedLM,64,0.9007,131.7379,107.5275,1.0136\n+cuda,MobileBertForQuestionAnswering,128,0.8435,167.9066,106.7049,0.8579\n+cuda,PLBartForCausalLM,8,1.5261,68.9224,19.5826,0.9887\n+cuda,PLBartForConditionalGeneration,4,1.5298,71.2811,45.6902,1.0495\n+cuda,PegasusForCausalLM,32,1.2212,57.5436,33.3863,0.9736\n+cuda,PegasusForConditionalGeneration,32,1.2822,106.4678,69.8825,1.0689\n+cuda,RobertaForCausalLM,16,1.6128,67.5706,34.7355,1.0496\n+cuda,RobertaForQuestionAnswering,16,1.6800,53.6267,33.8527,1.1704\n+cuda,Speech2Text2ForCausalLM,256,1.8230,32.9145,18.7201,0.8760\n+cuda,T5ForConditionalGeneration,4,1.6592,59.5324,39.4406,1.1814\n+cuda,T5Small,4,1.6581,59.5930,37.0471,1.1814\n+cuda,TrOCRForCausalLM,32,1.2586,106.2633,32.5330,0.9583\n+cuda,XLNetLMHeadModel,8,1.8108,142.8795,84.8197,1.1240\n+cuda,YituTechConvBert,16,1.5207,81.4595,53.1565,1.0362"}, {"filename": ".github/workflows/torchinductor/data/timm_models.csv", "status": "added", "additions": 54, "deletions": 0, "changes": 54, "file_content_changes": "@@ -0,0 +1,54 @@\n+dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio\n+cuda,adv_inception_v3,128,1.5923,102.5292,51.6032,1.0472\n+cuda,beit_base_patch16_224,64,1.3390,75.3027,29.7471,1.0156\n+cuda,coat_lite_mini,128,2.0579,53.3689,37.1856,1.0437\n+cuda,convmixer_768_32,32,1.0470,275.5328,23.8037,0.9999\n+cuda,convnext_base,64,1.5084,80.1811,42.5659,1.0373\n+cuda,crossvit_9_240,128,1.5392,37.1806,44.9986,0.9193\n+cuda,cspdarknet53,64,1.4721,75.0403,35.2882,1.0547\n+cuda,deit_base_distilled_patch16_224,64,1.1432,55.9737,23.4038,0.9816\n+cuda,dla102,128,1.5282,123.7284,49.3612,1.0430\n+cuda,dm_nfnet_f0,128,1.4354,79.7518,34.8994,1.1038\n+cuda,dpn107,32,1.2412,83.8921,58.9111,0.9952\n+cuda,eca_botnext26ts_256,128,1.5425,71.2406,28.8920,1.0270\n+cuda,ese_vovnet19b_dw,128,1.4647,42.4837,18.0285,1.0135\n+cuda,fbnetc_100,128,1.5795,53.8033,33.0222,1.0082\n+cuda,gernet_l,128,1.1684,63.4230,26.8687,1.0053\n+cuda,ghostnet_100,128,1.7812,54.4211,47.6168,1.0484\n+cuda,gluon_inception_v3,128,1.5952,102.5018,50.0857,1.0469\n+cuda,gmixer_24_224,128,1.6749,69.2430,42.0841,1.1921\n+cuda,gmlp_s16_224,128,1.5886,79.2132,43.0142,1.2343\n+cuda,hrnet_w18,128,1.3743,221.5304,134.2573,1.0100\n+cuda,inception_v3,128,1.5847,102.8333,49.7648,1.0472\n+cuda,jx_nest_base,32,1.3747,71.4190,61.4053,0.9905\n+cuda,lcnet_050,128,1.8159,18.0047,18.8249,1.0005\n+cuda,mixer_b16_224,128,1.2795,90.9229,21.0438,1.0133\n+cuda,mixnet_l,128,1.2273,149.9722,47.7482,1.0129\n+cuda,mnasnet_100,128,1.6594,40.0512,26.5165,1.0047\n+cuda,mobilenetv2_100,128,1.6085,41.1217,27.4450,1.1731\n+cuda,mobilenetv3_large_100,128,1.6610,37.9995,29.8185,1.0052\n+cuda,mobilevit_s,64,1.5212,55.4152,53.6475,1.0258\n+cuda,nfnet_l0,128,1.4927,65.7078,32.4067,0.9980\n+cuda,pit_b_224,64,1.2286,57.9484,26.5321,0.9606\n+cuda,pnasnet5large,16,1.0000,198.2494,93.4641,1.3184\n+cuda,poolformer_m36,64,1.3486,103.9235,62.3196,1.1942\n+cuda,regnety_002,128,1.3030,32.4968,27.2439,1.0014\n+cuda,repvgg_a2,128,1.2485,59.7729,26.9209,1.0185\n+cuda,res2net101_26w_4s,64,1.0813,94.1773,86.6520,0.9655\n+cuda,res2net50_14w_8s,128,1.3251,109.5258,79.9578,0.9830\n+cuda,res2next50,128,1.2518,125.5008,43.9754,0.9756\n+cuda,resmlp_12_224,128,1.3060,45.2373,19.3709,1.1048\n+cuda,resnest101e,64,1.4346,108.1945,78.1993,1.1037\n+cuda,rexnet_100,128,1.4637,55.0121,41.2075,1.0862\n+cuda,selecsls42b,128,1.4284,44.6645,23.3892,1.0139\n+cuda,spnasnet_100,128,1.5908,45.3189,32.0148,1.0048\n+cuda,swin_base_patch4_window7_224,64,1.6164,89.5854,75.5848,0.9299\n+cuda,swsl_resnext101_32x16d,32,1.0175,110.0041,45.7853,1.0003\n+cuda,tf_efficientnet_b0,128,1.5271,55.7361,34.5551,1.1079\n+cuda,tf_mixnet_l,128,1.2369,155.9027,48.6695,1.0921\n+cuda,tinynet_a,128,1.3792,53.0640,40.6346,1.1108\n+cuda,tnt_s_patch16_224,128,3.1078,104.8486,59.6028,1.0660\n+cuda,twins_pcpvt_base,64,1.5921,67.4600,84.4977,1.0909\n+cuda,visformer_small,128,1.1952,72.8705,23.7303,1.0410\n+cuda,vit_base_patch16_224,64,1.1309,56.4866,22.0208,0.9804\n+cuda,volo_d1_224,64,1.6868,72.0957,65.3011,0.9729"}, {"filename": ".github/workflows/torchinductor/data/torchbench.csv", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -0,0 +1,53 @@\n+dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio\n+cuda,BERT_pytorch,16,1.7111,24.2741,35.7065,1.3212\n+cuda,LearningToPaint,96,1.0513,10.7557,11.1879,0.9896\n+cuda,Super_SloMo,6,1.3267,60.4328,28.2097,1.2392\n+cuda,alexnet,128,1.1754,8.3246,5.3319,1.0003\n+cuda,attention_is_all_you_need_pytorch,256,1.3416,36.4401,39.5927,1.1774\n+cuda,dcgan,32,0.9151,2.6249,3.2964,1.0082\n+cuda,densenet121,4,0.9225,51.3747,68.5841,0.9930\n+cuda,doctr_det_predictor,0,0.0000\n+cuda,doctr_reco_predictor,0,0.0000\n+cuda,drq,1,0.9500,3.4884,4.8028,0.9687\n+cuda,fastNLP_Bert,6,1.4328,34.7753,35.4863,1.2368\n+cuda,functorch_dp_cifar10,64,1.2015,8.1625,12.9040,1.0609\n+cuda,functorch_maml_omniglot,1,0.9322,2.5844,3.8640,1.0000\n+cuda,hf_Albert,8,2.1228,30.3377,26.8282,1.2676\n+cuda,hf_Bart,4,1.2899,39.1935,47.2373,1.0080\n+cuda,hf_Bert,4,1.3262,26.1063,35.0281,1.0656\n+cuda,hf_Bert_large,4,1.4163,55.1021,67.2825,1.0915\n+cuda,hf_DistilBert,8,1.4051,21.7191,18.0399,1.0242\n+cuda,hf_GPT2,4,1.6661,26.9039,29.9473,1.1555\n+cuda,hf_Longformer,0,0.0000\n+cuda,hf_Reformer,4,1.1709,64.6979,15.7035,0.9267\n+cuda,hf_T5_large,2,1.7215,107.0798,148.8805,1.1684\n+cuda,lennard_jones,1000,0.8428,1.8488,3.0609,1.0001\n+cuda,maml_omniglot,32,0.9648,2.6869,3.9775,0.9999\n+cuda,mnasnet1_0,32,1.0469,21.6251,25.8232,0.9996\n+cuda,mobilenet_v2,96,1.5604,31.9572,27.0225,1.1734\n+cuda,nvidia_deeprecommender,256,1.0605,9.2080,4.1318,0.9711\n+cuda,phlippe_densenet,128,1.0237,27.5988,28.0400,1.0023\n+cuda,phlippe_resnet,128,1.0493,10.9751,10.2485,1.0092\n+cuda,pytorch_CycleGAN_and_pix2pix,1,1.3724,8.2225,11.9561,1.0219\n+cuda,pytorch_stargan,16,1.1835,11.9178,10.0507,1.0868\n+cuda,pytorch_unet,1,1.3787,29.7543,13.7711,1.0100\n+cuda,resnet152,32,0.9834,63.2446,67.7935,0.9991\n+cuda,resnet18,16,0.9451,9.4977,11.7663,0.9948\n+cuda,resnet50,32,1.0513,24.5141,24.6629,1.0021\n+cuda,resnext50_32x4d,8,0.9216,22.2460,24.3420,0.9984\n+cuda,shufflenet_v2_x1_0,128,1.1943,25.4520,28.8611,1.0951\n+cuda,soft_actor_critic,256,0.8691,1.9637,3.3716,0.9996\n+cuda,speech_transformer,32,1.2718,35.2922,46.9957,1.0897\n+cuda,squeezenet1_1,32,1.1302,8.4540,7.9625,1.0771\n+cuda,timm_efficientdet,1,1.3370,80.0377,120.1814,1.2713\n+cuda,timm_efficientnet,32,1.1874,27.6302,33.9059,1.0971\n+cuda,timm_nfnet,128,1.4525,77.3461,34.3270,1.1056\n+cuda,timm_regnet,32,1.0644,50.6953,35.7562,1.0000\n+cuda,timm_resnest,32,1.6200,14.7763,17.2245,1.0906\n+cuda,timm_vision_transformer,32,1.0800,19.4188,22.0255,0.9966\n+cuda,timm_vision_transformer_large,32,1.0081,393.1742,127.8083,0.9735\n+cuda,timm_vovnet,32,1.1472,22.4727,22.7328,1.0120\n+cuda,torchrec_dlrm,0,0.0000\n+cuda,tts_angular,64,0.8974,6.5057,2.5555,0.9973\n+cuda,vgg16,64,1.2909,50.7405,6.1510,0.9828\n+cuda,yolov3,16,1.2930,54.8069,41.9269,1.0563"}, {"filename": ".github/workflows/torchinductor/scripts/check_acc.py", "status": "added", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -0,0 +1,11 @@\n+import csv\n+import sys\n+\n+file_path = sys.argv[1]\n+with open(file_path) as f:\n+    reader = csv.reader(f)\n+    for i, row in enumerate(reader):\n+        if i == 0:\n+            continue\n+        if row[3] != \"pass\":\n+            print(f\"{row[1]} failed on device {row[0]} with batch size {row[2]}\")"}, {"filename": ".github/workflows/torchinductor/scripts/check_perf.py", "status": "added", "additions": 65, "deletions": 0, "changes": 65, "file_content_changes": "@@ -0,0 +1,65 @@\n+import argparse\n+import csv\n+from collections import namedtuple\n+\n+# Create a named tuple for the output of the benchmark\n+BenchmarkOutput = namedtuple(\n+    'BenchmarkOutput', ['dev', 'name', 'batch_size', 'speedup', 'latency'])\n+\n+\n+def parse_output(file_path: str) -> dict:\n+    entries = {}\n+    with open(file_path) as f:\n+        reader = csv.reader(f)\n+        for i, row in enumerate(reader):\n+            if i == 0 or len(row) < 5:\n+                continue\n+            dev = row[0]\n+            name = row[1]\n+            batch_size = row[2]\n+            speedup = float(row[3])\n+            latency = float(row[4])\n+            entries[name] = BenchmarkOutput(\n+                dev, name, batch_size, speedup, latency)\n+    return entries\n+\n+\n+def compare(baseline: dict, new: dict, threshold: float,\n+            geomean_threshold: float) -> bool:\n+    baseline_geomean = 1.0\n+    new_geomean = 1.0\n+    for key in new:\n+        if key not in baseline:\n+            print(f\"New benchmark {key} not found in baseline\")\n+        baseline_latency = baseline[key].latency\n+        new_latency = new[key].latency\n+        if new_latency < baseline_latency * (1 - threshold):\n+            print(\n+                f\"New benchmark {key} is faster than baseline: {new_latency} vs {baseline_latency}\")\n+        elif new_latency > baseline_latency * (1 + threshold):\n+            print(\n+                f\"New benchmark {key} is slower than baseline: {new_latency} vs {baseline_latency}\")\n+        baseline_geomean *= baseline[key].speedup\n+        new_geomean *= new[key].speedup\n+\n+    baseline_geomean = baseline_geomean ** (1 / len(baseline))\n+    new_geomean = new_geomean ** (1 / len(new))\n+    print(f\"Baseline geomean: {baseline_geomean}\")\n+    print(f\"New geomean: {new_geomean}\")\n+    assert new_geomean > baseline_geomean * (1 - geomean_threshold), \\\n+        f\"New geomean is slower than baseline: {new_geomean} vs {baseline_geomean}\"\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument('--baseline', required=True)\n+    parser.add_argument('--new', required=True)\n+    parser.add_argument('--threshold', type=float, default=0.1)\n+    parser.add_argument('--geomean-threshold', type=float, default=0.02)\n+    args = parser.parse_args()\n+    baseline = parse_output(args.baseline)\n+    new = parse_output(args.new)\n+    compare(baseline, new, args.threshold, args.geomean_threshold)\n+\n+\n+main()"}, {"filename": ".github/workflows/torchinductor/scripts/common.sh", "status": "added", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -0,0 +1,9 @@\n+#!/bin/bash\n+\n+TEST_REPORTS_DIR=/opt/torchinductor_reports\n+PYTORCH_DIR=/opt/pytorch\n+MODELS=(timm_models huggingface torchbench)\n+\n+echo \"$TEST_REPORTS_DIR\"\n+echo \"$PYTORCH_DIR\"\n+echo \"${MODELS[@]}\""}, {"filename": ".github/workflows/torchinductor/scripts/install_torchinductor.sh", "status": "added", "additions": 54, "deletions": 0, "changes": 54, "file_content_changes": "@@ -0,0 +1,54 @@\n+#!/bin/bash\n+\n+# remember where we started\n+ROOT=\"$(pwd)\"\n+\n+# torchinductor venv\n+whoami\n+python3 -m venv /opt/torchinductor_venv\n+# shellcheck source=/dev/null\n+source /opt/torchinductor_venv/bin/activate\n+# shellcheck source=/dev/null\n+source ./.github/workflows/torchinductor/scripts/common.sh\n+\n+# pytorch nightly\n+pip3 install --force-reinstall --pre torch torchtext torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu118\n+# pytorch source to get torchbench for dynamo\n+cd /opt || exit\n+git clone --recursive https://github.com/pytorch/pytorch\n+cd pytorch || exit\n+# if you are updating an existing checkout\n+git submodule sync\n+git submodule update --init --recursive\n+cd ..\n+\n+# required packages\n+pip3 install expecttest psutil\n+\n+# torchbench\n+pip3 install pyyaml\n+git clone https://github.com/pytorch/benchmark.git\n+cd benchmark || exit\n+python3 install.py\n+cd ..\n+\n+# timm\n+git clone https://github.com/huggingface/pytorch-image-models.git\n+cd pytorch-image-models || exit\n+pip3 install -e .\n+cd ..\n+\n+# build our own triton\n+cd \"$ROOT\" || exit\n+cd python || exit\n+rm -rf build\n+pip3 install -e .\n+pip3 uninstall pytorch-triton -y\n+\n+# clean up cache\n+rm -rf /tmp/torchinductor_root/\n+rm -rf ~/.triton/cache\n+rm -rf \"$TEST_REPORTS_DIR\"\n+\n+# go back to where we started\n+cd \"$ROOT\" || exit"}, {"filename": ".github/workflows/torchinductor/scripts/install_triton.sh", "status": "added", "additions": 24, "deletions": 0, "changes": 24, "file_content_changes": "@@ -0,0 +1,24 @@\n+#!/bin/bash\n+\n+# remember where we started\n+ROOT=\"$(pwd)\"\n+\n+# shellcheck source=/dev/null\n+source /opt/torchinductor_venv/bin/activate\n+# shellcheck source=/dev/null\n+source ./.github/workflows/torchinductor/scripts/common.sh\n+\n+# build our own triton\n+cd python || exit\n+pip3 install --pre pytorch-triton --extra-index-url https://download.pytorch.org/whl/nightly/cu118\n+rm -rf build\n+pip3 install -e .\n+pip3 uninstall pytorch-triton -y\n+\n+# clean up cache\n+rm -rf /tmp/torchinductor_root/\n+rm -rf ~/.triton/cache\n+rm -rf \"$TEST_REPORTS_DIR\"\n+\n+# go back to where we started\n+cd \"$ROOT\" || exit"}, {"filename": ".github/workflows/torchinductor/scripts/run_torchinductor_acc.sh", "status": "added", "additions": 35, "deletions": 0, "changes": 35, "file_content_changes": "@@ -0,0 +1,35 @@\n+#!/bin/bash\n+\n+# remember where we started\n+ROOT=\"$(pwd)\"\n+INDUCTOR=\"$ROOT\"/.github/workflows/torchinductor\n+\n+# shellcheck source=/dev/null\n+source /opt/torchinductor_venv/bin/activate\n+# shellcheck source=/dev/null\n+source \"$INDUCTOR\"/scripts/common.sh\n+\n+cd \"$PYTORCH_DIR\" || exit\n+TEST_REPORTS_DIR=$TEST_REPORTS_DIR/acc\n+mkdir -p \"$TEST_REPORTS_DIR\"\n+\n+for model in \"${MODELS[@]}\"; do\n+  echo \"Running accuracy test for $model\"\n+  python3 benchmarks/dynamo/\"$model\".py --ci --accuracy --timing --explain --inductor --device cuda \\\n+    --output \"$TEST_REPORTS_DIR\"/inference_\"$model\".csv\n+  python3 benchmarks/dynamo/\"$model\".py --ci --accuracy --timing --explain --inductor --training --amp --device cuda \\\n+    --output \"$TEST_REPORTS_DIR\"/training_\"$model\".csv\n+  python3 benchmarks/dynamo/\"$model\".py --ci --accuracy --timing --explain --inductor --dynamic-shapes --device cuda \\\n+    --output \"$TEST_REPORTS_DIR\"/dynamic_shapes_\"$model\".csv\n+done\n+\n+cd \"$ROOT\" || exit\n+for model in \"${MODELS[@]}\"; do\n+  echo \"Checking accuracy test for $model\"\n+  python3 \"$INDUCTOR\"/scripts/check_acc.py \"$TEST_REPORTS_DIR\"/inference_\"$model\".csv\n+  python3 \"$INDUCTOR\"/scripts/check_acc.py \"$TEST_REPORTS_DIR\"/training_\"$model\".csv\n+  python3 \"$INDUCTOR\"/scripts/check_acc.py \"$TEST_REPORTS_DIR\"/dynamic_shapes_\"$model\".csv\n+done\n+\n+# go back to where we started\n+cd \"$ROOT\" || exit"}, {"filename": ".github/workflows/torchinductor/scripts/run_torchinductor_perf.sh", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "file_content_changes": "@@ -0,0 +1,41 @@\n+#!/bin/bash\n+\n+# remember where we started\n+ROOT=\"$(pwd)\"\n+INDUCTOR=\"$ROOT\"/.github/workflows/torchinductor\n+\n+# shellcheck source=/dev/null\n+source /opt/torchinductor_venv/bin/activate\n+# shellcheck source=/dev/null\n+source \"$INDUCTOR\"/scripts/common.sh\n+\n+# lock GPU clocks to 1350 MHz\n+sudo nvidia-smi -i 0 -pm 1\n+sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n+\n+cd \"$PYTORCH_DIR\" || exit\n+TEST_REPORTS_DIR=$TEST_REPORTS_DIR/perf\n+mkdir -p \"$TEST_REPORTS_DIR\"\n+\n+for model in \"${MODELS[@]}\"; do\n+  echo \"Running performance test for $model\"\n+  python3 benchmarks/dynamo/\"$model\".py --ci --training --performance --disable-cudagraphs\\\n+    --device cuda --inductor --amp --output \"$TEST_REPORTS_DIR\"/\"$model\".csv\n+done\n+\n+cd \"$ROOT\" || exit\n+for model in \"${MODELS[@]}\"; do\n+  echo \"Checking performance test for $model\"\n+  python3 \"$INDUCTOR\"/scripts/check_perf.py --new \"$TEST_REPORTS_DIR\"/\"$model\".csv --baseline \"$INDUCTOR\"/data/\"$model\".csv\n+  EXIT_STATUS=$?\n+  if [ \"$EXIT_STATUS\" -ne 0 ]; then\n+    echo \"Performance test for $model failed\"\n+    exit \"$EXIT_STATUS\"\n+  fi\n+done\n+\n+# unlock GPU clocks\n+sudo nvidia-smi -i 0 -rgc\n+\n+# go back to where we started\n+cd \"$ROOT\" || exit"}, {"filename": ".github/workflows/wheels.yml", "status": "modified", "additions": 32, "deletions": 13, "changes": 45, "file_content_changes": "@@ -1,40 +1,59 @@\n name: Wheels\n on:\n   workflow_dispatch:\n-  schedule:    \n-    - cron: \"0 0 * * *\"\n+  schedule:\n+    - cron: \"0 2 * * *\"\n \n jobs:\n \n   Build-Wheels:\n-    \n+\n     runs-on: [self-hosted, V100]\n+    permissions:\n+      id-token: write\n+      contents: read\n \n     steps:\n \n       - name: Checkout\n         uses: actions/checkout@v3\n \n+      - name: Install Azure CLI\n+        run: |\n+          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n+\n+      - name: Azure login\n+        uses: azure/login@v1\n+        with:\n+          client-id: ${{ secrets.AZURE_CLIENT_ID }}\n+          tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n+          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n+\n+      - id: generate-token\n+        name: Generate token\n+        run: |\n+          AZ_TOKEN=$(az account get-access-token --query accessToken)\n+          echo \"::add-mask::$AZ_TOKEN\"\n+          echo \"access_token=$AZ_TOKEN\" >> \"$GITHUB_OUTPUT\"\n+\n       - name: Patch setup.py\n         run: |\n-          #sed -i 's/name\\=\\\"triton\\\"/name=\"triton-nightly\"/g' python/setup.py\n-          export LATEST_DATE=$(TZ=UTC0 git show --quiet --date='format-local:%Y%m%d' --format=\"%cd\")\n+          sed -i 's/name\\=\\\"triton\\\"/name=\"triton-nightly\"/g' python/setup.py\n+          export LATEST_DATE=$(TZ=UTC0 git show --quiet --date='format-local:%Y%m%d%H%M%S' --format=\"%cd\")\n           sed -i -r \"s/version\\=\\\"(.*)\\\"/version=\\\"\\1-dev\"$LATEST_DATE\"\\\"/g\" python/setup.py\n           echo \"\" >> python/setup.cfg\n           echo \"[build_ext]\" >> python/setup.cfg\n           echo \"base-dir=/project\" >> python/setup.cfg\n \n       - name: Build wheels\n         run: |\n-          export CIBW_MANYLINUX_X86_64_IMAGE=\"manylinux2014\"\n-          export CIBW_MANYLINUX_PYPY_X86_64_IMAGE=\"manylinux2014\"\n-          export CIBW_BEFORE_BUILD=\"pip install cmake;\\\n-                                    yum install -y llvm11 llvm11-devel llvm11-static llvm11-libs zlib-devel;\"\n-          export CIBW_SKIP=\"{cp,pp}35-*\"\n+          export CIBW_MANYLINUX_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n+          #export CIBW_MANYLINUX_PYPY_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n+          export CIBW_BEFORE_BUILD=\"pip install cmake;\"\n+          export CIBW_SKIP=\"{cp,pp}{35,36}-*\"\n           export CIBW_BUILD=\"{cp,pp}3*-manylinux_x86_64\"\n           python3 -m cibuildwheel python --output-dir wheelhouse\n \n-\n-      - name: Upload wheels to PyPI\n+      - name: Publish wheels to Azure DevOps\n         run: |\n-          python3 -m twine upload wheelhouse/* --skip-existing\n\\ No newline at end of file\n+          python3 -m twine upload -r Triton-Nightly -u TritonArtifactsSP -p ${{ steps.generate-token.outputs.access_token }} --config-file utils/nightly.pypirc --non-interactive --verbose wheelhouse/*"}, {"filename": ".gitignore", "status": "modified", "additions": 7, "deletions": 1, "changes": 8, "file_content_changes": "@@ -8,9 +8,15 @@ python/triton/_C/libtriton.pyd\n python/triton/_C/libtriton.so\n \n # Python caches\n-__pycache__\n+__pycache__/\n+*.py[cod]\n .pytest_cache\n \n+# Environments\n+.venv\n+venv/\n+venv.bak/\n+\n # VS Code project files\n .vscode\n .vs"}, {"filename": ".gitmodules", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -1,3 +1,3 @@\n-[submodule \"deps/dlfcn-win32\"]\n-\tpath = deps/dlfcn-win32\n-\turl = https://github.com/dlfcn-win32/dlfcn-win32.git\n+[submodule \"third_party/intel_xpu_backend\"]\n+\tpath = third_party/intel_xpu_backend\n+\turl = http://github.com/intel/intel-xpu-backend-for-triton"}, {"filename": ".hypothesis/unicode_data/13.0.0/charmap.json.gz", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": ".pre-commit-config.yaml", "status": "added", "additions": 61, "deletions": 0, "changes": 61, "file_content_changes": "@@ -0,0 +1,61 @@\n+repos:\n+  - repo: https://github.com/pre-commit/pre-commit-hooks\n+    rev: v4.4.0\n+    hooks:\n+      - id: check-symlinks\n+      - id: destroyed-symlinks\n+      - id: trailing-whitespace\n+      - id: end-of-file-fixer\n+      - id: check-yaml\n+      - id: check-toml\n+      - id: check-ast\n+      - id: check-added-large-files\n+      - id: check-merge-conflict\n+      - id: check-executables-have-shebangs\n+      - id: check-shebang-scripts-are-executable\n+      - id: detect-private-key\n+      - id: debug-statements\n+  - repo: https://github.com/PyCQA/isort\n+    rev: 5.12.0\n+    hooks:\n+      - id: isort\n+        stages: [commit, push, manual]\n+  - repo: https://github.com/pre-commit/mirrors-autopep8\n+    rev: v1.6.0\n+    hooks:\n+      - id: autopep8\n+        args: [\"-i\"]\n+        stages: [commit, push, manual]\n+  - repo: https://github.com/pycqa/flake8\n+    rev: 6.0.0\n+    hooks:\n+      - id: flake8\n+        # TODO: uncomment this to enable more flake8 plugins\n+        # additional_dependencies:\n+        #   - flake8-bugbear\n+        #   - flake8-comprehensions\n+        #   - flake8-docstrings\n+        #   - flake8-pyi\n+        #   - flake8-simplify\n+        stages: [commit, push, manual]\n+        exclude: |\n+          (?x)(\n+            ^test/|\n+            ^docs/conf.py$\n+          )\n+  - repo: https://github.com/pre-commit/mirrors-clang-format\n+    rev: v14.0.6\n+    hooks:\n+      - id: clang-format\n+        stages: [commit, push, manual]\n+        exclude: |\n+          (?x)(\n+            ^include/triton/external/|\n+            ^python/triton/third_party/\n+          )\n+\n+exclude: |\n+  (?x)(\n+    ^include/triton/external/|\n+    ^python/triton/third_party/\n+  )"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 83, "deletions": 50, "changes": 133, "file_content_changes": "@@ -1,4 +1,11 @@\n-cmake_minimum_required(VERSION 3.6)\n+cmake_minimum_required(VERSION 3.18)\n+\n+if(POLICY CMP0116)\n+# Introduced in cmake 3.20\n+# https://cmake.org/cmake/help/latest/policy/CMP0116.html\n+  cmake_policy(SET CMP0116 OLD)\n+endif()\n+\n include(ExternalProject)\n \n set(CMAKE_CXX_STANDARD 17)\n@@ -7,17 +14,18 @@ set(CMAKE_INCLUDE_CURRENT_DIR ON)\n \n project(triton)\n include(CTest)\n+\n if(NOT WIN32)\n   list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_SOURCE_DIR}/cmake\")\n endif()\n \n # Options\n option(TRITON_BUILD_TUTORIALS \"Build C++ Triton tutorials\" ON)\n option(TRITON_BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n+set(TRITON_CODEGEN_BACKENDS \"\" CACHE STRING \"Enable different codegen backends\")\n \n # Ensure Python3 vars are set correctly\n-#  used conditionally in this file and by lit tests\n-find_package(Python3 REQUIRED COMPONENTS Development Interpreter)\n+# used conditionally in this file and by lit tests\n \n # Customized release build type with assertions: TritonRelBuildWithAsserts\n set(CMAKE_C_FLAGS_TRITONRELBUILDWITHASSERTS \"-O2 -g\")\n@@ -30,7 +38,7 @@ if(NOT CMAKE_BUILD_TYPE)\n endif()\n \n if(NOT WIN32)\n-    find_library(TERMINFO_LIBRARY tinfo)\n+  find_library(TERMINFO_LIBRARY tinfo)\n endif()\n \n # Compiler flags\n@@ -39,23 +47,16 @@ include_directories(${CMAKE_CURRENT_SOURCE_DIR}/include)\n # Third-party\n include_directories(${PYBIND11_INCLUDE_DIR})\n \n-if(WIN32)\n-    SET(BUILD_SHARED_LIBS OFF)\n-    include_directories(${CMAKE_CURRENT_SOURCE_DIR}/deps/dlfcn-win32/src)\n-    add_subdirectory(deps/dlfcn-win32/src ${CMAKE_BINARY_DIR}/dlfcn-win32)\n-endif()\n-\n set(CMAKE_CXX_FLAGS \"${CMAKE_C_FLAGS} -D__STDC_FORMAT_MACROS  -fPIC -std=gnu++17 -fvisibility=hidden -fvisibility-inlines-hidden\")\n+\n if(APPLE)\n   set(CMAKE_OSX_DEPLOYMENT_TARGET 11.6)\n endif()\n \n-\n-\n-##########\n+# #########\n # LLVM\n-##########\n-if (NOT MLIR_DIR)\n+# #########\n+if(NOT MLIR_DIR)\n   if(NOT LLVM_LIBRARY_DIR)\n     if(WIN32)\n       find_package(LLVM 13 REQUIRED COMPONENTS nvptx amdgpu)\n@@ -71,10 +72,16 @@ if (NOT MLIR_DIR)\n     else()\n       find_package(LLVM 11 REQUIRED COMPONENTS \"nvptx;amdgpu\")\n     endif()\n+\n     message(STATUS \"Found LLVM ${LLVM_PACKAGE_VERSION}\")\n+\n+    # FindLLVM outputs LLVM_LIBRARY_DIRS but we expect LLVM_LIBRARY_DIR here\n+    set(LLVM_LIBRARY_DIR ${LLVM_LIBRARY_DIRS})\n+\n     if(APPLE)\n       set(CMAKE_OSX_DEPLOYMENT_TARGET \"10.14\")\n     endif()\n+\n   # sometimes we don't want to use llvm-config, since it may have been downloaded for some specific linux distros\n   else()\n     set(LLVM_LDFLAGS \"-L${LLVM_LIBRARY_DIR}\")\n@@ -134,37 +141,38 @@ if (NOT MLIR_DIR)\n       libLLVMAnalysis.a\n     )\n   endif()\n-  set (MLIR_DIR ${LLVM_LIBRARY_DIR}/cmake/mlir)\n+\n+  set(MLIR_DIR ${LLVM_LIBRARY_DIR}/cmake/mlir)\n endif()\n \n # Python module\n if(TRITON_BUILD_PYTHON_MODULE)\n-    message(STATUS \"Adding Python module\")\n-    set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n-    set(PYTHON_SRC ${PYTHON_SRC_PATH}/main.cc ${PYTHON_SRC_PATH}/triton.cc)\n-    include_directories(\".\" ${PYTHON_SRC_PATH})\n-    if (PYTHON_INCLUDE_DIRS)\n-      include_directories(${PYTHON_INCLUDE_DIRS})\n-    else()\n-      include_directories(${Python3_INCLUDE_DIRS})\n-      link_directories(${Python3_LIBRARY_DIRS})\n-      link_libraries(${Python3_LIBRARIES})\n-      add_link_options(${Python3_LINK_OPTIONS})\n-    endif()\n-endif()\n+  message(STATUS \"Adding Python module\")\n+  set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n+  set(PYTHON_SRC ${PYTHON_SRC_PATH}/main.cc ${PYTHON_SRC_PATH}/triton.cc)\n+  include_directories(\".\" ${PYTHON_SRC_PATH})\n \n+  if(PYTHON_INCLUDE_DIRS)\n+    include_directories(${PYTHON_INCLUDE_DIRS})\n+  else()\n+    find_package(Python3 REQUIRED COMPONENTS Development Interpreter)\n+    include_directories(${Python3_INCLUDE_DIRS})\n+    link_directories(${Python3_LIBRARY_DIRS})\n+    link_libraries(${Python3_LIBRARIES})\n+    add_link_options(${Python3_LINK_OPTIONS})\n+  endif()\n+endif()\n \n # # Triton\n # file(GLOB_RECURSE LIBTRITON_SRC lib/*.cc)\n # if (WIN32 AND TRITON_BUILD_PYTHON_MODULE)\n-#     Python3_add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n-#     set_target_properties(triton PROPERTIES SUFFIX \".pyd\")\n-#     set_target_properties(triton PROPERTIES PREFIX \"lib\")\n+# Python3_add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n+# set_target_properties(triton PROPERTIES SUFFIX \".pyd\")\n+# set_target_properties(triton PROPERTIES PREFIX \"lib\")\n # else()\n-#     add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n+# add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n # endif()\n \n-\n # MLIR\n find_package(MLIR REQUIRED CONFIG PATHS ${MLIR_DIR})\n \n@@ -182,14 +190,13 @@ include_directories(${MLIR_INCLUDE_DIRS})\n include_directories(${LLVM_INCLUDE_DIRS})\n include_directories(${PROJECT_SOURCE_DIR}/include)\n include_directories(${PROJECT_BINARY_DIR}/include) # Tablegen'd files\n-# link_directories(${LLVM_LIBRARY_DIR})\n \n+# link_directories(${LLVM_LIBRARY_DIR})\n add_subdirectory(include)\n add_subdirectory(lib)\n add_subdirectory(bin)\n \n # find_package(PythonLibs REQUIRED)\n-\n set(TRITON_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}\")\n set(TRITON_BINARY_DIR \"${CMAKE_CURRENT_BINARY_DIR}\")\n \n@@ -198,45 +205,71 @@ get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)\n \n if(TRITON_BUILD_PYTHON_MODULE)\n   add_library(triton SHARED ${PYTHON_SRC})\n-\n-  target_link_libraries(triton\n+  set(TRITON_LIBRARIES\n     TritonAnalysis\n     TritonTransforms\n     TritonGPUTransforms\n     TritonLLVMIR\n     TritonPTX\n+    TritonHSACO\n     ${dialect_libs}\n     ${conversion_libs}\n+\n     # optimizations\n+    MLIRBytecodeWriter\n     MLIRPass\n     MLIRTransforms\n-    MLIRLLVMIR\n+    MLIRLLVMDialect\n     MLIRSupport\n     MLIRTargetLLVMIRExport\n     MLIRExecutionEngine\n     MLIRMathToLLVM\n     MLIRNVVMToLLVMIRTranslation\n+    MLIRROCDLToLLVMIRTranslation\n     MLIRIR\n   )\n \n-  target_link_options(triton PRIVATE ${LLVM_LDFLAGS})\n-\n   if(WIN32)\n-      target_link_libraries(triton PRIVATE ${LLVM_LIBRARIES} dl) # dl is from dlfcn-win32\n+    target_link_libraries(triton PRIVATE ${LLVM_LIBRARIES} ${CMAKE_DL_LIBS}\n+      ${TRITON_LIBRARIES}\n+    )\n   elseif(APPLE)\n-      target_link_libraries(triton ${LLVM_LIBRARIES} z)\n+    target_link_libraries(triton ${LLVM_LIBRARIES} z\n+      ${TRITON_LIBRARIES}\n+    )\n   else()\n-      target_link_libraries(triton ${LLVM_LIBRARIES} z stdc++fs)\n+    target_link_libraries(triton ${LLVM_LIBRARIES} z\n+      ${TRITON_LIBRARIES}\n+    )\n+    # TODO: Figure out which target is sufficient to fix errors; triton is\n+    # apparently not enough\n+    link_libraries(stdc++fs)\n   endif()\n+\n+  target_link_options(triton PRIVATE ${LLVM_LDFLAGS})\n+endif()\n+\n+if(UNIX AND NOT APPLE)\n+  set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -Wl,--exclude-libs,ALL\")\n endif()\n \n if(TRITON_BUILD_PYTHON_MODULE AND NOT WIN32)\n-    set(CMAKE_SHARED_LIBRARY_SUFFIX \".so\")\n-    # Check if the platform is MacOS\n-    if(APPLE)\n-        set(PYTHON_LDFLAGS \"-undefined dynamic_lookup -flto\")\n-    endif()\n-    target_link_libraries(triton ${CUTLASS_LIBRARIES} ${PYTHON_LDFLAGS})\n+  set(CMAKE_SHARED_LIBRARY_SUFFIX \".so\")\n+\n+  # Check if the platform is MacOS\n+  if(APPLE)\n+    set(PYTHON_LDFLAGS \"-undefined dynamic_lookup -flto\")\n+  endif()\n+\n+  target_link_libraries(triton ${CUTLASS_LIBRARIES} ${PYTHON_LDFLAGS})\n+endif()\n+\n+list(LENGTH TRITON_CODEGEN_BACKENDS CODEGEN_BACKENDS_LEN)\n+if (${CODEGEN_BACKENDS_LEN} GREATER 0)\n+  set(PYTHON_THIRD_PARTY_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/triton/third_party)\n+  foreach(CODEGEN_BACKEND ${TRITON_CODEGEN_BACKENDS})\n+    add_subdirectory(third_party/${CODEGEN_BACKEND})\n+  endforeach()\n endif()\n \n add_subdirectory(test)"}, {"filename": "CONTRIBUTING.md", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -0,0 +1,53 @@\n+# Triton Programming Language Contributor's Guide\n+\n+First of all, thank you for considering contributing to the Triton programming language! We appreciate the time and effort you're willing to put into improving and expanding our project. In order to maintain a high standard of code and a welcoming atmosphere for collaboration, we kindly ask you to follow the guidelines outlined below.\n+\n+## General Guidelines\n+\n+1. **Quality Contributions:** We value meaningful contributions that aim to improve the project and help it grow. Please refrain from submitting low-effort pull requests (PR) -- such as minor formatting/typo fixes -- solely for the purpose of appearing in the commit history. Maintainers have limited bandwidth, and may decline to review such work.\n+\n+2. **Code Formatting:** Our Continuous Integration (CI) pipeline uses autopep8, isort, and clang-format to check code formatting. To avoid failing the CI workflow due to formatting issues, please utilize the provided `.pre-commit-config.yaml` pre-commit configuration file.\n+\n+3. **Unit Testing:** When contributing new functionalities, please also include appropriate tests. We aim to continuously improve and expand our CI pipeline to ensure the robustness and reliability of the project. PRs that add a large amount of untested code will be rejected.\n+\n+4. **Respectful Communication:** In all discussions related to PRs or other contributions, please maintain a courteous and civil tone. We strive to foster a collaborative environment that is inclusive and respectful to all contributors.\n+\n+\n+## Request for Comments (RFCs)\n+\n+RFCs are a crucial aspect of the collaborative development process, as they provide a structured way to propose and discuss significant changes or additions to the project. RFCs may encompass modifications to the language itself, extensive changes in the compiler backend, or other substantial updates that impact the Triton ecosystem.\n+\n+To ensure that RFCs are clear and easy to understand, consider the following guidelines when creating one:\n+\n+### Purpose\n+\n+The purpose of an RFC is to:\n+\n+- Clearly communicate your proposal to the Triton community\n+- Collect feedback from maintainers and other contributors\n+- Provide a platform for discussing and refining ideas\n+- Reach a consensus on the best approach for implementing the proposed changes\n+\n+### Structure\n+\n+A well-structured RFC should include:\n+\n+1. **Title:** A concise and descriptive title that reflects the main topic of the proposal.\n+\n+2. **Summary:** A brief overview of the proposed changes, including the motivation behind them and their intended impact on the project.\n+\n+3. **Detailed Design:** A thorough description of the proposed changes, including:\n+   - Technical details and implementation approach\n+   - Any new or modified components, functions, or data structures\n+   - Any potential challenges or limitations, as well as proposed solutions\n+\n+4. **Examples and Use Cases:** Provide examples of how the proposed changes would be used in real-world scenarios, as well as any use cases that demonstrate the benefits of the changes.\n+\n+5. **Performance Impact:** Discuss the expected performance impact of the proposed changes, including any potential bottlenecks or performance improvements.\n+\n+6. **Timeline and Milestones:** Outline a proposed timeline for implementing the changes, including any milestones or intermediate steps.\n+\n+\n+## New backends\n+\n+Due to limited resources, we need to prioritize the number of targets we support. We are committed to providing upstream support for Nvidia and AMD GPUs. However, if you wish to contribute support for other backends, please start your project in a fork. If your backend proves to be useful and meets our performance requirements, we will discuss the possibility of upstreaming it."}, {"filename": "LICENSE", "status": "modified", "additions": 15, "deletions": 15, "changes": 30, "file_content_changes": "@@ -1,23 +1,23 @@\n-/* \n+/*\n * Copyright 2018-2020 Philippe Tillet\n * Copyright 2020-2022 OpenAI\n-* \n-* Permission is hereby granted, free of charge, to any person obtaining \n-* a copy of this software and associated documentation files \n-* (the \"Software\"), to deal in the Software without restriction, \n-* including without limitation the rights to use, copy, modify, merge, \n-* publish, distribute, sublicense, and/or sell copies of the Software, \n-* and to permit persons to whom the Software is furnished to do so, \n+*\n+* Permission is hereby granted, free of charge, to any person obtaining\n+* a copy of this software and associated documentation files\n+* (the \"Software\"), to deal in the Software without restriction,\n+* including without limitation the rights to use, copy, modify, merge,\n+* publish, distribute, sublicense, and/or sell copies of the Software,\n+* and to permit persons to whom the Software is furnished to do so,\n * subject to the following conditions:\n-* \n-* The above copyright notice and this permission notice shall be \n+*\n+* The above copyright notice and this permission notice shall be\n * included in all copies or substantial portions of the Software.\n-* \n-* THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, \n-* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF \n+*\n+* THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n-* CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, \n-* TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE \n+* CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+* TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n */"}, {"filename": "README.md", "status": "modified", "additions": 10, "deletions": 16, "changes": 26, "file_content_changes": "@@ -2,7 +2,7 @@\n   <img src=\"https://cdn.openai.com/triton/assets/triton-logo.png\" alt=\"Triton logo\" width=\"88\" height=\"100\">\n </div>\n \n-[![Wheels](https://github.com/openai/triton/actions/workflows/wheels.yml/badge.svg)](https://github.com/openai/triton/actions/workflows/wheels.yml)\n+[![Wheels](https://github.com/openai/triton/actions/workflows/wheels.yml/badge.svg?branch=release/2.0.x)](https://github.com/openai/triton/actions/workflows/wheels.yml)\n \n \n **`Documentation`** |\n@@ -25,39 +25,37 @@ You can install the latest stable release of Triton from pip:\n ```bash\n pip install triton\n ```\n-Binary wheels are available for CPython 3.6-3.9 and PyPy 3.6-3.7.\n+Binary wheels are available for CPython 3.8-3.11 and PyPy 3.8-3.9.\n \n And the latest nightly release:\n \n ```bash\n-pip install -U --pre triton\n+pip install -U --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly\n ```\n \n # Install from source\n \n ```\n git clone https://github.com/openai/triton.git;\n cd triton/python;\n-pip install cmake; # build time dependency\n+pip install cmake; # build-time dependency\n pip install -e .\n ```\n \n # Changelog\n \n-Version 1.1 is out! New features include:\n-- Many, many bugfixes\n-- More documentation\n-- Automatic on-disk caching of compiled binary objects\n-- Random Number Generation\n-- Faster (up to 2x on A100), cleaner blocksparse ops\n+Version 2.0 is out! New features include:\n+- Many, many bug fixes\n+- Performance improvements\n+- Backend rewritten to use MLIR\n+- Support for kernels that contain back-to-back matmuls (e.g., flash attention)\n \n # Contributing\n \n-Community contributions are more than welcome, whether it be to fix bugs or to add new features. Feel free to open GitHub issues about your contribution ideas, and we will review them. A contributor's guide containing general guidelines is coming soon!\n+Community contributions are more than welcome, whether it be to fix bugs or to add new features at [github](https://github.com/openai/triton/). For more detailed instructions, please visit our [contributor's guide](CONTRIBUTING.md).\n \n If you\u2019re interested in joining our team and working on Triton & GPU kernels, [we\u2019re hiring](https://openai.com/jobs/#acceleration)!\n \n-\n # Compatibility\n \n Supported Platforms:\n@@ -66,7 +64,3 @@ Supported Platforms:\n Supported Hardware:\n   * NVIDIA GPUs (Compute Capability 7.0+)\n   * Under development: AMD GPUs, CPUs\n-\n-# Disclaimer\n-\n-Triton is a fairly recent project, and it is under active development. We expect it to be pretty useful in a wide variety of cases, but don't be surprised if it's a bit rough around the edges :)"}, {"filename": "bin/CMakeLists.txt", "status": "modified", "additions": 23, "deletions": 5, "changes": 28, "file_content_changes": "@@ -1,7 +1,3 @@\n-add_subdirectory(FileCheck)\n-# add_llvm_executable(FileCheck FileCheck/FileCheck.cpp)\n-# target_link_libraries(FileCheck PRIVATE LLVMFileCheck LLVMSupport)\n-\n get_property(dialect_libs GLOBAL PROPERTY MLIR_DIALECT_LIBS)\n get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)\n \n@@ -25,6 +21,26 @@ target_link_libraries(triton-opt PRIVATE\n \n mlir_check_all_link_libraries(triton-opt)\n \n+add_llvm_executable(triton-reduce triton-reduce.cpp PARTIAL_SOURCES_INTENDED)\n+mlir_check_all_link_libraries(triton-reduce)\n+\n+llvm_update_compile_flags(triton-reduce)\n+target_link_libraries(triton-reduce PRIVATE\n+  TritonAnalysis\n+  TritonTransforms\n+  TritonGPUTransforms\n+  ${dialect_libs}\n+  ${conversion_libs}\n+  # tests\n+  TritonTestAnalysis\n+  # MLIR core\n+  MLIRReduceLib\n+  MLIRPass\n+  MLIRTransforms\n+)\n+\n+mlir_check_all_link_libraries(triton-reduce)\n+\n \n add_llvm_executable(triton-translate triton-translate.cpp PARTIAL_SOURCES_INTENDED)\n llvm_update_compile_flags(triton-translate)\n@@ -34,6 +50,7 @@ llvm_update_compile_flags(triton-translate)\n          TritonGPUTransforms\n          TritonLLVMIR\n          TritonPTX\n+         TritonHSACO\n          ${dialect_libs}\n          ${conversion_libs}\n          # tests\n@@ -48,7 +65,7 @@ llvm_update_compile_flags(triton-translate)\n          # MLIR core\n          MLIROptLib\n          MLIRIR\n-         MLIRLLVMIR\n+         MLIRLLVMDialect\n          MLIRPass\n          MLIRSupport\n          MLIRTransforms\n@@ -57,5 +74,6 @@ llvm_update_compile_flags(triton-translate)\n          MLIRTransformUtils\n          MLIRLLVMToLLVMIRTranslation\n          MLIRNVVMToLLVMIRTranslation\n+         MLIRROCDLToLLVMIRTranslation\n          )\n mlir_check_all_link_libraries(triton-translate)"}, {"filename": "bin/FileCheck/CMakeLists.txt", "status": "removed", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -1,2 +0,0 @@\n-add_llvm_executable(FileCheck FileCheck.cpp)\n-target_link_libraries(FileCheck PRIVATE LLVMFileCheck LLVMSupport)\n\\ No newline at end of file"}, {"filename": "bin/FileCheck/FileCheck.cpp", "status": "removed", "additions": 0, "deletions": 882, "changes": 882, "file_content_changes": "@@ -1,882 +0,0 @@\n-//===- FileCheck.cpp - Check that File's Contents match what is expected --===//\n-//\n-// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n-// See https://llvm.org/LICENSE.txt for license information.\n-// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n-//\n-//===----------------------------------------------------------------------===//\n-//\n-// FileCheck does a line-by line check of a file that validates whether it\n-// contains the expected content.  This is useful for regression tests etc.\n-//\n-// This program exits with an exit status of 2 on error, exit status of 0 if\n-// the file matched the expected contents, and exit status of 1 if it did not\n-// contain the expected contents.\n-//\n-//===----------------------------------------------------------------------===//\n-\n-#include \"llvm/FileCheck/FileCheck.h\"\n-#include \"llvm/Support/CommandLine.h\"\n-#include \"llvm/Support/InitLLVM.h\"\n-#include \"llvm/Support/Process.h\"\n-#include \"llvm/Support/WithColor.h\"\n-#include \"llvm/Support/raw_ostream.h\"\n-#include <cmath>\n-#include <map>\n-using namespace llvm;\n-\n-static cl::extrahelp FileCheckOptsEnv(\n-    \"\\nOptions are parsed from the environment variable FILECHECK_OPTS and\\n\"\n-    \"from the command line.\\n\");\n-\n-static cl::opt<std::string>\n-    CheckFilename(cl::Positional, cl::desc(\"<check-file>\"), cl::Optional);\n-\n-static cl::opt<std::string>\n-    InputFilename(\"input-file\", cl::desc(\"File to check (defaults to stdin)\"),\n-                  cl::init(\"-\"), cl::value_desc(\"filename\"));\n-\n-static cl::list<std::string> CheckPrefixes(\n-    \"check-prefix\",\n-    cl::desc(\"Prefix to use from check file (defaults to 'CHECK')\"));\n-static cl::alias CheckPrefixesAlias(\n-    \"check-prefixes\", cl::aliasopt(CheckPrefixes), cl::CommaSeparated,\n-    cl::NotHidden,\n-    cl::desc(\n-        \"Alias for -check-prefix permitting multiple comma separated values\"));\n-\n-static cl::list<std::string> CommentPrefixes(\n-    \"comment-prefixes\", cl::CommaSeparated, cl::Hidden,\n-    cl::desc(\"Comma-separated list of comment prefixes to use from check file\\n\"\n-             \"(defaults to 'COM,RUN'). Please avoid using this feature in\\n\"\n-             \"LLVM's LIT-based test suites, which should be easier to\\n\"\n-             \"maintain if they all follow a consistent comment style. This\\n\"\n-             \"feature is meant for non-LIT test suites using FileCheck.\"));\n-\n-static cl::opt<bool> NoCanonicalizeWhiteSpace(\n-    \"strict-whitespace\",\n-    cl::desc(\"Do not treat all horizontal whitespace as equivalent\"));\n-\n-static cl::opt<bool> IgnoreCase(\"ignore-case\",\n-                                cl::desc(\"Use case-insensitive matching\"));\n-\n-static cl::list<std::string> ImplicitCheckNot(\n-    \"implicit-check-not\",\n-    cl::desc(\"Add an implicit negative check with this pattern to every\\n\"\n-             \"positive check. This can be used to ensure that no instances of\\n\"\n-             \"this pattern occur which are not matched by a positive pattern\"),\n-    cl::value_desc(\"pattern\"));\n-\n-static cl::list<std::string>\n-    GlobalDefines(\"D\", cl::AlwaysPrefix,\n-                  cl::desc(\"Define a variable to be used in capture patterns.\"),\n-                  cl::value_desc(\"VAR=VALUE\"));\n-\n-static cl::opt<bool> AllowEmptyInput(\n-    \"allow-empty\", cl::init(false),\n-    cl::desc(\"Allow the input file to be empty. This is useful when making\\n\"\n-             \"checks that some error message does not occur, for example.\"));\n-\n-static cl::opt<bool> AllowUnusedPrefixes(\n-    \"allow-unused-prefixes\", cl::init(false), cl::ZeroOrMore,\n-    cl::desc(\"Allow prefixes to be specified but not appear in the test.\"));\n-\n-static cl::opt<bool> MatchFullLines(\n-    \"match-full-lines\", cl::init(false),\n-    cl::desc(\"Require all positive matches to cover an entire input line.\\n\"\n-             \"Allows leading and trailing whitespace if --strict-whitespace\\n\"\n-             \"is not also passed.\"));\n-\n-static cl::opt<bool> EnableVarScope(\n-    \"enable-var-scope\", cl::init(false),\n-    cl::desc(\"Enables scope for regex variables. Variables with names that\\n\"\n-             \"do not start with '$' will be reset at the beginning of\\n\"\n-             \"each CHECK-LABEL block.\"));\n-\n-static cl::opt<bool> AllowDeprecatedDagOverlap(\n-    \"allow-deprecated-dag-overlap\", cl::init(false),\n-    cl::desc(\"Enable overlapping among matches in a group of consecutive\\n\"\n-             \"CHECK-DAG directives.  This option is deprecated and is only\\n\"\n-             \"provided for convenience as old tests are migrated to the new\\n\"\n-             \"non-overlapping CHECK-DAG implementation.\\n\"));\n-\n-static cl::opt<bool> Verbose(\n-    \"v\", cl::init(false), cl::ZeroOrMore,\n-    cl::desc(\"Print directive pattern matches, or add them to the input dump\\n\"\n-             \"if enabled.\\n\"));\n-\n-static cl::opt<bool> VerboseVerbose(\n-    \"vv\", cl::init(false), cl::ZeroOrMore,\n-    cl::desc(\"Print information helpful in diagnosing internal FileCheck\\n\"\n-             \"issues, or add it to the input dump if enabled.  Implies\\n\"\n-             \"-v.\\n\"));\n-\n-// The order of DumpInputValue members affects their precedence, as documented\n-// for -dump-input below.\n-enum DumpInputValue {\n-  DumpInputNever,\n-  DumpInputFail,\n-  DumpInputAlways,\n-  DumpInputHelp\n-};\n-\n-static cl::list<DumpInputValue> DumpInputs(\n-    \"dump-input\",\n-    cl::desc(\"Dump input to stderr, adding annotations representing\\n\"\n-             \"currently enabled diagnostics.  When there are multiple\\n\"\n-             \"occurrences of this option, the <value> that appears earliest\\n\"\n-             \"in the list below has precedence.  The default is 'fail'.\\n\"),\n-    cl::value_desc(\"mode\"),\n-    cl::values(clEnumValN(DumpInputHelp, \"help\", \"Explain input dump and quit\"),\n-               clEnumValN(DumpInputAlways, \"always\", \"Always dump input\"),\n-               clEnumValN(DumpInputFail, \"fail\", \"Dump input on failure\"),\n-               clEnumValN(DumpInputNever, \"never\", \"Never dump input\")));\n-\n-// The order of DumpInputFilterValue members affects their precedence, as\n-// documented for -dump-input-filter below.\n-enum DumpInputFilterValue {\n-  DumpInputFilterError,\n-  DumpInputFilterAnnotation,\n-  DumpInputFilterAnnotationFull,\n-  DumpInputFilterAll\n-};\n-\n-static cl::list<DumpInputFilterValue> DumpInputFilters(\n-    \"dump-input-filter\",\n-    cl::desc(\"In the dump requested by -dump-input, print only input lines of\\n\"\n-             \"kind <value> plus any context specified by -dump-input-context.\\n\"\n-             \"When there are multiple occurrences of this option, the <value>\\n\"\n-             \"that appears earliest in the list below has precedence.  The\\n\"\n-             \"default is 'error' when -dump-input=fail, and it's 'all' when\\n\"\n-             \"-dump-input=always.\\n\"),\n-    cl::values(clEnumValN(DumpInputFilterAll, \"all\", \"All input lines\"),\n-               clEnumValN(DumpInputFilterAnnotationFull, \"annotation-full\",\n-                          \"Input lines with annotations\"),\n-               clEnumValN(DumpInputFilterAnnotation, \"annotation\",\n-                          \"Input lines with starting points of annotations\"),\n-               clEnumValN(DumpInputFilterError, \"error\",\n-                          \"Input lines with starting points of error \"\n-                          \"annotations\")));\n-\n-static cl::list<unsigned> DumpInputContexts(\n-    \"dump-input-context\", cl::value_desc(\"N\"),\n-    cl::desc(\"In the dump requested by -dump-input, print <N> input lines\\n\"\n-             \"before and <N> input lines after any lines specified by\\n\"\n-             \"-dump-input-filter.  When there are multiple occurrences of\\n\"\n-             \"this option, the largest specified <N> has precedence.  The\\n\"\n-             \"default is 5.\\n\"));\n-\n-typedef cl::list<std::string>::const_iterator prefix_iterator;\n-\n-static void DumpCommandLine(int argc, char **argv) {\n-  errs() << \"FileCheck command line: \";\n-  for (int I = 0; I < argc; I++)\n-    errs() << \" \" << argv[I];\n-  errs() << \"\\n\";\n-}\n-\n-struct MarkerStyle {\n-  /// The starting char (before tildes) for marking the line.\n-  char Lead;\n-  /// What color to use for this annotation.\n-  raw_ostream::Colors Color;\n-  /// A note to follow the marker, or empty string if none.\n-  std::string Note;\n-  /// Does this marker indicate inclusion by -dump-input-filter=error?\n-  bool FiltersAsError;\n-  MarkerStyle() {}\n-  MarkerStyle(char Lead, raw_ostream::Colors Color,\n-              const std::string &Note = \"\", bool FiltersAsError = false)\n-      : Lead(Lead), Color(Color), Note(Note), FiltersAsError(FiltersAsError) {\n-    assert((!FiltersAsError || !Note.empty()) &&\n-           \"expected error diagnostic to have note\");\n-  }\n-};\n-\n-static MarkerStyle GetMarker(FileCheckDiag::MatchType MatchTy) {\n-  switch (MatchTy) {\n-  case FileCheckDiag::MatchFoundAndExpected:\n-    return MarkerStyle('^', raw_ostream::GREEN);\n-  case FileCheckDiag::MatchFoundButExcluded:\n-    return MarkerStyle('!', raw_ostream::RED, \"error: no match expected\",\n-                       /*FiltersAsError=*/true);\n-  case FileCheckDiag::MatchFoundButWrongLine:\n-    return MarkerStyle('!', raw_ostream::RED, \"error: match on wrong line\",\n-                       /*FiltersAsError=*/true);\n-  case FileCheckDiag::MatchFoundButDiscarded:\n-    return MarkerStyle('!', raw_ostream::CYAN,\n-                       \"discard: overlaps earlier match\");\n-  case FileCheckDiag::MatchFoundErrorNote:\n-    // Note should always be overridden within the FileCheckDiag.\n-    return MarkerStyle('!', raw_ostream::RED,\n-                       \"error: unknown error after match\",\n-                       /*FiltersAsError=*/true);\n-  case FileCheckDiag::MatchNoneAndExcluded:\n-    return MarkerStyle('X', raw_ostream::GREEN);\n-  case FileCheckDiag::MatchNoneButExpected:\n-    return MarkerStyle('X', raw_ostream::RED, \"error: no match found\",\n-                       /*FiltersAsError=*/true);\n-  case FileCheckDiag::MatchNoneForInvalidPattern:\n-    return MarkerStyle('X', raw_ostream::RED,\n-                       \"error: match failed for invalid pattern\",\n-                       /*FiltersAsError=*/true);\n-  case FileCheckDiag::MatchFuzzy:\n-    return MarkerStyle('?', raw_ostream::MAGENTA, \"possible intended match\",\n-                       /*FiltersAsError=*/true);\n-  }\n-  llvm_unreachable_internal(\"unexpected match type\");\n-}\n-\n-static void DumpInputAnnotationHelp(raw_ostream &OS) {\n-  OS << \"The following description was requested by -dump-input=help to\\n\"\n-     << \"explain the input dump printed by FileCheck.\\n\"\n-     << \"\\n\"\n-     << \"Related command-line options:\\n\"\n-     << \"\\n\"\n-     << \"  - -dump-input=<value> enables or disables the input dump\\n\"\n-     << \"  - -dump-input-filter=<value> filters the input lines\\n\"\n-     << \"  - -dump-input-context=<N> adjusts the context of filtered lines\\n\"\n-     << \"  - -v and -vv add more annotations\\n\"\n-     << \"  - -color forces colors to be enabled both in the dump and below\\n\"\n-     << \"  - -help documents the above options in more detail\\n\"\n-     << \"\\n\"\n-     << \"These options can also be set via FILECHECK_OPTS.  For example, for\\n\"\n-     << \"maximum debugging output on failures:\\n\"\n-     << \"\\n\"\n-     << \"  $ FILECHECK_OPTS='-dump-input-filter=all -vv -color' ninja check\\n\"\n-     << \"\\n\"\n-     << \"Input dump annotation format:\\n\"\n-     << \"\\n\";\n-\n-  // Labels for input lines.\n-  OS << \"  - \";\n-  WithColor(OS, raw_ostream::SAVEDCOLOR, true) << \"L:\";\n-  OS << \"     labels line number L of the input file\\n\"\n-     << \"           An extra space is added after each input line to represent\"\n-     << \" the\\n\"\n-     << \"           newline character\\n\";\n-\n-  // Labels for annotation lines.\n-  OS << \"  - \";\n-  WithColor(OS, raw_ostream::SAVEDCOLOR, true) << \"T:L\";\n-  OS << \"    labels the only match result for either (1) a pattern of type T\"\n-     << \" from\\n\"\n-     << \"           line L of the check file if L is an integer or (2) the\"\n-     << \" I-th implicit\\n\"\n-     << \"           pattern if L is \\\"imp\\\" followed by an integer \"\n-     << \"I (index origin one)\\n\";\n-  OS << \"  - \";\n-  WithColor(OS, raw_ostream::SAVEDCOLOR, true) << \"T:L'N\";\n-  OS << \"  labels the Nth match result for such a pattern\\n\";\n-\n-  // Markers on annotation lines.\n-  OS << \"  - \";\n-  WithColor(OS, raw_ostream::SAVEDCOLOR, true) << \"^~~\";\n-  OS << \"    marks good match (reported if -v)\\n\"\n-     << \"  - \";\n-  WithColor(OS, raw_ostream::SAVEDCOLOR, true) << \"!~~\";\n-  OS << \"    marks bad match, such as:\\n\"\n-     << \"           - CHECK-NEXT on same line as previous match (error)\\n\"\n-     << \"           - CHECK-NOT found (error)\\n\"\n-     << \"           - CHECK-DAG overlapping match (discarded, reported if \"\n-     << \"-vv)\\n\"\n-     << \"  - \";\n-  WithColor(OS, raw_ostream::SAVEDCOLOR, true) << \"X~~\";\n-  OS << \"    marks search range when no match is found, such as:\\n\"\n-     << \"           - CHECK-NEXT not found (error)\\n\"\n-     << \"           - CHECK-NOT not found (success, reported if -vv)\\n\"\n-     << \"           - CHECK-DAG not found after discarded matches (error)\\n\"\n-     << \"  - \";\n-  WithColor(OS, raw_ostream::SAVEDCOLOR, true) << \"?\";\n-  OS << \"      marks fuzzy match when no match is found\\n\";\n-\n-  // Elided lines.\n-  OS << \"  - \";\n-  WithColor(OS, raw_ostream::SAVEDCOLOR, true) << \"...\";\n-  OS << \"    indicates elided input lines and annotations, as specified by\\n\"\n-     << \"           -dump-input-filter and -dump-input-context\\n\";\n-\n-  // Colors.\n-  OS << \"  - colors \";\n-  WithColor(OS, raw_ostream::GREEN, true) << \"success\";\n-  OS << \", \";\n-  WithColor(OS, raw_ostream::RED, true) << \"error\";\n-  OS << \", \";\n-  WithColor(OS, raw_ostream::MAGENTA, true) << \"fuzzy match\";\n-  OS << \", \";\n-  WithColor(OS, raw_ostream::CYAN, true, false) << \"discarded match\";\n-  OS << \", \";\n-  WithColor(OS, raw_ostream::CYAN, true, true) << \"unmatched input\";\n-  OS << \"\\n\";\n-}\n-\n-/// An annotation for a single input line.\n-struct InputAnnotation {\n-  /// The index of the match result across all checks\n-  unsigned DiagIndex;\n-  /// The label for this annotation.\n-  std::string Label;\n-  /// Is this the initial fragment of a diagnostic that has been broken across\n-  /// multiple lines?\n-  bool IsFirstLine;\n-  /// What input line (one-origin indexing) this annotation marks.  This might\n-  /// be different from the starting line of the original diagnostic if\n-  /// !IsFirstLine.\n-  unsigned InputLine;\n-  /// The column range (one-origin indexing, open end) in which to mark the\n-  /// input line.  If InputEndCol is UINT_MAX, treat it as the last column\n-  /// before the newline.\n-  unsigned InputStartCol, InputEndCol;\n-  /// The marker to use.\n-  MarkerStyle Marker;\n-  /// Whether this annotation represents a good match for an expected pattern.\n-  bool FoundAndExpectedMatch;\n-};\n-\n-/// Get an abbreviation for the check type.\n-static std::string GetCheckTypeAbbreviation(Check::FileCheckType Ty) {\n-  switch (Ty) {\n-  case Check::CheckPlain:\n-    if (Ty.getCount() > 1)\n-      return \"count\";\n-    return \"check\";\n-  case Check::CheckNext:\n-    return \"next\";\n-  case Check::CheckSame:\n-    return \"same\";\n-  case Check::CheckNot:\n-    return \"not\";\n-  case Check::CheckDAG:\n-    return \"dag\";\n-  case Check::CheckLabel:\n-    return \"label\";\n-  case Check::CheckEmpty:\n-    return \"empty\";\n-  case Check::CheckComment:\n-    return \"com\";\n-  case Check::CheckEOF:\n-    return \"eof\";\n-  case Check::CheckBadNot:\n-    return \"bad-not\";\n-  case Check::CheckBadCount:\n-    return \"bad-count\";\n-  case Check::CheckNone:\n-    llvm_unreachable(\"invalid FileCheckType\");\n-  }\n-  llvm_unreachable(\"unknown FileCheckType\");\n-}\n-\n-static void\n-BuildInputAnnotations(const SourceMgr &SM, unsigned CheckFileBufferID,\n-                      const std::pair<unsigned, unsigned> &ImpPatBufferIDRange,\n-                      const std::vector<FileCheckDiag> &Diags,\n-                      std::vector<InputAnnotation> &Annotations,\n-                      unsigned &LabelWidth) {\n-  struct CompareSMLoc {\n-    bool operator()(const SMLoc &LHS, const SMLoc &RHS) const {\n-      return LHS.getPointer() < RHS.getPointer();\n-    }\n-  };\n-  // How many diagnostics does each pattern have?\n-  std::map<SMLoc, unsigned, CompareSMLoc> DiagCountPerPattern;\n-  for (auto Diag : Diags)\n-    ++DiagCountPerPattern[Diag.CheckLoc];\n-  // How many diagnostics have we seen so far per pattern?\n-  std::map<SMLoc, unsigned, CompareSMLoc> DiagIndexPerPattern;\n-  // How many total diagnostics have we seen so far?\n-  unsigned DiagIndex = 0;\n-  // What's the widest label?\n-  LabelWidth = 0;\n-  for (auto DiagItr = Diags.begin(), DiagEnd = Diags.end(); DiagItr != DiagEnd;\n-       ++DiagItr) {\n-    InputAnnotation A;\n-    A.DiagIndex = DiagIndex++;\n-\n-    // Build label, which uniquely identifies this check result.\n-    unsigned CheckBufferID = SM.FindBufferContainingLoc(DiagItr->CheckLoc);\n-    auto CheckLineAndCol =\n-        SM.getLineAndColumn(DiagItr->CheckLoc, CheckBufferID);\n-    llvm::raw_string_ostream Label(A.Label);\n-    Label << GetCheckTypeAbbreviation(DiagItr->CheckTy) << \":\";\n-    if (CheckBufferID == CheckFileBufferID)\n-      Label << CheckLineAndCol.first;\n-    else if (ImpPatBufferIDRange.first <= CheckBufferID &&\n-             CheckBufferID < ImpPatBufferIDRange.second)\n-      Label << \"imp\" << (CheckBufferID - ImpPatBufferIDRange.first + 1);\n-    else\n-      llvm_unreachable(\"expected diagnostic's check location to be either in \"\n-                       \"the check file or for an implicit pattern\");\n-    if (DiagCountPerPattern[DiagItr->CheckLoc] > 1)\n-      Label << \"'\" << DiagIndexPerPattern[DiagItr->CheckLoc]++;\n-    LabelWidth = std::max((std::string::size_type)LabelWidth, A.Label.size());\n-\n-    A.Marker = GetMarker(DiagItr->MatchTy);\n-    if (!DiagItr->Note.empty()) {\n-      A.Marker.Note = DiagItr->Note;\n-      // It's less confusing if notes that don't actually have ranges don't have\n-      // markers.  For example, a marker for 'with \"VAR\" equal to \"5\"' would\n-      // seem to indicate where \"VAR\" matches, but the location we actually have\n-      // for the marker simply points to the start of the match/search range for\n-      // the full pattern of which the substitution is potentially just one\n-      // component.\n-      if (DiagItr->InputStartLine == DiagItr->InputEndLine &&\n-          DiagItr->InputStartCol == DiagItr->InputEndCol)\n-        A.Marker.Lead = ' ';\n-    }\n-    if (DiagItr->MatchTy == FileCheckDiag::MatchFoundErrorNote) {\n-      assert(!DiagItr->Note.empty() &&\n-             \"expected custom note for MatchFoundErrorNote\");\n-      A.Marker.Note = \"error: \" + A.Marker.Note;\n-    }\n-    A.FoundAndExpectedMatch =\n-        DiagItr->MatchTy == FileCheckDiag::MatchFoundAndExpected;\n-\n-    // Compute the mark location, and break annotation into multiple\n-    // annotations if it spans multiple lines.\n-    A.IsFirstLine = true;\n-    A.InputLine = DiagItr->InputStartLine;\n-    A.InputStartCol = DiagItr->InputStartCol;\n-    if (DiagItr->InputStartLine == DiagItr->InputEndLine) {\n-      // Sometimes ranges are empty in order to indicate a specific point, but\n-      // that would mean nothing would be marked, so adjust the range to\n-      // include the following character.\n-      A.InputEndCol =\n-          std::max(DiagItr->InputStartCol + 1, DiagItr->InputEndCol);\n-      Annotations.push_back(A);\n-    } else {\n-      assert(DiagItr->InputStartLine < DiagItr->InputEndLine &&\n-             \"expected input range not to be inverted\");\n-      A.InputEndCol = UINT_MAX;\n-      Annotations.push_back(A);\n-      for (unsigned L = DiagItr->InputStartLine + 1, E = DiagItr->InputEndLine;\n-           L <= E; ++L) {\n-        // If a range ends before the first column on a line, then it has no\n-        // characters on that line, so there's nothing to render.\n-        if (DiagItr->InputEndCol == 1 && L == E)\n-          break;\n-        InputAnnotation B;\n-        B.DiagIndex = A.DiagIndex;\n-        B.Label = A.Label;\n-        B.IsFirstLine = false;\n-        B.InputLine = L;\n-        B.Marker = A.Marker;\n-        B.Marker.Lead = '~';\n-        B.Marker.Note = \"\";\n-        B.InputStartCol = 1;\n-        if (L != E)\n-          B.InputEndCol = UINT_MAX;\n-        else\n-          B.InputEndCol = DiagItr->InputEndCol;\n-        B.FoundAndExpectedMatch = A.FoundAndExpectedMatch;\n-        Annotations.push_back(B);\n-      }\n-    }\n-  }\n-}\n-\n-static unsigned FindInputLineInFilter(\n-    DumpInputFilterValue DumpInputFilter, unsigned CurInputLine,\n-    const std::vector<InputAnnotation>::iterator &AnnotationBeg,\n-    const std::vector<InputAnnotation>::iterator &AnnotationEnd) {\n-  if (DumpInputFilter == DumpInputFilterAll)\n-    return CurInputLine;\n-  for (auto AnnotationItr = AnnotationBeg; AnnotationItr != AnnotationEnd;\n-       ++AnnotationItr) {\n-    switch (DumpInputFilter) {\n-    case DumpInputFilterAll:\n-      llvm_unreachable(\"unexpected DumpInputFilterAll\");\n-      break;\n-    case DumpInputFilterAnnotationFull:\n-      return AnnotationItr->InputLine;\n-    case DumpInputFilterAnnotation:\n-      if (AnnotationItr->IsFirstLine)\n-        return AnnotationItr->InputLine;\n-      break;\n-    case DumpInputFilterError:\n-      if (AnnotationItr->IsFirstLine && AnnotationItr->Marker.FiltersAsError)\n-        return AnnotationItr->InputLine;\n-      break;\n-    }\n-  }\n-  return UINT_MAX;\n-}\n-\n-/// To OS, print a vertical ellipsis (right-justified at LabelWidth) if it would\n-/// occupy less lines than ElidedLines, but print ElidedLines otherwise.  Either\n-/// way, clear ElidedLines.  Thus, if ElidedLines is empty, do nothing.\n-static void DumpEllipsisOrElidedLines(raw_ostream &OS, std::string &ElidedLines,\n-                                      unsigned LabelWidth) {\n-  if (ElidedLines.empty())\n-    return;\n-  unsigned EllipsisLines = 3;\n-  if (EllipsisLines < StringRef(ElidedLines).count('\\n')) {\n-    for (unsigned i = 0; i < EllipsisLines; ++i) {\n-      WithColor(OS, raw_ostream::BLACK, /*Bold=*/true)\n-          << right_justify(\".\", LabelWidth);\n-      OS << '\\n';\n-    }\n-  } else\n-    OS << ElidedLines;\n-  ElidedLines.clear();\n-}\n-\n-static void DumpAnnotatedInput(raw_ostream &OS, const FileCheckRequest &Req,\n-                               DumpInputFilterValue DumpInputFilter,\n-                               unsigned DumpInputContext,\n-                               StringRef InputFileText,\n-                               std::vector<InputAnnotation> &Annotations,\n-                               unsigned LabelWidth) {\n-  OS << \"Input was:\\n<<<<<<\\n\";\n-\n-  // Sort annotations.\n-  llvm::sort(Annotations,\n-             [](const InputAnnotation &A, const InputAnnotation &B) {\n-               // 1. Sort annotations in the order of the input lines.\n-               //\n-               // This makes it easier to find relevant annotations while\n-               // iterating input lines in the implementation below.  FileCheck\n-               // does not always produce diagnostics in the order of input\n-               // lines due to, for example, CHECK-DAG and CHECK-NOT.\n-               if (A.InputLine != B.InputLine)\n-                 return A.InputLine < B.InputLine;\n-               // 2. Sort annotations in the temporal order FileCheck produced\n-               // their associated diagnostics.\n-               //\n-               // This sort offers several benefits:\n-               //\n-               // A. On a single input line, the order of annotations reflects\n-               //    the FileCheck logic for processing directives/patterns.\n-               //    This can be helpful in understanding cases in which the\n-               //    order of the associated directives/patterns in the check\n-               //    file or on the command line either (i) does not match the\n-               //    temporal order in which FileCheck looks for matches for the\n-               //    directives/patterns (due to, for example, CHECK-LABEL,\n-               //    CHECK-NOT, or `--implicit-check-not`) or (ii) does match\n-               //    that order but does not match the order of those\n-               //    diagnostics along an input line (due to, for example,\n-               //    CHECK-DAG).\n-               //\n-               //    On the other hand, because our presentation format presents\n-               //    input lines in order, there's no clear way to offer the\n-               //    same benefit across input lines.  For consistency, it might\n-               //    then seem worthwhile to have annotations on a single line\n-               //    also sorted in input order (that is, by input column).\n-               //    However, in practice, this appears to be more confusing\n-               //    than helpful.  Perhaps it's intuitive to expect annotations\n-               //    to be listed in the temporal order in which they were\n-               //    produced except in cases the presentation format obviously\n-               //    and inherently cannot support it (that is, across input\n-               //    lines).\n-               //\n-               // B. When diagnostics' annotations are split among multiple\n-               //    input lines, the user must track them from one input line\n-               //    to the next.  One property of the sort chosen here is that\n-               //    it facilitates the user in this regard by ensuring the\n-               //    following: when comparing any two input lines, a\n-               //    diagnostic's annotations are sorted in the same position\n-               //    relative to all other diagnostics' annotations.\n-               return A.DiagIndex < B.DiagIndex;\n-             });\n-\n-  // Compute the width of the label column.\n-  const unsigned char *InputFilePtr = InputFileText.bytes_begin(),\n-                      *InputFileEnd = InputFileText.bytes_end();\n-  unsigned LineCount = InputFileText.count('\\n');\n-  if (InputFileEnd[-1] != '\\n')\n-    ++LineCount;\n-  unsigned LineNoWidth = std::log10(LineCount) + 1;\n-  // +3 below adds spaces (1) to the left of the (right-aligned) line numbers\n-  // on input lines and (2) to the right of the (left-aligned) labels on\n-  // annotation lines so that input lines and annotation lines are more\n-  // visually distinct.  For example, the spaces on the annotation lines ensure\n-  // that input line numbers and check directive line numbers never align\n-  // horizontally.  Those line numbers might not even be for the same file.\n-  // One space would be enough to achieve that, but more makes it even easier\n-  // to see.\n-  LabelWidth = std::max(LabelWidth, LineNoWidth) + 3;\n-\n-  // Print annotated input lines.\n-  unsigned PrevLineInFilter = 0; // 0 means none so far\n-  unsigned NextLineInFilter = 0; // 0 means uncomputed, UINT_MAX means none\n-  std::string ElidedLines;\n-  raw_string_ostream ElidedLinesOS(ElidedLines);\n-  ColorMode TheColorMode =\n-      WithColor(OS).colorsEnabled() ? ColorMode::Enable : ColorMode::Disable;\n-  if (TheColorMode == ColorMode::Enable)\n-    ElidedLinesOS.enable_colors(true);\n-  auto AnnotationItr = Annotations.begin(), AnnotationEnd = Annotations.end();\n-  for (unsigned Line = 1;\n-       InputFilePtr != InputFileEnd || AnnotationItr != AnnotationEnd; ++Line) {\n-    const unsigned char *InputFileLine = InputFilePtr;\n-\n-    // Compute the previous and next line included by the filter.\n-    if (NextLineInFilter < Line)\n-      NextLineInFilter = FindInputLineInFilter(DumpInputFilter, Line,\n-                                               AnnotationItr, AnnotationEnd);\n-    assert(NextLineInFilter && \"expected NextLineInFilter to be computed\");\n-    if (NextLineInFilter == Line)\n-      PrevLineInFilter = Line;\n-\n-    // Elide this input line and its annotations if it's not within the\n-    // context specified by -dump-input-context of an input line included by\n-    // -dump-input-filter.  However, in case the resulting ellipsis would occupy\n-    // more lines than the input lines and annotations it elides, buffer the\n-    // elided lines and annotations so we can print them instead.\n-    raw_ostream *LineOS = &OS;\n-    if ((!PrevLineInFilter || PrevLineInFilter + DumpInputContext < Line) &&\n-        (NextLineInFilter == UINT_MAX ||\n-         Line + DumpInputContext < NextLineInFilter))\n-      LineOS = &ElidedLinesOS;\n-    else {\n-      LineOS = &OS;\n-      DumpEllipsisOrElidedLines(OS, ElidedLinesOS.str(), LabelWidth);\n-    }\n-\n-    // Print right-aligned line number.\n-    WithColor(*LineOS, raw_ostream::BLACK, /*Bold=*/true, /*BF=*/false,\n-              TheColorMode)\n-        << format_decimal(Line, LabelWidth) << \": \";\n-\n-    // For the case where -v and colors are enabled, find the annotations for\n-    // good matches for expected patterns in order to highlight everything\n-    // else in the line.  There are no such annotations if -v is disabled.\n-    std::vector<InputAnnotation> FoundAndExpectedMatches;\n-    if (Req.Verbose && TheColorMode == ColorMode::Enable) {\n-      for (auto I = AnnotationItr; I != AnnotationEnd && I->InputLine == Line;\n-           ++I) {\n-        if (I->FoundAndExpectedMatch)\n-          FoundAndExpectedMatches.push_back(*I);\n-      }\n-    }\n-\n-    // Print numbered line with highlighting where there are no matches for\n-    // expected patterns.\n-    bool Newline = false;\n-    {\n-      WithColor COS(*LineOS, raw_ostream::SAVEDCOLOR, /*Bold=*/false,\n-                    /*BG=*/false, TheColorMode);\n-      bool InMatch = false;\n-      if (Req.Verbose)\n-        COS.changeColor(raw_ostream::CYAN, true, true);\n-      for (unsigned Col = 1; InputFilePtr != InputFileEnd && !Newline; ++Col) {\n-        bool WasInMatch = InMatch;\n-        InMatch = false;\n-        for (auto M : FoundAndExpectedMatches) {\n-          if (M.InputStartCol <= Col && Col < M.InputEndCol) {\n-            InMatch = true;\n-            break;\n-          }\n-        }\n-        if (!WasInMatch && InMatch)\n-          COS.resetColor();\n-        else if (WasInMatch && !InMatch)\n-          COS.changeColor(raw_ostream::CYAN, true, true);\n-        if (*InputFilePtr == '\\n') {\n-          Newline = true;\n-          COS << ' ';\n-        } else\n-          COS << *InputFilePtr;\n-        ++InputFilePtr;\n-      }\n-    }\n-    *LineOS << '\\n';\n-    unsigned InputLineWidth = InputFilePtr - InputFileLine;\n-\n-    // Print any annotations.\n-    while (AnnotationItr != AnnotationEnd && AnnotationItr->InputLine == Line) {\n-      WithColor COS(*LineOS, AnnotationItr->Marker.Color, /*Bold=*/true,\n-                    /*BG=*/false, TheColorMode);\n-      // The two spaces below are where the \": \" appears on input lines.\n-      COS << left_justify(AnnotationItr->Label, LabelWidth) << \"  \";\n-      unsigned Col;\n-      for (Col = 1; Col < AnnotationItr->InputStartCol; ++Col)\n-        COS << ' ';\n-      COS << AnnotationItr->Marker.Lead;\n-      // If InputEndCol=UINT_MAX, stop at InputLineWidth.\n-      for (++Col; Col < AnnotationItr->InputEndCol && Col <= InputLineWidth;\n-           ++Col)\n-        COS << '~';\n-      const std::string &Note = AnnotationItr->Marker.Note;\n-      if (!Note.empty()) {\n-        // Put the note at the end of the input line.  If we were to instead\n-        // put the note right after the marker, subsequent annotations for the\n-        // same input line might appear to mark this note instead of the input\n-        // line.\n-        for (; Col <= InputLineWidth; ++Col)\n-          COS << ' ';\n-        COS << ' ' << Note;\n-      }\n-      COS << '\\n';\n-      ++AnnotationItr;\n-    }\n-  }\n-  DumpEllipsisOrElidedLines(OS, ElidedLinesOS.str(), LabelWidth);\n-\n-  OS << \">>>>>>\\n\";\n-}\n-\n-int main(int argc, char **argv) {\n-  // Enable use of ANSI color codes because FileCheck is using them to\n-  // highlight text.\n-  llvm::sys::Process::UseANSIEscapeCodes(true);\n-\n-  InitLLVM X(argc, argv);\n-  cl::ParseCommandLineOptions(argc, argv, /*Overview*/ \"\", /*Errs*/ nullptr,\n-                              \"FILECHECK_OPTS\");\n-\n-  // Select -dump-input* values.  The -help documentation specifies the default\n-  // value and which value to choose if an option is specified multiple times.\n-  // In the latter case, the general rule of thumb is to choose the value that\n-  // provides the most information.\n-  DumpInputValue DumpInput =\n-      DumpInputs.empty()\n-          ? DumpInputFail\n-          : *std::max_element(DumpInputs.begin(), DumpInputs.end());\n-  DumpInputFilterValue DumpInputFilter;\n-  if (DumpInputFilters.empty())\n-    DumpInputFilter = DumpInput == DumpInputAlways ? DumpInputFilterAll\n-                                                   : DumpInputFilterError;\n-  else\n-    DumpInputFilter =\n-        *std::max_element(DumpInputFilters.begin(), DumpInputFilters.end());\n-  unsigned DumpInputContext = DumpInputContexts.empty()\n-                                  ? 5\n-                                  : *std::max_element(DumpInputContexts.begin(),\n-                                                      DumpInputContexts.end());\n-\n-  if (DumpInput == DumpInputHelp) {\n-    DumpInputAnnotationHelp(outs());\n-    return 0;\n-  }\n-  if (CheckFilename.empty()) {\n-    errs() << \"<check-file> not specified\\n\";\n-    return 2;\n-  }\n-\n-  FileCheckRequest Req;\n-  append_range(Req.CheckPrefixes, CheckPrefixes);\n-\n-  append_range(Req.CommentPrefixes, CommentPrefixes);\n-\n-  append_range(Req.ImplicitCheckNot, ImplicitCheckNot);\n-\n-  bool GlobalDefineError = false;\n-  for (StringRef G : GlobalDefines) {\n-    size_t EqIdx = G.find('=');\n-    if (EqIdx == std::string::npos) {\n-      errs() << \"Missing equal sign in command-line definition '-D\" << G\n-             << \"'\\n\";\n-      GlobalDefineError = true;\n-      continue;\n-    }\n-    if (EqIdx == 0) {\n-      errs() << \"Missing variable name in command-line definition '-D\" << G\n-             << \"'\\n\";\n-      GlobalDefineError = true;\n-      continue;\n-    }\n-    Req.GlobalDefines.push_back(G);\n-  }\n-  if (GlobalDefineError)\n-    return 2;\n-\n-  Req.AllowEmptyInput = AllowEmptyInput;\n-  Req.AllowUnusedPrefixes = AllowUnusedPrefixes;\n-  Req.EnableVarScope = EnableVarScope;\n-  Req.AllowDeprecatedDagOverlap = AllowDeprecatedDagOverlap;\n-  Req.Verbose = Verbose;\n-  Req.VerboseVerbose = VerboseVerbose;\n-  Req.NoCanonicalizeWhiteSpace = NoCanonicalizeWhiteSpace;\n-  Req.MatchFullLines = MatchFullLines;\n-  Req.IgnoreCase = IgnoreCase;\n-\n-  if (VerboseVerbose)\n-    Req.Verbose = true;\n-\n-  FileCheck FC(Req);\n-  if (!FC.ValidateCheckPrefixes())\n-    return 2;\n-\n-  Regex PrefixRE = FC.buildCheckPrefixRegex();\n-  std::string REError;\n-  if (!PrefixRE.isValid(REError)) {\n-    errs() << \"Unable to combine check-prefix strings into a prefix regular \"\n-              \"expression! This is likely a bug in FileCheck's verification of \"\n-              \"the check-prefix strings. Regular expression parsing failed \"\n-              \"with the following error: \"\n-           << REError << \"\\n\";\n-    return 2;\n-  }\n-\n-  SourceMgr SM;\n-\n-  // Read the expected strings from the check file.\n-  ErrorOr<std::unique_ptr<MemoryBuffer>> CheckFileOrErr =\n-      MemoryBuffer::getFileOrSTDIN(CheckFilename, /*IsText=*/true);\n-  if (std::error_code EC = CheckFileOrErr.getError()) {\n-    errs() << \"Could not open check file '\" << CheckFilename\n-           << \"': \" << EC.message() << '\\n';\n-    return 2;\n-  }\n-  MemoryBuffer &CheckFile = *CheckFileOrErr.get();\n-\n-  SmallString<4096> CheckFileBuffer;\n-  StringRef CheckFileText = FC.CanonicalizeFile(CheckFile, CheckFileBuffer);\n-\n-  unsigned CheckFileBufferID =\n-      SM.AddNewSourceBuffer(MemoryBuffer::getMemBuffer(\n-                                CheckFileText, CheckFile.getBufferIdentifier()),\n-                            SMLoc());\n-\n-  std::pair<unsigned, unsigned> ImpPatBufferIDRange;\n-  if (FC.readCheckFile(SM, CheckFileText, PrefixRE, &ImpPatBufferIDRange))\n-    return 2;\n-\n-  // Open the file to check and add it to SourceMgr.\n-  ErrorOr<std::unique_ptr<MemoryBuffer>> InputFileOrErr =\n-      MemoryBuffer::getFileOrSTDIN(InputFilename, /*IsText=*/true);\n-  if (InputFilename == \"-\")\n-    InputFilename = \"<stdin>\"; // Overwrite for improved diagnostic messages\n-  if (std::error_code EC = InputFileOrErr.getError()) {\n-    errs() << \"Could not open input file '\" << InputFilename\n-           << \"': \" << EC.message() << '\\n';\n-    return 2;\n-  }\n-  MemoryBuffer &InputFile = *InputFileOrErr.get();\n-\n-  if (InputFile.getBufferSize() == 0 && !AllowEmptyInput) {\n-    errs() << \"FileCheck error: '\" << InputFilename << \"' is empty.\\n\";\n-    DumpCommandLine(argc, argv);\n-    return 2;\n-  }\n-\n-  SmallString<4096> InputFileBuffer;\n-  StringRef InputFileText = FC.CanonicalizeFile(InputFile, InputFileBuffer);\n-\n-  SM.AddNewSourceBuffer(MemoryBuffer::getMemBuffer(\n-                            InputFileText, InputFile.getBufferIdentifier()),\n-                        SMLoc());\n-\n-  std::vector<FileCheckDiag> Diags;\n-  int ExitCode = FC.checkInput(SM, InputFileText,\n-                               DumpInput == DumpInputNever ? nullptr : &Diags)\n-                     ? EXIT_SUCCESS\n-                     : 1;\n-  if (DumpInput == DumpInputAlways ||\n-      (ExitCode == 1 && DumpInput == DumpInputFail)) {\n-    errs() << \"\\n\"\n-           << \"Input file: \" << InputFilename << \"\\n\"\n-           << \"Check file: \" << CheckFilename << \"\\n\"\n-           << \"\\n\"\n-           << \"-dump-input=help explains the following input dump.\\n\"\n-           << \"\\n\";\n-    std::vector<InputAnnotation> Annotations;\n-    unsigned LabelWidth;\n-    BuildInputAnnotations(SM, CheckFileBufferID, ImpPatBufferIDRange, Diags,\n-                          Annotations, LabelWidth);\n-    DumpAnnotatedInput(errs(), Req, DumpInputFilter, DumpInputContext,\n-                       InputFileText, Annotations, LabelWidth);\n-  }\n-\n-  return ExitCode;\n-}"}, {"filename": "bin/RegisterTritonDialects.h", "status": "added", "additions": 38, "deletions": 0, "changes": 38, "file_content_changes": "@@ -0,0 +1,38 @@\n+#pragma once\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+\n+#include \"triton/Dialect/Triton/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+\n+#include \"triton/Conversion/TritonGPUToLLVM/Passes.h\"\n+#include \"triton/Conversion/TritonToTritonGPU/Passes.h\"\n+\n+#include \"mlir/InitAllPasses.h\"\n+\n+namespace mlir {\n+namespace test {\n+void registerTestAliasPass();\n+void registerTestAlignmentPass();\n+void registerTestAllocationPass();\n+void registerTestMembarPass();\n+} // namespace test\n+} // namespace mlir\n+\n+inline void registerTritonDialects(mlir::DialectRegistry &registry) {\n+  mlir::registerAllPasses();\n+  mlir::registerTritonPasses();\n+  mlir::registerTritonGPUPasses();\n+  mlir::test::registerTestAliasPass();\n+  mlir::test::registerTestAlignmentPass();\n+  mlir::test::registerTestAllocationPass();\n+  mlir::test::registerTestMembarPass();\n+  mlir::triton::registerConvertTritonToTritonGPUPass();\n+  mlir::triton::registerConvertTritonGPUToLLVMPass();\n+\n+  // TODO: register Triton & TritonGPU passes\n+  registry.insert<mlir::triton::TritonDialect, mlir::cf::ControlFlowDialect,\n+                  mlir::triton::gpu::TritonGPUDialect, mlir::math::MathDialect,\n+                  mlir::arith::ArithDialect, mlir::scf::SCFDialect,\n+                  mlir::gpu::GPUDialect>();\n+}"}, {"filename": "bin/triton-opt.cpp", "status": "modified", "additions": 3, "deletions": 34, "changes": 37, "file_content_changes": "@@ -1,41 +1,10 @@\n-#include \"triton/Dialect/Triton/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"./RegisterTritonDialects.h\"\n \n-#include \"triton/Dialect/Triton/Transforms/Passes.h\"\n-#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n-\n-#include \"triton/Conversion/Passes.h\"\n-\n-#include \"mlir/IR/Dialect.h\"\n-#include \"mlir/InitAllPasses.h\"\n-#include \"mlir/Support/MlirOptMain.h\"\n-\n-namespace mlir {\n-namespace test {\n-void registerTestAliasPass();\n-void registerTestAlignmentPass();\n-void registerTestAllocationPass();\n-void registerTestMembarPass();\n-} // namespace test\n-} // namespace mlir\n+#include \"mlir/Tools/mlir-opt/MlirOptMain.h\"\n \n int main(int argc, char **argv) {\n-  mlir::registerAllPasses();\n-  mlir::registerTritonPasses();\n-  mlir::registerTritonGPUPasses();\n-  mlir::test::registerTestAliasPass();\n-  mlir::test::registerTestAlignmentPass();\n-  mlir::test::registerTestAllocationPass();\n-  mlir::test::registerTestMembarPass();\n-  mlir::triton::registerConvertTritonToTritonGPUPass();\n-  mlir::triton::registerConvertTritonGPUToLLVMPass();\n-\n-  // TODO: register Triton & TritonGPU passes\n   mlir::DialectRegistry registry;\n-  registry.insert<mlir::triton::TritonDialect,\n-                  mlir::triton::gpu::TritonGPUDialect, mlir::math::MathDialect,\n-                  mlir::arith::ArithmeticDialect, mlir::StandardOpsDialect,\n-                  mlir::scf::SCFDialect, mlir::gpu::GPUDialect>();\n+  registerTritonDialects(registry);\n \n   return mlir::asMainReturnCode(mlir::MlirOptMain(\n       argc, argv, \"Triton (GPU) optimizer driver\\n\", registry));"}, {"filename": "bin/triton-reduce.cpp", "status": "added", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -0,0 +1,11 @@\n+#include \"./RegisterTritonDialects.h\"\n+\n+#include \"mlir/Tools/mlir-reduce/MlirReduceMain.h\"\n+\n+int main(int argc, char **argv) {\n+  mlir::DialectRegistry registry;\n+  registerTritonDialects(registry);\n+\n+  mlir::MLIRContext context(registry);\n+  return mlir::failed(mlir::mlirReduceMain(argc, argv, context));\n+}"}, {"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 32, "deletions": 9, "changes": 41, "file_content_changes": "@@ -3,7 +3,7 @@\n #include \"mlir/IR/AsmState.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/Dialect.h\"\n-#include \"mlir/Parser.h\"\n+#include \"mlir/Parser/Parser.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Support/FileUtilities.h\"\n@@ -14,14 +14,14 @@\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Target/HSACO/HSACOTranslation.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n #include \"triton/Target/PTX/PTXTranslation.h\"\n #include \"llvm/IR/LLVMContext.h\"\n #include \"llvm/Support/CommandLine.h\"\n #include \"llvm/Support/InitLLVM.h\"\n #include \"llvm/Support/SourceMgr.h\"\n #include \"llvm/Support/ToolOutputFile.h\"\n-#include <iostream>\n \n namespace mlir {\n namespace triton {\n@@ -36,9 +36,9 @@ OwningOpRef<ModuleOp> loadMLIRModule(llvm::StringRef inputFilename,\n   }\n \n   mlir::DialectRegistry registry;\n-  registry.insert<TritonDialect, triton::gpu::TritonGPUDialect,\n-                  mlir::math::MathDialect, arith::ArithmeticDialect,\n-                  StandardOpsDialect, scf::SCFDialect>();\n+  registry\n+      .insert<TritonDialect, triton::gpu::TritonGPUDialect,\n+              mlir::math::MathDialect, arith::ArithDialect, scf::SCFDialect>();\n \n   context.appendDialectRegistry(registry);\n \n@@ -50,7 +50,8 @@ OwningOpRef<ModuleOp> loadMLIRModule(llvm::StringRef inputFilename,\n     context.loadAllAvailableDialects();\n     context.allowUnregisteredDialects();\n \n-    OwningOpRef<ModuleOp> module(parseSourceFile(sourceMgr, &context));\n+    OwningOpRef<ModuleOp> module =\n+        parseSourceFile<ModuleOp>(sourceMgr, &context);\n     if (!module) {\n       llvm::errs() << \"Parse MLIR file failed.\";\n       return nullptr;\n@@ -78,7 +79,8 @@ LogicalResult tritonTranslateMain(int argc, char **argv,\n       llvm::cl::init(\"-\"));\n \n   static llvm::cl::opt<std::string> targetKind(\n-      \"target\", llvm::cl::desc(\"<translation target, options: llvmir/ptx>\"),\n+      \"target\",\n+      llvm::cl::desc(\"<translation target, options: llvmir/ptx/hsaco>\"),\n       llvm::cl::value_desc(\"target\"), llvm::cl::init(\"llvmir\"));\n \n   static llvm::cl::opt<int> SMArch(\"sm\", llvm::cl::desc(\"sm arch\"),\n@@ -87,6 +89,18 @@ LogicalResult tritonTranslateMain(int argc, char **argv,\n   static llvm::cl::opt<int> ptxVersion(\n       \"ptx-version\", llvm::cl::desc(\"PTX version\"), llvm::cl::init(10000));\n \n+  static llvm::cl::opt<std::string> GCNArch(\n+      \"gfx\", llvm::cl::desc(\"AMDGCN target. e.g. '90a'\"),\n+      llvm::cl::value_desc(\"architecture\"), llvm::cl::init(\"90a\"));\n+\n+  static llvm::cl::opt<std::string> GCNTriple(\n+      \"amdgcn\", llvm::cl::desc(\"AMDGCN triple. e.g. '-amd-amdhsa'\"),\n+      llvm::cl::value_desc(\"target triple\"), llvm::cl::init(\"-amd-amdhsa\"));\n+\n+  static llvm::cl::opt<std::string> GCNFeatures(\n+      \"\", llvm::cl::desc(\"AMDGCN features. e.g. '+sramecc,-xnack'\"),\n+      llvm::cl::value_desc(\"features\"), llvm::cl::init(\"+sramecc,-xnack\"));\n+\n   llvm::InitLLVM y(argc, argv);\n \n   registerAsmPrinterCLOptions();\n@@ -107,8 +121,8 @@ LogicalResult tritonTranslateMain(int argc, char **argv,\n   }\n \n   llvm::LLVMContext llvmContext;\n-  auto llvmir =\n-      translateTritonGPUToLLVMIR(&llvmContext, *module, SMArch.getValue());\n+  auto llvmir = translateTritonGPUToLLVMIR(&llvmContext, *module,\n+                                           SMArch.getValue(), false /*isRocm*/);\n   if (!llvmir) {\n     llvm::errs() << \"Translate to LLVM IR failed\";\n   }\n@@ -118,6 +132,15 @@ LogicalResult tritonTranslateMain(int argc, char **argv,\n   else if (targetKind == \"ptx\")\n     llvm::outs() << ::triton::translateLLVMIRToPTX(*llvmir, SMArch.getValue(),\n                                                    ptxVersion.getValue());\n+  else if (targetKind == \"hsaco\") {\n+    auto [module, hsaco] = ::triton::translateLLVMIRToHSACO(\n+        *llvmir, GCNArch.getValue(), GCNTriple.getValue(),\n+        GCNFeatures.getValue());\n+    llvm::outs() << hsaco;\n+  } else {\n+    llvm::errs() << \"Error: Unknown target specified: \" << targetKind << \"\\n\";\n+    return failure();\n+  }\n \n   return success();\n }"}, {"filename": "cmake/FindLLVM.cmake", "status": "modified", "additions": 6, "deletions": 9, "changes": 15, "file_content_changes": "@@ -1,4 +1,3 @@\n-\n # - Find LLVM headers and libraries.\n # This module locates LLVM and adapts the llvm-config output for use with\n # CMake.\n@@ -33,14 +32,12 @@\n # We also want an user-specified LLVM_ROOT_DIR to take precedence over the\n # system default locations such as /usr/local/bin. Executing find_program()\n # multiples times is the approach recommended in the docs.\n-set(llvm_config_names llvm-config-12.0 llvm-config120 llvm-config-12 llvm-config-12-64\n-                      llvm-config-11.0 llvm-config110 llvm-config-11 llvm-config-11-64\n-                      llvm-config-10.0 llvm-config100 llvm-config-10 llvm-config-10-64\n-                      llvm-config-9.0 llvm-config90 llvm-config-9 llvm-config-9-64\n-                      llvm-config-8.0 llvm-config80 llvm-config-8 llvm-config-8-64\n-                      llvm-config-7.0 llvm-config70 llvm-config-7 llvm-config-7-64\n-                      llvm-config-6.0 llvm-config60\n+set(llvm_config_names llvm-config-6.0 llvm-config60\n                       llvm-config)\n+foreach(v RANGE 7 17)\n+    # names like llvm-config-7.0 llvm-config70 llvm-config-7 llvm-config-7-64\n+    list(PREPEND llvm_config_names llvm-config-${v}.0 llvm-config${v}0 llvm-config-${v} llvm-config-${v}-64)\n+endforeach()\n find_program(LLVM_CONFIG\n     NAMES ${llvm_config_names}\n     PATHS ${LLVM_ROOT_DIR}/bin NO_DEFAULT_PATH\n@@ -111,7 +108,7 @@ else()\n         )\n         if(result_code)\n             _LLVM_FAIL(\"Failed to execute llvm-config ('${LLVM_CONFIG}', result code: '${result_code})'\")\n-        else()        \n+        else()\n             file(TO_CMAKE_PATH \"${tmplibs}\" tmplibs)\n             string(REGEX MATCHALL \"${pattern}[^ ]+\" LLVM_${var} ${tmplibs})\n         endif()"}, {"filename": "docs/Makefile", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -17,4 +17,4 @@ help:\n # Catch-all target: route all unknown targets to Sphinx using the new\n # \"make mode\" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).\n %: Makefile\n-\t@$(SPHINXBUILD) -M $@ \"$(SOURCEDIR)\" \"$(BUILDDIR)\" $(SPHINXOPTS) $(O)\n\\ No newline at end of file\n+\t@$(SPHINXBUILD) -M $@ \"$(SOURCEDIR)\" \"$(BUILDDIR)\" $(SPHINXOPTS) $(O)"}, {"filename": "docs/_templates/versions.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -24,4 +24,4 @@\n         {%- endif %}\n     </div>\n </div>\n-{%- endif %}\n\\ No newline at end of file\n+{%- endif %}"}, {"filename": "docs/backend/ldmatrixOperand0.svg", "status": "added", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -0,0 +1,16 @@\n+<svg version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 424.8784737977807 362.23070969826404\" width=\"849.7569475955614\" height=\"724.4614193965281\">\n+  <!-- svg-source:excalidraw -->\n+  <!-- payload-type:application/vnd.excalidraw+json --><!-- payload-version:2 --><!-- payload-start -->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<!-- payload-end -->\n+  <defs>\n+    <style class=\"style-fonts\">\n+      @font-face {\n+        font-family: \"Virgil\";\n+        src: url(\"https://excalidraw.com/Virgil.woff2\");\n+      }\n+      @font-face {\n+        font-family: \"Cascadia\";\n+        src: url(\"https://excalidraw.com/Cascadia.woff2\");\n+      }\n+    </style>\n+  </defs>\n+  <rect x=\"0\" y=\"0\" width=\"424.8784737977807\" height=\"362.23070969826404\" fill=\"#ffffff\"></rect><g stroke-linecap=\"round\" transform=\"translate(79.93831190187882 117.88969592192916) rotate(0 80 80)\"><path d=\"M0.55 -0.53 C35.65 -0.56, 70.89 -2.53, 159 1.35 M-0.99 -0.11 C55.76 -1.57, 112.28 -0.79, 160.37 -0.01 M158.04 -0.43 C160.66 33.27, 158.02 66.76, 161.23 161.5 M160.74 -0.86 C158.22 50.66, 160.04 102.31, 160.78 159.69 M158.04 158.86 C101.94 160.66, 46.43 158.64, -0.54 161.56 M160.98 159.8 C108.71 161.14, 56.49 161.13, -0.83 159.87 M0.33 161.22 C1.47 109.75, 0.1 55.84, 1.64 0.9 M-0.21 159.69 C-0.2 126.71, 0.8 95.3, 0.86 1\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g stroke-linecap=\"round\" transform=\"translate(155.88831652581894 118.42924791900441) rotate(0 20 20)\"><path d=\"M-1.73 0.19 C9.99 -0.74, 23.31 -0.22, 39.23 -1 M-0.55 0.4 C13.73 0.9, 26.57 0.36, 40.95 -0.96 M38.35 0.2 C41.98 15.81, 40.45 29.13, 40.28 40.97 M39.23 0.16 C40.03 15.91, 40.52 29.71, 39.28 40.95 M40.13 39.9 C31.59 41.29, 21.9 38.34, -1.12 38.64 M39.11 39.37 C29.18 39.67, 20.54 39.34, 0.96 39.36 M-0.37 41.59 C0.07 27.47, 0.79 18, -1.77 -0.74 M-0.43 39.86 C-0.73 26.58, -0.98 12.57, 0.53 0.94\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g stroke-linecap=\"round\" transform=\"translate(155.88831652581894 158.4292479190044) rotate(0 20 20)\"><path d=\"M0 0 C0 0, 0 0, 0 0 M0 0 C0 0, 0 0, 0 0 M0.54 6.57 C1.41 5.81, 1.95 3.62, 5.76 0.58 M-0.46 6.54 C2.2 4.22, 3.58 1.54, 5.14 0.46 M0.68 13.53 C3.49 9.1, 3.83 7.42, 10.39 0.09 M0.8 12.53 C4.4 7.04, 7.99 3.88, 11.29 0.57 M0.65 18.12 C4.61 13.28, 5.38 10.97, 15.46 0.55 M0.79 17.93 C3.8 14.94, 5.9 10.55, 14.64 0.94 M0.75 24.87 C7.11 15.33, 12.45 10.64, 23.06 0.28 M0.82 24.76 C7.27 17.52, 12.17 8.8, 21.76 0.8 M-1.95 29.4 C5.23 22.31, 13.15 16.58, 27.95 -1.5 M-0.62 31.05 C6.16 24.4, 10.15 18.29, 25.49 -0.08 M-1.83 36.85 C8.6 25.99, 18.04 15.23, 31.12 -1.2 M0.19 36.61 C8.5 25.9, 18 16.58, 32.16 -0.57 M1.55 40.21 C12.33 32.03, 21.34 18.88, 37.13 0.39 M2.35 41.19 C9.16 31.06, 16.5 22.9, 37.19 -0.66 M8.19 39.89 C18.11 30.15, 26.52 19.62, 40.86 2.88 M6.98 42.2 C19.82 26.66, 33.89 10.42, 41.64 1.39 M13.17 40.68 C20.06 31.85, 28.43 21.21, 41.67 8.76 M12.61 41.8 C22.02 29.33, 32.52 18.06, 42.2 7.73 M18.61 43.01 C20.92 36.85, 27.3 30.12, 39.95 13.37 M16.99 42.29 C25.94 30.38, 34.94 19.73, 41.71 12.94 M21.76 42.67 C28.75 34.27, 33.86 28.99, 42.72 20.71 M23.1 41.91 C27.03 36.11, 31.52 30.38, 40.56 20.01 M28.48 42.11 C31.22 34.13, 38.67 30.27, 40.99 25.65 M28.45 40.33 C31.82 36.4, 37.56 30.54, 41.98 24.67 M33.29 41.34 C34.92 37.73, 37.23 34.95, 42.01 32.81 M33.15 40.31 C35.74 37.19, 39.73 33.86, 40.87 31.35 M37.91 41.62 C39.99 39.9, 40.93 38.74, 42.02 37.06 M38.39 41.25 C39.49 40.04, 40.77 38.55, 41.69 37.53\" stroke=\"#b2f2bb\" stroke-width=\"0.5\" fill=\"none\"></path><path d=\"M0.09 -0.77 C7.52 0.65, 15.73 0.01, 38.9 0.79 M0.36 0.95 C13.03 0.83, 28.42 -0.98, 39.17 0.1 M38.13 0.28 C41.03 17.25, 40.04 32.36, 38.46 40.32 M39.19 -0.72 C41.21 11.56, 41.23 25.04, 40.07 39.95 M38.03 38.88 C24.01 39.3, 11.76 41.45, -1.78 38.74 M39.11 40.96 C30.42 39.92, 21.77 40.62, -0.18 40.79 M-0.71 38.23 C-0.37 29.05, 1.88 16.68, -0.86 -0.29 M0.22 40.53 C0.78 29.95, -0.84 20.36, -0.88 0.67\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g stroke-linecap=\"round\" transform=\"translate(155.88831652581894 198.4292479190044) rotate(0 20 20)\"><path d=\"M0 0 C0 0, 0 0, 0 0 M0 0 C0 0, 0 0, 0 0 M-0.53 6 C1.69 5.09, 3.56 1.69, 4.88 0.67 M-0.24 6.54 C1.86 4.67, 3.16 2.19, 5.21 0.3 M-1.09 13.38 C4.27 9.39, 5.09 5.26, 11.63 1.29 M0.23 12.35 C4.03 7.91, 7.72 2.22, 11.02 -0.1 M-0.4 20.27 C3.91 13.71, 10.72 5.36, 15.7 -1.63 M0.17 18.68 C6 12.35, 10.36 5.51, 15.74 1.22 M2 25.19 C8.27 17.73, 15.56 7.04, 20.53 0.94 M-0.33 23.1 C8.05 14.61, 15.75 6.45, 20.64 0.78 M0.34 29.24 C6.98 25.97, 11.38 17.09, 26.11 -1.23 M0.43 29.69 C6.91 22.3, 13.81 14.44, 26.2 -0.1 M-0.88 37.77 C7.19 27.7, 13.67 20.05, 31.53 0.43 M-1.08 36.92 C7.57 27.97, 17.09 18.23, 31.91 0.02 M2.92 42.8 C13.01 28.9, 21.93 15.61, 36.75 0.89 M2.36 41.89 C8.12 33.36, 16.54 24.08, 37.68 0.08 M5.13 42.45 C20.78 25.67, 33.53 11.65, 41.29 2.23 M7.37 41.91 C16.93 29.44, 27.62 17.66, 40.42 1.41 M11.46 42.03 C25.01 27.49, 34.71 15.01, 42.38 8.21 M11.35 41.75 C24.15 27.45, 35.03 15.34, 42.08 6.32 M15.91 41.19 C26 32.51, 29.97 24.07, 39.54 14.14 M16.3 40.86 C24.01 34.23, 29.3 26.75, 42.12 14.23 M20.86 41.73 C31.11 33.18, 35.79 25.93, 40.17 18.77 M22.81 41.33 C28.92 33.67, 33.5 28.29, 40.94 19.57 M26.56 42.82 C29.97 35.62, 36.78 32.99, 40.92 26.46 M27.39 41.26 C31.1 37.08, 33.44 33.49, 41.11 26.11 M34.4 40.52 C35.08 36.39, 38.72 35.05, 40.76 31.17 M32.77 40.41 C35.38 39.42, 36.77 37.02, 41.73 32.35 M38.74 41.39 C38.96 40.33, 40.14 39.62, 41.91 37.67 M38.08 41.45 C39.11 40.03, 40.58 38.77, 41.54 37.41\" stroke=\"#a5d8ff\" stroke-width=\"0.5\" fill=\"none\"></path><path d=\"M0.21 0.72 C15.48 -2.29, 28.92 0.5, 38.12 -1.65 M0.14 -0.93 C12.55 0.42, 25.62 -0.07, 40.56 -0.77 M41.05 -1.63 C38.58 14.53, 38.88 27.23, 41.56 40.13 M40.68 -0.99 C39.7 11.11, 39.78 24.09, 40.59 39.11 M41.31 38.22 C32.42 38.49, 20.35 39.06, 0.71 39.63 M40.32 39.65 C23.94 39.25, 10.1 40.37, 0.21 39.57 M-1.37 40.44 C1.22 30.46, 0.66 15.8, 1.9 -1.76 M0.3 39.85 C-1.3 25.59, -0.51 10.58, -0.4 0.57\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g stroke-linecap=\"round\" transform=\"translate(155.88831652581894 238.4292479190044) rotate(0 20 20)\"><path d=\"M0 0 C0 0, 0 0, 0 0 M0 0 C0 0, 0 0, 0 0 M-1.06 6.29 C1.77 4.54, 2.65 2.39, 5.03 0.64 M-0.53 6.63 C1.7 4.06, 4.07 2.03, 4.68 0.69 M0.88 13.04 C5.16 8.97, 6.56 4.84, 10.81 0.59 M-0.53 12.43 C2.98 8.75, 5.38 5.08, 10.52 0.7 M-1.96 18.52 C1.77 15.09, 7.29 8.8, 16.21 0.81 M0.01 18.57 C6.04 12.61, 10.53 6.7, 16.48 0.88 M-1.36 23.35 C8.82 16.73, 13.77 5.11, 20.07 -2.02 M1.2 23.46 C6.34 17.45, 9.66 12.51, 21.43 -0.69 M-0.64 30.35 C7.58 18.94, 18.27 9.5, 27.1 -1.28 M-0.1 30.44 C7.5 20.93, 15.45 12.36, 25.94 0.74 M1.02 36.52 C9.08 26.16, 16.83 18.78, 30.24 -0.01 M0.1 36.9 C7.95 27.02, 15.37 18.29, 32.63 0.79 M1.4 40.89 C13.19 26.5, 23.58 15.39, 38.69 2.01 M1.47 41.82 C8.94 32.46, 18.01 21.79, 36.23 0.8 M6.5 41.46 C20.78 25.87, 32.5 10.02, 43.07 2.46 M6.78 40.58 C16.36 30.82, 26.68 18.88, 40.9 1.9 M12.52 41.75 C24.6 29.43, 33.12 16.97, 40.14 8.56 M11.96 41.46 C22.93 27.7, 35.68 14.29, 41.02 6.94 M16.35 39.53 C27.47 29.54, 36.79 20.8, 39.81 12.46 M16.78 42.12 C27.24 30.99, 35.24 19.01, 40.42 13.8 M22.15 40.03 C29.03 32.93, 38.13 22.77, 41.28 20.6 M23.48 40.8 C28.86 34.01, 35.69 27.59, 40.51 20.57 M29.32 40.75 C31.55 36.48, 33.46 35.31, 40.87 25.36 M26.94 40.94 C33.1 36.82, 37.8 30.55, 42.36 25.13 M34.36 40.46 C34.33 38.12, 37.74 36.15, 40.08 30.85 M33.3 41.43 C36.7 37.01, 39.58 34.32, 41.71 32 M38.3 41.57 C39.47 40.21, 40.62 39.49, 41.11 37.85 M38.23 41.2 C38.94 40.69, 39.93 39.46, 41.37 37.65\" stroke=\"#ffec99\" stroke-width=\"0.5\" fill=\"none\"></path><path d=\"M0.86 -1.88 C14.13 -0.15, 33.38 -0.04, 40.27 -1.87 M-0.01 0.56 C11.8 -0.17, 25.16 0.32, 40.53 -0.81 M41.95 1.56 C40.51 9.03, 41.69 19.25, 41.35 38.03 M40.5 0.59 C38.84 9.14, 38.96 18.96, 40.65 39.11 M39.29 40.71 C23.81 41.45, 8.29 37.98, 0.65 39.29 M40.75 40.21 C31.1 40.04, 22.64 40.59, -0.69 40.22 M-1.36 41.9 C-2.14 27.24, 1.58 10.06, 0.59 -0.3 M-0.04 39.6 C0.46 31.03, 0.02 23.82, -0.91 0.32\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g transform=\"translate(177.88831652581894 118.42924791900441) rotate(0 6.133514404296875 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">0</text></g><g transform=\"translate(157.88831652581894 138.4292479190044) rotate(0 6.133514404296875 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">0</text></g><g transform=\"translate(177.88831652581894 138.4292479190044) rotate(0 6.133514404296875 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">0</text></g><g transform=\"translate(157.88831652581894 158.4292479190044) rotate(0 2.4159622192382812 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">1</text></g><g transform=\"translate(177.88831652581894 158.4292479190044) rotate(0 2.4159622192382812 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">1</text></g><g transform=\"translate(157.88831652581894 178.4292479190044) rotate(0 2.4159622192382812 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">1</text></g><g transform=\"translate(177.88831652581894 178.4292479190044) rotate(0 2.4159622192382812 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">1</text></g><g transform=\"translate(157.88831652581894 198.4292479190044) rotate(0 6.34747314453125 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">2</text></g><g transform=\"translate(177.88831652581894 198.4292479190044) rotate(0 6.34747314453125 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">2</text></g><g transform=\"translate(157.88831652581894 218.4292479190044) rotate(0 6.34747314453125 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">2</text></g><g transform=\"translate(177.88831652581894 218.4292479190044) rotate(0 6.34747314453125 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">2</text></g><g transform=\"translate(157.88831652581894 238.4292479190044) rotate(0 6.071113586425781 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">3</text></g><g transform=\"translate(177.88831652581894 238.4292479190044) rotate(0 6.071113586425781 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">3</text></g><g transform=\"translate(157.88831652581894 258.4292479190044) rotate(0 6.071113586425781 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">3</text></g><g transform=\"translate(177.88831652581894 258.4292479190044) rotate(0 6.071113586425781 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">3</text></g><g stroke-linecap=\"round\"><g transform=\"translate(215.71287003334896 197.56781798437805) rotate(0 -0.4813601946590751 19.60698575153947)\"><path d=\"M-1.93 -0.19 C2.32 9.13, 1.14 22.1, -0.69 39.34 M-0.51 -0.73 C-0.93 11.88, -0.28 23.26, 0.89 39.95\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(207.86931890450052 196.949311891869) rotate(0 6.163633934859973 -0.8799388702173019)\"><path d=\"M-1.2 -0.46 C3.72 -1.12, 8.99 0.46, 13.53 -1.46 M0.17 -0.38 C3.07 -0.18, 6.68 -0.4, 12.83 -0.74\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(208.16837108258932 239.73967994358281) rotate(0 6.40554426095413 -0.6995449279083914)\"><path d=\"M0.47 -0.46 C4.13 -0.58, 8.83 -1.75, 12.67 -1.36 M0.15 0.04 C4.66 -0.27, 8.26 -0.83, 12.47 -0.97\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g transform=\"translate(192.08837724826452 211.65282259933701) rotate(270.04899893767623 36.4482421875 5.743276743836759)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"9.572127906394257px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">warpMatOffset</text></g><g stroke-linecap=\"round\"><g transform=\"translate(204.40562464403547 160.22365728116893) rotate(0 -0.22235998715225946 8.11600589547379)\"><path d=\"M-1.26 0.18 C-0.55 6.93, 0.72 12.25, 0.82 16.45 M0.02 -0.22 C-0.37 4.25, -0.07 8.07, 0.35 15.76\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(201.1730329551815 159.96875008144343) rotate(0 2.478843485528472 -0.20424734881271434)\"><path d=\"M-0.2 0.15 C1.47 -0.28, 2.79 -0.17, 4.83 -0.61 M-0.09 0.2 C1.68 0.17, 3.47 -0.41, 5.16 -0.07\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(201.29628243358013 177.60410246898937) rotate(0 2.6787531413121997 -0.1259014179904625)\"><path d=\"M0.2 -0.2 C1.89 -0.12, 2.84 -0.5, 5.35 -0.3 M0.01 0.26 C1.55 -0.35, 2.78 0.03, 5.23 -0.51\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g transform=\"translate(170.75108725831223 138.1728464316293) rotate(270.04899893767623 42.0556640625 5.743276743836759)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"9.572127906394257px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">inWarpMatOffset</text></g><g transform=\"translate(93.88624769790528 333.0307096982633) rotate(0 60.9375 9.600000000000364)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"16px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">order = [1,0]</text></g><g transform=\"translate(46.84903545185722 185.12548855774367) rotate(0 4.6875 9.600000000000364)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"16px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">M</text></g><g transform=\"translate(147.85865172652103 303.32445062461557) rotate(0 4.6875 9.600000000000364)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"16px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">K</text></g><g stroke-linecap=\"round\"><g transform=\"translate(12.6870227320494 38.718905922416525) rotate(0 77.91551148433487 -1.7680472187557825)\"><path d=\"M1.6 -1.82 C47.91 -0.09, 92.98 -2.15, 156.73 -3.3 M-0.9 -0.24 C46.46 -1.6, 94.11 -0.66, 154.61 -1.21\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g transform=\"translate(12.6870227320494 38.718905922416525) rotate(0 77.91551148433487 -1.7680472187557825)\"><path d=\"M128.05 7.34 C136.09 6.81, 142.82 2.06, 156.25 -3.05 M125.56 8.92 C134.48 5.26, 143.73 3.51, 154.13 -0.96\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g transform=\"translate(12.6870227320494 38.718905922416525) rotate(0 77.91551148433487 -1.7680472187557825)\"><path d=\"M127.97 -13.18 C135.86 -7.51, 142.62 -6.06, 156.25 -3.05 M125.48 -11.6 C134.53 -9.05, 143.81 -4.6, 154.13 -0.96\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(10.61758429184556 35.26909019793857) rotate(0 0.6885733015975575 79.56700252496648)\"><path d=\"M1.43 1.28 C3.07 61.26, 0.71 125.47, -0.09 155.83 M-0.62 0.59 C0.46 59.24, -0.7 118.58, 0.51 158.55\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g transform=\"translate(10.61758429184556 35.26909019793857) rotate(0 0.6885733015975575 79.56700252496648)\"><path d=\"M-8.72 131.78 C-4 140.34, -2.14 153.16, 0.42 156.6 M-10.76 131.09 C-5.52 140.96, -2.67 151.44, 1.02 159.32\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g transform=\"translate(10.61758429184556 35.26909019793857) rotate(0 0.6885733015975575 79.56700252496648)\"><path d=\"M11.8 131.49 C8.39 140.24, 2.12 153.17, 0.42 156.6 M9.76 130.8 C7.27 140.62, 2.39 151.21, 1.02 159.32\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g transform=\"translate(-30.023818913817593 95.13172314810254) rotate(270 56.25 9.600000000000364)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"16px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">Strided Axis</text></g><g stroke-linecap=\"round\" transform=\"translate(154.74847610250004 118.1379378762067) rotate(0 9.318317843373706 10.337138646445055)\"><path d=\"M0 0 C0 0, 0 0, 0 0 M0 0 C0 0, 0 0, 0 0 M0.21 6.82 C1.09 4.53, 3.14 1.69, 4.19 0.34 M0.08 6.56 C1.24 5.18, 2.11 3.18, 5.12 0.32 M-0.49 10.6 C1.36 9.17, 6.22 5.86, 10.25 -1.35 M0.18 11.91 C2.92 9.17, 4.02 7.52, 10.32 -0.27 M-1.57 20.09 C3.09 13.7, 9.55 8.57, 14.24 -1.08 M0.33 19.31 C5.09 10.76, 11.05 3.84, 16.15 -0.67 M2.4 22.19 C7.25 16.48, 14.13 11.02, 21.59 -0.5 M0.81 23 C8.07 15.66, 13.93 7.83, 19.24 1.24 M6.41 21.86 C10.52 17.25, 11.63 15.38, 19.5 7.88 M7.91 22.04 C11.85 16.97, 15.79 11.85, 21.12 8 M12.83 21.36 C13.47 20.46, 16.43 18.91, 20.53 12.7 M12.68 22.8 C14.97 19.32, 17.63 16.37, 19.6 13.16\" stroke=\"#ffc9c9\" stroke-width=\"0.5\" fill=\"none\"></path><path d=\"M-1.62 -1.52 C7.79 1.27, 12.78 0.46, 17.58 1.48 M-0.52 0.2 C7.5 0.73, 13.77 0.75, 18.24 0.29 M19.63 0.03 C19.42 6.33, 19.05 10.26, 18.17 19.72 M18.19 -0.35 C18.42 7.67, 18.92 16.2, 17.69 20.57 M19.43 18.81 C12.46 20.05, 7.67 20.78, 1.38 19.84 M19.43 20.32 C12.75 19.93, 6.67 20.78, -0.26 20.74 M-0.7 19 C-0.86 15.28, -1.76 12.25, -0.9 0.77 M0.6 19.81 C0.82 13.93, 0.36 5.89, -0.33 -0.92\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g transform=\"translate(157.8815469523766 118.39314352731162) rotate(0 6.133514404296875 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">0</text></g><g stroke-linecap=\"round\" transform=\"translate(279.9383119018788 117.76286895847443) rotate(0 40 40)\"><path d=\"M0 0 C0 0, 0 0, 0 0 M0 0 C0 0, 0 0, 0 0 M0.44 5.72 C1.08 5.1, 2.69 2.63, 5.65 -0.26 M-0.4 6.23 C0.85 4.75, 2.75 3.83, 4.7 0.01 M-0.69 11.15 C3.08 7.6, 8.39 3.13, 11.1 0.78 M0.82 11.68 C2.9 8.41, 5.9 4.39, 10.9 0.49 M1.4 17.86 C4.72 16.14, 7.87 11.84, 14.4 0.87 M0.69 18.87 C5.58 11.63, 11.09 5, 15.61 -0.62 M1.67 23.98 C4.54 16.5, 12.93 11.29, 19.41 0.91 M0.56 23.59 C3.92 18.53, 8.58 13.88, 20.63 -0.75 M1.31 30.94 C6.45 24.92, 10.73 15.93, 26.33 1.95 M-0.52 30.67 C8.83 19.73, 17.87 8.22, 26.87 1.24 M1.19 35.49 C12.42 20.27, 25.49 5.83, 32.13 -1.57 M0.36 36.28 C11.34 22.43, 24.77 8.38, 31.49 -0.77 M-1.57 41.03 C14.43 26.02, 28.78 7.18, 36.11 1.72 M0.8 43.44 C12.22 28.68, 21.94 15.42, 37.36 -0.65 M-0.84 49.12 C13.72 36.81, 25.26 19.76, 43.97 -0.71 M-0.25 49.69 C16.96 29.8, 33.61 9.1, 42.88 -0.68 M1.87 55.4 C19.44 34.84, 36.29 12.24, 49.26 2.01 M0.73 55.16 C10.71 43.31, 21.33 30.02, 47.44 -0.09 M-1.06 60.93 C18.98 35.39, 39.87 11.83, 53.4 -0.64 M-0.79 60.02 C12.91 45.68, 25.83 32.93, 53.21 0.19 M-0.05 68.15 C22.48 43.95, 40.16 18.3, 59.2 -1.27 M-0.21 66.51 C17.4 46.01, 34.3 27.07, 58.56 1.05 M0.46 72.36 C17.24 55.33, 33.12 38.91, 62.79 0.63 M0.91 72.66 C15.66 54.96, 31.97 37.65, 63.93 0.45 M-1.17 81.28 C22.88 51.76, 44.87 27.28, 67.21 1.95 M-0.51 79.85 C23.38 50.82, 49.72 21.6, 69.2 0.88 M2.13 83.95 C26.15 57.37, 49.05 32.49, 74.35 -0.78 M2.73 81.93 C29.28 51.55, 56.05 20.57, 73.58 0.66 M6.45 84.11 C31.07 56.87, 51.22 33.04, 78.46 -0.15 M8.32 82.66 C35.25 52.01, 62.46 19.66, 78.94 -0.61 M12.85 83.34 C37.45 53.58, 63.86 23.66, 83.55 4.39 M14.23 81.06 C39.94 51.97, 66.73 22.34, 83.12 2.3 M18.15 82.41 C37.54 59.45, 61.07 32.74, 82.77 7.03 M19.39 82.73 C42.82 54.52, 68.29 26.08, 83.17 8.85 M24.29 83.4 C39.82 64.26, 55.1 46.01, 83.13 14.55 M24.91 82.3 C46.86 56.7, 70.01 28.7, 81.96 13.86 M27.8 81.07 C50.93 58.65, 69.33 36.33, 80.98 19.4 M29.47 82.76 C46.02 61.98, 63.63 41.13, 83.52 20.63 M35.2 82.8 C43.98 72.76, 54.07 60.99, 82.8 27.14 M34.54 83.57 C51.3 64.29, 67.94 44.95, 81.69 26.63 M39.74 84.09 C54.21 67.1, 68.5 48.7, 80.51 32.49 M39.03 82.34 C47.95 72.24, 57.46 62.01, 83.35 33.54 M46.19 84.09 C56.03 72.57, 65.6 60.03, 84.29 38.78 M43.94 82.43 C60.51 66.3, 72.97 50.05, 83.08 38.91 M50.2 81.58 C64.56 66.16, 75.97 54.57, 83.33 45.49 M49.59 83.04 C62.68 66.48, 74.49 52.56, 81.8 45.83 M55.76 82.22 C60.46 76.01, 65.79 70.23, 81.2 52.21 M55.01 82.65 C66.02 70.61, 77.59 57, 81.98 50.95 M59.26 81.8 C66.49 77.83, 73.15 69.55, 81.33 57.94 M61.75 82.33 C66.63 76.11, 71.69 70.68, 83.07 57.97 M66.95 81.49 C70.56 76.61, 76.55 68.32, 82.35 65.05 M65.36 83.19 C71.54 75.69, 77.26 70.25, 82.63 63.67 M71.04 82.97 C74.97 77.63, 77.07 74.54, 82.9 70.57 M71.24 81.73 C75.31 77.05, 79.23 72.65, 83.05 69.56 M75.76 83.34 C79.13 80.52, 80.17 78.33, 82.49 75.31 M76.22 82.72 C77.65 80.83, 79.43 78.96, 82.45 75.71\" stroke=\"#ffc9c9\" stroke-width=\"0.5\" fill=\"none\"></path><path d=\"M1.93 -1.9 C20.46 0.38, 41.83 -0.73, 79.16 1.11 M-0.82 -0.75 C25.26 0.13, 50.85 -0.17, 80.87 0.57 M80.65 1.9 C79.43 19.63, 81.99 34.34, 80.65 79.21 M79.06 0.11 C79.83 21.77, 78.64 44.36, 79.69 79.81 M81.71 79.86 C54.95 79.99, 28.62 80.63, 0 81.51 M79.71 79.1 C58.71 79.74, 38.86 81.55, -0.23 80.18 M0.62 78.08 C-1.42 54.42, 1.13 32.07, -1.09 1.81 M0.05 79.53 C-0.34 60.57, -0.58 40.88, 0.24 0.84\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g transform=\"translate(265.5849722613434 151.91292574012186) rotate(0 4.6875 9.600000000000364)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"16px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">8</text></g><g transform=\"translate(312.6368653465985 98.34906169546412) rotate(0 4.6875 9.600000000000364)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"16px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">8</text></g><g transform=\"translate(313.6856933884464 147.03174432900778) rotate(0 6.133514404296875 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">0</text></g><g transform=\"translate(359.9856373027346 146.45315372069854) rotate(270 48.2958984375 6.5969380575452305)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"10.994896762575022px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">stridedMatShape</text></g><g transform=\"translate(278.15232343570773 229.4152573344545) rotate(0 57.955078125 6.5969380575452305)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"10.994896762575022px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">contiguousMatShape</text></g><g stroke-linecap=\"round\"><g transform=\"translate(114.40690244201551 66.44430127338273) rotate(89.99999999999994 0.46020199046310495 35.12560267093704)\"><path d=\"M1.21 1.98 C-1.14 21.09, -1.44 42.67, 1.54 69.63 M0.69 -0.84 C0.83 18.81, -0.39 37.04, -0.05 71.09\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(143.34133837340505 102.3198873042711) rotate(89.99999999999994 6.523669514097534 -0.02764936668518203)\"><path d=\"M-0.46 -0.19 C3.12 -0.28, 8.55 -0.76, 12.39 0.17 M0.37 0.34 C2.78 0.34, 6.36 0.44, 13.51 -0.37\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(73.51176435958843 102.23012156040932) rotate(89.99999999999994 6.342675707007288 -0.46409428332481184)\"><path d=\"M0.88 -0.29 C3.6 -0.25, 8.09 -1.32, 12.2 -0.77 M-0.15 -0.56 C5.16 0.39, 9.92 -0.04, 12.84 -0.62\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(318.31129802998373 178.4391876030204) rotate(89.99999999999994 0.4052247926592827 36.165514284105484)\"><path d=\"M-0.34 -1.58 C0.79 23.47, 0.69 43.48, 1.15 72.24 M0.21 0.62 C-0.07 23.7, -0.31 46.57, -0.14 73.91\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(349.8767330084058 214.10282950173132) rotate(89.99999999999994 8.276647408843221 0.4612819473086347)\"><path d=\"M0.74 1.29 C6.1 -0.2, 11.81 -1.09, 16.33 0.4 M-0.33 0.46 C5.54 0.02, 12.01 0, 16.89 0.19\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(273.9022030562296 216.0125719003172) rotate(89.99999999999994 8.400400245853305 -0.13512166701457318)\"><path d=\"M0.66 -0.21 C4.48 -1.18, 7.36 0.47, 17.33 -0.03 M-0.53 0.25 C5.71 0.15, 12.44 0.19, 16.08 -0.5\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(381.69746482574715 121.45574149406275) rotate(179.9999999999999 0.24983211452485676 36.62254913459856)\"><path d=\"M-0.13 0.8 C2.02 28.43, -1.21 56.63, 0.27 71.81 M0.24 -0.36 C1.04 20.68, 0.1 43.81, 0.33 73.6\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(374.28685081933736 197.64033122892033) rotate(179.9999999999999 8.432895546592626 0.18645781023042218)\"><path d=\"M1.24 0.66 C3.76 0.97, 10.45 -1.62, 16 1.14 M0.28 -0.77 C4.56 0.38, 10.32 -0.05, 16.58 -0.49\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(373.26744533105716 120.65598562621744) rotate(179.9999999999999 7.4828192661773105 0.24619718034045945)\"><path d=\"M-1.36 1.15 C5.69 -1.51, 10.85 1.19, 16.29 0.02 M-0.47 0.43 C4.52 0.18, 9 -0.16, 16.32 -0.66\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g transform=\"translate(46.27379358874168 81.50926063742008) rotate(0 58.0078125 4.954826242058516)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"8.258043736764487px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">contiguousSliceMatOffset</text></g><g stroke-linecap=\"round\" transform=\"translate(80.10253566001609 116.8242400677409) rotate(0 36.538489372767316 80.67815846243957)\"><path d=\"M0 0 C0 0, 0 0, 0 0 M0 0 C0 0, 0 0, 0 0 M0.26 6.22 C1.55 4.57, 3.78 1.26, 5.76 0.14 M-0.25 6 C1.49 3.98, 3.1 2.71, 4.93 0.02 M1.61 13.11 C2 8.91, 3.23 5.41, 9.33 0.16 M-0.17 12.42 C4.13 6.92, 8.54 2.79, 9.98 0.33 M1.73 17.56 C2.1 12.47, 7.12 8.49, 16.77 -0.03 M-0.79 17.87 C4.7 13.24, 8.17 7.97, 15.9 0.4 M1.09 24.45 C8.61 13.35, 16.18 4.03, 20.47 -0.28 M0.61 23.77 C7.21 15.29, 15.18 7.22, 20.55 -0.28 M-0.15 28.51 C8.29 21.9, 15.13 14.63, 24.6 1.37 M-0.8 29.61 C4.74 24.46, 11.46 16.92, 26.34 -0.64 M-1.32 38.21 C8.22 27.9, 13.25 20.11, 30.42 0.79 M-1.07 37.21 C9.73 26.09, 17.37 15.2, 31.6 -0.38 M-1.64 40.97 C13.7 27.88, 26.98 10.5, 35.95 0.62 M-0.14 41.69 C14.23 26.98, 28.2 10.57, 36.51 0.75 M-0.88 48.25 C12.84 31.9, 27.92 19.98, 42.46 -1.04 M-0.4 48.4 C12.76 33.94, 25.99 17.52, 41.76 -0.48 M-1.59 52.97 C18.03 32.97, 34.84 14.66, 46.49 2.18 M1.24 54.22 C11.33 41.93, 22.55 29.2, 46.67 -0.05 M-1.06 62.3 C12.83 46.47, 23.76 36.82, 54.34 1.63 M-0.51 60.25 C19.13 39.91, 38.61 17.25, 53.78 -0.16 M-1.44 67.17 C17.19 51.4, 30.23 32.48, 58.04 -1.6 M0.66 67.14 C22.44 42.46, 43.7 15.81, 59.02 -0.55 M1.45 71.22 C21.31 51.36, 39.2 29.48, 61.92 0.09 M0.04 73.98 C14.17 56.55, 26.76 41.04, 64.32 0.08 M1.5 79.86 C15.46 60.71, 32.93 40.25, 70.53 1.52 M-0.65 79.19 C24.54 52.13, 47.23 25.62, 69.72 0.76 M0.08 85.72 C21.97 60.72, 45.26 33.72, 74.55 2.26 M0.66 85.87 C29.78 52.09, 58.38 18.87, 73.35 -0.44 M-1.92 89.45 C27.23 60.38, 51.4 33.15, 72.11 6.42 M0.54 90.44 C22.67 65.07, 46.57 39.49, 72.94 7.28 M1.73 97.16 C30.06 64.33, 56.28 30.18, 75.16 13.75 M0.03 98.01 C23.1 69.64, 47.12 44.11, 74.23 12.46 M1.9 105.47 C22.73 78.02, 48.55 48.37, 72.28 19.56 M0.02 103.26 C15.67 85.12, 33.06 66.04, 74.01 19.14 M1.38 110.51 C22.22 85.77, 44.48 60.37, 75.83 26.14 M0.71 109.3 C20.96 83.24, 43.58 57.73, 73.29 24.14 M0.72 117.35 C15.2 96.82, 35.42 77.13, 73.93 31.52 M-0.48 115.08 C21.19 90.24, 43.81 65.52, 73.48 31.77 M-1.8 120.86 C24.24 92.48, 52.74 61.91, 72.28 38.98 M-0.74 121.96 C28.12 91.74, 53 60.27, 74.3 37.12 M-0.26 127.12 C28.89 97.25, 55.52 64.53, 73.49 42.02 M0.07 129.15 C18.83 106.78, 38.23 83.92, 73.23 42.97 M2.12 134.45 C23.98 108.72, 45.57 80.31, 72.31 49.42 M0.83 134.51 C17.65 113.54, 37.71 92.26, 72.92 48.68 M0.28 138.43 C13.52 124.03, 29.69 104.97, 73.52 54.66 M0.58 139.52 C26.88 110.8, 53.18 80.67, 73.11 55.56 M1.19 145.37 C18.95 122.35, 41.59 100.02, 71.89 60.75 M0.49 146.55 C16.31 127.59, 30.34 110.69, 72.81 61.46 M-0.2 153.08 C15.96 136.05, 29.92 118.01, 74.05 69.67 M-0.05 151.58 C23.5 124.54, 47.17 99, 74.23 68.2 M0.93 160.36 C28.27 128.08, 56.97 96.33, 73.77 75.38 M0.6 159.49 C26.68 128.28, 54.87 97.33, 74.2 74.39 M-0.15 162.48 C20.03 140.62, 36.07 123.43, 75.2 80.88 M0.99 163.49 C16.05 144.83, 30.9 128.33, 73.59 80.36 M6.19 163.45 C29.22 138.68, 49.24 116.39, 74.8 83.61 M6.67 163.22 C32.43 133.05, 58.64 102.81, 73.27 85.47 M10.67 162.34 C35.75 135.45, 57 111.01, 73.24 92.28 M12.99 162.95 C33.95 136.02, 57.55 110.37, 74.2 92.81 M18.79 161.64 C39.44 137.87, 58.59 112.99, 73.33 100.34 M16.97 161.82 C29.97 147.86, 41.76 133.7, 73.47 98.51 M23.57 163.89 C35.62 146.91, 48.53 133.65, 75.23 102.97 M22.12 162.33 C36.64 147.41, 50.66 131.86, 73.63 104.64 M28.03 164.32 C43.26 148.79, 56.92 131.2, 71.78 110.11 M29.07 162.89 C45.43 145.34, 61.17 126.03, 74.16 110.12 M33.18 163.69 C46.9 148.64, 58.27 131.69, 74.34 115.92 M33.69 162.5 C44.13 149.23, 56.79 137.45, 73.4 116.02 M40.57 160.96 C52.63 149.27, 63.26 134.53, 72.76 123.57 M39.73 163.33 C50.43 148.99, 62.76 135.17, 74.68 122.25 M44.24 162.13 C49.9 155.78, 55.68 148.66, 73.24 130.73 M44.93 163.4 C52.77 152.95, 60.34 144.21, 74.31 128.9 M49.23 160.9 C53.59 155.57, 59.95 147.75, 73.62 134.53 M49.66 163.13 C55.41 154.82, 61.75 147.9, 72.96 135.13 M55.61 163.5 C61.53 155.86, 69.92 144.77, 74.66 140.63 M55.03 162.03 C59.03 157.71, 63.73 151.74, 73.07 140.06 M60.28 162.13 C61.07 159.84, 67.69 154.76, 73.86 144.64 M58.85 164.05 C64.07 158.56, 69 152.55, 74.84 146.09 M64.63 163.54 C66.08 159.25, 69.4 158, 73.6 152.94 M65.14 162.54 C67.08 160.91, 69.86 157.23, 74.15 153.36 M70.7 163.37 C71.51 161.59, 72.48 160.59, 73.75 159.53 M70.47 162.98 C71.15 161.65, 72.45 160.28, 73.51 159.54 M0.11 161.45 C0.11 161.45, 0.11 161.45, 0.11 161.45 M0.11 161.45 C0.11 161.45, 0.11 161.45, 0.11 161.45 M5.08 161.81 C3.98 159.09, 1.42 156.67, -0.77 156.5 M5.62 161 C3.63 159.32, 0.66 156.74, -0.18 155.87 M13.49 160.98 C7.94 156.9, 5.92 153.8, -1.31 152.18 M12.7 160.93 C9.09 157.31, 4.16 155.16, -0.39 150.39 M17.93 159.67 C14.54 156.21, 9.84 152.73, -1.07 147.16 M18.4 161.56 C12.46 156.06, 4.39 150.33, -0.9 144.96 M23.07 162.27 C18.76 154.01, 8.37 147.05, -0.11 139.84 M24.23 161.48 C17.16 156.08, 12.82 150.57, -0.47 140.94 M31.06 159.91 C21.49 150.68, 9.9 144.52, -2.03 133.32 M30.49 160.65 C23.76 154.61, 15.82 148.11, -1.36 133.87 M34.84 162.15 C28.28 151.2, 18.86 144.77, -1.67 130.73 M35.41 160.78 C25.67 153.22, 16.82 143.91, 0.79 129.96 M41.76 160.94 C31.6 152.83, 25.34 144.55, -1.51 125.73 M42.77 161.19 C28.18 148.59, 12.54 133.83, 0.93 124 M49.53 160.72 C33.3 144.71, 17.4 133.57, -0.15 119.19 M49.93 160.76 C31.77 146.13, 14.97 130.95, 0.28 117.99 M53.73 159.21 C37.36 145.13, 19.17 130.72, -0.78 111.97 M55.51 160.58 C37.49 145.55, 18.56 129.41, -0.07 113.7 M59.36 160.08 C38.76 144.67, 19.58 123.81, 0.77 107.05 M61.02 160.53 C46.02 146.23, 29.55 132.65, -1.04 108.75 M64.77 159.09 C44.77 140.35, 20.89 119.94, -0.7 101.51 M66.92 161.3 C47.49 145.87, 28.99 128.65, -0.16 102.4 M72.96 160.16 C54.11 146.8, 37.01 131.43, -0.5 96.05 M74.1 162.26 C44.62 135.98, 15.46 111.07, -0.26 97.02 M73.53 156.21 C52.05 139.14, 32.18 118.5, -0.79 90.99 M73.86 156.27 C53.61 140.43, 34.99 123.95, 0 91.89 M72.25 149.65 C55.68 136.87, 39.51 119.49, 0.62 87.78 M73.72 150 C50.5 132.48, 27.57 112.44, -0.57 86.64 M72.55 145.35 C47.34 124.18, 21.7 102.31, 1.57 80.96 M72.9 144.78 C46.6 120.55, 17.01 97.38, -0.27 81.06 M73.42 138.06 C54.49 123.48, 34.29 103.62, -1.21 75.71 M72.49 139.88 C57.54 125.48, 40.25 111.41, 0.82 76.51 M72.81 136.39 C45.25 110.6, 15.23 86.81, -0.87 71.82 M72.85 134.38 C50.35 115.39, 27.27 95.01, 0.26 71.82 M74.72 128.31 C47.54 108.83, 23.16 89.15, -1.17 65.2 M72.92 128.57 C48.45 107.92, 24.65 88.5, 0.59 66.61 M72.61 125.18 C44.79 101.26, 19.13 76.98, -1.24 59.87 M73.66 123.79 C57.87 110.95, 44 99.21, 0.75 61.19 M72.52 119.2 C47.06 95, 23.58 73.6, -0.34 53.47 M73.41 117.83 C56.41 102.03, 38.55 86.91, -0.6 55.95 M72.39 115.45 C48.14 90.2, 23.39 69.33, 1.44 49.27 M73.24 114.79 C51.27 94.46, 28.75 72.82, 0.34 49.21 M73.3 109.38 C49.87 88.66, 30.09 67.54, -1.65 45.62 M72.34 108.46 C47.8 85.44, 23.2 64.32, -0.16 44.62 M74.71 102.79 C52.66 84.71, 29.66 66.06, -1.69 40.04 M73.41 102.21 C45.33 78.27, 15.88 53.2, 0.06 39.02 M73.7 97.38 C50.62 77.06, 24.88 57.76, -1.92 34.63 M72 98.34 C51.03 77.8, 28.71 58.56, 0.41 33.79 M72.24 90.69 C56.32 79.28, 44.66 64.95, -1.4 28.12 M73.93 91.97 C49.04 70.47, 24.2 50.72, 0.9 28.79 M72.44 85.63 C45.77 65.88, 23.04 42.6, 0.15 21.35 M72.63 86.4 C51.96 69.42, 31.54 49.95, -0.43 23.67 M71.8 82.48 C56.21 64.95, 36.02 47.99, 1.68 17.28 M72.52 81.02 C49.66 63.92, 28.34 43.01, -0.2 17.78 M74.41 76.4 C47.16 54.07, 23.47 34.09, -0.01 14.49 M72.41 76.65 C44.76 51.92, 17.36 28.01, -0.28 13.54 M74.31 69.4 C54.04 55.9, 33.75 38.19, 1.05 7.18 M72.83 71.4 C43.84 46.86, 15.22 21.07, -0.4 8.29 M74.6 63.87 C47.24 42.21, 19.82 21.79, -0.08 3.31 M73.44 66.12 C46.09 43.66, 18.08 20, -0.06 2.38 M74.82 62.55 C47.24 36.12, 23.29 15.76, 1.83 -1.33 M72.42 60.2 C45.74 36.06, 19.31 13.59, 0.62 -0.99 M74.12 54.06 C46.64 31.81, 23.14 11.2, 9.65 -2.45 M71.78 54.96 C49.79 36.5, 26.18 16.74, 8.54 -1.14 M74.54 48.89 C51.99 33.61, 32.94 16.84, 14.83 -0.95 M72.82 49.57 C50.67 31.57, 28.58 11.44, 12.65 -1.76 M72.98 45.55 C56.91 28.02, 38.96 15.55, 20.73 -0.43 M74.17 45 C56.85 29.35, 38.5 14.85, 19.46 -2.53 M72.75 39.28 C57.86 24.39, 41.43 11.19, 25.35 -1.6 M73.48 38.72 C56.17 24.58, 38.78 10.27, 25.81 -1.69 M75.03 34.81 C61.4 22.73, 51.82 15.75, 32.57 -2.32 M73.03 35.18 C56.99 19.87, 42.2 5.94, 32.31 -2.19 M74.89 28.45 C59.55 20.69, 50.74 8.58, 40.14 0.56 M73.81 27.93 C62.99 19.8, 54.65 11.99, 37.45 -1.59 M72.52 24.15 C61.51 14.73, 54.48 4.93, 43.21 -0.04 M72.97 23.46 C62.88 14.49, 52.11 5.11, 43.19 -1.54 M72.51 18.99 C67.88 12.43, 60.14 5.91, 49.8 -0.76 M73.12 18.55 C65.73 12.37, 58.65 6.41, 50.62 -1.13 M75.08 15.12 C68.72 8.49, 66.04 8.56, 56.83 -1.61 M73.79 12.21 C67.62 6.82, 61.34 2.37, 56.1 -2.92 M71.81 8.81 C70.53 4.75, 66.45 3.53, 61.67 -1.9 M72.78 7.14 C68.68 3.57, 64.99 -0.09, 62.52 -1.2 M73.67 2.1 C71.09 0.65, 70.01 -0.57, 68.72 -1.75 M73.31 2.62 C72.52 1.9, 71.28 0.64, 69.1 -1.29\" stroke=\"#000000\" stroke-width=\"0.5\" fill=\"none\"></path><path d=\"M1.09 -1 C20.66 -1.42, 45.82 -0.71, 71.45 1.71 M-0.61 0.36 C15.94 -1.22, 33.02 -1.23, 72.92 0.41 M72.11 1.15 C75.62 50.09, 75.11 102.57, 74.81 161.12 M73.38 -0.97 C72.95 50.09, 73.96 102.54, 72.64 160.87 M72.81 161.42 C45.29 163.94, 19.76 163.97, 0.26 160.41 M72.96 160.6 C50.17 160.07, 26.09 161.22, 0.42 160.84 M1.26 162.38 C-0.03 97.24, 3.24 33.46, 1.22 -0.23 M-0.91 161.2 C0.41 106.2, -0.94 49.38, 0.44 -0.96\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g stroke-linecap=\"round\"><g transform=\"translate(236.7121677824274 67.83197707763793) rotate(89.99999999999994 5.644356315727521 -0.5741556693510574)\"><path d=\"M-1.17 -0.7 C5.75 1.19, 11.09 -0.43, 12.46 -1.38 M0.45 -0.02 C2.08 -0.32, 5.59 -0.04, 12.3 0.24\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(73.58107107830352 67.5581255056386) rotate(89.99999999999994 6.0056060940783595 -0.07226701323725138)\"><path d=\"M0.64 1.19 C3.65 -0.31, 5.13 -1.15, 11.68 -1.34 M-0.56 -0.24 C4.34 0.01, 7.45 0.35, 12.57 -0.81\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g transform=\"translate(126.7343002896763 53.96486571403511) rotate(0 41.0888671875 4.954826242058516)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"8.258043736764487px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">stridedSmemOffset</text></g><g stroke-linecap=\"round\"><g transform=\"translate(80.6797166166408 67.08493742600149) rotate(0 80.71455032326509 -0.6337505858391523)\"><path d=\"M1.22 -0.14 C45.76 0.21, 92.43 0.04, 160.55 -1.75 M0.55 0.48 C35.91 0.1, 71.04 -1.16, 160.88 0.18\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(198.77298521288378 247.33254615454098) rotate(89.99999999999994 -0.7733357358877271 40.76112670388193)\"><path d=\"M-1.86 -1.49 C-1.44 20.12, 2.24 41.08, -1.02 83.02 M-0.85 -0.34 C0.05 25.78, 0.9 52.34, -0.99 82.45\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(234.7791007141388 288.5831492576235) rotate(89.99999999999994 5.917166964948777 -0.15094759153544146)\"><path d=\"M-1.21 -0.77 C4.65 -0.99, 6.29 -0.25, 13.04 0.51 M-0.3 -0.03 C4.47 0.38, 7.81 -0.7, 12.61 -0.11\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(150.98984181694522 289.6319225218267) rotate(89.99999999999994 6.752748591846498 -0.4719803035031873)\"><path d=\"M1.17 -1.01 C2.26 -0.18, 5.95 0.74, 13.26 -1.38 M0.25 0.43 C2.8 -0.63, 6.58 0.03, 12.27 0.04\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g transform=\"translate(165.37803274491205 296.0507919030497) rotate(0 50.7568359375 4.954826242058516)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"8.258043736764487px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">contiguousTileNumMats</text></g><g stroke-linecap=\"round\"><g transform=\"translate(172.06961653977282 82.31596649306812) rotate(89.99999999999994 0.5833314675998906 20.02335274184952)\"><path d=\"M1.77 0.15 C0.28 9.57, -0.76 15.77, -0.58 39.89 M0.58 0.32 C0.27 8.4, 0 17.84, 0.26 38.31\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(187.41700099544835 102.45696483827669) rotate(89.99999999999994 6.612802408049063 -0.4202400686144756)\"><path d=\"M0.21 0.56 C3.1 -1.15, 6.75 0.7, 11.99 -1.4 M0.39 -0.28 C4.11 -0.02, 7.96 -0.37, 13.01 -0.79\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(146.63065818521693 102.27115781898101) rotate(89.99999999999994 6.515335117495141 -0.3013869968854124)\"><path d=\"M-0.46 -1.06 C3.59 0.72, 7.03 -1.34, 12.93 0.46 M0.21 0.11 C4.46 -0.2, 8.67 -0.64, 13.49 0.32\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g transform=\"translate(167.56407633918775 80.96340889967178) rotate(0 55.5908203125 4.954826242058516)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"8.258043736764487px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">contiguousLoadMatOffset</text></g><g transform=\"translate(10.5958779964771 10) rotate(0 70.3125 9.600000000000364)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"16px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">Contiguous axis</text></g></svg>"}, {"filename": "docs/backend/ldmatrixOperand1.svg", "status": "added", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -0,0 +1,16 @@\n+<svg version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 426.52345624194453 360.5412658636342\" width=\"853.0469124838891\" height=\"721.0825317272684\">\n+  <!-- svg-source:excalidraw -->\n+  <!-- payload-type:application/vnd.excalidraw+json --><!-- payload-version:2 --><!-- payload-start -->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<!-- payload-end -->\n+  <defs>\n+    <style class=\"style-fonts\">\n+      @font-face {\n+        font-family: \"Virgil\";\n+        src: url(\"https://excalidraw.com/Virgil.woff2\");\n+      }\n+      @font-face {\n+        font-family: \"Cascadia\";\n+        src: url(\"https://excalidraw.com/Cascadia.woff2\");\n+      }\n+    </style>\n+  </defs>\n+  <rect x=\"0\" y=\"0\" width=\"426.52345624194453\" height=\"360.5412658636342\" fill=\"#ffffff\"></rect><g stroke-linecap=\"round\" transform=\"translate(82.08001168945685 116.03415623874025) rotate(0 80 80)\"><path d=\"M1.32 -0.04 C52.83 0.45, 101.13 0.75, 158.84 -1.98 M0.17 -0.29 C41.41 2.21, 80.57 0.68, 160.27 -0.41 M161.66 0.4 C157.78 44.75, 159.15 93.59, 158.08 161.91 M160.36 0.26 C158.85 44.86, 158.93 89.55, 159.09 159.85 M159.62 160.62 C103.84 159.07, 49.71 159.81, -0.82 160.81 M159.75 160.75 C110.95 160.63, 62.9 160.91, -0.92 159.55 M1.05 159.72 C0.45 124.73, 0.69 88.37, 1.45 0.5 M-0.68 160.18 C-0.44 100.01, 0.21 39.69, 0.18 -0.32\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g stroke-linecap=\"round\" transform=\"translate(84.32221525206546 197.84717519892547) rotate(0 20 20)\"><path d=\"M1.3 1.3 C12.2 1.61, 27.09 -1.97, 39.06 0.56 M0.95 0.75 C8 -0.67, 16.97 -0.29, 39.86 -0.25 M38.17 -1.08 C39.88 16.59, 38.41 31.2, 39.55 38.86 M39.76 0.44 C39.98 10.73, 39.24 19.9, 40.48 40.66 M39.46 39.89 C30.99 40.75, 19.28 40.94, 0.05 40.78 M39.38 40.67 C27.89 41.25, 15.3 39.85, -0.75 39.04 M-1.08 40.77 C-1.05 27.67, 1.24 14.95, 1.52 -0.18 M-0.59 40.16 C0.33 30.34, 0.45 20.37, -0.12 0.46\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g transform=\"translate(86.32221525206546 217.84717519892547) rotate(0 6.133514404296875 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">0</text></g><g transform=\"translate(109.9853371431592 196.74952573762857) rotate(0 2.4159622192382812 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">1</text></g><g transform=\"translate(109.9853371431592 216.74952573762857) rotate(0 2.4159622192382812 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">1</text></g><g transform=\"translate(128.33616014457021 196.87821729150892) rotate(0 6.34747314453125 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">2</text></g><g transform=\"translate(128.33616014457021 216.87821729150892) rotate(0 6.34747314453125 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">2</text></g><g transform=\"translate(145.91186918604814 197.20068733537119) rotate(0 6.071113586425781 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">3</text></g><g transform=\"translate(145.91186918604814 217.20068733537119) rotate(0 6.071113586425781 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">3</text></g><g transform=\"translate(59.07354659857532 256.42038760611285) rotate(0 36.4482421875 5.743276743836759)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"9.572127906394257px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">warpMatOffset</text></g><g transform=\"translate(149.52033288977157 259.1894789826565) rotate(0 42.0556640625 5.743276743836759)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"9.572127906394257px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">inWarpMatOffset</text></g><g transform=\"translate(95.70682884493965 331.34126586363345) rotate(0 60.9375 9.600000000000364)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"16px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">order = [1,0]</text></g><g transform=\"translate(158.528179098664 304.07422779423905) rotate(0 4.6875 9.600000000000364)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"16px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">N</text></g><g transform=\"translate(50.1047716636067 186.88215328606384) rotate(0 4.6875 9.600000000000364)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"16px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">K</text></g><g stroke-linecap=\"round\"><g transform=\"translate(13.272357831304475 36.85386338491662) rotate(0 76.79950755376618 -0.4080429132536665)\"><path d=\"M-0.9 -1.5 C41.15 1.41, 87.18 0.34, 154.01 -0.68 M0.25 0.76 C39.9 -2.09, 79.3 -1.7, 154.5 -1.29\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g transform=\"translate(13.272357831304475 36.85386338491662) rotate(0 76.79950755376618 -0.4080429132536665)\"><path d=\"M125.36 7.34 C132.09 6.67, 142.9 3.19, 153.42 -0.52 M126.51 9.6 C133.43 5.43, 140.12 3.58, 153.91 -1.13\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g transform=\"translate(13.272357831304475 36.85386338491662) rotate(0 76.79950755376618 -0.4080429132536665)\"><path d=\"M125.46 -13.18 C132.36 -8.15, 143.14 -5.93, 153.42 -0.52 M126.61 -10.92 C133.54 -9.8, 140.21 -6.35, 153.91 -1.13\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(12.438165438879992 33.57964636330871) rotate(0 -0.046042397649955547 78.99569169559527)\"><path d=\"M1.05 -0.28 C-2.59 36.31, -2.35 71.52, 1.45 158.27 M-0.68 0.18 C-0.73 59.48, -0.08 118.63, 0.18 157.45\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g transform=\"translate(12.438165438879992 33.57964636330871) rotate(0 -0.046042397649955547 78.99569169559527)\"><path d=\"M-9.25 129.06 C-9.28 136.42, -6.69 142.43, 1.63 157.95 M-10.98 129.52 C-6.97 140.14, -2.38 150.54, 0.36 157.13\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g transform=\"translate(12.438165438879992 33.57964636330871) rotate(0 -0.046042397649955547 78.99569169559527)\"><path d=\"M11.27 128.9 C6.61 136.34, 4.59 142.38, 1.63 157.95 M9.54 129.36 C5.83 139.91, 2.71 150.38, 0.36 157.13\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g transform=\"translate(-28.560490300209608 91.55473987799996) rotate(270 56.25 9.600000000000364)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"16px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">Strided Axis</text></g><g stroke-linecap=\"round\" transform=\"translate(83.18237482874656 197.55586515612777) rotate(0 9.318317843373706 10.337138646445055)\"><path d=\"M0 0 C0 0, 0 0, 0 0 M0 0 C0 0, 0 0, 0 0 M-0.35 6.36 C2.33 4.19, 3.01 2.44, 5.06 0.02 M-0.36 6.43 C1.43 4.58, 3.08 2.56, 4.88 -0.03 M0.92 13.12 C3.79 9.06, 5.82 5.22, 9.54 0.08 M-0.26 11.92 C3.74 8.58, 6.17 5.04, 10.77 -0.6 M-1.02 19.72 C4.96 14.16, 8.28 6.17, 16.37 -1.56 M0.01 18.88 C5.12 13.85, 9.35 7.02, 15.44 0.97 M0.69 22.02 C5.16 19.92, 10.33 15.24, 18.38 -0.55 M1.27 22.49 C6.84 15.21, 13.32 8.79, 20.91 1.28 M8.33 21.76 C10.53 19.3, 14.11 13.69, 22.33 8.56 M8.06 21.33 C11.8 17.85, 15.79 11.13, 21.2 7.2 M13.15 22.62 C14.68 20.81, 17.11 16.81, 20.88 14.48 M12.37 22.81 C15.17 19.6, 17.5 16.52, 19.57 13.34\" stroke=\"#ffc9c9\" stroke-width=\"0.5\" fill=\"none\"></path><path d=\"M-0.79 0.41 C6.68 0.39, 12.07 -0.24, 17.54 -1.27 M0.06 0 C6.09 -0.33, 11.73 0.4, 19.18 0.67 M18.88 -1.81 C19.69 5.4, 20.14 8.78, 20.53 19.55 M19.53 -0.33 C18.93 6.3, 18.77 10.82, 19.1 20.27 M19.22 20.1 C12.39 20.01, 4.99 19.23, 0.95 19.21 M18.17 20.91 C11.2 20.85, 4.61 19.78, -0.93 21.04 M0.35 21.73 C1.1 14.56, -1.78 8.65, -1.05 -0.27 M-0.57 19.78 C0.03 12.15, 0.02 4.3, 0.12 -0.94\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g transform=\"translate(86.31544567862306 197.81107080723268) rotate(0 6.133514404296875 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">0</text></g><g stroke-linecap=\"round\" transform=\"translate(281.75889304891325 116.07342512384639) rotate(0 40 40)\"><path d=\"M0 0 C0 0, 0 0, 0 0 M0 0 C0 0, 0 0, 0 0 M-0.64 6.68 C1.3 4.28, 2.88 2.41, 4.86 0.16 M-0.23 6.33 C1.24 4.14, 2.64 2.63, 4.63 0.56 M1.18 10.44 C2.8 7.74, 7.26 2.59, 10.69 -0.82 M0.46 11.56 C3.15 8.07, 6.04 5.76, 10.37 -0.39 M-1.18 17.15 C6.32 9.87, 11.46 5.71, 15.12 0.6 M-0.3 18.14 C5.04 12.69, 9.7 4.88, 14.94 -0.12 M1.25 25.34 C4.47 15.35, 12.37 9.79, 22.7 -0.7 M-0.24 24.4 C7.58 16.92, 12.77 10.57, 20.97 0.48 M-0.93 31.05 C9.39 23.32, 15.68 13.24, 25.47 1.97 M-0.56 29.87 C6.51 23.87, 11.09 16.33, 25.93 1.08 M0.46 35.71 C10.88 26.98, 20.17 13.74, 33.22 0.44 M-0.31 37.13 C9.8 23.78, 21.5 10.77, 32.64 -0.78 M-1.24 42.72 C12.46 29.81, 26.42 11.14, 37.5 0.32 M1.05 42.57 C11.39 29.76, 23.08 15.79, 37.27 0.91 M-0.3 50.5 C10.85 36.56, 21.42 26.36, 42.13 1.69 M-0.89 49.32 C9.88 35.99, 21.33 24.57, 42 -0.78 M-0.72 53.94 C12.27 38.47, 29.91 23.39, 47.16 -0.12 M0.44 54.51 C16.2 35.9, 35.08 15.42, 47.12 0.9 M-0.25 61.19 C11.08 48.95, 19.97 37.59, 51.43 -1.8 M0.74 60.01 C15.84 42.08, 32.5 22.95, 53.66 -0.96 M-0.53 67.13 C10.5 53.61, 23.9 36.94, 56.51 0.74 M-1.19 67.74 C20.45 43.35, 43.03 18.47, 57.54 -0.35 M0.16 74.38 C21.34 44.74, 44.68 18.31, 65.24 1.02 M-0.07 72.55 C21.74 48.82, 42.7 24.62, 64.16 0.31 M-1.76 78.31 C28.35 46.81, 54.85 14.23, 69.71 -1.13 M-0.71 79.13 C14.17 62.14, 30.71 44.89, 68.55 0.8 M1.24 81.52 C23.25 61.54, 40.87 37.73, 73.79 1.06 M2.57 82.31 C21.43 59.74, 40.8 39.66, 74 -0.29 M6.57 80.6 C32.97 56.28, 57.53 26.04, 81.09 -1.87 M8.82 81.8 C35.54 50.39, 62.54 18.99, 80.13 1.07 M12.04 82.92 C30.78 60.62, 45.11 42.73, 82.85 4.12 M13.1 82.73 C35.49 57.91, 54.97 34.1, 82.97 2 M17.08 81.32 C38.12 60.87, 57.59 35.69, 83.86 6.54 M17.83 82.3 C42.78 55.41, 66.89 27.47, 82.93 8.54 M22.63 81.87 C37.78 68.54, 52.35 54.22, 81.65 14.9 M24.81 81.39 C35.37 67.57, 48.33 53.51, 82.18 14.04 M31.13 80.66 C40.81 71.39, 49.6 56.11, 84.5 19.18 M29.31 83.2 C40.21 69.04, 52.87 54.77, 82.73 19.53 M32.39 81.23 C51.3 65.2, 65.99 45.56, 82.68 26.89 M34.11 82.63 C44.03 70.75, 54.49 59.04, 81.82 26.64 M38.84 83.64 C53.08 67.72, 63.06 54.42, 82.32 32.5 M38.81 83.12 C51.99 68.07, 62.3 55.44, 81.93 32.92 M44.83 81.45 C57.14 70.59, 64.99 56.79, 84.69 38.49 M45.33 83.6 C53.4 72.4, 64.5 59.8, 82.12 38.73 M48.47 83.18 C59.32 74.48, 64.11 66.99, 81.46 44.34 M49.81 81.76 C63.6 68.69, 74.92 53.62, 83.53 46.16 M55.6 81.14 C64.6 71.78, 76.49 58.14, 84.55 52 M54.78 82.77 C65.2 70.29, 76.72 57.4, 83.25 50.32 M62.37 84.08 C67.5 74.16, 73.03 69.43, 82.16 58.68 M61.75 81.85 C68 75.1, 73.9 66.86, 82.53 58.04 M67.13 82.02 C73.34 75.3, 78.16 70.63, 80.59 62.33 M65.35 82.92 C72.79 75.64, 79.53 67.63, 82.67 63.21 M72.83 82.86 C74.13 77.5, 75.68 75.02, 83.41 70.43 M71.93 81.35 C75.11 78.81, 79.3 73.76, 82.55 69.6 M76.35 82.16 C78.58 80.95, 79.63 78.84, 82.87 74.96 M76.3 82.24 C78.88 80.27, 80.18 78.13, 82.19 75.87\" stroke=\"#ffc9c9\" stroke-width=\"0.5\" fill=\"none\"></path><path d=\"M1.52 -0.18 C24.61 -1.56, 51.91 -1.37, 80.33 0.12 M-0.12 0.46 C24.56 0.98, 47.09 0.11, 79.94 0.12 M81.54 -0.04 C82.33 15.62, 79.66 35.39, 79.1 81.8 M79.94 0.02 C80.55 20.95, 80.64 42.52, 80.01 80.31 M79.58 78.44 C59.49 78.14, 38.58 79.64, 0.08 78.99 M80.22 79.03 C52.95 80.53, 27.92 80.26, -0.38 80.18 M-1.75 80.5 C-1.49 51.49, 1.28 24.85, 0.07 -1.13 M0.15 79.61 C-0.02 62.21, 0.21 46.51, 0.79 0.19\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g transform=\"translate(267.40555340837784 150.22348190549383) rotate(0 4.6875 9.600000000000364)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"16px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">8</text></g><g transform=\"translate(314.4574464936329 96.65961786083426) rotate(0 4.6875 9.600000000000364)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"16px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">8</text></g><g transform=\"translate(315.5062745354809 145.3423004943761) rotate(0 6.133514404296875 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">0</text></g><g transform=\"translate(361.6306197468984 144.58811118319863) rotate(270 48.2958984375 6.5969380575452305)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"10.994896762575022px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">stridedMatShape</text></g><g transform=\"translate(279.9729045827422 227.72581349982647) rotate(0 57.955078125 6.5969380575452305)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"10.994896762575022px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">contiguousMatShape</text></g><g stroke-linecap=\"round\"><g transform=\"translate(190.9874722519221 208.82614924978407) rotate(89.99999999999994 0.4787712283782639 41.843465139614636)\"><path d=\"M0.92 1.17 C1.18 24.7, -0.56 52.75, 0.24 81.26 M-0.02 0.95 C-0.15 29.05, 1.36 58.42, 0.9 82.74\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(224.67477048883975 251.51468056300655) rotate(89.99999999999994 6.624241266101166 -0.3829116557199086)\"><path d=\"M0.93 -1.12 C3.53 -0.24, 7.6 -1.41, 13.61 0.35 M-0.36 -0.25 C5.48 -0.52, 10.31 -0.55, 12.97 0.29\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(143.7499937914124 250.95430991489957) rotate(89.99999999999994 6.856181393793577 -0.21391378346925194)\"><path d=\"M-0.19 -0.08 C4.78 -1.16, 9.96 0.54, 13.9 -0.31 M0.61 -0.29 C5.68 -0.28, 10.26 0.4, 12.87 -0.28\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(320.13187917701816 176.74974376839236) rotate(89.99999999999994 1.0689758136868477 35.61397959786791)\"><path d=\"M0.24 -1.81 C0.98 16.67, 1.43 31.32, 1.89 72.33 M0.9 -0.33 C0.8 20.08, 0.63 38.4, 0.46 73.04\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(351.6973141554402 212.41338566710328) rotate(89.99999999999994 8.309770272736689 0.3241811620473527)\"><path d=\"M0.87 0.8 C5.97 0.01, 10.99 -0.97, 15.47 1.17 M0.08 0.72 C4.05 -0.02, 8.57 -0.46, 16.54 -0.52\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(275.72278420326404 214.32312806568734) rotate(89.99999999999994 8.024190445331598 0.344395136957246)\"><path d=\"M1.24 -0.03 C4.89 -1.06, 6.11 1.3, 15.37 1.09 M-0.05 0.01 C4.67 -0.57, 9.04 -0.5, 16.1 -0.11\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(383.5180459727816 119.7662976594329) rotate(179.9999999999999 0.617379792034626 36.26610227424044)\"><path d=\"M1.89 -1.12 C-0.21 27.57, -0.34 53.13, -0.66 73.65 M0.46 -0.41 C0.51 28.4, -0.04 57.52, -0.31 73.65\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(376.1074319663718 195.95088739429048) rotate(179.9999999999999 8.45355971787382 0.5004660780796257)\"><path d=\"M-0.62 1.53 C4.06 1.43, 9.05 0.04, 17.53 -0.46 M0.45 -0.17 C4.86 0.21, 9.46 0.01, 15.81 -0.53\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(375.0880264780916 118.9665417915894) rotate(179.9999999999999 7.6969199352880775 0.6737641045028795)\"><path d=\"M-0.73 1.45 C3.39 -0.08, 9.29 -0.43, 16.12 0.39 M0.01 0.25 C3.92 0.1, 8.05 -0.32, 15.47 0.05\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\" transform=\"translate(82.08367612732235 115.13479623311287) rotate(0 79.34831020627445 40.59646194148172)\"><path d=\"M0 0 C0 0, 0 0, 0 0 M0 0 C0 0, 0 0, 0 0 M-0.46 6.72 C0.58 4.97, 1.82 3.36, 5.56 0.1 M-0.02 6.19 C0.78 5, 2 3.34, 5.1 0.26 M1.66 11.76 C2.9 7.93, 7.69 2.92, 9.14 -1.11 M0.77 12.43 C3.35 8.35, 4.74 6.14, 10.25 0.54 M-0.62 17.42 C4.76 15.64, 8.03 8.88, 16.72 -0.26 M0.69 17.97 C5.79 13.04, 11.84 6.09, 15.53 -0.07 M2 22.96 C7.31 16.52, 16.83 5.1, 19.41 -0.7 M0.91 24.44 C7.62 15.57, 16.98 6.13, 21.15 0.59 M0.59 30.37 C8.96 18.77, 16.56 10.1, 25.26 1.79 M-0.88 29.79 C6.03 23.12, 13 16.58, 25.38 0.6 M-0.27 38.7 C9.53 24.32, 20.65 13.01, 32.44 0.69 M-0.95 36.79 C12.53 23.94, 22.85 10.77, 31.85 -0.3 M1.11 40.98 C15.55 25.1, 27.83 10.46, 36.86 -0.78 M0.09 42.03 C8.48 31.44, 17.51 23.24, 37.54 0.08 M-1.15 49.25 C11.52 33.16, 25.36 22.72, 42.72 -0.92 M0.63 48.79 C16.17 30.96, 32.14 11.96, 43.43 -1 M1.31 56.03 C8.69 41.27, 20.91 30.36, 46.45 1.23 M0.05 54.42 C11.06 41.6, 21.88 30.02, 46.64 -0.59 M-0.87 59.09 C17.82 39.79, 34.1 23.03, 51.25 -1.38 M0.87 60.97 C13.36 44.71, 25.35 29.64, 53.01 0.55 M-0.96 66.94 C19.71 43.72, 41.99 18.69, 59.08 0.94 M0.69 66.54 C19.08 44.17, 39.86 20.17, 57.26 0.66 M-0.86 71.82 C15.16 53.96, 31.05 38.87, 65.14 -0.63 M-0.4 72.17 C23.31 46.66, 47.42 19.14, 63.62 -0.52 M0.22 78.73 C25.92 50.52, 48.98 23.11, 69.93 -0.68 M-0.38 78.74 C19.03 56.93, 40.8 33.93, 67.93 1 M1.52 84.64 C14.86 65.87, 29.27 48.23, 73.65 1.22 M2.57 82.03 C31.7 49.89, 58.87 15.82, 74.27 -0.02 M6.15 82.28 C35.27 54.25, 60.22 20.25, 80.31 1.22 M7.65 82.42 C22.95 64.07, 40.61 45.1, 79.1 0.64 M11.64 81.37 C27.2 66.29, 42.46 49.6, 84.14 -0.04 M12.05 81.98 C36.28 57, 59.63 31.57, 85.87 -0.73 M17.78 84.39 C39.55 55.96, 62.22 31.92, 89.65 -0.65 M17.01 84.09 C34.15 64.7, 51.82 44.93, 89.16 0.86 M21.89 83.04 C43.11 61.66, 58.28 43.59, 94.1 -0.73 M23.13 81.84 C37.76 65.45, 53.98 47.37, 95.28 -0.31 M26.81 83.01 C51.03 57.46, 71.82 32.81, 99.51 -1.67 M28.53 82.6 C49.67 60.31, 68.9 35.96, 100.44 0 M35.35 80.74 C55.08 61.96, 71.54 37.18, 107.16 -1.34 M33.42 82.71 C53.2 61.25, 73.14 37.54, 105.31 0.59 M38.36 84.16 C61.74 57.73, 81.48 37.33, 109.57 -1.37 M38.27 82.06 C61.06 60.67, 81.13 35.89, 111.22 -0.53 M45.19 81.45 C61.24 63.26, 82.01 40.13, 116.39 2.32 M44.42 82.74 C71.25 49.58, 100.42 18.22, 116.04 1.18 M51.68 81.77 C74.92 51.61, 100.88 23.64, 121.64 0.59 M50.21 83.66 C68.55 63.55, 84.11 43.63, 121.69 -0.28 M54.05 84.78 C80.54 53.12, 109 20.86, 126.99 0.02 M54.93 83.91 C81.06 53.07, 107.93 21.64, 127.46 -0.58 M60.14 81.62 C77.06 65.02, 93.66 46.69, 131.65 0.44 M60.27 83.65 C79.35 62.57, 96.12 41.07, 132.01 0.29 M64.69 83.94 C89.6 53.4, 119.09 24.95, 136.29 -0.33 M65.67 82.98 C89.06 55.85, 113.61 27.81, 137.51 0.65 M71.22 81.37 C85.09 67.63, 100.69 47.92, 141.65 -1.72 M70.32 83.33 C94.22 55.07, 118.2 27.65, 142.25 -0.23 M76.01 82.29 C104.65 48.91, 131.64 19.49, 148.42 0.32 M75.3 84.05 C95.92 61.51, 115.9 38.89, 148.75 0.48 M82.56 82.55 C110.28 49.03, 138.35 17.78, 151.82 -1.92 M82.16 83.08 C109.17 49.69, 137.88 17.87, 154.43 -0.19 M85.68 82.29 C114.44 50.73, 143.42 19.16, 157.6 0.87 M87.09 83.53 C110.36 54.35, 134.7 27.16, 159.68 -0.4 M93.89 82.59 C111.32 61.2, 129.24 41.07, 159.24 5.74 M91.53 82.33 C109.64 63.81, 127.98 42.33, 159.26 5.17 M97.51 82.31 C115.97 63.97, 132.02 44.59, 157.72 12.19 M96.26 82.87 C117.84 60.46, 138.06 36.99, 158.2 11.83 M103.09 82.76 C120.19 65.04, 137.5 44.47, 159.66 17.73 M102.09 83.58 C119.37 64.56, 133.13 48.26, 157.98 18.38 M109.44 84.55 C121.13 69.65, 131.17 55.93, 157.77 24.86 M107.94 82.6 C128.41 60.13, 148.16 37.3, 159.19 23.9 M114.14 84.24 C121.67 72.97, 132.32 61.61, 158.21 30.1 M113.47 82.38 C126.2 69.75, 139.05 55.5, 158.98 30.72 M117.52 83.14 C133.39 69.34, 144.37 53.14, 158.37 37.43 M119.22 82.32 C132.76 65.64, 147.34 48.52, 158.95 35.69 M125.17 84.05 C130.51 72.87, 140.62 65.14, 158.91 43.09 M123.21 83.37 C138.45 68.02, 150.95 51.47, 159.36 42.87 M127.93 84.03 C136.32 74.74, 145.6 62.6, 157.6 49.05 M129.1 82.69 C141.89 68.37, 151.94 56.16, 159.05 48.88 M136.41 83.11 C139.18 77.24, 145.95 68.53, 160.57 56.03 M134.8 82.23 C140.7 75.12, 148.49 67.23, 158.85 55.78 M140.98 85.08 C143.58 75.98, 151.1 69.85, 157.64 60.51 M140.56 83.41 C145.93 75.13, 152.3 67.92, 158.99 61.97 M146.25 82.2 C147.12 79.72, 151.05 72.95, 159.47 68.92 M145.66 81.69 C148.51 78.37, 152.53 73.48, 158.42 68.25 M150.86 83.67 C152.61 79.6, 155.51 75.51, 160.18 74.2 M150.46 83.18 C153.48 79.47, 157.06 76.61, 158.47 72.93 M155.88 82.52 C156.77 81.96, 156.87 81.57, 158.94 79.58 M155.87 82.84 C156.92 81.92, 157.62 80.88, 158.73 79.68 M-0.2 81.02 C-0.2 81.02, -0.2 81.02, -0.2 81.02 M-0.2 81.02 C-0.2 81.02, -0.2 81.02, -0.2 81.02 M5.84 81.44 C5.15 79.59, 3.25 78.93, 0.08 75.77 M6.16 81.33 C4.83 80.27, 2.89 78.93, 0.38 75.95 M11.23 79.93 C8.36 78.24, 3.74 74.57, -0.15 68.87 M11.29 80.73 C7.53 76.47, 3.54 72.49, -0.67 70.14 M19.79 80.26 C9.9 75.75, 3.7 71.12, -0.15 64.51 M18.13 81.73 C12.6 77.46, 9.55 72.48, 0.22 66.04 M26.42 81.93 C20.13 74.06, 11.07 72.11, -0.42 60.01 M25.36 81.8 C19.55 76.32, 14.46 71.85, 0.19 60.31 M31.59 80.77 C24.38 76.02, 15.17 68.43, -1.38 56.46 M30.5 80.27 C22.12 74.65, 14.84 67.04, 0.42 53.79 M37.06 83.04 C27.15 73.81, 18.58 66.48, -0.33 49.68 M36.23 81.13 C28.44 73.17, 18.95 66.34, 0.28 49.1 M41.7 79.37 C27.53 69, 15.28 55.75, -0.19 45.86 M42.33 81.09 C26.72 66.78, 10.72 52.98, 0.65 44.41 M48.8 80.78 C31.89 67.09, 19.05 53.03, -2.24 39.04 M49.1 81.41 C32.57 67.84, 18.85 53.95, -0.72 38.84 M53.99 80.77 C38.91 67.99, 21.47 53.36, 1.11 35.16 M55.43 81.97 C33.85 64.14, 13.31 46, 0.16 34.31 M60.82 80.12 C46.84 68.79, 36.9 59.18, 1.59 29.5 M60.48 81.82 C45.39 68.2, 30.32 55.57, -1.08 26.95 M68.21 79.56 C44.66 59.53, 17.32 41, -1.67 22.73 M67.27 81.79 C43.29 59.22, 17.56 37.74, 0.18 22.95 M74.44 81.92 C47.33 59.77, 21.64 37.96, 1.8 19.01 M72.82 80.55 C49.04 60.05, 25.16 40.58, -0.35 18.6 M80.46 80.17 C55.1 59.77, 31.57 42.28, 1.38 10.77 M79.53 80.45 C49.57 55.42, 18.85 28.21, -0.17 12.86 M85.83 79.55 C51.78 53.22, 19.29 22.26, 0.85 7.41 M85.28 81.56 C52.72 51.98, 20.77 24.14, -0.44 8.02 M92.25 82.29 C62.45 57.32, 36.44 30.29, -1.66 1.4 M91.08 80.26 C63.55 57.01, 37.44 35.73, 0.2 1.64 M99.06 80.31 C76.98 62.31, 57.91 48.67, 3.14 -2.3 M96.68 81.85 C65.06 54.89, 34.83 26.5, 2.47 -2.2 M103.2 81.43 C63.57 47.03, 27.69 13.29, 7.68 -0.19 M102.71 80.59 C70.94 53.46, 39.82 26.4, 8.61 -0.86 M110.54 81.32 C90.53 64.83, 67.39 44.83, 13.97 -1.32 M110.04 81.51 C73.59 49.37, 38.59 17.86, 12.92 -2.48 M115.8 81.68 C83.08 49.36, 46.73 20.75, 21.19 -0.09 M115.92 80.52 C93.49 61.69, 70.45 42.49, 19.49 -2.84 M122.09 82.94 C97.54 58.81, 72.53 37.57, 25.14 -2.05 M121.57 80.98 C98.44 58.28, 72.6 38.29, 25.76 -3.28 M128.42 81 C92.82 48.48, 53.61 17.99, 33.51 -3.87 M127.87 82.26 C106.69 62.51, 85.54 44.2, 31.95 -2.82 M133.65 81.38 C96.99 47.7, 61.91 18.94, 37.89 -3.18 M133.41 81.8 C115.12 64.24, 94.48 49.15, 39.13 -2.13 M139.8 79.21 C119.25 62.66, 94.02 42.43, 45.22 -1.91 M139.93 81 C117.41 61.74, 95.12 43.43, 44.84 -2.94 M148.01 81.41 C109.12 47.63, 71.99 17.8, 49.34 -1.49 M146.39 81.22 C112.22 52.18, 77.33 23.33, 50.8 -1.46 M154.06 79.36 C124 55.93, 96.28 32.02, 54.98 -0.74 M152.65 80.38 C122.81 54.08, 93.61 28.36, 56.92 -2.31 M159.86 81.68 C132.31 56.01, 100.53 29.66, 60.99 -3.89 M159.03 81.58 C124.31 50.83, 88.45 21.54, 62.76 -2.69 M162.43 77.72 C135.85 52.99, 107.04 32.89, 68.37 -2.36 M161.49 78.46 C142.65 62.61, 122.91 46.7, 69.64 -2.31 M163.2 74.83 C134.03 47.57, 105.68 20.85, 75.71 -2.11 M160.99 72.37 C127.8 45.47, 95.3 17.27, 74.21 -2.82 M159.24 67.84 C144.58 52.96, 126.64 36.75, 82.79 -2.86 M160.85 67.72 C130.38 41.81, 101.81 17.15, 82.14 -1.22 M160.5 61.83 C138.79 42.24, 115.92 23.84, 86.49 -3.98 M162.18 62.63 C142.66 45.45, 122.93 29.5, 86.23 -1.79 M160.04 56.25 C145 43.1, 128.61 26.22, 92.07 -0.48 M162.15 56.87 C136.1 34.8, 110.83 12.13, 93.77 -2.28 M161.34 52.91 C140.24 33.45, 117.29 13.13, 101.2 -0.82 M161.46 51.81 C138.26 31.64, 114.58 12.39, 98.65 -1.82 M161.45 45.49 C144.55 31.77, 128.31 19.48, 103.39 -2.24 M161.67 45.75 C144.36 33.05, 128.16 19.7, 105.03 -2.36 M162.38 39.54 C145.03 24.91, 123.01 7.75, 111.58 -2.01 M160.61 41.82 C142 25.6, 122.47 8.81, 110.94 -2.04 M162.22 33.97 C151.23 26.05, 139.34 15.36, 118.05 -3.29 M161.67 35.32 C150.24 24.46, 137.14 15.17, 116.38 -3.22 M162.45 29.07 C153.3 23.22, 143.1 11.11, 121.75 -3.12 M161.31 31.63 C150.5 20.52, 141.15 11.89, 122.77 -2.55 M159.2 23.98 C150.31 17.74, 139.72 5.77, 128.24 -2.27 M160.32 25.57 C153.7 19.54, 147.91 14.04, 130.98 -0.95 M162.48 20.72 C155.21 14.14, 147.74 8.67, 137.21 -0.49 M161.23 19.27 C152.05 12.05, 143.68 5.28, 135.02 -3.03 M159.4 13.68 C154.47 11.34, 152.43 7.08, 142 -3.86 M161.72 13.56 C155.98 8.68, 149.72 5.09, 141.64 -2.51 M161.5 9.59 C156.25 7.51, 153.76 3.99, 149.4 -2.55 M161.88 9.79 C157.27 5.12, 151.98 1.57, 148.4 -2.14 M160.51 4.4 C159.46 1.23, 156.31 1.1, 154.9 -2.47 M161.4 3.64 C159.56 2.56, 157.99 0.96, 153.82 -1.97\" stroke=\"#000000\" stroke-width=\"0.5\" fill=\"none\"></path><path d=\"M1.79 -0.13 C61.9 0.77, 122.91 0.95, 158.29 0.02 M-0.36 -0.21 C51.78 0.19, 104.32 0.49, 158.66 0.04 M159.06 0.43 C156.96 18.27, 159.68 40.59, 158.92 80.43 M158.77 -0.88 C159.38 25.28, 158.99 52.46, 159.36 81.23 M157.62 81.5 C119.3 81.69, 79.58 82.14, 0.41 82.77 M158.1 81.46 C107.13 81.73, 57.59 83.21, 0.89 80.94 M0.46 79.54 C-0.73 56.24, 2.35 32.93, 1.32 -0.18 M-0.12 80.72 C0.38 57.86, -0.39 34.48, -0.53 -0.16\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g stroke-linecap=\"round\"><g transform=\"translate(237.2316647824316 96.63496277393824) rotate(89.99999999999994 6.404542569099007 -0.042439816807927855)\"><path d=\"M0.02 0.6 C3.47 0.64, 7.74 -0.16, 13.31 -0.69 M-0.5 0.33 C3.63 0.46, 8.25 -0.13, 12.58 -0.12\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(74.10056807830767 96.36111120194073) rotate(89.99999999999994 6.322398120230048 -0.015279910105164163)\"><path d=\"M-0.53 0.72 C3.53 -0.13, 7.98 -0.63, 13.17 -0.75 M-0.51 0.53 C3.58 -0.12, 6.88 -0.06, 12.69 -0.17\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g transform=\"translate(127.2537972896805 82.94647767704919) rotate(0 41.0888671875 4.954826242058516)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"8.258043736764487px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">stridedSmemOffset</text></g><g stroke-linecap=\"round\"><g transform=\"translate(81.19921361664501 95.8879231223018) rotate(0 80.40463361650933 -0.5932375211268663)\"><path d=\"M0.62 -0.62 C57.39 -0.73, 113.54 -1.57, 161.31 -1.58 M-0.5 0.26 C60.84 0.12, 122.59 -1.03, 159.3 0.39\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(159.00407760916545 -7.111398685561653) rotate(89.99999999999994 0.335018597270448 79.50160417042935)\"><path d=\"M0.41 -0.71 C0.07 46.48, 1.52 94.5, 1.63 159.52 M-0.34 0.18 C-0.59 47.53, -1.8 97.39, 0.03 159.72\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(234.7144939800508 72.76570572739365) rotate(89.99999999999994 6.342868463551284 -0.5515136239391722)\"><path d=\"M-0.27 -1 C4.13 -1.05, 8 -0.16, 12.96 -0.94 M0.14 -0.63 C3.56 0.13, 8.43 -0.14, 12.66 -0.17\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(72.94046592118178 73.46844597377094) rotate(89.99999999999994 6.303369519856801 -0.3830021288631542)\"><path d=\"M0.66 -1.02 C4.47 -0.03, 6.88 -0.7, 13.24 -0.73 M-0.64 0.25 C4.12 0.11, 7.97 0.06, 13.25 0.07\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g transform=\"translate(116.80790170388161 57.00484395211788) rotate(0 50.7568359375 4.954826242058516)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"8.258043736764487px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">contiguousTileNumMats</text></g><g stroke-linecap=\"round\"><g transform=\"translate(92.99271586807276 239.2734711867197) rotate(90.90647774714418 -0.7847358369911888 9.531670418513386)\"><path d=\"M0.08 -1.01 C0.68 3.87, 0.53 9.54, -1.94 18.6 M-0.38 0.18 C0.54 7.54, 0.08 13.08, 0.25 20.07\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(96.88275381648032 250.63857342081246) rotate(89.99999999999994 6.871586024428723 -0.1520279873002437)\"><path d=\"M0.33 -0.44 C4.06 0.26, 6.39 -1.39, 13.41 -0.25 M0.34 0.36 C4.07 -0.68, 8.2 -0.54, 12.82 0.27\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(76.48744468076887 250.29497534538496) rotate(89.99999999999994 6.691613682715797 -0.24961091281420522)\"><path d=\"M0.23 0.28 C1.97 -1.32, 6.95 -0.06, 13.06 -0.78 M0.05 -0.57 C4.26 -0.5, 8.12 0.3, 13.34 -0.27\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g transform=\"translate(88.9534029906381 289.9840996931689) rotate(0 55.5908203125 4.954826242058516)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"8.258043736764487px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">contiguousLoadMatOffset</text></g><g stroke-linecap=\"round\" transform=\"translate(122.22517133365488 198.14169985519948) rotate(0 20 20)\"><path d=\"M1.85 0.76 C13.97 -0.87, 27.62 -0.9, 40.39 -0.34 M0.55 -0.88 C10.61 -0.97, 22.69 -0.06, 39.79 -0.13 M39.14 -1.06 C41.32 14.56, 41.32 28.76, 38.53 41.71 M40.39 0.72 C39.52 14.2, 40.13 27.87, 40.78 40.37 M39.57 38.19 C25.03 39.72, 8.84 38.27, 0.71 41.24 M40.7 40.97 C28.65 39.31, 17.91 40.57, 0.67 40.71 M1.65 39.36 C-0.83 27.02, 0.74 10.86, 1.1 1.33 M0.53 40.16 C0.18 26.29, 0.41 11.06, -0.02 -0.23\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g stroke-linecap=\"round\" transform=\"translate(162.2750910965367 197.9285435162201) rotate(0 20 20)\"><path d=\"M1.93 -1.89 C11.43 -0.02, 24.29 1.51, 40.17 1.79 M-0.06 0.21 C9.06 0.65, 16.91 0.92, 39.88 -0.57 M39.5 -1.53 C41.68 15.14, 37.88 25.15, 40.69 38.72 M39.05 -0.87 C40.52 8.12, 40.78 15.27, 39.32 40.71 M39.23 38.67 C24.41 38.42, 8.82 41.62, -0.62 41.19 M39.22 39.27 C31.08 39.07, 19.74 40.57, 0.26 39.12 M0.13 40.89 C1.27 33.03, 1.3 25.08, -0.79 -0.41 M0.22 40.71 C0.45 26.1, -0.35 10.42, 0.22 -0.06\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g transform=\"translate(164.2750910965367 217.9285435162201) rotate(0 6.133514404296875 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">0</text></g><g transform=\"translate(187.93821298763032 196.830894054925) rotate(0 2.4159622192382812 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">1</text></g><g transform=\"translate(187.93821298763032 216.830894054925) rotate(0 2.4159622192382812 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">1</text></g><g transform=\"translate(208.73894975525786 197.449568362048) rotate(0 6.34747314453125 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">2</text></g><g transform=\"translate(208.73894975525786 217.449568362048) rotate(0 6.34747314453125 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">2</text></g><g transform=\"translate(226.31465879673578 197.77203840590664) rotate(0 6.071113586425781 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">3</text></g><g transform=\"translate(226.31465879673578 217.77203840590664) rotate(0 6.071113586425781 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">3</text></g><g transform=\"translate(164.2683215230943 197.8924391245273) rotate(0 6.133514404296875 11.14386119255505)\"><text x=\"0\" y=\"0\" font-family=\"Virgil, Segoe UI Emoji\" font-size=\"17.830177908088363px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">0</text></g><g stroke-linecap=\"round\" transform=\"translate(202.62796094434248 198.71305092573675) rotate(0 20 20)\"><path d=\"M-1.61 -1.33 C11.03 -1.68, 25.05 1.43, 38.2 -1.96 M-0.19 -0.69 C12.43 -0.48, 27.35 0.42, 40.85 -0.92 M41.23 1.73 C39.52 7.95, 39.36 16.65, 38.15 41.28 M39.3 -0.99 C39.45 10.04, 41.21 21.01, 39.53 40.97 M38.31 41.39 C27.07 40.54, 14.81 41.11, 0.96 41.96 M39.09 39.33 C29.8 40.95, 19.25 40.72, -0.72 39.18 M0.85 41.36 C1.34 25.48, 1.89 13.44, -0.69 -1.5 M0.47 39.97 C-0.04 31.86, 0.81 21.83, 0.37 -0.63\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g><g stroke-linecap=\"round\"><g transform=\"translate(124.5801113672045 242.1667835110511) rotate(89.99999999999994 -0.4032124299556301 41.66588734213383)\"><path d=\"M-1.34 1.82 C0.42 31.4, 1.09 63.65, -0.29 83.27 M-0.76 0.06 C-0.46 22.28, -0.18 46.88, 0.53 82.57\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(160.58622686845956 283.4173866141373) rotate(89.99999999999994 6.764755702299254 -0.6985622456486453)\"><path d=\"M0.1 -0.34 C3.87 -0.33, 6.5 -0.3, 12.59 -1.34 M0.15 -0.44 C3.4 0.54, 6.5 -0.63, 13.43 -0.93\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g stroke-linecap=\"round\"><g transform=\"translate(76.79696797126599 284.46615987833684) rotate(89.99999999999994 6.514287768457876 -0.3276911292159639)\"><path d=\"M0.53 -0.13 C4.86 -0.11, 9.78 -1.24, 12.11 -1.27 M0.32 0.62 C4.3 -0.48, 8.43 -0.73, 12.71 -0.47\" stroke=\"#1e1e1e\" stroke-width=\"1\" fill=\"none\"></path></g></g><mask></mask><g transform=\"translate(10 10) rotate(0 70.3125 9.600000000000364)\"><text x=\"0\" y=\"0\" font-family=\"Cascadia, Segoe UI Emoji\" font-size=\"16px\" fill=\"#1e1e1e\" text-anchor=\"start\" style=\"white-space: pre;\" direction=\"ltr\" dominant-baseline=\"text-before-edge\">Contiguous axis</text></g></svg>"}, {"filename": "docs/conf.py", "status": "modified", "additions": 33, "deletions": 19, "changes": 52, "file_content_changes": "@@ -1,4 +1,3 @@\n-#!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n #\n # Triton documentation build configuration file, created by\n@@ -24,70 +23,84 @@\n # -- General configuration ------------------------------------------------\n \n \n+import os\n+import sys\n+\n+import sphinx_rtd_theme\n+from sphinx_gallery.sorting import FileNameSortKey\n \n \n def process_sig(app, what, name, obj, options, signature, return_annotation):\n     if signature and '_builder' in signature:\n-        signature = signature.split('_builder')[0] + \")\" \n+        signature = signature.split('_builder')[0] + \")\"\n     return (signature, return_annotation)\n \n+\n def setup(app):\n     \"\"\"Customize function args retrieving to get args under decorator.\"\"\"\n-    import sphinx\n     import os\n \n+    import sphinx\n+\n     app.connect(\"autodoc-process-signature\", process_sig)\n     os.system(\"pip install -e ../python\")\n \n-\n     def forward_jit_fn(func):\n         old = func\n \n         def wrapped(obj, **kwargs):\n             import triton\n-            if isinstance(obj, triton.code_gen.JITFunction):\n+            if isinstance(obj, triton.runtime.JITFunction):\n                 obj = obj.fn\n             return old(obj)\n \n         return wrapped\n \n-\n     old_documenter = sphinx.ext.autosummary.get_documenter\n \n     def documenter(app, obj, parent):\n         import triton\n-        if isinstance(obj, triton.code_gen.JITFunction):\n+        if isinstance(obj, triton.runtime.JITFunction):\n             obj = obj.fn\n         return old_documenter(app, obj, parent)\n \n     sphinx.ext.autosummary.get_documenter = documenter\n-    sphinx.util.inspect.unwrap_all = forward_jit_fn(sphinx.util.inspect.unwrap_all)\n-    sphinx.util.inspect.signature = forward_jit_fn(sphinx.util.inspect.signature)\n-    sphinx.util.inspect.object_description = forward_jit_fn(sphinx.util.inspect.object_description)\n+    sphinx.util.inspect.unwrap_all = forward_jit_fn(\n+        sphinx.util.inspect.unwrap_all)\n+    sphinx.util.inspect.signature = forward_jit_fn(\n+        sphinx.util.inspect.signature)\n+    sphinx.util.inspect.object_description = forward_jit_fn(\n+        sphinx.util.inspect.object_description)\n \n \n # Auto Doc\n-import sys\n-import os\n+\n sys.path.insert(0, os.path.abspath('../python/'))\n-extensions = ['sphinx.ext.autodoc', 'sphinx.ext.intersphinx', 'sphinx.ext.autosummary', 'sphinx.ext.coverage', 'sphinx.ext.napoleon', 'sphinx_multiversion']\n+extensions = [\n+    'sphinx.ext.autodoc',\n+    'sphinx.ext.intersphinx',\n+    'sphinx.ext.autosummary',\n+    'sphinx.ext.coverage',\n+    'sphinx.ext.napoleon',\n+    'sphinx_multiversion']\n autosummary_generate = True\n \n # versioning config\n-smv_tag_whitelist = r'^(v1.1.2)$'\n-smv_branch_whitelist = r'^master$'\n+smv_tag_whitelist = r'^(v2.1.0)$'\n+smv_branch_whitelist = r'^main$'\n smv_remote_whitelist = None\n smv_released_pattern = r'^tags/.*$'\n smv_outputdir_format = '{ref.name}'\n smv_prefer_remote_refs = False\n \n # Sphinx gallery\n extensions += ['sphinx_gallery.gen_gallery']\n-from sphinx_gallery.sorting import FileNameSortKey\n+\n sphinx_gallery_conf = {\n     'examples_dirs': '../python/tutorials/',\n     'gallery_dirs': 'getting-started/tutorials',\n     'filename_pattern': '',\n+    # XXX: Temporarily disable fused attention tutorial on V100\n     'ignore_pattern': r'__init__\\.py',\n     'within_subsection_order': FileNameSortKey,\n     'reference_url': {\n@@ -149,7 +162,7 @@ def documenter(app, obj, parent):\n # The theme to use for HTML and HTML Help pages.  See the documentation for\n # a list of builtin themes.\n #\n-import sphinx_rtd_theme\n+\n html_theme = 'sphinx_rtd_theme'\n html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n \n@@ -223,5 +236,6 @@ def documenter(app, obj, parent):\n # (source start file, target name, title, author,\n #  dir menu entry, description, category)\n texinfo_documents = [\n-    (master_doc, 'Triton', 'Triton Documentation', author, 'Triton', 'One line description of project.', 'Miscellaneous'),\n-]\n\\ No newline at end of file\n+    (master_doc, 'Triton', 'Triton Documentation', author,\n+     'Triton', 'One line description of project.', 'Miscellaneous'),\n+]"}, {"filename": "docs/getting-started/installation.rst", "status": "modified", "additions": 14, "deletions": 14, "changes": 28, "file_content_changes": "@@ -1,41 +1,41 @@\n-==============\n+============\n Installation\n-==============\n+============\n \n----------------------\n+--------------------\n Binary Distributions\n----------------------\n+--------------------\n \n You can install the latest stable release of Triton from pip:\n \n .. code-block:: bash\n \n       pip install triton\n \n-Binary wheels are available for CPython 3.6-3.9 and PyPy 3.6-3.7.\n+Binary wheels are available for CPython 3.8-3.11 and PyPy 3.8-3.9.\n \n And the latest nightly release:\n \n .. code-block:: bash\n-  \n-      pip install -U --pre triton\n \n+      pip install -U --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly\n \n---------------\n+\n+-----------\n From Source\n---------------\n+-----------\n \n-+++++++++++++++\n+++++++++++++++\n Python Package\n-+++++++++++++++\n+++++++++++++++\n \n You can install the Python package from source by running the following commands:\n \n .. code-block:: bash\n \n       git clone https://github.com/openai/triton.git;\n       cd triton/python;\n-      pip install cmake; # build time dependency\n+      pip install cmake; # build-time dependency\n       pip install -e .\n \n Note that, if llvm-11 is not present on your system, the setup.py script will download the official LLVM11 static libraries link against that.\n@@ -50,6 +50,6 @@ You can then test your installation by running the unit tests:\n and the benchmarks\n \n .. code-block:: bash\n-      \n-      cd bench/\n+\n+      cd bench\n       python -m run --with-plots --result-dir /tmp/triton-bench"}, {"filename": "docs/getting-started/tutorials/parallel_reduction.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "docs/index.rst", "status": "modified", "additions": 8, "deletions": 4, "changes": 12, "file_content_changes": "@@ -1,7 +1,8 @@\n Welcome to Triton's documentation!\n ==================================\n \n-Triton is a language and compiler for parallel programming. It aims to provide a Python-based programming environment for productively writing custom DNN compute kernels capable of running at maximal throughput on modern GPU hardware.\n+Triton_ is a language and compiler for parallel programming. It aims to provide a Python-based programming environment for productively writing custom DNN compute kernels capable of running at maximal throughput on modern GPU hardware.\n+\n \n Getting Started\n ---------------\n@@ -17,8 +18,9 @@ Getting Started\n    getting-started/installation\n    getting-started/tutorials/index\n \n+\n Python API\n--------------------\n+----------\n \n - :doc:`triton <python-api/triton>`\n - :doc:`triton.language <python-api/triton.language>`\n@@ -34,9 +36,9 @@ Python API\n    python-api/triton.language\n    python-api/triton.testing\n \n-   \n+\n Going Further\n-------------------\n+-------------\n \n Check out the following documents to learn more about Triton and how it compares against other DSLs for DNNs:\n \n@@ -50,3 +52,5 @@ Check out the following documents to learn more about Triton and how it compares\n \n    programming-guide/chapter-1/introduction\n    programming-guide/chapter-2/related-work\n+\n+.. _Triton: https://github.com/openai/triton"}, {"filename": "docs/programming-guide/chapter-1/introduction.rst", "status": "modified", "additions": 19, "deletions": 17, "changes": 36, "file_content_changes": "@@ -1,18 +1,18 @@\n-==============\n+============\n Introduction\n-==============\n+============\n \n---------------\n+-----------\n Motivations\n---------------\n+-----------\n \n-Over the past decade, Deep Neural Networks (DNNs) have emerged as an important class of Machine Learning (ML) models, capable of  achieving state-of-the-art performance across many domains ranging from natural language processing [SUTSKEVER2014]_ to computer vision [REDMON2016]_ to computational neuroscience [LEE2017]_. The strength of these models lies in their hierarchical structure, composed of a sequence of parametric (e.g., convolutional) and non-parametric (e.g., rectified linearity) *layers*. This pattern, though notoriously computationally expensive, also generates a large amount of highly parallelizable work particularly well suited for multi- and many- core processors.\n+Over the past decade, Deep Neural Networks (DNNs) have emerged as an important class of Machine Learning (ML) models, capable of achieving state-of-the-art performance across many domains ranging from natural language processing [SUTSKEVER2014]_ to computer vision [REDMON2016]_ to computational neuroscience [LEE2017]_. The strength of these models lies in their hierarchical structure, composed of a sequence of parametric (e.g., convolutional) and non-parametric (e.g., rectified linearity) *layers*. This pattern, though notoriously computationally expensive, also generates a large amount of highly parallelizable work particularly well suited for multi- and many- core processors.\n \n As a consequence, Graphics Processing Units (GPUs) have become a cheap and accessible resource for exploring and/or deploying novel research ideas in the field. This trend has been accelerated by the release of several frameworks for General-Purpose GPU (GPGPU) computing, such as CUDA and OpenCL, which have made the development of high-performance programs easier. Yet, GPUs remain incredibly challenging to optimize for locality and parallelism, especially for computations that cannot be efficiently implemented using a combination of pre-existing optimized primitives. To make matters worse, GPU architectures are also rapidly evolving and specializing, as evidenced by the addition of tensor cores to NVIDIA (and more recently AMD) micro-architectures.\n \n-This tension between the computational opportunities offered by DNNs and the practical difficulty of GPU programming has created substantial academic and industrial interest for Domain-Specific Languages (DSLs) and compilers. Regrettably, these systems -- whether they be based on  polyhedral machinery (*e.g.*, Tiramisu [BAGHDADI2021]_, Tensor Comprehensions [VASILACHE2018]_) or scheduling languages (*e.g.*, Halide [JRK2013]_, TVM [CHEN2018]_) -- remain less flexible and (for the same algorithm) markedly slower than the best handwritten compute kernels available in libraries like `cuBLAS <https://docs.nvidia.com/cuda/cublas/index.html>`_, `cuDNN <https://docs.nvidia.com/deeplearning/cudnn/api/index.html>`_ or `TensorRT <https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html>`_.\n+This tension between the computational opportunities offered by DNNs and the practical difficulty of GPU programming has created substantial academic and industrial interest for Domain-Specific Languages (DSLs) and compilers. Regrettably, these systems -- whether they be based on polyhedral machinery (e.g., Tiramisu [BAGHDADI2021]_, Tensor Comprehensions [VASILACHE2018]_) or scheduling languages (e.g., Halide [JRK2013]_, TVM [CHEN2018]_) -- remain less flexible and (for the same algorithm) markedly slower than the best handwritten compute kernels available in libraries like `cuBLAS <https://docs.nvidia.com/cuda/cublas/index.html>`_, `cuDNN <https://docs.nvidia.com/deeplearning/cudnn/api/index.html>`_ or `TensorRT <https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html>`_.\n \n-The main premise of this project is the following: programming paradigms based on blocked algorithms [LAM1991]_ can facilitate the construction of high-performance compute kernels for neural networks.  We specifically revisit traditional \"Single Program, Multiple Data\" (SPMD [AUGUIN1983]_) execution models for GPUs, and propose a variant in which programs -- rather than threads -- are blocked. For example, in the case of matrix multiplication, CUDA and Triton differ as follows:\n+The main premise of this project is the following: programming paradigms based on blocked algorithms [LAM1991]_ can facilitate the construction of high-performance compute kernels for neural networks. We specifically revisit traditional \"Single Program, Multiple Data\" (SPMD [AUGUIN1983]_) execution models for GPUs, and propose a variant in which programs -- rather than threads -- are blocked. For example, in the case of matrix multiplication, CUDA and Triton differ as follows:\n \n .. table::\n     :widths: 50 50\n@@ -27,14 +27,14 @@ The main premise of this project is the following: programming paradigms based o\n     |                                                     |   :force:                                           |\n     |                                                     |                                                     |\n     |   #pragma parallel                                  |   #pragma parallel                                  |\n-    |   for(int m = 0; i < M; m++)                        |   for(int m = 0; m < M; m += MB)                    |\n+    |   for(int m = 0; m < M; m++)                        |   for(int m = 0; m < M; m += MB)                    |\n     |   #pragma parallel                                  |   #pragma parallel                                  |\n-    |   for(int n = 0; j < N; n++){                       |   for(int n = 0; n < N; n += NB){                   |\n+    |   for(int n = 0; n < N; n++){                       |   for(int n = 0; n < N; n += NB){                   |\n     |     float acc = 0;                                  |     float acc[MB, NB] = 0;                          |\n-    |     for(int k = 0; k < K;k ++)                      |     for(int k = 0; k < K; k += KB)                  |\n-    |       acc += A[i, k]* B[k, j];                      |       acc +=  A[m:m+MB, k:k+KB]                     |\n+    |     for(int k = 0; k < K; k++)                      |     for(int k = 0; k < K; k += KB)                  |\n+    |       acc += A[m, k] * B[k, n];                     |       acc +=  A[m:m+MB, k:k+KB]                     |\n     |                                                     |             @ B[k:k+KB, n:n+NB];                    |\n-    |     C[i, j] = acc;                                  |     C[m:m+MB, n:n+NB] = acc;                        |\n+    |     C[m, n] = acc;                                  |     C[m:m+MB, n:n+NB] = acc;                        |\n     |   }                                                 |   }                                                 |\n     |                                                     |                                                     |\n     +-----------------------------------------------------+-----------------------------------------------------+\n@@ -48,15 +48,17 @@ The main premise of this project is the following: programming paradigms based o\n \n A key benefit of this approach is that it leads to block-structured iteration spaces that offer programmers more flexibility than existing DSLs when implementing sparse operations, all while allowing compilers to aggressively optimize programs for data locality and parallelism.\n \n---------------\n+\n+----------\n Challenges\n---------------\n+----------\n \n The main challenge posed by our proposed paradigm is that of work scheduling, i.e., how the work done by each program instance should be partitioned for efficient execution on modern GPUs. To address this issue, the Triton compiler makes heavy use of *block-level data-flow analysis*, a technique for scheduling iteration blocks statically based on the control- and data-flow structure of the target program. The resulting system actually works surprisingly well: our compiler manages to apply a broad range of interesting optimization automatically (e.g., automatic coalescing, thread swizzling, pre-fetching, automatic vectorization, tensor core-aware instruction selection, shared memory allocation/synchronization, asynchronous copy scheduling). Of course doing all this is not trivial; one of the purposes of this guide is to give you a sense of how it works.\n \n---------------\n+\n+----------\n References\n---------------\n+----------\n \n .. [SUTSKEVER2014] I. Sutskever et al., \"Sequence to Sequence Learning with Neural Networks\", NIPS 2014\n .. [REDMON2016] J. Redmon et al., \"You Only Look Once: Unified, Real-Time Object Detection\", CVPR 2016\n@@ -66,4 +68,4 @@ References\n .. [JRK2013] J. Ragan-Kelley et al., \"Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines\", PLDI 2013\n .. [CHEN2018] T. Chen et al., \"TVM: An Automated End-to-End Optimizing Compiler for Deep Learning\", OSDI 2018\n .. [LAM1991] M. Lam et al., \"The Cache Performance and Optimizations of Blocked Algorithms\", ASPLOS 1991\n-.. [AUGUIN1983] M. Auguin et al., \"Opsila: an advanced SIMD for numerical analysis and signal processing\", EUROMICRO 1983\n\\ No newline at end of file\n+.. [AUGUIN1983] M. Auguin et al., \"Opsila: an advanced SIMD for numerical analysis and signal processing\", EUROMICRO 1983"}, {"filename": "docs/programming-guide/chapter-2/related-work.rst", "status": "modified", "additions": 36, "deletions": 33, "changes": 69, "file_content_changes": "@@ -1,18 +1,19 @@\n-==============\n+============\n Related Work\n-==============\n+============\n \n At first sight, Triton may seem like just yet another DSL for DNNs. The purpose of this section is to contextualize Triton and highlight its differences with the two leading approaches in this domain: polyhedral compilation and scheduling languages.\n \n------------------------\n+\n+----------------------\n Polyhedral Compilation\n------------------------\n+----------------------\n \n Traditional compilers typically rely on intermediate representations, such as LLVM-IR [LATTNER2004]_, that encode control flow information using (un)conditional branches. This relatively low-level format makes it difficult to statically analyze the runtime behavior (e.g., cache misses) of input programs, and to  automatically optimize loops accordingly through the use of tiling [WOLFE1989]_, fusion [DARTE1999]_ and interchange [ALLEN1984]_. To solve this issue, polyhedral compilers [ANCOURT1991]_ rely on program representations that have statically predictable control flow, thereby enabling aggressive compile-time program transformations for data locality and parallelism. Though this strategy has been adopted by many languages and compilers for DNNs such as Tiramisu [BAGHDADI2021]_, Tensor Comprehensions [VASILACHE2018]_, Diesel [ELANGO2018]_ and the Affine dialect in MLIR [LATTNER2019]_, it also comes with a number of limitations that will be described later in this section.\n \n-+++++++++++++++++++++++\n+++++++++++++++++++++++\n Program Representation\n-+++++++++++++++++++++++\n+++++++++++++++++++++++\n \n Polyhedral compilation is a vast area of research. In this section we only outline the most basic aspects of this topic, but readers interested in the solid mathematical foundations underneath may refer to the ample literature on linear and integer programming.\n \n@@ -105,19 +106,19 @@ Where :math:`\\Theta_S(\\mathbf{x})` is a p-dimensional vector representing the sl\n \n where :math:`i` and :math:`j` are respectively the slowest and fastest growing loop indices in the nest. If :math:`T_S` is a vector (resp. tensor), then :math:`\\Theta_S` is a said to be one-dimensional (resp. multi-dimensional).\n \n-+++++++++++\n+++++++++++\n Advantages\n-+++++++++++\n+++++++++++\n \n Programs amenable to polyhedral compilation can be aggressively transformed and optimized. Most of these transformations actually boil down to the production of  schedules and iteration domains that enable loop transformations promoting parallelism and spatial/temporal data locality (e.g., fusion, interchange, tiling, parallelization).\n \n Polyhedral compilers can also automatically go through complex verification processes to ensure that the semantics of their input program is preserved throughout this optimization phase. Note that polyhedral optimizers are not incompatible with more standard optimization techniques. In fact, it is not uncommon for these systems to be implemented as a set of LLVM passes that can be run ahead of more traditional compilation techniques [GROSSER2012]_.\n \n-All in all, polyhedral machinery is extremely powerful, when applicable. It has been shown to support most common loop transformations, and has indeed achieved performance comparable to state-of-the-art GPU libraries for dense matrix multiplication [ELANGO2018]_. Additionally, it is also fully automatic and doesn't require any hint from programmers apart from source-code in a C-like format. \n+All in all, polyhedral machinery is extremely powerful, when applicable. It has been shown to support most common loop transformations, and has indeed achieved performance comparable to state-of-the-art GPU libraries for dense matrix multiplication [ELANGO2018]_. Additionally, it is also fully automatic and doesn't require any hint from programmers apart from source-code in a C-like format.\n \n-++++++++++++\n++++++++++++\n Limitations\n-++++++++++++\n++++++++++++\n \n Unfortunately, polyhedral compilers suffer from two major limitations that have prevented its adoption as a universal method for code generation in neural networks.\n \n@@ -127,46 +128,47 @@ Second, the polyhedral framework is not very generally applicable; SCoPs are rel\n \n On the other hand, blocked program representations advocated by this dissertation are less restricted in scope and can achieve close to peak performance using standard dataflow analysis.\n \n------------------------\n+\n+--------------------\n Scheduling Languages\n------------------------\n+--------------------\n \n-Separation of concerns [DIJKSTRA82]_ is a well-known design principle in computer science: programs should be decomposed into modular layers of abstraction that separate the semantics of their algorithms from the details of their implementation. Systems like Halide and TVM push this philosophy one step further, and enforce this separation at the grammatical level through the use of a  **scheduling language**. The benefits of this methodology are particularly visible in the case of matrix multiplication, where, as one can see below, the definition of the algorithm (Line 1-7) is completely disjoint from its implementation (Line 8-16), meaning that both can be maintained, optimized and distributed independently. \n+Separation of concerns [DIJKSTRA82]_ is a well-known design principle in computer science: programs should be decomposed into modular layers of abstraction that separate the semantics of their algorithms from the details of their implementation. Systems like Halide and TVM push this philosophy one step further, and enforce this separation at the grammatical level through the use of a  **scheduling language**. The benefits of this methodology are particularly visible in the case of matrix multiplication, where, as one can see below, the definition of the algorithm (Line 1-7) is completely disjoint from its implementation (Line 8-16), meaning that both can be maintained, optimized and distributed independently.\n \n .. code-block:: python\n   :linenos:\n \n   // algorithm\n   Var x(\"x\"), y(\"y\");\n-  Func matmul(\"matmul\"); \n-  RDom k(0, matrix_size); \n-  RVar ki; \n-  matmul(x, y) = 0.0f; \n-  matmul(x, y) += A(k, y) * B(x, k); \n+  Func matmul(\"matmul\");\n+  RDom k(0, matrix_size);\n+  RVar ki;\n+  matmul(x, y) = 0.0f;\n+  matmul(x, y) += A(k, y) * B(x, k);\n   // schedule\n-  Var xi(\"xi\"), xo(\"xo\"), yo(\"yo\"), yi(\"yo\"), yii(\"yii\"), xii(\"xii\"); \n-  matmul.vectorize(x, 8); \n-  matmul.update(0) \n-      .split(x, x, xi, block_size).split(xi, xi, xii, 8) \n-      .split(y, y, yi, block_size).split(yi, yi, yii, 4) \n-      .split(k, k, ki, block_size) \n-      .reorder(xii, yii, xi, ki, yi, k, x, y) \n+  Var xi(\"xi\"), xo(\"xo\"), yo(\"yo\"), yi(\"yo\"), yii(\"yii\"), xii(\"xii\");\n+  matmul.vectorize(x, 8);\n+  matmul.update(0)\n+      .split(x, x, xi, block_size).split(xi, xi, xii, 8)\n+      .split(y, y, yi, block_size).split(yi, yi, yii, 4)\n+      .split(k, k, ki, block_size)\n+      .reorder(xii, yii, xi, ki, yi, k, x, y)\n       .parallel(y).vectorize(xii).unroll(xi).unroll(yii);\n \n \n The resulting code may however not be completely portable, as schedules can sometimes rely on execution models (e.g., SPMD) or hardware intrinsics (e.g., matrix-multiply-accumulate) that are not widely available. This issue can be mitigated by auto-scheduling mechanisms [MULLAPUDI2016]_.\n \n-+++++++++++\n+++++++++++\n Advantages\n-+++++++++++\n+++++++++++\n \n The main advantage of this approach is that it allows programmers to write an algorithm *only once*, and focus on performance optimization separately. It makes it possible to manually specify optimizations that a polyhedral compiler wouldn't be able to figure out automatically using static data-flow analysis.\n \n Scheduling languages are, without a doubt, one of the most popular approaches for neural network code generation. The most popular system for this purpose is probably TVM, which provides good performance across a wide range of platforms as well as built-in automatic scheduling mechanisms.\n \n-++++++++++++\n++++++++++++\n Limitations\n-++++++++++++\n++++++++++++\n \n This ease-of-development comes at a cost. First of all, existing systems that follow this paradigm tend to be noticeably slower than Triton on modern hardware when applicable (e.g., V100/A100 tensor cores w/ equal tile sizes). I do believe that this is not a fundamental issue of scheduling languages -- in the sense that it could probably be solved with more efforts -- but it could mean that these systems are harder to engineer. More importantly, existing scheduling languages generate loops whose bounds and increments cannot depend on surrounding loop indices without at least imposing severe constraints on possible schedules -- if not breaking the system entirely. This is problematic for sparse computations, whose iteration spaces may be irregular.\n \n@@ -181,7 +183,7 @@ This ease-of-development comes at a cost. First of all, existing systems that fo\n     |   for(int j = 0; j < 4; j++)                        |                                                     |\n     |     float acc = 0;                                  |                                                     |\n     |     for(int k = 0; k < K[i]; k++)                   |                                                     |\n-    |       acc += A[i][col[i,k]]*B[k][j]                 |                                                     |\n+    |       acc += A[i][col[i, k]] * B[k][j]              |                                                     |\n     |     C[i][j] = acc;                                  |                                                     |\n     +-----------------------------------------------------+-----------------------------------------------------+\n \n@@ -190,9 +192,10 @@ This ease-of-development comes at a cost. First of all, existing systems that fo\n \n On the other hand, the block-based program representation that we advocate for through this work allows for block-structured iteration spaces and allows programmers to manually handle load-balancing as they wish.\n \n---------------\n+\n+----------\n References\n---------------\n+----------\n \n .. [LATTNER2004] C. Lattner et al., \"LLVM: a compilation framework for lifelong program analysis transformation\", CGO 2004\n .. [WOLFE1989] M. Wolfe, \"More Iteration Space Tiling\", SC 1989"}, {"filename": "docs/python-api/triton.language.rst", "status": "modified", "additions": 54, "deletions": 16, "changes": 70, "file_content_changes": "@@ -1,11 +1,11 @@\n triton.language\n-================\n+===============\n \n .. currentmodule:: triton.language\n \n \n Programming Model\n--------------------\n+-----------------\n \n .. autosummary::\n     :toctree: generated\n@@ -16,40 +16,46 @@ Programming Model\n \n \n Creation Ops\n--------------\n+------------\n \n .. autosummary::\n     :toctree: generated\n     :nosignatures:\n \n     arange\n+    cat\n+    full\n     zeros\n \n \n Shape Manipulation Ops\n------------------------\n+----------------------\n \n .. autosummary::\n     :toctree: generated\n     :nosignatures:\n \n+    broadcast\n     broadcast_to\n-    reshape\n+    expand_dims\n     ravel\n-\n+    reshape\n+    trans\n+    view\n \n \n Linear Algebra Ops\n--------------------\n+------------------\n \n .. autosummary::\n     :toctree: generated\n     :nosignatures:\n \n     dot\n \n+\n Memory Ops\n---------------------\n+----------\n \n .. autosummary::\n     :toctree: generated\n@@ -62,7 +68,7 @@ Memory Ops\n \n \n Indexing Ops\n---------------\n+------------\n \n .. autosummary::\n     :toctree: generated\n@@ -72,34 +78,42 @@ Indexing Ops\n \n \n Math Ops\n-----------\n+--------\n \n .. autosummary::\n     :toctree: generated\n     :nosignatures:\n \n+    abs\n     exp\n     log\n+    fdiv\n     cos\n     sin\n     sqrt\n     sigmoid\n     softmax\n+    umulhi\n \n \n Reduction Ops\n----------------\n+-------------\n \n .. autosummary::\n     :toctree: generated\n     :nosignatures:\n \n+    argmax\n+    argmin\n     max\n     min\n+    reduce\n     sum\n+    xor_sum\n+\n \n Atomic Ops\n----------------\n+----------\n \n .. autosummary::\n     :toctree: generated\n@@ -112,7 +126,7 @@ Atomic Ops\n \n \n Comparison ops\n----------------\n+--------------\n \n .. autosummary::\n     :toctree: generated\n@@ -124,7 +138,7 @@ Comparison ops\n .. _Random Number Generation:\n \n Random Number Generation\n--------------------------\n+------------------------\n \n .. autosummary::\n     :toctree: generated\n@@ -135,11 +149,35 @@ Random Number Generation\n     rand\n     randn\n \n+\n Compiler Hint Ops\n--------------------\n+-----------------\n+\n+.. autosummary::\n+    :toctree: generated\n+    :nosignatures:\n+\n+    debug_barrier\n+    max_contiguous\n+    multiple_of\n+\n+Debug Ops\n+-----------------\n+\n+.. autosummary::\n+    :toctree: generated\n+    :nosignatures:\n+\n+    static_print\n+    static_assert\n+    device_print\n+    device_assert\n+\n+Iterators\n+-----------------\n \n .. autosummary::\n     :toctree: generated\n     :nosignatures:\n \n-    multiple_of\n\\ No newline at end of file\n+    static_range"}, {"filename": "docs/python-api/triton.rst", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n triton\n-========\n+======\n \n .. currentmodule:: triton\n \n@@ -10,4 +10,4 @@ triton\n     jit\n     autotune\n     heuristics\n-    Config\n\\ No newline at end of file\n+    Config"}, {"filename": "docs/python-api/triton.testing.rst", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n triton.testing\n-================\n+==============\n \n .. currentmodule:: triton.testing\n \n@@ -9,4 +9,4 @@ triton.testing\n \n     do_bench\n     Benchmark\n-    perf_report\n\\ No newline at end of file\n+    perf_report"}, {"filename": "include/triton/Analysis/Alias.h", "status": "modified", "additions": 21, "deletions": 6, "changes": 27, "file_content_changes": "@@ -2,7 +2,7 @@\n #define TRITON_ANALYSIS_ALIAS_H\n \n #include \"mlir/Analysis/AliasAnalysis.h\"\n-#include \"mlir/Analysis/DataFlowAnalysis.h\"\n+#include \"mlir/Analysis/DataFlow/SparseAnalysis.h\"\n #include \"llvm/ADT/DenseSet.h\"\n \n namespace mlir {\n@@ -21,14 +21,18 @@ class AliasInfo {\n   }\n \n   /// The pessimistic value state of a value without alias\n-  static AliasInfo getPessimisticValueState(MLIRContext *context) {\n+  static AliasInfo getPessimisticValueState(MLIRContext *context = nullptr) {\n     return AliasInfo();\n   }\n   static AliasInfo getPessimisticValueState(Value value) { return AliasInfo(); }\n \n   /// The union of both arguments\n   static AliasInfo join(const AliasInfo &lhs, const AliasInfo &rhs);\n \n+  void print(raw_ostream &os) const {\n+    llvm::interleaveComma(allocs, os, [&](Value alloc) { alloc.print(os); });\n+  }\n+\n private:\n   /// The set of allocated values that are aliased by this lattice.\n   /// For now, we only consider aliased value produced by the following\n@@ -58,9 +62,13 @@ class AliasInfo {\n //===----------------------------------------------------------------------===//\n // Shared Memory Alias Analysis\n //===----------------------------------------------------------------------===//\n-class SharedMemoryAliasAnalysis : public ForwardDataFlowAnalysis<AliasInfo> {\n+class SharedMemoryAliasAnalysis\n+    : public dataflow::SparseDataFlowAnalysis<dataflow::Lattice<AliasInfo>> {\n public:\n-  using ForwardDataFlowAnalysis<AliasInfo>::ForwardDataFlowAnalysis;\n+  using dataflow::SparseDataFlowAnalysis<\n+      dataflow::Lattice<AliasInfo>>::SparseDataFlowAnalysis;\n+  using dataflow::SparseDataFlowAnalysis<\n+      dataflow::Lattice<AliasInfo>>::getLatticeElement;\n \n   /// XXX(Keren): Compatible interface with MLIR AliasAnalysis for future use.\n   /// Given two values, returns their aliasing behavior.\n@@ -69,10 +77,17 @@ class SharedMemoryAliasAnalysis : public ForwardDataFlowAnalysis<AliasInfo> {\n   /// Returns the modify-reference behavior of `op` on `location`.\n   ModRefResult getModRef(Operation *op, Value location);\n \n+  void setToEntryState(dataflow::Lattice<AliasInfo> *lattice) override {\n+    propagateIfChanged(\n+        lattice, lattice->join(\n+                     AliasInfo::getPessimisticValueState(lattice->getPoint())));\n+  }\n+\n   /// Computes if the alloc set of the results are changed.\n-  ChangeResult\n+  void\n   visitOperation(Operation *op,\n-                 ArrayRef<LatticeElement<AliasInfo> *> operands) override;\n+                 ArrayRef<const dataflow::Lattice<AliasInfo> *> operands,\n+                 ArrayRef<dataflow::Lattice<AliasInfo> *> results) override;\n };\n \n } // namespace mlir"}, {"filename": "include/triton/Analysis/Allocation.h", "status": "modified", "additions": 78, "deletions": 19, "changes": 97, "file_content_changes": "@@ -1,6 +1,7 @@\n #ifndef TRITON_ANALYSIS_ALLOCATION_H\n #define TRITON_ANALYSIS_ALLOCATION_H\n \n+#include \"triton/Analysis/Utility.h\"\n #include \"llvm/ADT/DenseMap.h\"\n #include \"llvm/ADT/MapVector.h\"\n #include \"llvm/ADT/SetVector.h\"\n@@ -49,18 +50,25 @@ template <typename T> class Interval {\n   T End = std::numeric_limits<T>::max();\n };\n \n+template <class T> Interval(T, T) -> Interval<T>;\n+\n class Allocation {\n public:\n   /// A unique identifier for shared memory buffers\n   using BufferId = size_t;\n   using BufferIdSetT = DenseSet<BufferId>;\n+  using FuncAllocMapT = CallGraph<Allocation>::FuncDataMapT;\n \n   static constexpr BufferId InvalidBufferId =\n       std::numeric_limits<BufferId>::max();\n \n+  Allocation() = default;\n   /// Creates a new Allocation analysis that computes the shared memory\n   /// information for all associated shared memory values.\n-  Allocation(Operation *operation) : operation(operation) { run(); }\n+  explicit Allocation(Operation *operation) : operation(operation) {}\n+\n+  /// Runs allocation analysis on the given top-level operation.\n+  void run(FuncAllocMapT &funcAllocMap);\n \n   /// Returns the operation this analysis was constructed from.\n   Operation *getOperation() const { return operation; }\n@@ -75,6 +83,12 @@ class Allocation {\n     return bufferSet.at(bufferId).size;\n   }\n \n+  /// Returns the allocated interval of the given buffer.\n+  Interval<size_t> getAllocatedInterval(BufferId bufferId) const {\n+    auto &buffer = bufferSet.at(bufferId);\n+    return Interval<size_t>(buffer.offset, buffer.offset + buffer.size);\n+  }\n+\n   /// Returns the buffer id of the given value.\n   /// This interface only returns the allocated buffer id.\n   /// If you want to get all the buffer ids that are associated with the given\n@@ -104,26 +118,28 @@ class Allocation {\n   BufferId getBufferId(Operation *operation) const {\n     if (opScratch.count(operation)) {\n       return opScratch.lookup(operation)->id;\n+    } else if (opVirtual.count(operation)) {\n+      return opVirtual.lookup(operation)->id;\n     } else {\n       return InvalidBufferId;\n     }\n   }\n \n+  /// Returns if the given buffer is a virtual buffer.\n+  bool isVirtualBuffer(BufferId bufferId) const {\n+    return bufferSet.at(bufferId).kind == BufferT::BufferKind::Virtual;\n+  }\n+\n   /// Returns the size of total shared memory allocated\n   size_t getSharedMemorySize() const { return sharedMemorySize; }\n \n-  bool isIntersected(BufferId lhsId, BufferId rhsId) const {\n-    if (lhsId == InvalidBufferId || rhsId == InvalidBufferId)\n-      return false;\n-    auto lhsBuffer = bufferSet.at(lhsId);\n-    auto rhsBuffer = bufferSet.at(rhsId);\n-    return lhsBuffer.intersects(rhsBuffer);\n-  }\n-\n private:\n   /// A class that represents a shared memory buffer\n   struct BufferT {\n-    enum class BufferKind { Explicit, Scratch };\n+    /// Explicit: triton_gpu.alloc_tensor\n+    /// Scratch: triton_gpu.convert_layout\n+    /// Virtual: triton.call\n+    enum class BufferKind { Explicit, Scratch, Virtual };\n \n     /// MT: thread-safe\n     inline static std::atomic<BufferId> nextId = 0;\n@@ -142,12 +158,6 @@ class Allocation {\n     BufferT(BufferKind kind, size_t size) : BufferT(kind, size, 0) {}\n     BufferT(BufferKind kind, size_t size, size_t offset)\n         : kind(kind), id(nextId++), size(size), offset(offset) {}\n-\n-    bool intersects(const BufferT &other) const {\n-      return Interval<size_t>(offset, offset + size)\n-          .intersects(\n-              Interval<size_t>(other.offset, other.offset + other.size));\n-    }\n   };\n \n   /// Op -> Scratch Buffer\n@@ -158,8 +168,6 @@ class Allocation {\n   using AliasBufferMapT = llvm::MapVector<Value, llvm::SetVector<BufferT *>>;\n   /// BufferId -> Buffer\n   using BufferSetT = std::map<BufferId, BufferT>;\n-  /// Runs allocation analysis on the given top-level operation.\n-  void run();\n \n private:\n   template <BufferT::BufferKind Kind, typename KeyType, typename... Args>\n@@ -168,6 +176,8 @@ class Allocation {\n     bufferSet[buffer.id] = std::move(buffer);\n     if constexpr (Kind == BufferT::BufferKind::Explicit) {\n       valueBuffer[key] = &bufferSet[buffer.id];\n+    } else if constexpr (Kind == BufferT::BufferKind::Virtual) {\n+      opVirtual[key] = &bufferSet[buffer.id];\n     } else {\n       opScratch[key] = &bufferSet[buffer.id];\n     }\n@@ -178,8 +188,9 @@ class Allocation {\n   }\n \n private:\n-  Operation *operation;\n+  Operation *operation = nullptr;\n   OpScratchMapT opScratch;\n+  OpScratchMapT opVirtual;\n   ValueBufferMapT valueBuffer;\n   AliasBufferMapT aliasBuffer;\n   BufferSetT bufferSet;\n@@ -188,6 +199,54 @@ class Allocation {\n   friend class triton::AllocationAnalysis;\n };\n \n+/// Static analysis that computes the allocation of shared memory buffers\n+/// of the entire call graph.\n+/// The allocation is performed in a post-order walk of the call graph.\n+/// Each call op is treated like convert_layout that allocates a scratch buffer.\n+/// At each call, we compute the start offset of the scratch buffer and pass it\n+/// as an argument to the callee.\n+class ModuleAllocation : public CallGraph<Allocation> {\n+public:\n+  using FuncOffsetMapT = DenseMap<FunctionOpInterface, Value>;\n+\n+  explicit ModuleAllocation(ModuleOp moduleOp)\n+      : CallGraph<Allocation>(moduleOp) {\n+    walk<WalkOrder::PreOrder, WalkOrder::PostOrder>(\n+        // Pre-order edge walk callback\n+        [](CallOpInterface callOp, FunctionOpInterface funcOp) {},\n+        // Post-order node walk callback\n+        [&](FunctionOpInterface funcOp) {\n+          auto [iter, inserted] = funcMap.try_emplace(funcOp, funcOp);\n+          if (inserted)\n+            iter->second.run(funcMap);\n+        });\n+  }\n+\n+  size_t getSharedMemorySize() {\n+    size_t size = 0;\n+    for (auto funcOp : getRoots()) {\n+      auto *alloc = getFuncData(funcOp);\n+      size = std::max(size, alloc->getSharedMemorySize());\n+    }\n+    return size;\n+  }\n+\n+  size_t getSharedMemorySize(FunctionOpInterface funcOp) {\n+    return getFuncData(funcOp)->getSharedMemorySize();\n+  }\n+\n+  void setFunctionSharedMemoryValue(FunctionOpInterface funcOp, Value value) {\n+    sharedMemoryValue[funcOp] = value;\n+  }\n+\n+  Value getFunctionSharedMemoryBase(FunctionOpInterface funcOp) {\n+    return sharedMemoryValue[funcOp];\n+  }\n+\n+private:\n+  FuncOffsetMapT sharedMemoryValue;\n+};\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_ALLOCATION_H"}, {"filename": "include/triton/Analysis/AxisInfo.h", "status": "modified", "additions": 252, "deletions": 38, "changes": 290, "file_content_changes": "@@ -1,69 +1,105 @@\n #ifndef TRITON_ANALYSIS_AXISINFO_H\n #define TRITON_ANALYSIS_AXISINFO_H\n \n-#include \"mlir/Analysis/DataFlowAnalysis.h\"\n+#include \"mlir/Analysis/DataFlow/SparseAnalysis.h\"\n #include \"llvm/Support/raw_ostream.h\"\n-#include <iostream>\n \n+#include \"mlir/Support/LLVM.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n+#include <optional>\n+#include <type_traits>\n+\n namespace mlir {\n \n //===----------------------------------------------------------------------===//\n // AxisInfo\n //===----------------------------------------------------------------------===//\n \n /// This lattice value represents known information on the axes of a lattice.\n-/// Axis information is represented by a std::map<int, int>\n class AxisInfo {\n public:\n-  typedef SmallVector<int, 4> DimVectorT;\n+  typedef SmallVector<int64_t, 4> DimVectorT;\n \n public:\n-  // Default constructor\n+  /// Default constructor\n   AxisInfo() : AxisInfo({}, {}, {}) {}\n-  // Construct contiguity info with known contiguity\n+  /// Construct contiguity info with known contiguity\n   AxisInfo(DimVectorT knownContiguity, DimVectorT knownDivisibility,\n            DimVectorT knownConstancy)\n+      : AxisInfo(knownContiguity, knownDivisibility, knownConstancy, {}) {}\n+  AxisInfo(DimVectorT knownContiguity, DimVectorT knownDivisibility,\n+           DimVectorT knownConstancy, std::optional<int64_t> knownConstantValue)\n       : contiguity(knownContiguity), divisibility(knownDivisibility),\n-        constancy(knownConstancy), rank(contiguity.size()) {\n-    assert(knownDivisibility.size() == (size_t)rank);\n-    assert(knownConstancy.size() == (size_t)rank);\n+        constancy(knownConstancy), constantValue(knownConstantValue),\n+        rank(contiguity.size()) {\n+    assert(knownContiguity.size() == static_cast<size_t>(rank));\n+    assert(knownDivisibility.size() == static_cast<size_t>(rank));\n+    assert(knownConstancy.size() == static_cast<size_t>(rank));\n   }\n \n-  // Accessors\n-  int getContiguity(size_t d) const { return contiguity[d]; }\n+  /// Accessors\n+  int64_t getContiguity(size_t dim) const { return contiguity[dim]; }\n   const DimVectorT &getContiguity() const { return contiguity; }\n \n-  int getDivisibility(size_t d) const { return divisibility[d]; }\n+  int64_t getDivisibility(size_t dim) const { return divisibility[dim]; }\n   const DimVectorT &getDivisibility() const { return divisibility; }\n \n-  int getConstancy(size_t d) const { return constancy[d]; }\n+  int64_t getConstancy(size_t dim) const { return constancy[dim]; }\n   const DimVectorT &getConstancy() const { return constancy; }\n \n   int getRank() const { return rank; }\n \n-  // Comparison\n+  std::optional<int64_t> getConstantValue() const { return constantValue; }\n+\n+  template <class T>\n+  static void\n+  initPessimisticStateFromFunc(int argNumber, T funcOp, DimVectorT *contiguity,\n+                               DimVectorT *divisibility, DimVectorT *constancy);\n+  /// Comparison\n   bool operator==(const AxisInfo &other) const {\n     return (contiguity == other.contiguity) &&\n            (divisibility == other.divisibility) &&\n-           (constancy == other.constancy);\n+           (constancy == other.constancy) &&\n+           (constantValue == other.constantValue) && (rank == other.rank);\n   }\n \n   /// The pessimistic value state of the contiguity is unknown.\n-  static AxisInfo getPessimisticValueState(MLIRContext *context) {\n+  static AxisInfo getPessimisticValueState(MLIRContext *context = nullptr) {\n     return AxisInfo();\n   }\n   static AxisInfo getPessimisticValueState(Value value);\n \n-  // The gcd of both arguments for each dimension\n+  /// The gcd of both arguments for each dimension\n   static AxisInfo join(const AxisInfo &lhs, const AxisInfo &rhs);\n \n+  void print(raw_ostream &os) const {\n+    auto print = [&](StringRef name, DimVectorT vec) {\n+      os << name << \" = [\";\n+      llvm::interleaveComma(vec, os);\n+      os << \"]\";\n+    };\n+    print(\"contiguity\", contiguity);\n+    print(\", divisibility\", divisibility);\n+    print(\", constancy\", constancy);\n+    os << \", constant_value = \";\n+    if (constantValue)\n+      os << *constantValue;\n+    else\n+      os << \"<none>\";\n+  }\n+\n private:\n   /// The _contiguity_ information maps the `d`-th\n   /// dimension to the length of the shortest\n-  /// sequence of contiguous integers along it\n+  /// sequence of contiguous integers along it.\n+  /// Suppose we have an array of N elements,\n+  /// with a contiguity value C,\n+  /// the array can be divided into a list of\n+  /// N/C sequences of C contiguous elements.\n+  /// Since we have N = 2^k, C must be a power of two.\n   /// For example:\n   /// [10, 11, 12, 13, 18, 19, 20, 21]\n   /// [20, 21, 22, 23, 28, 29, 30, 31]\n@@ -80,7 +116,8 @@ class AxisInfo {\n \n   /// The _divisibility_ information maps the `d`-th\n   /// dimension to the largest power-of-two that\n-  /// divides the first element of all the values along it\n+  /// divides the first element of all groups of\n+  // _contiguity_ values along it\n   /// For example:\n   /// [10, 11, 12, 13, 18, 19, 20, 21]\n   /// [20, 21, 22, 23, 28, 29, 30, 31]\n@@ -91,54 +128,231 @@ class AxisInfo {\n   /// [14, 18, 22, 26]\n   /// [15, 19, 23, 27]\n   //  would have divisibility [4, 1]\n+  //  On the other hand:\n+  //  [0, 1, 2, 0, 4, 5, 6, 7]\n+  //  would have divisibility 1 because\n+  //  _contiguity_=1\n   DimVectorT divisibility;\n \n   /// The _constancy_ information maps the `d`-th\n   /// dimension to the length of the shortest\n   /// sequence of constant integer along it. This is\n   /// particularly useful to infer the contiguity\n-  /// of operations (e.g., add) involving a constant\n+  /// of operations (e.g., add) involving a constant.\n+  /// Suppose we have an array of N elements,\n+  /// with a constancy value C,\n+  /// the array can be divided into a list of\n+  /// N/C sequences of C elements with the same value.\n+  /// Since we have N = 2^k, C must be a power of two.\n   /// For example\n   /// [8, 8, 8, 8, 12, 12, 12, 12]\n   /// [16, 16, 16, 16, 20, 20, 20, 20]\n   /// would have constancy [1, 4]\n   DimVectorT constancy;\n \n+  /// The constant value of the lattice if we can infer it.\n+  std::optional<int64_t> constantValue;\n+\n   // number of dimensions of the lattice\n-  int rank;\n+  int rank{};\n+};\n+\n+class AxisInfoVisitor {\n+public:\n+  AxisInfoVisitor() = default;\n+  virtual ~AxisInfoVisitor() = default;\n+\n+  static bool isContiguousDim(const AxisInfo &info, ArrayRef<int64_t> shape,\n+                              int dim) {\n+    return info.getContiguity(dim) == shape[dim];\n+  }\n+\n+  static bool isConstantDim(const AxisInfo &info, ArrayRef<int64_t> shape,\n+                            int dim) {\n+    return info.getConstancy(dim) == shape[dim];\n+  }\n+\n+  virtual AxisInfo\n+  getAxisInfo(Operation *op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) = 0;\n+\n+  virtual bool match(Operation *op) = 0;\n+};\n+\n+/// Base class for all operations\n+template <typename OpTy> class AxisInfoVisitorImpl : public AxisInfoVisitor {\n+public:\n+  using AxisInfoVisitor::AxisInfoVisitor;\n+\n+  AxisInfo\n+  getAxisInfo(Operation *op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) final {\n+    return getAxisInfo(cast<OpTy>(op), operands);\n+  }\n+\n+  bool match(Operation *op) final { return isa<OpTy>(op); }\n+\n+  virtual AxisInfo\n+  getAxisInfo(OpTy op, ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) {\n+    llvm_unreachable(\"Unimplemented getAxisInfo\");\n+  }\n+};\n+\n+/// Binary operations\n+template <typename OpTy>\n+class BinaryOpVisitorImpl : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto lhsInfo = operands[0]->getValue();\n+    auto rhsInfo = operands[1]->getValue();\n+    auto rank = lhsInfo.getRank();\n+    assert(operands.size() == 2 && \"Expected two operands\");\n+    AxisInfo::DimVectorT contiguity;\n+    AxisInfo::DimVectorT divisibility;\n+    AxisInfo::DimVectorT constancy;\n+    auto constantValue = getConstantValue(op, lhsInfo, rhsInfo);\n+    for (auto d = 0; d < rank; ++d) {\n+      if (constantValue.has_value()) {\n+        contiguity.push_back(1);\n+        constancy.push_back(\n+            std::max(lhsInfo.getConstancy(d), rhsInfo.getConstancy(d)));\n+        divisibility.push_back(highestPowOf2Divisor(constantValue.value()));\n+      } else {\n+        contiguity.push_back(getContiguity(op, lhsInfo, rhsInfo, d));\n+        constancy.push_back(getConstancy(op, lhsInfo, rhsInfo, d));\n+        divisibility.push_back(getDivisibility(op, lhsInfo, rhsInfo, d));\n+      }\n+    }\n+    return AxisInfo(contiguity, divisibility, constancy, constantValue);\n+  }\n+\n+protected:\n+  virtual int64_t getContiguity(OpTy op, const AxisInfo &lhs,\n+                                const AxisInfo &rhs, int dim) {\n+    return 1;\n+  }\n+\n+  virtual int64_t getDivisibility(OpTy op, const AxisInfo &lhs,\n+                                  const AxisInfo &rhs, int dim) {\n+    return 1;\n+  }\n+\n+  virtual int64_t getConstancy(OpTy op, const AxisInfo &lhs,\n+                               const AxisInfo &rhs, int dim) {\n+    return 1;\n+  }\n+\n+  virtual std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                                  const AxisInfo &rhs) {\n+    return {};\n+  }\n };\n \n-class AxisInfoAnalysis : public ForwardDataFlowAnalysis<AxisInfo> {\n+class AxisInfoVisitorList {\n+public:\n+  template <typename... Ts, typename = std::enable_if_t<sizeof...(Ts) != 0>>\n+  void append() {\n+    (visitors.emplace_back(std::make_unique<Ts>()), ...);\n+  }\n+\n+  AxisInfo apply(Operation *op,\n+                 ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) {\n+    for (auto &visitor : visitors)\n+      if (visitor->match(op))\n+        return visitor->getAxisInfo(op, operands);\n+    return AxisInfo();\n+  }\n \n private:\n-  static const int maxPow2Divisor = 65536;\n+  std::vector<std::unique_ptr<AxisInfoVisitor>> visitors;\n+};\n \n-  int highestPowOf2Divisor(int n) {\n-    if (n == 0)\n-      return maxPow2Divisor;\n-    return (n & (~(n - 1)));\n+class AxisInfoAnalysis\n+    : public dataflow::SparseDataFlowAnalysis<dataflow::Lattice<AxisInfo>> {\n+private:\n+  AxisInfoVisitorList visitors;\n+\n+  void setToEntryState(dataflow::Lattice<AxisInfo> *lattice) override {\n+    propagateIfChanged(\n+        lattice,\n+        lattice->join(AxisInfo::getPessimisticValueState(lattice->getPoint())));\n   }\n \n-  AxisInfo visitBinaryOp(\n-      Operation *op, AxisInfo lhsInfo, AxisInfo rhsInfo,\n-      const std::function<int(AxisInfo, AxisInfo, int)> &getContiguity,\n-      const std::function<int(AxisInfo, AxisInfo, int)> &getDivisibility,\n-      const std::function<int(AxisInfo, AxisInfo, int)> &getConstancy);\n+public:\n+  AxisInfoAnalysis(DataFlowSolver &solver);\n+  using dataflow::SparseDataFlowAnalysis<\n+      dataflow::Lattice<AxisInfo>>::getLatticeElement;\n+  using FuncAxisInfoMapT = DenseMap<FunctionOpInterface, AxisInfo>;\n \n+  void visitOperation(Operation *op,\n+                      ArrayRef<const dataflow::Lattice<AxisInfo> *> operands,\n+                      ArrayRef<dataflow::Lattice<AxisInfo> *> results) override;\n+};\n+\n+/// Module level axis info analysis based on the call graph, assuming that we\n+/// do not have recursive functions.\n+/// Since each function will be called multiple times, we need to\n+/// calculate the axis info based on the axis info of all the callers.\n+/// In the future, we can perform optimization using function cloning so that\n+/// each call site will have unique axis info.\n+using AxisInfoMapT = DenseMap<Value, AxisInfo>;\n+class ModuleAxisInfoAnalysis : public CallGraph<AxisInfoMapT> {\n public:\n-  using ForwardDataFlowAnalysis<AxisInfo>::ForwardDataFlowAnalysis;\n+  explicit ModuleAxisInfoAnalysis(ModuleOp moduleOp)\n+      : CallGraph<AxisInfoMapT>(moduleOp) {\n+    SmallVector<FunctionOpInterface> funcs;\n+    for (auto root : getRoots()) {\n+      walk<WalkOrder::PreOrder, WalkOrder::PostOrder>(\n+          // Pre-order edge walk callback\n+          [](CallOpInterface callOp, FunctionOpInterface funcOp) {},\n+          // Post-order node walk callback\n+          [&](FunctionOpInterface funcOp) {\n+            funcs.push_back(funcOp);\n+            funcMap.try_emplace(funcOp, AxisInfoMapT{});\n+          });\n+    }\n+    SetVector<FunctionOpInterface> sortedFuncs(funcs.begin(), funcs.end());\n+    SymbolTableCollection symbolTable;\n+    for (auto funcOp : llvm::reverse(sortedFuncs)) {\n+      initialize(funcOp);\n+      funcOp.walk([&](CallOpInterface callOp) {\n+        auto callee =\n+            dyn_cast<FunctionOpInterface>(callOp.resolveCallable(&symbolTable));\n+        update(callOp, callee);\n+      });\n+    }\n+  }\n \n-  ChangeResult\n-  visitOperation(Operation *op,\n-                 ArrayRef<LatticeElement<AxisInfo> *> operands) override;\n+  AxisInfo *getAxisInfo(Value value) {\n+    auto funcOp =\n+        value.getParentRegion()->getParentOfType<FunctionOpInterface>();\n+    auto *axisInfoMap = getFuncData(funcOp);\n+    if (!axisInfoMap) {\n+      return nullptr;\n+    }\n+    auto it = axisInfoMap->find(value);\n+    if (it == axisInfoMap->end()) {\n+      return nullptr;\n+    }\n+    return &(it->second);\n+  }\n \n-  unsigned getPtrVectorSize(Value ptr);\n+  unsigned getPtrContiguity(Value ptr);\n \n   unsigned getPtrAlignment(Value ptr);\n \n   unsigned getMaskAlignment(Value mask);\n+\n+private:\n+  void initialize(FunctionOpInterface funcOp);\n+\n+  void update(CallOpInterface callOp, FunctionOpInterface funcOp);\n };\n \n } // namespace mlir\n \n-#endif\n\\ No newline at end of file\n+#endif"}, {"filename": "include/triton/Analysis/Membar.h", "status": "modified", "additions": 99, "deletions": 66, "changes": 165, "file_content_changes": "@@ -4,20 +4,75 @@\n #include \"Allocation.h\"\n #include \"llvm/ADT/SmallPtrSet.h\"\n \n+#include <set>\n+\n namespace mlir {\n \n class OpBuilder;\n \n+struct BlockInfo {\n+  using BufferIdSetT = Allocation::BufferIdSetT;\n+  using IntervalSetT = std::set<Interval<size_t>>;\n+\n+  IntervalSetT syncReadIntervals;\n+  IntervalSetT syncWriteIntervals;\n+\n+  BlockInfo() = default;\n+\n+  /// Unions two BlockInfo objects.\n+  BlockInfo &join(const BlockInfo &other) {\n+    syncReadIntervals.insert(other.syncReadIntervals.begin(),\n+                             other.syncReadIntervals.end());\n+    syncWriteIntervals.insert(other.syncWriteIntervals.begin(),\n+                              other.syncWriteIntervals.end());\n+    return *this;\n+  }\n+\n+  /// Returns true if intervals in two BlockInfo objects are intersected.\n+  bool isIntersected(const BlockInfo &other) const {\n+    return /*RAW*/ isIntersected(syncWriteIntervals, other.syncReadIntervals) ||\n+           /*WAR*/\n+           isIntersected(syncReadIntervals, other.syncWriteIntervals) ||\n+           /*WAW*/\n+           isIntersected(syncWriteIntervals, other.syncWriteIntervals);\n+  }\n+\n+  /// Clears the intervals because a barrier is inserted.\n+  void sync() {\n+    syncReadIntervals.clear();\n+    syncWriteIntervals.clear();\n+  }\n+\n+  /// Compares two BlockInfo objects.\n+  bool operator==(const BlockInfo &other) const {\n+    return syncReadIntervals == other.syncReadIntervals &&\n+           syncWriteIntervals == other.syncWriteIntervals;\n+  }\n+\n+  bool operator!=(const BlockInfo &other) const { return !(*this == other); }\n+\n+private:\n+  bool isIntersected(const IntervalSetT &lhsIntervalSet,\n+                     const IntervalSetT &rhsIntervalSet) const {\n+    for (auto &lhs : lhsIntervalSet)\n+      for (auto &rhs : rhsIntervalSet)\n+        if (lhs.intersects(rhs))\n+          return true;\n+    return false;\n+  }\n+};\n+\n //===----------------------------------------------------------------------===//\n // Shared Memory Barrier Analysis\n //===----------------------------------------------------------------------===//\n class MembarAnalysis {\n public:\n+  using FuncBlockInfoMapT = CallGraph<BlockInfo>::FuncDataMapT;\n   /// Creates a new Membar analysis that generates the shared memory barrier\n   /// in the following circumstances:\n   /// - RAW: If a shared memory write is followed by a shared memory read, and\n   /// their addresses are intersected, a barrier is inserted.\n-  /// - WAR: If a shared memory read is followed by a shared memory read, and\n+  /// - WAR: If a shared memory read is followed by a shared memory write, and\n   /// their addresses are intersected, a barrier is inserted.\n   /// The following circumstances do not require a barrier:\n   /// - WAW: not possible because overlapped memory allocation is not allowed.\n@@ -26,66 +81,14 @@ class MembarAnalysis {\n   /// a shared memory read. If the temporary storage is written but not read,\n   /// it is considered as the problem of the operation itself but not the membar\n   /// analysis.\n-  /// The following circumstances are not considered yet:\n-  /// - Double buffers\n-  /// - N buffers\n-  MembarAnalysis(Allocation *allocation) : allocation(allocation) {}\n+  MembarAnalysis() = default;\n+  explicit MembarAnalysis(Allocation *allocation) : allocation(allocation) {}\n \n   /// Runs the membar analysis to the given operation, inserts a barrier if\n   /// necessary.\n-  void run();\n+  void run(FuncBlockInfoMapT &funcBlockInfoMap);\n \n private:\n-  struct RegionInfo {\n-    using BufferIdSetT = Allocation::BufferIdSetT;\n-\n-    BufferIdSetT syncReadBuffers;\n-    BufferIdSetT syncWriteBuffers;\n-\n-    RegionInfo() = default;\n-    RegionInfo(const BufferIdSetT &syncReadBuffers,\n-               const BufferIdSetT &syncWriteBuffers)\n-        : syncReadBuffers(syncReadBuffers), syncWriteBuffers(syncWriteBuffers) {\n-    }\n-\n-    /// Unions two RegionInfo objects.\n-    void join(const RegionInfo &other) {\n-      syncReadBuffers.insert(other.syncReadBuffers.begin(),\n-                             other.syncReadBuffers.end());\n-      syncWriteBuffers.insert(other.syncWriteBuffers.begin(),\n-                              other.syncWriteBuffers.end());\n-    }\n-\n-    /// Returns true if buffers in two RegionInfo objects are intersected.\n-    bool isIntersected(const RegionInfo &other, Allocation *allocation) const {\n-      return /*RAW*/ isIntersected(syncWriteBuffers, other.syncReadBuffers,\n-                                   allocation) ||\n-             /*WAR*/\n-             isIntersected(syncReadBuffers, other.syncWriteBuffers,\n-                           allocation) ||\n-             /*WAW*/\n-             isIntersected(syncWriteBuffers, other.syncWriteBuffers,\n-                           allocation);\n-    }\n-\n-    /// Clears the buffers because a barrier is inserted.\n-    void sync() {\n-      syncReadBuffers.clear();\n-      syncWriteBuffers.clear();\n-    }\n-\n-  private:\n-    /// Returns true if buffers in two sets are intersected.\n-    bool isIntersected(const BufferIdSetT &lhs, const BufferIdSetT &rhs,\n-                       Allocation *allocation) const {\n-      return std::any_of(lhs.begin(), lhs.end(), [&](auto lhsId) {\n-        return std::any_of(rhs.begin(), rhs.end(), [&](auto rhsId) {\n-          return allocation->isIntersected(lhsId, rhsId);\n-        });\n-      });\n-    }\n-  };\n-\n   /// Applies the barrier analysis based on the SCF dialect, in which each\n   /// region has a single basic block only.\n   /// Example:\n@@ -99,19 +102,49 @@ class MembarAnalysis {\n   ///        op5\n   ///        op6\n   ///   op7\n-  /// region2 and region3 started with the information of region1.\n-  /// Each region is analyzed separately and keeps their own copy of the\n-  /// information. At op7, we union the information of the region2 and region3\n-  /// and update the information of region1.\n-  void dfsOperation(Operation *operation, RegionInfo *blockInfo,\n-                    OpBuilder *builder);\n+  /// TODO: Explain why we don't use ForwardAnalysis:\n+  void resolve(FunctionOpInterface funcOp, FuncBlockInfoMapT *funcBlockInfoMap,\n+               OpBuilder *builder);\n \n-  /// Updates the RegionInfo operation based on the operation.\n-  void transfer(Operation *operation, RegionInfo *blockInfo,\n-                OpBuilder *builder);\n+  /// Updates the BlockInfo operation based on the operation.\n+  void update(Operation *operation, BlockInfo *blockInfo,\n+              FuncBlockInfoMapT *funcBlockInfoMap, OpBuilder *builder);\n+\n+  /// Collects the successors of the terminator\n+  void visitTerminator(Operation *operation, SmallVector<Block *> &successors);\n+\n+private:\n+  Allocation *allocation = nullptr;\n+};\n+\n+/// Postorder traversal on the callgraph to insert membar instructions\n+/// of each function.\n+/// Each function maintains a BlockInfo map that includes all potential buffers\n+/// after returning. This way users do not have to explicitly insert membars\n+/// before and after function calls, but might be a bit conservative.\n+class ModuleMembarAnalysis : public CallGraph<BlockInfo> {\n+public:\n+  ModuleMembarAnalysis(ModuleAllocation *moduleAllocation)\n+      : CallGraph<BlockInfo>(moduleAllocation->getModuleOp()),\n+        moduleAllocation(moduleAllocation) {}\n+\n+  void run() {\n+    walk<WalkOrder::PreOrder, WalkOrder::PostOrder>(\n+        // Pre-order walk callback\n+        [](CallOpInterface callOp, FunctionOpInterface funcOp) {},\n+        // Post-order walk callback\n+        [&](FunctionOpInterface funcOp) {\n+          auto *allocation = moduleAllocation->getFuncData(funcOp);\n+          auto [it, inserted] = funcMap.try_emplace(funcOp, BlockInfo());\n+          if (inserted) {\n+            MembarAnalysis analysis(allocation);\n+            analysis.run(funcMap);\n+          }\n+        });\n+  }\n \n private:\n-  Allocation *allocation;\n+  ModuleAllocation *moduleAllocation;\n };\n \n } // namespace mlir"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 201, "deletions": 12, "changes": 213, "file_content_changes": "@@ -1,6 +1,8 @@\n #ifndef TRITON_ANALYSIS_UTILITY_H\n #define TRITON_ANALYSIS_UTILITY_H\n \n+#include \"mlir/Analysis/DataFlowFramework.h\"\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include <algorithm>\n #include <numeric>\n@@ -10,20 +12,37 @@ namespace mlir {\n \n class ReduceOpHelper {\n public:\n-  explicit ReduceOpHelper(triton::ReduceOp op) : op(op) {\n-    srcTy = op.operand().getType().cast<RankedTensorType>();\n+  explicit ReduceOpHelper(triton::ReduceOp rop)\n+      : op(rop.getOperation()), axis(rop.getAxis()) {\n+    auto firstTy = rop.getOperands()[0].getType().cast<RankedTensorType>();\n+    srcShape = firstTy.getShape();\n+    srcEncoding = firstTy.getEncoding();\n+    srcElementTypes = rop.getElementTypes();\n+\n+    for (const auto &t : rop.getInputTypes()) {\n+      if (t.getShape() != srcShape) {\n+        rop.emitError() << \"shape mismatch\";\n+      }\n+      if (t.getEncoding() != srcEncoding) {\n+        rop.emitError() << \"encoding mismatch\";\n+      }\n+    }\n   }\n \n-  ArrayRef<int64_t> getSrcShape() { return srcTy.getShape(); }\n+  ArrayRef<int64_t> getSrcShape() { return srcShape; }\n \n-  Attribute getSrcLayout() { return srcTy.getEncoding(); }\n+  Attribute getSrcLayout() { return srcEncoding; }\n \n   bool isFastReduction();\n \n   unsigned getInterWarpSize();\n \n   unsigned getIntraWarpSize();\n \n+  unsigned getInterWarpSizeWithUniqueData();\n+\n+  unsigned getIntraWarpSizeWithUniqueData();\n+\n   unsigned getThreadsReductionAxis();\n \n   SmallVector<unsigned> getScratchConfigBasic();\n@@ -32,13 +51,16 @@ class ReduceOpHelper {\n \n   unsigned getScratchSizeInBytes();\n \n+  bool isSupportedLayout();\n+\n private:\n-  triton::ReduceOp op;\n-  RankedTensorType srcTy{};\n+  Operation *op;\n+  ArrayRef<int64_t> srcShape;\n+  Attribute srcEncoding;\n+  SmallVector<Type> srcElementTypes;\n+  int axis;\n };\n \n-bool isSharedEncoding(Value value);\n-\n bool maybeSharedAllocationOp(Operation *op);\n \n bool maybeAliasOp(Operation *op);\n@@ -47,9 +69,11 @@ bool supportMMA(triton::DotOp op, int version);\n \n bool supportMMA(Value value, int version);\n \n-Type getElementType(Value value);\n+bool isSingleValue(Value value);\n+\n+bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy);\n \n-std::string getValueOperandName(Value value, AsmState &state);\n+Type getElementType(Value value);\n \n template <typename T_OUT, typename T_IN>\n inline SmallVector<T_OUT> convertType(ArrayRef<T_IN> in) {\n@@ -77,8 +101,26 @@ SmallVector<RES_T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n   return result;\n }\n \n-bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n-                        triton::gpu::DotOperandEncodingAttr &dotOperandLayout);\n+/// Get the highest power of 2 divisor of an integer.\n+template <typename T> T highestPowOf2Divisor(T n) {\n+  if (n == 0) {\n+    return (static_cast<T>(1) << (sizeof(T) * 8 - 2));\n+  }\n+  return (n & (~(n - 1)));\n+}\n+\n+/// Get the next power of 2 for an integer (or the integer itself if it is a\n+/// power of 2).\n+template <typename T> T nextPowOf2(T n) {\n+  if (n == 0) {\n+    return 1;\n+  }\n+  n--;\n+  for (unsigned i = 1; i < sizeof(T) * 8; i <<= 1) {\n+    n |= n >> i;\n+  }\n+  return n + 1;\n+}\n \n /// Multi-root DAG topological sort.\n /// Performs a topological sort of the Operation in the `toSort` SetVector.\n@@ -88,6 +130,153 @@ bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n SetVector<Operation *>\n multiRootTopologicalSort(const SetVector<Operation *> &toSort);\n \n+/// This uses the toplogicalSort above\n+SetVector<Operation *>\n+multiRootGetSlice(Operation *op, TransitiveFilter backwardFilter = nullptr,\n+                  TransitiveFilter forwardFilter = nullptr);\n+\n+/// Create a basic DataFlowSolver with constant and dead code analysis included.\n+std::unique_ptr<DataFlowSolver> createDataFlowSolver();\n+\n+/// This class represents a call graph for a given ModuleOp and holds\n+/// data of type T associated with each FunctionOpInterface.\n+template <typename T> class CallGraph {\n+public:\n+  using FuncDataMapT = DenseMap<FunctionOpInterface, T>;\n+\n+  /// Constructor that builds the call graph for the given moduleOp.\n+  explicit CallGraph(ModuleOp moduleOp) : moduleOp(moduleOp) { build(); }\n+\n+  /// Walks the call graph and applies the provided update functions\n+  /// to the edges and nodes.\n+  template <WalkOrder UpdateEdgeOrder = WalkOrder::PreOrder,\n+            WalkOrder UpdateNodeOrder = WalkOrder::PreOrder,\n+            typename UpdateEdgeFn, typename UpdateNodeFn>\n+  void walk(UpdateEdgeFn updateEdgeFn, UpdateNodeFn updateNodeFn) {\n+    DenseSet<FunctionOpInterface> visited;\n+    for (auto root : roots) {\n+      doWalk<UpdateEdgeOrder, UpdateNodeOrder>(root, visited, updateEdgeFn,\n+                                               updateNodeFn);\n+    }\n+  }\n+\n+  /// Retrieves the data associated with a function\n+  T *getFuncData(FunctionOpInterface funcOp) {\n+    if (funcMap.count(funcOp)) {\n+      return &funcMap[funcOp];\n+    }\n+    return nullptr;\n+  }\n+\n+  /// Getters\n+  ModuleOp getModuleOp() const { return moduleOp; }\n+  SmallVector<FunctionOpInterface> getRoots() const { return roots; }\n+  size_t getNumFunctions() const { return funcMap.size(); }\n+\n+  /// Returns true if the given function is a root.\n+  bool isRoot(FunctionOpInterface funcOp) const {\n+    return llvm::is_contained(roots, funcOp);\n+  }\n+\n+  /// Maps the data and the graph nodes associated with a funcOp to a\n+  /// targetFuncOp.\n+  template <typename FROM, typename TO>\n+  void mapFuncOp(FROM funcOp, TO targetFuncOp) {\n+    // Iterate over graph and replace\n+    for (auto &kv : graph) {\n+      for (auto &edge : kv.second) {\n+        if (edge.second == funcOp) {\n+          edge.second = targetFuncOp;\n+        }\n+      }\n+    }\n+    graph[targetFuncOp] = graph[funcOp];\n+    // Replace in roots\n+    for (auto it = roots.begin(); it != roots.end(); ++it) {\n+      if (*it == funcOp) {\n+        *it = targetFuncOp;\n+        break;\n+      }\n+    }\n+    // Replace in funcMap\n+    funcMap[targetFuncOp] = funcMap[funcOp];\n+  }\n+\n+  /// Maps the graph edges associated with a callOp to a targetCallOp.\n+  template <typename FROM, typename TO>\n+  void mapCallOp(FROM callOp, TO targetCallOp) {\n+    // Iterate over graph and replace\n+    for (auto &kv : graph) {\n+      for (auto &edge : kv.second) {\n+        if (edge.first == callOp) {\n+          edge.first = targetCallOp;\n+        }\n+      }\n+    }\n+  }\n+\n+private:\n+  void build() {\n+    SymbolTableCollection symbolTable;\n+    DenseSet<FunctionOpInterface> visited;\n+    // Build graph\n+    moduleOp.walk([&](Operation *op) {\n+      auto caller = op->getParentOfType<FunctionOpInterface>();\n+      if (auto callOp = dyn_cast<CallOpInterface>(op)) {\n+        auto *callee = callOp.resolveCallable(&symbolTable);\n+        auto funcOp = dyn_cast_or_null<FunctionOpInterface>(callee);\n+        if (funcOp) {\n+          graph[caller].emplace_back(\n+              std::pair<CallOpInterface, FunctionOpInterface>(callOp, funcOp));\n+          visited.insert(funcOp);\n+        }\n+      }\n+    });\n+    // Find roots\n+    moduleOp.walk([&](FunctionOpInterface funcOp) {\n+      if (!visited.count(funcOp)) {\n+        roots.push_back(funcOp);\n+      }\n+    });\n+  }\n+\n+  template <WalkOrder UpdateEdgeOrder = WalkOrder::PreOrder,\n+            WalkOrder UpdateNodeOrder = WalkOrder::PreOrder,\n+            typename UpdateEdgeFn, typename UpdateNodeFn>\n+  void doWalk(FunctionOpInterface funcOp,\n+              DenseSet<FunctionOpInterface> &visited, UpdateEdgeFn updateEdgeFn,\n+              UpdateNodeFn updateNodeFn) {\n+    if (visited.count(funcOp)) {\n+      llvm::report_fatal_error(\"Cycle detected in call graph\");\n+    }\n+    if constexpr (UpdateNodeOrder == WalkOrder::PreOrder) {\n+      updateNodeFn(funcOp);\n+    }\n+    for (auto [callOp, callee] : graph[funcOp]) {\n+      if constexpr (UpdateEdgeOrder == WalkOrder::PreOrder) {\n+        updateEdgeFn(callOp, callee);\n+      }\n+      doWalk<UpdateEdgeOrder, UpdateNodeOrder>(callee, visited, updateEdgeFn,\n+                                               updateNodeFn);\n+      if constexpr (UpdateEdgeOrder == WalkOrder::PostOrder) {\n+        updateEdgeFn(callOp, callee);\n+      }\n+    }\n+    if constexpr (UpdateNodeOrder == WalkOrder::PostOrder) {\n+      updateNodeFn(funcOp);\n+    }\n+    visited.erase(funcOp);\n+  }\n+\n+protected:\n+  ModuleOp moduleOp;\n+  DenseMap<FunctionOpInterface,\n+           SmallVector<std::pair<CallOpInterface, FunctionOpInterface>>>\n+      graph;\n+  FuncDataMapT funcMap;\n+  SmallVector<FunctionOpInterface> roots;\n+};\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_UTILITY_H"}, {"filename": "include/triton/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,2 +1,2 @@\n add_subdirectory(Conversion)\n-add_subdirectory(Dialect)\n\\ No newline at end of file\n+add_subdirectory(Dialect)"}, {"filename": "include/triton/Conversion/CMakeLists.txt", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "file_content_changes": "@@ -1,4 +1,2 @@\n-\n-set(LLVM_TARGET_DEFINITIONS Passes.td)\n-mlir_tablegen(Passes.h.inc -gen-pass-decls)\n-add_public_tablegen_target(TritonConversionPassIncGen)\n\\ No newline at end of file\n+add_subdirectory(TritonToTritonGPU)\n+add_subdirectory(TritonGPUToLLVM)"}, {"filename": "include/triton/Conversion/Passes.td", "status": "removed", "additions": 0, "deletions": 54, "changes": 54, "file_content_changes": "@@ -1,54 +0,0 @@\n-#ifndef TRITON_CONVERSION_PASSES\n-#define TRITON_CONVERSION_PASSES\n-\n-include \"mlir/Pass/PassBase.td\"\n-\n-def ConvertTritonToTritonGPU: Pass<\"convert-triton-to-tritongpu\", \"mlir::ModuleOp\"> {\n-    let summary = \"Convert Triton to TritonGPU\";\n-    let description = [{\n-\n-    }];\n-    let constructor = \"mlir::triton::createConvertTritonToTritonGPUPass()\";\n-\n-    let dependentDialects = [\"mlir::arith::ArithmeticDialect\",\n-                             \"mlir::math::MathDialect\",\n-                             \"mlir::StandardOpsDialect\",\n-                             // TODO: Does this pass depend on SCF?\n-                             \"mlir::scf::SCFDialect\",\n-                             \"mlir::triton::TritonDialect\",\n-                             \"mlir::triton::gpu::TritonGPUDialect\"];\n-\n-   let options = [\n-       Option<\"numWarps\", \"num-warps\",\n-              \"int32_t\", /*default*/\"4\",\n-              \"number of warps\">\n-   ];\n-}\n-\n-\n-def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"> {\n-    let summary = \"Convert TritonGPU to LLVM\";\n-    let description = [{\n-\n-    }];\n-    let constructor = \"mlir::triton::createConvertTritonGPUToLLVMPass()\";\n-\n-    let dependentDialects = [\"mlir::arith::ArithmeticDialect\",\n-                             \"mlir::math::MathDialect\",\n-                             \"mlir::gpu::GPUDialect\",\n-                             \"mlir::scf::SCFDialect\",\n-                             \"mlir::LLVM::LLVMDialect\",\n-                             \"mlir::tensor::TensorDialect\",\n-                             \"mlir::triton::TritonDialect\",\n-                             \"mlir::triton::gpu::TritonGPUDialect\",\n-                             \"mlir::NVVM::NVVMDialect\",\n-                             \"mlir::StandardOpsDialect\"];\n-\n-    let options = [\n-        Option<\"computeCapability\", \"compute-capability\",\n-               \"int32_t\", /*default*/\"80\",\n-               \"device compute capability\">\n-    ];\n-}\n-\n-#endif"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/AsmFormat.h", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -0,0 +1,27 @@\n+#ifndef TRITON_CONVERSION_TRITON_GPU_TO_LLVM_ASM_FORMAT_H_\n+#define TRITON_CONVERSION_TRITON_GPU_TO_LLVM_ASM_FORMAT_H_\n+\n+#include \"mlir/IR/Value.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/ADT/StringExtras.h\"\n+#include \"llvm/ADT/StringRef.h\"\n+#include <memory>\n+#include <string>\n+\n+namespace mlir {\n+class ConversionPatternRewriter;\n+class Location;\n+\n+namespace triton {\n+using llvm::StringRef;\n+\n+inline std::string strJoin(llvm::ArrayRef<std::string> strs,\n+                           llvm::StringRef delimiter) {\n+  return llvm::join(strs.begin(), strs.end(), delimiter);\n+}\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif // TRITON_CONVERSION_TRITON_GPU_TO_LLVM_ASM_FORMAT_H_"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -0,0 +1,3 @@\n+set(LLVM_TARGET_DEFINITIONS Passes.td)\n+mlir_tablegen(Passes.h.inc -gen-pass-decls --name TritonGPUToLLVM)\n+add_public_tablegen_target(TritonGPUConversionPassIncGen)"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/GCNAsmFormat.h", "status": "added", "additions": 381, "deletions": 0, "changes": 381, "file_content_changes": "@@ -0,0 +1,381 @@\n+#ifndef TRITON_CONVERSION_TRITON_GPU_TO_LLVM_GCN_FORMAT_H_\n+#define TRITON_CONVERSION_TRITON_GPU_TO_LLVM_GCN_FORMAT_H_\n+\n+#include \"mlir/IR/Value.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/ADT/StringRef.h\"\n+#include <memory>\n+#include <string>\n+\n+namespace mlir {\n+class ConversionPatternRewriter;\n+class Location;\n+\n+namespace triton {\n+using llvm::StringRef;\n+\n+class GCNInstr;\n+class GCNInstrCommon;\n+class GCNInstrExecution;\n+\n+// GCNBuilder helps to manage a GCN asm program consists of one or multiple\n+// instructions.\n+//\n+// A helper for building an ASM program, the objective of GCNBuilder is to give\n+// a thin encapsulation and make the ASM code for MLIR LLVM Dialect more clear.\n+// Currently, several factors are introduced to reduce the need for mixing\n+// string and C++ if-else code.\n+//\n+// Usage:\n+// To create a multiplcation operation\n+//\n+//\n+// GCNBuilder gcnBuilder;\n+// unsigned bitwidth = elemTy.getIntOrFloatBitWidth();\n+//\n+// const std::string readConstraint = \"v\";\n+// const std::string writeConstraint = \"=v\";\n+// auto res = gcnBuilder.newOperand(writeConstraint);\n+// auto lhs = gcnBuilder.newOperand(operands[0], readConstraint);\n+// auto rhs = gcnBuilder.newOperand(operands[1], readConstraint);\n+//\n+// create inst\n+// auto &mul_inst =\n+// gcnBuilder.create<GCNInstr>(\"v_mul\")->float_op_type(bitwidth);\n+//\n+// launch insts\n+// mul_inst(res, lhs, rhs);\n+//\n+// return result\n+// Value ret = gcnBuilder.launch(rewriter, loc, elemTy, false);\n+// return ret;\n+// To get the asm code:\n+// builder.dump()\n+//\n+// To get all the mlir::Value used in the GCN code,\n+//\n+// builder.getAllMlirArgs() // get {pVal, iVal, jVal, kVal}\n+//\n+// To get the string containing all the constraints with \",\" separated,\n+// builder.getConstraints() // get \"=v,v,v\"\n+//\n+// GCNBuilder can build a GCN asm with multiple instructions, sample code:\n+//\n+// GCNBuilder builder;\n+// auto &rcp = gcnBuilder.create<GCNInstr>(\"v_rcp\")->float_op_type(bitwidth);\n+// auto &mul_inst =\n+// gcnBuilder.create<GCNInstr>(\"v_mul\")->float_op_type(bitwidth);\n+//\n+// rcp(...);\n+// mul_inst(...);\n+// This will get a GCN code with two instructions.\n+//\n+// Similar to a C function, a declared GCNInstr instance can be launched\n+// multiple times with different operands, e.g.\n+//\n+//   auto &mul_inst =\n+//   gcnBuilder.create<GCNInstr>(\"v_mul\")->float_op_type(bitwidth); mul_inst(...\n+//   some operands ...); mul_inst(... some different operands ...);\n+//\n+// Finally, we will get a GCN code with two mov instructions.\n+//\n+// There are several derived instruction type for typical instructions, for\n+// example, the GCNIOInstr for ld and st instructions.\n+struct GCNBuilder {\n+  struct Operand {\n+    std::string constraint;\n+    Value value;\n+    int idx{-1};\n+    llvm::SmallVector<Operand *> list;\n+    std::function<std::string(int idx)> repr;\n+\n+    // for list\n+    Operand() = default;\n+    Operand(const Operation &) = delete;\n+    Operand(Value value, StringRef constraint)\n+        : value(value), constraint(constraint) {}\n+\n+    bool isList() const { return !value && constraint.empty(); }\n+\n+    Operand *listAppend(Operand *arg) {\n+      list.push_back(arg);\n+      return this;\n+    }\n+\n+    Operand *listGet(size_t nth) const {\n+      assert(nth < list.size());\n+      return list[nth];\n+    }\n+\n+    std::string dump() const;\n+  };\n+\n+  struct Modifier {\n+    Value value;\n+    std::string modifier;\n+    std::string arg;\n+    llvm::SmallVector<Modifier *> list;\n+\n+    Modifier() = default;\n+    Modifier(const Operation &) = delete;\n+    Modifier(Value value, StringRef arg) : value(value), arg(arg) {}\n+\n+    bool isList() const { return !value && modifier.empty(); }\n+\n+    Modifier *listAppend(Modifier *arg) {\n+      list.push_back(arg);\n+      return this;\n+    }\n+\n+    Modifier *listGet(size_t index) const {\n+      assert(index < list.size());\n+      return list[index];\n+    }\n+\n+    std::string to_str() const {\n+      std::string str = modifier;\n+      if (!arg.empty()) {\n+        str += \":\" + arg;\n+      }\n+      return str;\n+    }\n+\n+    std::string dump() const;\n+  };\n+\n+  template <typename INSTR = GCNInstr, typename... Args>\n+  INSTR *create(Args &&...args) {\n+    instrs.emplace_back(std::make_unique<INSTR>(this, args...));\n+    return static_cast<INSTR *>(instrs.back().get());\n+  }\n+\n+  // Create a list of operands.\n+  Operand *newListOperand() { return newOperand(); }\n+\n+  Operand *newListOperand(ArrayRef<std::pair<mlir::Value, std::string>> items) {\n+    auto *list = newOperand();\n+    for (auto &item : items) {\n+      list->listAppend(newOperand(item.first, item.second));\n+    }\n+    return list;\n+  }\n+\n+  Operand *newListOperand(unsigned count, mlir::Value val,\n+                          const std::string &constraint) {\n+    auto *list = newOperand();\n+    for (int i = 0; i < count; ++i) {\n+      list->listAppend(newOperand(val, constraint));\n+    }\n+    return list;\n+  }\n+\n+  Operand *newListOperand(unsigned count, const std::string &constraint) {\n+    auto *list = newOperand();\n+    for (int i = 0; i < count; ++i) {\n+      list->listAppend(newOperand(constraint));\n+    }\n+    return list;\n+  }\n+\n+  // Create a new operand. It will not add to operand list.\n+  // @value: the MLIR value bind to this operand.\n+  // @constraint: ASM operand constraint, .e.g. \"=r\"\n+  // @formatter: extra format to represent this operand in ASM code, default is\n+  //             \"%{0}\".format(operand.idx).\n+  Operand *newOperand(mlir::Value value, StringRef constraint,\n+                      std::function<std::string(int idx)> formatter = nullptr);\n+\n+  // Create a new operand which is written to, that is, the constraint starts\n+  // with \"=\", e.g. \"=r\".\n+  Operand *newOperand(StringRef constraint);\n+\n+  // Create a constant integer operand.\n+  Operand *newConstantOperand(int v);\n+  // Create a constant operand with explicit code specified.\n+  Operand *newConstantOperand(const std::string &v);\n+\n+  Operand *newAddrOperand(mlir::Value addr, StringRef constraint);\n+\n+  Modifier *newModifier(StringRef modifier, StringRef arg);\n+\n+  llvm::SmallVector<Operand *, 4> getAllArgs() const;\n+\n+  llvm::SmallVector<Value, 4> getAllMLIRArgs() const;\n+\n+  std::string getConstraints() const;\n+\n+  std::string dump() const;\n+\n+  mlir::Value launch(ConversionPatternRewriter &rewriter, Location loc,\n+                     Type resTy, bool hasSideEffect = true,\n+                     bool isAlignStack = false,\n+                     ArrayRef<Attribute> attrs = {}) const;\n+\n+private:\n+  Operand *newOperand() {\n+    argArchive.emplace_back(std::make_unique<Operand>());\n+    return argArchive.back().get();\n+  }\n+\n+  Modifier *newModifier() {\n+    modArchive.emplace_back(std::make_unique<Modifier>());\n+    return modArchive.back().get();\n+  }\n+\n+  friend class GCNInstr;\n+  friend class GCNInstrCommon;\n+\n+protected:\n+  llvm::SmallVector<std::unique_ptr<Operand>, 6> argArchive;\n+  llvm::SmallVector<std::unique_ptr<Modifier>, 2> modArchive;\n+  llvm::SmallVector<std::unique_ptr<GCNInstrCommon>, 2> instrs;\n+  llvm::SmallVector<std::unique_ptr<GCNInstrExecution>, 4> executions;\n+  int oprCounter{};\n+};\n+\n+// GCN instruction common interface.\n+// Put the generic logic for all the instructions here.\n+struct GCNInstrCommon {\n+  explicit GCNInstrCommon(GCNBuilder *builder) : builder(builder) {}\n+\n+  using Operand = GCNBuilder::Operand;\n+  using Modifier = GCNBuilder::Modifier;\n+\n+  // clang-format off\n+  GCNInstrExecution& operator()() { return call({}, {}); }\n+  GCNInstrExecution& operator()(Operand* a) { return call({a}, {}); }\n+  GCNInstrExecution& operator()(Operand* a, Operand* b) { return call({a, b}, {}); }\n+  GCNInstrExecution& operator()(Operand* a, Operand* b, Operand* c) { return call({a, b, c}, {}); }\n+  GCNInstrExecution& operator()(Operand* a, Operand* b, Operand* c, Operand* d) { return call({a, b, c, d}, {}); }\n+  GCNInstrExecution& operator()(Operand* a, Operand* b, Operand* c, Operand* d, Operand * e) { return call({a, b, c, d, e}, {}); }\n+  GCNInstrExecution& operator()(Operand* a, Operand* b, Operand* c, Operand* d, Operand * e, Operand* f) { return call({a, b, c, d, e, f}, {}); }\n+  GCNInstrExecution& operator()(Operand* a, Operand* b, Operand* c, Operand* d, Operand * e, Operand* f, Operand* g) { return call({a, b, c, d, e, f, g}, {}); }\n+  // clang-format on\n+\n+  // Set operands of this instruction.\n+  GCNInstrExecution &operator()(llvm::ArrayRef<Operand *> oprs,\n+                                llvm::ArrayRef<Modifier *> mods);\n+\n+protected:\n+  GCNInstrExecution &call(llvm::ArrayRef<Operand *> oprs,\n+                          ArrayRef<Modifier *> mods);\n+\n+  GCNBuilder *builder{};\n+  llvm::SmallVector<std::string, 4> instrParts;\n+\n+  friend class GCNInstrExecution;\n+};\n+\n+template <class ConcreteT> struct GCNInstrBase : public GCNInstrCommon {\n+  using Operand = GCNBuilder::Operand;\n+  using Modifier = GCNBuilder::Modifier;\n+\n+  explicit GCNInstrBase(GCNBuilder *builder, const std::string &name)\n+      : GCNInstrCommon(builder) {\n+    o(name);\n+  }\n+\n+  ConcreteT &o(const std::string &suffix, bool predicate = true) {\n+    if (predicate)\n+      instrParts.push_back(suffix);\n+    return *static_cast<ConcreteT *>(this);\n+  }\n+};\n+\n+enum VectorWidth { Byte = 8, Short = 16, Dword = 32, Qword = 64 };\n+\n+struct GCNInstr : public GCNInstrBase<GCNInstr> {\n+  using GCNInstrBase<GCNInstr>::GCNInstrBase;\n+\n+  GCNInstr &float_op_type(int width) {\n+    switch (width) {\n+    case Byte:\n+      assert(Byte != width);\n+      break;\n+    case Short:\n+      o(\"f16\");\n+      break;\n+    case Dword:\n+      o(\"f32\");\n+      break;\n+    case Qword:\n+      o(\"f64\");\n+      break;\n+    default:\n+      break;\n+    }\n+    return *this;\n+  }\n+};\n+\n+struct GCNInstrExecution {\n+  using Operand = GCNBuilder::Operand;\n+  using Modifier = GCNBuilder::Modifier;\n+\n+  llvm::SmallVector<Operand *> argsInOrder;\n+  llvm::SmallVector<Modifier *> mods;\n+\n+  GCNInstrExecution() = default;\n+  explicit GCNInstrExecution(GCNInstrCommon *instr,\n+                             llvm::ArrayRef<Operand *> oprs,\n+                             llvm::ArrayRef<Modifier *> modifiers)\n+      : instr(instr), argsInOrder(oprs.begin(), oprs.end()),\n+        mods(modifiers.begin(), modifiers.end()) {}\n+\n+  std::string dump() const;\n+\n+  SmallVector<Operand *> getArgList() const;\n+\n+  GCNInstrCommon *instr{};\n+};\n+\n+struct GCNMemInstr : public GCNInstrBase<GCNMemInstr> {\n+  using GCNInstrBase<GCNMemInstr>::GCNInstrBase;\n+  // Add specific type suffix to instruction\n+\n+  GCNMemInstr &load_type(int width) {\n+    switch (width) {\n+    case Byte:\n+      o(\"ubyte\");\n+      break;\n+    case Short:\n+      o(\"ushort\");\n+      break;\n+    case Dword:\n+      o(\"dword\");\n+      break;\n+    case Qword:\n+      o(\"dwordx2\");\n+      break;\n+    default:\n+      break;\n+    }\n+    return *this;\n+  }\n+\n+  GCNMemInstr &store_type(int width) {\n+    switch (width) {\n+    case Byte:\n+      o(\"byte\");\n+      break;\n+    case Short:\n+      o(\"short\");\n+      break;\n+    case Dword:\n+      o(\"dword\");\n+      break;\n+    case Qword:\n+      o(\"dwordx2\");\n+      break;\n+    default:\n+      break;\n+    }\n+    return *this;\n+  }\n+};\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif // TRITON_CONVERSION_TRITON_GPU_TO_LLVM_ASM_FORMAT_H_"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h", "status": "modified", "additions": 10, "deletions": 3, "changes": 13, "file_content_changes": "@@ -1,5 +1,5 @@\n-#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_ASM_FORMAT_H\n-#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_ASM_FORMAT_H\n+#ifndef TRITON_CONVERSION_TRITON_GPU_TO_LLVM_PTX_ASM_FORMAT_H_\n+#define TRITON_CONVERSION_TRITON_GPU_TO_LLVM_PTX_ASM_FORMAT_H_\n \n #include \"mlir/IR/Value.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n@@ -144,7 +144,12 @@ struct PTXBuilder {\n \n   // Create a new operand which is written to, that is, the constraint starts\n   // with \"=\", e.g. \"=r\".\n-  Operand *newOperand(StringRef constraint);\n+  // If the operand will be used in predicated execution,\n+  // users may want to initialize it before use.\n+  // Otherwise if the register is only used in the true branch or the false\n+  // branch but not both, the register is undefined and ptxas can perform\n+  // aggressive optimizations that may lead to incorrect results.\n+  Operand *newOperand(StringRef constraint, bool init = false);\n \n   // Create a constant integer operand.\n   Operand *newConstantOperand(int64_t v);\n@@ -171,6 +176,8 @@ struct PTXBuilder {\n     return argArchive.back().get();\n   }\n \n+  void initOperand(Operand *opr);\n+\n   // Make the operands in argArchive follow the provided \\param order.\n   void reorderArgArchive(ArrayRef<Operand *> order) {\n     assert(order.size() == argArchive.size());"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/Passes.h", "status": "added", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -0,0 +1,16 @@\n+#ifndef TRITONGPU_CONVERSION_PASSES_H\n+#define TRITONGPU_CONVERSION_PASSES_H\n+\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n+\n+namespace mlir {\n+namespace triton {\n+\n+#define GEN_PASS_REGISTRATION\n+#include \"triton/Conversion/TritonGPUToLLVM/Passes.h.inc\"\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/Passes.td", "status": "added", "additions": 35, "deletions": 0, "changes": 35, "file_content_changes": "@@ -0,0 +1,35 @@\n+#ifndef TRITONGPU_CONVERSION_PASSES\n+#define TRITONGPU_CONVERSION_PASSES\n+\n+include \"mlir/Pass/PassBase.td\"\n+\n+\n+def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"> {\n+    let summary = \"Convert TritonGPU to LLVM\";\n+    let description = [{\n+\n+    }];\n+    let constructor = \"mlir::triton::createConvertTritonGPUToLLVMPass()\";\n+\n+    let dependentDialects = [\"mlir::arith::ArithDialect\",\n+                             \"mlir::math::MathDialect\",\n+                             \"mlir::gpu::GPUDialect\",\n+                             \"mlir::scf::SCFDialect\",\n+                             \"mlir::LLVM::LLVMDialect\",\n+                             \"mlir::tensor::TensorDialect\",\n+                             \"mlir::triton::TritonDialect\",\n+                             \"mlir::triton::gpu::TritonGPUDialect\",\n+                             \"mlir::ROCDL::ROCDLDialect\",\n+                             \"mlir::NVVM::NVVMDialect\"];\n+\n+    let options = [\n+        Option<\"computeCapability\", \"compute-capability\",\n+               \"int32_t\", /*default*/\"80\",\n+               \"device compute capability\">,\n+        Option<\"isROCM\", \"is-rocm\",\n+               \"bool\", /*default*/\"false\",\n+               \"compile for ROCM-compatible LLVM\">,\n+    ];\n+}\n+\n+#endif"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -13,7 +13,8 @@ template <typename T> class OperationPass;\n namespace triton {\n \n std::unique_ptr<OperationPass<ModuleOp>>\n-createConvertTritonGPUToLLVMPass(int computeCapability = 80);\n+createConvertTritonGPUToLLVMPass(int computeCapability = 80,\n+                                 bool isROCM = false);\n \n } // namespace triton\n "}, {"filename": "include/triton/Conversion/TritonToTritonGPU/CMakeLists.txt", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -0,0 +1,3 @@\n+set(LLVM_TARGET_DEFINITIONS Passes.td)\n+mlir_tablegen(Passes.h.inc -gen-pass-decls --name TritonToTritonGPU)\n+add_public_tablegen_target(TritonConversionPassIncGen)"}, {"filename": "include/triton/Conversion/TritonToTritonGPU/Passes.h", "status": "renamed", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -1,15 +1,13 @@\n #ifndef TRITON_CONVERSION_PASSES_H\n #define TRITON_CONVERSION_PASSES_H\n \n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n \n namespace mlir {\n namespace triton {\n \n #define GEN_PASS_REGISTRATION\n-#include \"triton/Conversion/Passes.h.inc\"\n+#include \"triton/Conversion/TritonToTritonGPU/Passes.h.inc\"\n \n } // namespace triton\n } // namespace mlir"}, {"filename": "include/triton/Conversion/TritonToTritonGPU/Passes.td", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "file_content_changes": "@@ -0,0 +1,31 @@\n+#ifndef TRITON_CONVERSION_PASSES\n+#define TRITON_CONVERSION_PASSES\n+\n+include \"mlir/Pass/PassBase.td\"\n+\n+def ConvertTritonToTritonGPU: Pass<\"convert-triton-to-tritongpu\", \"mlir::ModuleOp\"> {\n+    let summary = \"Convert Triton to TritonGPU\";\n+    let description = [{\n+\n+    }];\n+    let constructor = \"mlir::triton::createConvertTritonToTritonGPUPass()\";\n+\n+    let dependentDialects = [\"mlir::arith::ArithDialect\",\n+                             \"mlir::math::MathDialect\",\n+                             // TODO: Does this pass depend on SCF?\n+                             \"mlir::scf::SCFDialect\",\n+                             \"mlir::triton::TritonDialect\",\n+                             \"mlir::triton::gpu::TritonGPUDialect\"];\n+\n+   let options = [\n+       Option<\"numWarps\", \"num-warps\",\n+              \"int32_t\", /*default*/\"4\",\n+              \"number of warps\">,\n+\n+       Option<\"threadsPerWarp\", \"threads-per-warp\",\n+              \"int32_t\", /*default*/\"32\",\n+              \"number of threads per warp\">,\n+   ];\n+}\n+\n+#endif"}, {"filename": "include/triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -12,12 +12,14 @@ namespace triton {\n \n constexpr static char AttrNumWarpsName[] = \"triton_gpu.num-warps\";\n \n+constexpr static char AttrNumThreadsPerWarp[] = \"triton_gpu.threads-per-warp\";\n+\n // Create the pass with numWarps passed from cl::opt.\n std::unique_ptr<OperationPass<ModuleOp>> createConvertTritonToTritonGPUPass();\n \n // Create the pass with numWarps set explicitly.\n std::unique_ptr<OperationPass<ModuleOp>>\n-createConvertTritonToTritonGPUPass(int numWarps);\n+createConvertTritonToTritonGPUPass(int numWarps, int threadsPerWarp = 32);\n \n } // namespace triton\n } // namespace mlir"}, {"filename": "include/triton/Dialect/Triton/IR/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -8,7 +8,7 @@ set(LLVM_TARGET_DEFINITIONS TritonDialect.td)\n mlir_tablegen(Dialect.h.inc -gen-dialect-decls)\n mlir_tablegen(Dialect.cpp.inc -gen-dialect-defs)\n \n-set(LLVM_TARGET_DEFINITIONS TritonOps.td)\n+set(LLVM_TARGET_DEFINITIONS TritonTypes.td)\n mlir_tablegen(Types.h.inc -gen-typedef-decls)\n mlir_tablegen(Types.cpp.inc -gen-typedef-defs)\n "}, {"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -1,14 +1,15 @@\n #ifndef TRITON_DIALECT_TRITON_IR_DIALECT_H_\n #define TRITON_DIALECT_TRITON_IR_DIALECT_H_\n \n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlow.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/Dialect/Math/IR/Math.h\"\n-#include \"mlir/Dialect/SCF/SCF.h\"\n-#include \"mlir/Dialect/StandardOps/IR/Ops.h\"\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/Dialect.h\"\n #include \"mlir/Interfaces/ControlFlowInterfaces.h\"\n-\n #include \"triton/Dialect/Triton/IR/Dialect.h.inc\"\n #include \"triton/Dialect/Triton/IR/OpsEnums.h.inc\"\n #include \"triton/Dialect/Triton/IR/Traits.h\"\n@@ -36,14 +37,14 @@ class DialectInferLayoutInterface\n   virtual LogicalResult\n   inferExpandDimsOpEncoding(Attribute operandEncoding, unsigned axis,\n                             Attribute &resultEncoding,\n-                            Optional<Location> location) const = 0;\n+                            std::optional<Location> location) const = 0;\n \n   // Note: this function only verify operand encoding but doesn't infer result\n   // encoding\n   virtual LogicalResult\n   inferDotOpEncoding(Attribute operandEncoding, unsigned opIdx,\n                      Attribute retEncoding,\n-                     Optional<Location> location) const = 0;\n+                     std::optional<Location> location) const = 0;\n };\n \n } // namespace triton"}, {"filename": "include/triton/Dialect/Triton/IR/Traits.h", "status": "modified", "additions": 55, "deletions": 5, "changes": 60, "file_content_changes": "@@ -1,9 +1,8 @@\n #ifndef TRITON_IR_TRAITS_H_\n #define TRITON_IR_TRAITS_H_\n \n-#include \"mlir/IR/OpDefinition.h\"\n-\n #include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/OpDefinition.h\"\n #include \"mlir/Support/LogicalResult.h\"\n \n #include <iostream>\n@@ -12,11 +11,9 @@ namespace mlir {\n namespace OpTrait {\n \n // These functions are out-of-line implementations of the methods in the\n-// corresponding trait classes.  This avoids them being template\n+// corresponding trait classes. This avoids them being template\n // instantiated/duplicated.\n namespace impl {\n-LogicalResult verifySameOperandsAndResultEncoding(Operation *op);\n-LogicalResult verifySameOperandsEncoding(Operation *op);\n // The rationale for this trait is to prevent users from creating programs\n // that would have catastrophic register pressure and cause the compiler to\n // hang.\n@@ -25,7 +22,22 @@ LogicalResult verifySameOperandsEncoding(Operation *op);\n // but we probably should limit number of elements (rather than bytes) to\n // keep specs simple\n int constexpr maxTensorNumElements = 1048576;\n+\n LogicalResult verifyTensorSize(Operation *op);\n+\n+LogicalResult verifySameOperandsEncoding(Operation *op,\n+                                         bool allowTensorPointerType = false);\n+\n+LogicalResult\n+verifySameOperandsAndResultEncoding(Operation *op,\n+                                    bool allowTensorPointerType = false);\n+\n+LogicalResult verifySameLoadStoreOperandsShape(Operation *op);\n+\n+LogicalResult verifySameLoadStoreOperandsAndResultShape(Operation *op);\n+\n+bool verifyLoadStorePointerAndValueType(Type valueType, Type ptrType);\n+\n } // namespace impl\n \n template <class ConcreteType>\n@@ -54,6 +66,44 @@ class SameOperandsEncoding\n   }\n };\n \n+template <typename ConcreteType>\n+class SameLoadStoreOperandsShape\n+    : public TraitBase<ConcreteType, SameLoadStoreOperandsShape> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifySameLoadStoreOperandsShape(op);\n+  }\n+};\n+\n+template <typename ConcreteType>\n+class SameLoadStoreOperandsAndResultShape\n+    : public TraitBase<ConcreteType, SameLoadStoreOperandsAndResultShape> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifySameLoadStoreOperandsAndResultShape(op);\n+  }\n+};\n+\n+template <typename ConcreteType>\n+class SameLoadStoreOperandsEncoding\n+    : public TraitBase<ConcreteType, SameLoadStoreOperandsEncoding> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifySameOperandsEncoding(op,\n+                                            /*allowTensorPointerType=*/true);\n+  }\n+};\n+\n+template <typename ConcreteType>\n+class SameLoadStoreOperandsAndResultEncoding\n+    : public TraitBase<ConcreteType, SameLoadStoreOperandsAndResultEncoding> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifySameOperandsAndResultEncoding(\n+        op, /*allowTensorPointerType=*/true);\n+  }\n+};\n+\n } // namespace OpTrait\n } // namespace mlir\n "}, {"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 30, "deletions": 20, "changes": 50, "file_content_changes": "@@ -3,7 +3,7 @@\n \n include \"mlir/IR/EnumAttr.td\"\n \n-// Attrs for LoadOp\n+// Attributes for LoadOp\n def TT_CacheModifierAttr : I32EnumAttr<\n     \"CacheModifier\", \"\",\n     [\n@@ -13,6 +13,18 @@ def TT_CacheModifierAttr : I32EnumAttr<\n     ]> {\n     let cppNamespace = \"::mlir::triton\";\n }\n+\n+def TT_MemSemanticAttr : I32EnumAttr<\n+    \"MemSemantic\", \"\",\n+    [\n+      I32EnumAttrCase<\"RELAXED\", 1, \"relaxed\">,\n+      I32EnumAttrCase<\"ACQUIRE\", 2, \"acquire\">,\n+      I32EnumAttrCase<\"RELEASE\", 3, \"release\">,\n+      I32EnumAttrCase<\"ACQUIRE_RELEASE\", 4, \"acq_rel\">,\n+    ]> {\n+    let cppNamespace = \"::mlir::triton\";\n+}\n+\n def TT_EvictionPolicyAttr : I32EnumAttr<\n     \"EvictionPolicy\", \"\",\n     [\n@@ -23,26 +35,12 @@ def TT_EvictionPolicyAttr : I32EnumAttr<\n     let cppNamespace = \"::mlir::triton\";\n }\n \n-// reduction\n-def TT_RedOpAttr : I32EnumAttr<\n-    /*name*/\"RedOp\", /*summary*/\"\",\n-    /*case*/\n+def TT_PaddingOptionAttr : I32EnumAttr<\n+    \"PaddingOption\", \"\",\n     [\n-        I32EnumAttrCase</*sym*/\"ADD\", 1, /*str*/\"add\">,\n-        I32EnumAttrCase<\"FADD\", 2, \"fadd\">,\n-        I32EnumAttrCase<\"MIN\", 3, \"min\">,\n-        I32EnumAttrCase<\"MAX\", 4, \"max\">,\n-        I32EnumAttrCase<\"UMIN\", 5, \"umin\">,\n-        I32EnumAttrCase<\"UMAX\", 6, \"umax\">,\n-        I32EnumAttrCase<\"ARGMIN\", 7, \"argmin\">,\n-        I32EnumAttrCase<\"ARGMAX\", 8, \"argmax\">,\n-        I32EnumAttrCase<\"ARGUMIN\", 9, \"argumin\">,\n-        I32EnumAttrCase<\"ARGUMAX\", 10, \"argumax\">,\n-        I32EnumAttrCase<\"FMIN\", 11, \"fmin\">,\n-        I32EnumAttrCase<\"FMAX\", 12, \"fmax\">,\n-        I32EnumAttrCase<\"ARGFMIN\", 13, \"argfmin\">,\n-        I32EnumAttrCase<\"ARGFMAX\", 14, \"argfmax\">,\n-        I32EnumAttrCase<\"XOR\", 15, \"xor\">\n+        I32EnumAttrCase<\"PAD_ZERO\", 1, \"zero\">,\n+        // We can not set the string value to \"NAN\" because it is a keyword in C++\n+        I32EnumAttrCase<\"PAD_NAN\", 2, \"nan\">\n     ]> {\n     let cppNamespace = \"::mlir::triton\";\n }\n@@ -65,4 +63,16 @@ def TT_AtomicRMWAttr : I32EnumAttr<\n     let cppNamespace = \"::mlir::triton\";\n }\n \n+// Program ID dimensions.\n+def TT_ProgramDim : I32EnumAttr<\n+    \"ProgramIDDim\", \"\",\n+    [\n+        I32EnumAttrCase<\"X\", 0, \"x\">,\n+        I32EnumAttrCase<\"Y\", 1, \"y\">,\n+        I32EnumAttrCase<\"Z\", 2, \"z\">,\n+    ]> {\n+    let cppNamespace = \"::mlir::triton\";\n+}\n+\n+\n #endif"}, {"filename": "include/triton/Dialect/Triton/IR/TritonDialect.td", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -14,30 +14,30 @@ def Triton_Dialect : Dialect {\n     Triton Dialect.\n \n     Dependent Dialects:\n-      * Arithmetic:\n+      * Arith:\n         * addf, addi, andi, cmpf, cmpi, divf, fptosi, ...\n       * Math:\n         * exp, sin, cos, log, ...\n       * StructuredControlFlow:\n-        * ForOp, IfOp, WhileOp, YieldOp, ConditionOp\n+        * for, if, while, yield, condition\n+      * ControlFlow:\n+        * br, cond_br\n   }];\n \n   let dependentDialects = [\n-    \"arith::ArithmeticDialect\",\n+    \"arith::ArithDialect\",\n     \"math::MathDialect\",\n-    \"StandardOpsDialect\",\n     \"scf::SCFDialect\",\n-\n-    // Since LLVM 15\n-    // \"cf::ControlFlowDialect\",\n-    // \"func::FuncDialect\"\n+    \"cf::ControlFlowDialect\"\n   ];\n \n   let extraClassDeclaration = [{\n     void registerTypes();\n   }];\n \n   let hasConstantMaterializer = 1;\n+  let useDefaultTypePrinterParser = 1;\n+  let usePropertiesForAttributes = 1;\n }\n \n include \"triton/Dialect/Triton/IR/TritonTypes.td\""}, {"filename": "include/triton/Dialect/Triton/IR/TritonInterfaces.td", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "file_content_changes": "@@ -4,8 +4,11 @@\n include \"mlir/IR/OpBase.td\"\n \n def TensorSizeTrait : NativeOpTrait<\"TensorSizeTrait\">;\n-def SameOperandsAndResultEncoding : NativeOpTrait<\"SameOperandsAndResultEncoding\">;\n def SameOperandsEncoding : NativeOpTrait<\"SameOperandsEncoding\">;\n+def SameOperandsAndResultEncoding : NativeOpTrait<\"SameOperandsAndResultEncoding\">;\n+def SameLoadStoreOperandsShape : NativeOpTrait<\"SameLoadStoreOperandsShape\">;\n+def SameLoadStoreOperandsAndResultShape : NativeOpTrait<\"SameLoadStoreOperandsAndResultShape\">;\n+def SameLoadStoreOperandsEncoding : NativeOpTrait<\"SameLoadStoreOperandsEncoding\">;\n+def SameLoadStoreOperandsAndResultEncoding : NativeOpTrait<\"SameLoadStoreOperandsAndResultEncoding\">;\n \n-\n-#endif // TRITON_INTERFACES\n\\ No newline at end of file\n+#endif // TRITON_INTERFACES"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 405, "deletions": 80, "changes": 485, "file_content_changes": "@@ -6,11 +6,15 @@ include \"triton/Dialect/Triton/IR/TritonTypes.td\"\n include \"triton/Dialect/Triton/IR/TritonAttrDefs.td\"\n include \"triton/Dialect/Triton/IR/TritonInterfaces.td\"\n include \"mlir/IR/OpBase.td\"\n-include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n+include \"mlir/IR/FunctionInterfaces.td\" // FunctionOpInterface\n+include \"mlir/IR/SymbolInterfaces.td\" // SymbolUserOpInterface\n+include \"mlir/IR/OpAsmInterface.td\" // OpAsmOpInterface\n+include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/ControlFlowInterfaces.td\" // BranchOpInterface\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n-include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n+include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/CastInterfaces.td\" // CastOpInterface\n+include \"mlir/Interfaces/CallInterfaces.td\" // CallOpInterface\n \n //\n // Op Base\n@@ -20,7 +24,7 @@ class TT_Op<string mnemonic, list<Trait> traits = []> :\n }\n \n //\n-// CastOps\n+// Cast Ops\n //\n // Use cast ops in arith:\n //   bitcast\n@@ -29,7 +33,7 @@ class TT_Op<string mnemonic, list<Trait> traits = []> :\n //   extui, extsi, tructi\n def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n                                          SameOperandsAndResultEncoding,\n-                                         NoSideEffect,\n+                                         Pure,\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Cast int64 to pointer\";\n \n@@ -42,7 +46,7 @@ def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n \n def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n                                          SameOperandsAndResultEncoding,\n-                                         NoSideEffect,\n+                                         Pure,\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Cast pointer to int64\";\n \n@@ -56,7 +60,7 @@ def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n // arith.bitcast doesn't support pointers\n def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n                                      SameOperandsAndResultEncoding,\n-                                     NoSideEffect,\n+                                     Pure,\n                                      /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Cast between types of the same bitwidth\";\n \n@@ -71,8 +75,8 @@ def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n \n def TT_FpToFpOp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n                                      SameOperandsAndResultEncoding,\n-                                     NoSideEffect,\n-                                     DeclareOpInterfaceMethods<CastOpInterface>]> {\n+                                     Pure,\n+                                     /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Floating point casting for custom types\";\n \n     let description = [{\n@@ -93,9 +97,8 @@ def TT_FpToFpOp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n //\n // Pointer Arith Ops\n //\n-\n def TT_AddPtrOp : TT_Op<\"addptr\",\n-                     [NoSideEffect,\n+                     [Pure,\n                       SameOperandsAndResultShape,\n                       SameOperandsAndResultEncoding,\n                       TypesMatchWith<\"result type matches ptr type\",\n@@ -107,76 +110,110 @@ def TT_AddPtrOp : TT_Op<\"addptr\",\n     let assemblyFormat = \"$ptr `,` $offset attr-dict `:` type($result) `,` type($offset)\";\n }\n \n+def TT_AdvanceOp : TT_Op<\"advance\",\n+                         [Pure,\n+                          TypesMatchWith<\"result type matches ptr type\",\n+                                         \"result\", \"ptr\", \"$_self\">]> {\n+    let summary = \"Advance a tensor pointer by offsets\";\n+\n+    let arguments = (ins TT_TensorPtr:$ptr, Variadic<I32>:$offsets);\n+\n+    let results = (outs TT_TensorPtr:$result);\n+\n+    let assemblyFormat = \"$ptr `,` `[` $offsets `]` attr-dict `:` type($result)\";\n+}\n \n //\n // Load/Store Ops\n //\n def TT_LoadOp : TT_Op<\"load\",\n-                      [SameOperandsAndResultShape,\n-                       SameOperandsAndResultEncoding,\n+                      [SameLoadStoreOperandsAndResultShape,\n+                       SameLoadStoreOperandsAndResultEncoding,\n                        AttrSizedOperandSegments,\n                        MemoryEffects<[MemRead]>,\n                        TypesMatchWith<\"infer ptr type from result type\",\n-                                      \"result\", \"ptr\", \"getPointerTypeSameShape($_self)\">,\n+                                      \"result\", \"ptr\", \"$_self\",\n+                                      \"mlir::OpTrait::impl::verifyLoadStorePointerAndValueType\">,\n                        TypesMatchWith<\"infer mask type from result type or none\",\n                                       \"result\", \"mask\", \"getI1SameShape($_self)\",\n                                       \"($_op.getOperands().size() <= 1) || std::equal_to<>()\">,\n                        TypesMatchWith<\"infer other type from result type or none\",\n                                       \"result\", \"other\", \"$_self\",\n                                       \"($_op.getOperands().size() <= 2) || std::equal_to<>()\">]> {\n-    let summary = \"load\";\n+    let summary = \"Load from a tensor of pointers or from a tensor pointer\";\n \n-    let arguments = (ins TT_PtrLike:$ptr, Optional<TT_BoolLike>:$mask, Optional<TT_Type>:$other,\n-                         TT_CacheModifierAttr:$cache, TT_EvictionPolicyAttr:$evict,\n-                         BoolAttr:$isVolatile);\n+    let arguments = (ins AnyTypeOf<[TT_PtrLike, TT_TensorPtr]>:$ptr, Optional<TT_BoolLike>:$mask,\n+                         Optional<TT_Type>:$other, OptionalAttr<DenseI32ArrayAttr>:$boundaryCheck,\n+                         OptionalAttr<TT_PaddingOptionAttr>:$padding, TT_CacheModifierAttr:$cache,\n+                         TT_EvictionPolicyAttr:$evict, BoolAttr:$isVolatile);\n \n     let results = (outs TT_Type:$result);\n \n     let builders = [\n+        // A tensor of pointers or a pointer to a scalar\n         OpBuilder<(ins \"Value\":$ptr, \"triton::CacheModifier\":$cache,\n                        \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n+        // A tensor pointer with boundary check and padding\n+        OpBuilder<(ins \"Value\":$ptr, \"ArrayRef<int32_t>\":$boundaryCheck,\n+                       \"std::optional<triton::PaddingOption>\":$padding, \"triton::CacheModifier\":$cache,\n+                       \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n+        // A tensor of pointers or a pointer to a scalar with mask\n         OpBuilder<(ins \"Value\":$ptr, \"Value\":$mask, \"triton::CacheModifier\":$cache,\n                        \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n+        // A tensor of pointers or a pointer to a scalar with mask and other\n         OpBuilder<(ins \"Value\":$ptr, \"Value\":$mask, \"Value\":$other, \"triton::CacheModifier\":$cache,\n                        \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n+        // A utility function to build the operation with all attributes\n+        OpBuilder<(ins \"Value\":$ptr, \"Value\":$mask, \"Value\":$other,\n+                       \"std::optional<ArrayRef<int32_t>>\":$boundaryCheck,\n+                       \"std::optional<triton::PaddingOption>\":$padding, \"triton::CacheModifier\":$cache,\n+                       \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>\n     ];\n \n-    // let assemblyFormat = \"operands attr-dict `:` type($result)\";\n-    let parser = [{ return mlir::triton::parseLoadOp(parser, result); }];\n-\n-    let printer = [{ return mlir::triton::printLoadOp(p, *this); }];\n+    // Format: `tt.load operands attrs : optional(type(ptr)) -> type(result)`\n+    // We need an extra `optional(type(ptr))` for inferring the tensor pointer type with back compatibility\n+    let hasCustomAssemblyFormat = 1;\n \n     let hasCanonicalizer = 1;\n }\n \n def TT_StoreOp : TT_Op<\"store\",\n-                       [SameOperandsShape,\n-                        SameOperandsEncoding,\n+                       [SameLoadStoreOperandsShape,\n+                        SameLoadStoreOperandsEncoding,\n                         MemoryEffects<[MemWrite]>,\n                         TypesMatchWith<\"infer ptr type from value type\",\n-                                       \"value\", \"ptr\",\n-                                       \"getPointerTypeSameShape($_self)\">,\n+                                       \"value\", \"ptr\", \"$_self\",\n+                                       \"mlir::OpTrait::impl::verifyLoadStorePointerAndValueType\">,\n                         TypesMatchWith<\"infer mask type from value type\",\n                                        \"value\", \"mask\", \"getI1SameShape($_self)\",\n                                        \"($_op.getOperands().size() <= 2) || std::equal_to<>()\">]> {\n-    let summary = \"store\";\n+    let summary = \"Store by a tensor of pointers or by a tensor pointer\";\n \n-    let arguments = (ins TT_PtrLike:$ptr, TT_Type:$value, Optional<TT_BoolLike>:$mask);\n+    let arguments = (ins AnyTypeOf<[TT_PtrLike, TT_TensorPtr]>:$ptr, TT_Type:$value, Optional<TT_BoolLike>:$mask,\n+                         OptionalAttr<DenseI32ArrayAttr>:$boundaryCheck,\n+                         DefaultValuedAttr<TT_CacheModifierAttr, \"triton::CacheModifier::NONE\">:$cache,\n+                         DefaultValuedAttr<TT_EvictionPolicyAttr, \"triton::EvictionPolicy::NORMAL\">:$evict);\n \n     let builders = [\n-        OpBuilder<(ins \"Value\":$ptr, \"Value\":$value)>,\n+        // A tensor of pointers or a pointer to a scalar\n+        OpBuilder<(ins \"Value\":$ptr, \"Value\":$value, \"triton::CacheModifier\":$cache, \"triton::EvictionPolicy\":$evict)>,\n+        // A tensor of pointers or a pointer to a scalar with mask\n+        OpBuilder<(ins \"Value\":$ptr, \"Value\":$value, \"Value\":$mask, \"triton::CacheModifier\":$cache,\n+                       \"triton::EvictionPolicy\":$evict)>,\n+        // A tensor pointer with boundary check\n+        OpBuilder<(ins \"Value\":$ptr, \"Value\":$value, \"ArrayRef<int32_t>\":$boundaryCheck, \"triton::CacheModifier\":$cache,\n+                       \"triton::EvictionPolicy\":$evict)>\n     ];\n \n-    // let assemblyFormat = \"operands attr-dict `:` type($value)\";\n-    let parser = [{ return mlir::triton::parseStoreOp(parser, result); }];\n-\n-    let printer = [{ return mlir::triton::printStoreOp(p, *this); }];\n+    // Format: `tt.store operands attrs : optional(type(ptr)), type(val)\n+    // We need an extra `optional(type(ptr))` for inferring the tensor pointer type with back compatibility\n+    let hasCustomAssemblyFormat = 1;\n \n     let hasCanonicalizer = 1;\n }\n \n //\n-// Atomic Op\n+// Atomic Ops\n //\n def TT_AtomicRMWOp : TT_Op<\"atomic_rmw\", [SameOperandsAndResultShape,\n                                           SameOperandsAndResultEncoding,\n@@ -197,7 +234,8 @@ def TT_AtomicRMWOp : TT_Op<\"atomic_rmw\", [SameOperandsAndResultShape,\n     }];\n \n     let arguments = (ins TT_AtomicRMWAttr:$atomic_rmw_op, TT_PtrLike:$ptr,\n-                         TT_Type:$val, Optional<TT_BoolLike>:$mask);\n+                         TT_Type:$val, Optional<TT_BoolLike>:$mask,\n+                         TT_MemSemanticAttr:$sem);\n \n     let results = (outs TT_Type:$result);\n }\n@@ -218,17 +256,18 @@ def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [MemoryEffects<[MemRead]>,\n         return $old\n     }];\n \n-    let arguments = (ins TT_Ptr:$ptr, TT_Type:$cmp, TT_Type:$val);\n+    let arguments = (ins TT_PtrLike:$ptr, TT_Type:$cmp, TT_Type:$val,\n+                     TT_MemSemanticAttr:$sem);\n \n     let results = (outs TT_Type:$result);\n }\n \n-\n //\n // Shape Manipulation Ops\n //\n-def TT_SplatOp : TT_Op<\"splat\", [NoSideEffect,\n-                                 SameOperandsAndResultElementType]> {\n+def TT_SplatOp : TT_Op<\"splat\", [Pure,\n+                                 SameOperandsAndResultElementType,\n+                                 SameOperandsAndResultEncoding]> {\n     let summary = \"splat\";\n \n     let arguments = (ins TT_Type:$src);\n@@ -240,7 +279,7 @@ def TT_SplatOp : TT_Op<\"splat\", [NoSideEffect,\n     let hasFolder = 1;\n }\n \n-def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect,\n+def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [Pure,\n                                             DeclareOpInterfaceMethods<InferTypeOpInterface>,\n                                             SameOperandsAndResultElementType]> {\n     let summary = \"expand_dims\";\n@@ -250,9 +289,13 @@ def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect,\n     let results = (outs TT_Tensor:$result);\n \n     let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n+\n+    let hasCanonicalizeMethod = 1;\n+    let hasFolder = 1;\n }\n \n-def TT_ViewOp : TT_Op<\"view\", [NoSideEffect,\n+// view is not `pure` because it may reorder elements\n+def TT_ViewOp : TT_Op<\"view\", [NoMemoryEffect,\n                                SameOperandsAndResultElementType]> {\n     let summary = \"view\";\n \n@@ -262,10 +305,13 @@ def TT_ViewOp : TT_Op<\"view\", [NoSideEffect,\n \n     let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n \n+    let hasCanonicalizeMethod = 1;\n+    let hasFolder = 1;\n }\n \n-def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect,\n-                                         SameOperandsAndResultElementType]> {\n+def TT_BroadcastOp : TT_Op<\"broadcast\", [Pure,\n+                                         SameOperandsAndResultElementType,\n+                                         SameOperandsAndResultEncoding]> {\n     let summary = \"broadcast. No left-padding as of now.\";\n \n     let arguments = (ins TT_Type:$src);\n@@ -274,10 +320,12 @@ def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect,\n \n     let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n \n+    let hasCanonicalizeMethod = 1;\n     let hasFolder = 1;\n }\n \n-def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n+// cat is not `pure` because it may reorder elements\n+def TT_CatOp : TT_Op<\"cat\", [NoMemoryEffect,\n                              SameOperandsAndResultElementType]> {\n     let summary = \"concatenate 2 tensors\";\n \n@@ -288,8 +336,9 @@ def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n     let assemblyFormat = \"$lhs `,` $rhs attr-dict `:` functional-type(operands, results)\";\n }\n \n-def TT_TransOp : TT_Op<\"trans\", [NoSideEffect,\n-                                 DeclareOpInterfaceMethods<InferTypeOpInterface>]> {\n+def TT_TransOp : TT_Op<\"trans\", [Pure,\n+                                 DeclareOpInterfaceMethods<InferTypeOpInterface>,\n+                                 SameOperandsAndResultElementType]> {\n \n     let summary = \"transpose a tensor\";\n \n@@ -303,15 +352,21 @@ def TT_TransOp : TT_Op<\"trans\", [NoSideEffect,\n //\n // SPMD Ops\n //\n-def TT_GetProgramIdOp : TT_Op<\"get_program_id\", [NoSideEffect]> {\n-    let arguments = (ins I32Attr:$axis);\n+def TT_GetProgramIdOp : TT_Op<\"get_program_id\", [Pure]> {\n+    let arguments = (ins TT_ProgramDim:$axis);\n \n     let results = (outs I32:$result);\n \n-    let assemblyFormat = \"attr-dict `:` type($result)\";\n+    let assemblyFormat = \"$axis attr-dict `:` type($result)\";\n+\n+    let extraClassDeclaration = [{\n+      int32_t getAxisAsInt() {\n+        return static_cast<int32_t>(getAxis());\n+      }\n+    }];\n }\n \n-def TT_GetNumProgramsOp : TT_Op<\"get_num_programs\", [NoSideEffect]> {\n+def TT_GetNumProgramsOp : TT_Op<\"get_num_programs\", [Pure]> {\n     let arguments = (ins I32Attr:$axis);\n \n     let results = (outs I32:$result);\n@@ -322,7 +377,7 @@ def TT_GetNumProgramsOp : TT_Op<\"get_num_programs\", [NoSideEffect]> {\n //\n // Dot Op\n //\n-def TT_DotOp : TT_Op<\"dot\", [NoSideEffect,\n+def TT_DotOp : TT_Op<\"dot\", [Pure,\n                              DeclareOpInterfaceMethods<InferTypeOpInterface>,\n                              TypesMatchWith<\"result's type matches accumulator's type\",\n                                             \"d\", \"c\", \"$_self\">]> {\n@@ -342,53 +397,69 @@ def TT_DotOp : TT_Op<\"dot\", [NoSideEffect,\n //\n // Reduce Op\n //\n-def TT_ReduceOp : TT_Op<\"reduce\", [NoSideEffect,\n-                                   DeclareOpInterfaceMethods<InferTypeOpInterface>]> {\n-    let summary = \"reduce\";\n-\n-    let arguments = (ins TT_RedOpAttr:$redOp, TT_Tensor:$operand, I32Attr:$axis);\n-\n-    let results = (outs TT_Type:$result);\n-\n+def TT_ReduceOp: TT_Op<\"reduce\",\n+                       [Pure,\n+                        SameOperandsEncoding,\n+                        SingleBlock,\n+                        DeclareOpInterfaceMethods<InferTypeOpInterface>]> {\n+    let summary = \"Reduction using generic combination algorithm\";\n+    let arguments = (ins Variadic<TT_Tensor>:$operands, I32Attr:$axis);\n+    let results = (outs Variadic<TT_Type>:$result);\n+    let regions = (region SizedRegion<1>:$combineOp);\n     let builders = [\n-        OpBuilder<(ins \"triton::RedOp\":$redOp, \"Value\":$operand, \"int\":$axis)>,\n+        OpBuilder<(ins \"ValueRange\":$operands, \"int\":$axis)>,\n     ];\n-\n-    let assemblyFormat = \"$operand attr-dict `:` type($operand) `->` type($result)\";\n-\n+    let hasVerifier = 1;\n+    let hasRegionVerifier = 1;\n     let extraClassDeclaration = [{\n-        // This member function is marked static because we need to call it before the ReduceOp\n-        // is constructed, see the implementation of create_reduce in triton.cc.\n-        static bool withIndex(mlir::triton::RedOp redOp);\n+      llvm::SmallVector<RankedTensorType> getInputTypes();\n+      llvm::SmallVector<Type> getElementTypes();\n+      unsigned getNumOperands();\n     }];\n }\n \n+def TT_ReduceReturnOp: TT_Op<\"reduce.return\",\n+                             [HasParent<\"ReduceOp\">, Pure, Terminator, ReturnLike]> {\n+    let summary = \"terminator for reduce operator\";\n+    let arguments = (ins Variadic<AnyType>:$result);\n+    let assemblyFormat = \"$result attr-dict `:` type($result)\";\n+}\n+\n+\n //\n-// External elementwise op\n+// External Elementwise op\n //\n-def TT_ExtElemwiseOp : TT_Op<\"ext_elemwise\", [NoSideEffect, Elementwise, SameOperandsAndResultShape,\n-                                              SameOperandsAndResultEncoding,\n-                                              SameVariadicOperandSize]> {\n-    let summary = \"ext_elemwise\";\n+class TT_ExternElementwiseOpBase<string mnemonic, list<Trait> traits = []> :\n+    TT_Op<mnemonic,\n+         traits # [SameOperandsAndResultEncoding,\n+                   SameVariadicOperandSize]> {\n \n     let description = [{\n         call an external function $symbol implemented in $libpath/$libname with $args\n-\n         return $libpath/$libname:$symbol($args...)\n     }];\n \n     let arguments = (ins Variadic<TT_Type>:$args, StrAttr:$libname, StrAttr:$libpath, StrAttr:$symbol);\n \n     let results = (outs TT_Type:$result);\n \n-    let assemblyFormat = \"operands attr-dict `:` type(operands) `->` type($result)\";\n+    let assemblyFormat = \"operands attr-dict `:` functional-type(operands, $result)\";\n+}\n+\n+def TT_PureExternElementwiseOp : TT_ExternElementwiseOpBase<\"pure_extern_elementwise\", [Pure, Elementwise]> {\n+    let summary = \"FFI for pure element-wise extern LLVM bitcode functions\";\n+}\n+\n+def TT_ImpureExternElementwiseOp : TT_ExternElementwiseOpBase<\"impure_extern_elementwise\", [MemoryEffects<[MemRead]>,\n+                                                                                            MemoryEffects<[MemWrite]>]> {\n+    let summary = \"FFI for impure element-wise extern LLVM bitcode functions\";\n }\n \n //\n // Make Range Op\n //\n // TODO: should have ConstantLike as Trait\n-def TT_MakeRangeOp : TT_Op<\"make_range\", [NoSideEffect]> {\n+def TT_MakeRangeOp : TT_Op<\"make_range\", [Pure]> {\n     let summary = \"make range\";\n \n     let description = [{\n@@ -405,19 +476,273 @@ def TT_MakeRangeOp : TT_Op<\"make_range\", [NoSideEffect]> {\n }\n \n //\n-// Make PrintfOp\n+// Print Op\n //\n-def TT_PrintfOp : TT_Op<\"printf\", [MemoryEffects<[MemWrite]>]>,\n-  Arguments<(ins StrAttr:$prefix,\n-                Variadic<AnyTypeOf<[TT_Type]>>:$args)> {\n-  let summary = \"Device-side printf, as in CUDA for debugging\";\n+def TT_PrintOp : TT_Op<\"print\", [MemoryEffects<[MemWrite]>]>,\n+  Arguments<(ins StrAttr:$prefix, Variadic<AnyTypeOf<[TT_Type]>>:$args)> {\n+  let summary = \"Device-side print, as in CUDA for debugging\";\n   let description = [{\n-    `tt.printf` takes a literal string prefix and an arbitrary number of scalar or tensor arguments that should be printed.\n+    `tt.print` takes a literal string prefix and an arbitrary number of scalar or tensor arguments that should be printed.\n     format are generated automatically from the arguments.\n   }];\n   let assemblyFormat = [{\n-    $prefix attr-dict ($args^ `:` type($args))?\n+    $prefix attr-dict (`:` $args^ `:` type($args))?\n+  }];\n+}\n+\n+//\n+// Assert Op\n+//\n+def TT_AssertOp : TT_Op<\"assert\", [MemoryEffects<[MemWrite]>]> {\n+  let summary = \"Device-side assert, as in CUDA for correctness checking\";\n+  let description = [{\n+    `tt.assert` takes a condition tensor, a message string, a file string, a function string, and a line number.\n+    If the condition is false, the message is printed, and the program is aborted.\n+  }];\n+  let arguments = (ins TT_Tensor:$condition, StrAttr:$message, StrAttr:$file, StrAttr:$func, I32Attr:$line);\n+  let assemblyFormat = \"$condition `,` $message `,` $file `,` $func `,` $line attr-dict `:` type($condition)\";\n+}\n+\n+//\n+// Make Tensor Pointer Op\n+//\n+def TT_MakeTensorPtrOp : TT_Op<\"make_tensor_ptr\",\n+                               [Pure,\n+                                SameVariadicOperandSize,\n+                                TypesMatchWith<\"infer pointer type from the result type\",\n+                                               \"result\", \"base\",\n+                                               \"getPointerType(getElementTypeOfTensorPointerType($_self))\">]> {\n+  let summary = \"Make a tensor pointer type with meta information of the parent tensor and the block specified\";\n+\n+  let description = [{\n+      `tt.make_tensor_ptr` takes both meta information of the parent tensor and the block tensor, then it returns a\n+      pointer to the block tensor, e.g. returns a type of `tt.ptr<tensor<8x8xf16>>`.\n+  }];\n+\n+  // TODO(Chenggang): unify the integer types. Currently we cannot do that due to hardware constraints.\n+  let arguments = (ins\n+    TT_Ptr:$base,\n+    Variadic<I64>:$shape,\n+    Variadic<I64>:$strides,\n+    Variadic<I32>:$offsets,\n+    DenseI32ArrayAttr:$order\n+  );\n+\n+  let results = (outs TT_TensorPtr:$result);\n+\n+  // Add additional `[]` to increase readability and split variadic lists\n+  let assemblyFormat = \"$base `,` `[` $shape `]` `,` `[` $strides `]` `,` `[` $offsets `]` attr-dict `:` type($result)\";\n+\n+  let builders = [\n+    OpBuilder<(ins\n+        \"Value\":$base,\n+        \"ValueRange\":$shape,\n+        \"ValueRange\":$strides,\n+        \"ValueRange\":$offsets,\n+        \"ArrayRef<int32_t>\":$tensorShape,\n+        \"ArrayRef<int32_t>\":$order\n+    )>\n+  ];\n+}\n+\n+// The following ops, including `call`, `func`, and `return` are copied and modified from\n+// https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Func/IR/FuncOps.td\n+// We could revert it back once MLIR has a better inliner interface.\n+//\n+// Function Ops\n+//\n+def CallOp : TT_Op<\"call\", [CallOpInterface, /*MemRefsNormalizable, */DeclareOpInterfaceMethods<SymbolUserOpInterface>]> {\n+  let summary = \"call operation\";\n+  let description = [{\n+    The `tt.call` operation represents a direct call to a function that is\n+    within the same symbol scope as the call. The operands and result types of\n+    the call must match the specified function type. The callee is encoded as a\n+    symbol reference attribute named \"callee\".\n+\n+    Example:\n+\n+    ```mlir\n+    %2 = tt.call @my_add(%0, %1) : (f32, f32) -> f32\n+    ```\n+  }];\n+\n+  let arguments = (ins FlatSymbolRefAttr:$callee, Variadic<AnyType>:$operands);\n+  let results = (outs Variadic<AnyType>);\n+\n+  let builders = [\n+    OpBuilder<(ins \"FuncOp\":$callee, CArg<\"ValueRange\", \"{}\">:$operands), [{\n+      $_state.addOperands(operands);\n+      $_state.addAttribute(\"callee\", SymbolRefAttr::get(callee));\n+      $_state.addTypes(callee.getFunctionType().getResults());\n+    }]>,\n+    OpBuilder<(ins \"SymbolRefAttr\":$callee, \"TypeRange\":$results,\n+      CArg<\"ValueRange\", \"{}\">:$operands), [{\n+      $_state.addOperands(operands);\n+      $_state.addAttribute(\"callee\", callee);\n+      $_state.addTypes(results);\n+    }]>,\n+    OpBuilder<(ins \"StringAttr\":$callee, \"TypeRange\":$results,\n+      CArg<\"ValueRange\", \"{}\">:$operands), [{\n+      build($_builder, $_state, SymbolRefAttr::get(callee), results, operands);\n+    }]>,\n+    OpBuilder<(ins \"StringRef\":$callee, \"TypeRange\":$results,\n+      CArg<\"ValueRange\", \"{}\">:$operands), [{\n+      build($_builder, $_state, StringAttr::get($_builder.getContext(), callee),\n+            results, operands);\n+    }]>];\n+\n+  let extraClassDeclaration = [{\n+    FunctionType getCalleeType() {\n+      return FunctionType::get(getContext(), getOperandTypes(), getResultTypes());\n+    }\n+\n+    /// Get the argument operands to the called function.\n+    operand_range getArgOperands() {\n+      return {arg_operand_begin(), arg_operand_end()};\n+    }\n+\n+    operand_iterator arg_operand_begin() { return operand_begin(); }\n+    operand_iterator arg_operand_end() { return operand_end(); }\n+\n+    /// Return the callee of this operation.\n+    CallInterfaceCallable getCallableForCallee() {\n+      return (*this)->getAttrOfType<SymbolRefAttr>(\"callee\");\n+    }\n+\n+    /// Set the callee for this operation.\n+    void setCalleeFromCallable(CallInterfaceCallable callee) {\n+      (*this)->setAttr(\"callee\", callee.get<SymbolRefAttr>());\n+    }\n+  }];\n+\n+  let assemblyFormat = [{\n+    $callee `(` $operands `)` attr-dict `:` functional-type($operands, results)\n+  }];\n+}\n+\n+def FuncOp : TT_Op<\"func\", [AffineScope, AutomaticAllocationScope, CallableOpInterface, FunctionOpInterface, IsolatedFromAbove, OpAsmOpInterface]> {\n+  let summary = \"An operation with a name containing a single `SSACFG` region\";\n+  let description = [{\n+    Operations within the function cannot implicitly capture values defined\n+    outside of the function, i.e. Functions are `IsolatedFromAbove`. All\n+    external references must use function arguments or attributes that establish\n+    a symbolic connection (e.g. symbols referenced by name via a string\n+    attribute like SymbolRefAttr). An external function declaration (used when\n+    referring to a function declared in some other module) has no body. While\n+    the MLIR textual form provides a nice inline syntax for function arguments,\n+    they are internally represented as \u201cblock arguments\u201d to the first block in\n+    the region.\n+\n+    Only dialect attribute names may be specified in the attribute dictionaries\n+    for function arguments, results, or the function itself.\n+\n+    Example:\n+\n+    ```mlir\n+    // External function definitions.\n+    tt.func @abort()\n+    tt.func @scribble(i32, i64, memref<? x 128 x f32, #layout_map0>) -> f64\n+\n+    // A function that returns its argument twice:\n+    tt.func @count(%x: i64) -> (i64, i64)\n+      attributes {fruit: \"banana\"} {\n+      return %x, %x: i64, i64\n+    }\n+\n+    // A function with an argument attribute\n+    tt.func @example_fn_arg(%x: i32 {swift.self = unit})\n+\n+    // A function with a result attribute\n+    tt.func @example_fn_result() -> (f64 {dialectName.attrName = 0 : i64})\n+\n+    // A function with an attribute\n+    tt.func @example_fn_attr() attributes {dialectName.attrName = false}\n+    ```\n   }];\n+\n+  let arguments = (ins SymbolNameAttr:$sym_name,\n+                       TypeAttrOf<FunctionType>:$function_type,\n+                       OptionalAttr<StrAttr>:$sym_visibility,\n+                       OptionalAttr<DictArrayAttr>:$arg_attrs,\n+                       OptionalAttr<DictArrayAttr>:$res_attrs);\n+  let regions = (region AnyRegion:$body);\n+\n+  let builders = [OpBuilder<(ins\n+    \"StringRef\":$name, \"FunctionType\":$type,\n+    CArg<\"ArrayRef<NamedAttribute>\", \"{}\">:$attrs,\n+    CArg<\"ArrayRef<DictionaryAttr>\", \"{}\">:$argAttrs)\n+  >];\n+  let extraClassDeclaration = [{\n+    //===------------------------------------------------------------------===//\n+    // CallableOpInterface\n+    //===------------------------------------------------------------------===//\n+\n+    /// Returns the region on the current operation that is callable. This may\n+    /// return null in the case of an external callable object, e.g. an external\n+    /// function.\n+    ::mlir::Region *getCallableRegion() { return isExternal() ? nullptr : &getBody(); }\n+\n+    /// Returns the results types that the callable region produces when\n+    /// executed.\n+    ArrayRef<Type> getCallableResults() { return getFunctionType().getResults(); }\n+\n+    /// Returns the argument attributes for all callable region arguments or\n+    /// null if there are none.\n+    ::mlir::ArrayAttr getCallableArgAttrs() {\n+      return getArgAttrs().value_or(nullptr);\n+    }\n+\n+    /// Returns the result attributes for all callable region results or\n+    /// null if there are none.\n+    ::mlir::ArrayAttr getCallableResAttrs() {\n+      return getResAttrs().value_or(nullptr);\n+    }\n+\n+    //===------------------------------------------------------------------===//\n+    // FunctionOpInterface Methods\n+    //===------------------------------------------------------------------===//\n+\n+    /// Returns the argument types of this function.\n+    ArrayRef<Type> getArgumentTypes() { return getFunctionType().getInputs(); }\n+\n+    /// Returns the result types of this function.\n+    ArrayRef<Type> getResultTypes() { return getFunctionType().getResults(); }\n+\n+    //===------------------------------------------------------------------===//\n+    // SymbolOpInterface Methods\n+    //===------------------------------------------------------------------===//\n+\n+    bool isDeclaration() { return isExternal(); }\n+  }];\n+  let hasCustomAssemblyFormat = 1;\n+}\n+\n+def ReturnOp : TT_Op<\"return\", [Pure, HasParent<\"FuncOp\">, /*MemRefsNormalizable, */ReturnLike, Terminator]> {\n+  let summary = \"Function return operation\";\n+  let description = [{\n+    The `tt.return` operation represents a return operation within a function.\n+    The operation takes variable number of operands and produces no results.\n+    The operand number and types must match the signature of the function\n+    that contains the operation.\n+\n+    Example:\n+\n+    ```mlir\n+    tt.func @foo() : (i32, f8) {\n+      ...\n+      tt.return %0, %1 : i32, f8\n+    }\n+    ```\n+  }];\n+\n+  let arguments = (ins Variadic<AnyType>:$operands);\n+\n+  let builders = [OpBuilder<(ins), [{\n+    build($_builder, $_state, std::nullopt);\n+  }]>];\n+\n+  let assemblyFormat = \"attr-dict ($operands^ `:` type($operands))?\";\n+  let hasVerifier = 1;\n }\n \n #endif // Triton_OPS"}, {"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "modified", "additions": 32, "deletions": 10, "changes": 42, "file_content_changes": "@@ -1,6 +1,7 @@\n #ifndef TRITON_TYPES\n #define TRITON_TYPES\n \n+include \"mlir/IR/AttrTypeBase.td\"\n include \"triton/Dialect/Triton/IR/TritonDialect.td\"\n \n //\n@@ -13,9 +14,7 @@ class TritonTypeDef<string name, string _mnemonic>\n }\n \n // Floating-point Type\n-def F8 : TritonTypeDef<\"Float8\", \"f8\">;\n-\n-def TT_Float : AnyTypeOf<[F8, F16, BF16, F32, F64], \"floating-point\">;\n+def TT_Float : AnyTypeOf<[F8E4M3FN, F8E5M2, F16, BF16, F32, F64], \"floating-point\">;\n def TT_FloatTensor : TensorOf<[TT_Float]>;\n def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;\n \n@@ -32,19 +31,28 @@ def TT_IntLike : AnyTypeOf<[TT_Int, TT_IntTensor]>;\n // I32 Type\n // TT_I32 -> I32\n // TT_I32Tensor -> I32Tensor\n-def TT_I32Like: AnyTypeOf<[I32, I32Tensor]>;\n+def TT_I32Like : AnyTypeOf<[I32, I32Tensor]>;\n \n // I64 Type\n // TT_I64 -> I64\n // TT_I64Tensor -> I64Tensor\n-def TT_I64Like: AnyTypeOf<[I64, I64Tensor]>;\n+def TT_I64Like : AnyTypeOf<[I64, I64Tensor]>;\n+\n+// Pointer Type in TableGen\n+class TT_PtrOf<list<Type> pointeeTypes> :\n+    DialectType<Triton_Dialect,\n+                And<[CPred<\"$_self.isa<::mlir::triton::PointerType>()\">,\n+                     Concat<\"[](::mlir::Type pointeeType) { return \",\n+                            SubstLeaves<\"$_self\", \"pointeeType\", AnyTypeOf<pointeeTypes>.predicate>,\n+                                        \"; }($_self.cast<::mlir::triton::PointerType>().getPointeeType())\">]>,\n+                \"ptr\", \"::mlir::triton::PointerType\">;\n \n-// Pointer Type\n-def TT_Ptr : TritonTypeDef<\"Pointer\", \"ptr\"> {\n-    let summary = \"pointer type\";\n+// Pointer Type in C++ (corresponding to `TT_PtrOf`)\n+def TT_PtrType : TritonTypeDef<\"Pointer\", \"ptr\"> {\n+    let summary = \"Pointer type (`::mlir::triton::PointerType`) in Triton IR type system\";\n \n     let description = [{\n-        Triton PointerType\n+        Pointer type in Triton IR type system, which could be pointing to scalars or tensors.\n     }];\n \n     let parameters = (ins \"Type\":$pointeeType, \"int\":$addressSpace);\n@@ -58,14 +66,28 @@ def TT_Ptr : TritonTypeDef<\"Pointer\", \"ptr\"> {\n         }]>\n     ];\n \n+    let hasCustomAssemblyFormat = 1;\n+\n     let skipDefaultBuilders = 1;\n }\n+\n+// Scalar Pointer Type: `ptr<>`\n+def TT_Ptr : TT_PtrOf<[AnyType]>;\n+\n+// Tensor of Pointer Type\n def TT_PtrTensor : TensorOf<[TT_Ptr]>;\n+\n+// Tensor of Pointer Type or Pointer type: `tensor<ptr<>>` or `ptr<>`\n def TT_PtrLike : AnyTypeOf<[TT_Ptr, TT_PtrTensor]>;\n \n+// Tensor Type\n def TT_FpIntTensor : AnyTypeOf<[TT_FloatTensor, TT_IntTensor]>;\n def TT_Tensor : AnyTypeOf<[TT_FpIntTensor, TT_PtrTensor]>;\n \n-def TT_Type : AnyTypeOf<[TT_FloatLike, TT_IntLike, TT_PtrLike]>;\n+// Pointer Type to Tensor Type: `ptr<tensor<>>`\n+def TT_TensorPtr : TT_PtrOf<[TT_Tensor]>;\n+\n+// Any Type in Triton IR\n+def TT_Type : AnyTypeOf<[TT_FloatLike, TT_IntLike, TT_PtrLike, TT_TensorPtr]>;\n \n #endif"}, {"filename": "include/triton/Dialect/Triton/IR/Types.h", "status": "modified", "additions": 25, "deletions": 0, "changes": 25, "file_content_changes": "@@ -1,10 +1,35 @@\n #ifndef TRITON_IR_TYPES_H_\n #define TRITON_IR_TYPES_H_\n \n+#include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/TypeSupport.h\"\n #include \"mlir/IR/Types.h\"\n \n #define GET_TYPEDEF_CLASSES\n #include \"triton/Dialect/Triton/IR/Types.h.inc\"\n \n+namespace mlir {\n+\n+namespace triton {\n+\n+bool isTensorPointerType(Type type);\n+\n+unsigned getPointeeBitWidth(Type type);\n+\n+Type getPointeeType(Type type);\n+\n+Type getPointerType(Type type);\n+\n+Type getElementTypeOfTensorPointerType(Type type);\n+\n+Type getI1SameShape(Type type);\n+\n+Type getI32SameShape(Type type);\n+\n+Type getPointerTypeSameShape(Type type);\n+\n+} // namespace triton\n+\n+} // namespace mlir\n+\n #endif // TRITON_IR_TYPES_H_"}, {"filename": "include/triton/Dialect/Triton/Transforms/Passes.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -8,6 +8,9 @@ namespace triton {\n \n std::unique_ptr<Pass> createCombineOpsPass();\n \n+std::unique_ptr<Pass>\n+createRewriteTensorPointerPass(int computeCapability = 80);\n+\n } // namespace triton\n \n #define GEN_PASS_REGISTRATION"}, {"filename": "include/triton/Dialect/Triton/Transforms/Passes.td", "status": "modified", "additions": 20, "deletions": 2, "changes": 22, "file_content_changes": "@@ -16,8 +16,26 @@ def TritonCombineOps : Pass</*cli-arg*/\"triton-combine\", /*Op*/\"mlir::ModuleOp\">\n \n   let constructor = \"mlir::triton::createCombineOpsPass()\";\n \n-  let dependentDialects = [\"mlir::arith::ArithmeticDialect\",\n-                           /*SelectOp*/\"mlir::StandardOpsDialect\"];\n+  let dependentDialects = [\"mlir::arith::ArithDialect\"];\n+}\n+\n+def TritonRewriteTensorPointer : Pass</*cli-arg*/\"triton-rewrite-tensor-pointer\", /*Op*/\"mlir::ModuleOp\"> {\n+  let summary = \"Rewrite load/stores with tensor pointers into legacy load/stores\";\n+  let description = [{\n+    This pass rewrites all load/store semantics initiated by a `tt.make_tensor_ptr` and `tt.advance` into legacy\n+    semantics. After this pass, `tt.make_tensor_ptr` and `tt.advance` will disappear, and it generates logics to compute\n+    the pointer/mask/other for each load/store.\n+  }];\n+\n+  let constructor = \"mlir::triton::createRewriteTensorPointerPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::TritonDialect\"];\n+\n+  let options = [\n+    Option<\"computeCapability\", \"compute-capability\",\n+           \"int32_t\", /*default*/\"80\",\n+           \"device compute capability\">\n+  ];\n }\n \n #endif"}, {"filename": "include/triton/Dialect/TritonGPU/IR/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -9,4 +9,3 @@ set(LLVM_TARGET_DEFINITIONS TritonGPUAttrDefs.td)\n mlir_tablegen(TritonGPUAttrDefs.h.inc -gen-attrdef-decls)\n mlir_tablegen(TritonGPUAttrDefs.cpp.inc -gen-attrdef-defs)\n add_public_tablegen_target(TritonGPUAttrDefsIncGen)\n-"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 46, "deletions": 11, "changes": 57, "file_content_changes": "@@ -1,19 +1,17 @@\n #ifndef TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_\n #define TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_\n \n-#include \"mlir/Dialect/GPU/GPUDialect.h\"\n+#include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/Dialect.h\"\n \n // TritonGPU depends on Triton\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n-\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h.inc\"\n #include \"triton/Dialect/TritonGPU/IR/Traits.h\"\n \n #define GET_ATTRDEF_CLASSES\n-#include \"triton/Dialect/Triton/IR/AttrInterfaces.h.inc\"\n #include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.h.inc\"\n \n #define GET_OP_CLASSES\n@@ -23,28 +21,65 @@ namespace mlir {\n namespace triton {\n namespace gpu {\n \n-unsigned getElemsPerThread(Type type);\n+unsigned getTotalElemsPerThread(Type type);\n+\n+unsigned getTotalElemsPerThread(Attribute layout, ArrayRef<int64_t> shape,\n+                                Type eltTy);\n \n-SmallVector<unsigned> getThreadsPerWarp(const Attribute &layout);\n+SmallVector<unsigned> getElemsPerThread(Type type);\n \n-SmallVector<unsigned> getWarpsPerCTA(const Attribute &layout);\n+SmallVector<unsigned> getThreadsPerWarp(Attribute layout);\n \n-SmallVector<unsigned> getSizePerThread(const Attribute &layout);\n+SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n \n+SmallVector<unsigned> getSizePerThread(Attribute layout);\n+\n+// Returns the number of contiguous elements that each thread\n+// has access to, on each dimension of the tensor. E.g.\n+// for a blocked layout with sizePerThread = [1, 4], returns [1, 4],\n+// regardless of the shape of the tensor.\n SmallVector<unsigned> getContigPerThread(Attribute layout);\n \n-SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout);\n+// Returns the number of non-replicated contiguous elements that each thread\n+// has access to, on each dimension of the tensor. For a blocked layout\n+// with sizePerThread = [1, 4] and tensor shape = [128, 1], the elements\n+// for thread 0 would be [A_{0, 0}, A_{0, 0}, A_{0, 0}, A_{0, 0}], returns [1,\n+// 1]. Whereas for a tensor shape [128, 128], the elements for thread 0 would be\n+// [A_{0, 0}, A_{0, 1}, A_{0, 2}, A_{0, 3}], returns [1, 4].\n+SmallVector<unsigned> getUniqueContigPerThread(Type type);\n+\n+// Returns the number of threads per warp that have access to non-replicated\n+// elements of the tensor. E.g. for a blocked layout with sizePerThread = [1,\n+// 1], threadsPerWarp = [2, 16] and tensor shape = [2, 2], threads 0, 1, 16, 17\n+// have access to the full tensor, whereas the other threads have access to\n+// replicated elements, so this function returns [2, 2].\n+SmallVector<unsigned>\n+getThreadsPerWarpWithUniqueData(Attribute layout,\n+                                ArrayRef<int64_t> tensorShape);\n+\n+// Returns the number of warps per CTA that have access to non-replicated\n+// elements of the tensor. E.g. for a blocked layout with sizePerThread = [1,\n+// 1], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4] and tensor shape = [2, 2],\n+// returns [1, 1], since the first warp has access to the full tensor, whereas\n+// the other warps have access to replicated elements.\n+SmallVector<unsigned>\n+getWarpsPerCTAWithUniqueData(Attribute layout, ArrayRef<int64_t> tensorShape);\n+\n+SmallVector<unsigned> getThreadsPerCTA(Attribute layout);\n \n SmallVector<unsigned>\n-getShapePerCTA(const Attribute &layout,\n+getShapePerCTA(Attribute layout,\n                ArrayRef<int64_t> tensorShape = ArrayRef<int64_t>());\n \n-SmallVector<unsigned> getOrder(const Attribute &layout);\n+SmallVector<unsigned> getOrder(Attribute layout);\n \n-bool isaDistributedLayout(const Attribute &layout);\n+bool isaDistributedLayout(Attribute layout);\n+\n+bool isSharedEncoding(Value value);\n \n } // namespace gpu\n } // namespace triton\n+\n } // namespace mlir\n \n #endif // TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 84, "deletions": 43, "changes": 127, "file_content_changes": "@@ -1,6 +1,7 @@\n #ifndef TRITONGPU_ATTRDEFS\n #define TRITONGPU_ATTRDEFS\n \n+include \"mlir/IR/AttrTypeBase.td\"\n include \"triton/Dialect/TritonGPU/IR/TritonGPUDialect.td\"\n include \"triton/Dialect/Triton/IR/TritonInterfaces.td\"\n \n@@ -34,7 +35,8 @@ Right now, Triton implements two classes of layouts: shared, and distributed.\n   }];\n \n   code extraBaseClassDeclaration = [{\n-    unsigned getElemsPerThread(ArrayRef<int64_t> shape) const;\n+    unsigned getTotalElemsPerThread(ArrayRef<int64_t> shape, Type eltTy) const;\n+    SmallVector<unsigned> getElemsPerThread(ArrayRef<int64_t> shape, Type eltTy) const;\n     ::mlir::LogicalResult verifyLayoutForArg(::mlir::Operation* op, unsigned argNo) const;\n   }];\n }\n@@ -81,6 +83,7 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n         if(!mmaEnc)\n           return $_get(context, 1, 1, 1, order);\n \n+\n         int opIdx = dotOpEnc.getOpIdx();\n \n         // number of rows per phase\n@@ -136,6 +139,7 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n   ];\n \n   let extraClassDeclaration = extraBaseClassDeclaration;\n+  let hasCustomAssemblyFormat = 1;\n }\n \n //===----------------------------------------------------------------------===//\n@@ -225,31 +229,32 @@ for\n     AttrBuilder<(ins \"ArrayRef<int64_t>\":$shape,\n                      \"ArrayRef<unsigned>\":$sizePerThread,\n                      \"ArrayRef<unsigned>\":$order,\n-                     \"unsigned\":$numWarps), [{\n+                     \"unsigned\":$numWarps,\n+                     \"unsigned\":$threadsPerWarp), [{\n       int rank = sizePerThread.size();\n-      unsigned remainingLanes = 32;\n-      unsigned remainingThreads = numWarps*32;\n+      unsigned remainingLanes = threadsPerWarp;\n+      unsigned remainingThreads = numWarps*threadsPerWarp;\n       unsigned remainingWarps = numWarps;\n       unsigned prevLanes = 1;\n       unsigned prevWarps = 1;\n-      SmallVector<unsigned, 4> threadsPerWarp(rank);\n+      SmallVector<unsigned, 4> rankedThreadsPerWarp(rank);\n       SmallVector<unsigned, 4> warpsPerCTA(rank);\n       for (int _dim = 0; _dim < rank - 1; ++_dim) {\n         int i = order[_dim];\n         unsigned threadsPerCTA = std::clamp<unsigned>(remainingThreads, 1, shape[i] / sizePerThread[i]);\n-        threadsPerWarp[i] = std::clamp<unsigned>(threadsPerCTA, 1, remainingLanes);\n-        warpsPerCTA[i] = std::clamp<unsigned>(threadsPerCTA / threadsPerWarp[i], 1, remainingWarps);\n+        rankedThreadsPerWarp[i] = std::clamp<unsigned>(threadsPerCTA, 1, remainingLanes);\n+        warpsPerCTA[i] = std::clamp<unsigned>(threadsPerCTA / rankedThreadsPerWarp[i], 1, remainingWarps);\n         remainingWarps /= warpsPerCTA[i];\n-        remainingLanes /= threadsPerWarp[i];\n+        remainingLanes /= rankedThreadsPerWarp[i];\n         remainingThreads /= threadsPerCTA;\n-        prevLanes *= threadsPerWarp[i];\n+        prevLanes *= rankedThreadsPerWarp[i];\n         prevWarps *= warpsPerCTA[i];\n       }\n       // Expand the last dimension to fill the remaining lanes and warps\n-      threadsPerWarp[order[rank-1]] = 32 / prevLanes;\n+      rankedThreadsPerWarp[order[rank-1]] = threadsPerWarp / prevLanes;\n       warpsPerCTA[order[rank-1]] = numWarps / prevWarps;\n \n-      return $_get(context, sizePerThread, threadsPerWarp, warpsPerCTA, order);\n+      return $_get(context, sizePerThread, rankedThreadsPerWarp, warpsPerCTA, order);\n \n     }]>\n   ];\n@@ -273,6 +278,7 @@ for\n     // ArrayRefParameter<\"unsigned\">:$sizePerCTA\n   );\n \n+  let hasCustomAssemblyFormat = 1;\n }\n \n //===----------------------------------------------------------------------===//\n@@ -339,7 +345,7 @@ https://docs.nvidia.com/cuda/parallel-thread-execution/index.html\n (mma.16816 section, FP32 accumulator).\n \n For example, the matrix L corresponding to blockTileSize=[32,16] is:\n-                warp 0                          warp 1\n+                warp 0                          warp 2\n -----------------/\\-------------  ----------------/\\-------------\n [ 0   0   1   1   2   2   3   3   32  32  33  33  34  34  35  35\n [ 4   4   5   5   6   6   7   7   36  36  37  37  38  38  39  39\n@@ -350,7 +356,7 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n [ ..............................  ..............................\n [ 28  28  29  29  30  30  31  31  60  60  61  61  62  62  63  63\n \n-              warp 3                           warp 4\n+              warp 1                           warp 3\n ----------------/\\-------------   ----------------/\\-------------\n [ 64  64  65  65  66  66  67  67  96  96  97  97  98  98  99  99\n [ 68  68  69  69  70  70  71  71  100 100 101 101 102 102 103 103\n@@ -371,44 +377,63 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n   );\n \n   let builders = [\n-     // Specially for MMAV1(Volta)\n+    // Specially for MMAV1(Volta)\n     AttrBuilder<(ins \"int\":$versionMajor,\n                      \"int\":$numWarps,\n+                     \"ArrayRef<int64_t>\":$shapeC,\n+                     \"bool\":$isARow,\n+                     \"bool\":$isBRow,\n+                     \"bool\":$isAVec4,\n+                     \"bool\":$isBVec4,\n                      \"int\":$id), [{\n       assert(versionMajor == 1 && \"This builder is specially for versionMajor==1\");\n-      SmallVector<unsigned> wpt({static_cast<unsigned>(numWarps), 1});\n-      int versionMinor = 0;\n+      // 4-bits to encode 4 booleans: [isARow, isBRow, isAVec4, isBVec4]\n+      int versionMinor = (isARow * (1<<0)) |\\\n+                         (isBRow * (1<<1)) |\\\n+                         (isAVec4 * (1<<2)) |\\\n+                         (isBVec4 * (1<<3));\n+\n+\n+      // TODO: Share code with\n+      // DotOpMmaV1ConversionHelper::AParam/BParam, since same code to compute the\n+      // rep,spw and fpw.\n+      SmallVector<unsigned> wpt({1, 1});\n+      SmallVector<unsigned> wpt_nm1;\n+\n+      SmallVector<int, 2> rep(2), spw(2);\n+      std::array<int, 3> fpw{{2, 2, 1}};\n+      int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+      rep[0] = 2 * packSize0;\n+      spw[0] = fpw[0] * 4 * rep[0];\n \n-      assert(id < (1<<numBitsToHoldMmaV1ID) && \"MMAv1 ID exceeds the maximum\");\n-      for (int i = 0; i < numBitsToHoldMmaV1ID; ++i)\n-        versionMinor |= static_cast<bool>((1<<i) & id) * (1<<(4+i));\n+      int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+      rep[1] = 2 * packSize1;\n+      spw[1] = fpw[1] * 4 * rep[1];\n+\n+      do {\n+        wpt_nm1 = wpt;\n+        if (wpt[0] * wpt[1] < numWarps)\n+          wpt[0] = std::clamp<int>(wpt[0] * 2, 1, shapeC[0] / spw[0]);\n+        if (wpt[0] * wpt[1] < numWarps)\n+          wpt[1] = std::clamp<int>(wpt[1] * 2, 1, shapeC[1] / spw[1]);\n+      } while (wpt_nm1 != wpt);\n \n       return $_get(context, versionMajor, versionMinor, wpt);\n     }]>,\n \n-    // Specially for MMAV1(Volta)\n+\n     AttrBuilder<(ins \"int\":$versionMajor,\n-                     \"ArrayRef<unsigned>\":$warpsPerCTA,\n+                     \"int\":$numWarps,\n                      \"ArrayRef<int64_t>\":$shapeA,\n                      \"ArrayRef<int64_t>\":$shapeB,\n+                     \"ArrayRef<int64_t>\":$shapeC,\n                      \"bool\":$isARow,\n                      \"bool\":$isBRow,\n                      \"int\":$id), [{\n       assert(versionMajor == 1 && \"This builder is specially for versionMajor==1\");\n       bool isAVec4 = !isARow && (shapeA[isARow] <= 16);\n       bool isBVec4 = isBRow && (shapeB[isBRow] <= 16);\n-      // 4-bits to encode 4 booleans: [isARow, isBRow, isAVec4, isBVec4]\n-      // 3-bits to encode the MMA ID to make each unique\n-      int versionMinor = (isARow * (1<<0)) |\\\n-                         (isBRow * (1<<1)) |\\\n-                         (isAVec4 * (1<<2)) |\\\n-                         (isBVec4 * (1<<3));\n-\n-      assert(id < (1<<numBitsToHoldMmaV1ID) && \"MMAv1 ID exceeds the maximum\");\n-      for (int i = 0; i < numBitsToHoldMmaV1ID; ++i)\n-        versionMinor |= static_cast<bool>((1<<i) & id) * (1<<(4+i));\n-\n-      return $_get(context, versionMajor, versionMinor, warpsPerCTA);\n+      return get(context, versionMajor, numWarps, shapeC, isARow, isBRow, isAVec4, isBVec4, id);\n     }]>\n   ];\n \n@@ -422,6 +447,7 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n     static constexpr int numBitsToHoldMmaV1ID{5};\n   }];\n \n+  let hasCustomAssemblyFormat = 1;\n }\n \n def SliceEncodingAttr : DistributedEncoding<\"SliceEncoding\"> {\n@@ -456,6 +482,8 @@ def SliceEncodingAttr : DistributedEncoding<\"SliceEncoding\"> {\n     template<class T>\n     SmallVector<T> paddedShape(ArrayRef<T> shape) const;\n   }];\n+\n+  let hasCustomAssemblyFormat = 1;\n }\n \n def DotOperandEncodingAttr : DistributedEncoding<\"DotOperandEncoding\"> {\n@@ -476,23 +504,36 @@ section 9.7.13.4.1 for more details.\n     ins\n     \"unsigned\":$opIdx,\n     \"Attribute\":$parent,\n-    \"Attribute\":$isMMAv1Row\n+    \"unsigned\":$MMAv2kWidth\n   );\n \n   let builders = [\n+        // Specially for MMAV1(Volta)\n     AttrBuilder<(ins \"unsigned\":$opIdx,\n-                     \"Attribute\":$parent), [{\n-      Attribute isMMAv1Row;\n-      if(parent.isa<MmaEncodingAttr>() &&\n-         parent.cast<MmaEncodingAttr>().isVolta()){\n-        isMMAv1Row = BoolAttr::get(context, true);\n-      }\n-      return $_get(context, opIdx, parent, isMMAv1Row);\n+                     \"Attribute\":$parent,\n+                     \"Type\":$eltTy), [{\n+      MmaEncodingAttr parentAttr = parent.dyn_cast<MmaEncodingAttr>();\n+      if (!parentAttr || !parentAttr.isAmpere())\n+        return $_get(context, opIdx, parent, 0);\n+      unsigned bitwidth = eltTy.getIntOrFloatBitWidth();\n+      unsigned MMAv2kWidth = 32 / bitwidth;\n+      return $_get(context, opIdx, parent, MMAv2kWidth);\n     }]>\n-\n   ];\n \n-  let extraClassDeclaration = extraBaseClassDeclaration;\n+  let hasCustomAssemblyFormat = 1;\n+  let extraClassDeclaration = extraBaseClassDeclaration # [{\n+    bool getMMAv1IsRow() const;\n+    bool getMMAv1IsVec4() const;\n+    SmallVector<int> getMMAv1Rep() const;\n+    SmallVector<int> getMMAv1ShapePerWarp() const;\n+    int getMMAv1Vec() const;\n+    int getMMAv1NumOuter(ArrayRef<int64_t> shape) const;\n+    //\n+    SmallVector<int64_t> getMMAv2Rep(ArrayRef<int64_t> shape,\n+                                     int bitwidth) const;\n+\n+  }];\n }\n \n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUDialect.td", "status": "modified", "additions": 16, "deletions": 4, "changes": 20, "file_content_changes": "@@ -22,15 +22,27 @@ def TritonGPU_Dialect : Dialect {\n \n   let extraClassDeclaration = [{\n     static std::string getNumWarpsAttrName() { return \"triton_gpu.num-warps\"; }\n-    static int getNumWarps(ModuleOp mod) { \n-      if(!mod->hasAttr(\"triton_gpu.num-warps\"))\n+    static int getNumWarps(ModuleOp mod) {\n+      Attribute numWarps = mod->getDiscardableAttr(\"triton_gpu.num-warps\");\n+      if(!numWarps)\n         llvm::report_fatal_error(\n             \"TritonGPU module should contain a triton_gpu.num-warps attribute\");\n-      return mod->getAttr(\"triton_gpu.num-warps\").cast<IntegerAttr>().getInt();\n+      return numWarps.cast<IntegerAttr>().getInt();\n     }\n+\n+    static std::string getThreadsPerWarpAttrName() { return \"triton_gpu.threads-per-warp\"; }\n+    static int getThreadsPerWarp(ModuleOp mod) {\n+      Attribute threadsPerWarp = mod->getDiscardableAttr(\"triton_gpu.threads-per-warp\");\n+      if(!threadsPerWarp) {\n+        return 32;\n+      }\n+      return threadsPerWarp.cast<IntegerAttr>().getInt();\n+    }\n+\n   }];\n-  \n \n+  let useDefaultAttributePrinterParser = 1;\n+  let usePropertiesForAttributes = 1;\n }\n \n #endif"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 78, "deletions": 14, "changes": 92, "file_content_changes": "@@ -3,12 +3,14 @@\n \n include \"triton/Dialect/TritonGPU/IR/TritonGPUDialect.td\"\n include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td\"\n-include \"mlir/Dialect/Arithmetic/IR/ArithmeticBase.td\"\n+include \"mlir/Dialect/Arith/IR/ArithBase.td\"\n include \"triton/Dialect/Triton/IR/TritonTypes.td\"\n include \"triton/Dialect/Triton/IR/TritonAttrDefs.td\"\n include \"mlir/IR/OpBase.td\"\n-include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n+include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n+include \"mlir/Interfaces/DestinationStyleOpInterface.td\"\n+include \"mlir/Interfaces/ViewLikeInterface.td\"\n \n def ResultsAreSharedEncoding: NativeOpTrait<\"ResultsAreSharedEncoding\">;\n \n@@ -18,13 +20,15 @@ class TTG_Op<string mnemonic, list<Trait> traits = []> :\n def TTG_ConvertLayoutOp : TTG_Op<\"convert_layout\",\n                                  [SameOperandsAndResultShape,\n                                   SameOperandsAndResultElementType,\n-                                  NoSideEffect]> {\n+                                  Pure]> {\n   let summary = \"convert layout\";\n \n   let arguments = (ins TT_Tensor:$src);\n \n   let results = (outs TT_Tensor:$result);\n \n+  let hasCanonicalizeMethod = 1;\n+\n   let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n }\n \n@@ -59,8 +63,8 @@ def TTG_AsyncCommitGroupOp : TTG_Op<\"async_commit_group\"> {\n // This is needed because these ops don't\n // handle encodings\n // e.g., https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Arith/IR/ArithOps.td#L111\n-def TTG_CmpIOp : TTG_Op<\"cmpi\", [NoSideEffect, Elementwise, \n-                                 SameOperandsAndResultShape, \n+def TTG_CmpIOp : TTG_Op<\"cmpi\", [Pure, Elementwise,\n+                                 SameOperandsAndResultShape,\n                                  SameOperandsAndResultEncoding]> {\n   let summary = \"integer comparison operation\";\n \n@@ -73,8 +77,8 @@ def TTG_CmpIOp : TTG_Op<\"cmpi\", [NoSideEffect, Elementwise,\n   let results = (outs TT_BoolLike:$result);\n }\n \n-def TTG_CmpFOp : TTG_Op<\"cmpf\", [NoSideEffect, Elementwise, \n-                                 SameOperandsAndResultShape, \n+def TTG_CmpFOp : TTG_Op<\"cmpf\", [Pure, Elementwise,\n+                                 SameOperandsAndResultShape,\n                                  SameOperandsAndResultEncoding]> {\n   let summary = \"floating-point comparison operation\";\n \n@@ -88,8 +92,8 @@ def TTG_CmpFOp : TTG_Op<\"cmpf\", [NoSideEffect, Elementwise,\n }\n \n // TODO: migrate to arith::SelectOp on LLVM16\n-def TTG_SelectOp : TTG_Op<\"select\", [NoSideEffect, Elementwise, \n-                                     SameOperandsAndResultShape, \n+def TTG_SelectOp : TTG_Op<\"select\", [Pure, Elementwise,\n+                                     SameOperandsAndResultShape,\n                                      SameOperandsAndResultEncoding]> {\n   let summary = \"select operation\";\n \n@@ -99,9 +103,72 @@ def TTG_SelectOp : TTG_Op<\"select\", [NoSideEffect, Elementwise,\n                        TT_Tensor:$true_value,\n                        TT_Tensor:$false_value);\n \n-  let results = (outs TT_Tensor:$result);\n+  let results = (outs TT_Type:$result);\n+}\n+\n+\n+\n+def TTG_ExtractSliceOp : TTG_Op<\"extract_slice\",\n+                                [AttrSizedOperandSegments,\n+                                 ResultsAreSharedEncoding,\n+                                 Pure,\n+                                 OffsetSizeAndStrideOpInterface\n+                                 ]> {\n+  let summary = \"extract slice operation\";\n+  let description = [{\n+    same as tensor.extract_slice, but with int32 index. The motivations for re-implementing it are:\n+    We reimplement ExtractSliceOp with int32 index, because:\n+    - we want to enforce int32 indexing on GPUs since Triton tensors fit in SRAM\n+    - we still want to use indexWidth = 64 when lowering to LLVM because our loops can have\n+      64-bit induction variables and scf.for uses indexType for bounds/ivs\n+  }];\n+\n+  let arguments = (ins\n+    AnyRankedTensor:$source,\n+    Variadic<I32>:$offsets,\n+    Variadic<I32>:$sizes,\n+    Variadic<I32>:$strides,\n+    DenseI64ArrayAttr:$static_offsets,\n+    DenseI64ArrayAttr:$static_sizes,\n+    DenseI64ArrayAttr:$static_strides\n+  );\n+  let results = (outs AnyRankedTensor:$result);\n+\n+  let builders = [\n+    // Build an ExtractSliceOp with mixed static and dynamic entries and custom\n+    // result type. If the type passed is nullptr, it is inferred.\n+    OpBuilder<(ins \"RankedTensorType\":$resultType, \"Value\":$source,\n+      \"ArrayRef<OpFoldResult>\":$offsets, \"ArrayRef<OpFoldResult>\":$sizes,\n+      \"ArrayRef<OpFoldResult>\":$strides,\n+      CArg<\"ArrayRef<NamedAttribute>\", \"{}\">:$attrs)>,\n+  ];\n+\n+  let extraClassDeclaration = [{\n+    /// Return the number of leading operands before the `offsets`, `sizes` and\n+    /// and `strides` operands.\n+    static unsigned getOffsetSizeAndStrideStartOperandIndex() { return 1; }\n+\n+    /// Returns the type of the base tensor operand.\n+    RankedTensorType getSourceType() {\n+      return getSource().getType().cast<RankedTensorType>();\n+    }\n+\n+    std::array<unsigned, 3> getArrayAttrMaxRanks() {\n+      unsigned rank = getSourceType().getRank();\n+      return {rank, rank, rank};\n+    }\n+  }];\n+\n+  let assemblyFormat = [{\n+    $source ``\n+    custom<DynamicIndexList>($offsets, $static_offsets)\n+    custom<DynamicIndexList>($sizes, $static_sizes)\n+    custom<DynamicIndexList>($strides, $static_strides)\n+    attr-dict `:` type($source) `to` type($result)\n+  }];\n }\n \n+//\n \n def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n                                     [AttrSizedOperandSegments,\n@@ -188,10 +255,7 @@ def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n     }\n   }];\n \n-  // The custom parser could be replaced with oilist in LLVM-16\n-  let parser = [{ return parseInsertSliceAsyncOp(parser, result); }];\n-\n-  let printer = [{ return printInsertSliceAsyncOp(p, *this); }];\n+  let hasCustomAssemblyFormat = 1;\n }\n \n def TTG_AllocTensorOp : TTG_Op<\"alloc_tensor\", [MemoryEffects<[MemAlloc]>,  // Allocate shared memory"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.h", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -6,7 +6,9 @@\n namespace mlir {\n std::unique_ptr<Pass> createTritonGPUPipelinePass(int numStages = 2);\n \n-// TODO(Keren): prefetch pass not working yet\n+std::unique_ptr<Pass>\n+createTritonGPUAccelerateMatmulPass(int computeCapability = 80);\n+\n std::unique_ptr<Pass> createTritonGPUPrefetchPass();\n \n std::unique_ptr<Pass> createTritonGPUCanonicalizeLoopsPass();\n@@ -17,11 +19,11 @@ std::unique_ptr<Pass> createTritonGPUReorderInstructionsPass();\n \n std::unique_ptr<Pass> createTritonGPUDecomposeConversionsPass();\n \n-std::unique_ptr<Pass> createTritonGPUCombineOpsPass(int computeCapability = 80);\n+std::unique_ptr<Pass> createTritonGPURemoveLayoutConversionsPass();\n \n std::unique_ptr<Pass> createTritonGPUVerifier();\n \n-std::unique_ptr<Pass> createTritonGPUUpdateMmaForVoltaPass();\n+std::unique_ptr<Pass> createTritonGPUOptimizeDotOperandsPass();\n \n /// Generate the code for registering passes.\n #define GEN_PASS_REGISTRATION"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 44, "deletions": 42, "changes": 86, "file_content_changes": "@@ -7,14 +7,15 @@ def TritonGPUPipeline : Pass<\"tritongpu-pipeline\", \"mlir::ModuleOp\"> {\n   let summary = \"pipeline\";\n \n   let description = [{\n-    Unroll loops to hide global memory -> shared memory latency.\n+    Replace `LoadOp` in loops by `InsertSliceAsyncOp` instructions that asynchronously construct the data\n+    needed at the next iteration\n   }];\n \n   let constructor = \"mlir::createTritonGPUPipelinePass()\";\n \n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n                            \"mlir::scf::SCFDialect\",\n-                           \"mlir::arith::ArithmeticDialect\"];\n+                           \"mlir::arith::ArithDialect\"];\n \n   let options = [\n     Option<\"numStages\", \"num-stages\",\n@@ -27,14 +28,49 @@ def TritonGPUPrefetch : Pass<\"tritongpu-prefetch\", \"mlir::ModuleOp\"> {\n   let summary = \"prefetch\";\n \n   let description = [{\n-    Prefetch operands (a and b) of tt.dot into shared memory to hide shared memory -> register latency.\n+    Decompose `DotOp` instructions in loops into several finer-grained `DotOp`\n+    that may have their operands constructed at the end of the previous iteration\n   }];\n \n   let constructor = \"mlir::createTritonGPUPrefetchPass()\";\n \n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n                            \"mlir::scf::SCFDialect\",\n-                           \"mlir::arith::ArithmeticDialect\"];\n+                           \"mlir::arith::ArithDialect\"];\n+}\n+\n+def TritonGPUAccelerateMatmul : Pass<\"tritongpu-accelerate-matmul\", \"mlir::ModuleOp\"> {\n+  let summary = \"accelerate matmul\";\n+\n+  let description = [{\n+    Optimize the input/output layout of `dot` instruction to make them compatible hardware accelerators\n+    (e.g., Nvidia tensor cores)\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPUAccelerateMatmulPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::TritonDialect\"];\n+\n+  let options = [\n+    Option<\"computeCapability\", \"compute-capability\",\n+           \"int32_t\", /*default*/\"80\",\n+           \"device compute capability\">\n+  ];\n+}\n+\n+def TritonGPUOptimizeDotOperands : Pass<\"tritongpu-optimize-dot-operands\", \"mlir::ModuleOp\"> {\n+  let summary = \"fuse transpositions\";\n+\n+  let description = [{\n+    Re-arranged layouts of tensors used as matrix multiplication operands so as to promote the use of\n+    hardware-accelerated transpositions.\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPUOptimizeDotOperandsPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::TritonDialect\"];\n }\n \n def TritonGPUCoalesce: Pass<\"tritongpu-coalesce\", \"mlir::ModuleOp\"> {\n@@ -49,26 +85,17 @@ def TritonGPUCoalesce: Pass<\"tritongpu-coalesce\", \"mlir::ModuleOp\"> {\n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n }\n \n-def TritonGPUCombineOps : Pass<\"tritongpu-combine\", \"mlir::ModuleOp\"> {\n-  let summary = \"combine triton gpu ops\";\n \n-  let description = [{\n-    convert_layout(convert_layout(%src, #LAYOUT_0), #LAYOUT_1) =>\n-      convert_layout(%src, #LAYOUT_1)\n+def TritonGPURemoveLayoutConversions : Pass<\"tritongpu-remove-layout-conversions\", \"mlir::ModuleOp\"> {\n+  let summary = \"remove superfluous layout conversions\";\n \n-    convert_layout(%src, #LAYOUT) => %src if %src.layout() == #LAYOUT\n+  let description = [{\n   }];\n \n-  let constructor = \"mlir::createTritonGPUCombineOpsPass()\";\n+  let constructor = \"mlir::createTritonGPURemoveLayoutConversionsPass()\";\n \n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n                            \"mlir::triton::TritonDialect\"];\n-\n-  let options = [\n-    Option<\"computeCapability\", \"compute-capability\",\n-           \"int32_t\", /*default*/\"80\",\n-           \"device compute capability\">\n-  ];\n }\n \n def TritonGPUReorderInstructions: Pass<\"tritongpu-reorder-instructions\", \"mlir::ModuleOp\"> {\n@@ -95,29 +122,4 @@ def TritonGPUDecomposeConversions: Pass<\"tritongpu-decompose-conversions\", \"mlir\n                            \"mlir::triton::TritonDialect\"];\n }\n \n-def TritonGPUCanonicalizeLoops: Pass<\"tritongpu-canonicalize-loops\", \"mlir::ModuleOp\"> {\n-  let summary = \"canonicalize scf.ForOp ops\";\n-\n-  let description = [{\n-    This implements some optimizations that are missing in the standard scf.ForOp\n-    canonicalizer.\n-  }];\n-\n-  let constructor = \"mlir::createTritonGPUCanonicalizeLoopsPass()\";\n-\n-  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n-}\n-\n-def UpdateMmaForVolta : Pass<\"tritongpu-update-mma-for-volta\", \"mlir::ModuleOp\"> {\n-  let summary = \"Update mma encodings for Volta\";\n-\n-  let description = [{\n-    This helps to update the mma encodings for Volta.\n-  }];\n-\n-  let constructor = \"mlir::createTritonGPUUpdateMmaForVoltaPass()\";\n-\n-  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n-}\n-\n #endif"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -13,12 +13,15 @@ namespace mlir {\n \n class TritonGPUTypeConverter : public TypeConverter {\n public:\n-  TritonGPUTypeConverter(MLIRContext *context, int numWarps);\n+  TritonGPUTypeConverter(MLIRContext *context, int numWarps,\n+                         int threadsPerWarp);\n   int getNumWarps() const { return numWarps; }\n+  int getThreadsPerWarp() const { return threadsPerWarp; }\n \n private:\n   MLIRContext *context;\n   int numWarps;\n+  int threadsPerWarp;\n };\n \n class TritonGPUConversionTarget : public ConversionTarget {"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "added", "additions": 45, "deletions": 0, "changes": 45, "file_content_changes": "@@ -0,0 +1,45 @@\n+#ifndef TRITON_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#define TRITON_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"llvm/ADT/MapVector.h\"\n+\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+\n+namespace mlir {\n+\n+LogicalResult fixupLoops(ModuleOp mod);\n+\n+// TODO: Interface\n+LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n+                             Attribute &ret);\n+\n+bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n+\n+bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding);\n+\n+bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding);\n+\n+// skipInit is True when we only consider the operands of the initOp but\n+// not the initOp itself.\n+int simulateBackwardRematerialization(\n+    Operation *initOp, SetVector<Operation *> &processed,\n+    SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n+    Attribute targetEncoding);\n+\n+Operation *cloneWithInferType(mlir::OpBuilder &rewriter, Operation *op,\n+                              IRMapping &mapping);\n+\n+void rematerializeConversionChain(\n+    const llvm::MapVector<Value, Attribute> &toConvert,\n+    mlir::PatternRewriter &rewriter, SetVector<Operation *> &processed,\n+    IRMapping &mapping);\n+\n+LogicalResult canMoveOutOfLoop(BlockArgument arg,\n+                               SmallVector<Operation *> &cvts);\n+\n+} // namespace mlir\n+\n+#endif // TRITON_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "include/triton/Target/HSACO/HSACOTranslation.h", "status": "added", "additions": 21, "deletions": 0, "changes": 21, "file_content_changes": "@@ -0,0 +1,21 @@\n+#ifndef TRITON_TARGET_HSACOTRANSLATION_H\n+#define TRITON_TARGET_HSACOTRANSLATION_H\n+\n+#include <memory>\n+#include <string>\n+#include <tuple>\n+\n+namespace llvm {\n+class Module;\n+} // namespace llvm\n+\n+namespace triton {\n+\n+// Translate TritonGPU IR to HSACO code.\n+std::tuple<std::string, std::string>\n+translateLLVMIRToHSACO(llvm::Module &module, std::string gfx_arch,\n+                       std::string gfx_triple, std::string gfx_features);\n+\n+} // namespace triton\n+\n+#endif"}, {"filename": "include/triton/Target/LLVMIR/LLVMIRTranslation.h", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -25,11 +25,13 @@ void addExternalLibs(mlir::ModuleOp &module,\n // Translate TritonGPU dialect to LLVMIR, return null if failed.\n std::unique_ptr<llvm::Module>\n translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n-                           mlir::ModuleOp module, int computeCapability);\n+                           mlir::ModuleOp module, int computeCapability,\n+                           bool isROCM);\n \n // Translate mlir LLVM dialect to LLVMIR, return null if failed.\n std::unique_ptr<llvm::Module>\n-translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module);\n+translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n+                      bool isROCM);\n \n } // namespace triton\n } // namespace mlir"}, {"filename": "include/triton/Tools/Sys/GetPlatform.hpp", "status": "added", "additions": 36, "deletions": 0, "changes": 36, "file_content_changes": "@@ -0,0 +1,36 @@\n+/*\n+ * Copyright (c) 2015, PHILIPPE TILLET. All rights reserved.\n+ *\n+ * This file is part of ISAAC.\n+ *\n+ * ISAAC is free software; you can redistribute it and/or\n+ * modify it under the terms of the GNU Lesser General Public\n+ * License as published by the Free Software Foundation; either\n+ * version 2.1 of the License, or (at your option) any later version.\n+ *\n+ * This library is distributed in the hope that it will be useful,\n+ * but WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+ * Lesser General Public License for more details.\n+ *\n+ * You should have received a copy of the GNU Lesser General Public\n+ * License along with this library; if not, write to the Free Software\n+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston,\n+ * MA 02110-1301  USA\n+ */\n+\n+#ifndef TDL_TOOLS_SYS_GETPLATFORM_HPP\n+#define TDL_TOOLS_SYS_GETPLATFORM_HPP\n+\n+#include <algorithm>\n+#include <cstdlib>\n+#include <iostream>\n+#include <map>\n+#include <memory>\n+#include <string>\n+\n+// inline bool _isROCM = false;\n+// inline void setROCM() { _isROCM = true; }\n+// inline bool isROCM() { return _isROCM; }\n+\n+#endif"}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 8, "deletions": 10, "changes": 18, "file_content_changes": "@@ -18,15 +18,16 @@ AliasInfo AliasInfo::join(const AliasInfo &lhs, const AliasInfo &rhs) {\n   return ret;\n }\n \n-ChangeResult SharedMemoryAliasAnalysis::visitOperation(\n-    Operation *op, ArrayRef<LatticeElement<AliasInfo> *> operands) {\n+void SharedMemoryAliasAnalysis::visitOperation(\n+    Operation *op, ArrayRef<const dataflow::Lattice<AliasInfo> *> operands,\n+    ArrayRef<dataflow::Lattice<AliasInfo> *> results) {\n   AliasInfo aliasInfo;\n   bool pessimistic = true;\n   if (maybeSharedAllocationOp(op)) {\n     // These ops may allocate a new shared memory buffer.\n     auto result = op->getResult(0);\n     // XXX(Keren): the following ops are always aliasing for now\n-    if (isa<tensor::ExtractSliceOp, triton::TransOp>(op)) {\n+    if (isa<triton::gpu::ExtractSliceOp, triton::TransOp>(op)) {\n       // extract_slice %src\n       // trans %src\n       aliasInfo = AliasInfo(operands[0]->getValue());\n@@ -37,21 +38,18 @@ ChangeResult SharedMemoryAliasAnalysis::visitOperation(\n       // insert_slice %src into %dst[%offsets]\n       aliasInfo = AliasInfo(operands[1]->getValue());\n       pessimistic = false;\n-    } else if (isSharedEncoding(result)) {\n+    } else if (triton::gpu::isSharedEncoding(result)) {\n       aliasInfo.insert(result);\n       pessimistic = false;\n     }\n   }\n \n   if (pessimistic) {\n-    return markAllPessimisticFixpoint(op->getResults());\n+    return setAllToEntryStates(results);\n   }\n   // Join all lattice elements\n-  ChangeResult result = ChangeResult::NoChange;\n-  for (Value value : op->getResults()) {\n-    result |= getLatticeElement(value).join(aliasInfo);\n-  }\n-  return result;\n+  for (auto *result : results)\n+    propagateIfChanged(result, result->join(aliasInfo));\n }\n \n AliasResult SharedMemoryAliasAnalysis::alias(Value lhs, Value rhs) {"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 61, "deletions": 35, "changes": 96, "file_content_changes": "@@ -1,9 +1,9 @@\n #include \"triton/Analysis/Allocation.h\"\n+#include \"mlir/Analysis/DataFlowFramework.h\"\n #include \"mlir/Analysis/Liveness.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"triton/Analysis/Alias.h\"\n-#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"llvm/ADT/SmallVector.h\"\n \n@@ -32,11 +32,9 @@ namespace triton {\n constexpr int kPtrBitWidth = 64;\n \n static std::pair<SmallVector<unsigned>, SmallVector<unsigned>>\n-getCvtOrder(const Attribute &srcLayout, const Attribute &dstLayout) {\n-  auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>();\n+getCvtOrder(Attribute srcLayout, Attribute dstLayout) {\n   auto srcMmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n   auto srcDotLayout = srcLayout.dyn_cast<DotOperandEncodingAttr>();\n-  auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>();\n   auto dstMmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>();\n   auto dstDotLayout = dstLayout.dyn_cast<DotOperandEncodingAttr>();\n   assert(!(srcMmaLayout && dstMmaLayout) &&\n@@ -54,19 +52,19 @@ getCvtOrder(const Attribute &srcLayout, const Attribute &dstLayout) {\n SmallVector<unsigned>\n getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n                              unsigned &outVec) {\n-  auto srcTy = op.src().getType().cast<RankedTensorType>();\n-  auto dstTy = op.result().getType().cast<RankedTensorType>();\n+  auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n+  auto dstTy = op.getResult().getType().cast<RankedTensorType>();\n   Attribute srcLayout = srcTy.getEncoding();\n   Attribute dstLayout = dstTy.getEncoding();\n \n   // MmaToDotShortcut doesn't use shared mem\n-  if (auto mmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>())\n-    if (auto dotOperandLayout = dstLayout.dyn_cast<DotOperandEncodingAttr>())\n-      if (isMmaToDotShortcut(mmaLayout, dotOperandLayout))\n-        return {};\n+  if (srcLayout.isa<MmaEncodingAttr>() &&\n+      dstLayout.isa<DotOperandEncodingAttr>())\n+    if (isMmaToDotShortcut(srcTy, dstTy))\n+      return {};\n \n   assert(srcLayout && dstLayout &&\n-         \"Unexpect layout in getScratchConfigForCvtLayout()\");\n+         \"Unexpected layout in getScratchConfigForCvtLayout()\");\n   auto [inOrd, outOrd] = getCvtOrder(srcLayout, dstLayout);\n   unsigned srcContigPerThread = getContigPerThread(srcLayout)[inOrd[0]];\n   unsigned dstContigPerThread = getContigPerThread(dstLayout)[outOrd[0]];\n@@ -101,7 +99,7 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n // TODO: extend beyond scalars\n SmallVector<unsigned> getScratchConfigForAtomicRMW(triton::AtomicRMWOp op) {\n   SmallVector<unsigned> smemShape;\n-  if (op.ptr().getType().isa<RankedTensorType>()) {\n+  if (op.getPtr().getType().isa<RankedTensorType>()) {\n     // do nothing or just assert because shared memory is not used in tensor up\n     // to now\n   } else {\n@@ -118,8 +116,11 @@ SmallVector<unsigned> getScratchConfigForAtomicCAS(triton::AtomicCASOp op) {\n \n class AllocationAnalysis {\n public:\n-  AllocationAnalysis(Operation *operation, Allocation *allocation)\n-      : operation(operation), allocation(allocation) {\n+  AllocationAnalysis(Operation *operation,\n+                     Allocation::FuncAllocMapT *funcAllocMap,\n+                     Allocation *allocation)\n+      : operation(operation), funcAllocMap(funcAllocMap),\n+        allocation(allocation) {\n     run();\n   }\n \n@@ -150,7 +151,7 @@ class AllocationAnalysis {\n     }\n \n     for (Value result : op->getResults()) {\n-      if (isSharedEncoding(result)) {\n+      if (triton::gpu::isSharedEncoding(result)) {\n         // Bytes could be a different value once we support padding or other\n         // allocation policies.\n         auto tensorType = result.getType().dyn_cast<RankedTensorType>();\n@@ -168,8 +169,8 @@ class AllocationAnalysis {\n       unsigned bytes = helper.getScratchSizeInBytes();\n       allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n-      auto srcTy = cvtLayout.src().getType().cast<RankedTensorType>();\n-      auto dstTy = cvtLayout.result().getType().cast<RankedTensorType>();\n+      auto srcTy = cvtLayout.getSrc().getType().cast<RankedTensorType>();\n+      auto dstTy = cvtLayout.getResult().getType().cast<RankedTensorType>();\n       auto srcEncoding = srcTy.getEncoding();\n       auto dstEncoding = dstTy.getEncoding();\n       if (srcEncoding.isa<SharedEncodingAttr>() ||\n@@ -220,14 +221,20 @@ class AllocationAnalysis {\n                        ? elems * kPtrBitWidth / 8\n                        : elems * elemTy.getIntOrFloatBitWidth() / 8;\n       allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+    } else if (auto callOp = dyn_cast<CallOpInterface>(op)) {\n+      auto callable = callOp.resolveCallable();\n+      auto funcOp = dyn_cast<FunctionOpInterface>(callable);\n+      auto *funcAlloc = &(*funcAllocMap)[funcOp];\n+      auto bytes = funcAlloc->getSharedMemorySize();\n+      allocation->addBuffer<BufferT::BufferKind::Virtual>(op, bytes);\n     }\n   }\n \n   void getValueAlias(Value value, SharedMemoryAliasAnalysis &analysis) {\n-    LatticeElement<AliasInfo> *latticeElement =\n-        analysis.lookupLatticeElement(value);\n+    dataflow::Lattice<AliasInfo> *latticeElement =\n+        analysis.getLatticeElement(value);\n     if (latticeElement) {\n-      auto &info = latticeElement->getValue();\n+      AliasInfo &info = latticeElement->getValue();\n       if (!info.getAllocs().empty()) {\n         for (auto alloc : info.getAllocs()) {\n           allocation->addAlias(value, alloc);\n@@ -244,14 +251,19 @@ class AllocationAnalysis {\n       getScratchValueSize(op);\n     });\n     // Get the alias values\n-    SharedMemoryAliasAnalysis aliasAnalysis(operation->getContext());\n-    aliasAnalysis.run(operation);\n+    std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n+    SharedMemoryAliasAnalysis *aliasAnalysis =\n+        solver->load<SharedMemoryAliasAnalysis>();\n+    if (failed(solver->initializeAndRun(operation))) {\n+      // TODO: return error instead of bailing out..\n+      llvm_unreachable(\"failed to run SharedMemoryAliasAnalysis\");\n+    }\n     operation->walk<WalkOrder::PreOrder>([&](Operation *op) {\n       for (auto operand : op->getOperands()) {\n-        getValueAlias(operand, aliasAnalysis);\n+        getValueAlias(operand, *aliasAnalysis);\n       }\n       for (auto value : op->getResults()) {\n-        getValueAlias(value, aliasAnalysis);\n+        getValueAlias(value, *aliasAnalysis);\n       }\n     });\n   }\n@@ -294,15 +306,19 @@ class AllocationAnalysis {\n   /// allocated, but is used to store intermediate results.\n   void resolveScratchBufferLiveness(\n       const DenseMap<Operation *, size_t> &operationId) {\n-    // Analyze liveness of scratch buffers\n-    for (auto opScratchIter : allocation->opScratch) {\n-      // Any scratch memory's live range is the current operation's live\n-      // range.\n-      auto *op = opScratchIter.first;\n-      auto *buffer = opScratchIter.second;\n-      bufferRange.insert({buffer, Interval(operationId.lookup(op),\n-                                           operationId.lookup(op) + 1)});\n-    }\n+    // Analyze liveness of scratch buffers and vritual buffers.\n+    auto processScratchMemory = [&](const auto &container) {\n+      for (auto opScratchIter : container) {\n+        // Any scratch memory's live range is the current operation's live\n+        // range.\n+        auto *op = opScratchIter.first;\n+        auto *buffer = opScratchIter.second;\n+        bufferRange.insert({buffer, Interval(operationId.lookup(op),\n+                                             operationId.lookup(op) + 1)});\n+      }\n+    };\n+    processScratchMemory(allocation->opScratch);\n+    processScratchMemory(allocation->opVirtual);\n   }\n \n   /// Resolves liveness of all values involved under the root operation.\n@@ -385,7 +401,9 @@ class AllocationAnalysis {\n     //  | ******t1 ^^^^^^^^^v1^^^^^^^^^ ************t3\n     //  |---------------------------------------------| liveness range\n     //    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ...\n-    /// Start -> Liveness Range\n+    // If the available triple's range is less than a given buffer range,\n+    // we won't know if there has been an overlap without using graph coloring.\n+    // Start -> Liveness Range\n     using TripleMapT = std::multimap<size_t, Interval<size_t>>;\n     TripleMapT tripleMap;\n     tripleMap.insert(std::make_pair(0, Interval<size_t>()));\n@@ -411,6 +429,10 @@ class AllocationAnalysis {\n         tripleMap.insert(\n             {size + xSize, Interval{std::max(range.start(), xRange.start()),\n                                     std::min(range.end(), xRange.end())}});\n+        // We could either insert (range.start, xRange.start) or (range.start,\n+        // xRange.end), both are correct and determine the potential buffer\n+        // offset, and the graph coloring algorithm will solve the interference,\n+        // if any\n         if (range.start() < xRange.start())\n           tripleMap.insert({size, Interval{range.start(), xRange.end()}});\n         if (xRange.end() < range.end())\n@@ -489,11 +511,15 @@ class AllocationAnalysis {\n \n private:\n   Operation *operation;\n+  Allocation::FuncAllocMapT *funcAllocMap;\n   Allocation *allocation;\n   BufferRangeMapT bufferRange;\n };\n+\n } // namespace triton\n \n-void Allocation::run() { triton::AllocationAnalysis(getOperation(), this); }\n+void Allocation::run(FuncAllocMapT &funcAllocMap) {\n+  triton::AllocationAnalysis(getOperation(), &funcAllocMap, this);\n+}\n \n } // namespace mlir"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 878, "deletions": 186, "changes": 1064, "file_content_changes": "@@ -1,191 +1,458 @@\n-#include \"mlir/Analysis/DataFlowAnalysis.h\"\n+#include \"mlir/Analysis/DataFlowFramework.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"llvm/Support/raw_ostream.h\"\n-#include <iostream>\n \n #include \"triton/Analysis/AxisInfo.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n namespace mlir {\n \n-//===----------------------------------------------------------------------===//\n-// AxisInfo\n-//===----------------------------------------------------------------------===//\n-\n // Function for extended Euclidean Algorithm\n-static int gcd_impl(int a, int b, int *x, int *y) {\n+static int64_t gcdImpl(int64_t a, int64_t b, int64_t *x, int64_t *y) {\n   // Base Case\n   if (a == 0) {\n     *x = 0;\n     *y = 1;\n     return b;\n   }\n-  int x1, y1; // To store results of recursive call\n-  int gcd = gcd_impl(b % a, a, &x1, &y1);\n+  int64_t x1, y1; // To store results of recursive call\n+  int64_t gcd = gcdImpl(b % a, a, &x1, &y1);\n   // Update x and y using results of\n   // recursive call\n   *x = y1 - (b / a) * x1;\n   *y = x1;\n   return gcd;\n }\n \n-static int gcd(int a, int b) {\n-  int x, y;\n-  return gcd_impl(a, b, &x, &y);\n+static int64_t gcd(int64_t a, int64_t b) {\n+  if (a == 0)\n+    return b;\n+  if (b == 0)\n+    return a;\n+  int64_t x, y;\n+  return gcdImpl(a, b, &x, &y);\n+}\n+\n+static constexpr int log2Int(int64_t num) {\n+  return (num > 1) ? 1 + log2Int(num / 2) : 0;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// AxisInfo\n+//===----------------------------------------------------------------------===//\n+\n+template <class T>\n+void AxisInfo::initPessimisticStateFromFunc(int argNumber, T funcOp,\n+                                            DimVectorT *contiguity,\n+                                            DimVectorT *divisibility,\n+                                            DimVectorT *constancy) {\n+  // liast of attributes that we care about\n+  SmallVector<std::pair<DimVectorT *, std::string>> retVecs;\n+  retVecs.push_back({contiguity, \"tt.contiguity\"});\n+  retVecs.push_back({divisibility, \"tt.divisibility\"});\n+  retVecs.push_back({constancy, \"tt.constancy\"});\n+  // initialize attributes one by one\n+  for (auto [vec, attrName] : retVecs) {\n+    Attribute attr = funcOp.getArgAttr(argNumber, attrName);\n+    if (auto int_attr = attr.dyn_cast_or_null<IntegerAttr>())\n+      *vec = DimVectorT(contiguity->size(), int_attr.getValue().getZExtValue());\n+    if (auto dense_attr = attr.dyn_cast_or_null<DenseElementsAttr>()) {\n+      auto vals = dense_attr.getValues<int>();\n+      *vec = DimVectorT(vals.begin(), vals.end());\n+    }\n+  }\n }\n \n AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n-  size_t rank = 1;\n+  auto rank = 1;\n   if (TensorType ty = value.getType().dyn_cast<TensorType>())\n     rank = ty.getRank();\n-  int divHint = 1;\n+\n+  DimVectorT knownContiguity(rank, 1);\n+  DimVectorT knownDivisibility(rank, 1);\n+  DimVectorT knownConstancy(rank, 1);\n+\n   BlockArgument blockArg = value.dyn_cast<BlockArgument>();\n+\n   if (blockArg && blockArg.getOwner()->isEntryBlock()) {\n     Operation *op = blockArg.getOwner()->getParentOp();\n-    if (FuncOp fun = dyn_cast<FuncOp>(op)) {\n-      Attribute attr =\n-          fun.getArgAttr(blockArg.getArgNumber(), \"tt.divisibility\");\n-      if (attr)\n-        divHint = attr.cast<IntegerAttr>().getValue().getZExtValue();\n-    } else if (auto fun = dyn_cast<LLVM::LLVMFuncOp>(op)) {\n-      Attribute attr =\n-          fun.getArgAttr(blockArg.getArgNumber(), \"tt.divisibility\");\n-      if (attr)\n-        divHint = attr.cast<IntegerAttr>().getValue().getZExtValue();\n+    if (auto fun = dyn_cast<FunctionOpInterface>(op))\n+      initPessimisticStateFromFunc(blockArg.getArgNumber(), fun,\n+                                   &knownContiguity, &knownDivisibility,\n+                                   &knownConstancy);\n+    // llvm codegen check alignment to generate vector load/store\n+    // would be nice if this wasn't the case\n+    else if (auto fun = dyn_cast<LLVM::LLVMFuncOp>(op))\n+      initPessimisticStateFromFunc(blockArg.getArgNumber(), fun,\n+                                   &knownContiguity, &knownDivisibility,\n+                                   &knownConstancy);\n+    else {\n+      // Derive the divisibility of the induction variable only when\n+      // the step and the lower bound are both constants\n+      if (auto forOp = dyn_cast<scf::ForOp>(op)) {\n+        if (blockArg == forOp.getInductionVar()) {\n+          if (auto lowerBound =\n+                  forOp.getLowerBound().getDefiningOp<arith::ConstantOp>()) {\n+            if (auto step =\n+                    forOp.getStep().getDefiningOp<arith::ConstantOp>()) {\n+              auto lowerBoundVal = lowerBound.getValue()\n+                                       .cast<IntegerAttr>()\n+                                       .getValue()\n+                                       .getZExtValue();\n+              auto stepVal =\n+                  step.getValue().cast<IntegerAttr>().getValue().getZExtValue();\n+              auto k = gcd(lowerBoundVal, stepVal);\n+              if (k != 0)\n+                knownDivisibility = DimVectorT(rank, k);\n+            }\n+          }\n+        }\n+      }\n+    }\n+  } else if (Operation *op = value.getDefiningOp()) {\n+    if (Attribute attr = op->getDiscardableAttr(\"tt.divisibility\")) {\n+      auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+      knownDivisibility = DimVectorT(vals.begin(), vals.end());\n+    }\n+    if (Attribute attr = op->getDiscardableAttr(\"tt.contiguity\")) {\n+      auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+      knownContiguity = DimVectorT(vals.begin(), vals.end());\n+    }\n+    if (Attribute attr = op->getDiscardableAttr(\"tt.constancy\")) {\n+      auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+      knownConstancy = DimVectorT(vals.begin(), vals.end());\n     }\n   }\n-  DimVectorT contiguity(rank, 1);\n-  DimVectorT divisibility(rank, divHint);\n-  DimVectorT constancy(rank, 1);\n-  return AxisInfo(contiguity, divisibility, constancy);\n+\n+  return AxisInfo(knownContiguity, knownDivisibility, knownConstancy);\n }\n \n // The gcd of both arguments for each dimension\n AxisInfo AxisInfo::join(const AxisInfo &lhs, const AxisInfo &rhs) {\n-  DimVectorT retContiguity;\n-  DimVectorT retDivisibility;\n-  DimVectorT retConstancy;\n-  for (int d = 0; d < lhs.getRank(); ++d) {\n-    retContiguity.push_back(gcd(lhs.getContiguity(d), rhs.getContiguity(d)));\n-    retDivisibility.push_back(\n-        gcd(lhs.getDivisibility(d), rhs.getDivisibility(d)));\n-    retConstancy.push_back(gcd(lhs.getConstancy(d), rhs.getConstancy(d)));\n-  }\n-  return AxisInfo(retContiguity, retDivisibility, retConstancy);\n+  // If one argument is not initialized, return the other.\n+  if (lhs.getRank() == 0)\n+    return rhs;\n+  if (rhs.getRank() == 0)\n+    return lhs;\n+  DimVectorT contiguity;\n+  DimVectorT divisibility;\n+  DimVectorT constancy;\n+  for (auto d = 0; d < lhs.getRank(); ++d) {\n+    contiguity.push_back(gcd(lhs.getContiguity(d), rhs.getContiguity(d)));\n+    divisibility.push_back(gcd(lhs.getDivisibility(d), rhs.getDivisibility(d)));\n+    constancy.push_back(gcd(lhs.getConstancy(d), rhs.getConstancy(d)));\n+  }\n+  std::optional<int64_t> constantValue;\n+  if (lhs.getConstantValue().has_value() &&\n+      rhs.getConstantValue().has_value() &&\n+      lhs.getConstantValue() == rhs.getConstantValue())\n+    constantValue = lhs.getConstantValue();\n+  return AxisInfo(contiguity, divisibility, constancy, constantValue);\n }\n \n //===----------------------------------------------------------------------===//\n-// AxisInfoAnalysis\n+// AxisInfoVisitor\n //===----------------------------------------------------------------------===//\n \n-AxisInfo AxisInfoAnalysis::visitBinaryOp(\n-    Operation *op, AxisInfo lhsInfo, AxisInfo rhsInfo,\n-    const std::function<int(AxisInfo, AxisInfo, int)> &getContiguity,\n-    const std::function<int(AxisInfo, AxisInfo, int)> &getDivisibility,\n-    const std::function<int(AxisInfo, AxisInfo, int)> &getConstancy) {\n-  int rank = lhsInfo.getRank();\n-  AxisInfo::DimVectorT newContiguity;\n-  AxisInfo::DimVectorT newDivisibility;\n-  AxisInfo::DimVectorT newConstancy;\n-  for (int d = 0; d < rank; ++d) {\n-    newContiguity.push_back(getContiguity(lhsInfo, rhsInfo, d));\n-    newDivisibility.push_back(getDivisibility(lhsInfo, rhsInfo, d));\n-    newConstancy.push_back(getConstancy(lhsInfo, rhsInfo, d));\n-  }\n-  return AxisInfo(newContiguity, newDivisibility, newConstancy);\n-}\n+template <typename OpTy>\n+class CastOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    return operands[0]->getValue();\n+  }\n+};\n+\n+class MakeRangeOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::MakeRangeOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::MakeRangeOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(triton::MakeRangeOp op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto start = op.getStart();\n+    auto end = op.getEnd();\n+    return AxisInfo(/*contiguity=*/{end - start},\n+                    /*divisibility=*/{highestPowOf2Divisor(start)},\n+                    /*constancy=*/{1});\n+  }\n+};\n+\n+template <typename OpTy>\n+class ConstantOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n \n-ChangeResult AxisInfoAnalysis::visitOperation(\n-    Operation *op, ArrayRef<LatticeElement<AxisInfo> *> operands) {\n-  AxisInfo curr;\n-  // This preserves the input axes (e.g., cast):\n-  if (llvm::isa<arith::ExtSIOp, arith::ExtUIOp, arith::TruncIOp,\n-                triton::PtrToIntOp, triton::IntToPtrOp,\n-                triton::gpu::ConvertLayoutOp>(op))\n-    curr = operands[0]->getValue();\n-  // Constant ranges\n-  if (triton::MakeRangeOp make_range =\n-          llvm::dyn_cast<triton::MakeRangeOp>(op)) {\n-    int start = make_range.start();\n-    int end = make_range.end();\n-    AxisInfo::DimVectorT contiguity = {end - start};\n-    AxisInfo::DimVectorT divisibility = {highestPowOf2Divisor(start)};\n-    AxisInfo::DimVectorT constancy = {1};\n-    curr = AxisInfo(contiguity, divisibility, constancy);\n-  }\n-  // Constant\n-  if (arith::ConstantOp constant = llvm::dyn_cast<arith::ConstantOp>(op)) {\n-    auto intAttr = constant.getValue().dyn_cast<IntegerAttr>();\n-    if (intAttr) {\n-      size_t val = intAttr.getValue().getZExtValue();\n-      curr = AxisInfo({1}, {highestPowOf2Divisor(val)}, {1});\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto intAttr = op.getValue().template dyn_cast<IntegerAttr>();\n+    auto boolAttr = op.getValue().template dyn_cast<BoolAttr>();\n+    if (intAttr || boolAttr) {\n+      int64_t value{};\n+      if (intAttr)\n+        value = intAttr.getValue().getZExtValue();\n+      else\n+        value = boolAttr.getValue() ? 1 : 0;\n+      return AxisInfo(/*contiguity=*/{1},\n+                      /*divisibility=*/{highestPowOf2Divisor(value)},\n+                      /*constancy=*/{1},\n+                      /*knownConstantValue=*/{value});\n     }\n     // TODO: generalize to dense attr\n-    auto splatAttr = constant.getValue().dyn_cast<SplatElementsAttr>();\n-    if (splatAttr && splatAttr.getElementType().isInteger(32)) {\n-      auto value = splatAttr.getSplatValue<int>();\n-      TensorType ty = splatAttr.getType().cast<TensorType>();\n-      curr = AxisInfo(\n-          AxisInfo::DimVectorT(ty.getRank(), 1),\n+    auto splatAttr = op.getValue().template dyn_cast<SplatElementsAttr>();\n+    if (splatAttr && splatAttr.getElementType().isIntOrIndex()) {\n+      int64_t value = splatAttr.template getSplatValue<APInt>().getZExtValue();\n+      TensorType ty = splatAttr.getType().template cast<TensorType>();\n+      return AxisInfo(\n+          /*contiguity=*/AxisInfo::DimVectorT(ty.getRank(), 1),\n+          /*divisibility=*/\n           AxisInfo::DimVectorT(ty.getRank(), highestPowOf2Divisor(value)),\n-          AxisInfo::DimVectorT(ty.getShape().begin(), ty.getShape().end()));\n+          /*constancy=*/\n+          AxisInfo::DimVectorT(ty.getShape().begin(), ty.getShape().end()),\n+          /*knownConstantValue=*/{value});\n     }\n+    return AxisInfo();\n   }\n-  // TODO: refactor & complete binary ops\n-  // Addition\n-  if (llvm::isa<arith::AddIOp, triton::AddPtrOp>(op)) {\n-    auto newContiguity = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return std::max(gcd(lhs.getContiguity(d), rhs.getConstancy(d)),\n-                      gcd(lhs.getConstancy(d), rhs.getContiguity(d)));\n-    };\n-    auto newConstancy = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n-    };\n-    auto newDivisibility = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getDivisibility(d), rhs.getDivisibility(d));\n-    };\n-    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n-                         newContiguity, newDivisibility, newConstancy);\n-  }\n-  // Multiplication\n-  if (llvm::isa<arith::MulIOp>(op)) {\n-    auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };\n-    auto newConstancy = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n-    };\n-    auto newDivisibility = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return lhs.getDivisibility(d) * rhs.getDivisibility(d);\n-    };\n-    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n-                         newContiguity, newDivisibility, newConstancy);\n+};\n+\n+template <typename OpTy>\n+class AddSubOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    return std::max(gcd(lhs.getConstancy(dim), rhs.getContiguity(dim)),\n+                    gcd(lhs.getContiguity(dim), rhs.getConstancy(dim)));\n   }\n-  // Remainder\n-  if (llvm::isa<arith::RemSIOp, arith::RemUIOp>(op)) {\n-    auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getContiguity(d), rhs.getDivisibility(d));\n-    };\n-    auto newDivisibility = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getDivisibility(d), rhs.getDivisibility(d));\n-    };\n-    auto newConstancy = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n-    };\n-    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n-                         newContiguity, newDivisibility, newConstancy);\n-  }\n-  // TODO: All other binary ops\n-  if (llvm::isa<arith::AndIOp, arith::OrIOp>(op)) {\n-    auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };\n-    auto newDivisibility = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };\n-    auto newConstancy = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n-    };\n-    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n-                         newContiguity, newDivisibility, newConstancy);\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    // lhs = k * d_lhs = k * k' * gcd(d_lhs, d_rhs)\n+    // rhs = p * d_rhs = p * p' * gcd(d_lhs, d_rhs)\n+    // lhs + rhs = k * d_lhs + p * d_rhs = (k * d_lhs + p * d_rhs) *\n+    // gcd(d_lhs, d_rhs)\n+    auto elemSize = 1;\n+    if constexpr (std::is_same_v<OpTy, triton::AddPtrOp>) {\n+      //  %ptr = addptr %lhs, %rhs\n+      // is equivalent to\n+      //  %0 = mul %lhs, %elemSize\n+      //  %ptr = add %0, %rhs\n+      elemSize = std::max<unsigned int>(\n+          1, triton::getPointeeBitWidth(op.getPtr().getType()) / 8);\n+    }\n+    return gcd(lhs.getDivisibility(dim), rhs.getDivisibility(dim) * elemSize);\n+  }\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value()) {\n+      if constexpr (std::is_same_v<OpTy, arith::AddIOp> ||\n+                    std::is_same_v<OpTy, triton::AddPtrOp> ||\n+                    std::is_same_v<OpTy, LLVM::AddOp>) {\n+        return {lhs.getConstantValue().value() +\n+                rhs.getConstantValue().value()};\n+      } else if constexpr (std::is_same_v<OpTy, arith::SubIOp>) {\n+        return {lhs.getConstantValue().value() -\n+                rhs.getConstantValue().value()};\n+      }\n+    }\n+    return {};\n+  }\n+};\n+\n+class MulIOpAxisInfoVisitor final : public BinaryOpVisitorImpl<arith::MulIOp> {\n+public:\n+  using BinaryOpVisitorImpl<arith::MulIOp>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(arith::MulIOp op, const AxisInfo &lhs,\n+                        const AxisInfo &rhs, int dim) override {\n+    // lhs * 1 = lhs\n+    auto lhsContiguity =\n+        rhs.getConstantValue().has_value() && rhs.getConstantValue() == 1\n+            ? lhs.getContiguity(dim)\n+            : 1;\n+    // 1 * rhs = rhs\n+    auto rhsContiguity =\n+        lhs.getConstantValue().has_value() && lhs.getConstantValue() == 1\n+            ? rhs.getContiguity(dim)\n+            : 1;\n+    return std::max(lhsContiguity, rhsContiguity);\n+  }\n+\n+  int64_t getConstancy(arith::MulIOp op, const AxisInfo &lhs,\n+                       const AxisInfo &rhs, int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  int64_t getDivisibility(arith::MulIOp op, const AxisInfo &lhs,\n+                          const AxisInfo &rhs, int dim) override {\n+    // lhs = k * d_lhs\n+    // rhs = p * d_rhs\n+    // lhs * rhs = k * d_lhs * p * d_rhs = k * p * d_lhs * d_rhs\n+    return lhs.getDivisibility(dim) * rhs.getDivisibility(dim);\n+  }\n+\n+  std::optional<int64_t> getConstantValue(arith::MulIOp op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() * rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class DivOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    // lhs / 1 = lhs\n+    return rhs.getConstantValue().has_value() &&\n+                   rhs.getConstantValue().value() == 1\n+               ? lhs.getContiguity(dim)\n+               : 1;\n+  }\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return BinaryOpVisitorImpl<OpTy>::getConstancy(op, lhs, rhs, dim);\n+    auto shape = resTy.getShape();\n+    // Case 1: both lhs and rhs are constants.\n+    auto constancy = gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+    // Case 2: lhs contiguous, rhs constant.\n+    // lhs: d_lhs * k, d_lhs * k + 1, ..., d_lhs * k + n\n+    // rhs: d_rhs * p, d_rhs * p, ..., d_rhs * p\n+    // lhs / rhs = d_lhs * k / (d_rhs * p), (d_lhs * k + 1) / (d_rhs * p),\n+    // ..., (d_lhs * k + n) / (d_rhs * p)\n+    // Because d_lhs % d_rhs = 0 || d_rhs % d_lhs = 0,\n+    // the minimal constancy is gcd(d_lhs, d_rhs).\n+    // Since gcd(d_lhs, d_rhs) maybe > len(lhs),\n+    // we need to use another gcd to get the actual constancy.\n+    if (AxisInfoVisitor::isContiguousDim(lhs, shape, dim) &&\n+        AxisInfoVisitor::isConstantDim(rhs, shape, dim)) {\n+      constancy = std::max(constancy, gcd(lhs.getContiguity(dim),\n+                                          gcd(lhs.getDivisibility(dim),\n+                                              rhs.getDivisibility(dim))));\n+    }\n+    return constancy;\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    // Case 1: lhs is 0\n+    if (lhs.getConstantValue().has_value() &&\n+        lhs.getConstantValue().value() == 0)\n+      return lhs.getDivisibility(dim);\n+    // Case 2: rhs is 1\n+    if (rhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().value() == 1)\n+      return lhs.getDivisibility(dim);\n+    // otherwise: return 1\n+    return 1;\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() / rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class RemOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return BinaryOpVisitorImpl<OpTy>::getContiguity(op, lhs, rhs, dim);\n+    auto shape = resTy.getShape();\n+    int64_t contiguity = 1;\n+    // lhs contiguous, rhs constant\n+    // lhs: d_lhs * k, d_lhs * k + 1, ..., d_lhs * k + n\n+    // rhs: d_rhs * p, d_rhs * p, ..., d_rhs * p\n+    // lhs % rhs = d_lhs * k % (d_rhs * p), (d_lhs * k + 1) % (d_rhs * p),\n+    // ..., (d_lhs * k + n) % (d_rhs * p)\n+    // Because d_lhs % d_rhs = 0 || d_rhs % d_lhs = 0,\n+    // The minimal contiguity is gcd(d_lhs, d_rhs).\n+    // Since gcd(d_lhs, d_rhs) maybe > len(lhs),\n+    // we need to use another gcd to get the actual contiguity.\n+    if (AxisInfoVisitor::isContiguousDim(lhs, shape, dim) &&\n+        AxisInfoVisitor::isConstantDim(rhs, shape, dim)) {\n+      contiguity = std::max(contiguity, gcd(lhs.getContiguity(dim),\n+                                            gcd(lhs.getDivisibility(dim),\n+                                                rhs.getDivisibility(dim))));\n+    }\n+    return contiguity;\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    // lhs: d_lhs * k = gcd(d_lhs, d_rhs) * k' * k = gcd(d_lhs, d_rhs) * k''\n+    // rhs: d_rhs * p = gcd(d_lhs, d_rhs) * p' * p = gcd(d_lhs, d_rhs) * p''\n+    // lhs = gcd(d_lhs, d_rhs) * k'' = gcd(d_lhs, d_rhs) * d + r\n+    // r must be divisible by gcd(d_lhs, d_rhs)\n+    return gcd(lhs.getDivisibility(dim), rhs.getDivisibility(dim));\n+  };\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return BinaryOpVisitorImpl<OpTy>::getConstancy(op, lhs, rhs, dim);\n+    auto shape = resTy.getShape();\n+    // lhs % 1 = 0\n+    return rhs.getConstantValue().has_value() &&\n+                   rhs.getConstantValue().value() == 1\n+               ? shape[dim]\n+               : gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() % rhs.getConstantValue().value()};\n+    else if (rhs.getConstantValue().has_value() &&\n+             rhs.getConstantValue().value() == 1)\n+      return {0};\n+    return {};\n   }\n-  // Splat\n-  if (llvm::isa<triton::SplatOp>(op)) {\n+};\n+\n+class SplatOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::SplatOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::SplatOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(triton::SplatOp op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n     Type _retTy = *op->result_type_begin();\n     TensorType retTy = _retTy.cast<TensorType>();\n     AxisInfo opInfo = operands[0]->getValue();\n@@ -197,21 +464,39 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n       divisibility.push_back(opInfo.getDivisibility(0));\n       constancy.push_back(retTy.getShape()[d]);\n     }\n-    curr = AxisInfo(contiguity, divisibility, constancy);\n+    return AxisInfo(contiguity, divisibility, constancy,\n+                    operands[0]->getValue().getConstantValue());\n   }\n-  // expandDims\n-  if (auto expandDims = llvm::dyn_cast<triton::ExpandDimsOp>(op)) {\n+};\n+\n+class ExpandDimsOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::ExpandDimsOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::ExpandDimsOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(triton::ExpandDimsOp op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n     AxisInfo opInfo = operands[0]->getValue();\n     AxisInfo::DimVectorT contiguity = opInfo.getContiguity();\n     AxisInfo::DimVectorT divisibility = opInfo.getDivisibility();\n     AxisInfo::DimVectorT constancy = opInfo.getConstancy();\n-    contiguity.insert(contiguity.begin() + expandDims.axis(), 1);\n-    divisibility.insert(divisibility.begin() + expandDims.axis(), 1);\n-    constancy.insert(constancy.begin() + expandDims.axis(), 1);\n-    curr = AxisInfo(contiguity, divisibility, constancy);\n+    contiguity.insert(contiguity.begin() + op.getAxis(), 1);\n+    divisibility.insert(divisibility.begin() + op.getAxis(), 1);\n+    constancy.insert(constancy.begin() + op.getAxis(), 1);\n+    return AxisInfo(contiguity, divisibility, constancy,\n+                    operands[0]->getValue().getConstantValue());\n   }\n-  // Broadcast\n-  if (llvm::isa<triton::BroadcastOp>(op)) {\n+};\n+\n+class BroadcastOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::BroadcastOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::BroadcastOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(triton::BroadcastOp op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n     Type _retTy = *op->result_type_begin();\n     Type _opTy = *op->operand_type_begin();\n     TensorType retTy = _retTy.cast<TensorType>();\n@@ -228,94 +513,501 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n       constancy.push_back(opShape[d] == 1 ? retShape[d]\n                                           : opInfo.getConstancy(d));\n     }\n-    curr = AxisInfo(contiguity, divisibility, constancy);\n+    return AxisInfo(contiguity, divisibility, constancy,\n+                    operands[0]->getValue().getConstantValue());\n   }\n+};\n \n-  // CmpI\n-  if ((llvm::dyn_cast<arith::CmpIOp>(op) ||\n-       llvm::dyn_cast<triton::gpu::CmpIOp>(op)) &&\n-      op->getResult(0).getType().dyn_cast<TensorType>()) {\n-    auto resTy = op->getResult(0).getType().cast<TensorType>();\n+template <typename OpTy>\n+class CmpOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return AxisInfo();\n+    auto shape = resTy.getShape();\n     short rank = resTy.getRank();\n     auto lhsInfo = operands[0]->getValue();\n     auto rhsInfo = operands[1]->getValue();\n-    auto shape = resTy.getShape();\n \n     AxisInfo::DimVectorT contiguity, divisibility, constancy;\n+    std::optional<int64_t> constantValue;\n     for (short d = 0; d < rank; ++d) {\n-      if (rhsInfo.getConstancy(d) % lhsInfo.getContiguity(d) == 0 ||\n-          rhsInfo.getConstancy(d) % lhsInfo.getConstancy(d))\n-        constancy.push_back(\n-            gcd(lhsInfo.getDivisibility(d), rhsInfo.getDivisibility(d)));\n-      else\n-        constancy.push_back(1);\n+      int64_t constHint = 1;\n+      if (lhsInfo.getConstantValue().has_value() &&\n+          rhsInfo.getConstantValue().has_value()) {\n+        constHint = lhsInfo.getConstancy(d);\n+        constantValue =\n+            compare(getPredicate(op), lhsInfo.getConstantValue().value(),\n+                    rhsInfo.getConstantValue().value())\n+                ? 1\n+                : 0;\n+      } else {\n+        // Case 1: lhs and rhs are both partial constants\n+        constHint = gcd(lhsInfo.getConstancy(d), rhsInfo.getConstancy(d));\n+        // Case 2: lhs all constant, rhs all contiguous\n+        // NOTE:\n+        // lhs: 4 4 4 4\n+        // rhs: 4 5 6 7\n+        // lhs ge rhs: 1, 0, 0, 0\n+        // Case 3: lhs all contiguous, rhs all constant\n+        // NOTE\n+        // lhs: 4 5 6 7\n+        // rhs: 4 4 4 4\n+        // lhs sle rhs: 1, 0, 0, 0\n+        if (/*Case 2=*/(\n+                notGePredicate(getPredicate(op)) &&\n+                (AxisInfoVisitor::isConstantDim(lhsInfo, shape, d) &&\n+                 AxisInfoVisitor::isContiguousDim(rhsInfo, shape, d))) ||\n+            /*Case 3=*/(notLePredicate(getPredicate(op)) &&\n+                        (AxisInfoVisitor::isContiguousDim(lhsInfo, shape, d) &&\n+                         AxisInfoVisitor::isConstantDim(rhsInfo, shape, d)))) {\n+          constHint = std::max(constHint, gcd(lhsInfo.getContiguity(d),\n+                                              gcd(lhsInfo.getDivisibility(d),\n+                                                  rhsInfo.getDivisibility(d))));\n+        }\n+      }\n \n-      divisibility.push_back(shape[d]);\n+      constancy.push_back(constHint);\n+      divisibility.push_back(1);\n       contiguity.push_back(1);\n     }\n \n-    curr = AxisInfo(contiguity, divisibility, constancy);\n+    return AxisInfo(contiguity, divisibility, constancy, constantValue);\n   }\n \n-  // UnrealizedConversionCast\n+private:\n+  static arith::CmpIPredicate getPredicate(triton::gpu::CmpIOp op) {\n+    return op.getPredicate();\n+  }\n+\n+  static arith::CmpIPredicate getPredicate(arith::CmpIOp op) {\n+    return op.getPredicate();\n+  }\n+\n+  static bool notGePredicate(arith::CmpIPredicate predicate) {\n+    return predicate != arith::CmpIPredicate::sge &&\n+           predicate != arith::CmpIPredicate::uge;\n+  }\n+\n+  static bool notLePredicate(arith::CmpIPredicate predicate) {\n+    return predicate != arith::CmpIPredicate::sle &&\n+           predicate != arith::CmpIPredicate::ule;\n+  }\n+\n+  static bool compare(arith::CmpIPredicate predicate, int64_t lhs,\n+                      int64_t rhs) {\n+    switch (predicate) {\n+    case arith::CmpIPredicate::eq:\n+      return lhs == rhs;\n+    case arith::CmpIPredicate::ne:\n+      return lhs != rhs;\n+    case arith::CmpIPredicate::slt:\n+      return lhs < rhs;\n+    case arith::CmpIPredicate::sle:\n+      return lhs <= rhs;\n+    case arith::CmpIPredicate::sgt:\n+      return lhs > rhs;\n+    case arith::CmpIPredicate::sge:\n+      return lhs >= rhs;\n+    case arith::CmpIPredicate::ult:\n+      return (uint64_t)lhs < (uint64_t)rhs;\n+    case arith::CmpIPredicate::ule:\n+      return (uint64_t)lhs <= (uint64_t)rhs;\n+    case arith::CmpIPredicate::ugt:\n+      return (uint64_t)lhs > (uint64_t)rhs;\n+    case arith::CmpIPredicate::uge:\n+      return (uint64_t)lhs >= (uint64_t)rhs;\n+    default:\n+      break;\n+    }\n+    llvm_unreachable(\"unknown comparison predicate\");\n+  }\n+};\n+\n+template <typename OpTy>\n+class SelectOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return AxisInfo();\n+    auto shape = resTy.getShape();\n+    auto rank = shape.size();\n+    auto condConstancy = operands[0]->getValue().getConstancy();\n+    auto lhsInfo = operands[1]->getValue();\n+    auto rhsInfo = operands[2]->getValue();\n+\n+    AxisInfo::DimVectorT contiguity, divisibility, constancy;\n+    std::optional<int64_t> constantValue;\n+    if (operands[0]->getValue().getConstantValue().has_value()) {\n+      if (operands[0]->getValue().getConstantValue() == 0) {\n+        contiguity = rhsInfo.getContiguity();\n+        divisibility = rhsInfo.getDivisibility();\n+        constancy = rhsInfo.getConstancy();\n+        constantValue = rhsInfo.getConstantValue();\n+      } else {\n+        contiguity = lhsInfo.getContiguity();\n+        divisibility = lhsInfo.getDivisibility();\n+        constancy = lhsInfo.getConstancy();\n+        constantValue = lhsInfo.getConstantValue();\n+      }\n+    } else {\n+      for (auto d = 0; d < rank; ++d) {\n+        constancy.push_back(\n+            std::min(gcd(lhsInfo.getConstancy(d), condConstancy[d]),\n+                     gcd(rhsInfo.getConstancy(d), condConstancy[d])));\n+        divisibility.push_back(\n+            std::min(lhsInfo.getDivisibility(d), rhsInfo.getDivisibility(d)));\n+        contiguity.push_back(\n+            std::min(gcd(lhsInfo.getContiguity(d), condConstancy[d]),\n+                     gcd(rhsInfo.getContiguity(d), condConstancy[d])));\n+      }\n+      if (lhsInfo.getConstantValue().has_value() &&\n+          rhsInfo.getConstantValue().has_value() &&\n+          lhsInfo.getConstantValue() == rhsInfo.getConstantValue())\n+        constantValue = lhsInfo.getConstantValue();\n+    }\n+\n+    return AxisInfo(contiguity, divisibility, constancy, constantValue);\n+  }\n+};\n+\n+template <typename OpTy>\n+class LogicalOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value()) {\n+      if constexpr (std::is_same_v<OpTy, arith::AndIOp>) {\n+        return {lhs.getConstantValue().value() &\n+                rhs.getConstantValue().value()};\n+      } else if constexpr (std::is_same_v<OpTy, arith::OrIOp>) {\n+        return {lhs.getConstantValue().value() |\n+                rhs.getConstantValue().value()};\n+      } else if constexpr (std::is_same_v<OpTy, arith::XOrIOp>) {\n+        return {lhs.getConstantValue().value() ^\n+                rhs.getConstantValue().value()};\n+      }\n+    }\n+    return {};\n+  }\n+};\n+\n+class ShLIOpAxisInfoVisitor final : public BinaryOpVisitorImpl<arith::ShLIOp> {\n+public:\n+  using BinaryOpVisitorImpl<arith::ShLIOp>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(arith::ShLIOp op, const AxisInfo &lhs,\n+                        const AxisInfo &rhs, int dim) override {\n+    if (rhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().value() == 0)\n+      return lhs.getContiguity(dim);\n+    else\n+      return 1;\n+  }\n+\n+  int64_t getDivisibility(arith::ShLIOp op, const AxisInfo &lhs,\n+                          const AxisInfo &rhs, int dim) override {\n+    auto shift = rhs.getConstantValue().has_value()\n+                     ? rhs.getConstantValue().value()\n+                     : rhs.getDivisibility(dim);\n+    auto numBits = log2Int(lhs.getDivisibility(dim));\n+    auto maxBits = log2Int(highestPowOf2Divisor<int64_t>(0));\n+    // Make sure the return value doesn't exceed highestPowOf2Divisor<int64>(0)\n+    if (shift + numBits > maxBits)\n+      return highestPowOf2Divisor<int64_t>(0);\n+    return lhs.getDivisibility(dim) << shift;\n+  }\n+\n+  int64_t getConstancy(arith::ShLIOp op, const AxisInfo &lhs,\n+                       const AxisInfo &rhs, int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(arith::ShLIOp op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() << rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class ShROpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    if (rhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().value() == 0)\n+      return lhs.getContiguity(dim);\n+    else\n+      return 1;\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    if (rhs.getConstantValue().has_value())\n+      return std::max<int64_t>(1, lhs.getDivisibility(dim) /\n+                                      (1 << rhs.getConstantValue().value()));\n+    else\n+      return std::max<int64_t>(1, lhs.getDivisibility(dim) /\n+                                      (1 << rhs.getDivisibility(dim)));\n+  }\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() >> rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class MaxMinOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto lhsInfo = operands[0]->getValue();\n+    auto rhsInfo = operands[1]->getValue();\n+    std::optional<int64_t> constantValue;\n+    if (lhsInfo.getConstantValue().has_value() &&\n+        rhsInfo.getConstantValue().has_value()) {\n+      if constexpr (std::is_same_v<OpTy, arith::MaxSIOp> ||\n+                    std::is_same_v<OpTy, arith::MaxUIOp>) {\n+        constantValue = {std::max(lhsInfo.getConstantValue().value(),\n+                                  rhsInfo.getConstantValue().value())};\n+      } else if constexpr (std::is_same_v<OpTy, arith::MinSIOp> ||\n+                           std::is_same_v<OpTy, arith::MinUIOp>) {\n+        constantValue = {std::min(lhsInfo.getConstantValue().value(),\n+                                  rhsInfo.getConstantValue().value())};\n+      }\n+    }\n+    auto rank = lhsInfo.getRank();\n+    return AxisInfo(/*knownContiguity=*/AxisInfo::DimVectorT(rank, 1),\n+                    /*knownDivisibility=*/AxisInfo::DimVectorT(rank, 1),\n+                    /*knownConstancy=*/AxisInfo::DimVectorT(rank, 1),\n+                    /*constantValue=*/constantValue);\n+  }\n+};\n+\n+//===----------------------------------------------------------------------===//\n+// AxisInfoAnalysis\n+//===----------------------------------------------------------------------===//\n+\n+AxisInfoAnalysis::AxisInfoAnalysis(DataFlowSolver &solver)\n+    : dataflow::SparseDataFlowAnalysis<dataflow::Lattice<AxisInfo>>(solver) {\n+  // UnrealizedConversionCast:\n   // This is needed by TritonGPUToLLVM, to get AxisInfo when the graph is\n   // in the process of a PartialConversion, where UnrealizedConversionCast\n   // may exist\n-  if (llvm::isa<mlir::UnrealizedConversionCastOp>(op)) {\n-    curr = operands[0]->getValue();\n+  visitors.append<CastOpAxisInfoVisitor<arith::ExtSIOp>,\n+                  CastOpAxisInfoVisitor<arith::ExtUIOp>,\n+                  CastOpAxisInfoVisitor<arith::TruncIOp>,\n+                  CastOpAxisInfoVisitor<arith::IndexCastOp>,\n+                  CastOpAxisInfoVisitor<triton::PtrToIntOp>,\n+                  CastOpAxisInfoVisitor<triton::IntToPtrOp>,\n+                  CastOpAxisInfoVisitor<triton::gpu::ConvertLayoutOp>,\n+                  CastOpAxisInfoVisitor<mlir::UnrealizedConversionCastOp>,\n+                  CastOpAxisInfoVisitor<triton::BitcastOp>>();\n+  // TODO: Remove rules for LLVM::ConstantOp, LLVM::AddOp\n+  // when scf.for supports integers induction variable\n+  visitors.append<MakeRangeOpAxisInfoVisitor>();\n+  visitors.append<ConstantOpAxisInfoVisitor<arith::ConstantOp>,\n+                  ConstantOpAxisInfoVisitor<LLVM::ConstantOp>>();\n+  visitors.append<AddSubOpAxisInfoVisitor<triton::AddPtrOp>,\n+                  AddSubOpAxisInfoVisitor<arith::AddIOp>,\n+                  AddSubOpAxisInfoVisitor<arith::SubIOp>,\n+                  AddSubOpAxisInfoVisitor<LLVM::AddOp>>();\n+  visitors.append<MulIOpAxisInfoVisitor>();\n+  visitors.append<DivOpAxisInfoVisitor<arith::DivSIOp>,\n+                  DivOpAxisInfoVisitor<arith::DivUIOp>>();\n+  visitors.append<RemOpAxisInfoVisitor<arith::RemSIOp>,\n+                  RemOpAxisInfoVisitor<arith::RemUIOp>>();\n+  visitors.append<BroadcastOpAxisInfoVisitor>();\n+  visitors.append<SplatOpAxisInfoVisitor>();\n+  visitors.append<ExpandDimsOpAxisInfoVisitor>();\n+  visitors.append<CmpOpAxisInfoVisitor<arith::CmpIOp>,\n+                  CmpOpAxisInfoVisitor<triton::gpu::CmpIOp>>();\n+  visitors.append<LogicalOpAxisInfoVisitor<arith::AndIOp>,\n+                  LogicalOpAxisInfoVisitor<arith::OrIOp>,\n+                  LogicalOpAxisInfoVisitor<arith::XOrIOp>>();\n+  visitors.append<SelectOpAxisInfoVisitor<mlir::arith::SelectOp>,\n+                  SelectOpAxisInfoVisitor<triton::gpu::SelectOp>>();\n+  visitors.append<ShLIOpAxisInfoVisitor, ShROpAxisInfoVisitor<arith::ShRUIOp>,\n+                  ShROpAxisInfoVisitor<arith::ShRSIOp>>();\n+  visitors.append<MaxMinOpAxisInfoVisitor<arith::MaxSIOp>,\n+                  MaxMinOpAxisInfoVisitor<arith::MaxUIOp>,\n+                  MaxMinOpAxisInfoVisitor<arith::MinSIOp>,\n+                  MaxMinOpAxisInfoVisitor<arith::MinUIOp>>();\n+}\n+\n+void AxisInfoAnalysis::visitOperation(\n+    Operation *op, ArrayRef<const dataflow::Lattice<AxisInfo> *> operands,\n+    ArrayRef<dataflow::Lattice<AxisInfo> *> results) {\n+  // TODO: For sure not the right way to do this\n+  // but why is scf.if not initialized otherwise?\n+  for (auto op : operands)\n+    if (op->getValue().getRank() == 0)\n+      setToEntryState((dataflow::Lattice<AxisInfo> *)op);\n+  AxisInfo curr = visitors.apply(op, operands);\n+  if (curr.getRank() == 0)\n+    return setAllToEntryStates(results);\n+  // override with hint\n+  auto newContiguity = curr.getContiguity();\n+  auto newDivisibility = curr.getDivisibility();\n+  auto newConstancy = curr.getConstancy();\n+  if (Attribute attr = op->getDiscardableAttr(\"tt.contiguity\")) {\n+    auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+    newContiguity = AxisInfo::DimVectorT(vals.begin(), vals.end());\n   }\n-  if (curr.getRank() == 0) {\n-    return markAllPessimisticFixpoint(op->getResults());\n+  if (Attribute attr = op->getDiscardableAttr(\"tt.divisibility\")) {\n+    auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+    newDivisibility = AxisInfo::DimVectorT(vals.begin(), vals.end());\n   }\n-\n-  // join all lattice elements\n-  ChangeResult result = ChangeResult::NoChange;\n-  for (Value value : op->getResults()) {\n-    result |= getLatticeElement(value).join(curr);\n+  if (Attribute attr = op->getDiscardableAttr(\"tt.constancy\")) {\n+    auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+    newConstancy = AxisInfo::DimVectorT(vals.begin(), vals.end());\n   }\n-  return result;\n+  curr = mlir::AxisInfo(newContiguity, newDivisibility, newConstancy,\n+                        curr.getConstantValue());\n+  // join all lattice elements\n+  for (auto *result : results)\n+    propagateIfChanged(result, result->join(curr));\n }\n \n-unsigned AxisInfoAnalysis::getPtrVectorSize(Value ptr) {\n+unsigned ModuleAxisInfoAnalysis::getPtrContiguity(Value ptr) {\n   auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n   if (!tensorTy)\n     return 1;\n   auto layout = tensorTy.getEncoding();\n-  auto shape = tensorTy.getShape();\n \n   // Here order should be ordered by contiguous first, so the first element\n   // should have the largest contiguous.\n   auto order = triton::gpu::getOrder(layout);\n   unsigned align = getPtrAlignment(ptr);\n \n-  unsigned contigPerThread = triton::gpu::getSizePerThread(layout)[order[0]];\n-  unsigned vec = std::min(align, contigPerThread);\n-  vec = std::min<unsigned>(shape[order[0]], vec);\n+  auto uniqueContigPerThread = triton::gpu::getUniqueContigPerThread(tensorTy);\n+  assert(order[0] < uniqueContigPerThread.size() &&\n+         \"Unxpected uniqueContigPerThread size\");\n+  unsigned contiguity = uniqueContigPerThread[order[0]];\n+  contiguity = std::min(align, contiguity);\n \n-  return vec;\n+  return contiguity;\n }\n \n-unsigned AxisInfoAnalysis::getPtrAlignment(Value ptr) {\n+unsigned ModuleAxisInfoAnalysis::getPtrAlignment(Value ptr) {\n   auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n   if (!tensorTy)\n     return 1;\n-  auto axisInfo = lookupLatticeElement(ptr)->getValue();\n+  auto *axisInfo = getAxisInfo(ptr);\n+  if (!axisInfo)\n+    return 1;\n   auto layout = tensorTy.getEncoding();\n   auto order = triton::gpu::getOrder(layout);\n-  unsigned maxMultiple = axisInfo.getDivisibility(order[0]);\n-  unsigned maxContig = axisInfo.getContiguity(order[0]);\n+  auto maxMultipleBytes = axisInfo->getDivisibility(order[0]);\n+  auto maxContig = axisInfo->getContiguity(order[0]);\n+  auto elemNumBits = triton::getPointeeBitWidth(tensorTy);\n+  auto elemNumBytes = std::max<unsigned>(elemNumBits / 8, 1);\n+  auto maxMultiple = std::max<int64_t>(maxMultipleBytes / elemNumBytes, 1);\n   unsigned alignment = std::min(maxMultiple, maxContig);\n   return alignment;\n }\n \n-unsigned AxisInfoAnalysis::getMaskAlignment(Value mask) {\n+unsigned ModuleAxisInfoAnalysis::getMaskAlignment(Value mask) {\n   auto tensorTy = mask.getType().dyn_cast<RankedTensorType>();\n   if (!tensorTy)\n     return 1;\n+  auto *axisInfo = getAxisInfo(mask);\n+  if (!axisInfo)\n+    return 1;\n   auto maskOrder = triton::gpu::getOrder(tensorTy.getEncoding());\n-  auto maskAxis = lookupLatticeElement(mask)->getValue();\n-  auto alignment = std::max<unsigned>(maskAxis.getConstancy(maskOrder[0]), 1);\n+  auto alignment = std::max<unsigned>(axisInfo->getConstancy(maskOrder[0]), 1);\n   return alignment;\n }\n \n+void ModuleAxisInfoAnalysis::initialize(FunctionOpInterface funcOp) {\n+  std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n+  AxisInfoAnalysis *analysis = solver->load<AxisInfoAnalysis>();\n+  if (failed(solver->initializeAndRun(funcOp)))\n+    return;\n+  auto *axisInfoMap = getFuncData(funcOp);\n+  auto updateAxisInfoMap = [&](Value value) {\n+    auto axisInfo = analysis->getLatticeElement(value)->getValue();\n+    AxisInfo curAxisInfo;\n+    if (axisInfoMap->count(value)) {\n+      curAxisInfo = AxisInfo::join(axisInfo, axisInfoMap->lookup(value));\n+    } else {\n+      curAxisInfo = axisInfo;\n+    }\n+    (*axisInfoMap)[value] = curAxisInfo;\n+  };\n+  funcOp.walk([&](Operation *op) {\n+    for (auto value : op->getResults()) {\n+      updateAxisInfoMap(value);\n+    }\n+  });\n+  funcOp.walk([&](Block *block) {\n+    for (auto value : block->getArguments()) {\n+      updateAxisInfoMap(value);\n+    }\n+  });\n+}\n+\n+void ModuleAxisInfoAnalysis::update(CallOpInterface callOp,\n+                                    FunctionOpInterface callee) {\n+  auto caller = callOp->getParentOfType<FunctionOpInterface>();\n+  auto *axisInfoMap = getFuncData(caller);\n+  for (auto entry : llvm::enumerate(callOp->getOperands())) {\n+    auto index = entry.index();\n+    auto value = entry.value();\n+    auto setAttrFn = [&](StringRef attrName, int64_t prevValue) {\n+      auto curValue = highestPowOf2Divisor<int64_t>(0);\n+      if (callee.getArgAttrOfType<IntegerAttr>(index, attrName)) {\n+        curValue =\n+            callee.getArgAttrOfType<IntegerAttr>(index, attrName).getInt();\n+      }\n+      auto attr = IntegerAttr::get(IntegerType::get(callee.getContext(), 64),\n+                                   gcd(prevValue, curValue));\n+      callee.setArgAttr(index, attrName, attr);\n+    };\n+    auto axisInfo = axisInfoMap->lookup(value);\n+    assert(axisInfo.getRank() == 1 && \"only scalar arguments are supported\");\n+    setAttrFn(\"tt.contiguity\", axisInfo.getContiguity(0));\n+    setAttrFn(\"tt.divisibility\", axisInfo.getDivisibility(0));\n+    setAttrFn(\"tt.constancy\", axisInfo.getConstancy(0));\n+  }\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Analysis/CMakeLists.txt", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -8,4 +8,10 @@ add_mlir_library(TritonAnalysis\n   DEPENDS\n   TritonTableGen\n   TritonGPUAttrDefsIncGen\n+\n+  LINK_LIBS PUBLIC\n+  MLIRAnalysis\n+  MLIRLLVMDialect\n+  TritonIR\n+  TritonGPUIR\n )"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 127, "deletions": 85, "changes": 212, "file_content_changes": "@@ -2,136 +2,178 @@\n #include \"triton/Analysis/Alias.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n-#include \"mlir/Dialect/GPU/GPUDialect.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+#include <deque>\n \n namespace mlir {\n \n-void MembarAnalysis::run() {\n-  auto *operation = allocation->getOperation();\n-  RegionInfo regionInfo;\n-  OpBuilder builder(operation);\n-  dfsOperation(operation, &regionInfo, &builder);\n+void MembarAnalysis::run(FuncBlockInfoMapT &funcBlockInfoMap) {\n+  FunctionOpInterface funcOp =\n+      dyn_cast<FunctionOpInterface>(allocation->getOperation());\n+  OpBuilder builder(funcOp.getContext());\n+  resolve(funcOp, &funcBlockInfoMap, &builder);\n }\n \n-void MembarAnalysis::dfsOperation(Operation *operation,\n-                                  RegionInfo *parentRegionInfo,\n-                                  OpBuilder *builder) {\n-  transfer(operation, parentRegionInfo, builder);\n-  if (operation->getNumRegions()) {\n-    // If there's any nested regions, we need to visit them.\n-    // scf.if and scf.else: two regions\n-    // scf.if only: two regions\n-    // scf.for: one region\n-    RegionInfo curRegionInfo;\n-    auto traverseRegions = [&]() -> auto{\n-      for (auto &region : operation->getRegions()) {\n-        // Copy the parent info as the current info.\n-        RegionInfo regionInfo = *parentRegionInfo;\n-        for (auto &block : region.getBlocks()) {\n-          assert(region.getBlocks().size() == 1 &&\n-                 \"Multiple blocks in a region is not supported\");\n-          for (auto &op : block.getOperations()) {\n-            // Traverse the nested operation.\n-            dfsOperation(&op, &regionInfo, builder);\n-          }\n-        }\n-        curRegionInfo.join(regionInfo);\n+void MembarAnalysis::resolve(FunctionOpInterface funcOp,\n+                             FuncBlockInfoMapT *funcBlockInfoMap,\n+                             OpBuilder *builder) {\n+  // Initialize the blockList\n+  DenseMap<Block *, BlockInfo> inputBlockInfoMap;\n+  DenseMap<Block *, BlockInfo> outputBlockInfoMap;\n+  std::deque<Block *> blockList;\n+  funcOp.walk<WalkOrder::PreOrder>([&](Block *block) {\n+    for (auto &op : block->getOperations()) {\n+      // Check if the operation belongs to scf dialect, if so, we need to\n+      // throw an error\n+      if (op.getDialect()->getNamespace() == \"scf\") {\n+        llvm::report_fatal_error(\n+            \"scf dialect is not supported in membar. Please lower it \"\n+            \"to cf dialect first.\");\n+        return;\n       }\n-      // Set the parent region info as the union of the nested region info.\n-      *parentRegionInfo = curRegionInfo;\n-    };\n+    }\n+    if (block->isEntryBlock())\n+      blockList.emplace_back(block);\n+  });\n \n-    traverseRegions();\n-    if (isa<scf::ForOp>(operation)) {\n-      // scf.for can have two possible inputs: the init value and the\n-      // previous iteration's result. Although we've applied alias analysis,\n-      // there could be unsynced memory accesses on reused memories.\n-      // For example, consider the following code:\n-      // %1 = convert_layout %0: blocked -> shared\n-      // ...\n-      // gpu.barrier\n-      // ...\n-      // %5 = convert_layout %4 : shared -> dot\n-      // %6 = tt.dot %2, %5\n-      // scf.yield\n-      //\n-      // Though %5 could be released before scf.yield, it may shared the same\n-      // memory with %1. So we actually have to insert a barrier before %1 to\n-      // make sure the memory is synced.\n-      traverseRegions();\n+  // A fixed point algorithm\n+  while (!blockList.empty()) {\n+    auto *block = blockList.front();\n+    blockList.pop_front();\n+    // Make a copy of the inputblockInfo but not update\n+    auto inputBlockInfo = inputBlockInfoMap[block];\n+    SmallVector<Block *> successors;\n+    for (auto &op : block->getOperations()) {\n+      if (op.hasTrait<OpTrait::IsTerminator>()) {\n+        visitTerminator(&op, successors);\n+      } else {\n+        update(&op, &inputBlockInfo, funcBlockInfoMap, builder);\n+      }\n+    }\n+    // Get the reference because we want to update if it changed\n+    if (outputBlockInfoMap.count(block) &&\n+        inputBlockInfo == outputBlockInfoMap[block]) {\n+      // If we have seen the block before and the inputBlockInfo is the same as\n+      // the outputBlockInfo, we skip the successors\n+      continue;\n+    }\n+    // Update the current block\n+    outputBlockInfoMap[block].join(inputBlockInfo);\n+    // Update the successors\n+    for (auto *successor : successors) {\n+      inputBlockInfoMap[successor].join(outputBlockInfoMap[block]);\n+      blockList.emplace_back(successor);\n     }\n   }\n+\n+  // Update the final dangling buffers that haven't been synced\n+  auto &funcBlockInfo = (*funcBlockInfoMap)[funcOp];\n+  funcOp.walk<WalkOrder::PreOrder>([&](Block *block) {\n+    block->walk([&](triton::ReturnOp returnOp) {\n+      funcBlockInfo.join(outputBlockInfoMap[block]);\n+    });\n+  });\n }\n \n-void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n-                              OpBuilder *builder) {\n-  if (isa<scf::ForOp>(op) || isa<scf::IfOp>(op) || isa<scf::YieldOp>(op) ||\n-      isa<tensor::ExtractSliceOp>(op) || isa<triton::gpu::AllocTensorOp>(op)) {\n-    // Do not insert barriers before control flow operations and\n-    // alloc/extract/insert\n+void MembarAnalysis::visitTerminator(Operation *op,\n+                                     SmallVector<Block *> &successors) {\n+  if (auto branchInterface = dyn_cast<BranchOpInterface>(op)) {\n+    Block *parentBlock = branchInterface->getBlock();\n+    successors.append(std::begin(parentBlock->getSuccessors()),\n+                      std::end(parentBlock->getSuccessors()));\n+    return;\n+  }\n+  // Otherwise, it could be a return op\n+  if (isa<triton::ReduceReturnOp>(op) || isa<triton::ReturnOp>(op)) {\n+    return;\n+  }\n+  llvm_unreachable(\"Unknown terminator encountered in membar analysis\");\n+}\n+\n+void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n+                            FuncBlockInfoMapT *funcBlockInfoMap,\n+                            OpBuilder *builder) {\n+  if (isa<triton::gpu::ExtractSliceOp>(op) ||\n+      isa<triton::gpu::AllocTensorOp>(op) || isa<triton::TransOp>(op)) {\n     // alloc is an allocation op without memory write.\n     // FIXME(Keren): extract_slice is always alias for now\n     return;\n   }\n \n   if (isa<gpu::BarrierOp>(op)) {\n     // If the current op is a barrier, we sync previous reads and writes\n-    regionInfo->sync();\n+    blockInfo->sync();\n     return;\n   }\n \n   if (isa<triton::gpu::AsyncWaitOp>(op) &&\n       !isa<gpu::BarrierOp>(op->getNextNode())) {\n     // If the current op is an async wait and the next op is not a barrier we\n     // insert a barrier op and sync\n-    regionInfo->sync();\n+    blockInfo->sync();\n     OpBuilder::InsertionGuard g(*builder);\n     builder->setInsertionPointAfter(op);\n     builder->create<gpu::BarrierOp>(op->getLoc());\n-    regionInfo->sync();\n+    blockInfo->sync();\n     return;\n   }\n \n-  RegionInfo curRegionInfo;\n-  for (Value value : op->getOperands()) {\n-    for (auto bufferId : allocation->getBufferIds(value)) {\n-      if (bufferId != Allocation::InvalidBufferId) {\n-        if (isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n-            isa<tensor::InsertSliceOp>(op)) {\n-          // FIXME(Keren): insert_slice and insert_slice_async are always alias\n-          // for now\n-          curRegionInfo.syncWriteBuffers.insert(bufferId);\n-        } else {\n-          // ConvertLayoutOp: shared memory -> registers\n-          curRegionInfo.syncReadBuffers.insert(bufferId);\n+  BlockInfo curBlockInfo;\n+  if (isa<triton::CallOp>(op)) {\n+    // Inter-function dependencies\n+    auto callOpInterface = dyn_cast<CallOpInterface>(op);\n+    if (auto callee =\n+            dyn_cast<FunctionOpInterface>(callOpInterface.resolveCallable())) {\n+      curBlockInfo = funcBlockInfoMap->lookup(callee);\n+    }\n+  } else {\n+    // Intra-function dependencies\n+    for (Value value : op->getOperands()) {\n+      for (auto bufferId : allocation->getBufferIds(value)) {\n+        if (bufferId != Allocation::InvalidBufferId) {\n+          if (isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n+              isa<tensor::InsertSliceOp>(op)) {\n+            // FIXME(Keren): insert_slice and insert_slice_async are always\n+            // alias for now\n+            curBlockInfo.syncWriteIntervals.insert(\n+                allocation->getAllocatedInterval(bufferId));\n+          } else {\n+            // ConvertLayoutOp: shared memory -> registers\n+            curBlockInfo.syncReadIntervals.insert(\n+                allocation->getAllocatedInterval(bufferId));\n+          }\n         }\n       }\n     }\n-  }\n-  for (Value value : op->getResults()) {\n-    // ConvertLayoutOp: registers -> shared memory\n-    auto bufferId = allocation->getBufferId(value);\n+    for (Value value : op->getResults()) {\n+      // ConvertLayoutOp: registers -> shared memory\n+      auto bufferId = allocation->getBufferId(value);\n+      if (bufferId != Allocation::InvalidBufferId) {\n+        curBlockInfo.syncWriteIntervals.insert(\n+            allocation->getAllocatedInterval(bufferId));\n+      }\n+    }\n+    // Scratch buffer is considered as both shared memory write & read\n+    auto bufferId = allocation->getBufferId(op);\n     if (bufferId != Allocation::InvalidBufferId) {\n-      curRegionInfo.syncWriteBuffers.insert(bufferId);\n+      curBlockInfo.syncWriteIntervals.insert(\n+          allocation->getAllocatedInterval(bufferId));\n+      curBlockInfo.syncReadIntervals.insert(\n+          allocation->getAllocatedInterval(bufferId));\n     }\n   }\n-  // Scratch buffer is considered as both shared memory write & read\n-  auto bufferId = allocation->getBufferId(op);\n-  if (bufferId != Allocation::InvalidBufferId) {\n-    curRegionInfo.syncWriteBuffers.insert(bufferId);\n-    curRegionInfo.syncReadBuffers.insert(bufferId);\n-  }\n \n-  if (regionInfo->isIntersected(curRegionInfo, allocation)) {\n+  if (blockInfo->isIntersected(curBlockInfo)) {\n     OpBuilder::InsertionGuard g(*builder);\n     builder->setInsertionPoint(op);\n     builder->create<gpu::BarrierOp>(op->getLoc());\n-    regionInfo->sync();\n+    blockInfo->sync();\n   }\n   // Update the region info, even if barrier is inserted, we have to maintain\n   // the current op's read/write buffers.\n-  regionInfo->join(curRegionInfo);\n+  blockInfo->join(curBlockInfo);\n }\n \n } // namespace mlir"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 237, "deletions": 66, "changes": 303, "file_content_changes": "@@ -1,68 +1,80 @@\n #include \"triton/Analysis/Utility.h\"\n+#include \"mlir/Analysis/DataFlow/ConstantPropagationAnalysis.h\"\n+#include \"mlir/Analysis/DataFlow/DeadCodeAnalysis.h\"\n #include \"mlir/IR/Dialect.h\"\n+#include \"mlir/IR/Matchers.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include <deque>\n \n namespace mlir {\n \n bool ReduceOpHelper::isFastReduction() {\n-  auto srcLayout = srcTy.getEncoding();\n-  auto axis = op.axis();\n-  return axis == triton::gpu::getOrder(srcLayout)[0];\n+  return axis == triton::gpu::getOrder(getSrcLayout())[0];\n }\n \n unsigned ReduceOpHelper::getInterWarpSize() {\n-  auto srcLayout = srcTy.getEncoding();\n-  auto srcShape = srcTy.getShape();\n-  auto axis = op.axis();\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n   unsigned sizeIntraWarps = getIntraWarpSize();\n   return std::min(srcReduceDimSize / sizeIntraWarps,\n-                  triton::gpu::getWarpsPerCTA(srcLayout)[axis]);\n+                  triton::gpu::getWarpsPerCTA(getSrcLayout())[axis]);\n }\n \n unsigned ReduceOpHelper::getIntraWarpSize() {\n-  auto srcLayout = srcTy.getEncoding();\n-  auto srcShape = srcTy.getShape();\n-  auto axis = op.axis();\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n   return std::min(srcReduceDimSize,\n-                  triton::gpu::getThreadsPerWarp(srcLayout)[axis]);\n+                  triton::gpu::getThreadsPerWarp(getSrcLayout())[axis]);\n+}\n+\n+unsigned ReduceOpHelper::getInterWarpSizeWithUniqueData() {\n+  auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n+  unsigned sizeIntraWarps = getIntraWarpSizeWithUniqueData();\n+  return std::min(srcReduceDimSize / sizeIntraWarps,\n+                  triton::gpu::getWarpsPerCTAWithUniqueData(\n+                      getSrcLayout(), getSrcShape())[axis]);\n+}\n+\n+unsigned ReduceOpHelper::getIntraWarpSizeWithUniqueData() {\n+  auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n+  return std::min(srcReduceDimSize,\n+                  triton::gpu::getThreadsPerWarpWithUniqueData(\n+                      getSrcLayout(), getSrcShape())[axis]);\n }\n \n unsigned ReduceOpHelper::getThreadsReductionAxis() {\n-  auto srcLayout = srcTy.getEncoding();\n-  auto axis = op.axis();\n-  return triton::gpu::getThreadsPerWarp(srcLayout)[axis] *\n-         triton::gpu::getWarpsPerCTA(srcLayout)[axis];\n+  auto srcLayout = getSrcLayout();\n+  auto srcShape = getSrcShape();\n+  return triton::gpu::getThreadsPerWarpWithUniqueData(srcLayout,\n+                                                      srcShape)[axis] *\n+         triton::gpu::getWarpsPerCTAWithUniqueData(srcLayout, srcShape)[axis];\n }\n \n SmallVector<unsigned> ReduceOpHelper::getScratchConfigBasic() {\n-  auto axis = op.axis();\n   auto smemShape = convertType<unsigned>(getSrcShape());\n   smemShape[axis] = std::min(smemShape[axis], getThreadsReductionAxis());\n   return smemShape;\n }\n \n SmallVector<SmallVector<unsigned>> ReduceOpHelper::getScratchConfigsFast() {\n-  auto axis = op.axis();\n   SmallVector<SmallVector<unsigned>> smemShapes(3);\n \n-  auto argLayout = srcTy.getEncoding();\n+  auto argLayout = getSrcLayout();\n   auto argLayoutMma = argLayout.dyn_cast<triton::gpu::MmaEncodingAttr>();\n-  if (argLayoutMma && argLayoutMma.getVersionMajor() == 2 &&\n-      triton::gpu::getWarpsPerCTA(argLayout)[axis] == 1)\n-    return {{1, 1}, {1, 1}};\n+  // if (argLayoutMma && argLayoutMma.getVersionMajor() == 2 &&\n+  //     triton::gpu::getWarpsPerCTA(argLayout)[axis] == 1)\n+  //   return {{1, 1}, {1, 1}};\n \n   /// shared memory block0\n   smemShapes[0] = convertType<unsigned>(getSrcShape());\n   smemShapes[0][axis] = getInterWarpSize();\n \n   /// FIXME(Qingyi): This size is actually larger than required.\n   /// shared memory block1:\n-  auto mod = op.getOperation()->getParentOfType<ModuleOp>();\n+  auto mod = op->getParentOfType<ModuleOp>();\n   unsigned numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n-  smemShapes[1].push_back(numWarps * 32);\n+  unsigned threadsPerWarp =\n+      triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n+  smemShapes[1].push_back(numWarps * threadsPerWarp);\n \n   return smemShapes;\n }\n@@ -78,20 +90,25 @@ unsigned ReduceOpHelper::getScratchSizeInBytes() {\n     elems = product<unsigned>(smemShape);\n   }\n \n-  auto tensorType = op.operand().getType().cast<RankedTensorType>();\n-  unsigned bytes = elems * tensorType.getElementTypeBitWidth() / 8;\n-\n-  if (triton::ReduceOp::withIndex(op.redOp()))\n-    bytes += elems * sizeof(int32_t);\n-\n-  return bytes;\n+  unsigned bytesPerElem = 0;\n+  for (const auto &ty : srcElementTypes) {\n+    bytesPerElem += ty.getIntOrFloatBitWidth() / 8;\n+  }\n+  return bytesPerElem * elems;\n }\n \n-bool isSharedEncoding(Value value) {\n-  auto type = value.getType();\n-  if (auto tensorType = type.dyn_cast<RankedTensorType>()) {\n-    auto encoding = tensorType.getEncoding();\n-    return encoding && encoding.isa<triton::gpu::SharedEncodingAttr>();\n+bool ReduceOpHelper::isSupportedLayout() {\n+  auto srcLayout = getSrcLayout();\n+  if (srcLayout.isa<triton::gpu::BlockedEncodingAttr>()) {\n+    return true;\n+  }\n+  if (auto mmaLayout = srcLayout.dyn_cast<triton::gpu::MmaEncodingAttr>()) {\n+    if (mmaLayout.isAmpere()) {\n+      return true;\n+    }\n+  }\n+  if (auto sliceLayout = srcLayout.dyn_cast<triton::gpu::SliceEncodingAttr>()) {\n+    return true;\n   }\n   return false;\n }\n@@ -105,13 +122,12 @@ bool maybeSharedAllocationOp(Operation *op) {\n          (dialect->getTypeID() ==\n               mlir::TypeID::get<triton::gpu::TritonGPUDialect>() ||\n           dialect->getTypeID() == mlir::TypeID::get<triton::TritonDialect>() ||\n-          dialect->getTypeID() ==\n-              mlir::TypeID::get<arith::ArithmeticDialect>() ||\n+          dialect->getTypeID() == mlir::TypeID::get<arith::ArithDialect>() ||\n           dialect->getTypeID() == mlir::TypeID::get<tensor::TensorDialect>());\n }\n \n bool maybeAliasOp(Operation *op) {\n-  return isa<tensor::ExtractSliceOp>(op) || isa<triton::TransOp>(op) ||\n+  return isa<triton::gpu::ExtractSliceOp>(op) || isa<triton::TransOp>(op) ||\n          isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n          isa<tensor::InsertSliceOp>(op);\n }\n@@ -120,12 +136,12 @@ bool supportMMA(triton::DotOp op, int version) {\n   // Refer to mma section for the data type supported by Volta and Hopper\n   // Tensor Core in\n   // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-fragment-mma-884-f16\n-  auto aElemTy = op.a().getType().cast<RankedTensorType>().getElementType();\n-  auto bElemTy = op.b().getType().cast<RankedTensorType>().getElementType();\n+  auto aElemTy = op.getA().getType().cast<RankedTensorType>().getElementType();\n+  auto bElemTy = op.getB().getType().cast<RankedTensorType>().getElementType();\n   if (aElemTy.isF32() && bElemTy.isF32()) {\n-    return op.allowTF32() && version >= 2;\n+    return op.getAllowTF32() && version >= 2;\n   }\n-  return supportMMA(op.a(), version) && supportMMA(op.b(), version);\n+  return supportMMA(op.getA(), version) && supportMMA(op.getB(), version);\n }\n \n bool supportMMA(Value value, int version) {\n@@ -147,24 +163,69 @@ Type getElementType(Value value) {\n   return type;\n }\n \n-std::string getValueOperandName(Value value, AsmState &state) {\n-  std::string opName;\n-  llvm::raw_string_ostream ss(opName);\n-  value.printAsOperand(ss, state);\n-  return opName;\n-}\n-\n-bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n-                        triton::gpu::DotOperandEncodingAttr &dotOperandLayout) {\n+bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy) {\n   // dot_op<opIdx=0, parent=#mma> = #mma\n   // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n+  auto srcLayout = srcTy.getEncoding();\n+  auto dstLayout = dstTy.getEncoding();\n+  auto mmaLayout = srcLayout.cast<triton::gpu::MmaEncodingAttr>();\n+  auto dotOperandLayout = dstLayout.cast<triton::gpu::DotOperandEncodingAttr>();\n   return mmaLayout.getVersionMajor() == 2 &&\n          mmaLayout.getWarpsPerCTA()[1] == 1 &&\n          dotOperandLayout.getOpIdx() == 0 &&\n-         dotOperandLayout.getParent() == mmaLayout;\n+         dotOperandLayout.getParent() == mmaLayout &&\n+         !srcTy.getElementType().isF32();\n+}\n+\n+bool isSingleValue(Value value) {\n+  // Don't consider load as expensive if it is loading a scalar.\n+  if (auto tensorTy = value.getType().dyn_cast<RankedTensorType>())\n+    return tensorTy.getNumElements() == 1;\n+  // TODO: Handle other cases.\n+  // For example, when ptr is a tensor of single value.\n+  // It means that ptr is a resultant of broadcast or generated through\n+  // a chain of broadcast and other operations.\n+  // Rematerialize it without considering contiguous memory access pattern is\n+  // fine.\n+  return true;\n }\n \n namespace {\n+\n+/// A data structure similar to SetVector but maintains\n+/// a deque instead of a vector to allow for efficient\n+/// push_back and pop_front operations.\n+/// Using SetVector doesn't suffice our needs because\n+/// it only pushes and pops from the back.\n+/// For example, if we have a queue like this:\n+/// 0->4 1->2->3\n+///    ^--------\n+/// where 3 depends on 4, once we pop 3, we found\n+/// 4 is not ready, so we check 2 and push 3 back\n+/// to the queue.\n+struct DFSSubgraphState {\n+  DFSSubgraphState() : set(), deque() {}\n+  DenseSet<Operation *> set;\n+  std::deque<Operation *> deque;\n+\n+  bool push_back(Operation *op) {\n+    if (set.insert(op).second) {\n+      deque.push_back(op);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  Operation *pop_front() {\n+    Operation *op = deque.front();\n+    deque.pop_front();\n+    set.erase(op);\n+    return op;\n+  }\n+\n+  bool empty() { return deque.empty(); }\n+};\n+\n /// DFS post-order implementation that maintains a global count to work across\n /// multiple invocations, to help implement topological sort on multi-root DAGs.\n /// We traverse all operations but only record the ones that appear in\n@@ -174,23 +235,49 @@ struct DFSState {\n   const SetVector<Operation *> &toSort;\n   SmallVector<Operation *, 16> topologicalCounts;\n   DenseSet<Operation *> seen;\n+\n+  /// We mark each op as ready if all its operands are seen. If an op is ready,\n+  /// we add it to the queue. Otherwise, we keep adding its operands to the\n+  /// ancestors set.\n+  void addToReadyQueue(Operation *op, DFSSubgraphState &subGraph,\n+                       SmallVector<Operation *, 4> &readyQueue) {\n+    bool ready = true;\n+    for (Value operand : op->getOperands()) {\n+      auto def = operand.getDefiningOp();\n+      if (def && !seen.count(def)) {\n+        subGraph.push_back(def);\n+        ready = false;\n+      }\n+    }\n+    if (ready)\n+      readyQueue.push_back(op);\n+  }\n };\n \n void dfsPostorder(Operation *root, DFSState *state) {\n-  SmallVector<Operation *> queue(1, root);\n-  std::vector<Operation *> ops;\n-  while (!queue.empty()) {\n-    Operation *current = queue.pop_back_val();\n-    if (!state->seen.insert(current).second)\n-      continue;\n-    ops.push_back(current);\n-    for (Value result : current->getResults()) {\n-      for (Operation *op : result.getUsers())\n-        queue.push_back(op);\n-    }\n-    for (Region &region : current->getRegions()) {\n-      for (Operation &op : region.getOps())\n-        queue.push_back(&op);\n+  DFSSubgraphState subGraph;\n+  subGraph.push_back(root);\n+  SmallVector<Operation *> ops;\n+  while (!subGraph.empty()) {\n+    // Nodes in the ready queue are ready to be processed.\n+    // Meaning that either their operands are all seen or they have null\n+    // operands.\n+    SmallVector<Operation *, 4> readyQueue;\n+    auto *current = subGraph.pop_front();\n+    state->addToReadyQueue(current, subGraph, readyQueue);\n+    while (!readyQueue.empty()) {\n+      Operation *current = readyQueue.pop_back_val();\n+      if (!state->seen.insert(current).second)\n+        continue;\n+      ops.push_back(current);\n+      for (Value result : current->getResults()) {\n+        for (Operation *op : result.getUsers())\n+          state->addToReadyQueue(op, subGraph, readyQueue);\n+      }\n+      for (Region &region : current->getRegions()) {\n+        for (Operation &op : region.getOps())\n+          state->addToReadyQueue(&op, subGraph, readyQueue);\n+      }\n     }\n   }\n \n@@ -225,4 +312,88 @@ multiRootTopologicalSort(const SetVector<Operation *> &toSort) {\n   return res;\n }\n \n+SetVector<Operation *> multiRootGetSlice(Operation *op,\n+                                         TransitiveFilter backwardFilter,\n+                                         TransitiveFilter forwardFilter) {\n+  SetVector<Operation *> slice;\n+  slice.insert(op);\n+\n+  unsigned currentIndex = 0;\n+  SetVector<Operation *> backwardSlice;\n+  SetVector<Operation *> forwardSlice;\n+  while (currentIndex != slice.size()) {\n+    auto *currentOp = (slice)[currentIndex];\n+    // Compute and insert the backwardSlice starting from currentOp.\n+    backwardSlice.clear();\n+    getBackwardSlice(currentOp, &backwardSlice, backwardFilter);\n+    slice.insert(backwardSlice.begin(), backwardSlice.end());\n+\n+    // Compute and insert the forwardSlice starting from currentOp.\n+    forwardSlice.clear();\n+    getForwardSlice(currentOp, &forwardSlice, forwardFilter);\n+    slice.insert(forwardSlice.begin(), forwardSlice.end());\n+    ++currentIndex;\n+  }\n+  return multiRootTopologicalSort(slice);\n+}\n+\n+namespace {\n+// Copied from TestDeadCodeAnalysis.cpp, because some dead code analysis\n+// interacts with constant propagation, but SparseConstantPropagation\n+// doesn't seem to be sufficient.\n+class ConstantAnalysis : public DataFlowAnalysis {\n+public:\n+  using DataFlowAnalysis::DataFlowAnalysis;\n+\n+  LogicalResult initialize(Operation *top) override {\n+    WalkResult result = top->walk([&](Operation *op) {\n+      if (failed(visit(op)))\n+        return WalkResult::interrupt();\n+      return WalkResult::advance();\n+    });\n+    return success(!result.wasInterrupted());\n+  }\n+\n+  LogicalResult visit(ProgramPoint point) override {\n+    Operation *op = point.get<Operation *>();\n+    Attribute value;\n+    if (matchPattern(op, m_Constant(&value))) {\n+      auto *constant = getOrCreate<dataflow::Lattice<dataflow::ConstantValue>>(\n+          op->getResult(0));\n+      propagateIfChanged(constant, constant->join(dataflow::ConstantValue(\n+                                       value, op->getDialect())));\n+      return success();\n+    }\n+    // Dead code analysis requires every operands has initialized ConstantValue\n+    // state before it is visited.\n+    // https://github.com/llvm/llvm-project/blob/2ec1aba2b69faa1de5f71832a48e25aa3b5d5314/mlir/lib/Analysis/DataFlow/DeadCodeAnalysis.cpp#L322\n+    // That's why we need to set all operands to unknown constants.\n+    setAllToUnknownConstants(op->getResults());\n+    for (Region &region : op->getRegions()) {\n+      for (Block &block : region.getBlocks())\n+        setAllToUnknownConstants(block.getArguments());\n+    }\n+    return success();\n+  }\n+\n+private:\n+  /// Set all given values as not constants.\n+  void setAllToUnknownConstants(ValueRange values) {\n+    dataflow::ConstantValue unknownConstant(nullptr, nullptr);\n+    for (Value value : values) {\n+      auto *constant =\n+          getOrCreate<dataflow::Lattice<dataflow::ConstantValue>>(value);\n+      propagateIfChanged(constant, constant->join(unknownConstant));\n+    }\n+  }\n+};\n+} // namespace\n+\n+std::unique_ptr<DataFlowSolver> createDataFlowSolver() {\n+  auto solver = std::make_unique<DataFlowSolver>();\n+  solver->load<dataflow::DeadCodeAnalysis>();\n+  solver->load<ConstantAnalysis>();\n+  return solver;\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 14, "deletions": 1, "changes": 15, "file_content_changes": "@@ -1,5 +1,15 @@\n add_mlir_conversion_library(TritonGPUToLLVM\n+    TritonGPUToLLVM.cpp\n+    GCNAsmFormat.cpp\n+    PTXAsmFormat.cpp\n+    TritonGPUToLLVMPass.cpp\n+    ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp\n+    ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp\n+    ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp\n     ConvertLayoutOpToLLVM.cpp\n+    DotOpToLLVM/FMA.cpp\n+    DotOpToLLVM/MMAv1.cpp\n+    DotOpToLLVM/MMAv2.cpp\n     DotOpToLLVM.cpp\n     ElementwiseOpToLLVM.cpp\n     LoadStoreOpToLLVM.cpp\n@@ -8,13 +18,15 @@ add_mlir_conversion_library(TritonGPUToLLVM\n     PTXAsmFormat.cpp\n     ReduceOpToLLVM.cpp\n     Utility.cpp\n+    TypeConverter.cpp\n     ViewOpToLLVM.cpp\n \n     ADDITIONAL_HEADER_DIRS\n     ${PROJECT_SOURCE_DIR}/include/triton/Conversion/TritonGPUToLLVM\n+    ${PROJECT_BINARY_DIR}/include/triton/Conversion/TritonGPUToLLVM\n \n     DEPENDS\n-    TritonConversionPassIncGen\n+    TritonGPUConversionPassIncGen\n \n     LINK_COMPONENTS\n     Core\n@@ -24,6 +36,7 @@ add_mlir_conversion_library(TritonGPUToLLVM\n     MLIRPass\n     MLIRGPUOps\n     MLIRGPUToNVVMTransforms\n+    MLIRGPUToROCDLTransforms\n     MLIRGPUTransforms\n     TritonAnalysis\n     TritonIR"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 202, "deletions": 138, "changes": 340, "file_content_changes": "@@ -1,23 +1,52 @@\n #include \"ConvertLayoutOpToLLVM.h\"\n-#include \"DotOpHelpers.h\"\n #include \"Utility.h\"\n \n-using ::mlir::LLVM::DotOpFMAConversionHelper;\n-using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n-using ::mlir::LLVM::getElementsFromStruct;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStridesFromShapeAndOrder;\n-using ::mlir::LLVM::getStructFromElements;\n-using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getContigPerThread;\n-using ::mlir::triton::gpu::getElemsPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::getTotalElemsPerThread;\n using ::mlir::triton::gpu::isaDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n+// Forward declarations\n+\n+namespace SharedToDotOperandMMAv1 {\n+using CoordTy = SmallVector<Value>;\n+using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+\n+SmallVector<CoordTy> getMNCoords(Value thread,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 ArrayRef<unsigned int> wpt,\n+                                 const MmaEncodingAttr &mmaLayout,\n+                                 ArrayRef<int64_t> shape, bool isARow,\n+                                 bool isBRow, bool isAVec4, bool isBVec4);\n+\n+Value convertLayout(int opIdx, Value tensor, const SharedMemoryObject &smemObj,\n+                    Value thread, Location loc,\n+                    TritonGPUToLLVMTypeConverter *typeConverter,\n+                    ConversionPatternRewriter &rewriter, Type resultTy);\n+\n+} // namespace SharedToDotOperandMMAv1\n+\n+namespace SharedToDotOperandMMAv2 {\n+Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,\n+                    Location loc, Value tensor,\n+                    DotOperandEncodingAttr bEncoding,\n+                    const SharedMemoryObject &smemObj,\n+                    TritonGPUToLLVMTypeConverter *typeConverter, Value thread);\n+}\n+\n+namespace SharedToDotOperandFMA {\n+Value convertLayout(int opIdx, Value B, Value llB, BlockedEncodingAttr dLayout,\n+                    Value thread, Location loc,\n+                    TritonGPUToLLVMTypeConverter *typeConverter,\n+                    ConversionPatternRewriter &rewriter);\n+}\n+\n struct ConvertLayoutOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {\n public:\n@@ -27,8 +56,8 @@ struct ConvertLayoutOpConversion\n   LogicalResult\n   matchAndRewrite(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    Value src = op.src();\n-    Value dst = op.result();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n@@ -48,6 +77,10 @@ struct ConvertLayoutOpConversion\n         dstLayout.isa<DotOperandEncodingAttr>()) {\n       return lowerMmaToDotOperand(op, adaptor, rewriter);\n     }\n+    if (srcLayout.isa<SharedEncodingAttr>() &&\n+        isaDistributedLayout(dstLayout)) {\n+      return lowerSharedToDistributed(op, adaptor, rewriter);\n+    }\n     // TODO: to be implemented\n     llvm_unreachable(\"unsupported layout conversion\");\n     return failure();\n@@ -56,30 +89,43 @@ struct ConvertLayoutOpConversion\n private:\n   SmallVector<Value> getMultiDimOffset(Attribute layout, Location loc,\n                                        ConversionPatternRewriter &rewriter,\n-                                       unsigned elemId, ArrayRef<int64_t> shape,\n+                                       unsigned elemId, RankedTensorType type,\n                                        ArrayRef<unsigned> multiDimCTAInRepId,\n                                        ArrayRef<unsigned> shapePerCTA) const {\n+    auto shape = type.getShape();\n     unsigned rank = shape.size();\n     if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n       auto multiDimOffsetFirstElem =\n-          emitBaseIndexForLayout(loc, rewriter, blockedLayout, shape);\n+          emitBaseIndexForLayout(loc, rewriter, blockedLayout, type);\n       SmallVector<Value> multiDimOffset(rank);\n       SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n           elemId, getSizePerThread(layout), getOrder(layout));\n       for (unsigned d = 0; d < rank; ++d) {\n         multiDimOffset[d] = add(multiDimOffsetFirstElem[d],\n-                                idx_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n+                                i32_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n                                         multiDimElemId[d]));\n       }\n       return multiDimOffset;\n     }\n     if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n       unsigned dim = sliceLayout.getDim();\n-      auto multiDimOffsetParent =\n-          getMultiDimOffset(sliceLayout.getParent(), loc, rewriter, elemId,\n-                            sliceLayout.paddedShape(shape),\n-                            sliceLayout.paddedShape(multiDimCTAInRepId),\n-                            sliceLayout.paddedShape(shapePerCTA));\n+      auto parentEncoding = sliceLayout.getParent();\n+      auto parentSizePerThread = getSizePerThread(parentEncoding);\n+      auto parentShape = sliceLayout.paddedShape(shape);\n+      auto parentTy = RankedTensorType::get(parentShape, type.getElementType(),\n+                                            parentEncoding);\n+      auto offsets = emitOffsetForLayout(layout, type);\n+      auto parentOffset = emitOffsetForLayout(parentEncoding, parentTy);\n+      SmallVector<int> idxs;\n+      for (SmallVector<unsigned> off : offsets) {\n+        off.insert(off.begin() + dim, 0);\n+        auto it = std::find(parentOffset.begin(), parentOffset.end(), off);\n+        idxs.push_back(std::distance(parentOffset.begin(), it));\n+      }\n+      auto multiDimOffsetParent = getMultiDimOffset(\n+          parentEncoding, loc, rewriter, idxs[elemId], parentTy,\n+          sliceLayout.paddedShape(multiDimCTAInRepId),\n+          sliceLayout.paddedShape(shapePerCTA));\n       SmallVector<Value> multiDimOffset(rank);\n       for (unsigned d = 0; d < rank + 1; ++d) {\n         if (d == dim)\n@@ -93,21 +139,21 @@ struct ConvertLayoutOpConversion\n       SmallVector<Value> mmaColIdx(4);\n       SmallVector<Value> mmaRowIdx(2);\n       Value threadId = getThreadId(rewriter, loc);\n-      Value warpSize = idx_val(32);\n+      Value warpSize = i32_val(32);\n       Value laneId = urem(threadId, warpSize);\n       Value warpId = udiv(threadId, warpSize);\n       // TODO: fix the bug in MMAEncodingAttr document\n       SmallVector<Value> multiDimWarpId(2);\n-      multiDimWarpId[0] = urem(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n-      multiDimWarpId[1] = udiv(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n-      Value _1 = idx_val(1);\n-      Value _2 = idx_val(2);\n-      Value _4 = idx_val(4);\n-      Value _8 = idx_val(8);\n-      Value _16 = idx_val(16);\n+      multiDimWarpId[0] = urem(warpId, i32_val(mmaLayout.getWarpsPerCTA()[0]));\n+      multiDimWarpId[1] = udiv(warpId, i32_val(mmaLayout.getWarpsPerCTA()[0]));\n+      Value _1 = i32_val(1);\n+      Value _2 = i32_val(2);\n+      Value _4 = i32_val(4);\n+      Value _8 = i32_val(8);\n+      Value _16 = i32_val(16);\n       if (mmaLayout.isAmpere()) {\n-        multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n-        multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 8));\n+        multiDimWarpId[0] = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n+        multiDimWarpId[1] = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n         Value mmaGrpId = udiv(laneId, _4);\n         Value mmaGrpIdP8 = add(mmaGrpId, _8);\n         Value mmaThreadIdInGrp = urem(laneId, _4);\n@@ -131,16 +177,16 @@ struct ConvertLayoutOpConversion\n         multiDimOffset[0] = elemId < 2 ? mmaRowIdx[0] : mmaRowIdx[1];\n         multiDimOffset[1] = elemId % 2 == 0 ? mmaColIdx[0] : mmaColIdx[1];\n         multiDimOffset[0] = add(\n-            multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n+            multiDimOffset[0], i32_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n         multiDimOffset[1] = add(\n-            multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n+            multiDimOffset[1], i32_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n       } else if (mmaLayout.isVolta()) {\n-        auto [isARow, isBRow, isAVec4, isBVec4, mmaId] =\n+        auto [isARow, isBRow, isAVec4, isBVec4, _] =\n             mmaLayout.decodeVoltaLayoutStates();\n-        auto coords = DotOpMmaV1ConversionHelper::getMNCoords(\n-            threadId, rewriter, mmaLayout.getWarpsPerCTA(), shape, isARow,\n-            isBRow, isAVec4, isBVec4);\n-        return DotOpMmaV1ConversionHelper::getCoord(elemId, coords);\n+        auto coords = SharedToDotOperandMMAv1::getMNCoords(\n+            threadId, rewriter, mmaLayout.getWarpsPerCTA(), mmaLayout, shape,\n+            isARow, isBRow, isAVec4, isBVec4);\n+        return coords[elemId];\n       } else {\n         llvm_unreachable(\"Unexpected MMALayout version\");\n       }\n@@ -159,9 +205,6 @@ struct ConvertLayoutOpConversion\n                       Value smemBase) const {\n     auto accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n     auto layout = type.getEncoding();\n-    auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>();\n-    auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>();\n-    auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>();\n     auto rank = type.getRank();\n     auto sizePerThread = getSizePerThread(layout);\n     auto accumSizePerThread = product<unsigned>(sizePerThread);\n@@ -198,7 +241,7 @@ struct ConvertLayoutOpConversion\n       //       of performance issue observed.\n       for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n         SmallVector<Value> multiDimOffset =\n-            getMultiDimOffset(layout, loc, rewriter, elemId, type.getShape(),\n+            getMultiDimOffset(layout, loc, rewriter, elemId, type,\n                               multiDimCTAInRepId, shapePerCTA);\n         Value offset =\n             linearize(rewriter, loc, multiDimOffset, paddedRepShape, outOrd);\n@@ -215,13 +258,13 @@ struct ConvertLayoutOpConversion\n               currVal = zext(llvmElemTy, currVal);\n             else if (isPtr)\n               currVal = ptrtoint(llvmElemTy, currVal);\n-            valVec = insert_element(vecTy, valVec, currVal, idx_val(v));\n+            valVec = insert_element(vecTy, valVec, currVal, i32_val(v));\n           }\n           store(valVec, ptr);\n         } else {\n           Value valVec = load(ptr);\n           for (unsigned v = 0; v < vec; ++v) {\n-            Value currVal = extract_element(llvmElemTy, valVec, idx_val(v));\n+            Value currVal = extract_element(llvmElemTy, valVec, i32_val(v));\n             if (isInt1)\n               currVal = icmp_ne(currVal,\n                                 rewriter.create<LLVM::ConstantOp>(\n@@ -235,9 +278,9 @@ struct ConvertLayoutOpConversion\n     }\n   }\n \n-  // The MMAV1's result is quite different from the exising \"Replica\" structure,\n-  // add a new simple but clear implementation for it to avoid modificating the\n-  // logic of the exising one.\n+  // The MMAV1's result is quite different from the existing \"Replica\"\n+  // structure, add a new simple but clear implementation for it to avoid\n+  // modifying the logic of the existing one.\n   void processReplicaForMMAV1(Location loc, ConversionPatternRewriter &rewriter,\n                               bool stNotRd, RankedTensorType type,\n                               ArrayRef<unsigned> multiDimRepId, unsigned vec,\n@@ -288,18 +331,16 @@ struct ConvertLayoutOpConversion\n         // TODO[Superjomn]: Move the coordinate computation out of loop, it is\n         // duplicate in Volta.\n         SmallVector<Value> multiDimOffset =\n-            getMultiDimOffset(layout, loc, rewriter, elemId, type.getShape(),\n+            getMultiDimOffset(layout, loc, rewriter, elemId, type,\n                               multiDimCTAInRepId, shapePerCTA);\n         coord2val[elemId] = std::make_pair(multiDimOffset, vals[elemId]);\n       }\n \n       if (needTrans) {\n-        auto [isARow, isBRow, isAVec4, isBVec4, mmaId] =\n-            mma.decodeVoltaLayoutStates();\n-        DotOpMmaV1ConversionHelper helper(mma);\n         // do transpose\n-        int numM = helper.getElemsM(mma.getWarpsPerCTA()[0], shape[0], isARow,\n-                                    isAVec4);\n+        auto aEncoding =\n+            DotOperandEncodingAttr::get(mma.getContext(), 0, mma, 0);\n+        int numM = aEncoding.getMMAv1NumOuter(shape);\n         int numN = accumSizePerThread / numM;\n \n         for (int r = 0; r < numM; r++) {\n@@ -325,13 +366,13 @@ struct ConvertLayoutOpConversion\n         Value valVec = undef(vecTy);\n         for (unsigned v = 0; v < vec; ++v) {\n           auto currVal = coord2valT[elemId + v].second;\n-          valVec = insert_element(vecTy, valVec, currVal, idx_val(v));\n+          valVec = insert_element(vecTy, valVec, currVal, i32_val(v));\n         }\n         store(valVec, ptr);\n       } else {\n         Value valVec = load(ptr);\n         for (unsigned v = 0; v < vec; ++v) {\n-          Value currVal = extract_element(elemTy, valVec, idx_val(v));\n+          Value currVal = extract_element(elemTy, valVec, i32_val(v));\n           vals[elemId + v] = currVal;\n         }\n       }\n@@ -345,8 +386,8 @@ struct ConvertLayoutOpConversion\n                                 OpAdaptor adaptor,\n                                 ConversionPatternRewriter &rewriter) const {\n     auto loc = op.getLoc();\n-    Value src = op.src();\n-    Value dst = op.result();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n@@ -395,13 +436,14 @@ struct ConvertLayoutOpConversion\n     }\n     // Potentially we need to store for multiple CTAs in this replication\n     auto accumNumReplicates = product<unsigned>(numReplicates);\n-    // unsigned elems = getElemsPerThread(srcTy);\n-    auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n+    // unsigned elems = getTotalElemsPerThread(srcTy);\n+    auto vals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n+                                                     rewriter, srcTy);\n     unsigned inVec = 0;\n     unsigned outVec = 0;\n     auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n \n-    unsigned outElems = getElemsPerThread(dstTy);\n+    unsigned outElems = getTotalElemsPerThread(dstTy);\n     auto outOrd = getOrder(dstLayout);\n     SmallVector<Value> outVals(outElems);\n \n@@ -444,10 +486,42 @@ struct ConvertLayoutOpConversion\n       }\n     }\n \n-    SmallVector<Type> types(outElems, llvmElemTy);\n-    auto *ctx = llvmElemTy.getContext();\n-    Type structTy = struct_ty(types);\n-    Value result = getStructFromElements(loc, outVals, rewriter, structTy);\n+    Value result =\n+        getTypeConverter()->packLLElements(loc, outVals, rewriter, dstTy);\n+    rewriter.replaceOp(op, result);\n+\n+    return success();\n+  }\n+\n+  LogicalResult\n+  lowerSharedToDistributed(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+                           ConversionPatternRewriter &rewriter) const {\n+    auto loc = op.getLoc();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto srcShape = srcTy.getShape();\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto dstShape = dstTy.getShape();\n+    assert(dstShape.size() == 2 &&\n+           \"Unexpected rank of ConvertLayout(shared->blocked)\");\n+    auto srcSharedLayout = srcTy.getEncoding().cast<SharedEncodingAttr>();\n+    auto dstLayout = dstTy.getEncoding();\n+    auto inOrd = getOrder(srcSharedLayout);\n+\n+    auto smemObj =\n+        getSharedMemoryObjectFromStruct(loc, adaptor.getSrc(), rewriter);\n+    auto elemTy = getTypeConverter()->convertType(dstTy.getElementType());\n+\n+    auto srcStrides =\n+        getStridesFromShapeAndOrder(srcShape, inOrd, loc, rewriter);\n+    auto dstIndices = emitIndices(loc, rewriter, dstLayout, dstTy);\n+\n+    SmallVector<Value> outVals = loadSharedToDistributed(\n+        dst, dstIndices, src, smemObj, elemTy, loc, rewriter);\n+\n+    Value result =\n+        getTypeConverter()->packLLElements(loc, outVals, rewriter, dstTy);\n     rewriter.replaceOp(op, result);\n \n     return success();\n@@ -460,8 +534,8 @@ struct ConvertLayoutOpConversion\n   lowerDistributedToShared(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n                            ConversionPatternRewriter &rewriter) const {\n     auto loc = op.getLoc();\n-    Value src = op.src();\n-    Value dst = op.result();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto srcShape = srcTy.getShape();\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n@@ -479,8 +553,8 @@ struct ConvertLayoutOpConversion\n \n     auto dstStrides =\n         getStridesFromShapeAndOrder(dstShape, outOrd, loc, rewriter);\n-    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n-    storeDistributedToShared(src, adaptor.src(), dstStrides, srcIndices, dst,\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy);\n+    storeDistributedToShared(src, adaptor.getSrc(), dstStrides, srcIndices, dst,\n                              smemBase, elemTy, loc, rewriter);\n     auto smemObj =\n         SharedMemoryObject(smemBase, dstShape, outOrd, loc, rewriter);\n@@ -494,8 +568,8 @@ struct ConvertLayoutOpConversion\n   lowerSharedToDotOperand(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n                           ConversionPatternRewriter &rewriter) const {\n     auto loc = op.getLoc();\n-    Value src = op.src();\n-    Value dst = op.result();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n     auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n     auto srcTensorTy = src.getType().cast<RankedTensorType>();\n     auto dotOperandLayout =\n@@ -520,15 +594,10 @@ struct ConvertLayoutOpConversion\n                        .dyn_cast_or_null<BlockedEncodingAttr>()) {\n       auto dotOpLayout =\n           dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n-      DotOpFMAConversionHelper helper(blockedLayout);\n       auto thread = getThreadId(rewriter, loc);\n-      if (dotOpLayout.getOpIdx() == 0) { // $a\n-        res = helper.loadA(src, adaptor.src(), blockedLayout, thread, loc,\n-                           rewriter);\n-      } else { // $b\n-        res = helper.loadB(src, adaptor.src(), blockedLayout, thread, loc,\n-                           rewriter);\n-      }\n+      res = SharedToDotOperandFMA::convertLayout(\n+          dotOpLayout.getOpIdx(), src, adaptor.getSrc(), blockedLayout, thread,\n+          loc, getTypeConverter(), rewriter);\n     } else {\n       assert(false && \"Unsupported dot operand layout found\");\n     }\n@@ -542,30 +611,46 @@ struct ConvertLayoutOpConversion\n   lowerMmaToDotOperand(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n                        ConversionPatternRewriter &rewriter) const {\n     auto loc = op.getLoc();\n-    auto srcTy = op.src().getType().cast<RankedTensorType>();\n-    auto dstTy = op.result().getType().cast<RankedTensorType>();\n-    auto srcLayout = srcTy.getEncoding();\n-    auto dstLayout = dstTy.getEncoding();\n-    auto srcMmaLayout = srcLayout.cast<MmaEncodingAttr>();\n-    auto dstDotLayout = dstLayout.cast<DotOperandEncodingAttr>();\n-    if (isMmaToDotShortcut(srcMmaLayout, dstDotLayout)) {\n+    auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n+    auto dstTy = op.getResult().getType().cast<RankedTensorType>();\n+    if (isMmaToDotShortcut(srcTy, dstTy)) {\n       // get source values\n-      auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n-      unsigned elems = getElemsPerThread(srcTy);\n+      auto vals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n+                                                       rewriter, srcTy);\n+      unsigned elems = getTotalElemsPerThread(srcTy);\n       Type elemTy =\n           this->getTypeConverter()->convertType(srcTy.getElementType());\n       // for the destination type, we need to pack values together\n       // so they can be consumed by tensor core operations\n-      unsigned vecSize =\n-          std::max<unsigned>(32 / elemTy.getIntOrFloatBitWidth(), 1);\n-      Type vecTy = vec_ty(elemTy, vecSize);\n-      SmallVector<Type> types(elems / vecSize, vecTy);\n       SmallVector<Value> vecVals;\n-      for (unsigned i = 0; i < elems; i += vecSize) {\n-        Value packed = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n-        for (unsigned j = 0; j < vecSize; j++)\n-          packed = insert_element(vecTy, packed, vals[i + j], i32_val(j));\n-        vecVals.push_back(packed);\n+      SmallVector<Type> types;\n+      // For some reasons, LLVM's NVPTX backend inserts unnecessary (?) integer\n+      // instructions to pack & unpack sub-word integers. A workaround is to\n+      // store the results of ldmatrix in i32\n+      auto elemSize = elemTy.getIntOrFloatBitWidth();\n+      if (auto intTy = elemTy.dyn_cast<IntegerType>() && elemSize <= 16) {\n+        auto fold = 32 / elemSize;\n+        for (unsigned i = 0; i < elems; i += fold) {\n+          Value val = i32_val(0);\n+          for (unsigned j = 0; j < fold; j++) {\n+            auto ext =\n+                shl(i32_ty, zext(i32_ty, vals[i + j]), i32_val(elemSize * j));\n+            val = or_(i32_ty, val, ext);\n+          }\n+          vecVals.push_back(val);\n+        }\n+        elems = elems / (32 / elemSize);\n+        types = SmallVector<Type>(elems, i32_ty);\n+      } else {\n+        unsigned vecSize = std::max<unsigned>(32 / elemSize, 1);\n+        Type vecTy = vec_ty(elemTy, vecSize);\n+        types = SmallVector<Type>(elems / vecSize, vecTy);\n+        for (unsigned i = 0; i < elems; i += vecSize) {\n+          Value packed = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n+          for (unsigned j = 0; j < vecSize; j++)\n+            packed = insert_element(vecTy, packed, vals[i + j], i32_val(j));\n+          vecVals.push_back(packed);\n+        }\n       }\n \n       // This needs to be ordered the same way that\n@@ -575,18 +660,14 @@ struct ConvertLayoutOpConversion\n       // is implemented\n       SmallVector<Value> reorderedVals;\n       for (unsigned i = 0; i < vecVals.size(); i += 4) {\n-        reorderedVals.push_back(vecVals[i]);\n-        reorderedVals.push_back(vecVals[i + 2]);\n-        reorderedVals.push_back(vecVals[i + 1]);\n-        reorderedVals.push_back(vecVals[i + 3]);\n+        reorderedVals.push_back(bitcast(vecVals[i], i32_ty));\n+        reorderedVals.push_back(bitcast(vecVals[i + 2], i32_ty));\n+        reorderedVals.push_back(bitcast(vecVals[i + 1], i32_ty));\n+        reorderedVals.push_back(bitcast(vecVals[i + 3], i32_ty));\n       }\n \n-      // return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n-\n-      Type structTy =\n-          LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n-      Value view =\n-          getStructFromElements(loc, reorderedVals, rewriter, structTy);\n+      Value view = getTypeConverter()->packLLElements(loc, reorderedVals,\n+                                                      rewriter, dstTy);\n       rewriter.replaceOp(op, view);\n       return success();\n     }\n@@ -599,30 +680,22 @@ struct ConvertLayoutOpConversion\n       ConversionPatternRewriter &rewriter, const MmaEncodingAttr &mmaLayout,\n       const DotOperandEncodingAttr &dotOperandLayout, bool isOuter) const {\n     auto loc = op.getLoc();\n-    Value src = op.src();\n-    Value dst = op.result();\n-    bool isHMMA = supportMMA(dst, mmaLayout.getVersionMajor());\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n \n     auto smemObj =\n-        getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n+        getSharedMemoryObjectFromStruct(loc, adaptor.getSrc(), rewriter);\n     Value res;\n \n-    if (!isOuter && mmaLayout.isAmpere() && isHMMA) { // tensor core v2\n-      MMA16816ConversionHelper mmaHelper(src.getType(), mmaLayout,\n-                                         getThreadId(rewriter, loc), rewriter,\n-                                         getTypeConverter(), op.getLoc());\n-\n-      if (dotOperandLayout.getOpIdx() == 0) {\n-        // operand $a\n-        res = mmaHelper.loadA(src, smemObj);\n-      } else if (dotOperandLayout.getOpIdx() == 1) {\n-        // operand $b\n-        res = mmaHelper.loadB(src, smemObj);\n-      }\n-    } else if (!isOuter && mmaLayout.isVolta() && isHMMA) { // tensor core v1\n-      DotOpMmaV1ConversionHelper helper(mmaLayout);\n-      bool isMMAv1Row =\n-          dotOperandLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    if (!isOuter && mmaLayout.isAmpere()) { // tensor core v2\n+\n+      res = SharedToDotOperandMMAv2::convertLayout(\n+          dotOperandLayout.getOpIdx(), rewriter, loc, src, dotOperandLayout,\n+          smemObj, getTypeConverter(), tid_val());\n+\n+    } else if (!isOuter && mmaLayout.isVolta() &&\n+               supportMMA(dst, mmaLayout.getVersionMajor())) { // tensor core v1\n+      bool isMMAv1Row = dotOperandLayout.getMMAv1IsRow();\n       auto srcSharedLayout = src.getType()\n                                  .cast<RankedTensorType>()\n                                  .getEncoding()\n@@ -635,30 +708,21 @@ struct ConvertLayoutOpConversion\n         return Value();\n       }\n \n-      if (dotOperandLayout.getOpIdx() == 0) { // operand $a\n-        // TODO[Superjomn]: transA is not available here.\n-        bool transA = false;\n-        res = helper.loadA(src, smemObj, getThreadId(rewriter, loc), loc,\n-                           rewriter);\n-      } else if (dotOperandLayout.getOpIdx() == 1) { // operand $b\n-        // TODO[Superjomn]: transB is not available here.\n-        bool transB = false;\n-        res = helper.loadB(src, smemObj, getThreadId(rewriter, loc), loc,\n-                           rewriter);\n-      }\n+      res = SharedToDotOperandMMAv1::convertLayout(\n+          dotOperandLayout.getOpIdx(), src, smemObj, getThreadId(rewriter, loc),\n+          loc, getTypeConverter(), rewriter, dst.getType());\n     } else {\n       assert(false && \"Unsupported mma layout found\");\n     }\n     return res;\n   }\n-};\n+}; // namespace triton::gpu::ConvertLayoutOp\n \n void populateConvertLayoutOpToLLVMPatterns(\n-    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem,\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit) {\n-  patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n+  patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation,\n                                           indexCacheInfo, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.h", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -9,9 +9,8 @@ using namespace mlir::triton;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n \n void populateConvertLayoutOpToLLVMPatterns(\n-    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem,\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp", "status": "added", "additions": 227, "deletions": 0, "changes": 227, "file_content_changes": "@@ -0,0 +1,227 @@\n+#include \"../ConvertLayoutOpToLLVM.h\"\n+#include \"../Utility.h\"\n+\n+using ValueTable = std::map<std::pair<int, int>, Value>;\n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getStridesFromShapeAndOrder;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n+using ::mlir::triton::gpu::getOrder;\n+using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::getTotalElemsPerThread;\n+using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+SmallVector<Value>\n+getThreadIds(Value threadId, ArrayRef<unsigned int> shapePerCTA,\n+             ArrayRef<unsigned int> sizePerThread, ArrayRef<unsigned int> order,\n+             ConversionPatternRewriter &rewriter, Location loc) {\n+  int dim = order.size();\n+  SmallVector<Value> threadIds(dim);\n+  for (unsigned k = 0; k < dim - 1; k++) {\n+    Value dimK = i32_val(shapePerCTA[order[k]] / sizePerThread[order[k]]);\n+    Value rem = urem(threadId, dimK);\n+    threadId = udiv(threadId, dimK);\n+    threadIds[order[k]] = rem;\n+  }\n+  Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n+  threadIds[order[dim - 1]] = urem(threadId, dimK);\n+  return threadIds;\n+}\n+\n+int getShapePerCTAForMN(BlockedEncodingAttr layout, bool isM) {\n+  auto order = layout.getOrder();\n+  auto shapePerCTA = getShapePerCTA(layout);\n+\n+  int mShapePerCTA =\n+      order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  int nShapePerCTA =\n+      order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  return isM ? mShapePerCTA : nShapePerCTA;\n+}\n+\n+// Get sizePerThread for M or N axis.\n+int getSizePerThreadForMN(BlockedEncodingAttr layout, bool isM) {\n+  auto order = layout.getOrder();\n+  auto sizePerThread = getSizePerThread(layout);\n+\n+  int mSizePerThread =\n+      order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+  int nSizePerThread =\n+      order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+  return isM ? mSizePerThread : nSizePerThread;\n+}\n+\n+Value getStructFromValueTable(ArrayRef<Value> vals,\n+                              ConversionPatternRewriter &rewriter, Location loc,\n+                              TritonGPUToLLVMTypeConverter *typeConverter,\n+                              Type elemTy) {\n+  SmallVector<Type> elemTypes(vals.size(), elemTy);\n+  SmallVector<Value> elems;\n+  elems.reserve(vals.size());\n+  for (auto &val : vals) {\n+    elems.push_back(val);\n+  }\n+  MLIRContext *ctx = elemTy.getContext();\n+  Type structTy = struct_ty(elemTypes);\n+  return typeConverter->packLLElements(loc, elems, rewriter, structTy);\n+}\n+\n+ValueTable getValueTableFromStruct(Value val, int K, int n0, int shapePerCTA,\n+                                   int sizePerThread,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Location loc,\n+                                   TritonGPUToLLVMTypeConverter *typeConverter,\n+                                   Type type) {\n+  ValueTable res;\n+  auto elems = typeConverter->unpackLLElements(loc, val, rewriter, type);\n+  int index = 0;\n+  for (unsigned k = 0; k < K; ++k) {\n+    for (unsigned m = 0; m < n0; m += shapePerCTA)\n+      for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n+        res[{m + mm, k}] = elems[index++];\n+      }\n+  }\n+  return res;\n+}\n+\n+Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n+               Location loc, TritonGPUToLLVMTypeConverter *typeConverter,\n+               ConversionPatternRewriter &rewriter) {\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto aShape = aTensorTy.getShape();\n+\n+  auto aOrder = aLayout.getOrder();\n+  auto order = dLayout.getOrder();\n+\n+  bool isARow = aOrder[0] == 1;\n+\n+  auto aSmem = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n+  Value strideAM = aSmem.strides[0];\n+  Value strideAK = aSmem.strides[1];\n+  Value strideA0 = isARow ? strideAK : strideAM;\n+  Value strideA1 = isARow ? strideAM : strideAK;\n+  int aNumPtr = 8;\n+  int K = aShape[1];\n+  int M = aShape[0];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value mContig = i32_val(sizePerThread[order[1]]);\n+\n+  // threadId in blocked layout\n+  auto threadIds =\n+      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n+  Value threadIdM = threadIds[0];\n+\n+  Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n+  Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n+  SmallVector<Value> aOff(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i) {\n+    aOff[i] = add(mul(offA0, strideA0), mul(offA1, strideA1));\n+  }\n+  auto elemTy = A.getType().cast<RankedTensorType>().getElementType();\n+\n+  Type ptrTy = ptr_ty(elemTy, 3);\n+  SmallVector<Value> aPtrs(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i)\n+    aPtrs[i] = gep(ptrTy, aSmem.base, aOff[i]);\n+\n+  SmallVector<Value> vas;\n+\n+  int mShapePerCTA = getShapePerCTAForMN(dLayout, true /*isM*/);\n+  int mSizePerThread = getSizePerThreadForMN(dLayout, true /*isM*/);\n+\n+  for (unsigned k = 0; k < K; ++k)\n+    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+      for (unsigned mm = 0; mm < mSizePerThread; ++mm) {\n+        Value offset =\n+            add(mul(i32_val(m + mm), strideAM), mul(i32_val(k), strideAK));\n+        Value pa = gep(ptrTy, aPtrs[0], offset);\n+        Value va = load(pa);\n+        vas.emplace_back(va);\n+      }\n+\n+  return getStructFromValueTable(vas, rewriter, loc, typeConverter, elemTy);\n+}\n+\n+Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n+               Location loc, TritonGPUToLLVMTypeConverter *typeConverter,\n+               ConversionPatternRewriter &rewriter) {\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto bShape = bTensorTy.getShape();\n+\n+  auto bOrder = bLayout.getOrder();\n+  auto order = dLayout.getOrder();\n+\n+  bool isBRow = bOrder[0] == 1;\n+\n+  auto bSmem = getSharedMemoryObjectFromStruct(loc, llB, rewriter);\n+  Value strideBN = bSmem.strides[1];\n+  Value strideBK = bSmem.strides[0];\n+  Value strideB0 = isBRow ? strideBN : strideBK;\n+  Value strideB1 = isBRow ? strideBK : strideBN;\n+  int bNumPtr = 8;\n+  int K = bShape[0];\n+  int N = bShape[1];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value nContig = i32_val(sizePerThread[order[0]]);\n+\n+  // threadId in blocked layout\n+  auto threadIds =\n+      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n+  Value threadIdN = threadIds[1];\n+\n+  Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n+  Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n+  SmallVector<Value> bOff(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i) {\n+    bOff[i] = add(mul(offB0, strideB0), mul(offB1, strideB1));\n+  }\n+  auto elemTy = B.getType().cast<RankedTensorType>().getElementType();\n+\n+  Type ptrTy = ptr_ty(elemTy, 3);\n+  SmallVector<Value> bPtrs(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i)\n+    bPtrs[i] = gep(ptrTy, bSmem.base, bOff[i]);\n+\n+  SmallVector<Value> vbs;\n+\n+  int nShapePerCTA = getShapePerCTAForMN(dLayout, false /*isM*/);\n+  int nSizePerThread = getSizePerThreadForMN(dLayout, false /*isM*/);\n+\n+  for (unsigned k = 0; k < K; ++k)\n+    for (unsigned n = 0; n < N; n += nShapePerCTA)\n+      for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n+        Value offset =\n+            add(mul(i32_val(n + nn), strideBN), mul(i32_val(k), strideBK));\n+        Value pb = gep(ptrTy, bPtrs[0], offset);\n+        Value vb = load(pb);\n+        vbs.emplace_back(vb);\n+      }\n+\n+  return getStructFromValueTable(vbs, rewriter, loc, typeConverter, elemTy);\n+}\n+\n+namespace SharedToDotOperandFMA {\n+Value convertLayout(int opIdx, Value val, Value llVal,\n+                    BlockedEncodingAttr dLayout, Value thread, Location loc,\n+                    TritonGPUToLLVMTypeConverter *typeConverter,\n+                    ConversionPatternRewriter &rewriter) {\n+  if (opIdx == 0)\n+    return loadAFMA(val, llVal, dLayout, thread, loc, typeConverter, rewriter);\n+  else\n+    return loadBFMA(val, llVal, dLayout, thread, loc, typeConverter, rewriter);\n+}\n+} // namespace SharedToDotOperandFMA"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp", "status": "added", "additions": 460, "deletions": 0, "changes": 460, "file_content_changes": "@@ -0,0 +1,460 @@\n+#include \"../ConvertLayoutOpToLLVM.h\"\n+#include \"../Utility.h\"\n+\n+using CoordTy = SmallVector<Value>;\n+using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getStridesFromShapeAndOrder;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n+using ::mlir::triton::gpu::getOrder;\n+using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::getTotalElemsPerThread;\n+using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+// Compute the offset of the matrix to load.\n+// Returns offsetAM, offsetAK, offsetBN, offsetBK.\n+// NOTE, the information M(from $a) and N(from $b) couldn't be retrieved at\n+// the same time in the usage in convert_layout[shared->dot_op], we leave\n+// the noexist info to be 0 and only use the desired argument from the\n+// composed result. In this way we want to retain the original code\n+// structure in convert_mma884 method for easier debugging.\n+static std::tuple<Value, Value, Value, Value>\n+computeOffsets(Value threadId, bool isARow, bool isBRow, ArrayRef<int> fpw,\n+               ArrayRef<int> spw, ArrayRef<int> rep,\n+               ConversionPatternRewriter &rewriter, Location loc,\n+               Type resultTy) {\n+  auto *ctx = rewriter.getContext();\n+  auto wpt = resultTy.cast<RankedTensorType>()\n+                 .getEncoding()\n+                 .cast<DotOperandEncodingAttr>()\n+                 .getParent()\n+                 .cast<MmaEncodingAttr>()\n+                 .getWarpsPerCTA();\n+\n+  Value _1 = i32_val(1);\n+  Value _3 = i32_val(3);\n+  Value _4 = i32_val(4);\n+  Value _16 = i32_val(16);\n+  Value _32 = i32_val(32);\n+\n+  Value lane = urem(threadId, _32);\n+  Value warp = udiv(threadId, _32);\n+\n+  // warp offset\n+  Value warp0 = urem(warp, i32_val(wpt[0]));\n+  Value warp12 = udiv(warp, i32_val(wpt[0]));\n+  Value warp1 = urem(warp12, i32_val(wpt[1]));\n+  Value warpMOff = mul(warp0, i32_val(spw[0]));\n+  Value warpNOff = mul(warp1, i32_val(spw[1]));\n+  // Quad offset\n+  Value quadMOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[0]));\n+  Value quadNOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[1]));\n+  // Pair offset\n+  Value pairMOff = udiv(urem(lane, _16), _4);\n+  pairMOff = urem(pairMOff, i32_val(fpw[0]));\n+  pairMOff = mul(pairMOff, _4);\n+  Value pairNOff = udiv(urem(lane, _16), _4);\n+  pairNOff = udiv(pairNOff, i32_val(fpw[0]));\n+  pairNOff = urem(pairNOff, i32_val(fpw[1]));\n+  pairNOff = mul(pairNOff, _4);\n+  // scale\n+  pairMOff = mul(pairMOff, i32_val(rep[0] / 2));\n+  quadMOff = mul(quadMOff, i32_val(rep[0] / 2));\n+  pairNOff = mul(pairNOff, i32_val(rep[1] / 2));\n+  quadNOff = mul(quadNOff, i32_val(rep[1] / 2));\n+  // Quad pair offset\n+  Value laneMOff = add(pairMOff, quadMOff);\n+  Value laneNOff = add(pairNOff, quadNOff);\n+  // A offset\n+  Value offsetAM = add(warpMOff, laneMOff);\n+  Value offsetAK = and_(lane, _3);\n+  // B offset\n+  Value offsetBN = add(warpNOff, laneNOff);\n+  Value offsetBK = and_(lane, _3);\n+  // i indices\n+  Value offsetCM = add(and_(lane, _1), offsetAM);\n+  if (isARow) {\n+    offsetAM = add(offsetAM, urem(threadId, _4));\n+    offsetAK = i32_val(0);\n+  }\n+  if (!isBRow) {\n+    offsetBN = add(offsetBN, urem(threadId, _4));\n+    offsetBK = i32_val(0);\n+  }\n+\n+  return std::make_tuple(offsetAM, offsetAK, offsetBN, offsetBK);\n+}\n+\n+static Value loadA(Value tensor, const SharedMemoryObject &smemObj,\n+                   Value thread, Location loc,\n+                   TritonGPUToLLVMTypeConverter *typeConverter,\n+                   ConversionPatternRewriter &rewriter, Type resultTy) {\n+  static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n+  auto wpt = resultTy.cast<RankedTensorType>()\n+                 .getEncoding()\n+                 .cast<DotOperandEncodingAttr>()\n+                 .getParent()\n+                 .cast<MmaEncodingAttr>()\n+                 .getWarpsPerCTA();\n+\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto shape = tensorTy.getShape();\n+  auto order = sharedLayout.getOrder();\n+\n+  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+  Value smemBase = smemObj.getBaseBeforeSlice(order[0], loc, rewriter);\n+\n+  bool isARow = order[0] != 0;\n+  auto resultEncoding = resultTy.cast<RankedTensorType>()\n+                            .getEncoding()\n+                            .cast<DotOperandEncodingAttr>();\n+  auto [offsetAM, offsetAK, _3, _4] = computeOffsets(\n+      thread, isARow, false, fpw, resultEncoding.getMMAv1ShapePerWarp(),\n+      resultEncoding.getMMAv1Rep(), rewriter, loc, resultTy);\n+\n+  int vecA = sharedLayout.getVec();\n+\n+  auto strides = smemObj.strides;\n+  Value strideAM = isARow ? strides[0] : i32_val(1);\n+  Value strideAK = isARow ? i32_val(1) : strides[1];\n+  Value strideA0 = isARow ? strideAK : strideAM;\n+  Value strideA1 = isARow ? strideAM : strideAK;\n+\n+  int strideRepM = wpt[0] * fpw[0] * 8;\n+  int strideRepK = 1;\n+\n+  // swizzling\n+  int perPhaseA = sharedLayout.getPerPhase();\n+  int maxPhaseA = sharedLayout.getMaxPhase();\n+  int stepA0 = isARow ? strideRepK : strideRepM;\n+  int numPtrA = std::max(2 * perPhaseA * maxPhaseA / stepA0, 1);\n+  int NK = shape[1];\n+\n+  // pre-compute pointer lanes\n+  Value offA0 = isARow ? offsetAK : offsetAM;\n+  Value offA1 = isARow ? offsetAM : offsetAK;\n+  Value phaseA = urem(udiv(offA1, i32_val(perPhaseA)), i32_val(maxPhaseA));\n+  offA0 = add(offA0, cSwizzleOffset);\n+  SmallVector<Value> offA(numPtrA);\n+  for (int i = 0; i < numPtrA; i++) {\n+    Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));\n+    offA0I = udiv(offA0I, i32_val(vecA));\n+    offA0I = xor_(offA0I, phaseA);\n+    offA0I = mul(offA0I, i32_val(vecA));\n+    offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n+  }\n+\n+  Type elemX2Ty = vec_ty(f16_ty, 2);\n+  Type elemPtrTy = ptr_ty(f16_ty, 3);\n+  if (tensorTy.getElementType().isBF16()) {\n+    elemX2Ty = vec_ty(i16_ty, 2);\n+    elemPtrTy = ptr_ty(i16_ty, 3);\n+  }\n+\n+  // prepare arguments\n+  SmallVector<Value> ptrA(numPtrA);\n+\n+  std::map<std::pair<int, int>, std::pair<Value, Value>> has;\n+  for (int i = 0; i < numPtrA; i++)\n+    ptrA[i] = gep(ptr_ty(f16_ty, 3), smemBase, offA[i]);\n+\n+  auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+  auto loadA = [&](int m, int k) {\n+    int offidx = (isARow ? k / 4 : m) % numPtrA;\n+    Value thePtrA = gep(elemPtrTy, smemBase, offA[offidx]);\n+\n+    int stepAM = isARow ? m : m / numPtrA * numPtrA;\n+    int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n+    Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),\n+                       mul(i32_val(stepAK), strideAK));\n+    Value pa = gep(elemPtrTy, thePtrA, offset);\n+    Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n+    Value ha = load(bitcast(pa, aPtrTy));\n+    // record lds that needs to be moved\n+    Value ha00 = bitcast(extract_element(ha, i32_val(0)), elemX2Ty);\n+    Value ha01 = bitcast(extract_element(ha, i32_val(1)), elemX2Ty);\n+    ld(has, m, k, ha00, ha01);\n+\n+    if (vecA > 4) {\n+      Value ha10 = bitcast(extract_element(ha, i32_val(2)), elemX2Ty);\n+      Value ha11 = bitcast(extract_element(ha, i32_val(3)), elemX2Ty);\n+      if (isARow)\n+        ld(has, m, k + 4, ha10, ha11);\n+      else\n+        ld(has, m + 1, k, ha10, ha11);\n+    }\n+  };\n+\n+  bool isARow_ = resultEncoding.getMMAv1IsRow();\n+  bool isAVec4 = resultEncoding.getMMAv1IsVec4();\n+  unsigned numM = resultEncoding.getMMAv1NumOuter(shape);\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      if (!has.count({m, k}))\n+        loadA(m, k);\n+\n+  SmallVector<Value> elems;\n+  elems.reserve(has.size() * 2);\n+  for (auto item : has) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+\n+  Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);\n+  return res;\n+}\n+\n+static Value loadB(Value tensor, const SharedMemoryObject &smemObj,\n+                   Value thread, Location loc,\n+                   TritonGPUToLLVMTypeConverter *typeConverter,\n+                   ConversionPatternRewriter &rewriter, Type resultTy) {\n+  static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n+  auto wpt = resultTy.cast<RankedTensorType>()\n+                 .getEncoding()\n+                 .cast<DotOperandEncodingAttr>()\n+                 .getParent()\n+                 .cast<MmaEncodingAttr>()\n+                 .getWarpsPerCTA();\n+  // smem\n+  auto strides = smemObj.strides;\n+\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+\n+  auto shape = tensorTy.getShape();\n+  auto order = sharedLayout.getOrder();\n+\n+  Value smem = smemObj.getBaseBeforeSlice(order[0], loc, rewriter);\n+  bool isBRow = order[0] != 0; // is row-major in shared memory layout\n+  // isBRow_ indicates whether B is row-major in DotOperand layout\n+  auto resultEncoding = resultTy.cast<RankedTensorType>()\n+                            .getEncoding()\n+                            .cast<DotOperandEncodingAttr>();\n+\n+  int vecB = sharedLayout.getVec();\n+  Value strideBN = isBRow ? i32_val(1) : strides[1];\n+  Value strideBK = isBRow ? strides[0] : i32_val(1);\n+  Value strideB0 = isBRow ? strideBN : strideBK;\n+  Value strideB1 = isBRow ? strideBK : strideBN;\n+  int strideRepN = wpt[1] * fpw[1] * 8;\n+  int strideRepK = 1;\n+\n+  auto [_3, _4, offsetBN, offsetBK] = computeOffsets(\n+      thread, false, isBRow, fpw, resultEncoding.getMMAv1ShapePerWarp(),\n+      resultEncoding.getMMAv1Rep(), rewriter, loc, resultTy);\n+\n+  // swizzling\n+  int perPhaseB = sharedLayout.getPerPhase();\n+  int maxPhaseB = sharedLayout.getMaxPhase();\n+  int stepB0 = isBRow ? strideRepN : strideRepK;\n+  int numPtrB = std::max(2 * perPhaseB * maxPhaseB / stepB0, 1);\n+  int NK = shape[0];\n+\n+  Value offB0 = isBRow ? offsetBN : offsetBK;\n+  Value offB1 = isBRow ? offsetBK : offsetBN;\n+  Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n+  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+\n+  offB0 = add(offB0, cSwizzleOffset);\n+  SmallVector<Value> offB(numPtrB);\n+  for (int i = 0; i < numPtrB; ++i) {\n+    Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));\n+    offB0I = udiv(offB0I, i32_val(vecB));\n+    offB0I = xor_(offB0I, phaseB);\n+    offB0I = mul(offB0I, i32_val(vecB));\n+    offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));\n+  }\n+\n+  Type elemPtrTy = ptr_ty(f16_ty, 3);\n+  Type elemX2Ty = vec_ty(f16_ty, 2);\n+  if (tensorTy.getElementType().isBF16()) {\n+    elemPtrTy = ptr_ty(i16_ty, 3);\n+    elemX2Ty = vec_ty(i16_ty, 2);\n+  }\n+\n+  SmallVector<Value> ptrB(numPtrB);\n+  ValueTable hbs;\n+  for (int i = 0; i < numPtrB; ++i)\n+    ptrB[i] = gep(ptr_ty(f16_ty, 3), smem, offB[i]);\n+\n+  auto ld = [&](decltype(hbs) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+\n+  auto loadB = [&](int n, int K) {\n+    int offidx = (isBRow ? n : K / 4) % numPtrB;\n+    Value thePtrB = ptrB[offidx];\n+\n+    int stepBN = isBRow ? n / numPtrB * numPtrB : n;\n+    int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n+    Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n+                       mul(i32_val(stepBK), strideBK));\n+    Value pb = gep(elemPtrTy, thePtrB, offset);\n+\n+    Value hb =\n+        load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n+    // record lds that needs to be moved\n+    Value hb00 = bitcast(extract_element(hb, i32_val(0)), elemX2Ty);\n+    Value hb01 = bitcast(extract_element(hb, i32_val(1)), elemX2Ty);\n+    ld(hbs, n, K, hb00, hb01);\n+    if (vecB > 4) {\n+      Value hb10 = bitcast(extract_element(hb, i32_val(2)), elemX2Ty);\n+      Value hb11 = bitcast(extract_element(hb, i32_val(3)), elemX2Ty);\n+      if (isBRow)\n+        ld(hbs, n + 1, K, hb10, hb11);\n+      else\n+        ld(hbs, n, K + 4, hb10, hb11);\n+    }\n+  };\n+\n+  bool isBRow_ = resultEncoding.getMMAv1IsRow();\n+  assert(isBRow == isBRow_ && \"B need smem isRow\");\n+  bool isBVec4 = resultEncoding.getMMAv1IsVec4();\n+  unsigned numN = resultEncoding.getMMAv1NumOuter(shape);\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned n = 0; n < numN / 2; ++n) {\n+      if (!hbs.count({n, k}))\n+        loadB(n, k);\n+    }\n+\n+  SmallVector<Value> elems;\n+  for (auto &item : hbs) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+\n+  Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);\n+  return res;\n+}\n+\n+namespace SharedToDotOperandMMAv1 {\n+using CoordTy = SmallVector<Value>;\n+using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+\n+SmallVector<CoordTy> getMNCoords(Value thread,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 ArrayRef<unsigned int> wpt,\n+                                 const MmaEncodingAttr &mmaLayout,\n+                                 ArrayRef<int64_t> shape, bool isARow,\n+                                 bool isBRow, bool isAVec4, bool isBVec4) {\n+  static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n+\n+  auto *ctx = thread.getContext();\n+  auto loc = UnknownLoc::get(ctx);\n+  Value _1 = i32_val(1);\n+  Value _2 = i32_val(2);\n+  Value _4 = i32_val(4);\n+  Value _16 = i32_val(16);\n+  Value _32 = i32_val(32);\n+  Value _fpw0 = i32_val(fpw[0]);\n+  Value _fpw1 = i32_val(fpw[1]);\n+\n+  // A info\n+  auto aEncoding = DotOperandEncodingAttr::get(ctx, 0, mmaLayout, 0);\n+  auto aRep = aEncoding.getMMAv1Rep();\n+  auto aSpw = aEncoding.getMMAv1ShapePerWarp();\n+  // B info\n+  auto bEncoding = DotOperandEncodingAttr::get(ctx, 1, mmaLayout, 0);\n+  auto bSpw = bEncoding.getMMAv1ShapePerWarp();\n+  auto bRep = bEncoding.getMMAv1Rep();\n+\n+  SmallVector<int, 2> rep({aRep[0], bRep[1]});\n+  SmallVector<int, 2> spw({aSpw[0], bSpw[1]});\n+  SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n+\n+  Value lane = urem(thread, _32);\n+  Value warp = udiv(thread, _32);\n+\n+  Value warp0 = urem(warp, i32_val(wpt[0]));\n+  Value warp12 = udiv(warp, i32_val(wpt[0]));\n+  Value warp1 = urem(warp12, i32_val(wpt[1]));\n+\n+  // warp offset\n+  Value offWarpM = mul(warp0, i32_val(spw[0]));\n+  Value offWarpN = mul(warp1, i32_val(spw[1]));\n+  // quad offset\n+  Value offQuadM = mul(udiv(and_(lane, _16), _4), _fpw0);\n+  Value offQuadN = mul(udiv(and_(lane, _16), _4), _fpw1);\n+  // pair offset\n+  Value offPairM = udiv(urem(lane, _16), _4);\n+  offPairM = urem(offPairM, _fpw0);\n+  offPairM = mul(offPairM, _4);\n+  Value offPairN = udiv(urem(lane, _16), _4);\n+  offPairN = udiv(offPairN, _fpw0);\n+  offPairN = urem(offPairN, _fpw1);\n+  offPairN = mul(offPairN, _4);\n+\n+  // sclare\n+  offPairM = mul(offPairM, i32_val(rep[0] / 2));\n+  offQuadM = mul(offQuadM, i32_val(rep[0] / 2));\n+  offPairN = mul(offPairN, i32_val(rep[1] / 2));\n+  offQuadN = mul(offQuadN, i32_val(rep[1] / 2));\n+\n+  // quad pair offset\n+  Value offLaneM = add(offPairM, offQuadM);\n+  Value offLaneN = add(offPairN, offQuadN);\n+  // a, b offset\n+  Value offsetAM = add(offWarpM, offLaneM);\n+  Value offsetBN = add(offWarpN, offLaneN);\n+  // m indices\n+  Value offsetCM = add(and_(lane, _1), offsetAM);\n+  SmallVector<Value> idxM;\n+  for (unsigned m = 0; m < shape[0]; m += shapePerCTA[0])\n+    for (unsigned mm = 0; mm < rep[0]; ++mm)\n+      idxM.push_back(add(offsetCM, i32_val(m + mm * 2)));\n+\n+  // n indices\n+  Value offsetCN = add((and_(lane, _2)), (add(offWarpN, offPairN)));\n+  SmallVector<Value> idxN;\n+  for (int n = 0; n < shape[1]; n += shapePerCTA[1]) {\n+    for (int nn = 0; nn < rep[1]; ++nn) {\n+      idxN.push_back(add(\n+          offsetCN, i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1])));\n+      idxN.push_back(\n+          add(offsetCN,\n+              i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1] + 1)));\n+    }\n+  }\n+\n+  SmallVector<SmallVector<Value>> axes({idxM, idxN});\n+\n+  // product the axis M and axis N to get coords, ported from\n+  // generator::init_idx method from triton2.0\n+\n+  // TODO[Superjomn]: check the order.\n+  SmallVector<CoordTy> coords;\n+  for (Value x1 : axes[1]) {   // N\n+    for (Value x0 : axes[0]) { // M\n+      SmallVector<Value, 2> idx(2);\n+      idx[0] = x0; // M\n+      idx[1] = x1; // N\n+      coords.push_back(std::move(idx));\n+    }\n+  }\n+\n+  return coords; // {M,N} in row-major\n+}\n+\n+Value convertLayout(int opIdx, Value tensor, const SharedMemoryObject &smemObj,\n+                    Value thread, Location loc,\n+                    TritonGPUToLLVMTypeConverter *typeConverter,\n+                    ConversionPatternRewriter &rewriter, Type resultTy) {\n+  if (opIdx == 0)\n+    return loadA(tensor, smemObj, thread, loc, typeConverter, rewriter,\n+                 resultTy);\n+  else {\n+    assert(opIdx == 1);\n+    return loadB(tensor, smemObj, thread, loc, typeConverter, rewriter,\n+                 resultTy);\n+  }\n+}\n+\n+} // namespace SharedToDotOperandMMAv1"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "added", "additions": 612, "deletions": 0, "changes": 612, "file_content_changes": "@@ -0,0 +1,612 @@\n+#include \"../ConvertLayoutOpToLLVM.h\"\n+#include \"../Utility.h\"\n+\n+using namespace mlir;\n+\n+using ValueTable = std::map<std::pair<unsigned, unsigned>, Value>;\n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getStridesFromShapeAndOrder;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n+using ::mlir::triton::gpu::getOrder;\n+using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::getTotalElemsPerThread;\n+using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+// Data loader for mma.16816 instruction.\n+class MMA16816SmemLoader {\n+public:\n+  MMA16816SmemLoader(int warpsPerTile, ArrayRef<uint32_t> order,\n+                     ArrayRef<uint32_t> warpsPerCTA, uint32_t kOrder,\n+                     int kWidth, ArrayRef<Value> smemStrides,\n+                     ArrayRef<int64_t> tileShape, ArrayRef<int> instrShape,\n+                     ArrayRef<int> matShape, int perPhase, int maxPhase,\n+                     int elemBytes, ConversionPatternRewriter &rewriter,\n+                     TritonGPUToLLVMTypeConverter *typeConverter,\n+                     const Location &loc);\n+\n+  // lane = thread % 32\n+  // warpOff = (thread/32) % warpsPerTile(0)\n+  llvm::SmallVector<Value> computeOffsets(Value warpOff, Value lane,\n+                                          Value cSwizzleOffset) {\n+    if (canUseLdmatrix)\n+      return computeLdmatrixMatOffs(warpOff, lane, cSwizzleOffset);\n+    else\n+      return computeLdsMatOffs(warpOff, lane, cSwizzleOffset);\n+    return {};\n+  }\n+\n+  int getNumPtrs() const { return numPtrs; }\n+\n+  // Compute the offset to the matrix this thread(indexed by warpOff and lane)\n+  // mapped to.\n+  SmallVector<Value> computeLdmatrixMatOffs(Value warpId, Value lane,\n+                                            Value cSwizzleOffset);\n+  // compute 8-bit matrix offset.\n+  SmallVector<Value> computeLdsMatOffs(Value warpOff, Value lane,\n+                                       Value cSwizzleOffset);\n+\n+  // Load 4 matrices and returns 4 vec<2> elements.\n+  std::tuple<Value, Value, Value, Value> loadX4(int mat0, int mat1,\n+                                                ArrayRef<Value> ptrs,\n+                                                Type matTy,\n+                                                Type shemPtrTy) const;\n+\n+private:\n+  SmallVector<uint32_t> order;\n+  SmallVector<uint32_t> warpsPerCTA;\n+  int kOrder;\n+  int kWidth;\n+  SmallVector<int64_t> tileShape;\n+  SmallVector<int> instrShape;\n+  SmallVector<int> matShape;\n+  int perPhase;\n+  int maxPhase;\n+  int elemBytes;\n+  ConversionPatternRewriter &rewriter;\n+  const Location &loc;\n+  MLIRContext *ctx{};\n+\n+  // ldmatrix loads a matrix of size stridedMatShape x contiguousMatShape\n+  int contiguousMatShape;\n+  int stridedMatShape;\n+\n+  // Offset in shared memory to increment on the strided axis\n+  // This would be different than the tile shape in the case of a sliced tensor\n+  Value stridedSmemOffset;\n+\n+  bool needTrans;\n+  bool canUseLdmatrix;\n+\n+  int numPtrs;\n+\n+  // Load operations offset in number of Matrices on contiguous and strided axes\n+  int contiguousLoadMatOffset;\n+  int stridedLoadMatOffset;\n+\n+  // Offset in number of matrices to increment on non-k dim within a warp's 2x2\n+  // matrices\n+  int inWarpMatOffset;\n+  // Offset in number of matrices to increment on non-k dim across warps\n+  int warpMatOffset;\n+};\n+\n+SmallVector<Value>\n+MMA16816SmemLoader::computeLdmatrixMatOffs(Value warpId, Value lane,\n+                                           Value cSwizzleOffset) {\n+  // 4x4 matrices\n+  Value rowInMat = urem(lane, i32_val(8)); // row in the 8x8 matrix\n+  Value matIndex =\n+      udiv(lane, i32_val(8)); // linear index of the matrix in the 2x2 matrices\n+\n+  // Decompose matIndex => s_0, s_1, that is the coordinate in 2x2 matrices in a\n+  // warp\n+  Value s0 = urem(matIndex, i32_val(2));\n+  Value s1 = udiv(matIndex, i32_val(2));\n+\n+  // We use different orders for a and b for better performance.\n+  Value kMatArr = kOrder == 1 ? s1 : s0;  // index of matrix on the k dim\n+  Value nkMatArr = kOrder == 1 ? s0 : s1; // index of matrix on the non-k dim\n+\n+  // Matrix coordinates inside a CTA,\n+  // the matrix layout is [2warpsPerTile[0], 2] for A and [2, 2warpsPerTile[1]]\n+  // for B. e.g., Setting warpsPerTile=4, the data layout for A(kOrder=1) is\n+  //   |0 0|  -> 0,1,2,3 are the warpids\n+  //   |0 0|\n+  //   |1 1|\n+  //   |1 1|\n+  //   |2 2|\n+  //   |2 2|\n+  //   |3 3|\n+  //   |3 3|\n+  //\n+  // for B(kOrder=0) is\n+  //   |0 1 2 3 0 1 2 3| -> 0,1,2,3 are the warpids\n+  //   |0 1 2 3 0 1 2 3|\n+  // Note, for each warp, it handles a 2x2 matrices, that is the coordinate\n+  // address (s0,s1) annotates.\n+\n+  Value matOff[2];\n+  matOff[kOrder ^ 1] = add(\n+      mul(warpId, i32_val(warpMatOffset)), // warp offset (kOrder=1)\n+      mul(nkMatArr,\n+          i32_val(inWarpMatOffset))); // matrix offset inside a warp (kOrder=1)\n+  matOff[kOrder] = kMatArr;\n+\n+  // Physical offset (before swizzling)\n+  Value contiguousMatIndex = matOff[order[0]];\n+  Value stridedMatIndex = matOff[order[1]];\n+  // Add the offset of the slice\n+  Value contiguousSliceMatOffset =\n+      udiv(cSwizzleOffset, i32_val(contiguousMatShape));\n+\n+  SmallVector<Value> offs(numPtrs);\n+  Value phase = urem(udiv(rowInMat, i32_val(perPhase)), i32_val(maxPhase));\n+  // To prevent out-of-bound access of B when warpsPerTile * 16 > tile_size.\n+  // In such a case, we need to wrap around the offset of B.\n+  // |0 1 2 3 0 1 2 3| -> | 0(0) 1(1) 2(2) 3(3) |\n+  // |0 1 2 3 0 1 2 3|    | 0(0) 1(1) 2(2) 3(3) |\n+  //          ~~~~~~~ out-of-bound access\n+\n+  Value rowOffset =\n+      urem(add(rowInMat, mul(stridedMatIndex, i32_val(stridedMatShape))),\n+           i32_val(tileShape[order[1]]));\n+  auto contiguousTileNumMats = tileShape[order[0]] / matShape[order[0]];\n+\n+  for (int i = 0; i < numPtrs; ++i) {\n+    Value contiguousIndex =\n+        add(contiguousMatIndex, i32_val(i * contiguousLoadMatOffset));\n+    if (warpsPerCTA[order[0]] > contiguousTileNumMats ||\n+        contiguousTileNumMats % warpsPerCTA[order[0]] != 0)\n+      contiguousIndex = urem(contiguousIndex, i32_val(contiguousTileNumMats));\n+    contiguousIndex = add(contiguousIndex, contiguousSliceMatOffset);\n+    Value contiguousIndexSwizzled = xor_(contiguousIndex, phase);\n+    offs[i] = add(mul(contiguousIndexSwizzled, i32_val(contiguousMatShape)),\n+                  mul(rowOffset, stridedSmemOffset));\n+  }\n+\n+  return offs;\n+}\n+\n+// clang-format off\n+// Each `ldmatrix.x4` loads data as follows when `needTrans == False`:\n+//\n+//               quad width\n+// <----------------------------------------->\n+// vecWidth\n+// <------->\n+//  t0 ... t0  t1 ... t1  t2 ... t2  t3 ... t3   ||  t0 ... t0  t1 ... t1  t2 ... t2  t3 ... t3  /|\\\n+//  t4 ... t4  t5 ... t5  t6 ... t6  t7 ... t7   ||  t4 ... t4  t5 ... t5  t6 ... t6  t7 ... t7   |\n+//  t8 ... t8  t9 ... t9 t10 .. t10 t11 .. t11   ||  t8 ... t8  t9 ... t9 t10 .. t10 t11 .. t11   | quad height\n+// ...                                                                                            |\n+// t28 .. t28 t29 .. t29 t30 .. t30 t31 .. t31   || t28 .. t28 t29 .. t29 t30 .. t30 t31 .. t31  \\|/\n+// --------------------------------------------- || --------------------------------------------\n+//  t0 ... t0  t1 ... t1  t2 ... t2  t3 ... t3   ||  t0 ... t0  t1 ... t1  t2 ... t2  t3 ... t3\n+//  t4 ... t4  t5 ... t5  t6 ... t6  t7 ... t7   ||  t4 ... t4  t5 ... t5  t6 ... t6  t7 ... t7\n+//  t8 ... t8  t9 ... t9 t10 .. t10 t11 .. t11   ||  t8 ... t8  t9 ... t9 t10 .. t10 t11 .. t11\n+// ...\n+// t28 .. t28 t29 .. t29 t30 .. t30 t31 .. t31   || t28 .. t28 t29 .. t29 t30 .. t30 t31 .. t31\n+//\n+// we assume that the phase is < 8 so we don't need to maintain a separate pointer for the two\n+// lower quadrants. This pattern repeats every warpsPerTile[0] (resp. warpsPerTile[1]) blocks\n+// along the row (resp. col) dimension.\n+// clang-format on\n+\n+SmallVector<Value> MMA16816SmemLoader::computeLdsMatOffs(Value warpOff,\n+                                                         Value lane,\n+                                                         Value cSwizzleOffset) {\n+  int cTileShape = tileShape[order[0]];\n+  int sTileShape = tileShape[order[1]];\n+  if (!needTrans) {\n+    std::swap(cTileShape, sTileShape);\n+  }\n+\n+  SmallVector<Value> offs(numPtrs);\n+\n+  int vecWidth = kWidth;\n+  int threadsPerQuad[2] = {8, 4};\n+  int laneWidth = 4;\n+  int laneHeight = 8;\n+  int quadWidth = laneWidth * vecWidth;\n+  int quadHeight = laneHeight;\n+  int numQuadI = 2;\n+\n+  // outer index base\n+  Value iBase = udiv(lane, i32_val(laneWidth));\n+\n+  for (int rep = 0; rep < numPtrs / (2 * vecWidth); ++rep)\n+    for (int quadId = 0; quadId < 2; ++quadId)\n+      for (int elemId = 0; elemId < vecWidth; ++elemId) {\n+        int idx = rep * 2 * vecWidth + quadId * vecWidth + elemId;\n+        // inner index base\n+        Value jBase = mul(urem(lane, i32_val(laneWidth)), i32_val(vecWidth));\n+        jBase = add(jBase, i32_val(elemId));\n+        // inner index offset\n+        Value jOff = i32_val(0);\n+        if (!needTrans) {\n+          jOff = add(jOff, i32_val(quadId));\n+          jOff = add(jOff, i32_val(rep * contiguousLoadMatOffset));\n+        }\n+        // outer index offset\n+        Value iOff = mul(warpOff, i32_val(warpMatOffset));\n+        if (needTrans) {\n+          int pStride = kOrder == 1 ? 1 : 2;\n+          iOff = add(iOff, i32_val(quadId * inWarpMatOffset));\n+          iOff = add(iOff, i32_val(rep * contiguousLoadMatOffset * pStride));\n+        }\n+        // swizzle\n+        if (!needTrans) {\n+          Value phase = urem(udiv(iBase, i32_val(perPhase)), i32_val(maxPhase));\n+          jOff = add(jOff, udiv(cSwizzleOffset, i32_val(quadWidth)));\n+          jOff = xor_(jOff, phase);\n+        } else {\n+          Value phase = urem(udiv(jBase, i32_val(perPhase)), i32_val(maxPhase));\n+          iOff = add(iOff, udiv(cSwizzleOffset, i32_val(quadHeight)));\n+          iOff = xor_(iOff, phase);\n+        }\n+        // To prevent out-of-bound access when tile is too small.\n+        Value i = add(iBase, mul(iOff, i32_val(quadHeight)));\n+        Value j = add(jBase, mul(jOff, i32_val(quadWidth)));\n+        // wrap around the bounds\n+        // i = urem(i, i32_val(cTileShape));\n+        // j = urem(j, i32_val(sTileShape));\n+        if (needTrans) {\n+          offs[idx] = add(i, mul(j, stridedSmemOffset));\n+        } else {\n+          offs[idx] = add(mul(i, stridedSmemOffset), j);\n+        }\n+      }\n+\n+  return offs;\n+}\n+\n+std::tuple<Value, Value, Value, Value>\n+MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> ptrs, Type matTy,\n+                           Type shemPtrTy) const {\n+  assert(mat0 % 2 == 0 && mat1 % 2 == 0 && \"smem matrix load must be aligned\");\n+  int matIdx[2] = {mat0, mat1};\n+\n+  int ptrIdx{-1};\n+\n+  if (canUseLdmatrix)\n+    ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n+  else\n+    ptrIdx = matIdx[order[0]] * 4 / elemBytes;\n+\n+  // The main difference with the original triton code is we removed the\n+  // prefetch-related logic here for the upstream optimizer phase should\n+  // take care with it, and that is transparent in dot conversion.\n+  auto getPtr = [&](int idx) { return ptrs[idx]; };\n+  Value ptr = getPtr(ptrIdx);\n+\n+  // The struct should have exactly the same element types.\n+  auto resTy = matTy.cast<LLVM::LLVMStructType>();\n+  Type elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n+\n+  // For some reasons, LLVM's NVPTX backend inserts unnecessary (?) integer\n+  // instructions to pack & unpack sub-word integers. A workaround is to\n+  // store the results of ldmatrix in i32\n+  if (auto vecElemTy = elemTy.dyn_cast<VectorType>()) {\n+    Type elemElemTy = vecElemTy.getElementType();\n+    if (auto intTy = elemElemTy.dyn_cast<IntegerType>()) {\n+      if (intTy.getWidth() <= 16) {\n+        elemTy = rewriter.getI32Type();\n+        resTy =\n+            LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, elemTy));\n+      }\n+    }\n+  }\n+\n+  if (canUseLdmatrix) {\n+    Value stridedOffset =\n+        mul(i32_val(matIdx[order[1]] * stridedLoadMatOffset * stridedMatShape),\n+            stridedSmemOffset);\n+    Value readPtr = gep(shemPtrTy, ptr, stridedOffset);\n+\n+    PTXBuilder builder;\n+    // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n+    // thread.\n+    auto resArgs = builder.newListOperand(4, \"=r\");\n+    auto addrArg = builder.newAddrOperand(readPtr, \"r\");\n+\n+    auto ldmatrix = builder.create(\"ldmatrix.sync.aligned.m8n8.x4\")\n+                        ->o(\"trans\", needTrans /*predicate*/)\n+                        .o(\"shared.b16\");\n+    ldmatrix(resArgs, addrArg);\n+\n+    // The result type is 4xi32, each i32 is composed of 2xf16\n+    // elements (adjacent two columns in a row) or a single f32 element.\n+    Value resV4 = builder.launch(rewriter, loc, resTy);\n+    return {extract_val(elemTy, resV4, 0), extract_val(elemTy, resV4, 1),\n+            extract_val(elemTy, resV4, 2), extract_val(elemTy, resV4, 3)};\n+  } else {\n+    if (needTrans && (4 / elemBytes) != kWidth)\n+      llvm_unreachable(\"unimplemented Shared -> DotOperandMmav2 code path\");\n+    // base pointers\n+    std::array<std::array<Value, 4>, 2> ptrs;\n+    int vecWidth = 4 / elemBytes;\n+    for (int i = 0; i < vecWidth; i++)\n+      ptrs[0][i] = getPtr(ptrIdx + i);\n+    for (int i = 0; i < vecWidth; i++)\n+      ptrs[1][i] = getPtr(ptrIdx + i + vecWidth);\n+    // static offsets along outer dimension\n+    int _i0 = matIdx[order[1]] * (stridedLoadMatOffset * stridedMatShape);\n+    int _i1 = _i0;\n+    if (needTrans)\n+      _i1 += stridedLoadMatOffset * stridedMatShape;\n+    else\n+      _i1 += (kOrder == 1 ? 1 : stridedLoadMatOffset) * stridedMatShape;\n+    Value i0 = mul(i32_val(_i0), stridedSmemOffset);\n+    Value i1 = mul(i32_val(_i1), stridedSmemOffset);\n+    std::array<Value, 2> ii = {i0, i1};\n+    // load 4 32-bit values from shared memory\n+    // (equivalent to ldmatrix.x4)\n+    SmallVector<SmallVector<Value>> vptrs(4, SmallVector<Value>(vecWidth));\n+    for (int i = 0; i < 4; ++i)\n+      for (int j = 0; j < vecWidth; ++j)\n+        vptrs[i][j] = gep(shemPtrTy, ptrs[i / 2][j], ii[i % 2]);\n+    // row + trans and col + no-trans are equivalent\n+    bool isActualTrans =\n+        (needTrans && kOrder == 1) || (!needTrans && kOrder == 0);\n+    // pack loaded vectors into 4 32-bit values\n+    int inc = needTrans ? 1 : kWidth;\n+    VectorType packedTy = vec_ty(int_ty(8 * elemBytes), inc);\n+    int canonBits = std::min(32, 8 * elemBytes * inc);\n+    int canonWidth = (8 * elemBytes * inc) / canonBits;\n+    Type canonInt = int_ty(canonBits);\n+    std::array<Value, 4> retElems;\n+    retElems.fill(undef(vec_ty(canonInt, 32 / canonBits)));\n+    for (int r = 0; r < 2; ++r) {\n+      for (int em = 0; em < 2 * vecWidth; em += inc) {\n+        int e = em % vecWidth;\n+        int m = em / vecWidth;\n+        int idx = m * 2 + r;\n+        Value ptr = bitcast(vptrs[idx][e], ptr_ty(packedTy, 3));\n+        Value val = load(ptr);\n+        Value canonval = bitcast(val, vec_ty(canonInt, canonWidth));\n+        for (int w = 0; w < canonWidth; ++w) {\n+          int ridx = idx + w * kWidth / vecWidth;\n+          retElems[ridx] =\n+              insert_element(retElems[ridx],\n+                             extract_element(canonval, i32_val(w)), i32_val(e));\n+        }\n+      }\n+    }\n+    if (isActualTrans)\n+      std::swap(retElems[1], retElems[2]);\n+    return {bitcast(retElems[0], i32_ty), bitcast(retElems[1], i32_ty),\n+            bitcast(retElems[2], i32_ty), bitcast(retElems[3], i32_ty)};\n+  }\n+}\n+\n+MMA16816SmemLoader::MMA16816SmemLoader(\n+    int warpsPerTile, ArrayRef<uint32_t> order, ArrayRef<uint32_t> warpsPerCTA,\n+    uint32_t kOrder, int kWidth, ArrayRef<Value> smemStrides,\n+    ArrayRef<int64_t> tileShape, ArrayRef<int> instrShape,\n+    ArrayRef<int> matShape, int perPhase, int maxPhase, int elemBytes,\n+    ConversionPatternRewriter &rewriter,\n+    TritonGPUToLLVMTypeConverter *typeConverter, const Location &loc)\n+    : order(order.begin(), order.end()),\n+      warpsPerCTA(warpsPerCTA.begin(), warpsPerCTA.end()), kOrder(kOrder),\n+      kWidth(kWidth), tileShape(tileShape.begin(), tileShape.end()),\n+      instrShape(instrShape.begin(), instrShape.end()),\n+      matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n+      maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter), loc(loc),\n+      ctx(rewriter.getContext()) {\n+  contiguousMatShape = matShape[order[0]];\n+  stridedMatShape = matShape[order[1]];\n+\n+  stridedSmemOffset = smemStrides[order[1]];\n+\n+  // rule: k must be the fast-changing axis.\n+  needTrans = kOrder != order[0];\n+  canUseLdmatrix = elemBytes == 2 || (!needTrans);\n+  canUseLdmatrix = canUseLdmatrix && (kWidth == 4 / elemBytes);\n+\n+  if (canUseLdmatrix) {\n+    // Each CTA, the warps is arranged as [1xwarpsPerTile] if not transposed,\n+    // otherwise [warpsPerTilex1], and each warp will perform a mma.\n+    numPtrs = tileShape[order[0]] / (needTrans ? warpsPerTile : 1) /\n+              instrShape[order[0]];\n+  } else {\n+    numPtrs = tileShape[order[0]] / (needTrans ? warpsPerTile : 1) /\n+              matShape[order[0]];\n+    numPtrs *= 4 / elemBytes;\n+  }\n+  numPtrs = std::max<int>(numPtrs, 2);\n+\n+  // Special rule for i8/u8, 4 ptrs for each matrix\n+  // if (!canUseLdmatrix && elemBytes == 1)\n+\n+  int loadOffsetInMat[2];\n+  loadOffsetInMat[kOrder] =\n+      2; // instrShape[kOrder] / matShape[kOrder], always 2\n+  loadOffsetInMat[kOrder ^ 1] =\n+      warpsPerTile * (instrShape[kOrder ^ 1] / matShape[kOrder ^ 1]);\n+\n+  contiguousLoadMatOffset = loadOffsetInMat[order[0]];\n+\n+  stridedLoadMatOffset =\n+      loadOffsetInMat[order[1]] / (instrShape[order[1]] / matShape[order[1]]);\n+\n+  // The stride (in number of matrices) within warp\n+  inWarpMatOffset = kOrder == 1 ? 1 : warpsPerTile;\n+  // The stride (in number of matrices) of each warp\n+  warpMatOffset = instrShape[kOrder ^ 1] / matShape[kOrder ^ 1];\n+}\n+\n+Type getSharedMemPtrTy(Type argType) {\n+  MLIRContext *ctx = argType.getContext();\n+  if (argType.isF16())\n+    return ptr_ty(type::f16Ty(ctx), 3);\n+  else if (argType.isBF16())\n+    return ptr_ty(type::i16Ty(ctx), 3);\n+  else if (argType.isF32())\n+    return ptr_ty(type::f32Ty(ctx), 3);\n+  else if (argType.getIntOrFloatBitWidth() == 8)\n+    return ptr_ty(type::i8Ty(ctx), 3);\n+  else\n+    llvm::report_fatal_error(\"mma16816 data type not supported\");\n+}\n+\n+Value composeValuesToDotOperandLayoutStruct(\n+    const ValueTable &vals, int n0, int n1,\n+    TritonGPUToLLVMTypeConverter *typeConverter, Location loc,\n+    ConversionPatternRewriter &rewriter) {\n+  std::vector<Value> elems;\n+  for (int m = 0; m < n0; ++m)\n+    for (int k = 0; k < n1; ++k) {\n+      elems.push_back(vals.at({2 * m, 2 * k}));\n+      elems.push_back(vals.at({2 * m, 2 * k + 1}));\n+      elems.push_back(vals.at({2 * m + 1, 2 * k}));\n+      elems.push_back(vals.at({2 * m + 1, 2 * k + 1}));\n+    }\n+\n+  assert(!elems.empty());\n+\n+  Type elemTy = elems[0].getType();\n+  MLIRContext *ctx = elemTy.getContext();\n+  Type structTy = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(elems.size(), elemTy));\n+  auto result = typeConverter->packLLElements(loc, elems, rewriter, structTy);\n+  return result;\n+}\n+\n+std::function<void(int, int)> getLoadMatrixFn(\n+    Value tensor, const SharedMemoryObject &smemObj, MmaEncodingAttr mmaLayout,\n+    int warpsPerTile, uint32_t kOrder, int kWidth, SmallVector<int> instrShape,\n+    SmallVector<int> matShape, Value warpId, Value lane, ValueTable &vals,\n+    bool isA, TritonGPUToLLVMTypeConverter *typeConverter,\n+    ConversionPatternRewriter &rewriter, Location loc) {\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  Type eltTy = tensorTy.getElementType();\n+  // We assumes that the input operand of Dot should be from shared layout.\n+  // TODO(Superjomn) Consider other layouts if needed later.\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  const int perPhase = sharedLayout.getPerPhase();\n+  const int maxPhase = sharedLayout.getMaxPhase();\n+  const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n+  auto order = sharedLayout.getOrder();\n+\n+  // (a, b) is the coordinate.\n+  auto load = [=, &rewriter, &vals](int a, int b) {\n+    MMA16816SmemLoader loader(\n+        warpsPerTile, sharedLayout.getOrder(), mmaLayout.getWarpsPerCTA(),\n+        kOrder, kWidth, smemObj.strides, tensorTy.getShape() /*tileShape*/,\n+        instrShape, matShape, perPhase, maxPhase, elemBytes, rewriter,\n+        typeConverter, loc);\n+    // Offset of a slice within the original tensor in shared memory\n+    Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+    SmallVector<Value> offs =\n+        loader.computeOffsets(warpId, lane, cSwizzleOffset);\n+    // initialize pointers\n+    const int numPtrs = loader.getNumPtrs();\n+    SmallVector<Value> ptrs(numPtrs);\n+    Value smemBase = smemObj.getBaseBeforeSlice(order[0], loc, rewriter);\n+    Type smemPtrTy = getSharedMemPtrTy(eltTy);\n+    for (int i = 0; i < numPtrs; ++i)\n+      ptrs[i] = bitcast(gep(smemPtrTy, smemBase, offs[i]), smemPtrTy);\n+    // actually load from shared memory\n+    auto matTy = LLVM::LLVMStructType::getLiteral(eltTy.getContext(),\n+                                                  SmallVector<Type>(4, i32_ty));\n+    auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n+        (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, ptrs,\n+        matTy, getSharedMemPtrTy(eltTy));\n+    if (!isA)\n+      std::swap(ha1, ha2);\n+    // the following is incorrect\n+    // but causes dramatically better performance in ptxas\n+    // although it only changes the order of operands in\n+    // `mma.sync`\n+    // if(isA)\n+    //   std::swap(ha1, ha2);\n+    // update user-provided values in-place\n+    vals[{a, b}] = ha0;\n+    vals[{a + 1, b}] = ha1;\n+    vals[{a, b + 1}] = ha2;\n+    vals[{a + 1, b + 1}] = ha3;\n+  };\n+\n+  return load;\n+}\n+\n+Value loadArg(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n+              DotOperandEncodingAttr encoding,\n+              const SharedMemoryObject &smemObj,\n+              TritonGPUToLLVMTypeConverter *typeConverter, Value thread,\n+              bool isA) {\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  int bitwidth = tensorTy.getElementTypeBitWidth();\n+  auto mmaLayout = encoding.getParent().cast<MmaEncodingAttr>();\n+\n+  SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n+                             tensorTy.getShape().end());\n+\n+  ValueTable vals;\n+  int mmaInstrM = 16, mmaInstrN = 8, mmaInstrK = 4 * 64 / bitwidth;\n+  int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n+\n+  auto numRep = encoding.getMMAv2Rep(tensorTy.getShape(), bitwidth);\n+  int kWidth = encoding.getMMAv2kWidth();\n+\n+  auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n+  Value warp = udiv(thread, i32_val(32));\n+  Value lane = urem(thread, i32_val(32));\n+  // Note: warps are currently column major in MMA layout\n+  Value warpRowIndex = urem(warp, i32_val(warpsPerCTA[0]));\n+  Value warpColIndex =\n+      urem(udiv(warp, i32_val(warpsPerCTA[0])), i32_val(warpsPerCTA[1]));\n+  Value warpM = urem(warpRowIndex, i32_val(shape[0] / 16));\n+  Value warpN = urem(warpColIndex, i32_val(shape[1] / 8));\n+\n+  int warpsPerTile;\n+  if (isA)\n+    warpsPerTile = std::min<int>(warpsPerCTA[0], shape[0] / 16);\n+  else\n+    warpsPerTile = std::min<int>(warpsPerCTA[1], shape[1] / 16);\n+\n+  std::function<void(int, int)> loadFn;\n+  if (isA)\n+    loadFn = getLoadMatrixFn(\n+        tensor, smemObj, mmaLayout, warpsPerTile /*warpsPerTile*/, 1 /*kOrder*/,\n+        kWidth, {mmaInstrM, mmaInstrK} /*instrShape*/,\n+        {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, lane /*laneId*/,\n+        vals /*vals*/, isA /*isA*/, typeConverter /* typeConverter */,\n+        rewriter /*rewriter*/, loc /*loc*/);\n+  else\n+    loadFn = getLoadMatrixFn(\n+        tensor, smemObj, mmaLayout, warpsPerTile /*warpsPerTile*/, 0 /*kOrder*/,\n+        kWidth, {mmaInstrK, mmaInstrN} /*instrShape*/,\n+        {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, lane /*laneId*/,\n+        vals /*vals*/, isA /*isA*/, typeConverter /* typeConverter */,\n+        rewriter /*rewriter*/, loc /*loc*/);\n+\n+  // Perform loading.\n+  int numRepOuter = isA ? numRep[0] : std::max<int>(numRep[1] / 2, 1);\n+  int numRepK = isA ? numRep[1] : numRep[0];\n+  for (int m = 0; m < numRepOuter; ++m)\n+    for (int k = 0; k < numRepK; ++k)\n+      loadFn(2 * m, 2 * k);\n+\n+  // Format the values to LLVM::Struct to passing to mma codegen.\n+  return composeValuesToDotOperandLayoutStruct(vals, numRepOuter, numRepK,\n+                                               typeConverter, loc, rewriter);\n+}\n+\n+namespace SharedToDotOperandMMAv2 {\n+Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,\n+                    Location loc, Value tensor, DotOperandEncodingAttr encoding,\n+                    const SharedMemoryObject &smemObj,\n+                    TritonGPUToLLVMTypeConverter *typeConverter, Value thread) {\n+  if (opIdx == 0)\n+    return loadArg(rewriter, loc, tensor, encoding, smemObj, typeConverter,\n+                   thread, true);\n+  else {\n+    assert(opIdx == 1);\n+    return loadArg(rewriter, loc, tensor, encoding, smemObj, typeConverter,\n+                   thread, false);\n+  }\n+}\n+} // namespace SharedToDotOperandMMAv2"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "removed", "additions": 0, "deletions": 1933, "changes": 1933, "file_content_changes": "@@ -1,1933 +0,0 @@\n-#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_DOT_OP_HELPERS_H\n-#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_DOT_OP_HELPERS_H\n-\n-#include \"mlir/Analysis/SliceAnalysis.h\"\n-#include \"mlir/Conversion/ArithmeticToLLVM/ArithmeticToLLVM.h\"\n-#include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n-#include \"mlir/Conversion/LLVMCommon/LoweringOptions.h\"\n-#include \"mlir/Conversion/LLVMCommon/Pattern.h\"\n-#include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n-#include \"mlir/Conversion/SCFToStandard/SCFToStandard.h\"\n-#include \"mlir/Conversion/StandardToLLVM/ConvertStandardToLLVM.h\"\n-#include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n-#include \"mlir/Dialect/GPU/GPUDialect.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n-#include \"mlir/IR/Matchers.h\"\n-#include \"mlir/IR/TypeUtilities.h\"\n-#include \"mlir/Transforms/DialectConversion.h\"\n-#include \"triton/Analysis/AxisInfo.h\"\n-#include \"triton/Analysis/Utility.h\"\n-#include \"triton/Conversion/MLIRTypes.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n-#include \"triton/Dialect/Triton/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-#include \"llvm/Support/Format.h\"\n-#include \"llvm/Support/FormatVariadic.h\"\n-\n-#include \"Utility.h\"\n-\n-namespace mlir {\n-namespace LLVM {\n-using namespace mlir::triton;\n-using ::mlir::triton::gpu::BlockedEncodingAttr;\n-using ::mlir::triton::gpu::DotOperandEncodingAttr;\n-using ::mlir::triton::gpu::MmaEncodingAttr;\n-using ::mlir::triton::gpu::SharedEncodingAttr;\n-\n-// Helper for conversion of DotOp with mma<version=1>, that is sm<80\n-struct DotOpMmaV1ConversionHelper {\n-  MmaEncodingAttr mmaLayout;\n-  ArrayRef<unsigned> wpt;\n-  static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n-\n-  using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n-\n-  explicit DotOpMmaV1ConversionHelper(MmaEncodingAttr mmaLayout)\n-      : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()) {}\n-\n-  // Help to share some variables across multiple functions for A.\n-  // TODO[Superjomn]: refactor and restrict this to only use in DotOp\n-  // conversion.\n-  struct AParam {\n-    SmallVector<int> rep;\n-    SmallVector<int> spw;\n-    bool isAVec4{};\n-    int vec{}; // This could only used in DotOp, not in\n-               // loadA/loadB/TypeConverter\n-\n-    AParam(bool isARow, bool isAVec4) : isAVec4(isAVec4) { build(isARow); }\n-\n-  private:\n-    void build(bool isARow) {\n-      int packSize0 = (isARow || isAVec4) ? 1 : 2;\n-      int repM = 2 * packSize0;\n-      int repK = 1;\n-      int spwM = fpw[0] * 4 * repM;\n-      rep.assign({repM, 0, repK});\n-      spw.assign({spwM, 0, 1});\n-      vec = 2 * rep[0];\n-    }\n-  };\n-\n-  // Help to share some variables across multiple functions for A.\n-  // TODO[Superjomn]: refactor and restrict this to only use in DotOp\n-  // conversion.\n-  struct BParam {\n-    SmallVector<int> rep;\n-    SmallVector<int> spw;\n-    bool isBVec4{};\n-    int vec{}; // This could only used in DotOp, not in\n-               // loadA/loadB/TypeConverter\n-\n-    BParam(bool isBRow, bool isBVec4) : isBVec4(isBVec4) { build(isBRow); }\n-\n-  private:\n-    void build(bool isBRow) {\n-      int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n-      rep.assign({0, 2 * packSize1, 1});\n-      spw.assign({0, fpw[1] * 4 * rep[1], 1});\n-      vec = 2 * rep[1];\n-    }\n-  };\n-\n-  int getRepM(int M) const {\n-    return std::max<int>(M / (wpt[0] * instrShape[0]), 1);\n-  }\n-  int getRepN(int N) const {\n-    return std::max<int>(N / (wpt[1] * instrShape[1]), 1);\n-  }\n-\n-  static ArrayRef<unsigned> getMmaInstrShape() { return instrShape; }\n-\n-  static Type getMmaRetType(TensorType operand) {\n-    auto *ctx = operand.getContext();\n-    Type fp32Ty = type::f32Ty(ctx);\n-    // f16*f16+f32->f32\n-    return struct_ty(SmallVector<Type>{8, fp32Ty});\n-  }\n-\n-  static Type getMatType(TensorType operand) {\n-    auto *ctx = operand.getContext();\n-    Type fp16Ty = type::f16Ty(ctx);\n-    Type vecTy = vec_ty(fp16Ty, 2);\n-    return struct_ty(SmallVector<Type>{vecTy});\n-  }\n-\n-  // Get the number of fp16x2 elements for $a.\n-  unsigned getNumM(int M, bool isARow, bool isAVec4) const {\n-    AParam param(isARow, isAVec4);\n-\n-    unsigned numM = param.rep[0] * M / (param.spw[0] * wpt[0]);\n-    return numM;\n-  }\n-\n-  // Get the number of fp16x2 elements for $b.\n-  unsigned getNumN(int N, bool isBRow, bool isBVec4) const {\n-    BParam param(isBRow, isBVec4);\n-\n-    unsigned numN = param.rep[1] * N / (param.spw[1] * wpt[1]);\n-    return numN;\n-  }\n-\n-  int numElemsPerThreadA(ArrayRef<int64_t> shape, bool isARow, bool isAVec4,\n-                         int vec) const {\n-    int numM = getNumM(shape[0], isARow, isAVec4);\n-    int NK = shape[1];\n-    // Here we mimic the logic in loadA, the result cannot be calculated\n-    // directly.\n-    llvm::DenseSet<std::pair<int, int>> visited;\n-    auto ld = [&](int m, int k) {\n-      visited.insert({m, k});\n-      if (vec > 4) {\n-        if (isARow)\n-          visited.insert({m, k + 4});\n-        else\n-          visited.insert({m + 1, k});\n-      }\n-    };\n-\n-    for (unsigned k = 0; k < NK; k += 4)\n-      for (unsigned m = 0; m < numM / 2; ++m)\n-        if (!visited.count({m, k}))\n-          ld(m, k);\n-\n-    return visited.size() * 2;\n-  }\n-\n-  int numElemsPerThreadB(ArrayRef<int64_t> shape, bool isBRow, bool isBVec4,\n-                         int vec) const {\n-    unsigned numN = getNumN(shape[1], isBRow, isBVec4);\n-    int NK = shape[0];\n-    // Here we mimic the logic in loadA, the result cannot be calculated\n-    // directly.\n-    llvm::DenseSet<std::pair<int, int>> visited;\n-    int elemsPerLd = vec > 4 ? 4 : 2;\n-    auto ld = [&](int n, int k) {\n-      visited.insert({n, k});\n-      if (vec > 4) {\n-        if (isBRow)\n-          visited.insert({n + 1, k});\n-        else\n-          visited.insert({n, k + 4});\n-      }\n-    };\n-\n-    for (unsigned k = 0; k < NK; k += 4)\n-      for (unsigned n = 0; n < numN / 2; ++n) {\n-        if (!visited.count({n, k}))\n-          ld(n, k);\n-      }\n-\n-    return visited.size() * 2;\n-  }\n-\n-  // Loading $a from smem to registers, returns a LLVM::Struct.\n-  Value loadA(Value tensor, const SharedMemoryObject &smemObj, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const {\n-    auto *ctx = rewriter.getContext();\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-    auto shape = tensorTy.getShape();\n-    auto order = sharedLayout.getOrder();\n-\n-    Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n-    Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-\n-    bool isARow = order[0] != 0;\n-    auto [isARow_, _0, isAVec4, _1, _2] = mmaLayout.decodeVoltaLayoutStates();\n-\n-    AParam param(isARow_, isAVec4);\n-\n-    auto [offsetAM, offsetAK, _3, _4] = computeOffsets(\n-        thread, isARow, false, fpw, param.spw, param.rep, rewriter, loc);\n-\n-    int vecA = sharedLayout.getVec();\n-\n-    auto strides = smemObj.strides;\n-    Value strideAM = isARow ? strides[0] : i32_val(1);\n-    Value strideAK = isARow ? i32_val(1) : strides[1];\n-    Value strideA0 = isARow ? strideAK : strideAM;\n-    Value strideA1 = isARow ? strideAM : strideAK;\n-\n-    int strideRepM = wpt[0] * fpw[0] * 8;\n-    int strideRepK = 1;\n-\n-    // swizzling\n-    int perPhaseA = sharedLayout.getPerPhase();\n-    int maxPhaseA = sharedLayout.getMaxPhase();\n-    int stepA0 = isARow ? strideRepK : strideRepM;\n-    int numPtrA = std::max(2 * perPhaseA * maxPhaseA / stepA0, 1);\n-    int NK = shape[1];\n-\n-    // pre-compute pointer lanes\n-    Value offA0 = isARow ? offsetAK : offsetAM;\n-    Value offA1 = isARow ? offsetAM : offsetAK;\n-    Value phaseA = urem(udiv(offA1, i32_val(perPhaseA)), i32_val(maxPhaseA));\n-    offA0 = add(offA0, cSwizzleOffset);\n-    SmallVector<Value> offA(numPtrA);\n-    for (int i = 0; i < numPtrA; i++) {\n-      Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));\n-      offA0I = udiv(offA0I, i32_val(vecA));\n-      offA0I = xor_(offA0I, phaseA);\n-      offA0I = mul(offA0I, i32_val(vecA));\n-      offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n-    }\n-\n-    Type elemX2Ty = vec_ty(f16_ty, 2);\n-    Type elemPtrTy = ptr_ty(f16_ty);\n-    if (tensorTy.getElementType().isBF16()) {\n-      elemX2Ty = vec_ty(i16_ty, 2);\n-      elemPtrTy = ptr_ty(i16_ty);\n-    }\n-\n-    // prepare arguments\n-    SmallVector<Value> ptrA(numPtrA);\n-\n-    std::map<std::pair<int, int>, std::pair<Value, Value>> has;\n-    for (int i = 0; i < numPtrA; i++)\n-      ptrA[i] = gep(ptr_ty(f16_ty), smemBase, offA[i]);\n-\n-    auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n-      vals[{m, k}] = {val0, val1};\n-    };\n-    auto loadA = [&](int m, int k) {\n-      int offidx = (isARow ? k / 4 : m) % numPtrA;\n-      Value thePtrA = gep(elemPtrTy, smemBase, offA[offidx]);\n-\n-      int stepAM = isARow ? m : m / numPtrA * numPtrA;\n-      int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n-      Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),\n-                         mul(i32_val(stepAK), strideAK));\n-      Value pa = gep(elemPtrTy, thePtrA, offset);\n-      Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n-      Value ha = load(bitcast(pa, aPtrTy));\n-      // record lds that needs to be moved\n-      Value ha00 = bitcast(extract_element(ha, i32_val(0)), elemX2Ty);\n-      Value ha01 = bitcast(extract_element(ha, i32_val(1)), elemX2Ty);\n-      ld(has, m, k, ha00, ha01);\n-\n-      if (vecA > 4) {\n-        Value ha10 = bitcast(extract_element(ha, i32_val(2)), elemX2Ty);\n-        Value ha11 = bitcast(extract_element(ha, i32_val(3)), elemX2Ty);\n-        if (isARow)\n-          ld(has, m, k + 4, ha10, ha11);\n-        else\n-          ld(has, m + 1, k, ha10, ha11);\n-      }\n-    };\n-\n-    unsigned numM = getNumM(shape[0], isARow, isAVec4);\n-    for (unsigned k = 0; k < NK; k += 4)\n-      for (unsigned m = 0; m < numM / 2; ++m)\n-        if (!has.count({m, k}))\n-          loadA(m, k);\n-\n-    SmallVector<Value> elems;\n-    elems.reserve(has.size() * 2);\n-    for (auto item : has) { // has is a map, the key should be ordered.\n-      elems.push_back(item.second.first);\n-      elems.push_back(item.second.second);\n-    }\n-\n-    Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n-    Value res = getStructFromElements(loc, elems, rewriter, resTy);\n-    return res;\n-  }\n-\n-  // Loading $b from smem to registers, returns a LLVM::Struct.\n-  Value loadB(Value tensor, const SharedMemoryObject &smemObj, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const {\n-    // smem\n-    auto strides = smemObj.strides;\n-\n-    auto *ctx = rewriter.getContext();\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-\n-    auto shape = tensorTy.getShape();\n-    auto order = sharedLayout.getOrder();\n-\n-    Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-    bool isBRow = order[0] != 0; // is row-major in shared memory layout\n-    // isBRow_ indicates whether B is row-major in DotOperand layout\n-    auto [_0, isBRow_, _1, isBVec4, _2] = mmaLayout.decodeVoltaLayoutStates();\n-    assert(isBRow == isBRow_ && \"B need smem isRow\");\n-\n-    BParam param(isBRow_, isBVec4);\n-\n-    int vecB = sharedLayout.getVec();\n-    Value strideBN = isBRow ? i32_val(1) : strides[1];\n-    Value strideBK = isBRow ? strides[0] : i32_val(1);\n-    Value strideB0 = isBRow ? strideBN : strideBK;\n-    Value strideB1 = isBRow ? strideBK : strideBN;\n-    int strideRepN = wpt[1] * fpw[1] * 8;\n-    int strideRepK = 1;\n-\n-    auto [_3, _4, offsetBN, offsetBK] = computeOffsets(\n-        thread, false, isBRow, fpw, param.spw, param.rep, rewriter, loc);\n-\n-    // swizzling\n-    int perPhaseB = sharedLayout.getPerPhase();\n-    int maxPhaseB = sharedLayout.getMaxPhase();\n-    int stepB0 = isBRow ? strideRepN : strideRepK;\n-    int numPtrB = std::max(2 * perPhaseB * maxPhaseB / stepB0, 1);\n-    int NK = shape[0];\n-\n-    Value offB0 = isBRow ? offsetBN : offsetBK;\n-    Value offB1 = isBRow ? offsetBK : offsetBN;\n-    Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n-    Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n-\n-    offB0 = add(offB0, cSwizzleOffset);\n-    SmallVector<Value> offB(numPtrB);\n-    for (int i = 0; i < numPtrB; ++i) {\n-      Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));\n-      offB0I = udiv(offB0I, i32_val(vecB));\n-      offB0I = xor_(offB0I, phaseB);\n-      offB0I = mul(offB0I, i32_val(vecB));\n-      offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));\n-    }\n-\n-    Type elemPtrTy = ptr_ty(f16_ty);\n-    Type elemX2Ty = vec_ty(f16_ty, 2);\n-    if (tensorTy.getElementType().isBF16()) {\n-      elemPtrTy = ptr_ty(i16_ty);\n-      elemX2Ty = vec_ty(i16_ty, 2);\n-    }\n-\n-    SmallVector<Value> ptrB(numPtrB);\n-    ValueTable hbs;\n-    for (int i = 0; i < numPtrB; ++i)\n-      ptrB[i] = gep(ptr_ty(f16_ty), smem, offB[i]);\n-\n-    auto ld = [&](decltype(hbs) &vals, int m, int k, Value val0, Value val1) {\n-      vals[{m, k}] = {val0, val1};\n-    };\n-\n-    auto loadB = [&](int n, int K) {\n-      int offidx = (isBRow ? n : K / 4) % numPtrB;\n-      Value thePtrB = ptrB[offidx];\n-\n-      int stepBN = isBRow ? n / numPtrB * numPtrB : n;\n-      int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n-      Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n-                         mul(i32_val(stepBK), strideBK));\n-      Value pb = gep(elemPtrTy, thePtrB, offset);\n-\n-      Value hb =\n-          load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n-      // record lds that needs to be moved\n-      Value hb00 = bitcast(extract_element(hb, i32_val(0)), elemX2Ty);\n-      Value hb01 = bitcast(extract_element(hb, i32_val(1)), elemX2Ty);\n-      ld(hbs, n, K, hb00, hb01);\n-      if (vecB > 4) {\n-        Value hb10 = bitcast(extract_element(hb, i32_val(2)), elemX2Ty);\n-        Value hb11 = bitcast(extract_element(hb, i32_val(3)), elemX2Ty);\n-        if (isBRow)\n-          ld(hbs, n + 1, K, hb10, hb11);\n-        else\n-          ld(hbs, n, K + 4, hb10, hb11);\n-      }\n-    };\n-\n-    unsigned numN = getNumN(shape[1], isBRow, isBVec4);\n-    for (unsigned k = 0; k < NK; k += 4)\n-      for (unsigned n = 0; n < numN / 2; ++n) {\n-        if (!hbs.count({n, k}))\n-          loadB(n, k);\n-      }\n-\n-    SmallVector<Value> elems;\n-    for (auto &item : hbs) { // has is a map, the key should be ordered.\n-      elems.push_back(item.second.first);\n-      elems.push_back(item.second.second);\n-    }\n-\n-    Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n-    Value res = getStructFromElements(loc, elems, rewriter, resTy);\n-    return res;\n-  }\n-\n-  static ArrayRef<unsigned> getOrder() { return mmaOrder; }\n-\n-  // Compute the offset of the matrix to load.\n-  // Returns offsetAM, offsetAK, offsetBN, offsetBK.\n-  // NOTE, the information M(from $a) and N(from $b) couldn't be retrieved at\n-  // the same time in the usage in convert_layout[shared->dot_op], we leave\n-  // the noexist info to be 0 and only use the desired argument from the\n-  // composed result. In this way we want to retain the original code\n-  // structure in convert_mma884 method for easier debugging.\n-  std::tuple<Value, Value, Value, Value>\n-  computeOffsets(Value threadId, bool isARow, bool isBRow, ArrayRef<int> fpw,\n-                 ArrayRef<int> spw, ArrayRef<int> rep,\n-                 ConversionPatternRewriter &rewriter, Location loc) const {\n-    auto *ctx = rewriter.getContext();\n-    Value _1 = i32_val(1);\n-    Value _3 = i32_val(3);\n-    Value _4 = i32_val(4);\n-    Value _16 = i32_val(16);\n-    Value _32 = i32_val(32);\n-\n-    Value lane = urem(threadId, _32);\n-    Value warp = udiv(threadId, _32);\n-\n-    // warp offset\n-    Value warp0 = urem(warp, i32_val(wpt[0]));\n-    Value warp12 = udiv(warp, i32_val(wpt[0]));\n-    Value warp1 = urem(warp12, i32_val(wpt[1]));\n-    Value warpMOff = mul(warp0, i32_val(spw[0]));\n-    Value warpNOff = mul(warp1, i32_val(spw[1]));\n-    // Quad offset\n-    Value quadMOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[0]));\n-    Value quadNOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[1]));\n-    // Pair offset\n-    Value pairMOff = udiv(urem(lane, _16), _4);\n-    pairMOff = urem(pairMOff, i32_val(fpw[0]));\n-    pairMOff = mul(pairMOff, _4);\n-    Value pairNOff = udiv(urem(lane, _16), _4);\n-    pairNOff = udiv(pairNOff, i32_val(fpw[0]));\n-    pairNOff = urem(pairNOff, i32_val(fpw[1]));\n-    pairNOff = mul(pairNOff, _4);\n-    // scale\n-    pairMOff = mul(pairMOff, i32_val(rep[0] / 2));\n-    quadMOff = mul(quadMOff, i32_val(rep[0] / 2));\n-    pairNOff = mul(pairNOff, i32_val(rep[1] / 2));\n-    quadNOff = mul(quadNOff, i32_val(rep[1] / 2));\n-    // Quad pair offset\n-    Value laneMOff = add(pairMOff, quadMOff);\n-    Value laneNOff = add(pairNOff, quadNOff);\n-    // A offset\n-    Value offsetAM = add(warpMOff, laneMOff);\n-    Value offsetAK = and_(lane, _3);\n-    // B offset\n-    Value offsetBN = add(warpNOff, laneNOff);\n-    Value offsetBK = and_(lane, _3);\n-    // i indices\n-    Value offsetCM = add(and_(lane, _1), offsetAM);\n-    if (isARow) {\n-      offsetAM = add(offsetAM, urem(threadId, _4));\n-      offsetAK = i32_val(0);\n-    }\n-    if (!isBRow) {\n-      offsetBN = add(offsetBN, urem(threadId, _4));\n-      offsetBK = i32_val(0);\n-    }\n-\n-    return std::make_tuple(offsetAM, offsetAK, offsetBN, offsetBK);\n-  }\n-\n-  // Extract values belong to $a or $b from a LLVMStruct, the shape is n0xn1.\n-  DotOpMmaV1ConversionHelper::ValueTable\n-  extractLoadedOperand(Value llStruct, int NK,\n-                       ConversionPatternRewriter &rewriter) const {\n-    ValueTable rcds;\n-    SmallVector<Value> elems =\n-        getElementsFromStruct(llStruct.getLoc(), llStruct, rewriter);\n-\n-    int offset = 0;\n-    for (int i = 0; offset < elems.size(); ++i) {\n-      for (int k = 0; k < NK; k += 4) {\n-        rcds[{i, k}] = std::make_pair(elems[offset], elems[offset + 1]);\n-        offset += 2;\n-      }\n-    }\n-\n-    return rcds;\n-  }\n-\n-  // Get the number of elements of this thread in M axis. The N axis could be\n-  // further deduced with the accSize / elemsM. \\param wpt: the wpt in M axis\n-  // \\param M: the shape in M axis\n-  int getElemsM(int wpt, int M, bool isARow, bool isAVec4) {\n-    DotOpMmaV1ConversionHelper::AParam param(isARow, isAVec4);\n-    int shapePerCTAM = param.spw[0] * wpt;\n-    return M / shapePerCTAM * param.rep[0];\n-  }\n-\n-  using CoordTy = SmallVector<Value, 2>;\n-  // Get the coordinates(m,n) of the elements emit by a thread in accumulator.\n-  static SmallVector<CoordTy>\n-  getMNCoords(Value thread, ConversionPatternRewriter &rewriter,\n-              ArrayRef<unsigned> wpt, ArrayRef<int64_t> shape, bool isARow,\n-              bool isBRow, bool isAVec4, bool isBVec4) {\n-\n-    auto *ctx = thread.getContext();\n-    auto loc = UnknownLoc::get(ctx);\n-    Value _1 = i32_val(1);\n-    Value _2 = i32_val(2);\n-    Value _4 = i32_val(4);\n-    Value _16 = i32_val(16);\n-    Value _32 = i32_val(32);\n-    Value _fpw0 = i32_val(fpw[0]);\n-    Value _fpw1 = i32_val(fpw[1]);\n-\n-    DotOpMmaV1ConversionHelper::AParam aParam(isARow, isAVec4);\n-    DotOpMmaV1ConversionHelper::BParam bParam(isBRow, isBVec4);\n-\n-    SmallVector<int, 2> rep({aParam.rep[0], bParam.rep[1]});\n-    SmallVector<int, 2> spw({aParam.spw[0], bParam.spw[1]});\n-    SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n-\n-    Value lane = urem(thread, _32);\n-    Value warp = udiv(thread, _32);\n-\n-    Value warp0 = urem(warp, i32_val(wpt[0]));\n-    Value warp12 = udiv(warp, i32_val(wpt[0]));\n-    Value warp1 = urem(warp12, i32_val(wpt[1]));\n-\n-    // warp offset\n-    Value offWarpM = mul(warp0, i32_val(spw[0]));\n-    Value offWarpN = mul(warp1, i32_val(spw[1]));\n-    // quad offset\n-    Value offQuadM = mul(udiv(and_(lane, _16), _4), _fpw0);\n-    Value offQuadN = mul(udiv(and_(lane, _16), _4), _fpw1);\n-    // pair offset\n-    Value offPairM = udiv(urem(lane, _16), _4);\n-    offPairM = urem(offPairM, _fpw0);\n-    offPairM = mul(offPairM, _4);\n-    Value offPairN = udiv(urem(lane, _16), _4);\n-    offPairN = udiv(offPairN, _fpw0);\n-    offPairN = urem(offPairN, _fpw1);\n-    offPairN = mul(offPairN, _4);\n-\n-    // sclare\n-    offPairM = mul(offPairM, i32_val(rep[0] / 2));\n-    offQuadM = mul(offQuadM, i32_val(rep[0] / 2));\n-    offPairN = mul(offPairN, i32_val(rep[1] / 2));\n-    offQuadN = mul(offQuadN, i32_val(rep[1] / 2));\n-\n-    // quad pair offset\n-    Value offLaneM = add(offPairM, offQuadM);\n-    Value offLaneN = add(offPairN, offQuadN);\n-    // a, b offset\n-    Value offsetAM = add(offWarpM, offLaneM);\n-    Value offsetBN = add(offWarpN, offLaneN);\n-    // m indices\n-    Value offsetCM = add(and_(lane, _1), offsetAM);\n-    SmallVector<Value> idxM;\n-    for (unsigned m = 0; m < shape[0]; m += shapePerCTA[0])\n-      for (unsigned mm = 0; mm < rep[0]; ++mm)\n-        idxM.push_back(add(offsetCM, i32_val(m + mm * 2)));\n-\n-    // n indices\n-    Value offsetCN = add((and_(lane, _2)), (add(offWarpN, offPairN)));\n-    SmallVector<Value> idxN;\n-    for (int n = 0; n < shape[1]; n += shapePerCTA[1]) {\n-      for (int nn = 0; nn < rep[1]; ++nn) {\n-        idxN.push_back(add(offsetCN, i32_val(n + nn / 2 * 4 +\n-                                             (nn % 2) * 2 * fpw[1] * rep[1])));\n-        idxN.push_back(\n-            add(offsetCN,\n-                i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1] + 1)));\n-      }\n-    }\n-\n-    SmallVector<SmallVector<Value>> axes({idxM, idxN});\n-\n-    // product the axis M and axis N to get coords, ported from\n-    // generator::init_idx method from triton2.0\n-\n-    // TODO[Superjomn]: check the order.\n-    SmallVector<CoordTy> coords;\n-    for (Value x1 : axes[1]) {   // N\n-      for (Value x0 : axes[0]) { // M\n-        SmallVector<Value, 2> idx(2);\n-        idx[0] = x0; // M\n-        idx[1] = x1; // N\n-        coords.push_back(std::move(idx));\n-      }\n-    }\n-\n-    return coords; // {M,N} in row-major\n-  }\n-\n-  // \\param elemId the offset of the element in a thread\n-  static CoordTy getCoord(int elemId, ArrayRef<CoordTy> coords) {\n-    return coords[elemId];\n-  }\n-\n-private:\n-  static constexpr unsigned instrShape[] = {16, 16, 4};\n-  static constexpr unsigned mmaOrder[] = {0, 1};\n-};\n-\n-// Helper for conversion of DotOp with mma<version=2>, that is sm>=80\n-struct DotOpMmaV2ConversionHelper {\n-  enum class TensorCoreType : uint8_t {\n-    // floating-point tensor core instr\n-    FP32_FP16_FP16_FP32 = 0, // default\n-    FP32_BF16_BF16_FP32,\n-    FP32_TF32_TF32_FP32,\n-    // integer tensor core instr\n-    INT32_INT1_INT1_INT32, // Not implemented\n-    INT32_INT4_INT4_INT32, // Not implemented\n-    INT32_INT8_INT8_INT32, // Not implemented\n-    //\n-    NOT_APPLICABLE,\n-  };\n-\n-  MmaEncodingAttr mmaLayout;\n-  MLIRContext *ctx{};\n-\n-  explicit DotOpMmaV2ConversionHelper(MmaEncodingAttr mmaLayout)\n-      : mmaLayout(mmaLayout) {\n-    ctx = mmaLayout.getContext();\n-  }\n-\n-  void deduceMmaType(DotOp op) const { mmaType = getMmaType(op); }\n-  void deduceMmaType(Type operandTy) const {\n-    mmaType = getTensorCoreTypeFromOperand(operandTy);\n-  }\n-\n-  // Get the M and N of mma instruction shape.\n-  static std::tuple<int, int> getInstrShapeMN() {\n-    // According to DotOpConversionHelper::mmaInstrShape, all the M,N are\n-    // {16,8}\n-    return {16, 8};\n-  }\n-\n-  static std::tuple<int, int> getRepMN(const RankedTensorType &tensorTy) {\n-    auto mmaLayout = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n-    auto wpt = mmaLayout.getWarpsPerCTA();\n-\n-    int M = tensorTy.getShape()[0];\n-    int N = tensorTy.getShape()[1];\n-    auto [instrM, instrN] = getInstrShapeMN();\n-    int repM = std::max<int>(M / (wpt[0] * instrM), 1);\n-    int repN = std::max<int>(N / (wpt[1] * instrN), 1);\n-    return {repM, repN};\n-  }\n-\n-  Type getShemPtrTy() const {\n-    switch (mmaType) {\n-    case TensorCoreType::FP32_FP16_FP16_FP32:\n-      return ptr_ty(type::f16Ty(ctx), 3);\n-    case TensorCoreType::FP32_BF16_BF16_FP32:\n-      return ptr_ty(type::i16Ty(ctx), 3);\n-    case TensorCoreType::FP32_TF32_TF32_FP32:\n-      return ptr_ty(type::f32Ty(ctx), 3);\n-    case TensorCoreType::INT32_INT8_INT8_INT32:\n-      return ptr_ty(type::i8Ty(ctx), 3);\n-    default:\n-      llvm::report_fatal_error(\"mma16816 data type not supported\");\n-    }\n-    return Type{};\n-  }\n-\n-  // The type of matrix that loaded by either a ldmatrix or composed lds.\n-  Type getMatType() const {\n-    // floating point types\n-    Type fp32x1Ty = vec_ty(type::f32Ty(ctx), 1);\n-    Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n-    Type i16x2Ty = vec_ty(type::i16Ty(ctx), 2);\n-    Type fp16x2Pack4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n-    // LLVM 14.0 does not support bf16 type, so we use i16 instead.\n-    Type bf16x2Pack4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i16x2Ty));\n-    Type fp32Pack4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32x1Ty));\n-    // integer types\n-    Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n-    Type i8x4Pack4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i8x4Ty));\n-\n-    switch (mmaType) {\n-    case TensorCoreType::FP32_FP16_FP16_FP32:\n-      return fp16x2Pack4Ty;\n-    case TensorCoreType::FP32_BF16_BF16_FP32:\n-      return bf16x2Pack4Ty;\n-    case TensorCoreType::FP32_TF32_TF32_FP32:\n-      return fp32Pack4Ty;\n-    case TensorCoreType::INT32_INT8_INT8_INT32:\n-      return i8x4Pack4Ty;\n-    default:\n-      llvm::report_fatal_error(\"Unsupported mma type found\");\n-    }\n-\n-    return Type{};\n-  }\n-\n-  Type getLoadElemTy() {\n-    switch (mmaType) {\n-    case TensorCoreType::FP32_FP16_FP16_FP32:\n-      return vec_ty(type::f16Ty(ctx), 2);\n-    case TensorCoreType::FP32_BF16_BF16_FP32:\n-      return vec_ty(type::bf16Ty(ctx), 2);\n-    case TensorCoreType::FP32_TF32_TF32_FP32:\n-      return type::f32Ty(ctx);\n-    case TensorCoreType::INT32_INT8_INT8_INT32:\n-      return type::i32Ty(ctx);\n-    default:\n-      llvm::report_fatal_error(\"Unsupported mma type found\");\n-    }\n-\n-    return Type{};\n-  }\n-\n-  Type getMmaRetType() const {\n-    Type fp32Ty = type::f32Ty(ctx);\n-    Type i32Ty = type::i32Ty(ctx);\n-    Type fp32x4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n-    Type i32x4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i32Ty));\n-    switch (mmaType) {\n-    case TensorCoreType::FP32_FP16_FP16_FP32:\n-      return fp32x4Ty;\n-    case TensorCoreType::FP32_BF16_BF16_FP32:\n-      return fp32x4Ty;\n-    case TensorCoreType::FP32_TF32_TF32_FP32:\n-      return fp32x4Ty;\n-    case TensorCoreType::INT32_INT8_INT8_INT32:\n-      return i32x4Ty;\n-    default:\n-      llvm::report_fatal_error(\"Unsupported mma type found\");\n-    }\n-\n-    return Type{};\n-  }\n-\n-  ArrayRef<int> getMmaInstrShape() const {\n-    assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n-           \"Unknown mma type found.\");\n-    return mmaInstrShape.at(mmaType);\n-  }\n-\n-  static ArrayRef<int> getMmaInstrShape(TensorCoreType tensorCoreType) {\n-    assert(tensorCoreType != TensorCoreType::NOT_APPLICABLE &&\n-           \"Unknown mma type found.\");\n-    return mmaInstrShape.at(tensorCoreType);\n-  }\n-\n-  ArrayRef<int> getMmaMatShape() const {\n-    assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n-           \"Unknown mma type found.\");\n-    return mmaMatShape.at(mmaType);\n-  }\n-\n-  // Deduce the TensorCoreType from either $a or $b's type.\n-  static TensorCoreType getTensorCoreTypeFromOperand(Type operandTy) {\n-    auto tensorTy = operandTy.cast<RankedTensorType>();\n-    auto elemTy = tensorTy.getElementType();\n-    if (elemTy.isF16())\n-      return TensorCoreType::FP32_FP16_FP16_FP32;\n-    if (elemTy.isF32())\n-      return TensorCoreType::FP32_TF32_TF32_FP32;\n-    if (elemTy.isBF16())\n-      return TensorCoreType::FP32_BF16_BF16_FP32;\n-    if (elemTy.isInteger(8))\n-      return TensorCoreType::INT32_INT8_INT8_INT32;\n-    return TensorCoreType::NOT_APPLICABLE;\n-  }\n-\n-  int getVec() const {\n-    assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n-           \"Unknown mma type found.\");\n-    return mmaInstrVec.at(mmaType);\n-  }\n-\n-  StringRef getMmaInstr() const {\n-    assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n-           \"Unknown mma type found.\");\n-    return mmaInstrPtx.at(mmaType);\n-  }\n-\n-  static TensorCoreType getMmaType(triton::DotOp op) {\n-    Value A = op.a();\n-    Value B = op.b();\n-    auto aTy = A.getType().cast<RankedTensorType>();\n-    auto bTy = B.getType().cast<RankedTensorType>();\n-    // d = a*b + c\n-    auto dTy = op.d().getType().cast<RankedTensorType>();\n-\n-    if (dTy.getElementType().isF32()) {\n-      if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n-        return TensorCoreType::FP32_FP16_FP16_FP32;\n-      if (aTy.getElementType().isBF16() && bTy.getElementType().isBF16())\n-        return TensorCoreType::FP32_BF16_BF16_FP32;\n-      if (aTy.getElementType().isF32() && bTy.getElementType().isF32() &&\n-          op.allowTF32())\n-        return TensorCoreType::FP32_TF32_TF32_FP32;\n-    } else if (dTy.getElementType().isInteger(32)) {\n-      if (aTy.getElementType().isInteger(8) &&\n-          bTy.getElementType().isInteger(8))\n-        return TensorCoreType::INT32_INT8_INT8_INT32;\n-    }\n-\n-    return TensorCoreType::NOT_APPLICABLE;\n-  }\n-\n-private:\n-  mutable TensorCoreType mmaType{TensorCoreType::NOT_APPLICABLE};\n-\n-  // Used on nvidia GPUs mma layout .version == 2\n-  // Refer to\n-  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-storage\n-  // for more details.\n-  inline static const std::map<TensorCoreType, llvm::SmallVector<int>>\n-      mmaInstrShape = {\n-          {TensorCoreType::FP32_FP16_FP16_FP32, {16, 8, 16}},\n-          {TensorCoreType::FP32_BF16_BF16_FP32, {16, 8, 16}},\n-          {TensorCoreType::FP32_TF32_TF32_FP32, {16, 8, 8}},\n-\n-          {TensorCoreType::INT32_INT1_INT1_INT32, {16, 8, 256}},\n-          {TensorCoreType::INT32_INT4_INT4_INT32, {16, 8, 64}},\n-          {TensorCoreType::INT32_INT8_INT8_INT32, {16, 8, 32}},\n-  };\n-\n-  // shape of matrices loaded by ldmatrix (m-n-k, for mxk & kxn matrices)\n-  // Refer to\n-  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix\n-  // for more details.\n-  inline static const std::map<TensorCoreType, llvm::SmallVector<int>>\n-      mmaMatShape = {\n-          {TensorCoreType::FP32_FP16_FP16_FP32, {8, 8, 8}},\n-          {TensorCoreType::FP32_BF16_BF16_FP32, {8, 8, 8}},\n-          {TensorCoreType::FP32_TF32_TF32_FP32, {8, 8, 4}},\n-\n-          {TensorCoreType::INT32_INT1_INT1_INT32, {8, 8, 64}},\n-          {TensorCoreType::INT32_INT4_INT4_INT32, {8, 8, 32}},\n-          {TensorCoreType::INT32_INT8_INT8_INT32, {8, 8, 16}},\n-  };\n-\n-  // Supported mma instruction in PTX.\n-  // Refer to\n-  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-for-mma\n-  // for more details.\n-  inline static const std::map<TensorCoreType, std::string> mmaInstrPtx = {\n-      {TensorCoreType::FP32_FP16_FP16_FP32,\n-       \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"},\n-      {TensorCoreType::FP32_BF16_BF16_FP32,\n-       \"mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\"},\n-      {TensorCoreType::FP32_TF32_TF32_FP32,\n-       \"mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\"},\n-\n-      {TensorCoreType::INT32_INT1_INT1_INT32,\n-       \"mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc\"},\n-      {TensorCoreType::INT32_INT4_INT4_INT32,\n-       \"mma.sync.aligned.m16n8k64.row.col.satfinite.s32.s4.s4.s32\"},\n-      {TensorCoreType::INT32_INT8_INT8_INT32,\n-       \"mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\"},\n-  };\n-\n-  // vector length per ldmatrix (16*8/element_size_in_bits)\n-  inline static const std::map<TensorCoreType, uint8_t> mmaInstrVec = {\n-      {TensorCoreType::FP32_FP16_FP16_FP32, 8},\n-      {TensorCoreType::FP32_BF16_BF16_FP32, 8},\n-      {TensorCoreType::FP32_TF32_TF32_FP32, 4},\n-\n-      {TensorCoreType::INT32_INT1_INT1_INT32, 128},\n-      {TensorCoreType::INT32_INT4_INT4_INT32, 32},\n-      {TensorCoreType::INT32_INT8_INT8_INT32, 16},\n-  };\n-};\n-\n-// Data loader for mma.16816 instruction.\n-class MMA16816SmemLoader {\n-public:\n-  MMA16816SmemLoader(int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n-                     ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n-                     ArrayRef<int> instrShape, ArrayRef<int> matShape,\n-                     int perPhase, int maxPhase, int elemBytes,\n-                     ConversionPatternRewriter &rewriter,\n-                     TypeConverter *typeConverter, const Location &loc)\n-      : order(order.begin(), order.end()), kOrder(kOrder),\n-        tileShape(tileShape.begin(), tileShape.end()),\n-        instrShape(instrShape.begin(), instrShape.end()),\n-        matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n-        maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter), loc(loc),\n-        ctx(rewriter.getContext()) {\n-    cMatShape = matShape[order[0]];\n-    sMatShape = matShape[order[1]];\n-\n-    sStride = smemStrides[order[1]];\n-\n-    // rule: k must be the fast-changing axis.\n-    needTrans = kOrder != order[0];\n-    canUseLdmatrix = elemBytes == 2 || (!needTrans); // b16\n-\n-    if (canUseLdmatrix) {\n-      // Each CTA, the warps is arranged as [1xwpt] if not transposed,\n-      // otherwise [wptx1], and each warp will perform a mma.\n-      numPtrs =\n-          tileShape[order[0]] / (needTrans ? wpt : 1) / instrShape[order[0]];\n-    } else {\n-      numPtrs = tileShape[order[0]] / wpt / matShape[order[0]];\n-    }\n-    numPtrs = std::max<int>(numPtrs, 2);\n-\n-    // Special rule for i8/u8, 4 ptrs for each matrix\n-    if (!canUseLdmatrix && elemBytes == 1)\n-      numPtrs *= 4;\n-\n-    int loadStrideInMat[2];\n-    loadStrideInMat[kOrder] =\n-        2; // instrShape[kOrder] / matShape[kOrder], always 2\n-    loadStrideInMat[kOrder ^ 1] =\n-        wpt * (instrShape[kOrder ^ 1] / matShape[kOrder ^ 1]);\n-\n-    pLoadStrideInMat = loadStrideInMat[order[0]];\n-    sMatStride =\n-        loadStrideInMat[order[1]] / (instrShape[order[1]] / matShape[order[1]]);\n-\n-    // Each matArr contains warpOffStride matrices.\n-    matArrStride = kOrder == 1 ? 1 : wpt;\n-    warpOffStride = instrShape[kOrder ^ 1] / matShape[kOrder ^ 1];\n-  }\n-\n-  // lane = thread % 32\n-  // warpOff = (thread/32) % wpt(0)\n-  llvm::SmallVector<Value> computeOffsets(Value warpOff, Value lane,\n-                                          Value cSwizzleOffset) {\n-    if (canUseLdmatrix)\n-      return computeLdmatrixMatOffs(warpOff, lane, cSwizzleOffset);\n-    else if (elemBytes == 4 && needTrans)\n-      return computeB32MatOffs(warpOff, lane, cSwizzleOffset);\n-    else if (elemBytes == 1 && needTrans)\n-      return computeB8MatOffs(warpOff, lane, cSwizzleOffset);\n-    else\n-      llvm::report_fatal_error(\"Invalid smem load config\");\n-\n-    return {};\n-  }\n-\n-  int getNumPtrs() const { return numPtrs; }\n-\n-  // Compute the offset to the matrix this thread(indexed by warpOff and lane)\n-  // mapped to.\n-  SmallVector<Value> computeLdmatrixMatOffs(Value warpId, Value lane,\n-                                            Value cSwizzleOffset) {\n-    // 4x4 matrices\n-    Value c = urem(lane, i32_val(8));\n-    Value s = udiv(lane, i32_val(8)); // sub-warp-id\n-\n-    // Decompose s => s_0, s_1, that is the coordinate in 2x2 matrices in a\n-    // warp\n-    Value s0 = urem(s, i32_val(2));\n-    Value s1 = udiv(s, i32_val(2));\n-\n-    // We use different orders for a and b for better performance.\n-    Value kMatArr = kOrder == 1 ? s1 : s0;\n-    Value nkMatArr = kOrder == 1 ? s0 : s1;\n-\n-    // matrix coordinate inside a CTA, the matrix layout is [2x2wpt] for A and\n-    // [2wptx2] for B. e.g. Setting wpt=3, The data layout for A(kOrder=1) is\n-    //   |0 0 1 1 2 2| -> 0,1,2 are the warpids\n-    //   |0 0 1 1 2 2|\n-    //\n-    // for B(kOrder=0) is\n-    //   |0 0|  -> 0,1,2 are the warpids\n-    //   |1 1|\n-    //   |2 2|\n-    //   |0 0|\n-    //   |1 1|\n-    //   |2 2|\n-    // Note, for each warp, it handles a 2x2 matrices, that is the coordinate\n-    // address (s0,s1) annotates.\n-\n-    Value matOff[2];\n-    matOff[kOrder ^ 1] = add(\n-        mul(warpId, i32_val(warpOffStride)),   // warp offset\n-        mul(nkMatArr, i32_val(matArrStride))); // matrix offset inside a warp\n-    matOff[kOrder] = kMatArr;\n-\n-    // Physical offset (before swizzling)\n-    Value cMatOff = matOff[order[0]];\n-    Value sMatOff = matOff[order[1]];\n-    Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n-    cMatOff = add(cMatOff, cSwizzleMatOff);\n-\n-    // row offset inside a matrix, each matrix has 8 rows.\n-    Value sOffInMat = c;\n-\n-    SmallVector<Value> offs(numPtrs);\n-    Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n-    Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n-    for (int i = 0; i < numPtrs; ++i) {\n-      Value cMatOffI = add(cMatOff, i32_val(i * pLoadStrideInMat));\n-      cMatOffI = xor_(cMatOffI, phase);\n-      offs[i] = add(mul(cMatOffI, i32_val(cMatShape)), mul(sOff, sStride));\n-    }\n-\n-    return offs;\n-  }\n-\n-  // Compute 32-bit matrix offsets.\n-  SmallVector<Value> computeB32MatOffs(Value warpOff, Value lane,\n-                                       Value cSwizzleOffset) {\n-    assert(needTrans && \"Only used in transpose mode.\");\n-    // Load tf32 matrices with lds32\n-    Value cOffInMat = udiv(lane, i32_val(4));\n-    Value sOffInMat = urem(lane, i32_val(4));\n-\n-    Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n-    SmallVector<Value> offs(numPtrs);\n-\n-    for (int mat = 0; mat < 4; ++mat) { // Load 4 mats each time\n-      int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n-      int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n-      if (kMatArrInt > 0) // we don't need pointers for k\n-        continue;\n-      Value kMatArr = i32_val(kMatArrInt);\n-      Value nkMatArr = i32_val(nkMatArrInt);\n-\n-      Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n-                          mul(nkMatArr, i32_val(matArrStride)));\n-      Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n-      cMatOff = add(cMatOff, cSwizzleMatOff);\n-\n-      Value sMatOff = kMatArr;\n-      Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n-      // FIXME: (kOrder == 1?) is really dirty hack\n-      for (int i = 0; i < numPtrs / 2; ++i) {\n-        Value cMatOffI =\n-            add(cMatOff, i32_val(i * pLoadStrideInMat * (kOrder == 1 ? 1 : 2)));\n-        cMatOffI = xor_(cMatOffI, phase);\n-        Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n-        cOff = urem(cOff, i32_val(tileShape[order[0]]));\n-        sOff = urem(sOff, i32_val(tileShape[order[1]]));\n-        offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, sStride));\n-      }\n-    }\n-    return offs;\n-  }\n-\n-  // compute 8-bit matrix offset.\n-  SmallVector<Value> computeB8MatOffs(Value warpOff, Value lane,\n-                                      Value cSwizzleOffset) {\n-    assert(needTrans && \"Only used in transpose mode.\");\n-    Value cOffInMat = udiv(lane, i32_val(4));\n-    Value sOffInMat =\n-        mul(urem(lane, i32_val(4)), i32_val(4)); // each thread load 4 cols\n-\n-    SmallVector<Value> offs(numPtrs);\n-    for (int mat = 0; mat < 4; ++mat) {\n-      int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n-      int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n-      if (kMatArrInt > 0) // we don't need pointers for k\n-        continue;\n-      Value kMatArr = i32_val(kMatArrInt);\n-      Value nkMatArr = i32_val(nkMatArrInt);\n-\n-      Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n-                          mul(nkMatArr, i32_val(matArrStride)));\n-      Value sMatOff = kMatArr;\n-\n-      for (int loadx4Off = 0; loadx4Off < numPtrs / 8; ++loadx4Off) {\n-        for (int elemOff = 0; elemOff < 4; ++elemOff) {\n-          int ptrOff = loadx4Off * 8 + nkMatArrInt * 4 + elemOff;\n-          Value cMatOffI = add(cMatOff, i32_val(loadx4Off * pLoadStrideInMat *\n-                                                (kOrder == 1 ? 1 : 2)));\n-          Value sOffInMatElem = add(sOffInMat, i32_val(elemOff));\n-\n-          // disable swizzling ...\n-\n-          Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n-          Value sOff = add(sOffInMatElem, mul(sMatOff, i32_val(sMatShape)));\n-          // To prevent out-of-bound access when tile is too small.\n-          cOff = urem(cOff, i32_val(tileShape[order[0]]));\n-          sOff = urem(sOff, i32_val(tileShape[order[1]]));\n-          offs[ptrOff] = add(cOff, mul(sOff, sStride));\n-        }\n-      }\n-    }\n-    return offs;\n-  }\n-\n-  // Load 4 matrices and returns 4 vec<2> elements.\n-  std::tuple<Value, Value, Value, Value>\n-  loadX4(int mat0, int mat1, ArrayRef<Value> offs, ArrayRef<Value> ptrs,\n-         Type matTy, Type shemPtrTy) const {\n-    assert(mat0 % 2 == 0 && mat1 % 2 == 0 &&\n-           \"smem matrix load must be aligned\");\n-    int matIdx[2] = {mat0, mat1};\n-\n-    int ptrIdx{-1};\n-\n-    if (canUseLdmatrix)\n-      ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n-    else if (elemBytes == 4 && needTrans)\n-      ptrIdx = matIdx[order[0]];\n-    else if (elemBytes == 1 && needTrans)\n-      ptrIdx = matIdx[order[0]] * 4;\n-    else\n-      llvm::report_fatal_error(\"unsupported mma type found\");\n-\n-    // The main difference with the original triton code is we removed the\n-    // prefetch-related logic here for the upstream optimizer phase should\n-    // take care with it, and that is transparent in dot conversion.\n-    auto getPtr = [&](int idx) { return ptrs[idx]; };\n-\n-    Value ptr = getPtr(ptrIdx);\n-\n-    // The struct should have exactly the same element types.\n-    auto resTy = matTy.cast<LLVM::LLVMStructType>();\n-    Type elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n-\n-    // For some reasons, LLVM's NVPTX backend inserts unnecessary (?) integer\n-    // instructions to pack & unpack sub-word integers. A workaround is to\n-    // store the results of ldmatrix in i32\n-    if (auto vecElemTy = elemTy.dyn_cast<VectorType>()) {\n-      Type elemElemTy = vecElemTy.getElementType();\n-      if (auto intTy = elemElemTy.dyn_cast<IntegerType>()) {\n-        if (intTy.getWidth() <= 16) {\n-          elemTy = rewriter.getI32Type();\n-          resTy = LLVM::LLVMStructType::getLiteral(\n-              ctx, SmallVector<Type>(4, elemTy));\n-        }\n-      }\n-    }\n-\n-    if (canUseLdmatrix) {\n-      Value sOffset =\n-          mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sStride);\n-      Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n-\n-      PTXBuilder builder;\n-      // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n-      // thread.\n-      auto resArgs = builder.newListOperand(4, \"=r\");\n-      auto addrArg = builder.newAddrOperand(sOffsetPtr, \"r\");\n-\n-      auto ldmatrix = builder.create(\"ldmatrix.sync.aligned.m8n8.x4\")\n-                          ->o(\"trans\", needTrans /*predicate*/)\n-                          .o(\"shared.b16\");\n-      ldmatrix(resArgs, addrArg);\n-\n-      // The result type is 4xi32, each i32 is composed of 2xf16\n-      // elements (adjacent two columns in a row) or a single f32 element.\n-      Value resV4 = builder.launch(rewriter, loc, resTy);\n-      return {extract_val(elemTy, resV4, i32_arr_attr(0)),\n-              extract_val(elemTy, resV4, i32_arr_attr(1)),\n-              extract_val(elemTy, resV4, i32_arr_attr(2)),\n-              extract_val(elemTy, resV4, i32_arr_attr(3))};\n-    } else if (elemBytes == 4 &&\n-               needTrans) { // Use lds.32 to load tf32 matrices\n-      Value ptr2 = getPtr(ptrIdx + 1);\n-      assert(sMatStride == 1);\n-      int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n-      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n-      int sOffsetArrElem = sMatStride * sMatShape;\n-      Value sOffsetArrElemVal =\n-          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n-\n-      Value elems[4];\n-      if (kOrder == 1) {\n-        elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n-        elems[1] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n-        elems[2] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n-        elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n-      } else {\n-        elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n-        elems[2] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n-        elems[1] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n-        elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n-      }\n-      std::array<Value, 4> retElems;\n-      retElems.fill(undef(elemTy));\n-      for (auto i = 0; i < 4; ++i) {\n-        retElems[i] = insert_element(elemTy, retElems[i], elems[i], i32_val(0));\n-      }\n-      return {retElems[0], retElems[1], retElems[2], retElems[3]};\n-    } else if (elemBytes == 1 && needTrans) { // work with int8\n-      // Can't use i32 here. Use LLVM's VectorType\n-      elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n-      std::array<std::array<Value, 4>, 2> ptrs;\n-      ptrs[0] = {\n-          getPtr(ptrIdx),\n-          getPtr(ptrIdx + 1),\n-          getPtr(ptrIdx + 2),\n-          getPtr(ptrIdx + 3),\n-      };\n-\n-      ptrs[1] = {\n-          getPtr(ptrIdx + 4),\n-          getPtr(ptrIdx + 5),\n-          getPtr(ptrIdx + 6),\n-          getPtr(ptrIdx + 7),\n-      };\n-\n-      assert(sMatStride == 1);\n-      int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n-      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n-      int sOffsetArrElem = 1 * (sMatStride * sMatShape);\n-      Value sOffsetArrElemVal =\n-          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n-\n-      std::array<Value, 4> i8v4Elems;\n-      i8v4Elems.fill(undef(elemTy));\n-\n-      Value i8Elems[4][4];\n-      if (kOrder == 1) {\n-        for (int i = 0; i < 2; ++i)\n-          for (int j = 0; j < 4; ++j)\n-            i8Elems[i][j] = load(gep(shemPtrTy, ptrs[i][j], sOffsetElemVal));\n-\n-        for (int i = 2; i < 4; ++i)\n-          for (int j = 0; j < 4; ++j)\n-            i8Elems[i][j] =\n-                load(gep(shemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n-\n-        for (int m = 0; m < 4; ++m) {\n-          for (int e = 0; e < 4; ++e)\n-            i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n-                                          i8Elems[m][e], i32_val(e));\n-        }\n-      } else { // k first\n-        for (int j = 0; j < 4; ++j)\n-          i8Elems[0][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetElemVal));\n-        for (int j = 0; j < 4; ++j)\n-          i8Elems[2][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetElemVal));\n-        for (int j = 0; j < 4; ++j)\n-          i8Elems[1][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n-        for (int j = 0; j < 4; ++j)\n-          i8Elems[3][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n-\n-        for (int m = 0; m < 4; ++m) {\n-          for (int e = 0; e < 4; ++e)\n-            i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n-                                          i8Elems[m][e], i32_val(e));\n-        }\n-      }\n-\n-      return {bitcast(i8v4Elems[0], i32_ty), bitcast(i8v4Elems[1], i32_ty),\n-              bitcast(i8v4Elems[2], i32_ty), bitcast(i8v4Elems[3], i32_ty)};\n-    }\n-\n-    assert(false && \"Invalid smem load\");\n-    return {Value{}, Value{}, Value{}, Value{}};\n-  }\n-\n-private:\n-  SmallVector<uint32_t> order;\n-  int kOrder;\n-  SmallVector<int64_t> tileShape;\n-  SmallVector<int> instrShape;\n-  SmallVector<int> matShape;\n-  int perPhase;\n-  int maxPhase;\n-  int elemBytes;\n-  ConversionPatternRewriter &rewriter;\n-  const Location &loc;\n-  MLIRContext *ctx{};\n-\n-  int cMatShape;\n-  int sMatShape;\n-\n-  Value sStride;\n-\n-  bool needTrans;\n-  bool canUseLdmatrix;\n-\n-  int numPtrs;\n-\n-  int pLoadStrideInMat;\n-  int sMatStride;\n-\n-  int matArrStride;\n-  int warpOffStride;\n-};\n-\n-// This class helps to adapt the existing DotOpConversion to the latest\n-// DotOpOperand layout design. It decouples the exising implementation to two\n-// parts:\n-// 1. loading the specific operand matrix(for $a, $b, $c) from smem\n-// 2. passing the loaded value and perform the mma codegen\n-struct MMA16816ConversionHelper {\n-  MmaEncodingAttr mmaLayout;\n-  ArrayRef<unsigned int> wpt;\n-  SmallVector<unsigned int> properWpt;\n-\n-  Value thread, lane, warp;\n-\n-  DotOpMmaV2ConversionHelper helper;\n-  ConversionPatternRewriter &rewriter;\n-  TypeConverter *typeConverter;\n-  Location loc;\n-  MLIRContext *ctx{};\n-\n-  using ValueTable = std::map<std::pair<unsigned, unsigned>, Value>;\n-\n-  // dotOperand: type of either one operand of dotOp.\n-  MMA16816ConversionHelper(Type dotOperand, MmaEncodingAttr mmaLayout,\n-                           Value thread, ConversionPatternRewriter &rewriter,\n-                           TypeConverter *typeConverter, Location loc)\n-      : mmaLayout(mmaLayout), thread(thread), helper(mmaLayout),\n-        rewriter(rewriter), typeConverter(typeConverter), loc(loc),\n-        ctx(mmaLayout.getContext()), wpt(mmaLayout.getWarpsPerCTA()) {\n-    helper.deduceMmaType(dotOperand);\n-\n-    Value _32 = i32_val(32);\n-    lane = urem(thread, _32);\n-    warp = udiv(thread, _32);\n-  }\n-\n-  // Get a warpId for M axis.\n-  Value getWarpM(int M) const {\n-    auto matShape = helper.getMmaMatShape();\n-    return urem(urem(warp, i32_val(wpt[0])), i32_val(M / matShape[0]));\n-  }\n-\n-  // Get a warpId for N axis.\n-  Value getWarpN(int N) const {\n-    auto matShape = helper.getMmaMatShape();\n-    Value warpMN = udiv(warp, i32_val(wpt[0]));\n-    return urem(urem(warpMN, i32_val(wpt[1])), i32_val(N / matShape[1]));\n-  }\n-\n-  // Get the mmaInstrShape deducing either from $a or $b.\n-  std::tuple<int, int, int> getMmaInstrShape(Type operand) const {\n-    helper.deduceMmaType(operand);\n-    auto mmaInstrShape = helper.getMmaInstrShape();\n-    int mmaInstrM = mmaInstrShape[0];\n-    int mmaInstrN = mmaInstrShape[1];\n-    int mmaInstrK = mmaInstrShape[2];\n-    return std::make_tuple(mmaInstrM, mmaInstrN, mmaInstrK);\n-  }\n-\n-  // Get the mmaMatShape deducing either from $a or $b.\n-  std::tuple<int, int, int> getMmaMatShape(Type operand) const {\n-    helper.deduceMmaType(operand);\n-    auto matShape = helper.getMmaMatShape();\n-    int matShapeM = matShape[0];\n-    int matShapeN = matShape[1];\n-    int matShapeK = matShape[2];\n-    return std::make_tuple(matShapeM, matShapeN, matShapeK);\n-  }\n-\n-  // \\param operand is either $a or $b's type.\n-  inline int getNumRepM(Type operand, int M) const {\n-    return getNumRepM(operand, M, wpt[0]);\n-  }\n-\n-  // \\param operand is either $a or $b's type.\n-  inline int getNumRepN(Type operand, int N) const {\n-    return getNumRepN(operand, N, wpt[1]);\n-  }\n-\n-  // \\param operand is either $a or $b's type.\n-  inline int getNumRepK(Type operand, int K) const {\n-    return getNumRepK_(operand, K);\n-  }\n-\n-  static int getNumRepM(Type operand, int M, int wpt) {\n-    auto tensorCoreType =\n-        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n-    int mmaInstrM =\n-        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[0];\n-    return std::max<int>(M / (wpt * mmaInstrM), 1);\n-  }\n-\n-  static int getNumRepN(Type operand, int N, int wpt) {\n-    auto tensorCoreType =\n-        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n-    int mmaInstrN =\n-        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[1];\n-    return std::max<int>(N / (wpt * mmaInstrN), 1);\n-  }\n-\n-  static int getNumRepK_(Type operand, int K) {\n-    auto tensorCoreType =\n-        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n-    int mmaInstrK =\n-        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[2];\n-    return std::max<int>(K / mmaInstrK, 1);\n-  }\n-\n-  // Get number of elements per thread for $a operand.\n-  static size_t getANumElemsPerThread(RankedTensorType operand, int wpt) {\n-    auto shape = operand.getShape();\n-    int repM = getNumRepM(operand, shape[0], wpt);\n-    int repK = getNumRepK_(operand, shape[1]);\n-    return 4 * repM * repK;\n-  }\n-\n-  // Get number of elements per thread for $b operand.\n-  static size_t getBNumElemsPerThread(RankedTensorType operand, int wpt) {\n-    auto shape = operand.getShape();\n-    int repK = getNumRepK_(operand, shape[0]);\n-    int repN = getNumRepN(operand, shape[1], wpt);\n-    return 4 * std::max(repN / 2, 1) * repK;\n-  }\n-\n-  // Loading $a from smem to registers, returns a LLVM::Struct.\n-  Value loadA(Value tensor, const SharedMemoryObject &smemObj) const {\n-    auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n-\n-    SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n-                               aTensorTy.getShape().end());\n-\n-    ValueTable ha;\n-    std::function<void(int, int)> loadFn;\n-    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n-    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(aTensorTy);\n-\n-    int numRepM = getNumRepM(aTensorTy, shape[0]);\n-    int numRepK = getNumRepK(aTensorTy, shape[1]);\n-\n-    if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n-      Value warpM = getWarpM(shape[0]);\n-      // load from smem\n-      // we use ldmatrix.x4 so each warp processes 16x16 elements.\n-      int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / 16);\n-      loadFn =\n-          getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/,\n-                          {mmaInstrM, mmaInstrK} /*instrShape*/,\n-                          {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/,\n-                          ha /*vals*/, true /*isA*/);\n-    } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n-      // load from registers, used in gemm fuse\n-      // TODO(Superjomn) Port the logic.\n-      assert(false && \"Loading A from register is not supported yet.\");\n-    } else {\n-      assert(false && \"A's layout is not supported.\");\n-    }\n-\n-    // step1. Perform loading.\n-    for (int m = 0; m < numRepM; ++m)\n-      for (int k = 0; k < numRepK; ++k)\n-        loadFn(2 * m, 2 * k);\n-\n-    // step2. Format the values to LLVM::Struct to passing to mma codegen.\n-    return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n-  }\n-\n-  // Loading $b from smem to registers, returns a LLVM::Struct.\n-  Value loadB(Value tensor, const SharedMemoryObject &smemObj) {\n-    ValueTable hb;\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-\n-    SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n-                               tensorTy.getShape().end());\n-\n-    // TODO[Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n-    bool transB = false;\n-    if (transB) {\n-      std::swap(shape[0], shape[1]);\n-    }\n-\n-    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(tensorTy);\n-    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(tensorTy);\n-    int numRepK = getNumRepK(tensorTy, shape[0]);\n-    int numRepN = getNumRepN(tensorTy, shape[1]);\n-\n-    Value warpN = getWarpN(shape[1]);\n-    // we use ldmatrix.x4 so each warp processes 16x16 elements.\n-    int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / 16);\n-    auto loadFn =\n-        getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/,\n-                        {mmaInstrK, mmaInstrN} /*instrShape*/,\n-                        {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/,\n-                        hb /*vals*/, false /*isA*/);\n-\n-    for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n-      for (int k = 0; k < numRepK; ++k)\n-        loadFn(2 * n, 2 * k);\n-    }\n-\n-    Value result = composeValuesToDotOperandLayoutStruct(\n-        hb, std::max(numRepN / 2, 1), numRepK);\n-    return result;\n-  }\n-\n-  // Loading $c to registers, returns a Value.\n-  Value loadC(Value tensor, Value llTensor) const {\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(tensorTy);\n-    size_t fcSize = 4 * repM * repN;\n-\n-    assert(tensorTy.getEncoding().isa<MmaEncodingAttr>() &&\n-           \"Currently, we only support $c with a mma layout.\");\n-    // Load a normal C tensor with mma layout, that should be a\n-    // LLVM::struct with fcSize elements.\n-    auto structTy = llTensor.getType().cast<LLVM::LLVMStructType>();\n-    assert(structTy.getBody().size() == fcSize &&\n-           \"DotOp's $c operand should pass the same number of values as $d in \"\n-           \"mma layout.\");\n-    return llTensor;\n-  }\n-\n-  // Conduct the Dot conversion.\n-  // \\param a, \\param b, \\param c and \\param d are DotOp operands.\n-  // \\param loadedA, \\param loadedB, \\param loadedC, all of them are result of\n-  // loading.\n-  LogicalResult convertDot(Value a, Value b, Value c, Value d, Value loadedA,\n-                           Value loadedB, Value loadedC, DotOp op,\n-                           DotOpAdaptor adaptor) const {\n-    helper.deduceMmaType(op);\n-\n-    auto aTensorTy = a.getType().cast<RankedTensorType>();\n-    auto dTensorTy = d.getType().cast<RankedTensorType>();\n-\n-    SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n-                                aTensorTy.getShape().end());\n-\n-    auto dShape = dTensorTy.getShape();\n-\n-    // shape / shape_per_cta\n-    int numRepM = getNumRepM(aTensorTy, dShape[0]);\n-    int numRepN = getNumRepN(aTensorTy, dShape[1]);\n-    int numRepK = getNumRepK(aTensorTy, aShape[1]);\n-\n-    ValueTable ha =\n-        getValuesFromDotOperandLayoutStruct(loadedA, numRepM, numRepK);\n-    ValueTable hb = getValuesFromDotOperandLayoutStruct(\n-        loadedB, std::max(numRepN / 2, 1), numRepK);\n-    auto fc = getElementsFromStruct(loc, loadedC, rewriter);\n-\n-    auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n-      unsigned colsPerThread = numRepN * 2;\n-      PTXBuilder builder;\n-      auto &mma = *builder.create(helper.getMmaInstr().str());\n-      // using =r for float32 works but leads to less readable ptx.\n-      bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n-      auto retArgs = builder.newListOperand(4, isIntMMA ? \"=r\" : \"=f\");\n-      auto aArgs = builder.newListOperand({\n-          {ha[{m, k}], \"r\"},\n-          {ha[{m + 1, k}], \"r\"},\n-          {ha[{m, k + 1}], \"r\"},\n-          {ha[{m + 1, k + 1}], \"r\"},\n-      });\n-      auto bArgs =\n-          builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n-      auto cArgs = builder.newListOperand();\n-      for (int i = 0; i < 4; ++i) {\n-        cArgs->listAppend(builder.newOperand(fc[m * colsPerThread + 4 * n + i],\n-                                             std::to_string(i)));\n-        // reuse the output registers\n-      }\n-\n-      mma(retArgs, aArgs, bArgs, cArgs);\n-      Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n-\n-      Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n-      for (int i = 0; i < 4; ++i)\n-        fc[m * colsPerThread + 4 * n + i] =\n-            extract_val(elemTy, mmaOut, i32_arr_attr(i));\n-    };\n-\n-    for (int k = 0; k < numRepK; ++k)\n-      for (int m = 0; m < numRepM; ++m)\n-        for (int n = 0; n < numRepN; ++n)\n-          callMma(2 * m, n, 2 * k);\n-\n-    Type resElemTy = dTensorTy.getElementType();\n-\n-    for (auto &elem : fc) {\n-      elem = bitcast(elem, resElemTy);\n-    }\n-\n-    // replace with new packed result\n-    Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(fc.size(), resElemTy));\n-    Value res = getStructFromElements(loc, fc, rewriter, structTy);\n-    rewriter.replaceOp(op, res);\n-\n-    return success();\n-  }\n-\n-private:\n-  std::function<void(int, int)>\n-  getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n-                  MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder,\n-                  SmallVector<int> instrShape, SmallVector<int> matShape,\n-                  Value warpId, ValueTable &vals, bool isA) const {\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    // We assumes that the input operand of Dot should be from shared layout.\n-    // TODO(Superjomn) Consider other layouts if needed later.\n-    auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-    const int perPhase = sharedLayout.getPerPhase();\n-    const int maxPhase = sharedLayout.getMaxPhase();\n-    const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n-    auto order = sharedLayout.getOrder();\n-\n-    // the original register_lds2, but discard the prefetch logic.\n-    auto ld2 = [](ValueTable &vals, int mn, int k, Value val) {\n-      vals[{mn, k}] = val;\n-    };\n-\n-    // (a, b) is the coordinate.\n-    auto load = [=, &vals, &ld2](int a, int b) {\n-      MMA16816SmemLoader loader(\n-          wpt, sharedLayout.getOrder(), kOrder, smemObj.strides,\n-          tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n-          maxPhase, elemBytes, rewriter, typeConverter, loc);\n-      Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n-      SmallVector<Value> offs =\n-          loader.computeOffsets(warpId, lane, cSwizzleOffset);\n-      const int numPtrs = loader.getNumPtrs();\n-      SmallVector<Value> ptrs(numPtrs);\n-\n-      Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-\n-      Type smemPtrTy = helper.getShemPtrTy();\n-      for (int i = 0; i < numPtrs; ++i) {\n-        ptrs[i] =\n-            bitcast(gep(smemPtrTy, smemBase, ValueRange({offs[i]})), smemPtrTy);\n-      }\n-\n-      auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n-          (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n-          ptrs, helper.getMatType(), helper.getShemPtrTy());\n-\n-      if (isA) {\n-        ld2(vals, a, b, ha0);\n-        ld2(vals, a + 1, b, ha1);\n-        ld2(vals, a, b + 1, ha2);\n-        ld2(vals, a + 1, b + 1, ha3);\n-      } else {\n-        ld2(vals, a, b, ha0);\n-        ld2(vals, a + 1, b, ha2);\n-        ld2(vals, a, b + 1, ha1);\n-        ld2(vals, a + 1, b + 1, ha3);\n-      }\n-    };\n-\n-    return load;\n-  }\n-\n-  // Compose a map of Values to a LLVM::Struct.\n-  // The layout is a list of Value with coordinate of (i,j), the order is as\n-  // the follows:\n-  // [\n-  //  (0,0), (0,1), (1,0), (1,1), # i=0, j=0\n-  //  (0,2), (0,3), (1,2), (1,3), # i=0, j=1\n-  //  (0,4), (0,5), (1,4), (1,5), # i=0, j=2\n-  //  ...\n-  //  (2,0), (2,1), (3,0), (3,1), # i=1, j=0\n-  //  (2,2), (2,3), (3,2), (3,3), # i=1, j=1\n-  //  (2,4), (2,5), (3,4), (3,5), # i=1, j=2\n-  //  ...\n-  // ]\n-  // i \\in [0, n0) and j \\in [0, n1)\n-  // There should be \\param n0 * \\param n1 elements in the output Struct.\n-  Value composeValuesToDotOperandLayoutStruct(const ValueTable &vals, int n0,\n-                                              int n1) const {\n-    std::vector<Value> elems;\n-    for (int m = 0; m < n0; ++m)\n-      for (int k = 0; k < n1; ++k) {\n-        elems.push_back(vals.at({2 * m, 2 * k}));\n-        elems.push_back(vals.at({2 * m, 2 * k + 1}));\n-        elems.push_back(vals.at({2 * m + 1, 2 * k}));\n-        elems.push_back(vals.at({2 * m + 1, 2 * k + 1}));\n-      }\n-\n-    assert(!elems.empty());\n-\n-    Type elemTy = elems[0].getType();\n-    Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(elems.size(), elemTy));\n-    auto result = getStructFromElements(loc, elems, rewriter, structTy);\n-    return result;\n-  }\n-\n-  ValueTable getValuesFromDotOperandLayoutStruct(Value value, int n0,\n-                                                 int n1) const {\n-    auto elems = getElementsFromStruct(loc, value, rewriter);\n-\n-    int offset{};\n-    ValueTable vals;\n-    for (int i = 0; i < n0; ++i) {\n-      for (int j = 0; j < n1; j++) {\n-        vals[{2 * i, 2 * j}] = elems[offset++];\n-        vals[{2 * i, 2 * j + 1}] = elems[offset++];\n-        vals[{2 * i + 1, 2 * j}] = elems[offset++];\n-        vals[{2 * i + 1, 2 * j + 1}] = elems[offset++];\n-      }\n-    }\n-    return vals;\n-  }\n-};\n-\n-// Helper for conversion of FMA DotOp.\n-struct DotOpFMAConversionHelper {\n-  Attribute layout;\n-  MLIRContext *ctx{};\n-\n-  using ValueTable = std::map<std::pair<int, int>, Value>;\n-\n-  explicit DotOpFMAConversionHelper(Attribute layout)\n-      : layout(layout), ctx(layout.getContext()) {}\n-\n-  SmallVector<Value>\n-  getThreadIds(Value threadId, ArrayRef<unsigned> shapePerCTA,\n-               ArrayRef<unsigned> sizePerThread, ArrayRef<unsigned> order,\n-               ConversionPatternRewriter &rewriter, Location loc) const {\n-    int dim = order.size();\n-    SmallVector<Value> threadIds(dim);\n-    for (unsigned k = 0; k < dim - 1; k++) {\n-      Value dimK = i32_val(shapePerCTA[order[k]] / sizePerThread[order[k]]);\n-      Value rem = urem(threadId, dimK);\n-      threadId = udiv(threadId, dimK);\n-      threadIds[order[k]] = rem;\n-    }\n-    Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n-    threadIds[order[dim - 1]] = urem(threadId, dimK);\n-    return threadIds;\n-  }\n-\n-  Value loadA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const {\n-    auto aTensorTy = A.getType().cast<RankedTensorType>();\n-    auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-    auto aShape = aTensorTy.getShape();\n-\n-    auto aOrder = aLayout.getOrder();\n-    auto order = dLayout.getOrder();\n-\n-    bool isARow = aOrder[0] == 1;\n-\n-    auto aSmem = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n-    Value strideAM = aSmem.strides[0];\n-    Value strideAK = aSmem.strides[1];\n-    Value strideA0 = isARow ? strideAK : strideAM;\n-    Value strideA1 = isARow ? strideAM : strideAK;\n-    int aNumPtr = 8;\n-    int K = aShape[1];\n-    int M = aShape[0];\n-\n-    auto shapePerCTA = getShapePerCTA(dLayout);\n-    auto sizePerThread = getSizePerThread(dLayout);\n-\n-    Value _0 = i32_val(0);\n-\n-    Value mContig = i32_val(sizePerThread[order[1]]);\n-\n-    // threadId in blocked layout\n-    auto threadIds =\n-        getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n-    Value threadIdM = threadIds[0];\n-\n-    Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n-    Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n-    SmallVector<Value> aOff(aNumPtr);\n-    for (int i = 0; i < aNumPtr; ++i) {\n-      aOff[i] = add(mul(offA0, strideA0), mul(offA1, strideA1));\n-    }\n-    auto elemTy = A.getType().cast<RankedTensorType>().getElementType();\n-\n-    Type ptrTy = ptr_ty(elemTy);\n-    SmallVector<Value> aPtrs(aNumPtr);\n-    for (int i = 0; i < aNumPtr; ++i)\n-      aPtrs[i] = gep(ptrTy, aSmem.base, aOff[i]);\n-\n-    SmallVector<Value> vas;\n-\n-    int mShapePerCTA = getShapePerCTAForMN(dLayout, true /*isM*/);\n-    int mSizePerThread = getSizePerThreadForMN(dLayout, true /*isM*/);\n-\n-    for (unsigned k = 0; k < K; ++k)\n-      for (unsigned m = 0; m < M; m += mShapePerCTA)\n-        for (unsigned mm = 0; mm < mSizePerThread; ++mm) {\n-          Value offset =\n-              add(mul(i32_val(m + mm), strideAM), mul(i32_val(k), strideAK));\n-          Value pa = gep(ptrTy, aPtrs[0], offset);\n-          Value va = load(pa);\n-          vas.emplace_back(va);\n-        }\n-\n-    return getStructFromValueTable(vas, rewriter, loc, elemTy);\n-  }\n-\n-  Value loadB(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const {\n-    auto bTensorTy = B.getType().cast<RankedTensorType>();\n-    auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-    auto bShape = bTensorTy.getShape();\n-\n-    auto bOrder = bLayout.getOrder();\n-    auto order = dLayout.getOrder();\n-\n-    bool isBRow = bOrder[0] == 1;\n-\n-    auto bSmem = getSharedMemoryObjectFromStruct(loc, llB, rewriter);\n-    Value strideBN = bSmem.strides[1];\n-    Value strideBK = bSmem.strides[0];\n-    Value strideB0 = isBRow ? strideBN : strideBK;\n-    Value strideB1 = isBRow ? strideBK : strideBN;\n-    int bNumPtr = 8;\n-    int K = bShape[0];\n-    int N = bShape[1];\n-\n-    auto shapePerCTA = getShapePerCTA(dLayout);\n-    auto sizePerThread = getSizePerThread(dLayout);\n-\n-    Value _0 = i32_val(0);\n-\n-    Value nContig = i32_val(sizePerThread[order[0]]);\n-\n-    // threadId in blocked layout\n-    auto threadIds =\n-        getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n-    Value threadIdN = threadIds[1];\n-\n-    Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n-    Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n-    SmallVector<Value> bOff(bNumPtr);\n-    for (int i = 0; i < bNumPtr; ++i) {\n-      bOff[i] = add(mul(offB0, strideB0), mul(offB1, strideB1));\n-    }\n-    auto elemTy = B.getType().cast<RankedTensorType>().getElementType();\n-\n-    Type ptrTy = ptr_ty(elemTy);\n-    SmallVector<Value> bPtrs(bNumPtr);\n-    for (int i = 0; i < bNumPtr; ++i)\n-      bPtrs[i] = gep(ptrTy, bSmem.base, bOff[i]);\n-\n-    SmallVector<Value> vbs;\n-\n-    int nShapePerCTA = getShapePerCTAForMN(dLayout, false /*isM*/);\n-    int nSizePerThread = getSizePerThreadForMN(dLayout, false /*isM*/);\n-\n-    for (unsigned k = 0; k < K; ++k)\n-      for (unsigned n = 0; n < N; n += nShapePerCTA)\n-        for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n-          Value offset =\n-              add(mul(i32_val(n + nn), strideBN), mul(i32_val(k), strideBK));\n-          Value pb = gep(ptrTy, bPtrs[0], offset);\n-          Value vb = load(pb);\n-          vbs.emplace_back(vb);\n-        }\n-\n-    return getStructFromValueTable(vbs, rewriter, loc, elemTy);\n-  }\n-\n-  ValueTable getValueTableFromStruct(Value val, int K, int n0, int shapePerCTA,\n-                                     int sizePerThread,\n-                                     ConversionPatternRewriter &rewriter,\n-                                     Location loc) const {\n-    ValueTable res;\n-    auto elems = getElementsFromStruct(loc, val, rewriter);\n-    int index = 0;\n-    for (unsigned k = 0; k < K; ++k) {\n-      for (unsigned m = 0; m < n0; m += shapePerCTA)\n-        for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n-          res[{m + mm, k}] = elems[index++];\n-        }\n-    }\n-    return res;\n-  }\n-\n-  Value getStructFromValueTable(ArrayRef<Value> vals,\n-                                ConversionPatternRewriter &rewriter,\n-                                Location loc, Type elemTy) const {\n-    SmallVector<Type> elemTypes(vals.size(), elemTy);\n-    SmallVector<Value> elems;\n-    elems.reserve(vals.size());\n-    for (auto &val : vals) {\n-      elems.push_back(val);\n-    }\n-\n-    Type structTy = struct_ty(elemTypes);\n-    return getStructFromElements(loc, elems, rewriter, structTy);\n-  }\n-\n-  // get number of elements per thread for $a or $b.\n-  static int getNumElemsPerThread(ArrayRef<int64_t> shape,\n-                                  DotOperandEncodingAttr dotOpLayout) {\n-    auto blockedLayout = dotOpLayout.getParent().cast<BlockedEncodingAttr>();\n-    auto shapePerCTA = getShapePerCTA(blockedLayout);\n-    auto sizePerThread = getSizePerThread(blockedLayout);\n-\n-    // TODO[Superjomn]: we assume the k aixs is fixed for $a and $b here, fix it\n-    // if not.\n-    int K = dotOpLayout.getOpIdx() == 0 ? shape[1] : shape[0];\n-    int otherDim = dotOpLayout.getOpIdx() == 1 ? shape[1] : shape[0];\n-\n-    bool isM = dotOpLayout.getOpIdx() == 0;\n-    int shapePerCTAMN = getShapePerCTAForMN(blockedLayout, isM);\n-    int sizePerThreadMN = getSizePerThreadForMN(blockedLayout, isM);\n-    return K * std::max<int>(otherDim / shapePerCTAMN, 1) * sizePerThreadMN;\n-  }\n-\n-  // Get shapePerCTA for M or N axis.\n-  static int getShapePerCTAForMN(BlockedEncodingAttr layout, bool isM) {\n-    auto order = layout.getOrder();\n-    auto shapePerCTA = getShapePerCTA(layout);\n-\n-    int mShapePerCTA =\n-        order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-    int nShapePerCTA =\n-        order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-    return isM ? mShapePerCTA : nShapePerCTA;\n-  }\n-\n-  // Get sizePerThread for M or N axis.\n-  static int getSizePerThreadForMN(BlockedEncodingAttr layout, bool isM) {\n-    auto order = layout.getOrder();\n-    auto sizePerThread = getSizePerThread(layout);\n-\n-    int mSizePerThread =\n-        order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n-    int nSizePerThread =\n-        order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n-    return isM ? mSizePerThread : nSizePerThread;\n-  }\n-};\n-\n-} // namespace LLVM\n-} // namespace mlir\n-\n-#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 20, "deletions": 254, "changes": 274, "file_content_changes": "@@ -1,18 +1,24 @@\n #include \"DotOpToLLVM.h\"\n-#include \"DotOpHelpers.h\"\n #include \"Utility.h\"\n \n using namespace mlir;\n using namespace mlir::triton;\n \n-using ::mlir::LLVM::DotOpFMAConversionHelper;\n-using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n-using ::mlir::LLVM::getElementsFromStruct;\n-using ::mlir::LLVM::getStructFromElements;\n-using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n \n+LogicalResult convertFMADot(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                            TritonGPUToLLVMTypeConverter *typeConverter,\n+                            ConversionPatternRewriter &rewriter);\n+\n+LogicalResult convertMMA884(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                            TritonGPUToLLVMTypeConverter *typeConverter,\n+                            ConversionPatternRewriter &rewriter);\n+\n+LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                              TritonGPUToLLVMTypeConverter *typeConverter,\n+                              ConversionPatternRewriter &rewriter);\n+\n struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::DotOp>::ConvertTritonGPUOpToLLVMPattern;\n@@ -21,7 +27,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n   matchAndRewrite(triton::DotOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     // D = A * B + C\n-    Value A = op.a();\n+    Value A = op.getA();\n     Value D = op.getResult();\n \n     // Here we assume the DotOp's operands always comes from shared memory.\n@@ -36,9 +42,9 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                                     .dyn_cast<MmaEncodingAttr>();\n     if (!isOuter && mmaLayout && supportMMA(op, mmaLayout.getVersionMajor())) {\n       if (mmaLayout.isVolta())\n-        return convertMMA884(op, adaptor, rewriter);\n+        return convertMMA884(op, adaptor, getTypeConverter(), rewriter);\n       if (mmaLayout.isAmpere())\n-        return convertMMA16816(op, adaptor, rewriter);\n+        return convertMMA16816(op, adaptor, getTypeConverter(), rewriter);\n \n       llvm::report_fatal_error(\n           \"Unsupported MMA kind found when converting DotOp to LLVM.\");\n@@ -48,256 +54,16 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n             .cast<RankedTensorType>()\n             .getEncoding()\n             .isa<BlockedEncodingAttr>())\n-      return convertFMADot(op, adaptor, rewriter);\n+      return convertFMADot(op, adaptor, getTypeConverter(), rewriter);\n \n     llvm::report_fatal_error(\n         \"Unsupported DotOp found when converting TritonGPU to LLVM.\");\n   }\n-\n-private:\n-  // Convert to mma.m16n8k16\n-  LogicalResult convertMMA16816(triton::DotOp op, OpAdaptor adaptor,\n-                                ConversionPatternRewriter &rewriter) const {\n-    auto loc = op.getLoc();\n-    auto mmaLayout = op.getResult()\n-                         .getType()\n-                         .cast<RankedTensorType>()\n-                         .getEncoding()\n-                         .cast<MmaEncodingAttr>();\n-\n-    Value A = op.a();\n-    Value B = op.b();\n-    Value C = op.c();\n-\n-    MMA16816ConversionHelper mmaHelper(A.getType(), mmaLayout,\n-                                       getThreadId(rewriter, loc), rewriter,\n-                                       getTypeConverter(), loc);\n-\n-    auto ATensorTy = A.getType().cast<RankedTensorType>();\n-    auto BTensorTy = B.getType().cast<RankedTensorType>();\n-\n-    assert(ATensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n-           BTensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n-           \"Both $a and %b should be DotOperand layout.\");\n-\n-    Value loadedA, loadedB, loadedC;\n-    loadedA = adaptor.a();\n-    loadedB = adaptor.b();\n-    loadedC = mmaHelper.loadC(op.c(), adaptor.c());\n-\n-    return mmaHelper.convertDot(A, B, C, op.d(), loadedA, loadedB, loadedC, op,\n-                                adaptor);\n-  }\n-  /// Convert to mma.m8n8k4\n-  LogicalResult convertMMA884(triton::DotOp op, OpAdaptor adaptor,\n-                              ConversionPatternRewriter &rewriter) const {\n-    auto *ctx = op.getContext();\n-    auto loc = op.getLoc();\n-\n-    Value A = op.a();\n-    Value B = op.b();\n-    Value D = op.getResult();\n-    auto mmaLayout = D.getType()\n-                         .cast<RankedTensorType>()\n-                         .getEncoding()\n-                         .cast<MmaEncodingAttr>();\n-    auto ALayout = A.getType()\n-                       .cast<RankedTensorType>()\n-                       .getEncoding()\n-                       .cast<DotOperandEncodingAttr>();\n-    auto BLayout = B.getType()\n-                       .cast<RankedTensorType>()\n-                       .getEncoding()\n-                       .cast<DotOperandEncodingAttr>();\n-\n-    auto ATensorTy = A.getType().cast<RankedTensorType>();\n-    auto BTensorTy = B.getType().cast<RankedTensorType>();\n-    auto DTensorTy = D.getType().cast<RankedTensorType>();\n-    auto AShape = ATensorTy.getShape();\n-    auto BShape = BTensorTy.getShape();\n-    auto DShape = DTensorTy.getShape();\n-    auto wpt = mmaLayout.getWarpsPerCTA();\n-\n-    bool isARow = ALayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    bool isBRow = BLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    auto [isARow_, isBRow_, isAVec4_, isBVec4_, mmaId] =\n-        mmaLayout.decodeVoltaLayoutStates();\n-    assert(isARow == isARow_);\n-    assert(isBRow == isBRow_);\n-\n-    DotOpMmaV1ConversionHelper helper(mmaLayout);\n-\n-    unsigned numM = helper.getNumM(AShape[0], isARow, isAVec4_);\n-    unsigned numN = helper.getNumN(BShape[1], isBRow, isBVec4_);\n-    unsigned NK = AShape[1];\n-\n-    auto has = helper.extractLoadedOperand(adaptor.a(), NK, rewriter);\n-    auto hbs = helper.extractLoadedOperand(adaptor.b(), NK, rewriter);\n-\n-    // Initialize accumulators with external values, the acc holds the\n-    // accumulator value that is shared between the MMA instructions inside a\n-    // DotOp, we can call the order of the values the accumulator-internal\n-    // order.\n-    SmallVector<Value> acc = getElementsFromStruct(loc, adaptor.c(), rewriter);\n-    size_t resSize = acc.size();\n-\n-    // The resVals holds the final result of the DotOp.\n-    // NOTE The current order of resVals is different from acc, we call it the\n-    // accumulator-external order. and\n-    SmallVector<Value> resVals(resSize);\n-\n-    auto getIdx = [&](int m, int n) {\n-      std::vector<size_t> idx{{\n-          (m * 2 + 0) + (n * 4 + 0) * numM, // row0\n-          (m * 2 + 0) + (n * 4 + 1) * numM,\n-          (m * 2 + 1) + (n * 4 + 0) * numM, // row1\n-          (m * 2 + 1) + (n * 4 + 1) * numM,\n-          (m * 2 + 0) + (n * 4 + 2) * numM, // row2\n-          (m * 2 + 0) + (n * 4 + 3) * numM,\n-          (m * 2 + 1) + (n * 4 + 2) * numM, // row3\n-          (m * 2 + 1) + (n * 4 + 3) * numM,\n-      }};\n-      return idx;\n-    };\n-\n-    auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n-      auto ha = has.at({m, k});\n-      auto hb = hbs.at({n, k});\n-\n-      PTXBuilder builder;\n-      auto idx = getIdx(m, n);\n-\n-      // note: using \"=f\" for float leads to cleaner PTX\n-      bool isIntMMA = DTensorTy.getElementType().isInteger(32);\n-      auto *resOprs = builder.newListOperand(8, isIntMMA ? \"=r\" : \"=f\");\n-      auto *AOprs = builder.newListOperand({\n-          {ha.first, \"r\"},\n-          {ha.second, \"r\"},\n-      });\n-\n-      auto *BOprs = builder.newListOperand({\n-          {hb.first, \"r\"},\n-          {hb.second, \"r\"},\n-      });\n-      auto *COprs = builder.newListOperand();\n-      for (int i = 0; i < 8; ++i)\n-        COprs->listAppend(builder.newOperand(acc[idx[i]], std::to_string(i)));\n-\n-      auto mma = builder.create(\"mma.sync.aligned.m8n8k4\")\n-                     ->o(isARow ? \"row\" : \"col\")\n-                     .o(isBRow ? \"row\" : \"col\")\n-                     .o(\"f32.f16.f16.f32\");\n-\n-      mma(resOprs, AOprs, BOprs, COprs);\n-\n-      Value res =\n-          builder.launch(rewriter, loc, helper.getMmaRetType(ATensorTy));\n-\n-      for (auto i = 0; i < 8; i++) {\n-        Value elem = extract_val(f32_ty, res, i32_arr_attr(i));\n-        acc[idx[i]] = elem;\n-      }\n-    };\n-\n-    for (unsigned k = 0; k < NK; k += 4)\n-      for (unsigned m = 0; m < numM / 2; ++m)\n-        for (unsigned n = 0; n < numN / 2; ++n) {\n-          callMMA(m, n, k);\n-        }\n-\n-    // res holds the same layout of acc\n-    for (size_t i = 0; i < acc.size(); ++i) {\n-      resVals[i] = acc[i];\n-    }\n-\n-    Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(resSize, type::f32Ty(ctx)));\n-    Value res = getStructFromElements(loc, resVals, rewriter, structTy);\n-    rewriter.replaceOp(op, res);\n-    return success();\n-  }\n-\n-  LogicalResult convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n-                              ConversionPatternRewriter &rewriter) const {\n-    auto *ctx = rewriter.getContext();\n-    auto loc = op.getLoc();\n-    auto threadId = getThreadId(rewriter, loc);\n-\n-    auto A = op.a();\n-    auto B = op.b();\n-    auto C = op.c();\n-    auto D = op.getResult();\n-\n-    auto aTensorTy = A.getType().cast<RankedTensorType>();\n-    auto bTensorTy = B.getType().cast<RankedTensorType>();\n-    auto cTensorTy = C.getType().cast<RankedTensorType>();\n-    auto dTensorTy = D.getType().cast<RankedTensorType>();\n-\n-    auto aShape = aTensorTy.getShape();\n-    auto bShape = bTensorTy.getShape();\n-    auto cShape = cTensorTy.getShape();\n-\n-    BlockedEncodingAttr dLayout =\n-        dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n-    auto order = dLayout.getOrder();\n-    auto cc = getElementsFromStruct(loc, adaptor.c(), rewriter);\n-\n-    DotOpFMAConversionHelper helper(dLayout);\n-    Value llA = adaptor.a();\n-    Value llB = adaptor.b();\n-\n-    auto sizePerThread = getSizePerThread(dLayout);\n-    auto shapePerCTA = getShapePerCTA(dLayout);\n-\n-    int K = aShape[1];\n-    int M = aShape[0];\n-    int N = bShape[1];\n-\n-    int mShapePerCTA =\n-        order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-    int mSizePerThread =\n-        order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n-    int nShapePerCTA =\n-        order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-    int nSizePerThread =\n-        order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n-\n-    auto has = helper.getValueTableFromStruct(llA, K, M, mShapePerCTA,\n-                                              mSizePerThread, rewriter, loc);\n-    auto hbs = helper.getValueTableFromStruct(llB, K, N, nShapePerCTA,\n-                                              nSizePerThread, rewriter, loc);\n-\n-    SmallVector<Value> ret = cc;\n-    bool isCRow = order[0] == 1;\n-\n-    for (unsigned k = 0; k < K; k++) {\n-      for (unsigned m = 0; m < M; m += mShapePerCTA)\n-        for (unsigned n = 0; n < N; n += nShapePerCTA)\n-          for (unsigned mm = 0; mm < mSizePerThread; ++mm)\n-            for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n-              int mIdx = m / mShapePerCTA * mSizePerThread + mm;\n-              int nIdx = n / nShapePerCTA * nSizePerThread + nn;\n-\n-              int z = isCRow ? mIdx * N / nShapePerCTA * mSizePerThread + nIdx\n-                             : nIdx * M / mShapePerCTA * nSizePerThread + mIdx;\n-              ret[z] = rewriter.create<LLVM::FMulAddOp>(\n-                  loc, has[{m + mm, k}], hbs[{n + nn, k}], ret[z]);\n-            }\n-    }\n-\n-    auto res = getStructFromElements(\n-        loc, ret, rewriter,\n-        struct_ty(SmallVector<Type>(ret.size(), ret[0].getType())));\n-    rewriter.replaceOp(op, res);\n-\n-    return success();\n-  }\n };\n \n-void populateDotOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                 RewritePatternSet &patterns, int numWarps,\n-                                 AxisInfoAnalysis &axisInfoAnalysis,\n-                                 const Allocation *allocation, Value smem,\n+void populateDotOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n+                                 RewritePatternSet &patterns,\n+                                 ModuleAllocation &allocation,\n                                  PatternBenefit benefit) {\n-  patterns.add<DotOpConversion>(typeConverter, allocation, smem, benefit);\n+  patterns.add<DotOpConversion>(typeConverter, allocation, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.h", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -6,10 +6,9 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-void populateDotOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                 RewritePatternSet &patterns, int numWarps,\n-                                 AxisInfoAnalysis &axisInfoAnalysis,\n-                                 const Allocation *allocation, Value smem,\n+void populateDotOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n+                                 RewritePatternSet &patterns,\n+                                 ModuleAllocation &allocation,\n                                  PatternBenefit benefit);\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/FMA.cpp", "status": "added", "additions": 100, "deletions": 0, "changes": 100, "file_content_changes": "@@ -0,0 +1,100 @@\n+#include \"../DotOpToLLVM.h\"\n+#include \"../Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+\n+using ValueTableFMA = std::map<std::pair<int, int>, Value>;\n+\n+static ValueTableFMA getValueTableFromStructFMA(\n+    Value val, int K, int n0, int shapePerCTA, int sizePerThread,\n+    ConversionPatternRewriter &rewriter, Location loc,\n+    TritonGPUToLLVMTypeConverter *typeConverter, Type type) {\n+  ValueTableFMA res;\n+  auto elems = typeConverter->unpackLLElements(loc, val, rewriter, type);\n+  int index = 0;\n+  for (unsigned k = 0; k < K; ++k) {\n+    for (unsigned m = 0; m < n0; m += shapePerCTA)\n+      for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n+        res[{m + mm, k}] = elems[index++];\n+      }\n+  }\n+  return res;\n+}\n+\n+LogicalResult convertFMADot(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                            TritonGPUToLLVMTypeConverter *typeConverter,\n+                            ConversionPatternRewriter &rewriter) {\n+  auto *ctx = rewriter.getContext();\n+  auto loc = op.getLoc();\n+\n+  auto A = op.getA();\n+  auto B = op.getB();\n+  auto C = op.getC();\n+  auto D = op.getResult();\n+\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto dTensorTy = D.getType().cast<RankedTensorType>();\n+\n+  auto aShape = aTensorTy.getShape();\n+  auto bShape = bTensorTy.getShape();\n+\n+  BlockedEncodingAttr dLayout =\n+      dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto order = dLayout.getOrder();\n+  auto cc =\n+      typeConverter->unpackLLElements(loc, adaptor.getC(), rewriter, dTensorTy);\n+\n+  Value llA = adaptor.getA();\n+  Value llB = adaptor.getB();\n+\n+  auto sizePerThread = getSizePerThread(dLayout);\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+\n+  int K = aShape[1];\n+  int M = aShape[0];\n+  int N = bShape[1];\n+\n+  int mShapePerCTA =\n+      order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  int mSizePerThread =\n+      order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+  int nShapePerCTA =\n+      order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  int nSizePerThread =\n+      order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+\n+  auto has =\n+      getValueTableFromStructFMA(llA, K, M, mShapePerCTA, mSizePerThread,\n+                                 rewriter, loc, typeConverter, aTensorTy);\n+  auto hbs =\n+      getValueTableFromStructFMA(llB, K, N, nShapePerCTA, nSizePerThread,\n+                                 rewriter, loc, typeConverter, bTensorTy);\n+\n+  SmallVector<Value> ret = cc;\n+  bool isCRow = order[0] == 1;\n+\n+  for (unsigned k = 0; k < K; k++) {\n+    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+      for (unsigned n = 0; n < N; n += nShapePerCTA)\n+        for (unsigned mm = 0; mm < mSizePerThread; ++mm)\n+          for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n+            int mIdx = m / mShapePerCTA * mSizePerThread + mm;\n+            int nIdx = n / nShapePerCTA * nSizePerThread + nn;\n+\n+            int z = isCRow ? mIdx * N / nShapePerCTA * mSizePerThread + nIdx\n+                           : nIdx * M / mShapePerCTA * nSizePerThread + mIdx;\n+            ret[z] = rewriter.create<LLVM::FMulAddOp>(loc, has[{m + mm, k}],\n+                                                      hbs[{n + nn, k}], ret[z]);\n+          }\n+  }\n+\n+  auto res = typeConverter->packLLElements(loc, ret, rewriter, dTensorTy);\n+  rewriter.replaceOp(op, res);\n+\n+  return success();\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv1.cpp", "status": "added", "additions": 161, "deletions": 0, "changes": 161, "file_content_changes": "@@ -0,0 +1,161 @@\n+#include \"../DotOpToLLVM.h\"\n+#include \"../Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+\n+using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+\n+static Type getMmaRetType(TensorType operand) {\n+  auto *ctx = operand.getContext();\n+  Type fp32Ty = type::f32Ty(ctx);\n+  // f16*f16+f32->f32\n+  return struct_ty(SmallVector<Type>{8, fp32Ty});\n+}\n+\n+static ValueTable\n+extractLoadedOperand(Value llStruct, int NK,\n+                     ConversionPatternRewriter &rewriter,\n+                     TritonGPUToLLVMTypeConverter *typeConverter, Type type) {\n+  ValueTable rcds;\n+  SmallVector<Value> elems = typeConverter->unpackLLElements(\n+      llStruct.getLoc(), llStruct, rewriter, type);\n+\n+  int offset = 0;\n+  for (int i = 0; offset < elems.size(); ++i) {\n+    for (int k = 0; k < NK; k += 4) {\n+      rcds[{i, k}] = std::make_pair(elems[offset], elems[offset + 1]);\n+      offset += 2;\n+    }\n+  }\n+\n+  return rcds;\n+}\n+\n+LogicalResult convertMMA884(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                            TritonGPUToLLVMTypeConverter *typeConverter,\n+                            ConversionPatternRewriter &rewriter) {\n+  auto *ctx = op.getContext();\n+  auto loc = op.getLoc();\n+\n+  Value A = op.getA();\n+  Value B = op.getB();\n+  Value D = op.getResult();\n+  auto mmaLayout = D.getType()\n+                       .cast<RankedTensorType>()\n+                       .getEncoding()\n+                       .cast<MmaEncodingAttr>();\n+  auto ALayout = A.getType()\n+                     .cast<RankedTensorType>()\n+                     .getEncoding()\n+                     .cast<DotOperandEncodingAttr>();\n+  auto BLayout = B.getType()\n+                     .cast<RankedTensorType>()\n+                     .getEncoding()\n+                     .cast<DotOperandEncodingAttr>();\n+\n+  auto ATensorTy = A.getType().cast<RankedTensorType>();\n+  auto BTensorTy = B.getType().cast<RankedTensorType>();\n+  auto DTensorTy = D.getType().cast<RankedTensorType>();\n+  auto AShape = ATensorTy.getShape();\n+  auto BShape = BTensorTy.getShape();\n+\n+  bool isARow = ALayout.getMMAv1IsRow();\n+  bool isBRow = BLayout.getMMAv1IsRow();\n+  auto [isARow_, isBRow_, isAVec4_, isBVec4_, _] =\n+      mmaLayout.decodeVoltaLayoutStates();\n+  assert(isARow == isARow_);\n+  assert(isBRow == isBRow_);\n+\n+  unsigned numM = ALayout.getMMAv1NumOuter(AShape);\n+  unsigned numN = BLayout.getMMAv1NumOuter(BShape);\n+  unsigned NK = AShape[1];\n+\n+  auto has = extractLoadedOperand(adaptor.getA(), NK, rewriter, typeConverter,\n+                                  ATensorTy);\n+  auto hbs = extractLoadedOperand(adaptor.getB(), NK, rewriter, typeConverter,\n+                                  BTensorTy);\n+\n+  // Initialize accumulators with external values, the acc holds the\n+  // accumulator value that is shared between the MMA instructions inside a\n+  // DotOp, we can call the order of the values the accumulator-internal\n+  // order.\n+  SmallVector<Value> acc =\n+      typeConverter->unpackLLElements(loc, adaptor.getC(), rewriter, DTensorTy);\n+  size_t resSize = acc.size();\n+\n+  // The resVals holds the final result of the DotOp.\n+  // NOTE The current order of resVals is different from acc, we call it the\n+  // accumulator-external order. and\n+  SmallVector<Value> resVals(resSize);\n+\n+  auto getIdx = [&](int m, int n) {\n+    std::vector<size_t> idx{{\n+        (m * 2 + 0) + (n * 4 + 0) * numM, // row0\n+        (m * 2 + 0) + (n * 4 + 1) * numM,\n+        (m * 2 + 1) + (n * 4 + 0) * numM, // row1\n+        (m * 2 + 1) + (n * 4 + 1) * numM,\n+        (m * 2 + 0) + (n * 4 + 2) * numM, // row2\n+        (m * 2 + 0) + (n * 4 + 3) * numM,\n+        (m * 2 + 1) + (n * 4 + 2) * numM, // row3\n+        (m * 2 + 1) + (n * 4 + 3) * numM,\n+    }};\n+    return idx;\n+  };\n+\n+  auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n+    auto ha = has.at({m, k});\n+    auto hb = hbs.at({n, k});\n+\n+    PTXBuilder builder;\n+    auto idx = getIdx(m, n);\n+\n+    // note: using \"=f\" for float leads to cleaner PTX\n+    bool isIntMMA = DTensorTy.getElementType().isInteger(32);\n+    auto *resOprs = builder.newListOperand(8, isIntMMA ? \"=r\" : \"=f\");\n+    auto *AOprs = builder.newListOperand({\n+        {ha.first, \"r\"},\n+        {ha.second, \"r\"},\n+    });\n+\n+    auto *BOprs = builder.newListOperand({\n+        {hb.first, \"r\"},\n+        {hb.second, \"r\"},\n+    });\n+    auto *COprs = builder.newListOperand();\n+    for (int i = 0; i < 8; ++i)\n+      COprs->listAppend(builder.newOperand(acc[idx[i]], std::to_string(i)));\n+\n+    auto mma = builder.create(\"mma.sync.aligned.m8n8k4\")\n+                   ->o(isARow ? \"row\" : \"col\")\n+                   .o(isBRow ? \"row\" : \"col\")\n+                   .o(\"f32.f16.f16.f32\");\n+\n+    mma(resOprs, AOprs, BOprs, COprs);\n+\n+    Value res = builder.launch(rewriter, loc, getMmaRetType(ATensorTy));\n+\n+    for (auto i = 0; i < 8; i++) {\n+      Value elem = extract_val(f32_ty, res, i);\n+      acc[idx[i]] = elem;\n+    }\n+  };\n+\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      for (unsigned n = 0; n < numN / 2; ++n) {\n+        callMMA(m, n, k);\n+      }\n+\n+  // res holds the same layout of acc\n+  for (size_t i = 0; i < acc.size(); ++i) {\n+    resVals[i] = acc[i];\n+  }\n+\n+  Value res = typeConverter->packLLElements(loc, resVals, rewriter, DTensorTy);\n+  rewriter.replaceOp(op, res);\n+  return success();\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv2.cpp", "status": "added", "additions": 291, "deletions": 0, "changes": 291, "file_content_changes": "@@ -0,0 +1,291 @@\n+#include \"../DotOpToLLVM.h\"\n+#include \"../Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+\n+using ValueTableV2 = std::map<std::pair<unsigned, unsigned>, Value>;\n+\n+Value loadC(Value tensor, Value llTensor,\n+            TritonGPUToLLVMTypeConverter *typeConverter, Location loc,\n+            ConversionPatternRewriter &rewriter) {\n+  MLIRContext *ctx = tensor.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  size_t fcSize = triton::gpu::getTotalElemsPerThread(tensor.getType());\n+\n+  assert(tensorTy.getEncoding().isa<MmaEncodingAttr>() &&\n+         \"Currently, we only support $c with a mma layout.\");\n+  // Load a normal C tensor with mma layout, that should be a\n+  // LLVM::struct with fcSize elements.\n+  auto structTy = llTensor.getType().cast<LLVM::LLVMStructType>();\n+  assert(structTy.getBody().size() == fcSize &&\n+         \"DotOp's $c operand should pass the same number of values as $d in \"\n+         \"mma layout.\");\n+\n+  auto numMmaRets = tensorTy.getElementType().getIntOrFloatBitWidth() / 8;\n+  assert(numMmaRets == 4 || numMmaRets == 2);\n+  if (numMmaRets == 4) {\n+    return llTensor;\n+  } else if (numMmaRets == 2) {\n+    auto cPack = SmallVector<Value>();\n+    auto cElemTy = tensorTy.getElementType();\n+    int numCPackedElem = 4 / numMmaRets;\n+    Type cPackTy = vec_ty(cElemTy, numCPackedElem);\n+    for (int i = 0; i < fcSize; i += numCPackedElem) {\n+      Value pack = rewriter.create<LLVM::UndefOp>(loc, cPackTy);\n+      for (int j = 0; j < numCPackedElem; ++j) {\n+        pack = insert_element(\n+            cPackTy, pack, extract_val(cElemTy, llTensor, i + j), i32_val(j));\n+      }\n+      cPack.push_back(pack);\n+    }\n+\n+    Type structTy = LLVM::LLVMStructType::getLiteral(\n+        ctx, SmallVector<Type>(cPack.size(), cPackTy));\n+    Value result =\n+        typeConverter->packLLElements(loc, cPack, rewriter, structTy);\n+    return result;\n+  }\n+\n+  return llTensor;\n+}\n+\n+ValueTableV2 getValuesFromDotOperandLayoutStruct(\n+    TritonGPUToLLVMTypeConverter *typeConverter, Location loc,\n+    ConversionPatternRewriter &rewriter, Value value, int n0, int n1,\n+    RankedTensorType type) {\n+\n+  auto elems = typeConverter->unpackLLElements(loc, value, rewriter, type);\n+  int offset{};\n+  ValueTableV2 vals;\n+  for (int i = 0; i < n0; ++i) {\n+    for (int j = 0; j < n1; j++) {\n+      vals[{2 * i, 2 * j}] = elems[offset++];\n+      vals[{2 * i, 2 * j + 1}] = elems[offset++];\n+      vals[{2 * i + 1, 2 * j}] = elems[offset++];\n+      vals[{2 * i + 1, 2 * j + 1}] = elems[offset++];\n+    }\n+  }\n+  return vals;\n+}\n+\n+enum class TensorCoreType : uint8_t {\n+  // floating-point tensor core instr\n+  FP32_FP16_FP16_FP32 = 0, // default\n+  FP32_BF16_BF16_FP32,\n+  FP32_TF32_TF32_FP32,\n+  FP16_FP16_FP16_FP16,\n+  // integer tensor core instr\n+  INT32_INT1_INT1_INT32, // Not implemented\n+  INT32_INT4_INT4_INT32, // Not implemented\n+  INT32_INT8_INT8_INT32, // Not implemented\n+  //\n+  NOT_APPLICABLE,\n+};\n+\n+Type getMmaRetType(TensorCoreType mmaType, MLIRContext *ctx) {\n+  Type fp32Ty = type::f32Ty(ctx);\n+  Type fp16Ty = type::f16Ty(ctx);\n+  Type i32Ty = type::i32Ty(ctx);\n+  Type fp32x4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n+  Type i32x4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i32Ty));\n+  Type fp16x2Pack2Ty = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(2, vec_ty(fp16Ty, 2)));\n+  switch (mmaType) {\n+  case TensorCoreType::FP32_FP16_FP16_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::FP32_BF16_BF16_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::FP32_TF32_TF32_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::FP16_FP16_FP16_FP16:\n+    return fp16x2Pack2Ty;\n+  case TensorCoreType::INT32_INT8_INT8_INT32:\n+    return i32x4Ty;\n+  default:\n+    llvm::report_fatal_error(\"Unsupported mma type found\");\n+  }\n+\n+  return Type{};\n+}\n+\n+TensorCoreType getMmaType(triton::DotOp op) {\n+  Value A = op.getA();\n+  Value B = op.getB();\n+  auto aTy = A.getType().cast<RankedTensorType>();\n+  auto bTy = B.getType().cast<RankedTensorType>();\n+  // d = a*b + c\n+  auto dTy = op.getD().getType().cast<RankedTensorType>();\n+\n+  if (dTy.getElementType().isF32()) {\n+    if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n+      return TensorCoreType::FP32_FP16_FP16_FP32;\n+    if (aTy.getElementType().isBF16() && bTy.getElementType().isBF16())\n+      return TensorCoreType::FP32_BF16_BF16_FP32;\n+    if (aTy.getElementType().isF32() && bTy.getElementType().isF32() &&\n+        op.getAllowTF32())\n+      return TensorCoreType::FP32_TF32_TF32_FP32;\n+  } else if (dTy.getElementType().isInteger(32)) {\n+    if (aTy.getElementType().isInteger(8) && bTy.getElementType().isInteger(8))\n+      return TensorCoreType::INT32_INT8_INT8_INT32;\n+  } else if (dTy.getElementType().isF16()) {\n+    if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n+      return TensorCoreType::FP16_FP16_FP16_FP16;\n+  }\n+\n+  return TensorCoreType::NOT_APPLICABLE;\n+}\n+\n+inline static const std::map<TensorCoreType, std::string> mmaInstrPtx = {\n+    {TensorCoreType::FP32_FP16_FP16_FP32,\n+     \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"},\n+    {TensorCoreType::FP32_BF16_BF16_FP32,\n+     \"mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\"},\n+    {TensorCoreType::FP32_TF32_TF32_FP32,\n+     \"mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\"},\n+\n+    {TensorCoreType::INT32_INT1_INT1_INT32,\n+     \"mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc\"},\n+    {TensorCoreType::INT32_INT4_INT4_INT32,\n+     \"mma.sync.aligned.m16n8k64.row.col.satfinite.s32.s4.s4.s32\"},\n+    {TensorCoreType::INT32_INT8_INT8_INT32,\n+     \"mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\"},\n+\n+    {TensorCoreType::FP16_FP16_FP16_FP16,\n+     \"mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\"},\n+};\n+\n+LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n+                         ConversionPatternRewriter &rewriter, Location loc,\n+                         Value a, Value b, Value c, Value d, Value loadedA,\n+                         Value loadedB, Value loadedC, DotOp op,\n+                         DotOpAdaptor adaptor) {\n+  MLIRContext *ctx = c.getContext();\n+  auto aTensorTy = a.getType().cast<RankedTensorType>();\n+  auto bTensorTy = b.getType().cast<RankedTensorType>();\n+  auto dTensorTy = d.getType().cast<RankedTensorType>();\n+\n+  SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n+                              aTensorTy.getShape().end());\n+  auto dShape = dTensorTy.getShape();\n+  int bitwidth = aTensorTy.getElementType().getIntOrFloatBitWidth();\n+  auto repA =\n+      aTensorTy.getEncoding().cast<DotOperandEncodingAttr>().getMMAv2Rep(\n+          aTensorTy.getShape(), bitwidth);\n+  auto repB =\n+      bTensorTy.getEncoding().cast<DotOperandEncodingAttr>().getMMAv2Rep(\n+          bTensorTy.getShape(), bitwidth);\n+\n+  assert(repA[1] == repB[0]);\n+  int repM = repA[0], repN = repB[1], repK = repA[1];\n+\n+  // shape / shape_per_cta\n+  auto ha = getValuesFromDotOperandLayoutStruct(typeConverter, loc, rewriter,\n+                                                loadedA, repM, repK, aTensorTy);\n+  auto hb = getValuesFromDotOperandLayoutStruct(typeConverter, loc, rewriter,\n+                                                loadedB, std::max(repN / 2, 1),\n+                                                repK, bTensorTy);\n+  auto fc = typeConverter->unpackLLElements(loc, loadedC, rewriter, dTensorTy);\n+  auto numMmaRets = dTensorTy.getElementType().getIntOrFloatBitWidth() / 8;\n+  int numCPackedElem = 4 / numMmaRets;\n+\n+  auto mmaType = getMmaType(op);\n+\n+  auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n+    unsigned colsPerThread = repN * 2;\n+    PTXBuilder builder;\n+    auto &mma = *builder.create(mmaInstrPtx.at(mmaType));\n+    // using =r for float32 works but leads to less readable ptx.\n+    bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n+    bool isAccF16 = dTensorTy.getElementType().isF16();\n+    auto retArgs =\n+        builder.newListOperand(numMmaRets, isIntMMA || isAccF16 ? \"=r\" : \"=f\");\n+    auto aArgs = builder.newListOperand({\n+        {ha[{m, k}], \"r\"},\n+        {ha[{m + 1, k}], \"r\"},\n+        {ha[{m, k + 1}], \"r\"},\n+        {ha[{m + 1, k + 1}], \"r\"},\n+    });\n+    auto bArgs =\n+        builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n+    auto cArgs = builder.newListOperand();\n+    for (int i = 0; i < numMmaRets; ++i) {\n+      cArgs->listAppend(builder.newOperand(\n+          fc[(m * colsPerThread + 4 * n) / numCPackedElem + i],\n+          std::to_string(i)));\n+      // reuse the output registers\n+    }\n+\n+    mma(retArgs, aArgs, bArgs, cArgs);\n+    Value mmaOut =\n+        builder.launch(rewriter, loc, getMmaRetType(mmaType, op.getContext()));\n+\n+    Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n+    for (int i = 0; i < numMmaRets; ++i) {\n+      fc[(m * colsPerThread + 4 * n) / numCPackedElem + i] =\n+          extract_val(elemTy, mmaOut, i);\n+    }\n+  };\n+\n+  for (int k = 0; k < repK; ++k)\n+    for (int m = 0; m < repM; ++m)\n+      for (int n = 0; n < repN; ++n)\n+        callMma(2 * m, n, 2 * k);\n+\n+  Type resElemTy = dTensorTy.getElementType();\n+\n+  // replace with new packed result\n+  Type structTy = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(fc.size() * numCPackedElem, resElemTy));\n+  SmallVector<Value> results(fc.size() * numCPackedElem);\n+  for (int i = 0; i < fc.size(); ++i) {\n+    for (int j = 0; j < numCPackedElem; ++j) {\n+      results[i * numCPackedElem + j] =\n+          numCPackedElem > 1\n+              ? bitcast(extract_element(fc[i], i32_val(j)), resElemTy)\n+              : bitcast(fc[i], resElemTy);\n+    }\n+  }\n+  Value res = typeConverter->packLLElements(loc, results, rewriter, structTy);\n+\n+  rewriter.replaceOp(op, res);\n+\n+  return success();\n+}\n+\n+// Convert to mma.m16n8k16\n+LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                              TritonGPUToLLVMTypeConverter *typeConverter,\n+                              ConversionPatternRewriter &rewriter) {\n+  auto loc = op.getLoc();\n+  auto mmaLayout = op.getResult()\n+                       .getType()\n+                       .cast<RankedTensorType>()\n+                       .getEncoding()\n+                       .cast<MmaEncodingAttr>();\n+\n+  Value A = op.getA();\n+  Value B = op.getB();\n+  Value C = op.getC();\n+\n+  auto ATensorTy = A.getType().cast<RankedTensorType>();\n+  auto BTensorTy = B.getType().cast<RankedTensorType>();\n+\n+  assert(ATensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n+         BTensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n+         \"Both $a and %b should be DotOperand layout.\");\n+\n+  Value loadedA, loadedB, loadedC;\n+  loadedA = adaptor.getA();\n+  loadedB = adaptor.getB();\n+  loadedC =\n+      loadC(op.getC(), adaptor.getC(), typeConverter, op.getLoc(), rewriter);\n+\n+  return convertDot(typeConverter, rewriter, op.getLoc(), A, B, C, op.getD(),\n+                    loadedA, loadedB, loadedC, op, adaptor);\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 688, "deletions": 263, "changes": 951, "file_content_changes": "@@ -2,20 +2,150 @@\n \n using namespace mlir;\n using namespace mlir::triton;\n+using ::mlir::triton::gpu::getTotalElemsPerThread;\n+\n+static SmallVector<Value> reorderValues(const SmallVector<Value> &values,\n+                                        Type inType, Type ouType) {\n+  auto inTensorTy = inType.dyn_cast<RankedTensorType>();\n+  auto ouTensorTy = ouType.dyn_cast<RankedTensorType>();\n+  if (!inTensorTy || !ouTensorTy)\n+    return values;\n+  auto inEncoding =\n+      dyn_cast<triton::gpu::DotOperandEncodingAttr>(inTensorTy.getEncoding());\n+  auto ouEncoding =\n+      dyn_cast<triton::gpu::DotOperandEncodingAttr>(ouTensorTy.getEncoding());\n+  assert(inEncoding == ouEncoding);\n+  if (!inEncoding)\n+    return values;\n+  size_t inBitWidth = inTensorTy.getElementType().getIntOrFloatBitWidth();\n+  size_t ouBitWidth = ouTensorTy.getElementType().getIntOrFloatBitWidth();\n+  auto ouEltTy = ouTensorTy.getElementType();\n+  if (inBitWidth == ouBitWidth)\n+    return values;\n+  if (inBitWidth == 16 && ouBitWidth == 32) {\n+    SmallVector<Value> ret;\n+    for (unsigned i = 0; i < values.size(); i += 8) {\n+      ret.push_back(values[i]);\n+      ret.push_back(values[i + 1]);\n+      ret.push_back(values[i + 4]);\n+      ret.push_back(values[i + 5]);\n+      ret.push_back(values[i + 2]);\n+      ret.push_back(values[i + 3]);\n+      ret.push_back(values[i + 6]);\n+      ret.push_back(values[i + 7]);\n+    }\n+    return ret;\n+  }\n+  if (inBitWidth == 8 && ouBitWidth == 16) {\n+    SmallVector<Value> ret;\n+    for (unsigned i = 0; i < values.size(); i += 16) {\n+      ret.push_back(values[i + 0]);\n+      ret.push_back(values[i + 1]);\n+      ret.push_back(values[i + 2]);\n+      ret.push_back(values[i + 3]);\n+      ret.push_back(values[i + 8]);\n+      ret.push_back(values[i + 9]);\n+      ret.push_back(values[i + 10]);\n+      ret.push_back(values[i + 11]);\n+      ret.push_back(values[i + 4]);\n+      ret.push_back(values[i + 5]);\n+      ret.push_back(values[i + 6]);\n+      ret.push_back(values[i + 7]);\n+      ret.push_back(values[i + 12]);\n+      ret.push_back(values[i + 13]);\n+      ret.push_back(values[i + 14]);\n+      ret.push_back(values[i + 15]);\n+    }\n+    return ret;\n+    // for (unsigned i = 0; i < values.size(); i += 16) {\n+    //   ret.push_back(values[i]);\n+    //   ret.push_back(values[i + 1]);\n+    //   ret.push_back(values[i + 4]);\n+    //   ret.push_back(values[i + 5]);\n+    //   ret.push_back(values[i + 8]);\n+    //   ret.push_back(values[i + 9]);\n+    //   ret.push_back(values[i + 12]);\n+    //   ret.push_back(values[i + 13]);\n+\n+    //   ret.push_back(values[i + 2]);\n+    //   ret.push_back(values[i + 3]);\n+    //   ret.push_back(values[i + 6]);\n+    //   ret.push_back(values[i + 7]);\n+    //   ret.push_back(values[i + 10]);\n+    //   ret.push_back(values[i + 11]);\n+    //   ret.push_back(values[i + 14]);\n+    //   ret.push_back(values[i + 15]);\n+    // }\n+    return values;\n+  }\n+  llvm_unreachable(\"unimplemented code path\");\n+}\n+\n+inline SmallVector<Value> unpackI32(const SmallVector<Value> &inValues,\n+                                    Type srcTy,\n+                                    ConversionPatternRewriter &rewriter,\n+                                    Location loc,\n+                                    TypeConverter *typeConverter) {\n+  auto tensorTy = srcTy.dyn_cast<RankedTensorType>();\n+  if (!tensorTy)\n+    return inValues;\n+  auto encoding = tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>();\n+  if (!(encoding && encoding.getParent().isa<MmaEncodingAttr>()))\n+    return inValues;\n+  SmallVector<Value> outValues;\n+  for (auto v : inValues) {\n+    // cast i32 to appropriate eltType vector and extract elements\n+    auto eltType = typeConverter->convertType(tensorTy.getElementType());\n+    auto vecType = vec_ty(eltType, 32 / eltType.getIntOrFloatBitWidth());\n+    auto vec = bitcast(v, vecType);\n+    for (int i = 0; i < 32 / eltType.getIntOrFloatBitWidth(); i++) {\n+      outValues.push_back(extract_element(vec, i32_val(i)));\n+    }\n+  }\n+  return outValues;\n+}\n \n-using ::mlir::LLVM::getElementsFromStruct;\n-using ::mlir::LLVM::getStructFromElements;\n-using ::mlir::triton::gpu::getElemsPerThread;\n+inline SmallVector<Value> packI32(const SmallVector<Value> &inValues,\n+                                  Type srcTy,\n+                                  ConversionPatternRewriter &rewriter,\n+                                  Location loc, TypeConverter *typeConverter) {\n+  auto tensorTy = srcTy.dyn_cast<RankedTensorType>();\n+  if (!tensorTy)\n+    return inValues;\n+  auto encoding = tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>();\n+  if (!(encoding && encoding.getParent().isa<MmaEncodingAttr>()))\n+    return inValues;\n+  SmallVector<Value> outValues;\n+  auto eltType = typeConverter->convertType(tensorTy.getElementType());\n+  int vecWidth = 32 / eltType.getIntOrFloatBitWidth();\n+  auto vecType = vec_ty(eltType, vecWidth);\n+  for (int i = 0; i < inValues.size(); i += vecWidth) {\n+    Value vec = undef(vecType);\n+    for (int j = 0; j < vecWidth; j++) {\n+      vec = insert_element(vec, inValues[i + j], i32_val(j));\n+    }\n+    outValues.push_back(bitcast(vec, i32_ty));\n+  }\n+  return outValues;\n+}\n \n struct FpToFpOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::FpToFpOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::FpToFpOp>::ConvertTritonGPUOpToLLVMPattern;\n \n+  typedef std::function<SmallVector<Value>(\n+      Location, ConversionPatternRewriter &, const Value &, const Value &,\n+      const Value &, const Value &)>\n+      ConvertorT;\n+  /* ------------------ */\n+  // FP8 -> FP16\n+  /* ------------------ */\n+\n   static SmallVector<Value>\n   convertFp8x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n+                       const char *ptxAsm, const Value &v0, const Value &v1,\n+                       const Value &v2, const Value &v3) {\n     auto ctx = rewriter.getContext();\n     auto fp8x4VecTy = vec_ty(i8_ty, 4);\n     Value fp8x4Vec = undef(fp8x4VecTy);\n@@ -26,84 +156,67 @@ struct FpToFpOpConversion\n     fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n \n     PTXBuilder builder;\n-    auto *ptxAsm = \"{                                      \\n\"\n-                   \".reg .b32 a<2>, b<2>;                  \\n\"\n-                   \"prmt.b32 a0, 0, $2, 0x5040;            \\n\"\n-                   \"prmt.b32 a1, 0, $2, 0x7060;            \\n\"\n-                   \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"shr.b32  b0, b0, 1;                    \\n\"\n-                   \"shr.b32  b1, b1, 1;                    \\n\"\n-                   \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\"\n-                   \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\"\n-                   \"}\";\n-    auto &call = *builder.create(ptxAsm);\n+    auto &ptxOp = *builder.create(ptxAsm);\n \n     auto *o0 = builder.newOperand(\"=r\");\n     auto *o1 = builder.newOperand(\"=r\");\n     auto *i = builder.newOperand(fp8x4Vec, \"r\");\n-    call({o0, o1, i}, /*onlyAttachMLIRArgs=*/true);\n+    ptxOp({o0, o1, i}, /*onlyAttachMLIRArgs=*/true);\n \n     auto fp16x2VecTy = vec_ty(f16_ty, 2);\n     auto fp16x2x2StructTy =\n         struct_ty(SmallVector<Type>{fp16x2VecTy, fp16x2VecTy});\n     auto fp16x2x2Struct =\n         builder.launch(rewriter, loc, fp16x2x2StructTy, false);\n-    auto fp16x2Vec0 = extract_val(fp16x2VecTy, fp16x2x2Struct, i32_arr_attr(0));\n-    auto fp16x2Vec1 = extract_val(fp16x2VecTy, fp16x2x2Struct, i32_arr_attr(1));\n+    auto fp16x2Vec0 = extract_val(fp16x2VecTy, fp16x2x2Struct, 0);\n+    auto fp16x2Vec1 = extract_val(fp16x2VecTy, fp16x2x2Struct, 1);\n     return {extract_element(f16_ty, fp16x2Vec0, i32_val(0)),\n             extract_element(f16_ty, fp16x2Vec0, i32_val(1)),\n             extract_element(f16_ty, fp16x2Vec1, i32_val(0)),\n             extract_element(f16_ty, fp16x2Vec1, i32_val(1))};\n   }\n \n   static SmallVector<Value>\n-  convertFp16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto ctx = rewriter.getContext();\n-    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n-    Value fp16x2Vec0 = undef(fp16x2VecTy);\n-    Value fp16x2Vec1 = undef(fp16x2VecTy);\n-    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v0, i32_val(0));\n-    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v1, i32_val(1));\n-    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v2, i32_val(0));\n-    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v3, i32_val(1));\n-    fp16x2Vec0 = bitcast(fp16x2Vec0, i32_ty);\n-    fp16x2Vec1 = bitcast(fp16x2Vec1, i32_ty);\n+  convertFp8E4M3x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // WARN: subnormal (0bs0000xxx) are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+        \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n+        \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n+        \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n+        \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+        \"shr.b32  b0, b0, 1;                    \\n\" // b0 >>= 1\n+        \"shr.b32  b1, b1, 1;                    \\n\" // shift into fp16 position\n+        \"add.u32  b0, b0, 0x20002000;           \\n\" // b0.exp += 2**4-2**3\n+                                                    // exponent compensate = 8\n+        \"add.u32  b1, b1, 0x20002000;           \\n\" // b1 += 8<<10 | 8<<10<<16\n+        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+        \"}\";\n+    return convertFp8x4ToFp16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n \n-    PTXBuilder builder;\n-    auto *ptxAsm = \"{                                      \\n\"\n-                   \".reg .b32 a<2>, b<2>;                  \\n\"\n-                   \"shl.b32 a0, $1, 1;                     \\n\"\n-                   \"shl.b32 a1, $2, 1;                     \\n\"\n-                   \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"add.u32 a0, a0, 0x00800080;            \\n\"\n-                   \"add.u32 a1, a1, 0x00800080;            \\n\"\n-                   \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n-                   \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n-                   \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n+  static SmallVector<Value>\n+  convertFp8E5M2x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    // exponent bias of Fp8E5M2 and Fp16 are the same\n+    auto *ptxAsm = \"{                           \\n\"\n+                   \"prmt.b32 $0, 0, $2, 0x5140; \\n\\t\"\n+                   \"prmt.b32 $1, 0, $2, 0x7362; \\n\\t\"\n                    \"}\";\n-    auto &call = *builder.create(ptxAsm);\n-\n-    auto *o = builder.newOperand(\"=r\");\n-    auto *i0 = builder.newOperand(fp16x2Vec0, \"r\");\n-    auto *i1 = builder.newOperand(fp16x2Vec1, \"r\");\n-    call({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n-\n-    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n-    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n+    return convertFp8x4ToFp16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n   }\n \n+  /* ------------------ */\n+  // FP8 -> BF16\n+  /* ------------------ */\n   static SmallVector<Value>\n   convertFp8x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n+                       const char *ptxAsm, const Value &v0, const Value &v1,\n+                       const Value &v2, const Value &v3) {\n     auto ctx = rewriter.getContext();\n     auto fp8x4VecTy = vec_ty(i8_ty, 4);\n     Value fp8x4Vec = undef(fp8x4VecTy);\n@@ -114,46 +227,180 @@ struct FpToFpOpConversion\n     fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n \n     PTXBuilder builder;\n-    auto *ptxAsm = \"{                                          \\n\"\n-                   \".reg .b32 a<2>, sign<2>, nosign<2>, b<2>;  \\n\"\n-                   \"prmt.b32 a0, 0, $2, 0x5040;                \\n\"\n-                   \"prmt.b32 a1, 0, $2, 0x7060;                \\n\"\n-                   \"and.b32 sign0, a0, 0x80008000;             \\n\"\n-                   \"and.b32 sign1, a1, 0x80008000;             \\n\"\n-                   \"and.b32 nosign0, a0, 0x7fff7fff;           \\n\"\n-                   \"and.b32 nosign1, a1, 0x7fff7fff;           \\n\"\n-                   \"shr.b32 nosign0, nosign0, 4;               \\n\"\n-                   \"shr.b32 nosign1, nosign1, 4;               \\n\"\n-                   \"add.u32 nosign0, nosign0, 0x38003800;      \\n\"\n-                   \"add.u32 nosign1, nosign1, 0x38003800;      \\n\"\n-                   \"or.b32 $0, sign0, nosign0;                 \\n\"\n-                   \"or.b32 $1, sign1, nosign1;                 \\n\"\n-                   \"}\";\n-    auto &call = *builder.create(ptxAsm);\n+    auto &ptxOp = *builder.create(ptxAsm);\n \n     auto *o0 = builder.newOperand(\"=r\");\n     auto *o1 = builder.newOperand(\"=r\");\n     auto *i = builder.newOperand(fp8x4Vec, \"r\");\n-    call({o0, o1, i}, /* onlyAttachMLIRArgs */ true);\n+    ptxOp({o0, o1, i}, /* onlyAttachMLIRArgs */ true);\n \n     auto bf16x2VecTy = vec_ty(i16_ty, 2);\n     auto bf16x2x2StructTy =\n         struct_ty(SmallVector<Type>{bf16x2VecTy, bf16x2VecTy});\n     auto bf16x2x2Struct =\n         builder.launch(rewriter, loc, bf16x2x2StructTy, false);\n-    auto bf16x2Vec0 = extract_val(bf16x2VecTy, bf16x2x2Struct, i32_arr_attr(0));\n-    auto bf16x2Vec1 = extract_val(bf16x2VecTy, bf16x2x2Struct, i32_arr_attr(1));\n+    auto bf16x2Vec0 = extract_val(bf16x2VecTy, bf16x2x2Struct, 0);\n+    auto bf16x2Vec1 = extract_val(bf16x2VecTy, bf16x2x2Struct, 1);\n     return {extract_element(i16_ty, bf16x2Vec0, i32_val(0)),\n             extract_element(i16_ty, bf16x2Vec0, i32_val(1)),\n             extract_element(i16_ty, bf16x2Vec1, i32_val(0)),\n             extract_element(i16_ty, bf16x2Vec1, i32_val(1))};\n   }\n \n+  static SmallVector<Value>\n+  convertFp8E4M3x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // WARN: subnormal (0bs0000xxx) are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+        \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n+        \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n+        \"and.b32 b0, a0, 0x7fff7fff;            \\n\" // b0 = a0 & 0x7fff7fff\n+        \"and.b32 b1, a1, 0x7fff7fff;            \\n\" // (strip sign)\n+        \"shr.b32 b0, b0, 4;                     \\n\" // b0 >>= 4\n+        \"shr.b32 b1, b1, 4;                     \\n\" // shift into fp16 position\n+        \"add.u32 b0, b0, 0x3c003c00;            \\n\" // b0.exp += 2**7-2**3\n+                                                    // exponent compensate = 120\n+        \"add.u32 b1, b1, 0x3c003c00;            \\n\" // b1 += 120<<7 | 120<<7<<16\n+        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+        \"}\";\n+    return convertFp8x4ToBf16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  };\n+\n+  static SmallVector<Value>\n+  convertFp8E5M2x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // WARN: subnormal (0bs00000xx) are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+        \"prmt.b32 a0, 0, $2, 0x5140;            \\n\" // a0 = 0xf300f400\n+        \"prmt.b32 a1, 0, $2, 0x7362;            \\n\" // a1 = 0xf100f200\n+        \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n+        \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+        \"shr.b32  b0, b0, 3;                    \\n\" // b0 >>= 3\n+        \"shr.b32  b1, b1, 3;                    \\n\" // shift into bf16 position\n+        \"add.u32  b0, b0, 0x38003800;           \\n\" // b0.exp += 2**7-2**4\n+                                                    // exponent compensate = 112\n+        \"add.u32  b1, b1, 0x38003800;           \\n\" // b1 += 112<<7 | 112<<7<<16\n+        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+        \"}\";\n+    return convertFp8x4ToBf16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  };\n+\n+  /* ------------------ */\n+  // FP16 -> FP8\n+  /* ------------------ */\n+\n+  static SmallVector<Value>\n+  convertFp16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const char *ptxAsm, const Value &v0, const Value &v1,\n+                       const Value &v2, const Value &v3) {\n+    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n+    Value fp16x2Vec0 = undef(fp16x2VecTy);\n+    Value fp16x2Vec1 = undef(fp16x2VecTy);\n+    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v0, i32_val(0));\n+    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v1, i32_val(1));\n+    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v2, i32_val(0));\n+    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v3, i32_val(1));\n+    fp16x2Vec0 = bitcast(fp16x2Vec0, i32_ty);\n+    fp16x2Vec1 = bitcast(fp16x2Vec1, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto &ptxOp = *builder.create(ptxAsm);\n+\n+    auto *o = builder.newOperand(\"=r\");\n+    auto *i0 = builder.newOperand(fp16x2Vec0, \"r\");\n+    auto *i1 = builder.newOperand(fp16x2Vec1, \"r\");\n+    ptxOp({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n+\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n+    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp16x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // WARN: subnormal Fp8s are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // see Fp8E4M3x4ToFp16x4\n+        \"sub.u32 a0, $1, 0x20002000;            \\n\" // a0 = input0 - 0x20002000\n+                                                    // (compensate offset)\n+        \"sub.u32 a1, $2, 0x20002000;            \\n\" // a1 = input1 - 0x20002000\n+                                                    // (8 << 10 | 8 << 10 << 16)\n+        \"shl.b32 a0, a0, 1;                     \\n\" // a0 <<= 1\n+        \"shl.b32 a1, a1, 1;                     \\n\" // shift into fp8e4 position\n+        \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // a0 &= 0x7fff7fff\n+        \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+        \"add.u32 a0, a0, 0x00800080;            \\n\" // a0 += 0x00800080\n+        \"add.u32 a1, a1, 0x00800080;            \\n\" // (round to nearest)\n+        \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\" // b0 = a0|(0x80008000&in0)\n+        \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n+        \"prmt.b32 $0, b0, b1, 0x7531;           \\n\" // output = b1b0\n+        \"}\";\n+    return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm =\n+        \"{                            \\n\"\n+        \".reg .b32 a<2>;              \\n\"\n+        \"and.b32 a0, $1, 0x7fff7fff;  \\n\"           // a0 &= 0x7fff7fff\n+        \"and.b32 a1, $2, 0x7fff7fff;  \\n\"           // (strip sign)\n+        \"add.u32 a0, a0, 0x00800080;  \\n\"           // a0 += 0x00800080\n+        \"add.u32 a1, a1, 0x00800080;  \\n\"           // (round to nearest)\n+        \"lop3.b32 a0, $1, 0x80008000, a0, 0xea; \\n\" // a0 = a0|(0x80008000&in0)\n+        \"lop3.b32 a1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n+        \"prmt.b32 $0, a0, a1, 0x7531; \\n\\t\"         // output = a1a0\n+        \"}\";\n+    return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n+\n+  /* ------------------ */\n+  // FP32 -> FP8\n+  /* ------------------ */\n+\n+  static SmallVector<Value>\n+  convertFp32x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8E4M3x4(loc, rewriter, c0, c1, c2, c3);\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp32x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8E5M2x4(loc, rewriter, c0, c1, c2, c3);\n+  }\n+\n+  /* ------------------ */\n+  // BF16 -> FP8\n+  /* ------------------ */\n+\n   static SmallVector<Value>\n   convertBf16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto ctx = rewriter.getContext();\n+                       const char *ptxAsm, const Value &v0, const Value &v1,\n+                       const Value &v2, const Value &v3) {\n     auto bf16x2VecTy = vec_ty(i16_ty, 2);\n     Value bf16x2Vec0 = undef(bf16x2VecTy);\n     Value bf16x2Vec1 = undef(bf16x2VecTy);\n@@ -165,48 +412,12 @@ struct FpToFpOpConversion\n     bf16x2Vec1 = bitcast(bf16x2Vec1, i32_ty);\n \n     PTXBuilder builder;\n-    auto *ptxAsm = \"{                                            \\n\"\n-                   \".reg .u32 sign, sign<2>, nosign, nosign<2>;  \\n\"\n-                   \".reg .u32 fp8_min, fp8_max, rn_, zero;       \\n\"\n-                   \"mov.u32 fp8_min, 0x38003800;                 \\n\"\n-                   \"mov.u32 fp8_max, 0x3ff03ff0;                 \\n\"\n-                   \"mov.u32 rn_, 0x80008;                        \\n\"\n-                   \"mov.u32 zero, 0;                             \\n\"\n-                   \"and.b32 sign0, $1, 0x80008000;               \\n\"\n-                   \"and.b32 sign1, $2, 0x80008000;               \\n\"\n-                   \"prmt.b32 sign, sign0, sign1, 0x7531;         \\n\"\n-                   \"and.b32 nosign0, $1, 0x7fff7fff;             \\n\"\n-                   \"and.b32 nosign1, $2, 0x7fff7fff;             \\n\"\n-                   \".reg .u32 nosign_0_<2>, nosign_1_<2>;        \\n\"\n-                   \"and.b32 nosign_0_0, nosign0, 0xffff0000;     \\n\"\n-                   \"max.u32 nosign_0_0, nosign_0_0, 0x38000000;  \\n\"\n-                   \"min.u32 nosign_0_0, nosign_0_0, 0x3ff00000;  \\n\"\n-                   \"and.b32 nosign_0_1, nosign0, 0x0000ffff;     \\n\"\n-                   \"max.u32 nosign_0_1, nosign_0_1, 0x3800;      \\n\"\n-                   \"min.u32 nosign_0_1, nosign_0_1, 0x3ff0;      \\n\"\n-                   \"or.b32 nosign0, nosign_0_0, nosign_0_1;      \\n\"\n-                   \"and.b32 nosign_1_0, nosign1, 0xffff0000;     \\n\"\n-                   \"max.u32 nosign_1_0, nosign_1_0, 0x38000000;  \\n\"\n-                   \"min.u32 nosign_1_0, nosign_1_0, 0x3ff00000;  \\n\"\n-                   \"and.b32 nosign_1_1, nosign1, 0x0000ffff;     \\n\"\n-                   \"max.u32 nosign_1_1, nosign_1_1, 0x3800;      \\n\"\n-                   \"min.u32 nosign_1_1, nosign_1_1, 0x3ff0;      \\n\"\n-                   \"or.b32 nosign1, nosign_1_0, nosign_1_1;      \\n\"\n-                   \"add.u32 nosign0, nosign0, rn_;               \\n\"\n-                   \"add.u32 nosign1, nosign1, rn_;               \\n\"\n-                   \"sub.u32 nosign0, nosign0, 0x38003800;        \\n\"\n-                   \"sub.u32 nosign1, nosign1, 0x38003800;        \\n\"\n-                   \"shr.u32 nosign0, nosign0, 4;                 \\n\"\n-                   \"shr.u32 nosign1, nosign1, 4;                 \\n\"\n-                   \"prmt.b32 nosign, nosign0, nosign1, 0x6420;   \\n\"\n-                   \"or.b32 $0, nosign, sign;                     \\n\"\n-                   \"}\";\n-    auto &call = *builder.create(ptxAsm);\n+    auto &ptxOp = *builder.create(ptxAsm);\n \n     auto *o = builder.newOperand(\"=r\");\n     auto *i0 = builder.newOperand(bf16x2Vec0, \"r\");\n     auto *i1 = builder.newOperand(bf16x2Vec1, \"r\");\n-    call({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n+    ptxOp({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n \n     auto fp8x4VecTy = vec_ty(i8_ty, 4);\n     auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n@@ -217,54 +428,167 @@ struct FpToFpOpConversion\n   }\n \n   static SmallVector<Value>\n-  convertFp8x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto fp16Values = convertFp8x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+  convertBf16x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // bf16 is clamped firstly to fp8 min/max\n+        \"{                                           \\n\" // bf16=fp8>>4 + 120<<7\n+        \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n+        \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n+        \"mov.u32 fp8_min, 0x3c003c00;                \\n\" // so bf16_min = 0x3c00\n+        \"mov.u32 fp8_max, 0x43f043f0;                \\n\" // so bf16_max = 0x43f0\n+        \"mov.u32 rn_, 0x80008;                       \\n\" // round to nearest\n+        \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n+        \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n+        \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n+        \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n+        \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n+\n+        // nosign = clamp(nosign, min, max)\n+        \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n+        \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_0_0, nosign_0_0, 0x3c000000; \\n\"\n+        \"min.u32 nosign_0_0, nosign_0_0, 0x43f00000; \\n\"\n+        \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_0_1, nosign_0_1, 0x3c00;     \\n\"\n+        \"min.u32 nosign_0_1, nosign_0_1, 0x43f0;     \\n\"\n+        \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n+        \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_1_0, nosign_1_0, 0x3c000000; \\n\"\n+        \"min.u32 nosign_1_0, nosign_1_0, 0x43f00000; \\n\"\n+        \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_1_1, nosign_1_1, 0x3c00;     \\n\"\n+        \"min.u32 nosign_1_1, nosign_1_1, 0x43f0;     \\n\"\n+        \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n+\n+        \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n+        \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n+        \"sub.u32 nosign0, nosign0, 0x3c003c00;       \\n\" // nosign0-=0x3c003c00\n+        \"sub.u32 nosign1, nosign1, 0x3c003c00;       \\n\" // (compensate offset)\n+        \"shr.u32 nosign0, nosign0, 4;                \\n\" // nosign0 >>= 4\n+        \"shr.u32 nosign1, nosign1, 4;                \\n\" // shift into to fp8e4\n+        \"prmt.b32 nosign, nosign0, nosign1, 0x6420;  \\n\" // nosign0 = 0x00f100f2\n+                                                         // nosign1 = 0x00f300f4\n+                                                         // nosign = 0xf3f4f1f2\n+        \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n+        \"}\";\n+    return convertBf16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  };\n+\n+  static SmallVector<Value>\n+  convertBf16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // bf16 is clamped firstly to fp8 min/max\n+        \"{                                           \\n\" // bf16=fp8>>3 + 112<<7\n+        \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n+        \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n+        \"mov.u32 fp8_min, 0x38003800;                \\n\" // so bf16_min = 0x3800\n+        \"mov.u32 fp8_max, 0x57e057e0;                \\n\" // so bf16_max = 0x57e0\n+        \"mov.u32 rn_, 0x00100010;                    \\n\" // round to nearest\n+        \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n+        \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n+        \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n+        \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n+        \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n+\n+        // nosign = clamp(nosign, min, max)\n+        \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n+        \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_0_0, nosign_0_0, 0x38000000; \\n\"\n+        \"min.u32 nosign_0_0, nosign_0_0, 0x57e00000; \\n\"\n+        \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_0_1, nosign_0_1, 0x3800;     \\n\"\n+        \"min.u32 nosign_0_1, nosign_0_1, 0x57e0;     \\n\"\n+        \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n+        \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_1_0, nosign_1_0, 0x38000000; \\n\"\n+        \"min.u32 nosign_1_0, nosign_1_0, 0x57e00000; \\n\"\n+        \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_1_1, nosign_1_1, 0x3800;     \\n\"\n+        \"min.u32 nosign_1_1, nosign_1_1, 0x57e0;     \\n\"\n+        \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n+\n+        \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n+        \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n+        \"sub.u32 nosign0, nosign0, 0x38003800;       \\n\" // nosign0-=0x38003800\n+        \"sub.u32 nosign1, nosign1, 0x38003800;       \\n\" // (compensate offset)\n+        \"shl.b32 nosign0, nosign0, 3;                \\n\" // nosign0 <<= 3\n+        \"shl.b32 nosign1, nosign1, 3;                \\n\" // shift into to fp8e4\n+        \"prmt.b32 nosign, nosign0, nosign1, 0x7531;  \\n\" // nosign0 = 0xf100f200\n+                                                         // nosign1 = 0xf300f400\n+                                                         // nosign = 0xf3f4f1f2\n+        \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n+        \"}\";\n+    return convertBf16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n+\n+  /* ------------------ */\n+  // FP8 -> FP32\n+  /* ------------------ */\n+\n+  static SmallVector<Value>\n+  convertFp8E4M3x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto fp16Values = convertFp8E4M3x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+    return {convertFp16ToFp32(loc, rewriter, fp16Values[0]),\n+            convertFp16ToFp32(loc, rewriter, fp16Values[1]),\n+            convertFp16ToFp32(loc, rewriter, fp16Values[2]),\n+            convertFp16ToFp32(loc, rewriter, fp16Values[3])};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp8E5M2x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto fp16Values = convertFp8E5M2x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n     return {rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[0]),\n             rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[1]),\n             rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[2]),\n             rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[3])};\n   }\n \n-  static SmallVector<Value>\n-  convertFp32x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n-    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n-    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n-    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n-    return convertFp16x4ToFp8x4(loc, rewriter, c0, c1, c2, c3);\n-  }\n+  //\n \n   static SmallVector<Value>\n-  convertFp8x4ToFp64x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto fp16Values = convertFp8x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+  convertFp8E4M3x4ToFp64x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto fp16Values = convertFp8E4M3x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n     return {rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[0]),\n             rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[1]),\n             rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[2]),\n             rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[3])};\n   }\n \n   static SmallVector<Value>\n-  convertFp64x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n+  convertFp64x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n     auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n     auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n     auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n     auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n-    return convertFp16x4ToFp8x4(loc, rewriter, c0, c1, c2, c3);\n+    return convertFp16x4ToFp8E4M3x4(loc, rewriter, c0, c1, c2, c3);\n   }\n \n   static Value convertBf16ToFp32(Location loc,\n                                  ConversionPatternRewriter &rewriter,\n                                  const Value &v) {\n     PTXBuilder builder;\n-    auto &cvt = *builder.create(\"cvt.rn.f32.bf16\");\n+    auto &cvt = *builder.create(\"cvt.f32.bf16\");\n+    auto res = builder.newOperand(\"=r\");\n+    auto operand = builder.newOperand(v, \"h\");\n+    cvt(res, operand);\n+    return builder.launch(rewriter, loc, f32_ty, false);\n+  }\n+\n+  static Value convertFp16ToFp32(Location loc,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 const Value &v) {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.f32.f16\");\n     auto res = builder.newOperand(\"=r\");\n     auto operand = builder.newOperand(v, \"h\");\n     cvt(res, operand);\n@@ -284,66 +608,86 @@ struct FpToFpOpConversion\n     return builder.launch(rewriter, loc, i16_ty, false);\n   }\n \n+  static Value convertFp32ToFp16(Location loc,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 const Value &v) {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.rn.f16.f32\");\n+    auto res = builder.newOperand(\"=h\");\n+    auto operand = builder.newOperand(v, \"r\");\n+    cvt(res, operand);\n+    return builder.launch(rewriter, loc, f16_ty, false);\n+  }\n+\n+  ConvertorT getConversionFunc(Type srcTy, Type dstTy) const {\n+    auto F8E4M3TyID = TypeID::get<mlir::Float8E4M3FNType>();\n+    auto F8E5M2TyID = TypeID::get<mlir::Float8E5M2Type>();\n+    auto F16TyID = TypeID::get<mlir::Float16Type>();\n+    auto BF16TyID = TypeID::get<mlir::BFloat16Type>();\n+    auto F32TyID = TypeID::get<mlir::Float32Type>();\n+    auto F64TyID = TypeID::get<mlir::Float64Type>();\n+    static DenseMap<std::pair<TypeID, TypeID>, ConvertorT> convertorMap = {\n+        // F8 -> F16\n+        {{F8E4M3TyID, F16TyID}, convertFp8E4M3x4ToFp16x4},\n+        {{F8E5M2TyID, F16TyID}, convertFp8E5M2x4ToFp16x4},\n+        // F16 -> F8\n+        {{F16TyID, F8E4M3TyID}, convertFp16x4ToFp8E4M3x4},\n+        {{F16TyID, F8E5M2TyID}, convertFp16x4ToFp8E5M2x4},\n+        // F8 -> BF16\n+        {{F8E4M3TyID, BF16TyID}, convertFp8E4M3x4ToBf16x4},\n+        {{F8E5M2TyID, BF16TyID}, convertFp8E5M2x4ToBf16x4},\n+        // BF16 -> F8\n+        {{BF16TyID, F8E4M3TyID}, convertBf16x4ToFp8E4M3x4},\n+        {{BF16TyID, F8E5M2TyID}, convertBf16x4ToFp8E5M2x4},\n+        // F8 -> F32\n+        {{F8E4M3TyID, F32TyID}, convertFp8E4M3x4ToFp32x4},\n+        {{F8E5M2TyID, F32TyID}, convertFp8E5M2x4ToFp32x4},\n+        // F32 -> F8\n+        {{F32TyID, F8E4M3TyID}, convertFp32x4ToFp8E4M3x4},\n+        {{F32TyID, F8E5M2TyID}, convertFp32x4ToFp8E5M2x4},\n+    };\n+\n+    std::pair<TypeID, TypeID> key = {srcTy.getTypeID(), dstTy.getTypeID()};\n+    if (convertorMap.count(key) == 0) {\n+      llvm::errs() << \"Unsupported conversion from \" << srcTy << \" to \" << dstTy\n+                   << \"\\n\";\n+      llvm_unreachable(\"\");\n+    }\n+    return convertorMap.lookup(key);\n+  }\n+\n   LogicalResult\n   matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto srcTensorType = op.from().getType().cast<mlir::RankedTensorType>();\n-    auto dstTensorType = op.result().getType().cast<mlir::RankedTensorType>();\n-    auto srcEltType = srcTensorType.getElementType();\n-    auto dstEltType = dstTensorType.getElementType();\n+    // llvm::outs() << 0 << \"\\n\";\n+    auto srcTensorType = op.getFrom().getType().cast<mlir::RankedTensorType>();\n+    auto dstTensorType =\n+        op.getResult().getType().cast<mlir::RankedTensorType>();\n     auto loc = op->getLoc();\n-    auto elems = getElemsPerThread(dstTensorType);\n-    SmallVector<Value> resultVals;\n-\n-    // Select convertor\n-    if (srcEltType.isa<triton::Float8Type>() ||\n-        dstEltType.isa<triton::Float8Type>()) {\n-      std::function<SmallVector<Value>(Location, ConversionPatternRewriter &,\n-                                       const Value &, const Value &,\n-                                       const Value &, const Value &)>\n-          convertor;\n-      if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF16()) {\n-        convertor = convertFp8x4ToFp16x4;\n-      } else if (srcEltType.isF16() && dstEltType.isa<triton::Float8Type>()) {\n-        convertor = convertFp16x4ToFp8x4;\n-      } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isBF16()) {\n-        convertor = convertFp8x4ToBf16x4;\n-      } else if (srcEltType.isBF16() && dstEltType.isa<triton::Float8Type>()) {\n-        convertor = convertBf16x4ToFp8x4;\n-      } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF32()) {\n-        convertor = convertFp8x4ToFp32x4;\n-      } else if (srcEltType.isF32() && dstEltType.isa<triton::Float8Type>()) {\n-        convertor = convertFp32x4ToFp8x4;\n-      } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF64()) {\n-        convertor = convertFp8x4ToFp64x4;\n-      } else if (srcEltType.isF64() && dstEltType.isa<triton::Float8Type>()) {\n-        convertor = convertFp64x4ToFp8x4;\n-      } else {\n-        assert(false && \"unsupported fp8 casting\");\n-      }\n-\n-      // Vectorized casting\n-      assert(elems % 4 == 0 &&\n-             \"FP8 casting only support tensors with 4-aligned sizes\");\n-      auto elements = getElementsFromStruct(loc, adaptor.from(), rewriter);\n-      for (size_t i = 0; i < elems; i += 4) {\n-        auto converted = convertor(loc, rewriter, elements[i], elements[i + 1],\n-                                   elements[i + 2], elements[i + 3]);\n-        resultVals.append(converted);\n-      }\n-    } else if (srcEltType.isBF16() && dstEltType.isF32()) {\n-      resultVals.emplace_back(convertBf16ToFp32(loc, rewriter, adaptor.from()));\n-    } else if (srcEltType.isF32() && dstEltType.isBF16()) {\n-      resultVals.emplace_back(convertFp32ToBf16(loc, rewriter, adaptor.from()));\n-    } else {\n-      assert(false && \"unsupported type casting\");\n-    }\n-\n-    assert(resultVals.size() == elems);\n-    auto convertedDstTensorType =\n-        this->getTypeConverter()->convertType(dstTensorType);\n-    auto result = getStructFromElements(loc, resultVals, rewriter,\n-                                        convertedDstTensorType);\n+    // check that the number of elements is divisible by 4\n+    // Get convertor\n+    auto cvtFunc = getConversionFunc(srcTensorType.getElementType(),\n+                                     dstTensorType.getElementType());\n+    // Unpack value\n+    auto inVals = getTypeConverter()->unpackLLElements(loc, adaptor.getFrom(),\n+                                                       rewriter, srcTensorType);\n+    inVals =\n+        unpackI32(inVals, srcTensorType, rewriter, loc, getTypeConverter());\n+    // Cast\n+    SmallVector<Value> outVals;\n+    auto elems = inVals.size();\n+    assert(elems % 4 == 0 &&\n+           \"FP8 casting only support tensors with 4-aligned sizes\");\n+    for (size_t i = 0; i < elems; i += 4)\n+      outVals.append(cvtFunc(loc, rewriter, inVals[i], inVals[i + 1],\n+                             inVals[i + 2], inVals[i + 3]));\n+    // Pack values\n+    assert(outVals.size() == elems);\n+    outVals = reorderValues(outVals, srcTensorType, dstTensorType);\n+    outVals =\n+        packI32(outVals, dstTensorType, rewriter, loc, getTypeConverter());\n+    auto result = getTypeConverter()->packLLElements(loc, outVals, rewriter,\n+                                                     dstTensorType);\n     rewriter.replaceOp(op, result);\n     return success();\n   }\n@@ -355,50 +699,53 @@ class ElementwiseOpConversionBase\n public:\n   using OpAdaptor = typename SourceOp::Adaptor;\n \n-  explicit ElementwiseOpConversionBase(LLVMTypeConverter &typeConverter,\n-                                       PatternBenefit benefit = 1)\n+  explicit ElementwiseOpConversionBase(\n+      TritonGPUToLLVMTypeConverter &typeConverter, PatternBenefit benefit = 1)\n       : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n \n   LogicalResult\n   matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     auto resultTy = op.getType();\n     Location loc = op->getLoc();\n-\n-    unsigned elems = getElemsPerThread(resultTy);\n+    // element type\n     auto resultElementTy = getElementTypeOrSelf(resultTy);\n     Type elemTy = this->getTypeConverter()->convertType(resultElementTy);\n-    SmallVector<Type> types(elems, elemTy);\n-    Type structTy = this->getTypeConverter()->convertType(resultTy);\n-\n-    auto *concreteThis = static_cast<const ConcreteT *>(this);\n-    auto operands = getOperands(rewriter, adaptor, elems, loc);\n-    SmallVector<Value> resultVals(elems);\n-    for (unsigned i = 0; i < elems; ++i) {\n-      resultVals[i] = concreteThis->createDestOp(op, adaptor, rewriter, elemTy,\n-                                                 operands[i], loc);\n-      if (!bool(resultVals[i]))\n+    SmallVector<Value> resultVals;\n+    //\n+    SmallVector<SmallVector<Value>> allOperands;\n+    for (auto operand : adaptor.getOperands()) {\n+      auto argTy = op->getOperand(0).getType();\n+      auto sub_operands = this->getTypeConverter()->unpackLLElements(\n+          loc, operand, rewriter, argTy);\n+      sub_operands = unpackI32(sub_operands, argTy, rewriter, loc,\n+                               this->getTypeConverter());\n+      allOperands.resize(sub_operands.size());\n+      for (auto v : llvm::enumerate(sub_operands))\n+        allOperands[v.index()].push_back(v.value());\n+    }\n+    if (allOperands.size() == 0)\n+      allOperands.push_back({});\n+    for (const SmallVector<Value> &operands : allOperands) {\n+      Value curr =\n+          ((ConcreteT *)(this))\n+              ->createDestOp(op, adaptor, rewriter, elemTy, operands, loc);\n+      if (!bool(curr))\n         return failure();\n+      resultVals.push_back(curr);\n     }\n-    Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n+    if (op->getNumOperands() > 0) {\n+      auto argTy = op->getOperand(0).getType();\n+      resultVals = reorderValues(resultVals, argTy, resultTy);\n+    }\n+    resultVals =\n+        packI32(resultVals, resultTy, rewriter, loc, this->getTypeConverter());\n+    Value view = this->getTypeConverter()->packLLElements(loc, resultVals,\n+                                                          rewriter, resultTy);\n     rewriter.replaceOp(op, view);\n \n     return success();\n   }\n-\n-protected:\n-  SmallVector<SmallVector<Value>>\n-  getOperands(ConversionPatternRewriter &rewriter, OpAdaptor adaptor,\n-              const unsigned elems, Location loc) const {\n-    SmallVector<SmallVector<Value>> operands(elems);\n-    for (auto operand : adaptor.getOperands()) {\n-      auto sub_operands = getElementsFromStruct(loc, operand, rewriter);\n-      for (size_t i = 0; i < elems; ++i) {\n-        operands[i].push_back(sub_operands[i]);\n-      }\n-    }\n-    return operands;\n-  }\n };\n \n template <typename SourceOp, typename DestOp>\n@@ -438,7 +785,7 @@ struct CmpIOpConversion\n                             ConversionPatternRewriter &rewriter, Type elemTy,\n                             ValueRange operands, Location loc) const {\n     return rewriter.create<LLVM::ICmpOp>(\n-        loc, elemTy, ArithCmpIPredicateToLLVM(op.predicate()), operands[0],\n+        loc, elemTy, ArithCmpIPredicateToLLVM(op.getPredicate()), operands[0],\n         operands[1]);\n   }\n \n@@ -462,7 +809,7 @@ struct CmpIOpConversion\n \n #undef __PRED_ENUM\n     }\n-    return LLVM::ICmpPredicate::eq;\n+    llvm_unreachable(\"Unknown arith::CmpIPredicate\");\n   }\n };\n \n@@ -480,7 +827,7 @@ struct CmpFOpConversion\n                                    Type elemTy, ValueRange operands,\n                                    Location loc) {\n     return rewriter.create<LLVM::FCmpOp>(\n-        loc, elemTy, ArithCmpFPredicateToLLVM(op.predicate()), operands[0],\n+        loc, elemTy, ArithCmpFPredicateToLLVM(op.getPredicate()), operands[0],\n         operands[1]);\n   }\n \n@@ -510,29 +857,29 @@ struct CmpFOpConversion\n \n #undef __PRED_ENUM\n     }\n-    return LLVM::FCmpPredicate::_true;\n+    llvm_unreachable(\"Unknown arith::CmpFPredicate\");\n   }\n };\n \n-struct ExtElemwiseOpConversion\n-    : public ElementwiseOpConversionBase<triton::ExtElemwiseOp,\n-                                         ExtElemwiseOpConversion> {\n-  using Base = ElementwiseOpConversionBase<triton::ExtElemwiseOp,\n-                                           ExtElemwiseOpConversion>;\n+template <class T>\n+struct ExternElementwiseOpConversion\n+    : public ElementwiseOpConversionBase<T, ExternElementwiseOpConversion<T>> {\n+  using Base = ElementwiseOpConversionBase<T, ExternElementwiseOpConversion<T>>;\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n+  typedef typename Base::OpAdaptor OpAdaptor;\n \n-  Value createDestOp(triton::ExtElemwiseOp op, OpAdaptor adaptor,\n+  Value createDestOp(T op, OpAdaptor adaptor,\n                      ConversionPatternRewriter &rewriter, Type elemTy,\n                      ValueRange operands, Location loc) const {\n-    StringRef funcName = op.symbol();\n+    StringRef funcName = op.getSymbol();\n     if (funcName.empty())\n-      llvm::errs() << \"ExtElemwiseOpConversion\";\n+      llvm::errs() << \"ExternElementwiseOpConversion\";\n \n     Type funcType = getFunctionType(elemTy, operands);\n     LLVM::LLVMFuncOp funcOp =\n         appendOrGetFuncOp(rewriter, op, funcName, funcType);\n-    return rewriter.create<LLVM::CallOp>(loc, funcOp, operands).getResult(0);\n+    return rewriter.create<LLVM::CallOp>(loc, funcOp, operands).getResult();\n   }\n \n private:\n@@ -541,8 +888,7 @@ struct ExtElemwiseOpConversion\n     return LLVM::LLVMFunctionType::get(resultType, operandTypes);\n   }\n \n-  LLVM::LLVMFuncOp appendOrGetFuncOp(ConversionPatternRewriter &rewriter,\n-                                     triton::ExtElemwiseOp op,\n+  LLVM::LLVMFuncOp appendOrGetFuncOp(ConversionPatternRewriter &rewriter, T op,\n                                      StringRef funcName, Type funcType) const {\n     using LLVM::LLVMFuncOp;\n \n@@ -551,12 +897,13 @@ struct ExtElemwiseOpConversion\n     if (funcOp)\n       return cast<LLVMFuncOp>(*funcOp);\n \n-    mlir::OpBuilder b(op->getParentOfType<LLVMFuncOp>());\n+    auto parent = ((Operation *)op)->getParentOfType<mlir::LLVM::LLVMFuncOp>();\n+    mlir::OpBuilder b(parent);\n     auto ret = b.create<LLVMFuncOp>(op->getLoc(), funcName, funcType);\n     ret.getOperation()->setAttr(\n-        \"libname\", StringAttr::get(op->getContext(), op.libname()));\n+        \"libname\", StringAttr::get(op->getContext(), op.getLibname()));\n     ret.getOperation()->setAttr(\n-        \"libpath\", StringAttr::get(op->getContext(), op.libpath()));\n+        \"libpath\", StringAttr::get(op->getContext(), op.getLibpath()));\n     return ret;\n   }\n };\n@@ -775,8 +1122,8 @@ struct ExpOpConversionApprox\n   Value createDestOp(mlir::math::ExpOp op, OpAdaptor adaptor,\n                      ConversionPatternRewriter &rewriter, Type elemTy,\n                      ValueRange operands, Location loc) const {\n-    // For FP64 input, call __nv_expf for higher-precision calculation\n-    if (elemTy.getIntOrFloatBitWidth() == 64)\n+    // For non-FP32 input, call __nv_expf for higher-precision calculation\n+    if (elemTy.getIntOrFloatBitWidth() != 32)\n       return {};\n \n     const double log2e = 1.4426950408889634;\n@@ -791,15 +1138,84 @@ struct ExpOpConversionApprox\n   }\n };\n \n-void populateElementwiseOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                         RewritePatternSet &patterns,\n-                                         int numWarps,\n-                                         AxisInfoAnalysis &axisInfoAnalysis,\n-                                         const Allocation *allocation,\n-                                         Value smem, PatternBenefit benefit) {\n+struct AbsIOpConversion\n+    : ElementwiseOpConversionBase<mlir::math::AbsIOp, AbsIOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::math::AbsIOp, AbsIOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::math::AbsIOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto boolFalse = rewriter.getBoolAttr(false);\n+    auto constFalse = rewriter.create<LLVM::ConstantOp>(loc, boolFalse);\n+    return rewriter.create<LLVM::AbsOp>(loc, elemTy, operands[0],\n+                                        /*is_int_min_poison=*/constFalse);\n+  }\n+};\n+\n+struct AbsFOpConversion\n+    : ElementwiseOpConversionBase<mlir::math::AbsFOp, AbsFOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::math::AbsFOp, AbsFOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::math::AbsFOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    if (llvm::isa<IntegerType>(elemTy)) {\n+      // Mask out the sign bit\n+      auto num_bits =\n+          getElementTypeOrSelf(op.getType()).getIntOrFloatBitWidth();\n+      assert(num_bits <= 16);\n+      auto mask = (1u << (num_bits - 1u)) - 1u;\n+      auto maskAttr = rewriter.getIntegerAttr(elemTy, mask);\n+      auto maskConst = rewriter.create<LLVM::ConstantOp>(loc, maskAttr);\n+      return and_(operands[0], maskConst);\n+    }\n+\n+    return rewriter.create<LLVM::FAbsOp>(loc, elemTy, operands[0]);\n+  }\n+};\n+\n+/// The lowering of index_cast becomes an integer conversion since index\n+/// becomes an integer.  If the bit width of the source and target integer\n+/// types is the same, just erase the cast.  If the target type is wider,\n+/// sign-extend the value, otherwise truncate it.\n+struct IndexCastOpLowering\n+    : public ElementwiseOpConversionBase<arith::IndexCastOp,\n+                                         IndexCastOpLowering> {\n+  using Base =\n+      ElementwiseOpConversionBase<arith::IndexCastOp, IndexCastOpLowering>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(arith::IndexCastOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto inElemTy =\n+        this->getTypeConverter()->convertType(getElementType(op.getIn()));\n+    unsigned targetBits = elemTy.getIntOrFloatBitWidth();\n+    unsigned sourceBits = inElemTy.getIntOrFloatBitWidth();\n+\n+    if (targetBits == sourceBits)\n+      return operands[0];\n+    if (targetBits < sourceBits)\n+      return rewriter.replaceOpWithNewOp<LLVM::TruncOp>(op, elemTy,\n+                                                        operands[0]);\n+    return rewriter.replaceOpWithNewOp<LLVM::SExtOp>(op, elemTy, operands[0]);\n+  }\n+};\n+\n+void populateElementwiseOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    PatternBenefit benefit) {\n #define POPULATE_TERNARY_OP(SRC_OP, DST_OP)                                    \\\n   patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n   POPULATE_TERNARY_OP(triton::gpu::SelectOp, LLVM::SelectOp)\n+  POPULATE_TERNARY_OP(arith::SelectOp, LLVM::SelectOp)\n #undef POPULATE_TERNARY_OP\n \n #define POPULATE_BINARY_OP(SRC_OP, DST_OP)                                     \\\n@@ -812,12 +1228,14 @@ void populateElementwiseOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   POPULATE_BINARY_OP(arith::RemFOp, LLVM::FRemOp) // %\n   POPULATE_BINARY_OP(arith::RemSIOp, LLVM::SRemOp)\n   POPULATE_BINARY_OP(arith::RemUIOp, LLVM::URemOp)\n-  POPULATE_BINARY_OP(arith::AndIOp, LLVM::AndOp)   // &\n-  POPULATE_BINARY_OP(arith::OrIOp, LLVM::OrOp)     // |\n-  POPULATE_BINARY_OP(arith::XOrIOp, LLVM::XOrOp)   // ^\n-  POPULATE_BINARY_OP(arith::ShLIOp, LLVM::ShlOp)   // <<\n-  POPULATE_BINARY_OP(arith::ShRSIOp, LLVM::AShrOp) // >>\n-  POPULATE_BINARY_OP(arith::ShRUIOp, LLVM::LShrOp) // >>\n+  POPULATE_BINARY_OP(arith::AndIOp, LLVM::AndOp)    // &\n+  POPULATE_BINARY_OP(arith::OrIOp, LLVM::OrOp)      // |\n+  POPULATE_BINARY_OP(arith::XOrIOp, LLVM::XOrOp)    // ^\n+  POPULATE_BINARY_OP(arith::ShLIOp, LLVM::ShlOp)    // <<\n+  POPULATE_BINARY_OP(arith::ShRSIOp, LLVM::AShrOp)  // >>\n+  POPULATE_BINARY_OP(arith::ShRUIOp, LLVM::LShrOp)  // >>\n+  POPULATE_BINARY_OP(arith::MinFOp, LLVM::MinNumOp) // fmin\n+  POPULATE_BINARY_OP(arith::MinSIOp, LLVM::SMinOp)  // smin\n #undef POPULATE_BINARY_OP\n \n #define POPULATE_UNARY_OP(SRC_OP, DST_OP)                                      \\\n@@ -837,6 +1255,8 @@ void populateElementwiseOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   POPULATE_UNARY_OP(triton::PtrToIntOp, LLVM::PtrToIntOp)\n #undef POPULATE_UNARY_OP\n \n+  patterns.add<AbsIOpConversion>(typeConverter, benefit);\n+  patterns.add<AbsFOpConversion>(typeConverter, benefit);\n   patterns.add<CmpIOpConversion>(typeConverter, benefit);\n   patterns.add<CmpFOpConversion>(typeConverter, benefit);\n \n@@ -849,12 +1269,17 @@ void populateElementwiseOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<TruncFOpConversion>(typeConverter, benefit);\n   patterns.add<FPToSIOpConversion>(typeConverter, benefit);\n   patterns.add<SIToFPOpConversion>(typeConverter, benefit);\n+  patterns.add<IndexCastOpLowering>(typeConverter, benefit);\n \n   patterns.add<FpToFpOpConversion>(typeConverter, benefit);\n \n-  patterns.add<ExtElemwiseOpConversion>(typeConverter, benefit);\n-  // ExpOpConversionApprox will try using ex2.approx if the input type is FP32.\n-  // For FP64 input type, ExpOpConversionApprox will return failure and\n+  patterns.add<ExternElementwiseOpConversion<triton::PureExternElementwiseOp>>(\n+      typeConverter, benefit);\n+  patterns\n+      .add<ExternElementwiseOpConversion<triton::ImpureExternElementwiseOp>>(\n+          typeConverter, benefit);\n+  // ExpOpConversionApprox will try using ex2.approx if the input type is\n+  // FP32. For other input types, ExpOpConversionApprox will return failure and\n   // ElementwiseOpConversion<math::ExpOp, math::ExpOp> defined below will call\n   // __nv_expf for higher-precision calculation\n   patterns.add<ExpOpConversionApprox>(typeConverter, benefit);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.h", "status": "modified", "additions": 5, "deletions": 6, "changes": 11, "file_content_changes": "@@ -6,11 +6,10 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-void populateElementwiseOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                         RewritePatternSet &patterns,\n-                                         int numWarps,\n-                                         AxisInfoAnalysis &axisInfoAnalysis,\n-                                         const Allocation *allocation,\n-                                         Value smem, PatternBenefit benefit);\n+void populateElementwiseOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    PatternBenefit benefit);\n+\n+bool isLegalElementwiseOp(Operation *op);\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/GCNAsmFormat.cpp", "status": "added", "additions": 191, "deletions": 0, "changes": 191, "file_content_changes": "@@ -0,0 +1,191 @@\n+#include \"triton/Conversion/TritonGPUToLLVM/GCNAsmFormat.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/AsmFormat.h\"\n+#include \"llvm/Support/raw_ostream.h\"\n+#include <sstream> // unify to llvm::raw_string_ostream ?\n+\n+namespace mlir {\n+namespace triton {\n+\n+GCNInstr::Operand *\n+GCNBuilder::newOperand(mlir::Value value, StringRef constraint,\n+                       std::function<std::string(int)> formatter) {\n+  argArchive.emplace_back(std::make_unique<Operand>(value, constraint));\n+  auto *opr = argArchive.back().get();\n+  opr->repr = formatter;\n+  opr->idx = oprCounter++;\n+  return opr;\n+}\n+\n+GCNBuilder::Operand *GCNBuilder::newOperand(StringRef constraint) {\n+  // Constraint should be something like \"=r\"\n+  assert(!constraint.empty() && constraint[0] == '=');\n+  auto *opr = newOperand();\n+  opr->idx = oprCounter++;\n+  opr->constraint = constraint;\n+  return opr;\n+}\n+\n+GCNBuilder::Modifier *GCNBuilder::newModifier(StringRef modifier,\n+                                              StringRef arg) {\n+  assert(!modifier.empty());\n+  auto *mod = newModifier();\n+  mod->modifier = modifier;\n+  mod->arg = arg;\n+  return mod;\n+}\n+\n+GCNBuilder::Operand *GCNBuilder::newConstantOperand(const std::string &v) {\n+  argArchive.emplace_back(std::make_unique<Operand>());\n+  argArchive.back()->repr = [v](int idx) { return v; };\n+  return argArchive.back().get();\n+}\n+\n+GCNBuilder::Operand *GCNBuilder::newConstantOperand(int v) {\n+  std::stringstream ss;\n+  ss << \"0x\" << std::hex << v;\n+  return newConstantOperand(ss.str());\n+}\n+\n+std::string GCNBuilder::getConstraints() const {\n+  auto args = getAllArgs();\n+  llvm::SmallVector<std::string, 4> argReprs;\n+  for (auto arg : args)\n+    argReprs.push_back(arg->constraint);\n+  return strJoin(argReprs, \",\");\n+}\n+\n+llvm::SmallVector<Value, 4> GCNBuilder::getAllMLIRArgs() const {\n+  llvm::SmallVector<Value, 4> res;\n+  for (auto &arg : argArchive) {\n+    if (!arg->isList() && arg->value)\n+      res.push_back(arg->value);\n+  }\n+  return res;\n+}\n+\n+SmallVector<GCNBuilder::Operand *, 4> GCNBuilder::getAllArgs() const {\n+  llvm::SmallVector<Operand *, 4> res;\n+  for (auto &x : argArchive)\n+    if (!x->isList())\n+      res.push_back(x.get());\n+  return res;\n+}\n+\n+mlir::Value GCNBuilder::launch(ConversionPatternRewriter &rewriter,\n+                               Location loc, Type resTy, bool hasSideEffect,\n+                               bool isAlignStack,\n+                               ArrayRef<Attribute> attrs) const {\n+  auto *ctx = rewriter.getContext();\n+  auto inlineAsm = rewriter.create<LLVM::InlineAsmOp>(\n+      loc, resTy, getAllMLIRArgs(), // operands\n+      dump(),                       // asm_string\n+      getConstraints(),             // constraints\n+      hasSideEffect,                // has_side_effects\n+      isAlignStack,                 // is_align_stack\n+      LLVM::AsmDialectAttr::get(ctx,\n+                                LLVM::AsmDialect::AD_ATT), // asm_dialect\n+      ArrayAttr::get(ctx, attrs)                           // operand_attrs\n+  );\n+\n+  return inlineAsm.getRes();\n+}\n+\n+std::string GCNInstr::Operand::dump() const {\n+  if (repr)\n+    return repr(idx);\n+  if (!isList())\n+    return \"$\" + std::to_string(idx);\n+\n+  llvm::SmallVector<std::string> oprs;\n+  for (auto *opr : list)\n+    oprs.push_back(opr->dump());\n+  return strJoin(oprs, \", \");\n+}\n+\n+std::string GCNInstr::Modifier::dump() const {\n+  if (!isList())\n+    return to_str();\n+\n+  llvm::SmallVector<std::string> mods;\n+  for (auto *mod : list)\n+    mods.push_back(mod->dump());\n+  return strJoin(mods, \" \");\n+}\n+\n+GCNInstr::Operand *GCNBuilder::newAddrOperand(mlir::Value addr,\n+                                              StringRef constraint) {\n+  auto *opr = newOperand(addr, constraint);\n+  opr->repr = [](int idx) -> std::string {\n+    std::stringstream ss;\n+    ss << \"$\" << idx;\n+    return ss.str();\n+  };\n+\n+  return opr;\n+}\n+\n+std::string GCNBuilder::dump() const {\n+  llvm::SmallVector<std::string> lines;\n+  for (auto &exec : executions) {\n+    lines.push_back(exec->dump());\n+  }\n+\n+  return strJoin(lines, \"\\n\\t\");\n+}\n+\n+GCNInstrExecution &GCNInstrCommon::call(ArrayRef<Operand *> oprs,\n+                                        ArrayRef<Modifier *> mods) {\n+  builder->executions.emplace_back(\n+      std::make_unique<GCNInstrExecution>(this, oprs, mods));\n+  return *builder->executions.back();\n+}\n+\n+GCNInstrExecution &GCNInstrCommon::operator()(ArrayRef<Operand *> oprs,\n+                                              ArrayRef<Modifier *> mods) {\n+  return call(oprs, mods);\n+}\n+\n+std::string GCNInstrExecution::dump() const {\n+  std::string osStr;\n+  llvm::raw_string_ostream os(osStr);\n+\n+  std::string instrRepr = strJoin(instr->instrParts, \"_\");\n+\n+  llvm::SmallVector<std::string, 4> argReprs;\n+  for (auto *arg : argsInOrder) {\n+    argReprs.push_back(arg->dump());\n+  }\n+\n+  std::string argsRepr = strJoin(argReprs, \", \");\n+\n+  llvm::SmallVector<std::string, 4> modReprs;\n+  for (auto *mod : mods) {\n+    modReprs.push_back(mod->dump());\n+  }\n+\n+  std::string modsRepr = strJoin(modReprs, \" \");\n+  if (!modsRepr.empty()) {\n+    os << instrRepr << \" \" << argsRepr << \" \" << modsRepr;\n+  } else {\n+    os << instrRepr << \" \" << argsRepr;\n+  }\n+  os.flush();\n+  return osStr;\n+}\n+\n+SmallVector<GCNInstrExecution::Operand *>\n+GCNInstrExecution::getArgList() const {\n+  SmallVector<Operand *> args;\n+  for (auto *arg : argsInOrder) {\n+    if (arg->isList())\n+      args.insert(args.end(), arg->list.begin(), arg->list.end());\n+    else\n+      args.push_back(arg);\n+  }\n+  return args;\n+}\n+\n+} // namespace triton\n+} // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 226, "deletions": 187, "changes": 413, "file_content_changes": "@@ -7,40 +7,38 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-using ::mlir::LLVM::getElementsFromStruct;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n-using ::mlir::LLVM::getStructFromElements;\n-using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getTotalElemsPerThread;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n // Contains some helper functions for both Load and Store conversions.\n struct LoadStoreConversionBase {\n-  explicit LoadStoreConversionBase(AxisInfoAnalysis &axisAnalysisPass)\n+  explicit LoadStoreConversionBase(ModuleAxisInfoAnalysis &axisAnalysisPass)\n       : axisAnalysisPass(axisAnalysisPass) {}\n \n-  // Get corresponding LLVM element values of \\param value.\n-  static SmallVector<Value> getLLVMElems(Value value, Value llValue,\n-                                         ConversionPatternRewriter &rewriter,\n-                                         Location loc) {\n-    if (!value)\n-      return {};\n-    if (!llValue.getType().isa<LLVM::LLVMStructType>())\n-      return {llValue};\n-    // Here, we assume that all inputs should have a blockedLayout\n-    auto valueVals = getElementsFromStruct(loc, llValue, rewriter);\n-    return valueVals;\n+  unsigned getContiguity(Value ptr) const {\n+    auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return 1;\n+    return axisAnalysisPass.getPtrContiguity(ptr);\n   }\n \n   unsigned getVectorSize(Value ptr) const {\n-    return axisAnalysisPass.getPtrVectorSize(ptr);\n+    auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return 1;\n+    auto contiguity = getContiguity(ptr);\n+    auto pointeeBitWidth = triton::getPointeeBitWidth(tensorTy);\n+    // The maximum vector size is 128 bits on NVIDIA GPUs.\n+    return std::min<unsigned>(128 / pointeeBitWidth, contiguity);\n   }\n \n   unsigned getMaskAlignment(Value mask) const {\n     return axisAnalysisPass.getMaskAlignment(mask);\n   }\n \n protected:\n-  AxisInfoAnalysis &axisAnalysisPass;\n+  ModuleAxisInfoAnalysis &axisAnalysisPass;\n };\n \n struct LoadOpConversion\n@@ -49,8 +47,9 @@ struct LoadOpConversion\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::LoadOp>::ConvertTritonGPUOpToLLVMPattern;\n \n-  LoadOpConversion(LLVMTypeConverter &converter,\n-                   AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n+  LoadOpConversion(TritonGPUToLLVMTypeConverter &converter,\n+                   ModuleAxisInfoAnalysis &axisAnalysisPass,\n+                   PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>(converter, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n \n@@ -60,32 +59,34 @@ struct LoadOpConversion\n     auto loc = op->getLoc();\n \n     // original values\n-    Value ptr = op.ptr();\n-    Value mask = op.mask();\n-    Value other = op.other();\n+    Value ptr = op.getPtr();\n+    Value mask = op.getMask();\n+    Value other = op.getOther();\n \n     // adaptor values\n-    Value llPtr = adaptor.ptr();\n-    Value llMask = adaptor.mask();\n-    Value llOther = adaptor.other();\n+    Value llPtr = adaptor.getPtr();\n+    Value llMask = adaptor.getMask();\n+    Value llOther = adaptor.getOther();\n \n     // Determine the vectorization size\n     Type valueTy = op.getResult().getType();\n     Type valueElemTy =\n         typeConverter->convertType(getElementTypeOrSelf(valueTy));\n     unsigned vec = getVectorSize(ptr);\n-    unsigned numElems = getElemsPerThread(ptr.getType());\n+    unsigned numElems = getTotalElemsPerThread(ptr.getType());\n     if (llMask)\n       vec = std::min<size_t>(vec, getMaskAlignment(mask));\n \n     // Get the LLVM values for pointers\n-    auto ptrElems = getLLVMElems(ptr, llPtr, rewriter, loc);\n+    auto ptrElems = getTypeConverter()->unpackLLElements(loc, llPtr, rewriter,\n+                                                         ptr.getType());\n     assert(ptrElems.size() == numElems);\n \n     // Get the LLVM values for mask\n     SmallVector<Value> maskElems;\n     if (llMask) {\n-      maskElems = getLLVMElems(mask, llMask, rewriter, loc);\n+      maskElems = getTypeConverter()->unpackLLElements(loc, llMask, rewriter,\n+                                                       mask.getType());\n       assert(maskElems.size() == numElems);\n     }\n \n@@ -96,14 +97,19 @@ struct LoadOpConversion\n     DenseElementsAttr constAttr;\n     int64_t splatVal = 0;\n     if (other && valueElemTy.isa<IntegerType>() &&\n-        matchPattern(other, m_Constant(&constAttr)) && constAttr.isSplat()) {\n+        matchPattern(other, m_Constant(&constAttr)) && constAttr.isSplat() &&\n+        constAttr.getElementType().isa<IntegerType>()) {\n       otherIsSplatConstInt = true;\n       splatVal = constAttr.getSplatValue<APInt>().getSExtValue();\n     }\n-    auto otherElems = getLLVMElems(other, llOther, rewriter, loc);\n+    SmallVector<Value> otherElems;\n+    if (other) {\n+      otherElems = getTypeConverter()->unpackLLElements(loc, llOther, rewriter,\n+                                                        other.getType());\n+    }\n \n     // vectorized iteration through all the pointer/mask/other elements\n-    const int valueElemNbits =\n+    const int valueElemNBits =\n         std::max(8u, valueElemTy.getIntOrFloatBitWidth());\n     const int numVecs = numElems / vec;\n \n@@ -112,11 +118,12 @@ struct LoadOpConversion\n       // TODO: optimization when ptr is GEP with constant offset\n       size_t in_off = 0;\n \n-      const size_t maxWordWidth = std::max<size_t>(32, valueElemNbits);\n-      const size_t totalWidth = valueElemNbits * vec;\n+      const size_t maxWordWidth = std::max<size_t>(32, valueElemNBits);\n+      const size_t totalWidth = valueElemNBits * vec;\n       const size_t width = std::min(totalWidth, maxWordWidth);\n       const size_t nWords = std::max<size_t>(1, totalWidth / width);\n-      const size_t wordNElems = width / valueElemNbits;\n+      const size_t wordNElems = width / valueElemNBits;\n+      const size_t movWidth = width < 16 ? 16 : width;\n       assert(wordNElems * nWords * numVecs == numElems);\n \n       // TODO(Superjomn) Add cache policy fields to StoreOp.\n@@ -135,7 +142,8 @@ struct LoadOpConversion\n       // prepare asm operands\n       auto *dstsOpr = ptxBuilder.newListOperand();\n       for (size_t wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n-        auto *opr = ptxBuilder.newOperand(writeConstraint); // =r operations\n+        auto *opr = ptxBuilder.newOperand(writeConstraint,\n+                                          /*init=*/true); // =r operations\n         dstsOpr->listAppend(opr);\n       }\n \n@@ -144,14 +152,14 @@ struct LoadOpConversion\n \n       // Define the instruction opcode\n       auto &ld = ptxBuilder.create<>(\"ld\")\n-                     ->o(\"volatile\", op.isVolatile())\n+                     ->o(\"volatile\", op.getIsVolatile())\n                      .global()\n-                     .o(\"ca\", op.cache() == triton::CacheModifier::CA)\n-                     .o(\"cg\", op.cache() == triton::CacheModifier::CG)\n+                     .o(\"ca\", op.getCache() == triton::CacheModifier::CA)\n+                     .o(\"cg\", op.getCache() == triton::CacheModifier::CG)\n                      .o(\"L1::evict_first\",\n-                        op.evict() == triton::EvictionPolicy::EVICT_FIRST)\n+                        op.getEvict() == triton::EvictionPolicy::EVICT_FIRST)\n                      .o(\"L1::evict_last\",\n-                        op.evict() == triton::EvictionPolicy::EVICT_LAST)\n+                        op.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n                      .o(\"L1::cache_hint\", hasL2EvictPolicy)\n                      .v(nWords)\n                      .b(width);\n@@ -170,11 +178,10 @@ struct LoadOpConversion\n       if (other) {\n         for (size_t ii = 0; ii < nWords; ++ii) {\n           // PTX doesn't support mov.u8, so we need to use mov.u16\n-          auto movWidth = width < 16 ? 16 : width;\n           PTXInstr &mov =\n               ptxBuilder.create<>(\"mov\")->o(\"u\" + std::to_string(movWidth));\n \n-          size_t size = width / valueElemNbits;\n+          size_t size = width / valueElemNBits;\n \n           auto vecTy = LLVM::getFixedVectorType(valueElemTy, size);\n           Value v = undef(vecTy);\n@@ -187,9 +194,12 @@ struct LoadOpConversion\n           v = bitcast(v, IntegerType::get(getContext(), width));\n \n           PTXInstr::Operand *opr{};\n-          if (otherIsSplatConstInt)\n+\n+          if (otherIsSplatConstInt) {\n+            for (size_t s = 0; s < 32; s += valueElemNBits)\n+              splatVal |= splatVal << valueElemNBits;\n             opr = ptxBuilder.newConstantOperand(splatVal);\n-          else\n+          } else\n             opr = ptxBuilder.newOperand(v, readConstraint);\n \n           mov(dstsOpr->listGet(ii), opr).predicateNot(pred, \"b\");\n@@ -213,16 +223,15 @@ struct LoadOpConversion\n       for (unsigned int ii = 0; ii < nWords; ++ii) {\n         Value curr;\n         if (retTy.isa<LLVM::LLVMStructType>()) {\n-          curr = extract_val(IntegerType::get(getContext(), width), ret,\n-                             i64_arr_attr(ii));\n+          curr = extract_val(IntegerType::get(getContext(), width), ret, ii);\n         } else {\n           curr = ret;\n         }\n         curr = bitcast(curr, LLVM::getFixedVectorType(valueElemTy,\n-                                                      width / valueElemNbits));\n+                                                      width / valueElemNBits));\n         rets.push_back(curr);\n       }\n-      int tmp = width / valueElemNbits;\n+      int tmp = width / valueElemNBits;\n       for (size_t ii = 0; ii < vec; ++ii) {\n         Value vecIdx = createIndexAttrConstant(\n             rewriter, loc, this->getTypeConverter()->getIndexType(), ii % tmp);\n@@ -232,8 +241,8 @@ struct LoadOpConversion\n     } // end vec\n \n     Type llvmResultStructTy = getTypeConverter()->convertType(valueTy);\n-    Value resultStruct =\n-        getStructFromElements(loc, loadedVals, rewriter, llvmResultStructTy);\n+    Value resultStruct = getTypeConverter()->packLLElements(\n+        loc, loadedVals, rewriter, llvmResultStructTy);\n     rewriter.replaceOp(op, {resultStruct});\n     return success();\n   }\n@@ -245,21 +254,21 @@ struct StoreOpConversion\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::StoreOp>::ConvertTritonGPUOpToLLVMPattern;\n \n-  StoreOpConversion(LLVMTypeConverter &converter,\n-                    AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n+  StoreOpConversion(TritonGPUToLLVMTypeConverter &converter,\n+                    ModuleAxisInfoAnalysis &axisAnalysisPass,\n+                    PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::StoreOp>(converter, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n \n   LogicalResult\n   matchAndRewrite(triton::StoreOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    Value ptr = op.ptr();\n-    Value mask = op.mask();\n-    Value value = op.value();\n+    Value ptr = op.getPtr();\n+    Value value = op.getValue();\n \n-    Value llPtr = adaptor.ptr();\n-    Value llMask = adaptor.mask();\n-    Value llValue = adaptor.value();\n+    Value llPtr = adaptor.getPtr();\n+    Value llMask = adaptor.getMask();\n+    Value llValue = adaptor.getValue();\n \n     auto loc = op->getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n@@ -269,37 +278,42 @@ struct StoreOpConversion\n         typeConverter->convertType(getElementTypeOrSelf(valueTy));\n \n     unsigned vec = getVectorSize(ptr);\n-    unsigned numElems = getElemsPerThread(ptr.getType());\n+    unsigned elemsPerThread = getTotalElemsPerThread(ptr.getType());\n \n-    auto ptrElems = getLLVMElems(ptr, llPtr, rewriter, loc);\n-    auto valueElems = getLLVMElems(value, llValue, rewriter, loc);\n+    auto ptrElems = getTypeConverter()->unpackLLElements(loc, llPtr, rewriter,\n+                                                         ptr.getType());\n+    auto valueElems = getTypeConverter()->unpackLLElements(\n+        loc, llValue, rewriter, value.getType());\n     assert(ptrElems.size() == valueElems.size());\n \n     // Determine the vectorization size\n     SmallVector<Value> maskElems;\n     if (llMask) {\n-      maskElems = getLLVMElems(mask, llMask, rewriter, loc);\n+      Value mask = op.getMask();\n+      maskElems = getTypeConverter()->unpackLLElements(loc, llMask, rewriter,\n+                                                       mask.getType());\n       assert(valueElems.size() == maskElems.size());\n \n       unsigned maskAlign = getMaskAlignment(mask);\n       vec = std::min(vec, maskAlign);\n     }\n \n+    Value mask = getMask(valueTy, rewriter, loc);\n     const size_t dtsize =\n         std::max<int>(1, valueElemTy.getIntOrFloatBitWidth() / 8);\n-    const size_t valueElemNbits = dtsize * 8;\n+    const size_t valueElemNBits = dtsize * 8;\n \n-    const int numVecs = numElems / vec;\n-    for (size_t vecStart = 0; vecStart < numElems; vecStart += vec) {\n+    const int numVecs = elemsPerThread / vec;\n+    for (size_t vecStart = 0; vecStart < elemsPerThread; vecStart += vec) {\n       // TODO: optimization when ptr is AddPtr with constant offset\n       size_t in_off = 0;\n \n-      const size_t maxWordWidth = std::max<size_t>(32, valueElemNbits);\n-      const size_t totalWidth = valueElemNbits * vec;\n+      const size_t maxWordWidth = std::max<size_t>(32, valueElemNBits);\n+      const size_t totalWidth = valueElemNBits * vec;\n       const size_t width = std::min(totalWidth, maxWordWidth);\n       const size_t nWords = std::max<size_t>(1, totalWidth / width);\n-      const size_t wordNElems = width / valueElemNbits;\n-      assert(wordNElems * nWords * numVecs == numElems);\n+      const size_t wordNElems = width / valueElemNBits;\n+      assert(wordNElems * nWords * numVecs == elemsPerThread);\n \n       // TODO(Superjomn) Add cache policy fields to StoreOp.\n       // TODO(Superjomn) Deal with cache policy here.\n@@ -317,10 +331,9 @@ struct StoreOpConversion\n           assert(elemOffset < valueElems.size());\n           Value elem = valueElems[elemOffset];\n           if (elem.getType().isInteger(1))\n-            elem = rewriter.create<LLVM::SExtOp>(loc, type::i8Ty(ctx), elem);\n+            elem = sext(i8_ty, elem);\n           elem = bitcast(elem, valueElemTy);\n \n-          Type u32Ty = typeConverter->convertType(type::u32Ty(ctx));\n           llWord = insert_element(wordTy, llWord, elem, i32_val(elemIdx));\n         }\n         llWord = bitcast(llWord, valArgTy);\n@@ -333,13 +346,20 @@ struct StoreOpConversion\n       PTXBuilder ptxBuilder;\n       auto *asmArgList = ptxBuilder.newListOperand(asmArgs);\n \n-      Value maskVal = llMask ? maskElems[vecStart] : int_val(1, 1);\n+      Value maskVal = llMask ? and_(mask, maskElems[vecStart]) : mask;\n \n       auto *asmAddr =\n           ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n \n       auto &ptxStoreInstr =\n-          ptxBuilder.create<>(\"st\")->global().v(nWords).b(width);\n+          ptxBuilder.create<>(\"st\")\n+              ->global()\n+              .o(\"L1::evict_first\",\n+                 op.getEvict() == triton::EvictionPolicy::EVICT_FIRST)\n+              .o(\"L1::evict_last\",\n+                 op.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n+              .v(nWords)\n+              .b(width);\n       ptxStoreInstr(asmAddr, asmArgList).predicate(maskVal, \"b\");\n \n       Type boolTy = getTypeConverter()->convertType(rewriter.getIntegerType(1));\n@@ -361,67 +381,67 @@ struct AtomicCASOpConversion\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::AtomicCASOp>::ConvertTritonGPUOpToLLVMPattern;\n \n-  AtomicCASOpConversion(LLVMTypeConverter &converter,\n-                        const Allocation *allocation, Value smem,\n-                        AxisInfoAnalysis &axisAnalysisPass,\n+  AtomicCASOpConversion(TritonGPUToLLVMTypeConverter &converter,\n+                        ModuleAllocation &allocation,\n+                        ModuleAxisInfoAnalysis &axisAnalysisPass,\n                         PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::AtomicCASOp>(\n-            converter, allocation, smem, benefit),\n+            converter, allocation, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n \n   LogicalResult\n   matchAndRewrite(triton::AtomicCASOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     auto loc = op.getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n-    Value ptr = op.ptr();\n \n-    Value llPtr = adaptor.ptr();\n-    Value llCmp = adaptor.cmp();\n-    Value llVal = adaptor.val();\n+    Value llPtr = adaptor.getPtr();\n+    Value llCmp = adaptor.getCmp();\n+    Value llVal = adaptor.getVal();\n \n-    auto ptrElements = getElementsFromStruct(loc, llPtr, rewriter);\n-    auto cmpElements = getElementsFromStruct(loc, llCmp, rewriter);\n-    auto valElements = getElementsFromStruct(loc, llVal, rewriter);\n+    auto ptrElements = getTypeConverter()->unpackLLElements(\n+        loc, llPtr, rewriter, op.getPtr().getType());\n+    auto cmpElements = getTypeConverter()->unpackLLElements(\n+        loc, llCmp, rewriter, op.getCmp().getType());\n+    auto valElements = getTypeConverter()->unpackLLElements(\n+        loc, llVal, rewriter, op.getVal().getType());\n \n-    auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    auto valueTy = op.getResult().getType();\n+    auto TensorTy = valueTy.dyn_cast<RankedTensorType>();\n     Type valueElemTy =\n-        valueTy ? getTypeConverter()->convertType(valueTy.getElementType())\n-                : op.getResult().getType();\n-    auto tid = tid_val();\n-    Value pred = icmp_eq(tid, i32_val(0));\n-    PTXBuilder ptxBuilderMemfence;\n-    auto memfence = ptxBuilderMemfence.create<PTXInstr>(\"membar\")->o(\"gl\");\n-    memfence();\n-    auto ASMReturnTy = void_ty(ctx);\n-    ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n+        TensorTy ? getTypeConverter()->convertType(TensorTy.getElementType())\n+                 : valueTy;\n+    auto valueElemNBits = valueElemTy.getIntOrFloatBitWidth();\n+    Value mask = getMask(valueTy, rewriter, loc);\n \n     Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());\n     atomPtr = bitcast(atomPtr, ptr_ty(valueElemTy, 3));\n-\n     Value casPtr = ptrElements[0];\n     Value casCmp = cmpElements[0];\n     Value casVal = valElements[0];\n \n     PTXBuilder ptxBuilderAtomicCAS;\n-    auto *dstOpr = ptxBuilderAtomicCAS.newOperand(\"=r\");\n+    auto *dstOpr = ptxBuilderAtomicCAS.newOperand(\"=r\", /*init=*/true);\n     auto *ptrOpr = ptxBuilderAtomicCAS.newAddrOperand(casPtr, \"l\");\n     auto *cmpOpr = ptxBuilderAtomicCAS.newOperand(casCmp, \"r\");\n     auto *valOpr = ptxBuilderAtomicCAS.newOperand(casVal, \"r\");\n     auto &atom = *ptxBuilderAtomicCAS.create<PTXInstr>(\"atom\");\n-    atom.global().o(\"cas\").o(\"b32\");\n-    atom(dstOpr, ptrOpr, cmpOpr, valOpr).predicate(pred);\n+    std::string semStr;\n+    llvm::raw_string_ostream os(semStr);\n+    os << op.getSem();\n+    atom.global().o(semStr).o(\"cas\").o(\"b32\");\n+    atom(dstOpr, ptrOpr, cmpOpr, valOpr).predicate(mask);\n     auto old = ptxBuilderAtomicCAS.launch(rewriter, loc, valueElemTy);\n     barrier();\n \n     PTXBuilder ptxBuilderStore;\n-    auto *dstOprStore = ptxBuilderStore.newAddrOperand(atomPtr, \"l\");\n+    auto *dstOprStore = ptxBuilderStore.newAddrOperand(atomPtr, \"r\");\n     auto *valOprStore = ptxBuilderStore.newOperand(old, \"r\");\n     auto &st = *ptxBuilderStore.create<PTXInstr>(\"st\");\n     st.shared().o(\"b32\");\n-    st(dstOprStore, valOprStore).predicate(pred);\n+    st(dstOprStore, valOprStore).predicate(mask);\n+    auto ASMReturnTy = void_ty(ctx);\n     ptxBuilderStore.launch(rewriter, loc, ASMReturnTy);\n-    ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n     barrier();\n     Value ret = load(atomPtr);\n     barrier();\n@@ -436,52 +456,56 @@ struct AtomicRMWOpConversion\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::AtomicRMWOp>::ConvertTritonGPUOpToLLVMPattern;\n \n-  AtomicRMWOpConversion(LLVMTypeConverter &converter,\n-                        const Allocation *allocation, Value smem,\n-                        AxisInfoAnalysis &axisAnalysisPass,\n+  AtomicRMWOpConversion(TritonGPUToLLVMTypeConverter &converter,\n+                        ModuleAllocation &allocation,\n+                        ModuleAxisInfoAnalysis &axisAnalysisPass,\n                         PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>(\n-            converter, allocation, smem, benefit),\n+            converter, allocation, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n \n   LogicalResult\n   matchAndRewrite(triton::AtomicRMWOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     auto loc = op.getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n+    //\n+    auto atomicRmwAttr = op.getAtomicRmwOp();\n \n-    auto atomicRmwAttr = op.atomic_rmw_op();\n-    Value ptr = op.ptr();\n-    Value val = op.val();\n+    Value val = op.getVal();\n+    Value ptr = op.getPtr();\n \n-    Value llPtr = adaptor.ptr();\n-    Value llVal = adaptor.val();\n-    Value llMask = adaptor.mask();\n+    Value llPtr = adaptor.getPtr();\n+    Value llVal = adaptor.getVal();\n+    Value llMask = adaptor.getMask();\n \n-    auto valElements = getElementsFromStruct(loc, llVal, rewriter);\n-    auto ptrElements = getElementsFromStruct(loc, llPtr, rewriter);\n-    auto maskElements = getElementsFromStruct(loc, llMask, rewriter);\n+    auto valElements = getTypeConverter()->unpackLLElements(\n+        loc, llVal, rewriter, val.getType());\n+    auto ptrElements = getTypeConverter()->unpackLLElements(\n+        loc, llPtr, rewriter, ptr.getType());\n+    SmallVector<Value> maskElements;\n+    if (llMask)\n+      maskElements = getTypeConverter()->unpackLLElements(\n+          loc, llMask, rewriter, op.getMask().getType());\n \n-    auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    auto valueTy = op.getResult().getType();\n+    auto tensorTy = valueTy.dyn_cast<RankedTensorType>();\n     Type valueElemTy =\n-        valueTy ? getTypeConverter()->convertType(valueTy.getElementType())\n-                : op.getResult().getType();\n-    const size_t valueElemNbits = valueElemTy.getIntOrFloatBitWidth();\n-    auto elemsPerThread = getElemsPerThread(val.getType());\n-    // vec = 1 for scalar\n+        tensorTy ? getTypeConverter()->convertType(tensorTy.getElementType())\n+                 : valueTy;\n+    const size_t valueElemNBits = valueElemTy.getIntOrFloatBitWidth();\n+    auto elemsPerThread = getTotalElemsPerThread(val.getType());\n+    // vec = 1, numElements = 1 for scalar\n     auto vec = getVectorSize(ptr);\n-    Value mask = int_val(1, 1);\n-    auto tid = tid_val();\n+    int numElems = 1;\n     // tensor\n-    if (valueTy) {\n+    if (tensorTy) {\n       auto valTy = val.getType().cast<RankedTensorType>();\n       vec = std::min<unsigned>(vec, valTy.getElementType().isF16() ? 2 : 1);\n       // mask\n-      auto shape = valueTy.getShape();\n-      auto numElements = product(shape);\n-      mask = and_(mask, icmp_slt(mul(tid, i32_val(elemsPerThread)),\n-                                 i32_val(numElements)));\n+      numElems = tensorTy.getNumElements();\n     }\n+    Value mask = getMask(valueTy, rewriter, loc);\n \n     auto vecTy = vec_ty(valueElemTy, vec);\n     SmallVector<Value> resultVals(elemsPerThread);\n@@ -494,20 +518,19 @@ struct AtomicRMWOpConversion\n       }\n \n       Value rmwPtr = ptrElements[i];\n-      Value rmwMask = maskElements[i];\n-      rmwMask = and_(rmwMask, mask);\n+      Value rmwMask = llMask ? and_(mask, maskElements[i]) : mask;\n       std::string sTy;\n       PTXBuilder ptxBuilderAtomicRMW;\n-      std::string tyId = valueElemNbits * vec == 64\n+      std::string tyId = valueElemNBits * vec == 64\n                              ? \"l\"\n-                             : (valueElemNbits * vec == 32 ? \"r\" : \"h\");\n-      auto *dstOpr = ptxBuilderAtomicRMW.newOperand(\"=\" + tyId);\n+                             : (valueElemNBits * vec == 32 ? \"r\" : \"h\");\n+      auto *dstOpr = ptxBuilderAtomicRMW.newOperand(\"=\" + tyId, /*init=*/true);\n       auto *ptrOpr = ptxBuilderAtomicRMW.newAddrOperand(rmwPtr, \"l\");\n       auto *valOpr = ptxBuilderAtomicRMW.newOperand(rmwVal, tyId);\n \n       auto &atom = ptxBuilderAtomicRMW.create<>(\"atom\")->global().o(\"gpu\");\n       auto rmwOp = stringifyRMWOp(atomicRmwAttr).str();\n-      auto sBits = std::to_string(valueElemNbits);\n+      auto sBits = std::to_string(valueElemNBits);\n       switch (atomicRmwAttr) {\n       case RMWOp::AND:\n         sTy = \"b\" + sBits;\n@@ -523,9 +546,9 @@ struct AtomicRMWOpConversion\n         break;\n       case RMWOp::FADD:\n         rmwOp = \"add\";\n-        rmwOp += (valueElemNbits == 16 ? \".noftz\" : \"\");\n+        rmwOp += (valueElemNBits == 16 ? \".noftz\" : \"\");\n         sTy = \"f\" + sBits;\n-        sTy += (vec == 2 && valueElemNbits == 16) ? \"x2\" : \"\";\n+        sTy += (vec == 2 && valueElemNBits == 16) ? \"x2\" : \"\";\n         break;\n       case RMWOp::MAX:\n         sTy = \"s\" + sBits;\n@@ -547,37 +570,46 @@ struct AtomicRMWOpConversion\n       default:\n         return failure();\n       }\n-      atom.o(rmwOp).o(sTy);\n-      if (valueTy) {\n+      std::string semStr;\n+      llvm::raw_string_ostream os(semStr);\n+      os << op.getSem();\n+      atom.o(semStr).o(rmwOp).o(sTy);\n+      if (tensorTy) {\n         atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n         auto retType = vec == 1 ? valueElemTy : vecTy;\n         auto ret = ptxBuilderAtomicRMW.launch(rewriter, loc, retType);\n         for (int ii = 0; ii < vec; ++ii) {\n           resultVals[i + ii] =\n-              vec == 1 ? ret : extract_element(valueElemTy, ret, idx_val(ii));\n+              vec == 1 ? ret : extract_element(valueElemTy, ret, i32_val(ii));\n         }\n       } else {\n-        PTXBuilder ptxBuilderMemfence;\n-        auto memfenc = ptxBuilderMemfence.create<PTXInstr>(\"membar\")->o(\"gl\");\n-        memfenc();\n         auto ASMReturnTy = void_ty(ctx);\n-        ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n-        rmwMask = and_(rmwMask, icmp_eq(tid, i32_val(0)));\n         atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n         auto old = ptxBuilderAtomicRMW.launch(rewriter, loc, valueElemTy);\n+        if (op->user_begin() == op->user_end()) {\n+          rewriter.replaceOp(op, {old});\n+          return success();\n+        }\n         Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());\n         atomPtr = bitcast(atomPtr, ptr_ty(valueElemTy, 3));\n-        store(old, atomPtr);\n+        // Only threads with rmwMask = True store the result\n+        PTXBuilder ptxBuilderStore;\n+        auto &storeShared =\n+            ptxBuilderStore.create<>(\"st\")->shared().o(\"b\" + sBits);\n+        auto *ptrOpr = ptxBuilderStore.newAddrOperand(atomPtr, \"r\");\n+        auto *valOpr = ptxBuilderStore.newOperand(old, tyId);\n+        storeShared(ptrOpr, valOpr).predicate(rmwMask);\n+        ptxBuilderStore.launch(rewriter, loc, void_ty(ctx));\n         barrier();\n         Value ret = load(atomPtr);\n         barrier();\n         rewriter.replaceOp(op, {ret});\n       }\n     }\n-    if (valueTy) {\n-      Type structTy = getTypeConverter()->convertType(valueTy);\n-      Value resultStruct =\n-          getStructFromElements(loc, resultVals, rewriter, structTy);\n+    if (tensorTy) {\n+      Type structTy = getTypeConverter()->convertType(tensorTy);\n+      Value resultStruct = getTypeConverter()->packLLElements(\n+          loc, resultVals, rewriter, structTy);\n       rewriter.replaceOp(op, {resultStruct});\n     }\n     return success();\n@@ -594,10 +626,12 @@ struct InsertSliceOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     // %dst = insert_slice %src into %dst[%offsets]\n     Location loc = op->getLoc();\n-    Value dst = op.dest();\n-    Value src = op.source();\n-    Value res = op.result();\n-    assert(allocation->getBufferId(res) == Allocation::InvalidBufferId &&\n+    Value dst = op.getDest();\n+    Value src = op.getSource();\n+    Value res = op.getResult();\n+    auto funcOp = op->getParentOfType<FunctionOpInterface>();\n+    auto *funcAllocation = allocation->getFuncData(funcOp);\n+    assert(funcAllocation->getBufferId(res) == Allocation::InvalidBufferId &&\n            \"Only support in-place insert_slice for now\");\n \n     auto srcTy = src.getType().dyn_cast<RankedTensorType>();\n@@ -607,7 +641,7 @@ struct InsertSliceOpConversion\n \n     auto dstTy = dst.getType().dyn_cast<RankedTensorType>();\n     auto dstLayout = dstTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n-    auto llDst = adaptor.dest();\n+    auto llDst = adaptor.getDest();\n     assert(dstLayout && \"Unexpected dstLayout in InsertSliceOpConversion\");\n     assert(op.hasUnitStride() &&\n            \"Only unit stride supported by InsertSliceOpConversion\");\n@@ -620,7 +654,7 @@ struct InsertSliceOpConversion\n     auto mixedOffsets = op.getMixedOffsets();\n     for (auto i = 0; i < mixedOffsets.size(); ++i) {\n       if (op.isDynamicOffset(i)) {\n-        offsets.emplace_back(adaptor.offsets()[i]);\n+        offsets.emplace_back(adaptor.getOffsets()[i]);\n       } else {\n         offsets.emplace_back(i32_val(op.getStaticOffset(i)));\n       }\n@@ -638,8 +672,8 @@ struct InsertSliceOpConversion\n     auto elemPtrTy = ptr_ty(elemTy, 3);\n     auto smemBase = gep(elemPtrTy, smemObj.base, offset);\n \n-    auto llSrc = adaptor.source();\n-    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n+    auto llSrc = adaptor.getSource();\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy);\n     storeDistributedToShared(src, llSrc, srcStrides, srcIndices, dst, smemBase,\n                              elemTy, loc, rewriter);\n     // Barrier is not necessary.\n@@ -657,24 +691,26 @@ struct InsertSliceAsyncOpConversion\n       triton::gpu::InsertSliceAsyncOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   InsertSliceAsyncOpConversion(\n-      LLVMTypeConverter &converter, const Allocation *allocation, Value smem,\n+      TritonGPUToLLVMTypeConverter &converter, ModuleAllocation &allocation,\n       ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n-      AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n+      ModuleAxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::gpu::InsertSliceAsyncOp>(\n-            converter, allocation, smem, indexCacheInfo, benefit),\n+            converter, allocation, indexCacheInfo, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n \n   LogicalResult\n   matchAndRewrite(triton::gpu::InsertSliceAsyncOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     // insert_slice_async %src, %dst, %index, %mask, %other\n     auto loc = op.getLoc();\n-    Value src = op.src();\n-    Value dst = op.dst();\n-    Value res = op.result();\n-    Value mask = op.mask();\n-    Value other = op.other();\n-    assert(allocation->getBufferId(res) == Allocation::InvalidBufferId &&\n+    Value src = op.getSrc();\n+    Value dst = op.getDst();\n+    Value res = op.getResult();\n+    Value mask = op.getMask();\n+    Value other = op.getOther();\n+    auto funcOp = op->getParentOfType<FunctionOpInterface>();\n+    auto *funcAllocation = allocation->getFuncData(funcOp);\n+    assert(funcAllocation->getBufferId(res) == Allocation::InvalidBufferId &&\n            \"Only support in-place insert_slice_async for now\");\n \n     auto srcTy = src.getType().cast<RankedTensorType>();\n@@ -686,14 +722,15 @@ struct InsertSliceAsyncOpConversion\n     assert(srcShape.size() == 2 &&\n            \"insert_slice_async: Unexpected rank of %src\");\n \n-    Value llDst = adaptor.dst();\n-    Value llSrc = adaptor.src();\n-    Value llMask = adaptor.mask();\n-    Value llOther = adaptor.other();\n-    Value llIndex = adaptor.index();\n+    Value llDst = adaptor.getDst();\n+    Value llSrc = adaptor.getSrc();\n+    Value llMask = adaptor.getMask();\n+    Value llOther = adaptor.getOther();\n+    Value llIndex = adaptor.getIndex();\n \n     // %src\n-    auto srcElems = getLLVMElems(src, llSrc, rewriter, loc);\n+    auto srcElems = getTypeConverter()->unpackLLElements(loc, llSrc, rewriter,\n+                                                         src.getType());\n \n     // %dst\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n@@ -719,7 +756,8 @@ struct InsertSliceAsyncOpConversion\n     // %mask\n     SmallVector<Value> maskElems;\n     if (llMask) {\n-      maskElems = getLLVMElems(mask, llMask, rewriter, loc);\n+      maskElems = getTypeConverter()->unpackLLElements(loc, llMask, rewriter,\n+                                                       mask.getType());\n       assert(srcElems.size() == maskElems.size());\n     }\n \n@@ -730,14 +768,18 @@ struct InsertSliceAsyncOpConversion\n       // It's not necessary for now because the pipeline pass will skip\n       // generating insert_slice_async if the load op has any \"other\" tensor.\n       // assert(false && \"insert_slice_async: Other value not supported yet\");\n-      otherElems = getLLVMElems(other, llOther, rewriter, loc);\n+      otherElems = getTypeConverter()->unpackLLElements(loc, llOther, rewriter,\n+                                                        other.getType());\n       assert(srcElems.size() == otherElems.size());\n     }\n \n-    unsigned inVec = getVectorSize(src);\n+    // We don't use getVec() here because we are copying from memory to memory.\n+    // If contiguity > vector size, we can have one pointer maintaining the\n+    // start of the vector and the other pointer moving to the next vector.\n+    unsigned inVec = getContiguity(src);\n     unsigned outVec = resSharedLayout.getVec();\n     unsigned minVec = std::min(outVec, inVec);\n-    unsigned numElems = getElemsPerThread(srcTy);\n+    unsigned numElems = getTotalElemsPerThread(srcTy);\n     unsigned perPhase = resSharedLayout.getPerPhase();\n     unsigned maxPhase = resSharedLayout.getMaxPhase();\n     auto sizePerThread = srcBlockedLayout.getSizePerThread();\n@@ -757,10 +799,9 @@ struct InsertSliceAsyncOpConversion\n     // single vector read into multiple ones\n     auto numVecCols = std::max<unsigned>(inVec / outVec, 1);\n \n-    auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcShape);\n+    auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcTy);\n \n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n-\n       // 16 * 8 = 128bits\n       auto maxBitWidth =\n           std::max<unsigned>(128, resElemTy.getIntOrFloatBitWidth());\n@@ -788,7 +829,7 @@ struct InsertSliceAsyncOpConversion\n             ptxBuilder.newAddrOperand(srcElems[elemIdx + wordElemIdx], \"l\");\n         auto *copySize = ptxBuilder.newConstantOperand(byteWidth);\n         auto *srcSize = copySize;\n-        if (op.mask()) {\n+        if (op.getMask()) {\n           // We don't use predicate in this case, setting src-size to 0\n           // if there's any mask. cp.async will automatically fill the\n           // remaining slots with 0 if cp-size > src-size.\n@@ -808,20 +849,18 @@ struct InsertSliceAsyncOpConversion\n };\n \n void populateLoadStoreOpToLLVMPatterns(\n-    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem,\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    ModuleAxisInfoAnalysis &axisInfoAnalysis, ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit) {\n   patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<StoreOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n-  patterns.add<AtomicCASOpConversion>(typeConverter, allocation, smem,\n+  patterns.add<AtomicCASOpConversion>(typeConverter, allocation,\n                                       axisInfoAnalysis, benefit);\n-  patterns.add<AtomicRMWOpConversion>(typeConverter, allocation, smem,\n+  patterns.add<AtomicRMWOpConversion>(typeConverter, allocation,\n                                       axisInfoAnalysis, benefit);\n-  patterns.add<InsertSliceOpConversion>(typeConverter, allocation, smem,\n+  patterns.add<InsertSliceOpConversion>(typeConverter, allocation,\n                                         indexCacheInfo, benefit);\n-  patterns.add<InsertSliceAsyncOpConversion>(typeConverter, allocation, smem,\n-                                             indexCacheInfo, axisInfoAnalysis,\n-                                             benefit);\n+  patterns.add<InsertSliceAsyncOpConversion>(\n+      typeConverter, allocation, indexCacheInfo, axisInfoAnalysis, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.h", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -7,9 +7,8 @@ using namespace mlir;\n using namespace mlir::triton;\n \n void populateLoadStoreOpToLLVMPatterns(\n-    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem,\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    ModuleAxisInfoAnalysis &axisInfoAnalysis, ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/PTXAsmFormat.cpp", "status": "modified", "additions": 25, "deletions": 16, "changes": 41, "file_content_changes": "@@ -1,27 +1,14 @@\n #include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n-\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Transforms/DialectConversion.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/AsmFormat.h\"\n #include \"llvm/Support/raw_ostream.h\"\n // TODO(Superjomn): unify to llvm::raw_string_ostream\n #include <sstream>\n \n namespace mlir {\n namespace triton {\n \n-// TODO(Superjomn) Move to a global utility file?\n-std::string strJoin(llvm::ArrayRef<std::string> strs,\n-                    llvm::StringRef delimiter) {\n-  std::string osStr;\n-  llvm::raw_string_ostream os(osStr);\n-  for (size_t i = 0; !strs.empty() && i < strs.size() - 1; ++i)\n-    os << strs[i] << delimiter;\n-  if (!strs.empty())\n-    os << strs.back();\n-  os.flush();\n-  return osStr;\n-}\n-\n PTXInstr::Operand *\n PTXBuilder::newOperand(mlir::Value value, StringRef constraint,\n                        std::function<std::string(int)> formatter) {\n@@ -32,12 +19,34 @@ PTXBuilder::newOperand(mlir::Value value, StringRef constraint,\n   return opr;\n }\n \n-PTXBuilder::Operand *PTXBuilder::newOperand(StringRef constraint) {\n+void PTXBuilder::initOperand(Operand *opr) {\n+  auto numBits = 0;\n+  // Derive numBits from the constraint.\n+  if (opr->constraint[1] == 'c' || opr->constraint[1] == 'h')\n+    numBits = 16;\n+  else if (opr->constraint[1] == 'r')\n+    numBits = 32;\n+  else if (opr->constraint[1] == 'l')\n+    numBits = 64;\n+  else\n+    llvm_unreachable((\"Unknown constraint: \" + opr->constraint).c_str());\n+  // If numBits is less than 16, we use 16 as default because PTX does not\n+  // support 8-bit mov.\n+  numBits = numBits < 16 ? 16 : numBits;\n+  auto *zero = newConstantOperand(0);\n+  auto &init = create<>(\"mov\")->o(\"u\" + std::to_string(numBits));\n+  init(opr, zero);\n+}\n+\n+PTXBuilder::Operand *PTXBuilder::newOperand(StringRef constraint, bool init) {\n   // Constraint should be something like \"=r\"\n-  assert(!constraint.empty() && constraint[0] == '=');\n+  assert(constraint.size() == 2 && constraint[0] == '=');\n   auto *opr = newOperand();\n   opr->idx = oprCounter++;\n   opr->constraint = constraint;\n+  if (init) {\n+    initOperand(opr);\n+  }\n   return opr;\n }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 301, "deletions": 288, "changes": 589, "file_content_changes": "@@ -3,12 +3,10 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-using ::mlir::LLVM::getElementsFromStruct;\n-using ::mlir::LLVM::getStructFromElements;\n using ::mlir::LLVM::shflSync;\n using ::mlir::LLVM::storeShared;\n-using ::mlir::triton::gpu::getElemsPerThread;\n using ::mlir::triton::gpu::getOrder;\n+using ::mlir::triton::gpu::getTotalElemsPerThread;\n \n struct ReduceOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::ReduceOp> {\n@@ -25,163 +23,174 @@ struct ReduceOpConversion\n   }\n \n private:\n-  void accumulate(ConversionPatternRewriter &rewriter, Location loc,\n-                  RedOp redOp, Value &acc, Value cur, bool isFirst) const {\n+  void accumulate(ConversionPatternRewriter &rewriter, Region &combineOp,\n+                  llvm::SmallVectorImpl<Value> &acc, ValueRange cur,\n+                  bool isFirst) const {\n     if (isFirst) {\n-      acc = cur;\n+      acc.resize(cur.size());\n+      for (unsigned i = 0; i < cur.size(); ++i) {\n+        acc[i] = cur[i];\n+      }\n       return;\n     }\n-    switch (redOp) {\n-    case RedOp::ADD:\n-      acc = add(acc, cur);\n-      break;\n-    case RedOp::FADD:\n-      acc = fadd(acc.getType(), acc, cur);\n-      break;\n-    case RedOp::MIN:\n-      acc = smin(acc, cur);\n-      break;\n-    case RedOp::MAX:\n-      acc = smax(acc, cur);\n-      break;\n-    case RedOp::UMIN:\n-      acc = umin(acc, cur);\n-      break;\n-    case RedOp::UMAX:\n-      acc = umax(acc, cur);\n-      break;\n-    case RedOp::FMIN:\n-      acc = fmin(acc, cur);\n-      break;\n-    case RedOp::FMAX:\n-      acc = fmax(acc, cur);\n-      break;\n-    case RedOp::XOR:\n-      acc = xor_(acc, cur);\n-      break;\n-    case RedOp::ARGMIN:\n-    case RedOp::ARGMAX:\n-    case RedOp::ARGUMIN:\n-    case RedOp::ARGUMAX:\n-    case RedOp::ARGFMIN:\n-    case RedOp::ARGFMAX:\n-      llvm::report_fatal_error(\n-          \"This accumulate implementation is not for argmin / argmax\");\n-    default:\n-      llvm::report_fatal_error(\"Unsupported reduce op\");\n+\n+    // Create a new copy of the reduce block, and inline it\n+    Block *currentBlock = rewriter.getBlock();\n+    Region &parent = *currentBlock->getParent();\n+    rewriter.cloneRegionBefore(combineOp, &parent.front());\n+    auto &newReduce = parent.front();\n+    auto returnOp = dyn_cast<triton::ReduceReturnOp>(newReduce.getTerminator());\n+\n+    llvm::SmallVector<Value> combineArgs(2 * acc.size());\n+    for (unsigned i = 0; i < acc.size(); ++i) {\n+      combineArgs[i] = acc[i];\n+      combineArgs[acc.size() + i] = cur[i];\n+    }\n+\n+    rewriter.inlineBlockBefore(&newReduce, &*rewriter.getInsertionPoint(),\n+                               combineArgs);\n+\n+    auto results = returnOp.getResult();\n+    for (unsigned i = 0; i < acc.size(); ++i) {\n+      acc[i] = results[i];\n     }\n+\n+    // Delete the terminator, which is no longer used\n+    rewriter.eraseOp(returnOp);\n   }\n \n-  void accumulateWithIndex(ConversionPatternRewriter &rewriter, Location loc,\n-                           RedOp redOp, Value &acc, Value &accIndex, Value cur,\n-                           Value curIndex, bool isFirst) const {\n-    if (isFirst) {\n-      acc = cur;\n-      accIndex = curIndex;\n+  SmallVector<SmallVector<Value>>\n+  unpackInputs(Location loc, triton::ReduceOp op, OpAdaptor adaptor,\n+               ConversionPatternRewriter &rewriter) const {\n+    auto types = op.getInputTypes();\n+    auto operands = adaptor.getOperands();\n+    unsigned srcElems = getTotalElemsPerThread(types[0]);\n+    SmallVector<SmallVector<Value>> srcValues(srcElems);\n+    for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+      auto values = getTypeConverter()->unpackLLElements(loc, operands[i],\n+                                                         rewriter, types[i]);\n+\n+      assert(values.size() == srcValues.size());\n+      for (unsigned j = 0; j < srcValues.size(); ++j) {\n+        srcValues[j].push_back(values[j]);\n+      }\n+    }\n+    return srcValues;\n+  }\n+\n+  // Calculates the write index in the shared memory where we would be writing\n+  // the within-thread accumulations before we start doing across-threads\n+  // accumulations. `index` is the index of the within-thread accumulations in\n+  // the full tensor, whereas `writeIdx` is the mapped-to index in the shared\n+  // memory\n+  void getWriteIndexBasic(ConversionPatternRewriter &rewriter, Location loc,\n+                          Attribute layout, SmallVector<Value> &index,\n+                          SmallVector<Value> &writeIdx,\n+                          std::map<int, Value> &ints, unsigned axis) const {\n+    if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+      auto dim = sliceLayout.getDim();\n+      assert(dim != axis && \"Reduction axis cannot be sliced\");\n+      auto parentLayout = sliceLayout.getParent();\n+      getWriteIndexBasic(rewriter, loc, parentLayout, index, writeIdx, ints,\n+                         axis);\n       return;\n     }\n-    switch (redOp) {\n-    case RedOp::ARGMIN:\n-      accIndex = select(\n-          icmp_slt(acc, cur), accIndex,\n-          select(icmp_sgt(acc, cur), curIndex, smin(accIndex, curIndex)));\n-      acc = smin(acc, cur);\n-      break;\n-    case RedOp::ARGMAX:\n-      accIndex = select(\n-          icmp_sgt(acc, cur), accIndex,\n-          select(icmp_slt(acc, cur), curIndex, smin(accIndex, curIndex)));\n-      acc = smax(acc, cur);\n-      break;\n-    case RedOp::ARGUMIN:\n-      accIndex = select(\n-          icmp_ult(acc, cur), accIndex,\n-          select(icmp_ugt(acc, cur), curIndex, smin(accIndex, curIndex)));\n-      acc = umin(acc, cur);\n-      break;\n-    case RedOp::ARGUMAX:\n-      accIndex = select(\n-          icmp_ugt(acc, cur), accIndex,\n-          select(icmp_ult(acc, cur), curIndex, smin(accIndex, curIndex)));\n-      acc = umax(acc, cur);\n-      break;\n-    case RedOp::ARGFMIN:\n-      accIndex = select(\n-          fcmp_olt(acc, cur), accIndex,\n-          select(fcmp_ogt(acc, cur), curIndex, smin(accIndex, curIndex)));\n-      acc = fmin(acc, cur);\n-      break;\n-    case RedOp::ARGFMAX:\n-      accIndex = select(\n-          fcmp_ogt(acc, cur), accIndex,\n-          select(fcmp_olt(acc, cur), curIndex, smin(accIndex, curIndex)));\n-      acc = fmax(acc, cur);\n-      break;\n-    case RedOp::ADD:\n-    case RedOp::FADD:\n-    case RedOp::MIN:\n-    case RedOp::MAX:\n-    case RedOp::UMIN:\n-    case RedOp::UMAX:\n-    case RedOp::FMIN:\n-    case RedOp::FMAX:\n-    case RedOp::XOR:\n-      llvm::report_fatal_error(\n-          \"This accumulate implementation is only for argmin / argmax\");\n-    default:\n-      llvm::report_fatal_error(\"Unsupported reduce op\");\n+\n+    writeIdx = index;\n+    auto sizePerThread = triton::gpu::getSizePerThread(layout);\n+    Value axisSizePerThread = ints[sizePerThread[axis]];\n+    Value _8 = ints[8];\n+    Value _16 = ints[16];\n+    if (layout.isa<BlockedEncodingAttr>()) {\n+      // A single thread owns axisSizePerThread contiguous values\n+      // on the reduction axis. After within thread reduction,\n+      // we would have a single accumulation every `axisSizePerThread`\n+      // contiguous values in the original tensor, so we would need\n+      // to map every `axisSizePerThread` to 1 value in smem as:\n+      // writeIdx[axis] = index[axis] / axisSizePerThread\n+      writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n+    } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+      if (!mmaLayout.isAmpere()) {\n+        llvm::report_fatal_error(\"Unsupported layout\");\n+      }\n+      if (axis == 0) {\n+        // Because warpTileSize = [16, 8] and threadsPerWarp = [8, 4], each 8\n+        // rows in smem would correspond to a warp. The mapping\n+        // is: (warp_index) x 8 + (row index within warp)\n+        writeIdx[axis] =\n+            add(mul(udiv(index[axis], _16), _8), urem(index[axis], _8));\n+      } else {\n+        // Same as BlockedEncodingAttr case\n+        writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n+      }\n+    } else {\n+      llvm::report_fatal_error(\"Unsupported layout\");\n     }\n   }\n \n   // Use shared memory for reduction within warps and across warps\n   LogicalResult\n   matchAndRewriteBasic(triton::ReduceOp op, OpAdaptor adaptor,\n                        ConversionPatternRewriter &rewriter) const {\n-    Location loc = op->getLoc();\n-    unsigned axis = op.axis();\n-    bool withIndex = triton::ReduceOp::withIndex(op.redOp());\n-\n-    auto srcTy = op.operand().getType().cast<RankedTensorType>();\n-    auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n-    auto srcOrd = srcLayout.getOrder();\n-    auto srcShape = srcTy.getShape();\n+    ReduceOpHelper helper(op);\n+    Location loc = op.getLoc();\n+    unsigned axis = op.getAxis();\n \n-    auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n+    auto srcTys = op.getInputTypes();\n+    auto srcLayout = helper.getSrcLayout();\n+    if (!helper.isSupportedLayout()) {\n+      assert(false && \"Unexpected srcLayout in ReduceOpConversion\");\n+    }\n+    // The order of the axes for the the threads within the warp\n+    auto srcOrd = triton::gpu::getOrder(srcLayout);\n+    auto sizePerThread = triton::gpu::getSizePerThread(srcLayout);\n+    auto srcShape = helper.getSrcShape();\n+\n+    SmallVector<Type> elemPtrTys(srcTys.size());\n+    for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+      auto ty = srcTys[i].getElementType();\n+      auto llvmElemTy = getTypeConverter()->convertType(ty);\n+      elemPtrTys[i] = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+    }\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n-    auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n     auto indexPtrTy = LLVM::LLVMPointerType::get(llvmIndexTy, 3);\n-    Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n-    smemBase = bitcast(smemBase, elemPtrTy);\n \n-    ReduceOpHelper helper(op);\n     auto smemShape = helper.getScratchConfigBasic();\n     unsigned elems = product<unsigned>(smemShape);\n-    Value indexSmemBase = gep(elemPtrTy, smemBase, i32_val(elems));\n-    indexSmemBase = bitcast(indexSmemBase, indexPtrTy);\n \n-    unsigned srcElems = getElemsPerThread(srcTy);\n-    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n-    auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n+    SmallVector<Value> smemBases(op.getNumOperands());\n+    smemBases[0] = bitcast(\n+        getSharedMemoryBase(loc, rewriter, op.getOperation()), elemPtrTys[0]);\n+    for (unsigned i = 1; i < op.getNumOperands(); ++i) {\n+      smemBases[i] =\n+          bitcast(gep(elemPtrTys[i - 1], smemBases[i - 1], i32_val(elems)),\n+                  elemPtrTys[i]);\n+    }\n+\n+    unsigned srcElems = getTotalElemsPerThread(srcTys[0]);\n+    // Emits indices of the original tensor that each thread\n+    // would own\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTys[0]);\n+    auto srcValues = unpackInputs(loc, op, adaptor, rewriter);\n \n+    // Emits offsets (the offset from the base index)\n+    // of the original tensor that each thread would own\n+    // NOTE: Assumes offsets don't actually depend on type\n     SmallVector<SmallVector<unsigned>> offset =\n-        emitOffsetForLayout(srcLayout, srcShape);\n+        emitOffsetForLayout(srcLayout, srcTys[0]);\n \n-    std::map<SmallVector<unsigned>, Value> accs;\n-    std::map<SmallVector<unsigned>, Value> accIndices;\n+    // Keep track of accumulations and their indices\n+    std::map<SmallVector<unsigned>, SmallVector<Value>> accs;\n     std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n \n+    Region *combineOp = &op.getCombineOp();\n+\n     // reduce within threads\n     for (unsigned i = 0; i < srcElems; ++i) {\n       SmallVector<unsigned> key = offset[i];\n       key[axis] = 0;\n       bool isFirst = accs.find(key) == accs.end();\n-      if (!withIndex) {\n-        accumulate(rewriter, loc, op.redOp(), accs[key], srcValues[i], isFirst);\n-      } else {\n-        Value curIndex = srcIndices[i][axis];\n-        accumulateWithIndex(rewriter, loc, op.redOp(), accs[key],\n-                            accIndices[key], srcValues[i], curIndex, isFirst);\n-      }\n+      accumulate(rewriter, *combineOp, accs[key], srcValues[i], isFirst);\n       if (isFirst)\n         indices[key] = srcIndices[i];\n     }\n@@ -191,147 +200,164 @@ struct ReduceOpConversion\n     ints[0] = i32_val(0);\n     for (int N = smemShape[axis] / 2; N > 0; N >>= 1)\n       ints[N] = i32_val(N);\n-    Value sizePerThread = i32_val(srcLayout.getSizePerThread()[axis]);\n+    ints[sizePerThread[axis]] = i32_val(sizePerThread[axis]);\n+    ints[8] = i32_val(8);\n+    ints[16] = i32_val(16);\n \n     // reduce across threads\n     for (auto it : accs) {\n       const SmallVector<unsigned> &key = it.first;\n-      Value acc = it.second;\n-      Value accIndex;\n-      if (withIndex)\n-        accIndex = accIndices[key];\n-      SmallVector<Value> writeIdx = indices[key];\n+      auto &acc = it.second;\n+      // get the writeIdx at which to write in smem\n+      SmallVector<Value> writeIdx;\n+      getWriteIndexBasic(rewriter, loc, srcLayout, indices[key], writeIdx, ints,\n+                         axis);\n \n-      writeIdx[axis] = udiv(writeIdx[axis], sizePerThread);\n+      // calculate the offset in smem for that writeIdx\n       Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, srcOrd);\n-      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n-      Value indexWritePtr = gep(indexPtrTy, indexSmemBase, writeOffset);\n-      store(acc, writePtr);\n-      if (withIndex)\n-        store(accIndex, indexWritePtr);\n+      SmallVector<Value> writePtrs(op.getNumOperands());\n+      for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+        // Store the within-thread accumulated value into shared memory\n+        writePtrs[i] = gep(elemPtrTys[i], smemBases[i], writeOffset);\n+        store(acc[i], writePtrs[i]);\n+      }\n \n       SmallVector<Value> readIdx(writeIdx.size(), ints[0]);\n+      // Perform parallel reduction with sequential addressing\n+      // E.g. We reduce `smemShape[axis]` elements into `smemShape[axis]/2`\n+      // elements using `smemShape[axis]/2` threads where each thread\n+      // would accumalte values that are `smemShape[axis]/2` apart\n+      // to avoid bank conflicts. Then we repeat with `smemShape[axis]/4`\n+      // threads, .. etc.\n       for (int N = smemShape[axis] / 2; N > 0; N >>= 1) {\n+        // The readIdx will be N elements away on the reduction axis\n         readIdx[axis] = ints[N];\n+        // If the writeIdx is greater or equal to N, do nothing\n         Value readMask = icmp_slt(writeIdx[axis], ints[N]);\n+        // Calculate the readOffset, if readMask is False, readOffset=0\n+        // meaning we reduce the value at writeIdx with itself\n         Value readOffset = select(\n             readMask, linearize(rewriter, loc, readIdx, smemShape, srcOrd),\n             ints[0]);\n-        Value readPtr = gep(elemPtrTy, writePtr, readOffset);\n+        SmallVector<Value> readPtrs(op.getNumOperands());\n+        for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+          // The readPtr is readOffset away from writePtr\n+          readPtrs[i] = gep(elemPtrTys[i], writePtrs[i], readOffset);\n+        }\n+\n+        barrier();\n+        // Combine accumulator value from another thread\n+        SmallVector<Value> cur(op.getNumOperands());\n+        for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+          cur[i] = load(readPtrs[i]);\n+        }\n+        accumulate(rewriter, *combineOp, acc, cur, false);\n+\n         barrier();\n-        if (!withIndex) {\n-          Value cur = load(readPtr);\n-          accumulate(rewriter, loc, op.redOp(), acc, cur, false);\n-          barrier();\n-          store(acc, writePtr);\n-        } else {\n-          Value cur = load(readPtr);\n-          Value indexReadPtr = gep(indexPtrTy, indexWritePtr, readOffset);\n-          Value curIndex = load(indexReadPtr);\n-          accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, cur,\n-                              curIndex, false);\n-          barrier();\n-          store(acc, writePtr);\n-          store(accIndex, indexWritePtr);\n+        // Publish our new accumulator value to shared memory\n+        for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+          store(acc[i], writePtrs[i]);\n         }\n       }\n     }\n \n     barrier();\n \n     // set output values\n-    if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n-      // nd-tensor where n >= 1\n-      auto resultLayout = resultTy.getEncoding();\n-      auto resultShape = resultTy.getShape();\n-\n-      unsigned resultElems = getElemsPerThread(resultTy);\n-      auto resultIndices =\n-          emitIndices(loc, rewriter, resultLayout, resultShape);\n-      assert(resultIndices.size() == resultElems);\n-\n-      SmallVector<Value> resultVals(resultElems);\n-      for (unsigned i = 0; i < resultElems; ++i) {\n-        SmallVector<Value> readIdx = resultIndices[i];\n-        readIdx.insert(readIdx.begin() + axis, ints[0]);\n-        Value readOffset = linearize(rewriter, loc, readIdx, smemShape, srcOrd);\n-        Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n-        Value indexReadPtr = gep(indexPtrTy, indexSmemBase, readOffset);\n-        resultVals[i] = withIndex ? load(indexReadPtr) : load(readPtr);\n+    SmallVector<Value> results(op.getNumOperands());\n+    for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+      if (auto resultTy =\n+              op.getResult()[i].getType().dyn_cast<RankedTensorType>()) {\n+        // nd-tensor where n >= 1\n+\n+        auto resultLayout = resultTy.getEncoding();\n+\n+        unsigned resultElems = getTotalElemsPerThread(resultTy);\n+        auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultTy);\n+        assert(resultIndices.size() == resultElems);\n+\n+        SmallVector<Value> resultVals(resultElems);\n+        for (unsigned j = 0; j < resultElems; ++j) {\n+          SmallVector<Value> readIdx = resultIndices[j];\n+          readIdx.insert(readIdx.begin() + axis, ints[0]);\n+          Value readOffset =\n+              linearize(rewriter, loc, readIdx, smemShape, srcOrd);\n+          Value readPtr = gep(elemPtrTys[i], smemBases[i], readOffset);\n+          resultVals[j] = load(readPtr);\n+        }\n+        results[i] = getTypeConverter()->packLLElements(loc, resultVals,\n+                                                        rewriter, resultTy);\n+      } else {\n+        // 0d-tensor -> scalar\n+        results[i] = load(smemBases[i]);\n       }\n-\n-      SmallVector<Type> resultTypes(resultElems,\n-                                    withIndex ? llvmIndexTy : llvmElemTy);\n-      Type structTy =\n-          LLVM::LLVMStructType::getLiteral(this->getContext(), resultTypes);\n-      Value ret = getStructFromElements(loc, resultVals, rewriter, structTy);\n-      rewriter.replaceOp(op, ret);\n-    } else {\n-      // 0d-tensor -> scalar\n-      Value resultVal = withIndex ? load(indexSmemBase) : load(smemBase);\n-      rewriter.replaceOp(op, resultVal);\n     }\n \n+    auto parentBlock = op.getOperation()->getBlock();\n+    rewriter.replaceOp(op, results);\n     return success();\n   }\n \n   // Use warp shuffle for reduction within warps and shared memory for data\n   // exchange across warps\n   LogicalResult matchAndRewriteFast(triton::ReduceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter) const {\n+    ReduceOpHelper helper(op);\n     Location loc = op->getLoc();\n-    unsigned axis = adaptor.axis();\n-    bool withIndex = triton::ReduceOp::withIndex(op.redOp());\n-\n-    auto srcTy = op.operand().getType().cast<RankedTensorType>();\n-    auto srcLayout = srcTy.getEncoding();\n-    auto srcShape = srcTy.getShape();\n-    auto srcRank = srcTy.getRank();\n-    auto order = getOrder(srcLayout);\n-\n-    auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcLayout);\n-    auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcLayout);\n+    unsigned axis = adaptor.getAxis();\n \n-    auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n+    auto srcTys = op.getInputTypes();\n+    auto srcLayout = helper.getSrcLayout();\n+    if (!helper.isSupportedLayout()) {\n+      assert(false && \"Unexpected srcLayout in ReduceOpConversion\");\n+    }\n+    auto srcOrd = triton::gpu::getOrder(srcLayout);\n+    auto srcShape = helper.getSrcShape();\n+\n+    SmallVector<Type> elemPtrTys(srcTys.size());\n+    for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+      auto ty = srcTys[i].getElementType();\n+      auto llvmElemTy = getTypeConverter()->convertType(ty);\n+      elemPtrTys[i] = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+    }\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n-    auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n     auto indexPtrTy = LLVM::LLVMPointerType::get(llvmIndexTy, 3);\n-    Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n-    smemBase = bitcast(smemBase, elemPtrTy);\n \n-    ReduceOpHelper helper(op);\n     auto smemShapes = helper.getScratchConfigsFast();\n     unsigned elems = product<unsigned>(smemShapes[0]);\n     unsigned maxElems = std::max(elems, product<unsigned>(smemShapes[1]));\n-    Value indexSmemBase = gep(elemPtrTy, smemBase, i32_val(maxElems));\n-    indexSmemBase = bitcast(indexSmemBase, indexPtrTy);\n \n-    unsigned sizeIntraWarps = helper.getIntraWarpSize();\n-    unsigned sizeInterWarps = helper.getInterWarpSize();\n+    SmallVector<Value> smemBases(op.getNumOperands());\n+    smemBases[0] = bitcast(\n+        getSharedMemoryBase(loc, rewriter, op.getOperation()), elemPtrTys[0]);\n+    for (unsigned i = 1; i < op.getNumOperands(); ++i) {\n+      smemBases[i] =\n+          bitcast(gep(elemPtrTys[i - 1], smemBases[i - 1], i32_val(maxElems)),\n+                  elemPtrTys[i]);\n+    }\n+\n+    unsigned sizeIntraWarps = helper.getIntraWarpSizeWithUniqueData();\n+    unsigned sizeInterWarps = helper.getInterWarpSizeWithUniqueData();\n+\n+    unsigned srcElems = getTotalElemsPerThread(srcTys[0]);\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTys[0]);\n+    auto srcValues = unpackInputs(loc, op, adaptor, rewriter);\n \n-    unsigned srcElems = getElemsPerThread(srcTy);\n-    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n-    auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n+    std::map<SmallVector<unsigned>, SmallVector<Value>> accs;\n+    std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n \n+    // Assumes offsets don't actually depend on type\n     SmallVector<SmallVector<unsigned>> offset =\n-        emitOffsetForLayout(srcLayout, srcShape);\n+        emitOffsetForLayout(srcLayout, srcTys[0]);\n \n-    std::map<SmallVector<unsigned>, Value> accs;\n-    std::map<SmallVector<unsigned>, Value> accIndices;\n-    std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n+    auto *combineOp = &op.getCombineOp();\n \n     // reduce within threads\n     for (unsigned i = 0; i < srcElems; ++i) {\n       SmallVector<unsigned> key = offset[i];\n       key[axis] = 0;\n       bool isFirst = accs.find(key) == accs.end();\n-      if (!withIndex) {\n-        accumulate(rewriter, loc, op.redOp(), accs[key], srcValues[i], isFirst);\n-      } else {\n-        Value curIndex = srcIndices[i][axis];\n-        accumulateWithIndex(rewriter, loc, op.redOp(), accs[key],\n-                            accIndices[key], srcValues[i], curIndex, isFirst);\n-      }\n+      accumulate(rewriter, *combineOp, accs[key], srcValues[i], isFirst);\n       if (isFirst)\n         indices[key] = srcIndices[i];\n     }\n@@ -341,6 +367,9 @@ struct ReduceOpConversion\n     Value warpId = udiv(threadId, warpSize);\n     Value laneId = urem(threadId, warpSize);\n \n+    auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcLayout);\n+    auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcLayout);\n+    auto order = getOrder(srcLayout);\n     SmallVector<Value> multiDimLaneId =\n         delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n     SmallVector<Value> multiDimWarpId =\n@@ -351,36 +380,27 @@ struct ReduceOpConversion\n \n     Value zero = i32_val(0);\n     Value laneZero = icmp_eq(laneIdAxis, zero);\n-    Value warpZero = icmp_eq(warpIdAxis, zero);\n \n     for (auto it : accs) {\n       const SmallVector<unsigned> &key = it.first;\n-      Value acc = it.second;\n-      Value accIndex;\n-      if (withIndex)\n-        accIndex = accIndices[key];\n+      SmallVector<Value> acc = it.second;\n \n       // Reduce within warps\n       for (unsigned N = sizeIntraWarps / 2; N > 0; N >>= 1) {\n-        Value shfl = shflSync(loc, rewriter, acc, N);\n-        if (!withIndex) {\n-          accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n-        } else {\n-          Value shflIndex = shflSync(loc, rewriter, accIndex, N);\n-          accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, shfl,\n-                              shflIndex, false);\n+        SmallVector<Value> shfl(op.getNumOperands());\n+        for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+          shfl[i] = shflSync(loc, rewriter, acc[i], N);\n         }\n+        accumulate(rewriter, *combineOp, acc, shfl, false);\n       }\n \n       SmallVector<Value> writeIdx = indices[key];\n       writeIdx[axis] = (sizeInterWarps == 1) ? zero : warpIdAxis;\n       Value writeOffset =\n           linearize(rewriter, loc, writeIdx, smemShapes[0], order);\n-      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n-      storeShared(rewriter, loc, writePtr, acc, laneZero);\n-      if (withIndex) {\n-        Value indexWritePtr = gep(indexPtrTy, indexSmemBase, writeOffset);\n-        storeShared(rewriter, loc, indexWritePtr, accIndex, laneZero);\n+      for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+        Value writePtr = gep(elemPtrTys[i], smemBases[i], writeOffset);\n+        storeShared(rewriter, loc, writePtr, acc[i], laneZero);\n       }\n     }\n \n@@ -397,39 +417,36 @@ struct ReduceOpConversion\n     unsigned elemsPerThread = std::max<unsigned>(elems / numThreads, 1);\n     Value readOffset = threadId;\n     for (unsigned round = 0; round < elemsPerThread; ++round) {\n-      Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n       // FIXME(Qingyi): need predicate icmp_slt(threadId,\n       // i32_val(sizeInerWarps))\n-      Value acc = load(readPtr);\n-      Value accIndex;\n-      if (withIndex) {\n-        Value readIndexPtr = gep(indexPtrTy, indexSmemBase, readOffset);\n-        accIndex = load(readIndexPtr);\n+      SmallVector<Value> acc(op.getNumOperands());\n+      for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+        Value readPtr = gep(elemPtrTys[i], smemBases[i], readOffset);\n+        acc[i] = load(readPtr);\n       }\n \n       for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n-        Value shfl = shflSync(loc, rewriter, acc, N);\n-        if (!withIndex) {\n-          accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n-        } else {\n-          Value shflIndex = shflSync(loc, rewriter, accIndex, N);\n-          accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, shfl,\n-                              shflIndex, false);\n+        SmallVector<Value> shfl(op.getNumOperands());\n+        for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+          shfl[i] = shflSync(loc, rewriter, acc[i], N);\n         }\n+        accumulate(rewriter, *combineOp, acc, shfl, false);\n       }\n \n       // only the first thread in each sizeInterWarps is writing\n       Value writeOffset = readOffset;\n-      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+      SmallVector<Value> writePtrs(op.getNumOperands());\n+      for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+        writePtrs[i] = gep(elemPtrTys[i], smemBases[i], writeOffset);\n+      }\n       Value threadIsNeeded = icmp_slt(threadId, i32_val(elems));\n       Value laneIdModSizeInterWarps = urem(laneId, i32_val(sizeInterWarps));\n       Value laneIdModSizeInterWarpsIsZero =\n           icmp_eq(laneIdModSizeInterWarps, zero);\n       Value pred = and_(threadIsNeeded, laneIdModSizeInterWarpsIsZero);\n-      storeShared(rewriter, loc, writePtr, acc, pred);\n-      if (withIndex) {\n-        Value writeIndexPtr = gep(indexPtrTy, indexSmemBase, writeOffset);\n-        storeShared(rewriter, loc, writeIndexPtr, accIndex, pred);\n+\n+      for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+        storeShared(rewriter, loc, writePtrs[i], acc[i], pred);\n       }\n \n       if (round != elemsPerThread - 1) {\n@@ -439,52 +456,48 @@ struct ReduceOpConversion\n \n     // We could avoid this barrier in some of the layouts, however this is not\n     // the general case.\n-    // TODO: optimize the barrier incase the layouts are accepted.\n+    // TODO: optimize the barrier in case the layouts are accepted.\n     barrier();\n \n     // set output values\n-    if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n-      // nd-tensor where n >= 1\n-      auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n-      auto resultShape = resultTy.getShape();\n-      unsigned resultElems = getElemsPerThread(resultTy);\n-      auto resultIndices =\n-          emitIndices(loc, rewriter, resultLayout, resultShape);\n-      assert(resultIndices.size() == resultElems);\n-\n-      SmallVector<Value> resultVals(resultElems);\n-      for (size_t i = 0; i < resultElems; ++i) {\n-        SmallVector<Value> readIdx = resultIndices[i];\n-        readIdx.insert(readIdx.begin() + axis, i32_val(0));\n-        Value readOffset =\n-            linearize(rewriter, loc, readIdx, smemShapes[0], order);\n-        Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n-        Value indexReadPtr = gep(indexPtrTy, indexSmemBase, readOffset);\n-        resultVals[i] = withIndex ? load(indexReadPtr) : load(readPtr);\n-      }\n+    SmallVector<Value> results(op.getNumOperands());\n+    for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+      if (auto resultTy =\n+              op.getResult()[i].getType().dyn_cast<RankedTensorType>()) {\n+        // nd-tensor where n >= 1\n+        auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n+        unsigned resultElems = getTotalElemsPerThread(resultTy);\n+        auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultTy);\n+        assert(resultIndices.size() == resultElems);\n+\n+        SmallVector<Value> resultVals(resultElems);\n+        for (size_t j = 0; j < resultElems; ++j) {\n+          SmallVector<Value> readIdx = resultIndices[j];\n+          readIdx.insert(readIdx.begin() + axis, i32_val(0));\n+          Value readOffset =\n+              linearize(rewriter, loc, readIdx, smemShapes[0], order);\n+          Value readPtr = gep(elemPtrTys[i], smemBases[i], readOffset);\n+          resultVals[j] = load(readPtr);\n+        }\n \n-      SmallVector<Type> resultTypes(resultElems,\n-                                    withIndex ? llvmIndexTy : llvmElemTy);\n-      Type structTy =\n-          LLVM::LLVMStructType::getLiteral(this->getContext(), resultTypes);\n-      Value ret = getStructFromElements(loc, resultVals, rewriter, structTy);\n-      rewriter.replaceOp(op, ret);\n-    } else {\n-      // 0d-tensor -> scalar\n-      Value resultVal = withIndex ? load(indexSmemBase) : load(smemBase);\n-      rewriter.replaceOp(op, resultVal);\n+        results[i] = getTypeConverter()->packLLElements(loc, resultVals,\n+                                                        rewriter, resultTy);\n+      } else {\n+        // 0d-tensor -> scalar\n+        results[i] = load(smemBases[i]);\n+      }\n     }\n+    rewriter.replaceOp(op, results);\n \n     return success();\n   }\n };\n \n void populateReduceOpToLLVMPatterns(\n-    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem,\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit) {\n-  patterns.add<ReduceOpConversion>(typeConverter, allocation, smem,\n-                                   indexCacheInfo, benefit);\n+  patterns.add<ReduceOpConversion>(typeConverter, allocation, indexCacheInfo,\n+                                   benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.h", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -7,10 +7,9 @@ using namespace mlir;\n using namespace mlir::triton;\n \n void populateReduceOpToLLVMPatterns(\n-    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem,\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit);\n \n-#endif\n\\ No newline at end of file\n+#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 185, "deletions": 153, "changes": 338, "file_content_changes": "@@ -1,21 +1,19 @@\n #include \"TritonGPUToLLVM.h\"\n-#include \"DotOpHelpers.h\"\n #include \"Utility.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n \n using namespace mlir;\n using namespace mlir::triton;\n \n-using ::mlir::LLVM::getElementsFromStruct;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n-using ::mlir::LLVM::getStructFromElements;\n-using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getTotalElemsPerThread;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n-struct ReturnOpConversion : public ConvertOpToLLVMPattern<::mlir::ReturnOp> {\n-  using ConvertOpToLLVMPattern<ReturnOp>::ConvertOpToLLVMPattern;\n+struct ReturnOpConversion : public ConvertOpToLLVMPattern<triton::ReturnOp> {\n+  using ConvertOpToLLVMPattern<triton::ReturnOp>::ConvertOpToLLVMPattern;\n \n   LogicalResult\n-  matchAndRewrite(ReturnOp op, OpAdaptor adaptor,\n+  matchAndRewrite(triton::ReturnOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     unsigned numArguments = op.getNumOperands();\n \n@@ -54,9 +52,9 @@ struct BroadcastOpConversion\n     // regardless of the order of the layout\n     //\n     Location loc = op->getLoc();\n-    Value src = adaptor.src();\n-    Value result = op.result();\n-    auto srcTy = op.src().getType().cast<RankedTensorType>();\n+    Value src = adaptor.getSrc();\n+    Value result = op.getResult();\n+    auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n     auto resultTy = result.getType().cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding();\n     auto resultLayout = resultTy.getEncoding();\n@@ -66,38 +64,10 @@ struct BroadcastOpConversion\n \n     assert(rank == resultTy.getRank());\n     auto order = triton::gpu::getOrder(srcLayout);\n-    auto srcOffsets = emitOffsetForLayout(srcLayout, srcShape);\n-    auto resultOffsets = emitOffsetForLayout(resultLayout, resultShape);\n-    SmallVector<Value> srcVals = getElementsFromStruct(loc, src, rewriter);\n-    if (auto srcMma = srcLayout.dyn_cast<MmaEncodingAttr>()) {\n-      // NOTE: This is just an naive fix, but for MMA layout, and 2-d fix should\n-      // be all right.\n-      // TODO[Superjomn]: Replace this with a generic implementation.\n-      if (srcMma.isVolta()) {\n-        assert(srcTy.getElementType().isF16() &&\n-               \"Unexpected data type on Volta\");\n-        int numElemsPerThread = srcMma.getElemsPerThread(resultTy.getShape());\n-        int srcUniqElems = srcVals.size() / 2;\n-        int dup = numElemsPerThread / srcUniqElems;\n-        SmallVector<Value> retVals;\n-        if (srcShape[0] == 1) { // add-cols\n-          for (int i = 0; i < srcUniqElems; ++i)\n-            for (int k = 0; k < dup; ++k)\n-              retVals.push_back(srcVals[i * 2]);\n-\n-        } else { // add-rows\n-          for (int k = 0; k < dup; ++k)\n-            for (int i = 0; i < srcUniqElems; ++i)\n-              retVals.push_back(srcVals[i]);\n-        }\n-\n-        auto llvmStructTy = getTypeConverter()->convertType(resultTy);\n-        Value ret = getStructFromElements(loc, retVals, rewriter, llvmStructTy);\n-\n-        rewriter.replaceOp(op, {ret});\n-        return success();\n-      }\n-    }\n+    auto srcOffsets = emitOffsetForLayout(srcLayout, srcTy);\n+    auto resultOffsets = emitOffsetForLayout(resultLayout, resultTy);\n+    SmallVector<Value> srcVals =\n+        getTypeConverter()->unpackLLElements(loc, src, rewriter, srcTy);\n \n     DenseMap<SmallVector<unsigned>, Value, SmallVectorKeyInfo> srcValues;\n     for (size_t i = 0; i < srcOffsets.size(); i++) {\n@@ -113,33 +83,33 @@ struct BroadcastOpConversion\n       resultVals.push_back(srcValues.lookup(offset));\n     }\n \n-    auto llvmStructTy = getTypeConverter()->convertType(resultTy);\n     Value resultStruct =\n-        getStructFromElements(loc, resultVals, rewriter, llvmStructTy);\n+        getTypeConverter()->packLLElements(loc, resultVals, rewriter, resultTy);\n     rewriter.replaceOp(op, {resultStruct});\n     return success();\n   }\n };\n \n-struct PrintfOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintfOp> {\n+struct PrintOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n-      triton::PrintfOp>::ConvertTritonGPUOpToLLVMPattern;\n+      triton::PrintOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   LogicalResult\n-  matchAndRewrite(triton::PrintfOp op, OpAdaptor adaptor,\n+  matchAndRewrite(triton::PrintOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     auto loc = op->getLoc();\n     SmallVector<Value, 16> operands;\n-    for (auto operand : adaptor.getOperands()) {\n-      auto sub_operands = getElementsFromStruct(loc, operand, rewriter);\n+    for (size_t i = 0; i < op.getNumOperands(); i++) {\n+      auto sub_operands = getTypeConverter()->unpackLLElements(\n+          loc, adaptor.getOperands()[i], rewriter, op.getOperand(i).getType());\n       for (auto elem : sub_operands) {\n         operands.push_back(elem);\n       }\n     }\n     std::string formatStr;\n     llvm::raw_string_ostream os(formatStr);\n-    os << op.prefix();\n+    os << op.getPrefix();\n     if (!operands.empty()) {\n       os << getFormatSubstr(operands[0]);\n     }\n@@ -198,74 +168,45 @@ struct PrintfOpConversion\n     auto type = value.getType();\n     Value newOp = value;\n     Type newType = type;\n+    auto loc = UnknownLoc::get(context);\n \n     bool bUnsigned = type.isUnsignedInteger();\n     if (type.isIntOrIndex() && type.getIntOrFloatBitWidth() < 32) {\n       if (bUnsigned) {\n         newType = ui32_ty;\n-        newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n-                                              value);\n+        newOp = zext(newType, value);\n       } else {\n         newType = i32_ty;\n-        newOp = rewriter.create<LLVM::SExtOp>(UnknownLoc::get(context), newType,\n-                                              value);\n+        newOp = sext(newType, value);\n       }\n     } else if (type.isBF16() || type.isF16() || type.isF32()) {\n       newType = f64_ty;\n-      newOp = rewriter.create<LLVM::FPExtOp>(UnknownLoc::get(context), newType,\n-                                             value);\n+      newOp = fpext(newType, value);\n     }\n \n     return {newType, newOp};\n   }\n \n   static void llPrintf(StringRef msg, ValueRange args,\n                        ConversionPatternRewriter &rewriter) {\n-    static const char formatStringPrefix[] = \"printfFormat_\";\n     assert(!msg.empty() && \"printf with empty string not support\");\n     Type int8Ptr = ptr_ty(i8_ty);\n \n-    auto *context = rewriter.getContext();\n+    auto *ctx = rewriter.getContext();\n     auto moduleOp =\n         rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n     auto funcOp = getVprintfDeclaration(rewriter);\n+    auto loc = UnknownLoc::get(ctx);\n \n-    Value one = rewriter.create<LLVM::ConstantOp>(\n-        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(1));\n-    Value zero = rewriter.create<LLVM::ConstantOp>(\n-        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(0));\n-\n-    unsigned stringNumber = 0;\n-    SmallString<16> stringConstName;\n-    do {\n-      stringConstName.clear();\n-      (formatStringPrefix + Twine(stringNumber++)).toStringRef(stringConstName);\n-    } while (moduleOp.lookupSymbol(stringConstName));\n-\n-    llvm::SmallString<64> formatString(msg);\n-    formatString.push_back('\\n');\n-    formatString.push_back('\\0');\n-    size_t formatStringSize = formatString.size_in_bytes();\n-    auto globalType = LLVM::LLVMArrayType::get(i8_ty, formatStringSize);\n-\n-    LLVM::GlobalOp global;\n-    {\n-      ConversionPatternRewriter::InsertionGuard guard(rewriter);\n-      rewriter.setInsertionPointToStart(moduleOp.getBody());\n-      global = rewriter.create<LLVM::GlobalOp>(\n-          UnknownLoc::get(context), globalType,\n-          /*isConstant=*/true, LLVM::Linkage::Internal, stringConstName,\n-          rewriter.getStringAttr(formatString));\n-    }\n-\n-    Value globalPtr =\n-        rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(context), global);\n-    Value stringStart = rewriter.create<LLVM::GEPOp>(\n-        UnknownLoc::get(context), int8Ptr, globalPtr,\n-        SmallVector<Value>({zero, zero}));\n+    Value one = i32_val(1);\n+    Value zero = i32_val(0);\n \n-    Value bufferPtr =\n-        rewriter.create<LLVM::NullOp>(UnknownLoc::get(context), int8Ptr);\n+    llvm::SmallString<64> msgNewline(msg);\n+    msgNewline.push_back('\\n');\n+    msgNewline.push_back('\\0');\n+    Value prefixString =\n+        LLVM::addStringToModule(loc, rewriter, \"printfFormat_\", msgNewline);\n+    Value bufferPtr = null(int8Ptr);\n \n     SmallVector<Value, 16> newArgs;\n     if (args.size() >= 1) {\n@@ -278,65 +219,157 @@ struct PrintfOpConversion\n         newArgs.push_back(newArg);\n       }\n \n-      Type structTy = LLVM::LLVMStructType::getLiteral(context, argTypes);\n-      auto allocated = rewriter.create<LLVM::AllocaOp>(UnknownLoc::get(context),\n-                                                       ptr_ty(structTy), one,\n-                                                       /*alignment=*/0);\n+      Type structTy = LLVM::LLVMStructType::getLiteral(ctx, argTypes);\n+      auto allocated =\n+          rewriter.create<LLVM::AllocaOp>(loc, ptr_ty(structTy), one,\n+                                          /*alignment=*/0);\n \n       for (const auto &entry : llvm::enumerate(newArgs)) {\n-        auto index = rewriter.create<LLVM::ConstantOp>(\n-            UnknownLoc::get(context), i32_ty,\n-            rewriter.getI32IntegerAttr(entry.index()));\n-        auto fieldPtr = rewriter.create<LLVM::GEPOp>(\n-            UnknownLoc::get(context), ptr_ty(argTypes[entry.index()]),\n-            allocated, ArrayRef<Value>{zero, index});\n-        rewriter.create<LLVM::StoreOp>(UnknownLoc::get(context), entry.value(),\n-                                       fieldPtr);\n+        auto index = i32_val(entry.index());\n+        auto fieldPtr = gep(ptr_ty(argTypes[entry.index()]), allocated,\n+                            ArrayRef<Value>{zero, index});\n+        store(entry.value(), fieldPtr);\n       }\n-      bufferPtr = rewriter.create<LLVM::BitcastOp>(UnknownLoc::get(context),\n-                                                   int8Ptr, allocated);\n+      bufferPtr = bitcast(allocated, int8Ptr);\n     }\n \n-    SmallVector<Value> operands{stringStart, bufferPtr};\n-    rewriter.create<LLVM::CallOp>(UnknownLoc::get(context), funcOp, operands);\n+    SmallVector<Value> operands{prefixString, bufferPtr};\n+    call(funcOp, operands);\n+  }\n+};\n+\n+struct AssertOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::AssertOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::AssertOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::AssertOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op.getLoc();\n+    auto ctx = rewriter.getContext();\n+    auto elems = getTypeConverter()->unpackLLElements(\n+        loc, adaptor.getCondition(), rewriter, op.getCondition().getType());\n+    auto elemTy = elems[0].getType();\n+    Value condition = int_val(elemTy.getIntOrFloatBitWidth(), 0);\n+    for (auto elem : elems) {\n+      if (elemTy.isSignedInteger() || elemTy.isSignlessInteger()) {\n+        condition =\n+            or_(condition,\n+                icmp_eq(elem, rewriter.create<LLVM::ConstantOp>(\n+                                  loc, elemTy, rewriter.getZeroAttr(elemTy))));\n+      } else {\n+        assert(false && \"Unsupported type for assert\");\n+        return failure();\n+      }\n+    }\n+    llAssert(op, condition, adaptor.getMessage(), adaptor.getFile(),\n+             adaptor.getFunc(), adaptor.getLine(), rewriter);\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+\n+  // op: the op at which the assert is inserted. Unlike printf, we need to\n+  // know about the op to split the block.\n+  static void llAssert(Operation *op, Value condition, StringRef message,\n+                       StringRef file, StringRef func, int line,\n+                       ConversionPatternRewriter &rewriter) {\n+    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+    auto ctx = rewriter.getContext();\n+    auto loc = op->getLoc();\n+\n+    // #block1\n+    // if (condition) {\n+    //   #block2\n+    //   __assertfail(message);\n+    // }\n+    // #block3\n+    Block *prevBlock = op->getBlock();\n+    Block *ifBlock = rewriter.splitBlock(prevBlock, op->getIterator());\n+    rewriter.setInsertionPointToStart(ifBlock);\n+\n+    auto funcOp = getAssertfailDeclaration(rewriter);\n+    auto moduleOp =\n+        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    Value messageString =\n+        LLVM::addStringToModule(loc, rewriter, \"assertMessage_\", message);\n+    Value fileString =\n+        LLVM::addStringToModule(loc, rewriter, \"assertFile_\", file);\n+    Value funcString =\n+        LLVM::addStringToModule(loc, rewriter, \"assertFunc_\", func);\n+    Value lineNumber = i32_val(line);\n+    Value charSize = int_val(sizeof(size_t) * 8, sizeof(char));\n+\n+    SmallVector<Value> operands = {messageString, fileString, lineNumber,\n+                                   funcString, charSize};\n+    auto ret = call(funcOp, operands);\n+\n+    // Split a block after the call.\n+    Block *thenBlock = rewriter.splitBlock(ifBlock, op->getIterator());\n+    rewriter.setInsertionPointToEnd(ifBlock);\n+    rewriter.create<cf::BranchOp>(loc, thenBlock);\n+    rewriter.setInsertionPointToEnd(prevBlock);\n+    rewriter.create<cf::CondBranchOp>(loc, condition, ifBlock, thenBlock);\n+  }\n+\n+  static LLVM::LLVMFuncOp\n+  getAssertfailDeclaration(ConversionPatternRewriter &rewriter) {\n+    auto moduleOp =\n+        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    StringRef funcName(\"__assertfail\");\n+    Operation *funcOp = moduleOp.lookupSymbol(funcName);\n+    if (funcOp)\n+      return cast<LLVM::LLVMFuncOp>(*funcOp);\n+\n+    // void __assert_fail(const char * assertion, const char * file, unsigned\n+    // int line, const char * function);\n+    auto *ctx = rewriter.getContext();\n+    SmallVector<Type> argsType{ptr_ty(i8_ty), ptr_ty(i8_ty), i32_ty,\n+                               ptr_ty(i8_ty),\n+                               rewriter.getIntegerType(sizeof(size_t) * 8)};\n+    auto funcType = LLVM::LLVMFunctionType::get(void_ty(ctx), argsType);\n+\n+    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+    rewriter.setInsertionPointToStart(moduleOp.getBody());\n+\n+    return rewriter.create<LLVM::LLVMFuncOp>(UnknownLoc::get(ctx), funcName,\n+                                             funcType);\n   }\n };\n \n struct MakeRangeOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp> {\n \n   MakeRangeOpConversion(\n-      LLVMTypeConverter &converter,\n+      TritonGPUToLLVMTypeConverter &converter,\n       ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n       PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp>(\n-            converter, /*Allocation*/ nullptr, Value{}, indexCacheInfo,\n-            benefit) {}\n+            converter, indexCacheInfo, benefit) {}\n \n   LogicalResult\n   matchAndRewrite(triton::MakeRangeOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n-    auto rankedTy = op.result().getType().dyn_cast<RankedTensorType>();\n+    auto rankedTy = op.getResult().getType().cast<RankedTensorType>();\n     auto shape = rankedTy.getShape();\n     auto layout = rankedTy.getEncoding();\n \n     auto elemTy = rankedTy.getElementType();\n     assert(elemTy.isInteger(32));\n-    Value start = createIndexAttrConstant(rewriter, loc, elemTy, op.start());\n-    auto idxs = emitIndices(loc, rewriter, layout, shape);\n+    Value start = createIndexAttrConstant(rewriter, loc, elemTy, op.getStart());\n+    auto idxs = emitIndices(loc, rewriter, layout, rankedTy);\n     unsigned elems = idxs.size();\n     SmallVector<Value> retVals(elems);\n     // TODO: slice layout has more elements than expected.\n     // Unexpected behavior for make range, but generally OK when followed by\n     // expand dims + broadcast. very weird behavior otherwise potentially.\n-    for (const auto multiDim : llvm::enumerate(idxs)) {\n+    for (const auto &multiDim : llvm::enumerate(idxs)) {\n       assert(multiDim.value().size() == 1);\n       retVals[multiDim.index()] = add(multiDim.value()[0], start);\n     }\n-    SmallVector<Type> types(elems, elemTy);\n-    Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n-    Value result = getStructFromElements(loc, retVals, rewriter, structTy);\n+    Value result =\n+        getTypeConverter()->packLLElements(loc, retVals, rewriter, rankedTy);\n     rewriter.replaceOp(op, result);\n     return success();\n   }\n@@ -351,13 +384,11 @@ struct GetProgramIdOpConversion\n   matchAndRewrite(triton::GetProgramIdOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n-    assert(op.axis() < 3);\n+    assert(op.getAxisAsInt() < 3);\n \n-    Value blockId = rewriter.create<::mlir::gpu::BlockIdOp>(\n-        loc, rewriter.getIndexType(), dims[op.axis()]);\n-    auto llvmIndexTy = getTypeConverter()->getIndexType();\n-    rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>(\n-        op, TypeRange{llvmIndexTy}, ValueRange{blockId});\n+    Value blockId =\n+        rewriter.create<::mlir::gpu::BlockIdOp>(loc, dims[op.getAxisAsInt()]);\n+    rewriter.replaceOpWithNewOp<arith::IndexCastOp>(op, i32_ty, blockId);\n     return success();\n   }\n \n@@ -375,13 +406,12 @@ struct GetNumProgramsOpConversion\n   matchAndRewrite(triton::GetNumProgramsOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n-    assert(op.axis() < 3);\n+    assert(op.getAxis() < 3);\n+\n+    Value blockId =\n+        rewriter.create<::mlir::gpu::GridDimOp>(loc, dims[op.getAxis()]);\n+    rewriter.replaceOpWithNewOp<arith::IndexCastOp>(op, i32_ty, blockId);\n \n-    Value blockId = rewriter.create<::mlir::gpu::GridDimOp>(\n-        loc, rewriter.getIndexType(), dims[op.axis()]);\n-    auto llvmIndexTy = getTypeConverter()->getIndexType();\n-    rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>(\n-        op, TypeRange{llvmIndexTy}, ValueRange{blockId});\n     return success();\n   }\n \n@@ -400,25 +430,28 @@ struct AddPtrOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n     auto resultTy = op.getType();\n+    auto offsetTy = op.getOffset().getType();\n+    auto ptrTy = op.getPtr().getType();\n     auto resultTensorTy = resultTy.dyn_cast<RankedTensorType>();\n     if (resultTensorTy) {\n-      unsigned elems = getElemsPerThread(resultTy);\n+      unsigned elems = getTotalElemsPerThread(resultTy);\n       Type elemTy =\n           getTypeConverter()->convertType(resultTensorTy.getElementType());\n-      SmallVector<Type> types(elems, elemTy);\n-      Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n-      auto ptrs = getElementsFromStruct(loc, adaptor.ptr(), rewriter);\n-      auto offsets = getElementsFromStruct(loc, adaptor.offset(), rewriter);\n+      auto ptrs = getTypeConverter()->unpackLLElements(loc, adaptor.getPtr(),\n+                                                       rewriter, ptrTy);\n+      auto offsets = getTypeConverter()->unpackLLElements(\n+          loc, adaptor.getOffset(), rewriter, offsetTy);\n       SmallVector<Value> resultVals(elems);\n       for (unsigned i = 0; i < elems; ++i) {\n         resultVals[i] = gep(elemTy, ptrs[i], offsets[i]);\n       }\n-      Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n+      Value view = getTypeConverter()->packLLElements(loc, resultVals, rewriter,\n+                                                      resultTy);\n       rewriter.replaceOp(op, view);\n     } else {\n       assert(resultTy.isa<triton::PointerType>());\n       Type llResultTy = getTypeConverter()->convertType(resultTy);\n-      Value result = gep(llResultTy, adaptor.ptr(), adaptor.offset());\n+      Value result = gep(llResultTy, adaptor.getPtr(), adaptor.getOffset());\n       rewriter.replaceOp(op, result);\n     }\n     return success();\n@@ -459,16 +492,16 @@ struct AllocTensorOpConversion\n };\n \n struct ExtractSliceOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<tensor::ExtractSliceOp> {\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ExtractSliceOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n-      tensor::ExtractSliceOp>::ConvertTritonGPUOpToLLVMPattern;\n+      triton::gpu::ExtractSliceOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   LogicalResult\n-  matchAndRewrite(tensor::ExtractSliceOp op, OpAdaptor adaptor,\n+  matchAndRewrite(triton::gpu::ExtractSliceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     // %dst = extract_slice %src[%offsets]\n     Location loc = op->getLoc();\n-    auto srcTy = op.source().getType().dyn_cast<RankedTensorType>();\n+    auto srcTy = op.getSource().getType().dyn_cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n     assert(srcLayout && \"Unexpected resultLayout in ExtractSliceOpConversion\");\n     assert(op.hasUnitStride() &&\n@@ -477,13 +510,13 @@ struct ExtractSliceOpConversion\n     // newBase = base + offset\n     // Triton supports either static and dynamic offsets\n     auto smemObj =\n-        getSharedMemoryObjectFromStruct(loc, adaptor.source(), rewriter);\n+        getSharedMemoryObjectFromStruct(loc, adaptor.getSource(), rewriter);\n     SmallVector<Value, 4> opOffsetVals;\n     SmallVector<Value, 4> offsetVals;\n     auto mixedOffsets = op.getMixedOffsets();\n     for (auto i = 0; i < mixedOffsets.size(); ++i) {\n       if (op.isDynamicOffset(i))\n-        opOffsetVals.emplace_back(adaptor.offsets()[i]);\n+        opOffsetVals.emplace_back(adaptor.getOffsets()[i]);\n       else\n         opOffsetVals.emplace_back(i32_val(op.getStaticOffset(i)));\n       offsetVals.emplace_back(add(smemObj.offsets[i], opOffsetVals[i]));\n@@ -504,7 +537,6 @@ struct ExtractSliceOpConversion\n \n     auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n     auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n-    auto resTy = op.getType().dyn_cast<RankedTensorType>();\n     smemObj = SharedMemoryObject(gep(elemPtrTy, smemObj.base, offset),\n                                  strideVals, offsetVals);\n     auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n@@ -560,7 +592,7 @@ namespace LLVM {\n \n void vprintf(StringRef msg, ValueRange args,\n              ConversionPatternRewriter &rewriter) {\n-  PrintfOpConversion::llPrintf(msg, args, rewriter);\n+  PrintOpConversion::llPrintf(msg, args, rewriter);\n }\n \n void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n@@ -579,23 +611,23 @@ void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n } // namespace mlir\n \n void populateTritonGPUToLLVMPatterns(\n-    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem,\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    ModuleAllocation &moduleAllocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit) {\n   patterns.add<AddPtrOpConversion>(typeConverter, benefit);\n-  patterns.add<AllocTensorOpConversion>(typeConverter, allocation, smem,\n+  patterns.add<AllocTensorOpConversion>(typeConverter, moduleAllocation,\n                                         benefit);\n   patterns.add<AsyncCommitGroupOpConversion>(typeConverter, benefit);\n   patterns.add<AsyncWaitOpConversion>(typeConverter, benefit);\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n \n-  patterns.add<ExtractSliceOpConversion>(typeConverter, allocation, smem,\n+  patterns.add<ExtractSliceOpConversion>(typeConverter, moduleAllocation,\n                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n   patterns.add<GetNumProgramsOpConversion>(typeConverter, benefit);\n   patterns.add<MakeRangeOpConversion>(typeConverter, indexCacheInfo, benefit);\n   patterns.add<ReturnOpConversion>(typeConverter, benefit);\n-  patterns.add<PrintfOpConversion>(typeConverter, benefit);\n+  patterns.add<PrintOpConversion>(typeConverter, benefit);\n+  patterns.add<AssertOpConversion>(typeConverter, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -7,9 +7,8 @@ using namespace mlir;\n using namespace mlir::triton;\n \n void populateTritonGPUToLLVMPatterns(\n-    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem,\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 401, "deletions": 195, "changes": 596, "file_content_changes": "@@ -6,16 +6,18 @@\n // and <atomic>\n #include \"triton/Analysis/Allocation.h\"\n \n+#include \"TypeConverter.h\"\n //\n #include \"Utility.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n-\n+#include <set>\n using namespace mlir;\n using namespace mlir::triton;\n \n using ::mlir::LLVM::SharedMemoryObject;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;\n \n@@ -38,20 +40,19 @@ void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n // TODO(Superjomn): remove the code when MLIR v15.0 is included.\n // All the rights are reserved by the LLVM community.\n \n-struct FuncOpConversionBase : public ConvertOpToLLVMPattern<FuncOp> {\n-private:\n+struct FuncOpConversionBase : public ConvertOpToLLVMPattern<triton::FuncOp> {\n+protected:\n   /// Only retain those attributes that are not constructed by\n   /// `LLVMFuncOp::build`. If `filterArgAttrs` is set, also filter out argument\n   /// attributes.\n-  static void filterFuncAttributes(ArrayRef<NamedAttribute> attrs,\n-                                   bool filterArgAttrs,\n+  static void filterFuncAttributes(triton::FuncOp op, bool filterArgAttrs,\n                                    SmallVectorImpl<NamedAttribute> &result) {\n-    for (const auto &attr : attrs) {\n+\n+    for (const auto &attr : op->getAttrs()) {\n       if (attr.getName() == SymbolTable::getSymbolAttrName() ||\n-          attr.getName() == FunctionOpInterface::getTypeAttrName() ||\n+          attr.getName() == op.getFunctionTypeAttrName() ||\n           attr.getName() == \"std.varargs\" ||\n-          (filterArgAttrs &&\n-           attr.getName() == FunctionOpInterface::getArgDictAttrName()))\n+          (filterArgAttrs && attr.getName() == op.getArgAttrsAttrName()))\n         continue;\n       result.push_back(attr);\n     }\n@@ -64,36 +65,36 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<FuncOp> {\n   }\n \n protected:\n-  using ConvertOpToLLVMPattern<FuncOp>::ConvertOpToLLVMPattern;\n+  using ConvertOpToLLVMPattern<triton::FuncOp>::ConvertOpToLLVMPattern;\n \n   // Convert input FuncOp to LLVMFuncOp by using the LLVMTypeConverter provided\n   // to this legalization pattern.\n   LLVM::LLVMFuncOp\n-  convertFuncOpToLLVMFuncOp(FuncOp funcOp,\n+  convertFuncOpToLLVMFuncOp(triton::FuncOp funcOp,\n                             ConversionPatternRewriter &rewriter) const {\n     // Convert the original function arguments. They are converted using the\n     // LLVMTypeConverter provided to this legalization pattern.\n     auto varargsAttr = funcOp->getAttrOfType<BoolAttr>(\"func.varargs\");\n     TypeConverter::SignatureConversion result(funcOp.getNumArguments());\n     auto llvmType = getTypeConverter()->convertFunctionSignature(\n-        funcOp.getType(), varargsAttr && varargsAttr.getValue(), result);\n+        funcOp.getFunctionType(), varargsAttr && varargsAttr.getValue(), false,\n+        result);\n     if (!llvmType)\n       return nullptr;\n \n     // Propagate argument/result attributes to all converted arguments/result\n     // obtained after converting a given original argument/result.\n     SmallVector<NamedAttribute, 4> attributes;\n-    filterFuncAttributes(funcOp->getAttrs(), /*filterArgAttrs=*/true,\n-                         attributes);\n+    filterFuncAttributes(funcOp, /*filterArgAttrs=*/true, attributes);\n     if (ArrayAttr resAttrDicts = funcOp.getAllResultAttrs()) {\n       assert(!resAttrDicts.empty() && \"expected array to be non-empty\");\n       auto newResAttrDicts =\n           (funcOp.getNumResults() == 1)\n               ? resAttrDicts\n               : rewriter.getArrayAttr(\n                     {wrapAsStructAttrs(rewriter, resAttrDicts)});\n-      attributes.push_back(rewriter.getNamedAttr(\n-          FunctionOpInterface::getResultDictAttrName(), newResAttrDicts));\n+      attributes.push_back(\n+          rewriter.getNamedAttr(funcOp.getResAttrsAttrName(), newResAttrDicts));\n     }\n     if (ArrayAttr argAttrDicts = funcOp.getAllArgAttrs()) {\n       SmallVector<Attribute, 4> newArgAttrs(\n@@ -104,9 +105,8 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<FuncOp> {\n         for (size_t j = 0; j < mapping->size; ++j)\n           newArgAttrs[mapping->inputNo + j] = argAttrDicts[i];\n       }\n-      attributes.push_back(\n-          rewriter.getNamedAttr(FunctionOpInterface::getArgDictAttrName(),\n-                                rewriter.getArrayAttr(newArgAttrs)));\n+      attributes.push_back(rewriter.getNamedAttr(\n+          funcOp.getArgAttrsAttrName(), rewriter.getArrayAttr(newArgAttrs)));\n     }\n     for (const auto &pair : llvm::enumerate(attributes)) {\n       if (pair.value().getName() == \"llvm.linkage\") {\n@@ -118,9 +118,8 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<FuncOp> {\n     // Create an LLVM function, use external linkage by default until MLIR\n     // functions have linkage.\n     LLVM::Linkage linkage = LLVM::Linkage::External;\n-    if (funcOp->hasAttr(\"llvm.linkage\")) {\n-      auto attr =\n-          funcOp->getAttr(\"llvm.linkage\").dyn_cast<mlir::LLVM::LinkageAttr>();\n+    if (auto linkageAttr = funcOp->getDiscardableAttr(\"llvm.linkage\")) {\n+      auto attr = linkageAttr.dyn_cast<mlir::LLVM::LinkageAttr>();\n       if (!attr) {\n         funcOp->emitError()\n             << \"Contains llvm.linkage attribute not of type LLVM::LinkageAttr\";\n@@ -130,7 +129,7 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<FuncOp> {\n     }\n     auto newFuncOp = rewriter.create<LLVM::LLVMFuncOp>(\n         funcOp.getLoc(), funcOp.getName(), llvmType, linkage,\n-        /*dsoLocal*/ false, attributes);\n+        /*dsoLocal*/ false, LLVM::CConv::C, attributes);\n     rewriter.inlineRegionBefore(funcOp.getBody(), newFuncOp.getBody(),\n                                 newFuncOp.end());\n     if (failed(rewriter.convertRegionTypes(&newFuncOp.getBody(), *typeConverter,\n@@ -141,25 +140,26 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<FuncOp> {\n   }\n };\n \n-using IndexCacheKeyT = std::pair<Attribute, SmallVector<int64_t>>;\n+using IndexCacheKeyT = std::pair<Attribute, RankedTensorType>;\n \n struct CacheKeyDenseMapInfo {\n   static IndexCacheKeyT getEmptyKey() {\n     auto *pointer = llvm::DenseMapInfo<void *>::getEmptyKey();\n     return std::make_pair(\n         mlir::Attribute(static_cast<mlir::Attribute::ImplType *>(pointer)),\n-        SmallVector<int64_t>{});\n+        RankedTensorType{});\n   }\n   static IndexCacheKeyT getTombstoneKey() {\n     auto *pointer = llvm::DenseMapInfo<void *>::getTombstoneKey();\n+    auto tombstone = llvm::DenseMapInfo<RankedTensorType>::getTombstoneKey();\n     return std::make_pair(\n         mlir::Attribute(static_cast<mlir::Attribute::ImplType *>(pointer)),\n-        SmallVector<int64_t>{std::numeric_limits<int64_t>::max()});\n+        tombstone);\n   }\n   static unsigned getHashValue(IndexCacheKeyT key) {\n-    return llvm::hash_combine(\n-        mlir::hash_value(key.first),\n-        llvm::hash_combine_range(key.second.begin(), key.second.end()));\n+    auto shape = key.second.getShape();\n+    return llvm::hash_combine(mlir::hash_value(key.first),\n+                              mlir::hash_value(key.second));\n   }\n   static bool isEqual(IndexCacheKeyT LHS, IndexCacheKeyT RHS) {\n     return LHS == RHS;\n@@ -178,22 +178,26 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     OpBuilder::InsertPoint *indexInsertPoint;\n   };\n \n-  explicit ConvertTritonGPUOpToLLVMPatternBase(LLVMTypeConverter &typeConverter)\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(\n+      TritonGPUToLLVMTypeConverter &typeConverter)\n       : converter(&typeConverter) {}\n \n-  explicit ConvertTritonGPUOpToLLVMPatternBase(LLVMTypeConverter &typeConverter,\n-                                               const Allocation *allocation,\n-                                               Value smem)\n-      : converter(&typeConverter), allocation(allocation), smem(smem) {}\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(\n+      TritonGPUToLLVMTypeConverter &typeConverter,\n+      IndexCacheInfo indexCacheInfo)\n+      : converter(&typeConverter), indexCacheInfo(indexCacheInfo) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(\n+      TritonGPUToLLVMTypeConverter &typeConverter, ModuleAllocation &allocation)\n+      : converter(&typeConverter), allocation(&allocation) {}\n \n-  explicit ConvertTritonGPUOpToLLVMPatternBase(LLVMTypeConverter &typeConverter,\n-                                               const Allocation *allocation,\n-                                               Value smem,\n-                                               IndexCacheInfo indexCacheInfo)\n-      : converter(&typeConverter), indexCacheInfo(indexCacheInfo),\n-        allocation(allocation), smem(smem) {}\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(\n+      TritonGPUToLLVMTypeConverter &typeConverter, ModuleAllocation &allocation,\n+      IndexCacheInfo indexCacheInfo)\n+      : converter(&typeConverter), allocation(&allocation),\n+        indexCacheInfo(indexCacheInfo) {}\n \n-  LLVMTypeConverter *getTypeConverter() const { return converter; }\n+  TritonGPUToLLVMTypeConverter *getTypeConverter() const { return converter; }\n \n   static Value\n   getStructFromSharedMemoryObject(Location loc,\n@@ -203,18 +207,19 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto types = smemObj.getTypes();\n     auto structTy =\n         LLVM::LLVMStructType::getLiteral(rewriter.getContext(), types);\n-    return getStructFromElements(loc, elems, rewriter, structTy);\n+    // pack into struct\n+    Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structTy);\n+    for (const auto &v : llvm::enumerate(elems)) {\n+      assert(v.value() && \"can not insert null values\");\n+      llvmStruct = insert_val(structTy, llvmStruct, v.value(), v.index());\n+    }\n+    return llvmStruct;\n   }\n \n   Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n-    auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n-    auto cast = rewriter.create<UnrealizedConversionCastOp>(\n-        loc, TypeRange{llvmIndexTy},\n-        ValueRange{rewriter.create<::mlir::gpu::ThreadIdOp>(\n-            loc, rewriter.getIndexType(), ::mlir::gpu::Dimension::x)});\n-    Value threadId = cast.getResult(0);\n-\n-    return threadId;\n+    auto tid = rewriter.create<::mlir::gpu::ThreadIdOp>(\n+        loc, ::mlir::gpu::Dimension::x);\n+    return rewriter.create<arith::IndexCastOp>(loc, i32_ty, tid);\n   }\n \n   // -----------------------------------------------------------------------\n@@ -223,13 +228,20 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   template <typename T>\n   Value getSharedMemoryBase(Location loc, ConversionPatternRewriter &rewriter,\n                             T value) const {\n-\n     auto ptrTy = LLVM::LLVMPointerType::get(\n         this->getTypeConverter()->convertType(rewriter.getI8Type()), 3);\n-    auto bufferId = allocation->getBufferId(value);\n+    FunctionOpInterface funcOp;\n+    if constexpr (std::is_pointer_v<T>)\n+      funcOp = value->template getParentOfType<FunctionOpInterface>();\n+    else\n+      funcOp = value.getParentRegion()\n+                   ->template getParentOfType<FunctionOpInterface>();\n+    auto *funcAllocation = allocation->getFuncData(funcOp);\n+    auto smem = allocation->getFunctionSharedMemoryBase(funcOp);\n+    auto bufferId = funcAllocation->getBufferId(value);\n     assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n-    size_t offset = allocation->getOffset(bufferId);\n-    Value offVal = idx_val(offset);\n+    size_t offset = funcAllocation->getOffset(bufferId);\n+    Value offVal = i32_val(offset);\n     Value base = gep(ptrTy, smem, offVal);\n     return base;\n   }\n@@ -244,8 +256,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     // This utililty computes the pointers for accessing the provided swizzled\n     // shared memory layout `resSharedLayout`. More specifically, it computes,\n     // for all indices (row, col) of `srcEncoding` such that idx % inVec = 0,\n-    // the pointer: ptr[(row, col)] = base + (rowOff * strides[ord[1]] + colOff)\n-    // where :\n+    // the pointer: ptr[(row, col)] = base + (rowOff * strides[ord[1]] +\n+    // colOff) where :\n     //   compute phase = (row // perPhase) % maxPhase\n     //   rowOff = row\n     //   colOff = colOffSwizzled + colOffOrdered\n@@ -255,8 +267,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     // Note 1:\n     // -------\n     // Because swizzling happens at a granularity of outVec, we need to\n-    // decompose the offset into a swizzled factor and a non-swizzled (ordered)\n-    // factor\n+    // decompose the offset into a swizzled factor and a non-swizzled\n+    // (ordered) factor\n     //\n     // Note 2:\n     // -------\n@@ -273,7 +285,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n     auto srcEncoding = srcTy.getEncoding();\n     auto srcShape = srcTy.getShape();\n-    unsigned numElems = triton::gpu::getElemsPerThread(srcTy);\n+    unsigned numElems = triton::gpu::getTotalElemsPerThread(srcTy);\n     // swizzling params as described in TritonGPUAttrDefs.td\n     unsigned outVec = resSharedLayout.getVec();\n     unsigned perPhase = resSharedLayout.getPerPhase();\n@@ -282,7 +294,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto inOrder = triton::gpu::getOrder(srcEncoding);\n     auto outOrder = triton::gpu::getOrder(resSharedLayout);\n     // tensor indices held by the current thread, as LLVM values\n-    auto srcIndices = emitIndices(loc, rewriter, srcEncoding, srcShape);\n+    auto srcIndices = emitIndices(loc, rewriter, srcEncoding, srcTy);\n     // return values\n     DenseMap<unsigned, Value> ret;\n     // cache for non-immediate offsets\n@@ -291,11 +303,11 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n       // extract multi dimensional index for current element\n       auto idx = srcIndices[elemIdx];\n-      Value idxCol = idx[inOrder[0]]; // contiguous dimension\n-      Value idxRow = idx[inOrder[1]]; // discontiguous dimension\n-      Value strideCol = srcStrides[inOrder[0]];\n-      Value strideRow = srcStrides[inOrder[1]];\n-      // extract dynamic/static offset for immediate offseting\n+      Value idxCol = idx[outOrder[0]]; // contiguous dimension\n+      Value idxRow = idx[outOrder[1]]; // discontiguous dimension\n+      Value strideCol = srcStrides[outOrder[0]];\n+      Value strideRow = srcStrides[outOrder[1]];\n+      // extract dynamic/static offset for immediate offsetting\n       unsigned immedateOffCol = 0;\n       if (auto add = dyn_cast_or_null<LLVM::AddOp>(idxCol.getDefiningOp()))\n         if (auto _cst = dyn_cast_or_null<LLVM::ConstantOp>(\n@@ -307,7 +319,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n           idxCol = cacheCol[key];\n           immedateOffCol = cst / (outVec * maxPhase) * (outVec * maxPhase);\n         }\n-      // extract dynamic/static offset for immediate offseting\n+      // extract dynamic/static offset for immediate offsetting\n       unsigned immedateOffRow = 0;\n       if (auto add = dyn_cast_or_null<LLVM::AddOp>(idxRow.getDefiningOp()))\n         if (auto _cst = dyn_cast_or_null<LLVM::ConstantOp>(\n@@ -338,13 +350,62 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       Value currPtr = gep(dstPtrTy, dstPtrBase, offset);\n       // compute immediate offset\n       Value immedateOff =\n-          add(mul(i32_val(immedateOffRow), srcStrides[inOrder[1]]),\n+          add(mul(i32_val(immedateOffRow), srcStrides[outOrder[1]]),\n               i32_val(immedateOffCol));\n       ret[elemIdx] = gep(dstPtrTy, currPtr, immedateOff);\n     }\n     return ret;\n   }\n \n+  SmallVector<Value>\n+  loadSharedToDistributed(Value dst, ArrayRef<SmallVector<Value>> dstIndices,\n+                          Value src, SharedMemoryObject smemObj, Type elemTy,\n+                          Location loc,\n+                          ConversionPatternRewriter &rewriter) const {\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto dstShape = dstTy.getShape();\n+    assert(dstShape.size() == 2 &&\n+           \"Unexpected rank of loadSharedToDistributed\");\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto dstDistributedLayout = dstTy.getEncoding();\n+    if (auto mmaLayout = dstDistributedLayout.dyn_cast<MmaEncodingAttr>()) {\n+      assert((!mmaLayout.isVolta()) &&\n+             \"ConvertLayout Shared->MMAv1 is not supported yet\");\n+    }\n+    auto srcSharedLayout =\n+        srcTy.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n+    auto srcElemTy = srcTy.getElementType();\n+    auto dstElemTy = dstTy.getElementType();\n+    auto inOrd = triton::gpu::getOrder(srcSharedLayout);\n+    auto outOrd = triton::gpu::getOrder(dstDistributedLayout);\n+    unsigned outVec =\n+        inOrd == outOrd\n+            ? triton::gpu::getContigPerThread(dstDistributedLayout)[outOrd[0]]\n+            : 1;\n+    unsigned inVec = srcSharedLayout.getVec();\n+    unsigned minVec = std::min(outVec, inVec);\n+    unsigned outElems = triton::gpu::getTotalElemsPerThread(dstTy);\n+    assert(outElems == dstIndices.size());\n+\n+    DenseMap<unsigned, Value> sharedPtrs = getSwizzledSharedPtrs(\n+        loc, outVec, dstTy, srcSharedLayout, srcElemTy, smemObj, rewriter,\n+        smemObj.offsets, smemObj.strides);\n+    assert(outElems % minVec == 0 && \"Unexpected number of elements\");\n+    unsigned numVecs = outElems / minVec;\n+    auto wordTy = vec_ty(elemTy, minVec);\n+    SmallVector<Value> outVals(outElems);\n+    for (unsigned i = 0; i < numVecs; ++i) {\n+      Value smemAddr = sharedPtrs[i * minVec];\n+      smemAddr = bitcast(smemAddr, ptr_ty(wordTy, 3));\n+      Value valVec = load(smemAddr);\n+      for (unsigned v = 0; v < minVec; ++v) {\n+        Value currVal = extract_element(dstElemTy, valVec, i32_val(v));\n+        outVals[i * minVec + v] = currVal;\n+      }\n+    }\n+    return outVals;\n+  }\n+\n   void storeDistributedToShared(Value src, Value llSrc,\n                                 ArrayRef<Value> dstStrides,\n                                 ArrayRef<SmallVector<Value>> srcIndices,\n@@ -359,7 +420,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto srcDistributedLayout = srcTy.getEncoding();\n     if (auto mmaLayout = srcDistributedLayout.dyn_cast<MmaEncodingAttr>()) {\n       assert((!mmaLayout.isVolta()) &&\n-             \"ConvertLayout MMAv1->Shared is not suppported yet\");\n+             \"ConvertLayout MMAv1->Shared is not supported yet\");\n     }\n     auto dstSharedLayout =\n         dstTy.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n@@ -372,15 +433,11 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n             : 1;\n     unsigned outVec = dstSharedLayout.getVec();\n     unsigned minVec = std::min(outVec, inVec);\n-    unsigned perPhase = dstSharedLayout.getPerPhase();\n-    unsigned maxPhase = dstSharedLayout.getMaxPhase();\n-    unsigned numElems = triton::gpu::getElemsPerThread(srcTy);\n+    unsigned numElems = triton::gpu::getTotalElemsPerThread(srcTy);\n     assert(numElems == srcIndices.size());\n-    auto inVals = LLVM::getElementsFromStruct(loc, llSrc, rewriter);\n+    auto inVals =\n+        getTypeConverter()->unpackLLElements(loc, llSrc, rewriter, srcTy);\n     auto wordTy = vec_ty(elemTy, minVec);\n-    auto elemPtrTy = ptr_ty(elemTy);\n-    Value outVecVal = i32_val(outVec);\n-    Value minVecVal = i32_val(minVec);\n     Value word;\n \n     SmallVector<Value> srcStrides = {dstStrides[0], dstStrides[1]};\n@@ -391,8 +448,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n         getSwizzledSharedPtrs(loc, inVec, srcTy, dstSharedLayout, dstElemTy,\n                               smemObj, rewriter, offsetVals, srcStrides);\n \n-    std::map<unsigned, Value> cache0;\n-    std::map<unsigned, Value> cache1;\n     for (unsigned i = 0; i < numElems; ++i) {\n       if (i % minVec == 0)\n         word = undef(wordTy);\n@@ -408,6 +463,46 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // -----------------------------------------------------------------------\n   // Utilities\n   // -----------------------------------------------------------------------\n+  Value getMask(Type valueTy, ConversionPatternRewriter &rewriter,\n+                Location loc) const {\n+    auto tensorTy = valueTy.dyn_cast<RankedTensorType>();\n+    Value mask = int_val(1, 1);\n+    auto tid = tid_val();\n+    if (tensorTy) {\n+      auto layout = tensorTy.getEncoding();\n+      auto shape = tensorTy.getShape();\n+      unsigned rank = shape.size();\n+      auto sizePerThread = triton::gpu::getSizePerThread(layout);\n+      auto threadsPerWarp = triton::gpu::getThreadsPerWarp(layout);\n+      auto warpsPerCTA = triton::gpu::getWarpsPerCTA(layout);\n+      auto order = triton::gpu::getOrder(layout);\n+      auto shapePerCTA = triton::gpu::getShapePerCTA(layout, shape);\n+      Value warpSize = i32_val(32);\n+      Value laneId = urem(tid, warpSize);\n+      Value warpId = udiv(tid, warpSize);\n+      SmallVector<Value> multiDimWarpId =\n+          delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+      SmallVector<Value> multiDimThreadId =\n+          delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n+      for (unsigned dim = 0; dim < rank; ++dim) {\n+        // if there is no data replication across threads on this dimension\n+        if (shape[dim] >= shapePerCTA[dim])\n+          continue;\n+        // Otherwise, we need to mask threads that will replicate data on this\n+        // dimension. Calculate the thread index on this dimension for the CTA\n+        Value threadDim =\n+            add(mul(multiDimWarpId[dim], i32_val(threadsPerWarp[dim])),\n+                multiDimThreadId[dim]);\n+        mask = and_(mask, icmp_slt(mul(threadDim, i32_val(sizePerThread[dim])),\n+                                   i32_val(shape[dim])));\n+      }\n+    } else {\n+      // If the tensor is not ranked, then it is a scalar and only thread 0 can\n+      // write\n+      mask = and_(mask, icmp_eq(tid, i32_val(0)));\n+    }\n+    return mask;\n+  }\n \n   // Convert an \\param index to a multi-dim coordinate given \\param shape and\n   // \\param order.\n@@ -437,7 +532,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     } else {\n       Value remained = linear;\n       for (auto &&en : llvm::enumerate(shape.drop_back())) {\n-        Value dimSize = idx_val(en.value());\n+        Value dimSize = i32_val(en.value());\n         multiDim[en.index()] = urem(remained, dimSize);\n         remained = udiv(remained, dimSize);\n       }\n@@ -456,12 +551,12 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n                   ArrayRef<Value> multiDim, ArrayRef<unsigned> shape) const {\n     auto rank = multiDim.size();\n-    Value linear = idx_val(0);\n+    Value linear = i32_val(0);\n     if (rank > 0) {\n       linear = multiDim.back();\n       for (auto [dim, dimShape] :\n            llvm::reverse(llvm::zip(multiDim.drop_back(), shape.drop_back()))) {\n-        Value dimSize = idx_val(dimShape);\n+        Value dimSize = i32_val(dimShape);\n         linear = add(mul(linear, dimSize), dim);\n       }\n     }\n@@ -471,7 +566,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   Value dot(ConversionPatternRewriter &rewriter, Location loc,\n             ArrayRef<Value> offsets, ArrayRef<Value> strides) const {\n     assert(offsets.size() == strides.size());\n-    Value ret = idx_val(0);\n+    Value ret = i32_val(0);\n     for (auto [offset, stride] : llvm::zip(offsets, strides)) {\n       ret = add(ret, mul(offset, stride));\n     }\n@@ -500,45 +595,56 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n   SmallVector<Value> emitBaseIndexForLayout(Location loc,\n                                             ConversionPatternRewriter &rewriter,\n-                                            const Attribute &layout,\n-                                            ArrayRef<int64_t> shape) const {\n-    IndexCacheKeyT key = std::make_pair(layout, llvm::to_vector(shape));\n+                                            Attribute layout,\n+                                            RankedTensorType type) const {\n+    IndexCacheKeyT key = std::make_pair(layout, type);\n     auto cache = indexCacheInfo.baseIndexCache;\n-    assert(cache && \"baseIndexCache is nullptr\");\n     auto insertPt = indexCacheInfo.indexInsertPoint;\n-    if (cache->count(key) > 0) {\n+    if (cache && cache->count(key) > 0) {\n       return cache->lookup(key);\n     } else {\n       ConversionPatternRewriter::InsertionGuard guard(rewriter);\n-      restoreInsertionPointIfSet(insertPt, rewriter);\n+      if (cache)\n+        restoreInsertionPointIfSet(insertPt, rewriter);\n       SmallVector<Value> result;\n       if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n         result =\n-            emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n+            emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, type);\n       } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n         if (mmaLayout.isVolta())\n-          result = emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, shape);\n+          result = emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, type);\n         if (mmaLayout.isAmpere())\n-          result = emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, shape);\n+          result = emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, type);\n+      } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+        auto parentLayout = sliceLayout.getParent();\n+        auto parentShape = sliceLayout.paddedShape(type.getShape());\n+        RankedTensorType parentTy = RankedTensorType::get(\n+            parentShape, type.getElementType(), parentLayout);\n+        result = emitBaseIndexForLayout(loc, rewriter, parentLayout, parentTy);\n+        result.erase(result.begin() + sliceLayout.getDim());\n       } else {\n         llvm_unreachable(\"unsupported emitBaseIndexForLayout\");\n       }\n-      cache->insert(std::make_pair(key, result));\n-      *insertPt = rewriter.saveInsertionPoint();\n+      if (cache) {\n+        cache->insert(std::make_pair(key, result));\n+        *insertPt = rewriter.saveInsertionPoint();\n+      }\n       return result;\n     }\n   }\n \n   SmallVector<SmallVector<unsigned>>\n-  emitOffsetForLayout(const Attribute &layout, ArrayRef<int64_t> shape) const {\n+  emitOffsetForLayout(Attribute layout, RankedTensorType type) const {\n     if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n-      return emitOffsetForBlockedLayout(blockedLayout, shape);\n+      return emitOffsetForBlockedLayout(blockedLayout, type);\n     if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n       if (mmaLayout.isVolta())\n-        return emitOffsetForMmaLayoutV1(mmaLayout, shape);\n+        return emitOffsetForMmaLayoutV1(mmaLayout, type);\n       if (mmaLayout.isAmpere())\n-        return emitOffsetForMmaLayoutV2(mmaLayout, shape);\n+        return emitOffsetForMmaLayoutV2(mmaLayout, type);\n     }\n+    if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>())\n+      return emitOffsetForSliceLayout(sliceLayout, type);\n     llvm_unreachable(\"unsupported emitOffsetForLayout\");\n   }\n \n@@ -547,31 +653,33 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // -----------------------------------------------------------------------\n   SmallVector<SmallVector<Value>> emitIndices(Location loc,\n                                               ConversionPatternRewriter &b,\n-                                              const Attribute &layout,\n-                                              ArrayRef<int64_t> shape) const {\n-    IndexCacheKeyT key(layout, llvm::to_vector(shape));\n+                                              Attribute layout,\n+                                              RankedTensorType type) const {\n+    IndexCacheKeyT key(layout, type);\n     auto cache = indexCacheInfo.indexCache;\n-    assert(cache && \"indexCache is nullptr\");\n     auto insertPt = indexCacheInfo.indexInsertPoint;\n-    if (cache->count(key) > 0) {\n+    if (cache && cache->count(key) > 0) {\n       return cache->lookup(key);\n     } else {\n       ConversionPatternRewriter::InsertionGuard guard(b);\n-      restoreInsertionPointIfSet(insertPt, b);\n+      if (cache)\n+        restoreInsertionPointIfSet(insertPt, b);\n       SmallVector<SmallVector<Value>> result;\n       if (auto blocked = layout.dyn_cast<BlockedEncodingAttr>()) {\n-        result = emitIndicesForDistributedLayout(loc, b, blocked, shape);\n+        result = emitIndicesForDistributedLayout(loc, b, blocked, type);\n       } else if (auto mma = layout.dyn_cast<MmaEncodingAttr>()) {\n-        result = emitIndicesForDistributedLayout(loc, b, mma, shape);\n+        result = emitIndicesForDistributedLayout(loc, b, mma, type);\n       } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n-        result = emitIndicesForSliceLayout(loc, b, slice, shape);\n+        result = emitIndicesForDistributedLayout(loc, b, slice, type);\n       } else {\n         llvm_unreachable(\n             \"emitIndices for layouts other than blocked & slice not \"\n             \"implemented yet\");\n       }\n-      cache->insert(std::make_pair(key, result));\n-      *insertPt = b.saveInsertionPoint();\n+      if (cache) {\n+        cache->insert(std::make_pair(key, result));\n+        *insertPt = b.saveInsertionPoint();\n+      }\n       return result;\n     }\n   }\n@@ -593,13 +701,12 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // -----------------------------------------------------------------------\n \n   // Get an index-base for each dimension for a \\param blocked_layout.\n-  SmallVector<Value>\n-  emitBaseIndexForBlockedLayout(Location loc,\n-                                ConversionPatternRewriter &rewriter,\n-                                const BlockedEncodingAttr &blocked_layout,\n-                                ArrayRef<int64_t> shape) const {\n+  SmallVector<Value> emitBaseIndexForBlockedLayout(\n+      Location loc, ConversionPatternRewriter &rewriter,\n+      const BlockedEncodingAttr &blocked_layout, RankedTensorType type) const {\n+    auto shape = type.getShape();\n     Value threadId = getThreadId(rewriter, loc);\n-    Value warpSize = idx_val(32);\n+    Value warpSize = i32_val(32);\n     Value laneId = urem(threadId, warpSize);\n     Value warpId = udiv(threadId, warpSize);\n     auto sizePerThread = blocked_layout.getSizePerThread();\n@@ -616,18 +723,18 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n     SmallVector<Value> multiDimBase(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n-      // Wrap around multiDimWarpId/multiDimThreadId incase\n+      // Wrap around multiDimWarpId/multiDimThreadId in case\n       // shape[k] > shapePerCTA[k]\n       auto maxWarps =\n           ceil<unsigned>(shape[k], sizePerThread[k] * threadsPerWarp[k]);\n       auto maxThreads = ceil<unsigned>(shape[k], sizePerThread[k]);\n-      multiDimWarpId[k] = urem(multiDimWarpId[k], idx_val(maxWarps));\n-      multiDimThreadId[k] = urem(multiDimThreadId[k], idx_val(maxThreads));\n+      multiDimWarpId[k] = urem(multiDimWarpId[k], i32_val(maxWarps));\n+      multiDimThreadId[k] = urem(multiDimThreadId[k], i32_val(maxThreads));\n       // multiDimBase[k] = (multiDimThreadId[k] +\n       //                    multiDimWarpId[k] * threadsPerWarp[k]) *\n       //                   sizePerThread[k];\n-      Value threadsPerWarpK = idx_val(threadsPerWarp[k]);\n-      Value sizePerThreadK = idx_val(sizePerThread[k]);\n+      Value threadsPerWarpK = i32_val(threadsPerWarp[k]);\n+      Value sizePerThreadK = i32_val(sizePerThread[k]);\n       multiDimBase[k] =\n           mul(sizePerThreadK, add(multiDimThreadId[k],\n                                   mul(multiDimWarpId[k], threadsPerWarpK)));\n@@ -637,7 +744,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n   SmallVector<SmallVector<unsigned>>\n   emitOffsetForBlockedLayout(const BlockedEncodingAttr &blockedLayout,\n-                             ArrayRef<int64_t> shape) const {\n+                             RankedTensorType type) const {\n+    auto shape = type.getShape();\n     auto sizePerThread = blockedLayout.getSizePerThread();\n     auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n     auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n@@ -666,7 +774,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                                   threadOffset * sizePerThread[k] + elemOffset);\n     }\n \n-    unsigned elemsPerThread = blockedLayout.getElemsPerThread(shape);\n+    unsigned elemsPerThread = triton::gpu::getTotalElemsPerThread(type);\n     unsigned totalSizePerThread = product<unsigned>(sizePerThread);\n     SmallVector<SmallVector<unsigned>> reorderedOffset(elemsPerThread);\n     for (unsigned n = 0; n < elemsPerThread; ++n) {\n@@ -694,59 +802,155 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   SmallVector<Value>\n   emitBaseIndexForMmaLayoutV1(Location loc, ConversionPatternRewriter &rewriter,\n                               const MmaEncodingAttr &mmaLayout,\n-                              ArrayRef<int64_t> shape) const {\n-    llvm_unreachable(\"emitIndicesForMmaLayoutV1 not implemented\");\n+                              RankedTensorType type) const {\n+    auto shape = type.getShape();\n+\n+    auto wpt = mmaLayout.getWarpsPerCTA();\n+    static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n+    auto [isARow, isBRow, isAVec4, isBVec4, _] =\n+        mmaLayout.decodeVoltaLayoutStates();\n+\n+    Value thread = getThreadId(rewriter, loc);\n+    auto *ctx = thread.getContext();\n+    Value _1 = i32_val(1);\n+    Value _2 = i32_val(2);\n+    Value _4 = i32_val(4);\n+    Value _16 = i32_val(16);\n+    Value _32 = i32_val(32);\n+    Value _fpw0 = i32_val(fpw[0]);\n+    Value _fpw1 = i32_val(fpw[1]);\n+\n+    // A info\n+    auto aEncoding = DotOperandEncodingAttr::get(ctx, 0, mmaLayout, 0);\n+    auto aRep = aEncoding.getMMAv1Rep();\n+    auto aSpw = aEncoding.getMMAv1ShapePerWarp();\n+    // B info\n+    auto bEncoding = DotOperandEncodingAttr::get(ctx, 1, mmaLayout, 0);\n+    auto bSpw = bEncoding.getMMAv1ShapePerWarp();\n+    auto bRep = bEncoding.getMMAv1Rep();\n+\n+    SmallVector<int, 2> rep({aRep[0], bRep[1]});\n+    SmallVector<int, 2> spw({aSpw[0], bSpw[1]});\n+    SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n+\n+    Value lane = urem(thread, _32);\n+    Value warp = udiv(thread, _32);\n+\n+    Value warp0 = urem(warp, i32_val(wpt[0]));\n+    Value warp12 = udiv(warp, i32_val(wpt[0]));\n+    Value warp1 = urem(warp12, i32_val(wpt[1]));\n+\n+    // warp offset\n+    Value offWarpM = mul(warp0, i32_val(spw[0]));\n+    Value offWarpN = mul(warp1, i32_val(spw[1]));\n+    // quad offset\n+    Value offQuadM = mul(udiv(and_(lane, _16), _4), _fpw0);\n+    Value offQuadN = mul(udiv(and_(lane, _16), _4), _fpw1);\n+    // pair offset\n+    Value offPairM = udiv(urem(lane, _16), _4);\n+    offPairM = urem(offPairM, _fpw0);\n+    offPairM = mul(offPairM, _4);\n+    Value offPairN = udiv(urem(lane, _16), _4);\n+    offPairN = udiv(offPairN, _fpw0);\n+    offPairN = urem(offPairN, _fpw1);\n+    offPairN = mul(offPairN, _4);\n+    offPairM = mul(offPairM, i32_val(rep[0] / 2));\n+    offQuadM = mul(offQuadM, i32_val(rep[0] / 2));\n+    offPairN = mul(offPairN, i32_val(rep[1] / 2));\n+    offQuadN = mul(offQuadN, i32_val(rep[1] / 2));\n+    // quad pair offset\n+    Value offLaneM = add(offPairM, offQuadM);\n+    Value offLaneN = add(offPairN, offQuadN);\n+    // a, b offset\n+    Value offsetAM = add(offWarpM, offLaneM);\n+    Value offsetBN = add(offWarpN, offLaneN);\n+    // m indices\n+    Value offsetCM = add(and_(lane, _1), offsetAM);\n+    // n indices\n+    Value offsetCN = add((and_(lane, _2)), (add(offWarpN, offPairN)));\n+    return {offsetCM, offsetCN};\n   }\n \n   SmallVector<SmallVector<unsigned>>\n   emitOffsetForMmaLayoutV1(const MmaEncodingAttr &mmaLayout,\n-                           ArrayRef<int64_t> shape) const {\n-    SmallVector<SmallVector<unsigned>> ret;\n-\n-    for (unsigned i = 0; i < shape[0];\n-         i += getShapePerCTA(mmaLayout, shape)[0]) {\n-      for (unsigned j = 0; j < shape[1];\n-           j += getShapePerCTA(mmaLayout, shape)[1]) {\n-        ret.push_back({i, j});\n-        ret.push_back({i, j + 1});\n-        ret.push_back({i + 2, j});\n-        ret.push_back({i + 2, j + 1});\n-        ret.push_back({i, j + 8});\n-        ret.push_back({i, j + 9});\n-        ret.push_back({i + 2, j + 8});\n-        ret.push_back({i + 2, j + 9});\n+                           RankedTensorType type) const {\n+    auto shape = type.getShape();\n+\n+    auto [isARow, isBRow, isAVec4, isBVec4, _] =\n+        mmaLayout.decodeVoltaLayoutStates();\n+\n+    // TODO: seems like the apttern below to get `rep`/`spw` appears quite often\n+    // A info\n+    auto aEncoding =\n+        DotOperandEncodingAttr::get(type.getContext(), 0, mmaLayout, 0);\n+    auto aRep = aEncoding.getMMAv1Rep();\n+    auto aSpw = aEncoding.getMMAv1ShapePerWarp();\n+    // B info\n+    auto bEncoding =\n+        DotOperandEncodingAttr::get(type.getContext(), 1, mmaLayout, 0);\n+    auto bSpw = bEncoding.getMMAv1ShapePerWarp();\n+    auto bRep = bEncoding.getMMAv1Rep();\n+\n+    auto wpt = mmaLayout.getWarpsPerCTA();\n+    static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n+    SmallVector<int, 2> rep({aRep[0], bRep[1]});\n+    SmallVector<int, 2> spw({aSpw[0], bSpw[1]});\n+    SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n+\n+    SmallVector<unsigned> idxM;\n+    for (unsigned m = 0; m < shape[0]; m += shapePerCTA[0])\n+      for (unsigned mm = 0; mm < rep[0]; ++mm)\n+        idxM.push_back(m + mm * 2);\n+\n+    SmallVector<unsigned> idxN;\n+    for (int n = 0; n < shape[1]; n += shapePerCTA[1]) {\n+      for (int nn = 0; nn < rep[1]; ++nn) {\n+        idxN.push_back(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1]);\n+        idxN.push_back(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1] + 1);\n       }\n     }\n \n+    SmallVector<SmallVector<unsigned>> ret;\n+    for (unsigned x1 : idxN) {   // N\n+      for (unsigned x0 : idxM) { // M\n+        SmallVector<unsigned> idx(2);\n+        idx[0] = x0; // M\n+        idx[1] = x1; // N\n+        ret.push_back(std::move(idx));\n+      }\n+    }\n     return ret;\n   }\n \n   SmallVector<Value>\n   emitBaseIndexForMmaLayoutV2(Location loc, ConversionPatternRewriter &rewriter,\n                               const MmaEncodingAttr &mmaLayout,\n-                              ArrayRef<int64_t> shape) const {\n+                              RankedTensorType type) const {\n+    auto shape = type.getShape();\n     auto _warpsPerCTA = mmaLayout.getWarpsPerCTA();\n     assert(_warpsPerCTA.size() == 2);\n-    SmallVector<Value> warpsPerCTA = {idx_val(_warpsPerCTA[0]),\n-                                      idx_val(_warpsPerCTA[1])};\n+    SmallVector<Value> warpsPerCTA = {i32_val(_warpsPerCTA[0]),\n+                                      i32_val(_warpsPerCTA[1])};\n     Value threadId = getThreadId(rewriter, loc);\n-    Value warpSize = idx_val(32);\n+    Value warpSize = i32_val(32);\n     Value laneId = urem(threadId, warpSize);\n     Value warpId = udiv(threadId, warpSize);\n-    Value warpId0 = urem(warpId, warpsPerCTA[0]);\n-    Value warpId1 = urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]);\n-    Value offWarp0 = mul(warpId0, idx_val(16));\n-    Value offWarp1 = mul(warpId1, idx_val(8));\n+    Value warpId0 = urem(urem(warpId, warpsPerCTA[0]), i32_val(shape[0] / 16));\n+    Value warpId1 = urem(urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]),\n+                         i32_val(shape[1] / 8));\n+    Value offWarp0 = mul(warpId0, i32_val(16));\n+    Value offWarp1 = mul(warpId1, i32_val(8));\n \n     SmallVector<Value> multiDimBase(2);\n-    multiDimBase[0] = add(udiv(laneId, idx_val(4)), offWarp0);\n-    multiDimBase[1] = add(mul(idx_val(2), urem(laneId, idx_val(4))), offWarp1);\n+    multiDimBase[0] = add(udiv(laneId, i32_val(4)), offWarp0);\n+    multiDimBase[1] = add(mul(i32_val(2), urem(laneId, i32_val(4))), offWarp1);\n     return multiDimBase;\n   }\n \n   SmallVector<SmallVector<unsigned>>\n   emitOffsetForMmaLayoutV2(const MmaEncodingAttr &mmaLayout,\n-                           ArrayRef<int64_t> shape) const {\n+                           RankedTensorType type) const {\n+    auto shape = type.getShape();\n     SmallVector<SmallVector<unsigned>> ret;\n \n     for (unsigned i = 0; i < shape[0]; i += getShapePerCTA(mmaLayout)[0]) {\n@@ -762,57 +966,54 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n   // Emit indices calculation within each ConversionPattern, and returns a\n   // [elemsPerThread X rank] index matrix.\n-\n-  // TODO: [phil] redundant indices computation do not appear to hurt\n-  // performance much, but they could still significantly slow down\n-  // computations.\n   SmallVector<SmallVector<Value>> emitIndicesForDistributedLayout(\n-      Location loc, ConversionPatternRewriter &rewriter,\n-      const Attribute &layout, ArrayRef<int64_t> shape) const {\n-    if (auto mmaLayout = layout.template dyn_cast<MmaEncodingAttr>()) {\n-      assert(!mmaLayout.isVolta());\n-    }\n-\n+      Location loc, ConversionPatternRewriter &rewriter, Attribute layout,\n+      RankedTensorType type) const {\n     // step 1, delinearize threadId to get the base index\n-    auto multiDimBase = emitBaseIndexForLayout(loc, rewriter, layout, shape);\n+    auto multiDimBase = emitBaseIndexForLayout(loc, rewriter, layout, type);\n     // step 2, get offset of each element\n-    auto offset = emitOffsetForLayout(layout, shape);\n+    auto offset = emitOffsetForLayout(layout, type);\n     // step 3, add offset to base, and reorder the sequence of indices to\n     // guarantee that elems in the same sizePerThread are adjacent in order\n+    auto shape = type.getShape();\n     unsigned rank = shape.size();\n     unsigned elemsPerThread = offset.size();\n     SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread,\n                                                 SmallVector<Value>(rank));\n     for (unsigned n = 0; n < elemsPerThread; ++n)\n       for (unsigned k = 0; k < rank; ++k)\n-        multiDimIdx[n][k] = add(multiDimBase[k], idx_val(offset[n][k]));\n-\n+        multiDimIdx[n][k] = add(multiDimBase[k], i32_val(offset[n][k]));\n     return multiDimIdx;\n   }\n \n-  SmallVector<SmallVector<Value>>\n-  emitIndicesForSliceLayout(Location loc, ConversionPatternRewriter &rewriter,\n-                            const SliceEncodingAttr &sliceLayout,\n-                            ArrayRef<int64_t> shape) const {\n-    auto parent = sliceLayout.getParent();\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForSliceLayout(const SliceEncodingAttr &sliceLayout,\n+                           RankedTensorType type) const {\n+    auto parentEncoding = sliceLayout.getParent();\n     unsigned dim = sliceLayout.getDim();\n-    size_t rank = shape.size();\n-    auto parentIndices =\n-        emitIndices(loc, rewriter, parent, sliceLayout.paddedShape(shape));\n-    unsigned numIndices = parentIndices.size();\n-    SmallVector<SmallVector<Value>> resultIndices;\n-    for (unsigned i = 0; i < numIndices; ++i) {\n-      SmallVector<Value> indices = parentIndices[i];\n-      indices.erase(indices.begin() + dim);\n-      resultIndices.push_back(indices);\n+    auto parentShape = sliceLayout.paddedShape(type.getShape());\n+    RankedTensorType parentTy = RankedTensorType::get(\n+        parentShape, type.getElementType(), parentEncoding);\n+    auto parentOffsets = emitOffsetForLayout(parentEncoding, parentTy);\n+\n+    unsigned numOffsets = parentOffsets.size();\n+    SmallVector<SmallVector<unsigned>> resultOffsets;\n+    std::set<SmallVector<unsigned>> uniqueOffsets;\n+\n+    for (unsigned i = 0; i < numOffsets; ++i) {\n+      SmallVector<unsigned> offsets = parentOffsets[i];\n+      offsets.erase(offsets.begin() + dim);\n+      if (uniqueOffsets.find(offsets) == uniqueOffsets.end()) {\n+        resultOffsets.push_back(offsets);\n+        uniqueOffsets.insert(offsets);\n+      }\n     }\n-    return resultIndices;\n+    return resultOffsets;\n   }\n \n protected:\n-  LLVMTypeConverter *converter;\n-  const Allocation *allocation;\n-  Value smem;\n+  TritonGPUToLLVMTypeConverter *converter;\n+  ModuleAllocation *allocation;\n   IndexCacheInfo indexCacheInfo;\n };\n \n@@ -823,30 +1024,35 @@ class ConvertTritonGPUOpToLLVMPattern\n public:\n   using OpAdaptor = typename SourceOp::Adaptor;\n \n-  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n-                                           PatternBenefit benefit = 1)\n+  explicit ConvertTritonGPUOpToLLVMPattern(\n+      TritonGPUToLLVMTypeConverter &typeConverter, PatternBenefit benefit = 1)\n       : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n         ConvertTritonGPUOpToLLVMPatternBase(typeConverter) {}\n \n-  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n-                                           const Allocation *allocation,\n-                                           Value smem,\n-                                           PatternBenefit benefit = 1)\n+  explicit ConvertTritonGPUOpToLLVMPattern(\n+      TritonGPUToLLVMTypeConverter &typeConverter,\n+      IndexCacheInfo indexCacheInfo, PatternBenefit benefit = 1)\n+      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, indexCacheInfo) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPattern(\n+      TritonGPUToLLVMTypeConverter &typeConverter, ModuleAllocation &allocation,\n+      PatternBenefit benefit = 1)\n       : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n-        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation, smem) {}\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation) {}\n \n-  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n-                                           const Allocation *allocation,\n-                                           Value smem,\n-                                           IndexCacheInfo indexCacheInfo,\n-                                           PatternBenefit benefit = 1)\n+  explicit ConvertTritonGPUOpToLLVMPattern(\n+      TritonGPUToLLVMTypeConverter &typeConverter, ModuleAllocation &allocation,\n+      IndexCacheInfo indexCacheInfo, PatternBenefit benefit = 1)\n       : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n-        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation, smem,\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation,\n                                             indexCacheInfo) {}\n \n protected:\n-  LLVMTypeConverter *getTypeConverter() const {\n-    return ((ConvertTritonGPUOpToLLVMPatternBase *)this)->getTypeConverter();\n+  TritonGPUToLLVMTypeConverter *getTypeConverter() const {\n+    LLVMTypeConverter *ret =\n+        ((ConvertTritonGPUOpToLLVMPatternBase *)this)->getTypeConverter();\n+    return (TritonGPUToLLVMTypeConverter *)ret;\n   }\n };\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 351, "deletions": 165, "changes": 516, "file_content_changes": "@@ -1,18 +1,25 @@\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n \n-#include \"mlir/Conversion/ArithmeticToLLVM/ArithmeticToLLVM.h\"\n+#include \"mlir/Analysis/DataFlowFramework.h\"\n+#include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n+#include \"mlir/Conversion/ControlFlowToLLVM/ControlFlowToLLVM.h\"\n #include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n+#include \"mlir/Conversion/GPUToROCDL/GPUToROCDLPass.h\"\n+#include \"mlir/Conversion/LLVMCommon/VectorPattern.h\"\n #include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n-#include \"mlir/Conversion/SCFToStandard/SCFToStandard.h\"\n-#include \"mlir/Conversion/StandardToLLVM/ConvertStandardToLLVM.h\"\n+#include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n+#include \"mlir/Dialect/Index/IR/IndexDialect.h\"\n+#include \"mlir/Dialect/Index/IR/IndexOps.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"mlir/Dialect/LLVMIR/ROCDLDialect.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"triton/Analysis/Allocation.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n #include \"triton/Analysis/Membar.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Tools/Sys/GetPlatform.hpp\"\n \n #include \"ConvertLayoutOpToLLVM.h\"\n #include \"DotOpToLLVM.h\"\n@@ -23,215 +30,380 @@\n #include \"TypeConverter.h\"\n #include \"ViewOpToLLVM.h\"\n \n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n+\n using namespace mlir;\n using namespace mlir::triton;\n \n #define GEN_PASS_CLASSES\n-#include \"triton/Conversion/Passes.h.inc\"\n-\n-namespace mlir {\n+#include \"triton/Conversion/TritonGPUToLLVM/Passes.h.inc\"\n \n-class TritonLLVMConversionTarget : public ConversionTarget {\n-public:\n-  explicit TritonLLVMConversionTarget(MLIRContext &ctx)\n-      : ConversionTarget(ctx) {\n-    addLegalDialect<LLVM::LLVMDialect>();\n-    addLegalDialect<NVVM::NVVMDialect>();\n-    addIllegalDialect<triton::TritonDialect>();\n-    addIllegalDialect<triton::gpu::TritonGPUDialect>();\n-    addIllegalDialect<mlir::gpu::GPUDialect>();\n-    addIllegalDialect<mlir::StandardOpsDialect>();\n-    addLegalOp<mlir::UnrealizedConversionCastOp>();\n-  }\n-};\n+namespace {\n \n class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n public:\n-  explicit TritonLLVMFunctionConversionTarget(MLIRContext &ctx)\n+  explicit TritonLLVMFunctionConversionTarget(MLIRContext &ctx, bool isROCM)\n       : ConversionTarget(ctx) {\n+    addLegalDialect<index::IndexDialect>();\n     addLegalDialect<LLVM::LLVMDialect>();\n-    addLegalDialect<NVVM::NVVMDialect>();\n-    addIllegalOp<mlir::FuncOp>();\n+    if (isROCM) {\n+      addLegalDialect<ROCDL::ROCDLDialect>();\n+    } else {\n+      addLegalDialect<NVVM::NVVMDialect>();\n+    }\n     addLegalOp<mlir::UnrealizedConversionCastOp>();\n   }\n };\n \n-} // namespace mlir\n+struct ReturnOpConversion : public ConvertOpToLLVMPattern<triton::ReturnOp> {\n+  using ConvertOpToLLVMPattern<triton::ReturnOp>::ConvertOpToLLVMPattern;\n \n-namespace {\n+  LogicalResult\n+  matchAndRewrite(triton::ReturnOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto funcOp = op->getParentOfType<LLVM::LLVMFuncOp>();\n+    if (funcOp->hasAttr(\"nvvm.kernel\")) {\n+      // A GPU kernel\n+      if (op.getNumOperands() > 0) {\n+        return rewriter.notifyMatchFailure(\n+            op, \"Kernel functions do not support return with operands\");\n+      }\n+      rewriter.replaceOpWithNewOp<LLVM::ReturnOp>(op, TypeRange(), ValueRange(),\n+                                                  op->getAttrs());\n+    } else {\n+      // A device function\n+      LLVM::ReturnOp newOp;\n+      if (adaptor.getOperands().size() < 2) {\n+        // Single or no return value.\n+        newOp =\n+            rewriter.create<LLVM::ReturnOp>(op.getLoc(), adaptor.getOperands());\n+      } else {\n+        // Pack the results into a struct.\n+        auto packedResultsTy = this->getTypeConverter()->packFunctionResults(\n+            funcOp.getResultTypes());\n+        Value packedResults =\n+            rewriter.create<LLVM::UndefOp>(op.getLoc(), packedResultsTy);\n+        auto loc = op.getLoc();\n+        for (auto it : llvm::enumerate(adaptor.getOperands())) {\n+          packedResults = insert_val(packedResultsTy, packedResults, it.value(),\n+                                     it.index());\n+        }\n+        newOp = rewriter.create<LLVM::ReturnOp>(op.getLoc(), packedResults);\n+      }\n+      newOp->setAttrs(op->getAttrs());\n+      rewriter.replaceOp(op, newOp->getResults());\n+    }\n+    return success();\n+  }\n+};\n \n /// FuncOp legalization pattern that converts MemRef arguments to pointers to\n /// MemRef descriptors (LLVM struct data types) containing all the MemRef type\n /// information.\n struct FuncOpConversion : public FuncOpConversionBase {\n   FuncOpConversion(LLVMTypeConverter &converter, int numWarps,\n-                   PatternBenefit benefit)\n-      : FuncOpConversionBase(converter, benefit), numWarps(numWarps) {}\n+                   ModuleAllocation &allocation, PatternBenefit benefit)\n+      : FuncOpConversionBase(converter, benefit), numWarps(numWarps),\n+        allocation(allocation) {}\n+\n+  triton::FuncOp amendFuncOp(triton::FuncOp funcOp,\n+                             ConversionPatternRewriter &rewriter) const {\n+    // Push back a variable that indicates the current stack pointer of shared\n+    // memory to the function arguments.\n+    auto loc = funcOp.getLoc();\n+    auto ctx = funcOp->getContext();\n+    auto ptrTy = LLVM::LLVMPointerType::get(\n+        this->getTypeConverter()->convertType(rewriter.getI8Type()), 3);\n+    // 1. Modify the function type to add the new argument.\n+    auto funcTy = funcOp.getFunctionType();\n+    auto amendedInputTy = llvm::to_vector<4>(funcTy.getInputs());\n+    amendedInputTy.push_back(ptrTy);\n+    auto amendedFuncTy = FunctionType::get(funcTy.getContext(), amendedInputTy,\n+                                           funcTy.getResults());\n+    // 2. Modify the argument attributes to add the new argument.\n+    SmallVector<NamedAttribute> amendedAttrs;\n+    filterFuncAttributes(funcOp, /*filterArgAttrs=*/true, amendedAttrs);\n+    auto amendedArgAttrs = llvm::to_vector<4>(funcOp.getAllArgAttrs());\n+    amendedArgAttrs.emplace_back(DictionaryAttr::get(ctx));\n+    amendedAttrs.push_back(rewriter.getNamedAttr(\n+        funcOp.getArgAttrsAttrName(), rewriter.getArrayAttr(amendedArgAttrs)));\n+    // 3. Add a new argument to the region\n+    auto amendedFuncOp = rewriter.create<triton::FuncOp>(\n+        funcOp.getLoc(), funcOp.getName(), amendedFuncTy, amendedAttrs);\n+    auto &region = funcOp.getBody();\n+    region.addArgument(ptrTy, loc);\n+    rewriter.inlineRegionBefore(region, amendedFuncOp.getBody(),\n+                                amendedFuncOp.end());\n+    return amendedFuncOp;\n+  }\n \n   LogicalResult\n-  matchAndRewrite(FuncOp funcOp, OpAdaptor adaptor,\n+  matchAndRewrite(triton::FuncOp funcOp, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto newFuncOp = convertFuncOpToLLVMFuncOp(funcOp, rewriter);\n-    if (!newFuncOp)\n+    // Prevent LLVM's inliner to inline this function\n+    auto amendedFuncOp = funcOp;\n+    if (!allocation.isRoot(funcOp))\n+      amendedFuncOp = amendFuncOp(funcOp, rewriter);\n+\n+    auto newFuncOp = convertFuncOpToLLVMFuncOp(amendedFuncOp, rewriter);\n+    if (!newFuncOp) {\n       return failure();\n+    }\n \n     auto ctx = funcOp->getContext();\n \n-    // Set an attribute to indicate this function is a kernel entry.\n-    newFuncOp->setAttr(\"nvvm.kernel\",\n-                       rewriter.getIntegerAttr(type::u1Ty(ctx), 1));\n-\n+    if (allocation.isRoot(funcOp)) {\n+      // Set an attribute to indicate this function is a kernel entry.\n+      newFuncOp->setAttr(\"nvvm.kernel\",\n+                         rewriter.getIntegerAttr(type::u1Ty(ctx), 1));\n+    } else {\n+      // The noinline attribute will be used by the LLVM codegen to prevent\n+      // inlining.\n+      // https://github.com/llvm/llvm-project/blob/main/mlir/lib/Dialect/LLVMIR/IR/LLVMInlining.cpp#L267\n+      newFuncOp.setPassthroughAttr(\n+          ArrayAttr::get(ctx, rewriter.getStringAttr(\"noinline\")));\n+      rewriter.eraseOp(amendedFuncOp);\n+    }\n     // Set an attribute for maxntidx, it could be used in latter LLVM codegen\n     // for `nvvm.annotation` metadata.\n-    newFuncOp->setAttr(\"nvvm.maxntid\",\n-                       rewriter.getIntegerAttr(i32_ty, 32 * numWarps));\n+    newFuncOp->setAttr(\"nvvm.maxntid\", rewriter.getI32ArrayAttr(32 * numWarps));\n+    // The call graph is updated by mapping the old function to the new one.\n+    allocation.mapFuncOp(funcOp, newFuncOp);\n \n     rewriter.eraseOp(funcOp);\n     return success();\n   }\n \n private:\n   int numWarps{0};\n+  ModuleAllocation &allocation;\n+};\n+\n+// CallOpInterfaceLowering is adapted from\n+// https://github.com/llvm/llvm-project/blob/fae656b2dd80246c3c6f01e9c77c49560368752c/mlir/lib/Conversion/FuncToLLVM/FuncToLLVM.cpp#L485\n+struct CallOpConversion : public ConvertOpToLLVMPattern<triton::CallOp> {\n+  CallOpConversion(LLVMTypeConverter &converter, int numWarps,\n+                   ModuleAllocation &allocation, PatternBenefit benefit)\n+      : ConvertOpToLLVMPattern<triton::CallOp>(converter, benefit),\n+        numWarps(numWarps), allocation(allocation) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::CallOp callOp,\n+                  typename triton::CallOp::Adaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto promotedOperands = promoteOperands(callOp, adaptor, rewriter);\n+    auto newCallOp =\n+        convertCallOpToLLVMCallOp(callOp, promotedOperands, rewriter);\n+    if (!newCallOp)\n+      return failure();\n+    allocation.mapCallOp(callOp, newCallOp);\n+    auto results = getCallOpResults(callOp, newCallOp, rewriter);\n+    rewriter.replaceOp(callOp, results);\n+    return success();\n+  }\n+\n+private:\n+  SmallVector<Value, 4>\n+  promoteOperands(triton::CallOp callOp,\n+                  typename triton::CallOp::Adaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const {\n+    // Get the last argument of the caller, which is the current stack pointer\n+    // of shared memory and append it to the operands of the callOp.\n+    auto loc = callOp.getLoc();\n+    auto caller = callOp->getParentOfType<FunctionOpInterface>();\n+    auto base = allocation.getFunctionSharedMemoryBase(caller);\n+    auto *funcAllocation = allocation.getFuncData(caller);\n+    auto bufferId = funcAllocation->getBufferId(callOp);\n+    auto offset = funcAllocation->getOffset(bufferId);\n+    auto ptrTy = LLVM::LLVMPointerType::get(\n+        this->getTypeConverter()->convertType(rewriter.getI8Type()),\n+        NVVM::kSharedMemorySpace);\n+    auto offsetValue = gep(ptrTy, base, i32_val(offset));\n+    auto promotedOperands = this->getTypeConverter()->promoteOperands(\n+        callOp.getLoc(), /*opOperands=*/callOp->getOperands(),\n+        adaptor.getOperands(), rewriter);\n+    promotedOperands.push_back(offsetValue);\n+    return promotedOperands;\n+  }\n+\n+  LLVM::CallOp\n+  convertCallOpToLLVMCallOp(triton::CallOp callOp,\n+                            ArrayRef<Value> promotedOperands,\n+                            ConversionPatternRewriter &rewriter) const {\n+    // Pack the result types into a struct.\n+    Type packedResult = nullptr;\n+    unsigned numResults = callOp.getNumResults();\n+    auto resultTypes = llvm::to_vector<4>(callOp.getResultTypes());\n+\n+    if (numResults != 0) {\n+      if (!(packedResult =\n+                this->getTypeConverter()->packFunctionResults(resultTypes)))\n+        return nullptr;\n+    }\n+\n+    auto newCallOp = rewriter.create<LLVM::CallOp>(\n+        callOp.getLoc(), packedResult ? TypeRange(packedResult) : TypeRange(),\n+        promotedOperands, callOp->getAttrs());\n+    return newCallOp;\n+  }\n+\n+  SmallVector<Value>\n+  getCallOpResults(triton::CallOp callOp, LLVM::CallOp newCallOp,\n+                   ConversionPatternRewriter &rewriter) const {\n+    auto numResults = callOp.getNumResults();\n+    SmallVector<Value> results;\n+    if (numResults < 2) {\n+      // If < 2 results, packing did not do anything and we can just return.\n+      results.append(newCallOp.result_begin(), newCallOp.result_end());\n+    } else {\n+      // Otherwise, it had been converted to an operation producing a structure.\n+      // Extract individual results from the structure and return them as list.\n+      results.reserve(numResults);\n+      for (unsigned i = 0; i < numResults; ++i) {\n+        results.push_back(rewriter.create<LLVM::ExtractValueOp>(\n+            callOp.getLoc(), newCallOp->getResult(0), i));\n+      }\n+    }\n+    return results;\n+  }\n+\n+  int numWarps{0};\n+  ModuleAllocation &allocation;\n+};\n+\n+class TritonLLVMConversionTarget : public ConversionTarget {\n+public:\n+  explicit TritonLLVMConversionTarget(MLIRContext &ctx, bool isROCM)\n+      : ConversionTarget(ctx) {\n+    addLegalDialect<LLVM::LLVMDialect>();\n+    if (isROCM) {\n+      addLegalDialect<ROCDL::ROCDLDialect>();\n+    } else {\n+      addLegalDialect<NVVM::NVVMDialect>();\n+    }\n+    addIllegalDialect<triton::TritonDialect>();\n+    addIllegalDialect<triton::gpu::TritonGPUDialect>();\n+    addIllegalDialect<mlir::gpu::GPUDialect>();\n+    addLegalOp<mlir::UnrealizedConversionCastOp>();\n+  }\n };\n \n class ConvertTritonGPUToLLVM\n     : public ConvertTritonGPUToLLVMBase<ConvertTritonGPUToLLVM> {\n \n public:\n-  explicit ConvertTritonGPUToLLVM(int computeCapability)\n-      : computeCapability(computeCapability) {}\n+  explicit ConvertTritonGPUToLLVM(int computeCapability, bool isROCM)\n+      : computeCapability(computeCapability), isROCM(isROCM) {}\n \n   void runOnOperation() override {\n     MLIRContext *context = &getContext();\n     ModuleOp mod = getOperation();\n-\n     mlir::LowerToLLVMOptions option(context);\n     option.overrideIndexBitwidth(32);\n     TritonGPUToLLVMTypeConverter typeConverter(context, option);\n-    TritonLLVMFunctionConversionTarget funcTarget(*context);\n-    TritonLLVMConversionTarget target(*context);\n-\n+    TritonLLVMConversionTarget target(*context, isROCM);\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+    int threadsPerWarp = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n \n-    // Step 1: Decompose unoptimized layout conversions to use shared memory\n-    // Step 2: Decompose insert_slice_async to use load + insert_slice for\n-    //   pre-Ampere architectures or unsupported vectorized load sizes\n-    // Step 3: Allocate shared memories and insert barriers\n-    // Step 4: Convert SCF to CFG\n-    // Step 5: Convert FuncOp to LLVMFuncOp via partial conversion\n-    // Step 6: Get axis and shared memory info\n-    // Step 7: Convert the rest of ops via partial conversion\n-    //\n-    // The reason for putting step 3 before step 4 is that the membar\n-    // analysis currently only supports SCF but not CFG. The reason for a\n-    // separation between 5/7 is that, step 6 is out of the scope of Dialect\n-    // Conversion, thus we need to make sure the smem is not revised during the\n-    // conversion of step 7.\n-\n-    // Step 1\n-    decomposeMmaToDotOperand(mod, numWarps);\n+    // Preprocess\n+    decomposeMmaToDotOperand(mod, numWarps, threadsPerWarp);\n     decomposeBlockedToDotOperand(mod);\n-\n-    // Step 2\n     decomposeInsertSliceAsyncOp(mod);\n \n-    // Step 3\n-    Allocation allocation(mod);\n-    MembarAnalysis membarPass(&allocation);\n+    // Allocate shared memory and set barrier\n+    ModuleAllocation allocation(mod);\n+    ModuleMembarAnalysis membarPass(&allocation);\n     membarPass.run();\n \n-    // Step 4\n-    RewritePatternSet scf_patterns(context);\n-    mlir::populateLoopToStdConversionPatterns(scf_patterns);\n-    mlir::ConversionTarget scf_target(*context);\n-    scf_target.addIllegalOp<scf::ForOp, scf::IfOp, scf::ParallelOp,\n-                            scf::WhileOp, scf::ExecuteRegionOp>();\n-    scf_target.markUnknownOpDynamicallyLegal([](Operation *) { return true; });\n-    if (failed(\n-            applyPartialConversion(mod, scf_target, std::move(scf_patterns))))\n-      return signalPassFailure();\n-\n-    // Step 5\n-    RewritePatternSet func_patterns(context);\n-    func_patterns.add<FuncOpConversion>(typeConverter, numWarps, /*benefit=*/1);\n-    if (failed(\n-            applyPartialConversion(mod, funcTarget, std::move(func_patterns))))\n-      return signalPassFailure();\n-\n-    // Step 6 - get axis and shared memory info\n-    AxisInfoAnalysis axisInfoAnalysis(mod.getContext());\n-    axisInfoAnalysis.run(mod);\n-    initSharedMemory(allocation.getSharedMemorySize(), typeConverter);\n-    mod->setAttr(\"triton_gpu.shared\",\n-                 mlir::IntegerAttr::get(mlir::IntegerType::get(context, 32),\n-                                        allocation.getSharedMemorySize()));\n-\n-    // Step 7 - rewrite rest of ops\n-    // We set a higher benefit here to ensure triton's patterns runs before\n-    // arith patterns for some encoding not supported by the community\n-    // patterns.\n+    // Lower functions\n+    {\n+      mlir::LowerToLLVMOptions option(context);\n+      TritonGPUToLLVMTypeConverter typeConverter(context, option);\n+      TritonLLVMFunctionConversionTarget funcTarget(*context, isROCM);\n+      RewritePatternSet funcPatterns(context);\n+      funcPatterns.add<FuncOpConversion>(typeConverter, numWarps, allocation,\n+                                         /*benefit=*/1);\n+      mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n+                                                            funcPatterns);\n+      if (failed(\n+              applyPartialConversion(mod, funcTarget, std::move(funcPatterns))))\n+        return signalPassFailure();\n+    }\n+\n+    // initSharedMemory is run before the conversion of call and ret ops,\n+    // because the call op has to know the shared memory base address of each\n+    // function\n+    initSharedMemory(allocation, typeConverter);\n+\n+    // Convert call and ret ops\n+    {\n+      mlir::LowerToLLVMOptions option(context);\n+      TritonGPUToLLVMTypeConverter typeConverter(context, option);\n+      TritonLLVMFunctionConversionTarget funcTarget(*context, isROCM);\n+      RewritePatternSet funcPatterns(context);\n+      funcPatterns.add<CallOpConversion>(typeConverter, numWarps, allocation,\n+                                         /*benefit=*/1);\n+      funcPatterns.add<ReturnOpConversion>(typeConverter, /*benefit=*/1);\n+      if (failed(\n+              applyPartialConversion(mod, funcTarget, std::move(funcPatterns))))\n+        return signalPassFailure();\n+    }\n+\n+    ModuleAxisInfoAnalysis axisInfoAnalysis(mod);\n+    // Rewrite ops\n+    RewritePatternSet patterns(context);\n+    // TritonGPU lowering patterns\n     OpBuilder::InsertPoint indexInsertPoint;\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo indexCacheInfo{\n         &baseIndexCache, &indexCache, &indexInsertPoint};\n-\n-    RewritePatternSet patterns(context);\n-\n-    // Normal conversions\n-    populateTritonGPUToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                    axisInfoAnalysis, &allocation, smem,\n-                                    indexCacheInfo, /*benefit=*/10);\n-    // ConvertLayoutOp\n-    populateConvertLayoutOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                          axisInfoAnalysis, &allocation, smem,\n-                                          indexCacheInfo, /*benefit=*/10);\n-    // DotOp\n-    populateDotOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                axisInfoAnalysis, &allocation, smem,\n-                                /*benefit=*/10);\n-    // ElementwiseOp\n-    populateElementwiseOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                        axisInfoAnalysis, &allocation, smem,\n-                                        /*benefit=*/10);\n-    // LoadStoreOp\n-    populateLoadStoreOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                      axisInfoAnalysis, &allocation, smem,\n-                                      indexCacheInfo, /*benefit=*/10);\n-    // ReduceOp\n-    populateReduceOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                   axisInfoAnalysis, &allocation, smem,\n-                                   indexCacheInfo, /*benefit=*/10);\n-    // ViewOp\n-    populateViewOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                 axisInfoAnalysis, &allocation, smem,\n-                                 /*benefit=*/10);\n-\n-    // Add arith/math's patterns to help convert scalar expression to LLVM.\n-    mlir::arith::populateArithmeticToLLVMConversionPatterns(typeConverter,\n-                                                            patterns);\n-    mlir::populateMathToLLVMConversionPatterns(typeConverter, patterns);\n-    mlir::populateStdToLLVMConversionPatterns(typeConverter, patterns);\n-    mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n-\n+    // TODO: enable index cache if there are multiple functions\n+    if (axisInfoAnalysis.getNumFunctions() > 1) {\n+      indexCacheInfo = {nullptr, nullptr, nullptr};\n+    }\n+    populateTritonGPUToLLVMPatterns(typeConverter, patterns, allocation,\n+                                    indexCacheInfo, /*benefit=*/1);\n+    populateConvertLayoutOpToLLVMPatterns(typeConverter, patterns, allocation,\n+                                          indexCacheInfo, /*benefit=*/1);\n+    populateDotOpToLLVMPatterns(typeConverter, patterns, allocation,\n+                                /*benefit=*/1);\n+    populateElementwiseOpToLLVMPatterns(typeConverter, patterns, /*benefit=*/1);\n+    populateLoadStoreOpToLLVMPatterns(typeConverter, patterns, axisInfoAnalysis,\n+                                      allocation, indexCacheInfo,\n+                                      /*benefit=*/1);\n+    populateReduceOpToLLVMPatterns(typeConverter, patterns, allocation,\n+                                   indexCacheInfo, /*benefit=*/1);\n+    populateViewOpToLLVMPatterns(typeConverter, patterns, /*benefit=*/1);\n+\n+    // Native lowering patterns\n+    if (isROCM) {\n+      mlir::populateGpuToROCDLConversionPatterns(typeConverter, patterns,\n+                                                 mlir::gpu::amd::HIP);\n+    } else {\n+      mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n+    }\n+\n+    mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n+                                                          patterns);\n     if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n       return signalPassFailure();\n   }\n \n private:\n-  Value smem;\n-\n-  using IndexCacheKeyT = std::pair<Attribute, SmallVector<int64_t>>;\n+  using IndexCacheKeyT = std::pair<Attribute, RankedTensorType>;\n   DenseMap<IndexCacheKeyT, SmallVector<Value>, CacheKeyDenseMapInfo>\n       baseIndexCache;\n   DenseMap<IndexCacheKeyT, SmallVector<SmallVector<Value>>,\n            CacheKeyDenseMapInfo>\n       indexCache;\n \n   int computeCapability{};\n+  bool isROCM{};\n \n-  void initSharedMemory(size_t size,\n+  void initSharedMemory(ModuleAllocation &allocation,\n                         TritonGPUToLLVMTypeConverter &typeConverter) {\n     ModuleOp mod = getOperation();\n     OpBuilder b(mod.getBodyRegion());\n+    auto ctx = mod.getContext();\n     auto loc = mod.getLoc();\n     auto elemTy = typeConverter.convertType(b.getIntegerType(8));\n     // Set array size 0 and external linkage indicates that we use dynamic\n@@ -240,19 +412,29 @@ class ConvertTritonGPUToLLVM\n     auto global = b.create<LLVM::GlobalOp>(\n         loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::External,\n         \"global_smem\", /*value=*/Attribute(), /*alignment=*/0,\n-        mlir::gpu::GPUDialect::getWorkgroupAddressSpace());\n-    SmallVector<LLVM::LLVMFuncOp> funcs;\n-    mod.walk([&](LLVM::LLVMFuncOp func) { funcs.push_back(func); });\n-    assert(funcs.size() == 1 &&\n-           \"Inliner pass is expected before TritonGPUToLLVM\");\n-    b.setInsertionPointToStart(&funcs[0].getBody().front());\n-    smem = b.create<LLVM::AddressOfOp>(loc, global);\n-    auto ptrTy =\n-        LLVM::LLVMPointerType::get(typeConverter.convertType(b.getI8Type()), 3);\n-    smem = b.create<LLVM::BitcastOp>(loc, ptrTy, smem);\n+        // Add ROCm support.\n+        static_cast<unsigned>(NVVM::NVVMMemorySpace::kSharedMemorySpace));\n+    mod.walk([&](FunctionOpInterface funcOp) {\n+      Value funcSmem;\n+      b.setInsertionPointToStart(&funcOp.getFunctionBody().front());\n+      if (allocation.isRoot(funcOp)) {\n+        funcSmem = b.create<LLVM::AddressOfOp>(loc, global);\n+      } else {\n+        funcSmem = funcOp.getArgument(funcOp.getNumArguments() - 1);\n+      }\n+      auto ptrTy =\n+          LLVM::LLVMPointerType::get(typeConverter.convertType(b.getI8Type()),\n+                                     NVVM::NVVMMemorySpace::kSharedMemorySpace);\n+      funcSmem = b.create<LLVM::BitcastOp>(loc, ptrTy, funcSmem);\n+      allocation.setFunctionSharedMemoryValue(funcOp, funcSmem);\n+    });\n+    mod->setAttr(\"triton_gpu.shared\",\n+                 mlir::IntegerAttr::get(mlir::IntegerType::get(ctx, 32),\n+                                        allocation.getSharedMemorySize()));\n   }\n \n-  void decomposeMmaToDotOperand(ModuleOp mod, int numWarps) const {\n+  void decomposeMmaToDotOperand(ModuleOp mod, int numWarps,\n+                                int threadsPerWarp) const {\n     // Replace `mma -> dot_op` with `mma -> blocked -> dot_op`\n     // unless certain conditions are met\n     mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n@@ -263,12 +445,12 @@ class ConvertTritonGPUToLLVM\n           srcType.getEncoding().dyn_cast<triton::gpu::MmaEncodingAttr>();\n       auto dstDotOp =\n           dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n-      if (srcMma && dstDotOp && !isMmaToDotShortcut(srcMma, dstDotOp)) {\n+      if (srcMma && dstDotOp && !isMmaToDotShortcut(srcType, dstType)) {\n         auto tmpType = RankedTensorType::get(\n             dstType.getShape(), dstType.getElementType(),\n             triton::gpu::BlockedEncodingAttr::get(\n                 mod.getContext(), srcType.getShape(), getSizePerThread(srcMma),\n-                getOrder(srcMma), numWarps));\n+                getOrder(srcMma), numWarps, threadsPerWarp));\n         auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n             cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n         auto newConvert = builder.create<triton::gpu::ConvertLayoutOp>(\n@@ -307,8 +489,7 @@ class ConvertTritonGPUToLLVM\n   }\n \n   void decomposeInsertSliceAsyncOp(ModuleOp mod) const {\n-    AxisInfoAnalysis axisInfoAnalysis(mod.getContext());\n-    axisInfoAnalysis.run(mod);\n+    ModuleAxisInfoAnalysis axisInfoAnalysis(mod);\n     // TODO(Keren): This is a hacky knob that may cause performance regression\n     // when decomposition has been performed. We should remove this knob once we\n     // have thorough analysis on async wait. Currently, we decompose\n@@ -333,16 +514,20 @@ class ConvertTritonGPUToLLVM\n       OpBuilder builder(insertSliceAsyncOp);\n \n       // Get the vectorized load size\n-      auto src = insertSliceAsyncOp.src();\n-      auto dst = insertSliceAsyncOp.dst();\n+      auto src = insertSliceAsyncOp.getSrc();\n+      auto dst = insertSliceAsyncOp.getDst();\n+      auto mask = insertSliceAsyncOp.getMask();\n       auto srcTy = src.getType().cast<RankedTensorType>();\n       auto dstTy = dst.getType().cast<RankedTensorType>();\n       auto srcBlocked =\n           srcTy.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>();\n       auto resSharedLayout =\n           dstTy.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n       auto resElemTy = dstTy.getElementType();\n-      unsigned inVec = axisInfoAnalysis.getPtrVectorSize(src);\n+      unsigned inVec = axisInfoAnalysis.getPtrContiguity(src);\n+      if (mask)\n+        inVec =\n+            std::min<unsigned>(axisInfoAnalysis.getMaskAlignment(mask), inVec);\n       unsigned outVec = resSharedLayout.getVec();\n       unsigned minVec = std::min(outVec, inVec);\n       auto maxBitWidth =\n@@ -362,24 +547,26 @@ class ConvertTritonGPUToLLVM\n       auto tmpTy =\n           RankedTensorType::get(srcTy.getShape(), resElemTy, srcBlocked);\n       auto loadOp = builder.create<triton::LoadOp>(\n-          insertSliceAsyncOp.getLoc(), tmpTy, insertSliceAsyncOp.src(),\n-          insertSliceAsyncOp.mask(), insertSliceAsyncOp.other(),\n-          insertSliceAsyncOp.cache(), insertSliceAsyncOp.evict(),\n-          insertSliceAsyncOp.isVolatile());\n+          insertSliceAsyncOp.getLoc(), tmpTy, insertSliceAsyncOp.getSrc(),\n+          insertSliceAsyncOp.getMask(), insertSliceAsyncOp.getOther(),\n+          // TODO(Chenggang): confirm `boundaryCheck` and `padding`\n+          /*boundaryCheck=*/nullptr, /*padding=*/nullptr,\n+          insertSliceAsyncOp.getCache(), insertSliceAsyncOp.getEvict(),\n+          insertSliceAsyncOp.getIsVolatile());\n \n       // insert_slice\n-      auto axis = insertSliceAsyncOp.axis();\n+      auto axis = insertSliceAsyncOp.getAxis();\n       auto intAttr = [&](int64_t v) { return builder.getI64IntegerAttr(v); };\n       auto offsets = SmallVector<OpFoldResult>(dstTy.getRank(), intAttr(0));\n       auto sizes = SmallVector<OpFoldResult>(dstTy.getRank(), intAttr(1));\n       auto strides = SmallVector<OpFoldResult>(dstTy.getRank(), intAttr(1));\n-      offsets[axis] = insertSliceAsyncOp.index();\n+      offsets[axis] = insertSliceAsyncOp.getIndex();\n       for (size_t i = 0; i < dstTy.getRank(); i++) {\n         if (i != axis)\n           sizes[i] = intAttr(dstTy.getShape()[i]);\n       }\n       auto insertSliceOp = builder.create<tensor::InsertSliceOp>(\n-          insertSliceAsyncOp.getLoc(), loadOp, insertSliceAsyncOp.dst(),\n+          insertSliceAsyncOp.getLoc(), loadOp, insertSliceAsyncOp.getDst(),\n           offsets, sizes, strides);\n \n       // Replace\n@@ -400,8 +587,7 @@ class ConvertTritonGPUToLLVM\n       } else if (decomposed) {\n         // Wait for all previous async ops\n         OpBuilder builder(asyncWaitOp);\n-        auto newAsyncWaitOp =\n-            builder.create<triton::gpu::AsyncWaitOp>(asyncWaitOp.getLoc(), 0);\n+        builder.create<triton::gpu::AsyncWaitOp>(asyncWaitOp.getLoc(), 0);\n         asyncWaitOp.erase();\n       }\n     });\n@@ -414,8 +600,8 @@ namespace mlir {\n namespace triton {\n \n std::unique_ptr<OperationPass<ModuleOp>>\n-createConvertTritonGPUToLLVMPass(int computeCapability) {\n-  return std::make_unique<::ConvertTritonGPUToLLVM>(computeCapability);\n+createConvertTritonGPUToLLVMPass(int computeCapability, bool isROCM) {\n+  return std::make_unique<::ConvertTritonGPUToLLVM>(computeCapability, isROCM);\n }\n \n } // namespace triton"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "added", "additions": 141, "deletions": 0, "changes": 141, "file_content_changes": "@@ -0,0 +1,141 @@\n+#include \"TypeConverter.h\"\n+#include \"Utility.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"triton/Conversion/MLIRTypes.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getTotalElemsPerThread;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+using ::mlir::triton::gpu::SliceEncodingAttr;\n+\n+TritonGPUToLLVMTypeConverter::TritonGPUToLLVMTypeConverter(\n+    MLIRContext *ctx, LowerToLLVMOptions &option,\n+    const DataLayoutAnalysis *analysis)\n+    : LLVMTypeConverter(ctx, option, analysis) {\n+  addConversion([&](triton::PointerType type) -> std::optional<Type> {\n+    return convertTritonPointerType(type);\n+  });\n+  addConversion([&](RankedTensorType type) -> std::optional<Type> {\n+    return convertTritonTensorType(type);\n+  });\n+  // Internally store float8 as int8\n+  addConversion([&](mlir::Float8E4M3FNType type) -> std::optional<Type> {\n+    return IntegerType::get(type.getContext(), 8);\n+  });\n+  addConversion([&](mlir::Float8E5M2Type type) -> std::optional<Type> {\n+    return IntegerType::get(type.getContext(), 8);\n+  });\n+  // Internally store bfloat16 as int16\n+  addConversion([&](BFloat16Type type) -> std::optional<Type> {\n+    return IntegerType::get(type.getContext(), 16);\n+  });\n+}\n+\n+Type TritonGPUToLLVMTypeConverter::convertTritonPointerType(\n+    triton::PointerType type) {\n+  // Recursively translate pointee type\n+  return LLVM::LLVMPointerType::get(convertType(type.getPointeeType()),\n+                                    type.getAddressSpace());\n+}\n+\n+Value TritonGPUToLLVMTypeConverter::packLLElements(\n+    Location loc, ValueRange resultVals, ConversionPatternRewriter &rewriter,\n+    Type type) {\n+  auto structType = this->convertType(type).dyn_cast<LLVM::LLVMStructType>();\n+  if (!structType) {\n+    assert(resultVals.size() == 1);\n+    return *resultVals.begin();\n+  }\n+\n+  auto elementTypes = structType.getBody();\n+  if (elementTypes.size() != resultVals.size()) {\n+    emitError(loc) << \" size mismatch when packing elements for LLVM struct\"\n+                   << \" expected \" << elementTypes.size() << \" but got \"\n+                   << resultVals.size();\n+  }\n+  Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structType);\n+  for (const auto &v : llvm::enumerate(resultVals)) {\n+    if (!v.value()) {\n+      emitError(loc)\n+          << \"cannot insert null values into struct, but tried to insert\"\n+          << v.value();\n+    }\n+    if (v.value().getType() != elementTypes[v.index()]) {\n+      emitError(loc) << \"invalid element type in packLLEElements. Expected \"\n+                     << elementTypes[v.index()] << \" but got \"\n+                     << v.value().getType();\n+    }\n+    llvmStruct = insert_val(structType, llvmStruct, v.value(), v.index());\n+  }\n+  return llvmStruct;\n+}\n+\n+SmallVector<Value> TritonGPUToLLVMTypeConverter::unpackLLElements(\n+    Location loc, Value llvmStruct, ConversionPatternRewriter &rewriter,\n+    Type type) {\n+  assert(bool(llvmStruct) && \"can not unpack null values\");\n+  if (llvmStruct.getType().isIntOrIndexOrFloat() ||\n+      llvmStruct.getType().isa<triton::PointerType>() ||\n+      llvmStruct.getType().isa<LLVM::LLVMPointerType>())\n+    return {llvmStruct};\n+  ArrayRef<Type> types =\n+      llvmStruct.getType().cast<LLVM::LLVMStructType>().getBody();\n+  SmallVector<Value> results(types.size());\n+  for (unsigned i = 0; i < types.size(); ++i) {\n+    Type type = types[i];\n+    results[i] = extract_val(type, llvmStruct, i);\n+  }\n+  return results;\n+}\n+\n+Type TritonGPUToLLVMTypeConverter::getElementTypeForStruct(\n+    RankedTensorType type) {\n+  auto ctx = type.getContext();\n+  Attribute layout = type.getEncoding();\n+  Type elemTy = convertType(type.getElementType());\n+  auto dotOpLayout = layout.dyn_cast<DotOperandEncodingAttr>();\n+  if (!dotOpLayout)\n+    return elemTy;\n+  auto mmaParent = dotOpLayout.getParent().dyn_cast<MmaEncodingAttr>();\n+  if (!mmaParent)\n+    return elemTy;\n+  if (mmaParent.isAmpere()) {\n+    int bitwidth = elemTy.getIntOrFloatBitWidth();\n+    assert(bitwidth <= 32);\n+    return IntegerType::get(ctx, 32);\n+  } else {\n+    assert(mmaParent.isVolta());\n+    return vec_ty(elemTy, 2);\n+  }\n+}\n+\n+Type TritonGPUToLLVMTypeConverter::convertTritonTensorType(\n+    RankedTensorType type) {\n+  auto ctx = type.getContext();\n+  Attribute layout = type.getEncoding();\n+  SmallVector<int64_t> shape(type.getShape().begin(), type.getShape().end());\n+  Type eltType = getElementTypeForStruct(type);\n+\n+  if (auto shared_layout = layout.dyn_cast<SharedEncodingAttr>()) {\n+    SmallVector<Type, 4> types;\n+    // base ptr\n+    auto ptrType = LLVM::LLVMPointerType::get(eltType, 3);\n+    types.push_back(ptrType);\n+    // shape dims\n+    auto rank = type.getRank();\n+    // offsets + strides\n+    for (auto i = 0; i < rank * 2; i++) {\n+      types.push_back(IntegerType::get(ctx, 32));\n+    }\n+    return LLVM::LLVMStructType::getLiteral(ctx, types);\n+  }\n+\n+  unsigned numElementsPerThread = getTotalElemsPerThread(type);\n+  SmallVector<Type, 4> types(numElementsPerThread, eltType);\n+  return LLVM::LLVMStructType::getLiteral(ctx, types);\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.h", "status": "modified", "additions": 10, "deletions": 133, "changes": 143, "file_content_changes": "@@ -1,154 +1,31 @@\n #ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_TYPECONVERTER_H\n #define TRITON_CONVERSION_TRITONGPU_TO_LLVM_TYPECONVERTER_H\n \n+#include \"mlir/Conversion/LLVMCommon/TypeConverter.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"triton/Conversion/MLIRTypes.h\"\n \n-#include \"DotOpHelpers.h\"\n-#include \"Utility.h\"\n-\n using namespace mlir;\n using namespace mlir::triton;\n \n-using ::mlir::LLVM::DotOpFMAConversionHelper;\n-using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n-using ::mlir::LLVM::MMA16816ConversionHelper;\n-using ::mlir::triton::gpu::BlockedEncodingAttr;\n-using ::mlir::triton::gpu::DotOperandEncodingAttr;\n-using ::mlir::triton::gpu::getElemsPerThread;\n-using ::mlir::triton::gpu::MmaEncodingAttr;\n-using ::mlir::triton::gpu::SharedEncodingAttr;\n-using ::mlir::triton::gpu::SliceEncodingAttr;\n-\n class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n public:\n   using TypeConverter::convertType;\n \n   TritonGPUToLLVMTypeConverter(MLIRContext *ctx, LowerToLLVMOptions &option,\n-                               const DataLayoutAnalysis *analysis = nullptr)\n-      : LLVMTypeConverter(ctx, option, analysis) {\n-    addConversion([&](triton::PointerType type) -> llvm::Optional<Type> {\n-      return convertTritonPointerType(type);\n-    });\n-    addConversion([&](RankedTensorType type) -> llvm::Optional<Type> {\n-      return convertTritonTensorType(type);\n-    });\n-    // Internally store float8 as int8\n-    addConversion([&](triton::Float8Type type) -> llvm::Optional<Type> {\n-      return IntegerType::get(type.getContext(), 8);\n-    });\n-    // Internally store bfloat16 as int16\n-    addConversion([&](BFloat16Type type) -> llvm::Optional<Type> {\n-      return IntegerType::get(type.getContext(), 16);\n-    });\n-  }\n-\n-  Type convertTritonPointerType(triton::PointerType type) {\n-    // Recursively translate pointee type\n-    return LLVM::LLVMPointerType::get(convertType(type.getPointeeType()),\n-                                      type.getAddressSpace());\n-  }\n-\n-  llvm::Optional<Type> convertTritonTensorType(RankedTensorType type) {\n-    auto ctx = type.getContext();\n-    Attribute layout = type.getEncoding();\n-    SmallVector<int64_t> shape(type.getShape().begin(), type.getShape().end());\n-\n-    if (layout &&\n-        (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n-         layout.isa<MmaEncodingAttr>())) {\n-      unsigned numElementsPerThread = getElemsPerThread(type);\n-      SmallVector<Type, 4> types(numElementsPerThread,\n-                                 convertType(type.getElementType()));\n-      return LLVM::LLVMStructType::getLiteral(ctx, types);\n-    } else if (auto shared_layout =\n-                   layout.dyn_cast_or_null<SharedEncodingAttr>()) {\n-      SmallVector<Type, 4> types;\n-      // base ptr\n-      auto ptrType =\n-          LLVM::LLVMPointerType::get(convertType(type.getElementType()), 3);\n-      types.push_back(ptrType);\n-      // shape dims\n-      auto rank = type.getRank();\n-      // offsets + strides\n-      for (auto i = 0; i < rank * 2; i++) {\n-        types.push_back(IntegerType::get(ctx, 32));\n-      }\n-      return LLVM::LLVMStructType::getLiteral(ctx, types);\n-    } else if (auto dotOpLayout =\n-                   layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n-      if (dotOpLayout.getParent()\n-              .isa<BlockedEncodingAttr>()) { // for parent is blocked layout\n-        int numElemsPerThread =\n-            DotOpFMAConversionHelper::getNumElemsPerThread(shape, dotOpLayout);\n+                               const DataLayoutAnalysis *analysis = nullptr);\n \n-        return LLVM::LLVMStructType::getLiteral(\n-            ctx, SmallVector<Type>(numElemsPerThread, type::f32Ty(ctx)));\n-      } else { // for parent is MMA layout\n-        auto mmaLayout = dotOpLayout.getParent().cast<MmaEncodingAttr>();\n-        auto wpt = mmaLayout.getWarpsPerCTA();\n-        Type elemTy = convertType(type.getElementType());\n-        if (mmaLayout.isAmpere()) {\n-          const llvm::DenseMap<int, Type> targetTyMap = {\n-              {32, vec_ty(elemTy, 1)},\n-              {16, vec_ty(elemTy, 2)},\n-              {8, vec_ty(elemTy, 4)},\n-          };\n-          Type targetTy;\n-          if (targetTyMap.count(elemTy.getIntOrFloatBitWidth())) {\n-            targetTy = targetTyMap.lookup(elemTy.getIntOrFloatBitWidth());\n-            // <2xi16>/<4xi8> => i32\n-            // We are doing this because NVPTX inserts extra integer instrs to\n-            // pack & unpack vectors of sub-word integers\n-            // Note: this needs to be synced with\n-            //       DotOpMmaV2ConversionHelper::loadX4\n-            if (elemTy.isa<IntegerType>() &&\n-                (elemTy.getIntOrFloatBitWidth() == 8 ||\n-                 elemTy.getIntOrFloatBitWidth() == 16))\n-              targetTy = IntegerType::get(ctx, 32);\n-          } else {\n-            assert(false && \"Unsupported element type\");\n-          }\n-          if (dotOpLayout.getOpIdx() == 0) { // $a\n-            auto elems =\n-                MMA16816ConversionHelper::getANumElemsPerThread(type, wpt[0]);\n-            return struct_ty(SmallVector<Type>(elems, targetTy));\n-          }\n-          if (dotOpLayout.getOpIdx() == 1) { // $b\n-            auto elems =\n-                MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt[1]);\n-            return struct_ty(SmallVector<Type>(elems, targetTy));\n-          }\n-        }\n+  Type getElementTypeForStruct(RankedTensorType type);\n+  Type convertTritonPointerType(triton::PointerType type);\n \n-        if (mmaLayout.isVolta()) {\n-          auto [isARow, isBRow, isAVec4, isBVec4, mmaId] =\n-              mmaLayout.decodeVoltaLayoutStates();\n-          DotOpMmaV1ConversionHelper helper(mmaLayout);\n-          if (dotOpLayout.getOpIdx() == 0) { // $a\n-            DotOpMmaV1ConversionHelper::AParam param(isARow, isAVec4);\n-            int elems =\n-                helper.numElemsPerThreadA(shape, isARow, isAVec4, param.vec);\n-            Type x2Ty = vec_ty(elemTy, 2);\n-            return struct_ty(SmallVector<Type>(elems, x2Ty));\n-          }\n-          if (dotOpLayout.getOpIdx() == 1) { // $b\n-            DotOpMmaV1ConversionHelper::BParam param(isBRow, isBVec4);\n-            int elems =\n-                helper.numElemsPerThreadB(shape, isBRow, isBVec4, param.vec);\n-            Type x2Ty = vec_ty(elemTy, 2);\n-            return struct_ty(SmallVector<Type>(elems, x2Ty));\n-          }\n-        }\n-      }\n+  Value packLLElements(Location loc, ValueRange resultVals,\n+                       ConversionPatternRewriter &rewriter, Type type);\n \n-      llvm::errs() << \"Unexpected dot operand layout detected in \"\n-                      \"TritonToLLVMTypeConverter\";\n-      return llvm::None;\n-    }\n+  SmallVector<Value> unpackLLElements(Location loc, Value llvmStruct,\n+                                      ConversionPatternRewriter &rewriter,\n+                                      Type type);\n \n-    return llvm::None;\n-  }\n+  Type convertTritonTensorType(RankedTensorType type);\n };\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.cpp", "status": "modified", "additions": 43, "deletions": 33, "changes": 76, "file_content_changes": "@@ -1,42 +1,11 @@\n #include \"Utility.h\"\n+#include \"TypeConverter.h\"\n \n namespace mlir {\n \n namespace LLVM {\n using namespace mlir::triton;\n \n-Value getStructFromElements(Location loc, ValueRange resultVals,\n-                            ConversionPatternRewriter &rewriter,\n-                            Type structType) {\n-  if (!structType.isa<LLVM::LLVMStructType>()) {\n-    return *resultVals.begin();\n-  }\n-\n-  Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structType);\n-  for (const auto &v : llvm::enumerate(resultVals)) {\n-    assert(v.value() && \"can not insert null values\");\n-    llvmStruct = insert_val(structType, llvmStruct, v.value(),\n-                            rewriter.getI64ArrayAttr(v.index()));\n-  }\n-  return llvmStruct;\n-}\n-\n-SmallVector<Value> getElementsFromStruct(Location loc, Value llvmStruct,\n-                                         ConversionPatternRewriter &rewriter) {\n-  if (llvmStruct.getType().isIntOrIndexOrFloat() ||\n-      llvmStruct.getType().isa<triton::PointerType>() ||\n-      llvmStruct.getType().isa<LLVM::LLVMPointerType>())\n-    return {llvmStruct};\n-  ArrayRef<Type> types =\n-      llvmStruct.getType().cast<LLVM::LLVMStructType>().getBody();\n-  SmallVector<Value> results(types.size());\n-  for (unsigned i = 0; i < types.size(); ++i) {\n-    Type type = types[i];\n-    results[i] = extract_val(type, llvmStruct, i64_arr_attr(i));\n-  }\n-  return results;\n-}\n-\n Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v) {\n   auto i32ty = rewriter.getIntegerType(32);\n   return rewriter.create<LLVM::ConstantOp>(loc, i32ty,\n@@ -74,7 +43,14 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n SharedMemoryObject\n getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n                                 ConversionPatternRewriter &rewriter) {\n-  auto elems = getElementsFromStruct(loc, llvmStruct, rewriter);\n+  ArrayRef<Type> types =\n+      llvmStruct.getType().cast<LLVM::LLVMStructType>().getBody();\n+  SmallVector<Value> elems(types.size());\n+  for (unsigned i = 0; i < types.size(); ++i) {\n+    Type type = types[i];\n+    elems[i] = extract_val(type, llvmStruct, i);\n+  }\n+\n   auto rank = (elems.size() - 1) / 2;\n   return {/*base=*/elems[0],\n           /*strides=*/{elems.begin() + 1, elems.begin() + 1 + rank},\n@@ -136,5 +112,39 @@ Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n   return builder.launch(rewriter, loc, val.getType(), false);\n }\n \n+Value addStringToModule(Location loc, ConversionPatternRewriter &rewriter,\n+                        StringRef key, StringRef content) {\n+  auto moduleOp = rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+  auto ctx = moduleOp.getContext();\n+  unsigned stringNumber = 0;\n+  SmallString<16> stringConstName;\n+  do {\n+    stringConstName.clear();\n+    (key + Twine(stringNumber++)).toStringRef(stringConstName);\n+  } while (moduleOp.lookupSymbol(stringConstName));\n+\n+  llvm::SmallString<64> contentStr(content);\n+  size_t contentSize = contentStr.size_in_bytes();\n+  auto globalType = LLVM::LLVMArrayType::get(i8_ty, contentSize);\n+\n+  LLVM::GlobalOp global;\n+  {\n+    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+    rewriter.setInsertionPointToStart(moduleOp.getBody());\n+    global = rewriter.create<LLVM::GlobalOp>(\n+        UnknownLoc::get(ctx), globalType,\n+        /*isConstant=*/true, LLVM::Linkage::Internal, stringConstName,\n+        rewriter.getStringAttr(contentStr));\n+  }\n+\n+  Value zero = i32_val(0);\n+  Value globalPtr =\n+      rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(ctx), global);\n+  Value stringStart =\n+      rewriter.create<LLVM::GEPOp>(UnknownLoc::get(ctx), ptr_ty(i8_ty),\n+                                   globalPtr, SmallVector<Value>({zero, zero}));\n+  return stringStart;\n+}\n+\n } // namespace LLVM\n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 20, "deletions": 19, "changes": 39, "file_content_changes": "@@ -12,6 +12,8 @@\n #define inttoptr(...) rewriter.create<LLVM::IntToPtrOp>(loc, __VA_ARGS__)\n #define ptrtoint(...) rewriter.create<LLVM::PtrToIntOp>(loc, __VA_ARGS__)\n #define zext(...) rewriter.create<LLVM::ZExtOp>(loc, __VA_ARGS__)\n+#define sext(...) rewriter.create<LLVM::SExtOp>(loc, __VA_ARGS__)\n+#define fpext(...) rewriter.create<LLVM::FPExtOp>(loc, __VA_ARGS__)\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n #define urem(...) rewriter.create<LLVM::URemOp>(loc, __VA_ARGS__)\n #define add(...) rewriter.create<LLVM::AddOp>(loc, __VA_ARGS__)\n@@ -25,8 +27,10 @@\n #define smin(...) rewriter.create<LLVM::SMinOp>(loc, __VA_ARGS__)\n #define umin(...) rewriter.create<LLVM::UMinOp>(loc, __VA_ARGS__)\n #define fmin(...) rewriter.create<LLVM::MinNumOp>(loc, __VA_ARGS__)\n+#define shl(...) rewriter.create<LLVM::ShlOp>(loc, __VA_ARGS__)\n #define and_(...) rewriter.create<LLVM::AndOp>(loc, __VA_ARGS__)\n #define xor_(...) rewriter.create<LLVM::XOrOp>(loc, __VA_ARGS__)\n+#define or_(...) rewriter.create<LLVM::OrOp>(loc, __VA_ARGS__)\n #define bitcast(val__, type__)                                                 \\\n   rewriter.create<LLVM::BitcastOp>(loc, type__, val__)\n #define gep(...) rewriter.create<LLVM::GEPOp>(loc, __VA_ARGS__)\n@@ -45,6 +49,9 @@\n #define fcmp_olt(lhs, rhs)                                                     \\\n   rewriter.create<LLVM::FCmpOp>(loc, rewriter.getI1Type(),                     \\\n                                 LLVM::FCmpPredicate::olt, lhs, rhs)\n+#define fcmp_eq(lhs, rhs)                                                      \\\n+  rewriter.create<LLVM::FCmpOp>(loc, rewriter.getI1Type(),                     \\\n+                                LLVM::FCmpPredicate::oeq, lhs, rhs)\n #define icmp_eq(...)                                                           \\\n   rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::eq, __VA_ARGS__)\n #define icmp_ne(...)                                                           \\\n@@ -69,30 +76,32 @@\n #define address_of(...) rewriter.create<LLVM::AddressOfOp>(loc, __VA_ARGS__)\n #define barrier() rewriter.create<mlir::gpu::BarrierOp>(loc)\n #define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n+#define null(...) rewriter.create<LLVM::NullOp>(loc, __VA_ARGS__)\n+#define call(...) rewriter.create<LLVM::CallOp>(loc, __VA_ARGS__)\n \n // Types\n+#define int_ty(width) rewriter.getIntegerType(width)\n+#define i64_ty rewriter.getIntegerType(64)\n #define i32_ty rewriter.getIntegerType(32)\n #define i16_ty rewriter.getIntegerType(16)\n #define ui32_ty rewriter.getIntegerType(32, false)\n #define f16_ty rewriter.getF16Type()\n #define bf16_ty rewriter.getBF16Type()\n #define i8_ty rewriter.getIntegerType(8)\n+#define i1_ty rewriter.getI1Type()\n #define f32_ty rewriter.getF32Type()\n #define f64_ty rewriter.getF64Type()\n #define vec_ty(type, num) VectorType::get(num, type)\n-#define f32_val(...) LLVM::createConstantF32(loc, rewriter, __VA_ARGS__)\n-#define f64_val(...) LLVM::createConstantF64(loc, rewriter, __VA_ARGS__)\n #define void_ty(ctx) LLVM::LLVMVoidType::get(ctx)\n #define struct_ty(...) LLVM::LLVMStructType::getLiteral(ctx, __VA_ARGS__)\n #define array_ty(elemTy, count) LLVM::LLVMArrayType::get(elemTy, count)\n \n // Constants\n+#define f32_val(...) LLVM::createConstantF32(loc, rewriter, __VA_ARGS__)\n+#define f64_val(...) LLVM::createConstantF64(loc, rewriter, __VA_ARGS__)\n #define i32_val(...) LLVM::createConstantI32(loc, rewriter, __VA_ARGS__)\n #define int_val(width, val)                                                    \\\n   LLVM::createLLVMIntegerConstant(rewriter, loc, width, val)\n-#define idx_val(...)                                                           \\\n-  LLVM::createIndexConstant(rewriter, loc, this->getTypeConverter(),           \\\n-                            __VA_ARGS__)\n #define tid_val() getThreadId(rewriter, loc)\n \n // Attributes\n@@ -165,14 +174,6 @@ T getLinearIndex(llvm::ArrayRef<T> multiDimIndex, llvm::ArrayRef<T> shape,\n namespace LLVM {\n using namespace mlir::triton;\n \n-Value getStructFromElements(Location loc, ValueRange resultVals,\n-                            ConversionPatternRewriter &rewriter,\n-                            Type structType);\n-\n-SmallVector<Value> getElementsFromStruct(Location loc, Value llvmStruct,\n-                                         ConversionPatternRewriter &rewriter);\n-\n-/// Create a 32-bit integer constant.\n Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v);\n \n /// Create a 32-bit float constant.\n@@ -220,10 +221,7 @@ struct SharedMemoryObject {\n                      ConversionPatternRewriter &rewriter)\n       : base(base) {\n     strides = getStridesFromShapeAndOrder(shape, order, loc, rewriter);\n-\n-    for (auto idx : order) {\n-      offsets.emplace_back(i32_val(0));\n-    }\n+    offsets.append(order.size(), i32_val(0));\n   }\n \n   SmallVector<Value> getElems() const {\n@@ -247,8 +245,8 @@ struct SharedMemoryObject {\n     return offsets[order];\n   }\n \n-  Value getBaseBeforeSwizzle(int order, Location loc,\n-                             ConversionPatternRewriter &rewriter) const {\n+  Value getBaseBeforeSlice(int order, Location loc,\n+                           ConversionPatternRewriter &rewriter) const {\n     Value cSwizzleOffset = getCSwizzleOffset(order);\n     Value offset = sub(i32_val(0), cSwizzleOffset);\n     Type type = base.getType();\n@@ -266,6 +264,9 @@ Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n                int i);\n \n+Value addStringToModule(Location loc, ConversionPatternRewriter &rewriter,\n+                        StringRef key, StringRef content);\n+\n } // namespace LLVM\n } // namespace mlir\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.h", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonToTritonGPU/CMakeLists.txt", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 383, "deletions": 118, "changes": 501, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/IR/CMakeLists.txt", "status": "modified", "additions": 3, "deletions": 8, "changes": 11, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/IR/Dialect.cpp", "status": "modified", "additions": 48, "deletions": 4, "changes": 52, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 576, "deletions": 148, "changes": 724, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/IR/Traits.cpp", "status": "modified", "additions": 95, "deletions": 23, "changes": 118, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/IR/Types.cpp", "status": "modified", "additions": 72, "deletions": 0, "changes": 72, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/Transforms/CMakeLists.txt", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 17, "deletions": 95, "changes": 112, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.td", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/Transforms/RewriteTensorPointer.cpp", "status": "added", "additions": 504, "deletions": 0, "changes": 504, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/IR/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 585, "deletions": 113, "changes": 698, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/IR/Traits.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "added", "additions": 219, "deletions": 0, "changes": 219, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/CMakeLists.txt", "status": "modified", "additions": 7, "deletions": 10, "changes": 17, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/CanonicalizeLoops.cpp", "status": "removed", "additions": 0, "deletions": 55, "changes": 55, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 108, "deletions": 51, "changes": 159, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "removed", "additions": 0, "deletions": 1292, "changes": 1292, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.td", "status": "removed", "additions": 0, "deletions": 7, "changes": 7, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/DecomposeConversions.cpp", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "added", "additions": 224, "deletions": 0, "changes": 224, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 678, "deletions": 343, "changes": 1021, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 110, "deletions": 46, "changes": 156, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "added", "additions": 618, "deletions": 0, "changes": 618, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/ReorderInstructions.cpp", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 28, "deletions": 21, "changes": 49, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/UpdateMmaForVolta.cpp", "status": "removed", "additions": 0, "deletions": 352, "changes": 352, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 286, "deletions": 4, "changes": 290, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.h", "status": "removed", "additions": 0, "deletions": 12, "changes": 12, "file_content_changes": "N/A"}, {"filename": "lib/Target/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "N/A"}, {"filename": "lib/Target/HSACO/CMakeLists.txt", "status": "added", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "N/A"}, {"filename": "lib/Target/HSACO/HSACOTranslation.cpp", "status": "added", "additions": 182, "deletions": 0, "changes": 182, "file_content_changes": "N/A"}, {"filename": "lib/Target/LLVMIR/CMakeLists.txt", "status": "modified", "additions": 10, "deletions": 2, "changes": 12, "file_content_changes": "N/A"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 163, "deletions": 58, "changes": 221, "file_content_changes": "N/A"}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 28, "deletions": 19, "changes": 47, "file_content_changes": "N/A"}, {"filename": "python/MANIFEST.in", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "N/A"}, {"filename": "python/examples/copy_strided.py", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "N/A"}, {"filename": "python/pyproject.toml", "status": "added", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "N/A"}, {"filename": "python/setup.cfg", "status": "removed", "additions": 0, "deletions": 8, "changes": 8, "file_content_changes": "N/A"}, {"filename": "python/setup.py", "status": "modified", "additions": 136, "deletions": 49, "changes": 185, "file_content_changes": "N/A"}, {"filename": "python/src/extra/cuda.ll", "status": "added", "additions": 17, "deletions": 0, "changes": 17, "file_content_changes": "N/A"}, {"filename": "python/src/main.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "N/A"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 406, "deletions": 164, "changes": 570, "file_content_changes": "N/A"}, {"filename": "python/test/backend/extension_backend.c", "status": "added", "additions": 42, "deletions": 0, "changes": 42, "file_content_changes": "N/A"}, {"filename": "python/test/backend/test_device_backend.py", "status": "added", "additions": 262, "deletions": 0, "changes": 262, "file_content_changes": "N/A"}, {"filename": "python/test/backend/third_party_backends/conftest.py", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "N/A"}, {"filename": "python/test/backend/third_party_backends/test_xpu_backend.py", "status": "added", "additions": 33, "deletions": 0, "changes": 33, "file_content_changes": "N/A"}, {"filename": "python/test/regression/test_functional_regressions.py", "status": "added", "additions": 230, "deletions": 0, "changes": 230, "file_content_changes": "N/A"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 84, "deletions": 36, "changes": 120, "file_content_changes": "N/A"}, {"filename": "python/test/unit/interpreter/test_interpreter.py", "status": "added", "additions": 69, "deletions": 0, "changes": 69, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/assert_helper.py", "status": "added", "additions": 131, "deletions": 0, "changes": 131, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/print_helper.py", "status": "added", "additions": 46, "deletions": 0, "changes": 46, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/printf_helper.py", "status": "removed", "additions": 0, "deletions": 56, "changes": 56, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/test_annotations.py", "status": "added", "additions": 21, "deletions": 0, "changes": 21, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/test_block_pointer.py", "status": "added", "additions": 102, "deletions": 0, "changes": 102, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 1445, "deletions": 202, "changes": 1647, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/test_printf.py", "status": "removed", "additions": 0, "deletions": 22, "changes": 22, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/test_subprocess.py", "status": "added", "additions": 80, "deletions": 0, "changes": 80, "file_content_changes": "N/A"}, {"filename": "python/test/unit/operators/test_blocksparse.py", "status": "modified", "additions": 49, "deletions": 22, "changes": 71, "file_content_changes": "N/A"}, {"filename": "python/test/unit/operators/test_cross_entropy.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "N/A"}, {"filename": "python/test/unit/operators/test_flash_attention.py", "status": "added", "additions": 49, "deletions": 0, "changes": 49, "file_content_changes": "N/A"}, {"filename": "python/test/unit/operators/test_inductor.py", "status": "added", "additions": 155, "deletions": 0, "changes": 155, "file_content_changes": "N/A"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 87, "deletions": 49, "changes": 136, "file_content_changes": "N/A"}, {"filename": "python/test/unit/runtime/test_autotuner.py", "status": "added", "additions": 22, "deletions": 0, "changes": 22, "file_content_changes": "N/A"}, {"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 57, "deletions": 55, "changes": 112, "file_content_changes": "N/A"}, {"filename": "python/test/unit/runtime/test_driver.py", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "N/A"}, {"filename": "python/test/unit/runtime/test_launch.py", "status": "added", "additions": 106, "deletions": 0, "changes": 106, "file_content_changes": "N/A"}, {"filename": "python/test/unit/runtime/test_subproc.py", "status": "added", "additions": 83, "deletions": 0, "changes": 83, "file_content_changes": "N/A"}, {"filename": "python/test/unit/tools/test_aot.py", "status": "added", "additions": 205, "deletions": 0, "changes": 205, "file_content_changes": "N/A"}, {"filename": "python/triton/__init__.py", "status": "modified", "additions": 30, "deletions": 14, "changes": 44, "file_content_changes": "N/A"}, {"filename": "python/triton/common/__init__.py", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "N/A"}, {"filename": "python/triton/common/backend.py", "status": "added", "additions": 96, "deletions": 0, "changes": 96, "file_content_changes": "N/A"}, {"filename": "python/triton/common/build.py", "status": "added", "additions": 115, "deletions": 0, "changes": 115, "file_content_changes": "N/A"}, {"filename": "python/triton/compiler.py", "status": "removed", "additions": 0, "deletions": 1775, "changes": 1775, "file_content_changes": "N/A"}, {"filename": "python/triton/compiler/__init__.py", "status": "added", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "N/A"}, {"filename": "python/triton/compiler/code_generator.py", "status": "added", "additions": 1089, "deletions": 0, "changes": 1089, "file_content_changes": "N/A"}, {"filename": "python/triton/compiler/compiler.py", "status": "added", "additions": 625, "deletions": 0, "changes": 625, "file_content_changes": "N/A"}, {"filename": "python/triton/compiler/errors.py", "status": "added", "additions": 52, "deletions": 0, "changes": 52, "file_content_changes": "N/A"}, {"filename": "python/triton/compiler/make_launcher.py", "status": "added", "additions": 373, "deletions": 0, "changes": 373, "file_content_changes": "N/A"}, {"filename": "python/triton/impl/__init__.py", "status": "removed", "additions": 0, "deletions": 18, "changes": 18, "file_content_changes": "N/A"}, {"filename": "python/triton/impl/base.py", "status": "removed", "additions": 0, "deletions": 36, "changes": 36, "file_content_changes": "N/A"}, {"filename": "python/triton/interpreter/__init__.py", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "lib/Dialect/Triton/IR/Interfaces.cpp"}, {"filename": "python/triton/interpreter/core.py", "status": "added", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "N/A"}, {"filename": "python/triton/interpreter/interpreter.py", "status": "added", "additions": 171, "deletions": 0, "changes": 171, "file_content_changes": "N/A"}, {"filename": "python/triton/interpreter/memory_map.py", "status": "added", "additions": 102, "deletions": 0, "changes": 102, "file_content_changes": "N/A"}, {"filename": "python/triton/interpreter/tl_lang.py", "status": "added", "additions": 626, "deletions": 0, "changes": 626, "file_content_changes": "N/A"}, {"filename": "python/triton/interpreter/torch_wrapper.py", "status": "added", "additions": 18, "deletions": 0, "changes": 18, "file_content_changes": "N/A"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 35, "deletions": 17, "changes": 52, "file_content_changes": "N/A"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 673, "deletions": 212, "changes": 885, "file_content_changes": "N/A"}, {"filename": "python/triton/language/extern.py", "status": "removed", "additions": 0, "deletions": 77, "changes": 77, "file_content_changes": "N/A"}, {"filename": "python/triton/language/extra/__init__.py", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "N/A"}, {"filename": "python/triton/language/extra/cuda.bc", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/language/extra/cuda.py", "status": "added", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "N/A"}, {"filename": "python/triton/language/libdevice.py", "status": "removed", "additions": 0, "deletions": 1526, "changes": 1526, "file_content_changes": "N/A"}, {"filename": "python/triton/language/math.py", "status": "added", "additions": 1534, "deletions": 0, "changes": 1534, "file_content_changes": "N/A"}, {"filename": "python/triton/language/random.py", "status": "modified", "additions": 22, "deletions": 17, "changes": 39, "file_content_changes": "N/A"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 474, "deletions": 170, "changes": 644, "file_content_changes": "N/A"}, {"filename": "python/triton/language/standard.py", "status": "added", "additions": 98, "deletions": 0, "changes": 98, "file_content_changes": "N/A"}, {"filename": "python/triton/ops/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "N/A"}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 15, "deletions": 12, "changes": 27, "file_content_changes": "N/A"}, {"filename": "python/triton/ops/blocksparse/softmax.py", "status": "modified", "additions": 10, "deletions": 7, "changes": 17, "file_content_changes": "N/A"}, {"filename": "python/triton/ops/cross_entropy.py", "status": "modified", "additions": 11, "deletions": 19, "changes": 30, "file_content_changes": "N/A"}, {"filename": "python/triton/ops/flash_attention.py", "status": "added", "additions": 269, "deletions": 0, "changes": 269, "file_content_changes": "N/A"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 59, "deletions": 43, "changes": 102, "file_content_changes": "N/A"}, {"filename": "python/triton/ops/matmul_perf_model.py", "status": "modified", "additions": 12, "deletions": 14, "changes": 26, "file_content_changes": "N/A"}, {"filename": "python/triton/runtime/__init__.py", "status": "modified", "additions": 11, "deletions": 2, "changes": 13, "file_content_changes": "N/A"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 37, "deletions": 12, "changes": 49, "file_content_changes": "N/A"}, {"filename": "python/triton/runtime/backends/cuda.c", "status": "added", "additions": 124, "deletions": 0, "changes": 124, "file_content_changes": "N/A"}, {"filename": "python/triton/runtime/backends/hip.c", "status": "added", "additions": 120, "deletions": 0, "changes": 120, "file_content_changes": "N/A"}, {"filename": "python/triton/runtime/cache.py", "status": "added", "additions": 131, "deletions": 0, "changes": 131, "file_content_changes": "N/A"}, {"filename": "python/triton/runtime/driver.py", "status": "added", "additions": 174, "deletions": 0, "changes": 174, "file_content_changes": "N/A"}, {"filename": "python/triton/runtime/errors.py", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "N/A"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 224, "deletions": 77, "changes": 301, "file_content_changes": "N/A"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 87, "deletions": 136, "changes": 223, "file_content_changes": "N/A"}, {"filename": "python/triton/third_party/cuda/bin/ptxas", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/third_party/cuda/include/cuda.h", "status": "added", "additions": 19348, "deletions": 0, "changes": 19348, "file_content_changes": "N/A"}, {"filename": "python/triton/third_party/cuda/lib/libdevice.10.bc", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "python/triton/language/libdevice.10.bc"}, {"filename": "python/triton/tools/aot.py", "status": "removed", "additions": 0, "deletions": 62, "changes": 62, "file_content_changes": "N/A"}, {"filename": "python/triton/tools/build_extern.py", "status": "modified", "additions": 19, "deletions": 6, "changes": 25, "file_content_changes": "N/A"}, {"filename": "python/triton/tools/compile.c", "status": "added", "additions": 64, "deletions": 0, "changes": 64, "file_content_changes": "N/A"}, {"filename": "python/triton/tools/compile.h", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "N/A"}, {"filename": "python/triton/tools/compile.py", "status": "added", "additions": 103, "deletions": 0, "changes": 103, "file_content_changes": "N/A"}, {"filename": "python/triton/tools/link.py", "status": "added", "additions": 192, "deletions": 0, "changes": 192, "file_content_changes": "N/A"}, {"filename": "python/triton/utils.py", "status": "removed", "additions": 0, "deletions": 67, "changes": 67, "file_content_changes": "N/A"}, {"filename": "python/tutorials/01-vector-add.py", "status": "modified", "additions": 18, "deletions": 10, "changes": 28, "file_content_changes": "N/A"}, {"filename": "python/tutorials/02-fused-softmax.py", "status": "modified", "additions": 21, "deletions": 12, "changes": 33, "file_content_changes": "N/A"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 105, "deletions": 92, "changes": 197, "file_content_changes": "N/A"}, {"filename": "python/tutorials/04-low-memory-dropout.py", "status": "modified", "additions": 29, "deletions": 22, "changes": 51, "file_content_changes": "N/A"}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 153, "deletions": 57, "changes": 210, "file_content_changes": "N/A"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 14, "deletions": 11, "changes": 25, "file_content_changes": "N/A"}, {"filename": "python/tutorials/07-math-functions.py", "status": "added", "additions": 73, "deletions": 0, "changes": 73, "file_content_changes": "N/A"}, {"filename": "python/tutorials/08-experimental-block-pointer.py", "status": "added", "additions": 228, "deletions": 0, "changes": 228, "file_content_changes": "N/A"}, {"filename": "python/tutorials/README.rst", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "N/A"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 63, "deletions": 32, "changes": 95, "file_content_changes": "N/A"}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 486, "deletions": 53, "changes": 539, "file_content_changes": "N/A"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 221, "deletions": 46, "changes": 267, "file_content_changes": "N/A"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 452, "deletions": 140, "changes": 592, "file_content_changes": "N/A"}, {"filename": "test/CMakeLists.txt", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "N/A"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 82, "deletions": 25, "changes": 107, "file_content_changes": "N/A"}, {"filename": "test/Conversion/triton_to_tritongpu.mlir", "status": "modified", "additions": 35, "deletions": 19, "changes": 54, "file_content_changes": "N/A"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 239, "deletions": 125, "changes": 364, "file_content_changes": "N/A"}, {"filename": "test/Target/tritongpu_to_llvmir.mlir", "status": "removed", "additions": 0, "deletions": 16, "changes": 16, "file_content_changes": "N/A"}, {"filename": "test/Target/tritongpu_to_ptx.mlir", "status": "removed", "additions": 0, "deletions": 14, "changes": 14, "file_content_changes": "N/A"}, {"filename": "test/Triton/combine.mlir", "status": "modified", "additions": 99, "deletions": 33, "changes": 132, "file_content_changes": "N/A"}, {"filename": "test/Triton/rewrite-tensor-pointer.mlir", "status": "added", "additions": 83, "deletions": 0, "changes": 83, "file_content_changes": "N/A"}, {"filename": "test/Triton/vecadd.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/coalesce.mlir", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 1049, "deletions": 82, "changes": 1131, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "added", "additions": 128, "deletions": 0, "changes": 128, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 294, "deletions": 43, "changes": 337, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/matmul.mlir", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/prefetch.mlir", "status": "modified", "additions": 29, "deletions": 26, "changes": 55, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/update-mma-for-volta.mlir", "status": "removed", "additions": 0, "deletions": 73, "changes": 73, "file_content_changes": "N/A"}, {"filename": "test/lib/Analysis/CMakeLists.txt", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "N/A"}, {"filename": "test/lib/Analysis/TestAlias.cpp", "status": "modified", "additions": 35, "deletions": 16, "changes": 51, "file_content_changes": "N/A"}, {"filename": "test/lib/Analysis/TestAllocation.cpp", "status": "modified", "additions": 30, "deletions": 25, "changes": 55, "file_content_changes": "N/A"}, {"filename": "test/lib/Analysis/TestAxisInfo.cpp", "status": "modified", "additions": 19, "deletions": 39, "changes": 58, "file_content_changes": "N/A"}, {"filename": "test/lib/Analysis/TestMembar.cpp", "status": "modified", "additions": 9, "deletions": 22, "changes": 31, "file_content_changes": "N/A"}, {"filename": "test/lib/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "N/A"}, {"filename": "test/lit.cfg.py", "status": "modified", "additions": 12, "deletions": 6, "changes": 18, "file_content_changes": "N/A"}, {"filename": "third_party/intel_xpu_backend", "status": "added", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "N/A"}, {"filename": "unittest/Analysis/CMakeLists.txt", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "N/A"}, {"filename": "unittest/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "N/A"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/PTXAsmFormatTest.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "N/A"}, {"filename": "unittest/Dialect/TritonGPU/SwizzleTest.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "N/A"}, {"filename": "utils/nightly.pypirc", "status": "added", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "N/A"}]