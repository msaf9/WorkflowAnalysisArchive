[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -421,6 +421,15 @@ def where_kernel(cond_ptr, a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n         res = tl.where(mask, vals, 0.)\n         tl.store(out_ptr + yoffsets + BLOCK_SIZE * xoffsets, res)\n \n+    @triton.jit\n+    def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n+        xoffsets = tl.reshape(tl.arange(0, BLOCK_SIZE), [BLOCK_SIZE, 1])\n+        yoffsets = tl.reshape(tl.arange(0, BLOCK_SIZE), [1, BLOCK_SIZE])\n+        mask = 0\n+        vals = tl.load(a_ptr + yoffsets + BLOCK_SIZE * xoffsets)\n+        res = tl.where(mask, vals, 0.)\n+        tl.store(out_ptr + yoffsets + BLOCK_SIZE * xoffsets, res)\n+\n     SIZE = 32\n     dtype = 'float32'\n     rs = RandomState(17)\n@@ -432,6 +441,9 @@ def where_kernel(cond_ptr, a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n     z_tri = to_triton(np.empty((SIZE, SIZE), dtype=z.dtype), device='cuda', dst_type=dtype)\n     where_kernel[(1,)](cond_tri, x_tri, z_tri, SIZE)\n     assert (z == to_numpy(z_tri)).all()\n+    where_scalar_condition[(1,)](x_tri, z_tri, SIZE)\n+    z = np.where(0, x, 0)\n+    assert (z == to_numpy(z_tri)).all()\n \n # ---------------\n # test unary ops"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -826,7 +826,7 @@ def make_triton_ir(fn, signature, specialization, constants):\n     gscope = fn.__globals__.copy()\n     function_name = '_'.join([fn.__name__, kernel_suffix(signature.values(), specialization)])\n     tys = list(signature.values())\n-    new_constants = {k: True if tys[k] == \"i1\" else 1 for k in specialization.equal_to_1}\n+    new_constants = {k: True if k in tys and tys[k] == \"i1\" else 1 for k in specialization.equal_to_1}\n     new_attrs = {k: (\"multiple_of\", 16) for k in specialization.divisible_by_16}\n     all_constants = constants.copy()\n     all_constants.update(new_constants)\n@@ -1130,6 +1130,12 @@ def libcuda_dirs():\n     return [os.path.dirname(loc) for loc in locs]\n \n \n+@functools.lru_cache()\n+def cuda_home_dirs():\n+    default_dir = \"/usr/local/cuda\"\n+    return os.getenv(\"CUDA_HOME\", default=default_dir)\n+\n+\n @contextlib.contextmanager\n def quiet():\n     old_stdout, old_stderr = sys.stdout, sys.stderr\n@@ -1142,7 +1148,7 @@ def quiet():\n \n def _build(name, src, srcdir):\n     cuda_lib_dirs = libcuda_dirs()\n-    cu_include_dir = \"/usr/local/cuda/include\"\n+    cu_include_dir = os.path.join(cuda_home_dirs(), \"include\")\n     suffix = sysconfig.get_config_var('EXT_SUFFIX')\n     so = os.path.join(srcdir, '{name}{suffix}'.format(name=name, suffix=suffix))\n     # try to avoid setuptools if possible"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -978,6 +978,8 @@ def where(condition: tl.tensor,\n         condition, x = broadcast_impl_value(condition, x, builder)\n \n     x, y = binary_op_type_checking_impl(x, y, builder, True, True)\n+    if not condition.type.is_block():\n+        condition, _ = broadcast_impl_value(condition, x, builder)\n     ret_ty = x.type\n     return tl.tensor(builder.create_select(condition.handle, x.handle, y.handle), ret_ty)\n "}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -260,12 +260,12 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     except KeyError:\n       # build dict of constant values\n       args = [{args}]\n-      configs = self._get_config(*args),\n+      all_args = {', '.join([f'{arg}' for arg in self.arg_names])},\n+      configs = self._get_config(*all_args),\n       constants = self._make_constants(constexpr_key)\n-      constants.update({{i: None for i, arg in enumerate(args) if arg is None}})\n+      constants.update({{i: None for i, arg in enumerate(all_args) if arg is None}})\n       constants.update({{i: 1 for i in configs[0].equal_to_1}})\n       # build kernel signature -- doesn't include specialized arguments\n-      all_args = {', '.join([f'{arg}' for arg in self.arg_names])}\n       signature = {{ i: self._type_of(_key_of(arg)) for i, arg in enumerate(all_args) if i not in self.constexprs }}\n       # build stub signature -- includes arguments that are specialized\n       for i, arg in constants.items():"}]