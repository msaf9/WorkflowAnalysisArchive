[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 561, "deletions": 194, "changes": 755, "file_content_changes": "@@ -64,6 +64,12 @@ Value createConstantF32(Location loc, PatternRewriter &rewriter, float v) {\n                                            rewriter.getF32FloatAttr(v));\n }\n \n+Value createConstantF64(Location loc, PatternRewriter &rewriter, float v) {\n+  auto type = type::f64Ty(rewriter.getContext());\n+  return rewriter.create<LLVM::ConstantOp>(loc, type,\n+                                           rewriter.getF64FloatAttr(v));\n+}\n+\n // Create an index type constant.\n static Value createIndexConstant(OpBuilder &builder, Location loc,\n                                  TypeConverter *converter, int64_t value) {\n@@ -87,6 +93,8 @@ void llPrintf(StringRef msg, ValueRange args,\n               ConversionPatternRewriter &rewriter);\n \n // Shortcuts for some commonly used LLVM ops to keep code simple and intuitive\n+#define inttoptr(...) rewriter.create<LLVM::IntToPtrOp>(loc, __VA_ARGS__)\n+#define ptrtoint(...) rewriter.create<LLVM::PtrToIntOp>(loc, __VA_ARGS__)\n #define zext(...) rewriter.create<LLVM::ZExtOp>(loc, __VA_ARGS__)\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n #define urem(...) rewriter.create<LLVM::URemOp>(loc, __VA_ARGS__)\n@@ -132,6 +140,7 @@ void llPrintf(StringRef msg, ValueRange args,\n #define f64_ty rewriter.getF64Type()\n #define vec_ty(type, num) VectorType::get(num, type)\n #define f32_val(...) LLVM::createConstantF32(loc, rewriter, __VA_ARGS__)\n+#define f64_val(...) LLVM::createConstantF64(loc, rewriter, __VA_ARGS__)\n #define void_ty(ctx) LLVM::LLVMVoidType::get(ctx)\n #define struct_ty(...) LLVM::LLVMStructType::getLiteral(ctx, __VA_ARGS__)\n \n@@ -347,7 +356,8 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n   return llvmStruct;\n }\n \n-// Delinearize on compile-time consts, assuming the order is [n, .. 2, 1, 0]\n+// TODO[goostavz]: to be deprecated\n+// delinearize supposing order is [n, .. , 2, 1, 0]\n template <typename T>\n static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n   // shape: {a, b, c, d}  ->  accMul: {b*c*d, c*d, d, 1}\n@@ -365,7 +375,40 @@ static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n   return multiDimIndex;\n }\n \n-// Linearize on compile-time consts, assuming the order is [n, .. 2, 1, 0]\n+// delinearize supposing order is [0, 1, .. , n]\n+template <typename T>\n+static SmallVector<T> getMultiDimIndexImpl(T linearIndex, ArrayRef<T> shape) {\n+  // shape: {a, b, c, d}  ->  accMul: {1, a, a*b, a*b*c}\n+  size_t rank = shape.size();\n+  T accMul = product(shape.drop_back());\n+  T linearRemain = linearIndex;\n+  SmallVector<T> multiDimIndex(rank);\n+  for (int i = rank - 1; i >= 0; --i) {\n+    multiDimIndex[i] = linearRemain / accMul;\n+    linearRemain = linearRemain % accMul;\n+    if (i != 0) {\n+      accMul = accMul / shape[i - 1];\n+    }\n+  }\n+  return multiDimIndex;\n+}\n+\n+template <typename T>\n+static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape,\n+                                       ArrayRef<unsigned> order) {\n+  size_t rank = shape.size();\n+  assert(rank == order.size());\n+  auto reordered = reorder(shape, order);\n+  auto reorderedMultiDim = getMultiDimIndexImpl<T>(linearIndex, reordered);\n+  SmallVector<T> multiDim(rank);\n+  for (unsigned i = 0; i < rank; ++i) {\n+    multiDim[order[i]] = reorderedMultiDim[i];\n+  }\n+  return multiDim;\n+}\n+\n+// TODO[goostavz]: to be deprecated\n+// linearize supposing order is [n, .. , 2, 1, 0]\n template <typename T>\n static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   assert(multiDimIndex.size() == shape.size());\n@@ -382,6 +425,30 @@ static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   return linearIndex;\n }\n \n+template <typename T>\n+static T getLinearIndexImpl(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n+  assert(multiDimIndex.size() == shape.size());\n+  // shape: {a, b, c, d}  ->  accMul: {1, a, a*b, a*b*c}\n+  size_t rank = shape.size();\n+  T accMul = product(shape.drop_back());\n+  T linearIndex = 0;\n+  for (int i = rank - 1; i >= 0; --i) {\n+    linearIndex += multiDimIndex[i] * accMul;\n+    if (i != 0) {\n+      accMul = accMul / shape[i - 1];\n+    }\n+  }\n+  return linearIndex;\n+}\n+\n+template <typename T>\n+static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape,\n+                        ArrayRef<unsigned> order) {\n+  assert(shape.size() == order.size());\n+  return getLinearIndexImpl<T>(reorder(multiDimIndex, order),\n+                               reorder(shape, order));\n+}\n+\n static Value storeShared(ConversionPatternRewriter &rewriter, Location loc,\n                          Value ptr, Value val, Value pred) {\n   MLIRContext *ctx = rewriter.getContext();\n@@ -457,6 +524,25 @@ struct ConvertTritonGPUOpToLLVMPatternBase {\n     }\n     return results;\n   }\n+\n+  static SharedMemoryObject\n+  getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n+                                  ConversionPatternRewriter &rewriter) {\n+    auto elems = getElementsFromStruct(loc, llvmStruct, rewriter);\n+    return SharedMemoryObject(/*base=*/elems[0],\n+                              /*strides=*/{elems.begin() + 1, elems.end()});\n+  }\n+\n+  static Value\n+  getStructFromSharedMemoryObject(Location loc,\n+                                  const SharedMemoryObject &smemObj,\n+                                  ConversionPatternRewriter &rewriter) {\n+    auto elems = smemObj.getElems();\n+    auto types = smemObj.getTypes();\n+    auto structTy =\n+        LLVM::LLVMStructType::getLiteral(rewriter.getContext(), types);\n+    return getStructFromElements(loc, elems, rewriter, structTy);\n+  }\n };\n \n template <typename SourceOp>\n@@ -613,6 +699,7 @@ class ConvertTritonGPUOpToLLVMPattern\n     auto sizePerThread = blockedLayout.getSizePerThread();\n     auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n     auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n+    auto order = blockedLayout.getOrder();\n \n     unsigned rank = shape.size();\n     SmallVector<unsigned> shapePerCTA = getShapePerCTA(blockedLayout);\n@@ -644,9 +731,9 @@ class ConvertTritonGPUOpToLLVMPattern\n       unsigned linearNanoTileId = n / totalSizePerThread;\n       unsigned linearNanoTileElemId = n % totalSizePerThread;\n       SmallVector<unsigned> multiDimNanoTileId =\n-          getMultiDimIndex<unsigned>(linearNanoTileId, tilesPerDim);\n-      SmallVector<unsigned> multiDimNanoTileElemId =\n-          getMultiDimIndex<unsigned>(linearNanoTileElemId, sizePerThread);\n+          getMultiDimIndex<unsigned>(linearNanoTileId, tilesPerDim, order);\n+      SmallVector<unsigned> multiDimNanoTileElemId = getMultiDimIndex<unsigned>(\n+          linearNanoTileElemId, sizePerThread, order);\n       for (unsigned k = 0; k < rank; ++k) {\n         unsigned reorderedMultiDimId =\n             multiDimNanoTileId[k] *\n@@ -830,25 +917,6 @@ class ConvertTritonGPUOpToLLVMPattern\n     return base;\n   }\n \n-  static SharedMemoryObject\n-  getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n-                                  ConversionPatternRewriter &rewriter) {\n-    auto elems = getElementsFromStruct(loc, llvmStruct, rewriter);\n-    return SharedMemoryObject(/*base=*/elems[0],\n-                              /*strides=*/{elems.begin() + 1, elems.end()});\n-  }\n-\n-  static Value\n-  getStructFromSharedMemoryObject(Location loc,\n-                                  const SharedMemoryObject &smemObj,\n-                                  ConversionPatternRewriter &rewriter) {\n-    auto elems = smemObj.getElems();\n-    auto types = smemObj.getTypes();\n-    auto structTy =\n-        LLVM::LLVMStructType::getLiteral(rewriter.getContext(), types);\n-    return getStructFromElements(loc, elems, rewriter, structTy);\n-  }\n-\n protected:\n   const Allocation *allocation;\n   Value smem;\n@@ -1360,8 +1428,8 @@ struct BroadcastOpConversion\n     Value result = op.result();\n     auto srcTy = op.src().getType().cast<RankedTensorType>();\n     auto resultTy = result.getType().cast<RankedTensorType>();\n-    auto srcLayout = srcTy.getEncoding();\n-    auto resultLayout = resultTy.getEncoding();\n+    auto srcLayout = srcTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n+    auto resultLayout = resultTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n     assert(srcLayout && (srcLayout == resultLayout) &&\n            \"Unexpected layout of BroadcastOp\");\n     auto srcShape = srcTy.getShape();\n@@ -1373,22 +1441,22 @@ struct BroadcastOpConversion\n     SmallVector<int64_t, 4> resultLogicalShape(2 * rank);\n     SmallVector<unsigned, 2> broadcastDims;\n     for (unsigned d = 0; d < rank; ++d) {\n-      unsigned resultShapePerCTA = triton::gpu::getSizePerThread(resultLayout)[d] *\n-                                   triton::gpu::getThreadsPerWarp(resultLayout)[d] *\n-                                   triton::gpu::getWarpsPerCTA(resultLayout)[d];\n+      unsigned resultShapePerCTA = resultLayout.getSizePerThread()[d] *\n+                                   resultLayout.getThreadsPerWarp()[d] *\n+                                   resultLayout.getWarpsPerCTA()[d];\n       int64_t numCtas = ceil<unsigned>(resultShape[d], resultShapePerCTA);\n       if (srcShape[d] != resultShape[d]) {\n         assert(srcShape[d] == 1);\n         broadcastDims.push_back(d);\n         srcLogicalShape[d] = 1;\n         srcLogicalShape[d + rank] =\n-            std::max<unsigned>(1, triton::gpu::getSizePerThread(srcLayout)[d]);\n+            std::max<unsigned>(1, srcLayout.getSizePerThread()[d]);\n       } else {\n         srcLogicalShape[d] = numCtas;\n-        srcLogicalShape[d + rank] = triton::gpu::getSizePerThread(resultLayout)[d];\n+        srcLogicalShape[d + rank] = resultLayout.getSizePerThread()[d];\n       }\n       resultLogicalShape[d] = numCtas;\n-      resultLogicalShape[d + rank] = triton::gpu::getSizePerThread(resultLayout)[d];\n+      resultLogicalShape[d + rank] = resultLayout.getSizePerThread()[d];\n     }\n     int64_t duplicates = 1;\n     SmallVector<int64_t, 2> broadcastSizes(broadcastDims.size() * 2);\n@@ -1881,8 +1949,6 @@ struct PrintfOpConversion\n   // currently support pointer, i8, i16, i32, i64, f16, bf16, f32, f64\n   std::string getFormatSubstr(Value value) const {\n     Type type = value.getType();\n-    unsigned width = type.getIntOrFloatBitWidth();\n-\n     if (type.isa<LLVM::LLVMPointerType>()) {\n       return \"%p\";\n     } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n@@ -1924,13 +1990,11 @@ struct PrintfOpConversion\n   promoteValue(ConversionPatternRewriter &rewriter, Value value) {\n     auto *context = rewriter.getContext();\n     auto type = value.getType();\n-    type.dump();\n-    unsigned width = type.getIntOrFloatBitWidth();\n     Value newOp = value;\n     Type newType = type;\n \n     bool bUnsigned = type.isUnsignedInteger();\n-    if (type.isIntOrIndex() && width < 32) {\n+    if (type.isIntOrIndex() && type.getIntOrFloatBitWidth() < 32) {\n       if (bUnsigned) {\n         newType = ui32_ty;\n         newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n@@ -2074,13 +2138,43 @@ struct GetProgramIdOpConversion\n   matchAndRewrite(triton::GetProgramIdOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n+    assert(op.axis() < 3);\n+\n     Value blockId = rewriter.create<::mlir::gpu::BlockIdOp>(\n-        loc, rewriter.getIndexType(), ::mlir::gpu::Dimension::x);\n+        loc, rewriter.getIndexType(), dims[op.axis()]);\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n     rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>(\n         op, TypeRange{llvmIndexTy}, ValueRange{blockId});\n     return success();\n   }\n+\n+  static constexpr mlir::gpu::Dimension dims[] = {mlir::gpu::Dimension::x,\n+                                                  mlir::gpu::Dimension::y,\n+                                                  mlir::gpu::Dimension::z};\n+};\n+\n+struct GetNumProgramsOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::GetNumProgramsOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::GetNumProgramsOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::GetNumProgramsOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    assert(op.axis() < 3);\n+\n+    Value blockId = rewriter.create<::mlir::gpu::BlockDimOp>(\n+        loc, rewriter.getIndexType(), dims[op.axis()]);\n+    auto llvmIndexTy = getTypeConverter()->getIndexType();\n+    rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>(\n+        op, TypeRange{llvmIndexTy}, ValueRange{blockId});\n+    return success();\n+  }\n+\n+  static constexpr mlir::gpu::Dimension dims[] = {mlir::gpu::Dimension::x,\n+                                                  mlir::gpu::Dimension::y,\n+                                                  mlir::gpu::Dimension::z};\n };\n \n struct AddPtrOpConversion\n@@ -2537,6 +2631,8 @@ class ElementwiseOpConversionBase\n     for (unsigned i = 0; i < elems; ++i) {\n       resultVals[i] = concreteThis->createDestOp(op, adaptor, rewriter, elemTy,\n                                                  operands[i], loc);\n+      if (!bool(resultVals[i]))\n+        return failure();\n     }\n     Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n@@ -2851,8 +2947,13 @@ void ConvertLayoutOpConversion::processReplica(\n   }\n   auto elemTy = type.getElementType();\n   bool isInt1 = elemTy.isInteger(1);\n+  bool isPtr = elemTy.isa<triton::PointerType>();\n+  auto llvmElemTyOrig = getTypeConverter()->convertType(elemTy);\n   if (isInt1)\n     elemTy = IntegerType::get(elemTy.getContext(), 8);\n+  else if (isPtr)\n+    elemTy = IntegerType::get(elemTy.getContext(), 64);\n+\n   auto llvmElemTy = getTypeConverter()->convertType(elemTy);\n \n   for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n@@ -2884,6 +2985,8 @@ void ConvertLayoutOpConversion::processReplica(\n           auto currVal = vals[elemId + linearCTAId * accumSizePerThread + v];\n           if (isInt1)\n             currVal = zext(llvmElemTy, currVal);\n+          else if (isPtr)\n+            currVal = ptrtoint(llvmElemTy, currVal);\n \n           valVec = insert_element(vecTy, valVec, currVal, idx_val(v));\n         }\n@@ -2896,6 +2999,8 @@ void ConvertLayoutOpConversion::processReplica(\n             currVal =\n                 icmp_ne(currVal, rewriter.create<LLVM::ConstantOp>(\n                                      loc, i8_ty, rewriter.getI8IntegerAttr(0)));\n+          else if (isPtr)\n+            currVal = inttoptr(llvmElemTyOrig, currVal);\n           vals[elemId + linearCTAId * accumSizePerThread + v] = currVal;\n         }\n       }\n@@ -3057,23 +3162,24 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n     }\n     unsigned linearIdxInNanoTile = i % srcAccumSizeInThreads;\n     auto multiDimIdxInNanoTile = getMultiDimIndex<unsigned>(\n-        linearIdxInNanoTile, srcBlockedLayout.getSizePerThread());\n+        linearIdxInNanoTile, srcBlockedLayout.getSizePerThread(), inOrd);\n     unsigned pos = multiDimIdxInNanoTile[inOrd[0]] % minVec;\n     multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n     unsigned wordVecIdx =\n-        getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep);\n+        getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep, inOrd);\n     wordVecs[wordVecIdx] =\n         insert_element(wordTy, wordVecs[wordVecIdx], inVals[i], idx_val(pos));\n \n     if (i % srcAccumSizeInThreads == srcAccumSizeInThreads - 1) {\n       // end of replication, store the vectors into shared memory\n       unsigned linearRepIdx = i / srcAccumSizeInThreads;\n-      auto multiDimRepIdx = getMultiDimIndex<unsigned>(linearRepIdx, reps);\n+      auto multiDimRepIdx =\n+          getMultiDimIndex<unsigned>(linearRepIdx, reps, inOrd);\n       for (unsigned linearWordIdx = 0; linearWordIdx < numWordsEachRep;\n            ++linearWordIdx) {\n         // step 1: recover the multidim_index from the index of input_elements\n         auto multiDimWordIdx =\n-            getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep);\n+            getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep, inOrd);\n         SmallVector<Value> multiDimIdx(2);\n         auto wordOffset0 = multiDimRepIdx[0] * srcShapePerCTA[0] +\n                            multiDimWordIdx[0] * (inOrd[0] == 0 ? minVec : 1);\n@@ -3083,12 +3189,12 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n         multiDimIdx[1] = add(multiDimOffsetFirstElem[1], idx_val(wordOffset1));\n \n         // step 2: do swizzling\n-        Value remained = urem(multiDimIdx[inOrd[0]], outVecVal);\n-        multiDimIdx[inOrd[0]] = udiv(multiDimIdx[inOrd[0]], outVecVal);\n-        Value off_1 = mul(multiDimIdx[inOrd[1]], idx_val(srcShape[inOrd[0]]));\n-        Value phaseId = udiv(multiDimIdx[inOrd[1]], idx_val(perPhase));\n+        Value remained = urem(multiDimIdx[outOrd[0]], outVecVal);\n+        multiDimIdx[outOrd[0]] = udiv(multiDimIdx[outOrd[0]], outVecVal);\n+        Value off_1 = mul(multiDimIdx[outOrd[1]], idx_val(srcShape[outOrd[0]]));\n+        Value phaseId = udiv(multiDimIdx[outOrd[1]], idx_val(perPhase));\n         phaseId = urem(phaseId, idx_val(maxPhase));\n-        Value off_0 = xor_(multiDimIdx[inOrd[0]], phaseId);\n+        Value off_0 = xor_(multiDimIdx[outOrd[0]], phaseId);\n         off_0 = mul(off_0, outVecVal);\n         remained = udiv(remained, minVecVal);\n         off_0 = add(off_0, mul(remained, minVecVal));\n@@ -3566,7 +3672,8 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     }\n \n     if (op.getType().cast<RankedTensorType>().getElementType().isF32() &&\n-        A.getType().cast<RankedTensorType>().getElementType().isF32())\n+        A.getType().cast<RankedTensorType>().getElementType().isF32() &&\n+        !op.allowTF32())\n       return convertFMADot(op, adaptor, rewriter);\n \n     llvm::report_fatal_error(\n@@ -4385,6 +4492,90 @@ struct MMA16816ConversionHelper {\n   }\n };\n \n+// Helper for conversion of FMA DotOp.\n+struct DotOpFMAConversionHelper {\n+  Attribute layout;\n+  MLIRContext *ctx{};\n+\n+  using ValueTable = std::map<std::pair<int, int>, Value>;\n+\n+  explicit DotOpFMAConversionHelper(Attribute layout)\n+      : layout(layout), ctx(layout.getContext()) {}\n+\n+  SmallVector<Value> getThreadIds(Value threadId,\n+                                  ArrayRef<unsigned> shapePerCTA,\n+                                  ArrayRef<unsigned> order,\n+                                  ConversionPatternRewriter &rewriter,\n+                                  Location loc) const;\n+\n+  Value loadA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n+\n+  Value loadB(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n+\n+  ValueTable getValueTableFromStruct(Value val, int K, int n0, int shapePerCTA,\n+                                     int sizePerThread,\n+                                     ConversionPatternRewriter &rewriter,\n+                                     Location loc) const;\n+\n+  Value getStructFromValueTable(ValueTable vals,\n+                                ConversionPatternRewriter &rewriter,\n+                                Location loc) const {\n+    SmallVector<Type> elemTypes(vals.size(), f32_ty);\n+    SmallVector<Value> elems;\n+    elems.reserve(vals.size());\n+    for (auto &item : vals) {\n+      elems.push_back(item.second);\n+    }\n+\n+    Type structTy = struct_ty(elemTypes);\n+    return getStructFromElements(loc, elems, rewriter, structTy);\n+  }\n+  // get number of elements per thread for $a or $b.\n+  static int getNumElemsPerThread(ArrayRef<int64_t> shape,\n+                                  DotOperandEncodingAttr dotOpLayout) {\n+    auto blockedLayout = dotOpLayout.getParent().cast<BlockedEncodingAttr>();\n+    auto shapePerCTA = getShapePerCTA(blockedLayout);\n+    auto sizePerThread = getSizePerThread(blockedLayout);\n+    auto order = blockedLayout.getOrder();\n+\n+    // TODO[Superjomn]: we assume the k aixs is fixed for $a and $b here, fix it\n+    // if not.\n+    int K = dotOpLayout.getOpIdx() == 0 ? shape[1] : shape[0];\n+    int otherDim = dotOpLayout.getOpIdx() == 1 ? shape[1] : shape[0];\n+\n+    bool isM = dotOpLayout.getOpIdx() == 0;\n+    int shapePerCTAMN = getShapePerCTAForMN(blockedLayout, isM);\n+    int sizePerThreadMN = getsizePerThreadForMN(blockedLayout, isM);\n+    return K * std::max<int>(otherDim / shapePerCTAMN, 1) * sizePerThreadMN;\n+  }\n+\n+  // Get shapePerCTA for M or N axis.\n+  static int getShapePerCTAForMN(BlockedEncodingAttr layout, bool isM) {\n+    auto order = layout.getOrder();\n+    auto shapePerCTA = getShapePerCTA(layout);\n+\n+    int mShapePerCTA =\n+        order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+    int nShapePerCTA =\n+        order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+    return isM ? mShapePerCTA : nShapePerCTA;\n+  }\n+\n+  // Get sizePerThread for M or N axis.\n+  static int getsizePerThreadForMN(BlockedEncodingAttr layout, bool isM) {\n+    auto order = layout.getOrder();\n+    auto sizePerThread = getSizePerThread(layout);\n+\n+    int mSizePerThread =\n+        order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+    int nSizePerThread =\n+        order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+    return isM ? mSizePerThread : nSizePerThread;\n+  }\n+};\n+\n Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n     triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n     ConversionPatternRewriter &rewriter, const MmaEncodingAttr &mmaLayout,\n@@ -4393,14 +4584,15 @@ Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n   Value src = op.src();\n   Value dst = op.result();\n   auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n-  // TODO[Superjomn]: the allowTF32 is not available in ConvertLayoutOp for it\n-  // is an attribute of DotOp.\n+  // TODO[Superjomn]: allowTF32 is not accessible here for it is an attribute of\n+  // an Op instance.\n   bool allowTF32 = false;\n   bool isHMMA = DotOpConversion::isDotHMMA(dstTensorTy, allowTF32,\n                                            mmaLayout.getVersion());\n \n   auto smemObj = getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n   Value res;\n+\n   if (!isOuter && mmaLayout.getVersion() == 2 && isHMMA) { // tensor core v2\n     MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n                                        rewriter, getTypeConverter(),\n@@ -4459,7 +4651,25 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n   } else if (auto blockedLayout =\n                  dotOperandLayout.getParent()\n                      .dyn_cast_or_null<BlockedEncodingAttr>()) {\n-    assert(false && \"Blocked layout is not supported yet\");\n+    // TODO[Superjomn]: the allowTF32 is not available in ConvertLayoutOp for it\n+    // is an attribute of DotOp.\n+    bool allowTF32 = false;\n+    bool isFMADot = dstTensorTy.getElementType().isF32() && !allowTF32;\n+    if (isFMADot) {\n+      auto dotOpLayout =\n+          dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+      auto blockedLayout = dotOpLayout.getParent().cast<BlockedEncodingAttr>();\n+      DotOpFMAConversionHelper helper(blockedLayout);\n+      auto thread = getThreadId(rewriter, loc);\n+      if (dotOpLayout.getOpIdx() == 0) { // $a\n+        res = helper.loadA(src, adaptor.src(), blockedLayout, thread, loc,\n+                           rewriter);\n+      } else { // $b\n+        res = helper.loadB(src, adaptor.src(), blockedLayout, thread, loc,\n+                           rewriter);\n+      }\n+    } else\n+      assert(false && \"Unsupported dot operand layout found\");\n   } else {\n     assert(false && \"Unsupported dot operand layout found\");\n   }\n@@ -4925,62 +5135,28 @@ DotOpMmaV1ConversionHelper::extractLoadedOperand(\n   return rcds;\n }\n \n-LogicalResult\n-DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n-                               ConversionPatternRewriter &rewriter) const {\n-  auto *ctx = rewriter.getContext();\n-  auto loc = op.getLoc();\n-  auto threadId = getThreadId(rewriter, loc);\n-\n-  using ValueTable = std::map<std::pair<int, int>, Value>;\n-\n-  auto A = op.a();\n-  auto B = op.b();\n-  auto C = op.c();\n-  auto D = op.getResult();\n-\n+Value DotOpFMAConversionHelper::loadA(\n+    Value A, Value llA, BlockedEncodingAttr dLayout, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n   auto aTensorTy = A.getType().cast<RankedTensorType>();\n-  auto bTensorTy = B.getType().cast<RankedTensorType>();\n-  auto cTensorTy = C.getType().cast<RankedTensorType>();\n-  auto dTensorTy = D.getType().cast<RankedTensorType>();\n-\n-  auto aShape = aTensorTy.getShape();\n-  auto bShape = bTensorTy.getShape();\n-  auto cShape = cTensorTy.getShape();\n-\n   auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto cLayout = cTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n-  auto dLayout = dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto aShape = aTensorTy.getShape();\n \n   auto aOrder = aLayout.getOrder();\n-  auto bOrder = bLayout.getOrder();\n-\n   auto order = dLayout.getOrder();\n \n   bool isARow = aOrder[0] == 1;\n-  bool isBRow = bOrder[0] == 1;\n \n   int strideAM = isARow ? aShape[1] : 1;\n   int strideAK = isARow ? 1 : aShape[0];\n-  int strideBN = isBRow ? 1 : bShape[0];\n-  int strideBK = isBRow ? bShape[1] : 1;\n   int strideA0 = isARow ? strideAK : strideAM;\n   int strideA1 = isARow ? strideAM : strideAK;\n-  int strideB0 = isBRow ? strideBN : strideBK;\n-  int strideB1 = isBRow ? strideBK : strideBN;\n   int lda = isARow ? strideAM : strideAK;\n-  int ldb = isBRow ? strideBK : strideBN;\n-  int aPerPhase = aLayout.getPerPhase();\n-  int aMaxPhase = aLayout.getMaxPhase();\n-  int bPerPhase = bLayout.getPerPhase();\n-  int bMaxPhase = bLayout.getMaxPhase();\n   int aNumPtr = 8;\n   int bNumPtr = 8;\n   int NK = aShape[1];\n \n   auto shapePerCTA = getShapePerCTA(dLayout);\n-\n   auto sizePerThread = getSizePerThread(dLayout);\n \n   Value _0 = i32_val(0);\n@@ -4989,19 +5165,7 @@ DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n   Value nContig = i32_val(sizePerThread[order[0]]);\n \n   // threadId in blocked layout\n-  SmallVector<Value> threadIds;\n-  {\n-    int dim = cShape.size();\n-    threadIds.resize(dim);\n-    for (unsigned k = 0; k < dim - 1; k++) {\n-      Value dimK = i32_val(shapePerCTA[order[k]]);\n-      Value rem = urem(threadId, dimK);\n-      threadId = udiv(threadId, dimK);\n-      threadIds[order[k]] = rem;\n-    }\n-    Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n-    threadIds[order[dim - 1]] = urem(threadId, dimK);\n-  }\n+  auto threadIds = getThreadIds(thread, shapePerCTA, order, rewriter, loc);\n \n   Value threadIdM = threadIds[0];\n   Value threadIdN = threadIds[1];\n@@ -5013,55 +5177,226 @@ DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n     aOff[i] = add(mul(offA0, i32_val(strideA0)), mul(offA1, i32_val(strideA1)));\n   }\n \n+  auto aSmem =\n+      ConvertTritonGPUOpToLLVMPatternBase::getSharedMemoryObjectFromStruct(\n+          loc, llA, rewriter);\n+\n+  Type f32PtrTy = ptr_ty(f32_ty);\n+  SmallVector<Value> aPtrs(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i)\n+    aPtrs[i] = gep(f32PtrTy, aSmem.base, aOff[i]);\n+\n+  ValueTable has;\n+  int M = aShape[aOrder[1]];\n+\n+  int mShapePerCTA = getShapePerCTAForMN(dLayout, true /*isM*/);\n+  int mSizePerThread = getsizePerThreadForMN(dLayout, true /*isM*/);\n+\n+  for (unsigned k = 0; k < NK; ++k) {\n+    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+      for (unsigned mm = 0; mm < mSizePerThread; ++mm)\n+        if (!has.count({m + mm, k})) {\n+          Value pa = gep(f32PtrTy, aPtrs[0],\n+                         i32_val((m + mm) * strideAM + k * strideAK));\n+          Value va = load(pa);\n+          has[{m + mm, k}] = va;\n+        }\n+  }\n+\n+  return getStructFromValueTable(has, rewriter, loc);\n+}\n+\n+Value DotOpFMAConversionHelper::loadB(\n+    Value B, Value llB, BlockedEncodingAttr dLayout, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto bShape = bTensorTy.getShape();\n+\n+  auto bOrder = bLayout.getOrder();\n+  auto order = dLayout.getOrder();\n+\n+  bool isBRow = bOrder[0] == 1;\n+\n+  int strideBN = isBRow ? 1 : bShape[0];\n+  int strideBK = isBRow ? bShape[1] : 1;\n+  int strideB0 = isBRow ? strideBN : strideBK;\n+  int strideB1 = isBRow ? strideBK : strideBN;\n+  int ldb = isBRow ? strideBK : strideBN;\n+  int bNumPtr = 8;\n+  int NK = bShape[0];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value mContig = i32_val(sizePerThread[order[1]]);\n+  Value nContig = i32_val(sizePerThread[order[0]]);\n+\n+  // threadId in blocked layout\n+  auto threadIds = getThreadIds(thread, shapePerCTA, order, rewriter, loc);\n+  Value threadIdN = threadIds[1];\n+\n   Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n   Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n   SmallVector<Value> bOff(bNumPtr);\n   for (int i = 0; i < bNumPtr; ++i) {\n     bOff[i] = add(mul(offB0, i32_val(strideB0)), mul(offB1, i32_val(strideB1)));\n   }\n \n-  auto aSmem = getSharedMemoryObjectFromStruct(loc, adaptor.a(), rewriter);\n-  auto bSmem = getSharedMemoryObjectFromStruct(loc, adaptor.b(), rewriter);\n+  auto bSmem =\n+      ConvertTritonGPUOpToLLVMPatternBase::getSharedMemoryObjectFromStruct(\n+          loc, llB, rewriter);\n \n   Type f32PtrTy = ptr_ty(f32_ty);\n-  SmallVector<Value> aPtrs(aNumPtr);\n-  for (int i = 0; i < aNumPtr; ++i)\n-    aPtrs[i] = gep(f32PtrTy, aSmem.base, aOff[i]);\n-\n   SmallVector<Value> bPtrs(bNumPtr);\n   for (int i = 0; i < bNumPtr; ++i)\n     bPtrs[i] = gep(f32PtrTy, bSmem.base, bOff[i]);\n \n+  int N = bShape[bOrder[0]];\n+  ValueTable hbs;\n+\n+  int nShapePerCTA = getShapePerCTAForMN(dLayout, false /*isM*/);\n+  int nSizePerThread = getsizePerThreadForMN(dLayout, false /*isM*/);\n+\n+  for (unsigned k = 0; k < NK; ++k)\n+    for (unsigned n = 0; n < N; n += nShapePerCTA)\n+      for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n+        Value pb = gep(f32PtrTy, bPtrs[0],\n+                       i32_val((n + nn) * strideBN + k * strideBK));\n+        Value vb = load(pb);\n+        hbs[{n + nn, k}] = vb;\n+      }\n+\n+  return getStructFromValueTable(hbs, rewriter, loc);\n+}\n+\n+DotOpFMAConversionHelper::ValueTable\n+DotOpFMAConversionHelper::getValueTableFromStruct(\n+    Value val, int K, int n0, int shapePerCTA, int sizePerThread,\n+    ConversionPatternRewriter &rewriter, Location loc) const {\n+  ValueTable res;\n+  auto elems = ConvertTritonGPUOpToLLVMPatternBase::getElementsFromStruct(\n+      loc, val, rewriter);\n+  int id = 0;\n+  std::set<std::pair<int, int>> keys; // ordered\n+  for (unsigned k = 0; k < K; ++k) {\n+    for (unsigned m = 0; m < n0; m += shapePerCTA)\n+      for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n+        keys.insert({m + mm, k});\n+      }\n+  }\n+\n+  for (auto &key : llvm::enumerate(keys)) {\n+    res[key.value()] = elems[key.index()];\n+  }\n+\n+  return res;\n+}\n+SmallVector<Value> DotOpFMAConversionHelper::getThreadIds(\n+    Value threadId, ArrayRef<unsigned int> shapePerCTA,\n+    ArrayRef<unsigned int> order, ConversionPatternRewriter &rewriter,\n+    Location loc) const {\n+  int dim = order.size();\n+  SmallVector<Value> threadIds(dim);\n+  for (unsigned k = 0; k < dim - 1; k++) {\n+    Value dimK = i32_val(shapePerCTA[order[k]]);\n+    Value rem = urem(threadId, dimK);\n+    threadId = udiv(threadId, dimK);\n+    threadIds[order[k]] = rem;\n+  }\n+  Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n+  threadIds[order[dim - 1]] = urem(threadId, dimK);\n+  return threadIds;\n+}\n+\n+LogicalResult\n+DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n+                               ConversionPatternRewriter &rewriter) const {\n+  auto *ctx = rewriter.getContext();\n+  auto loc = op.getLoc();\n+  auto threadId = getThreadId(rewriter, loc);\n+\n+  using ValueTable = std::map<std::pair<int, int>, Value>;\n+\n+  auto A = op.a();\n+  auto B = op.b();\n+  auto C = op.c();\n+  auto D = op.getResult();\n+\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto cTensorTy = C.getType().cast<RankedTensorType>();\n+  auto dTensorTy = D.getType().cast<RankedTensorType>();\n+\n+  auto aShape = aTensorTy.getShape();\n+  auto bShape = bTensorTy.getShape();\n+  auto cShape = cTensorTy.getShape();\n+\n   ValueTable has, hbs;\n+  int mShapePerCTA{-1}, nShapePerCTA{-1};\n+  int mSizePerThread{-1}, nSizePerThread{-1};\n+  ArrayRef<unsigned> aOrder, bOrder;\n+  Value llA, llB;\n+  BlockedEncodingAttr dLayout =\n+      dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto order = dLayout.getOrder();\n   auto cc = getElementsFromStruct(loc, adaptor.c(), rewriter);\n-  SmallVector<Value> ret = cc;\n-  // is this compatible with blocked layout?\n \n-  for (unsigned k = 0; k < NK; k++) {\n-    int z = 0;\n-    for (unsigned i = 0; i < cShape[order[1]]; i += shapePerCTA[order[1]])\n-      for (unsigned j = 0; j < cShape[order[0]]; j += shapePerCTA[order[0]])\n-        for (unsigned ii = 0; ii < sizePerThread[order[1]]; ++ii)\n-          for (unsigned jj = 0; jj < sizePerThread[order[0]]; ++jj) {\n-            unsigned m = order[0] == 1 ? i : j;\n-            unsigned n = order[0] == 1 ? j : i;\n-            unsigned mm = order[0] == 1 ? ii : jj;\n-            unsigned nn = order[0] == 1 ? jj : ii;\n-            if (!has.count({m + mm, k})) {\n-              Value pa = gep(f32PtrTy, aPtrs[0],\n-                             i32_val((m + mm) * strideAM + k * strideAK));\n-              Value va = load(pa);\n-              has[{m + mm, k}] = va;\n-            }\n-            if (!hbs.count({n + nn, k})) {\n-              Value pb = gep(f32PtrTy, bPtrs[0],\n-                             i32_val((n + nn) * strideBN + k * strideBK));\n-              Value vb = load(pb);\n-              hbs[{n + nn, k}] = vb;\n-            }\n+  DotOpFMAConversionHelper helper(dLayout);\n+  if (auto aDotOpLayout =\n+          aTensorTy.getEncoding()\n+              .dyn_cast<DotOperandEncodingAttr>()) { // get input from\n+                                                     // convert_layout\n+    auto bDotOpLayout =\n+        bTensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>();\n+    auto aLayout = aDotOpLayout.getParent().cast<BlockedEncodingAttr>();\n+    auto bLayout = bDotOpLayout.getParent().cast<BlockedEncodingAttr>();\n+\n+    assert(bLayout);\n+    llA = adaptor.a();\n+    llB = adaptor.b();\n+  } else if (auto aLayout =\n+                 aTensorTy.getEncoding()\n+                     .dyn_cast<SharedEncodingAttr>()) { // load input from smem\n+    auto bLayout = bTensorTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n+    assert(bLayout);\n+    Value thread = getThreadId(rewriter, loc);\n+    llA = helper.loadA(A, adaptor.a(), dLayout, thread, loc, rewriter);\n+    llB = helper.loadB(B, adaptor.b(), dLayout, thread, loc, rewriter);\n+  }\n+\n+  auto sizePerThread = getSizePerThread(dLayout);\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+\n+  int K = aShape[1];\n+  int M = aShape[0];\n+  int N = bShape[1];\n+\n+  mShapePerCTA = order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  mSizePerThread =\n+      order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+  nShapePerCTA = order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  nSizePerThread =\n+      order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+\n+  has = helper.getValueTableFromStruct(llA, K, M, mShapePerCTA, mSizePerThread,\n+                                       rewriter, loc);\n+  hbs = helper.getValueTableFromStruct(llB, K, N, nShapePerCTA, nSizePerThread,\n+                                       rewriter, loc);\n \n+  SmallVector<Value> ret = cc;\n+  for (unsigned k = 0; k < K; k++) {\n+    int z = 0;\n+    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+      for (unsigned n = 0; n < N; n += nShapePerCTA)\n+        for (unsigned mm = 0; mm < mSizePerThread; ++mm)\n+          for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n             ret[z] = rewriter.create<LLVM::FMulAddOp>(loc, has[{m + mm, k}],\n                                                       hbs[{n + nn, k}], ret[z]);\n+\n             ++z;\n           }\n   }\n@@ -5138,6 +5473,13 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n     auto ctx = type.getContext();\n     Attribute layout = type.getEncoding();\n     auto shape = type.getShape();\n+\n+    // TODO[Keren, Superjomn]: fix it, allowTF32 is not accessible here for it\n+    // is bound to an Op instance.\n+    bool allowTF32 = false;\n+    bool isFMADot = type.getElementType().isF32() && !allowTF32 &&\n+                    layout.dyn_cast_or_null<DotOperandEncodingAttr>();\n+\n     if (layout &&\n         (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n          layout.isa<MmaEncodingAttr>())) {\n@@ -5158,65 +5500,55 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n         types.push_back(IntegerType::get(ctx, 32));\n       }\n       return LLVM::LLVMStructType::getLiteral(ctx, types);\n-    } else if (auto mmaLayout = layout.dyn_cast_or_null<MmaEncodingAttr>()) {\n-      if (mmaLayout.getVersion() == 2) {\n-        auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(type);\n-        size_t fcSize = 4 * repM * repN;\n-        return LLVM::LLVMStructType::getLiteral(\n-            ctx, SmallVector<Type>(fcSize, convertType(type.getElementType())));\n-      }\n+    } else if (auto dotOpLayout =\n+                   layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n+      if (isFMADot) { // for parent is blocked layout\n+        int numElemsPerThread =\n+            DotOpFMAConversionHelper::getNumElemsPerThread(shape, dotOpLayout);\n \n-      if (mmaLayout.getVersion() == 1) {\n-        DotOpMmaV1ConversionHelper helper(mmaLayout);\n-        int repM = helper.getRepM(shape[0]);\n-        int repN = helper.getRepN(shape[1]);\n-        int elems = 8 * repM * repN;\n         return LLVM::LLVMStructType::getLiteral(\n-            ctx, SmallVector<Type>(elems, convertType(type.getElementType())));\n-      }\n-\n-      llvm::errs()\n-          << \"Unexpected mma layout detected in TritonToLLVMTypeConverter\";\n-      return llvm::None;\n-\n-    } else if (auto dot_op_layout =\n-                   layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n-      auto mmaLayout = dot_op_layout.getParent().cast<MmaEncodingAttr>();\n-      auto wpt = mmaLayout.getWarpsPerCTA();\n-      Type elemTy = convertType(type.getElementType());\n-      auto vecSize = 1;\n-      if (elemTy.getIntOrFloatBitWidth() == 16) {\n-        vecSize = 2;\n-      } else if (elemTy.getIntOrFloatBitWidth() == 8) {\n-        vecSize = 4;\n-      } else {\n-        assert(false && \"Unsupported element type\");\n-      }\n-      Type vecTy = vec_ty(elemTy, vecSize);\n-      if (mmaLayout.getVersion() == 2) {\n-        if (dot_op_layout.getOpIdx() == 0) { // $a\n-          int elems =\n-              MMA16816ConversionHelper::getANumElemsPerThread(type, wpt);\n-          return LLVM::LLVMStructType::getLiteral(\n-              ctx, SmallVector<Type>(elems, vecTy));\n+            ctx, SmallVector<Type>(numElemsPerThread, type::f32Ty(ctx)));\n+\n+      } else { // for parent is MMA layout\n+        auto mmaLayout = dotOpLayout.getParent().cast<MmaEncodingAttr>();\n+        auto wpt = mmaLayout.getWarpsPerCTA();\n+        Type elemTy = convertType(type.getElementType());\n+        auto vecSize = 1;\n+        if (elemTy.getIntOrFloatBitWidth() == 16) {\n+          vecSize = 2;\n+        } else if (elemTy.getIntOrFloatBitWidth() == 8) {\n+          vecSize = 4;\n+        } else {\n+          assert(false && \"Unsupported element type\");\n         }\n-        if (dot_op_layout.getOpIdx() == 1) { // $b\n-          int elems =\n-              MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt);\n-          return struct_ty(SmallVector<Type>(elems, vecTy));\n+        Type vecTy = vec_ty(elemTy, vecSize);\n+        if (mmaLayout.getVersion() == 2) {\n+          if (dotOpLayout.getOpIdx() == 0) { // $a\n+            int elems =\n+                MMA16816ConversionHelper::getANumElemsPerThread(type, wpt);\n+            return LLVM::LLVMStructType::getLiteral(\n+                ctx, SmallVector<Type>(elems, vecTy));\n+          }\n+          if (dotOpLayout.getOpIdx() == 1) { // $b\n+            int elems =\n+                MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt);\n+            return struct_ty(SmallVector<Type>(elems, vecTy));\n+          }\n         }\n-      }\n \n-      if (mmaLayout.getVersion() == 1) {\n-        DotOpMmaV1ConversionHelper helper(mmaLayout);\n+        if (mmaLayout.getVersion() == 1) {\n+          DotOpMmaV1ConversionHelper helper(mmaLayout);\n \n-        if (dot_op_layout.getOpIdx() == 0) { // $a\n-          int elems = helper.numElemsPerThreadA(type);\n-          return struct_ty(SmallVector<Type>(elems, vecTy));\n-        }\n-        if (dot_op_layout.getOpIdx() == 1) { // $b\n-          int elems = helper.numElemsPerThreadB(type);\n-          return struct_ty(SmallVector<Type>(elems, vecTy));\n+          if (dotOpLayout.getOpIdx() == 0) { // $a\n+            int elems = helper.numElemsPerThreadA(type);\n+            Type x2Ty = vec_ty(elemTy, 2);\n+            return struct_ty(SmallVector<Type>(elems, x2Ty));\n+          }\n+          if (dotOpLayout.getOpIdx() == 1) { // $b\n+            int elems = helper.numElemsPerThreadB(type);\n+            Type x2Ty = vec_ty(elemTy, 2);\n+            return struct_ty(SmallVector<Type>(elems, x2Ty));\n+          }\n         }\n       }\n \n@@ -5552,6 +5884,32 @@ struct FDivOpConversion\n   }\n };\n \n+struct ExpOpConversionApprox\n+    : ElementwiseOpConversionBase<mlir::math::ExpOp, LLVM::InlineAsmOp,\n+                                  ExpOpConversionApprox> {\n+  using Base = ElementwiseOpConversionBase<mlir::math::ExpOp, LLVM::InlineAsmOp,\n+                                           ExpOpConversionApprox>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::math::ExpOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    // For FP64 input, call __nv_expf for higher-precision calculation\n+    if (elemTy.getIntOrFloatBitWidth() == 64)\n+      return {};\n+    const double log2e = 1.4426950408889634;\n+    Value prod =\n+        rewriter.create<LLVM::FMulOp>(loc, f32_ty, operands[0], f32_val(log2e));\n+    PTXBuilder ptxBuilder;\n+    auto &exp2 = ptxBuilder.create<PTXInstr>(\"ex2\")->o(\"approx\").o(\"f32\");\n+    auto output = ptxBuilder.newOperand(\"=f\");\n+    auto input = ptxBuilder.newOperand(prod, \"f\");\n+    exp2(output, input);\n+    return ptxBuilder.launch(rewriter, loc, f32_ty, false);\n+  }\n+};\n+\n /// ====================== atomic_rmw codegen begin ==========================\n struct AtomicRMWOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>,\n@@ -5600,19 +5958,19 @@ struct AtomicRMWOpConversion\n     auto elemsPerThread = getElemsPerThread(val.getType());\n     SmallVector<Value> resultVals(elemsPerThread);\n     for (size_t i = 0; i < elemsPerThread; i += vec) {\n-      Value rmvVal = undef(vecTy);\n+      Value rmwVal = undef(vecTy);\n       for (int ii = 0; ii < vec; ++ii) {\n         Value iiVal = createIndexAttrConstant(\n             rewriter, loc, getTypeConverter()->getIndexType(), ii);\n-        rmvVal = insert_element(vecTy, rmvVal, valElements[i], iiVal);\n+        rmwVal = insert_element(vecTy, rmwVal, valElements[i + ii], iiVal);\n       }\n-      Value rmwPtr = bitcast(ptrElements[i], ptr_ty(valTy.getElementType()));\n+      Value rmwPtr = ptrElements[i];\n       std::string sTy;\n       PTXBuilder ptxBuilder;\n \n       auto *dstOpr = ptxBuilder.newOperand(\"=r\");\n-      auto *ptrOpr = ptxBuilder.newAddrOperand(rmwPtr, \"r\");\n-      auto *valOpr = ptxBuilder.newOperand(rmvVal, \"r\");\n+      auto *ptrOpr = ptxBuilder.newAddrOperand(rmwPtr, \"l\");\n+      auto *valOpr = ptxBuilder.newOperand(rmwVal, \"r\");\n \n       auto &atom = ptxBuilder.create<>(\"atom\")->global().o(\"gpu\");\n       auto rmwOp = stringifyRMWOp(atomicRmwAttr).str();\n@@ -5654,12 +6012,13 @@ struct AtomicRMWOpConversion\n         return failure();\n       }\n       atom.o(rmwOp).o(sTy);\n-\n-      atom(dstOpr, ptrOpr, valOpr);\n-      auto ret = ptxBuilder.launch(rewriter, loc, valueElemTy, false);\n+      //TODO:[dongdongl] actual mask support\n+      Value pred = int_val(1, 1);\n+      atom(dstOpr, ptrOpr, valOpr).predicate(pred);\n+      auto ret = ptxBuilder.launch(rewriter, loc, valueElemTy);\n       for (int ii = 0; ii < vec; ++ii) {\n         resultVals[i * vec + ii] =\n-            vec == 1 ? ret : extract_element(vecTy, ret, idx_val(ii));\n+            vec == 1 ? ret : extract_element(valueElemTy, ret, idx_val(ii));\n       }\n     }\n     Type structTy = getTypeConverter()->convertType(valueTy);\n@@ -5712,6 +6071,13 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n \n   patterns.add<CmpIOpConversion>(typeConverter, benefit);\n   patterns.add<CmpFOpConversion>(typeConverter, benefit);\n+\n+  // ExpOpConversionApprox will try using ex2.approx if the input type is FP32.\n+  // For FP64 input type, ExpOpConversionApprox will return failure and\n+  // ElementwiseOpConversion<math::ExpOp, math::ExpOp> defined below will call\n+  // __nv_expf for higher-precision calculation\n+  patterns.add<ExpOpConversionApprox>(typeConverter, benefit);\n+\n #define POPULATE_UNARY_OP(SRC_OP, DST_OP)                                      \\\n   patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n \n@@ -5748,6 +6114,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<ExtractSliceOpConversion>(typeConverter, allocation, smem,\n                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n+  patterns.add<GetNumProgramsOpConversion>(typeConverter, benefit);\n   patterns.add<InsertSliceAsyncOpConversion>(typeConverter, allocation, smem,\n                                              axisInfoAnalysis, benefit);\n   patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);"}]