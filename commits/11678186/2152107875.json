[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 20, "deletions": 20, "changes": 40, "file_content_changes": "@@ -86,38 +86,38 @@ def patch_kernel(template, to_replace):\n \n @pytest.mark.parametrize(\"dtype_x\", [dtype_x for dtype_x in dtypes])\n def test_empty_kernel(dtype_x, device='cuda'):\n-    IZE = 128\n+    SIZE = 128\n \n     @triton.jit\n-    def kernel(X, IZE: tl.constexpr):\n+    def kernel(X, SIZE: tl.constexpr):\n         pass\n-    x = to_triton(numpy_random(IZE, dtype_str=dtype_x), device=device)\n-    kernel[(1, )](x, IZE=IZE, num_warps=4)\n+    x = to_triton(numpy_random(SIZE, dtype_str=dtype_x), device=device)\n+    kernel[(1, )](x, SIZE=SIZE, num_warps=4)\n \n \n # generic test functions\n def _test_unary(dtype_x, expr, numpy_expr=None, device='cuda'):\n-    IZE = 128\n+    SIZE = 128\n     # define the kernel / launch-grid\n \n     @triton.jit\n-    def kernel(Z, X, IZE: tl.constexpr):\n-        off = tl.arange(0, IZE)\n+    def kernel(Z, X, SIZE: tl.constexpr):\n+        off = tl.arange(0, SIZE)\n         x = tl.load(X + off)\n         z = GENERATE_TEST_HERE\n         tl.store(Z + off, z)\n \n     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': expr})\n     # inputs\n-    x = numpy_random(IZE, dtype_str=dtype_x)\n+    x = numpy_random(SIZE, dtype_str=dtype_x)\n     if 'log' in expr:\n         x = np.abs(x) + 0.01\n     # reference result\n     z_ref = eval(expr if numpy_expr is None else numpy_expr)\n     # triton result\n     x_tri = to_triton(x, device=device)\n     z_tri = to_triton(np.empty_like(z_ref), device=device)\n-    kernel[(1, )](z_tri, x_tri, IZE=IZE, num_warps=4)\n+    kernel[(1, )](z_tri, x_tri, SIZE=SIZE, num_warps=4)\n     # compare\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n \n@@ -154,12 +154,12 @@ def _binary_op_dtype_override(a: str, b: str) -> Optional[np.dtype]:\n \n \n def _test_binary(dtype_x, dtype_y, expr, numpy_expr=None, mode_x='real', mode_y='real', device='cuda', y_low=None, y_high=None):\n-    IZE = 128\n+    SIZE = 128\n     # define the kernel / launch-grid\n \n     @triton.jit\n-    def kernel(Z, X, Y, IZE: tl.constexpr):\n-        off = tl.arange(0, IZE)\n+    def kernel(Z, X, Y, SIZE: tl.constexpr):\n+        off = tl.arange(0, SIZE)\n         x = tl.load(X + off)\n         y = tl.load(Y + off)\n         z = GENERATE_TEST_HERE\n@@ -168,8 +168,8 @@ def kernel(Z, X, Y, IZE: tl.constexpr):\n     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': expr})\n     # inputs\n     rs = RandomState(17)\n-    x = numpy_random(IZE, dtype_str=dtype_x, rs=rs)\n-    y = numpy_random(IZE, dtype_str=dtype_y, rs=rs, low=y_low, high=y_high)\n+    x = numpy_random(SIZE, dtype_str=dtype_x, rs=rs)\n+    y = numpy_random(SIZE, dtype_str=dtype_y, rs=rs, low=y_low, high=y_high)\n     if mode_x == 'nan':\n         x[:] = float('nan')\n     if mode_y == 'nan':\n@@ -182,8 +182,8 @@ def kernel(Z, X, Y, IZE: tl.constexpr):\n     # triton result\n     x_tri = to_triton(x, device=device)\n     y_tri = to_triton(y, device=device)\n-    z_tri = to_triton(np.empty(IZE, dtype=z_ref.dtype), device=device)\n-    kernel[(1, )](z_tri, x_tri, y_tri, IZE=IZE, num_warps=4)\n+    z_tri = to_triton(np.empty(SIZE, dtype=z_ref.dtype), device=device)\n+    kernel[(1, )](z_tri, x_tri, y_tri, SIZE=SIZE, num_warps=4)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), err_msg=expr, rtol=0.01)\n \n \n@@ -388,9 +388,9 @@ def test_index1d(expr, dtype_str, device='cuda'):\n \n     # Triton kernel\n     @triton.jit\n-    def kernel(Z, X, IZE: tl.constexpr):\n-        m = tl.arange(0, IZE)\n-        n = tl.arange(0, IZE)\n+    def kernel(Z, X, SIZE: tl.constexpr):\n+        m = tl.arange(0, SIZE)\n+        n = tl.arange(0, SIZE)\n         x = tl.load(X_PTR_EXPR)\n         z = GENERATE_TEST_HERE\n         tl.store(Z_PTR_EXPR, z)\n@@ -409,7 +409,7 @@ def kernel(Z, X, IZE: tl.constexpr):\n     # triton result\n     z_tri = to_triton(np.empty_like(z_ref), device=device)\n     x_tri = to_triton(x)\n-    kernel[(1, )](z_tri, x_tri, num_warps=1, IZE=shape_x[0])\n+    kernel[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n     # compare\n     assert (z_ref == to_numpy(z_tri)).all()\n "}]