[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 14, "deletions": 12, "changes": 26, "file_content_changes": "@@ -13,7 +13,7 @@ concurrency:\n   cancel-in-progress: ${{ github.ref != 'refs/heads/master' }}\n \n env:\n-  TRITON_USE_ASSERT_ENABLED_LLVM: 'TRUE'\n+  TRITON_USE_ASSERT_ENABLED_LLVM: \"TRUE\"\n \n jobs:\n   Runner-Preparation:\n@@ -71,25 +71,27 @@ jobs:\n         if: ${{ env.BACKEND != 'ROCM'}}\n         run: |\n           cd python\n+          python3 -m pip install --upgrade pip\n           python3 -m pip install cmake==3.24\n-          python3 -m pip install -vvv -e '.[tests]'\n+          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n       - name: Install Triton on ROCM\n         if: ${{ env.BACKEND == 'ROCM'}}\n         run: |\n           cd python\n+          python3 -m pip install --upgrade pip\n           python3 -m pip install cmake==3.24\n           python3 -m pip install torch==1.13.1 --index-url https://download.pytorch.org/whl/rocm5.2\n-          python3 -m pip install -vvv -e '.[tests]'\n+          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n       - name: Run lit tests\n         if: ${{ env.BACKEND != 'ROCM'}}\n         run: |\n           python3 -m pip install lit\n           cd python\n-          LIT_TEST_DIR=\"build/$(ls build)/test\"\n+          LIT_TEST_DIR=\"build/$(ls build | grep -i temp)/test\"\n           if [ ! -d \"${LIT_TEST_DIR}\" ]; then\n-            echo \"Not found '${LIT_TEST_DIR}'.  Did you change an installation method?\" ; exit -1\n+            echo \"Coult not find '${LIT_TEST_DIR}'\" ; exit -1\n           fi\n           lit -v \"${LIT_TEST_DIR}\"\n \n@@ -99,19 +101,19 @@ jobs:\n           cd python/test/unit\n           python3 -m pytest\n \n-      - name: Run python tests on ROCM\n-        if: ${{ env.BACKEND == 'ROCM'}}\n-        run: |\n-          cd python/test/unit/language\n-          python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py::test_empty_kernel\"\n-\n       - name: Run CXX unittests\n         if: ${{ env.BACKEND != 'ROCM'}}\n         run: |\n           cd python\n-          cd \"build/$(ls build)\"\n+          cd \"build/$(ls build | grep -i temp)\"\n           ctest\n \n+      - name: Run python tests on ROCM\n+        if: ${{ env.BACKEND == 'ROCM'}}\n+        run: |\n+          cd python/test/unit/language\n+          python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py::test_empty_kernel\"\n+\n       - name: Regression tests\n         if: ${{ contains(matrix.runner, 'A100') }}\n         run: |"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 27, "deletions": 19, "changes": 46, "file_content_changes": "@@ -16,6 +16,11 @@\n #######################\n \n \n+def print_perf(cur_ms, cur_util, ref_util):\n+    # print on the same line cur_ms, cur_util and ref_util with 3 decimal places\n+    print(f'{cur_ms:.3f} ms \\t cur: {cur_util:.3f} \\t ref: {ref_util:.3f} \\t dif={cur_util - ref_util:.3f}', end='\\t')\n+\n+\n def nvsmi(attrs):\n     attrs = ','.join(attrs)\n     cmd = ['nvidia-smi', '-i', '0', '--query-gpu=' + attrs, '--format=csv,noheader,nounits']\n@@ -55,21 +60,21 @@ def nvsmi(attrs):\n     # A100 in the CI server is slow-ish for some reason.\n     # On some other servers, we are getting about 90% peak for 8kx8x8k float16\n     'a100': {\n-        (512, 512, 512): {'float16': 0.08, 'float32': 0.13, 'int8': 0.05},\n-        (1024, 1024, 1024): {'float16': 0.33, 'float32': 0.35, 'int8': 0.169},\n-        (2048, 2048, 2048): {'float16': 0.62, 'float32': 0.57, 'int8': 0.34},\n-        (4096, 4096, 4096): {'float16': 0.81, 'float32': 0.75, 'int8': 0.46},\n-        (8192, 8192, 8192): {'float16': 0.77, 'float32': 0.85, 'int8': 0.51},\n+        (512, 512, 512): {'float16': 0.084, 'float32': 0.13, 'int8': 0.05},\n+        (1024, 1024, 1024): {'float16': 0.332, 'float32': 0.35, 'int8': 0.169},\n+        (2048, 2048, 2048): {'float16': 0.641, 'float32': 0.57, 'int8': 0.34},\n+        (4096, 4096, 4096): {'float16': 0.785, 'float32': 0.75, 'int8': 0.46},\n+        (8192, 8192, 8192): {'float16': 0.805, 'float32': 0.85, 'int8': 0.51},\n         # tall-skinny\n         (16, 1024, 1024): {'float16': 0.0077, 'float32': 0.0127, 'int8': 0.005},\n-        (16, 4096, 4096): {'float16': 0.0363, 'float32': 0.0457, 'int8': 0.0259},\n+        (16, 4096, 4096): {'float16': 0.044, 'float32': 0.0457, 'int8': 0.0259},\n         (16, 8192, 8192): {'float16': 0.07, 'float32': 0.0648, 'int8': 0.0431},\n-        (64, 1024, 1024): {'float16': 0.0271, 'float32': 0.0509, 'int8': 0.0169},\n-        (64, 4096, 4096): {'float16': 0.16, 'float32': 0.162, 'int8': 0.097},\n-        (64, 8192, 8192): {'float16': 0.30, 'float32': 0.257, 'int8': 0.174},\n-        (1024, 64, 1024): {'float16': 0.037, 'float32': 0.0458, 'int8': 0.017},\n+        (64, 1024, 1024): {'float16': 0.030, 'float32': 0.0509, 'int8': 0.0169},\n+        (64, 4096, 4096): {'float16': 0.163, 'float32': 0.162, 'int8': 0.097},\n+        (64, 8192, 8192): {'float16': 0.285, 'float32': 0.257, 'int8': 0.174},\n+        (1024, 64, 1024): {'float16': 0.033, 'float32': 0.0458, 'int8': 0.017},\n         (4096, 64, 4096): {'float16': 0.16, 'float32': 0.177, 'int8': 0.102},\n-        (8192, 64, 8192): {'float16': 0.25, 'float32': 0.230, 'int8': 0.177},\n+        (8192, 64, 8192): {'float16': 0.254, 'float32': 0.230, 'int8': 0.177},\n     }\n }\n \n@@ -94,9 +99,10 @@ def test_matmul(M, N, K, dtype_str):\n         a = torch.randn((M, K), dtype=dtype, device='cuda')\n         b = torch.randn((K, N), dtype=dtype, device='cuda')\n     fn = lambda: triton.ops.matmul(a, b)\n-    ms = triton.testing.do_bench(fn, warmup=100, rep=300)\n+    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n+    print_perf(ms, cur_gpu_util, ref_gpu_util)\n     triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n \n@@ -129,12 +135,12 @@ def _add(x_ptr, y_ptr, output_ptr, n_elements,\n         1024 * 65536: 0.939,\n     },\n     'a100': {\n-        1024 * 16: 0.008,\n-        1024 * 64: 0.034,\n+        1024 * 16: 0.010,\n+        1024 * 64: 0.040,\n         1024 * 256: 0.132,\n-        1024 * 1024: 0.352,\n-        1024 * 4096: 0.580,\n-        1024 * 16384: 0.782,\n+        1024 * 1024: 0.353,\n+        1024 * 4096: 0.605,\n+        1024 * 16384: 0.758,\n         1024 * 65536: 0.850,\n     }\n }\n@@ -150,9 +156,10 @@ def test_elementwise(N):\n     y = torch.randn_like(z)\n     grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n     fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n-    ms = triton.testing.do_bench(fn, warmup=100, rep=500)\n+    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=500)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n+    print_perf(ms, cur_gpu_util, ref_gpu_util)\n     triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n #######################\n@@ -189,7 +196,7 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n         o = fn()\n         do = torch.randn_like(o)\n         fn = lambda: o.backward(do, retain_graph=True)\n-    ms = triton.testing.do_bench(fn, warmup=100, rep=500)\n+    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=500)\n     # compute flops\n     flops_per_matmul = 2. * Z * H * N_CTX * N_CTX * D_HEAD * 0.5\n     total_flops = 2 * flops_per_matmul\n@@ -201,4 +208,5 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n+    print_perf(ms, cur_gpu_util, ref_gpu_util)\n     triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)"}]