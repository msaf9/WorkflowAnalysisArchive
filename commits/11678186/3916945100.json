[{"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 3, "deletions": 32, "changes": 35, "file_content_changes": "@@ -27,30 +27,17 @@ bool isZero(mlir::Value val) {\n   return false;\n }\n \n-bool isZero(mlir::Attribute attr) {\n-  if (auto intAttr = attr.dyn_cast<mlir::IntegerAttr>())\n-    return intAttr.getValue().isNullValue();\n-  if (auto floatAttr = attr.dyn_cast<mlir::FloatAttr>())\n-    return floatAttr.getValue().isZero();\n-  if (auto denseAttr = attr.dyn_cast<mlir::DenseElementsAttr>())\n-    return denseAttr.isSplat() && isZero(denseAttr.getSplatValue<Attribute>());\n-  return false;\n-}\n-\n bool isBroadcastConstantCombinable(Attribute value) {\n   if (auto denseValue = value.dyn_cast<DenseElementsAttr>()) {\n     return denseValue.isSplat();\n   }\n   return value.isa<FloatAttr, IntegerAttr>();\n }\n \n-bool isAddConstantCombinable(Attribute value, Value addRes) {\n-  // Check if Attribute value is zero and addRes is a constant.\n-  return isZero(value) && mlir::matchPattern(addRes, mlir::m_Constant());\n-}\n+DenseElementsAttr getConstantValue(Builder &builder, Attribute value,\n+                                   Value bcast_res) {\n \n-DenseElementsAttr getBroadcastConstantValue(Attribute value, Value bcastRes) {\n-  Type resType = bcastRes.getType();\n+  Type resType = bcast_res.getType();\n   DenseElementsAttr res;\n   if (auto denseValue = value.dyn_cast<DenseElementsAttr>()) {\n     res =\n@@ -61,14 +48,6 @@ DenseElementsAttr getBroadcastConstantValue(Attribute value, Value bcastRes) {\n   return res;\n }\n \n-DenseElementsAttr getAddConstantValue(Attribute value, Value addRes) {\n-  // Get constant value from addRes and construct a DenseElementsAttr.\n-  Type resType = value.getType();\n-  Attribute constantAttr;\n-  mlir::matchPattern(addRes, mlir::m_Constant(&constantAttr));\n-  return DenseElementsAttr::get(resType, constantAttr);\n-}\n-\n #include \"TritonCombine.inc\"\n \n } // anonymous namespace\n@@ -219,14 +198,6 @@ class CombineOpsPass : public TritonCombineOpsBase<CombineOpsPass> {\n     patterns.add<CombineSelectMaskedLoadPattern>(context);\n     // patterns.add<CombineAddPtrPattern>(context);\n     patterns.add<CombineBroadcastConstantPattern>(context);\n-    patterns.add<CombineAddIConstantPattern>(context);\n-    patterns.add<CombineAddFConstantPattern>(context);\n-    patterns.add<CombineAddIRevConstantPattern>(context);\n-    patterns.add<CombineAddFRevConstantPattern>(context);\n-    patterns.add<CombineExtFConstantPattern>(context);\n-    patterns.add<CombineTruncFConstantPattern>(context);\n-    patterns.add<CombineExtSIConstantPattern>(context);\n-    patterns.add<CombineExtUIConstantPattern>(context);\n \n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n       signalPassFailure();"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.td", "status": "modified", "additions": 3, "deletions": 46, "changes": 49, "file_content_changes": "@@ -39,53 +39,10 @@ def CombineDotAddFRevPattern : Pat<\n //         (TT_AddPtrOp $ptr, (Arith_AddIOp $idx0, $idx1))>;\n \n // broadcast(cst) => cst\n-def getBroadcastConstantValue : NativeCodeCall<\"getBroadcastConstantValue($0, $1)\">;\n+def getConstantValue : NativeCodeCall<\"getConstantValue($_builder, $0, $1)\">;\n def CombineBroadcastConstantPattern : Pat<\n-    (TT_BroadcastOp:$bcastRes (Arith_ConstantOp $value)),\n-    (Arith_ConstantOp (getBroadcastConstantValue $value, $bcastRes)),\n+    (TT_BroadcastOp:$bcast_res (Arith_ConstantOp $value)),\n+    (Arith_ConstantOp (getConstantValue $value, $bcast_res)),\n     [(Constraint<CPred<\"isBroadcastConstantCombinable($0)\">> $value)]>;\n \n-// extf/truncf/extsi/extui(constant(cst)) => constant(cst)\n-def CombineExtFConstantPattern : Pat<\n-    (Arith_ExtFOp:$res (Arith_ConstantOp $value)),\n-    (Arith_ConstantOp (getBroadcastConstantValue $value, $res)),\n-    [(Constraint<CPred<\"isBroadcastConstantCombinable($0)\">> $value)]>;\n-\n-def CombineTruncFConstantPattern : Pat<\n-    (Arith_TruncFOp:$res (Arith_ConstantOp $value)),\n-    (Arith_ConstantOp (getBroadcastConstantValue $value, $res)),\n-    [(Constraint<CPred<\"isBroadcastConstantCombinable($0)\">> $value)]>;\n-    \n-def CombineExtSIConstantPattern : Pat<\n-    (Arith_ExtSIOp:$res (Arith_ConstantOp $value)),\n-    (Arith_ConstantOp (getBroadcastConstantValue $value, $res)),\n-    [(Constraint<CPred<\"isBroadcastConstantCombinable($0)\">> $value)]>;\n-\n-def CombineExtUIConstantPattern : Pat<\n-    (Arith_ExtUIOp:$res (Arith_ConstantOp $value)),\n-    (Arith_ConstantOp (getBroadcastConstantValue $value, $res)),\n-    [(Constraint<CPred<\"isBroadcastConstantCombinable($0)\">> $value)]>;\n-\n-// add(cst, constant(0)) => constant(cst)\n-def getAddConstantValue : NativeCodeCall<\"getAddConstantValue($0, $1)\">;\n-def CombineAddIConstantPattern : Pat<\n-    (Arith_AddIOp $a, (Arith_ConstantOp $value)),\n-    (Arith_ConstantOp (getAddConstantValue $value, $a)),\n-    [(Constraint<CPred<\"isAddConstantCombinable($0, $1)\">> $value, $a)]>;\n-\n-def CombineAddFConstantPattern : Pat<\n-    (Arith_AddFOp $a, (Arith_ConstantOp $value)),\n-    (Arith_ConstantOp (getAddConstantValue $value, $a)),\n-    [(Constraint<CPred<\"isAddConstantCombinable($0, $1)\">> $value, $a)]>;\n-\n-def CombineAddIRevConstantPattern : Pat<\n-    (Arith_AddIOp (Arith_ConstantOp $value), $b),\n-    (Arith_ConstantOp (getAddConstantValue $value, $b)),\n-    [(Constraint<CPred<\"isAddConstantCombinable($0, $1)\">> $value, $b)]>;\n-\n-def CombineAddFRevConstantPattern : Pat<\n-    (Arith_AddFOp (Arith_ConstantOp $value), $b),\n-    (Arith_ConstantOp (getAddConstantValue $value, $b)),\n-    [(Constraint<CPred<\"isAddConstantCombinable($0, $1)\">> $value, $b)]>;\n-\n #endif"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 30, "deletions": 5, "changes": 35, "file_content_changes": "@@ -468,6 +468,12 @@ void init_triton_ir(py::module &&m) {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n                  loc, v, self.getI1Type()));\n            })\n+      .def(\"get_int8\",\n+           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 loc, v, self.getI8Type()));\n+           })\n       .def(\"get_int32\",\n            [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n@@ -480,13 +486,32 @@ void init_triton_ir(py::module &&m) {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n                  loc, v, self.getI64Type()));\n            })\n-      // .def(\"get_uint32\", &ir::builder::get_int32, ret::reference)\n-      // .def(\"get_float16\", &ir::builder::get_float16, ret::reference)\n-      .def(\"get_float32\",\n+      .def(\"get_bf16\",\n            [](mlir::OpBuilder &self, float v) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::ConstantOp>(\n-                 loc, self.getF32FloatAttr(v));\n+             auto type = self.getBF16Type();\n+             return self.create<mlir::arith::ConstantFloatOp>(\n+                 loc,\n+                 mlir::APFloat(type.getFloatSemantics(), std::to_string(v)),\n+                 type);\n+           })\n+      .def(\"get_fp16\",\n+           [](mlir::OpBuilder &self, float v) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             auto type = self.getF16Type();\n+             return self.create<mlir::arith::ConstantFloatOp>(\n+                 loc,\n+                 mlir::APFloat(type.getFloatSemantics(), std::to_string(v)),\n+                 type);\n+           })\n+      .def(\"get_fp32\",\n+           [](mlir::OpBuilder &self, float v) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             auto type = self.getF32Type();\n+             return self.create<mlir::arith::ConstantFloatOp>(\n+                 loc,\n+                 mlir::APFloat(type.getFloatSemantics(), std::to_string(v)),\n+                 type);\n            })\n       .def(\"get_null_value\",\n            [](mlir::OpBuilder &self, mlir::Type type) -> mlir::Value {"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1241,7 +1241,7 @@ def _kernel(out):\n         out_ptr = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n         tl.store(out_ptr, c)\n \n-    kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"(tl.zeros((32, 32), tl.{dtype_str}) + 1.0).to(tl.{dtype_str})\"})\n+    kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.fill((32, 32), 1.0, tl.{dtype_str})\"})\n     a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n     b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n     out_ref = torch.matmul(a, b)"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -31,6 +31,7 @@\n     dot,\n     dtype,\n     exp,\n+    fill,\n     fdiv,\n     float16,\n     float32,\n@@ -124,6 +125,7 @@\n     \"float32\",\n     \"float64\",\n     \"float8\",\n+    \"fill\",\n     \"function_type\",\n     \"int1\",\n     \"int16\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 23, "deletions": 1, "changes": 24, "file_content_changes": "@@ -26,7 +26,7 @@ def _to_tensor(x, builder):\n         else:\n             raise RuntimeError(f'Nonrepresentable integer {x}.')\n     elif isinstance(x, float):\n-        return tensor(builder.get_float32(x), float32)\n+        return tensor(builder.get_fp32(x), float32)\n     elif isinstance(x, constexpr):\n         return _to_tensor(x.value, builder)\n     elif isinstance(x, tensor):\n@@ -709,6 +709,28 @@ def zeros(shape, dtype, _builder=None):\n     dtype = _constexpr_to_value(dtype)\n     return semantic.zeros(shape, dtype, _builder)\n \n+@builtin\n+def fill(shape, value, dtype, _builder=None):\n+    \"\"\"\n+    Returns a tensor filled with the scalar value for the given :code:`shape` and :code:`dtype`.\n+\n+    :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n+    :value value: Value to fill the array with, must be a constant\n+    :type shape: tuple of ints\n+    :param dtype: Data-type of the new array, e.g., :code:`tl.float16`\n+    :type dtype: DType\n+    \"\"\"\n+    if not isinstance(value, constexpr):\n+        raise TypeError(f\"Value must have type `constexpr`\")\n+    for i, d in enumerate(shape):\n+        if not isinstance(d, constexpr):\n+            raise TypeError(f\"Shape element {i} must have type `constexpr`\")\n+        if not isinstance(d.value, int):\n+            raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n+    shape = [x.value for x in shape]\n+    dtype = _constexpr_to_value(dtype)\n+    return semantic.fill(shape, value, dtype, _builder)\n+\n \n # -----------------------\n # Shape Manipulation"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 13, "deletions": 5, "changes": 18, "file_content_changes": "@@ -476,6 +476,14 @@ def zeros(shape: List[int], dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n     ret_ty = tl.block_type(dtype, shape)\n     return tl.tensor(builder.create_splat(_0, shape), ret_ty)\n \n+\n+def fill(shape: List[int], value: tl.constexpr, dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n+    get_value_fn = getattr(builder, f\"get_{dtype.name}\")\n+    _value = get_value_fn(value.value)\n+    ret_ty = tl.block_type(dtype, shape)\n+    return tl.tensor(builder.create_splat(_value, shape), ret_ty)\n+\n+\n # ===----------------------------------------------------------------------===//\n #                               Shape Manipulation\n # ===----------------------------------------------------------------------===//\n@@ -891,8 +899,8 @@ def atomic_max(ptr: tl.tensor,\n     # return atomic_umin(i_ptr, i_val) if val < 0\n     i_val = bitcast(val, tl.int32, builder)\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n-    pos = greater_equal(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n-    neg = less_than(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n+    pos = greater_equal(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n+    neg = less_than(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n     pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX, i_ptr.handle, i_val.handle, and_(mask, pos, builder).handle), i_val.type)\n     neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN, i_ptr.handle, i_val.handle, and_(mask, neg, builder).handle), i_val.type)\n     return where(pos, pos_ret, neg_ret, builder)\n@@ -923,8 +931,8 @@ def atomic_min(ptr: tl.tensor,\n     # return atomic_umax(i_ptr, i_val) if val < 0\n     i_val = bitcast(val, tl.int32, builder)\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n-    pos = greater_equal(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n-    neg = less_than(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n+    pos = greater_equal(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n+    neg = less_than(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n     pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MIN,\n                                                   i_ptr.handle,\n                                                   i_val.handle,\n@@ -998,7 +1006,7 @@ def dot(lhs: tl.tensor,\n         _0 = builder.get_int32(0)\n         ret_scalar_ty = tl.int32\n     else:\n-        _0 = builder.get_float32(0)\n+        _0 = builder.get_fp32(0)\n         ret_scalar_ty = tl.float32\n     M = lhs.type.shape[0]\n     N = rhs.type.shape[1]"}]