[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -4,9 +4,13 @@ on:\n   workflow_dispatch:\n   pull_request:\n     branches:\n-      - master\n+      - main\n       - triton-mlir\n \n+concurrency:\n+  group: ${{ github.ref }}\n+  cancel-in-progress: ${{ github.ref != 'refs/heads/master' }}\n+\n jobs:\n   Runner-Preparation:\n     runs-on: ubuntu-latest\n@@ -55,8 +59,8 @@ jobs:\n         if: ${{ matrix.runner != 'macos-10.15' }}\n         run: |\n           pip install clang-format\n-          find . -regex '.*\\.\\(cpp\\|hpp\\|h\\|cc\\)' -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file --dry-run -Werror -i ||\n-          (echo '::error title=Style issues:: Please run `find . -regex \".*\\.\\(cpp\\|hpp\\|h\\|cc\\)\" -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file -i`' ; exit 1)\n+          find . -regex '.*\\.\\(cpp\\|hpp\\|h\\|cc\\)' -not -path \"./python/triton/*\" -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file --dry-run -Werror -i ||\n+          (echo '::error title=Style issues:: Please run `find . -regex \".*\\.\\(cpp\\|hpp\\|h\\|cc\\)\" -not -path \"./python/triton/*\" -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file -i`' ; exit 1)\n \n       - name: Flake8\n         if: ${{ matrix.runner != 'macos-10.15' }}"}, {"filename": ".github/workflows/wheels.yml", "status": "modified", "additions": 6, "deletions": 7, "changes": 13, "file_content_changes": "@@ -1,8 +1,8 @@\n name: Wheels\n on:\n   workflow_dispatch:\n-  schedule:    \n-    - cron: \"0 0 * * *\"\n+  #schedule:    \n+  #  - cron: \"0 0 * * *\"\n \n jobs:\n \n@@ -26,15 +26,14 @@ jobs:\n \n       - name: Build wheels\n         run: |\n-          export CIBW_MANYLINUX_X86_64_IMAGE=\"manylinux2014\"\n-          export CIBW_MANYLINUX_PYPY_X86_64_IMAGE=\"manylinux2014\"\n-          export CIBW_BEFORE_BUILD=\"pip install cmake;\\\n-                                    yum install -y llvm11 llvm11-devel llvm11-static llvm11-libs zlib-devel;\"\n+          export CIBW_MANYLINUX_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n+          #export CIBW_MANYLINUX_PYPY_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n+          export CIBW_BEFORE_BUILD=\"pip install cmake;\"\n           export CIBW_SKIP=\"{cp,pp}35-*\"\n           export CIBW_BUILD=\"{cp,pp}3*-manylinux_x86_64\"\n           python3 -m cibuildwheel python --output-dir wheelhouse\n \n \n       - name: Upload wheels to PyPI\n         run: |\n-          python3 -m twine upload wheelhouse/* --skip-existing\n\\ No newline at end of file\n+          python3 -m twine upload wheelhouse/* -u __token__ -p ${{ secrets.PYPY_API_TOKEN }}\n\\ No newline at end of file"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 21, "deletions": 13, "changes": 34, "file_content_changes": "@@ -1,4 +1,7 @@\n cmake_minimum_required(VERSION 3.6)\n+\n+cmake_policy(SET CMP0116 OLD)\n+\n include(ExternalProject)\n \n set(CMAKE_CXX_STANDARD 17)\n@@ -17,7 +20,6 @@ option(TRITON_BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n \n # Ensure Python3 vars are set correctly\n #  used conditionally in this file and by lit tests\n-find_package(Python3 REQUIRED COMPONENTS Development Interpreter)\n \n # Customized release build type with assertions: TritonRelBuildWithAsserts\n set(CMAKE_C_FLAGS_TRITONRELBUILDWITHASSERTS \"-O2 -g\")\n@@ -41,8 +43,8 @@ include_directories(${PYBIND11_INCLUDE_DIR})\n \n if(WIN32)\n     SET(BUILD_SHARED_LIBS OFF)\n-    include_directories(${CMAKE_CURRENT_SOURCE_DIR}/deps/dlfcn-win32/src)\n-    add_subdirectory(deps/dlfcn-win32/src ${CMAKE_BINARY_DIR}/dlfcn-win32)\n+    find_package(dlfcn-win32 REQUIRED)\n+    set(CMAKE_DL_LIBS dlfcn-win32::dl)\n endif()\n \n set(CMAKE_CXX_FLAGS \"${CMAKE_C_FLAGS} -D__STDC_FORMAT_MACROS  -fPIC -std=gnu++17 -fvisibility=hidden -fvisibility-inlines-hidden\")\n@@ -72,6 +74,8 @@ if (NOT MLIR_DIR)\n       find_package(LLVM 11 REQUIRED COMPONENTS \"nvptx;amdgpu\")\n     endif()\n     message(STATUS \"Found LLVM ${LLVM_PACKAGE_VERSION}\")\n+    # FindLLVM outputs LLVM_LIBRARY_DIRS but we expect LLVM_LIBRARY_DIR here\n+    set(LLVM_LIBRARY_DIR ${LLVM_LIBRARY_DIRS})\n     if(APPLE)\n       set(CMAKE_OSX_DEPLOYMENT_TARGET \"10.14\")\n     endif()\n@@ -146,14 +150,14 @@ if(TRITON_BUILD_PYTHON_MODULE)\n     if (PYTHON_INCLUDE_DIRS)\n       include_directories(${PYTHON_INCLUDE_DIRS})\n     else()\n+      find_package(Python3 REQUIRED COMPONENTS Development Interpreter)\n       include_directories(${Python3_INCLUDE_DIRS})\n       link_directories(${Python3_LIBRARY_DIRS})\n       link_libraries(${Python3_LIBRARIES})\n       add_link_options(${Python3_LINK_OPTIONS})\n     endif()\n endif()\n \n-\n # # Triton\n # file(GLOB_RECURSE LIBTRITON_SRC lib/*.cc)\n # if (WIN32 AND TRITON_BUILD_PYTHON_MODULE)\n@@ -198,8 +202,7 @@ get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)\n \n if(TRITON_BUILD_PYTHON_MODULE)\n   add_library(triton SHARED ${PYTHON_SRC})\n-\n-  target_link_libraries(triton\n+  set(TRITON_LIBRARIES\n     TritonAnalysis\n     TritonTransforms\n     TritonGPUTransforms\n@@ -210,24 +213,29 @@ if(TRITON_BUILD_PYTHON_MODULE)\n     # optimizations\n     MLIRPass\n     MLIRTransforms\n-    MLIRLLVMIR\n+    MLIRLLVMDialect\n     MLIRSupport\n     MLIRTargetLLVMIRExport\n     MLIRExecutionEngine\n     MLIRMathToLLVM\n     MLIRNVVMToLLVMIRTranslation\n     MLIRIR\n   )\n-\n-  target_link_options(triton PRIVATE ${LLVM_LDFLAGS})\n-\n   if(WIN32)\n-      target_link_libraries(triton PRIVATE ${LLVM_LIBRARIES} dl) # dl is from dlfcn-win32\n+    target_link_libraries(triton PRIVATE ${LLVM_LIBRARIES} ${CMAKE_DL_LIBS}\n+      ${TRITON_LIBRARIES}\n+    )\n   elseif(APPLE)\n-      target_link_libraries(triton ${LLVM_LIBRARIES} z)\n+    target_link_libraries(triton ${LLVM_LIBRARIES} z\n+      ${TRITON_LIBRARIES}\n+    )\n   else()\n-      target_link_libraries(triton ${LLVM_LIBRARIES} z stdc++fs)\n+    target_link_libraries(triton ${LLVM_LIBRARIES} z stdc++fs\n+      ${TRITON_LIBRARIES}\n+    )\n   endif()\n+  \n+  target_link_options(triton PRIVATE ${LLVM_LDFLAGS})\n endif()\n \n if(TRITON_BUILD_PYTHON_MODULE AND NOT WIN32)"}, {"filename": "bin/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -48,7 +48,7 @@ llvm_update_compile_flags(triton-translate)\n          # MLIR core\n          MLIROptLib\n          MLIRIR\n-         MLIRLLVMIR\n+         MLIRLLVMDialect\n          MLIRPass\n          MLIRSupport\n          MLIRTransforms"}, {"filename": "bin/FileCheck/FileCheck.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -19,6 +19,7 @@\n #include \"llvm/Support/CommandLine.h\"\n #include \"llvm/Support/InitLLVM.h\"\n #include \"llvm/Support/Process.h\"\n+#include \"llvm/Support/SourceMgr.h\"\n #include \"llvm/Support/WithColor.h\"\n #include \"llvm/Support/raw_ostream.h\"\n #include <cmath>\n@@ -360,6 +361,8 @@ static std::string GetCheckTypeAbbreviation(Check::FileCheckType Ty) {\n     return \"bad-not\";\n   case Check::CheckBadCount:\n     return \"bad-count\";\n+  case Check::CheckMisspelled:\n+    return \"misspelled\";\n   case Check::CheckNone:\n     llvm_unreachable(\"invalid FileCheckType\");\n   }"}, {"filename": "bin/triton-opt.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -8,7 +8,7 @@\n \n #include \"mlir/IR/Dialect.h\"\n #include \"mlir/InitAllPasses.h\"\n-#include \"mlir/Support/MlirOptMain.h\"\n+#include \"mlir/Tools/mlir-opt/MlirOptMain.h\"\n \n namespace mlir {\n namespace test {\n@@ -33,8 +33,8 @@ int main(int argc, char **argv) {\n   // TODO: register Triton & TritonGPU passes\n   mlir::DialectRegistry registry;\n   registry.insert<mlir::triton::TritonDialect,\n-                  mlir::triton::gpu::TritonGPUDialect, mlir::math::MathDialect,\n-                  mlir::arith::ArithmeticDialect, mlir::StandardOpsDialect,\n+                  mlir::triton::gpu::TritonGPUDialect, mlir::func::FuncDialect,\n+                  mlir::math::MathDialect, mlir::arith::ArithDialect,\n                   mlir::scf::SCFDialect, mlir::gpu::GPUDialect>();\n \n   return mlir::asMainReturnCode(mlir::MlirOptMain("}, {"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -3,7 +3,7 @@\n #include \"mlir/IR/AsmState.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/Dialect.h\"\n-#include \"mlir/Parser.h\"\n+#include \"mlir/Parser/Parser.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Support/FileUtilities.h\"\n@@ -36,9 +36,9 @@ OwningOpRef<ModuleOp> loadMLIRModule(llvm::StringRef inputFilename,\n   }\n \n   mlir::DialectRegistry registry;\n-  registry.insert<TritonDialect, triton::gpu::TritonGPUDialect,\n-                  mlir::math::MathDialect, arith::ArithmeticDialect,\n-                  StandardOpsDialect, scf::SCFDialect>();\n+  registry\n+      .insert<TritonDialect, triton::gpu::TritonGPUDialect,\n+              mlir::math::MathDialect, arith::ArithDialect, scf::SCFDialect>();\n \n   context.appendDialectRegistry(registry);\n \n@@ -50,7 +50,8 @@ OwningOpRef<ModuleOp> loadMLIRModule(llvm::StringRef inputFilename,\n     context.loadAllAvailableDialects();\n     context.allowUnregisteredDialects();\n \n-    OwningOpRef<ModuleOp> module(parseSourceFile(sourceMgr, &context));\n+    OwningOpRef<ModuleOp> module =\n+        parseSourceFile<ModuleOp>(sourceMgr, &context);\n     if (!module) {\n       llvm::errs() << \"Parse MLIR file failed.\";\n       return nullptr;"}, {"filename": "cmake/FindLLVM.cmake", "status": "modified", "additions": 5, "deletions": 7, "changes": 12, "file_content_changes": "@@ -33,14 +33,12 @@\n # We also want an user-specified LLVM_ROOT_DIR to take precedence over the\n # system default locations such as /usr/local/bin. Executing find_program()\n # multiples times is the approach recommended in the docs.\n-set(llvm_config_names llvm-config-12.0 llvm-config120 llvm-config-12 llvm-config-12-64\n-                      llvm-config-11.0 llvm-config110 llvm-config-11 llvm-config-11-64\n-                      llvm-config-10.0 llvm-config100 llvm-config-10 llvm-config-10-64\n-                      llvm-config-9.0 llvm-config90 llvm-config-9 llvm-config-9-64\n-                      llvm-config-8.0 llvm-config80 llvm-config-8 llvm-config-8-64\n-                      llvm-config-7.0 llvm-config70 llvm-config-7 llvm-config-7-64\n-                      llvm-config-6.0 llvm-config60\n+set(llvm_config_names llvm-config-6.0 llvm-config60\n                       llvm-config)\n+foreach(v RANGE 7 17)\n+    # names like llvm-config-7.0 llvm-config70 llvm-config-7 llvm-config-7-64\n+    list(PREPEND llvm_config_names llvm-config-${v}.0 llvm-config${v}0 llvm-config-${v} llvm-config-${v}-64)\n+endforeach()\n find_program(LLVM_CONFIG\n     NAMES ${llvm_config_names}\n     PATHS ${LLVM_ROOT_DIR}/bin NO_DEFAULT_PATH"}, {"filename": "docs/getting-started/tutorials/parallel_reduction.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "include/triton/Analysis/Alias.h", "status": "modified", "additions": 21, "deletions": 6, "changes": 27, "file_content_changes": "@@ -2,7 +2,7 @@\n #define TRITON_ANALYSIS_ALIAS_H\n \n #include \"mlir/Analysis/AliasAnalysis.h\"\n-#include \"mlir/Analysis/DataFlowAnalysis.h\"\n+#include \"mlir/Analysis/DataFlow/SparseAnalysis.h\"\n #include \"llvm/ADT/DenseSet.h\"\n \n namespace mlir {\n@@ -21,14 +21,18 @@ class AliasInfo {\n   }\n \n   /// The pessimistic value state of a value without alias\n-  static AliasInfo getPessimisticValueState(MLIRContext *context) {\n+  static AliasInfo getPessimisticValueState(MLIRContext *context = nullptr) {\n     return AliasInfo();\n   }\n   static AliasInfo getPessimisticValueState(Value value) { return AliasInfo(); }\n \n   /// The union of both arguments\n   static AliasInfo join(const AliasInfo &lhs, const AliasInfo &rhs);\n \n+  void print(raw_ostream &os) const {\n+    llvm::interleaveComma(allocs, os, [&](Value alloc) { alloc.print(os); });\n+  }\n+\n private:\n   /// The set of allocated values that are aliased by this lattice.\n   /// For now, we only consider aliased value produced by the following\n@@ -58,9 +62,13 @@ class AliasInfo {\n //===----------------------------------------------------------------------===//\n // Shared Memory Alias Analysis\n //===----------------------------------------------------------------------===//\n-class SharedMemoryAliasAnalysis : public ForwardDataFlowAnalysis<AliasInfo> {\n+class SharedMemoryAliasAnalysis\n+    : public dataflow::SparseDataFlowAnalysis<dataflow::Lattice<AliasInfo>> {\n public:\n-  using ForwardDataFlowAnalysis<AliasInfo>::ForwardDataFlowAnalysis;\n+  using dataflow::SparseDataFlowAnalysis<\n+      dataflow::Lattice<AliasInfo>>::SparseDataFlowAnalysis;\n+  using dataflow::SparseDataFlowAnalysis<\n+      dataflow::Lattice<AliasInfo>>::getLatticeElement;\n \n   /// XXX(Keren): Compatible interface with MLIR AliasAnalysis for future use.\n   /// Given two values, returns their aliasing behavior.\n@@ -69,10 +77,17 @@ class SharedMemoryAliasAnalysis : public ForwardDataFlowAnalysis<AliasInfo> {\n   /// Returns the modify-reference behavior of `op` on `location`.\n   ModRefResult getModRef(Operation *op, Value location);\n \n+  void setToEntryState(dataflow::Lattice<AliasInfo> *lattice) override {\n+    propagateIfChanged(\n+        lattice, lattice->join(\n+                     AliasInfo::getPessimisticValueState(lattice->getPoint())));\n+  }\n+\n   /// Computes if the alloc set of the results are changed.\n-  ChangeResult\n+  void\n   visitOperation(Operation *op,\n-                 ArrayRef<LatticeElement<AliasInfo> *> operands) override;\n+                 ArrayRef<const dataflow::Lattice<AliasInfo> *> operands,\n+                 ArrayRef<dataflow::Lattice<AliasInfo> *> results) override;\n };\n \n } // namespace mlir"}, {"filename": "include/triton/Analysis/Allocation.h", "status": "modified", "additions": 9, "deletions": 6, "changes": 15, "file_content_changes": "@@ -67,12 +67,12 @@ class Allocation {\n \n   /// Returns the offset of the given buffer in the shared memory.\n   size_t getOffset(BufferId bufferId) const {\n-    return bufferSet.lookup(bufferId).offset;\n+    return bufferSet.at(bufferId).offset;\n   }\n \n   /// Returns the size of the given buffer in the shared memory.\n   size_t getAllocatedSize(BufferId bufferId) const {\n-    return bufferSet.lookup(bufferId).size;\n+    return bufferSet.at(bufferId).size;\n   }\n \n   /// Returns the buffer id of the given value.\n@@ -115,8 +115,8 @@ class Allocation {\n   bool isIntersected(BufferId lhsId, BufferId rhsId) const {\n     if (lhsId == InvalidBufferId || rhsId == InvalidBufferId)\n       return false;\n-    auto lhsBuffer = bufferSet.lookup(lhsId);\n-    auto rhsBuffer = bufferSet.lookup(rhsId);\n+    auto lhsBuffer = bufferSet.at(lhsId);\n+    auto rhsBuffer = bufferSet.at(rhsId);\n     return lhsBuffer.intersects(rhsBuffer);\n   }\n \n@@ -137,7 +137,8 @@ class Allocation {\n     bool operator<(const BufferT &other) const { return id < other.id; }\n \n     BufferT() : BufferT(BufferKind::Explicit) {}\n-    BufferT(BufferKind kind) : BufferT(kind, 0, 0) {}\n+    BufferT(BufferKind kind)\n+        : kind(kind), id(InvalidBufferId), size(0), offset(0) {}\n     BufferT(BufferKind kind, size_t size) : BufferT(kind, size, 0) {}\n     BufferT(BufferKind kind, size_t size, size_t offset)\n         : kind(kind), id(nextId++), size(size), offset(offset) {}\n@@ -156,7 +157,7 @@ class Allocation {\n   /// Value -> Alias Buffer\n   using AliasBufferMapT = llvm::MapVector<Value, llvm::SetVector<BufferT *>>;\n   /// BufferId -> Buffer\n-  using BufferSetT = DenseMap<BufferId, BufferT>;\n+  using BufferSetT = std::map<BufferId, BufferT>;\n   /// Runs allocation analysis on the given top-level operation.\n   void run();\n \n@@ -187,6 +188,8 @@ class Allocation {\n   friend class triton::AllocationAnalysis;\n };\n \n+template <typename T> Interval(T, T) -> Interval<T>;\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_ALLOCATION_H"}, {"filename": "include/triton/Analysis/AxisInfo.h", "status": "modified", "additions": 188, "deletions": 38, "changes": 226, "file_content_changes": "@@ -1,69 +1,101 @@\n #ifndef TRITON_ANALYSIS_AXISINFO_H\n #define TRITON_ANALYSIS_AXISINFO_H\n \n-#include \"mlir/Analysis/DataFlowAnalysis.h\"\n+#include \"mlir/Analysis/DataFlow/SparseAnalysis.h\"\n #include \"llvm/Support/raw_ostream.h\"\n-#include <iostream>\n \n+#include \"mlir/Support/LLVM.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n+#include <optional>\n+#include <type_traits>\n+\n namespace mlir {\n \n //===----------------------------------------------------------------------===//\n // AxisInfo\n //===----------------------------------------------------------------------===//\n \n /// This lattice value represents known information on the axes of a lattice.\n-/// Axis information is represented by a std::map<int, int>\n class AxisInfo {\n public:\n-  typedef SmallVector<int, 4> DimVectorT;\n+  typedef SmallVector<int64_t, 4> DimVectorT;\n \n public:\n-  // Default constructor\n+  /// Default constructor\n   AxisInfo() : AxisInfo({}, {}, {}) {}\n-  // Construct contiguity info with known contiguity\n+  /// Construct contiguity info with known contiguity\n   AxisInfo(DimVectorT knownContiguity, DimVectorT knownDivisibility,\n            DimVectorT knownConstancy)\n+      : AxisInfo(knownContiguity, knownDivisibility, knownConstancy, {}) {}\n+  AxisInfo(DimVectorT knownContiguity, DimVectorT knownDivisibility,\n+           DimVectorT knownConstancy, std::optional<int64_t> knownConstantValue)\n       : contiguity(knownContiguity), divisibility(knownDivisibility),\n-        constancy(knownConstancy), rank(contiguity.size()) {\n-    assert(knownDivisibility.size() == (size_t)rank);\n-    assert(knownConstancy.size() == (size_t)rank);\n+        constancy(knownConstancy), constantValue(knownConstantValue),\n+        rank(contiguity.size()) {\n+    assert(knownContiguity.size() == static_cast<size_t>(rank));\n+    assert(knownDivisibility.size() == static_cast<size_t>(rank));\n+    assert(knownConstancy.size() == static_cast<size_t>(rank));\n   }\n \n-  // Accessors\n-  int getContiguity(size_t d) const { return contiguity[d]; }\n+  /// Accessors\n+  int64_t getContiguity(size_t dim) const { return contiguity[dim]; }\n   const DimVectorT &getContiguity() const { return contiguity; }\n \n-  int getDivisibility(size_t d) const { return divisibility[d]; }\n+  int64_t getDivisibility(size_t dim) const { return divisibility[dim]; }\n   const DimVectorT &getDivisibility() const { return divisibility; }\n \n-  int getConstancy(size_t d) const { return constancy[d]; }\n+  int64_t getConstancy(size_t dim) const { return constancy[dim]; }\n   const DimVectorT &getConstancy() const { return constancy; }\n \n   int getRank() const { return rank; }\n \n-  // Comparison\n+  std::optional<int64_t> getConstantValue() const { return constantValue; }\n+\n+  /// Comparison\n   bool operator==(const AxisInfo &other) const {\n     return (contiguity == other.contiguity) &&\n            (divisibility == other.divisibility) &&\n-           (constancy == other.constancy);\n+           (constancy == other.constancy) &&\n+           (constantValue == other.constantValue) && (rank == other.rank);\n   }\n \n   /// The pessimistic value state of the contiguity is unknown.\n-  static AxisInfo getPessimisticValueState(MLIRContext *context) {\n+  static AxisInfo getPessimisticValueState(MLIRContext *context = nullptr) {\n     return AxisInfo();\n   }\n   static AxisInfo getPessimisticValueState(Value value);\n \n-  // The gcd of both arguments for each dimension\n+  /// The gcd of both arguments for each dimension\n   static AxisInfo join(const AxisInfo &lhs, const AxisInfo &rhs);\n \n+  void print(raw_ostream &os) const {\n+    auto print = [&](StringRef name, DimVectorT vec) {\n+      os << name << \" = [\";\n+      llvm::interleaveComma(vec, os);\n+      os << \"]\";\n+    };\n+    print(\"contiguity\", contiguity);\n+    print(\", divisibility\", divisibility);\n+    print(\", constancy\", constancy);\n+    os << \", constant_value = \";\n+    if (constantValue)\n+      os << *constantValue;\n+    else\n+      os << \"<none>\";\n+  }\n+\n private:\n   /// The _contiguity_ information maps the `d`-th\n   /// dimension to the length of the shortest\n-  /// sequence of contiguous integers along it\n+  /// sequence of contiguous integers along it.\n+  /// Suppose we have an array of N elements,\n+  /// with a contiguity value C,\n+  /// the array can be divided into a list of\n+  /// N/C sequences of C contiguous elements.\n+  /// Since we have N = 2^k, C must be a power of two.\n   /// For example:\n   /// [10, 11, 12, 13, 18, 19, 20, 21]\n   /// [20, 21, 22, 23, 28, 29, 30, 31]\n@@ -97,42 +129,160 @@ class AxisInfo {\n   /// dimension to the length of the shortest\n   /// sequence of constant integer along it. This is\n   /// particularly useful to infer the contiguity\n-  /// of operations (e.g., add) involving a constant\n+  /// of operations (e.g., add) involving a constant.\n+  /// Suppose we have an array of N elements,\n+  /// with a constancy value C,\n+  /// the array can be divided into a list of\n+  /// N/C sequences of C elements with the same value.\n+  /// Since we have N = 2^k, C must be a power of two.\n   /// For example\n   /// [8, 8, 8, 8, 12, 12, 12, 12]\n   /// [16, 16, 16, 16, 20, 20, 20, 20]\n   /// would have constancy [1, 4]\n   DimVectorT constancy;\n \n+  /// The constant value of the lattice if we can infer it.\n+  std::optional<int64_t> constantValue;\n+\n   // number of dimensions of the lattice\n-  int rank;\n+  int rank{};\n };\n \n-class AxisInfoAnalysis : public ForwardDataFlowAnalysis<AxisInfo> {\n+class AxisInfoVisitor {\n+public:\n+  AxisInfoVisitor() = default;\n+  virtual ~AxisInfoVisitor() = default;\n \n-private:\n-  static const int maxPow2Divisor = 65536;\n+  static bool isContiguousDim(const AxisInfo &info, ArrayRef<int64_t> shape,\n+                              int dim) {\n+    return info.getContiguity(dim) == shape[dim];\n+  }\n \n-  int highestPowOf2Divisor(int n) {\n-    if (n == 0)\n-      return maxPow2Divisor;\n-    return (n & (~(n - 1)));\n+  static bool isConstantDim(const AxisInfo &info, ArrayRef<int64_t> shape,\n+                            int dim) {\n+    return info.getConstancy(dim) == shape[dim];\n   }\n \n-  AxisInfo visitBinaryOp(\n-      Operation *op, AxisInfo lhsInfo, AxisInfo rhsInfo,\n-      const std::function<int(AxisInfo, AxisInfo, int)> &getContiguity,\n-      const std::function<int(AxisInfo, AxisInfo, int)> &getDivisibility,\n-      const std::function<int(AxisInfo, AxisInfo, int)> &getConstancy);\n+  virtual AxisInfo\n+  getAxisInfo(Operation *op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) = 0;\n+\n+  virtual bool match(Operation *op) = 0;\n+};\n+\n+/// Base class for all operations\n+template <typename OpTy> class AxisInfoVisitorImpl : public AxisInfoVisitor {\n+public:\n+  using AxisInfoVisitor::AxisInfoVisitor;\n+\n+  AxisInfo\n+  getAxisInfo(Operation *op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) final {\n+    return getAxisInfo(cast<OpTy>(op), operands);\n+  }\n+\n+  bool match(Operation *op) final { return isa<OpTy>(op); }\n+\n+  virtual AxisInfo\n+  getAxisInfo(OpTy op, ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) {\n+    llvm_unreachable(\"Unimplemented getAxisInfo\");\n+  }\n+};\n+\n+/// Binary operations\n+template <typename OpTy>\n+class BinaryOpVisitorImpl : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto lhsInfo = operands[0]->getValue();\n+    auto rhsInfo = operands[1]->getValue();\n+    auto rank = lhsInfo.getRank();\n+    assert(operands.size() == 2 && \"Expected two operands\");\n+    AxisInfo::DimVectorT contiguity;\n+    AxisInfo::DimVectorT divisibility;\n+    AxisInfo::DimVectorT constancy;\n+    auto constantValue = getConstantValue(op, lhsInfo, rhsInfo);\n+    for (auto d = 0; d < rank; ++d) {\n+      if (constantValue.has_value()) {\n+        contiguity.push_back(1);\n+        constancy.push_back(\n+            std::max(lhsInfo.getConstancy(d), rhsInfo.getConstancy(d)));\n+        divisibility.push_back(highestPowOf2Divisor(constantValue.value()));\n+      } else {\n+        contiguity.push_back(getContiguity(op, lhsInfo, rhsInfo, d));\n+        constancy.push_back(getConstancy(op, lhsInfo, rhsInfo, d));\n+        divisibility.push_back(getDivisibility(op, lhsInfo, rhsInfo, d));\n+      }\n+    }\n+    return AxisInfo(contiguity, divisibility, constancy, constantValue);\n+  }\n+\n+protected:\n+  virtual int64_t getContiguity(OpTy op, const AxisInfo &lhs,\n+                                const AxisInfo &rhs, int dim) {\n+    return 1;\n+  }\n+\n+  virtual int64_t getDivisibility(OpTy op, const AxisInfo &lhs,\n+                                  const AxisInfo &rhs, int dim) {\n+    return 1;\n+  }\n+\n+  virtual int64_t getConstancy(OpTy op, const AxisInfo &lhs,\n+                               const AxisInfo &rhs, int dim) {\n+    return 1;\n+  }\n+\n+  virtual std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                                  const AxisInfo &rhs) {\n+    return {};\n+  }\n+};\n+\n+class AxisInfoVisitorList {\n+public:\n+  template <typename... Ts, typename = std::enable_if_t<sizeof...(Ts) != 0>>\n+  void append() {\n+    (visitors.emplace_back(std::make_unique<Ts>()), ...);\n+  }\n+\n+  AxisInfo apply(Operation *op,\n+                 ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) {\n+    for (auto &visitor : visitors)\n+      if (visitor->match(op))\n+        return visitor->getAxisInfo(op, operands);\n+    return AxisInfo();\n+  }\n+\n+private:\n+  std::vector<std::unique_ptr<AxisInfoVisitor>> visitors;\n+};\n+\n+class AxisInfoAnalysis\n+    : public dataflow::SparseDataFlowAnalysis<dataflow::Lattice<AxisInfo>> {\n+private:\n+  AxisInfoVisitorList visitors;\n+\n+  void setToEntryState(dataflow::Lattice<AxisInfo> *lattice) override {\n+    propagateIfChanged(\n+        lattice,\n+        lattice->join(AxisInfo::getPessimisticValueState(lattice->getPoint())));\n+  }\n \n public:\n-  using ForwardDataFlowAnalysis<AxisInfo>::ForwardDataFlowAnalysis;\n+  AxisInfoAnalysis(DataFlowSolver &solver);\n+  using dataflow::SparseDataFlowAnalysis<\n+      dataflow::Lattice<AxisInfo>>::getLatticeElement;\n \n-  ChangeResult\n-  visitOperation(Operation *op,\n-                 ArrayRef<LatticeElement<AxisInfo> *> operands) override;\n+  void visitOperation(Operation *op,\n+                      ArrayRef<const dataflow::Lattice<AxisInfo> *> operands,\n+                      ArrayRef<dataflow::Lattice<AxisInfo> *> results) override;\n \n-  unsigned getPtrVectorSize(Value ptr);\n+  unsigned getPtrContiguity(Value ptr);\n \n   unsigned getPtrAlignment(Value ptr);\n \n@@ -141,4 +291,4 @@ class AxisInfoAnalysis : public ForwardDataFlowAnalysis<AxisInfo> {\n \n } // namespace mlir\n \n-#endif\n\\ No newline at end of file\n+#endif"}, {"filename": "include/triton/Analysis/Membar.h", "status": "modified", "additions": 26, "deletions": 17, "changes": 43, "file_content_changes": "@@ -36,28 +36,29 @@ class MembarAnalysis {\n   void run();\n \n private:\n-  struct RegionInfo {\n+  struct BlockInfo {\n     using BufferIdSetT = Allocation::BufferIdSetT;\n \n     BufferIdSetT syncReadBuffers;\n     BufferIdSetT syncWriteBuffers;\n \n-    RegionInfo() = default;\n-    RegionInfo(const BufferIdSetT &syncReadBuffers,\n-               const BufferIdSetT &syncWriteBuffers)\n+    BlockInfo() = default;\n+    BlockInfo(const BufferIdSetT &syncReadBuffers,\n+              const BufferIdSetT &syncWriteBuffers)\n         : syncReadBuffers(syncReadBuffers), syncWriteBuffers(syncWriteBuffers) {\n     }\n \n-    /// Unions two RegionInfo objects.\n-    void join(const RegionInfo &other) {\n+    /// Unions two BlockInfo objects.\n+    BlockInfo &join(const BlockInfo &other) {\n       syncReadBuffers.insert(other.syncReadBuffers.begin(),\n                              other.syncReadBuffers.end());\n       syncWriteBuffers.insert(other.syncWriteBuffers.begin(),\n                               other.syncWriteBuffers.end());\n+      return *this;\n     }\n \n-    /// Returns true if buffers in two RegionInfo objects are intersected.\n-    bool isIntersected(const RegionInfo &other, Allocation *allocation) const {\n+    /// Returns true if buffers in two BlockInfo objects are intersected.\n+    bool isIntersected(const BlockInfo &other, Allocation *allocation) const {\n       return /*RAW*/ isIntersected(syncWriteBuffers, other.syncReadBuffers,\n                                    allocation) ||\n              /*WAR*/\n@@ -74,6 +75,14 @@ class MembarAnalysis {\n       syncWriteBuffers.clear();\n     }\n \n+    /// Compares two BlockInfo objects.\n+    bool operator==(const BlockInfo &other) const {\n+      return syncReadBuffers == other.syncReadBuffers &&\n+             syncWriteBuffers == other.syncWriteBuffers;\n+    }\n+\n+    bool operator!=(const BlockInfo &other) const { return !(*this == other); }\n+\n   private:\n     /// Returns true if buffers in two sets are intersected.\n     bool isIntersected(const BufferIdSetT &lhs, const BufferIdSetT &rhs,\n@@ -99,19 +108,19 @@ class MembarAnalysis {\n   ///        op5\n   ///        op6\n   ///   op7\n-  /// region2 and region3 started with the information of region1.\n-  /// Each region is analyzed separately and keeps their own copy of the\n-  /// information. At op7, we union the information of the region2 and region3\n-  /// and update the information of region1.\n-  void dfsOperation(Operation *operation, RegionInfo *blockInfo,\n-                    OpBuilder *builder);\n+  /// TODO: Explain why we don't use ForwardAnalysis:\n+  void resolve(Operation *operation, OpBuilder *builder);\n+\n+  /// Updates the BlockInfo operation based on the operation.\n+  void update(Operation *operation, BlockInfo *blockInfo, OpBuilder *builder);\n \n-  /// Updates the RegionInfo operation based on the operation.\n-  void transfer(Operation *operation, RegionInfo *blockInfo,\n-                OpBuilder *builder);\n+  /// Collects the successors of the terminator\n+  void visitTerminator(Operation *operation, SmallVector<Block *> &successors);\n \n private:\n   Allocation *allocation;\n+  DenseMap<Block *, BlockInfo> inputBlockInfoMap;\n+  DenseMap<Block *, BlockInfo> outputBlockInfoMap;\n };\n \n } // namespace mlir"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 31, "deletions": 1, "changes": 32, "file_content_changes": "@@ -1,6 +1,8 @@\n #ifndef TRITON_ANALYSIS_UTILITY_H\n #define TRITON_ANALYSIS_UTILITY_H\n \n+#include \"mlir/Analysis/DataFlowFramework.h\"\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include <algorithm>\n #include <numeric>\n@@ -11,7 +13,7 @@ namespace mlir {\n class ReduceOpHelper {\n public:\n   explicit ReduceOpHelper(triton::ReduceOp op) : op(op) {\n-    srcTy = op.operand().getType().cast<RankedTensorType>();\n+    srcTy = op.getOperand().getType().cast<RankedTensorType>();\n   }\n \n   ArrayRef<int64_t> getSrcShape() { return srcTy.getShape(); }\n@@ -77,6 +79,34 @@ SmallVector<RES_T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n   return result;\n }\n \n+template <typename T> T highestPowOf2Divisor(T n) {\n+  if (n == 0) {\n+    return (static_cast<T>(1) << (sizeof(T) * 8 - 2));\n+  }\n+  return (n & (~(n - 1)));\n+}\n+\n+bool isSingleValue(Value value);\n+\n+bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n+                        triton::gpu::DotOperandEncodingAttr &dotOperandLayout);\n+\n+/// Multi-root DAG topological sort.\n+/// Performs a topological sort of the Operation in the `toSort` SetVector.\n+/// Returns a topologically sorted SetVector.\n+/// It is faster than mlir::topologicalSort because it prunes nodes that have\n+/// been visited before.\n+SetVector<Operation *>\n+multiRootTopologicalSort(const SetVector<Operation *> &toSort);\n+\n+// This uses the toplogicalSort above\n+SetVector<Operation *>\n+multiRootGetSlice(Operation *op, TransitiveFilter backwardFilter = nullptr,\n+                  TransitiveFilter forwardFilter = nullptr);\n+\n+// Create a basic DataFlowSolver with constant and dead code analysis included.\n+std::unique_ptr<DataFlowSolver> createDataFlowSolver();\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_UTILITY_H"}, {"filename": "include/triton/Conversion/MLIRTypes.h", "status": "modified", "additions": 14, "deletions": 15, "changes": 29, "file_content_changes": "@@ -1,5 +1,5 @@\n-#ifndef TRITON_CONVERSION_MLIR_TYPES_H_\n-#define TRITON_CONVERSION_MLIR_TYPES_H_\n+#ifndef TRITON_CONVERSION_MLIR_TYPES_H\n+#define TRITON_CONVERSION_MLIR_TYPES_H\n \n #include \"mlir/Transforms/DialectConversion.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n@@ -10,31 +10,30 @@ namespace triton {\n namespace type {\n \n // Integer types\n-// TODO(Superjomn): may change `static` into better implementations\n-static Type i32Ty(MLIRContext *ctx) { return IntegerType::get(ctx, 32); }\n-static Type i16Ty(MLIRContext *ctx) { return IntegerType::get(ctx, 16); }\n-static Type i8Ty(MLIRContext *ctx) { return IntegerType::get(ctx, 8); }\n-static Type u32Ty(MLIRContext *ctx) {\n+inline Type i32Ty(MLIRContext *ctx) { return IntegerType::get(ctx, 32); }\n+inline Type i16Ty(MLIRContext *ctx) { return IntegerType::get(ctx, 16); }\n+inline Type i8Ty(MLIRContext *ctx) { return IntegerType::get(ctx, 8); }\n+inline Type u32Ty(MLIRContext *ctx) {\n   return IntegerType::get(ctx, 32, IntegerType::Unsigned);\n }\n-static Type u1Ty(MLIRContext *ctx) {\n+inline Type u1Ty(MLIRContext *ctx) {\n   return IntegerType::get(ctx, 1, IntegerType::Unsigned);\n }\n \n // Float types\n-static Type f16Ty(MLIRContext *ctx) { return FloatType::getF16(ctx); }\n-static Type f32Ty(MLIRContext *ctx) { return FloatType::getF32(ctx); }\n-static Type f64Ty(MLIRContext *ctx) { return FloatType::getF64(ctx); }\n-static Type bf16Ty(MLIRContext *ctx) { return FloatType::getBF16(ctx); }\n+inline Type f16Ty(MLIRContext *ctx) { return FloatType::getF16(ctx); }\n+inline Type f32Ty(MLIRContext *ctx) { return FloatType::getF32(ctx); }\n+inline Type f64Ty(MLIRContext *ctx) { return FloatType::getF64(ctx); }\n+inline Type bf16Ty(MLIRContext *ctx) { return FloatType::getBF16(ctx); }\n \n-static bool isFloat(Type type) {\n+inline bool isFloat(Type type) {\n   return type.isF32() || type.isF64() || type.isF16() || type.isF128();\n }\n \n-static bool isInt(Type type) { return type.isIntOrFloat() && !isFloat(type); }\n+inline bool isInt(Type type) { return type.isIntOrFloat() && !isFloat(type); }\n \n } // namespace type\n } // namespace triton\n } // namespace mlir\n \n-#endif // TRITON_CONVERSION_MLIR_TYPES_H_\n+#endif // TRITON_CONVERSION_MLIR_TYPES_H"}, {"filename": "include/triton/Conversion/Passes.td", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "file_content_changes": "@@ -10,9 +10,8 @@ def ConvertTritonToTritonGPU: Pass<\"convert-triton-to-tritongpu\", \"mlir::ModuleO\n     }];\n     let constructor = \"mlir::triton::createConvertTritonToTritonGPUPass()\";\n \n-    let dependentDialects = [\"mlir::arith::ArithmeticDialect\",\n+    let dependentDialects = [\"mlir::arith::ArithDialect\",\n                              \"mlir::math::MathDialect\",\n-                             \"mlir::StandardOpsDialect\",\n                              // TODO: Does this pass depend on SCF?\n                              \"mlir::scf::SCFDialect\",\n                              \"mlir::triton::TritonDialect\",\n@@ -33,16 +32,15 @@ def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"\n     }];\n     let constructor = \"mlir::triton::createConvertTritonGPUToLLVMPass()\";\n \n-    let dependentDialects = [\"mlir::arith::ArithmeticDialect\",\n+    let dependentDialects = [\"mlir::arith::ArithDialect\",\n                              \"mlir::math::MathDialect\",\n                              \"mlir::gpu::GPUDialect\",\n                              \"mlir::scf::SCFDialect\",\n                              \"mlir::LLVM::LLVMDialect\",\n                              \"mlir::tensor::TensorDialect\",\n                              \"mlir::triton::TritonDialect\",\n                              \"mlir::triton::gpu::TritonGPUDialect\",\n-                             \"mlir::NVVM::NVVMDialect\",\n-                             \"mlir::StandardOpsDialect\"];\n+                             \"mlir::NVVM::NVVMDialect\"];\n \n     let options = [\n         Option<\"computeCapability\", \"compute-capability\","}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -161,9 +161,8 @@ struct PTXBuilder {\n \n   std::string dump() const;\n \n-  mlir::Value launch(ConversionPatternRewriter &rewriter, Location loc,\n-                     Type resTy, bool hasSideEffect = true,\n-                     bool isAlignStack = false,\n+  mlir::Value launch(OpBuilder &rewriter, Location loc, Type resTy,\n+                     bool hasSideEffect = true, bool isAlignStack = false,\n                      ArrayRef<Attribute> attrs = {}) const;\n \n private:"}, {"filename": "include/triton/Dialect/Triton/IR/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -8,7 +8,7 @@ set(LLVM_TARGET_DEFINITIONS TritonDialect.td)\n mlir_tablegen(Dialect.h.inc -gen-dialect-decls)\n mlir_tablegen(Dialect.cpp.inc -gen-dialect-defs)\n \n-set(LLVM_TARGET_DEFINITIONS TritonOps.td)\n+set(LLVM_TARGET_DEFINITIONS TritonTypes.td)\n mlir_tablegen(Types.h.inc -gen-typedef-decls)\n mlir_tablegen(Types.cpp.inc -gen-typedef-defs)\n "}, {"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -1,14 +1,15 @@\n #ifndef TRITON_DIALECT_TRITON_IR_DIALECT_H_\n #define TRITON_DIALECT_TRITON_IR_DIALECT_H_\n \n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlow.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/Dialect/Math/IR/Math.h\"\n-#include \"mlir/Dialect/SCF/SCF.h\"\n-#include \"mlir/Dialect/StandardOps/IR/Ops.h\"\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/Dialect.h\"\n #include \"mlir/Interfaces/ControlFlowInterfaces.h\"\n-\n #include \"triton/Dialect/Triton/IR/Dialect.h.inc\"\n #include \"triton/Dialect/Triton/IR/OpsEnums.h.inc\"\n #include \"triton/Dialect/Triton/IR/Traits.h\""}, {"filename": "include/triton/Dialect/Triton/IR/TritonDialect.td", "status": "modified", "additions": 5, "deletions": 7, "changes": 12, "file_content_changes": "@@ -14,7 +14,7 @@ def Triton_Dialect : Dialect {\n     Triton Dialect.\n \n     Dependent Dialects:\n-      * Arithmetic:\n+      * Arith:\n         * addf, addi, andi, cmpf, cmpi, divf, fptosi, ...\n       * Math:\n         * exp, sin, cos, log, ...\n@@ -23,21 +23,19 @@ def Triton_Dialect : Dialect {\n   }];\n \n   let dependentDialects = [\n-    \"arith::ArithmeticDialect\",\n+    \"arith::ArithDialect\",\n     \"math::MathDialect\",\n-    \"StandardOpsDialect\",\n     \"scf::SCFDialect\",\n-\n-    // Since LLVM 15\n-    // \"cf::ControlFlowDialect\",\n-    // \"func::FuncDialect\"\n+    \"cf::ControlFlowDialect\",\n+    \"func::FuncDialect\"\n   ];\n \n   let extraClassDeclaration = [{\n     void registerTypes();\n   }];\n \n   let hasConstantMaterializer = 1;\n+  let useDefaultTypePrinterParser = 1;\n }\n \n include \"triton/Dialect/Triton/IR/TritonTypes.td\""}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 36, "deletions": 36, "changes": 72, "file_content_changes": "@@ -6,10 +6,10 @@ include \"triton/Dialect/Triton/IR/TritonTypes.td\"\n include \"triton/Dialect/Triton/IR/TritonAttrDefs.td\"\n include \"triton/Dialect/Triton/IR/TritonInterfaces.td\"\n include \"mlir/IR/OpBase.td\"\n-include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n+include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/ControlFlowInterfaces.td\" // BranchOpInterface\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n-include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n+include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/CastInterfaces.td\" // CastOpInterface\n \n //\n@@ -29,7 +29,7 @@ class TT_Op<string mnemonic, list<Trait> traits = []> :\n //   extui, extsi, tructi\n def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n                                          SameOperandsAndResultEncoding,\n-                                         NoSideEffect,\n+                                         Pure,\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Cast int64 to pointer\";\n \n@@ -42,7 +42,7 @@ def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n \n def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n                                          SameOperandsAndResultEncoding,\n-                                         NoSideEffect,\n+                                         Pure,\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Cast pointer to int64\";\n \n@@ -56,7 +56,7 @@ def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n // arith.bitcast doesn't support pointers\n def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n                                      SameOperandsAndResultEncoding,\n-                                     NoSideEffect,\n+                                     Pure,\n                                      /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Cast between types of the same bitwidth\";\n \n@@ -71,7 +71,7 @@ def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n \n def TT_FpToFpOp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n                                      SameOperandsAndResultEncoding,\n-                                     NoSideEffect,\n+                                     Pure,\n                                      DeclareOpInterfaceMethods<CastOpInterface>]> {\n     let summary = \"Floating point casting for custom types\";\n \n@@ -95,9 +95,9 @@ def TT_FpToFpOp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n //\n \n def TT_AddPtrOp : TT_Op<\"addptr\",\n-                     [NoSideEffect,\n-                     SameOperandsAndResultShape,\n-                     SameOperandsAndResultEncoding,\n+                     [Pure,\n+                      SameOperandsAndResultShape,\n+                      SameOperandsAndResultEncoding,\n                       TypesMatchWith<\"result type matches ptr type\",\n                                      \"result\", \"ptr\", \"$_self\">]> {\n     let arguments = (ins TT_PtrLike:$ptr, TT_IntLike:$offset);\n@@ -141,11 +141,7 @@ def TT_LoadOp : TT_Op<\"load\",\n                        \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n     ];\n \n-    // let assemblyFormat = \"operands attr-dict `:` type($result)\";\n-    let parser = [{ return mlir::triton::parseLoadOp(parser, result); }];\n-\n-    let printer = [{ return mlir::triton::printLoadOp(p, *this); }];\n-\n+    let hasCustomAssemblyFormat = 1;\n     let hasCanonicalizer = 1;\n }\n \n@@ -161,17 +157,16 @@ def TT_StoreOp : TT_Op<\"store\",\n                                        \"($_op.getOperands().size() <= 2) || std::equal_to<>()\">]> {\n     let summary = \"store\";\n \n-    let arguments = (ins TT_PtrLike:$ptr, TT_Type:$value, Optional<TT_BoolLike>:$mask);\n+    let arguments = (ins TT_PtrLike:$ptr, TT_Type:$value, Optional<TT_BoolLike>:$mask,\n+                     DefaultValuedAttr<TT_CacheModifierAttr, \"triton::CacheModifier::NONE\">:$cache,\n+                     DefaultValuedAttr<TT_EvictionPolicyAttr, \"triton::EvictionPolicy::NORMAL\">:$evict);\n \n     let builders = [\n-        OpBuilder<(ins \"Value\":$ptr, \"Value\":$value)>,\n+        OpBuilder<(ins \"Value\":$ptr, \"Value\":$value, \"triton::CacheModifier\":$cache,\n+                       \"triton::EvictionPolicy\":$evict)>,\n     ];\n \n-    // let assemblyFormat = \"operands attr-dict `:` type($value)\";\n-    let parser = [{ return mlir::triton::parseStoreOp(parser, result); }];\n-\n-    let printer = [{ return mlir::triton::printStoreOp(p, *this); }];\n-\n+    let hasCustomAssemblyFormat = 1;\n     let hasCanonicalizer = 1;\n }\n \n@@ -227,8 +222,9 @@ def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [MemoryEffects<[MemRead]>,\n //\n // Shape Manipulation Ops\n //\n-def TT_SplatOp : TT_Op<\"splat\", [NoSideEffect,\n-                                 SameOperandsAndResultElementType]> {\n+def TT_SplatOp : TT_Op<\"splat\", [Pure,\n+                                 SameOperandsAndResultElementType,\n+                                 SameOperandsAndResultEncoding]> {\n     let summary = \"splat\";\n \n     let arguments = (ins TT_Type:$src);\n@@ -240,7 +236,7 @@ def TT_SplatOp : TT_Op<\"splat\", [NoSideEffect,\n     let hasFolder = 1;\n }\n \n-def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect,\n+def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [Pure,\n                                             DeclareOpInterfaceMethods<InferTypeOpInterface>,\n                                             SameOperandsAndResultElementType]> {\n     let summary = \"expand_dims\";\n@@ -252,7 +248,8 @@ def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect,\n     let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n }\n \n-def TT_ViewOp : TT_Op<\"view\", [NoSideEffect,\n+// view is not `pure` because it may reorder elements\n+def TT_ViewOp : TT_Op<\"view\", [NoMemoryEffect,\n                                SameOperandsAndResultElementType]> {\n     let summary = \"view\";\n \n@@ -264,8 +261,9 @@ def TT_ViewOp : TT_Op<\"view\", [NoSideEffect,\n \n }\n \n-def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect,\n-                                         SameOperandsAndResultElementType]> {\n+def TT_BroadcastOp : TT_Op<\"broadcast\", [Pure,\n+                                         SameOperandsAndResultElementType,\n+                                         SameOperandsAndResultEncoding]> {\n     let summary = \"broadcast. No left-padding as of now.\";\n \n     let arguments = (ins TT_Type:$src);\n@@ -277,7 +275,8 @@ def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect,\n     let hasFolder = 1;\n }\n \n-def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n+// cat is not `pure` because it may reorder elements\n+def TT_CatOp : TT_Op<\"cat\", [NoMemoryEffect,\n                              SameOperandsAndResultElementType]> {\n     let summary = \"concatenate 2 tensors\";\n \n@@ -288,8 +287,9 @@ def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n     let assemblyFormat = \"$lhs `,` $rhs attr-dict `:` functional-type(operands, results)\";\n }\n \n-def TT_TransOp : TT_Op<\"trans\", [NoSideEffect,\n-                                 DeclareOpInterfaceMethods<InferTypeOpInterface>]> {\n+def TT_TransOp : TT_Op<\"trans\", [Pure,\n+                                 DeclareOpInterfaceMethods<InferTypeOpInterface>,\n+                                 SameOperandsAndResultElementType]> {\n \n     let summary = \"transpose a tensor\";\n \n@@ -303,15 +303,15 @@ def TT_TransOp : TT_Op<\"trans\", [NoSideEffect,\n //\n // SPMD Ops\n //\n-def TT_GetProgramIdOp : TT_Op<\"get_program_id\", [NoSideEffect]> {\n+def TT_GetProgramIdOp : TT_Op<\"get_program_id\", [Pure]> {\n     let arguments = (ins I32Attr:$axis);\n \n     let results = (outs I32:$result);\n \n     let assemblyFormat = \"attr-dict `:` type($result)\";\n }\n \n-def TT_GetNumProgramsOp : TT_Op<\"get_num_programs\", [NoSideEffect]> {\n+def TT_GetNumProgramsOp : TT_Op<\"get_num_programs\", [Pure]> {\n     let arguments = (ins I32Attr:$axis);\n \n     let results = (outs I32:$result);\n@@ -322,7 +322,7 @@ def TT_GetNumProgramsOp : TT_Op<\"get_num_programs\", [NoSideEffect]> {\n //\n // Dot Op\n //\n-def TT_DotOp : TT_Op<\"dot\", [NoSideEffect,\n+def TT_DotOp : TT_Op<\"dot\", [Pure,\n                              DeclareOpInterfaceMethods<InferTypeOpInterface>,\n                              TypesMatchWith<\"result's type matches accumulator's type\",\n                                             \"d\", \"c\", \"$_self\">]> {\n@@ -342,7 +342,7 @@ def TT_DotOp : TT_Op<\"dot\", [NoSideEffect,\n //\n // Reduce Op\n //\n-def TT_ReduceOp : TT_Op<\"reduce\", [NoSideEffect,\n+def TT_ReduceOp : TT_Op<\"reduce\", [Pure,\n                                    DeclareOpInterfaceMethods<InferTypeOpInterface>]> {\n     let summary = \"reduce\";\n \n@@ -366,7 +366,7 @@ def TT_ReduceOp : TT_Op<\"reduce\", [NoSideEffect,\n //\n // External elementwise op\n //\n-def TT_ExtElemwiseOp : TT_Op<\"ext_elemwise\", [NoSideEffect, Elementwise, SameOperandsAndResultShape,\n+def TT_ExtElemwiseOp : TT_Op<\"ext_elemwise\", [Pure, Elementwise, SameOperandsAndResultShape,\n                                               SameOperandsAndResultEncoding,\n                                               SameVariadicOperandSize]> {\n     let summary = \"ext_elemwise\";\n@@ -388,7 +388,7 @@ def TT_ExtElemwiseOp : TT_Op<\"ext_elemwise\", [NoSideEffect, Elementwise, SameOpe\n // Make Range Op\n //\n // TODO: should have ConstantLike as Trait\n-def TT_MakeRangeOp : TT_Op<\"make_range\", [NoSideEffect]> {\n+def TT_MakeRangeOp : TT_Op<\"make_range\", [Pure]> {\n     let summary = \"make range\";\n \n     let description = [{"}, {"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -1,6 +1,7 @@\n #ifndef TRITON_TYPES\n #define TRITON_TYPES\n \n+include \"mlir/IR/AttrTypeBase.td\"\n include \"triton/Dialect/Triton/IR/TritonDialect.td\"\n \n //\n@@ -58,6 +59,7 @@ def TT_Ptr : TritonTypeDef<\"Pointer\", \"ptr\"> {\n         }]>\n     ];\n \n+    let hasCustomAssemblyFormat = 1;\n     let skipDefaultBuilders = 1;\n }\n def TT_PtrTensor : TensorOf<[TT_Ptr]>;"}, {"filename": "include/triton/Dialect/Triton/IR/Types.h", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -1,10 +1,17 @@\n #ifndef TRITON_IR_TYPES_H_\n #define TRITON_IR_TYPES_H_\n \n+#include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/TypeSupport.h\"\n #include \"mlir/IR/Types.h\"\n \n #define GET_TYPEDEF_CLASSES\n #include \"triton/Dialect/Triton/IR/Types.h.inc\"\n \n+namespace mlir {\n+\n+unsigned getPointeeBitWidth(RankedTensorType tensorTy);\n+\n+}\n+\n #endif // TRITON_IR_TYPES_H_"}, {"filename": "include/triton/Dialect/Triton/Transforms/Passes.td", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -16,8 +16,7 @@ def TritonCombineOps : Pass</*cli-arg*/\"triton-combine\", /*Op*/\"mlir::ModuleOp\">\n \n   let constructor = \"mlir::triton::createCombineOpsPass()\";\n \n-  let dependentDialects = [\"mlir::arith::ArithmeticDialect\",\n-                           /*SelectOp*/\"mlir::StandardOpsDialect\"];\n+  let dependentDialects = [\"mlir::arith::ArithDialect\"];\n }\n \n #endif"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -1,19 +1,17 @@\n #ifndef TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_\n #define TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_\n \n-#include \"mlir/Dialect/GPU/GPUDialect.h\"\n+#include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/Dialect.h\"\n \n // TritonGPU depends on Triton\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n-\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h.inc\"\n #include \"triton/Dialect/TritonGPU/IR/Traits.h\"\n \n #define GET_ATTRDEF_CLASSES\n-#include \"triton/Dialect/Triton/IR/AttrInterfaces.h.inc\"\n #include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.h.inc\"\n \n #define GET_OP_CLASSES\n@@ -31,11 +29,13 @@ SmallVector<unsigned> getWarpsPerCTA(const Attribute &layout);\n \n SmallVector<unsigned> getSizePerThread(const Attribute &layout);\n \n-SmallVector<unsigned> getContigPerThread(Attribute layout);\n+SmallVector<unsigned> getContigPerThread(const Attribute &layout);\n \n SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout);\n \n-SmallVector<unsigned> getShapePerCTA(const Attribute &layout);\n+SmallVector<unsigned>\n+getShapePerCTA(const Attribute &layout,\n+               ArrayRef<int64_t> tensorShape = ArrayRef<int64_t>());\n \n SmallVector<unsigned> getOrder(const Attribute &layout);\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 8, "deletions": 10, "changes": 18, "file_content_changes": "@@ -1,6 +1,7 @@\n #ifndef TRITONGPU_ATTRDEFS\n #define TRITONGPU_ATTRDEFS\n \n+include \"mlir/IR/AttrTypeBase.td\"\n include \"triton/Dialect/TritonGPU/IR/TritonGPUDialect.td\"\n include \"triton/Dialect/Triton/IR/TritonInterfaces.td\"\n \n@@ -95,9 +96,6 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n           bool is_row = order[0] != 0;\n           bool is_vec4 = opIdx == 0 ? !is_row && (shape[order[0]] <= 16) :\n               is_row && (shape[order[0]] <= 16);\n-          // TODO[Superjomn]: Support the case when is_vec4=false later\n-          // Currently, we only support ld.v2, for the mma layout varies with different ld vector width.\n-          is_vec4 = true;\n           int pack_size = opIdx == 0 ? ((is_row || is_vec4) ? 1 : 2) :\n                                        ((is_row && !is_vec4) ? 2 : 1);\n           int rep = 2 * pack_size;\n@@ -135,12 +133,11 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n \n         // ---- not implemented ----\n         llvm_unreachable(\"unsupported swizzling for provided MMA version\");\n-\n-\n     }]>\n   ];\n \n   let extraClassDeclaration = extraBaseClassDeclaration;\n+  let hasCustomAssemblyFormat = 1;\n }\n \n //===----------------------------------------------------------------------===//\n@@ -278,6 +275,7 @@ for\n     // ArrayRefParameter<\"unsigned\">:$sizePerCTA\n   );\n \n+  let hasCustomAssemblyFormat = 1;\n }\n \n //===----------------------------------------------------------------------===//\n@@ -403,6 +401,7 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n       bool isAVec4 = !isARow && (shapeA[isARow] <= 16);\n       bool isBVec4 = isBRow && (shapeB[isBRow] <= 16);\n       // 4-bits to encode 4 booleans: [isARow, isBRow, isAVec4, isBVec4]\n+      // 3-bits to encode the MMA ID to make each unique\n       int versionMinor = (isARow * (1<<0)) |\\\n                          (isBRow * (1<<1)) |\\\n                          (isAVec4 * (1<<2)) |\\\n@@ -424,13 +423,9 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n     // Number of bits in versionMinor to hold the ID of the MMA encoding instance.\n     // Here 5 bits can hold 32 IDs in a single module.\n     static constexpr int numBitsToHoldMmaV1ID{5};\n-\n-    // Here is a temporary flag that indicates whether we need to update the warpsPerCTA for MMAv1, since the current backend cannot support the updated wpt.\n-    // The mmav1's wpt-related logic is separated into multiple files, so a global flag is added here for universal coordination.\n-    // TODO[Superjomn]: Remove this flag once the MMAv1 backend is ready.\n-    static constexpr bool _mmaV1UpdateWpt{false};\n   }];\n \n+  let hasCustomAssemblyFormat = 1;\n }\n \n def SliceEncodingAttr : DistributedEncoding<\"SliceEncoding\"> {\n@@ -465,6 +460,8 @@ def SliceEncodingAttr : DistributedEncoding<\"SliceEncoding\"> {\n     template<class T>\n     SmallVector<T> paddedShape(ArrayRef<T> shape) const;\n   }];\n+\n+  let hasCustomAssemblyFormat = 1;\n }\n \n def DotOperandEncodingAttr : DistributedEncoding<\"DotOperandEncoding\"> {\n@@ -501,6 +498,7 @@ section 9.7.13.4.1 for more details.\n \n   ];\n \n+  let hasCustomAssemblyFormat = 1;\n   let extraClassDeclaration = extraBaseClassDeclaration;\n }\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUDialect.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -30,7 +30,7 @@ def TritonGPU_Dialect : Dialect {\n     }\n   }];\n   \n-\n+  let useDefaultAttributePrinterParser = 1;\n }\n \n #endif"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 24, "deletions": 12, "changes": 36, "file_content_changes": "@@ -3,11 +3,11 @@\n \n include \"triton/Dialect/TritonGPU/IR/TritonGPUDialect.td\"\n include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td\"\n-include \"mlir/Dialect/Arithmetic/IR/ArithmeticBase.td\"\n+include \"mlir/Dialect/Arith/IR/ArithBase.td\"\n include \"triton/Dialect/Triton/IR/TritonTypes.td\"\n include \"triton/Dialect/Triton/IR/TritonAttrDefs.td\"\n include \"mlir/IR/OpBase.td\"\n-include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n+include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n \n def ResultsAreSharedEncoding: NativeOpTrait<\"ResultsAreSharedEncoding\">;\n@@ -16,7 +16,9 @@ class TTG_Op<string mnemonic, list<Trait> traits = []> :\n     Op<TritonGPU_Dialect, mnemonic, traits>;\n \n def TTG_ConvertLayoutOp : TTG_Op<\"convert_layout\",\n-                                 [SameOperandsAndResultShape, NoSideEffect]> {\n+                                 [SameOperandsAndResultShape,\n+                                  SameOperandsAndResultElementType,\n+                                  Pure]> {\n   let summary = \"convert layout\";\n \n   let arguments = (ins TT_Tensor:$src);\n@@ -40,11 +42,24 @@ def TTG_AsyncWaitOp : TTG_Op<\"async_wait\"> {\n   }];\n }\n \n+def TTG_AsyncCommitGroupOp : TTG_Op<\"async_commit_group\"> {\n+  let summary = \"async commit group\";\n+\n+  let assemblyFormat = \"attr-dict\";\n+\n+  let extraClassDeclaration = [{\n+    static bool isSupported(int computeCapability) {\n+      return computeCapability >= 80;\n+    }\n+  }];\n+}\n+\n+\n // Port Arith_CmpIOp & Arith_CmpFOp & Std_SelectOp to TritonGPU.\n // This is needed because these ops don't\n // handle encodings\n // e.g., https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Arith/IR/ArithOps.td#L111\n-def TTG_CmpIOp : TTG_Op<\"cmpi\", [NoSideEffect, Elementwise, \n+def TTG_CmpIOp : TTG_Op<\"cmpi\", [Pure, Elementwise,\n                                  SameOperandsAndResultShape, \n                                  SameOperandsAndResultEncoding]> {\n   let summary = \"integer comparison operation\";\n@@ -58,7 +73,7 @@ def TTG_CmpIOp : TTG_Op<\"cmpi\", [NoSideEffect, Elementwise,\n   let results = (outs TT_BoolLike:$result);\n }\n \n-def TTG_CmpFOp : TTG_Op<\"cmpf\", [NoSideEffect, Elementwise, \n+def TTG_CmpFOp : TTG_Op<\"cmpf\", [Pure, Elementwise,\n                                  SameOperandsAndResultShape, \n                                  SameOperandsAndResultEncoding]> {\n   let summary = \"floating-point comparison operation\";\n@@ -73,9 +88,9 @@ def TTG_CmpFOp : TTG_Op<\"cmpf\", [NoSideEffect, Elementwise,\n }\n \n // TODO: migrate to arith::SelectOp on LLVM16\n-def TTG_SelectOp : TTG_Op<\"select\", [NoSideEffect, Elementwise, \n-                                 SameOperandsAndResultShape, \n-                                 SameOperandsAndResultEncoding]> {\n+def TTG_SelectOp : TTG_Op<\"select\", [Pure, Elementwise,\n+                                     SameOperandsAndResultShape,\n+                                     SameOperandsAndResultEncoding]> {\n   let summary = \"select operation\";\n \n   let description = [{}];\n@@ -173,10 +188,7 @@ def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n     }\n   }];\n \n-  // The custom parser could be replaced with oilist in LLVM-16\n-  let parser = [{ return parseInsertSliceAsyncOp(parser, result); }];\n-\n-  let printer = [{ return printInsertSliceAsyncOp(p, *this); }];\n+  let hasCustomAssemblyFormat = 1;\n }\n \n def TTG_AllocTensorOp : TTG_Op<\"alloc_tensor\", [MemoryEffects<[MemAlloc]>,  // Allocate shared memory"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -14,7 +14,7 @@ def TritonGPUPipeline : Pass<\"tritongpu-pipeline\", \"mlir::ModuleOp\"> {\n \n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n                            \"mlir::scf::SCFDialect\",\n-                           \"mlir::arith::ArithmeticDialect\"];\n+                           \"mlir::arith::ArithDialect\"];\n \n   let options = [\n     Option<\"numStages\", \"num-stages\",\n@@ -34,7 +34,7 @@ def TritonGPUPrefetch : Pass<\"tritongpu-prefetch\", \"mlir::ModuleOp\"> {\n \n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n                            \"mlir::scf::SCFDialect\",\n-                           \"mlir::arith::ArithmeticDialect\"];\n+                           \"mlir::arith::ArithDialect\"];\n }\n \n def TritonGPUCoalesce: Pass<\"tritongpu-coalesce\", \"mlir::ModuleOp\"> {"}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 6, "deletions": 8, "changes": 14, "file_content_changes": "@@ -18,8 +18,9 @@ AliasInfo AliasInfo::join(const AliasInfo &lhs, const AliasInfo &rhs) {\n   return ret;\n }\n \n-ChangeResult SharedMemoryAliasAnalysis::visitOperation(\n-    Operation *op, ArrayRef<LatticeElement<AliasInfo> *> operands) {\n+void SharedMemoryAliasAnalysis::visitOperation(\n+    Operation *op, ArrayRef<const dataflow::Lattice<AliasInfo> *> operands,\n+    ArrayRef<dataflow::Lattice<AliasInfo> *> results) {\n   AliasInfo aliasInfo;\n   bool pessimistic = true;\n   if (maybeSharedAllocationOp(op)) {\n@@ -44,14 +45,11 @@ ChangeResult SharedMemoryAliasAnalysis::visitOperation(\n   }\n \n   if (pessimistic) {\n-    return markAllPessimisticFixpoint(op->getResults());\n+    return setAllToEntryStates(results);\n   }\n   // Join all lattice elements\n-  ChangeResult result = ChangeResult::NoChange;\n-  for (Value value : op->getResults()) {\n-    result |= getLatticeElement(value).join(aliasInfo);\n-  }\n-  return result;\n+  for (auto *result : results)\n+    propagateIfChanged(result, result->join(aliasInfo));\n }\n \n AliasResult SharedMemoryAliasAnalysis::alias(Value lhs, Value rhs) {"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 29, "deletions": 16, "changes": 45, "file_content_changes": "@@ -1,4 +1,5 @@\n #include \"triton/Analysis/Allocation.h\"\n+#include \"mlir/Analysis/DataFlowFramework.h\"\n #include \"mlir/Analysis/Liveness.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n@@ -33,10 +34,8 @@ constexpr int kPtrBitWidth = 64;\n \n static std::pair<SmallVector<unsigned>, SmallVector<unsigned>>\n getCvtOrder(const Attribute &srcLayout, const Attribute &dstLayout) {\n-  auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>();\n   auto srcMmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n   auto srcDotLayout = srcLayout.dyn_cast<DotOperandEncodingAttr>();\n-  auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>();\n   auto dstMmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>();\n   auto dstDotLayout = dstLayout.dyn_cast<DotOperandEncodingAttr>();\n   assert(!(srcMmaLayout && dstMmaLayout) &&\n@@ -54,10 +53,17 @@ getCvtOrder(const Attribute &srcLayout, const Attribute &dstLayout) {\n SmallVector<unsigned>\n getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n                              unsigned &outVec) {\n-  auto srcTy = op.src().getType().cast<RankedTensorType>();\n-  auto dstTy = op.result().getType().cast<RankedTensorType>();\n+  auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n+  auto dstTy = op.getResult().getType().cast<RankedTensorType>();\n   Attribute srcLayout = srcTy.getEncoding();\n   Attribute dstLayout = dstTy.getEncoding();\n+\n+  // MmaToDotShortcut doesn't use shared mem\n+  if (auto mmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>())\n+    if (auto dotOperandLayout = dstLayout.dyn_cast<DotOperandEncodingAttr>())\n+      if (isMmaToDotShortcut(mmaLayout, dotOperandLayout))\n+        return {};\n+\n   assert(srcLayout && dstLayout &&\n          \"Unexpect layout in getScratchConfigForCvtLayout()\");\n   auto [inOrd, outOrd] = getCvtOrder(srcLayout, dstLayout);\n@@ -68,8 +74,10 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   inVec = outOrd[0] == 0 ? 1 : inOrd[0] == 0 ? 1 : srcContigPerThread;\n   outVec = outOrd[0] == 0 ? 1 : dstContigPerThread;\n \n-  auto srcShapePerCTA = getShapePerCTA(srcLayout);\n-  auto dstShapePerCTA = getShapePerCTA(dstLayout);\n+  auto srcShape = srcTy.getShape();\n+  auto dstShape = dstTy.getShape();\n+  auto srcShapePerCTA = getShapePerCTA(srcLayout, srcShape);\n+  auto dstShapePerCTA = getShapePerCTA(dstLayout, dstShape);\n \n   unsigned rank = dstTy.getRank();\n   SmallVector<unsigned> paddedRepShape(rank);\n@@ -92,7 +100,7 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n // TODO: extend beyond scalars\n SmallVector<unsigned> getScratchConfigForAtomicRMW(triton::AtomicRMWOp op) {\n   SmallVector<unsigned> smemShape;\n-  if (op.ptr().getType().isa<RankedTensorType>()) {\n+  if (op.getPtr().getType().isa<RankedTensorType>()) {\n     // do nothing or just assert because shared memory is not used in tensor up\n     // to now\n   } else {\n@@ -159,8 +167,8 @@ class AllocationAnalysis {\n       unsigned bytes = helper.getScratchSizeInBytes();\n       allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n-      auto srcTy = cvtLayout.src().getType().cast<RankedTensorType>();\n-      auto dstTy = cvtLayout.result().getType().cast<RankedTensorType>();\n+      auto srcTy = cvtLayout.getSrc().getType().cast<RankedTensorType>();\n+      auto dstTy = cvtLayout.getResult().getType().cast<RankedTensorType>();\n       auto srcEncoding = srcTy.getEncoding();\n       auto dstEncoding = dstTy.getEncoding();\n       if (srcEncoding.isa<SharedEncodingAttr>() ||\n@@ -215,10 +223,10 @@ class AllocationAnalysis {\n   }\n \n   void getValueAlias(Value value, SharedMemoryAliasAnalysis &analysis) {\n-    LatticeElement<AliasInfo> *latticeElement =\n-        analysis.lookupLatticeElement(value);\n+    dataflow::Lattice<AliasInfo> *latticeElement =\n+        analysis.getLatticeElement(value);\n     if (latticeElement) {\n-      auto &info = latticeElement->getValue();\n+      AliasInfo &info = latticeElement->getValue();\n       if (!info.getAllocs().empty()) {\n         for (auto alloc : info.getAllocs()) {\n           allocation->addAlias(value, alloc);\n@@ -235,14 +243,19 @@ class AllocationAnalysis {\n       getScratchValueSize(op);\n     });\n     // Get the alias values\n-    SharedMemoryAliasAnalysis aliasAnalysis(operation->getContext());\n-    aliasAnalysis.run(operation);\n+    std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n+    SharedMemoryAliasAnalysis *aliasAnalysis =\n+        solver->load<SharedMemoryAliasAnalysis>();\n+    if (failed(solver->initializeAndRun(operation))) {\n+      // TODO: return error instead of bailing out..\n+      llvm_unreachable(\"failed to run SharedMemoryAliasAnalysis\");\n+    }\n     operation->walk<WalkOrder::PreOrder>([&](Operation *op) {\n       for (auto operand : op->getOperands()) {\n-        getValueAlias(operand, aliasAnalysis);\n+        getValueAlias(operand, *aliasAnalysis);\n       }\n       for (auto value : op->getResults()) {\n-        getValueAlias(value, aliasAnalysis);\n+        getValueAlias(value, *aliasAnalysis);\n       }\n     });\n   }"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 779, "deletions": 173, "changes": 952, "file_content_changes": "@@ -1,49 +1,58 @@\n-#include \"mlir/Analysis/DataFlowAnalysis.h\"\n+#include \"mlir/Analysis/DataFlowFramework.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"llvm/Support/raw_ostream.h\"\n-#include <iostream>\n \n #include \"triton/Analysis/AxisInfo.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n namespace mlir {\n \n-//===----------------------------------------------------------------------===//\n-// AxisInfo\n-//===----------------------------------------------------------------------===//\n-\n // Function for extended Euclidean Algorithm\n-static int gcd_impl(int a, int b, int *x, int *y) {\n+static int64_t gcdImpl(int64_t a, int64_t b, int64_t *x, int64_t *y) {\n   // Base Case\n   if (a == 0) {\n     *x = 0;\n     *y = 1;\n     return b;\n   }\n-  int x1, y1; // To store results of recursive call\n-  int gcd = gcd_impl(b % a, a, &x1, &y1);\n+  int64_t x1, y1; // To store results of recursive call\n+  int64_t gcd = gcdImpl(b % a, a, &x1, &y1);\n   // Update x and y using results of\n   // recursive call\n   *x = y1 - (b / a) * x1;\n   *y = x1;\n   return gcd;\n }\n \n-static int gcd(int a, int b) {\n-  int x, y;\n-  return gcd_impl(a, b, &x, &y);\n+static int64_t gcd(int64_t a, int64_t b) {\n+  if (a == 0)\n+    return b;\n+  if (b == 0)\n+    return a;\n+  int64_t x, y;\n+  return gcdImpl(a, b, &x, &y);\n }\n \n+static constexpr int log2Int(int64_t num) {\n+  return (num > 1) ? 1 + log2Int(num / 2) : 0;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// AxisInfo\n+//===----------------------------------------------------------------------===//\n+\n AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n-  size_t rank = 1;\n+  auto rank = 1;\n   if (TensorType ty = value.getType().dyn_cast<TensorType>())\n     rank = ty.getRank();\n-  int divHint = 1;\n+  auto contiHint = 1;\n+  auto divHint = 1;\n+  auto constHint = 1;\n   BlockArgument blockArg = value.dyn_cast<BlockArgument>();\n   if (blockArg && blockArg.getOwner()->isEntryBlock()) {\n     Operation *op = blockArg.getOwner()->getParentOp();\n-    if (FuncOp fun = dyn_cast<FuncOp>(op)) {\n+    if (func::FuncOp fun = dyn_cast<func::FuncOp>(op)) {\n       Attribute attr =\n           fun.getArgAttr(blockArg.getArgNumber(), \"tt.divisibility\");\n       if (attr)\n@@ -53,139 +62,371 @@ AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n           fun.getArgAttr(blockArg.getArgNumber(), \"tt.divisibility\");\n       if (attr)\n         divHint = attr.cast<IntegerAttr>().getValue().getZExtValue();\n+    } else {\n+      // Derive the divisibility of the induction variable only when\n+      // the step and the lower bound are both constants\n+      if (auto forOp = dyn_cast<scf::ForOp>(op)) {\n+        if (blockArg == forOp.getInductionVar()) {\n+          if (auto lowerBound =\n+                  forOp.getLowerBound().getDefiningOp<arith::ConstantOp>()) {\n+            if (auto step =\n+                    forOp.getStep().getDefiningOp<arith::ConstantOp>()) {\n+              auto lowerBoundVal = lowerBound.getValue()\n+                                       .cast<IntegerAttr>()\n+                                       .getValue()\n+                                       .getZExtValue();\n+              auto stepVal =\n+                  step.getValue().cast<IntegerAttr>().getValue().getZExtValue();\n+              auto k = gcd(lowerBoundVal, stepVal);\n+              if (k != 0)\n+                divHint = k;\n+            }\n+          }\n+        }\n+      }\n+    }\n+  } else if (Operation *op = value.getDefiningOp()) {\n+    DimVectorT knownContiguity(rank, 1);\n+    DimVectorT knownDivisibility(rank, 1);\n+    DimVectorT knownConstancy(rank, 1);\n+    if (Attribute attr = op->getAttr(\"tt.divisibility\")) {\n+      auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+      knownDivisibility = DimVectorT(vals.begin(), vals.end());\n+    }\n+    if (Attribute attr = op->getAttr(\"tt.contiguity\")) {\n+      auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+      knownContiguity = DimVectorT(vals.begin(), vals.end());\n+    }\n+    if (Attribute attr = op->getAttr(\"tt.constancy\")) {\n+      auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+      knownConstancy = DimVectorT(vals.begin(), vals.end());\n     }\n+    return AxisInfo(knownContiguity, knownDivisibility, knownConstancy);\n   }\n-  DimVectorT contiguity(rank, 1);\n-  DimVectorT divisibility(rank, divHint);\n-  DimVectorT constancy(rank, 1);\n-  return AxisInfo(contiguity, divisibility, constancy);\n+\n+  return AxisInfo(/*knownContiguity=*/DimVectorT(rank, contiHint),\n+                  /*knownDivisibility=*/DimVectorT(rank, divHint),\n+                  /*knownConstancy=*/DimVectorT(rank, constHint));\n }\n \n // The gcd of both arguments for each dimension\n AxisInfo AxisInfo::join(const AxisInfo &lhs, const AxisInfo &rhs) {\n-  DimVectorT retContiguity;\n-  DimVectorT retDivisibility;\n-  DimVectorT retConstancy;\n-  for (int d = 0; d < lhs.getRank(); ++d) {\n-    retContiguity.push_back(gcd(lhs.getContiguity(d), rhs.getContiguity(d)));\n-    retDivisibility.push_back(\n-        gcd(lhs.getDivisibility(d), rhs.getDivisibility(d)));\n-    retConstancy.push_back(gcd(lhs.getConstancy(d), rhs.getConstancy(d)));\n-  }\n-  return AxisInfo(retContiguity, retDivisibility, retConstancy);\n+  // If one argument is not initialized, return the other.\n+  if (lhs.getRank() == 0)\n+    return rhs;\n+  if (rhs.getRank() == 0)\n+    return lhs;\n+  DimVectorT contiguity;\n+  DimVectorT divisibility;\n+  DimVectorT constancy;\n+  for (auto d = 0; d < lhs.getRank(); ++d) {\n+    contiguity.push_back(gcd(lhs.getContiguity(d), rhs.getContiguity(d)));\n+    divisibility.push_back(gcd(lhs.getDivisibility(d), rhs.getDivisibility(d)));\n+    constancy.push_back(gcd(lhs.getConstancy(d), rhs.getConstancy(d)));\n+  }\n+  std::optional<int64_t> constantValue;\n+  if (lhs.getConstantValue().has_value() &&\n+      rhs.getConstantValue().has_value() &&\n+      lhs.getConstantValue() == rhs.getConstantValue())\n+    constantValue = lhs.getConstantValue();\n+  return AxisInfo(contiguity, divisibility, constancy, constantValue);\n }\n \n //===----------------------------------------------------------------------===//\n-// AxisInfoAnalysis\n+// AxisInfoVisitor\n //===----------------------------------------------------------------------===//\n \n-AxisInfo AxisInfoAnalysis::visitBinaryOp(\n-    Operation *op, AxisInfo lhsInfo, AxisInfo rhsInfo,\n-    const std::function<int(AxisInfo, AxisInfo, int)> &getContiguity,\n-    const std::function<int(AxisInfo, AxisInfo, int)> &getDivisibility,\n-    const std::function<int(AxisInfo, AxisInfo, int)> &getConstancy) {\n-  int rank = lhsInfo.getRank();\n-  AxisInfo::DimVectorT newContiguity;\n-  AxisInfo::DimVectorT newDivisibility;\n-  AxisInfo::DimVectorT newConstancy;\n-  for (int d = 0; d < rank; ++d) {\n-    newContiguity.push_back(getContiguity(lhsInfo, rhsInfo, d));\n-    newDivisibility.push_back(getDivisibility(lhsInfo, rhsInfo, d));\n-    newConstancy.push_back(getConstancy(lhsInfo, rhsInfo, d));\n-  }\n-  return AxisInfo(newContiguity, newDivisibility, newConstancy);\n-}\n+template <typename OpTy>\n+class CastOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n \n-ChangeResult AxisInfoAnalysis::visitOperation(\n-    Operation *op, ArrayRef<LatticeElement<AxisInfo> *> operands) {\n-  AxisInfo curr;\n-  // This preserves the input axes (e.g., cast):\n-  if (llvm::isa<arith::ExtSIOp, arith::ExtUIOp, arith::TruncIOp,\n-                triton::PtrToIntOp, triton::IntToPtrOp,\n-                triton::gpu::ConvertLayoutOp>(op))\n-    curr = operands[0]->getValue();\n-  // Constant ranges\n-  if (triton::MakeRangeOp make_range =\n-          llvm::dyn_cast<triton::MakeRangeOp>(op)) {\n-    int start = make_range.start();\n-    int end = make_range.end();\n-    AxisInfo::DimVectorT contiguity = {end - start};\n-    AxisInfo::DimVectorT divisibility = {highestPowOf2Divisor(start)};\n-    AxisInfo::DimVectorT constancy = {1};\n-    curr = AxisInfo(contiguity, divisibility, constancy);\n-  }\n-  // Constant\n-  if (arith::ConstantOp constant = llvm::dyn_cast<arith::ConstantOp>(op)) {\n-    auto intAttr = constant.getValue().dyn_cast<IntegerAttr>();\n-    if (intAttr) {\n-      size_t val = intAttr.getValue().getZExtValue();\n-      curr = AxisInfo({1}, {highestPowOf2Divisor(val)}, {1});\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    return operands[0]->getValue();\n+  }\n+};\n+\n+class MakeRangeOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::MakeRangeOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::MakeRangeOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(triton::MakeRangeOp op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto start = op.getStart();\n+    auto end = op.getEnd();\n+    return AxisInfo(/*contiguity=*/{end - start},\n+                    /*divisibility=*/{highestPowOf2Divisor(start)},\n+                    /*constancy=*/{1});\n+  }\n+};\n+\n+class ConstantOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<arith::ConstantOp> {\n+public:\n+  using AxisInfoVisitorImpl<arith::ConstantOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(arith::ConstantOp op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto intAttr = op.getValue().dyn_cast<IntegerAttr>();\n+    auto boolAttr = op.getValue().dyn_cast<BoolAttr>();\n+    if (intAttr || boolAttr) {\n+      int64_t value{};\n+      if (intAttr)\n+        value = intAttr.getValue().getZExtValue();\n+      else\n+        value = boolAttr.getValue() ? 1 : 0;\n+      return AxisInfo(/*contiguity=*/{1},\n+                      /*divisibility=*/{highestPowOf2Divisor(value)},\n+                      /*constancy=*/{1},\n+                      /*knownConstantValue=*/{value});\n     }\n     // TODO: generalize to dense attr\n-    auto splatAttr = constant.getValue().dyn_cast<SplatElementsAttr>();\n-    if (splatAttr && splatAttr.getElementType().isInteger(32)) {\n-      auto value = splatAttr.getSplatValue<int>();\n+    auto splatAttr = op.getValue().dyn_cast<SplatElementsAttr>();\n+    if (splatAttr && splatAttr.getElementType().isIntOrIndex()) {\n+      int64_t value = splatAttr.getSplatValue<APInt>().getZExtValue();\n       TensorType ty = splatAttr.getType().cast<TensorType>();\n-      curr = AxisInfo(\n-          AxisInfo::DimVectorT(ty.getRank(), 1),\n+      return AxisInfo(\n+          /*contiguity=*/AxisInfo::DimVectorT(ty.getRank(), 1),\n+          /*divisibility=*/\n           AxisInfo::DimVectorT(ty.getRank(), highestPowOf2Divisor(value)),\n-          AxisInfo::DimVectorT(ty.getShape().begin(), ty.getShape().end()));\n+          /*constancy=*/\n+          AxisInfo::DimVectorT(ty.getShape().begin(), ty.getShape().end()),\n+          /*knownConstantValue=*/{value});\n+    }\n+    return AxisInfo();\n+  }\n+};\n+\n+template <typename OpTy>\n+class AddSubOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    return std::max(gcd(lhs.getConstancy(dim), rhs.getContiguity(dim)),\n+                    gcd(lhs.getContiguity(dim), rhs.getConstancy(dim)));\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    // lhs = k * d_lhs = k * k' * gcd(d_lhs, d_rhs)\n+    // rhs = p * d_rhs = p * p' * gcd(d_lhs, d_rhs)\n+    // lhs + rhs = k * d_lhs + p * d_rhs = (k * d_lhs + p * d_rhs) *\n+    // gcd(d_lhs, d_rhs)\n+    return gcd(lhs.getDivisibility(dim), rhs.getDivisibility(dim));\n+  }\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value()) {\n+      if constexpr (std::is_same_v<OpTy, arith::AddIOp> ||\n+                    std::is_same_v<OpTy, triton::AddPtrOp>) {\n+        return {lhs.getConstantValue().value() +\n+                rhs.getConstantValue().value()};\n+      } else if constexpr (std::is_same_v<OpTy, arith::SubIOp>) {\n+        return {lhs.getConstantValue().value() -\n+                rhs.getConstantValue().value()};\n+      }\n+    }\n+    return {};\n+  }\n+};\n+\n+class MulIOpAxisInfoVisitor final : public BinaryOpVisitorImpl<arith::MulIOp> {\n+public:\n+  using BinaryOpVisitorImpl<arith::MulIOp>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(arith::MulIOp op, const AxisInfo &lhs,\n+                        const AxisInfo &rhs, int dim) override {\n+    // lhs * 1 = lhs\n+    auto lhsContiguity =\n+        rhs.getConstantValue().has_value() && rhs.getConstantValue() == 1\n+            ? lhs.getContiguity(dim)\n+            : 1;\n+    // 1 * rhs = rhs\n+    auto rhsContiguity =\n+        lhs.getConstantValue().has_value() && lhs.getConstantValue() == 1\n+            ? rhs.getContiguity(dim)\n+            : 1;\n+    return std::max(lhsContiguity, rhsContiguity);\n+  }\n+\n+  int64_t getConstancy(arith::MulIOp op, const AxisInfo &lhs,\n+                       const AxisInfo &rhs, int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  int64_t getDivisibility(arith::MulIOp op, const AxisInfo &lhs,\n+                          const AxisInfo &rhs, int dim) override {\n+    // lhs = k * d_lhs\n+    // rhs = p * d_rhs\n+    // lhs * rhs = k * d_lhs * p * d_rhs = k * p * d_lhs * d_rhs\n+    return lhs.getDivisibility(dim) * rhs.getDivisibility(dim);\n+  }\n+\n+  std::optional<int64_t> getConstantValue(arith::MulIOp op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() * rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class DivOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    // lhs / 1 = lhs\n+    return rhs.getConstantValue().has_value() &&\n+                   rhs.getConstantValue().value() == 1\n+               ? lhs.getContiguity(dim)\n+               : 1;\n+  }\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return BinaryOpVisitorImpl<OpTy>::getConstancy(op, lhs, rhs, dim);\n+    auto shape = resTy.getShape();\n+    // Case 1: both lhs and rhs are constants.\n+    auto constancy = gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+    // Case 2: lhs contiguous, rhs constant.\n+    // lhs: d_lhs * k, d_lhs * k + 1, ..., d_lhs * k + n\n+    // rhs: d_rhs * p, d_rhs * p, ..., d_rhs * p\n+    // lhs / rhs = d_lhs * k / (d_rhs * p), (d_lhs * k + 1) / (d_rhs * p),\n+    // ..., (d_lhs * k + n) / (d_rhs * p)\n+    // Because d_lhs % d_rhs = 0 || d_rhs % d_lhs = 0,\n+    // the minimal constancy is gcd(d_lhs, d_rhs).\n+    // Since gcd(d_lhs, d_rhs) maybe > len(lhs),\n+    // we need to use another gcd to get the actual constancy.\n+    if (AxisInfoVisitor::isContiguousDim(lhs, shape, dim) &&\n+        AxisInfoVisitor::isConstantDim(rhs, shape, dim)) {\n+      constancy = std::max(constancy, gcd(lhs.getContiguity(dim),\n+                                          gcd(lhs.getDivisibility(dim),\n+                                              rhs.getDivisibility(dim))));\n+    }\n+    return constancy;\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    // Case 1: lhs is 0\n+    if (lhs.getConstantValue().has_value() &&\n+        lhs.getConstantValue().value() == 0)\n+      return lhs.getDivisibility(dim);\n+    // Case 2: rhs is constant\n+    if (rhs.getConstantValue().has_value()) {\n+      auto lhsDivisibility = lhs.getDivisibility(dim);\n+      auto rhsValue = rhs.getConstantValue().value();\n+      if (lhsDivisibility % rhsValue == 0)\n+        return lhsDivisibility / rhsValue;\n     }\n+    // Case 3: both are not constant\n+    return 1;\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() / rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class RemOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return BinaryOpVisitorImpl<OpTy>::getContiguity(op, lhs, rhs, dim);\n+    auto shape = resTy.getShape();\n+    int64_t contiguity = 1;\n+    // lhs contiguous, rhs constant\n+    // lhs: d_lhs * k, d_lhs * k + 1, ..., d_lhs * k + n\n+    // rhs: d_rhs * p, d_rhs * p, ..., d_rhs * p\n+    // lhs % rhs = d_lhs * k % (d_rhs * p), (d_lhs * k + 1) % (d_rhs * p),\n+    // ..., (d_lhs * k + n) % (d_rhs * p)\n+    // Because d_lhs % d_rhs = 0 || d_rhs % d_lhs = 0,\n+    // The minimal contiguity is gcd(d_lhs, d_rhs).\n+    // Since gcd(d_lhs, d_rhs) maybe > len(lhs),\n+    // we need to use another gcd to get the actual contiguity.\n+    if (AxisInfoVisitor::isContiguousDim(lhs, shape, dim) &&\n+        AxisInfoVisitor::isConstantDim(rhs, shape, dim)) {\n+      contiguity = std::max(contiguity, gcd(lhs.getContiguity(dim),\n+                                            gcd(lhs.getDivisibility(dim),\n+                                                rhs.getDivisibility(dim))));\n+    }\n+    return contiguity;\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    // lhs: d_lhs * k = gcd(d_lhs, d_rhs) * k' * k = gcd(d_lhs, d_rhs) * k''\n+    // rhs: d_rhs * p = gcd(d_lhs, d_rhs) * p' * p = gcd(d_lhs, d_rhs) * p''\n+    // lhs = gcd(d_lhs, d_rhs) * k'' = gcd(d_lhs, d_rhs) * d + r\n+    // r must be divisible by gcd(d_lhs, d_rhs)\n+    return gcd(lhs.getDivisibility(dim), rhs.getDivisibility(dim));\n+  };\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return BinaryOpVisitorImpl<OpTy>::getConstancy(op, lhs, rhs, dim);\n+    auto shape = resTy.getShape();\n+    // lhs % 1 = 0\n+    return rhs.getConstantValue().has_value() &&\n+                   rhs.getConstantValue().value() == 1\n+               ? shape[dim]\n+               : gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() % rhs.getConstantValue().value()};\n+    else if (rhs.getConstantValue().has_value() &&\n+             rhs.getConstantValue().value() == 1)\n+      return {0};\n+    return {};\n   }\n-  // TODO: refactor & complete binary ops\n-  // Addition\n-  if (llvm::isa<arith::AddIOp, triton::AddPtrOp>(op)) {\n-    auto newContiguity = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return std::max(gcd(lhs.getContiguity(d), rhs.getConstancy(d)),\n-                      gcd(lhs.getConstancy(d), rhs.getContiguity(d)));\n-    };\n-    auto newConstancy = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n-    };\n-    auto newDivisibility = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getDivisibility(d), rhs.getDivisibility(d));\n-    };\n-    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n-                         newContiguity, newDivisibility, newConstancy);\n-  }\n-  // Multiplication\n-  if (llvm::isa<arith::MulIOp>(op)) {\n-    auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };\n-    auto newConstancy = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n-    };\n-    auto newDivisibility = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return lhs.getDivisibility(d) * rhs.getDivisibility(d);\n-    };\n-    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n-                         newContiguity, newDivisibility, newConstancy);\n-  }\n-  // Remainder\n-  if (llvm::isa<arith::RemSIOp, arith::RemUIOp>(op)) {\n-    auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getContiguity(d), rhs.getDivisibility(d));\n-    };\n-    auto newDivisibility = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getDivisibility(d), rhs.getDivisibility(d));\n-    };\n-    auto newConstancy = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n-    };\n-    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n-                         newContiguity, newDivisibility, newConstancy);\n-  }\n-  // TODO: All other binary ops\n-  if (llvm::isa<arith::AndIOp, arith::OrIOp>(op)) {\n-    auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };\n-    auto newDivisibility = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };\n-    auto newConstancy = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n-    };\n-    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n-                         newContiguity, newDivisibility, newConstancy);\n-  }\n-  // Splat\n-  if (llvm::isa<triton::SplatOp>(op)) {\n+};\n+\n+class SplatOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::SplatOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::SplatOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(triton::SplatOp op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n     Type _retTy = *op->result_type_begin();\n     TensorType retTy = _retTy.cast<TensorType>();\n     AxisInfo opInfo = operands[0]->getValue();\n@@ -197,21 +438,39 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n       divisibility.push_back(opInfo.getDivisibility(0));\n       constancy.push_back(retTy.getShape()[d]);\n     }\n-    curr = AxisInfo(contiguity, divisibility, constancy);\n+    return AxisInfo(contiguity, divisibility, constancy,\n+                    operands[0]->getValue().getConstantValue());\n   }\n-  // expandDims\n-  if (auto expandDims = llvm::dyn_cast<triton::ExpandDimsOp>(op)) {\n+};\n+\n+class ExpandDimsOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::ExpandDimsOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::ExpandDimsOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(triton::ExpandDimsOp op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n     AxisInfo opInfo = operands[0]->getValue();\n     AxisInfo::DimVectorT contiguity = opInfo.getContiguity();\n     AxisInfo::DimVectorT divisibility = opInfo.getDivisibility();\n     AxisInfo::DimVectorT constancy = opInfo.getConstancy();\n-    contiguity.insert(contiguity.begin() + expandDims.axis(), 1);\n-    divisibility.insert(divisibility.begin() + expandDims.axis(), 1);\n-    constancy.insert(constancy.begin() + expandDims.axis(), 1);\n-    curr = AxisInfo(contiguity, divisibility, constancy);\n+    contiguity.insert(contiguity.begin() + op.getAxis(), 1);\n+    divisibility.insert(divisibility.begin() + op.getAxis(), 1);\n+    constancy.insert(constancy.begin() + op.getAxis(), 1);\n+    return AxisInfo(contiguity, divisibility, constancy,\n+                    operands[0]->getValue().getConstantValue());\n   }\n-  // Broadcast\n-  if (llvm::isa<triton::BroadcastOp>(op)) {\n+};\n+\n+class BroadcastOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::BroadcastOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::BroadcastOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(triton::BroadcastOp op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n     Type _retTy = *op->result_type_begin();\n     Type _opTy = *op->operand_type_begin();\n     TensorType retTy = _retTy.cast<TensorType>();\n@@ -228,55 +487,393 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n       constancy.push_back(opShape[d] == 1 ? retShape[d]\n                                           : opInfo.getConstancy(d));\n     }\n-    curr = AxisInfo(contiguity, divisibility, constancy);\n+    return AxisInfo(contiguity, divisibility, constancy,\n+                    operands[0]->getValue().getConstantValue());\n   }\n+};\n+\n+template <typename OpTy>\n+class CmpOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n \n-  // CmpI\n-  if ((llvm::dyn_cast<arith::CmpIOp>(op) ||\n-       llvm::dyn_cast<triton::gpu::CmpIOp>(op)) &&\n-      op->getResult(0).getType().dyn_cast<TensorType>()) {\n-    auto resTy = op->getResult(0).getType().cast<TensorType>();\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return AxisInfo();\n+    auto shape = resTy.getShape();\n     short rank = resTy.getRank();\n     auto lhsInfo = operands[0]->getValue();\n     auto rhsInfo = operands[1]->getValue();\n-    auto shape = resTy.getShape();\n \n     AxisInfo::DimVectorT contiguity, divisibility, constancy;\n+    std::optional<int64_t> constantValue;\n     for (short d = 0; d < rank; ++d) {\n-      if (rhsInfo.getConstancy(d) % lhsInfo.getContiguity(d) == 0 ||\n-          rhsInfo.getConstancy(d) % lhsInfo.getConstancy(d))\n-        constancy.push_back(\n-            gcd(lhsInfo.getDivisibility(d), rhsInfo.getDivisibility(d)));\n-      else\n-        constancy.push_back(1);\n+      int64_t constHint = 1;\n+      if (lhsInfo.getConstantValue().has_value() &&\n+          rhsInfo.getConstantValue().has_value()) {\n+        constHint = lhsInfo.getConstancy(d);\n+        constantValue =\n+            compare(getPredicate(op), lhsInfo.getConstantValue().value(),\n+                    rhsInfo.getConstantValue().value())\n+                ? 1\n+                : 0;\n+      } else {\n+        // Case 1: lhs and rhs are both partial constants\n+        constHint = gcd(lhsInfo.getConstancy(d), rhsInfo.getConstancy(d));\n+        // Case 2: lhs all constant, rhs all contiguous\n+        // NOTE:\n+        // lhs: 4 4 4 4\n+        // rhs: 4 5 6 7\n+        // lhs ge rhs: 1, 0, 0, 0\n+        // Case 3: lhs all contiguous, rhs all constant\n+        // NOTE\n+        // lhs: 4 5 6 7\n+        // rhs: 4 4 4 4\n+        // lhs sle rhs: 1, 0, 0, 0\n+        if (/*Case 2=*/(\n+                notGePredicate(getPredicate(op)) &&\n+                (AxisInfoVisitor::isConstantDim(lhsInfo, shape, d) &&\n+                 AxisInfoVisitor::isContiguousDim(rhsInfo, shape, d))) ||\n+            /*Case 3=*/(notLePredicate(getPredicate(op)) &&\n+                        (AxisInfoVisitor::isContiguousDim(lhsInfo, shape, d) &&\n+                         AxisInfoVisitor::isConstantDim(rhsInfo, shape, d)))) {\n+          constHint = std::max(constHint, gcd(lhsInfo.getContiguity(d),\n+                                              gcd(lhsInfo.getDivisibility(d),\n+                                                  rhsInfo.getDivisibility(d))));\n+        }\n+      }\n \n-      divisibility.push_back(shape[d]);\n+      constancy.push_back(constHint);\n+      divisibility.push_back(1);\n       contiguity.push_back(1);\n     }\n \n-    curr = AxisInfo(contiguity, divisibility, constancy);\n+    return AxisInfo(contiguity, divisibility, constancy, constantValue);\n+  }\n+\n+private:\n+  static arith::CmpIPredicate getPredicate(triton::gpu::CmpIOp op) {\n+    return op.getPredicate();\n+  }\n+\n+  static arith::CmpIPredicate getPredicate(arith::CmpIOp op) {\n+    return op.getPredicate();\n+  }\n+\n+  static bool notGePredicate(arith::CmpIPredicate predicate) {\n+    return predicate != arith::CmpIPredicate::sge &&\n+           predicate != arith::CmpIPredicate::uge;\n+  }\n+\n+  static bool notLePredicate(arith::CmpIPredicate predicate) {\n+    return predicate != arith::CmpIPredicate::sle &&\n+           predicate != arith::CmpIPredicate::ule;\n+  }\n+\n+  static bool compare(arith::CmpIPredicate predicate, int64_t lhs,\n+                      int64_t rhs) {\n+    switch (predicate) {\n+    case arith::CmpIPredicate::eq:\n+      return lhs == rhs;\n+    case arith::CmpIPredicate::ne:\n+      return lhs != rhs;\n+    case arith::CmpIPredicate::slt:\n+      return lhs < rhs;\n+    case arith::CmpIPredicate::sle:\n+      return lhs <= rhs;\n+    case arith::CmpIPredicate::sgt:\n+      return lhs > rhs;\n+    case arith::CmpIPredicate::sge:\n+      return lhs >= rhs;\n+    case arith::CmpIPredicate::ult:\n+      return (uint64_t)lhs < (uint64_t)rhs;\n+    case arith::CmpIPredicate::ule:\n+      return (uint64_t)lhs <= (uint64_t)rhs;\n+    case arith::CmpIPredicate::ugt:\n+      return (uint64_t)lhs > (uint64_t)rhs;\n+    case arith::CmpIPredicate::uge:\n+      return (uint64_t)lhs >= (uint64_t)rhs;\n+    default:\n+      break;\n+    }\n+    llvm_unreachable(\"unknown comparison predicate\");\n+  }\n+};\n+\n+template <typename OpTy>\n+class SelectOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return AxisInfo();\n+    auto shape = resTy.getShape();\n+    auto rank = shape.size();\n+    auto condConstancy = operands[0]->getValue().getConstancy();\n+    auto lhsInfo = operands[1]->getValue();\n+    auto rhsInfo = operands[2]->getValue();\n+\n+    AxisInfo::DimVectorT contiguity, divisibility, constancy;\n+    std::optional<int64_t> constantValue;\n+    if (operands[0]->getValue().getConstantValue().has_value()) {\n+      if (operands[0]->getValue().getConstantValue() == 0) {\n+        contiguity = rhsInfo.getContiguity();\n+        divisibility = rhsInfo.getDivisibility();\n+        constancy = rhsInfo.getConstancy();\n+        constantValue = rhsInfo.getConstantValue();\n+      } else {\n+        contiguity = lhsInfo.getContiguity();\n+        divisibility = lhsInfo.getDivisibility();\n+        constancy = lhsInfo.getConstancy();\n+        constantValue = lhsInfo.getConstantValue();\n+      }\n+    } else {\n+      for (auto d = 0; d < rank; ++d) {\n+        constancy.push_back(\n+            std::min(gcd(lhsInfo.getConstancy(d), condConstancy[d]),\n+                     gcd(rhsInfo.getConstancy(d), condConstancy[d])));\n+        divisibility.push_back(\n+            std::min(lhsInfo.getDivisibility(d), rhsInfo.getDivisibility(d)));\n+        contiguity.push_back(\n+            std::min(gcd(lhsInfo.getContiguity(d), condConstancy[d]),\n+                     gcd(rhsInfo.getContiguity(d), condConstancy[d])));\n+      }\n+      if (lhsInfo.getConstantValue().has_value() &&\n+          rhsInfo.getConstantValue().has_value() &&\n+          lhsInfo.getConstantValue() == rhsInfo.getConstantValue())\n+        constantValue = lhsInfo.getConstantValue();\n+    }\n+\n+    return AxisInfo(contiguity, divisibility, constancy, constantValue);\n+  }\n+};\n+\n+template <typename OpTy>\n+class LogicalOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value()) {\n+      if constexpr (std::is_same<OpTy, arith::AndIOp>::value) {\n+        return {lhs.getConstantValue().value() &\n+                rhs.getConstantValue().value()};\n+      } else if constexpr (std::is_same<OpTy, arith::OrIOp>::value) {\n+        return {lhs.getConstantValue().value() |\n+                rhs.getConstantValue().value()};\n+      } else if constexpr (std::is_same<OpTy, arith::XOrIOp>::value) {\n+        return {lhs.getConstantValue().value() ^\n+                rhs.getConstantValue().value()};\n+      }\n+    }\n+    return {};\n+  }\n+};\n+\n+class ShLIOpAxisInfoVisitor final : public BinaryOpVisitorImpl<arith::ShLIOp> {\n+public:\n+  using BinaryOpVisitorImpl<arith::ShLIOp>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(arith::ShLIOp op, const AxisInfo &lhs,\n+                        const AxisInfo &rhs, int dim) override {\n+    if (rhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().value() == 0)\n+      return lhs.getContiguity(dim);\n+    else\n+      return 1;\n   }\n \n-  // UnrealizedConversionCast\n+  int64_t getDivisibility(arith::ShLIOp op, const AxisInfo &lhs,\n+                          const AxisInfo &rhs, int dim) override {\n+    auto shift = rhs.getConstantValue().has_value()\n+                     ? rhs.getConstantValue().value()\n+                     : rhs.getDivisibility(dim);\n+    auto numBits = log2Int(lhs.getDivisibility(dim));\n+    auto maxBits = log2Int(highestPowOf2Divisor<int64_t>(0));\n+    // Make sure the return value doesn't exceed highestPowOf2Divisor<int64>(0)\n+    if (shift + numBits > maxBits)\n+      return highestPowOf2Divisor<int64_t>(0);\n+    return lhs.getDivisibility(dim) << shift;\n+  }\n+\n+  int64_t getConstancy(arith::ShLIOp op, const AxisInfo &lhs,\n+                       const AxisInfo &rhs, int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(arith::ShLIOp op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() << rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class ShROpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    if (rhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().value() == 0)\n+      return lhs.getContiguity(dim);\n+    else\n+      return 1;\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    if (rhs.getConstantValue().has_value())\n+      return std::max<int64_t>(1, lhs.getDivisibility(dim) /\n+                                      (1 << rhs.getConstantValue().value()));\n+    else\n+      return std::max<int64_t>(1, lhs.getDivisibility(dim) /\n+                                      (1 << rhs.getDivisibility(dim)));\n+  }\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() >> rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class MaxMinOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto lhsInfo = operands[0]->getValue();\n+    auto rhsInfo = operands[1]->getValue();\n+    std::optional<int64_t> constantValue;\n+    if (lhsInfo.getConstantValue().has_value() &&\n+        rhsInfo.getConstantValue().has_value()) {\n+      if constexpr (std::is_same_v<OpTy, arith::MaxSIOp> ||\n+                    std::is_same_v<OpTy, arith::MaxUIOp>) {\n+        constantValue = {std::max(lhsInfo.getConstantValue().value(),\n+                                  rhsInfo.getConstantValue().value())};\n+      } else if constexpr (std::is_same_v<OpTy, arith::MinSIOp> ||\n+                           std::is_same_v<OpTy, arith::MinUIOp>) {\n+        constantValue = {std::min(lhsInfo.getConstantValue().value(),\n+                                  rhsInfo.getConstantValue().value())};\n+      }\n+    }\n+    auto rank = lhsInfo.getRank();\n+    return AxisInfo(/*knownContiguity=*/AxisInfo::DimVectorT(rank, 1),\n+                    /*knownDivisibility=*/AxisInfo::DimVectorT(rank, 1),\n+                    /*knownConstancy=*/AxisInfo::DimVectorT(rank, 1),\n+                    /*constantValue=*/constantValue);\n+  }\n+};\n+\n+//===----------------------------------------------------------------------===//\n+// AxisInfoAnalysis\n+//===----------------------------------------------------------------------===//\n+\n+AxisInfoAnalysis::AxisInfoAnalysis(DataFlowSolver &solver)\n+    : dataflow::SparseDataFlowAnalysis<dataflow::Lattice<AxisInfo>>(solver) {\n+  // UnrealizedConversionCast:\n   // This is needed by TritonGPUToLLVM, to get AxisInfo when the graph is\n   // in the process of a PartialConversion, where UnrealizedConversionCast\n   // may exist\n-  if (llvm::isa<mlir::UnrealizedConversionCastOp>(op)) {\n-    curr = operands[0]->getValue();\n-  }\n+  visitors.append<CastOpAxisInfoVisitor<arith::ExtSIOp>,\n+                  CastOpAxisInfoVisitor<arith::ExtUIOp>,\n+                  CastOpAxisInfoVisitor<arith::TruncIOp>,\n+                  CastOpAxisInfoVisitor<arith::IndexCastOp>,\n+                  CastOpAxisInfoVisitor<triton::PtrToIntOp>,\n+                  CastOpAxisInfoVisitor<triton::IntToPtrOp>,\n+                  CastOpAxisInfoVisitor<triton::gpu::ConvertLayoutOp>,\n+                  CastOpAxisInfoVisitor<mlir::UnrealizedConversionCastOp>,\n+                  CastOpAxisInfoVisitor<triton::BitcastOp>>();\n+  visitors.append<MakeRangeOpAxisInfoVisitor>();\n+  visitors.append<ConstantOpAxisInfoVisitor>();\n+  visitors.append<AddSubOpAxisInfoVisitor<triton::AddPtrOp>,\n+                  AddSubOpAxisInfoVisitor<arith::AddIOp>,\n+                  AddSubOpAxisInfoVisitor<arith::SubIOp>>();\n+  visitors.append<MulIOpAxisInfoVisitor>();\n+  visitors.append<DivOpAxisInfoVisitor<arith::DivSIOp>,\n+                  DivOpAxisInfoVisitor<arith::DivUIOp>>();\n+  visitors.append<RemOpAxisInfoVisitor<arith::RemSIOp>,\n+                  RemOpAxisInfoVisitor<arith::RemUIOp>>();\n+  visitors.append<BroadcastOpAxisInfoVisitor>();\n+  visitors.append<SplatOpAxisInfoVisitor>();\n+  visitors.append<ExpandDimsOpAxisInfoVisitor>();\n+  visitors.append<CmpOpAxisInfoVisitor<arith::CmpIOp>,\n+                  CmpOpAxisInfoVisitor<triton::gpu::CmpIOp>>();\n+  visitors.append<LogicalOpAxisInfoVisitor<arith::AndIOp>,\n+                  LogicalOpAxisInfoVisitor<arith::OrIOp>,\n+                  LogicalOpAxisInfoVisitor<arith::XOrIOp>>();\n+  visitors.append<SelectOpAxisInfoVisitor<mlir::arith::SelectOp>,\n+                  SelectOpAxisInfoVisitor<triton::gpu::SelectOp>>();\n+  visitors.append<ShLIOpAxisInfoVisitor, ShROpAxisInfoVisitor<arith::ShRUIOp>,\n+                  ShROpAxisInfoVisitor<arith::ShRSIOp>>();\n+  visitors.append<MaxMinOpAxisInfoVisitor<arith::MaxSIOp>,\n+                  MaxMinOpAxisInfoVisitor<arith::MaxUIOp>,\n+                  MaxMinOpAxisInfoVisitor<arith::MinSIOp>,\n+                  MaxMinOpAxisInfoVisitor<arith::MinUIOp>>();\n+}\n+\n+void AxisInfoAnalysis::visitOperation(\n+    Operation *op, ArrayRef<const dataflow::Lattice<AxisInfo> *> operands,\n+    ArrayRef<dataflow::Lattice<AxisInfo> *> results) {\n+  AxisInfo curr = visitors.apply(op, operands);\n   if (curr.getRank() == 0) {\n-    return markAllPessimisticFixpoint(op->getResults());\n+    return setAllToEntryStates(results);\n   }\n-\n-  // join all lattice elements\n-  ChangeResult result = ChangeResult::NoChange;\n-  for (Value value : op->getResults()) {\n-    result |= getLatticeElement(value).join(curr);\n+  // override with hint\n+  auto newContiguity = curr.getContiguity();\n+  auto newDivisibility = curr.getDivisibility();\n+  auto newConstancy = curr.getConstancy();\n+  if (Attribute attr = op->getAttr(\"tt.contiguity\")) {\n+    auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+    newContiguity = AxisInfo::DimVectorT(vals.begin(), vals.end());\n+  }\n+  if (Attribute attr = op->getAttr(\"tt.divisibility\")) {\n+    auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+    newDivisibility = AxisInfo::DimVectorT(vals.begin(), vals.end());\n   }\n-  return result;\n+  if (Attribute attr = op->getAttr(\"tt.constancy\")) {\n+    auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+    newConstancy = AxisInfo::DimVectorT(vals.begin(), vals.end());\n+  }\n+  curr = mlir::AxisInfo(newContiguity, newDivisibility, newConstancy,\n+                        curr.getConstantValue());\n+  // join all lattice elements\n+  for (auto *result : results)\n+    propagateIfChanged(result, result->join(curr));\n }\n \n-unsigned AxisInfoAnalysis::getPtrVectorSize(Value ptr) {\n+unsigned AxisInfoAnalysis::getPtrContiguity(Value ptr) {\n   auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n   if (!tensorTy)\n     return 1;\n@@ -289,21 +886,27 @@ unsigned AxisInfoAnalysis::getPtrVectorSize(Value ptr) {\n   unsigned align = getPtrAlignment(ptr);\n \n   unsigned contigPerThread = triton::gpu::getSizePerThread(layout)[order[0]];\n-  unsigned vec = std::min(align, contigPerThread);\n-  vec = std::min<unsigned>(shape[order[0]], vec);\n+  contigPerThread = std::min(align, contigPerThread);\n+  contigPerThread = std::min<unsigned>(shape[order[0]], contigPerThread);\n \n-  return vec;\n+  return contigPerThread;\n }\n \n unsigned AxisInfoAnalysis::getPtrAlignment(Value ptr) {\n   auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n   if (!tensorTy)\n     return 1;\n-  auto axisInfo = lookupLatticeElement(ptr)->getValue();\n+  dataflow::Lattice<AxisInfo> *latticeElement = getLatticeElement(ptr);\n+  if (!latticeElement)\n+    return 1;\n+  auto axisInfo = latticeElement->getValue();\n   auto layout = tensorTy.getEncoding();\n   auto order = triton::gpu::getOrder(layout);\n-  unsigned maxMultiple = axisInfo.getDivisibility(order[0]);\n-  unsigned maxContig = axisInfo.getContiguity(order[0]);\n+  auto maxMultipleBytes = axisInfo.getDivisibility(order[0]);\n+  auto maxContig = axisInfo.getContiguity(order[0]);\n+  auto elemNumBits = getPointeeBitWidth(tensorTy);\n+  auto elemNumBytes = std::max<unsigned>(elemNumBits / 8, 1);\n+  auto maxMultiple = std::max<int64_t>(maxMultipleBytes / elemNumBytes, 1);\n   unsigned alignment = std::min(maxMultiple, maxContig);\n   return alignment;\n }\n@@ -312,8 +915,11 @@ unsigned AxisInfoAnalysis::getMaskAlignment(Value mask) {\n   auto tensorTy = mask.getType().dyn_cast<RankedTensorType>();\n   if (!tensorTy)\n     return 1;\n+  dataflow::Lattice<AxisInfo> *latticeElement = getLatticeElement(mask);\n+  if (!latticeElement)\n+    return 1;\n+  auto maskAxis = latticeElement->getValue();\n   auto maskOrder = triton::gpu::getOrder(tensorTy.getEncoding());\n-  auto maskAxis = lookupLatticeElement(mask)->getValue();\n   auto alignment = std::max<unsigned>(maskAxis.getConstancy(maskOrder[0]), 1);\n   return alignment;\n }"}, {"filename": "lib/Analysis/CMakeLists.txt", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -8,4 +8,7 @@ add_mlir_library(TritonAnalysis\n   DEPENDS\n   TritonTableGen\n   TritonGPUAttrDefsIncGen\n+\n+  LINK_LIBS PUBLIC\n+  MLIRAnalysis\n )"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 77, "deletions": 67, "changes": 144, "file_content_changes": "@@ -2,109 +2,119 @@\n #include \"triton/Analysis/Alias.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n-#include \"mlir/Dialect/GPU/GPUDialect.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+#include <deque>\n \n namespace mlir {\n \n void MembarAnalysis::run() {\n   auto *operation = allocation->getOperation();\n-  RegionInfo regionInfo;\n   OpBuilder builder(operation);\n-  dfsOperation(operation, &regionInfo, &builder);\n+  resolve(operation, &builder);\n }\n \n-void MembarAnalysis::dfsOperation(Operation *operation,\n-                                  RegionInfo *parentRegionInfo,\n-                                  OpBuilder *builder) {\n-  transfer(operation, parentRegionInfo, builder);\n-  if (operation->getNumRegions()) {\n-    // If there's any nested regions, we need to visit them.\n-    // scf.if and scf.else: two regions\n-    // scf.if only: two regions\n-    // scf.for: one region\n-    RegionInfo curRegionInfo;\n-    auto traverseRegions = [&]() -> auto{\n-      for (auto &region : operation->getRegions()) {\n-        // Copy the parent info as the current info.\n-        RegionInfo regionInfo = *parentRegionInfo;\n-        for (auto &block : region.getBlocks()) {\n-          assert(region.getBlocks().size() == 1 &&\n-                 \"Multiple blocks in a region is not supported\");\n-          for (auto &op : block.getOperations()) {\n-            // Traverse the nested operation.\n-            dfsOperation(&op, &regionInfo, builder);\n-          }\n-        }\n-        curRegionInfo.join(regionInfo);\n+void MembarAnalysis::resolve(Operation *operation, OpBuilder *builder) {\n+  // Initialize the blockList\n+  std::deque<Block *> blockList;\n+  operation->walk<WalkOrder::PreOrder>([&](Block *block) {\n+    for (auto &op : block->getOperations()) {\n+      // Check if the operation belongs to scf dialect, if so, we need to\n+      // throw an error\n+      if (op.getDialect()->getNamespace() == \"scf\") {\n+        op.emitError(\"scf dialect is not supported in membar. Please lower it \"\n+                     \"to cf dialect first.\");\n+        return;\n       }\n-      // Set the parent region info as the union of the nested region info.\n-      *parentRegionInfo = curRegionInfo;\n-    };\n+    }\n+    if (block->isEntryBlock())\n+      blockList.emplace_back(block);\n+  });\n+\n+  // A fixed point algorithm\n+  while (!blockList.empty()) {\n+    auto *block = blockList.front();\n+    blockList.pop_front();\n+    // Make a copy of the inputblockInfo but not update\n+    auto inputBlockInfo = inputBlockInfoMap.lookup(block);\n+    SmallVector<Block *> successors;\n+    for (auto &op : block->getOperations()) {\n+      if (op.hasTrait<OpTrait::IsTerminator>()) {\n+        visitTerminator(&op, successors);\n+      } else {\n+        update(&op, &inputBlockInfo, builder);\n+      }\n+    }\n+    // Get the reference because we want to update if it changed\n+    if (outputBlockInfoMap.count(block) &&\n+        inputBlockInfo == outputBlockInfoMap[block]) {\n+      // If we have seen the block before and the inputBlockInfo is the same as\n+      // the outputBlockInfo, we skip the successors\n+      continue;\n+    }\n+    // Update the current block\n+    outputBlockInfoMap[block].join(inputBlockInfo);\n+    // Update the successors\n+    for (auto *successor : successors) {\n+      inputBlockInfoMap[successor].join(outputBlockInfoMap[block]);\n+      blockList.emplace_back(successor);\n+    }\n+  }\n+}\n \n-    traverseRegions();\n-    if (isa<scf::ForOp>(operation)) {\n-      // scf.for can have two possible inputs: the init value and the\n-      // previous iteration's result. Although we've applied alias analysis,\n-      // there could be unsynced memory accesses on reused memories.\n-      // For example, consider the following code:\n-      // %1 = convert_layout %0: blocked -> shared\n-      // ...\n-      // gpu.barrier\n-      // ...\n-      // %5 = convert_layout %4 : shared -> dot\n-      // %6 = tt.dot %2, %5\n-      // scf.yield\n-      //\n-      // Though %5 could be released before scf.yield, it may shared the same\n-      // memory with %1. So we actually have to insert a barrier before %1 to\n-      // make sure the memory is synced.\n-      traverseRegions();\n+void MembarAnalysis::visitTerminator(Operation *op,\n+                                     SmallVector<Block *> &successors) {\n+  if (auto branchInterface = dyn_cast<BranchOpInterface>(op)) {\n+    Block *parentBlock = branchInterface->getBlock();\n+    for (Block *successor : parentBlock->getSuccessors()) {\n+      successors.push_back(successor);\n     }\n+    return;\n   }\n+  // Otherwise, it could be a return op\n+  assert(isa<func::ReturnOp>(op) && \"Unknown terminator\");\n }\n \n-void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n-                              OpBuilder *builder) {\n-  if (isa<scf::ForOp>(op) || isa<scf::IfOp>(op) || isa<scf::YieldOp>(op) ||\n-      isa<tensor::ExtractSliceOp>(op) || isa<triton::gpu::AllocTensorOp>(op)) {\n-    // Do not insert barriers before control flow operations and\n-    // alloc/extract/insert\n+void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n+                            OpBuilder *builder) {\n+  if (isa<tensor::ExtractSliceOp>(op) || isa<triton::gpu::AllocTensorOp>(op) ||\n+      isa<triton::TransOp>(op)) {\n     // alloc is an allocation op without memory write.\n     // FIXME(Keren): extract_slice is always alias for now\n     return;\n   }\n \n   if (isa<gpu::BarrierOp>(op)) {\n     // If the current op is a barrier, we sync previous reads and writes\n-    regionInfo->sync();\n+    blockInfo->sync();\n     return;\n   }\n \n   if (isa<triton::gpu::AsyncWaitOp>(op) &&\n       !isa<gpu::BarrierOp>(op->getNextNode())) {\n     // If the current op is an async wait and the next op is not a barrier we\n     // insert a barrier op and sync\n-    regionInfo->sync();\n+    blockInfo->sync();\n     OpBuilder::InsertionGuard g(*builder);\n     builder->setInsertionPointAfter(op);\n     builder->create<gpu::BarrierOp>(op->getLoc());\n-    regionInfo->sync();\n+    blockInfo->sync();\n     return;\n   }\n \n-  RegionInfo curRegionInfo;\n+  BlockInfo curBlockInfo;\n   for (Value value : op->getOperands()) {\n     for (auto bufferId : allocation->getBufferIds(value)) {\n       if (bufferId != Allocation::InvalidBufferId) {\n         if (isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n             isa<tensor::InsertSliceOp>(op)) {\n-          // FIXME(Keren): insert_slice and insert_slice_async are always alias\n-          // for now\n-          curRegionInfo.syncWriteBuffers.insert(bufferId);\n+          // FIXME(Keren): insert_slice and insert_slice_async are always\n+          // alias for now\n+          curBlockInfo.syncWriteBuffers.insert(bufferId);\n         } else {\n           // ConvertLayoutOp: shared memory -> registers\n-          curRegionInfo.syncReadBuffers.insert(bufferId);\n+          curBlockInfo.syncReadBuffers.insert(bufferId);\n         }\n       }\n     }\n@@ -113,25 +123,25 @@ void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n     // ConvertLayoutOp: registers -> shared memory\n     auto bufferId = allocation->getBufferId(value);\n     if (bufferId != Allocation::InvalidBufferId) {\n-      curRegionInfo.syncWriteBuffers.insert(bufferId);\n+      curBlockInfo.syncWriteBuffers.insert(bufferId);\n     }\n   }\n   // Scratch buffer is considered as both shared memory write & read\n   auto bufferId = allocation->getBufferId(op);\n   if (bufferId != Allocation::InvalidBufferId) {\n-    curRegionInfo.syncWriteBuffers.insert(bufferId);\n-    curRegionInfo.syncReadBuffers.insert(bufferId);\n+    curBlockInfo.syncWriteBuffers.insert(bufferId);\n+    curBlockInfo.syncReadBuffers.insert(bufferId);\n   }\n \n-  if (regionInfo->isIntersected(curRegionInfo, allocation)) {\n+  if (blockInfo->isIntersected(curBlockInfo, allocation)) {\n     OpBuilder::InsertionGuard g(*builder);\n     builder->setInsertionPoint(op);\n     builder->create<gpu::BarrierOp>(op->getLoc());\n-    regionInfo->sync();\n+    blockInfo->sync();\n   }\n   // Update the region info, even if barrier is inserted, we have to maintain\n   // the current op's read/write buffers.\n-  regionInfo->join(curRegionInfo);\n+  blockInfo->join(curBlockInfo);\n }\n \n } // namespace mlir"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 252, "deletions": 14, "changes": 266, "file_content_changes": "@@ -1,20 +1,24 @@\n #include \"triton/Analysis/Utility.h\"\n+#include \"mlir/Analysis/DataFlow/ConstantPropagationAnalysis.h\"\n+#include \"mlir/Analysis/DataFlow/DeadCodeAnalysis.h\"\n #include \"mlir/IR/Dialect.h\"\n+#include \"mlir/IR/Matchers.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include <deque>\n \n namespace mlir {\n \n bool ReduceOpHelper::isFastReduction() {\n   auto srcLayout = srcTy.getEncoding();\n-  auto axis = op.axis();\n+  auto axis = op.getAxis();\n   return axis == triton::gpu::getOrder(srcLayout)[0];\n }\n \n unsigned ReduceOpHelper::getInterWarpSize() {\n   auto srcLayout = srcTy.getEncoding();\n   auto srcShape = srcTy.getShape();\n-  auto axis = op.axis();\n+  auto axis = op.getAxis();\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n   unsigned sizeIntraWarps = getIntraWarpSize();\n   return std::min(srcReduceDimSize / sizeIntraWarps,\n@@ -24,30 +28,36 @@ unsigned ReduceOpHelper::getInterWarpSize() {\n unsigned ReduceOpHelper::getIntraWarpSize() {\n   auto srcLayout = srcTy.getEncoding();\n   auto srcShape = srcTy.getShape();\n-  auto axis = op.axis();\n+  auto axis = op.getAxis();\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n   return std::min(srcReduceDimSize,\n                   triton::gpu::getThreadsPerWarp(srcLayout)[axis]);\n }\n \n unsigned ReduceOpHelper::getThreadsReductionAxis() {\n   auto srcLayout = srcTy.getEncoding();\n-  auto axis = op.axis();\n+  auto axis = op.getAxis();\n   return triton::gpu::getThreadsPerWarp(srcLayout)[axis] *\n          triton::gpu::getWarpsPerCTA(srcLayout)[axis];\n }\n \n SmallVector<unsigned> ReduceOpHelper::getScratchConfigBasic() {\n-  auto axis = op.axis();\n+  auto axis = op.getAxis();\n   auto smemShape = convertType<unsigned>(getSrcShape());\n   smemShape[axis] = std::min(smemShape[axis], getThreadsReductionAxis());\n   return smemShape;\n }\n \n SmallVector<SmallVector<unsigned>> ReduceOpHelper::getScratchConfigsFast() {\n-  auto axis = op.axis();\n+  auto axis = op.getAxis();\n   SmallVector<SmallVector<unsigned>> smemShapes(3);\n \n+  auto argLayout = srcTy.getEncoding();\n+  auto argLayoutMma = argLayout.dyn_cast<triton::gpu::MmaEncodingAttr>();\n+  if (argLayoutMma && argLayoutMma.getVersionMajor() == 2 &&\n+      triton::gpu::getWarpsPerCTA(argLayout)[axis] == 1)\n+    return {{1, 1}, {1, 1}};\n+\n   /// shared memory block0\n   smemShapes[0] = convertType<unsigned>(getSrcShape());\n   smemShapes[0][axis] = getInterWarpSize();\n@@ -72,10 +82,10 @@ unsigned ReduceOpHelper::getScratchSizeInBytes() {\n     elems = product<unsigned>(smemShape);\n   }\n \n-  auto tensorType = op.operand().getType().cast<RankedTensorType>();\n+  auto tensorType = op.getOperand().getType().cast<RankedTensorType>();\n   unsigned bytes = elems * tensorType.getElementTypeBitWidth() / 8;\n \n-  if (triton::ReduceOp::withIndex(op.redOp()))\n+  if (triton::ReduceOp::withIndex(op.getRedOp()))\n     bytes += elems * sizeof(int32_t);\n \n   return bytes;\n@@ -99,8 +109,7 @@ bool maybeSharedAllocationOp(Operation *op) {\n          (dialect->getTypeID() ==\n               mlir::TypeID::get<triton::gpu::TritonGPUDialect>() ||\n           dialect->getTypeID() == mlir::TypeID::get<triton::TritonDialect>() ||\n-          dialect->getTypeID() ==\n-              mlir::TypeID::get<arith::ArithmeticDialect>() ||\n+          dialect->getTypeID() == mlir::TypeID::get<arith::ArithDialect>() ||\n           dialect->getTypeID() == mlir::TypeID::get<tensor::TensorDialect>());\n }\n \n@@ -114,12 +123,12 @@ bool supportMMA(triton::DotOp op, int version) {\n   // Refer to mma section for the data type supported by Volta and Hopper\n   // Tensor Core in\n   // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-fragment-mma-884-f16\n-  auto aElemTy = op.a().getType().cast<RankedTensorType>().getElementType();\n-  auto bElemTy = op.b().getType().cast<RankedTensorType>().getElementType();\n+  auto aElemTy = op.getA().getType().cast<RankedTensorType>().getElementType();\n+  auto bElemTy = op.getB().getType().cast<RankedTensorType>().getElementType();\n   if (aElemTy.isF32() && bElemTy.isF32()) {\n-    return op.allowTF32() && version >= 2;\n+    return op.getAllowTF32() && version >= 2;\n   }\n-  return supportMMA(op.a(), version) && supportMMA(op.b(), version);\n+  return supportMMA(op.getA(), version) && supportMMA(op.getB(), version);\n }\n \n bool supportMMA(Value value, int version) {\n@@ -148,4 +157,233 @@ std::string getValueOperandName(Value value, AsmState &state) {\n   return opName;\n }\n \n+bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n+                        triton::gpu::DotOperandEncodingAttr &dotOperandLayout) {\n+  // dot_op<opIdx=0, parent=#mma> = #mma\n+  // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n+  return mmaLayout.getVersionMajor() == 2 &&\n+         mmaLayout.getWarpsPerCTA()[1] == 1 &&\n+         dotOperandLayout.getOpIdx() == 0 &&\n+         dotOperandLayout.getParent() == mmaLayout;\n+}\n+\n+bool isSingleValue(Value value) {\n+  // Don't consider load as expensive if it is loading a scalar.\n+  if (auto tensorTy = value.getType().dyn_cast<RankedTensorType>())\n+    return tensorTy.getNumElements() == 1;\n+  // TODO: Handle other cases.\n+  // For example, when ptr is a tensor of single value.\n+  // It means that ptr is a resultant of broadcast or generated through\n+  // a chain of broadcast and other operations.\n+  // Rematerialize it without considering contiguous memory access pattern is\n+  // fine.\n+  return true;\n+}\n+\n+namespace {\n+\n+/// A data structure similar to SetVector but maintains\n+/// a deque instead of a vector to allow for efficient\n+/// push_back and pop_front operations.\n+/// Using SetVector doesn't suffice our needs because\n+/// it only pushes and pops from the back.\n+/// For example, if we have a queue like this:\n+/// 0->4 1->2->3\n+///    ^--------\n+/// where 3 depends on 4, once we pop 3, we found\n+/// 4 is not ready, so we check 2 and push 3 back\n+/// to the queue.\n+struct DFSSubgraphState {\n+  DFSSubgraphState() : set(), deque() {}\n+  DenseSet<Operation *> set;\n+  std::deque<Operation *> deque;\n+\n+  bool push_back(Operation *op) {\n+    if (set.insert(op).second) {\n+      deque.push_back(op);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  Operation *pop_front() {\n+    Operation *op = deque.front();\n+    deque.pop_front();\n+    set.erase(op);\n+    return op;\n+  }\n+\n+  bool empty() { return deque.empty(); }\n+};\n+\n+/// DFS post-order implementation that maintains a global count to work across\n+/// multiple invocations, to help implement topological sort on multi-root DAGs.\n+/// We traverse all operations but only record the ones that appear in\n+/// `toSort` for the final result.\n+struct DFSState {\n+  DFSState(const SetVector<Operation *> &set) : toSort(set), seen() {}\n+  const SetVector<Operation *> &toSort;\n+  SmallVector<Operation *, 16> topologicalCounts;\n+  DenseSet<Operation *> seen;\n+\n+  /// We mark each op as ready if all its operands are seen. If an op is ready,\n+  /// we add it to the queue. Otherwise, we keep adding its operands to the\n+  /// ancestors set.\n+  void addToReadyQueue(Operation *op, DFSSubgraphState &subGraph,\n+                       SmallVector<Operation *, 4> &readyQueue) {\n+    bool ready = true;\n+    for (Value operand : op->getOperands()) {\n+      auto def = operand.getDefiningOp();\n+      if (def && !seen.count(def)) {\n+        subGraph.push_back(def);\n+        ready = false;\n+      }\n+    }\n+    if (ready)\n+      readyQueue.push_back(op);\n+  }\n+};\n+\n+void dfsPostorder(Operation *root, DFSState *state) {\n+  DFSSubgraphState subGraph;\n+  subGraph.push_back(root);\n+  SmallVector<Operation *> ops;\n+  while (!subGraph.empty()) {\n+    // Nodes in the ready queue are ready to be processed.\n+    // Meaning that either their operands are all seen or they have null\n+    // operands.\n+    SmallVector<Operation *, 4> readyQueue;\n+    auto *current = subGraph.pop_front();\n+    state->addToReadyQueue(current, subGraph, readyQueue);\n+    while (!readyQueue.empty()) {\n+      Operation *current = readyQueue.pop_back_val();\n+      if (!state->seen.insert(current).second)\n+        continue;\n+      ops.push_back(current);\n+      for (Value result : current->getResults()) {\n+        for (Operation *op : result.getUsers())\n+          state->addToReadyQueue(op, subGraph, readyQueue);\n+      }\n+      for (Region &region : current->getRegions()) {\n+        for (Operation &op : region.getOps())\n+          state->addToReadyQueue(&op, subGraph, readyQueue);\n+      }\n+    }\n+  }\n+\n+  for (Operation *op : llvm::reverse(ops)) {\n+    if (state->toSort.count(op) > 0)\n+      state->topologicalCounts.push_back(op);\n+  }\n+}\n+\n+} // namespace\n+\n+SetVector<Operation *>\n+multiRootTopologicalSort(const SetVector<Operation *> &toSort) {\n+  if (toSort.empty()) {\n+    return toSort;\n+  }\n+\n+  // Run from each root with global count and `seen` set.\n+  DFSState state(toSort);\n+  for (auto *s : toSort) {\n+    assert(toSort.count(s) == 1 && \"NYI: multi-sets not supported\");\n+    dfsPostorder(s, &state);\n+  }\n+\n+  // Reorder and return.\n+  SetVector<Operation *> res;\n+  for (auto it = state.topologicalCounts.rbegin(),\n+            eit = state.topologicalCounts.rend();\n+       it != eit; ++it) {\n+    res.insert(*it);\n+  }\n+  return res;\n+}\n+\n+SetVector<Operation *> multiRootGetSlice(Operation *op,\n+                                         TransitiveFilter backwardFilter,\n+                                         TransitiveFilter forwardFilter) {\n+  SetVector<Operation *> slice;\n+  slice.insert(op);\n+\n+  unsigned currentIndex = 0;\n+  SetVector<Operation *> backwardSlice;\n+  SetVector<Operation *> forwardSlice;\n+  while (currentIndex != slice.size()) {\n+    auto *currentOp = (slice)[currentIndex];\n+    // Compute and insert the backwardSlice starting from currentOp.\n+    backwardSlice.clear();\n+    getBackwardSlice(currentOp, &backwardSlice, backwardFilter);\n+    slice.insert(backwardSlice.begin(), backwardSlice.end());\n+\n+    // Compute and insert the forwardSlice starting from currentOp.\n+    forwardSlice.clear();\n+    getForwardSlice(currentOp, &forwardSlice, forwardFilter);\n+    slice.insert(forwardSlice.begin(), forwardSlice.end());\n+    ++currentIndex;\n+  }\n+  return multiRootTopologicalSort(slice);\n+}\n+\n+namespace {\n+// Copied from TestDeadCodeAnalysis.cpp, because some dead code analysis\n+// interacts with constant propagation, but SparseConstantPropagation\n+// doesn't seem to be sufficient.\n+class ConstantAnalysis : public DataFlowAnalysis {\n+public:\n+  using DataFlowAnalysis::DataFlowAnalysis;\n+\n+  LogicalResult initialize(Operation *top) override {\n+    WalkResult result = top->walk([&](Operation *op) {\n+      if (failed(visit(op)))\n+        return WalkResult::interrupt();\n+      return WalkResult::advance();\n+    });\n+    return success(!result.wasInterrupted());\n+  }\n+\n+  LogicalResult visit(ProgramPoint point) override {\n+    Operation *op = point.get<Operation *>();\n+    Attribute value;\n+    if (matchPattern(op, m_Constant(&value))) {\n+      auto *constant = getOrCreate<dataflow::Lattice<dataflow::ConstantValue>>(\n+          op->getResult(0));\n+      propagateIfChanged(constant, constant->join(dataflow::ConstantValue(\n+                                       value, op->getDialect())));\n+      return success();\n+    }\n+    // Dead code analysis requires every operands has initialized ConstantValue\n+    // state before it is visited.\n+    // https://github.com/llvm/llvm-project/blob/2ec1aba2b69faa1de5f71832a48e25aa3b5d5314/mlir/lib/Analysis/DataFlow/DeadCodeAnalysis.cpp#L322\n+    // That's why we need to set all operands to unknown constants.\n+    setAllToUnknownConstants(op->getResults());\n+    for (Region &region : op->getRegions()) {\n+      for (Block &block : region.getBlocks())\n+        setAllToUnknownConstants(block.getArguments());\n+    }\n+    return success();\n+  }\n+\n+private:\n+  /// Set all given values as not constants.\n+  void setAllToUnknownConstants(ValueRange values) {\n+    dataflow::ConstantValue unknownConstant(nullptr, nullptr);\n+    for (Value value : values) {\n+      auto *constant =\n+          getOrCreate<dataflow::Lattice<dataflow::ConstantValue>>(value);\n+      propagateIfChanged(constant, constant->join(unknownConstant));\n+    }\n+  }\n+};\n+} // namespace\n+\n+std::unique_ptr<DataFlowSolver> createDataFlowSolver() {\n+  auto solver = std::make_unique<DataFlowSolver>();\n+  solver->load<dataflow::DeadCodeAnalysis>();\n+  solver->load<ConstantAnalysis>();\n+  return solver;\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "file_content_changes": "@@ -1,13 +1,15 @@\n add_mlir_conversion_library(TritonGPUToLLVM\n-    TritonGPUToLLVM.cpp\n-    TritonGPUToLLVMPass.cpp\n-    PTXAsmFormat.cpp\n     ConvertLayoutOpToLLVM.cpp\n+    DotOpToLLVM.cpp\n     ElementwiseOpToLLVM.cpp\n-    ViewOpToLLVM.cpp\n     LoadStoreOpToLLVM.cpp\n-    DotOpToLLVM.cpp\n+    TritonGPUToLLVM.cpp\n+    TritonGPUToLLVMPass.cpp\n+    PTXAsmFormat.cpp\n     ReduceOpToLLVM.cpp\n+    Utility.cpp\n+    ViewOpToLLVM.cpp\n+    DotOpHelpers.cpp\n \n     ADDITIONAL_HEADER_DIRS\n     ${PROJECT_SOURCE_DIR}/include/triton/Conversion/TritonGPUToLLVM"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 171, "deletions": 84, "changes": 255, "file_content_changes": "@@ -1,5 +1,6 @@\n #include \"ConvertLayoutOpToLLVM.h\"\n #include \"DotOpHelpers.h\"\n+#include \"Utility.h\"\n \n using ::mlir::LLVM::DotOpFMAConversionHelper;\n using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n@@ -17,15 +18,6 @@ using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::isaDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n-bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n-                        DotOperandEncodingAttr &dotOperandLayout) {\n-  // dot_op<opIdx=0, parent=#mma> = #mma\n-  // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n-  return mmaLayout.getWarpsPerCTA()[1] == 1 &&\n-         dotOperandLayout.getOpIdx() == 0 &&\n-         dotOperandLayout.getParent() == mmaLayout;\n-}\n-\n struct ConvertLayoutOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {\n public:\n@@ -35,8 +27,8 @@ struct ConvertLayoutOpConversion\n   LogicalResult\n   matchAndRewrite(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    Value src = op.src();\n-    Value dst = op.result();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n@@ -128,27 +120,7 @@ struct ConvertLayoutOpConversion\n         mmaColIdx[0] = add(mmaThreadIdInGrpM2, colWarpOffset);\n         mmaColIdx[1] = add(mmaThreadIdInGrpM2P1, colWarpOffset);\n       } else if (mmaLayout.isVolta()) {\n-        multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n-        multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 16));\n-        Value laneIdDiv16 = udiv(laneId, _16);\n-        Value laneIdRem16 = urem(laneId, _16);\n-        Value laneIdRem2 = urem(laneId, _2);\n-        Value laneIdRem16Div8 = udiv(laneIdRem16, _8);\n-        Value laneIdRem16Div4 = udiv(laneIdRem16, _4);\n-        Value laneIdRem16Div4Rem2 = urem(laneIdRem16Div4, _2);\n-        Value laneIdRem4Div2 = udiv(urem(laneId, _4), _2);\n-        Value rowWarpOffset = mul(multiDimWarpId[0], _16);\n-        Value colWarpOffset = mul(multiDimWarpId[1], _16);\n-        mmaRowIdx[0] =\n-            add(add(mul(laneIdDiv16, _8), mul(laneIdRem16Div4Rem2, _4)),\n-                laneIdRem2);\n-        mmaRowIdx[0] = add(mmaRowIdx[0], rowWarpOffset);\n-        mmaRowIdx[1] = add(mmaRowIdx[0], _2);\n-        mmaColIdx[0] = add(mul(laneIdRem16Div8, _4), mul(laneIdRem4Div2, _2));\n-        mmaColIdx[0] = add(mmaColIdx[0], colWarpOffset);\n-        mmaColIdx[1] = add(mmaColIdx[0], _1);\n-        mmaColIdx[2] = add(mmaColIdx[0], _8);\n-        mmaColIdx[3] = add(mmaColIdx[0], idx_val(9));\n+        // Volta doesn't follow the pattern here.\"\n       } else {\n         llvm_unreachable(\"Unexpected MMALayout version\");\n       }\n@@ -163,26 +135,12 @@ struct ConvertLayoutOpConversion\n         multiDimOffset[1] = add(\n             multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n       } else if (mmaLayout.isVolta()) {\n-        // the order of elements in a thread:\n-        //   c0, c1, ...  c4, c5\n-        //   c2, c3, ...  c6, c7\n-        if (elemId < 2) {\n-          multiDimOffset[0] = mmaRowIdx[0];\n-          multiDimOffset[1] = mmaColIdx[elemId % 2];\n-        } else if (elemId >= 2 && elemId < 4) {\n-          multiDimOffset[0] = mmaRowIdx[1];\n-          multiDimOffset[1] = mmaColIdx[elemId % 2];\n-        } else if (elemId >= 4 && elemId < 6) {\n-          multiDimOffset[0] = mmaRowIdx[0];\n-          multiDimOffset[1] = mmaColIdx[elemId % 2 + 2];\n-        } else if (elemId >= 6) {\n-          multiDimOffset[0] = mmaRowIdx[1];\n-          multiDimOffset[1] = mmaColIdx[elemId % 2 + 2];\n-        }\n-        multiDimOffset[0] = add(\n-            multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n-        multiDimOffset[1] = add(\n-            multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n+        auto [isARow, isBRow, isAVec4, isBVec4, mmaId] =\n+            mmaLayout.decodeVoltaLayoutStates();\n+        auto coords = DotOpMmaV1ConversionHelper::getMNCoords(\n+            threadId, rewriter, mmaLayout.getWarpsPerCTA(), shape, isARow,\n+            isBRow, isAVec4, isBVec4);\n+        return DotOpMmaV1ConversionHelper::getCoord(elemId, coords);\n       } else {\n         llvm_unreachable(\"Unexpected MMALayout version\");\n       }\n@@ -201,14 +159,11 @@ struct ConvertLayoutOpConversion\n                       Value smemBase) const {\n     auto accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n     auto layout = type.getEncoding();\n-    auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>();\n-    auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>();\n-    auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>();\n     auto rank = type.getRank();\n     auto sizePerThread = getSizePerThread(layout);\n     auto accumSizePerThread = product<unsigned>(sizePerThread);\n     SmallVector<unsigned> numCTAs(rank);\n-    auto shapePerCTA = getShapePerCTA(layout);\n+    auto shapePerCTA = getShapePerCTA(layout, type.getShape());\n     auto order = getOrder(layout);\n     for (unsigned d = 0; d < rank; ++d) {\n       numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n@@ -277,15 +232,118 @@ struct ConvertLayoutOpConversion\n     }\n   }\n \n+  // The MMAV1's result is quite different from the exising \"Replica\" structure,\n+  // add a new simple but clear implementation for it to avoid modificating the\n+  // logic of the exising one.\n+  void processReplicaForMMAV1(Location loc, ConversionPatternRewriter &rewriter,\n+                              bool stNotRd, RankedTensorType type,\n+                              ArrayRef<unsigned> multiDimRepId, unsigned vec,\n+                              ArrayRef<unsigned> paddedRepShape,\n+                              ArrayRef<unsigned> outOrd,\n+                              SmallVector<Value> &vals, Value smemBase,\n+                              ArrayRef<int64_t> shape,\n+                              bool isDestMma = false) const {\n+    unsigned accumNumCTAsEachRep = 1;\n+    auto layout = type.getEncoding();\n+    MmaEncodingAttr mma = layout.dyn_cast<MmaEncodingAttr>();\n+    auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>();\n+    if (sliceLayout)\n+      mma = sliceLayout.getParent().cast<MmaEncodingAttr>();\n+\n+    auto order = getOrder(layout);\n+    auto rank = type.getRank();\n+    int accumSizePerThread = vals.size();\n+\n+    SmallVector<unsigned> numCTAs(rank, 1);\n+    SmallVector<unsigned> numCTAsEachRep(rank, 1);\n+    SmallVector<unsigned> shapePerCTA = getShapePerCTA(layout, shape);\n+    auto elemTy = type.getElementType();\n+\n+    int ctaId = 0;\n+\n+    auto multiDimCTAInRepId =\n+        getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep, order);\n+    SmallVector<unsigned> multiDimCTAId(rank);\n+    for (const auto &it : llvm::enumerate(multiDimCTAInRepId)) {\n+      auto d = it.index();\n+      multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n+    }\n+\n+    std::vector<std::pair<SmallVector<Value>, Value>> coord2valT(\n+        accumSizePerThread);\n+    bool needTrans = outOrd[0] != 0;\n+    if (sliceLayout || isDestMma)\n+      needTrans = false;\n+\n+    vec = needTrans ? 2 : 1;\n+    {\n+      // We need to transpose the coordinates and values here to enable vec=2\n+      // when store to smem.\n+      std::vector<std::pair<SmallVector<Value>, Value>> coord2val(\n+          accumSizePerThread);\n+      for (unsigned elemId = 0; elemId < accumSizePerThread; ++elemId) {\n+        // TODO[Superjomn]: Move the coordinate computation out of loop, it is\n+        // duplicate in Volta.\n+        SmallVector<Value> multiDimOffset =\n+            getMultiDimOffset(layout, loc, rewriter, elemId, type.getShape(),\n+                              multiDimCTAInRepId, shapePerCTA);\n+        coord2val[elemId] = std::make_pair(multiDimOffset, vals[elemId]);\n+      }\n+\n+      if (needTrans) {\n+        auto [isARow, isBRow, isAVec4, isBVec4, mmaId] =\n+            mma.decodeVoltaLayoutStates();\n+        DotOpMmaV1ConversionHelper helper(mma);\n+        // do transpose\n+        int numM = helper.getElemsM(mma.getWarpsPerCTA()[0], shape[0], isARow,\n+                                    isAVec4);\n+        int numN = accumSizePerThread / numM;\n+\n+        for (int r = 0; r < numM; r++) {\n+          for (int c = 0; c < numN; c++) {\n+            coord2valT[r * numN + c] = std::move(coord2val[c * numM + r]);\n+          }\n+        }\n+      } else {\n+        coord2valT = std::move(coord2val);\n+      }\n+    }\n+\n+    // Now the coord2valT has the transposed and contiguous elements(with\n+    // vec=2), the original vals is not needed.\n+    for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n+      auto coord = coord2valT[elemId].first;\n+      Value offset = linearize(rewriter, loc, coord, paddedRepShape, outOrd);\n+      auto elemPtrTy = ptr_ty(elemTy, 3);\n+      Value ptr = gep(elemPtrTy, smemBase, offset);\n+      auto vecTy = vec_ty(elemTy, vec);\n+      ptr = bitcast(ptr, ptr_ty(vecTy, 3));\n+      if (stNotRd) {\n+        Value valVec = undef(vecTy);\n+        for (unsigned v = 0; v < vec; ++v) {\n+          auto currVal = coord2valT[elemId + v].second;\n+          valVec = insert_element(vecTy, valVec, currVal, idx_val(v));\n+        }\n+        store(valVec, ptr);\n+      } else {\n+        Value valVec = load(ptr);\n+        for (unsigned v = 0; v < vec; ++v) {\n+          Value currVal = extract_element(elemTy, valVec, idx_val(v));\n+          vals[elemId + v] = currVal;\n+        }\n+      }\n+    }\n+  }\n+\n   // blocked/mma -> blocked/mma.\n   // Data padding in shared memory to avoid bank conflict.\n   LogicalResult\n   lowerDistributedToDistributed(triton::gpu::ConvertLayoutOp op,\n                                 OpAdaptor adaptor,\n                                 ConversionPatternRewriter &rewriter) const {\n     auto loc = op.getLoc();\n-    Value src = op.src();\n-    Value dst = op.result();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n@@ -301,8 +359,26 @@ struct ConvertLayoutOpConversion\n     SmallVector<unsigned> outNumCTAsEachRep(rank);\n     SmallVector<unsigned> inNumCTAs(rank);\n     SmallVector<unsigned> outNumCTAs(rank);\n-    auto srcShapePerCTA = getShapePerCTA(srcLayout);\n-    auto dstShapePerCTA = getShapePerCTA(dstLayout);\n+    auto srcShapePerCTA = getShapePerCTA(srcLayout, srcTy.getShape());\n+    auto dstShapePerCTA = getShapePerCTA(dstLayout, shape);\n+\n+    // For Volta, all the coords for a CTA are calculated.\n+    bool isSrcMmaV1{}, isDstMmaV1{};\n+    if (auto mmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>()) {\n+      isSrcMmaV1 = mmaLayout.isVolta();\n+    }\n+    if (auto sliceLayout = srcLayout.dyn_cast<SliceEncodingAttr>()) {\n+      isSrcMmaV1 = sliceLayout.getParent().isa<MmaEncodingAttr>() &&\n+                   sliceLayout.getParent().cast<MmaEncodingAttr>().isVolta();\n+    }\n+    if (auto mmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>()) {\n+      isDstMmaV1 = mmaLayout.isVolta();\n+    }\n+    if (auto sliceLayout = dstLayout.dyn_cast<SliceEncodingAttr>()) {\n+      isDstMmaV1 = sliceLayout.getParent().isa<MmaEncodingAttr>() &&\n+                   sliceLayout.getParent().cast<MmaEncodingAttr>().isVolta();\n+    }\n+\n     for (unsigned d = 0; d < rank; ++d) {\n       unsigned inPerCTA = std::min<unsigned>(shape[d], srcShapePerCTA[d]);\n       unsigned outPerCTA = std::min<unsigned>(shape[d], dstShapePerCTA[d]);\n@@ -317,7 +393,7 @@ struct ConvertLayoutOpConversion\n     // Potentially we need to store for multiple CTAs in this replication\n     auto accumNumReplicates = product<unsigned>(numReplicates);\n     // unsigned elems = getElemsPerThread(srcTy);\n-    auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n+    auto vals = getElementsFromStruct(loc, adaptor.getSrc(), rewriter);\n     unsigned inVec = 0;\n     unsigned outVec = 0;\n     auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n@@ -334,20 +410,31 @@ struct ConvertLayoutOpConversion\n       if (srcLayout.isa<BlockedEncodingAttr>() ||\n           srcLayout.isa<SliceEncodingAttr>() ||\n           srcLayout.isa<MmaEncodingAttr>()) {\n-        processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,\n-                       multiDimRepId, inVec, paddedRepShape, outOrd, vals,\n-                       smemBase);\n+        if (isSrcMmaV1)\n+          processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ true, srcTy,\n+                                 multiDimRepId, inVec, paddedRepShape, outOrd,\n+                                 vals, smemBase, shape);\n+        else\n+          processReplica(loc, rewriter, /*stNotRd*/ true, srcTy,\n+                         inNumCTAsEachRep, multiDimRepId, inVec, paddedRepShape,\n+                         outOrd, vals, smemBase);\n       } else {\n         assert(0 && \"ConvertLayout with input layout not implemented\");\n         return failure();\n       }\n+\n       barrier();\n       if (dstLayout.isa<BlockedEncodingAttr>() ||\n           dstLayout.isa<SliceEncodingAttr>() ||\n           dstLayout.isa<MmaEncodingAttr>()) {\n-        processReplica(loc, rewriter, /*stNotRd*/ false, dstTy,\n-                       outNumCTAsEachRep, multiDimRepId, outVec, paddedRepShape,\n-                       outOrd, outVals, smemBase);\n+        if (isDstMmaV1)\n+          processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ false, dstTy,\n+                                 multiDimRepId, outVec, paddedRepShape, outOrd,\n+                                 outVals, smemBase, shape, /*isDestMma=*/true);\n+        else\n+          processReplica(loc, rewriter, /*stNotRd*/ false, dstTy,\n+                         outNumCTAsEachRep, multiDimRepId, outVec,\n+                         paddedRepShape, outOrd, outVals, smemBase);\n       } else {\n         assert(0 && \"ConvertLayout with output layout not implemented\");\n         return failure();\n@@ -370,8 +457,8 @@ struct ConvertLayoutOpConversion\n   lowerDistributedToShared(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n                            ConversionPatternRewriter &rewriter) const {\n     auto loc = op.getLoc();\n-    Value src = op.src();\n-    Value dst = op.result();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto srcShape = srcTy.getShape();\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n@@ -390,7 +477,7 @@ struct ConvertLayoutOpConversion\n     auto dstStrides =\n         getStridesFromShapeAndOrder(dstShape, outOrd, loc, rewriter);\n     auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n-    storeDistributedToShared(src, adaptor.src(), dstStrides, srcIndices, dst,\n+    storeDistributedToShared(src, adaptor.getSrc(), dstStrides, srcIndices, dst,\n                              smemBase, elemTy, loc, rewriter);\n     auto smemObj =\n         SharedMemoryObject(smemBase, dstShape, outOrd, loc, rewriter);\n@@ -404,8 +491,8 @@ struct ConvertLayoutOpConversion\n   lowerSharedToDotOperand(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n                           ConversionPatternRewriter &rewriter) const {\n     auto loc = op.getLoc();\n-    Value src = op.src();\n-    Value dst = op.result();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n     auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n     auto srcTensorTy = src.getType().cast<RankedTensorType>();\n     auto dotOperandLayout =\n@@ -433,10 +520,10 @@ struct ConvertLayoutOpConversion\n       DotOpFMAConversionHelper helper(blockedLayout);\n       auto thread = getThreadId(rewriter, loc);\n       if (dotOpLayout.getOpIdx() == 0) { // $a\n-        res = helper.loadA(src, adaptor.src(), blockedLayout, thread, loc,\n+        res = helper.loadA(src, adaptor.getSrc(), blockedLayout, thread, loc,\n                            rewriter);\n       } else { // $b\n-        res = helper.loadB(src, adaptor.src(), blockedLayout, thread, loc,\n+        res = helper.loadB(src, adaptor.getSrc(), blockedLayout, thread, loc,\n                            rewriter);\n       }\n     } else {\n@@ -452,15 +539,15 @@ struct ConvertLayoutOpConversion\n   lowerMmaToDotOperand(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n                        ConversionPatternRewriter &rewriter) const {\n     auto loc = op.getLoc();\n-    auto srcTy = op.src().getType().cast<RankedTensorType>();\n-    auto dstTy = op.result().getType().cast<RankedTensorType>();\n+    auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n+    auto dstTy = op.getResult().getType().cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding();\n     auto dstLayout = dstTy.getEncoding();\n     auto srcMmaLayout = srcLayout.cast<MmaEncodingAttr>();\n     auto dstDotLayout = dstLayout.cast<DotOperandEncodingAttr>();\n     if (isMmaToDotShortcut(srcMmaLayout, dstDotLayout)) {\n       // get source values\n-      auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n+      auto vals = getElementsFromStruct(loc, adaptor.getSrc(), rewriter);\n       unsigned elems = getElemsPerThread(srcTy);\n       Type elemTy =\n           this->getTypeConverter()->convertType(srcTy.getElementType());\n@@ -509,12 +596,12 @@ struct ConvertLayoutOpConversion\n       ConversionPatternRewriter &rewriter, const MmaEncodingAttr &mmaLayout,\n       const DotOperandEncodingAttr &dotOperandLayout, bool isOuter) const {\n     auto loc = op.getLoc();\n-    Value src = op.src();\n-    Value dst = op.result();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n     bool isHMMA = supportMMA(dst, mmaLayout.getVersionMajor());\n \n     auto smemObj =\n-        getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n+        getSharedMemoryObjectFromStruct(loc, adaptor.getSrc(), rewriter);\n     Value res;\n \n     if (!isOuter && mmaLayout.isAmpere() && isHMMA) { // tensor core v2\n@@ -548,13 +635,13 @@ struct ConvertLayoutOpConversion\n       if (dotOperandLayout.getOpIdx() == 0) { // operand $a\n         // TODO[Superjomn]: transA is not available here.\n         bool transA = false;\n-        res = helper.loadA(src, transA, smemObj, getThreadId(rewriter, loc),\n-                           loc, rewriter);\n+        res = helper.loadA(src, smemObj, getThreadId(rewriter, loc), loc,\n+                           rewriter);\n       } else if (dotOperandLayout.getOpIdx() == 1) { // operand $b\n         // TODO[Superjomn]: transB is not available here.\n         bool transB = false;\n-        res = helper.loadB(src, transB, smemObj, getThreadId(rewriter, loc),\n-                           loc, rewriter);\n+        res = helper.loadB(src, smemObj, getThreadId(rewriter, loc), loc,\n+                           rewriter);\n       }\n     } else {\n       assert(false && \"Unsupported mma layout found\");"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.h", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -8,9 +8,6 @@ using namespace mlir::triton;\n \n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n \n-bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n-                        DotOperandEncodingAttr &dotOperandLayout);\n-\n void populateConvertLayoutOpToLLVMPatterns(\n     mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.cpp", "status": "added", "additions": 1422, "deletions": 0, "changes": 1422, "file_content_changes": "@@ -0,0 +1,1422 @@\n+#include \"DotOpHelpers.h\"\n+\n+namespace mlir {\n+namespace LLVM {\n+\n+int DotOpMmaV1ConversionHelper::numElemsPerThreadA(ArrayRef<int64_t> shape,\n+                                                   bool isARow, bool isAVec4,\n+                                                   int vec) const {\n+  int numM = getNumM(shape[0], isARow, isAVec4);\n+  int NK = shape[1];\n+  // Here we mimic the logic in loadA, the result cannot be calculated\n+  // directly.\n+  llvm::DenseSet<std::pair<int, int>> visited;\n+  auto ld = [&](int m, int k) {\n+    visited.insert({m, k});\n+    if (vec > 4) {\n+      if (isARow)\n+        visited.insert({m, k + 4});\n+      else\n+        visited.insert({m + 1, k});\n+    }\n+  };\n+\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      if (!visited.count({m, k}))\n+        ld(m, k);\n+\n+  return visited.size() * 2;\n+}\n+\n+int DotOpMmaV1ConversionHelper::numElemsPerThreadB(ArrayRef<int64_t> shape,\n+                                                   bool isBRow, bool isBVec4,\n+                                                   int vec) const {\n+  unsigned numN = getNumN(shape[1], isBRow, isBVec4);\n+  int NK = shape[0];\n+  // Here we mimic the logic in loadA, the result cannot be calculated\n+  // directly.\n+  llvm::DenseSet<std::pair<int, int>> visited;\n+  int elemsPerLd = vec > 4 ? 4 : 2;\n+  auto ld = [&](int n, int k) {\n+    visited.insert({n, k});\n+    if (vec > 4) {\n+      if (isBRow)\n+        visited.insert({n + 1, k});\n+      else\n+        visited.insert({n, k + 4});\n+    }\n+  };\n+\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned n = 0; n < numN / 2; ++n) {\n+      if (!visited.count({n, k}))\n+        ld(n, k);\n+    }\n+\n+  return visited.size() * 2;\n+}\n+\n+Value DotOpMmaV1ConversionHelper::loadA(\n+    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto shape = tensorTy.getShape();\n+  auto order = sharedLayout.getOrder();\n+\n+  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+  Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+\n+  bool isARow = order[0] != 0;\n+  auto [isARow_, _0, isAVec4, _1, _2] = mmaLayout.decodeVoltaLayoutStates();\n+\n+  AParam param(isARow_, isAVec4);\n+\n+  auto [offsetAM, offsetAK, _3, _4] = computeOffsets(\n+      thread, isARow, false, fpw, param.spw, param.rep, rewriter, loc);\n+\n+  int vecA = sharedLayout.getVec();\n+\n+  auto strides = smemObj.strides;\n+  Value strideAM = isARow ? strides[0] : i32_val(1);\n+  Value strideAK = isARow ? i32_val(1) : strides[1];\n+  Value strideA0 = isARow ? strideAK : strideAM;\n+  Value strideA1 = isARow ? strideAM : strideAK;\n+\n+  int strideRepM = wpt[0] * fpw[0] * 8;\n+  int strideRepK = 1;\n+\n+  // swizzling\n+  int perPhaseA = sharedLayout.getPerPhase();\n+  int maxPhaseA = sharedLayout.getMaxPhase();\n+  int stepA0 = isARow ? strideRepK : strideRepM;\n+  int numPtrA = std::max(2 * perPhaseA * maxPhaseA / stepA0, 1);\n+  int NK = shape[1];\n+\n+  // pre-compute pointer lanes\n+  Value offA0 = isARow ? offsetAK : offsetAM;\n+  Value offA1 = isARow ? offsetAM : offsetAK;\n+  Value phaseA = urem(udiv(offA1, i32_val(perPhaseA)), i32_val(maxPhaseA));\n+  offA0 = add(offA0, cSwizzleOffset);\n+  SmallVector<Value> offA(numPtrA);\n+  for (int i = 0; i < numPtrA; i++) {\n+    Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));\n+    offA0I = udiv(offA0I, i32_val(vecA));\n+    offA0I = xor_(offA0I, phaseA);\n+    offA0I = mul(offA0I, i32_val(vecA));\n+    offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n+  }\n+\n+  Type elemX2Ty = vec_ty(f16_ty, 2);\n+  Type elemPtrTy = ptr_ty(f16_ty, 3);\n+  if (tensorTy.getElementType().isBF16()) {\n+    elemX2Ty = vec_ty(i16_ty, 2);\n+    elemPtrTy = ptr_ty(i16_ty, 3);\n+  }\n+\n+  // prepare arguments\n+  SmallVector<Value> ptrA(numPtrA);\n+\n+  std::map<std::pair<int, int>, std::pair<Value, Value>> has;\n+  for (int i = 0; i < numPtrA; i++)\n+    ptrA[i] = gep(ptr_ty(f16_ty, 3), smemBase, offA[i]);\n+\n+  auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+  auto loadA = [&](int m, int k) {\n+    int offidx = (isARow ? k / 4 : m) % numPtrA;\n+    Value thePtrA = gep(elemPtrTy, smemBase, offA[offidx]);\n+\n+    int stepAM = isARow ? m : m / numPtrA * numPtrA;\n+    int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n+    Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),\n+                       mul(i32_val(stepAK), strideAK));\n+    Value pa = gep(elemPtrTy, thePtrA, offset);\n+    Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n+    Value ha = load(bitcast(pa, aPtrTy));\n+    // record lds that needs to be moved\n+    Value ha00 = bitcast(extract_element(ha, i32_val(0)), elemX2Ty);\n+    Value ha01 = bitcast(extract_element(ha, i32_val(1)), elemX2Ty);\n+    ld(has, m, k, ha00, ha01);\n+\n+    if (vecA > 4) {\n+      Value ha10 = bitcast(extract_element(ha, i32_val(2)), elemX2Ty);\n+      Value ha11 = bitcast(extract_element(ha, i32_val(3)), elemX2Ty);\n+      if (isARow)\n+        ld(has, m, k + 4, ha10, ha11);\n+      else\n+        ld(has, m + 1, k, ha10, ha11);\n+    }\n+  };\n+\n+  unsigned numM = getNumM(shape[0], isARow, isAVec4);\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      if (!has.count({m, k}))\n+        loadA(m, k);\n+\n+  SmallVector<Value> elems;\n+  elems.reserve(has.size() * 2);\n+  for (auto item : has) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+\n+  Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n+  Value res = getStructFromElements(loc, elems, rewriter, resTy);\n+  return res;\n+}\n+\n+Value DotOpMmaV1ConversionHelper::loadB(\n+    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+  // smem\n+  auto strides = smemObj.strides;\n+\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+\n+  auto shape = tensorTy.getShape();\n+  auto order = sharedLayout.getOrder();\n+\n+  Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+  bool isBRow = order[0] != 0; // is row-major in shared memory layout\n+  // isBRow_ indicates whether B is row-major in DotOperand layout\n+  auto [_0, isBRow_, _1, isBVec4, _2] = mmaLayout.decodeVoltaLayoutStates();\n+  assert(isBRow == isBRow_ && \"B need smem isRow\");\n+\n+  BParam param(isBRow_, isBVec4);\n+\n+  int vecB = sharedLayout.getVec();\n+  Value strideBN = isBRow ? i32_val(1) : strides[1];\n+  Value strideBK = isBRow ? strides[0] : i32_val(1);\n+  Value strideB0 = isBRow ? strideBN : strideBK;\n+  Value strideB1 = isBRow ? strideBK : strideBN;\n+  int strideRepN = wpt[1] * fpw[1] * 8;\n+  int strideRepK = 1;\n+\n+  auto [_3, _4, offsetBN, offsetBK] = computeOffsets(\n+      thread, false, isBRow, fpw, param.spw, param.rep, rewriter, loc);\n+\n+  // swizzling\n+  int perPhaseB = sharedLayout.getPerPhase();\n+  int maxPhaseB = sharedLayout.getMaxPhase();\n+  int stepB0 = isBRow ? strideRepN : strideRepK;\n+  int numPtrB = std::max(2 * perPhaseB * maxPhaseB / stepB0, 1);\n+  int NK = shape[0];\n+\n+  Value offB0 = isBRow ? offsetBN : offsetBK;\n+  Value offB1 = isBRow ? offsetBK : offsetBN;\n+  Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n+  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+\n+  offB0 = add(offB0, cSwizzleOffset);\n+  SmallVector<Value> offB(numPtrB);\n+  for (int i = 0; i < numPtrB; ++i) {\n+    Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));\n+    offB0I = udiv(offB0I, i32_val(vecB));\n+    offB0I = xor_(offB0I, phaseB);\n+    offB0I = mul(offB0I, i32_val(vecB));\n+    offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));\n+  }\n+\n+  Type elemPtrTy = ptr_ty(f16_ty, 3);\n+  Type elemX2Ty = vec_ty(f16_ty, 2);\n+  if (tensorTy.getElementType().isBF16()) {\n+    elemPtrTy = ptr_ty(i16_ty, 3);\n+    elemX2Ty = vec_ty(i16_ty, 2);\n+  }\n+\n+  SmallVector<Value> ptrB(numPtrB);\n+  ValueTable hbs;\n+  for (int i = 0; i < numPtrB; ++i)\n+    ptrB[i] = gep(ptr_ty(f16_ty, 3), smem, offB[i]);\n+\n+  auto ld = [&](decltype(hbs) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+\n+  auto loadB = [&](int n, int K) {\n+    int offidx = (isBRow ? n : K / 4) % numPtrB;\n+    Value thePtrB = ptrB[offidx];\n+\n+    int stepBN = isBRow ? n / numPtrB * numPtrB : n;\n+    int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n+    Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n+                       mul(i32_val(stepBK), strideBK));\n+    Value pb = gep(elemPtrTy, thePtrB, offset);\n+\n+    Value hb =\n+        load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n+    // record lds that needs to be moved\n+    Value hb00 = bitcast(extract_element(hb, i32_val(0)), elemX2Ty);\n+    Value hb01 = bitcast(extract_element(hb, i32_val(1)), elemX2Ty);\n+    ld(hbs, n, K, hb00, hb01);\n+    if (vecB > 4) {\n+      Value hb10 = bitcast(extract_element(hb, i32_val(2)), elemX2Ty);\n+      Value hb11 = bitcast(extract_element(hb, i32_val(3)), elemX2Ty);\n+      if (isBRow)\n+        ld(hbs, n + 1, K, hb10, hb11);\n+      else\n+        ld(hbs, n, K + 4, hb10, hb11);\n+    }\n+  };\n+\n+  unsigned numN = getNumN(shape[1], isBRow, isBVec4);\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned n = 0; n < numN / 2; ++n) {\n+      if (!hbs.count({n, k}))\n+        loadB(n, k);\n+    }\n+\n+  SmallVector<Value> elems;\n+  for (auto &item : hbs) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+\n+  Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n+  Value res = getStructFromElements(loc, elems, rewriter, resTy);\n+  return res;\n+}\n+\n+std::tuple<Value, Value, Value, Value>\n+DotOpMmaV1ConversionHelper::computeOffsets(Value threadId, bool isARow,\n+                                           bool isBRow, ArrayRef<int> fpw,\n+                                           ArrayRef<int> spw, ArrayRef<int> rep,\n+                                           ConversionPatternRewriter &rewriter,\n+                                           Location loc) const {\n+  auto *ctx = rewriter.getContext();\n+  Value _1 = i32_val(1);\n+  Value _3 = i32_val(3);\n+  Value _4 = i32_val(4);\n+  Value _16 = i32_val(16);\n+  Value _32 = i32_val(32);\n+\n+  Value lane = urem(threadId, _32);\n+  Value warp = udiv(threadId, _32);\n+\n+  // warp offset\n+  Value warp0 = urem(warp, i32_val(wpt[0]));\n+  Value warp12 = udiv(warp, i32_val(wpt[0]));\n+  Value warp1 = urem(warp12, i32_val(wpt[1]));\n+  Value warpMOff = mul(warp0, i32_val(spw[0]));\n+  Value warpNOff = mul(warp1, i32_val(spw[1]));\n+  // Quad offset\n+  Value quadMOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[0]));\n+  Value quadNOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[1]));\n+  // Pair offset\n+  Value pairMOff = udiv(urem(lane, _16), _4);\n+  pairMOff = urem(pairMOff, i32_val(fpw[0]));\n+  pairMOff = mul(pairMOff, _4);\n+  Value pairNOff = udiv(urem(lane, _16), _4);\n+  pairNOff = udiv(pairNOff, i32_val(fpw[0]));\n+  pairNOff = urem(pairNOff, i32_val(fpw[1]));\n+  pairNOff = mul(pairNOff, _4);\n+  // scale\n+  pairMOff = mul(pairMOff, i32_val(rep[0] / 2));\n+  quadMOff = mul(quadMOff, i32_val(rep[0] / 2));\n+  pairNOff = mul(pairNOff, i32_val(rep[1] / 2));\n+  quadNOff = mul(quadNOff, i32_val(rep[1] / 2));\n+  // Quad pair offset\n+  Value laneMOff = add(pairMOff, quadMOff);\n+  Value laneNOff = add(pairNOff, quadNOff);\n+  // A offset\n+  Value offsetAM = add(warpMOff, laneMOff);\n+  Value offsetAK = and_(lane, _3);\n+  // B offset\n+  Value offsetBN = add(warpNOff, laneNOff);\n+  Value offsetBK = and_(lane, _3);\n+  // i indices\n+  Value offsetCM = add(and_(lane, _1), offsetAM);\n+  if (isARow) {\n+    offsetAM = add(offsetAM, urem(threadId, _4));\n+    offsetAK = i32_val(0);\n+  }\n+  if (!isBRow) {\n+    offsetBN = add(offsetBN, urem(threadId, _4));\n+    offsetBK = i32_val(0);\n+  }\n+\n+  return std::make_tuple(offsetAM, offsetAK, offsetBN, offsetBK);\n+}\n+\n+DotOpMmaV1ConversionHelper::ValueTable\n+DotOpMmaV1ConversionHelper::extractLoadedOperand(\n+    Value llStruct, int NK, ConversionPatternRewriter &rewriter) const {\n+  ValueTable rcds;\n+  SmallVector<Value> elems =\n+      getElementsFromStruct(llStruct.getLoc(), llStruct, rewriter);\n+\n+  int offset = 0;\n+  for (int i = 0; offset < elems.size(); ++i) {\n+    for (int k = 0; k < NK; k += 4) {\n+      rcds[{i, k}] = std::make_pair(elems[offset], elems[offset + 1]);\n+      offset += 2;\n+    }\n+  }\n+\n+  return rcds;\n+}\n+\n+SmallVector<DotOpMmaV1ConversionHelper::CoordTy>\n+DotOpMmaV1ConversionHelper::getMNCoords(Value thread,\n+                                        ConversionPatternRewriter &rewriter,\n+                                        ArrayRef<unsigned int> wpt,\n+                                        ArrayRef<int64_t> shape, bool isARow,\n+                                        bool isBRow, bool isAVec4,\n+                                        bool isBVec4) {\n+\n+  auto *ctx = thread.getContext();\n+  auto loc = UnknownLoc::get(ctx);\n+  Value _1 = i32_val(1);\n+  Value _2 = i32_val(2);\n+  Value _4 = i32_val(4);\n+  Value _16 = i32_val(16);\n+  Value _32 = i32_val(32);\n+  Value _fpw0 = i32_val(fpw[0]);\n+  Value _fpw1 = i32_val(fpw[1]);\n+\n+  DotOpMmaV1ConversionHelper::AParam aParam(isARow, isAVec4);\n+  DotOpMmaV1ConversionHelper::BParam bParam(isBRow, isBVec4);\n+\n+  SmallVector<int, 2> rep({aParam.rep[0], bParam.rep[1]});\n+  SmallVector<int, 2> spw({aParam.spw[0], bParam.spw[1]});\n+  SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n+\n+  Value lane = urem(thread, _32);\n+  Value warp = udiv(thread, _32);\n+\n+  Value warp0 = urem(warp, i32_val(wpt[0]));\n+  Value warp12 = udiv(warp, i32_val(wpt[0]));\n+  Value warp1 = urem(warp12, i32_val(wpt[1]));\n+\n+  // warp offset\n+  Value offWarpM = mul(warp0, i32_val(spw[0]));\n+  Value offWarpN = mul(warp1, i32_val(spw[1]));\n+  // quad offset\n+  Value offQuadM = mul(udiv(and_(lane, _16), _4), _fpw0);\n+  Value offQuadN = mul(udiv(and_(lane, _16), _4), _fpw1);\n+  // pair offset\n+  Value offPairM = udiv(urem(lane, _16), _4);\n+  offPairM = urem(offPairM, _fpw0);\n+  offPairM = mul(offPairM, _4);\n+  Value offPairN = udiv(urem(lane, _16), _4);\n+  offPairN = udiv(offPairN, _fpw0);\n+  offPairN = urem(offPairN, _fpw1);\n+  offPairN = mul(offPairN, _4);\n+\n+  // sclare\n+  offPairM = mul(offPairM, i32_val(rep[0] / 2));\n+  offQuadM = mul(offQuadM, i32_val(rep[0] / 2));\n+  offPairN = mul(offPairN, i32_val(rep[1] / 2));\n+  offQuadN = mul(offQuadN, i32_val(rep[1] / 2));\n+\n+  // quad pair offset\n+  Value offLaneM = add(offPairM, offQuadM);\n+  Value offLaneN = add(offPairN, offQuadN);\n+  // a, b offset\n+  Value offsetAM = add(offWarpM, offLaneM);\n+  Value offsetBN = add(offWarpN, offLaneN);\n+  // m indices\n+  Value offsetCM = add(and_(lane, _1), offsetAM);\n+  SmallVector<Value> idxM;\n+  for (unsigned m = 0; m < shape[0]; m += shapePerCTA[0])\n+    for (unsigned mm = 0; mm < rep[0]; ++mm)\n+      idxM.push_back(add(offsetCM, i32_val(m + mm * 2)));\n+\n+  // n indices\n+  Value offsetCN = add((and_(lane, _2)), (add(offWarpN, offPairN)));\n+  SmallVector<Value> idxN;\n+  for (int n = 0; n < shape[1]; n += shapePerCTA[1]) {\n+    for (int nn = 0; nn < rep[1]; ++nn) {\n+      idxN.push_back(add(\n+          offsetCN, i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1])));\n+      idxN.push_back(\n+          add(offsetCN,\n+              i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1] + 1)));\n+    }\n+  }\n+\n+  SmallVector<SmallVector<Value>> axes({idxM, idxN});\n+\n+  // product the axis M and axis N to get coords, ported from\n+  // generator::init_idx method from triton2.0\n+\n+  // TODO[Superjomn]: check the order.\n+  SmallVector<CoordTy> coords;\n+  for (Value x1 : axes[1]) {   // N\n+    for (Value x0 : axes[0]) { // M\n+      SmallVector<Value, 2> idx(2);\n+      idx[0] = x0; // M\n+      idx[1] = x1; // N\n+      coords.push_back(std::move(idx));\n+    }\n+  }\n+\n+  return coords; // {M,N} in row-major\n+}\n+\n+void DotOpMmaV1ConversionHelper::AParam::build(bool isARow) {\n+  int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+  int repM = 2 * packSize0;\n+  int repK = 1;\n+  int spwM = fpw[0] * 4 * repM;\n+  rep.assign({repM, 0, repK});\n+  spw.assign({spwM, 0, 1});\n+  vec = 2 * rep[0];\n+}\n+\n+void DotOpMmaV1ConversionHelper::BParam::build(bool isBRow) {\n+  int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+  rep.assign({0, 2 * packSize1, 1});\n+  spw.assign({0, fpw[1] * 4 * rep[1], 1});\n+  vec = 2 * rep[1];\n+}\n+\n+std::tuple<int, int>\n+DotOpMmaV2ConversionHelper::getRepMN(const RankedTensorType &tensorTy) {\n+  auto mmaLayout = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n+  auto wpt = mmaLayout.getWarpsPerCTA();\n+\n+  int M = tensorTy.getShape()[0];\n+  int N = tensorTy.getShape()[1];\n+  auto [instrM, instrN] = getInstrShapeMN();\n+  int repM = std::max<int>(M / (wpt[0] * instrM), 1);\n+  int repN = std::max<int>(N / (wpt[1] * instrN), 1);\n+  return {repM, repN};\n+}\n+\n+Type DotOpMmaV2ConversionHelper::getShemPtrTy() const {\n+  switch (mmaType) {\n+  case TensorCoreType::FP32_FP16_FP16_FP32:\n+    return ptr_ty(type::f16Ty(ctx), 3);\n+  case TensorCoreType::FP32_BF16_BF16_FP32:\n+    return ptr_ty(type::i16Ty(ctx), 3);\n+  case TensorCoreType::FP32_TF32_TF32_FP32:\n+    return ptr_ty(type::f32Ty(ctx), 3);\n+  case TensorCoreType::INT32_INT8_INT8_INT32:\n+    return ptr_ty(type::i8Ty(ctx), 3);\n+  default:\n+    llvm::report_fatal_error(\"mma16816 data type not supported\");\n+  }\n+  return Type{};\n+}\n+\n+Type DotOpMmaV2ConversionHelper::getMatType() const {\n+  // floating point types\n+  Type fp32x1Ty = vec_ty(type::f32Ty(ctx), 1);\n+  Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n+  Type i16x2Ty = vec_ty(type::i16Ty(ctx), 2);\n+  Type fp16x2Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n+  // LLVM 14.0 does not support bf16 type, so we use i16 instead.\n+  Type bf16x2Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i16x2Ty));\n+  Type fp32Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32x1Ty));\n+  // integer types\n+  Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n+  Type i8x4Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i8x4Ty));\n+\n+  switch (mmaType) {\n+  case TensorCoreType::FP32_FP16_FP16_FP32:\n+    return fp16x2Pack4Ty;\n+  case TensorCoreType::FP32_BF16_BF16_FP32:\n+    return bf16x2Pack4Ty;\n+  case TensorCoreType::FP32_TF32_TF32_FP32:\n+    return fp32Pack4Ty;\n+  case TensorCoreType::INT32_INT8_INT8_INT32:\n+    return i8x4Pack4Ty;\n+  default:\n+    llvm::report_fatal_error(\"Unsupported mma type found\");\n+  }\n+\n+  return Type{};\n+}\n+\n+Type DotOpMmaV2ConversionHelper::getLoadElemTy() {\n+  switch (mmaType) {\n+  case TensorCoreType::FP32_FP16_FP16_FP32:\n+    return vec_ty(type::f16Ty(ctx), 2);\n+  case TensorCoreType::FP32_BF16_BF16_FP32:\n+    return vec_ty(type::bf16Ty(ctx), 2);\n+  case TensorCoreType::FP32_TF32_TF32_FP32:\n+    return type::f32Ty(ctx);\n+  case TensorCoreType::INT32_INT8_INT8_INT32:\n+    return type::i32Ty(ctx);\n+  default:\n+    llvm::report_fatal_error(\"Unsupported mma type found\");\n+  }\n+\n+  return Type{};\n+}\n+\n+Type DotOpMmaV2ConversionHelper::getMmaRetType() const {\n+  Type fp32Ty = type::f32Ty(ctx);\n+  Type i32Ty = type::i32Ty(ctx);\n+  Type fp32x4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n+  Type i32x4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i32Ty));\n+  switch (mmaType) {\n+  case TensorCoreType::FP32_FP16_FP16_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::FP32_BF16_BF16_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::FP32_TF32_TF32_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::INT32_INT8_INT8_INT32:\n+    return i32x4Ty;\n+  default:\n+    llvm::report_fatal_error(\"Unsupported mma type found\");\n+  }\n+\n+  return Type{};\n+}\n+\n+DotOpMmaV2ConversionHelper::TensorCoreType\n+DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(Type operandTy) {\n+  auto tensorTy = operandTy.cast<RankedTensorType>();\n+  auto elemTy = tensorTy.getElementType();\n+  if (elemTy.isF16())\n+    return TensorCoreType::FP32_FP16_FP16_FP32;\n+  if (elemTy.isF32())\n+    return TensorCoreType::FP32_TF32_TF32_FP32;\n+  if (elemTy.isBF16())\n+    return TensorCoreType::FP32_BF16_BF16_FP32;\n+  if (elemTy.isInteger(8))\n+    return TensorCoreType::INT32_INT8_INT8_INT32;\n+  return TensorCoreType::NOT_APPLICABLE;\n+}\n+\n+DotOpMmaV2ConversionHelper::TensorCoreType\n+DotOpMmaV2ConversionHelper::getMmaType(triton::DotOp op) {\n+  Value A = op.getA();\n+  Value B = op.getB();\n+  auto aTy = A.getType().cast<RankedTensorType>();\n+  auto bTy = B.getType().cast<RankedTensorType>();\n+  // d = a*b + c\n+  auto dTy = op.getD().getType().cast<RankedTensorType>();\n+\n+  if (dTy.getElementType().isF32()) {\n+    if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n+      return TensorCoreType::FP32_FP16_FP16_FP32;\n+    if (aTy.getElementType().isBF16() && bTy.getElementType().isBF16())\n+      return TensorCoreType::FP32_BF16_BF16_FP32;\n+    if (aTy.getElementType().isF32() && bTy.getElementType().isF32() &&\n+        op.getAllowTF32())\n+      return TensorCoreType::FP32_TF32_TF32_FP32;\n+  } else if (dTy.getElementType().isInteger(32)) {\n+    if (aTy.getElementType().isInteger(8) && bTy.getElementType().isInteger(8))\n+      return TensorCoreType::INT32_INT8_INT8_INT32;\n+  }\n+\n+  return TensorCoreType::NOT_APPLICABLE;\n+}\n+\n+SmallVector<Value>\n+MMA16816SmemLoader::computeLdmatrixMatOffs(Value warpId, Value lane,\n+                                           Value cSwizzleOffset) {\n+  // 4x4 matrices\n+  Value c = urem(lane, i32_val(8));\n+  Value s = udiv(lane, i32_val(8)); // sub-warp-id\n+\n+  // Decompose s => s_0, s_1, that is the coordinate in 2x2 matrices in a\n+  // warp\n+  Value s0 = urem(s, i32_val(2));\n+  Value s1 = udiv(s, i32_val(2));\n+\n+  // We use different orders for a and b for better performance.\n+  Value kMatArr = kOrder == 1 ? s1 : s0;\n+  Value nkMatArr = kOrder == 1 ? s0 : s1;\n+\n+  // matrix coordinate inside a CTA, the matrix layout is [2x2wpt] for A and\n+  // [2wptx2] for B. e.g. Setting wpt=3, The data layout for A(kOrder=1) is\n+  //   |0 0 1 1 2 2| -> 0,1,2 are the warpids\n+  //   |0 0 1 1 2 2|\n+  //\n+  // for B(kOrder=0) is\n+  //   |0 0|  -> 0,1,2 are the warpids\n+  //   |1 1|\n+  //   |2 2|\n+  //   |0 0|\n+  //   |1 1|\n+  //   |2 2|\n+  // Note, for each warp, it handles a 2x2 matrices, that is the coordinate\n+  // address (s0,s1) annotates.\n+\n+  Value matOff[2];\n+  matOff[kOrder ^ 1] =\n+      add(mul(warpId, i32_val(warpOffStride)),   // warp offset\n+          mul(nkMatArr, i32_val(matArrStride))); // matrix offset inside a warp\n+  matOff[kOrder] = kMatArr;\n+\n+  // Physical offset (before swizzling)\n+  Value cMatOff = matOff[order[0]];\n+  Value sMatOff = matOff[order[1]];\n+  Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n+  cMatOff = add(cMatOff, cSwizzleMatOff);\n+\n+  // row offset inside a matrix, each matrix has 8 rows.\n+  Value sOffInMat = c;\n+\n+  SmallVector<Value> offs(numPtrs);\n+  Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n+  Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n+  for (int i = 0; i < numPtrs; ++i) {\n+    Value cMatOffI = add(cMatOff, i32_val(i * pLoadStrideInMat));\n+    cMatOffI = xor_(cMatOffI, phase);\n+    offs[i] = add(mul(cMatOffI, i32_val(cMatShape)), mul(sOff, sStride));\n+  }\n+\n+  return offs;\n+}\n+\n+SmallVector<Value> MMA16816SmemLoader::computeB32MatOffs(Value warpOff,\n+                                                         Value lane,\n+                                                         Value cSwizzleOffset) {\n+  assert(needTrans && \"Only used in transpose mode.\");\n+  // Load tf32 matrices with lds32\n+  Value cOffInMat = udiv(lane, i32_val(4));\n+  Value sOffInMat = urem(lane, i32_val(4));\n+\n+  Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n+  SmallVector<Value> offs(numPtrs);\n+\n+  for (int mat = 0; mat < 4; ++mat) { // Load 4 mats each time\n+    int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n+    int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n+    if (kMatArrInt > 0) // we don't need pointers for k\n+      continue;\n+    Value kMatArr = i32_val(kMatArrInt);\n+    Value nkMatArr = i32_val(nkMatArrInt);\n+\n+    Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n+                        mul(nkMatArr, i32_val(matArrStride)));\n+    Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n+    cMatOff = add(cMatOff, cSwizzleMatOff);\n+\n+    Value sMatOff = kMatArr;\n+    Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n+    // FIXME: (kOrder == 1?) is really dirty hack\n+    for (int i = 0; i < numPtrs / 2; ++i) {\n+      Value cMatOffI =\n+          add(cMatOff, i32_val(i * pLoadStrideInMat * (kOrder == 1 ? 1 : 2)));\n+      cMatOffI = xor_(cMatOffI, phase);\n+      Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n+      cOff = urem(cOff, i32_val(tileShape[order[0]]));\n+      sOff = urem(sOff, i32_val(tileShape[order[1]]));\n+      offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, sStride));\n+    }\n+  }\n+  return offs;\n+}\n+\n+SmallVector<Value> MMA16816SmemLoader::computeB8MatOffs(Value warpOff,\n+                                                        Value lane,\n+                                                        Value cSwizzleOffset) {\n+  assert(needTrans && \"Only used in transpose mode.\");\n+  Value cOffInMat = udiv(lane, i32_val(4));\n+  Value sOffInMat =\n+      mul(urem(lane, i32_val(4)), i32_val(4)); // each thread load 4 cols\n+\n+  SmallVector<Value> offs(numPtrs);\n+  for (int mat = 0; mat < 4; ++mat) {\n+    int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n+    int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n+    if (kMatArrInt > 0) // we don't need pointers for k\n+      continue;\n+    Value kMatArr = i32_val(kMatArrInt);\n+    Value nkMatArr = i32_val(nkMatArrInt);\n+\n+    Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n+                        mul(nkMatArr, i32_val(matArrStride)));\n+    Value sMatOff = kMatArr;\n+\n+    for (int loadx4Off = 0; loadx4Off < numPtrs / 8; ++loadx4Off) {\n+      for (int elemOff = 0; elemOff < 4; ++elemOff) {\n+        int ptrOff = loadx4Off * 8 + nkMatArrInt * 4 + elemOff;\n+        Value cMatOffI = add(cMatOff, i32_val(loadx4Off * pLoadStrideInMat *\n+                                              (kOrder == 1 ? 1 : 2)));\n+        Value sOffInMatElem = add(sOffInMat, i32_val(elemOff));\n+\n+        // disable swizzling ...\n+\n+        Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n+        Value sOff = add(sOffInMatElem, mul(sMatOff, i32_val(sMatShape)));\n+        // To prevent out-of-bound access when tile is too small.\n+        cOff = urem(cOff, i32_val(tileShape[order[0]]));\n+        sOff = urem(sOff, i32_val(tileShape[order[1]]));\n+        offs[ptrOff] = add(cOff, mul(sOff, sStride));\n+      }\n+    }\n+  }\n+  return offs;\n+}\n+\n+std::tuple<Value, Value, Value, Value>\n+MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> offs,\n+                           ArrayRef<Value> ptrs, Type matTy,\n+                           Type shemPtrTy) const {\n+  assert(mat0 % 2 == 0 && mat1 % 2 == 0 && \"smem matrix load must be aligned\");\n+  int matIdx[2] = {mat0, mat1};\n+\n+  int ptrIdx{-1};\n+\n+  if (canUseLdmatrix)\n+    ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n+  else if (elemBytes == 4 && needTrans)\n+    ptrIdx = matIdx[order[0]];\n+  else if (elemBytes == 1 && needTrans)\n+    ptrIdx = matIdx[order[0]] * 4;\n+  else\n+    llvm::report_fatal_error(\"unsupported mma type found\");\n+\n+  // The main difference with the original triton code is we removed the\n+  // prefetch-related logic here for the upstream optimizer phase should\n+  // take care with it, and that is transparent in dot conversion.\n+  auto getPtr = [&](int idx) { return ptrs[idx]; };\n+\n+  Value ptr = getPtr(ptrIdx);\n+\n+  // The struct should have exactly the same element types.\n+  auto resTy = matTy.cast<LLVM::LLVMStructType>();\n+  Type elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n+\n+  // For some reasons, LLVM's NVPTX backend inserts unnecessary (?) integer\n+  // instructions to pack & unpack sub-word integers. A workaround is to\n+  // store the results of ldmatrix in i32\n+  if (auto vecElemTy = elemTy.dyn_cast<VectorType>()) {\n+    Type elemElemTy = vecElemTy.getElementType();\n+    if (auto intTy = elemElemTy.dyn_cast<IntegerType>()) {\n+      if (intTy.getWidth() <= 16) {\n+        elemTy = rewriter.getI32Type();\n+        resTy =\n+            LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, elemTy));\n+      }\n+    }\n+  }\n+\n+  if (canUseLdmatrix) {\n+    Value sOffset =\n+        mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sStride);\n+    Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n+\n+    PTXBuilder builder;\n+    // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n+    // thread.\n+    auto resArgs = builder.newListOperand(4, \"=r\");\n+    auto addrArg = builder.newAddrOperand(sOffsetPtr, \"r\");\n+\n+    auto ldmatrix = builder.create(\"ldmatrix.sync.aligned.m8n8.x4\")\n+                        ->o(\"trans\", needTrans /*predicate*/)\n+                        .o(\"shared.b16\");\n+    ldmatrix(resArgs, addrArg);\n+\n+    // The result type is 4xi32, each i32 is composed of 2xf16\n+    // elements (adjacent two columns in a row) or a single f32 element.\n+    Value resV4 = builder.launch(rewriter, loc, resTy);\n+    return {extract_val(elemTy, resV4, 0), extract_val(elemTy, resV4, 1),\n+            extract_val(elemTy, resV4, 2), extract_val(elemTy, resV4, 3)};\n+  } else if (elemBytes == 4 && needTrans) { // Use lds.32 to load tf32 matrices\n+    Value ptr2 = getPtr(ptrIdx + 1);\n+    assert(sMatStride == 1);\n+    int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n+    Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n+    int sOffsetArrElem = sMatStride * sMatShape;\n+    Value sOffsetArrElemVal =\n+        add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n+\n+    Value elems[4];\n+    if (kOrder == 1) {\n+      elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n+      elems[1] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n+      elems[2] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n+      elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n+    } else {\n+      elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n+      elems[2] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n+      elems[1] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n+      elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n+    }\n+    std::array<Value, 4> retElems;\n+    retElems.fill(undef(elemTy));\n+    for (auto i = 0; i < 4; ++i) {\n+      retElems[i] = insert_element(elemTy, retElems[i], elems[i], i32_val(0));\n+    }\n+    return {retElems[0], retElems[1], retElems[2], retElems[3]};\n+  } else if (elemBytes == 1 && needTrans) { // work with int8\n+    // Can't use i32 here. Use LLVM's VectorType\n+    elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n+    std::array<std::array<Value, 4>, 2> ptrs;\n+    ptrs[0] = {\n+        getPtr(ptrIdx),\n+        getPtr(ptrIdx + 1),\n+        getPtr(ptrIdx + 2),\n+        getPtr(ptrIdx + 3),\n+    };\n+\n+    ptrs[1] = {\n+        getPtr(ptrIdx + 4),\n+        getPtr(ptrIdx + 5),\n+        getPtr(ptrIdx + 6),\n+        getPtr(ptrIdx + 7),\n+    };\n+\n+    assert(sMatStride == 1);\n+    int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n+    Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n+    int sOffsetArrElem = 1 * (sMatStride * sMatShape);\n+    Value sOffsetArrElemVal =\n+        add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n+\n+    std::array<Value, 4> i8v4Elems;\n+    i8v4Elems.fill(undef(elemTy));\n+\n+    Value i8Elems[4][4];\n+    if (kOrder == 1) {\n+      for (int i = 0; i < 2; ++i)\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[i][j] = load(gep(shemPtrTy, ptrs[i][j], sOffsetElemVal));\n+\n+      for (int i = 2; i < 4; ++i)\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[i][j] =\n+              load(gep(shemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n+\n+      for (int m = 0; m < 4; ++m) {\n+        for (int e = 0; e < 4; ++e)\n+          i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n+                                        i8Elems[m][e], i32_val(e));\n+      }\n+    } else { // k first\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[0][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetElemVal));\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[2][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetElemVal));\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[1][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[3][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n+\n+      for (int m = 0; m < 4; ++m) {\n+        for (int e = 0; e < 4; ++e)\n+          i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n+                                        i8Elems[m][e], i32_val(e));\n+      }\n+    }\n+\n+    return {bitcast(i8v4Elems[0], i32_ty), bitcast(i8v4Elems[1], i32_ty),\n+            bitcast(i8v4Elems[2], i32_ty), bitcast(i8v4Elems[3], i32_ty)};\n+  }\n+\n+  assert(false && \"Invalid smem load\");\n+  return {Value{}, Value{}, Value{}, Value{}};\n+}\n+\n+MMA16816SmemLoader::MMA16816SmemLoader(\n+    int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n+    ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n+    ArrayRef<int> instrShape, ArrayRef<int> matShape, int perPhase,\n+    int maxPhase, int elemBytes, ConversionPatternRewriter &rewriter,\n+    TypeConverter *typeConverter, const Location &loc)\n+    : order(order.begin(), order.end()), kOrder(kOrder),\n+      tileShape(tileShape.begin(), tileShape.end()),\n+      instrShape(instrShape.begin(), instrShape.end()),\n+      matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n+      maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter), loc(loc),\n+      ctx(rewriter.getContext()) {\n+  cMatShape = matShape[order[0]];\n+  sMatShape = matShape[order[1]];\n+\n+  sStride = smemStrides[order[1]];\n+\n+  // rule: k must be the fast-changing axis.\n+  needTrans = kOrder != order[0];\n+  canUseLdmatrix = elemBytes == 2 || (!needTrans); // b16\n+\n+  if (canUseLdmatrix) {\n+    // Each CTA, the warps is arranged as [1xwpt] if not transposed,\n+    // otherwise [wptx1], and each warp will perform a mma.\n+    numPtrs =\n+        tileShape[order[0]] / (needTrans ? wpt : 1) / instrShape[order[0]];\n+  } else {\n+    numPtrs = tileShape[order[0]] / wpt / matShape[order[0]];\n+  }\n+  numPtrs = std::max<int>(numPtrs, 2);\n+\n+  // Special rule for i8/u8, 4 ptrs for each matrix\n+  if (!canUseLdmatrix && elemBytes == 1)\n+    numPtrs *= 4;\n+\n+  int loadStrideInMat[2];\n+  loadStrideInMat[kOrder] =\n+      2; // instrShape[kOrder] / matShape[kOrder], always 2\n+  loadStrideInMat[kOrder ^ 1] =\n+      wpt * (instrShape[kOrder ^ 1] / matShape[kOrder ^ 1]);\n+\n+  pLoadStrideInMat = loadStrideInMat[order[0]];\n+  sMatStride =\n+      loadStrideInMat[order[1]] / (instrShape[order[1]] / matShape[order[1]]);\n+\n+  // Each matArr contains warpOffStride matrices.\n+  matArrStride = kOrder == 1 ? 1 : wpt;\n+  warpOffStride = instrShape[kOrder ^ 1] / matShape[kOrder ^ 1];\n+}\n+Value MMA16816ConversionHelper::loadA(Value tensor,\n+                                      const SharedMemoryObject &smemObj) const {\n+  auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n+\n+  SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n+                             aTensorTy.getShape().end());\n+\n+  ValueTable ha;\n+  std::function<void(int, int)> loadFn;\n+  auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n+  auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(aTensorTy);\n+\n+  int numRepM = getNumRepM(aTensorTy, shape[0]);\n+  int numRepK = getNumRepK(aTensorTy, shape[1]);\n+\n+  if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n+    Value warpM = getWarpM(shape[0]);\n+    // load from smem\n+    // we use ldmatrix.x4 so each warp processes 16x16 elements.\n+    int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / 16);\n+    loadFn =\n+        getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/,\n+                        {mmaInstrM, mmaInstrK} /*instrShape*/,\n+                        {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/,\n+                        ha /*vals*/, true /*isA*/);\n+  } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n+    // load from registers, used in gemm fuse\n+    // TODO(Superjomn) Port the logic.\n+    assert(false && \"Loading A from register is not supported yet.\");\n+  } else {\n+    assert(false && \"A's layout is not supported.\");\n+  }\n+\n+  // step1. Perform loading.\n+  for (int m = 0; m < numRepM; ++m)\n+    for (int k = 0; k < numRepK; ++k)\n+      loadFn(2 * m, 2 * k);\n+\n+  // step2. Format the values to LLVM::Struct to passing to mma codegen.\n+  return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n+}\n+Value MMA16816ConversionHelper::loadB(Value tensor,\n+                                      const SharedMemoryObject &smemObj) {\n+  ValueTable hb;\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+\n+  SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n+                             tensorTy.getShape().end());\n+\n+  // TODO[Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n+  bool transB = false;\n+  if (transB) {\n+    std::swap(shape[0], shape[1]);\n+  }\n+\n+  auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(tensorTy);\n+  auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(tensorTy);\n+  int numRepK = getNumRepK(tensorTy, shape[0]);\n+  int numRepN = getNumRepN(tensorTy, shape[1]);\n+\n+  Value warpN = getWarpN(shape[1]);\n+  // we use ldmatrix.x4 so each warp processes 16x16 elements.\n+  int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / 16);\n+  auto loadFn =\n+      getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/,\n+                      {mmaInstrK, mmaInstrN} /*instrShape*/,\n+                      {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/,\n+                      hb /*vals*/, false /*isA*/);\n+\n+  for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n+    for (int k = 0; k < numRepK; ++k)\n+      loadFn(2 * n, 2 * k);\n+  }\n+\n+  Value result = composeValuesToDotOperandLayoutStruct(\n+      hb, std::max(numRepN / 2, 1), numRepK);\n+  return result;\n+}\n+Value MMA16816ConversionHelper::loadC(Value tensor, Value llTensor) const {\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(tensorTy);\n+  size_t fcSize = 4 * repM * repN;\n+\n+  assert(tensorTy.getEncoding().isa<MmaEncodingAttr>() &&\n+         \"Currently, we only support $c with a mma layout.\");\n+  // Load a normal C tensor with mma layout, that should be a\n+  // LLVM::struct with fcSize elements.\n+  auto structTy = llTensor.getType().cast<LLVM::LLVMStructType>();\n+  assert(structTy.getBody().size() == fcSize &&\n+         \"DotOp's $c operand should pass the same number of values as $d in \"\n+         \"mma layout.\");\n+  return llTensor;\n+}\n+LogicalResult MMA16816ConversionHelper::convertDot(Value a, Value b, Value c,\n+                                                   Value d, Value loadedA,\n+                                                   Value loadedB, Value loadedC,\n+                                                   DotOp op,\n+                                                   DotOpAdaptor adaptor) const {\n+  helper.deduceMmaType(op);\n+\n+  auto aTensorTy = a.getType().cast<RankedTensorType>();\n+  auto dTensorTy = d.getType().cast<RankedTensorType>();\n+\n+  SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n+                              aTensorTy.getShape().end());\n+\n+  auto dShape = dTensorTy.getShape();\n+\n+  // shape / shape_per_cta\n+  int numRepM = getNumRepM(aTensorTy, dShape[0]);\n+  int numRepN = getNumRepN(aTensorTy, dShape[1]);\n+  int numRepK = getNumRepK(aTensorTy, aShape[1]);\n+\n+  ValueTable ha =\n+      getValuesFromDotOperandLayoutStruct(loadedA, numRepM, numRepK);\n+  ValueTable hb = getValuesFromDotOperandLayoutStruct(\n+      loadedB, std::max(numRepN / 2, 1), numRepK);\n+  auto fc = getElementsFromStruct(loc, loadedC, rewriter);\n+\n+  auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n+    unsigned colsPerThread = numRepN * 2;\n+    PTXBuilder builder;\n+    auto &mma = *builder.create(helper.getMmaInstr().str());\n+    // using =r for float32 works but leads to less readable ptx.\n+    bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n+    auto retArgs = builder.newListOperand(4, isIntMMA ? \"=r\" : \"=f\");\n+    auto aArgs = builder.newListOperand({\n+        {ha[{m, k}], \"r\"},\n+        {ha[{m + 1, k}], \"r\"},\n+        {ha[{m, k + 1}], \"r\"},\n+        {ha[{m + 1, k + 1}], \"r\"},\n+    });\n+    auto bArgs =\n+        builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n+    auto cArgs = builder.newListOperand();\n+    for (int i = 0; i < 4; ++i) {\n+      cArgs->listAppend(builder.newOperand(fc[m * colsPerThread + 4 * n + i],\n+                                           std::to_string(i)));\n+      // reuse the output registers\n+    }\n+\n+    mma(retArgs, aArgs, bArgs, cArgs);\n+    Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n+\n+    Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n+    for (int i = 0; i < 4; ++i)\n+      fc[m * colsPerThread + 4 * n + i] = extract_val(elemTy, mmaOut, i);\n+  };\n+\n+  for (int k = 0; k < numRepK; ++k)\n+    for (int m = 0; m < numRepM; ++m)\n+      for (int n = 0; n < numRepN; ++n)\n+        callMma(2 * m, n, 2 * k);\n+\n+  Type resElemTy = dTensorTy.getElementType();\n+\n+  for (auto &elem : fc) {\n+    elem = bitcast(elem, resElemTy);\n+  }\n+\n+  // replace with new packed result\n+  Type structTy = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(fc.size(), resElemTy));\n+  Value res = getStructFromElements(loc, fc, rewriter, structTy);\n+  rewriter.replaceOp(op, res);\n+\n+  return success();\n+}\n+std::function<void(int, int)> MMA16816ConversionHelper::getLoadMatrixFn(\n+    Value tensor, const SharedMemoryObject &smemObj, MmaEncodingAttr mmaLayout,\n+    int wpt, uint32_t kOrder, SmallVector<int> instrShape,\n+    SmallVector<int> matShape, Value warpId,\n+    MMA16816ConversionHelper::ValueTable &vals, bool isA) const {\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  // We assumes that the input operand of Dot should be from shared layout.\n+  // TODO(Superjomn) Consider other layouts if needed later.\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  const int perPhase = sharedLayout.getPerPhase();\n+  const int maxPhase = sharedLayout.getMaxPhase();\n+  const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n+  auto order = sharedLayout.getOrder();\n+\n+  // the original register_lds2, but discard the prefetch logic.\n+  auto ld2 = [](ValueTable &vals, int mn, int k, Value val) {\n+    vals[{mn, k}] = val;\n+  };\n+\n+  // (a, b) is the coordinate.\n+  auto load = [=, &vals, &ld2](int a, int b) {\n+    MMA16816SmemLoader loader(\n+        wpt, sharedLayout.getOrder(), kOrder, smemObj.strides,\n+        tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n+        maxPhase, elemBytes, rewriter, typeConverter, loc);\n+    Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+    SmallVector<Value> offs =\n+        loader.computeOffsets(warpId, lane, cSwizzleOffset);\n+    const int numPtrs = loader.getNumPtrs();\n+    SmallVector<Value> ptrs(numPtrs);\n+\n+    Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+\n+    Type smemPtrTy = helper.getShemPtrTy();\n+    for (int i = 0; i < numPtrs; ++i) {\n+      ptrs[i] =\n+          bitcast(gep(smemPtrTy, smemBase, ValueRange({offs[i]})), smemPtrTy);\n+    }\n+\n+    auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n+        (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n+        ptrs, helper.getMatType(), helper.getShemPtrTy());\n+\n+    if (isA) {\n+      ld2(vals, a, b, ha0);\n+      ld2(vals, a + 1, b, ha1);\n+      ld2(vals, a, b + 1, ha2);\n+      ld2(vals, a + 1, b + 1, ha3);\n+    } else {\n+      ld2(vals, a, b, ha0);\n+      ld2(vals, a + 1, b, ha2);\n+      ld2(vals, a, b + 1, ha1);\n+      ld2(vals, a + 1, b + 1, ha3);\n+    }\n+  };\n+\n+  return load;\n+}\n+Value MMA16816ConversionHelper::composeValuesToDotOperandLayoutStruct(\n+    const MMA16816ConversionHelper::ValueTable &vals, int n0, int n1) const {\n+  std::vector<Value> elems;\n+  for (int m = 0; m < n0; ++m)\n+    for (int k = 0; k < n1; ++k) {\n+      elems.push_back(vals.at({2 * m, 2 * k}));\n+      elems.push_back(vals.at({2 * m, 2 * k + 1}));\n+      elems.push_back(vals.at({2 * m + 1, 2 * k}));\n+      elems.push_back(vals.at({2 * m + 1, 2 * k + 1}));\n+    }\n+\n+  assert(!elems.empty());\n+\n+  Type elemTy = elems[0].getType();\n+  Type structTy = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(elems.size(), elemTy));\n+  auto result = getStructFromElements(loc, elems, rewriter, structTy);\n+  return result;\n+}\n+MMA16816ConversionHelper::ValueTable\n+MMA16816ConversionHelper::getValuesFromDotOperandLayoutStruct(Value value,\n+                                                              int n0,\n+                                                              int n1) const {\n+  auto elems = getElementsFromStruct(loc, value, rewriter);\n+\n+  int offset{};\n+  ValueTable vals;\n+  for (int i = 0; i < n0; ++i) {\n+    for (int j = 0; j < n1; j++) {\n+      vals[{2 * i, 2 * j}] = elems[offset++];\n+      vals[{2 * i, 2 * j + 1}] = elems[offset++];\n+      vals[{2 * i + 1, 2 * j}] = elems[offset++];\n+      vals[{2 * i + 1, 2 * j + 1}] = elems[offset++];\n+    }\n+  }\n+  return vals;\n+}\n+SmallVector<Value> DotOpFMAConversionHelper::getThreadIds(\n+    Value threadId, ArrayRef<unsigned int> shapePerCTA,\n+    ArrayRef<unsigned int> sizePerThread, ArrayRef<unsigned int> order,\n+    ConversionPatternRewriter &rewriter, Location loc) const {\n+  int dim = order.size();\n+  SmallVector<Value> threadIds(dim);\n+  for (unsigned k = 0; k < dim - 1; k++) {\n+    Value dimK = i32_val(shapePerCTA[order[k]] / sizePerThread[order[k]]);\n+    Value rem = urem(threadId, dimK);\n+    threadId = udiv(threadId, dimK);\n+    threadIds[order[k]] = rem;\n+  }\n+  Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n+  threadIds[order[dim - 1]] = urem(threadId, dimK);\n+  return threadIds;\n+}\n+Value DotOpFMAConversionHelper::loadA(\n+    Value A, Value llA, BlockedEncodingAttr dLayout, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto aShape = aTensorTy.getShape();\n+\n+  auto aOrder = aLayout.getOrder();\n+  auto order = dLayout.getOrder();\n+\n+  bool isARow = aOrder[0] == 1;\n+\n+  auto aSmem = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n+  Value strideAM = aSmem.strides[0];\n+  Value strideAK = aSmem.strides[1];\n+  Value strideA0 = isARow ? strideAK : strideAM;\n+  Value strideA1 = isARow ? strideAM : strideAK;\n+  int aNumPtr = 8;\n+  int K = aShape[1];\n+  int M = aShape[0];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value mContig = i32_val(sizePerThread[order[1]]);\n+\n+  // threadId in blocked layout\n+  auto threadIds =\n+      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n+  Value threadIdM = threadIds[0];\n+\n+  Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n+  Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n+  SmallVector<Value> aOff(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i) {\n+    aOff[i] = add(mul(offA0, strideA0), mul(offA1, strideA1));\n+  }\n+  auto elemTy = A.getType().cast<RankedTensorType>().getElementType();\n+\n+  Type ptrTy = ptr_ty(elemTy, 3);\n+  SmallVector<Value> aPtrs(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i)\n+    aPtrs[i] = gep(ptrTy, aSmem.base, aOff[i]);\n+\n+  SmallVector<Value> vas;\n+\n+  int mShapePerCTA = getShapePerCTAForMN(dLayout, true /*isM*/);\n+  int mSizePerThread = getSizePerThreadForMN(dLayout, true /*isM*/);\n+\n+  for (unsigned k = 0; k < K; ++k)\n+    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+      for (unsigned mm = 0; mm < mSizePerThread; ++mm) {\n+        Value offset =\n+            add(mul(i32_val(m + mm), strideAM), mul(i32_val(k), strideAK));\n+        Value pa = gep(ptrTy, aPtrs[0], offset);\n+        Value va = load(pa);\n+        vas.emplace_back(va);\n+      }\n+\n+  return getStructFromValueTable(vas, rewriter, loc, elemTy);\n+}\n+Value DotOpFMAConversionHelper::loadB(\n+    Value B, Value llB, BlockedEncodingAttr dLayout, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto bShape = bTensorTy.getShape();\n+\n+  auto bOrder = bLayout.getOrder();\n+  auto order = dLayout.getOrder();\n+\n+  bool isBRow = bOrder[0] == 1;\n+\n+  auto bSmem = getSharedMemoryObjectFromStruct(loc, llB, rewriter);\n+  Value strideBN = bSmem.strides[1];\n+  Value strideBK = bSmem.strides[0];\n+  Value strideB0 = isBRow ? strideBN : strideBK;\n+  Value strideB1 = isBRow ? strideBK : strideBN;\n+  int bNumPtr = 8;\n+  int K = bShape[0];\n+  int N = bShape[1];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value nContig = i32_val(sizePerThread[order[0]]);\n+\n+  // threadId in blocked layout\n+  auto threadIds =\n+      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n+  Value threadIdN = threadIds[1];\n+\n+  Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n+  Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n+  SmallVector<Value> bOff(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i) {\n+    bOff[i] = add(mul(offB0, strideB0), mul(offB1, strideB1));\n+  }\n+  auto elemTy = B.getType().cast<RankedTensorType>().getElementType();\n+\n+  Type ptrTy = ptr_ty(elemTy, 3);\n+  SmallVector<Value> bPtrs(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i)\n+    bPtrs[i] = gep(ptrTy, bSmem.base, bOff[i]);\n+\n+  SmallVector<Value> vbs;\n+\n+  int nShapePerCTA = getShapePerCTAForMN(dLayout, false /*isM*/);\n+  int nSizePerThread = getSizePerThreadForMN(dLayout, false /*isM*/);\n+\n+  for (unsigned k = 0; k < K; ++k)\n+    for (unsigned n = 0; n < N; n += nShapePerCTA)\n+      for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n+        Value offset =\n+            add(mul(i32_val(n + nn), strideBN), mul(i32_val(k), strideBK));\n+        Value pb = gep(ptrTy, bPtrs[0], offset);\n+        Value vb = load(pb);\n+        vbs.emplace_back(vb);\n+      }\n+\n+  return getStructFromValueTable(vbs, rewriter, loc, elemTy);\n+}\n+DotOpFMAConversionHelper::ValueTable\n+DotOpFMAConversionHelper::getValueTableFromStruct(\n+    Value val, int K, int n0, int shapePerCTA, int sizePerThread,\n+    ConversionPatternRewriter &rewriter, Location loc) const {\n+  ValueTable res;\n+  auto elems = getElementsFromStruct(loc, val, rewriter);\n+  int index = 0;\n+  for (unsigned k = 0; k < K; ++k) {\n+    for (unsigned m = 0; m < n0; m += shapePerCTA)\n+      for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n+        res[{m + mm, k}] = elems[index++];\n+      }\n+  }\n+  return res;\n+}\n+Value DotOpFMAConversionHelper::getStructFromValueTable(\n+    ArrayRef<Value> vals, ConversionPatternRewriter &rewriter, Location loc,\n+    Type elemTy) const {\n+  SmallVector<Type> elemTypes(vals.size(), elemTy);\n+  SmallVector<Value> elems;\n+  elems.reserve(vals.size());\n+  for (auto &val : vals) {\n+    elems.push_back(val);\n+  }\n+\n+  Type structTy = struct_ty(elemTypes);\n+  return getStructFromElements(loc, elems, rewriter, structTy);\n+}\n+int DotOpFMAConversionHelper::getNumElemsPerThread(\n+    ArrayRef<int64_t> shape, DotOperandEncodingAttr dotOpLayout) {\n+  auto blockedLayout = dotOpLayout.getParent().cast<BlockedEncodingAttr>();\n+  auto shapePerCTA = getShapePerCTA(blockedLayout);\n+  auto sizePerThread = getSizePerThread(blockedLayout);\n+\n+  // TODO[Superjomn]: we assume the k aixs is fixed for $a and $b here, fix it\n+  // if not.\n+  int K = dotOpLayout.getOpIdx() == 0 ? shape[1] : shape[0];\n+  int otherDim = dotOpLayout.getOpIdx() == 1 ? shape[1] : shape[0];\n+\n+  bool isM = dotOpLayout.getOpIdx() == 0;\n+  int shapePerCTAMN = getShapePerCTAForMN(blockedLayout, isM);\n+  int sizePerThreadMN = getSizePerThreadForMN(blockedLayout, isM);\n+  return K * std::max<int>(otherDim / shapePerCTAMN, 1) * sizePerThreadMN;\n+}\n+} // namespace LLVM\n+} // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 97, "deletions": 1256, "changes": 1353, "file_content_changes": "@@ -2,15 +2,13 @@\n #define TRITON_CONVERSION_TRITONGPU_TO_LLVM_DOT_OP_HELPERS_H\n \n #include \"mlir/Analysis/SliceAnalysis.h\"\n-#include \"mlir/Conversion/ArithmeticToLLVM/ArithmeticToLLVM.h\"\n+#include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n #include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n #include \"mlir/Conversion/LLVMCommon/LoweringOptions.h\"\n #include \"mlir/Conversion/LLVMCommon/Pattern.h\"\n #include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n-#include \"mlir/Conversion/SCFToStandard/SCFToStandard.h\"\n-#include \"mlir/Conversion/StandardToLLVM/ConvertStandardToLLVM.h\"\n-#include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n-#include \"mlir/Dialect/GPU/GPUDialect.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/Matchers.h\"\n@@ -47,41 +45,35 @@ struct DotOpMmaV1ConversionHelper {\n       : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()) {}\n \n   // Help to share some variables across multiple functions for A.\n+  // TODO[Superjomn]: refactor and restrict this to only use in DotOp\n+  // conversion.\n   struct AParam {\n     SmallVector<int> rep;\n     SmallVector<int> spw;\n+    bool isAVec4{};\n+    int vec{}; // This could only used in DotOp, not in\n+               // loadA/loadB/TypeConverter\n \n-    // TODO[Superjomn]: Support the case when isAVec4=false later\n-    // Currently, we only support ld.v2, for the mma layout varies with\n-    // different ld vector width.\n-    // bool isAVec4 = !isARow && shapeTransed[orderTransed[0]] <= 16;\n-    const bool isAVec4{true};\n-\n-    explicit AParam(bool isARow) {\n-      int packSize0 = (isARow || isAVec4) ? 1 : 2;\n-      int repM = 2 * packSize0;\n-      int repK = 1;\n-      int spwM = fpw[0] * 4 * repM;\n-      rep.assign({repM, 0, repK});\n-      spw.assign({spwM, 0, 1});\n-    }\n+    AParam(bool isARow, bool isAVec4) : isAVec4(isAVec4) { build(isARow); }\n+\n+  private:\n+    void build(bool isARow);\n   };\n \n   // Help to share some variables across multiple functions for A.\n+  // TODO[Superjomn]: refactor and restrict this to only use in DotOp\n+  // conversion.\n   struct BParam {\n     SmallVector<int> rep;\n     SmallVector<int> spw;\n-    // TODO[Superjomn]: Support the case when isBVec4=false later\n-    // Currently, we only support ld.v2, for the mma layout varies with\n-    // different ld vector width.\n-    // bool isBVec4 = isBRow && shapeTransed[orderTransed[0]] <= 16;\n-    const bool isBVec4{true};\n-\n-    explicit BParam(bool isBRow) {\n-      int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n-      rep.assign({0, 2 * packSize1, 1});\n-      spw.assign({0, fpw[1] * 4 * rep[1], 1});\n-    }\n+    bool isBVec4{};\n+    int vec{}; // This could only used in DotOp, not in\n+               // loadA/loadB/TypeConverter\n+\n+    BParam(bool isBRow, bool isBVec4) : isBVec4(isBVec4) { build(isBRow); }\n+\n+  private:\n+    void build(bool isBRow);\n   };\n \n   int getRepM(int M) const {\n@@ -100,286 +92,42 @@ struct DotOpMmaV1ConversionHelper {\n     return struct_ty(SmallVector<Type>{8, fp32Ty});\n   }\n \n+  static Type getMatType(TensorType operand) {\n+    auto *ctx = operand.getContext();\n+    Type fp16Ty = type::f16Ty(ctx);\n+    Type vecTy = vec_ty(fp16Ty, 2);\n+    return struct_ty(SmallVector<Type>{vecTy});\n+  }\n+\n   // Get the number of fp16x2 elements for $a.\n-  // \\param shapeTransed: A's shape or reordered shape if transpose needed.\n-  // \\param orderTransed: the order or reordered order if transpose needed.\n-  unsigned getNumM(ArrayRef<int64_t> shapeTransed, bool isARow) const {\n-    AParam param(isARow);\n+  unsigned getNumM(int M, bool isARow, bool isAVec4) const {\n+    AParam param(isARow, isAVec4);\n \n-    unsigned numM = param.rep[0] * shapeTransed[0] / (param.spw[0] * wpt[0]);\n+    unsigned numM = param.rep[0] * M / (param.spw[0] * wpt[0]);\n     return numM;\n   }\n \n   // Get the number of fp16x2 elements for $b.\n-  // \\param shapeTransed: B' shape or reordered shape if transpose needed.\n-  // \\param orderTransed: the order or reordered order if transpose needed.\n-  unsigned getNumN(ArrayRef<int64_t> shapeTransed, bool isBRow) const {\n-    BParam param(isBRow);\n+  unsigned getNumN(int N, bool isBRow, bool isBVec4) const {\n+    BParam param(isBRow, isBVec4);\n \n-    unsigned numN = param.rep[1] * shapeTransed[1] / (param.spw[1] * wpt[1]);\n+    unsigned numN = param.rep[1] * N / (param.spw[1] * wpt[1]);\n     return numN;\n   }\n \n-  int numElemsPerThreadA(ArrayRef<int64_t> shapeTransed,\n-                         ArrayRef<unsigned> orderTransed) const {\n-    int numM = getNumM(shapeTransed, orderTransed[0] == 1);\n-    int NK = shapeTransed[1];\n-\n-    // NOTE: We couldn't get the vec from the shared layout.\n-    // int vecA = sharedLayout.getVec();\n-    // TODO[Superjomn]: Consider the case when vecA > 4\n-    bool vecGt4 = false;\n-    int elemsPerLd = vecGt4 ? 4 : 2;\n-    return (numM / 2) * (NK / 4) * elemsPerLd;\n-  }\n+  int numElemsPerThreadA(ArrayRef<int64_t> shape, bool isARow, bool isAVec4,\n+                         int vec) const;\n \n-  int numElemsPerThreadB(ArrayRef<int64_t> shapeTransed,\n-                         ArrayRef<unsigned> orderTransed) const {\n-    unsigned numN = getNumN(shapeTransed, orderTransed[0] == 1);\n-    int NK = shapeTransed[0];\n-    // NOTE: We couldn't get the vec from the shared layout.\n-    // int vecB = sharedLayout.getVec();\n-    // TODO[Superjomn]: Consider the case when vecA > 4\n-    bool vecGt4 = false;\n-    int elemsPerLd = vecGt4 ? 4 : 2;\n-    return (numN / 2) * (NK / 4) * elemsPerLd;\n-  }\n+  int numElemsPerThreadB(ArrayRef<int64_t> shape, bool isBRow, bool isBVec4,\n+                         int vec) const;\n \n   // Loading $a from smem to registers, returns a LLVM::Struct.\n-  Value loadA(Value tensor, bool transA, const SharedMemoryObject &smemObj,\n-              Value thread, Location loc,\n-              ConversionPatternRewriter &rewriter) const {\n-    auto *ctx = rewriter.getContext();\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-    SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n-                               tensorTy.getShape().end());\n-    SmallVector<unsigned> order(sharedLayout.getOrder().begin(),\n-                                sharedLayout.getOrder().end());\n-\n-    Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n-    Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-\n-    bool isARow = order[0] != 0;\n-    AParam param(isARow);\n-\n-    auto [offsetAM, offsetAK, _0, _1] = computeOffsets(\n-        thread, isARow, false, fpw, param.spw, param.rep, rewriter, loc);\n-\n-    if (transA) {\n-      std::swap(shape[0], shape[1]);\n-      std::swap(offsetAM, offsetAK);\n-      std::swap(order[0], order[1]);\n-    }\n-\n-    int vecA = sharedLayout.getVec();\n-\n-    auto strides = smemObj.strides;\n-    Value strideAM = isARow ? strides[0] : i32_val(1);\n-    Value strideAK = isARow ? i32_val(1) : strides[1];\n-    Value strideA0 = isARow ? strideAK : strideAM;\n-    Value strideA1 = isARow ? strideAM : strideAK;\n-\n-    int strideRepM = wpt[0] * fpw[0] * 8;\n-    int strideRepK = 1;\n-\n-    // swizzling\n-    int perPhaseA = sharedLayout.getPerPhase();\n-    int maxPhaseA = sharedLayout.getMaxPhase();\n-    int stepA0 = isARow ? strideRepK : strideRepM;\n-    int numPtrA = std::max(2 * perPhaseA * maxPhaseA / stepA0, 1);\n-    int NK = shape[1];\n-\n-    // pre-compute pointer lanes\n-    Value offA0 = isARow ? offsetAK : offsetAM;\n-    Value offA1 = isARow ? offsetAM : offsetAK;\n-    Value phaseA = urem(udiv(offA1, i32_val(perPhaseA)), i32_val(maxPhaseA));\n-    offA0 = add(offA0, cSwizzleOffset);\n-    SmallVector<Value> offA(numPtrA);\n-    for (int i = 0; i < numPtrA; i++) {\n-      Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));\n-      offA0I = udiv(offA0I, i32_val(vecA));\n-      offA0I = xor_(offA0I, phaseA);\n-      offA0I = mul(offA0I, i32_val(vecA));\n-      offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n-    }\n-\n-    Type elemX2Ty = vec_ty(f16_ty, 2);\n-    Type elemPtrTy = ptr_ty(f16_ty);\n-    if (tensorTy.getElementType().isBF16()) {\n-      elemX2Ty = vec_ty(i16_ty, 2);\n-      elemPtrTy = ptr_ty(i16_ty);\n-    }\n-\n-    // prepare arguments\n-    SmallVector<Value> ptrA(numPtrA);\n-\n-    std::map<std::pair<int, int>, std::pair<Value, Value>> has;\n-    for (int i = 0; i < numPtrA; i++)\n-      ptrA[i] = gep(ptr_ty(f16_ty), smemBase, offA[i]);\n-\n-    auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n-      vals[{m, k}] = {val0, val1};\n-    };\n-    auto loadA = [&](int m, int k) {\n-      int offidx = (isARow ? k / 4 : m) % numPtrA;\n-      Value thePtrA = gep(elemPtrTy, smemBase, offA[offidx]);\n-\n-      int stepAM = isARow ? m : m / numPtrA * numPtrA;\n-      int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n-      Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),\n-                         mul(i32_val(stepAK), strideAK));\n-      Value pa = gep(elemPtrTy, thePtrA, offset);\n-      Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n-      Value ha = load(bitcast(pa, aPtrTy));\n-      // record lds that needs to be moved\n-      Value ha00 = bitcast(extract_element(ha, i32_val(0)), elemX2Ty);\n-      Value ha01 = bitcast(extract_element(ha, i32_val(1)), elemX2Ty);\n-      ld(has, m, k, ha00, ha01);\n-\n-      if (vecA > 4) {\n-        Value ha10 = bitcast(extract_element(ha, i32_val(2)), elemX2Ty);\n-        Value ha11 = bitcast(extract_element(ha, i32_val(3)), elemX2Ty);\n-        if (isARow)\n-          ld(has, m, k + 4, ha10, ha11);\n-        else\n-          ld(has, m + 1, k, ha10, ha11);\n-      }\n-    };\n-\n-    unsigned numM = getNumM(shape, order[0] == 1);\n-    for (unsigned k = 0; k < NK; k += 4)\n-      for (unsigned m = 0; m < numM / 2; ++m)\n-        loadA(m, k);\n-\n-    SmallVector<Value> elems;\n-    elems.reserve(has.size() * 2);\n-    for (auto item : has) { // has is a map, the key should be ordered.\n-      elems.push_back(item.second.first);\n-      elems.push_back(item.second.second);\n-    }\n-\n-    Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n-    Value res = getStructFromElements(loc, elems, rewriter, resTy);\n-    return res;\n-  }\n+  Value loadA(Value tensor, const SharedMemoryObject &smemObj, Value thread,\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n \n   // Loading $b from smem to registers, returns a LLVM::Struct.\n-  Value loadB(Value tensor, bool transB, const SharedMemoryObject &smemObj,\n-              Value thread, Location loc,\n-              ConversionPatternRewriter &rewriter) const {\n-    // smem\n-    auto strides = smemObj.strides;\n-\n-    auto *ctx = rewriter.getContext();\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-\n-    SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n-                               tensorTy.getShape().end());\n-    SmallVector<unsigned> order(sharedLayout.getOrder().begin(),\n-                                sharedLayout.getOrder().end());\n-\n-    Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-    bool isBRow = order[0] != 0;\n-    BParam param(isBRow);\n-\n-    int vecB = sharedLayout.getVec();\n-    Value strideBN = isBRow ? i32_val(1) : strides[1];\n-    Value strideBK = isBRow ? strides[0] : i32_val(1);\n-    Value strideB0 = isBRow ? strideBN : strideBK;\n-    Value strideB1 = isBRow ? strideBK : strideBN;\n-    int strideRepN = wpt[1] * fpw[1] * 8;\n-    int strideRepK = 1;\n-\n-    auto [_0, _1, offsetBN, offsetBK] = computeOffsets(\n-        thread, false, isBRow, fpw, param.spw, param.rep, rewriter, loc);\n-    if (transB) {\n-      std::swap(order[0], order[1]);\n-      std::swap(shape[0], shape[1]);\n-      std::swap(offsetBK, offsetBN);\n-    }\n-\n-    // swizzling\n-    int perPhaseB = sharedLayout.getPerPhase();\n-    int maxPhaseB = sharedLayout.getMaxPhase();\n-    int stepB0 = isBRow ? strideRepN : strideRepK;\n-    int numPtrB = std::max(2 * perPhaseB * maxPhaseB / stepB0, 1);\n-    int NK = shape[0];\n-\n-    Value offB0 = isBRow ? offsetBN : offsetBK;\n-    Value offB1 = isBRow ? offsetBK : offsetBN;\n-    Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n-    Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n-\n-    offB0 = add(offB0, cSwizzleOffset);\n-    SmallVector<Value> offB(numPtrB);\n-    for (int i = 0; i < numPtrB; ++i) {\n-      Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));\n-      offB0I = udiv(offB0I, i32_val(vecB));\n-      offB0I = xor_(offB0I, phaseB);\n-      offB0I = mul(offB0I, i32_val(vecB));\n-      offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));\n-    }\n-\n-    Type elemPtrTy = ptr_ty(f16_ty);\n-    Type elemX2Ty = vec_ty(f16_ty, 2);\n-    if (tensorTy.getElementType().isBF16()) {\n-      elemPtrTy = ptr_ty(i16_ty);\n-      elemX2Ty = vec_ty(i16_ty, 2);\n-    }\n-\n-    SmallVector<Value> ptrB(numPtrB);\n-    ValueTable hbs;\n-    for (int i = 0; i < numPtrB; ++i)\n-      ptrB[i] = gep(ptr_ty(f16_ty), smem, offB[i]);\n-\n-    auto ld = [&](decltype(hbs) &vals, int m, int k, Value val0, Value val1) {\n-      vals[{m, k}] = {val0, val1};\n-    };\n-\n-    auto loadB = [&](int n, int K) {\n-      int offidx = (isBRow ? n : K / 4) % numPtrB;\n-      Value thePtrB = ptrB[offidx];\n-\n-      int stepBN = isBRow ? n / numPtrB * numPtrB : n;\n-      int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n-      Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n-                         mul(i32_val(stepBK), strideBK));\n-      Value pb = gep(elemPtrTy, thePtrB, offset);\n-\n-      Value hb =\n-          load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n-      // record lds that needs to be moved\n-      Value hb00 = bitcast(extract_element(hb, i32_val(0)), elemX2Ty);\n-      Value hb01 = bitcast(extract_element(hb, i32_val(1)), elemX2Ty);\n-      ld(hbs, n, K, hb00, hb01);\n-      if (vecB > 4) {\n-        Value hb10 = bitcast(extract_element(hb, i32_val(2)), elemX2Ty);\n-        Value hb11 = bitcast(extract_element(hb, i32_val(3)), elemX2Ty);\n-        if (isBRow)\n-          ld(hbs, n + 1, K, hb10, hb11);\n-        else\n-          ld(hbs, n, K + 4, hb10, hb11);\n-      }\n-    };\n-\n-    unsigned numN = getNumN(shape, order[0] == 1);\n-    for (unsigned k = 0; k < NK; k += 4)\n-      for (unsigned n = 0; n < numN / 2; ++n) {\n-        if (!hbs.count({n, k}))\n-          loadB(n, k);\n-      }\n-\n-    SmallVector<Value> elems;\n-    for (auto &item : hbs) { // has is a map, the key should be ordered.\n-      elems.push_back(item.second.first);\n-      elems.push_back(item.second.second);\n-    }\n-    Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n-    Value res = getStructFromElements(loc, elems, rewriter, resTy);\n-    return res;\n-  }\n+  Value loadB(Value tensor, const SharedMemoryObject &smemObj, Value thread,\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n \n   static ArrayRef<unsigned> getOrder() { return mmaOrder; }\n \n@@ -393,79 +141,32 @@ struct DotOpMmaV1ConversionHelper {\n   std::tuple<Value, Value, Value, Value>\n   computeOffsets(Value threadId, bool isARow, bool isBRow, ArrayRef<int> fpw,\n                  ArrayRef<int> spw, ArrayRef<int> rep,\n-                 ConversionPatternRewriter &rewriter, Location loc) const {\n-    auto *ctx = rewriter.getContext();\n-    Value _1 = i32_val(1);\n-    Value _3 = i32_val(3);\n-    Value _4 = i32_val(4);\n-    Value _16 = i32_val(16);\n-    Value _32 = i32_val(32);\n-\n-    Value lane = urem(threadId, _32);\n-    Value warp = udiv(threadId, _32);\n-\n-    // warp offset\n-    Value warp0 = urem(warp, i32_val(wpt[0]));\n-    Value warp12 = udiv(warp, i32_val(wpt[0]));\n-    Value warp1 = urem(warp12, i32_val(wpt[1]));\n-    Value warpMOff = mul(warp0, i32_val(spw[0]));\n-    Value warpNOff = mul(warp1, i32_val(spw[1]));\n-    // Quad offset\n-    Value quadMOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[0]));\n-    Value quadNOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[1]));\n-    // Pair offset\n-    Value pairMOff = udiv(urem(lane, _16), _4);\n-    pairMOff = urem(pairMOff, i32_val(fpw[0]));\n-    pairMOff = mul(pairMOff, _4);\n-    Value pairNOff = udiv(urem(lane, _16), _4);\n-    pairNOff = udiv(pairNOff, i32_val(fpw[0]));\n-    pairNOff = urem(pairNOff, i32_val(fpw[1]));\n-    pairNOff = mul(pairNOff, _4);\n-    // scale\n-    pairMOff = mul(pairMOff, i32_val(rep[0] / 2));\n-    quadMOff = mul(quadMOff, i32_val(rep[0] / 2));\n-    pairNOff = mul(pairNOff, i32_val(rep[1] / 2));\n-    quadNOff = mul(quadNOff, i32_val(rep[1] / 2));\n-    // Quad pair offset\n-    Value laneMOff = add(pairMOff, quadMOff);\n-    Value laneNOff = add(pairNOff, quadNOff);\n-    // A offset\n-    Value offsetAM = add(warpMOff, laneMOff);\n-    Value offsetAK = and_(lane, _3);\n-    // B offset\n-    Value offsetBN = add(warpNOff, laneNOff);\n-    Value offsetBK = and_(lane, _3);\n-    // i indices\n-    Value offsetCM = add(and_(lane, _1), offsetAM);\n-    if (isARow) {\n-      offsetAM = add(offsetAM, urem(threadId, _4));\n-      offsetAK = i32_val(0);\n-    }\n-    if (!isBRow) {\n-      offsetBN = add(offsetBN, urem(threadId, _4));\n-      offsetBK = i32_val(0);\n-    }\n-\n-    return std::make_tuple(offsetAM, offsetAK, offsetBN, offsetBK);\n-  }\n+                 ConversionPatternRewriter &rewriter, Location loc) const;\n \n   // Extract values belong to $a or $b from a LLVMStruct, the shape is n0xn1.\n   DotOpMmaV1ConversionHelper::ValueTable\n   extractLoadedOperand(Value llStruct, int NK,\n-                       ConversionPatternRewriter &rewriter) const {\n-    ValueTable rcds;\n-    SmallVector<Value> elems =\n-        getElementsFromStruct(llStruct.getLoc(), llStruct, rewriter);\n-\n-    int offset = 0;\n-    for (int i = 0; offset < elems.size(); ++i) {\n-      for (int k = 0; k < NK; k += 4) {\n-        rcds[{i, k}] = std::make_pair(elems[offset], elems[offset + 1]);\n-        offset += 2;\n-      }\n-    }\n-\n-    return rcds;\n+                       ConversionPatternRewriter &rewriter) const;\n+\n+  // Get the number of elements of this thread in M axis. The N axis could be\n+  // further deduced with the accSize / elemsM. \\param wpt: the wpt in M axis\n+  // \\param M: the shape in M axis\n+  int getElemsM(int wpt, int M, bool isARow, bool isAVec4) {\n+    DotOpMmaV1ConversionHelper::AParam param(isARow, isAVec4);\n+    int shapePerCTAM = param.spw[0] * wpt;\n+    return M / shapePerCTAM * param.rep[0];\n+  }\n+\n+  using CoordTy = SmallVector<Value>;\n+  // Get the coordinates(m,n) of the elements emit by a thread in accumulator.\n+  static SmallVector<CoordTy>\n+  getMNCoords(Value thread, ConversionPatternRewriter &rewriter,\n+              ArrayRef<unsigned> wpt, ArrayRef<int64_t> shape, bool isARow,\n+              bool isBRow, bool isAVec4, bool isBVec4);\n+\n+  // \\param elemId the offset of the element in a thread\n+  static CoordTy getCoord(int elemId, ArrayRef<CoordTy> coords) {\n+    return coords[elemId];\n   }\n \n private:\n@@ -508,107 +209,16 @@ struct DotOpMmaV2ConversionHelper {\n     return {16, 8};\n   }\n \n-  static std::tuple<int, int> getRepMN(const RankedTensorType &tensorTy) {\n-    auto mmaLayout = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n-    auto wpt = mmaLayout.getWarpsPerCTA();\n+  static std::tuple<int, int> getRepMN(const RankedTensorType &tensorTy);\n \n-    int M = tensorTy.getShape()[0];\n-    int N = tensorTy.getShape()[1];\n-    auto [instrM, instrN] = getInstrShapeMN();\n-    int repM = std::max<int>(M / (wpt[0] * instrM), 1);\n-    int repN = std::max<int>(N / (wpt[1] * instrN), 1);\n-    return {repM, repN};\n-  }\n-\n-  Type getShemPtrTy() const {\n-    switch (mmaType) {\n-    case TensorCoreType::FP32_FP16_FP16_FP32:\n-      return ptr_ty(type::f16Ty(ctx), 3);\n-    case TensorCoreType::FP32_BF16_BF16_FP32:\n-      return ptr_ty(type::i16Ty(ctx), 3);\n-    case TensorCoreType::FP32_TF32_TF32_FP32:\n-      return ptr_ty(type::f32Ty(ctx), 3);\n-    case TensorCoreType::INT32_INT8_INT8_INT32:\n-      return ptr_ty(type::i8Ty(ctx), 3);\n-    default:\n-      llvm::report_fatal_error(\"mma16816 data type not supported\");\n-    }\n-    return Type{};\n-  }\n+  Type getShemPtrTy() const;\n \n   // The type of matrix that loaded by either a ldmatrix or composed lds.\n-  Type getMatType() const {\n-    Type fp32Ty = type::f32Ty(ctx);\n-    Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n-    Type i16x2Ty = vec_ty(type::i16Ty(ctx), 2);\n-    // floating point types\n-    Type fp16x2Pack4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n-    // LLVM 14.0 does not support bf16 type, so we use i16 instead.\n-    Type bf16x2Pack4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i16x2Ty));\n-    Type fp32Pack4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n-    // integer types\n-    Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n-    Type i8x4Pack4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i8x4Ty));\n-\n-    switch (mmaType) {\n-    case TensorCoreType::FP32_FP16_FP16_FP32:\n-      return fp16x2Pack4Ty;\n-    case TensorCoreType::FP32_BF16_BF16_FP32:\n-      return bf16x2Pack4Ty;\n-    case TensorCoreType::FP32_TF32_TF32_FP32:\n-      return fp32Pack4Ty;\n-    case TensorCoreType::INT32_INT8_INT8_INT32:\n-      return i8x4Pack4Ty;\n-    default:\n-      llvm::report_fatal_error(\"Unsupported mma type found\");\n-    }\n-\n-    return Type{};\n-  }\n+  Type getMatType() const;\n \n-  Type getLoadElemTy() {\n-    switch (mmaType) {\n-    case TensorCoreType::FP32_FP16_FP16_FP32:\n-      return vec_ty(type::f16Ty(ctx), 2);\n-    case TensorCoreType::FP32_BF16_BF16_FP32:\n-      return vec_ty(type::bf16Ty(ctx), 2);\n-    case TensorCoreType::FP32_TF32_TF32_FP32:\n-      return type::f32Ty(ctx);\n-    case TensorCoreType::INT32_INT8_INT8_INT32:\n-      return type::i32Ty(ctx);\n-    default:\n-      llvm::report_fatal_error(\"Unsupported mma type found\");\n-    }\n-\n-    return Type{};\n-  }\n+  Type getLoadElemTy();\n \n-  Type getMmaRetType() const {\n-    Type fp32Ty = type::f32Ty(ctx);\n-    Type i32Ty = type::i32Ty(ctx);\n-    Type fp32x4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n-    Type i32x4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i32Ty));\n-    switch (mmaType) {\n-    case TensorCoreType::FP32_FP16_FP16_FP32:\n-      return fp32x4Ty;\n-    case TensorCoreType::FP32_BF16_BF16_FP32:\n-      return fp32x4Ty;\n-    case TensorCoreType::FP32_TF32_TF32_FP32:\n-      return fp32x4Ty;\n-    case TensorCoreType::INT32_INT8_INT8_INT32:\n-      return i32x4Ty;\n-    default:\n-      llvm::report_fatal_error(\"Unsupported mma type found\");\n-    }\n-\n-    return Type{};\n-  }\n+  Type getMmaRetType() const;\n \n   ArrayRef<int> getMmaInstrShape() const {\n     assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n@@ -629,19 +239,7 @@ struct DotOpMmaV2ConversionHelper {\n   }\n \n   // Deduce the TensorCoreType from either $a or $b's type.\n-  static TensorCoreType getTensorCoreTypeFromOperand(Type operandTy) {\n-    auto tensorTy = operandTy.cast<RankedTensorType>();\n-    auto elemTy = tensorTy.getElementType();\n-    if (elemTy.isF16())\n-      return TensorCoreType::FP32_FP16_FP16_FP32;\n-    if (elemTy.isF32())\n-      return TensorCoreType::FP32_TF32_TF32_FP32;\n-    if (elemTy.isBF16())\n-      return TensorCoreType::FP32_BF16_BF16_FP32;\n-    if (elemTy.isInteger(8))\n-      return TensorCoreType::INT32_INT8_INT8_INT32;\n-    return TensorCoreType::NOT_APPLICABLE;\n-  }\n+  static TensorCoreType getTensorCoreTypeFromOperand(Type operandTy);\n \n   int getVec() const {\n     assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n@@ -655,30 +253,7 @@ struct DotOpMmaV2ConversionHelper {\n     return mmaInstrPtx.at(mmaType);\n   }\n \n-  static TensorCoreType getMmaType(triton::DotOp op) {\n-    Value A = op.a();\n-    Value B = op.b();\n-    auto aTy = A.getType().cast<RankedTensorType>();\n-    auto bTy = B.getType().cast<RankedTensorType>();\n-    // d = a*b + c\n-    auto dTy = op.d().getType().cast<RankedTensorType>();\n-\n-    if (dTy.getElementType().isF32()) {\n-      if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n-        return TensorCoreType::FP32_FP16_FP16_FP32;\n-      if (aTy.getElementType().isBF16() && bTy.getElementType().isBF16())\n-        return TensorCoreType::FP32_BF16_BF16_FP32;\n-      if (aTy.getElementType().isF32() && bTy.getElementType().isF32() &&\n-          op.allowTF32())\n-        return TensorCoreType::FP32_TF32_TF32_FP32;\n-    } else if (dTy.getElementType().isInteger(32)) {\n-      if (aTy.getElementType().isInteger(8) &&\n-          bTy.getElementType().isInteger(8))\n-        return TensorCoreType::INT32_INT8_INT8_INT32;\n-    }\n-\n-    return TensorCoreType::NOT_APPLICABLE;\n-  }\n+  static TensorCoreType getMmaType(triton::DotOp op);\n \n private:\n   mutable TensorCoreType mmaType{TensorCoreType::NOT_APPLICABLE};\n@@ -753,50 +328,7 @@ class MMA16816SmemLoader {\n                      ArrayRef<int> instrShape, ArrayRef<int> matShape,\n                      int perPhase, int maxPhase, int elemBytes,\n                      ConversionPatternRewriter &rewriter,\n-                     TypeConverter *typeConverter, const Location &loc)\n-      : order(order.begin(), order.end()), kOrder(kOrder),\n-        tileShape(tileShape.begin(), tileShape.end()),\n-        instrShape(instrShape.begin(), instrShape.end()),\n-        matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n-        maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter), loc(loc),\n-        ctx(rewriter.getContext()) {\n-    cMatShape = matShape[order[0]];\n-    sMatShape = matShape[order[1]];\n-\n-    sStride = smemStrides[order[1]];\n-\n-    // rule: k must be the fast-changing axis.\n-    needTrans = kOrder != order[0];\n-    canUseLdmatrix = elemBytes == 2 || (!needTrans); // b16\n-\n-    if (canUseLdmatrix) {\n-      // Each CTA, the warps is arranged as [1xwpt] if not transposed,\n-      // otherwise [wptx1], and each warp will perform a mma.\n-      numPtrs =\n-          tileShape[order[0]] / (needTrans ? wpt : 1) / instrShape[order[0]];\n-    } else {\n-      numPtrs = tileShape[order[0]] / wpt / matShape[order[0]];\n-    }\n-    numPtrs = std::max<int>(numPtrs, 2);\n-\n-    // Special rule for i8/u8, 4 ptrs for each matrix\n-    if (!canUseLdmatrix && elemBytes == 1)\n-      numPtrs *= 4;\n-\n-    int loadStrideInMat[2];\n-    loadStrideInMat[kOrder] =\n-        2; // instrShape[kOrder] / matShape[kOrder], always 2\n-    loadStrideInMat[kOrder ^ 1] =\n-        wpt * (instrShape[kOrder ^ 1] / matShape[kOrder ^ 1]);\n-\n-    pLoadStrideInMat = loadStrideInMat[order[0]];\n-    sMatStride =\n-        loadStrideInMat[order[1]] / (instrShape[order[1]] / matShape[order[1]]);\n-\n-    // Each matArr contains warpOffStride matrices.\n-    matArrStride = kOrder == 1 ? 1 : wpt;\n-    warpOffStride = instrShape[kOrder ^ 1] / matShape[kOrder ^ 1];\n-  }\n+                     TypeConverter *typeConverter, const Location &loc);\n \n   // lane = thread % 32\n   // warpOff = (thread/32) % wpt(0)\n@@ -819,299 +351,20 @@ class MMA16816SmemLoader {\n   // Compute the offset to the matrix this thread(indexed by warpOff and lane)\n   // mapped to.\n   SmallVector<Value> computeLdmatrixMatOffs(Value warpId, Value lane,\n-                                            Value cSwizzleOffset) {\n-    // 4x4 matrices\n-    Value c = urem(lane, i32_val(8));\n-    Value s = udiv(lane, i32_val(8)); // sub-warp-id\n-\n-    // Decompose s => s_0, s_1, that is the coordinate in 2x2 matrices in a\n-    // warp\n-    Value s0 = urem(s, i32_val(2));\n-    Value s1 = udiv(s, i32_val(2));\n-\n-    // We use different orders for a and b for better performance.\n-    Value kMatArr = kOrder == 1 ? s1 : s0;\n-    Value nkMatArr = kOrder == 1 ? s0 : s1;\n-\n-    // matrix coordinate inside a CTA, the matrix layout is [2x2wpt] for A and\n-    // [2wptx2] for B. e.g. Setting wpt=3, The data layout for A(kOrder=1) is\n-    //   |0 0 1 1 2 2| -> 0,1,2 are the warpids\n-    //   |0 0 1 1 2 2|\n-    //\n-    // for B(kOrder=0) is\n-    //   |0 0|  -> 0,1,2 are the warpids\n-    //   |1 1|\n-    //   |2 2|\n-    //   |0 0|\n-    //   |1 1|\n-    //   |2 2|\n-    // Note, for each warp, it handles a 2x2 matrices, that is the coordinate\n-    // address (s0,s1) annotates.\n-\n-    Value matOff[2];\n-    matOff[kOrder ^ 1] = add(\n-        mul(warpId, i32_val(warpOffStride)),   // warp offset\n-        mul(nkMatArr, i32_val(matArrStride))); // matrix offset inside a warp\n-    matOff[kOrder] = kMatArr;\n-\n-    // Physical offset (before swizzling)\n-    Value cMatOff = matOff[order[0]];\n-    Value sMatOff = matOff[order[1]];\n-    Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n-    cMatOff = add(cMatOff, cSwizzleMatOff);\n-\n-    // row offset inside a matrix, each matrix has 8 rows.\n-    Value sOffInMat = c;\n-\n-    SmallVector<Value> offs(numPtrs);\n-    Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n-    Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n-    for (int i = 0; i < numPtrs; ++i) {\n-      Value cMatOffI = add(cMatOff, i32_val(i * pLoadStrideInMat));\n-      cMatOffI = xor_(cMatOffI, phase);\n-      offs[i] = add(mul(cMatOffI, i32_val(cMatShape)), mul(sOff, sStride));\n-    }\n-\n-    return offs;\n-  }\n+                                            Value cSwizzleOffset);\n \n   // Compute 32-bit matrix offsets.\n   SmallVector<Value> computeB32MatOffs(Value warpOff, Value lane,\n-                                       Value cSwizzleOffset) {\n-    assert(needTrans && \"Only used in transpose mode.\");\n-    // Load tf32 matrices with lds32\n-    Value cOffInMat = udiv(lane, i32_val(4));\n-    Value sOffInMat = urem(lane, i32_val(4));\n-\n-    Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n-    SmallVector<Value> offs(numPtrs);\n-\n-    for (int mat = 0; mat < 4; ++mat) { // Load 4 mats each time\n-      int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n-      int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n-      if (kMatArrInt > 0) // we don't need pointers for k\n-        continue;\n-      Value kMatArr = i32_val(kMatArrInt);\n-      Value nkMatArr = i32_val(nkMatArrInt);\n-\n-      Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n-                          mul(nkMatArr, i32_val(matArrStride)));\n-      Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n-      cMatOff = add(cMatOff, cSwizzleMatOff);\n-\n-      Value sMatOff = kMatArr;\n-      Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n-      // FIXME: (kOrder == 1?) is really dirty hack\n-      for (int i = 0; i < numPtrs / 2; ++i) {\n-        Value cMatOffI =\n-            add(cMatOff, i32_val(i * pLoadStrideInMat * (kOrder == 1 ? 1 : 2)));\n-        cMatOffI = xor_(cMatOffI, phase);\n-        Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n-        cOff = urem(cOff, i32_val(tileShape[order[0]]));\n-        sOff = urem(sOff, i32_val(tileShape[order[1]]));\n-        offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, sStride));\n-      }\n-    }\n-    return offs;\n-  }\n+                                       Value cSwizzleOffset);\n \n   // compute 8-bit matrix offset.\n   SmallVector<Value> computeB8MatOffs(Value warpOff, Value lane,\n-                                      Value cSwizzleOffset) {\n-    assert(needTrans && \"Only used in transpose mode.\");\n-    Value cOffInMat = udiv(lane, i32_val(4));\n-    Value sOffInMat =\n-        mul(urem(lane, i32_val(4)), i32_val(4)); // each thread load 4 cols\n-\n-    SmallVector<Value> offs(numPtrs);\n-    for (int mat = 0; mat < 4; ++mat) {\n-      int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n-      int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n-      if (kMatArrInt > 0) // we don't need pointers for k\n-        continue;\n-      Value kMatArr = i32_val(kMatArrInt);\n-      Value nkMatArr = i32_val(nkMatArrInt);\n-\n-      Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n-                          mul(nkMatArr, i32_val(matArrStride)));\n-      Value sMatOff = kMatArr;\n-\n-      for (int loadx4Off = 0; loadx4Off < numPtrs / 8; ++loadx4Off) {\n-        for (int elemOff = 0; elemOff < 4; ++elemOff) {\n-          int ptrOff = loadx4Off * 8 + nkMatArrInt * 4 + elemOff;\n-          Value cMatOffI = add(cMatOff, i32_val(loadx4Off * pLoadStrideInMat *\n-                                                (kOrder == 1 ? 1 : 2)));\n-          Value sOffInMatElem = add(sOffInMat, i32_val(elemOff));\n-\n-          // disable swizzling ...\n-\n-          Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n-          Value sOff = add(sOffInMatElem, mul(sMatOff, i32_val(sMatShape)));\n-          // To prevent out-of-bound access when tile is too small.\n-          cOff = urem(cOff, i32_val(tileShape[order[0]]));\n-          sOff = urem(sOff, i32_val(tileShape[order[1]]));\n-          offs[ptrOff] = add(cOff, mul(sOff, sStride));\n-        }\n-      }\n-    }\n-    return offs;\n-  }\n+                                      Value cSwizzleOffset);\n \n   // Load 4 matrices and returns 4 vec<2> elements.\n   std::tuple<Value, Value, Value, Value>\n   loadX4(int mat0, int mat1, ArrayRef<Value> offs, ArrayRef<Value> ptrs,\n-         Type ldmatrixRetTy, Type shemPtrTy) const {\n-    assert(mat0 % 2 == 0 && mat1 % 2 == 0 &&\n-           \"smem matrix load must be aligned\");\n-    int matIdx[2] = {mat0, mat1};\n-\n-    int ptrIdx{-1};\n-\n-    if (canUseLdmatrix)\n-      ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n-    else if (elemBytes == 4 && needTrans)\n-      ptrIdx = matIdx[order[0]];\n-    else if (elemBytes == 1 && needTrans)\n-      ptrIdx = matIdx[order[0]] * 4;\n-    else\n-      llvm::report_fatal_error(\"unsupported mma type found\");\n-\n-    // The main difference with the original triton code is we removed the\n-    // prefetch-related logic here for the upstream optimizer phase should\n-    // take care with it, and that is transparent in dot conversion.\n-    auto getPtr = [&](int idx) { return ptrs[idx]; };\n-\n-    Value ptr = getPtr(ptrIdx);\n-\n-    if (canUseLdmatrix) {\n-      Value sOffset =\n-          mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sStride);\n-      Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n-\n-      PTXBuilder builder;\n-      // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n-      // thread.\n-      auto resArgs = builder.newListOperand(4, \"=r\");\n-      auto addrArg = builder.newAddrOperand(sOffsetPtr, \"r\");\n-\n-      auto ldmatrix = builder.create(\"ldmatrix.sync.aligned.m8n8.x4\")\n-                          ->o(\"trans\", needTrans /*predicate*/)\n-                          .o(\"shared.b16\");\n-      ldmatrix(resArgs, addrArg);\n-\n-      // The result type is 4xi32, each i32 is composed of 2xf16\n-      // elements(adjacent two columns in a row)\n-      Value resV4 = builder.launch(rewriter, loc, ldmatrixRetTy);\n-\n-      auto getIntAttr = [&](int v) {\n-        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n-      };\n-\n-      // The struct should have exactly the same element types.\n-      Type elemType = resV4.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n-\n-      return {extract_val(elemType, resV4, getIntAttr(0)),\n-              extract_val(elemType, resV4, getIntAttr(1)),\n-              extract_val(elemType, resV4, getIntAttr(2)),\n-              extract_val(elemType, resV4, getIntAttr(3))};\n-    } else if (elemBytes == 4 &&\n-               needTrans) { // Use lds.32 to load tf32 matrices\n-      Value ptr2 = getPtr(ptrIdx + 1);\n-      assert(sMatStride == 1);\n-      int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n-      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n-      int sOffsetArrElem = sMatStride * sMatShape;\n-      Value sOffsetArrElemVal =\n-          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n-\n-      Value elems[4];\n-      Type elemTy = type::f32Ty(ctx);\n-      Type elemPtrTy = ptr_ty(elemTy);\n-      if (kOrder == 1) {\n-        elems[0] = load(gep(elemPtrTy, ptr, sOffsetElemVal));\n-        elems[1] = load(gep(elemPtrTy, ptr2, sOffsetElemVal));\n-        elems[2] = load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n-        elems[3] = load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n-      } else {\n-        elems[0] = load(gep(elemPtrTy, ptr, sOffsetElemVal));\n-        elems[2] = load(gep(elemPtrTy, ptr2, sOffsetElemVal));\n-        elems[1] = load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n-        elems[3] = load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n-      }\n-      return {elems[0], elems[1], elems[2], elems[3]};\n-\n-    } else if (elemBytes == 1 && needTrans) { // work with int8\n-      std::array<std::array<Value, 4>, 2> ptrs;\n-      ptrs[0] = {\n-          getPtr(ptrIdx),\n-          getPtr(ptrIdx + 1),\n-          getPtr(ptrIdx + 2),\n-          getPtr(ptrIdx + 3),\n-      };\n-\n-      ptrs[1] = {\n-          getPtr(ptrIdx + 4),\n-          getPtr(ptrIdx + 5),\n-          getPtr(ptrIdx + 6),\n-          getPtr(ptrIdx + 7),\n-      };\n-\n-      assert(sMatStride == 1);\n-      int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n-      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n-      int sOffsetArrElem = 1 * (sMatStride * sMatShape);\n-      Value sOffsetArrElemVal =\n-          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n-\n-      std::array<Value, 4> i8v4Elems;\n-      std::array<Value, 4> i32Elems;\n-      i8v4Elems.fill(\n-          rewriter.create<LLVM::UndefOp>(loc, vec_ty(type::i8Ty(ctx), 4)));\n-\n-      Value i8Elems[4][4];\n-      Type elemTy = type::i8Ty(ctx);\n-      Type elemPtrTy = ptr_ty(elemTy);\n-      Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n-      if (kOrder == 1) {\n-        for (int i = 0; i < 2; ++i)\n-          for (int j = 0; j < 4; ++j)\n-            i8Elems[i][j] = load(gep(elemPtrTy, ptrs[i][j], sOffsetElemVal));\n-\n-        for (int i = 2; i < 4; ++i)\n-          for (int j = 0; j < 4; ++j)\n-            i8Elems[i][j] =\n-                load(gep(elemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n-\n-        for (int m = 0; m < 4; ++m) {\n-          for (int e = 0; e < 4; ++e)\n-            i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n-                                          i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bitcast(i8v4Elems[m], i8x4Ty);\n-        }\n-      } else { // k first\n-        for (int j = 0; j < 4; ++j)\n-          i8Elems[0][j] = load(gep(elemPtrTy, ptrs[0][j], sOffsetElemVal));\n-        for (int j = 0; j < 4; ++j)\n-          i8Elems[2][j] = load(gep(elemPtrTy, ptrs[1][j], sOffsetElemVal));\n-        for (int j = 0; j < 4; ++j)\n-          i8Elems[1][j] = load(gep(elemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n-        for (int j = 0; j < 4; ++j)\n-          i8Elems[3][j] = load(gep(elemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n-\n-        for (int m = 0; m < 4; ++m) {\n-          for (int e = 0; e < 4; ++e)\n-            i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n-                                          i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bitcast(i8v4Elems[m], i8x4Ty);\n-        }\n-      }\n-\n-      return {i32Elems[0], i32Elems[1], i32Elems[2], i32Elems[3]};\n-    }\n-\n-    assert(false && \"Invalid smem load\");\n-    return {Value{}, Value{}, Value{}, Value{}};\n-  }\n+         Type matTy, Type shemPtrTy) const;\n \n private:\n   SmallVector<uint32_t> order;\n@@ -1167,9 +420,9 @@ struct MMA16816ConversionHelper {\n   MMA16816ConversionHelper(Type dotOperand, MmaEncodingAttr mmaLayout,\n                            Value thread, ConversionPatternRewriter &rewriter,\n                            TypeConverter *typeConverter, Location loc)\n-      : mmaLayout(mmaLayout), thread(thread), helper(mmaLayout),\n-        rewriter(rewriter), typeConverter(typeConverter), loc(loc),\n-        ctx(mmaLayout.getContext()), wpt(mmaLayout.getWarpsPerCTA()) {\n+      : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()), thread(thread),\n+        helper(mmaLayout), rewriter(rewriter), typeConverter(typeConverter),\n+        loc(loc), ctx(mmaLayout.getContext()) {\n     helper.deduceMmaType(dotOperand);\n \n     Value _32 = i32_val(32);\n@@ -1179,15 +432,15 @@ struct MMA16816ConversionHelper {\n \n   // Get a warpId for M axis.\n   Value getWarpM(int M) const {\n-    auto matShape = helper.getMmaMatShape();\n-    return urem(urem(warp, i32_val(wpt[0])), i32_val(M / matShape[0]));\n+    auto matInstrShape = helper.getMmaInstrShape();\n+    return urem(urem(warp, i32_val(wpt[0])), i32_val(M / matInstrShape[0]));\n   }\n \n   // Get a warpId for N axis.\n   Value getWarpN(int N) const {\n-    auto matShape = helper.getMmaMatShape();\n+    auto matInstrShape = helper.getMmaInstrShape();\n     Value warpMN = udiv(warp, i32_val(wpt[0]));\n-    return urem(urem(warpMN, i32_val(wpt[1])), i32_val(N / matShape[1]));\n+    return urem(urem(warpMN, i32_val(wpt[1])), i32_val(N / matInstrShape[1]));\n   }\n \n   // Get the mmaInstrShape deducing either from $a or $b.\n@@ -1266,242 +519,28 @@ struct MMA16816ConversionHelper {\n   }\n \n   // Loading $a from smem to registers, returns a LLVM::Struct.\n-  Value loadA(Value tensor, const SharedMemoryObject &smemObj) const {\n-    auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n-\n-    SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n-                               aTensorTy.getShape().end());\n-\n-    ValueTable ha;\n-    std::function<void(int, int)> loadFn;\n-    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n-    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(aTensorTy);\n-\n-    int numRepM = getNumRepM(aTensorTy, shape[0]);\n-    int numRepK = getNumRepK(aTensorTy, shape[1]);\n-\n-    if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n-      Value warpM = getWarpM(shape[0]);\n-      // load from smem\n-      // we use ldmatrix.x4 so each warp processes 16x16 elements.\n-      int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / 16);\n-      loadFn =\n-          getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/,\n-                          {mmaInstrM, mmaInstrK} /*instrShape*/,\n-                          {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/,\n-                          ha /*vals*/, true /*isA*/);\n-    } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n-      // load from registers, used in gemm fuse\n-      // TODO(Superjomn) Port the logic.\n-      assert(false && \"Loading A from register is not supported yet.\");\n-    } else {\n-      assert(false && \"A's layout is not supported.\");\n-    }\n-\n-    // step1. Perform loading.\n-    for (int m = 0; m < numRepM; ++m)\n-      for (int k = 0; k < numRepK; ++k)\n-        loadFn(2 * m, 2 * k);\n-\n-    // step2. Format the values to LLVM::Struct to passing to mma codegen.\n-    return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n-  }\n+  Value loadA(Value tensor, const SharedMemoryObject &smemObj) const;\n \n   // Loading $b from smem to registers, returns a LLVM::Struct.\n-  Value loadB(Value tensor, const SharedMemoryObject &smemObj) {\n-    ValueTable hb;\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-\n-    SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n-                               tensorTy.getShape().end());\n-\n-    // TODO[Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n-    bool transB = false;\n-    if (transB) {\n-      std::swap(shape[0], shape[1]);\n-    }\n-\n-    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(tensorTy);\n-    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(tensorTy);\n-    int numRepK = getNumRepK(tensorTy, shape[0]);\n-    int numRepN = getNumRepN(tensorTy, shape[1]);\n-\n-    Value warpN = getWarpN(shape[1]);\n-    // we use ldmatrix.x4 so each warp processes 16x16 elements.\n-    int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / 16);\n-    auto loadFn =\n-        getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/,\n-                        {mmaInstrK, mmaInstrN} /*instrShape*/,\n-                        {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/,\n-                        hb /*vals*/, false /*isA*/);\n-\n-    for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n-      for (int k = 0; k < numRepK; ++k)\n-        loadFn(2 * n, 2 * k);\n-    }\n-\n-    Value result = composeValuesToDotOperandLayoutStruct(\n-        hb, std::max(numRepN / 2, 1), numRepK);\n-    return result;\n-  }\n+  Value loadB(Value tensor, const SharedMemoryObject &smemObj);\n \n   // Loading $c to registers, returns a Value.\n-  Value loadC(Value tensor, Value llTensor) const {\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(tensorTy);\n-    size_t fcSize = 4 * repM * repN;\n-\n-    assert(tensorTy.getEncoding().isa<MmaEncodingAttr>() &&\n-           \"Currently, we only support $c with a mma layout.\");\n-    // Load a normal C tensor with mma layout, that should be a\n-    // LLVM::struct with fcSize elements.\n-    auto structTy = llTensor.getType().cast<LLVM::LLVMStructType>();\n-    assert(structTy.getBody().size() == fcSize &&\n-           \"DotOp's $c operand should pass the same number of values as $d in \"\n-           \"mma layout.\");\n-    return llTensor;\n-  }\n+  Value loadC(Value tensor, Value llTensor) const;\n \n   // Conduct the Dot conversion.\n   // \\param a, \\param b, \\param c and \\param d are DotOp operands.\n   // \\param loadedA, \\param loadedB, \\param loadedC, all of them are result of\n   // loading.\n   LogicalResult convertDot(Value a, Value b, Value c, Value d, Value loadedA,\n                            Value loadedB, Value loadedC, DotOp op,\n-                           DotOpAdaptor adaptor) const {\n-    helper.deduceMmaType(op);\n-\n-    auto aTensorTy = a.getType().cast<RankedTensorType>();\n-    auto dTensorTy = d.getType().cast<RankedTensorType>();\n-\n-    SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n-                                aTensorTy.getShape().end());\n-\n-    auto dShape = dTensorTy.getShape();\n-\n-    // shape / shape_per_cta\n-    int numRepM = getNumRepM(aTensorTy, dShape[0]);\n-    int numRepN = getNumRepN(aTensorTy, dShape[1]);\n-    int numRepK = getNumRepK(aTensorTy, aShape[1]);\n-\n-    ValueTable ha =\n-        getValuesFromDotOperandLayoutStruct(loadedA, numRepM, numRepK);\n-    ValueTable hb = getValuesFromDotOperandLayoutStruct(\n-        loadedB, std::max(numRepN / 2, 1), numRepK);\n-    auto fc = getElementsFromStruct(loc, loadedC, rewriter);\n-\n-    auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n-      unsigned colsPerThread = numRepN * 2;\n-      PTXBuilder builder;\n-      auto &mma = *builder.create(helper.getMmaInstr().str());\n-      auto retArgs = builder.newListOperand(4, \"=r\");\n-      auto aArgs = builder.newListOperand({\n-          {ha[{m, k}], \"r\"},\n-          {ha[{m + 1, k}], \"r\"},\n-          {ha[{m, k + 1}], \"r\"},\n-          {ha[{m + 1, k + 1}], \"r\"},\n-      });\n-      auto bArgs =\n-          builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n-      auto cArgs = builder.newListOperand();\n-      for (int i = 0; i < 4; ++i) {\n-        cArgs->listAppend(builder.newOperand(fc[m * colsPerThread + 4 * n + i],\n-                                             std::to_string(i)));\n-        // reuse the output registers\n-      }\n-\n-      mma(retArgs, aArgs, bArgs, cArgs);\n-      Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n-\n-      auto getIntAttr = [&](int v) {\n-        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n-      };\n-\n-      Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n-      for (int i = 0; i < 4; ++i)\n-        fc[m * colsPerThread + 4 * n + i] =\n-            extract_val(elemTy, mmaOut, getIntAttr(i));\n-    };\n-\n-    for (int k = 0; k < numRepK; ++k)\n-      for (int m = 0; m < numRepM; ++m)\n-        for (int n = 0; n < numRepN; ++n)\n-          callMma(2 * m, n, 2 * k);\n-\n-    Type resElemTy = dTensorTy.getElementType();\n-\n-    for (auto &elem : fc) {\n-      elem = bitcast(elem, resElemTy);\n-    }\n-\n-    // replace with new packed result\n-    Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(fc.size(), resElemTy));\n-    Value res = getStructFromElements(loc, fc, rewriter, structTy);\n-    rewriter.replaceOp(op, res);\n-\n-    return success();\n-  }\n+                           DotOpAdaptor adaptor) const;\n \n private:\n   std::function<void(int, int)>\n   getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n                   MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder,\n                   SmallVector<int> instrShape, SmallVector<int> matShape,\n-                  Value warpId, ValueTable &vals, bool isA) const {\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    // We assumes that the input operand of Dot should be from shared layout.\n-    // TODO(Superjomn) Consider other layouts if needed later.\n-    auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-    const int perPhase = sharedLayout.getPerPhase();\n-    const int maxPhase = sharedLayout.getMaxPhase();\n-    const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n-    auto order = sharedLayout.getOrder();\n-\n-    // the original register_lds2, but discard the prefetch logic.\n-    auto ld2 = [](ValueTable &vals, int mn, int k, Value val) {\n-      vals[{mn, k}] = val;\n-    };\n-\n-    // (a, b) is the coordinate.\n-    auto load = [=, &vals, &ld2](int a, int b) {\n-      MMA16816SmemLoader loader(\n-          wpt, sharedLayout.getOrder(), kOrder, smemObj.strides,\n-          tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n-          maxPhase, elemBytes, rewriter, typeConverter, loc);\n-      Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n-      SmallVector<Value> offs =\n-          loader.computeOffsets(warpId, lane, cSwizzleOffset);\n-      const int numPtrs = loader.getNumPtrs();\n-      SmallVector<Value> ptrs(numPtrs);\n-\n-      Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-\n-      Type smemPtrTy = helper.getShemPtrTy();\n-      for (int i = 0; i < numPtrs; ++i) {\n-        ptrs[i] =\n-            bitcast(gep(smemPtrTy, smemBase, ValueRange({offs[i]})), smemPtrTy);\n-      }\n-\n-      auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n-          (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n-          ptrs, helper.getMatType(), helper.getShemPtrTy());\n-\n-      if (isA) {\n-        ld2(vals, a, b, ha0);\n-        ld2(vals, a + 1, b, ha1);\n-        ld2(vals, a, b + 1, ha2);\n-        ld2(vals, a + 1, b + 1, ha3);\n-      } else {\n-        ld2(vals, a, b, ha0);\n-        ld2(vals, a + 1, b, ha2);\n-        ld2(vals, a, b + 1, ha1);\n-        ld2(vals, a + 1, b + 1, ha3);\n-      }\n-    };\n-\n-    return load;\n-  }\n+                  Value warpId, ValueTable &vals, bool isA) const;\n \n   // Compose a map of Values to a LLVM::Struct.\n   // The layout is a list of Value with coordinate of (i,j), the order is as\n@@ -1519,41 +558,10 @@ struct MMA16816ConversionHelper {\n   // i \\in [0, n0) and j \\in [0, n1)\n   // There should be \\param n0 * \\param n1 elements in the output Struct.\n   Value composeValuesToDotOperandLayoutStruct(const ValueTable &vals, int n0,\n-                                              int n1) const {\n-    std::vector<Value> elems;\n-    for (int m = 0; m < n0; ++m)\n-      for (int k = 0; k < n1; ++k) {\n-        elems.push_back(vals.at({2 * m, 2 * k}));\n-        elems.push_back(vals.at({2 * m, 2 * k + 1}));\n-        elems.push_back(vals.at({2 * m + 1, 2 * k}));\n-        elems.push_back(vals.at({2 * m + 1, 2 * k + 1}));\n-      }\n-\n-    assert(!elems.empty());\n-\n-    Type elemTy = elems[0].getType();\n-    Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(elems.size(), elemTy));\n-    auto result = getStructFromElements(loc, elems, rewriter, structTy);\n-    return result;\n-  }\n+                                              int n1) const;\n \n   ValueTable getValuesFromDotOperandLayoutStruct(Value value, int n0,\n-                                                 int n1) const {\n-    auto elems = getElementsFromStruct(loc, value, rewriter);\n-\n-    int offset{};\n-    ValueTable vals;\n-    for (int i = 0; i < n0; ++i) {\n-      for (int j = 0; j < n1; j++) {\n-        vals[{2 * i, 2 * j}] = elems[offset++];\n-        vals[{2 * i, 2 * j + 1}] = elems[offset++];\n-        vals[{2 * i + 1, 2 * j}] = elems[offset++];\n-        vals[{2 * i + 1, 2 * j + 1}] = elems[offset++];\n-      }\n-    }\n-    return vals;\n-  }\n+                                                 int n1) const;\n };\n \n // Helper for conversion of FMA DotOp.\n@@ -1569,193 +577,26 @@ struct DotOpFMAConversionHelper {\n   SmallVector<Value>\n   getThreadIds(Value threadId, ArrayRef<unsigned> shapePerCTA,\n                ArrayRef<unsigned> sizePerThread, ArrayRef<unsigned> order,\n-               ConversionPatternRewriter &rewriter, Location loc) const {\n-    int dim = order.size();\n-    SmallVector<Value> threadIds(dim);\n-    for (unsigned k = 0; k < dim - 1; k++) {\n-      Value dimK = i32_val(shapePerCTA[order[k]] / sizePerThread[order[k]]);\n-      Value rem = urem(threadId, dimK);\n-      threadId = udiv(threadId, dimK);\n-      threadIds[order[k]] = rem;\n-    }\n-    Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n-    threadIds[order[dim - 1]] = urem(threadId, dimK);\n-    return threadIds;\n-  }\n+               ConversionPatternRewriter &rewriter, Location loc) const;\n \n   Value loadA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const {\n-    auto aTensorTy = A.getType().cast<RankedTensorType>();\n-    auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-    auto aShape = aTensorTy.getShape();\n-\n-    auto aOrder = aLayout.getOrder();\n-    auto order = dLayout.getOrder();\n-\n-    bool isARow = aOrder[0] == 1;\n-\n-    auto aSmem = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n-    Value strideAM = aSmem.strides[0];\n-    Value strideAK = aSmem.strides[1];\n-    Value strideA0 = isARow ? strideAK : strideAM;\n-    Value strideA1 = isARow ? strideAM : strideAK;\n-    int aNumPtr = 8;\n-    int K = aShape[1];\n-    int M = aShape[0];\n-\n-    auto shapePerCTA = getShapePerCTA(dLayout);\n-    auto sizePerThread = getSizePerThread(dLayout);\n-\n-    Value _0 = i32_val(0);\n-\n-    Value mContig = i32_val(sizePerThread[order[1]]);\n-\n-    // threadId in blocked layout\n-    auto threadIds =\n-        getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n-    Value threadIdM = threadIds[0];\n-\n-    Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n-    Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n-    SmallVector<Value> aOff(aNumPtr);\n-    for (int i = 0; i < aNumPtr; ++i) {\n-      aOff[i] = add(mul(offA0, strideA0), mul(offA1, strideA1));\n-    }\n-    auto elemTy = A.getType().cast<RankedTensorType>().getElementType();\n-\n-    Type ptrTy = ptr_ty(elemTy);\n-    SmallVector<Value> aPtrs(aNumPtr);\n-    for (int i = 0; i < aNumPtr; ++i)\n-      aPtrs[i] = gep(ptrTy, aSmem.base, aOff[i]);\n-\n-    SmallVector<Value> vas;\n-\n-    int mShapePerCTA = getShapePerCTAForMN(dLayout, true /*isM*/);\n-    int mSizePerThread = getSizePerThreadForMN(dLayout, true /*isM*/);\n-\n-    for (unsigned k = 0; k < K; ++k)\n-      for (unsigned m = 0; m < M; m += mShapePerCTA)\n-        for (unsigned mm = 0; mm < mSizePerThread; ++mm) {\n-          Value offset =\n-              add(mul(i32_val(m + mm), strideAM), mul(i32_val(k), strideAK));\n-          Value pa = gep(ptrTy, aPtrs[0], offset);\n-          Value va = load(pa);\n-          vas.emplace_back(va);\n-        }\n-\n-    return getStructFromValueTable(vas, rewriter, loc, elemTy);\n-  }\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n \n   Value loadB(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const {\n-    auto bTensorTy = B.getType().cast<RankedTensorType>();\n-    auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-    auto bShape = bTensorTy.getShape();\n-\n-    auto bOrder = bLayout.getOrder();\n-    auto order = dLayout.getOrder();\n-\n-    bool isBRow = bOrder[0] == 1;\n-\n-    auto bSmem = getSharedMemoryObjectFromStruct(loc, llB, rewriter);\n-    Value strideBN = bSmem.strides[1];\n-    Value strideBK = bSmem.strides[0];\n-    Value strideB0 = isBRow ? strideBN : strideBK;\n-    Value strideB1 = isBRow ? strideBK : strideBN;\n-    int bNumPtr = 8;\n-    int K = bShape[0];\n-    int N = bShape[1];\n-\n-    auto shapePerCTA = getShapePerCTA(dLayout);\n-    auto sizePerThread = getSizePerThread(dLayout);\n-\n-    Value _0 = i32_val(0);\n-\n-    Value nContig = i32_val(sizePerThread[order[0]]);\n-\n-    // threadId in blocked layout\n-    auto threadIds =\n-        getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n-    Value threadIdN = threadIds[1];\n-\n-    Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n-    Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n-    SmallVector<Value> bOff(bNumPtr);\n-    for (int i = 0; i < bNumPtr; ++i) {\n-      bOff[i] = add(mul(offB0, strideB0), mul(offB1, strideB1));\n-    }\n-    auto elemTy = B.getType().cast<RankedTensorType>().getElementType();\n-\n-    Type ptrTy = ptr_ty(elemTy);\n-    SmallVector<Value> bPtrs(bNumPtr);\n-    for (int i = 0; i < bNumPtr; ++i)\n-      bPtrs[i] = gep(ptrTy, bSmem.base, bOff[i]);\n-\n-    SmallVector<Value> vbs;\n-\n-    int nShapePerCTA = getShapePerCTAForMN(dLayout, false /*isM*/);\n-    int nSizePerThread = getSizePerThreadForMN(dLayout, false /*isM*/);\n-\n-    for (unsigned k = 0; k < K; ++k)\n-      for (unsigned n = 0; n < N; n += nShapePerCTA)\n-        for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n-          Value offset =\n-              add(mul(i32_val(n + nn), strideBN), mul(i32_val(k), strideBK));\n-          Value pb = gep(ptrTy, bPtrs[0], offset);\n-          Value vb = load(pb);\n-          vbs.emplace_back(vb);\n-        }\n-\n-    return getStructFromValueTable(vbs, rewriter, loc, elemTy);\n-  }\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n \n   ValueTable getValueTableFromStruct(Value val, int K, int n0, int shapePerCTA,\n                                      int sizePerThread,\n                                      ConversionPatternRewriter &rewriter,\n-                                     Location loc) const {\n-    ValueTable res;\n-    auto elems = getElementsFromStruct(loc, val, rewriter);\n-    int index = 0;\n-    for (unsigned k = 0; k < K; ++k) {\n-      for (unsigned m = 0; m < n0; m += shapePerCTA)\n-        for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n-          res[{m + mm, k}] = elems[index++];\n-        }\n-    }\n-    return res;\n-  }\n+                                     Location loc) const;\n \n   Value getStructFromValueTable(ArrayRef<Value> vals,\n                                 ConversionPatternRewriter &rewriter,\n-                                Location loc, Type elemTy) const {\n-    SmallVector<Type> elemTypes(vals.size(), elemTy);\n-    SmallVector<Value> elems;\n-    elems.reserve(vals.size());\n-    for (auto &val : vals) {\n-      elems.push_back(val);\n-    }\n-\n-    Type structTy = struct_ty(elemTypes);\n-    return getStructFromElements(loc, elems, rewriter, structTy);\n-  }\n+                                Location loc, Type elemTy) const;\n \n   // get number of elements per thread for $a or $b.\n   static int getNumElemsPerThread(ArrayRef<int64_t> shape,\n-                                  DotOperandEncodingAttr dotOpLayout) {\n-    auto blockedLayout = dotOpLayout.getParent().cast<BlockedEncodingAttr>();\n-    auto shapePerCTA = getShapePerCTA(blockedLayout);\n-    auto sizePerThread = getSizePerThread(blockedLayout);\n-\n-    // TODO[Superjomn]: we assume the k aixs is fixed for $a and $b here, fix it\n-    // if not.\n-    int K = dotOpLayout.getOpIdx() == 0 ? shape[1] : shape[0];\n-    int otherDim = dotOpLayout.getOpIdx() == 1 ? shape[1] : shape[0];\n-\n-    bool isM = dotOpLayout.getOpIdx() == 0;\n-    int shapePerCTAMN = getShapePerCTAForMN(blockedLayout, isM);\n-    int sizePerThreadMN = getSizePerThreadForMN(blockedLayout, isM);\n-    return K * std::max<int>(otherDim / shapePerCTAMN, 1) * sizePerThreadMN;\n-  }\n+                                  DotOperandEncodingAttr dotOpLayout);\n \n   // Get shapePerCTA for M or N axis.\n   static int getShapePerCTAForMN(BlockedEncodingAttr layout, bool isM) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 37, "deletions": 49, "changes": 86, "file_content_changes": "@@ -21,7 +21,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n   matchAndRewrite(triton::DotOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     // D = A * B + C\n-    Value A = op.a();\n+    Value A = op.getA();\n     Value D = op.getResult();\n \n     // Here we assume the DotOp's operands always comes from shared memory.\n@@ -65,9 +65,9 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                          .getEncoding()\n                          .cast<MmaEncodingAttr>();\n \n-    Value A = op.a();\n-    Value B = op.b();\n-    Value C = op.c();\n+    Value A = op.getA();\n+    Value B = op.getB();\n+    Value C = op.getC();\n \n     MMA16816ConversionHelper mmaHelper(A.getType(), mmaLayout,\n                                        getThreadId(rewriter, loc), rewriter,\n@@ -81,21 +81,21 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n            \"Both $a and %b should be DotOperand layout.\");\n \n     Value loadedA, loadedB, loadedC;\n-    loadedA = adaptor.a();\n-    loadedB = adaptor.b();\n-    loadedC = mmaHelper.loadC(op.c(), adaptor.c());\n+    loadedA = adaptor.getA();\n+    loadedB = adaptor.getB();\n+    loadedC = mmaHelper.loadC(op.getC(), adaptor.getC());\n \n-    return mmaHelper.convertDot(A, B, C, op.d(), loadedA, loadedB, loadedC, op,\n-                                adaptor);\n+    return mmaHelper.convertDot(A, B, C, op.getD(), loadedA, loadedB, loadedC,\n+                                op, adaptor);\n   }\n   /// Convert to mma.m8n8k4\n   LogicalResult convertMMA884(triton::DotOp op, OpAdaptor adaptor,\n                               ConversionPatternRewriter &rewriter) const {\n     auto *ctx = op.getContext();\n     auto loc = op.getLoc();\n \n-    Value A = op.a();\n-    Value B = op.b();\n+    Value A = op.getA();\n+    Value B = op.getB();\n     Value D = op.getResult();\n     auto mmaLayout = D.getType()\n                          .cast<RankedTensorType>()\n@@ -115,26 +115,29 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     auto DTensorTy = D.getType().cast<RankedTensorType>();\n     auto AShape = ATensorTy.getShape();\n     auto BShape = BTensorTy.getShape();\n-    auto DShape = DTensorTy.getShape();\n-    auto wpt = mmaLayout.getWarpsPerCTA();\n \n     bool isARow = ALayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n     bool isBRow = BLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    auto [isARow_, isBRow_, isAVec4_, isBVec4_, mmaId] =\n+        mmaLayout.decodeVoltaLayoutStates();\n+    assert(isARow == isARow_);\n+    assert(isBRow == isBRow_);\n \n     DotOpMmaV1ConversionHelper helper(mmaLayout);\n \n-    unsigned numM = helper.getNumM(AShape, isARow);\n-    unsigned numN = helper.getNumN(BShape, isBRow);\n+    unsigned numM = helper.getNumM(AShape[0], isARow, isAVec4_);\n+    unsigned numN = helper.getNumN(BShape[1], isBRow, isBVec4_);\n     unsigned NK = AShape[1];\n \n-    auto has = helper.extractLoadedOperand(adaptor.a(), NK, rewriter);\n-    auto hbs = helper.extractLoadedOperand(adaptor.b(), NK, rewriter);\n+    auto has = helper.extractLoadedOperand(adaptor.getA(), NK, rewriter);\n+    auto hbs = helper.extractLoadedOperand(adaptor.getB(), NK, rewriter);\n \n     // Initialize accumulators with external values, the acc holds the\n     // accumulator value that is shared between the MMA instructions inside a\n     // DotOp, we can call the order of the values the accumulator-internal\n     // order.\n-    SmallVector<Value> acc = getElementsFromStruct(loc, adaptor.c(), rewriter);\n+    SmallVector<Value> acc =\n+        getElementsFromStruct(loc, adaptor.getC(), rewriter);\n     size_t resSize = acc.size();\n \n     // The resVals holds the final result of the DotOp.\n@@ -156,28 +159,16 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n       return idx;\n     };\n \n-    { // convert the acc's value from accumuator-external order to\n-      // accumulator-internal order.\n-      SmallVector<Value> accInit(acc.size());\n-\n-      for (unsigned m = 0; m < numM / 2; ++m)\n-        for (unsigned n = 0; n < numN / 2; ++n) {\n-          auto idx = getIdx(m, n);\n-          for (unsigned i = 0; i < 8; ++i)\n-            accInit[idx[i]] = acc[(m * numN / 2 + n) * 8 + i];\n-        }\n-\n-      acc = accInit;\n-    }\n-\n     auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n       auto ha = has.at({m, k});\n       auto hb = hbs.at({n, k});\n \n       PTXBuilder builder;\n       auto idx = getIdx(m, n);\n \n-      auto *resOprs = builder.newListOperand(8, \"=f\");\n+      // note: using \"=f\" for float leads to cleaner PTX\n+      bool isIntMMA = DTensorTy.getElementType().isInteger(32);\n+      auto *resOprs = builder.newListOperand(8, isIntMMA ? \"=r\" : \"=f\");\n       auto *AOprs = builder.newListOperand({\n           {ha.first, \"r\"},\n           {ha.second, \"r\"},\n@@ -201,14 +192,9 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n       Value res =\n           builder.launch(rewriter, loc, helper.getMmaRetType(ATensorTy));\n \n-      auto getIntAttr = [&](int v) {\n-        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n-      };\n-\n-      for (unsigned i = 0; i < 8; i++) {\n-        Value elem = extract_val(f32_ty, res, getIntAttr(i));\n+      for (auto i = 0; i < 8; i++) {\n+        Value elem = extract_val(f32_ty, res, i);\n         acc[idx[i]] = elem;\n-        resVals[(m * numN / 2 + n) * 8 + i] = elem;\n       }\n     };\n \n@@ -218,6 +204,11 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n           callMMA(m, n, k);\n         }\n \n+    // res holds the same layout of acc\n+    for (size_t i = 0; i < acc.size(); ++i) {\n+      resVals[i] = acc[i];\n+    }\n+\n     Type structTy = LLVM::LLVMStructType::getLiteral(\n         ctx, SmallVector<Type>(resSize, type::f32Ty(ctx)));\n     Value res = getStructFromElements(loc, resVals, rewriter, structTy);\n@@ -229,30 +220,27 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                               ConversionPatternRewriter &rewriter) const {\n     auto *ctx = rewriter.getContext();\n     auto loc = op.getLoc();\n-    auto threadId = getThreadId(rewriter, loc);\n \n-    auto A = op.a();\n-    auto B = op.b();\n-    auto C = op.c();\n+    auto A = op.getA();\n+    auto B = op.getB();\n+    auto C = op.getC();\n     auto D = op.getResult();\n \n     auto aTensorTy = A.getType().cast<RankedTensorType>();\n     auto bTensorTy = B.getType().cast<RankedTensorType>();\n-    auto cTensorTy = C.getType().cast<RankedTensorType>();\n     auto dTensorTy = D.getType().cast<RankedTensorType>();\n \n     auto aShape = aTensorTy.getShape();\n     auto bShape = bTensorTy.getShape();\n-    auto cShape = cTensorTy.getShape();\n \n     BlockedEncodingAttr dLayout =\n         dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n     auto order = dLayout.getOrder();\n-    auto cc = getElementsFromStruct(loc, adaptor.c(), rewriter);\n+    auto cc = getElementsFromStruct(loc, adaptor.getC(), rewriter);\n \n     DotOpFMAConversionHelper helper(dLayout);\n-    Value llA = adaptor.a();\n-    Value llB = adaptor.b();\n+    Value llA = adaptor.getA();\n+    Value llB = adaptor.getB();\n \n     auto sizePerThread = getSizePerThread(dLayout);\n     auto shapePerCTA = getShapePerCTA(dLayout);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 20, "deletions": 23, "changes": 43, "file_content_changes": "@@ -49,10 +49,8 @@ struct FpToFpOpConversion\n         struct_ty(SmallVector<Type>{fp16x2VecTy, fp16x2VecTy});\n     auto fp16x2x2Struct =\n         builder.launch(rewriter, loc, fp16x2x2StructTy, false);\n-    auto fp16x2Vec0 =\n-        extract_val(fp16x2VecTy, fp16x2x2Struct, rewriter.getI32ArrayAttr({0}));\n-    auto fp16x2Vec1 =\n-        extract_val(fp16x2VecTy, fp16x2x2Struct, rewriter.getI32ArrayAttr({1}));\n+    auto fp16x2Vec0 = extract_val(fp16x2VecTy, fp16x2x2Struct, 0);\n+    auto fp16x2Vec1 = extract_val(fp16x2VecTy, fp16x2x2Struct, 1);\n     return {extract_element(f16_ty, fp16x2Vec0, i32_val(0)),\n             extract_element(f16_ty, fp16x2Vec0, i32_val(1)),\n             extract_element(f16_ty, fp16x2Vec1, i32_val(0)),\n@@ -63,7 +61,6 @@ struct FpToFpOpConversion\n   convertFp16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n                        const Value &v0, const Value &v1, const Value &v2,\n                        const Value &v3) {\n-    auto ctx = rewriter.getContext();\n     auto fp16x2VecTy = vec_ty(f16_ty, 2);\n     Value fp16x2Vec0 = undef(fp16x2VecTy);\n     Value fp16x2Vec1 = undef(fp16x2VecTy);\n@@ -143,10 +140,8 @@ struct FpToFpOpConversion\n         struct_ty(SmallVector<Type>{bf16x2VecTy, bf16x2VecTy});\n     auto bf16x2x2Struct =\n         builder.launch(rewriter, loc, bf16x2x2StructTy, false);\n-    auto bf16x2Vec0 =\n-        extract_val(bf16x2VecTy, bf16x2x2Struct, rewriter.getI32ArrayAttr({0}));\n-    auto bf16x2Vec1 =\n-        extract_val(bf16x2VecTy, bf16x2x2Struct, rewriter.getI32ArrayAttr({1}));\n+    auto bf16x2Vec0 = extract_val(bf16x2VecTy, bf16x2x2Struct, 0);\n+    auto bf16x2Vec1 = extract_val(bf16x2VecTy, bf16x2x2Struct, 1);\n     return {extract_element(i16_ty, bf16x2Vec0, i32_val(0)),\n             extract_element(i16_ty, bf16x2Vec0, i32_val(1)),\n             extract_element(i16_ty, bf16x2Vec1, i32_val(0)),\n@@ -157,7 +152,6 @@ struct FpToFpOpConversion\n   convertBf16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n                        const Value &v0, const Value &v1, const Value &v2,\n                        const Value &v3) {\n-    auto ctx = rewriter.getContext();\n     auto bf16x2VecTy = vec_ty(i16_ty, 2);\n     Value bf16x2Vec0 = undef(bf16x2VecTy);\n     Value bf16x2Vec1 = undef(bf16x2VecTy);\n@@ -291,8 +285,9 @@ struct FpToFpOpConversion\n   LogicalResult\n   matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto srcTensorType = op.from().getType().cast<mlir::RankedTensorType>();\n-    auto dstTensorType = op.result().getType().cast<mlir::RankedTensorType>();\n+    auto srcTensorType = op.getFrom().getType().cast<mlir::RankedTensorType>();\n+    auto dstTensorType =\n+        op.getResult().getType().cast<mlir::RankedTensorType>();\n     auto srcEltType = srcTensorType.getElementType();\n     auto dstEltType = dstTensorType.getElementType();\n     auto loc = op->getLoc();\n@@ -329,16 +324,18 @@ struct FpToFpOpConversion\n       // Vectorized casting\n       assert(elems % 4 == 0 &&\n              \"FP8 casting only support tensors with 4-aligned sizes\");\n-      auto elements = getElementsFromStruct(loc, adaptor.from(), rewriter);\n+      auto elements = getElementsFromStruct(loc, adaptor.getFrom(), rewriter);\n       for (size_t i = 0; i < elems; i += 4) {\n         auto converted = convertor(loc, rewriter, elements[i], elements[i + 1],\n                                    elements[i + 2], elements[i + 3]);\n         resultVals.append(converted);\n       }\n     } else if (srcEltType.isBF16() && dstEltType.isF32()) {\n-      resultVals.emplace_back(convertBf16ToFp32(loc, rewriter, adaptor.from()));\n+      resultVals.emplace_back(\n+          convertBf16ToFp32(loc, rewriter, adaptor.getFrom()));\n     } else if (srcEltType.isF32() && dstEltType.isBF16()) {\n-      resultVals.emplace_back(convertFp32ToBf16(loc, rewriter, adaptor.from()));\n+      resultVals.emplace_back(\n+          convertFp32ToBf16(loc, rewriter, adaptor.getFrom()));\n     } else {\n       assert(false && \"unsupported type casting\");\n     }\n@@ -442,7 +439,7 @@ struct CmpIOpConversion\n                             ConversionPatternRewriter &rewriter, Type elemTy,\n                             ValueRange operands, Location loc) const {\n     return rewriter.create<LLVM::ICmpOp>(\n-        loc, elemTy, ArithCmpIPredicateToLLVM(op.predicate()), operands[0],\n+        loc, elemTy, ArithCmpIPredicateToLLVM(op.getPredicate()), operands[0],\n         operands[1]);\n   }\n \n@@ -466,7 +463,7 @@ struct CmpIOpConversion\n \n #undef __PRED_ENUM\n     }\n-    return LLVM::ICmpPredicate::eq;\n+    llvm_unreachable(\"Unknown arith::CmpIPredicate\");\n   }\n };\n \n@@ -484,7 +481,7 @@ struct CmpFOpConversion\n                                    Type elemTy, ValueRange operands,\n                                    Location loc) {\n     return rewriter.create<LLVM::FCmpOp>(\n-        loc, elemTy, ArithCmpFPredicateToLLVM(op.predicate()), operands[0],\n+        loc, elemTy, ArithCmpFPredicateToLLVM(op.getPredicate()), operands[0],\n         operands[1]);\n   }\n \n@@ -514,7 +511,7 @@ struct CmpFOpConversion\n \n #undef __PRED_ENUM\n     }\n-    return LLVM::FCmpPredicate::_true;\n+    llvm_unreachable(\"Unknown arith::CmpFPredicate\");\n   }\n };\n \n@@ -529,14 +526,14 @@ struct ExtElemwiseOpConversion\n   Value createDestOp(triton::ExtElemwiseOp op, OpAdaptor adaptor,\n                      ConversionPatternRewriter &rewriter, Type elemTy,\n                      ValueRange operands, Location loc) const {\n-    StringRef funcName = op.symbol();\n+    StringRef funcName = op.getSymbol();\n     if (funcName.empty())\n       llvm::errs() << \"ExtElemwiseOpConversion\";\n \n     Type funcType = getFunctionType(elemTy, operands);\n     LLVM::LLVMFuncOp funcOp =\n         appendOrGetFuncOp(rewriter, op, funcName, funcType);\n-    return rewriter.create<LLVM::CallOp>(loc, funcOp, operands).getResult(0);\n+    return rewriter.create<LLVM::CallOp>(loc, funcOp, operands).getResult();\n   }\n \n private:\n@@ -558,9 +555,9 @@ struct ExtElemwiseOpConversion\n     mlir::OpBuilder b(op->getParentOfType<LLVMFuncOp>());\n     auto ret = b.create<LLVMFuncOp>(op->getLoc(), funcName, funcType);\n     ret.getOperation()->setAttr(\n-        \"libname\", StringAttr::get(op->getContext(), op.libname()));\n+        \"libname\", StringAttr::get(op->getContext(), op.getLibname()));\n     ret.getOperation()->setAttr(\n-        \"libpath\", StringAttr::get(op->getContext(), op.libpath()));\n+        \"libpath\", StringAttr::get(op->getContext(), op.getLibpath()));\n     return ret;\n   }\n };"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 64, "deletions": 54, "changes": 118, "file_content_changes": "@@ -31,8 +31,21 @@ struct LoadStoreConversionBase {\n     return valueVals;\n   }\n \n+  unsigned getContiguity(Value ptr) const {\n+    auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return 1;\n+    return axisAnalysisPass.getPtrContiguity(ptr);\n+  }\n+\n   unsigned getVectorSize(Value ptr) const {\n-    return axisAnalysisPass.getPtrVectorSize(ptr);\n+    auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return 1;\n+    auto contiguity = getContiguity(ptr);\n+    auto pointeeBitWidth = getPointeeBitWidth(tensorTy);\n+    // The maximum vector size is 128 bits on NVIDIA GPUs.\n+    return std::min<unsigned>(128 / pointeeBitWidth, contiguity);\n   }\n \n   unsigned getMaskAlignment(Value mask) const {\n@@ -60,14 +73,14 @@ struct LoadOpConversion\n     auto loc = op->getLoc();\n \n     // original values\n-    Value ptr = op.ptr();\n-    Value mask = op.mask();\n-    Value other = op.other();\n+    Value ptr = op.getPtr();\n+    Value mask = op.getMask();\n+    Value other = op.getOther();\n \n     // adaptor values\n-    Value llPtr = adaptor.ptr();\n-    Value llMask = adaptor.mask();\n-    Value llOther = adaptor.other();\n+    Value llPtr = adaptor.getPtr();\n+    Value llMask = adaptor.getMask();\n+    Value llOther = adaptor.getOther();\n \n     // Determine the vectorization size\n     Type valueTy = op.getResult().getType();\n@@ -96,7 +109,8 @@ struct LoadOpConversion\n     DenseElementsAttr constAttr;\n     int64_t splatVal = 0;\n     if (other && valueElemTy.isa<IntegerType>() &&\n-        matchPattern(other, m_Constant(&constAttr)) && constAttr.isSplat()) {\n+        matchPattern(other, m_Constant(&constAttr)) && constAttr.isSplat() &&\n+        constAttr.getElementType().isa<IntegerType>()) {\n       otherIsSplatConstInt = true;\n       splatVal = constAttr.getSplatValue<APInt>().getSExtValue();\n     }\n@@ -144,14 +158,14 @@ struct LoadOpConversion\n \n       // Define the instruction opcode\n       auto &ld = ptxBuilder.create<>(\"ld\")\n-                     ->o(\"volatile\", op.isVolatile())\n+                     ->o(\"volatile\", op.getIsVolatile())\n                      .global()\n-                     .o(\"ca\", op.cache() == triton::CacheModifier::CA)\n-                     .o(\"cg\", op.cache() == triton::CacheModifier::CG)\n+                     .o(\"ca\", op.getCache() == triton::CacheModifier::CA)\n+                     .o(\"cg\", op.getCache() == triton::CacheModifier::CG)\n                      .o(\"L1::evict_first\",\n-                        op.evict() == triton::EvictionPolicy::EVICT_FIRST)\n+                        op.getEvict() == triton::EvictionPolicy::EVICT_FIRST)\n                      .o(\"L1::evict_last\",\n-                        op.evict() == triton::EvictionPolicy::EVICT_LAST)\n+                        op.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n                      .o(\"L1::cache_hint\", hasL2EvictPolicy)\n                      .v(nWords)\n                      .b(width);\n@@ -213,8 +227,7 @@ struct LoadOpConversion\n       for (unsigned int ii = 0; ii < nWords; ++ii) {\n         Value curr;\n         if (retTy.isa<LLVM::LLVMStructType>()) {\n-          curr = extract_val(IntegerType::get(getContext(), width), ret,\n-                             rewriter.getI64ArrayAttr(ii));\n+          curr = extract_val(IntegerType::get(getContext(), width), ret, ii);\n         } else {\n           curr = ret;\n         }\n@@ -253,13 +266,13 @@ struct StoreOpConversion\n   LogicalResult\n   matchAndRewrite(triton::StoreOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    Value ptr = op.ptr();\n-    Value mask = op.mask();\n-    Value value = op.value();\n+    Value ptr = op.getPtr();\n+    Value mask = op.getMask();\n+    Value value = op.getValue();\n \n-    Value llPtr = adaptor.ptr();\n-    Value llMask = adaptor.mask();\n-    Value llValue = adaptor.value();\n+    Value llPtr = adaptor.getPtr();\n+    Value llMask = adaptor.getMask();\n+    Value llValue = adaptor.getValue();\n \n     auto loc = op->getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n@@ -320,7 +333,6 @@ struct StoreOpConversion\n             elem = rewriter.create<LLVM::SExtOp>(loc, type::i8Ty(ctx), elem);\n           elem = bitcast(elem, valueElemTy);\n \n-          Type u32Ty = typeConverter->convertType(type::u32Ty(ctx));\n           llWord = insert_element(wordTy, llWord, elem, i32_val(elemIdx));\n         }\n         llWord = bitcast(llWord, valArgTy);\n@@ -374,11 +386,10 @@ struct AtomicCASOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     auto loc = op.getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n-    Value ptr = op.ptr();\n \n-    Value llPtr = adaptor.ptr();\n-    Value llCmp = adaptor.cmp();\n-    Value llVal = adaptor.val();\n+    Value llPtr = adaptor.getPtr();\n+    Value llCmp = adaptor.getCmp();\n+    Value llVal = adaptor.getVal();\n \n     auto ptrElements = getElementsFromStruct(loc, llPtr, rewriter);\n     auto cmpElements = getElementsFromStruct(loc, llCmp, rewriter);\n@@ -450,13 +461,13 @@ struct AtomicRMWOpConversion\n     auto loc = op.getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n \n-    auto atomicRmwAttr = op.atomic_rmw_op();\n-    Value ptr = op.ptr();\n-    Value val = op.val();\n+    auto atomicRmwAttr = op.getAtomicRmwOp();\n+    Value ptr = op.getPtr();\n+    Value val = op.getVal();\n \n-    Value llPtr = adaptor.ptr();\n-    Value llVal = adaptor.val();\n-    Value llMask = adaptor.mask();\n+    Value llPtr = adaptor.getPtr();\n+    Value llVal = adaptor.getVal();\n+    Value llMask = adaptor.getMask();\n \n     auto valElements = getElementsFromStruct(loc, llVal, rewriter);\n     auto ptrElements = getElementsFromStruct(loc, llPtr, rewriter);\n@@ -594,9 +605,9 @@ struct InsertSliceOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     // %dst = insert_slice %src into %dst[%offsets]\n     Location loc = op->getLoc();\n-    Value dst = op.dest();\n-    Value src = op.source();\n-    Value res = op.result();\n+    Value dst = op.getDest();\n+    Value src = op.getSource();\n+    Value res = op.getResult();\n     assert(allocation->getBufferId(res) == Allocation::InvalidBufferId &&\n            \"Only support in-place insert_slice for now\");\n \n@@ -607,7 +618,7 @@ struct InsertSliceOpConversion\n \n     auto dstTy = dst.getType().dyn_cast<RankedTensorType>();\n     auto dstLayout = dstTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n-    auto llDst = adaptor.dest();\n+    auto llDst = adaptor.getDest();\n     assert(dstLayout && \"Unexpected dstLayout in InsertSliceOpConversion\");\n     assert(op.hasUnitStride() &&\n            \"Only unit stride supported by InsertSliceOpConversion\");\n@@ -620,7 +631,7 @@ struct InsertSliceOpConversion\n     auto mixedOffsets = op.getMixedOffsets();\n     for (auto i = 0; i < mixedOffsets.size(); ++i) {\n       if (op.isDynamicOffset(i)) {\n-        offsets.emplace_back(adaptor.offsets()[i]);\n+        offsets.emplace_back(adaptor.getOffsets()[i]);\n       } else {\n         offsets.emplace_back(i32_val(op.getStaticOffset(i)));\n       }\n@@ -638,7 +649,7 @@ struct InsertSliceOpConversion\n     auto elemPtrTy = ptr_ty(elemTy, 3);\n     auto smemBase = gep(elemPtrTy, smemObj.base, offset);\n \n-    auto llSrc = adaptor.source();\n+    auto llSrc = adaptor.getSource();\n     auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n     storeDistributedToShared(src, llSrc, srcStrides, srcIndices, dst, smemBase,\n                              elemTy, loc, rewriter);\n@@ -669,11 +680,11 @@ struct InsertSliceAsyncOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     // insert_slice_async %src, %dst, %index, %mask, %other\n     auto loc = op.getLoc();\n-    Value src = op.src();\n-    Value dst = op.dst();\n-    Value res = op.result();\n-    Value mask = op.mask();\n-    Value other = op.other();\n+    Value src = op.getSrc();\n+    Value dst = op.getDst();\n+    Value res = op.getResult();\n+    Value mask = op.getMask();\n+    Value other = op.getOther();\n     assert(allocation->getBufferId(res) == Allocation::InvalidBufferId &&\n            \"Only support in-place insert_slice_async for now\");\n \n@@ -686,11 +697,11 @@ struct InsertSliceAsyncOpConversion\n     assert(srcShape.size() == 2 &&\n            \"insert_slice_async: Unexpected rank of %src\");\n \n-    Value llDst = adaptor.dst();\n-    Value llSrc = adaptor.src();\n-    Value llMask = adaptor.mask();\n-    Value llOther = adaptor.other();\n-    Value llIndex = adaptor.index();\n+    Value llDst = adaptor.getDst();\n+    Value llSrc = adaptor.getSrc();\n+    Value llMask = adaptor.getMask();\n+    Value llOther = adaptor.getOther();\n+    Value llIndex = adaptor.getIndex();\n \n     // %src\n     auto srcElems = getLLVMElems(src, llSrc, rewriter, loc);\n@@ -734,7 +745,10 @@ struct InsertSliceAsyncOpConversion\n       assert(srcElems.size() == otherElems.size());\n     }\n \n-    unsigned inVec = getVectorSize(src);\n+    // We don't use getVec() here because we are copying from memory to memory.\n+    // If contiguity > vector size, we can have one pointer maintaining the\n+    // start of the vector and the other pointer moving to the next vector.\n+    unsigned inVec = getContiguity(src);\n     unsigned outVec = resSharedLayout.getVec();\n     unsigned minVec = std::min(outVec, inVec);\n     unsigned numElems = getElemsPerThread(srcTy);\n@@ -760,7 +774,6 @@ struct InsertSliceAsyncOpConversion\n     auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcShape);\n \n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n-\n       // 16 * 8 = 128bits\n       auto maxBitWidth =\n           std::max<unsigned>(128, resElemTy.getIntOrFloatBitWidth());\n@@ -788,7 +801,7 @@ struct InsertSliceAsyncOpConversion\n             ptxBuilder.newAddrOperand(srcElems[elemIdx + wordElemIdx], \"l\");\n         auto *copySize = ptxBuilder.newConstantOperand(byteWidth);\n         auto *srcSize = copySize;\n-        if (op.mask()) {\n+        if (op.getMask()) {\n           // We don't use predicate in this case, setting src-size to 0\n           // if there's any mask. cp.async will automatically fill the\n           // remaining slots with 0 if cp-size > src-size.\n@@ -802,9 +815,6 @@ struct InsertSliceAsyncOpConversion\n       }\n     }\n \n-    PTXBuilder ptxBuilder;\n-    ptxBuilder.create<>(\"cp.async.commit_group\")->operator()();\n-    ptxBuilder.launch(rewriter, loc, void_ty(getContext()));\n     rewriter.replaceOp(op, llDst);\n     return success();\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/PTXAsmFormat.cpp", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -78,9 +78,8 @@ SmallVector<PTXBuilder::Operand *, 4> PTXBuilder::getAllArgs() const {\n   return res;\n }\n \n-mlir::Value PTXBuilder::launch(ConversionPatternRewriter &rewriter,\n-                               Location loc, Type resTy, bool hasSideEffect,\n-                               bool isAlignStack,\n+mlir::Value PTXBuilder::launch(OpBuilder &rewriter, Location loc, Type resTy,\n+                               bool hasSideEffect, bool isAlignStack,\n                                ArrayRef<Attribute> attrs) const {\n   auto *ctx = rewriter.getContext();\n   auto inlineAsm = rewriter.create<LLVM::InlineAsmOp>("}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 20, "deletions": 20, "changes": 40, "file_content_changes": "@@ -138,10 +138,10 @@ struct ReduceOpConversion\n   matchAndRewriteBasic(triton::ReduceOp op, OpAdaptor adaptor,\n                        ConversionPatternRewriter &rewriter) const {\n     Location loc = op->getLoc();\n-    unsigned axis = op.axis();\n-    bool withIndex = triton::ReduceOp::withIndex(op.redOp());\n+    unsigned axis = op.getAxis();\n+    bool withIndex = triton::ReduceOp::withIndex(op.getRedOp());\n \n-    auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+    auto srcTy = op.getOperand().getType().cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n     auto srcOrd = srcLayout.getOrder();\n     auto srcShape = srcTy.getShape();\n@@ -161,7 +161,7 @@ struct ReduceOpConversion\n \n     unsigned srcElems = getElemsPerThread(srcTy);\n     auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n-    auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n+    auto srcValues = getElementsFromStruct(loc, adaptor.getOperand(), rewriter);\n \n     SmallVector<SmallVector<unsigned>> offset =\n         emitOffsetForLayout(srcLayout, srcShape);\n@@ -176,10 +176,11 @@ struct ReduceOpConversion\n       key[axis] = 0;\n       bool isFirst = accs.find(key) == accs.end();\n       if (!withIndex) {\n-        accumulate(rewriter, loc, op.redOp(), accs[key], srcValues[i], isFirst);\n+        accumulate(rewriter, loc, op.getRedOp(), accs[key], srcValues[i],\n+                   isFirst);\n       } else {\n         Value curIndex = srcIndices[i][axis];\n-        accumulateWithIndex(rewriter, loc, op.redOp(), accs[key],\n+        accumulateWithIndex(rewriter, loc, op.getRedOp(), accs[key],\n                             accIndices[key], srcValues[i], curIndex, isFirst);\n       }\n       if (isFirst)\n@@ -221,14 +222,14 @@ struct ReduceOpConversion\n         barrier();\n         if (!withIndex) {\n           Value cur = load(readPtr);\n-          accumulate(rewriter, loc, op.redOp(), acc, cur, false);\n+          accumulate(rewriter, loc, op.getRedOp(), acc, cur, false);\n           barrier();\n           store(acc, writePtr);\n         } else {\n           Value cur = load(readPtr);\n           Value indexReadPtr = gep(indexPtrTy, indexWritePtr, readOffset);\n           Value curIndex = load(indexReadPtr);\n-          accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, cur,\n+          accumulateWithIndex(rewriter, loc, op.getRedOp(), acc, accIndex, cur,\n                               curIndex, false);\n           barrier();\n           store(acc, writePtr);\n@@ -280,13 +281,12 @@ struct ReduceOpConversion\n   LogicalResult matchAndRewriteFast(triton::ReduceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter) const {\n     Location loc = op->getLoc();\n-    unsigned axis = adaptor.axis();\n-    bool withIndex = triton::ReduceOp::withIndex(op.redOp());\n+    unsigned axis = adaptor.getAxis();\n+    bool withIndex = triton::ReduceOp::withIndex(op.getRedOp());\n \n-    auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+    auto srcTy = op.getOperand().getType().cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding();\n     auto srcShape = srcTy.getShape();\n-    auto srcRank = srcTy.getRank();\n     auto order = getOrder(srcLayout);\n \n     auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcLayout);\n@@ -311,7 +311,7 @@ struct ReduceOpConversion\n \n     unsigned srcElems = getElemsPerThread(srcTy);\n     auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n-    auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n+    auto srcValues = getElementsFromStruct(loc, adaptor.getOperand(), rewriter);\n \n     SmallVector<SmallVector<unsigned>> offset =\n         emitOffsetForLayout(srcLayout, srcShape);\n@@ -326,10 +326,11 @@ struct ReduceOpConversion\n       key[axis] = 0;\n       bool isFirst = accs.find(key) == accs.end();\n       if (!withIndex) {\n-        accumulate(rewriter, loc, op.redOp(), accs[key], srcValues[i], isFirst);\n+        accumulate(rewriter, loc, op.getRedOp(), accs[key], srcValues[i],\n+                   isFirst);\n       } else {\n         Value curIndex = srcIndices[i][axis];\n-        accumulateWithIndex(rewriter, loc, op.redOp(), accs[key],\n+        accumulateWithIndex(rewriter, loc, op.getRedOp(), accs[key],\n                             accIndices[key], srcValues[i], curIndex, isFirst);\n       }\n       if (isFirst)\n@@ -351,7 +352,6 @@ struct ReduceOpConversion\n \n     Value zero = i32_val(0);\n     Value laneZero = icmp_eq(laneIdAxis, zero);\n-    Value warpZero = icmp_eq(warpIdAxis, zero);\n \n     for (auto it : accs) {\n       const SmallVector<unsigned> &key = it.first;\n@@ -364,10 +364,10 @@ struct ReduceOpConversion\n       for (unsigned N = sizeIntraWarps / 2; N > 0; N >>= 1) {\n         Value shfl = shflSync(loc, rewriter, acc, N);\n         if (!withIndex) {\n-          accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n+          accumulate(rewriter, loc, op.getRedOp(), acc, shfl, false);\n         } else {\n           Value shflIndex = shflSync(loc, rewriter, accIndex, N);\n-          accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, shfl,\n+          accumulateWithIndex(rewriter, loc, op.getRedOp(), acc, accIndex, shfl,\n                               shflIndex, false);\n         }\n       }\n@@ -410,10 +410,10 @@ struct ReduceOpConversion\n       for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n         Value shfl = shflSync(loc, rewriter, acc, N);\n         if (!withIndex) {\n-          accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n+          accumulate(rewriter, loc, op.getRedOp(), acc, shfl, false);\n         } else {\n           Value shflIndex = shflSync(loc, rewriter, accIndex, N);\n-          accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, shfl,\n+          accumulateWithIndex(rewriter, loc, op.getRedOp(), acc, accIndex, shfl,\n                               shflIndex, false);\n         }\n       }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 67, "deletions": 22, "changes": 89, "file_content_changes": "@@ -11,11 +11,11 @@ using ::mlir::LLVM::getStructFromElements;\n using ::mlir::triton::gpu::getElemsPerThread;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n-struct ReturnOpConversion : public ConvertOpToLLVMPattern<::mlir::ReturnOp> {\n-  using ConvertOpToLLVMPattern<ReturnOp>::ConvertOpToLLVMPattern;\n+struct ReturnOpConversion : public ConvertOpToLLVMPattern<func::ReturnOp> {\n+  using ConvertOpToLLVMPattern<func::ReturnOp>::ConvertOpToLLVMPattern;\n \n   LogicalResult\n-  matchAndRewrite(ReturnOp op, OpAdaptor adaptor,\n+  matchAndRewrite(func::ReturnOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     unsigned numArguments = op.getNumOperands();\n \n@@ -54,9 +54,9 @@ struct BroadcastOpConversion\n     // regardless of the order of the layout\n     //\n     Location loc = op->getLoc();\n-    Value src = adaptor.src();\n-    Value result = op.result();\n-    auto srcTy = op.src().getType().cast<RankedTensorType>();\n+    Value src = adaptor.getSrc();\n+    Value result = op.getResult();\n+    auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n     auto resultTy = result.getType().cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding();\n     auto resultLayout = resultTy.getEncoding();\n@@ -69,10 +69,12 @@ struct BroadcastOpConversion\n     auto srcOffsets = emitOffsetForLayout(srcLayout, srcShape);\n     auto resultOffsets = emitOffsetForLayout(resultLayout, resultShape);\n     SmallVector<Value> srcVals = getElementsFromStruct(loc, src, rewriter);\n+\n     DenseMap<SmallVector<unsigned>, Value, SmallVectorKeyInfo> srcValues;\n     for (size_t i = 0; i < srcOffsets.size(); i++) {\n       srcValues[srcOffsets[i]] = srcVals[i];\n     }\n+\n     SmallVector<Value> resultVals;\n     for (size_t i = 0; i < resultOffsets.size(); i++) {\n       auto offset = resultOffsets[i];\n@@ -81,7 +83,9 @@ struct BroadcastOpConversion\n           offset[j] = 0;\n       resultVals.push_back(srcValues.lookup(offset));\n     }\n+\n     auto llvmStructTy = getTypeConverter()->convertType(resultTy);\n+\n     Value resultStruct =\n         getStructFromElements(loc, resultVals, rewriter, llvmStructTy);\n     rewriter.replaceOp(op, {resultStruct});\n@@ -107,7 +111,7 @@ struct PrintfOpConversion\n     }\n     std::string formatStr;\n     llvm::raw_string_ostream os(formatStr);\n-    os << op.prefix();\n+    os << op.getPrefix();\n     if (!operands.empty()) {\n       os << getFormatSubstr(operands[0]);\n     }\n@@ -285,20 +289,20 @@ struct MakeRangeOpConversion\n   matchAndRewrite(triton::MakeRangeOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n-    auto rankedTy = op.result().getType().dyn_cast<RankedTensorType>();\n+    auto rankedTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n     auto shape = rankedTy.getShape();\n     auto layout = rankedTy.getEncoding();\n \n     auto elemTy = rankedTy.getElementType();\n     assert(elemTy.isInteger(32));\n-    Value start = createIndexAttrConstant(rewriter, loc, elemTy, op.start());\n+    Value start = createIndexAttrConstant(rewriter, loc, elemTy, op.getStart());\n     auto idxs = emitIndices(loc, rewriter, layout, shape);\n     unsigned elems = idxs.size();\n     SmallVector<Value> retVals(elems);\n     // TODO: slice layout has more elements than expected.\n     // Unexpected behavior for make range, but generally OK when followed by\n     // expand dims + broadcast. very weird behavior otherwise potentially.\n-    for (const auto multiDim : llvm::enumerate(idxs)) {\n+    for (const auto &multiDim : llvm::enumerate(idxs)) {\n       assert(multiDim.value().size() == 1);\n       retVals[multiDim.index()] = add(multiDim.value()[0], start);\n     }\n@@ -319,10 +323,10 @@ struct GetProgramIdOpConversion\n   matchAndRewrite(triton::GetProgramIdOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n-    assert(op.axis() < 3);\n+    assert(op.getAxis() < 3);\n \n     Value blockId = rewriter.create<::mlir::gpu::BlockIdOp>(\n-        loc, rewriter.getIndexType(), dims[op.axis()]);\n+        loc, rewriter.getIndexType(), dims[op.getAxis()]);\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n     rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>(\n         op, TypeRange{llvmIndexTy}, ValueRange{blockId});\n@@ -343,10 +347,10 @@ struct GetNumProgramsOpConversion\n   matchAndRewrite(triton::GetNumProgramsOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n-    assert(op.axis() < 3);\n+    assert(op.getAxis() < 3);\n \n     Value blockId = rewriter.create<::mlir::gpu::GridDimOp>(\n-        loc, rewriter.getIndexType(), dims[op.axis()]);\n+        loc, rewriter.getIndexType(), dims[op.getAxis()]);\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n     rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>(\n         op, TypeRange{llvmIndexTy}, ValueRange{blockId});\n@@ -375,8 +379,8 @@ struct AddPtrOpConversion\n           getTypeConverter()->convertType(resultTensorTy.getElementType());\n       SmallVector<Type> types(elems, elemTy);\n       Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n-      auto ptrs = getElementsFromStruct(loc, adaptor.ptr(), rewriter);\n-      auto offsets = getElementsFromStruct(loc, adaptor.offset(), rewriter);\n+      auto ptrs = getElementsFromStruct(loc, adaptor.getPtr(), rewriter);\n+      auto offsets = getElementsFromStruct(loc, adaptor.getOffset(), rewriter);\n       SmallVector<Value> resultVals(elems);\n       for (unsigned i = 0; i < elems; ++i) {\n         resultVals[i] = gep(elemTy, ptrs[i], offsets[i]);\n@@ -386,7 +390,7 @@ struct AddPtrOpConversion\n     } else {\n       assert(resultTy.isa<triton::PointerType>());\n       Type llResultTy = getTypeConverter()->convertType(resultTy);\n-      Value result = gep(llResultTy, adaptor.ptr(), adaptor.offset());\n+      Value result = gep(llResultTy, adaptor.getPtr(), adaptor.getOffset());\n       rewriter.replaceOp(op, result);\n     }\n     return success();\n@@ -436,7 +440,7 @@ struct ExtractSliceOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     // %dst = extract_slice %src[%offsets]\n     Location loc = op->getLoc();\n-    auto srcTy = op.source().getType().dyn_cast<RankedTensorType>();\n+    auto srcTy = op.getSource().getType().dyn_cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n     assert(srcLayout && \"Unexpected resultLayout in ExtractSliceOpConversion\");\n     assert(op.hasUnitStride() &&\n@@ -445,13 +449,13 @@ struct ExtractSliceOpConversion\n     // newBase = base + offset\n     // Triton supports either static and dynamic offsets\n     auto smemObj =\n-        getSharedMemoryObjectFromStruct(loc, adaptor.source(), rewriter);\n+        getSharedMemoryObjectFromStruct(loc, adaptor.getSource(), rewriter);\n     SmallVector<Value, 4> opOffsetVals;\n     SmallVector<Value, 4> offsetVals;\n     auto mixedOffsets = op.getMixedOffsets();\n     for (auto i = 0; i < mixedOffsets.size(); ++i) {\n       if (op.isDynamicOffset(i))\n-        opOffsetVals.emplace_back(adaptor.offsets()[i]);\n+        opOffsetVals.emplace_back(adaptor.getOffsets()[i]);\n       else\n         opOffsetVals.emplace_back(i32_val(op.getStaticOffset(i)));\n       offsetVals.emplace_back(add(smemObj.offsets[i], opOffsetVals[i]));\n@@ -472,7 +476,6 @@ struct ExtractSliceOpConversion\n \n     auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n     auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n-    auto resTy = op.getType().dyn_cast<RankedTensorType>();\n     smemObj = SharedMemoryObject(gep(elemPtrTy, smemObj.base, offset),\n                                  strideVals, offsetVals);\n     auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n@@ -505,6 +508,47 @@ struct AsyncWaitOpConversion\n   }\n };\n \n+struct AsyncCommitGroupOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::AsyncCommitGroupOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::gpu::AsyncCommitGroupOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::gpu::AsyncCommitGroupOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+\n+    PTXBuilder ptxBuilder;\n+    ptxBuilder.create<>(\"cp.async.commit_group\")->operator()();\n+    ptxBuilder.launch(rewriter, op.getLoc(), void_ty(op.getContext()));\n+    // Safe to remove the op since it doesn't have any return value.\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+};\n+\n+namespace mlir {\n+namespace LLVM {\n+\n+void vprintf(StringRef msg, ValueRange args,\n+             ConversionPatternRewriter &rewriter) {\n+  PrintfOpConversion::llPrintf(msg, args, rewriter);\n+}\n+\n+void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n+                   std::string elem_repr, ConversionPatternRewriter &builder) {\n+  std::string fmt = info + \" t-%d \";\n+  std::vector<Value> new_arr({thread});\n+  for (int i = 0; i < arr.size(); ++i) {\n+    fmt += elem_repr + ((i == arr.size() - 1) ? \"\" : \", \");\n+    new_arr.push_back(arr[i]);\n+  }\n+\n+  vprintf(fmt, new_arr, builder);\n+}\n+\n+} // namespace LLVM\n+} // namespace mlir\n+\n void populateTritonGPUToLLVMPatterns(\n     mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n@@ -514,6 +558,7 @@ void populateTritonGPUToLLVMPatterns(\n   patterns.add<AddPtrOpConversion>(typeConverter, benefit);\n   patterns.add<AllocTensorOpConversion>(typeConverter, allocation, smem,\n                                         benefit);\n+  patterns.add<AsyncCommitGroupOpConversion>(typeConverter, benefit);\n   patterns.add<AsyncWaitOpConversion>(typeConverter, benefit);\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n \n@@ -524,4 +569,4 @@ void populateTritonGPUToLLVMPatterns(\n   patterns.add<MakeRangeOpConversion>(typeConverter, indexCacheInfo, benefit);\n   patterns.add<ReturnOpConversion>(typeConverter, benefit);\n   patterns.add<PrintfOpConversion>(typeConverter, benefit);\n-}\n\\ No newline at end of file\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 127, "deletions": 53, "changes": 180, "file_content_changes": "@@ -4,9 +4,11 @@\n // TODO: refactor so that it doesn't fail if Allocation.h\n // is included after utility.h (due to conflict in `store` macro\n // and <atomic>\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"triton/Analysis/Allocation.h\"\n \n //\n+#include \"DotOpHelpers.h\"\n #include \"Utility.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n@@ -18,26 +20,39 @@ using ::mlir::LLVM::SharedMemoryObject;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;\n+\n+namespace mlir {\n+namespace LLVM {\n+\n+// Helper function for using printf in LLVM conversion.\n+void vprintf(StringRef msg, ValueRange args,\n+             ConversionPatternRewriter &rewriter);\n+\n+void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n+                   std::string elem_repr, ConversionPatternRewriter &builder);\n+\n+} // namespace LLVM\n+} // namespace mlir\n+\n // FuncOpConversion/FuncOpConversionBase is borrowed from\n // https://github.com/llvm/llvm-project/blob/fae656b2dd80246c3c6f01e9c77c49560368752c/mlir/lib/Conversion/FuncToLLVM/FuncToLLVM.cpp#L276\n // since it is not exposed on header files in mlir v14\n // TODO(Superjomn): remove the code when MLIR v15.0 is included.\n // All the rights are reserved by the LLVM community.\n \n-struct FuncOpConversionBase : public ConvertOpToLLVMPattern<FuncOp> {\n+struct FuncOpConversionBase : public ConvertOpToLLVMPattern<func::FuncOp> {\n private:\n   /// Only retain those attributes that are not constructed by\n   /// `LLVMFuncOp::build`. If `filterArgAttrs` is set, also filter out argument\n   /// attributes.\n-  static void filterFuncAttributes(ArrayRef<NamedAttribute> attrs,\n-                                   bool filterArgAttrs,\n+  static void filterFuncAttributes(func::FuncOp op, bool filterArgAttrs,\n                                    SmallVectorImpl<NamedAttribute> &result) {\n-    for (const auto &attr : attrs) {\n+\n+    for (const auto &attr : op->getAttrs()) {\n       if (attr.getName() == SymbolTable::getSymbolAttrName() ||\n-          attr.getName() == FunctionOpInterface::getTypeAttrName() ||\n+          attr.getName() == op.getFunctionTypeAttrName() ||\n           attr.getName() == \"std.varargs\" ||\n-          (filterArgAttrs &&\n-           attr.getName() == FunctionOpInterface::getArgDictAttrName()))\n+          (filterArgAttrs && attr.getName() == op.getArgAttrsAttrName()))\n         continue;\n       result.push_back(attr);\n     }\n@@ -50,36 +65,36 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<FuncOp> {\n   }\n \n protected:\n-  using ConvertOpToLLVMPattern<FuncOp>::ConvertOpToLLVMPattern;\n+  using ConvertOpToLLVMPattern<func::FuncOp>::ConvertOpToLLVMPattern;\n \n   // Convert input FuncOp to LLVMFuncOp by using the LLVMTypeConverter provided\n   // to this legalization pattern.\n   LLVM::LLVMFuncOp\n-  convertFuncOpToLLVMFuncOp(FuncOp funcOp,\n+  convertFuncOpToLLVMFuncOp(func::FuncOp funcOp,\n                             ConversionPatternRewriter &rewriter) const {\n     // Convert the original function arguments. They are converted using the\n     // LLVMTypeConverter provided to this legalization pattern.\n     auto varargsAttr = funcOp->getAttrOfType<BoolAttr>(\"func.varargs\");\n     TypeConverter::SignatureConversion result(funcOp.getNumArguments());\n     auto llvmType = getTypeConverter()->convertFunctionSignature(\n-        funcOp.getType(), varargsAttr && varargsAttr.getValue(), result);\n+        funcOp.getFunctionType(), varargsAttr && varargsAttr.getValue(),\n+        result);\n     if (!llvmType)\n       return nullptr;\n \n     // Propagate argument/result attributes to all converted arguments/result\n     // obtained after converting a given original argument/result.\n     SmallVector<NamedAttribute, 4> attributes;\n-    filterFuncAttributes(funcOp->getAttrs(), /*filterArgAttrs=*/true,\n-                         attributes);\n+    filterFuncAttributes(funcOp, /*filterArgAttrs=*/true, attributes);\n     if (ArrayAttr resAttrDicts = funcOp.getAllResultAttrs()) {\n       assert(!resAttrDicts.empty() && \"expected array to be non-empty\");\n       auto newResAttrDicts =\n           (funcOp.getNumResults() == 1)\n               ? resAttrDicts\n               : rewriter.getArrayAttr(\n                     {wrapAsStructAttrs(rewriter, resAttrDicts)});\n-      attributes.push_back(rewriter.getNamedAttr(\n-          FunctionOpInterface::getResultDictAttrName(), newResAttrDicts));\n+      attributes.push_back(\n+          rewriter.getNamedAttr(funcOp.getResAttrsAttrName(), newResAttrDicts));\n     }\n     if (ArrayAttr argAttrDicts = funcOp.getAllArgAttrs()) {\n       SmallVector<Attribute, 4> newArgAttrs(\n@@ -90,9 +105,8 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<FuncOp> {\n         for (size_t j = 0; j < mapping->size; ++j)\n           newArgAttrs[mapping->inputNo + j] = argAttrDicts[i];\n       }\n-      attributes.push_back(\n-          rewriter.getNamedAttr(FunctionOpInterface::getArgDictAttrName(),\n-                                rewriter.getArrayAttr(newArgAttrs)));\n+      attributes.push_back(rewriter.getNamedAttr(\n+          funcOp.getArgAttrsAttrName(), rewriter.getArrayAttr(newArgAttrs)));\n     }\n     for (const auto &pair : llvm::enumerate(attributes)) {\n       if (pair.value().getName() == \"llvm.linkage\") {\n@@ -116,7 +130,7 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<FuncOp> {\n     }\n     auto newFuncOp = rewriter.create<LLVM::LLVMFuncOp>(\n         funcOp.getLoc(), funcOp.getName(), llvmType, linkage,\n-        /*dsoLocal*/ false, attributes);\n+        /*dsoLocal*/ false, LLVM::CConv::C, attributes);\n     rewriter.inlineRegionBefore(funcOp.getBody(), newFuncOp.getBody(),\n                                 newFuncOp.end());\n     if (failed(rewriter.convertRegionTypes(&newFuncOp.getBody(), *typeConverter,\n@@ -176,8 +190,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                                                const Allocation *allocation,\n                                                Value smem,\n                                                IndexCacheInfo indexCacheInfo)\n-      : converter(&typeConverter), indexCacheInfo(indexCacheInfo),\n-        allocation(allocation), smem(smem) {}\n+      : converter(&typeConverter), allocation(allocation), smem(smem),\n+        indexCacheInfo(indexCacheInfo) {}\n \n   LLVMTypeConverter *getTypeConverter() const { return converter; }\n \n@@ -199,6 +213,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n         ValueRange{rewriter.create<::mlir::gpu::ThreadIdOp>(\n             loc, rewriter.getIndexType(), ::mlir::gpu::Dimension::x)});\n     Value threadId = cast.getResult(0);\n+\n     return threadId;\n   }\n \n@@ -330,16 +345,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return ret;\n   }\n \n-  bool isMmaToDotShortcut(\n-      MmaEncodingAttr &mmaLayout,\n-      triton::gpu::DotOperandEncodingAttr &dotOperandLayout) const {\n-    // dot_op<opIdx=0, parent=#mma> = #mma\n-    // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n-    return mmaLayout.getWarpsPerCTA()[1] == 1 &&\n-           dotOperandLayout.getOpIdx() == 0 &&\n-           dotOperandLayout.getParent() == mmaLayout;\n-  }\n-\n   void storeDistributedToShared(Value src, Value llSrc,\n                                 ArrayRef<Value> dstStrides,\n                                 ArrayRef<SmallVector<Value>> srcIndices,\n@@ -386,8 +391,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n         getSwizzledSharedPtrs(loc, inVec, srcTy, dstSharedLayout, dstElemTy,\n                               smemObj, rewriter, offsetVals, srcStrides);\n \n-    std::map<unsigned, Value> cache0;\n-    std::map<unsigned, Value> cache1;\n     for (unsigned i = 0; i < numElems; ++i) {\n       if (i % minVec == 0)\n         word = undef(wordTy);\n@@ -690,24 +693,101 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   emitBaseIndexForMmaLayoutV1(Location loc, ConversionPatternRewriter &rewriter,\n                               const MmaEncodingAttr &mmaLayout,\n                               ArrayRef<int64_t> shape) const {\n-    llvm_unreachable(\"emitIndicesForMmaLayoutV1 not implemented\");\n+\n+    auto wpt = mmaLayout.getWarpsPerCTA();\n+    auto fpw = LLVM::DotOpMmaV1ConversionHelper::fpw;\n+    auto [isARow, isBRow, isAVec4, isBVec4, id] =\n+        mmaLayout.decodeVoltaLayoutStates();\n+\n+    Value thread = getThreadId(rewriter, loc);\n+    auto *ctx = thread.getContext();\n+    Value _1 = i32_val(1);\n+    Value _2 = i32_val(2);\n+    Value _4 = i32_val(4);\n+    Value _16 = i32_val(16);\n+    Value _32 = i32_val(32);\n+    Value _fpw0 = i32_val(fpw[0]);\n+    Value _fpw1 = i32_val(fpw[1]);\n+\n+    LLVM::DotOpMmaV1ConversionHelper::AParam aParam(isARow, isAVec4);\n+    LLVM::DotOpMmaV1ConversionHelper::BParam bParam(isBRow, isBVec4);\n+\n+    SmallVector<int, 2> rep({aParam.rep[0], bParam.rep[1]});\n+    SmallVector<int, 2> spw({aParam.spw[0], bParam.spw[1]});\n+    SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n+\n+    Value lane = urem(thread, _32);\n+    Value warp = udiv(thread, _32);\n+\n+    Value warp0 = urem(warp, i32_val(wpt[0]));\n+    Value warp12 = udiv(warp, i32_val(wpt[0]));\n+    Value warp1 = urem(warp12, i32_val(wpt[1]));\n+\n+    // warp offset\n+    Value offWarpM = mul(warp0, i32_val(spw[0]));\n+    Value offWarpN = mul(warp1, i32_val(spw[1]));\n+    // quad offset\n+    Value offQuadM = mul(udiv(and_(lane, _16), _4), _fpw0);\n+    Value offQuadN = mul(udiv(and_(lane, _16), _4), _fpw1);\n+    // pair offset\n+    Value offPairM = udiv(urem(lane, _16), _4);\n+    offPairM = urem(offPairM, _fpw0);\n+    offPairM = mul(offPairM, _4);\n+    Value offPairN = udiv(urem(lane, _16), _4);\n+    offPairN = udiv(offPairN, _fpw0);\n+    offPairN = urem(offPairN, _fpw1);\n+    offPairN = mul(offPairN, _4);\n+    offPairM = mul(offPairM, i32_val(rep[0] / 2));\n+    offQuadM = mul(offQuadM, i32_val(rep[0] / 2));\n+    offPairN = mul(offPairN, i32_val(rep[1] / 2));\n+    offQuadN = mul(offQuadN, i32_val(rep[1] / 2));\n+    // quad pair offset\n+    Value offLaneM = add(offPairM, offQuadM);\n+    Value offLaneN = add(offPairN, offQuadN);\n+    // a, b offset\n+    Value offsetAM = add(offWarpM, offLaneM);\n+    Value offsetBN = add(offWarpN, offLaneN);\n+    // m indices\n+    Value offsetCM = add(and_(lane, _1), offsetAM);\n+    // n indices\n+    Value offsetCN = add((and_(lane, _2)), (add(offWarpN, offPairN)));\n+    return {offsetCM, offsetCN};\n   }\n \n   SmallVector<SmallVector<unsigned>>\n   emitOffsetForMmaLayoutV1(const MmaEncodingAttr &mmaLayout,\n                            ArrayRef<int64_t> shape) const {\n-    SmallVector<SmallVector<unsigned>> ret;\n \n-    for (unsigned i = 0; i < shape[0]; i += getShapePerCTA(mmaLayout)[0]) {\n-      for (unsigned j = 0; j < shape[1]; j += getShapePerCTA(mmaLayout)[1]) {\n-        ret.push_back({i, j});\n-        ret.push_back({i, j + 1});\n-        ret.push_back({i + 2, j});\n-        ret.push_back({i + 2, j + 1});\n-        ret.push_back({i, j + 8});\n-        ret.push_back({i, j + 9});\n-        ret.push_back({i + 2, j + 8});\n-        ret.push_back({i + 2, j + 9});\n+    auto [isARow, isBRow, isAVec4, isBVec4, id] =\n+        mmaLayout.decodeVoltaLayoutStates();\n+    LLVM::DotOpMmaV1ConversionHelper::AParam aParam(isARow, isAVec4);\n+    LLVM::DotOpMmaV1ConversionHelper::BParam bParam(isBRow, isBVec4);\n+    auto wpt = mmaLayout.getWarpsPerCTA();\n+    auto fpw = LLVM::DotOpMmaV1ConversionHelper::fpw;\n+    SmallVector<int, 2> rep({aParam.rep[0], bParam.rep[1]});\n+    SmallVector<int, 2> spw({aParam.spw[0], bParam.spw[1]});\n+    SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n+\n+    SmallVector<unsigned> idxM;\n+    for (unsigned m = 0; m < shape[0]; m += shapePerCTA[0])\n+      for (unsigned mm = 0; mm < rep[0]; ++mm)\n+        idxM.push_back(m + mm * 2);\n+\n+    SmallVector<unsigned> idxN;\n+    for (int n = 0; n < shape[1]; n += shapePerCTA[1]) {\n+      for (int nn = 0; nn < rep[1]; ++nn) {\n+        idxN.push_back(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1]);\n+        idxN.push_back(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1] + 1);\n+      }\n+    }\n+\n+    SmallVector<SmallVector<unsigned>> ret;\n+    for (unsigned x1 : idxN) {   // N\n+      for (unsigned x0 : idxM) { // M\n+        SmallVector<unsigned> idx(2);\n+        idx[0] = x0; // M\n+        idx[1] = x1; // N\n+        ret.push_back(std::move(idx));\n       }\n     }\n     return ret;\n@@ -725,8 +805,9 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     Value warpSize = idx_val(32);\n     Value laneId = urem(threadId, warpSize);\n     Value warpId = udiv(threadId, warpSize);\n-    Value warpId0 = urem(warpId, warpsPerCTA[0]);\n-    Value warpId1 = urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]);\n+    Value warpId0 = urem(urem(warpId, warpsPerCTA[0]), idx_val(shape[0] / 16));\n+    Value warpId1 = urem(urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]),\n+                         idx_val(shape[1] / 8));\n     Value offWarp0 = mul(warpId0, idx_val(16));\n     Value offWarp1 = mul(warpId1, idx_val(8));\n \n@@ -754,14 +835,9 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n   // Emit indices calculation within each ConversionPattern, and returns a\n   // [elemsPerThread X rank] index matrix.\n-\n-  // TODO: [phil] redundant indices computation do not appear to hurt\n-  // performance much, but they could still significantly slow down\n-  // computations.\n   SmallVector<SmallVector<Value>> emitIndicesForDistributedLayout(\n       Location loc, ConversionPatternRewriter &rewriter,\n       const Attribute &layout, ArrayRef<int64_t> shape) const {\n-\n     // step 1, delinearize threadId to get the base index\n     auto multiDimBase = emitBaseIndexForLayout(loc, rewriter, layout, shape);\n     // step 2, get offset of each element\n@@ -775,7 +851,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     for (unsigned n = 0; n < elemsPerThread; ++n)\n       for (unsigned k = 0; k < rank; ++k)\n         multiDimIdx[n][k] = add(multiDimBase[k], idx_val(offset[n][k]));\n-\n     return multiDimIdx;\n   }\n \n@@ -785,7 +860,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                             ArrayRef<int64_t> shape) const {\n     auto parent = sliceLayout.getParent();\n     unsigned dim = sliceLayout.getDim();\n-    size_t rank = shape.size();\n     auto parentIndices =\n         emitIndices(loc, rewriter, parent, sliceLayout.paddedShape(shape));\n     unsigned numIndices = parentIndices.size();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 65, "deletions": 58, "changes": 123, "file_content_changes": "@@ -1,10 +1,11 @@\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n \n-#include \"mlir/Conversion/ArithmeticToLLVM/ArithmeticToLLVM.h\"\n+#include \"mlir/Analysis/DataFlowFramework.h\"\n+#include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n+#include \"mlir/Conversion/ControlFlowToLLVM//ControlFlowToLLVM.h\"\n #include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n #include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n-#include \"mlir/Conversion/SCFToStandard/SCFToStandard.h\"\n-#include \"mlir/Conversion/StandardToLLVM/ConvertStandardToLLVM.h\"\n+#include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n #include \"mlir/Pass/Pass.h\"\n@@ -40,7 +41,6 @@ class TritonLLVMConversionTarget : public ConversionTarget {\n     addIllegalDialect<triton::TritonDialect>();\n     addIllegalDialect<triton::gpu::TritonGPUDialect>();\n     addIllegalDialect<mlir::gpu::GPUDialect>();\n-    addIllegalDialect<mlir::StandardOpsDialect>();\n     addLegalOp<mlir::UnrealizedConversionCastOp>();\n   }\n };\n@@ -51,7 +51,7 @@ class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n       : ConversionTarget(ctx) {\n     addLegalDialect<LLVM::LLVMDialect>();\n     addLegalDialect<NVVM::NVVMDialect>();\n-    addIllegalOp<mlir::FuncOp>();\n+    addIllegalOp<mlir::func::FuncOp>();\n     addLegalOp<mlir::UnrealizedConversionCastOp>();\n   }\n };\n@@ -69,7 +69,7 @@ struct FuncOpConversion : public FuncOpConversionBase {\n       : FuncOpConversionBase(converter, benefit), numWarps(numWarps) {}\n \n   LogicalResult\n-  matchAndRewrite(FuncOp funcOp, OpAdaptor adaptor,\n+  matchAndRewrite(func::FuncOp funcOp, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     auto newFuncOp = convertFuncOpToLLVMFuncOp(funcOp, rewriter);\n     if (!newFuncOp)\n@@ -83,8 +83,7 @@ struct FuncOpConversion : public FuncOpConversionBase {\n \n     // Set an attribute for maxntidx, it could be used in latter LLVM codegen\n     // for `nvvm.annotation` metadata.\n-    newFuncOp->setAttr(\"nvvm.maxntid\",\n-                       rewriter.getIntegerAttr(i32_ty, 32 * numWarps));\n+    newFuncOp->setAttr(\"nvvm.maxntid\", rewriter.getI32ArrayAttr(32 * numWarps));\n \n     rewriter.eraseOp(funcOp);\n     return success();\n@@ -116,51 +115,51 @@ class ConvertTritonGPUToLLVM\n     // Step 1: Decompose unoptimized layout conversions to use shared memory\n     // Step 2: Decompose insert_slice_async to use load + insert_slice for\n     //   pre-Ampere architectures or unsupported vectorized load sizes\n-    // Step 3: Allocate shared memories and insert barriers\n-    // Step 4: Convert SCF to CFG\n+    // Step 3: Convert SCF to CFG\n+    // Step 4: Allocate shared memories and insert barriers\n     // Step 5: Convert FuncOp to LLVMFuncOp via partial conversion\n     // Step 6: Get axis and shared memory info\n     // Step 7: Convert the rest of ops via partial conversion\n     //\n-    // The reason for putting step 3 before step 4 is that the membar\n-    // analysis currently only supports SCF but not CFG. The reason for a\n-    // separation between 5/7 is that, step 6 is out of the scope of Dialect\n-    // Conversion, thus we need to make sure the smem is not revised during the\n-    // conversion of step 7.\n+    // The reason for a separation between 5/7 is that, step 6 is out of the\n+    // scope of Dialect Conversion, thus we need to make sure the smem is not\n+    // revised during the conversion of step 7.\n \n     // Step 1\n     decomposeMmaToDotOperand(mod, numWarps);\n     decomposeBlockedToDotOperand(mod);\n \n     // Step 2\n-    decomposeInsertSliceAsyncOp(mod);\n+    if (failed(decomposeInsertSliceAsyncOp(mod)))\n+      return signalPassFailure();\n \n     // Step 3\n+    RewritePatternSet scfPatterns(context);\n+    mlir::populateSCFToControlFlowConversionPatterns(scfPatterns);\n+    mlir::ConversionTarget scfTarget(*context);\n+    scfTarget.addIllegalOp<scf::ForOp, scf::IfOp, scf::ParallelOp, scf::WhileOp,\n+                           scf::ExecuteRegionOp>();\n+    scfTarget.markUnknownOpDynamicallyLegal([](Operation *) { return true; });\n+    if (failed(applyPartialConversion(mod, scfTarget, std::move(scfPatterns))))\n+      return signalPassFailure();\n+\n+    // Step 4\n     Allocation allocation(mod);\n     MembarAnalysis membarPass(&allocation);\n     membarPass.run();\n \n-    // Step 4\n-    RewritePatternSet scf_patterns(context);\n-    mlir::populateLoopToStdConversionPatterns(scf_patterns);\n-    mlir::ConversionTarget scf_target(*context);\n-    scf_target.addIllegalOp<scf::ForOp, scf::IfOp, scf::ParallelOp,\n-                            scf::WhileOp, scf::ExecuteRegionOp>();\n-    scf_target.markUnknownOpDynamicallyLegal([](Operation *) { return true; });\n-    if (failed(\n-            applyPartialConversion(mod, scf_target, std::move(scf_patterns))))\n-      return signalPassFailure();\n-\n     // Step 5\n-    RewritePatternSet func_patterns(context);\n-    func_patterns.add<FuncOpConversion>(typeConverter, numWarps, /*benefit=*/1);\n+    RewritePatternSet funcPatterns(context);\n+    funcPatterns.add<FuncOpConversion>(typeConverter, numWarps, /*benefit=*/1);\n     if (failed(\n-            applyPartialConversion(mod, funcTarget, std::move(func_patterns))))\n+            applyPartialConversion(mod, funcTarget, std::move(funcPatterns))))\n       return signalPassFailure();\n \n     // Step 6 - get axis and shared memory info\n-    AxisInfoAnalysis axisInfoAnalysis(mod.getContext());\n-    axisInfoAnalysis.run(mod);\n+    std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n+    AxisInfoAnalysis *axisInfoAnalysis = solver->load<AxisInfoAnalysis>();\n+    if (failed(solver->initializeAndRun(mod)))\n+      return signalPassFailure();\n     initSharedMemory(allocation.getSharedMemorySize(), typeConverter);\n     mod->setAttr(\"triton_gpu.shared\",\n                  mlir::IntegerAttr::get(mlir::IntegerType::get(context, 32),\n@@ -178,38 +177,38 @@ class ConvertTritonGPUToLLVM\n \n     // Normal conversions\n     populateTritonGPUToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                    axisInfoAnalysis, &allocation, smem,\n+                                    *axisInfoAnalysis, &allocation, smem,\n                                     indexCacheInfo, /*benefit=*/10);\n     // ConvertLayoutOp\n     populateConvertLayoutOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                          axisInfoAnalysis, &allocation, smem,\n+                                          *axisInfoAnalysis, &allocation, smem,\n                                           indexCacheInfo, /*benefit=*/10);\n     // DotOp\n     populateDotOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                axisInfoAnalysis, &allocation, smem,\n+                                *axisInfoAnalysis, &allocation, smem,\n                                 /*benefit=*/10);\n     // ElementwiseOp\n     populateElementwiseOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                        axisInfoAnalysis, &allocation, smem,\n+                                        *axisInfoAnalysis, &allocation, smem,\n                                         /*benefit=*/10);\n     // LoadStoreOp\n     populateLoadStoreOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                      axisInfoAnalysis, &allocation, smem,\n+                                      *axisInfoAnalysis, &allocation, smem,\n                                       indexCacheInfo, /*benefit=*/10);\n     // ReduceOp\n     populateReduceOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                   axisInfoAnalysis, &allocation, smem,\n+                                   *axisInfoAnalysis, &allocation, smem,\n                                    indexCacheInfo, /*benefit=*/10);\n     // ViewOp\n     populateViewOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                 axisInfoAnalysis, &allocation, smem,\n+                                 *axisInfoAnalysis, &allocation, smem,\n                                  /*benefit=*/10);\n \n     // Add arith/math's patterns to help convert scalar expression to LLVM.\n-    mlir::arith::populateArithmeticToLLVMConversionPatterns(typeConverter,\n-                                                            patterns);\n+    mlir::arith::populateArithToLLVMConversionPatterns(typeConverter, patterns);\n     mlir::populateMathToLLVMConversionPatterns(typeConverter, patterns);\n-    mlir::populateStdToLLVMConversionPatterns(typeConverter, patterns);\n+    mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n+                                                          patterns);\n     mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n \n     if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n@@ -240,7 +239,8 @@ class ConvertTritonGPUToLLVM\n     auto global = b.create<LLVM::GlobalOp>(\n         loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::External,\n         \"global_smem\", /*value=*/Attribute(), /*alignment=*/0,\n-        mlir::gpu::GPUDialect::getWorkgroupAddressSpace());\n+        // Add ROCm support.\n+        static_cast<unsigned>(NVVM::NVVMMemorySpace::kSharedMemorySpace));\n     SmallVector<LLVM::LLVMFuncOp> funcs;\n     mod.walk([&](LLVM::LLVMFuncOp func) { funcs.push_back(func); });\n     assert(funcs.size() == 1 &&\n@@ -306,9 +306,11 @@ class ConvertTritonGPUToLLVM\n     });\n   }\n \n-  void decomposeInsertSliceAsyncOp(ModuleOp mod) const {\n-    AxisInfoAnalysis axisInfoAnalysis(mod.getContext());\n-    axisInfoAnalysis.run(mod);\n+  LogicalResult decomposeInsertSliceAsyncOp(ModuleOp mod) const {\n+    std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n+    AxisInfoAnalysis *axisInfoAnalysis = solver->load<AxisInfoAnalysis>();\n+    if (failed(solver->initializeAndRun(mod)))\n+      return failure();\n     // TODO(Keren): This is a hacky knob that may cause performance regression\n     // when decomposition has been performed. We should remove this knob once we\n     // have thorough analysis on async wait. Currently, we decompose\n@@ -333,16 +335,16 @@ class ConvertTritonGPUToLLVM\n       OpBuilder builder(insertSliceAsyncOp);\n \n       // Get the vectorized load size\n-      auto src = insertSliceAsyncOp.src();\n-      auto dst = insertSliceAsyncOp.dst();\n+      auto src = insertSliceAsyncOp.getSrc();\n+      auto dst = insertSliceAsyncOp.getDst();\n       auto srcTy = src.getType().cast<RankedTensorType>();\n       auto dstTy = dst.getType().cast<RankedTensorType>();\n       auto srcBlocked =\n           srcTy.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>();\n       auto resSharedLayout =\n           dstTy.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n       auto resElemTy = dstTy.getElementType();\n-      unsigned inVec = axisInfoAnalysis.getPtrVectorSize(src);\n+      unsigned inVec = axisInfoAnalysis->getPtrContiguity(src);\n       unsigned outVec = resSharedLayout.getVec();\n       unsigned minVec = std::min(outVec, inVec);\n       auto maxBitWidth =\n@@ -362,24 +364,24 @@ class ConvertTritonGPUToLLVM\n       auto tmpTy =\n           RankedTensorType::get(srcTy.getShape(), resElemTy, srcBlocked);\n       auto loadOp = builder.create<triton::LoadOp>(\n-          insertSliceAsyncOp.getLoc(), tmpTy, insertSliceAsyncOp.src(),\n-          insertSliceAsyncOp.mask(), insertSliceAsyncOp.other(),\n-          insertSliceAsyncOp.cache(), insertSliceAsyncOp.evict(),\n-          insertSliceAsyncOp.isVolatile());\n+          insertSliceAsyncOp.getLoc(), tmpTy, insertSliceAsyncOp.getSrc(),\n+          insertSliceAsyncOp.getMask(), insertSliceAsyncOp.getOther(),\n+          insertSliceAsyncOp.getCache(), insertSliceAsyncOp.getEvict(),\n+          insertSliceAsyncOp.getIsVolatile());\n \n       // insert_slice\n-      auto axis = insertSliceAsyncOp.axis();\n+      auto axis = insertSliceAsyncOp.getAxis();\n       auto intAttr = [&](int64_t v) { return builder.getI64IntegerAttr(v); };\n       auto offsets = SmallVector<OpFoldResult>(dstTy.getRank(), intAttr(0));\n       auto sizes = SmallVector<OpFoldResult>(dstTy.getRank(), intAttr(1));\n       auto strides = SmallVector<OpFoldResult>(dstTy.getRank(), intAttr(1));\n-      offsets[axis] = insertSliceAsyncOp.index();\n+      offsets[axis] = insertSliceAsyncOp.getIndex();\n       for (size_t i = 0; i < dstTy.getRank(); i++) {\n         if (i != axis)\n           sizes[i] = intAttr(dstTy.getShape()[i]);\n       }\n       auto insertSliceOp = builder.create<tensor::InsertSliceOp>(\n-          insertSliceAsyncOp.getLoc(), loadOp, insertSliceAsyncOp.dst(),\n+          insertSliceAsyncOp.getLoc(), loadOp, insertSliceAsyncOp.getDst(),\n           offsets, sizes, strides);\n \n       // Replace\n@@ -388,18 +390,23 @@ class ConvertTritonGPUToLLVM\n       decomposed = true;\n     });\n \n+    mod.walk([&](triton::gpu::AsyncCommitGroupOp asyncCommitGroupOp) -> void {\n+      if (!triton::gpu::AsyncCommitGroupOp::isSupported(computeCapability))\n+        asyncCommitGroupOp.erase();\n+    });\n+\n     mod.walk([&](triton::gpu::AsyncWaitOp asyncWaitOp) -> void {\n       if (!triton::gpu::AsyncWaitOp::isSupported(computeCapability)) {\n         // async wait is supported in Ampere and later\n         asyncWaitOp.erase();\n       } else if (decomposed) {\n         // Wait for all previous async ops\n         OpBuilder builder(asyncWaitOp);\n-        auto newAsyncWaitOp =\n-            builder.create<triton::gpu::AsyncWaitOp>(asyncWaitOp.getLoc(), 0);\n+        builder.create<triton::gpu::AsyncWaitOp>(asyncWaitOp.getLoc(), 0);\n         asyncWaitOp.erase();\n       }\n     });\n+    return success();\n   }\n };\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.h", "status": "modified", "additions": 21, "deletions": 17, "changes": 38, "file_content_changes": "@@ -90,21 +90,29 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n         Type elemTy = convertType(type.getElementType());\n         if (mmaLayout.isAmpere()) {\n           const llvm::DenseMap<int, Type> targetTyMap = {\n-              {32, elemTy},\n+              {32, vec_ty(elemTy, 1)},\n               {16, vec_ty(elemTy, 2)},\n               {8, vec_ty(elemTy, 4)},\n           };\n           Type targetTy;\n           if (targetTyMap.count(elemTy.getIntOrFloatBitWidth())) {\n             targetTy = targetTyMap.lookup(elemTy.getIntOrFloatBitWidth());\n+            // <2xi16>/<4xi8> => i32\n+            // We are doing this because NVPTX inserts extra integer instrs to\n+            // pack & unpack vectors of sub-word integers\n+            // Note: this needs to be synced with\n+            //       DotOpMmaV2ConversionHelper::loadX4\n+            if (elemTy.isa<IntegerType>() &&\n+                (elemTy.getIntOrFloatBitWidth() == 8 ||\n+                 elemTy.getIntOrFloatBitWidth() == 16))\n+              targetTy = IntegerType::get(ctx, 32);\n           } else {\n             assert(false && \"Unsupported element type\");\n           }\n           if (dotOpLayout.getOpIdx() == 0) { // $a\n             auto elems =\n                 MMA16816ConversionHelper::getANumElemsPerThread(type, wpt[0]);\n-            return LLVM::LLVMStructType::getLiteral(\n-                ctx, SmallVector<Type>(elems, targetTy));\n+            return struct_ty(SmallVector<Type>(elems, targetTy));\n           }\n           if (dotOpLayout.getOpIdx() == 1) { // $b\n             auto elems =\n@@ -114,24 +122,20 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n         }\n \n         if (mmaLayout.isVolta()) {\n+          auto [isARow, isBRow, isAVec4, isBVec4, mmaId] =\n+              mmaLayout.decodeVoltaLayoutStates();\n           DotOpMmaV1ConversionHelper helper(mmaLayout);\n-\n-          // TODO[Superjomn]: Both transA and transB are not available here.\n-          bool trans = false;\n-          // TODO[Superjomn]: The order of A and B are not available here.\n-          SmallVector<unsigned> order({1, 0});\n-          if (trans) {\n-            std::swap(shape[0], shape[1]);\n-            std::swap(order[0], order[1]);\n-          }\n-\n           if (dotOpLayout.getOpIdx() == 0) { // $a\n-            int elems = helper.numElemsPerThreadA(shape, order);\n+            DotOpMmaV1ConversionHelper::AParam param(isARow, isAVec4);\n+            int elems =\n+                helper.numElemsPerThreadA(shape, isARow, isAVec4, param.vec);\n             Type x2Ty = vec_ty(elemTy, 2);\n             return struct_ty(SmallVector<Type>(elems, x2Ty));\n           }\n           if (dotOpLayout.getOpIdx() == 1) { // $b\n-            int elems = helper.numElemsPerThreadB(shape, order);\n+            DotOpMmaV1ConversionHelper::BParam param(isBRow, isBVec4);\n+            int elems =\n+                helper.numElemsPerThreadB(shape, isBRow, isBVec4, param.vec);\n             Type x2Ty = vec_ty(elemTy, 2);\n             return struct_ty(SmallVector<Type>(elems, x2Ty));\n           }\n@@ -140,10 +144,10 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n \n       llvm::errs() << \"Unexpected dot operand layout detected in \"\n                       \"TritonToLLVMTypeConverter\";\n-      return llvm::None;\n+      return std::nullopt;\n     }\n \n-    return llvm::None;\n+    return std::nullopt;\n   }\n };\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.cpp", "status": "added", "additions": 139, "deletions": 0, "changes": 139, "file_content_changes": "@@ -0,0 +1,139 @@\n+#include \"Utility.h\"\n+\n+namespace mlir {\n+\n+namespace LLVM {\n+using namespace mlir::triton;\n+\n+Value getStructFromElements(Location loc, ValueRange resultVals,\n+                            ConversionPatternRewriter &rewriter,\n+                            Type structType) {\n+  if (!structType.isa<LLVM::LLVMStructType>()) {\n+    return *resultVals.begin();\n+  }\n+\n+  Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structType);\n+  for (const auto &v : llvm::enumerate(resultVals)) {\n+    assert(v.value() && \"can not insert null values\");\n+    llvmStruct = insert_val(structType, llvmStruct, v.value(), v.index());\n+  }\n+  return llvmStruct;\n+}\n+\n+SmallVector<Value> getElementsFromStruct(Location loc, Value llvmStruct,\n+                                         ConversionPatternRewriter &rewriter) {\n+  if (llvmStruct.getType().isIntOrIndexOrFloat() ||\n+      llvmStruct.getType().isa<triton::PointerType>() ||\n+      llvmStruct.getType().isa<LLVM::LLVMPointerType>())\n+    return {llvmStruct};\n+  ArrayRef<Type> types =\n+      llvmStruct.getType().cast<LLVM::LLVMStructType>().getBody();\n+  SmallVector<Value> results(types.size());\n+  for (unsigned i = 0; i < types.size(); ++i) {\n+    Type type = types[i];\n+    results[i] = extract_val(type, llvmStruct, i);\n+  }\n+  return results;\n+}\n+\n+Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v) {\n+  auto i32ty = rewriter.getIntegerType(32);\n+  return rewriter.create<LLVM::ConstantOp>(loc, i32ty,\n+                                           IntegerAttr::get(i32ty, v));\n+}\n+\n+Value createConstantF32(Location loc, PatternRewriter &rewriter, float v) {\n+  auto type = type::f32Ty(rewriter.getContext());\n+  return rewriter.create<LLVM::ConstantOp>(loc, type,\n+                                           rewriter.getF32FloatAttr(v));\n+}\n+\n+Value createConstantF64(Location loc, PatternRewriter &rewriter, float v) {\n+  auto type = type::f64Ty(rewriter.getContext());\n+  return rewriter.create<LLVM::ConstantOp>(loc, type,\n+                                           rewriter.getF64FloatAttr(v));\n+}\n+\n+// Create an index type constant.\n+Value createIndexConstant(OpBuilder &builder, Location loc,\n+                          TypeConverter *converter, int64_t value) {\n+  Type ty = converter->convertType(builder.getIndexType());\n+  return builder.create<LLVM::ConstantOp>(loc, ty,\n+                                          builder.getIntegerAttr(ty, value));\n+}\n+\n+// Create an integer constant of \\param width bits.\n+Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n+                                int64_t value) {\n+  Type ty = builder.getIntegerType(width);\n+  return builder.create<LLVM::ConstantOp>(loc, ty,\n+                                          builder.getIntegerAttr(ty, value));\n+}\n+\n+SharedMemoryObject\n+getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n+                                ConversionPatternRewriter &rewriter) {\n+  auto elems = getElementsFromStruct(loc, llvmStruct, rewriter);\n+  auto rank = (elems.size() - 1) / 2;\n+  return {/*base=*/elems[0],\n+          /*strides=*/{elems.begin() + 1, elems.begin() + 1 + rank},\n+          /*offsets=*/{elems.begin() + 1 + rank, elems.end()}};\n+}\n+\n+SmallVector<Value>\n+getStridesFromShapeAndOrder(ArrayRef<int64_t> shape, ArrayRef<unsigned> order,\n+                            Location loc, ConversionPatternRewriter &rewriter) {\n+  auto rank = shape.size();\n+  SmallVector<Value> strides(rank);\n+  int64_t stride = 1;\n+  for (auto idx : order) {\n+    strides[idx] = i32_val(stride);\n+    stride *= shape[idx];\n+  }\n+  return strides;\n+}\n+\n+Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n+                  Value val, Value pred) {\n+  MLIRContext *ctx = rewriter.getContext();\n+  unsigned bits = val.getType().getIntOrFloatBitWidth();\n+  const char *c = bits == 64 ? \"l\" : (bits == 16 ? \"h\" : \"r\");\n+\n+  PTXBuilder builder;\n+  auto *ptrOpr = builder.newAddrOperand(ptr, \"r\");\n+  auto *valOpr = builder.newOperand(val, c);\n+  auto &st = builder.create<>(\"st\")->shared().b(bits);\n+  st(ptrOpr, valOpr).predicate(pred, \"b\");\n+  return builder.launch(rewriter, loc, void_ty(ctx));\n+}\n+\n+Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n+               int i) {\n+  unsigned bits = val.getType().getIntOrFloatBitWidth();\n+\n+  if (bits == 64) {\n+    Type vecTy = vec_ty(f32_ty, 2);\n+    Value vec = bitcast(val, vecTy);\n+    Value val0 = extract_element(f32_ty, vec, i32_val(0));\n+    Value val1 = extract_element(f32_ty, vec, i32_val(1));\n+    val0 = shflSync(loc, rewriter, val0, i);\n+    val1 = shflSync(loc, rewriter, val1, i);\n+    vec = undef(vecTy);\n+    vec = insert_element(vecTy, vec, val0, i32_val(0));\n+    vec = insert_element(vecTy, vec, val1, i32_val(1));\n+    return bitcast(vec, val.getType());\n+  }\n+\n+  PTXBuilder builder;\n+  auto &shfl = builder.create(\"shfl.sync\")->o(\"bfly\").o(\"b32\");\n+  auto *dOpr = builder.newOperand(\"=r\");\n+  auto *aOpr = builder.newOperand(val, \"r\");\n+  auto *bOpr = builder.newConstantOperand(i);\n+  auto *cOpr = builder.newConstantOperand(\"0x1f\");\n+  auto *maskOpr = builder.newConstantOperand(\"0xffffffff\");\n+  shfl(dOpr, aOpr, bOpr, cOpr, maskOpr);\n+  return builder.launch(rewriter, loc, val.getType(), false);\n+}\n+\n+} // namespace LLVM\n+} // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 33, "deletions": 133, "changes": 166, "file_content_changes": "@@ -95,6 +95,10 @@\n                             __VA_ARGS__)\n #define tid_val() getThreadId(rewriter, loc)\n \n+// Attributes\n+#define i32_arr_attr(...) rewriter.getI32ArrayAttr({__VA_ARGS__})\n+#define i64_arr_attr(...) rewriter.getI64ArrayAttr({__VA_ARGS__})\n+\n namespace mlir {\n namespace triton {\n \n@@ -133,8 +137,7 @@ llvm::SmallVector<T> getMultiDimIndex(T linearIndex, llvm::ArrayRef<T> shape,\n \n // Linearize supposing order is [0, 1, .. , n]\n template <typename T>\n-static T getLinearIndexImpl(llvm::ArrayRef<T> multiDimIndex,\n-                            llvm::ArrayRef<T> shape) {\n+T getLinearIndexImpl(llvm::ArrayRef<T> multiDimIndex, llvm::ArrayRef<T> shape) {\n   assert(multiDimIndex.size() == shape.size());\n   // shape: {a, b, c, d}  ->  accMul: {1, a, a*b, a*b*c}\n   size_t rank = shape.size();\n@@ -150,9 +153,8 @@ static T getLinearIndexImpl(llvm::ArrayRef<T> multiDimIndex,\n }\n \n template <typename T>\n-static T getLinearIndex(llvm::ArrayRef<T> multiDimIndex,\n-                        llvm::ArrayRef<T> shape,\n-                        llvm::ArrayRef<unsigned> order) {\n+T getLinearIndex(llvm::ArrayRef<T> multiDimIndex, llvm::ArrayRef<T> shape,\n+                 llvm::ArrayRef<unsigned> order) {\n   assert(shape.size() == order.size());\n   return getLinearIndexImpl<T>(reorder(multiDimIndex, order),\n                                reorder(shape, order));\n@@ -163,91 +165,34 @@ static T getLinearIndex(llvm::ArrayRef<T> multiDimIndex,\n namespace LLVM {\n using namespace mlir::triton;\n \n-static Value getStructFromElements(Location loc, ValueRange resultVals,\n-                                   ConversionPatternRewriter &rewriter,\n-                                   Type structType) {\n-  if (!structType.isa<LLVM::LLVMStructType>()) {\n-    return *resultVals.begin();\n-  }\n-\n-  Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structType);\n-  for (const auto &v : llvm::enumerate(resultVals)) {\n-    assert(v.value() && \"can not insert null values\");\n-    llvmStruct = insert_val(structType, llvmStruct, v.value(),\n-                            rewriter.getI64ArrayAttr(v.index()));\n-  }\n-  return llvmStruct;\n-}\n+Value getStructFromElements(Location loc, ValueRange resultVals,\n+                            ConversionPatternRewriter &rewriter,\n+                            Type structType);\n \n-static SmallVector<Value>\n-getElementsFromStruct(Location loc, Value llvmStruct,\n-                      ConversionPatternRewriter &rewriter) {\n-  if (llvmStruct.getType().isIntOrIndexOrFloat() ||\n-      llvmStruct.getType().isa<triton::PointerType>() ||\n-      llvmStruct.getType().isa<LLVM::LLVMPointerType>())\n-    return {llvmStruct};\n-  ArrayRef<Type> types =\n-      llvmStruct.getType().cast<LLVM::LLVMStructType>().getBody();\n-  SmallVector<Value> results(types.size());\n-  for (unsigned i = 0; i < types.size(); ++i) {\n-    Type type = types[i];\n-    results[i] = extract_val(type, llvmStruct, rewriter.getI64ArrayAttr(i));\n-  }\n-  return results;\n-}\n+SmallVector<Value> getElementsFromStruct(Location loc, Value llvmStruct,\n+                                         ConversionPatternRewriter &rewriter);\n \n-// Create a 32-bit integer constant.\n-static Value createConstantI32(Location loc, PatternRewriter &rewriter,\n-                               int32_t v) {\n-  auto i32ty = rewriter.getIntegerType(32);\n-  return rewriter.create<LLVM::ConstantOp>(loc, i32ty,\n-                                           IntegerAttr::get(i32ty, v));\n-}\n+/// Create a 32-bit integer constant.\n+Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v);\n \n-static Value createConstantF32(Location loc, PatternRewriter &rewriter,\n-                               float v) {\n-  auto type = type::f32Ty(rewriter.getContext());\n-  return rewriter.create<LLVM::ConstantOp>(loc, type,\n-                                           rewriter.getF32FloatAttr(v));\n-}\n+/// Create a 32-bit float constant.\n+Value createConstantF32(Location loc, PatternRewriter &rewriter, float v);\n \n-static Value createConstantF64(Location loc, PatternRewriter &rewriter,\n-                               float v) {\n-  auto type = type::f64Ty(rewriter.getContext());\n-  return rewriter.create<LLVM::ConstantOp>(loc, type,\n-                                           rewriter.getF64FloatAttr(v));\n-}\n+/// Create a 64-bit float constant.\n+Value createConstantF64(Location loc, PatternRewriter &rewriter, float v);\n \n-// Create an index type constant.\n-static Value createIndexConstant(OpBuilder &builder, Location loc,\n-                                 TypeConverter *converter, int64_t value) {\n-  Type ty = converter->convertType(builder.getIndexType());\n-  return builder.create<LLVM::ConstantOp>(loc, ty,\n-                                          builder.getIntegerAttr(ty, value));\n-}\n+/// Create an index type constant.\n+Value createIndexConstant(OpBuilder &builder, Location loc,\n+                          TypeConverter *converter, int64_t value);\n \n-// Create an integer constant of \\param width bits.\n-static Value createLLVMIntegerConstant(OpBuilder &builder, Location loc,\n-                                       short width, int64_t value) {\n-  Type ty = builder.getIntegerType(width);\n-  return builder.create<LLVM::ConstantOp>(loc, ty,\n-                                          builder.getIntegerAttr(ty, value));\n-}\n+/// Create an integer constant of \\param width bits.\n+Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n+                                int64_t value);\n \n /// Helper function to get strides from a given shape and its order\n-static SmallVector<Value>\n+SmallVector<Value>\n getStridesFromShapeAndOrder(ArrayRef<int64_t> shape, ArrayRef<unsigned> order,\n-                            Location loc, ConversionPatternRewriter &rewriter) {\n-  auto rank = shape.size();\n-  SmallVector<Value> strides(rank);\n-  int64_t stride = 1;\n-  for (auto idx : order) {\n-    strides[idx] = i32_val(stride);\n-    stride *= shape[idx];\n-  }\n-  return strides;\n-}\n-\n+                            Location loc, ConversionPatternRewriter &rewriter);\n struct SharedMemoryObject {\n   Value base; // i32 ptr. The start address of the shared memory object.\n   // We need to store strides as Values but not integers because the\n@@ -275,10 +220,7 @@ struct SharedMemoryObject {\n                      ConversionPatternRewriter &rewriter)\n       : base(base) {\n     strides = getStridesFromShapeAndOrder(shape, order, loc, rewriter);\n-\n-    for (auto idx : order) {\n-      offsets.emplace_back(i32_val(0));\n-    }\n+    offsets.append(order.size(), i32_val(0));\n   }\n \n   SmallVector<Value> getElems() const {\n@@ -311,57 +253,15 @@ struct SharedMemoryObject {\n   }\n };\n \n-static SharedMemoryObject\n+SharedMemoryObject\n getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n-                                ConversionPatternRewriter &rewriter) {\n-  auto elems = getElementsFromStruct(loc, llvmStruct, rewriter);\n-  auto rank = (elems.size() - 1) / 2;\n-  return {/*base=*/elems[0],\n-          /*strides=*/{elems.begin() + 1, elems.begin() + 1 + rank},\n-          /*offsets=*/{elems.begin() + 1 + rank, elems.end()}};\n-}\n+                                ConversionPatternRewriter &rewriter);\n \n-static Value storeShared(ConversionPatternRewriter &rewriter, Location loc,\n-                         Value ptr, Value val, Value pred) {\n-  MLIRContext *ctx = rewriter.getContext();\n-  unsigned bits = val.getType().getIntOrFloatBitWidth();\n-  const char *c = bits == 64 ? \"l\" : (bits == 16 ? \"h\" : \"r\");\n+Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n+                  Value val, Value pred);\n \n-  PTXBuilder builder;\n-  auto *ptrOpr = builder.newAddrOperand(ptr, \"r\");\n-  auto *valOpr = builder.newOperand(val, c);\n-  auto &st = builder.create<>(\"st\")->shared().b(bits);\n-  st(ptrOpr, valOpr).predicate(pred, \"b\");\n-  return builder.launch(rewriter, loc, void_ty(ctx));\n-}\n-\n-static Value shflSync(Location loc, ConversionPatternRewriter &rewriter,\n-                      Value val, int i) {\n-  unsigned bits = val.getType().getIntOrFloatBitWidth();\n-\n-  if (bits == 64) {\n-    Type vecTy = vec_ty(f32_ty, 2);\n-    Value vec = bitcast(val, vecTy);\n-    Value val0 = extract_element(f32_ty, vec, i32_val(0));\n-    Value val1 = extract_element(f32_ty, vec, i32_val(1));\n-    val0 = shflSync(loc, rewriter, val0, i);\n-    val1 = shflSync(loc, rewriter, val1, i);\n-    vec = undef(vecTy);\n-    vec = insert_element(vecTy, vec, val0, i32_val(0));\n-    vec = insert_element(vecTy, vec, val1, i32_val(1));\n-    return bitcast(vec, val.getType());\n-  }\n-\n-  PTXBuilder builder;\n-  auto &shfl = builder.create(\"shfl.sync\")->o(\"bfly\").o(\"b32\");\n-  auto *dOpr = builder.newOperand(\"=r\");\n-  auto *aOpr = builder.newOperand(val, \"r\");\n-  auto *bOpr = builder.newConstantOperand(i);\n-  auto *cOpr = builder.newConstantOperand(\"0x1f\");\n-  auto *maskOpr = builder.newConstantOperand(\"0xffffffff\");\n-  shfl(dOpr, aOpr, bOpr, cOpr, maskOpr);\n-  return builder.launch(rewriter, loc, val.getType(), false);\n-}\n+Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n+               int i);\n \n } // namespace LLVM\n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 15, "deletions": 94, "changes": 109, "file_content_changes": "@@ -29,100 +29,21 @@ struct SplatOpConversion\n                                   ConversionPatternRewriter &rewriter,\n                                   Location loc) {\n     auto tensorTy = resType.cast<RankedTensorType>();\n-    if (tensorTy.getEncoding().isa<BlockedEncodingAttr>() ||\n-        tensorTy.getEncoding().isa<SliceEncodingAttr>()) {\n-      auto srcType = typeConverter->convertType(elemType);\n-      auto llSrc = bitcast(constVal, srcType);\n-      size_t elemsPerThread = getElemsPerThread(tensorTy);\n-      llvm::SmallVector<Value> elems(elemsPerThread, llSrc);\n-      llvm::SmallVector<Type> elemTypes(elems.size(), srcType);\n-      auto structTy =\n-          LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);\n-\n-      return getStructFromElements(loc, elems, rewriter, structTy);\n-    } else if (auto dotLayout =\n-                   tensorTy.getEncoding()\n-                       .dyn_cast<triton::gpu::DotOperandEncodingAttr>()) {\n-      return convertSplatLikeOpWithDotOperandLayout(\n-          dotLayout, resType, elemType, constVal, typeConverter, rewriter, loc);\n-    } else if (auto mmaLayout =\n-                   tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>()) {\n-      return convertSplatLikeOpWithMmaLayout(\n-          mmaLayout, resType, elemType, constVal, typeConverter, rewriter, loc);\n-    } else\n-      assert(false && \"Unsupported layout found in ConvertSplatLikeOp\");\n-\n-    return {};\n-  }\n-\n-  static Value convertSplatLikeOpWithDotOperandLayout(\n-      const triton::gpu::DotOperandEncodingAttr &layout, Type resType,\n-      Type elemType, Value constVal, TypeConverter *typeConverter,\n-      ConversionPatternRewriter &rewriter, Location loc) {\n-    auto tensorTy = resType.cast<RankedTensorType>();\n-    auto shape = tensorTy.getShape();\n-    auto parent = layout.getParent();\n-    int numElems{};\n-    if (auto mmaLayout = parent.dyn_cast<MmaEncodingAttr>()) {\n-      if (mmaLayout.isAmpere()) {\n-        numElems = layout.getOpIdx() == 0\n-                       ? MMA16816ConversionHelper::getANumElemsPerThread(\n-                             tensorTy, mmaLayout.getWarpsPerCTA()[0])\n-                       : MMA16816ConversionHelper::getBNumElemsPerThread(\n-                             tensorTy, mmaLayout.getWarpsPerCTA()[1]);\n-      } else if (mmaLayout.isVolta()) {\n-        DotOpMmaV1ConversionHelper helper(mmaLayout);\n-        numElems = layout.getOpIdx() == 0\n-                       ? helper.numElemsPerThreadA(shape, {0, 1})\n-                       : helper.numElemsPerThreadB(shape, {0, 1});\n-      }\n-    } else if (auto blockedLayout = parent.dyn_cast<BlockedEncodingAttr>()) {\n-      numElems = DotOpFMAConversionHelper::getNumElemsPerThread(shape, layout);\n-    } else {\n-      assert(false && \"Unsupported layout found\");\n-    }\n-    auto structTy = LLVM::LLVMStructType::getLiteral(\n-        rewriter.getContext(), SmallVector<Type>(numElems, elemType));\n-    return getStructFromElements(loc, SmallVector<Value>(numElems, constVal),\n-                                 rewriter, structTy);\n-  }\n-\n-  static Value convertSplatLikeOpWithMmaLayout(\n-      const MmaEncodingAttr &layout, Type resType, Type elemType,\n-      Value constVal, TypeConverter *typeConverter,\n-      ConversionPatternRewriter &rewriter, Location loc) {\n-    auto tensorTy = resType.cast<RankedTensorType>();\n-    auto shape = tensorTy.getShape();\n-    if (layout.isAmpere()) {\n-      auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(tensorTy);\n-      size_t fcSize = 4 * repM * repN;\n-\n-      auto structTy = LLVM::LLVMStructType::getLiteral(\n-          rewriter.getContext(), SmallVector<Type>(fcSize, elemType));\n-      return getStructFromElements(loc, SmallVector<Value>(fcSize, constVal),\n-                                   rewriter, structTy);\n-    }\n-    if (layout.isVolta()) {\n-      DotOpMmaV1ConversionHelper helper(layout);\n-      int repM = helper.getRepM(shape[0]);\n-      int repN = helper.getRepN(shape[1]);\n-      // According to mma layout of v1, each thread process 8 elements.\n-      int elems = 8 * repM * repN;\n-\n-      auto structTy = LLVM::LLVMStructType::getLiteral(\n-          rewriter.getContext(), SmallVector<Type>(elems, elemType));\n-      return getStructFromElements(loc, SmallVector<Value>(elems, constVal),\n-                                   rewriter, structTy);\n-    }\n-\n-    assert(false && \"Unsupported mma layout found\");\n-    return {};\n+    auto srcType = typeConverter->convertType(elemType);\n+    auto llSrc = bitcast(constVal, srcType);\n+    size_t elemsPerThread = getElemsPerThread(tensorTy);\n+    llvm::SmallVector<Value> elems(elemsPerThread, llSrc);\n+    llvm::SmallVector<Type> elemTypes(elems.size(), srcType);\n+    auto structTy =\n+        LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);\n+\n+    return getStructFromElements(loc, elems, rewriter, structTy);\n   }\n \n   LogicalResult matchAndRewrite(triton::SplatOp op, OpAdaptor adaptor,\n                                 ConversionPatternRewriter &rewriter) const {\n     auto loc = op->getLoc();\n-    auto src = adaptor.src();\n+    auto src = adaptor.getSrc();\n     auto llStruct = convertSplatLikeOp(src.getType(), op.getType(), src,\n                                        getTypeConverter(), rewriter, loc);\n     rewriter.replaceOp(op, {llStruct});\n@@ -188,8 +109,8 @@ struct CatOpConversion : public ConvertTritonGPUOpToLLVMPattern<CatOp> {\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n     SmallVector<Type> types(elems, elemTy);\n     // unpack input values\n-    auto lhsVals = getElementsFromStruct(loc, adaptor.lhs(), rewriter);\n-    auto rhsVals = getElementsFromStruct(loc, adaptor.rhs(), rewriter);\n+    auto lhsVals = getElementsFromStruct(loc, adaptor.getLhs(), rewriter);\n+    auto rhsVals = getElementsFromStruct(loc, adaptor.getRhs(), rewriter);\n     // concatenate (and potentially reorder) values\n     SmallVector<Value> retVals;\n     for (Value v : lhsVals)\n@@ -214,7 +135,7 @@ struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n   LogicalResult\n   matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    // We cannot directly run `rewriter.replaceOp(op, adaptor.src())`\n+    // We cannot directly run `rewriter.replaceOp(op, adaptor.getSrc())`\n     // due to MLIR's restrictions\n     Location loc = op->getLoc();\n     auto resultTy = op.getType().template cast<RankedTensorType>();\n@@ -223,7 +144,7 @@ struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n     SmallVector<Type> types(elems, elemTy);\n     Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n-    auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n+    auto vals = getElementsFromStruct(loc, adaptor.getSrc(), rewriter);\n     Value view = getStructFromElements(loc, vals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n     return success();\n@@ -240,7 +161,7 @@ struct TransOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n     auto srcSmemObj =\n-        getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n+        getSharedMemoryObjectFromStruct(loc, adaptor.getSrc(), rewriter);\n     SmallVector<Value> dstStrides = {srcSmemObj.strides[1],\n                                      srcSmemObj.strides[0]};\n     SmallVector<Value> dstOffsets = {srcSmemObj.offsets[1],"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 157, "deletions": 66, "changes": 223, "file_content_changes": "@@ -1,10 +1,10 @@\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n \n-#include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n-#include \"mlir/Dialect/GPU/GPUDialect.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n+#include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n-#include \"mlir/Dialect/StandardOps/IR/Ops.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Transforms/DialectConversion.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n@@ -59,38 +59,40 @@ class ArithConstantPattern : public OpConversionPattern<arith::ConstantOp> {\n     Type retType = getTypeConverter()->convertType(op.getType());\n     auto value = adaptor.getValue().dyn_cast<DenseElementsAttr>();\n     assert(value);\n-    rewriter.replaceOpWithNewOp<arith::ConstantOp>(\n-        op, retType,\n-        value.reshape(retType) // This is a hack. We just want to add encoding\n-    );\n+    if (value.getElementType().isInteger(1) && value.isSplat())\n+      // Workaround until https://reviews.llvm.org/D133743 is included.\n+      value = DenseElementsAttr::get(retType, value.getSplatValue<bool>());\n+    else\n+      // This is a hack. We just want to add encoding\n+      value = value.reshape(retType);\n+    rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, retType, value);\n     return success();\n   }\n };\n \n-class ConvertArithmeticOp : public ConversionPattern {\n+class ConvertArithOp : public ConversionPattern {\n public:\n-  ConvertArithmeticOp(TritonGPUTypeConverter &typeConverter,\n-                      MLIRContext *context)\n+  ConvertArithOp(TritonGPUTypeConverter &typeConverter, MLIRContext *context)\n       : ConversionPattern(typeConverter, MatchAnyOpTypeTag(), /*benefit=*/1,\n                           context) {}\n \n   LogicalResult\n   matchAndRewrite(Operation *op, ArrayRef<Value> operands,\n                   ConversionPatternRewriter &rewriter) const override {\n     Dialect *dialect = op->getDialect();\n-    if (dialect->getTypeID() != mlir::TypeID::get<arith::ArithmeticDialect>())\n+    if (dialect->getTypeID() != mlir::TypeID::get<arith::ArithDialect>())\n       return failure();\n     return success();\n   }\n };\n \n-void populateArithmeticPatternsAndLegality(\n-    TritonGPUTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    TritonGPUConversionTarget &target) {\n+void populateArithPatternsAndLegality(TritonGPUTypeConverter &typeConverter,\n+                                      RewritePatternSet &patterns,\n+                                      TritonGPUConversionTarget &target) {\n   // --------------\n   // Add legality and rewrite pattern rules for operations\n-  // from the Arithmetic dialect. The basic premise is that\n-  // arithmetic operations require both inputs to have the same\n+  // from the Arith dialect. The basic premise is that\n+  // Arith operations require both inputs to have the same\n   // non-null encoding\n   // --------------\n   MLIRContext *context = patterns.getContext();\n@@ -127,12 +129,12 @@ void populateArithmeticPatternsAndLegality(\n }\n \n // this shouldn't exist if mlir's SelectOp checked encodings properly\n-class StdSelectPattern : public OpConversionPattern<SelectOp> {\n+class StdSelectPattern : public OpConversionPattern<arith::SelectOp> {\n public:\n-  using OpConversionPattern<SelectOp>::OpConversionPattern;\n+  using OpConversionPattern<arith::SelectOp>::OpConversionPattern;\n \n   LogicalResult\n-  matchAndRewrite(SelectOp op, typename SelectOp::Adaptor adaptor,\n+  matchAndRewrite(arith::SelectOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n     rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n@@ -148,8 +150,8 @@ void populateStdPatternsAndLegality(TritonGPUTypeConverter &typeConverter,\n   MLIRContext *context = patterns.getContext();\n   // Rewrite rule\n   patterns.add<StdSelectPattern>(typeConverter, context);\n-  target.addLegalOp<ReturnOp>(); // this is ok because all functions are inlined\n-                                 // by the frontend\n+  target.addLegalOp<func::ReturnOp>(); // this is ok because all functions are\n+                                       // inlined by the frontend\n }\n \n void populateMathPatternsAndLegality(TritonGPUTypeConverter &typeConverter,\n@@ -175,7 +177,7 @@ struct TritonMakeRangePattern\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = getTypeConverter()->convertType(op.getType());\n     rewriter.replaceOpWithNewOp<triton::MakeRangeOp>(\n-        op, retType, adaptor.start(), adaptor.end());\n+        op, retType, adaptor.getStart(), adaptor.getEnd());\n     return success();\n   }\n };\n@@ -188,21 +190,22 @@ struct TritonExpandDimsPattern\n   matchAndRewrite(triton::ExpandDimsOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     // Type retType = op.getType());\n-    RankedTensorType argType = adaptor.src().getType().cast<RankedTensorType>();\n+    RankedTensorType argType =\n+        adaptor.getSrc().getType().cast<RankedTensorType>();\n     Attribute _argEncoding = argType.getEncoding();\n     if (!_argEncoding)\n       return failure();\n     auto argEncoding = _argEncoding.cast<triton::gpu::BlockedEncodingAttr>();\n     // return shape\n     auto retShape = argType.getShape().vec();\n-    retShape.insert(retShape.begin() + op.axis(), 1);\n+    retShape.insert(retShape.begin() + op.getAxis(), 1);\n     // return encoding\n     auto retSizePerThread = argEncoding.getSizePerThread().vec();\n-    retSizePerThread.insert(retSizePerThread.begin() + op.axis(), 1);\n+    retSizePerThread.insert(retSizePerThread.begin() + op.getAxis(), 1);\n     auto retThreadsPerWarp = argEncoding.getThreadsPerWarp().vec();\n-    retThreadsPerWarp.insert(retThreadsPerWarp.begin() + op.axis(), 1);\n+    retThreadsPerWarp.insert(retThreadsPerWarp.begin() + op.getAxis(), 1);\n     auto retWarpsPerCTA = argEncoding.getWarpsPerCTA().vec();\n-    retWarpsPerCTA.insert(retWarpsPerCTA.begin() + op.axis(), 1);\n+    retWarpsPerCTA.insert(retWarpsPerCTA.begin() + op.getAxis(), 1);\n     SmallVector<unsigned, 4> retOrder(retShape.size());\n     std::iota(retOrder.begin(), retOrder.end(), 0);\n     triton::gpu::BlockedEncodingAttr retEncoding =\n@@ -211,14 +214,14 @@ struct TritonExpandDimsPattern\n                                               retOrder);\n     // convert operand to slice of return type\n     Attribute newArgEncoding = triton::gpu::SliceEncodingAttr::get(\n-        getContext(), op.axis(), retEncoding);\n+        getContext(), op.getAxis(), retEncoding);\n     RankedTensorType newArgType = RankedTensorType::get(\n         argType.getShape(), argType.getElementType(), newArgEncoding);\n     // construct new op\n     auto newSrc = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        op.getLoc(), newArgType, adaptor.src());\n+        op.getLoc(), newArgType, adaptor.getSrc());\n     rewriter.replaceOpWithNewOp<triton::ExpandDimsOp>(op, newSrc,\n-                                                      adaptor.axis());\n+                                                      adaptor.getAxis());\n     return success();\n   }\n };\n@@ -245,15 +248,15 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n     RankedTensorType retType =\n         RankedTensorType::get(origShape, origType.getElementType(), dEncoding);\n     // a & b must be of smem layout\n-    auto aType = adaptor.a().getType().cast<RankedTensorType>();\n-    auto bType = adaptor.b().getType().cast<RankedTensorType>();\n+    auto aType = adaptor.getA().getType().cast<RankedTensorType>();\n+    auto bType = adaptor.getB().getType().cast<RankedTensorType>();\n     Attribute aEncoding = aType.getEncoding();\n     Attribute bEncoding = bType.getEncoding();\n     if (!aEncoding || !bEncoding)\n       return failure();\n-    Value a = adaptor.a();\n-    Value b = adaptor.b();\n-    Value c = adaptor.c();\n+    Value a = adaptor.getA();\n+    Value b = adaptor.getB();\n+    Value c = adaptor.getC();\n     if (!aEncoding.isa<triton::gpu::DotOperandEncodingAttr>()) {\n       Attribute encoding =\n           triton::gpu::DotOperandEncodingAttr::get(getContext(), 0, dEncoding);\n@@ -271,7 +274,7 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n     c = rewriter.create<triton::gpu::ConvertLayoutOp>(c.getLoc(), retType, c);\n \n     rewriter.replaceOpWithNewOp<triton::DotOp>(op, retType, a, b, c,\n-                                               adaptor.allowTF32());\n+                                               adaptor.getAllowTF32());\n     return success();\n   }\n };\n@@ -299,7 +302,7 @@ struct TritonTransPattern : public OpConversionPattern<triton::TransOp> {\n   LogicalResult\n   matchAndRewrite(triton::TransOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    Value src = adaptor.src();\n+    Value src = adaptor.getSrc();\n     auto srcType = src.getType().cast<RankedTensorType>();\n     Attribute srcEncoding = srcType.getEncoding();\n     if (!srcEncoding)\n@@ -331,9 +334,9 @@ struct TritonLoadPattern : public OpConversionPattern<triton::LoadOp> {\n   matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     rewriter.replaceOpWithNewOp<triton::LoadOp>(\n-        op, typeConverter->convertType(op.getType()), adaptor.ptr(),\n-        adaptor.mask(), adaptor.other(), adaptor.cache(), adaptor.evict(),\n-        adaptor.isVolatile());\n+        op, typeConverter->convertType(op.getType()), adaptor.getPtr(),\n+        adaptor.getMask(), adaptor.getOther(), adaptor.getCache(),\n+        adaptor.getEvict(), adaptor.getIsVolatile());\n     return success();\n   }\n };\n@@ -345,7 +348,8 @@ struct TritonStorePattern : public OpConversionPattern<triton::StoreOp> {\n   matchAndRewrite(triton::StoreOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     rewriter.replaceOpWithNewOp<triton::StoreOp>(\n-        op, adaptor.ptr(), adaptor.value(), adaptor.mask());\n+        op, adaptor.getPtr(), adaptor.getValue(), adaptor.getMask(),\n+        adaptor.getCache(), adaptor.getEvict());\n     return success();\n   }\n };\n@@ -358,8 +362,8 @@ struct TritonAtomicCASPattern\n   matchAndRewrite(triton::AtomicCASOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     rewriter.replaceOpWithNewOp<triton::AtomicCASOp>(\n-        op, typeConverter->convertType(op.getType()), adaptor.ptr(),\n-        adaptor.cmp(), adaptor.val());\n+        op, typeConverter->convertType(op.getType()), adaptor.getPtr(),\n+        adaptor.getCmp(), adaptor.getVal());\n     return success();\n   }\n };\n@@ -372,8 +376,8 @@ struct TritonAtomicRMWPattern\n   matchAndRewrite(triton::AtomicRMWOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     rewriter.replaceOpWithNewOp<triton::AtomicRMWOp>(\n-        op, typeConverter->convertType(op.getType()), adaptor.atomic_rmw_op(),\n-        adaptor.ptr(), adaptor.val(), adaptor.mask());\n+        op, typeConverter->convertType(op.getType()), adaptor.getAtomicRmwOp(),\n+        adaptor.getPtr(), adaptor.getVal(), adaptor.getMask());\n     return success();\n   }\n };\n@@ -386,8 +390,8 @@ struct TritonExtElemwisePattern\n   matchAndRewrite(triton::ExtElemwiseOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     rewriter.replaceOpWithNewOp<triton::ExtElemwiseOp>(\n-        op, typeConverter->convertType(op.getType()), adaptor.args(),\n-        adaptor.libname(), adaptor.libpath(), adaptor.symbol());\n+        op, typeConverter->convertType(op.getType()), adaptor.getArgs(),\n+        adaptor.getLibname(), adaptor.getLibpath(), adaptor.getSymbol());\n     return success();\n   }\n };\n@@ -413,7 +417,7 @@ struct TritonBroadcastPattern\n   LogicalResult\n   matchAndRewrite(BroadcastOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto srcType = adaptor.src().getType().cast<RankedTensorType>();\n+    auto srcType = adaptor.getSrc().getType().cast<RankedTensorType>();\n     auto srcEncoding = srcType.getEncoding();\n     if (!srcEncoding)\n       return failure();\n@@ -434,7 +438,7 @@ struct TritonReducePattern : public OpConversionPattern<triton::ReduceOp> {\n   matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     rewriter.replaceOpWithNewOp<triton::ReduceOp>(\n-        op, adaptor.redOp(), adaptor.operand(), adaptor.axis());\n+        op, adaptor.getRedOp(), adaptor.getOperand(), adaptor.getAxis());\n     return success();\n   }\n };\n@@ -445,7 +449,7 @@ struct TritonPrintfPattern : public OpConversionPattern<triton::PrintfOp> {\n   LogicalResult\n   matchAndRewrite(PrintfOp op, typename PrintfOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<triton::PrintfOp>(op, op.prefixAttr(),\n+    rewriter.replaceOpWithNewOp<triton::PrintfOp>(op, op.getPrefixAttr(),\n                                                   adaptor.getOperands());\n     return success();\n   }\n@@ -454,18 +458,19 @@ struct TritonPrintfPattern : public OpConversionPattern<triton::PrintfOp> {\n void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n                             RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n-  patterns.add< // TODO: view should have custom pattern that views the layout\n-      TritonGenericPattern<triton::ViewOp>,\n-      TritonGenericPattern<triton::BitcastOp>,\n-      TritonGenericPattern<triton::FpToFpOp>,\n-      TritonGenericPattern<triton::IntToPtrOp>,\n-      TritonGenericPattern<triton::PtrToIntOp>,\n-      TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n-      TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n-      TritonReducePattern, TritonTransPattern, TritonExpandDimsPattern,\n-      TritonMakeRangePattern, TritonDotPattern, TritonLoadPattern,\n-      TritonStorePattern, TritonExtElemwisePattern, TritonPrintfPattern,\n-      TritonAtomicRMWPattern>(typeConverter, context);\n+  patterns\n+      .insert< // TODO: view should have custom pattern that views the layout\n+          TritonGenericPattern<triton::ViewOp>,\n+          TritonGenericPattern<triton::BitcastOp>,\n+          TritonGenericPattern<triton::FpToFpOp>,\n+          TritonGenericPattern<triton::IntToPtrOp>,\n+          TritonGenericPattern<triton::PtrToIntOp>,\n+          TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n+          TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n+          TritonReducePattern, TritonTransPattern, TritonExpandDimsPattern,\n+          TritonMakeRangePattern, TritonDotPattern, TritonLoadPattern,\n+          TritonStorePattern, TritonExtElemwisePattern, TritonPrintfPattern,\n+          TritonAtomicRMWPattern>(typeConverter, context);\n }\n \n //\n@@ -496,7 +501,7 @@ struct SCFForPattern : public OpConversionPattern<scf::ForOp> {\n       return rewriter.notifyMatchFailure(op, \"could not convert body types\");\n     }\n     // Change the clone to use the updated operands. We could have cloned with\n-    // a BlockAndValueMapping, but this seems a bit more direct.\n+    // a IRMapping, but this seems a bit more direct.\n     newOp->setOperands(adaptor.getOperands());\n     // Update the result types to the new converted types.\n     SmallVector<Type> newResultTypes;\n@@ -573,13 +578,98 @@ class SCFIfPattern : public OpConversionPattern<scf::IfOp> {\n   }\n };\n \n+// This is borrowed from ConvertFIfOpTypes in\n+//    SCF/Transforms/StructuralTypeConversions.cpp\n+class SCFWhilePattern : public OpConversionPattern<scf::WhileOp> {\n+public:\n+  using OpConversionPattern<scf::WhileOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(scf::WhileOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto *converter = getTypeConverter();\n+    assert(converter);\n+    SmallVector<Type> newResultTypes;\n+    if (failed(converter->convertTypes(op.getResultTypes(), newResultTypes)))\n+      return failure();\n+\n+    auto newOp = rewriter.create<scf::WhileOp>(op.getLoc(), newResultTypes,\n+                                               adaptor.getOperands());\n+    for (auto i : {0u, 1u}) {\n+      auto &dstRegion = newOp.getRegion(i);\n+      rewriter.inlineRegionBefore(op.getRegion(i), dstRegion, dstRegion.end());\n+      if (failed(rewriter.convertRegionTypes(&dstRegion, *converter)))\n+        return rewriter.notifyMatchFailure(op, \"could not convert body types\");\n+    }\n+    rewriter.replaceOp(op, newOp.getResults());\n+    return success();\n+  }\n+};\n+\n+class SCFConditionPattern : public OpConversionPattern<scf::ConditionOp> {\n+public:\n+  using OpConversionPattern<scf::ConditionOp>::OpConversionPattern;\n+  LogicalResult\n+  matchAndRewrite(scf::ConditionOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    rewriter.updateRootInPlace(\n+        op, [&]() { op->setOperands(adaptor.getOperands()); });\n+    return success();\n+  }\n+};\n+\n void populateSCFPatterns(TritonGPUTypeConverter &typeConverter,\n                          RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n-  patterns.add<SCFYieldPattern, SCFForPattern, SCFIfPattern>(typeConverter,\n-                                                             context);\n+  patterns.add<SCFYieldPattern, SCFForPattern, SCFIfPattern, SCFWhilePattern,\n+               SCFConditionPattern>(typeConverter, context);\n }\n \n+// CF\n+\n+class CFBranchPattern : public OpConversionPattern<cf::BranchOp> {\n+public:\n+  using OpConversionPattern<cf::BranchOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(cf::BranchOp op, cf::BranchOp::Adaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    rewriter.replaceOpWithNewOp<cf::BranchOp>(op, op.getSuccessor(),\n+                                              adaptor.getOperands());\n+    return success();\n+  }\n+};\n+\n+class CFCondBranchPattern : public OpConversionPattern<cf::CondBranchOp> {\n+public:\n+  using OpConversionPattern<cf::CondBranchOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(cf::CondBranchOp op, cf::CondBranchOp::Adaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto converter = getTypeConverter();\n+    auto newOp = rewriter.replaceOpWithNewOp<cf::CondBranchOp>(\n+        op, adaptor.getCondition(), op.getTrueDest(),\n+        adaptor.getTrueDestOperands(), op.getFalseDest(),\n+        adaptor.getFalseDestOperands());\n+\n+    if (failed(rewriter.convertRegionTypes(newOp.getTrueDest()->getParent(),\n+                                           *converter)))\n+      return failure();\n+    if (failed(rewriter.convertRegionTypes(newOp.getFalseDest()->getParent(),\n+                                           *converter)))\n+      return failure();\n+    return success();\n+  }\n+};\n+\n+void populateCFPatterns(TritonGPUTypeConverter &typeConverter,\n+                        RewritePatternSet &patterns) {\n+  MLIRContext *context = patterns.getContext();\n+  patterns.add<CFBranchPattern, CFCondBranchPattern>(typeConverter, context);\n+}\n+//\n+\n class ConvertTritonToTritonGPU\n     : public ConvertTritonToTritonGPUBase<ConvertTritonToTritonGPU> {\n public:\n@@ -597,12 +687,13 @@ class ConvertTritonToTritonGPU\n     RewritePatternSet patterns(context);\n     // add rules\n     populateStdPatternsAndLegality(typeConverter, patterns, target);\n-    populateArithmeticPatternsAndLegality(typeConverter, patterns, target);\n+    populateArithPatternsAndLegality(typeConverter, patterns, target);\n     populateMathPatternsAndLegality(typeConverter, patterns, target);\n     populateTritonPatterns(typeConverter, patterns);\n     // TODO: can we use\n     //    mlir::scf::populateSCFStructurealTypeConversionsAndLegality(...) here?\n     populateSCFPatterns(typeConverter, patterns);\n+    populateCFPatterns(typeConverter, patterns);\n \n     if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n       return signalPassFailure();"}, {"filename": "lib/Dialect/Triton/IR/CMakeLists.txt", "status": "modified", "additions": 3, "deletions": 7, "changes": 10, "file_content_changes": "@@ -10,11 +10,7 @@ add_mlir_dialect_library(TritonIR\n \n   LINK_LIBS PUBLIC\n   MLIRIR\n-  MLIRArithmetic\n-  MLIRSCF\n-\n-  # Since LLVM 15\n-  # MLIRFunc\n-  # else\n-  MLIRStandard\n+  MLIRArithDialect\n+  MLIRSCFDialect\n+  MLIRFuncDialect\n )"}, {"filename": "lib/Dialect/Triton/IR/Dialect.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -22,11 +22,11 @@ namespace {\n struct TritonInlinerInterface : public DialectInlinerInterface {\n   using DialectInlinerInterface::DialectInlinerInterface;\n   bool isLegalToInline(Region *dest, Region *src, bool wouldBeCloned,\n-                       BlockAndValueMapping &valueMapping) const final {\n+                       IRMapping &valueMapping) const final {\n     return true;\n   }\n   bool isLegalToInline(Operation *, Region *, bool wouldBeCloned,\n-                       BlockAndValueMapping &) const final {\n+                       IRMapping &) const final {\n     return true;\n   }\n };"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 33, "deletions": 30, "changes": 63, "file_content_changes": "@@ -1,10 +1,9 @@\n-#include \"triton/Dialect/Triton/IR/Dialect.h\"\n-#include \"triton/Dialect/Triton/IR/Types.h\"\n-\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/OperationSupport.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/Triton/IR/Types.h\"\n \n namespace mlir {\n namespace triton {\n@@ -38,8 +37,8 @@ static Type getPointerTypeSameShape(Type type) {\n }\n \n // Parser & printer for assembly forms\n-ParseResult parseLoadOp(OpAsmParser &parser, OperationState &result) {\n-  SmallVector<OpAsmParser::OperandType, 4> allOperands;\n+ParseResult LoadOp::parse(OpAsmParser &parser, OperationState &result) {\n+  SmallVector<OpAsmParser::UnresolvedOperand, 4> allOperands;\n   Type resultTypes[1];\n   SMLoc allOperandLoc = parser.getCurrentLocation();\n   if (parser.parseOperandList(allOperands) ||\n@@ -66,25 +65,25 @@ ParseResult parseLoadOp(OpAsmParser &parser, OperationState &result) {\n     return failure();\n   // Deduce operand_segment_sizes from the number of the operands.\n   auto operand_segment_sizesAttrName =\n-      LoadOp::operand_segment_sizesAttrName(result.name);\n+      LoadOp::getOperandSegmentSizesAttrName(result.name);\n   result.addAttribute(\n       operand_segment_sizesAttrName,\n-      parser.getBuilder().getI32VectorAttr({1, hasMask, hasOther}));\n+      parser.getBuilder().getDenseI32ArrayAttr({1, hasMask, hasOther}));\n   return success();\n }\n \n-void printLoadOp(OpAsmPrinter &printer, LoadOp loadOp) {\n+void LoadOp::print(OpAsmPrinter &printer) {\n   printer << \" \";\n-  printer << loadOp.getOperation()->getOperands();\n+  printer << getOperation()->getOperands();\n   // \"operand_segment_sizes\" can be deduced, so we don't print it.\n-  printer.printOptionalAttrDict(loadOp->getAttrs(),\n-                                {loadOp.operand_segment_sizesAttrName()});\n+  printer.printOptionalAttrDict(getOperation()->getAttrs(),\n+                                {getOperandSegmentSizesAttrName()});\n   printer << \" : \";\n-  printer.printStrippedAttrOrType(loadOp.result().getType());\n+  printer.printStrippedAttrOrType(getResult().getType());\n }\n \n-ParseResult parseStoreOp(OpAsmParser &parser, OperationState &result) {\n-  SmallVector<OpAsmParser::OperandType, 4> allOperands;\n+ParseResult StoreOp::parse(OpAsmParser &parser, OperationState &result) {\n+  SmallVector<OpAsmParser::UnresolvedOperand, 4> allOperands;\n   Type valueType;\n   SMLoc allOperandLoc = parser.getCurrentLocation();\n   if (parser.parseOperandList(allOperands) ||\n@@ -104,12 +103,12 @@ ParseResult parseStoreOp(OpAsmParser &parser, OperationState &result) {\n   return success();\n }\n \n-void printStoreOp(OpAsmPrinter &printer, StoreOp storeOp) {\n+void StoreOp::print(OpAsmPrinter &printer) {\n   printer << \" \";\n-  printer << storeOp.getOperation()->getOperands();\n-  printer.printOptionalAttrDict(storeOp->getAttrs(), /*elidedAttrs=*/{});\n+  printer << getOperation()->getOperands();\n+  printer.printOptionalAttrDict(getOperation()->getAttrs(), /*elidedAttrs=*/{});\n   printer << \" : \";\n-  printer.printStrippedAttrOrType(storeOp.value().getType());\n+  printer.printStrippedAttrOrType(getValue().getType());\n }\n \n } // namespace triton\n@@ -149,8 +148,11 @@ bool FpToFpOp::areCastCompatible(::mlir::TypeRange inputs,\n \n //-- StoreOp --\n void StoreOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n-                    ::mlir::Value ptr, ::mlir::Value value) {\n-  StoreOp::build(builder, state, ptr, value, mlir::Value());\n+                    ::mlir::Value ptr, ::mlir::Value value,\n+                    ::mlir::triton::CacheModifier cache,\n+                    ::mlir::triton::EvictionPolicy evict) {\n+  return StoreOp::build(builder, state, ptr, value, mlir::Value(), cache,\n+                        evict);\n }\n \n //-- LoadOp --\n@@ -193,15 +195,15 @@ void LoadOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n     }\n   }\n   state.addAttribute(\n-      operand_segment_sizesAttrName(state.name),\n-      builder.getI32VectorAttr({1, (mask ? 1 : 0), (other ? 1 : 0)}));\n+      getOperandSegmentSizesAttrName(state.name),\n+      builder.getDenseI32ArrayAttr({1, (mask ? 1 : 0), (other ? 1 : 0)}));\n   state.addAttribute(\n-      cacheAttrName(state.name),\n+      getCacheAttrName(state.name),\n       ::mlir::triton::CacheModifierAttr::get(builder.getContext(), cache));\n   state.addAttribute(\n-      evictAttrName(state.name),\n+      getEvictAttrName(state.name),\n       ::mlir::triton::EvictionPolicyAttr::get(builder.getContext(), evict));\n-  state.addAttribute(isVolatileAttrName(state.name),\n+  state.addAttribute(getIsVolatileAttrName(state.name),\n                      builder.getBoolAttr(isVolatile));\n   state.addTypes({resultType});\n }\n@@ -311,12 +313,13 @@ bool mlir::triton::ReduceOp::withIndex(mlir::triton::RedOp redOp) {\n }\n \n //-- SplatOp --\n-OpFoldResult SplatOp::fold(ArrayRef<Attribute> operands) {\n-  auto constOperand = src().getDefiningOp<arith::ConstantOp>();\n+OpFoldResult SplatOp::fold(FoldAdaptor adaptor) {\n+  auto constOperand = getSrc().getDefiningOp<arith::ConstantOp>();\n   if (!constOperand)\n     return {};\n   auto shapedType = getType().cast<ShapedType>();\n-  auto ret = SplatElementsAttr::get(shapedType, {constOperand.getValue()});\n+  auto ret = SplatElementsAttr::get(\n+      shapedType, ArrayRef<Attribute>(constOperand.getValue()));\n   return ret;\n }\n \n@@ -350,8 +353,8 @@ mlir::LogicalResult mlir::triton::ExpandDimsOp::inferReturnTypes(\n }\n \n //-- BroadcastOp --\n-OpFoldResult BroadcastOp::fold(ArrayRef<Attribute> operands) {\n-  auto constOperand = src().getDefiningOp<arith::ConstantOp>();\n+OpFoldResult BroadcastOp::fold(FoldAdaptor adaptor) {\n+  auto constOperand = getSrc().getDefiningOp<arith::ConstantOp>();\n   if (!constOperand)\n     return {};\n "}, {"filename": "lib/Dialect/Triton/IR/Traits.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -3,7 +3,7 @@\n static mlir::LogicalResult verifySameEncoding(mlir::Type tyA, mlir::Type tyB) {\n   using namespace mlir;\n   auto encA = tyA.dyn_cast<RankedTensorType>();\n-  auto encB = tyA.dyn_cast<RankedTensorType>();\n+  auto encB = tyB.dyn_cast<RankedTensorType>();\n   if (!encA || !encB)\n     return success();\n   return encA.getEncoding() == encB.getEncoding() ? success() : failure();"}, {"filename": "lib/Dialect/Triton/IR/Types.cpp", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -37,3 +37,15 @@ Type PointerType::parse(AsmParser &parser) {\n void PointerType::print(AsmPrinter &printer) const {\n   printer << \"<\" << getPointeeType() << \">\";\n }\n+\n+namespace mlir {\n+\n+unsigned getPointeeBitWidth(RankedTensorType tensorTy) {\n+  auto ptrTy = tensorTy.getElementType().cast<triton::PointerType>();\n+  auto pointeeType = ptrTy.getPointeeType();\n+  return pointeeType.isa<triton::Float8Type>()\n+             ? 8\n+             : pointeeType.getIntOrFloatBitWidth();\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 24, "deletions": 15, "changes": 39, "file_content_changes": "@@ -20,8 +20,8 @@ bool isZero(mlir::Value val) {\n     return true;\n   // broadcast(constant_0)\n   if (auto bc = val.getDefiningOp<mlir::triton::BroadcastOp>()) {\n-    if (mlir::matchPattern(bc.src(), mlir::m_Zero()) ||\n-        mlir::matchPattern(bc.src(), mlir::m_AnyZeroFloat()))\n+    if (mlir::matchPattern(bc.getSrc(), mlir::m_Zero()) ||\n+        mlir::matchPattern(bc.getSrc(), mlir::m_AnyZeroFloat()))\n       return true;\n   }\n   return false;\n@@ -48,6 +48,9 @@ DenseElementsAttr getConstantValue(Builder &builder, Attribute value,\n   return res;\n }\n \n+// TODO(csigg): remove after next LLVM integrate.\n+using FastMathFlags = arith::FastMathFlags;\n+\n #include \"TritonCombine.inc\"\n \n } // anonymous namespace\n@@ -57,25 +60,26 @@ DenseElementsAttr getConstantValue(Builder &builder, Attribute value,\n class CombineSelectMaskedLoadPattern : public mlir::RewritePattern {\n public:\n   CombineSelectMaskedLoadPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(mlir::SelectOp::getOperationName(), 3, context,\n-                             {triton::LoadOp::getOperationName()}) {}\n+      : mlir::RewritePattern(mlir::arith::SelectOp::getOperationName(), 3,\n+                             context, {triton::LoadOp::getOperationName()}) {}\n \n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n-    auto selectOp = llvm::dyn_cast<mlir::SelectOp>(op);\n+    auto selectOp = llvm::dyn_cast<mlir::arith::SelectOp>(op);\n     if (!selectOp)\n       return mlir::failure();\n \n     mlir::Value trueValue = selectOp.getTrueValue();\n     mlir::Value falseValue = selectOp.getFalseValue();\n+    mlir::Value condSelect = selectOp.getCondition();\n \n     auto *loadOpCandidate = trueValue.getDefiningOp();\n     auto loadOp = llvm::dyn_cast_or_null<triton::LoadOp>(loadOpCandidate);\n     if (!loadOp)\n       return mlir::failure();\n \n-    mlir::Value mask = loadOp.mask();\n+    mlir::Value mask = loadOp.getMask();\n     if (!mask)\n       return mlir::failure();\n \n@@ -85,9 +89,13 @@ class CombineSelectMaskedLoadPattern : public mlir::RewritePattern {\n     if (!broadcastOp)\n       return mlir::failure();\n \n+    auto broadcastCond = broadcastOp.getSrc();\n+    if (broadcastCond != condSelect)\n+      return mlir::failure();\n+\n     rewriter.replaceOpWithNewOp<triton::LoadOp>(\n-        op, loadOp.ptr(), loadOp.mask(), falseValue, loadOp.cache(),\n-        loadOp.evict(), loadOp.isVolatile());\n+        op, loadOp.getPtr(), loadOp.getMask(), falseValue, loadOp.getCache(),\n+        loadOp.getEvict(), loadOp.getIsVolatile());\n     return mlir::success();\n   }\n };\n@@ -102,7 +110,7 @@ struct CanonicalizeMaskedLoadPattern\n   mlir::LogicalResult\n   matchAndRewrite(triton::LoadOp loadOp,\n                   mlir::PatternRewriter &rewriter) const override {\n-    auto mask = loadOp.mask();\n+    auto mask = loadOp.getMask();\n     if (!mask)\n       return mlir::failure();\n \n@@ -118,14 +126,14 @@ struct CanonicalizeMaskedLoadPattern\n     if (splatMask.getSplatValue<IntegerAttr>().getValue() == true) {\n       // mask = splat(1)\n       rewriter.replaceOpWithNewOp<triton::LoadOp>(\n-          loadOp, loadOp.getType(), loadOp.ptr(), Value(), Value(),\n-          loadOp.cache(), loadOp.evict(), loadOp.isVolatile());\n+          loadOp, loadOp.getType(), loadOp.getPtr(), Value(), Value(),\n+          loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n     } else {\n       // mask = splat(0)\n \n       // If there's no \"other\", the value is \"undef\".  Perhaps we want to\n       // optimize it in the future.x\n-      auto otherVal = loadOp.other();\n+      auto otherVal = loadOp.getOther();\n       if (!otherVal)\n         return mlir::failure();\n       rewriter.replaceOp(loadOp, otherVal);\n@@ -149,7 +157,7 @@ struct CanonicalizeMaskedStorePattern\n   mlir::LogicalResult\n   matchAndRewrite(triton::StoreOp storeOp,\n                   mlir::PatternRewriter &rewriter) const override {\n-    auto mask = storeOp.mask();\n+    auto mask = storeOp.getMask();\n     if (!mask)\n       return mlir::failure();\n \n@@ -164,8 +172,9 @@ struct CanonicalizeMaskedStorePattern\n \n     if (splatMask.getSplatValue<IntegerAttr>().getValue() == true) {\n       // mask = splat(1)\n-      rewriter.replaceOpWithNewOp<triton::StoreOp>(storeOp, storeOp.ptr(),\n-                                                   storeOp.value());\n+      rewriter.replaceOpWithNewOp<triton::StoreOp>(\n+          storeOp, storeOp.getPtr(), storeOp.getValue(), storeOp.getCache(),\n+          storeOp.getEvict());\n     } else {\n       // mask = splat(0)\n       rewriter.eraseOp(storeOp);"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.td", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -1,9 +1,9 @@\n #ifndef TRITON_PATTERNS\n #define TRITON_PATTERNS\n \n-include \"mlir/Dialect/StandardOps/IR/Ops.td\"\n-include \"mlir/Dialect/Arithmetic/IR/ArithmeticOps.td\"\n+include \"mlir/Dialect/Arith/IR/ArithOps.td\"\n include \"triton/Dialect/Triton/IR/TritonOps.td\"\n+include \"mlir/IR/PatternBase.td\"\n \n \n // AddIOp(DotOp(a, b, c), d) and c==0 => DotOp(a, b, d)\n@@ -16,7 +16,7 @@ def CombineDotAddIPattern : Pat<\n         (TT_DotOp $a, $b, $d, $allowTF32),\n         [(Constraint<CPred<\"isZero($0)\">> $c)]>;\n def CombineDotAddFPattern : Pat<\n-        (Arith_AddFOp $d, (TT_DotOp:$res $a, $b, $c, $allowTF32)),\n+        (Arith_AddFOp $d, (TT_DotOp:$res $a, $b, $c, $allowTF32), $fastmath),\n         (TT_DotOp $a, $b, $d, $allowTF32),\n         [(Constraint<CPred<\"isZero($0)\">> $c)]>;\n \n@@ -25,15 +25,15 @@ def CombineDotAddIRevPattern : Pat<\n         (TT_DotOp $a, $b, $d, $allowTF32),\n         [(Constraint<CPred<\"isZero($0)\">> $c)]>;\n def CombineDotAddFRevPattern : Pat<\n-        (Arith_AddFOp (TT_DotOp:$res $a, $b, $c, $allowTF32), $d),\n+        (Arith_AddFOp (TT_DotOp:$res $a, $b, $c, $allowTF32), $d, $fastmath),\n         (TT_DotOp $a, $b, $d, $allowTF32),\n         [(Constraint<CPred<\"isZero($0)\">> $c)]>;\n \n // TODO: this fails for addptr(addptr(ptr, i32), i64)\n // Commented out until fixed\n // addptr(addptr(%ptr, %idx0), %idx1) => addptr(%ptr, AddI(%idx0, %idx1))\n-//   Note: leave (sub %c0, %c0) canceling to ArithmeticDialect\n-//         (ref: ArithmeticCanonicalization.td)\n+//   Note: leave (sub %c0, %c0) canceling to ArithDialect\n+//         (ref: ArithCanonicalization.td)\n // def CombineAddPtrPattern : Pat<\n //         (TT_AddPtrOp (TT_AddPtrOp $ptr, $idx0), $idx1),\n //         (TT_AddPtrOp $ptr, (Arith_AddIOp $idx0, $idx1))>;"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 41, "deletions": 42, "changes": 83, "file_content_changes": "@@ -1,14 +1,14 @@\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+\n #include <numeric>\n \n #include \"mlir/IR/DialectImplementation.h\"\n #include \"mlir/IR/OpImplementation.h\"\n #include \"triton/Analysis/Utility.h\"\n-#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.cpp.inc\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"llvm/ADT/TypeSwitch.h\"\n \n-#include \"triton/Dialect/TritonGPU/IR/Dialect.cpp.inc\"\n-\n using namespace mlir;\n using namespace mlir::triton::gpu;\n \n@@ -112,9 +112,7 @@ SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n     if (mmaLayout.isAmpere()) {\n       return {2, 2};\n     } else if (mmaLayout.isVolta()) {\n-      // Note: here the definition of sizePerThread is obscure, which doesn't\n-      // mean vecSize=4 can be supported in the last dimension.\n-      return {2, 4};\n+      return {1, 2};\n     } else {\n       llvm_unreachable(\"Unexpected mma version\");\n     }\n@@ -145,7 +143,7 @@ SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n   }\n }\n \n-SmallVector<unsigned> getContigPerThread(Attribute layout) {\n+SmallVector<unsigned> getContigPerThread(const Attribute &layout) {\n   if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     assert(mmaLayout.isVolta() || mmaLayout.isAmpere());\n     return {1, 2};\n@@ -173,7 +171,8 @@ SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout) {\n   return threads;\n }\n \n-SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n+SmallVector<unsigned> getShapePerCTA(const Attribute &layout,\n+                                     ArrayRef<int64_t> tensorShape) {\n   SmallVector<unsigned> shape;\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     for (unsigned d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n@@ -186,23 +185,28 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n     for (unsigned d = 0, n = getOrder(parent).size(); d < n; ++d) {\n       if (d == dim)\n         continue;\n-      shape.push_back(getShapePerCTA(parent)[d]);\n+      shape.push_back(getShapePerCTA(parent, tensorShape)[d]);\n     }\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     if (mmaLayout.isAmpere())\n       return {16 * mmaLayout.getWarpsPerCTA()[0],\n               8 * mmaLayout.getWarpsPerCTA()[1]};\n-    if (mmaLayout.isVolta())\n-      return {16 * mmaLayout.getWarpsPerCTA()[0],\n-              16 * mmaLayout.getWarpsPerCTA()[1]};\n+    if (mmaLayout.isVolta()) {\n+      assert(!tensorShape.empty() && \"Volta needs the tensorShape\");\n+      if (tensorShape.size() == 1) // must be SliceEncoding\n+        return {static_cast<unsigned>(tensorShape[0]),\n+                static_cast<unsigned>(tensorShape[0])};\n+      return {static_cast<unsigned>(tensorShape[0]),\n+              static_cast<unsigned>(tensorShape[1])};\n+    }\n     assert(0 && \"Unexpected MMA layout version found\");\n   } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n     auto parentLayout = dotLayout.getParent();\n     assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n     if (auto parentMmaLayout = parentLayout.dyn_cast<MmaEncodingAttr>()) {\n       assert(parentMmaLayout.isAmpere() &&\n              \"mmaLayout version = 1 is not implemented yet\");\n-      auto parentShapePerCTA = getShapePerCTA(parentLayout);\n+      auto parentShapePerCTA = getShapePerCTA(parentLayout, tensorShape);\n       auto opIdx = dotLayout.getOpIdx();\n       if (opIdx == 0) {\n         return {parentShapePerCTA[0], 16};\n@@ -215,16 +219,6 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n       assert(0 && \"DotOperandEncodingAttr non-MmaEncodingAttr parent not \"\n                   \"supported yet\");\n     }\n-  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    if (mmaLayout.isAmpere()) {\n-      return {16 * mmaLayout.getWarpsPerCTA()[0],\n-              8 * mmaLayout.getWarpsPerCTA()[1]};\n-    } else if (mmaLayout.isVolta()) {\n-      return {16 * mmaLayout.getWarpsPerCTA()[0],\n-              16 * mmaLayout.getWarpsPerCTA()[1]};\n-    } else {\n-      llvm_unreachable(\"Unexpected mma version\");\n-    }\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n   }\n@@ -372,7 +366,6 @@ template SmallVector<int64_t>\n SliceEncodingAttr::paddedShape<int64_t>(ArrayRef<int64_t> shape) const;\n \n unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n-  size_t rank = shape.size();\n   auto parent = getParent();\n   return ::getElemsPerThread(parent, paddedShape(shape));\n }\n@@ -384,11 +377,19 @@ unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n \n   int res = 0;\n   if (isVolta()) {\n-    unsigned mmasRow = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]);\n-    unsigned mmasCol = ceil<unsigned>(shape[1], 16 * getWarpsPerCTA()[1]);\n-    // Each warp-level mma884 will perform a m16xn16xk4 mma, thus get a m16xn16\n-    // matrix as result.\n-    res = mmasRow * mmasCol * (16 * 16 / 32);\n+    auto [isARow, isBRow, isAVec4, isBVec4, id] = decodeVoltaLayoutStates();\n+    static constexpr std::array<unsigned, 2> fpw{{2, 2}};\n+    unsigned packSize0 = (isARow || isAVec4) ? 1 : 2;\n+    unsigned packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+    unsigned repM = 2 * packSize0;\n+    unsigned repN = 2 * packSize1;\n+    unsigned spwM = fpw[0] * 4 * repM;\n+    unsigned spwN = fpw[1] * 4 * repN;\n+    unsigned wptM = getWarpsPerCTA()[0];\n+    unsigned wptN = getWarpsPerCTA()[1];\n+    unsigned resM = repM * std::max<int>(1, shape[0] / (spwM * wptM));\n+    unsigned resN = 2 * repN * std::max<int>(1, shape[1] / (spwN * wptN));\n+    res = resM * resN;\n   } else if (isAmpere()) {\n     unsigned elemsCol = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]) * 2;\n     unsigned elemsRow = ceil<unsigned>(shape[1], 8 * getWarpsPerCTA()[1]) * 2;\n@@ -653,9 +654,9 @@ void DotOperandEncodingAttr::print(mlir::AsmPrinter &printer) const {\n // InsertSliceAsyncOp\n //===----------------------------------------------------------------------===//\n \n-ParseResult parseInsertSliceAsyncOp(OpAsmParser &parser,\n-                                    OperationState &result) {\n-  SmallVector<OpAsmParser::OperandType, 8> allOperands;\n+ParseResult InsertSliceAsyncOp::parse(OpAsmParser &parser,\n+                                      OperationState &result) {\n+  SmallVector<OpAsmParser::UnresolvedOperand, 8> allOperands;\n   Type srcType, dstType;\n   SMLoc allOperandLoc = parser.getCurrentLocation();\n   if (parser.parseOperandList(allOperands) ||\n@@ -687,25 +688,23 @@ ParseResult parseInsertSliceAsyncOp(OpAsmParser &parser,\n \n   // Deduce operand_segment_sizes from the number of the operands.\n   auto operand_segment_sizesAttrName =\n-      InsertSliceAsyncOp::operand_segment_sizesAttrName(result.name);\n+      InsertSliceAsyncOp::getOperandSegmentSizesAttrName(result.name);\n   result.addAttribute(\n       operand_segment_sizesAttrName,\n-      parser.getBuilder().getI32VectorAttr({1, 1, 1, hasMask, hasOther}));\n+      parser.getBuilder().getDenseI32ArrayAttr({1, 1, 1, hasMask, hasOther}));\n   return success();\n }\n \n-void printInsertSliceAsyncOp(OpAsmPrinter &printer,\n-                             InsertSliceAsyncOp insertSliceAsyncOp) {\n+void InsertSliceAsyncOp::print(OpAsmPrinter &printer) {\n   printer << \" \";\n-  printer << insertSliceAsyncOp.getOperation()->getOperands();\n+  printer << getOperation()->getOperands();\n   // \"operand_segment_sizes\" can be deduced, so we don't print it.\n-  printer.printOptionalAttrDict(\n-      insertSliceAsyncOp->getAttrs(),\n-      {insertSliceAsyncOp.operand_segment_sizesAttrName()});\n+  printer.printOptionalAttrDict(getOperation()->getAttrs(),\n+                                {getOperandSegmentSizesAttrName()});\n   printer << \" : \";\n-  printer.printStrippedAttrOrType(insertSliceAsyncOp.src().getType());\n+  printer.printStrippedAttrOrType(getSrc().getType());\n   printer << \" -> \";\n-  printer.printStrippedAttrOrType(insertSliceAsyncOp.result().getType());\n+  printer.printStrippedAttrOrType(getResult().getType());\n }\n \n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 99, "deletions": 42, "changes": 141, "file_content_changes": "@@ -1,3 +1,4 @@\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n@@ -10,38 +11,61 @@ using namespace mlir::triton;\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n+template <class T> SmallVector<unsigned, 4> argSort(const T &arr) {\n+  SmallVector<unsigned, 4> ret(arr.size());\n+  std::iota(ret.begin(), ret.end(), 0);\n+  std::sort(ret.begin(), ret.end(),\n+            [&](unsigned x, unsigned y) { return arr[x] > arr[y]; });\n+  return ret;\n+}\n+\n+typedef DenseMap<Value, std::function<Type(Type)>> LayoutMap;\n+\n struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n   Attribute getCoalescedEncoding(AxisInfoAnalysis &axisInfo, Value ptr,\n                                  int numWarps) {\n     auto origType = ptr.getType().cast<RankedTensorType>();\n     // Get the shape of the tensor.\n     size_t rank = origType.getRank();\n-    AxisInfo info = axisInfo.lookupLatticeElement(ptr)->getValue();\n-    // Layout order in decreasing order of contiguity\n-    SmallVector<unsigned, 4> order(rank);\n-    std::iota(order.begin(), order.end(), 0);\n-    auto contiguity = info.getContiguity();\n-    std::sort(order.begin(), order.end(), [&](unsigned x, unsigned y) {\n-      return contiguity[x] > contiguity[y];\n-    });\n-\n+    dataflow::Lattice<AxisInfo> *latticeElement =\n+        axisInfo.getLatticeElement(ptr);\n+    AxisInfo info = latticeElement ? latticeElement->getValue() : AxisInfo();\n+    // Get the contiguity order of `ptr`\n+    auto order = argSort(info.getContiguity());\n+    // The desired divisibility is the maximum divisibility\n+    // among all dependent pointers who have the same order as\n+    // `ptr`\n+    SetVector<Value> withSameOrder;\n+    withSameOrder.insert(ptr);\n+    if (ptr.getDefiningOp())\n+      for (Operation *op : mlir::multiRootGetSlice(ptr.getDefiningOp())) {\n+        for (Value val : op->getResults()) {\n+          if (val.getType() != origType)\n+            continue;\n+          auto valInfo = axisInfo.getLatticeElement(val);\n+          auto currOrder = argSort(valInfo->getValue().getContiguity());\n+          if (order == currOrder)\n+            withSameOrder.insert(val);\n+        }\n+      }\n     int numElems = product(origType.getShape());\n     int numThreads = numWarps * 32;\n     int numElemsPerThread = std::max(numElems / numThreads, 1);\n-\n     // Thread tile size depends on memory alignment\n     SmallVector<unsigned, 4> sizePerThread(rank, 1);\n-    PointerType ptrType = origType.getElementType().cast<PointerType>();\n-    auto pointeeType = ptrType.getPointeeType();\n-    unsigned numBits = pointeeType.isa<triton::Float8Type>()\n-                           ? 8\n-                           : pointeeType.getIntOrFloatBitWidth();\n-    unsigned maxMultiple = info.getDivisibility(order[0]);\n-    unsigned maxContig = info.getContiguity(order[0]);\n-    unsigned alignment = std::min(maxMultiple, maxContig);\n-    unsigned perThread = std::min(alignment, 128 / numBits);\n+    unsigned elemNumBits = getPointeeBitWidth(origType);\n+    unsigned elemNumBytes = std::max(elemNumBits / 8, 1u);\n+    unsigned perThread = 1;\n+    for (Value val : withSameOrder) {\n+      AxisInfo info = axisInfo.getLatticeElement(val)->getValue();\n+      unsigned maxMultipleBytes = info.getDivisibility(order[0]);\n+      unsigned maxMultiple = std::max(maxMultipleBytes / elemNumBytes, 1u);\n+      unsigned maxContig = info.getContiguity(order[0]);\n+      unsigned alignment = std::min(maxMultiple, maxContig);\n+      unsigned currPerThread = std::min(alignment, 128 / elemNumBits);\n+      perThread = std::max(perThread, currPerThread);\n+    }\n     sizePerThread[order[0]] = std::min<int>(perThread, numElemsPerThread);\n-\n     SmallVector<unsigned> dims(rank);\n     std::iota(dims.begin(), dims.end(), 0);\n     // create encoding\n@@ -61,16 +85,12 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n   }\n \n   template <class T>\n-  void coalesceOp(AxisInfoAnalysis &axisInfo, Operation *op, Value ptr,\n+  void coalesceOp(LayoutMap &layoutMap, Operation *op, Value ptr,\n                   OpBuilder builder) {\n     RankedTensorType ty = ptr.getType().template dyn_cast<RankedTensorType>();\n     if (!ty)\n       return;\n-    auto mod = op->getParentOfType<ModuleOp>();\n-    int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n-\n-    AxisInfo info = axisInfo.lookupLatticeElement(ptr)->getValue();\n-    auto convertType = getTypeConverter(axisInfo, ptr, numWarps);\n+    auto convertType = layoutMap.lookup(ptr);\n     // convert operands\n     SmallVector<Value, 4> newArgs;\n     for (auto v : op->getOperands()) {\n@@ -105,9 +125,37 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n   void runOnOperation() override {\n     Operation *op = getOperation();\n     // Run axis info analysis\n-    AxisInfoAnalysis axisInfo(&getContext());\n-    axisInfo.run(op);\n-    OpBuilder builder(op);\n+    std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n+    AxisInfoAnalysis *axisInfo = solver->load<AxisInfoAnalysis>();\n+    if (failed(solver->initializeAndRun(op)))\n+      return signalPassFailure();\n+\n+    // For each i/o operation, we determine what layout\n+    // the pointers should have for best memory coalescing\n+    LayoutMap layoutMap;\n+    op->walk([&](Operation *curr) {\n+      Value ptr;\n+      if (auto op = dyn_cast<triton::LoadOp>(curr))\n+        ptr = op.getPtr();\n+      if (auto op = dyn_cast<triton::AtomicRMWOp>(curr))\n+        ptr = op.getPtr();\n+      if (auto op = dyn_cast<triton::AtomicCASOp>(curr))\n+        ptr = op.getPtr();\n+      if (auto op = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr))\n+        ptr = op.getSrc();\n+      if (auto op = dyn_cast<triton::StoreOp>(curr))\n+        ptr = op.getPtr();\n+      if (!ptr)\n+        return;\n+      RankedTensorType ty = ptr.getType().template dyn_cast<RankedTensorType>();\n+      if (!ty || !ty.getElementType().isa<PointerType>())\n+        return;\n+      AxisInfo info = axisInfo->getLatticeElement(ptr)->getValue();\n+      auto mod = curr->getParentOfType<ModuleOp>();\n+      int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+      auto convertType = getTypeConverter(*axisInfo, ptr, numWarps);\n+      layoutMap[ptr] = convertType;\n+    });\n \n     // For each memory op that has a layout L1:\n     // 1. Create a coalesced memory layout L2 of the pointer operands\n@@ -117,19 +165,28 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     // 4. Convert the output of this new memory op back to L1\n     // 5. Replace all the uses of the original memory op by the new one\n     op->walk([&](Operation *curr) {\n-      OpBuilder::InsertionGuard g(builder);\n-      builder.setInsertionPoint(curr);\n-      if (auto load = dyn_cast<triton::LoadOp>(curr))\n-        coalesceOp<triton::LoadOp>(axisInfo, curr, load.ptr(), builder);\n-      if (auto op = dyn_cast<triton::AtomicRMWOp>(curr))\n-        coalesceOp<triton::AtomicRMWOp>(axisInfo, curr, op.ptr(), builder);\n-      if (auto op = dyn_cast<triton::AtomicCASOp>(curr))\n-        coalesceOp<triton::AtomicCASOp>(axisInfo, curr, op.ptr(), builder);\n-      if (auto load = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr))\n-        coalesceOp<triton::gpu::InsertSliceAsyncOp>(axisInfo, curr, load.src(),\n-                                                    builder);\n-      if (auto store = dyn_cast<triton::StoreOp>(curr))\n-        coalesceOp<triton::StoreOp>(axisInfo, curr, store.ptr(), builder);\n+      OpBuilder builder(curr);\n+      if (auto load = dyn_cast<triton::LoadOp>(curr)) {\n+        coalesceOp<triton::LoadOp>(layoutMap, curr, load.getPtr(), builder);\n+        return;\n+      }\n+      if (auto op = dyn_cast<triton::AtomicRMWOp>(curr)) {\n+        coalesceOp<triton::AtomicRMWOp>(layoutMap, curr, op.getPtr(), builder);\n+        return;\n+      }\n+      if (auto op = dyn_cast<triton::AtomicCASOp>(curr)) {\n+        coalesceOp<triton::AtomicCASOp>(layoutMap, curr, op.getPtr(), builder);\n+        return;\n+      }\n+      if (auto load = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr)) {\n+        coalesceOp<triton::gpu::InsertSliceAsyncOp>(layoutMap, curr,\n+                                                    load.getSrc(), builder);\n+        return;\n+      }\n+      if (auto store = dyn_cast<triton::StoreOp>(curr)) {\n+        coalesceOp<triton::StoreOp>(layoutMap, curr, store.getPtr(), builder);\n+        return;\n+      }\n     });\n   }\n };"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 248, "deletions": 254, "changes": 502, "file_content_changes": "@@ -1,8 +1,8 @@\n #include \"Utility.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n-#include \"mlir/Dialect/SCF/SCF.h\"\n-#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/IRMapping.h\"\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"mlir/IR/Verifier.h\"\n@@ -95,7 +95,7 @@ class SimplifyReduceCvt : public mlir::RewritePattern {\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n     auto reduce = cast<triton::ReduceOp>(*op);\n-    auto reduceArg = dyn_cast<triton::gpu::ConvertLayoutOp>(\n+    auto reduceArg = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n         reduce.getOperand().getDefiningOp());\n     if (!reduceArg)\n       return mlir::failure();\n@@ -107,7 +107,8 @@ class SimplifyReduceCvt : public mlir::RewritePattern {\n             .isa<triton::gpu::MmaEncodingAttr>())\n       return mlir::failure();\n     auto newReduce = rewriter.create<triton::ReduceOp>(\n-        op->getLoc(), reduce.redOp(), reduceArg.getOperand(), reduce.axis());\n+        op->getLoc(), reduce.getRedOp(), reduceArg.getOperand(),\n+        reduce.getAxis());\n     if (isa<triton::gpu::ConvertLayoutOp>(\n             *reduceArg.getOperand().getDefiningOp()))\n       return mlir::failure();\n@@ -154,6 +155,12 @@ class SimplifyConversion : public mlir::RewritePattern {\n     // block argument\n     if (!arg)\n       return mlir::failure();\n+    // cvt(view) -> view\n+    if (auto view = dyn_cast<triton::ViewOp>(arg)) {\n+      rewriter.replaceOpWithNewOp<triton::ViewOp>(\n+          op, op->getResult(0).getType(), view.getResult());\n+      return mlir::success();\n+    }\n     // cvt(alloc_tensor(x), type2) -> alloc_tensor(x, type2)\n     auto alloc_tensor = dyn_cast<triton::gpu::AllocTensorOp>(arg);\n     if (alloc_tensor) {\n@@ -177,12 +184,13 @@ class SimplifyConversion : public mlir::RewritePattern {\n       OpBuilder::InsertionGuard guard(rewriter);\n       rewriter.setInsertionPoint(insert_slice);\n       auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          op->getLoc(), newType, insert_slice.dst());\n+          op->getLoc(), newType, insert_slice.getDst());\n       rewriter.replaceOpWithNewOp<triton::gpu::InsertSliceAsyncOp>(\n-          op, newType, insert_slice.src(), newArg.getResult(),\n-          insert_slice.index(), insert_slice.mask(), insert_slice.other(),\n-          insert_slice.cache(), insert_slice.evict(), insert_slice.isVolatile(),\n-          insert_slice.axis());\n+          op, newType, insert_slice.getSrc(), newArg.getResult(),\n+          insert_slice.getIndex(), insert_slice.getMask(),\n+          insert_slice.getOther(), insert_slice.getCache(),\n+          insert_slice.getEvict(), insert_slice.getIsVolatile(),\n+          insert_slice.getAxis());\n       return mlir::success();\n     }\n     // cvt(extract_slice(x), type2) -> extract_slice(cvt(x, type2))\n@@ -191,7 +199,8 @@ class SimplifyConversion : public mlir::RewritePattern {\n       if (!isSharedEncoding(op->getResult(0))) {\n         return mlir::failure();\n       }\n-      auto origType = extract_slice.source().getType().cast<RankedTensorType>();\n+      auto origType =\n+          extract_slice.getSource().getType().cast<RankedTensorType>();\n       auto newType = RankedTensorType::get(\n           origType.getShape(), origType.getElementType(),\n           op->getResult(0).getType().cast<RankedTensorType>().getEncoding());\n@@ -205,7 +214,7 @@ class SimplifyConversion : public mlir::RewritePattern {\n       OpBuilder::InsertionGuard guard(rewriter);\n       rewriter.setInsertionPoint(extract_slice);\n       auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          op->getLoc(), newType, extract_slice.source());\n+          op->getLoc(), newType, extract_slice.getSource());\n       rewriter.replaceOpWithNewOp<tensor::ExtractSliceOp>(\n           op, resType, newArg.getResult(), extract_slice.offsets(),\n           extract_slice.sizes(), extract_slice.strides(),\n@@ -238,13 +247,13 @@ class SimplifyConversion : public mlir::RewritePattern {\n     // cvt(type1, splat(type2, x)) -> splat(type1, x)\n     if (auto splat = llvm::dyn_cast<triton::SplatOp>(arg)) {\n       rewriter.replaceOpWithNewOp<triton::SplatOp>(op, op->getResultTypes(),\n-                                                   splat.src());\n+                                                   splat.getSrc());\n       return mlir::success();\n     }\n     // cvt(type1, make_range(type2, x)) -> make_range(type1, x)\n     if (auto range = llvm::dyn_cast<triton::MakeRangeOp>(arg)) {\n       rewriter.replaceOpWithNewOp<triton::MakeRangeOp>(\n-          op, op->getResultTypes(), range.start(), range.end());\n+          op, op->getResultTypes(), range.getStart(), range.getEnd());\n       return mlir::success();\n     }\n     // cvt(type, constant) -> constant\n@@ -269,48 +278,70 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n   ret = targetEncoding;\n   if (auto expand_dims = dyn_cast<triton::ExpandDimsOp>(op)) {\n     ret = triton::gpu::SliceEncodingAttr::get(\n-        op->getContext(), expand_dims.axis(), targetEncoding);\n+        op->getContext(), expand_dims.getAxis(), targetEncoding);\n   }\n   if (auto reduce = dyn_cast<triton::ReduceOp>(op)) {\n     auto sliceEncoding =\n         targetEncoding.dyn_cast<triton::gpu::SliceEncodingAttr>();\n     if (!sliceEncoding)\n       return failure();\n+    if (sliceEncoding.getDim() != reduce.getAxis())\n+      return failure();\n     ret = sliceEncoding.getParent();\n   }\n+  if (auto view = dyn_cast<triton::ViewOp>(op)) {\n+    return failure();\n+  }\n   return success();\n }\n \n-// TODO: Interface\n-LogicalResult getForwardEncoding(Attribute sourceEncoding, Operation *op,\n-                                 Attribute &ret) {\n-  if (op->hasTrait<mlir::OpTrait::Elementwise>()) {\n-    ret = sourceEncoding;\n-    return success();\n-  }\n-  if (isa<triton::ReduceOp>(op)) {\n-    ret = Attribute();\n-    return success();\n+inline bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n+  // Case 1: A size 1 tensor is not expensive since all threads will load the\n+  // same\n+  if (isSingleValue(op->getOperand(0)))\n+    return false;\n+  auto ptr = op->getOperand(0);\n+  // Case 2: We assume that `evict_last` loads/stores have high hit rate\n+  if (auto load = dyn_cast<triton::LoadOp>(op))\n+    if (load.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n+      return false;\n+  if (auto store = dyn_cast<triton::StoreOp>(op))\n+    if (store.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n+      return false;\n+  if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n+    auto encoding = tensorTy.getEncoding();\n+    // Case 3: Different type conversion is expensive (e.g., mma <-> block)\n+    if (encoding.getTypeID() != targetEncoding.getTypeID())\n+      return true;\n+    auto sizePerThread = triton::gpu::getSizePerThread(encoding);\n+    auto targetSizePerThread = triton::gpu::getSizePerThread(targetEncoding);\n+    auto order = triton::gpu::getOrder(encoding);\n+    auto targetOrder = triton::gpu::getOrder(targetEncoding);\n+    // Case 4: The targeEncoding may expose more vectorization opportunities\n+    return sizePerThread[order[0]] >= targetSizePerThread[targetOrder[0]];\n   }\n-  return failure();\n+  return false;\n }\n \n-inline bool expensive_to_remat(Operation *op) {\n+inline bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   if (!op)\n     return true;\n+  if (isa<triton::LoadOp, triton::StoreOp>(op))\n+    return expensiveLoadOrStore(op, targetEncoding);\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n-          triton::gpu::InsertSliceAsyncOp, triton::LoadOp, triton::StoreOp,\n-          triton::AtomicRMWOp, triton::AtomicCASOp, triton::DotOp>(op))\n+          triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n+          triton::AtomicCASOp, triton::DotOp>(op))\n     return true;\n-  if (isa<scf::YieldOp, scf::ForOp>(op))\n+  if (isa<scf::YieldOp, scf::ForOp, scf::IfOp, scf::WhileOp, scf::ConditionOp>(\n+          op))\n     return true;\n   return false;\n }\n \n LogicalResult simulateBackwardRematerialization(\n     Operation *initOp, SetVector<Operation *> &processed,\n     SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n-    Attribute targetEncoding) {\n+    const Attribute &targetEncoding) {\n   // DFS\n   std::vector<std::pair<Operation *, Attribute>> queue;\n   queue.emplace_back(initOp, targetEncoding);\n@@ -324,34 +355,41 @@ LogicalResult simulateBackwardRematerialization(\n     queue.pop_back();\n     // If the current operation is expensive to rematerialize,\n     // we stop everything\n-    if (expensive_to_remat(currOp))\n-      return mlir::failure();\n-    // we would propagate the conversion here\n+    if (expensiveToRemat(currOp, currLayout))\n+      break;\n+    // A conversion will be removed here (i.e. transferred to operands)\n     numCvts -= 1;\n-    // check if the conversion could be folded at this operation\n-    if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n-            triton::MakeRangeOp, triton::SplatOp>(*currOp))\n-      continue;\n-    // done processing\n+    // Done processing\n     processed.insert(currOp);\n     layout.insert(currLayout);\n-    // add all operands to the queue\n+    // Add all operands to the queue\n     for (Value argI : currOp->getOperands()) {\n       Attribute newEncoding;\n-      // cannot invert the current encoding for this operand\n+      // Cannot invert the current encoding for this operand\n       // we stop everything\n-      if (failed(invertEncoding(currLayout, currOp, newEncoding))) {\n+      if (failed(invertEncoding(currLayout, currOp, newEncoding)))\n         return mlir::failure();\n-      }\n       if (toConvert.count(argI) && toConvert[argI] != newEncoding)\n         return mlir::failure();\n-      //\n       Operation *opArgI = argI.getDefiningOp();\n       toConvert.insert({argI, newEncoding});\n-      if (!opArgI || processed.contains(opArgI) ||\n-          (opArgI->getBlock() != initOp->getBlock()))\n+      // 1. Only convert RankedTensorType\n+      // 2. Skip if there's no defining op\n+      // 3. Skip if the defining op has already been processed\n+      // 4. Skip or the defining op is in a different block\n+      if (!argI.getType().isa<RankedTensorType>() || !opArgI ||\n+          processed.contains(opArgI) ||\n+          opArgI->getBlock() != currOp->getBlock())\n+        continue;\n+      // If the conversion can be folded into opArgI then\n+      // we don't count this conversion as expensive\n+      if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n+              triton::MakeRangeOp, triton::SplatOp>(*opArgI))\n         continue;\n-      // we add one expensive conversion for the current operand\n+      if (auto view = dyn_cast<triton::ViewOp>(opArgI))\n+        continue;\n+\n+      // We add one expensive conversion for the current operand\n       numCvts += 1;\n       queue.emplace_back(opArgI, newEncoding);\n     }\n@@ -366,25 +404,65 @@ LogicalResult simulateBackwardRematerialization(\n //\n \n Operation *cloneWithInferType(mlir::PatternRewriter &rewriter, Operation *op,\n-                              BlockAndValueMapping &mapping) {\n+                              IRMapping &mapping) {\n   Operation *newOp = rewriter.clone(*op, mapping);\n   auto origType = op->getResult(0).getType().cast<RankedTensorType>();\n+  auto argType = newOp->getOperand(0).getType().cast<RankedTensorType>();\n   auto newType = RankedTensorType::get(\n-      origType.getShape(), origType.getElementType(),\n-      newOp->getOperand(0).getType().cast<RankedTensorType>().getEncoding());\n+      origType.getShape(), origType.getElementType(), argType.getEncoding());\n   newOp->getResult(0).setType(newType);\n   auto typeInfer = dyn_cast<InferTypeOpInterface>(newOp);\n   if (typeInfer) {\n-    SmallVector<Type, 1> newType;\n+    SmallVector<Type, 1> newTypes;\n     auto success = typeInfer.inferReturnTypes(\n         newOp->getContext(), newOp->getLoc(), newOp->getOperands(),\n-        newOp->getAttrDictionary(), newOp->getRegions(), newType);\n+        newOp->getAttrDictionary(), newOp->getRegions(), newTypes);\n     if (succeeded(success))\n-      newOp->getResult(0).setType(newType.front());\n+      newOp->getResult(0).setType(newTypes.front());\n   }\n   return newOp;\n }\n \n+// op(cvt(arg_0), arg_1, ..., arg_n)\n+// -> cvt(op(arg_0, cvt(arg_1), ..., cvt(arg_n)))\n+void pushConversionForward(triton::gpu::ConvertLayoutOp cvt,\n+                           SetVector<Operation *> &cvtSlices,\n+                           mlir::PatternRewriter &rewriter) {\n+  auto srcEncoding =\n+      cvt.getOperand().getType().cast<RankedTensorType>().getEncoding();\n+  auto dstEncoding =\n+      cvt.getResult().getType().cast<RankedTensorType>().getEncoding();\n+  IRMapping mapping;\n+  auto op = cvtSlices.front();\n+  for (Value arg : op->getOperands()) {\n+    if (arg.getDefiningOp() == cvt)\n+      mapping.map(arg, cvt.getOperand());\n+    else {\n+      auto oldType = arg.getType().cast<RankedTensorType>();\n+      auto newType = RankedTensorType::get(\n+          oldType.getShape(), oldType.getElementType(), srcEncoding);\n+      auto cvtI = rewriter.create<triton::gpu::ConvertLayoutOp>(arg.getLoc(),\n+                                                                newType, arg);\n+      if (Operation *argOp = arg.getDefiningOp())\n+        cvtI->moveAfter(argOp);\n+      mapping.map(arg, cvtI);\n+    }\n+  }\n+  rewriter.setInsertionPoint(op);\n+  if (op->getNumResults() == 0) {\n+    Operation *newOp = rewriter.clone(*op, mapping);\n+    rewriter.eraseOp(op);\n+    return;\n+  }\n+  auto *newOp = cloneWithInferType(rewriter, op, mapping);\n+  auto newType = newOp->getResult(0).getType().cast<RankedTensorType>();\n+  auto newCvtType = RankedTensorType::get(\n+      newType.getShape(), newType.getElementType(), dstEncoding);\n+  auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+      newOp->getLoc(), newCvtType, newOp->getResult(0));\n+  rewriter.replaceOp(op, newCvt->getResults());\n+}\n+\n //\n class MoveConvertOutOfIf : public mlir::RewritePattern {\n public:\n@@ -395,56 +473,83 @@ class MoveConvertOutOfIf : public mlir::RewritePattern {\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n     auto ifOp = cast<scf::IfOp>(*op);\n+    // If \u201cscf.if\u201d defines no values, \u201cscf.yield\u201d will be inserted implicitly.\n+    // However, \"scf.else\" is not required to be present, so we need to check\n+    // if it exists.\n     auto thenYield = ifOp.thenYield();\n-    auto elseYield = ifOp.elseYield();\n     int numOps = thenYield.getNumOperands();\n     SmallVector<Value> newThenYieldOps = thenYield.getOperands();\n-    SmallVector<Value> newElseYieldOps = elseYield.getOperands();\n     SetVector<Operation *> thenCvts;\n-    SetVector<Operation *> elseCvts;\n     SmallVector<Type> newRetTypes;\n \n-    BlockAndValueMapping mapping;\n+    bool hasElse = !ifOp.getElseRegion().empty();\n+\n+    scf::YieldOp elseYield;\n+    SmallVector<Value> newElseYieldOps;\n+    SetVector<Operation *> elseCvts;\n+    if (hasElse) {\n+      elseYield = ifOp.elseYield();\n+      newElseYieldOps = elseYield.getOperands();\n+    }\n+\n+    IRMapping mapping;\n     for (size_t i = 0; i < numOps; i++) {\n       auto thenCvt = dyn_cast<triton::gpu::ConvertLayoutOp>(\n           thenYield.getOperand(i).getDefiningOp());\n-      auto elseCvt = dyn_cast<triton::gpu::ConvertLayoutOp>(\n-          elseYield.getOperand(i).getDefiningOp());\n-      if (thenCvt && elseCvt &&\n-          std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1 &&\n-          std::distance(elseCvt->user_begin(), elseCvt->user_end()) == 1 &&\n-          thenCvt.getOperand().getType() == elseCvt.getOperand().getType()) {\n-        mapping.map(thenCvt.getResult(), thenCvt.getOperand());\n-        mapping.map(elseCvt.getResult(), elseCvt.getOperand());\n-        newRetTypes.push_back(thenCvt.getOperand().getType());\n-        thenCvts.insert((Operation *)thenCvt);\n-        elseCvts.insert((Operation *)elseCvt);\n-      } else\n-        newRetTypes.push_back(thenYield.getOperand(i).getType());\n+      if (hasElse) {\n+        auto elseYield = ifOp.elseYield();\n+        auto elseCvt = dyn_cast<triton::gpu::ConvertLayoutOp>(\n+            elseYield.getOperand(i).getDefiningOp());\n+        if (thenCvt && elseCvt &&\n+            std::distance(elseCvt->user_begin(), elseCvt->user_end()) == 1 &&\n+            std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1 &&\n+            thenCvt.getOperand().getType() == elseCvt.getOperand().getType()) {\n+          // If thenCvt and elseCvt's type are the same, it means a single\n+          // conversion is enough to replace both of them. We can move the\n+          // conversion out of scf.if and replace both thenCvt and elseCvt with\n+          // the new conversion.\n+          mapping.map(thenCvt.getResult(), thenCvt.getOperand());\n+          thenCvts.insert((Operation *)thenCvt);\n+          newRetTypes.push_back(thenCvt.getOperand().getType());\n+          mapping.map(elseCvt.getResult(), elseCvt.getOperand());\n+          elseCvts.insert((Operation *)elseCvt);\n+        } else\n+          // Cannot move out of scf.if because thenCvt != elseCvt\n+          // Moving it out of scf.if will introduce a new conversion\n+          newRetTypes.push_back(thenYield.getOperand(i).getType());\n+      } else {\n+        if (thenCvt &&\n+            std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1) {\n+          // If there's only a single use of the conversion then we can move it\n+          mapping.map(thenCvt.getResult(), thenCvt.getOperand());\n+          thenCvts.insert((Operation *)thenCvt);\n+          newRetTypes.push_back(thenCvt.getOperand().getType());\n+        } else\n+          // Cannot move out of scf.if because either there's another use of\n+          // the conversion or there's no conversion at all\n+          newRetTypes.push_back(thenYield.getOperand(i).getType());\n+      }\n     }\n     if (mapping.getValueMap().empty())\n       return mlir::failure();\n \n-    rewriter.setInsertionPoint(op);\n     auto newIfOp = rewriter.create<scf::IfOp>(ifOp.getLoc(), newRetTypes,\n-                                              ifOp.getCondition(), true);\n-    // rematerialize `then` block\n-    rewriter.setInsertionPointToEnd(newIfOp.thenBlock());\n-    for (Operation &op : ifOp.thenBlock()->getOperations()) {\n-      if (thenCvts.contains(&op)) {\n-        mapping.map(op.getResult(0), mapping.lookup(op.getOperand(0)));\n-        continue;\n-      }\n-      rewriter.clone(op, mapping);\n-    }\n-    // rematerialize `else` block\n-    rewriter.setInsertionPointToEnd(newIfOp.elseBlock());\n-    for (Operation &op : ifOp.elseBlock()->getOperations()) {\n-      if (elseCvts.contains(&op)) {\n-        mapping.map(op.getResult(0), mapping.lookup(op.getOperand(0)));\n-        continue;\n+                                              ifOp.getCondition(), hasElse);\n+    auto rematerialize = [&](Block *block, SetVector<Operation *> &cvts) {\n+      for (Operation &op : block->getOperations()) {\n+        if (cvts.contains(&op)) {\n+          if (mapping.contains(op.getOperand(0)))\n+            mapping.map(op.getResult(0), mapping.lookup(op.getOperand(0)));\n+          continue;\n+        }\n+        rewriter.clone(op, mapping);\n       }\n-      rewriter.clone(op, mapping);\n+    };\n+    rewriter.setInsertionPointToEnd(newIfOp.thenBlock());\n+    rematerialize(ifOp.thenBlock(), thenCvts);\n+    if (hasElse) {\n+      rewriter.setInsertionPointToEnd(newIfOp.elseBlock());\n+      rematerialize(ifOp.elseBlock(), elseCvts);\n     }\n \n     rewriter.setInsertionPointAfter(newIfOp);\n@@ -477,6 +582,7 @@ class FoldConvertAndReduce : public mlir::RewritePattern {\n         cvt.getOperand().getType().cast<RankedTensorType>().getEncoding();\n     auto dstEncoding =\n         cvt.getResult().getType().cast<RankedTensorType>().getEncoding();\n+    // XXX: why is this needed?\n     if (srcEncoding.isa<triton::gpu::SliceEncodingAttr>())\n       return failure();\n     SetVector<Operation *> cvtSlices;\n@@ -487,19 +593,23 @@ class FoldConvertAndReduce : public mlir::RewritePattern {\n              !isa<triton::gpu::ConvertLayoutOp>(op) && !isa<scf::YieldOp>(op);\n     };\n     mlir::getForwardSlice(cvt.getResult(), &cvtSlices, filter);\n-    if (cvtSlices.empty())\n+    if (cvtSlices.empty()) {\n       return failure();\n+    }\n \n     llvm::MapVector<Value, Attribute> toConvert;\n     for (Operation *op : cvtSlices) {\n       // don't rematerialize anything expensive\n-      if (expensive_to_remat(op))\n+      if (expensiveToRemat(op, dstEncoding)) {\n         return failure();\n+      }\n       // don't rematerialize non-element-wise\n-      if (!op->hasTrait<mlir::OpTrait::Elementwise>())\n+      if (!isa<triton::ViewOp, triton::CatOp>(op) &&\n+          !op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n+          !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n+          !isa<triton::StoreOp>(op)) {\n         return failure();\n-      Attribute dstEncoding =\n-          cvt.getOperand().getType().cast<RankedTensorType>().getEncoding();\n+      }\n       // don't rematerialize if it adds an extra conversion that can't\n       // be removed\n       for (Value arg : op->getOperands()) {\n@@ -509,39 +619,13 @@ class FoldConvertAndReduce : public mlir::RewritePattern {\n         llvm::MapVector<Value, Attribute> toConvert;\n         if (argOp && (argOp != cvt) && cvtSlices.count(argOp) == 0 &&\n             failed(simulateBackwardRematerialization(argOp, processed, layout,\n-                                                     toConvert, dstEncoding))) {\n+                                                     toConvert, srcEncoding))) {\n           return failure();\n         }\n       }\n     }\n \n-    BlockAndValueMapping mapping;\n-    auto op = cvtSlices.front();\n-    for (Value arg : op->getOperands()) {\n-      if (arg.getDefiningOp() == cvt)\n-        mapping.map(arg, cvt.getOperand());\n-      else {\n-        auto cvtI = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-            arg.getLoc(), cvt.getOperand().getType(), arg);\n-        if (Operation *argOp = arg.getDefiningOp())\n-          cvtI->moveAfter(argOp);\n-        mapping.map(arg, cvtI);\n-      }\n-    }\n-    rewriter.setInsertionPoint(op);\n-    Operation *newOp = rewriter.clone(*op, mapping);\n-    auto oldType = op->getResult(0).getType().cast<RankedTensorType>();\n-    auto newType = RankedTensorType::get(\n-        oldType.getShape(), oldType.getElementType(),\n-        cvt.getOperand().getType().cast<RankedTensorType>().getEncoding());\n-\n-    newOp->getResult(0).setType(newType);\n-    auto newCvtType = RankedTensorType::get(\n-        oldType.getShape(), oldType.getElementType(),\n-        cvt.getResult().getType().cast<RankedTensorType>().getEncoding());\n-    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        newOp->getLoc(), newCvtType, newOp->getResult(0));\n-    rewriter.replaceOp(op, newCvt->getResults());\n+    pushConversionForward(cvt, cvtSlices, rewriter);\n     return success();\n   }\n };\n@@ -581,50 +665,8 @@ class RematerializeBackward : public mlir::RewritePattern {\n     SetVector<Attribute> layout;\n     llvm::MapVector<Value, Attribute> toConvert;\n     std::vector<std::pair<Operation *, Attribute>> queue;\n-    queue.emplace_back(cvt, targetType.getEncoding());\n-    int numCvts = 1;\n-    while (!queue.empty()) {\n-      Operation *currOp;\n-      Attribute currLayout;\n-      std::tie(currOp, currLayout) = queue.back();\n-      queue.pop_back();\n-      // If the current operation is expensive to rematerialize,\n-      // we stop everything\n-      if (expensive_to_remat(currOp))\n-        break;\n-      // a conversion will be removed here (i.e. transferred to operands)\n-      numCvts -= 1;\n-      // done processing\n-      processed.insert(currOp);\n-      layout.insert(currLayout);\n-      // add all operands to the queue\n-      for (Value argI : currOp->getOperands()) {\n-        Attribute newEncoding;\n-        // cannot invert the current encoding for this operand\n-        // we stop everything\n-        if (failed(invertEncoding(currLayout, currOp, newEncoding)))\n-          return mlir::failure();\n-        if (toConvert.count(argI) && toConvert[argI] != newEncoding)\n-          return mlir::failure();\n-        //\n-        Operation *opArgI = argI.getDefiningOp();\n-        toConvert.insert({argI, newEncoding});\n-        if (!opArgI || processed.contains(opArgI) ||\n-            (opArgI->getBlock() != cvt->getBlock()))\n-          continue;\n-        // if the conversion can be folded into opArgI then\n-        // we don't count this conversion as expensive\n-        if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n-                triton::MakeRangeOp, triton::SplatOp>(*opArgI))\n-          continue;\n-        // we add one expensive conversion for the current operand\n-        numCvts += 1;\n-        queue.emplace_back(opArgI, newEncoding);\n-      }\n-    }\n-    // if rematerialization would add more conversions than it removes\n-    // then we don't do it\n-    if (numCvts > 0)\n+    if (failed(simulateBackwardRematerialization(\n+            cvt, processed, layout, toConvert, targetType.getEncoding())))\n       return mlir::failure();\n \n     SmallVector<Value, 4> sortedValues;\n@@ -636,18 +678,21 @@ class RematerializeBackward : public mlir::RewritePattern {\n       else\n         sortedValues.push_back(v);\n     }\n-    tmp = mlir::topologicalSort(tmp);\n+    tmp = mlir::multiRootTopologicalSort(tmp);\n     for (Operation *op : tmp)\n       sortedValues.push_back(op->getResult(0));\n \n-    BlockAndValueMapping mapping;\n+    IRMapping mapping;\n     for (Value currOperand : sortedValues) {\n       // unpack information\n       Attribute targetLayout = toConvert.lookup(currOperand);\n       // rematerialize the operand if necessary\n       Operation *currOperation = currOperand.getDefiningOp();\n       if (processed.contains(currOperation)) {\n-        currOperation = cloneWithInferType(rewriter, currOperation, mapping);\n+        Operation *newOperation =\n+            cloneWithInferType(rewriter, currOperation, mapping);\n+        newOperation->moveAfter(currOperation);\n+        currOperation = newOperation;\n         currOperand = currOperation->getResult(0);\n       }\n       // compute target type for the layout cast\n@@ -658,6 +703,10 @@ class RematerializeBackward : public mlir::RewritePattern {\n           currOperand.getLoc(), newType, currOperand);\n       if (currOperation)\n         newOperand->moveAfter(currOperation);\n+      else {\n+        Block *block = currOperand.cast<BlockArgument>().getOwner();\n+        newOperand->moveAfter(block, block->begin());\n+      }\n       mapping.map(currOperand, newOperand);\n     }\n     rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n@@ -689,7 +738,7 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n         forOp.getStep(), newInitArgs);\n     newForOp->moveBefore(forOp);\n     rewriter.setInsertionPointToStart(newForOp.getBody());\n-    BlockAndValueMapping mapping;\n+    IRMapping mapping;\n     for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n       mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n     mapping.map(origConversion.getResult(), newForOp.getRegionIterArgs()[i]);\n@@ -820,39 +869,25 @@ class RematerializeForward : public mlir::RewritePattern {\n       return failure();\n \n     for (Operation *op : cvtSlices) {\n-      if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n-          !op->hasTrait<mlir::OpTrait::SameOperandsAndResultType>())\n+      if (!isa<triton::ViewOp, triton::CatOp>(op) &&\n+          !op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n+          !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n+          !isa<triton::StoreOp>(op))\n         return failure();\n       for (Value arg : op->getOperands()) {\n         Operation *argOp = arg.getDefiningOp();\n         if (argOp && (argOp != cvt) &&\n-            !isa<arith::ConstantOp, triton::SplatOp>(argOp)) {\n+            !isa<arith::ConstantOp, triton::SplatOp, triton::MakeRangeOp>(\n+                argOp)) {\n           return failure();\n         }\n       }\n     }\n \n-    // otherwise, we push the conversion forward\n+    // Otherwise, we push the conversion forward\n     // since we'll be able to move it out of\n     // the loop once it reaches the yield op\n-    // op(cvt(arg_0), arg_1, ..., arg_n)\n-    // -> cvt(op(arg_0, cvt(arg_1), ..., cvt(arg_n)))\n-    BlockAndValueMapping mapping;\n-    auto op = cvtSlices.front();\n-    for (Value arg : op->getOperands()) {\n-      if (arg.getDefiningOp() == cvt)\n-        mapping.map(arg, cvt.getOperand());\n-      else {\n-        auto cvtI = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-            arg.getLoc(), cvt.getOperand().getType(), arg);\n-        mapping.map(arg, cvtI);\n-      }\n-    }\n-    Operation *newOp = rewriter.clone(*op, mapping);\n-    newOp->getResult(0).setType(cvt.getOperand().getType());\n-    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        newOp->getLoc(), cvt.getResult().getType(), newOp->getResult(0));\n-    rewriter.replaceOp(op, newCvt->getResults());\n+    pushConversionForward(cvt, cvtSlices, rewriter);\n     return success();\n   }\n };\n@@ -868,8 +903,11 @@ int computeCapabilityToMMAVersion(int computeCapability) {\n     return 1;\n   } else if (computeCapability < 90) {\n     return 2;\n+  } else if (computeCapability < 100) {\n+    // FIXME: temporarily add this to pass unis tests\n+    return 2;\n   } else {\n-    assert(false && \"computeCapability > 90 not supported\");\n+    assert(false && \"computeCapability > 100 not supported\");\n     return 3;\n   }\n }\n@@ -887,31 +925,8 @@ SmallVector<int64_t, 2> mmaVersionToShapePerWarp(int version) {\n \n SmallVector<unsigned, 2> warpsPerTileV1(const ArrayRef<int64_t> shape,\n                                         int numWarps) {\n-  if (!MmaEncodingAttr::_mmaV1UpdateWpt) {\n-    SmallVector<unsigned, 2> ret = {1, 1};\n-    SmallVector<int64_t, 2> shapePerWarp =\n-        mmaVersionToShapePerWarp(1 /*version*/);\n-    bool changed = false;\n-    do {\n-      changed = false;\n-      int pre = ret[0];\n-      if (ret[0] * ret[1] < numWarps) {\n-        ret[0] =\n-            std::clamp<unsigned>(ret[0] * 2, 1, shape[0] / shapePerWarp[0]);\n-        changed = pre != ret[0];\n-      }\n-      if (ret[0] * ret[1] < numWarps) {\n-        pre = ret[1];\n-        ret[1] =\n-            std::clamp<unsigned>(ret[1] * 2, 1, shape[1] / shapePerWarp[1]);\n-        changed = pre != ret[1];\n-      }\n-    } while (changed);\n-    return ret;\n-  } else {\n-    // Set a default value and ensure product of wpt equals numWarps\n-    return {static_cast<unsigned>(numWarps), 1};\n-  }\n+  // Set a default value and ensure product of wpt equals numWarps\n+  return {static_cast<unsigned>(numWarps), 1};\n }\n \n SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n@@ -1079,25 +1094,11 @@ class BlockedToMMA : public mlir::RewritePattern {\n         oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n       return failure();\n \n-    auto AType = dotOp.getOperand(0).getType().cast<RankedTensorType>();\n-    auto BType = dotOp.getOperand(1).getType().cast<RankedTensorType>();\n-\n     // for FMA, should retain the blocked layout.\n     int versionMajor = computeCapabilityToMMAVersion(computeCapability);\n     if (!supportMMA(dotOp, versionMajor))\n       return failure();\n \n-    auto AOrder = AType.getEncoding()\n-                      .cast<triton::gpu::DotOperandEncodingAttr>()\n-                      .getParent()\n-                      .cast<triton::gpu::BlockedEncodingAttr>()\n-                      .getOrder();\n-    auto BOrder = BType.getEncoding()\n-                      .cast<triton::gpu::DotOperandEncodingAttr>()\n-                      .getParent()\n-                      .cast<triton::gpu::BlockedEncodingAttr>()\n-                      .getOrder();\n-\n     // get MMA encoding for the given number of warps\n     auto retShape = oldRetType.getShape();\n     auto mod = op->getParentOfType<mlir::ModuleOp>();\n@@ -1107,13 +1108,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n         getWarpsPerTile(dotOp, retShape, versionMajor, numWarps);\n     triton::gpu::MmaEncodingAttr mmaEnc;\n     if (versionMajor == 1) {\n-      if (MmaEncodingAttr::_mmaV1UpdateWpt)\n-        mmaEnc = triton::gpu::MmaEncodingAttr::get(\n-            oldRetType.getContext(), versionMajor, numWarps, mmaV1Counter++);\n-      else\n-        mmaEnc = triton::gpu::MmaEncodingAttr::get(\n-            dotOp->getContext(), versionMajor, 0 /*versionMinor*/,\n-            warpsPerTileV1(retShape, numWarps));\n+      mmaEnc = triton::gpu::MmaEncodingAttr::get(\n+          oldRetType.getContext(), versionMajor, numWarps, mmaV1Counter++);\n     } else if (versionMajor == 2) {\n       mmaEnc = triton::gpu::MmaEncodingAttr::get(\n           oldRetType.getContext(), versionMajor, 0 /*versionMinor*/,\n@@ -1128,8 +1124,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto oldAcc = dotOp.getOperand(2);\n     auto newAcc = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         oldAcc.getLoc(), newRetType, oldAcc);\n-    Value a = dotOp.a();\n-    Value b = dotOp.b();\n+    Value a = dotOp.getA();\n+    Value b = dotOp.getB();\n     auto oldAType = a.getType().cast<RankedTensorType>();\n     auto oldBType = b.getType().cast<RankedTensorType>();\n     auto oldAOrder = oldAType.getEncoding()\n@@ -1160,8 +1156,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n \n     a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), newAType, a);\n     b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), newBType, b);\n-    auto newDot = rewriter.create<triton::DotOp>(dotOp.getLoc(), newRetType, a,\n-                                                 b, newAcc, dotOp.allowTF32());\n+    auto newDot = rewriter.create<triton::DotOp>(\n+        dotOp.getLoc(), newRetType, a, b, newAcc, dotOp.getAllowTF32());\n \n     rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(\n         op, oldRetType, newDot.getResult());\n@@ -1184,27 +1180,24 @@ class ConvertTransConvert : public mlir::RewritePattern {\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n     auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto tmpOp = dyn_cast_or_null<triton::TransOp>(dstOp.src().getDefiningOp());\n+    auto tmpOp =\n+        dyn_cast_or_null<triton::TransOp>(dstOp.getSrc().getDefiningOp());\n     if (!tmpOp)\n       return mlir::failure();\n     auto srcOp = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n-        tmpOp.src().getDefiningOp());\n+        tmpOp.getSrc().getDefiningOp());\n     if (!srcOp)\n       return mlir::failure();\n-    auto arg = srcOp.src();\n-    auto X = tmpOp.src();\n-    auto Y = dstOp.src();\n+    auto arg = srcOp.getSrc();\n+    auto X = tmpOp.getSrc();\n     // types\n     auto argType = arg.getType().cast<RankedTensorType>();\n     auto XType = X.getType().cast<RankedTensorType>();\n-    auto YType = Y.getType().cast<RankedTensorType>();\n     auto ZType = dstOp.getResult().getType().cast<RankedTensorType>();\n     // encodings\n     auto argEncoding = argType.getEncoding();\n     auto XEncoding =\n         XType.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n-    auto YEncoding =\n-        YType.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n     auto ZEncoding =\n         ZType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n     if (!ZEncoding)\n@@ -1239,7 +1232,8 @@ class ConvertDotConvert : public mlir::RewritePattern {\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n     auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto dotOp = dyn_cast_or_null<triton::DotOp>(dstOp.src().getDefiningOp());\n+    auto dotOp =\n+        dyn_cast_or_null<triton::DotOp>(dstOp.getSrc().getDefiningOp());\n     if (!dotOp)\n       return mlir::failure();\n     if (std::distance(dstOp->user_begin(), dstOp->user_end()) != 1 ||\n@@ -1249,7 +1243,8 @@ class ConvertDotConvert : public mlir::RewritePattern {\n         dotOp.getOperand(2).getDefiningOp());\n     if (!cvtOp)\n       return mlir::failure();\n-    auto loadOp = dyn_cast_or_null<triton::LoadOp>(cvtOp.src().getDefiningOp());\n+    auto loadOp =\n+        dyn_cast_or_null<triton::LoadOp>(cvtOp.getSrc().getDefiningOp());\n     if (!loadOp)\n       return mlir::failure();\n     auto dstTy = dstOp.getResult().getType().cast<RankedTensorType>();\n@@ -1264,11 +1259,10 @@ class ConvertDotConvert : public mlir::RewritePattern {\n         op->getLoc(), dotOp.getResult().getType(), _0f);\n     auto newDot = rewriter.create<triton::DotOp>(\n         op->getLoc(), dotOp.getResult().getType(), dotOp.getOperand(0),\n-        dotOp.getOperand(1), _0, dotOp.allowTF32());\n+        dotOp.getOperand(1), _0, dotOp.getAllowTF32());\n     auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         op->getLoc(), dstTy, newDot.getResult());\n-    auto newAdd = rewriter.replaceOpWithNewOp<arith::AddFOp>(\n-        op, newCvt, cvtOp.getOperand());\n+    rewriter.replaceOpWithNewOp<arith::AddFOp>(op, newCvt, cvtOp.getOperand());\n     return mlir::success();\n   }\n };"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.td", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -3,5 +3,6 @@\n \n include \"triton/Dialect/TritonGPU/IR/TritonGPUOps.td\"\n include \"triton/Dialect/Triton/IR/TritonOps.td\"\n+include \"mlir/IR/PatternBase.td\"\n \n #endif"}, {"filename": "lib/Dialect/TritonGPU/Transforms/DecomposeConversions.cpp", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -1,7 +1,7 @@\n #include \"mlir/Analysis/SliceAnalysis.h\"\n-#include \"mlir/Dialect/SCF/SCF.h\"\n-#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/IRMapping.h\"\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"mlir/IR/Verifier.h\"\n@@ -28,7 +28,6 @@ class TritonGPUDecomposeConversionsPass\n   TritonGPUDecomposeConversionsPass() = default;\n \n   void runOnOperation() override {\n-    MLIRContext *context = &getContext();\n     ModuleOp mod = getOperation();\n     mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n       OpBuilder builder(cvtOp);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 111, "deletions": 43, "changes": 154, "file_content_changes": "@@ -1,5 +1,8 @@\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n-#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/IR/IRMapping.h\"\n+#include \"mlir/IR/TypeUtilities.h\"\n+#include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n \n@@ -62,13 +65,13 @@ class LoopPipeliner {\n   DenseMap<Value, SmallVector<Value>> valueMapping;\n \n   /// Block arguments that loads depend on\n-  DenseSet<BlockArgument> depArgs;\n+  SetVector<BlockArgument> depArgs;\n \n   /// Operations (inside the loop body) that loads depend on\n-  DenseSet<Operation *> depOps;\n+  SetVector<Operation *> depOps;\n \n   /// collect values that v depends on and are defined inside the loop\n-  void collectDeps(Value v, int stages, DenseSet<Value> &deps);\n+  void collectDeps(Value v, int stages, SetVector<Value> &deps);\n \n   void setValueMapping(Value origin, Value newValue, int stage);\n \n@@ -112,7 +115,7 @@ Value LoopPipeliner::lookupOrDefault(Value origin, int stage) {\n   return valueMapping[origin][stage];\n }\n \n-void LoopPipeliner::collectDeps(Value v, int stages, DenseSet<Value> &deps) {\n+void LoopPipeliner::collectDeps(Value v, int stages, SetVector<Value> &deps) {\n   // Loop-invariant value, skip\n   if (v.getParentRegion() != &forOp.getLoopBody())\n     return;\n@@ -158,20 +161,37 @@ ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(Operation *op,\n LogicalResult LoopPipeliner::initialize() {\n   Block *loop = forOp.getBody();\n \n+  std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n+  AxisInfoAnalysis *axisInfoAnalysis = solver->load<AxisInfoAnalysis>();\n+  if (failed(solver->initializeAndRun(forOp->getParentOfType<ModuleOp>()))) {\n+    return failure();\n+  }\n+\n   // can we use forOp.walk(...) here?\n   SmallVector<triton::LoadOp, 2> allLoads;\n   for (Operation &op : *loop)\n-    if (auto loadOp = dyn_cast<triton::LoadOp>(&op))\n-      allLoads.push_back(loadOp);\n+    if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n+      auto ptr = loadOp.getPtr();\n+      unsigned vec = axisInfoAnalysis->getPtrContiguity(ptr);\n+      auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+      if (!tensorTy)\n+        continue;\n+      auto ty = tensorTy.getElementType()\n+                    .cast<triton::PointerType>()\n+                    .getPointeeType();\n+      unsigned width = vec * ty.getIntOrFloatBitWidth();\n+      if (width >= 32)\n+        allLoads.push_back(loadOp);\n+    }\n \n   // Early stop: no need to continue if there is no load in the loop.\n   if (allLoads.empty())\n     return failure();\n \n   // load => values that it depends on\n-  DenseMap<Value, DenseSet<Value>> loadDeps;\n+  DenseMap<Value, SetVector<Value>> loadDeps;\n   for (triton::LoadOp loadOp : allLoads) {\n-    DenseSet<Value> deps;\n+    SetVector<Value> deps;\n     for (Value op : loadOp->getOperands())\n       collectDeps(op, numStages - 1, deps);\n     loadDeps[loadOp] = deps;\n@@ -195,6 +215,20 @@ LogicalResult LoopPipeliner::initialize() {\n     if (isCandidate && loadOp.getResult().hasOneUse()) {\n       isCandidate = false;\n       Operation *use = *loadOp.getResult().getUsers().begin();\n+\n+      // advance to the first conversion as long\n+      // as the use resides in shared memory and it has\n+      // a single use itself\n+      while (use) {\n+        if (use->getNumResults() != 1 || !use->getResult(0).hasOneUse())\n+          break;\n+        auto tensorType =\n+            use->getResult(0).getType().dyn_cast<RankedTensorType>();\n+        if (!tensorType.getEncoding().isa<ttg::SharedEncodingAttr>())\n+          break;\n+        use = *use->getResult(0).getUsers().begin();\n+      }\n+\n       if (auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use)) {\n         if (auto tensorType = convertLayout.getResult()\n                                   .getType()\n@@ -284,7 +318,7 @@ void LoopPipeliner::emitPrologue() {\n         }\n         // load => copy async\n         if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n-          Value mask = lookupOrDefault(loadOp.mask(), stage);\n+          Value mask = lookupOrDefault(loadOp.getMask(), stage);\n           Value newMask;\n           if (mask) {\n             Value splatCond = builder.create<triton::SplatOp>(\n@@ -298,10 +332,11 @@ void LoopPipeliner::emitPrologue() {\n           // TODO: check if the hardware supports async copy\n           newOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n               op->getLoc(), loadsBuffer[loadOp].getType(),\n-              lookupOrDefault(loadOp.ptr(), stage),\n+              lookupOrDefault(loadOp.getPtr(), stage),\n               loadStageBuffer[loadOp][stage], pipelineIterIdx, newMask,\n-              lookupOrDefault(loadOp.other(), stage), loadOp.cache(),\n-              loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n+              lookupOrDefault(loadOp.getOther(), stage), loadOp.getCache(),\n+              loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n+          builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n           loadStageBuffer[loadOp].push_back(newOp->getResult(0));\n         } else\n           llvm_unreachable(\"This should be LoadOp\");\n@@ -319,6 +354,9 @@ void LoopPipeliner::emitPrologue() {\n       }\n \n       // Update mapping of results\n+      // if (stage == numStages - 2)\n+      //   continue;\n+\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n         Value originalResult = op->getResult(dstIdx);\n         // copy_async will update the value of its only use\n@@ -348,10 +386,14 @@ void LoopPipeliner::emitPrologue() {\n                                    loads.size() * (numStages - 2));\n   loopIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n   for (Value loadOp : loads) {\n+    auto bufferType = loadStageBuffer[loadOp][numStages - 1]\n+                          .getType()\n+                          .cast<RankedTensorType>();\n+    auto bufferShape = bufferType.getShape();\n     auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n-    sliceType =\n-        RankedTensorType::get(sliceType.getShape(), sliceType.getElementType(),\n-                              loadsBufferType[loadOp].getEncoding());\n+    sliceType = RankedTensorType::get({bufferShape[1], bufferShape[2]},\n+                                      sliceType.getElementType(),\n+                                      loadsBufferType[loadOp].getEncoding());\n     Value extractSlice = builder.create<tensor::ExtractSliceOp>(\n         loadOp.getLoc(), sliceType, loadStageBuffer[loadOp][numStages - 1],\n         SmallVector<OpFoldResult>{int_attr(0), int_attr(0), int_attr(0)},\n@@ -383,7 +425,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   //   (original args)\n   //   (insertSliceAsync buffer at stage numStages - 1) for each load\n   //   (extracted tensor) for each load\n-  //   (depArgs at stage numStages - 1)\n+  //   (depArgs at stage numStages - 2)\n   //   (iv at stage numStages - 2)\n   //   (pipeline iteration index)\n   //   (loop iteration index)\n@@ -404,7 +446,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   size_t depArgsBeginIdx = newLoopArgs.size();\n   for (BlockArgument depArg : depArgs) {\n     depArgsIdx[depArg] = newLoopArgs.size();\n-    newLoopArgs.push_back(valueMapping[depArg][numStages - 1]);\n+    newLoopArgs.push_back(valueMapping[depArg][numStages - 2]);\n   }\n \n   size_t nextIVIdx = newLoopArgs.size();\n@@ -422,7 +464,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   // 2. body of the new ForOp\n   builder.setInsertionPointToStart(newForOp.getBody());\n-  BlockAndValueMapping mapping;\n+  IRMapping mapping;\n   for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n   mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n@@ -459,14 +501,16 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   }\n   assert(depOps.size() + loads.size() == orderedDeps.size() &&\n          \"depOps contains invalid values\");\n-  BlockAndValueMapping nextMapping;\n+  IRMapping nextMapping;\n   DenseMap<BlockArgument, Value> depArgsMapping;\n   size_t argIdx = 0;\n   for (BlockArgument arg : depArgs) {\n-    nextMapping.map(arg,\n-                    newForOp.getRegionIterArgs()[argIdx + depArgsBeginIdx]);\n+    BlockArgument nextArg =\n+        newForOp.getRegionIterArgs()[argIdx + depArgsBeginIdx];\n+    nextMapping.map(arg, nextArg);\n     ++argIdx;\n   }\n+\n   // Special handling for iv & loop condition\n   Value nextIV = builder.create<arith::AddIOp>(\n       newForOp.getInductionVar().getLoc(),\n@@ -491,12 +535,31 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   extractSliceIndex = builder.create<arith::IndexCastOp>(\n       extractSliceIndex.getLoc(), builder.getIndexType(), extractSliceIndex);\n \n+  for (Operation *op : orderedDeps)\n+    if (!loads.contains(op->getResult(0))) {\n+      Operation *nextOp = builder.clone(*op, nextMapping);\n+\n+      auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n+      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n+        for (OpOperand &operand : originYield->getOpOperands()) {\n+          if (operand.get() == op->getResult(dstIdx)) {\n+            size_t originIdx = operand.getOperandNumber();\n+            size_t newArgIdx = depArgsIdx[forOp.getRegionIterArgs()[originIdx]];\n+            BlockArgument newArg = newForOp.getRegionIterArgs()[newArgIdx];\n+            nextMapping.map(forOp.getRegionIterArgs()[originIdx],\n+                            nextOp->getResult(dstIdx));\n+            depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n+          }\n+        }\n+      }\n+    }\n+\n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n     // Update loading mask\n     if (loads.contains(op->getResult(0))) {\n       auto loadOp = llvm::cast<triton::LoadOp>(op);\n-      Value mask = loadOp.mask();\n+      Value mask = loadOp.getMask();\n       Value newMask;\n       if (mask) {\n         Value splatCond = builder.create<triton::SplatOp>(\n@@ -507,22 +570,27 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n         // once\n         if (!(forOp.isDefinedOutsideOfLoop(mask) && nextMapping.contains(mask)))\n           nextMapping.map(mask, newMask);\n-        newMask = nextMapping.lookupOrDefault(loadOp.mask());\n+        newMask = nextMapping.lookupOrDefault(loadOp.getMask());\n       } else\n         newMask = builder.create<triton::SplatOp>(\n             loadOp.getLoc(), getI1SameShape(loadOp), nextLoopCond);\n       Value insertAsyncOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n           op->getLoc(), loadsBuffer[loadOp].getType(),\n-          nextMapping.lookupOrDefault(loadOp.ptr()),\n+          nextMapping.lookupOrDefault(loadOp.getPtr()),\n           newForOp.getRegionIterArgs()[bufferIdx + nextBuffers.size()],\n           insertSliceIndex, newMask,\n-          nextMapping.lookupOrDefault(loadOp.other()), loadOp.cache(),\n-          loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n+          nextMapping.lookupOrDefault(loadOp.getOther()), loadOp.getCache(),\n+          loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n+      builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n       nextBuffers.push_back(insertAsyncOp);\n+      // ExtractSlice\n+      auto bufferType = insertAsyncOp.getType().cast<RankedTensorType>();\n+      auto bufferShape = bufferType.getShape();\n       auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n-      sliceType = RankedTensorType::get(sliceType.getShape(),\n+      sliceType = RankedTensorType::get({bufferShape[1], bufferShape[2]},\n                                         sliceType.getElementType(),\n                                         loadsBufferType[loadOp].getEncoding());\n+\n       nextOp = builder.create<tensor::ExtractSliceOp>(\n           op->getLoc(), sliceType, insertAsyncOp,\n           SmallVector<OpFoldResult>{extractSliceIndex, int_attr(0),\n@@ -532,19 +600,19 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n                                     int_attr(sliceType.getShape()[1])},\n           SmallVector<OpFoldResult>{int_attr(1), int_attr(1), int_attr(1)});\n       extractSlices.push_back(nextOp->getResult(0));\n-    } else\n-      nextOp = builder.clone(*op, nextMapping);\n-    // Update mapping of results\n-    for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n-      nextMapping.map(op->getResult(dstIdx), nextOp->getResult(dstIdx));\n-      // If this is a loop-carried value, update the mapping for yield\n-      auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n-      for (OpOperand &operand : originYield->getOpOperands()) {\n-        if (operand.get() == op->getResult(dstIdx)) {\n-          size_t originIdx = operand.getOperandNumber();\n-          size_t newArgIdx = depArgsIdx[forOp.getRegionIterArgs()[originIdx]];\n-          BlockArgument newArg = newForOp.getRegionIterArgs()[newArgIdx];\n-          depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n+\n+      // Update mapping of results\n+      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n+        nextMapping.map(op->getResult(dstIdx), nextOp->getResult(dstIdx));\n+        // If this is a loop-carried value, update the mapping for yield\n+        auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n+        for (OpOperand &operand : originYield->getOpOperands()) {\n+          if (operand.get() == op->getResult(dstIdx)) {\n+            size_t originIdx = operand.getOperandNumber();\n+            size_t newArgIdx = depArgsIdx[forOp.getRegionIterArgs()[originIdx]];\n+            BlockArgument newArg = newForOp.getRegionIterArgs()[newArgIdx];\n+            depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n+          }\n         }\n       }\n     }\n@@ -556,8 +624,8 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n       if (auto dotOp = llvm::dyn_cast<triton::DotOp>(&op)) {\n         builder.setInsertionPoint(&op);\n         auto dotType = dotOp.getType().cast<RankedTensorType>();\n-        Value a = dotOp.a();\n-        Value b = dotOp.b();\n+        Value a = dotOp.getA();\n+        Value b = dotOp.getB();\n         auto layoutCast = [&](Value dotOperand, int opIdx) -> Value {\n           auto tensorType = dotOperand.getType().cast<RankedTensorType>();\n           if (!tensorType.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 31, "deletions": 24, "changes": 55, "file_content_changes": "@@ -26,7 +26,7 @@\n // }\n //===----------------------------------------------------------------------===//\n \n-#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/IR/IRMapping.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n@@ -63,8 +63,8 @@ class Prefetcher {\n \n   Value generatePrefetch(Value v, unsigned opIdx, bool isPrologue,\n                          Attribute dotEncoding, OpBuilder &builder,\n-                         llvm::Optional<int64_t> offsetK = llvm::None,\n-                         llvm::Optional<int64_t> shapeK = llvm::None);\n+                         llvm::Optional<int64_t> offsetK = std::nullopt,\n+                         llvm::Optional<int64_t> shapeK = std::nullopt);\n \n public:\n   Prefetcher() = delete;\n@@ -140,7 +140,7 @@ LogicalResult Prefetcher::initialize() {\n   auto getPrefetchSrc = [](Value v) -> Value {\n     if (auto cvt = v.getDefiningOp<triton::gpu::ConvertLayoutOp>())\n       if (isSharedEncoding(cvt.getOperand()))\n-        return cvt.src();\n+        return cvt.getSrc();\n     return Value();\n   };\n \n@@ -158,12 +158,18 @@ LogicalResult Prefetcher::initialize() {\n   };\n \n   for (triton::DotOp dot : dotsInFor) {\n-    auto kSize = dot.a().getType().cast<RankedTensorType>().getShape()[1];\n+    auto kSize = dot.getA().getType().cast<RankedTensorType>().getShape()[1];\n+\n+    // works better with nvidia tensor cores\n+    unsigned elementWidth =\n+        dot.getA().getType().cast<RankedTensorType>().getElementTypeBitWidth();\n+    prefetchWidth = 256 / elementWidth;\n+\n     // Skip prefetching if kSize is less than prefetchWidth\n     if (kSize < prefetchWidth)\n       continue;\n-    Value aSmem = getPrefetchSrc(dot.a());\n-    Value bSmem = getPrefetchSrc(dot.b());\n+    Value aSmem = getPrefetchSrc(dot.getA());\n+    Value bSmem = getPrefetchSrc(dot.getB());\n     if (aSmem && bSmem) {\n       Value aHeaderDef = getIncomingOp(aSmem);\n       Value bHeaderDef = getIncomingOp(bSmem);\n@@ -191,10 +197,12 @@ void Prefetcher::emitPrologue() {\n         dot.getType().cast<RankedTensorType>().getEncoding();\n     Value aPrefetched =\n         generatePrefetch(dot2aHeaderDef[dot], 0, true, dotEncoding, builder);\n-    operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().a()] = aPrefetched;\n+    operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().getA()] =\n+        aPrefetched;\n     Value bPrefetched =\n         generatePrefetch(dot2bHeaderDef[dot], 1, true, dotEncoding, builder);\n-    operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().b()] = bPrefetched;\n+    operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().getB()] =\n+        bPrefetched;\n   }\n }\n \n@@ -206,55 +214,54 @@ scf::ForOp Prefetcher::createNewForOp() {\n     loopArgs.push_back(v);\n   for (Value dot : dots) {\n     loopArgs.push_back(\n-        operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().a()]);\n+        operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().getA()]);\n     loopArgs.push_back(\n-        operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().b()]);\n+        operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().getB()]);\n   }\n \n   auto newForOp = builder.create<scf::ForOp>(\n       forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n       forOp.getStep(), loopArgs);\n \n-  auto largestPow2 = [](int64_t n) -> int64_t {\n-    while ((n & (n - 1)) != 0)\n-      n = n & (n - 1);\n-    return n;\n-  };\n-\n   builder.setInsertionPointToStart(newForOp.getBody());\n-  BlockAndValueMapping mapping;\n+  IRMapping mapping;\n   for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n   mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n \n   for (Operation &op : forOp.getBody()->without_terminator()) {\n     Operation *newOp = builder.clone(op, mapping);\n     auto dot = dyn_cast<triton::DotOp>(&op);\n-    if (dots.contains(dot)) {\n+    if (dot && dots.contains(dot)) {\n       Attribute dotEncoding =\n           dot.getType().cast<RankedTensorType>().getEncoding();\n       // prefetched dot\n       Operation *firstDot = builder.clone(*dot, mapping);\n-      if (Value a = operand2headPrefetch.lookup(dot.a()))\n+      if (Value a = operand2headPrefetch.lookup(dot.getA()))\n         firstDot->setOperand(\n             0, newForOp.getRegionIterArgForOpOperand(*a.use_begin()));\n-      if (Value b = operand2headPrefetch.lookup(dot.b()))\n+      if (Value b = operand2headPrefetch.lookup(dot.getB()))\n         firstDot->setOperand(\n             1, newForOp.getRegionIterArgForOpOperand(*b.use_begin()));\n \n       // remaining part\n       int64_t kOff = prefetchWidth;\n-      int64_t kRem = dot.a().getType().cast<RankedTensorType>().getShape()[1] -\n-                     prefetchWidth;\n+      int64_t kRem =\n+          dot.getA().getType().cast<RankedTensorType>().getShape()[1] -\n+          prefetchWidth;\n       Operation *prevDot = firstDot;\n       while (kRem != 0) {\n-        int64_t kShape = largestPow2(kRem);\n+        // int64_t kShape = largestPow2(kRem);\n+        int64_t kShape = prefetchWidth;\n+        auto insertionPoint = builder.saveInsertionPoint();\n+        builder.setInsertionPoint(prevDot);\n         Value aRem =\n             generatePrefetch(mapping.lookup(dot2aLoopArg[dot]), 0, false,\n                              dotEncoding, builder, kOff, kShape);\n         Value bRem =\n             generatePrefetch(mapping.lookup(dot2bLoopArg[dot]), 1, false,\n                              dotEncoding, builder, kOff, kShape);\n+        builder.restoreInsertionPoint(insertionPoint);\n         newOp = builder.clone(*dot, mapping);\n         newOp->setOperand(0, aRem);\n         newOp->setOperand(1, bRem);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/ReorderInstructions.cpp", "status": "modified", "additions": 13, "deletions": 6, "changes": 19, "file_content_changes": "@@ -1,7 +1,7 @@\n #include \"mlir/Analysis/SliceAnalysis.h\"\n-#include \"mlir/Dialect/SCF/SCF.h\"\n-#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/IRMapping.h\"\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"mlir/IR/Verifier.h\"\n@@ -41,7 +41,6 @@ class TritonGPUReorderInstructionsPass\n   TritonGPUReorderInstructionsPass() = default;\n \n   void runOnOperation() override {\n-    MLIRContext *context = &getContext();\n     ModuleOp m = getOperation();\n     // Sink conversions into loops when they will increase\n     // register pressure\n@@ -53,6 +52,9 @@ class TritonGPUReorderInstructionsPass\n       auto user_end = op->user_end();\n       if (std::distance(user_begin, user_end) != 1)\n         return;\n+      if (user_begin->getParentOfType<scf::ForOp>() ==\n+          op->getParentOfType<scf::ForOp>())\n+        return;\n       opToMove.insert({op, *user_begin});\n     });\n     for (auto &kv : opToMove)\n@@ -85,12 +87,17 @@ class TritonGPUReorderInstructionsPass\n       if (!dstEncoding)\n         return;\n       int opIdx = dstEncoding.getOpIdx();\n-      if (opIdx != 1)\n+      if (opIdx != 0)\n         return;\n       if (op->getUsers().empty())\n         return;\n-      auto user_begin = op->user_begin();\n-      op->moveBefore(*user_begin);\n+      auto dotUser = dyn_cast<triton::DotOp>(*op->user_begin());\n+      if (!dotUser)\n+        return;\n+      auto BOp = dotUser.getOperand(1).getDefiningOp();\n+      if (!BOp)\n+        return;\n+      op->moveBefore(BOp);\n     });\n     return;\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 13, "deletions": 13, "changes": 26, "file_content_changes": "@@ -1,5 +1,5 @@\n #include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n-#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/IR/IRMapping.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include <algorithm>\n@@ -43,15 +43,15 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n                                  RankedTensorType tensorType, ValueRange inputs,\n                                  Location loc) {\n     llvm_unreachable(\"Argument rematerialization not implemented\");\n-    return llvm::None;\n+    return std::nullopt;\n   });\n \n   // If the origValue still has live user(s), use this to\n   // convert origValue to newValue\n   addSourceMaterialization([&](OpBuilder &builder, RankedTensorType tensorType,\n                                ValueRange inputs, Location loc) {\n     llvm_unreachable(\"Source rematerialization not implemented\");\n-    return llvm::None;\n+    return std::nullopt;\n   });\n \n   // This will be called when (desiredType != newOperandType)\n@@ -64,7 +64,7 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n     return Optional<Value>(cast.getResult());\n     // return Optional<Value>(cast.getResult(0));\n     // llvm_unreachable(\"Not implemented\");\n-    // return llvm::None;\n+    // return std::nullopt;\n   });\n }\n \n@@ -81,20 +81,20 @@ TritonGPUConversionTarget::TritonGPUConversionTarget(\n   addIllegalOp<scf::ExecuteRegionOp, scf::ParallelOp, scf::ReduceOp,\n                scf::ReduceReturnOp>();\n \n-  addDynamicallyLegalDialect<arith::ArithmeticDialect, math::MathDialect,\n-                             triton::TritonDialect, StandardOpsDialect,\n-                             scf::SCFDialect>([&](Operation *op) {\n-    if (typeConverter.isLegal(op))\n-      return true;\n-    return false;\n-  });\n+  addDynamicallyLegalDialect<arith::ArithDialect, math::MathDialect,\n+                             triton::TritonDialect, scf::SCFDialect>(\n+      [&](Operation *op) {\n+        if (typeConverter.isLegal(op))\n+          return true;\n+        return false;\n+      });\n \n   // We have requirements for the data layouts\n   addDynamicallyLegalOp<triton::DotOp>([](triton::DotOp dotOp) -> bool {\n     Attribute aEncoding =\n-        dotOp.a().getType().cast<RankedTensorType>().getEncoding();\n+        dotOp.getA().getType().cast<RankedTensorType>().getEncoding();\n     Attribute bEncoding =\n-        dotOp.b().getType().cast<RankedTensorType>().getEncoding();\n+        dotOp.getB().getType().cast<RankedTensorType>().getEncoding();\n     if (aEncoding && aEncoding.isa<triton::gpu::DotOperandEncodingAttr>() &&\n         bEncoding && bEncoding.isa<triton::gpu::DotOperandEncodingAttr>())\n       return true;"}, {"filename": "lib/Dialect/TritonGPU/Transforms/UpdateMmaForVolta.cpp", "status": "modified", "additions": 165, "deletions": 307, "changes": 472, "file_content_changes": "@@ -1,349 +1,207 @@\n #include \"Utility.h\"\n-#include \"mlir/Dialect/SCF/SCF.h\"\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n namespace mlir {\n namespace {\n using triton::DotOp;\n using triton::gpu::ConvertLayoutOp;\n using triton::gpu::DotOperandEncodingAttr;\n using triton::gpu::MmaEncodingAttr;\n+using triton::gpu::SharedEncodingAttr;\n using triton::gpu::SliceEncodingAttr;\n \n-// This pattern collects the wrong Mma those need to update and create the right\n-// ones for each.\n-// TODO[Superjomn]: RewirtePattern is not needed here, Rewrite this to a method\n-class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n-  // Holds the mapping from old(wrong) mmaEncodingAttr to the new(correct)\n-  // mmaEncodingAttr.\n-  DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n-\n-public:\n-  CollectMmaToUpdateForVolta(\n-      mlir::MLIRContext *ctx,\n-      DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n-      : mlir::RewritePattern(triton::DotOp::getOperationName(), 1, ctx),\n-        mmaToUpdate(mmaToUpdate) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-\n-    auto dotOp = cast<triton::DotOp>(op);\n-    auto *ctx = dotOp->getContext();\n-    auto AT = dotOp.a().getType().cast<RankedTensorType>();\n-    auto BT = dotOp.b().getType().cast<RankedTensorType>();\n-    auto DT = dotOp.d().getType().cast<RankedTensorType>();\n-    if (!DT.getEncoding())\n-      return failure();\n-    auto mmaLayout = DT.getEncoding().dyn_cast<MmaEncodingAttr>();\n-    if (!(mmaLayout && mmaLayout.isVolta()))\n-      return failure();\n-\n-    // Has processed.\n-    if (mmaToUpdate.count(mmaLayout))\n-      return failure();\n-\n-    auto dotOperandA = AT.getEncoding().cast<DotOperandEncodingAttr>();\n-    auto dotOperandB = BT.getEncoding().cast<DotOperandEncodingAttr>();\n-    bool isARow = dotOperandA.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    bool isBRow = dotOperandB.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    auto [isARow_, isBRow_, isAVec4, isBVec4, mmaId] =\n-        mmaLayout.decodeVoltaLayoutStates();\n-\n-    // The wpt of MMAv1 is also determined by isARow, isBRow and shape, and it\n-    // could only be set here for those states might be updated by previous\n-    // patterns in the Combine Pass.\n-    if (isARow_ == isARow && isBRow_ == isBRow) {\n-      auto tgtWpt =\n-          getWarpsPerCTA(DT.getShape(), isARow, isBRow, isAVec4, isBVec4,\n-                         product(mmaLayout.getWarpsPerCTA()));\n-      // Check if the wpt should be updated.\n-      if (tgtWpt == mmaLayout.getWarpsPerCTA() ||\n-          !MmaEncodingAttr::_mmaV1UpdateWpt)\n-        return failure();\n-    }\n-\n-    MmaEncodingAttr newMmaLayout;\n-    {\n-      // Create a temporary MMA layout to obtain the isAVec4 and isBVec4\n-      auto tmpMmaLayout = MmaEncodingAttr::get(\n-          ctx, mmaLayout.getVersionMajor(), mmaLayout.getWarpsPerCTA(),\n-          AT.getShape(), BT.getShape(), isARow, isBRow, mmaId);\n-      auto [isARow_, isBRow_, isAVec4_, isBVec4_, _] =\n-          tmpMmaLayout.decodeVoltaLayoutStates();\n-\n-      // Recalculate the wpt, for here we could get the latest information, the\n-      // wpt should be updated.\n-      auto updatedWpt =\n-          getWarpsPerCTA(DT.getShape(), isARow_, isBRow_, isAVec4_, isBVec4_,\n-                         product(mmaLayout.getWarpsPerCTA()));\n-      auto newWpt = MmaEncodingAttr::_mmaV1UpdateWpt\n-                        ? updatedWpt\n-                        : mmaLayout.getWarpsPerCTA();\n-      newMmaLayout = MmaEncodingAttr::get(ctx, mmaLayout.getVersionMajor(),\n-                                          newWpt, AT.getShape(), BT.getShape(),\n-                                          isARow, isBRow, mmaId);\n-    }\n-\n-    // Collect the wrong MMA Layouts, and mark need to update.\n-    mmaToUpdate.try_emplace(mmaLayout, newMmaLayout);\n-\n-    return failure();\n-  }\n-\n-  // Get the wpt for MMAv1 using more information.\n-  // Reference the original logic here\n-  // https://github.com/openai/triton/blob/0e4691e6dd91e001a8d33b71badf8b3314325459/lib/codegen/analysis/layout.cc#L223\n-  SmallVector<unsigned, 2> getWarpsPerCTA(ArrayRef<int64_t> shape, bool isARow,\n-                                          bool isBRow, bool isAVec4,\n-                                          bool isBVec4, int numWarps) const {\n-    // TODO[Superjomn]: Share code with\n-    // DotOpMmaV1ConversionHelper::AParam/BParam, since same code to compute the\n-    // rep,spw and fpw.\n-    SmallVector<unsigned, 2> wpt({1, 1});\n-    SmallVector<unsigned, 2> wpt_nm1;\n-\n-    SmallVector<int, 2> rep(2), spw(2);\n-    std::array<int, 3> fpw{{2, 2, 1}};\n-    int packSize0 = (isARow || isAVec4) ? 1 : 2;\n-    rep[0] = 2 * packSize0;\n-    spw[0] = fpw[0] * 4 * rep[0];\n-\n-    int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n-    rep[1] = 2 * packSize1;\n-    spw[1] = fpw[1] * 4 * rep[1];\n-\n-    do {\n-      wpt_nm1 = wpt;\n-      if (wpt[0] * wpt[1] < numWarps)\n-        wpt[0] = std::clamp<int>(wpt[0] * 2, 1, shape[0] / spw[0]);\n-      if (wpt[0] * wpt[1] < numWarps)\n-        wpt[1] = std::clamp<int>(wpt[1] * 2, 1, shape[1] / spw[1]);\n-    } while (wpt_nm1 != wpt);\n-\n-    return wpt;\n-  }\n-};\n-\n-class UpdateMMAForMMAv1 : public mlir::RewritePattern {\n-  const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n-\n-public:\n-  UpdateMMAForMMAv1(\n-      MLIRContext *context,\n-      const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n-      : RewritePattern(MatchAnyOpTypeTag{}, 1, context),\n-        mmaToUpdate(mmaToUpdate) {}\n-\n-  LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    // Nothing to update\n-    if (mmaToUpdate.empty())\n-      return failure();\n-\n-    if (auto dotOp = llvm::dyn_cast<DotOp>(op))\n-      return rewriteDotOp(op, rewriter);\n-    else if (auto cvtOp = llvm::dyn_cast<ConvertLayoutOp>(op))\n-      return rewriteCvtOp(op, rewriter);\n-    else if (auto expandDimsOp = llvm::dyn_cast<triton::ExpandDimsOp>(op))\n-      return rewriteExpandDimsOp(op, rewriter);\n-    else if (auto constOp = llvm::dyn_cast<arith::ConstantOp>(op))\n-      return rewriteConstantOp(op, rewriter);\n-    else\n-      return rewriteElementwiseOp(op, rewriter);\n-    return failure();\n-  }\n-\n-  LogicalResult rewriteDotOp(Operation *op,\n-                             mlir::PatternRewriter &rewriter) const {\n-    auto dotOp = llvm::cast<DotOp>(op);\n-    auto tensorTy = dotOp->getResult(0).getType().dyn_cast<RankedTensorType>();\n-    if (!tensorTy)\n-      return failure();\n-\n-    auto mma = dotOp.d()\n-                   .getType()\n-                   .cast<RankedTensorType>()\n-                   .getEncoding()\n-                   .dyn_cast<MmaEncodingAttr>();\n-    if (!mma || !mmaToUpdate.count(mma))\n-      return failure();\n-\n-    auto newTensorTy = getUpdatedType(tensorTy);\n-    rewriter.replaceOpWithNewOp<DotOp>(op, newTensorTy, dotOp.a(), dotOp.b(),\n-                                       dotOp.c(), dotOp.allowTF32());\n-    return success();\n-  }\n-\n-  LogicalResult rewriteCvtOp(Operation *op,\n-                             mlir::PatternRewriter &rewriter) const {\n-    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n-    if (!needUpdate(cvt.getResult().getType()))\n-      return failure();\n-    auto tensorTy = cvt.result().getType().dyn_cast<RankedTensorType>();\n+// Get the wpt for MMAv1 using more information.\n+// Reference the original logic here\n+// https://github.com/openai/triton/blob/0e4691e6dd91e001a8d33b71badf8b3314325459/lib/codegen/analysis/layout.cc#L223\n+SmallVector<unsigned> getWarpsPerCTA(ArrayRef<int64_t> shape, bool isARow,\n+                                     bool isBRow, bool isAVec4, bool isBVec4,\n+                                     int numWarps) {\n+  // TODO[Superjomn]: Share code with\n+  // DotOpMmaV1ConversionHelper::AParam/BParam, since same code to compute the\n+  // rep,spw and fpw.\n+  SmallVector<unsigned> wpt({1, 1});\n+  SmallVector<unsigned> wpt_nm1;\n+\n+  SmallVector<int, 2> rep(2), spw(2);\n+  std::array<int, 3> fpw{{2, 2, 1}};\n+  int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+  rep[0] = 2 * packSize0;\n+  spw[0] = fpw[0] * 4 * rep[0];\n+\n+  int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+  rep[1] = 2 * packSize1;\n+  spw[1] = fpw[1] * 4 * rep[1];\n+\n+  do {\n+    wpt_nm1 = wpt;\n+    if (wpt[0] * wpt[1] < numWarps)\n+      wpt[0] = std::clamp<int>(wpt[0] * 2, 1, shape[0] / spw[0]);\n+    if (wpt[0] * wpt[1] < numWarps)\n+      wpt[1] = std::clamp<int>(wpt[1] * 2, 1, shape[1] / spw[1]);\n+  } while (wpt_nm1 != wpt);\n+\n+  return wpt;\n+}\n \n-    auto newTensorTy = getUpdatedType(tensorTy);\n-    auto newOp = rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n-                                                              cvt.getOperand());\n-    return success();\n+// Given a (potentially malformed) DotOp, determines the optimal\n+// MMAEncoding to use on V100\n+LogicalResult getOptimizedV100MMaLayout(triton::DotOp dotOp,\n+                                        MmaEncodingAttr &old,\n+                                        MmaEncodingAttr &ret) {\n+  auto *ctx = dotOp->getContext();\n+  auto AT = dotOp.getA().getType().cast<RankedTensorType>();\n+  auto BT = dotOp.getB().getType().cast<RankedTensorType>();\n+  auto DT = dotOp.getD().getType().cast<RankedTensorType>();\n+  auto shapeA = AT.getShape();\n+  auto shapeB = BT.getShape();\n+  if (!DT.getEncoding())\n+    return mlir::failure();\n+  auto mmaLayout = DT.getEncoding().dyn_cast<MmaEncodingAttr>();\n+  if (!(mmaLayout && mmaLayout.isVolta()))\n+    return mlir::failure();\n+  // We have an MmaEncodingAttr here. Find the correct layout for it.\n+  auto dotOperandA = AT.getEncoding().cast<DotOperandEncodingAttr>();\n+  auto dotOperandB = BT.getEncoding().cast<DotOperandEncodingAttr>();\n+  bool isARow = dotOperandA.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+  bool isBRow = dotOperandB.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+  auto [isARow_, isBRow_, isAVec4_, isBVec4_, mmaId] =\n+      mmaLayout.decodeVoltaLayoutStates();\n+  bool isAVec4 = !isARow && (shapeA[isARow] <= 16);\n+  bool isBVec4 = isBRow && (shapeB[isBRow] <= 16);\n+  // The wpt of MMAv1 is also determined by isARow, isBRow and shape, and it\n+  // could only be set here for those states might be updated by previous\n+  // patterns in the Combine Pass.\n+  auto tgtWpt = getWarpsPerCTA(DT.getShape(), isARow, isBRow, isAVec4, isBVec4,\n+                               product(mmaLayout.getWarpsPerCTA()));\n+  if (isARow == isARow_ && isBRow == isBRow_ && isAVec4 == isAVec4_ &&\n+      isBVec4 == isBVec4_) {\n+    if (tgtWpt == mmaLayout.getWarpsPerCTA())\n+      return mlir::failure();\n   }\n+  // Recalculate the wpt, for here we could get the latest information, the\n+  // wpt should be updated.\n+  auto updatedWpt =\n+      getWarpsPerCTA(DT.getShape(), isARow, isBRow, isAVec4, isBVec4,\n+                     product(mmaLayout.getWarpsPerCTA()));\n+  // return results\n+  old = mmaLayout;\n+  ret =\n+      MmaEncodingAttr::get(ctx, mmaLayout.getVersionMajor(), updatedWpt,\n+                           AT.getShape(), BT.getShape(), isARow, isBRow, mmaId);\n+  return mlir::success();\n+}\n \n-  LogicalResult rewriteExpandDimsOp(Operation *op,\n-                                    mlir::PatternRewriter &rewriter) const {\n-    auto expandDims = llvm::cast<triton::ExpandDimsOp>(op);\n-    auto srcTy = expandDims.src().getType();\n-    auto resTy = expandDims.getResult().getType();\n-\n-    // the result type need to update\n-    if (!needUpdate(srcTy) && needUpdate(resTy)) {\n-      rewriter.replaceOpWithNewOp<triton::ExpandDimsOp>(op, expandDims.src(),\n-                                                        expandDims.axis());\n-      return success();\n-    }\n-\n-    return failure();\n+// Replace result op type\n+void setOpResultType(Operation *op, ArrayRef<Type> newTypes) {\n+  if (op->getNumResults() != newTypes.size())\n+    llvm_unreachable(\"number of types different from number of results\");\n+  // nothing to do\n+  if (op->getNumResults() == 0)\n+    return;\n+  // replace types\n+  for (unsigned i = 0; i < op->getNumResults(); i++) {\n+    Type newType = newTypes[i];\n+    op->getResult(i).setType(newType);\n   }\n-\n-  LogicalResult rewriteConstantOp(Operation *op,\n-                                  mlir::PatternRewriter &rewriter) const {\n-    auto constant = llvm::cast<arith::ConstantOp>(op);\n-    auto resTy = constant.getResult().getType();\n-    if (!needUpdate(resTy))\n-      return failure();\n-\n-    auto tensorTy = constant.getResult().getType().cast<RankedTensorType>();\n-    auto mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n-    if ((!mma))\n-      return failure();\n-\n-    auto newTensorTy = getUpdatedType(tensorTy);\n-    if (auto attr = constant.getValue().dyn_cast<SplatElementsAttr>()) {\n-      auto newRet =\n-          SplatElementsAttr::get(newTensorTy, attr.getSplatValue<Attribute>());\n-      rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newTensorTy, newRet);\n-      return success();\n+  // special case: arith.constant: we need to change the value attr\n+  if (isa<arith::ConstantOp>(op)) {\n+    Type newType = newTypes[0];\n+    auto attr = op->getAttrDictionary()\n+                    .get(\"value\")\n+                    .dyn_cast<mlir::DenseElementsAttr>();\n+    if (attr) {\n+      auto newAttr =\n+          mlir::DenseElementsAttr::getFromRawBuffer(newType, attr.getRawData());\n+      op->setAttr(\"value\", newAttr);\n     }\n-\n-    return failure();\n   }\n+}\n \n-  LogicalResult rewriteElementwiseOp(Operation *op,\n-                                     mlir::PatternRewriter &rewriter) const {\n-    if (op->getNumOperands() != 1 || op->getNumResults() != 1)\n-      return failure();\n-\n-    auto srcTy = op->getOperand(0).getType();\n-    auto resTy = op->getResult(0).getType();\n-    if (!needUpdate(srcTy) && needUpdate(resTy)) {\n-      op->getResult(0).setType(\n-          getUpdatedType(resTy.dyn_cast<RankedTensorType>()));\n-      return success();\n-    }\n-    return failure();\n+// update style type given the provided layoutMap\n+Type updateStaleType(\n+    const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &layoutMap,\n+    RankedTensorType type) {\n+  auto encoding = type.getEncoding();\n+  // mma encoding\n+  if (auto mma = encoding.dyn_cast<MmaEncodingAttr>()) {\n+    auto newMma = layoutMap.lookup(mma);\n+    if (!newMma)\n+      return Type();\n+    return RankedTensorType::get(type.getShape(), type.getElementType(),\n+                                 newMma);\n   }\n-\n-  RankedTensorType getUpdatedType(RankedTensorType type) const {\n-    if (!needUpdate(type))\n-      return type;\n-    auto encoding = type.getEncoding();\n-    if (auto mma = encoding.dyn_cast<MmaEncodingAttr>()) {\n-      auto newMma = mmaToUpdate.lookup(mma);\n+  // slice encoding\n+  else if (auto slice = encoding.dyn_cast<SliceEncodingAttr>()) {\n+    if (auto mma = slice.getParent().dyn_cast<MmaEncodingAttr>()) {\n+      auto newMma = layoutMap.lookup(mma);\n+      if (!newMma)\n+        return Type();\n+      auto newSlice =\n+          SliceEncodingAttr::get(slice.getContext(), slice.getDim(), newMma);\n       return RankedTensorType::get(type.getShape(), type.getElementType(),\n-                                   newMma);\n-    } else if (auto slice = encoding.dyn_cast<SliceEncodingAttr>()) {\n-      if (auto mma = slice.getParent().dyn_cast<MmaEncodingAttr>()) {\n-        auto newMma = mmaToUpdate.lookup(mma);\n-        auto newSlice =\n-            SliceEncodingAttr::get(slice.getContext(), slice.getDim(), newMma);\n-        return RankedTensorType::get(type.getShape(), type.getElementType(),\n-                                     newSlice);\n-      }\n-    } else if (auto dotOp = encoding.dyn_cast<DotOperandEncodingAttr>()) {\n-      if (auto mma = dotOp.getParent().dyn_cast<MmaEncodingAttr>()) {\n-        auto newMma = mmaToUpdate.lookup(mma);\n-        auto newDotOp =\n-            DotOperandEncodingAttr::get(dotOp.getContext(), dotOp.getOpIdx(),\n-                                        newMma, dotOp.getIsMMAv1Row());\n-        return RankedTensorType::get(type.getShape(), type.getElementType(),\n-                                     newDotOp);\n-      }\n+                                   newSlice);\n     }\n-    return type;\n-  }\n-\n-  // Tell if this type contains a wrong MMA encoding and need to update.\n-  bool needUpdate(Type type) const {\n-    auto tensorTy = type.dyn_cast<RankedTensorType>();\n-    if (!tensorTy)\n-      return false;\n-    return needUpdate(tensorTy);\n   }\n-\n-  // Tell if this type contains a wrong MMA encoding and need to update.\n-  bool needUpdate(RankedTensorType type) const {\n-    auto encoding = type.getEncoding();\n-    if (!encoding)\n-      return false;\n-\n-    MmaEncodingAttr mma;\n-    if ((mma = encoding.dyn_cast<MmaEncodingAttr>())) {\n-    } else if (auto slice = encoding.dyn_cast<SliceEncodingAttr>()) {\n-      mma = slice.getParent().dyn_cast<MmaEncodingAttr>();\n-    } else if (auto dotOp = encoding.dyn_cast<DotOperandEncodingAttr>()) {\n-      mma = dotOp.getParent().dyn_cast<MmaEncodingAttr>();\n+  // dot operand encoding\n+  else if (auto dotOp = encoding.dyn_cast<DotOperandEncodingAttr>()) {\n+    if (auto mma = dotOp.getParent().dyn_cast<MmaEncodingAttr>()) {\n+      auto newMma = layoutMap.lookup(mma);\n+      if (!newMma)\n+        return Type();\n+      auto newDotOp = DotOperandEncodingAttr::get(\n+          dotOp.getContext(), dotOp.getOpIdx(), newMma, dotOp.getIsMMAv1Row());\n+      return RankedTensorType::get(type.getShape(), type.getElementType(),\n+                                   newDotOp);\n     }\n-\n-    return mma && mmaToUpdate.count(mma);\n   }\n-};\n+  return Type();\n+}\n \n } // namespace\n \n-#define GEN_PASS_CLASSES\n-#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n-\n class UpdateMmaForVoltaPass\n     : public UpdateMmaForVoltaBase<UpdateMmaForVoltaPass> {\n public:\n   UpdateMmaForVoltaPass() = default;\n   void runOnOperation() override {\n     MLIRContext *context = &getContext();\n     ModuleOp m = getOperation();\n-\n-    llvm::DenseMap<MmaEncodingAttr, MmaEncodingAttr> mmaToUpdate;\n-    {\n-      mlir::RewritePatternSet patterns(context);\n-      patterns.add<CollectMmaToUpdateForVolta>(context, mmaToUpdate);\n-\n-      GreedyRewriteConfig config;\n-      config.enableRegionSimplification =\n-          false; // The pattern doesn't modify the IR\n-      if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n-        signalPassFailure();\n-    }\n-\n-    if (!mmaToUpdate.empty()) {\n-      mlir::RewritePatternSet patterns(context);\n-      patterns.add<UpdateMMAForMMAv1>(context, mmaToUpdate);\n-\n-      mlir::GreedyRewriteConfig config;\n-      // Make sure the slice and dot_operand layouts' parent mma are updated\n-      // before updating DotOp or it will get a mismatch parent-encoding.\n-      config.useTopDownTraversal = true;\n-\n-      if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n-        signalPassFailure();\n-\n-      if (fixupLoops(m).failed())\n-        signalPassFailure();\n-    }\n+    // Step 1:\n+    // Build a map from old MMA encoding to new MMA encoding.\n+    DenseMap<MmaEncodingAttr, MmaEncodingAttr> layoutMap;\n+    m.walk([&layoutMap](triton::DotOp dotOp) {\n+      MmaEncodingAttr newLayout;\n+      MmaEncodingAttr oldLayout;\n+      if (failed(getOptimizedV100MMaLayout(dotOp, oldLayout, newLayout)))\n+        return;\n+      layoutMap[oldLayout] = newLayout;\n+    });\n+    // Step 2:\n+    // Replace all wrong layouts with the right one\n+    m.walk([&layoutMap](Operation *op) {\n+      if (op->getNumResults() != 1)\n+        return;\n+      auto type = op->getResult(0).getType().dyn_cast<RankedTensorType>();\n+      if (!type)\n+        return;\n+      Type newType = updateStaleType(layoutMap, type);\n+      if (!newType)\n+        return;\n+      setOpResultType(op, {newType});\n+    });\n+    // Step 3:\n+    // We may have messed up some loops in the process.\n+    // Fix them up\n+    if (fixupLoops(m).failed())\n+      signalPassFailure();\n   }\n };\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -1,6 +1,6 @@\n #include \"Utility.h\"\n-#include \"mlir/Dialect/SCF/SCF.h\"\n-#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n+#include \"mlir/IR/IRMapping.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n \n namespace mlir {\n@@ -36,7 +36,7 @@ class FixupLoop : public mlir::RewritePattern {\n         forOp.getStep(), newInitArgs);\n     newForOp->moveBefore(forOp);\n     rewriter.setInsertionPointToStart(newForOp.getBody());\n-    BlockAndValueMapping mapping;\n+    IRMapping mapping;\n     for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n       mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n     mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());"}, {"filename": "lib/Target/LLVMIR/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -6,8 +6,7 @@ add_mlir_translation_library(TritonLLVMIR\n \n         LINK_LIBS PUBLIC\n         MLIRIR\n-        MLIRLLVMIR\n-        MLIRSCFToStandard\n+        MLIRLLVMDialect\n         MLIRSupport\n         MLIRTargetLLVMIRExport\n         )"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 67, "deletions": 25, "changes": 92, "file_content_changes": "@@ -14,19 +14,24 @@\n #include \"mlir/Transforms/Passes.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Tools/Sys/GetEnv.hpp\"\n+#include \"llvm/ADT/APInt.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/IR/Constants.h\"\n #include \"llvm/IRReader/IRReader.h\"\n #include \"llvm/Linker/Linker.h\"\n #include \"llvm/Support/SourceMgr.h\"\n+#include <dlfcn.h>\n #include <filesystem>\n+#include <iterator>\n \n namespace mlir {\n namespace triton {\n \n // Describes NVVM Metadata. It is used to record the nvvm related meta\n // information from mlir module.\n struct NVVMMetadata {\n-  int maxntidx{-1};\n+  SmallVector<int, 3> maxntid;\n   bool isKernel{};\n   // Free to extend with other information.\n };\n@@ -36,13 +41,26 @@ static void amendLLVMFunc(llvm::Function *func, const NVVMMetadata &metadata) {\n   auto *module = func->getParent();\n   auto &ctx = func->getContext();\n \n-  if (metadata.maxntidx > 0) {\n-    auto warps = llvm::ConstantInt::get(llvm::IntegerType::get(ctx, 32),\n-                                        llvm::APInt(32, metadata.maxntidx));\n-\n-    llvm::Metadata *md_args[] = {llvm::ValueAsMetadata::get(func),\n-                                 llvm::MDString::get(ctx, \"maxntidx\"),\n-                                 llvm::ValueAsMetadata::get(warps)};\n+  if (!metadata.maxntid.empty()) {\n+    auto maxntid =\n+        llvm::to_vector(llvm::map_range(metadata.maxntid, [&](int value) {\n+          return llvm::ConstantInt::get(llvm::IntegerType::get(ctx, 32),\n+                                        llvm::APInt(32, value));\n+        }));\n+\n+    SmallVector<llvm::Metadata *> md_args = {llvm::ValueAsMetadata::get(func)};\n+    if (maxntid.size() > 0) {\n+      md_args.push_back(llvm::MDString::get(ctx, \"maxntidx\"));\n+      md_args.push_back(llvm::ValueAsMetadata::get(maxntid[0]));\n+    }\n+    if (maxntid.size() > 1) {\n+      md_args.push_back(llvm::MDString::get(ctx, \"maxntidy\"));\n+      md_args.push_back(llvm::ValueAsMetadata::get(maxntid[1]));\n+    }\n+    if (maxntid.size() > 2) {\n+      md_args.push_back(llvm::MDString::get(ctx, \"maxntidz\"));\n+      md_args.push_back(llvm::ValueAsMetadata::get(maxntid[2]));\n+    }\n \n     module->getOrInsertNamedMetadata(\"nvvm.annotations\")\n         ->addOperand(llvm::MDNode::get(ctx, md_args));\n@@ -67,9 +85,10 @@ extractNVVMMetadata(mlir::ModuleOp module,\n     bool hasMetadata{};\n \n     // maxntid\n-    if (op->hasAttr(\"nvvm.maxntid\")) {\n-      auto attr = op->getAttr(\"nvvm.maxntid\");\n-      meta.maxntidx = attr.dyn_cast<IntegerAttr>().getInt();\n+    if (auto attr = op->getAttrOfType<ArrayAttr>(\"nvvm.maxntid\")) {\n+      llvm::transform(attr.getAsValueRange<IntegerAttr>(),\n+                      std::back_inserter(meta.maxntid),\n+                      [](llvm::APInt value) { return value.getZExtValue(); });\n       hasMetadata = true;\n     }\n \n@@ -116,21 +135,44 @@ static std::map<std::string, std::string> getExternLibs(mlir::ModuleOp module) {\n   }\n \n   if (!funcs.empty()) {\n-    // When using the Math Dialect, it is possible that some ops (e.g., log) are\n-    // lowered to a function call. In this case, we need to link libdevice\n-    // using its default path:\n-    // [triton root dir]/python/triton/language/libdevice.10.bc\n-    // TODO(Keren): handle external linkage other than libdevice?\n-    namespace fs = std::filesystem;\n     static const std::string libdevice = \"libdevice\";\n-    static const std::filesystem::path path = std::filesystem::path(__FILE__)\n-                                                  .parent_path()\n-                                                  .parent_path()\n-                                                  .parent_path()\n-                                                  .parent_path() /\n-                                              \"python\" / \"triton\" / \"language\" /\n-                                              \"libdevice.10.bc\";\n-    externLibs.try_emplace(libdevice, path.string());\n+    namespace fs = std::filesystem;\n+    // Search for libdevice relative to its library path if used from Python\n+    // Then native code is in `triton/_C/libtriton.so` and libdevice in\n+    // `triton/third_party/cuda/lib/libdevice.10.bc`\n+    static const auto this_library_path = [] {\n+      Dl_info fileinfo;\n+      if (dladdr(reinterpret_cast<void *>(&getExternLibs), &fileinfo) == 0) {\n+        return std::filesystem::path();\n+      }\n+      return std::filesystem::path(fileinfo.dli_fname);\n+    }();\n+    static const auto runtime_path =\n+        this_library_path.parent_path().parent_path() / \"third_party\" / \"cuda\" /\n+        \"lib\" / \"libdevice.10.bc\";\n+    if (fs::exists(runtime_path)) {\n+      externLibs.try_emplace(libdevice, runtime_path.string());\n+    } else {\n+      // When using the Math Dialect, it is possible that some ops (e.g., log)\n+      // are lowered to a function call. In this case, we need to link libdevice\n+      // using its default path:\n+      // [triton root dir]/python/triton/language/libdevice.10.bc\n+      // TODO(Keren): handle external linkage other than libdevice?\n+      static const auto this_file_path = std::filesystem::path(__FILE__);\n+      static const auto compiletime_path = this_file_path.parent_path()\n+                                               .parent_path()\n+                                               .parent_path()\n+                                               .parent_path() /\n+                                           \"python\" / \"triton\" / \"third_party\" /\n+                                           \"cuda\" / \"lib\" / \"libdevice.10.bc\";\n+      if (!fs::exists(compiletime_path)) {\n+        std::string error_msg = \"Can't find libdevice at neither \" +\n+                                runtime_path.string() + \" nor \" +\n+                                compiletime_path.string();\n+        llvm::report_fatal_error(error_msg.c_str());\n+      }\n+      externLibs.try_emplace(libdevice, compiletime_path.string());\n+    }\n   }\n \n   return externLibs;"}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -1,11 +1,14 @@\n #include \"triton/Target/PTX/PTXTranslation.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n+#include <optional>\n \n #include \"llvm/IR/IRBuilder.h\"\n #include \"llvm/IR/LegacyPassManager.h\"\n #include \"llvm/IR/Module.h\"\n #include \"llvm/IR/Verifier.h\"\n #include \"llvm/MC/TargetRegistry.h\"\n+#include \"llvm/Pass.h\"\n+#include \"llvm/Support/CommandLine.h\"\n #include \"llvm/Support/TargetSelect.h\"\n #include \"llvm/Target/TargetMachine.h\"\n \n@@ -34,15 +37,15 @@ std::string translateLLVMIRToPTX(llvm::Module &module, int cc, int version) {\n   // LLVM version in use may not officially support target hardware.\n   // Supported versions for LLVM 14 are here:\n   // https://github.com/llvm/llvm-project/blob/f28c006a5895fc0e329fe15fead81e37457cb1d1/clang/include/clang/Basic/BuiltinsNVPTX.def\n-  int maxPTX = std::min(75, version);\n-  int maxCC = std::min(86, cc);\n+  int maxPTX = std::min(80, version);\n+  int maxCC = std::min(90, cc);\n   // options\n   auto options = llvm::cl::getRegisteredOptions();\n   auto *shortPtr =\n       static_cast<llvm::cl::opt<bool> *>(options[\"nvptx-short-ptr\"]);\n   assert(shortPtr);\n   shortPtr->setValue(true);\n-  std::string sm = \"sm_\" + std::to_string(maxCC);\n+  std::string sm = cc == 90 ? \"sm_90a\" : \"sm_\" + std::to_string(cc);\n   // max PTX version\n   int ptxMajor = maxPTX / 10;\n   int ptxMinor = maxPTX % 10;\n@@ -72,7 +75,7 @@ std::string translateLLVMIRToPTX(llvm::Module &module, int cc, int version) {\n   opt.NoNaNsFPMath = true;\n   llvm::TargetMachine *machine = target->createTargetMachine(\n       module.getTargetTriple(), proc, features, opt, llvm::Reloc::PIC_,\n-      llvm::None, llvm::CodeGenOpt::Aggressive);\n+      std::nullopt, llvm::CodeGenOpt::Aggressive);\n   // set data layout\n   if (layout.empty())\n     module.setDataLayout(machine->createDataLayout());"}, {"filename": "python/setup.py", "status": "modified", "additions": 58, "deletions": 22, "changes": 80, "file_content_changes": "@@ -6,6 +6,7 @@\n import subprocess\n import sys\n import tarfile\n+import tempfile\n import urllib.request\n from distutils.version import LooseVersion\n from typing import NamedTuple\n@@ -30,9 +31,9 @@ def get_build_type():\n         # TODO: change to release when stable enough\n         return \"TritonRelBuildWithAsserts\"\n \n-\n # --- third party packages -----\n \n+\n class Package(NamedTuple):\n     package: str\n     name: str\n@@ -42,24 +43,33 @@ class Package(NamedTuple):\n     lib_flag: str\n     syspath_var_name: str\n \n+# pybind11\n+\n \n def get_pybind11_package_info():\n     name = \"pybind11-2.10.0\"\n     url = \"https://github.com/pybind/pybind11/archive/refs/tags/v2.10.0.tar.gz\"\n     return Package(\"pybind11\", name, url, \"include/pybind11/pybind11.h\", \"PYBIND11_INCLUDE_DIR\", \"\", \"PYBIND11_SYSPATH\")\n \n+# llvm\n+\n \n def get_llvm_package_info():\n     # download if nothing is installed\n     system = platform.system()\n-    system_suffix = {\"Linux\": \"linux-gnu-ubuntu-18.04\", \"Darwin\": \"apple-darwin\"}[system]\n-    use_assert_enabled_llvm = check_env_flag(\"TRITON_USE_ASSERT_ENABLED_LLVM\", \"False\")\n-    if use_assert_enabled_llvm:\n-        name = 'llvm+mlir-14.0.0-x86_64-{}-assert'.format(system_suffix)\n-        url = \"https://github.com/shintaro-iwasaki/llvm-releases/releases/download/llvm-14.0.0-329fda39c507/{}.tar.xz\".format(name)\n+    if system == \"Darwin\":\n+        system_suffix = \"apple-darwin\"\n+    elif system == \"Linux\":\n+        vglibc = tuple(map(int, platform.libc_ver()[1].split('.')))\n+        vglibc = vglibc[0] * 100 + vglibc[1]\n+        linux_suffix = 'ubuntu-18.04' if vglibc > 217 else 'centos-7'\n+        system_suffix = f\"linux-gnu-{linux_suffix}\"\n     else:\n-        name = 'clang+llvm-14.0.0-x86_64-{}'.format(system_suffix)\n-        url = \"https://github.com/llvm/llvm-project/releases/download/llvmorg-14.0.0/{}.tar.xz\".format(name)\n+        return Package(\"llvm\", \"LLVM-C.lib\", \"\", \"lib\", \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n+    use_assert_enabled_llvm = check_env_flag(\"TRITON_USE_ASSERT_ENABLED_LLVM\", \"False\")\n+    release_suffix = \"assert\" if use_assert_enabled_llvm else \"release\"\n+    name = f'llvm+mlir-17.0.0-x86_64-{system_suffix}-{release_suffix}'\n+    url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/llvm-17.0.0-37b7a60cd74b/{name}.tar.xz\"\n     return Package(\"llvm\", name, url, \"lib\", \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n \n \n@@ -69,25 +79,47 @@ def get_thirdparty_packages(triton_cache_path):\n     for p in packages:\n         package_root_dir = os.path.join(triton_cache_path, p.package)\n         package_dir = os.path.join(package_root_dir, p.name)\n-        test_file_path = os.path.join(package_dir, p.test_file)\n         if p.syspath_var_name in os.environ:\n             package_dir = os.environ[p.syspath_var_name]\n+        test_file_path = os.path.join(package_dir, p.test_file)\n         if not os.path.exists(test_file_path):\n             try:\n                 shutil.rmtree(package_root_dir)\n             except Exception:\n                 pass\n             os.makedirs(package_root_dir, exist_ok=True)\n-            print('downloading and extracting {} ...'.format(p.url))\n+            print(f'downloading and extracting {p.url} ...')\n             ftpstream = urllib.request.urlopen(p.url)\n             file = tarfile.open(fileobj=ftpstream, mode=\"r|*\")\n             file.extractall(path=package_root_dir)\n         if p.include_flag:\n-            thirdparty_cmake_args.append(\"-D{}={}/include\".format(p.include_flag, package_dir))\n+            thirdparty_cmake_args.append(f\"-D{p.include_flag}={package_dir}/include\")\n         if p.lib_flag:\n-            thirdparty_cmake_args.append(\"-D{}={}/lib\".format(p.lib_flag, package_dir))\n+            thirdparty_cmake_args.append(f\"-D{p.lib_flag}={package_dir}/lib\")\n     return thirdparty_cmake_args\n \n+# ---- package data ---\n+\n+\n+def download_and_copy_ptxas():\n+    base_dir = os.path.dirname(__file__)\n+    src_path = \"bin/ptxas\"\n+    url = \"https://conda.anaconda.org/nvidia/label/cuda-12.0.0/linux-64/cuda-nvcc-12.0.76-0.tar.bz2\"\n+    dst_prefix = os.path.join(base_dir, \"triton\")\n+    dst_suffix = os.path.join(\"third_party\", \"cuda\", src_path)\n+    dst_path = os.path.join(dst_prefix, dst_suffix)\n+    if not os.path.exists(dst_path):\n+        print(f'downloading and extracting {url} ...')\n+        ftpstream = urllib.request.urlopen(url)\n+        file = tarfile.open(fileobj=ftpstream, mode=\"r|*\")\n+        with tempfile.TemporaryDirectory() as temp_dir:\n+            file.extractall(path=temp_dir)\n+            src_path = os.path.join(temp_dir, src_path)\n+            os.makedirs(os.path.split(dst_path)[0], exist_ok=True)\n+            shutil.copy(src_path, dst_path)\n+    return dst_suffix\n+\n+\n # ---- cmake extension ----\n \n \n@@ -127,7 +159,11 @@ def run(self):\n \n     def build_extension(self, ext):\n         lit_dir = shutil.which('lit')\n-        triton_cache_path = os.path.join(os.environ[\"HOME\"], \".triton\")\n+        user_home = os.getenv(\"HOME\") or os.getenv(\"USERPROFILE\") or \\\n+            os.getenv(\"HOMEPATH\") or None\n+        if not user_home:\n+            raise RuntimeError(\"Could not find user home directory\")\n+        triton_cache_path = os.path.join(user_home, \".triton\")\n         # lit is used by the test suite\n         thirdparty_cmake_args = get_thirdparty_packages(triton_cache_path)\n         extdir = os.path.abspath(os.path.dirname(self.get_ext_fullpath(ext.path)))\n@@ -144,28 +180,32 @@ def build_extension(self, ext):\n             \"-DPython3_EXECUTABLE:FILEPATH=\" + sys.executable,\n             \"-DCMAKE_VERBOSE_MAKEFILE:BOOL=ON\",\n             \"-DPYTHON_INCLUDE_DIRS=\" + python_include_dir,\n-            \"-DLLVM_EXTERNAL_LIT=\" + lit_dir,\n-        ] + thirdparty_cmake_args\n+        ]\n+        if lit_dir is not None:\n+            cmake_args.append(\"-DLLVM_EXTERNAL_LIT=\" + lit_dir)\n+        cmake_args.extend(thirdparty_cmake_args)\n \n         # configuration\n         cfg = get_build_type()\n         build_args = [\"--config\", cfg]\n \n         if platform.system() == \"Windows\":\n-            cmake_args += [\"-DCMAKE_RUNTIME_OUTPUT_DIRECTORY_{}={}\".format(cfg.upper(), extdir)]\n+            cmake_args += [f\"-DCMAKE_RUNTIME_OUTPUT_DIRECTORY_{cfg.upper()}={extdir}\"]\n             if sys.maxsize > 2**32:\n                 cmake_args += [\"-A\", \"x64\"]\n             build_args += [\"--\", \"/m\"]\n         else:\n             import multiprocessing\n             cmake_args += [\"-DCMAKE_BUILD_TYPE=\" + cfg]\n-            build_args += [\"--\", '-j' + str(2 * multiprocessing.cpu_count())]\n+            build_args += ['-j' + str(2 * multiprocessing.cpu_count())]\n \n         env = os.environ.copy()\n         subprocess.check_call([\"cmake\", self.base_dir] + cmake_args, cwd=self.build_temp, env=env)\n         subprocess.check_call([\"cmake\", \"--build\", \".\"] + build_args, cwd=self.build_temp)\n \n \n+download_and_copy_ptxas()\n+\n setup(\n     name=\"triton\",\n     version=\"2.0.0\",\n@@ -180,11 +220,7 @@ def build_extension(self, ext):\n         \"torch\",\n         \"lit\",\n     ],\n-    package_data={\n-        \"triton/ops\": [\"*.c\"],\n-        \"triton/ops/blocksparse\": [\"*.c\"],\n-        \"triton/language\": [\"*.bc\"]\n-    },\n+    package_data={\"triton\": [\"third_party/**/*\"]},\n     include_package_data=True,\n     ext_modules=[CMakeExtension(\"triton\", \"triton/_C/\")],\n     cmdclass={\"build_ext\": CMakeBuild},"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 144, "deletions": 68, "changes": 212, "file_content_changes": "@@ -8,9 +8,11 @@\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Transforms/Passes.h\"\n \n-#include \"mlir/Parser.h\"\n+#include \"mlir/Parser/Parser.h\"\n #include \"mlir/Support/FileUtilities.h\"\n \n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlow.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"triton/Analysis/Allocation.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n@@ -190,14 +192,22 @@ void init_triton_ir(py::module &&m) {\n              if (mlir::Operation *definingOp = self.getDefiningOp())\n                definingOp->setAttr(name, attr);\n              else {\n-               /* issue a warning */\n+               auto arg = self.cast<mlir::BlockArgument>();\n+               int id = arg.getArgNumber();\n+               std::string attrName = name + \"_arg\" + std::to_string(id);\n+               mlir::Block *owner = arg.getOwner();\n+               if (owner->isEntryBlock() &&\n+                   !mlir::isa<mlir::func::FuncOp>(owner->getParentOp())) {\n+                 owner->getParentOp()->setAttr(attrName, attr);\n+               }\n              }\n            })\n       .def(\"get_context\", &mlir::Value::getContext)\n       .def(\"replace_all_uses_with\",\n            [](mlir::Value &self, mlir::Value &newValue) {\n              self.replaceAllUsesWith(newValue);\n-           });\n+           })\n+      .def(\"get_type\", &mlir::Value::getType);\n \n   py::class_<mlir::BlockArgument, mlir::Value>(m, \"block_argument\");\n \n@@ -211,6 +221,11 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::Block &self, int index) -> mlir::BlockArgument {\n              return self.getArgument(index);\n            })\n+      .def(\"add_argument\",\n+           [](mlir::Block &self, mlir::Type ty) {\n+             auto loc = mlir::UnknownLoc::get(ty.getContext());\n+             self.addArgument(ty, loc);\n+           })\n       .def(\"get_num_arguments\", &mlir::Block::getNumArguments)\n       .def(\"dump\", &mlir::Block::dump)\n       .def(\"move_before\", &mlir::Block::moveBefore)\n@@ -226,20 +241,34 @@ void init_triton_ir(py::module &&m) {\n              self.dropAllUses();\n              self.erase();\n            })\n-      .def(\"replace_use_in_block_with\", [](mlir::Block &self, mlir::Value &v,\n-                                           mlir::Value &newVal) {\n-        v.replaceUsesWithIf(newVal, [&](mlir::OpOperand &operand) {\n-          mlir::Operation *user = operand.getOwner();\n-          mlir::Block *currentBlock = user->getBlock();\n-          while (currentBlock) {\n-            if (currentBlock == &self)\n-              return true;\n-            // Move up one level\n-            currentBlock = currentBlock->getParent()->getParentOp()->getBlock();\n-          }\n-          return false;\n-        });\n-      });\n+      .def(\"replace_use_in_block_with\",\n+           [](mlir::Block &self, mlir::Value &v, mlir::Value &newVal) {\n+             v.replaceUsesWithIf(newVal, [&](mlir::OpOperand &operand) {\n+               mlir::Operation *user = operand.getOwner();\n+               mlir::Block *currentBlock = user->getBlock();\n+               while (currentBlock) {\n+                 if (currentBlock == &self)\n+                   return true;\n+                 // Move up one level\n+                 currentBlock =\n+                     currentBlock->getParent()->getParentOp()->getBlock();\n+               }\n+               return false;\n+             });\n+           })\n+      .def(\"__str__\",\n+           [](mlir::Block &self) {\n+             std::string str;\n+             llvm::raw_string_ostream os(str);\n+             self.print(os);\n+             return str;\n+           })\n+      .def(\"has_terminator\",\n+           [](mlir::Block &self) {\n+             return !self.empty() &&\n+                    self.back().hasTrait<mlir::OpTrait::IsTerminator>();\n+           })\n+      .def(\"erase\", [](mlir::Block &self) { self.erase(); });\n \n   // using eattr = ir::attribute_kind_t;\n   // py::enum_<eattr>(m, \"attribute_kind\")\n@@ -321,7 +350,7 @@ void init_triton_ir(py::module &&m) {\n              return str;\n            })\n       .def(\"push_back\",\n-           [](mlir::ModuleOp &self, mlir::FuncOp &funcOp) -> void {\n+           [](mlir::ModuleOp &self, mlir::func::FuncOp &funcOp) -> void {\n              self.push_back(funcOp);\n            })\n       .def(\"has_function\",\n@@ -331,16 +360,18 @@ void init_triton_ir(py::module &&m) {\n              return false;\n            })\n       .def(\"get_function\",\n-           [](mlir::ModuleOp &self, std::string &funcName) -> mlir::FuncOp {\n-             return self.lookupSymbol<mlir::FuncOp>(funcName);\n-           })\n-      .def(\"get_single_function\", [](mlir::ModuleOp &self) -> mlir::FuncOp {\n-        llvm::SmallVector<mlir::FuncOp> funcs;\n-        self.walk([&](mlir::FuncOp func) { funcs.push_back(func); });\n-        if (funcs.size() != 1)\n-          throw std::runtime_error(\"Expected a single function\");\n-        return funcs[0];\n-      });\n+           [](mlir::ModuleOp &self,\n+              std::string &funcName) -> mlir::func::FuncOp {\n+             return self.lookupSymbol<mlir::func::FuncOp>(funcName);\n+           })\n+      .def(\"get_single_function\",\n+           [](mlir::ModuleOp &self) -> mlir::func::FuncOp {\n+             llvm::SmallVector<mlir::func::FuncOp> funcs;\n+             self.walk([&](mlir::func::FuncOp func) { funcs.push_back(func); });\n+             if (funcs.size() != 1)\n+               throw std::runtime_error(\"Expected a single function\");\n+             return funcs[0];\n+           });\n \n   m.def(\"make_attr\",\n         [](const std::vector<int> &values, mlir::MLIRContext &context) {\n@@ -360,48 +391,50 @@ void init_triton_ir(py::module &&m) {\n         mlir::DialectRegistry registry;\n         registry.insert<mlir::triton::TritonDialect,\n                         mlir::triton::gpu::TritonGPUDialect,\n-                        mlir::math::MathDialect, mlir::arith::ArithmeticDialect,\n-                        mlir::StandardOpsDialect, mlir::scf::SCFDialect>();\n+                        mlir::math::MathDialect, mlir::arith::ArithDialect,\n+                        mlir::func::FuncDialect, mlir::scf::SCFDialect,\n+                        mlir::cf::ControlFlowDialect>();\n         context.appendDialectRegistry(registry);\n         context.loadAllAvailableDialects();\n \n         // parse module\n-        mlir::OwningOpRef<mlir::ModuleOp> module(\n-            mlir::parseSourceFile(inputFilename, &context));\n+        mlir::OwningOpRef<mlir::ModuleOp> module =\n+            mlir::parseSourceFile<mlir::ModuleOp>(inputFilename, &context);\n+        if (!module)\n+          throw std::runtime_error(\"Parse MLIR file failed.\");\n         // locations are incompatible with ptx < 7.5 !\n         module->walk([](mlir::Operation *op) {\n           op->setLoc(mlir::UnknownLoc::get(op->getContext()));\n         });\n-        if (!module)\n-          throw std::runtime_error(\"Parse MLIR file failed.\");\n \n         return module->clone();\n       },\n       ret::take_ownership);\n \n-  py::class_<mlir::FuncOp, mlir::OpState>(m, \"function\")\n+  py::class_<mlir::func::FuncOp, mlir::OpState>(m, \"function\")\n       // .def_property_readonly(\"attrs\", &ir::function::attrs)\n       // .def(\"add_attr\", &ir::function::add_attr);\n       .def(\"args\",\n-           [](mlir::FuncOp &self, unsigned idx) -> mlir::BlockArgument {\n+           [](mlir::func::FuncOp &self, unsigned idx) -> mlir::BlockArgument {\n              return self.getArgument(idx);\n            })\n       .def(\n           \"add_entry_block\",\n-          [](mlir::FuncOp &self) -> mlir::Block * {\n+          [](mlir::func::FuncOp &self) -> mlir::Block * {\n             return self.addEntryBlock();\n           },\n           ret::reference)\n       .def(\n           \"set_arg_attr\",\n-          [](mlir::FuncOp &self, int arg_no, const std::string &name, int val) {\n+          [](mlir::func::FuncOp &self, int arg_no, const std::string &name,\n+             int val) {\n             // set arg attributes \"name\" to value \"val\"\n             auto attrTy = mlir::IntegerType::get(self.getContext(), 32);\n             self.setArgAttr(arg_no, name, mlir::IntegerAttr::get(attrTy, val));\n           },\n           ret::reference)\n-      .def_property_readonly(\"type\", &mlir::FuncOp::getType)\n-      .def(\"reset_type\", &mlir::FuncOp::setType);\n+      .def_property_readonly(\"type\", &mlir::func::FuncOp::getFunctionType)\n+      .def(\"reset_type\", &mlir::func::FuncOp::setType);\n \n   py::class_<mlir::OpBuilder::InsertPoint>(m, \"InsertPoint\");\n \n@@ -418,13 +451,13 @@ void init_triton_ir(py::module &&m) {\n       .def(\"ret\",\n            [](mlir::OpBuilder &self, std::vector<mlir::Value> &vals) -> void {\n              auto loc = self.getUnknownLoc();\n-             self.create<mlir::ReturnOp>(loc, vals);\n+             self.create<mlir::func::ReturnOp>(loc, vals);\n            })\n       .def(\"call\",\n-           [](mlir::OpBuilder &self, mlir::FuncOp &func,\n+           [](mlir::OpBuilder &self, mlir::func::FuncOp &func,\n               std::vector<mlir::Value> &args) -> mlir::OpState {\n              auto loc = self.getUnknownLoc();\n-             return self.create<mlir::CallOp>(loc, func, args);\n+             return self.create<mlir::func::CallOp>(loc, func, args);\n            })\n       // insertion block/point\n       .def(\"set_insertion_point_to_start\",\n@@ -435,6 +468,10 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::OpBuilder &self, mlir::Block &block) {\n              self.setInsertionPointToEnd(&block);\n            })\n+      .def(\"set_insertion_point_after\",\n+           [](mlir::OpBuilder &self, mlir::Operation &op) {\n+             self.setInsertionPointAfter(&op);\n+           })\n       .def(\n           \"get_insertion_block\",\n           [](mlir::OpBuilder &self) -> mlir::Block * {\n@@ -468,6 +505,12 @@ void init_triton_ir(py::module &&m) {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n                  loc, v, self.getI1Type()));\n            })\n+      .def(\"get_int8\",\n+           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 loc, v, self.getI8Type()));\n+           })\n       .def(\"get_int32\",\n            [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n@@ -480,9 +523,23 @@ void init_triton_ir(py::module &&m) {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n                  loc, v, self.getI64Type()));\n            })\n-      // .def(\"get_uint32\", &ir::builder::get_int32, ret::reference)\n-      // .def(\"get_float16\", &ir::builder::get_float16, ret::reference)\n-      .def(\"get_float32\",\n+      // bfloat16 cannot be initialized as it is treated as int16 for now\n+      //.def(\"get_bf16\",\n+      //     [](mlir::OpBuilder &self, float v) -> mlir::Value {\n+      //       auto loc = self.getUnknownLoc();\n+      //       auto type = self.getBF16Type();\n+      //       return self.create<mlir::arith::ConstantFloatOp>(\n+      //           loc,\n+      //           mlir::APFloat(type.getFloatSemantics(), std::to_string(v)),\n+      //           type);\n+      //     })\n+      .def(\"get_fp16\",\n+           [](mlir::OpBuilder &self, float v) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<mlir::arith::ConstantOp>(\n+                 loc, self.getF16FloatAttr(v));\n+           })\n+      .def(\"get_fp32\",\n            [](mlir::OpBuilder &self, float v) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::ConstantOp>(\n@@ -567,15 +624,16 @@ void init_triton_ir(py::module &&m) {\n       .def(\"get_or_insert_function\",\n            [](mlir::OpBuilder &self, mlir::ModuleOp &module,\n               std::string &funcName, mlir::Type &funcType,\n-              std::string &visibility) -> mlir::FuncOp {\n+              std::string &visibility) -> mlir::func::FuncOp {\n              if (mlir::Operation *funcOperation = module.lookupSymbol(funcName))\n-               return llvm::dyn_cast<mlir::FuncOp>(funcOperation);\n+               return llvm::dyn_cast<mlir::func::FuncOp>(funcOperation);\n              auto loc = self.getUnknownLoc();\n              if (auto funcTy = funcType.dyn_cast<mlir::FunctionType>()) {\n                llvm::SmallVector<mlir::NamedAttribute> attrs = {\n                    mlir::NamedAttribute(self.getStringAttr(\"sym_visibility\"),\n                                         self.getStringAttr(visibility))};\n-               return self.create<mlir::FuncOp>(loc, funcName, funcTy, attrs);\n+               return self.create<mlir::func::FuncOp>(loc, funcName, funcTy,\n+                                                      attrs);\n              }\n              throw std::runtime_error(\"invalid function type\");\n            })\n@@ -602,6 +660,22 @@ void init_triton_ir(py::module &&m) {\n             return new mlir::Block();\n           },\n           ret::reference)\n+      // Unstructured control flow\n+      .def(\"create_cond_branch\",\n+           [](mlir::OpBuilder &self, mlir::Value condition,\n+              mlir::Block *trueDest, mlir::Block *falseDest) {\n+             auto loc = self.getUnknownLoc();\n+             self.create<mlir::cf::CondBranchOp>(loc, condition, trueDest,\n+                                                 falseDest);\n+             return;\n+           })\n+      .def(\"create_branch\",\n+           [](mlir::OpBuilder &self, mlir::Block *dest,\n+              std::vector<mlir::Value> &args) {\n+             auto loc = self.getUnknownLoc();\n+             self.create<mlir::cf::BranchOp>(loc, dest, args);\n+             return;\n+           })\n       // Structured control flow\n       .def(\"create_for_op\",\n            [](mlir::OpBuilder &self, mlir::Value &lb, mlir::Value &ub,\n@@ -645,12 +719,6 @@ void init_triton_ir(py::module &&m) {\n              return self.create<mlir::triton::MakeRangeOp>(loc, retType, start,\n                                                            end);\n            })\n-      .def(\"create_get_program_id\",\n-           [](mlir::OpBuilder &self, int axis) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::GetProgramIdOp>(\n-                 loc, self.getI32Type(), axis);\n-           })\n \n       // Cast instructions\n       // Conversions for custom FP types (FP8)\n@@ -731,14 +799,14 @@ void init_triton_ir(py::module &&m) {\n       .def(\"create_to_index\",\n            [](mlir::OpBuilder &self, mlir::Value &input) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::IndexCastOp>(loc, input,\n-                                                          self.getIndexType());\n+             return self.create<mlir::arith::IndexCastOp>(\n+                 loc, self.getIndexType(), input);\n            })\n       .def(\"create_index_to_si\",\n            [](mlir::OpBuilder &self, mlir::Value &input) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::IndexCastOp>(loc, input,\n-                                                          self.getI32Type());\n+             return self.create<mlir::arith::IndexCastOp>(\n+                 loc, self.getI32Type(), input);\n            })\n       .def(\"create_fmul\",\n            [](mlir::OpBuilder &self, mlir::Value &lhs,\n@@ -1028,10 +1096,12 @@ void init_triton_ir(py::module &&m) {\n                  loc, ptrs, cacheModifier, evictionPolicy, isVolatile);\n            })\n       .def(\"create_store\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptrs,\n-              mlir::Value &value) -> void {\n+           [](mlir::OpBuilder &self, mlir::Value &ptrs, mlir::Value &value,\n+              mlir::triton::CacheModifier cacheModifier,\n+              mlir::triton::EvictionPolicy evictionPolicy) -> void {\n              auto loc = self.getUnknownLoc();\n-             self.create<mlir::triton::StoreOp>(loc, ptrs, value);\n+             self.create<mlir::triton::StoreOp>(loc, ptrs, value, cacheModifier,\n+                                                evictionPolicy);\n            })\n       .def(\"create_masked_load\",\n            [](mlir::OpBuilder &self, mlir::Value &ptrs, mlir::Value &mask,\n@@ -1046,9 +1116,11 @@ void init_triton_ir(py::module &&m) {\n            })\n       .def(\"create_masked_store\",\n            [](mlir::OpBuilder &self, mlir::Value &ptrs, mlir::Value &val,\n-              mlir::Value &mask) -> void {\n+              mlir::Value &mask, mlir::triton::CacheModifier cacheModifier,\n+              mlir::triton::EvictionPolicy evictionPolicy) -> void {\n              auto loc = self.getUnknownLoc();\n-             self.create<mlir::triton::StoreOp>(loc, ptrs, val, mask);\n+             self.create<mlir::triton::StoreOp>(loc, ptrs, val, mask,\n+                                                cacheModifier, evictionPolicy);\n            })\n       .def(\"create_view\",\n            [](mlir::OpBuilder &self, mlir::Value &arg,\n@@ -1251,8 +1323,8 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::OpBuilder &self, mlir::Value &condition,\n               mlir::Value &trueValue, mlir::Value &falseValue) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             return self.create<mlir::SelectOp>(loc, condition, trueValue,\n-                                                falseValue);\n+             return self.create<mlir::arith::SelectOp>(loc, condition,\n+                                                       trueValue, falseValue);\n            })\n       .def(\"create_printf\",\n            [](mlir::OpBuilder &self, const std::string &prefix,\n@@ -1364,7 +1436,7 @@ void init_triton_ir(py::module &&m) {\n              self.addPass(mlir::triton::createConvertTritonGPUToLLVMPass());\n            })\n       .def(\"add_scf_to_cfg\", [](mlir::PassManager &self) {\n-        self.addPass(mlir::createLowerToCFGPass());\n+        self.addPass(mlir::createConvertSCFToCFPass());\n       });\n }\n \n@@ -1379,6 +1451,7 @@ void init_triton_translation(py::module &m) {\n   m.def(\n       \"translate_triton_gpu_to_llvmir\",\n       [](mlir::ModuleOp op, int computeCapability) {\n+        py::gil_scoped_release allow_threads;\n         llvm::LLVMContext llvmContext;\n         auto llvmModule = ::mlir::triton::translateTritonGPUToLLVMIR(\n             &llvmContext, op, computeCapability);\n@@ -1396,6 +1469,7 @@ void init_triton_translation(py::module &m) {\n   m.def(\n       \"translate_llvmir_to_ptx\",\n       [](const std::string llvmIR, int capability, int version) -> std::string {\n+        py::gil_scoped_release allow_threads;\n         // create LLVM module from C++\n         llvm::LLVMContext context;\n         std::unique_ptr<llvm::MemoryBuffer> buffer =\n@@ -1439,7 +1513,9 @@ void init_triton_translation(py::module &m) {\n           std::string cmd;\n           int err;\n           cmd = ptxasPath + \" -v --gpu-name=sm_\" + std::to_string(capability) +\n-                \" \" + _fsrc + \" -o \" + _fsrc + \".o 2> \" + _flog;\n+                (capability == 90 ? \"a \" : \" \") + _fsrc + \" -o \" + _fsrc +\n+                \".o 2> \" + _flog;\n+\n           err = system(cmd.c_str());\n           if (err != 0) {\n             std::ifstream _log(_flog);"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 197, "deletions": 46, "changes": 243, "file_content_changes": "@@ -110,7 +110,7 @@ def check_type_supported(dtype):\n         pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n \n \n-@pytest.mark.parametrize(\"dtype_x\", [dtype_x for dtype_x in dtypes] + [\"bfloat16\"])\n+@pytest.mark.parametrize(\"dtype_x\", list(dtypes) + [\"bfloat16\"])\n def test_empty_kernel(dtype_x, device='cuda'):\n     SIZE = 128\n \n@@ -330,7 +330,10 @@ def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n def test_shift_op(dtype_x, dtype_y, op, device='cuda'):\n     expr = f'x {op} y'\n     bw = max(_bitwidth(dtype_x), _bitwidth(dtype_y))\n-    dtype_z = f'uint{bw}'\n+    if dtype_x.startswith('int'):\n+        dtype_z = f'int{bw}'\n+    else:\n+        dtype_z = f'uint{bw}'\n     numpy_expr = f'x.astype(np.{dtype_z}) {op} y.astype(np.{dtype_z})'\n     _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device, y_low=0, y_high=65)\n \n@@ -627,7 +630,7 @@ def kernel(X, Z):\n \n     # triton result\n     rs = RandomState(17)\n-    x = numpy_random((n_programs, ), dtype_str=dtype_x_str, rs=rs)\n+    x = np.array([2**i for i in range(n_programs)], dtype=getattr(np, dtype_x_str))\n     if mode == 'all_neg':\n         x = -np.abs(x)\n     if mode == 'all_pos':\n@@ -773,7 +776,7 @@ def kernel(X, Z, BITCAST: tl.constexpr):\n         assert to_numpy(z_tri) == z_ref\n \n \n-@pytest.mark.parametrize(\"dtype_str\", [dtype_str for dtype_str in torch_dtypes])\n+@pytest.mark.parametrize(\"dtype_str\", list(torch_dtypes))\n def test_store_constant(dtype_str):\n     check_type_supported(dtype_str)\n \n@@ -1080,7 +1083,7 @@ def kernel(X, stride_xm, stride_xn,\n \n @pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype\",\n                          [(*shape, 4, False, False, epilogue, allow_tf32, dtype)\n-                          for shape in [(64, 64, 64)]\n+                          for shape in [(64, 64, 64), (16, 16, 16)]\n                           for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n                           for allow_tf32 in [True, False]\n                           for dtype in ['float16', 'float32']\n@@ -1094,7 +1097,7 @@ def kernel(X, stride_xm, stride_xn,\n                                            [64, 128, 128, 4],\n                                            [32, 128, 64, 2],\n                                            [128, 128, 64, 2],\n-                                           [64, 128, 128, 4]]\n+                                           [64, 128, 128, 2]]\n                           for allow_tf32 in [True]\n                           for col_a in [True, False]\n                           for col_b in [True, False]\n@@ -1108,6 +1111,10 @@ def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype, devi\n             pytest.skip(\"Only test int8 on devices with sm >= 80\")\n         elif dtype == 'float32' and allow_tf32:\n             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n+    if capability[0] == 7:\n+        if (M, N, K, num_warps) == (128, 256, 32, 8):\n+            pytest.skip(\"shared memory out of resource\")\n+\n     torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n \n     # triton kernel\n@@ -1221,8 +1228,10 @@ def kernel(X, stride_xm, stride_xk,\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n     # make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']\n-    assert 'ld.global.v4' in ptx\n-    assert 'st.global.v4' in ptx\n+    if K > 16 or N > 16 or M > 16:\n+        # XXX: skip small sizes because they are not vectorized\n+        assert 'ld.global.v4' in ptx\n+        assert 'st.global.v4' in ptx\n     if dtype == 'float32' and allow_tf32:\n         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n     elif dtype == 'float32' and allow_tf32:\n@@ -1231,19 +1240,24 @@ def kernel(X, stride_xm, stride_xk,\n         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n \n \n-def test_dot_without_load():\n-    @triton.jit\n-    def kernel(out):\n-        pid = tl.program_id(axis=0)\n-        a = tl.zeros((32, 32), tl.float32)\n-        b = tl.zeros((32, 32), tl.float32)\n-        c = tl.zeros((32, 32), tl.float32)\n-        c = tl.dot(a, b)\n-        pout = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n-        tl.store(pout, c)\n-\n-    out = torch.ones((32, 32), dtype=torch.float32, device=\"cuda\")\n-    kernel[(1,)](out)\n+# TODO: uncomment once DotOperandEncoding::getElemsPerThread is implemented\n+# @pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n+# def test_dot_without_load(dtype_str):\n+#     @triton.jit\n+#     def _kernel(out):\n+#         a = GENERATE_TEST_HERE\n+#         b = GENERATE_TEST_HERE\n+#         c = tl.dot(a, b)\n+#         out_ptr = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n+#         tl.store(out_ptr, c)\n+\n+#     kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.full((32, 32), 1.0, tl.{dtype_str})\"})\n+#     a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+#     b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+#     out_ref = torch.matmul(a, b)\n+#     out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+#     kernel[(1,)](out)\n+#     assert torch.all(out == out_ref)\n \n # ---------------\n # test arange\n@@ -1449,18 +1463,18 @@ def kernel(x):\n     kernel[(1, )](x)\n \n \n-@pytest.mark.parametrize(\"device\", ['cuda', 'cpu'])\n+@pytest.mark.parametrize(\"device\", ['cuda', 'cpu', 'cpu_pinned'])\n def test_pointer_arguments(device):\n     @triton.jit\n     def kernel(x):\n         pass\n-    x = torch.empty(1024, device=device)\n-    result = True\n-    try:\n-        kernel[(1,)](x)\n-    except ValueError:\n-        result = True if device == 'cpu' else False\n-    assert result\n+    pin_memory = 'pinned' in device\n+    x = torch.empty(1024, device=device.split('_')[0], pin_memory=pin_memory)\n+    if device == \"cpu\":\n+        with pytest.raises(ValueError):\n+            kernel[(1,)](x)\n+    else:\n+        kernel[(1, )](x)\n \n \n @pytest.mark.parametrize(\"value, value_type\", [\n@@ -1640,23 +1654,10 @@ def _kernel(dst):\n # -------------\n \n \n-def system_libdevice_path() -> str:\n-    _SYSTEM_LIBDEVICE_SEARCH_PATHS = [\n-        '/usr/lib/cuda/nvvm/libdevice/libdevice.10.bc',\n-        '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc',\n-    ]\n-    SYSTEM_LIBDEVICE_PATH: Optional[str] = None\n-    for _p in _SYSTEM_LIBDEVICE_SEARCH_PATHS:\n-        if os.path.exists(_p):\n-            SYSTEM_LIBDEVICE_PATH = _p\n-    assert SYSTEM_LIBDEVICE_PATH is not None, \\\n-        \"Could not find libdevice.10.bc path\"\n-    return SYSTEM_LIBDEVICE_PATH\n-\n-\n @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n                          [('int32', 'libdevice.ffs', ''),\n-                          ('float32', 'libdevice.pow', system_libdevice_path()),\n+                          ('float32', 'libdevice.log2', ''),\n+                          ('float32', 'libdevice.pow', tl.libdevice.LIBDEVICE_PATH),\n                           ('float64', 'libdevice.norm4d', '')])\n def test_libdevice_tensor(dtype_str, expr, lib_path):\n \n@@ -1671,7 +1672,10 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n \n-    if expr == 'libdevice.ffs':\n+    if expr == 'libdevice.log2':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.broadcast_to(tl.libdevice.log2(5.0), x.shape)'})\n+        y_ref = np.log2(5.0)\n+    elif expr == 'libdevice.ffs':\n         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.ffs(x)'})\n         y_ref = np.zeros(shape, dtype=x.dtype)\n         for i in range(shape[0]):\n@@ -1724,6 +1728,140 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n \n+# -----------------------\n+# test control flow\n+# -----------------------\n+\n+\n+def test_if_else():\n+\n+    @triton.jit\n+    def kernel(Cond, TrueVal, FalseVal, Out):\n+        if tl.load(Cond):\n+            val = tl.load(TrueVal)\n+        else:\n+            val = tl.load(FalseVal)\n+        tl.store(Out, val)\n+\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    true_val = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n+    false_val = to_triton(np.full((1,), 2, dtype=np.int32), device='cuda')\n+    cond = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    # True\n+    cond[0] = True\n+    kernel[(1,)](cond, true_val, false_val, out)\n+    assert to_numpy(out)[0] == true_val[0]\n+    # False\n+    cond[0] = False\n+    kernel[(1,)](cond, true_val, false_val, out)\n+    assert to_numpy(out)[0] == false_val[0]\n+\n+\n+def test_if_return():\n+\n+    @triton.jit\n+    def kernel(ExitEarly, Out):\n+        if tl.load(ExitEarly):\n+            tl.store(Out, 0)\n+            return\n+        tl.store(Out, 1)\n+\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    exit_early = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    # exit early path taken\n+    exit_early[0] = 1\n+    kernel[(1,)](exit_early, out)\n+    assert to_numpy(out)[0] == 0\n+    # exit early path not taken\n+    exit_early[0] = 0\n+    kernel[(1,)](exit_early, out)\n+    assert to_numpy(out)[0] == 1\n+\n+\n+@pytest.mark.parametrize(\"_cond1\", [True, False])\n+@pytest.mark.parametrize(\"_cond2\", [True, False])\n+@pytest.mark.parametrize(\"_cond3\", [True, False])\n+def test_nested_if_else_return(_cond1, _cond2, _cond3):\n+\n+    @triton.jit\n+    def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n+        val = 0\n+        if tl.load(Cond1):\n+            if tl.load(Cond2):\n+                val = tl.load(Val1)\n+            else:\n+                return\n+        else:\n+            if tl.load(Cond3):\n+                val = tl.load(Val2)\n+            else:\n+                val = tl.load(Val3)\n+        tl.store(Out, val)\n+\n+    out = to_triton(np.full((1,), -1, dtype=np.int32), device='cuda')\n+    cond1 = to_triton(np.full((1,), _cond1, dtype=np.int32), device='cuda')\n+    cond2 = to_triton(np.full((1,), _cond2, dtype=np.int32), device='cuda')\n+    cond3 = to_triton(np.full((1,), _cond3, dtype=np.int32), device='cuda')\n+    val1 = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n+    val2 = to_triton(np.full((1,), 2, dtype=np.int32), device='cuda')\n+    val3 = to_triton(np.full((1,), 3, dtype=np.int32), device='cuda')\n+    kernel[(1,)](cond1, cond2, cond3, val1, val2, val3, out)\n+    targets = {\n+        (True, True, True): val1[0],\n+        (True, True, False): val1[0],\n+        (True, False, True): out[0],\n+        (True, False, False): out[0],\n+        (False, True, True): val2[0],\n+        (False, True, False): val3[0],\n+        (False, False, True): val2[0],\n+        (False, False, False): val3[0],\n+    }\n+    assert out[0] == targets[(_cond1, _cond2, _cond3)]\n+\n+\n+def test_while():\n+\n+    @triton.jit\n+    def kernel(InitI, Bound, CutOff, OutI, OutJ):\n+        init_i = tl.load(InitI)\n+        curr_i = init_i\n+        j = 0\n+        while curr_i == init_i and j < tl.load(Bound):\n+            curr_i = curr_i + (j == tl.load(CutOff))\n+            j += 1\n+        tl.store(OutI, curr_i)\n+        tl.store(OutJ, j)\n+\n+    out_i = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    out_j = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    init_i = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n+    bound = to_triton(np.full((1,), 10, dtype=np.int32), device='cuda')\n+    cut_off = to_triton(np.full((1,), 5, dtype=np.int32), device='cuda')\n+    kernel[(1,)](init_i, bound, cut_off, out_i, out_j)\n+    assert out_i[0] == init_i[0] + 1\n+    assert out_j[0] == cut_off[0] + 1\n+\n+# def test_for_if():\n+\n+#     @triton.jit\n+#     def kernel(bound, cutoff, M, N):\n+#         m = 0\n+#         n = 0\n+#         for i in range(bound):\n+#             if i > cutoff:\n+#                 m = m + 1\n+#             else:\n+#                 n = n + 1\n+#         tl.store(M, m)\n+#         tl.store(N, n)\n+\n+#     m = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+#     n = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+#     kernel[(1,)](10, 7, m, n)\n+#     print(m[0])\n+#     print(n[0])\n+\n+\n # -----------------------\n # test layout conversions\n # -----------------------\n@@ -1780,7 +1918,7 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n #dst = {dst_layout}\n \"\"\" + \"\"\"\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func public @kernel_0d1d(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n+  func.func public @kernel_0d1d(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n     %cst = arith.constant dense<128> : tensor<128x1xi32, #src>\n     %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #src}>>\n     %1 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #src}>>\n@@ -1815,3 +1953,16 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n     kernel[(1, 1, 1)](x.data_ptr(), z.data_ptr())\n \n     assert torch.equal(z, x)\n+\n+\n+def test_load_scalar_with_mask():\n+    @triton.jit\n+    def kernel(Input, Index, Out, N: int):\n+        index = tl.load(Index)\n+        scalar = tl.load(Input + index, mask=index < N, other=0)\n+        tl.store(Out, scalar, mask=index < N)\n+    Index = torch.tensor([0], dtype=torch.int32, device='cuda')\n+    Input = torch.tensor([0], dtype=torch.int32, device='cuda')\n+    Out = torch.empty_like(Index, device='cuda')\n+    kernel[(1,)](Input, Index, Out, Index.numel())\n+    assert Out.data[0] == 0"}, {"filename": "python/test/unit/operators/test_flash_attention.py", "status": "added", "additions": 43, "deletions": 0, "changes": 43, "file_content_changes": "@@ -0,0 +1,43 @@\n+import pytest\n+import torch\n+\n+import triton\n+\n+\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n+def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] < 8:\n+        pytest.skip(\"Flash attention only supported for compute capability < 80\")\n+    torch.manual_seed(20)\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n+    sm_scale = 0.2\n+    dout = torch.randn_like(q)\n+    # reference implementation\n+    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n+    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n+    for z in range(Z):\n+        for h in range(H):\n+            p[:, :, M == 0] = float(\"-inf\")\n+    p = torch.softmax(p.float(), dim=-1).half()\n+    # p = torch.exp(p)\n+    ref_out = torch.matmul(p, v)\n+    ref_out.backward(dout)\n+    ref_dv, v.grad = v.grad.clone(), None\n+    ref_dk, k.grad = k.grad.clone(), None\n+    ref_dq, q.grad = q.grad.clone(), None\n+    # # triton implementation\n+    tri_out = triton.ops.attention(q, k, v, sm_scale)\n+    # print(ref_out)\n+    # print(tri_out)\n+    tri_out.backward(dout)\n+    tri_dv, v.grad = v.grad.clone(), None\n+    tri_dk, k.grad = k.grad.clone(), None\n+    tri_dq, q.grad = q.grad.clone(), None\n+    # compare\n+    triton.testing.assert_almost_equal(ref_out, tri_out)\n+    triton.testing.assert_almost_equal(ref_dv, tri_dv)\n+    triton.testing.assert_almost_equal(ref_dk, tri_dk)\n+    triton.testing.assert_almost_equal(ref_dq, tri_dq)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 260, "deletions": 157, "changes": 417, "file_content_changes": "@@ -97,10 +97,11 @@ def __enter__(self):\n         self.prev_defs = self.generator.local_defs.copy()\n         self.generator.local_defs = {}\n         self.insert_block = self.generator.builder.get_insertion_block()\n+        self.insert_point = self.generator.builder.get_insertion_point()\n         return self.liveins, self.insert_block\n \n     def __exit__(self, *args, **kwargs):\n-        self.generator.builder.set_insertion_point_to_end(self.insert_block)\n+        self.generator.builder.restore_insertion_point(self.insert_point)\n         self.generator.lscope = self.liveins\n         self.generator.local_defs = self.prev_defs\n \n@@ -127,6 +128,7 @@ def __init__(self, context, prototype, gscope, attributes, constants, function_n\n             'isinstance': isinstance,\n             'getattr': getattr,\n         }\n+        self.scf_stack = []\n         # SSA-construction\n         # name => triton.language.tensor\n         self.local_defs: Dict[str, triton.language.tensor] = {}\n@@ -177,6 +179,18 @@ def visit_compound_statement(self, stmts):\n                 break\n         return stmts and isinstance(stmt, ast.Return)\n \n+    def contains_return_op(self, node):\n+        if isinstance(node, ast.Return):\n+            return True\n+        elif isinstance(node, ast.If):\n+            pred = lambda s: self.contains_return_op(s)\n+            ret = any(pred(s) for s in node.body)\n+            if node.orelse:\n+                ret = ret or any(pred(s) for s in node.orelse)\n+            return ret\n+        else:\n+            return False\n+\n     def visit_Module(self, node):\n         ast.NodeVisitor.generic_visit(self, node)\n \n@@ -189,18 +203,25 @@ def visit_List(self, node):\n     # By design, only non-kernel functions can return\n     def visit_Return(self, node):\n         ret_value = self.visit(node.value)\n+        # ret_block = self.builder.create_block()\n+        # post_ret_block = self.builder.create_block()\n+        # self.builder.create_branch(ret_block)\n+        # self.builder.set_insertion_point_to_end(ret_block)\n         if ret_value is None:\n             self.builder.ret([])\n-            return None\n-        if isinstance(ret_value, tuple):\n+            ret_ty = None\n+        elif isinstance(ret_value, tuple):\n             ret_values = [triton.language.core._to_tensor(v, self.builder) for v in ret_value]\n             ret_types = [v.type for v in ret_values]\n             self.builder.ret([v.handle for v in ret_values])\n-            return tuple(ret_types)\n+            ret_ty = tuple(ret_types)\n         else:\n             ret = triton.language.core._to_tensor(ret_value, self.builder)\n             self.builder.ret([ret.handle])\n-            return ret.type\n+            ret_ty = ret.type\n+        # self.builder.create_branch(post_ret_block)\n+        # self.builder.set_insertion_point_to_end(post_ret_block)\n+        return ret_ty\n \n     def visit_FunctionDef(self, node):\n         arg_names, kwarg_names = self.visit(node.args)\n@@ -350,81 +371,126 @@ def visit_BinOp(self, node):\n         else:\n             return getattr(lhs, fn)(rhs)\n \n+    def visit_then_else_blocks(self, node, liveins, then_block, else_block):\n+        # then block\n+        self.builder.set_insertion_point_to_start(then_block)\n+        self.visit_compound_statement(node.body)\n+        then_block = self.builder.get_insertion_block()\n+        then_defs = self.local_defs.copy()\n+        # else block\n+        else_defs = {}\n+        if node.orelse:\n+            self.builder.set_insertion_point_to_start(else_block)\n+            self.lscope = liveins.copy()\n+            self.local_defs = {}\n+            self.visit_compound_statement(node.orelse)\n+            else_defs = self.local_defs.copy()\n+            else_block = self.builder.get_insertion_block()\n+\n+        # update block arguments\n+        names = []\n+        ret_types = []\n+        ir_ret_types = []\n+        # variables in livein whose value is updated in `if`\n+        for name in liveins:\n+            # check type\n+            for defs, block_name in [(then_defs, 'then'), (else_defs, 'else')]:\n+                if name in defs:\n+                    assert defs[name].type == liveins[name].type,\\\n+                        f'initial value for `{name}` is of type {liveins[name].type}, '\\\n+                        f'but the {block_name} block redefines it as {defs[name].type}'\n+            if name in then_defs or name in else_defs:\n+                names.append(name)\n+                ret_types.append(then_defs[name].type if name in then_defs else else_defs[name].type)\n+                ir_ret_types.append(then_defs[name].handle.get_type() if name in then_defs else else_defs[name].handle.get_type())\n+            # variable defined in then but not in else\n+            if name in then_defs and name not in else_defs:\n+                else_defs[name] = liveins[name]\n+            # variable defined in else but not in then\n+            if name in else_defs and name not in then_defs:\n+                then_defs[name] = liveins[name]\n+        # variables that are both in then and else but not in liveins\n+        # TODO: could probably be cleaned up\n+        for name in then_defs.keys() & else_defs.keys():\n+            if name in names:\n+                continue\n+            then_ty = then_defs[name].type\n+            else_ty = else_defs[name].type\n+            assert then_ty == else_ty,\\\n+                f'mismatched type for {name} between then block ({then_ty}) '\\\n+                f'and else block ({else_ty})'\n+            names.append(name)\n+            ret_types.append(then_ty)\n+            ir_ret_types.append(then_defs[name].handle.get_type())\n+\n+        return then_defs, else_defs, then_block, else_block, names, ret_types, ir_ret_types\n+\n+    def visit_if_top_level(self, cond, node):\n+        with enter_sub_region(self) as sr:\n+            liveins, ip_block = sr\n+            then_block = self.builder.create_block()\n+            else_block = self.builder.create_block()\n+            # create basic-block after conditional\n+            endif_block = self.builder.create_block()\n+            # create branch\n+            self.builder.set_insertion_point_to_end(ip_block)\n+            self.builder.create_cond_branch(cond.handle, then_block, else_block)\n+            # visit then and else blocks\n+            then_defs, else_defs, then_block, else_block, names, ret_types, ir_ret_types = \\\n+                self.visit_then_else_blocks(node, liveins, then_block, else_block)\n+            # then terminator\n+            self.builder.set_insertion_point_to_end(then_block)\n+            if not then_block.has_terminator():\n+                self.builder.create_branch(endif_block, [then_defs[n].handle for n in names])\n+            # else terminator\n+            self.builder.set_insertion_point_to_end(else_block)\n+            if not else_block.has_terminator():\n+                self.builder.create_branch(endif_block, [else_defs[n].handle for n in names])\n+            for ty in ir_ret_types:\n+                endif_block.add_argument(ty)\n+        # change block\n+        self.builder.set_insertion_point_to_start(endif_block)\n+        # update value\n+        for i, name in enumerate(names):\n+            new_tensor = triton.language.core.tensor(endif_block.arg(i), ret_types[i])\n+            self.set_value(name, new_tensor)\n+\n+    # TODO: refactor\n+    def visit_if_scf(self, cond, node):\n+        with enter_sub_region(self) as sr:\n+            liveins, _ = sr\n+            ip = self.builder.get_insertion_point()\n+            then_block = self.builder.create_block()\n+            else_block = self.builder.create_block() if node.orelse else None\n+            then_defs, else_defs, then_block, else_block, names, ret_types, _ = \\\n+                self.visit_then_else_blocks(node, liveins, then_block, else_block)\n+            # create if op\n+            self.builder.restore_insertion_point(ip)\n+            if_op = self.builder.create_if_op([ty.to_ir(self.builder) for ty in ret_types], cond.handle, True)\n+            then_block.merge_block_before(if_op.get_then_block())\n+            self.builder.set_insertion_point_to_end(if_op.get_then_block())\n+            if len(names) > 0:\n+                self.builder.create_yield_op([then_defs[n].handle for n in names])\n+            if not node.orelse:\n+                else_block = if_op.get_else_block()\n+            else:\n+                else_block.merge_block_before(if_op.get_else_block())\n+            self.builder.set_insertion_point_to_end(if_op.get_else_block())\n+            if len(names) > 0:\n+                self.builder.create_yield_op([else_defs[n].handle for n in names])\n+        # update values\n+        for i, name in enumerate(names):\n+            new_tensor = triton.language.core.tensor(if_op.get_result(i), ret_types[i])\n+            self.set_value(name, new_tensor)\n+\n     def visit_If(self, node):\n         cond = self.visit(node.test)\n         if isinstance(cond, triton.language.tensor):\n             cond = cond.to(triton.language.int1, _builder=self.builder)\n-            with enter_sub_region(self) as sr:\n-                liveins, ip_block = sr\n-                liveins_copy = liveins.copy()\n-                then_block = self.builder.create_block()\n-                self.builder.set_insertion_point_to_start(then_block)\n-                self.visit_compound_statement(node.body)\n-                then_defs = self.local_defs.copy()\n-\n-                # when need an else block when:\n-                # 1. we have an orelse node\n-                #   or\n-                # 2. the then block defines new variable\n-                else_defs = {}\n-                if then_defs or node.orelse:\n-                    if node.orelse:\n-                        self.lscope = liveins\n-                        self.local_defs = {}\n-                        else_block = self.builder.create_block()\n-                        self.builder.set_insertion_point_to_end(else_block)\n-                        self.visit_compound_statement(node.orelse)\n-                        else_defs = self.local_defs.copy()\n-                    else:\n-                        # collect else_defs\n-                        for name in then_defs:\n-                            if name in liveins:\n-                                assert self.is_triton_tensor(then_defs[name])\n-                                assert self.is_triton_tensor(liveins[name])\n-                                else_defs[name] = liveins[name]\n-                # collect yields\n-                names = []\n-                ret_types = []\n-                for then_name in then_defs:\n-                    for else_name in else_defs:\n-                        if then_name == else_name:\n-                            if then_defs[then_name].type == else_defs[else_name].type:\n-                                names.append(then_name)\n-                                ret_types.append(then_defs[then_name].type)\n-\n-                # defined in else block but not in then block\n-                # to find in parent scope and yield them\n-                for else_name in else_defs:\n-                    if else_name in liveins and else_name not in then_defs:\n-                        if else_defs[else_name].type == liveins[else_name].type:\n-                            names.append(else_name)\n-                            ret_types.append(else_defs[else_name].type)\n-                            then_defs[else_name] = liveins_copy[else_name]\n-                self.builder.set_insertion_point_to_end(ip_block)\n-\n-                if then_defs or node.orelse:  # with else block\n-                    if_op = self.builder.create_if_op([ty.to_ir(self.builder) for ty in ret_types], cond.handle, True)\n-                    then_block.merge_block_before(if_op.get_then_block())\n-                    self.builder.set_insertion_point_to_end(if_op.get_then_block())\n-                    if len(names) > 0:\n-                        self.builder.create_yield_op([then_defs[n].handle for n in names])\n-                    if not node.orelse:\n-                        else_block = if_op.get_else_block()\n-                    else:\n-                        else_block.merge_block_before(if_op.get_else_block())\n-                    self.builder.set_insertion_point_to_end(if_op.get_else_block())\n-                    if len(names) > 0:\n-                        self.builder.create_yield_op([else_defs[n].handle for n in names])\n-                else:  # no else block\n-                    if_op = self.builder.create_if_op([ty.to_ir(self.builder) for ty in ret_types], cond.handle, False)\n-                    then_block.merge_block_before(if_op.get_then_block())\n-\n-            # update values yielded by IfOp\n-            for i, name in enumerate(names):\n-                new_tensor = triton.language.core.tensor(if_op.get_result(i), ret_types[i])\n-                self.lscope[name] = new_tensor\n-                self.local_defs[name] = new_tensor\n-\n+            if self.scf_stack or not self.contains_return_op(node):\n+                self.visit_if_scf(cond, node)\n+            else:\n+                self.visit_if_top_level(cond, node)\n         else:\n             if isinstance(cond, triton.language.constexpr):\n                 cond = cond.value\n@@ -474,12 +540,10 @@ def visit_Compare(self, node):\n \n     def visit_UnaryOp(self, node):\n         op = self.visit(node.operand)\n-        if type(node.op) == ast.Not:\n-            assert isinstance(op, triton.language.constexpr), \"`not` only supported for constexpr at the moment\"\n-            return triton.language.constexpr(not op)\n         fn = {\n             ast.USub: '__neg__',\n             ast.UAdd: '__pos__',\n+            ast.Not: '__not__',\n             ast.Invert: '__invert__',\n         }[type(node.op)]\n         if self.is_triton_tensor(op):\n@@ -490,54 +554,65 @@ def visit_While(self, node):\n         with enter_sub_region(self) as sr:\n             liveins, insert_block = sr\n \n-            # condition (the before region)\n-            cond_block = self.builder.create_block()\n-            self.builder.set_insertion_point_to_start(cond_block)\n-            cond = self.visit(node.test)\n-\n             # loop body (the after region)\n-            loop_block = self.builder.create_block()\n-            self.builder.set_insertion_point_to_start(loop_block)\n+            # loop_block = self.builder.create_block()\n+            dummy = self.builder.create_block()\n+            self.builder.set_insertion_point_to_start(dummy)\n+            self.scf_stack.append(node)\n             self.visit_compound_statement(node.body)\n+            self.scf_stack.pop()\n             loop_defs = self.local_defs\n \n             # collect loop-carried values\n             names = []\n             ret_types = []\n             init_args = []\n-            yields = []\n             for name in loop_defs:\n                 if name in liveins:\n                     # We should not def new constexpr\n                     assert self.is_triton_tensor(loop_defs[name])\n                     assert self.is_triton_tensor(liveins[name])\n-                    if loop_defs[name].type == liveins[name].type:\n-                        # these are loop-carried values\n-                        names.append(name)\n-                        ret_types.append(loop_defs[name].type)\n-                        init_args.append(liveins[name])\n-                        yields.append(loop_defs[name])\n+                    assert loop_defs[name].type == liveins[name].type\n+                    # these are loop-carried values\n+                    names.append(name)\n+                    ret_types.append(loop_defs[name].type)\n+                    init_args.append(liveins[name])\n \n             self.builder.set_insertion_point_to_end(insert_block)\n             while_op = self.builder.create_while_op([ty.to_ir(self.builder) for ty in ret_types],\n                                                     [arg.handle for arg in init_args])\n             # merge the condition region\n             before_block = self.builder.create_block_with_parent(while_op.get_before(),\n                                                                  [ty.to_ir(self.builder) for ty in ret_types])\n-            cond_block.merge_block_before(before_block)\n+            self.builder.set_insertion_point_to_start(before_block)\n+            for i, name in enumerate(names):\n+                self.lscope[name] = triton.language.core.tensor(before_block.arg(i), ret_types[i])\n+                self.local_defs[name] = self.lscope[name]\n+            cond = self.visit(node.test)\n             self.builder.set_insertion_point_to_end(before_block)\n             # create ConditionOp: e.g., scf.condition(%cond) %arg0, %arg1, ...\n             self.builder.create_condition_op(cond.handle, [before_block.arg(i) for i in range(len(init_args))])\n             # merge the loop body\n             after_block = self.builder.create_block_with_parent(while_op.get_after(),\n                                                                 [ty.to_ir(self.builder) for ty in ret_types])\n-            loop_block.merge_block_before(after_block)\n-            self.builder.set_insertion_point_to_end(after_block)\n+\n+            # generate loop body\n+            self.builder.set_insertion_point_to_start(after_block)\n+            for i, name in enumerate(names):\n+                self.lscope[name] = triton.language.core.tensor(after_block.arg(i), ret_types[i])\n+                self.local_defs[name] = self.lscope[name]\n+            self.scf_stack.append(node)\n+            self.visit_compound_statement(node.body)\n+            self.scf_stack.pop()\n+            loop_defs = self.local_defs\n+            yields = []\n+            for name in loop_defs:\n+                if name in liveins:\n+                    yields.append(loop_defs[name])\n             self.builder.create_yield_op([y.handle for y in yields])\n \n         # update global uses in while_op\n         for i, name in enumerate(names):\n-            before_block.replace_use_in_block_with(init_args[i].handle, before_block.arg(i))\n             after_block.replace_use_in_block_with(init_args[i].handle, after_block.arg(i))\n \n         # WhileOp defines new values, update the symbol table (lscope, local_defs)\n@@ -562,29 +637,29 @@ def visit_ExtSlice(self, node):\n         return [self.visit(dim) for dim in node.dims]\n \n     def visit_For(self, node):\n-        iterator = self.visit(node.iter.func)\n-        if iterator != self.builtins['range']:\n-            raise RuntimeError('Only `range` iterator currently supported')\n+        IteratorClass = self.visit(node.iter.func)\n+        iter_args = [self.visit(arg) for arg in node.iter.args]\n+        if IteratorClass == triton.language.static_range:\n+            iterator = IteratorClass(*iter_args)\n+            static_range = range(iterator.start.value,\n+                                 iterator.end.value,\n+                                 iterator.step.value)\n+            for i in static_range:\n+                self.lscope[node.target.id] = triton.language.constexpr(i)\n+                self.visit_compound_statement(node.body)\n+                for stmt in node.orelse:\n+                    ast.NodeVisitor.generic_visit(self, stmt)\n+            return\n+\n+        if IteratorClass != self.builtins['range']:\n+            raise RuntimeError('Only `range` and `static_range` iterators are currently supported')\n+\n         # visit iterator arguments\n         # note: only `range` iterator is supported now\n-        iter_args = [self.visit(arg) for arg in node.iter.args]\n         # collect lower bound (lb), upper bound (ub), and step\n         lb = iter_args[0] if len(iter_args) > 1 else self.visit(ast.Num(0))\n         ub = iter_args[1] if len(iter_args) > 1 else self.visit(node.iter.args[0])\n         step = iter_args[2] if len(iter_args) > 2 else self.visit(ast.Num(1))\n-        # static for loops: all iterator arguments are constexpr\n-        if isinstance(lb, triton.language.constexpr) and \\\n-           isinstance(ub, triton.language.constexpr) and \\\n-           isinstance(step, triton.language.constexpr):\n-            sta_range = iterator(lb.value, ub.value, step.value)\n-            static_unrolling = os.environ.get('TRITON_STATIC_LOOP_UNROLLING', False)\n-            if static_unrolling and len(sta_range) <= 10:\n-                for i in sta_range:\n-                    self.lscope[node.target.id] = triton.language.constexpr(i)\n-                    self.visit_compound_statement(node.body)\n-                    for stmt in node.orelse:\n-                        ast.NodeVisitor.generic_visit(self, stmt)\n-                return\n         # handle negative constant step (not supported by scf.for in MLIR)\n         negative_step = False\n         if isinstance(step, triton.language.constexpr) and step.value < 0:\n@@ -605,13 +680,16 @@ def visit_For(self, node):\n \n         with enter_sub_region(self) as sr:\n             liveins, insert_block = sr\n+            ip = self.builder.get_insertion_point()\n \n             # create loop body block\n             block = self.builder.create_block()\n             self.builder.set_insertion_point_to_start(block)\n-\n-            # visit loop body\n+            # dry visit loop body\n+            self.scf_stack.append(node)\n             self.visit_compound_statement(node.body)\n+            self.scf_stack.pop()\n+            block.erase()\n \n             # If a variable (name) is defined in both its parent & itself, then it's\n             # a loop-carried variable. (They must be of the same type)\n@@ -622,17 +700,35 @@ def visit_For(self, node):\n                 if name in liveins:\n                     assert self.is_triton_tensor(self.local_defs[name]), f'{name} is not tensor'\n                     assert self.is_triton_tensor(liveins[name])\n-                    if self.local_defs[name].type != liveins[name].type:\n-                        local_value = self.local_defs[name]\n-                        self.local_defs[name] = local_value.to(liveins[name].dtype, _builder=self.builder)\n+                    assert self.local_defs[name].type == liveins[name].type,\\\n+                        f'Loop-carried variable {name} has initial type {liveins[name].type} '\\\n+                        f'but is re-assigned to {self.local_defs[name].type} in loop! '\\\n+                        f'Please make sure that the type stays consistent.'\n+\n                     names.append(name)\n                     init_args.append(triton.language.core._to_tensor(liveins[name], self.builder))\n                     yields.append(triton.language.core._to_tensor(self.local_defs[name], self.builder))\n \n             # create ForOp\n-            self.builder.set_insertion_point_to_end(insert_block)\n+            self.builder.restore_insertion_point(ip)\n             for_op = self.builder.create_for_op(lb, ub, step, [arg.handle for arg in init_args])\n-            block.merge_block_before(for_op.get_body(0))\n+\n+            self.scf_stack.append(node)\n+            self.builder.set_insertion_point_to_start(for_op.get_body(0))\n+            for i, name in enumerate(names):\n+                self.set_value(name, triton.language.core.tensor(for_op.get_body(0).arg(i + 1), yields[i].type))\n+            self.visit_compound_statement(node.body)\n+            self.scf_stack.pop()\n+            yields = []\n+            for name in self.local_defs:\n+                if name in liveins:\n+                    yields.append(triton.language.core._to_tensor(self.local_defs[name], self.builder))\n+\n+            # create YieldOp\n+            if len(yields) > 0:\n+                self.builder.create_yield_op([y.handle for y in yields])\n+            for_op_region = for_op.get_body(0).get_parent()\n+            assert for_op_region.size() == 1, \"We use SCF, so the loop body should only have one block\"\n \n             # update induction variable with actual value, and replace all uses\n             self.builder.set_insertion_point_to_start(for_op.get_body(0))\n@@ -643,17 +739,6 @@ def visit_For(self, node):\n             self.lscope[node.target.id].handle.replace_all_uses_with(iv)\n             self.set_value(node.target.id, triton.language.core.tensor(iv, triton.language.core.int32))\n \n-            # create YieldOp\n-            self.builder.set_insertion_point_to_end(for_op.get_body(0))\n-            if len(yields) > 0:\n-                self.builder.create_yield_op([y.handle for y in yields])\n-            for_op_region = for_op.get_body(0).get_parent()\n-            assert for_op_region.size() == 1, \"We use SCF, so the loop body should only have one block\"\n-            # replace global uses with block arguments\n-            for i, name in enumerate(names):\n-                # arg0 is the induction variable\n-                for_op.get_body(0).replace_use_in_block_with(init_args[i].handle, for_op.get_body(0).arg(i + 1))\n-\n         # update lscope & local_defs (ForOp defines new values)\n         for i, name in enumerate(names):\n             self.set_value(name, triton.language.core.tensor(for_op.get_result(i), yields[i].type))\n@@ -832,6 +917,13 @@ def kernel_suffix(signature, specialization):\n # ------------------------------------------------------------------------------\n \n \n+def parse_mlir_module(path, context):\n+    module = _triton.ir.parse_mlir_module(path, context)\n+    # module takes ownership of the context\n+    module.context = context\n+    return module\n+\n+\n def build_triton_ir(fn, signature, specialization, constants):\n     # canonicalize signature\n     if isinstance(signature, str):\n@@ -874,6 +966,7 @@ def optimize_triton_ir(mod):\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n     pm.add_licm_pass()\n+    pm.add_symbol_dce_pass()\n     pm.run(mod)\n     return mod\n \n@@ -900,12 +993,13 @@ def ttir_to_ttgir(mod, num_warps, num_stages, compute_capability):\n     pm.add_tritongpu_combine_pass(compute_capability)\n     pm.add_licm_pass()\n     pm.add_tritongpu_combine_pass(compute_capability)\n+    pm.add_cse_pass()\n+    pm.add_tritongpu_decompose_conversions_pass()\n     if compute_capability // 10 == 7:\n         # The update_mma_for_volta pass helps to compute some information for MMA encoding specifically for MMAv1\n+        # NOTE this pass should be placed after all the passes those modifies mma layout\n         pm.add_tritongpu_update_mma_for_volta_pass()\n     pm.add_cse_pass()\n-    pm.add_tritongpu_decompose_conversions_pass()\n-    pm.add_cse_pass()\n     pm.add_symbol_dce_pass()\n     pm.add_tritongpu_reorder_instructions_pass()\n     pm.run(mod)\n@@ -980,15 +1074,14 @@ def ptx_get_version(cuda_version) -> int:\n \n \n def path_to_ptxas():\n-    prefixes = [\n+    base_dir = os.path.dirname(__file__)\n+    paths = [\n         os.environ.get(\"TRITON_PTXAS_PATH\", \"\"),\n-        \"\",\n-        \"/usr\",\n-        os.environ.get('CUDA_PATH', default_cuda_dir())\n+        os.path.join(base_dir, \"third_party\", \"cuda\", \"bin\", \"ptxas\")\n     ]\n-    for prefix in prefixes:\n-        ptxas = os.path.join(prefix, \"bin\", \"ptxas\")\n-        if os.path.exists(ptxas):\n+\n+    for ptxas in paths:\n+        if os.path.exists(ptxas) and os.path.isfile(ptxas):\n             result = subprocess.check_output([ptxas, \"--version\"], stderr=subprocess.STDOUT)\n             if result is not None:\n                 version = re.search(r\".*release (\\d+\\.\\d+).*\", result.decode(\"utf-8\"), flags=re.MULTILINE)\n@@ -1129,15 +1222,16 @@ def format_of(ty):\n       return ptr_info;\n     }}\n     ptr_info.dev_ptr = PyLong_AsUnsignedLongLong(ret);\n-    unsigned attr;\n-    CUresult status =\n-        cuPointerGetAttribute(&attr, CU_POINTER_ATTRIBUTE_MEMORY_TYPE, ptr_info.dev_ptr);\n-    if (!(attr == CU_MEMORYTYPE_DEVICE || attr == CU_MEMORYTYPE_UNIFIED) ||\n-        !(status == CUDA_SUCCESS)) {{\n+    if(!ptr_info.dev_ptr)\n+      return ptr_info;\n+    uint64_t dev_ptr;\n+    int status = cuPointerGetAttribute(&dev_ptr, CU_POINTER_ATTRIBUTE_DEVICE_POINTER, ptr_info.dev_ptr);\n+    if (status == CUDA_ERROR_INVALID_VALUE) {{\n         PyErr_Format(PyExc_ValueError,\n                      \"Pointer argument (at %d) cannot be accessed from Triton (cpu tensor?)\", idx);\n         ptr_info.valid = false;\n     }}\n+    ptr_info.dev_ptr = dev_ptr;\n     return ptr_info;\n   }}\n   PyErr_SetString(PyExc_TypeError, \"Pointer argument must be either uint64 or have data_ptr method\");\n@@ -1199,13 +1293,13 @@ def format_of(ty):\n \n static struct PyModuleDef ModuleDef = {{\n   PyModuleDef_HEAD_INIT,\n-  \\\"launcher\\\",\n+  \\\"__triton_launcher\\\",\n   NULL, //documentation\n   -1, //size\n   ModuleMethods\n }};\n \n-PyMODINIT_FUNC PyInit_launcher(void) {{\n+PyMODINIT_FUNC PyInit___triton_launcher(void) {{\n   PyObject *m = PyModule_Create(&ModuleDef);\n   if(m == NULL) {{\n     return NULL;\n@@ -1286,6 +1380,11 @@ def _build(name, src, srcdir):\n     cuda_lib_dirs = libcuda_dirs()\n     cuda_path = os.environ.get('CUDA_PATH', default_cuda_dir())\n     cu_include_dir = os.path.join(cuda_path, \"include\")\n+    triton_include_dir = os.path.join(os.path.dirname(__file__), \"include\")\n+    cuda_header = os.path.join(cu_include_dir, \"cuda.h\")\n+    triton_cuda_header = os.path.join(triton_include_dir, \"cuda.h\")\n+    if not os.path.exists(cuda_header) and os.path.exists(triton_cuda_header):\n+        cu_include_dir = triton_include_dir\n     suffix = sysconfig.get_config_var('EXT_SUFFIX')\n     so = os.path.join(srcdir, '{name}{suffix}'.format(name=name, suffix=suffix))\n     # try to avoid setuptools if possible\n@@ -1295,6 +1394,8 @@ def _build(name, src, srcdir):\n         clang = shutil.which(\"clang\")\n         gcc = shutil.which(\"gcc\")\n         cc = gcc if gcc is not None else clang\n+        if cc is None:\n+            raise RuntimeError(\"Failed to find C compiler. Please specify via CC environment variable.\")\n     py_include_dir = get_paths()[\"include\"]\n \n     cc_cmd = [cc, src, \"-O3\", f\"-I{cu_include_dir}\", f\"-I{py_include_dir}\", f\"-I{srcdir}\", \"-shared\", \"-fPIC\", \"-lcuda\", \"-o\", so]\n@@ -1413,14 +1514,14 @@ def make_hash(fn, **kwargs):\n     return hashlib.md5((Path(fn).read_text() + triton.runtime.jit.version_key()).encode(\"utf-8\")).hexdigest()\n \n \n-# - ^\\s*func\\s+ : match the start of the string, any leading whitespace, the keyword func,\n+# - ^\\s*func\\.func\\s+ : match the start of the string, any leading whitespace, the keyword func,\n #    and any following whitespace\n # - (public\\s+)? : optionally match the keyword public and any following whitespace\n # - (@\\w+) : match an @ symbol followed by one or more word characters\n #   (letters, digits, or underscores), and capture it as group 1 (the function name)\n # - (\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\)) : match a pair of parentheses enclosing\n #   zero or more arguments separated by commas, and capture it as group 2 (the argument list)\n-mlir_prototype_pattern = r'^\\s*func\\s+(?:public\\s+)?(@\\w+)(\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\))\\s*\\{\\s*$'\n+mlir_prototype_pattern = r'^\\s*func\\.func\\s+(?:public\\s+)?(@\\w+)(\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\))\\s*\\{\\s*$'\n ptx_prototype_pattern = r\"\\.(?:visible|extern)\\s+\\.(?:entry|func)\\s+(\\w+)\\s*\\(([^)]*)\\)\"\n prototype_pattern = {\n     \"ttir\": mlir_prototype_pattern,\n@@ -1456,11 +1557,11 @@ def compile(fn, **kwargs):\n     # build compilation stages\n     stages = {\n         \"ast\": (lambda path: fn, None),\n-        \"ttir\": (lambda path: _triton.ir.parse_mlir_module(path, context),\n+        \"ttir\": (lambda path: parse_mlir_module(path, context),\n                  lambda src: ast_to_ttir(src, signature, configs[0], constants)),\n-        \"ttgir\": (lambda path: _triton.ir.parse_mlir_module(path, context),\n+        \"ttgir\": (lambda path: parse_mlir_module(path, context),\n                   lambda src: ttir_to_ttgir(src, num_warps, num_stages, capability)),\n-        \"llir\": (lambda path: Path(path).read_bytes(),\n+        \"llir\": (lambda path: Path(path).read_text(),\n                  lambda src: ttgir_to_llir(src, extern_libs, capability)),\n         \"ptx\": (lambda path: Path(path).read_text(),\n                 lambda src: llir_to_ptx(src, capability)),\n@@ -1487,9 +1588,9 @@ def compile(fn, **kwargs):\n         import re\n         match = re.search(prototype_pattern[ir], src, re.MULTILINE)\n         name, signature = match.group(1), match.group(2)\n-        print(name, signature)\n+        # print(name, signature)\n         types = re.findall(arg_type_pattern[ir], signature)\n-        print(types)\n+        # print(types)\n         param_tys = [convert_type_repr(ty) for ty in types]\n         signature = {k: v for k, v in enumerate(param_tys)}\n         first_stage = list(stages.keys()).index(ir)\n@@ -1553,7 +1654,7 @@ class CompiledKernel:\n     def __init__(self, so_path, metadata, asm):\n         # initialize launcher\n         import importlib.util\n-        spec = importlib.util.spec_from_file_location(\"launcher\", so_path)\n+        spec = importlib.util.spec_from_file_location(\"__triton_launcher\", so_path)\n         mod = importlib.util.module_from_spec(spec)\n         spec.loader.exec_module(mod)\n         self.c_wrapper = getattr(mod, \"launch\")\n@@ -1580,6 +1681,7 @@ def _init_handles(self):\n         if self.shared > max_shared:\n             raise OutOfResources(self.shared, max_shared, \"shared memory\")\n         mod, func, n_regs, n_spills = cuda_utils.load_binary(self.metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n+        # print(self.shared, n_regs, n_spills)\n         self.cu_module = mod\n         self.cu_function = func\n \n@@ -1619,7 +1721,8 @@ def __new__(cls):\n             cls.instance = super(CudaUtils, cls).__new__(cls)\n         return cls.instance\n \n-    def _generate_src(self):\n+    @staticmethod\n+    def _generate_src():\n         return \"\"\"\n         #include <cuda.h>\n "}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -31,6 +31,7 @@\n     dot,\n     dtype,\n     exp,\n+    full,\n     fdiv,\n     float16,\n     float32,\n@@ -64,6 +65,7 @@\n     store,\n     sum,\n     swizzle2d,\n+    static_range,\n     tensor,\n     trans,\n     triton,\n@@ -124,6 +126,7 @@\n     \"float32\",\n     \"float64\",\n     \"float8\",\n+    \"full\",\n     \"function_type\",\n     \"int1\",\n     \"int16\",\n@@ -160,6 +163,7 @@\n     \"sin\",\n     \"softmax\",\n     \"sqrt\",\n+    \"static_range\",\n     \"store\",\n     \"sum\",\n     \"swizzle2d\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 97, "deletions": 21, "changes": 118, "file_content_changes": "@@ -26,12 +26,12 @@ def _to_tensor(x, builder):\n         else:\n             raise RuntimeError(f'Nonrepresentable integer {x}.')\n     elif isinstance(x, float):\n-        return tensor(builder.get_float32(x), float32)\n+        return tensor(builder.get_fp32(x), float32)\n     elif isinstance(x, constexpr):\n         return _to_tensor(x.value, builder)\n     elif isinstance(x, tensor):\n         return x\n-    assert False, f'cannot convert {x} to tensor'\n+    assert False, f\"cannot convert {x} of type {type(x)} to tensor\"\n \n \n class dtype:\n@@ -139,13 +139,16 @@ def is_int(self):\n     def is_bool(self):\n         return self.is_int1()\n \n-    def is_void(self):\n+    @staticmethod\n+    def is_void():\n         raise RuntimeError(\"Not implemented\")\n \n-    def is_block(self):\n+    @staticmethod\n+    def is_block():\n         return False\n \n-    def is_ptr(self):\n+    @staticmethod\n+    def is_ptr():\n         return False\n \n     def __eq__(self, other: dtype):\n@@ -421,6 +424,18 @@ def __pos__(self):\n     def __invert__(self):\n         return constexpr(~self.value)\n \n+    def __pow__(self, other):\n+        return constexpr(self.value ** other.value)\n+\n+    def __rshift__(self, other):\n+        return constexpr(self.value >> other.value)\n+\n+    def __lshift__(self, other):\n+        return constexpr(self.value << other.value)\n+\n+    def __not__(self):\n+        return constexpr(not self.value)\n+\n     def __call__(self, *args, **kwds):\n         return self.value(*args, **kwds)\n \n@@ -534,7 +549,10 @@ def __lshift__(self, other, _builder=None):\n     @builtin\n     def __rshift__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n-        return semantic.lshr(self, other, _builder)\n+        if self.dtype.is_int_signed():\n+            return semantic.ashr(self, other, _builder)\n+        else:\n+            return semantic.lshr(self, other, _builder)\n \n     # comparison operators\n \n@@ -603,6 +621,12 @@ def logical_or(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n         return semantic.logical_or(self, other, _builder)\n \n+    # note: __not__ isn't actually a magic method in python\n+    # but it's ok because our ASTVisitor handles it\n+    @builtin\n+    def __not__(self, _builder=None):\n+        return semantic.not_(self, _builder)\n+\n     @builtin\n     def __getitem__(self, slices, _builder=None):\n         if isinstance(slices, slice):\n@@ -690,24 +714,31 @@ def arange(start, end, _builder=None):\n     return semantic.arange(start, end, _builder)\n \n \n+def _shape_check_impl(shape):\n+    shape = _constexpr_to_value(shape)\n+    for i, d in enumerate(shape):\n+        if not isinstance(d, constexpr):\n+            raise TypeError(f\"Shape element {i} must have type `constexpr`\")\n+        if not isinstance(d.value, int):\n+            raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n+    return [_constexpr_to_value(x) for x in shape]\n+\n+\n @builtin\n-def zeros(shape, dtype, _builder=None):\n+def full(shape, value, dtype, _builder=None):\n     \"\"\"\n-    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n+    Returns a tensor filled with the scalar value for the given :code:`shape` and :code:`dtype`.\n \n     :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n+    :value value: A scalar value to fill the array with\n     :type shape: tuple of ints\n     :param dtype: Data-type of the new array, e.g., :code:`tl.float16`\n     :type dtype: DType\n     \"\"\"\n-    for i, d in enumerate(shape):\n-        if not isinstance(d, constexpr):\n-            raise TypeError(f\"Shape element {i} must have type `constexpr`\")\n-        if not isinstance(d.value, int):\n-            raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n-    shape = [x.value for x in shape]\n+    shape = _shape_check_impl(shape)\n+    value = _constexpr_to_value(value)\n     dtype = _constexpr_to_value(dtype)\n-    return semantic.zeros(shape, dtype, _builder)\n+    return semantic.full(shape, value, dtype, _builder)\n \n \n # -----------------------\n@@ -738,6 +769,7 @@ def broadcast_to(input, shape, _builder=None):\n     :param shape: The desired shape.\n     :type shape: Tuple[int]\n     \"\"\"\n+    shape = _shape_check_impl(shape)\n     return semantic.broadcast_impl_shape(input, shape, _builder)\n \n \n@@ -775,15 +807,14 @@ def view(input, shape, _builder=None):\n     :type shape: Tuple[int]\n \n     \"\"\"\n-    shape = [x.value for x in shape]\n+    shape = _shape_check_impl(shape)\n     return semantic.view(input, shape, _builder)\n \n \n @builtin\n def reshape(input, shape, _builder=None):\n-    # TODO: should be more than just a view\n-    shape = [x.value for x in shape]\n-    return semantic.view(input, shape, _builder)\n+    shape = _shape_check_impl(shape)\n+    return semantic.reshape(input, shape, _builder)\n \n # -----------------------\n # Linear Algebra\n@@ -841,7 +872,7 @@ def load(pointer, mask=None, other=None, cache_modifier=\"\", eviction_policy=\"\",\n \n \n @builtin\n-def store(pointer, value, mask=None, _builder=None):\n+def store(pointer, value, mask=None, cache_modifier=\"\", eviction_policy=\"\", _builder=None):\n     \"\"\"\n     Stores :code:`value` tensor of elements in memory, element-wise, at the memory locations specified by :code:`pointer`.\n \n@@ -858,7 +889,9 @@ def store(pointer, value, mask=None, _builder=None):\n     value = _to_tensor(value, _builder)\n     if _constexpr_to_value(mask) is not None:\n         mask = _to_tensor(mask, _builder)\n-    return semantic.store(pointer, value, mask, _builder)\n+    cache_modifier = _constexpr_to_value(cache_modifier)\n+    eviction_policy = _constexpr_to_value(eviction_policy)\n+    return semantic.store(pointer, value, mask, cache_modifier, eviction_policy, _builder)\n \n \n # -----------------------\n@@ -1243,6 +1276,19 @@ def swizzle2d(i, j, size_i, size_j, size_g):\n     return new_i, new_j\n \n \n+@triton.jit\n+def zeros(shape, dtype):\n+    \"\"\"\n+    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n+\n+    :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n+    :type shape: tuple of ints\n+    :param dtype: Data-type of the new array, e.g., :code:`tl.float16`\n+    :type dtype: DType\n+    \"\"\"\n+    return full(shape, 0, dtype)\n+\n+\n @triton.jit\n def zeros_like(input):\n     return zeros(input.shape, input.dtype)\n@@ -1265,3 +1311,33 @@ def printf(prefix, *args, _builder=None):\n     for arg in args:\n         new_args.append(_to_tensor(arg, _builder))\n     return semantic.printf(new_prefix, new_args, _builder)\n+\n+# -----------------------\n+# Iterators\n+# -----------------------\n+\n+\n+class static_range:\n+\n+    \"\"\"Iterator that counts upward forever.\"\"\"\n+\n+    def __init__(self, arg1, arg2=None, step=None):\n+        assert isinstance(arg1, constexpr)\n+        if step is None:\n+            self.step = constexpr(1)\n+        else:\n+            assert isinstance(step, constexpr)\n+            self.step = step\n+        if arg2 is None:\n+            self.start = constexpr(0)\n+            self.end = arg1\n+        else:\n+            assert isinstance(arg2, constexpr)\n+            self.start = arg1\n+            self.end = arg2\n+\n+    def __iter__(self):\n+        raise RuntimeError(\"static_range can only be used in @triton.jit'd functions\")\n+\n+    def __next__(self):\n+        raise RuntimeError(\"static_range can only be used in @triton.jit'd functions\")"}, {"filename": "python/triton/language/extern.py", "status": "modified", "additions": 13, "deletions": 24, "changes": 37, "file_content_changes": "@@ -58,31 +58,20 @@ def elementwise(lib_name: str, lib_path: str, args: list, arg_type_symbol_dict:\n     dispatch_args = args.copy()\n     all_scalar = True\n     ret_shape = None\n-    for dispatch_arg in dispatch_args:\n-        if dispatch_arg.type.is_block():\n+    for i in range(len(dispatch_args)):\n+        dispatch_args[i] = core._to_tensor(dispatch_args[i], _builder)\n+        if dispatch_args[i].type.is_block():\n             all_scalar = False\n     if not all_scalar:\n-        if len(args) == 1:\n-            dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n-            ret_shape = dispatch_args[0].shape\n-        elif len(args) == 2:\n-            dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n-            dispatch_args[1] = core._to_tensor(dispatch_args[1], _builder)\n-            dispatch_args[0], dispatch_args[1] = semantic.binary_op_type_checking_impl(\n-                dispatch_args[0], dispatch_args[1], _builder)\n-            ret_shape = dispatch_args[0].shape\n-        else:\n-            for i in range(len(dispatch_args)):\n-                dispatch_args[i] = core._to_tensor(dispatch_args[i], _builder)\n-            broadcast_arg = dispatch_args[0]\n-            # Get the broadcast shape over all the arguments\n-            for i in range(len(dispatch_args)):\n-                _, broadcast_arg = semantic.binary_op_type_checking_impl(\n-                    dispatch_args[i], broadcast_arg, _builder)\n-            # Change the shape of each argument based on the broadcast shape\n-            for i in range(len(dispatch_args)):\n-                dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n-                    dispatch_args[i], broadcast_arg, _builder)\n-            ret_shape = broadcast_arg.shape\n+        broadcast_arg = dispatch_args[0]\n+        # Get the broadcast shape over all the arguments\n+        for i, item in enumerate(dispatch_args):\n+            _, broadcast_arg = semantic.binary_op_type_checking_impl(\n+                item, broadcast_arg, _builder)\n+        # Change the shape of each argument based on the broadcast shape\n+        for i in range(len(dispatch_args)):\n+            dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n+                dispatch_args[i], broadcast_arg, _builder)\n+        ret_shape = broadcast_arg.shape\n     func = getattr(_builder, \"create_external_elementwise\")\n     return dispatch(func, lib_name, lib_path, dispatch_args, arg_type_symbol_dict, ret_shape, _builder)"}, {"filename": "python/triton/language/libdevice.py", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -3,8 +3,7 @@\n from .. import impl\n from . import core, extern\n \n-LIBDEVICE_PATH = os.path.dirname(\n-    os.path.abspath(__file__)) + \"/libdevice.10.bc\"\n+LIBDEVICE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\", \"cuda\", \"lib\", \"libdevice.10.bc\")\n \n \n @impl.extern"}, {"filename": "python/triton/language/random.py", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -17,7 +17,8 @@ def philox_impl(c0, c1, c2, c3, k0, k1, n_rounds: tl.constexpr = N_ROUNDS_DEFAUL\n     \"\"\"\n     Run `n_rounds` rounds of Philox for state (c0, c1, c2, c3) and key (k0, k1).\n     \"\"\"\n-    for _ in range(n_rounds):\n+    for _ in tl.static_range(n_rounds):\n+        # for _ in range(n_rounds):\n         # update random state\n         A = PHILOX_ROUND_A\n         B = PHILOX_ROUND_B\n@@ -37,6 +38,10 @@ def philox(seed, c0, c1, c2, c3, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     seed = seed.to(tl.uint64)\n     seed_hi = ((seed >> 32) & 0xffffffff).to(tl.uint32)\n     seed_lo = (seed & 0xffffffff).to(tl.uint32)\n+    c0 = c0.to(tl.uint32, bitcast=True)\n+    c1 = c1.to(tl.uint32, bitcast=True)\n+    c2 = c2.to(tl.uint32, bitcast=True)\n+    c3 = c3.to(tl.uint32, bitcast=True)\n     return philox_impl(c0, c1, c2, c3, seed_lo, seed_hi, n_rounds)\n \n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 85, "deletions": 37, "changes": 122, "file_content_changes": "@@ -312,13 +312,26 @@ def logical_or(input: tl.tensor, other: tl.tensor, builder: ir.builder) -> tl.te\n     return or_(input, other, builder)\n \n \n+def not_(input: tl.tensor, builder: ir.builder):\n+    if not input.type.is_int1():\n+        input = bitcast(input, tl.dtype(\"int1\"), builder)\n+    return invert(input, builder)\n+\n+\n def lshr(input: tl.tensor,\n          other: tl.tensor,\n          builder: ir.builder) -> tl.tensor:\n     input, other = bitwise_op_type_checking_impl(input, other, builder)\n     return tl.tensor(builder.create_lshr(input.handle, other.handle), input.type)\n \n \n+def ashr(input: tl.tensor,\n+         other: tl.tensor,\n+         builder: ir.builder) -> tl.tensor:\n+    input, other = bitwise_op_type_checking_impl(input, other, builder)\n+    return tl.tensor(builder.create_ashr(input.handle, other.handle), input.type)\n+\n+\n def shl(input: tl.tensor,\n         other: tl.tensor,\n         builder: ir.builder) -> tl.tensor:\n@@ -471,10 +484,15 @@ def arange(start: int, end: int, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_make_range(start, end), ret_ty)\n \n \n-def zeros(shape: List[int], dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n-    _0 = builder.get_null_value(dtype.to_ir(builder))\n+def full(shape: List[int], value, dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n+    if value == 0:\n+        _value = builder.get_null_value(dtype.to_ir(builder))\n+    else:\n+        get_value_fn = getattr(builder, f\"get_{dtype.name}\")\n+        _value = get_value_fn(value)\n     ret_ty = tl.block_type(dtype, shape)\n-    return tl.tensor(builder.create_splat(_0, shape), ret_ty)\n+    return tl.tensor(builder.create_splat(_value, shape), ret_ty)\n+\n \n # ===----------------------------------------------------------------------===//\n #                               Shape Manipulation\n@@ -496,8 +514,15 @@ def view(input: tl.tensor,\n     return tl.tensor(builder.create_view(input.handle, dst_shape), ret_ty)\n \n \n+def reshape(input: tl.tensor,\n+            dst_shape: List[int],\n+            builder: ir.builder) -> tl.tensor:\n+    raise ValueError(\"`reshape` is not supported yet. Please use `view` instead if applicable. \"\n+                     \"Note that view may reorder elements in an implementation- and context- dependent way.\")\n+\n+\n def expand_dims(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n-    dst_shape = [s for s in input.type.shape]\n+    dst_shape = list(input.type.shape)\n     dst_shape.insert(axis, 1)\n     ret_ty = tl.block_type(input.type.scalar, dst_shape)\n     return tl.tensor(builder.create_expand_dims(input.handle, axis), ret_ty)\n@@ -528,10 +553,10 @@ def broadcast_impl_shape(input: tl.tensor,\n         raise ValueError(f\"Cannot broadcast, rank mismatch: {src_shape}, {shape}\")\n     if shape == src_shape:\n         return input\n-    for i in range(len(src_shape)):\n-        if shape[i] != src_shape[i] and src_shape[i] != 1:\n+    for i, item in enumerate(src_shape):\n+        if shape[i] != item and item != 1:\n             raise ValueError(f\"Cannot broadcast, the expanded size of the tensor ({shape[i]})\"\n-                             f\" must match the existing size ({src_shape[i]}) at non-singleton dimension\"\n+                             f\" must match the existing size ({item}) at non-singleton dimension\"\n                              f\" {i}: {src_shape}, {shape}\")\n     ret_ty = tl.block_type(input.type.scalar, shape)\n     return tl.tensor(builder.create_broadcast(input.handle, shape), ret_ty)\n@@ -571,8 +596,7 @@ def broadcast_impl_value(lhs: tl.tensor,\n         assert len(rhs_shape) == len(lhs_shape)\n \n         ret_shape = []\n-        for i in range(len(lhs_shape)):\n-            left = lhs_shape[i]\n+        for i, left in enumerate(lhs_shape):\n             right = rhs_shape[i]\n             if left == 1:\n                 ret_shape.append(right)\n@@ -730,6 +754,30 @@ def cast(input: tl.tensor,\n # ===----------------------------------------------------------------------===//\n \n \n+def str_to_cache_modifier(cache_modifier):\n+    cache = ir.CACHE_MODIFIER.NONE  # default\n+    if cache_modifier:\n+        if cache_modifier == \".ca\":\n+            cache = ir.CACHE_MODIFIER.CA\n+        elif cache_modifier == \".cg\":\n+            cache = ir.CACHE_MODIFIER.CG\n+        else:\n+            raise ValueError(f\"Cache modifier {cache_modifier} not supported\")\n+    return cache\n+\n+\n+def str_to_eviction_policy(eviction_policy):\n+    eviction = ir.EVICTION_POLICY.NORMAL  # default\n+    if eviction_policy:\n+        if eviction_policy == \"evict_last\":\n+            eviction = ir.EVICTION_POLICY.EVICT_LAST\n+        elif eviction_policy == \"evict_first\":\n+            eviction = ir.EVICTION_POLICY.EVICT_FIRST\n+        else:\n+            raise ValueError(f\"Eviction policy {eviction_policy} not supported\")\n+    return eviction\n+\n+\n def load(ptr: tl.tensor,\n          mask: Optional[tl.tensor],\n          other: Optional[tl.tensor],\n@@ -739,6 +787,11 @@ def load(ptr: tl.tensor,\n          builder: ir.builder) -> tl.tensor:\n     if not ptr.type.scalar.is_ptr():\n         raise ValueError(\"Pointer argument of load instruction is \" + ptr.type.__repr__())\n+    if not ptr.type.is_block():\n+        if mask and mask.type.is_block():\n+            raise ValueError(\"Mask argument cannot be block type if pointer argument is not a block\")\n+        if other and other.type.is_block():\n+            raise ValueError(\"Other argument cannot be block type if pointer argument is not a block\")\n     if ptr.type.is_block():\n         if mask:\n             mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n@@ -758,31 +811,16 @@ def load(ptr: tl.tensor,\n         other = cast(other, elt_ty, builder)\n \n     # cache modifier\n-    cache = ir.CACHE_MODIFIER.NONE  # default\n-    if cache_modifier:\n-        if cache_modifier == \".ca\":\n-            cache = ir.CACHE_MODIFIER.CA\n-        elif cache_modifier == \".cg\":\n-            cache = ir.CACHE_MODIFIER.CG\n-        else:\n-            raise ValueError(f\"Cache modifier {cache_modifier} not supported\")\n-\n-    # eviction policy\n-    eviction = ir.EVICTION_POLICY.NORMAL  # default\n-    if eviction_policy:\n-        if eviction_policy == \"evict_last\":\n-            eviction = ir.EVICTION_POLICY.EVICT_LAST\n-        elif eviction_policy == \"evict_first\":\n-            eviction = ir.EVICTION_POLICY.EVICT_FIRST\n-        else:\n-            raise ValueError(f\"Eviction policy {eviction_policy} not supported\")\n \n     if ptr.type.is_block():\n         shape = ptr.type.get_block_shapes()\n         dst_ty = tl.block_type(elt_ty, shape)\n     else:\n         dst_ty = elt_ty\n \n+    cache = str_to_cache_modifier(cache_modifier)\n+    eviction = str_to_eviction_policy(eviction_policy)\n+\n     if not mask:\n         if other:\n             raise ValueError(\"`other` cannot be provided without `mask`\")\n@@ -799,12 +837,19 @@ def load(ptr: tl.tensor,\n def store(ptr: tl.tensor,\n           val: tl.tensor,\n           mask: Optional[tl.tensor],\n+          cache_modifier: str,\n+          eviction_policy: str,\n           builder: ir.builder) -> tl.tensor:\n     if not ptr.type.scalar.is_ptr():\n         raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n+    if not ptr.type.is_block():\n+        if val.type.is_block():\n+            raise ValueError(\"Value argument cannot be block type if pointer argument is not a block\")\n+        if mask and mask.type.is_block():\n+            raise ValueError(\"Mask argument cannot be block type if pointer argument is not a block\")\n     if ptr.type.is_block():\n         val = broadcast_impl_shape(val, ptr.type.get_block_shapes(), builder)\n-    if mask:\n+    if mask and ptr.type.is_block():\n         mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n     ptr_ty = ptr.type.scalar\n     elt_ty = ptr_ty.element_ty\n@@ -813,14 +858,16 @@ def store(ptr: tl.tensor,\n         elt_ty = tl.int8\n         ptr_ty = tl.pointer_type(elt_ty, ptr_ty.address_space)\n         ptr = cast(ptr, ptr_ty, builder)\n-\n+    # attributes\n+    cache = str_to_cache_modifier(cache_modifier)\n+    eviction = str_to_eviction_policy(eviction_policy)\n     # cast to target data-type\n     val = cast(val, elt_ty, builder)\n     if not mask:\n-        return tl.tensor(builder.create_store(ptr.handle, val.handle), tl.void)\n+        return tl.tensor(builder.create_store(ptr.handle, val.handle, cache, eviction), tl.void)\n     if not mask.type.scalar.is_bool():\n         raise ValueError(\"Mask must have boolean scalar type\")\n-    return tl.tensor(builder.create_masked_store(ptr.handle, val.handle, mask.handle), tl.void)\n+    return tl.tensor(builder.create_masked_store(ptr.handle, val.handle, mask.handle, cache, eviction), tl.void)\n \n #########\n # atomic\n@@ -849,7 +896,7 @@ def atom_red_typechecking_impl(ptr: tl.tensor,\n     if element_ty is tl.float16 and op != 'add':\n         raise ValueError(\"atomic_\" + op + \" does not support fp16\")\n     if element_ty in [tl.int1, tl.int8, tl.int16, tl.bfloat16]:\n-        raise ValueError(\"atomic_\" + op + \" does not support \" + element_ty)\n+        raise ValueError(\"atomic_\" + op + \" does not support \" + str(element_ty))\n     if ptr.type.is_block():\n         if mask:\n             mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n@@ -891,8 +938,8 @@ def atomic_max(ptr: tl.tensor,\n     # return atomic_umin(i_ptr, i_val) if val < 0\n     i_val = bitcast(val, tl.int32, builder)\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n-    pos = greater_equal(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n-    neg = less_than(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n+    pos = greater_equal(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n+    neg = less_than(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n     pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX, i_ptr.handle, i_val.handle, and_(mask, pos, builder).handle), i_val.type)\n     neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN, i_ptr.handle, i_val.handle, and_(mask, neg, builder).handle), i_val.type)\n     return where(pos, pos_ret, neg_ret, builder)\n@@ -923,8 +970,8 @@ def atomic_min(ptr: tl.tensor,\n     # return atomic_umax(i_ptr, i_val) if val < 0\n     i_val = bitcast(val, tl.int32, builder)\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n-    pos = greater_equal(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n-    neg = less_than(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n+    pos = greater_equal(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n+    neg = less_than(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n     pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MIN,\n                                                   i_ptr.handle,\n                                                   i_val.handle,\n@@ -989,6 +1036,7 @@ def dot(lhs: tl.tensor,\n         allow_tf32: bool,\n         builder: ir.builder) -> tl.tensor:\n     assert lhs.type.is_block() and rhs.type.is_block()\n+    assert lhs.dtype == rhs.dtype, \"lhs and rhs must have the same dtype!\"\n     assert len(lhs.shape) == 2 and len(rhs.shape) == 2\n     assert lhs.shape[1].value == rhs.shape[0].value\n     assert lhs.shape[0].value >= 16 and lhs.shape[1].value >= 16 \\\n@@ -998,7 +1046,7 @@ def dot(lhs: tl.tensor,\n         _0 = builder.get_int32(0)\n         ret_scalar_ty = tl.int32\n     else:\n-        _0 = builder.get_float32(0)\n+        _0 = builder.get_fp32(0)\n         ret_scalar_ty = tl.float32\n     M = lhs.type.shape[0]\n     N = rhs.type.shape[1]"}, {"filename": "python/triton/ops/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -1,6 +1,7 @@\n # from .conv import _conv, conv\n from . import blocksparse\n from .cross_entropy import _cross_entropy, cross_entropy\n+from .flash_attention import attention\n from .matmul import _matmul, matmul\n \n __all__ = [\n@@ -9,4 +10,5 @@\n     \"cross_entropy\",\n     \"_matmul\",\n     \"matmul\",\n+    \"attention\",\n ]"}, {"filename": "python/triton/ops/blocksparse/softmax.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -231,7 +231,7 @@ def __init__(self, layout, block, device, is_dense=False):\n \n     def __call__(self, a, *, scale=1.0, rel_logits=None, is_causal=False):\n         if rel_logits is not None and rel_logits.dtype != a.dtype:\n-            raise ValueError(\"relative position embedding must be %s\" % a.dtype)\n+            raise ValueError(f\"relative position embedding must be {a.dtype}\")\n         a = _softmax.apply(\n             a, scale, rel_logits, is_causal,\n             self.spdims, self.block, self.lut, self.maxlut, self.is_dense,"}, {"filename": "python/triton/ops/flash_attention.py", "status": "added", "additions": 267, "deletions": 0, "changes": 267, "file_content_changes": "@@ -0,0 +1,267 @@\n+\"\"\"\n+Fused Attention\n+===============\n+This is a Triton implementation of the Flash Attention algorithm\n+(see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n+\"\"\"\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def _fwd_kernel(\n+    Q, K, V, sm_scale,\n+    L, M,\n+    Out,\n+    stride_qz, stride_qh, stride_qm, stride_qk,\n+    stride_kz, stride_kh, stride_kn, stride_kk,\n+    stride_vz, stride_vh, stride_vk, stride_vn,\n+    stride_oz, stride_oh, stride_om, stride_on,\n+    Z, H, N_CTX,\n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+    BLOCK_N: tl.constexpr,\n+):\n+    start_m = tl.program_id(0)\n+    off_hz = tl.program_id(1)\n+    # initialize offsets\n+    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+    offs_n = tl.arange(0, BLOCK_N)\n+    offs_d = tl.arange(0, BLOCK_DMODEL)\n+    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n+    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    # Initialize pointers to Q, K, V\n+    q_ptrs = Q + off_q\n+    k_ptrs = K + off_k\n+    v_ptrs = V + off_v\n+    # initialize pointer to m and l\n+    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n+    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    # load q: it will stay in SRAM throughout\n+    q = tl.load(q_ptrs)\n+    # loop over k, v and update accumulator\n+    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n+        # -- compute qk ----\n+        k = tl.load(k_ptrs)\n+        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+        qk += tl.dot(q, k)\n+        qk *= sm_scale\n+        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+        # compute new m\n+        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n+        # correct old l\n+        l_prev *= tl.exp(m_prev - m_curr)\n+        # attention weights\n+        p = tl.exp(qk - m_curr[:, None])\n+        l_curr = tl.sum(p, 1) + l_prev\n+        # rescale operands of matmuls\n+        l_rcp = 1. / l_curr\n+        p *= l_rcp\n+        acc *= (l_prev * l_rcp)[:, None]\n+        # update acc\n+        p = p.to(tl.float16)\n+        v = tl.load(v_ptrs)\n+        acc += tl.dot(p, v)\n+        # update m_i and l_i\n+        l_prev = l_curr\n+        m_prev = m_curr\n+        # update pointers\n+        k_ptrs += BLOCK_N * stride_kn\n+        v_ptrs += BLOCK_N * stride_vk\n+    # rematerialize offsets to save registers\n+    start_m = tl.program_id(0)\n+    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+    # write back l and m\n+    l_ptrs = L + off_hz * N_CTX + offs_m\n+    m_ptrs = M + off_hz * N_CTX + offs_m\n+    tl.store(l_ptrs, l_prev)\n+    tl.store(m_ptrs, m_prev)\n+    # initialize pointers to output\n+    offs_n = tl.arange(0, BLOCK_DMODEL)\n+    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n+    out_ptrs = Out + off_o\n+    tl.store(out_ptrs, acc)\n+\n+\n+@triton.jit\n+def _bwd_preprocess(\n+    Out, DO, L,\n+    NewDO, Delta,\n+    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n+):\n+    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n+    off_n = tl.arange(0, D_HEAD)\n+    # load\n+    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+    denom = tl.load(L + off_m).to(tl.float32)\n+    # compute\n+    do = do / denom[:, None]\n+    delta = tl.sum(o * do, axis=1)\n+    # write-back\n+    tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\n+    tl.store(Delta + off_m, delta)\n+\n+\n+@triton.jit\n+def _bwd_kernel(\n+    Q, K, V, sm_scale, Out, DO,\n+    DQ, DK, DV,\n+    L, M,\n+    D,\n+    stride_qz, stride_qh, stride_qm, stride_qk,\n+    stride_kz, stride_kh, stride_kn, stride_kk,\n+    stride_vz, stride_vh, stride_vk, stride_vn,\n+    Z, H, N_CTX,\n+    num_block,\n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+    BLOCK_N: tl.constexpr,\n+):\n+    off_hz = tl.program_id(0)\n+    off_z = off_hz // H\n+    off_h = off_hz % H\n+    # offset pointers for batch/head\n+    Q += off_z * stride_qz + off_h * stride_qh\n+    K += off_z * stride_qz + off_h * stride_qh\n+    V += off_z * stride_qz + off_h * stride_qh\n+    DO += off_z * stride_qz + off_h * stride_qh\n+    DQ += off_z * stride_qz + off_h * stride_qh\n+    DK += off_z * stride_qz + off_h * stride_qh\n+    DV += off_z * stride_qz + off_h * stride_qh\n+    for start_n in range(0, num_block):\n+        lo = start_n * BLOCK_M\n+        # initialize row/col offsets\n+        offs_qm = lo + tl.arange(0, BLOCK_M)\n+        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n+        offs_m = tl.arange(0, BLOCK_N)\n+        offs_k = tl.arange(0, BLOCK_DMODEL)\n+        # initialize pointers to value-like data\n+        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        # pointer to row-wise quantities in value-like data\n+        D_ptrs = D + off_hz * N_CTX\n+        m_ptrs = M + off_hz * N_CTX\n+        # initialize dv amd dk\n+        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        # k and v stay in SRAM throughout\n+        k = tl.load(k_ptrs)\n+        v = tl.load(v_ptrs)\n+        # loop over rows\n+        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n+            offs_m_curr = start_m + offs_m\n+            # load q, k, v, do on-chip\n+            q = tl.load(q_ptrs)\n+            # recompute p = softmax(qk, dim=-1).T\n+            # NOTE: `do` is pre-divided by `l`; no normalization here\n+            qk = tl.dot(q, tl.trans(k))\n+            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n+            m = tl.load(m_ptrs + offs_m_curr)\n+            p = tl.exp(qk * sm_scale - m[:, None])\n+            # compute dv\n+            do = tl.load(do_ptrs)\n+            dv += tl.dot(tl.trans(p.to(tl.float16)), do)\n+            # compute dp = dot(v, do)\n+            Di = tl.load(D_ptrs + offs_m_curr)\n+            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n+            dp += tl.dot(do, tl.trans(v))\n+            # compute ds = p * (dp - delta[:, None])\n+            ds = p * dp * sm_scale\n+            # compute dk = dot(ds.T, q)\n+            dk += tl.dot(tl.trans(ds.to(tl.float16)), q)\n+            # compute dq\n+            dq = tl.load(dq_ptrs)\n+            dq += tl.dot(ds.to(tl.float16), k)\n+            tl.store(dq_ptrs, dq)\n+            # increment pointers\n+            dq_ptrs += BLOCK_M * stride_qm\n+            q_ptrs += BLOCK_M * stride_qm\n+            do_ptrs += BLOCK_M * stride_qm\n+        # write-back\n+        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+        tl.store(dv_ptrs, dv)\n+        tl.store(dk_ptrs, dk)\n+\n+\n+class _attention(torch.autograd.Function):\n+\n+    @staticmethod\n+    def forward(ctx, q, k, v, sm_scale):\n+        # only support for Ampere now\n+        capability = torch.cuda.get_device_capability()\n+        if capability[0] < 8:\n+            raise RuntimeError(\"Flash attention currently only supported for compute capability < 80\")\n+        BLOCK = 128\n+        # shape constraints\n+        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n+        assert Lq == Lk and Lk == Lv\n+        # assert Lk in {16, 32, 64, 128}\n+        assert Lk in {64}  # TODO: fix other cases\n+        o = torch.empty_like(q)\n+        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n+        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        num_warps = 4 if Lk <= 64 else 8\n+\n+        _fwd_kernel[grid](\n+            q, k, v, sm_scale,\n+            L, m,\n+            o,\n+            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+            q.shape[0], q.shape[1], q.shape[2],\n+            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+            BLOCK_DMODEL=Lk, num_warps=num_warps,\n+            num_stages=2,\n+        )\n+\n+        ctx.save_for_backward(q, k, v, o, L, m)\n+        ctx.grid = grid\n+        ctx.sm_scale = sm_scale\n+        ctx.BLOCK_DMODEL = Lk\n+        return o\n+\n+    @staticmethod\n+    def backward(ctx, do):\n+        BLOCK = 128\n+        q, k, v, o, l, m = ctx.saved_tensors\n+        do = do.contiguous()\n+        dq = torch.zeros_like(q, dtype=torch.float32)\n+        dk = torch.empty_like(k)\n+        dv = torch.empty_like(v)\n+        do_scaled = torch.empty_like(do)\n+        delta = torch.empty_like(l)\n+        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n+            o, do, l,\n+            do_scaled, delta,\n+            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n+        )\n+        _bwd_kernel[(ctx.grid[1],)](\n+            q, k, v, ctx.sm_scale,\n+            o, do_scaled,\n+            dq, dk, dv,\n+            l, m,\n+            delta,\n+            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+            q.shape[0], q.shape[1], q.shape[2],\n+            ctx.grid[0],\n+            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+            num_stages=1,\n+        )\n+        return dq, dk, dv, None\n+\n+\n+attention = _attention.apply"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 7, "deletions": 6, "changes": 13, "file_content_changes": "@@ -71,8 +71,8 @@ def _kernel(A, B, C, M, N, K,\n     # matrix multiplication\n     pid = tl.program_id(0)\n     pid_z = tl.program_id(1)\n-    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n-    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n+    grid_m = tl.cdiv(M, BLOCK_M)\n+    grid_n = tl.cdiv(N, BLOCK_N)\n     # re-order program ID for better L2 performance\n     width = GROUP_M * grid_n\n     group_id = pid // width\n@@ -89,13 +89,14 @@ def _kernel(A, B, C, M, N, K,\n     A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n     B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n     acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n-    for k in range(K, 0, -BLOCK_K * SPLIT_K):\n+    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n         if EVEN_K:\n             a = tl.load(A)\n             b = tl.load(B)\n         else:\n-            a = tl.load(A, mask=rk[None, :] < k, other=0.)\n-            b = tl.load(B, mask=rk[:, None] < k, other=0.)\n+            k_remaining = K - k * (BLOCK_K * SPLIT_K)\n+            a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.)\n+            b = tl.load(B, mask=rk[:, None] < k_remaining, other=0.)\n         acc += tl.dot(a, b)\n         A += BLOCK_K * SPLIT_K * stride_ak\n         B += BLOCK_K * SPLIT_K * stride_bk\n@@ -115,7 +116,7 @@ def _kernel(A, B, C, M, N, K,\n class _matmul(torch.autograd.Function):\n     kernel = _kernel\n \n-    _locks = dict()\n+    _locks = {}\n \n     @staticmethod\n     def _call(a, b):"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -18,11 +18,11 @@ def __init__(self, fn, arg_names, configs, key, reset_to_zero, prune_configs_by:\n             'prune_num_stages_by'(optional): a function used to prune num_stages. It take configs:List[Config] as its input, and returns pruned configs.\n         '''\n         if not configs:\n-            self.configs = [Config(dict(), num_warps=4, num_stages=2)]\n+            self.configs = [Config({}, num_warps=4, num_stages=2)]\n         else:\n             self.configs = configs\n         self.key_idx = [arg_names.index(k) for k in key]\n-        self.cache = dict()\n+        self.cache = {}\n         # hook to reset all required tensor to zeros before relaunching a kernel\n         self.hook = lambda args: 0\n         if reset_to_zero is not None:\n@@ -62,14 +62,14 @@ def kernel_call():\n             self.hook(args)\n             self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **current)\n         try:\n-            return do_bench(kernel_call)\n+            return do_bench(kernel_call, percentiles=(0.5, 0.2, 0.8))\n         except OutOfResources:\n-            return float('inf')\n+            return (float('inf'), float('inf'), float('inf'))\n \n     def run(self, *args, **kwargs):\n         self.nargs = dict(zip(self.arg_names, args))\n         if len(self.configs) > 1:\n-            key = tuple([args[i] for i in self.key_idx])\n+            key = tuple(args[i] for i in self.key_idx)\n             if key not in self.cache:\n                 # prune configs\n                 pruned_configs = self.prune_configs(kwargs)"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -195,7 +195,7 @@ def _make_signature(self, sig_key):\n         return signature\n \n     def _make_constants(self, constexpr_key):\n-        constants = {i: k for i, k in zip(self.constexprs, constexpr_key)}\n+        constants = dict(zip(self.constexprs, constexpr_key))\n         return constants\n \n     def _call_hook(self, key, signature, device, constants, num_warps, num_stages, extern_libs, configs):\n@@ -241,8 +241,8 @@ def _make_launcher(self):\n         src = f\"\"\"\n def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False):\n     sig_key =  {sig_keys},\n-    constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else tuple()}\n-    spec_key = {f'{spec_keys},' if len(spec_keys) > 0 else tuple()}\n+    constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else ()}\n+    spec_key = {f'{spec_keys},' if len(spec_keys) > 0 else ()}\n     key = (version_key, sig_key, constexpr_key, spec_key)\n     if not extern_libs is None:\n       key = (key, tuple(extern_libs.items()))\n@@ -298,10 +298,10 @@ def __init__(self, fn, version=None, do_not_specialize=None):\n         # function signature information\n         signature = inspect.signature(fn)\n         self.arg_names = [v.name for v in signature.parameters.values()]\n-        self.has_defaults = any([v.default != inspect._empty for v in signature.parameters.values()])\n+        self.has_defaults = any(v.default != inspect._empty for v in signature.parameters.values())\n         # specialization hints\n         self.do_not_specialize = [] if do_not_specialize is None else do_not_specialize\n-        self.do_not_specialize = set([self.arg_names.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize])\n+        self.do_not_specialize = {self.arg_names.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize}\n         # function source code (without decorators)\n         self.src = textwrap.dedent(inspect.getsource(fn))\n         self.src = self.src[self.src.find(\"def\"):]"}, {"filename": "python/triton/third_party/cuda/bin/ptxas", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/third_party/cuda/include/cuda.h", "status": "added", "additions": 19348, "deletions": 0, "changes": 19348, "file_content_changes": "N/A"}, {"filename": "python/triton/third_party/cuda/lib/libdevice.10.bc", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "python/triton/language/libdevice.10.bc"}, {"filename": "python/triton/tools/build_extern.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -157,7 +157,8 @@ def __init__(self, path) -> None:\n         super().__init__(\"libdevice\", path)\n         self._symbol_groups = {}\n \n-    def _extract_symbol(self, line) -> Optional[Symbol]:\n+    @staticmethod\n+    def _extract_symbol(line) -> Optional[Symbol]:\n         # Extract symbols from line in the following format:\n         # \"define [internal] <ret_type> @<name>(<arg_types>,)\"\n         entries = line.split(\"@\")\n@@ -288,7 +289,7 @@ def _output_stubs(self) -> str:\n         #   return extern.dispatch(\"libdevice\", <path>, <args>, <arg_type_symbol_dict>, _builder)\n         import_str = \"from . import core, extern\\n\"\n         import_str += \"import os\\n\"\n-        header_str = \"LIBDEVICE_PATH = os.path.dirname(\\n\\tos.path.abspath(__file__)) + \\\"/libdevice.10.bc\\\"\\n\"\n+        header_str = \"LIBDEVICE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \\\"..\\\", \\\"third_party\\\", \\\"cuda\\\", \\\"lib\\\", \\\"libdevice.10.bc\\\")\"\n         func_str = \"\"\n         for symbols in self._symbol_groups.values():\n             func_str += \"@extern.extern\\n\""}, {"filename": "python/triton/utils.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -33,7 +33,8 @@ def wrap_dtype(arg):\n     def __init__(self, dtype):\n         self.dtype = dtype\n \n-    def data_ptr(self):\n+    @staticmethod\n+    def data_ptr():\n         return 0  # optimistically assumes multiple of 16\n \n "}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -187,6 +187,7 @@ def matmul_kernel(\n     pid = tl.program_id(axis=0)\n     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n     num_pid_in_group = GROUP_SIZE_M * num_pid_n\n     group_id = pid // num_pid_in_group\n     first_pid_m = group_id * GROUP_SIZE_M\n@@ -213,7 +214,7 @@ def matmul_kernel(\n     # of fp32 values for higher accuracy.\n     # `accumulator` will be converted back to fp16 after the loop\n     accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-    for k in range(0, K, BLOCK_SIZE_K):\n+    for k in range(0, num_pid_k):\n         # Note that for simplicity, we don't apply a mask here.\n         # This means that if K is not a multiple of BLOCK_SIZE_K,\n         # this will access out-of-bounds memory and produce an"}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 140, "deletions": 51, "changes": 191, "file_content_changes": "@@ -1,8 +1,30 @@\n \"\"\"\n Layer Normalization\n ====================\n+In this tutorial, you will write a high-performance layer normalization\n+kernel that runs faster than the PyTorch implementation.\n+You will specifically learn about:\n+\n+- How to implement backward pass in Triton\n+- How to implement parallel reduction in Triton\n \"\"\"\n \n+# %%\n+# Motivations\n+# -------------\n+# The *LayerNorm* operator was first introduced in [BA2016]_ as a way to improve the performance\n+# of sequential models (e.g., Transformers) or neural networks with small batch size.\n+# It takes a vector :math:`x` as input and produces a vector :math:`y` of the same shape as output.\n+# The normalization is performed by subtracting the mean and dividing by the standard deviation of :math:`x`.\n+# After the normalization, a learnable linear transformation with weights :math:`w` and biases :math:`b` is applied.\n+# The forward pass can be expressed as follows:\n+#\n+# .. math::\n+#    y = \\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} } * w + b\n+#\n+# where :math:`\\epsilon` is a small constant added to the denominator for numerical stability.\n+# Let\u2019s first take a look at the foward pass implementation.\n+\n import torch\n \n import triton\n@@ -19,128 +41,189 @@\n \n @triton.jit\n def _layer_norm_fwd_fused(\n-    A,\n-    Out,\n-    Weight,\n-    Bias,\n-    Mean, Rstd,\n-    stride, N, eps,\n+    X,  # pointer to the input\n+    Y,  # pointer to the output\n+    W,  # pointer to the weights\n+    B,  # pointer to the biases\n+    Mean,  # pointer to the mean\n+    Rstd,  # pointer to the 1/std\n+    stride,  # how much to increase the pointer when moving by 1 row\n+    N,  # number of columns in X\n+    eps,  # epsilon to avoid division by zero\n     BLOCK_SIZE: tl.constexpr,\n ):\n-    # position of elements processed by this program\n+    # Map the program id to the row of X and Y it should compute.\n     row = tl.program_id(0)\n-    Out += row * stride\n-    A += row * stride\n-    # compute mean\n+    Y += row * stride\n+    X += row * stride\n+    # Compute mean\n     mean = 0\n     _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n     for off in range(0, N, BLOCK_SIZE):\n         cols = off + tl.arange(0, BLOCK_SIZE)\n-        a = tl.load(A + cols, mask=cols < N, other=0.).to(tl.float32)\n+        a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n         _mean += a\n     mean = tl.sum(_mean, axis=0) / N\n-    # compute variance\n+    # Compute variance\n     _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n     for off in range(0, N, BLOCK_SIZE):\n         cols = off + tl.arange(0, BLOCK_SIZE)\n-        a = tl.load(A + cols, mask=cols < N, other=0.).to(tl.float32)\n-        a = tl.where(cols < N, a - mean, 0.)\n-        _var += a * a\n+        x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n+        x = tl.where(cols < N, x - mean, 0.)\n+        _var += x * x\n     var = tl.sum(_var, axis=0) / N\n     rstd = 1 / tl.sqrt(var + eps)\n-    # write-back mean/rstd\n+    # Write mean / rstd\n     tl.store(Mean + row, mean)\n     tl.store(Rstd + row, rstd)\n-    # multiply by weight and add bias\n+    # Normalize and apply linear transformation\n     for off in range(0, N, BLOCK_SIZE):\n         cols = off + tl.arange(0, BLOCK_SIZE)\n         mask = cols < N\n-        weight = tl.load(Weight + cols, mask=mask)\n-        bias = tl.load(Bias + cols, mask=mask)\n-        a = tl.load(A + cols, mask=mask, other=0.).to(tl.float32)\n-        a_hat = (a - mean) * rstd\n-        out = a_hat * weight + bias\n-        # # write-back\n-        tl.store(Out + cols, out, mask=mask)\n+        w = tl.load(W + cols, mask=mask)\n+        b = tl.load(B + cols, mask=mask)\n+        x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)\n+        x_hat = (x - mean) * rstd\n+        y = x_hat * w + b\n+        # Write output\n+        tl.store(Y + cols, y, mask=mask)\n \n \n-# Backward pass (DX + partial DW + partial DB)\n+# %%\n+# Backward pass\n+# ---------------------------------\n+# The backward pass for the layer normalization operator is a bit more involved than the forward pass.\n+# Let :math:`\\hat{x}` be the normalized inputs :math:`\\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} }` before the linear transformation,\n+# the Vector-Jacobian Products (VJP) :math:`\\nabla_{x}` of :math:`x` are given by:\n+#\n+# .. math::\n+#    \\nabla_{x} = \\frac{1}{\\sigma}\\Big( \\nabla_{y} \\odot w - \\underbrace{ \\big( \\frac{1}{N} \\hat{x} \\cdot (\\nabla_{y} \\odot w) \\big) }_{c_1} \\odot \\hat{x} - \\underbrace{ \\frac{1}{N} \\nabla_{y} \\cdot w }_{c_2} \\Big)\n+#\n+# where :math:`\\odot` denotes the element-wise multiplication, :math:`\\cdot` denotes the dot product, and :math:`\\sigma` is the standard deviation.\n+# :math:`c_1` and :math:`c_2` are intermediate constants that improve the readability of the following implementation.\n+#\n+# For the weights :math:`w` and biases :math:`b`, the VJPs :math:`\\nabla_{w}` and :math:`\\nabla_{b}` are more straightforward:\n+#\n+# .. math::\n+#    \\nabla_{w} = \\nabla_{y} \\odot \\hat{x} \\quad \\text{and} \\quad \\nabla_{b} = \\nabla_{y}\n+#\n+# Since the same weights :math:`w` and biases :math:`b` are used for all rows in the same batch, their gradients need to sum up.\n+# To perform this step efficiently, we use a parallel reduction strategy: each kernel instance accumulates\n+# partial :math:`\\nabla_{w}` and :math:`\\nabla_{b}` across certain rows into one of :math:`\\text{GROUP_SIZE_M}` independent buffers.\n+# These buffers stay in the L2 cache and then are further reduced by another function to compute the actual :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n+#\n+# Let the number of input rows :math:`M = 4` and :math:`\\text{GROUP_SIZE_M} = 2`,\n+# here's a diagram of the parallel reduction strategy for :math:`\\nabla_{w}` (:math:`\\nabla_{b}` is omitted for brevity):\n+#\n+#   .. image:: parallel_reduction.png\n+#\n+# In Stage 1, the rows of X that have the same color share the same buffer and thus a lock is used to ensure that only one kernel instance writes to the buffer at a time.\n+# In Stage 2, the buffers are further reduced to compute the final :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n+# In the following implementation, Stage 1 is implemented by the function :code:`_layer_norm_bwd_dx_fused` and Stage 2 is implemented by the function :code:`_layer_norm_bwd_dwdb`.\n+\n @triton.jit\n-def _layer_norm_bwd_dx_fused(DX, DY, DW, DB, X, W, B, M, V, Lock, stride, N, eps,\n-                             GROUP_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):\n-    # position of elements processed by this program\n+def _layer_norm_bwd_dx_fused(\n+    DX,  # pointer to the input gradient\n+    DY,  # pointer to the output gradient\n+    DW,  # pointer to the partial sum of weights gradient\n+    DB,  # pointer to the partial sum of biases gradient\n+    X,   # pointer to the input\n+    W,   # pointer to the weights\n+    B,   # pointer to the biases\n+    Mean,   # pointer to the mean\n+    Rstd,   # pointer to the 1/std\n+    Lock,  # pointer to the lock\n+    stride,  # how much to increase the pointer when moving by 1 row\n+    N,  # number of columns in X\n+    eps,  # epsilon to avoid division by zero\n+    GROUP_SIZE_M: tl.constexpr,\n+    BLOCK_SIZE_N: tl.constexpr\n+):\n+    # Map the program id to the elements of X, DX, and DY it should compute.\n     row = tl.program_id(0)\n     cols = tl.arange(0, BLOCK_SIZE_N)\n     mask = cols < N\n-    # offset data pointers to start at the row of interest\n     X += row * stride\n     DY += row * stride\n     DX += row * stride\n-    # offset locks and weight/bias gradient pointer\n-    # each kernel instance accumulates partial sums for\n-    # DW and DB into one of GROUP_SIZE_M independent buffers\n-    # these buffers stay in the L2, which allow this kernel\n-    # to be fast\n+    # Offset locks and weights/biases gradient pointer for parallel reduction\n     lock_id = row % GROUP_SIZE_M\n     Lock += lock_id\n     Count = Lock + GROUP_SIZE_M\n     DW = DW + lock_id * N + cols\n     DB = DB + lock_id * N + cols\n-    # load data to SRAM\n+    # Load data to SRAM\n     x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n     dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n     w = tl.load(W + cols, mask=mask).to(tl.float32)\n-    mean = tl.load(M + row)\n-    rstd = tl.load(V + row)\n-    # compute dx\n+    mean = tl.load(Mean + row)\n+    rstd = tl.load(Rstd + row)\n+    # Compute dx\n     xhat = (x - mean) * rstd\n     wdy = w * dy\n     xhat = tl.where(mask, xhat, 0.)\n     wdy = tl.where(mask, wdy, 0.)\n-    mean1 = tl.sum(xhat * wdy, axis=0) / N\n-    mean2 = tl.sum(wdy, axis=0) / N\n-    dx = (wdy - (xhat * mean1 + mean2)) * rstd\n-    # write-back dx\n+    c1 = tl.sum(xhat * wdy, axis=0) / N\n+    c2 = tl.sum(wdy, axis=0) / N\n+    dx = (wdy - (xhat * c1 + c2)) * rstd\n+    # Write dx\n     tl.store(DX + cols, dx, mask=mask)\n-    # accumulate partial sums for dw/db\n+    # Accumulate partial sums for dw/db\n     partial_dw = (dy * xhat).to(w.dtype)\n     partial_db = (dy).to(w.dtype)\n     while tl.atomic_cas(Lock, 0, 1) == 1:\n         pass\n     count = tl.load(Count)\n-    # first store doesn't accumulate\n+    # First store doesn't accumulate\n     if count == 0:\n         tl.atomic_xchg(Count, 1)\n     else:\n         partial_dw += tl.load(DW, mask=mask)\n         partial_db += tl.load(DB, mask=mask)\n     tl.store(DW, partial_dw, mask=mask)\n     tl.store(DB, partial_db, mask=mask)\n-    # release lock\n+    # Release the lock\n     tl.atomic_xchg(Lock, 0)\n \n-# Backward pass (total DW + total DB)\n-\n \n @triton.jit\n-def _layer_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, M, N,\n-                         BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):\n+def _layer_norm_bwd_dwdb(\n+    DW,  # pointer to the partial sum of weights gradient\n+    DB,  # pointer to the partial sum of biases gradient\n+    FINAL_DW,  # pointer to the weights gradient\n+    FINAL_DB,  # pointer to the biases gradient\n+    M,  # GROUP_SIZE_M\n+    N,  # number of columns\n+    BLOCK_SIZE_M: tl.constexpr,\n+    BLOCK_SIZE_N: tl.constexpr\n+):\n+    # Map the program id to the elements of DW and DB it should compute.\n     pid = tl.program_id(0)\n     cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n     dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n     db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+    # Iterate through the rows of DW and DB to sum the partial sums.\n     for i in range(0, M, BLOCK_SIZE_M):\n         rows = i + tl.arange(0, BLOCK_SIZE_M)\n         mask = (rows[:, None] < M) & (cols[None, :] < N)\n         offs = rows[:, None] * N + cols[None, :]\n         dw += tl.load(DW + offs, mask=mask, other=0.)\n         db += tl.load(DB + offs, mask=mask, other=0.)\n+    # Write the final sum to the output.\n     sum_dw = tl.sum(dw, axis=0)\n     sum_db = tl.sum(db, axis=0)\n     tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n     tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n \n \n+# %%\n+# Benchmark\n+# ---------------------------------\n+# We can now compare the performance of our kernel against that of PyTorch.\n+# Here we focus on inputs that have Less than 64KB per feature.\n+# Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward pass.\n+\n+\n class LayerNorm(torch.autograd.Function):\n \n     @staticmethod\n@@ -172,7 +255,7 @@ def forward(ctx, x, normalized_shape, weight, bias, eps):\n     @staticmethod\n     def backward(ctx, dy):\n         x, w, b, m, v = ctx.saved_tensors\n-        # heuristics for amount of parallel reduction stream for DG/DB\n+        # heuristics for amount of parallel reduction stream for DW/DB\n         N = w.shape[0]\n         GROUP_SIZE_M = 64\n         if N <= 8192: GROUP_SIZE_M = 96\n@@ -275,4 +358,10 @@ def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='c\n \n \n test_layer_norm(1151, 8192, torch.float16)\n-# bench_layer_norm.run(save_path='.', print_data=True)\n+bench_layer_norm.run(save_path='.', print_data=True)\n+\n+# %%\n+# References\n+# --------------\n+#\n+# .. [BA2016] Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \"Layer Normalization\", Arxiv 2016"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 30, "deletions": 32, "changes": 62, "file_content_changes": "@@ -15,7 +15,7 @@\n @triton.jit\n def _fwd_kernel(\n     Q, K, V, sm_scale,\n-    TMP, L, M,  # NOTE: TMP is a scratchpad buffer to work around a compiler bug\n+    L, M,\n     Out,\n     stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n@@ -39,51 +39,48 @@ def _fwd_kernel(\n     k_ptrs = K + off_k\n     v_ptrs = V + off_v\n     # initialize pointer to m and l\n-    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n-    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n+    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n     acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n     # load q: it will stay in SRAM throughout\n     q = tl.load(q_ptrs)\n     # loop over k, v and update accumulator\n     for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n-        # start_n = tl.multiple_of(start_n, BLOCK_N)\n         # -- compute qk ----\n-        k = tl.load(k_ptrs + start_n * stride_kn)\n+        k = tl.load(k_ptrs)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         qk += tl.dot(q, tl.trans(k))\n         qk *= sm_scale\n-        qk += tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), 0, float(\"-inf\"))\n-        # -- compute m_ij, p, l_ij\n-        m_ij = tl.max(qk, 1)\n-        p = tl.exp(qk - m_ij[:, None])\n-        l_ij = tl.sum(p, 1)\n-        # -- update m_i and l_i\n-        m_i_new = tl.maximum(m_i, m_ij)\n-        alpha = tl.exp(m_i - m_i_new)\n-        beta = tl.exp(m_ij - m_i_new)\n-        l_i_new = alpha * l_i + beta * l_ij\n-        # -- update output accumulator --\n-        # scale p\n-        p_scale = beta / l_i_new\n-        p = p * p_scale[:, None]\n-        # scale acc\n-        acc_scale = l_i / l_i_new * alpha\n-        acc = acc * acc_scale[:, None]\n+        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+        # compute new m\n+        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n+        # correct old l\n+        l_prev *= tl.exp(m_prev - m_curr)\n+        # attention weights\n+        p = tl.exp(qk - m_curr[:, None])\n+        l_curr = tl.sum(p, 1) + l_prev\n+        # rescale operands of matmuls\n+        l_rcp = 1. / l_curr\n+        p *= l_rcp\n+        acc *= (l_prev * l_rcp)[:, None]\n         # update acc\n-        v = tl.load(v_ptrs + start_n * stride_vk)\n         p = p.to(tl.float16)\n+        v = tl.load(v_ptrs)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n-        l_i = l_i_new\n-        m_i = m_i_new\n+        l_prev = l_curr\n+        m_prev = m_curr\n+        # update pointers\n+        k_ptrs += BLOCK_N * stride_kn\n+        v_ptrs += BLOCK_N * stride_vk\n     # rematerialize offsets to save registers\n     start_m = tl.program_id(0)\n     offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n     # write back l and m\n     l_ptrs = L + off_hz * N_CTX + offs_m\n     m_ptrs = M + off_hz * N_CTX + offs_m\n-    tl.store(l_ptrs, l_i)\n-    tl.store(m_ptrs, m_i)\n+    tl.store(l_ptrs, l_prev)\n+    tl.store(m_ptrs, m_prev)\n     # initialize pointers to output\n     offs_n = tl.arange(0, BLOCK_DMODEL)\n     off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n@@ -209,14 +206,13 @@ def forward(ctx, q, k, v, sm_scale):\n         assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n         grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n-        tmp = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         num_warps = 4 if Lk <= 64 else 8\n \n         _fwd_kernel[grid](\n             q, k, v, sm_scale,\n-            tmp, L, m,\n+            L, m,\n             o,\n             q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n             k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n@@ -316,15 +312,15 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n # vary seq length for fixed head and batch=4\n configs = [triton.testing.Benchmark(\n     x_names=['N_CTX'],\n-    x_vals=[2**i for i in range(10, 15)],\n+    x_vals=[2**i for i in range(10, 14)],\n     line_arg='provider',\n     line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n     line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n     styles=[('red', '-'), ('blue', '-')],\n     ylabel='ms',\n     plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n     args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\n-) for mode in ['fwd']]\n+) for mode in ['fwd', 'bwd']]\n \n \n @triton.testing.perf_report(configs)\n@@ -357,4 +353,6 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n         ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n         return ms\n \n-# bench_flash_attention.run(save_path='.', print_data=True)\n+\n+# only works on post-Ampere GPUs right now\n+bench_flash_attention.run(save_path='.', print_data=True)"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 47, "deletions": 16, "changes": 63, "file_content_changes": "@@ -11,7 +11,7 @@\n \n // CHECK-LABEL: matmul_loop\n // There shouldn't be any aliasing with the dot op encoding.\n-func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n   %a_mask = arith.constant dense<true> : tensor<128x32xi1, #AL>\n@@ -36,7 +36,7 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n }\n \n // CHECK-LABEL: alloc\n-func @alloc(%A : !tt.ptr<f16>) {\n+func.func @alloc(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n@@ -46,15 +46,15 @@ func @alloc(%A : !tt.ptr<f16>) {\n }\n \n // CHECK-LABEL: convert\n-func @convert(%A : !tt.ptr<f16>) {\n+func.func @convert(%A : !tt.ptr<f16>) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   // CHECK: %0 -> %0\n   %cst1 = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: trans\n-func @trans(%A : !tt.ptr<f16>) {\n+func.func @trans(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %tensor = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n   // CHECK: %0 -> %cst\n@@ -63,7 +63,7 @@ func @trans(%A : !tt.ptr<f16>) {\n }\n \n // CHECK-LABEL: insert_slice_async\n-func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n+func.func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n@@ -76,31 +76,31 @@ func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n }\n \n // CHECK-LABEL: insert_slice\n-func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n+func.func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   // CHECK: %cst_0 -> %cst_0\n   %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : index\n   %a = tt.load %a_ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #AL>\n-  // CHECK: %3 -> %cst_0\n+  // CHECK: %inserted_slice -> %cst_0\n   %b = tensor.insert_slice %a into %tensor[%index, 0, 0][1, 16, 16][1, 1, 1]: tensor<16x16xf16, #AL> into tensor<1x16x16xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: extract_slice\n-func @extract_slice(%A : !tt.ptr<f16>) {\n+func.func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : index\n-  // CHECK-NEXT: %0 -> %cst\n+  // CHECK-NEXT: %extracted_slice -> %cst\n   %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: if_cat\n-func @if_cat(%i1 : i1) {\n+func.func @if_cat(%i1 : i1) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK: %cst_0 -> %cst_0\n@@ -119,7 +119,7 @@ func @if_cat(%i1 : i1) {\n }\n \n // CHECK-LABEL: if_alias\n-func @if_alias(%i1 : i1) {\n+func.func @if_alias(%i1 : i1) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n@@ -134,7 +134,7 @@ func @if_alias(%i1 : i1) {\n }\n \n // CHECK-LABEL: for\n-func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n@@ -154,7 +154,7 @@ func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.p\n }\n \n // CHECK-LABEL: for_if\n-func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+func.func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: %cst -> %cst\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n@@ -170,7 +170,7 @@ func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !t\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     scf.if %i1 {\n       %index = arith.constant 8 : index\n-      // CHECK-NEXT: %1 -> %cst,%cst_0\n+      // CHECK-NEXT: %extracted_slice -> %cst,%cst_0\n       %cst0 = tensor.extract_slice %a_shared[%index, 0][1, 32][1, 1] : tensor<128x32xf16, #A_SHARED> to tensor<32xf16, #A_SHARED>\n       scf.yield\n     }\n@@ -179,8 +179,8 @@ func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !t\n   return\n }\n \n-// CHECK-LABEL: for_if_for\n-func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+// CHECK-LABEL: for_for_if\n+func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: %cst -> %cst\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n@@ -213,3 +213,34 @@ func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   }\n   return\n }\n+\n+// CHECK-LABEL: cf_for\n+func.func @cf_for(%arg0: index, %arg1: index, %arg2: index, %arg3: !tt.ptr<f16>, %arg4: !tt.ptr<f16>) {\n+  // CHECK: %cst -> %cst\n+  %cst = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK-NEXT: %cst_0 -> %cst_0\n+  %cst_0 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #A_SHARED>\n+  gpu.barrier\n+  // CHECK-NEXT: %0 -> %0\n+  %0 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+  // CHECK-NEXT: %cst_1 -> %cst_1\n+  %cst_1 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK-NEXT: %2 -> %cst,%cst_0,%cst_1\n+  // CHECK-NEXT: %3 -> %cst,%cst_0,%cst_1\n+  // CHECK-NEXT: %4 -> %cst,%cst_0,%cst_1\n+  cf.br ^bb1(%arg0, %cst, %cst_0, %cst_1 : index, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>)\n+^bb1(%1: index, %2: tensor<128x32xf16, #A_SHARED>, %3: tensor<128x32xf16, #A_SHARED>, %4: tensor<128x32xf16, #A_SHARED>):  // 2 preds: ^bb0, ^bb2\n+  %5 = arith.cmpi slt, %1, %arg1 : index\n+  cf.cond_br %5, ^bb2, ^bb3\n+^bb2:  // pred: ^bb1\n+  %6 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #blocked>\n+  gpu.barrier\n+  %7 = tt.cat %2, %3 {axis = 0 : i64} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #blocked>\n+  %8 = arith.addi %1, %arg2 : index\n+  cf.br ^bb1(%8, %4, %2, %3 : index, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>)\n+^bb3:  // pred: ^bb1\n+  gpu.barrier\n+  // CHECK-NEXT: %9 -> %9\n+  %9 = tt.cat %0, %0 {axis = 0 : i64} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n+  return\n+}"}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 336, "deletions": 42, "changes": 378, "file_content_changes": "@@ -1,51 +1,342 @@\n-// RUN: triton-opt %s -test-print-alignment -split-input-file 2>&1 | FileCheck %s\n+// RUN: triton-opt %s -test-print-alignment -split-input-file -o %t 2>&1 | FileCheck %s\n \n-func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n-  // CHECK: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1]\n+// CHECK-LABEL: @cast\n+func.func @cast() {\n+  // CHECK: contiguity = [1], divisibility = [1], constancy = [1], constant_value = 1\n+  %cst = arith.constant 1 : i32\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = 1\n+  %0 = arith.extsi %cst : i32 to i64\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n+  %cst_tensor = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n+  %1 = tt.bitcast %cst_tensor : tensor<128xi32> -> tensor<128xi64>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @add\n+func.func @add() {\n+  // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n+  %1 = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [128], divisibility = [1], constancy = [1], constant_value = <none>\n+  %2 = arith.addi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 127\n+  %3 = arith.constant dense<127> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [128], constant_value = 128\n+  %4 = arith.addi %1, %3 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @sub\n+func.func @sub() {\n+  // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n+  %1 = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [128], divisibility = [1], constancy = [1], constant_value = <none>\n+  %2 = arith.subi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 129\n+  %3 = arith.constant dense<129> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [128], constant_value = 128\n+  %4 = arith.subi %3, %1 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @mul\n+func.func @mul() {\n+  // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n+  %1 = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n+  %2 = arith.muli %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [128], constant_value = 128\n+  %3 = arith.constant dense<128> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [128], constant_value = 128\n+  %4 = arith.muli %3, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [2], constancy = [128], constant_value = 2\n+  %5 = arith.constant dense<2> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [256], constancy = [128], constant_value = 256\n+  %6 = arith.muli %4, %5 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @div\n+func.func @div() {\n+  // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n+  %1 = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n+  %2 = arith.divsi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n+  %3 = arith.divui %1, %0 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [64], constancy = [128], constant_value = 64\n+  %4 = arith.constant dense<64> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [16777216], constancy = [64], constant_value = <none>\n+  %5 = arith.divsi %0, %4 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n+  %6 = arith.divsi %4, %0 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [64], constancy = [128], constant_value = 64\n+  %7 = arith.divsi %4, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [2], constancy = [128], constant_value = 66\n+  %8 = arith.constant dense<66> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [2], constant_value = <none>\n+  %9 = arith.divui %0, %8 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [128], divisibility = [8192], constancy = [1], constant_value = <none>\n+  %10 = tt.make_range {end = 8320 : i32, start = 8192 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [64], constant_value = <none>\n+  %11 = arith.divsi %10, %4 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @rem\n+func.func @rem() {\n+  // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n+  %1 = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [4611686018427387904], constancy = [128], constant_value = 0\n+  %2 = arith.remsi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n+  %3 = arith.remui %1, %0 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [64], constancy = [128], constant_value = 64\n+  %4 = arith.constant dense<64> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [64], divisibility = [64], constancy = [1], constant_value = <none>\n+  %5 = arith.remsi %0, %4 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [64], constancy = [1], constant_value = <none>\n+  %6 = arith.remsi %4, %0 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [2], constancy = [128], constant_value = 66\n+  %7 = arith.constant dense<66> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [2], divisibility = [2], constancy = [1], constant_value = <none>\n+  %8 = arith.remui %0, %7 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @broadcast\n+func.func @broadcast() {\n+  // CHECK: contiguity = [1], divisibility = [64], constancy = [128], constant_value = 64\n+  %0 = arith.constant dense<64> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1, 1], divisibility = [64, 1], constancy = [128, 1], constant_value = 64\n+  %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n+  // CHECK-NEXT: contiguity = [1, 1], divisibility = [64, 1], constancy = [128, 128], constant_value = 64\n+  %2 = tt.broadcast %1 : (tensor<128x1xi32>) -> tensor<128x128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @splat\n+func.func @splat(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n+  // CHECK: contiguity = [1, 1], divisibility = [16, 16], constancy = [128, 128], constant_value = <none>\n+  %0 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @cmp\n+func.func @cmp() {\n+  // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [4611686018427387904], constancy = [128], constant_value = 0\n+  %1 = arith.constant dense<0> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = <none>\n+  %2 = arith.cmpi eq, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = <none>\n+  %3 = arith.cmpi slt, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n+  %4 = arith.cmpi sle, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = <none>\n+  %5 = arith.cmpi sge, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [8], constancy = [128], constant_value = 8\n+  %6 = arith.constant dense<8> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [8], constant_value = <none>\n+  %7 = arith.cmpi sgt, %0, %6 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 0\n+  %8 = arith.cmpi sgt, %1, %6 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @logic\n+func.func @logic() {\n+  // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [64], constancy = [128], constant_value = 64\n+  %1 = arith.constant dense<64> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [16777216], constancy = [64], constant_value = <none>\n+  %2 = arith.divsi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [8], constancy = [128], constant_value = 8\n+  %3 = arith.constant dense<8> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [134217728], constancy = [8], constant_value = <none>\n+  %4 = arith.divsi %0, %3 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n+  %5 = arith.andi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n+  %6 = arith.ori %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n+  %7 = arith.xori %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [8], constant_value = <none>\n+  %8 = arith.andi %2, %4 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [8], constant_value = <none>\n+  %9 = arith.ori %2, %4 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [8], constant_value = <none>\n+  %10 = arith.xori %2, %4 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @select\n+func.func @select() {\n+  // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [4611686018427387904], constancy = [128], constant_value = 0\n+  %1 = arith.constant dense<0> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = <none>\n+  %2 = arith.cmpi eq, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = <none>\n+  %3 = arith.cmpi slt, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [4611686018427387904], constancy = [1], constant_value = 0\n+  %4 = arith.constant 0 : i1\n+  // CHECK-NEXT: contiguity = [1], divisibility = [4611686018427387904], constancy = [128], constant_value = 0\n+  %7 = tt.splat %4 : (i1) -> tensor<128xi1>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [4611686018427387904], constancy = [128], constant_value = 0\n+  %5 = arith.select %4, %3, %7 : tensor<128xi1>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = <none>\n+  %8 = \"triton_gpu.select\"(%7, %3, %2) : (tensor<128xi1>, tensor<128xi1>, tensor<128xi1>) -> tensor<128xi1>\n+  return\n+}\n+\n+// -----\n+\n+func.func @shift() {\n+  // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [8], constancy = [128], constant_value = 8\n+  %1 = arith.constant dense<8> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [4], constancy = [128], constant_value = 4\n+  %2 = arith.constant dense<4> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [274877906944], constancy = [1], constant_value = <none>\n+  %3 = arith.shli %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [67108864], constancy = [1], constant_value = <none>\n+  %4 = arith.shrsi %0, %2 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [128], constant_value = 128\n+  %5 = arith.shli %1, %2 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+func.func @max_min() {\n+  // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [128], divisibility = [64], constancy = [1], constant_value = <none>\n+  %1 = tt.make_range {end = 192 : i32, start = 64 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n+  %2 = arith.maxsi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n+  %3 = arith.minsi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [8], constancy = [128], constant_value = 8\n+  %4 = arith.constant dense<8> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [4], constancy = [128], constant_value = 4\n+  %5 = arith.constant dense<4> : tensor<128xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = 8\n+  %6 = arith.maxsi %4, %5 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @for\n+func.func @for() {\n+  // CHECK: contiguity = [1, 1], divisibility = [4611686018427387904, 4611686018427387904], constancy = [128, 32], constant_value = 0\n+  %a_init = arith.constant dense<0> : tensor<128x32xi32>\n+  // CHECK-NEXT: contiguity = [1, 1], divisibility = [1, 1], constancy = [128, 32], constant_value = 1\n+  %b_init = arith.constant dense<1> : tensor<128x32xi32>\n+  // CHECK-NEXT: contiguity = [1, 1], divisibility = [4, 4], constancy = [128, 32], constant_value = 4\n+  %c_init = arith.constant dense<4> : tensor<128x32xi32>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [1], constant_value = 128\n+  %ub = arith.constant 128 : index\n+  // CHECK-NEXT: contiguity = [1], divisibility = [4611686018427387904], constancy = [1], constant_value = 0\n+  %lb = arith.constant 0 : index\n+  // CHECK-NEXT: contiguity = [1], divisibility = [16], constancy = [1], constant_value = 16\n+  %step = arith.constant 16 : index\n+  %a, %b, %c = scf.for %iv = %lb to %ub step %step iter_args(%a = %a_init, %b = %b_init, %c = %c_init) -> (tensor<128x32xi32>, tensor<128x32xi32>, tensor<128x32xi32>) {\n+    // CHECK-NEXT: contiguity = [1], divisibility = [16], constancy = [1], constant_value = <none>\n+    %t = arith.index_cast %iv : index to i32\n+    // CHECK: contiguity = [1, 1], divisibility = [1, 1], constancy = [128, 32], constant_value = <none>\n+    // CHECK: contiguity = [1, 1], divisibility = [1, 1], constancy = [128, 32], constant_value = <none>\n+    // CHECK: contiguity = [1, 1], divisibility = [4, 4], constancy = [128, 32], constant_value = 4\n+    scf.yield %b, %a, %c : tensor<128x32xi32>, tensor<128x32xi32>, tensor<128x32xi32>\n+  }\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: @permute_2d\n+func.func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n+  // CHECK: contiguity = [1, 1], divisibility = [1, 1], constancy = [128, 128], constant_value = 1\n   %cst = arith.constant dense<true> : tensor<128x128xi1>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: contiguity = [1, 1], divisibility = [1, 1], constancy = [1, 1], constant_value = <none>\n   %cst_0 = arith.constant dense<0.000000e+00> : tensor<128x128xf32>\n-  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [65536] ; Constancy: [1]\n+  // CHECK-NEXT: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [65536] ; Constancy: [1]\n+  // CHECK-NEXT: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %1 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [65536, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: contiguity = [128, 1], divisibility = [1073741824, 1], constancy = [1, 1], constant_value = <none>\n   %2 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n+  // CHECK-NEXT: contiguity = [1, 1], divisibility = [16, 16], constancy = [128, 1], constant_value = <none>\n   %3 = tt.splat %arg1 : (i32) -> tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1048576, 16] ; Constancy: [1, 1]\n+  // CHECK-NEXT: contiguity = [1, 1], divisibility = [17179869184, 16], constancy = [1, 1], constant_value = <none>\n   %4 = arith.muli %2, %3 : tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n+  // CHECK-NEXT: contiguity = [1, 1], divisibility = [16, 16], constancy = [128, 1], constant_value = <none>\n   %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 1]\n+  // CHECK-NEXT: contiguity = [1, 1], divisibility = [16, 16], constancy = [1, 1], constant_value = <none>\n   %6 = tt.addptr %5, %4 : tensor<128x1x!tt.ptr<f32>>, tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [1, 1]\n+  // CHECK-NEXT: contiguity = [1, 128], divisibility = [1, 1073741824], constancy = [1, 1], constant_value = <none>\n   %7 = tt.expand_dims %1 {axis = 0 : i32}: (tensor<128xi32>) -> tensor<1x128xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128]\n+  // CHECK-NEXT: contiguity = [1, 1], divisibility = [16, 16], constancy = [1, 128], constant_value = <none>\n   %8 = tt.broadcast %6 : (tensor<128x1x!tt.ptr<f32>>) -> tensor<128x128x!tt.ptr<f32>>\n-  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [128, 1]\n+  // CHECK-NEXT: contiguity = [1, 128], divisibility = [1, 1073741824], constancy = [128, 1], constant_value = <none>\n   %9 = tt.broadcast %7 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n-  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 16] ; Constancy: [1, 1]\n+  // CHECK-NEXT: contiguity = [1, 128], divisibility = [1, 16], constancy = [1, 1], constant_value = <none>\n   %10 = tt.addptr %8, %9 : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n-  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [65536, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: contiguity = [128, 1], divisibility = [1073741824, 1], constancy = [1, 1], constant_value = <none>\n   %11 = tt.expand_dims %0 {axis = 1 : i32}: (tensor<128xi32>) -> tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n+  // CHECK-NEXT: contiguity = [1, 1], divisibility = [16, 16], constancy = [128, 1], constant_value = <none>\n   %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>>\n-  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: contiguity = [128, 1], divisibility = [16, 1], constancy = [1, 1], constant_value = <none>\n   %13 = tt.addptr %12, %11 : tensor<128x1x!tt.ptr<f32>>, tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [1, 1]\n+  // CHECK-NEXT: contiguity = [1, 128], divisibility = [1, 1073741824], constancy = [1, 1], constant_value = <none>\n   %14 = tt.expand_dims %1 {axis = 0 : i32} : (tensor<128xi32>) -> tensor<1x128xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128]\n+  // CHECK-NEXT: contiguity = [1, 1], divisibility = [16, 16], constancy = [1, 128], constant_value = <none>\n   %15 = tt.splat %arg3 : (i32) -> tensor<1x128xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 1048576] ; Constancy: [1, 1]\n+  // CHECK-NEXT: contiguity = [1, 1], divisibility = [16, 17179869184], constancy = [1, 1], constant_value = <none>\n   %16 = arith.muli %14, %15 : tensor<1x128xi32>\n-  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 128]\n+  // CHECK-NEXT: contiguity = [128, 1], divisibility = [16, 1], constancy = [1, 128], constant_value = <none>\n   %17 = tt.broadcast %13 : (tensor<128x1x!tt.ptr<f32>>) -> tensor<128x128x!tt.ptr<f32>>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 1048576] ; Constancy: [128, 1]\n+  // CHECK-NEXT: contiguity = [1, 1], divisibility = [16, 17179869184], constancy = [128, 1], constant_value = <none>\n   %18 = tt.broadcast %16 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n-  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: contiguity = [128, 1], divisibility = [16, 1], constancy = [1, 1], constant_value = <none>\n   %19 = tt.addptr %17, %18 : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: contiguity = [1, 1], divisibility = [1, 1], constancy = [1, 1], constant_value = <none>\n   %20 = tt.load %10, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf32>\n   tt.store %19, %20, %cst : tensor<128x128xf32>\n   return\n@@ -56,28 +347,29 @@ func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {t\n module {\n \n // This is a tiny test for verifying StoreOp-related alignment, It simply store a constant to a buffer.\n-func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n: i32 {tt.divisibility = 16 : i32}) {\n-  // CHECK: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1]\n+// CHECK-LABEL: @store_constant_align\n+func.func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n: i32 {tt.divisibility = 16 : i32}) {\n+  // CHECK: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n   %pid = tt.get_program_id {axis = 0 : i32} : i32\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [1]\n+  // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [1], constant_value = 128\n   %c128_i32 = arith.constant 128 : i32\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [1]\n+  // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [1], constant_value = <none>\n   %1 = arith.muli %pid, %c128_i32 : i32\n-  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [65536] ; Constancy: [1]\n+  // CHECK-NEXT: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n- // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128]\n+ // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [128], constant_value = <none>\n   %3 = tt.splat %1 : (i32) -> tensor<128xi32>\n- // CHECK-NEXT: Contiguity: [128] ; Divisibility: [128] ; Constancy: [1]\n+ // CHECK-NEXT: contiguity = [128], divisibility = [128], constancy = [1], constant_value = <none>\n   %4 = arith.addi %3, %2 : tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128]\n+  // CHECK-NEXT: contiguity = [1], divisibility = [16], constancy = [128], constant_value = <none>\n   %5 = tt.splat %addr : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n-  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [16] ; Constancy: [1]\n+  // CHECK-NEXT: contiguity = [128], divisibility = [16], constancy = [1], constant_value = <none>\n   %6 = tt.addptr %5, %4 : tensor<128x!tt.ptr<f32>>, tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128]\n+  // CHECK-NEXT: contiguity = [1], divisibility = [16], constancy = [128], constant_value = <none>\n   %9 = tt.splat %n : (i32) -> tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [16]\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [16], constant_value = <none>\n   %mask = arith.cmpi slt, %4, %9 : tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1]\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n   %cst = arith.constant dense<0.0> : tensor<128xf32>\n   tt.store %5, %cst, %mask : tensor<128xf32>\n   return\n@@ -89,7 +381,8 @@ func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n:\n \n // This IR is dumped from vecadd test.\n // Note, the hint {tt.divisibility = 16 : i32} for %n_elements affects the alignment of mask.\n-func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32 {tt.divisibility = 16 : i32}) {\n+// CHECK-LABEL: @vecadd_mask_align_16\n+func.func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32 {tt.divisibility = 16 : i32}) {\n   %c64_i32 = arith.constant 64 : i32\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n   %1 = arith.muli %0, %c64_i32 : i32\n@@ -101,13 +394,13 @@ func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %ar\n   %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n   %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   %9 = tt.splat %n_elements : (i32) -> tensor<64xi32>\n-  // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [16] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n+  // CHECK: arith.cmpi slt, %{{.*}} => contiguity = [1], divisibility = [1], constancy = [16], constant_value = <none>\n   %mask = arith.cmpi slt, %4, %9 : tensor<64xi32>\n   %11 = tt.load %6, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %12 = tt.load %8, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %13 = arith.addf %11, %12 : tensor<64xf32>\n   %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n-  // CHECK: Contiguity: [64] ; Divisibility: [16] ; Constancy: [1] ( %{{.*}} = tt.addptr %{{.*}}, %{{.*}} : tensor<64x!tt.ptr<f32>>, tensor<64xi32> )\n+  // CHECK: tt.addptr %{{.*}} => contiguity = [64], divisibility = [16], constancy = [1], constant_value = <none>\n   %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   tt.store %15, %13, %mask : tensor<64xf32>\n   return\n@@ -117,7 +410,8 @@ func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %ar\n \n // This IR is dumped from vecadd test.\n // Note, there is no divisibility hint for %n_elements, Triton should assume its divisibility to be 1 by default.\n-func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n+// CHECK-LABEL: @vecadd_mask_align_1\n+func.func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n   %c64_i32 = arith.constant 64 : i32\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n   %1 = arith.muli %0, %c64_i32 : i32\n@@ -129,7 +423,7 @@ func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg\n   %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n   %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   %9 = tt.splat %n_elements : (i32) -> tensor<64xi32>\n-  // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [1] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n+  // CHECK: arith.cmpi slt, %{{.*}} => contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n   %10 = arith.cmpi slt, %4, %9 : tensor<64xi32>\n   %11 = tt.load %6, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %12 = tt.load %8, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 17, "deletions": 17, "changes": 34, "file_content_changes": "@@ -13,7 +13,7 @@\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // CHECK-LABEL: matmul_loop\n-func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n \n@@ -46,7 +46,7 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n \n // Shared memory is available after a tensor's liveness range ends\n // CHECK-LABEL: reusable\n-func @reusable(%A : !tt.ptr<f16>) {\n+func.func @reusable(%A : !tt.ptr<f16>) {\n   %cst1 = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n   %cst3 = arith.constant dense<true> : tensor<32x128xi1, #AL>\n@@ -78,7 +78,7 @@ func @reusable(%A : !tt.ptr<f16>) {\n // %cst1->%cst4\n // %cst3->%g->%h->%i\n // CHECK-LABEL: preallocate\n-func @preallocate(%A : !tt.ptr<f16>) {\n+func.func @preallocate(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 512\n@@ -113,7 +113,7 @@ func @preallocate(%A : !tt.ptr<f16>) {\n \n // Unused tensors are immediately released\n // CHECK-LABEL: unused\n-func @unused(%A : !tt.ptr<f16>) {\n+func.func @unused(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 1024\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 0, size = 512\n@@ -128,7 +128,7 @@ func @unused(%A : !tt.ptr<f16>) {\n \n // cst0 is alive through the entire function, it cannot be released before the end of the function\n // CHECK-LABEL: longlive\n-func @longlive(%A : !tt.ptr<f16>) {\n+func.func @longlive(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n@@ -156,7 +156,7 @@ func @longlive(%A : !tt.ptr<f16>) {\n }\n \n // CHECK-LABEL: alloc\n-func @alloc(%A : !tt.ptr<f16>) {\n+func.func @alloc(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n@@ -167,7 +167,7 @@ func @alloc(%A : !tt.ptr<f16>) {\n }\n \n // CHECK-LABEL: scratch\n-func @scratch() {\n+func.func @scratch() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   // CHECK: scratch offset = 0, size = 512\n   %b = tt.reduce %cst0 {redOp = 1 : i32, axis = 0 : i32} : tensor<16x16xf16, #AL> -> tensor<16xf16, #sliceAd0>\n@@ -176,15 +176,15 @@ func @scratch() {\n }\n \n // CHECK-LABEL: trans\n-func @trans(%A : !tt.ptr<f16>) {\n+func.func @trans(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 1024\n   %tensor = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n   %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n   return\n }\n \n // CHECK-LABEL: insert_slice_async\n-func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n+func.func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n@@ -197,7 +197,7 @@ func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n }\n \n // CHECK-LABEL: extract_slice\n-func @extract_slice(%A : !tt.ptr<f16>) {\n+func.func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : index\n@@ -209,7 +209,7 @@ func @extract_slice(%A : !tt.ptr<f16>) {\n // B0 -> (B1) -> B0\n // Memory used by B1 can be reused by B0.\n // CHECK-LABEL: if\n-func @if(%i1 : i1) {\n+func.func @if(%i1 : i1) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n@@ -233,7 +233,7 @@ func @if(%i1 : i1) {\n // B0 -> (B1) -> (B2) -> B0\n // Memory used by B0 cannot be reused by B1 or B2.\n // CHECK-LABEL: if_else\n-func @if_else(%i1 : i1) {\n+func.func @if_else(%i1 : i1) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n@@ -260,7 +260,7 @@ func @if_else(%i1 : i1) {\n // Block arguments and yields are memory aliases that do not trigger a new\n // allocation.\n // CHECK-LABEL: for\n-func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 8192\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n@@ -275,7 +275,7 @@ func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.p\n }\n \n // CHECK-LABEL: for_if_slice\n-func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+func.func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: offset = 0, size = 8192\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n@@ -296,7 +296,7 @@ func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %\n \n // c0 cannot be released in the loop\n // CHECK-LABEL: for_use_ancestor\n-func @for_use_ancestor(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+func.func @for_use_ancestor(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: offset = 0, size = 8192\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n@@ -315,8 +315,8 @@ func @for_use_ancestor(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16\n \n // a_shared_init, b_shared_init, and c_shared_init's liveness ranges are span over the entire function before cst2.\n // So they cannot be reused by cst0 and cst1, but can be reused by cst2.\n-// CHECK-LABEL: for_if_for\n-func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+// CHECK-LABEL: for_for_if\n+func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: offset = 0, size = 8192\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 292, "deletions": 117, "changes": 409, "file_content_changes": "@@ -14,7 +14,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // CHECK-LABEL: matmul_loop\n // There shouldn't be any membar with the dot op encoding.\n-func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n \n@@ -42,148 +42,167 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n }\n \n // CHECK-LABEL: raw_single_block\n-func @raw_single_block(%A : !tt.ptr<f16>) {\n+func.func @raw_single_block(%A : !tt.ptr<f16>) {\n   %cst1 = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n-  %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n-  %a1_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-  %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n-  // CHECK: Membar 5\n-  %a2 = triton_gpu.convert_layout %a1 : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #A_SHARED>\n+  %0 = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n+  %1 = tt.load %0, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n+  %2 = triton_gpu.convert_layout %1 : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %3 = triton_gpu.convert_layout %2 : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: war_single_block\n-func @war_single_block(%A : !tt.ptr<f16>) {\n+func.func @war_single_block(%A : !tt.ptr<f16>) {\n   %cst1 = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n-  %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n-  %a1_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-  %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n-  // CHECK: Membar 5\n-  %a2 = triton_gpu.convert_layout %a1 : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL>\n-  // a2's liveness range ends here, and a3 and a2 have the same address range.\n-  // So it makes sense to have a WAR dependency between a2 and a3.\n-  // CHECK-NEXT: Membar 7\n-  %a3 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n+  %0 = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n+  %1 = tt.load %0, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n+  %2 = triton_gpu.convert_layout %1 : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %3 = triton_gpu.convert_layout %2 : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: %4 = triton_gpu.convert_layout\n+  %4 = triton_gpu.convert_layout %1 : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: scratch\n-func @scratch() {\n+func.func @scratch() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n-  // CHECK: Membar 1\n-  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-  // CHECK-NEXT: Membar 3\n-  %aa = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n-  %b = tt.reduce %aa {redOp = 1 : i32, axis = 0 : i32} : tensor<32x16xf16, #AL> -> tensor<16xf16, #sliceAd0>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %0 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %1 = triton_gpu.convert_layout %0 : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n+  %2 = tt.reduce %1 {redOp = 1 : i32, axis = 0 : i32} : tensor<32x16xf16, #AL> -> tensor<16xf16, #sliceAd0>\n   return\n }\n \n // CHECK-LABEL: async_wait\n-func @async_wait() {\n+func.func @async_wait() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n-  // CHECK: Membar 1\n-  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %0 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   triton_gpu.async_wait {num = 4 : i32}\n-  // CHECK-NEXT: Membar 4\n-  %a_ = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %1 = triton_gpu.convert_layout %0 : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n   return\n }\n \n // CHECK-LABEL: alloc\n-func @alloc() {\n-  %cst0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n-  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-  // CHECK: Membar 2\n-  %b = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n+func.func @alloc() {\n+  %0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n+  %1 = tt.cat %0, %0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %2 = triton_gpu.convert_layout %1 : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n   return\n }\n \n // CHECK-LABEL: extract_slice\n-func @extract_slice() {\n+func.func @extract_slice() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : index\n-  %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n-  // CHECK: Membar 3\n-  %cst2 = triton_gpu.convert_layout %cst1 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n-  // CHECK-NEXT: Membar 5\n-  %cst3 = triton_gpu.convert_layout %cst2 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A_SHARED>\n+  %0 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %1 = triton_gpu.convert_layout %0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %2 = triton_gpu.convert_layout %1 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: trans\n-func @trans() {\n+func.func @trans() {\n+  // CHECK-NOT: gpu.barrier\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n   %b = tt.trans %cst0 : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n   return\n }\n \n-// CHECK-LABEL: insert_slice_async\n-func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n+// CHECK-LABEL: insert_slice_async_op\n+func.func @insert_slice_async_op(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   %tensor = triton_gpu.alloc_tensor : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n-  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SHARED>\n-  // CHECK: Membar 6\n-  %b = tt.cat %a, %a {axis = 0} : (tensor<1x16x16xf16, #A_SHARED>, tensor<1x16x16xf16, #A_SHARED>) -> tensor<2x16x16xf16, #A_SHARED>\n-  // CHECK: Membar 8\n-  %c = tt.cat %b, %b {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n+  %3 = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %4 = tt.cat %3, %3 {axis = 0} : (tensor<1x16x16xf16, #A_SHARED>, tensor<1x16x16xf16, #A_SHARED>) -> tensor<2x16x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %5 = tt.cat %4, %4 {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n   return\n }\n \n-// CHECK-LABEL: insert_slice\n-func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n+// CHECK-LABEL: insert_slice_op\n+func.func @insert_slice_op(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : index\n-  %al = tt.load %a_ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #AL>\n-  // CHECK: Membar 6\n-  %a = tensor.insert_slice %al into %tensor[%index, 0, 0][1, 16, 16][1, 1, 1]: tensor<16x16xf16, #AL> into tensor<1x16x16xf16, #A_SHARED>\n-  // CHECK: Membar 8\n-  %b = tt.cat %a, %a {axis = 0} : (tensor<1x16x16xf16, #A_SHARED>, tensor<1x16x16xf16, #A_SHARED>) -> tensor<2x16x16xf16, #A_SHARED>\n-  // CHECK: Membar 10\n-  %c = tt.cat %b, %b {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n+  %2 = tt.load %a_ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #AL>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tensor.insert_slice\n+  %3 = tensor.insert_slice %2 into %tensor[%index, 0, 0][1, 16, 16][1, 1, 1]: tensor<16x16xf16, #AL> into tensor<1x16x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %4 = tt.cat %3, %3 {axis = 0} : (tensor<1x16x16xf16, #A_SHARED>, tensor<1x16x16xf16, #A_SHARED>) -> tensor<2x16x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %5 = tt.cat %4, %4 {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n   return\n }\n \n // If branch inserted a barrier for %cst0 and %cst1, but else didn't, then the barrier should be inserted in the parent region\n // CHECK-LABEL: multi_blocks\n-func @multi_blocks(%i1 : i1) {\n+func.func @multi_blocks(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n-    // CHECK: Membar 2\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %0 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   } else {\n     %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n     %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n-    // CHECK-NEXT: Membar 7\n-    %b = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %1 = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   }\n-  // CHECK-NEXT: Membar 10\n-  %c = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %2 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   return\n }\n \n // Both branches inserted a barrier for %cst0 and %cst1, then the barrier doesn't need to be inserted in the parent region\n // CHECK-LABEL: multi_blocks_join_barrier\n-func @multi_blocks_join_barrier(%i1 : i1) {\n+func.func @multi_blocks_join_barrier(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n-    // CHECK: Membar 2\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %0 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   } else {\n-    // CHECK-NEXT: Membar 5\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %1 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   }\n   %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n@@ -192,68 +211,100 @@ func @multi_blocks_join_barrier(%i1 : i1) {\n \n // Read yielded tensor requires a barrier\n // CHECK-LABEL: multi_blocks_yield\n-func @multi_blocks_yield(%i1 : i1) {\n+func.func @multi_blocks_yield(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %a = scf.if %i1 -> (tensor<32x16xf16, #A_SHARED>) {\n-    // CHECK: Membar 2\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-    scf.yield %a : tensor<32x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %0 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    scf.yield %0 : tensor<32x16xf16, #A_SHARED>\n   } else {\n-    // CHECK-NEXT: Membar 5\n-    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-    scf.yield %b : tensor<32x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %1 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    scf.yield %1 : tensor<32x16xf16, #A_SHARED>\n   }\n   %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n-  // CHECK-NEXT: Membar 9\n-  %b = tt.cat %a, %a {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %4 = tt.cat %a, %a {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n+  return\n+}\n+\n+// Even though the entry block doesn't have a barrier, the successors should have barriers\n+// CHECK-LABEL: multi_blocks_entry_no_shared\n+func.func @multi_blocks_entry_no_shared(%i1 : i1) {\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n+  %a = scf.if %i1 -> (tensor<32x16xf16, #A_SHARED>) {\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+    %0 = tt.cat %cst1, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    scf.yield %0 : tensor<32x16xf16, #A_SHARED>\n+  } else {\n+    // CHECK-NOT: gpu.barrier\n+    // CHECK: arith.constant\n+    %cst1 = arith.constant dense<0.000000e+00> : tensor<32x16xf16, #A_SHARED>\n+    scf.yield %cst1 : tensor<32x16xf16, #A_SHARED>\n+  }\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %1 = tt.cat %a, %a {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n   return\n }\n \n // Conservatively add a barrier as if the branch (%i1) is never taken\n // CHECK-LABEL: multi_blocks_noelse\n-func @multi_blocks_noelse(%i1 : i1) {\n+func.func @multi_blocks_noelse(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n-    // CHECK: Membar 2\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %0 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   }\n-  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %1 = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n   return\n }\n \n // Conservatively add a barrier as if the branch (%i2) is never taken\n // CHECK-LABEL: multi_blocks_nested_scf\n-func @multi_blocks_nested_scf(%i1 : i1, %i2 : i1) {\n+func.func @multi_blocks_nested_scf(%i1 : i1, %i2 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n     scf.if %i2 {\n-      // CHECK: Membar 2\n-      %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+      // CHECK: gpu.barrier\n+      // CHECK-NEXT: tt.cat\n+      %0 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n       scf.yield\n     }\n     scf.yield\n   } else {\n-    // CHECK-NEXT: Membar 6\n-    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %1 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   }\n-  // CHECK-NEXT: Membar 9\n-  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %2 = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n   return\n }\n \n // CHECK-LABEL: for\n-func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n-    // CHECK-NEXT: Membar 3\n-    %cst0 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %5 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   return\n@@ -262,64 +313,188 @@ func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.p\n // Although a_shared and b_shared are synced before entering the loop,\n // they are reassociated with aliases (c_shared) and thus require a barrier.\n // CHECK-LABEL: for_alias\n-func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+func.func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n-  // CHECK-NEXT: Membar 2\n-  %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #AL>\n-    // CHECK-NEXT: Membar 6\n-    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #AL>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %7 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #AL>\n     scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  // CHECK-NEXT: Membar 9\n-  %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %9 = tt.cat %0, %0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n   return\n }\n \n // Although cst2 is not an argument of scf.yield, its memory is reused by cst1.\n // So we need a barrier both before and after cst1\n // CHECK-LABEL: for_reuse\n-func @for_reuse(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+func.func @for_reuse(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n-  // CHECK-NEXT: Membar 2\n-  %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n-    // CHECK-NEXT: Membar 5\n-    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n-    // CHECK-NEXT: Membar 7\n-    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %6 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %7 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n     scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  // CHECK-NEXT: Membar 10\n-  %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %9 = tt.cat %0, %0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n   return\n }\n \n-\n // CHECK-LABEL: for_reuse_nested\n-func @for_reuse_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+func.func @for_reuse_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n-  // CHECK-NEXT: Membar 2\n-  %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n-    // CHECK-NEXT: Membar 5\n-    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %6 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n     %a_shared_next, %b_shared_next, %c_shared_next = scf.for %ivv = %lb to %ub step %step iter_args(%a_shared_nested = %a_shared_init, %b_shared_nested = %b_shared_init, %c_shared_nested = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n-      // CHECK-NEXT: Membar 7\n-      %cst2 = tt.cat %a_shared_nested, %b_shared_nested {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+      // CHECK: gpu.barrier\n+      // CHECK-NEXT:  tt.cat\n+      %12 = tt.cat %a_shared_nested, %b_shared_nested {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n       scf.yield %c_shared_nested, %a_shared_nested, %b_shared_nested : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n     }\n     scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  // CHECK-NEXT: Membar 11\n-  %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT:  tt.cat\n+  %15 = tt.cat %0, %0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n+  return\n+}\n+\n+// repeatedly write to the same shared memory addresses\n+// CHECK-LABEL: for_for_if\n+func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    %c_shared_next = scf.for %jv = %lb to %ub step %step iter_args(%c_shared_next = %c_shared) -> (tensor<128x32xf16, #A_SHARED>) {\n+      %c_shared_next_next = scf.if %i1 -> tensor<128x32xf16, #A_SHARED> {\n+        // CHECK: gpu.barrier\n+        // CHECK-NEXT: arith.constant\n+        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+        scf.yield %cst0 : tensor<128x32xf16, #A_SHARED>\n+      } else {\n+        // CHECK: gpu.barrier\n+        // CHECK-NEXT: arith.constant\n+        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+        scf.yield %cst0 : tensor<128x32xf16, #A_SHARED>\n+      }\n+      scf.yield %c_shared_next_next : tensor<128x32xf16, #A_SHARED>\n+    }\n+    scf.yield %a_shared, %b_shared, %c_shared_next : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+  }\n+  return\n+}\n+\n+// c_block_next can either be converted from c_shared_init or c_shared_next_next\n+// CHECK-LABEL: for_if_for\n+func.func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  %c_blocked = triton_gpu.convert_layout %c_shared_init : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL> \n+\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    %c_shared_next_next = scf.if %i1 -> tensor<128x32xf16, #A_SHARED> {\n+      // CHECK: gpu.barrier\n+      // CHECK-NEXT: arith.constant\n+      %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+      scf.yield %cst0 : tensor<128x32xf16, #A_SHARED>\n+    } else {\n+      %c_shared_ = scf.for %jv = %lb to %ub step %step iter_args(%c_shared_next = %c_shared) -> (tensor<128x32xf16, #A_SHARED>) {\n+        // CHECK: gpu.barrier\n+        // CHECK-NEXT: triton_gpu.convert_layout\n+        %c_blocked_next = triton_gpu.convert_layout %c_shared_next : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL> \n+        scf.yield %c_shared : tensor<128x32xf16, #A_SHARED>\n+      }\n+      scf.yield %c_shared_ : tensor<128x32xf16, #A_SHARED>\n+    }\n+    // CHECK-NOT: gpu.barrier\n+    %b_blocked_next = triton_gpu.convert_layout %b_shared: (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL> \n+    scf.yield %a_shared, %b_shared, %c_shared_next_next : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+  }\n+  return\n+}\n+\n+// CHECK-LABEL: cf_if\n+func.func @cf_if(%i1 : i1) {\n+  %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  %cst_0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  cf.cond_br %i1, ^bb1, ^bb2\n+^bb1:  // pred: ^bb0\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %0 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  cf.br ^bb2\n+^bb2:  // 2 preds: ^bb0, ^bb1\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %1 = triton_gpu.convert_layout %cst : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<16x16xf16, #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>>\n+  return\n+}\n+\n+func.func @cf_if_else(%i1 : i1) {\n+  %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  %cst_0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  cf.cond_br %i1, ^bb1, ^bb2\n+^bb1:  // pred: ^bb0\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %0 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  cf.br ^bb3(%0 : tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>)\n+^bb2:  // pred: ^bb0\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %1 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  cf.br ^bb3(%1 : tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>)\n+^bb3(%2: tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>):  // 2 preds: ^bb1, ^bb2\n+  cf.br ^bb4\n+^bb4:  // pred: ^bb3\n+  %3 = triton_gpu.convert_layout %cst : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<16x16xf16, #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %4 = tt.cat %2, %2 {axis = 0 : i64} : (tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<64x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  return\n+}\n+\n+func.func @cf_if_else_return(%i1 : i1) {\n+  %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  %cst_0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  cf.cond_br %i1, ^bb1, ^bb2\n+^bb1:  // pred: ^bb0\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %0 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  return\n+^bb2:  // pred: ^bb0\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %1 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n   return\n }\n "}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -1,6 +1,6 @@\n // RUN: triton-opt %s | FileCheck %s\n \n-func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n+func.func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n   // scalar -> scalar\n   // CHECK:  i64 -> !tt.ptr<f32>\n   %0 = tt.int_to_ptr %scalar_i64 : i64 -> !tt.ptr<f32>\n@@ -35,7 +35,7 @@ func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n   return\n }\n \n-func @addptr_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_i32: i32) {\n+func.func @addptr_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_i32: i32) {\n   // scalar -> scalar\n   // CHECK: !tt.ptr<f32>\n   %0 = tt.addptr %scalar_ptr, %scalar_i32 : !tt.ptr<f32>, i32\n@@ -54,7 +54,7 @@ func @addptr_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_i32: i32) {\n   return\n }\n \n-func @load_store_ops_scalar(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %mask : i1) {\n+func.func @load_store_ops_scalar(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %mask : i1) {\n   // Test if Load/Store ops can handle scalar values\n   %other = arith.constant 0.0e+0 : f32\n \n@@ -67,16 +67,16 @@ func @load_store_ops_scalar(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %ma\n   %c = tt.load %ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = true} : f32\n \n   // store scalar\n-  // CHECK: tt.store %{{.*}}, %[[L0]] : f32\n+  // CHECK: tt.store %{{.*}}, %[[L0]] {cache = 1 : i32, evict = 1 : i32} : f32\n   tt.store %ptr, %a : f32\n-  // CHECK: tt.store %{{.*}}, %[[L1]], %{{.*}} : f32\n+  // CHECK: tt.store %{{.*}}, %[[L1]], %{{.*}} {cache = 1 : i32, evict = 1 : i32} : f32\n   tt.store %ptr, %b, %mask : f32\n-  // CHECK: tt.store %{{.*}}, %[[L2]], %{{.*}} : f32\n+  // CHECK: tt.store %{{.*}}, %[[L2]], %{{.*}} {cache = 1 : i32, evict = 1 : i32} : f32\n   tt.store %ptr, %c, %mask : f32\n   return\n }\n \n-func @reduce_ops_infer(%ptr: !tt.ptr<f32>, %v : tensor<1x2x4xf32>) {\n+func.func @reduce_ops_infer(%ptr: !tt.ptr<f32>, %v : tensor<1x2x4xf32>) {\n   // Test if reduce ops infer types correctly\n \n   // CHECK: %{{.*}} = tt.reduce %{{.*}} -> tensor<2x4xf32>\n@@ -101,7 +101,7 @@ func @reduce_ops_infer(%ptr: !tt.ptr<f32>, %v : tensor<1x2x4xf32>) {\n   return\n }\n \n-func @dot_ops_infer(%ptr: !tt.ptr<f32>, %v : f32) {\n+func.func @dot_ops_infer(%ptr: !tt.ptr<f32>, %v : f32) {\n   // Test if reduce ops infer types correctly\n   %v128x32 = tt.splat %v : (f32) -> tensor<128x32xf32>\n   %v32x128 = tt.splat %v : (f32) -> tensor<32x128xf32>"}, {"filename": "test/Conversion/triton_to_tritongpu.mlir", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -1,6 +1,6 @@\n // RUN: triton-opt %s -split-input-file -convert-triton-to-tritongpu=num-warps=2 | FileCheck %s\n \n-func @ops() {\n+func.func @ops() {\n   // CHECK: module attributes {\"triton_gpu.num-warps\" = 2 : i32} {{.*}}\n   %a = arith.constant dense<1.00e+00> : tensor<128x32xf16>\n   %b = arith.constant dense<2.00e+00> : tensor<32x128xf16>\n@@ -11,7 +11,7 @@ func @ops() {\n \n // -----\n \n-func @load_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n+func.func @load_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   // Test if LoadOp is lowered properly (see #771)\n   %ptrs = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n   %mask = arith.constant dense<true> : tensor<128xi1>\n@@ -30,23 +30,23 @@ func @load_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n \n // -----\n \n-func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n+func.func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   // Test if the total number of threadsPerWarp is 32\n   // Test if the total number of warps is 2\n-  // CHECK: #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 2], order = [0, 1]}>\n-  // CHECK: #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 2], order = [0, 1]}>\n-  // CHECK: #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [16, 2], warpsPerCTA = [1, 2], order = [0, 1]}>\n+  // CHECK: #[[blocked0:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 2], order = [0, 1]}>\n+  // CHECK: #[[blocked1:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 2], order = [0, 1]}>\n+  // CHECK: #[[blocked2:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [16, 2], warpsPerCTA = [1, 2], order = [0, 1]}>\n   // CHECK: module attributes {\"triton_gpu.num-warps\" = 2 : i32} {{.*}}\n   %c0 = arith.constant dense<1.00e+00> : tensor<4x4xf32>\n   %c1 = arith.constant dense<2.00e+00> : tensor<8x2xf32>\n   %c2 = arith.constant dense<3.00e+00> : tensor<16x16xf32>\n-  // CHECK: tensor<4x4xf32, #blocked0> -> tensor<4xf32, #triton_gpu.slice<{dim = 0, parent = #blocked0}>>\n+  // CHECK: tensor<4x4xf32, #[[blocked0]]> -> tensor<4xf32, #triton_gpu.slice<{dim = 0, parent = #[[blocked0]]}>>\n   %c0_ = tt.reduce %c0 {redOp = 1 : i32, axis = 0 : i32} : tensor<4x4xf32> -> tensor<4xf32>\n-  // CHECK: tensor<8x2xf32, #blocked1> -> tensor<2xf32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>\n+  // CHECK: tensor<8x2xf32, #[[blocked1]]> -> tensor<2xf32, #triton_gpu.slice<{dim = 0, parent = #[[blocked1]]}>\n   %c1_ = tt.reduce %c1 {redOp = 1 : i32, axis = 0 : i32} : tensor<8x2xf32> -> tensor<2xf32>\n-  // CHECK: tensor<8x2xf32, #blocked1> -> tensor<8xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  // CHECK: tensor<8x2xf32, #[[blocked1]]> -> tensor<8xf32, #triton_gpu.slice<{dim = 1, parent = #[[blocked1]]}>>\n   %c2_ = tt.reduce %c1 {redOp = 1 : i32, axis = 1 : i32} : tensor<8x2xf32> -> tensor<8xf32>\n-  // CHECK: tensor<16x16xf32, #blocked2> -> tensor<16xf32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>\n+  // CHECK: tensor<16x16xf32, #[[blocked2]]> -> tensor<16xf32, #triton_gpu.slice<{dim = 0, parent = #[[blocked2]]}>>\n   %c3_ = tt.reduce %c2 {redOp = 1 : i32, axis = 0 : i32} : tensor<16x16xf32> -> tensor<16xf32>\n \n   return"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 116, "deletions": 69, "changes": 185, "file_content_changes": "@@ -3,8 +3,8 @@\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: llvm.func @test_empty_kernel(%arg0: i32, %arg1: !llvm.ptr<f16, 1>)\n   // Here the 128 comes from the 4 in module attribute multiples 32\n-  // CHECK:  attributes {nvvm.kernel = 1 : ui1, nvvm.maxntid = 128 : i32} {{.*}}\n-  func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n+  // CHECK:  attributes {nvvm.kernel = 1 : ui1, nvvm.maxntid = [128 : i32]} {{.*}}\n+  func.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n     // CHECK:  llvm.return\n     return\n   }\n@@ -15,7 +15,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_load\n-  func @basic_load(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf32, #blocked0>) {\n+  func.func @basic_load(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf32, #blocked0>) {\n     // CHECK: llvm.inline_asm\n     // CHECK: llvm.inline_asm\n     %1 = tt.load %a_ptr_init, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n@@ -28,7 +28,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: vectorized_load\n-  func @vectorized_load(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf32, #blocked0>) {\n+  func.func @vectorized_load(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf32, #blocked0>) {\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ld.global.b32\n     // CHECK: llvm.inline_asm\n@@ -43,7 +43,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: vectorized_load_f16\n-  func @vectorized_load_f16(%a_ptr_init: tensor<256x!tt.ptr<f16>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf16, #blocked0>) {\n+  func.func @vectorized_load_f16(%a_ptr_init: tensor<256x!tt.ptr<f16>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf16, #blocked0>) {\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ld.global.b16\n     // CHECK: llvm.inline_asm\n@@ -59,7 +59,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: masked_load_const_other\n-  func @masked_load_const_other(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>) {\n+  func.func @masked_load_const_other(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>) {\n     %cst_0 = arith.constant dense<0.000000e+00> : tensor<256xf32, #blocked0>\n     %1 = tt.load %a_ptr_init, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n     return\n@@ -72,7 +72,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: masked_load_const_other_vec\n-  func @masked_load_const_other_vec(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>) {\n+  func.func @masked_load_const_other_vec(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>) {\n     %cst_0 = arith.constant dense<0.000000e+00> : tensor<256xf32, #blocked0>\n     %1 = tt.load %a_ptr_init, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n     return\n@@ -84,7 +84,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   // CHECK-LABEL: global_load_store_no_vec\n-  func @global_load_store_no_vec(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+  func.func @global_load_store_no_vec(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -128,7 +128,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   // CHECK-LABEL: global_load_store_vec4\n-  func @global_load_store_vec4(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+  func.func @global_load_store_vec4(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -165,7 +165,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n #blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n // Note, the %n_elements doesn't have a \"tt.divisibility\" hint, so Triton assumes it's divisibility is 1, this should effect the mask's alignment and further restrict the load/store ops' vector width to be 1.\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n-  func @vecadd_masked_vec1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n+  func.func @vecadd_masked_vec1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n     %c64_i32 = arith.constant 64 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c64_i32 : i32\n@@ -192,10 +192,55 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n \n // -----\n \n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: global_load_store_vec2\n+    func.func @global_load_store_vec2(%arg0: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg3: i32) {\n+    %c256_i32 = arith.constant 256 : i32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %c256_i32 : i32\n+    %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n+    %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n+    %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n+    %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n+\n+    // Load 8 elements from A with four vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    // Load 8 elements from B with four vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    %9 = tt.load %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n+    %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n+\n+    // Store 8 elements to global with four vectorized store instruction\n+    // CHECK: @${{.*}} st.global.v2.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}} };\n+    // CHECK: @${{.*}} st.global.v2.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}} };\n+    // CHECK: @${{.*}} st.global.v2.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}} };\n+    // CHECK: @${{.*}} st.global.v2.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}} };\n+    tt.store %13, %11 : tensor<256xf32, #blocked0>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: global_load_store_vec8\n-    func @global_load_store_vec8(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+    func.func @global_load_store_vec8(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -238,7 +283,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_view_broadcast\n-  func @basic_view_broadcast(%arg : tensor<256xf32,#blocked0>) {\n+  func.func @basic_view_broadcast(%arg : tensor<256xf32,#blocked0>) {\n     // CHECK: llvm.mlir.undef\n     // CHECK: %[[T0:.*]] = llvm.extractvalue\n     // CHECK: %[[T1:.*]] = llvm.extractvalue\n@@ -262,7 +307,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_make_range\n-  func @basic_make_range() {\n+  func.func @basic_make_range() {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n     // CHECK: llvm.mlir.undef\n     // CHECK: llvm.insertvalue\n@@ -277,7 +322,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_addf\n-  func @basic_addf(%arg0 : tensor<256xf32,#blocked0>, %arg1 : tensor<256xf32,#blocked0>) {\n+  func.func @basic_addf(%arg0 : tensor<256xf32,#blocked0>, %arg1 : tensor<256xf32,#blocked0>) {\n     // CHECK: llvm.fadd\n     // CHECK: llvm.fadd\n     %1 = arith.addf %arg0, %arg1 : tensor<256xf32,#blocked0>\n@@ -290,7 +335,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_addi\n-  func @basic_addi(%arg0 : tensor<256xi32,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n+  func.func @basic_addi(%arg0 : tensor<256xi32,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n     // CHECK: llvm.add\n     // CHECK: llvm.add\n     %1 = arith.addi %arg0, %arg1 : tensor<256xi32,#blocked0>\n@@ -302,7 +347,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_program_id\n-  func @basic_program_id() {\n+  func.func @basic_program_id() {\n     // CHECK: nvvm.read.ptx.sreg.ctaid.x : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     return\n@@ -314,7 +359,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_addptr\n-  func @basic_addptr(%arg0 : tensor<256x!tt.ptr<f32>,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n+  func.func @basic_addptr(%arg0 : tensor<256x!tt.ptr<f32>,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n     // CHECK: llvm.getelementptr\n     // CHECK: llvm.getelementptr\n     %0 = tt.addptr %arg0, %arg1 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n@@ -328,7 +373,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem\n   // CHECK-LABEL: basic_alloc_tensor\n-  func @basic_alloc_tensor() {\n+  func.func @basic_alloc_tensor() {\n     // CHECK: llvm.mlir.addressof @global_smem\n     // CHECK-NEXT: llvm.bitcast\n     // CHECK-NEXT: llvm.mlir.constant\n@@ -345,7 +390,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem\n   // CHECK-LABEL: basic_extract_slice\n-  func @basic_extract_slice() {\n+  func.func @basic_extract_slice() {\n     // CHECK: llvm.mlir.addressof @global_smem\n     // CHECK: llvm.extractvalue\n     // CHECK-NEXT: llvm.extractvalue\n@@ -378,7 +423,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_async_wait\n-  func @basic_async_wait() {\n+  func.func @basic_async_wait() {\n     // CHECK: cp.async.wait_group 0x4\n     triton_gpu.async_wait {num = 4: i32}\n     return\n@@ -397,7 +442,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #A = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_fallback\n-  func @basic_insert_slice_async_fallback(%arg0: !tt.ptr<f16> {tt.divisibility = 1 : i32}) {\n+  func.func @basic_insert_slice_async_fallback(%arg0: !tt.ptr<f16> {tt.divisibility = 1 : i32}) {\n     %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<64xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #slice2d1>) -> tensor<16x1xi32, #block2>\n@@ -436,7 +481,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #A = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_v4\n-  func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 8 : i32}) {\n+  func.func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 32 : i32}) {\n     %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<64xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #slice2d1>) -> tensor<16x1xi32, #block2>\n@@ -461,6 +506,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x64x!tt.ptr<f32>, #AL> -> tensor<2x16x64xf32, #A>\n+    triton_gpu.async_commit_group\n     return\n   }\n }\n@@ -477,7 +523,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #A = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_v1\n-  func @basic_insert_slice_async_v1(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n+  func.func @basic_insert_slice_async_v1(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n     %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #slice2d1>) -> tensor<16x1xi32, #block2>\n@@ -506,6 +552,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x32x!tt.ptr<f32>, #AL> -> tensor<2x16x32xf32, #A>\n+    triton_gpu.async_commit_group\n     return\n   }\n }\n@@ -521,7 +568,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #A = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_v1_multictas\n-  func @basic_insert_slice_async_v1_multictas(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n+  func.func @basic_insert_slice_async_v1_multictas(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n     %off0_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<32xi32, #slice2d1>) -> tensor<32x1xi32, #block2>\n@@ -562,6 +609,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32x!tt.ptr<f32>, #AL> -> tensor<2x32x32xf32, #A>\n+    triton_gpu.async_commit_group\n     return\n   }\n }\n@@ -571,7 +619,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: basic_splat\n-  func @basic_splat(%ptr: !tt.ptr<f32>) {\n+  func.func @basic_splat(%ptr: !tt.ptr<f32>) {\n     // CHECK: llvm.mlir.undef\n     // CHECK: llvm.insertvalue\n     // CHECK: llvm.insertvalue\n@@ -585,7 +633,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_store\n-  func @basic_store(%ptrs: tensor<256x!tt.ptr<f32>, #blocked0>, %vals: tensor<256xf32, #blocked0>, %mask: tensor<256xi1, #blocked0>) {\n+  func.func @basic_store(%ptrs: tensor<256x!tt.ptr<f32>, #blocked0>, %vals: tensor<256xf32, #blocked0>, %mask: tensor<256xi1, #blocked0>) {\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     // CHECK: llvm.inline_asm\n@@ -602,7 +650,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked\n-  func @convert_layout_blocked_blocked(%arg0: tensor<16x16xf32, #blocked0>) {\n+  func.func @convert_layout_blocked_blocked(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n@@ -649,7 +697,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked_vec\n-  func @convert_layout_blocked_blocked_vec(%arg0: tensor<16x16xf32, #blocked0>) {\n+  func.func @convert_layout_blocked_blocked_vec(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n@@ -672,7 +720,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked_multi_rep\n-  func @convert_layout_blocked_blocked_multi_rep(%arg0: tensor<16x16xf32, #blocked0>) {\n+  func.func @convert_layout_blocked_blocked_multi_rep(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n@@ -703,7 +751,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_dot\n-  func @convert_dot(%A: tensor<16x16xf16, #blocked0>, %B: tensor<16x16xf16, #blocked0>) {\n+  func.func @convert_dot(%A: tensor<16x16xf16, #blocked0>, %B: tensor<16x16xf16, #blocked0>) {\n     %AA = triton_gpu.convert_layout %A : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #shared0>\n     %BB = triton_gpu.convert_layout %B : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #shared0>\n     // CHECK: llvm.inline_asm\n@@ -727,7 +775,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n // TODO: problems in MLIR's parser on slice layout\n // #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n // module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-//   func @make_range_sliced_layout() {\n+//   func.func @make_range_sliced_layout() {\n //     %0 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked0}>>\n //     return\n //   }\n@@ -740,7 +788,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mmav2_block\n-  func @convert_layout_mmav2_blocked(%arg0: tensor<32x16xf32, #mma>) {\n+  func.func @convert_layout_mmav2_blocked(%arg0: tensor<32x16xf32, #mma>) {\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n     // CHECK: llvm.store\n@@ -755,12 +803,12 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n \n // -----\n \n-#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [1, 0]}>\n-#mma = #triton_gpu.mma<{versionMajor = 1, warpsPerCTA = [2, 1]}>\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#mma = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [2, 2]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mmav1_block\n-  func @convert_layout_mmav1_blocked(%arg0: tensor<32x16xf32, #mma>) {\n+  func.func @convert_layout_mmav1_blocked(%arg0: tensor<32x64xf32, #mma>) {\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n     // CHECK: llvm.store\n@@ -772,19 +820,18 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: nvvm.barrier0\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n-    %0 = triton_gpu.convert_layout %arg0 : (tensor<32x16xf32, #mma>) -> tensor<32x16xf32, #blocked0>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<32x64xf32, #mma>) -> tensor<32x64xf32, #blocked>\n     return\n   }\n }\n \n // -----\n-\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n #shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_shared\n-  func @convert_layout_blocked_shared(%arg0: tensor<128x32xf32, #blocked0>) {\n+  func.func @convert_layout_blocked_shared(%arg0: tensor<128x32xf32, #blocked0>) {\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<8xf32>, 3>\n     // CHECK: llvm.store\n@@ -800,7 +847,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_blocked1d_to_slice0\n-  func @convert_blocked1d_to_slice0(%src:tensor<32xi32, #blocked0>) {\n+  func.func @convert_blocked1d_to_slice0(%src:tensor<32xi32, #blocked0>) {\n     // CHECK-COUNT-4: llvm.load {{.*}} : !llvm.ptr<vector<1xi32>, 3>\n     %cvt = triton_gpu.convert_layout %src : (tensor<32xi32, #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>\n     return\n@@ -813,7 +860,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_blocked1d_to_slice1\n-  func @convert_blocked1d_to_slice1(%src:tensor<32xi32, #blocked0>) {\n+  func.func @convert_blocked1d_to_slice1(%src:tensor<32xi32, #blocked0>) {\n     // CHECK-COUNT-32: llvm.load {{.*}} : !llvm.ptr<vector<1xi32>, 3>\n     %cvt = triton_gpu.convert_layout %src : (tensor<32xi32, #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n     return\n@@ -826,7 +873,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_blocked_to_blocked_ptr\n-  func @convert_blocked_to_blocked_ptr(%src:tensor<32x!tt.ptr<f32>, #blocked0>) {\n+  func.func @convert_blocked_to_blocked_ptr(%src:tensor<32x!tt.ptr<f32>, #blocked0>) {\n     // CHECK: llvm.ptrtoint\n     // CHECK: llvm.store\n     // CHECK: nvvm.barrier0\n@@ -845,7 +892,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func @matmul_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  func.func @matmul_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<128x32xf16, #shared>, %b:tensor<32x256xf16, #shared>) {\n     %cst = arith.constant dense<0.000000e+00> : tensor<128x256xf32, #mma>\n     // CHECK: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n@@ -865,24 +912,24 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n // -----\n \n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n-#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n-#mma = #triton_gpu.mma<{versionMajor = 1, warpsPerCTA = [2, 2]}>\n+#shared0 = #triton_gpu.shared<{vec = 4, perPhase = 1, maxPhase = 8, order = [1, 0]}>\n+#shared1 = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n+#mma = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [2, 2]}>\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma, isMMAv1Row=true}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma, isMMAv1Row=true}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func @matmul884_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n-  %a:tensor<128x32xf16, #shared>, %b:tensor<32x256xf16, #shared>) {\n-    %cst = arith.constant dense<0.000000e+00> : tensor<128x256xf32, #mma>\n+  func.func @matmul884_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  %a:tensor<32x64xf16, #shared0>, %b:tensor<64x64xf16, #shared1>) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x64xf32, #mma>\n     // CHECK: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    %a_mat = triton_gpu.convert_layout %a : (tensor<128x32xf16, #shared>) -> tensor<128x32xf16, #dot_operand_a>\n-    %b_mat = triton_gpu.convert_layout %b : (tensor<32x256xf16, #shared>) -> tensor<32x256xf16, #dot_operand_b>\n+    %a_mat = triton_gpu.convert_layout %a : (tensor<32x64xf16, #shared0>) -> tensor<32x64xf16, #dot_operand_a>\n+    %b_mat = triton_gpu.convert_layout %b : (tensor<64x64xf16, #shared1>) -> tensor<64x64xf16, #dot_operand_b>\n \n-    %28 = tt.dot %a_mat, %b_mat, %cst {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #dot_operand_a> * tensor<32x256xf16, #dot_operand_b> -> tensor<128x256xf32, #mma>\n-    // TODO[goostavz]: uncomment the following lines after convert_layout[mma<v1> -> blocked] is ready.\n-    // %38 = triton_gpu.convert_layout %28 : (tensor<128x256xf32, #mma>) -> tensor<128x256xf32, #blocked>\n-    // %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #blocked>\n-    // %36 = tt.broadcast %30 : (tensor<128x1x!tt.ptr<f32>, #blocked>) -> tensor<128x256x!tt.ptr<f32>, #blocked>\n-    // tt.store %36, %38 : tensor<128x256xf32, #blocked>\n+    %28 = tt.dot %a_mat, %b_mat, %cst {allowTF32 = true, transA = false, transB = false} : tensor<32x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<32x64xf32, #mma>\n+    %38 = triton_gpu.convert_layout %28 : (tensor<32x64xf32, #mma>) -> tensor<32x64xf32, #blocked>\n+    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n+    %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x64x!tt.ptr<f32>, #blocked>\n+    tt.store %36, %38 : tensor<32x64xf32, #blocked>\n     return\n   }\n }\n@@ -894,7 +941,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#blocked}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#blocked}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func @matmul_fmadot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  func.func @matmul_fmadot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #blocked>\n     // CHECK: llvm.intr.fmuladd\n@@ -918,15 +965,15 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: matmul_tf32dot\n-  func @matmul_tf32dot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  func.func @matmul_tf32dot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    // CHECK-SAME: (f32, f32, f32, f32)\n+    // CHECK-SAME: (vector<1xf32>, vector<1xf32>, vector<1xf32>, vector<1xf32>)\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    // CHECK-SAME: (f32, f32, f32, f32)\n+    // CHECK-SAME: (vector<1xf32>, vector<1xf32>, vector<1xf32>, vector<1xf32>)\n     %a_mat = triton_gpu.convert_layout %a : (tensor<32x16xf32, #shared>) -> tensor<32x16xf32, #dot_operand_a>\n     %b_mat = triton_gpu.convert_layout %b : (tensor<16x32xf32, #shared>) -> tensor<16x32xf32, #dot_operand_b>\n \n@@ -953,7 +1000,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32\n-  func @atomic_add_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xi1, #blocked0>, %arg2 : tensor<256xf32, #blocked0>) {\n+  func.func @atomic_add_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xi1, #blocked0>, %arg2 : tensor<256xf32, #blocked0>) {\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: atom.global.gpu.add.f32\n     %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xf32, #blocked0>, tensor<256xi1, #blocked0>) -> tensor<256xf32, #blocked0>\n@@ -965,7 +1012,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n-func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n+func.func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n   %blockidx = tt.get_program_id {axis=0:i32} : i32\n   %blockidy = tt.get_program_id {axis=1:i32} : i32\n   %blockidz = tt.get_program_id {axis=2:i32} : i32\n@@ -985,7 +1032,7 @@ func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n // -----\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n+  func.func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n     // CHECK: nvvm.read.ptx.sreg.nctaid.x\n     // CHECK: nvvm.read.ptx.sreg.nctaid.y\n     // CHECK: nvvm.read.ptx.sreg.nctaid.z\n@@ -996,16 +1043,16 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %v1 = arith.addi %v0, %blockdimz : i32\n     %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n     tt.store %a, %0 : tensor<32xi32, #blocked0>\n-  \n+\n     return\n   }\n }\n \n // -----\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  // CHECK-LABEL: test_index_cache \n-  func @test_index_cache() {\n+  // CHECK-LABEL: test_index_cache\n+  func.func @test_index_cache() {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n     %0 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n     // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n@@ -1018,8 +1065,8 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n #shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK-LABEL: test_base_index_cache \n-  func @test_base_index_cache(%arg0: tensor<128x32xf32, #blocked0>) {\n+  // CHECK-LABEL: test_base_index_cache\n+  func.func @test_base_index_cache(%arg0: tensor<128x32xf32, #blocked0>) {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n     // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n@@ -1033,7 +1080,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: test_index_cache_different_block\n-  func @test_index_cache_different_block(%arg0: tensor<128x32xf32, #blocked0>, %arg1: i1) {\n+  func.func @test_index_cache_different_block(%arg0: tensor<128x32xf32, #blocked0>, %arg1: i1) {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n     scf.if %arg1 {\n@@ -1042,4 +1089,4 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     }\n     return\n   }\n-}\n\\ No newline at end of file\n+}"}, {"filename": "test/Target/tritongpu_to_llvmir.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -4,11 +4,11 @@\n // CHECK-LABEL: ; ModuleID = 'LLVMDialectModule'\n // CHECK: define void @test_empty_kernel\n // CHECK: !nvvm.annotations\n-// CHECK: !{void (i32, half addrspace(1)*)* @test_empty_kernel, !\"maxntidx\", i32 128}\n+// CHECK: !{ptr @test_empty_kernel, !\"maxntidx\", i32 128}\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n-func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n+func.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n \n   return\n }"}, {"filename": "test/Target/tritongpu_to_ptx.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -6,7 +6,7 @@\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n-func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n+func.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n \n   return\n }"}, {"filename": "test/Triton/combine.mlir", "status": "modified", "additions": 26, "deletions": 20, "changes": 46, "file_content_changes": "@@ -2,10 +2,10 @@\n // RUN: triton-opt %s -split-input-file -canonicalize -triton-combine | FileCheck %s\n \n // CHECK-LABEL: @test_combine_dot_add_pattern\n-func @test_combine_dot_add_pattern() -> (tensor<128x128xf32>, tensor<128x128xf32>) {\n-    // CHECK: %[[d:.*]] = arith.constant dense<3.000000e+00> : tensor<128x128xf32>\n-    // CHECK: %[[b:.*]] = arith.constant dense<2.000000e+00> : tensor<128x128xf32>\n-    // CHECK: %[[a:.*]] = arith.constant dense<1.000000e+00> : tensor<128x128xf32>\n+func.func @test_combine_dot_add_pattern() -> (tensor<128x128xf32>, tensor<128x128xf32>) {\n+    // CHECK-DAG: %[[d:.*]] = arith.constant dense<3.000000e+00> : tensor<128x128xf32>\n+    // CHECK-DAG: %[[b:.*]] = arith.constant dense<2.000000e+00> : tensor<128x128xf32>\n+    // CHECK-DAG: %[[a:.*]] = arith.constant dense<1.000000e+00> : tensor<128x128xf32>\n     %a = arith.constant dense<1.0> : tensor<128x128xf32>\n     %b = arith.constant dense<2.0> : tensor<128x128xf32>\n     %zero = arith.constant dense<0.0> : tensor<128x128xf32>\n@@ -24,7 +24,7 @@ func @test_combine_dot_add_pattern() -> (tensor<128x128xf32>, tensor<128x128xf32\n \n \n // COM: CHECK-LABEL: @test_combine_addptr_pattern\n-func @test_combine_addptr_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>> {\n+func.func @test_combine_addptr_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>> {\n     %off0 = arith.constant 10 : i32\n     %off1 = arith.constant 15 : i32\n \n@@ -47,40 +47,46 @@ func @test_combine_addptr_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>>\n \n \n // CHECK-LABEL: @test_combine_select_masked_load_pattern\n-func @test_combine_select_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %cond: i1) -> (tensor<8xf32>, tensor<8xf32>) {\n+func.func @test_combine_select_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %cond: i1) -> (tensor<8xf32>, tensor<8xf32>) {\n     %mask = tt.broadcast %cond : (i1) -> tensor<8xi1>\n     %false_val = arith.constant dense<0.0> : tensor<8xf32>\n \n     // CHECK: %[[res1:.*]] = tt.load %{{.*}}, %{{.*}}, %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n     %x = tt.load %ptr, %mask, %false_val {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n-    %0 = select %cond, %x, %false_val : tensor<8xf32>\n+    %0 = arith.select %cond, %x, %false_val : tensor<8xf32>\n \n     // CHECK: %[[res2:.*]] = tt.load %{{.*}}, %{{.*}}, %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n     %y = tt.load %ptr, %mask, %false_val {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n-    %1 = select %cond, %y, %false_val : tensor<8xf32>\n+    %1 = arith.select %cond, %y, %false_val : tensor<8xf32>\n \n     // CHECK: return %[[res1]], %[[res2]] : tensor<8xf32>, tensor<8xf32>\n     return %0, %1 : tensor<8xf32>, tensor<8xf32>\n }\n \n // CHECK-LABEL: @test_combine_select_masked_load_fail_pattern\n-func @test_combine_select_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %dummy_load: tensor<8xf32>, %dummy_broadcast: tensor<8xi1>, %cond: i1) -> (tensor<8xf32>, tensor<8xf32>) {\n+func.func @test_combine_select_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %dummy_load: tensor<8xf32>, %dummy_broadcast: tensor<8xi1>, %cond0: i1, %cond1: i1) -> (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) {\n     %false_val = arith.constant dense<0.0> : tensor<8xf32>\n \n     // Case 1: value at the \"load\" position is not an \"op\".  Select should not be canonicalized.\n-    // CHECK: %{{.*}} = select %{{.*}}, %{{.*}}, %{{.*}} : tensor<8xf32>\n-    %0 = select %cond, %dummy_load, %false_val : tensor<8xf32>\n+    // CHECK: %{{.*}} = arith.select %{{.*}}, %{{.*}}, %{{.*}} : tensor<8xf32>\n+    %0 = arith.select %cond0, %dummy_load, %false_val : tensor<8xf32>\n \n     // Case 2: value at the \"broadcast\" position is not an \"op\".  Select should not be canonicalized.\n-    %real_load = tt.load %ptr, %dummy_broadcast, %false_val {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n-    // CHECK: %{{.*}} = select %{{.*}}, %{{.*}}, %{{.*}} : tensor<8xf32>\n-    %1 = select %cond, %real_load, %false_val : tensor<8xf32>\n+    %real_load0 = tt.load %ptr, %dummy_broadcast, %false_val {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n+    // CHECK: %{{.*}} = arith.select %{{.*}}, %{{.*}}, %{{.*}} : tensor<8xf32>\n+    %1 = arith.select %cond0, %real_load0, %false_val : tensor<8xf32>\n \n-    return %0, %1 : tensor<8xf32>, tensor<8xf32>\n+    // Case 3: condition of \"broadcast\" is not the same as the condition of \"select\".  Select should not be canonicalized.\n+    %cond0_ = tt.broadcast %cond0 : (i1) -> tensor<8xi1>\n+    %real_load1 = tt.load %ptr, %cond0_, %false_val {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n+    // CHECK: %{{.*}} = arith.select %{{.*}}, %{{.*}}, %{{.*}} : tensor<8xf32>\n+    %2 = arith.select %cond1, %real_load1, %false_val : tensor<8xf32>\n+\n+    return %0, %1, %2 : tensor<8xf32>, tensor<8xf32>, tensor<8xf32>\n }\n \n // CHECK-LABEL: @test_combine_broadcast_constant_pattern\n-func @test_combine_broadcast_constant_pattern(%cst : f32) -> tensor<8x2xf32> {\n+func.func @test_combine_broadcast_constant_pattern(%cst : f32) -> tensor<8x2xf32> {\n     // CHECK: %[[cst:.*]] = arith.constant dense<1.000000e+00> : tensor<8x2xf32>\n     %const = arith.constant dense<1.0> : tensor<8xf32>\n     %bst_out = tt.broadcast %const : (tensor<8xf32>) -> tensor<8x2xf32>\n@@ -90,7 +96,7 @@ func @test_combine_broadcast_constant_pattern(%cst : f32) -> tensor<8x2xf32> {\n }\n \n // CHECK-LABEL: @test_canonicalize_masked_load_pattern\n-func @test_canonicalize_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>) -> (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) {\n+func.func @test_canonicalize_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>) -> (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) {\n     %true_mask = arith.constant dense<true> : tensor<8xi1>\n     %false_mask = arith.constant dense<false> : tensor<8xi1>\n     %other_val = arith.constant dense<0.0> : tensor<8xf32>\n@@ -111,7 +117,7 @@ func @test_canonicalize_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>) -> (te\n }\n \n // CHECK-LABEL: @test_canonicalize_masked_load_fail_pattern\n-func @test_canonicalize_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %mask: tensor<8xi1>) -> (tensor<8xf32>, tensor<8xf32>) {\n+func.func @test_canonicalize_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %mask: tensor<8xi1>) -> (tensor<8xf32>, tensor<8xf32>) {\n     %other_val = arith.constant dense<0.0> : tensor<8xf32>\n \n     // Case: value at the \"mask\" position is not an \"op\".  Load should not be canonicalized.\n@@ -124,7 +130,7 @@ func @test_canonicalize_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %\n }\n \n // CHECK-LABEL: @test_canonicalize_masked_store_pattern\n-func @test_canonicalize_masked_store_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %val: tensor<8xf32>) {\n+func.func @test_canonicalize_masked_store_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %val: tensor<8xf32>) {\n     %true_mask = arith.constant dense<true> : tensor<8xi1>\n     %false_mask = arith.constant dense<false> : tensor<8xi1>\n \n@@ -138,7 +144,7 @@ func @test_canonicalize_masked_store_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %val:\n }\n \n // CHECK-LABEL: @test_canonicalize_masked_store_fail_pattern\n-func @test_canonicalize_masked_store_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %val: tensor<8xf32>, %mask: tensor<8xi1>) {\n+func.func @test_canonicalize_masked_store_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %val: tensor<8xf32>, %mask: tensor<8xi1>) {\n     // Case: value at the \"mask\" position is not an \"op\".  Store should not be canonicalized.\n     // CHECK: tt.store %{{.*}}, %{{.*}}, %{{.*}} : tensor<8xf32>\n     tt.store %ptr, %val, %mask : tensor<8xf32>"}, {"filename": "test/Triton/vecadd.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,7 +1,7 @@\n // RUN: triton-opt %s -verify-diagnostics\n \n module {\n-  func @add_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32__(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32) {\n+  func.func @add_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32__(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32) {\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %c256_i32 = arith.constant 256 : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -43,7 +43,7 @@ module {\n   }\n }\n // module {\n-//   func @add_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32__(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32) {\n+//   func.func @add_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32__(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32) {\n //     %c64 = arith.constant 64 : index\n //     %c32 = arith.constant 32 : index\n //     %c0 = arith.constant 0 : index"}, {"filename": "test/TritonGPU/coalesce.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -19,7 +19,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n // CHECK: [[store_val:%.*]] = triton_gpu.convert_layout {{.*}} -> tensor<64x64xf32, [[col_layout]]>\n // CHECK: [[store_mask:%.*]] = triton_gpu.convert_layout {{.*}} -> tensor<64x64xi1, [[col_layout]]>\n // CHECK: tt.store [[store_ptr]], [[store_val]], [[store_mask]]\n-func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+func.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n                 %arg1: i32 {tt.divisibility = 16 : i32},\n                 %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n                 %arg3: i32 {tt.divisibility = 16 : i32}) {"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 846, "deletions": 31, "changes": 877, "file_content_changes": "@@ -2,36 +2,41 @@\n \n #layout0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n #layout1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+#layout2 = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [4, 1]}>\n \n-// CHECK: [[target_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n-// CHECK: [[row_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n-// CHECK: [[col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n-// CHECK: [[col_layout_novec:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n-func @cst() -> tensor<1024xi32, #layout1> {\n+// CHECK: [[$target_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+// CHECK: [[$row_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+// CHECK: [[$col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n+// CHECK: [[$col_layout_novec:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n+// CHECK-LABEL: cst\n+func.func @cst() -> tensor<1024xi32, #layout1> {\n   %cst = arith.constant dense<0> : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %cst : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: return %cst : tensor<1024xi32, [[target_layout]]>\n+  // CHECK: return %cst : tensor<1024xi32, [[$target_layout]]>\n   return %1: tensor<1024xi32, #layout1>\n }\n \n-func @range() -> tensor<1024xi32, #layout1> {\n+// CHECK-LABEL: range\n+func.func @range() -> tensor<1024xi32, #layout1> {\n   %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %0 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: return %0 : tensor<1024xi32, [[target_layout]]>\n+  // CHECK: return %0 : tensor<1024xi32, [[$target_layout]]>\n   return %1: tensor<1024xi32, #layout1>\n }\n \n-func @splat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n+// CHECK-LABEL: splat\n+func.func @splat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   %0 = tt.splat %arg0 : (i32) -> tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %0 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: return %0 : tensor<1024xi32, [[target_layout]]>\n+  // CHECK: return %0 : tensor<1024xi32, [[$target_layout]]>\n   return %1: tensor<1024xi32, #layout1>\n }\n \n-func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n+// CHECK-LABEL: remat\n+func.func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #layout0>\n   %1 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #layout0>\n   %2 = arith.muli %0, %1 : tensor<1024xi32, #layout0>\n@@ -40,14 +45,151 @@ func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   %5 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   %6 = arith.addi %3, %5 : tensor<1024xi32, #layout1>\n   return %6: tensor<1024xi32, #layout1>\n-  // CHECK: %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[target_layout]]>\n-  // CHECK: %1 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[target_layout]]>\n-  // CHECK: %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[target_layout]]>\n-  // CHECK: %3 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[target_layout]]>\n-  // CHECK: %4 = arith.muli %0, %2 : tensor<1024xi32, [[target_layout]]>\n-  // CHECK: %5 = arith.muli %1, %3 : tensor<1024xi32, [[target_layout]]>\n-  // CHECK: %6 = arith.addi %4, %5 : tensor<1024xi32, [[target_layout]]>\n-  // CHECK: return %6 : tensor<1024xi32, [[target_layout]]>\n+  // CHECK: %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>\n+  // CHECK: %1 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>\n+  // CHECK: %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>\n+  // CHECK: %3 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>\n+  // CHECK: %4 = arith.muli %0, %2 : tensor<1024xi32, [[$target_layout]]>\n+  // CHECK: %5 = arith.muli %1, %3 : tensor<1024xi32, [[$target_layout]]>\n+  // CHECK: %6 = arith.addi %4, %5 : tensor<1024xi32, [[$target_layout]]>\n+  // CHECK: return %6 : tensor<1024xi32, [[$target_layout]]>\n+}\n+\n+// CHECK-LABEL: remat_load_store\n+func.func @remat_load_store(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout0>\n+  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout0>\n+  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout0>, tensor<64xi32, #layout0>\n+  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout0>\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout0>) -> tensor<64xi32, #layout1>\n+  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout0>) -> tensor<64x!tt.ptr<i32>, #layout1>\n+  tt.store %5, %4 : tensor<64xi32, #layout1>\n+  return\n+}\n+\n+// Don't rematerialize vectorized loads\n+// CHECK-LABEL: remat_expensive\n+func.func @remat_expensive(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout1>\n+  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout1>\n+  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout1>, tensor<64xi32, #layout1>\n+  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout1>\n+  // CHECK: triton_gpu.convert_layout\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout1>) -> tensor<64xi32, #layout0>\n+  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout1>) -> tensor<64x!tt.ptr<i32>, #layout0>\n+  tt.store %5, %4 : tensor<64xi32, #layout0>\n+  return\n+}\n+\n+// Don't rematerialize loads when original and target layouts are different\n+// CHECK-LABEL: remat_multi_layout\n+func.func @remat_multi_layout(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout0>\n+  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout0>\n+  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout0>, tensor<64xi32, #layout0>\n+  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout0>\n+  // CHECK: triton_gpu.convert_layout\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout0>) -> tensor<64xi32, #layout2>\n+  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout0>) -> tensor<64x!tt.ptr<i32>, #layout2>\n+  tt.store %5, %4 : tensor<64xi32, #layout2>\n+  return\n+}\n+\n+// Always rematerialize single value loads\n+// CHECK-LABEL: remat_single_value\n+func.func @remat_single_value(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %0 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<1x!tt.ptr<i32>, #layout1>\n+  %1 = tt.load %0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1xi32, #layout1>\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %2 = triton_gpu.convert_layout %1 : (tensor<1xi32, #layout1>) -> tensor<1xi32, #layout0>\n+  %3 = triton_gpu.convert_layout %0 : (tensor<1x!tt.ptr<i32>, #layout1>) -> tensor<1x!tt.ptr<i32>, #layout0>\n+  tt.store %3, %2 : tensor<1xi32, #layout0>\n+  return\n+}\n+\n+// CHECK-LABEL: if\n+func.func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout1>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout1>\n+  %2 = arith.muli %1, %c32_i32 : tensor<1024xi32, #layout1>\n+  %3 = arith.addi %2, %c32_i32 : tensor<1024xi32, #layout1>\n+  %4 = arith.cmpi sgt, %0, %arg0 : i32\n+  %5 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>, #layout0>\n+  scf.if %4 {\n+    %6 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout1>) -> tensor<1024xi32, #layout0>\n+    tt.store %5, %6 : tensor<1024xi32, #layout0>\n+  }\n+  return\n+}\n+\n+// CHECK-LABEL: if_convert_else_not\n+func.func @if_convert_else_not(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n+  %9 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout1>\n+  %2 = arith.muli %1, %c32_i32 : tensor<1024xi32, #layout0>\n+  %3 = arith.addi %2, %c32_i32 : tensor<1024xi32, #layout0>\n+  %4 = arith.cmpi sgt, %0, %arg0 : i32\n+  %5 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>, #layout1>\n+  %8 = scf.if %4 -> tensor<1024xi32, #layout1> {\n+    %6 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n+    scf.yield %6 : tensor<1024xi32, #layout1>\n+  } else {\n+    scf.yield %9 : tensor<1024xi32, #layout1>\n+  }\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  tt.store %5, %8 : tensor<1024xi32, #layout1>\n+  return\n+}\n+\n+// CHECK-LABEL: if_not_else_convert\n+func.func @if_not_else_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n+  %9 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout1>\n+  %2 = arith.muli %1, %c32_i32 : tensor<1024xi32, #layout0>\n+  %3 = arith.addi %2, %c32_i32 : tensor<1024xi32, #layout0>\n+  %4 = arith.cmpi sgt, %0, %arg0 : i32\n+  %5 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>, #layout1>\n+  %8 = scf.if %4 -> tensor<1024xi32, #layout1> {\n+    scf.yield %9 : tensor<1024xi32, #layout1>\n+  } else {\n+    %7 = triton_gpu.convert_layout %3 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n+    scf.yield %7 : tensor<1024xi32, #layout1>\n+  }\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  tt.store %5, %8 : tensor<1024xi32, #layout1>\n+  return\n+}\n+\n+// CHECK-LABEL: if_else_both_convert\n+func.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n+  %2 = arith.muli %1, %c32_i32 : tensor<1024xi32, #layout0>\n+  %3 = arith.addi %2, %c32_i32 : tensor<1024xi32, #layout0>\n+  %4 = arith.cmpi sgt, %0, %arg0 : i32\n+  %5 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>, #layout1>\n+  %8 = scf.if %4 -> tensor<1024xi32, #layout1> {\n+    %6 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n+    scf.yield %6 : tensor<1024xi32, #layout1>\n+  } else {\n+    %7 = triton_gpu.convert_layout %3 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n+    scf.yield %7 : tensor<1024xi32, #layout1>\n+  }\n+  // TODO(csigg): seems like the whole function is converted to layout1.\n+  // disabledCHECK: triton_gpu.convert_layout\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  tt.store %5, %8 : tensor<1024xi32, #layout1>\n+  return\n }\n \n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n@@ -59,11 +201,11 @@ func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n #blocked4 = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n \n // CHECK-LABEL: transpose\n-func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n+func.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, {{%cst.*}}, {{%cst.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, [[row_layout]]>\n-  // CHECK: [[cvt_val:%.*]] = triton_gpu.convert_layout [[loaded_val]] : (tensor<64x64xf32, [[row_layout]]>) -> tensor<64x64xf32, [[col_layout]]>\n-  // CHECK: tt.store {{.*}}, [[cvt_val]], {{%cst.*}} : tensor<64x64xf32, [[col_layout]]>\n+  // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, {{%cst.*}}, {{%cst.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, [[$row_layout]]>\n+  // CHECK: [[cvt_val:%.*]] = triton_gpu.convert_layout [[loaded_val]] : (tensor<64x64xf32, [[$row_layout]]>) -> tensor<64x64xf32, [[$col_layout]]>\n+  // CHECK: tt.store {{.*}}, [[cvt_val]], {{%cst.*}} : tensor<64x64xf32, [[$col_layout]]>\n   // CHECK: return\n   %cst = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked1>\n   %cst_0 = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n@@ -100,15 +242,15 @@ func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt\n }\n \n // CHECK-LABEL: loop\n-func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n+func.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n     // CHECK-NOT: triton_gpu.convert_layout\n-    // CHECK: [[loop_ret:%.*]]:2 = scf.for {{.*}} -> (tensor<64x64xf32, [[row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[row_layout]]>)\n-    // CHECK-NEXT: {{.*}} = tt.load {{.*}} : tensor<64x64xf32, [[row_layout]]>\n-    // CHECK-NEXT: {{.*}} = arith.addf {{.*}} : tensor<64x64xf32, [[row_layout]]>\n-    // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32>, [[row_layout]]>, tensor<64x64xi32, [[row_layout]]>\n-    // CHECK-NEXT: scf.yield {{.*}} : tensor<64x64xf32, [[row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n+    // CHECK: [[loop_ret:%.*]]:2 = scf.for {{.*}} -> (tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>)\n+    // CHECK-NEXT: {{.*}} = tt.load {{.*}} : tensor<64x64xf32, [[$row_layout]]>\n+    // CHECK-NEXT: {{.*}} = arith.addf {{.*}} : tensor<64x64xf32, [[$row_layout]]>\n+    // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>, tensor<64x64xi32, [[$row_layout]]>\n+    // CHECK-NEXT: scf.yield {{.*}} : tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>\n     // CHECK-NEXT: }\n-    // CHECK-NEXT: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[row_layout]]>) -> tensor<64x64xf32, [[col_layout_novec]]>\n+    // CHECK-NEXT: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[$row_layout]]>) -> tensor<64x64xf32, [[$col_layout_novec]]>\n     // CHECK-NOT: triton_gpu.convert_layout\n     %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n     %cst_0 = arith.constant dense<64> : tensor<64x64xi32, #blocked1>\n@@ -154,7 +296,7 @@ func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %ar\n }\n \n // CHECK-LABEL: vecadd\n-func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n+func.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %c256_i32 = arith.constant 256 : i32\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n@@ -183,3 +325,676 @@ func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f3\n   tt.store %21, %22 : tensor<256xf32, #layout1>\n   return\n }\n+\n+// Select has args with different element types\n+// CHECK-LABEL: select\n+func.func @select(%arg0: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}) {\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %cst = arith.constant dense<30000> : tensor<1x1xi32, #blocked2>\n+  %cst_0 = arith.constant dense<30000> : tensor<1x512xi32, #blocked2>\n+  %c512 = arith.constant 512 : index\n+  %c30000 = arith.constant 30000 : index\n+  %c0 = arith.constant 0 : index\n+  %cst_1 = arith.constant dense<2048> : tensor<1x1xi32, #blocked2>\n+  %cst_2 = arith.constant dense<0.000000e+00> : tensor<1x512xf64, #blocked2>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = tt.make_range {end = 1 : i32, start = 0 : i32} : tensor<1xi32, #blocked0>\n+  %2 = triton_gpu.convert_layout %1 : (tensor<1xi32, #blocked0>) -> tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %3 = tt.expand_dims %2 {axis = 1 : i32} : (tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<1x1xi32, #blocked1>\n+  %4 = triton_gpu.convert_layout %3 : (tensor<1x1xi32, #blocked1>) -> tensor<1x1xi32, #blocked2>\n+  %5 = tt.splat %0 : (i32) -> tensor<1x1xi32, #blocked2>\n+  %6 = arith.addi %5, %4 : tensor<1x1xi32, #blocked2>\n+  %7 = \"triton_gpu.cmpi\"(%6, %cst_1) {predicate = 2 : i64} : (tensor<1x1xi32, #blocked2>, tensor<1x1xi32, #blocked2>) -> tensor<1x1xi1, #blocked2>\n+  %8 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked0>\n+  %9 = triton_gpu.convert_layout %8 : (tensor<512xi32, #blocked0>) -> tensor<512xi32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>\n+  %10 = tt.expand_dims %9 {axis = 0 : i32} : (tensor<512xi32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>) -> tensor<1x512xi32, #blocked2>\n+  %11 = arith.muli %6, %cst : tensor<1x1xi32, #blocked2>\n+  %12 = tt.broadcast %11 : (tensor<1x1xi32, #blocked2>) -> tensor<1x512xi32, #blocked2>\n+  %13 = tt.splat %arg0 : (!tt.ptr<f64>) -> tensor<1x512x!tt.ptr<f64>, #blocked2>\n+  %14 = tt.broadcast %7 : (tensor<1x1xi1, #blocked2>) -> tensor<1x512xi1, #blocked2>\n+  %15 = scf.for %arg3 = %c0 to %c30000 step %c512 iter_args(%arg4 = %cst_2) -> (tensor<1x512xf64, #blocked2>) {\n+    %16 = arith.index_cast %arg3 : index to i32\n+    %17 = tt.splat %16 : (i32) -> tensor<1x512xi32, #blocked2>\n+    %18 = arith.addi %17, %10 : tensor<1x512xi32, #blocked2>\n+    %19 = \"triton_gpu.cmpi\"(%18, %cst_0) {predicate = 2 : i64} : (tensor<1x512xi32, #blocked2>, tensor<1x512xi32, #blocked2>) -> tensor<1x512xi1, #blocked2>\n+    %20 = arith.addi %18, %12 : tensor<1x512xi32, #blocked2>\n+    %21 = tt.addptr %13, %20 : tensor<1x512x!tt.ptr<f64>, #blocked2>, tensor<1x512xi32, #blocked2>\n+    %22 = arith.andi %19, %14 : tensor<1x512xi1, #blocked2>\n+    %23 = triton_gpu.convert_layout %21 : (tensor<1x512x!tt.ptr<f64>, #blocked2>) -> tensor<1x512x!tt.ptr<f64>, #blocked3>\n+    %24 = triton_gpu.convert_layout %22 : (tensor<1x512xi1, #blocked2>) -> tensor<1x512xi1, #blocked3>\n+    %25 = tt.load %23, %24 {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<1x512xf64, #blocked3>\n+    %26 = triton_gpu.convert_layout %25 : (tensor<1x512xf64, #blocked3>) -> tensor<1x512xf64, #blocked2>\n+    %27 = arith.andi %14, %19 : tensor<1x512xi1, #blocked2>\n+    %28 = \"triton_gpu.cmpf\"(%arg4, %26) {predicate = 4 : i64} : (tensor<1x512xf64, #blocked2>, tensor<1x512xf64, #blocked2>) -> tensor<1x512xi1, #blocked2>\n+    %29 = arith.andi %27, %28 : tensor<1x512xi1, #blocked2>\n+    %30 = \"triton_gpu.select\"(%29, %26, %arg4) : (tensor<1x512xi1, #blocked2>, tensor<1x512xf64, #blocked2>, tensor<1x512xf64, #blocked2>) -> tensor<1x512xf64, #blocked2>\n+    %31 = triton_gpu.convert_layout %21 : (tensor<1x512x!tt.ptr<f64>, #blocked2>) -> tensor<1x512x!tt.ptr<f64>, #blocked3>\n+    %32 = triton_gpu.convert_layout %30 : (tensor<1x512xf64, #blocked2>) -> tensor<1x512xf64, #blocked3>\n+    %33 = triton_gpu.convert_layout %27 : (tensor<1x512xi1, #blocked2>) -> tensor<1x512xi1, #blocked3>\n+    tt.store %31, %32, %33 : tensor<1x512xf64, #blocked3>\n+    scf.yield %30 : tensor<1x512xf64, #blocked2>\n+  }\n+  return\n+}\n+\n+// Make sure the following IR doesn't hang the compiler.\n+// CHECK-LABEL: long_func\n+func.func public @long_func(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg5: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg7: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg9: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg10: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg11: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg12: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg13: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg14: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg15: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}) {\n+  %cst = arith.constant dense<1.000000e+00> : tensor<1024xf32, #blocked0>\n+  %cst_0 = arith.constant dense<5.000000e-04> : tensor<1024xf32, #blocked0>\n+  %cst_1 = arith.constant dense<0.999499976> : tensor<1024xf32, #blocked0>\n+  %cst_2 = arith.constant dense<1.000000e+04> : tensor<1024xf32, #blocked0>\n+  %cst_3 = arith.constant dense<5000> : tensor<1024xi32, #blocked0>\n+  %cst_4 = arith.constant dense<150> : tensor<1024xi32, #blocked0>\n+  %cst_5 = arith.constant dense<false> : tensor<1024xi1, #blocked0>\n+  %cst_6 = arith.constant dense<2> : tensor<1024xi32, #blocked0>\n+  %cst_7 = arith.constant dense<4999> : tensor<1024xi32, #blocked0>\n+  %cst_8 = arith.constant dense<2499> : tensor<1024xi32, #blocked0>\n+  %cst_9 = arith.constant dense<2500> : tensor<1024xi32, #blocked0>\n+  %cst_10 = arith.constant dense<0.91629076> : tensor<1024xf32, #blocked0>\n+  %c2499_i32 = arith.constant 2499 : i32\n+  %cst_11 = arith.constant dense<1024> : tensor<1024xi32, #blocked0>\n+  %c1024_i32 = arith.constant 1024 : i32\n+  %cst_12 = arith.constant dense<1> : tensor<1024xi32, #blocked0>\n+  %cst_13 = arith.constant dense<0.000000e+00> : tensor<1024xf32, #blocked0>\n+  %cst_14 = arith.constant dense<0> : tensor<1024xi32, #blocked0>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = arith.muli %0, %c1024_i32 : i32\n+  %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #blocked0>\n+  %3 = tt.splat %1 : (i32) -> tensor<1024xi32, #blocked0>\n+  %4 = arith.addi %3, %2 : tensor<1024xi32, #blocked0>\n+  %5 = \"triton_gpu.cmpi\"(%4, %cst_11) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %6 = tt.splat %arg5 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %7 = tt.addptr %6, %4 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %8 = triton_gpu.convert_layout %7 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked1>\n+  %9 = triton_gpu.convert_layout %5 : (tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked1>\n+  %10 = tt.load %8, %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked1>\n+  %11 = triton_gpu.convert_layout %10 : (tensor<1024xf32, #blocked1>) -> tensor<1024xf32, #blocked0>\n+  %12 = tt.splat %arg7 : (!tt.ptr<i64>) -> tensor<1024x!tt.ptr<i64>, #blocked0>\n+  %13 = tt.addptr %12, %4 : tensor<1024x!tt.ptr<i64>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %14 = triton_gpu.convert_layout %13 : (tensor<1024x!tt.ptr<i64>, #blocked0>) -> tensor<1024x!tt.ptr<i64>, #blocked2>\n+  %15 = triton_gpu.convert_layout %5 : (tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked2>\n+  %16 = tt.load %14, %15 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xi64, #blocked2>\n+  %17 = triton_gpu.convert_layout %16 : (tensor<1024xi64, #blocked2>) -> tensor<1024xi64, #blocked0>\n+  %18 = tt.splat %arg8 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %19 = tt.addptr %18, %4 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %20 = triton_gpu.convert_layout %19 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked1>\n+  %21 = triton_gpu.convert_layout %5 : (tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked1>\n+  %22 = tt.load %20, %21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked1>\n+  %23 = triton_gpu.convert_layout %22 : (tensor<1024xf32, #blocked1>) -> tensor<1024xf32, #blocked0>\n+  %24 = arith.subf %cst_13, %11 : tensor<1024xf32, #blocked0>\n+  %25 = math.exp %24 : tensor<1024xf32, #blocked0>\n+  %26 = arith.sitofp %cst_12 : tensor<1024xi32, #blocked0> to tensor<1024xf32, #blocked0>\n+  %27 = arith.addf %25, %26 : tensor<1024xf32, #blocked0>\n+  %28 = arith.divf %26, %27 : tensor<1024xf32, #blocked0>\n+  %29 = tt.addptr %arg6, %c2499_i32 : !tt.ptr<f32>, i32\n+  %30 = tt.load %29 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : f32\n+  %31 = arith.subf %11, %cst_10 : tensor<1024xf32, #blocked0>\n+  %32 = arith.subf %cst_13, %31 : tensor<1024xf32, #blocked0>\n+  %33 = math.exp %32 : tensor<1024xf32, #blocked0>\n+  %34 = arith.addf %33, %26 : tensor<1024xf32, #blocked0>\n+  %35 = arith.divf %26, %34 : tensor<1024xf32, #blocked0>\n+  %36 = tt.splat %30 : (f32) -> tensor<1024xf32, #blocked0>\n+  %37 = \"triton_gpu.cmpf\"(%36, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %38 = \"triton_gpu.select\"(%37, %cst_14, %cst_9) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %39 = \"triton_gpu.select\"(%37, %cst_8, %cst_7) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %40 = arith.subi %39, %38 : tensor<1024xi32, #blocked0>\n+  %41 = \"triton_gpu.cmpi\"(%40, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %42 = \"triton_gpu.cmpi\"(%41, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %43 = arith.remsi %40, %cst_6 : tensor<1024xi32, #blocked0>\n+  %44 = \"triton_gpu.cmpi\"(%43, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %45 = arith.divsi %40, %cst_6 : tensor<1024xi32, #blocked0>\n+  %46 = arith.subi %45, %cst_12 : tensor<1024xi32, #blocked0>\n+  %47 = \"triton_gpu.select\"(%44, %46, %45) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %48 = \"triton_gpu.select\"(%42, %47, %45) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %49 = arith.addi %38, %48 : tensor<1024xi32, #blocked0>\n+  %50 = \"triton_gpu.cmpi\"(%38, %39) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %51 = \"triton_gpu.select\"(%50, %49, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %52 = tt.splat %arg6 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %53 = tt.addptr %52, %51 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %54 = triton_gpu.convert_layout %53 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %55 = tt.load %54 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %56 = \"triton_gpu.cmpf\"(%55, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %57 = \"triton_gpu.cmpi\"(%56, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %58 = arith.andi %57, %50 : tensor<1024xi1, #blocked0>\n+  %59 = arith.addi %51, %cst_12 : tensor<1024xi32, #blocked0>\n+  %60 = \"triton_gpu.select\"(%58, %59, %38) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %61 = arith.andi %56, %50 : tensor<1024xi1, #blocked0>\n+  %62 = \"triton_gpu.select\"(%61, %51, %39) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %63 = \"triton_gpu.cmpi\"(%60, %62) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %64 = arith.subi %62, %60 : tensor<1024xi32, #blocked0>\n+  %65 = \"triton_gpu.cmpi\"(%64, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %66 = \"triton_gpu.cmpi\"(%65, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %67 = arith.remsi %64, %cst_6 : tensor<1024xi32, #blocked0>\n+  %68 = \"triton_gpu.cmpi\"(%67, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %69 = arith.divsi %64, %cst_6 : tensor<1024xi32, #blocked0>\n+  %70 = arith.subi %69, %cst_12 : tensor<1024xi32, #blocked0>\n+  %71 = \"triton_gpu.select\"(%68, %70, %69) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %72 = \"triton_gpu.select\"(%66, %71, %69) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %73 = arith.addi %60, %72 : tensor<1024xi32, #blocked0>\n+  %74 = \"triton_gpu.select\"(%63, %73, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %75 = tt.addptr %52, %74 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %76 = triton_gpu.convert_layout %75 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %77 = tt.load %76 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %78 = \"triton_gpu.cmpf\"(%77, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %79 = \"triton_gpu.cmpi\"(%78, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %80 = arith.andi %79, %63 : tensor<1024xi1, #blocked0>\n+  %81 = arith.addi %74, %cst_12 : tensor<1024xi32, #blocked0>\n+  %82 = \"triton_gpu.select\"(%80, %81, %60) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %83 = arith.andi %78, %63 : tensor<1024xi1, #blocked0>\n+  %84 = \"triton_gpu.select\"(%83, %74, %62) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %85 = \"triton_gpu.cmpi\"(%82, %84) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %86 = arith.subi %84, %82 : tensor<1024xi32, #blocked0>\n+  %87 = \"triton_gpu.cmpi\"(%86, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %88 = \"triton_gpu.cmpi\"(%87, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %89 = arith.remsi %86, %cst_6 : tensor<1024xi32, #blocked0>\n+  %90 = \"triton_gpu.cmpi\"(%89, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %91 = arith.divsi %86, %cst_6 : tensor<1024xi32, #blocked0>\n+  %92 = arith.subi %91, %cst_12 : tensor<1024xi32, #blocked0>\n+  %93 = \"triton_gpu.select\"(%90, %92, %91) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %94 = \"triton_gpu.select\"(%88, %93, %91) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %95 = arith.addi %82, %94 : tensor<1024xi32, #blocked0>\n+  %96 = \"triton_gpu.select\"(%85, %95, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %97 = tt.addptr %52, %96 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %98 = triton_gpu.convert_layout %97 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %99 = tt.load %98 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %100 = \"triton_gpu.cmpf\"(%99, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %101 = \"triton_gpu.cmpi\"(%100, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %102 = arith.andi %101, %85 : tensor<1024xi1, #blocked0>\n+  %103 = arith.addi %96, %cst_12 : tensor<1024xi32, #blocked0>\n+  %104 = \"triton_gpu.select\"(%102, %103, %82) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %105 = arith.andi %100, %85 : tensor<1024xi1, #blocked0>\n+  %106 = \"triton_gpu.select\"(%105, %96, %84) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %107 = \"triton_gpu.cmpi\"(%104, %106) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %108 = arith.subi %106, %104 : tensor<1024xi32, #blocked0>\n+  %109 = \"triton_gpu.cmpi\"(%108, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %110 = \"triton_gpu.cmpi\"(%109, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %111 = arith.remsi %108, %cst_6 : tensor<1024xi32, #blocked0>\n+  %112 = \"triton_gpu.cmpi\"(%111, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %113 = arith.divsi %108, %cst_6 : tensor<1024xi32, #blocked0>\n+  %114 = arith.subi %113, %cst_12 : tensor<1024xi32, #blocked0>\n+  %115 = \"triton_gpu.select\"(%112, %114, %113) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %116 = \"triton_gpu.select\"(%110, %115, %113) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %117 = arith.addi %104, %116 : tensor<1024xi32, #blocked0>\n+  %118 = \"triton_gpu.select\"(%107, %117, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %119 = tt.addptr %52, %118 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %120 = triton_gpu.convert_layout %119 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %121 = tt.load %120 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %122 = \"triton_gpu.cmpf\"(%121, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %123 = \"triton_gpu.cmpi\"(%122, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %124 = arith.andi %123, %107 : tensor<1024xi1, #blocked0>\n+  %125 = arith.addi %118, %cst_12 : tensor<1024xi32, #blocked0>\n+  %126 = \"triton_gpu.select\"(%124, %125, %104) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %127 = arith.andi %122, %107 : tensor<1024xi1, #blocked0>\n+  %128 = \"triton_gpu.select\"(%127, %118, %106) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %129 = \"triton_gpu.cmpi\"(%126, %128) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %130 = arith.subi %128, %126 : tensor<1024xi32, #blocked0>\n+  %131 = \"triton_gpu.cmpi\"(%130, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %132 = \"triton_gpu.cmpi\"(%131, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %133 = arith.remsi %130, %cst_6 : tensor<1024xi32, #blocked0>\n+  %134 = \"triton_gpu.cmpi\"(%133, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %135 = arith.divsi %130, %cst_6 : tensor<1024xi32, #blocked0>\n+  %136 = arith.subi %135, %cst_12 : tensor<1024xi32, #blocked0>\n+  %137 = \"triton_gpu.select\"(%134, %136, %135) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %138 = \"triton_gpu.select\"(%132, %137, %135) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %139 = arith.addi %126, %138 : tensor<1024xi32, #blocked0>\n+  %140 = \"triton_gpu.select\"(%129, %139, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %141 = tt.addptr %52, %140 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %142 = triton_gpu.convert_layout %141 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %143 = tt.load %142 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %144 = \"triton_gpu.cmpf\"(%143, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %145 = \"triton_gpu.cmpi\"(%144, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %146 = arith.andi %145, %129 : tensor<1024xi1, #blocked0>\n+  %147 = arith.addi %140, %cst_12 : tensor<1024xi32, #blocked0>\n+  %148 = \"triton_gpu.select\"(%146, %147, %126) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %149 = arith.andi %144, %129 : tensor<1024xi1, #blocked0>\n+  %150 = \"triton_gpu.select\"(%149, %140, %128) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %151 = \"triton_gpu.cmpi\"(%148, %150) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %152 = arith.subi %150, %148 : tensor<1024xi32, #blocked0>\n+  %153 = \"triton_gpu.cmpi\"(%152, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %154 = \"triton_gpu.cmpi\"(%153, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %155 = arith.remsi %152, %cst_6 : tensor<1024xi32, #blocked0>\n+  %156 = \"triton_gpu.cmpi\"(%155, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %157 = arith.divsi %152, %cst_6 : tensor<1024xi32, #blocked0>\n+  %158 = arith.subi %157, %cst_12 : tensor<1024xi32, #blocked0>\n+  %159 = \"triton_gpu.select\"(%156, %158, %157) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %160 = \"triton_gpu.select\"(%154, %159, %157) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %161 = arith.addi %148, %160 : tensor<1024xi32, #blocked0>\n+  %162 = \"triton_gpu.select\"(%151, %161, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %163 = tt.addptr %52, %162 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %164 = triton_gpu.convert_layout %163 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %165 = tt.load %164 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %166 = \"triton_gpu.cmpf\"(%165, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %167 = \"triton_gpu.cmpi\"(%166, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %168 = arith.andi %167, %151 : tensor<1024xi1, #blocked0>\n+  %169 = arith.addi %162, %cst_12 : tensor<1024xi32, #blocked0>\n+  %170 = \"triton_gpu.select\"(%168, %169, %148) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %171 = arith.andi %166, %151 : tensor<1024xi1, #blocked0>\n+  %172 = \"triton_gpu.select\"(%171, %162, %150) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %173 = \"triton_gpu.cmpi\"(%170, %172) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %174 = arith.subi %172, %170 : tensor<1024xi32, #blocked0>\n+  %175 = \"triton_gpu.cmpi\"(%174, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %176 = \"triton_gpu.cmpi\"(%175, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %177 = arith.remsi %174, %cst_6 : tensor<1024xi32, #blocked0>\n+  %178 = \"triton_gpu.cmpi\"(%177, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %179 = arith.divsi %174, %cst_6 : tensor<1024xi32, #blocked0>\n+  %180 = arith.subi %179, %cst_12 : tensor<1024xi32, #blocked0>\n+  %181 = \"triton_gpu.select\"(%178, %180, %179) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %182 = \"triton_gpu.select\"(%176, %181, %179) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %183 = arith.addi %170, %182 : tensor<1024xi32, #blocked0>\n+  %184 = \"triton_gpu.select\"(%173, %183, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %185 = tt.addptr %52, %184 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %186 = triton_gpu.convert_layout %185 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %187 = tt.load %186 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %188 = \"triton_gpu.cmpf\"(%187, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %189 = \"triton_gpu.cmpi\"(%188, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %190 = arith.andi %189, %173 : tensor<1024xi1, #blocked0>\n+  %191 = arith.addi %184, %cst_12 : tensor<1024xi32, #blocked0>\n+  %192 = \"triton_gpu.select\"(%190, %191, %170) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %193 = arith.andi %188, %173 : tensor<1024xi1, #blocked0>\n+  %194 = \"triton_gpu.select\"(%193, %184, %172) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %195 = \"triton_gpu.cmpi\"(%192, %194) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %196 = arith.subi %194, %192 : tensor<1024xi32, #blocked0>\n+  %197 = \"triton_gpu.cmpi\"(%196, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %198 = \"triton_gpu.cmpi\"(%197, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %199 = arith.remsi %196, %cst_6 : tensor<1024xi32, #blocked0>\n+  %200 = \"triton_gpu.cmpi\"(%199, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %201 = arith.divsi %196, %cst_6 : tensor<1024xi32, #blocked0>\n+  %202 = arith.subi %201, %cst_12 : tensor<1024xi32, #blocked0>\n+  %203 = \"triton_gpu.select\"(%200, %202, %201) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %204 = \"triton_gpu.select\"(%198, %203, %201) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %205 = arith.addi %192, %204 : tensor<1024xi32, #blocked0>\n+  %206 = \"triton_gpu.select\"(%195, %205, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %207 = tt.addptr %52, %206 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %208 = triton_gpu.convert_layout %207 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %209 = tt.load %208 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %210 = \"triton_gpu.cmpf\"(%209, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %211 = \"triton_gpu.cmpi\"(%210, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %212 = arith.andi %211, %195 : tensor<1024xi1, #blocked0>\n+  %213 = arith.addi %206, %cst_12 : tensor<1024xi32, #blocked0>\n+  %214 = \"triton_gpu.select\"(%212, %213, %192) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %215 = arith.andi %210, %195 : tensor<1024xi1, #blocked0>\n+  %216 = \"triton_gpu.select\"(%215, %206, %194) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %217 = \"triton_gpu.cmpi\"(%214, %216) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %218 = arith.subi %216, %214 : tensor<1024xi32, #blocked0>\n+  %219 = \"triton_gpu.cmpi\"(%218, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %220 = \"triton_gpu.cmpi\"(%219, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %221 = arith.remsi %218, %cst_6 : tensor<1024xi32, #blocked0>\n+  %222 = \"triton_gpu.cmpi\"(%221, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %223 = arith.divsi %218, %cst_6 : tensor<1024xi32, #blocked0>\n+  %224 = arith.subi %223, %cst_12 : tensor<1024xi32, #blocked0>\n+  %225 = \"triton_gpu.select\"(%222, %224, %223) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %226 = \"triton_gpu.select\"(%220, %225, %223) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %227 = arith.addi %214, %226 : tensor<1024xi32, #blocked0>\n+  %228 = \"triton_gpu.select\"(%217, %227, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %229 = tt.addptr %52, %228 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %230 = triton_gpu.convert_layout %229 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %231 = tt.load %230 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %232 = \"triton_gpu.cmpf\"(%231, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %233 = \"triton_gpu.cmpi\"(%232, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %234 = arith.andi %233, %217 : tensor<1024xi1, #blocked0>\n+  %235 = arith.addi %228, %cst_12 : tensor<1024xi32, #blocked0>\n+  %236 = \"triton_gpu.select\"(%234, %235, %214) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %237 = arith.andi %232, %217 : tensor<1024xi1, #blocked0>\n+  %238 = \"triton_gpu.select\"(%237, %228, %216) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %239 = \"triton_gpu.cmpi\"(%236, %238) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %240 = arith.subi %238, %236 : tensor<1024xi32, #blocked0>\n+  %241 = \"triton_gpu.cmpi\"(%240, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %242 = \"triton_gpu.cmpi\"(%241, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %243 = arith.remsi %240, %cst_6 : tensor<1024xi32, #blocked0>\n+  %244 = \"triton_gpu.cmpi\"(%243, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %245 = arith.divsi %240, %cst_6 : tensor<1024xi32, #blocked0>\n+  %246 = arith.subi %245, %cst_12 : tensor<1024xi32, #blocked0>\n+  %247 = \"triton_gpu.select\"(%244, %246, %245) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %248 = \"triton_gpu.select\"(%242, %247, %245) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %249 = arith.addi %236, %248 : tensor<1024xi32, #blocked0>\n+  %250 = \"triton_gpu.select\"(%239, %249, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %251 = tt.addptr %52, %250 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %252 = triton_gpu.convert_layout %251 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %253 = tt.load %252 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %254 = \"triton_gpu.cmpf\"(%253, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %255 = \"triton_gpu.cmpi\"(%254, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %256 = arith.andi %255, %239 : tensor<1024xi1, #blocked0>\n+  %257 = arith.addi %250, %cst_12 : tensor<1024xi32, #blocked0>\n+  %258 = \"triton_gpu.select\"(%256, %257, %236) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %259 = arith.andi %254, %239 : tensor<1024xi1, #blocked0>\n+  %260 = \"triton_gpu.select\"(%259, %250, %238) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %261 = \"triton_gpu.cmpi\"(%258, %260) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %262 = arith.subi %260, %258 : tensor<1024xi32, #blocked0>\n+  %263 = \"triton_gpu.cmpi\"(%262, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %264 = \"triton_gpu.cmpi\"(%263, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %265 = arith.remsi %262, %cst_6 : tensor<1024xi32, #blocked0>\n+  %266 = \"triton_gpu.cmpi\"(%265, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %267 = arith.divsi %262, %cst_6 : tensor<1024xi32, #blocked0>\n+  %268 = arith.subi %267, %cst_12 : tensor<1024xi32, #blocked0>\n+  %269 = \"triton_gpu.select\"(%266, %268, %267) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %270 = \"triton_gpu.select\"(%264, %269, %267) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %271 = arith.addi %258, %270 : tensor<1024xi32, #blocked0>\n+  %272 = \"triton_gpu.select\"(%261, %271, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %273 = tt.addptr %52, %272 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %274 = triton_gpu.convert_layout %273 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %275 = tt.load %274 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %276 = \"triton_gpu.cmpf\"(%275, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %277 = \"triton_gpu.cmpi\"(%276, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %278 = arith.andi %277, %261 : tensor<1024xi1, #blocked0>\n+  %279 = arith.addi %272, %cst_12 : tensor<1024xi32, #blocked0>\n+  %280 = \"triton_gpu.select\"(%278, %279, %258) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %281 = arith.andi %276, %261 : tensor<1024xi1, #blocked0>\n+  %282 = \"triton_gpu.select\"(%281, %272, %260) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %283 = \"triton_gpu.cmpi\"(%280, %282) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %284 = arith.subi %282, %280 : tensor<1024xi32, #blocked0>\n+  %285 = \"triton_gpu.cmpi\"(%284, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %286 = \"triton_gpu.cmpi\"(%285, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %287 = arith.remsi %284, %cst_6 : tensor<1024xi32, #blocked0>\n+  %288 = \"triton_gpu.cmpi\"(%287, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %289 = arith.divsi %284, %cst_6 : tensor<1024xi32, #blocked0>\n+  %290 = arith.subi %289, %cst_12 : tensor<1024xi32, #blocked0>\n+  %291 = \"triton_gpu.select\"(%288, %290, %289) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %292 = \"triton_gpu.select\"(%286, %291, %289) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %293 = arith.addi %280, %292 : tensor<1024xi32, #blocked0>\n+  %294 = \"triton_gpu.select\"(%283, %293, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %295 = tt.addptr %52, %294 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %296 = triton_gpu.convert_layout %295 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %297 = tt.load %296 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %298 = \"triton_gpu.cmpf\"(%297, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %299 = \"triton_gpu.cmpi\"(%298, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %300 = arith.andi %299, %283 : tensor<1024xi1, #blocked0>\n+  %301 = arith.addi %294, %cst_12 : tensor<1024xi32, #blocked0>\n+  %302 = \"triton_gpu.select\"(%300, %301, %280) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %303 = arith.extsi %cst_12 : tensor<1024xi32, #blocked0> to tensor<1024xi64, #blocked0>\n+  %304 = \"triton_gpu.cmpi\"(%17, %303) {predicate = 0 : i64} : (tensor<1024xi64, #blocked0>, tensor<1024xi64, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %305 = arith.fptosi %23 : tensor<1024xf32, #blocked0> to tensor<1024xi64, #blocked0>\n+  %306 = arith.extsi %cst_14 : tensor<1024xi32, #blocked0> to tensor<1024xi64, #blocked0>\n+  %307 = \"triton_gpu.cmpi\"(%306, %305) {predicate = 4 : i64} : (tensor<1024xi64, #blocked0>, tensor<1024xi64, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %308 = arith.extsi %cst_4 : tensor<1024xi32, #blocked0> to tensor<1024xi64, #blocked0>\n+  %309 = \"triton_gpu.cmpi\"(%305, %308) {predicate = 4 : i64} : (tensor<1024xi64, #blocked0>, tensor<1024xi64, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %310 = \"triton_gpu.select\"(%309, %306, %305) : (tensor<1024xi1, #blocked0>, tensor<1024xi64, #blocked0>, tensor<1024xi64, #blocked0>) -> tensor<1024xi64, #blocked0>\n+  %311 = \"triton_gpu.select\"(%307, %306, %310) : (tensor<1024xi1, #blocked0>, tensor<1024xi64, #blocked0>, tensor<1024xi64, #blocked0>) -> tensor<1024xi64, #blocked0>\n+  %312 = \"triton_gpu.select\"(%304, %311, %306) : (tensor<1024xi1, #blocked0>, tensor<1024xi64, #blocked0>, tensor<1024xi64, #blocked0>) -> tensor<1024xi64, #blocked0>\n+  %313 = arith.extsi %cst_3 : tensor<1024xi32, #blocked0> to tensor<1024xi64, #blocked0>\n+  %314 = arith.muli %312, %313 : tensor<1024xi64, #blocked0>\n+  %315 = arith.extsi %302 : tensor<1024xi32, #blocked0> to tensor<1024xi64, #blocked0>\n+  %316 = arith.addi %315, %314 : tensor<1024xi64, #blocked0>\n+  %317 = arith.trunci %316 : tensor<1024xi64, #blocked0> to tensor<1024xi32, #blocked0>\n+  %318 = arith.extsi %317 : tensor<1024xi32, #blocked0> to tensor<1024xi64, #blocked0>\n+  %319 = tt.splat %arg9 : (!tt.ptr<f64>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %320 = tt.addptr %319, %318 : tensor<1024x!tt.ptr<f64>, #blocked0>, tensor<1024xi64, #blocked0>\n+  %321 = triton_gpu.convert_layout %320 : (tensor<1024x!tt.ptr<f64>, #blocked0>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %322 = tt.load %321 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf64, #blocked0>\n+  %323 = arith.extf %cst_2 : tensor<1024xf32, #blocked0> to tensor<1024xf64, #blocked0>\n+  %324 = \"triton_gpu.cmpf\"(%322, %323) {predicate = 2 : i64} : (tensor<1024xf64, #blocked0>, tensor<1024xf64, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %325 = tt.splat %arg10 : (!tt.ptr<f64>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %326 = tt.addptr %325, %318 : tensor<1024x!tt.ptr<f64>, #blocked0>, tensor<1024xi64, #blocked0>\n+  %327 = triton_gpu.convert_layout %326 : (tensor<1024x!tt.ptr<f64>, #blocked0>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %328 = tt.load %327 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf64, #blocked0>\n+  %329 = arith.divf %328, %322 : tensor<1024xf64, #blocked0>\n+  %330 = arith.truncf %329 : tensor<1024xf64, #blocked0> to tensor<1024xf32, #blocked0>\n+  %331 = arith.mulf %330, %cst_1 : tensor<1024xf32, #blocked0>\n+  %332 = arith.mulf %35, %cst_0 : tensor<1024xf32, #blocked0>\n+  %333 = arith.addf %331, %332 : tensor<1024xf32, #blocked0>\n+  %334 = \"triton_gpu.select\"(%324, %333, %35) : (tensor<1024xi1, #blocked0>, tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xf32, #blocked0>\n+  %335 = tt.addptr %319, %317 : tensor<1024x!tt.ptr<f64>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %336 = triton_gpu.convert_layout %335 : (tensor<1024x!tt.ptr<f64>, #blocked0>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %337 = tt.load %336 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf64, #blocked0>\n+  %338 = arith.extf %cst : tensor<1024xf32, #blocked0> to tensor<1024xf64, #blocked0>\n+  %339 = arith.mulf %337, %338 : tensor<1024xf64, #blocked0>\n+  %340 = tt.addptr %325, %317 : tensor<1024x!tt.ptr<f64>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %341 = triton_gpu.convert_layout %340 : (tensor<1024x!tt.ptr<f64>, #blocked0>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %342 = tt.load %341 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf64, #blocked0>\n+  %343 = arith.mulf %342, %338 : tensor<1024xf64, #blocked0>\n+  %344 = tt.splat %arg11 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %345 = tt.addptr %344, %4 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %346 = triton_gpu.convert_layout %345 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked1>\n+  %347 = triton_gpu.convert_layout %28 : (tensor<1024xf32, #blocked0>) -> tensor<1024xf32, #blocked1>\n+  %348 = triton_gpu.convert_layout %5 : (tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked1>\n+  tt.store %346, %347, %348 : tensor<1024xf32, #blocked1>\n+  %349 = tt.splat %arg12 : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>, #blocked0>\n+  %350 = tt.addptr %349, %4 : tensor<1024x!tt.ptr<i32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %351 = triton_gpu.convert_layout %350 : (tensor<1024x!tt.ptr<i32>, #blocked0>) -> tensor<1024x!tt.ptr<i32>, #blocked1>\n+  %352 = triton_gpu.convert_layout %317 : (tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked1>\n+  %353 = triton_gpu.convert_layout %5 : (tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked1>\n+  tt.store %351, %352, %353 : tensor<1024xi32, #blocked1>\n+  %354 = tt.splat %arg13 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %355 = tt.addptr %354, %4 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %356 = triton_gpu.convert_layout %355 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked1>\n+  %357 = triton_gpu.convert_layout %334 : (tensor<1024xf32, #blocked0>) -> tensor<1024xf32, #blocked1>\n+  %358 = triton_gpu.convert_layout %5 : (tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked1>\n+  tt.store %356, %357, %358 : tensor<1024xf32, #blocked1>\n+  %359 = tt.splat %arg14 : (!tt.ptr<f64>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %360 = tt.addptr %359, %318 : tensor<1024x!tt.ptr<f64>, #blocked0>, tensor<1024xi64, #blocked0>\n+  %361 = triton_gpu.convert_layout %360 : (tensor<1024x!tt.ptr<f64>, #blocked0>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %362 = triton_gpu.convert_layout %339 : (tensor<1024xf64, #blocked0>) -> tensor<1024xf64, #blocked0>\n+  tt.store %361, %362 : tensor<1024xf64, #blocked0>\n+  %363 = tt.splat %arg15 : (!tt.ptr<f64>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %364 = tt.addptr %363, %318 : tensor<1024x!tt.ptr<f64>, #blocked0>, tensor<1024xi64, #blocked0>\n+  %365 = triton_gpu.convert_layout %364 : (tensor<1024x!tt.ptr<f64>, #blocked0>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %366 = triton_gpu.convert_layout %343 : (tensor<1024xf64, #blocked0>) -> tensor<1024xf64, #blocked0>\n+  tt.store %365, %366 : tensor<1024xf64, #blocked0>\n+  return\n+}\n+\n+// A mnist model from torch inductor.\n+// Check if topological sort is working correct and there's no unnecessary convert\n+// CHECK-LABEL: mnist\n+func.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: i32) {\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %cst = arith.constant dense<10> : tensor<16x1xi32, #blocked2>\n+  %cst_0 = arith.constant dense<10> : tensor<1x16xi32, #blocked3>\n+  %c16_i32 = arith.constant 16 : i32\n+  %cst_1 = arith.constant dense<64> : tensor<16x1xi32, #blocked2>\n+  %cst_2 = arith.constant dense<0xFF800000> : tensor<16x16xf32, #blocked2>\n+  %cst_3 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked2>\n+  %cst_4 = arith.constant dense<0> : tensor<16x16xi32, #blocked2>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = arith.muli %0, %c16_i32 : i32\n+  %2 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked0>\n+  %3 = triton_gpu.convert_layout %2 : (tensor<16xi32, #blocked0>) -> tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %4 = tt.expand_dims %3 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<16x1xi32, #blocked1>\n+  %5 = triton_gpu.convert_layout %4 : (tensor<16x1xi32, #blocked1>) -> tensor<16x1xi32, #blocked2>\n+  %6 = tt.splat %1 : (i32) -> tensor<16x1xi32, #blocked2>\n+  %7 = arith.addi %6, %5 : tensor<16x1xi32, #blocked2>\n+  %8 = \"triton_gpu.cmpi\"(%7, %cst_1) {predicate = 2 : i64} : (tensor<16x1xi32, #blocked2>, tensor<16x1xi32, #blocked2>) -> tensor<16x1xi1, #blocked2>\n+  %9 = triton_gpu.convert_layout %2 : (tensor<16xi32, #blocked0>) -> tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>\n+  %10 = tt.expand_dims %9 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>) -> tensor<1x16xi32, #blocked3>\n+  %11 = \"triton_gpu.cmpi\"(%10, %cst_0) {predicate = 2 : i64} : (tensor<1x16xi32, #blocked3>, tensor<1x16xi32, #blocked3>) -> tensor<1x16xi1, #blocked3>\n+  %12 = arith.muli %7, %cst : tensor<16x1xi32, #blocked2>\n+  %13 = tt.broadcast %10 : (tensor<1x16xi32, #blocked3>) -> tensor<16x16xi32, #blocked3>\n+  %14 = triton_gpu.convert_layout %13 : (tensor<16x16xi32, #blocked3>) -> tensor<16x16xi32, #blocked2>\n+  %15 = tt.broadcast %12 : (tensor<16x1xi32, #blocked2>) -> tensor<16x16xi32, #blocked2>\n+  %16 = arith.addi %14, %15 : tensor<16x16xi32, #blocked2>\n+  %17 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<16x16x!tt.ptr<f32>, #blocked2>\n+  %18 = tt.addptr %17, %16 : tensor<16x16x!tt.ptr<f32>, #blocked2>, tensor<16x16xi32, #blocked2>\n+  %19 = tt.broadcast %11 : (tensor<1x16xi1, #blocked3>) -> tensor<16x16xi1, #blocked3>\n+  %20 = triton_gpu.convert_layout %19 : (tensor<16x16xi1, #blocked3>) -> tensor<16x16xi1, #blocked2>\n+  %21 = tt.broadcast %8 : (tensor<16x1xi1, #blocked2>) -> tensor<16x16xi1, #blocked2>\n+  %22 = arith.andi %20, %21 : tensor<16x16xi1, #blocked2>\n+  %23 = triton_gpu.convert_layout %18 : (tensor<16x16x!tt.ptr<f32>, #blocked2>) -> tensor<16x16x!tt.ptr<f32>, #blocked4>\n+  %24 = triton_gpu.convert_layout %22 : (tensor<16x16xi1, #blocked2>) -> tensor<16x16xi1, #blocked4>\n+  %25 = tt.load %23, %24 {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<16x16xf32, #blocked4>\n+  %26 = triton_gpu.convert_layout %25 : (tensor<16x16xf32, #blocked4>) -> tensor<16x16xf32, #blocked2>\n+  %27 = \"triton_gpu.cmpf\"(%cst_2, %26) {predicate = 4 : i64} : (tensor<16x16xf32, #blocked2>, tensor<16x16xf32, #blocked2>) -> tensor<16x16xi1, #blocked2>\n+  %28 = arith.andi %22, %27 : tensor<16x16xi1, #blocked2>\n+  %29 = \"triton_gpu.select\"(%28, %26, %cst_2) : (tensor<16x16xi1, #blocked2>, tensor<16x16xf32, #blocked2>, tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>\n+  %30 = tt.reduce %29 {axis = 1 : i32, redOp = 12 : i32} : tensor<16x16xf32, #blocked2> -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+  %31 = triton_gpu.convert_layout %30 : (tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<16xf32, #blocked0>\n+  %32 = triton_gpu.convert_layout %31 : (tensor<16xf32, #blocked0>) -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %33 = tt.expand_dims %32 {axis = 1 : i32} : (tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<16x1xf32, #blocked1>\n+  %34 = triton_gpu.convert_layout %33 : (tensor<16x1xf32, #blocked1>) -> tensor<16x1xf32, #blocked2>\n+  %35 = arith.sitofp %cst_4 : tensor<16x16xi32, #blocked2> to tensor<16x16xf32, #blocked2>\n+  %36 = arith.addf %35, %cst_3 : tensor<16x16xf32, #blocked2>\n+  %37 = triton_gpu.convert_layout %18 : (tensor<16x16x!tt.ptr<f32>, #blocked2>) -> tensor<16x16x!tt.ptr<f32>, #blocked4>\n+  %38 = triton_gpu.convert_layout %22 : (tensor<16x16xi1, #blocked2>) -> tensor<16x16xi1, #blocked4>\n+  %39 = tt.load %37, %38 {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<16x16xf32, #blocked4>\n+  %40 = triton_gpu.convert_layout %39 : (tensor<16x16xf32, #blocked4>) -> tensor<16x16xf32, #blocked2>\n+  %41 = tt.broadcast %34 : (tensor<16x1xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>\n+  %42 = arith.subf %40, %41 : tensor<16x16xf32, #blocked2>\n+  %43 = math.exp %42 : tensor<16x16xf32, #blocked2>\n+  %44 = arith.addf %36, %43 : tensor<16x16xf32, #blocked2>\n+  %45 = \"triton_gpu.select\"(%22, %44, %36) : (tensor<16x16xi1, #blocked2>, tensor<16x16xf32, #blocked2>, tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>\n+  %46 = tt.reduce %45 {axis = 1 : i32, redOp = 2 : i32} : tensor<16x16xf32, #blocked2> -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+  %47 = triton_gpu.convert_layout %46 : (tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<16xf32, #blocked0>\n+  %48 = triton_gpu.convert_layout %47 : (tensor<16xf32, #blocked0>) -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %49 = tt.expand_dims %48 {axis = 1 : i32} : (tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<16x1xf32, #blocked1>\n+  %50 = triton_gpu.convert_layout %49 : (tensor<16x1xf32, #blocked1>) -> tensor<16x1xf32, #blocked2>\n+  %51 = triton_gpu.convert_layout %18 : (tensor<16x16x!tt.ptr<f32>, #blocked2>) -> tensor<16x16x!tt.ptr<f32>, #blocked4>\n+  %52 = triton_gpu.convert_layout %22 : (tensor<16x16xi1, #blocked2>) -> tensor<16x16xi1, #blocked4>\n+  %53 = tt.load %51, %52 {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<16x16xf32, #blocked4>\n+  %54 = triton_gpu.convert_layout %53 : (tensor<16x16xf32, #blocked4>) -> tensor<16x16xf32, #blocked2>\n+  %55 = arith.subf %54, %41 : tensor<16x16xf32, #blocked2>\n+  %56 = math.log %50 : tensor<16x1xf32, #blocked2>\n+  %57 = tt.broadcast %56 : (tensor<16x1xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>\n+  %58 = arith.subf %55, %57 : tensor<16x16xf32, #blocked2>\n+  %59 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<16x16x!tt.ptr<f32>, #blocked2>\n+  %60 = tt.addptr %59, %16 : tensor<16x16x!tt.ptr<f32>, #blocked2>, tensor<16x16xi32, #blocked2>\n+  %61 = triton_gpu.convert_layout %60 : (tensor<16x16x!tt.ptr<f32>, #blocked2>) -> tensor<16x16x!tt.ptr<f32>, #blocked4>\n+  %62 = triton_gpu.convert_layout %58 : (tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked4>\n+  %63 = triton_gpu.convert_layout %22 : (tensor<16x16xi1, #blocked2>) -> tensor<16x16xi1, #blocked4>\n+  tt.store %61, %62, %63 : tensor<16x16xf32, #blocked4>\n+  return\n+}\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [8, 1], order = [0, 1]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [2, 4], order = [0, 1]}>\n+#blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 8], order = [0, 1]}>\n+#blocked4 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [8, 1], order = [1, 0]}>\n+#blocked5 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [8, 1], order = [1, 0]}>\n+// cmpf and cmpi have different operands and result types\n+// CHECK-LABEL: cmp\n+func.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) {\n+  %c64 = arith.constant 64 : index\n+  %c2048 = arith.constant 2048 : index\n+  %c0 = arith.constant 0 : index\n+  %c64_i32 = arith.constant 64 : i32\n+  %cst = arith.constant dense<-3.40282347E+38> : tensor<64x64xf32, #blocked2>\n+  %cst_0 = arith.constant dense<4194304> : tensor<64x1xi32, #blocked2>\n+  %cst_1 = arith.constant dense<12> : tensor<64x1xi32, #blocked2>\n+  %cst_2 = arith.constant dense<2048> : tensor<1x64xi32, #blocked3>\n+  %cst_3 = arith.constant dense<0> : tensor<64x64xi32, #blocked2>\n+  %cst_4 = arith.constant dense<2048> : tensor<64x1xi32, #blocked2>\n+  %cst_5 = arith.constant dense<49152> : tensor<64x1xi32, #blocked2>\n+  %cst_6 = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked2>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = arith.muli %0, %c64_i32 : i32\n+  %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked0>\n+  %3 = triton_gpu.convert_layout %2 : (tensor<64xi32, #blocked0>) -> tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %4 = tt.expand_dims %3 {axis = 1 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<64x1xi32, #blocked1>\n+  %5 = triton_gpu.convert_layout %4 : (tensor<64x1xi32, #blocked1>) -> tensor<64x1xi32, #blocked2>\n+  %6 = tt.splat %1 : (i32) -> tensor<64x1xi32, #blocked2>\n+  %7 = arith.addi %6, %5 : tensor<64x1xi32, #blocked2>\n+  %8 = \"triton_gpu.cmpi\"(%7, %cst_5) {predicate = 2 : i64} : (tensor<64x1xi32, #blocked2>, tensor<64x1xi32, #blocked2>) -> tensor<64x1xi1, #blocked2>\n+  %9 = triton_gpu.convert_layout %2 : (tensor<64xi32, #blocked0>) -> tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>\n+  %10 = tt.expand_dims %9 {axis = 0 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>) -> tensor<1x64xi32, #blocked3>\n+  %11 = arith.remsi %7, %cst_4 : tensor<64x1xi32, #blocked2>\n+  %12 = arith.divsi %7, %cst_4 : tensor<64x1xi32, #blocked2>\n+  %13 = arith.sitofp %cst_3 : tensor<64x64xi32, #blocked2> to tensor<64x64xf32, #blocked2>\n+  %14 = arith.addf %13, %cst_6 : tensor<64x64xf32, #blocked2>\n+  %15 = arith.muli %7, %cst_4 : tensor<64x1xi32, #blocked2>\n+  %16 = tt.broadcast %15 : (tensor<64x1xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %17 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<64x64x!tt.ptr<f16>, #blocked2>\n+  %18 = tt.broadcast %8 : (tensor<64x1xi1, #blocked2>) -> tensor<64x64xi1, #blocked2>\n+  %19 = arith.muli %11, %cst_4 : tensor<64x1xi32, #blocked2>\n+  %20 = tt.broadcast %19 : (tensor<64x1xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %21 = arith.divsi %12, %cst_1 : tensor<64x1xi32, #blocked2>\n+  %22 = arith.muli %21, %cst_0 : tensor<64x1xi32, #blocked2>\n+  %23 = tt.broadcast %22 : (tensor<64x1xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %24 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>, #blocked2>\n+  %25 = scf.for %arg6 = %c0 to %c2048 step %c64 iter_args(%arg7 = %14) -> (tensor<64x64xf32, #blocked2>) {\n+    %44 = arith.index_cast %arg6 : index to i32\n+    %45 = tt.splat %44 : (i32) -> tensor<1x64xi32, #blocked3>\n+    %46 = arith.addi %45, %10 : tensor<1x64xi32, #blocked3>\n+    %47 = \"triton_gpu.cmpi\"(%46, %cst_2) {predicate = 2 : i64} : (tensor<1x64xi32, #blocked3>, tensor<1x64xi32, #blocked3>) -> tensor<1x64xi1, #blocked3>\n+    %48 = tt.broadcast %46 : (tensor<1x64xi32, #blocked3>) -> tensor<64x64xi32, #blocked3>\n+    %49 = triton_gpu.convert_layout %48 : (tensor<64x64xi32, #blocked3>) -> tensor<64x64xi32, #blocked2>\n+    %50 = arith.addi %49, %16 : tensor<64x64xi32, #blocked2>\n+    %51 = tt.addptr %17, %50 : tensor<64x64x!tt.ptr<f16>, #blocked2>, tensor<64x64xi32, #blocked2>\n+    %52 = tt.broadcast %47 : (tensor<1x64xi1, #blocked3>) -> tensor<64x64xi1, #blocked3>\n+    %53 = triton_gpu.convert_layout %52 : (tensor<64x64xi1, #blocked3>) -> tensor<64x64xi1, #blocked2>\n+    %54 = arith.andi %53, %18 : tensor<64x64xi1, #blocked2>\n+    %55 = triton_gpu.convert_layout %51 : (tensor<64x64x!tt.ptr<f16>, #blocked2>) -> tensor<64x64x!tt.ptr<f16>, #blocked4>\n+    %56 = triton_gpu.convert_layout %54 : (tensor<64x64xi1, #blocked2>) -> tensor<64x64xi1, #blocked4>\n+    %57 = tt.load %55, %56 {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<64x64xf16, #blocked4>\n+    %58 = triton_gpu.convert_layout %57 : (tensor<64x64xf16, #blocked4>) -> tensor<64x64xf16, #blocked2>\n+    %59 = arith.extf %58 : tensor<64x64xf16, #blocked2> to tensor<64x64xf32, #blocked2>\n+    %60 = arith.addi %49, %20 : tensor<64x64xi32, #blocked2>\n+    %61 = arith.addi %60, %23 : tensor<64x64xi32, #blocked2>\n+    %62 = tt.addptr %24, %61 : tensor<64x64x!tt.ptr<f32>, #blocked2>, tensor<64x64xi32, #blocked2>\n+    %63 = triton_gpu.convert_layout %62 : (tensor<64x64x!tt.ptr<f32>, #blocked2>) -> tensor<64x64x!tt.ptr<f32>, #blocked5>\n+    %64 = triton_gpu.convert_layout %54 : (tensor<64x64xi1, #blocked2>) -> tensor<64x64xi1, #blocked5>\n+    %65 = tt.load %63, %64 {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<64x64xf32, #blocked5>\n+    %66 = triton_gpu.convert_layout %65 : (tensor<64x64xf32, #blocked5>) -> tensor<64x64xf32, #blocked2>\n+    %67 = arith.addf %59, %66 : tensor<64x64xf32, #blocked2>\n+    %68 = \"triton_gpu.cmpf\"(%67, %67) {predicate = 13 : i64} : (tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xi1, #blocked2>\n+    %69 = \"triton_gpu.cmpf\"(%67, %cst) {predicate = 2 : i64} : (tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xi1, #blocked2>\n+    %70 = \"triton_gpu.select\"(%69, %67, %cst) : (tensor<64x64xi1, #blocked2>, tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xf32, #blocked2>\n+    %71 = \"triton_gpu.select\"(%68, %67, %70) : (tensor<64x64xi1, #blocked2>, tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xf32, #blocked2>\n+    %72 = math.exp %71 : tensor<64x64xf32, #blocked2>\n+    %73 = arith.addf %arg7, %72 : tensor<64x64xf32, #blocked2>\n+    %74 = \"triton_gpu.select\"(%54, %73, %arg7) : (tensor<64x64xi1, #blocked2>, tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xf32, #blocked2>\n+    scf.yield %74 : tensor<64x64xf32, #blocked2>\n+  }\n+  %26 = tt.reduce %25 {axis = 1 : i32, redOp = 2 : i32} : tensor<64x64xf32, #blocked2> -> tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+  %27 = triton_gpu.convert_layout %26 : (tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<64xf32, #blocked0>\n+  %28 = triton_gpu.convert_layout %27 : (tensor<64xf32, #blocked0>) -> tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %29 = tt.expand_dims %28 {axis = 1 : i32} : (tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<64x1xf32, #blocked1>\n+  %30 = triton_gpu.convert_layout %29 : (tensor<64x1xf32, #blocked1>) -> tensor<64x1xf32, #blocked2>\n+  %31 = arith.muli %7, %cst_4 : tensor<64x1xi32, #blocked2>\n+  %32 = tt.broadcast %31 : (tensor<64x1xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %33 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<64x64x!tt.ptr<f16>, #blocked2>\n+  %34 = tt.broadcast %8 : (tensor<64x1xi1, #blocked2>) -> tensor<64x64xi1, #blocked2>\n+  %35 = arith.muli %11, %cst_4 : tensor<64x1xi32, #blocked2>\n+  %36 = tt.broadcast %35 : (tensor<64x1xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %37 = arith.divsi %12, %cst_1 : tensor<64x1xi32, #blocked2>\n+  %38 = arith.muli %37, %cst_0 : tensor<64x1xi32, #blocked2>\n+  %39 = tt.broadcast %38 : (tensor<64x1xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %40 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>, #blocked2>\n+  %41 = tt.broadcast %30 : (tensor<64x1xf32, #blocked2>) -> tensor<64x64xf32, #blocked2>\n+  %42 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>, #blocked2>\n+  %43 = tt.splat %arg3 : (!tt.ptr<f16>) -> tensor<64x64x!tt.ptr<f16>, #blocked2>\n+  scf.for %arg6 = %c0 to %c2048 step %c64 {\n+    %44 = arith.index_cast %arg6 : index to i32\n+    %45 = tt.splat %44 : (i32) -> tensor<1x64xi32, #blocked3>\n+    %46 = arith.addi %45, %10 : tensor<1x64xi32, #blocked3>\n+    %47 = \"triton_gpu.cmpi\"(%46, %cst_2) {predicate = 2 : i64} : (tensor<1x64xi32, #blocked3>, tensor<1x64xi32, #blocked3>) -> tensor<1x64xi1, #blocked3>\n+    %48 = tt.broadcast %46 : (tensor<1x64xi32, #blocked3>) -> tensor<64x64xi32, #blocked3>\n+    %49 = triton_gpu.convert_layout %48 : (tensor<64x64xi32, #blocked3>) -> tensor<64x64xi32, #blocked2>\n+    %50 = arith.addi %49, %32 : tensor<64x64xi32, #blocked2>\n+    %51 = tt.addptr %33, %50 : tensor<64x64x!tt.ptr<f16>, #blocked2>, tensor<64x64xi32, #blocked2>\n+    %52 = tt.broadcast %47 : (tensor<1x64xi1, #blocked3>) -> tensor<64x64xi1, #blocked3>\n+    %53 = triton_gpu.convert_layout %52 : (tensor<64x64xi1, #blocked3>) -> tensor<64x64xi1, #blocked2>\n+    %54 = arith.andi %53, %34 : tensor<64x64xi1, #blocked2>\n+    %55 = triton_gpu.convert_layout %51 : (tensor<64x64x!tt.ptr<f16>, #blocked2>) -> tensor<64x64x!tt.ptr<f16>, #blocked4>\n+    %56 = triton_gpu.convert_layout %54 : (tensor<64x64xi1, #blocked2>) -> tensor<64x64xi1, #blocked4>\n+    %57 = tt.load %55, %56 {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<64x64xf16, #blocked4>\n+    %58 = triton_gpu.convert_layout %57 : (tensor<64x64xf16, #blocked4>) -> tensor<64x64xf16, #blocked2>\n+    %59 = arith.extf %58 : tensor<64x64xf16, #blocked2> to tensor<64x64xf32, #blocked2>\n+    %60 = arith.addi %49, %36 : tensor<64x64xi32, #blocked2>\n+    %61 = arith.addi %60, %39 : tensor<64x64xi32, #blocked2>\n+    %62 = tt.addptr %40, %61 : tensor<64x64x!tt.ptr<f32>, #blocked2>, tensor<64x64xi32, #blocked2>\n+    %63 = triton_gpu.convert_layout %62 : (tensor<64x64x!tt.ptr<f32>, #blocked2>) -> tensor<64x64x!tt.ptr<f32>, #blocked5>\n+    %64 = triton_gpu.convert_layout %54 : (tensor<64x64xi1, #blocked2>) -> tensor<64x64xi1, #blocked5>\n+    %65 = tt.load %63, %64 {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<64x64xf32, #blocked5>\n+    %66 = triton_gpu.convert_layout %65 : (tensor<64x64xf32, #blocked5>) -> tensor<64x64xf32, #blocked2>\n+    %67 = arith.addf %59, %66 : tensor<64x64xf32, #blocked2>\n+    %68 = \"triton_gpu.cmpf\"(%67, %67) {predicate = 13 : i64} : (tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xi1, #blocked2>\n+    %69 = \"triton_gpu.cmpf\"(%67, %cst) {predicate = 2 : i64} : (tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xi1, #blocked2>\n+    %70 = \"triton_gpu.select\"(%69, %67, %cst) : (tensor<64x64xi1, #blocked2>, tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xf32, #blocked2>\n+    %71 = \"triton_gpu.select\"(%68, %67, %70) : (tensor<64x64xi1, #blocked2>, tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xf32, #blocked2>\n+    %72 = math.exp %71 : tensor<64x64xf32, #blocked2>\n+    %73 = arith.divf %72, %41 : tensor<64x64xf32, #blocked2>\n+    %74 = tt.addptr %42, %50 : tensor<64x64x!tt.ptr<f32>, #blocked2>, tensor<64x64xi32, #blocked2>\n+    %75 = triton_gpu.convert_layout %74 : (tensor<64x64x!tt.ptr<f32>, #blocked2>) -> tensor<64x64x!tt.ptr<f32>, #blocked5>\n+    %76 = triton_gpu.convert_layout %73 : (tensor<64x64xf32, #blocked2>) -> tensor<64x64xf32, #blocked5>\n+    %77 = triton_gpu.convert_layout %54 : (tensor<64x64xi1, #blocked2>) -> tensor<64x64xi1, #blocked5>\n+    tt.store %75, %76, %77 : tensor<64x64xf32, #blocked5>\n+    %78 = tt.addptr %43, %50 : tensor<64x64x!tt.ptr<f16>, #blocked2>, tensor<64x64xi32, #blocked2>\n+    %79 = arith.truncf %73 : tensor<64x64xf32, #blocked2> to tensor<64x64xf16, #blocked2>\n+    %80 = triton_gpu.convert_layout %78 : (tensor<64x64x!tt.ptr<f16>, #blocked2>) -> tensor<64x64x!tt.ptr<f16>, #blocked4>\n+    %81 = triton_gpu.convert_layout %79 : (tensor<64x64xf16, #blocked2>) -> tensor<64x64xf16, #blocked4>\n+    %82 = triton_gpu.convert_layout %54 : (tensor<64x64xi1, #blocked2>) -> tensor<64x64xi1, #blocked4>\n+    tt.store %80, %81, %82 : tensor<64x64xf16, #blocked4>\n+  }\n+  return\n+}"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 51, "deletions": 12, "changes": 63, "file_content_changes": "@@ -4,11 +4,13 @@\n // matmul: 128x32 @ 32x128 -> 128x128\n #AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#ALs0 = #triton_gpu.slice<{parent=#AL, dim=0}>\n+#BLs0 = #triton_gpu.slice<{parent=#BL, dim=0}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n-// CHECK: func @matmul_loop\n+// CHECK: func.func @matmul_loop\n // CHECK-DAG: %[[CONSTANT_0:.*]] = arith.constant 0 : i32\n // CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n // CHECK-DAG: %[[CONSTANT_2:.*]] = arith.constant 2 : i32\n@@ -44,9 +46,22 @@\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:   scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n-func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n-  %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n-  %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n+func.func @matmul_loop(%lb : index, %ub : index, %step : index,\n+                  %A : !tt.ptr<f16> {tt.divisibility = 16 : i32},\n+                  %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n+  // A ptrs\n+  %a_ptr_splat = tt.splat %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n+  %a_tmp0 = tt.make_range {end = 32: i32, start = 0: i32} : tensor<32xi32, #ALs0>\n+  %a_tmp1 = tt.expand_dims %a_tmp0 {axis = 0 : i32} : (tensor<32xi32, #ALs0>) -> tensor<1x32xi32, #AL>\n+  %a_offs = tt.broadcast %a_tmp1 : (tensor<1x32xi32, #AL>) -> tensor<128x32xi32, #AL>\n+  %a_ptr_init = tt.addptr %a_ptr_splat, %a_offs : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+  // B ptrs\n+  %b_ptr_splat = tt.splat %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n+  %b_tmp0 = tt.make_range {end = 128: i32, start = 0: i32} : tensor<128xi32, #BLs0>\n+  %b_tmp1 = tt.expand_dims %b_tmp0 {axis = 0 : i32} : (tensor<128xi32, #BLs0>) -> tensor<1x128xi32, #BL>\n+  %b_offs = tt.broadcast %b_tmp1 : (tensor<1x128xi32, #BL>) -> tensor<32x128xi32, #BL>\n+  %b_ptr_init = tt.addptr %b_ptr_splat, %b_offs : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n+\n \n   %a_mask = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %a_other = arith.constant dense<0.00e+00> : tensor<128x32xf16, #AL>\n@@ -73,7 +88,7 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n }\n \n \n-// CHECK: func @matmul_loop_nested\n+// CHECK: func.func @matmul_loop_nested\n // CHECK-DAG: %[[CONSTANT_0:.*]] = arith.constant 0 : i32\n // CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n // CHECK-DAG: %[[CONSTANT_2:.*]] = arith.constant 2 : i32\n@@ -103,10 +118,22 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n-func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n+                         %A : !tt.ptr<f16> {tt.divisibility = 16 : i32},\n+                         %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n   scf.for %iv0 = %lb to %ub step %step {\n-    %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n-    %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n+    // A ptrs\n+    %a_ptr_splat = tt.splat %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n+    %a_tmp0 = tt.make_range {end = 32: i32, start = 0: i32} : tensor<32xi32, #ALs0>\n+    %a_tmp1 = tt.expand_dims %a_tmp0 {axis = 0 : i32} : (tensor<32xi32, #ALs0>) -> tensor<1x32xi32, #AL>\n+    %a_offs = tt.broadcast %a_tmp1 : (tensor<1x32xi32, #AL>) -> tensor<128x32xi32, #AL>\n+    %a_ptr_init = tt.addptr %a_ptr_splat, %a_offs : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+    // B ptrs\n+    %b_ptr_splat = tt.splat %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n+    %b_tmp0 = tt.make_range {end = 128: i32, start = 0: i32} : tensor<128xi32, #BLs0>\n+    %b_tmp1 = tt.expand_dims %b_tmp0 {axis = 0 : i32} : (tensor<128xi32, #BLs0>) -> tensor<1x128xi32, #BL>\n+    %b_offs = tt.broadcast %b_tmp1 : (tensor<1x128xi32, #BL>) -> tensor<32x128xi32, #BL>\n+    %b_ptr_init = tt.addptr %b_ptr_splat, %b_offs : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n \n     %a_mask = arith.constant dense<true> : tensor<128x32xi1, #AL>\n     %a_other = arith.constant dense<0.00e+00> : tensor<128x32xf16, #AL>\n@@ -134,7 +161,7 @@ func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f\n }\n \n \n-// CHECK: func @matmul_loop_single_pipeline\n+// CHECK: func.func @matmul_loop_single_pipeline\n // CHECK-DAG: %[[CONSTANT_0:.*]] = arith.constant 0 : i32\n // CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n // CHECK-DAG: %[[CONSTANT_2:.*]] = arith.constant 2 : i32\n@@ -156,9 +183,21 @@ func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:   scf.yield {{.*}}, {{.*}}, %[[NEXT_B_BUFFER]], %[[NEXT_B]], {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n-func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n-  %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n-  %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n+func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n+                                  %A : !tt.ptr<f16> {tt.divisibility = 16 : i32},\n+                                  %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n+  // A ptrs\n+  %a_ptr_splat = tt.splat %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n+  %a_tmp0 = tt.make_range {end = 32: i32, start = 0: i32} : tensor<32xi32, #ALs0>\n+  %a_tmp1 = tt.expand_dims %a_tmp0 {axis = 0 : i32} : (tensor<32xi32, #ALs0>) -> tensor<1x32xi32, #AL>\n+  %a_offs = tt.broadcast %a_tmp1 : (tensor<1x32xi32, #AL>) -> tensor<128x32xi32, #AL>\n+  %a_ptr_init = tt.addptr %a_ptr_splat, %a_offs : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+  // B ptrs\n+  %b_ptr_splat = tt.splat %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n+  %b_tmp0 = tt.make_range {end = 128: i32, start = 0: i32} : tensor<128xi32, #BLs0>\n+  %b_tmp1 = tt.expand_dims %b_tmp0 {axis = 0 : i32} : (tensor<128xi32, #BLs0>) -> tensor<1x128xi32, #BL>\n+  %b_offs = tt.broadcast %b_tmp1 : (tensor<1x128xi32, #BL>) -> tensor<32x128xi32, #BL>\n+  %b_ptr_init = tt.addptr %b_ptr_splat, %b_offs : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n \n   %a_mask = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %a_other = arith.constant dense<0.00e+00> : tensor<128x32xf16, #AL>"}, {"filename": "test/TritonGPU/matmul.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -4,7 +4,7 @@\n // CHECK: offset = 49152, size = 49152\n // CHECK: size = 98304\n module {\n-func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c64_13c64_14c64_15c8(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32) {\n+func.func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c64_13c64_14c64_15c8(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32) {\n     %cst = arith.constant dense<true> : tensor<64x64xi1>\n     %c64 = arith.constant 64 : index\n     %c0 = arith.constant 0 : index\n@@ -22,7 +22,7 @@ func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c6\n     %7 = arith.muli %6, %c8_i32 : i32\n     %8 = arith.subi %2, %7 : i32\n     %9 = arith.cmpi slt, %8, %c8_i32 : i32\n-    %10 = select %9, %8, %c8_i32 : i32\n+    %10 = arith.select %9, %8, %c8_i32 : i32\n     %11 = arith.remsi %0, %10 : i32\n     %12 = arith.addi %7, %11 : i32\n     %13 = arith.remsi %0, %5 : i32"}, {"filename": "test/TritonGPU/prefetch.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -11,24 +11,24 @@\n #B_OP = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n \n-// CHECK: func @matmul_loop\n+// CHECK: func.func @matmul_loop\n // CHECK-DAG: %[[A0_PREFETCH_SMEM:.*]] = tensor.extract_slice %[[A0:.*]][0, 0] [128, 16]\n // CHECK-DAG: %[[A0_PREFETCH:.*]] = triton_gpu.convert_layout %[[A0_PREFETCH_SMEM]]\n // CHECK-DAG: %[[B0_PREFETCH_SMEM:.*]] = tensor.extract_slice %[[B0:.*]][0, 0] [16, 128]\n // CHECK-DAG: %[[B0_PREFETCH:.*]] = triton_gpu.convert_layout %[[B0_PREFETCH_SMEM]]\n // CHECK:     scf.for {{.*}} iter_args({{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, %[[a0_prefetch:.*]] = %[[A0_PREFETCH]], %[[b0_prefetch:.*]] = %[[B0_PREFETCH]]\n-// CHECK:       %[[D_FIRST:.*]] = tt.dot %[[a0_prefetch]], %[[b0_prefetch:.*]], {{.*}}\n // CHECK-DAG:   %[[A_REM_SMEM:.*]] = tensor.extract_slice %[[arg_a0]][0, 16] [128, 16]\n // CHECK-DAG:   %[[A_REM:.*]] = triton_gpu.convert_layout %[[A_REM_SMEM]]\n // CHECK-DAG:   %[[B_REM_SMEM:.*]] = tensor.extract_slice %[[arg_b0]][16, 0] [16, 128]\n // CHECK-DAG:   %[[B_REM:.*]] = triton_gpu.convert_layout %[[B_REM_SMEM]]\n+// CHECK:       %[[D_FIRST:.*]] = tt.dot %[[a0_prefetch]], %[[b0_prefetch:.*]], {{.*}}\n // CHECK:       tt.dot %[[A_REM]], %[[B_REM]], %[[D_FIRST:.*]]\n // CHECK-DAG:   %[[NEXT_A_PREFETCH_SMEM:.*]] = tensor.extract_slice {{.*}}[0, 0] [128, 16]\n // CHECK-DAG:   %[[NEXT_A_PREFETCH:.*]] = triton_gpu.convert_layout %[[NEXT_A_PREFETCH_SMEM]]\n // CHECK-DAG:   %[[NEXT_B_PREFETCH_SMEM:.*]] = tensor.extract_slice {{.*}}[0, 0] [16, 128]\n // CHECK-DAG:   %[[NEXT_B_PREFETCH:.*]] = triton_gpu.convert_layout %[[NEXT_B_PREFETCH_SMEM]]\n // CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_PREFETCH]], %[[NEXT_B_PREFETCH]]\n-func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n "}, {"filename": "test/TritonGPU/update-mma-for-volta.mlir", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -12,16 +12,16 @@\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n // It creates a new MMA layout to fit with $a and $b's dot_operand, and get the right warpsPerCTA\n // The ID of this MMA instance should be 0.\n-// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 4]}>\n+// CHECK: [[$new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 2]}>\n module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n   // CHECK-LABEL: dot_mmav1\n-  func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n+  func.func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n     %C = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked0>\n     %AA = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a>\n     %BB = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b>\n     %CC = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma0>\n \n-    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, %cst {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma]]>\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, %cst {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[$new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[$new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[$new_mma]]>\n     %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<64x64xf32, #mma0>\n     %res = triton_gpu.convert_layout %D : (tensor<64x64xf32, #mma0>) -> tensor<64x64xf32, #blocked0>\n \n@@ -40,8 +40,8 @@ module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n #mma1 = #triton_gpu.mma<{versionMajor=1, versionMinor=16, warpsPerCTA=[4,4]}>\n \n // Will still get two MMA layouts\n-// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 4]}>\n-// CHECK: [[new_mma1:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 19, warpsPerCTA = [4, 4]}>\n+// CHECK-DAG: [[$new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 2]}>\n+// CHECK-DAG: [[$new_mma1:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 19, warpsPerCTA = [4, 2]}>\n \n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n@@ -50,7 +50,7 @@ module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n \n module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n   // CHECK-LABEL: dot_mmav1\n-  func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n+  func.func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n     %C = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked0>\n     %AA = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a>\n     %BB = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b>\n@@ -60,8 +60,8 @@ module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n     %BB1 = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b1>\n     %CC1 = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma1>\n \n-    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma]]>\n-    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma1]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma1]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma1]]>\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[$new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[$new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[$new_mma]]>\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[$new_mma1]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[$new_mma1]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[$new_mma1]]>\n     %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<64x64xf32, #mma0>\n     %D1 = tt.dot %AA1, %BB1, %CC1 {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a1> * tensor<64x64xf16, #dot_operand_b1> -> tensor<64x64xf32, #mma1>\n     %res = triton_gpu.convert_layout %D : (tensor<64x64xf32, #mma0>) -> tensor<64x64xf32, #blocked0>"}, {"filename": "test/lib/Analysis/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -6,4 +6,5 @@ add_mlir_library(TritonTestAnalysis\n \n   LINK_LIBS PUBLIC\n   TritonAnalysis\n+  ${dialect_libs}\n )\n\\ No newline at end of file"}, {"filename": "test/lib/Analysis/TestAlias.cpp", "status": "modified", "additions": 28, "deletions": 16, "changes": 44, "file_content_changes": "@@ -9,10 +9,10 @@ using namespace mlir;\n namespace {\n \n struct TestAliasPass\n-    : public PassWrapper<TestAliasPass, OperationPass<FuncOp>> {\n+    : public PassWrapper<TestAliasPass, OperationPass<func::FuncOp>> {\n+\n+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAliasPass);\n \n-  // LLVM15+\n-  // MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAliasPass);\n   static void print(StringRef name, SmallVector<std::string, 4> &vals,\n                     raw_ostream &os) {\n     if (vals.empty())\n@@ -36,26 +36,27 @@ struct TestAliasPass\n   void runOnOperation() override {\n     Operation *operation = getOperation();\n     auto &os = llvm::errs();\n-    auto op_name = SymbolTable::getSymbolName(operation).getValue().str();\n-    os << op_name << \"\\n\";\n+    auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n+    os << opName << \"\\n\";\n \n-    SharedMemoryAliasAnalysis analysis(&getContext());\n-    analysis.run(operation);\n+    std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n+    SharedMemoryAliasAnalysis *analysis =\n+        solver->load<SharedMemoryAliasAnalysis>();\n+    if (failed(solver->initializeAndRun(operation)))\n+      return signalPassFailure();\n \n     AsmState state(operation->getParentOfType<ModuleOp>());\n     // Get operation ids of value's aliases\n     auto getAllocOpNames = [&](Value value) {\n-      LatticeElement<AliasInfo> *latticeElement =\n-          analysis.lookupLatticeElement(value);\n+      dataflow::Lattice<AliasInfo> *latticeElement =\n+          analysis->getLatticeElement(value);\n       SmallVector<std::string, 4> opNames;\n       if (latticeElement) {\n         auto &info = latticeElement->getValue();\n-        if (!info.getAllocs().empty()) {\n-          for (auto &alias : info.getAllocs()) {\n-            auto opName =\n-                getValueOperandName(alias.getDefiningOp()->getResult(0), state);\n-            opNames.push_back(std::move(opName));\n-          }\n+        for (auto &alias : info.getAllocs()) {\n+          auto opName =\n+              getValueOperandName(alias.getDefiningOp()->getResult(0), state);\n+          opNames.push_back(std::move(opName));\n         }\n       }\n       // Ensure deterministic output\n@@ -64,8 +65,19 @@ struct TestAliasPass\n     };\n \n     operation->walk<WalkOrder::PreOrder>([&](Operation *op) {\n-      if (op->getNumResults() < 1)\n+      if (op->getNumResults() < 1) {\n+        // cond br, br\n+        if (auto branch = dyn_cast<BranchOpInterface>(op)) {\n+          auto *block = branch->getBlock();\n+          for (auto arg : llvm::enumerate(block->getArguments())) {\n+            auto operand = block->getArgument(arg.index());\n+            auto opNames = getAllocOpNames(operand);\n+            auto argName = getValueOperandName(arg.value(), state);\n+            print(argName, opNames, os);\n+          }\n+        }\n         return;\n+      }\n       if (auto forOp = dyn_cast<scf::ForOp>(op)) {\n         for (auto arg : llvm::enumerate(forOp.getRegionIterArgs())) {\n           auto operand = forOp.getOpOperandForRegionIterArg(arg.value()).get();"}, {"filename": "test/lib/Analysis/TestAllocation.cpp", "status": "modified", "additions": 5, "deletions": 6, "changes": 11, "file_content_changes": "@@ -6,10 +6,9 @@ using namespace mlir;\n namespace {\n \n struct TestAllocationPass\n-    : public PassWrapper<TestAllocationPass, OperationPass<FuncOp>> {\n+    : public PassWrapper<TestAllocationPass, OperationPass<func::FuncOp>> {\n \n-  // LLVM15+\n-  // MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAllocationPass);\n+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAllocationPass);\n \n   StringRef getArgument() const final { return \"test-print-allocation\"; }\n   StringRef getDescription() const final {\n@@ -19,9 +18,9 @@ struct TestAllocationPass\n   void runOnOperation() override {\n     Operation *operation = getOperation();\n     auto &os = llvm::errs();\n-    // Convert to std::string can remove quotes from op_name\n-    auto op_name = SymbolTable::getSymbolName(operation).getValue().str();\n-    os << op_name << \"\\n\";\n+    // Convert to std::string can remove quotes from opName\n+    auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n+    os << opName << \"\\n\";\n     Allocation allocation(operation);\n     operation->walk([&](Operation *op) {\n       auto scratchBufferId = allocation.getBufferId(op);"}, {"filename": "test/lib/Analysis/TestAxisInfo.cpp", "status": "modified", "additions": 12, "deletions": 33, "changes": 45, "file_content_changes": "@@ -1,25 +1,15 @@\n #include \"mlir/Pass/Pass.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Analysis/Utility.h\"\n \n using namespace mlir;\n \n namespace {\n \n struct TestAxisInfoPass\n-    : public PassWrapper<TestAxisInfoPass, OperationPass<FuncOp>> {\n+    : public PassWrapper<TestAxisInfoPass, OperationPass<func::FuncOp>> {\n \n-  // LLVM15+\n-  // MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAlignmentPass);\n-\n-  void print(const std::string &name, raw_ostream &os, ArrayRef<int> vals) {\n-    os << name << \": [\";\n-    for (size_t d = 0; d < vals.size(); d++) {\n-      if (d != 0)\n-        os << \", \";\n-      os << vals[d];\n-    }\n-    os << \"]\";\n-  }\n+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAxisInfoPass);\n \n   StringRef getArgument() const final { return \"test-print-alignment\"; }\n   StringRef getDescription() const final {\n@@ -29,31 +19,20 @@ struct TestAxisInfoPass\n   void runOnOperation() override {\n     Operation *operation = getOperation();\n     auto &os = llvm::errs();\n-    os << \"Testing: \" << operation->getName() << \"\\n\";\n-    AxisInfoAnalysis analysis(&getContext());\n-    analysis.run(operation);\n+    auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n+    os << \"@\" << opName << \"\\n\";\n+\n+    std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n+    AxisInfoAnalysis *analysis = solver->load<AxisInfoAnalysis>();\n+    if (failed(solver->initializeAndRun(operation)))\n+      return signalPassFailure();\n     operation->walk([&](Operation *op) {\n       if (op->getNumResults() < 1)\n         return;\n       for (Value result : op->getResults()) {\n-        // std::ostringstream oss;\n-        // result.print(oss);\n-        // os << \" => \";\n-        LatticeElement<AxisInfo> *latticeElement =\n-            analysis.lookupLatticeElement(result);\n-        if (!latticeElement) {\n-          os << \"None\\n\";\n-          return;\n-        }\n-        AxisInfo &info = latticeElement->getValue();\n-        print(\"Contiguity\", os, info.getContiguity());\n-        os << \" ; \";\n-        print(\"Divisibility\", os, info.getDivisibility());\n-        os << \" ; \";\n-        print(\"Constancy\", os, info.getConstancy());\n-        os << \" ( \";\n         result.print(os);\n-        os << \" ) \";\n+        os << \" => \";\n+        analysis->getLatticeElement(result)->getValue().print(os);\n         os << \"\\n\";\n       }\n     });"}, {"filename": "test/lib/Analysis/TestMembar.cpp", "status": "modified", "additions": 22, "deletions": 17, "changes": 39, "file_content_changes": "@@ -1,6 +1,8 @@\n-#include \"mlir/Dialect/GPU/GPUDialect.h\"\n+#include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n+#include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n #include \"mlir/IR/Dialect.h\"\n #include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n #include \"triton/Analysis/Allocation.h\"\n #include \"triton/Analysis/Membar.h\"\n \n@@ -9,10 +11,9 @@ using namespace mlir;\n namespace {\n \n struct TestMembarPass\n-    : public PassWrapper<TestMembarPass, OperationPass<FuncOp>> {\n+    : public PassWrapper<TestMembarPass, OperationPass<func::FuncOp>> {\n \n-  // LLVM15+\n-  // MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestMembarPass);\n+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestMembarPass);\n \n   StringRef getArgument() const final { return \"test-print-membar\"; }\n   StringRef getDescription() const final {\n@@ -23,23 +24,27 @@ struct TestMembarPass\n     Operation *operation = getOperation();\n     auto &os = llvm::errs();\n     // Convert to std::string can remove quotes from op_name\n-    auto op_name = SymbolTable::getSymbolName(operation).getValue().str();\n-    os << op_name << \"\\n\";\n+    auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n+    os << opName << \"\\n\";\n+\n+    // Lower the module to the cf dialect\n+    auto *context = operation->getContext();\n+    RewritePatternSet scfPatterns(context);\n+    mlir::populateSCFToControlFlowConversionPatterns(scfPatterns);\n+    mlir::ConversionTarget scfTarget(*context);\n+    scfTarget.addIllegalOp<scf::ForOp, scf::IfOp, scf::ParallelOp, scf::WhileOp,\n+                           scf::ExecuteRegionOp>();\n+    scfTarget.markUnknownOpDynamicallyLegal([](Operation *) { return true; });\n+    if (failed(applyPartialConversion(operation, scfTarget,\n+                                      std::move(scfPatterns))))\n+      return signalPassFailure();\n+\n+    // Print all ops after membar pass\n     Allocation allocation(operation);\n     MembarAnalysis membarPass(&allocation);\n     membarPass.run();\n \n-    size_t operationId = 0;\n-    operation->walk<WalkOrder::PreOrder>([&](Operation *op) {\n-      if (isa<gpu::BarrierOp>(op)) {\n-        os << \"Membar \" << operationId << \"\\n\";\n-      }\n-      if (op->getNumRegions() == 0) {\n-        // Don't count parent Operation to simplify the test.\n-        operationId++;\n-      }\n-      return;\n-    });\n+    os << *operation << \"\\n\";\n   }\n };\n "}, {"filename": "unittest/Conversion/TritonGPUToLLVM/PTXAsmFormatTest.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n #include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n-#include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n \n@@ -12,7 +12,7 @@ class PTXAsmFormatTest : public ::testing::Test {\n   static constexpr int numValues = 4;\n \n   PTXAsmFormatTest() {\n-    ctx.loadDialect<arith::ArithmeticDialect>();\n+    ctx.loadDialect<arith::ArithDialect>();\n \n     createValues();\n   }"}]