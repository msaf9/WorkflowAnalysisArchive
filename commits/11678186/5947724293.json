[{"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 11, "deletions": 8, "changes": 19, "file_content_changes": "@@ -20,15 +20,18 @@ class SharedEncodingAttr;\n }\n } // namespace triton\n \n-SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n-                                                ArrayRef<int64_t> shapePerCTA,\n-                                                RankedTensorType type);\n SmallVector<unsigned, 3>\n-mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n-                       ArrayRef<int64_t> shapePerCTA);\n-SmallVector<unsigned, 3>\n-mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n-                       ArrayRef<int64_t> shapePerCTA, unsigned opIdx);\n+mmaVersionToInstrShape(int version, ArrayRef<int64_t> outputShapePerCTA,\n+                       RankedTensorType inputType);\n+SmallVector<unsigned, 2>\n+mmaVersionToInstrShapeOfMN(triton::gpu::MmaEncodingAttr mma,\n+                           ArrayRef<int64_t> outputShapePerCTA);\n+\n+SmallVector<unsigned, 2> mmav3ToInstrShapeOfMK(triton::gpu::MmaEncodingAttr mma,\n+                                               RankedTensorType inputType);\n+\n+SmallVector<unsigned, 2> mmav3ToInstrShapeOfNK(triton::gpu::MmaEncodingAttr mma,\n+                                               RankedTensorType inputType);\n \n /// Returns true if the Load is for TMA\n bool isLoadFromTensorPtr(triton::LoadOp op);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -153,7 +153,7 @@ struct ConvertLayoutOpConversion\n     }\n     if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n       auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n-      auto instrShape = mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n+      auto instrShape = mmaVersionToInstrShapeOfMN(mmaLayout, shapePerCTA);\n       SmallVector<Value> mmaColIdx(4);\n       SmallVector<Value> mmaRowIdx(2);\n       Value threadId = getThreadId(rewriter, loc);\n@@ -729,7 +729,7 @@ struct ConvertLayoutOpConversion\n                                                          rewriter, srcTy);\n \n       auto srcShapePerCTA = getShapePerCTA(mmaLayout, srcShape);\n-      auto instrShape = mmaVersionToInstrShape(mmaLayout, srcShapePerCTA);\n+      auto instrShape = mmaVersionToInstrShapeOfMN(mmaLayout, srcShapePerCTA);\n       auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n       uint32_t repM =\n           ceil<unsigned>(srcShapePerCTA[0], instrShape[0] * warpsPerCTA[0]);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/WGMMA.cpp", "status": "modified", "additions": 11, "deletions": 10, "changes": 21, "file_content_changes": "@@ -161,13 +161,13 @@ DotOpMmaV3SmemLoader loadA(TritonGPUToLLVMTypeConverter *typeConverter,\n   auto aSharedLayout = aTensorTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n   assert(aSharedLayout && \"only support load dot operand from shared.\");\n   auto shapePerCTA = getShapePerCTA(aTensorTy);\n-  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA, 0);\n+  auto instrShape = mmav3ToInstrShapeOfMK(mmaEncoding, aTensorTy);\n   auto wpt = mmaEncoding.getWarpsPerCTA();\n   auto aOrd = aSharedLayout.getOrder();\n   bool transA = aOrd[0] == 0;\n \n   int numRepM = ceil<unsigned>(shapePerCTA[0], instrShape[0] * wpt[0]);\n-  int numRepK = ceil<unsigned>(shapePerCTA[1], instrShape[2]);\n+  int numRepK = ceil<unsigned>(shapePerCTA[1], instrShape[1]);\n \n   Value warp = udiv(thread, i32_val(32));\n   Value warpM = urem(warp, i32_val(wpt[0]));\n@@ -179,7 +179,7 @@ DotOpMmaV3SmemLoader loadA(TritonGPUToLLVMTypeConverter *typeConverter,\n           warpId,\n           wpt[0],\n           transA,\n-          {instrShape[0], instrShape[2]},\n+          {instrShape[0], instrShape[1]},\n           rewriter,\n           loc};\n }\n@@ -192,26 +192,26 @@ DotOpMmaV3SmemLoader loadB(TritonGPUToLLVMTypeConverter *typeConverter,\n   auto bSharedLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n   assert(bSharedLayout && \"only support load B from shared.\");\n   auto shapePerCTA = triton::gpu::getShapePerCTA(bTensorTy);\n-  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA, 1);\n+  auto instrShape = mmav3ToInstrShapeOfNK(mmaEncoding, bTensorTy);\n   auto wpt = mmaEncoding.getWarpsPerCTA();\n   auto bOrd = bSharedLayout.getOrder();\n   bool transB = bOrd[0] == 1;\n \n-  int numRepK = ceil<unsigned>(shapePerCTA[0], instrShape[2]);\n-  int numRepN = ceil<unsigned>(shapePerCTA[1], instrShape[1] * wpt[1]);\n+  int numRepK = ceil<unsigned>(shapePerCTA[0], instrShape[1]);\n+  int numRepN = ceil<unsigned>(shapePerCTA[1], instrShape[0] * wpt[1]);\n \n   Value warp = udiv(thread, i32_val(32));\n   Value warpMN = udiv(warp, i32_val(wpt[0]));\n   Value warpN = urem(warpMN, i32_val(wpt[1]));\n-  Value warpId = urem(warpN, i32_val(shapePerCTA[1] / instrShape[1]));\n+  Value warpId = urem(warpN, i32_val(shapePerCTA[1] / instrShape[0]));\n \n   return {tensor,\n           smemObj,\n           shapePerCTA,\n           warpId,\n           wpt[1],\n           transB,\n-          {instrShape[1], instrShape[2]},\n+          {instrShape[0], instrShape[1]},\n           rewriter,\n           loc};\n }\n@@ -278,7 +278,8 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n   bool transA = aOrd[0] == 0;\n   bool transB = bOrd[0] == 1;\n   auto dShapePerCTA = getShapePerCTA(dTensorTy);\n-  auto instrShape = mmaVersionToInstrShape(mmaEncoding, dShapePerCTA);\n+  auto instrShape = mmaVersionToInstrShape(mmaEncoding.getVersionMajor(),\n+                                           dShapePerCTA, aTensorTy);\n   auto accSize = 2 * (instrShape[1] / 4);\n   int M = 4 * instrShape[0];\n   int N = instrShape[1];\n@@ -360,7 +361,7 @@ Value loadC(Value tensor, Value llTensor) {\n   auto mmaEncoding = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n   assert(mmaEncoding && \"Currently, we only support $c with a mma layout.\");\n   auto shapePerCTA = getShapePerCTA(tensorTy);\n-  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA);\n+  auto instrShape = mmaVersionToInstrShapeOfMN(mmaEncoding, shapePerCTA);\n   auto wpt = mmaEncoding.getWarpsPerCTA();\n   auto shapePerCTATile = getShapePerCTATile(mmaEncoding, tensorTy.getShape());\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -683,7 +683,7 @@ struct StoreAsyncOpConversion\n \n     unsigned numElems = triton::gpu::getTotalElemsPerThread(srcTy);\n \n-    auto instrShape = mmaVersionToInstrShape(\n+    auto instrShape = mmaVersionToInstrShapeOfMN(\n         mmaLayout, getShapePerCTA(mmaLayout, srcTy.getShape()));\n     auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n     uint32_t repM ="}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1038,7 +1038,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     assert(_warpsPerCTA.size() == 2);\n     auto order = triton::gpu::getOrder(mmaLayout);\n     auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n-    auto instrShape = mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n+    auto instrShape = mmaVersionToInstrShapeOfMN(mmaLayout, shapePerCTA);\n     SmallVector<Value> warpsPerCTA = {i32_val(_warpsPerCTA[0]),\n                                       i32_val(_warpsPerCTA[1])};\n \n@@ -1092,7 +1092,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto shape = type.getShape();\n     auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n     SmallVector<SmallVector<unsigned>> ret;\n-    auto instrShape = mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n+    auto instrShape = mmaVersionToInstrShapeOfMN(mmaLayout, shapePerCTA);\n \n     for (unsigned i = 0; i < shapePerCTA[0];\n          i += getShapePerCTATile(mmaLayout, shape)[0]) {"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 15, "deletions": 21, "changes": 36, "file_content_changes": "@@ -202,7 +202,7 @@ SmallVector<unsigned> getSizePerThread(Attribute layout,\n     } else if (mmaLayout.isVolta()) {\n       return {1, 2};\n     } else if (mmaLayout.isHopper()) {\n-      auto instrShape = mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n+      auto instrShape = mmaVersionToInstrShapeOfMN(mmaLayout, shapePerCTA);\n       // TODO(thomas): what are those magic numbers?\n       return SmallVector<unsigned>{instrShape[0] * 4 / 32, instrShape[1] / 4};\n     } else {\n@@ -301,7 +301,7 @@ SmallVector<unsigned> getShapePerCTATile(BlockedEncodingAttr blockedLayout) {\n \n static SmallVector<unsigned>\n getShapePerCTATileOfMma(MmaEncodingAttr mmaLayout,\n-                        ArrayRef<int64_t> tensorShape, int opIdx) {\n+                        ArrayRef<int64_t> tensorShape) {\n   if (mmaLayout.isAmpere())\n     return {16 * mmaLayout.getWarpsPerCTA()[0],\n             8 * mmaLayout.getWarpsPerCTA()[1]};\n@@ -315,13 +315,7 @@ getShapePerCTATileOfMma(MmaEncodingAttr mmaLayout,\n   }\n   if (mmaLayout.isHopper()) {\n     auto shapePerCTA = getShapePerCTA(mmaLayout, tensorShape);\n-    SmallVector<unsigned, 3> instrShape;\n-    if (opIdx < 0)\n-      instrShape = mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n-    else\n-      instrShape =\n-          mmaVersionToInstrShape(mmaLayout, shapePerCTA, (unsigned)opIdx);\n-\n+    auto instrShape = mmaVersionToInstrShapeOfMN(mmaLayout, shapePerCTA);\n     return {16 * mmaLayout.getWarpsPerCTA()[0],\n             instrShape[1] * mmaLayout.getWarpsPerCTA()[1]};\n   }\n@@ -340,7 +334,7 @@ SmallVector<unsigned> getShapePerCTATile(Attribute layout,\n     shape = getShapePerCTATile(sliceLayout.getParent(), parentTensorShape);\n     shape.erase(shape.begin() + sliceLayout.getDim());\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    return getShapePerCTATileOfMma(mmaLayout, tensorShape, -1);\n+    return getShapePerCTATileOfMma(mmaLayout, tensorShape);\n   } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n     auto parentLayout = dotLayout.getParent();\n     assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n@@ -349,7 +343,7 @@ SmallVector<unsigned> getShapePerCTATile(Attribute layout,\n              \"mmaLayout version = 1 is not implemented yet\");\n       int opIdx = dotLayout.getOpIdx();\n       auto parentShapePerCTATile =\n-          getShapePerCTATileOfMma(parentMmaLayout, tensorShape, opIdx);\n+          getShapePerCTATileOfMma(parentMmaLayout, tensorShape);\n       if (opIdx == 0) {\n         return {parentShapePerCTATile[0], 16};\n       } else if (opIdx == 1) {\n@@ -790,11 +784,11 @@ MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape, Type eltTy) const {\n     elemsPerThread[1] = elemsCol;\n   } else if (isHopper()) {\n     auto wpt = getWarpsPerCTA();\n-    auto instrMNK = mmaVersionToInstrShape(*this, shapePerCTA);\n-    int repM = ceil<unsigned>(shapePerCTA[0], instrMNK[0] * wpt[0]);\n-    int repN = ceil<unsigned>(shapePerCTA[1], instrMNK[1] * wpt[1]);\n+    auto instrMN = mmaVersionToInstrShapeOfMN(*this, shapePerCTA);\n+    int repM = ceil<unsigned>(shapePerCTA[0], instrMN[0] * wpt[0]);\n+    int repN = ceil<unsigned>(shapePerCTA[1], instrMN[1] * wpt[1]);\n     elemsPerThread[0] = 2 * repM;\n-    elemsPerThread[1] = (instrMNK[1] / 4) * repN;\n+    elemsPerThread[1] = (instrMN[1] / 4) * repN;\n   } else {\n     llvm_unreachable(\"Unexpected mma version\");\n   }\n@@ -817,17 +811,17 @@ MmaEncodingAttr::getElemsPerThreadOfOperand(int opIdx,\n         \"getElemsPerThreadOfOperand() not supported for version 2\");\n   } else if (isHopper()) {\n     auto wpt = getWarpsPerCTA();\n-    auto instrMNK = mmaVersionToInstrShape(*this, shapePerCTA);\n+    auto instrMN = mmaVersionToInstrShapeOfMN(*this, shapePerCTA);\n     if (opIdx == 0) {\n-      int repM = ceil<unsigned>(shapePerCTA[0], instrMNK[0] * wpt[0]);\n-      int repK = ceil<unsigned>(shapePerCTA[1], instrMNK[2]);\n+      int repM = ceil<unsigned>(shapePerCTA[0], instrMN[0] * wpt[0]);\n+      int repK = ceil<unsigned>(shapePerCTA[1], instrMN[2]);\n       return 8 * repM * repK;\n \n     } else if (opIdx == 1) {\n-      int repK = ceil<unsigned>(shapePerCTA[0], instrMNK[2]);\n-      int repN = ceil<unsigned>(shapePerCTA[1], instrMNK[1] * wpt[1]);\n+      int repK = ceil<unsigned>(shapePerCTA[0], instrMN[2]);\n+      int repN = ceil<unsigned>(shapePerCTA[1], instrMN[1] * wpt[1]);\n       // benzh@ here need more check\n-      return 4 * std::max<int>(instrMNK[1] / 32, 1) * repK * repN;\n+      return 4 * std::max<int>(instrMN[1] / 32, 1) * repK * repN;\n     }\n   }\n   return res;"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 27, "deletions": 34, "changes": 61, "file_content_changes": "@@ -50,38 +50,22 @@ mmaVersionToInstrShapeImpl(int version, const ArrayRef<int64_t> &shape,\n   }\n }\n \n-SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n-                                                ArrayRef<int64_t> shapePerCTA,\n-                                                RankedTensorType type) {\n-  auto instrShape =\n-      mmaVersionToInstrShapeImpl(version, shapePerCTA, type.getElementType());\n+SmallVector<unsigned, 3>\n+mmaVersionToInstrShape(int version, ArrayRef<int64_t> outputShapePerCTA,\n+                       RankedTensorType inputType) {\n   if (version == 3) {\n-    if (shapePerCTA[0] % 64 != 0 || shapePerCTA[1] % 8 != 0) {\n+    if (outputShapePerCTA[0] % 64 != 0 || outputShapePerCTA[1] % 8 != 0) {\n       assert(false && \"type not supported\");\n       return {0, 0, 0};\n     }\n   }\n-  assert(instrShape[1] > 0 && \"type not supported\");\n-  return instrShape;\n+  return mmaVersionToInstrShapeImpl(version, outputShapePerCTA,\n+                                    inputType.getElementType());\n }\n \n-SmallVector<unsigned, 3>\n-mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n-                       ArrayRef<int64_t> shapePerCTA) {\n-  Type eltType;\n-  auto ctx = mma.getContext();\n-  OpBuilder builder(ctx);\n-  if (mma.getIsInt8Input())\n-    eltType = builder.getIntegerType(8);\n-  else\n-    eltType = builder.getF16Type();\n-  return mmaVersionToInstrShapeImpl(\n-      mma.getVersionMajor(), shapePerCTA,\n-      RankedTensorType::get({1, 1}, eltType, mma));\n-}\n-SmallVector<unsigned, 3>\n-mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n-                       ArrayRef<int64_t> shapePerCTA, unsigned opIdx) {\n+SmallVector<unsigned, 2>\n+mmaVersionToInstrShapeOfMN(triton::gpu::MmaEncodingAttr mma,\n+                           ArrayRef<int64_t> outputShapePerCTA) {\n   Type eltType;\n   auto ctx = mma.getContext();\n   OpBuilder builder(ctx);\n@@ -90,16 +74,25 @@ mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n   else\n     eltType = builder.getF16Type();\n   auto instrShape =\n-      mmaVersionToInstrShapeImpl(mma.getVersionMajor(), shapePerCTA, eltType);\n+      mmaVersionToInstrShape(mma.getVersionMajor(), outputShapePerCTA,\n+                             RankedTensorType::get({1, 1}, eltType, mma));\n \n-  if (opIdx == 0) {\n-    instrShape[1] = 0;\n-    assert(shapePerCTA[0] % 64 == 0 && \"M direction not compatible\");\n-  } else {\n-    instrShape[0] = 0;\n-    assert(shapePerCTA[1] % 8 == 0 && \"N direction not compatible\");\n-  }\n-  return instrShape;\n+  return {instrShape[0], instrShape[1]};\n+}\n+\n+SmallVector<unsigned, 2> mmav3ToInstrShapeOfMK(triton::gpu::MmaEncodingAttr mma,\n+                                               RankedTensorType inputType) {\n+  unsigned k = 256 / inputType.getElementType().getIntOrFloatBitWidth();\n+  return {16, k};\n+}\n+\n+SmallVector<unsigned, 2> mmav3ToInstrShapeOfNK(triton::gpu::MmaEncodingAttr mma,\n+                                               RankedTensorType inputType) {\n+  unsigned k = 256 / inputType.getElementType().getIntOrFloatBitWidth();\n+  auto shapePerCTA = triton::gpu::getShapePerCTA(inputType.getEncoding(),\n+                                                 inputType.getShape());\n+  unsigned n = getValidNOfMmaV3(shapePerCTA[1], inputType.getElementType());\n+  return {n, k};\n }\n \n bool isLoadFromTensorPtr(triton::LoadOp op) {"}]