[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 29, "deletions": 17, "changes": 46, "file_content_changes": "@@ -6,6 +6,7 @@\n #include \"mlir/Conversion/LLVMCommon/LoweringOptions.h\"\n #include \"mlir/Conversion/LLVMCommon/Pattern.h\"\n #include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n+#include \"mlir/Conversion/StandardToLLVM/ConvertStandardToLLVM.h\"\n #include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n #include \"mlir/Dialect/GPU/GPUDialect.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n@@ -1295,24 +1296,33 @@ struct AddPtrOpConversion\n   matchAndRewrite(triton::AddPtrOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n-    auto resultTy = op.getType().dyn_cast<RankedTensorType>();\n-    auto resultLayout = resultTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n-    assert(resultLayout && \"Unexpected resultLayout in AddPtrOpConversion\");\n-    auto resultShape = resultTy.getShape();\n-    unsigned elems = resultLayout.getElemsPerThread(resultShape);\n-    Type elemTy =\n-        this->getTypeConverter()->convertType(resultTy.getElementType());\n-    SmallVector<Type> types(elems, elemTy);\n-    Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n-    auto ptrs = getElementsFromStruct(loc, adaptor.ptr(), elems, rewriter);\n-    auto offsets =\n-        getElementsFromStruct(loc, adaptor.offset(), elems, rewriter);\n-    SmallVector<Value> resultVals(elems);\n-    for (unsigned i = 0; i < elems; ++i) {\n-      resultVals[i] = gep(elemTy, ptrs[i], offsets[i]);\n+    auto resultTy = op.getType();\n+    auto resultTensorTy = resultTy.dyn_cast<RankedTensorType>();\n+    if (resultTensorTy) {\n+      auto resultLayout =\n+          resultTensorTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n+      assert(resultLayout && \"Unexpected resultLayout in AddPtrOpConversion\");\n+      auto resultShape = resultTensorTy.getShape();\n+      unsigned elems = resultLayout.getElemsPerThread(resultShape);\n+      Type elemTy =\n+          getTypeConverter()->convertType(resultTensorTy.getElementType());\n+      SmallVector<Type> types(elems, elemTy);\n+      Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n+      auto ptrs = getElementsFromStruct(loc, adaptor.ptr(), elems, rewriter);\n+      auto offsets =\n+          getElementsFromStruct(loc, adaptor.offset(), elems, rewriter);\n+      SmallVector<Value> resultVals(elems);\n+      for (unsigned i = 0; i < elems; ++i) {\n+        resultVals[i] = gep(elemTy, ptrs[i], offsets[i]);\n+      }\n+      Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n+      rewriter.replaceOp(op, view);\n+    } else {\n+      assert(resultTy.isa<triton::PointerType>());\n+      Type llResultTy = getTypeConverter()->convertType(resultTy);\n+      Value result = gep(llResultTy, adaptor.ptr(), adaptor.offset());\n+      rewriter.replaceOp(op, result);\n     }\n-    Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n-    rewriter.replaceOp(op, view);\n     return success();\n   }\n };\n@@ -3202,6 +3212,7 @@ class ConvertTritonGPUToLLVM\n     mlir::arith::populateArithmeticToLLVMConversionPatterns(typeConverter,\n                                                             patterns);\n     mlir::populateMathToLLVMConversionPatterns(typeConverter, patterns);\n+    mlir::populateStdToLLVMConversionPatterns(typeConverter, patterns);\n \n     mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n \n@@ -3258,6 +3269,7 @@ TritonLLVMConversionTarget::TritonLLVMConversionTarget(\n   // addIllegalDialect<triton::TritonDialect>();\n   // addIllegalDialect<triton::gpu::TritonGPUDialect>();\n   addIllegalDialect<mlir::gpu::GPUDialect>();\n+  addIllegalDialect<mlir::StandardOpsDialect>();\n   addLegalOp<mlir::UnrealizedConversionCastOp>();\n }\n "}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -1,4 +1,5 @@\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n+#include \"mlir/Conversion/Passes.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/ExecutionEngine/ExecutionEngine.h\"\n #include \"mlir/ExecutionEngine/OptUtils.h\"\n@@ -135,6 +136,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n       /*printAfterOnlyOnChange=*/true,\n       /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n+  pm.addPass(mlir::createLowerToCFGPass());\n   pm.addPass(createConvertTritonGPUToLLVMPass());\n   // Conanicalize to eliminate the remaining UnrealizedConversionCastOp\n   pm.addPass(mlir::createCanonicalizerPass());"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 9, "deletions": 2, "changes": 11, "file_content_changes": "@@ -3,6 +3,7 @@\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/Verifier.h\"\n \n+#include \"mlir/Conversion/Passes.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Transforms/Passes.h\"\n@@ -1185,8 +1186,12 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::createTritonGPUVerifier());\n            })\n-      .def(\"add_triton_gpu_to_llvm\", [](mlir::PassManager &self) {\n-        self.addPass(mlir::triton::createConvertTritonGPUToLLVMPass());\n+      .def(\"add_triton_gpu_to_llvm\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::triton::createConvertTritonGPUToLLVMPass());\n+           })\n+      .def(\"add_scf_to_cfg\", [](mlir::PassManager &self) {\n+        self.addPass(mlir::createLowerToCFGPass());\n       });\n }\n \n@@ -1205,6 +1210,8 @@ void init_triton_translation(py::module &m) {\n         llvm::LLVMContext llvmContext;\n         auto llvmModule =\n             ::mlir::triton::translateTritonGPUToLLVMIR(&llvmContext, op);\n+        if (!llvmModule)\n+          llvm::report_fatal_error(\"Failed to translate TritonGPU to LLVM IR.\");\n \n         std::string str;\n         llvm::raw_string_ostream os(str);"}, {"filename": "python/tests/test_vecadd.py", "status": "added", "additions": 219, "deletions": 0, "changes": 219, "file_content_changes": "@@ -0,0 +1,219 @@\n+import math\n+import random\n+\n+import pytest\n+import torch\n+from torch.testing import assert_close\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, iter_size', [\n+    [4, 256, 1],\n+    [4, 1024, 256],\n+])\n+def test_vecadd_scf_no_mask(num_warps, block_size, iter_size):\n+\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               z_ptr,\n+               block_size,\n+               iter_size: tl.constexpr):\n+        pid = tl.program_id(axis=0)\n+        for i in range(0, block_size, iter_size):\n+            offset = pid * block_size + tl.arange(0, iter_size)\n+            x_ptrs = x_ptr + offset\n+            y_ptrs = y_ptr + offset\n+            x = tl.load(x_ptrs)\n+            y = tl.load(y_ptrs)\n+            z = x + y\n+            z_ptrs = z_ptr + offset\n+            tl.store(z_ptrs, z)\n+            x_ptr += iter_size\n+            y_ptr += iter_size\n+            z_ptr += iter_size\n+\n+    x = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n+    y = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n+    z = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (x.shape.numel() // (block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z,\n+                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps)\n+\n+    golden_z = x + y\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n+\n+\n+@pytest.mark.parametrize('shape, num_warps, block_size, iter_size', [\n+    [(127, 3), 2, 128, 1],\n+    [(127, 3), 2, 128, 32],\n+])\n+def test_vecadd_scf_mask(shape, num_warps, block_size, iter_size):\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               z_ptr,\n+               num_elements,\n+               block_size: tl.constexpr,\n+               iter_size: tl.constexpr\n+               ):\n+        '''\n+        @block_size: size of a block\n+        @iter_size: size of the iteration, a block has multiple iterations\n+        @num_elements: number of elements\n+        '''\n+        pid = tl.program_id(axis=0)\n+        offset = pid * block_size + tl.arange(0, iter_size)\n+        for i in range(math.ceil(block_size / iter_size)):\n+            x_ptrs = x_ptr + offset\n+            y_ptrs = y_ptr + offset\n+\n+            x = tl.load(x_ptrs, mask=offset < num_elements)\n+            y = tl.load(y_ptrs, mask=offset < num_elements)\n+            z = x + y\n+            z_ptrs = z_ptr + offset\n+            tl.store(z_ptrs, z, mask=offset < num_elements)\n+\n+            x_ptr += iter_size\n+            y_ptr += iter_size\n+            z_ptr += iter_size\n+\n+    x = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    y = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    z = torch.empty(shape, device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (math.ceil(x.numel() / block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z,\n+                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps,\n+                 num_elements=x.numel())\n+\n+    golden_z = x + y\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n+\n+\n+def vecadd_no_scf_tester(num_warps, block_size, shape):\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               z_ptr,\n+               n_elements,\n+               block_size_N: tl.constexpr):\n+        pid = tl.program_id(axis=0)\n+\n+        offset = pid * block_size_N + tl.arange(0, block_size_N)\n+        x_ptrs = x_ptr + offset\n+        y_ptrs = y_ptr + offset\n+\n+        mask = offset < n_elements\n+\n+        x = tl.load(x_ptrs, mask=mask)\n+        y = tl.load(y_ptrs, mask=mask)\n+        z = x + y\n+        z_ptrs = z_ptr + offset\n+        tl.store(z_ptrs, z, mask=mask)\n+\n+    x = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    y = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    z = torch.empty(shape, device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (math.ceil(x.shape.numel() / block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, n_elements=x.shape.numel(), block_size_N=block_size, num_warps=num_warps)\n+\n+    golden_z = x + y\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n+\n+\n+def vecadd_fcmp_no_scf_tester(num_warps, block_size, shape):\n+    '''\n+    vecadd tester with float comparation as load/store mask.\n+    '''\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               z_ptr,\n+               n_elements,\n+               block_size_N: tl.constexpr):\n+        pid = tl.program_id(axis=0)\n+\n+        offset = pid * block_size_N + tl.arange(0, block_size_N)\n+        x_ptrs = x_ptr + offset\n+        y_ptrs = y_ptr + offset\n+\n+        io_mask = offset < n_elements\n+        x = tl.load(x_ptrs, mask=io_mask)\n+        y = tl.load(y_ptrs, mask=io_mask)\n+\n+        z = x + y\n+        val_mask = offset < n_elements and z > 0.\n+\n+        z_ptrs = z_ptr + offset\n+        tl.store(z_ptrs, z, mask=val_mask)\n+\n+    x = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    y = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    z = torch.zeros(shape, device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (math.ceil(x.shape.numel() / block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, n_elements=x.shape.numel(), block_size_N=block_size, num_warps=num_warps)\n+\n+    golden_z: torch.Tensor = x + y\n+    gz_data = torch.flatten(golden_z)\n+    for i in range(golden_z.numel()):\n+        gz_data[i] = gz_data[i] if gz_data[i] > 0. else 0.\n+\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, shape', [\n+    [4, 256, (256,)],\n+    [2, 256, (256,)],\n+    [1, 256, (256,)],\n+    [4, 16, (256,)],\n+    [2, 64, (256,)],\n+    [1, 128, (256,)],\n+])\n+def test_vecadd_no_scf(num_warps, block_size, shape):\n+    vecadd_no_scf_tester(num_warps, block_size, shape)\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, shape', [\n+    [1, 128, (256 + 1,)],\n+    [1, 256, (256 + 1,)],\n+    [2, 256, (3, 256 + 7)],\n+    [4, 256, (3, 256 + 7)],\n+])\n+def test_vecadd__no_scf_masked(num_warps, block_size, shape):\n+    vecadd_no_scf_tester(num_warps, block_size, shape)\n+\n+\n+def test_vecadd_no_scf_masked_randomly():\n+    random.seed(0)  # fix seed to make random test reproducible\n+    for i in range(10):\n+        num_elements = random.randint(128, 2048)\n+        shape = (num_elements,)\n+        max_warps = num_elements // 32  # floor div\n+        for num_warps in range(1, max_warps):\n+            is_power2 = num_warps & (num_warps - 1) == 0 and num_warps != 0\n+            if not is_power2: continue\n+            block_size = min(32, num_warps * 32)\n+            vecadd_no_scf_tester(num_warps, block_size, shape)\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, shape', [\n+    [1, 128, (256 + 1,)],\n+    [1, 256, (256 + 1,)],\n+    [2, 256, (3, 256 + 7)],\n+    [4, 256, (3, 256 + 7)],\n+])\n+def test_vecadd_fcmp_no_scf_masked(num_warps, block_size, shape):\n+    vecadd_fcmp_no_scf_tester(num_warps, block_size, shape)\n+\n+\n+if __name__ == '__main__':\n+    test_vecadd_scf_mask(shape=(128, 3),\n+                         num_warps=2,\n+                         block_size=128,\n+                         iter_size=1)"}, {"filename": "python/tests/test_vecadd_no_scf.py", "status": "removed", "additions": 0, "deletions": 131, "changes": 131, "file_content_changes": "@@ -1,131 +0,0 @@\n-import math\n-import random\n-\n-import pytest\n-import torch\n-from torch.testing import assert_close\n-\n-import triton\n-import triton.language as tl\n-\n-\n-def vecadd_tester(num_warps, block_size, shape):\n-    @triton.jit\n-    def kernel(x_ptr,\n-               y_ptr,\n-               z_ptr,\n-               n_elements,\n-               BLOCK_SIZE_N: tl.constexpr):\n-        pid = tl.program_id(axis=0)\n-\n-        offset = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-        x_ptrs = x_ptr + offset\n-        y_ptrs = y_ptr + offset\n-\n-        mask = offset < n_elements\n-\n-        x = tl.load(x_ptrs, mask=mask)\n-        y = tl.load(y_ptrs, mask=mask)\n-        z = x + y\n-        z_ptrs = z_ptr + offset\n-        tl.store(z_ptrs, z, mask=mask)\n-\n-    x = torch.randn(shape, device='cuda', dtype=torch.float32)\n-    y = torch.randn(shape, device='cuda', dtype=torch.float32)\n-    z = torch.empty(shape, device=x.device, dtype=x.dtype)\n-\n-    grid = lambda EA: (math.ceil(x.shape.numel() / block_size),)\n-    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, n_elements=x.shape.numel(), BLOCK_SIZE_N=block_size, num_warps=num_warps)\n-\n-    golden_z = x + y\n-    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n-\n-\n-def vecadd_fcmp_tester(num_warps, block_size, shape):\n-    '''\n-    vecadd tester with float comparation as load/store mask.\n-    '''\n-    @triton.jit\n-    def kernel(x_ptr,\n-               y_ptr,\n-               z_ptr,\n-               n_elements,\n-               BLOCK_SIZE_N: tl.constexpr):\n-        pid = tl.program_id(axis=0)\n-\n-        offset = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-        x_ptrs = x_ptr + offset\n-        y_ptrs = y_ptr + offset\n-\n-        io_mask = offset < n_elements\n-        x = tl.load(x_ptrs, mask=io_mask)\n-        y = tl.load(y_ptrs, mask=io_mask)\n-\n-        z = x + y\n-        val_mask = offset < n_elements and z > 0.\n-\n-        z_ptrs = z_ptr + offset\n-        tl.store(z_ptrs, z, mask=val_mask)\n-\n-    x = torch.randn(shape, device='cuda', dtype=torch.float32)\n-    y = torch.randn(shape, device='cuda', dtype=torch.float32)\n-    z = torch.zeros(shape, device=x.device, dtype=x.dtype)\n-\n-    grid = lambda EA: (math.ceil(x.shape.numel() / block_size),)\n-    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, n_elements=x.shape.numel(), BLOCK_SIZE_N=block_size, num_warps=num_warps)\n-\n-    golden_z: torch.Tensor = x + y\n-    gz_data = torch.flatten(golden_z)\n-    for i in range(golden_z.numel()):\n-        gz_data[i] = gz_data[i] if gz_data[i] > 0. else 0.\n-\n-    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n-\n-\n-@pytest.mark.parametrize('num_warps, block_size, shape', [\n-    [4, 256, (256,)],\n-    [2, 256, (256,)],\n-    [1, 256, (256,)],\n-    [4, 16, (256,)],\n-    [2, 64, (256,)],\n-    [1, 128, (256,)],\n-])\n-def test_vecadd_no_scf(num_warps, block_size, shape):\n-    vecadd_tester(num_warps, block_size, shape)\n-\n-\n-@pytest.mark.parametrize('num_warps, block_size, shape', [\n-    [1, 128, (256 + 1,)],\n-    [1, 256, (256 + 1,)],\n-    [2, 256, (3, 256 + 7)],\n-    [4, 256, (3, 256 + 7)],\n-])\n-def test_vecadd__no_scf_masked(num_warps, block_size, shape):\n-    vecadd_tester(num_warps, block_size, shape)\n-\n-\n-def test_vecadd_no_scf_masked_randomly():\n-    random.seed(0)  # fix seed to make random test reproducible\n-    for i in range(10):\n-        num_elements = random.randint(128, 2048)\n-        shape = (num_elements,)\n-        max_warps = num_elements // 32  # floor div\n-        for num_warps in range(1, max_warps):\n-            is_power2 = num_warps & (num_warps - 1) == 0 and num_warps != 0\n-            if not is_power2: continue\n-            block_size = min(32, num_warps * 32)\n-            vecadd_tester(num_warps, block_size, shape)\n-\n-\n-@pytest.mark.parametrize('num_warps, block_size, shape', [\n-    [1, 128, (256 + 1,)],\n-    [1, 256, (256 + 1,)],\n-    [2, 256, (3, 256 + 7)],\n-    [4, 256, (3, 256 + 7)],\n-])\n-def test_vecadd_fcmp_no_scf_masked(num_warps, block_size, shape):\n-    vecadd_fcmp_tester(num_warps, block_size, shape)\n-\n-\n-if __name__ == '__main__':\n-    test_vecadd_no_scf_masked_randomly()"}]