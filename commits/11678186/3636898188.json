[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -89,7 +89,7 @@ jobs:\n         if: ${{matrix.runner[0] == 'self-hosted' && matrix.runner[1] == 'V100'}}\n         run: |\n           cd python/tests\n-          pytest test_gemm.py::test_gemm_no_scf_for_mmav1\n+          pytest test_gemm.py::test_gemm_for_mmav1\n \n       - name: Run CXX unittests\n         run: |"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 10, "deletions": 1, "changes": 11, "file_content_changes": "@@ -72,6 +72,11 @@ struct DotOpMmaV1ConversionHelper {\n \n     bool isARow = order[0] != 0;\n     bool isAVec4 = !isARow && shape[order[0]] <= 16; // fp16*4 = 16bytes\n+    // TODO[Superjomn]: Support the case when isAVec4=false later\n+    // Currently, we only support ld.v2, for the mma layout varies with\n+    // different ld vector width.\n+    isAVec4 = true;\n+\n     int packSize0 = (isARow || isAVec4) ? 1 : 2;\n \n     SmallVector<int> fpw({2, 2, 1});\n@@ -98,6 +103,11 @@ struct DotOpMmaV1ConversionHelper {\n     auto order = getOrder();\n     bool isBRow = order[0] != 0;\n     bool isBVec4 = isBRow && shape[order[0]] <= 16;\n+    // TODO[Superjomn]: Support the case when isBVec4=false later\n+    // Currently, we only support ld.v2, for the mma layout varies with\n+    // different ld vector width.\n+    isBVec4 = true;\n+\n     int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n     SmallVector<int> fpw({2, 2, 1});\n     SmallVector<int> rep({0, 2 * packSize1, 1});       // pad M with 0\n@@ -1455,7 +1465,6 @@ Value DotOpMmaV1ConversionHelper::loadB(\n   SmallVector<unsigned> order(sharedLayout.getOrder().begin(),\n                               sharedLayout.getOrder().end());\n \n-\n   Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n   bool isBRow = order[0] != 0;\n   bool isBVec4 = isBRow && shape[order[0]] <= 16;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 10, "deletions": 7, "changes": 17, "file_content_changes": "@@ -3527,7 +3527,7 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n   // The resVals holds the final result of the DotOp.\n   // NOTE The current layout of resVals is different from acc, we call it the\n   // accumulator-external layout. and\n-  SmallVector<Value> resVals(acc.size());\n+  SmallVector<Value> resVals(resSize);\n \n   auto getIdx = [&](int m, int n) {\n     std::vector<size_t> idx{{\n@@ -3545,10 +3545,15 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n \n   { // convert the acc's value from accumuator-external layout to\n     // accumulator-internal layout.\n-    SmallVector<Value> acc2(acc.size());\n-    auto idx = getIdx(0 /*m*/, 0 /*n*/);\n-    for (unsigned i = 0; i < 8; ++i)\n-      acc2[idx[i]] = acc[(0 * numN / 2 + 0) * 8 + i];\n+    SmallVector<Value> acc2(acc);\n+\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      for (unsigned n = 0; n < numN / 2; ++n) {\n+        auto idx = getIdx(m, n);\n+        for (unsigned i = 0; i < 8; ++i)\n+          acc2[idx[i]] = acc[(m * numN / 2 + n) * 8 + i];\n+      }\n+\n     acc = acc2;\n   }\n \n@@ -3589,8 +3594,6 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n     for (unsigned i = 0; i < 8; i++) {\n       Value elem = extract_val(f32_ty, res, getIntAttr(i));\n       acc[idx[i]] = elem;\n-      // TODO[goostavz]: double confirm this when m/n/k = [32, 32, x] has been\n-      // verified before MMA\n       resVals[(m * numN / 2 + n) * 8 + i] = elem;\n     }\n   };"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -1382,10 +1382,11 @@ void init_triton_translation(py::module &m) {\n         llvm::SMDiagnostic error;\n         std::unique_ptr<llvm::Module> module =\n             llvm::parseIR(buffer->getMemBufferRef(), error, context);\n-        if (!module)\n+        if (!module) {\n           llvm::report_fatal_error(\n               \"failed to parse IR: \" + error.getMessage() +\n               \"lineno: \" + std::to_string(error.getLineNo()));\n+        }\n \n         // translate module to PTX\n         auto ptxCode ="}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 2, "deletions": 17, "changes": 19, "file_content_changes": "@@ -295,23 +295,6 @@ def matmul_kernel(\n \n \n # NOTE this is useful only on Volta GPU.\n-@pytest.mark.parametrize('SHAPE,NUM_WARPS,TRANS_A,TRANS_B', [\n-    (shape, num_warps, trans_a, trans_b)\n-    for shape in [\n-        [16, 16, 16],\n-        [16, 16, 32],\n-        [32, 16, 16],\n-        [32, 32, 32],\n-        [128, 16, 16],\n-    ]\n-    for num_warps in [1]\n-    for trans_a in [False]\n-    for trans_b in [False]\n-])\n-def test_gemm_no_scf_for_mmav1(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n-    test_gemm_no_scf(SHAPE, NUM_WARPS, TRANS_A, TRANS_B)\n-\n-\n @pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,TRANS_A,TRANS_B', [\n     # Non-forloop\n     [16, 16, 16, 1, 16, 16, 16, False, False],\n@@ -320,7 +303,9 @@ def test_gemm_no_scf_for_mmav1(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n     [32, 32, 32, 1, 32, 32, 32, False, False],\n     [128, 32, 32, 1, 128, 32, 32, False, False],\n \n+    # split-K\n     [16, 16, 32, 1, 16, 16, 16, False, False],\n+    [64, 64, 128, 1, 64, 64, 32, False, False],\n ])\n def test_gemm_for_mmav1(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n     test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B)"}]