[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 53, "deletions": 11, "changes": 64, "file_content_changes": "@@ -11,7 +11,7 @@\n #include \"Utility.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n-\n+#include <set>\n using namespace mlir;\n using namespace mlir::triton;\n \n@@ -521,6 +521,13 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n           result = emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, type);\n         if (mmaLayout.isAmpere())\n           result = emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, type);\n+      } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+        auto parentLayout = sliceLayout.getParent();\n+        auto parentShape = sliceLayout.paddedShape(type.getShape());\n+        RankedTensorType parentTy = RankedTensorType::get(\n+            parentShape, type.getElementType(), parentLayout);\n+        result = emitBaseIndexForLayout(loc, rewriter, parentLayout, parentTy);\n+        result.erase(result.begin() + sliceLayout.getDim());\n       } else {\n         llvm_unreachable(\"unsupported emitBaseIndexForLayout\");\n       }\n@@ -540,6 +547,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       if (mmaLayout.isAmpere())\n         return emitOffsetForMmaLayoutV2(mmaLayout, type);\n     }\n+    if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>())\n+      return emitOffsetForSliceLayout(sliceLayout, type);\n     llvm_unreachable(\"unsupported emitOffsetForLayout\");\n   }\n \n@@ -879,24 +888,57 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return multiDimIdx;\n   }\n \n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForSliceLayout(const SliceEncodingAttr &sliceLayout,\n+                           RankedTensorType type) const {\n+    auto parentEncoding = sliceLayout.getParent();\n+    unsigned dim = sliceLayout.getDim();\n+    auto parentShape = sliceLayout.paddedShape(type.getShape());\n+    RankedTensorType parentTy = RankedTensorType::get(\n+        parentShape, type.getElementType(), parentEncoding);\n+    auto parentOffsets = emitOffsetForLayout(parentEncoding, parentTy);\n+\n+    unsigned numOffsets = parentOffsets.size();\n+    SmallVector<SmallVector<unsigned>> resultOffsets;\n+    std::set<SmallVector<unsigned>> unique_offsets;\n+\n+    for (unsigned i = 0; i < numOffsets; ++i) {\n+      SmallVector<unsigned> offsets = parentOffsets[i];\n+      offsets.erase(offsets.begin() + dim);\n+      if (unique_offsets.find(offsets) == unique_offsets.end()) {\n+        resultOffsets.push_back(offsets);\n+        unique_offsets.insert(offsets);\n+      }\n+    }\n+    return resultOffsets;\n+  }\n+\n   SmallVector<SmallVector<Value>>\n   emitIndicesForSliceLayout(Location loc, ConversionPatternRewriter &rewriter,\n                             const SliceEncodingAttr &sliceLayout,\n                             RankedTensorType type) const {\n     auto parentEncoding = sliceLayout.getParent();\n-    unsigned dim = sliceLayout.getDim();\n     auto parentShape = sliceLayout.paddedShape(type.getShape());\n     RankedTensorType parentTy = RankedTensorType::get(\n         parentShape, type.getElementType(), parentEncoding);\n-    auto parentIndices = emitIndices(loc, rewriter, parentEncoding, parentTy);\n-    unsigned numIndices = parentIndices.size();\n-    SmallVector<SmallVector<Value>> resultIndices;\n-    for (unsigned i = 0; i < numIndices; ++i) {\n-      SmallVector<Value> indices = parentIndices[i];\n-      indices.erase(indices.begin() + dim);\n-      resultIndices.push_back(indices);\n-    }\n-    return resultIndices;\n+\n+    unsigned dim = sliceLayout.getDim();\n+    // step 1, delinearize threadId to get the base index\n+    auto multiDimBase =\n+        emitBaseIndexForLayout(loc, rewriter, sliceLayout, type);\n+    // step 2, get offset of each element\n+    auto offset = emitOffsetForSliceLayout(sliceLayout, type);\n+    // step 3, add offset to base, and reorder the sequence of indices to\n+    // guarantee that elems in the same sizePerThread are adjacent in order\n+    auto shape = type.getShape();\n+    unsigned rank = shape.size();\n+    unsigned elemsPerThread = offset.size();\n+    SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread,\n+                                                SmallVector<Value>(rank));\n+    for (unsigned n = 0; n < elemsPerThread; ++n)\n+      for (unsigned k = 0; k < rank; ++k)\n+        multiDimIdx[n][k] = add(multiDimBase[k], i32_val(offset[n][k]));\n+    return multiDimIdx;\n   }\n \n protected:"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 28, "deletions": 7, "changes": 35, "file_content_changes": "@@ -129,9 +129,9 @@ struct ViewOpConversion : public ConvertTritonGPUOpToLLVMPattern<ViewOp> {\n     auto resultTy = op.getType().template cast<RankedTensorType>();\n     auto vals = this->getTypeConverter()->unpackLLElements(\n         loc, adaptor.getSrc(), rewriter, op.getOperand().getType());\n-    Value view =\n+    Value ret =\n         this->getTypeConverter()->packLLElements(loc, vals, rewriter, resultTy);\n-    rewriter.replaceOp(op, view);\n+    rewriter.replaceOp(op, ret);\n     return success();\n   }\n };\n@@ -147,12 +147,33 @@ struct ExpandDimsOpConversion\n   matchAndRewrite(ExpandDimsOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n-    auto resultTy = op.getType().template cast<RankedTensorType>();\n-    auto vals = this->getTypeConverter()->unpackLLElements(\n+    auto srcVals = this->getTypeConverter()->unpackLLElements(\n         loc, adaptor.getSrc(), rewriter, op.getOperand().getType());\n-    Value view =\n-        this->getTypeConverter()->packLLElements(loc, vals, rewriter, resultTy);\n-    rewriter.replaceOp(op, view);\n+\n+    auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n+    auto resultTy = op.getType().template cast<RankedTensorType>();\n+\n+    assert(srcTy.getEncoding().isa<SliceEncodingAttr>() &&\n+           \"ExpandDimsOp only support SliceEncodingAttr\");\n+    auto srcLayout = srcTy.getEncoding().dyn_cast<SliceEncodingAttr>();\n+    auto resultLayout = resultTy.getEncoding();\n+\n+    auto srcOffsets = emitOffsetForLayout(srcLayout, srcTy);\n+    auto resultOffsets = emitOffsetForLayout(resultLayout, resultTy);\n+    DenseMap<SmallVector<unsigned>, Value, SmallVectorKeyInfo> srcValues;\n+    for (size_t i = 0; i < srcOffsets.size(); i++) {\n+      srcValues[srcOffsets[i]] = srcVals[i];\n+    }\n+\n+    SmallVector<Value> resultVals;\n+    for (size_t i = 0; i < resultOffsets.size(); i++) {\n+      auto offset = resultOffsets[i];\n+      offset.erase(offset.begin() + srcLayout.getDim());\n+      resultVals.push_back(srcValues.lookup(offset));\n+    }\n+    Value ret = this->getTypeConverter()->packLLElements(loc, resultVals,\n+                                                         rewriter, resultTy);\n+    rewriter.replaceOp(op, ret);\n     return success();\n   }\n };"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -103,10 +103,10 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n     return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n                                  blockedLayout.getSizePerThread().end());\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-    auto ret = getSizePerThread(sliceLayout.getParent());\n-    return ret;\n-    // ret.erase(ret.begin() + sliceLayout.getDim());\n-    return ret;\n+    // TODO: maybe should not be supported\n+    auto sizePerThread = getSizePerThread(sliceLayout.getParent());\n+    sizePerThread.erase(sizePerThread.begin() + sliceLayout.getDim());\n+    return sizePerThread;\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     if (mmaLayout.isAmpere()) {\n       return {2, 2};\n@@ -375,6 +375,7 @@ SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n   auto parent = getParent();\n   auto parentElemsPerThread =\n       ::getElemsPerThread(parent, paddedShape(shape), eltTy);\n+  parentElemsPerThread.erase(parentElemsPerThread.begin() + getDim());\n   return parentElemsPerThread;\n }\n unsigned SliceEncodingAttr::getTotalElemsPerThread(ArrayRef<int64_t> shape,"}]