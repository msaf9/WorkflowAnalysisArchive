[{"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1061,7 +1061,6 @@ class BlockedToMMA : public mlir::RewritePattern {\n \n     // for FMA, should retain the blocked layout.\n     int versionMajor = computeCapabilityToMMAVersion(computeCapability);\n-    versionMajor = 1; // DEBUG\n     if (!supportMMA(dotOp, versionMajor))\n       return failure();\n "}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 20, "deletions": 21, "changes": 41, "file_content_changes": "@@ -758,12 +758,12 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n \n // -----\n \n-#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [1, 0]}>\n-#mma = #triton_gpu.mma<{versionMajor = 1, warpsPerCTA = [2, 1]}>\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#mma = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [2, 2]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mmav1_block\n-  func @convert_layout_mmav1_blocked(%arg0: tensor<32x16xf32, #mma>) {\n+  func @convert_layout_mmav1_blocked(%arg0: tensor<32x64xf32, #mma>) {\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n     // CHECK: llvm.store\n@@ -775,13 +775,12 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: nvvm.barrier0\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n-    %0 = triton_gpu.convert_layout %arg0 : (tensor<32x16xf32, #mma>) -> tensor<32x16xf32, #blocked0>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<32x64xf32, #mma>) -> tensor<32x64xf32, #blocked>\n     return\n   }\n }\n \n // -----\n-\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n #shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n@@ -868,24 +867,24 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n // -----\n \n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n-#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n-#mma = #triton_gpu.mma<{versionMajor = 1, warpsPerCTA = [2, 2]}>\n+#shared0 = #triton_gpu.shared<{vec = 4, perPhase = 1, maxPhase = 8, order = [1, 0]}>\n+#shared1 = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n+#mma = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [2, 2]}>\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma, isMMAv1Row=true}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma, isMMAv1Row=true}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   func @matmul884_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n-  %a:tensor<128x32xf16, #shared>, %b:tensor<32x256xf16, #shared>) {\n-    %cst = arith.constant dense<0.000000e+00> : tensor<128x256xf32, #mma>\n+  %a:tensor<32x64xf16, #shared0>, %b:tensor<64x64xf16, #shared1>) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x64xf32, #mma>\n     // CHECK: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    %a_mat = triton_gpu.convert_layout %a : (tensor<128x32xf16, #shared>) -> tensor<128x32xf16, #dot_operand_a>\n-    %b_mat = triton_gpu.convert_layout %b : (tensor<32x256xf16, #shared>) -> tensor<32x256xf16, #dot_operand_b>\n+    %a_mat = triton_gpu.convert_layout %a : (tensor<32x64xf16, #shared0>) -> tensor<32x64xf16, #dot_operand_a>\n+    %b_mat = triton_gpu.convert_layout %b : (tensor<64x64xf16, #shared1>) -> tensor<64x64xf16, #dot_operand_b>\n \n-    %28 = tt.dot %a_mat, %b_mat, %cst {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #dot_operand_a> * tensor<32x256xf16, #dot_operand_b> -> tensor<128x256xf32, #mma>\n-    // TODO[goostavz]: uncomment the following lines after convert_layout[mma<v1> -> blocked] is ready.\n-    // %38 = triton_gpu.convert_layout %28 : (tensor<128x256xf32, #mma>) -> tensor<128x256xf32, #blocked>\n-    // %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #blocked>\n-    // %36 = tt.broadcast %30 : (tensor<128x1x!tt.ptr<f32>, #blocked>) -> tensor<128x256x!tt.ptr<f32>, #blocked>\n-    // tt.store %36, %38 : tensor<128x256xf32, #blocked>\n+    %28 = tt.dot %a_mat, %b_mat, %cst {allowTF32 = true, transA = false, transB = false} : tensor<32x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<32x64xf32, #mma>\n+    %38 = triton_gpu.convert_layout %28 : (tensor<32x64xf32, #mma>) -> tensor<32x64xf32, #blocked>\n+    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n+    %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x64x!tt.ptr<f32>, #blocked>\n+    tt.store %36, %38 : tensor<32x64xf32, #blocked>\n     return\n   }\n }\n@@ -999,15 +998,15 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %v1 = arith.addi %v0, %blockdimz : i32\n     %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n     tt.store %a, %0 : tensor<32xi32, #blocked0>\n-  \n+\n     return\n   }\n }\n \n // -----\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  // CHECK-LABEL: test_index_cache \n+  // CHECK-LABEL: test_index_cache\n   func @test_index_cache() {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n     %0 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n@@ -1021,7 +1020,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n #shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK-LABEL: test_base_index_cache \n+  // CHECK-LABEL: test_base_index_cache\n   func @test_base_index_cache(%arg0: tensor<128x32xf32, #blocked0>) {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n@@ -1045,4 +1044,4 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     }\n     return\n   }\n-}\n\\ No newline at end of file\n+}"}, {"filename": "test/TritonGPU/update-mma-for-volta.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -12,7 +12,7 @@\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n // It creates a new MMA layout to fit with $a and $b's dot_operand, and get the right warpsPerCTA\n // The ID of this MMA instance should be 0.\n-// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 4]}>\n+// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 2]}>\n module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n   // CHECK-LABEL: dot_mmav1\n   func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n@@ -40,8 +40,8 @@ module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n #mma1 = #triton_gpu.mma<{versionMajor=1, versionMinor=16, warpsPerCTA=[4,4]}>\n \n // Will still get two MMA layouts\n-// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 4]}>\n-// CHECK: [[new_mma1:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 19, warpsPerCTA = [4, 4]}>\n+// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 2]}>\n+// CHECK: [[new_mma1:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 19, warpsPerCTA = [4, 2]}>\n \n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>"}]