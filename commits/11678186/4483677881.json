[{"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv2.cpp", "status": "modified", "additions": 70, "deletions": 14, "changes": 84, "file_content_changes": "@@ -9,7 +9,10 @@ using ::mlir::triton::gpu::MmaEncodingAttr;\n \n using ValueTableV2 = std::map<std::pair<unsigned, unsigned>, Value>;\n \n-Value loadC(Value tensor, Value llTensor) {\n+Value loadC(Value tensor, Value llTensor,\n+            TritonGPUToLLVMTypeConverter *typeConverter, Location loc,\n+            ConversionPatternRewriter &rewriter) {\n+  MLIRContext *ctx = tensor.getContext();\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n   size_t fcSize = triton::gpu::getElemsPerThread(tensor.getType());\n \n@@ -21,6 +24,32 @@ Value loadC(Value tensor, Value llTensor) {\n   assert(structTy.getBody().size() == fcSize &&\n          \"DotOp's $c operand should pass the same number of values as $d in \"\n          \"mma layout.\");\n+\n+  auto numMmaRets = tensorTy.getElementType().getIntOrFloatBitWidth() / 8;\n+  assert(numMmaRets == 4 || numMmaRets == 2);\n+  if (numMmaRets == 4) {\n+    return llTensor;\n+  } else if (numMmaRets == 2) {\n+    auto cPack = SmallVector<Value>();\n+    auto cElemTy = tensorTy.getElementType();\n+    int numCPackedElem = 4 / numMmaRets;\n+    Type cPackTy = vec_ty(cElemTy, numCPackedElem);\n+    for (int i = 0; i < fcSize; i += numCPackedElem) {\n+      Value pack = rewriter.create<LLVM::UndefOp>(loc, cPackTy);\n+      for (int j = 0; j < numCPackedElem; ++j) {\n+        pack = insert_element(\n+            cPackTy, pack, extract_val(cElemTy, llTensor, i + j), i32_val(j));\n+      }\n+      cPack.push_back(pack);\n+    }\n+\n+    Type structTy = LLVM::LLVMStructType::getLiteral(\n+        ctx, SmallVector<Type>(cPack.size(), cPackTy));\n+    Value result =\n+        typeConverter->packLLElements(loc, cPack, rewriter, structTy);\n+    return result;\n+  }\n+\n   return llTensor;\n }\n \n@@ -48,6 +77,7 @@ enum class TensorCoreType : uint8_t {\n   FP32_FP16_FP16_FP32 = 0, // default\n   FP32_BF16_BF16_FP32,\n   FP32_TF32_TF32_FP32,\n+  FP16_FP16_FP16_FP16,\n   // integer tensor core instr\n   INT32_INT1_INT1_INT32, // Not implemented\n   INT32_INT4_INT4_INT32, // Not implemented\n@@ -58,18 +88,23 @@ enum class TensorCoreType : uint8_t {\n \n Type getMmaRetType(TensorCoreType mmaType, MLIRContext *ctx) {\n   Type fp32Ty = type::f32Ty(ctx);\n+  Type fp16Ty = type::f16Ty(ctx);\n   Type i32Ty = type::i32Ty(ctx);\n   Type fp32x4Ty =\n       LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n   Type i32x4Ty =\n       LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i32Ty));\n+  Type fp16x2Pack2Ty = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(2, vec_ty(fp16Ty, 2)));\n   switch (mmaType) {\n   case TensorCoreType::FP32_FP16_FP16_FP32:\n     return fp32x4Ty;\n   case TensorCoreType::FP32_BF16_BF16_FP32:\n     return fp32x4Ty;\n   case TensorCoreType::FP32_TF32_TF32_FP32:\n     return fp32x4Ty;\n+  case TensorCoreType::FP16_FP16_FP16_FP16:\n+    return fp16x2Pack2Ty;\n   case TensorCoreType::INT32_INT8_INT8_INT32:\n     return i32x4Ty;\n   default:\n@@ -98,6 +133,9 @@ TensorCoreType getMmaType(triton::DotOp op) {\n   } else if (dTy.getElementType().isInteger(32)) {\n     if (aTy.getElementType().isInteger(8) && bTy.getElementType().isInteger(8))\n       return TensorCoreType::INT32_INT8_INT8_INT32;\n+  } else if (dTy.getElementType().isF16()) {\n+    if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n+      return TensorCoreType::FP16_FP16_FP16_FP16;\n   }\n \n   return TensorCoreType::NOT_APPLICABLE;\n@@ -117,13 +155,17 @@ inline static const std::map<TensorCoreType, std::string> mmaInstrPtx = {\n      \"mma.sync.aligned.m16n8k64.row.col.satfinite.s32.s4.s4.s32\"},\n     {TensorCoreType::INT32_INT8_INT8_INT32,\n      \"mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\"},\n+\n+    {TensorCoreType::FP16_FP16_FP16_FP16,\n+     \"mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\"},\n };\n \n LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n                          ConversionPatternRewriter &rewriter, Location loc,\n                          Value a, Value b, Value c, Value d, Value loadedA,\n                          Value loadedB, Value loadedC, DotOp op,\n                          DotOpAdaptor adaptor) {\n+  MLIRContext *ctx = c.getContext();\n   auto aTensorTy = a.getType().cast<RankedTensorType>();\n   auto bTensorTy = b.getType().cast<RankedTensorType>();\n   auto dTensorTy = d.getType().cast<RankedTensorType>();\n@@ -149,6 +191,8 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n                                                 loadedB, std::max(repN / 2, 1),\n                                                 repK, bTensorTy);\n   auto fc = typeConverter->unpackLLElements(loc, loadedC, rewriter, dTensorTy);\n+  auto numMmaRets = dTensorTy.getElementType().getIntOrFloatBitWidth() / 8;\n+  int numCPackedElem = 4 / numMmaRets;\n \n   auto mmaType = getMmaType(op);\n \n@@ -158,7 +202,9 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n     auto &mma = *builder.create(mmaInstrPtx.at(mmaType));\n     // using =r for float32 works but leads to less readable ptx.\n     bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n-    auto retArgs = builder.newListOperand(4, isIntMMA ? \"=r\" : \"=f\");\n+    bool isAccF16 = dTensorTy.getElementType().isF16();\n+    auto retArgs =\n+        builder.newListOperand(numMmaRets, isIntMMA || isAccF16 ? \"=r\" : \"=f\");\n     auto aArgs = builder.newListOperand({\n         {ha[{m, k}], \"r\"},\n         {ha[{m + 1, k}], \"r\"},\n@@ -168,9 +214,10 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n     auto bArgs =\n         builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n     auto cArgs = builder.newListOperand();\n-    for (int i = 0; i < 4; ++i) {\n-      cArgs->listAppend(builder.newOperand(fc[m * colsPerThread + 4 * n + i],\n-                                           std::to_string(i)));\n+    for (int i = 0; i < numMmaRets; ++i) {\n+      cArgs->listAppend(builder.newOperand(\n+          fc[(m * colsPerThread + 4 * n) / numCPackedElem + i],\n+          std::to_string(i)));\n       // reuse the output registers\n     }\n \n@@ -179,8 +226,10 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n         builder.launch(rewriter, loc, getMmaRetType(mmaType, op.getContext()));\n \n     Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n-    for (int i = 0; i < 4; ++i)\n-      fc[m * colsPerThread + 4 * n + i] = extract_val(elemTy, mmaOut, i);\n+    for (int i = 0; i < numMmaRets; ++i) {\n+      fc[(m * colsPerThread + 4 * n) / numCPackedElem + i] =\n+          extract_val(elemTy, mmaOut, i);\n+    }\n   };\n \n   for (int k = 0; k < repK; ++k)\n@@ -190,14 +239,20 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n \n   Type resElemTy = dTensorTy.getElementType();\n \n-  for (auto &elem : fc) {\n-    elem = bitcast(elem, resElemTy);\n-  }\n-\n   // replace with new packed result\n   Type structTy = LLVM::LLVMStructType::getLiteral(\n-      op.getContext(), SmallVector<Type>(fc.size(), resElemTy));\n-  Value res = typeConverter->packLLElements(loc, fc, rewriter, structTy);\n+      ctx, SmallVector<Type>(fc.size() * numCPackedElem, resElemTy));\n+  SmallVector<Value> results(fc.size() * numCPackedElem);\n+  for (int i = 0; i < fc.size(); ++i) {\n+    for (int j = 0; j < numCPackedElem; ++j) {\n+      results[i * numCPackedElem + j] =\n+          numCPackedElem > 1\n+              ? bitcast(extract_element(fc[i], i32_val(j)), resElemTy)\n+              : bitcast(fc[i], resElemTy);\n+    }\n+  }\n+  Value res = typeConverter->packLLElements(loc, results, rewriter, structTy);\n+\n   rewriter.replaceOp(op, res);\n \n   return success();\n@@ -228,7 +283,8 @@ LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n   Value loadedA, loadedB, loadedC;\n   loadedA = adaptor.getA();\n   loadedB = adaptor.getB();\n-  loadedC = loadC(op.getC(), adaptor.getC());\n+  loadedC =\n+      loadC(op.getC(), adaptor.getC(), typeConverter, op.getLoc(), rewriter);\n \n   return convertDot(typeConverter, rewriter, op.getLoc(), A, B, C, op.getD(),\n                     loadedA, loadedB, loadedC, op, adaptor);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 126, "deletions": 43, "changes": 169, "file_content_changes": "@@ -25,9 +25,17 @@ namespace ttg = triton::gpu;\n static Type getI1SameShape(Value v) {\n   Type vType = v.getType();\n   auto i1Type = IntegerType::get(vType.getContext(), 1);\n-  auto tensorType = vType.cast<RankedTensorType>();\n-  return RankedTensorType::get(tensorType.getShape(), i1Type,\n-                               tensorType.getEncoding());\n+  if (auto tensorType = vType.dyn_cast<RankedTensorType>())\n+    return RankedTensorType::get(tensorType.getShape(), i1Type,\n+                                 tensorType.getEncoding());\n+  return i1Type;\n+}\n+\n+// pass named attrs (e.g., tt.contiguity) from Triton to Triton\n+static void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n+  for (const NamedAttribute attr : dictAttrs.getValue())\n+    if (!op->hasAttr(attr.getName()))\n+      op->setAttr(attr.getName(), attr.getValue());\n }\n \n #define int_attr(num) builder.getI64IntegerAttr(num)\n@@ -69,6 +77,23 @@ class LoopPipeliner {\n   /// Block arguments that loads depend on\n   SetVector<BlockArgument> depArgs;\n \n+  /// If we have a load that immediately depends on a block argument in the\n+  /// current iteration, it is an immediate dependency. Otherwise, it is a\n+  /// non-immediate dependency, which means the load depends on a block argument\n+  /// in the previous iterations.\n+  /// For example:\n+  /// scf.for (%arg0, %arg1, %arg2) {\n+  ///   %0 = load %arg0  <--- immediate dep, this address is initialized at\n+  ///   numStages-2\n+  ///   %1 = load %arg1\n+  ///   %2 = add %1, %arg2\n+  ///   %3 = load %2  <--- non-immediate dep, %arg1 must be an update-to-date\n+  ///   value\n+  /// }\n+  SetVector<BlockArgument> immedidateDepArgs;\n+\n+  SetVector<BlockArgument> nonImmedidateDepArgs;\n+\n   /// Operations (inside the loop body) that loads depend on\n   SetVector<Operation *> depOps;\n \n@@ -79,6 +104,9 @@ class LoopPipeliner {\n \n   Value lookupOrDefault(Value origin, int stage);\n \n+  Value getLoadMask(triton::LoadOp loadOp, Value mappedMask, Value loopCond,\n+                    OpBuilder &builder);\n+\n   /// Returns a empty buffer of size <numStages, ...>\n   ttg::AllocTensorOp allocateEmptyBuffer(Operation *op, OpBuilder &builder);\n \n@@ -170,7 +198,7 @@ LogicalResult LoopPipeliner::initialize() {\n   }\n \n   // can we use forOp.walk(...) here?\n-  SmallVector<triton::LoadOp, 2> allLoads;\n+  SmallVector<triton::LoadOp, 2> validLoads;\n   for (Operation &op : *loop)\n     if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n       auto ptr = loadOp.getPtr();\n@@ -184,30 +212,31 @@ LogicalResult LoopPipeliner::initialize() {\n                     .cast<triton::PointerType>()\n                     .getPointeeType();\n       unsigned width = vec * ty.getIntOrFloatBitWidth();\n+      // cp.async's cp-size can only be 4, 8 and 16.\n       if (width >= 32)\n-        allLoads.push_back(loadOp);\n+        validLoads.push_back(loadOp);\n     }\n \n   // Early stop: no need to continue if there is no load in the loop.\n-  if (allLoads.empty())\n+  if (validLoads.empty())\n     return failure();\n \n   // load => values that it depends on\n   DenseMap<Value, SetVector<Value>> loadDeps;\n-  for (triton::LoadOp loadOp : allLoads) {\n+  for (triton::LoadOp loadOp : validLoads) {\n     SetVector<Value> deps;\n     for (Value op : loadOp->getOperands())\n       collectDeps(op, numStages - 1, deps);\n     loadDeps[loadOp] = deps;\n   }\n \n-  // Don't pipeline loads that depend on other loads\n-  // (Because if a load depends on another load, this load needs to wait on the\n-  //  other load in the prologue, which is against the point of the pipeline\n-  //  pass)\n-  for (triton::LoadOp loadOp : allLoads) {\n+  // Don't pipeline valid loads that depend on other valid loads\n+  // (Because if a valid load depends on another valid load, this load needs to\n+  // wait on the other load in the prologue, which is against the point of the\n+  // pipeline pass)\n+  for (triton::LoadOp loadOp : validLoads) {\n     bool isCandidate = true;\n-    for (triton::LoadOp other : allLoads) {\n+    for (triton::LoadOp other : validLoads) {\n       if (loadDeps[loadOp].contains(other)) {\n         isCandidate = false;\n         break;\n@@ -264,20 +293,54 @@ LogicalResult LoopPipeliner::initialize() {\n   if (!loads.empty()) {\n     // Update depArgs & depOps\n     for (Value loadOp : loads) {\n-      for (Value dep : loadDeps[loadOp]) {\n-        // TODO: we should record the stage that the value is depended on\n-        if (auto arg = dep.dyn_cast<BlockArgument>())\n+      auto &deps = loadDeps[loadOp];\n+      for (auto &dep : deps) {\n+        if (auto arg = dep.dyn_cast<BlockArgument>()) {\n           depArgs.insert(arg);\n-        else\n+          if (deps.front().isa<BlockArgument>()) {\n+            immedidateDepArgs.insert(arg);\n+          } else {\n+            nonImmedidateDepArgs.insert(arg);\n+          }\n+        } else\n           depOps.insert(dep.getDefiningOp());\n       }\n     }\n     return success();\n   }\n \n+  // Check if immedidateDepArgs and nonImmedidateDepArgs are disjoint\n+  // If yes, we cannot pipeline the loop for now\n+  for (BlockArgument arg : immedidateDepArgs)\n+    if (nonImmedidateDepArgs.contains(arg)) {\n+      return failure();\n+    }\n+\n   return failure();\n }\n \n+Value LoopPipeliner::getLoadMask(triton::LoadOp loadOp, Value mappedMask,\n+                                 Value loopCond, OpBuilder &builder) {\n+  Type maskType = getI1SameShape(loadOp);\n+  Value mask = loadOp.getMask();\n+  Value newMask;\n+  if (mask) {\n+    Value cond = loopCond;\n+    if (isa<RankedTensorType>(maskType)) {\n+      cond = builder.create<triton::SplatOp>(mask.getLoc(), maskType, loopCond);\n+    }\n+    newMask = builder.create<arith::AndIOp>(mask.getLoc(), mappedMask, cond);\n+  } else {\n+    if (isa<RankedTensorType>(maskType)) {\n+      newMask = builder.create<triton::SplatOp>(loopCond.getLoc(), maskType,\n+                                                loopCond);\n+    } else {\n+      newMask = loopCond;\n+    }\n+  }\n+  return newMask;\n+}\n+\n void LoopPipeliner::emitPrologue() {\n   // llvm::errs() << \"loads to pipeline...:\\n\";\n   // for (Value load : loads)\n@@ -322,17 +385,9 @@ void LoopPipeliner::emitPrologue() {\n         }\n         // load => copy async\n         if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n-          Value mask = lookupOrDefault(loadOp.getMask(), stage);\n-          Value newMask;\n-          if (mask) {\n-            Value splatCond = builder.create<triton::SplatOp>(\n-                mask.getLoc(), mask.getType(), loopCond);\n-            newMask =\n-                builder.create<arith::AndIOp>(mask.getLoc(), mask, splatCond);\n-          } else {\n-            newMask = builder.create<triton::SplatOp>(\n-                loopCond.getLoc(), getI1SameShape(loadOp), loopCond);\n-          }\n+          Value newMask =\n+              getLoadMask(loadOp, lookupOrDefault(loadOp.getMask(), stage),\n+                          loopCond, builder);\n           // TODO: check if the hardware supports async copy\n           newOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n               op->getLoc(), loadsBuffer[loadOp].getType(),\n@@ -345,7 +400,19 @@ void LoopPipeliner::emitPrologue() {\n         } else\n           llvm_unreachable(\"This should be LoadOp\");\n       } else {\n-        newOp = builder.clone(*op);\n+        if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n+          Value newMask =\n+              getLoadMask(loadOp, lookupOrDefault(loadOp.getMask(), stage),\n+                          loopCond, builder);\n+          newOp = builder.create<triton::LoadOp>(\n+              loadOp.getLoc(), loadOp.getResult().getType(),\n+              lookupOrDefault(loadOp.getPtr(), stage), newMask,\n+              lookupOrDefault(loadOp.getOther(), stage), loadOp.getCache(),\n+              loadOp.getEvict(), loadOp.getIsVolatile());\n+          addNamedAttrs(newOp, op->getAttrDictionary());\n+        } else {\n+          newOp = builder.clone(*op);\n+        }\n         // Update loop-carried uses\n         for (unsigned opIdx = 0; opIdx < op->getNumOperands(); ++opIdx) {\n           auto it = valueMapping.find(op->getOperand(opIdx));\n@@ -429,7 +496,10 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   //   (original args)\n   //   (insertSliceAsync buffer at stage numStages - 1) for each load\n   //   (extracted tensor) for each load\n-  //   (depArgs at stage numStages - 2)\n+  //   (depArgs at stage numStages - 1):\n+  //   for each dep arg that is not an immediate block argument\n+  //   (depArgs at stage numStages - 2):\n+  //   for each dep arg that is an immediate block argument\n   //   (iv at stage numStages - 2)\n   //   (pipeline iteration index)\n   //   (loop iteration index)\n@@ -450,7 +520,10 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   size_t depArgsBeginIdx = newLoopArgs.size();\n   for (BlockArgument depArg : depArgs) {\n     depArgsIdx[depArg] = newLoopArgs.size();\n-    newLoopArgs.push_back(valueMapping[depArg][numStages - 2]);\n+    if (immedidateDepArgs.contains(depArg)) {\n+      newLoopArgs.push_back(valueMapping[depArg][numStages - 2]);\n+    } else\n+      newLoopArgs.push_back(valueMapping[depArg][numStages - 1]);\n   }\n \n   size_t nextIVIdx = newLoopArgs.size();\n@@ -549,7 +622,21 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   for (Operation *op : orderedDeps)\n     if (!loads.contains(op->getResult(0))) {\n-      Operation *nextOp = builder.clone(*op, nextMapping);\n+      Operation *nextOp;\n+      if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n+        auto newMask =\n+            getLoadMask(loadOp, nextMapping.lookupOrDefault(loadOp.getMask()),\n+                        nextLoopCond, builder);\n+        nextOp = builder.create<triton::LoadOp>(\n+            loadOp.getLoc(), loadOp.getResult().getType(),\n+            nextMapping.lookupOrDefault(loadOp.getPtr()), newMask,\n+            nextMapping.lookupOrDefault(loadOp.getOther()), loadOp.getCache(),\n+            loadOp.getEvict(), loadOp.getIsVolatile());\n+        addNamedAttrs(nextOp, op->getAttrDictionary());\n+        nextMapping.map(loadOp.getResult(), nextOp->getResult(0));\n+      } else {\n+        nextOp = builder.clone(*op, nextMapping);\n+      }\n \n       auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n@@ -571,21 +658,17 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n     // Update loading mask\n     if (loads.contains(op->getResult(0))) {\n       auto loadOp = llvm::cast<triton::LoadOp>(op);\n-      Value mask = loadOp.getMask();\n-      Value newMask;\n+      auto mask = loadOp.getMask();\n+      auto newMask =\n+          getLoadMask(loadOp, nextMapping.lookupOrDefault(loadOp.getMask()),\n+                      nextLoopCond, builder);\n       if (mask) {\n-        Value splatCond = builder.create<triton::SplatOp>(\n-            mask.getLoc(), mask.getType(), nextLoopCond);\n-        newMask = builder.create<arith::AndIOp>(\n-            mask.getLoc(), splatCond, nextMapping.lookupOrDefault(mask));\n         // If mask is defined outside the loop, don't update the map more than\n         // once\n         if (!(forOp.isDefinedOutsideOfLoop(mask) && nextMapping.contains(mask)))\n-          nextMapping.map(mask, newMask);\n-        newMask = nextMapping.lookupOrDefault(loadOp.getMask());\n-      } else\n-        newMask = builder.create<triton::SplatOp>(\n-            loadOp.getLoc(), getI1SameShape(loadOp), nextLoopCond);\n+          nextMapping.map(loadOp.getMask(), newMask);\n+        newMask = nextMapping.lookupOrDefault(mask);\n+      }\n       Value insertAsyncOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n           op->getLoc(), loadsBuffer[loadOp].getType(),\n           nextMapping.lookupOrDefault(loadOp.getPtr()),"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 35, "deletions": 20, "changes": 55, "file_content_changes": "@@ -84,41 +84,46 @@ class DecomposeDotOperand : public mlir::RewritePattern {\n   }\n };\n \n+// It's beneficial to move the conversion\n+// to after the reduce if necessary since it will be\n+// done on a rank-reduced tensor hence cheaper\n class SimplifyReduceCvt : public mlir::RewritePattern {\n public:\n   explicit SimplifyReduceCvt(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::ReduceOp::getOperationName(), 2, context) {\n-  }\n+      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n+                             2, context) {}\n \n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n-    auto reduce = cast<triton::ReduceOp>(*op);\n-    auto reduceArg = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n-        reduce.getOperand().getDefiningOp());\n-    if (!reduceArg)\n+    if (!llvm::isa<triton::gpu::ConvertLayoutOp>(op))\n+      return mlir::failure();\n+    auto convert = llvm::cast<triton::gpu::ConvertLayoutOp>(op);\n+    triton::ReduceOp reduce;\n+    for (auto &use : convert.getOperand().getUses()) {\n+      auto owner = use.getOwner();\n+      if (llvm::isa_and_nonnull<triton::ReduceOp>(owner)) {\n+        reduce = llvm::cast<triton::ReduceOp>(owner);\n+        break;\n+      }\n+    }\n+    if (!reduce)\n       return mlir::failure();\n     // this may generate unsupported conversions in the LLVM codegen\n-    if (reduceArg.getOperand()\n+    if (convert.getOperand()\n             .getType()\n             .cast<RankedTensorType>()\n             .getEncoding()\n             .isa<triton::gpu::MmaEncodingAttr>())\n       return mlir::failure();\n     auto newReduce = rewriter.create<triton::ReduceOp>(\n-        op->getLoc(), reduce.getRedOp(), reduceArg.getOperand(),\n+        op->getLoc(), reduce.getRedOp(), convert.getOperand(),\n         reduce.getAxis());\n-    if (isa_and_nonnull<triton::gpu::ConvertLayoutOp>(\n-            *reduceArg.getOperand().getDefiningOp()))\n-      return mlir::failure();\n     Value newRet = newReduce.getResult();\n-    // it's still beneficial to move the conversion\n-    // to after the reduce if necessary since it will be\n-    // done on a rank-reduced tensor hence cheaper\n     if (newRet.getType() != reduce.getResult().getType())\n       newRet = rewriter.create<triton::gpu::ConvertLayoutOp>(\n           op->getLoc(), reduce.getResult().getType(), newRet);\n-    rewriter.replaceOp(op, newRet);\n+    rewriter.replaceAllUsesWith(reduce, newRet);\n \n     return success();\n   }\n@@ -327,8 +332,7 @@ class RematerializeForward : public mlir::RewritePattern {\n         return failure();\n       }\n       // don't rematerialize non-element-wise\n-      if (!isa<triton::ViewOp, triton::CatOp>(op) &&\n-          !op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n+      if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n           !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n           !isa<triton::StoreOp>(op)) {\n         return failure();\n@@ -363,7 +367,7 @@ class RematerializeBackward : public mlir::RewritePattern {\n public:\n   explicit RematerializeBackward(mlir::MLIRContext *context)\n       : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             2, context) {}\n+                             3, context) {}\n \n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *cvt,\n@@ -555,8 +559,19 @@ class ConvertDotConvert : public mlir::RewritePattern {\n       return mlir::failure();\n \n     // TODO: int tensor cores\n-    auto _0f = rewriter.create<arith::ConstantFloatOp>(\n-        op->getLoc(), APFloat(0.0f), dstTy.getElementType().cast<FloatType>());\n+    auto out_dtype = dstTy.getElementType().cast<FloatType>();\n+    APFloat value(0.0f);\n+    if (out_dtype.isBF16())\n+      value = APFloat(APFloat::IEEEhalf(), APInt(16, 0));\n+    else if (out_dtype.isF16())\n+      value = APFloat(APFloat::IEEEhalf(), APInt(16, 0));\n+    else if (out_dtype.isF32())\n+      value = APFloat(0.0f);\n+    else\n+      llvm_unreachable(\"unsupported data type\");\n+\n+    auto _0f =\n+        rewriter.create<arith::ConstantFloatOp>(op->getLoc(), value, out_dtype);\n     auto _0 = rewriter.create<triton::SplatOp>(\n         op->getLoc(), dotOp.getResult().getType(), _0f);\n     auto newDot = rewriter.create<triton::DotOp>("}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -179,6 +179,8 @@ int simulateBackwardRematerialization(\n       if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n               triton::MakeRangeOp, triton::SplatOp>(*opArgI))\n         continue;\n+      if (isa<triton::ViewOp, triton::CatOp>(opArgI))\n+        continue;\n \n       // We add one expensive conversion for the current operand\n       numCvts += 1;\n@@ -229,6 +231,7 @@ void rematerializeConversionChain(\n     sortedValues.push_back(op->getResult(0));\n \n   for (Value currOperand : sortedValues) {\n+    Value origOperand = currOperand;\n     // unpack information\n     Attribute targetLayout = toConvert.lookup(currOperand);\n     // rematerialize the operand if necessary\n@@ -252,7 +255,7 @@ void rematerializeConversionChain(\n       Block *block = currOperand.cast<BlockArgument>().getOwner();\n       newOperand->moveAfter(block, block->begin());\n     }\n-    mapping.map(currOperand, newOperand);\n+    mapping.map(origOperand, newOperand);\n   }\n }\n "}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 10, "deletions": 5, "changes": 15, "file_content_changes": "@@ -1,6 +1,5 @@\n #include \"triton/Target/PTX/PTXTranslation.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n-#include <optional>\n \n #include \"llvm/IR/IRBuilder.h\"\n #include \"llvm/IR/LegacyPassManager.h\"\n@@ -12,13 +11,19 @@\n #include \"llvm/Support/TargetSelect.h\"\n #include \"llvm/Target/TargetMachine.h\"\n \n+#include <mutex>\n+#include <optional>\n+\n namespace triton {\n \n static void initLLVM() {\n-  LLVMInitializeNVPTXTargetInfo();\n-  LLVMInitializeNVPTXTarget();\n-  LLVMInitializeNVPTXTargetMC();\n-  LLVMInitializeNVPTXAsmPrinter();\n+  static std::once_flag init_flag;\n+  std::call_once(init_flag, []() {\n+    LLVMInitializeNVPTXTargetInfo();\n+    LLVMInitializeNVPTXTarget();\n+    LLVMInitializeNVPTXTargetMC();\n+    LLVMInitializeNVPTXAsmPrinter();\n+  });\n }\n \n static bool findAndReplace(std::string &str, const std::string &begin,"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 51, "deletions": 26, "changes": 77, "file_content_changes": "@@ -1173,15 +1173,17 @@ def kernel(X, stride_xm, stride_xn,\n # ---------------\n \n \n-@pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype\",\n-                         [(*shape, 4, False, False, epilogue, allow_tf32, dtype)\n+@pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype\",\n+                         [(*shape, 4, False, False, epilogue, allow_tf32, in_dtype, out_dtype)\n                           for shape in [(64, 64, 64), (16, 16, 16)]\n                           for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n                           for allow_tf32 in [True, False]\n-                          for dtype in ['float16', 'float32']\n-                          if not (allow_tf32 and (dtype in ['float16']))] +\n+                          for in_dtype, out_dtype in [('float16', 'float16'),\n+                                                      ('float16', 'float32'),\n+                                                      ('float32', 'float32')]\n+                          if not (allow_tf32 and (in_dtype in ['float16']))] +\n \n-                         [(*shape_nw, col_a, col_b, 'none', allow_tf32, dtype)\n+                         [(*shape_nw, col_a, col_b, 'none', allow_tf32, in_dtype, out_dtype)\n                           for shape_nw in [[128, 256, 32, 8],\n                                            [128, 16, 32, 4],\n                                            [32, 128, 64, 4],\n@@ -1194,19 +1196,25 @@ def kernel(X, stride_xm, stride_xn,\n                           for allow_tf32 in [False]\n                           for col_a in [True, False]\n                           for col_b in [True, False]\n-                          for dtype in ['int8', 'float16', 'float32']])\n-def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype, device='cuda'):\n+                          for in_dtype, out_dtype in [('int8', 'int8'),\n+                                                      ('float16', 'float16'),\n+                                                      ('float16', 'float32'),\n+                                                      ('float32', 'float32')]])\n+def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype, device='cuda'):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if capability[0] < 8:\n-        if dtype == 'int8':\n+        if in_dtype == 'int8':\n             pytest.skip(\"Only test int8 on devices with sm >= 80\")\n-        elif dtype == 'float32' and allow_tf32:\n+        elif in_dtype == 'float32' and allow_tf32:\n             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n     if capability[0] == 7:\n         if (M, N, K, num_warps) == (128, 256, 32, 8):\n             pytest.skip(\"shared memory out of resource\")\n+        if out_dtype == 'float16':\n+            # TODO: support out_dtype=float16 for tl.dot on V100\n+            pytest.skip(\"Only test out_dtype=float16 on devices with sm >=80\")\n \n     torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n \n@@ -1216,6 +1224,7 @@ def kernel(X, stride_xm, stride_xk,\n                Y, stride_yk, stride_yn,\n                W, stride_wn, stride_wl,\n                Z, stride_zm, stride_zn,\n+               out_dtype: tl.constexpr,\n                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n                ALLOW_TF32: tl.constexpr,\n@@ -1231,7 +1240,7 @@ def kernel(X, stride_xm, stride_xk,\n         Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n         x = tl.load(Xs)\n         y = tl.load(Ys)\n-        z = tl.dot(x, y, allow_tf32=ALLOW_TF32)\n+        z = tl.dot(x, y, allow_tf32=ALLOW_TF32, out_dtype=out_dtype)\n         if ADD_MATRIX:\n             z += tl.load(Zs)\n         if ADD_ROWS:\n@@ -1248,42 +1257,54 @@ def kernel(X, stride_xm, stride_xk,\n             z = num / den[:, None]\n         if CHAIN_DOT:\n             w = tl.load(Ws)\n-            z = tl.dot(z.to(w.dtype), w)\n+            z = tl.dot(z.to(w.dtype), w, out_dtype=out_dtype)\n         tl.store(Zs, z)\n     # input\n     rs = RandomState(17)\n     if col_a:\n-        x = numpy_random((K, M), dtype_str=dtype, rs=rs).T\n+        x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T\n     else:\n-        x = numpy_random((M, K), dtype_str=dtype, rs=rs)\n+        x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)\n     if col_b:\n-        y = numpy_random((N, K), dtype_str=dtype, rs=rs).T\n+        y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T\n     else:\n-        y = numpy_random((K, N), dtype_str=dtype, rs=rs)\n-    w = numpy_random((N, N), dtype_str=dtype, rs=rs)\n-    if 'int' not in dtype:\n+        y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)\n+    w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)\n+    if 'int' not in in_dtype:\n         x *= .1\n         y *= .1\n-    if dtype == 'float32' and allow_tf32:\n+    if in_dtype == 'float32' and allow_tf32:\n         x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n         y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n         w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n     x_tri = to_triton(x, device=device)\n     y_tri = to_triton(y, device=device)\n     w_tri = to_triton(w, device=device)\n     # triton result\n-    if dtype == 'int8':\n+    if out_dtype == 'int8':\n         z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)\n     else:\n-        z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs) * .1\n+        z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1\n \n     z_tri = to_triton(z, device=device)\n     if epilogue == 'trans':\n         z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])\n+\n+    if out_dtype == 'int8':\n+        out_dtype = tl.int8\n+    elif out_dtype == 'float16' and epilogue != 'softmax':\n+        # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will\n+        # fail with the following error: 'llvm.fmul' op requires the same type\n+        # for all operands and results\n+        out_dtype = tl.float16\n+    else:\n+        out_dtype = tl.float32\n+\n     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n                          y_tri, y_tri.stride(0), y_tri.stride(1),\n                          w_tri, w_tri.stride(0), w_tri.stride(1),\n                          z_tri, z_tri.stride(0), z_tri.stride(1),\n+                         out_dtype,\n                          COL_A=col_a, COL_B=col_b,\n                          BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n                          ADD_MATRIX=epilogue == 'add-matrix',\n@@ -1294,7 +1315,7 @@ def kernel(X, stride_xm, stride_xk,\n                          ALLOW_TF32=allow_tf32,\n                          num_warps=num_warps)\n     # torch result\n-    if dtype == 'int8':\n+    if in_dtype == 'int8':\n         z_ref = np.matmul(x.astype(np.float32),\n                           y.astype(np.float32())).astype(np.int32)\n     else:\n@@ -1314,9 +1335,11 @@ def kernel(X, stride_xm, stride_xk,\n         z_ref = np.matmul(z_ref, w)\n     # compare\n     # print(z_ref[:,0], z_tri[:,0])\n-    if dtype == 'float32':\n+    if in_dtype == 'float32':\n         # XXX: Somehow there's a larger difference when we use float32\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n+    elif out_dtype == tl.float16:\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n     else:\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n     # make sure ld/st are vectorized\n@@ -1325,12 +1348,14 @@ def kernel(X, stride_xm, stride_xk,\n         # XXX: skip small sizes because they are not vectorized\n         assert 'ld.global.v4' in ptx\n         assert 'st.global.v4' in ptx\n-    if dtype == 'float32' and allow_tf32:\n+    if in_dtype == 'float32' and allow_tf32:\n         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n-    elif dtype == 'float32' and allow_tf32:\n+    elif in_dtype == 'float32' and allow_tf32:\n         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' not in ptx\n-    elif dtype == 'int8':\n+    elif in_dtype == 'int8':\n         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n+    elif out_dtype == tl.float16:\n+        assert 'mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16' in ptx\n \n \n @pytest.mark.parametrize(\"dtype_str\", int_dtypes + float_dtypes + ['bfloat16'])\n@@ -1467,7 +1492,7 @@ def _kernel(in1_ptr, in2_ptr, output_ptr,\n         w = tl.load(in2_ptr + in2_offsets, mask=in2_offsets < in2_numel)\n \n         # Without a dot product the memory doesn't get promoted to shared.\n-        o = tl.dot(x, w)\n+        o = tl.dot(x, w, out_dtype=tl.float32)\n \n         # Store output\n         output_offsets = M_offsets[:, None] * out_stride + N_offsets[None, :]"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -862,7 +862,7 @@ def reshape(input, shape, _builder=None):\n \n \n @builtin\n-def dot(input, other, allow_tf32=True, _builder=None):\n+def dot(input, other, allow_tf32=True, out_dtype=float32, _builder=None):\n     \"\"\"\n     Returns the matrix product of two blocks.\n \n@@ -874,7 +874,8 @@ def dot(input, other, allow_tf32=True, _builder=None):\n     :type other: 2D tensor of scalar-type in {:code:`float16`, :code:`bfloat16`, :code:`float32`}\n     \"\"\"\n     allow_tf32 = _constexpr_to_value(allow_tf32)\n-    return semantic.dot(input, other, allow_tf32, _builder)\n+    out_dtype = _constexpr_to_value(out_dtype)\n+    return semantic.dot(input, other, allow_tf32, out_dtype, _builder)\n \n \n # -----------------------"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -1051,6 +1051,7 @@ def atomic_xchg(ptr: tl.tensor,\n def dot(lhs: tl.tensor,\n         rhs: tl.tensor,\n         allow_tf32: bool,\n+        out_dtype: tl.dtype,\n         builder: ir.builder) -> tl.tensor:\n     assert lhs.type.is_block() and rhs.type.is_block()\n     assert lhs.dtype == rhs.dtype, \"lhs and rhs must have the same dtype!\"\n@@ -1062,9 +1063,13 @@ def dot(lhs: tl.tensor,\n     if lhs.type.scalar.is_int():\n         _0 = builder.get_int32(0)\n         ret_scalar_ty = tl.int32\n-    else:\n+    elif lhs.type.scalar.is_fp32() or lhs.type.scalar.is_bf16():\n         _0 = builder.get_fp32(0)\n         ret_scalar_ty = tl.float32\n+    else:\n+        _0 = builder.get_fp16(0) if out_dtype.is_fp16() else builder.get_fp32(0)\n+        ret_scalar_ty = out_dtype\n+\n     M = lhs.type.shape[0]\n     N = rhs.type.shape[1]\n     _0 = builder.create_splat(_0, [M, N])"}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -64,7 +64,7 @@ def _sdd_kernel(\n         else:\n             a = tl.load(a_ptrs, mask=offs_ak[None, :] < k, other=0.)\n             b = tl.load(b_ptrs, mask=offs_bk[:, None] < k, other=0.)\n-        acc += tl.dot(a, b)\n+        acc += tl.dot(a, b, out_dtype=tl.float32)\n         a_ptrs += TILE_K * stride_ak\n         b_ptrs += TILE_K * stride_bk\n     c = acc.to(C.dtype.element_ty)\n@@ -183,7 +183,7 @@ def _dsd_kernel(\n     for k in range(K, 0, -TILE_K):\n         a = tl.load(pa)\n         b = tl.load(pb)\n-        acc += tl.dot(a, b)\n+        acc += tl.dot(a, b, out_dtype=tl.float32)\n         pa += inc_a\n         pb += inc_b * stride_bk\n         pinc += 2"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 21, "deletions": 9, "changes": 30, "file_content_changes": "@@ -64,9 +64,9 @@ def _kernel(A, B, C, M, N, K,\n             stride_am, stride_ak,\n             stride_bk, stride_bn,\n             stride_cm, stride_cn,\n+            dot_out_dtype: tl.constexpr,\n             BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n             GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr,\n-            ACC_TYPE: tl.constexpr\n             ):\n     # matrix multiplication\n     pid = tl.program_id(0)\n@@ -88,7 +88,7 @@ def _kernel(A, B, C, M, N, K,\n     # pointers\n     A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n     B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n-    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n+    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=dot_out_dtype)\n     for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n         if EVEN_K:\n             a = tl.load(A)\n@@ -97,7 +97,7 @@ def _kernel(A, B, C, M, N, K,\n             k_remaining = K - k * (BLOCK_K * SPLIT_K)\n             a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.)\n             b = tl.load(B, mask=rk[:, None] < k_remaining, other=0.)\n-        acc += tl.dot(a, b)\n+        acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n         A += BLOCK_K * SPLIT_K * stride_ak\n         B += BLOCK_K * SPLIT_K * stride_bk\n     acc = acc.to(C.dtype.element_ty)\n@@ -119,7 +119,7 @@ class _matmul(torch.autograd.Function):\n     _locks = {}\n \n     @staticmethod\n-    def _call(a, b):\n+    def _call(a, b, dot_out_dtype):\n         device = a.device\n         # handle non-contiguous inputs if necessary\n         if a.stride(0) > 1 and a.stride(1) > 1:\n@@ -132,20 +132,32 @@ def _call(a, b):\n         _, N = b.shape\n         # allocates output\n         c = torch.empty((M, N), device=device, dtype=a.dtype)\n-        # accumulator types\n-        ACC_TYPE = tl.float32 if a.dtype in [torch.float16, torch.bfloat16, torch.float32] else tl.int32\n+        if dot_out_dtype is None:\n+            if a.dtype in [torch.float16, torch.float32, torch.bfloat16]:\n+                dot_out_dtype = tl.float32\n+            else:\n+                dot_out_dtype = tl.int32\n+        else:\n+            assert isinstance(dot_out_dtype, torch.dtype), \"dot_out_dtype must be a torch.dtype\"\n+            if dot_out_dtype == torch.float16:\n+                dot_out_dtype = tl.float16\n+            elif dot_out_dtype in [torch.float32, torch.bfloat16]:\n+                dot_out_dtype = tl.float32\n+            else:\n+                dot_out_dtype = tl.int32\n         # launch kernel\n         grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']), META['SPLIT_K'])\n         _kernel[grid](a, b, c, M, N, K,\n                       a.stride(0), a.stride(1),\n                       b.stride(0), b.stride(1),\n                       c.stride(0), c.stride(1),\n-                      GROUP_M=8, ACC_TYPE=ACC_TYPE)\n+                      dot_out_dtype=dot_out_dtype,\n+                      GROUP_M=8)\n         return c\n \n     @staticmethod\n-    def forward(ctx, a, b):\n-        return _matmul._call(a, b)\n+    def forward(ctx, a, b, dot_out_dtype=None):\n+        return _matmul._call(a, b, dot_out_dtype=dot_out_dtype)\n \n \n matmul = _matmul.apply"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 35, "deletions": 0, "changes": 35, "file_content_changes": "@@ -956,6 +956,7 @@ func.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !\n   return\n }\n \n+// -----\n \n // Just make sure it doesn't crash on non-tensor types.\n // CHECK-LABEL: if_no_tensor\n@@ -982,3 +983,37 @@ func.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   tt.store %9, %8 {cache = 1 : i32, evict = 1 : i32} : f32\n   return\n }\n+\n+// -----\n+\n+// Check if the SimplifyReduceCvt rewriter pattern doesn't hang.\n+// CHECK-LABEL: reduce_cvt\n+// CHECK-NOT: triton_gpu.convert_layout\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 2], order = [0, 1]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [2, 1], order = [0, 1]}>\n+#blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [16, 2], warpsPerCTA = [2, 1], order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n+  func.func public @reduce_cvt(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg1: i32, %arg2: i32) {\n+    %cst = arith.constant dense<0> : tensor<1x2xi32, #blocked>\n+    %cst_0 = arith.constant dense<2> : tensor<1x2xi32, #blocked>\n+    %0 = tt.make_range {end = 2 : i32, start = 0 : i32} : tensor<2xi32, #blocked1>\n+    %1 = triton_gpu.convert_layout %0 : (tensor<2xi32, #blocked1>) -> tensor<2xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+    %2 = tt.expand_dims %1 {axis = 0 : i32} : (tensor<2xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x2xi32, #blocked>\n+    %3 = \"triton_gpu.cmpi\"(%2, %cst_0) {predicate = 2 : i64} : (tensor<1x2xi32, #blocked>, tensor<1x2xi32, #blocked>) -> tensor<1x2xi1, #blocked>\n+    %4 = tt.reduce %cst {axis = 1 : i32, redOp = 1 : i32} : tensor<1x2xi32, #blocked> -> tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+    %5 = triton_gpu.convert_layout %4 : (tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<1xi32, #blocked1>\n+    %6 = triton_gpu.convert_layout %5 : (tensor<1xi32, #blocked1>) -> tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+    %7 = tt.expand_dims %6 {axis = 1 : i32} : (tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<1x1xi32, #blocked2>\n+    %8 = triton_gpu.convert_layout %7 : (tensor<1x1xi32, #blocked2>) -> tensor<1x1xi32, #blocked>\n+    %9 = tt.splat %arg0 : (!tt.ptr<i64>) -> tensor<1x2x!tt.ptr<i64>, #blocked>\n+    %10 = tt.addptr %9, %2 : tensor<1x2x!tt.ptr<i64>, #blocked>, tensor<1x2xi32, #blocked>\n+    %11 = tt.broadcast %8 : (tensor<1x1xi32, #blocked>) -> tensor<1x2xi32, #blocked>\n+    %12 = arith.extsi %11 : tensor<1x2xi32, #blocked> to tensor<1x2xi64, #blocked>\n+    %13 = triton_gpu.convert_layout %10 : (tensor<1x2x!tt.ptr<i64>, #blocked>) -> tensor<1x2x!tt.ptr<i64>, #blocked3>\n+    %14 = triton_gpu.convert_layout %12 : (tensor<1x2xi64, #blocked>) -> tensor<1x2xi64, #blocked3>\n+    %15 = triton_gpu.convert_layout %3 : (tensor<1x2xi1, #blocked>) -> tensor<1x2xi1, #blocked3>\n+    tt.store %13, %14, %15 {cache = 1 : i32, evict = 1 : i32} : tensor<1x2xi64, #blocked3>\n+    return\n+  }\n+}"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 129, "deletions": 1, "changes": 130, "file_content_changes": "@@ -222,4 +222,132 @@ func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n     scf.yield %next_b_ptr, %c : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return %loop#1 : tensor<128x128xf32, #C>\n-}\n\\ No newline at end of file\n+}\n+\n+// CHECK: func.func @lut_bmm\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: %[[LUT_PTR:.*]] = tt.addptr\n+// CHECK: %arg27 = %[[LUT_PTR]]\n+// CHECK: %[[LUT_BUFFER_0:.*]] = tt.load %arg27, {{.*}}\n+// CHECK: %[[LUT_BUFFER_1:.*]] = arith.muli {{.*}}, %[[LUT_BUFFER_0]]\n+// CHECK: %[[LUT_BUFFER_2:.*]] = tt.splat %[[LUT_BUFFER_1]]\n+// CHECK: %[[NEXT_BUFFER_0:.*]] = tt.addptr {{.*}}, %[[LUT_BUFFER_2]]\n+// CHECK: %[[NEXT_BUFFER_1:.*]] = tt.addptr %arg26, {{.*}}\n+// CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_1]]\n+// CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_0]]\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}> \n+func.func @lut_bmm(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32 {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}, %arg17: i32 {tt.divisibility = 16 : i32}) { \n+  %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma> \n+  %c4_i32 = arith.constant 4 : i32\n+  %c1 = arith.constant 1 : index\n+  %c0 = arith.constant 0 : index \n+  %c0_i64 = arith.constant 0 : i64\n+  %c1_i32 = arith.constant 1 : i32\n+  %0 = tt.get_program_id {axis = 2 : i32} : i32\n+  %1 = tt.get_program_id {axis = 0 : i32} : i32\n+  %2 = tt.get_program_id {axis = 1 : i32} : i32 \n+  %3 = tt.get_num_programs {axis = 0 : i32} : i32 \n+  %4 = tt.get_num_programs {axis = 1 : i32} : i32 \n+  %5 = arith.muli %1, %4 : i32\n+  %6 = arith.addi %5, %2 : i32\n+  %7 = arith.muli %4, %c4_i32 : i32\n+  %8 = arith.divsi %6, %7 : i32\n+  %9 = arith.muli %8, %c4_i32 : i32 \n+  %10 = arith.subi %3, %9 : i32 \n+  %11 = arith.cmpi slt, %10, %c4_i32 : i32 \n+  %12 = arith.select %11, %10, %c4_i32 : i32 \n+  %13 = arith.remsi %6, %12 : i32 \n+  %14 = arith.addi %9, %13 : i32 \n+  %15 = arith.remsi %6, %7 : i32 \n+  %16 = arith.divsi %15, %12 : i32 \n+  %17 = arith.muli %arg5, %0 : i32 \n+  %18 = tt.addptr %arg4, %17 : !tt.ptr<i64>, i32\n+  %19 = tt.addptr %18, %14 : !tt.ptr<i64>, i32\n+  %20 = tt.load %19 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64 \n+  %21 = tt.addptr %19, %c1_i32 : !tt.ptr<i64>, i32\n+  %22 = tt.load %21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64 \n+  %23 = arith.subi %22, %20 : i64 \n+  %24 = arith.cmpi eq, %23, %c0_i64 : i64 \n+  cf.cond_br %24, ^bb1, ^bb2\n+^bb1:  // pred: ^bb0\n+  return\n+^bb2:  // pred: ^bb0\n+  %25 = arith.muli %arg1, %0 : i32 \n+  %26 = tt.addptr %arg0, %25 : !tt.ptr<f16>, i32\n+  %27 = arith.extsi %arg2 : i32 to i64\n+  %28 = arith.muli %27, %20 : i64 \n+  %29 = tt.addptr %26, %28 : !tt.ptr<f16>, i64\n+  %30 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+  %31 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %32 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+  %33 = tt.expand_dims %30 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<16x1xi32, #blocked>\n+  %34 = tt.expand_dims %31 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<16x1xi32, #blocked1>\n+  %35 = tt.expand_dims %32 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<16x1xi32, #blocked>\n+  %36 = tt.splat %arg3 : (i32) -> tensor<16x1xi32, #blocked>\n+  %37 = arith.muli %36, %33 : tensor<16x1xi32, #blocked>\n+  %38 = tt.splat %29 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked>\n+  %39 = tt.addptr %38, %37 : tensor<16x1x!tt.ptr<f16>, #blocked>, tensor<16x1xi32, #blocked>\n+  %40 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+  %41 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>\n+  %42 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+  %43 = tt.expand_dims %40 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x16xi32, #blocked>\n+  %44 = tt.expand_dims %41 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>) -> tensor<1x16xi32, #blocked1>\n+  %45 = tt.expand_dims %42 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x16xi32, #blocked>\n+  %46 = tt.broadcast %39 : (tensor<16x1x!tt.ptr<f16>, #blocked>) -> tensor<16x16x!tt.ptr<f16>, #blocked>\n+  %47 = tt.broadcast %43 : (tensor<1x16xi32, #blocked>) -> tensor<16x16xi32, #blocked>\n+  %48 = tt.broadcast %45 : (tensor<1x16xi32, #blocked>) -> tensor<16x16xi32, #blocked>\n+  %49 = tt.addptr %46, %47 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n+  %50 = arith.muli %arg9, %0 : i32 \n+  %51 = tt.addptr %arg8, %50 : !tt.ptr<f16>, i32\n+  %52 = arith.muli %arg11, %16 : i32 \n+  %53 = tt.addptr %51, %52 : !tt.ptr<f16>, i32\n+  %54 = tt.splat %53 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked1>\n+  %55 = tt.addptr %54, %34 : tensor<16x1x!tt.ptr<f16>, #blocked1>, tensor<16x1xi32, #blocked1>\n+  %56 = tt.splat %arg12 : (i32) -> tensor<1x16xi32, #blocked1>\n+  %57 = arith.muli %56, %44 : tensor<1x16xi32, #blocked1>\n+  %58 = tt.broadcast %55 : (tensor<16x1x!tt.ptr<f16>, #blocked1>) -> tensor<16x16x!tt.ptr<f16>, #blocked1>\n+  %59 = tt.broadcast %57 : (tensor<1x16xi32, #blocked1>) -> tensor<16x16xi32, #blocked1>\n+  %60 = tt.addptr %58, %59 : tensor<16x16x!tt.ptr<f16>, #blocked1>, tensor<16x16xi32, #blocked1>\n+  %61 = arith.muli %arg14, %0 : i32\n+  %62 = tt.addptr %arg13, %61 : !tt.ptr<f16>, i32\n+  %63 = arith.muli %arg15, %14 : i32\n+  %64 = tt.addptr %62, %63 : !tt.ptr<f16>, i32\n+  %65 = arith.muli %arg16, %16 : i32\n+  %66 = tt.addptr %64, %65 : !tt.ptr<f16>, i32\n+  %67 = tt.splat %arg17 : (i32) -> tensor<16x1xi32, #blocked>\n+  %68 = arith.muli %67, %35 : tensor<16x1xi32, #blocked>\n+  %69 = tt.splat %66 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked>\n+  %70 = tt.addptr %69, %68 : tensor<16x1x!tt.ptr<f16>, #blocked>, tensor<16x1xi32, #blocked>\n+  %71 = tt.broadcast %70 : (tensor<16x1x!tt.ptr<f16>, #blocked>) -> tensor<16x16x!tt.ptr<f16>, #blocked>\n+  %72 = tt.addptr %71, %48 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n+  %73 = arith.muli %arg7, %0 : i32\n+  %74 = tt.addptr %arg6, %73 : !tt.ptr<i64>, i32\n+  %75 = tt.addptr %74, %20 : !tt.ptr<i64>, i64\n+  %76 = arith.index_cast %23 : i64 to index\n+  %77 = arith.extsi %arg10 : i32 to i64\n+  %78 = tt.splat %arg2 : (i32) -> tensor<16x16xi32, #blocked>\n+  %79:3 = scf.for %arg18 = %c0 to %76 step %c1 iter_args(%arg19 = %cst, %arg20 = %49, %arg21 = %75) -> (tensor<16x16xf32, #mma>, tensor<16x16x!tt.ptr<f16>, #blocked>, !tt.ptr<i64>) {\n+    %82 = tt.load %arg20 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blocked>\n+    %83 = tt.load %arg21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64\n+    %84 = arith.muli %77, %83 : i64\n+    %85 = tt.splat %84 : (i64) -> tensor<16x16xi64, #blocked1>\n+    %86 = tt.addptr %60, %85 : tensor<16x16x!tt.ptr<f16>, #blocked1>, tensor<16x16xi64, #blocked1>\n+    %87 = tt.load %86 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blocked1>\n+    %88 = triton_gpu.convert_layout %82 : (tensor<16x16xf16, #blocked>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma}>>\n+    %89 = triton_gpu.convert_layout %87 : (tensor<16x16xf16, #blocked1>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma}>>\n+    %90 = tt.dot %88, %89, %arg19 {allowTF32 = true} : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma}>> -> tensor<16x16xf32, #mma>\n+    %91 = tt.addptr %arg20, %78 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n+    %92 = tt.addptr %arg21, %c1_i32 : !tt.ptr<i64>, i32\n+    scf.yield %90, %91, %92 : tensor<16x16xf32, #mma>, tensor<16x16x!tt.ptr<f16>, #blocked>, !tt.ptr<i64>\n+  }\n+  %80 = arith.truncf %79#0 : tensor<16x16xf32, #mma> to tensor<16x16xf16, #mma>\n+  %81 = triton_gpu.convert_layout %80 : (tensor<16x16xf16, #mma>) -> tensor<16x16xf16, #blocked>\n+  tt.store %72, %81 {cache = 1 : i32, evict = 1 : i32} : tensor<16x16xf16, #blocked>\n+  return\n+}"}]