[{"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 31, "deletions": 0, "changes": 31, "file_content_changes": "@@ -1004,6 +1004,8 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK: llvm.icmp \"slt\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$3 atom.global.gpu.add.f32\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: @$3 atom.global.gpu.add.f32\n     %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xf32, #blocked0>, tensor<256xi1, #blocked0>) -> tensor<256xf32, #blocked0>\n     return\n   }\n@@ -1024,6 +1026,35 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // -----\n \n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: store_f32\n+  func.func @store_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xf32, #blocked0>) {\n+    // CHECK: llvm.icmp \"slt\"\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: @$2 st.global.b32\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: @$2 st.global.b32\n+    tt.store %arg0, %arg1 : tensor<256xf32, #blocked0>\n+    return\n+  }\n+}\n+\n+// -----\n+\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: store_f32_scalar\n+  func.func @store_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : f32) {\n+    // CHECK: llvm.icmp \"slt\"\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: @$2 st.global.b32\n+    tt.store %arg0, %arg1 : f32\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n "}]