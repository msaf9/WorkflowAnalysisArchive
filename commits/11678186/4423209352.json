[{"filename": "include/triton/Analysis/AxisInfo.h", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -112,7 +112,8 @@ class AxisInfo {\n \n   /// The _divisibility_ information maps the `d`-th\n   /// dimension to the largest power-of-two that\n-  /// divides the first element of all the values along it\n+  /// divides the first element of all groups of\n+  // _contiguity_ values along it\n   /// For example:\n   /// [10, 11, 12, 13, 18, 19, 20, 21]\n   /// [20, 21, 22, 23, 28, 29, 30, 31]\n@@ -123,6 +124,10 @@ class AxisInfo {\n   /// [14, 18, 22, 26]\n   /// [15, 19, 23, 27]\n   //  would have divisibility [4, 1]\n+  //  On the other hand:\n+  //  [0, 1, 2, 0, 4, 5, 6, 7]\n+  //  would have divisibility 1 because\n+  //  _contiguity_=1\n   DimVectorT divisibility;\n \n   /// The _constancy_ information maps the `d`-th"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 5, "deletions": 8, "changes": 13, "file_content_changes": "@@ -334,14 +334,11 @@ class DivOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n     if (lhs.getConstantValue().has_value() &&\n         lhs.getConstantValue().value() == 0)\n       return lhs.getDivisibility(dim);\n-    // Case 2: rhs is constant\n-    if (rhs.getConstantValue().has_value()) {\n-      auto lhsDivisibility = lhs.getDivisibility(dim);\n-      auto rhsValue = rhs.getConstantValue().value();\n-      if (lhsDivisibility % rhsValue == 0)\n-        return lhsDivisibility / rhsValue;\n-    }\n-    // Case 3: both are not constant\n+    // Case 2: rhs is 1\n+    if (rhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().value() == 1)\n+      return lhs.getDivisibility(dim);\n+    // otherwise: return 1\n     return 1;\n   }\n "}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -260,6 +260,15 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module) {\n       return nullptr;\n   }\n \n+  auto optPipeline = mlir::makeOptimizingTransformer(\n+      /*optLevel=*/0, /*sizeLevel=*/0,\n+      /*targetMachine=*/nullptr);\n+\n+  if (auto err = optPipeline(llvmModule.get())) {\n+    llvm::errs() << \"Failed to optimize LLVM IR \" << err << \"\\n\";\n+    return nullptr;\n+  }\n+\n   for (auto &func : llvmModule->functions()) {\n     auto it = nvvmMetadata.find(func.getName());\n     if (it != nvvmMetadata.end())"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 37, "deletions": 0, "changes": 37, "file_content_changes": "@@ -295,6 +295,43 @@ def test_floordiv(dtype_x, dtype_y, device='cuda'):\n     _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n \n \n+def test_unsigned_name_mangling(device='cuda'):\n+    # Test that uint32 and int32 are mangled differently by the compiler\n+    SIZE = 128\n+    # define the kernel / launch-grid\n+\n+    @triton.jit\n+    def kernel(O1, O2, X, Y, SIZE: tl.constexpr):\n+        off = tl.arange(0, SIZE)\n+        x = tl.load(X + off)\n+        y = tl.load(Y + off)\n+        out1 = tl.abs(x)  # uint32 -> nop\n+        out2 = tl.abs(-y)  # int32 -> should have an effect\n+        tl.store(O1 + off, out1)\n+        tl.store(O2 + off, out2)\n+\n+    dtype_x = 'uint32'\n+    dtype_y = 'int32'\n+    # inputs\n+    rs = RandomState(17)\n+    x = numpy_random(SIZE, dtype_str=dtype_x, rs=rs)\n+    y = numpy_random(SIZE, dtype_str=dtype_y, rs=rs)\n+    # reference result\n+    expect = (np.abs(x), np.abs(-y))\n+    # triton result\n+    x_tri = to_triton(x, device=device, dst_type=dtype_x)\n+    y_tri = to_triton(y, device=device, dst_type=dtype_y)\n+    actual = tuple(\n+        to_triton(np.empty_like(e), device=device)\n+        for e in expect\n+    )\n+    kernel[(1, )](actual[0], actual[1], x_tri, y_tri, SIZE=SIZE, num_warps=4)\n+\n+    # Bitwise op, so expect exact equality\n+    assert (expect[0] == to_numpy(actual[0])).all()\n+    assert (expect[1] == to_numpy(actual[1])).all()\n+\n+\n # ---------------\n # test bitwise ops\n # ---------------"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -57,7 +57,9 @@ def mangle_ty(ty):\n     if ty.is_ptr():\n         return 'P' + mangle_ty(ty.element_ty)\n     if ty.is_int():\n-        return 'i' + str(ty.int_bitwidth)\n+        SIGNED = triton.language.dtype.SIGNEDNESS.SIGNED\n+        prefix = 'i' if ty.int_signedness == SIGNED else 'u'\n+        return prefix + str(ty.int_bitwidth)\n     if ty.is_fp8():\n         return 'fp8'\n     if ty.is_fp16():"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -37,8 +37,8 @@ def _to_tensor(x, builder):\n \n \n class dtype:\n-    SINT_TYPES = ['int1', 'int8', 'int16', 'int32', 'int64']\n-    UINT_TYPES = ['uint8', 'uint16', 'uint32', 'uint64']\n+    SINT_TYPES = ['int8', 'int16', 'int32', 'int64']\n+    UINT_TYPES = ['int1', 'uint8', 'uint16', 'uint32', 'uint64']\n     FP_TYPES = ['fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n     STANDARD_FP_TYPES = ['fp16', 'bf16', 'fp32', 'fp64']\n     OTHER_TYPES = ['void']"}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -82,7 +82,7 @@ func.func @div() {\n   %3 = arith.divui %1, %0 : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [64], constancy = [128], constant_value = 64\n   %4 = arith.constant dense<64> : tensor<128xi32>\n-  // CHECK-NEXT: contiguity = [1], divisibility = [16777216], constancy = [64], constant_value = <none>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [64], constant_value = <none>\n   %5 = arith.divsi %0, %4 : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n   %6 = arith.divsi %4, %0 : tensor<128xi32>\n@@ -94,11 +94,12 @@ func.func @div() {\n   %9 = arith.divui %0, %8 : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [128], divisibility = [8192], constancy = [1], constant_value = <none>\n   %10 = tt.make_range {end = 8320 : i32, start = 8192 : i32} : tensor<128xi32>\n-  // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [64], constant_value = <none>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [64], constant_value = <none>\n   %11 = arith.divsi %10, %4 : tensor<128xi32>\n   return\n }\n \n+\n // -----\n \n // CHECK-LABEL: @rem\n@@ -179,11 +180,11 @@ func.func @logic() {\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [64], constancy = [128], constant_value = 64\n   %1 = arith.constant dense<64> : tensor<128xi32>\n-  // CHECK-NEXT: contiguity = [1], divisibility = [16777216], constancy = [64], constant_value = <none>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [64], constant_value = <none>\n   %2 = arith.divsi %0, %1 : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [8], constancy = [128], constant_value = 8\n   %3 = arith.constant dense<8> : tensor<128xi32>\n-  // CHECK-NEXT: contiguity = [1], divisibility = [134217728], constancy = [8], constant_value = <none>\n+  // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [8], constant_value = <none>\n   %4 = arith.divsi %0, %3 : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n   %5 = arith.andi %0, %1 : tensor<128xi32>"}]