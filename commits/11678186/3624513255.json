[{"filename": "python/triton/__init__.py", "status": "modified", "additions": 39, "deletions": 4, "changes": 43, "file_content_changes": "@@ -1,15 +1,50 @@\n \"\"\"isort:skip_file\"\"\"\n-# flake8: noqa: F401\n __version__ = '2.0.0'\n \n+# ---------------------------------------\n+# Note: import order is significant here.\n+\n # TODO: torch needs to be imported first\n # or pybind11 shows `munmap_chunk(): invalid pointer`\n-import torch\n+import torch  # noqa: F401\n+\n # submodules\n-from .utils import *\n-from .runtime import Config, autotune, heuristics, JITFunction, KernelInterface\n+from .utils import (\n+    cdiv,\n+    MockTensor,\n+    next_power_of_2,\n+    reinterpret,\n+    TensorWrapper,\n+)\n+from .runtime import (\n+    autotune,\n+    Config,\n+    heuristics,\n+    JITFunction,\n+    KernelInterface,\n+)\n from .runtime.jit import jit\n from .compiler import compile, CompilationError\n from . import language\n from . import testing\n from . import ops\n+\n+__all__ = [\n+    \"autotune\",\n+    \"cdiv\",\n+    \"CompilationError\",\n+    \"compile\",\n+    \"Config\",\n+    \"heuristics\",\n+    \"jit\",\n+    \"JITFunction\",\n+    \"KernelInterface\",\n+    \"language\",\n+    \"MockTensor\",\n+    \"next_power_of_2\",\n+    \"ops\",\n+    \"reinterpret\",\n+    \"runtime\",\n+    \"TensorWrapper\",\n+    \"testing\",\n+]"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 170, "deletions": 3, "changes": 173, "file_content_changes": "@@ -1,4 +1,171 @@\n-# flake8: noqa: F401\n+\"\"\"isort:skip_file\"\"\"\n+# Import order is significant here.\n+\n+from triton._C.libtriton.triton import ir\n+\n from . import core, extern, libdevice, random\n-from .core import *\n-from .random import *\n+from .core import (\n+    abs,\n+    arange,\n+    argmin,\n+    argmax,\n+    atomic_add,\n+    atomic_and,\n+    atomic_cas,\n+    atomic_max,\n+    atomic_min,\n+    atomic_or,\n+    atomic_xchg,\n+    atomic_xor,\n+    bfloat16,\n+    block_type,\n+    builtin,\n+    cat,\n+    cdiv,\n+    constexpr,\n+    cos,\n+    debug_barrier,\n+    dot,\n+    dtype,\n+    exp,\n+    fdiv,\n+    float16,\n+    float32,\n+    float64,\n+    float8,\n+    function_type,\n+    int1,\n+    int16,\n+    int32,\n+    int64,\n+    int8,\n+    load,\n+    log,\n+    max,\n+    max_contiguous,\n+    maximum,\n+    min,\n+    minimum,\n+    multiple_of,\n+    num_programs,\n+    pi32_t,\n+    pointer_type,\n+    printf,\n+    program_id,\n+    ravel,\n+    sigmoid,\n+    sin,\n+    softmax,\n+    sqrt,\n+    store,\n+    sum,\n+    swizzle2d,\n+    tensor,\n+    trans,\n+    triton,\n+    uint16,\n+    uint32,\n+    uint64,\n+    uint8,\n+    umulhi,\n+    void,\n+    where,\n+    xor_sum,\n+    zeros,\n+    zeros_like,\n+)\n+from .random import (\n+    pair_uniform_to_normal,\n+    philox,\n+    philox_impl,\n+    rand,\n+    rand4x,\n+    randint,\n+    randint4x,\n+    randn,\n+    randn4x,\n+    uint32_to_uniform_float,\n+)\n+\n+\n+__all__ = [\n+    \"abs\",\n+    \"arange\",\n+    \"argmin\",\n+    \"argmax\",\n+    \"atomic_add\",\n+    \"atomic_and\",\n+    \"atomic_cas\",\n+    \"atomic_max\",\n+    \"atomic_min\",\n+    \"atomic_or\",\n+    \"atomic_xchg\",\n+    \"atomic_xor\",\n+    \"bfloat16\",\n+    \"block_type\",\n+    \"builtin\",\n+    \"cat\",\n+    \"cdiv\",\n+    \"constexpr\",\n+    \"cos\",\n+    \"debug_barrier\",\n+    \"dot\",\n+    \"dtype\",\n+    \"exp\",\n+    \"fdiv\",\n+    \"float16\",\n+    \"float32\",\n+    \"float64\",\n+    \"float8\",\n+    \"function_type\",\n+    \"int1\",\n+    \"int16\",\n+    \"int32\",\n+    \"int64\",\n+    \"int8\",\n+    \"ir\",\n+    \"load\",\n+    \"log\",\n+    \"max\",\n+    \"max_contiguous\",\n+    \"maximum\",\n+    \"min\",\n+    \"minimum\",\n+    \"multiple_of\",\n+    \"num_programs\",\n+    \"pair_uniform_to_normal\",\n+    \"philox\",\n+    \"philox_impl\",\n+    \"pi32_t\",\n+    \"pointer_type\",\n+    \"printf\",\n+    \"program_id\",\n+    \"rand\",\n+    \"rand4x\",\n+    \"randint\",\n+    \"randint4x\",\n+    \"randn\",\n+    \"randn4x\",\n+    \"ravel\",\n+    \"sigmoid\",\n+    \"sin\",\n+    \"softmax\",\n+    \"sqrt\",\n+    \"store\",\n+    \"sum\",\n+    \"swizzle2d\",\n+    \"tensor\",\n+    \"trans\",\n+    \"triton\",\n+    \"uint16\",\n+    \"uint32\",\n+    \"uint32_to_uniform_float\",\n+    \"uint64\",\n+    \"uint8\",\n+    \"umulhi\",\n+    \"void\",\n+    \"where\",\n+    \"xor_sum\",\n+    \"zeros\",\n+    \"zeros_like\",\n+]"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -410,10 +410,10 @@ def __bool__(self):\n \n     def __neg__(self):\n         return constexpr(-self.value)\n-    \n+\n     def __pos__(self):\n         return constexpr(+self.value)\n-    \n+\n     def __invert__(self):\n         return constexpr(~self.value)\n "}, {"filename": "python/triton/ops/__init__.py", "status": "modified", "additions": 9, "deletions": 2, "changes": 11, "file_content_changes": "@@ -1,5 +1,12 @@\n-# flake8: noqa: F401\n-#from .conv import _conv, conv\n+# from .conv import _conv, conv\n from . import blocksparse\n from .cross_entropy import _cross_entropy, cross_entropy\n from .matmul import _matmul, matmul\n+\n+__all__ = [\n+    \"blocksparse\",\n+    \"_cross_entropy\",\n+    \"cross_entropy\",\n+    \"_matmul\",\n+    \"matmul\",\n+]"}, {"filename": "python/triton/ops/blocksparse/__init__.py", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -1,3 +1,7 @@\n-# flake8: noqa: F401\n from .matmul import matmul\n from .softmax import softmax\n+\n+__all__ = [\n+    \"matmul\",\n+    \"softmax\",\n+]"}, {"filename": "python/triton/runtime/__init__.py", "status": "modified", "additions": 12, "deletions": 2, "changes": 14, "file_content_changes": "@@ -1,2 +1,12 @@\n-from .autotuner import Config, Heuristics, autotune, heuristics  # noqa: F401\n-from .jit import JITFunction, KernelInterface, version_key  # noqa: F401\n+from .autotuner import Config, Heuristics, autotune, heuristics\n+from .jit import JITFunction, KernelInterface, version_key\n+\n+__all__ = [\n+    \"Config\",\n+    \"Heuristics\",\n+    \"autotune\",\n+    \"heuristics\",\n+    \"JITFunction\",\n+    \"KernelInterface\",\n+    \"version_key\",\n+]"}]