[{"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -10,8 +10,8 @@\n #include \"mlir/Support/LogicalResult.h\"\n #include \"mlir/Target/LLVMIR/Dialect/LLVMIR/LLVMToLLVMIRTranslation.h\"\n #include \"mlir/Target/LLVMIR/Export.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h\"\n-#include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPU.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n+#include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\""}, {"filename": "include/triton/Conversion/MLIRTypes.h", "status": "modified", "additions": 9, "deletions": 8, "changes": 17, "file_content_changes": "@@ -10,20 +10,21 @@ namespace triton {\n namespace type {\n \n // Integer types\n-Type i32Ty(MLIRContext *ctx) { return IntegerType::get(ctx, 32); }\n-Type i8Ty(MLIRContext *ctx) { return IntegerType::get(ctx, 8); }\n-Type u32Ty(MLIRContext *ctx) {\n+// TODO(Superjomn): may change `static` into better implementations\n+static Type i32Ty(MLIRContext *ctx) { return IntegerType::get(ctx, 32); }\n+static Type i8Ty(MLIRContext *ctx) { return IntegerType::get(ctx, 8); }\n+static Type u32Ty(MLIRContext *ctx) {\n   return IntegerType::get(ctx, 32, IntegerType::Unsigned);\n }\n-Type u1Ty(MLIRContext *ctx) {\n+static Type u1Ty(MLIRContext *ctx) {\n   return IntegerType::get(ctx, 1, IntegerType::Unsigned);\n }\n \n // Float types\n-Type f16Ty(MLIRContext *ctx) { return FloatType::getF16(ctx); }\n-Type f32Ty(MLIRContext *ctx) { return FloatType::getF32(ctx); }\n-Type f64Ty(MLIRContext *ctx) { return FloatType::getF64(ctx); }\n-Type bf16Ty(MLIRContext *ctx) { return FloatType::getBF16(ctx); }\n+static Type f16Ty(MLIRContext *ctx) { return FloatType::getF16(ctx); }\n+static Type f32Ty(MLIRContext *ctx) { return FloatType::getF32(ctx); }\n+static Type f64Ty(MLIRContext *ctx) { return FloatType::getF64(ctx); }\n+static Type bf16Ty(MLIRContext *ctx) { return FloatType::getBF16(ctx); }\n \n static bool isFloat(Type type) {\n   return type.isF32() || type.isF64() || type.isF16() || type.isF128();"}, {"filename": "include/triton/Conversion/Passes.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -2,8 +2,8 @@\n #define TRITON_CONVERSION_PASSES_H\n \n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h\"\n-#include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPU.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n+#include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n \n namespace mlir {\n namespace triton {"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h", "status": "renamed", "additions": 8, "deletions": 9, "changes": 17, "file_content_changes": "@@ -1,10 +1,10 @@\n-#ifndef TRITON_CONVERSION_TRITON_GPU_TO_LLVM_ASM_FORMAT_H_\n-#define TRITON_CONVERSION_TRITON_GPU_TO_LLVM_ASM_FORMAT_H_\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_ASM_FORMAT_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_ASM_FORMAT_H\n \n-#include \"mlir/IR/Value.h\"\n-#include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/ADT/StringRef.h\"\n+#include \"mlir/IR/Value.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include <memory>\n #include <string>\n \n@@ -172,11 +172,11 @@ struct PTXBuilder {\n     return argArchive.back().get();\n   }\n \n-  // Make the oprands in argArchive follow the provided \\param order.\n+  // Make the operands in argArchive follow the provided \\param order.\n   void reorderArgArchive(ArrayRef<Operand *> order) {\n     assert(order.size() == argArchive.size());\n     // The order in argArchive is unnecessary when onlyAttachMLIRArgs=false, but\n-    // it do necessary when onlyAttachMLIRArgs is true for the $0,$1.. are\n+    // it does necessary when onlyAttachMLIRArgs is true for the $0, $1... are\n     // determined by PTX code snippet passed from external.\n     sort(argArchive.begin(), argArchive.end(),\n          [&](std::unique_ptr<Operand> &a, std::unique_ptr<Operand> &b) {\n@@ -306,8 +306,7 @@ struct PTXInstrExecution {\n   bool onlyAttachMLIRArgs{};\n };\n \n-//// =============================== Some instruction wrappers\n-///===============================\n+/// ====== Some instruction wrappers ======\n // We add the wrappers to make the usage more intuitive by avoiding mixing the\n // PTX code with some trivial C++ code.\n \n@@ -324,4 +323,4 @@ struct PTXCpAsyncLoadInstr : PTXInstrBase<PTXCpAsyncLoadInstr> {\n } // namespace triton\n } // namespace mlir\n \n-#endif // TRITON_CONVERSION_TRITON_GPU_TO_LLVM_ASM_FORMAT_H_\n+#endif"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h", "status": "removed", "additions": 0, "deletions": 48, "changes": 48, "file_content_changes": "@@ -1,48 +0,0 @@\n-#ifndef TRITON_CONVERSION_TRITONGPUTOLLVM_TRITONGPUTOLLVMPASS_H_\n-#define TRITON_CONVERSION_TRITONGPUTOLLVM_TRITONGPUTOLLVMPASS_H_\n-\n-#include \"mlir/Conversion/LLVMCommon/TypeConverter.h\"\n-#include \"mlir/Transforms/DialectConversion.h\"\n-#include <memory>\n-\n-namespace mlir {\n-\n-class ModuleOp;\n-template <typename T> class OperationPass;\n-\n-class TritonLLVMConversionTarget : public ConversionTarget {\n-public:\n-  explicit TritonLLVMConversionTarget(MLIRContext &ctx,\n-                                      mlir::LLVMTypeConverter &typeConverter);\n-};\n-\n-class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n-public:\n-  explicit TritonLLVMFunctionConversionTarget(\n-      MLIRContext &ctx, mlir::LLVMTypeConverter &typeConverter);\n-};\n-\n-namespace LLVM {\n-void vprintf(StringRef msg, ValueRange args,\n-             ConversionPatternRewriter &rewriter);\n-}\n-\n-namespace triton {\n-\n-// Names for identifying different NVVM annotations. It is used as attribute\n-// names in MLIR modules. Refer to\n-// https://docs.nvidia.com/cuda/nvvm-ir-spec/index.html#supported-properties for\n-// the full list.\n-struct NVVMMetadataField {\n-  static constexpr char MaxNTid[] = \"nvvm.maxntid\";\n-  static constexpr char Kernel[] = \"nvvm.kernel\";\n-};\n-\n-std::unique_ptr<OperationPass<ModuleOp>>\n-createConvertTritonGPUToLLVMPass(int computeCapability = 80);\n-\n-} // namespace triton\n-\n-} // namespace mlir\n-\n-#endif"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h", "status": "added", "additions": 22, "deletions": 0, "changes": 22, "file_content_changes": "@@ -0,0 +1,22 @@\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_PASS_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_PASS_H\n+\n+#include \"mlir/Conversion/LLVMCommon/TypeConverter.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include <memory>\n+\n+namespace mlir {\n+\n+class ModuleOp;\n+template <typename T> class OperationPass;\n+\n+namespace triton {\n+\n+std::unique_ptr<OperationPass<ModuleOp>>\n+createConvertTritonGPUToLLVMPass(int computeCapability = 80);\n+\n+} // namespace triton\n+\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h", "status": "renamed", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n-#ifndef TRITON_CONVERSION_TRITONTOTRITONGPU_TRITONTOTRITONGPUPASS_H_\n-#define TRITON_CONVERSION_TRITONTOTRITONGPU_TRITONTOTRITONGPUPASS_H_\n+#ifndef TRITON_CONVERSION_TRITONTOTRITONGPU_TRITONTOTRITONGPUPASS_H\n+#define TRITON_CONVERSION_TRITONTOTRITONGPU_TRITONTOTRITONGPUPASS_H\n \n #include <memory>\n "}, {"filename": "include/triton/Tools/Sys/GetEnv.hpp", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "include/triton/tools/sys/getenv.hpp"}, {"filename": "lib/Conversion/PassDetail.h", "status": "removed", "additions": 0, "deletions": 20, "changes": 20, "file_content_changes": "@@ -1,20 +0,0 @@\n-#ifndef TRITON_CONVERSION_PASSDETAIL_H\n-#define TRITON_CONVERSION_PASSDETAIL_H\n-\n-#include \"mlir/Dialect/GPU/GPUDialect.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n-#include \"mlir/Pass/Pass.h\"\n-#include \"triton/Dialect/Triton/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-\n-namespace mlir {\n-namespace triton {\n-\n-#define GEN_PASS_CLASSES\n-#include \"triton/Conversion/Passes.h.inc\"\n-\n-} // namespace triton\n-} // namespace mlir\n-\n-#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,6 +1,6 @@\n add_mlir_conversion_library(TritonGPUToLLVM\n-    TritonGPUToLLVM.cpp\n-    PtxAsmFormat.cpp\n+    TritonGPUToLLVMPass.cpp\n+    PTXAsmFormat.cpp\n \n     ADDITIONAL_HEADER_DIRS\n     ${PROJECT_SOURCE_DIR}/include/triton/Conversion/TritonGPUToLLVM"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 5, "deletions": 7, "changes": 12, "file_content_changes": "@@ -1,7 +1,8 @@\n #ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_DOT_HELPERS_H\n #define TRITON_CONVERSION_TRITONGPU_TO_LLVM_DOT_HELPERS_H\n \n-#include \"./Utility.h\"\n+#include \"llvm/Support/Format.h\"\n+#include \"llvm/Support/FormatVariadic.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Conversion/ArithmeticToLLVM/ArithmeticToLLVM.h\"\n #include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n@@ -22,14 +23,11 @@\n #include \"triton/Analysis/Membar.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Conversion/MLIRTypes.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-#include \"llvm/Support/Format.h\"\n-#include \"llvm/Support/FormatVariadic.h\"\n-#include <memory>\n-#include <numeric>\n-#include <string>\n+\n+#include \"Utility.h\"\n \n namespace mlir {\n namespace LLVM {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/PTXAsmFormat.cpp", "status": "renamed", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -1,8 +1,10 @@\n-#include \"triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n+\n+#include \"llvm/Support/raw_ostream.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Transforms/DialectConversion.h\"\n-#include \"llvm/Support/raw_ostream.h\"\n-#include <sstream> // unify to llvm::raw_string_ostream ?\n+// TODO(Superjomn): unify to llvm::raw_string_ostream\n+#include <sstream>\n \n namespace mlir {\n namespace triton {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h", "status": "renamed", "additions": 1047, "deletions": 2250, "changes": 3297, "file_content_changes": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "added", "additions": 541, "deletions": 0, "changes": 541, "file_content_changes": "@@ -0,0 +1,541 @@\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_BASE_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_BASE_H\n+\n+#include \"Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::LLVM::SharedMemoryObject;\n+using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+using ::mlir::triton::gpu::SliceEncodingAttr;\n+\n+// FuncOpConversion/FuncOpConversionBase is borrowed from\n+// https://github.com/llvm/llvm-project/blob/fae656b2dd80246c3c6f01e9c77c49560368752c/mlir/lib/Conversion/FuncToLLVM/FuncToLLVM.cpp#L276\n+// since it is not exposed on header files in mlir v14\n+// TODO(Superjomn): remove the code when MLIR v15.0 is included.\n+// All the rights are reserved by the LLVM community.\n+\n+struct FuncOpConversionBase : public ConvertOpToLLVMPattern<FuncOp> {\n+private:\n+  /// Only retain those attributes that are not constructed by\n+  /// `LLVMFuncOp::build`. If `filterArgAttrs` is set, also filter out argument\n+  /// attributes.\n+  static void filterFuncAttributes(ArrayRef<NamedAttribute> attrs,\n+                                   bool filterArgAttrs,\n+                                   SmallVectorImpl<NamedAttribute> &result) {\n+    for (const auto &attr : attrs) {\n+      if (attr.getName() == SymbolTable::getSymbolAttrName() ||\n+          attr.getName() == FunctionOpInterface::getTypeAttrName() ||\n+          attr.getName() == \"std.varargs\" ||\n+          (filterArgAttrs &&\n+           attr.getName() == FunctionOpInterface::getArgDictAttrName()))\n+        continue;\n+      result.push_back(attr);\n+    }\n+  }\n+\n+  /// Helper function for wrapping all attributes into a single DictionaryAttr\n+  static auto wrapAsStructAttrs(OpBuilder &b, ArrayAttr attrs) {\n+    return DictionaryAttr::get(\n+        b.getContext(), b.getNamedAttr(\"llvm.struct_attrs\", attrs));\n+  }\n+\n+protected:\n+  using ConvertOpToLLVMPattern<FuncOp>::ConvertOpToLLVMPattern;\n+\n+  // Convert input FuncOp to LLVMFuncOp by using the LLVMTypeConverter provided\n+  // to this legalization pattern.\n+  LLVM::LLVMFuncOp\n+  convertFuncOpToLLVMFuncOp(FuncOp funcOp,\n+                            ConversionPatternRewriter &rewriter) const {\n+    // Convert the original function arguments. They are converted using the\n+    // LLVMTypeConverter provided to this legalization pattern.\n+    auto varargsAttr = funcOp->getAttrOfType<BoolAttr>(\"func.varargs\");\n+    TypeConverter::SignatureConversion result(funcOp.getNumArguments());\n+    auto llvmType = getTypeConverter()->convertFunctionSignature(\n+        funcOp.getType(), varargsAttr && varargsAttr.getValue(), result);\n+    if (!llvmType)\n+      return nullptr;\n+\n+    // Propagate argument/result attributes to all converted arguments/result\n+    // obtained after converting a given original argument/result.\n+    SmallVector<NamedAttribute, 4> attributes;\n+    filterFuncAttributes(funcOp->getAttrs(), /*filterArgAttrs=*/true,\n+                         attributes);\n+    if (ArrayAttr resAttrDicts = funcOp.getAllResultAttrs()) {\n+      assert(!resAttrDicts.empty() && \"expected array to be non-empty\");\n+      auto newResAttrDicts =\n+          (funcOp.getNumResults() == 1)\n+              ? resAttrDicts\n+              : rewriter.getArrayAttr(\n+                    {wrapAsStructAttrs(rewriter, resAttrDicts)});\n+      attributes.push_back(rewriter.getNamedAttr(\n+          FunctionOpInterface::getResultDictAttrName(), newResAttrDicts));\n+    }\n+    if (ArrayAttr argAttrDicts = funcOp.getAllArgAttrs()) {\n+      SmallVector<Attribute, 4> newArgAttrs(\n+          llvmType.cast<LLVM::LLVMFunctionType>().getNumParams());\n+      for (unsigned i = 0, e = funcOp.getNumArguments(); i < e; ++i) {\n+        auto mapping = result.getInputMapping(i);\n+        assert(mapping && \"unexpected deletion of function argument\");\n+        for (size_t j = 0; j < mapping->size; ++j)\n+          newArgAttrs[mapping->inputNo + j] = argAttrDicts[i];\n+      }\n+      attributes.push_back(\n+          rewriter.getNamedAttr(FunctionOpInterface::getArgDictAttrName(),\n+                                rewriter.getArrayAttr(newArgAttrs)));\n+    }\n+    for (const auto &pair : llvm::enumerate(attributes)) {\n+      if (pair.value().getName() == \"llvm.linkage\") {\n+        attributes.erase(attributes.begin() + pair.index());\n+        break;\n+      }\n+    }\n+\n+    // Create an LLVM function, use external linkage by default until MLIR\n+    // functions have linkage.\n+    LLVM::Linkage linkage = LLVM::Linkage::External;\n+    if (funcOp->hasAttr(\"llvm.linkage\")) {\n+      auto attr =\n+          funcOp->getAttr(\"llvm.linkage\").dyn_cast<mlir::LLVM::LinkageAttr>();\n+      if (!attr) {\n+        funcOp->emitError()\n+            << \"Contains llvm.linkage attribute not of type LLVM::LinkageAttr\";\n+        return nullptr;\n+      }\n+      linkage = attr.getLinkage();\n+    }\n+    auto newFuncOp = rewriter.create<LLVM::LLVMFuncOp>(\n+        funcOp.getLoc(), funcOp.getName(), llvmType, linkage,\n+        /*dsoLocal*/ false, attributes);\n+    rewriter.inlineRegionBefore(funcOp.getBody(), newFuncOp.getBody(),\n+                                newFuncOp.end());\n+    if (failed(rewriter.convertRegionTypes(&newFuncOp.getBody(), *typeConverter,\n+                                           &result)))\n+      return nullptr;\n+\n+    return newFuncOp;\n+  }\n+};\n+\n+struct ConvertTritonGPUOpToLLVMPatternBase {\n+  static Value\n+  getStructFromSharedMemoryObject(Location loc,\n+                                  const SharedMemoryObject &smemObj,\n+                                  ConversionPatternRewriter &rewriter) {\n+    auto elems = smemObj.getElems();\n+    auto types = smemObj.getTypes();\n+    auto structTy =\n+        LLVM::LLVMStructType::getLiteral(rewriter.getContext(), types);\n+    return getStructFromElements(loc, elems, rewriter, structTy);\n+  }\n+};\n+\n+template <typename SourceOp>\n+class ConvertTritonGPUOpToLLVMPattern\n+    : public ConvertOpToLLVMPattern<SourceOp>,\n+      public ConvertTritonGPUOpToLLVMPatternBase {\n+public:\n+  using OpAdaptor = typename SourceOp::Adaptor;\n+\n+  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n+                                           PatternBenefit benefit = 1)\n+      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n+                                           const Allocation *allocation,\n+                                           Value smem,\n+                                           PatternBenefit benefit = 1)\n+      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n+        allocation(allocation), smem(smem) {}\n+\n+  Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n+    auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n+    auto cast = rewriter.create<UnrealizedConversionCastOp>(\n+        loc, TypeRange{llvmIndexTy},\n+        ValueRange{rewriter.create<::mlir::gpu::ThreadIdOp>(\n+            loc, rewriter.getIndexType(), ::mlir::gpu::Dimension::x)});\n+    Value threadId = cast.getResult(0);\n+    return threadId;\n+  }\n+\n+  // -----------------------------------------------------------------------\n+  // Utilities\n+  // -----------------------------------------------------------------------\n+\n+  // Convert an \\param index to a multi-dim coordinate given \\param shape and\n+  // \\param order.\n+  SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n+                                 Location loc, Value linear,\n+                                 ArrayRef<unsigned> shape,\n+                                 ArrayRef<unsigned> order) const {\n+    unsigned rank = shape.size();\n+    assert(rank == order.size());\n+    auto reordered = reorder(shape, order);\n+    auto reorderedMultiDim = delinearize(rewriter, loc, linear, reordered);\n+    SmallVector<Value> multiDim(rank);\n+    for (unsigned i = 0; i < rank; ++i) {\n+      multiDim[order[i]] = reorderedMultiDim[i];\n+    }\n+    return multiDim;\n+  }\n+\n+  SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n+                                 Location loc, Value linear,\n+                                 ArrayRef<unsigned> shape) const {\n+    unsigned rank = shape.size();\n+    assert(rank > 0);\n+    SmallVector<Value> multiDim(rank);\n+    if (rank == 1) {\n+      multiDim[0] = linear;\n+    } else {\n+      Value remained = linear;\n+      for (auto &&en : llvm::enumerate(shape.drop_back())) {\n+        Value dimSize = idx_val(en.value());\n+        multiDim[en.index()] = urem(remained, dimSize);\n+        remained = udiv(remained, dimSize);\n+      }\n+      multiDim[rank - 1] = remained;\n+    }\n+    return multiDim;\n+  }\n+\n+  Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n+                  ArrayRef<Value> multiDim, ArrayRef<unsigned> shape,\n+                  ArrayRef<unsigned> order) const {\n+    return linearize(rewriter, loc, reorder<Value>(multiDim, order),\n+                     reorder<unsigned>(shape, order));\n+  }\n+\n+  Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n+                  ArrayRef<Value> multiDim, ArrayRef<unsigned> shape) const {\n+    auto rank = multiDim.size();\n+    Value linear = idx_val(0);\n+    if (rank > 0) {\n+      linear = multiDim.back();\n+      for (auto [dim, dimShape] :\n+           llvm::reverse(llvm::zip(multiDim.drop_back(), shape.drop_back()))) {\n+        Value dimSize = idx_val(dimShape);\n+        linear = add(mul(linear, dimSize), dim);\n+      }\n+    }\n+    return linear;\n+  }\n+\n+  Value dot(ConversionPatternRewriter &rewriter, Location loc,\n+            ArrayRef<Value> offsets, ArrayRef<Value> strides) const {\n+    assert(offsets.size() == strides.size());\n+    Value ret = idx_val(0);\n+    for (auto [offset, stride] : llvm::zip(offsets, strides)) {\n+      ret = add(ret, mul(offset, stride));\n+    }\n+    return ret;\n+  }\n+\n+  // -----------------------------------------------------------------------\n+  // Blocked layout indices\n+  // -----------------------------------------------------------------------\n+\n+  // Get an index-base for each dimension for a \\param blocked_layout.\n+  SmallVector<Value>\n+  emitBaseIndexForBlockedLayout(Location loc,\n+                                ConversionPatternRewriter &rewriter,\n+                                const BlockedEncodingAttr &blocked_layout,\n+                                ArrayRef<int64_t> shape) const {\n+    Value threadId = getThreadId(rewriter, loc);\n+    Value warpSize = idx_val(32);\n+    Value laneId = urem(threadId, warpSize);\n+    Value warpId = udiv(threadId, warpSize);\n+    auto sizePerThread = blocked_layout.getSizePerThread();\n+    auto threadsPerWarp = blocked_layout.getThreadsPerWarp();\n+    auto warpsPerCTA = blocked_layout.getWarpsPerCTA();\n+    auto order = blocked_layout.getOrder();\n+    unsigned rank = shape.size();\n+\n+    // delinearize threadId to get the base index\n+    SmallVector<Value> multiDimWarpId =\n+        delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+    SmallVector<Value> multiDimThreadId =\n+        delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n+\n+    SmallVector<Value> multiDimBase(rank);\n+    for (unsigned k = 0; k < rank; ++k) {\n+      // Wrap around multiDimWarpId/multiDimThreadId incase\n+      // shape[k] > shapePerCTA[k]\n+      auto maxWarps =\n+          ceil<unsigned>(shape[k], sizePerThread[k] * threadsPerWarp[k]);\n+      auto maxThreads = ceil<unsigned>(shape[k], sizePerThread[k]);\n+      multiDimWarpId[k] = urem(multiDimWarpId[k], idx_val(maxWarps));\n+      multiDimThreadId[k] = urem(multiDimThreadId[k], idx_val(maxThreads));\n+      // multiDimBase[k] = (multiDimThreadId[k] +\n+      //                    multiDimWarpId[k] * threadsPerWarp[k]) *\n+      //                   sizePerThread[k];\n+      Value threadsPerWarpK = idx_val(threadsPerWarp[k]);\n+      Value sizePerThreadK = idx_val(sizePerThread[k]);\n+      multiDimBase[k] =\n+          mul(sizePerThreadK, add(multiDimThreadId[k],\n+                                  mul(multiDimWarpId[k], threadsPerWarpK)));\n+    }\n+    return multiDimBase;\n+  }\n+\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForBlockedLayout(const BlockedEncodingAttr &blockedLayout,\n+                             ArrayRef<int64_t> shape) const {\n+    auto sizePerThread = blockedLayout.getSizePerThread();\n+    auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n+    auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n+    auto order = blockedLayout.getOrder();\n+\n+    unsigned rank = shape.size();\n+    SmallVector<unsigned> shapePerCTA = getShapePerCTA(blockedLayout);\n+    SmallVector<unsigned> tilesPerDim(rank);\n+    for (unsigned k = 0; k < rank; ++k)\n+      tilesPerDim[k] = ceil<unsigned>(shape[k], shapePerCTA[k]);\n+\n+    SmallVector<SmallVector<unsigned>> offset(rank);\n+    for (unsigned k = 0; k < rank; ++k) {\n+      // 1 block in minimum if shape[k] is less than shapePerCTA[k]\n+      for (unsigned blockOffset = 0; blockOffset < tilesPerDim[k];\n+           ++blockOffset)\n+        for (unsigned warpOffset = 0; warpOffset < warpsPerCTA[k]; ++warpOffset)\n+          for (unsigned threadOffset = 0; threadOffset < threadsPerWarp[k];\n+               ++threadOffset)\n+            for (unsigned elemOffset = 0; elemOffset < sizePerThread[k];\n+                 ++elemOffset)\n+              offset[k].push_back(blockOffset * sizePerThread[k] *\n+                                      threadsPerWarp[k] * warpsPerCTA[k] +\n+                                  warpOffset * sizePerThread[k] *\n+                                      threadsPerWarp[k] +\n+                                  threadOffset * sizePerThread[k] + elemOffset);\n+    }\n+\n+    unsigned elemsPerThread = blockedLayout.getElemsPerThread(shape);\n+    unsigned totalSizePerThread = product<unsigned>(sizePerThread);\n+    SmallVector<SmallVector<unsigned>> reorderedOffset(elemsPerThread);\n+    for (unsigned n = 0; n < elemsPerThread; ++n) {\n+      unsigned linearNanoTileId = n / totalSizePerThread;\n+      unsigned linearNanoTileElemId = n % totalSizePerThread;\n+      SmallVector<unsigned> multiDimNanoTileId =\n+          getMultiDimIndex<unsigned>(linearNanoTileId, tilesPerDim, order);\n+      SmallVector<unsigned> multiDimNanoTileElemId = getMultiDimIndex<unsigned>(\n+          linearNanoTileElemId, sizePerThread, order);\n+      for (unsigned k = 0; k < rank; ++k) {\n+        unsigned reorderedMultiDimId =\n+            multiDimNanoTileId[k] *\n+                (sizePerThread[k] * threadsPerWarp[k] * warpsPerCTA[k]) +\n+            multiDimNanoTileElemId[k];\n+        reorderedOffset[n].push_back(offset[k][reorderedMultiDimId]);\n+      }\n+    }\n+    return reorderedOffset;\n+  }\n+\n+  // -----------------------------------------------------------------------\n+  // Mma layout indices\n+  // -----------------------------------------------------------------------\n+\n+  SmallVector<Value>\n+  emitBaseIndexForMmaLayoutV1(Location loc, ConversionPatternRewriter &rewriter,\n+                              const MmaEncodingAttr &mmaLayout,\n+                              ArrayRef<int64_t> shape) const {\n+    llvm_unreachable(\"emitIndicesForMmaLayoutV1 not implemented\");\n+  }\n+\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForMmaLayoutV1(const MmaEncodingAttr &mmaLayout,\n+                           ArrayRef<int64_t> shape) const {\n+    SmallVector<SmallVector<unsigned>> ret;\n+\n+    for (unsigned i = 0; i < shape[0]; i += getShapePerCTA(mmaLayout)[0]) {\n+      for (unsigned j = 0; j < shape[1]; j += getShapePerCTA(mmaLayout)[1]) {\n+        ret.push_back({i, j});\n+        ret.push_back({i, j + 1});\n+        ret.push_back({i + 2, j});\n+        ret.push_back({i + 2, j + 1});\n+        ret.push_back({i, j + 8});\n+        ret.push_back({i, j + 9});\n+        ret.push_back({i + 2, j + 8});\n+        ret.push_back({i + 2, j + 9});\n+      }\n+    }\n+    return ret;\n+  }\n+\n+  SmallVector<Value>\n+  emitBaseIndexForMmaLayoutV2(Location loc, ConversionPatternRewriter &rewriter,\n+                              const MmaEncodingAttr &mmaLayout,\n+                              ArrayRef<int64_t> shape) const {\n+    auto _warpsPerCTA = mmaLayout.getWarpsPerCTA();\n+    assert(_warpsPerCTA.size() == 2);\n+    SmallVector<Value> warpsPerCTA = {idx_val(_warpsPerCTA[0]),\n+                                      idx_val(_warpsPerCTA[1])};\n+    Value threadId = getThreadId(rewriter, loc);\n+    Value warpSize = idx_val(32);\n+    Value laneId = urem(threadId, warpSize);\n+    Value warpId = udiv(threadId, warpSize);\n+    Value warpId0 = urem(warpId, warpsPerCTA[0]);\n+    Value warpId1 = urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]);\n+    Value offWarp0 = mul(warpId0, idx_val(16));\n+    Value offWarp1 = mul(warpId1, idx_val(8));\n+\n+    SmallVector<Value> multiDimBase(2);\n+    multiDimBase[0] = add(udiv(laneId, idx_val(4)), offWarp0);\n+    multiDimBase[1] = add(mul(idx_val(2), urem(laneId, idx_val(4))), offWarp1);\n+    return multiDimBase;\n+  }\n+\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForMmaLayoutV2(const MmaEncodingAttr &mmaLayout,\n+                           ArrayRef<int64_t> shape) const {\n+    SmallVector<SmallVector<unsigned>> ret;\n+\n+    for (unsigned i = 0; i < shape[0]; i += getShapePerCTA(mmaLayout)[0]) {\n+      for (unsigned j = 0; j < shape[1]; j += getShapePerCTA(mmaLayout)[1]) {\n+        ret.push_back({i, j});\n+        ret.push_back({i, j + 1});\n+        ret.push_back({i + 8, j});\n+        ret.push_back({i + 8, j + 1});\n+      }\n+    }\n+    return ret;\n+  }\n+\n+  // -----------------------------------------------------------------------\n+  // Get offsets / indices for any layout\n+  // -----------------------------------------------------------------------\n+\n+  SmallVector<Value> emitBaseIndexForLayout(Location loc,\n+                                            ConversionPatternRewriter &rewriter,\n+                                            const Attribute &layout,\n+                                            ArrayRef<int64_t> shape) const {\n+    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n+      return emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n+    if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+      if (mmaLayout.isVolta())\n+        return emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, shape);\n+      if (mmaLayout.isAmpere())\n+        return emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, shape);\n+    }\n+    llvm_unreachable(\"unsupported emitBaseIndexForLayout\");\n+  }\n+\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForLayout(const Attribute &layout, ArrayRef<int64_t> shape) const {\n+    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n+      return emitOffsetForBlockedLayout(blockedLayout, shape);\n+    if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+      if (mmaLayout.isVolta())\n+        return emitOffsetForMmaLayoutV1(mmaLayout, shape);\n+      if (mmaLayout.isAmpere())\n+        return emitOffsetForMmaLayoutV2(mmaLayout, shape);\n+    }\n+    llvm_unreachable(\"unsupported emitOffsetForLayout\");\n+  }\n+\n+  // Emit indices calculation within each ConversionPattern, and returns a\n+  // [elemsPerThread X rank] index matrix.\n+\n+  // TODO: [phil] redundant indices computation do not appear to hurt\n+  // performance much, but they could still significantly slow down\n+  // computations.\n+  SmallVector<SmallVector<Value>> emitIndicesForDistributedLayout(\n+      Location loc, ConversionPatternRewriter &rewriter,\n+      const Attribute &layout, ArrayRef<int64_t> shape) const {\n+\n+    // step 1, delinearize threadId to get the base index\n+    auto multiDimBase = emitBaseIndexForLayout(loc, rewriter, layout, shape);\n+    // step 2, get offset of each element\n+    auto offset = emitOffsetForLayout(layout, shape);\n+    // step 3, add offset to base, and reorder the sequence of indices to\n+    // guarantee that elems in the same sizePerThread are adjacent in order\n+    unsigned rank = shape.size();\n+    unsigned elemsPerThread = offset.size();\n+    SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread,\n+                                                SmallVector<Value>(rank));\n+    for (unsigned n = 0; n < elemsPerThread; ++n)\n+      for (unsigned k = 0; k < rank; ++k)\n+        multiDimIdx[n][k] = add(multiDimBase[k], idx_val(offset[n][k]));\n+\n+    return multiDimIdx;\n+  }\n+\n+  struct SmallVectorKeyInfo {\n+    static unsigned getHashValue(const SmallVector<unsigned> &key) {\n+      return llvm::hash_combine_range(key.begin(), key.end());\n+    }\n+    static bool isEqual(const SmallVector<unsigned> &lhs,\n+                        const SmallVector<unsigned> &rhs) {\n+      return lhs == rhs;\n+    }\n+    static SmallVector<unsigned> getEmptyKey() {\n+      return SmallVector<unsigned>();\n+    }\n+    static SmallVector<unsigned> getTombstoneKey() {\n+      return {std::numeric_limits<unsigned>::max()};\n+    }\n+  };\n+\n+  SmallVector<SmallVector<Value>>\n+  emitIndicesForSliceLayout(Location loc, ConversionPatternRewriter &rewriter,\n+                            const SliceEncodingAttr &sliceLayout,\n+                            ArrayRef<int64_t> shape) const {\n+    auto parent = sliceLayout.getParent();\n+    unsigned dim = sliceLayout.getDim();\n+    size_t rank = shape.size();\n+    auto parentIndices =\n+        emitIndices(loc, rewriter, parent, sliceLayout.paddedShape(shape));\n+    unsigned numIndices = parentIndices.size();\n+    SmallVector<SmallVector<Value>> resultIndices;\n+    for (unsigned i = 0; i < numIndices; ++i) {\n+      SmallVector<Value> indices = parentIndices[i];\n+      indices.erase(indices.begin() + dim);\n+      resultIndices.push_back(indices);\n+    }\n+    return resultIndices;\n+  }\n+\n+  // -----------------------------------------------------------------------\n+  // Emit indices\n+  // -----------------------------------------------------------------------\n+  SmallVector<SmallVector<Value>> emitIndices(Location loc,\n+                                              ConversionPatternRewriter &b,\n+                                              const Attribute &layout,\n+                                              ArrayRef<int64_t> shape) const {\n+    if (auto blocked = layout.dyn_cast<BlockedEncodingAttr>()) {\n+      return emitIndicesForDistributedLayout(loc, b, blocked, shape);\n+    } else if (auto mma = layout.dyn_cast<MmaEncodingAttr>()) {\n+      return emitIndicesForDistributedLayout(loc, b, mma, shape);\n+    } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n+      return emitIndicesForSliceLayout(loc, b, slice, shape);\n+    } else {\n+      assert(0 && \"emitIndices for layouts other than blocked & slice not \"\n+                  \"implemented yet\");\n+      return {};\n+    }\n+  }\n+\n+  // -----------------------------------------------------------------------\n+  // Shared memory utilities\n+  // -----------------------------------------------------------------------\n+  template <typename T>\n+  Value getSharedMemoryBase(Location loc, ConversionPatternRewriter &rewriter,\n+                            T value) const {\n+    auto ptrTy = LLVM::LLVMPointerType::get(\n+        this->getTypeConverter()->convertType(rewriter.getI8Type()), 3);\n+    auto bufferId = allocation->getBufferId(value);\n+    assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n+    size_t offset = allocation->getOffset(bufferId);\n+    Value offVal = idx_val(offset);\n+    Value base = gep(ptrTy, smem, offVal);\n+    return base;\n+  }\n+\n+protected:\n+  const Allocation *allocation;\n+  Value smem;\n+};\n+\n+#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "added", "additions": 339, "deletions": 0, "changes": 339, "file_content_changes": "@@ -0,0 +1,339 @@\n+#include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n+\n+#include \"mlir/Conversion/ArithmeticToLLVM/ArithmeticToLLVM.h\"\n+#include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n+#include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n+#include \"mlir/Conversion/SCFToStandard/SCFToStandard.h\"\n+#include \"mlir/Conversion/StandardToLLVM/ConvertStandardToLLVM.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"triton/Analysis/Allocation.h\"\n+#include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Analysis/Membar.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+\n+#include \"TritonGPUToLLVM.h\"\n+#include \"TypeConverter.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Conversion/Passes.h.inc\"\n+\n+namespace mlir {\n+\n+class TritonLLVMConversionTarget : public ConversionTarget {\n+public:\n+  explicit TritonLLVMConversionTarget(MLIRContext &ctx)\n+      : ConversionTarget(ctx) {\n+    addLegalDialect<LLVM::LLVMDialect>();\n+    addLegalDialect<NVVM::NVVMDialect>();\n+    addIllegalDialect<triton::TritonDialect>();\n+    addIllegalDialect<triton::gpu::TritonGPUDialect>();\n+    addIllegalDialect<mlir::gpu::GPUDialect>();\n+    addIllegalDialect<mlir::StandardOpsDialect>();\n+    addLegalOp<mlir::UnrealizedConversionCastOp>();\n+  }\n+};\n+\n+class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n+public:\n+  explicit TritonLLVMFunctionConversionTarget(MLIRContext &ctx)\n+      : ConversionTarget(ctx) {\n+    addLegalDialect<LLVM::LLVMDialect>();\n+    addLegalDialect<NVVM::NVVMDialect>();\n+    addIllegalOp<mlir::FuncOp>();\n+    addLegalOp<mlir::UnrealizedConversionCastOp>();\n+  }\n+};\n+\n+} // namespace mlir\n+\n+namespace {\n+\n+class ConvertTritonGPUToLLVM\n+    : public ConvertTritonGPUToLLVMBase<ConvertTritonGPUToLLVM> {\n+\n+public:\n+  explicit ConvertTritonGPUToLLVM(int computeCapability)\n+      : computeCapability(computeCapability) {}\n+\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp mod = getOperation();\n+\n+    mlir::LowerToLLVMOptions option(context);\n+    option.overrideIndexBitwidth(32);\n+    TritonGPUToLLVMTypeConverter typeConverter(context, option);\n+    TritonLLVMFunctionConversionTarget funcTarget(*context);\n+    TritonLLVMConversionTarget target(*context);\n+\n+    int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+\n+    // Step 1: Decompose unoptimized layout conversions to use shared memory\n+    // Step 2: Decompose insert_slice_async to use load + insert_slice for\n+    //   pre-Ampere architectures or unsupported vectorized load sizes\n+    // Step 3: Allocate shared memories and insert barriers\n+    // Step 4: Convert SCF to CFG\n+    // Step 5: Convert FuncOp to LLVMFuncOp via partial conversion\n+    // Step 6: Convert the rest of ops via partial conversion\n+    //\n+    // The reason for putting step 1 before step 2 is that the membar\n+    // analysis currently only supports SCF but not CFG. The reason for a\n+    // separation between 1/4 is that, step 3 is out of the scope of Dialect\n+    // Conversion, thus we need to make sure the smem is not revised during the\n+    // conversion of step 4.\n+\n+    // Step 1\n+    decomposeMmaToDotOperand(mod, numWarps);\n+    decomposeBlockedToDotOperand(mod);\n+\n+    // Step 2\n+    decomposeInsertSliceAsyncOp(mod);\n+\n+    // Step 3\n+    Allocation allocation(mod);\n+    MembarAnalysis membarPass(&allocation);\n+    membarPass.run();\n+\n+    // Step 4\n+    RewritePatternSet scf_patterns(context);\n+    mlir::populateLoopToStdConversionPatterns(scf_patterns);\n+    mlir::ConversionTarget scf_target(*context);\n+    scf_target.addIllegalOp<scf::ForOp, scf::IfOp, scf::ParallelOp,\n+                            scf::WhileOp, scf::ExecuteRegionOp>();\n+    scf_target.markUnknownOpDynamicallyLegal([](Operation *) { return true; });\n+    if (failed(\n+            applyPartialConversion(mod, scf_target, std::move(scf_patterns))))\n+      return signalPassFailure();\n+\n+    // Step 5\n+    RewritePatternSet func_patterns(context);\n+    func_patterns.add<FuncOpConversion>(typeConverter, numWarps, /*benefit=*/1);\n+    if (failed(\n+            applyPartialConversion(mod, funcTarget, std::move(func_patterns))))\n+      return signalPassFailure();\n+\n+    // Step 6 - get axis and shared memory info\n+    AxisInfoAnalysis axisInfoAnalysis(mod.getContext());\n+    axisInfoAnalysis.run(mod);\n+    initSharedMemory(allocation.getSharedMemorySize(), typeConverter);\n+    mod->setAttr(\"triton_gpu.shared\",\n+                 mlir::IntegerAttr::get(mlir::IntegerType::get(context, 32),\n+                                        allocation.getSharedMemorySize()));\n+\n+    // Step 6 - rewrite rest of ops\n+    // We set a higher benefit here to ensure triton's patterns runs before\n+    // arith patterns for some encoding not supported by the community\n+    // patterns.\n+    RewritePatternSet patterns(context);\n+    populateTritonToLLVMPatterns(typeConverter, patterns, numWarps,\n+                                 axisInfoAnalysis, &allocation, smem,\n+                                 /*benefit=*/10);\n+    // Add arith/math's patterns to help convert scalar expression to LLVM.\n+    mlir::arith::populateArithmeticToLLVMConversionPatterns(typeConverter,\n+                                                            patterns);\n+    mlir::populateMathToLLVMConversionPatterns(typeConverter, patterns);\n+    mlir::populateStdToLLVMConversionPatterns(typeConverter, patterns);\n+    mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n+\n+    if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n+      return signalPassFailure();\n+  }\n+\n+private:\n+  Value smem;\n+\n+  int computeCapability{};\n+\n+  void initSharedMemory(size_t size,\n+                        TritonGPUToLLVMTypeConverter &typeConverter) {\n+    ModuleOp mod = getOperation();\n+    OpBuilder b(mod.getBodyRegion());\n+    auto loc = mod.getLoc();\n+    auto elemTy = typeConverter.convertType(b.getIntegerType(8));\n+    // Set array size 0 and external linkage indicates that we use dynamic\n+    // shared allocation to allow a larger shared memory size for each kernel.\n+    auto arrayTy = LLVM::LLVMArrayType::get(elemTy, 0);\n+    auto global = b.create<LLVM::GlobalOp>(\n+        loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::External,\n+        \"global_smem\", /*value=*/Attribute(), /*alignment=*/0,\n+        mlir::gpu::GPUDialect::getWorkgroupAddressSpace());\n+    SmallVector<LLVM::LLVMFuncOp> funcs;\n+    mod.walk([&](LLVM::LLVMFuncOp func) { funcs.push_back(func); });\n+    assert(funcs.size() == 1 &&\n+           \"Inliner pass is expected before TritonGPUToLLVM\");\n+    b.setInsertionPointToStart(&funcs[0].getBody().front());\n+    smem = b.create<LLVM::AddressOfOp>(loc, global);\n+    auto ptrTy =\n+        LLVM::LLVMPointerType::get(typeConverter.convertType(b.getI8Type()), 3);\n+    smem = b.create<LLVM::BitcastOp>(loc, ptrTy, smem);\n+  }\n+\n+  void decomposeMmaToDotOperand(ModuleOp mod, int numWarps) const {\n+    // Replace `mma -> dot_op` with `mma -> blocked -> dot_op`\n+    // unless certain conditions are met\n+    mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n+      OpBuilder builder(cvtOp);\n+      auto srcType = cvtOp.getOperand().getType().cast<RankedTensorType>();\n+      auto dstType = cvtOp.getType().cast<RankedTensorType>();\n+      auto srcMma =\n+          srcType.getEncoding().dyn_cast<triton::gpu::MmaEncodingAttr>();\n+      auto dstDotOp =\n+          dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+      if (srcMma && dstDotOp &&\n+          !ConvertLayoutOpConversion::isMmaToDotShortcut(srcMma, dstDotOp)) {\n+        auto tmpType = RankedTensorType::get(\n+            dstType.getShape(), dstType.getElementType(),\n+            triton::gpu::BlockedEncodingAttr::get(\n+                mod.getContext(), srcType.getShape(), getSizePerThread(srcMma),\n+                getOrder(srcMma), numWarps));\n+        auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n+            cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n+        auto newConvert = builder.create<triton::gpu::ConvertLayoutOp>(\n+            cvtOp.getLoc(), dstType, tmp);\n+        cvtOp.replaceAllUsesWith(newConvert.getResult());\n+        cvtOp.erase();\n+      }\n+    });\n+  }\n+\n+  void decomposeBlockedToDotOperand(ModuleOp mod) const {\n+    // Replace `blocked -> dot_op` with `blocked -> shared -> dot_op`\n+    // because the codegen doesn't handle `blocked -> dot_op` directly\n+    mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n+      OpBuilder builder(cvtOp);\n+      auto srcType = cvtOp.getOperand().getType().cast<RankedTensorType>();\n+      auto dstType = cvtOp.getType().cast<RankedTensorType>();\n+      auto srcBlocked =\n+          srcType.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>();\n+      auto dstDotOp =\n+          dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+      if (srcBlocked && dstDotOp) {\n+        auto tmpType = RankedTensorType::get(\n+            dstType.getShape(), dstType.getElementType(),\n+            triton::gpu::SharedEncodingAttr::get(\n+                mod.getContext(), dstDotOp, srcType.getShape(),\n+                getOrder(srcBlocked), srcType.getElementType()));\n+        auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n+            cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n+        auto newConvert = builder.create<triton::gpu::ConvertLayoutOp>(\n+            cvtOp.getLoc(), dstType, tmp);\n+        cvtOp.replaceAllUsesWith(newConvert.getResult());\n+        cvtOp.erase();\n+      }\n+    });\n+  }\n+\n+  void decomposeInsertSliceAsyncOp(ModuleOp mod) const {\n+    AxisInfoAnalysis axisInfoAnalysis(mod.getContext());\n+    axisInfoAnalysis.run(mod);\n+    // TODO(Keren): This is a hacky knob that may cause performance regression\n+    // when decomposition has been performed. We should remove this knob once we\n+    // have thorough analysis on async wait. Currently, we decompose\n+    // `insert_slice_async` into `load` and `insert_slice` without knowing which\n+    // `async_wait` is responsible for the `insert_slice_async`. To guarantee\n+    // correctness, we blindly set the `async_wait` to wait for all async ops.\n+    //\n+    // There are two options to improve this:\n+    // 1. We can perform a dataflow analysis to find the `async_wait` that is\n+    // responsible for the `insert_slice_async` in the backend.\n+    // 2. We can modify the pipeline to perform the decomposition before the\n+    // `async_wait` is inserted. However, it is also risky because we don't know\n+    // the correct vectorized shape yet in the pipeline pass. Making the\n+    // pipeline pass aware of the vectorization could introduce additional\n+    // dependencies on the AxisInfoAnalysis and the Coalesce analysis.\n+    bool decomposed = false;\n+    // insert_slice_async %src, %dst, %idx, %mask, %other\n+    // =>\n+    // %tmp = load %src, %mask, %other\n+    // %res = insert_slice %tmp into %dst[%idx]\n+    mod.walk([&](triton::gpu::InsertSliceAsyncOp insertSliceAsyncOp) -> void {\n+      OpBuilder builder(insertSliceAsyncOp);\n+\n+      // Get the vectorized load size\n+      auto src = insertSliceAsyncOp.src();\n+      auto dst = insertSliceAsyncOp.dst();\n+      auto srcTy = src.getType().cast<RankedTensorType>();\n+      auto dstTy = dst.getType().cast<RankedTensorType>();\n+      auto srcBlocked =\n+          srcTy.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>();\n+      auto resSharedLayout =\n+          dstTy.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n+      auto resElemTy = dstTy.getElementType();\n+      unsigned inVec = axisInfoAnalysis.getPtrVectorSize(src);\n+      unsigned outVec = resSharedLayout.getVec();\n+      unsigned minVec = std::min(outVec, inVec);\n+      auto maxBitWidth =\n+          std::max<unsigned>(128, resElemTy.getIntOrFloatBitWidth());\n+      auto vecBitWidth = resElemTy.getIntOrFloatBitWidth() * minVec;\n+      auto bitWidth = std::min<unsigned>(maxBitWidth, vecBitWidth);\n+      auto byteWidth = bitWidth / 8;\n+\n+      // If the load byte width is not eligible or the current compute\n+      // capability does not support async copy, then we do decompose\n+      if (triton::gpu::InsertSliceAsyncOp::getEligibleLoadByteWidth(\n+              computeCapability)\n+              .contains(byteWidth))\n+        return;\n+\n+      // load\n+      auto tmpTy =\n+          RankedTensorType::get(srcTy.getShape(), resElemTy, srcBlocked);\n+      auto loadOp = builder.create<triton::LoadOp>(\n+          insertSliceAsyncOp.getLoc(), tmpTy, insertSliceAsyncOp.src(),\n+          insertSliceAsyncOp.mask(), insertSliceAsyncOp.other(),\n+          insertSliceAsyncOp.cache(), insertSliceAsyncOp.evict(),\n+          insertSliceAsyncOp.isVolatile());\n+\n+      // insert_slice\n+      auto axis = insertSliceAsyncOp.axis();\n+      auto intAttr = [&](int64_t v) { return builder.getI64IntegerAttr(v); };\n+      auto offsets = SmallVector<OpFoldResult>(dstTy.getRank(), intAttr(0));\n+      auto sizes = SmallVector<OpFoldResult>(dstTy.getRank(), intAttr(1));\n+      auto strides = SmallVector<OpFoldResult>(dstTy.getRank(), intAttr(1));\n+      offsets[axis] = insertSliceAsyncOp.index();\n+      for (size_t i = 0; i < dstTy.getRank(); i++) {\n+        if (i != axis)\n+          sizes[i] = intAttr(dstTy.getShape()[i]);\n+      }\n+      auto insertSliceOp = builder.create<tensor::InsertSliceOp>(\n+          insertSliceAsyncOp.getLoc(), loadOp, insertSliceAsyncOp.dst(),\n+          offsets, sizes, strides);\n+\n+      // Replace\n+      insertSliceAsyncOp.replaceAllUsesWith(insertSliceOp.getResult());\n+      insertSliceAsyncOp.erase();\n+      decomposed = true;\n+    });\n+\n+    mod.walk([&](triton::gpu::AsyncWaitOp asyncWaitOp) -> void {\n+      if (!triton::gpu::AsyncWaitOp::isSupported(computeCapability)) {\n+        // async wait is supported in Ampere and later\n+        asyncWaitOp.erase();\n+      } else if (decomposed) {\n+        // Wait for all previous async ops\n+        OpBuilder builder(asyncWaitOp);\n+        auto newAsyncWaitOp =\n+            builder.create<triton::gpu::AsyncWaitOp>(asyncWaitOp.getLoc(), 0);\n+        asyncWaitOp.erase();\n+      }\n+    });\n+  }\n+};\n+\n+} // anonymous namespace\n+\n+namespace mlir {\n+namespace triton {\n+\n+std::unique_ptr<OperationPass<ModuleOp>>\n+createConvertTritonGPUToLLVMPass(int computeCapability) {\n+  return std::make_unique<::ConvertTritonGPUToLLVM>(computeCapability);\n+}\n+\n+} // namespace triton\n+} // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.h", "status": "added", "additions": 146, "deletions": 0, "changes": 146, "file_content_changes": "@@ -0,0 +1,146 @@\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_TYPECONVERTER_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_TYPECONVERTER_H\n+\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"triton/Conversion/MLIRTypes.h\"\n+\n+#include \"DotHelpers.h\"\n+#include \"Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::LLVM::DotOpFMAConversionHelper;\n+using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n+using ::mlir::LLVM::MMA16816ConversionHelper;\n+using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+using ::mlir::triton::gpu::SliceEncodingAttr;\n+using ::mlir::triton::gpu::getElemsPerThread;\n+\n+class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n+public:\n+  using TypeConverter::convertType;\n+\n+  TritonGPUToLLVMTypeConverter(MLIRContext *ctx, LowerToLLVMOptions &option,\n+                               const DataLayoutAnalysis *analysis = nullptr)\n+      : LLVMTypeConverter(ctx, option, analysis) {\n+    addConversion([&](triton::PointerType type) -> llvm::Optional<Type> {\n+      return convertTritonPointerType(type);\n+    });\n+    addConversion([&](RankedTensorType type) -> llvm::Optional<Type> {\n+      return convertTritonTensorType(type);\n+    });\n+    // Internally store float8 as int8\n+    addConversion([&](triton::Float8Type type) -> llvm::Optional<Type> {\n+      return IntegerType::get(type.getContext(), 8);\n+    });\n+  }\n+\n+  Type convertTritonPointerType(triton::PointerType type) {\n+    // Recursively translate pointee type\n+    return LLVM::LLVMPointerType::get(convertType(type.getPointeeType()),\n+                                      type.getAddressSpace());\n+  }\n+\n+  llvm::Optional<Type> convertTritonTensorType(RankedTensorType type) {\n+    auto ctx = type.getContext();\n+    Attribute layout = type.getEncoding();\n+    SmallVector<int64_t> shape(type.getShape().begin(), type.getShape().end());\n+\n+    if (layout &&\n+        (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n+         layout.isa<MmaEncodingAttr>())) {\n+      unsigned numElementsPerThread = getElemsPerThread(type);\n+      SmallVector<Type, 4> types(numElementsPerThread,\n+                                 convertType(type.getElementType()));\n+      return LLVM::LLVMStructType::getLiteral(ctx, types);\n+    } else if (auto shared_layout =\n+                   layout.dyn_cast_or_null<SharedEncodingAttr>()) {\n+      SmallVector<Type, 4> types;\n+      // base ptr\n+      auto ptrType =\n+          LLVM::LLVMPointerType::get(convertType(type.getElementType()), 3);\n+      types.push_back(ptrType);\n+      // shape dims\n+      auto rank = type.getRank();\n+      // offsets + strides\n+      for (auto i = 0; i < rank * 2; i++) {\n+        types.push_back(IntegerType::get(ctx, 32));\n+      }\n+      return LLVM::LLVMStructType::getLiteral(ctx, types);\n+    } else if (auto dotOpLayout =\n+                   layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n+      if (dotOpLayout.getParent()\n+              .isa<BlockedEncodingAttr>()) { // for parent is blocked layout\n+        int numElemsPerThread =\n+            DotOpFMAConversionHelper::getNumElemsPerThread(shape, dotOpLayout);\n+\n+        return LLVM::LLVMStructType::getLiteral(\n+            ctx, SmallVector<Type>(numElemsPerThread, type::f32Ty(ctx)));\n+      } else { // for parent is MMA layout\n+        auto mmaLayout = dotOpLayout.getParent().cast<MmaEncodingAttr>();\n+        auto wpt = mmaLayout.getWarpsPerCTA();\n+        Type elemTy = convertType(type.getElementType());\n+        if (mmaLayout.isAmpere()) {\n+          const llvm::DenseMap<int, Type> targetTyMap = {\n+              {32, elemTy},\n+              {16, vec_ty(elemTy, 2)},\n+              {8, vec_ty(elemTy, 4)},\n+          };\n+          Type targetTy;\n+          if (targetTyMap.count(elemTy.getIntOrFloatBitWidth())) {\n+            targetTy = targetTyMap.lookup(elemTy.getIntOrFloatBitWidth());\n+          } else {\n+            assert(false && \"Unsupported element type\");\n+          }\n+          if (dotOpLayout.getOpIdx() == 0) { // $a\n+            auto elems =\n+                MMA16816ConversionHelper::getANumElemsPerThread(type, wpt[0]);\n+            return LLVM::LLVMStructType::getLiteral(\n+                ctx, SmallVector<Type>(elems, targetTy));\n+          }\n+          if (dotOpLayout.getOpIdx() == 1) { // $b\n+            auto elems =\n+                MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt[1]);\n+            return struct_ty(SmallVector<Type>(elems, targetTy));\n+          }\n+        }\n+\n+        if (mmaLayout.isVolta()) {\n+          DotOpMmaV1ConversionHelper helper(mmaLayout);\n+\n+          // TODO[Superjomn]: Both transA and transB are not available here.\n+          bool trans = false;\n+          // TODO[Superjomn]: The order of A and B are not available here.\n+          SmallVector<unsigned> order({1, 0});\n+          if (trans) {\n+            std::swap(shape[0], shape[1]);\n+            std::swap(order[0], order[1]);\n+          }\n+\n+          if (dotOpLayout.getOpIdx() == 0) { // $a\n+            int elems = helper.numElemsPerThreadA(shape, order);\n+            Type x2Ty = vec_ty(elemTy, 2);\n+            return struct_ty(SmallVector<Type>(elems, x2Ty));\n+          }\n+          if (dotOpLayout.getOpIdx() == 1) { // $b\n+            int elems = helper.numElemsPerThreadB(shape, order);\n+            Type x2Ty = vec_ty(elemTy, 2);\n+            return struct_ty(SmallVector<Type>(elems, x2Ty));\n+          }\n+        }\n+      }\n+\n+      llvm::errs() << \"Unexpected dot operand layout detected in \"\n+                      \"TritonToLLVMTypeConverter\";\n+      return llvm::None;\n+    }\n+\n+    return llvm::None;\n+  }\n+};\n+\n+#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 84, "deletions": 40, "changes": 124, "file_content_changes": "@@ -1,34 +1,11 @@\n #ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_UTILITY_H\n #define TRITON_CONVERSION_TRITONGPU_TO_LLVM_UTILITY_H\n \n-#include \"mlir/Analysis/SliceAnalysis.h\"\n-#include \"mlir/Conversion/ArithmeticToLLVM/ArithmeticToLLVM.h\"\n-#include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n-#include \"mlir/Conversion/LLVMCommon/LoweringOptions.h\"\n #include \"mlir/Conversion/LLVMCommon/Pattern.h\"\n-#include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n-#include \"mlir/Conversion/SCFToStandard/SCFToStandard.h\"\n-#include \"mlir/Conversion/StandardToLLVM/ConvertStandardToLLVM.h\"\n-#include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n-#include \"mlir/Dialect/GPU/GPUDialect.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n-#include \"mlir/IR/Matchers.h\"\n-#include \"mlir/IR/TypeUtilities.h\"\n-#include \"mlir/Transforms/DialectConversion.h\"\n-#include \"triton/Analysis/Allocation.h\"\n-#include \"triton/Analysis/AxisInfo.h\"\n-#include \"triton/Analysis/Membar.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Conversion/MLIRTypes.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h\"\n-#include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPU.h\"\n-#include \"triton/Dialect/Triton/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-#include \"llvm/Support/Format.h\"\n-#include \"llvm/Support/FormatVariadic.h\"\n-#include <memory>\n-#include <numeric>\n+#include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n \n // Shortcuts for some commonly used LLVM ops to keep code simple and intuitive\n // Operators\n@@ -115,13 +92,76 @@\n #define idx_val(...)                                                           \\\n   LLVM::createIndexConstant(rewriter, loc, this->getTypeConverter(),           \\\n                             __VA_ARGS__)\n-\n #define tid_val() getThreadId(rewriter, loc)\n \n namespace mlir {\n+namespace triton {\n+\n+// Delinearize supposing order is [0, 1, .. , n]\n+template <typename T>\n+llvm::SmallVector<T> getMultiDimIndexImpl(T linearIndex,\n+                                          llvm::ArrayRef<T> shape) {\n+  // shape: {a, b, c, d}  ->  accMul: {1, a, a*b, a*b*c}\n+  size_t rank = shape.size();\n+  T accMul = product(shape.drop_back());\n+  T linearRemain = linearIndex;\n+  llvm::SmallVector<T> multiDimIndex(rank);\n+  for (int i = rank - 1; i >= 0; --i) {\n+    multiDimIndex[i] = linearRemain / accMul;\n+    linearRemain = linearRemain % accMul;\n+    if (i != 0) {\n+      accMul = accMul / shape[i - 1];\n+    }\n+  }\n+  return multiDimIndex;\n+}\n+\n+template <typename T>\n+llvm::SmallVector<T> getMultiDimIndex(T linearIndex, llvm::ArrayRef<T> shape,\n+                                      llvm::ArrayRef<unsigned> order) {\n+  size_t rank = shape.size();\n+  assert(rank == order.size());\n+  auto reordered = reorder(shape, order);\n+  auto reorderedMultiDim = getMultiDimIndexImpl<T>(linearIndex, reordered);\n+  llvm::SmallVector<T> multiDim(rank);\n+  for (unsigned i = 0; i < rank; ++i) {\n+    multiDim[order[i]] = reorderedMultiDim[i];\n+  }\n+  return multiDim;\n+}\n+\n+// Linearize supposing order is [0, 1, .. , n]\n+template <typename T>\n+static T getLinearIndexImpl(llvm::ArrayRef<T> multiDimIndex, llvm::ArrayRef<T> shape) {\n+  assert(multiDimIndex.size() == shape.size());\n+  // shape: {a, b, c, d}  ->  accMul: {1, a, a*b, a*b*c}\n+  size_t rank = shape.size();\n+  T accMul = product(shape.drop_back());\n+  T linearIndex = 0;\n+  for (int i = rank - 1; i >= 0; --i) {\n+    linearIndex += multiDimIndex[i] * accMul;\n+    if (i != 0) {\n+      accMul = accMul / shape[i - 1];\n+    }\n+  }\n+  return linearIndex;\n+}\n+\n+template <typename T>\n+static T getLinearIndex(llvm::ArrayRef<T> multiDimIndex,\n+                        llvm::ArrayRef<T> shape,\n+                        llvm::ArrayRef<unsigned> order) {\n+  assert(shape.size() == order.size());\n+  return getLinearIndexImpl<T>(reorder(multiDimIndex, order),\n+                               reorder(shape, order));\n+}\n+\n+} // namespace triton\n+\n namespace LLVM {\n using namespace mlir::triton;\n \n+static\n Value getStructFromElements(Location loc, ValueRange resultVals,\n                             ConversionPatternRewriter &rewriter,\n                             Type structType) {\n@@ -138,6 +178,7 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n   return llvmStruct;\n }\n \n+static\n SmallVector<Value> getElementsFromStruct(Location loc, Value llvmStruct,\n                                          ConversionPatternRewriter &rewriter) {\n   if (llvmStruct.getType().isIntOrIndexOrFloat() ||\n@@ -155,47 +196,50 @@ SmallVector<Value> getElementsFromStruct(Location loc, Value llvmStruct,\n }\n \n // Create a 32-bit integer constant.\n-Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v) {\n+static Value createConstantI32(Location loc,\n+                               PatternRewriter &rewriter, int32_t v) {\n   auto i32ty = rewriter.getIntegerType(32);\n   return rewriter.create<LLVM::ConstantOp>(loc, i32ty,\n                                            IntegerAttr::get(i32ty, v));\n }\n \n-Value createConstantF32(Location loc, PatternRewriter &rewriter, float v) {\n+static Value createConstantF32(Location loc,\n+                               PatternRewriter &rewriter, float v) {\n   auto type = type::f32Ty(rewriter.getContext());\n   return rewriter.create<LLVM::ConstantOp>(loc, type,\n                                            rewriter.getF32FloatAttr(v));\n }\n \n-Value createConstantF64(Location loc, PatternRewriter &rewriter, float v) {\n+static Value createConstantF64(Location loc,\n+                               PatternRewriter &rewriter, float v) {\n   auto type = type::f64Ty(rewriter.getContext());\n   return rewriter.create<LLVM::ConstantOp>(loc, type,\n                                            rewriter.getF64FloatAttr(v));\n }\n \n // Create an index type constant.\n-Value createIndexConstant(OpBuilder &builder, Location loc,\n-                          TypeConverter *converter, int64_t value) {\n+static Value createIndexConstant(OpBuilder &builder, Location loc,\n+                                 TypeConverter *converter, int64_t value) {\n   Type ty = converter->convertType(builder.getIndexType());\n   return builder.create<LLVM::ConstantOp>(loc, ty,\n                                           builder.getIntegerAttr(ty, value));\n }\n \n // Create an integer constant of \\param width bits.\n-Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n-                                int64_t value) {\n+static Value createLLVMIntegerConstant(OpBuilder &builder, Location loc,\n+                                       short width, int64_t value) {\n   Type ty = builder.getIntegerType(width);\n   return builder.create<LLVM::ConstantOp>(loc, ty,\n                                           builder.getIntegerAttr(ty, value));\n }\n \n /// Helper function to get strides from a given shape and its order\n-SmallVector<Value>\n+static SmallVector<Value>\n getStridesFromShapeAndOrder(ArrayRef<int64_t> shape, ArrayRef<unsigned> order,\n                             Location loc, ConversionPatternRewriter &rewriter) {\n   auto rank = shape.size();\n   SmallVector<Value> strides(rank);\n-  auto stride = 1;\n+  int64_t stride = 1;\n   for (auto idx : order) {\n     strides[idx] = i32_val(stride);\n     stride *= shape[idx];\n@@ -206,7 +250,7 @@ getStridesFromShapeAndOrder(ArrayRef<int64_t> shape, ArrayRef<unsigned> order,\n struct SharedMemoryObject {\n   Value base; // i32 ptr. The start address of the shared memory object.\n   // We need to store strides as Values but not integers because the\n-  // extract_slice instruction can take a slice at artibary offsets.\n+  // extract_slice instruction can take a slice at arbitrary offsets.\n   // Take $a[16:32, 16:32] as an example, though we know the stride of $a[0] is\n   // 32, we need to let the instruction that uses $a to be aware of that.\n   // Otherwise, when we use $a, we only know that the shape of $a is 16x16. If\n@@ -266,7 +310,7 @@ struct SharedMemoryObject {\n   }\n };\n \n-SharedMemoryObject\n+static SharedMemoryObject\n getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n                                 ConversionPatternRewriter &rewriter) {\n   auto elems = getElementsFromStruct(loc, llvmStruct, rewriter);\n@@ -276,8 +320,8 @@ getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n           /*offsets=*/{elems.begin() + 1 + rank, elems.end()}};\n }\n \n-Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n-                  Value val, Value pred) {\n+static Value storeShared(ConversionPatternRewriter &rewriter, Location loc,\n+                         Value ptr, Value val, Value pred) {\n   MLIRContext *ctx = rewriter.getContext();\n   unsigned bits = val.getType().getIntOrFloatBitWidth();\n   const char *c = bits == 64 ? \"l\" : (bits == 16 ? \"h\" : \"r\");\n@@ -290,8 +334,8 @@ Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n   return builder.launch(rewriter, loc, void_ty(ctx));\n }\n \n-Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n-               int i) {\n+static Value shflSync(Location loc, ConversionPatternRewriter &rewriter,\n+                      Value val, int i) {\n   unsigned bits = val.getType().getIntOrFloatBitWidth();\n \n   if (bits == 64) {"}, {"filename": "lib/Conversion/TritonToTritonGPU/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,5 +1,5 @@\n add_mlir_conversion_library(TritonToTritonGPU\n-    TritonToTritonGPU.cpp    \n+    TritonToTritonGPUPass.cpp\n \n     ADDITIONAL_HEADER_DIRS\n     ${PROJECT_SOURCE_DIR}/include/triton/Conversion/TritonToTritonGPU"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "renamed", "additions": 10, "deletions": 2, "changes": 12, "file_content_changes": "@@ -1,16 +1,24 @@\n-#include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPU.h\"\n-#include \"../PassDetail.h\"\n+#include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n+\n #include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n+#include \"mlir/Dialect/GPU/GPUDialect.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n #include \"mlir/Dialect/StandardOps/IR/Ops.h\"\n+#include \"mlir/Pass/Pass.h\"\n #include \"mlir/Transforms/DialectConversion.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n #include \"llvm/ADT/APSInt.h\"\n #include <numeric>\n+\n using namespace mlir;\n using namespace mlir::triton;\n \n+#define GEN_PASS_CLASSES\n+#include \"triton/Conversion/Passes.h.inc\"\n+\n namespace {\n \n template <class Op> class GenericOpPattern : public OpConversionPattern<Op> {"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -1,4 +1,5 @@\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n+\n #include \"mlir/Conversion/Passes.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/ExecutionEngine/ExecutionEngine.h\"\n@@ -11,8 +12,8 @@\n #include \"mlir/Target/LLVMIR/Export.h\"\n #include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n #include \"mlir/Transforms/Passes.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h\"\n-#include \"triton/tools/sys/getenv.hpp\"\n+#include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n+#include \"triton/Tools/Sys/GetEnv.hpp\"\n #include \"llvm/IR/Constants.h\"\n #include \"llvm/IRReader/IRReader.h\"\n #include \"llvm/Linker/Linker.h\"\n@@ -65,14 +66,14 @@ void extractNVVMMetadata(mlir::ModuleOp module,\n     bool hasMetadata{};\n \n     // maxntid\n-    if (op->hasAttr(NVVMMetadataField::MaxNTid)) {\n-      auto attr = op->getAttr(NVVMMetadataField::MaxNTid);\n+    if (op->hasAttr(\"nvvm.maxntid\")) {\n+      auto attr = op->getAttr(\"nvvm.maxntid\");\n       meta.maxntidx = attr.dyn_cast<IntegerAttr>().getInt();\n       hasMetadata = true;\n     }\n \n     // kernel\n-    if (op->hasAttr(NVVMMetadataField::Kernel)) {\n+    if (op->hasAttr(\"nvvm.kernel\")) {\n       meta.is_kernel = true;\n       hasMetadata = true;\n     }\n@@ -208,7 +209,6 @@ void addExternalLibs(mlir::ModuleOp &module,\n \n   DictionaryAttr dict = DictionaryAttr::get(module->getContext(), attrs);\n   module.getOperation()->setAttr(\"triton_gpu.externs\", dict);\n-  return;\n }\n \n bool linkExternLib(llvm::Module &module, llvm::StringRef path) {"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -13,15 +13,15 @@\n \n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"triton/Analysis/Allocation.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h\"\n-#include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPU.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n+#include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n #include \"triton/Dialect/Triton/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n #include \"triton/Target/PTX/PTXTranslation.h\"\n-#include \"triton/tools/sys/getenv.hpp\"\n+#include \"triton/Tools/Sys/GetEnv.hpp\"\n \n #include \"llvm/IR/LegacyPassManager.h\"\n #include \"llvm/IR/Module.h\""}, {"filename": "unittest/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,5 +1,5 @@\n add_triton_ut(\n \tNAME TestPtxAsmFormat\n-\tSRCS PtxAsmFormatTest.cpp\n+\tSRCS PTXAsmFormatTest.cpp\n \tLIBS TritonGPUToLLVM\n )"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/PTXAsmFormatTest.cpp", "status": "renamed", "additions": 8, "deletions": 7, "changes": 15, "file_content_changes": "@@ -1,16 +1,17 @@\n-#include \"triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h\"\n #include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n #include \"mlir/IR/Builders.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n+\n #include <gtest/gtest.h>\n \n namespace mlir {\n namespace triton {\n-class PtxAsmFormatTest : public ::testing::Test {\n+class PTXAsmFormatTest : public ::testing::Test {\n protected:\n   static constexpr int numValues = 4;\n \n-  PtxAsmFormatTest() {\n+  PTXAsmFormatTest() {\n     ctx.loadDialect<arith::ArithmeticDialect>();\n \n     createValues();\n@@ -34,7 +35,7 @@ class PtxAsmFormatTest : public ::testing::Test {\n   Value v[numValues + 1];\n };\n \n-TEST_F(PtxAsmFormatTest, basic) {\n+TEST_F(PTXAsmFormatTest, basic) {\n   PTXBuilder builder;\n \n   // Create the operands needed by the instructions in the PTX code.\n@@ -55,7 +56,7 @@ TEST_F(PtxAsmFormatTest, basic) {\n   ASSERT_EQ(constraints, \"=r,b\"); // $0 -> =r, $1 -> b\n }\n \n-TEST_F(PtxAsmFormatTest, complexInstruction) {\n+TEST_F(PTXAsmFormatTest, complexInstruction) {\n   using triton::CacheModifier;\n   using triton::EvictionPolicy;\n \n@@ -99,7 +100,7 @@ TEST_F(PtxAsmFormatTest, complexInstruction) {\n   EXPECT_EQ(builder.getConstraints(), \"l,b\");\n }\n \n-TEST_F(PtxAsmFormatTest, MultiLinePTX) {\n+TEST_F(PTXAsmFormatTest, MultiLinePTX) {\n   PTXBuilder builder;\n \n   auto *constVal = builder.newConstantOperand(1);\n@@ -121,7 +122,7 @@ TEST_F(PtxAsmFormatTest, MultiLinePTX) {\n   EXPECT_EQ(values[1], v[2]); // $1 -> v[2]\n }\n \n-TEST_F(PtxAsmFormatTest, onlyAttachMLIRArgs) {\n+TEST_F(PTXAsmFormatTest, onlyAttachMLIRArgs) {\n   PTXBuilder builder;\n   const char *ptxCode =\n       \".param .b64 param0;\\n\" // prepare param0 (format string)"}]