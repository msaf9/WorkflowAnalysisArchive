[{"filename": "lib/codegen/analysis/layout.cc", "status": "modified", "additions": 12, "deletions": 8, "changes": 20, "file_content_changes": "@@ -367,12 +367,16 @@ void shared_layout::extract_double_bufferable(ir::value *v, std::shared_ptr<doub\n     res.reset(new double_buffer_info_t{value_1, value_0, phi});\n }\n \n-static bool is_smem(ir::value* v) {\n-  if (dynamic_cast<ir::copy_to_shared_inst*>(v) ||\n-      dynamic_cast<ir::masked_load_async_inst*>(v))\n-    return true;\n-  else\n-    return false;\n+static bool is_smem_in(ir::value* v, const ir::basic_block* bb) {\n+  if (ir::instruction *instr = dynamic_cast<ir::instruction*>(v)) {\n+    if (instr->get_parent() != bb)\n+      return false;\n+    if (dynamic_cast<ir::copy_to_shared_inst*>(v) ||\n+        dynamic_cast<ir::masked_load_async_inst*>(v)) {\n+      return true;\n+    }\n+  }\n+  return false;\n }\n \n /// param:\n@@ -387,14 +391,14 @@ static bool is_multistage_pipe_phi(ir::phi_node* phi, ir::basic_block* bb0, ir::\n     ir::basic_block *cbb0 = cphi->get_incoming_block(0);\n     ir::basic_block *cbb1 = cphi->get_incoming_block(1);\n \n-    if (is_smem(c0)) {\n+    if (is_smem_in(c0, cbb0)) {\n       assert(cbb0 == bb0);\n       values_0.push_back(c0);\n       if (auto phi1 = dynamic_cast<ir::phi_node*>(c1)) {\n         next = phi1;\n         continue;\n       } else {\n-        if (is_smem(c1)) {\n+        if (is_smem_in(c1, cbb1)) {\n           value_1 = c1;\n           assert(cbb1 == bb1);\n           return true;"}, {"filename": "lib/codegen/transform/inline.cc", "status": "modified", "additions": 40, "deletions": 20, "changes": 60, "file_content_changes": "@@ -53,36 +53,56 @@ void inliner::do_inline(ir::function* fn, ir::call_inst* callsite, ir::builder&\n   for(size_t k = 0; k < fn->args().size(); k++)\n     arg_map[fn->args()[k]] = callsite->ops()[k];\n   std::vector<ir::basic_block*> rpo = ir::cfg::reverse_post_order(fn);\n+  // clone instructions\n   for(size_t i = 0; i < new_blocks.size(); i++){\n     ir::basic_block* old_block = fn->blocks()[i];\n     ir::basic_block* new_block = new_blocks[i];\n     builder.set_insert_point(new_block);\n     for(ir::instruction* old_inst: old_block->get_inst_list()){\n-      // clone instruction\n       ir::instruction* new_inst = old_inst->clone();\n-      // replace basic block\n-      for(size_t k = 0; k < new_blocks.size(); k++)\n-        new_inst->replace_uses_of_with(fn->blocks()[k], new_blocks[k]);\n-      // replace values\n-      for(size_t k = 0; k < new_inst->get_num_operands(); k++){\n-        ir::value* op = new_inst->get_operand(k);\n-        if(auto arg_op = dynamic_cast<ir::argument*>(op))\n-          new_inst->set_operand(k, arg_map.at(arg_op));\n-        if(auto inst_op = dynamic_cast<ir::instruction*>(op))\n-          if(inst_map.find(inst_op) != inst_map.end())\n-            new_inst->set_operand(k, inst_map.at(inst_op));\n-      }\n-       // `ret` instruction is a special case:\n-      // instead of returning we need to branch to after the function call\n-      if(ir::return_inst* ret = dynamic_cast<ir::return_inst*>(new_inst)){\n-        if(ir::value* ret_val = ret->get_return_value())\n-          exit_val->add_incoming(ret_val, new_block);\n-        new_inst = ir::branch_inst::create(exit);\n-      }\n       inst_map[old_inst] = new_inst;\n       builder.insert(new_inst);\n     }\n   }\n+  // update basic blocks\n+  for(size_t i = 0; i < new_blocks.size(); i++) {\n+    for (ir::instruction* new_inst: new_blocks[i]->get_inst_list()) {\n+      // replace basic use cases\n+      for(size_t k = 0; k < new_blocks.size(); k++)\n+         new_inst->replace_uses_of_with(fn->blocks()[k], new_blocks[k]);\n+      if(ir::phi_node* phi = dynamic_cast<ir::phi_node*>(new_inst)) {\n+        // additionally replace basic blocks of phi-nodes since\n+        // replace_uses_of_with() does not replace them.\n+        for(unsigned in = 0; in < phi->get_num_incoming(); in++)\n+          for(size_t k = 0; k < new_blocks.size(); k++)\n+            if (phi->get_incoming_block(in) == fn->blocks()[k])\n+              phi->set_incoming_block(in, new_blocks[k]);\n+      }\n+    }\n+  }\n+  // replace operands of instructions after constructing inst_map\n+  for (auto& it: inst_map) {\n+    ir::instruction* new_inst = it.second;\n+    for(size_t k = 0; k < new_inst->get_num_operands(); k++) {\n+      ir::value* op = new_inst->get_operand(k);\n+      if(auto arg_op = dynamic_cast<ir::argument*>(op))\n+        new_inst->set_operand(k, arg_map.at(arg_op));\n+      if(auto inst_op = dynamic_cast<ir::instruction*>(op))\n+        if(inst_map.find(inst_op) != inst_map.end())\n+          new_inst->set_operand(k, inst_map.at(inst_op));\n+    }\n+    // handles a ret instruciton.\n+    // instead of returning we need to branch to after the function call\n+    if(ir::return_inst* ret = dynamic_cast<ir::return_inst*>(new_inst)) {\n+      if(ir::value* ret_val = ret->get_return_value())\n+        exit_val->add_incoming(ret_val, new_inst->get_parent());\n+      // replace ret with branch\n+      ir::instruction* new_br_inst = ir::branch_inst::create(exit);\n+      builder.set_insert_point(new_inst->get_parent());\n+      builder.insert(new_br_inst);\n+      new_inst->erase_from_parent();\n+    }\n+  }\n   if(exit_val->get_num_incoming() == 1)\n     exit_val->replace_all_uses_with(exit_val->get_incoming_value(0));\n   // done -- make sure insert point is properly set to exit block"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 40, "deletions": 35, "changes": 75, "file_content_changes": "@@ -437,46 +437,51 @@ typedef std::map<std::string, py::object> asm_map_t;\n void init_triton_codegen(py::module &&m) {\n   m.def(\"compile_ttir\",\n       [](backend_t backend, ir::module &ir, uint64_t device, int num_warps, int num_stages, py::dict& extern_libs, size_t cc) {\n-          py::gil_scoped_release allow_threads;\n-          std::string name = ir.get_function_list()[0]->get_name();\n-          // record asm as we generate\n-          asm_map_t asm_map;\n           std::ostringstream ttir;\n-          ir.print(ttir);\n-          asm_map[\"ttir\"] = py::cast(ttir.str());\n-          llvm::LLVMContext ctx;\n-          // construct extern lib map\n-          triton::codegen::ExternLibMap extern_lib_map;\n-          for (auto item : extern_libs) {\n-            auto name = item.first.cast<std::string>();\n-            auto path = item.second.cast<std::string>();\n-            extern_lib_map.emplace(\n-                name, triton::codegen::create_extern_lib(name, path));\n-          }\n-          // device properties\n-          if (cc == 0) {\n-            CUdevice dev = (CUdevice)device;\n-            size_t major = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR>(dev);\n-            size_t minor = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR>(dev);\n-            cc = major*10 + minor;\n-          }\n-          int version;\n-          std::string ptxas_path = drv::path_to_ptxas(version);\n-          // Triton-IR -> NVPTX LLVM-IR\n-          triton::codegen::nvidia_cu_target target(cc);\n           int n_shared_bytes;\n-          auto llvm = triton::codegen::add_passes_to_emit_bin(\n-              ir, ctx, &target, num_warps, num_stages, n_shared_bytes, extern_lib_map);\n           std::string tmp;\n-          llvm::raw_string_ostream llir(tmp);\n-          llir << *llvm;\n-          llir.flush();\n+          std::string ptx;\n+          std::string cubin;\n+          std::string name;\n+          { // Scope where the GIL is released\n+            py::gil_scoped_release allow_threads;\n+            name = ir.get_function_list()[0]->get_name();\n+            ir.print(ttir);\n+            llvm::LLVMContext ctx;\n+            // construct extern lib map\n+            triton::codegen::ExternLibMap extern_lib_map;\n+            for (auto item : extern_libs) {\n+              auto name = item.first.cast<std::string>();\n+              auto path = item.second.cast<std::string>();\n+              extern_lib_map.emplace(\n+                  name, triton::codegen::create_extern_lib(name, path));\n+            }\n+            // device properties\n+            if (cc == 0) {\n+              CUdevice dev = (CUdevice)device;\n+              size_t major = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR>(dev);\n+              size_t minor = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR>(dev);\n+              cc = major*10 + minor;\n+            }\n+            int version;\n+            std::string ptxas_path = drv::path_to_ptxas(version);\n+            // Triton-IR -> NVPTX LLVM-IR\n+            triton::codegen::nvidia_cu_target target(cc);\n+            auto llvm = triton::codegen::add_passes_to_emit_bin(\n+                ir, ctx, &target, num_warps, num_stages, n_shared_bytes, extern_lib_map);\n+            llvm::raw_string_ostream llir(tmp);\n+            llir << *llvm;\n+            llir.flush();\n+            // LLVM-IR -> PTX\n+            ptx = drv::llir_to_ptx(llvm.get(), cc, version);\n+            // PTX -> Binary\n+            cubin = drv::ptx_to_cubin(ptx, ptxas_path, cc);\n+          }\n+          asm_map_t asm_map;\n+          asm_map[\"ttir\"] = py::cast(ttir.str());\n           asm_map[\"llir\"] = py::cast(tmp);\n-          // LLVM-IR -> PTX\n-          std::string ptx = drv::llir_to_ptx(llvm.get(), cc, version);\n           asm_map[\"ptx\"] = py::cast(ptx);\n-          // PTX -> Binary\n-          std::string cubin = drv::ptx_to_cubin(ptx, ptxas_path, cc);\n+\n           if(!cubin.empty()){\n             py::bytes bytes(cubin);\n             asm_map[\"cubin\"] = bytes;"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 35, "deletions": 0, "changes": 35, "file_content_changes": "@@ -1401,6 +1401,41 @@ def kernel(X, s):\n     kernel[(1,)](x_tri, 32)\n     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256) % 8)\n \n+# -------------\n+# test call\n+# -------------\n+\n+\n+@triton.jit\n+def val_multiplier(val, i):\n+    return val * i\n+\n+\n+@triton.jit\n+def vecmul_kernel(ptr, n_elements, rep):\n+    pid = tl.program_id(axis=0)\n+    offsets = pid * 128 + tl.arange(0, 128)\n+    mask = offsets < n_elements\n+    vec = tl.load(ptr + offsets, mask=mask)\n+    for i in range(1, rep):\n+        vec = val_multiplier(vec, i)\n+    tl.store(ptr + offsets, vec, mask=mask)\n+\n+\n+def test_call():\n+\n+    @triton.jit\n+    def kernel(ptr, n_elements, num1, num2):\n+        vecmul_kernel(ptr, n_elements, num1)\n+        vecmul_kernel(ptr, n_elements, num2)\n+\n+    size = 1024\n+    rand_val = numpy_random((size,), dtype_str=\"float32\")\n+    rand_val_tri = to_triton(rand_val, device='cuda')\n+    kernel[(size // 128,)](rand_val_tri, size, 3, 5)\n+\n+    ans = rand_val * 1 * 2 * 1 * 2 * 3 * 4\n+    np.testing.assert_equal(to_numpy(rand_val_tri), ans)\n \n # -------------\n # test if"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -249,7 +249,8 @@ def backward(ctx, do):\n             BLOCK_M=ctx.BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n         )\n \n-        num_warps = 4 if ctx.BLOCK_DMODEL <= 64 else 8\n+        # NOTE: kernel currently buggy for other values of `num_warps`\n+        num_warps = 8\n         _bwd_kernel[(ctx.grid[1],)](\n             q, k, v, ctx.sm_scale,\n             o, do_scaled,"}]