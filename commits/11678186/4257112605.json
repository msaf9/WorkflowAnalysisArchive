[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -405,13 +405,13 @@ def TT_MakeRangeOp : TT_Op<\"make_range\", [Pure]> {\n }\n \n //\n-// Make PrintfOp\n+// Make PrintOp\n //\n-def TT_PrintfOp : TT_Op<\"printf\", [MemoryEffects<[MemWrite]>]>,\n+def TT_PrintOp : TT_Op<\"print\", [MemoryEffects<[MemWrite]>]>,\n   Arguments<(ins StrAttr:$prefix, Variadic<AnyTypeOf<[TT_Type]>>:$args)> {\n-  let summary = \"Device-side printf, as in CUDA for debugging\";\n+  let summary = \"Device-side print, as in CUDA for debugging\";\n   let description = [{\n-    `tt.printf` takes a literal string prefix and an arbitrary number of scalar or tensor arguments that should be printed.\n+    `tt.print` takes a literal string prefix and an arbitrary number of scalar or tensor arguments that should be printed.\n     format are generated automatically from the arguments.\n   }];\n   let assemblyFormat = [{"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -94,13 +94,13 @@ struct BroadcastOpConversion\n   }\n };\n \n-struct PrintfOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintfOp> {\n+struct PrintOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n-      triton::PrintfOp>::ConvertTritonGPUOpToLLVMPattern;\n+      triton::PrintOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   LogicalResult\n-  matchAndRewrite(triton::PrintfOp op, OpAdaptor adaptor,\n+  matchAndRewrite(triton::PrintOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     auto loc = op->getLoc();\n     SmallVector<Value, 16> operands;\n@@ -596,7 +596,7 @@ namespace LLVM {\n \n void vprintf(StringRef msg, ValueRange args,\n              ConversionPatternRewriter &rewriter) {\n-  PrintfOpConversion::llPrintf(msg, args, rewriter);\n+  PrintOpConversion::llPrintf(msg, args, rewriter);\n }\n \n void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n@@ -633,6 +633,6 @@ void populateTritonGPUToLLVMPatterns(\n   patterns.add<GetNumProgramsOpConversion>(typeConverter, benefit);\n   patterns.add<MakeRangeOpConversion>(typeConverter, indexCacheInfo, benefit);\n   patterns.add<ReturnOpConversion>(typeConverter, benefit);\n-  patterns.add<PrintfOpConversion>(typeConverter, benefit);\n+  patterns.add<PrintOpConversion>(typeConverter, benefit);\n   patterns.add<AssertOpConversion>(typeConverter, benefit);\n }"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 5, "deletions": 6, "changes": 11, "file_content_changes": "@@ -443,15 +443,14 @@ struct TritonReducePattern : public OpConversionPattern<triton::ReduceOp> {\n   }\n };\n \n-struct TritonPrintfPattern : public OpConversionPattern<triton::PrintfOp> {\n-  using OpConversionPattern<triton::PrintfOp>::OpConversionPattern;\n+struct TritonPrintfPattern : public OpConversionPattern<triton::PrintOp> {\n+  using OpConversionPattern<triton::PrintOp>::OpConversionPattern;\n \n   LogicalResult\n-  matchAndRewrite(triton::PrintfOp op,\n-                  typename triton::PrintfOp::Adaptor adaptor,\n+  matchAndRewrite(triton::PrintOp op, typename triton::PrintOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<triton::PrintfOp>(op, op.getPrefixAttr(),\n-                                                  adaptor.getOperands());\n+    rewriter.replaceOpWithNewOp<triton::PrintOp>(op, op.getPrefixAttr(),\n+                                                 adaptor.getOperands());\n     return success();\n   }\n };"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1326,11 +1326,11 @@ void init_triton_ir(py::module &&m) {\n              return self.create<mlir::arith::SelectOp>(loc, condition,\n                                                        trueValue, falseValue);\n            })\n-      .def(\"create_printf\",\n+      .def(\"create_print\",\n            [](mlir::OpBuilder &self, const std::string &prefix,\n               const std::vector<mlir::Value> &values) -> void {\n              auto loc = self.getUnknownLoc();\n-             self.create<mlir::triton::PrintfOp>(\n+             self.create<mlir::triton::PrintOp>(\n                  loc,\n                  mlir::StringAttr::get(self.getContext(),\n                                        llvm::StringRef(prefix)),"}, {"filename": "python/test/unit/language/assert_helper.py", "status": "modified", "additions": 30, "deletions": 8, "changes": 38, "file_content_changes": "@@ -1,23 +1,45 @@\n+import sys\n+\n import torch\n from torch.testing import assert_close\n \n import triton\n import triton.language as tl\n \n \n-def test_device_assert():\n-    @triton.jit\n-    def kernel(X, Y, BLOCK: tl.constexpr):\n-        x = tl.load(X + tl.arange(0, BLOCK))\n-        tl.device_assert(x == 0, \"x != 0\")\n-        tl.store(Y + tl.arange(0, BLOCK), x)\n+@triton.jit\n+def kernel_device_assert(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    tl.device_assert(x == 0, \"x != 0\")\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+@triton.jit\n+def kernel_assert(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    assert x == 0, \"x != 0\"\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+@triton.jit\n+def kernel_static_assert(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    tl.static_assert(BLOCK == 128, \"BLOCK != 128\")\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n \n+def test_assert(func: str):\n     shape = (128, )\n     x = torch.arange(0, shape[0], dtype=torch.int32, device='cuda')\n     y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n-    kernel[(1,)](x, y, BLOCK=shape[0])\n+    if func == \"device_assert\":\n+        kernel_device_assert[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"assert\":\n+        kernel_assert[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"static_assert\":\n+        kernel_static_assert[(1,)](x, y, BLOCK=shape[0])\n     assert_close(y, x)\n \n \n if __name__ == \"__main__\":\n-    test_device_assert()\n+    test_assert(sys.argv[1])"}, {"filename": "python/test/unit/language/print_helper.py", "status": "added", "additions": 46, "deletions": 0, "changes": 46, "file_content_changes": "@@ -0,0 +1,46 @@\n+import sys\n+\n+import torch\n+from torch.testing import assert_close\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def kernel_device_print(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    tl.device_print(\"\", x)\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+@triton.jit\n+def kernel_print(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    print(\"\", x)\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+@triton.jit\n+def kernel_static_print(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    tl.static_print(x)\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+def test_print(func: str, data_type: str):\n+    shape = (128, )\n+    # limit the range of integers so that the sum does not overflow\n+    x = torch.arange(0, shape[0], dtype=torch.int32, device='cuda').to(getattr(torch, data_type))\n+    y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n+    if func == \"device_print\":\n+        kernel_device_print[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"print\":\n+        kernel_print[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"static_print\":\n+        kernel_static_print[(1,)](x, y, BLOCK=shape[0])\n+    assert_close(y, x)\n+\n+\n+if __name__ == \"__main__\":\n+    test_print(sys.argv[1], sys.argv[2])"}, {"filename": "python/test/unit/language/printf_helper.py", "status": "removed", "additions": 0, "deletions": 26, "changes": 26, "file_content_changes": "@@ -1,26 +0,0 @@\n-import sys\n-\n-import torch\n-from torch.testing import assert_close\n-\n-import triton\n-import triton.language as tl\n-\n-\n-def test_device_print(data_type: str):\n-    @triton.jit\n-    def kernel(X, Y, BLOCK: tl.constexpr):\n-        x = tl.load(X + tl.arange(0, BLOCK))\n-        tl.device_print(\"\", x)\n-        tl.store(Y + tl.arange(0, BLOCK), x)\n-\n-    shape = (128, )\n-    # limit the range of integers so that the sum does not overflow\n-    x = torch.arange(0, shape[0], dtype=torch.int32, device='cuda').to(getattr(torch, data_type))\n-    y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n-    kernel[(1,)](x, y, BLOCK=shape[0])\n-    assert_close(y, x)\n-\n-\n-if __name__ == \"__main__\":\n-    test_device_print(sys.argv[1])"}, {"filename": "python/test/unit/language/test_subprocess.py", "status": "modified", "additions": 23, "deletions": 11, "changes": 34, "file_content_changes": "@@ -5,38 +5,50 @@\n import pytest\n \n dir_path = os.path.dirname(os.path.realpath(__file__))\n-printf_path = os.path.join(dir_path, \"printf_helper.py\")\n+print_path = os.path.join(dir_path, \"print_helper.py\")\n assert_path = os.path.join(dir_path, \"assert_helper.py\")\n \n # TODO: bfloat16 after LLVM-15\n+func_types = [\"device_assert\", \"assert\", \"static_assert\"]\n torch_types = [\"int8\", \"uint8\", \"int16\", \"int32\", \"long\", \"float16\", \"float32\", \"float64\"]\n \n \n-@pytest.mark.parametrize(\"data_type\", torch_types)\n-def test_device_print(data_type: str):\n-    proc = subprocess.Popen([sys.executable, printf_path, data_type], stdout=subprocess.PIPE, shell=False)\n+@pytest.mark.parametrize(\"func_type, data_type\",\n+                         [(\"device_print\", data_type) for data_type in torch_types] +\n+                         [(\"print\", \"int32\"), (\"static_print\", \"int32\")])\n+def test_print(func_type: str, data_type: str):\n+    proc = subprocess.Popen([sys.executable, print_path, func_type, data_type], stdout=subprocess.PIPE, shell=False)\n     outs, _ = proc.communicate()\n     outs = outs.split()\n     new_lines = set()\n     for line in outs:\n         try:\n-            value = int(float(line))\n+            value = line\n+            if func_type != \"static_print\":\n+                value = int(float(line))\n             new_lines.add(value)\n         except Exception as e:\n             print(e)\n-    for i in range(128):\n-        assert i in new_lines\n-    assert len(new_lines) == 128\n+    if func_type != \"static_print\":\n+        for i in range(128):\n+            assert i in new_lines\n+        assert len(new_lines) == 128\n+    else:\n+        assert len(new_lines) == 1\n \n \n-def test_device_assert():\n+@pytest.mark.parametrize(\"func_type\", func_types)\n+def test_assert(func_type: str):\n     os.environ[\"TRITON_ENABLE_DEVICE_ASSERT\"] = \"1\"\n-    proc = subprocess.Popen([sys.executable, assert_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)\n+    proc = subprocess.Popen([sys.executable, assert_path, func_type], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)\n     _, errs = proc.communicate()\n     errs = errs.splitlines()\n     num_errs = 0\n     for err in errs:\n         if \"x != 0\" in err.decode(\"utf-8\"):\n             num_errs += 1\n     os.environ[\"TRITON_ENABLE_DEVICE_ASSERT\"] = \"0\"\n-    assert num_errs == 127\n+    if func_type != \"static_assert\":\n+        assert num_errs == 127\n+    else:\n+        assert num_errs == 0"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 16, "deletions": 5, "changes": 21, "file_content_changes": "@@ -123,10 +123,13 @@ def __init__(self, context, prototype, gscope, attributes, constants, function_n\n             'min': triton.language.minimum,\n             'float': float,\n             'int': int,\n-            'print': print,\n+            'print': triton.language.core.device_print,\n             'isinstance': isinstance,\n             'getattr': getattr,\n         }\n+        self.static_functions = [\n+            'static_print', 'static_assert'\n+        ]\n         self.scf_stack = []\n         # SSA-construction\n         # name => triton.language.tensor\n@@ -759,8 +762,10 @@ def visit_keyword(self, node):\n         return {node.arg: self.visit(node.value)}\n \n     def visit_Assert(self, node) -> Any:\n+        test = self.visit(node.test)\n+        msg = self.visit(node.msg)\n         # Convert assert to triton's device_assert which happens on the device\n-        return triton.language.core.device_assert(node.test, node.msg, _builder=self.builder)\n+        return triton.language.core.device_assert(test, msg, _builder=self.builder)\n \n     def visit_Call(self, node):\n         fn = self.visit(node.func)\n@@ -770,6 +775,15 @@ def visit_Call(self, node):\n         for keyword in node.keywords:\n             kws.update(self.visit(keyword))\n         args = [self.visit(arg) for arg in node.args]\n+        if fn.__name__ == 'print':\n+            fn = self.builtins['print']\n+        elif fn.__name__ in self.static_functions:\n+            if fn.__name__ == \"static_print\":\n+                print(*args, **kws)\n+                return\n+            elif fn.__name__ == \"static_assert\":\n+                assert args[0], args[1]\n+                return\n         if isinstance(fn, triton.runtime.JITFunction):\n             from inspect import getcallargs\n             args = getcallargs(fn.fn, *args, **kws)\n@@ -813,9 +827,6 @@ def visit_Call(self, node):\n         if fn in self.builtins.values():\n             args = [arg.value if isinstance(arg, triton.language.constexpr) else arg\n                     for arg in args]\n-        if fn.__name__ == 'print':\n-            prefix = args[0]\n-            return triton.language.core.device_print(prefix, *args[1:], _builder=self.builder)\n         return fn(*args, **kws)\n \n     def visit_Constant(self, node):"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -63,6 +63,8 @@\n     sin,\n     softmax,\n     sqrt,\n+    static_assert,\n+    static_print,\n     store,\n     sum,\n     swizzle2d,\n@@ -166,6 +168,8 @@\n     \"softmax\",\n     \"sqrt\",\n     \"static_range\",\n+    \"static_assert\",\n+    \"static_print\",\n     \"store\",\n     \"sum\",\n     \"swizzle2d\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1304,13 +1304,13 @@ def zeros_like(input):\n \n \n @builtin\n-def static_print(*values, sep: str = \" \", end: str = \"\\n\", file=None, flush=False):\n-    print(*values, sep=sep, end=end, file=file, flush=flush)\n+def static_print(*values, sep: str = \" \", end: str = \"\\n\", file=None, flush=False, _builder=None):\n+    pass\n \n \n @builtin\n-def static_assert(cond, msg=\"\"):\n-    assert cond, msg\n+def static_assert(cond, msg=\"\", _builder=None):\n+    pass\n \n \n @builtin"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1229,7 +1229,7 @@ def device_print(prefix: str, args: List[tl.tensor], builder: ir.builder) -> tl.\n     new_args = []\n     for arg in args:\n         new_args.append(arg.handle)\n-    return tl.tensor(builder.create_printf(prefix, new_args), tl.void)\n+    return tl.tensor(builder.create_print(prefix, new_args), tl.void)\n \n \n def device_assert(cond: tl.tensor, msg: str, file_name: str, func_name, lineno: int, builder: ir.builder) -> tl.tensor:"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -157,7 +157,8 @@ def is_divisible_by_16(x):\n             return False\n         divisible_by_16 = {i for i, arg in enumerate(args) if is_divisible_by_16(arg) and i not in self.do_not_specialize}\n         equal_to_1 = {i for i, arg in enumerate(args) if isinstance(arg, int) and arg == 1 and i not in self.do_not_specialize}\n-        return namedtuple(\"instance_descriptor\", [\"divisible_by_16\", \"equal_to_1\"])(tuple(divisible_by_16), tuple(equal_to_1))\n+        assert_enabled = os.environ.get(\"TRITON_ASSERT_ENABLED\", \"0\") == \"1\"\n+        return namedtuple(\"instance_descriptor\", [\"divisible_by_16\", \"equal_to_1\", \"asssert_enabled\"])(tuple(divisible_by_16), tuple(equal_to_1), assert_enabled)\n         # return _triton.code_gen.instance_descriptor(divisible_by_16, equal_to_1)\n \n     @staticmethod"}]