[{"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 71, "deletions": 97, "changes": 168, "file_content_changes": "@@ -295,7 +295,7 @@ LogicalResult getForwardEncoding(Attribute sourceEncoding, Operation *op,\n   return failure();\n }\n \n-inline bool expensive_to_remat(Operation *op) {\n+inline bool expensiveToRemat(Operation *op, const Attribute &targetEncoding) {\n   if (!op)\n     return true;\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n@@ -311,7 +311,7 @@ inline bool expensive_to_remat(Operation *op) {\n LogicalResult simulateBackwardRematerialization(\n     Operation *initOp, SetVector<Operation *> &processed,\n     SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n-    Attribute targetEncoding) {\n+    const Attribute &targetEncoding) {\n   // DFS\n   std::vector<std::pair<Operation *, Attribute>> queue;\n   queue.emplace_back(initOp, targetEncoding);\n@@ -325,34 +325,38 @@ LogicalResult simulateBackwardRematerialization(\n     queue.pop_back();\n     // If the current operation is expensive to rematerialize,\n     // we stop everything\n-    if (expensive_to_remat(currOp))\n-      return mlir::failure();\n-    // we would propagate the conversion here\n+    if (expensiveToRemat(currOp, currLayout))\n+      break;\n+    // A conversion will be removed here (i.e. transferred to operands)\n     numCvts -= 1;\n-    // check if the conversion could be folded at this operation\n-    if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n-            triton::MakeRangeOp, triton::SplatOp>(*currOp))\n-      continue;\n-    // done processing\n+    // Done processing\n     processed.insert(currOp);\n     layout.insert(currLayout);\n-    // add all operands to the queue\n+    // Add all operands to the queue\n     for (Value argI : currOp->getOperands()) {\n       Attribute newEncoding;\n-      // cannot invert the current encoding for this operand\n+      // Cannot invert the current encoding for this operand\n       // we stop everything\n-      if (failed(invertEncoding(currLayout, currOp, newEncoding))) {\n+      if (failed(invertEncoding(currLayout, currOp, newEncoding)))\n         return mlir::failure();\n-      }\n       if (toConvert.count(argI) && toConvert[argI] != newEncoding)\n         return mlir::failure();\n-      //\n       Operation *opArgI = argI.getDefiningOp();\n       toConvert.insert({argI, newEncoding});\n-      if (!opArgI || processed.contains(opArgI) ||\n-          (opArgI->getBlock() != initOp->getBlock()))\n+      // 1. Only convert RankedTensorType\n+      // 2. Skip if there's no defining op\n+      // 3. Skip if the defining op has already been processed\n+      // 4. Skip or the defining op is in a different block\n+      if (!argI.getType().isa<RankedTensorType>() || !opArgI ||\n+          processed.contains(opArgI) ||\n+          opArgI->getBlock() != currOp->getBlock())\n+        continue;\n+      // If the conversion can be folded into opArgI then\n+      // we don't count this conversion as expensive\n+      if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n+              triton::MakeRangeOp, triton::SplatOp>(*opArgI))\n         continue;\n-      // we add one expensive conversion for the current operand\n+      // We add one expensive conversion for the current operand\n       numCvts += 1;\n       queue.emplace_back(opArgI, newEncoding);\n     }\n@@ -396,6 +400,9 @@ class MoveConvertOutOfIf : public mlir::RewritePattern {\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n     auto ifOp = cast<scf::IfOp>(*op);\n+    // If \u201cscf.if\u201d defines no values, \u201cscf.yield\u201d will be inserted implicitly.\n+    // However, \"scf.else\" is not required to be present, so we need to check\n+    // if it exists.\n     auto thenYield = ifOp.thenYield();\n     int numOps = thenYield.getNumOperands();\n     SmallVector<Value> newThenYieldOps = thenYield.getOperands();\n@@ -414,52 +421,62 @@ class MoveConvertOutOfIf : public mlir::RewritePattern {\n \n     BlockAndValueMapping mapping;\n     for (size_t i = 0; i < numOps; i++) {\n-      // Handle then\n-      if (auto thenCvt = dyn_cast<triton::gpu::ConvertLayoutOp>(\n-              thenYield.getOperand(i).getDefiningOp())) {\n-        if (std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1) {\n+      auto thenCvt = dyn_cast<triton::gpu::ConvertLayoutOp>(\n+          thenYield.getOperand(i).getDefiningOp());\n+      if (hasElse) {\n+        auto elseYield = ifOp.elseYield();\n+        auto elseCvt = dyn_cast<triton::gpu::ConvertLayoutOp>(\n+            elseYield.getOperand(i).getDefiningOp());\n+        if (thenCvt && elseCvt &&\n+            std::distance(elseCvt->user_begin(), elseCvt->user_end()) == 1 &&\n+            std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1 &&\n+            thenCvt.getOperand().getType() == elseCvt.getOperand().getType()) {\n+          // If thenCvt and elseCvt's type are the same, it means a single\n+          // conversion is enough to replace both of them. We can move the\n+          // conversion out of scf.if and replace both thenCvt and elseCvt with\n+          // the new conversion.\n           mapping.map(thenCvt.getResult(), thenCvt.getOperand());\n           thenCvts.insert((Operation *)thenCvt);\n           newRetTypes.push_back(thenCvt.getOperand().getType());\n-        } else {\n-          newRetTypes.push_back(thenYield.getOperand(i).getType());\n-        }\n-      }\n-      // Handle else\n-      if (!hasElse)\n-        continue;\n-      if (auto elseCvt = dyn_cast<triton::gpu::ConvertLayoutOp>(\n-              elseYield.getOperand(i).getDefiningOp()))\n-        if (std::distance(elseCvt->user_begin(), elseCvt->user_end()) == 1) {\n           mapping.map(elseCvt.getResult(), elseCvt.getOperand());\n           elseCvts.insert((Operation *)elseCvt);\n-        }\n+        } else\n+          // Cannot move out of scf.if because thenCvt != elseCvt\n+          // Moving it out of scf.if will introduce a new conversion\n+          newRetTypes.push_back(thenYield.getOperand(i).getType());\n+      } else {\n+        if (thenCvt &&\n+            std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1) {\n+          // If there's only a single use of the conversion then we can move it\n+          mapping.map(thenCvt.getResult(), thenCvt.getOperand());\n+          thenCvts.insert((Operation *)thenCvt);\n+          newRetTypes.push_back(thenCvt.getOperand().getType());\n+        } else\n+          // Cannot move out of scf.if because either there's another use of\n+          // the conversion or there's no conversion at all\n+          newRetTypes.push_back(thenYield.getOperand(i).getType());\n+      }\n     }\n     if (mapping.getValueMap().empty())\n       return mlir::failure();\n \n-    rewriter.setInsertionPoint(op);\n     auto newIfOp = rewriter.create<scf::IfOp>(ifOp.getLoc(), newRetTypes,\n                                               ifOp.getCondition(), hasElse);\n-    // rematerialize `then` block\n-    rewriter.setInsertionPointToEnd(newIfOp.thenBlock());\n-    for (Operation &op : ifOp.thenBlock()->getOperations()) {\n-      if (thenCvts.contains(&op)) {\n-        mapping.map(op.getResult(0), mapping.lookup(op.getOperand(0)));\n-        continue;\n-      }\n-      rewriter.clone(op, mapping);\n-    }\n-    // rematerialize `else` block\n-    if (hasElse) {\n-      rewriter.setInsertionPointToEnd(newIfOp.elseBlock());\n-      for (Operation &op : ifOp.elseBlock()->getOperations()) {\n-        if (elseCvts.contains(&op)) {\n-          mapping.map(op.getResult(0), mapping.lookup(op.getOperand(0)));\n+    auto rematerialize = [&](Block *block, SetVector<Operation *> &cvts) {\n+      for (Operation &op : block->getOperations()) {\n+        if (cvts.contains(&op)) {\n+          if (mapping.contains(op.getOperand(0)))\n+            mapping.map(op.getResult(0), mapping.lookup(op.getOperand(0)));\n           continue;\n         }\n         rewriter.clone(op, mapping);\n       }\n+    };\n+    rewriter.setInsertionPointToEnd(newIfOp.thenBlock());\n+    rematerialize(ifOp.thenBlock(), thenCvts);\n+    if (hasElse) {\n+      rewriter.setInsertionPointToEnd(newIfOp.elseBlock());\n+      rematerialize(ifOp.elseBlock(), elseCvts);\n     }\n \n     rewriter.setInsertionPointAfter(newIfOp);\n@@ -508,7 +525,7 @@ class FoldConvertAndReduce : public mlir::RewritePattern {\n     llvm::MapVector<Value, Attribute> toConvert;\n     for (Operation *op : cvtSlices) {\n       // don't rematerialize anything expensive\n-      if (expensive_to_remat(op))\n+      if (expensiveToRemat(op, srcEncoding))\n         return failure();\n       // don't rematerialize non-element-wise\n       if (!op->hasTrait<mlir::OpTrait::Elementwise>())\n@@ -595,52 +612,8 @@ class RematerializeBackward : public mlir::RewritePattern {\n     SetVector<Attribute> layout;\n     llvm::MapVector<Value, Attribute> toConvert;\n     std::vector<std::pair<Operation *, Attribute>> queue;\n-    queue.emplace_back(cvt, targetType.getEncoding());\n-    int numCvts = 1;\n-    while (!queue.empty()) {\n-      Operation *currOp;\n-      Attribute currLayout;\n-      std::tie(currOp, currLayout) = queue.back();\n-      queue.pop_back();\n-      // If the current operation is expensive to rematerialize,\n-      // we stop everything\n-      if (expensive_to_remat(currOp))\n-        break;\n-      // a conversion will be removed here (i.e. transferred to operands)\n-      numCvts -= 1;\n-      // done processing\n-      processed.insert(currOp);\n-      layout.insert(currLayout);\n-      // add all operands to the queue\n-      for (Value argI : currOp->getOperands()) {\n-        Attribute newEncoding;\n-        // cannot invert the current encoding for this operand\n-        // we stop everything\n-        if (failed(invertEncoding(currLayout, currOp, newEncoding)))\n-          return mlir::failure();\n-        if (toConvert.count(argI) && toConvert[argI] != newEncoding)\n-          return mlir::failure();\n-        //\n-        Operation *opArgI = argI.getDefiningOp();\n-        if (expensive_to_remat(opArgI))\n-          return mlir::failure();\n-        toConvert.insert({argI, newEncoding});\n-        if (!opArgI || processed.contains(opArgI) ||\n-            (opArgI->getBlock() != cvt->getBlock()))\n-          continue;\n-        // if the conversion can be folded into opArgI then\n-        // we don't count this conversion as expensive\n-        if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n-                triton::MakeRangeOp, triton::SplatOp>(*opArgI))\n-          continue;\n-        // we add one expensive conversion for the current operand\n-        numCvts += 1;\n-        queue.emplace_back(opArgI, newEncoding);\n-      }\n-    }\n-    // if rematerialization would add more conversions than it removes\n-    // then we don't do it\n-    if (numCvts > 0)\n+    if (failed(simulateBackwardRematerialization(\n+            cvt, processed, layout, toConvert, targetType.getEncoding())))\n       return mlir::failure();\n \n     SmallVector<Value, 4> sortedValues;\n@@ -842,7 +815,8 @@ class RematerializeForward : public mlir::RewritePattern {\n       for (Value arg : op->getOperands()) {\n         Operation *argOp = arg.getDefiningOp();\n         if (argOp && (argOp != cvt) &&\n-            !isa<arith::ConstantOp, triton::SplatOp>(argOp)) {\n+            !isa<arith::ConstantOp, triton::SplatOp, triton::MakeRangeOp>(\n+                argOp)) {\n           return failure();\n         }\n       }"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 84, "deletions": 0, "changes": 84, "file_content_changes": "@@ -7,6 +7,7 @@\n // CHECK: [[row_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n // CHECK: [[col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK: [[col_layout_novec:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n+// CHECK-LABEL: cst\n func @cst() -> tensor<1024xi32, #layout1> {\n   %cst = arith.constant dense<0> : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %cst : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n@@ -15,6 +16,7 @@ func @cst() -> tensor<1024xi32, #layout1> {\n   return %1: tensor<1024xi32, #layout1>\n }\n \n+// CHECK-LABEL: range\n func @range() -> tensor<1024xi32, #layout1> {\n   %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %0 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n@@ -23,6 +25,7 @@ func @range() -> tensor<1024xi32, #layout1> {\n   return %1: tensor<1024xi32, #layout1>\n }\n \n+// CHECK-LABEL: splat\n func @splat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   %0 = tt.splat %arg0 : (i32) -> tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %0 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n@@ -31,6 +34,7 @@ func @splat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   return %1: tensor<1024xi32, #layout1>\n }\n \n+// CHECK-LABEL: remat\n func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #layout0>\n   %1 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #layout0>\n@@ -50,6 +54,86 @@ func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   // CHECK: return %6 : tensor<1024xi32, [[target_layout]]>\n }\n \n+// CHECK-LABEL: if\n+func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout1>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout1>\n+  %2 = arith.muli %1, %c32_i32 : tensor<1024xi32, #layout1>\n+  %3 = arith.addi %2, %c32_i32 : tensor<1024xi32, #layout1>\n+  %4 = arith.cmpi sgt, %0, %arg0 : i32\n+  %5 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>, #layout0>\n+  scf.if %4 {\n+    %6 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout1>) -> tensor<1024xi32, #layout0>\n+    tt.store %5, %6 : tensor<1024xi32, #layout0>\n+  }\n+  return\n+}\n+\n+// CHECK-LABEL: if_convert_else_not\n+func @if_convert_else_not(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n+  %9 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout1>\n+  %2 = arith.muli %1, %c32_i32 : tensor<1024xi32, #layout0>\n+  %3 = arith.addi %2, %c32_i32 : tensor<1024xi32, #layout0>\n+  %4 = arith.cmpi sgt, %0, %arg0 : i32\n+  %5 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>, #layout1>\n+  %8 = scf.if %4 -> tensor<1024xi32, #layout1> {\n+    %6 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n+    scf.yield %6 : tensor<1024xi32, #layout1>\n+  } else {\n+    scf.yield %9 : tensor<1024xi32, #layout1>\n+  }\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  tt.store %5, %8 : tensor<1024xi32, #layout1>\n+  return\n+}\n+\n+// CHECK-LABEL: if_not_else_convert\n+func @if_not_else_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n+  %9 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout1>\n+  %2 = arith.muli %1, %c32_i32 : tensor<1024xi32, #layout0>\n+  %3 = arith.addi %2, %c32_i32 : tensor<1024xi32, #layout0>\n+  %4 = arith.cmpi sgt, %0, %arg0 : i32\n+  %5 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>, #layout1>\n+  %8 = scf.if %4 -> tensor<1024xi32, #layout1> {\n+    scf.yield %9 : tensor<1024xi32, #layout1>\n+  } else {\n+    %7 = triton_gpu.convert_layout %3 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n+    scf.yield %7 : tensor<1024xi32, #layout1>\n+  }\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  tt.store %5, %8 : tensor<1024xi32, #layout1>\n+  return\n+}\n+\n+// CHECK-LABEL: if_else_both_convert\n+func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n+  %2 = arith.muli %1, %c32_i32 : tensor<1024xi32, #layout0>\n+  %3 = arith.addi %2, %c32_i32 : tensor<1024xi32, #layout0>\n+  %4 = arith.cmpi sgt, %0, %arg0 : i32\n+  %5 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>, #layout1>\n+  %8 = scf.if %4 -> tensor<1024xi32, #layout1> {\n+    %6 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n+    scf.yield %6 : tensor<1024xi32, #layout1>\n+  } else {\n+    %7 = triton_gpu.convert_layout %3 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n+    scf.yield %7 : tensor<1024xi32, #layout1>\n+  }\n+  // CHECK: triton_gpu.convert_layout\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  tt.store %5, %8 : tensor<1024xi32, #layout1>\n+  return\n+}\n \n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>"}]