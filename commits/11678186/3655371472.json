[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -89,7 +89,9 @@ jobs:\n         if: ${{matrix.runner[0] == 'self-hosted' && matrix.runner[1] == 'V100'}}\n         run: |\n           cd python/tests\n+          pytest -v -k \"not test_where_broadcast and not test_dot\" test_core.py\n           pytest test_gemm.py::test_gemm_for_mmav1\n+          pytest test_backend.py\n \n       - name: Run CXX unittests\n         run: |"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 29, "deletions": 0, "changes": 29, "file_content_changes": "@@ -43,6 +43,9 @@ def matmul_no_scf_kernel(\n     for trans_b in [False, True]\n ])\n def test_gemm_no_scf(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n+    if not valid_on_Volta(NUM_WARPS, TRANS_A, TRANS_B):\n+        pytest.skip(\"Not valid on Volta\")\n+\n     SIZE_M, SIZE_N, SIZE_K = SHAPE\n     if (TRANS_A):\n         a = torch.randn((SIZE_K, SIZE_M), device='cuda', dtype=torch.float16).T\n@@ -81,6 +84,9 @@ def test_gemm_no_scf(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n     for trans_b in [False, True]\n ])\n def test_gemm_no_scf_int8(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n+    if not valid_on_Volta(NUM_WARPS, TRANS_A, TRANS_B, is_int8=True):\n+        pytest.skip(\"Not valid on Volta\")\n+\n     SIZE_M, SIZE_N, SIZE_K = SHAPE\n \n     if (TRANS_A):\n@@ -195,6 +201,9 @@ def get_proper_err(a, b, golden):\n     [128, 64, 128, 4, 128, 64, 32, False, True],\n ])\n def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n+    if not valid_on_Volta(NUM_WARPS, TRANS_A, TRANS_B):\n+        pytest.skip(\"Not valid on Volta\")\n+\n     if (TRANS_A):\n         a = torch.randn((SIZE_K, SIZE_M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -270,6 +279,9 @@ def matmul_kernel(\n         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n         tl.store(c_ptrs, accumulator, c_mask)\n \n+    if not valid_on_Volta(num_warps, trans_a=False, trans_b=False, is_tf32=allow_tf32):\n+        pytest.skip(\"Not valid on Volta\")\n+\n     # Configure the pytorch counterpart\n     torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n \n@@ -294,6 +306,19 @@ def matmul_kernel(\n         torch.testing.assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err))\n \n \n+def valid_on_Volta(num_warps, trans_a, trans_b, is_int8=False, is_tf32=False):\n+    '''\n+    Tell whether the test case is valid on Volta GPU.\n+    Some features are WIP, so the corresponding support are missing.\n+    '''\n+    if is_int8 or is_tf32:\n+        return False\n+\n+    capability = torch.cuda.get_device_capability()\n+    is_on_Volta = capability[0] < 8\n+    return is_on_Volta and num_warps == 1 and not (trans_a or trans_b)\n+\n+\n # NOTE this is useful only on Volta GPU.\n @pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,TRANS_A,TRANS_B', [\n     # Non-forloop\n@@ -308,4 +333,8 @@ def matmul_kernel(\n     [64, 64, 128, 1, 64, 64, 32, False, False],\n ])\n def test_gemm_for_mmav1(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] >= 8:\n+        pytest.skip(\"Only test test_gemm_for_mmav1 on Volta.\")\n+\n     test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B)"}]