[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUDialect.td", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -23,14 +23,16 @@ def TritonGPU_Dialect : Dialect {\n   let extraClassDeclaration = [{\n     static std::string getNumWarpsAttrName() { return \"triton_gpu.num-warps\"; }\n     static int getNumWarps(ModuleOp mod) {\n-      if(!mod->hasAttr(\"triton_gpu.num-warps\"))\n+      Attribute numWarps = mod->getDiscardableAttr(\"triton_gpu.num-warps\");\n+      if(!numWarps)\n         llvm::report_fatal_error(\n             \"TritonGPU module should contain a triton_gpu.num-warps attribute\");\n-      return mod->getAttr(\"triton_gpu.num-warps\").cast<IntegerAttr>().getInt();\n+      return numWarps.cast<IntegerAttr>().getInt();\n     }\n   }];\n \n   let useDefaultAttributePrinterParser = 1;\n+  let usePropertiesForAttributes = 1;\n }\n \n #endif"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -111,15 +111,15 @@ AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n       }\n     }\n   } else if (Operation *op = value.getDefiningOp()) {\n-    if (Attribute attr = op->getAttr(\"tt.divisibility\")) {\n+    if (Attribute attr = op->getDiscardableAttr(\"tt.divisibility\")) {\n       auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n       knownDivisibility = DimVectorT(vals.begin(), vals.end());\n     }\n-    if (Attribute attr = op->getAttr(\"tt.contiguity\")) {\n+    if (Attribute attr = op->getDiscardableAttr(\"tt.contiguity\")) {\n       auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n       knownContiguity = DimVectorT(vals.begin(), vals.end());\n     }\n-    if (Attribute attr = op->getAttr(\"tt.constancy\")) {\n+    if (Attribute attr = op->getDiscardableAttr(\"tt.constancy\")) {\n       auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n       knownConstancy = DimVectorT(vals.begin(), vals.end());\n     }\n@@ -888,15 +888,15 @@ void AxisInfoAnalysis::visitOperation(\n   auto newContiguity = curr.getContiguity();\n   auto newDivisibility = curr.getDivisibility();\n   auto newConstancy = curr.getConstancy();\n-  if (Attribute attr = op->getAttr(\"tt.contiguity\")) {\n+  if (Attribute attr = op->getDiscardableAttr(\"tt.contiguity\")) {\n     auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n     newContiguity = AxisInfo::DimVectorT(vals.begin(), vals.end());\n   }\n-  if (Attribute attr = op->getAttr(\"tt.divisibility\")) {\n+  if (Attribute attr = op->getDiscardableAttr(\"tt.divisibility\")) {\n     auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n     newDivisibility = AxisInfo::DimVectorT(vals.begin(), vals.end());\n   }\n-  if (Attribute attr = op->getAttr(\"tt.constancy\")) {\n+  if (Attribute attr = op->getDiscardableAttr(\"tt.constancy\")) {\n     auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n     newConstancy = AxisInfo::DimVectorT(vals.begin(), vals.end());\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -118,9 +118,8 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<triton::FuncOp> {\n     // Create an LLVM function, use external linkage by default until MLIR\n     // functions have linkage.\n     LLVM::Linkage linkage = LLVM::Linkage::External;\n-    if (funcOp->hasAttr(\"llvm.linkage\")) {\n-      auto attr =\n-          funcOp->getAttr(\"llvm.linkage\").dyn_cast<mlir::LLVM::LinkageAttr>();\n+    if (auto linkageAttr = funcOp->getDiscardableAttr(\"llvm.linkage\")) {\n+      auto attr = linkageAttr.dyn_cast<mlir::LLVM::LinkageAttr>();\n       if (!attr) {\n         funcOp->emitError()\n             << \"Contains llvm.linkage attribute not of type LLVM::LinkageAttr\";"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 15, "deletions": 5, "changes": 20, "file_content_changes": "@@ -25,9 +25,19 @@ namespace ttg = triton::gpu;\n \n // pass named attrs (e.g., tt.contiguity) from Triton to Triton\n static void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n-  for (const NamedAttribute attr : dictAttrs.getValue())\n-    if (!op->hasAttr(attr.getName()))\n-      op->setAttr(attr.getName(), attr.getValue());\n+  NamedAttrList attrs = op->getDiscardableAttrs();\n+  // Collect the attributes to propagate: the ones in dictAttrs and not yet on\n+  // the operation.\n+  SmallVector<NamedAttribute> toPropagate;\n+  for (const NamedAttribute attr : dictAttrs.getValue()) {\n+    if (!attrs.get(attr.getName()))\n+      toPropagate.push_back(attr);\n+  }\n+  // If we found any, let's set them here as a single step.\n+  if (toPropagate.size()) {\n+    attrs.append(toPropagate);\n+    op->setDiscardableAttrs(attrs);\n+  }\n }\n \n #define int_attr(num) builder.getI64IntegerAttr(num)\n@@ -428,7 +438,7 @@ void LoopPipeliner::emitPrologue() {\n               lookupOrDefault(loadOp.getOther(), stage),\n               loadOp.getBoundaryCheckAttr(), loadOp.getPaddingAttr(),\n               loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n-          addNamedAttrs(newOp, op->getAttrDictionary());\n+          addNamedAttrs(newOp, op->getDiscardableAttrDictionary());\n         } else {\n           newOp = builder.clone(*op);\n         }\n@@ -652,7 +662,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n             nextMapping.lookupOrDefault(loadOp.getOther()),\n             loadOp.getBoundaryCheckAttr(), loadOp.getPaddingAttr(),\n             loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n-        addNamedAttrs(nextOp, op->getAttrDictionary());\n+        addNamedAttrs(nextOp, op->getDiscardableAttrDictionary());\n         nextMapping.map(loadOp.getResult(), nextOp->getResult(0));\n       } else {\n         nextOp = builder.clone(*op, nextMapping);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 9, "deletions": 49, "changes": 58, "file_content_changes": "@@ -349,27 +349,23 @@ class RematerializeForward : public mlir::RewritePattern {\n     SetVector<Operation *> cvtSlices;\n     auto filter = [&](Operation *op) {\n       return op->getBlock() == cvt->getBlock() &&\n+             !isa<triton::gpu::ConvertLayoutOp, scf::YieldOp>(op) &&\n              !(isa<triton::ReduceOp>(op) &&\n-               !op->getResult(0).getType().isa<RankedTensorType>()) &&\n-             !isa<triton::gpu::ConvertLayoutOp>(op) && !isa<scf::YieldOp>(op);\n+               !op->getResult(0).getType().isa<RankedTensorType>());\n     };\n     mlir::getForwardSlice(cvt.getResult(), &cvtSlices, filter);\n-    if (cvtSlices.empty()) {\n+    if (cvtSlices.empty())\n       return failure();\n-    }\n \n-    llvm::MapVector<Value, Attribute> toConvert;\n     for (Operation *op : cvtSlices) {\n       // don't rematerialize anything expensive\n-      if (expensiveToRemat(op, dstEncoding)) {\n+      if (expensiveToRemat(op, dstEncoding))\n         return failure();\n-      }\n       // don't rematerialize non-element-wise\n       if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n           !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n-          !isa<triton::StoreOp>(op) && !isa<triton::ReduceOp>(op)) {\n+          !isa<triton::StoreOp, triton::ReduceOp>(op))\n         return failure();\n-      }\n       // don't rematerialize if it adds an extra conversion that can't\n       // be removed\n       for (Value arg : op->getOperands()) {\n@@ -380,9 +376,8 @@ class RematerializeForward : public mlir::RewritePattern {\n         int numAddedConvs = simulateBackwardRematerialization(\n             argOp, processed, layout, toConvert, srcEncoding);\n         if (argOp && !isa<triton::gpu::ConvertLayoutOp>(argOp) &&\n-            cvtSlices.count(argOp) == 0 && numAddedConvs > 0) {\n+            cvtSlices.count(argOp) == 0 && numAddedConvs > 0)\n           return failure();\n-        }\n       }\n     }\n \n@@ -425,7 +420,6 @@ class RematerializeBackward : public mlir::RewritePattern {\n     SetVector<Operation *> processed;\n     SetVector<Attribute> layout;\n     llvm::MapVector<Value, Attribute> toConvert;\n-    std::vector<std::pair<Operation *, Attribute>> queue;\n     if (simulateBackwardRematerialization(cvt, processed, layout, toConvert,\n                                           targetType.getEncoding()) > 0)\n       return mlir::failure();\n@@ -507,49 +501,15 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n     auto forOp = cast<scf::ForOp>(op);\n     auto iterArgs = forOp.getRegionIterArgs();\n     for (const auto &iterArg : llvm::enumerate(iterArgs)) {\n-      // if (iterArg.index() != 1)\n-      //   continue;\n       // skip non-tensor types\n       if (!iterArg.value().getType().isa<RankedTensorType>())\n         continue;\n-      // we only move `iterArg` out of the loop if\n-      //   - there is only a single conversion use\n-      //   - moving this conversion out of the loop will not generate\n-      //     any extra non-removable conversion\n-      auto users = iterArg.value().getUsers();\n-      // check first condition\n-      SetVector<Type> cvtTargetTypes;\n-      for (auto user : users) {\n-        if (isa<triton::gpu::ConvertLayoutOp>(user)) {\n-          auto newType =\n-              user->getResults()[0].getType().cast<RankedTensorType>();\n-          auto oldType = user->getOperand(0).getType().cast<RankedTensorType>();\n-          if (oldType.getEncoding().isa<triton::gpu::SharedEncodingAttr>() &&\n-              newType.getEncoding()\n-                  .isa<triton::gpu::DotOperandEncodingAttr>()) {\n-            continue;\n-          }\n-          if (newType.getEncoding().isa<triton::gpu::SharedEncodingAttr>()) {\n-            if (newType.getEncoding()\n-                    .cast<triton::gpu::SharedEncodingAttr>()\n-                    .getVec() == 1)\n-              continue;\n-          }\n-          cvtTargetTypes.insert(newType);\n-        }\n-      }\n-      if (cvtTargetTypes.size() != 1)\n+      SmallVector<Operation *> cvts;\n+      if (canMoveOutOfLoop(iterArg.value(), cvts).failed())\n         continue;\n-      // TODO: check second condition\n-      for (auto user : users) {\n-        if (isa<triton::gpu::ConvertLayoutOp>(user))\n-          continue;\n-      }\n       // check\n-      for (auto op : iterArg.value().getUsers()) {\n+      for (auto *op : cvts) {\n         auto cvt = dyn_cast<triton::gpu::ConvertLayoutOp>(op);\n-        if (!cvt)\n-          continue;\n         auto targetType = op->getResultTypes()[0].cast<RankedTensorType>();\n         auto newFor = rematerializeForLoop(rewriter, forOp, iterArg.index(),\n                                            targetType, cvt);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 60, "deletions": 0, "changes": 60, "file_content_changes": "@@ -269,4 +269,64 @@ void rematerializeConversionChain(\n   }\n }\n \n+LogicalResult canMoveOutOfLoop(BlockArgument arg,\n+                               SmallVector<Operation *> &cvts) {\n+  auto parentOp = arg.getOwner()->getParentOp();\n+  // Don't move if arg is defined in a while loop\n+  if (isa<scf::WhileOp>(parentOp))\n+    return failure();\n+  // Skip if arg is not defined in scf.for\n+  if (!isa<scf::ForOp>(parentOp))\n+    return success();\n+  auto forOp = cast<scf::ForOp>(parentOp);\n+  // We only move `iterArg` out of the loop if\n+  // 1. There is no conversion\n+  // 2. There is only a single conversion\n+  // 3. Moving this conversion out of the loop will not generate any extra\n+  // non-removable conversion\n+  DenseSet<Type> cvtTypes;\n+  SetVector<Operation *> others;\n+  auto oldType = arg.getType().cast<RankedTensorType>();\n+  for (auto user : arg.getUsers()) {\n+    if (isa<triton::gpu::ConvertLayoutOp>(user)) {\n+      // Don't move if the conversion target is a dot operand or shared memory\n+      auto newType = user->getResults()[0].getType().cast<RankedTensorType>();\n+      if (oldType.getEncoding().isa<triton::gpu::SharedEncodingAttr>() &&\n+          newType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>()) {\n+        continue;\n+      }\n+      if (newType.getEncoding().isa<triton::gpu::SharedEncodingAttr>()) {\n+        if (newType.getEncoding()\n+                .cast<triton::gpu::SharedEncodingAttr>()\n+                .getVec() == 1)\n+          continue;\n+      }\n+      cvts.emplace_back(user);\n+      cvtTypes.insert(newType);\n+    } else\n+      others.insert(user);\n+  }\n+  // First condition\n+  if (cvts.empty())\n+    return success();\n+  if (cvtTypes.size() == 1) {\n+    // Second condition\n+    if (others.empty())\n+      return success();\n+    // Third condition: not complete\n+    // If the other or the cvt is in the different block, we cannot push the\n+    // conversion forward or backward\n+    for (auto *cvt : cvts) {\n+      if (cvt->getBlock() != forOp.getBody())\n+        return failure();\n+    }\n+    for (auto *other : others) {\n+      if (other->getBlock() != forOp.getBody())\n+        return failure();\n+    }\n+    return success();\n+  }\n+  return failure();\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -16,6 +16,8 @@ bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n \n bool expensiveToRemat(Operation *op, Attribute &targetEncoding);\n \n+// skipInit is True when we only consider the operands of the initOp but\n+// not the initOp itself.\n int simulateBackwardRematerialization(\n     Operation *initOp, SetVector<Operation *> &processed,\n     SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n@@ -28,6 +30,10 @@ void rematerializeConversionChain(\n     const llvm::MapVector<Value, Attribute> &toConvert,\n     mlir::PatternRewriter &rewriter, SetVector<Operation *> &processed,\n     IRMapping &mapping);\n+\n+LogicalResult canMoveOutOfLoop(BlockArgument arg,\n+                               SmallVector<Operation *> &cvts);\n+\n } // namespace mlir\n \n #endif // TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 8, "deletions": 11, "changes": 19, "file_content_changes": "@@ -154,24 +154,21 @@ static std::map<std::string, std::string> getExternLibs(mlir::ModuleOp module) {\n       funcs.push_back(func);\n   });\n \n-  for (auto &func : funcs) {\n-    if (func.getOperation()->hasAttr(\"libname\")) {\n-      auto name =\n-          func.getOperation()->getAttr(\"libname\").dyn_cast<StringAttr>();\n-      auto path =\n-          func.getOperation()->getAttr(\"libpath\").dyn_cast<StringAttr>();\n+  for (LLVM::LLVMFuncOp func : funcs) {\n+    if (auto libnameAttr = func->getDiscardableAttr(\"libname\")) {\n+      auto name = libnameAttr.dyn_cast<StringAttr>();\n+      auto path = func.getOperation()\n+                      ->getDiscardableAttr(\"libpath\")\n+                      .dyn_cast<StringAttr>();\n       if (name) {\n         std::string libName = name.str();\n         externLibs[libName] = path.str();\n       }\n     }\n   }\n \n-  if (module.getOperation()->hasAttr(\"triton_gpu.externs\")) {\n-    auto dict = module.getOperation()\n-                    ->getAttr(\"triton_gpu.externs\")\n-                    .dyn_cast<DictionaryAttr>();\n-    for (auto &attr : dict) {\n+  if (auto externsAttr = module->getDiscardableAttr(\"triton_gpu.externs\")) {\n+    for (auto &attr : externsAttr.cast<DictionaryAttr>()) {\n       externLibs[attr.getName().strref().trim().str()] =\n           attr.getValue().dyn_cast<StringAttr>().strref().trim().str();\n     }"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 52, "deletions": 0, "changes": 52, "file_content_changes": "@@ -267,6 +267,58 @@ tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32,\n   tt.return\n }\n \n+// CHECK-LABEL: loop_if\n+tt.func @loop_if(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n+  %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n+  %cst_0 = arith.constant dense<64> : tensor<64x64xi32, #blocked1>\n+  %c1 = arith.constant 1 : index\n+  %c32 = arith.constant 32 : index\n+  %c0 = arith.constant 0 : index\n+  %i0 = arith.constant 0 : i32\n+  %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked1>\n+  %00 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #slice1dim1>\n+  %01 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #slice2dim0>\n+  %1 = tt.expand_dims %00 {axis = 1 : i32} : (tensor<64xi32, #slice1dim1>) -> tensor<64x1xi32, #blocked1>\n+  %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n+  %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n+  %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n+  %6 = tt.expand_dims %01 {axis = 0 : i32} : (tensor<64xi32, #slice2dim0>) -> tensor<1x64xi32, #blocked2>\n+  %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n+  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n+  %11:2 = scf.for %arg5 = %c0 to %c32 step %c1 iter_args(%arg6 = %cst_1, %arg7 = %10) -> (tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>) {\n+    %33 = \"triton_gpu.cmpi\"(%i0, %i0) {predicate = 4 : i64} : (i32, i32) -> i1\n+    %34 = scf.if %33 -> (tensor<64x64xf32, #blocked1>) {\n+      %23 = triton_gpu.convert_layout %arg7 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n+      %24 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n+      %25 = triton_gpu.convert_layout %cst_1 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n+      %26 = tt.load %23, %24, %25 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked3>\n+      %27 = triton_gpu.convert_layout %26 : (tensor<64x64xf32, #blocked3>) -> tensor<64x64xf32, #blocked1>\n+      scf.yield %27 : tensor<64x64xf32, #blocked1>\n+    } else {\n+      scf.yield %arg6 : tensor<64x64xf32, #blocked1>\n+    }\n+    %28 = arith.addf %arg6, %34 : tensor<64x64xf32, #blocked1>\n+    %29 = tt.addptr %arg7, %cst_0 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n+    scf.yield %28, %29 : tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  }\n+  %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %13 = tt.addptr %12, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n+  %14 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n+  %15 = arith.muli %6, %14 : tensor<1x64xi32, #blocked2>\n+  %16 = tt.broadcast %13 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %17 = tt.broadcast %15 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %18 = triton_gpu.convert_layout %17 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n+  %19 = tt.addptr %16, %18 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n+  %20 = triton_gpu.convert_layout %19 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %21 = triton_gpu.convert_layout %11#0 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked1>\n+  %22 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked1>\n+  tt.store %20, %21, %22 : tensor<64x64xf32, #blocked1>\n+  tt.return\n+}\n+\n // CHECK-LABEL: vecadd\n tt.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout"}]