[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -249,7 +249,8 @@ def backward(ctx, do):\n             BLOCK_M=ctx.BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n         )\n \n-        num_warps = 4 if ctx.BLOCK_DMODEL <= 64 else 8\n+        # NOTE: kernel currently buggy for other values of `num_warps`\n+        num_warps = 8\n         _bwd_kernel[(ctx.grid[1],)](\n             q, k, v, ctx.sm_scale,\n             o, do_scaled,"}]