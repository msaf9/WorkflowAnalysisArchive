[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 5, "deletions": 6, "changes": 11, "file_content_changes": "@@ -1963,6 +1963,11 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n   smemBase = bitcast(elemPtrTy, smemBase);\n   unsigned numWordsEachRep = product<unsigned>(wordsInEachRep);\n   SmallVector<Value> wordVecs(numWordsEachRep);\n+  // TODO: We should get less barriers if it is handled by membar pass\n+  //       instead of the backend, since the later can only handle it in\n+  //       the most conservative way. However just keep for now and revisit\n+  //       in the future in case necessary.\n+  barrier;\n   for (unsigned i = 0; i < numElems; ++i) {\n     if (i % srcAccumSizeInThreads == 0) {\n       // start of a replication\n@@ -2016,7 +2021,6 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n       }\n     }\n   }\n-  // TODO: double confirm if the Barrier is necessary here\n   barrier;\n   rewriter.replaceOp(op, smemBase);\n   return success();\n@@ -3057,11 +3061,6 @@ struct MMA16816ConversionHelper {\n         for (unsigned n = 0; n < numRepN; ++n)\n           callMma(2 * m, n, 2 * k);\n \n-    // NOTE, the barrier here is a temporary trick making the gemm with a\n-    // k-forloop pass the precision test, or it will fail.\n-    // TODO[Superjomn]: Fix with a more general and performance-friendly way.\n-    barrier;\n-\n     // replace with new packed result\n     Type structTy = LLVM::LLVMStructType::getLiteral(\n         ctx, SmallVector<Type>(fc.size(), type::f32Ty(ctx)));"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 29, "deletions": 2, "changes": 31, "file_content_changes": "@@ -83,6 +83,23 @@ def matmul_kernel(\n # TODO: DotConversion in TritonGPUToLLVM cannot support non-splat C for the moment\n \n \n+def get_variant_golden(a, b):\n+    SIZE_M = a.shape[0]\n+    SIZE_K = a.shape[1]\n+    SIZE_N = b.shape[1]\n+    assert a.shape[1] == b.shape[0]\n+    zero_M_K = torch.zeros((SIZE_M, SIZE_K)).cuda()\n+    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K)).cuda()\n+    zero_K_N = torch.zeros((SIZE_K, SIZE_N)).cuda()\n+    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N)).cuda()\n+    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n+    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n+    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n+    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n+    c_padded = torch.matmul(a_padded, b_padded)\n+    return c_padded[:SIZE_M, :SIZE_N]\n+\n+\n @pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K', [\n     # Non-forloop\n     [64, 32, 64, 4, 64, 32, 64],\n@@ -94,8 +111,8 @@ def matmul_kernel(\n     [32, 64, 128, 4, 32, 64, 32],\n     [32, 128, 256, 4, 32, 128, 64],\n     [64, 128, 64, 4, 64, 128, 32],\n-    [128, 128, 64, 4, 128, 128, 32],\n     [64, 64, 128, 4, 64, 64, 32],\n+    [128, 128, 64, 4, 128, 128, 32],\n     [128, 128, 128, 4, 128, 128, 32],\n     [128, 128, 256, 4, 128, 128, 64],\n     [128, 256, 128, 4, 128, 256, 32],\n@@ -115,5 +132,15 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n                         BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n                         num_warps=NUM_WARPS)\n     golden = torch.matmul(a, b)\n+\n+    # It's not easy to get a proper error threshold in different size\n+    # Here the gemm calculation is padded to a different size in order to get\n+    # a variant version of the golden result. And the error between golden and\n+    # golden_variant provide reference on selecting the proper rtol / atol.\n+    golden_variant = get_variant_golden(a, b)\n+    golden_diff = golden - golden_variant\n+    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n+    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n+\n     torch.set_printoptions(profile=\"full\")\n-    assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n+    assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)"}]