[{"filename": "lib/Conversion/TritonGPUToLLVM/PtxAsmFormat.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -124,7 +124,7 @@ std::string PTXInstrExecution::dump() const {\n     if (!pred->repr)\n       os << \"@\" << pred->dump() << \" \";\n     else\n-      os << pred->repr(pred->idx);\n+      os << pred->repr(pred->idx) << \" \";\n \n   std::string instrRepr = strJoin(instr->instrParts, \".\");\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 11, "deletions": 10, "changes": 21, "file_content_changes": "@@ -375,25 +375,26 @@ class ConvertTritonGPUOpToLLVMPattern\n   //       be eliminated in the consequent MLIR/LLVM optimization\n   SmallVector<SmallVector<Value>>\n   emitIndicesForBlockedLayout(Location loc, ConversionPatternRewriter &b,\n-                              const BlockedEncodingAttr &blocked_layout,\n+                              const BlockedEncodingAttr &blockedLayout,\n                               ArrayRef<int64_t> shape) const {\n     auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n     auto cast = b.create<UnrealizedConversionCastOp>(\n         loc, TypeRange{llvmIndexTy},\n         ValueRange{b.create<::mlir::gpu::ThreadIdOp>(\n             loc, b.getIndexType(), ::mlir::gpu::Dimension::x)});\n     Value threadId = cast.getResult(0);\n-    Value warpSize = createIndexAttrConstant(b, loc, llvmIndexTy, 32);\n+    Value warpSize = createIndexAttrConstant(\n+        b, loc, llvmIndexTy, product(blockedLayout.getThreadsPerWarp()));\n     Value laneId = b.create<LLVM::URemOp>(loc, threadId, warpSize);\n     Value warpId = b.create<LLVM::UDivOp>(loc, threadId, warpSize);\n-    auto sizePerThread = blocked_layout.getSizePerThread();\n-    auto threadsPerWarp = blocked_layout.getThreadsPerWarp();\n-    auto warpsPerCTA = blocked_layout.getWarpsPerCTA();\n-    auto order = blocked_layout.getOrder();\n+    auto sizePerThread = blockedLayout.getSizePerThread();\n+    auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n+    auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n+    auto order = blockedLayout.getOrder();\n     unsigned rank = shape.size();\n     SmallVector<Value, 4> threadIds(rank);\n \n-    // step 1, delinearize threadId to get the base index\n+    // step 1, delinearise threadId to get the base index\n     SmallVector<Value> multiDimWarpId =\n         delinearize(b, loc, warpId, warpsPerCTA, order);\n     SmallVector<Value> multiDimThreadId =\n@@ -619,15 +620,15 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n     // should have the largest contiguous.\n     auto order = layout.getOrder();\n     unsigned align = getAlignment(ptr, layout);\n-\n     auto getTensorShape = [](Value val) -> ArrayRef<int64_t> {\n       auto ty = val.getType().cast<RankedTensorType>();\n       auto shape = ty.getShape();\n       return shape;\n     };\n \n-    // unsigned contigPerThread = layout.getSizePerThread()[order[0]];\n-    unsigned contigPerThread = getElemsPerThread(layout, getTensorShape(ptr));\n+    unsigned elemsPerThread = getElemsPerThread(layout, getTensorShape(ptr));\n+    unsigned contigPerThread =\n+        std::min<unsigned>(layout.getSizePerThread()[order[0]], elemsPerThread);\n \n     unsigned vec = std::min(align, contigPerThread);\n "}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -31,6 +31,7 @@ translateTritonGPUToPTX(mlir::ModuleOp module, uint64_t device) {\n   int version;\n   std::string ptxasPath;\n   getCuCCAndVersionFromDevice(device, &cc, &version, &ptxasPath);\n+  llvm::outs() << \"ptxaspath: \" << ptxasPath << \"\\n\";\n \n   llvm::LLVMContext ctx;\n   auto llModule = mlir::triton::translateTritonGPUToLLVMIR(&ctx, module);"}, {"filename": "python/tests/test_vecadd_no_scf.py", "status": "modified", "additions": 14, "deletions": 10, "changes": 24, "file_content_changes": "@@ -1,18 +1,19 @@\n import torch\n-from torch.testing import assert_allclose\n+from torch.testing import assert_close\n \n import triton\n import triton.language as tl\n import triton.runtime as runtime\n \n \n-def vecadd_no_scf_tester(num_warps, block_size):\n+def vecadd_no_scf_tester(num_warps, block_size, tensor_shape):\n     @triton.jit\n     def kernel(x_ptr,\n                y_ptr,\n                z_ptr,\n                BLOCK_SIZE_N: tl.constexpr):\n         pid = tl.program_id(axis=0)\n+\n         offset = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n         x_ptrs = x_ptr + offset\n         y_ptrs = y_ptr + offset\n@@ -29,12 +30,11 @@ def kernel(x_ptr,\n                                   num_warps=num_warps,\n                                   num_stages=3)\n \n-    x = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n-    y = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n-    z = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n+    x = torch.randn(tensor_shape, device='cuda', dtype=torch.float32)\n+    y = torch.randn(tensor_shape, device='cuda', dtype=torch.float32)\n+    z = torch.empty(tensor_shape, device=x.device, dtype=x.dtype)\n \n-    assert x.shape.numel() % block_size == 0, \"Only test load without mask here\"\n-    grid = lambda EA: (x.shape.numel() // block_size,)\n+    grid = lambda EA: (triton.cdiv(x.shape.numel(), block_size),)\n \n     runtime.launch_kernel(kernel=binary,\n                           grid=grid,\n@@ -44,12 +44,16 @@ def kernel(x_ptr,\n                           z_ptr=z,\n                           BLOCK_SIZE_N=tl.constexpr(block_size))\n     golden_z = x + y\n-    assert_allclose(z, golden_z, rtol=1e-7, atol=1e-7)\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n \n \n def test_vecadd_no_scf():\n-    vecadd_no_scf_tester(num_warps=2, block_size=256)\n-    vecadd_no_scf_tester(num_warps=1, block_size=256)\n+    vecadd_no_scf_tester(num_warps=2, block_size=256, tensor_shape=(256,))\n+    vecadd_no_scf_tester(num_warps=1, block_size=256, tensor_shape=(256,))\n+\n+    # masked load/store\n+    vecadd_no_scf_tester(num_warps=1, block_size=256, tensor_shape=(301,))\n+    vecadd_no_scf_tester(num_warps=2, block_size=256, tensor_shape=(513,))\n \n \n if __name__ == '__main__':"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 92, "deletions": 4, "changes": 96, "file_content_changes": "@@ -59,7 +59,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n // -----\n \n // TODO: Pending on the support of isSplat constant\n-#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: masked_load_const_other\n   func @masked_load_const_other(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>) {\n@@ -86,16 +86,104 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n     %8 = tt.getelementptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n \n-    // CHECK: ld.global.v4.b32\n+    // Load 4 elements from vector0\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    // Load 4 elements from vector1\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+\n     %9 = tt.load %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n-    // CHECK: ld.global.v4.b32\n     %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n     %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n     %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n     %13 = tt.getelementptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n \n     // Store 4 elements to global\n-    // CHECK: st.global.b32.v4\n+    // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n+    // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n+    // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n+    // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n+    tt.store %13, %11 : tensor<256xf32, #blocked0>\n+    return\n+  }\n+}\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n+  // CHECK-LABEL: kernel__Pfp32_Pfp32_Pfp32_i32__3c256_vec4\n+  func @kernel__Pfp32_Pfp32_Pfp32_i32__3c256_vec4(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+    %c256_i32 = arith.constant 256 : i32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %c256_i32 : i32\n+    %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n+    %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n+    %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.getelementptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.getelementptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+\n+    // Load 4 elements from A with single one vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    // Load 4 elements from B with single one vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    %9 = tt.load %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n+    %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.getelementptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+\n+    // Store 4 elements to global with single one vectorized store instruction\n+    // CHECK: @$5 st.global.b32.v4 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n+    tt.store %13, %11 : tensor<256xf32, #blocked0>\n+    return\n+  }\n+}\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: kernel__Pfp32_Pfp32_Pfp32_i32__3c256_vec8\n+  func @kernel__Pfp32_Pfp32_Pfp32_i32__3c256_vec8(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+    %c256_i32 = arith.constant 256 : i32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %c256_i32 : i32\n+    %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n+    %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n+    %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.getelementptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.getelementptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+\n+    // Load 8 elements from A with two vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    // Load 8 elements from B with two vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    %9 = tt.load %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n+    %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.getelementptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+\n+    // Store 8 elements to global with two vectorized store instruction\n+    // CHECK: @$5 st.global.b32.v4 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n+    // CHECK: @$5 st.global.b32.v4 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n     tt.store %13, %11 : tensor<256xf32, #blocked0>\n     return\n   }"}]