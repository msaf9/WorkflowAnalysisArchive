[{"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -92,17 +92,19 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n     if DTYPE == torch.int8:\n         a = (torch.rand((K, M) if AT else (M, K), device=\"cuda\") * 255 - 128).to(torch.int8)\n         b = (torch.rand((N, K) if BT else (K, N), device=\"cuda\") * 255 - 128).to(torch.int8)\n-        out_dtype = torch.int32\n+        gemm_out_dtype = torch.int32\n     else:\n         a = .1 * torch.randn((K, M) if AT else (M, K), device=\"cuda\", dtype=DTYPE)\n         b = .1 * torch.randn((N, K) if BT else (K, N), device=\"cuda\", dtype=DTYPE)\n-        out_dtype = DTYPE\n+        gemm_out_dtype = DTYPE\n     a = a.t() if AT else a\n     b = b.t() if BT else b\n     # run test\n-    th_c = torch.matmul(a.to(torch.float32), b.to(torch.float32)).to(out_dtype)\n+    th_c = torch.matmul(a.to(torch.float32), b.to(torch.float32)).to(gemm_out_dtype)\n+    # use None for auto inference of dot output dtype\n+    dot_out_dtype = None\n     try:\n-        tt_c = triton.ops.matmul(a, b, gemm_out_dtype=out_dtype)\n+        tt_c = triton.ops.matmul(a, b, dot_out_dtype, gemm_out_dtype)\n         torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n     except triton.OutOfResources as e:\n         pytest.skip(str(e))"}]