[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 62, "deletions": 35, "changes": 97, "file_content_changes": "@@ -2239,16 +2239,16 @@ struct AllocTensorOpConversion\n     smemBase = bitcast(smemBase, elemPtrTy);\n     auto order = resultTy.getEncoding().cast<SharedEncodingAttr>().getOrder();\n     // workaround for 3D tensors\n-    // TODO: We need to modify the pipeline pass to give a proper shared encoding to 3D tensors\n+    // TODO: We need to modify the pipeline pass to give a proper shared\n+    // encoding to 3D tensors\n     SmallVector<unsigned> newOrder;\n-    if (resultTy.getShape().size() == 3) \n+    if (resultTy.getShape().size() == 3)\n       newOrder = {1 + order[0], 1 + order[1], 0};\n     else\n       newOrder = SmallVector<unsigned>(order.begin(), order.end());\n \n-    \n-    auto smemObj =\n-        SharedMemoryObject(smemBase, resultTy.getShape(), newOrder, loc, rewriter);\n+    auto smemObj = SharedMemoryObject(smemBase, resultTy.getShape(), newOrder,\n+                                      loc, rewriter);\n     auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n     rewriter.replaceOp(op, retVal);\n     return success();\n@@ -2882,6 +2882,8 @@ struct ConvertLayoutOpConversion\n       SmallVector<Value> multiDimWarpId(2);\n       multiDimWarpId[0] = urem(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n       multiDimWarpId[1] = udiv(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n+      multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n+      multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 8));\n       Value four = idx_val(4);\n       Value mmaGrpId = udiv(laneId, four);\n       Value mmaGrpIdP8 = add(mmaGrpId, idx_val(8));\n@@ -3661,7 +3663,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n \n     // Here we assume the DotOp's operands always comes from shared memory.\n     auto AShape = A.getType().cast<RankedTensorType>().getShape();\n-    size_t reduceAxis = 1;\n+    size_t reduceAxis = op.transA() ? 0 : 1;\n     unsigned K = AShape[reduceAxis];\n     bool isOuter = K == 1;\n \n@@ -4124,8 +4126,9 @@ struct DotOpMmaV2ConversionHelper {\n struct MMA16816ConversionHelper {\n   MmaEncodingAttr mmaLayout;\n   ArrayRef<unsigned int> wpt;\n+  SmallVector<unsigned int> properWpt;\n \n-  Value thread, lane, warp, warpMN, warpN, warpM;\n+  Value thread, lane, warp;\n \n   DotOpMmaV2ConversionHelper helper;\n   ConversionPatternRewriter &rewriter;\n@@ -4135,23 +4138,34 @@ struct MMA16816ConversionHelper {\n \n   using ValueTable = std::map<std::pair<unsigned, unsigned>, Value>;\n \n-  MMA16816ConversionHelper(MmaEncodingAttr mmaLayout, Value thread,\n-                           ConversionPatternRewriter &rewriter,\n+  // dotOperand: type of either one operand of dotOp.\n+  MMA16816ConversionHelper(Type dotOperand, MmaEncodingAttr mmaLayout,\n+                           Value thread, ConversionPatternRewriter &rewriter,\n                            TypeConverter *typeConverter, Location loc)\n       : mmaLayout(mmaLayout), thread(thread), helper(mmaLayout),\n         rewriter(rewriter), typeConverter(typeConverter), loc(loc),\n-        ctx(mmaLayout.getContext()) {\n-    wpt = mmaLayout.getWarpsPerCTA();\n+        ctx(mmaLayout.getContext()), wpt(mmaLayout.getWarpsPerCTA()) {\n+    helper.deduceMmaType(dotOperand);\n \n     Value _32 = i32_val(32);\n     lane = urem(thread, _32);\n     warp = udiv(thread, _32);\n-    warpMN = udiv(warp, i32_val(wpt[0]));\n-    warpM = urem(warp, i32_val(wpt[0]));\n-    warpN = urem(warpMN, i32_val(wpt[1]));\n   }\n \n-  // Get the mmaInstrShape from either $a or $b.\n+  // Get a warpId for M axis.\n+  Value getWarpM(int M) const {\n+    auto matShape = helper.getMmaMatShape();\n+    return urem(urem(warp, i32_val(wpt[0])), i32_val(M / matShape[0]));\n+  }\n+\n+  // Get a warpId for N axis.\n+  Value getWarpN(int N) const {\n+    auto matShape = helper.getMmaMatShape();\n+    Value warpMN = udiv(warp, i32_val(wpt[0]));\n+    return urem(urem(warpMN, i32_val(wpt[1])), i32_val(N / matShape[1]));\n+  }\n+\n+  // Get the mmaInstrShape deducing either from $a or $b.\n   std::tuple<int, int, int> getMmaInstrShape(Type operand) const {\n     helper.deduceMmaType(operand);\n     auto mmaInstrShape = helper.getMmaInstrShape();\n@@ -4161,6 +4175,7 @@ struct MMA16816ConversionHelper {\n     return std::make_tuple(mmaInstrM, mmaInstrN, mmaInstrK);\n   }\n \n+  // Get the mmaMatShape deducing either from $a or $b.\n   std::tuple<int, int, int> getMmaMatShape(Type operand) const {\n     helper.deduceMmaType(operand);\n     auto matShape = helper.getMmaMatShape();\n@@ -4210,28 +4225,28 @@ struct MMA16816ConversionHelper {\n   }\n \n   // Get number of elements per thread for $a operand.\n-  static size_t getANumElemsPerThread(RankedTensorType operand,\n-                                      ArrayRef<unsigned> wpt) {\n+  static size_t getANumElemsPerThread(RankedTensorType operand, int wpt) {\n     auto shape = operand.getShape();\n-    int repM = getNumRepM(operand, shape[0], wpt[0]);\n+    int repM = getNumRepM(operand, shape[0], wpt);\n     int repK = getNumRepK_(operand, shape[1]);\n     return 4 * repM * repK;\n   }\n \n   // Get number of elements per thread for $b operand.\n-  static size_t getBNumElemsPerThread(RankedTensorType operand,\n-                                      ArrayRef<unsigned> wpt) {\n+  static size_t getBNumElemsPerThread(RankedTensorType operand, int wpt) {\n     auto shape = operand.getShape();\n     int repK = getNumRepK_(operand, shape[0]);\n-    int repN = getNumRepN(operand, shape[1], wpt[1]);\n+    int repN = getNumRepN(operand, shape[1], wpt);\n     return 4 * std::max(repN / 2, 1) * repK;\n   }\n \n   // Loading $a from smem to registers, returns a LLVM::Struct.\n   Value loadA(Value tensor, const SharedMemoryObject &smemObj) const {\n     auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto shape = aTensorTy.getShape();\n+    auto layout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n \n+    SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n+                               aTensorTy.getShape().end());\n     ValueTable ha;\n     std::function<void(int, int)> loadFn;\n     auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n@@ -4241,6 +4256,7 @@ struct MMA16816ConversionHelper {\n     int numRepK = getNumRepK(aTensorTy, shape[1]);\n \n     if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n+      Value warpM = getWarpM(shape[0]);\n       // load from smem\n       loadFn = getLoadMatrixFn(\n           tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n@@ -4268,12 +4284,17 @@ struct MMA16816ConversionHelper {\n   Value loadB(Value tensor, const SharedMemoryObject &smemObj) {\n     ValueTable hb;\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto shape = tensorTy.getShape();\n+    auto layout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+\n+    SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n+                               tensorTy.getShape().end());\n+\n     auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(tensorTy);\n     auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(tensorTy);\n     int numRepK = getNumRepK(tensorTy, shape[0]);\n     int numRepN = getNumRepN(tensorTy, shape[1]);\n \n+    Value warpN = getWarpN(shape[1]);\n     auto loadFn = getLoadMatrixFn(\n         tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n         0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShape*/,\n@@ -4319,7 +4340,11 @@ struct MMA16816ConversionHelper {\n     auto aTensorTy = a.getType().cast<RankedTensorType>();\n     auto dTensorTy = d.getType().cast<RankedTensorType>();\n \n-    auto aShape = aTensorTy.getShape();\n+    SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n+                                aTensorTy.getShape().end());\n+    if (op.transA())\n+      std::swap(aShape[0], aShape[1]);\n+\n     auto dShape = dTensorTy.getShape();\n \n     // shape / shape_per_cta\n@@ -4602,9 +4627,9 @@ Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n   Value res;\n \n   if (!isOuter && mmaLayout.getVersion() == 2 && isHMMA) { // tensor core v2\n-    MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n-                                       rewriter, getTypeConverter(),\n-                                       op.getLoc());\n+    MMA16816ConversionHelper mmaHelper(src.getType(), mmaLayout,\n+                                       getThreadId(rewriter, loc), rewriter,\n+                                       getTypeConverter(), op.getLoc());\n \n     if (dotOperandLayout.getOpIdx() == 0) {\n       // operand $a\n@@ -4695,12 +4720,15 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adaptor,\n                        .cast<RankedTensorType>()\n                        .getEncoding()\n                        .cast<MmaEncodingAttr>();\n-  MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n-                                     rewriter, getTypeConverter(), loc);\n \n   Value A = op.a();\n   Value B = op.b();\n   Value C = op.c();\n+\n+  MMA16816ConversionHelper mmaHelper(A.getType(), mmaLayout,\n+                                     getThreadId(rewriter, loc), rewriter,\n+                                     getTypeConverter(), loc);\n+\n   auto ATensorTy = A.getType().cast<RankedTensorType>();\n   auto BTensorTy = B.getType().cast<RankedTensorType>();\n \n@@ -5532,13 +5560,13 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n         if (mmaLayout.getVersion() == 2) {\n           if (dotOpLayout.getOpIdx() == 0) { // $a\n             int elems =\n-                MMA16816ConversionHelper::getANumElemsPerThread(type, wpt);\n+                MMA16816ConversionHelper::getANumElemsPerThread(type, wpt[0]);\n             return LLVM::LLVMStructType::getLiteral(\n                 ctx, SmallVector<Type>(elems, vecTy));\n           }\n           if (dotOpLayout.getOpIdx() == 1) { // $b\n             int elems =\n-                MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt);\n+                MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt[1]);\n             return struct_ty(SmallVector<Type>(elems, vecTy));\n           }\n         }\n@@ -6175,10 +6203,9 @@ class ConvertTritonGPUToLLVM\n       if (srcBlocked && dstDotOp) {\n         auto tmpType = RankedTensorType::get(\n             dstType.getShape(), dstType.getElementType(),\n-            triton::gpu::SharedEncodingAttr::get(mod.getContext(), dstDotOp,\n-                                                 srcType.getShape(),\n-                                                 getOrder(srcBlocked),\n-                                                 srcType.getElementType()));\n+            triton::gpu::SharedEncodingAttr::get(\n+                mod.getContext(), dstDotOp, srcType.getShape(),\n+                getOrder(srcBlocked), srcType.getElementType()));\n         auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n             cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n         auto newConvert = builder.create<triton::gpu::ConvertLayoutOp>("}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -27,8 +27,6 @@ def matmul_no_scf_kernel(\n     c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n     tl.store(c_ptrs, c)\n \n-# TODO: num_warps could only be 4 for now\n-\n \n @pytest.mark.parametrize('SHAPE,NUM_WARPS,TRANS_A,TRANS_B', [\n     (shape, num_warps, trans_a, trans_b)\n@@ -172,6 +170,7 @@ def get_proper_err(a, b, golden):\n     # Non-forloop\n     [64, 32, 64, 4, 64, 32, 64, False, False],\n     [128, 64, 128, 4, 128, 64, 128, False, False],\n+    [16, 16, 16, 16, 16, 16, 16, False, False],  # wpt overflow issue\n     # K-Forloop\n     [64, 32, 128, 4, 64, 32, 64, False, False],\n     [128, 16, 128, 4, 128, 16, 32, False, False],\n@@ -186,6 +185,7 @@ def get_proper_err(a, b, golden):\n     [128, 256, 128, 4, 128, 256, 32, False, False],\n     [256, 128, 64, 4, 256, 128, 16, False, False],\n     [128, 64, 128, 4, 128, 64, 32, False, False],\n+    # [16, 16, 64, 4, 16, 16, 16, False, False],  # TODO failed due to pipeline pass\n     # trans\n     [128, 64, 128, 4, 128, 64, 32, True, False],\n     [128, 64, 128, 4, 128, 64, 32, False, True],"}]