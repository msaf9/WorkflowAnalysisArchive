[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 70, "deletions": 22, "changes": 92, "file_content_changes": "@@ -2779,7 +2779,7 @@ struct ConvertLayoutOpConversion\n       return multiDimOffset;\n     }\n     if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-      SmallVector<Value> mmaColIdx(2);\n+      SmallVector<Value> mmaColIdx(4);\n       SmallVector<Value> mmaRowIdx(2);\n       Value threadId = getThreadId(rewriter, loc);\n       Value warpSize = idx_val(32);\n@@ -2789,31 +2789,79 @@ struct ConvertLayoutOpConversion\n       SmallVector<Value> multiDimWarpId(2);\n       multiDimWarpId[0] = urem(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n       multiDimWarpId[1] = udiv(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n-      multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n-      multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 8));\n+      Value one = idx_val(1);\n+      Value two = idx_val(2);\n       Value four = idx_val(4);\n-      Value mmaGrpId = udiv(laneId, four);\n-      Value mmaGrpIdP8 = add(mmaGrpId, idx_val(8));\n-      Value mmaThreadIdInGrp = urem(laneId, four);\n-      Value mmaThreadIdInGrpM2 = mul(mmaThreadIdInGrp, idx_val(2));\n-      Value mmaThreadIdInGrpM2P1 = add(mmaThreadIdInGrpM2, idx_val(1));\n-      Value colWarpOffset = mul(multiDimWarpId[0], idx_val(16));\n-      mmaColIdx[0] = add(mmaGrpId, colWarpOffset);\n-      mmaColIdx[1] = add(mmaGrpIdP8, colWarpOffset);\n-      Value rowWarpOffset = mul(multiDimWarpId[1], idx_val(8));\n-      mmaRowIdx[0] = add(mmaThreadIdInGrpM2, rowWarpOffset);\n-      mmaRowIdx[1] = add(mmaThreadIdInGrpM2P1, rowWarpOffset);\n+      Value eight = idx_val(8);\n+      Value sixteen = idx_val(16);\n+      if (mmaLayout.getVersion() == 2) {\n+        multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n+        multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 8));\n+        Value mmaGrpId = udiv(laneId, four);\n+        Value mmaGrpIdP8 = add(mmaGrpId, idx_val(8));\n+        Value mmaThreadIdInGrp = urem(laneId, four);\n+        Value mmaThreadIdInGrpM2 = mul(mmaThreadIdInGrp, idx_val(2));\n+        Value mmaThreadIdInGrpM2P1 = add(mmaThreadIdInGrpM2, idx_val(1));\n+        Value colWarpOffset = mul(multiDimWarpId[0], sixteen);\n+        mmaColIdx[0] = add(mmaGrpId, colWarpOffset);\n+        mmaColIdx[1] = add(mmaGrpIdP8, colWarpOffset);\n+        Value rowWarpOffset = mul(multiDimWarpId[1], eight);\n+        mmaRowIdx[0] = add(mmaThreadIdInGrpM2, rowWarpOffset);\n+        mmaRowIdx[1] = add(mmaThreadIdInGrpM2P1, rowWarpOffset);\n+      } else if (mmaLayout.getVersion() == 1) {\n+        multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n+        multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 16));\n+        Value partId = udiv(laneId, four);\n+        Value partIdDiv4 = udiv(partId, four);\n+        Value partIdRem4 = urem(partId, four);\n+        Value partRowOffset = mul(udiv(partIdRem4, two), eight);\n+        partRowOffset = add(mul(partIdDiv4, four), partRowOffset);\n+        Value partColOffset = mul(urem(partIdRem4, two), eight);\n+        Value colOffset = add(mul(multiDimWarpId[0], sixteen), partColOffset);\n+        Value rowOffset = add(mul(multiDimWarpId[1], sixteen), partRowOffset);\n+        mmaRowIdx[0] = add(urem(laneId, two), rowOffset);\n+        mmaRowIdx[1] = add(mmaRowIdx[0], two);\n+        mmaColIdx[0] = add(udiv(urem(laneId, four), two), colOffset);\n+        mmaColIdx[1] = add(mmaColIdx[0], one);\n+        mmaColIdx[2] = add(mmaColIdx[0], four);\n+        mmaColIdx[3] = add(mmaColIdx[0], idx_val(5));\n+      } else {\n+        llvm_unreachable(\"Unexpected MMALayout version\");\n+      }\n \n       assert(rank == 2);\n-      assert(mmaLayout.getVersion() == 2 &&\n-             \"mmaLayout ver1 not implemented yet\");\n       SmallVector<Value> multiDimOffset(rank);\n-      multiDimOffset[0] = elemId < 2 ? mmaColIdx[0] : mmaColIdx[1];\n-      multiDimOffset[1] = elemId % 2 == 0 ? mmaRowIdx[0] : mmaRowIdx[1];\n-      multiDimOffset[0] = add(multiDimOffset[0],\n-                              idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n-      multiDimOffset[1] = add(multiDimOffset[1],\n-                              idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n+      if (mmaLayout.getVersion() == 2) {\n+        multiDimOffset[0] = elemId < 2 ? mmaColIdx[0] : mmaColIdx[1];\n+        multiDimOffset[1] = elemId % 2 == 0 ? mmaRowIdx[0] : mmaRowIdx[1];\n+        multiDimOffset[0] = add(\n+            multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n+        multiDimOffset[1] = add(\n+            multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n+      } else if (mmaLayout.getVersion() == 1) {\n+        // the order of elements in a thread:\n+        //   c0, c1, c4, c5\n+        //   c2, c3, c6, c7\n+        if (elemId < 2) {\n+          multiDimOffset[0] = mmaColIdx[elemId % 2];\n+          multiDimOffset[1] = mmaRowIdx[0];\n+        } else if (elemId >= 2 && elemId < 4) {\n+          multiDimOffset[0] = mmaColIdx[elemId % 2];\n+          multiDimOffset[1] = mmaRowIdx[1];\n+        } else if (elemId >= 4 && elemId < 6) {\n+          multiDimOffset[0] = mmaColIdx[elemId % 2 + 2];\n+          multiDimOffset[1] = mmaRowIdx[0];\n+        } else if (elemId >= 6) {\n+          multiDimOffset[0] = mmaColIdx[elemId % 2 + 2];\n+          multiDimOffset[1] = mmaRowIdx[1];\n+        }\n+        multiDimOffset[0] = add(\n+            multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n+        multiDimOffset[1] = add(\n+            multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n+      } else {\n+        llvm_unreachable(\"Unexpected MMALayout version\");\n+      }\n       return multiDimOffset;\n     }\n     llvm_unreachable(\"unexpected layout in getMultiDimOffset\");"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 19, "deletions": 3, "changes": 22, "file_content_changes": "@@ -106,9 +106,13 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     return getSizePerThread(sliceLayout.getParent());\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    assert(mmaLayout.getVersion() == 2 &&\n-           \"mmaLayout version = 1 is not implemented yet\");\n-    return SmallVector<unsigned>{2, 2};\n+    if (mmaLayout.getVersion() == 2) {\n+      return SmallVector<unsigned>{2, 2};\n+    } else if (mmaLayout.getVersion() == 1) {\n+      return SmallVector<unsigned>{2, 4};\n+    } else {\n+      llvm_unreachable(\"Unexpected mma version\");\n+    }\n   } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n     auto parentLayout = dotLayout.getParent();\n     assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n@@ -194,6 +198,16 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n       assert(0 && \"DotOperandEncodingAttr non-MmaEncodingAttr parent not \"\n                   \"supported yet\");\n     }\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    if (mmaLayout.getVersion() == 2) {\n+      return {16 * mmaLayout.getWarpsPerCTA()[0],\n+              8 * mmaLayout.getWarpsPerCTA()[1]};\n+    } else if (mmaLayout.getVersion() == 1) {\n+      return {16 * mmaLayout.getWarpsPerCTA()[0],\n+              16 * mmaLayout.getWarpsPerCTA()[1]};\n+    } else {\n+      llvm_unreachable(\"Unexpected mma version\");\n+    }\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n   }\n@@ -358,6 +372,8 @@ unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n     unsigned elemsCol = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]) * 2;\n     unsigned elemsRow = ceil<unsigned>(shape[1], 8 * getWarpsPerCTA()[1]) * 2;\n     res = elemsCol * elemsRow;\n+  } else {\n+    llvm_unreachable(\"Unexpected mma version\");\n   }\n \n   return res;"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 22, "deletions": 2, "changes": 24, "file_content_changes": "@@ -712,8 +712,8 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n-  // CHECK-LABEL: convert_layout_mma_block\n-  func @convert_layout_mma_blocked(%arg0: tensor<32x16xf32, #mma>) {\n+  // CHECK-LABEL: convert_layout_mmav2_block\n+  func @convert_layout_mmav2_blocked(%arg0: tensor<32x16xf32, #mma>) {\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n     // CHECK: llvm.store\n@@ -728,6 +728,26 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n \n // -----\n \n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#mma = #triton_gpu.mma<{version = 1, warpsPerCTA = [2, 1]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n+  // CHECK-LABEL: convert_layout_mmav1_block\n+  func @convert_layout_mmav1_blocked(%arg0: tensor<32x16xf32, #mma>) {\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<32x16xf32, #mma>) -> tensor<32x16xf32, #blocked0>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n #shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {"}]