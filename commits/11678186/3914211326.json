[{"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -3,6 +3,7 @@\n #AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#A_SHARED_T = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [0, 1]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n@@ -57,7 +58,7 @@ func @trans(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %tensor = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n   // CHECK: %0 -> %cst\n-  %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n   return\n }\n "}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -4,6 +4,7 @@\n #sliceAd0 = #triton_gpu.slice<{dim = 0, parent = #AL}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#A_SHARED_T = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [0, 1]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n@@ -178,7 +179,7 @@ func @scratch() {\n func @trans(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 1024\n   %tensor = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n-  %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n   return\n }\n \n@@ -303,7 +304,7 @@ func @for_use_ancestor(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16\n   // CHECK-NEXT: offset = 16384, size = 8192\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %a_shared, %b_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n-    %c0 = tt.trans %c_shared_init : (tensor<128x32xf16, #A_SHARED>) -> tensor<32x128xf16, #A_SHARED>\n+    %c0 = tt.trans %c_shared_init : (tensor<128x32xf16, #A_SHARED>) -> tensor<32x128xf16, #A_SHARED_T>\n     // CHECK-NEXT: offset = 24576, size = 8192\n     %c1 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n     scf.yield %b_shared, %a_shared: tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -4,6 +4,7 @@\n #sliceAd0 = #triton_gpu.slice<{dim = 0, parent = #AL}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#A_SHARED_T = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [0, 1]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n@@ -114,7 +115,7 @@ func @extract_slice() {\n // CHECK-LABEL: trans\n func @trans() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n-  %b = tt.trans %cst0 : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  %b = tt.trans %cst0 : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n   return\n }\n "}]