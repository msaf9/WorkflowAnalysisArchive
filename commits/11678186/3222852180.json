[{"filename": "include/triton/codegen/analysis/align.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -14,6 +14,7 @@ namespace ir {\n   class cast_inst;\n   class cmp_inst;\n   class reshape_inst;\n+  class dequantize_inst;\n   class broadcast_inst;\n   class binary_operator;\n   class getelementptr_inst;\n@@ -34,6 +35,7 @@ class align {\n   std::vector<cst_info> populate_is_constant_phi(ir::phi_node* x);\n   std::vector<cst_info> populate_is_constant_splat(ir::splat_inst* x);\n   std::vector<cst_info> populate_is_constant_reshape(ir::reshape_inst* x);\n+  std::vector<cst_info> populate_is_constant_dequantize(ir::dequantize_inst* x);\n   std::vector<cst_info> populate_is_constant_broadcast(ir::broadcast_inst* x);\n   std::vector<cst_info> populate_is_constant_binop(ir::binary_operator* x);\n   std::vector<cst_info> populate_is_constant_cmp(ir::cmp_inst* x);\n@@ -44,6 +46,7 @@ class align {\n   std::vector<unsigned> populate_max_contiguous_phi(ir::phi_node* x);\n   std::vector<unsigned> populate_max_contiguous_splat(ir::splat_inst* x);\n   std::vector<unsigned> populate_max_contiguous_reshape(ir::reshape_inst* x);\n+  std::vector<unsigned> populate_max_contiguous_dequantize(ir::dequantize_inst* x);\n   std::vector<unsigned> populate_max_contiguous_broadcast(ir::broadcast_inst* x);\n   std::vector<unsigned> populate_max_contiguous_binop(ir::binary_operator* x);\n   std::vector<unsigned> populate_max_contiguous_gep(ir::getelementptr_inst* x);\n@@ -54,6 +57,7 @@ class align {\n   std::vector<unsigned> populate_starting_multiple_phi(ir::phi_node* x);\n   std::vector<unsigned> populate_starting_multiple_splat(ir::splat_inst* x);\n   std::vector<unsigned> populate_starting_multiple_reshape(ir::reshape_inst* x);\n+  std::vector<unsigned> populate_starting_multiple_dequantize(ir::dequantize_inst* x);\n   std::vector<unsigned> populate_starting_multiple_broadcast(ir::broadcast_inst* x);\n   std::vector<unsigned> populate_starting_multiple_binop(ir::binary_operator* x);\n   std::vector<unsigned> populate_starting_multiple_gep(ir::getelementptr_inst* x);"}, {"filename": "include/triton/codegen/analysis/axes.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -25,6 +25,7 @@ class axes {\n   void update_graph_reduce(ir::instruction *i);\n   void update_graph_reshape(ir::instruction *i);\n   void update_graph_trans(ir::instruction *i);\n+  void update_graph_dequantize(ir::instruction *i);\n   void update_graph_broadcast(ir::instruction *i);\n   void update_graph_dot(ir::instruction *i);\n   void update_graph_elementwise(ir::instruction *i,"}, {"filename": "include/triton/codegen/selection/generator.h", "status": "modified", "additions": 10, "deletions": 2, "changes": 12, "file_content_changes": "@@ -152,7 +152,15 @@ class generator: public ir::visitor, public analysis::layout_visitor {\n   std::tuple<Value*, Value*, Value*, Value*> bf16x4_to_fp8x4(Value *in0, Value *in1, Value *in2, Value *in3);\n   Value* bf16_to_fp32(Value *in0);\n   Value* fp32_to_bf16(Value *in0);\n-\n+  std::tuple<Value*, Value*, Value*, Value*, Value*, Value*, Value*, Value*> int16_to_float16x8(\n+    Value *in0, Value *scale_x512, Value *shift\n+  );\n+  std::tuple<Value*, Value*, Value*, Value*, Value*, Value*, Value*, Value*> int32_to_float16x8(\n+    Value *in0, Value *scale_x512, Value *shift\n+  );\n+  std::tuple<Value*, Value*, Value*, Value*> int32_to_float16x4(Value *in0, Value *scale_x512, Value *shift);\n+  std::tuple<Value*, Value*> prepare_scale_shift(Value *scale, Value *shift);\n+  void visit_dequantize_inst(ir::dequantize_inst*);\n   void visit_cast_inst(ir::cast_inst*);\n   void visit_return_inst(ir::return_inst*);\n   void visit_cond_branch_inst(ir::cond_branch_inst*);\n@@ -265,7 +273,7 @@ class generator: public ir::visitor, public analysis::layout_visitor {\n   /// idx for multi-stage pipeline\n   std::map<analysis::data_layout*, Value*> read_smem_idx_;\n   std::map<analysis::data_layout*, Value*> write_smem_idx_;\n-  \n+\n   /// triton bb -> llvm bb\n   std::map<ir::value*, BasicBlock *> bbs_;\n   std::map<ir::value*, std::vector<int>> ords_;"}, {"filename": "include/triton/ir/builder.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -73,6 +73,8 @@ class builder{\n   value* create_cond_br(value *cond, basic_block* if_dest, basic_block* else_dest);\n   value* create_ret_void();\n   value* create_ret(value *ret);\n+  // Dequantize instructions\n+  value* create_dequantize(value *src, value *scale, value *shift, type *dest_ty);\n   // Cast instructions\n   value* create_bitcast(value *src, type *dest_ty);\n   value *create_cast(cast_op_t op, value *v, type *dst_ty);"}, {"filename": "include/triton/ir/enums.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -108,6 +108,8 @@ enum value_id_t: unsigned {\n   // cmp\n   INST_ICMP,\n   INST_FCMP,\n+  // dequantize\n+  INST_DEQUANTIZE,\n   // cast\n   INST_CAST_TRUNC,\n   INST_CAST_ZEXT,"}, {"filename": "include/triton/ir/instructions.h", "status": "modified", "additions": 25, "deletions": 7, "changes": 32, "file_content_changes": "@@ -274,6 +274,24 @@ class unary_inst: public instruction {\n   unary_inst(type *ty, value_id_t id, value *v, const std::string &name, instruction *next);\n };\n \n+//===----------------------------------------------------------------------===//\n+//                               dequantize_inst classes\n+//===----------------------------------------------------------------------===//\n+\n+class dequantize_inst: public instruction{\n+private:\n+  std::string repr_impl() const override { return \"dequantize\"; }\n+\n+protected:\n+  dequantize_inst(type *ty, value *v, value *scale, value *shift, const std::string &name, instruction *next);\n+\n+public:\n+  static dequantize_inst *create(value *arg, value *scale, value *shift, type *ty,\n+                           const std::string &name = \"\", instruction *next = nullptr);\n+\n+  _TRITON_DEFINE_CLONE(dequantize_inst)\n+  _TRITON_DEFINE_ACCEPT(dequantize_inst)\n+};\n \n //===----------------------------------------------------------------------===//\n //                               cast_inst classes\n@@ -482,7 +500,7 @@ class load_inst: public io_inst {\n   std::string get_cache_modifier_repr() const {\n     if (cache_ == CA) return \".ca\";\n     if (cache_ == CG) return \".cg\";\n-    return \"\"; \n+    return \"\";\n   }\n   CACHE_MODIFIER cache_;\n \n@@ -850,16 +868,16 @@ class log_inst: public builtin_inst {\n class dot_inst: public builtin_inst {\n public:\n   enum TransT { NoTrans, Trans };\n-  enum DataType { \n-    FP8, FP16, BF16, TF32, FP32, \n-    INT1, INT4, INT8, INT32, \n+  enum DataType {\n+    FP8, FP16, BF16, TF32, FP32,\n+    INT1, INT4, INT8, INT32,\n     UNKNOWN,\n   };\n \n private:\n   dot_inst(value *A, value *B, value *C, TransT AT, TransT BT, bool allow_tf32, const std::string &name, instruction *next);\n   std::string repr_impl() const { return \"dot\"; }\n-  \n+\n public:\n   bool is_prefetched() const { return is_prefetched_; }\n   void set_prefetched(bool is_prefetched) { is_prefetched_ = is_prefetched; }\n@@ -1046,11 +1064,11 @@ class prefetch_s_inst : public instruction {\n   std::string repr_impl() const { return \"prefetch_s\"; }\n   _TRITON_DEFINE_CLONE(prefetch_s_inst)\n   _TRITON_DEFINE_ACCEPT(prefetch_s_inst)\n-  \n+\n   /// inc_: 0->first, 1->latch\n   int inc_ = 0;\n public:\n-  prefetch_s_inst(context &ctx, value *arg, int inc, const std::string &name, instruction *next) \n+  prefetch_s_inst(context &ctx, value *arg, int inc, const std::string &name, instruction *next)\n     : instruction(type::get_void_ty(ctx), INST_PREFETCH_S, 1, name, next), inc_(inc) {\n     set_operand(0, arg);\n   }"}, {"filename": "include/triton/ir/visitor.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -20,6 +20,7 @@ class getelementptr_inst;\n \n class icmp_inst;\n class fcmp_inst;\n+class dequantize_inst;\n class cast_inst;\n class trunc_inst;\n class z_ext_inst;\n@@ -124,6 +125,7 @@ class visitor {\n \n   virtual void visit_icmp_inst(icmp_inst*) = 0;\n   virtual void visit_fcmp_inst(fcmp_inst*) = 0;\n+  virtual void visit_dequantize_inst(dequantize_inst*) = 0;\n   virtual void visit_cast_inst(cast_inst*) = 0;\n \n   virtual void visit_return_inst(return_inst*) = 0;"}, {"filename": "lib/codegen/analysis/align.cc", "status": "modified", "additions": 53, "deletions": 1, "changes": 54, "file_content_changes": "@@ -115,6 +115,18 @@ std::vector<align::cst_info> align::populate_is_constant_reshape(ir::reshape_ins\n   return add_to_cache(x, result, is_constant_);\n }\n \n+std::vector<align::cst_info> align::populate_is_constant_dequantize(ir::dequantize_inst* x) {\n+  auto x_shapes = get_shapes(x);\n+  std::vector<cst_info> result;\n+  ir::value *op = x->get_operand(0);\n+  auto op_shapes = op->get_type()->get_block_shapes();\n+  auto op_cst = populate_is_constant(op);\n+  for(size_t d = 0; d < x_shapes.size(); d++) {\n+    result.push_back(op_cst[d]);\n+  }\n+  return add_to_cache(x, result, is_constant_);\n+}\n+\n std::vector<align::cst_info> align::populate_is_constant_broadcast(ir::broadcast_inst* x) {\n   auto x_shapes = get_shapes(x);\n   std::vector<cst_info> result;\n@@ -146,7 +158,7 @@ std::vector<align::cst_info> align::populate_is_constant_cmp(ir::cmp_inst* x) {\n     //   16 17 18 ... 32   <  24 24 24 ... 24 => equal in groups of 8\n     //   16 17 18 ... 32   <  20 20 20 ... 20 => equal in groups of 4\n     //   16 17 18 ... 32   <  16 16 16 ... 16 => equal in groups of 16\n-    //   \n+    //\n     //   if LHS is a range of N continuous (or equal) elements that starts at M,\n     //   and RHS is a set of N constants that start at K\n     //   then the result in constant in groups of gcd(M, K)\n@@ -212,6 +224,8 @@ std::vector<align::cst_info> align::populate_is_constant(ir::value *v) {\n     return populate_is_constant_splat(x);\n   if(auto *x = dynamic_cast<ir::reshape_inst*>(v))\n     return populate_is_constant_reshape(x);\n+  if(auto *x = dynamic_cast<ir::dequantize_inst*>(v))\n+    return populate_is_constant_dequantize(x);\n   if(auto *x = dynamic_cast<ir::broadcast_inst*>(v))\n     return populate_is_constant_broadcast(x);\n   if(auto *x = dynamic_cast<ir::binary_operator*>(v))\n@@ -279,6 +293,23 @@ std::vector<unsigned> align::populate_max_contiguous_reshape(ir::reshape_inst* x\n   return add_to_cache(x, result, max_contiguous_);\n }\n \n+std::vector<unsigned> align::populate_max_contiguous_dequantize(ir::dequantize_inst* x) {\n+  auto shapes = get_shapes(x);\n+  std::vector<unsigned> result;\n+  ir::value *op = x->get_operand(0);\n+  auto ret_last_dim = (x->get_type()->get_block_shapes()).back();\n+  auto op_last_dim = (op->get_type()->get_block_shapes()).back();\n+  auto op_mc = populate_max_contiguous(op);\n+  for(size_t d = 0; d < shapes.size(); d++) {\n+    unsigned factor = 1;\n+    if (d == shapes.size() - 1) {\n+      factor = ret_last_dim / op_last_dim;\n+    }\n+    result.push_back(factor * op_mc[d]);\n+  }\n+  return add_to_cache(x, result, max_contiguous_);\n+}\n+\n std::vector<unsigned> align::populate_max_contiguous_broadcast(ir::broadcast_inst* x) {\n   auto shapes = get_shapes(x);\n   std::vector<unsigned> result;\n@@ -376,6 +407,8 @@ std::vector<unsigned> align::populate_max_contiguous(ir::value *v){\n     return populate_max_contiguous_splat(x);\n   if(auto *x = dynamic_cast<ir::reshape_inst*>(v))\n     return populate_max_contiguous_reshape(x);\n+  if(auto *x = dynamic_cast<ir::dequantize_inst*>(v))\n+    return populate_max_contiguous_dequantize(x);\n   if(auto *x = dynamic_cast<ir::broadcast_inst*>(v))\n     return populate_max_contiguous_broadcast(x);\n   if(auto *x = dynamic_cast<ir::binary_operator*>(v))\n@@ -420,6 +453,23 @@ std::vector<unsigned> align::populate_starting_multiple_reshape(ir::reshape_inst\n   return add_to_cache(x, result, starting_multiple_);\n }\n \n+std::vector<unsigned> align::populate_starting_multiple_dequantize(ir::dequantize_inst* x){\n+  auto shapes = get_shapes(x);\n+  std::vector<unsigned> result;\n+  ir::value *op = x->get_operand(0);\n+  auto ret_last_dim = (x->get_type()->get_block_shapes()).back();\n+  auto op_last_dim = (op->get_type()->get_block_shapes()).back();\n+  auto op_multiple = populate_starting_multiple(op);\n+  for(size_t d = 0; d < shapes.size(); d++) {\n+    unsigned factor = 1;\n+    if (d == shapes.size() - 1) {\n+      factor = ret_last_dim / op_last_dim;\n+    }\n+    result.push_back(factor * op_multiple[d]);\n+  }\n+  return add_to_cache(x, result, starting_multiple_);\n+}\n+\n std::vector<unsigned> align::populate_starting_multiple_broadcast(ir::broadcast_inst* x){\n   auto result = populate_starting_multiple(x->get_operand(0));\n   return add_to_cache(x, result, starting_multiple_);\n@@ -539,6 +589,8 @@ std::vector<unsigned> align::populate_starting_multiple(ir::value *v){\n     return populate_starting_multiple_splat(x);\n   if(auto *x = dynamic_cast<ir::reshape_inst*>(v))\n     return populate_starting_multiple_reshape(x);\n+  if(auto *x = dynamic_cast<ir::dequantize_inst*>(v))\n+    return populate_starting_multiple_dequantize(x);\n   if(auto *x = dynamic_cast<ir::broadcast_inst*>(v))\n     return populate_starting_multiple_broadcast(x);\n   if(auto *x = dynamic_cast<ir::phi_node*>(v))"}, {"filename": "lib/codegen/analysis/axes.cc", "status": "modified", "additions": 13, "deletions": 1, "changes": 14, "file_content_changes": "@@ -56,6 +56,17 @@ void axes::update_graph_trans(ir::instruction *i) {\n     graph_.add_edge({i, perm[d]}, {op, d});\n }\n \n+void axes::update_graph_dequantize(ir::instruction *i) {\n+  auto *dequantize = static_cast<ir::dequantize_inst*>(i);\n+  auto shapes = dequantize->get_type()->get_block_shapes();\n+  ir::value *op = dequantize->get_operand(0);\n+\n+  // add edge except the last axis\n+  for(unsigned d = 0; d < shapes.size() - 1; d ++){\n+    graph_.add_edge({i, d}, {op, d});\n+  }\n+}\n+\n void axes::update_graph_broadcast(ir::instruction *i) {\n   auto *broadcast = static_cast<ir::broadcast_inst*>(i);\n   auto shapes = broadcast->get_type()->get_block_shapes();\n@@ -79,7 +90,7 @@ void axes::update_graph_dot(ir::instruction *i) {\n     graph_.add_edge({dot, d}, {D, d});\n }\n \n-void axes::update_graph_elementwise(ir::instruction *i, \n+void axes::update_graph_elementwise(ir::instruction *i,\n                                     bool is_masked_load_async) {\n   if(i->get_num_operands() == 0)\n     return;\n@@ -119,6 +130,7 @@ void axes::update_graph(ir::instruction *i) {\n     case ir::INST_SPLAT:             return update_graph_no_edge(i);\n     case ir::INST_CAT:               return update_graph_elementwise(i, true);\n     case ir::INST_TRANS:             return update_graph_trans(i);\n+    case ir::INST_DEQUANTIZE:        return update_graph_dequantize(i);\n     case ir::INST_BROADCAST:         return update_graph_broadcast(i);\n     case ir::INST_DOT:               return update_graph_dot(i);\n     case ir::INST_COPY_TO_SHARED:    return update_graph_no_edge(i);"}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 265, "deletions": 36, "changes": 301, "file_content_changes": "@@ -99,6 +99,7 @@ Value* geper::operator()(Value *ptr, Value* off, const std::string& name){\n #define vec_ty(type, num_el) VectorType::get(type, num_el, false)\n #define ptr_ty(...)          PointerType::get(__VA_ARGS__)\n // constants\n+#define i16(...)             builder_->getInt16(__VA_ARGS__)\n #define i32(...)             builder_->getInt32(__VA_ARGS__)\n // ops\n #define and_(...)            builder_->CreateAnd(__VA_ARGS__)\n@@ -854,6 +855,234 @@ void generator::visit_cast_inst(ir::cast_inst* x) {\n   }\n }\n \n+std::tuple<Value*, Value*, Value*, Value*, Value*, Value*, Value*, Value*> generator::int16_to_float16x8(\n+  Value *in0, Value *scale_x512, Value *shift\n+){\n+  /* unpacking 8 int2s packed into an int16 to 8 float16s\n+   * the algorithm is similar to\n+   * https://github.com/pytorch/FBGEMM/blob/6a59bb6621ba9ec7d650ccb78b78ea24d62a3904/\n+     fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh#L1492-L1563\n+   */\n+  Type *ret_ty = StructType::get(*ctx_, {vec_ty(f16_ty, 2), vec_ty(f16_ty, 2), vec_ty(f16_ty, 2), vec_ty(f16_ty, 2)});\n+  InlineAsm *ptx = InlineAsm::get(FunctionType::get(ret_ty, {i32_ty, i32_ty, i32_ty}, false),\n+  \"{\"\n+  \".reg .b32 a<2>, b<4>;                   \\n\\t\" // input is 0xab,cd,ef,gh,ab,cd,ef,gh, each a, b etc occupies two bits.\n+  \"and.b32 a0, 0x30300303, $4;            \\n\\t\" // set a0 to 0x0b,00,0f,00,00,0d,00,0h\n+  \"and.b32 a1, 0xc0c00c0c, $4;            \\n\\t\" // set a1 to 0xa0,00,e0,00,00,c0,00,g0\n+  \"prmt.b32 b0, 0, a0, 0x0504;            \\n\\t\" // set b0 to 0x00,00,00,0d,00,00,00,0h\n+  \"prmt.b32 b1, 0, a1, 0x0504;            \\n\\t\" // set b1 to 0x00,00,00,c0,00,00,00,g0\n+  \"prmt.b32 b2, 0, a0, 0x0706;            \\n\\t\" // set b2 to 0x00,00,0b,00,00,00,0f,00\n+  \"prmt.b32 b3, 0, a1, 0x0706;            \\n\\t\" // set b3 to 0x00,00,a0,00,00,00,e0,00\n+  \"mov.b32 a0, 0x78007800;               \\n\\t\" // a0 = 32768\n+  \"mov.b32 a1, 0x70007000;               \\n\\t\" // a1 = 8192\n+  \"mul.f16x2 b0, b0, a0; \\n\\t\" // b0 = b0 * 32768.\n+  \"mul.f16x2 b1, b1, a1; \\n\\t\" // b1 = b1 * 8192.\n+  \"mov.b32 a0, 0x68006800;               \\n\\t\"  // a0 = 2048\n+  \"mov.b32 a1, 0x60006000;               \\n\\t\"  // a1 = 512\n+  \"mul.f16x2 b2, b2, a0; \\n\\t\" // b2 = b2 * 2048.\n+  \"mul.f16x2 b3, b3, a1; \\n\\t\" // b3 = b3 * 512.\n+  \"fma.rn.f16x2 $0, b0, $5, $6; \\n\\t\" // out0 = b0 * scale + shift.\n+  \"fma.rn.f16x2 $1, b1, $5, $6; \\n\\t\" // out1 = b1 * scale + shift.\n+  \"fma.rn.f16x2 $2, b2, $5, $6; \\n\\t\" // out2 = b2 * scale + shift.\n+  \"fma.rn.f16x2 $3, b3, $5, $6; \\n\\t\" // out3 = b3 * scale + shift.\n+  \"}\", \"=r,=r,=r,=r,r,r,r\", false);\n+\n+  Value *packed_in = UndefValue::get(vec_ty(i16_ty, 2));\n+  packed_in = insert_elt(packed_in, in0, (int)0);\n+  packed_in = insert_elt(packed_in, in0, (int)1);\n+  Value *in = bit_cast(packed_in, i32_ty);\n+\n+  Value *ret = call(ptx, {in, scale_x512, shift});\n+  Value *packed_ret0 = extract_val(ret, {0});\n+  Value *packed_ret1 = extract_val(ret, {1});\n+  Value *packed_ret2 = extract_val(ret, {2});\n+  Value *packed_ret3 = extract_val(ret, {3});\n+  Value *ret0 = extract_elt(packed_ret0, (uint64_t)0); // h\n+  Value *ret1 = extract_elt(packed_ret1, (uint64_t)0); // g\n+  Value *ret2 = extract_elt(packed_ret2, (uint64_t)0); // f\n+  Value *ret3 = extract_elt(packed_ret3, (uint64_t)0); // e\n+  Value *ret4 = extract_elt(packed_ret0, (uint64_t)1); // d\n+  Value *ret5 = extract_elt(packed_ret1, (uint64_t)1); // c\n+  Value *ret6 = extract_elt(packed_ret2, (uint64_t)1); // b\n+  Value *ret7 = extract_elt(packed_ret3, (uint64_t)1); // a\n+  return std::make_tuple(ret0, ret1, ret2, ret3, ret4, ret5, ret6, ret7);\n+}\n+\n+std::tuple<Value*, Value*, Value*, Value*, Value*, Value*, Value*, Value*> generator::int32_to_float16x8(\n+  Value *in0, Value *scale_x512, Value *shift\n+){\n+  /* unpacking 8 int4s packed into an int32 to 8 float16s\n+   * the algorithm is similar to\n+   * https://github.com/pytorch/FBGEMM/blob/6a59bb6621ba9ec7d650ccb78b78ea24d62a3904/\n+     fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh#L1566-L1619\n+   */\n+  Type *ret_ty = StructType::get(*ctx_, {vec_ty(f16_ty, 2), vec_ty(f16_ty, 2), vec_ty(f16_ty, 2), vec_ty(f16_ty, 2)});\n+  InlineAsm *ptx = InlineAsm::get(FunctionType::get(ret_ty, {i32_ty, i32_ty, i32_ty}, false),\n+  \"{\"\n+  \".reg .b32 a<2>, b<4>;                  \\n\\t\"\n+  \"and.b32 a0, 0x0f0f0f0f, $4;            \\n\\t\" // If input is 0xabcdefgh set a to 0x0b0d0f0h\n+  \"and.b32 a1, 0xf0f0f0f0, $4;            \\n\\t\" // If input is 0xabcdefgh set a to 0xa0c0e0g0\n+  \"prmt.b32 b0, 0, a0, 0x0504;            \\n\\t\" // set b0 to 0x000f000h\n+  \"prmt.b32 b1, 0, a1, 0x0504;            \\n\\t\" // set b1 to 0x00e000g0\n+  \"prmt.b32 b2, 0, a0, 0x0706;            \\n\\t\" // set b2 to 0x000b000d\n+  \"prmt.b32 b3, 0, a1, 0x0706;            \\n\\t\" // set b3 to 0x00a000c0\n+  \"mov.b32 a0, 0x78007800;               \\n\\t\"\n+  \"mov.b32 a1, 0x68006800;               \\n\\t\"\n+  \"mul.f16x2 b0, b0, a0; \\n\\t\" // b0 = b0 * 32768.\n+  \"mul.f16x2 b1, b1, a1; \\n\\t\" // b1 = b1 * 2048.\n+  \"mul.f16x2 b2, b2, a0; \\n\\t\" // b2 = b2 * 32768.\n+  \"mul.f16x2 b3, b3, a1; \\n\\t\" // b3 = b3 * 2048.\n+  \"fma.rn.f16x2 $0, b0, $5, $6; \\n\\t\" // out0 = b0 * scale + shift.\n+  \"fma.rn.f16x2 $1, b1, $5, $6; \\n\\t\" // out1 = b1 * scale + shift.\n+  \"fma.rn.f16x2 $2, b2, $5, $6; \\n\\t\" // out0 = b0 * scale + shift.\n+  \"fma.rn.f16x2 $3, b3, $5, $6; \\n\\t\" // out1 = b1 * scale + shift.\n+  \"}\", \"=r,=r,=r,=r,r,r,r\", false);\n+\n+  Value *ret = call(ptx, {in0, scale_x512, shift});\n+  Value *packed_ret0 = extract_val(ret, {0});\n+  Value *packed_ret1 = extract_val(ret, {1});\n+  Value *packed_ret2 = extract_val(ret, {2});\n+  Value *packed_ret3 = extract_val(ret, {3});\n+  Value *ret0 = extract_elt(packed_ret0, (uint64_t)0); // h\n+  Value *ret1 = extract_elt(packed_ret1, (uint64_t)0); // g\n+  Value *ret2 = extract_elt(packed_ret0, (uint64_t)1); // f\n+  Value *ret3 = extract_elt(packed_ret1, (uint64_t)1); // e\n+  Value *ret4 = extract_elt(packed_ret2, (uint64_t)0); // d\n+  Value *ret5 = extract_elt(packed_ret3, (uint64_t)0); // c\n+  Value *ret6 = extract_elt(packed_ret2, (uint64_t)1); // b\n+  Value *ret7 = extract_elt(packed_ret3, (uint64_t)1); // a\n+  return std::make_tuple(ret0, ret1, ret2, ret3, ret4, ret5, ret6, ret7);\n+}\n+\n+std::tuple<Value*, Value*, Value*, Value*> generator::int32_to_float16x4(Value *in0, Value *scale_x512, Value *shift){\n+  /* unpacking 4 int8s packed into an int32 to 4 fp16s\n+   * the algorithm is similar to\n+   * https://github.com/pytorch/FBGEMM/blob/6a59bb6621ba9ec7d650ccb78b78ea24d62a3904/\n+     fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh#L1622-L1646\n+   */\n+  Type *ret_ty = StructType::get(*ctx_, {vec_ty(f16_ty, 2), vec_ty(f16_ty, 2)});\n+  InlineAsm *ptx = InlineAsm::get(FunctionType::get(ret_ty, {i32_ty, i32_ty, i32_ty}, false),\n+  \"{\"\n+  \".reg .b32 a, b<2>;                    \\n\\t\"\n+  \"prmt.b32 b0, 0, $2, 0x0504;            \\n\\t\" // If input is 0xabcdefgh set b0 to 0x00ef00gh\n+  \"prmt.b32 b1, 0, $2, 0x0706;            \\n\\t\" // If input is 0xabcdefgh set b1 to 0x00ab00cd\n+  \"mov.b32 a, 0x78007800;               \\n\\t\"\n+  \"mul.f16x2 b0, b0, a; \\n\\t\" // b0 = b0 * 32768.\n+  \"mul.f16x2 b1, b1, a; \\n\\t\" // b1 = b1 * 32768.\n+  \"fma.rn.f16x2 $0, b0, $3, $4; \\n\\t\" // out0 = b0 * scale + shift.\n+  \"fma.rn.f16x2 $1, b1, $3, $4; \\n\\t\" // out1 = b1 * scale + shift.\n+  \"}\", \"=r,=r,r,r,r\", false);\n+\n+  Value *ret = call(ptx, {in0, scale_x512, shift});\n+  Value *packed_ret0 = extract_val(ret, {0});\n+  Value *packed_ret1 = extract_val(ret, {1});\n+  Value *ret0 = extract_elt(packed_ret0, (uint64_t)0); // gh\n+  Value *ret1 = extract_elt(packed_ret0, (uint64_t)1); // ef\n+  Value *ret2 = extract_elt(packed_ret1, (uint64_t)0); // cd\n+  Value *ret3 = extract_elt(packed_ret1, (uint64_t)1); // ab\n+  return std::make_tuple(ret0, ret1, ret2, ret3);\n+}\n+\n+std::tuple<Value*, Value*> generator::prepare_scale_shift(Value *scale, Value *shift){\n+  Value *scale_x512 = fmul(scale, bit_cast(i16(0x6000), f16_ty));\n+  Value *p_scale_x512 = UndefValue::get(vec_ty(f16_ty, 2));\n+  p_scale_x512 = insert_elt(p_scale_x512, scale_x512, (int)0);\n+  p_scale_x512 = insert_elt(p_scale_x512, scale_x512, (int)1);\n+  p_scale_x512 = bit_cast(p_scale_x512, i32_ty);\n+\n+  Value *p_shift = UndefValue::get(vec_ty(f16_ty, 2));\n+  p_shift = insert_elt(p_shift, shift, (int)0);\n+  p_shift = insert_elt(p_shift, shift, (int)1);\n+  p_shift = bit_cast(p_shift, i32_ty);\n+\n+  return std::make_tuple(p_scale_x512, p_shift);\n+}\n+\n+/**\n+ * \\brief Code Generation for `dequantize`\n+ */\n+void generator::visit_dequantize_inst(ir::dequantize_inst* x) {\n+  ir::value *op = x->get_operand(0);\n+\n+  auto src_ty_size_in_bits = op->get_type()->get_scalar_ty()->get_primitive_size_in_bits();\n+\n+  auto ret_last_dim = (x->get_type()->get_block_shapes()).back();\n+  auto op_last_dim = (op->get_type()->get_block_shapes()).back();\n+\n+  auto x_idxs = idxs_.at(x);\n+  auto op_idxs = idxs_.at(op);\n+\n+  ir::value *scale = x->get_operand(1);\n+  ir::value *shift = x->get_operand(2);\n+\n+  Value *p_scale_x512, *p_shift;\n+  std::tie(p_scale_x512, p_shift) = prepare_scale_shift(vals_[scale][{}], vals_[shift][{}]);\n+\n+  int ld = layouts_->get(x)->get_order(0);\n+  int contiguous = layouts_->get(x)->to_scanline()->nts(ld);\n+\n+  int op_ld = layouts_->get(op)->get_order(0);\n+  int op_contiguous = layouts_->get(op)->to_scanline()->nts(op_ld);\n+\n+  std::string err_msg;\n+  err_msg = \"unsupported dequantization, cannot vectorize properly. x_idxs.size(): \"\n+            + std::to_string(x_idxs.size()) + \"; op_idxs.size(): \"\n+            + std::to_string(op_idxs.size()) + \"; contiguous: \"\n+            + std::to_string(contiguous) + \"; op_contiguous: \"\n+            + std::to_string(op_contiguous) + \". if the condition \"\n+            \"is not met, please try adjusting block_size, num_warps or \"\n+            \"using tl.multiple_of to hint the input/output ptr address.\";\n+\n+  if (ret_last_dim == 8 * op_last_dim) {\n+    if((x_idxs.size() != 8 * op_idxs.size()) || (contiguous != 8 * op_contiguous)) {\n+      throw std::runtime_error(err_msg);\n+    }\n+\n+    auto cvt = [&](\n+      Value* a, Value* scale, Value* shift\n+    ){\n+      if (src_ty_size_in_bits == 16){ // int2 quantization, int16 to 8 fp16s\n+        return int16_to_float16x8(a, scale, shift);\n+      } else if (src_ty_size_in_bits == 32) { // int4 quantization, int32 to 8 fp16s\n+        return int32_to_float16x8(a, scale, shift);\n+      } else {\n+        throw std::runtime_error(\"unsupported conversion\");\n+      }\n+    };\n+\n+    for(size_t j = 0; j < op_idxs.size(); j++){\n+        size_t i = j * 8;\n+        std::tie(vals_[x][x_idxs[i+0]],\n+                  vals_[x][x_idxs[i+1]],\n+                  vals_[x][x_idxs[i+2]],\n+                  vals_[x][x_idxs[i+3]],\n+                  vals_[x][x_idxs[i+4]],\n+                  vals_[x][x_idxs[i+5]],\n+                  vals_[x][x_idxs[i+6]],\n+                  vals_[x][x_idxs[i+7]]) = cvt(vals_[op][op_idxs[j]], p_scale_x512, p_shift);\n+    }\n+  } else if (ret_last_dim == 4 * op_last_dim && src_ty_size_in_bits == 32) { // int8 quantization, int32 to 4 fp16s\n+    if((x_idxs.size() != 4 * op_idxs.size()) || (contiguous != 4 * op_contiguous)) {\n+      throw std::runtime_error(err_msg);\n+    }\n+\n+    auto cvt = [&](Value* a, Value* scale, Value* shift){\n+      return int32_to_float16x4(a, scale, shift);\n+    };\n+\n+    for(size_t j = 0; j < op_idxs.size(); j++){\n+        size_t i = j * 4;\n+        std::tie(vals_[x][x_idxs[i+0]],\n+                  vals_[x][x_idxs[i+1]],\n+                  vals_[x][x_idxs[i+2]],\n+                  vals_[x][x_idxs[i+3]]) = cvt(vals_[op][op_idxs[j]], p_scale_x512, p_shift);\n+    }\n+  } else {\n+    throw std::runtime_error(\"unsupported dequantization\");\n+  }\n+  return;\n+}\n+\n /**\n  * \\brief Code Generation for `return`\n  */\n@@ -907,7 +1136,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n \n     vec = std::min<size_t>(layout->contig_per_thread(ord[0]), aln);\n     // TODO: generalize\n-    is_mma_first_row = (ord.size() >= 1) && layout->to_mma() && \n+    is_mma_first_row = (ord.size() >= 1) && layout->to_mma() &&\n                        (a_axes_->get(x, ord[0]) == layouts_->get(x)->get_axis(1));\n     if(is_mma_first_row)\n       vec = std::min<size_t>(2, aln);\n@@ -1009,7 +1238,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n     std::vector<Type*> arg_tys = {pred->getType(), ptr->getType()};\n     for(Value *v: others)\n         arg_tys.push_back(v->getType());\n-    if (has_l2_evict_policy) \n+    if (has_l2_evict_policy)\n       arg_tys.push_back(i64_ty);\n     FunctionType *asm_ty = FunctionType::get(ret_ty, arg_tys, false);\n     // ---\n@@ -1025,7 +1254,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n       asm_cstrt += \",\";\n       asm_cstrt += (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n     }\n-    if (has_l2_evict_policy) \n+    if (has_l2_evict_policy)\n       asm_cstrt += \",l\";\n     // ---\n     // finally call inline ASM\n@@ -1036,8 +1265,8 @@ void generator::visit_load_inst(ir::load_inst* x){\n         args.push_back(v);\n     if (has_l2_evict_policy)\n       args.push_back(policies_.at(x->get_eviction_policy()));\n-  \n-    \n+\n+\n     Value *_ret = call(inlineAsm, args);\n     // if(!op->get_type()->is_block_ty()){\n     //   Value* cond = icmp_eq(tid, i32(0));\n@@ -1050,7 +1279,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n     //   _ret = load(shptr);\n     //   add_barrier();\n     // }\n-      \n+\n     // ---\n     // extract and store return values\n     // ---\n@@ -1104,7 +1333,7 @@ void generator::visit_store_inst(ir::store_inst * x){\n     // vec  = std::min(nts, aln);\n     vec = std::min<size_t>(layout->contig_per_thread(ord[0]), aln);\n     // TODO: generalize\n-    bool is_mma_first_row = (ord.size() >= 1) && layout->to_mma() && \n+    bool is_mma_first_row = (ord.size() >= 1) && layout->to_mma() &&\n                        (a_axes_->get(ptr_op, ord[0]) == layouts_->get(ptr_op)->get_axis(1));\n     if(is_mma_first_row)\n       vec = std::min<size_t>(2, aln);\n@@ -1166,7 +1395,7 @@ void generator::visit_store_inst(ir::store_inst * x){\n     std::vector<Type*> arg_tys = {pred->getType(), ptr->getType()};\n     for(int ii = 0; ii < n_words; ii++)\n       arg_tys.push_back(val_arg_ty);\n-    if (has_l2_evict_policy) \n+    if (has_l2_evict_policy)\n       arg_tys.push_back(i64_ty);\n     FunctionType *asm_ty = FunctionType::get(builder_->getVoidTy(), arg_tys, false);\n     // ---\n@@ -1177,7 +1406,7 @@ void generator::visit_store_inst(ir::store_inst * x){\n       asm_cstrt += \",\";\n       asm_cstrt += (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n     }\n-    if (has_l2_evict_policy) \n+    if (has_l2_evict_policy)\n       asm_cstrt += \",l\";\n     // ---\n     // finally call inline ASM\n@@ -1817,13 +2046,13 @@ void generator::visit_mma884(ir::dot_inst* C, ir::value *A, ir::value *B, ir::va\n namespace {\n class mma16816_smem_loader {\n public:\n-  mma16816_smem_loader(int wpt, std::vector<int> order, int k_order, \n-                       std::vector<unsigned> tile_shape, \n-                       std::vector<int> instr_shape, std::vector<int> mat_shape, \n-                       int per_phase, int max_phase, int dtsize, Builder *builder, \n+  mma16816_smem_loader(int wpt, std::vector<int> order, int k_order,\n+                       std::vector<unsigned> tile_shape,\n+                       std::vector<int> instr_shape, std::vector<int> mat_shape,\n+                       int per_phase, int max_phase, int dtsize, Builder *builder,\n                        adder add, multiplier mul, geper gep)\n                       : wpt_(wpt), order_(order), k_order_(k_order), tile_shape_(tile_shape),\n-                        instr_shape_(instr_shape), mat_shape_(mat_shape), \n+                        instr_shape_(instr_shape), mat_shape_(mat_shape),\n                         per_phase_(per_phase), max_phase_(max_phase), dtsize_(dtsize), builder_(builder),\n                         add(add), mul(mul), gep(gep) {\n     // compute compile-time constant variables & types\n@@ -1837,7 +2066,7 @@ class mma16816_smem_loader {\n     need_trans_ = k_order_ != order_[0];\n     can_use_ldmatrix_ = dtsize == 2 || (!need_trans_);\n \n-    // we need more pointers at the fast-changing axis, \n+    // we need more pointers at the fast-changing axis,\n     if (can_use_ldmatrix_)\n       num_ptr_ = tile_shape[order[0]] / (order[0] == k_order? 1 : wpt) / instr_shape[order[0]];\n     else // warning: this only works for tf32 & need transpose\n@@ -1873,7 +2102,7 @@ class mma16816_smem_loader {\n       Value *s0 = urem(s, i32(2));\n       Value *s1 = udiv(s, i32(2));\n \n-      // We use different orders for a & b for better performance. \n+      // We use different orders for a & b for better performance.\n       Value *k_mat_arr  = (k_order_ == 1) ? s1 : s0;\n       Value *nk_mat_arr = (k_order_ == 1) ? s0 : s1;\n       mat_off[k_order_^1] = add(mul(warp_off,   i32(warp_off_stride_)),\n@@ -1884,7 +2113,7 @@ class mma16816_smem_loader {\n       Value *s_mat_off = mat_off[order_[1]];\n       // offset inside a matrix\n       Value *s_off_in_mat = c;\n-      \n+\n       std::vector<Value*> offs(num_ptr_);\n       Value *phase = urem(udiv(s_off_in_mat, i32(per_phase_)), i32(max_phase_));\n       // pre-compute strided offset\n@@ -1898,7 +2127,7 @@ class mma16816_smem_loader {\n     } else if (dtsize_ == 4 && need_trans_) {\n       // load tf32 matrices with lds32\n       Value *c_off_in_mat = udiv(lane, i32(4)); // 4 = mat_shape[order[1]]\n-      Value *s_off_in_mat = urem(lane, i32(4)); // \n+      Value *s_off_in_mat = urem(lane, i32(4)); //\n \n       Value *phase = urem(udiv(s_off_in_mat, i32(per_phase_)), i32(max_phase_));\n       std::vector<Value*> offs(num_ptr_);\n@@ -1945,7 +2174,7 @@ class mma16816_smem_loader {\n         Value *c_mat_off = add(mul(warp_off, i32(warp_off_stride_)),\n                                mul(nk_mat_arr, i32(mat_arr_stride_)));\n         Value *s_mat_off = k_mat_arr; // always 0?\n-        \n+\n         for (int loadx4_off = 0; loadx4_off < num_ptr_/8; ++loadx4_off) {\n           for (int elem_off = 0; elem_off < 4; ++elem_off) {\n             int ptr_off = loadx4_off*8 + nk_mat_arr_int*4 + elem_off;\n@@ -1971,10 +2200,10 @@ class mma16816_smem_loader {\n       throw std::runtime_error(\"invalid smem load config\");\n   }\n \n-  std::tuple<Value*, Value*, Value*, Value*> \n+  std::tuple<Value*, Value*, Value*, Value*>\n   load_x4(int mat0, int mat1, int inc, bool is_prefetch, ir::phi_node *pn,\n           Value *pre_ptr, Value *next_ptr, std::vector<Value*> &off, std::vector<Value*> &ptrs,\n-          FunctionType *ldmatrix_ty, Type *smem_ptr_ty, \n+          FunctionType *ldmatrix_ty, Type *smem_ptr_ty,\n           std::map<ir::value*, std::vector<Value*>> &prefetch_latch_to_bb_) {\n     assert(mat0 % 2 == 0 && mat1 % 2 == 0 && \"smem matrix load must be aligned\");\n     int mat_idx[2] = {mat0, mat1};\n@@ -2006,7 +2235,7 @@ class mma16816_smem_loader {\n       std::string trans = need_trans_ ? \".trans\" : \"\";\n       // the offset (in byte) on the strided axis is a constant\n       int s_offset = mat_idx[order_[1]] * (s_mat_stride_*s_mat_shape_) * s_stride_ * dtsize_;\n-      InlineAsm *ld_fn = InlineAsm::get(ldmatrix_ty, \n+      InlineAsm *ld_fn = InlineAsm::get(ldmatrix_ty,\n                                         \"ldmatrix.sync.aligned.m8n8.x4\" + trans + \".shared.b16 \"\n                                         \"{$0, $1, $2, $3}, \"\n                                         \"[$4 + \" + std::to_string(s_offset) + \"];\",\n@@ -2015,7 +2244,7 @@ class mma16816_smem_loader {\n       res_v4 = call(ldmatrix_ty, ld_fn, {ptr});\n       if (k == 0 && inc == 1 && is_prefetch)\n         prefetch_latch_to_bb_[pn->get_incoming_value(1)].push_back(res_v4);\n-      return {extract_val(res_v4, std::vector<unsigned>{0}), \n+      return {extract_val(res_v4, std::vector<unsigned>{0}),\n               extract_val(res_v4, std::vector<unsigned>{1}),\n               extract_val(res_v4, std::vector<unsigned>{2}),\n               extract_val(res_v4, std::vector<unsigned>{3})};\n@@ -2062,13 +2291,13 @@ class mma16816_smem_loader {\n       Value *i32_elems[4];\n       for (int i=0; i<4; ++i)\n         i8v4_elems[i] = UndefValue::get(vec_ty(i8_ty, 4));\n-      \n+\n       Value *elem00, *elem01, *elem02, *elem03;\n       Value *elem10, *elem11, *elem12, *elem13;\n       Value *elem20, *elem21, *elem22, *elem23;\n       Value *elem30, *elem31, *elem32, *elem33;\n       Value *i8_elems[4*4];\n-      if (k_order_ == 1) { // \n+      if (k_order_ == 1) { //\n         i8_elems[0*4 + 0] = load(gep(ptr00, i32(s_offset_elem)));\n         i8_elems[0*4 + 1] = load(gep(ptr01, i32(s_offset_elem)));\n         i8_elems[0*4 + 2] = load(gep(ptr02, i32(s_offset_elem)));\n@@ -2155,7 +2384,7 @@ class mma16816_smem_loader {\n   int s_mat_stride_;\n   // stride when moving to next not-k mat\n   int warp_off_stride_;\n-  int mat_arr_stride_; // matrix arrangement (inside a load) stride  \n+  int mat_arr_stride_; // matrix arrangement (inside a load) stride\n   bool need_trans_, can_use_ldmatrix_;\n   int num_ptr_;\n \n@@ -2232,7 +2461,7 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n     mma_ty = FunctionType::get(fp32_pack4_ty, std::vector<llvm::Type*>{fp16x2_ty, fp16x2_ty, fp16x2_ty, fp16x2_ty, fp16x2_ty, fp16x2_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty}, false);\n     smem_ptr_ty = ptr_ty(f16_ty, 3);\n     ldmatrix_ty = FunctionType::get(fp16x2_pack4_ty, std::vector<llvm::Type*>{smem_ptr_ty}, false);\n-    phi_ty = fp16x2_ty;    \n+    phi_ty = fp16x2_ty;\n   } else if (A_ir_ty->is_bf16_ty() && B_ir_ty->is_bf16_ty()) {\n     mma_ty = FunctionType::get(fp32_pack4_ty, std::vector<llvm::Type*>{bf16x2_ty, bf16x2_ty, bf16x2_ty, bf16x2_ty, bf16x2_ty, bf16x2_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty}, false);\n     smem_ptr_ty = ptr_ty(bf16_ty, 3);\n@@ -2303,8 +2532,8 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n   if(is_a_shared) {\n     const int per_phase_a = swizzle_->get_per_phase(layout_a);\n     const int max_phase_a = swizzle_->get_max_phase(layout_a);\n-    mma16816_smem_loader a_loader(layout->wpt(0), ord_a, /*k_order*/1, shape_a, \n-                                  {mma_instr_m, mma_instr_k}, {mat_shape_m, mat_shape_k}, \n+    mma16816_smem_loader a_loader(layout->wpt(0), ord_a, /*k_order*/1, shape_a,\n+                                  {mma_instr_m, mma_instr_k}, {mat_shape_m, mat_shape_k},\n                                   per_phase_a, max_phase_a, dtsize_a, builder_, add, mul, gep);\n     std::vector<Value*> off_a = a_loader.compute_offs(warp_m, lane);\n     int num_ptr_a = a_loader.get_num_ptr();\n@@ -2319,7 +2548,7 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n     // loading function\n     load_a = [&,a_loader,ptrs_a,off_a](int m, int k, int inc, bool is_prefetch) mutable {\n       auto [ha0, ha1, ha2, ha3] = a_loader.load_x4(m, k, inc, is_prefetch, phiA, shared_pre_ptr_[layout_a],\n-                                                  shared_next_ptr_[layout_a], off_a, ptrs_a, \n+                                                  shared_next_ptr_[layout_a], off_a, ptrs_a,\n                                                   ldmatrix_ty, smem_ptr_ty, prefetch_latch_to_bb_);\n       register_lds2(ha, m,   k,   inc, ha0, is_prefetch);\n       register_lds2(ha, m+1, k,   inc, ha1, is_prefetch);\n@@ -2389,12 +2618,12 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n   for(int i = 0; i < num_ptr_b; i++)\n     ptrs_b[i] = bit_cast(gep(shmems_[B], {off_b[i]}), smem_ptr_ty);\n \n-    \n+\n   // loading function\n   std::function<void(int,int,int,bool)> load_b;\n   load_b = [&](int n, int k, int inc, bool is_prefetch) {\n       auto [hb0, hb1, hb2, hb3] = b_loader.load_x4(k, n, inc, is_prefetch, phiB, shared_pre_ptr_[layout_b],\n-                                                   shared_next_ptr_[layout_b], off_b, ptrs_b, \n+                                                   shared_next_ptr_[layout_b], off_b, ptrs_b,\n                                                    ldmatrix_ty, smem_ptr_ty, prefetch_latch_to_bb_);\n       register_lds2(hb, n,   k,   inc, hb0, is_prefetch);\n       register_lds2(hb, n+1, k,   inc, hb2, is_prefetch);\n@@ -2419,7 +2648,7 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n         (m + 1)*cols_per_thread + (n*2 + 0),\n         (m + 1)*cols_per_thread + (n*2 + 1)\n       };\n-      Value *nc = call(mma_ty, mma_fn, \n+      Value *nc = call(mma_ty, mma_fn,\n                        {ha[{m, k}], ha[{m+1, k}], ha[{m, k+1}], ha[{m+1, k+1}],\n                         hb[{n, k}], hb[{n, k+1}],\n                         fc[idx[0]], fc[idx[1]], fc[idx[2]], fc[idx[3]]});\n@@ -2608,7 +2837,7 @@ void generator::visit_dot_inst(ir::dot_inst* dot) {\n     return visit_mma884(dot, A, B, D, NK);\n   if(!is_outer && is_mma && tgt_->as_nvidia()->sm() >= 80)\n     return visit_mma16816(dot, A, B, D, NK); // rename it as visit_mma_v2()?\n-  if (dot->get_type()->get_scalar_ty()->is_fp32_ty() && \n+  if (dot->get_type()->get_scalar_ty()->is_fp32_ty() &&\n       A->get_type()->get_scalar_ty()->is_fp32_ty())\n     return visit_fmadot(dot, A, B, D, NK, c_ty, f_mul_add);\n   throw std::runtime_error(\"dot has invalid operand type\");\n@@ -2710,7 +2939,7 @@ void generator::visit_reducend_inst_fast(ir::reduce_inst* x, acc_fn_t do_acc, Va\n     warps_per_inner = layout->to_mma()->wpt(1);\n     col_per_thread = axes_.at(a_axes_->get(arg, 1)).values.size();\n     warp_j = axes_.at(a_axes_->get(arg, 1)).thread_id;\n-  } \n+  }\n   assert(warp_j != nullptr);\n \n   // unsigned col_per_thread = 2 * shapes[order[0]] / layout->shape_per_cta(order[0]);\n@@ -3367,7 +3596,7 @@ void generator::visit_constant_fp(ir::constant_fp *x){\n     if (x->get_type()->get_scalar_ty()->is_bf16_ty()) {\n       // highest 16 bits of fp32\n       float fp32_value = x->get_value();\n-      uint16_t bf16_raw = (*reinterpret_cast<uint32_t*>(&fp32_value) \n+      uint16_t bf16_raw = (*reinterpret_cast<uint32_t*>(&fp32_value)\n                             & 0xffff0000) >> 16;\n       std::stringstream const_str;\n       const_str << \"0x\" << std::hex << bf16_raw << \"U\"; // unsigned"}, {"filename": "lib/ir/builder.cc", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -120,6 +120,14 @@ value *builder::create_ret(value* val) {\n   return insert(return_inst::create(ctx_, val));\n }\n \n+//===----------------------------------------------------------------------===//\n+//                               dequantize instructions\n+//===----------------------------------------------------------------------===//\n+\n+value* builder::create_dequantize(value *src, value *scale, value *shift, type *dst_ty){\n+  return insert(dequantize_inst::create(src, scale, shift, dst_ty));\n+}\n+\n //===----------------------------------------------------------------------===//\n //                               cast instructions\n //===----------------------------------------------------------------------===//"}, {"filename": "lib/ir/instructions.cc", "status": "modified", "additions": 16, "deletions": 1, "changes": 17, "file_content_changes": "@@ -323,6 +323,21 @@ unary_inst::unary_inst(type *ty, value_id_t id, value *v, const std::string &nam\n   set_operand(0, v);\n }\n \n+//===----------------------------------------------------------------------===//\n+//                               dequantize_inst classes\n+//===----------------------------------------------------------------------===//\n+\n+dequantize_inst::dequantize_inst(type *ty, value *v, value *scale, value *shift, const std::string &name, instruction *next)\n+  : instruction(ty, INST_DEQUANTIZE, 3, name, next) {\n+  set_operand(0, v);\n+  set_operand(1, scale);\n+  set_operand(2, shift);\n+}\n+\n+dequantize_inst *dequantize_inst::create(value *arg, value *scale, value *shift, type *ty, const std::string &name, instruction *next){\n+  return new dequantize_inst(ty, arg, scale, shift, name, next);\n+}\n+\n //===----------------------------------------------------------------------===//\n //                               cast_inst classes\n //===----------------------------------------------------------------------===//\n@@ -584,7 +599,7 @@ masked_store_inst::masked_store_inst(value *ptr, value *val, value *mask, EVICTI\n   set_operand(2, mask);\n }\n \n-masked_store_inst* masked_store_inst::create(value *ptr, value *val, value *mask, EVICTION_POLICY eviction, \n+masked_store_inst* masked_store_inst::create(value *ptr, value *val, value *mask, EVICTION_POLICY eviction,\n                                              const std::string &name, instruction *next)  {\n   return new masked_store_inst(ptr, val, mask, eviction, name, next);\n }"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 27, "deletions": 25, "changes": 52, "file_content_changes": "@@ -83,8 +83,8 @@ void cu_enqueue(uint64_t stream, uint64_t kernel,\n       CU_LAUNCH_PARAM_BUFFER_SIZE,    &args_size,\n       CU_LAUNCH_PARAM_END\n   };\n-  drv::dispatch::cuLaunchKernel((CUfunction)kernel, grid_0, grid_1, grid_2, \n-                                block_0, block_1, block_2, \n+  drv::dispatch::cuLaunchKernel((CUfunction)kernel, grid_0, grid_1, grid_2,\n+                                block_0, block_1, block_2,\n                                 shared_mem, (CUstream)stream, nullptr, config);\n }\n \n@@ -97,8 +97,8 @@ void hip_enqueue(uint64_t stream, uint64_t kernel,\n       HIP_LAUNCH_PARAM_BUFFER_SIZE,    &args_size,\n       HIP_LAUNCH_PARAM_END\n   };\n-  drv::dispatch::hipModuleLaunchKernel((hipFunction_t)kernel, grid_0, grid_1, grid_2, \n-                                block_0, block_1, block_2, \n+  drv::dispatch::hipModuleLaunchKernel((hipFunction_t)kernel, grid_0, grid_1, grid_2,\n+                                block_0, block_1, block_2,\n                                 shared_mem, (hipStream_t)stream, nullptr, config);\n \n }\n@@ -302,8 +302,8 @@ void init_triton_runtime(py::module &&m) {\n \n \n   // cache key\n-  m.def(\"launch\", [](py::list args, py::list do_not_specialize, const std::string& func_key, py::list& arg_names, \n-                     py::object device, py::int_ stream, py::dict bin_cache, py::int_ num_warps, py::int_ num_stages, \n+  m.def(\"launch\", [](py::list args, py::list do_not_specialize, const std::string& func_key, py::list& arg_names,\n+                     py::object device, py::int_ stream, py::dict bin_cache, py::int_ num_warps, py::int_ num_stages,\n                      py::dict extern_libs, py::function add_to_cache, py::object grid){\n     // parse arguments to compute cache key, compile-time constants and packed kernel arguments\n     long _num_warps = PyLong_AsLong(num_warps.ptr());\n@@ -351,8 +351,8 @@ void init_triton_runtime(py::module &&m) {\n       // release the gil in case the enqueue blocks\n       // cuda will block if too many ops are enqueued\n       py::gil_scoped_release allow_threads;\n-      drv::dispatch::cuLaunchKernel((CUfunction)kernel, grid_0, grid_1, grid_2, \n-                                    _num_warps*32, 1, 1, shared_mem, (CUstream)_stream, \n+      drv::dispatch::cuLaunchKernel((CUfunction)kernel, grid_0, grid_1, grid_2,\n+                                    _num_warps*32, 1, 1, shared_mem, (CUstream)_stream,\n                                      nullptr, config);\n    }\n     return bin;\n@@ -372,7 +372,7 @@ void init_triton_runtime(py::module &&m) {\n   m.def(\"max_shared_memory\", [](backend_t backend, uint64_t device) {\n       if (backend == HOST)\n         return 0;\n-      if(backend == CUDA) \n+      if(backend == CUDA)\n         return cuGetInfo<CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN>(device);\n       if(backend == ROCM)\n         return hipGetInfo<hipDeviceAttributeMaxSharedMemoryPerBlock>(device);\n@@ -422,17 +422,17 @@ void init_triton_runtime(py::module &&m) {\n       hip_enqueue(stream, kernel, grid_0, grid_1, grid_2, block_0, block_1, block_2, args_ptr, args_size, shared_mem);\n   });\n \n-  \n+\n }\n \n /*****************************************************************************/\n /* Python bindings for triton::codegen                                       */\n /*****************************************************************************/\n typedef std::map<std::string, py::object> asm_map_t;\n \n-// --------------------------------------- \n+// ---------------------------------------\n // Compile Triton-IR to assembly\n-// --------------------------------------- \n+// ---------------------------------------\n \n void init_triton_codegen(py::module &&m) {\n   m.def(\"compile_ttir\",\n@@ -550,13 +550,13 @@ void init_triton_ir(py::module &&m) {\n       .value(\"CA\", ir::load_inst::CA)\n       .value(\"CG\", ir::load_inst::CG)\n       .export_values();\n-  \n+\n   py::enum_<ir::load_inst::EVICTION_POLICY>(m, \"EVICTION_POLICY\")\n       .value(\"NORMAL\", ir::load_inst::NORMAL)\n       .value(\"EVICT_FIRST\", ir::load_inst::EVICT_FIRST)\n       .value(\"EVICT_LAST\", ir::load_inst::EVICT_LAST)\n       .export_values();\n-  \n+\n   py::enum_<ir::reduce_inst::op_t>(m, \"REDUCE_OP\")\n       .value(\"ADD\", ir::reduce_inst::ADD)\n       .value(\"FADD\", ir::reduce_inst::FADD)\n@@ -573,7 +573,7 @@ void init_triton_ir(py::module &&m) {\n       .value(\"ARGFMIN\", ir::reduce_inst::ARGFMIN)\n       .value(\"ARGFMAX\", ir::reduce_inst::ARGFMAX)\n       .value(\"XOR\", ir::reduce_inst::XOR);\n-  \n+\n   py::enum_<ir::atomic_rmw_op_t>(m, \"ATOMIC_OP\")\n       .value(\"ADD\", ir::atomic_rmw_op_t::Add)\n       .value(\"FADD\", ir::atomic_rmw_op_t::FAdd)\n@@ -704,7 +704,7 @@ void init_triton_ir(py::module &&m) {\n \n   py::class_<ir::function_type, ir::type>(m, \"function_type\")\n       .def_property_readonly(\"ret_ty\", &ir::function_type::get_return_ty)\n-      .def_property_readonly(\"arg_tys\", [](ir::function_type* self){ \n+      .def_property_readonly(\"arg_tys\", [](ir::function_type* self){\n         return std::vector<ir::type*>(self->params_begin(), self->params_end());\n       });\n \n@@ -713,7 +713,7 @@ void init_triton_ir(py::module &&m) {\n   py::class_<ir::block_type, ir::type>(m, \"block_type\")\n       .def_property_readonly(\"shape\", &ir::block_type::get_shapes)\n       .def_property_readonly(\"numel\", &ir::type::get_tile_num_elements);\n-  \n+\n   py::class_<ir::struct_type, ir::type>(m, \"struct_type\")\n       .def(\"get\", &ir::struct_type::get, ret::reference)\n       .def_property_readonly(\"num_types\", &ir::struct_type::get_num_types);\n@@ -834,6 +834,8 @@ void init_triton_ir(py::module &&m) {\n       .def(\"create_br\", &ir::builder::create_br, ret::reference)\n       .def(\"create_cond_br\", &ir::builder::create_cond_br, ret::reference)\n       .def(\"create_ret_void\", &ir::builder::create_ret_void, ret::reference)\n+      // Dequantize instructions\n+      .def(\"create_dequantize\", &ir::builder::create_dequantize, ret::reference)\n       // Cast instructions\n       .def(\"create_bitcast\", &ir::builder::create_bitcast, ret::reference)\n       .def(\"create_cast\", &ir::builder::create_cast, ret::reference)\n@@ -857,27 +859,27 @@ void init_triton_ir(py::module &&m) {\n       .def(\"create_frem\", &ir::builder::create_frem, ret::reference)\n       .def(\"create_fadd\", &ir::builder::create_fadd, ret::reference)\n       .def(\"create_fsub\", &ir::builder::create_fsub, ret::reference)\n-      .def(\"create_mul\", &ir::builder::create_mul, ret::reference, \n-                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+      .def(\"create_mul\", &ir::builder::create_mul, ret::reference,\n+                          py::arg(\"lhs\"), py::arg(\"rhs\"),\n                           py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n       .def(\"create_sdiv\", &ir::builder::create_sdiv, ret::reference)\n       .def(\"create_udiv\", &ir::builder::create_udiv, ret::reference)\n       .def(\"create_srem\", &ir::builder::create_srem, ret::reference)\n       .def(\"create_urem\", &ir::builder::create_urem, ret::reference)\n-      .def(\"create_add\", &ir::builder::create_add, ret::reference, \n-                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+      .def(\"create_add\", &ir::builder::create_add, ret::reference,\n+                          py::arg(\"lhs\"), py::arg(\"rhs\"),\n                           py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n       .def(\"create_sub\", &ir::builder::create_sub, ret::reference,\n-                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+                          py::arg(\"lhs\"), py::arg(\"rhs\"),\n                           py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n       .def(\"create_shl\", &ir::builder::create_shl, ret::reference,\n-                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+                          py::arg(\"lhs\"), py::arg(\"rhs\"),\n                           py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n       .def(\"create_lshr\", &ir::builder::create_lshr, ret::reference,\n-                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+                          py::arg(\"lhs\"), py::arg(\"rhs\"),\n                           py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n       .def(\"create_ashr\", &ir::builder::create_ashr, ret::reference,\n-                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+                          py::arg(\"lhs\"), py::arg(\"rhs\"),\n                           py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n       // GEP\n       .def(\"create_gep\", &ir::builder::create_gep, ret::reference)"}, {"filename": "python/test/unit/language/test_dequantize.py", "status": "added", "additions": 261, "deletions": 0, "changes": 261, "file_content_changes": "@@ -0,0 +1,261 @@\n+# flake8: noqa: F821,F841\n+\n+import random\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def dequantize_kernel_int8(output_ptr, input_ptr, size, BLOCK_SIZE: tl.constexpr):\n+    w_offsets = tl.arange(0, BLOCK_SIZE // 4)\n+    mask = w_offsets < (size // 4)\n+    input_ptrs = input_ptr + 1 + w_offsets\n+    input = tl.load(input_ptrs, mask=mask, other=0)\n+    scale_shift = tl.load(input_ptr)\n+    scale = (scale_shift & 65535).to(tl.int16).to(tl.float16, bitcast=True)\n+    shift = (scale_shift >> 16).to(tl.int16).to(tl.float16, bitcast=True)\n+    output = tl.dequantize(input, scale, shift, 8)\n+    offsets = tl.arange(0, BLOCK_SIZE)\n+    output_ptrs = tl.multiple_of(output_ptr + offsets, 4)\n+    tl.store(output_ptrs, output, mask=offsets < size)\n+\n+\n+@triton.jit\n+def dequantize_kernel_scale_shift_int8(\n+    output_ptr, input_ptr, scale_ptr, shift_ptr, size, BLOCK_SIZE: tl.constexpr\n+):\n+    w_offsets = tl.arange(0, BLOCK_SIZE // 4)\n+    mask = w_offsets < (size // 4)\n+    input_ptrs = tl.multiple_of(input_ptr + w_offsets, 1)\n+    input = tl.load(input_ptrs, mask=mask, other=0)\n+    scale = tl.load(scale_ptr)\n+    shift = tl.load(shift_ptr)\n+    output = tl.dequantize(input, scale, shift, 8)\n+    offsets = tl.arange(0, BLOCK_SIZE)\n+    output_ptrs = tl.multiple_of(output_ptr + offsets, 4)\n+    tl.store(output_ptrs, output, mask=offsets < size)\n+\n+\n+@triton.jit\n+def dequantize_kernel_int4(output_ptr, input_ptr, size, BLOCK_SIZE: tl.constexpr):\n+    w_offsets = tl.arange(0, BLOCK_SIZE // 8)\n+    mask = w_offsets < (size // 8)\n+    input_ptrs = input_ptr + 1 + w_offsets\n+    input = tl.load(input_ptrs, mask=mask, other=0)\n+    scale_shift = tl.load(input_ptr)\n+    scale = (scale_shift & 65535).to(tl.int16).to(tl.float16, bitcast=True)\n+    shift = (scale_shift >> 16).to(tl.int16).to(tl.float16, bitcast=True)\n+    output = tl.dequantize(input, scale, shift, 4)\n+    offsets = tl.arange(0, BLOCK_SIZE)\n+    output_ptrs = tl.multiple_of(output_ptr + offsets, 8)\n+    tl.store(output_ptrs, output, mask=offsets < size)\n+\n+\n+@triton.jit\n+def dequantize_kernel_scale_shift_int4(\n+    output_ptr, input_ptr, scale_ptr, shift_ptr, size, BLOCK_SIZE: tl.constexpr\n+):\n+    w_offsets = tl.arange(0, BLOCK_SIZE // 8)\n+    mask = w_offsets < (size // 8)\n+    input_ptrs = tl.multiple_of(input_ptr + w_offsets, 1)\n+    input = tl.load(input_ptrs, mask=mask, other=0)\n+    scale = tl.load(scale_ptr)\n+    shift = tl.load(shift_ptr)\n+    output = tl.dequantize(input, scale, shift, 4)\n+    offsets = tl.arange(0, BLOCK_SIZE)\n+    output_ptrs = tl.multiple_of(output_ptr + offsets, 8)\n+    tl.store(output_ptrs, output, mask=offsets < size)\n+\n+\n+@triton.jit\n+def dequantize_kernel_int2(output_ptr, input_ptr, size, BLOCK_SIZE: tl.constexpr):\n+    w_offsets = tl.arange(0, BLOCK_SIZE // 8)\n+    mask = w_offsets < (size // 8)\n+    input_ptrs = tl.multiple_of(input_ptr + 2 + w_offsets, 1)\n+    input = tl.load(input_ptrs, mask=mask, other=0)\n+    scale = tl.load(input_ptr).to(tl.float16, bitcast=True)\n+    shift = tl.load(input_ptr + 1).to(tl.float16, bitcast=True)\n+    output = tl.dequantize(input, scale, shift, 2)\n+    offsets = tl.arange(0, BLOCK_SIZE)\n+    output_ptrs = tl.multiple_of(output_ptr + offsets, 8)\n+    tl.store(output_ptrs, output, mask=offsets < size)\n+\n+\n+@triton.jit\n+def dequantize_kernel_scale_shift_int2(\n+    output_ptr, input_ptr, scale_ptr, shift_ptr, size, BLOCK_SIZE: tl.constexpr\n+):\n+    w_offsets = tl.arange(0, BLOCK_SIZE // 8)\n+    mask = w_offsets < (size // 8)\n+    input_ptrs = tl.multiple_of(input_ptr + w_offsets, 1)\n+    input = tl.load(input_ptrs, mask=mask, other=0)\n+    scale = tl.load(scale_ptr)\n+    shift = tl.load(shift_ptr)\n+    output = tl.dequantize(input, scale, shift, 2)\n+    offsets = tl.arange(0, BLOCK_SIZE)\n+    output_ptrs = tl.multiple_of(output_ptr + offsets, 8)\n+    tl.store(output_ptrs, output, mask=offsets < size)\n+\n+\n+def test_dequantize_int8() -> None:\n+    for i in range(10):\n+        if i < 5:\n+            size = random.randrange(16, 128, 4)\n+        else:\n+            size = random.randrange(132, 1024, 4)\n+        device = torch.device(torch.cuda.current_device())\n+\n+        scale_val = random.uniform(0.1, 4.0)\n+        shift_val = random.uniform(-10.0, 10.0)\n+        scale = torch.tensor(scale_val, dtype=torch.float16, device=device)\n+        shift = torch.tensor(shift_val, dtype=torch.float16, device=device)\n+        scale_shift = torch.tensor(\n+            [scale_val, shift_val],\n+            dtype=torch.float16,\n+            device=device,\n+        ).view(torch.int32)\n+\n+        input_int8 = torch.randint(\n+            0, 256, (size,), dtype=torch.uint8, device=device\n+        )\n+        input_int32 = input_int8.view(torch.int32)\n+\n+        input = torch.cat((scale_shift, input_int32))\n+        expected = (input_int8 * scale + shift).to(torch.float16)\n+\n+        output = torch.empty([size], dtype=torch.float16, device=device)\n+        block_size = max(triton.next_power_of_2(size), 128)\n+        grid = (1,)\n+        dequantize_kernel_int8[grid](\n+            output, input, size, BLOCK_SIZE=block_size, num_warps=1\n+        )\n+        rtol, atol = 1e-02, 1e-02\n+        assert torch.allclose(output, expected, rtol, atol)\n+\n+        output = torch.empty([size], dtype=torch.float16, device=device)\n+        dequantize_kernel_scale_shift_int8[grid](\n+            output,\n+            input_int32,\n+            scale,\n+            shift,\n+            size,\n+            BLOCK_SIZE=block_size,\n+            num_warps=1,\n+        )\n+        assert torch.allclose(output, expected, rtol, atol)\n+\n+\n+def test_dequantize_int4() -> None:\n+    for i in range(10):\n+        if i < 5:\n+            size = random.randrange(16, 256, 8)\n+        else:\n+            size = random.randrange(264, 1024, 8)\n+        device = torch.device(torch.cuda.current_device())\n+\n+        scale_val = random.uniform(0.1, 4.0)\n+        shift_val = random.uniform(-10.0, 10.0)\n+        scale = torch.tensor(scale_val, dtype=torch.float16, device=device)\n+        shift = torch.tensor(shift_val, dtype=torch.float16, device=device)\n+        scale_shift = torch.tensor(\n+            [scale_val, shift_val],\n+            dtype=torch.float16,\n+            device=device,\n+        ).view(torch.int32)\n+\n+        input_int8 = torch.randint(\n+            0, 256, (size // 2,), dtype=torch.uint8, device=device\n+        )\n+        input_int32 = input_int8.view(torch.int32)\n+\n+        input_int8_h1 = input_int8 >> 4\n+        input_int8_h0 = input_int8 & 15\n+\n+        input_int4_val = torch.stack(\n+            (input_int8_h0, input_int8_h1), dim=1\n+        ).flatten()\n+\n+        input = torch.cat((scale_shift, input_int32))\n+        expected = (input_int4_val * scale + shift).to(torch.float16)\n+\n+        output = torch.empty([size], dtype=torch.float16, device=device)\n+        block_size = max(triton.next_power_of_2(size), 256)\n+        grid = (1,)\n+        dequantize_kernel_int4[grid](\n+            output, input, size, BLOCK_SIZE=block_size, num_warps=1\n+        )\n+        rtol, atol = 1e-02, 1e-02\n+        assert torch.allclose(output, expected, rtol, atol)\n+\n+        output = torch.empty([size], dtype=torch.float16, device=device)\n+        dequantize_kernel_scale_shift_int4[grid](\n+            output,\n+            input_int32,\n+            scale,\n+            shift,\n+            size,\n+            BLOCK_SIZE=block_size,\n+            num_warps=1,\n+        )\n+        assert torch.allclose(output, expected, rtol, atol)\n+\n+\n+def test_dequantize_int2() -> None:\n+    for i in range(10):\n+        if i < 5:\n+            size = random.randrange(16, 256, 8)\n+        else:\n+            size = random.randrange(264, 1024, 8)\n+        device = torch.device(torch.cuda.current_device())\n+\n+        scale_val = random.uniform(0.1, 4.0)\n+        shift_val = random.uniform(-10.0, 10.0)\n+        scale = torch.tensor(scale_val, dtype=torch.float16, device=device)\n+        shift = torch.tensor(shift_val, dtype=torch.float16, device=device)\n+        scale_shift = torch.tensor(\n+            [scale_val, shift_val],\n+            dtype=torch.float16,\n+            device=device,\n+        ).view(torch.int16)\n+\n+        input_int8 = torch.randint(\n+            0, 256, (size // 4,), dtype=torch.uint8, device=device\n+        )\n+        input_int16 = input_int8.view(torch.int16)\n+\n+        input_int8_q3 = input_int8 >> 6\n+        input_int8_q2 = (input_int8 >> 4) & 3\n+        input_int8_q1 = (input_int8 >> 2) & 3\n+        input_int8_q0 = input_int8 & 3\n+\n+        input_int2_val = torch.stack(\n+            (input_int8_q0, input_int8_q1, input_int8_q2, input_int8_q3), dim=1\n+        ).flatten()\n+\n+        input = torch.cat((scale_shift, input_int16))\n+        expected = (input_int2_val * scale + shift).to(torch.float16)\n+\n+        output = torch.empty([size], dtype=torch.float16, device=device)\n+        block_size = max(triton.next_power_of_2(size), 256)\n+        grid = (1,)\n+\n+        dequantize_kernel_int2[grid](\n+            output, input, size, BLOCK_SIZE=block_size, num_warps=1\n+        )\n+        rtol, atol = 1e-02, 1e-02\n+        assert torch.allclose(output, expected, rtol, atol)\n+\n+        output = torch.empty([size], dtype=torch.float16, device=device)\n+        dequantize_kernel_scale_shift_int2[grid](\n+            output,\n+            input_int16,\n+            scale,\n+            shift,\n+            size,\n+            BLOCK_SIZE=block_size,\n+            num_warps=1,\n+        )\n+        assert torch.allclose(output, expected, rtol, atol)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -685,6 +685,20 @@ def zeros(shape, dtype, _builder=None):\n     return semantic.zeros(shape, dtype, _builder)\n \n \n+# -----------------------\n+# dequantize\n+# -----------------------\n+\n+\n+@builtin\n+def dequantize(input, scale, shift, nbit, dst_ty=float16, _builder=None):\n+    \"\"\"\n+    Tries to dequantize the input to given dtype\n+    \"\"\"\n+    nbit = _constexpr_to_value(nbit)\n+    return semantic.dequantize(input, scale, shift, nbit, dst_ty, _builder)\n+\n+\n # -----------------------\n # Shape Manipulation\n # -----------------------"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 26, "deletions": 0, "changes": 26, "file_content_changes": "@@ -544,6 +544,32 @@ def broadcast_impl_value(lhs: tl.tensor,\n     # (scalar, scalar) => returns original blocks\n     return lhs, rhs\n \n+\n+#######\n+# dequantize\n+#######\n+\n+def dequantize(input: tl.tensor,\n+         scale: tl.tensor,\n+         shift: tl.tensor,\n+         nbit: int,\n+         dst_ty: tl.dtype,\n+         builder: ir.builder,\n+) -> tl.tensor:\n+    input_ty = input.type\n+    assert input_ty.is_block()\n+    assert input_ty.element_ty.is_int32() or input_ty.element_ty.is_int16()\n+    assert nbit in [2, 4, 8]\n+    assert dst_ty == tl.float16\n+\n+    shape = input_ty.get_block_shapes()\n+    factor = input_ty.element_ty.primitive_bitwidth // nbit\n+    dst_shape = shape[:-1] + [factor * shape[-1]]\n+\n+    dst_ty = tl.block_type(dst_ty, dst_shape)\n+    return tl.tensor(builder.create_dequantize(input.handle, scale.handle, shift.handle, dst_ty.to_ir(builder)), dst_ty)\n+\n+\n #######\n # cast\n #######"}]