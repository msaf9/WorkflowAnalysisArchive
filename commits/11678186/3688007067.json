[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -3336,9 +3336,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n           \"Unsupported MMA kind found when converting DotOp to LLVM.\");\n     }\n \n-    if (op.getType().cast<RankedTensorType>().getElementType().isF32() &&\n-        A.getType().cast<RankedTensorType>().getElementType().isF32() &&\n-        !op.allowTF32())\n+    if (D.getType().cast<RankedTensorType>().getEncoding().isa<BlockedEncodingAttr>())\n       return convertFMADot(op, adaptor, rewriter);\n \n     llvm::report_fatal_error("}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -794,7 +794,7 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto B = dotOp.getOperand(1).getType().cast<RankedTensorType>();\n     // for FMA, should retain the blocked layout.\n     if (A.getElementType().isF32() && B.getElementType().isF32() &&\n-        !dotOp.allowTF32())\n+        (computeCapability < 75 || !dotOp.allowTF32()))\n       return failure();\n \n     // get MMA encoding for the given number of warps"}, {"filename": "python/test/unit/operators/test_blocksparse.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -10,7 +10,7 @@\n @pytest.mark.parametrize(\"TRANS_B\", [False, True])\n @pytest.mark.parametrize(\"BLOCK\", [16, 32, 64])\n # TODO: float32 fails\n-@pytest.mark.parametrize(\"DTYPE\", [torch.float16])\n+@pytest.mark.parametrize(\"DTYPE\", [torch.float32])\n def test_matmul(MODE, TRANS_A, TRANS_B, BLOCK, DTYPE, Z=3, H=2, M=512, N=384, K=256):\n     seed = 0\n     torch.manual_seed(seed)"}]