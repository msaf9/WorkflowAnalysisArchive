[{"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 0, "deletions": 22, "changes": 22, "file_content_changes": "@@ -150,25 +150,3 @@ def kernel(X, c: tl.constexpr):\n     except BaseException:\n         error = True\n     assert error is True\n-\n-\n-def test_jit_warmup_cache() -> None:\n-    @triton.jit\n-    def kernel_add(a, b, o, N: tl.constexpr):\n-        idx = tl.arange(0, N)\n-        tl.store(o + idx,\n-                 tl.load(a + idx) + tl.load(b + idx))\n-\n-    args = [\n-        torch.randn(32, dtype=torch.float32, device=\"cuda\"),\n-        torch.randn(32, dtype=torch.float32, device=\"cuda\"),\n-        torch.randn(32, dtype=torch.float32, device=\"cuda\"),\n-        32,\n-    ]\n-    assert len(kernel_add.cache) == 0\n-    kernel_add[(1,)].warmup(torch.float32, torch.float32, torch.float32, 32)\n-    assert len(kernel_add.cache) == 1\n-    kernel_add[(1,)].warmup(*args)\n-    assert len(kernel_add.cache) == 1\n-    kernel_add[(1,)](*args)\n-    assert len(kernel_add.cache) == 1"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 10, "deletions": 30, "changes": 40, "file_content_changes": "@@ -68,7 +68,16 @@ def run(self, *args, **kwargs):\n             key = tuple([args[i] for i in self.key_idx])\n             if key not in self.cache:\n                 # prune configs\n-                pruned_configs = self.prune_configs(kwargs)\n+                pruned_configs = self.configs\n+                if self.early_config_prune:\n+                    pruned_configs = self.early_config_prune(self.configs, self.nargs)\n+                if self.perf_model:\n+                    top_k = self.configs_top_k\n+                    if isinstance(top_k, float) and top_k <= 1.0:\n+                        top_k = int(len(self.configs) * top_k)\n+                    if len(pruned_configs) > top_k:\n+                        est_timing = {config: self.perf_model(**self.nargs, **kwargs, **config.kwargs, num_stages=config.num_stages, num_warps=config.num_warps) for config in pruned_configs}\n+                        pruned_configs = sorted(est_timing.keys(), key=lambda x: est_timing[x])[:top_k]\n                 bench_start = time.time()\n                 timings = {config: self._bench(*args, config=config, **kwargs)\n                            for config in pruned_configs}\n@@ -85,35 +94,6 @@ def run(self, *args, **kwargs):\n             config.pre_hook(self.nargs)\n         return self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n \n-    def prune_configs(self, kwargs):\n-        pruned_configs = self.configs\n-        if self.early_config_prune:\n-            pruned_configs = self.early_config_prune(self.configs, self.nargs)\n-        if self.perf_model:\n-            top_k = self.configs_top_k\n-            if isinstance(top_k, float) and top_k <= 1.0:\n-                top_k = int(len(self.configs) * top_k)\n-            if len(pruned_configs) > top_k:\n-                est_timing = {\n-                    config: self.perf_model(**self.nargs, **kwargs, **config.kwargs, num_stages=config.num_stages,\n-                                            num_warps=config.num_warps)\n-                    for config in pruned_configs\n-                }\n-                pruned_configs = sorted(est_timing.keys(), key=lambda x: est_timing[x])[:top_k]\n-        return pruned_configs\n-\n-    def warmup(self, *args, **kwargs):\n-        self.nargs = dict(zip(self.arg_names, args))\n-        for config in self.prune_configs(kwargs):\n-            self.fn.warmup(\n-                *args,\n-                num_warps=config.num_warps,\n-                num_stages=config.num_stages,\n-                **kwargs,\n-                **config.kwargs,\n-            )\n-        self.nargs = None\n-\n \n class Config:\n     \"\"\""}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 9, "deletions": 20, "changes": 29, "file_content_changes": "@@ -12,7 +12,6 @@\n import torch\n \n import triton\n-from triton.utils import MockTensor\n \n try:\n     from torch._C import _cuda_getCurrentRawStream as get_cuda_stream\n@@ -102,16 +101,9 @@ def __getitem__(self, grid):\n         Hence JITFunction.__getitem__ returns a callable proxy that\n         memorizes the grid.\n         \"\"\"\n-        class Launcher:\n-            @staticmethod\n-            def __call__(*args, **kwargs):\n-                return self.run(*args, grid=grid, **kwargs)\n-\n-            @staticmethod\n-            def warmup(*args, **kwargs):\n-                return self.warmup(*args, grid=grid, **kwargs)\n-\n-        return Launcher()\n+        def launcher(*args, **kwargs):\n+            return self.run(*args, grid=grid, **kwargs)\n+        return launcher\n \n \n class JITFunction(KernelInterface):\n@@ -239,7 +231,7 @@ def _make_launcher(self):\n         grid_args = ','.join([f'\"{arg}\": {arg}' for arg in self.arg_names])\n \n         src = f\"\"\"\n-def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False):\n+def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None):\n     sig_key =  {sig_keys},\n     constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else tuple()}\n     spec_key = {f'{spec_keys},' if len(spec_keys) > 0 else tuple()}\n@@ -255,12 +247,11 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     grid_2 = grid[2] if grid_size > 2 else 1\n     device = torch.cuda.current_device()\n     torch.cuda.set_device(device)\n-    if stream is None and not warmup:\n+    if stream is None:\n       stream = get_cuda_stream(device)\n     try:\n       bin = cache[key]\n-      if not warmup:\n-          bin.c_wrapper(grid_0, grid_1, grid_2, stream, {args})\n+      bin.c_wrapper(grid_0, grid_1, grid_2, stream, {args})\n       return bin\n     # kernel not cached -- compile\n     except KeyError:\n@@ -280,8 +271,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n       device = 0\n       if not self._call_hook(key, signature, device, constants, num_warps, num_stages, extern_libs, configs):\n         bin = triton.compile(self, signature, device, constants, num_warps, num_stages, extern_libs=extern_libs, configs=configs)\n-        if not warmup:\n-            bin.c_wrapper(grid_0, grid_1, grid_2, stream, *args)\n+        bin.c_wrapper(grid_0, grid_1, grid_2, stream, *args)\n         self.cache[key] = bin\n         return bin\n       return None\n@@ -327,6 +317,7 @@ def __init__(self, fn, version=None, do_not_specialize=None):\n         self.__module__ = fn.__module__\n \n     @property\n+    @functools.lru_cache()\n     def cache_key(self):\n         # TODO : hash should be attribute of `self`\n         if self.hash is None:\n@@ -335,9 +326,6 @@ def cache_key(self):\n             self.hash = dependencies_finder.ret + version_key()\n         return self.hash\n \n-    def warmup(self, *args, **kwargs):\n-        return self.run(*map(MockTensor.wrap_dtype, args), **kwargs, warmup=True)\n-\n     # we do not parse `src` in the constructor because\n     # the user might want to monkey-patch self.src dynamically.\n     # Our unit tests do this, for example.\n@@ -361,6 +349,7 @@ def __setattr__(self, name, value):\n         #   to be reinitialized\n         if name == 'src':\n             self.hash = None\n+            JITFunction.cache_key.fget.cache_clear()\n \n     def __repr__(self):\n         return f\"JITFunction({self.module}:{self.fn.__name__})\""}, {"filename": "python/triton/utils.py", "status": "modified", "additions": 0, "deletions": 18, "changes": 18, "file_content_changes": "@@ -19,24 +19,6 @@ def next_power_of_2(n):\n     return n\n \n \n-class MockTensor:\n-    \"\"\"\n-    Can be used in place of real tensors when calling:\n-        kernel.warmup(MockTensor(torch.float32), ...)\n-    \"\"\"\n-    @staticmethod\n-    def wrap_dtype(arg):\n-        if isinstance(arg, torch.dtype):\n-            return MockTensor(arg)\n-        return arg\n-\n-    def __init__(self, dtype):\n-        self.dtype = dtype\n-\n-    def data_ptr(self):\n-        return 0  # optimistically assumes multiple of 16\n-\n-\n class TensorWrapper:\n     def __init__(self, base, dtype):\n         self.dtype = dtype"}]