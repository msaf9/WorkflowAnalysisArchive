[{"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -342,7 +342,7 @@ def backward(ctx, do):\n         dk = torch.empty_like(k)\n         dv = torch.empty_like(v)\n         delta = torch.empty_like(L)\n-        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n+        _bwd_preprocess[(cdiv(q.shape[2], BLOCK) * ctx.grid[1], )](\n             o, do,\n             delta,\n             BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,"}]