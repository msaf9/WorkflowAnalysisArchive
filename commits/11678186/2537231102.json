[{"filename": "include/triton/codegen/transform/cts.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -29,11 +29,11 @@ class cts {\n void add_copy(ir::instruction *parent, ir::value *x, ir::builder &builder, bool to_shared, std::map<ir::value*,ir::value*>& copies);\n \n public:\n-  cts(analysis::layouts* layouts, bool use_async = false): layouts_(layouts), use_async_(use_async) {}\n+  cts(analysis::layouts* layouts, bool has_sm80 = false): layouts_(layouts), has_sm80_(has_sm80) {}\n   void run(ir::module &mod);\n \n private:\n-  bool use_async_;\n+  bool has_sm80_;\n   analysis::layouts* layouts_;\n };\n "}, {"filename": "lib/codegen/pass.cc", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -31,15 +31,15 @@ std::unique_ptr<llvm::Module> add_passes_to_emit_bin(ir::module &ir, llvm::LLVMC\n   std::string name = ir.get_function_list()[0]->get_name();\n   std::unique_ptr<llvm::Module> llvm(new llvm::Module(name, ctx));\n   // optimizations\n-  bool cts_use_async = target->as_nvidia() && target->as_nvidia()->sm() >= 80;\n+  bool has_sm80 = target->as_nvidia() && target->as_nvidia()->sm() >= 80;\n   // create passes\n   codegen::analysis::align align;\n   codegen::transform::inliner inliner;\n   codegen::analysis::axes axes;\n-  codegen::transform::pipeline pipeline(cts_use_async, num_stages);\n+  codegen::transform::pipeline pipeline(has_sm80, num_stages);\n   codegen::transform::disassociate disassociate;\n   codegen::analysis::layouts layouts(&axes, &align, num_warps, target);\n-  codegen::transform::cts cts(&layouts, cts_use_async);\n+  codegen::transform::cts cts(&layouts, has_sm80);\n   codegen::analysis::liveness liveness(&layouts);\n   codegen::analysis::swizzle swizzle(&layouts, target);\n   codegen::analysis::allocation allocation(&liveness);"}, {"filename": "lib/codegen/transform/cts.cc", "status": "modified", "additions": 2, "deletions": 22, "changes": 24, "file_content_changes": "@@ -13,20 +13,8 @@ namespace transform{\n \n \n bool cts::is_shmem_op(ir::instruction* i, int op) {\n-  if(i->get_id() == ir::INST_DOT){\n-    // std::cout << i << \" \" << i->get_operand(0) << \" \" << layouts_->has(i->get_operand(0)) << \" \" << layouts_->has(i->get_operand(1)) << std::endl;\n-    // FP16 MMA layout can be kept in register for the LHS\n-    // Anything else has to be in shared memory\n-    // ir::value* lhs = i->get_operand(0);\n-    // if(op == 0){\n-      // i->print(std::cout);\n-    //   std::cout << layouts_->has(lhs) << std::endl;\n-    //   analysis::mma_layout* mma_lhs = layouts_->get(lhs)->to_mma();\n-    //   bool is_lhs_shmem = !(mma_lhs && lhs->get_type()->get_primitive_size_in_bits() == 16);\n-    //   // return is_lhs_shmem;\n-    // }\n+  if(i->get_id() == ir::INST_DOT)\n     return op == 0 || op == 1;\n-  }\n   if(i->get_id() == ir::INST_COPY_FROM_SHARED)\n     return op==0;\n   if(i->get_id() == ir::INST_TRANS)\n@@ -94,19 +82,11 @@ void cts::run(ir::module &mod) {\n       ir::type* ty = lhs->get_type()->get_scalar_ty();\n       analysis::mma_layout* mma_lhs = layouts_->get(lhs)->to_mma();\n       // TODO: V100\n-      bool is_lhs_shmem = !(mma_lhs && ty->get_primitive_size_in_bits() == 16 && !dot->is_trans_a());\n+      bool is_lhs_shmem = !(mma_lhs && has_sm80_ && ty->get_primitive_size_in_bits() == 16 && !dot->is_trans_a());\n       if(is_lhs_shmem)\n         shmem_ops.insert(lhs);\n       shmem_ops.insert(i->get_operand(1));\n     }\n-      // std::cout << i << \" \" << i->get_operand(0) << \" \" << layouts_->has(i->get_operand(0)) << \" \" << layouts_->has(i->get_operand(1)) << std::endl;\n-      // FP16 MMA layout can be kept in register for the LHS\n-      // Anything else has to be in shared memory\n-      // if(op == 0){\n-        // i->print(std::cout);\n-      //   std::cout << layouts_->has(lhs) << std::endl;\n-      //   // return is_lhs_shmem;\n-      // }\n     if(i->get_id() == ir::INST_COPY_FROM_SHARED)\n       shmem_ops.insert(i->get_operand(0));\n     if(i->get_id() == ir::INST_TRANS)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -913,8 +913,8 @@ def kernel(X, stride_xm, stride_xk,\n                          ADD_MATRIX=epilogue == 'add-matrix',\n                          ADD_ROWS=epilogue == 'add-rows',\n                          ADD_COLS=epilogue == 'add-cols',\n-                         DO_SOFTMAX = epilogue=='softmax',\n-                         CHAIN_DOT = epilogue=='chain-dot',\n+                         DO_SOFTMAX=epilogue == 'softmax',\n+                         CHAIN_DOT=epilogue == 'chain-dot',\n                          ALLOW_TF32=allow_tf32,\n                          num_warps=num_warps)\n     # torch result\n@@ -924,9 +924,9 @@ def kernel(X, stride_xm, stride_xk,\n     if epilogue == 'add-matrix':\n         z_ref += z\n     if epilogue == 'add-rows':\n-        z_ref += z[:,0][:, None]\n+        z_ref += z[:, 0][:, None]\n     if epilogue == 'add-cols':\n-        z_ref += z[0,:][None, :]\n+        z_ref += z[0, :][None, :]\n     if epilogue == 'softmax':\n         num = np.exp(z_ref - np.max(z_ref, axis=-1, keepdims=True))\n         denom = np.sum(num, axis=-1, keepdims=True)"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -647,6 +647,7 @@ def cast(input: tl.tensor,\n #                               Memory Operators\n # ===----------------------------------------------------------------------===//\n \n+\n def _parse_eviction_policy(eviction_policy):\n     eviction = ir.EVICTION_POLICY.NORMAL  # default\n     if eviction_policy:\n@@ -922,8 +923,8 @@ def dot(a: tl.tensor,\n     else:\n         _0 = builder.get_float32(0)\n         ret_scalar_ty = tl.float32\n-    M = a.type.shape[in_a^1]\n-    N = b.type.shape[in_b^1]\n+    M = a.type.shape[in_a ^ 1]\n+    N = b.type.shape[in_b ^ 1]\n     _0 = builder.create_splat(_0, [M, N])\n     ret_ty = tl.block_type(ret_scalar_ty, [M, N])\n     ret = builder.create_dot(a.handle, b.handle, _0, trans_a, trans_b, allow_tf32)"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 10, "deletions": 9, "changes": 19, "file_content_changes": "@@ -1,7 +1,8 @@\n+import pytest\n import torch\n+\n import triton\n import triton.language as tl\n-import pytest\n \n # fmt: off\n \n@@ -31,21 +32,21 @@ def _fwd_kernel(\n     q_ptrs = Q + (off_z * stride_qz \\\n                 + off_h * stride_qh \\\n                 + offs_qm[:, None] * stride_qm \\\n-                + offs_qk[None, :] * stride_qk)\n+                + offs_qk[None,:] * stride_qk)\n     # initialize pointers to K\n     offs_kk = tl.arange(0, BLOCK_DMODEL)\n     offs_kn = start_kn * BLOCK_N + tl.arange(0, BLOCK_N)\n     k_ptrs = K + (off_z * stride_kz \\\n                 + off_h * stride_kh \\\n-                + offs_kn[None, :] * stride_kn \\\n+                + offs_kn[None,:] * stride_kn \\\n                 + offs_kk[:, None] * stride_kk)\n     # initialize pointers to V\n     off_vk = tl.arange(0, BLOCK_N)\n     off_vn = tl.arange(0, BLOCK_DMODEL)\n     v_ptrs = V +  off_z * stride_vz \\\n                 + off_h * stride_vh \\\n                 + off_vk[:, None] * stride_vk \\\n-                + off_vn[None, :] * stride_vn\n+                + off_vn[None,:] * stride_vn\n     # initialize pointer to m and l\n     t_ptrs = TMP + off_hz*N_CTX + offs_qm\n \n@@ -59,7 +60,7 @@ def _fwd_kernel(\n         # -- compute qk ----\n         k = tl.load(k_ptrs)\n         qk = tl.dot(q, k)\n-        qk = tl.where(offs_qm[:, None] >= (start_n*BLOCK_N + offs_kn[None, :]), qk, float(\"-inf\"))\n+        qk = tl.where(offs_qm[:, None] >= (start_n*BLOCK_N + offs_kn[None,:]), qk, float(\"-inf\"))\n         # -- compute m_ij, p, l_ij\n         m_ij = tl.max(qk, 1)\n         p = tl.exp(qk - m_ij[:, None])\n@@ -84,7 +85,7 @@ def _fwd_kernel(\n         v_ptrs += BLOCK_N*stride_vk\n         l_i = l_i_new\n         m_i = m_i_new\n-    \n+\n     # write back l and m\n     l_ptrs = L   + off_hz*N_CTX + offs_qm\n     m_ptrs = M   + off_hz*N_CTX + offs_qm\n@@ -96,7 +97,7 @@ def _fwd_kernel(\n     out_ptrs = Out + off_z * stride_oz \\\n                  + off_h * stride_oh \\\n                  + offs_om[:, None] * stride_om \\\n-                 + offs_on[None, :] * stride_on\n+                 + offs_on[None,:] * stride_on\n     tl.store(out_ptrs, acc)\n \n \n@@ -130,7 +131,7 @@ def forward(ctx, q, k, v):\n         ctx.BLOCK = BLOCK\n         ctx.grid = grid\n         return o\n-    \n+\n attention = _attention.apply\n \n @pytest.mark.parametrize('Z, H, N_CTX, D_MODEL', [(2, 3, 1024, 64)])\n@@ -146,7 +147,7 @@ def test_op(Z, H, N_CTX, D_MODEL, dtype=torch.float16):\n     ref_qk = torch.matmul(q, k)\n     for z in range(Z):\n         for h in range(H):\n-            ref_qk[:,:,M==0] = float(\"-inf\")\n+            ref_qk[:,:, M==0] = float(\"-inf\")\n     ref_qk = torch.softmax(ref_qk, dim=-1)\n     ref_out = torch.matmul(ref_qk, v)\n     # compare"}]