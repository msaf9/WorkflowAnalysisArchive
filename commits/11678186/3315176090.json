[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -35,7 +35,7 @@ def TTG_AsyncWaitOp : TTG_Op<\"async_wait\"> {\n // Port Arith_CmpIOp & Arith_CmpFOp & Std_SelectOp to TritonGPU.\n // This is needed because these ops don't\n // handle encodings\n-// e.g., https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Arithmetic/IR/ArithmeticOps.td#L111\n+// e.g., https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Arith/IR/ArithOps.td#L111\n def TTG_CmpIOp : TTG_Op<\"cmpi\", [NoSideEffect]> {\n   let summary = \"integer comparison operation\";\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -29,7 +29,7 @@ namespace {\n // convert(blocked, dot_operand) ->\n // convert(blocked, mma) + convert(mma,  dot_operand)\n // if this value is itself the result of a dot operation\n-// this is a hueiristics to accomodate some pattern seen in fused attention\n+// this is a heuristic to accomodate some pattern seen in fused attention\n // kernels.\n // TODO: replace this by something more generic, i.e. layout-aware CSE\n class DecomposeDotOperand : public mlir::RewritePattern {"}]