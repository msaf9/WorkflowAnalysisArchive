[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -100,7 +100,7 @@ jobs:\n         if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '0' && env.ENABLE_MMA_V3 == '0'}}\n         run: |\n           cd python/test/unit\n-          python3 -m pytest -n 8 --ignore=runtime\n+          python3 -m pytest -n 8 --ignore=runtime --ignore=hopper\n           # run runtime tests serially to avoid race condition with cache handling.\n           python3 -m pytest runtime/\n "}, {"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -14,7 +14,7 @@ class TritonTypeDef<string name, string _mnemonic>\n }\n \n // Floating-point Type\n-def TT_Float : AnyTypeOf<[F8E4M3FNUZ, F8E4M3B11FNUZ, F8E5M2, F16, BF16, F32, F64], \"floating-point\">;\n+def TT_Float : AnyTypeOf<[F8E4M3FNUZ, F8E4M3FN, F8E4M3B11FNUZ, F8E5M2, F16, BF16, F32, F64], \"floating-point\">;\n def TT_FloatTensor : TensorOf<[TT_Float]>;\n def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -576,7 +576,8 @@ struct ConvertLayoutOpConversion\n     unsigned inVec = 0;\n     unsigned outVec = 0;\n     auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n-    if (getElementTypeOrSelf(op.getType()).isa<mlir::Float8E4M3B11FNUZType>()) {\n+    if (getElementTypeOrSelf(op.getType())\n+            .isa<mlir::Float8E4M3B11FNUZType, mlir::Float8E4M3FNType>()) {\n       assert(inVec % 4 == 0 && \"conversion not supported for FP8E4M3B15\");\n       assert(outVec % 4 == 0 && \"conversion not supported for FP8E4M3B15\");\n     }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -503,7 +503,7 @@ std::function<void(int, int)> getLoadMatrixFn(\n   if (tensor.getType()\n           .cast<RankedTensorType>()\n           .getElementType()\n-          .isa<mlir::Float8E4M3B11FNUZType>()) {\n+          .isa<mlir::Float8E4M3B11FNUZType, mlir::Float8E4M3FNType>()) {\n     bool noTrans = (isA ^ (order[0] == 0));\n     assert(noTrans && \"float8e4b15 must have row-col layout\");\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -627,17 +627,20 @@ struct FpToFpOpConversion\n     auto F8E4M3B15TyID = TypeID::get<mlir::Float8E4M3B11FNUZType>();\n     auto F8E4M3TyID = TypeID::get<mlir::Float8E4M3FNUZType>();\n     auto F8E5M2TyID = TypeID::get<mlir::Float8E5M2Type>();\n+    auto F8E4M3FNTyID = TypeID::get<mlir::Float8E4M3FNType>();\n     auto F16TyID = TypeID::get<mlir::Float16Type>();\n     auto BF16TyID = TypeID::get<mlir::BFloat16Type>();\n     auto F32TyID = TypeID::get<mlir::Float32Type>();\n     auto F64TyID = TypeID::get<mlir::Float64Type>();\n     static DenseMap<std::pair<TypeID, TypeID>, std::string> srcMap = {\n         // F8 -> F16\n         {{F8E4M3B15TyID, F16TyID}, Fp8E4M3B15_to_Fp16},\n+        {{F8E4M3FNTyID, F16TyID}, Fp8E4M3B15x4_to_Fp16},\n         {{F8E4M3TyID, F16TyID}, Fp8E4M3_to_Fp16},\n         {{F8E5M2TyID, F16TyID}, Fp8E5M2_to_Fp16},\n         // F16 -> F8\n         {{F16TyID, F8E4M3B15TyID}, Fp16_to_Fp8E4M3B15},\n+        {{F16TyID, F8E4M3FNTyID}, Fp16_to_Fp8E4M3B15x4},\n         {{F16TyID, F8E4M3TyID}, Fp16_to_Fp8E4M3},\n         {{F16TyID, F8E5M2TyID}, Fp16_to_Fp8E5M2},\n         // F8 -> BF16\n@@ -1181,7 +1184,11 @@ void populateElementwiseOpToLLVMPatterns(\n   POPULATE_BINARY_OP(arith::ShRSIOp, LLVM::AShrOp)  // >>\n   POPULATE_BINARY_OP(arith::ShRUIOp, LLVM::LShrOp)  // >>\n   POPULATE_BINARY_OP(arith::MinFOp, LLVM::MinNumOp) // fmin\n+  POPULATE_BINARY_OP(arith::MaxFOp, LLVM::MaxNumOp) // fmax\n   POPULATE_BINARY_OP(arith::MinSIOp, LLVM::SMinOp)  // smin\n+  POPULATE_BINARY_OP(arith::MaxSIOp, LLVM::SMaxOp)  // smax\n+  POPULATE_BINARY_OP(arith::MinUIOp, LLVM::UMinOp)  // umin\n+  POPULATE_BINARY_OP(arith::MaxUIOp, LLVM::UMaxOp)  // umax\n #undef POPULATE_BINARY_OP\n \n #define POPULATE_UNARY_OP(SRC_OP, DST_OP)                                      \\"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 8, "deletions": 11, "changes": 19, "file_content_changes": "@@ -321,17 +321,14 @@ struct ReduceOpConversion\n       return NVVM::ReduxKind::OR;\n     if (isa<arith::XOrIOp>(reduceOp))\n       return NVVM::ReduxKind::XOR;\n-    if (auto externalCall =\n-            dyn_cast<triton::PureExternElementwiseOp>(reduceOp)) {\n-      if (externalCall.getSymbol() == \"__nv_min\")\n-        return NVVM::ReduxKind::MIN;\n-      if (externalCall.getSymbol() == \"__nv_umin\")\n-        return NVVM::ReduxKind::UMIN;\n-      if (externalCall.getSymbol() == \"__nv_max\")\n-        return NVVM::ReduxKind::MAX;\n-      if (externalCall.getSymbol() == \"__nv_umax\")\n-        return NVVM::ReduxKind::UMAX;\n-    }\n+    if (isa<arith::MinSIOp>(reduceOp))\n+      return NVVM::ReduxKind::MIN;\n+    if (isa<arith::MinUIOp>(reduceOp))\n+      return NVVM::ReduxKind::UMIN;\n+    if (isa<arith::MaxSIOp>(reduceOp))\n+      return NVVM::ReduxKind::MAX;\n+    if (isa<arith::MaxUIOp>(reduceOp))\n+      return NVVM::ReduxKind::UMAX;\n     return std::nullopt;\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -610,7 +610,8 @@ class ConvertTritonGPUToLLVM\n   void decomposeFp8e4b15Convert(ModuleOp mod) const {\n     mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n       OpBuilder builder(cvtOp);\n-      if (!getElementTypeOrSelf(cvtOp).isa<mlir::Float8E4M3B11FNUZType>())\n+      if (!getElementTypeOrSelf(cvtOp)\n+               .isa<mlir::Float8E4M3B11FNUZType, mlir::Float8E4M3FNType>())\n         return;\n       auto shape = cvtOp.getType().cast<RankedTensorType>().getShape();\n       auto argEncoding ="}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -27,6 +27,9 @@ TritonGPUToLLVMTypeConverter::TritonGPUToLLVMTypeConverter(\n   addConversion([&](mlir::Float8E4M3B11FNUZType type) -> std::optional<Type> {\n     return IntegerType::get(type.getContext(), 8);\n   });\n+  addConversion([&](mlir::Float8E4M3FNType type) -> std::optional<Type> {\n+    return IntegerType::get(type.getContext(), 8);\n+  });\n   addConversion([&](mlir::Float8E4M3FNUZType type) -> std::optional<Type> {\n     return IntegerType::get(type.getContext(), 8);\n   });"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 36, "deletions": 0, "changes": 36, "file_content_changes": "@@ -765,6 +765,12 @@ void init_triton_ir(py::module &&m) {\n              // have a float-like type compatible with float only native ops\n              return self.getBuilder().getType<mlir::Float8E4M3B11FNUZType>();\n            })\n+      .def(\"get_fp8e4b15x4_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             // TODO: upstream FP8E4B15 into MLIR, or find a way to externally\n+             // have a float-like type compatible with float only native ops\n+             return self.getBuilder().getType<mlir::Float8E4M3FNType>();\n+           })\n       .def(\"get_fp8e5_ty\",\n            [](TritonOpBuilder &self) -> mlir::Type {\n              return self.getBuilder().getType<mlir::Float8E5M2Type>();\n@@ -1066,6 +1072,36 @@ void init_triton_ir(py::module &&m) {\n               mlir::Value &rhs) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ShRSIOp>(lhs, rhs));\n            })\n+      .def(\"create_minsi\",\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n+              mlir::Value &rhs) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::MinSIOp>(lhs, rhs));\n+           })\n+      .def(\"create_minui\",\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n+              mlir::Value &rhs) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::MinUIOp>(lhs, rhs));\n+           })\n+      .def(\"create_minf\",\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n+              mlir::Value &rhs) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::MinFOp>(lhs, rhs));\n+           })\n+      .def(\"create_maxsi\",\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n+              mlir::Value &rhs) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::MaxSIOp>(lhs, rhs));\n+           })\n+      .def(\"create_maxui\",\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n+              mlir::Value &rhs) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::MaxUIOp>(lhs, rhs));\n+           })\n+      .def(\"create_maxf\",\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n+              mlir::Value &rhs) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::MaxFOp>(lhs, rhs));\n+           })\n       // AddPtr (similar to GEP)\n       .def(\"create_addptr\",\n            [](TritonOpBuilder &self, mlir::Value &ptr,"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 26, "deletions": 30, "changes": 56, "file_content_changes": "@@ -1384,43 +1384,39 @@ def test_convert_float16_to_float32(in_dtype, device):\n \n \n def serialize_fp8(np_data, in_dtype):\n-    return np_data\n-# def serialize_fp8(np_data, in_dtype):\n-#     if in_dtype == tl.float8e4b15:\n-#         # triton's f8e4b15 format is optimized for software emulation\n-#         # as a result, each pack of 4xfp8 values:\n-#         # s0b0s1b1s2b2s3b3 (for s, b sign and bits respectively)\n-#         # is actually internally stored as\n-#         # s0s2b0b2s1s3b1b3\n-#         # we apply the conversion here\n-#         f8x4 = np_data.view(np.uint32)\n-#         s = [(f8x4 & (0x80000000 >> i)) << i for i in range(0, 32, 8)]\n-#         b = [(f8x4 & (0x7f000000 >> i)) << i for i in range(0, 32, 8)]\n-#         signs = (s[0] >> 0) | (s[1] >> 16) | (s[2] >> 1) | (s[3] >> 17)\n-#         bits = (b[0] >> 1) | (b[1] >> 17) | (b[2] >> 8) | (b[3] >> 24)\n-#         # tensor of triton fp8 data\n-#         return (signs | bits).view(np.int8)\n-#     else:\n-#         return np_data\n+    if in_dtype == tl.float8e4b15x4:\n+        # triton's f8e4b15 format is optimized for software emulation\n+        # as a result, each pack of 4xfp8 values:\n+        # s0b0s1b1s2b2s3b3 (for s, b sign and bits respectively)\n+        # is actually internally stored as\n+        # s0s2b0b2s1s3b1b3\n+        # we apply the conversion here\n+        f8x4 = np_data.view(np.uint32)\n+        s = [(f8x4 & (0x80000000 >> i)) << i for i in range(0, 32, 8)]\n+        b = [(f8x4 & (0x7f000000 >> i)) << i for i in range(0, 32, 8)]\n+        signs = (s[0] >> 0) | (s[1] >> 16) | (s[2] >> 1) | (s[3] >> 17)\n+        bits = (b[0] >> 1) | (b[1] >> 17) | (b[2] >> 8) | (b[3] >> 24)\n+        # tensor of triton fp8 data\n+        return (signs | bits).view(np.int8)\n+    else:\n+        return np_data\n \n # inverse of `serialize_fp8`\n \n \n def deserialize_fp8(np_data, in_dtype):\n-    return np_data\n-# def deserialize_fp8(np_data, in_dtype):\n-#     if in_dtype == tl.float8e4b15:\n-#         f8x4 = np_data.view(np.uint32)\n-#         s = [(f8x4 & (0x80000000 >> i)) << i for i in [0, 16, 1, 17]]\n-#         b = [(f8x4 & (0x7f000000 >> i)) << i for i in [1, 17, 8, 24]]\n-#         signs = (s[0] >> 0) | (s[1] >> 8) | (s[2] >> 16) | (s[3] >> 24)\n-#         bits = (b[0] >> 0) | (b[1] >> 8) | (b[2] >> 16) | (b[3] >> 24)\n-#         return (signs | bits).view(np.int8)\n-#     else:\n-#         return np_data\n+    if in_dtype == tl.float8e4b15x4:\n+        f8x4 = np_data.view(np.uint32)\n+        s = [(f8x4 & (0x80000000 >> i)) << i for i in [0, 16, 1, 17]]\n+        b = [(f8x4 & (0x7f000000 >> i)) << i for i in [1, 17, 8, 24]]\n+        signs = (s[0] >> 0) | (s[1] >> 8) | (s[2] >> 16) | (s[3] >> 24)\n+        bits = (b[0] >> 0) | (b[1] >> 8) | (b[2] >> 16) | (b[3] >> 24)\n+        return (signs | bits).view(np.int8)\n+    else:\n+        return np_data\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4b15x4, tl.float8e4, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.float32])\n def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     \"\"\""}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 26, "deletions": 28, "changes": 54, "file_content_changes": "@@ -58,14 +58,10 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                # split-k\n-                (64, 64, 16, 2, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 64, 16, 4, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 64, 16, 8, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 # variable input\n                 (128, 128, 32, 1, 4, 2, 256, 384, 160, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 128, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 83, AT, BT, DTYPE, DTYPE),\n                 (128, 256, 64, 1, 8, 3, 256, 512, 160, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n@@ -77,9 +73,6 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (128, 64, 16, 1, 4, STAGES, 256, 128, 80, AT, BT, DTYPE, DTYPE),\n                 (256, 128, 32, 1, 8, STAGES, 512, 256, 160, AT, BT, DTYPE, DTYPE),\n                 (128, 128, 32, 1, 4, STAGES, 256, 256, 160, AT, BT, DTYPE, DTYPE),\n-                # split-k\n-                (64, 64, 16, 8, 4, STAGES, 128, 128, 768, AT, BT, DTYPE, DTYPE),\n-                (64, 64, 16, 8, 4, STAGES, 128, 128, 32, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [4]\n         ],\n         # mixed-precision\n@@ -88,7 +81,6 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n-                (128, 128, 32, 8, 4, 2, 256, 256, 128, AT, BT, ADTYPE, BDTYPE),\n             ] for ADTYPE, BDTYPE in [(\"float8e4\", \"float8e5\"),\n                                      (\"float8e4\", \"float16\"),\n                                      (\"float16\", \"float8e5\"),\n@@ -131,35 +123,44 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n     M = BLOCK_M if M is None else M\n     N = BLOCK_N if N is None else N\n     K = BLOCK_K * SPLIT_K if K is None else K\n-    a_fp8 = \"float8\" in ADTYPE\n-    b_fp8 = \"float8\" in BDTYPE\n \n     def maybe_upcast(x, dtype, is_float8):\n         if is_float8:\n             return f8_to_f16(x, dtype)\n         return x\n \n-    def init_input(n, m, t, dtype, is_float8):\n-        if t:\n-            return init_input(m, n, False, dtype, is_float8).t()\n-        if is_float8:\n-            return torch.randint(20, 50, (n, m), device=\"cuda\", dtype=torch.int8)\n+    def init_input(m, n, dtype):\n+        if 'float8' in dtype:\n+            ewidth = {'float8e4b15': 4, 'float8e4': 4, 'float8e5': 5}[dtype]\n+            sign = torch.randint(2, size=(m, n), device=\"cuda\", dtype=torch.int8) * 128\n+            val = torch.randint(2**3 - 1, size=(m, n), device=\"cuda\", dtype=torch.int8) << 7 - ewidth\n+            return sign | val\n         if dtype == \"int8\":\n-            return torch.randint(-128, 127, (n, m), device=\"cuda\", dtype=torch.int8)\n+            return torch.randint(-128, 127, (m, n), device=\"cuda\", dtype=torch.int8)\n         dtype = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[dtype]\n-        return .1 * torch.randn((n, m), device=\"cuda\", dtype=dtype)\n+        exponents = torch.randint(-10, 0, size=(m, n))\n+        ret = (2. ** exponents).to(dtype).to(\"cuda\")\n+        return ret\n \n     # allocate/transpose inputs\n-    a = init_input(M, K, AT, ADTYPE, a_fp8)\n-    b = init_input(K, N, BT, BDTYPE, b_fp8)\n+    a = init_input(M, K, ADTYPE)\n+    b = init_input(K, N, BDTYPE)\n+    a = a if not AT else a.T.contiguous().T\n+    b = b if not BT else b.T.contiguous().T\n     # run test\n-    th_a = maybe_upcast(a, ADTYPE, a_fp8).to(torch.float32)\n+    a_fp8 = \"float8\" in ADTYPE\n+    b_fp8 = \"float8\" in BDTYPE\n+    th_a = maybe_upcast(a, ADTYPE, a_fp8)\n     if AT and a_fp8:\n         th_a = th_a.view(th_a.shape[::-1]).T\n-    th_b = maybe_upcast(b, BDTYPE, b_fp8).to(torch.float32)\n+    th_b = maybe_upcast(b, BDTYPE, b_fp8)\n     if BT and b_fp8:\n         th_b = th_b.view(th_b.shape[::-1]).T\n-    th_c = torch.matmul(th_a, th_b)\n+    if th_a.is_floating_point():\n+        ab_dtype = th_a.dtype if th_a.element_size() > th_b.element_size() else th_b.dtype\n+    else:\n+        ab_dtype = torch.float32\n+    th_c = torch.matmul(th_a.to(ab_dtype), th_b.to(ab_dtype))\n     if ADTYPE == \"int8\" or BDTYPE == \"int8\":\n         th_c = th_c.to(torch.int8)\n     try:\n@@ -168,9 +169,6 @@ def init_input(n, m, t, dtype, is_float8):\n         if b_fp8:\n             b = triton.reinterpret(b, getattr(tl, BDTYPE))\n         tt_c = triton.ops.matmul(a, b)\n-        atol, rtol = 1e-2, 0\n-        if ADTYPE == torch.bfloat16 or BDTYPE == torch.bfloat16:\n-            atol, rtol = 3.5e-2, 0\n-        torch.testing.assert_allclose(th_c, tt_c, atol=atol, rtol=rtol)\n+        torch.testing.assert_allclose(th_c, tt_c, atol=0, rtol=0)\n     except triton.OutOfResources as e:\n         pytest.skip(str(e))"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1065,6 +1065,7 @@ def str_to_ty(name):\n         \"fp8e4\": language.float8e4,\n         \"fp8e5\": language.float8e5,\n         \"fp8e4b15\": language.float8e4b15,\n+        \"fp8e4b15x4\": language.float8e4b15x4,\n         \"fp16\": language.float16,\n         \"bf16\": language.bfloat16,\n         \"fp32\": language.float32,"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 15, "deletions": 16, "changes": 31, "file_content_changes": "@@ -11,10 +11,9 @@\n from pathlib import Path\n from typing import Any, Tuple\n \n-import triton\n-import triton._C.libtriton.triton as _triton\n-from .._C.libtriton.triton import (add_external_libs, compile_ptx_to_cubin,\n-                                   get_num_warps, get_shared_memory_size, ir,\n+from .._C.libtriton.triton import (ClusterInfo, TMAInfos, add_external_libs,\n+                                   compile_ptx_to_cubin, get_env_vars, get_num_warps,\n+                                   get_shared_memory_size, ir,\n                                    translate_llvmir_to_hsaco, translate_llvmir_to_ptx,\n                                    translate_triton_gpu_to_llvmir)\n from ..common.backend import get_backend, path_to_ptxas\n@@ -101,8 +100,8 @@ def optimize_ttgir(mod, num_stages, num_warps, num_ctas, arch,\n     if arch // 10 >= 9 and enable_warp_specialization and num_warps == 4:\n         pm.add_tritongpu_ws_feasibility_checking_pass(arch)\n         pm.run(mod)\n-        ws_enabled = _triton.ir.is_ws_supported(mod)\n-        pm = _triton.ir.pass_manager(mod.context)\n+        ws_enabled = ir.is_ws_supported(mod)\n+        pm = ir.pass_manager(mod.context)\n         pm.enable_debug()\n     if ws_enabled:\n         pm.add_tritongpu_wsdecomposing_pass(arch)\n@@ -426,12 +425,12 @@ def compile(fn, **kwargs):\n     if os.environ.get('OPTIMIZE_EPILOGUE', '') == '1':\n         optimize_epilogue = True\n     #\n-    cluster_info = _triton.ClusterInfo()\n+    cluster_info = ClusterInfo()\n     if \"clusterDims\" in kwargs:\n         cluster_info.clusterDimX = kwargs[\"clusterDims\"][0]\n         cluster_info.clusterDimY = kwargs[\"clusterDims\"][1]\n         cluster_info.clusterDimZ = kwargs[\"clusterDims\"][2]\n-    tma_infos = _triton.TMAInfos()\n+    tma_infos = TMAInfos()\n     # build compilation stages\n     stages = dict()\n     stages[\"ast\"] = (lambda path: fn, None)\n@@ -479,7 +478,7 @@ def compile(fn, **kwargs):\n         first_stage = list(stages.keys()).index(ir_name)\n \n     # create cache manager\n-    fn_cache_manager = get_cache_manager(make_hash(fn, arch, _triton.get_env_vars(), **kwargs))\n+    fn_cache_manager = get_cache_manager(make_hash(fn, arch, get_env_vars(), **kwargs))\n     # determine name and extension type of provided function\n     if isinstance(fn, JITFunction):\n         name, ext = fn.__name__, \"ast\"\n@@ -512,7 +511,7 @@ def compile(fn, **kwargs):\n                     \"constants\": _get_jsonable_constants(constants),\n                     \"debug\": debug,\n                     \"arch\": arch, }\n-        metadata.update(_triton.get_env_vars())\n+        metadata.update(get_env_vars())\n         if ext == \"ptx\":\n             assert \"shared\" in kwargs, \"ptx compilation must provide shared memory size\"\n             metadata[\"shared\"] = kwargs[\"shared\"]\n@@ -558,7 +557,7 @@ def compile(fn, **kwargs):\n         if ir_name == \"llir\" and \"shared\" not in metadata:\n             metadata[\"shared\"] = get_shared_memory_size(module)\n         if ir_name == \"ttgir\":\n-            metadata[\"enable_warp_specialization\"] = _triton.ir.is_ws_supported(next_module)\n+            metadata[\"enable_warp_specialization\"] = ir.is_ws_supported(next_module)\n             if metadata[\"enable_warp_specialization\"]:\n                 metadata[\"num_warps\"] = get_num_warps(next_module)\n         if ir_name == \"ptx\":\n@@ -570,7 +569,7 @@ def compile(fn, **kwargs):\n             _device_backend.add_meta_info(ir_name, module, next_module, metadata, asm)\n         module = next_module\n \n-    ids_of_folded_args = tuple([int(k) for k in configs[0].ids_of_folded_args]) if isinstance(fn, triton.runtime.JITFunction) else ()\n+    ids_of_folded_args = tuple([int(k) for k in configs[0].ids_of_folded_args]) if isinstance(fn, JITFunction) else ()\n     if \"clusterDims\" not in metadata:\n         metadata[\"clusterDims\"] = [\n             cluster_info.clusterDimX,\n@@ -585,10 +584,10 @@ def compile(fn, **kwargs):\n             metadata[\"tensormaps_info\"][i].ids_of_folded_args = ids_of_folded_args\n \n     ids_of_tensormaps = get_ids_of_tensormaps(metadata.get(\"tensormaps_info\", None))\n-    if isinstance(fn, triton.runtime.JITFunction) and \"tensormaps_info\" in metadata:\n+    if isinstance(fn, JITFunction) and \"tensormaps_info\" in metadata:\n         fn.tensormaps_info = metadata[\"tensormaps_info\"]\n \n-    ids_of_const_exprs = tuple(fn.constexprs) if isinstance(fn, triton.runtime.JITFunction) else ()\n+    ids_of_const_exprs = tuple(fn.constexprs) if isinstance(fn, JITFunction) else ()\n     ids = {\"ids_of_tensormaps\": ids_of_tensormaps, \"ids_of_folded_args\": ids_of_folded_args, \"ids_of_const_exprs\": ids_of_const_exprs}\n     # cache manager\n     if is_cuda or is_hip:\n@@ -674,7 +673,7 @@ def __getattribute__(self, name):\n         return super().__getattribute__(name)\n \n     # capture args and expand args with cutensormap*\n-    def assemble_tensormap_to_arg(self, args, constants):\n+    def assemble_tensormap_to_arg(self, args):\n         args_with_tma = list(args)\n         if hasattr(self, 'tensormaps_info'):\n             # tuple for hashable\n@@ -687,7 +686,7 @@ def __getitem__(self, grid):\n         self._init_handles()\n \n         def runner(*args, stream=None):\n-            args_expand = self.assemble_tensormap_to_arg(args, self.constants)\n+            args_expand = self.assemble_tensormap_to_arg(args)\n             if stream is None:\n                 if self.device_type in [\"cuda\", \"hip\"]:\n                     stream = get_cuda_stream()"}, {"filename": "python/triton/compiler/make_launcher.py", "status": "modified", "additions": 28, "deletions": 2, "changes": 30, "file_content_changes": "@@ -250,6 +250,7 @@ def format_of(ty):\n #include \\\"cuda.h\\\"\n #include <stdbool.h>\n #include <Python.h>\n+#include <dlfcn.h>\n \n static inline void gpuAssert(CUresult code, const char *file, int line)\n {{\n@@ -267,9 +268,30 @@ def format_of(ty):\n \n #define CUDA_CHECK(ans) {{ gpuAssert((ans), __FILE__, __LINE__); }}\n \n+typedef CUresult (*cuLaunchKernelEx_t)(const CUlaunchConfig* config, CUfunction f, void** kernelParams, void** extra);\n+\n+static cuLaunchKernelEx_t getLaunchKernelExHandle() {{\n+  // Open the shared library\n+  void* handle = dlopen(\"libcuda.so\", RTLD_LAZY);\n+  if (!handle) {{\n+    PyErr_SetString(PyExc_RuntimeError, \"Failed to open libcuda.so\");\n+    return NULL;\n+  }}\n+  // Clear any existing error\n+  dlerror();\n+  cuLaunchKernelEx_t cuLaunchKernelExHandle = (cuLaunchKernelEx_t)dlsym(handle, \"cuLaunchKernelEx\");\n+  // Check for errors\n+  const char *dlsym_error = dlerror();\n+  if (dlsym_error) {{\n+    PyErr_SetString(PyExc_RuntimeError, \"Failed to retrieve cuLaunchKernelEx from libcuda.so\");\n+    return NULL;\n+  }}\n+  return cuLaunchKernelExHandle;\n+}}\n+\n static void _launch(int gridX, int gridY, int gridZ, int num_warps, int num_ctas, int clusterDimX, int clusterDimY, int clusterDimZ, int shared_memory, CUstream stream, CUfunction function{', ' + arg_decls if len(arg_decls) > 0 else ''}) {{\n   void *params[] = {{ {', '.join(f\"&arg{i}\" for i in params)} }};\n-  if(gridX*gridY*gridZ > 0){{\n+  if (gridX*gridY*gridZ > 0) {{\n     if (num_ctas == 1) {{\n       CUDA_CHECK(cuLaunchKernel(function, gridX, gridY, gridZ, 32*num_warps, 1, 1, shared_memory, stream, params, 0));\n     }} else {{\n@@ -291,7 +313,11 @@ def format_of(ty):\n       config.hStream = stream;\n       config.attrs = launchAttr;\n       config.numAttrs = 2;\n-      CUDA_CHECK(cuLaunchKernelEx(&config, function, params, 0));\n+      static cuLaunchKernelEx_t cuLaunchKernelExHandle = NULL;\n+      if (cuLaunchKernelExHandle == NULL) {{\n+        cuLaunchKernelExHandle = getLaunchKernelExHandle();\n+      }}\n+      CUDA_CHECK(cuLaunchKernelExHandle(&config, function, params, 0));\n     }}\n   }}\n }}"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -50,6 +50,7 @@\n     float32,\n     float64,\n     float8e4b15,\n+    float8e4b15x4,\n     float8e4,\n     float8e5,\n     function_type,\n@@ -148,6 +149,7 @@\n     \"float32\",\n     \"float64\",\n     \"float8e4b15\",\n+    \"float8e4b15x4\",\n     \"float8e4\",\n     \"float8e5\",\n     \"full\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 25, "deletions": 15, "changes": 40, "file_content_changes": "@@ -76,7 +76,7 @@ def _to_tensor(x, builder):\n class dtype:\n     SINT_TYPES = ['int8', 'int16', 'int32', 'int64']\n     UINT_TYPES = ['int1', 'uint8', 'uint16', 'uint32', 'uint64']\n-    FP_TYPES = ['fp8e4b15', 'fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n+    FP_TYPES = ['fp8e4b15', 'fp8e4b15x4', 'fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n     STANDARD_FP_TYPES = ['fp16', 'bf16', 'fp32', 'fp64']\n     OTHER_TYPES = ['void']\n \n@@ -100,6 +100,10 @@ def __init__(self, name):\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n                 self.exponent_bias = 15\n+            elif name == 'fp8e4b15x4':\n+                self.fp_mantissa_width = 3\n+                self.primitive_bitwidth = 8\n+                self.exponent_bias = 15\n             elif name == 'fp8e4':\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n@@ -138,6 +142,9 @@ def is_fp8e4(self):\n     def is_fp8e4b15(self):\n         return self.name == 'fp8e4b15'\n \n+    def is_fp8e4b15x4(self):\n+        return self.name == 'fp8e4b15x4'\n+\n     def is_fp16(self):\n         return self.name == 'fp16'\n \n@@ -241,6 +248,8 @@ def to_ir(self, builder: ir.builder) -> ir.type:\n             return builder.get_fp8e4_ty()\n         elif self.name == 'fp8e4b15':\n             return builder.get_fp8e4b15_ty()\n+        elif self.name == 'fp8e4b15x4':\n+            return builder.get_fp8e4b15x4_ty()\n         elif self.name == 'fp16':\n             return builder.get_half_ty()\n         elif self.name == 'bf16':\n@@ -375,6 +384,7 @@ def to_ir(self, builder: ir.builder):\n float8e5 = dtype('fp8e5')\n float8e4 = dtype('fp8e4')\n float8e4b15 = dtype('fp8e4b15')\n+float8e4b15x4 = dtype('fp8e4b15x4')\n float16 = dtype('fp16')\n bfloat16 = dtype('bf16')\n float32 = dtype('fp32')\n@@ -1382,7 +1392,7 @@ def minimum(x, y):\n     :param other: the second input tensor\n     :type other: Block\n     \"\"\"\n-    return where(x < y, x, y)\n+    return math.min(x, y)\n \n \n @jit\n@@ -1395,7 +1405,7 @@ def maximum(x, y):\n     :param other: the second input tensor\n     :type other: Block\n     \"\"\"\n-    return where(x > y, x, y)\n+    return math.max(x, y)\n \n # max and argmax\n \n@@ -1422,11 +1432,6 @@ def _argmax_combine_tie_break_fast(value1, index1, value2, index2):\n     return _argmax_combine(value1, index1, value2, index2, False)\n \n \n-@jit\n-def _fast_max(x, y):\n-    return math.max(x, y)\n-\n-\n @jit\n @_add_reduction_docstr(\"maximum\",\n                        return_indices_arg=\"return_indices\",\n@@ -1445,7 +1450,7 @@ def max(input, axis=None, return_indices=False, return_indices_tie_break_left=Tr\n             else:\n                 assert input.dtype.is_integer_type()\n                 input = input.to(int32)\n-        return reduce(input, axis, _fast_max)\n+        return reduce(input, axis, maximum)\n \n \n @jit\n@@ -1479,11 +1484,6 @@ def _argmin_combine_tie_break_fast(value1, index1, value2, index2):\n     return _argmin_combine(value1, index1, value2, index2, False)\n \n \n-@jit\n-def _fast_min(x, y):\n-    return math.min(x, y)\n-\n-\n @jit\n @_add_reduction_docstr(\"minimum\",\n                        return_indices_arg=\"return_indices\",\n@@ -1502,7 +1502,7 @@ def min(input, axis=None, return_indices=False, return_indices_tie_break_left=Tr\n             else:\n                 assert input.dtype.is_integer_type()\n                 input = input.to(int32)\n-        return reduce(input, axis, _fast_min)\n+        return reduce(input, axis, minimum)\n \n \n @jit\n@@ -1926,6 +1926,16 @@ def extern_elementwise(lib_name: str, lib_path: str, args: list, arg_type_symbol\n     return dispatch(func, lib_name, lib_path, dispatch_args, arg_type_symbol_dict, ret_shape, is_pure, _builder)\n \n \n+def binary_op_type_legalization(lhs, rhs, builder):\n+    '''\n+        Convert both operands to a single common type\n+        :param lhs: the left operand\n+        :param rhs: the right operand\n+        :param builder: the builder\n+    '''\n+    return semantic.binary_op_type_checking_impl(lhs, rhs, builder)\n+\n+\n def extern(fn):\n     \"\"\"A decorator for external functions.\"\"\"\n     return builtin(fn)"}, {"filename": "python/triton/language/math.py", "status": "modified", "additions": 24, "deletions": 16, "changes": 40, "file_content_changes": "@@ -40,26 +40,34 @@ def byte_perm(arg0, arg1, arg2, _builder=None):\n \n @core.extern\n def min(arg0, arg1, _builder=None):\n-    return core.extern_elementwise(\"libdevice\", libdevice_path(), [arg0, arg1, ],\n-                                   {(core.dtype(\"int32\"), core.dtype(\"int32\"),): (\"__nv_min\", core.dtype(\"int32\")),\n-                                    (core.dtype(\"uint32\"), core.dtype(\"uint32\"),): (\"__nv_umin\", core.dtype(\"uint32\")),\n-                                    (core.dtype(\"int64\"), core.dtype(\"int64\"),): (\"__nv_llmin\", core.dtype(\"int64\")),\n-                                    (core.dtype(\"uint64\"), core.dtype(\"uint64\"),): (\"__nv_ullmin\", core.dtype(\"uint64\")),\n-                                    (core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fminf\", core.dtype(\"fp32\")),\n-                                    (core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_fmin\", core.dtype(\"fp64\")),\n-                                    }, is_pure=True, _builder=_builder)\n+    arg0 = core._to_tensor(arg0, _builder)\n+    arg1 = core._to_tensor(arg1, _builder)\n+    arg0, arg1 = core.binary_op_type_legalization(arg0, arg1, _builder)\n+    dtype = arg0.dtype\n+    if dtype.is_floating():\n+        return core.tensor(_builder.create_minf(arg0.handle, arg1.handle), arg0.type)\n+    elif dtype.is_int_signed():\n+        return core.tensor(_builder.create_minsi(arg0.handle, arg1.handle), arg0.type)\n+    elif dtype.is_int_unsigned():\n+        return core.tensor(_builder.create_minui(arg0.handle, arg1.handle), arg0.dtype)\n+    else:\n+        assert False, f\"Unexpected dtype {dtype}\"\n \n \n @core.extern\n def max(arg0, arg1, _builder=None):\n-    return core.extern_elementwise(\"libdevice\", libdevice_path(), [arg0, arg1, ],\n-                                   {(core.dtype(\"int32\"), core.dtype(\"int32\"),): (\"__nv_max\", core.dtype(\"int32\")),\n-                                    (core.dtype(\"uint32\"), core.dtype(\"uint32\"),): (\"__nv_umax\", core.dtype(\"uint32\")),\n-                                    (core.dtype(\"int64\"), core.dtype(\"int64\"),): (\"__nv_llmax\", core.dtype(\"int64\")),\n-                                    (core.dtype(\"uint64\"), core.dtype(\"uint64\"),): (\"__nv_ullmax\", core.dtype(\"uint64\")),\n-                                    (core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmaxf\", core.dtype(\"fp32\")),\n-                                    (core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_fmax\", core.dtype(\"fp64\")),\n-                                    }, is_pure=True, _builder=_builder)\n+    arg0 = core._to_tensor(arg0, _builder)\n+    arg1 = core._to_tensor(arg1, _builder)\n+    arg0, arg1 = core.binary_op_type_legalization(arg0, arg1, _builder)\n+    dtype = arg0.dtype\n+    if dtype.is_floating():\n+        return core.tensor(_builder.create_maxf(arg0.handle, arg1.handle), arg0.type)\n+    elif dtype.is_int_signed():\n+        return core.tensor(_builder.create_maxsi(arg0.handle, arg1.handle), arg0.type)\n+    elif dtype.is_int_unsigned():\n+        return core.tensor(_builder.create_maxui(arg0.handle, arg1.handle), arg0.dtype)\n+    else:\n+        assert False, f\"Unexpected dtype {dtype}\"\n \n \n @core.extern"}, {"filename": "python/triton/language/random.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -55,7 +55,7 @@ def randint(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     using `randint4x` is likely to be faster than calling `randint` 4 times.\n \n     :param seed: The seed for generating random numbers.\n-    :param offsets: The offsets to generate random numbers for.\n+    :param offset: The offsets to generate random numbers for.\n     \"\"\"\n     ret, _, _, _ = randint4x(seed, offset, n_rounds)\n     return ret\n@@ -120,7 +120,7 @@ def rand(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n def rand4x(seed, offsets, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offsets` block,\n-    returns a 4 blocks of random :code:`float32` in :math:`U(0, 1)`.\n+    returns 4 blocks of random :code:`float32` in :math:`U(0, 1)`.\n \n     :param seed: The seed for generating random numbers.\n     :param offsets: The offsets to generate random numbers for.\n@@ -167,7 +167,7 @@ def randn(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n def randn4x(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offset` block,\n-    returns a 4 blocks of random :code:`float32` in :math:`\\\\mathcal{N}(0, 1)`.\n+    returns 4 blocks of random :code:`float32` in :math:`\\\\mathcal{N}(0, 1)`.\n \n     :param seed: The seed for generating random numbers.\n     :param offsets: The offsets to generate random numbers for."}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -192,7 +192,7 @@ def truediv(input: tl.tensor,\n     elif input_scalar_ty.is_int() and other_scalar_ty.is_int():\n         input = cast(input, tl.float32, builder)\n         other = cast(other, tl.float32, builder)\n-    # float / float (cast to highest exponent type)\n+    # float / float (cast to the highest exponent type)\n     elif input_scalar_ty.is_floating() and other_scalar_ty.is_floating():\n         if input_scalar_ty.fp_mantissa_width > other_scalar_ty.fp_mantissa_width:\n             other = cast(other, input_scalar_ty, builder)\n@@ -1364,7 +1364,7 @@ def wrap_tensor(x, scalar_ty):\n \n def _check_dtype(dtypes: List[str]) -> T:\n     \"\"\"\n-    We following libdevice's convention to check accepted data types for math functions.\n+    We're following libdevice's convention to check accepted data types for math functions.\n     It is not a good practice to support all data types as accelerators/GPUs don't support\n     many float16 and bfloat16 math operations.\n     We should let the users know that they are using and invoke explicit cast to convert"}, {"filename": "python/triton/runtime/backends/cuda.c", "status": "modified", "additions": 39, "deletions": 5, "changes": 44, "file_content_changes": "@@ -1,4 +1,5 @@\n #include \"cuda.h\"\n+#include <dlfcn.h>\n #define PY_SSIZE_T_CLEAN\n #include <Python.h>\n \n@@ -338,6 +339,36 @@ static cuuint32_t *list_to_cuuint32_array(PyObject *listObj) {\n   return array;\n }\n \n+typedef CUresult (*cuTensorMapEncodeTiled_t)(\n+    CUtensorMap *tensorMap, CUtensorMapDataType tensorDataType,\n+    cuuint32_t tensorRank, void *globalAddress, const cuuint64_t *globalDim,\n+    const cuuint64_t *globalStrides, const cuuint32_t *boxDim,\n+    const cuuint32_t *elementStrides, CUtensorMapInterleave interleave,\n+    CUtensorMapSwizzle swizzle, CUtensorMapL2promotion l2Promotion,\n+    CUtensorMapFloatOOBfill oobFill);\n+\n+static cuTensorMapEncodeTiled_t getCuTensorMapEncodeTiledHandle() {\n+  // Open the shared library\n+  void *handle = dlopen(\"libcuda.so\", RTLD_LAZY);\n+  if (!handle) {\n+    PyErr_SetString(PyExc_RuntimeError, \"Failed to open libcuda.so\");\n+    return NULL;\n+  }\n+  // Clear any existing error\n+  dlerror();\n+  cuTensorMapEncodeTiled_t cuTensorMapEncodeTiledHandle =\n+      (cuTensorMapEncodeTiled_t)dlsym(handle, \"cuTensorMapEncodeTiled\");\n+  // Check for errors\n+  const char *dlsym_error = dlerror();\n+  if (dlsym_error) {\n+    PyErr_SetString(\n+        PyExc_RuntimeError,\n+        \"Failed to retrieve cuTensorMapEncodeTiled from libcuda.so\");\n+    return NULL;\n+  }\n+  return cuTensorMapEncodeTiledHandle;\n+}\n+\n static PyObject *tensorMapEncodeTiled(PyObject *self, PyObject *args) {\n   CUtensorMap *tensorMap = (CUtensorMap *)malloc(sizeof(CUtensorMap));\n   CUtensorMapDataType tensorDataType;\n@@ -364,18 +395,21 @@ static PyObject *tensorMapEncodeTiled(PyObject *self, PyObject *args) {\n   cuuint32_t *boxDim = list_to_cuuint32_array(boxDimObj);\n   cuuint32_t *elementStrides = list_to_cuuint32_array(elementStridesObj);\n \n+  static cuTensorMapEncodeTiled_t cuTensorMapEncodeTiledHandle = NULL;\n+  if (cuTensorMapEncodeTiledHandle == NULL) {\n+    cuTensorMapEncodeTiledHandle = getCuTensorMapEncodeTiledHandle();\n+  }\n   // Call the function\n-  CUDA_CHECK(cuTensorMapEncodeTiled(tensorMap, tensorDataType, tensorRank,\n-                                    globalAddress, globalDim, globalStrides,\n-                                    boxDim, elementStrides, interleave, swizzle,\n-                                    l2Promotion, oobFill));\n+  CUDA_CHECK(cuTensorMapEncodeTiledHandle(\n+      tensorMap, tensorDataType, tensorRank, globalAddress, globalDim,\n+      globalStrides, boxDim, elementStrides, interleave, swizzle, l2Promotion,\n+      oobFill));\n \n   // Clean up\n   free(globalDim);\n   free(globalStrides);\n   free(boxDim);\n   free(elementStrides);\n-\n   // Return the tensor map as a normal pointer\n   return PyLong_FromUnsignedLongLong((unsigned long long)tensorMap);\n }"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 5, "deletions": 9, "changes": 14, "file_content_changes": "@@ -11,7 +11,7 @@\n from typing import (Callable, Generic, Iterable, List, Optional, TypeVar, Union, cast,\n                     overload)\n \n-import triton\n+from .._C.libtriton.triton import TMAInfos\n from ..common.backend import get_backend, path_to_ptxas\n \n TRITON_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n@@ -248,6 +248,7 @@ def _type_of(key):\n             \"float8e4\": \"fp8e4\",\n             \"float8e5\": \"fp8e5\",\n             \"float8e4b15\": \"fp8e4b15\",\n+            \"float8e4b15x4\": \"fp8e4b15x4\",\n             \"float16\": \"fp16\",\n             \"bfloat16\": \"bf16\",\n             \"float32\": \"fp32\",\n@@ -406,13 +407,8 @@ def {self.fn.__name__}({args_signature}grid=None, num_warps=4, num_ctas=1, num_s\n     if bin is not None:\n       # build dict of constant values\n       args = [{args}]\n-      all_args = {', '.join([f'{arg}' for arg in self.arg_names]) + ', ' if len(self.arg_names) > 0 else ()}\n-      configs = self._get_config(*all_args),\n-      constants = self._make_constants(constexpr_key)\n-      constants.update({{i: None for i, arg in enumerate(all_args) if arg is None}})\n-      constants.update({{i: 1 for i in configs[0].equal_to_1}})\n       # Create tensormaps and append to args\n-      args = bin.assemble_tensormap_to_arg(args, constants)\n+      args = bin.assemble_tensormap_to_arg(args)\n       if not warmup:\n           bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.num_ctas, bin.clusterDims[0], bin.clusterDims[1], bin.clusterDims[2], bin.shared, stream, bin.cu_function, CompiledKernel.launch_enter_hook, CompiledKernel.launch_exit_hook, bin, *args)\n       return bin\n@@ -434,7 +430,7 @@ def {self.fn.__name__}({args_signature}grid=None, num_warps=4, num_ctas=1, num_s\n       if not self._call_hook(key, signature, device, constants, num_warps, num_ctas, num_stages, enable_warp_specialization, extern_libs, configs):\n         bin = compile(self, signature=signature, device=device, constants=constants, num_warps=num_warps, num_ctas=num_ctas, num_stages=num_stages, enable_warp_specialization=enable_warp_specialization, extern_libs=extern_libs, configs=configs, debug=self.debug, device_type=device_type)\n         # Create tensormaps and append to args\n-        args = bin.assemble_tensormap_to_arg(args, constants)\n+        args = bin.assemble_tensormap_to_arg(args)\n         if not warmup:\n             bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.num_ctas, bin.clusterDims[0], bin.clusterDims[1], bin.clusterDims[2], bin.shared, stream, bin.cu_function, CompiledKernel.launch_enter_hook, CompiledKernel.launch_exit_hook, bin, *args)\n         self.cache[device][key] = bin\n@@ -485,7 +481,7 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         # index of constexprs\n         self.constexprs = [self.arg_names.index(name) for name, ty in self.__annotations__.items() if 'constexpr' in ty]\n         # tma info\n-        self.tensormaps_info = triton._C.libtriton.triton.TMAInfos()\n+        self.tensormaps_info = TMAInfos()\n         # launcher\n         self.run = self._make_launcher()\n         # re-use docs of wrapped function"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -87,10 +87,10 @@ def _fwd_kernel(\n         k = tl.load(K_block_ptr)\n         v = tl.load(V_block_ptr)\n         # -- compute qk ---\n-        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float16)\n         if IS_CAUSAL:\n             qk = tl.where(P_SEQ + offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n-        qk += tl.dot(q, k)\n+        qk += tl.dot(q, k, out_dtype=tl.float16)\n         # -- compute scaling constant ---\n         m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n         alpha = tl.math.exp2(m_i - m_i_new)\n@@ -243,11 +243,12 @@ def forward(ctx, q, k, v, causal, sm_scale):\n         assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n         BLOCK_M = 128\n-        BLOCK_N = 64\n+        BLOCK_N = 64 if Lk <= 64 else 32\n+        num_stages = 4 if Lk <= 64 else 3\n+        num_warps = 4\n         grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         P_SEQ = 0 if q.shape[-2] == k.shape[-2] else k.shape[-2] - q.shape[-2]\n-        num_warps = 4 if Lk <= 64 else 8\n         _fwd_kernel[grid](\n             q, k, v, sm_scale,\n             L,\n@@ -260,7 +261,7 @@ def forward(ctx, q, k, v, causal, sm_scale):\n             BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,\n             IS_CAUSAL=causal,\n             num_warps=num_warps,\n-            num_stages=4)\n+            num_stages=num_stages)\n \n         ctx.save_for_backward(q, k, v, o, L)\n         ctx.grid = grid"}, {"filename": "python/tutorials/09-experimental-tma-matrix-multiplication.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -1,3 +1,10 @@\n+\"\"\"\n+Matrix Multiplication with TMA (Experimental)\n+================================================\n+In this tutorial, you will write a very short high-performance multiplication kernel that achieves\n+performance on parallel with cuBLAS.\n+\"\"\"\n+\n # Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n #\n # Permission is hereby granted, free of charge, to any person obtaining"}, {"filename": "python/tutorials/10-experimental-tmastg-matrix-multiplication.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -1,3 +1,10 @@\n+\"\"\"\n+Matrix Multiplication with TMASTG (Experimental)\n+================================================\n+In this tutorial, you will write a very short high-performance multiplication kernel that achieves\n+performance on parallel with cuBLAS.\n+\"\"\"\n+\n # Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n #\n # Permission is hereby granted, free of charge, to any person obtaining"}]