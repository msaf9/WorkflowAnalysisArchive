[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -76,7 +76,8 @@ def TTG_SelectOp : TTG_Op<\"select\", [NoSideEffect]> {\n \n def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n                                     [SameVariadicOperandSize,\n-                                     MemoryEffects<[MemRead, MemWrite]>,\n+                                     // MemoryEffects<[MemRead]>, doesn't work with CSE but seems like it should?\n+                                     NoSideEffect,\n                                      TypesMatchWith<\"infer mask type from src type\",\n                                                     \"src\", \"mask\", \"getI1SameShape($_self)\",\n                                                     \"($_op.getOperands().size() <= 3) || std::equal_to<>()\">,"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -40,7 +40,8 @@ AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n   if (TensorType ty = value.getType().dyn_cast<TensorType>())\n     rank = ty.getRank();\n   int divHint = 1;\n-  if (BlockArgument blockArg = value.dyn_cast<BlockArgument>()) {\n+  BlockArgument blockArg = value.dyn_cast<BlockArgument>();\n+  if (blockArg && blockArg.getOwner()->isEntryBlock()) {\n     Operation *op = blockArg.getOwner()->getParentOp();\n     if (FuncOp fun = dyn_cast<FuncOp>(op)) {\n       Attribute attr ="}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1867,8 +1867,8 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n     unsigned linearIdxInNanoTile = i % srcAccumSizeInThreads;\n     auto multiDimIdxInNanoTile = getMultiDimIndex<unsigned>(\n         linearIdxInNanoTile, srcBlockedLayout.getSizePerThread());\n-    multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n     unsigned pos = multiDimIdxInNanoTile[inOrd[0]] % minVec;\n+    multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n     unsigned wordVecIdx =\n         getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep);\n     wordVecs[wordVecIdx] ="}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -71,7 +71,8 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     // convert operands\n     SmallVector<Value, 4> newArgs;\n     for (auto v : op->getOperands()) {\n-      if (v.getType().isa<RankedTensorType>())\n+      auto vTy = v.getType().dyn_cast<RankedTensorType>();\n+      if (vTy && !vTy.getEncoding().isa<triton::gpu::SharedEncodingAttr>())\n         newArgs.push_back(builder.create<triton::gpu::ConvertLayoutOp>(\n             op->getLoc(), convertType(v.getType()), v));\n       else"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 34, "deletions": 1, "changes": 35, "file_content_changes": "@@ -102,7 +102,40 @@ class SimplifyConversion : public mlir::RewritePattern {\n     // block argument\n     if (!arg)\n       return mlir::failure();\n-    // cvt(type2, cvt(type1, x)) -> cvt(type2, x)\n+    // cvt(alloc_tensor(x), type2) -> alloc_tensor(x, type2)\n+    // cvt(insert_slice(x), type2) -> extract_slice(cvt(x, type2))\n+    auto alloc_tensor = dyn_cast<triton::gpu::AllocTensorOp>(arg);\n+    if (alloc_tensor) {\n+      rewriter.replaceOpWithNewOp<triton::gpu::AllocTensorOp>(\n+          op, op->getResult(0).getType());\n+      return mlir::success();\n+    }\n+    auto insert_slice = dyn_cast<triton::gpu::InsertSliceAsyncOp>(arg);\n+    if (insert_slice) {\n+      auto newType = op->getResult(0).getType();\n+      auto new_arg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+          op->getLoc(), newType, insert_slice.dst());\n+      rewriter.replaceOpWithNewOp<triton::gpu::InsertSliceAsyncOp>(\n+          op, newType, insert_slice.src(), new_arg.getResult(),\n+          insert_slice.index(), insert_slice.mask(), insert_slice.other(),\n+          insert_slice.cache(), insert_slice.evict(), insert_slice.isVolatile(),\n+          insert_slice.axis());\n+      return mlir::success();\n+    }\n+    // cvt(extract_slice(x), type2) ->extract_slice(cvt(x, type2))\n+    auto extract_slice = dyn_cast<triton::gpu::ExtractSliceOp>(arg);\n+    if (extract_slice) {\n+      auto origType = extract_slice.src().getType().cast<RankedTensorType>();\n+      auto newType = RankedTensorType::get(\n+          origType.getShape(), origType.getElementType(),\n+          op->getResult(0).getType().cast<RankedTensorType>().getEncoding());\n+      auto new_arg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+          op->getLoc(), newType, extract_slice.src());\n+      rewriter.replaceOpWithNewOp<triton::gpu::ExtractSliceOp>(\n+          op, new_arg.getResult(), extract_slice.index(), extract_slice.axis());\n+      return mlir::success();\n+    }\n+    // cvt(type2, x)\n     if (llvm::isa<triton::gpu::ConvertLayoutOp>(arg)) {\n       rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(\n           op, op->getResultTypes().front(), arg->getOperand(0));"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Swizzle.cpp", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -50,8 +50,6 @@ struct SwizzlePass : public TritonGPUSwizzleBase<SwizzlePass> {\n         int vec = order[0] == 1 ? mat_shape[2] : mat_shape[0]; // k : m\n         int mmaStride = order[0] == 1 ? mat_shape[0] : mat_shape[2];\n         int maxPhase = mmaStride / perPhase;\n-        std::cout << perPhase << \" \" << mat_shape[0] << \" \" << mat_shape[1]\n-                  << \" \" << mat_shape[2] << std::endl;\n         return SwizzleInfo{vec, perPhase, maxPhase};\n       }\n       // compute swizzling for B operand"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -1194,6 +1194,10 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::createTritonGPUCombineOpsPass());\n            })\n+      .def(\"add_triton_gpu_swizzle_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::createTritonGPUSwizzlePass());\n+           })\n       .def(\"add_triton_gpu_to_llvm\",\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::triton::createConvertTritonGPUToLLVMPass());"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 57, "deletions": 8, "changes": 65, "file_content_changes": "@@ -7,7 +7,7 @@\n \n \n @triton.jit\n-def matmul_kernel(\n+def matmul_no_scf_kernel(\n     a_ptr, b_ptr, c_ptr,\n     stride_am, stride_ak,\n     stride_bk, stride_bn,\n@@ -36,17 +36,66 @@ def matmul_kernel(\n     [128, 16, 32, 4],\n     [32, 128, 64, 4],\n ])\n-def test_gemm_impl(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n+def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n     a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n     b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n     grid = lambda META: (1, )\n-    matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n-                        stride_am=a.stride(0), stride_ak=a.stride(1),\n-                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n-                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n-                        M=SIZE_M, N=SIZE_N, K=SIZE_K,\n-                        num_warps=NUM_WARPS)\n+    matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n+                               stride_am=a.stride(0), stride_ak=a.stride(1),\n+                               stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                               stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                               M=SIZE_M, N=SIZE_N, K=SIZE_K,\n+                               num_warps=NUM_WARPS)\n     golden = torch.matmul(a, b)\n     torch.set_printoptions(profile=\"full\")\n     assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n+\n+\n+@triton.jit\n+def matmul_kernel(\n+    a_ptr, b_ptr, c_ptr,\n+    stride_am, stride_ak,\n+    stride_bk, stride_bn,\n+    stride_cm, stride_cn,\n+    M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n+    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+):\n+    offs_m = tl.arange(0, BLOCK_SIZE_M)\n+    offs_n = tl.arange(0, BLOCK_SIZE_N)\n+    offs_k = tl.arange(0, BLOCK_SIZE_K)\n+    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n+    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+    for k in range(0, K, BLOCK_SIZE_K):\n+        a = tl.load(a_ptrs)\n+        b = tl.load(b_ptrs)\n+        accumulator += tl.dot(a, b)\n+        a_ptrs += BLOCK_SIZE_K * stride_ak\n+        b_ptrs += BLOCK_SIZE_K * stride_bk\n+\n+    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n+    tl.store(c_ptrs, accumulator)\n+\n+# TODO: DotConversion in TritonGPUToLLVM cannot support non-splat C for the moment\n+# @pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K', [\n+#    [128, 256, 128, 4, 128, 256, 32],\n+#    # [256, 128, 64, 4, 256, 128, 16],\n+#    # [128, 16, 128, 4, 128, 16, 32],\n+#    # [32, 128, 256, 4, 32, 128, 64],\n+# ])\n+# def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K):\n+#    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+#    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+#    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n+#    grid = lambda META: (1, )\n+#    matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n+#                        stride_am=a.stride(0), stride_ak=a.stride(1),\n+#                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n+#                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n+#                        M=a.shape[0], N=b.shape[1], K=a.shape[1],\n+#                        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n+#                        num_warps=NUM_WARPS)\n+#    golden = torch.matmul(a, b)\n+#    torch.set_printoptions(profile=\"full\")\n+#    assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -559,7 +559,10 @@ def visit_For(self, node):\n             raise RuntimeError('Only `range` iterator currently supported')\n         # static for loops: all iterator arguments are constexpr\n         iter_args = [self.visit(arg) for arg in node.iter.args]\n-        is_static = all([isinstance(x, triton.language.constexpr) for x in iter_args])\n+        static_unrolling = os.environ.get('TRITON_STATIC_LOOP_UNROLLING', False)\n+        is_static = False\n+        if static_unrolling:\n+            is_static = all([isinstance(x, triton.language.constexpr) for x in iter_args])\n         if is_static:\n             iter_args = [arg.value for arg in iter_args]\n             range = iterator(*iter_args)\n@@ -875,6 +878,8 @@ def optimize_tritongpu_ir(mod, num_stages):\n     pm.add_coalesce_pass()\n     pm.add_triton_gpu_combine_pass()\n     pm.add_licm_pass()\n+    pm.add_triton_gpu_swizzle_pass()\n+    pm.add_triton_gpu_combine_pass()\n     pm.add_cse_pass()\n     pm.run(mod)\n     return mod"}]