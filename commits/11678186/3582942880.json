[{"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 1, "deletions": 12, "changes": 13, "file_content_changes": "@@ -1432,16 +1432,6 @@ Value DotOpMmaV1ConversionHelper::loadA(\n     Value ha00 = bitcast(extract_element(ha, i32_val(0)), f16x2Ty);\n     Value ha01 = bitcast(extract_element(ha, i32_val(1)), f16x2Ty);\n     ld(has, m, k, ha00, ha01);\n-    {\n-      auto get_f16 = [&](Value value, int idx) {\n-        return extract_element(value, i32_val(idx));\n-      };\n-      std::vector<Value> args;\n-      args.push_back(get_f16(ha00, 0));\n-      args.push_back(get_f16(ha00, 1));\n-      args.push_back(get_f16(ha01, 0));\n-      args.push_back(get_f16(ha01, 1));\n-    }\n \n     if (vecA > 4) {\n       Value ha10 = bitcast(extract_element(ha, i32_val(2)), f16x2Ty);\n@@ -1529,8 +1519,7 @@ Value DotOpMmaV1ConversionHelper::loadB(\n   Value offB0 = isBRow ? offsetBN : offsetBK;\n   Value offB1 = isBRow ? offsetBK : offsetBN;\n   Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n-  // Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n-  // offB0 = add(offB0, cSwizzleOffset);\n+\n   SmallVector<Value> offB(numPtrB);\n   for (int i = 0; i < numPtrB; ++i) {\n     Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 2, "deletions": 11, "changes": 13, "file_content_changes": "@@ -1,5 +1,3 @@\n-import pickle\n-\n import pytest\n import torch\n from torch.testing import assert_close\n@@ -174,8 +172,8 @@ def get_proper_err(a, b, golden):\n     [128, 64, 128, 4, 128, 64, 128, False, False],\n     [16, 16, 16, 16, 16, 16, 16, False, False],  # wpt overflow issue\n     # K-Forloop\n-    [32, 32, 64, 4, 32, 32, 32, False, False],  # Single shared encoding\n-    [16, 16, 128, 4, 16, 16, 16, False, False],  # Single shared encoding and small k\n+    [32, 32, 64, 4, 32, 32, 32, False, False], # Single shared encoding\n+    [16, 16, 128, 4, 16, 16, 16, False, False], # Single shared encoding and small k\n     [64, 32, 128, 4, 64, 32, 64, False, False],\n     [128, 16, 128, 4, 128, 16, 32, False, False],\n     [32, 16, 128, 4, 32, 16, 32, False, False],\n@@ -206,13 +204,6 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n     else:\n         b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n \n-    with open('a.pkl', 'rb') as f:\n-        a = pickle.load(f)\n-    with open('b.pkl', 'rb') as f:\n-        b = pickle.load(f)\n-    print('a', a)\n-    print('b', b)\n-\n     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n     grid = lambda META: (1, )\n     matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 38, "deletions": 38, "changes": 76, "file_content_changes": "@@ -1345,12 +1345,12 @@ def make_hash(fn, **kwargs):\n     return hashlib.md5((Path(fn).read_text() + triton.runtime.jit.version_key()).encode(\"utf-8\")).hexdigest()\n \n \n-# - ^\\s*func\\s+ : match the start of the string, any leading whitespace, the keyword func,\n+# - ^\\s*func\\s+ : match the start of the string, any leading whitespace, the keyword func, \n #    and any following whitespace\n # - (public\\s+)? : optionally match the keyword public and any following whitespace\n-# - (@\\w+) : match an @ symbol followed by one or more word characters\n+# - (@\\w+) : match an @ symbol followed by one or more word characters \n #   (letters, digits, or underscores), and capture it as group 1 (the function name)\n-# - (\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\)) : match a pair of parentheses enclosing\n+# - (\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\)) : match a pair of parentheses enclosing \n #   zero or more arguments separated by commas, and capture it as group 2 (the argument list)\n mlir_prototype_pattern = r'^\\s*func\\s+(?:public\\s+)?(@\\w+)(\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\))\\s*\\{\\s*$'\n ptx_prototype_pattern = r\"\\.(?:visible|extern)\\s+\\.(?:entry|func)\\s+(\\w+)\\s*\\(([^)]*)\\)\"\n@@ -1382,20 +1382,20 @@ def compile(fn, **kwargs):\n     extern_libs = kwargs.get(\"extern_libs\", dict())\n     device = kwargs.get(\"device\", torch.cuda.current_device())\n     capability = torch.cuda.get_device_capability()\n-    capability = capability[0] * 10 + capability[1]\n+    capability = capability[0]*10 + capability[1]\n     # build compilation stages\n     stages = {\n-        \"ast\": (lambda path: fn, None),\n-        \"ttir\": (lambda path: _triton.ir.parse_mlir_module(path, context),\n-                 lambda src: ast_to_ttir(src, signature, configs[0], constants)),\n-        \"ttgir\": (lambda path: _triton.ir.parse_mlir_module(path, context),\n-                  lambda src: ttir_to_ttgir(src, num_warps, num_stages, capability)),\n-        \"llir\": (lambda path: Path(path).read_bytes(),\n-                 lambda src: ttgir_to_llir(src, extern_libs, capability)),\n-        \"ptx\": (lambda path: Path(path).read_text(),\n-                lambda src: llir_to_ptx(src, capability)),\n-        \"cubin\": (lambda path: Path(path).read_bytes(),\n-                  lambda src: ptx_to_cubin(src, capability))\n+      \"ast\" : (lambda path: fn, None),\n+      \"ttir\": (lambda path: _triton.ir.parse_mlir_module(path, context), \n+               lambda src: ast_to_ttir(src, signature, configs[0], constants)),\n+      \"ttgir\": (lambda path: _triton.ir.parse_mlir_module(path, context), \n+                lambda src: ttir_to_ttgir(src, num_warps, num_stages, capability)),\n+      \"llir\": (lambda path: Path(path).read_bytes(), \n+              lambda src: ttgir_to_llir(src, extern_libs, capability)),\n+      \"ptx\":  (lambda path: Path(path).read_text(), \n+              lambda src: llir_to_ptx(src, capability)),\n+      \"cubin\": (lambda path: Path(path).read_bytes(), \n+               lambda src: ptx_to_cubin(src, capability))\n     }\n     # find out the signature of the function\n     if isinstance(fn, triton.runtime.JITFunction):\n@@ -1430,42 +1430,42 @@ def compile(fn, **kwargs):\n     if isinstance(fn, triton.runtime.JITFunction):\n         name, ext = fn.__name__, \"ast\"\n     else:\n-        name, ext = os.path.basename(fn).split(\".\")\n+      name, ext = os.path.basename(fn).split(\".\")\n \n     # load metadata if any\n     metadata = None\n     if fn_cache_manager.has_file(f'{name}.json'):\n         with open(fn_cache_manager._make_path(f\"{name}.json\")) as f:\n             metadata = json.load(f)\n     else:\n-        metadata = {\"num_warps\": num_warps, \"num_stages\": num_stages, \"ctime\": dict()}\n-        if ext == \"ptx\":\n-            assert \"shared\" in kwargs, \"ptx compilation must provide shared memory size\"\n-            metadata[\"shared\"] = kwargs[\"shared\"]\n+      metadata = {\"num_warps\": num_warps, \"num_stages\": num_stages, \"ctime\": dict()}\n+      if ext == \"ptx\":\n+        assert \"shared\" in kwargs, \"ptx compilation must provide shared memory size\"\n+        metadata[\"shared\"] = kwargs[\"shared\"]\n \n     first_stage = list(stages.keys()).index(ext)\n     asm = dict()\n     module = fn\n     # run compilation pipeline  and populate metadata\n     for ir, (parse, compile) in list(stages.items())[first_stage:]:\n-        path = fn_cache_manager._make_path(f\"{name}.{ir}\")\n-        if ir == ext:\n-            next_module = parse(fn)\n-        elif os.path.exists(path) and\\\n-                ir in metadata[\"ctime\"] and\\\n-                os.path.getctime(path) == metadata[\"ctime\"][ir]:\n-            next_module = parse(path)\n-        else:\n-            next_module = compile(module)\n-            fn_cache_manager.put(next_module, f\"{name}.{ir}\")\n-        if os.path.exists(path):\n-            metadata[\"ctime\"][ir] = os.path.getctime(path)\n-        asm[ir] = next_module if ir == \"cubin\" else str(next_module)\n-        if ir == \"llir\" and \"shared\" not in metadata:\n-            metadata[\"shared\"] = _triton.get_shared_memory_size(module)\n-        if ir == \"ptx\":\n-            metadata[\"name\"] = ptx_get_kernel_name(next_module)\n-        module = next_module\n+      path = fn_cache_manager._make_path(f\"{name}.{ir}\")\n+      if ir == ext:\n+        next_module = parse(fn)\n+      elif os.path.exists(path) and\\\n+           ir in metadata[\"ctime\"] and\\\n+           os.path.getctime(path) == metadata[\"ctime\"][ir]:\n+        next_module = parse(path)\n+      else:\n+        next_module = compile(module)\n+        fn_cache_manager.put(next_module, f\"{name}.{ir}\")\n+      if os.path.exists(path):\n+        metadata[\"ctime\"][ir] = os.path.getctime(path)\n+      asm[ir] = next_module if ir == \"cubin\" else str(next_module)\n+      if ir == \"llir\" and \"shared\" not in metadata:\n+        metadata[\"shared\"] = _triton.get_shared_memory_size(module)\n+      if ir == \"ptx\":\n+        metadata[\"name\"] = ptx_get_kernel_name(next_module)\n+      module = next_module\n     # write-back metadata\n     fn_cache_manager.put(json.dumps(metadata), f\"{name}.json\", binary=False)\n     # return handle to compiled kernel"}]