[{"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/RewriteTensorPointer.cpp", "status": "modified", "additions": 90, "deletions": 3, "changes": 93, "file_content_changes": "@@ -366,6 +366,22 @@ class TritonGPURewriteTensorPointerPass\n       : computeCapability(computeCapability) {}\n \n   static bool needRewrite(Operation *op, const DenseSet<Value> &valueToRemove) {\n+    if (auto ifOp = dyn_cast<scf::IfOp>(op)) {\n+      if (op->getNumResults() == 0)\n+        return false;\n+      Operation *thenYield = ifOp.thenYield().getOperation();\n+      if (!ifOp.getElseRegion().empty()) {\n+        Operation *elseYield = ifOp.elseYield().getOperation();\n+        for (unsigned i = 0; i < thenYield->getNumOperands(); ++i) {\n+          bool thenNeedRewrite = valueToRemove.count(thenYield->getOperand(i));\n+          bool elseNeedRewrite = valueToRemove.count(elseYield->getOperand(i));\n+          assert(!(thenNeedRewrite ^ elseNeedRewrite) &&\n+                 \"For IfOp, operand(i) of thenYield and operand(i) of \"\n+                 \"elseYield should be either all need rewrite or all not\");\n+        }\n+      }\n+      op = thenYield;\n+    }\n     return std::any_of(op->getOperands().begin(), op->getOperands().end(),\n                        [&valueToRemove](Value operand) {\n                          return tt::isTensorPointerType(operand.getType()) &&\n@@ -615,6 +631,77 @@ class TritonGPURewriteTensorPointerPass\n     return nullptr;\n   }\n \n+  Operation *rewriteIfOp(OpBuilder &builder, scf::IfOp op,\n+                         std::stack<Operation *> &eraser,\n+                         DenseSet<Value> &valueToRemove) {\n+    auto thenYieldOp = op.thenYield();\n+    assert(op.getNumResults() == thenYieldOp.getNumOperands());\n+    SmallVector<Value> results = thenYieldOp.getOperands();\n+\n+    // get new result types\n+    SmallVector<Type> newRetTypes;\n+    for (unsigned i = 0; i < results.size(); ++i) {\n+      if (!tt::isTensorPointerType(results[i].getType()) ||\n+          !valueToRemove.count(results[i])) {\n+        newRetTypes.push_back(results[i].getType());\n+        continue;\n+      }\n+      auto makeTensorPtrOp = getMakeTensorPtrOp(results[i]);\n+      assert(rewritedInfo.count(makeTensorPtrOp.getResult()));\n+      auto info = rewritedInfo[makeTensorPtrOp.getResult()];\n+      for (unsigned j = 0; j < info.length(); ++j) {\n+        newRetTypes.push_back(builder.getI64Type());\n+      }\n+    }\n+\n+    // create and clone new IfOp\n+    bool hasElse = !op.getElseRegion().empty();\n+    scf::IfOp newOp = builder.create<scf::IfOp>(op.getLoc(), newRetTypes,\n+                                                op.getCondition(), hasElse);\n+    IRMapping mapping;\n+    for (unsigned i = 0; i < op->getNumOperands(); ++i) {\n+      mapping.map(op->getOperand(i), newOp->getOperand(i));\n+    }\n+    auto rematerialize = [&](Block *block) {\n+      for (Operation &opInIf : block->getOperations()) {\n+        auto newOp = builder.clone(opInIf, mapping);\n+      }\n+    };\n+    builder.setInsertionPointToStart(newOp.thenBlock());\n+    rematerialize(op.thenBlock());\n+    if (hasElse) {\n+      builder.setInsertionPointToStart(newOp.elseBlock());\n+      rematerialize(op.elseBlock());\n+    }\n+\n+    // supported nested ops\n+    for (auto &[k, v] : mapping.getValueMap())\n+      if (valueToRemove.find(k) != valueToRemove.end())\n+        valueToRemove.insert(v);\n+\n+    // update rewritedInfo\n+    unsigned oldResIdx = 0, newResIdx = 0;\n+    while (oldResIdx < results.size()) {\n+      if (!tt::isTensorPointerType(results[oldResIdx].getType()) ||\n+          !valueToRemove.count(results[oldResIdx])) {\n+        oldResIdx++;\n+        newResIdx++;\n+      } else {\n+        auto makeTensorPtrOp = getMakeTensorPtrOp(results[oldResIdx]);\n+        assert(rewritedInfo.count(makeTensorPtrOp.getResult()));\n+        auto info = rewritedInfo[makeTensorPtrOp.getResult()];\n+        for (unsigned j = 0; j < info.length(); ++j) {\n+          info.setOffset(j, newOp->getResult(newResIdx++));\n+        }\n+        rewritedInfo[op.getResult(oldResIdx)] = info;\n+        oldResIdx++;\n+      }\n+    }\n+\n+    eraser.push(op);\n+    return newOp;\n+  }\n+\n   Operation *rewriteOp(Operation *op, std::stack<Operation *> &eraser,\n                        DenseSet<Value> &valueToRemove) {\n     OpBuilder builder(op);\n@@ -630,8 +717,6 @@ class TritonGPURewriteTensorPointerPass\n       return rewriteAdvanceOp(builder, advanceOp, eraser, valueToRemove);\n     } else if (isa<tt::LoadOp>(op) || isa<tt::StoreOp>(op)) {\n       return rewriteLoadStoreOp(builder, op, eraser, valueToRemove);\n-    } else if (auto storeOp = dyn_cast<tt::StoreOp>(op)) {\n-      return rewriteLoadStoreOp(builder, op, eraser, valueToRemove);\n     } else if (op->getDialect()->getNamespace() == \"scf\" ||\n                op->getDialect()->getNamespace() == \"cf\") {\n       if (!needRewrite(op, valueToRemove))\n@@ -641,9 +726,11 @@ class TritonGPURewriteTensorPointerPass\n         return rewriteForOp(builder, forOp, eraser, valueToRemove);\n       } else if (auto yieldOp = dyn_cast<scf::YieldOp>(op)) {\n         return rewriteYieldOp(builder, yieldOp, eraser, valueToRemove);\n+      } else if (auto ifOp = dyn_cast<scf::IfOp>(op)) {\n+        return rewriteIfOp(builder, ifOp, eraser, valueToRemove);\n       } else {\n         llvm_unreachable(\"Currently we only support tensor pointer usages \"\n-                         \"inside a `scf::ForOp`, others such as `scf::IfOp`,\"\n+                         \"inside a `scf::ForOp` or `scf::IfOp`, others such as \"\n                          \"`scf::WhileOp`, `cf::BranchOp` or `cf::CondBranchOp` \"\n                          \"are not supported yet\");\n       }"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 0, "deletions": 5, "changes": 5, "file_content_changes": "@@ -125,9 +125,6 @@ def static_persistent_tma_matmul_kernel(\n ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n def test_user_defined_persistent_non_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, TRANS_A, TRANS_B):\n-    # TODO: fix RewriteTensorPtrPass\n-    pytest.skip('RewriteTensorPtrPass issue')\n-\n     if (TRANS_A):\n         a = .1 * torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -458,8 +455,6 @@ def static_persistent_tma_warp_specialized_matmul_kernel(\n ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n def test_user_defined_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B):\n-    # TODO: fix RewriteTensorPtrPass\n-    pytest.skip('RewriteTensorPtrPass issue')\n     if (TRANS_A):\n         a = .1 * torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:"}, {"filename": "test/TritonGPU/rewrite-tensor-pointer.mlir", "status": "modified", "additions": 58, "deletions": 1, "changes": 59, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -tritongpu-rewrite-tensor-pointer | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -tritongpu-rewrite-tensor-pointer | FileCheck %s\n \n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [2, 2], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n@@ -62,3 +62,60 @@ module attributes {\"triton_gpu.compute-capability\" = 90 : i32, \"triton_gpu.num-c\n     tt.return\n   }\n }\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [2, 2], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0]}>\n+module attributes {\"triton_gpu.compute-capability\" = 90 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @if_for_if(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg4: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg5: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg6: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg7: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}) attributes {noinline = false} {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<64x1xf32, #blocked>\n+    %c63_i32 = arith.constant 63 : i32\n+    %c-16_i32 = arith.constant -16 : i32\n+    %c132_i32 = arith.constant 132 : i32\n+    %c0_i32 = arith.constant 0 : i32\n+    %c1_i64 = arith.constant 1 : i64\n+    %c64_i32 = arith.constant 64 : i32\n+    %0 = tt.get_program_id x : i32\n+    %1 = arith.addi %arg3, %c63_i32 : i32\n+    %2 = arith.divsi %1, %c64_i32 : i32\n+    %3 = arith.muli %0, %c64_i32 : i32\n+    %4 = arith.extsi %arg3 : i32 to i64\n+    %5 = arith.extsi %arg4 : i32 to i64\n+    %6 = arith.extsi %arg5 : i32 to i64\n+    // CHECK-NOT: tt.make_tensor_ptr\n+    %7 = tt.make_tensor_ptr %arg0, [%4, %5], [%6, %c1_i64], [%3, %c0_i32] {order = array<i32: 1, 0>} : <tensor<64x16xf16, #blocked>, 1>\n+    %8 = \"triton_gpu.cmpi\"(%2, %c132_i32) <{predicate = 5 : i64}> : (i32, i32) -> i1\n+    scf.if %8 {\n+      %9 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked1>\n+      %10 = tt.splat %arg7 : (i32) -> tensor<64x1xi32, #blocked>\n+      %11 = tt.splat %arg2 : (!tt.ptr<f32, 1>) -> tensor<64x1x!tt.ptr<f32, 1>, #blocked>\n+      %12 = scf.for %arg8 = %0 to %2 step %c132_i32 iter_args(%arg9 = %7) -> (!tt.ptr<tensor<64x16xf16, #blocked>, 1>)  : i32 {\n+        %13 = \"triton_gpu.cmpi\"(%arg8, %c132_i32) <{predicate = 5 : i64}> : (i32, i32) -> i1\n+        %14 = scf.if %13 -> (!tt.ptr<tensor<64x16xf16, #blocked>, 1>) {\n+          %25 = arith.subi %arg8, %0 : i32\n+          %26 = arith.muli %25, %c64_i32 : i32\n+          // CHECK-NOT: tt.advance\n+          %27 = tt.advance %arg9, [%26, %c-16_i32] : <tensor<64x16xf16, #blocked>, 1>\n+          scf.yield %27 : !tt.ptr<tensor<64x16xf16, #blocked>, 1>\n+        } else {\n+          scf.yield %arg9 : !tt.ptr<tensor<64x16xf16, #blocked>, 1>\n+        }\n+        %15 = arith.muli %arg8, %c64_i32 : i32\n+        %16 = tt.splat %15 : (i32) -> tensor<64xi32, #blocked1>\n+        %17 = arith.addi %9, %16 : tensor<64xi32, #blocked1>\n+        %18 = triton_gpu.convert_layout %17 : (tensor<64xi32, #blocked1>) -> tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+        %19 = tt.expand_dims %18 {axis = 1 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<64x1xi32, #blocked2>\n+        %20 = triton_gpu.convert_layout %19 : (tensor<64x1xi32, #blocked2>) -> tensor<64x1xi32, #blocked>\n+        %21 = arith.muli %20, %10 : tensor<64x1xi32, #blocked>\n+        %22 = tt.addptr %11, %21 : tensor<64x1x!tt.ptr<f32, 1>, #blocked>, tensor<64x1xi32, #blocked>\n+        %23 = triton_gpu.convert_layout %22 : (tensor<64x1x!tt.ptr<f32, 1>, #blocked>) -> tensor<64x1x!tt.ptr<f32, 1>, #blocked>\n+        %24 = triton_gpu.convert_layout %cst : (tensor<64x1xf32, #blocked>) -> tensor<64x1xf32, #blocked>\n+        tt.store %23, %24 {cache = 1 : i32, evict = 1 : i32} : tensor<64x1xf32, #blocked>\n+        scf.yield %14 : !tt.ptr<tensor<64x16xf16, #blocked>, 1>\n+      }\n+    }\n+    tt.return\n+  }\n+}"}]