[{"filename": "include/triton/Conversion/Passes.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -2,7 +2,6 @@\n #define TRITON_CONVERSION_PASSES_H\n \n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n "}, {"filename": "include/triton/Conversion/Passes.td", "status": "modified", "additions": 0, "deletions": 14, "changes": 14, "file_content_changes": "@@ -49,18 +49,4 @@ def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"\n     ];\n }\n \n-def TritonConvertArithToIndex : Pass<\"triton-convert-arith-to-index\", \"mlir::ModuleOp\"> {\n-\n-    let summary = \"Convert arith to index\";\n-    \n-    let constructor = \"mlir::triton::createTritonConvertArithToIndexPass()\";\n-\n-    let description = [{\n-      Convert arith operation on index values to corresponding ops in the index dialect.\n-      We need this because SCFToCF conversion currently generates arith ops on indices.\n-    }];\n-\n-    let dependentDialects = [\"mlir::index::IndexDialect\"];\n-}\n-\n #endif"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h", "status": "removed", "additions": 0, "deletions": 20, "changes": 20, "file_content_changes": "@@ -1,20 +0,0 @@\n-#ifndef TRITON_CONVERSION_ARITH_TO_INDEX_H\n-#define TRITON_CONVERSION_ARITH_TO_INDEX_H\n-\n-#include \"mlir/Conversion/LLVMCommon/TypeConverter.h\"\n-#include \"mlir/Transforms/DialectConversion.h\"\n-#include <memory>\n-\n-namespace mlir {\n-\n-class ModuleOp;\n-template <typename T> class OperationPass;\n-\n-namespace triton {\n-\n-std::unique_ptr<OperationPass<ModuleOp>> createTritonConvertArithToIndexPass();\n-\n-}\n-} // namespace mlir\n-\n-#endif\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ArithToIndexPass.cpp", "status": "removed", "additions": 0, "deletions": 90, "changes": 90, "file_content_changes": "@@ -1,90 +0,0 @@\n-#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n-#include \"mlir/Analysis/DataFlowFramework.h\"\n-#include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n-#include \"mlir/Conversion/ControlFlowToLLVM//ControlFlowToLLVM.h\"\n-#include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n-#include \"mlir/Conversion/LLVMCommon/VectorPattern.h\"\n-#include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n-#include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n-#include \"mlir/Dialect/Index/IR/IndexDialect.h\"\n-#include \"mlir/Dialect/Index/IR/IndexOps.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n-#include \"mlir/Pass/Pass.h\"\n-#include \"triton/Dialect/Triton/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-\n-using namespace mlir;\n-using namespace mlir::triton;\n-\n-#define GEN_PASS_CLASSES\n-#include \"triton/Conversion/Passes.h.inc\"\n-\n-namespace {\n-class TritonArithToIndexConversionTarget : public mlir::ConversionTarget {\n-public:\n-  static bool hasIndexResultOrOperand(Operation *op) {\n-    if (!op)\n-      return false;\n-    bool hasRetIndex = llvm::find_if(op->getResultTypes(), [](Type type) {\n-                         return type.isIndex();\n-                       }) != op->getResultTypes().end();\n-    bool hasArgIndex = llvm::find_if(op->getOperandTypes(), [](Type type) {\n-                         return type.isIndex();\n-                       }) != op->getOperandTypes().end();\n-    return !hasRetIndex && !hasArgIndex;\n-  }\n-\n-  explicit TritonArithToIndexConversionTarget(MLIRContext &ctx)\n-      : ConversionTarget(ctx) {\n-    addLegalDialect<index::IndexDialect>();\n-    addDynamicallyLegalDialect<arith::ArithDialect>(hasIndexResultOrOperand);\n-  }\n-};\n-\n-template <class SrcOp, class DstOp>\n-LogicalResult replaceArithWithIndex(SrcOp op, PatternRewriter &rewriter) {\n-  // if (!hasIndexResultOrOperand(&*op))\n-  //   return failure();\n-  rewriter.replaceOpWithNewOp<DstOp>(op, op->getResultTypes(),\n-                                     op->getOperands(), op->getAttrs());\n-  return success();\n-}\n-\n-LogicalResult replaceArithCmpWithIndexCmp(arith::CmpIOp op,\n-                                          PatternRewriter &rewriter) {\n-  // if (!hasIndexResultOrOperand(&*op))\n-  //   return failure();\n-  rewriter.replaceOpWithNewOp<index::CmpOp>(\n-      op, op.getResult().getType(), (index::IndexCmpPredicate)op.getPredicate(),\n-      op.getOperand(0), op.getOperand(1));\n-  return success();\n-}\n-\n-class ArithToIndex : public TritonConvertArithToIndexBase<ArithToIndex> {\n-public:\n-  void runOnOperation() override {\n-    MLIRContext *context = &getContext();\n-    ModuleOp mod = getOperation();\n-    TritonArithToIndexConversionTarget target(*context);\n-    RewritePatternSet patterns(context);\n-    patterns.add(replaceArithWithIndex<arith::IndexCastOp, index::CastSOp>);\n-    patterns.add(replaceArithWithIndex<arith::ConstantOp, index::ConstantOp>);\n-    patterns.add(replaceArithWithIndex<arith::AddIOp, index::AddOp>);\n-    patterns.add(replaceArithCmpWithIndexCmp);\n-    if (failed(applyPartialConversion(mod, target, std::move(patterns)))) {\n-      return signalPassFailure();\n-    }\n-  }\n-};\n-} // namespace\n-\n-namespace mlir {\n-namespace triton {\n-\n-std::unique_ptr<OperationPass<ModuleOp>> createTritonConvertArithToIndexPass() {\n-  return std::make_unique<::ArithToIndex>();\n-}\n-\n-} // namespace triton\n-} // namespace mlir\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,5 +1,4 @@\n add_mlir_conversion_library(TritonGPUToLLVM\n-    ArithToIndexPass.cpp\n     ConvertLayoutOpToLLVM.cpp\n     DotOpToLLVM.cpp\n     ElementwiseOpToLLVM.cpp"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 29, "deletions": 4, "changes": 33, "file_content_changes": "@@ -686,9 +686,12 @@ class CFBranchPattern : public OpConversionPattern<cf::BranchOp> {\n   LogicalResult\n   matchAndRewrite(cf::BranchOp op, cf::BranchOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    addNamedAttrs(rewriter.replaceOpWithNewOp<cf::BranchOp>(\n-                      op, op.getSuccessor(), adaptor.getOperands()),\n-                  adaptor.getAttributes());\n+    auto converter = getTypeConverter();\n+    auto newOp = rewriter.replaceOpWithNewOp<cf::BranchOp>(\n+        op, op.getSuccessor(), adaptor.getOperands());\n+    if (failed(rewriter.convertRegionTypes(newOp.getSuccessor()->getParent(),\n+                                           *converter)))\n+      return failure();\n     return success();\n   }\n };\n@@ -713,14 +716,36 @@ class CFCondBranchPattern : public OpConversionPattern<cf::CondBranchOp> {\n     if (failed(rewriter.convertRegionTypes(newOp.getFalseDest()->getParent(),\n                                            *converter)))\n       return failure();\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+};\n+\n+class FuncOpPattern : public OpConversionPattern<func::FuncOp> {\n+public:\n+  using OpConversionPattern<func::FuncOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(func::FuncOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto converter = getTypeConverter();\n+    auto newOp = rewriter.replaceOpWithNewOp<func::FuncOp>(\n+        op, op.getName(), op.getFunctionType());\n+    addNamedAttrs(newOp, adaptor.getAttributes());\n+    rewriter.inlineRegionBefore(op.getBody(), newOp.getBody(),\n+                                newOp.getBody().end());\n+    if (failed(rewriter.convertRegionTypes(&newOp.getBody(), *converter)))\n+      return failure();\n+\n     return success();\n   }\n };\n \n void populateCFPatterns(TritonGPUTypeConverter &typeConverter,\n                         RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n-  patterns.add<CFBranchPattern, CFCondBranchPattern>(typeConverter, context);\n+  patterns.add<FuncOpPattern, CFCondBranchPattern, CFBranchPattern>(\n+      typeConverter, context);\n }\n //\n "}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -946,6 +946,12 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n                                                 view.getResult());\n     return mlir::success();\n   }\n+  // cvt(cat) -> cat\n+  if (auto cat = dyn_cast<triton::CatOp>(arg)) {\n+    rewriter.replaceOpWithNewOp<triton::CatOp>(op, op->getResult(0).getType(),\n+                                               cat.getOperands());\n+    return mlir::success();\n+  }\n   // cvt(alloc_tensor(x), type2) -> alloc_tensor(x, type2)\n   auto alloc_tensor = dyn_cast<triton::gpu::AllocTensorOp>(arg);\n   if (alloc_tensor) {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -606,7 +606,7 @@ class TritonGPURemoveLayoutConversionsPass\n     patterns.add<DecomposeDotOperand>(context);\n     patterns.add<ConvertDotConvert>(context);\n \n-    if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n+    if (mlir::applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n       signalPassFailure();\n     }\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 16, "deletions": 11, "changes": 27, "file_content_changes": "@@ -14,11 +14,10 @@ using namespace mlir::triton::gpu;\n TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n                                                int numWarps)\n     : context(context), numWarps(numWarps) {\n-  // TODO: how does MLIR pick the right conversion?\n   addConversion([](Type type) { return type; });\n   addConversion([this](RankedTensorType tensorType) -> RankedTensorType {\n     // types with encoding are already in the right format\n-    // TODO: check for layout encodings specifically\n+    // TODO: check for layout encodings more specifically\n     if (tensorType.getEncoding())\n       return tensorType;\n     // pessimistic values for attributes:\n@@ -41,16 +40,19 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n   // This will create newArg, and map(origArg, newArg)\n   addArgumentMaterialization([&](OpBuilder &builder,\n                                  RankedTensorType tensorType, ValueRange inputs,\n-                                 Location loc) {\n-    llvm_unreachable(\"Argument rematerialization not implemented\");\n+                                 Location loc) -> llvm::Optional<Value> {\n+    llvm_unreachable(\"Argument rematerialization should not happen in Triton \"\n+                     \"-> TritonGPU conversion\");\n     return std::nullopt;\n   });\n \n   // If the origValue still has live user(s), use this to\n   // convert origValue to newValue\n   addSourceMaterialization([&](OpBuilder &builder, RankedTensorType tensorType,\n-                               ValueRange inputs, Location loc) {\n-    llvm_unreachable(\"Source rematerialization not implemented\");\n+                               ValueRange inputs,\n+                               Location loc) -> llvm::Optional<Value> {\n+    llvm_unreachable(\"Source rematerialization should not happen in Triton -> \"\n+                     \"TritonGPU Conversion\");\n     return std::nullopt;\n   });\n \n@@ -62,9 +64,6 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n     auto cast =\n         builder.create<triton::gpu::ConvertLayoutOp>(loc, tensorType, inputs);\n     return Optional<Value>(cast.getResult());\n-    // return Optional<Value>(cast.getResult(0));\n-    // llvm_unreachable(\"Not implemented\");\n-    // return std::nullopt;\n   });\n }\n \n@@ -82,10 +81,16 @@ TritonGPUConversionTarget::TritonGPUConversionTarget(\n                scf::ReduceReturnOp>();\n \n   addDynamicallyLegalDialect<arith::ArithDialect, math::MathDialect,\n-                             triton::TritonDialect, scf::SCFDialect>(\n+                             func::FuncDialect, triton::TritonDialect,\n+                             cf::ControlFlowDialect, scf::SCFDialect>(\n       [&](Operation *op) {\n-        if (typeConverter.isLegal(op))\n+        bool hasLegalRegions = true;\n+        for (auto &region : op->getRegions()) {\n+          hasLegalRegions = hasLegalRegions && typeConverter.isLegal(&region);\n+        }\n+        if (hasLegalRegions && typeConverter.isLegal(op)) {\n           return true;\n+        }\n         return false;\n       });\n "}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -12,7 +12,6 @@\n #include \"mlir/Target/LLVMIR/Export.h\"\n #include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n #include \"mlir/Transforms/Passes.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Tools/Sys/GetEnv.hpp\"\n #include \"llvm/ADT/APInt.h\"\n@@ -261,7 +260,7 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module) {\n   }\n \n   auto optPipeline = mlir::makeOptimizingTransformer(\n-      /*optLevel=*/0, /*sizeLevel=*/0,\n+      /*optLevel=*/3, /*sizeLevel=*/0,\n       /*targetMachine=*/nullptr);\n \n   if (auto err = optPipeline(llvmModule.get())) {\n@@ -296,7 +295,6 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n       /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n   pm.addPass(mlir::createConvertSCFToCFPass());\n-  pm.addPass(createTritonConvertArithToIndexPass());\n   pm.addPass(mlir::createConvertIndexToLLVMPass());\n   pm.addPass(createConvertTritonGPUToLLVMPass(computeCapability));\n   pm.addPass(mlir::createArithToLLVMConversionPass());"}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -69,7 +69,7 @@ def get_llvm_package_info():\n     use_assert_enabled_llvm = check_env_flag(\"TRITON_USE_ASSERT_ENABLED_LLVM\", \"False\")\n     release_suffix = \"assert\" if use_assert_enabled_llvm else \"release\"\n     name = f'llvm+mlir-17.0.0-x86_64-{system_suffix}-{release_suffix}'\n-    version = \"llvm-17.0.0-8e5a41e8271f\"\n+    version = \"llvm-17.0.0-2538e550420f\"\n     url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n     return Package(\"llvm\", name, url, \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n "}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -56,7 +56,7 @@ def nvsmi(attrs):\n     'a100': {\n         (512, 512, 512): {'float16': 0.08, 'float32': 0.13, 'int8': 0.05},\n         (1024, 1024, 1024): {'float16': 0.33, 'float32': 0.35, 'int8': 0.169},\n-        (2048, 2048, 2048): {'float16': 0.59, 'float32': 0.57, 'int8': 0.34},\n+        (2048, 2048, 2048): {'float16': 0.62, 'float32': 0.57, 'int8': 0.34},\n         (4096, 4096, 4096): {'float16': 0.81, 'float32': 0.75, 'int8': 0.46},\n         (8192, 8192, 8192): {'float16': 0.77, 'float32': 0.85, 'int8': 0.51},\n         # tall-skinny"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 29, "deletions": 18, "changes": 47, "file_content_changes": "@@ -520,6 +520,17 @@ def test_unary_op(dtype_x, expr, device='cuda'):\n def test_math_op(expr, device='cuda'):\n     _test_unary('float32', f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n \n+# ----------------\n+# test abs\n+# ----------------\n+\n+\n+@pytest.mark.parametrize(\"dtype_x\", [\n+    (dtype_x)\n+    for dtype_x in dtypes_with_bfloat16\n+])\n+def test_abs(dtype_x, device='cuda'):\n+    _test_unary(dtype_x, 'tl.abs(x)', 'np.abs(x) ', device=device)\n \n # ----------------\n # test indexing\n@@ -1816,11 +1827,11 @@ def _kernel(dst):\n \n \n @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n-                         [('int32', 'libdevice.ffs', ''),\n-                          ('float32', 'libdevice.log2', ''),\n-                          ('float32', 'libdevice.pow', tl.libdevice.LIBDEVICE_PATH),\n-                          ('float64', 'libdevice.norm4d', '')])\n-def test_libdevice_tensor(dtype_str, expr, lib_path):\n+                         [('int32', 'math.ffs', ''),\n+                          ('float32', 'math.log2', ''),\n+                          ('float32', 'math.pow', tl.math.LIBDEVICE_PATH),\n+                          ('float64', 'math.norm4d', '')])\n+def test_math_tensor(dtype_str, expr, lib_path):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -1833,37 +1844,37 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n \n-    if expr == 'libdevice.log2':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.broadcast_to(tl.libdevice.log2(5.0), x.shape)'})\n+    if expr == 'math.log2':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.broadcast_to(tl.math.log2(5.0), x.shape)'})\n         y_ref = np.log2(5.0)\n-    elif expr == 'libdevice.ffs':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.ffs(x)'})\n+    elif expr == 'math.ffs':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.ffs(x)'})\n         y_ref = np.zeros(shape, dtype=x.dtype)\n         for i in range(shape[0]):\n             y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n-    elif expr == 'libdevice.pow':\n+    elif expr == 'math.pow':\n         # numpy does not allow negative factors in power, so we use abs()\n         x = np.abs(x)\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.pow(x, x)'})\n         y_ref = np.power(x, x)\n-    elif expr == 'libdevice.norm4d':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.norm4d(x, x, x, x)'})\n+    elif expr == 'math.norm4d':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.norm4d(x, x, x, x)'})\n         y_ref = np.sqrt(4 * np.power(x, 2))\n \n     x_tri = to_triton(x)\n     # triton result\n     y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n     # compare\n-    if expr == 'libdevice.ffs':\n+    if expr == 'math.ffs':\n         np.testing.assert_equal(y_ref, to_numpy(y_tri))\n     else:\n         np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n \n \n @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n-                         [('float32', 'libdevice.pow', '')])\n-def test_libdevice_scalar(dtype_str, expr, lib_path):\n+                         [('float32', 'math.pow', '')])\n+def test_math_scalar(dtype_str, expr, lib_path):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -1879,13 +1890,13 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n \n     # numpy does not allow negative factors in power, so we use abs()\n     x = np.abs(x)\n-    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.pow(x, x)'})\n     y_ref[:] = np.power(x, x)\n \n     # triton result\n     x_tri = to_triton(x)[0].item()\n     y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n-    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'math': lib_path})\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n "}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 16, "deletions": 12, "changes": 28, "file_content_changes": "@@ -150,10 +150,10 @@ def local_lookup(name: str, absent):\n                 self.global_uses[name] = value\n             return value\n \n-        lookup_order = local_lookup, self.gscope.get, self.builtin_namespace.get\n         absent_marker = object()\n \n         def name_lookup(name: str) -> Any:\n+            lookup_order = local_lookup, self.gscope.get, self.builtin_namespace.get\n             absent = absent_marker\n             for lookup_function in lookup_order:\n                 value = lookup_function(name, absent)\n@@ -201,7 +201,11 @@ def contains_return_op(self, node):\n         elif isinstance(node, ast.Call):\n             fn = self.visit(node.func)\n             if isinstance(fn, triton.JITFunction):\n-                return self.contains_return_op(fn.parse())\n+                old_gscope = self.gscope\n+                self.gscope = sys.modules[fn.fn.__module__].__dict__\n+                ret = self.contains_return_op(fn.parse())\n+                self.gscope = old_gscope\n+                return ret\n             return False\n         elif isinstance(node, ast.If):\n             pred = lambda s: self.contains_return_op(s)\n@@ -337,11 +341,13 @@ def visit_Assign(self, node):\n             names = [names]\n         if not isinstance(values, tuple):\n             values = [values]\n+        native_nontensor_types = (triton.language.dtype, )\n         for name, value in zip(names, values):\n             # by default, constexpr are assigned into python variable\n             if isinstance(value, triton.language.constexpr):\n                 value = value.value\n-            if not isinstance(value, triton.language.tensor):\n+            if not isinstance(value, triton.language.tensor) and \\\n+               not isinstance(value, native_nontensor_types):\n                 value = triton.language.core._to_tensor(value, self.builder)\n             self.set_value(name, value)\n \n@@ -695,14 +701,15 @@ def visit_For(self, node):\n         iv_type = triton.language.semantic.integer_promote_impl(lb.dtype, ub.dtype)\n         iv_type = triton.language.semantic.integer_promote_impl(iv_type, step.dtype)\n         iv_ir_type = iv_type.to_ir(self.builder)\n+        iv_is_signed = iv_type.int_signedness == triton.language.core.dtype.SIGNEDNESS.SIGNED\n         # lb/ub/step might be constexpr, we need to cast them to tensor\n         lb = lb.handle\n         ub = ub.handle\n         step = step.handle\n         # ForOp can only accept IndexType as lb/ub/step. Cast integer to Index\n-        lb = self.builder.create_to_index(lb)\n-        ub = self.builder.create_to_index(ub)\n-        step = self.builder.create_to_index(step)\n+        lb = self.builder.create_int_cast(lb, iv_ir_type, iv_is_signed)\n+        ub = self.builder.create_int_cast(ub, iv_ir_type, iv_is_signed)\n+        step = self.builder.create_int_cast(step, iv_ir_type, iv_is_signed)\n         # Create placeholder for the loop induction variable\n         iv = self.builder.create_undef(iv_ir_type)\n         self.set_value(node.target.id, triton.language.core.tensor(iv, iv_type))\n@@ -761,12 +768,9 @@ def visit_For(self, node):\n \n             # update induction variable with actual value, and replace all uses\n             self.builder.set_insertion_point_to_start(for_op.get_body(0))\n-            iv = self.builder.create_index_to_si(for_op.get_induction_var())\n-            iv = self.builder.create_int_cast(iv, iv_ir_type, True)\n+            iv = for_op.get_induction_var()\n             if negative_step:\n-                ub_si = self.builder.create_index_to_si(ub)\n-                ub_si = self.builder.create_int_cast(ub_si, iv_ir_type, True)\n-                iv = self.builder.create_sub(ub_si, iv)\n+                iv = self.builder.create_sub(ub, iv)\n             self.lscope[node.target.id].handle.replace_all_uses_with(iv)\n             self.set_value(node.target.id, triton.language.core.tensor(iv, iv_type))\n \n@@ -1420,7 +1424,7 @@ def format_of(ty):\n \n \n def default_cache_dir():\n-    return os.path.join(os.environ[\"HOME\"], \".triton\", \"cache\")\n+    return os.path.join(Path.home(), \".triton\", \"cache\")\n \n \n class CacheManager:"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -5,7 +5,7 @@\n     ir,\n     builtin,\n )\n-from . import libdevice\n+from . import math\n from .core import (\n     abs,\n     arange,\n@@ -141,7 +141,7 @@\n     \"int64\",\n     \"int8\",\n     \"ir\",\n-    \"libdevice\",\n+    \"math\",\n     \"load\",\n     \"log\",\n     \"max\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 10, "deletions": 1, "changes": 11, "file_content_changes": "@@ -1216,7 +1216,16 @@ def max_contiguous(input, values, _builder=None):\n \n @triton.jit\n def abs(x):\n-    return where(x >= 0, x, -x)\n+    x_dtype = x.dtype\n+    if x_dtype.is_floating():\n+        num_bits: constexpr = x.dtype.primitive_bitwidth\n+        int_dtype = dtype(f'int{num_bits}')\n+        mask = 2 ** (num_bits - 1) - 1\n+        ret = x.to(int_dtype, bitcast=True) & mask.to(int_dtype)\n+        ret = ret.to(x_dtype, bitcast=True)\n+    else:\n+        ret = where(x >= 0, x, -x)\n+    return ret\n \n \n @triton.jit"}, {"filename": "python/triton/language/math.py", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "python/triton/language/libdevice.py"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1193,14 +1193,14 @@ def xor_sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n def umulhi(x: tl.tensor, y: tl.tensor, builder: ir.builder) -> tl.tensor:\n     x, y = binary_op_type_checking_impl(x, y, builder)\n     # FIXME(Keren): not portable, should be fixed\n-    from . import libdevice\n-    return libdevice.mulhi(x, y, _builder=builder)\n+    from . import math\n+    return math.mulhi(x, y, _builder=builder)\n \n \n def floor(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     # FIXME(Keren): not portable, should be fixed\n-    from . import libdevice\n-    return libdevice.floor(x, _builder=builder)\n+    from . import math\n+    return math.floor(x, _builder=builder)\n \n \n def exp(x: tl.tensor, builder: ir.builder) -> tl.tensor:"}]