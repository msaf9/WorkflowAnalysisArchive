[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 23, "deletions": 19, "changes": 42, "file_content_changes": "@@ -3218,8 +3218,12 @@ class MMA16816SmemLoader {\n     cMatShape = matShape[order[0]];\n     sMatShape = matShape[order[1]];\n \n-    cTileStride = smemStrides[order[0]];\n-    sTileStride = smemStrides[order[1]];\n+    // TODO[Superjomn]: Seems the smemStrides not works here(the master branch\n+    // will fail too).\n+    // cStride = smemStrides[order[0]];\n+    // sStride = smemStrides[order[1]];\n+    cStride = i32_val(tileShape[order[0]]);\n+    sStride = i32_val(tileShape[order[1]]);\n \n     // rule: k must be the fast-changing axis.\n     needTrans = kOrder != order[0];\n@@ -3322,7 +3326,7 @@ class MMA16816SmemLoader {\n     for (int i = 0; i < numPtr; ++i) {\n       Value cMatOffI = add(cMatOff, i32_val(i * pLoadStrideInMat));\n       cMatOffI = xor_(cMatOffI, phase);\n-      offs[i] = add(mul(cMatOffI, i32_val(cMatShape)), mul(sOff, sTileStride));\n+      offs[i] = add(mul(cMatOffI, i32_val(cMatShape)), mul(sOff, sStride));\n     }\n \n     return offs;\n@@ -3358,7 +3362,7 @@ class MMA16816SmemLoader {\n         Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n         cOff = urem(cOff, i32_val(tileShape[order[0]]));\n         sOff = urem(sOff, i32_val(tileShape[order[1]]));\n-        offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, sTileStride));\n+        offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, sStride));\n       }\n     }\n     return offs;\n@@ -3398,7 +3402,7 @@ class MMA16816SmemLoader {\n           // To prevent out-of-bound access when tile is too small.\n           cOff = urem(cOff, i32_val(tileShape[order[0]]));\n           sOff = urem(sOff, i32_val(tileShape[order[1]]));\n-          offs[ptrOff] = add(cOff, mul(sOff, sTileStride));\n+          offs[ptrOff] = add(cOff, mul(sOff, sStride));\n         }\n       }\n     }\n@@ -3433,7 +3437,7 @@ class MMA16816SmemLoader {\n \n     if (canUseLdmatrix) {\n       Value sOffset =\n-          mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sTileStride);\n+          mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sStride);\n       Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n \n       PTXBuilder builder;\n@@ -3467,10 +3471,10 @@ class MMA16816SmemLoader {\n       Value ptr2 = getPtr(ptrIdx + 1);\n       assert(sMatStride == 1);\n       int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n-      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sTileStride);\n+      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n       int sOffsetArrElem = sMatStride * sMatShape;\n       Value sOffsetArrElemVal =\n-          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sTileStride));\n+          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n \n       Value elems[4];\n       Type elemTy = type::f32Ty(ctx);\n@@ -3510,10 +3514,10 @@ class MMA16816SmemLoader {\n \n       assert(sMatStride == 1);\n       int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n-      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sTileStride);\n+      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n       int sOffsetArrElem = 1 * (sMatStride * sMatShape);\n       Value sOffsetArrElemVal =\n-          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sTileStride));\n+          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n \n       std::array<Value, 4> i8v4Elems;\n       std::array<Value, 4> i32Elems;\n@@ -3581,8 +3585,8 @@ class MMA16816SmemLoader {\n   int cMatShape;\n   int sMatShape;\n \n-  Value cTileStride;\n-  Value sTileStride;\n+  Value cStride;\n+  Value sStride;\n \n   bool needTrans;\n   bool canUseLdmatrix;\n@@ -4224,7 +4228,8 @@ struct MMA16816ConversionHelper {\n       loadFn = getLoadMatrixFn(\n           tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n           1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShape*/,\n-          {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/);\n+          {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/,\n+          true /*isA*/);\n     } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n       // load from registers, used in gemm fuse\n       // TODO(Superjomn) Port the logic.\n@@ -4255,7 +4260,8 @@ struct MMA16816ConversionHelper {\n     auto loadFn = getLoadMatrixFn(\n         tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n         0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShape*/,\n-        {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/);\n+        {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/,\n+        false /*isA*/);\n \n     for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n       for (int k = 0; k < numRepK; ++k)\n@@ -4369,7 +4375,7 @@ struct MMA16816ConversionHelper {\n   getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n                   MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder,\n                   ArrayRef<int> instrShape, ArrayRef<int> matShape,\n-                  Value warpId, ValueTable &vals) const {\n+                  Value warpId, ValueTable &vals, bool isA) const {\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n     // We assumes that the input operand of Dot should be from shared layout.\n     // TODO(Superjomn) Consider other layouts if needed later.\n@@ -4379,8 +4385,6 @@ struct MMA16816ConversionHelper {\n     const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n     auto order = sharedLayout.getOrder();\n \n-    bool needTrans = kOrder != order[0];\n-\n     // the original register_lds2, but discard the prefetch logic.\n     auto ld2 = [](ValueTable &vals, int mn, int k, Value val) {\n       vals[{mn, k}] = val;\n@@ -4406,7 +4410,7 @@ struct MMA16816ConversionHelper {\n           (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n           ptrs, helper.getMatType(), helper.getShemPtrTy());\n \n-      if (!needTrans) {\n+      if (isA) {\n         ld2(vals, a, b, ha0);\n         ld2(vals, a + 1, b, ha1);\n         ld2(vals, a, b + 1, ha2);\n@@ -5996,7 +6000,7 @@ struct AtomicRMWOpConversion\n         return failure();\n       }\n       atom.o(rmwOp).o(sTy);\n-      //TODO:[dongdongl] actual mask support\n+      // TODO:[dongdongl] actual mask support\n       Value pred = int_val(1, 1);\n       atom(dstOpr, ptrOpr, valOpr).predicate(pred);\n       auto ret = ptxBuilder.launch(rewriter, loc, valueElemTy);"}]