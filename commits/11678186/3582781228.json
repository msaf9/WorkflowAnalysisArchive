[{"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 14, "deletions": 16, "changes": 30, "file_content_changes": "@@ -42,10 +42,6 @@ using ::mlir::triton::gpu::SharedEncodingAttr;\n // Forward declaration for functions from TritonGPUToLLVM.cpp\n void llPrintf(StringRef msg, ValueRange args,\n               ConversionPatternRewriter &rewriter);\n-#define vprintf llPrintf\n-\n-void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n-                   std::string elem_repr, ConversionPatternRewriter &builder);\n \n // Helper for conversion of DotOp with mma<version=1>, that is sm<80\n struct DotOpMmaV1ConversionHelper {\n@@ -970,12 +966,13 @@ struct MMA16816ConversionHelper {\n     if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n       Value warpM = getWarpM(shape[0]);\n       // load from smem\n-      int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / matShapeM);\n-      loadFn = getLoadMatrixFn(\n-          tensor, smemObj, mmaLayout, wpt /*wpt*/,\n-          1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShape*/,\n-          {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/,\n-          true /*isA*/);\n+      int wpt =\n+          std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / matShapeM);\n+      loadFn =\n+          getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/,\n+                          {mmaInstrM, mmaInstrK} /*instrShape*/,\n+                          {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/,\n+                          ha /*vals*/, true /*isA*/);\n     } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n       // load from registers, used in gemm fuse\n       // TODO(Superjomn) Port the logic.\n@@ -1014,12 +1011,13 @@ struct MMA16816ConversionHelper {\n     int numRepN = getNumRepN(tensorTy, shape[1]);\n \n     Value warpN = getWarpN(shape[1]);\n-    int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / matShapeN);\n-    auto loadFn = getLoadMatrixFn(\n-        tensor, smemObj, mmaLayout,  wpt /*wpt*/,\n-        0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShape*/,\n-        {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/,\n-        false /*isA*/);\n+    int wpt =\n+        std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / matShapeN);\n+    auto loadFn =\n+        getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/,\n+                        {mmaInstrK, mmaInstrN} /*instrShape*/,\n+                        {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/,\n+                        hb /*vals*/, false /*isA*/);\n \n     for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n       for (int k = 0; k < numRepK; ++k)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 27, "deletions": 40, "changes": 67, "file_content_changes": "@@ -64,22 +64,9 @@ static StringRef getStructAttrsAttrName() { return \"llvm.struct_attrs\"; }\n // A helper function for using printf in LLVM conversion.\n void llPrintf(StringRef msg, ValueRange args,\n               ConversionPatternRewriter &rewriter);\n-#define vprintf llPrintf\n-void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n-                   std::string elem_repr, ConversionPatternRewriter &builder) {\n-  std::string fmt = info + \" t-%d \";\n-  std::vector<Value> new_arr({thread});\n-  for (auto v : arr) {\n-    fmt += elem_repr + \", \";\n-    new_arr.push_back(v);\n-  }\n-  fmt += \"\";\n-\n-  vprintf(fmt, new_arr, builder);\n-}\n \n // Helper function\n-#define llprintf(fmt, ...) LLVM::llPrintf(fmt, {__VA_ARGS__}, rewriter)\n+#define vprintf(fmt, ...) LLVM::llPrintf(fmt, {__VA_ARGS__}, rewriter)\n \n } // namespace LLVM\n } // namespace mlir\n@@ -657,7 +644,6 @@ class ConvertTritonGPUOpToLLVMPattern\n     return multiDimIdx;\n   }\n \n-\n   struct SmallVectorKeyInfo {\n     static unsigned getHashValue(const SmallVector<unsigned> &key) {\n       return llvm::hash_combine_range(key.begin(), key.end());\n@@ -685,7 +671,7 @@ class ConvertTritonGPUOpToLLVMPattern\n         emitIndices(loc, rewriter, parent, sliceLayout.paddedShape(shape));\n     unsigned numIndices = parentIndices.size();\n     SmallVector<SmallVector<Value>> resultIndices;\n-    for (unsigned i = 0; i < numIndices; ++i){\n+    for (unsigned i = 0; i < numIndices; ++i) {\n       SmallVector<Value> indices = parentIndices[i];\n       indices.erase(indices.begin() + dim);\n       resultIndices.push_back(indices);\n@@ -1252,14 +1238,14 @@ struct BroadcastOpConversion\n     auto resultOffsets = emitOffsetForLayout(resultLayout, resultShape);\n     SmallVector<Value> srcVals = getElementsFromStruct(loc, src, rewriter);\n     DenseMap<SmallVector<unsigned>, Value, SmallVectorKeyInfo> srcValues;\n-    for(size_t i = 0; i < srcOffsets.size(); i++){\n+    for (size_t i = 0; i < srcOffsets.size(); i++) {\n       srcValues[srcOffsets[i]] = srcVals[i];\n     }\n     SmallVector<Value> resultVals;\n-    for(size_t i = 0; i < resultOffsets.size(); i++) {\n+    for (size_t i = 0; i < resultOffsets.size(); i++) {\n       auto offset = resultOffsets[i];\n-      for(size_t j = 0; j < srcShape.size(); j++)\n-        if(srcShape[j]==1)\n+      for (size_t j = 0; j < srcShape.size(); j++)\n+        if (srcShape[j] == 1)\n           offset[j] = 0;\n       resultVals.push_back(srcValues.lookup(offset));\n     }\n@@ -1993,8 +1979,8 @@ struct MakeRangeOpConversion\n     unsigned elems = idxs.size();\n     SmallVector<Value> retVals(elems);\n     // TODO: slice layout has more elements than expected.\n-    // Unexpected behavior for make range, but genereally ok when followed by expand dims + broadcast.\n-    // very weird behavior otherwise potentially.\n+    // Unexpected behavior for make range, but genereally ok when followed by\n+    // expand dims + broadcast. very weird behavior otherwise potentially.\n     for (const auto multiDim : llvm::enumerate(idxs)) {\n       assert(multiDim.value().size() == 1);\n       retVals[multiDim.index()] = add(multiDim.value()[0], start);\n@@ -2700,13 +2686,13 @@ struct ConvertLayoutOpConversion\n     }\n     // dot_op<opIdx=0, parent=#mma> = #mma\n     // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n-    if(srcLayout.isa<MmaEncodingAttr>() &&\n+    if (srcLayout.isa<MmaEncodingAttr>() &&\n         dstLayout.isa<DotOperandEncodingAttr>()) {\n       auto srcMmaLayout = srcLayout.cast<MmaEncodingAttr>();\n       auto dstDotLayout = dstLayout.cast<DotOperandEncodingAttr>();\n-      if(srcMmaLayout.getWarpsPerCTA()[1] == 1 &&\n-         dstDotLayout.getOpIdx() == 0 &&\n-         dstDotLayout.getParent() == srcMmaLayout) {\n+      if (srcMmaLayout.getWarpsPerCTA()[1] == 1 &&\n+          dstDotLayout.getOpIdx() == 0 &&\n+          dstDotLayout.getParent() == srcMmaLayout) {\n         // get source values\n         Location loc = op->getLoc();\n         auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n@@ -2715,35 +2701,37 @@ struct ConvertLayoutOpConversion\n             this->getTypeConverter()->convertType(srcTy.getElementType());\n         // for the destination type, we need to pack values together\n         // so they can be consumed by tensor core operations\n-        unsigned vecSize = std::max<unsigned>(32 / elemTy.getIntOrFloatBitWidth(), 1);\n+        unsigned vecSize =\n+            std::max<unsigned>(32 / elemTy.getIntOrFloatBitWidth(), 1);\n         Type vecTy = vec_ty(elemTy, vecSize);\n-        SmallVector<Type> types(elems/vecSize, vecTy);\n+        SmallVector<Type> types(elems / vecSize, vecTy);\n         SmallVector<Value> vecVals;\n-        for(unsigned i = 0; i < elems; i += vecSize) {\n+        for (unsigned i = 0; i < elems; i += vecSize) {\n           Value packed = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n-          for(unsigned j = 0; j < vecSize; j++)\n-            packed = insert_element(vecTy, packed, vals[i+j], i32_val(j));\n+          for (unsigned j = 0; j < vecSize; j++)\n+            packed = insert_element(vecTy, packed, vals[i + j], i32_val(j));\n           vecVals.push_back(packed);\n         }\n-    \n+\n         // This needs to be ordered the same way that\n         // ldmatrix.x4 would order it\n         // TODO: this needs to be refactor so we don't\n         // implicitly depends on how emitOffsetsForMMAV2\n         // is implemented\n         SmallVector<Value> reorderedVals;\n-        for(unsigned i = 0; i < vecVals.size(); i += 4) {\n+        for (unsigned i = 0; i < vecVals.size(); i += 4) {\n           reorderedVals.push_back(vecVals[i]);\n-          reorderedVals.push_back(vecVals[i+2]);\n-          reorderedVals.push_back(vecVals[i+1]);\n-          reorderedVals.push_back(vecVals[i+3]);\n+          reorderedVals.push_back(vecVals[i + 2]);\n+          reorderedVals.push_back(vecVals[i + 1]);\n+          reorderedVals.push_back(vecVals[i + 3]);\n         }\n \n         // return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n \n-\n-        Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n-        Value view = getStructFromElements(loc, reorderedVals, rewriter, structTy);\n+        Type structTy =\n+            LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n+        Value view =\n+            getStructFromElements(loc, reorderedVals, rewriter, structTy);\n         rewriter.replaceOp(op, view);\n         return success();\n       }\n@@ -3636,7 +3624,6 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n                    .o(\"f32.f16.f16.f32\");\n \n     mma(resOprs, AOprs, BOprs, COprs);\n-    LLVM::vprintf(\"after mma\", {}, rewriter);\n \n     Value res = builder.launch(rewriter, loc, helper.getMmaRetType(ATensorTy));\n "}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -290,6 +290,3 @@ def matmul_kernel(\n     golden = torch.matmul(a, b)\n     golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n     torch.testing.assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err))\n-\n-\n-test_gemm(*[16, 16, 16, 1, 16, 16, 16, False, False])"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 40, "deletions": 40, "changes": 80, "file_content_changes": "@@ -881,10 +881,10 @@ def ttir_to_ttgir(mod, num_warps, num_stages, compute_capability):\n     # can get shared memory swizzled correctly.\n     pm.add_coalesce_pass()\n     pm.add_triton_gpu_combine_pass(compute_capability)\n-    # pm.add_tritongpu_pipeline_pass(num_stages)\n+    pm.add_tritongpu_pipeline_pass(num_stages)\n     # Prefetch must be done after pipeline pass because pipeline pass\n     # extracts slices from the original tensor.\n-    # pm.add_tritongpu_prefetch_pass()\n+    pm.add_tritongpu_prefetch_pass()\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n     pm.add_triton_gpu_combine_pass(compute_capability)\n@@ -1345,12 +1345,12 @@ def make_hash(fn, **kwargs):\n     return hashlib.md5((Path(fn).read_text() + triton.runtime.jit.version_key()).encode(\"utf-8\")).hexdigest()\n \n \n-# - ^\\s*func\\s+ : match the start of the string, any leading whitespace, the keyword func, \n+# - ^\\s*func\\s+ : match the start of the string, any leading whitespace, the keyword func,\n #    and any following whitespace\n # - (public\\s+)? : optionally match the keyword public and any following whitespace\n-# - (@\\w+) : match an @ symbol followed by one or more word characters \n+# - (@\\w+) : match an @ symbol followed by one or more word characters\n #   (letters, digits, or underscores), and capture it as group 1 (the function name)\n-# - (\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\)) : match a pair of parentheses enclosing \n+# - (\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\)) : match a pair of parentheses enclosing\n #   zero or more arguments separated by commas, and capture it as group 2 (the argument list)\n mlir_prototype_pattern = r'^\\s*func\\s+(?:public\\s+)?(@\\w+)(\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\))\\s*\\{\\s*$'\n ptx_prototype_pattern = r\"\\.(?:visible|extern)\\s+\\.(?:entry|func)\\s+(\\w+)\\s*\\(([^)]*)\\)\"\n@@ -1382,20 +1382,20 @@ def compile(fn, **kwargs):\n     extern_libs = kwargs.get(\"extern_libs\", dict())\n     device = kwargs.get(\"device\", torch.cuda.current_device())\n     capability = torch.cuda.get_device_capability()\n-    capability = capability[0]*10 + capability[1]\n+    capability = capability[0] * 10 + capability[1]\n     # build compilation stages\n     stages = {\n-      \"ast\" : (lambda path: fn, None),\n-      \"ttir\": (lambda path: _triton.ir.parse_mlir_module(path, context), \n-               lambda src: ast_to_ttir(src, signature, configs[0], constants)),\n-      \"ttgir\": (lambda path: _triton.ir.parse_mlir_module(path, context), \n-                lambda src: ttir_to_ttgir(src, num_warps, num_stages, capability)),\n-      \"llir\": (lambda path: Path(path).read_bytes(), \n-              lambda src: ttgir_to_llir(src, extern_libs, capability)),\n-      \"ptx\":  (lambda path: Path(path).read_text(), \n-              lambda src: llir_to_ptx(src, capability)),\n-      \"cubin\": (lambda path: Path(path).read_bytes(), \n-               lambda src: ptx_to_cubin(src, capability))\n+        \"ast\": (lambda path: fn, None),\n+        \"ttir\": (lambda path: _triton.ir.parse_mlir_module(path, context),\n+                 lambda src: ast_to_ttir(src, signature, configs[0], constants)),\n+        \"ttgir\": (lambda path: _triton.ir.parse_mlir_module(path, context),\n+                  lambda src: ttir_to_ttgir(src, num_warps, num_stages, capability)),\n+        \"llir\": (lambda path: Path(path).read_bytes(),\n+                 lambda src: ttgir_to_llir(src, extern_libs, capability)),\n+        \"ptx\": (lambda path: Path(path).read_text(),\n+                lambda src: llir_to_ptx(src, capability)),\n+        \"cubin\": (lambda path: Path(path).read_bytes(),\n+                  lambda src: ptx_to_cubin(src, capability))\n     }\n     # find out the signature of the function\n     if isinstance(fn, triton.runtime.JITFunction):\n@@ -1430,42 +1430,42 @@ def compile(fn, **kwargs):\n     if isinstance(fn, triton.runtime.JITFunction):\n         name, ext = fn.__name__, \"ast\"\n     else:\n-      name, ext = os.path.basename(fn).split(\".\")\n+        name, ext = os.path.basename(fn).split(\".\")\n \n     # load metadata if any\n     metadata = None\n     if fn_cache_manager.has_file(f'{name}.json'):\n         with open(fn_cache_manager._make_path(f\"{name}.json\")) as f:\n             metadata = json.load(f)\n     else:\n-      metadata = {\"num_warps\": num_warps, \"num_stages\": num_stages, \"ctime\": dict()}\n-      if ext == \"ptx\":\n-        assert \"shared\" in kwargs, \"ptx compilation must provide shared memory size\"\n-        metadata[\"shared\"] = kwargs[\"shared\"]\n+        metadata = {\"num_warps\": num_warps, \"num_stages\": num_stages, \"ctime\": dict()}\n+        if ext == \"ptx\":\n+            assert \"shared\" in kwargs, \"ptx compilation must provide shared memory size\"\n+            metadata[\"shared\"] = kwargs[\"shared\"]\n \n     first_stage = list(stages.keys()).index(ext)\n     asm = dict()\n     module = fn\n     # run compilation pipeline  and populate metadata\n     for ir, (parse, compile) in list(stages.items())[first_stage:]:\n-      path = fn_cache_manager._make_path(f\"{name}.{ir}\")\n-      if ir == ext:\n-        next_module = parse(fn)\n-      elif os.path.exists(path) and\\\n-           ir in metadata[\"ctime\"] and\\\n-           os.path.getctime(path) == metadata[\"ctime\"][ir]:\n-        next_module = parse(path)\n-      else:\n-        next_module = compile(module)\n-        fn_cache_manager.put(next_module, f\"{name}.{ir}\")\n-      if os.path.exists(path):\n-        metadata[\"ctime\"][ir] = os.path.getctime(path)\n-      asm[ir] = next_module if ir == \"cubin\" else str(next_module)\n-      if ir == \"llir\" and \"shared\" not in metadata:\n-        metadata[\"shared\"] = _triton.get_shared_memory_size(module)\n-      if ir == \"ptx\":\n-        metadata[\"name\"] = ptx_get_kernel_name(next_module)\n-      module = next_module\n+        path = fn_cache_manager._make_path(f\"{name}.{ir}\")\n+        if ir == ext:\n+            next_module = parse(fn)\n+        elif os.path.exists(path) and\\\n+                ir in metadata[\"ctime\"] and\\\n+                os.path.getctime(path) == metadata[\"ctime\"][ir]:\n+            next_module = parse(path)\n+        else:\n+            next_module = compile(module)\n+            fn_cache_manager.put(next_module, f\"{name}.{ir}\")\n+        if os.path.exists(path):\n+            metadata[\"ctime\"][ir] = os.path.getctime(path)\n+        asm[ir] = next_module if ir == \"cubin\" else str(next_module)\n+        if ir == \"llir\" and \"shared\" not in metadata:\n+            metadata[\"shared\"] = _triton.get_shared_memory_size(module)\n+        if ir == \"ptx\":\n+            metadata[\"name\"] = ptx_get_kernel_name(next_module)\n+        module = next_module\n     # write-back metadata\n     fn_cache_manager.put(json.dumps(metadata), f\"{name}.json\", binary=False)\n     # return handle to compiled kernel"}]