[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 8, "deletions": 3, "changes": 11, "file_content_changes": "@@ -4783,10 +4783,15 @@ class ConvertTritonGPUToLLVM\n       decomposed = true;\n     });\n \n-    // async wait is supported in Ampere and later\n     mod.walk([&](triton::gpu::AsyncWaitOp asyncWaitOp) -> void {\n-      if (!triton::gpu::AsyncWaitOp::isSupported(computeCapability) ||\n-          decomposed) {\n+      if (!triton::gpu::AsyncWaitOp::isSupported(computeCapability)) {\n+        // async wait is supported in Ampere and later\n+        asyncWaitOp.erase();\n+      } else if (decomposed) {\n+        // Wait for all previous async ops\n+        OpBuilder builder(asyncWaitOp);\n+        auto newAsyncWaitOp =\n+            builder.create<triton::gpu::AsyncWaitOp>(asyncWaitOp.getLoc(), 0);\n         asyncWaitOp.erase();\n       }\n     });"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 7, "deletions": 9, "changes": 16, "file_content_changes": "@@ -262,10 +262,10 @@ struct TritonCatPattern : public OpConversionPattern<triton::CatOp> {\n     // For now, this behaves like generic, but this will evolve when\n     // we add support for `can_reorder=False`\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    rewriter.replaceOpWithNewOp<triton::CatOp>(op, retType, adaptor.getOperands());\n+    rewriter.replaceOpWithNewOp<triton::CatOp>(op, retType,\n+                                               adaptor.getOperands());\n     return success();\n   }\n-\n };\n \n struct TritonTransPattern : public OpConversionPattern<triton::TransOp> {\n@@ -450,13 +450,11 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n       TritonGenericPattern<triton::IntToPtrOp>,\n       TritonGenericPattern<triton::PtrToIntOp>,\n       TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n-      TritonGenericPattern<triton::AddPtrOp>, \n-      TritonCatPattern,\n-      TritonReducePattern,\n-      TritonTransPattern, TritonExpandDimsPattern, TritonMakeRangePattern,\n-      TritonDotPattern, TritonLoadPattern, TritonStorePattern,\n-      TritonExtElemwisePattern, TritonPrintfPattern, TritonAtomicRMWPattern>(\n-      typeConverter, context);\n+      TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n+      TritonReducePattern, TritonTransPattern, TritonExpandDimsPattern,\n+      TritonMakeRangePattern, TritonDotPattern, TritonLoadPattern,\n+      TritonStorePattern, TritonExtElemwisePattern, TritonPrintfPattern,\n+      TritonAtomicRMWPattern>(typeConverter, context);\n }\n \n //"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -782,8 +782,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n                                                  newRetType.getEncoding()));\n     a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), newAType, a);\n     b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), newBType, b);\n-    auto newDot = rewriter.create<triton::DotOp>(\n-        dotOp.getLoc(), newRetType, a, b, newAcc, dotOp.allowTF32());\n+    auto newDot = rewriter.create<triton::DotOp>(dotOp.getLoc(), newRetType, a,\n+                                                 b, newAcc, dotOp.allowTF32());\n \n     rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(\n         op, oldRetType, newDot.getResult());"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -225,6 +225,7 @@ scf::ForOp Prefetcher::createNewForOp() {\n   BlockAndValueMapping mapping;\n   for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n+  mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n \n   for (Operation &op : forOp.getBody()->without_terminator()) {\n     Operation *newOp = builder.clone(op, mapping);"}, {"filename": "python/tests/test_matmul.py", "status": "added", "additions": 101, "deletions": 0, "changes": 101, "file_content_changes": "@@ -0,0 +1,101 @@\n+import itertools\n+\n+import pytest\n+import torch\n+\n+import triton\n+import triton._C.libtriton.triton as _triton\n+\n+\n+@pytest.mark.parametrize(\n+    \"BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, DTYPE\",\n+    itertools.chain(\n+        *[\n+            [\n+                # 1 warp\n+                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n+                (32, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n+                (16, 32, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n+                (16, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n+                (32, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n+                (16, 32, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n+                (16, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n+                (64, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n+                (16, 64, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n+                # 2 warp\n+                (64, 32, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n+                (32, 64, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n+                (64, 32, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n+                (32, 64, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n+                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n+                (32, 128, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n+                # 4 warp\n+                (128, 64, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n+                (64, 128, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n+                (128, 32, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n+                (32, 128, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n+                (128, 32, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n+                (32, 128, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n+                # 8 warp\n+                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE),\n+                (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE),\n+                (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE),\n+                # split-k\n+                (64, 64, 16, 2, 4, 2, None, None, None, AT, BT, DTYPE),\n+                (64, 64, 16, 4, 4, 2, None, None, None, AT, BT, DTYPE),\n+                (64, 64, 16, 8, 4, 2, None, None, None, AT, BT, DTYPE),\n+                # variable input\n+                (128, 128, 32, 1, 4, 2, 1024, 1024, 1024, AT, BT, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 384, 128, 640, AT, BT, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE),\n+            ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n+        ],\n+        # n-stage\n+        *[\n+            [\n+                (16, 16, 16, 1, 1, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n+                (64, 32, 64, 1, 2, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n+                (128, 64, 16, 1, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n+                (256, 128, 32, 1, 8, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n+                (128, 128, 32, 1, 4, STAGES, 384, 128, 640, AT, BT, DTYPE),\n+                # split-k\n+                (64, 64, 16, 8, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n+                (64, 64, 16, 8, 4, STAGES, 1024, 1024, 32, AT, BT, DTYPE),\n+            ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [2, 3, 4]\n+        ]\n+    ),\n+)\n+def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, DTYPE):\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] < 7:\n+        pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n+    if capability[0] < 8 and DTYPE == \"bfloat16\":\n+        pytest.skip(\"Only test bfloat16 on devices with sm >= 80\")\n+    #if DTYPE == \"bfloat16\" and SPLIT_K != 1:\n+    #    pytest.skip(\"bfloat16 matmuls don't allow split_k for now\")\n+    if DTYPE == \"bfloat16\":\n+        pytest.skip(\"bfloat16 matmuls doesn't support for now\")\n+    torch.manual_seed(0)\n+    # nuke kernel decorators -- will set meta-parameters manually\n+    kwargs = {'BLOCK_M': BLOCK_M, 'BLOCK_N': BLOCK_N, 'BLOCK_K': BLOCK_K, 'SPLIT_K': SPLIT_K}\n+    pre_hook = None if SPLIT_K == 1 else lambda nargs: nargs['C'].zero_()\n+    configs = [triton.Config(kwargs=kwargs, num_warps=NWARP, num_stages=NSTAGE, pre_hook=pre_hook)]\n+    kernel = triton.ops._matmul.kernel\n+    kernel.configs = configs\n+    # kernel.run = kernel.run.run.run\n+\n+    # get matrix shape\n+    M = BLOCK_M if M is None else M\n+    N = BLOCK_N if N is None else N\n+    K = BLOCK_K * SPLIT_K if K is None else K\n+    # allocate/transpose inputs\n+    DTYPE = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[DTYPE]\n+    a = .1 * torch.randn((K, M) if AT else (M, K), device=\"cuda\", dtype=DTYPE)\n+    b = .1 * torch.randn((N, K) if BT else (K, N), device=\"cuda\", dtype=DTYPE)\n+    a = a.t() if AT else a\n+    b = b.t() if BT else b\n+    # run test\n+    th_c = torch.matmul(a, b)\n+    tt_c = triton.testing.catch_oor(lambda: triton.ops.matmul(a, b), pytest)\n+    triton.testing.assert_almost_equal(th_c, tt_c)"}, {"filename": "python/tests/test_vecadd.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -68,7 +68,7 @@ def kernel(x_ptr,\n         @num_elements: number of elements\n         '''\n         pid = tl.program_id(axis=0)\n-        for i in range(math.ceil(block_size / iter_size)):\n+        for i in range(tl.cdiv(block_size, iter_size)):\n             # TODO: a bug here, if put the offset outside the forloop, there will be a GPU mis-aligned error.\n             offset = pid * block_size + tl.arange(0, iter_size)\n             x_ptrs = x_ptr + offset"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 61, "deletions": 25, "changes": 86, "file_content_changes": "@@ -329,10 +329,6 @@ def visit_Tuple(self, node):\n     def visit_BinOp(self, node):\n         lhs = self.visit(node.left)\n         rhs = self.visit(node.right)\n-        if isinstance(lhs, triton.language.constexpr):\n-            lhs = lhs.value\n-        if isinstance(rhs, triton.language.constexpr):\n-            rhs = rhs.value\n         fn = {\n             ast.Add: '__add__',\n             ast.Sub: '__sub__',\n@@ -591,8 +587,10 @@ def visit_For(self, node):\n                         ast.NodeVisitor.generic_visit(self, stmt)\n                 return\n         # handle negative constant step (not supported by scf.for in MLIR)\n+        negative_step = False\n         if isinstance(step, triton.language.constexpr) and step.value < 0:\n             step = triton.language.constexpr(-step.value)\n+            negative_step = True\n             lb, ub = ub, lb\n         # lb/ub/step might be constexpr, we need to cast them to tensor\n         lb = triton.language.core._to_tensor(lb, self.builder).handle\n@@ -640,6 +638,9 @@ def visit_For(self, node):\n             # update induction variable with actual value, and replace all uses\n             self.builder.set_insertion_point_to_start(for_op.get_body(0))\n             iv = self.builder.create_index_to_si(for_op.get_induction_var())\n+            if negative_step:\n+                ub_si = self.builder.create_index_to_si(ub)\n+                iv = self.builder.create_sub(ub_si, iv)\n             self.lscope[node.target.id].handle.replace_all_uses_with(iv)\n             self.set_value(name, triton.language.core.tensor(iv, triton.language.core.int32))\n \n@@ -890,9 +891,9 @@ def ttir_to_ttgir(mod, num_warps, num_stages, compute_capability):\n     pm = _triton.ir.pass_manager(mod.context)\n     pm.add_convert_triton_to_tritongpu_pass(num_warps)\n     pm.enable_debug()\n-    # Convert blocked layout to mma layout for dot ops so that pipeline\n-    # can get shared memory swizzled correctly.\n     pm.add_coalesce_pass()\n+    # The combine pass converts blocked layout to mma layout\n+    # for dot ops so that pipeline can get shared memory swizzled correctly.\n     pm.add_triton_gpu_combine_pass(compute_capability)\n     pm.add_tritongpu_pipeline_pass(num_stages)\n     # Prefetch must be done after pipeline pass because pipeline pass\n@@ -1358,12 +1359,12 @@ def make_hash(fn, **kwargs):\n     return hashlib.md5((Path(fn).read_text() + triton.runtime.jit.version_key()).encode(\"utf-8\")).hexdigest()\n \n \n-# - ^\\s*func\\s+ : match the start of the string, any leading whitespace, the keyword func, \n+# - ^\\s*func\\s+ : match the start of the string, any leading whitespace, the keyword func,\n #    and any following whitespace\n # - (public\\s+)? : optionally match the keyword public and any following whitespace\n-# - (@\\w+) : match an @ symbol followed by one or more word characters \n+# - (@\\w+) : match an @ symbol followed by one or more word characters\n #   (letters, digits, or underscores), and capture it as group 1 (the function name)\n-# - (\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\)) : match a pair of parentheses enclosing \n+# - (\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\)) : match a pair of parentheses enclosing\n #   zero or more arguments separated by commas, and capture it as group 2 (the argument list)\n mlir_prototype_pattern = r'^\\s*func\\s+(?:public\\s+)?(@\\w+)(\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\))\\s*\\{\\s*$'\n ptx_prototype_pattern = r\"\\.(?:visible|extern)\\s+\\.(?:entry|func)\\s+(\\w+)\\s*\\(([^)]*)\\)\"\n@@ -1395,20 +1396,20 @@ def compile(fn, **kwargs):\n     extern_libs = kwargs.get(\"extern_libs\", dict())\n     device = kwargs.get(\"device\", torch.cuda.current_device())\n     capability = torch.cuda.get_device_capability()\n-    capability = capability[0]*10 + capability[1]\n+    capability = capability[0] * 10 + capability[1]\n     # build compilation stages\n     stages = {\n-      \"ast\" : (lambda path: fn, None),\n-      \"ttir\": (lambda path: _triton.ir.parse_mlir_module(path, context), \n-               lambda src: ast_to_ttir(src, signature, configs[0], constants)),\n-      \"ttgir\": (lambda path: _triton.ir.parse_mlir_module(path, context), \n-                lambda src: ttir_to_ttgir(src, num_warps, num_stages, capability)),\n-      \"llir\": (lambda path: Path(path).read_bytes(), \n-              lambda src: ttgir_to_llir(src, extern_libs, capability)),\n-      \"ptx\":  (lambda path: Path(path).read_text(), \n-              lambda src: llir_to_ptx(src, capability)),\n-      \"cubin\": (lambda path: Path(path).read_bytes(), \n-               lambda src: ptx_to_cubin(src, capability))\n+        \"ast\": (lambda path: fn, None),\n+        \"ttir\": (lambda path: _triton.ir.parse_mlir_module(path, context),\n+                 lambda src: ast_to_ttir(src, signature, configs[0], constants)),\n+        \"ttgir\": (lambda path: _triton.ir.parse_mlir_module(path, context),\n+                  lambda src: ttir_to_ttgir(src, num_warps, num_stages, capability)),\n+        \"llir\": (lambda path: Path(path).read_bytes(),\n+                 lambda src: ttgir_to_llir(src, extern_libs, capability)),\n+        \"ptx\": (lambda path: Path(path).read_text(),\n+                lambda src: llir_to_ptx(src, capability)),\n+        \"cubin\": (lambda path: Path(path).read_bytes(),\n+                  lambda src: ptx_to_cubin(src, capability))\n     }\n     # find out the signature of the function\n     if isinstance(fn, triton.runtime.JITFunction):\n@@ -1467,8 +1468,8 @@ def compile(fn, **kwargs):\n       if ir == ext:\n         next_module = parse(fn)\n       elif os.path.exists(path) and\\\n-           ir in metadata[\"ctime\"] and\\\n-           os.path.getctime(path) == metadata[\"ctime\"][ir]:\n+              ir in metadata[\"ctime\"] and\\\n+              os.path.getctime(path) == metadata[\"ctime\"][ir]:\n         next_module = parse(path)\n       else:\n         next_module = compile(module)\n@@ -1504,8 +1505,7 @@ def __init__(self, so_path, metadata, asm):\n         self.asm = asm\n         device = torch.cuda.current_device()\n         global cuda_utils\n-        if cuda_utils is None:\n-            cuda_utils = CudaUtils()\n+        init_cuda_utils()\n         mod, func, n_regs, n_spills = cuda_utils.load_binary(metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n         self.cu_module = mod\n         self.cu_function = func\n@@ -1562,6 +1562,34 @@ def _generate_src(self):\n \n         #define CUDA_CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); if(PyErr_Occurred()) return NULL; }\n \n+        static PyObject* getDeviceProperties(PyObject* self, PyObject* args){\n+            int device_id;\n+            if(!PyArg_ParseTuple(args, \"i\", &device_id))\n+                return NULL;\n+            // Get device handle\n+            CUdevice device;\n+            cuDeviceGet(&device, device_id);\n+\n+            // create a struct to hold device properties\n+            int max_shared_mem;\n+            int multiprocessor_count;\n+            int sm_clock_rate;\n+            int mem_clock_rate;\n+            int mem_bus_width;\n+            CUDA_CHECK(cuDeviceGetAttribute(&max_shared_mem, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK, device));\n+            CUDA_CHECK(cuDeviceGetAttribute(&multiprocessor_count, CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT, device));\n+            CUDA_CHECK(cuDeviceGetAttribute(&sm_clock_rate, CU_DEVICE_ATTRIBUTE_CLOCK_RATE, device));\n+            CUDA_CHECK(cuDeviceGetAttribute(&mem_clock_rate, CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE, device));\n+            CUDA_CHECK(cuDeviceGetAttribute(&mem_bus_width, CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH, device));\n+\n+\n+            return Py_BuildValue(\"{s:i, s:i, s:i, s:i, s:i}\", \"max_shared_mem\", max_shared_mem,\n+                                       \"multiprocessor_count\", multiprocessor_count,\n+                                       \"sm_clock_rate\", sm_clock_rate,\n+                                       \"mem_clock_rate\", mem_clock_rate,\n+                                       \"mem_bus_width\", mem_bus_width);\n+        }\n+\n         static PyObject* loadBinary(PyObject* self, PyObject* args) {\n             const char* name;\n             const char* data;\n@@ -1601,6 +1629,7 @@ def _generate_src(self):\n \n         static PyMethodDef ModuleMethods[] = {\n           {\"load_binary\", loadBinary, METH_VARARGS, \"Load provided cubin into CUDA driver\"},\n+          {\"get_device_properties\", getDeviceProperties, METH_VARARGS, \"Get the properties for a given device\"},\n           {NULL, NULL, 0, NULL} // sentinel\n         };\n \n@@ -1640,6 +1669,13 @@ def __init__(self):\n         mod = importlib.util.module_from_spec(spec)\n         spec.loader.exec_module(mod)\n         self.load_binary = mod.load_binary\n+        self.get_device_properties = mod.get_device_properties\n+\n+\n+def init_cuda_utils():\n+    global cuda_utils\n+    if cuda_utils is None:\n+        cuda_utils = CudaUtils()\n \n \n cuda_utils = None"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -9,6 +9,7 @@\n \n T = TypeVar('T')\n \n+\n def _to_tensor(x, builder):\n     if isinstance(x, bool):\n         return tensor(builder.get_int1(x), int1)\n@@ -348,6 +349,9 @@ def __rsub__(self, other):\n     def __mul__(self, other):\n         return constexpr(self.value * other.value)\n \n+    def __mod__(self, other):\n+        return constexpr(self.value % other.value)\n+\n     def __rmul__(self, other):\n         return constexpr(other.value * self.value)\n \n@@ -726,10 +730,12 @@ def broadcast_to(input, shape, _builder=None):\n     \"\"\"\n     return semantic.broadcast_impl_shape(input, shape, _builder)\n \n+\n @builtin\n def trans(input, _builder=None):\n     return semantic.trans(input, _builder)\n \n+\n @builtin\n def cat(input, other, can_reorder=False, _builder=None):\n     \"\"\"\n@@ -762,6 +768,7 @@ def view(input, shape, _builder=None):\n     shape = [x.value for x in shape]\n     return semantic.view(input, shape, _builder)\n \n+\n @builtin\n def reshape(input, shape, _builder=None):\n     # TODO: should be more than just a view"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -481,7 +481,8 @@ def view(input: tl.tensor,\n          dst_shape: List[int],\n          builder: ir.builder) -> tl.tensor:\n     # TODO: disable when TritonToTritonGPU handles views properly\n-    assert len(input.shape) == len(dst_shape)\n+    \n+    # assert len(input.shape) == len(dst_shape)\n     numel = 1\n     for s in dst_shape:\n         numel *= s"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -26,9 +26,6 @@ def get_configs_io_bound():\n     return configs\n \n \n-@triton.heuristics({\n-    'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] * args['SPLIT_K']) == 0,\n-})\n @triton.autotune(\n     configs=[\n         # basic configs for compute-bound matmuls\n@@ -59,6 +56,9 @@ def get_configs_io_bound():\n         'top_k': 10\n     },\n )\n+@triton.heuristics({\n+    'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] * args['SPLIT_K']) == 0,\n+})\n @triton.jit\n def _kernel(A, B, C, M, N, K,\n             stride_am, stride_ak,"}, {"filename": "python/triton/ops/matmul_perf_model.py", "status": "modified", "additions": 13, "deletions": 8, "changes": 21, "file_content_changes": "@@ -10,22 +10,24 @@\n def get_tensorcore_tflops(backend, device, num_ctas, num_warps, dtype):\n     ''' return compute throughput in TOPS '''\n     total_warps = num_ctas * min(num_warps, 4)\n-    num_subcores = _triton.runtime.num_sm(backend, device) * 4  # on recent GPUs\n+    triton.compiler.init_cuda_utils()\n+\n+    num_subcores = triton.compiler.cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n     tflops = min(num_subcores, total_warps) / num_subcores * get_max_tensorcore_tflops(dtype, backend, device)\n     return tflops\n \n \n def get_simd_tflops(backend, device, num_ctas, num_warps, dtype):\n     ''' return compute throughput in TOPS '''\n     total_warps = num_ctas * min(num_warps, 4)\n-    num_subcores = _triton.runtime.num_sm(backend, device) * 4  # on recent GPUs\n+    num_subcores = triton.compiler.cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n     tflops = min(num_subcores, total_warps) / num_subcores * get_max_simd_tflops(dtype, backend, device)\n     return tflops\n \n \n def get_tflops(backend, device, num_ctas, num_warps, dtype):\n-    cc = _triton.runtime.cc(backend, device)\n-    if cc < 80 and dtype == torch.float32:\n+    capability = torch.cuda.get_device_capability(device)\n+    if capability[0] < 8 and dtype == torch.float32:\n         return get_simd_tflops(backend, device, num_ctas, num_warps, dtype)\n     return get_tensorcore_tflops(backend, device, num_ctas, num_warps, dtype)\n \n@@ -59,7 +61,7 @@ def estimate_matmul_time(\n     compute_ms = total_ops / tput\n \n     # time to load data\n-    num_sm = _triton.runtime.num_sm(backend, device)\n+    num_sm = triton.compiler.cuda_utils.get_device_properties(device)[\"multiprocessor_count\"]\n     active_cta_ratio = min(1, num_ctas / num_sm)\n     active_cta_ratio_bw1 = min(1, num_ctas / 32)  # 32 active ctas are enough to saturate\n     active_cta_ratio_bw2 = max(min(1, (num_ctas - 32) / (108 - 32)), 0)  # 32-108, remaining 5%\n@@ -99,7 +101,7 @@ def estimate_matmul_time(\n def early_config_prune(configs, named_args):\n     backend = _triton.runtime.backend.CUDA\n     device = torch.cuda.current_device()\n-    cc = _triton.runtime.cc(backend, device)\n+    capability = torch.cuda.get_device_capability()\n     # BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, num_warps, num_stages\n     dtsize = named_args['A'].element_size()\n     dtype = named_args['A'].dtype\n@@ -110,7 +112,10 @@ def early_config_prune(configs, named_args):\n         kw = config.kwargs\n         BLOCK_M, BLOCK_N, BLOCK_K, num_stages = \\\n             kw['BLOCK_M'], kw['BLOCK_N'], kw['BLOCK_K'], config.num_stages\n-        max_shared_memory = _triton.runtime.max_shared_memory(backend, device)\n+\n+        # TODO: move to `cuda_utils` submodule\n+        triton.compiler.init_cuda_utils()\n+        max_shared_memory = triton.compiler.cuda_utils.get_device_properties(device)[\"max_shared_mem\"]\n         required_shared_memory = (BLOCK_M + BLOCK_N) * BLOCK_K * num_stages * dtsize\n         if required_shared_memory <= max_shared_memory:\n             pruned_configs.append(config)\n@@ -136,7 +141,7 @@ def early_config_prune(configs, named_args):\n     pruned_configs = []\n     for k, v in configs_map.items():\n         BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, num_warps = k\n-        if cc >= 80:\n+        if capability[0] >= 8:\n             # compute cycles (only works for ampere GPUs)\n             mmas = BLOCK_M * BLOCK_N * BLOCK_K / (16 * 8 * 16)\n             mma_cycles = mmas / min(4, num_warps) * 8"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 11, "deletions": 6, "changes": 17, "file_content_changes": "@@ -16,6 +16,9 @@\n     _cutlass = None\n     has_cutlass = False\n \n+# TODO: move to separate module\n+import triton\n+\n \n def catch_oor(kernel, pytest_handle=None):\n     try:\n@@ -330,8 +333,8 @@ def get_dram_gbps(backend=None, device=None):\n         backend = _triton.runtime.backend.CUDA\n     if not device:\n         device = torch.cuda.current_device()\n-    mem_clock_khz = _triton.runtime.memory_clock_rate(backend, device)\n-    bus_width = _triton.runtime.global_memory_bus_width(backend, device)\n+    mem_clock_khz = triton.compiler.cuda_utils.get_device_properties(device)[\"mem_clock_rate\"] # in kHz\n+    bus_width = triton.compiler.cuda_utils.get_device_properties(device)[\"mem_bus_width\"] \n     bw_gbps = mem_clock_khz * bus_width * 2 / 1e6 / 8  # In GB/s\n     return bw_gbps\n \n@@ -341,11 +344,13 @@ def get_max_tensorcore_tflops(dtype: torch.dtype, backend=None, device=None, clo\n         backend = _triton.runtime.backend.CUDA\n     if not device:\n         device = torch.cuda.current_device()\n-    num_subcores = _triton.runtime.num_sm(backend, device) * 4  # on recent GPUs\n+\n+    triton.compiler.init_cuda_utils()\n+    num_subcores = triton.compiler.cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4\n     if not clock_rate:\n-        clock_rate = _triton.runtime.clock_rate(backend, device)  # in kHz\n-    cc = _triton.runtime.cc(backend, device)\n-    if cc < 80:\n+        clock_rate = triton.compiler.cuda_utils.get_device_properties(device)[\"sm_clock_rate\"] # in kHz\n+    capability = torch.cuda.get_device_capability(device)\n+    if capability[0] < 8:\n         assert dtype == torch.float16\n         ops_per_sub_core = 256  # 2 4x4x4 Tensor Cores\n     else:"}]