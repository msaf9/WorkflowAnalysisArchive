[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 0, "deletions": 8, "changes": 8, "file_content_changes": "@@ -482,8 +482,6 @@ getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n   auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n   const int perPhase = sharedLayout.getPerPhase();\n   const int maxPhase = sharedLayout.getMaxPhase();\n-  // llvm::outs() << sharedLayout.getVec() << \" \" << perPhase << \" \" << maxPhase\n-  //              << \"\\n\";\n   const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n   auto order = sharedLayout.getOrder();\n \n@@ -594,12 +592,6 @@ Value loadB(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n   SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n                              tensorTy.getShape().end());\n \n-  // TODO[Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n-  bool transB = false;\n-  if (transB) {\n-    std::swap(shape[0], shape[1]);\n-  }\n-\n   int mmaInstrM = 16, mmaInstrN = 8, mmaInstrK = 4 * 64 / bitwidth;\n   int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n "}]