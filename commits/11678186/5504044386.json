[{"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -8,7 +8,7 @@\n import triton.ops\n \n \n-def f8_to_f16(x):\n+def f8_to_f16(x, dtype):\n \n     @triton.jit\n     def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n@@ -20,7 +20,7 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n \n     ret = torch.empty(x.shape, dtype=torch.float16, device=x.device)\n     grid = lambda META: (triton.cdiv(x.numel(), META['BLOCK_SIZE']),)\n-    dtype = getattr(tl, 'float8e5')\n+    dtype = getattr(tl, dtype)\n     kernel[grid](ret, triton.reinterpret(x, dtype), ret.numel(), BLOCK_SIZE=1024)\n     return ret\n \n@@ -89,9 +89,9 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n                 (128, 128, 32, 8, 4, 2, 256, 256, 128, AT, BT, ADTYPE, BDTYPE),\n-            ] for ADTYPE, BDTYPE in [(\"float8e4b15\", \"float8e5\"),\n+            ] for ADTYPE, BDTYPE in [(\"float8e5\", \"float8e5\"),\n                                      (\"float8e5\", \"float16\"),\n-                                     (\"float16\", \"float8e4b15\"),\n+                                     (\"float16\", \"float8e5\"),\n                                      (\"float16\", \"float32\"),\n                                      (\"float32\", \"float16\"),\n                                      (\"bfloat16\", \"float32\"),\n@@ -123,9 +123,9 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n     a_fp8 = \"float8\" in ADTYPE\n     b_fp8 = \"float8\" in BDTYPE\n \n-    def maybe_upcast(x, is_float8):\n+    def maybe_upcast(x, dtype, is_float8):\n         if is_float8:\n-            return f8_to_f16(x)\n+            return f8_to_f16(x, dtype)\n         return x\n \n     def init_input(n, m, t, dtype, is_float8):\n@@ -140,10 +140,10 @@ def init_input(n, m, t, dtype, is_float8):\n     a = init_input(M, K, AT, ADTYPE, a_fp8)\n     b = init_input(K, N, BT, BDTYPE, b_fp8)\n     # run test\n-    th_a = maybe_upcast(a, a_fp8).to(torch.float32)\n+    th_a = maybe_upcast(a, ADTYPE, a_fp8).to(torch.float32)\n     if AT and a_fp8:\n         th_a = th_a.view(th_a.shape[::-1]).T\n-    th_b = maybe_upcast(b, b_fp8).to(torch.float32)\n+    th_b = maybe_upcast(b, BDTYPE, b_fp8).to(torch.float32)\n     if BT and b_fp8:\n         th_b = th_b.view(th_b.shape[::-1]).T\n     th_c = torch.matmul(th_a, th_b)"}]