[{"filename": "CMakeLists.txt", "status": "modified", "additions": 50, "deletions": 40, "changes": 90, "file_content_changes": "@@ -1,5 +1,7 @@\n-cmake_minimum_required(VERSION 3.6)\n+cmake_minimum_required(VERSION 3.20)\n \n+# Introduced in cmake 3.20\n+# https://cmake.org/cmake/help/latest/policy/CMP0116.html\n cmake_policy(SET CMP0116 OLD)\n \n include(ExternalProject)\n@@ -10,6 +12,7 @@ set(CMAKE_INCLUDE_CURRENT_DIR ON)\n \n project(triton)\n include(CTest)\n+\n if(NOT WIN32)\n   list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_SOURCE_DIR}/cmake\")\n endif()\n@@ -19,7 +22,7 @@ option(TRITON_BUILD_TUTORIALS \"Build C++ Triton tutorials\" ON)\n option(TRITON_BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n \n # Ensure Python3 vars are set correctly\n-#  used conditionally in this file and by lit tests\n+# used conditionally in this file and by lit tests\n \n # Customized release build type with assertions: TritonRelBuildWithAsserts\n set(CMAKE_C_FLAGS_TRITONRELBUILDWITHASSERTS \"-O2 -g\")\n@@ -32,7 +35,7 @@ if(NOT CMAKE_BUILD_TYPE)\n endif()\n \n if(NOT WIN32)\n-    find_library(TERMINFO_LIBRARY tinfo)\n+  find_library(TERMINFO_LIBRARY tinfo)\n endif()\n \n # Compiler flags\n@@ -42,22 +45,21 @@ include_directories(${CMAKE_CURRENT_SOURCE_DIR}/include)\n include_directories(${PYBIND11_INCLUDE_DIR})\n \n if(WIN32)\n-    SET(BUILD_SHARED_LIBS OFF)\n-    find_package(dlfcn-win32 REQUIRED)\n-    set(CMAKE_DL_LIBS dlfcn-win32::dl)\n+  SET(BUILD_SHARED_LIBS OFF)\n+  find_package(dlfcn-win32 REQUIRED)\n+  set(CMAKE_DL_LIBS dlfcn-win32::dl)\n endif()\n \n set(CMAKE_CXX_FLAGS \"${CMAKE_C_FLAGS} -D__STDC_FORMAT_MACROS  -fPIC -std=gnu++17 -fvisibility=hidden -fvisibility-inlines-hidden\")\n+\n if(APPLE)\n   set(CMAKE_OSX_DEPLOYMENT_TARGET 11.6)\n endif()\n \n-\n-\n-##########\n+# #########\n # LLVM\n-##########\n-if (NOT MLIR_DIR)\n+# #########\n+if(NOT MLIR_DIR)\n   if(NOT LLVM_LIBRARY_DIR)\n     if(WIN32)\n       find_package(LLVM 13 REQUIRED COMPONENTS nvptx amdgpu)\n@@ -73,12 +75,16 @@ if (NOT MLIR_DIR)\n     else()\n       find_package(LLVM 11 REQUIRED COMPONENTS \"nvptx;amdgpu\")\n     endif()\n+\n     message(STATUS \"Found LLVM ${LLVM_PACKAGE_VERSION}\")\n+\n     # FindLLVM outputs LLVM_LIBRARY_DIRS but we expect LLVM_LIBRARY_DIR here\n     set(LLVM_LIBRARY_DIR ${LLVM_LIBRARY_DIRS})\n+\n     if(APPLE)\n       set(CMAKE_OSX_DEPLOYMENT_TARGET \"10.14\")\n     endif()\n+\n   # sometimes we don't want to use llvm-config, since it may have been downloaded for some specific linux distros\n   else()\n     set(LLVM_LDFLAGS \"-L${LLVM_LIBRARY_DIR}\")\n@@ -138,37 +144,38 @@ if (NOT MLIR_DIR)\n       libLLVMAnalysis.a\n     )\n   endif()\n-  set (MLIR_DIR ${LLVM_LIBRARY_DIR}/cmake/mlir)\n+\n+  set(MLIR_DIR ${LLVM_LIBRARY_DIR}/cmake/mlir)\n endif()\n \n # Python module\n if(TRITON_BUILD_PYTHON_MODULE)\n-    message(STATUS \"Adding Python module\")\n-    set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n-    set(PYTHON_SRC ${PYTHON_SRC_PATH}/main.cc ${PYTHON_SRC_PATH}/triton.cc)\n-    include_directories(\".\" ${PYTHON_SRC_PATH})\n-    if (PYTHON_INCLUDE_DIRS)\n-      include_directories(${PYTHON_INCLUDE_DIRS})\n-    else()\n-      find_package(Python3 REQUIRED COMPONENTS Development Interpreter)\n-      include_directories(${Python3_INCLUDE_DIRS})\n-      link_directories(${Python3_LIBRARY_DIRS})\n-      link_libraries(${Python3_LIBRARIES})\n-      add_link_options(${Python3_LINK_OPTIONS})\n-    endif()\n+  message(STATUS \"Adding Python module\")\n+  set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n+  set(PYTHON_SRC ${PYTHON_SRC_PATH}/main.cc ${PYTHON_SRC_PATH}/triton.cc)\n+  include_directories(\".\" ${PYTHON_SRC_PATH})\n+\n+  if(PYTHON_INCLUDE_DIRS)\n+    include_directories(${PYTHON_INCLUDE_DIRS})\n+  else()\n+    find_package(Python3 REQUIRED COMPONENTS Development Interpreter)\n+    include_directories(${Python3_INCLUDE_DIRS})\n+    link_directories(${Python3_LIBRARY_DIRS})\n+    link_libraries(${Python3_LIBRARIES})\n+    add_link_options(${Python3_LINK_OPTIONS})\n+  endif()\n endif()\n \n # # Triton\n # file(GLOB_RECURSE LIBTRITON_SRC lib/*.cc)\n # if (WIN32 AND TRITON_BUILD_PYTHON_MODULE)\n-#     Python3_add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n-#     set_target_properties(triton PROPERTIES SUFFIX \".pyd\")\n-#     set_target_properties(triton PROPERTIES PREFIX \"lib\")\n+# Python3_add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n+# set_target_properties(triton PROPERTIES SUFFIX \".pyd\")\n+# set_target_properties(triton PROPERTIES PREFIX \"lib\")\n # else()\n-#     add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n+# add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n # endif()\n \n-\n # MLIR\n find_package(MLIR REQUIRED CONFIG PATHS ${MLIR_DIR})\n \n@@ -186,14 +193,13 @@ include_directories(${MLIR_INCLUDE_DIRS})\n include_directories(${LLVM_INCLUDE_DIRS})\n include_directories(${PROJECT_SOURCE_DIR}/include)\n include_directories(${PROJECT_BINARY_DIR}/include) # Tablegen'd files\n-# link_directories(${LLVM_LIBRARY_DIR})\n \n+# link_directories(${LLVM_LIBRARY_DIR})\n add_subdirectory(include)\n add_subdirectory(lib)\n add_subdirectory(bin)\n \n # find_package(PythonLibs REQUIRED)\n-\n set(TRITON_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}\")\n set(TRITON_BINARY_DIR \"${CMAKE_CURRENT_BINARY_DIR}\")\n \n@@ -210,6 +216,7 @@ if(TRITON_BUILD_PYTHON_MODULE)\n     TritonPTX\n     ${dialect_libs}\n     ${conversion_libs}\n+\n     # optimizations\n     MLIRPass\n     MLIRTransforms\n@@ -221,6 +228,7 @@ if(TRITON_BUILD_PYTHON_MODULE)\n     MLIRNVVMToLLVMIRTranslation\n     MLIRIR\n   )\n+\n   if(WIN32)\n     target_link_libraries(triton PRIVATE ${LLVM_LIBRARIES} ${CMAKE_DL_LIBS}\n       ${TRITON_LIBRARIES}\n@@ -234,21 +242,23 @@ if(TRITON_BUILD_PYTHON_MODULE)\n       ${TRITON_LIBRARIES}\n     )\n   endif()\n-  \n+\n   target_link_options(triton PRIVATE ${LLVM_LDFLAGS})\n endif()\n \n-if (UNIX AND NOT APPLE)\n+if(UNIX AND NOT APPLE)\n   set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -Wl,--exclude-libs,ALL\")\n endif()\n \n if(TRITON_BUILD_PYTHON_MODULE AND NOT WIN32)\n-    set(CMAKE_SHARED_LIBRARY_SUFFIX \".so\")\n-    # Check if the platform is MacOS\n-    if(APPLE)\n-        set(PYTHON_LDFLAGS \"-undefined dynamic_lookup -flto\")\n-    endif()\n-    target_link_libraries(triton ${CUTLASS_LIBRARIES} ${PYTHON_LDFLAGS})\n+  set(CMAKE_SHARED_LIBRARY_SUFFIX \".so\")\n+\n+  # Check if the platform is MacOS\n+  if(APPLE)\n+    set(PYTHON_LDFLAGS \"-undefined dynamic_lookup -flto\")\n+  endif()\n+\n+  target_link_libraries(triton ${CUTLASS_LIBRARIES} ${PYTHON_LDFLAGS})\n endif()\n \n add_subdirectory(test)"}, {"filename": "README.md", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -48,7 +48,7 @@ Version 2.0 is out! New features include:\n - Many, many bugfixes\n - Performance improvements\n - Backend rewritten to use MLIR\n-- Support for kernels taht contain back-to-back matmuls (e.g., flash attention)\n+- Support for kernels that contain back-to-back matmuls (e.g., flash attention)\n \n # Contributing\n \n@@ -64,4 +64,4 @@ Supported Platforms:\n \n Supported Hardware:\n   * NVIDIA GPUs (Compute Capability 7.0+)\n-  * Under development: AMD GPUs, CPUs\n\\ No newline at end of file\n+  * Under development: AMD GPUs, CPUs"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 19, "deletions": 7, "changes": 26, "file_content_changes": "@@ -405,19 +405,31 @@ def TT_MakeRangeOp : TT_Op<\"make_range\", [Pure]> {\n }\n \n //\n-// Make PrintfOp\n+// Make PrintOp\n //\n-def TT_PrintfOp : TT_Op<\"printf\", [MemoryEffects<[MemWrite]>]>,\n-  Arguments<(ins StrAttr:$prefix,\n-                Variadic<AnyTypeOf<[TT_Type]>>:$args)> {\n-  let summary = \"Device-side printf, as in CUDA for debugging\";\n+def TT_PrintOp : TT_Op<\"print\", [MemoryEffects<[MemWrite]>]>,\n+  Arguments<(ins StrAttr:$prefix, Variadic<AnyTypeOf<[TT_Type]>>:$args)> {\n+  let summary = \"Device-side print, as in CUDA for debugging\";\n   let description = [{\n-    `tt.printf` takes a literal string prefix and an arbitrary number of scalar or tensor arguments that should be printed.\n+    `tt.print` takes a literal string prefix and an arbitrary number of scalar or tensor arguments that should be printed.\n     format are generated automatically from the arguments.\n   }];\n   let assemblyFormat = [{\n-    $prefix attr-dict ($args^ `:` type($args))?\n+    $prefix attr-dict `:` ($args^ `:` type($args))?\n   }];\n }\n \n+//\n+// Make AssertOp\n+//\n+def TT_AssertOp : TT_Op<\"assert\", [MemoryEffects<[MemWrite]>]> {\n+  let summary = \"Device-side assert, as in CUDA for correctness checking\";\n+  let description = [{\n+    `tt.assert` takes a condition tensor, a message string, a file string, a function string, and a line number.\n+    If the condition is false, the message is printed, and the program is aborted.\n+  }];\n+  let arguments = (ins TT_Tensor:$condition, StrAttr:$message, StrAttr:$file, StrAttr:$func, I32Attr:$line);\n+  let assemblyFormat = \"$condition `,` $message `,` $file `,` $func `,` $line attr-dict `:` type($condition)\";\n+}\n+\n #endif // Triton_OPS"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -34,12 +34,12 @@ struct FpToFpOpConversion\n                    \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\"\n                    \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\"\n                    \"}\";\n-    auto &call = *builder.create(ptxAsm);\n+    auto &ptxOp = *builder.create(ptxAsm);\n \n     auto *o0 = builder.newOperand(\"=r\");\n     auto *o1 = builder.newOperand(\"=r\");\n     auto *i = builder.newOperand(fp8x4Vec, \"r\");\n-    call({o0, o1, i}, /*onlyAttachMLIRArgs=*/true);\n+    ptxOp({o0, o1, i}, /*onlyAttachMLIRArgs=*/true);\n \n     auto fp16x2VecTy = vec_ty(f16_ty, 2);\n     auto fp16x2x2StructTy =\n@@ -81,12 +81,12 @@ struct FpToFpOpConversion\n                    \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n                    \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n                    \"}\";\n-    auto &call = *builder.create(ptxAsm);\n+    auto &ptxOp = *builder.create(ptxAsm);\n \n     auto *o = builder.newOperand(\"=r\");\n     auto *i0 = builder.newOperand(fp16x2Vec0, \"r\");\n     auto *i1 = builder.newOperand(fp16x2Vec1, \"r\");\n-    call({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n+    ptxOp({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n \n     auto fp8x4VecTy = vec_ty(i8_ty, 4);\n     auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n@@ -125,12 +125,12 @@ struct FpToFpOpConversion\n                    \"or.b32 $0, sign0, nosign0;                 \\n\"\n                    \"or.b32 $1, sign1, nosign1;                 \\n\"\n                    \"}\";\n-    auto &call = *builder.create(ptxAsm);\n+    auto &ptxOp = *builder.create(ptxAsm);\n \n     auto *o0 = builder.newOperand(\"=r\");\n     auto *o1 = builder.newOperand(\"=r\");\n     auto *i = builder.newOperand(fp8x4Vec, \"r\");\n-    call({o0, o1, i}, /* onlyAttachMLIRArgs */ true);\n+    ptxOp({o0, o1, i}, /* onlyAttachMLIRArgs */ true);\n \n     auto bf16x2VecTy = vec_ty(i16_ty, 2);\n     auto bf16x2x2StructTy =\n@@ -196,12 +196,12 @@ struct FpToFpOpConversion\n                    \"prmt.b32 nosign, nosign0, nosign1, 0x6420;   \\n\"\n                    \"or.b32 $0, nosign, sign;                     \\n\"\n                    \"}\";\n-    auto &call = *builder.create(ptxAsm);\n+    auto &ptxOp = *builder.create(ptxAsm);\n \n     auto *o = builder.newOperand(\"=r\");\n     auto *i0 = builder.newOperand(bf16x2Vec0, \"r\");\n     auto *i1 = builder.newOperand(bf16x2Vec1, \"r\");\n-    call({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n+    ptxOp({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n \n     auto fp8x4VecTy = vec_ty(i8_ty, 4);\n     auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -324,7 +324,7 @@ struct StoreOpConversion\n           assert(elemOffset < valueElems.size());\n           Value elem = valueElems[elemOffset];\n           if (elem.getType().isInteger(1))\n-            elem = rewriter.create<LLVM::SExtOp>(loc, type::i8Ty(ctx), elem);\n+            elem = sext(i8_ty, elem);\n           elem = bitcast(elem, valueElemTy);\n \n           llWord = insert_element(wordTy, llWord, elem, i32_val(elemIdx));"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 131, "deletions": 65, "changes": 196, "file_content_changes": "@@ -1,6 +1,7 @@\n #include \"TritonGPUToLLVM.h\"\n #include \"DotOpHelpers.h\"\n #include \"Utility.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n \n using namespace mlir;\n using namespace mlir::triton;\n@@ -90,13 +91,13 @@ struct BroadcastOpConversion\n   }\n };\n \n-struct PrintfOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintfOp> {\n+struct PrintOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n-      triton::PrintfOp>::ConvertTritonGPUOpToLLVMPattern;\n+      triton::PrintOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   LogicalResult\n-  matchAndRewrite(triton::PrintfOp op, OpAdaptor adaptor,\n+  matchAndRewrite(triton::PrintOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     auto loc = op->getLoc();\n     SmallVector<Value, 16> operands;\n@@ -168,74 +169,45 @@ struct PrintfOpConversion\n     auto type = value.getType();\n     Value newOp = value;\n     Type newType = type;\n+    auto loc = UnknownLoc::get(context);\n \n     bool bUnsigned = type.isUnsignedInteger();\n     if (type.isIntOrIndex() && type.getIntOrFloatBitWidth() < 32) {\n       if (bUnsigned) {\n         newType = ui32_ty;\n-        newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n-                                              value);\n+        newOp = zext(newType, value);\n       } else {\n         newType = i32_ty;\n-        newOp = rewriter.create<LLVM::SExtOp>(UnknownLoc::get(context), newType,\n-                                              value);\n+        newOp = sext(newType, value);\n       }\n     } else if (type.isBF16() || type.isF16() || type.isF32()) {\n       newType = f64_ty;\n-      newOp = rewriter.create<LLVM::FPExtOp>(UnknownLoc::get(context), newType,\n-                                             value);\n+      newOp = fpext(newType, value);\n     }\n \n     return {newType, newOp};\n   }\n \n   static void llPrintf(StringRef msg, ValueRange args,\n                        ConversionPatternRewriter &rewriter) {\n-    static const char formatStringPrefix[] = \"printfFormat_\";\n     assert(!msg.empty() && \"printf with empty string not support\");\n     Type int8Ptr = ptr_ty(i8_ty);\n \n-    auto *context = rewriter.getContext();\n+    auto *ctx = rewriter.getContext();\n     auto moduleOp =\n         rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n     auto funcOp = getVprintfDeclaration(rewriter);\n+    auto loc = UnknownLoc::get(ctx);\n \n-    Value one = rewriter.create<LLVM::ConstantOp>(\n-        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(1));\n-    Value zero = rewriter.create<LLVM::ConstantOp>(\n-        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(0));\n-\n-    unsigned stringNumber = 0;\n-    SmallString<16> stringConstName;\n-    do {\n-      stringConstName.clear();\n-      (formatStringPrefix + Twine(stringNumber++)).toStringRef(stringConstName);\n-    } while (moduleOp.lookupSymbol(stringConstName));\n-\n-    llvm::SmallString<64> formatString(msg);\n-    formatString.push_back('\\n');\n-    formatString.push_back('\\0');\n-    size_t formatStringSize = formatString.size_in_bytes();\n-    auto globalType = LLVM::LLVMArrayType::get(i8_ty, formatStringSize);\n-\n-    LLVM::GlobalOp global;\n-    {\n-      ConversionPatternRewriter::InsertionGuard guard(rewriter);\n-      rewriter.setInsertionPointToStart(moduleOp.getBody());\n-      global = rewriter.create<LLVM::GlobalOp>(\n-          UnknownLoc::get(context), globalType,\n-          /*isConstant=*/true, LLVM::Linkage::Internal, stringConstName,\n-          rewriter.getStringAttr(formatString));\n-    }\n-\n-    Value globalPtr =\n-        rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(context), global);\n-    Value stringStart = rewriter.create<LLVM::GEPOp>(\n-        UnknownLoc::get(context), int8Ptr, globalPtr,\n-        SmallVector<Value>({zero, zero}));\n+    Value one = i32_val(1);\n+    Value zero = i32_val(0);\n \n-    Value bufferPtr =\n-        rewriter.create<LLVM::NullOp>(UnknownLoc::get(context), int8Ptr);\n+    llvm::SmallString<64> msgNewline(msg);\n+    msgNewline.push_back('\\n');\n+    msgNewline.push_back('\\0');\n+    Value prefixString =\n+        LLVM::addStringToModule(loc, rewriter, \"printfFormat_\", msgNewline);\n+    Value bufferPtr = null(int8Ptr);\n \n     SmallVector<Value, 16> newArgs;\n     if (args.size() >= 1) {\n@@ -248,27 +220,120 @@ struct PrintfOpConversion\n         newArgs.push_back(newArg);\n       }\n \n-      Type structTy = LLVM::LLVMStructType::getLiteral(context, argTypes);\n-      auto allocated = rewriter.create<LLVM::AllocaOp>(UnknownLoc::get(context),\n-                                                       ptr_ty(structTy), one,\n-                                                       /*alignment=*/0);\n+      Type structTy = LLVM::LLVMStructType::getLiteral(ctx, argTypes);\n+      auto allocated =\n+          rewriter.create<LLVM::AllocaOp>(loc, ptr_ty(structTy), one,\n+                                          /*alignment=*/0);\n \n       for (const auto &entry : llvm::enumerate(newArgs)) {\n-        auto index = rewriter.create<LLVM::ConstantOp>(\n-            UnknownLoc::get(context), i32_ty,\n-            rewriter.getI32IntegerAttr(entry.index()));\n-        auto fieldPtr = rewriter.create<LLVM::GEPOp>(\n-            UnknownLoc::get(context), ptr_ty(argTypes[entry.index()]),\n-            allocated, ArrayRef<Value>{zero, index});\n-        rewriter.create<LLVM::StoreOp>(UnknownLoc::get(context), entry.value(),\n-                                       fieldPtr);\n+        auto index = i32_val(entry.index());\n+        auto fieldPtr = gep(ptr_ty(argTypes[entry.index()]), allocated,\n+                            ArrayRef<Value>{zero, index});\n+        store(entry.value(), fieldPtr);\n       }\n-      bufferPtr = rewriter.create<LLVM::BitcastOp>(UnknownLoc::get(context),\n-                                                   int8Ptr, allocated);\n+      bufferPtr = bitcast(allocated, int8Ptr);\n     }\n \n-    SmallVector<Value> operands{stringStart, bufferPtr};\n-    rewriter.create<LLVM::CallOp>(UnknownLoc::get(context), funcOp, operands);\n+    SmallVector<Value> operands{prefixString, bufferPtr};\n+    call(funcOp, operands);\n+  }\n+};\n+\n+struct AssertOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::AssertOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::AssertOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::AssertOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op.getLoc();\n+    auto ctx = rewriter.getContext();\n+    auto elems = getElementsFromStruct(loc, adaptor.getCondition(), rewriter);\n+    auto elemTy = elems[0].getType();\n+    Value condition = int_val(elemTy.getIntOrFloatBitWidth(), 0);\n+    for (auto elem : elems) {\n+      if (elemTy.isSignedInteger() || elemTy.isSignlessInteger()) {\n+        condition =\n+            or_(condition,\n+                icmp_eq(elem, rewriter.create<LLVM::ConstantOp>(\n+                                  loc, elemTy, rewriter.getZeroAttr(elemTy))));\n+      } else {\n+        assert(false && \"Unsupported type for assert\");\n+        return failure();\n+      }\n+    }\n+    llAssert(op, condition, adaptor.getMessage(), adaptor.getFile(),\n+             adaptor.getFunc(), adaptor.getLine(), rewriter);\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+\n+  // op: the op at which the assert is inserted. Unlike printf, we need to\n+  // know about the op to split the block.\n+  static void llAssert(Operation *op, Value condition, StringRef message,\n+                       StringRef file, StringRef func, int line,\n+                       ConversionPatternRewriter &rewriter) {\n+    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+    auto ctx = rewriter.getContext();\n+    auto loc = op->getLoc();\n+\n+    // #block1\n+    // if (condition) {\n+    //   #block2\n+    //   __assertfail(message);\n+    // }\n+    // #block3\n+    Block *prevBlock = op->getBlock();\n+    Block *ifBlock = rewriter.splitBlock(prevBlock, op->getIterator());\n+    rewriter.setInsertionPointToStart(ifBlock);\n+\n+    auto funcOp = getAssertfailDeclaration(rewriter);\n+    auto moduleOp =\n+        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    Value messageString =\n+        LLVM::addStringToModule(loc, rewriter, \"assertMessage_\", message);\n+    Value fileString =\n+        LLVM::addStringToModule(loc, rewriter, \"assertFile_\", file);\n+    Value funcString =\n+        LLVM::addStringToModule(loc, rewriter, \"assertFunc_\", func);\n+    Value lineNumber = i32_val(line);\n+    Value charSize = int_val(sizeof(size_t) * 8, sizeof(char));\n+\n+    SmallVector<Value> operands = {messageString, fileString, lineNumber,\n+                                   funcString, charSize};\n+    auto ret = call(funcOp, operands);\n+\n+    // Split a block after the call.\n+    Block *thenBlock = rewriter.splitBlock(ifBlock, op->getIterator());\n+    rewriter.setInsertionPointToEnd(ifBlock);\n+    rewriter.create<cf::BranchOp>(loc, thenBlock);\n+    rewriter.setInsertionPointToEnd(prevBlock);\n+    rewriter.create<cf::CondBranchOp>(loc, condition, ifBlock, thenBlock);\n+  }\n+\n+  static LLVM::LLVMFuncOp\n+  getAssertfailDeclaration(ConversionPatternRewriter &rewriter) {\n+    auto moduleOp =\n+        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    StringRef funcName(\"__assertfail\");\n+    Operation *funcOp = moduleOp.lookupSymbol(funcName);\n+    if (funcOp)\n+      return cast<LLVM::LLVMFuncOp>(*funcOp);\n+\n+    // void __assert_fail(const char * assertion, const char * file, unsigned\n+    // int line, const char * function);\n+    auto *ctx = rewriter.getContext();\n+    SmallVector<Type> argsType{ptr_ty(i8_ty), ptr_ty(i8_ty), i32_ty,\n+                               ptr_ty(i8_ty),\n+                               rewriter.getIntegerType(sizeof(size_t) * 8)};\n+    auto funcType = LLVM::LLVMFunctionType::get(void_ty(ctx), argsType);\n+\n+    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+    rewriter.setInsertionPointToStart(moduleOp.getBody());\n+\n+    return rewriter.create<LLVM::LLVMFuncOp>(UnknownLoc::get(ctx), funcName,\n+                                             funcType);\n   }\n };\n \n@@ -528,7 +593,7 @@ namespace LLVM {\n \n void vprintf(StringRef msg, ValueRange args,\n              ConversionPatternRewriter &rewriter) {\n-  PrintfOpConversion::llPrintf(msg, args, rewriter);\n+  PrintOpConversion::llPrintf(msg, args, rewriter);\n }\n \n void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n@@ -565,5 +630,6 @@ void populateTritonGPUToLLVMPatterns(\n   patterns.add<GetNumProgramsOpConversion>(typeConverter, benefit);\n   patterns.add<MakeRangeOpConversion>(typeConverter, indexCacheInfo, benefit);\n   patterns.add<ReturnOpConversion>(typeConverter, benefit);\n-  patterns.add<PrintfOpConversion>(typeConverter, benefit);\n+  patterns.add<PrintOpConversion>(typeConverter, benefit);\n+  patterns.add<AssertOpConversion>(typeConverter, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -127,9 +127,6 @@ class TritonLLVMConversionTarget : public ConversionTarget {\n   }\n };\n \n-using FPTruncLowering =\n-    VectorConvertToLLVMPattern<LLVM::FPTruncOp, arith::TruncFOp>;\n-\n class ConvertTritonGPUToLLVM\n     : public ConvertTritonGPUToLLVMBase<ConvertTritonGPUToLLVM> {\n \n@@ -205,6 +202,8 @@ class ConvertTritonGPUToLLVM\n     populatePatterns2(populateViewOpToLLVMPatterns);\n     // Native lowering patterns\n     mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n+    mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n+                                                          patterns);\n     if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n       return signalPassFailure();\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.cpp", "status": "modified", "additions": 34, "deletions": 0, "changes": 34, "file_content_changes": "@@ -112,5 +112,39 @@ Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n   return builder.launch(rewriter, loc, val.getType(), false);\n }\n \n+Value addStringToModule(Location loc, ConversionPatternRewriter &rewriter,\n+                        StringRef key, StringRef content) {\n+  auto moduleOp = rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+  auto ctx = moduleOp.getContext();\n+  unsigned stringNumber = 0;\n+  SmallString<16> stringConstName;\n+  do {\n+    stringConstName.clear();\n+    (key + Twine(stringNumber++)).toStringRef(stringConstName);\n+  } while (moduleOp.lookupSymbol(stringConstName));\n+\n+  llvm::SmallString<64> contentStr(content);\n+  size_t contentSize = contentStr.size_in_bytes();\n+  auto globalType = LLVM::LLVMArrayType::get(i8_ty, contentSize);\n+\n+  LLVM::GlobalOp global;\n+  {\n+    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+    rewriter.setInsertionPointToStart(moduleOp.getBody());\n+    global = rewriter.create<LLVM::GlobalOp>(\n+        UnknownLoc::get(ctx), globalType,\n+        /*isConstant=*/true, LLVM::Linkage::Internal, stringConstName,\n+        rewriter.getStringAttr(contentStr));\n+  }\n+\n+  Value zero = i32_val(0);\n+  Value globalPtr =\n+      rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(ctx), global);\n+  Value stringStart =\n+      rewriter.create<LLVM::GEPOp>(UnknownLoc::get(ctx), ptr_ty(i8_ty),\n+                                   globalPtr, SmallVector<Value>({zero, zero}));\n+  return stringStart;\n+}\n+\n } // namespace LLVM\n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 14, "deletions": 2, "changes": 16, "file_content_changes": "@@ -12,6 +12,8 @@\n #define inttoptr(...) rewriter.create<LLVM::IntToPtrOp>(loc, __VA_ARGS__)\n #define ptrtoint(...) rewriter.create<LLVM::PtrToIntOp>(loc, __VA_ARGS__)\n #define zext(...) rewriter.create<LLVM::ZExtOp>(loc, __VA_ARGS__)\n+#define sext(...) rewriter.create<LLVM::SExtOp>(loc, __VA_ARGS__)\n+#define fpext(...) rewriter.create<LLVM::FPExtOp>(loc, __VA_ARGS__)\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n #define urem(...) rewriter.create<LLVM::URemOp>(loc, __VA_ARGS__)\n #define add(...) rewriter.create<LLVM::AddOp>(loc, __VA_ARGS__)\n@@ -27,6 +29,7 @@\n #define fmin(...) rewriter.create<LLVM::MinNumOp>(loc, __VA_ARGS__)\n #define and_(...) rewriter.create<LLVM::AndOp>(loc, __VA_ARGS__)\n #define xor_(...) rewriter.create<LLVM::XOrOp>(loc, __VA_ARGS__)\n+#define or_(...) rewriter.create<LLVM::OrOp>(loc, __VA_ARGS__)\n #define bitcast(val__, type__)                                                 \\\n   rewriter.create<LLVM::BitcastOp>(loc, type__, val__)\n #define gep(...) rewriter.create<LLVM::GEPOp>(loc, __VA_ARGS__)\n@@ -45,6 +48,9 @@\n #define fcmp_olt(lhs, rhs)                                                     \\\n   rewriter.create<LLVM::FCmpOp>(loc, rewriter.getI1Type(),                     \\\n                                 LLVM::FCmpPredicate::olt, lhs, rhs)\n+#define fcmp_eq(lhs, rhs)                                                      \\\n+  rewriter.create<LLVM::FCmpOp>(loc, rewriter.getI1Type(),                     \\\n+                                LLVM::FCmpPredicate::oeq, lhs, rhs)\n #define icmp_eq(...)                                                           \\\n   rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::eq, __VA_ARGS__)\n #define icmp_ne(...)                                                           \\\n@@ -69,6 +75,8 @@\n #define address_of(...) rewriter.create<LLVM::AddressOfOp>(loc, __VA_ARGS__)\n #define barrier() rewriter.create<mlir::gpu::BarrierOp>(loc)\n #define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n+#define null(...) rewriter.create<LLVM::NullOp>(loc, __VA_ARGS__)\n+#define call(...) rewriter.create<LLVM::CallOp>(loc, __VA_ARGS__)\n \n // Types\n #define i64_ty rewriter.getIntegerType(64)\n@@ -78,16 +86,17 @@\n #define f16_ty rewriter.getF16Type()\n #define bf16_ty rewriter.getBF16Type()\n #define i8_ty rewriter.getIntegerType(8)\n+#define i1_ty rewriter.getI1Type()\n #define f32_ty rewriter.getF32Type()\n #define f64_ty rewriter.getF64Type()\n #define vec_ty(type, num) VectorType::get(num, type)\n-#define f32_val(...) LLVM::createConstantF32(loc, rewriter, __VA_ARGS__)\n-#define f64_val(...) LLVM::createConstantF64(loc, rewriter, __VA_ARGS__)\n #define void_ty(ctx) LLVM::LLVMVoidType::get(ctx)\n #define struct_ty(...) LLVM::LLVMStructType::getLiteral(ctx, __VA_ARGS__)\n #define array_ty(elemTy, count) LLVM::LLVMArrayType::get(elemTy, count)\n \n // Constants\n+#define f32_val(...) LLVM::createConstantF32(loc, rewriter, __VA_ARGS__)\n+#define f64_val(...) LLVM::createConstantF64(loc, rewriter, __VA_ARGS__)\n #define i32_val(...) LLVM::createConstantI32(loc, rewriter, __VA_ARGS__)\n #define int_val(width, val)                                                    \\\n   LLVM::createLLVMIntegerConstant(rewriter, loc, width, val)\n@@ -253,6 +262,9 @@ Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n                int i);\n \n+Value addStringToModule(Location loc, ConversionPatternRewriter &rewriter,\n+                        StringRef key, StringRef content);\n+\n } // namespace LLVM\n } // namespace mlir\n "}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 21, "deletions": 6, "changes": 27, "file_content_changes": "@@ -476,19 +476,34 @@ struct TritonReducePattern : public OpConversionPattern<triton::ReduceOp> {\n   }\n };\n \n-struct TritonPrintfPattern : public OpConversionPattern<triton::PrintfOp> {\n-  using OpConversionPattern<PrintfOp>::OpConversionPattern;\n+struct TritonPrintPattern : public OpConversionPattern<triton::PrintOp> {\n+  using OpConversionPattern<triton::PrintOp>::OpConversionPattern;\n \n   LogicalResult\n-  matchAndRewrite(PrintfOp op, typename PrintfOp::Adaptor adaptor,\n+  matchAndRewrite(triton::PrintOp op, typename triton::PrintOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::PrintfOp>(\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::PrintOp>(\n                       op, op.getPrefixAttr(), adaptor.getOperands()),\n                   adaptor.getAttributes());\n     return success();\n   }\n };\n \n+struct TritonAssertPattern : public OpConversionPattern<triton::AssertOp> {\n+  using OpConversionPattern<triton::AssertOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::AssertOp op,\n+                  typename triton::AssertOp::Adaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::AssertOp>(\n+                      op, adaptor.getCondition(), op.getMessageAttr(),\n+                      op.getFileAttr(), op.getFuncAttr(), op.getLineAttr()),\n+                  adaptor.getAttributes());\n+    return success();\n+  }\n+};\n+\n void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n                             RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n@@ -503,8 +518,8 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n           TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n           TritonReducePattern, TritonTransPattern, TritonExpandDimsPattern,\n           TritonMakeRangePattern, TritonDotPattern, TritonLoadPattern,\n-          TritonStorePattern, TritonExtElemwisePattern, TritonPrintfPattern,\n-          TritonAtomicRMWPattern>(typeConverter, context);\n+          TritonStorePattern, TritonExtElemwisePattern, TritonPrintPattern,\n+          TritonAssertPattern, TritonAtomicRMWPattern>(typeConverter, context);\n }\n \n //"}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -215,7 +215,7 @@ def build_extension(self, ext):\n     long_description=\"\",\n     packages=[\"triton\", \"triton/_C\", \"triton/language\", \"triton/tools\", \"triton/impl\", \"triton/ops\", \"triton/runtime\", \"triton/ops/blocksparse\"],\n     install_requires=[\n-        \"cmake\",\n+        \"cmake>=3.20\",\n         \"filelock\",\n         \"torch\",\n         \"lit\","}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 18, "deletions": 2, "changes": 20, "file_content_changes": "@@ -1341,16 +1341,32 @@ void init_triton_ir(py::module &&m) {\n              return self.create<mlir::arith::SelectOp>(loc, condition,\n                                                        trueValue, falseValue);\n            })\n-      .def(\"create_printf\",\n+      .def(\"create_print\",\n            [](mlir::OpBuilder &self, const std::string &prefix,\n               const std::vector<mlir::Value> &values) -> void {\n              auto loc = self.getUnknownLoc();\n-             self.create<mlir::triton::PrintfOp>(\n+             self.create<mlir::triton::PrintOp>(\n                  loc,\n                  mlir::StringAttr::get(self.getContext(),\n                                        llvm::StringRef(prefix)),\n                  values);\n            })\n+      .def(\"create_assert\",\n+           [](mlir::OpBuilder &self, mlir::Value &condition,\n+              const std::string &message, const std::string &fileName,\n+              const std::string &funcName, unsigned lineNo) -> void {\n+             auto loc = self.getUnknownLoc();\n+             auto messageAttr = mlir::StringAttr::get(self.getContext(),\n+                                                      llvm::StringRef(message));\n+             auto fileNameAttr = mlir::StringAttr::get(\n+                 self.getContext(), llvm::StringRef(fileName));\n+             auto funcNameAttr = mlir::StringAttr::get(\n+                 self.getContext(), llvm::StringRef(funcName));\n+             auto lineNoAttr = self.getI32IntegerAttr(lineNo);\n+             self.create<mlir::triton::AssertOp>(loc, condition, messageAttr,\n+                                                 fileNameAttr, funcNameAttr,\n+                                                 lineNoAttr);\n+           })\n       // Undef\n       .def(\"create_undef\",\n            [](mlir::OpBuilder &self, mlir::Type &type) -> mlir::Value {"}, {"filename": "python/test/unit/language/assert_helper.py", "status": "added", "additions": 45, "deletions": 0, "changes": 45, "file_content_changes": "@@ -0,0 +1,45 @@\n+import sys\n+\n+import torch\n+from torch.testing import assert_close\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def kernel_device_assert(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    tl.device_assert(x == 0, \"x != 0\")\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+@triton.jit\n+def kernel_assert(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    assert x == 0, \"x != 0\"\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+@triton.jit\n+def kernel_static_assert(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    tl.static_assert(BLOCK == 128, \"BLOCK != 128\")\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+def test_assert(func: str):\n+    shape = (128, )\n+    x = torch.arange(0, shape[0], dtype=torch.int32, device='cuda')\n+    y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n+    if func == \"device_assert\":\n+        kernel_device_assert[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"assert\":\n+        kernel_assert[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"static_assert\":\n+        kernel_static_assert[(1,)](x, y, BLOCK=shape[0])\n+    assert_close(y, x)\n+\n+\n+if __name__ == \"__main__\":\n+    test_assert(sys.argv[1])"}, {"filename": "python/test/unit/language/print_helper.py", "status": "added", "additions": 46, "deletions": 0, "changes": 46, "file_content_changes": "@@ -0,0 +1,46 @@\n+import sys\n+\n+import torch\n+from torch.testing import assert_close\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def kernel_device_print(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    tl.device_print(\"\", x)\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+@triton.jit\n+def kernel_print(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    print(\"\", x)\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+@triton.jit\n+def kernel_static_print(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    tl.static_print(x)\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+def test_print(func: str, data_type: str):\n+    shape = (128, )\n+    # limit the range of integers so that the sum does not overflow\n+    x = torch.arange(0, shape[0], dtype=torch.int32, device='cuda').to(getattr(torch, data_type))\n+    y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n+    if func == \"device_print\":\n+        kernel_device_print[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"print\":\n+        kernel_print[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"static_print\":\n+        kernel_static_print[(1,)](x, y, BLOCK=shape[0])\n+    assert_close(y, x)\n+\n+\n+if __name__ == \"__main__\":\n+    test_print(sys.argv[1], sys.argv[2])"}, {"filename": "python/test/unit/language/printf_helper.py", "status": "removed", "additions": 0, "deletions": 56, "changes": 56, "file_content_changes": "@@ -1,56 +0,0 @@\n-import torch\n-from torch.testing import assert_close\n-\n-import triton\n-import triton.language as tl\n-\n-torch_type = {\n-    \"bool\": torch.bool,\n-    'int8': torch.int8,\n-    'uint8': torch.uint8,\n-    'int16': torch.int16,\n-    \"int32\": torch.int32,\n-    'int64': torch.long,\n-    'float16': torch.float16,\n-    'bfloat16': torch.bfloat16,\n-    \"float32\": torch.float32,\n-    \"float64\": torch.float64\n-}\n-\n-\n-def get_tensor(shape, data_type, b_positive=False):\n-    x = None\n-    if data_type.startswith('int'):\n-        x = torch.arange(0, shape[0], dtype=torch_type[data_type], device='cuda')\n-    else:\n-        x = torch.arange(0, shape[0], dtype=torch_type[data_type], device='cuda')\n-\n-    return x\n-\n-# @pytest.mark.parametrize('data_type',\n-#                          [(\"int8\"),\n-#                           ('int16'),\n-#                           ('int32'),\n-#                           (\"int64\"),\n-#                           ('float16'),\n-#                           (\"float32\"),\n-#                           (\"float64\")])\n-\n-\n-def printf(data_type):\n-    @triton.jit\n-    def kernel(X, Y, BLOCK: tl.constexpr):\n-        x = tl.load(X + tl.arange(0, BLOCK))\n-        tl.printf(\"\", x)\n-        tl.store(Y + tl.arange(0, BLOCK), x)\n-\n-    shape = (128, )\n-    # limit the range of integers so that the sum does not overflow\n-    x = get_tensor(shape, data_type)\n-    y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n-    kernel[(1,)](x, y, BLOCK=shape[0])\n-    assert_close(y, x)\n-\n-\n-printf(\"float16\")\n-printf(\"int8\")"}, {"filename": "python/test/unit/language/test_printf.py", "status": "removed", "additions": 0, "deletions": 22, "changes": 22, "file_content_changes": "@@ -1,22 +0,0 @@\n-import os\n-import subprocess\n-import sys\n-\n-dir_path = os.path.dirname(os.path.realpath(__file__))\n-printf_path = os.path.join(dir_path, \"printf_helper.py\")\n-\n-\n-def test_printf():\n-    proc = subprocess.Popen([sys.executable, printf_path], stdout=subprocess.PIPE, shell=False)\n-    (outs, err) = proc.communicate()\n-    outs = outs.split()\n-    new_lines = set()\n-    for line in outs:\n-        try:\n-            value = int(float(line))\n-            new_lines.add(value)\n-        except Exception as e:\n-            print(e)\n-    for i in range(128):\n-        assert i in new_lines\n-    assert len(new_lines) == 128"}, {"filename": "python/test/unit/language/test_subprocess.py", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -0,0 +1,53 @@\n+import os\n+import subprocess\n+import sys\n+\n+import pytest\n+\n+dir_path = os.path.dirname(os.path.realpath(__file__))\n+print_path = os.path.join(dir_path, \"print_helper.py\")\n+assert_path = os.path.join(dir_path, \"assert_helper.py\")\n+\n+# TODO: bfloat16 after LLVM-15\n+func_types = [\"device_assert\", \"assert\", \"static_assert\"]\n+torch_types = [\"int8\", \"uint8\", \"int16\", \"int32\", \"long\", \"float16\", \"float32\", \"float64\"]\n+\n+\n+@pytest.mark.parametrize(\"func_type, data_type\",\n+                         [(\"device_print\", data_type) for data_type in torch_types] + [(\"print\", \"int32\"), (\"static_print\", \"int32\")])\n+def test_print(func_type: str, data_type: str):\n+    proc = subprocess.Popen([sys.executable, print_path, func_type, data_type], stdout=subprocess.PIPE, shell=False)\n+    outs, _ = proc.communicate()\n+    outs = outs.split()\n+    new_lines = set()\n+    for line in outs:\n+        try:\n+            value = line\n+            if func_type != \"static_print\":\n+                value = int(float(line))\n+            new_lines.add(value)\n+        except Exception as e:\n+            print(e)\n+    if func_type != \"static_print\":\n+        for i in range(128):\n+            assert i in new_lines\n+        assert len(new_lines) == 128\n+    else:\n+        assert len(new_lines) == 1\n+\n+\n+@pytest.mark.parametrize(\"func_type\", func_types)\n+def test_assert(func_type: str):\n+    os.environ[\"TRITON_DEBUG\"] = \"1\"\n+    proc = subprocess.Popen([sys.executable, assert_path, func_type], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)\n+    _, errs = proc.communicate()\n+    errs = errs.splitlines()\n+    num_errs = 0\n+    for err in errs:\n+        if \"x != 0\" in err.decode(\"utf-8\"):\n+            num_errs += 1\n+    os.environ[\"TRITON_DEBUG\"] = \"0\"\n+    if func_type != \"static_assert\":\n+        assert num_errs == 127\n+    else:\n+        assert num_errs == 0"}, {"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "file_content_changes": "@@ -148,6 +148,26 @@ def kernel_add(a, b, o, N: tl.constexpr):\n     assert len(kernel_add.cache) == 1\n \n \n+def test_jit_debug() -> None:\n+    @triton.jit\n+    def kernel_add(a, b, o, N: tl.constexpr):\n+        idx = tl.arange(0, N)\n+        tl.device_assert(idx < 32, \"idx < 32\")\n+        tl.store(o + idx,\n+                 tl.load(a + idx) + tl.load(b + idx))\n+\n+    device = torch.cuda.current_device()\n+    assert len(kernel_add.cache[device]) == 0\n+    kernel_add.warmup(torch.float32, torch.float32, torch.float32, 32, grid=(1,))\n+    assert len(kernel_add.cache[device]) == 1\n+    kernel_add.debug = False\n+    kernel_add.warmup(torch.float32, torch.float32, torch.float32, 32, grid=(1,))\n+    assert len(kernel_add.cache[device]) == 1\n+    kernel_add.debug = True\n+    kernel_add.warmup(torch.float32, torch.float32, torch.float32, 32, grid=(1,))\n+    assert len(kernel_add.cache[device]) == 2\n+\n+\n def test_compile_in_subproc() -> None:\n     @triton.jit\n     def kernel_sub(a, b, o, N: tl.constexpr):"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -28,6 +28,8 @@\n     constexpr,\n     cos,\n     debug_barrier,\n+    device_assert,\n+    device_print,\n     dot,\n     dtype,\n     exp,\n@@ -54,14 +56,15 @@\n     num_programs,\n     pi32_t,\n     pointer_type,\n-    printf,\n     program_id,\n     ravel,\n     reshape,\n     sigmoid,\n     sin,\n     softmax,\n     sqrt,\n+    static_assert,\n+    static_print,\n     store,\n     sum,\n     swizzle2d,\n@@ -118,6 +121,8 @@\n     \"constexpr\",\n     \"cos\",\n     \"debug_barrier\",\n+    \"device_assert\",\n+    \"device_print\",\n     \"dot\",\n     \"dtype\",\n     \"exp\",\n@@ -149,7 +154,6 @@\n     \"philox_impl\",\n     \"pi32_t\",\n     \"pointer_type\",\n-    \"printf\",\n     \"program_id\",\n     \"rand\",\n     \"rand4x\",\n@@ -164,6 +168,8 @@\n     \"softmax\",\n     \"sqrt\",\n     \"static_range\",\n+    \"static_assert\",\n+    \"static_print\",\n     \"store\",\n     \"sum\",\n     \"swizzle2d\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 40, "deletions": 8, "changes": 48, "file_content_changes": "@@ -1329,24 +1329,56 @@ def zeros(shape, dtype):\n def zeros_like(input):\n     return zeros(input.shape, input.dtype)\n \n+# -----------------------\n+# Debugging functions\n+# -----------------------\n+\n+\n+@builtin\n+def static_print(*values, sep: str = \" \", end: str = \"\\n\", file=None, flush=False, _builder=None):\n+    pass\n+\n \n @builtin\n-def printf(prefix, *args, _builder=None):\n+def static_assert(cond, msg=\"\", _builder=None):\n+    pass\n+\n+\n+@builtin\n+def device_print(prefix, *args, _builder=None):\n     import string\n-    new_prefix = prefix\n-    if isinstance(prefix, constexpr):\n-        new_prefix = prefix.value\n-    assert isinstance(new_prefix, str), f\"{new_prefix} is not string\"\n+    prefix = _constexpr_to_value(prefix)\n+    assert isinstance(prefix, str), f\"{prefix} is not string\"\n     b_ascii = True\n-    for ch in new_prefix:\n+    for ch in prefix:\n         if ch not in string.printable:\n             b_ascii = False\n             break\n-    assert b_ascii, f\"{new_prefix} is not an ascii string\"\n+    assert b_ascii, f\"{prefix} is not an ascii string\"\n     new_args = []\n     for arg in args:\n         new_args.append(_to_tensor(arg, _builder))\n-    return semantic.printf(new_prefix, new_args, _builder)\n+    return semantic.device_print(prefix, new_args, _builder)\n+\n+\n+@builtin\n+def device_assert(cond, msg=\"\", _builder=None):\n+    msg = _constexpr_to_value(msg)\n+    import inspect\n+    frame = inspect.currentframe()\n+    module = inspect.getmodule(frame)\n+    # The triton function module doesn't have the name attribute.\n+    # We use this trick to find the caller.\n+    while hasattr(module, \"__name__\"):\n+        frame = frame.f_back\n+        module = inspect.getmodule(frame)\n+    func_name = frame.f_code.co_name\n+    file_name = frame.f_back.f_code.co_filename\n+    # TODO: The line number currently indicates the line\n+    # where the triton function is called but not where the\n+    # device_assert is called. Need to enhance this.\n+    lineno = frame.f_back.f_lineno\n+    return semantic.device_assert(_to_tensor(cond, _builder), msg, file_name, func_name, lineno, _builder)\n \n # -----------------------\n # Iterators"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -1234,8 +1234,12 @@ def debug_barrier(builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_barrier(), tl.void)\n \n \n-def printf(prefix: str, args: List[tl.tensor], builder: ir.builder) -> tl.tensor:\n+def device_print(prefix: str, args: List[tl.tensor], builder: ir.builder) -> tl.tensor:\n     new_args = []\n     for arg in args:\n         new_args.append(arg.handle)\n-    return tl.tensor(builder.create_printf(prefix, new_args), tl.void)\n+    return tl.tensor(builder.create_print(prefix, new_args), tl.void)\n+\n+\n+def device_assert(cond: tl.tensor, msg: str, file_name: str, func_name, lineno: int, builder: ir.builder) -> tl.tensor:\n+    return tl.tensor(builder.create_assert(cond.handle, msg, file_name, func_name, lineno), tl.void)"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -241,7 +241,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     sig_key =  {sig_keys},\n     constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else ()}\n     spec_key = {f'{spec_keys},' if len(spec_keys) > 0 else ()}\n-    key = (version_key, sig_key, constexpr_key, spec_key, num_warps, num_stages)\n+    key = (version_key, sig_key, constexpr_key, spec_key, num_warps, num_stages, self.debug)\n     if not extern_libs is None:\n       key = (key, tuple(extern_libs.items()))\n     assert num_warps > 0 and (num_warps & (num_warps - 1)) == 0, \"num_warps must be a power of 2\"\n@@ -276,7 +276,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n         if callable(arg):\n           raise TypeError(f\"Callable constexpr at index {{i}} is not supported\")\n       if not self._call_hook(key, signature, device, constants, num_warps, num_stages, extern_libs, configs):\n-        bin = triton.compile(self, signature=signature, device=device, constants=constants, num_warps=num_warps, num_stages=num_stages, extern_libs=extern_libs, configs=configs)\n+        bin = triton.compile(self, signature=signature, device=device, constants=constants, num_warps=num_warps, num_stages=num_stages, extern_libs=extern_libs, configs=configs, debug=self.debug)\n         if not warmup:\n             bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, triton.compiler.CompiledKernel.launch_enter_hook, triton.compiler.CompiledKernel.launch_exit_hook, bin, *args)\n         self.cache[device][key] = bin\n@@ -289,7 +289,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n         exec(src, scope)\n         return scope[self.fn.__name__]\n \n-    def __init__(self, fn, version=None, do_not_specialize=None):\n+    def __init__(self, fn, version=None, do_not_specialize=None, debug=None):\n         self.fn = fn\n         self.module = fn.__module__\n         self.version = version\n@@ -310,6 +310,7 @@ def __init__(self, fn, version=None, do_not_specialize=None):\n         # when called with a grid using __getitem__\n         self.kernel_decorators = []\n         self.kernel = None\n+        self.debug = os.environ.get(\"TRITON_DEBUG\", \"0\") == \"1\" if debug is None else debug\n         # annotations\n         self.annotations = {self.arg_names.index(name): ty for name, ty in fn.__annotations__.items()}\n         self.__annotations__ = fn.__annotations__\n@@ -378,6 +379,7 @@ def jit(\n     *,\n     version=None,\n     do_not_specialize: Optional[Iterable[int]] = None,\n+    debug: Optional[bool] = None,\n ) -> Callable[[T], JITFunction[T]]:\n     ...\n \n@@ -387,6 +389,7 @@ def jit(\n     *,\n     version=None,\n     do_not_specialize: Optional[Iterable[int]] = None,\n+    debug: Optional[bool] = None,\n ) -> Union[JITFunction[T], Callable[[T], JITFunction[T]]]:\n     \"\"\"\n     Decorator for JIT-compiling a function using the Triton compiler.\n@@ -411,6 +414,7 @@ def decorator(fn: T) -> JITFunction[T]:\n             fn,\n             version=version,\n             do_not_specialize=do_not_specialize,\n+            debug=debug,\n         )\n \n     if fn is not None:"}]