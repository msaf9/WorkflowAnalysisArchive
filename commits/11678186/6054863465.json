[{"filename": ".github/workflows/documentation.yml", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -24,6 +24,7 @@ jobs:\n         run: |\n           pip3 install tabulate\n           pip3 install cmake\n+          pip3 install sphinx\n \n       #- name: Fetch dependent branches\n       #  run: |\n@@ -33,7 +34,7 @@ jobs:\n         run: |\n           cd docs\n           export PATH=$(python3 -c \"import cmake; print(cmake.CMAKE_BIN_DIR)\"):$PATH\n-          python3 -m sphinx_multiversion . _build/html/\n+          python3 -m sphinx . _build/html/main\n \n       - name: Update docs\n         run: |"}, {"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -27,7 +27,7 @@ jobs:\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n             echo '::set-output name=matrix-required::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"H100\"]]'\n-            echo '::set-output name=matrix-optional::[]'\n+            echo '::set-output name=matrix-optional::[[\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]'\n           else\n             echo '::set-output name=matrix-required::[\"ubuntu-latest\"]'\n             echo '::set-output name=matrix-optional::[\"ubuntu-latest\"]'\n@@ -209,10 +209,12 @@ jobs:\n       - name: Install Triton on ROCM\n         if: ${{ env.BACKEND == 'ROCM'}}\n         run: |\n+          git submodule update --init --recursive\n           cd python\n           python3 -m pip install --upgrade pip\n           python3 -m pip install cmake==3.24\n           python3 -m pip install torch==1.13.1 --index-url https://download.pytorch.org/whl/rocm5.2\n+          export TRITON_CODEGEN_AMD_HIP_BACKEND=1\n           python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n       - name: Install Triton on XPU\n@@ -234,7 +236,7 @@ jobs:\n         if: ${{ env.BACKEND == 'ROCM'}}\n         run: |\n           cd python/test/unit/language\n-          python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py::test_empty_kernel\"\n+          python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py\"\n \n       - name: Run python tests on XPU\n         if: ${{ env.BACKEND == 'XPU'}}"}, {"filename": ".gitmodules", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -1,3 +1,7 @@\n [submodule \"third_party/intel_xpu_backend\"]\n \tpath = third_party/intel_xpu_backend\n \turl = http://github.com/intel/intel-xpu-backend-for-triton\n+[submodule \"third_party/amd_hip_backend\"]\n+\tpath = third_party/amd_hip_backend\n+\turl = https://github.com/ROCmSoftwarePlatform/triton\n+\tbranch = third_party_backend_2"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -212,7 +212,6 @@ if(TRITON_BUILD_PYTHON_MODULE)\n     TritonNvidiaGPUTransforms\n     TritonLLVMIR\n     TritonPTX\n-    TritonHSACO\n     ${dialect_libs}\n     ${conversion_libs}\n "}, {"filename": "README.md", "status": "modified", "additions": 27, "deletions": 4, "changes": 31, "file_content_changes": "@@ -4,11 +4,38 @@\n \n [![Wheels](https://github.com/openai/triton/actions/workflows/wheels.yml/badge.svg?branch=release/2.0.x)](https://github.com/openai/triton/actions/workflows/wheels.yml)\n \n+We're hiring! If you are interested in working on Triton at OpenAI, we have roles open for [Compiler Engineers](https://openai.com/careers/software-engineer-triton-compiler) and [Kernel Engineers](https://openai.com/careers/kernel-engineer).\n \n **`Documentation`** |\n ------------------- |\n [![Documentation](https://github.com/openai/triton/actions/workflows/documentation.yml/badge.svg)](https://triton-lang.org/)\n \n+# Triton Developer Conference Registration Open\n+The Triton Developer Conference will be held in a hybrid mode at the Microsoft Silicon Valley Campus in Mountain View, California. The conference will be held on September 20th from 10am to 4pm, followed by a reception till 5:30 pm. Please use the link below to register to attend either in-person or virtually online.\n+\n+Registration Link for Triton Developer Conference is [here](https://forms.office.com/r/m4jQXShDts)\n+\n+Tentative Agenda for the conference (subject to change):\n+\n+|Time    |Title  |Speaker\n+|--------|-------|-------|\n+|10:00 AM|Welcome|Kevin Scott (Microsoft)|\n+|10:20 AM|The Triton Compiler: Past, Present and Future|Phil Tillet (OpenAI)|\n+|11:00 AM|**Break**||\n+|11:20 AM|Hopper support in Triton|Gustav Zhu (Nvidia)|\n+|11:40 AM|Bringing Triton to AMD GPUs|Jason Furmanek, Lixun Zhang (AMD)|\n+|12:00 PM|Intel XPU Backend for Triton|Eikan Wang (Intel)|\n+|12:20 PM|Vectorization of Triton Kernels for Qualcomm Hexagon Backend|Javed Absar (Qualcomm)|\n+|12:30 PM|**Lunch**||\n+|1:40 PM |Triton for MTIA|Roman Levenstein et al, (Meta)|\n+|2:00 PM |Using Triton IR for high-performance fusions in XLA|George Karpenkov (Google)|\n+|2:20 PM |Triton for All: Triton as a device-independent language|Ian Bearman (Microsoft)|\n+|2:40 PM|**Break**||\n+|3:00 PM|PyTorch 2.0 and TorchInductor|Jason Ansel, Horace He (Meta)|\n+|3:20 PM|Pallas: A JAX Kernel Language|Sharad Vikram (Google)|\n+|3:40 PM|Writing Grouped GEMMs in Triton|Vinod Grover (Nvidia)|\n+|4:00 PM|**Reception**||\n+\n \n # Triton\n \n@@ -56,10 +83,6 @@ Version 2.0 is out! New features include:\n \n Community contributions are more than welcome, whether it be to fix bugs or to add new features at [github](https://github.com/openai/triton/). For more detailed instructions, please visit our [contributor's guide](CONTRIBUTING.md).\n \n-If you\u2019re interested in joining our team and working on Triton & GPU kernels, [we\u2019re hiring](https://openai.com/jobs/#acceleration)!\n-\n-\n-\n \n # Compatibility\n "}, {"filename": "bin/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -53,7 +53,6 @@ llvm_update_compile_flags(triton-translate)\n          TritonNvidiaGPUTransforms\n          TritonLLVMIR\n          TritonPTX\n-         TritonHSACO\n          ${dialect_libs}\n          ${conversion_libs}\n          # tests"}, {"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 2, "deletions": 8, "changes": 10, "file_content_changes": "@@ -15,7 +15,6 @@\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n-#include \"triton/Target/HSACO/HSACOTranslation.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n #include \"triton/Target/PTX/PTXTranslation.h\"\n #include \"llvm/IR/LLVMContext.h\"\n@@ -131,16 +130,11 @@ LogicalResult tritonTranslateMain(int argc, char **argv,\n     llvm::errs() << \"Translate to LLVM IR failed\";\n   }\n \n-  if (targetKind == \"llvmir\")\n+  if (targetKind == \"llvmir\") {\n     llvm::outs() << *llvmir << '\\n';\n-  else if (targetKind == \"ptx\")\n+  } else if (targetKind == \"ptx\") {\n     llvm::outs() << ::triton::translateLLVMIRToPTX(*llvmir, SMArch.getValue(),\n                                                    ptxVersion.getValue());\n-  else if (targetKind == \"hsaco\") {\n-    auto [module, hsaco] = ::triton::translateLLVMIRToHSACO(\n-        *llvmir, GCNArch.getValue(), GCNTriple.getValue(),\n-        GCNFeatures.getValue());\n-    llvm::outs() << hsaco;\n   } else {\n     llvm::errs() << \"Error: Unknown target specified: \" << targetKind << \"\\n\";\n     return failure();"}, {"filename": "docs/conf.py", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -101,11 +101,12 @@ def documenter(app, obj, parent):\n     'gallery_dirs': 'getting-started/tutorials',\n     'filename_pattern': '',\n     # XXX: Temporarily disable fused attention tutorial on V100\n-    'ignore_pattern': r'__init__\\.py',\n+    'ignore_pattern': r'(__init__\\.py|09.*\\.py|10.*\\.py)',\n     'within_subsection_order': FileNameSortKey,\n     'reference_url': {\n         'sphinx_gallery': None,\n-    }\n+    },\n+    'abort_on_example_error': True,\n }\n \n # Add any paths that contain templates here, relative to this directory.\n@@ -144,7 +145,7 @@ def documenter(app, obj, parent):\n #\n # This is also used if you do content translation via gettext catalogs.\n # Usually you set \"language\" from the command line for these cases.\n-language = None\n+language = 'en'\n \n # List of patterns, relative to source directory, that match files and\n # directories to ignore when looking for source files."}, {"filename": "docs/meetups/08-22-2023.md", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "file_content_changes": "@@ -11,3 +11,31 @@\n 5. Intel working on the CPU backend for Triton.\n 6. AMD updates\n 7. Open discussion\n+\n+##### Minutes:\n+Recording link [here](https://drive.google.com/file/d/19Nnc0i7zUyn-ni2RSFHbPHHiPkYU96Mz/view)\n+\n+1. H100 updates:\n+   - Preliminary support is merged, disabled by default, can be enabled with env variables\n+   - Supports latest tensor cores, FP8s. Support for Flash Attention on the main branch coming soon.\n+   - Performance is very good on Matmuls, 80-90% of cublas on large Matmuls right now, will eventually reach parity with cublas. Above 600 teraflops on fp16 on xxm card, cublas is 670 on random input data. FP8 is twice that, around 1.2 petaflops.\n+   - Hopper support includes the full FP8 support for compute.\n+2. Triton release plan update\n+   - No specific dates for now, plan is to release before end of 2023.\n+   - Will move to 3.0 release due to minor backward compatibility breaking changes. For eg. Will move compiler options in the indexing operators as hardcoded operators in the kernel, will bump the major version.\n+   - Functionally the main goal will be to have 3rd party plugins for Intel and AMD gpus.\n+   - May synchronise with a PyTorch release so that PyTorch can benefit from the latest features, however continuous integration workflow is the default release cadence expected.\n+   - Will switch the default behavior to optimized mode for the release, needs more discussion with Nvidia.\n+   - Will expose flags for a user to enable kernel selection themselves.\n+   - Open question: Pytorch hasn\u2019t rebased to latest triton, it is close to PyTorch code freeze \u2013 will PyTorch still sync with Triton 2.0? Will we have another release to support triton 2.0?\n+   - Community can start with the latest stable branch and rebase 3rd party plugin on top of that. OAI has no resources to commit to, but community can contribute.\n+3. Linalg updates\n+   - Discussion on Github for Linalg as a middle layer between the language and target hardware. Includes support for block pointers and modulo operators.\n+   - Please join the conversation [here](https://github.com/openai/triton/discussions/1842)\n+   - Branch pushed is behind the tip, will work on getting it caught up on the tip.\n+4. Intel GPU Backend status update.\n+   - Please refer to slides [here](https://github.com/openai/triton/blob/main/docs/meetups/Intel%20XPU%20Backend%20for%20Triton%20-%20Update%20-%200823.pptx)\n+5. Intel working on the CPU backend for Triton.\n+   - Please refer to slides [here](https://github.com/openai/triton/blob/main/docs/meetups/Intel%20XPU%20Backend%20for%20Triton%20-%20Update%20-%200823.pptx)\n+6. AMD updates\n+   - Please refer to slides [here](https://github.com/openai/triton/blob/main/docs/meetups/Triton_AMD_update_0823.pdf)."}, {"filename": "docs/meetups/Intel XPU Backend for Triton - Update - 0823.pptx", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "docs/meetups/Triton_AMD_update_0823.pdf", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -137,7 +137,7 @@ def TT_LoadOp : TT_Op<\"load\",\n                       [SameLoadStoreOperandsAndResultShape,\n                        SameLoadStoreOperandsAndResultEncoding,\n                        AttrSizedOperandSegments,\n-                       MemoryEffects<[MemRead]>,\n+                       DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,\n                        TypesMatchWith<\"infer ptr type from result type\",\n                                       \"result\", \"ptr\", \"$_self\",\n                                       \"mlir::OpTrait::impl::verifyLoadStorePointerAndValueType\">,\n@@ -498,6 +498,8 @@ def TT_MakeRangeOp : TT_Op<\"make_range\", [Pure]> {\n     let results = (outs TT_IntTensor:$result);\n \n     let assemblyFormat = \"attr-dict `:` type($result)\";\n+\n+    let hasFolder = 1;\n }\n \n //"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 14, "deletions": 17, "changes": 31, "file_content_changes": "@@ -101,31 +101,28 @@ class GraphLayoutMarker : public GraphDumper {\n   std::string getColor(const Type &type) const;\n };\n \n-// TODO: Interface\n-LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n-                             Attribute &ret);\n+// Infers the encoding of the result of op given the source encoding.\n+std::optional<Attribute> inferDstEncoding(Operation *op, Attribute encoding);\n \n-bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n+// Infers the encoding of the source of op given the result encoding.\n+std::optional<Attribute> inferSrcEncoding(Operation *op, Attribute encoding);\n \n-bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding);\n+bool isExpensiveLoadOrStore(Operation *op);\n \n-// skipInit is True when we only consider the operands of the initOp but\n-// not the initOp itself.\n-int simulateBackwardRematerialization(\n-    Operation *initOp, SetVector<Operation *> &processed,\n-    SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n-    Attribute targetEncoding);\n+bool canFoldIntoConversion(Operation *op, Attribute targetEncoding);\n \n Operation *cloneWithInferType(mlir::OpBuilder &rewriter, Operation *op,\n                               IRMapping &mapping);\n \n-void rematerializeConversionChain(\n-    const llvm::MapVector<Value, Attribute> &toConvert,\n-    mlir::PatternRewriter &rewriter, SetVector<Operation *> &processed,\n-    IRMapping &mapping);\n+// Get backward slice of tensor values starting from the root node along with\n+// encoding propagation.\n+LogicalResult getConvertBackwardSlice(\n+    Value root, SetVector<Value> &slice, Attribute rootEncoding,\n+    DenseMap<Value, Attribute> &layout,\n+    std::function<bool(Operation *)> stopPropagation = nullptr);\n \n-LogicalResult canMoveOutOfLoop(BlockArgument arg,\n-                               SmallVector<Operation *> &cvts);\n+// Populate pattern to remove dead cycles in ForOp.\n+void populateForOpDeadArgumentElimination(RewritePatternSet &patterns);\n \n // Convert an \\param index to a multi-dim coordinate given \\param shape and\n // \\param order."}, {"filename": "include/triton/Target/AMDGCN/AMDGCNTranslation.h", "status": "removed", "additions": 0, "deletions": 19, "changes": 19, "file_content_changes": "@@ -1,19 +0,0 @@\n-#ifndef TRITON_TARGET_AMDGCNTRANSLATION_H\n-#define TRITON_TARGET_AMDGCNTRANSLATION_H\n-\n-#include <string>\n-#include <tuple>\n-\n-namespace llvm {\n-class Module;\n-} // namespace llvm\n-\n-namespace triton {\n-\n-// Translate LLVM IR to AMDGCN code.\n-std::tuple<std::string, std::string>\n-translateLLVMIRToAMDGCN(llvm::Module &module, std::string cc);\n-\n-} // namespace triton\n-\n-#endif"}, {"filename": "include/triton/Target/HSACO/HSACOTranslation.h", "status": "removed", "additions": 0, "deletions": 21, "changes": 21, "file_content_changes": "@@ -1,21 +0,0 @@\n-#ifndef TRITON_TARGET_HSACOTRANSLATION_H\n-#define TRITON_TARGET_HSACOTRANSLATION_H\n-\n-#include <memory>\n-#include <string>\n-#include <tuple>\n-\n-namespace llvm {\n-class Module;\n-} // namespace llvm\n-\n-namespace triton {\n-\n-// Translate TritonGPU IR to HSACO code.\n-std::tuple<std::string, std::string>\n-translateLLVMIRToHSACO(llvm::Module &module, std::string gfx_arch,\n-                       std::string gfx_triple, std::string gfx_features);\n-\n-} // namespace triton\n-\n-#endif"}, {"filename": "include/triton/Tools/Sys/GetEnv.hpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -46,7 +46,7 @@ inline std::string getenv(const char *name) {\n \n inline bool getBoolEnv(const std::string &env) {\n   std::string msg = \"Environment variable \" + env + \" is not recognized\";\n-  assert(triton::ENV_VARS.find(env.c_str()) != triton::ENV_VARS.end() &&\n+  assert(::triton::ENV_VARS.find(env.c_str()) != ::triton::ENV_VARS.end() &&\n          msg.c_str());\n   const char *s = std::getenv(env.c_str());\n   std::string str(s ? s : \"\");"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 13, "deletions": 3, "changes": 16, "file_content_changes": "@@ -473,9 +473,11 @@ struct DFSState {\n   SmallVector<Operation *, 16> topologicalCounts;\n   DenseSet<Operation *> seen;\n \n-  /// We mark each op as ready if all its operands are seen. If an op is ready,\n-  /// we add it to the queue. Otherwise, we keep adding its operands to the\n-  /// ancestors set.\n+  /// We mark each op as ready if all its operands and parents ops are seen. If\n+  /// an op is ready, we add it to the queue. Otherwise, we keep adding its\n+  /// operands to the ancestors set.\n+  /// We always want an op to be scheduled after all its parents to handle\n+  /// correctly cases with scf operations.\n   void addToReadyQueue(Operation *op, DFSSubgraphState &subGraph,\n                        SmallVector<Operation *, 4> &readyQueue) {\n     bool ready = true;\n@@ -486,6 +488,14 @@ struct DFSState {\n         ready = false;\n       }\n     }\n+    Operation *parent = op->getParentOp();\n+    while (parent) {\n+      if (!seen.count(parent)) {\n+        subGraph.push_back(parent);\n+        ready = false;\n+      }\n+      parent = parent->getParentOp();\n+    }\n     if (ready)\n       readyQueue.push_back(op);\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -346,7 +346,7 @@ struct ConvertLayoutOpConversion\n     SmallVector<unsigned> numCTAsEachRep(rank, 1);\n     SmallVector<unsigned> shapePerCTATile = getShapePerCTATile(layout, shape);\n     SmallVector<int64_t> shapePerCTA = getShapePerCTA(layout, shape);\n-    auto elemTy = type.getElementType();\n+    auto elemTy = getTypeConverter()->convertType(type.getElementType());\n \n     int ctaId = 0;\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -128,7 +128,8 @@ Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n   for (int i = 0; i < aNumPtr; ++i) {\n     aOff[i] = add(mul(offA0, strideA0), mul(offA1, strideA1));\n   }\n-  auto elemTy = A.getType().cast<RankedTensorType>().getElementType();\n+  auto elemTy = typeConverter->convertType(\n+      A.getType().cast<RankedTensorType>().getElementType());\n \n   Type ptrTy = ptr_ty(elemTy, 3);\n   SmallVector<Value> aPtrs(aNumPtr);\n@@ -192,7 +193,8 @@ Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n   for (int i = 0; i < bNumPtr; ++i) {\n     bOff[i] = add(mul(offB0, strideB0), mul(offB1, strideB1));\n   }\n-  auto elemTy = B.getType().cast<RankedTensorType>().getElementType();\n+  auto elemTy = typeConverter->convertType(\n+      B.getType().cast<RankedTensorType>().getElementType());\n \n   Type ptrTy = ptr_ty(elemTy, 3);\n   SmallVector<Value> bPtrs(bNumPtr);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -203,8 +203,8 @@ static Value loadA(Value tensor, const SharedMemoryObject &smemObj,\n   SmallVector<Value> elems;\n   elems.reserve(has.size() * 2);\n   for (auto item : has) { // has is a map, the key should be ordered.\n-    elems.push_back(item.second.first);\n-    elems.push_back(item.second.second);\n+    elems.push_back(bitcast(item.second.first, i32_ty));\n+    elems.push_back(bitcast(item.second.second, i32_ty));\n   }\n \n   Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);\n@@ -327,8 +327,8 @@ static Value loadB(Value tensor, const SharedMemoryObject &smemObj,\n \n   SmallVector<Value> elems;\n   for (auto &item : hbs) { // has is a map, the key should be ordered.\n-    elems.push_back(item.second.first);\n-    elems.push_back(item.second.second);\n+    elems.push_back(bitcast(item.second.first, i32_ty));\n+    elems.push_back(bitcast(item.second.second, i32_ty));\n   }\n \n   Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 0, "deletions": 8, "changes": 8, "file_content_changes": "@@ -511,14 +511,6 @@ std::function<void(int, int)> getLoadMatrixFn(\n   const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n   auto order = sharedLayout.getOrder();\n \n-  if (tensor.getType()\n-          .cast<RankedTensorType>()\n-          .getElementType()\n-          .isa<mlir::Float8E4M3B11FNUZType, mlir::Float8E4M3FNType>()) {\n-    bool noTrans = (isA ^ (order[0] == 0));\n-    assert(noTrans && \"float8e4b15 must have row-col layout\");\n-  }\n-\n   if (kWidth != (4 / elemBytes))\n     assert(vecPhase == 1 || vecPhase == 4 * kWidth);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 41, "deletions": 16, "changes": 57, "file_content_changes": "@@ -101,21 +101,40 @@ const std::string Fp8E4M3B15_to_Fp16 =\n     \"shl.b32 $1, b1, 7;                     \\n\"\n     \"}                                      \\n\";\n \n-const std::string Fp16_to_Fp8E4M3B15 =\n-    \"{                                      \\n\"\n-    \".reg .b32 a<2>, b<2>;                  \\n\"\n-    \".reg .b32 max_val;                     \\n\"\n-    \"mov.b32 max_val, 0x3F803F80;           \\n\"\n-    \"and.b32 a0, $1, 0x7fff7fff;            \\n\"\n-    \"and.b32 a1, $2, 0x7fff7fff;            \\n\"\n-    \"min.f16x2 a0, a0, max_val;             \\n\"\n-    \"min.f16x2 a1, a1, max_val;             \\n\"\n-    \"mad.lo.u32 a0, a0, 2, 0x00800080;      \\n\"\n-    \"mad.lo.u32 a1, a1, 2, 0x00800080;      \\n\"\n-    \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n-    \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n-    \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n-    \"}\";\n+const std::string Fp16_to_Fp8E4M3B15(bool has_minx2) {\n+  std::string ret;\n+  ret += \"{                                      \\n\"\n+         \".reg .pred p<4>;                       \\n\"\n+         \".reg .b32 a<2>, b<2>;                  \\n\"\n+         \".reg .b16 c<4>;                        \\n\"\n+         \".reg .b16 max_val_f16;                 \\n\"\n+         \".reg .b32 max_val_f16x2;               \\n\"\n+         \"mov.b16 max_val_f16,   0x3F80;         \\n\"\n+         \"mov.b32 max_val_f16x2, 0x3F803F80;     \\n\"\n+         \"and.b32 a0, $1, 0x7fff7fff;            \\n\"\n+         \"and.b32 a1, $2, 0x7fff7fff;            \\n\";\n+  if (has_minx2)\n+    ret += \"min.f16x2 a0, a0, max_val_f16x2;      \\n\"\n+           \"min.f16x2 a1, a1, max_val_f16x2;      \\n\";\n+  else\n+    ret += \"setp.lt.f16x2  p0|p1, a0, max_val_f16x2;   \\n\"\n+           \"setp.lt.f16x2  p2|p3, a1, max_val_f16x2;   \\n\"\n+           \"mov.b32 {c0, c1}, a0;                \\n\"\n+           \"mov.b32 {c2, c3}, a1;                \\n\"\n+           \"selp.b16  c0, c0, max_val_f16, p0;   \\n\"\n+           \"selp.b16  c1, c1, max_val_f16, p1;   \\n\"\n+           \"selp.b16  c2, c2, max_val_f16, p2;   \\n\"\n+           \"selp.b16  c3, c3, max_val_f16, p3;   \\n\"\n+           \"mov.b32 a0, {c0, c1};                \\n\"\n+           \"mov.b32 a1, {c2, c3};                \\n\";\n+  ret += \"mad.lo.u32 a0, a0, 2, 0x00800080;      \\n\"\n+         \"mad.lo.u32 a1, a1, 2, 0x00800080;      \\n\"\n+         \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n+         \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n+         \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n+         \"}\";\n+  return ret;\n+}\n \n /* ----- FP8E4M3B15X4 ------ */\n // NOTE: NOT USED RIGHT NOW\n@@ -190,6 +209,12 @@ static SmallVector<Value> reorderValues(const SmallVector<Value> &values,\n   assert(inEncoding == ouEncoding);\n   if (!inEncoding)\n     return values;\n+  // If the parent of the dot operand is in block encoding, we don't need to\n+  // reorder elements\n+  auto parentEncoding =\n+      dyn_cast<triton::gpu::MmaEncodingAttr>(ouEncoding.getParent());\n+  if (!parentEncoding)\n+    return values;\n   size_t inBitWidth = inTensorTy.getElementType().getIntOrFloatBitWidth();\n   size_t ouBitWidth = ouTensorTy.getElementType().getIntOrFloatBitWidth();\n   auto ouEltTy = ouTensorTy.getElementType();\n@@ -551,7 +576,7 @@ struct FpToFpOpConversion\n         {{F8E4M3TyID, F16TyID}, Fp8E4M3Nv_to_Fp16},\n         {{F8E5M2TyID, F16TyID}, Fp8E5M2_to_Fp16},\n         // F16 -> F8\n-        {{F16TyID, F8E4M3B15TyID}, Fp16_to_Fp8E4M3B15},\n+        {{F16TyID, F8E4M3B15TyID}, Fp16_to_Fp8E4M3B15(computeCapability >= 80)},\n         {{F16TyID, F8E4M3FNTyID}, Fp16_to_Fp8E4M3B15x4},\n         {{F16TyID, F8E4M3TyID}, Fp16_to_Fp8E4M3Nv},\n         {{F16TyID, F8E5M2TyID}, Fp16_to_Fp8E5M2},"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 26, "deletions": 6, "changes": 32, "file_content_changes": "@@ -914,6 +914,18 @@ struct StoreAsyncOpConversion\n   const TensorPtrMapT *tensorPtrMap;\n };\n \n+namespace {\n+void createBarrier(ConversionPatternRewriter &rewriter, Location loc,\n+                   int numCTAs) {\n+  if (numCTAs == 1) {\n+    barrier();\n+  } else {\n+    rewriter.create<triton::nvidia_gpu::ClusterArriveOp>(loc, false);\n+    rewriter.create<triton::nvidia_gpu::ClusterWaitOp>(loc);\n+  }\n+}\n+} // namespace\n+\n struct AtomicCASOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::AtomicCASOp>,\n       public LoadStoreConversionBase {\n@@ -934,6 +946,10 @@ struct AtomicCASOpConversion\n     auto loc = op.getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n \n+    auto moduleOp = op->getParentOfType<ModuleOp>();\n+    assert(moduleOp && \"Parent ModuleOp not found for AtomicCASOp\");\n+    int numCTAs = triton::gpu::TritonGPUDialect::getNumCTAs(moduleOp);\n+\n     Value llPtr = adaptor.getPtr();\n     Value llCmp = adaptor.getCmp();\n     Value llVal = adaptor.getVal();\n@@ -971,7 +987,7 @@ struct AtomicCASOpConversion\n     atom.global().o(semStr).o(\"cas\").o(\"b32\");\n     atom(dstOpr, ptrOpr, cmpOpr, valOpr).predicate(mask);\n     auto old = ptxBuilderAtomicCAS.launch(rewriter, loc, valueElemTy);\n-    barrier();\n+    createBarrier(rewriter, loc, numCTAs);\n \n     PTXBuilder ptxBuilderStore;\n     auto *dstOprStore = ptxBuilderStore.newAddrOperand(atomPtr, \"r\");\n@@ -981,9 +997,9 @@ struct AtomicCASOpConversion\n     st(dstOprStore, valOprStore).predicate(mask);\n     auto ASMReturnTy = void_ty(ctx);\n     ptxBuilderStore.launch(rewriter, loc, ASMReturnTy);\n-    barrier();\n+    createBarrier(rewriter, loc, numCTAs);\n     Value ret = load(atomPtr);\n-    barrier();\n+    createBarrier(rewriter, loc, numCTAs);\n     rewriter.replaceOp(op, {ret});\n     return success();\n   }\n@@ -1008,7 +1024,11 @@ struct AtomicRMWOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     auto loc = op.getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n-    //\n+\n+    auto moduleOp = op->getParentOfType<ModuleOp>();\n+    assert(moduleOp && \"Parent ModuleOp not found for AtomicRMWOp\");\n+    int numCTAs = triton::gpu::TritonGPUDialect::getNumCTAs(moduleOp);\n+\n     auto atomicRmwAttr = op.getAtomicRmwOp();\n \n     Value val = op.getVal();\n@@ -1139,9 +1159,9 @@ struct AtomicRMWOpConversion\n         auto *valOpr = ptxBuilderStore.newOperand(old, tyId);\n         storeShared(ptrOpr, valOpr).predicate(rmwMask);\n         ptxBuilderStore.launch(rewriter, loc, void_ty(ctx));\n-        barrier();\n+        createBarrier(rewriter, loc, numCTAs);\n         Value ret = load(atomPtr);\n-        barrier();\n+        createBarrier(rewriter, loc, numCTAs);\n         rewriter.replaceOp(op, {ret});\n       }\n     }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -11,6 +11,7 @@ using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::linearize;\n using ::mlir::LLVM::shflSync;\n using ::mlir::LLVM::storeShared;\n+using ::mlir::triton::gpu::getCTASplitNum;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n \n@@ -28,6 +29,12 @@ struct ReduceOpConversion\n   LogicalResult\n   matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n+    // When cross-CTA reduction is implemented in the future, this assertion can\n+    // be removed\n+    assert(isReduceWithinCTA(op) &&\n+           \"Layout optimization passes such as PlanCTAPass and \"\n+           \"RemoveLayoutConversionPass should avoid cross-CTA reduction\");\n+\n     if (ReduceOpHelper(op).isFastReduction())\n       return matchAndRewriteFast(op, adaptor, rewriter);\n     return matchAndRewriteBasic(op, adaptor, rewriter);\n@@ -36,6 +43,15 @@ struct ReduceOpConversion\n private:\n   int computeCapability;\n \n+  bool isReduceWithinCTA(triton::ReduceOp op) const {\n+    auto axis = op.getAxis();\n+    ReduceOpHelper helper(op);\n+    auto srcLayout = helper.getSrcLayout();\n+    auto CTASplitNum = getCTASplitNum(srcLayout);\n+    assert(axis < CTASplitNum.size());\n+    return CTASplitNum[axis] == 1;\n+  }\n+\n   void accumulate(ConversionPatternRewriter &rewriter, Region &combineOp,\n                   SmallVector<Value> &acc, ValueRange cur, bool isFirst) const {\n     if (isFirst) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 15, "deletions": 6, "changes": 21, "file_content_changes": "@@ -541,6 +541,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto tensorTy = valueTy.dyn_cast<RankedTensorType>();\n     Value mask = int_val(1, 1);\n     auto tid = tid_val();\n+    auto clusterCTAId = getClusterCTAId(rewriter, loc);\n     if (tensorTy) {\n       auto layout = tensorTy.getEncoding();\n       auto shape = tensorTy.getShape();\n@@ -576,7 +577,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n         auto CTASplitNum = triton::gpu::getCTASplitNum(layout);\n         auto CTAOrder = triton::gpu::getCTAOrder(layout);\n \n-        auto clusterCTAId = getClusterCTAId(rewriter, loc);\n         auto multiDimClusterCTAId =\n             delinearize(rewriter, loc, clusterCTAId, CTAsPerCGA, CTAOrder);\n \n@@ -586,14 +586,23 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n             continue;\n           // This wrapping rule must be consistent with emitCTAOffsetForLayout\n           unsigned splitNum = std::min<unsigned>(shape[dim], CTASplitNum[dim]);\n-          multiDimClusterCTAId[dim] =\n-              urem(multiDimClusterCTAId[dim], i32_val(splitNum));\n-          mask = and_(mask, icmp_eq(multiDimClusterCTAId[dim], _0));\n+          Value repId = udiv(multiDimClusterCTAId[dim], i32_val(splitNum));\n+          // Consider the example where CTAsPerCGA = [4] and CTASplitNum = [2]:\n+          //     CTA0 and CTA2 holds data of block0,\n+          //     CTA1 and CTA3 holds data of block1.\n+          // Only CTA0 and CTA1 are expected to write while CTA2 and CTA3 should\n+          // be masked. We add the following mask:\n+          //     multiDimClusterCTAId[dim] / splitNum == 0\n+          // Actually in all existing cases of multicast, splitNum is always 1.\n+          // The mask is equivalent to:\n+          //     multiDimClusterCTAId[dim] == 0\n+          mask = and_(mask, icmp_eq(repId, _0));\n         }\n       }\n     } else {\n-      // If the tensor is not ranked, then it is a scalar and only thread 0 can\n-      // write\n+      // If the tensor is not ranked, then it is a scalar and only thread 0 of\n+      // CTA0 can write\n+      mask = and_(mask, icmp_eq(clusterCTAId, i32_val(0)));\n       mask = and_(mask, icmp_eq(tid, i32_val(0)));\n     }\n     return mask;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "modified", "additions": 3, "deletions": 8, "changes": 11, "file_content_changes": "@@ -130,14 +130,9 @@ Type TritonGPUToLLVMTypeConverter::getElementTypeForStruct(\n   auto mmaParent = dotOpLayout.getParent().dyn_cast<MmaEncodingAttr>();\n   if (!mmaParent)\n     return elemTy;\n-  if (mmaParent.isAmpere()) {\n-    int bitwidth = elemTy.getIntOrFloatBitWidth();\n-    assert(bitwidth <= 32);\n-    return IntegerType::get(ctx, 32);\n-  } else {\n-    assert(mmaParent.isVolta());\n-    return vec_ty(elemTy, 2);\n-  }\n+  int bitwidth = elemTy.getIntOrFloatBitWidth();\n+  assert(bitwidth <= 32);\n+  return IntegerType::get(ctx, 32);\n }\n \n Type TritonGPUToLLVMTypeConverter::convertTritonTensorType("}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "file_content_changes": "@@ -80,6 +80,16 @@ void LoadOp::print(OpAsmPrinter &printer) {\n   printer.printStrippedAttrOrType(getResult().getType());\n }\n \n+void LoadOp::getEffects(\n+    SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>\n+        &effects) {\n+  effects.emplace_back(MemoryEffects::Read::get(), getPtr(),\n+                       SideEffects::DefaultResource::get());\n+  if (getIsVolatile())\n+    effects.emplace_back(MemoryEffects::Write::get(),\n+                         SideEffects::DefaultResource::get());\n+}\n+\n ParseResult StoreOp::parse(OpAsmParser &parser, OperationState &result) {\n   // Parse operands\n   SmallVector<OpAsmParser::UnresolvedOperand, 4> allOperands;\n@@ -418,6 +428,16 @@ LogicalResult mlir::triton::DotOp::verify() {\n                                                      bEncoding);\n }\n \n+//-- MakeRangeOp --\n+OpFoldResult MakeRangeOp::fold(FoldAdaptor adaptor) {\n+  // make_range(start, start + 1) -> constant(start)\n+  if (adaptor.getStart() + 1 == adaptor.getEnd()) {\n+    auto shapedType = getType().cast<ShapedType>();\n+    return SplatElementsAttr::get(shapedType, adaptor.getStartAttr());\n+  }\n+  return {};\n+}\n+\n //-- ReduceOp --\n static mlir::LogicalResult\n inferReduceReturnShape(const RankedTensorType &argTy, const Type &retEltTy,"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 34, "deletions": 0, "changes": 34, "file_content_changes": "@@ -102,6 +102,8 @@ warpsPerTileV3(tt::DotOp dotOp, const ArrayRef<int64_t> shape, int numWarps,\n class BlockedToMMA : public mlir::RewritePattern {\n   int computeCapability;\n   mutable int mmaV1Counter{}; // used to generate ID for MMAv1 encoding\n+  mutable llvm::SmallVector<llvm::SetVector<Operation *>> dotOpSetVector;\n+  mutable llvm::SmallVector<unsigned> mmaV3InstrNs;\n \n   static bool bwdFilter(Operation *op) {\n     return op->getNumOperands() == 1 &&\n@@ -144,6 +146,36 @@ class BlockedToMMA : public mlir::RewritePattern {\n     }\n   }\n \n+  unsigned getMmaV3InstrN(tt::DotOp dotOp, unsigned currN) const {\n+    auto type = dotOp.getResult().getType().cast<RankedTensorType>();\n+    if (type.getEncoding().isa<MmaEncodingAttr>())\n+      return currN;\n+    for (size_t i = 0; i < dotOpSetVector.size(); ++i) {\n+      if (dotOpSetVector[i].count(dotOp.getOperation()) > 0)\n+        return mmaV3InstrNs[i];\n+    }\n+\n+    SetVector<Operation *> slices;\n+    mlir::getForwardSlice(dotOp.getResult(), &slices);\n+    mlir::getBackwardSlice(dotOp.getOperation(), &slices);\n+    unsigned N = currN;\n+    llvm::SetVector<Operation *> dotOpSet;\n+    for (Operation *iter : slices) {\n+      if (auto nextDotOp = dyn_cast<tt::DotOp>(iter)) {\n+        auto type = nextDotOp.getResult().getType().cast<RankedTensorType>();\n+        auto AType = nextDotOp.getOperand(0).getType().cast<RankedTensorType>();\n+        auto shapePerCTA = ttg::getShapePerCTA(type);\n+        auto instrShape = mmaVersionToInstrShape(3, shapePerCTA, AType);\n+        dotOpSet.insert(iter);\n+        if (instrShape[1] < N)\n+          N = instrShape[1];\n+      }\n+    }\n+    mmaV3InstrNs.push_back(N);\n+    dotOpSetVector.push_back(dotOpSet);\n+    return N;\n+  }\n+\n   static Value getMMAv3Operand(Value v, mlir::PatternRewriter &rewriter,\n                                int opIdx) {\n     auto cvtOp = dyn_cast_or_null<ttg::ConvertLayoutOp>(v.getDefiningOp());\n@@ -201,6 +233,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n \n     auto instrShape =\n         mmaVersionToInstrShape(versionMajor, retShapePerCTA, AType);\n+    if (versionMajor == 3)\n+      instrShape[1] = getMmaV3InstrN(dotOp, instrShape[1]);\n \n     // operands\n     Value a = dotOp.getA();"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -121,6 +121,9 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n         cvtArgOp->getDialect()->getTypeID() !=\n             mlir::TypeID::get<arith::ArithDialect>())\n       return mlir::failure();\n+    // not handled in elementwise lowering.\n+    if (isa<arith::TruncIOp, arith::TruncFOp>(cvtArgOp))\n+      return mlir::failure();\n     // only considers conversions to dot operand\n     if (!cvtTy.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n       return mlir::failure();\n@@ -244,7 +247,8 @@ class TritonGPUOptimizeDotOperandsPass\n \n     mlir::RewritePatternSet patterns(context);\n     patterns.add<ConvertTransConvert>(context);\n-    patterns.add<MoveOpAfterLayoutConversion>(context);\n+    if (triton::gpu::TritonGPUDialect::getComputeCapability(m) >= 80)\n+      patterns.add<MoveOpAfterLayoutConversion>(context);\n     patterns.add<FuseTransHopper>(context);\n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n       signalPassFailure();"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 877, "deletions": 478, "changes": 1355, "file_content_changes": "@@ -12,11 +12,11 @@\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"mlir/Transforms/Passes.h\"\n #include \"mlir/Transforms/RegionUtils.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n-\n #include <memory>\n \n using namespace mlir;\n@@ -82,542 +82,914 @@ class DecomposeDotOperand : public mlir::RewritePattern {\n   }\n };\n \n-// It's beneficial to move the conversion\n-// to after the reduce if necessary since it will be\n-// done on a rank-reduced tensor hence cheaper\n-class SimplifyReduceCvt : public mlir::RewritePattern {\n+//\n+class ConvertDotConvert : public mlir::RewritePattern {\n public:\n-  explicit SimplifyReduceCvt(mlir::MLIRContext *context)\n+  ConvertDotConvert(mlir::MLIRContext *context)\n       : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             2, context) {}\n+                             1, context) {}\n \n-  mlir::LogicalResult\n+  LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n-    if (!llvm::isa<triton::gpu::ConvertLayoutOp>(op))\n+    auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto dotOp = dstOp.getSrc().getDefiningOp<triton::DotOp>();\n+    if (!dotOp)\n       return mlir::failure();\n-    auto convert = llvm::cast<triton::gpu::ConvertLayoutOp>(op);\n-    triton::ReduceOp reduce;\n-    for (auto &use : convert.getResult().getUses()) {\n-      auto owner = llvm::dyn_cast<triton::ReduceOp>(use.getOwner());\n-      if (!owner) {\n-        continue;\n-      }\n+    if (std::distance(dstOp->user_begin(), dstOp->user_end()) != 1 ||\n+        std::distance(dotOp->user_begin(), dotOp->user_end()) != 1)\n+      return mlir::failure();\n+    auto cvtOp =\n+        dotOp.getOperand(2).getDefiningOp<triton::gpu::ConvertLayoutOp>();\n+    if (!cvtOp)\n+      return mlir::failure();\n+    if (!cvtOp.getSrc().getDefiningOp<triton::LoadOp>())\n+      return failure();\n+    auto dstTy = dstOp.getResult().getType().cast<RankedTensorType>();\n+    auto srcTy = cvtOp.getOperand().getType().cast<RankedTensorType>();\n+    if (dstTy != srcTy)\n+      return mlir::failure();\n+\n+    auto _0f = rewriter.create<arith::ConstantOp>(\n+        op->getLoc(), dstTy.getElementType(),\n+        rewriter.getZeroAttr(dstTy.getElementType()));\n+    auto _0 = rewriter.create<triton::SplatOp>(\n+        op->getLoc(), dotOp.getResult().getType(), _0f);\n+    auto newDot = rewriter.create<triton::DotOp>(\n+        op->getLoc(), dotOp.getResult().getType(), dotOp.getOperand(0),\n+        dotOp.getOperand(1), _0, dotOp.getAllowTF32());\n+    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        op->getLoc(), dstTy, newDot.getResult());\n+    rewriter.replaceOpWithNewOp<arith::AddFOp>(op, newCvt, cvtOp.getOperand());\n+    return mlir::success();\n+  }\n+};\n+\n+// Class to propagate layout globally within a function.\n+// The current algorithm works by analysis the IR and doing a one shot rewrite\n+// based on the analysis. The algorithm is as follows:\n+// 1. Find all the anchor ops. These are ops that have a layout we want to\n+// preserve.\n+//\n+// 2. Propagate the layout to every op reachable which is a transitive child of\n+// an anchor op until we reach a fix point.\n+// An op can have multiple transitive anchor parents therefore at this stage\n+// it may have multiple layout associated to it.\n+//\n+// 3. Resolve conflicts by deciding which of the multiple layouts the op should\n+// keep. If one of the parents has a different layout than what is picked a\n+// convert operation will be inserted. After this stage each value should have\n+// only one layout associated.\n+//\n+// 4. Rewrite the IR by walking the function following dominance order. Since we\n+// assume the IR is structured we just need to process the regions in the\n+// correct order. For each op rewrite it using the layout decided by the\n+// analysis phase.\n+class LayoutPropagation {\n+public:\n+  // Structure to keep track of the layout associated to a value.\n+  struct LayoutInfo {\n+    LayoutInfo(Attribute encoding) { encodings.insert(encoding); }\n+    LayoutInfo() {}\n+    llvm::SmallSetVector<Attribute, 8> encodings;\n+  };\n+  LayoutPropagation(triton::FuncOp F) : funcOp(F) {}\n+  // Find the anchor ops and set their layout in the data structure.\n+  void initAnchorLayout();\n+  // Recursively Propagate the layout to all the users of the anchor ops until\n+  // we reach a fix point.\n+  void propagateLayout();\n+  // Add layouts given in `Info` to the uses of `value`.\n+  SmallVector<Value> propagateToUsers(Value value, LayoutInfo &info);\n+  // Set the encoding to all the values and fill out the values with new layout\n+  // in `changed`.\n+  void setEncoding(ValueRange values, LayoutInfo &info,\n+                   SmallVector<Value> &changed, Operation *op);\n+  // Resolve cases where a value has multiple layouts associated to it.\n+  void resolveConflicts();\n+  // Rewrite the IR for the full module.\n+  void rewrite();\n+  // Rewrite the IR for a region.\n+  void rewriteRegion(Region &R);\n+  // Rewrite an op based on the layout picked by the analysis.\n+  Operation *rewriteOp(Operation *op);\n+  // Rewrite a for op based on the layout picked by the analysis.\n+  Operation *rewriteForOp(scf::ForOp forOp);\n+  Operation *rewriteWhileOp(scf::WhileOp whileOp);\n+  Operation *rewriteIfOp(scf::IfOp ifOp);\n+  void rewriteYieldOp(scf::YieldOp yieldOp);\n+  void rewriteConditionOp(scf::ConditionOp conditionOp);\n+  void rewriteReduceToScalar(Operation *reduceOp);\n+  Operation *cloneElementwise(OpBuilder &rewriter, Operation *op,\n+                              Attribute encoding);\n+  // Map the original value to the rewritten one.\n+  void map(Value old, Value newV);\n+  // Return the mapped value in the given encoding. This will insert a convert\n+  // if the encoding is different than the encoding decided at resolve time.\n+  Value getValueAs(Value value, Attribute encoding);\n+  // Dump the current stage of layout information.\n+  void dump();\n \n-      // TODO: This only moves conversions from the first argument which is\n-      // fine for argmin/argmax but may not be optimal generally\n-      if (convert.getResult() != owner.getOperands()[0]) {\n+private:\n+  // map from value to layout information.\n+  llvm::MapVector<Value, LayoutInfo> layouts;\n+  // map of the values rewrite based on their encoding.\n+  DenseMap<std::pair<Value, Attribute>, Value> rewriteMapping;\n+  std::vector<Operation *> opToDelete;\n+  triton::FuncOp funcOp;\n+};\n+\n+} // namespace\n+\n+// Look ahead to at the transitive uses and see if there is a convert to mma\n+// operations.\n+static bool hasConvertToMMATransisitiveUse(Operation *op, Attribute encoding) {\n+  SmallVector<Value> queue = {op->getResult(0)};\n+  SetVector<Operation *> forwardSlice;\n+  llvm::SmallDenseSet<Value> seen;\n+  while (!queue.empty()) {\n+    Value currentValue = queue.back();\n+    queue.pop_back();\n+    getForwardSlice(currentValue, &forwardSlice);\n+    for (Operation *op : forwardSlice) {\n+      if (auto convertOp = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n+        if (convertOp.getResult()\n+                .getType()\n+                .cast<RankedTensorType>()\n+                .getEncoding() == encoding)\n+          return true;\n+      }\n+      auto yield = dyn_cast<scf::YieldOp>(op);\n+      if (!yield)\n         continue;\n+      auto forOp = dyn_cast<scf::ForOp>(yield.getOperation()->getParentOp());\n+      if (!forOp)\n+        continue;\n+      for (OpOperand &operand : yield->getOpOperands()) {\n+        Operation *def = operand.get().getDefiningOp();\n+        if (def && forwardSlice.count(def) &&\n+            (seen.insert(operand.get()).second == true))\n+          queue.push_back(forOp.getRegionIterArg(operand.getOperandNumber()));\n       }\n-      reduce = owner;\n-      break;\n     }\n-    if (!reduce)\n-      return mlir::failure();\n+  }\n+  return false;\n+}\n \n-    SmallVector<Value> newOperands = reduce.getOperands();\n+// Return true if the op is an op with a layout we don't want to change. We will\n+// propagate the layout starting from anchor ops.\n+static bool isLayoutAnchor(Operation *op) {\n+  if (isa<triton::LoadOp, triton::StoreOp>(op))\n+    return isExpensiveLoadOrStore(op);\n+  if (isa<triton::DotOp, triton::AtomicRMWOp, triton::AtomicCASOp>(op))\n+    return true;\n+  return false;\n+}\n \n-    newOperands[0] = convert.getOperand();\n-    auto newEncoding =\n-        newOperands[0].getType().cast<RankedTensorType>().getEncoding();\n+void LayoutPropagation::initAnchorLayout() {\n+  funcOp.walk([&](Operation *op) {\n+    if (isLayoutAnchor(op)) {\n+      for (auto result : op->getResults()) {\n+        if (auto tensorType = result.getType().dyn_cast<RankedTensorType>()) {\n+          // Workaround, don't popagate MMA layout unless there is a convert\n+          // back to mma further down to avoid generating reduction with MMA\n+          // layout that may have lower performance.\n+          // This can be improved with more aggressive backward propagation.\n+          if (tensorType.getEncoding().isa<triton::gpu::MmaEncodingAttr>() &&\n+              !hasConvertToMMATransisitiveUse(op, tensorType.getEncoding()))\n+            continue;\n+          layouts.insert({result, tensorType.getEncoding()});\n+        }\n+      }\n+    }\n+  });\n+}\n \n-    // this may generate unsupported conversions in the LLVM codegen\n-    if (newEncoding.isa<triton::gpu::MmaEncodingAttr>()) {\n-      return failure();\n+void LayoutPropagation::setEncoding(ValueRange values, LayoutInfo &info,\n+                                    SmallVector<Value> &changed,\n+                                    Operation *op) {\n+  for (Value value : values) {\n+    if (!value.getType().isa<RankedTensorType>())\n+      continue;\n+    bool hasChanged = false;\n+    for (auto encoding : info.encodings) {\n+      auto dstEncoding = inferDstEncoding(op, encoding);\n+      if (dstEncoding)\n+        hasChanged |= layouts[value].encodings.insert(*dstEncoding);\n     }\n+    if (hasChanged)\n+      changed.push_back(value);\n+  }\n+}\n \n-    // ReduceOp does not support SharedLayout as its src layout, therefore\n-    // ConvertLayoutOp and ReduceOp should not be swapped when the conversion is\n-    // from SharedLayout to DistributedLayout\n-    if (newEncoding.isa<triton::gpu::SharedEncodingAttr>()) {\n-      return failure();\n+SmallVector<Value> LayoutPropagation::propagateToUsers(Value value,\n+                                                       LayoutInfo &info) {\n+  SmallVector<Value> changed;\n+  for (OpOperand &use : value.getUses()) {\n+    Operation *user = use.getOwner();\n+    if (auto forOp = dyn_cast<scf::ForOp>(user)) {\n+      Value arg = forOp.getRegionIterArgForOpOperand(use);\n+      Value result = forOp.getResultForOpOperand(use);\n+      setEncoding({arg, result}, info, changed, user);\n+      continue;\n+    }\n+    if (auto whileOp = dyn_cast<scf::WhileOp>(user)) {\n+      Value arg = whileOp.getBeforeArguments()[use.getOperandNumber()];\n+      setEncoding({arg}, info, changed, user);\n+      continue;\n+    }\n+    if (auto yieldOp = dyn_cast<scf::YieldOp>(user)) {\n+      auto parent = yieldOp->getParentOp();\n+      SmallVector<Value> valuesToPropagate;\n+      if (isa<scf::ForOp, scf::IfOp>(parent))\n+        valuesToPropagate.push_back(parent->getResult(use.getOperandNumber()));\n+      if (auto forOp = dyn_cast<scf::ForOp>(parent))\n+        valuesToPropagate.push_back(\n+            forOp.getRegionIterArg(use.getOperandNumber()));\n+      if (auto whileOp = dyn_cast<scf::WhileOp>(parent)) {\n+        valuesToPropagate.push_back(\n+            whileOp.getBeforeArguments()[use.getOperandNumber()]);\n+        valuesToPropagate.push_back(\n+            whileOp->getOperand(use.getOperandNumber()));\n+      }\n+      if (isa<scf::ForOp, scf::IfOp, scf::WhileOp>(parent))\n+        setEncoding(valuesToPropagate, info, changed, user);\n+      continue;\n+    }\n+    if (auto conditionOp = dyn_cast<scf::ConditionOp>(user)) {\n+      auto whileOp = cast<scf::WhileOp>(conditionOp->getParentOp());\n+      // Skip arg 0 as it is the condition.\n+      unsigned argIndex = use.getOperandNumber() - 1;\n+      Value afterArg = whileOp.getAfterArguments()[argIndex];\n+      Value result = whileOp->getResult(argIndex);\n+      setEncoding({afterArg, result}, info, changed, user);\n+      continue;\n+    }\n+    // Workaround: don't propagate through truncI\n+    if (isa<arith::TruncIOp>(user))\n+      continue;\n+    if (user->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() ||\n+        user->hasTrait<mlir::OpTrait::Elementwise>() ||\n+        isa<triton::ReduceOp, triton::ExpandDimsOp,\n+            triton::gpu::ConvertLayoutOp>(user)) {\n+      setEncoding(user->getResults(), info, changed, user);\n+      continue;\n     }\n+  }\n+  return changed;\n+}\n \n-    for (unsigned i = 1; i < newOperands.size(); ++i) {\n-      auto oldTy = newOperands[i].getType().cast<RankedTensorType>();\n-      RankedTensorType newTy =\n-          RankedTensorType::Builder(oldTy).setEncoding(newEncoding);\n+void LayoutPropagation::propagateLayout() {\n+  SmallVector<Value> queue;\n+  for (auto it : layouts) {\n+    queue.push_back(it.first);\n+  }\n+  while (!queue.empty()) {\n+    Value currentValue = queue.back();\n+    LayoutInfo info = layouts[currentValue];\n+    queue.pop_back();\n+    SmallVector<Value> changed = propagateToUsers(currentValue, info);\n+    queue.insert(queue.end(), changed.begin(), changed.end());\n+  }\n+}\n \n-      newOperands[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          op->getLoc(), newTy, newOperands[i]);\n+void LayoutPropagation::resolveConflicts() {\n+  for (auto &it : layouts) {\n+    LayoutInfo &info = it.second;\n+    if (info.encodings.size() <= 1)\n+      continue;\n+    // Hacky resolve, prefer block encoding.\n+    // TODO: add a proper heuristic.\n+    Attribute encoding = *info.encodings.begin();\n+    for (Attribute e : info.encodings) {\n+      if (e.isa<triton::gpu::BlockedEncodingAttr>()) {\n+        encoding = e;\n+        break;\n+      }\n     }\n+    info.encodings.clear();\n+    info.encodings.insert(encoding);\n+  }\n+}\n \n-    rewriter.setInsertionPoint(reduce);\n-    auto newReduce = rewriter.create<triton::ReduceOp>(\n-        op->getLoc(), newOperands, reduce.getAxis());\n-    auto &newCombineOp = newReduce.getCombineOp();\n-    rewriter.cloneRegionBefore(reduce.getCombineOp(), newCombineOp,\n-                               newCombineOp.end());\n-\n-    SmallVector<Value> newRet = newReduce.getResult();\n-    auto oldTypes = reduce.getResult().getType();\n-    for (unsigned i = 0; i < reduce.getNumOperands(); ++i) {\n-      // it's still beneficial to move the conversion\n-      // to after the reduce if necessary since it will be\n-      // done on a rank-reduced tensor hence cheaper\n-      if (newRet[i].getType() != oldTypes[i])\n-        newRet[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-            op->getLoc(), oldTypes[i], newRet[i]);\n+void LayoutPropagation::dump() {\n+  for (auto it : layouts) {\n+    llvm::errs() << \"Value: \";\n+    OpPrintingFlags flags;\n+    flags.skipRegions();\n+    it.first.print(llvm::errs(), flags);\n+    llvm::errs() << \" \\n encoding:\\n\";\n+    for (auto encoding : it.second.encodings) {\n+      encoding.print(llvm::errs());\n+      llvm::errs() << \"\\n\";\n     }\n-    rewriter.replaceAllUsesWith(reduce.getResult(), newRet);\n-\n-    return success();\n+    llvm::errs() << \"--\\n\";\n   }\n-};\n+}\n \n-// Layout conversions can't deduce their return type automatically.\n-// IIUC they are therefore not handled by DRR right now\n-class SimplifyConversion : public mlir::RewritePattern {\n-public:\n-  explicit SimplifyConversion(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             4, context) {}\n+void LayoutPropagation::rewrite() { rewriteRegion(funcOp->getRegion(0)); }\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    if (!llvm::isa<triton::gpu::ConvertLayoutOp>(op))\n-      return mlir::failure();\n-    auto convert = llvm::cast<triton::gpu::ConvertLayoutOp>(op);\n-    return ConvertLayoutOp::canonicalize(convert, rewriter);\n+static bool reduceToScalar(Operation *op) {\n+  // For reductions returning a scalar we can change the src encoding without\n+  // affecting the output.\n+  return isa<triton::ReduceOp>(op) &&\n+         !op->getResultTypes()[0].isa<RankedTensorType>();\n+}\n+\n+void LayoutPropagation::rewriteRegion(Region &region) {\n+  SmallVector<Region *> queue = {&region};\n+  while (!queue.empty()) {\n+    Region *currentRegion = queue.back();\n+    queue.pop_back();\n+    for (Operation &op : currentRegion->getOps()) {\n+      bool needRewrite = false;\n+      SmallVector<Value> results = op.getResults();\n+      for (Value result : results) {\n+        auto it = layouts.find(result);\n+        // If we haven't mapped this value skip.\n+        if (it == layouts.end())\n+          continue;\n+        LayoutInfo &info = it->second;\n+        assert(info.encodings.size() == 1 &&\n+               \"we should have resolved to a single encoding\");\n+        auto encoding = result.getType().cast<RankedTensorType>().getEncoding();\n+        // If the encoding is already what we want skip.\n+        if (encoding == *info.encodings.begin())\n+          continue;\n+        needRewrite = true;\n+      }\n+      if (needRewrite) {\n+        Operation *newOp = rewriteOp(&op);\n+        for (Region &R : newOp->getRegions())\n+          queue.push_back(&R);\n+      } else if (auto yieldOp = dyn_cast<scf::YieldOp>(&op)) {\n+        rewriteYieldOp(yieldOp);\n+      } else if (auto conditionOp = dyn_cast<scf::ConditionOp>(&op)) {\n+        rewriteConditionOp(conditionOp);\n+      } else if (reduceToScalar(&op)) {\n+        rewriteReduceToScalar(&op);\n+      } else {\n+        // If we don't need to rewrite the op we still need to remap the\n+        // operands.\n+        for (OpOperand &operand : op.getOpOperands()) {\n+          auto it = layouts.find(operand.get());\n+          if (it == layouts.end())\n+            continue;\n+          Attribute encoding =\n+              operand.get().getType().cast<RankedTensorType>().getEncoding();\n+          Value newOperand = getValueAs(operand.get(), encoding);\n+          op.setOperand(operand.getOperandNumber(), newOperand);\n+        }\n+        for (Region &R : op.getRegions())\n+          queue.push_back(&R);\n+      }\n+    }\n   }\n-};\n+  for (Operation *op : llvm::reverse(opToDelete))\n+    op->erase();\n+}\n \n-// -----------------------------------------------------------------------------\n-//\n-// -----------------------------------------------------------------------------\n+void LayoutPropagation::map(Value old, Value newV) {\n+  rewriteMapping[{old, newV.getType().cast<RankedTensorType>().getEncoding()}] =\n+      newV;\n+}\n \n-// op(cvt(arg_0), arg_1, ..., arg_n)\n-// -> cvt(op(arg_0, cvt(arg_1), ..., cvt(arg_n)))\n-void pushConversionForward(triton::gpu::ConvertLayoutOp cvt,\n-                           SetVector<Operation *> &cvtSlices,\n-                           mlir::PatternRewriter &rewriter) {\n-  auto srcEncoding =\n-      cvt.getOperand().getType().cast<RankedTensorType>().getEncoding();\n-  auto dstEncoding =\n-      cvt.getResult().getType().cast<RankedTensorType>().getEncoding();\n-  IRMapping mapping;\n-  auto op = cvtSlices.front();\n-  for (Value arg : op->getOperands()) {\n-    if (arg.getDefiningOp() == cvt)\n-      mapping.map(arg, cvt.getOperand());\n-    else {\n-      auto oldType = arg.getType().dyn_cast<RankedTensorType>();\n-      // TODO: we may be creating block pointer load/store with mismatching\n-      // pointer type.\n-      if (!oldType)\n-        continue;\n-      auto newType = RankedTensorType::get(\n-          oldType.getShape(), oldType.getElementType(), srcEncoding);\n-      auto cvtI = rewriter.create<triton::gpu::ConvertLayoutOp>(arg.getLoc(),\n-                                                                newType, arg);\n-      if (Operation *argOp = arg.getDefiningOp())\n-        cvtI->moveAfter(argOp);\n-      mapping.map(arg, cvtI);\n+Value LayoutPropagation::getValueAs(Value value, Attribute encoding) {\n+  if (auto tensorType = value.getType().dyn_cast<RankedTensorType>()) {\n+    Value rewrittenValue;\n+    auto layoutIt = layouts.find(value);\n+    if (layoutIt == layouts.end()) {\n+      rewrittenValue = value;\n+    } else {\n+      assert(layoutIt->second.encodings.size() == 1 &&\n+             \"we should have resolved to a single encoding\");\n+      Attribute encodingPicked = *(layoutIt->second.encodings.begin());\n+      if (encodingPicked == tensorType.getEncoding())\n+        rewrittenValue = value;\n+      else\n+        rewrittenValue = rewriteMapping[{value, encodingPicked}];\n     }\n+    assert(rewrittenValue);\n+    if (rewrittenValue.getType().cast<RankedTensorType>().getEncoding() ==\n+        encoding)\n+      return rewrittenValue;\n+    OpBuilder rewriter(value.getContext());\n+    rewriter.setInsertionPointAfterValue(rewrittenValue);\n+    auto tmpType = RankedTensorType::get(tensorType.getShape(),\n+                                         tensorType.getElementType(), encoding);\n+    Value converted = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        value.getLoc(), tmpType, rewrittenValue);\n+    // TODO: we could cache the conversion.\n+    return converted;\n   }\n-  rewriter.setInsertionPoint(op);\n-  if (op->getNumResults() == 0) {\n-    Operation *newOp = cloneWithInferType(rewriter, op, mapping);\n-    rewriter.eraseOp(op);\n-    return;\n+  return value;\n+}\n+\n+Operation *LayoutPropagation::cloneElementwise(OpBuilder &rewriter,\n+                                               Operation *op,\n+                                               Attribute encoding) {\n+  Operation *newOp = rewriter.clone(*op);\n+  for (OpOperand &operand : op->getOpOperands())\n+    newOp->setOperand(\n+        operand.getOperandNumber(),\n+        getValueAs(operand.get(), *inferSrcEncoding(op, encoding)));\n+  for (unsigned i = 0, e = op->getNumResults(); i < e; ++i) {\n+    auto origType = op->getResult(i).getType().dyn_cast<RankedTensorType>();\n+    if (!origType)\n+      continue;\n+    auto newType = RankedTensorType::get(origType.getShape(),\n+                                         origType.getElementType(), encoding);\n+    newOp->getResult(i).setType(newType);\n   }\n-  auto *newOp = cloneWithInferType(rewriter, op, mapping);\n-  auto newType = newOp->getResult(0).getType().cast<RankedTensorType>();\n-  auto newCvtType = RankedTensorType::get(\n-      newType.getShape(), newType.getElementType(), dstEncoding);\n-  auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-      newOp->getLoc(), newCvtType, newOp->getResult(0));\n-  rewriter.replaceOp(op, newCvt->getResults());\n+  return newOp;\n }\n \n-//\n-class MoveConvertOutOfIf : public mlir::RewritePattern {\n-public:\n-  explicit MoveConvertOutOfIf(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(scf::IfOp::getOperationName(), 2, context) {}\n+Operation *LayoutPropagation::rewriteForOp(scf::ForOp forOp) {\n+  SmallVector<Value> operands;\n+  OpBuilder rewriter(forOp);\n+  for (auto [operand, result] :\n+       llvm::zip(forOp.getInitArgs(), forOp.getResults())) {\n+    Value convertedOperand = operand;\n+    if (layouts.count(result))\n+      convertedOperand =\n+          getValueAs(operand, *layouts[result].encodings.begin());\n+    operands.push_back(convertedOperand);\n+  }\n+  auto newForOp = rewriter.create<scf::ForOp>(\n+      forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n+      forOp.getStep(), operands);\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ifOp = cast<scf::IfOp>(*op);\n-    // If \u201cscf.if\u201d defines no values, \u201cscf.yield\u201d will be inserted implicitly.\n-    // However, \"scf.else\" is not required to be present, so we need to check\n-    // if it exists.\n-    auto thenYield = ifOp.thenYield();\n-    int numOps = thenYield.getNumOperands();\n-    SmallVector<Value> newThenYieldOps = thenYield.getOperands();\n-    SetVector<Operation *> thenCvts;\n-    SmallVector<Type> newRetTypes;\n-\n-    bool hasElse = !ifOp.getElseRegion().empty();\n-\n-    scf::YieldOp elseYield;\n-    SmallVector<Value> newElseYieldOps;\n-    SetVector<Operation *> elseCvts;\n-    if (hasElse) {\n-      elseYield = ifOp.elseYield();\n-      newElseYieldOps = elseYield.getOperands();\n-    }\n+  newForOp.getBody()->getOperations().splice(\n+      newForOp.getBody()->getOperations().begin(),\n+      forOp.getBody()->getOperations());\n \n-    IRMapping mapping;\n-    for (size_t i = 0; i < numOps; i++) {\n-      auto thenCvt =\n-          thenYield.getOperand(i).getDefiningOp<triton::gpu::ConvertLayoutOp>();\n-      if (hasElse) {\n-        auto elseYield = ifOp.elseYield();\n-        auto elseCvt = elseYield.getOperand(i)\n-                           .getDefiningOp<triton::gpu::ConvertLayoutOp>();\n-        if (thenCvt && elseCvt &&\n-            std::distance(elseCvt->user_begin(), elseCvt->user_end()) == 1 &&\n-            std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1 &&\n-            thenCvt.getOperand().getType() == elseCvt.getOperand().getType()) {\n-          // If thenCvt and elseCvt's type are the same, it means a single\n-          // conversion is enough to replace both of them. We can move the\n-          // conversion out of scf.if and replace both thenCvt and elseCvt with\n-          // the new conversion.\n-          mapping.map(thenCvt.getResult(), thenCvt.getOperand());\n-          thenCvts.insert((Operation *)thenCvt);\n-          newRetTypes.push_back(thenCvt.getOperand().getType());\n-          mapping.map(elseCvt.getResult(), elseCvt.getOperand());\n-          elseCvts.insert((Operation *)elseCvt);\n-        } else\n-          // Cannot move out of scf.if because thenCvt != elseCvt\n-          // Moving it out of scf.if will introduce a new conversion\n-          newRetTypes.push_back(thenYield.getOperand(i).getType());\n-      } else {\n-        if (thenCvt &&\n-            std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1) {\n-          // If there's only a single use of the conversion then we can move it\n-          mapping.map(thenCvt.getResult(), thenCvt.getOperand());\n-          thenCvts.insert((Operation *)thenCvt);\n-          newRetTypes.push_back(thenCvt.getOperand().getType());\n-        } else\n-          // Cannot move out of scf.if because either there's another use of\n-          // the conversion or there's no conversion at all\n-          newRetTypes.push_back(thenYield.getOperand(i).getType());\n-      }\n+  for (auto [oldResult, newResult] :\n+       llvm::zip(forOp.getResults(), newForOp.getResults())) {\n+    if (oldResult.getType() == newResult.getType()) {\n+      oldResult.replaceAllUsesWith(newResult);\n+      continue;\n     }\n-    if (mapping.getValueMap().empty())\n-      return mlir::failure();\n+    map(oldResult, newResult);\n+  }\n \n-    auto newIfOp = rewriter.create<scf::IfOp>(ifOp.getLoc(), newRetTypes,\n-                                              ifOp.getCondition(), hasElse);\n-    auto rematerialize = [&](Block *block, SetVector<Operation *> &cvts) {\n-      for (Operation &op : block->getOperations()) {\n-        if (cvts.contains(&op)) {\n-          if (mapping.contains(op.getOperand(0)))\n-            mapping.map(op.getResult(0), mapping.lookup(op.getOperand(0)));\n-          continue;\n-        }\n-        cloneWithInferType(rewriter, &op, mapping);\n-      }\n-    };\n-    rewriter.setInsertionPointToEnd(newIfOp.thenBlock());\n-    rematerialize(ifOp.thenBlock(), thenCvts);\n-    if (hasElse) {\n-      rewriter.setInsertionPointToEnd(newIfOp.elseBlock());\n-      rematerialize(ifOp.elseBlock(), elseCvts);\n+  for (auto [oldArg, newArg] : llvm::zip(forOp.getBody()->getArguments(),\n+                                         newForOp.getBody()->getArguments())) {\n+    if (oldArg.getType() == newArg.getType()) {\n+      oldArg.replaceAllUsesWith(newArg);\n+      continue;\n     }\n+    map(oldArg, newArg);\n+  }\n+  return newForOp.getOperation();\n+}\n \n-    rewriter.setInsertionPointAfter(newIfOp);\n-    SmallVector<Value> newRetValues = newIfOp.getResults();\n-    for (size_t i = 0; i < numOps; i++) {\n-      if (newIfOp.getResult(i).getType() != ifOp.getResult(i).getType()) {\n-        newRetValues[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-            newIfOp.getLoc(), ifOp.getResult(i).getType(),\n-            newIfOp.getResult(i));\n-      }\n+Operation *LayoutPropagation::rewriteWhileOp(scf::WhileOp whileOp) {\n+  SmallVector<Value> operands;\n+  SmallVector<Type> returnTypes;\n+  OpBuilder rewriter(whileOp);\n+  for (auto [operand, arg] :\n+       llvm::zip(whileOp->getOperands(), whileOp.getBeforeArguments())) {\n+    Value convertedOperand = operand;\n+    if (layouts.count(arg))\n+      convertedOperand = getValueAs(operand, *layouts[arg].encodings.begin());\n+    operands.push_back(convertedOperand);\n+  }\n+  for (Value ret : whileOp.getResults()) {\n+    auto it = layouts.find(ret);\n+    if (it == layouts.end()) {\n+      returnTypes.push_back(ret.getType());\n+      continue;\n     }\n-\n-    rewriter.replaceOp(op, newRetValues);\n-    return mlir::success();\n+    auto origType = ret.getType().dyn_cast<RankedTensorType>();\n+    auto newType =\n+        RankedTensorType::get(origType.getShape(), origType.getElementType(),\n+                              it->second.encodings[0]);\n+    returnTypes.push_back(newType);\n   }\n-};\n \n-//\n-class RematerializeForward : public mlir::RewritePattern {\n-public:\n-  explicit RematerializeForward(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             1, context) {}\n+  auto newWhileOp =\n+      rewriter.create<scf::WhileOp>(whileOp.getLoc(), returnTypes, operands);\n+  SmallVector<Type> argsTypesBefore;\n+  for (Value operand : operands)\n+    argsTypesBefore.push_back(operand.getType());\n+  SmallVector<Location> bbArgLocsBefore(argsTypesBefore.size(),\n+                                        whileOp.getLoc());\n+  SmallVector<Location> bbArgLocsAfter(returnTypes.size(), whileOp.getLoc());\n+  rewriter.createBlock(&newWhileOp.getBefore(), {}, argsTypesBefore,\n+                       bbArgLocsBefore);\n+  rewriter.createBlock(&newWhileOp.getAfter(), {}, returnTypes, bbArgLocsAfter);\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *cvtOp,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto cvt = dyn_cast<triton::gpu::ConvertLayoutOp>(*cvtOp);\n-    auto srcEncoding =\n-        cvt.getOperand().getType().cast<RankedTensorType>().getEncoding();\n-    auto dstEncoding =\n-        cvt.getResult().getType().cast<RankedTensorType>().getEncoding();\n-    if (srcEncoding.isa<triton::gpu::SharedEncodingAttr>() ||\n-        dstEncoding.isa<triton::gpu::SharedEncodingAttr>())\n-      return failure();\n-    // heuristics for flash attention\n-    if (srcEncoding.isa<triton::gpu::SliceEncodingAttr>())\n-      return failure();\n-    // For cases like:\n-    // %0 = convert_layout %arg0\n-    // We should try to move %0 out of scf.for first, if it couldn't be moved\n-    // out additional conversions will be added to the loop body.\n-    if (!cvt.getOperand().getDefiningOp() &&\n-        isa<scf::ForOp>(cvt->getParentOp()))\n-      return failure();\n+  for (int i = 0; i < whileOp.getNumRegions(); ++i) {\n+    newWhileOp->getRegion(i).front().getOperations().splice(\n+        newWhileOp->getRegion(i).front().getOperations().begin(),\n+        whileOp->getRegion(i).front().getOperations());\n+  }\n \n-    SetVector<Operation *> cvtSlices;\n-    auto filter = [&](Operation *op) {\n-      return op->getBlock() == cvt->getBlock() &&\n-             !isa<triton::gpu::ConvertLayoutOp, scf::YieldOp>(op) &&\n-             !(isa<triton::ReduceOp>(op) &&\n-               !op->getResult(0).getType().isa<RankedTensorType>());\n-    };\n-    mlir::getForwardSlice(cvt.getResult(), &cvtSlices, {filter});\n-    if (cvtSlices.empty())\n-      return failure();\n+  auto remapArg = [&](Value oldVal, Value newVal) {\n+    if (oldVal.getType() == newVal.getType())\n+      oldVal.replaceAllUsesWith(newVal);\n+    else\n+      map(oldVal, newVal);\n+  };\n+  for (auto [oldResult, newResult] :\n+       llvm::zip(whileOp.getResults(), newWhileOp.getResults()))\n+    remapArg(oldResult, newResult);\n+  for (auto [oldArg, newArg] :\n+       llvm::zip(whileOp.getBeforeArguments(), newWhileOp.getBeforeArguments()))\n+    remapArg(oldArg, newArg);\n+  for (auto [oldArg, newArg] :\n+       llvm::zip(whileOp.getAfterArguments(), newWhileOp.getAfterArguments()))\n+    remapArg(oldArg, newArg);\n+  return newWhileOp.getOperation();\n+}\n \n-    for (Operation *op : cvtSlices) {\n-      // don't rematerialize anything expensive\n-      if (isExpensiveToRemat(op, srcEncoding))\n-        return failure();\n-      // don't rematerialize non-element-wise\n-      if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n-          !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n-          !isa<triton::StoreOp, triton::AssertOp, triton::PrintOp,\n-               triton::ReduceOp>(op))\n-        return failure();\n-      // don't rematerialize if it adds an extra conversion that can't\n-      // be removed\n-      for (Value arg : op->getOperands()) {\n-        Operation *argOp = arg.getDefiningOp();\n-        SetVector<Operation *> processed;\n-        SetVector<Attribute> layout;\n-        llvm::MapVector<Value, Attribute> toConvert;\n-        int numAddedConvs = simulateBackwardRematerialization(\n-            argOp, processed, layout, toConvert, srcEncoding);\n-        if (argOp && !isa<triton::gpu::ConvertLayoutOp>(argOp) &&\n-            cvtSlices.count(argOp) == 0 && numAddedConvs > 0)\n-          return failure();\n-      }\n+Operation *LayoutPropagation::rewriteIfOp(scf::IfOp ifOp) {\n+  SmallVector<Value> operands;\n+  OpBuilder rewriter(ifOp);\n+  SmallVector<Type> newResultTypes(ifOp->getResultTypes());\n+  for (unsigned i = 0, e = ifOp->getNumResults(); i < e; ++i) {\n+    auto it = layouts.find(ifOp->getResult(i));\n+    if (it == layouts.end())\n+      continue;\n+    auto origType = ifOp->getResult(i).getType().cast<RankedTensorType>();\n+    Attribute encoding = *(it->second.encodings.begin());\n+    newResultTypes[i] = RankedTensorType::get(\n+        origType.getShape(), origType.getElementType(), encoding);\n+  }\n+  auto newIfOp = rewriter.create<scf::IfOp>(ifOp.getLoc(), newResultTypes,\n+                                            ifOp.getCondition(), true, true);\n+  newIfOp.getThenRegion().takeBody(ifOp.getThenRegion());\n+  newIfOp.getElseRegion().takeBody(ifOp.getElseRegion());\n+  for (auto [oldResult, newResult] :\n+       llvm::zip(ifOp.getResults(), newIfOp.getResults())) {\n+    if (oldResult.getType() == newResult.getType()) {\n+      oldResult.replaceAllUsesWith(newResult);\n+      continue;\n     }\n+    map(oldResult, newResult);\n+  }\n+  return newIfOp.getOperation();\n+}\n \n-    // Call SimplifyReduceCvt instead of the general push conversion forward\n-    if (isa<triton::ReduceOp>(cvtSlices.front()))\n-      return failure();\n+void LayoutPropagation::rewriteYieldOp(scf::YieldOp yieldOp) {\n+  Operation *parentOp = yieldOp->getParentOp();\n+  for (OpOperand &operand : yieldOp->getOpOperands()) {\n+    Type yieldType = operand.get().getType();\n+    if (isa<scf::ForOp, scf::IfOp>(parentOp))\n+      yieldType = parentOp->getResult(operand.getOperandNumber()).getType();\n+    if (auto whileOp = dyn_cast<scf::WhileOp>(parentOp))\n+      yieldType =\n+          whileOp.getBeforeArguments()[operand.getOperandNumber()].getType();\n+    auto tensorType = yieldType.dyn_cast<RankedTensorType>();\n+    if (!tensorType)\n+      continue;\n+    Value newOperand = getValueAs(operand.get(), tensorType.getEncoding());\n+    yieldOp->setOperand(operand.getOperandNumber(), newOperand);\n+  }\n+}\n \n-    pushConversionForward(cvt, cvtSlices, rewriter);\n-    return success();\n+void LayoutPropagation::rewriteConditionOp(scf::ConditionOp conditionOp) {\n+  scf::WhileOp whileOp = cast<scf::WhileOp>(conditionOp->getParentOp());\n+  for (unsigned i = 1; i < conditionOp->getNumOperands(); ++i) {\n+    OpOperand &operand = conditionOp->getOpOperand(i);\n+    Type argType = whileOp->getResult(operand.getOperandNumber() - 1).getType();\n+    auto tensorType = argType.dyn_cast<RankedTensorType>();\n+    if (!tensorType)\n+      continue;\n+    Value newOperand = getValueAs(operand.get(), tensorType.getEncoding());\n+    conditionOp->setOperand(operand.getOperandNumber(), newOperand);\n   }\n-};\n+}\n \n-// Layout conversions are expensive. They require going through\n-// shared memory, which is orders of magnitude slower than\n-// other non-i/o operations in the dialect.\n-// It therefore makes sense to remove them whenever possible,\n-// even if it means rematerializing all values whose definitions\n-// are reachable from it without passing through any memory operation.\n-class RematerializeBackward : public mlir::RewritePattern {\n-public:\n-  explicit RematerializeBackward(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             3, context) {}\n+void LayoutPropagation::rewriteReduceToScalar(Operation *reduceOp) {\n+  OpBuilder rewriter(reduceOp);\n+  Attribute srcEncoding;\n+  // Since all the operands need to have the same encoding pick the first one\n+  // and use it for all the operands.\n+  for (Value operand : reduceOp->getOperands()) {\n+    auto it = layouts.find(operand);\n+    if (it != layouts.end()) {\n+      srcEncoding = it->second.encodings[0];\n+      break;\n+    }\n+  }\n+  if (!srcEncoding)\n+    return;\n+  for (OpOperand &operand : reduceOp->getOpOperands()) {\n+    Value newOperand = getValueAs(operand.get(), srcEncoding);\n+    reduceOp->setOperand(operand.getOperandNumber(), newOperand);\n+  }\n+}\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *cvt,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    if (!llvm::isa<triton::gpu::ConvertLayoutOp>(cvt))\n-      return mlir::failure();\n-    // we don't touch block arguments\n-    Operation *op = cvt->getOperand(0).getDefiningOp();\n-    if (!op)\n-      return mlir::failure();\n-    // we don't want to rematerialize any conversion to/from shared\n-    if (triton::gpu::isSharedEncoding(cvt->getResults()[0]) ||\n-        triton::gpu::isSharedEncoding(cvt->getOperand(0)))\n-      return mlir::failure();\n-    // we don't handle conversions to DotOperandEncodingAttr\n-    // this is a heuristics to accommodate fused attention\n-    auto targetType = cvt->getResultTypes()[0].cast<RankedTensorType>();\n-    if (targetType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n-      return mlir::failure();\n-    // DFS\n-    SetVector<Operation *> processed;\n-    SetVector<Attribute> layout;\n-    llvm::MapVector<Value, Attribute> toConvert;\n-    if (simulateBackwardRematerialization(cvt, processed, layout, toConvert,\n-                                          targetType.getEncoding()) > 0)\n-      return mlir::failure();\n+Operation *LayoutPropagation::rewriteOp(Operation *op) {\n+  opToDelete.push_back(op);\n+  if (auto forOp = dyn_cast<scf::ForOp>(op))\n+    return rewriteForOp(forOp);\n+  if (auto whileOp = dyn_cast<scf::WhileOp>(op))\n+    return rewriteWhileOp(whileOp);\n+  if (auto ifOp = dyn_cast<scf::IfOp>(op))\n+    return rewriteIfOp(ifOp);\n+  OpBuilder rewriter(op);\n+  Attribute encoding = *layouts[op->getResult(0)].encodings.begin();\n+  if (auto convertOp = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n+    Attribute srcEncoding =\n+        convertOp.getOperand().getType().cast<RankedTensorType>().getEncoding();\n+    auto it = layouts.find(convertOp.getOperand());\n+    if (it != layouts.end())\n+      srcEncoding = *(it->second.encodings.begin());\n+    Value src = getValueAs(convertOp.getOperand(), srcEncoding);\n+    auto tensorType = op->getResult(0).getType().cast<RankedTensorType>();\n+    auto newType = RankedTensorType::get(tensorType.getShape(),\n+                                         tensorType.getElementType(), encoding);\n+    auto cvt = rewriter.create<triton::gpu::ConvertLayoutOp>(op->getLoc(),\n+                                                             newType, src);\n+    map(op->getResult(0), cvt.getResult());\n+    return cvt.getOperation();\n+  }\n+  if (canFoldIntoConversion(op, encoding)) {\n+    Operation *newOp = rewriter.clone(*op);\n+    auto tensorType = op->getResult(0).getType().cast<RankedTensorType>();\n+    auto newType = RankedTensorType::get(tensorType.getShape(),\n+                                         tensorType.getElementType(), encoding);\n+    auto cvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        op->getLoc(), newType, newOp->getResult(0));\n+    map(op->getResult(0), cvt.getResult());\n+    return cvt.getOperation();\n+  }\n+  if (op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() ||\n+      op->hasTrait<mlir::OpTrait::Elementwise>() ||\n+      isa<triton::ReduceOp, triton::ExpandDimsOp, triton::gpu::ConvertLayoutOp>(\n+          op)) {\n+    Operation *newOp = cloneElementwise(rewriter, op, encoding);\n+    for (auto [oldResult, newResult] :\n+         llvm::zip(op->getResults(), newOp->getResults()))\n+      map(oldResult, newResult);\n+    return newOp;\n+  }\n+  assert(0 && \"unexpected op in rewrite\");\n+  return nullptr;\n+}\n \n-    IRMapping mapping;\n-    rematerializeConversionChain(toConvert, rewriter, processed, mapping);\n-    rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n+static bool canBeRemat(Operation *op) {\n+  if (isa<triton::LoadOp, triton::StoreOp>(op))\n+    return !isExpensiveLoadOrStore(op);\n+  if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n+          triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n+          triton::AtomicCASOp, triton::DotOp>(op))\n+    return false;\n+  if (isa<scf::IfOp, scf::WhileOp, scf::ConditionOp>(op))\n+    return false;\n \n-    return mlir::success();\n-  }\n-};\n+  return true;\n+}\n \n-// -----------------------------------------------------------------------------\n-//\n-// -----------------------------------------------------------------------------\n+// Replace ForOp with a new ForOp with extra operands. The YieldOp is not\n+// updated and needs to be updated separatly for the loop to be correct.\n+static scf::ForOp replaceForOpWithNewSignature(OpBuilder &rewriter,\n+                                               scf::ForOp loop,\n+                                               ValueRange newIterOperands) {\n+  OpBuilder::InsertionGuard g(rewriter);\n+  rewriter.setInsertionPoint(loop);\n \n-class MoveConvertOutOfLoop : public mlir::RewritePattern {\n-public:\n-  explicit MoveConvertOutOfLoop(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(scf::ForOp::getOperationName(), 1, context) {}\n-\n-  SmallVector<Value, 4>\n-  rematerializeForLoop(mlir::PatternRewriter &rewriter, scf::ForOp &forOp,\n-                       size_t i, RankedTensorType newType,\n-                       triton::gpu::ConvertLayoutOp origConversion) const {\n-    // Rewrite init argument\n-    auto origType = forOp.getInitArgs()[i].getType().cast<RankedTensorType>();\n-    SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n-    newInitArgs[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        newInitArgs[i].getLoc(), newType, newInitArgs[i]);\n-    // Clone for loop\n-    auto newForOp = rewriter.create<scf::ForOp>(\n-        forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n-        forOp.getStep(), newInitArgs);\n-    newForOp->moveBefore(forOp);\n-    rewriter.setInsertionPointToStart(newForOp.getBody());\n-    IRMapping mapping;\n-    for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n-      mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n-    mapping.map(origConversion.getResult(), newForOp.getRegionIterArgs()[i]);\n-\n-    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n-    for (Operation &op : forOp.getBody()->without_terminator()) {\n-      if (dyn_cast<triton::gpu::ConvertLayoutOp>(op) == origConversion)\n-        continue;\n+  // Create a new loop before the existing one, with the extra operands.\n+  rewriter.setInsertionPoint(loop);\n+  auto operands = llvm::to_vector<4>(loop.getIterOperands());\n+  operands.append(newIterOperands.begin(), newIterOperands.end());\n+  scf::ForOp newLoop = rewriter.create<scf::ForOp>(\n+      loop.getLoc(), loop.getLowerBound(), loop.getUpperBound(), loop.getStep(),\n+      operands);\n+  newLoop.getBody()->erase();\n \n-      bool convert = llvm::any_of(op.getOperands(), [&](auto operand) {\n-        return operand == origConversion.getOperand();\n-      });\n-      auto convertLayout = [&](Value operand, Value value, Attribute encoding) {\n-        auto tensorType = value.getType().cast<RankedTensorType>();\n-        auto cvtType = RankedTensorType::get(\n-            tensorType.getShape(), tensorType.getElementType(), encoding);\n-        auto cvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-            op.getLoc(), cvtType, value);\n-        mapping.map(operand, cvt);\n-      };\n-      DenseMap<Value, Value> cvtValues;\n-      if (convert) {\n-        for (auto operand : op.getOperands()) {\n-          if (operand == origConversion.getOperand() ||\n-              !isa<RankedTensorType>(operand.getType()))\n-            continue;\n-          auto value = mapping.lookupOrDefault(operand);\n-          // Convert to the new type\n-          convertLayout(operand, value, newType.getEncoding());\n-          // Other ops don't use the converted value and we need to restore\n-          cvtValues[operand] = value;\n+  newLoop.getLoopBody().getBlocks().splice(\n+      newLoop.getLoopBody().getBlocks().begin(),\n+      loop.getLoopBody().getBlocks());\n+  for (Value operand : newIterOperands)\n+    newLoop.getBody()->addArgument(operand.getType(), operand.getLoc());\n+\n+  for (auto it : llvm::zip(loop.getResults(), newLoop.getResults().take_front(\n+                                                  loop.getNumResults())))\n+    std::get<0>(it).replaceAllUsesWith(std::get<1>(it));\n+  return newLoop;\n+}\n+\n+static void rewriteSlice(SetVector<Value> &slice,\n+                         DenseMap<Value, Attribute> &layout,\n+                         ConvertLayoutOp convertOp, IRMapping &mapping) {\n+  SetVector<Operation *> opsToRewrite;\n+  for (Value v : slice) {\n+    if (v.getDefiningOp()) {\n+      opsToRewrite.insert(v.getDefiningOp());\n+    } else {\n+      opsToRewrite.insert(v.cast<BlockArgument>().getOwner()->getParentOp());\n+      // We also need to rewrite the yield op.\n+      opsToRewrite.insert(v.cast<BlockArgument>().getOwner()->getTerminator());\n+    }\n+  }\n+  opsToRewrite = multiRootTopologicalSort(opsToRewrite);\n+\n+  SmallVector<Operation *> deadLoops;\n+  OpBuilder builder(slice.begin()->getContext());\n+  for (Operation *op : opsToRewrite) {\n+    if (auto forOp = dyn_cast<scf::ForOp>(op)) {\n+      // Keep a mapping of the operands index to the new operands index.\n+      SmallVector<std::pair<size_t, size_t>> argMapping;\n+      SmallVector<Value> newOperands;\n+      for (auto arg : forOp.getRegionIterArgs()) {\n+        if (slice.count(arg)) {\n+          OpOperand &initVal = forOp.getOpOperandForRegionIterArg(arg);\n+          argMapping.push_back(\n+              std::make_pair(*forOp.getIterArgNumberForOpOperand(initVal),\n+                             forOp.getNumIterOperands() + newOperands.size()));\n+          newOperands.push_back(mapping.lookup(initVal.get()));\n         }\n       }\n-      auto *newOp = cloneWithInferType(rewriter, &op, mapping);\n-      if (convert) {\n-        for (auto result : op.getResults()) {\n-          if (!isa<RankedTensorType>(result.getType()))\n-            continue;\n-          auto value = mapping.lookupOrDefault(result);\n-          auto tensorType = result.getType().cast<RankedTensorType>();\n-          // Convert to the original type\n-          convertLayout(result, value, tensorType.getEncoding());\n-        }\n-        // Restore original values\n-        for (auto [operand, value] : cvtValues)\n-          mapping.map(operand, value);\n+      // Create a new for loop with the new operands.\n+      scf::ForOp newForOp =\n+          replaceForOpWithNewSignature(builder, forOp, newOperands);\n+      deadLoops.push_back(forOp.getOperation());\n+      Block &loopBody = *newForOp.getBody();\n+      for (auto m : argMapping) {\n+        mapping.map(newForOp.getResult(m.first), newForOp.getResult(m.second));\n+        int numIndVars = newForOp.getNumInductionVars();\n+        mapping.map(loopBody.getArgument(m.first + numIndVars),\n+                    loopBody.getArgument(m.second + numIndVars));\n       }\n+      continue;\n+    }\n+    builder.setInsertionPoint(op);\n+    if (auto yieldOp = dyn_cast<scf::YieldOp>(op)) {\n+      auto yieldOperands = llvm::to_vector(yieldOp.getOperands());\n+      for (Value operand : yieldOp.getOperands()) {\n+        if (slice.count(operand) == 0)\n+          continue;\n+        yieldOperands.push_back(mapping.lookup(operand));\n+      }\n+      builder.create<scf::YieldOp>(op->getLoc(), yieldOperands);\n+      op->erase();\n+      continue;\n+    }\n+    if (isa<arith::ConstantOp>(op)) {\n+      Operation *newOp = builder.clone(*op);\n+      auto tensorType = op->getResult(0).getType().cast<RankedTensorType>();\n+      auto newType = RankedTensorType::get(tensorType.getShape(),\n+                                           tensorType.getElementType(),\n+                                           layout[op->getResult(0)]);\n+      auto cvt = builder.create<triton::gpu::ConvertLayoutOp>(\n+          op->getLoc(), newType, newOp->getResult(0));\n+      mapping.map(op->getResult(0), cvt.getResult());\n+      continue;\n+    }\n+    Operation *newOp = builder.clone(*op, mapping);\n+    for (auto [old, newV] : llvm::zip(op->getResults(), newOp->getResults())) {\n+      auto it = layout.find(old);\n+      if (it == layout.end())\n+        continue;\n+      auto newType = RankedTensorType::get(\n+          old.getType().cast<RankedTensorType>().getShape(),\n+          old.getType().cast<RankedTensorType>().getElementType(), it->second);\n+      newV.setType(newType);\n     }\n-    // create yield, inserting conversions if necessary\n-    auto yieldOp = forOp.getBody()->getTerminator();\n-    SmallVector<Value, 4> newYieldArgs;\n-    // We use the new type for the result of the conversion\n-    for (Value arg : yieldOp->getOperands())\n-      newYieldArgs.push_back(mapping.lookup(arg));\n-    if (newYieldArgs[i].getType() != newType)\n-      newYieldArgs[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          yieldOp->getLoc(), newType, newYieldArgs[i]);\n-    rewriter.create<scf::YieldOp>(forOp.getLoc(), newYieldArgs);\n-\n-    // replace\n-    SmallVector<Value, 4> newResults = newForOp->getResults();\n-    newResults[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        newForOp.getLoc(), origType, newForOp->getResult(i));\n-    newResults[i].getDefiningOp()->moveAfter(newForOp);\n-\n-    return newResults;\n   }\n+  convertOp.replaceAllUsesWith(mapping.lookup(convertOp.getOperand()));\n+  convertOp.erase();\n+  for (Operation *op : deadLoops)\n+    op->erase();\n+}\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto forOp = cast<scf::ForOp>(op);\n-    auto iterArgs = forOp.getRegionIterArgs();\n-    for (const auto &iterArg : llvm::enumerate(iterArgs)) {\n-      // skip non-tensor types\n-      if (!iterArg.value().getType().isa<RankedTensorType>())\n-        continue;\n-      SmallVector<Operation *> cvts;\n-      if (canMoveOutOfLoop(iterArg.value(), cvts).failed())\n+static void rewriteSlice(SetVector<Value> &slice,\n+                         DenseMap<Value, Attribute> &layout,\n+                         ConvertLayoutOp convertOp) {\n+  IRMapping mapping;\n+  rewriteSlice(slice, layout, convertOp, mapping);\n+}\n+\n+static LogicalResult getRematerializableSlice(\n+    Value root, Attribute rootEncoding, SetVector<Value> &slice,\n+    DenseMap<Value, Attribute> &layout,\n+    std::function<bool(Operation *)> stopPropagation = nullptr) {\n+  LogicalResult result = getConvertBackwardSlice(root, slice, rootEncoding,\n+                                                 layout, stopPropagation);\n+  if (result.failed() || slice.empty())\n+    return failure();\n+\n+  // Check if all the operations in the slice can be rematerialized.\n+  for (Value v : slice) {\n+    if (Operation *op = v.getDefiningOp()) {\n+      if (!canBeRemat(op))\n+        return failure();\n+    }\n+  }\n+  return success();\n+}\n+\n+static void backwardRematerialization(ConvertLayoutOp convertOp) {\n+  // we don't want to rematerialize any conversion to/from shared\n+  if (triton::gpu::isSharedEncoding(convertOp.getResult()) ||\n+      triton::gpu::isSharedEncoding(convertOp.getOperand()))\n+    return;\n+  // we don't handle conversions to DotOperandEncodingAttr\n+  // this is a heuristics to accommodate fused attention\n+  auto targetType = convertOp->getResultTypes()[0].cast<RankedTensorType>();\n+  if (targetType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n+    return;\n+\n+  // 1. Take a backward slice of all the tensor dependencies that can be\n+  // rematerialized.\n+  SetVector<Value> slice;\n+  DenseMap<Value, Attribute> layout;\n+  LogicalResult result = getRematerializableSlice(\n+      convertOp.getOperand(), targetType.getEncoding(), slice, layout);\n+  if (result.failed())\n+    return;\n+\n+  // 2. Rewrite the slice.\n+  rewriteSlice(slice, layout, convertOp);\n+}\n+\n+// For convert left we try to hoist them above type extension to reduce the cost\n+// of the convert.\n+static void hoistConvertOnTopOfExt(ConvertLayoutOp convertOp) {\n+  // we don't want to rematerialize any conversion to/from shared\n+  if (triton::gpu::isSharedEncoding(convertOp.getResult()) ||\n+      triton::gpu::isSharedEncoding(convertOp.getOperand()))\n+    return;\n+  // we don't handle conversions to DotOperandEncodingAttr\n+  // this is a heuristics to accommodate fused attention\n+  auto targetType = convertOp->getResultTypes()[0].cast<RankedTensorType>();\n+  if (targetType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n+    return;\n+\n+  auto isExtOp = [](Operation *op) {\n+    return isa<arith::ExtSIOp, arith::ExtUIOp, arith::ExtFOp>(op);\n+  };\n+  // 1. Take a backward slice of all the tensor dependencies.\n+  SetVector<Value> slice;\n+  DenseMap<Value, Attribute> layout;\n+  LogicalResult result = getRematerializableSlice(\n+      convertOp.getOperand(), targetType.getEncoding(), slice, layout, isExtOp);\n+  if (result.failed())\n+    return;\n+\n+  Operation *extOp = nullptr;\n+  unsigned sliceSize = slice.size();\n+  for (unsigned i = 0; i < sliceSize; i++) {\n+    Value v = slice[i];\n+    Operation *op = v.getDefiningOp();\n+    if (!op)\n+      continue;\n+    if (isExtOp(op)) {\n+      SetVector<Value> tempSlice;\n+      DenseMap<Value, Attribute> tempLayout;\n+      LogicalResult result = getRematerializableSlice(\n+          op->getOperand(0), layout[v], tempSlice, tempLayout);\n+      // If we can rematerialize the rest of the ext slice we can ignore this\n+      // ext as it won't need a convert.\n+      if (result.succeeded()) {\n+        slice.insert(tempSlice.begin(), tempSlice.end());\n+        layout.insert(tempLayout.begin(), tempLayout.end());\n         continue;\n-      // check\n-      for (auto *op : cvts) {\n-        auto cvt = dyn_cast<triton::gpu::ConvertLayoutOp>(op);\n-        auto targetType = op->getResultTypes()[0].cast<RankedTensorType>();\n-        auto newFor = rematerializeForLoop(rewriter, forOp, iterArg.index(),\n-                                           targetType, cvt);\n-        rewriter.replaceOp(forOp, newFor);\n-        return success();\n       }\n+      // Only apply it if there is a single ext op otherwise we would have to\n+      // duplicate the convert.\n+      if (extOp != nullptr)\n+        return;\n+      extOp = op;\n     }\n-    return failure();\n   }\n-};\n \n-//\n-class ConvertDotConvert : public mlir::RewritePattern {\n-public:\n-  ConvertDotConvert(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             1, context) {}\n-\n-  LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto dotOp = dstOp.getSrc().getDefiningOp<triton::DotOp>();\n-    if (!dotOp)\n-      return mlir::failure();\n-    if (std::distance(dstOp->user_begin(), dstOp->user_end()) != 1 ||\n-        std::distance(dotOp->user_begin(), dotOp->user_end()) != 1)\n-      return mlir::failure();\n-    auto cvtOp =\n-        dotOp.getOperand(2).getDefiningOp<triton::gpu::ConvertLayoutOp>();\n-    if (!cvtOp)\n-      return mlir::failure();\n-    if (!cvtOp.getSrc().getDefiningOp<triton::LoadOp>())\n-      return failure();\n-    auto dstTy = dstOp.getResult().getType().cast<RankedTensorType>();\n-    auto srcTy = cvtOp.getOperand().getType().cast<RankedTensorType>();\n-    if (dstTy != srcTy)\n-      return mlir::failure();\n+  if (extOp == nullptr)\n+    return;\n+  // Move the convert before the ext op and rewrite the slice.\n+  OpBuilder builder(extOp);\n+  auto tensorType = extOp->getOperand(0).getType().cast<RankedTensorType>();\n+  auto newType =\n+      RankedTensorType::get(tensorType.getShape(), tensorType.getElementType(),\n+                            layout[extOp->getResult(0)]);\n+  auto newConvertOp = builder.create<ConvertLayoutOp>(\n+      convertOp.getLoc(), newType, extOp->getOperand(0));\n+  IRMapping mapping;\n+  mapping.map(extOp->getOperand(0), newConvertOp.getResult());\n+  // 3. Rewrite the slice.\n+  rewriteSlice(slice, layout, convertOp, mapping);\n+}\n \n-    auto _0f = rewriter.create<arith::ConstantOp>(\n-        op->getLoc(), dstTy.getElementType(),\n-        rewriter.getZeroAttr(dstTy.getElementType()));\n-    auto _0 = rewriter.create<triton::SplatOp>(\n-        op->getLoc(), dotOp.getResult().getType(), _0f);\n-    auto newDot = rewriter.create<triton::DotOp>(\n-        op->getLoc(), dotOp.getResult().getType(), dotOp.getOperand(0),\n-        dotOp.getOperand(1), _0, dotOp.getAllowTF32());\n-    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        op->getLoc(), dstTy, newDot.getResult());\n-    rewriter.replaceOpWithNewOp<arith::AddFOp>(op, newCvt, cvtOp.getOperand());\n-    return mlir::success();\n+static void backwardRematerialization(ModuleOp module) {\n+  SmallVector<ConvertLayoutOp> convertOps;\n+  module.walk(\n+      [&](ConvertLayoutOp convertOp) { convertOps.push_back(convertOp); });\n+  for (ConvertLayoutOp convertOp : convertOps) {\n+    backwardRematerialization(convertOp);\n   }\n-};\n+}\n \n-} // namespace\n+static void hoistConvert(ModuleOp module) {\n+  SmallVector<ConvertLayoutOp> convertOps;\n+  module.walk(\n+      [&](ConvertLayoutOp convertOp) { convertOps.push_back(convertOp); });\n+  for (ConvertLayoutOp convertOp : convertOps) {\n+    hoistConvertOnTopOfExt(convertOp);\n+  }\n+}\n \n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n@@ -632,18 +1004,45 @@ class TritonGPURemoveLayoutConversionsPass\n     MLIRContext *context = &getContext();\n     ModuleOp m = getOperation();\n \n-    mlir::RewritePatternSet patterns(context);\n+    // 1. Propagate layout forward starting from \"anchor\" ops.\n+    m.walk([](triton::FuncOp funcOp) {\n+      LayoutPropagation layoutPropagation(funcOp);\n+      layoutPropagation.initAnchorLayout();\n+      layoutPropagation.propagateLayout();\n+      layoutPropagation.resolveConflicts();\n+      layoutPropagation.rewrite();\n+    });\n+\n+    mlir::RewritePatternSet cleanUpPatterns(context);\n+    ConvertLayoutOp::getCanonicalizationPatterns(cleanUpPatterns, context);\n+    if (mlir::applyPatternsAndFoldGreedily(m, std::move(cleanUpPatterns))\n+            .failed()) {\n+      signalPassFailure();\n+    }\n+\n+    // 2. For convert ops left try to rematerialize the slice of producer\n+    // operation to avoid having to convert.\n+    backwardRematerialization(m);\n+    // 3. For converts left try to hoist them above cast generating larger size\n+    // types in order to reduce the cost of the convert op.\n+    hoistConvert(m);\n \n-    patterns.add<SimplifyConversion>(context);\n-    patterns.add<SimplifyReduceCvt>(context);\n-    patterns.add<RematerializeBackward>(context);\n-    patterns.add<RematerializeForward>(context);\n-    patterns.add<MoveConvertOutOfLoop>(context);\n-    patterns.add<MoveConvertOutOfIf>(context);\n-    patterns.add<DecomposeDotOperand>(context);\n-    patterns.add<ConvertDotConvert>(context);\n+    mlir::RewritePatternSet decomposePatterns(context);\n+    decomposePatterns.add<DecomposeDotOperand>(context);\n+    decomposePatterns.add<ConvertDotConvert>(context);\n+    if (mlir::applyPatternsAndFoldGreedily(m, std::move(decomposePatterns))\n+            .failed()) {\n+      signalPassFailure();\n+    }\n \n-    if (mlir::applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n+    // 4. Apply clean up patterns to remove remove dead convert and dead code\n+    // generated by the previous transformations.\n+    mlir::RewritePatternSet cleanUpPatterns2(context);\n+    populateForOpDeadArgumentElimination(cleanUpPatterns2);\n+    scf::ForOp::getCanonicalizationPatterns(cleanUpPatterns2, context);\n+    ConvertLayoutOp::getCanonicalizationPatterns(cleanUpPatterns2, context);\n+    if (mlir::applyPatternsAndFoldGreedily(m, std::move(cleanUpPatterns2))\n+            .failed()) {\n       signalPassFailure();\n     }\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 215, "deletions": 286, "changes": 501, "file_content_changes": "@@ -240,30 +240,59 @@ std::string GraphLayoutMarker::getColor(const Type &type) const {\n }\n // -------------------------------------------------------------------------- //\n \n-// TODO: Interface\n-LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n-                             Attribute &ret) {\n-  ret = targetEncoding;\n-  if (auto expand_dims = dyn_cast<triton::ExpandDimsOp>(op)) {\n-    ret = triton::gpu::SliceEncodingAttr::get(\n-        op->getContext(), expand_dims.getAxis(), targetEncoding);\n-  }\n-  if (auto reduce = dyn_cast<triton::ReduceOp>(op)) {\n-    auto sliceEncoding =\n-        targetEncoding.dyn_cast<triton::gpu::SliceEncodingAttr>();\n-    if (!sliceEncoding)\n-      return failure();\n-    if (sliceEncoding.getDim() != reduce.getAxis())\n-      return failure();\n-    ret = sliceEncoding.getParent();\n-  }\n-  if (isa<triton::ViewOp, triton::CatOp>(op)) {\n-    return failure();\n-  }\n-  return success();\n+static std::optional<Attribute> inferDstEncoding(triton::ReduceOp op,\n+                                                 Attribute encoding) {\n+  return triton::gpu::SliceEncodingAttr::get(op->getContext(), op.getAxis(),\n+                                             encoding);\n+}\n+\n+static std::optional<Attribute> inferDstEncoding(triton::ExpandDimsOp op,\n+                                                 Attribute encoding) {\n+  auto sliceEncoding = encoding.dyn_cast<triton::gpu::SliceEncodingAttr>();\n+  if (!sliceEncoding)\n+    return std::nullopt;\n+  if (op.getAxis() != sliceEncoding.getDim())\n+    return std::nullopt;\n+  return sliceEncoding.getParent();\n+}\n+\n+static std::optional<Attribute> inferSrcEncoding(triton::ReduceOp op,\n+                                                 Attribute encoding) {\n+  auto sliceEncoding = encoding.dyn_cast<triton::gpu::SliceEncodingAttr>();\n+  if (!sliceEncoding)\n+    return std::nullopt;\n+  if (op.getAxis() != sliceEncoding.getDim())\n+    return std::nullopt;\n+  return sliceEncoding.getParent();\n }\n \n-bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n+static std::optional<Attribute> inferSrcEncoding(triton::ExpandDimsOp op,\n+                                                 Attribute encoding) {\n+  return triton::gpu::SliceEncodingAttr::get(op->getContext(), op.getAxis(),\n+                                             encoding);\n+}\n+\n+std::optional<Attribute> inferSrcEncoding(Operation *op, Attribute encoding) {\n+  if (auto reduceOp = dyn_cast<triton::ReduceOp>(op))\n+    return inferSrcEncoding(reduceOp, encoding);\n+  if (auto expand = dyn_cast<triton::ExpandDimsOp>(op))\n+    return inferSrcEncoding(expand, encoding);\n+  if (isa<triton::ViewOp, triton::CatOp>(op))\n+    return std::nullopt;\n+  return encoding;\n+}\n+\n+std::optional<Attribute> inferDstEncoding(Operation *op, Attribute encoding) {\n+  if (auto reduceOp = dyn_cast<triton::ReduceOp>(op))\n+    return inferDstEncoding(reduceOp, encoding);\n+  if (auto expand = dyn_cast<triton::ExpandDimsOp>(op))\n+    return inferDstEncoding(expand, encoding);\n+  if (isa<triton::ViewOp, triton::CatOp>(op))\n+    return std::nullopt;\n+  return encoding;\n+}\n+\n+bool isExpensiveLoadOrStore(Operation *op) {\n   // Case 1: Pointer of tensor is always expensive\n   auto operandType = op->getOperand(0).getType();\n   if (triton::isTensorPointerType(operandType))\n@@ -287,7 +316,7 @@ bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   if (!op)\n     return true;\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n-    return isExpensiveLoadOrStore(op, targetEncoding);\n+    return isExpensiveLoadOrStore(op);\n   if (isa<triton::CatOp>(op))\n     return triton::gpu::isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n@@ -300,75 +329,21 @@ bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   return false;\n }\n \n-bool canFoldConversion(Operation *op, Attribute targetEncoding) {\n+bool canFoldIntoConversion(Operation *op, Attribute targetEncoding) {\n   if (isa<triton::CatOp>(op))\n     return !triton::gpu::isExpensiveCat(cast<triton::CatOp>(op),\n                                         targetEncoding);\n-  return isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n-             triton::MakeRangeOp, triton::SplatOp, triton::ViewOp>(op);\n-}\n-\n-int simulateBackwardRematerialization(\n-    Operation *initOp, SetVector<Operation *> &processed,\n-    SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n-    Attribute targetEncoding) {\n-  // DFS\n-  std::vector<std::pair<Operation *, Attribute>> queue;\n-  queue.emplace_back(initOp, targetEncoding);\n-  // We want to see the effect of converting `initOp` to a new layout\n-  // so we initialize `numCvts = 1`.\n-  int numCvts = 1;\n-  while (!queue.empty()) {\n-    Operation *currOp;\n-    Attribute currLayout;\n-    std::tie(currOp, currLayout) = queue.back();\n-    queue.pop_back();\n-    // If the current operation is expensive to rematerialize,\n-    // we stop everything\n-    if (isExpensiveToRemat(currOp, currLayout))\n-      break;\n-    // A conversion will be removed here (i.e. transferred to operands)\n-    numCvts -= 1;\n-    // Done processing\n-    processed.insert(currOp);\n-    layout.insert(currLayout);\n-    // Add all operands to the queue\n-    for (Value argI : currOp->getOperands()) {\n-      Attribute newEncoding;\n-      // Cannot invert the current encoding for this operand\n-      // we stop everything\n-      if (failed(invertEncoding(currLayout, currOp, newEncoding)))\n-        return INT_MAX;\n-      if (toConvert.count(argI) && toConvert[argI] != newEncoding)\n-        return INT_MAX;\n-      if (auto ptrTy = argI.getType().dyn_cast<triton::PointerType>()) {\n-        if (ptrTy.getPointeeType().isa<RankedTensorType>()) {\n-          return INT_MAX;\n-        }\n-      }\n-\n-      Operation *opArgI = argI.getDefiningOp();\n-      toConvert.insert({argI, newEncoding});\n-      // 1. Only convert RankedTensorType\n-      // 2. Skip if there's no defining op\n-      // 3. Skip if the defining op has already been processed\n-      // 4. Skip or the defining op is in a different block\n-      if (!argI.getType().isa<RankedTensorType>() || !opArgI ||\n-          processed.contains(opArgI) ||\n-          opArgI->getBlock() != currOp->getBlock())\n-        continue;\n-      // If the conversion can be folded into opArgI then\n-      // we don't count this conversion as expensive\n-      if (canFoldConversion(opArgI, newEncoding))\n-        continue;\n-\n-      // We add one expensive conversion for the current operand\n-      numCvts += 1;\n-      queue.emplace_back(opArgI, newEncoding);\n+  if (auto convert = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n+    if (targetEncoding.isa<triton::gpu::MmaEncodingAttr>()) {\n+      auto srcEncoding =\n+          convert.getOperand().getType().cast<RankedTensorType>().getEncoding();\n+      if (targetEncoding != srcEncoding)\n+        return false;\n     }\n+    return true;\n   }\n-  // return net number of conversions\n-  return numCvts;\n+  return isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n+             triton::MakeRangeOp, triton::SplatOp, triton::ViewOp>(op);\n }\n \n //\n@@ -409,213 +384,54 @@ Operation *cloneWithInferType(mlir::OpBuilder &rewriter, Operation *op,\n   return newOp;\n }\n \n-namespace {\n-\n-struct OpUseInfo {\n-  Value value;\n-  Operation *op;\n-  unsigned index;\n-};\n-\n-void getForwardSliceOpUseInfo(Operation *op,\n-                              SetVector<Operation *> *forwardSliceOps,\n-                              SmallVector<OpUseInfo> *forwardOpUseInfo) {\n-  if (!op)\n-    return;\n-\n-  for (Region &region : op->getRegions())\n-    for (Block &block : region)\n-      for (Operation &blockOp : block)\n-        if (forwardSliceOps->count(&blockOp) == 0)\n-          getForwardSliceOpUseInfo(&blockOp, forwardSliceOps, forwardOpUseInfo);\n-  for (Value result : op->getResults()) {\n-    for (OpOperand &operand : result.getUses()) {\n-      auto *blockOp = operand.getOwner();\n-      forwardOpUseInfo->push_back(\n-          {operand.get(), blockOp, operand.getOperandNumber()});\n-      if (forwardSliceOps->count(blockOp) == 0)\n-        getForwardSliceOpUseInfo(blockOp, forwardSliceOps, forwardOpUseInfo);\n-    }\n-  }\n-\n-  forwardSliceOps->insert(op);\n-}\n-} // namespace\n-\n-LogicalResult simulateForwardRematerializationInLoop(Operation *startOp,\n-                                                     BlockArgument arg,\n-                                                     Attribute targetEncoding) {\n-  // heuristics for flash attention\n-  if (targetEncoding.isa<triton::gpu::SharedEncodingAttr>())\n-    return failure();\n-  SetVector<Operation *> cvtSliceOps;\n-  SmallVector<OpUseInfo> cvtSliceOpUseInfo;\n-  getForwardSliceOpUseInfo(startOp, &cvtSliceOps, &cvtSliceOpUseInfo);\n-\n-  // Check if any additional conversion is needed along the way\n-  for (Operation *op : cvtSliceOps) {\n-    if (isa<scf::YieldOp>(op))\n+LogicalResult\n+getConvertBackwardSlice(Value root, SetVector<Value> &slice,\n+                        Attribute rootEncoding,\n+                        DenseMap<Value, Attribute> &layout,\n+                        std::function<bool(Operation *)> stopPropagation) {\n+  SmallVector<std::pair<Value, Attribute>> queue = {{root, rootEncoding}};\n+  while (!queue.empty()) {\n+    auto [currentValue, encoding] = queue.back();\n+    queue.pop_back();\n+    if (!currentValue.getType().isa<RankedTensorType>())\n       continue;\n-    // The first op doesn't push forward any conversion\n-    if (op != startOp) {\n-      if (isa<triton::ReduceOp>(op) &&\n-          !op->getResult(0).getType().isa<RankedTensorType>())\n-        return failure();\n-      // don't rematerialize anything expensive\n-      if (isExpensiveToRemat(op, targetEncoding))\n-        return failure();\n-      // don't rematerialize non-element-wise\n-      if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n-          !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n-          !isa<triton::StoreOp, triton::AssertOp, triton::PrintOp,\n-               triton::ReduceOp>(op))\n-        return failure();\n-    }\n-    // don't rematerialize if it adds an extra conversion that can't\n-    // be removed\n-    for (Value value : op->getOperands()) {\n-      Operation *argOp = arg.getDefiningOp();\n-      SetVector<Operation *> processed;\n-      SetVector<Attribute> layout;\n-      llvm::MapVector<Value, Attribute> toConvert;\n-      int numAddedConvs = simulateBackwardRematerialization(\n-          argOp, processed, layout, toConvert, targetEncoding);\n-      if (argOp && !isa<triton::gpu::ConvertLayoutOp>(argOp) &&\n-          cvtSliceOps.count(argOp) == 0 && numAddedConvs > 0)\n+    // Skip propagating through for op results for now.\n+    // TODO: enable this based on needs.\n+    if (currentValue.getDefiningOp<scf::ForOp>())\n+      return failure();\n+    slice.insert(currentValue);\n+    layout[currentValue] = encoding;\n+    if (auto *definingOp = currentValue.getDefiningOp()) {\n+      if (canFoldIntoConversion(definingOp, encoding))\n+        continue;\n+      if (stopPropagation && stopPropagation(definingOp))\n+        continue;\n+      if (isa<triton::CatOp>(definingOp))\n         return failure();\n-    }\n-  }\n-\n-  // We apply conservative analysis. Only when the final operand's index\n-  // matches the argument's index or their encoding match, we can rematerialize.\n-  for (auto &opUseInfo : cvtSliceOpUseInfo) {\n-    Operation *op = opUseInfo.op;\n-    if (isa<scf::YieldOp>(op)) {\n-      auto yieldIdx = opUseInfo.index;\n-      // 0 is the induction variable\n-      auto argIdx = arg.getArgNumber() - 1;\n-      if (yieldIdx != argIdx) {\n-        auto argType = arg.getType().cast<RankedTensorType>();\n-        auto yieldType =\n-            op->getOperand(yieldIdx).getType().dyn_cast<RankedTensorType>();\n-        if (!yieldType || argType.getEncoding() != yieldType.getEncoding())\n+      for (Value operand : definingOp->getOperands()) {\n+        auto srcEncoding = inferSrcEncoding(definingOp, encoding);\n+        if (!srcEncoding)\n           return failure();\n+        if (slice.count(operand) == 0)\n+          queue.push_back({operand, *srcEncoding});\n       }\n+      continue;\n     }\n-  }\n-  return success();\n-}\n-\n-void rematerializeConversionChain(\n-    const llvm::MapVector<Value, Attribute> &toConvert,\n-    mlir::PatternRewriter &rewriter, SetVector<Operation *> &processed,\n-    IRMapping &mapping) {\n-  SmallVector<Value, 4> sortedValues;\n-  SetVector<Operation *> tmp;\n-  for (auto &item : toConvert) {\n-    Value v = item.first;\n-    if (v.getDefiningOp())\n-      tmp.insert(v.getDefiningOp());\n-    else\n-      sortedValues.push_back(v);\n-  }\n-  tmp = mlir::multiRootTopologicalSort(tmp);\n-  for (Operation *op : tmp)\n-    sortedValues.push_back(op->getResult(0));\n-\n-  for (Value currOperand : sortedValues) {\n-    Value origOperand = currOperand;\n-    // unpack information\n-    Attribute targetLayout = toConvert.lookup(currOperand);\n-    // rematerialize the operand if necessary\n-    Operation *currOperation = currOperand.getDefiningOp();\n-    if (processed.contains(currOperation)) {\n-      Operation *newOperation =\n-          cloneWithInferType(rewriter, currOperation, mapping);\n-      newOperation->moveAfter(currOperation);\n-      currOperation = newOperation;\n-      currOperand = currOperation->getResult(0);\n-    }\n-    // compute target type for the layout cast\n-    auto currType = currOperand.getType().cast<RankedTensorType>();\n-    auto newType = RankedTensorType::get(\n-        currType.getShape(), currType.getElementType(), targetLayout);\n-    auto newOperand = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        currOperand.getLoc(), newType, currOperand);\n-    if (currOperation)\n-      newOperand->moveAfter(currOperation);\n-    else {\n-      Block *block = currOperand.cast<BlockArgument>().getOwner();\n-      newOperand->moveBefore(block, block->begin());\n+    auto blockArg = cast<BlockArgument>(currentValue);\n+    Block *block = blockArg.getOwner();\n+    Operation *parentOp = block->getParentOp();\n+    if (auto forOp = dyn_cast<scf::ForOp>(parentOp)) {\n+      OpOperand &initOperand = forOp.getOpOperandForRegionIterArg(blockArg);\n+      Value yieldOperand = forOp.getBody()->getTerminator()->getOperand(\n+          blockArg.getArgNumber() - forOp.getNumInductionVars());\n+      queue.push_back({initOperand.get(), encoding});\n+      queue.push_back({yieldOperand, encoding});\n+      continue;\n     }\n-    mapping.map(origOperand, newOperand);\n-  }\n-}\n-\n-LogicalResult canMoveOutOfLoop(BlockArgument arg,\n-                               SmallVector<Operation *> &cvts) {\n-  auto parentOp = arg.getOwner()->getParentOp();\n-  // Don't move if arg is defined in a while loop\n-  if (isa<scf::WhileOp>(parentOp))\n+    // TODO: add support for WhileOp and other region types.\n     return failure();\n-  // Skip if arg is not defined in scf.for\n-  if (!isa<scf::ForOp>(parentOp))\n-    return success();\n-  auto forOp = cast<scf::ForOp>(parentOp);\n-  // We only move `iterArg` out of the loop if\n-  // 1. There is no conversion\n-  // 2. There is only a single conversion\n-  // 3. Moving this conversion out of the loop will not generate any extra\n-  // non-removable conversion\n-  SetVector<RankedTensorType> cvtTypes;\n-  SetVector<Operation *> others;\n-  auto oldType = arg.getType().cast<RankedTensorType>();\n-  for (auto user : arg.getUsers()) {\n-    if (isa<triton::gpu::ConvertLayoutOp>(user)) {\n-      // Don't move if the conversion target is a dot operand or shared memory\n-      auto newType = user->getResults()[0].getType().cast<RankedTensorType>();\n-      if (oldType.getEncoding().isa<triton::gpu::SharedEncodingAttr>() &&\n-          newType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>()) {\n-        continue;\n-      }\n-      if (newType.getEncoding().isa<triton::gpu::SharedEncodingAttr>()) {\n-        if (newType.getEncoding()\n-                .cast<triton::gpu::SharedEncodingAttr>()\n-                .getVec() == 1)\n-          continue;\n-      }\n-      cvts.emplace_back(user);\n-      cvtTypes.insert(newType);\n-    } else\n-      others.insert(user);\n   }\n-  // First condition\n-  if (cvts.empty())\n-    return success();\n-  if (cvtTypes.size() == 1) {\n-    // Third condition - part 1:\n-    // If the other or the cvt is in the different block, we cannot push the\n-    // conversion forward or backward\n-    for (auto *cvt : cvts) {\n-      if (cvt->getBlock() != forOp.getBody())\n-        return failure();\n-    }\n-    auto targetEncoding = cvtTypes.front().getEncoding();\n-    for (auto *other : others) {\n-      // Third condition - part 2:\n-      // If the other non-cvt op is in the different block, we cannot push the\n-      // conversion forward or backward\n-      if (other->getBlock() != forOp.getBody())\n-        return failure();\n-      // Third condition - part 3:\n-      // Check if we can directly use arg without conversion\n-      if (simulateForwardRematerializationInLoop(other, arg, targetEncoding)\n-              .failed())\n-        return failure();\n-    }\n-    return success();\n-  }\n-  return failure();\n+  return success();\n }\n \n // TODO(thomas): this is duplicated with what is in GPUToLLVM\n@@ -700,4 +516,117 @@ void setRoleId(Operation *op, int roleId) {\n   op->setAttr(\"agent.mutex_role\", attr);\n }\n \n+namespace {\n+\n+/// Detect dead arguments in scf.for op by assuming all the values are dead and\n+/// propagate liveness property.\n+struct ForOpDeadArgElimination : public OpRewritePattern<scf::ForOp> {\n+  using OpRewritePattern<scf::ForOp>::OpRewritePattern;\n+\n+  LogicalResult matchAndRewrite(scf::ForOp forOp,\n+                                PatternRewriter &rewriter) const final {\n+    Block &block = *forOp.getBody();\n+    auto yieldOp = cast<scf::YieldOp>(block.getTerminator());\n+    // Assume that nothing is live at the beginning and mark values as live\n+    // based on uses.\n+    DenseSet<Value> aliveValues;\n+    SmallVector<Value> queue;\n+    // Helper to mark values as live and add them to the queue of value to\n+    // propagate if it is the first time we detect the value as live.\n+    auto markLive = [&](Value val) {\n+      if (!forOp->isAncestor(val.getParentRegion()->getParentOp()))\n+        return;\n+      if (aliveValues.insert(val).second)\n+        queue.push_back(val);\n+    };\n+    // Mark all yield operands as live if the associated forOp result has any\n+    // use.\n+    for (auto result : llvm::enumerate(forOp.getResults())) {\n+      if (!result.value().use_empty())\n+        markLive(yieldOp.getOperand(result.index()));\n+    }\n+    if (aliveValues.size() == forOp.getNumResults())\n+      return failure();\n+    // Operations with side-effects are always live. Mark all theirs operands as\n+    // live.\n+    block.walk([&](Operation *op) {\n+      if (!isa<scf::YieldOp, scf::ForOp>(op) && !wouldOpBeTriviallyDead(op)) {\n+        for (Value operand : op->getOperands())\n+          markLive(operand);\n+      }\n+    });\n+    // Propagate live property until reaching a fixed point.\n+    while (!queue.empty()) {\n+      Value value = queue.pop_back_val();\n+      if (auto nestedFor = value.getDefiningOp<scf::ForOp>()) {\n+        auto result = value.cast<OpResult>();\n+        OpOperand &forOperand = nestedFor.getOpOperandForResult(result);\n+        markLive(forOperand.get());\n+        auto nestedYieldOp =\n+            cast<scf::YieldOp>(nestedFor.getBody()->getTerminator());\n+        Value nestedYieldOperand =\n+            nestedYieldOp.getOperand(result.getResultNumber());\n+        markLive(nestedYieldOperand);\n+        continue;\n+      }\n+      if (auto nestedIf = value.getDefiningOp<scf::IfOp>()) {\n+        auto result = value.cast<OpResult>();\n+        for (scf::YieldOp nestedYieldOp :\n+             {nestedIf.thenYield(), nestedIf.elseYield()}) {\n+          Value nestedYieldOperand =\n+              nestedYieldOp.getOperand(result.getResultNumber());\n+          markLive(nestedYieldOperand);\n+        }\n+        continue;\n+      }\n+      if (Operation *def = value.getDefiningOp()) {\n+        // TODO: support while ops.\n+        if (isa<scf::WhileOp>(def))\n+          return failure();\n+        for (Value operand : def->getOperands())\n+          markLive(operand);\n+        continue;\n+      }\n+      // If an argument block is live then the associated yield operand and\n+      // forOp operand are live.\n+      auto arg = value.cast<BlockArgument>();\n+      if (auto forOwner = dyn_cast<scf::ForOp>(arg.getOwner()->getParentOp())) {\n+        if (arg.getArgNumber() < forOwner.getNumInductionVars())\n+          continue;\n+        unsigned iterIdx = arg.getArgNumber() - forOwner.getNumInductionVars();\n+        Value yieldOperand =\n+            forOwner.getBody()->getTerminator()->getOperand(iterIdx);\n+        markLive(yieldOperand);\n+        markLive(forOwner.getIterOperands()[iterIdx]);\n+      }\n+    }\n+    SmallVector<unsigned> deadArg;\n+    for (auto yieldOperand : llvm::enumerate(yieldOp->getOperands())) {\n+      if (aliveValues.contains(yieldOperand.value()))\n+        continue;\n+      if (yieldOperand.value() == block.getArgument(yieldOperand.index() + 1))\n+        continue;\n+      deadArg.push_back(yieldOperand.index());\n+    }\n+    if (deadArg.empty())\n+      return failure();\n+    rewriter.updateRootInPlace(forOp, [&]() {\n+      // For simplicity we just change the dead yield operand to use the\n+      // associated argument and leave the operations and argument removal to\n+      // dead code elimination.\n+      for (unsigned deadArgIdx : deadArg) {\n+        BlockArgument arg = block.getArgument(deadArgIdx + 1);\n+        yieldOp.setOperand(deadArgIdx, arg);\n+      }\n+    });\n+    return success();\n+  }\n+};\n+\n+} // namespace\n+\n+void populateForOpDeadArgumentElimination(RewritePatternSet &patterns) {\n+  patterns.add<ForOpDeadArgElimination>(patterns.getContext());\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Target/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,3 +1,2 @@\n add_subdirectory(LLVMIR)\n add_subdirectory(PTX)\n-add_subdirectory(HSACO)"}, {"filename": "lib/Target/HSACO/CMakeLists.txt", "status": "removed", "additions": 0, "deletions": 9, "changes": 9, "file_content_changes": "@@ -1,9 +0,0 @@\n-add_mlir_translation_library(TritonHSACO\n-        HSACOTranslation.cpp\n-\n-        LINK_COMPONENTS\n-        Core\n-\n-        LINK_LIBS PUBLIC\n-        TritonLLVMIR\n-        )"}, {"filename": "lib/Target/HSACO/HSACOTranslation.cpp", "status": "removed", "additions": 0, "deletions": 182, "changes": 182, "file_content_changes": "@@ -1,182 +0,0 @@\n-#include \"triton/Target/HSACO/HSACOTranslation.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"mlir/ExecutionEngine/ExecutionEngine.h\"\n-#include \"mlir/ExecutionEngine/OptUtils.h\"\n-#include \"mlir/IR/BuiltinOps.h\"\n-#include \"mlir/IR/Dialect.h\"\n-#include \"mlir/Pass/Pass.h\"\n-#include \"mlir/Pass/PassManager.h\"\n-#include \"mlir/Support/LogicalResult.h\"\n-#include \"mlir/Target/LLVMIR/Dialect/LLVMIR/LLVMToLLVMIRTranslation.h\"\n-#include \"mlir/Target/LLVMIR/Export.h\"\n-#include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n-#include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n-#include \"triton/Tools/Sys/GetEnv.hpp\"\n-\n-#include \"llvm/ExecutionEngine/ExecutionEngine.h\"\n-#include \"llvm/ExecutionEngine/SectionMemoryManager.h\"\n-#include \"llvm/IR/IRBuilder.h\"\n-#include \"llvm/IR/IRPrintingPasses.h\"\n-#include \"llvm/IR/LegacyPassManager.h\"\n-#include \"llvm/IR/Module.h\"\n-#include \"llvm/IR/Verifier.h\"\n-#include \"llvm/MC/TargetRegistry.h\"\n-#include \"llvm/Support/CodeGen.h\"\n-#include \"llvm/Support/CommandLine.h\"\n-#include \"llvm/Support/SourceMgr.h\"\n-#include \"llvm/Support/TargetSelect.h\"\n-#include \"llvm/Support/raw_ostream.h\"\n-#include \"llvm/Target/TargetMachine.h\"\n-#include \"llvm/Target/TargetOptions.h\"\n-#include \"llvm/Transforms/Scalar.h\"\n-#include \"llvm/Transforms/Utils/Cloning.h\"\n-#include <filesystem>\n-#include <iostream>\n-#include <memory>\n-#include <random>\n-\n-namespace {\n-\n-void init_llvm() {\n-  LLVMInitializeAMDGPUTarget();\n-  LLVMInitializeAMDGPUTargetInfo();\n-  LLVMInitializeAMDGPUTargetMC();\n-  LLVMInitializeAMDGPUAsmParser();\n-  LLVMInitializeAMDGPUAsmPrinter();\n-}\n-\n-std::unique_ptr<llvm::TargetMachine>\n-initialize_module(llvm::Module *module, const std::string &triple,\n-                  const std::string &proc, const std::string &features) {\n-  // verify and store llvm\n-  llvm::legacy::PassManager pm;\n-  pm.add(llvm::createVerifierPass());\n-  pm.run(*module);\n-\n-  module->setTargetTriple(triple);\n-\n-  std::string error;\n-  auto target =\n-      llvm::TargetRegistry::lookupTarget(module->getTargetTriple(), error);\n-  llvm::TargetOptions opt;\n-  opt.AllowFPOpFusion = llvm::FPOpFusion::Fast;\n-  opt.UnsafeFPMath = false;\n-  opt.NoInfsFPMath = false;\n-  opt.NoNaNsFPMath = true;\n-  llvm::TargetMachine *machine = target->createTargetMachine(\n-      module->getTargetTriple(), proc, features, opt, llvm::Reloc::PIC_,\n-      std::nullopt, llvm::CodeGenOpt::Aggressive);\n-\n-  module->setDataLayout(machine->createDataLayout());\n-\n-  for (llvm::Function &f : module->functions())\n-    f.addFnAttr(llvm::Attribute::AlwaysInline);\n-\n-  return std::unique_ptr<llvm::TargetMachine>(machine);\n-}\n-\n-std::string generate_amdgcn_assembly(llvm::Module *module,\n-                                     const std::string &triple,\n-                                     const std::string &proc,\n-                                     const std::string &features) {\n-  auto machine = initialize_module(module, triple, proc, features);\n-  llvm::SmallVector<char, 0> buffer;\n-  llvm::legacy::PassManager pass;\n-  llvm::raw_svector_ostream stream(buffer);\n-\n-  // emit\n-  machine->addPassesToEmitFile(pass, stream, nullptr,\n-                               llvm::CodeGenFileType::CGFT_AssemblyFile);\n-  pass.run(*module);\n-\n-  std::string amdgcn(buffer.begin(), buffer.end());\n-  if (::triton::tools::getBoolEnv(\"AMDGCN_ENABLE_DUMP\")) {\n-    std::cout << \"// -----// AMDGCN Dump //----- //\\n\" << amdgcn << std::endl;\n-  }\n-\n-  return amdgcn;\n-}\n-\n-std::string generate_hsaco(llvm::Module *module, const std::string &triple,\n-                           const std::string &proc,\n-                           const std::string &features) {\n-  auto machine = initialize_module(module, triple, proc, features);\n-\n-  // create unique dir for kernel's binary and hsaco\n-  std::error_code ec;\n-  std::string kernel_name_base = \"amd_triton_kernel\";\n-  std::filesystem::path tmp = std::filesystem::temp_directory_path();\n-  std::filesystem::path kernel_dir_base(kernel_name_base);\n-  llvm::SmallString<256> unique_dir;\n-  ec = llvm::sys::fs::createUniqueDirectory((tmp / kernel_dir_base).string(),\n-                                            unique_dir);\n-  if (ec) {\n-    std::cerr << \"Directory for \" << kernel_name_base\n-              << \" was not created. error code: \" << ec << std::endl;\n-  }\n-  std::filesystem::path kernel_dir(unique_dir.data());\n-  std::string kernel_name = kernel_dir.stem();\n-\n-  // Save GCN ISA binary.\n-  std::filesystem::path isa_binary(kernel_name + \".o\");\n-  std::string isabin_path = (kernel_dir / isa_binary).string();\n-  std::unique_ptr<llvm::raw_fd_ostream> isabin_fs(\n-      new llvm::raw_fd_ostream(isabin_path, ec, llvm::sys::fs::OF_Text));\n-  if (ec) {\n-    std::cerr << isabin_path << \" was not created. error code: \" << ec\n-              << std::endl;\n-  }\n-\n-  // emit\n-  llvm::legacy::PassManager pass;\n-  machine->addPassesToEmitFile(pass, *isabin_fs, nullptr,\n-                               llvm::CGFT_ObjectFile);\n-  pass.run(*module);\n-\n-  // generate HASCO file\n-  std::filesystem::path hsaco(kernel_name + \".hsaco\");\n-  std::string hsaco_path = (kernel_dir / hsaco).string();\n-  std::string error_message;\n-  std::string lld_path = \"/opt/rocm/llvm/bin/ld.lld\";\n-  int lld_result = llvm::sys::ExecuteAndWait(\n-      lld_path,\n-      {lld_path, \"-flavor\", \"gnu\", \"-shared\", \"-o\", hsaco_path, isabin_path},\n-      std::nullopt, {}, 0, 0, &error_message);\n-  if (lld_result) {\n-    std::cout << \"ld.lld execute fail: \" << std::endl;\n-    std::cout << error_message << std::endl;\n-    std::cout << lld_result << std::endl;\n-  }\n-\n-  return hsaco_path;\n-}\n-\n-std::tuple<std::string, std::string>\n-llir_to_amdgcn_and_hsaco(llvm::Module *module, std::string gfx_arch,\n-                         std::string gfx_triple, std::string gfx_features) {\n-\n-  init_llvm();\n-\n-  // verify and store llvm\n-  auto module_obj = llvm::CloneModule(*module);\n-  auto amdgcn =\n-      generate_amdgcn_assembly(module, gfx_triple, gfx_arch, gfx_features);\n-  auto hsaco_path =\n-      generate_hsaco(module_obj.get(), gfx_triple, gfx_arch, gfx_features);\n-\n-  return std::make_tuple(amdgcn, hsaco_path);\n-}\n-\n-} // namespace\n-\n-namespace triton {\n-\n-std::tuple<std::string, std::string>\n-translateLLVMIRToHSACO(llvm::Module &module, std::string gfx_arch,\n-                       std::string gfx_triple, std::string gfx_features) {\n-  auto hsacoCode =\n-      llir_to_amdgcn_and_hsaco(&module, gfx_arch, gfx_triple, gfx_features);\n-  return hsacoCode;\n-}\n-\n-} // namespace triton"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 1, "deletions": 19, "changes": 20, "file_content_changes": "@@ -33,7 +33,6 @@\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n-#include \"triton/Target/HSACO/HSACOTranslation.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n #include \"triton/Target/PTX/PTXTranslation.h\"\n #include \"triton/Target/PTX/TmaMetadata.h\"\n@@ -257,6 +256,7 @@ void init_triton_ir(py::module &&m) {\n         // we load LLVM because the frontend uses LLVM.undef for\n         // some placeholders\n         self.getOrLoadDialect<mlir::LLVM::LLVMDialect>();\n+        self.getOrLoadDialect<mlir::tensor::TensorDialect>();\n       });\n   // .def(py::init([](){\n   //   mlir::MLIRContext context;\n@@ -1958,24 +1958,6 @@ void init_triton_translation(py::module &m) {\n            const std::vector<std::string> &paths) {\n           ::mlir::triton::addExternalLibs(op, names, paths);\n         });\n-\n-  m.def(\n-      \"translate_llvmir_to_hsaco\",\n-      [](const std::string llvmIR, std::string gfx_arch, std::string gfx_triple,\n-         std::string gfx_features) -> std::tuple<std::string, std::string> {\n-        // create LLVM module from C++\n-        llvm::LLVMContext context;\n-        std::unique_ptr<llvm::MemoryBuffer> buffer =\n-            llvm::MemoryBuffer::getMemBuffer(llvmIR.c_str());\n-        llvm::SMDiagnostic error;\n-        std::unique_ptr<llvm::Module> module =\n-            llvm::parseIR(buffer->getMemBufferRef(), error, context);\n-        // translate module to HSACO\n-        auto hsacoCode = triton::translateLLVMIRToHSACO(\n-            *module, gfx_arch, gfx_triple, gfx_features);\n-        return hsacoCode;\n-      },\n-      ret::take_ownership);\n }\n \n void init_triton(py::module &m) {"}, {"filename": "python/test/unit/hopper/test_gemm.py", "status": "modified", "additions": 87, "deletions": 76, "changes": 163, "file_content_changes": "@@ -211,102 +211,113 @@ def matmul_kernel(\n \n \n @pytest.mark.parametrize('BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,M,N,K,TRANS_A,TRANS_B,TRANS_OUTPUT,epilogue,out_dtype,USE_TMA_STORE,NUM_STAGES,ENABLE_WS',\n-                         [(128, 128, 64, 4, 1, *shape_w_c, 'none', out_dtype, use_tma_store, 3, enable_ws)\n-                          for shape_w_c in [\n-                             # badcase from cublas-important-layers\n-                             [4096, 1, 1024, False, False, True],\n-                             [2048, 204, 1000, True, False, True],\n-                             [4096, 1, 1024, False, False, False],\n-                             [2048, 204, 1000, True, False, False],\n-                         ]\n+                         [\n+                             # corner shapes\n+                             (128, 128, 64, 4, 1, *shape_w_c, 'none', out_dtype, use_tma_store, 3, enable_ws)\n+                             for shape_w_c in [\n+                                 [4096, 1, 1024, False, False, True],\n+                                 [2048, 204, 1000, True, False, True],\n+                                 [4096, 1, 1024, False, False, False],\n+                                 [2048, 204, 1000, True, False, False],\n+                             ]\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n                              for enable_ws in [False, True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n-                              # softmax works for one CTA\n-                              for shape_w_c in [\n-                             [64, 64, 16, 4, 1, 64, 64, 64],\n-                             [128, 128, 64, 4, 1, None, None, None],\n-                             [16, 16, 64, 4, 1, 16, 16, 64],\n-                             [64, 64, 32, 8, 1, 64, 64, 64],\n-                             [128, 128, 64, 4, 1, 128, 128, 128],\n-                         ]\n+                         ] + [\n+                             # softmax epilogue\n+                             (*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [64, 64, 16, 4, 1, 64, 64, 64],\n+                                 [128, 128, 64, 4, 1, None, None, None],\n+                                 [16, 16, 64, 4, 1, 16, 16, 64],\n+                                 [64, 64, 32, 8, 1, 64, 64, 64],\n+                                 [128, 128, 64, 4, 1, 128, 128, 128],\n+                             ]\n                              for epilogue in ['softmax']\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n-                             for trans_a in [False, True]\n-                             for trans_b in [False, True]\n-                             for trans_output in [False, True]\n+                             for trans_a in [False,]\n+                             for trans_b in [True,]\n+                             for trans_output in [False,]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n-                              for shape_w_c in [\n-                             [64, 64, 16, 4, 1, 128, 128, 64],\n-                             *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4, 8] for num_ctas in [1, 2, 4]],\n-                             # for chain-dot\n-                             [128, 128, 64, 4, 1, None, None, None],\n-                             [64, 64, 16, 4, 1, None, None, None],\n-                             # small BLOCK_M and BLOCK_K\n-                             [16, 16, 64, 4, 1, 128, 128, 64],\n-                             *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4, 8] for num_ctas in [1, 2]],\n-                             # repeat\n-                             [64, 64, 32, 8, 1, 128, 256, 64],\n-                             [64, 64, 16, 8, 2, 128, 128, 64],\n-                             # irregular shape\n-                             [128, 128, 64, 4, 1, 500, 200, 128],\n-                             [128, 128, 64, 4, 2, 513, 193, 192],\n-                         ]\n+                         ] + [\n+                             # loop over epilogues besides of softmax\n+                             (*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [64, 64, 16, 4, 1, 128, 128, 64],\n+                                 *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4, 8] for num_ctas in [1, 2, 4]],\n+                                 # for chain-dot\n+                                 [128, 128, 64, 4, 1, None, None, None],\n+                                 [64, 64, 16, 4, 1, None, None, None],\n+                                 # small BLOCK_M and BLOCK_K\n+                                 [16, 16, 64, 4, 1, 128, 128, 64],\n+                                 *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4, 8] for num_ctas in [1, 2]],\n+                                 # repeat\n+                                 [64, 64, 32, 8, 1, 128, 256, 64],\n+                                 [64, 64, 16, 8, 2, 128, 128, 64],\n+                                 # irregular shape\n+                                 [128, 128, 64, 4, 1, 500, 200, 128],\n+                                 [128, 128, 64, 4, 2, 513, 193, 192],\n+                             ]\n                              for epilogue in ['none', 'add-matrix', 'add-rows', 'add-cols', 'chain-dot']\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n-                             for trans_a in [False, True]\n-                             for trans_b in [False, True]\n-                             for trans_output in [False, True]\n+                             for trans_a in [False,]\n+                             for trans_b in [True,]\n+                             for trans_output in [False,]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n                              if not (epilogue == 'chain-dot' and (shape_w_c[6] is not None or shape_w_c[1] != shape_w_c[6]))\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n-                              for shape_w_c in [\n-                             [64, 64, 32, 4, 1, 128, 256, 64],\n-                             [128, 128, 16, 4, 4, 512, 256, 64],\n-                             [128, 256, 32, 4, 8, 256, 256, 192],\n-                             [512, 256, 32, 4, 8, 1024, 256, 192],\n-                             # BLOCK_K >= 128\n-                             [64, 128, 128, 4, 1, 512, 256, 256],\n-                             [128, 128, 128, 4, 1, 256, 256, 192],\n-                             [128, 128, 128, 4, 2, 256, 256, 192],\n-                             # small BLOCK_M and BLOCK_K\n-                             [16, 32, 32, 4, 1, 128, 256, 64],\n-                             [32, 32, 16, 4, 1, 256, 256, 192],\n-                             [16, 32, 64, 4, 4, 512, 256, 64],\n-                         ]\n-                             for out_dtype in ['float16', 'float32']\n-                             for use_tma_store in [False, True]\n+                         ] + [\n+                             # loop over tile shapes and transpose combinations\n+                             (*shape_w_c, trans_a, trans_b, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [64, 64, 32, 4, 1, 128, 256, 64],\n+                                 [128, 128, 16, 4, 4, 512, 256, 64],\n+                                 [128, 256, 32, 4, 8, 256, 256, 192],\n+                                 [512, 256, 32, 4, 8, 1024, 256, 192],\n+                                 # BLOCK_K >= 128\n+                                 [64, 128, 128, 4, 1, 512, 256, 256],\n+                                 [128, 128, 128, 4, 1, 256, 256, 192],\n+                                 [128, 128, 128, 4, 2, 256, 256, 192],\n+                                 # small BLOCK_M and BLOCK_K\n+                                 [16, 32, 32, 4, 1, 128, 256, 64],\n+                                 [32, 32, 16, 4, 1, 256, 256, 192],\n+                                 [16, 32, 64, 4, 4, 512, 256, 64],\n+                             ]\n+                             for out_dtype in ['float32',]\n+                             for use_tma_store in [False,]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n                              for trans_output in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                         ] + [(64, n, 16, 4, 1, 512, 256, 256, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n-                              # loop over instr shapes\n-                              for n in [16, 32, 64, 128, 256]\n-                              for trans_output in [False, True]\n-                              for out_dtype in ['float16', 'float32']\n-                              for use_tma_store in [False, True]\n-                              for num_stages in [2, 4, 5, 7]\n-                              for enable_ws in [False, True]\n-                              ] + [(*shape_w_c, *shape, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n-                                   # irregular shapes\n-                                   for shape_w_c in [\n-                                       [128, 128, 64, 4, 1],\n-                                       [256, 128, 64, 4, 2],\n-                                       [128, 128, 128, 4, 2],\n-                              ]\n-                             for shape in list(itertools.product([*range(512, 4096, 360)], [*range(512, 4096, 360)], [512, 1024]))\n-                             for trans_output in [False, True]\n-                             for out_dtype in ['float16', 'float32']\n+                         ] + [\n+                             # loop over instr shapes & pipeline stages\n+                             (64, n, 16, 4, 1, 512, 256, 256, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for n in [16, 32, 64, 128, 256]\n+                             for trans_output in [False,]\n+                             for out_dtype in ['float32',]\n+                             for use_tma_store in [False,]\n+                             for num_stages in [2, 4, 5, 7]\n+                             for enable_ws in [False, True]\n+                         ] + [\n+                             # irregular shapes\n+                             (*shape_w_c, *shape, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [128, 128, 64, 4, 1],\n+                                 [256, 128, 64, 4, 2],\n+                                 [128, 128, 128, 4, 2],\n+                             ]\n+                             for shape in [\n+                                 [512, 360, 1024],\n+                                 [360, 4096, 512],\n+                             ]\n+                             for trans_output in [False,]\n+                             for out_dtype in ['float32',]\n                              for use_tma_store in [False, True]\n-                             for num_stages in [2, 3, 4]\n+                             for num_stages in [3, 4]\n                              for enable_ws in [False, True]\n                          ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 39, "deletions": 32, "changes": 71, "file_content_changes": "@@ -696,9 +696,9 @@ def full_static_persistent_matmul_kernel(\n \n @pytest.mark.parametrize('BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,M,N,K,TRANS_A,TRANS_B,epilogue,out_dtype,USE_TMA_STORE,NUM_STAGES,ENABLE_WS',\n                          [\n+                             # corner shapes\n                              (128, 128, 64, 4, 1, *shape_w_c, 'none', out_dtype, use_tma_store, 3, enable_ws)\n                              for shape_w_c in [\n-                                 # bad from cublas-important-layers\n                                  [4096, 1, 1024, False, False],\n                                  [2048, 204, 1000, True, False],\n                                  [16, 524288, 32, False, True],\n@@ -707,6 +707,7 @@ def full_static_persistent_matmul_kernel(\n                              for use_tma_store in [False, True]\n                              for enable_ws in [True]\n                          ] + [\n+                             # softmax epilogue\n                              (*shape_w_c, trans_a, trans_b, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n                              # softmax works for one CTA\n                              for shape_w_c in [\n@@ -720,11 +721,12 @@ def full_static_persistent_matmul_kernel(\n                              for epilogue in ['softmax']\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n-                             for trans_a in [False, True]\n-                             for trans_b in [False, True]\n+                             for trans_a in [False,]\n+                             for trans_b in [True,]\n                              for num_stages in [3]\n                              for enable_ws in [True]\n                          ] + [\n+                             # loop over tile shapes and transpose combinations\n                              (*shape_w_c, trans_a, trans_b, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                              for shape_w_c in [\n                                  [64, 64, 32, 4, 1, 128, 256, 64],\n@@ -740,58 +742,63 @@ def full_static_persistent_matmul_kernel(\n                                  [32, 32, 16, 4, 1, 256, 256, 192],\n                                  [16, 32, 64, 4, 4, 512, 256, 64],\n                              ]\n-                             for out_dtype in ['float16', 'float32']\n-                             for use_tma_store in [False, True]\n+                             for out_dtype in ['float32',]\n+                             for use_tma_store in [False,]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n-                              for shape_w_c in [\n-                             [64, 64, 16, 4, 1, 128, 128, 64],\n-                             *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4] for num_ctas in [1, 2, 4]],\n-                             # for chain-dot\n-                             [128, 128, 64, 4, 1, None, None, None],\n-                             [64, 64, 16, 4, 1, None, None, None],\n-                             # small BLOCK_M and BLOCK_K\n-                             [16, 16, 64, 4, 1, 128, 128, 64],\n-                             *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4] for num_ctas in [1, 2]],\n-                             #  # TODO: enable when num_warps != 4 is supported.\n-                             #  # repeat\n-                             #  # [64, 64, 32, 8, 1, 128, 256, 64],\n-                             #  # [64, 64, 16, 8, 2, 128, 128, 64],\n-                             # irregular shape\n-                             [128, 128, 64, 4, 1, 500, 200, 128],\n-                             [128, 128, 64, 4, 1, 513, 193, 192],\n-                         ]\n+                         ] + [\n+                             # loop over epilogues besides of softmax\n+                             (*shape_w_c, trans_a, trans_b, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [64, 64, 16, 4, 1, 128, 128, 64],\n+                                 *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4] for num_ctas in [1, 2, 4]],\n+                                 # for chain-dot\n+                                 [128, 128, 64, 4, 1, None, None, None],\n+                                 [64, 64, 16, 4, 1, None, None, None],\n+                                 # small BLOCK_M and BLOCK_K\n+                                 [16, 16, 64, 4, 1, 128, 128, 64],\n+                                 *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4] for num_ctas in [1, 2]],\n+                                 #  # TODO: enable when num_warps != 4 is supported.\n+                                 #  # repeat\n+                                 #  # [64, 64, 32, 8, 1, 128, 256, 64],\n+                                 #  # [64, 64, 16, 8, 2, 128, 128, 64],\n+                                 # irregular shape\n+                                 [128, 128, 64, 4, 1, 500, 200, 128],\n+                                 [128, 128, 64, 4, 1, 513, 193, 192],\n+                             ]\n                              for epilogue in ['none', 'add-matrix', 'add-rows', 'add-cols', 'chain-dot']\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n-                             for trans_a in [False, True]\n-                             for trans_b in [False, True]\n+                             for trans_a in [False,]\n+                             for trans_b in [True,]\n                              for num_stages in [3]\n                              for enable_ws in [True]\n                              if not (epilogue == 'chain-dot' and (shape_w_c[5] is not None or shape_w_c[0] != shape_w_c[1]))\n                          ] + [\n+                             # loop over instr shapes & pipeline stages\n                              (64, n, 16, 4, 1, 512, 256, 256, False, True, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n-                             # loop over instr shapes\n                              for n in [16, 32, 64, 128, 256]\n-                             for out_dtype in ['float16', 'float32']\n-                             for use_tma_store in [False, True]\n+                             for out_dtype in ['float32']\n+                             for use_tma_store in [False,]\n                              for num_stages in [2, 4, 5, 7]\n                              for enable_ws in [True]\n                          ] + [\n-                             (*shape_w_c, *shape, False, True, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                              # irregular shapes\n+                             (*shape_w_c, *shape, False, True, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                              for shape_w_c in [\n                                  [128, 128, 64, 4, 1],\n                                  [256, 128, 64, 4, 2],\n                                  [128, 128, 128, 4, 2]\n                              ]\n-                             for shape in list(itertools.product([*range(512, 4096, 360)], [*range(512, 4096, 360)], [512, 1024]))\n-                             for out_dtype in ['float16', 'float32']\n+                             for shape in [\n+                                 [512, 360, 1024],\n+                                 [360, 4096, 512],\n+                             ]\n+                             for out_dtype in ['float32']\n                              for use_tma_store in [False, True]\n-                             for num_stages in [2, 3, 4]\n+                             for num_stages in [3, 4]\n                              for enable_ws in [True]\n                          ]\n                          )"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 182, "deletions": 58, "changes": 240, "file_content_changes": "@@ -12,6 +12,7 @@\n import triton\n import triton._C.libtriton.triton as _triton\n import triton.language as tl\n+from triton.common.build import is_hip\n from triton.runtime.jit import JITFunction, TensorWrapper, reinterpret\n \n int_dtypes = ['int8', 'int16', 'int32', 'int64']\n@@ -25,6 +26,13 @@\n # num_ctas_list = [1, 4] if torch.cuda.get_device_capability()[0] == 9 else [1]\n num_ctas_list = [1]\n \n+if is_hip():\n+    GPU_DIALECT = \"triton_gpu_rocm\"\n+    THREADS_PER_WARP = 64\n+else:\n+    GPU_DIALECT = \"triton_gpu\"\n+    THREADS_PER_WARP = 32\n+\n \n def _bitwidth(dtype: str) -> int:\n     # ex.: \"int64\" -> 64\n@@ -137,7 +145,7 @@ def __init__(self, version, warps_per_cta, ctas_per_cga, cta_split_num, cta_orde\n         self.instr_shape = str(instr_shape)\n \n     def __str__(self):\n-        return f\"#triton_gpu.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}, instrShape={self.instr_shape}}}>\"\n+        return f\"#{GPU_DIALECT}.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}, instrShape={self.instr_shape}}}>\"\n \n \n class BlockedLayout:\n@@ -151,7 +159,7 @@ def __init__(self, size_per_thread, threads_per_warp, warps_per_cta, order, ctas\n         self.cta_order = str(cta_order)\n \n     def __str__(self):\n-        return f\"#triton_gpu.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}}}>\"\n+        return f\"#{GPU_DIALECT}.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}}}>\"\n \n \n class SharedLayout:\n@@ -165,7 +173,7 @@ def __init__(self, vec, per_phase, max_phase, order, ctas_per_cga, cta_split_num\n         self.cta_order = str(cta_order)\n \n     def __str__(self):\n-        return f\"#triton_gpu.shared<{{vec={self.vec}, perPhase={self.per_phase}, maxPhase={self.max_phase}, order={self.order}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}}}>\"\n+        return f\"#{GPU_DIALECT}.shared<{{vec={self.vec}, perPhase={self.per_phase}, maxPhase={self.max_phase}, order={self.order}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}}}>\"\n \n \n @pytest.mark.parametrize(\"dtype_x\", list(dtypes) + [\"bfloat16\"])\n@@ -851,6 +859,8 @@ def test_abs(dtype_x, device):\n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4nv, tl.float8e5])\n def test_abs_fp8(in_dtype, device):\n+    if is_hip():\n+        pytest.skip('test_abs_fp8 not supported on HIP.')\n \n     @triton.jit\n     def abs_kernel(X, Z, SIZE: tl.constexpr):\n@@ -1056,6 +1066,9 @@ def noinline_multi_values_fn(x, y, Z):\n \n @pytest.mark.parametrize(\"mode\", [\"simple\", \"call_graph\", \"shared\", \"dynamic\", \"multi_values\"])\n def test_noinline(mode, device):\n+    if is_hip() and mode == \"shared\":\n+        pytest.skip('test_noinline[\"shared\"] not supported on HIP.')\n+\n     @triton.jit\n     def kernel(X, Y, Z):\n         x = tl.load(X)\n@@ -1141,6 +1154,9 @@ def kernel(X, Z):\n     else:\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n     sem_str = \"acq_rel\" if sem is None else sem\n+    if is_hip():\n+        return\n+\n     assert f\"atom.global.gpu.{sem_str}\" in h.asm[\"ptx\"]\n \n \n@@ -1232,6 +1248,8 @@ def serialized_add(data, Lock, SEM: tl.constexpr):\n     h = serialized_add[(64,)](data, Lock, SEM=sem, num_ctas=num_ctas)\n     sem_str = \"acq_rel\" if sem is None else sem\n     np.testing.assert_allclose(to_numpy(data), to_numpy(ref))\n+    if is_hip():\n+        return\n     assert f\"atom.global.{sem_str}\" in h.asm[\"ptx\"]\n \n \n@@ -1261,6 +1279,9 @@ def test_cast(dtype_x, dtype_z, bitcast, num_ctas, device):\n     check_type_supported(dtype_x, device)\n     check_type_supported(dtype_z, device)\n \n+    if is_hip() and (dtype_z == \"bfloat16\"):\n+        pytest.skip(f'test_cast{(dtype_x, dtype_z)} cast to bfloat16 not supported on HIP.')\n+\n     size = 1024\n     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n     if dtype_x.startswith('bfloat'):\n@@ -1358,7 +1379,10 @@ def kernel(in_out_ptr):\n \n     for _ in range(1000):\n         x = torch.ones((65536,), device=device, dtype=torch.float32)\n-        kernel[(65536,)](x, num_warps=32)\n+        if is_hip():\n+            kernel[(65536,)](x, num_warps=16)  # threads per Warp for ROCM is 64\n+        else:\n+            kernel[(65536,)](x, num_warps=32)\n         assert torch.all(x == 2)\n \n \n@@ -1452,6 +1476,8 @@ def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     \"\"\"\n     check_type_supported(in_dtype, device)\n     check_type_supported(out_dtype, device)\n+    if is_hip():\n+        pytest.skip('test_abs_fp8 not supported on HIP.')\n \n     @triton.jit\n     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n@@ -1507,6 +1533,9 @@ def get_reduced_dtype(dtype_str, op):\n def test_reduce1d(op, dtype_str, shape, num_ctas, device):\n     check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n+    if is_hip():\n+        pytest.skip(f\"test_reduce1d not supported on HIP\")\n+\n     # triton kernel\n     @triton.jit\n     def kernel(X, Z, BLOCK: tl.constexpr):\n@@ -1597,7 +1626,10 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n def test_reduce2d(op, dtype_str, shape, axis, num_ctas, device):\n     check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n+    if is_hip():\n+        pytest.skip(f\"test_reduce2d not supported on HIP\")\n     # triton kernel\n+\n     @triton.jit\n     def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n         range_m = tl.arange(0, BLOCK_M)\n@@ -1667,6 +1699,8 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n \n @pytest.mark.parametrize(\"op, dtype_str, shape, axis, num_warps\", scan_configs)\n def test_scan2d(op, dtype_str, shape, axis, num_warps, device):\n+    if is_hip():\n+        pytest.skip(\"test_scan2d is not supported in HIP\")\n     check_type_supported(dtype_str, device)\n \n     # triton kernel\n@@ -1720,6 +1754,9 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n @pytest.mark.parametrize(\"src_layout\", scan_layouts)\n @pytest.mark.parametrize(\"axis\", [0, 1])\n def test_scan_layouts(M, N, src_layout, axis, device):\n+    if is_hip():\n+        pytest.skip(\"test_scan_layouts is not supported in HIP\")\n+\n     ir = f\"\"\"\n     #blocked = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n@@ -1783,6 +1820,9 @@ def test_scan_layouts(M, N, src_layout, axis, device):\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"axis\", [0, 1])\n def test_reduce_layouts(M, N, src_layout, axis, device):\n+    if is_hip():\n+        pytest.skip(\"test_reduce_layouts is not supported in HIP\")\n+\n     rdims_2d = f\"1x{N}\" if axis == 0 else f\"{M}x1\"\n     rdims_1d = f\"{N}\" if axis == 0 else f\"{M}\"\n     store_range = \"%7\" if axis == 0 else \"%1\"\n@@ -1792,28 +1832,28 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n     tt.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: i32 {{tt.divisibility = 16 : i32}}, %arg2: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n-        %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>\n-        %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>\n+        %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #blocked}}>>\n+        %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>\n         %2 = tt.splat %arg1 : (i32) -> tensor<{M}x1xi32, #blocked>\n         %3 = arith.muli %1, %2 : tensor<{M}x1xi32, #blocked>\n         %4 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x1x!tt.ptr<i32>, #blocked>\n         %5 = tt.addptr %4, %3 : tensor<{M}x1x!tt.ptr<i32>, #blocked>, tensor<{M}x1xi32, #blocked>\n-        %6 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>\n-        %7 = tt.expand_dims %6 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x{N}xi32, #blocked>\n+        %6 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #{GPU_DIALECT}.slice<{{dim = 0, parent = #blocked}}>>\n+        %7 = tt.expand_dims %6 {{axis = 0 : i32}} : (tensor<{N}xi32, #{GPU_DIALECT}.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x{N}xi32, #blocked>\n         %8 = tt.broadcast %5 : (tensor<{M}x1x!tt.ptr<i32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<i32>, #blocked>\n         %9 = tt.broadcast %7 : (tensor<1x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #blocked>\n         %10 = tt.addptr %8, %9 : tensor<{M}x{N}x!tt.ptr<i32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n         %11 = tt.splat %arg2 : (!tt.ptr<i32>) -> tensor<{rdims_2d}x!tt.ptr<i32>, #blocked>\n         %12 = tt.addptr %11, {store_range} : tensor<{rdims_2d}x!tt.ptr<i32>, #blocked>, tensor<{rdims_2d}xi32, #blocked>\n         %13 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #blocked>\n-        %14 = triton_gpu.convert_layout %13 : (tensor<{M}x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #src>\n+        %14 = {GPU_DIALECT}.convert_layout %13 : (tensor<{M}x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #src>\n         %15 = \"tt.reduce\"(%14) ({{\n         ^bb0(%arg3: i32, %arg4: i32):\n           %17 = arith.addi %arg3, %arg4 : i32\n           tt.reduce.return %17 : i32\n-        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n-        %18 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n-        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xi32, #blocked>\n+        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{rdims_1d}xi32, #{GPU_DIALECT}.slice<{{dim = {axis}, parent = #src}}>>\n+        %18 = {GPU_DIALECT}.convert_layout %15 : (tensor<{rdims_1d}xi32, #{GPU_DIALECT}.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xi32, #{GPU_DIALECT}.slice<{{dim = {axis}, parent = #blocked}}>>\n+        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xi32, #{GPU_DIALECT}.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xi32, #blocked>\n         tt.store %12, %19 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xi32, #blocked>\n         tt.return\n     }}\n@@ -1854,17 +1894,20 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n @pytest.mark.parametrize(\"M\", [32, 64, 128, 256])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n def test_store_op(M, src_layout, device):\n+    if is_hip():\n+        pytest.skip(\"test_convert1d is not supported yet in HIP\")\n+\n     ir = f\"\"\"\n     #src = {src_layout}\n-    module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n+    module attributes {{\"{GPU_DIALECT}.num-warps\" = 4 : i32, \"{GPU_DIALECT}.num-ctas\" = 1 : i32, \"{GPU_DIALECT}.threads-per-warp\" = {THREADS_PER_WARP} : i32}} {{\n         tt.func public @kernel(%arg0: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}) {{\n-            %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-            %1 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<{M}x!tt.ptr<f32>, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-            %2 = tt.addptr %1, %0 : tensor<{M}x!tt.ptr<f32>, #triton_gpu.slice<{{dim = 1, parent = #src}}>>, tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-            %3 = tt.load %2 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}xf32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-            %4 = tt.expand_dims %3 {{axis = 1 : i32}} : (tensor<{M}xf32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xf32, #src>\n-            %5 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-            %6 = tt.expand_dims %5 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n+            %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+            %1 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<{M}x!tt.ptr<f32>, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+            %2 = tt.addptr %1, %0 : tensor<{M}x!tt.ptr<f32>, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>, tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+            %3 = tt.load %2 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}xf32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+            %4 = tt.expand_dims %3 {{axis = 1 : i32}} : (tensor<{M}xf32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xf32, #src>\n+            %5 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+            %6 = tt.expand_dims %5 {{axis = 1 : i32}} : (tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n             %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<{M}x1x!tt.ptr<f32>, #src>\n             %8 = tt.addptr %7, %6 : tensor<{M}x1x!tt.ptr<f32>, #src>, tensor<{M}x1xi32, #src>\n             tt.store %8, %4 : tensor<{M}x1xf32, #src>\n@@ -1903,20 +1946,23 @@ def test_store_op(M, src_layout, device):\n @pytest.mark.parametrize(\"src_dim\", [0, 1])\n @pytest.mark.parametrize(\"dst_dim\", [0, 1])\n def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device):\n+    if is_hip():\n+        pytest.skip(\"test_convert1d is not supported in HIP\")\n+\n     ir = f\"\"\"\n     #dst = {dst_layout}\n     #src = {src_layout}\n-    module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n+    module attributes {{\"{GPU_DIALECT}.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = {THREADS_PER_WARP} : i32}} {{\n         tt.func public @kernel(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n-            %0 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n-            %1 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n-            %2 = tt.addptr %0, %1 : tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>, tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n-            %3 = tt.load %2 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n-            %4 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n-            %5 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n-            %6 = tt.addptr %4, %5 : tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>, tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n-            %7 = triton_gpu.convert_layout %3 : (tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>) -> tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n-            tt.store %6, %7 : tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %0 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x!tt.ptr<i32>, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %1 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %2 = tt.addptr %0, %1 : tensor<{M}x!tt.ptr<i32>, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>, tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %3 = tt.load %2 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %4 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<{M}x!tt.ptr<i32>, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %5 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %6 = tt.addptr %4, %5 : tensor<{M}x!tt.ptr<i32>, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>, tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %7 = {GPU_DIALECT}.convert_layout %3 : (tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>) -> tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            tt.store %6, %7 : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>\n             tt.return\n         }}\n     }}\n@@ -1962,26 +2008,29 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n @pytest.mark.parametrize(\"op\", [\"sum\", \"max\"])\n @pytest.mark.parametrize(\"first_axis\", [0, 1])\n def test_chain_reduce(M, N, src_layout, op, device, first_axis):\n+    if is_hip():\n+        pytest.skip(\"test_chain_reduce is not supported in HIP\")\n+\n     op_str = \"\"\n     if op == \"sum\":\n         op_str = f\"\"\"\n         %13 = arith.addi %arg2, %arg3 : i32\n         tt.reduce.return %13 : i32\"\"\"\n     elif op == \"max\":\n         op_str = f\"\"\"\n-        %13 = \"triton_gpu.cmpi\"(%arg2, %arg3) <{{predicate = 4 : i64}}> : (i32, i32) -> i1\n+        %13 = \"{GPU_DIALECT}.cmpi\"(%arg2, %arg3) <{{predicate = 4 : i64}}> : (i32, i32) -> i1\n         %14 = arith.select %13, %arg2, %arg3 : i32\n         tt.reduce.return %14 : i32\"\"\"\n     ir = f\"\"\"\n     #src = {src_layout}\n-    module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n+    module attributes {{\"{GPU_DIALECT}.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = {THREADS_PER_WARP} : i32}} {{\n     tt.func public @sum_kernel_0d1d(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n         %cst = arith.constant dense<{N}> : tensor<{M}x1xi32, #src>\n-        %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-        %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n+        %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+        %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n         %2 = arith.muli %1, %cst : tensor<{M}x1xi32, #src>\n-        %3 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #src}}>>\n-        %4 = tt.expand_dims %3 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #src}}>>) -> tensor<1x{N}xi32, #src>\n+        %3 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #{GPU_DIALECT}.slice<{{dim = 0, parent = #src}}>>\n+        %4 = tt.expand_dims %3 {{axis = 0 : i32}} : (tensor<{N}xi32, #{GPU_DIALECT}.slice<{{dim = 0, parent = #src}}>>) -> tensor<1x{N}xi32, #src>\n         %5 = tt.broadcast %2 : (tensor<{M}x1xi32, #src>) -> tensor<{M}x{N}xi32, #src>\n         %6 = tt.broadcast %4 : (tensor<1x{N}xi32, #src>) -> tensor<{M}x{N}xi32, #src>\n         %7 = arith.addi %5, %6 : tensor<{M}x{N}xi32, #src>\n@@ -1991,11 +2040,11 @@ def test_chain_reduce(M, N, src_layout, op, device, first_axis):\n         %11 = \"tt.reduce\"(%10) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n         {op_str}\n-        }}) {{axis = {first_axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M if first_axis == 1 else N}xi32, #triton_gpu.slice<{{dim = {first_axis}, parent = #src}}>>\n+        }}) {{axis = {first_axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M if first_axis == 1 else N}xi32, #{GPU_DIALECT}.slice<{{dim = {first_axis}, parent = #src}}>>\n         %12 = \"tt.reduce\"(%11) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n         {op_str}\n-        }}) {{axis = 0 : i32}} : (tensor<{M if first_axis == 1 else N}xi32, #triton_gpu.slice<{{dim = {first_axis}, parent = #src}}>>) -> i32\n+        }}) {{axis = 0 : i32}} : (tensor<{M if first_axis == 1 else N}xi32, #{GPU_DIALECT}.slice<{{dim = {first_axis}, parent = #src}}>>) -> i32\n         tt.store %arg1, %12 {{cache = 1 : i32, evict = 1 : i32}} : i32\n         tt.return\n     }}\n@@ -2063,6 +2112,8 @@ def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n @pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n def test_permute(dtype_str, shape, perm, num_ctas, device):\n     check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n+    if is_hip():\n+        pytest.skip(f\"test_permute is not supported in HIP\")\n \n     # triton kernel\n     @triton.jit\n@@ -2099,6 +2150,10 @@ def kernel(X, stride_xm, stride_xn,\n     # compare\n     np.testing.assert_allclose(to_numpy(z_tri), z_ref)\n     np.testing.assert_allclose(to_numpy(z_tri_contiguous), z_ref)\n+\n+    if is_hip():\n+        return\n+\n     # parse ptx to make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']\n     assert 'ld.global.v4' in ptx\n@@ -2115,7 +2170,7 @@ def kernel(X, stride_xm, stride_xn,\n \n @pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype\",\n                          [(*shape, 4, False, False, epilogue, allow_tf32, in_dtype, out_dtype)\n-                          for shape in [(64, 64, 64), (16, 16, 16)]\n+                          for shape in [(64, 64, 64), (32, 32, 32), (16, 16, 16)]\n                           for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n                           for allow_tf32 in [True, False]\n                           for in_dtype, out_dtype in [('float16', 'float16'),\n@@ -2146,6 +2201,17 @@ def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, o\n     check_cuda_only(device)\n \n     capability = torch.cuda.get_device_capability()\n+\n+    if is_hip():\n+        # set capability to large number to jump over check below\n+        # check are not relevant to amd gpu, left them for smaller diff between test_core.py and test_core_amd.py tests\n+        capability = (100, 100)\n+        if out_dtype is None:\n+            if in_dtype in float_dtypes:\n+                out_dtype = \"float32\"\n+            else:\n+                out_dtype = \"int32\"\n+\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if capability[0] < 8:\n@@ -2160,6 +2226,16 @@ def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, o\n             # TODO: support out_dtype=float16 for tl.dot on V100\n             pytest.skip(\"Only test out_dtype=float16 on devices with sm >=80\")\n \n+    if is_hip():\n+        if (M, N, K) in [(64, 128, 128)]:\n+            pytest.skip(f\"test_dot{(M, N, K)} not supported on HIP: memory out of resource.\")\n+        if (M, N, K, num_warps) in [(128, 256, 32, 8), (128, 128, 64, 4)]:\n+            pytest.skip(f\"test_dot{(M, N, K)} not supported on HIP. Reduce Warp to work\")\n+        if M == 16 or N == 16 or K == 16:\n+            pytest.skip(f\"test_dot{(M, N, K)} segfaults on HIP\")\n+        if epilogue == \"softmax\":\n+            pytest.skip(f\"test_dot{epilogue} segfaults on HIP\")\n+\n     torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n \n     if num_ctas > 1 and in_dtype == 'int8':\n@@ -2172,12 +2248,12 @@ def kernel(X, stride_xm, stride_xk,\n                Y, stride_yk, stride_yn,\n                W, stride_wn, stride_wl,\n                Z, stride_zm, stride_zn,\n-               out_dtype: tl.constexpr,\n                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n                ALLOW_TF32: tl.constexpr,\n                DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n-               COL_A: tl.constexpr, COL_B: tl.constexpr):\n+               COL_A: tl.constexpr, COL_B: tl.constexpr,\n+               out_dtype: tl.constexpr = tl.float32):\n         off_m = tl.arange(0, BLOCK_M)\n         off_n = tl.arange(0, BLOCK_N)\n         off_l = tl.arange(0, BLOCK_N)\n@@ -2247,11 +2323,11 @@ def kernel(X, stride_xm, stride_xk,\n         out_dtype = tl.float16\n     else:\n         out_dtype = tl.float32\n+\n     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n                          y_tri, y_tri.stride(0), y_tri.stride(1),\n                          w_tri, w_tri.stride(0), w_tri.stride(1),\n                          z_tri, z_tri.stride(0), z_tri.stride(1),\n-                         out_dtype,\n                          COL_A=col_a, COL_B=col_b,\n                          BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n                          ADD_MATRIX=epilogue == 'add-matrix',\n@@ -2260,21 +2336,26 @@ def kernel(X, stride_xm, stride_xk,\n                          DO_SOFTMAX=epilogue == 'softmax',\n                          CHAIN_DOT=epilogue == 'chain-dot',\n                          ALLOW_TF32=allow_tf32,\n-                         num_warps=num_warps, num_ctas=num_ctas)\n+                         num_warps=num_warps, num_ctas=num_ctas,\n+                         out_dtype=out_dtype)\n+\n     if epilogue == 'softmax' and (in_dtype != 'float32' or allow_tf32):\n-        ptx = pgm.asm[\"ptx\"]\n-        start = ptx.find(\"shfl.sync\")\n-        end = ptx.find(\"cvt.rn.f16.f32\")\n-        red_code = ptx[start:end]\n-        assert len(red_code) > 0\n-        import os\n-        enable_mmav3 = os.environ.get('ENABLE_MMA_V3', 'not found').lower()\n-        enable_tma = os.environ.get('ENABLE_TMA', 'not found').lower()\n-        # skip this check on hopper because there are some functions whose name contain \"shared\" in ptx.\n-        # TODO: we should eliminate these unused functions in ptx code.\n-        if not (enable_mmav3 in [\"on\", \"true\", \"1\"] and enable_tma in [\"on\", \"true\", \"1\"]):\n-            assert \"shared\" not in red_code\n-        assert \"bar.sync\" not in red_code\n+        if is_hip():\n+            pass\n+        else:\n+            ptx = pgm.asm[\"ptx\"]\n+            start = ptx.find(\"shfl.sync\")\n+            end = ptx.find(\"cvt.rn.f16.f32\")\n+            red_code = ptx[start:end]\n+            assert len(red_code) > 0\n+            import os\n+            enable_mmav3 = os.environ.get('ENABLE_MMA_V3', 'not found').lower()\n+            enable_tma = os.environ.get('ENABLE_TMA', 'not found').lower()\n+            # skip this check on hopper because there are some functions whose name contain \"shared\" in ptx.\n+            # TODO: we should eliminate these unused functions in ptx code.\n+            if not (enable_mmav3 in [\"on\", \"true\", \"1\"] and enable_tma in [\"on\", \"true\", \"1\"]):\n+                assert \"shared\" not in red_code\n+            assert \"bar.sync\" not in red_code\n     # torch result\n     if in_dtype == 'int8':\n         z_ref = np.matmul(x.astype(np.float32),\n@@ -2300,9 +2381,12 @@ def kernel(X, stride_xm, stride_xk,\n         # XXX: Somehow there's a larger difference when we use float32\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n     elif out_dtype == tl.float16:\n-        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-2)\n     else:\n-        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+        # added atol, to loose precision for float16xfloat16->float32 case\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n+    if is_hip():\n+        return\n     # make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']\n     if (K > 16 or N > 16 or M > 16) and (M * N // (num_warps * 32) >= 4):\n@@ -2366,6 +2450,9 @@ def kernel(Z, X, Y,\n     h = kernel[grid](z_tri, x_tri, y_tri, M, N, K, BM, BN, BK)\n     z_ref = np.matmul(x, y)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), atol=0.01)\n+\n+    if is_hip():\n+        return\n     assert \"tt.dot\" in h.asm['ttir']\n     # with option ENABLE_MMA_V3 on, we will not pipeline the load op for Y\n     # as the loaded value is in rowmajor. But MMAv3 requires it's second\n@@ -2432,6 +2519,9 @@ def test_dot_without_load(dtype_str, device):\n     capability = torch.cuda.get_device_capability()\n     allow_tf32 = capability[0] > 7\n \n+    if is_hip() and dtype_str == \"float16\":\n+        pytest.skip(\"test_dot_without_load[float16] not supported in HIP\")\n+\n     @triton.jit\n     def _kernel(out, ALLOW_TF32: tl.constexpr):\n         a = GENERATE_TEST_HERE\n@@ -2512,6 +2602,9 @@ def _kernel(in_ptr, out_ptr, in_size: tl.constexpr, out_size: tl.constexpr):\n # FIXME: Shape too small for ldmatrix when num_ctas=4\n @pytest.mark.parametrize(\"dtype\", [torch.bfloat16, torch.float16, torch.float32])\n def test_masked_load_shared_memory(dtype, device):\n+    if is_hip():\n+        pytest.skip(\"test_masked_load_shared_memory is not supported in HIP\")\n+\n     check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     M = 32\n@@ -2571,6 +2664,9 @@ def _kernel(dst, src, CACHE: tl.constexpr):\n         tl.store(dst + offsets, x)\n \n     pgm = _kernel[(1,)](dst, src, CACHE=cache)\n+    if is_hip():\n+        return\n+\n     ptx = pgm.asm['ptx']\n     if cache == '':\n         assert 'ld.global.ca' not in ptx\n@@ -2597,6 +2693,10 @@ def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n         tl.store(dst + offsets, x, mask=offsets < N)\n     pgm = _kernel[(1,)](\n         dst, src, N=N, BLOCK_SIZE=block_size)\n+\n+    if is_hip():\n+        return\n+\n     ptx = pgm.asm[\"ptx\"]\n     if N % 16 == 0:\n         assert \"ld.global.v4.b32\" in ptx\n@@ -2620,6 +2720,9 @@ def _kernel(dst, src, off, N, BLOCK_SIZE: tl.constexpr, HINT: tl.constexpr):\n         x = tl.load(src + offsets, mask=offsets < N)\n         tl.store(dst + offsets, x, mask=offsets < N)\n     pgm = _kernel[(1,)](dst, src, off, N=1024, BLOCK_SIZE=src.shape[0], HINT=has_hints)\n+    if is_hip():\n+        return\n+\n     ptx = pgm.asm[\"ptx\"]\n     if has_hints:\n         assert \"ld.global.v4.b32\" in ptx\n@@ -2642,6 +2745,8 @@ def _kernel(dst, src, CACHE: tl.constexpr):\n         x = tl.load(src + offsets)\n         tl.store(dst + offsets, x, cache_modifier=CACHE)\n \n+    if is_hip():\n+        return\n     pgm = _kernel[(1,)](dst, src, CACHE=cache)\n     ptx = pgm.asm['ptx']\n     if cache == '':\n@@ -2793,6 +2898,9 @@ def kernel(VALUE, X):\n @pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n @pytest.mark.parametrize(\"is_rhs_constexpr\", [True, False])\n def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr, device):\n+    if is_hip():\n+        if (is_rhs_constexpr, is_lhs_constexpr, op) in [(False, False, \"<<\"), (False, False, \">>\"), (False, True, \"<<\")]:\n+            pytest.skip(f\"test_bin_op_constexpr[{is_lhs_constexpr}-{is_rhs_constexpr}-{op}] is not supported in HIP\")\n \n     @triton.jit\n     def kernel(Z, X, Y):\n@@ -2968,6 +3076,9 @@ def _kernel(dst):\n @pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n def test_math_tensor(dtype_str, expr, lib_path, num_ctas, device):\n \n+    if is_hip() and expr == \"math.scalbn\":\n+        pytest.skip(\"test_math_tensor[math.scalbn] is not supported in HIP\")\n+\n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))\n@@ -3063,6 +3174,9 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n def test_inline_asm(num_ctas, device):\n     check_cuda_only(device)\n \n+    if is_hip():\n+        pytest.skip(\"test_inline_asm is not supported in HIP\")\n+\n     @triton.jit\n     def kernel(X, Y, Z, n: tl.constexpr, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))\n@@ -3089,6 +3203,9 @@ def kernel(X, Y, Z, n: tl.constexpr, BLOCK: tl.constexpr):\n def test_inline_asm_packed(num_ctas, device):\n     check_cuda_only(device)\n \n+    if is_hip():\n+        pytest.skip(\"test_inline_asm is not supported in HIP\")\n+\n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))\n@@ -3392,6 +3509,8 @@ def nested_while(data, countPtr):\n \n \n def test_globaltimer(device):\n+    if is_hip():\n+        pytest.skip(\"test_globaltimer is not supported in HIP\")\n     check_cuda_only(device)\n \n     @triton.jit\n@@ -3411,6 +3530,8 @@ def kernel(Out1, Out2):\n \n \n def test_smid(device):\n+    if is_hip():\n+        pytest.skip(\"test_smid is not supported in HIP\")\n     check_cuda_only(device)\n \n     @triton.jit\n@@ -3456,6 +3577,9 @@ def kernel(Out):\n @pytest.mark.parametrize(\"interm_layout\", intermediate_layouts)\n @pytest.mark.parametrize(\"dst_layout\", layouts)\n def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device):\n+    if is_hip():\n+        pytest.skip(\"test_convert2d is not supported in HIP\")\n+\n     if str(src_layout) == str(dst_layout):\n         pytest.skip()\n     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):"}, {"filename": "python/test/unit/language/test_line_info.py", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -116,5 +116,3 @@ def test_line_info(func: str):\n         assert (check_file_lines(file_lines, \"standard.py\", 33))\n         assert (check_file_lines(file_lines, \"standard.py\", 34))\n         assert (check_file_lines(file_lines, \"standard.py\", 36))\n-        # core.py is changed frequently, so we only check if it exists\n-        assert (check_file_lines(file_lines, \"core.py\", -1))"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 51, "deletions": 48, "changes": 99, "file_content_changes": "@@ -26,86 +26,89 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n \n \n @pytest.mark.parametrize(\n-    \"BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE\",\n+    \"BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE, ALLOW_TF32\",\n     itertools.chain(\n         *[\n             [\n                 # 1 warp\n-                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 32, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 32, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 64, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 32, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 32, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (64, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 64, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n                 # 2 warp\n-                (64, 32, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 64, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 32, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 64, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 128, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (64, 32, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 64, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (64, 32, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 64, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 128, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n                 # 4 warp\n-                (128, 64, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 128, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (128, 32, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 128, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (128, 32, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 128, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (128, 64, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (64, 128, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (128, 32, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 128, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (128, 32, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 128, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n                 # 8 warp\n-                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n                 # variable input\n-                (128, 128, 32, 1, 4, 2, 256, 384, 160, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 128, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 83, AT, BT, DTYPE, DTYPE),\n-                (128, 256, 64, 1, 8, 3, 256, 512, 160, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 256, 384, 160, AT, BT, DTYPE, DTYPE, True),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 128, AT, BT, DTYPE, DTYPE, True),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 83, AT, BT, DTYPE, DTYPE, True),\n+                (128, 256, 64, 1, 8, 3, 256, 512, 160, AT, BT, DTYPE, DTYPE, True),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n         # n-stage\n         *[\n             [\n-                (16, 16, 16, 1, 1, STAGES, 32, 32, 80, AT, BT, DTYPE, DTYPE),\n-                (64, 32, 64, 1, 2, STAGES, 128, 64, 128, AT, BT, DTYPE, DTYPE),\n-                (128, 64, 16, 1, 4, STAGES, 256, 128, 80, AT, BT, DTYPE, DTYPE),\n-                (256, 128, 32, 1, 8, STAGES, 512, 256, 160, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, STAGES, 256, 256, 160, AT, BT, DTYPE, DTYPE),\n+                (16, 16, 16, 1, 1, STAGES, 32, 32, 80, AT, BT, DTYPE, DTYPE, True),\n+                (64, 32, 64, 1, 2, STAGES, 128, 64, 128, AT, BT, DTYPE, DTYPE, True),\n+                (128, 64, 16, 1, 4, STAGES, 256, 128, 80, AT, BT, DTYPE, DTYPE, True),\n+                (256, 128, 32, 1, 8, STAGES, 512, 256, 160, AT, BT, DTYPE, DTYPE, True),\n+                (128, 128, 32, 1, 4, STAGES, 256, 256, 160, AT, BT, DTYPE, DTYPE, True),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [4]\n         ],\n         # mixed-precision\n         *[\n             [\n-                (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n+                (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE, True),\n+                (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE, True),\n+                (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE, True),\n             ] for ADTYPE, BDTYPE in [(\"float8e4nv\", \"float8e5\"),\n                                      (\"float8e4nv\", \"float8e4nv\"),\n                                      (\"float8e5\", \"float8e4nv\"),\n                                      (\"float8e5\", \"float8e5\"),\n+                                     (\"float8e4b15\", \"float8e4b15\"),\n                                      (\"float8e4nv\", \"float16\"),\n                                      (\"float16\", \"float8e5\"),\n                                      (\"float16\", \"float32\"),\n                                      (\"float32\", \"float16\"),\n                                      (\"bfloat16\", \"float32\"),\n                                      (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n         ],\n+        # mixed-precision block layout\n         *[\n-            # float8e4b15 only supports row-col layout\n             [\n-                (128, 128, 32, 1, 4, 2, None, None, None, False, True, ADTYPE, BDTYPE),\n-            ] for ADTYPE, BDTYPE in [(\"float8e4b15\", \"float8e5\"),\n-                                     (\"float8e4b15\", \"float16\"),\n-                                     (\"float16\", \"float8e4b15\"),\n-                                     (\"float8e5\", \"float8e5\"),\n-                                     (\"float8e4nv\", \"float8e4nv\"),\n-                                     (\"int8\", \"int8\")]\n-        ]\n+                (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE, False),\n+                (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE, False),\n+                (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE, False),\n+            ] for ADTYPE, BDTYPE in [(\"float8e4nv\", \"float16\"),\n+                                     (\"float16\", \"float8e5\"),\n+                                     (\"float16\", \"float32\"),\n+                                     (\"float32\", \"float16\"),\n+                                     (\"bfloat16\", \"float32\"),\n+                                     (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n+        ],\n     ),\n )\n-def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE):\n+def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE, ALLOW_TF32):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n@@ -173,7 +176,7 @@ def init_input(m, n, dtype):\n             a = triton.reinterpret(a, getattr(tl, ADTYPE))\n         if b_fp8:\n             b = triton.reinterpret(b, getattr(tl, BDTYPE))\n-        tt_c = triton.ops.matmul(a, b)\n+        tt_c = triton.ops.matmul(a, b, None, ALLOW_TF32)\n         torch.testing.assert_allclose(th_c, tt_c, atol=0, rtol=0)\n     except triton.OutOfResources as e:\n         pytest.skip(str(e))"}, {"filename": "python/triton/common/backend.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -5,6 +5,7 @@\n import os\n import re\n import subprocess\n+import traceback\n from typing import Dict\n \n from ..runtime.driver import DriverBase\n@@ -94,7 +95,7 @@ def get_backend(device_type: str):\n             try:\n                 importlib.import_module(device_backend_package_name, package=__spec__.name)\n             except Exception:\n-                return None\n+                traceback.print_exc()\n         else:\n             return None\n     return _backends[device_type] if device_type in _backends else None"}, {"filename": "python/triton/compiler/__init__.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -1,4 +1,5 @@\n-from .compiler import CompiledKernel, compile, instance_descriptor\n+from .compiler import (CompiledKernel, compile, get_arch_default_num_stages,\n+                       get_arch_default_num_warps, instance_descriptor)\n from .errors import CompilationError\n \n-__all__ = [\"compile\", \"instance_descriptor\", \"CompiledKernel\", \"CompilationError\"]\n+__all__ = [\"compile\", \"instance_descriptor\", \"CompiledKernel\", \"CompilationError\", \"get_arch_default_num_warps\", \"get_arch_default_num_stages\"]"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 76, "deletions": 116, "changes": 192, "file_content_changes": "@@ -5,18 +5,18 @@\n import json\n import os\n import re\n-import subprocess\n import tempfile\n from collections import namedtuple\n from pathlib import Path\n-from typing import Any, Tuple\n+from typing import Any\n \n from .._C.libtriton.triton import (ClusterInfo, TMAInfos, add_external_libs,\n                                    compile_ptx_to_cubin, get_env_vars, get_num_warps,\n                                    get_shared_memory_size, ir, runtime,\n-                                   translate_llvmir_to_hsaco, translate_llvmir_to_ptx,\n+                                   translate_llvmir_to_ptx,\n                                    translate_triton_gpu_to_llvmir)\n from ..common.backend import get_backend, path_to_ptxas\n+from ..common.build import is_hip\n # from ..runtime import driver, jit, JITFunction\n # TODO: runtime.errors\n from ..runtime.autotuner import OutOfResources\n@@ -188,72 +188,6 @@ def ptx_to_cubin(ptx: str, arch: int):\n     return compile_ptx_to_cubin(ptx, ptxas, arch)\n \n \n-# AMDGCN translation\n-\n-def get_amdgcn_bitcode_paths(arch):\n-    gpu_arch_agnostic_bitcode_libraries = [\"opencl.bc\",\n-                                           \"ocml.bc\",\n-                                           \"ockl.bc\",\n-                                           \"oclc_finite_only_off.bc\",\n-                                           \"oclc_daz_opt_off.bc\",\n-                                           \"oclc_correctly_rounded_sqrt_on.bc\",\n-                                           \"oclc_unsafe_math_off.bc\",\n-                                           \"oclc_wavefrontsize64_on.bc\"]\n-\n-    gfx_arch = arch[1]\n-    gfx_arch_id = re.search('gfx(\\\\w+)', gfx_arch).group(1).strip()\n-\n-    gpu_arch_specific_bitcode_library = 'oclc_isa_version_' + gfx_arch_id + \".bc\"\n-    bitcode_path_dir = os.path.join(Path(__file__).parent.resolve(), \"third_party/rocm/lib/bitcode/\")\n-\n-    amdgcn_bitcode_paths = {}\n-    i = 1\n-    for bc_lib in gpu_arch_agnostic_bitcode_libraries:\n-        bc_path = bitcode_path_dir + bc_lib\n-        if os.path.exists(bc_path):\n-            amdgcn_bitcode_paths['library_' + str(i)] = bc_path\n-            i += 1\n-    bc_gfx_path = bitcode_path_dir + gpu_arch_specific_bitcode_library\n-    if os.path.exists(bc_gfx_path):\n-        amdgcn_bitcode_paths['library_' + str(i)] = bc_gfx_path\n-\n-    return amdgcn_bitcode_paths\n-\n-\n-def get_amdgpu_arch_fulldetails():\n-    \"\"\"\n-    get the amdgpu fulll ISA details for compiling:\n-    i.e., arch_triple: amdgcn-amd-amdhsa; arch_name: gfx906; arch_features: sramecc+:xnack-\n-    \"\"\"\n-    try:\n-        # TODO: package rocm.cc with Triton\n-        rocm_path_dir = os.getenv(\"ROCM_PATH\", default=\"/opt/rocm\")\n-        rocminfo = subprocess.check_output(rocm_path_dir + '/bin/rocminfo').decode()\n-        gfx_arch_details = re.search('amd.*', rocminfo).group(0).strip().split('--')\n-        arch_triple = gfx_arch_details[0]\n-        arch_name_features = gfx_arch_details[1].split(':')\n-        arch_name = arch_name_features[0]\n-        arch_features = \"\"\n-\n-        if (len(arch_name_features) == 3):\n-            arch_features = \"+\" + re.search('\\\\w+', arch_name_features[1]).group(0) + \",\"\\\n-                            \"-\" + re.search('\\\\w+', arch_name_features[2]).group(0)\n-        return [arch_triple, arch_name, arch_features]\n-    except BaseException:\n-        return None\n-\n-\n-def llir_to_amdgcn_and_hsaco(mod: Any, gfx_arch: str, gfx_triple: str, gfx_features: str) -> Tuple[str, str]:\n-    '''\n-    Translate TritonGPU module to HSACO code based on full details of gpu architecture.\n-    :param mod: a TritonGPU dialect module\n-    :return:\n-        - AMDGCN code\n-        - Path to HSACO object\n-    '''\n-    return translate_llvmir_to_hsaco(mod, gfx_arch, gfx_triple, gfx_features)\n-\n-\n # ------------------------------------------------------------------------------\n # compiler\n # ------------------------------------------------------------------------------\n@@ -320,8 +254,10 @@ def make_hash(fn, arch, env_vars, **kwargs):\n     \"ttgir\": mlir_arg_type_pattern,\n     \"ptx\": ptx_arg_type_pattern,\n }\n-\n-ttgir_num_warps_pattern = r'\"triton_gpu.num-warps\"\\s?=\\s?(\\d+)\\s?:'\n+if is_hip():\n+    ttgir_num_warps_pattern = r'\"triton_gpu_rocm.num-warps\"\\s?=\\s?(\\d+)\\s?:'\n+else:\n+    ttgir_num_warps_pattern = r'\"triton_gpu.num-warps\"\\s?=\\s?(\\d+)\\s?:'\n \n \n def _get_jsonable_constants(constants):\n@@ -354,35 +290,37 @@ def _is_cuda(arch):\n \n \n def get_architecture_descriptor(capability):\n-    try:\n-        import torch\n-    except ImportError:\n-        raise ImportError(\"Triton requires PyTorch to be installed\")\n     if capability is None:\n-        if torch.version.hip is None:\n-            device = get_current_device()\n-            capability = get_device_capability(device)\n-            capability = capability[0] * 10 + capability[1]\n-        else:\n-            capability = get_amdgpu_arch_fulldetails()\n+        device = get_current_device()\n+        capability = get_device_capability(device)\n+        capability = capability[0] * 10 + capability[1]\n     return capability\n \n \n-def add_rocm_stages(arch, extern_libs, stages):\n-    extern_libs.update(get_amdgcn_bitcode_paths(arch))\n+def get_arch_default_num_warps(device_type):\n+    if device_type in [\"cuda\", \"hip\"]:\n+        num_warps = 4\n+    else:\n+        _device_backend = get_backend(device_type)\n+        assert _device_backend\n+        arch = _device_backend.get_architecture_descriptor()\n+        num_warps = arch[\"num_warps\"]\n+\n+    return num_warps\n+\n \n-    for key in list(extern_libs):\n-        if extern_libs[key] == '' or extern_libs[key] is None:\n-            extern_libs.pop(key)\n+def get_arch_default_num_stages(device_type, capability=None):\n+    if device_type in [\"cuda\", \"hip\"]:\n+        arch = get_architecture_descriptor(capability)\n+        is_cuda = device_type == \"cuda\" and _is_cuda(arch)\n+        num_stages = 3 if is_cuda and arch >= 75 else 2\n+    else:\n+        _device_backend = get_backend(device_type)\n+        assert _device_backend\n+        arch = _device_backend.get_architecture_descriptor()\n+        num_stages = arch[\"num_stages\"]\n \n-    gfx_arch_full_details = arch\n-    gfx_arch = os.environ.get('MI_GPU_ARCH', gfx_arch_full_details[1])\n-    if gfx_arch is None:\n-        raise RuntimeError('gfx_arch is None (not specified)')\n-    stages[\"amdgcn\"] = (lambda path: Path(path).read_text(),\n-                        lambda src: llir_to_amdgcn_and_hsaco(src, gfx_arch,\n-                                                             gfx_arch_full_details[0],\n-                                                             gfx_arch_full_details[2]))\n+    return num_stages\n \n \n def add_cuda_stages(arch, extern_libs, stages):\n@@ -396,22 +334,28 @@ def add_cuda_stages(arch, extern_libs, stages):\n def compile(fn, **kwargs):\n     # Get device type to decide which backend should be used\n     device_type = kwargs.get(\"device_type\", \"cuda\")\n-    _device_backend = get_backend(device_type)\n+    capability = kwargs.get(\"cc\", None)\n \n-    if device_type in [\"cuda\", \"hip\"]:\n-        arch = get_architecture_descriptor(kwargs.get(\"cc\", None))\n+    if is_hip():\n+        device_type = \"hip\"\n+\n+    if device_type == \"cuda\":\n+        _device_backend = get_backend(device_type)\n+        arch = get_architecture_descriptor(capability)\n     else:\n         _device_backend = get_backend(device_type)\n         assert _device_backend\n         arch = _device_backend.get_architecture_descriptor(**kwargs)\n \n     is_cuda = device_type == \"cuda\" and _is_cuda(arch)\n-    is_hip = device_type in [\"cuda\", \"hip\"] and not is_cuda\n+    if is_hip():\n+        is_cuda = False\n     context = ir.context()\n     constants = kwargs.get(\"constants\", dict())\n-    num_warps = kwargs.get(\"num_warps\", 4)\n+    num_warps = kwargs.get(\"num_warps\", get_arch_default_num_warps(device_type))\n+    assert num_warps > 0 and (num_warps & (num_warps - 1)) == 0, \"num_warps must be a power of 2\"\n     num_ctas = kwargs.get(\"num_ctas\", 1)\n-    num_stages = kwargs.get(\"num_stages\", 3 if is_cuda and arch >= 75 else 2)\n+    num_stages = kwargs.get(\"num_stages\", get_arch_default_num_stages(device_type, capability=capability))\n     # TODO[shuhaoj]: Default should be to enable warp specialization once possible\n     enable_warp_specialization = kwargs.get(\"enable_warp_specialization\", False)\n     # TODO[shuhaoj]: persistent can be decoupled with warp specialization\n@@ -436,15 +380,25 @@ def compile(fn, **kwargs):\n     stages[\"ast\"] = (lambda path: fn, None)\n     stages[\"ttir\"] = (lambda path: parse_mlir_module(path, context),\n                       lambda src: optimize_ttir(ast_to_ttir(src, signature, configs[0], constants, debug=debug, arch=arch), arch))\n-    stages[\"ttgir\"] = (lambda path: parse_mlir_module(path, context),\n-                       lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps, num_ctas, arch), num_stages, num_warps, num_ctas, arch, cluster_info, enable_warp_specialization, enable_persistent, optimize_epilogue))\n-    stages[\"llir\"] = (lambda path: Path(path).read_text(),\n-                      lambda src: ttgir_to_llir(src, extern_libs, arch, tma_infos))\n     if is_cuda:\n+        stages[\"ttgir\"] = (lambda path: parse_mlir_module(path, context),\n+                           lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps, num_ctas, arch), num_stages, num_warps, num_ctas, arch, cluster_info, enable_warp_specialization, enable_persistent, optimize_epilogue))\n+        stages[\"llir\"] = (lambda path: Path(path).read_text(),\n+                          lambda src: ttgir_to_llir(src, extern_libs, arch, tma_infos))\n         add_cuda_stages(arch, extern_libs, stages)\n-    elif is_hip:\n-        add_rocm_stages(arch, extern_libs, stages)\n+    elif device_type == \"hip\":\n+        _device_backend.add_stages(arch, extern_libs, stages, num_warps=num_warps, num_stages=num_stages)\n+    elif device_type == \"xpu\":\n+        stages[\"ttgir\"] = (lambda path: parse_mlir_module(path, context),\n+                           lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps, num_ctas, arch), num_stages, num_warps, num_ctas, arch, cluster_info, enable_warp_specialization, enable_persistent, optimize_epilogue))\n+        stages[\"llir\"] = (lambda path: Path(path).read_text(),\n+                          lambda src: ttgir_to_llir(src, extern_libs, arch, tma_infos))\n+        _device_backend.add_stages(arch, extern_libs, stages)\n     else:\n+        # pass the user's configuration to the backend device.\n+        arch[\"num_warps\"] = num_warps\n+        arch[\"num_stages\"] = num_stages\n+        arch[\"num_ctas\"] = num_ctas\n         _device_backend.add_stages(arch, extern_libs, stages)\n \n     # find out the signature of the function\n@@ -532,7 +486,7 @@ def compile(fn, **kwargs):\n             path = metadata_group.get(ir_filename)\n             if path is None:\n                 next_module = compile_kernel(module)\n-                if ir == \"amdgcn\":\n+                if ir_name == \"amdgcn\":\n                     extra_file_name = f\"{name}.hsaco_path\"\n                     metadata_group[ir_filename] = fn_cache_manager.put(next_module[0], ir_filename)\n                     metadata_group[extra_file_name] = fn_cache_manager.put(next_module[1], extra_file_name)\n@@ -555,17 +509,23 @@ def compile(fn, **kwargs):\n         else:\n             asm[ir_name] = str(next_module)\n         if ir_name == \"llir\" and \"shared\" not in metadata:\n-            metadata[\"shared\"] = get_shared_memory_size(module)\n+            if is_hip():\n+                metadata[\"shared\"] = _device_backend.get_shared_memory_size(module)\n+            else:\n+                metadata[\"shared\"] = get_shared_memory_size(module)\n         if ir_name == \"ttgir\":\n-            metadata[\"enable_warp_specialization\"] = ir.is_ws_supported(next_module)\n-            if metadata[\"enable_warp_specialization\"]:\n-                metadata[\"num_warps\"] = get_num_warps(next_module)\n+            if is_hip():\n+                metadata[\"num_warps\"] = _device_backend.get_num_warps(next_module)\n+            else:\n+                metadata[\"enable_warp_specialization\"] = ir.is_ws_supported(next_module)\n+                if metadata[\"enable_warp_specialization\"]:\n+                    metadata[\"num_warps\"] = get_num_warps(next_module)\n         if ir_name == \"ptx\":\n             metadata[\"name\"] = get_kernel_name(next_module, pattern='// .globl')\n         if ir_name == \"amdgcn\":\n             metadata[\"name\"] = get_kernel_name(next_module[0], pattern='.globl')\n             asm[\"hsaco_path\"] = next_module[1]\n-        if not is_cuda and not is_hip:\n+        if not is_cuda and not is_hip():\n             _device_backend.add_meta_info(ir_name, module, next_module, metadata, asm)\n         module = next_module\n \n@@ -590,7 +550,7 @@ def compile(fn, **kwargs):\n     ids_of_const_exprs = tuple(fn.constexprs) if isinstance(fn, JITFunction) else ()\n     ids = {\"ids_of_tensormaps\": ids_of_tensormaps, \"ids_of_folded_args\": ids_of_folded_args, \"ids_of_const_exprs\": ids_of_const_exprs}\n     # cache manager\n-    if is_cuda or is_hip:\n+    if is_cuda:\n         so_path = make_stub(name, signature, constants, ids, enable_warp_specialization=enable_warp_specialization)\n     else:\n         so_path = _device_backend.make_launcher_stub(name, signature, constants, ids)\n@@ -628,7 +588,7 @@ def __init__(self, fn, so_path, metadata, asm):\n             self.tensormaps_info = metadata[\"tensormaps_info\"]\n         self.constants = metadata[\"constants\"]\n         self.device_type = metadata[\"device_type\"]\n-        self.device_backend = get_backend(self.device_type) if self.device_type not in [\"cuda\", \"hip\"] else None\n+        self.device_backend = get_backend(self.device_type) if self.device_type not in [\"cuda\"] else None\n         # initialize asm dict\n         self.asm = asm\n         # binaries are lazily initialized\n@@ -642,7 +602,7 @@ def _init_handles(self):\n         if self.cu_module is not None:\n             return\n \n-        if self.device_type in [\"cuda\", \"hip\"]:\n+        if self.device_type in [\"cuda\"]:\n             device = get_current_device()\n             bin_path = {\n                 driver.HIP: \"hsaco_path\",\n@@ -688,7 +648,7 @@ def __getitem__(self, grid):\n         def runner(*args, stream=None):\n             args_expand = self.assemble_tensormap_to_arg(args)\n             if stream is None:\n-                if self.device_type in [\"cuda\", \"hip\"]:\n+                if self.device_type in [\"cuda\"]:\n                     stream = get_cuda_stream()\n                 else:\n                     stream = get_backend(self.device_type).get_stream(None)"}, {"filename": "python/triton/compiler/make_launcher.py", "status": "modified", "additions": 14, "deletions": 157, "changes": 171, "file_content_changes": "@@ -3,16 +3,11 @@\n import tempfile\n \n from ..common import _build\n+from ..common.build import is_hip\n from ..runtime.cache import get_cache_manager\n from ..runtime.jit import version_key\n from .utils import generate_cu_signature\n \n-\n-def is_hip():\n-    import torch\n-    return torch.version.hip is not None\n-\n-\n # ----- stub --------\n \n \n@@ -103,150 +98,9 @@ def format_of(ty):\n     format = \"iiiiiiiiiKKOOO\" + ''.join([format_of(_extracted_type(ty)) for ty in signature.values()])\n \n     # generate glue code\n-    if is_hip():\n-        src = f\"\"\"\n-    #define __HIP_PLATFORM_AMD__\n-    #include <hip/hip_runtime.h>\n-    #include <Python.h>\n-    #include <stdio.h>\n-\n-    static inline void gpuAssert(hipError_t code, const char *file, int line)\n-    {{\n-      if (code != HIP_SUCCESS)\n-      {{\n-         const char* prefix = \"Triton Error [HIP]: \";\n-         const char* str = hipGetErrorString(code);\n-         char err[1024] = {{0}};\n-         snprintf(err, 1024, \"%s Code: %d, Messsage: %s\", prefix, code, str );\n-         PyErr_SetString(PyExc_RuntimeError, err);\n-      }}\n-    }}\n-\n-    #define HIP_CHECK(ans) {{ gpuAssert((ans), __FILE__, __LINE__); }}\n-\n-    static void _launch(int gridX, int gridY, int gridZ, int num_warps, int shared_memory, hipStream_t stream, hipFunction_t function, {arg_decls}) {{\n-      void *params[] = {{ {', '.join(f\"&arg{i}\" for i in signature.keys() if i not in constants)} }};\n-      if (gridX*gridY*gridZ > 0) {{\n-          HIP_CHECK(hipModuleLaunchKernel(function, gridX, gridY, gridZ, 64*num_warps, 1, 1, shared_memory, stream, params, 0));\n-      }}\n-    }}\n-\n-    typedef struct _DevicePtrInfo {{\n-      hipDeviceptr_t dev_ptr;\n-      bool valid;\n-    }} DevicePtrInfo;\n-\n-    static inline DevicePtrInfo getPointer(PyObject *obj, int idx) {{\n-      DevicePtrInfo ptr_info;\n-      ptr_info.dev_ptr = 0;\n-      ptr_info.valid = true;\n-\n-      if (PyLong_Check(obj)) {{\n-        ptr_info.dev_ptr = (hipDeviceptr_t)PyLong_AsUnsignedLongLong(obj);\n-        return ptr_info;\n-      }}\n-\n-      if (obj == Py_None) {{\n-        // valid nullptr\n-        return ptr_info;\n-      }}\n-\n-      PyObject *ptr = PyObject_GetAttrString(obj, \"data_ptr\");\n-\n-      if (ptr) {{\n-        PyObject *empty_tuple = PyTuple_New(0);\n-        PyObject *ret = PyObject_Call(ptr, empty_tuple, NULL);\n-        Py_DECREF(empty_tuple);\n-        Py_DECREF(ptr);\n-\n-        if (!PyLong_Check(ret)) {{\n-          PyErr_SetString(PyExc_TypeError, \"data_ptr method of Pointer object must return 64-bit int\");\n-          ptr_info.valid = false;\n-          return ptr_info;\n-        }}\n-\n-        ptr_info.dev_ptr = (hipDeviceptr_t)PyLong_AsUnsignedLongLong(ret);\n-\n-        if (!ptr_info.dev_ptr)\n-          return ptr_info;\n-\n-        uint64_t dev_ptr;\n-        hipError_t status = hipPointerGetAttribute(&dev_ptr, HIP_POINTER_ATTRIBUTE_DEVICE_POINTER, ptr_info.dev_ptr);\n-        if (status == hipErrorInvalidValue) {{\n-            PyErr_Format(PyExc_ValueError,\n-                         \"Pointer argument (at %d) cannot be accessed from Triton (cpu tensor?)\", idx);\n-            ptr_info.valid = false;\n-        }}\n-\n-        ptr_info.dev_ptr = (hipDeviceptr_t)dev_ptr;\n-        return ptr_info;\n-      }}\n-\n-      PyErr_SetString(PyExc_TypeError, \"Pointer argument must be either uint64 or have data_ptr method\");\n-      return ptr_info;\n-    }}\n-\n-    static PyObject* launch(PyObject* self, PyObject* args) {{\n-\n-      int gridX, gridY, gridZ;\n-      uint64_t _stream;\n-      uint64_t _function;\n-      int num_warps;\n-      int shared_memory;\n-      PyObject *launch_enter_hook = NULL;\n-      PyObject *launch_exit_hook = NULL;\n-      PyObject *compiled_kernel = NULL;\n-\n-      {' '.join([f\"{_extracted_type(ty)} _arg{i}; \" for i, ty in signature.items()])}\n-      if (!PyArg_ParseTuple(args, \\\"{format}\\\", &gridX, &gridY, &gridZ, &num_warps, &shared_memory, &_stream, &_function, &launch_enter_hook, &launch_exit_hook, &compiled_kernel{', ' + ', '.join(f\"&_arg{i}\" for i, ty in signature.items()) if len(signature) > 0 else ''})) {{\n-        return NULL;\n-      }}\n-\n-      if (launch_enter_hook != Py_None) {{\n-        PyObject_CallObject(launch_enter_hook, args);\n-      }}\n-\n-      // raise exception asap\n-      {\"; \".join([f\"DevicePtrInfo ptr_info{i} = getPointer(_arg{i}, {i}); if (!ptr_info{i}.valid) return NULL;\" if ty[0] == \"*\" else \"\" for i, ty in signature.items()])};\n-      _launch(gridX, gridY, gridZ, num_warps, shared_memory, (hipStream_t)_stream, (hipFunction_t)_function{', ' + ', '.join(f\"ptr_info{i}.dev_ptr\" if ty[0]==\"*\" else f\"_arg{i}\" for i, ty in signature.items()) if len(signature) > 0 else ''});\n-      if (launch_exit_hook != Py_None) {{\n-        PyObject_CallObject(launch_exit_hook, args);\n-      }}\n-      if (PyErr_Occurred()) {{\n-        return NULL;\n-      }}\n-\n-      // return None\n-      Py_INCREF(Py_None);\n-      return Py_None;\n-    }}\n-\n-    static PyMethodDef ModuleMethods[] = {{\n-      {{\"launch\", launch, METH_VARARGS, \"Entry point for all kernels with this signature\"}},\n-      {{NULL, NULL, 0, NULL}} // sentinel\n-    }};\n-\n-    static struct PyModuleDef ModuleDef = {{\n-      PyModuleDef_HEAD_INIT,\n-      \\\"__triton_launcher\\\",\n-      NULL, //documentation\n-      -1, //size\n-      ModuleMethods\n-    }};\n-\n-    PyMODINIT_FUNC PyInit___triton_launcher(void) {{\n-      PyObject *m = PyModule_Create(&ModuleDef);\n-      if(m == NULL) {{\n-        return NULL;\n-      }}\n-      PyModule_AddFunctions(m, ModuleMethods);\n-      return m;\n-    }}\n-    \"\"\"\n-    else:\n-        folded_without_constexprs = [c for c in ids['ids_of_folded_args'] if c not in ids['ids_of_const_exprs']]\n-        params = [i for i in signature.keys() if i >= start_desc or (i not in constants and i not in folded_without_constexprs)]\n-        src = f\"\"\"\n+    folded_without_constexprs = [c for c in ids['ids_of_folded_args'] if c not in ids['ids_of_const_exprs']]\n+    params = [i for i in signature.keys() if i >= start_desc or (i not in constants and i not in folded_without_constexprs)]\n+    src = f\"\"\"\n #include \\\"cuda.h\\\"\n #include <stdbool.h>\n #include <Python.h>\n@@ -262,7 +116,10 @@ def format_of(ty):\n       char err[1024] = {{0}};\n       strcat(err, prefix);\n       strcat(err, str);\n+      PyGILState_STATE gil_state;\n+      gil_state = PyGILState_Ensure();\n       PyErr_SetString(PyExc_RuntimeError, err);\n+      PyGILState_Release(gil_state);\n    }}\n }}\n \n@@ -365,6 +222,7 @@ def format_of(ty):\n     return ptr_info;\n   }}\n   PyErr_SetString(PyExc_TypeError, \"Pointer argument must be either uint64 or have data_ptr method\");\n+  ptr_info.valid = false;\n   return ptr_info;\n }}\n \n@@ -386,22 +244,21 @@ def format_of(ty):\n     return NULL;\n   }}\n \n-  if (launch_enter_hook != Py_None) {{\n-    PyObject_CallObject(launch_enter_hook, args);\n+  if (launch_enter_hook != Py_None && !PyObject_CallObject(launch_enter_hook, args)) {{\n+    return NULL;\n   }}\n \n \n   // raise exception asap\n   {\"; \".join([f\"DevicePtrInfo ptr_info{i} = getPointer(_arg{i}, {i}); if (!ptr_info{i}.valid) return NULL;\" if ty[0] == \"*\" else \"\" for i, ty in signature.items()])};\n+  Py_BEGIN_ALLOW_THREADS;\n   _launch(gridX, gridY, gridZ, num_warps, num_ctas, clusterDimX, clusterDimY, clusterDimZ, shared_memory, (CUstream)_stream, (CUfunction)_function{', ' + ', '.join(f\"ptr_info{i}.dev_ptr\" if ty[0]==\"*\" else f\"_arg{i}\"for i, ty in signature.items()) if len(signature) > 0 else ''});\n+  Py_END_ALLOW_THREADS;\n \n-  if (launch_exit_hook != Py_None) {{\n-    PyObject_CallObject(launch_exit_hook, args);\n-  }}\n-\n-  if(PyErr_Occurred()) {{\n+  if (launch_exit_hook != Py_None && !PyObject_CallObject(launch_exit_hook, args)) {{\n     return NULL;\n   }}\n+\n   // return None\n   Py_INCREF(Py_None);\n   return Py_None;"}, {"filename": "python/triton/hopper_lib/libhopper_helpers.bc", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -4,11 +4,21 @@\n from . import math\n from . import extra\n from .standard import (\n+    argmax,\n+    argmin,\n     cdiv,\n+    cumprod,\n+    cumsum,\n+    max,\n+    maximum,\n+    min,\n+    minimum,\n     sigmoid,\n     softmax,\n+    sum,\n     ravel,\n     swizzle2d,\n+    xor_sum,\n     zeros,\n     zeros_like,\n )\n@@ -17,8 +27,6 @@\n     abs,\n     advance,\n     arange,\n-    argmin,\n-    argmax,\n     associative_scan,\n     atomic_add,\n     atomic_and,\n@@ -35,8 +43,6 @@\n     cat,\n     constexpr,\n     cos,\n-    cumprod,\n-    cumsum,\n     debug_barrier,\n     device_assert,\n     device_print,\n@@ -63,12 +69,8 @@\n     load,\n     log,\n     make_block_ptr,\n-    max,\n     max_constancy,\n     max_contiguous,\n-    maximum,\n-    min,\n-    minimum,\n     multiple_of,\n     num_programs,\n     pi32_t,\n@@ -81,7 +83,6 @@\n     static_assert,\n     static_print,\n     store,\n-    sum,\n     static_range,\n     tensor,\n     trans,\n@@ -94,7 +95,6 @@\n     view,\n     void,\n     where,\n-    xor_sum,\n )\n from .random import (\n     pair_uniform_to_normal,"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 5, "deletions": 191, "changes": 196, "file_content_changes": "@@ -6,8 +6,7 @@\n from typing import Callable, List, Sequence, TypeVar\n \n from .._C.libtriton.triton import ir\n-from ..runtime.jit import jit\n-from . import math, semantic\n+from . import semantic\n \n T = TypeVar('T')\n \n@@ -205,6 +204,10 @@ def is_int(self):\n     def is_bool(self):\n         return self.is_int1()\n \n+    @staticmethod\n+    def is_dtype(type_str):\n+        return type_str in dtype.SINT_TYPES + dtype.UINT_TYPES + dtype.FP_TYPES + dtype.OTHER_TYPES\n+\n     @staticmethod\n     def is_void():\n         raise RuntimeError(\"Not implemented\")\n@@ -1380,170 +1383,6 @@ def _reduce_with_indices(input, axis, combine_fn, _builder=None, _generator=None\n     return rvalue, rindices\n \n \n-@jit\n-def minimum(x, y):\n-    \"\"\"\n-    Computes the element-wise minimum of :code:`x` and :code:`y`.\n-\n-    :param input: the first input tensor\n-    :type input: Block\n-    :param other: the second input tensor\n-    :type other: Block\n-    \"\"\"\n-    return math.min(x, y)\n-\n-\n-@jit\n-def maximum(x, y):\n-    \"\"\"\n-    Computes the element-wise maximum of :code:`x` and :code:`y`.\n-\n-    :param input: the first input tensor\n-    :type input: Block\n-    :param other: the second input tensor\n-    :type other: Block\n-    \"\"\"\n-    return math.max(x, y)\n-\n-# max and argmax\n-\n-\n-@jit\n-def _argmax_combine(value1, index1, value2, index2, tie_break_left):\n-    if tie_break_left:\n-        tie = value1 == value2 and index1 < index2\n-    else:\n-        tie = False\n-    gt = value1 > value2 or tie\n-    v_ret = where(gt, value1, value2)\n-    i_ret = where(gt, index1, index2)\n-    return v_ret, i_ret\n-\n-\n-@jit\n-def _argmax_combine_tie_break_left(value1, index1, value2, index2):\n-    return _argmax_combine(value1, index1, value2, index2, True)\n-\n-\n-@jit\n-def _argmax_combine_tie_break_fast(value1, index1, value2, index2):\n-    return _argmax_combine(value1, index1, value2, index2, False)\n-\n-\n-@jit\n-@_add_reduction_docstr(\"maximum\",\n-                       return_indices_arg=\"return_indices\",\n-                       tie_break_arg=\"return_indices_tie_break_left\")\n-def max(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n-    input = _promote_reduction_input(input)\n-    if return_indices:\n-        if return_indices_tie_break_left:\n-            return _reduce_with_indices(input, axis, _argmax_combine_tie_break_left)\n-        else:\n-            return _reduce_with_indices(input, axis, _argmax_combine_tie_break_fast)\n-    else:\n-        if constexpr(input.dtype.primitive_bitwidth) < 32:\n-            if constexpr(input.dtype.is_floating()):\n-                input = input.to(float32)\n-            else:\n-                assert input.dtype.is_integer_type()\n-                input = input.to(int32)\n-        return reduce(input, axis, maximum)\n-\n-\n-@jit\n-@_add_reduction_docstr(\"maximum index\", tie_break_arg=\"tie_break_left\")\n-def argmax(input, axis, tie_break_left=True):\n-    (_, ret) = max(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n-    return ret\n-\n-# min and argmin\n-\n-\n-@jit\n-def _argmin_combine(value1, index1, value2, index2, tie_break_left):\n-    if tie_break_left:\n-        tie = value1 == value2 and index1 < index2\n-    else:\n-        tie = False\n-    lt = value1 < value2 or tie\n-    value_ret = where(lt, value1, value2)\n-    index_ret = where(lt, index1, index2)\n-    return value_ret, index_ret\n-\n-\n-@jit\n-def _argmin_combine_tie_break_left(value1, index1, value2, index2):\n-    return _argmin_combine(value1, index1, value2, index2, True)\n-\n-\n-@jit\n-def _argmin_combine_tie_break_fast(value1, index1, value2, index2):\n-    return _argmin_combine(value1, index1, value2, index2, False)\n-\n-\n-@jit\n-@_add_reduction_docstr(\"minimum\",\n-                       return_indices_arg=\"return_indices\",\n-                       tie_break_arg=\"return_indices_tie_break_left\")\n-def min(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n-    input = _promote_reduction_input(input)\n-    if return_indices:\n-        if return_indices_tie_break_left:\n-            return _reduce_with_indices(input, axis, _argmin_combine_tie_break_left)\n-        else:\n-            return _reduce_with_indices(input, axis, _argmin_combine_tie_break_fast)\n-    else:\n-        if constexpr(input.dtype.primitive_bitwidth) < 32:\n-            if constexpr(input.dtype.is_floating()):\n-                input = input.to(float32)\n-            else:\n-                assert input.dtype.is_integer_type()\n-                input = input.to(int32)\n-        return reduce(input, axis, minimum)\n-\n-\n-@jit\n-@_add_reduction_docstr(\"minimum index\",\n-                       tie_break_arg=\"tie_break_left\")\n-def argmin(input, axis, tie_break_left=True):\n-    _, ret = min(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n-    return ret\n-\n-\n-@jit\n-def _sum_combine(a, b):\n-    return a + b\n-\n-# sum\n-\n-\n-@jit\n-@_add_reduction_docstr(\"sum\")\n-def sum(input, axis=None):\n-    input = _promote_reduction_input(input)\n-    return reduce(input, axis, _sum_combine)\n-\n-\n-@jit\n-def _xor_combine(a, b):\n-    return a ^ b\n-\n-\n-# xor sum\n-\n-@builtin\n-@_add_reduction_docstr(\"xor sum\")\n-def xor_sum(input, axis=None, _builder=None, _generator=None):\n-    scalar_ty = input.type.scalar\n-    if not scalar_ty.is_int():\n-        raise ValueError(\"xor_sum only supported for integers\")\n-\n-    input = _promote_reduction_input(input, _builder=_builder)\n-    return reduce(input, axis, _xor_combine,\n-                  _builder=_builder, _generator=_generator)\n-\n-\n # -----------------------\n # Scans\n # -----------------------\n@@ -1594,31 +1433,6 @@ def make_combine_region(scan_op):\n     axis = _constexpr_to_value(axis)\n     return semantic.associative_scan(input, axis, make_combine_region, _builder)\n \n-# cumsum\n-\n-\n-@jit\n-@_add_scan_docstr(\"cumsum\")\n-def cumsum(input, axis=0):\n-    # todo rename this to a generic function name\n-    input = _promote_reduction_input(input)\n-    return associative_scan(input, axis, _sum_combine)\n-\n-# cumprod\n-\n-\n-@jit\n-def _prod_combine(a, b):\n-    return a * b\n-\n-\n-@jit\n-@_add_scan_docstr(\"cumprod\")\n-def cumprod(input, axis=0):\n-    # todo rename this to a generic function name\n-    input = _promote_reduction_input(input)\n-    return associative_scan(input, axis, _prod_combine)\n-\n # -----------------------\n # Compiler Hint Ops\n # -----------------------"}, {"filename": "python/triton/language/math.py", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -1,17 +1,18 @@\n import functools\n import os\n \n+from ..common.build import is_hip\n from . import core\n \n \n @functools.lru_cache()\n def libdevice_path():\n-    import torch\n     third_party_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\")\n-    if torch.version.hip is None:\n-        default = os.path.join(third_party_dir, \"cuda\", \"lib\", \"libdevice.10.bc\")\n+    if is_hip():\n+        default = os.path.join(third_party_dir, \"hip\", \"lib\", \"bitcode\", \"cuda2gcn.bc\")\n     else:\n-        default = ''\n+        default = os.path.join(third_party_dir, \"cuda\", \"lib\", \"libdevice.10.bc\")\n+\n     return os.getenv(\"TRITON_LIBDEVICE_PATH\", default)\n \n "}, {"filename": "python/triton/language/random.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -1,5 +1,6 @@\n from ..runtime.jit import jit\n from . import core as tl\n+from . import standard\n \n PHILOX_KEY_A: tl.constexpr = 0x9E3779B9\n PHILOX_KEY_B: tl.constexpr = 0xBB67AE85\n@@ -141,7 +142,7 @@ def rand4x(seed, offsets, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n @jit\n def pair_uniform_to_normal(u1, u2):\n     \"\"\"Box-Muller transform\"\"\"\n-    u1 = tl.maximum(1.0e-7, u1)\n+    u1 = standard.maximum(1.0e-7, u1)\n     th = 6.283185307179586 * u2\n     r = tl.sqrt(-2.0 * tl.log(u1))\n     return r * tl.cos(th), r * tl.sin(th)"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 40, "deletions": 0, "changes": 40, "file_content_changes": "@@ -4,6 +4,7 @@\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n from .._C.libtriton.triton import ir\n+from ..common.build import is_hip\n from . import core as tl\n \n T = TypeVar('T')\n@@ -1239,6 +1240,19 @@ def atomic_xchg(ptr: tl.tensor,\n # ===----------------------------------------------------------------------===//\n \n \n+def gpu_has_mfma() -> bool:\n+    if not is_hip():\n+        return False\n+    return True  # mfma supported in ['gfx908', 'gfx90a']\n+\n+\n+def mfma_supported(M, N, K, allow_tf32, ret_scalar_ty) -> bool:\n+    if not gpu_has_mfma():\n+        return False\n+    # TODO: Add check for configurations and types.\n+    return True\n+\n+\n def dot(lhs: tl.tensor,\n         rhs: tl.tensor,\n         allow_tf32: bool,\n@@ -1292,6 +1306,32 @@ def assert_dtypes_valid(lhs_dtype, rhs_dtype, arch):\n \n     M = lhs.type.shape[0]\n     N = rhs.type.shape[1]\n+\n+    # Cast operands of types f16 and i8 for configurations where FMA only supported.\n+    if is_hip() and not mfma_supported(M, N, lhs.type.shape[1], allow_tf32, ret_scalar_ty):\n+        ret_cast_scalar_ty = tl.float32 if lhs.type.scalar.is_int() else ret_scalar_ty\n+        lhs = cast(lhs, ret_cast_scalar_ty, builder)\n+        rhs = cast(rhs, ret_cast_scalar_ty, builder)\n+        if ret_cast_scalar_ty == tl.float16:\n+            _0 = builder.create_splat(builder.get_fp16(0), [M, N])\n+        else:\n+            _0 = builder.create_splat(builder.get_fp32(0), [M, N])\n+        ret_ty = tl.block_type(ret_cast_scalar_ty, [M, N])\n+        ret = tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32),\n+                        ret_ty)\n+        return cast(ret, ret_scalar_ty, builder)\n+    if is_hip() and mfma_supported(M, N, lhs.type.shape[1], allow_tf32, ret_scalar_ty) and ret_scalar_ty.primitive_bitwidth < 32:\n+        if lhs.type.scalar.is_int():\n+            ret_dot_scalar_ty = tl.int32\n+            _0 = builder.create_splat(builder.get_int32(0), [M, N])\n+        else:\n+            ret_dot_scalar_ty = tl.float32\n+            _0 = builder.create_splat(builder.get_fp32(0), [M, N])\n+        ret_ty = tl.block_type(ret_dot_scalar_ty, [M, N])\n+        ret = tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32),\n+                        ret_ty)\n+        return cast(ret, ret_scalar_ty, builder)\n+\n     _0 = builder.create_splat(_0, [M, N])\n     ret_ty = tl.block_type(ret_scalar_ty, [M, N])\n     return tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32),"}, {"filename": "python/triton/language/standard.py", "status": "modified", "additions": 193, "deletions": 4, "changes": 197, "file_content_changes": "@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n from ..runtime.jit import jit\n-from . import core\n+from . import core, math\n \n # -----------------------\n # Standard library\n@@ -30,9 +30,9 @@ def sigmoid(x):\n @jit\n @core._add_math_1arg_docstr(\"softmax\")\n def softmax(x, ieee_rounding=False):\n-    z = x - core.max(x, 0)\n+    z = x - max(x, 0)\n     num = core.exp(z)\n-    den = core.sum(num, 0)\n+    den = sum(num, 0)\n     return core.fdiv(num, den, ieee_rounding)\n \n \n@@ -73,7 +73,7 @@ def swizzle2d(i, j, size_i, size_j, size_g):\n     # row-index of the first element of this group\n     off_i = group_id * size_g\n     # last group may have fewer rows\n-    size_g = core.minimum(size_i - off_i, size_g)\n+    size_g = minimum(size_i - off_i, size_g)\n     # new row and column indices\n     new_i = off_i + (ij % size_g)\n     new_j = (ij % size_gj) // size_g\n@@ -96,3 +96,192 @@ def zeros(shape, dtype):\n @jit\n def zeros_like(input):\n     return zeros(input.shape, input.dtype)\n+\n+\n+@jit\n+def minimum(x, y):\n+    \"\"\"\n+    Computes the element-wise minimum of :code:`x` and :code:`y`.\n+\n+    :param input: the first input tensor\n+    :type input: Block\n+    :param other: the second input tensor\n+    :type other: Block\n+    \"\"\"\n+    return math.min(x, y)\n+\n+\n+@jit\n+def maximum(x, y):\n+    \"\"\"\n+    Computes the element-wise maximum of :code:`x` and :code:`y`.\n+\n+    :param input: the first input tensor\n+    :type input: Block\n+    :param other: the second input tensor\n+    :type other: Block\n+    \"\"\"\n+    return math.max(x, y)\n+\n+# max and argmax\n+\n+\n+@jit\n+def _argmax_combine(value1, index1, value2, index2, tie_break_left):\n+    if tie_break_left:\n+        tie = value1 == value2 and index1 < index2\n+    else:\n+        tie = False\n+    gt = value1 > value2 or tie\n+    v_ret = core.where(gt, value1, value2)\n+    i_ret = core.where(gt, index1, index2)\n+    return v_ret, i_ret\n+\n+\n+@jit\n+def _argmax_combine_tie_break_left(value1, index1, value2, index2):\n+    return _argmax_combine(value1, index1, value2, index2, True)\n+\n+\n+@jit\n+def _argmax_combine_tie_break_fast(value1, index1, value2, index2):\n+    return _argmax_combine(value1, index1, value2, index2, False)\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"maximum\",\n+                            return_indices_arg=\"return_indices\",\n+                            tie_break_arg=\"return_indices_tie_break_left\")\n+def max(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n+    input = core._promote_reduction_input(input)\n+    if return_indices:\n+        if return_indices_tie_break_left:\n+            return core._reduce_with_indices(input, axis, _argmax_combine_tie_break_left)\n+        else:\n+            return core._reduce_with_indices(input, axis, _argmax_combine_tie_break_fast)\n+    else:\n+        if core.constexpr(input.dtype.primitive_bitwidth) < 32:\n+            if core.constexpr(input.dtype.is_floating()):\n+                input = input.to(core.float32)\n+            else:\n+                assert input.dtype.is_integer_type()\n+                input = input.to(core.int32)\n+        return core.reduce(input, axis, maximum)\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"maximum index\", tie_break_arg=\"tie_break_left\")\n+def argmax(input, axis, tie_break_left=True):\n+    (_, ret) = max(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n+    return ret\n+\n+# min and argmin\n+\n+\n+@jit\n+def _argmin_combine(value1, index1, value2, index2, tie_break_left):\n+    if tie_break_left:\n+        tie = value1 == value2 and index1 < index2\n+    else:\n+        tie = False\n+    lt = value1 < value2 or tie\n+    value_ret = core.where(lt, value1, value2)\n+    index_ret = core.where(lt, index1, index2)\n+    return value_ret, index_ret\n+\n+\n+@jit\n+def _argmin_combine_tie_break_left(value1, index1, value2, index2):\n+    return _argmin_combine(value1, index1, value2, index2, True)\n+\n+\n+@jit\n+def _argmin_combine_tie_break_fast(value1, index1, value2, index2):\n+    return _argmin_combine(value1, index1, value2, index2, False)\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"minimum\",\n+                            return_indices_arg=\"return_indices\",\n+                            tie_break_arg=\"return_indices_tie_break_left\")\n+def min(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n+    input = core._promote_reduction_input(input)\n+    if return_indices:\n+        if return_indices_tie_break_left:\n+            return core._reduce_with_indices(input, axis, _argmin_combine_tie_break_left)\n+        else:\n+            return core._reduce_with_indices(input, axis, _argmin_combine_tie_break_fast)\n+    else:\n+        if core.constexpr(input.dtype.primitive_bitwidth) < 32:\n+            if core.constexpr(input.dtype.is_floating()):\n+                input = input.to(core.float32)\n+            else:\n+                assert input.dtype.is_integer_type()\n+                input = input.to(core.int32)\n+        return core.reduce(input, axis, minimum)\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"minimum index\",\n+                            tie_break_arg=\"tie_break_left\")\n+def argmin(input, axis, tie_break_left=True):\n+    _, ret = min(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n+    return ret\n+\n+\n+@jit\n+def _sum_combine(a, b):\n+    return a + b\n+\n+# sum\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"sum\")\n+def sum(input, axis=None):\n+    input = core._promote_reduction_input(input)\n+    return core.reduce(input, axis, _sum_combine)\n+\n+\n+@jit\n+def _xor_combine(a, b):\n+    return a ^ b\n+\n+# xor sum\n+\n+\n+@core.builtin\n+@core._add_reduction_docstr(\"xor sum\")\n+def xor_sum(input, axis=None, _builder=None, _generator=None):\n+    scalar_ty = input.type.scalar\n+    if not scalar_ty.is_int():\n+        raise ValueError(\"xor_sum only supported for integers\")\n+\n+    input = core._promote_reduction_input(input, _builder=_builder)\n+    return core.reduce(input, axis, _xor_combine,\n+                       _builder=_builder, _generator=_generator)\n+\n+# cumsum\n+\n+\n+@jit\n+@core._add_scan_docstr(\"cumsum\")\n+def cumsum(input, axis=0):\n+    # todo rename this to a generic function name\n+    input = core._promote_reduction_input(input)\n+    return core.associative_scan(input, axis, _sum_combine)\n+\n+# cumprod\n+\n+\n+@jit\n+def _prod_combine(a, b):\n+    return a * b\n+\n+\n+@jit\n+@core._add_scan_docstr(\"cumprod\")\n+def cumprod(input, axis=0):\n+    # todo rename this to a generic function name\n+    input = core._promote_reduction_input(input)\n+    return core.associative_scan(input, axis, _prod_combine)"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -21,7 +21,7 @@ def _fwd_kernel(\n     Out,\n     stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n-    stride_vz, stride_vh, stride_vk, stride_vn,\n+    stride_vz, stride_vh, stride_vn, stride_vk,\n     stride_oz, stride_oh, stride_om, stride_on,\n     Z, H, N_CTX,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n@@ -50,7 +50,7 @@ def _fwd_kernel(\n     V_block_ptr = tl.make_block_ptr(\n         base=V + qvk_offset,\n         shape=(N_CTX, BLOCK_DMODEL),\n-        strides=(stride_vk, stride_vn),\n+        strides=(stride_vn, stride_vk),\n         offsets=(0, 0),\n         block_shape=(BLOCK_N, BLOCK_DMODEL),\n         order=(1, 0)\n@@ -137,7 +137,7 @@ def _bwd_kernel_one_col_block(\n     D,\n     stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n-    stride_vz, stride_vh, stride_vk, stride_vn,\n+    stride_vz, stride_vh, stride_vn, stride_vk,\n     Z, H, N_CTX,\n     off_hz, start_n, num_block,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n@@ -159,7 +159,7 @@ def _bwd_kernel_one_col_block(\n     # initialize pointers to value-like data\n     q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n     k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n-    v_ptrs = V + (offs_n[:, None] * stride_vk + offs_k[None, :] * stride_vn)\n+    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_k[None, :] * stride_vk)\n     do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n     dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n     # pointer to row-wise quantities in value-like data\n@@ -212,7 +212,7 @@ def _bwd_kernel_one_col_block(\n         q_ptrs += BLOCK_M * stride_qm\n         do_ptrs += BLOCK_M * stride_qm\n     # write-back\n-    dv_ptrs = DV + (offs_n[:, None] * stride_vk + offs_k[None, :] * stride_vn)\n+    dv_ptrs = DV + (offs_n[:, None] * stride_vn + offs_k[None, :] * stride_vk)\n     dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n     tl.store(dv_ptrs, dv)\n     tl.store(dk_ptrs, dk)\n@@ -228,7 +228,7 @@ def _bwd_kernel(\n     D,\n     stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n-    stride_vz, stride_vh, stride_vk, stride_vn,\n+    stride_vz, stride_vh, stride_vn, stride_vk,\n     Z, H, N_CTX,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n@@ -259,7 +259,7 @@ def _bwd_kernel(\n                 D,\n                 stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n                 stride_kz, stride_kh, stride_kn, stride_kk,\n-                stride_vz, stride_vh, stride_vk, stride_vn,\n+                stride_vz, stride_vh, stride_vn, stride_vk,\n                 Z, H, N_CTX,\n                 off_hz, start_n, num_block_n,\n                 BLOCK_M=BLOCK_M, BLOCK_DMODEL=BLOCK_DMODEL,\n@@ -276,7 +276,7 @@ def _bwd_kernel(\n             D,\n             stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n             stride_kz, stride_kh, stride_kn, stride_kk,\n-            stride_vz, stride_vh, stride_vk, stride_vn,\n+            stride_vz, stride_vh, stride_vn, stride_vk,\n             Z, H, N_CTX,\n             off_hz, start_n, num_block_n,\n             BLOCK_M=BLOCK_M, BLOCK_DMODEL=BLOCK_DMODEL,"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -81,6 +81,7 @@ def _kernel(A, B, C, M, N, K,\n             stride_bk, stride_bn,\n             stride_cm, stride_cn,\n             dot_out_dtype: tl.constexpr,\n+            allow_tf32: tl.constexpr,\n             BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n             GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr, AB_DTYPE: tl.constexpr\n             ):\n@@ -117,7 +118,7 @@ def _kernel(A, B, C, M, N, K,\n         if AB_DTYPE:\n             a = a.to(C.dtype.element_ty)\n             b = b.to(C.dtype.element_ty)\n-        acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n+        acc += tl.dot(a, b, out_dtype=dot_out_dtype, allow_tf32=allow_tf32)\n         A += BLOCK_K * SPLIT_K * stride_ak\n         B += BLOCK_K * SPLIT_K * stride_bk\n     acc = acc.to(C.dtype.element_ty)\n@@ -139,7 +140,7 @@ class _matmul(torch.autograd.Function):\n     _locks = {}\n \n     @staticmethod\n-    def _call(a, b, dot_out_dtype):\n+    def _call(a, b, dot_out_dtype, allow_tf32):\n         device = a.device\n         # handle non-contiguous inputs if necessary\n         if a.stride(0) > 1 and a.stride(1) > 1:\n@@ -180,12 +181,13 @@ def _call(a, b, dot_out_dtype):\n                       b.stride(0), b.stride(1),\n                       c.stride(0), c.stride(1),\n                       dot_out_dtype=dot_out_dtype,\n+                      allow_tf32=allow_tf32,\n                       GROUP_M=8, AB_DTYPE=ab_dtype)\n         return c\n \n     @staticmethod\n-    def forward(ctx, a, b, dot_out_dtype=None):\n-        return _matmul._call(a, b, dot_out_dtype=dot_out_dtype)\n+    def forward(ctx, a, b, dot_out_dtype=None, allow_tf32=True):\n+        return _matmul._call(a, b, dot_out_dtype=dot_out_dtype, allow_tf32=allow_tf32)\n \n \n matmul = _matmul.apply"}, {"filename": "python/triton/runtime/backends/cuda.c", "status": "modified", "additions": 15, "deletions": 3, "changes": 18, "file_content_changes": "@@ -11,15 +11,16 @@ static inline void gpuAssert(CUresult code, const char *file, int line) {\n     char err[1024] = {0};\n     strcat(err, prefix);\n     strcat(err, str);\n+    PyGILState_STATE gil_state;\n+    gil_state = PyGILState_Ensure();\n     PyErr_SetString(PyExc_RuntimeError, err);\n+    PyGILState_Release(gil_state);\n   }\n }\n \n #define CUDA_CHECK(ans)                                                        \\\n   {                                                                            \\\n-    gpuAssert((ans), __FILE__, __LINE__);                                      \\\n-    if (PyErr_Occurred())                                                      \\\n-      return NULL;                                                             \\\n+    { gpuAssert((ans), __FILE__, __LINE__); }                                  \\\n   }\n \n #define ADD_ENUM_ITEM(value)                                                   \\\n@@ -234,6 +235,8 @@ static PyObject *loadBinary(PyObject *self, PyObject *args) {\n   int32_t n_spills = 0;\n   // create driver handles\n   CUcontext pctx = 0;\n+\n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuCtxGetCurrent(&pctx));\n   if (!pctx) {\n     CUDA_CHECK(cuDevicePrimaryCtxRetain(&pctx, device));\n@@ -264,6 +267,7 @@ static PyObject *loadBinary(PyObject *self, PyObject *args) {\n         cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES,\n                            shared_optin - shared_static));\n   }\n+  Py_END_ALLOW_THREADS;\n \n   if (PyErr_Occurred()) {\n     return NULL;\n@@ -281,7 +285,9 @@ static PyObject *memAlloc(PyObject *self, PyObject *args) {\n     return NULL; // Error parsing arguments\n   }\n \n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuMemAlloc(&dptr, bytesize));\n+  Py_END_ALLOW_THREADS;\n \n   return PyLong_FromUnsignedLongLong((unsigned long long)dptr);\n }\n@@ -300,7 +306,9 @@ static PyObject *memcpyHtoD(PyObject *self, PyObject *args) {\n   dstDevice = (CUdeviceptr)dstDevicePtr;\n   srcHost = (const void *)srcHostPtr;\n \n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuMemcpyHtoD(dstDevice, srcHost, byteCount));\n+  Py_END_ALLOW_THREADS;\n \n   Py_RETURN_NONE;\n }\n@@ -312,7 +320,9 @@ static PyObject *memFree(PyObject *self, PyObject *args) {\n     return NULL; // Error parsing arguments\n   }\n \n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuMemFree(dptr));\n+  Py_END_ALLOW_THREADS;\n \n   Py_RETURN_NONE;\n }\n@@ -400,10 +410,12 @@ static PyObject *tensorMapEncodeTiled(PyObject *self, PyObject *args) {\n     cuTensorMapEncodeTiledHandle = getCuTensorMapEncodeTiledHandle();\n   }\n   // Call the function\n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuTensorMapEncodeTiledHandle(\n       tensorMap, tensorDataType, tensorRank, globalAddress, globalDim,\n       globalStrides, boxDim, elementStrides, interleave, swizzle, l2Promotion,\n       oobFill));\n+  Py_END_ALLOW_THREADS;\n \n   // Clean up\n   free(globalDim);"}, {"filename": "python/triton/runtime/backends/hip.c", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -13,7 +13,10 @@ static inline void gpuAssert(hipError_t code, const char *file, int line) {\n         const char *str = hipGetErrorString(code);\n         char err[1024] = {0};\n         snprintf(err, 1024, \"%s Code: %d, Messsage: %s\", prefix, code, str);\n+        PyGILState_STATE gil_state;\n+        gil_state = PyGILState_Ensure();\n         PyErr_SetString(PyExc_RuntimeError, err);\n+        PyGILState_Release(gil_state);\n       }\n     }\n   }"}, {"filename": "python/triton/runtime/cache.py", "status": "modified", "additions": 8, "deletions": 6, "changes": 14, "file_content_changes": "@@ -40,18 +40,20 @@ def __init__(self, key):\n         self.key = key\n         self.lock_path = None\n         # create cache directory if it doesn't exist\n-        self.cache_dir = os.environ.get('TRITON_CACHE_DIR', default_cache_dir())\n+        self.cache_dir = os.getenv('TRITON_CACHE_DIR', \"\").strip() or default_cache_dir()\n         if self.cache_dir:\n             self.cache_dir = os.path.join(self.cache_dir, self.key)\n             self.lock_path = os.path.join(self.cache_dir, \"lock\")\n             os.makedirs(self.cache_dir, exist_ok=True)\n+        else:\n+            raise RuntimeError(\"Could not create or locate cache dir\")\n \n     def _make_path(self, filename) -> str:\n         return os.path.join(self.cache_dir, filename)\n \n-    def has_file(self, filename):\n+    def has_file(self, filename) -> bool:\n         if not self.cache_dir:\n-            return False\n+            raise RuntimeError(\"Could not create or locate cache dir\")\n         return os.path.exists(self._make_path(filename))\n \n     def get_file(self, filename) -> Optional[str]:\n@@ -80,16 +82,16 @@ def get_group(self, filename: str) -> Optional[Dict[str, str]]:\n         return result\n \n     # Note a group of pushed files as being part of a group\n-    def put_group(self, filename: str, group: Dict[str, str]):\n+    def put_group(self, filename: str, group: Dict[str, str]) -> str:\n         if not self.cache_dir:\n-            return\n+            raise RuntimeError(\"Could not create or locate cache dir\")\n         grp_contents = json.dumps({\"child_paths\": sorted(list(group.keys()))})\n         grp_filename = f\"__grp__{filename}\"\n         return self.put(grp_contents, grp_filename, binary=False)\n \n     def put(self, data, filename, binary=True) -> str:\n         if not self.cache_dir:\n-            return\n+            raise RuntimeError(\"Could not create or locate cache dir\")\n         binary = isinstance(data, bytes)\n         if not binary:\n             data = str(data)"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 21, "deletions": 13, "changes": 34, "file_content_changes": "@@ -13,6 +13,7 @@\n \n from .._C.libtriton.triton import TMAInfos\n from ..common.backend import get_backend, path_to_ptxas\n+from ..language.core import dtype\n \n TRITON_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n TRITON_VERSION = \"2.1.0\"\n@@ -358,19 +359,16 @@ def _make_launcher(self):\n \n         spec_keys = ', '.join(specializations)\n         grid_args = ','.join([f'\"{arg}\": {arg}' for arg in self.arg_names])\n-        args_signature = ', '.join(name if dflt == inspect._empty else f'{name} = {dflt}' for name, dflt in zip(self.arg_names, self.arg_defaults))\n+        args_signature = ', '.join(name if dflt == inspect._empty else f'{name} = triton.language.dtype(\\'{dflt}\\')' if dtype.is_dtype(f'{dflt}') else f'{name} = {dflt}' for name, dflt in zip(self.arg_names, self.arg_defaults))\n         args_signature = args_signature + ', ' if len(args_signature) > 0 else ''\n \n         src = f\"\"\"\n-def {self.fn.__name__}({args_signature}grid=None, num_warps=4, num_ctas=1, num_stages=3, enable_warp_specialization=False, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\n-    from ..compiler import compile, CompiledKernel\n+import triton\n+def {self.fn.__name__}({args_signature}grid=None, num_warps=None, num_ctas=1, num_stages=None, enable_warp_specialization=False, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\n+    from ..compiler import compile, CompiledKernel, get_arch_default_num_warps, get_arch_default_num_stages\n     sig_key = {f'{sig_keys},' if len(sig_keys) > 0 else ()}\n     constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else ()}\n     spec_key = {f'{spec_keys},' if len(spec_keys) > 0 else ()}\n-    key = (version_key, sig_key, constexpr_key, spec_key, num_warps, num_ctas, num_stages, enable_warp_specialization, self.debug)\n-    if not extern_libs is None:\n-      key = (key, tuple(extern_libs.items()))\n-    assert num_warps > 0 and (num_warps & (num_warps - 1)) == 0, \"num_warps must be a power of 2\"\n     assert num_ctas > 0\n     assert grid is not None\n     if callable(grid):\n@@ -385,24 +383,33 @@ def {self.fn.__name__}({args_signature}grid=None, num_warps=4, num_ctas=1, num_s\n         device_type = self._conclude_device_type(device_types, {pinned_memory_flags})\n \n     device_backend = None\n-    if device_type not in ['cuda', 'hip']:\n+    if device_type not in ['cuda']:\n         device_backend = get_backend(device_type)\n         if device_backend is None:\n             raise ValueError('Cannot find backend for ' + device_type)\n \n     if device is None:\n-        if device_type in ['cuda', 'hip']:\n+        if device_type in ['cuda']:\n             device = get_current_device()\n             set_current_device(device)\n         else:\n             device = device_backend.get_current_device()\n             device_backend.set_current_device(device)\n     if stream is None and not warmup:\n-        if device_type in ['cuda', 'hip']:\n+        if device_type in ['cuda']:\n             stream = get_cuda_stream(device)\n         else:\n             stream = device_backend.get_stream()\n \n+    if num_warps is None:\n+        num_warps = get_arch_default_num_warps(device_type)\n+    if num_stages is None:\n+        num_stages = get_arch_default_num_stages(device_type)\n+\n+    key = (version_key, sig_key, constexpr_key, spec_key, num_warps, num_ctas, num_stages, enable_warp_specialization, self.debug)\n+    if not extern_libs is None:\n+      key = (key, tuple(extern_libs.items()))\n+\n     bin = cache[device].get(key, None)\n     if bin is not None:\n       # build dict of constant values\n@@ -461,9 +468,6 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         self.arg_names = [v.name for v in signature.parameters.values()]\n         self.arg_defaults = [v.default for v in signature.parameters.values()]\n         self.has_defaults = any(v != inspect._empty for v in self.arg_defaults)\n-        # specialization hints\n-        self.do_not_specialize = [] if do_not_specialize is None else do_not_specialize\n-        self.do_not_specialize = {self.arg_names.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize}\n         # function source code (without decorators)\n         self.src = textwrap.dedent(inspect.getsource(fn))\n         self.src = self.src[self.src.find(\"def\"):]\n@@ -480,6 +484,10 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         self.__annotations__ = {name: _normalize_ty(ty) for name, ty in fn.__annotations__.items()}\n         # index of constexprs\n         self.constexprs = [self.arg_names.index(name) for name, ty in self.__annotations__.items() if 'constexpr' in ty]\n+        # specialization hints\n+        regular_args = [arg for i, arg in enumerate(self.arg_names) if i not in self.constexprs]\n+        self.do_not_specialize = [] if do_not_specialize is None else do_not_specialize\n+        self.do_not_specialize = {regular_args.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize}\n         # tma info\n         self.tensormaps_info = TMAInfos()\n         # launcher"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 40, "deletions": 32, "changes": 72, "file_content_changes": "@@ -3,6 +3,7 @@\n import subprocess\n import sys\n from contextlib import contextmanager\n+from typing import Any, Dict, List\n \n from . import language as tl\n from ._C.libtriton.triton import runtime\n@@ -201,37 +202,41 @@ class Benchmark:\n \n     def __init__(\n         self,\n-        x_names,\n-        x_vals,\n-        line_arg,\n-        line_vals,\n-        line_names,\n-        plot_name,\n-        args,\n-        xlabel='',\n-        ylabel='',\n-        x_log=False,\n-        y_log=False,\n+        x_names: List[str],\n+        x_vals: List[Any],\n+        line_arg: str,\n+        line_vals: List[Any],\n+        line_names: List[str],\n+        plot_name: str,\n+        args: Dict[str, Any],\n+        xlabel: str = '',\n+        ylabel: str = '',\n+        x_log: bool = False,\n+        y_log: bool = False,\n         color=None,\n         styles=None,\n     ):\n         \"\"\"\n-        Constructor\n+        Constructor.\n+        x_vals can be a list of scalars or a list of tuples/lists. If x_vals is a list\n+        of scalars and there are multiple x_names, all arguments will have the same value.\n+        If x_vals is a list of tuples/lists, each element should have the same length as\n+        x_names.\n \n-        :param x_names: Name of the arguments that should appear on the x axis of the plot. If the list contains more than one element, all the arguments are assumed to have the same value.\n+        :param x_names: Name of the arguments that should appear on the x axis of the plot.\n         :type x_names: List[str]\n         :param x_vals: List of values to use for the arguments in :code:`x_names`.\n         :type x_vals: List[Any]\n         :param line_arg: Argument name for which different values correspond to different lines in the plot.\n         :type line_arg: str\n         :param line_vals: List of values to use for the arguments in :code:`line_arg`.\n-        :type line_vals: List[str]\n+        :type line_vals: List[Any]\n         :param line_names: Label names for the different lines.\n         :type line_names: List[str]\n         :param plot_name: Name of the plot.\n         :type plot_name: str\n-        :param args: List of arguments to remain fixed throughout the benchmark.\n-        :type args: List[str]\n+        :param args: Dictionary of keyword arguments to remain fixed throughout the benchmark.\n+        :type args: Dict[str, Any]\n         :param xlabel: Label for the x axis of the plot.\n         :type xlabel: str, optional\n         :param ylabel: Label for the y axis of the plot.\n@@ -261,23 +266,25 @@ def __init__(self, fn, benchmarks):\n         self.fn = fn\n         self.benchmarks = benchmarks\n \n-    def _run(self, bench, save_path, show_plots, print_data):\n+    def _run(self, bench: Benchmark, save_path: str, show_plots: bool, print_data: bool):\n         import os\n \n         import matplotlib.pyplot as plt\n         import pandas as pd\n         y_mean = bench.line_names\n         y_min = [f'{x}-min' for x in bench.line_names]\n         y_max = [f'{x}-max' for x in bench.line_names]\n-        x_names_str = str(bench.x_names)\n-        df = pd.DataFrame(columns=[x_names_str] + y_mean + y_min + y_max)\n+        x_names = list(bench.x_names)\n+        df = pd.DataFrame(columns=x_names + y_mean + y_min + y_max)\n         for x in bench.x_vals:\n-            if not isinstance(x, list):\n-                x = [x]\n-            if len(x) == 1:\n-                x = x * len(bench.x_names)\n-            x_str = str(x)\n-            x_args = {x_name: x_in for x_name, x_in in zip(bench.x_names, x)}\n+            # x can be a single value or a sequence of values.\n+            if not isinstance(x, (list, tuple)):\n+                x = [x for _ in x_names]\n+\n+            if len(x) != len(x_names):\n+                raise ValueError(f\"Expected {len(x_names)} values, got {x}\")\n+            x_args = dict(zip(x_names, x))\n+\n             row_mean, row_min, row_max = [], [], []\n             for y in bench.line_vals:\n                 ret = self.fn(**x_args, **{bench.line_arg: y}, **bench.args)\n@@ -288,23 +295,24 @@ def _run(self, bench, save_path, show_plots, print_data):\n                 row_mean += [y_mean]\n                 row_min += [y_min]\n                 row_max += [y_max]\n-            df.loc[len(df)] = [x_str] + row_mean + row_min + row_max\n+            df.loc[len(df)] = list(x) + row_mean + row_min + row_max\n+\n         if bench.plot_name:\n             plt.figure()\n             ax = plt.subplot()\n-            x = x_names_str\n+            # Plot first x value on x axis if there are multiple.\n+            first_x = x_names[0]\n             for i, y in enumerate(bench.line_names):\n                 y_min, y_max = df[y + '-min'], df[y + '-max']\n                 col = bench.styles[i][0] if bench.styles else None\n                 sty = bench.styles[i][1] if bench.styles else None\n-                ax.plot(df[x], df[y], label=y, color=col, ls=sty)\n+                ax.plot(df[first_x], df[y], label=y, color=col, ls=sty)\n                 if not y_min.isnull().all() and not y_max.isnull().all():\n                     y_min = y_min.astype(float)\n                     y_max = y_max.astype(float)\n-                    ax.fill_between(df[x], y_min, y_max, alpha=0.15, color=col)\n+                    ax.fill_between(df[first_x], y_min, y_max, alpha=0.15, color=col)\n             ax.legend()\n-            xlabel = bench.xlabel if bench.xlabel else \" = \".join(bench.x_names)\n-            ax.set_xlabel(xlabel)\n+            ax.set_xlabel(bench.xlabel or first_x)\n             ax.set_ylabel(bench.ylabel)\n             # ax.set_title(bench.plot_name)\n             ax.set_xscale(\"log\" if bench.x_log else \"linear\")\n@@ -313,7 +321,7 @@ def _run(self, bench, save_path, show_plots, print_data):\n                 plt.show()\n             if save_path:\n                 plt.savefig(os.path.join(save_path, f\"{bench.plot_name}.png\"))\n-        df = df[[x_names_str] + bench.line_names]\n+        df = df[x_names + bench.line_names]\n         if print_data:\n             print(bench.plot_name + ':')\n             print(df)"}, {"filename": "python/tutorials/09-experimental-tma-matrix-multiplication.py", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -32,6 +32,11 @@\n import triton\n import triton.language as tl\n \n+if torch.cuda.get_device_capability()[0] < 9:\n+    import sys\n+    print(\"Skipping TMA benchmark for GPU with compute capability < 9\")\n+    sys.exit(0)\n+\n \n @triton.autotune(\n     configs=[\n@@ -191,10 +196,5 @@ def perf(ms):\n     return perf(ms), perf(max_ms), perf(min_ms)\n \n \n-if torch.cuda.get_device_capability()[0] < 9:\n-    import sys\n-    print(\"Skipping TMA benchmark for GPU with compute capability < 9\")\n-    sys.exit(0)\n-\n test_matmul()\n benchmark.run(show_plots=False, print_data=True)"}, {"filename": "python/tutorials/10-experimental-tma-store-matrix-multiplication.py", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -32,6 +32,11 @@\n import triton\n import triton.language as tl\n \n+if torch.cuda.get_device_capability()[0] < 9:\n+    import sys\n+    print(\"Skipping TMA benchmark for GPU with compute capability < 9\")\n+    sys.exit(0)\n+\n \n @triton.autotune(\n     configs=[\n@@ -171,9 +176,4 @@ def perf(ms):\n     return perf(ms), perf(max_ms), perf(min_ms)\n \n \n-if torch.cuda.get_device_capability()[0] < 9:\n-    import sys\n-    print(\"Skipping TMA benchmark for GPU with compute capability < 9\")\n-    sys.exit(0)\n-\n benchmark.run(show_plots=False, print_data=True)"}, {"filename": "test/Triton/canonicalize.mlir", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -0,0 +1,27 @@\n+// RUN: triton-opt %s -split-input-file -canonicalize | FileCheck %s\n+\n+// CHECK-LABEL: dead_load\n+tt.func @dead_load(%ptr: tensor<32x128x!tt.ptr<f16>>) {\n+  %mask = arith.constant dense<true> : tensor<32x128xi1>\n+  %other = arith.constant dense<0.00e+00> : tensor<32x128xf16>\n+  // CHECK-NOT: tt.load {{.*}} isVolatile = false\n+  //     CHECK: tt.load {{.*}} isVolatile = true\n+  %a = tt.load %ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16>\n+  %b = tt.load %ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = true} : tensor<32x128xf16>\n+  tt.return\n+}\n+\n+\n+// CHECK-LABEL: make_range\n+tt.func @make_range() -> (tensor<128x1xi32>, tensor<1xi32>) {\n+  // CHECK-DAG: %[[c:.*]] = arith.constant dense<0> : tensor<128x1xi32>\n+  %a = tt.make_range {end = 1 : i32, start = 0 : i32} : tensor<1xi32>\n+  %b = tt.expand_dims %a {axis = 1 : i32} : (tensor<1xi32>) -> tensor<1x1xi32>\n+  %c = tt.broadcast %b : (tensor<1x1xi32>) -> tensor<128x1xi32>\n+\n+  // CHECK-DAG: %[[d:.*]] = arith.constant dense<1> : tensor<1xi32>\n+  %d = tt.make_range {end = 2 : i32, start = 1 : i32} : tensor<1xi32>\n+\n+  // CHECK-DAG: tt.return %[[c]], %[[d]] : tensor<128x1xi32>, tensor<1xi32>\n+  tt.return %c, %d : tensor<128x1xi32>, tensor<1xi32>\n+}"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 337, "deletions": 3, "changes": 340, "file_content_changes": "@@ -77,6 +77,36 @@ tt.func @remat_fast_load(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   tt.return\n }\n \n+// Hoist the convert on top of ext to make it cheaper.\n+// CHECK-LABEL: hoist_above_ext\n+tt.func @hoist_above_ext(%arg0: tensor<1024xf16, #layout0>, %arg1: f32) -> tensor<1024xf32, #layout1> {\n+// CHECK: %[[CVT:.+]] = triton_gpu.convert_layout\n+// CHECK: arith.extf %[[CVT]]\n+// CHECK-NOT: triton_gpu.convert_layout\n+// CHECK: tt.return\n+  %0 = arith.extf %arg0 : tensor<1024xf16, #layout0> to tensor<1024xf32, #layout0>\n+  %1 = tt.splat %arg1 : (f32) -> tensor<1024xf32, #layout0>\n+  %2 = arith.addf %0, %1 : tensor<1024xf32, #layout0>\n+  %3 = triton_gpu.convert_layout %2 : (tensor<1024xf32, #layout0>) -> tensor<1024xf32, #layout1>\n+  tt.return %3 : tensor<1024xf32, #layout1>\n+}\n+\n+// CHECK-LABEL: hoist_above_ext2\n+tt.func @hoist_above_ext2(%arg0: tensor<1024xf16, #layout0>, %arg1: f16) -> tensor<1024xf32, #layout1> {\n+// CHECK: %[[CVT:.+]] = triton_gpu.convert_layout\n+// CHECK: arith.extf %[[CVT]]\n+// CHECK-NOT: triton_gpu.convert_layout\n+// CHECK: tt.return\n+  %0 = arith.extf %arg0 : tensor<1024xf16, #layout0> to tensor<1024xf32, #layout0>\n+  %1 = tt.splat %arg1 : (f16) -> tensor<1024xf16, #layout0>\n+  %2 = arith.extf %1 : tensor<1024xf16, #layout0> to tensor<1024xf32, #layout0>\n+  %3 = arith.addf %0, %2 : tensor<1024xf32, #layout0>\n+  %4 = triton_gpu.convert_layout %3 : (tensor<1024xf32, #layout0>) -> tensor<1024xf32, #layout1>\n+  tt.return %4 : tensor<1024xf32, #layout1>\n+}\n+\n+\n+\n // CHECK-LABEL: if\n tt.func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n@@ -229,8 +259,10 @@ tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32,\n   // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32, 1>, [[$row_layout]]>, tensor<64x64xi32, [[$row_layout]]>\n   // CHECK-NEXT: scf.yield {{.*}} : tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32, 1>, [[$row_layout]]>\n   // CHECK-NEXT: }\n-  // CHECK-NEXT: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[$row_layout]]>) -> tensor<64x64xf32, [[$col_layout_novec]]>\n   // CHECK-NOT: triton_gpu.convert_layout\n+  //     CHECK: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[$row_layout]]>) -> tensor<64x64xf32, [[$col_layout_novec]]>\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  //    CHECK:  tt.return\n   %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n   %cst_0 = arith.constant dense<64> : tensor<64x64xi32, #blocked1>\n   %c1 = arith.constant 1 : index\n@@ -276,6 +308,19 @@ tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32,\n }\n \n // CHECK-LABEL: loop_if\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK: scf.for\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK:   scf.if\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK:     scf.yield\n+//     CHECK:   else\n+//     CHECK:     scf.yield\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK:   scf.yield\n+//     CHECK: triton_gpu.convert_layout\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK: tt.store\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @loop_if(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n   %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n@@ -1125,14 +1170,14 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n \n // -----\n \n-// Check if the SimplifyReduceCvt handles convert_layout lifted from the for loop.\n // CHECK-LABEL: reduce_cvt2\n // Match the reduction\n // CHECK: tt.reduce\n // CHECK-SAME: axis = 1\n // CHECK: (tensor<1x256xf32, #blocked>) -> tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n-// CHECK-NEXT: triton_gpu.convert_layout\n+// CHECK: triton_gpu.convert_layout\n // CHECK-NOT: triton_gpu.convert_layout\n+// CHECK: tt.return\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0]}>\n@@ -1347,6 +1392,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-war\n // Check if MoveConvertOutOfLoop hangs because of adding additional conversions\n // CHECK-LABEL: loop_print\n // CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK: tt.return\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [0, 1]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n #blocked2 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n@@ -1502,3 +1548,291 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-war\n     tt.return\n   }\n }\n+\n+\n+// -----\n+\n+// Check that we don't have extra convert for flash attention IR.\n+#blocked = #triton_gpu.blocked<{sizePerThread = [4, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked4 = #triton_gpu.blocked<{sizePerThread = [1, 1, 8], threadsPerWarp = [4, 1, 8], warpsPerCTA = [4, 1, 1], order = [1, 2, 0], CTAsPerCGA = [1, 1, 1], CTASplitNum = [1, 1, 1], CTAOrder = [1, 0, 2]}>\n+#blocked5 = #triton_gpu.blocked<{sizePerThread = [1, 1, 8], threadsPerWarp = [1, 4, 8], warpsPerCTA = [1, 4, 1], order = [0, 2, 1], CTAsPerCGA = [1, 1, 1], CTASplitNum = [1, 1, 1], CTAOrder = [0, 1, 2]}>\n+#blocked6 = #triton_gpu.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked7 = #triton_gpu.blocked<{sizePerThread = [8, 1, 1], threadsPerWarp = [8, 1, 4], warpsPerCTA = [1, 1, 4], order = [1, 0, 2], CTAsPerCGA = [1, 1, 1], CTASplitNum = [1, 1, 1], CTAOrder = [1, 0, 2]}>\n+#blocked8 = #triton_gpu.blocked<{sizePerThread = [1, 8, 1], threadsPerWarp = [1, 8, 4], warpsPerCTA = [1, 1, 4], order = [0, 1, 2], CTAsPerCGA = [1, 1, 1], CTASplitNum = [1, 1, 1], CTAOrder = [0, 1, 2]}>\n+#blocked9 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0]}>\n+module attributes {\"triton_gpu.compute-capability\" = 90 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @attention_fw(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg3: f32, %arg4: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg5: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg7: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg8: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg9: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg10: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg11: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg12: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg13: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg14: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg15: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg16: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg17: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg18: i32, %arg19: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg20: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg21: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}) attributes {noinline = false} {\n+    %c0_i64 = arith.constant 0 : i64\n+    %c64_i64 = arith.constant 64 : i64\n+    %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf16, #blocked>\n+    %cst_0 = arith.constant dense<0xFF800000> : tensor<128xf32, #blocked1>\n+    %cst_1 = arith.constant dense<0.000000e+00> : tensor<128xf32, #blocked1>\n+    %c64_i32 = arith.constant 64 : i32\n+    %c0_i32 = arith.constant 0 : i32\n+    %cst_2 = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #blocked2>\n+    %cst_3 = arith.constant 1.44269502 : f32\n+    %c128_i32 = arith.constant 128 : i32\n+    %0 = tt.get_program_id x : i32\n+    %1 = tt.get_program_id y : i32\n+    %2 = arith.muli %1, %arg7 : i32\n+    %3 = arith.muli %1, %arg10 : i32\n+    %4 = tt.addptr %arg0, %2 : !tt.ptr<f16, 1>, i32\n+    %5 = arith.muli %0, %c128_i32 : i32\n+    %6 = arith.extsi %arg8 : i32 to i64\n+    %7 = arith.extsi %5 : i32 to i64\n+    %8 = tt.addptr %arg1, %3 : !tt.ptr<f16, 1>, i32\n+    %9 = arith.addi %arg20, %arg21 : i32\n+    %10 = arith.extsi %arg11 : i32 to i64\n+    %11 = tt.addptr %arg2, %3 : !tt.ptr<f16, 1>, i32\n+    %12 = arith.extsi %arg14 : i32 to i64\n+    %13 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked1>\n+    %14 = tt.splat %5 : (i32) -> tensor<128xi32, #blocked1>\n+    %15 = arith.addi %14, %13 : tensor<128xi32, #blocked1>\n+    %16 = arith.mulf %arg3, %cst_3 : f32\n+    %17 = tt.splat %4 : (!tt.ptr<f16, 1>) -> tensor<128x64x!tt.ptr<f16, 1>, #blocked3>\n+    %18 = tt.splat %7 : (i64) -> tensor<128xi64, #blocked3>\n+    %19 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked3>\n+    %20 = arith.extsi %19 : tensor<128xi32, #blocked3> to tensor<128xi64, #blocked3>\n+    %21 = arith.addi %18, %20 : tensor<128xi64, #blocked3>\n+    %22 = triton_gpu.convert_layout %21 : (tensor<128xi64, #blocked3>) -> tensor<128xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>\n+    %23 = tt.expand_dims %22 {axis = 1 : i32} : (tensor<128xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>) -> tensor<128x1xi64, #blocked4>\n+    %24 = tt.splat %6 : (i64) -> tensor<128x1xi64, #blocked4>\n+    %25 = arith.muli %23, %24 : tensor<128x1xi64, #blocked4>\n+    %26 = tt.broadcast %25 : (tensor<128x1xi64, #blocked4>) -> tensor<128x64xi64, #blocked4>\n+    %27 = triton_gpu.convert_layout %26 : (tensor<128x64xi64, #blocked4>) -> tensor<128x64xi64, #blocked3>\n+    %28 = tt.addptr %17, %27 : tensor<128x64x!tt.ptr<f16, 1>, #blocked3>, tensor<128x64xi64, #blocked3>\n+    %29 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked3>\n+    %30 = arith.extsi %29 : tensor<64xi32, #blocked3> to tensor<64xi64, #blocked3>\n+    %31 = triton_gpu.convert_layout %30 : (tensor<64xi64, #blocked3>) -> tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>\n+    %32 = tt.expand_dims %31 {axis = 0 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>) -> tensor<1x64xi64, #blocked5>\n+    %33 = tt.broadcast %32 : (tensor<1x64xi64, #blocked5>) -> tensor<128x64xi64, #blocked5>\n+    %34 = triton_gpu.convert_layout %33 : (tensor<128x64xi64, #blocked5>) -> tensor<128x64xi64, #blocked3>\n+    %35 = tt.addptr %28, %34 : tensor<128x64x!tt.ptr<f16, 1>, #blocked3>, tensor<128x64xi64, #blocked3>\n+    %36 = tt.load %35 {boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x64xf16, #blocked3>\n+    %37 = triton_gpu.convert_layout %36 : (tensor<128x64xf16, #blocked3>) -> tensor<128x64xf16, #blocked2>\n+    %38 = tt.splat %16 : (f32) -> tensor<128x64xf32, #blocked2>\n+    %39 = arith.extf %37 : tensor<128x64xf16, #blocked2> to tensor<128x64xf32, #blocked2>\n+    %40 = arith.mulf %39, %38 : tensor<128x64xf32, #blocked2>\n+    %41 = arith.truncf %40 : tensor<128x64xf32, #blocked2> to tensor<128x64xf16, #blocked2>\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK: scf.for\n+// CHECK-NOT:   triton_gpu.convert_layout\n+//     CHECK:   triton_gpu.convert_layout %{{.*}} #triton_gpu.dot_op\n+//     CHECK:   triton_gpu.convert_layout %{{.*}} #triton_gpu.dot_op\n+// CHECK-NOT:   triton_gpu.convert_layout\n+//     CHECK:   tt.dot\n+// CHECK-NOT:   triton_gpu.convert_layout\n+//     CHECK:   triton_gpu.convert_layout %{{.*}} #triton_gpu.dot_op\n+//     CHECK:   triton_gpu.convert_layout %{{.*}} #triton_gpu.dot_op\n+// CHECK-NOT:   triton_gpu.convert_layout\n+//     CHECK:   tt.dot\n+//     CHECK:   scf.yield\n+    %42:5 = scf.for %arg22 = %c0_i32 to %9 step %c64_i32 iter_args(%arg23 = %cst_2, %arg24 = %cst_1, %arg25 = %cst_0, %arg26 = %c0_i64, %arg27 = %c0_i64) -> (tensor<128x64xf32, #blocked2>, tensor<128xf32, #blocked1>, tensor<128xf32, #blocked1>, i64, i64)  : i32 {\n+      %78 = tt.splat %8 : (!tt.ptr<f16, 1>) -> tensor<64x64x!tt.ptr<f16, 1>, #blocked6>\n+      %79 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked6>\n+      %80 = arith.extsi %79 : tensor<64xi32, #blocked6> to tensor<64xi64, #blocked6>\n+      %81 = triton_gpu.convert_layout %80 : (tensor<64xi64, #blocked6>) -> tensor<64xi64, #triton_gpu.slice<{dim = 1, parent = #blocked7}>>\n+      %82 = tt.expand_dims %81 {axis = 1 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 1, parent = #blocked7}>>) -> tensor<64x1xi64, #blocked7>\n+      %83 = tt.broadcast %82 : (tensor<64x1xi64, #blocked7>) -> tensor<64x64xi64, #blocked7>\n+      %84 = triton_gpu.convert_layout %83 : (tensor<64x64xi64, #blocked7>) -> tensor<64x64xi64, #blocked6>\n+      %85 = tt.addptr %78, %84 : tensor<64x64x!tt.ptr<f16, 1>, #blocked6>, tensor<64x64xi64, #blocked6>\n+      %86 = tt.splat %arg26 : (i64) -> tensor<64xi64, #blocked6>\n+      %87 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked6>\n+      %88 = arith.extsi %87 : tensor<64xi32, #blocked6> to tensor<64xi64, #blocked6>\n+      %89 = arith.addi %86, %88 : tensor<64xi64, #blocked6>\n+      %90 = triton_gpu.convert_layout %89 : (tensor<64xi64, #blocked6>) -> tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked8}>>\n+      %91 = tt.expand_dims %90 {axis = 0 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked8}>>) -> tensor<1x64xi64, #blocked8>\n+      %92 = tt.splat %10 : (i64) -> tensor<1x64xi64, #blocked8>\n+      %93 = arith.muli %91, %92 : tensor<1x64xi64, #blocked8>\n+      %94 = tt.broadcast %93 : (tensor<1x64xi64, #blocked8>) -> tensor<64x64xi64, #blocked8>\n+      %95 = triton_gpu.convert_layout %94 : (tensor<64x64xi64, #blocked8>) -> tensor<64x64xi64, #blocked6>\n+      %96 = tt.addptr %85, %95 : tensor<64x64x!tt.ptr<f16, 1>, #blocked6>, tensor<64x64xi64, #blocked6>\n+      %97 = tt.load %96 {boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf16, #blocked6>\n+      %98 = tt.splat %11 : (!tt.ptr<f16, 1>) -> tensor<64x64x!tt.ptr<f16, 1>, #blocked3>\n+      %99 = tt.splat %arg27 : (i64) -> tensor<64xi64, #blocked3>\n+      %100 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked3>\n+      %101 = arith.extsi %100 : tensor<64xi32, #blocked3> to tensor<64xi64, #blocked3>\n+      %102 = arith.addi %99, %101 : tensor<64xi64, #blocked3>\n+      %103 = triton_gpu.convert_layout %102 : (tensor<64xi64, #blocked3>) -> tensor<64xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>\n+      %104 = tt.expand_dims %103 {axis = 1 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>) -> tensor<64x1xi64, #blocked4>\n+      %105 = tt.splat %12 : (i64) -> tensor<64x1xi64, #blocked4>\n+      %106 = arith.muli %104, %105 : tensor<64x1xi64, #blocked4>\n+      %107 = tt.broadcast %106 : (tensor<64x1xi64, #blocked4>) -> tensor<64x64xi64, #blocked4>\n+      %108 = triton_gpu.convert_layout %107 : (tensor<64x64xi64, #blocked4>) -> tensor<64x64xi64, #blocked3>\n+      %109 = tt.addptr %98, %108 : tensor<64x64x!tt.ptr<f16, 1>, #blocked3>, tensor<64x64xi64, #blocked3>\n+      %110 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked3>\n+      %111 = arith.extsi %110 : tensor<64xi32, #blocked3> to tensor<64xi64, #blocked3>\n+      %112 = triton_gpu.convert_layout %111 : (tensor<64xi64, #blocked3>) -> tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>\n+      %113 = tt.expand_dims %112 {axis = 0 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>) -> tensor<1x64xi64, #blocked5>\n+      %114 = tt.broadcast %113 : (tensor<1x64xi64, #blocked5>) -> tensor<64x64xi64, #blocked5>\n+      %115 = triton_gpu.convert_layout %114 : (tensor<64x64xi64, #blocked5>) -> tensor<64x64xi64, #blocked3>\n+      %116 = tt.addptr %109, %115 : tensor<64x64x!tt.ptr<f16, 1>, #blocked3>, tensor<64x64xi64, #blocked3>\n+      %117 = tt.load %116 {boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf16, #blocked3>\n+      %118 = triton_gpu.convert_layout %41 : (tensor<128x64xf16, #blocked2>) -> tensor<128x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>>\n+      %119 = triton_gpu.convert_layout %97 : (tensor<64x64xf16, #blocked6>) -> tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>>\n+      %120 = tt.dot %118, %119, %cst {allowTF32 = true} : tensor<128x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<128x64xf16, #blocked>\n+      %121 = triton_gpu.convert_layout %120 : (tensor<128x64xf16, #blocked>) -> tensor<128x64xf16, #blocked2>\n+      %122 = arith.extf %121 : tensor<128x64xf16, #blocked2> to tensor<128x64xf32, #blocked2>\n+      %123 = \"tt.reduce\"(%122) <{axis = 1 : i32}> ({\n+      ^bb0(%arg28: f32, %arg29: f32):\n+        %153 = arith.maxf %arg28, %arg29 : f32\n+        tt.reduce.return %153 : f32\n+      }) : (tensor<128x64xf32, #blocked2>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %124 = triton_gpu.convert_layout %123 : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<128xf32, #blocked1>\n+      %125 = arith.maxf %arg25, %124 : tensor<128xf32, #blocked1>\n+      %126 = arith.subf %arg25, %125 : tensor<128xf32, #blocked1>\n+      %127 = tt.extern_elementwise %126 {pure = true, libname = \"libdevice\", libpath = \"/root/.pyenv/versions/3.9.9/lib/python3.9/site-packages/triton/language/../third_party/cuda/lib/libdevice.10.bc\", symbol = \"__nv_exp2f\"} : (tensor<128xf32, #blocked1>) -> tensor<128xf32, #blocked1>\n+      %128 = triton_gpu.convert_layout %125 : (tensor<128xf32, #blocked1>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>\n+      %129 = tt.expand_dims %128 {axis = 1 : i32} : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>) -> tensor<128x1xf32, #blocked9>\n+      %130 = triton_gpu.convert_layout %129 : (tensor<128x1xf32, #blocked9>) -> tensor<128x1xf32, #blocked2>\n+      %131 = tt.broadcast %130 : (tensor<128x1xf32, #blocked2>) -> tensor<128x64xf32, #blocked2>\n+      %132 = arith.subf %122, %131 : tensor<128x64xf32, #blocked2>\n+      %133 = tt.extern_elementwise %132 {pure = true, libname = \"libdevice\", libpath = \"/root/.pyenv/versions/3.9.9/lib/python3.9/site-packages/triton/language/../third_party/cuda/lib/libdevice.10.bc\", symbol = \"__nv_exp2f\"} : (tensor<128x64xf32, #blocked2>) -> tensor<128x64xf32, #blocked2>\n+      %134 = arith.mulf %arg24, %cst_1 : tensor<128xf32, #blocked1>\n+      %135 = arith.addf %134, %127 : tensor<128xf32, #blocked1>\n+      %136 = triton_gpu.convert_layout %135 : (tensor<128xf32, #blocked1>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>\n+      %137 = tt.expand_dims %136 {axis = 1 : i32} : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>) -> tensor<128x1xf32, #blocked9>\n+      %138 = triton_gpu.convert_layout %137 : (tensor<128x1xf32, #blocked9>) -> tensor<128x1xf32, #blocked2>\n+      %139 = tt.broadcast %138 : (tensor<128x1xf32, #blocked2>) -> tensor<128x64xf32, #blocked2>\n+      %140 = arith.mulf %arg23, %139 : tensor<128x64xf32, #blocked2>\n+      %141 = arith.truncf %133 : tensor<128x64xf32, #blocked2> to tensor<128x64xf16, #blocked2>\n+      %142 = triton_gpu.convert_layout %141 : (tensor<128x64xf16, #blocked2>) -> tensor<128x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>>\n+      %143 = triton_gpu.convert_layout %117 : (tensor<64x64xf16, #blocked3>) -> tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>>\n+      %144 = triton_gpu.convert_layout %140 : (tensor<128x64xf32, #blocked2>) -> tensor<128x64xf32, #blocked>\n+      %145 = tt.dot %142, %143, %144 {allowTF32 = true} : tensor<128x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<128x64xf32, #blocked>\n+      %146 = triton_gpu.convert_layout %145 : (tensor<128x64xf32, #blocked>) -> tensor<128x64xf32, #blocked2>\n+      %147 = arith.mulf %arg24, %127 : tensor<128xf32, #blocked1>\n+      %148 = \"tt.reduce\"(%133) <{axis = 1 : i32}> ({\n+      ^bb0(%arg28: f32, %arg29: f32):\n+        %153 = arith.addf %arg28, %arg29 : f32\n+        tt.reduce.return %153 : f32\n+      }) : (tensor<128x64xf32, #blocked2>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %149 = triton_gpu.convert_layout %148 : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<128xf32, #blocked1>\n+      %150 = arith.addf %147, %149 : tensor<128xf32, #blocked1>\n+      %151 = arith.addi %arg26, %c64_i64 : i64\n+      %152 = arith.addi %arg27, %c64_i64 : i64\n+      scf.yield %146, %150, %125, %151, %152 : tensor<128x64xf32, #blocked2>, tensor<128xf32, #blocked1>, tensor<128xf32, #blocked1>, i64, i64\n+    }\n+    %43 = triton_gpu.convert_layout %42#1 : (tensor<128xf32, #blocked1>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>\n+    %44 = tt.expand_dims %43 {axis = 1 : i32} : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>) -> tensor<128x1xf32, #blocked9>\n+    %45 = triton_gpu.convert_layout %44 : (tensor<128x1xf32, #blocked9>) -> tensor<128x1xf32, #blocked2>\n+    %46 = tt.broadcast %45 : (tensor<128x1xf32, #blocked2>) -> tensor<128x64xf32, #blocked2>\n+    %47 = arith.divf %42#0, %46 : tensor<128x64xf32, #blocked2>\n+    %48 = arith.muli %1, %arg20 : i32\n+    %49 = tt.addptr %arg4, %48 : !tt.ptr<f32, 1>, i32\n+    %50 = tt.splat %49 : (!tt.ptr<f32, 1>) -> tensor<128x!tt.ptr<f32, 1>, #blocked1>\n+    %51 = tt.addptr %50, %15 : tensor<128x!tt.ptr<f32, 1>, #blocked1>, tensor<128xi32, #blocked1>\n+    %52 = tt.extern_elementwise %42#1 {pure = true, libname = \"libdevice\", libpath = \"/root/.pyenv/versions/3.9.9/lib/python3.9/site-packages/triton/language/../third_party/cuda/lib/libdevice.10.bc\", symbol = \"__nv_log2f\"} : (tensor<128xf32, #blocked1>) -> tensor<128xf32, #blocked1>\n+    %53 = arith.addf %42#2, %52 : tensor<128xf32, #blocked1>\n+    tt.store %51, %53 {cache = 1 : i32, evict = 1 : i32} : tensor<128xf32, #blocked1>\n+    %54 = tt.addptr %arg5, %2 : !tt.ptr<f16, 1>, i32\n+    %55 = arith.extsi %arg17 : i32 to i64\n+    %56 = arith.extsi %5 : i32 to i64\n+    %57 = arith.truncf %47 : tensor<128x64xf32, #blocked2> to tensor<128x64xf16, #blocked2>\n+    %58 = triton_gpu.convert_layout %57 : (tensor<128x64xf16, #blocked2>) -> tensor<128x64xf16, #blocked3>\n+    %59 = tt.splat %54 : (!tt.ptr<f16, 1>) -> tensor<128x64x!tt.ptr<f16, 1>, #blocked3>\n+    %60 = tt.splat %56 : (i64) -> tensor<128xi64, #blocked3>\n+    %61 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked3>\n+    %62 = arith.extsi %61 : tensor<128xi32, #blocked3> to tensor<128xi64, #blocked3>\n+    %63 = arith.addi %60, %62 : tensor<128xi64, #blocked3>\n+    %64 = triton_gpu.convert_layout %63 : (tensor<128xi64, #blocked3>) -> tensor<128xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>\n+    %65 = tt.expand_dims %64 {axis = 1 : i32} : (tensor<128xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>) -> tensor<128x1xi64, #blocked4>\n+    %66 = tt.splat %55 : (i64) -> tensor<128x1xi64, #blocked4>\n+    %67 = arith.muli %65, %66 : tensor<128x1xi64, #blocked4>\n+    %68 = tt.broadcast %67 : (tensor<128x1xi64, #blocked4>) -> tensor<128x64xi64, #blocked4>\n+    %69 = triton_gpu.convert_layout %68 : (tensor<128x64xi64, #blocked4>) -> tensor<128x64xi64, #blocked3>\n+    %70 = tt.addptr %59, %69 : tensor<128x64x!tt.ptr<f16, 1>, #blocked3>, tensor<128x64xi64, #blocked3>\n+    %71 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked3>\n+    %72 = arith.extsi %71 : tensor<64xi32, #blocked3> to tensor<64xi64, #blocked3>\n+    %73 = triton_gpu.convert_layout %72 : (tensor<64xi64, #blocked3>) -> tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>\n+    %74 = tt.expand_dims %73 {axis = 0 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>) -> tensor<1x64xi64, #blocked5>\n+    %75 = tt.broadcast %74 : (tensor<1x64xi64, #blocked5>) -> tensor<128x64xi64, #blocked5>\n+    %76 = triton_gpu.convert_layout %75 : (tensor<128x64xi64, #blocked5>) -> tensor<128x64xi64, #blocked3>\n+    %77 = tt.addptr %70, %76 : tensor<128x64x!tt.ptr<f16, 1>, #blocked3>, tensor<128x64xi64, #blocked3>\n+    tt.store %77, %58 {cache = 1 : i32, evict = 1 : i32} : tensor<128x64xf16, #blocked3>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+// CHECK-LABEL: axis_mismatch\n+tt.func @axis_mismatch(%arg0: f32) -> tensor<1xf32, #triton_gpu.slice<{dim = 0, parent = #blocked}>> {\n+// CHECK: %[[R:.+]] = \"tt.reduce\"(%0) <{axis = 1 : i32}>\n+// CHECK: %[[C:.+]] = triton_gpu.convert_layout %[[R]]\n+// CHECK: tt.return %[[C]]\n+  %0 = tt.splat %arg0 : (f32) -> tensor<1x16xf32, #blocked>\n+  %1 = \"tt.reduce\"(%0) <{axis = 1 : i32}> ({\n+    ^bb0(%arg9: f32, %arg10: f32):\n+    %60 = arith.addf %arg9, %arg10 : f32\n+    tt.reduce.return %60 : f32\n+  }) : (tensor<1x16xf32, #blocked>) -> tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+  %2 = triton_gpu.convert_layout %1 : (tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<1xf32, #blocked1>\n+  %3 = triton_gpu.convert_layout %2 : (tensor<1xf32, #blocked1>) -> tensor<1xf32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+  tt.return %3: tensor<1xf32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+}\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+// CHECK-LABEL: reduce_to_scalar\n+//   CHECK-NOT:   triton_gpu.convert_layout\n+//       CHECK:   tt.return\n+tt.func @reduce_to_scalar(%ptr: tensor<1024x!tt.ptr<f32>, #blocked>) -> (f32, i32) {\n+  %0 = tt.load %ptr {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked>\n+  %1 = triton_gpu.convert_layout %0 : (tensor<1024xf32, #blocked>) -> tensor<1024xf32, #blocked1>\n+  %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #blocked1>\n+  %3:2 = \"tt.reduce\"(%1, %2) <{axis = 0 : i32}> ({\n+    ^bb0(%arg7: f32, %arg8: i32, %arg9: f32, %arg10: i32):\n+    %51 = \"triton_gpu.cmpf\"(%arg7, %arg9) <{predicate = 1 : i64}> : (f32, f32) -> i1\n+    %52 = \"triton_gpu.cmpi\"(%arg8, %arg10) <{predicate = 2 : i64}> : (i32, i32) -> i1\n+    %53 = arith.andi %51, %52 : i1\n+    %54 = \"triton_gpu.cmpf\"(%arg7, %arg9) <{predicate = 2 : i64}> : (f32, f32) -> i1\n+    %55 = arith.ori %54, %53 : i1\n+    %56 = arith.select %55, %arg7, %arg9 : f32\n+    %57 = arith.select %55, %arg8, %arg10 : i32\n+    tt.reduce.return %56, %57 : f32, i32\n+  }) : (tensor<1024xf32, #blocked1>, tensor<1024xi32, #blocked1>) -> (f32, i32)\n+  tt.return %3#0, %3#1: f32, i32\n+}\n+}\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+// CHECK-LABEL: whileop\n+//       CHECK: %[[L:.+]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked>\n+//       CHECK: %[[W:.+]] = scf.while (%[[I:.+]] = %[[L]], %{{.*}} = %{{.*}}) : (tensor<1024xf32, #blocked>, i1) -> tensor<1024xf32, #blocked> {\n+//       CHECK:   scf.condition(%{{.*}}) %[[I]] : tensor<1024xf32, #blocked>\n+//       CHECK: } do {\n+//       CHECK: ^bb0(%[[ARG1:.+]]: tensor<1024xf32, #blocked>):\n+//       CHECK:    %[[ADD:.+]] = arith.addf %[[ARG1]], %[[ARG1]] : tensor<1024xf32, #blocked>\n+//       CHECK:    scf.yield %[[ADD]], %{{.*}} : tensor<1024xf32, #blocked>, i1\n+//       CHECK:  }\n+//       CHECK:  tt.store %{{.*}}, %[[W]] {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32, #blocked>\n+tt.func @whileop(%ptr: tensor<1024x!tt.ptr<f32>, #blocked>, %cond: i1) {\n+  %0 = tt.load %ptr {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked>\n+  %1 = triton_gpu.convert_layout %0 : (tensor<1024xf32, #blocked>) -> tensor<1024xf32, #blocked1>\n+  %2 = scf.while (%arg0 = %1, %arg1 = %cond) : (tensor<1024xf32, #blocked1>, i1) -> (tensor<1024xf32, #blocked1>) {\n+      scf.condition(%arg1) %arg0 : tensor<1024xf32, #blocked1>\n+    } do {\n+    ^bb0(%arg0: tensor<1024xf32, #blocked1>):\n+      %4 = triton_gpu.convert_layout %arg0 : (tensor<1024xf32, #blocked1>) -> tensor<1024xf32, #blocked>\n+      %5 = arith.addf %4, %4 : tensor<1024xf32, #blocked>\n+      %6 = triton_gpu.convert_layout %5 : (tensor<1024xf32, #blocked>) -> tensor<1024xf32, #blocked1>\n+      scf.yield %6, %cond : tensor<1024xf32, #blocked1>, i1\n+    }\n+  %3 = triton_gpu.convert_layout %2 : (tensor<1024xf32, #blocked1>) -> tensor<1024xf32, #blocked>\n+  tt.store %ptr, %3 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32, #blocked>\n+  tt.return\n+}\n+}"}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file -tritongpu-optimize-dot-operands -tritongpu-remove-layout-conversions -canonicalize | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -tritongpu-optimize-dot-operands -canonicalize | FileCheck %s\n \n #Cv2 = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #Av2k1 = #triton_gpu.dot_op<{opIdx = 0, parent = #Cv2, kWidth=1}>\n@@ -15,7 +15,7 @@\n #BLR = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #BLC = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [0, 1]}>\n \n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.compute-capability\" = 80} {\n \n // CHECK: tt.func @push_elementwise\n // CHECK: %[[ALOAD:.*]] = tt.load %arg0\n@@ -69,7 +69,7 @@ tt.func @succeeds_if_arg_is_not_convert_layout(\n #blockedA = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #blockedB = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n #mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.compute-capability\" = 80} {\n \n // CHECK: #[[BA:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [], CTASplitNum = [], CTAOrder = []}>\n // CHECK: #[[BB:.*]] = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [], CTASplitNum = [], CTAOrder = []}>\n@@ -104,7 +104,7 @@ tt.func @push_convert_both_operands(\n #blockedA = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #blockedB = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n #mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.compute-capability\" = 80} {\n \n // CHECK: #[[BA:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [], CTASplitNum = [], CTAOrder = []}>\n // CHECK: #[[BB:.*]] = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [], CTASplitNum = [], CTAOrder = []}>"}, {"filename": "third_party/amd_hip_backend", "status": "added", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -0,0 +1 @@\n+Subproject commit d0ad70d55df3ebe11cc80bbb364a91551e6b6248"}]