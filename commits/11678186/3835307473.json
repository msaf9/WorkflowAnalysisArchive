[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -289,7 +289,7 @@ def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n }\n \n def TT_TransOp : TT_Op<\"trans\", [NoSideEffect,\n-                               SameOperandsAndResultElementType]> {\n+                                 SameOperandsAndResultElementType]> {\n \n     let summary = \"transpose a tensor\";\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -39,6 +39,8 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout);\n \n SmallVector<unsigned> getOrder(const Attribute &layout);\n \n+bool isaDistributedLayout(const Attribute &layout);\n+\n } // namespace gpu\n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -25,13 +25,14 @@ ChangeResult SharedMemoryAliasAnalysis::visitOperation(\n   if (maybeSharedAllocationOp(op)) {\n     // These ops may allocate a new shared memory buffer.\n     auto result = op->getResult(0);\n-    // FIXME(Keren): extract and insert are always alias for now\n+    // XXX(Keren): the following ops are always aliasing for now\n     if (isa<tensor::ExtractSliceOp, triton::TransOp>(op)) {\n       // extract_slice %src\n+      // trans %src\n       aliasInfo = AliasInfo(operands[0]->getValue());\n       pessimistic = false;\n-    } else if (isa<tensor::InsertSliceOp>(op) ||\n-               isa<triton::gpu::InsertSliceAsyncOp>(op)) {\n+    } else if (isa<tensor::InsertSliceOp, triton::gpu::InsertSliceAsyncOp>(\n+                   op)) {\n       // insert_slice_async %src, %dst, %index\n       // insert_slice %src into %dst[%offsets]\n       aliasInfo = AliasInfo(operands[1]->getValue());"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 17, "deletions": 3, "changes": 20, "file_content_changes": "@@ -298,10 +298,24 @@ class AllocationAnalysis {\n \n   /// Resolves liveness of all values involved under the root operation.\n   void resolveLiveness() {\n-    // In the SCF dialect, we always have a sequentially nested structure of\n-    // blocks\n+    // Assign an ID to each operation using post-order traversal.\n+    // To achieve the correct liveness range, the parent operation's ID\n+    // should be greater than each of its child operation's ID .\n+    // Example:\n+    //     ...\n+    //     %5 = triton.convert_layout %4\n+    //     %6 = scf.for ... iter_args(%arg0 = %0) -> (i32) {\n+    //       %2 = triton.convert_layout %5\n+    //       ...\n+    //       scf.yield %arg0\n+    //     }\n+    // For example, %5 is defined in the parent region and used in\n+    // the child region, and is not passed as a block argument.\n+    // %6 should should have an ID greater than its child operations,\n+    // otherwise %5 liveness range ends before the child operation's liveness\n+    // range ends.\n     DenseMap<Operation *, size_t> operationId;\n-    operation->walk<WalkOrder::PreOrder>(\n+    operation->walk<WalkOrder::PostOrder>(\n         [&](Operation *op) { operationId[op] = operationId.size(); });\n \n     // Analyze liveness of explicit buffers"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 59, "deletions": 110, "changes": 169, "file_content_changes": "@@ -9,10 +9,12 @@ using ::mlir::LLVM::getStridesFromShapeAndOrder;\n using ::mlir::LLVM::getStructFromElements;\n using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getElemsPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::isaDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n@@ -24,111 +26,63 @@ bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n          dotOperandLayout.getParent() == mmaLayout;\n }\n \n-void storeBlockedToShared(Value src, Value llSrc, ArrayRef<Value> srcStrides,\n-                          ArrayRef<Value> srcIndices, Value dst, Value smemBase,\n-                          Type elemTy, Location loc,\n-                          ConversionPatternRewriter &rewriter) {\n+void storeDistributedToShared(Value src, Value llSrc,\n+                              ArrayRef<Value> dstStrides,\n+                              ArrayRef<SmallVector<Value>> srcIndices,\n+                              Value dst, Value smemBase, Type elemTy,\n+                              Location loc,\n+                              ConversionPatternRewriter &rewriter) {\n   auto srcTy = src.getType().cast<RankedTensorType>();\n   auto srcShape = srcTy.getShape();\n-  assert(srcShape.size() == 2 && \"Unexpected rank of insertSlice\");\n-\n+  assert(srcShape.size() == 2 && \"Unexpected rank of storeDistributedToShared\");\n   auto dstTy = dst.getType().cast<RankedTensorType>();\n-  auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto srcDistributedLayout = srcTy.getEncoding();\n+  if (auto mmaLayout = srcDistributedLayout.dyn_cast<MmaEncodingAttr>()) {\n+    assert((!mmaLayout.isVolta()) &&\n+           \"ConvertLayout MMAv1->Shared is not suppported yet\");\n+  }\n   auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto inOrd = srcBlockedLayout.getOrder();\n+  auto inOrd = getOrder(srcDistributedLayout);\n   auto outOrd = dstSharedLayout.getOrder();\n-  if (inOrd != outOrd)\n-    llvm_unreachable(\n-        \"blocked -> shared with different order not yet implemented\");\n   unsigned inVec =\n-      inOrd == outOrd ? srcBlockedLayout.getSizePerThread()[inOrd[0]] : 1;\n+      inOrd == outOrd ? getContigPerThread(srcDistributedLayout)[inOrd[0]] : 1;\n   unsigned outVec = dstSharedLayout.getVec();\n   unsigned minVec = std::min(outVec, inVec);\n   unsigned perPhase = dstSharedLayout.getPerPhase();\n   unsigned maxPhase = dstSharedLayout.getMaxPhase();\n   unsigned numElems = getElemsPerThread(srcTy);\n+  assert(numElems == srcIndices.size());\n   auto inVals = getElementsFromStruct(loc, llSrc, rewriter);\n-  auto srcAccumSizeInThreads =\n-      product<unsigned>(srcBlockedLayout.getSizePerThread());\n   auto wordTy = vec_ty(elemTy, minVec);\n   auto elemPtrTy = ptr_ty(elemTy);\n-\n-  // TODO: [goostavz] We should make a cache for the calculation of\n-  // emitBaseIndexForBlockedLayout in case backend compiler not being able to\n-  // optimize that\n-  SmallVector<unsigned> srcShapePerCTA = getShapePerCTA(srcBlockedLayout);\n-  SmallVector<unsigned> reps{ceil<unsigned>(srcShape[0], srcShapePerCTA[0]),\n-                             ceil<unsigned>(srcShape[1], srcShapePerCTA[1])};\n-\n-  // Visit each input value in the order they are placed in inVals\n-  //\n-  // Please note that the order was not awaring of blockLayout.getOrder(),\n-  // thus the adjacent elems may not belong to a same word. This could be\n-  // improved if we update the elements order by emitIndicesForBlockedLayout()\n-  SmallVector<unsigned> wordsInEachRep(2);\n-  wordsInEachRep[0] = inOrd[0] == 0\n-                          ? srcBlockedLayout.getSizePerThread()[0] / minVec\n-                          : srcBlockedLayout.getSizePerThread()[0];\n-  wordsInEachRep[1] = inOrd[0] == 0\n-                          ? srcBlockedLayout.getSizePerThread()[1]\n-                          : srcBlockedLayout.getSizePerThread()[1] / minVec;\n   Value outVecVal = i32_val(outVec);\n   Value minVecVal = i32_val(minVec);\n-  auto numWordsEachRep = product<unsigned>(wordsInEachRep);\n-  SmallVector<Value> wordVecs(numWordsEachRep);\n+  Value word;\n   for (unsigned i = 0; i < numElems; ++i) {\n-    if (i % srcAccumSizeInThreads == 0) {\n-      // start of a replication\n-      for (unsigned w = 0; w < numWordsEachRep; ++w) {\n-        wordVecs[w] = undef(wordTy);\n-      }\n-    }\n-    unsigned linearIdxInNanoTile = i % srcAccumSizeInThreads;\n-    auto multiDimIdxInNanoTile = getMultiDimIndex<unsigned>(\n-        linearIdxInNanoTile, srcBlockedLayout.getSizePerThread(), inOrd);\n-    unsigned pos = multiDimIdxInNanoTile[inOrd[0]] % minVec;\n-    multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n-    auto wordVecIdx =\n-        getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep, inOrd);\n-    wordVecs[wordVecIdx] =\n-        insert_element(wordTy, wordVecs[wordVecIdx], inVals[i], i32_val(pos));\n-\n-    if (i % srcAccumSizeInThreads == srcAccumSizeInThreads - 1) {\n-      // end of replication, store the vectors into shared memory\n-      unsigned linearRepIdx = i / srcAccumSizeInThreads;\n-      auto multiDimRepIdx =\n-          getMultiDimIndex<unsigned>(linearRepIdx, reps, inOrd);\n-      for (unsigned linearWordIdx = 0; linearWordIdx < numWordsEachRep;\n-           ++linearWordIdx) {\n-        // step 1: recover the multidim_index from the index of\n-        // input_elements\n-        auto multiDimWordIdx =\n-            getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep, inOrd);\n-        SmallVector<Value> multiDimIdx(2);\n-        auto wordOffset0 = multiDimRepIdx[0] * srcShapePerCTA[0] +\n-                           multiDimWordIdx[0] * (inOrd[0] == 0 ? minVec : 1);\n-        auto wordOffset1 = multiDimRepIdx[1] * srcShapePerCTA[1] +\n-                           multiDimWordIdx[1] * (inOrd[0] == 1 ? minVec : 1);\n-        multiDimIdx[0] = add(srcIndices[0], i32_val(wordOffset0));\n-        multiDimIdx[1] = add(srcIndices[1], i32_val(wordOffset1));\n-\n-        // step 2: do swizzling\n-        Value remained = urem(multiDimIdx[outOrd[0]], outVecVal);\n-        multiDimIdx[outOrd[0]] = udiv(multiDimIdx[outOrd[0]], outVecVal);\n-        Value off_1 = mul(multiDimIdx[outOrd[1]], srcStrides[outOrd[1]]);\n-        Value phaseId = udiv(multiDimIdx[outOrd[1]], i32_val(perPhase));\n-        phaseId = urem(phaseId, i32_val(maxPhase));\n-        Value off_0 = xor_(multiDimIdx[outOrd[0]], phaseId);\n-        off_0 = mul(off_0, outVecVal);\n-        remained = udiv(remained, minVecVal);\n-        off_0 = add(off_0, mul(remained, minVecVal));\n-        Value offset = add(off_1, off_0);\n-\n-        // step 3: store\n-        Value smemAddr = gep(elemPtrTy, smemBase, offset);\n-        smemAddr = bitcast(smemAddr, ptr_ty(wordTy, 3));\n-        store(wordVecs[linearWordIdx], smemAddr);\n-      }\n+    if (i % minVec == 0)\n+      word = undef(wordTy);\n+    word = insert_element(wordTy, word, inVals[i], i32_val(i % minVec));\n+    if (i % minVec == minVec - 1) {\n+      // step 1: recover the multidim_index from the index of\n+      SmallVector<Value> multiDimIdx = srcIndices[i];\n+      SmallVector<Value> dbgVal = srcIndices[i];\n+\n+      // step 2: do swizzling\n+      Value remained = urem(multiDimIdx[outOrd[0]], outVecVal);\n+      multiDimIdx[outOrd[0]] = udiv(multiDimIdx[outOrd[0]], outVecVal);\n+      Value off_1 = mul(multiDimIdx[outOrd[1]], dstStrides[outOrd[1]]);\n+      Value phaseId = udiv(multiDimIdx[outOrd[1]], i32_val(perPhase));\n+      phaseId = urem(phaseId, i32_val(maxPhase));\n+      Value off_0 = xor_(multiDimIdx[outOrd[0]], phaseId);\n+      off_0 = mul(off_0, outVecVal);\n+      remained = udiv(remained, minVecVal);\n+      off_0 = add(off_0, mul(remained, minVecVal));\n+      Value offset = add(off_1, mul(off_0, dstStrides[outOrd[0]]));\n+\n+      // step 3: store\n+      Value smemAddr = gep(elemPtrTy, smemBase, offset);\n+      smemAddr = bitcast(smemAddr, ptr_ty(wordTy, 3));\n+      store(word, smemAddr);\n     }\n   }\n }\n@@ -148,20 +102,15 @@ struct ConvertLayoutOpConversion\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n     Attribute dstLayout = dstTy.getEncoding();\n-    if (srcLayout.isa<BlockedEncodingAttr>() &&\n+    if (isaDistributedLayout(srcLayout) &&\n         dstLayout.isa<SharedEncodingAttr>()) {\n-      return lowerBlockedToShared(op, adaptor, rewriter);\n+      return lowerDistributedToShared(op, adaptor, rewriter);\n     }\n     if (srcLayout.isa<SharedEncodingAttr>() &&\n         dstLayout.isa<DotOperandEncodingAttr>()) {\n       return lowerSharedToDotOperand(op, adaptor, rewriter);\n     }\n-    if ((srcLayout.isa<BlockedEncodingAttr>() ||\n-         srcLayout.isa<MmaEncodingAttr>() ||\n-         srcLayout.isa<SliceEncodingAttr>()) &&\n-        (dstLayout.isa<BlockedEncodingAttr>() ||\n-         dstLayout.isa<MmaEncodingAttr>() ||\n-         dstLayout.isa<SliceEncodingAttr>())) {\n+    if (isaDistributedLayout(srcLayout) && isaDistributedLayout(dstLayout)) {\n       return lowerDistributedToDistributed(op, adaptor, rewriter);\n     }\n     if (srcLayout.isa<MmaEncodingAttr>() &&\n@@ -182,7 +131,7 @@ struct ConvertLayoutOpConversion\n     unsigned rank = shape.size();\n     if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n       auto multiDimOffsetFirstElem =\n-          emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n+          emitBaseIndexForLayout(loc, rewriter, blockedLayout, shape);\n       SmallVector<Value> multiDimOffset(rank);\n       SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n           elemId, getSizePerThread(layout), getOrder(layout));\n@@ -479,8 +428,8 @@ struct ConvertLayoutOpConversion\n   // Swizzling in shared memory to avoid bank conflict. Normally used for\n   // A/B operands of dots.\n   LogicalResult\n-  lowerBlockedToShared(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n-                       ConversionPatternRewriter &rewriter) const {\n+  lowerDistributedToShared(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+                           ConversionPatternRewriter &rewriter) const {\n     auto loc = op.getLoc();\n     Value src = op.src();\n     Value dst = op.result();\n@@ -490,22 +439,20 @@ struct ConvertLayoutOpConversion\n     auto dstShape = dstTy.getShape();\n     assert(srcShape.size() == 2 &&\n            \"Unexpected rank of ConvertLayout(blocked->shared)\");\n-    auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+    auto srcLayout = srcTy.getEncoding();\n     auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n-    auto inOrd = srcBlockedLayout.getOrder();\n+    auto inOrd = getOrder(srcLayout);\n     auto outOrd = dstSharedLayout.getOrder();\n     Value smemBase = getSharedMemoryBase(loc, rewriter, dst);\n     auto elemTy = getTypeConverter()->convertType(srcTy.getElementType());\n     auto elemPtrTy = ptr_ty(getTypeConverter()->convertType(elemTy), 3);\n     smemBase = bitcast(smemBase, elemPtrTy);\n \n-    auto srcStrides =\n-        getStridesFromShapeAndOrder(srcShape, inOrd, loc, rewriter);\n-    auto srcIndices = emitBaseIndexForBlockedLayout(loc, rewriter,\n-                                                    srcBlockedLayout, srcShape);\n-    storeBlockedToShared(src, adaptor.src(), srcStrides, srcIndices, dst,\n-                         smemBase, elemTy, loc, rewriter);\n-\n+    auto dstStrides =\n+        getStridesFromShapeAndOrder(dstShape, outOrd, loc, rewriter);\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n+    storeDistributedToShared(src, adaptor.src(), dstStrides, srcIndices, dst,\n+                             smemBase, elemTy, loc, rewriter);\n     auto smemObj =\n         SharedMemoryObject(smemBase, dstShape, outOrd, loc, rewriter);\n     auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n@@ -680,7 +627,9 @@ struct ConvertLayoutOpConversion\n void populateConvertLayoutOpToLLVMPatterns(\n     mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem, PatternBenefit benefit) {\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n-                                          benefit);\n+                                          indexCacheInfo, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.h", "status": "modified", "additions": 9, "deletions": 5, "changes": 14, "file_content_changes": "@@ -11,14 +11,18 @@ using ::mlir::triton::gpu::DotOperandEncodingAttr;\n bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n                         DotOperandEncodingAttr &dotOperandLayout);\n \n-void storeBlockedToShared(Value src, Value llSrc, ArrayRef<Value> srcStrides,\n-                          ArrayRef<Value> srcIndices, Value dst, Value smemBase,\n-                          Type elemPtrTy, Location loc,\n-                          ConversionPatternRewriter &rewriter);\n+void storeDistributedToShared(Value src, Value llSrc,\n+                              ArrayRef<Value> srcStrides,\n+                              ArrayRef<SmallVector<Value>> srcIndices,\n+                              Value dst, Value smemBase, Type elemPtrTy,\n+                              Location loc,\n+                              ConversionPatternRewriter &rewriter);\n \n void populateConvertLayoutOpToLLVMPatterns(\n     mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem, PatternBenefit benefit);\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 25, "deletions": 19, "changes": 44, "file_content_changes": "@@ -204,7 +204,12 @@ struct DotOpMmaV1ConversionHelper {\n       offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n     }\n \n-    Type f16x2Ty = vec_ty(f16_ty, 2);\n+    Type elemX2Ty = vec_ty(f16_ty, 2);\n+    Type elemPtrTy = ptr_ty(f16_ty);\n+    if (tensorTy.getElementType().isBF16()) {\n+      elemX2Ty = vec_ty(i16_ty, 2);\n+      elemPtrTy = ptr_ty(i16_ty);\n+    }\n \n     // prepare arguments\n     SmallVector<Value> ptrA(numPtrA);\n@@ -213,30 +218,28 @@ struct DotOpMmaV1ConversionHelper {\n     for (int i = 0; i < numPtrA; i++)\n       ptrA[i] = gep(ptr_ty(f16_ty), smemBase, offA[i]);\n \n-    Type f16PtrTy = ptr_ty(f16_ty);\n-\n     auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n       vals[{m, k}] = {val0, val1};\n     };\n     auto loadA = [&](int m, int k) {\n       int offidx = (isARow ? k / 4 : m) % numPtrA;\n-      Value thePtrA = gep(f16PtrTy, smemBase, offA[offidx]);\n+      Value thePtrA = gep(elemPtrTy, smemBase, offA[offidx]);\n \n       int stepAM = isARow ? m : m / numPtrA * numPtrA;\n       int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n       Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),\n                          mul(i32_val(stepAK), strideAK));\n-      Value pa = gep(f16PtrTy, thePtrA, offset);\n+      Value pa = gep(elemPtrTy, thePtrA, offset);\n       Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n       Value ha = load(bitcast(pa, aPtrTy));\n       // record lds that needs to be moved\n-      Value ha00 = bitcast(extract_element(ha, i32_val(0)), f16x2Ty);\n-      Value ha01 = bitcast(extract_element(ha, i32_val(1)), f16x2Ty);\n+      Value ha00 = bitcast(extract_element(ha, i32_val(0)), elemX2Ty);\n+      Value ha01 = bitcast(extract_element(ha, i32_val(1)), elemX2Ty);\n       ld(has, m, k, ha00, ha01);\n \n       if (vecA > 4) {\n-        Value ha10 = bitcast(extract_element(ha, i32_val(2)), f16x2Ty);\n-        Value ha11 = bitcast(extract_element(ha, i32_val(3)), f16x2Ty);\n+        Value ha10 = bitcast(extract_element(ha, i32_val(2)), elemX2Ty);\n+        Value ha11 = bitcast(extract_element(ha, i32_val(3)), elemX2Ty);\n         if (isARow)\n           ld(has, m, k + 4, ha10, ha11);\n         else\n@@ -256,7 +259,7 @@ struct DotOpMmaV1ConversionHelper {\n       elems.push_back(item.second.second);\n     }\n \n-    Type resTy = struct_ty(SmallVector<Type>(elems.size(), f16x2Ty));\n+    Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n     Value res = getStructFromElements(loc, elems, rewriter, resTy);\n     return res;\n   }\n@@ -319,8 +322,12 @@ struct DotOpMmaV1ConversionHelper {\n       offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));\n     }\n \n-    Type f16PtrTy = ptr_ty(f16_ty);\n-    Type f16x2Ty = vec_ty(f16_ty, 2);\n+    Type elemPtrTy = ptr_ty(f16_ty);\n+    Type elemX2Ty = vec_ty(f16_ty, 2);\n+    if (tensorTy.getElementType().isBF16()) {\n+      elemPtrTy = ptr_ty(i16_ty);\n+      elemX2Ty = vec_ty(i16_ty, 2);\n+    }\n \n     SmallVector<Value> ptrB(numPtrB);\n     ValueTable hbs;\n@@ -339,17 +346,17 @@ struct DotOpMmaV1ConversionHelper {\n       int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n       Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n                          mul(i32_val(stepBK), strideBK));\n-      Value pb = gep(f16PtrTy, thePtrB, offset);\n+      Value pb = gep(elemPtrTy, thePtrB, offset);\n \n       Value hb =\n           load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n       // record lds that needs to be moved\n-      Value hb00 = bitcast(extract_element(hb, i32_val(0)), f16x2Ty);\n-      Value hb01 = bitcast(extract_element(hb, i32_val(1)), f16x2Ty);\n+      Value hb00 = bitcast(extract_element(hb, i32_val(0)), elemX2Ty);\n+      Value hb01 = bitcast(extract_element(hb, i32_val(1)), elemX2Ty);\n       ld(hbs, n, K, hb00, hb01);\n       if (vecB > 4) {\n-        Value hb10 = bitcast(extract_element(hb, i32_val(2)), f16x2Ty);\n-        Value hb11 = bitcast(extract_element(hb, i32_val(3)), f16x2Ty);\n+        Value hb10 = bitcast(extract_element(hb, i32_val(2)), elemX2Ty);\n+        Value hb11 = bitcast(extract_element(hb, i32_val(3)), elemX2Ty);\n         if (isBRow)\n           ld(hbs, n + 1, K, hb10, hb11);\n         else\n@@ -369,8 +376,7 @@ struct DotOpMmaV1ConversionHelper {\n       elems.push_back(item.second.first);\n       elems.push_back(item.second.second);\n     }\n-    Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n-    Type resTy = struct_ty(SmallVector<Type>(elems.size(), fp16x2Ty));\n+    Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n     Value res = getStructFromElements(loc, elems, rewriter, resTy);\n     return res;\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "file_content_changes": "@@ -14,7 +14,7 @@ using ::mlir::triton::gpu::getElemsPerThread;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n // Contains some helper functions for both Load and Store conversions.\n-struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n+struct LoadStoreConversionBase {\n   explicit LoadStoreConversionBase(AxisInfoAnalysis &axisAnalysisPass)\n       : axisAnalysisPass(axisAnalysisPass) {}\n \n@@ -639,10 +639,9 @@ struct InsertSliceOpConversion\n     auto smemBase = gep(elemPtrTy, smemObj.base, offset);\n \n     auto llSrc = adaptor.source();\n-    auto srcIndices =\n-        emitBaseIndexForBlockedLayout(loc, rewriter, srcLayout, srcShape);\n-    storeBlockedToShared(src, llSrc, srcStrides, srcIndices, dst, smemBase,\n-                         elemTy, loc, rewriter);\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n+    storeDistributedToShared(src, llSrc, srcStrides, srcIndices, dst, smemBase,\n+                             elemTy, loc, rewriter);\n     // Barrier is not necessary.\n     // The membar pass knows that it writes to shared memory and will handle it\n     // properly.\n@@ -657,12 +656,12 @@ struct InsertSliceAsyncOpConversion\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::gpu::InsertSliceAsyncOp>::ConvertTritonGPUOpToLLVMPattern;\n \n-  InsertSliceAsyncOpConversion(LLVMTypeConverter &converter,\n-                               const Allocation *allocation, Value smem,\n-                               AxisInfoAnalysis &axisAnalysisPass,\n-                               PatternBenefit benefit)\n+  InsertSliceAsyncOpConversion(\n+      LLVMTypeConverter &converter, const Allocation *allocation, Value smem,\n+      ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+      AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::gpu::InsertSliceAsyncOp>(\n-            converter, allocation, smem, benefit),\n+            converter, allocation, smem, indexCacheInfo, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n \n   LogicalResult\n@@ -865,20 +864,21 @@ struct InsertSliceAsyncOpConversion\n   }\n };\n \n-void populateLoadStoreOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                       RewritePatternSet &patterns,\n-                                       int numWarps,\n-                                       AxisInfoAnalysis &axisInfoAnalysis,\n-                                       const Allocation *allocation, Value smem,\n-                                       PatternBenefit benefit) {\n+void populateLoadStoreOpToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n   patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<StoreOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<AtomicCASOpConversion>(typeConverter, allocation, smem,\n                                       axisInfoAnalysis, benefit);\n   patterns.add<AtomicRMWOpConversion>(typeConverter, allocation, smem,\n                                       axisInfoAnalysis, benefit);\n   patterns.add<InsertSliceOpConversion>(typeConverter, allocation, smem,\n-                                        benefit);\n+                                        indexCacheInfo, benefit);\n   patterns.add<InsertSliceAsyncOpConversion>(typeConverter, allocation, smem,\n-                                             axisInfoAnalysis, benefit);\n+                                             indexCacheInfo, axisInfoAnalysis,\n+                                             benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.h", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -6,11 +6,11 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-void populateLoadStoreOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                       RewritePatternSet &patterns,\n-                                       int numWarps,\n-                                       AxisInfoAnalysis &axisInfoAnalysis,\n-                                       const Allocation *allocation, Value smem,\n-                                       PatternBenefit benefit);\n+void populateLoadStoreOpToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 9, "deletions": 7, "changes": 16, "file_content_changes": "@@ -164,7 +164,7 @@ struct ReduceOpConversion\n     auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n \n     SmallVector<SmallVector<unsigned>> offset =\n-        emitOffsetForBlockedLayout(srcLayout, srcShape);\n+        emitOffsetForLayout(srcLayout, srcShape);\n \n     std::map<SmallVector<unsigned>, Value> accs;\n     std::map<SmallVector<unsigned>, Value> accIndices;\n@@ -479,10 +479,12 @@ struct ReduceOpConversion\n   }\n };\n \n-void populateReduceOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                    RewritePatternSet &patterns, int numWarps,\n-                                    AxisInfoAnalysis &axisInfoAnalysis,\n-                                    const Allocation *allocation, Value smem,\n-                                    PatternBenefit benefit) {\n-  patterns.add<ReduceOpConversion>(typeConverter, allocation, smem, benefit);\n+void populateReduceOpToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n+  patterns.add<ReduceOpConversion>(typeConverter, allocation, smem,\n+                                   indexCacheInfo, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.h", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -6,10 +6,11 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-void populateReduceOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                    RewritePatternSet &patterns, int numWarps,\n-                                    AxisInfoAnalysis &axisInfoAnalysis,\n-                                    const Allocation *allocation, Value smem,\n-                                    PatternBenefit benefit);\n+void populateReduceOpToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n \n #endif\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 15, "deletions": 9, "changes": 24, "file_content_changes": "@@ -63,6 +63,7 @@ struct BroadcastOpConversion\n     auto srcShape = srcTy.getShape();\n     auto resultShape = resultTy.getShape();\n     unsigned rank = srcTy.getRank();\n+\n     assert(rank == resultTy.getRank());\n     auto order = triton::gpu::getOrder(srcLayout);\n     auto srcOffsets = emitOffsetForLayout(srcLayout, srcShape);\n@@ -272,9 +273,13 @@ struct PrintfOpConversion\n struct MakeRangeOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp> {\n \n-  MakeRangeOpConversion(LLVMTypeConverter &converter, PatternBenefit benefit)\n-      : ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp>(converter,\n-                                                             benefit) {}\n+  MakeRangeOpConversion(\n+      LLVMTypeConverter &converter,\n+      ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+      PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp>(\n+            converter, /*Allocation*/ nullptr, Value{}, indexCacheInfo,\n+            benefit) {}\n \n   LogicalResult\n   matchAndRewrite(triton::MakeRangeOp op, OpAdaptor adaptor,\n@@ -500,11 +505,12 @@ struct AsyncWaitOpConversion\n   }\n };\n \n-void populateTritonGPUToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                     RewritePatternSet &patterns, int numWarps,\n-                                     AxisInfoAnalysis &axisInfoAnalysis,\n-                                     const Allocation *allocation, Value smem,\n-                                     PatternBenefit benefit) {\n+void populateTritonGPUToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n   patterns.add<AddPtrOpConversion>(typeConverter, benefit);\n   patterns.add<AllocTensorOpConversion>(typeConverter, allocation, smem,\n                                         benefit);\n@@ -515,7 +521,7 @@ void populateTritonGPUToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n   patterns.add<GetNumProgramsOpConversion>(typeConverter, benefit);\n-  patterns.add<MakeRangeOpConversion>(typeConverter, benefit);\n+  patterns.add<MakeRangeOpConversion>(typeConverter, indexCacheInfo, benefit);\n   patterns.add<ReturnOpConversion>(typeConverter, benefit);\n   patterns.add<PrintfOpConversion>(typeConverter, benefit);\n }\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -6,10 +6,11 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-void populateTritonGPUToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                     RewritePatternSet &patterns, int numWarps,\n-                                     AxisInfoAnalysis &axisInfoAnalysis,\n-                                     const Allocation *allocation, Value smem,\n-                                     PatternBenefit benefit);\n+void populateTritonGPUToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 216, "deletions": 105, "changes": 321, "file_content_changes": "@@ -18,7 +18,6 @@ using ::mlir::LLVM::SharedMemoryObject;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;\n-\n // FuncOpConversion/FuncOpConversionBase is borrowed from\n // https://github.com/llvm/llvm-project/blob/fae656b2dd80246c3c6f01e9c77c49560368752c/mlir/lib/Conversion/FuncToLLVM/FuncToLLVM.cpp#L276\n // since it is not exposed on header files in mlir v14\n@@ -128,7 +127,60 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<FuncOp> {\n   }\n };\n \n-struct ConvertTritonGPUOpToLLVMPatternBase {\n+using IndexCacheKeyT = std::pair<Attribute, SmallVector<int64_t>>;\n+\n+struct CacheKeyDenseMapInfo {\n+  static IndexCacheKeyT getEmptyKey() {\n+    auto *pointer = llvm::DenseMapInfo<void *>::getEmptyKey();\n+    return std::make_pair(\n+        mlir::Attribute(static_cast<mlir::Attribute::ImplType *>(pointer)),\n+        SmallVector<int64_t>{});\n+  }\n+  static IndexCacheKeyT getTombstoneKey() {\n+    auto *pointer = llvm::DenseMapInfo<void *>::getTombstoneKey();\n+    return std::make_pair(\n+        mlir::Attribute(static_cast<mlir::Attribute::ImplType *>(pointer)),\n+        SmallVector<int64_t>{std::numeric_limits<int64_t>::max()});\n+  }\n+  static unsigned getHashValue(IndexCacheKeyT key) {\n+    return llvm::hash_combine(\n+        mlir::hash_value(key.first),\n+        llvm::hash_combine_range(key.second.begin(), key.second.end()));\n+  }\n+  static bool isEqual(IndexCacheKeyT LHS, IndexCacheKeyT RHS) {\n+    return LHS == RHS;\n+  }\n+};\n+\n+class ConvertTritonGPUOpToLLVMPatternBase {\n+public:\n+  // Two levels of value cache in emitting indices calculation:\n+  // Key: pair<layout, shape>\n+  struct IndexCacheInfo {\n+    DenseMap<IndexCacheKeyT, SmallVector<Value>, CacheKeyDenseMapInfo>\n+        *baseIndexCache;\n+    DenseMap<IndexCacheKeyT, SmallVector<SmallVector<Value>>,\n+             CacheKeyDenseMapInfo> *indexCache;\n+    OpBuilder::InsertPoint *indexInsertPoint;\n+  };\n+\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(LLVMTypeConverter &typeConverter)\n+      : converter(&typeConverter) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(LLVMTypeConverter &typeConverter,\n+                                               const Allocation *allocation,\n+                                               Value smem)\n+      : converter(&typeConverter), allocation(allocation), smem(smem) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(LLVMTypeConverter &typeConverter,\n+                                               const Allocation *allocation,\n+                                               Value smem,\n+                                               IndexCacheInfo indexCacheInfo)\n+      : converter(&typeConverter), indexCacheInfo(indexCacheInfo),\n+        allocation(allocation), smem(smem) {}\n+\n+  LLVMTypeConverter *getTypeConverter() const { return converter; }\n+\n   static Value\n   getStructFromSharedMemoryObject(Location loc,\n                                   const SharedMemoryObject &smemObj,\n@@ -139,25 +191,6 @@ struct ConvertTritonGPUOpToLLVMPatternBase {\n         LLVM::LLVMStructType::getLiteral(rewriter.getContext(), types);\n     return getStructFromElements(loc, elems, rewriter, structTy);\n   }\n-};\n-\n-template <typename SourceOp>\n-class ConvertTritonGPUOpToLLVMPattern\n-    : public ConvertOpToLLVMPattern<SourceOp>,\n-      public ConvertTritonGPUOpToLLVMPatternBase {\n-public:\n-  using OpAdaptor = typename SourceOp::Adaptor;\n-\n-  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n-                                           PatternBenefit benefit = 1)\n-      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n-\n-  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n-                                           const Allocation *allocation,\n-                                           Value smem,\n-                                           PatternBenefit benefit = 1)\n-      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n-        allocation(allocation), smem(smem) {}\n \n   Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n     auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n@@ -169,6 +202,23 @@ class ConvertTritonGPUOpToLLVMPattern\n     return threadId;\n   }\n \n+  // -----------------------------------------------------------------------\n+  // Shared memory utilities\n+  // -----------------------------------------------------------------------\n+  template <typename T>\n+  Value getSharedMemoryBase(Location loc, ConversionPatternRewriter &rewriter,\n+                            T value) const {\n+\n+    auto ptrTy = LLVM::LLVMPointerType::get(\n+        this->getTypeConverter()->convertType(rewriter.getI8Type()), 3);\n+    auto bufferId = allocation->getBufferId(value);\n+    assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n+    size_t offset = allocation->getOffset(bufferId);\n+    Value offVal = idx_val(offset);\n+    Value base = gep(ptrTy, smem, offVal);\n+    return base;\n+  }\n+\n   // -----------------------------------------------------------------------\n   // Utilities\n   // -----------------------------------------------------------------------\n@@ -242,6 +292,116 @@ class ConvertTritonGPUOpToLLVMPattern\n     return ret;\n   }\n \n+  struct SmallVectorKeyInfo {\n+    static unsigned getHashValue(const SmallVector<unsigned> &key) {\n+      return llvm::hash_combine_range(key.begin(), key.end());\n+    }\n+    static bool isEqual(const SmallVector<unsigned> &lhs,\n+                        const SmallVector<unsigned> &rhs) {\n+      return lhs == rhs;\n+    }\n+    static SmallVector<unsigned> getEmptyKey() {\n+      return SmallVector<unsigned>();\n+    }\n+    static SmallVector<unsigned> getTombstoneKey() {\n+      return {std::numeric_limits<unsigned>::max()};\n+    }\n+  };\n+\n+  // -----------------------------------------------------------------------\n+  // Get offsets / indices for any layout\n+  // -----------------------------------------------------------------------\n+\n+  SmallVector<Value> emitBaseIndexForLayout(Location loc,\n+                                            ConversionPatternRewriter &rewriter,\n+                                            const Attribute &layout,\n+                                            ArrayRef<int64_t> shape) const {\n+    IndexCacheKeyT key = std::make_pair(layout, llvm::to_vector(shape));\n+    auto cache = indexCacheInfo.baseIndexCache;\n+    assert(cache && \"baseIndexCache is nullptr\");\n+    auto insertPt = indexCacheInfo.indexInsertPoint;\n+    if (cache->count(key) > 0) {\n+      return cache->lookup(key);\n+    } else {\n+      ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+      restoreInsertionPointIfSet(insertPt, rewriter);\n+      SmallVector<Value> result;\n+      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+        result =\n+            emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n+      } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+        if (mmaLayout.isVolta())\n+          result = emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, shape);\n+        if (mmaLayout.isAmpere())\n+          result = emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, shape);\n+      } else {\n+        llvm_unreachable(\"unsupported emitBaseIndexForLayout\");\n+      }\n+      cache->insert(std::make_pair(key, result));\n+      *insertPt = rewriter.saveInsertionPoint();\n+      return result;\n+    }\n+  }\n+\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForLayout(const Attribute &layout, ArrayRef<int64_t> shape) const {\n+    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n+      return emitOffsetForBlockedLayout(blockedLayout, shape);\n+    if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+      if (mmaLayout.isVolta())\n+        return emitOffsetForMmaLayoutV1(mmaLayout, shape);\n+      if (mmaLayout.isAmpere())\n+        return emitOffsetForMmaLayoutV2(mmaLayout, shape);\n+    }\n+    llvm_unreachable(\"unsupported emitOffsetForLayout\");\n+  }\n+\n+  // -----------------------------------------------------------------------\n+  // Emit indices\n+  // -----------------------------------------------------------------------\n+  SmallVector<SmallVector<Value>> emitIndices(Location loc,\n+                                              ConversionPatternRewriter &b,\n+                                              const Attribute &layout,\n+                                              ArrayRef<int64_t> shape) const {\n+    IndexCacheKeyT key(layout, llvm::to_vector(shape));\n+    auto cache = indexCacheInfo.indexCache;\n+    assert(cache && \"indexCache is nullptr\");\n+    auto insertPt = indexCacheInfo.indexInsertPoint;\n+    if (cache->count(key) > 0) {\n+      return cache->lookup(key);\n+    } else {\n+      ConversionPatternRewriter::InsertionGuard guard(b);\n+      restoreInsertionPointIfSet(insertPt, b);\n+      SmallVector<SmallVector<Value>> result;\n+      if (auto blocked = layout.dyn_cast<BlockedEncodingAttr>()) {\n+        result = emitIndicesForDistributedLayout(loc, b, blocked, shape);\n+      } else if (auto mma = layout.dyn_cast<MmaEncodingAttr>()) {\n+        result = emitIndicesForDistributedLayout(loc, b, mma, shape);\n+      } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n+        result = emitIndicesForSliceLayout(loc, b, slice, shape);\n+      } else {\n+        llvm_unreachable(\n+            \"emitIndices for layouts other than blocked & slice not \"\n+            \"implemented yet\");\n+      }\n+      cache->insert(std::make_pair(key, result));\n+      *insertPt = b.saveInsertionPoint();\n+      return result;\n+    }\n+  }\n+\n+private:\n+  void restoreInsertionPointIfSet(OpBuilder::InsertPoint *insertPt,\n+                                  ConversionPatternRewriter &rewriter) const {\n+    if (insertPt->isSet()) {\n+      rewriter.restoreInsertionPoint(*insertPt);\n+    } else {\n+      auto func =\n+          rewriter.getInsertionPoint()->getParentOfType<LLVM::LLVMFuncOp>();\n+      rewriter.setInsertionPointToStart(&func.getBody().front());\n+    }\n+  }\n+\n   // -----------------------------------------------------------------------\n   // Blocked layout indices\n   // -----------------------------------------------------------------------\n@@ -411,38 +571,6 @@ class ConvertTritonGPUOpToLLVMPattern\n     return ret;\n   }\n \n-  // -----------------------------------------------------------------------\n-  // Get offsets / indices for any layout\n-  // -----------------------------------------------------------------------\n-\n-  SmallVector<Value> emitBaseIndexForLayout(Location loc,\n-                                            ConversionPatternRewriter &rewriter,\n-                                            const Attribute &layout,\n-                                            ArrayRef<int64_t> shape) const {\n-    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n-      return emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n-    if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-      if (mmaLayout.isVolta())\n-        return emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, shape);\n-      if (mmaLayout.isAmpere())\n-        return emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, shape);\n-    }\n-    llvm_unreachable(\"unsupported emitBaseIndexForLayout\");\n-  }\n-\n-  SmallVector<SmallVector<unsigned>>\n-  emitOffsetForLayout(const Attribute &layout, ArrayRef<int64_t> shape) const {\n-    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n-      return emitOffsetForBlockedLayout(blockedLayout, shape);\n-    if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-      if (mmaLayout.isVolta())\n-        return emitOffsetForMmaLayoutV1(mmaLayout, shape);\n-      if (mmaLayout.isAmpere())\n-        return emitOffsetForMmaLayoutV2(mmaLayout, shape);\n-    }\n-    llvm_unreachable(\"unsupported emitOffsetForLayout\");\n-  }\n-\n   // Emit indices calculation within each ConversionPattern, and returns a\n   // [elemsPerThread X rank] index matrix.\n \n@@ -470,22 +598,6 @@ class ConvertTritonGPUOpToLLVMPattern\n     return multiDimIdx;\n   }\n \n-  struct SmallVectorKeyInfo {\n-    static unsigned getHashValue(const SmallVector<unsigned> &key) {\n-      return llvm::hash_combine_range(key.begin(), key.end());\n-    }\n-    static bool isEqual(const SmallVector<unsigned> &lhs,\n-                        const SmallVector<unsigned> &rhs) {\n-      return lhs == rhs;\n-    }\n-    static SmallVector<unsigned> getEmptyKey() {\n-      return SmallVector<unsigned>();\n-    }\n-    static SmallVector<unsigned> getTombstoneKey() {\n-      return {std::numeric_limits<unsigned>::max()};\n-    }\n-  };\n-\n   SmallVector<SmallVector<Value>>\n   emitIndicesForSliceLayout(Location loc, ConversionPatternRewriter &rewriter,\n                             const SliceEncodingAttr &sliceLayout,\n@@ -505,46 +617,45 @@ class ConvertTritonGPUOpToLLVMPattern\n     return resultIndices;\n   }\n \n-  // -----------------------------------------------------------------------\n-  // Emit indices\n-  // -----------------------------------------------------------------------\n-  SmallVector<SmallVector<Value>> emitIndices(Location loc,\n-                                              ConversionPatternRewriter &b,\n-                                              const Attribute &layout,\n-                                              ArrayRef<int64_t> shape) const {\n-    if (auto blocked = layout.dyn_cast<BlockedEncodingAttr>()) {\n-      return emitIndicesForDistributedLayout(loc, b, blocked, shape);\n-    } else if (auto mma = layout.dyn_cast<MmaEncodingAttr>()) {\n-      return emitIndicesForDistributedLayout(loc, b, mma, shape);\n-    } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n-      return emitIndicesForSliceLayout(loc, b, slice, shape);\n-    } else {\n-      assert(0 && \"emitIndices for layouts other than blocked & slice not \"\n-                  \"implemented yet\");\n-      return {};\n-    }\n-  }\n+protected:\n+  LLVMTypeConverter *converter;\n+  const Allocation *allocation;\n+  Value smem;\n+  IndexCacheInfo indexCacheInfo;\n+};\n \n-  // -----------------------------------------------------------------------\n-  // Shared memory utilities\n-  // -----------------------------------------------------------------------\n-  template <typename T>\n-  Value getSharedMemoryBase(Location loc, ConversionPatternRewriter &rewriter,\n-                            T value) const {\n+template <typename SourceOp>\n+class ConvertTritonGPUOpToLLVMPattern\n+    : public ConvertOpToLLVMPattern<SourceOp>,\n+      public ConvertTritonGPUOpToLLVMPatternBase {\n+public:\n+  using OpAdaptor = typename SourceOp::Adaptor;\n \n-    auto ptrTy = LLVM::LLVMPointerType::get(\n-        this->getTypeConverter()->convertType(rewriter.getI8Type()), 3);\n-    auto bufferId = allocation->getBufferId(value);\n-    assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n-    size_t offset = allocation->getOffset(bufferId);\n-    Value offVal = idx_val(offset);\n-    Value base = gep(ptrTy, smem, offVal);\n-    return base;\n-  }\n+  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n+                                           PatternBenefit benefit = 1)\n+      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n+                                           const Allocation *allocation,\n+                                           Value smem,\n+                                           PatternBenefit benefit = 1)\n+      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation, smem) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n+                                           const Allocation *allocation,\n+                                           Value smem,\n+                                           IndexCacheInfo indexCacheInfo,\n+                                           PatternBenefit benefit = 1)\n+      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation, smem,\n+                                            indexCacheInfo) {}\n \n protected:\n-  const Allocation *allocation;\n-  Value smem;\n+  LLVMTypeConverter *getTypeConverter() const {\n+    return ((ConvertTritonGPUOpToLLVMPatternBase *)this)->getTypeConverter();\n+  }\n };\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 15, "deletions": 4, "changes": 19, "file_content_changes": "@@ -170,16 +170,20 @@ class ConvertTritonGPUToLLVM\n     // We set a higher benefit here to ensure triton's patterns runs before\n     // arith patterns for some encoding not supported by the community\n     // patterns.\n+    OpBuilder::InsertPoint indexInsertPoint;\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo indexCacheInfo{\n+        &baseIndexCache, &indexCache, &indexInsertPoint};\n+\n     RewritePatternSet patterns(context);\n \n     // Normal conversions\n     populateTritonGPUToLLVMPatterns(typeConverter, patterns, numWarps,\n                                     axisInfoAnalysis, &allocation, smem,\n-                                    /*benefit=*/10);\n+                                    indexCacheInfo, /*benefit=*/10);\n     // ConvertLayoutOp\n     populateConvertLayoutOpToLLVMPatterns(typeConverter, patterns, numWarps,\n                                           axisInfoAnalysis, &allocation, smem,\n-                                          /*benefit=*/10);\n+                                          indexCacheInfo, /*benefit=*/10);\n     // DotOp\n     populateDotOpToLLVMPatterns(typeConverter, patterns, numWarps,\n                                 axisInfoAnalysis, &allocation, smem,\n@@ -191,11 +195,11 @@ class ConvertTritonGPUToLLVM\n     // LoadStoreOp\n     populateLoadStoreOpToLLVMPatterns(typeConverter, patterns, numWarps,\n                                       axisInfoAnalysis, &allocation, smem,\n-                                      /*benefit=*/10);\n+                                      indexCacheInfo, /*benefit=*/10);\n     // ReduceOp\n     populateReduceOpToLLVMPatterns(typeConverter, patterns, numWarps,\n                                    axisInfoAnalysis, &allocation, smem,\n-                                   /*benefit=*/10);\n+                                   indexCacheInfo, /*benefit=*/10);\n     // ViewOp\n     populateViewOpToLLVMPatterns(typeConverter, patterns, numWarps,\n                                  axisInfoAnalysis, &allocation, smem,\n@@ -215,6 +219,13 @@ class ConvertTritonGPUToLLVM\n private:\n   Value smem;\n \n+  using IndexCacheKeyT = std::pair<Attribute, SmallVector<int64_t>>;\n+  DenseMap<IndexCacheKeyT, SmallVector<Value>, CacheKeyDenseMapInfo>\n+      baseIndexCache;\n+  DenseMap<IndexCacheKeyT, SmallVector<SmallVector<Value>>,\n+           CacheKeyDenseMapInfo>\n+      indexCache;\n+\n   int computeCapability{};\n \n   void initSharedMemory(size_t size,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 39, "deletions": 0, "changes": 39, "file_content_changes": "@@ -4,11 +4,13 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n+using ::mlir::LLVM::DotOpFMAConversionHelper;\n using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n using ::mlir::LLVM::DotOpMmaV2ConversionHelper;\n using ::mlir::LLVM::getElementsFromStruct;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStructFromElements;\n+using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::getElemsPerThread;\n \n struct SplatOpConversion\n@@ -38,6 +40,11 @@ struct SplatOpConversion\n           LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);\n \n       return getStructFromElements(loc, elems, rewriter, structTy);\n+    } else if (auto dotLayout =\n+                   tensorTy.getEncoding()\n+                       .dyn_cast<triton::gpu::DotOperandEncodingAttr>()) {\n+      return convertSplatLikeOpWithDotOperandLayout(\n+          dotLayout, resType, elemType, constVal, typeConverter, rewriter, loc);\n     } else if (auto mmaLayout =\n                    tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>()) {\n       return convertSplatLikeOpWithMmaLayout(\n@@ -48,6 +55,38 @@ struct SplatOpConversion\n     return {};\n   }\n \n+  static Value convertSplatLikeOpWithDotOperandLayout(\n+      const triton::gpu::DotOperandEncodingAttr &layout, Type resType,\n+      Type elemType, Value constVal, TypeConverter *typeConverter,\n+      ConversionPatternRewriter &rewriter, Location loc) {\n+    auto tensorTy = resType.cast<RankedTensorType>();\n+    auto shape = tensorTy.getShape();\n+    auto parent = layout.getParent();\n+    int numElems{};\n+    if (auto mmaLayout = parent.dyn_cast<MmaEncodingAttr>()) {\n+      if (mmaLayout.isAmpere()) {\n+        numElems = layout.getOpIdx() == 0\n+                       ? MMA16816ConversionHelper::getANumElemsPerThread(\n+                             tensorTy, mmaLayout.getWarpsPerCTA()[0])\n+                       : MMA16816ConversionHelper::getBNumElemsPerThread(\n+                             tensorTy, mmaLayout.getWarpsPerCTA()[1]);\n+      } else if (mmaLayout.isVolta()) {\n+        DotOpMmaV1ConversionHelper helper(mmaLayout);\n+        numElems = layout.getOpIdx() == 0\n+                       ? helper.numElemsPerThreadA(shape, {0, 1})\n+                       : helper.numElemsPerThreadB(shape, {0, 1});\n+      }\n+    } else if (auto blockedLayout = parent.dyn_cast<BlockedEncodingAttr>()) {\n+      numElems = DotOpFMAConversionHelper::getNumElemsPerThread(shape, layout);\n+    } else {\n+      assert(false && \"Unsupported layout found\");\n+    }\n+    auto structTy = LLVM::LLVMStructType::getLiteral(\n+        rewriter.getContext(), SmallVector<Type>(numElems, elemType));\n+    return getStructFromElements(loc, SmallVector<Value>(numElems, constVal),\n+                                 rewriter, structTy);\n+  }\n+\n   static Value convertSplatLikeOpWithMmaLayout(\n       const MmaEncodingAttr &layout, Type resType, Type elemType,\n       Value constVal, TypeConverter *typeConverter,"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -254,6 +254,11 @@ SmallVector<unsigned> getOrder(const Attribute &layout) {\n   }\n };\n \n+bool isaDistributedLayout(const Attribute &layout) {\n+  return layout.isa<BlockedEncodingAttr>() || layout.isa<MmaEncodingAttr>() ||\n+         layout.isa<SliceEncodingAttr>();\n+}\n+\n } // namespace gpu\n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 224, "deletions": 3, "changes": 227, "file_content_changes": "@@ -22,6 +22,10 @@\n using namespace mlir;\n namespace {\n #include \"TritonGPUCombine.inc\"\n+using triton::DotOp;\n+using triton::gpu::ConvertLayoutOp;\n+using triton::gpu::DotOperandEncodingAttr;\n+using triton::gpu::MmaEncodingAttr;\n \n // -----------------------------------------------------------------------------\n //\n@@ -1019,6 +1023,7 @@ class OptimizeConvertToDotOperand : public mlir::RewritePattern {\n         dstDotOperandLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n     if ((order[0] == 1 && isMMAv1Row) || (order[0] == 0 && !isMMAv1Row))\n       return failure();\n+\n     auto newIsRow = BoolAttr::get(op->getContext(), !isMMAv1Row);\n     auto newDstEncoding = triton::gpu::DotOperandEncodingAttr::get(\n         op->getContext(), dstDotOperandLayout.getOpIdx(),\n@@ -1060,7 +1065,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto dotOp = cast<triton::DotOp>(op);\n     // TODO: Check data-types and SM compatibility\n     auto oldRetType = dotOp.getResult().getType().cast<RankedTensorType>();\n-    if (oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n+    if (!oldRetType.getEncoding() ||\n+        oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n       return failure();\n \n     auto AType = dotOp.getOperand(0).getType().cast<RankedTensorType>();\n@@ -1170,7 +1176,8 @@ class FixupLoop : public mlir::RewritePattern {\n     for (size_t i = 0; i < newInitArgs.size(); i++) {\n       auto initArg = newInitArgs[i];\n       auto regionArg = forOp.getRegionIterArgs()[i];\n-      if (newInitArgs[i].getType() != forOp.getRegionIterArgs()[i].getType()) {\n+      if (newInitArgs[i].getType() != forOp.getRegionIterArgs()[i].getType() ||\n+          newInitArgs[i].getType() != forOp.getResultTypes()[i]) {\n         shouldRematerialize = true;\n         break;\n       }\n@@ -1186,15 +1193,207 @@ class FixupLoop : public mlir::RewritePattern {\n     BlockAndValueMapping mapping;\n     for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n       mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n+    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n \n     for (Operation &op : forOp.getBody()->getOperations()) {\n-      Operation *newOp = rewriter.clone(op, mapping);\n+      rewriter.clone(op, mapping);\n     }\n     rewriter.replaceOp(forOp, newForOp.getResults());\n     return success();\n   }\n };\n \n+// This pattern collects the wrong Mma those need to update and create the right\n+// ones for each.\n+class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n+  DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n+\n+public:\n+  CollectMmaToUpdateForVolta(\n+      mlir::MLIRContext *ctx,\n+      DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n+      : mlir::RewritePattern(triton::DotOp::getOperationName(), 1, ctx),\n+        mmaToUpdate(mmaToUpdate) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+\n+    auto dotOp = cast<triton::DotOp>(op);\n+    auto *ctx = dotOp->getContext();\n+    auto AT = dotOp.a().getType().cast<RankedTensorType>();\n+    auto BT = dotOp.b().getType().cast<RankedTensorType>();\n+    auto DT = dotOp.d().getType().cast<RankedTensorType>();\n+    if (!DT.getEncoding())\n+      return failure();\n+    auto mmaLayout = DT.getEncoding().dyn_cast<MmaEncodingAttr>();\n+    if (!(mmaLayout && mmaLayout.isVolta()))\n+      return failure();\n+\n+    // Has processed.\n+    if (mmaToUpdate.count(mmaLayout))\n+      return failure();\n+\n+    auto dotOperandA = AT.getEncoding().cast<DotOperandEncodingAttr>();\n+    auto dotOperandB = BT.getEncoding().cast<DotOperandEncodingAttr>();\n+    bool isARow = dotOperandA.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    bool isBRow = dotOperandB.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    auto [isARow_, isBRow_, isAVec4, isBVec4] =\n+        mmaLayout.decodeVoltaLayoutStates();\n+    if (isARow_ == isARow && isBRow_ == isBRow) {\n+      return failure(); // No need to update\n+    }\n+\n+    auto newMmaLayout = MmaEncodingAttr::get(\n+        ctx, mmaLayout.getVersionMajor(), mmaLayout.getWarpsPerCTA(),\n+        AT.getShape(), BT.getShape(), isARow, isBRow);\n+\n+    // Collect the wrong MMA Layouts, and mark need to update.\n+    mmaToUpdate.try_emplace(mmaLayout, newMmaLayout);\n+\n+    return failure();\n+  }\n+};\n+\n+// Correct the versionMinor field in MmaEncodingAttr for Volta.\n+class UpdateMMAVersionMinorForVolta : public mlir::RewritePattern {\n+  const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n+  enum class Kind {\n+    kUnk,\n+    kCvtToMma,\n+    kCvtToDotOp,\n+    kDot,\n+    kConstant,\n+  };\n+  mutable Kind rewriteKind{Kind::kUnk};\n+\n+public:\n+  UpdateMMAVersionMinorForVolta(\n+      mlir::MLIRContext *ctx, llvm::StringRef opName,\n+      const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n+      : RewritePattern(opName, 1 /*benefit*/, ctx), mmaToUpdate(mmaToUpdate) {}\n+\n+  LogicalResult match(Operation *op) const override {\n+    MmaEncodingAttr mma;\n+    if (mmaToUpdate.empty())\n+      return failure();\n+    if (op->getNumResults() != 1)\n+      return failure();\n+    auto tensorTy = op->getResult(0).getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return failure();\n+\n+    // ConvertLayoutOp\n+    if (auto cvt = llvm::dyn_cast<ConvertLayoutOp>(op)) {\n+      // cvt X -> dot_operand\n+      if (auto dotOperand =\n+              tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>()) {\n+        mma = dotOperand.getParent().dyn_cast<MmaEncodingAttr>();\n+        rewriteKind = Kind::kCvtToDotOp;\n+        if (mma && mmaToUpdate.count(mma))\n+          return success();\n+      }\n+      if ((mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>())) {\n+        // cvt X -> mma\n+        rewriteKind = Kind::kCvtToMma;\n+        if (mma && mmaToUpdate.count(mma))\n+          return success();\n+      }\n+    } else if (auto dot = llvm::dyn_cast<DotOp>(op)) {\n+      // DotOp\n+      mma = dot.d()\n+                .getType()\n+                .cast<RankedTensorType>()\n+                .getEncoding()\n+                .dyn_cast<MmaEncodingAttr>();\n+      rewriteKind = Kind::kDot;\n+    } else if (auto constant = llvm::dyn_cast<arith::ConstantOp>(op)) {\n+      // ConstantOp\n+      mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n+      rewriteKind = Kind::kConstant;\n+    }\n+\n+    return success(mma && mmaToUpdate.count(mma));\n+  }\n+\n+  void rewrite(Operation *op, PatternRewriter &rewriter) const override {\n+    switch (rewriteKind) {\n+    case Kind::kDot:\n+      rewriteDot(op, rewriter);\n+      break;\n+    case Kind::kConstant:\n+      rewriteConstant(op, rewriter);\n+      break;\n+    case Kind::kCvtToDotOp:\n+      rewriteCvtDotOp(op, rewriter);\n+      break;\n+    case Kind::kCvtToMma:\n+      rewriteCvtToMma(op, rewriter);\n+      break;\n+    default:\n+      llvm::report_fatal_error(\"Not supported rewrite kind\");\n+    }\n+  }\n+\n+private:\n+  void rewriteCvtDotOp(Operation *op, PatternRewriter &rewriter) const {\n+    auto *ctx = op->getContext();\n+    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n+    auto tensorTy = cvt.result().getType().cast<RankedTensorType>();\n+    auto dotOperand = tensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+    MmaEncodingAttr newMma =\n+        mmaToUpdate.lookup(dotOperand.getParent().cast<MmaEncodingAttr>());\n+    auto newDotOperand = DotOperandEncodingAttr::get(\n+        ctx, dotOperand.getOpIdx(), newMma, dotOperand.getIsMMAv1Row());\n+    auto newTensorTy = RankedTensorType::get(\n+        tensorTy.getShape(), tensorTy.getElementType(), newDotOperand);\n+    rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n+                                                 cvt.getOperand());\n+  }\n+\n+  void rewriteDot(Operation *op, PatternRewriter &rewriter) const {\n+    auto *ctx = op->getContext();\n+    auto dot = llvm::cast<DotOp>(op);\n+    auto tensorTy = dot.d().getType().cast<RankedTensorType>();\n+    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto newMma = mmaToUpdate.lookup(mma);\n+    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n+                                             tensorTy.getElementType(), newMma);\n+    rewriter.replaceOpWithNewOp<DotOp>(op, newTensorTy, dot.a(), dot.b(),\n+                                       dot.c(), dot.allowTF32());\n+  }\n+\n+  void rewriteCvtToMma(Operation *op, PatternRewriter &rewriter) const {\n+    auto *ctx = op->getContext();\n+    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n+    auto tensorTy = cvt.result().getType().cast<RankedTensorType>();\n+    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto newMma = mmaToUpdate.lookup(mma);\n+    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n+                                             tensorTy.getElementType(), newMma);\n+    rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n+                                                 cvt.getOperand());\n+  }\n+\n+  void rewriteConstant(Operation *op, PatternRewriter &rewriter) const {\n+    auto *ctx = op->getContext();\n+    auto constant = llvm::cast<arith::ConstantOp>(op);\n+    auto tensorTy = constant.getResult().getType().dyn_cast<RankedTensorType>();\n+    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto newMma = mmaToUpdate.lookup(mma);\n+    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n+                                             tensorTy.getElementType(), newMma);\n+    if (auto attr = constant.getValue().dyn_cast<SplatElementsAttr>()) {\n+      auto newRet =\n+          SplatElementsAttr::get(newTensorTy, attr.getSplatValue<Attribute>());\n+      rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newTensorTy, newRet);\n+      return;\n+    }\n+\n+    assert(false && \"Not supported ConstantOp value type\");\n+  }\n+};\n+\n } // namespace\n \n #define GEN_PASS_CLASSES\n@@ -1229,6 +1428,28 @@ class TritonGPUCombineOpsPass\n       signalPassFailure();\n     }\n \n+    llvm::DenseMap<MmaEncodingAttr, MmaEncodingAttr> mmaToUpdate;\n+    {\n+      mlir::RewritePatternSet patterns(context);\n+      patterns.add<CollectMmaToUpdateForVolta>(context, mmaToUpdate);\n+      if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n+        signalPassFailure();\n+    }\n+    {\n+      mlir::RewritePatternSet patterns(context);\n+      patterns.add<UpdateMMAVersionMinorForVolta>(\n+          context, DotOp::getOperationName(), mmaToUpdate);\n+      patterns.add<UpdateMMAVersionMinorForVolta>(\n+          context, ConvertLayoutOp::getOperationName(), mmaToUpdate);\n+      patterns.add<UpdateMMAVersionMinorForVolta>(\n+          context, arith::ConstantOp::getOperationName(), mmaToUpdate);\n+      mlir::GreedyRewriteConfig config;\n+      config.useTopDownTraversal = true;\n+\n+      if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n+        signalPassFailure();\n+    }\n+\n     mlir::RewritePatternSet loopFixup(context);\n     loopFixup.add<FixupLoop>(context);\n     if (applyPatternsAndFoldGreedily(m, std::move(loopFixup)).failed()) {"}, {"filename": "python/setup.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -141,10 +141,10 @@ def build_extension(self, ext):\n             \"-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=\" + extdir,\n             \"-DTRITON_BUILD_TUTORIALS=OFF\",\n             \"-DTRITON_BUILD_PYTHON_MODULE=ON\",\n-            # '-DPYTHON_EXECUTABLE=' + sys.executable,\n-            '-DCMAKE_VERBOSE_MAKEFILE:BOOL=ON',\n+            \"-DPython3_EXECUTABLE:FILEPATH=\" + sys.executable,\n+            \"-DCMAKE_VERBOSE_MAKEFILE:BOOL=ON\",\n             \"-DPYTHON_INCLUDE_DIRS=\" + python_include_dir,\n-            \"-DLLVM_EXTERNAL_LIT=\" + lit_dir\n+            \"-DLLVM_EXTERNAL_LIT=\" + lit_dir,\n         ] + thirdparty_cmake_args\n \n         # configuration"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 17, "deletions": 18, "changes": 35, "file_content_changes": "@@ -491,10 +491,9 @@ def make_ptr_str(name, shape):\n # TODO: handle `%4 = triton_gpu.convert_layout %3 : (tensor<32xi32, #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>``\n @pytest.mark.parametrize(\"expr, dtype_str\", [\n     (f'x[{s}]', d)\n-    for s in ['None, :', ':, None']\n-    # FIXME: 3d indexing doesn't work\n-    #'None, :, :',\n-    # ':, :, None']\n+    for s in ['None, :', ':, None',\n+              'None, :, :',\n+              ':, :, None']\n     for d in ['int32', 'uint32', 'uint16']\n ])\n def test_index1d(expr, dtype_str, device='cuda'):\n@@ -1228,20 +1227,20 @@ def kernel(X, stride_xm, stride_xk,\n     elif dtype == 'int8':\n         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n \n-# FIXME: Unsupported layout found in ConvertSplatLikeOp\n-# def test_dot_without_load():\n-#    @triton.jit\n-#    def kernel(out):\n-#        pid = tl.program_id(axis=0)\n-#        a = tl.zeros((32, 32), tl.float32)\n-#        b = tl.zeros((32, 32), tl.float32)\n-#        c = tl.zeros((32, 32), tl.float32)\n-#        c = tl.dot(a, b)\n-#        pout = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n-#        tl.store(pout, c)\n-#\n-#    out = torch.ones((32, 32), dtype=torch.float32, device=\"cuda\")\n-#    kernel[(1,)](out)\n+\n+def test_dot_without_load():\n+    @triton.jit\n+    def kernel(out):\n+        pid = tl.program_id(axis=0)\n+        a = tl.zeros((32, 32), tl.float32)\n+        b = tl.zeros((32, 32), tl.float32)\n+        c = tl.zeros((32, 32), tl.float32)\n+        c = tl.dot(a, b)\n+        pout = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n+        tl.store(pout, c)\n+\n+    out = torch.ones((32, 32), dtype=torch.float32, device=\"cuda\")\n+    kernel[(1,)](out)\n \n # ---------------\n # test arange"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -5,6 +5,7 @@\n     ir,\n     builtin,\n )\n+from . import libdevice\n from .core import (\n     abs,\n     arange,\n@@ -20,6 +21,8 @@\n     atomic_xor,\n     bfloat16,\n     block_type,\n+    broadcast,\n+    broadcast_to,\n     cat,\n     cdiv,\n     constexpr,\n@@ -105,6 +108,8 @@\n     \"atomic_xor\",\n     \"bfloat16\",\n     \"block_type\",\n+    \"broadcast\",\n+    \"broadcast_to\",\n     \"builtin\",\n     \"cat\",\n     \"cdiv\",\n@@ -126,6 +131,7 @@\n     \"int64\",\n     \"int8\",\n     \"ir\",\n+    \"libdevice\",\n     \"load\",\n     \"log\",\n     \"max\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -596,11 +596,9 @@ def __getitem__(self, slices, _builder=None):\n         if isinstance(slices, slice):\n             slices = [slices]\n         ret = self\n-        n_inserted = 0\n         for dim, sl in enumerate(slices):\n             if isinstance(sl, constexpr) and sl.value is None:\n-                ret = semantic.expand_dims(ret, dim + n_inserted, _builder)\n-                n_inserted += 1\n+                ret = semantic.expand_dims(ret, dim, _builder)\n             elif sl == slice(None, None, None):\n                 pass\n             else:"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -52,6 +52,15 @@ func @convert(%A : !tt.ptr<f16>) {\n   return\n }\n \n+// CHECK-LABEL: trans\n+func @trans(%A : !tt.ptr<f16>) {\n+  // CHECK: %cst -> %cst\n+  %tensor = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n+  // CHECK: %0 -> %cst\n+  %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  return\n+}\n+\n // CHECK-LABEL: insert_slice_async\n func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -174,6 +174,14 @@ func @scratch() {\n   // CHECK-NEXT: size = 512\n }\n \n+// CHECK-LABEL: trans\n+func @trans(%A : !tt.ptr<f16>) {\n+  // CHECK: offset = 0, size = 1024\n+  %tensor = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n+  %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  return\n+}\n+\n // CHECK-LABEL: insert_slice_async\n func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n@@ -285,6 +293,25 @@ func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %\n   // CHECK-NEXT: size = 24576\n }\n \n+// c0 cannot be released in the loop\n+// CHECK-LABEL: for_use_ancestor\n+func @for_use_ancestor(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+  // CHECK: offset = 0, size = 8192\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK-NEXT: offset = 8192, size = 8192\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK-NEXT: offset = 16384, size = 8192\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    %c0 = tt.trans %c_shared_init : (tensor<128x32xf16, #A_SHARED>) -> tensor<32x128xf16, #A_SHARED>\n+    // CHECK-NEXT: offset = 24576, size = 8192\n+    %c1 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+    scf.yield %b_shared, %a_shared: tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+  }\n+  return\n+  // CHECK-NEXT: size = 32768\n+}\n+\n // a_shared_init, b_shared_init, and c_shared_init's liveness ranges are span over the entire function before cst2.\n // So they cannot be reused by cst0 and cst1, but can be reused by cst2.\n // CHECK-LABEL: for_if_for"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -111,6 +111,13 @@ func @extract_slice() {\n   return\n }\n \n+// CHECK-LABEL: trans\n+func @trans() {\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n+  %b = tt.trans %cst0 : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  return\n+}\n+\n // CHECK-LABEL: insert_slice_async\n func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 54, "deletions": 13, "changes": 67, "file_content_changes": "@@ -997,20 +997,61 @@ func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n // -----\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n+    // CHECK: nvvm.read.ptx.sreg.nctaid.x\n+    // CHECK: nvvm.read.ptx.sreg.nctaid.y\n+    // CHECK: nvvm.read.ptx.sreg.nctaid.z\n+    %blockdimx = tt.get_num_programs {axis=0:i32} : i32\n+    %blockdimy = tt.get_num_programs {axis=1:i32} : i32\n+    %blockdimz = tt.get_num_programs {axis=2:i32} : i32\n+    %v0 = arith.addi %blockdimx, %blockdimy : i32\n+    %v1 = arith.addi %v0, %blockdimz : i32\n+    %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n+    tt.store %a, %0 : tensor<32xi32, #blocked0>\n+  \n+    return\n+  }\n+}\n \n-func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n-  // CHECK: nvvm.read.ptx.sreg.nctaid.x\n-  // CHECK: nvvm.read.ptx.sreg.nctaid.y\n-  // CHECK: nvvm.read.ptx.sreg.nctaid.z\n-  %blockdimx = tt.get_num_programs {axis=0:i32} : i32\n-  %blockdimy = tt.get_num_programs {axis=1:i32} : i32\n-  %blockdimz = tt.get_num_programs {axis=2:i32} : i32\n-  %v0 = arith.addi %blockdimx, %blockdimy : i32\n-  %v1 = arith.addi %v0, %blockdimz : i32\n-  %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n-  tt.store %a, %0 : tensor<32xi32, #blocked0>\n-\n-  return\n+// -----\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: test_index_cache \n+  func @test_index_cache() {\n+    // CHECK: nvvm.read.ptx.sreg.tid.x\n+    %0 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n+    // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n+    %1 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n+    return\n+  }\n }\n \n+// -----\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: test_base_index_cache \n+  func @test_base_index_cache(%arg0: tensor<128x32xf32, #blocked0>) {\n+    // CHECK: nvvm.read.ptx.sreg.tid.x\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n+    // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n+    %1 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n+    return\n+  }\n }\n+\n+// -----\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: test_index_cache_different_block\n+  func @test_index_cache_different_block(%arg0: tensor<128x32xf32, #blocked0>, %arg1: i1) {\n+    // CHECK: nvvm.read.ptx.sreg.tid.x\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n+    scf.if %arg1 {\n+      // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n+      %1 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n+    }\n+    return\n+  }\n+}\n\\ No newline at end of file"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 32, "deletions": 4, "changes": 36, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -tritongpu-combine 2>&1 | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -tritongpu-combine 2>&1 | FileCheck %s\n \n #layout0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n #layout1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n@@ -7,7 +7,6 @@\n // CHECK: [[row_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n // CHECK: [[col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK: [[col_layout_novec:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n-\n func @cst() -> tensor<1024xi32, #layout1> {\n   %cst = arith.constant dense<0> : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %cst : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n@@ -62,9 +61,9 @@ func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n // CHECK-LABEL: transpose\n func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, [[row_layout]]>\n+  // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, {{%cst.*}}, {{%cst.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, [[row_layout]]>\n   // CHECK: [[cvt_val:%.*]] = triton_gpu.convert_layout [[loaded_val]] : (tensor<64x64xf32, [[row_layout]]>) -> tensor<64x64xf32, [[col_layout]]>\n-  // CHECK: tt.store {{.*}}, [[cvt_val]], %cst_1 : tensor<64x64xf32, [[col_layout]]>\n+  // CHECK: tt.store {{.*}}, [[cvt_val]], {{%cst.*}} : tensor<64x64xf32, [[col_layout]]>\n   // CHECK: return\n   %cst = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked1>\n   %cst_0 = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n@@ -184,3 +183,32 @@ func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f3\n   tt.store %21, %22 : tensor<256xf32, #layout1>\n   return\n }\n+\n+\n+// -----\n+\n+// check the UpdateMMAVersionMinorForVolta pattern\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n+#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[1,1]}>\n+// Here, the isMMAv1Row of a and b's dot_operands mismatch #mma0's versionMinor,\n+// and the pattern should update the versionMinor.\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n+// It creates a new MMA layout to fit with $a and $b's dot_operand\n+// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 11, warpsPerCTA = [1, 1]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: dot_mmav1\n+  func @dot_mmav1(%A: tensor<16x16xf16, #blocked0>, %B: tensor<16x16xf16, #blocked0>) -> tensor<16x16xf32, #blocked0> {\n+    %C = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked0>\n+    %AA = triton_gpu.convert_layout %A : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #dot_operand_a>\n+    %BB = triton_gpu.convert_layout %B : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #dot_operand_b>\n+    %CC = triton_gpu.convert_layout %C : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #mma0>\n+\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, %cst {allowTF32 = true} : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<16x16xf32, [[new_mma]]>\n+    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<16x16xf16, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n+    %res = triton_gpu.convert_layout %D : (tensor<16x16xf32, #mma0>) -> tensor<16x16xf32, #blocked0>\n+\n+    return %res : tensor<16x16xf32, #blocked0>\n+  }\n+}"}]