[{"filename": "python/src/triton.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -487,7 +487,7 @@ void init_triton_codegen(py::module &&m) {\n   // --------------------------------------- \n   // Load provided assembly code into driver\n   // --------------------------------------- \n-  m.def(\"load_binary\", [](backend_t backend, const std::string& name, const std::string& data, size_t n_shared_bytes, uint64_t device){\n+  m.def(\"load_binary\", [](const std::string& name, const std::string& data, size_t n_shared_bytes, uint64_t device){\n \t      py::gil_scoped_release allow_threads;\n         // create driver handles\n         CUfunction fun;"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 92, "deletions": 59, "changes": 151, "file_content_changes": "@@ -15,6 +15,7 @@\n from sysconfig import get_paths\n from typing import Any, Dict, Set, Tuple, Union\n \n+import json\n import setuptools\n import torch\n from filelock import FileLock\n@@ -926,7 +927,7 @@ def binary_name_to_header_name(name):\n     return f\"{name}.h\"\n \n \n-def generate_torch_glue(kernel_name, constants, signature):\n+def generate_launcher(identifier, constants, signature):\n     arg_decls = ', '.join(f\"{ty_to_cpp(ty)} arg{i}\" for i, ty in signature.items())\n \n     def _extracted_type(ty):\n@@ -957,7 +958,7 @@ def format_of(ty):\n     format = \"iiiiiKK\" + ''.join([format_of(_extracted_type(ty)) for ty in signature.values()])\n \n     # generate glue code\n-    src += f\"\"\"\n+    src = f\"\"\"\n #include \\\"cuda.h\\\"\n #include <Python.h>\n \n@@ -985,7 +986,7 @@ def format_of(ty):\n }}\n \n \n-inline CUdeviceptr getPointer(PyObject *obj, int idx) {{\n+static inline CUdeviceptr getPointer(PyObject *obj, int idx) {{\n   if (PyLong_Check(obj)) {{\n     return (CUdeviceptr)PyLong_AsUnsignedLongLong(obj);\n   }}\n@@ -1012,12 +1013,14 @@ def format_of(ty):\n   int gridX, gridY, gridZ;\n   uint64_t _stream;\n   uint64_t _function;\n+  int num_warps;\n+  int shared_memory;\n   {' '.join([f\"{_extracted_type(ty)} _arg{i}; \" for i, ty in signature.items()])}\n-  if(!PyArg_ParseTuple(args, \\\"{format}\\\", &gridX, &gridY, &gridZ, &_stream, &_function, {', '.join(f\"&_arg{i}\" for i, ty in signature.items())})) {{\n+  if(!PyArg_ParseTuple(args, \\\"{format}\\\", &gridX, &gridY, &gridZ, &num_warps, &shared_memory, &_stream, &_function, {', '.join(f\"&_arg{i}\" for i, ty in signature.items())})) {{\n     return NULL;\n   }}\n \n-  _launch(gridX, gridY, gridZ, (CUstream)stream, (CUfunction)function, {', '.join(f\"getPointer(_arg{i},{i})\" if ty[0]==\"*\" else f\"_arg{i}\"for i, ty in signature.items())});\n+  _launch(gridX, gridY, gridZ, num_warps, shared_memory, (CUstream)_stream, (CUfunction)_function, {', '.join(f\"getPointer(_arg{i},{i})\" if ty[0]==\"*\" else f\"_arg{i}\"for i, ty in signature.items())});\n \n \n   if(PyErr_Occurred()) {{\n@@ -1029,26 +1032,26 @@ def format_of(ty):\n }}\n \n static PyMethodDef ModuleMethods[] = {{\n-  {{\"{kernel_name}\", {kernel_name}, METH_VARARGS, \"Call {kernel_name} kernel\"}},\n+  {{\"launch\", launch, METH_VARARGS, \"Entry point for all kernels with this signature\"}},\n   {{NULL, NULL, 0, NULL}} // sentinel\n }};\n \n static struct PyModuleDef ModuleDef = {{\n   PyModuleDef_HEAD_INIT,\n-  \\\"{kernel_name}\\\",\n+  \\\"launcher\\\",\n   NULL, //documentation\n   -1, //size\n   ModuleMethods\n }};\n \n-PyMODINIT_FUNC PyInit_{kernel_name}(void) {{\n+PyMODINIT_FUNC PyInit_launcher(void) {{\n   PyObject *m = PyModule_Create(&ModuleDef);\n   if(m == NULL) {{\n     return NULL;\n   }}\n   PyModule_AddFunctions(m, ModuleMethods);\n   return m;\n-}\n+}}\n \"\"\"\n \n     return src\n@@ -1062,35 +1065,35 @@ class CacheManager:\n \n     def __init__(self, key):\n         self.key = key\n-        self.bin_path = None\n         self.lock_path = None\n-        # if caching is enabled, get the lock and bin path\n+        # create cache directory if it doesn't exist\n         self.cache_dir = os.environ.get('TRITON_CACHE_DIR', default_cache_dir())\n         if self.cache_dir:\n+            self.cache_dir = os.path.join(self.cache_dir, self.key)\n+            self.lock_path = os.path.join(self.cache_dir, \"lock\")\n             os.makedirs(self.cache_dir, exist_ok=True)\n-        if self.cache_dir:\n-            self.bin_path = os.path.join(self.cache_dir, self.key + \".so\")\n-            self.lock_path = self.bin_path + \".lock\"\n \n-    def has_file(self):\n-        return self.bin_path and os.path.exists(self.bin_path)\n+    def _make_path(self, filename):\n+        return os.path.join(self.cache_dir, filename)\n \n-    def put(self, binary):\n-        if self.bin_path:\n-            assert self.lock_path is not None\n-            with FileLock(self.lock_path):\n-                with open(self.bin_path + \".tmp\", \"wb\") as f:\n-                    f.write(binary)\n-                os.rename(self.bin_path + \".tmp\", self.bin_path)\n+    def has_file(self, filename):\n+        if not self.cache_dir:\n+            return False\n+        return os.path.exists(self._make_path(filename))\n+\n+    def put(self, data, filename, binary=True):\n+        if not self.cache_dir:\n+            return\n+        assert self.lock_path is not None\n+        filepath = self._make_path(filename)\n+        with FileLock(self.lock_path):\n+            # use tempfile to be robust against program interruptions\n+            mode = \"wb\" if binary else \"w\"\n+            with open(filepath + \".tmp\", mode) as f:\n+                f.write(data)\n+            os.rename(filepath + \".tmp\", filepath)\n \n \n-def make_cache_key(fn, signature, configs, constants, num_warps, num_stages):\n-    # Get unique key for the compiled code\n-    get_conf_key = lambda conf: (sorted(conf.divisible_by_16), sorted(conf.equal_to_1))\n-    configs_key = [get_conf_key(conf) for conf in configs]\n-    key = f\"{fn.cache_key}-{''.join(signature.values())}-{configs_key}-{constants}-{num_warps}-{num_stages}\"\n-    key = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n-    return key\n \n # utilties for generating and compiling C wrappers\n \n@@ -1159,51 +1162,81 @@ def _build(name, src, srcdir):\n         setuptools.setup(**args)\n     return so\n \n+def make_so_cache_key(signature, constants):\n+    # Get unique key for the compiled code\n+    signature = {k: 'ptr' if v[0] == '*' else v for k, v in signature.items()}\n+    key = f\"{''.join(signature.values())}{constants}\"\n+    key = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n+    return key\n+\n+def make_fn_cache_key(fn_hash, signature, configs, constants, num_warps, num_stages):\n+    # Get unique key for the compiled code\n+    get_conf_key = lambda conf: (sorted(conf.divisible_by_16), sorted(conf.equal_to_1))\n+    configs_key = [get_conf_key(conf) for conf in configs]\n+    key = f\"{fn_hash}-{''.join(signature.values())}-{configs_key}-{constants}-{num_warps}-{num_stages}\"\n+    key = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n+    return key\n \n def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, configs=None):\n     # we get the kernel, i.e. the first function generated in the module\n-    if configs is None:\n-        assert False, \"automatic specialization is not supported yet\"\n-        ref, _ = make_triton_ir(fn, signature, _triton.code_gen.instance_descriptor(), constants)\n-        fns = ref.get_functions()\n-        configs = _triton.infer_specialization_configs(fns[0])\n     assert len(configs) == 1\n     # cache manager\n-    cache_key = make_cache_key(fn, signature, configs, constants, num_warps, num_stages)\n-    cache_manager = CacheManager(cache_key)\n+    name = fn.__name__\n+    # name of files that are cached\n+    so_cache_key = make_so_cache_key(signature, constants)\n+    so_cache_manager = CacheManager(so_cache_key)\n+    so_name = f\"{name}.so\"\n+    # retrieve stub from cache if it exists\n+    if not so_cache_manager.has_file(so_name):\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+          src = generate_launcher(name, constants, signature)\n+          src_path = os.path.join(tmpdir, \"main.c\")\n+          with open(src_path, \"w\") as f:\n+              f.write(src)\n+          so = _build(fn.__name__, src_path, tmpdir)\n+          with open(so, \"rb\") as f:\n+              so_cache_manager.put(f.read(), so_name, binary=True)\n+\n     # retrieve cached shared object if it exists\n-    if cache_manager.has_file():\n-        return CompiledKernel(fn.__name__, cache_manager.bin_path)\n-    # compile all the configs\n-    binary = _compile(fn, signature, device, constants, configs[0], num_warps, num_stages, extern_libs, \"cubin\")\n-    # generate and compile glue code into shared object\n-    with tempfile.TemporaryDirectory() as tmpdir:\n-        src = generate_torch_glue(fn.__name__, constants, signature, num_warps, binaries, tmpdir)\n-        src_path = os.path.join(tmpdir, \"main.c\")\n-        with open(src_path, \"w\") as f:\n-            f.write(src)\n-        so = _build(fn.__name__, src_path, tmpdir)\n-        with open(so, \"rb\") as f:\n-            cache_manager.put(f.read())\n-\n-    return CompiledKernel(fn.__name__, cache_manager.bin_path)\n+    fn_cache_key = make_fn_cache_key(fn.cache_key, signature, configs, constants, num_warps, num_stages)\n+    fn_cache_manager = CacheManager(fn_cache_key)\n+    cubin_name = f\"{name}.cubin\"\n+    data_name = f\"{name}.json\"\n+    if not fn_cache_manager.has_file(cubin_name) or not fn_cache_manager.has_file(data_name):\n+        asm, shared, kernel_name = _compile(fn, signature, device, constants, configs[0], num_warps, num_stages, extern_libs, \"cubin\")\n+        metadata = {\"name\": kernel_name, \"shared\": shared, \"num_warps\": num_warps, \"num_stages\": num_stages}\n+        fn_cache_manager.put(asm[\"cubin\"], cubin_name)\n+        fn_cache_manager.put(json.dumps(metadata), data_name, binary=False)\n+    \n+    return CompiledKernel(name, so_cache_manager._make_path(so_name), fn_cache_manager.cache_dir)\n \n \n class CompiledKernel:\n \n-    def __init__(self, fn_name, data_path):\n+    def __init__(self, fn_name, so_path, cache_dir):\n+        # initialize launcher\n         import importlib.util\n-        spec = importlib.util.spec_from_file_location(fn_name, data_path)\n+        spec = importlib.util.spec_from_file_location(\"launcher\", so_path)\n         mod = importlib.util.module_from_spec(spec)\n         spec.loader.exec_module(mod)\n-        self.c_wrapper = getattr(mod, fn_name)\n-        ptx = getattr(mod, \"ptx\")\n-        if len(ptx) == 1:\n-            self.asm = {\"ptx\": list(ptx.values())[0]}\n+        self.c_wrapper = getattr(mod, \"launch\")\n+        # initialize cuModule and data\n+        with open(os.path.join(cache_dir, f\"{fn_name}.json\")) as f:\n+            metadata = json.load(f)\n+        self.shared = metadata[\"shared\"]\n+        self.num_warps = metadata[\"num_warps\"]\n+        self.num_stages = metadata[\"num_stages\"]\n+        cu_path = os.path.join(cache_dir, f\"{fn_name}.cubin\")\n+        device = torch.cuda.current_device()\n+        with open(cu_path, \"rb\") as cubin:\n+          mod, func, n_regs, n_spills = _triton.code_gen.load_binary(metadata[\"name\"], cubin.read(), self.shared, device)\n+          self.cu_module = mod\n+          self.cu_function = func\n+\n \n     def __getitem__(self, grid):\n         def runner(*args, stream=None):\n             if stream is None:\n                 stream = torch.cuda.current_stream().cuda_stream\n-            self.c_wrapper(grid[0], grid[1], grid[2], stream, *args)\n+            self.c_wrapper(grid[0], grid[1], grid[2], self.num_warps, self.shared, stream, self.cu_function, *args)\n         return runner"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -253,7 +253,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     try:\n       bin = cache[key]\n       if not warmup:\n-          bin.c_wrapper(grid_0, grid_1, grid_2, stream, {args})\n+          bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, {args})\n       return bin\n     # kernel not cached -- compile\n     except KeyError:\n@@ -274,7 +274,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n       if not self._call_hook(key, signature, device, constants, num_warps, num_stages, extern_libs, configs):\n         bin = triton.compile(self, signature, device, constants, num_warps, num_stages, extern_libs=extern_libs, configs=configs)\n         if not warmup:\n-            bin.c_wrapper(grid_0, grid_1, grid_2, stream, *args)\n+            bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, *args)\n         self.cache[key] = bin\n         return bin\n       return None"}]