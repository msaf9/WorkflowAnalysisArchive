[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 2, "deletions": 15, "changes": 17, "file_content_changes": "@@ -79,24 +79,11 @@ jobs:\n           lit -v \"$LIT_TEST_DIR\"\n \n       - name: Run python tests\n-        if: ${{matrix.runner[0] == 'self-hosted' && matrix.runner[1] == 'A10'}}\n+        if: ${{matrix.runner[0] == 'self-hosted'}}\n         run: |\n-          cd python/tests\n+          cd python/test/unit/\n           pytest\n \n-      # TODO[Superjomn] Enable all the tests on V100 if available\n-      - name: Run python tests on V100\n-        if: ${{matrix.runner[0] == 'self-hosted' && matrix.runner[1] == 'V100'}}\n-        run: |\n-          cd python/tests\n-          pytest -k \"not test_where_broadcast and not test_dot\" test_core.py\n-          pytest test_gemm.py\n-          pytest test_backend.py\n-          pytest test_reduce.py\n-          pytest test_vecadd.py\n-          pytest test_elementwise.py\n-          pytest test_ext_elemwise.py\n-          pytest test_transpose.py\n \n       - name: Run CXX unittests\n         run: |"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 13, "deletions": 11, "changes": 24, "file_content_changes": "@@ -1088,16 +1088,16 @@ def kernel(X, stride_xm, stride_xn,\n                                                 [64, 128, 128, 4],\n                                                 [32, 128, 64, 2],\n                                                 [128, 128, 64, 2],\n-                                                [64, 128, 128, 2]]\\\n-                            # TODO: fma bug for some shapes and transposition modes?\n-                            for allow_tf32 in [False]\\\n+                                                [64, 128, 128, 4]]\\\n+                            for allow_tf32 in [True]\\\n                             for col_a in [True, False]\\\n                             for col_b in [True, False]\\\n-                            for dtype in ['int8']])\n+                            for dtype in ['int8', 'float16', 'float32']])\n                             \n def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype, device='cuda'):\n-    if allow_tf32 and dtype != 'float32':\n-        pytest.skip(\"Only test tf32 on fp32\")\n+    # TODO: fma bug for some shapes and transposition modes?\n+    # if dtype == 'float32' and not allow_tf32:\n+    #     pytest.skip(\"Seems to have bugs\")\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8:\n         if dtype == 'int8':\n@@ -1157,10 +1157,10 @@ def kernel(X, stride_xm, stride_xk,\n     else:\n         y = numpy_random((K, N), dtype_str=dtype, rs=rs)\n     w = numpy_random((N, N), dtype_str=dtype, rs=rs)\n-    # if epilogue != 'none' and 'int' not in dtype:\n-    #     x *= .1\n-    #     y *= .1\n-    if allow_tf32:\n+    if 'int' not in dtype:\n+        x *= .1\n+        y *= .1\n+    if dtype=='float32' and allow_tf32:\n         x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n         y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n         w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n@@ -1193,6 +1193,8 @@ def kernel(X, stride_xm, stride_xk,\n     if dtype == 'int8':\n         z_ref = np.matmul(x.astype(np.float32), \n                           y.astype(np.float32())).astype(np.int32)\n+    else:\n+        z_ref = np.matmul(x, y)\n \n     if epilogue == 'add-matrix':\n         z_ref += z\n@@ -1217,7 +1219,7 @@ def kernel(X, stride_xm, stride_xk,\n     ptx = pgm.asm['ptx']\n     assert 'ld.global.v4' in ptx\n     assert 'st.global.v4' in ptx\n-    if allow_tf32:\n+    if dtype=='float32' and allow_tf32:\n         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n     elif dtype == 'float32' and allow_tf32:\n         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' not in ptx"}, {"filename": "python/tests/__init__.py", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/tests/libdevice_testutil.py", "status": "removed", "additions": 0, "deletions": 18, "changes": 18, "file_content_changes": "@@ -1,18 +0,0 @@\n-import os\n-from typing import Optional\n-\n-_SYSTEM_LIBDEVICE_SEARCH_PATHS = [\n-    '/usr/lib/cuda/nvvm/libdevice/libdevice.10.bc',\n-    '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc',\n-]\n-\n-SYSTEM_LIBDEVICE_PATH: Optional[str] = None\n-for _p in _SYSTEM_LIBDEVICE_SEARCH_PATHS:\n-    if os.path.exists(_p):\n-        SYSTEM_LIBDEVICE_PATH = _p\n-\n-def system_libdevice_path() -> str:\n-    assert SYSTEM_LIBDEVICE_PATH is not None, \\\n-        \"Could not find libdevice.10.bc path\"\n-    return SYSTEM_LIBDEVICE_PATH\n-"}, {"filename": "python/tests/test_blocksparse.py", "status": "removed", "additions": 0, "deletions": 188, "changes": 188, "file_content_changes": "@@ -1,188 +0,0 @@\n-import pytest\n-import torch\n-\n-import triton\n-\n-# TODO: float32 fails\n-\n-@pytest.mark.parametrize(\"MODE\", [\"sdd\", \"dds\", \"dsd\"])\n-@pytest.mark.parametrize(\"TRANS_B\", [False, True])\n-@pytest.mark.parametrize(\"TRANS_A\", [False, True])\n-@pytest.mark.parametrize(\"BLOCK\", [16, 32, 64])\n-@pytest.mark.parametrize(\"DTYPE\", [torch.float16])\n-def test_matmul(MODE, TRANS_A, TRANS_B, BLOCK, DTYPE, Z=3, H=2, M=512, N=256, K=384):\n-    seed = 0\n-    torch.manual_seed(seed)\n-    is_sdd = MODE == \"sdd\"\n-    is_dsd = MODE == \"dsd\"\n-    is_dds = MODE == \"dds\"\n-    do_sparsify = lambda x: triton.testing.sparsify_tensor(x, layout, BLOCK)\n-    do_mask = lambda x: triton.testing.mask_tensor(x, layout, BLOCK)\n-    # create inputs\n-    # create op\n-    a_shape = (Z, H, K, M) if TRANS_A else (Z, H, M, K)\n-    b_shape = (Z, H, N, K) if TRANS_B else (Z, H, K, N)\n-    c_shape = (Z, H, M, N)\n-    shape = {\n-        \"sdd\": (M, N),\n-        \"dsd\": (a_shape[2], a_shape[3]),\n-        \"dds\": (b_shape[2], b_shape[3]),\n-    }[MODE]\n-    layout = torch.randint(2, (H, shape[0] // BLOCK, shape[1] // BLOCK))\n-    layout[1, 2, :] = 0\n-    layout[1, :, 1] = 0\n-    # create data\n-    a_ref, a_tri = triton.testing.make_pair(a_shape, alpha=.1, dtype=DTYPE)\n-    b_ref, b_tri = triton.testing.make_pair(b_shape, alpha=.1, dtype=DTYPE)\n-    dc_ref, dc_tri = triton.testing.make_pair(c_shape, dtype=DTYPE)\n-    # compute [torch]\n-    dc_ref = do_mask(dc_ref) if is_sdd else dc_ref\n-    a_ref = do_mask(a_ref) if is_dsd else a_ref\n-    b_ref = do_mask(b_ref) if is_dds else b_ref\n-    a_ref.requires_grad_().retain_grad()\n-    b_ref.requires_grad_().retain_grad()\n-    c_ref = torch.matmul(a_ref.transpose(2, 3) if TRANS_A else a_ref,\n-                         b_ref.transpose(2, 3) if TRANS_B else b_ref)\n-    c_ref.backward(dc_ref)\n-    c_ref = do_sparsify(c_ref) if is_sdd else c_ref\n-    da_ref = do_sparsify(a_ref.grad) if is_dsd else a_ref.grad\n-    db_ref = do_sparsify(b_ref.grad) if is_dds else b_ref.grad\n-    # triton result\n-    dc_tri = do_sparsify(dc_tri) if is_sdd else dc_tri\n-    a_tri = do_sparsify(a_tri) if is_dsd else a_tri\n-    b_tri = do_sparsify(b_tri) if is_dds else b_tri\n-    a_tri.requires_grad_().retain_grad()\n-    b_tri.requires_grad_().retain_grad()\n-    op = triton.ops.blocksparse.matmul(layout, BLOCK, MODE, trans_a=TRANS_A, trans_b=TRANS_B, device=\"cuda\")\n-    c_tri = triton.testing.catch_oor(lambda: op(a_tri, b_tri), pytest)\n-    triton.testing.catch_oor(lambda: c_tri.backward(dc_tri), pytest)\n-    da_tri = a_tri.grad\n-    db_tri = b_tri.grad\n-    # compare\n-    triton.testing.assert_almost_equal(c_ref, c_tri)\n-    triton.testing.assert_almost_equal(da_ref, da_tri)\n-    triton.testing.assert_almost_equal(db_ref, db_tri)\n-\n-\n-configs = [\n-    (16, 256),\n-    (32, 576),\n-    (64, 1871),\n-    (128, 2511),\n-]\n-\n-\n-@pytest.mark.parametrize(\"is_dense\", [False, True])\n-@pytest.mark.parametrize(\"BLOCK, WIDTH\", configs)\n-def test_softmax(BLOCK, WIDTH, is_dense, Z=2, H=2, is_causal=True, scale=0.4):\n-    # set seed\n-    torch.random.manual_seed(0)\n-    Z, H, M, N = 2, 3, WIDTH, WIDTH\n-    # initialize layout\n-    # make sure each row has at least one non-zero element\n-    layout = torch.randint(2, (H, M // BLOCK, N // BLOCK))\n-    if is_dense:\n-        layout[:] = 1\n-    else:\n-        layout[1, 2, :] = 0\n-        layout[1, :, 1] = 0\n-    # initialize data\n-    a_shape = (Z, H, M, N)\n-    a_ref, a_tri = triton.testing.make_pair(a_shape)\n-    dout_ref, dout_tri = triton.testing.make_pair(a_shape)\n-    # compute [torch]\n-    a_ref = triton.testing.mask_tensor(a_ref, layout, BLOCK, value=float(\"-inf\"))\n-    a_ref.retain_grad()\n-    at_mask = torch.ones((M, N), device=\"cuda\")\n-    if is_causal:\n-        at_mask = torch.tril(at_mask)\n-    M = at_mask[None, None, :, :] + torch.zeros_like(a_ref)\n-    a_ref[M == 0] = float(\"-inf\")\n-    out_ref = torch.softmax(a_ref * scale, -1)\n-    out_ref.backward(dout_ref)\n-    out_ref = triton.testing.sparsify_tensor(out_ref, layout, BLOCK)\n-    da_ref = triton.testing.sparsify_tensor(a_ref.grad, layout, BLOCK)\n-    # compute [triton]\n-    a_tri = triton.testing.sparsify_tensor(a_tri, layout, BLOCK)\n-    a_tri.retain_grad()\n-    dout_tri = triton.testing.sparsify_tensor(dout_tri, layout, BLOCK)\n-    op = triton.ops.blocksparse.softmax(layout, BLOCK, device=\"cuda\", is_dense=is_dense)\n-    out_tri = op(a_tri, scale=scale, is_causal=is_causal)\n-    out_tri.backward(dout_tri)\n-    da_tri = a_tri.grad\n-    # compare\n-    triton.testing.assert_almost_equal(out_tri, out_ref)\n-    triton.testing.assert_almost_equal(da_tri, da_ref)\n-\n-\n-@pytest.mark.parametrize(\"block\", [16, 32, 64])\n-@pytest.mark.parametrize(\"dtype\", [torch.float16])\n-def test_attention_fwd_bwd(\n-    block,\n-    dtype,\n-    input_scale=1.0,\n-    scale=1 / 8.0,\n-    n_ctx=256,\n-    batch_size=2,\n-    n_heads=2,\n-):\n-    # inputs\n-    qkv_shape = (batch_size, n_heads, n_ctx, 64)\n-    qkvs = [\n-        torch.nn.Parameter(input_scale * torch.randn(qkv_shape), requires_grad=True).to(dtype).cuda() for _ in range(3)\n-    ]\n-\n-    # Triton:\n-    n_blocks = n_ctx // block\n-    layout = torch.tril(torch.ones([n_heads, n_blocks, n_blocks], dtype=torch.long))\n-    query, key, value = [x.clone() for x in qkvs]\n-    query.retain_grad()\n-    key.retain_grad()\n-    value.retain_grad()\n-    attn_out = triton_attention(layout, block, query=query, key=key, value=value, scale=scale)\n-    # ad hoc loss\n-    loss = (attn_out ** 2).mean()\n-    loss.backward()\n-    grads = [query.grad, key.grad, value.grad]\n-\n-    # Torch version:\n-    torch_q, torch_k, torch_v = [x.clone() for x in qkvs]\n-    attn_mask = torch.ones([n_ctx, n_ctx], device=\"cuda\", dtype=dtype)\n-    attn_mask = torch.tril(attn_mask, diagonal=0)\n-    attn_mask = 1e6 * (-1 + (attn_mask.reshape((1, 1, n_ctx, n_ctx)).cuda()))\n-    torch_q.retain_grad()\n-    torch_k.retain_grad()\n-    torch_v.retain_grad()\n-    scores = scale * torch.einsum(\"bhsd,bhtd->bhst\", torch_q, torch_k)\n-    scores = scores + attn_mask\n-    probs = torch.softmax(scores, dim=-1)\n-    torch_attn_out = torch.einsum(\"bhst,bhtd->bhsd\", probs, torch_v)\n-    # ad hoc loss\n-    torch_loss = (torch_attn_out ** 2).mean()\n-    torch_loss.backward()\n-    torch_grads = [torch_q.grad, torch_k.grad, torch_v.grad]\n-\n-    # comparison\n-    # print(f\"Triton loss {loss} and torch loss {torch_loss}.  Also checking grads...\")\n-    triton.testing.assert_almost_equal(loss, torch_loss)\n-    for g1, g2 in zip(grads, torch_grads):\n-        triton.testing.assert_almost_equal(g1, g2)\n-\n-\n-@pytest.mark.parametrize(\"block\", [16, 32, 64])\n-def triton_attention(\n-    layout,\n-    block: int,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    scale: float,\n-):\n-    sparse_dot_sdd_nt = triton.ops.blocksparse.matmul(layout, block, \"sdd\", trans_a=False, trans_b=True, device=value.device)\n-    sparse_dot_dsd_nn = triton.ops.blocksparse.matmul(layout, block, \"dsd\", trans_a=False, trans_b=False, device=value.device)\n-    sparse_softmax = triton.ops.blocksparse.softmax(layout, block, device=value.device)\n-\n-    w = sparse_dot_sdd_nt(query, key)\n-    w = sparse_softmax(w, scale=scale, is_causal=True)\n-    a = sparse_dot_dsd_nn(w, value)\n-    return a"}, {"filename": "python/tests/test_compiler.py", "status": "removed", "additions": 0, "deletions": 32, "changes": 32, "file_content_changes": "@@ -1,32 +0,0 @@\n-import torch\n-\n-import triton\n-import triton.language as tl\n-\n-# trigger the torch.device implicitly to ensure cuda context initialization\n-torch.zeros([10], device=torch.device('cuda'))\n-\n-\n-@triton.jit\n-def empty_kernel(X, stride_xm, BLOCK: tl.constexpr):\n-    pass\n-\n-\n-def test_empty_kernel_cubin_compile():\n-\n-    device = torch.cuda.current_device()\n-    kernel = triton.compile(empty_kernel,\n-                            signature=\"*fp32,i32,i32\",\n-                            device=device,\n-                            constants={\"BLOCK\": 256})\n-\n-    assert len(kernel.asm[\"cubin\"]) > 0\n-\n-\n-def test_empty_kernel_launch():\n-    grid = lambda META: (\n-        triton.cdiv(1024, META['BLOCK']) * triton.cdiv(1024, META['BLOCK']),\n-    )\n-\n-    A = torch.zeros([1024], device=\"cuda\")\n-    empty_kernel[grid](X=A, stride_xm=256, BLOCK=256)"}, {"filename": "python/tests/test_elementwise.py", "status": "removed", "additions": 0, "deletions": 201, "changes": 201, "file_content_changes": "@@ -1,201 +0,0 @@\n-import tempfile\n-from inspect import Parameter, Signature\n-\n-import _testcapi\n-import pytest\n-import torch\n-from torch.testing import assert_close\n-from tests.libdevice_testutil import system_libdevice_path\n-\n-import triton\n-import triton.language as tl\n-\n-torch_type = {\n-    \"bool\": torch.bool,\n-    \"int32\": torch.int32,\n-    \"float32\": torch.float32,\n-    \"float64\": torch.float64\n-}\n-\n-torch_ops = {\n-    \"log\": \"log\",\n-    \"cos\": \"cos\",\n-    \"sin\": \"sin\",\n-    \"sqrt\": \"sqrt\",\n-    \"abs\": \"abs\",\n-    \"exp\": \"exp\",\n-    \"sigmoid\": \"sigmoid\",\n-    \"umulhi\": None,\n-    \"cdiv\": None,\n-    \"fdiv\": \"div\",\n-    \"minimum\": \"minimum\",\n-    \"maximum\": \"maximum\",\n-    \"where\": \"where\",\n-}\n-\n-\n-def get_tensor(shape, data_type, b_positive=False):\n-    x = None\n-    if data_type.startswith('int'):\n-        x = torch.randint(2**31 - 1, shape, dtype=torch_type[data_type], device='cuda')\n-    elif data_type.startswith('bool'):\n-        x = torch.randint(1, shape, dtype=torch_type[data_type], device='cuda')\n-    else:\n-        x = torch.randn(shape, dtype=torch_type[data_type], device='cuda')\n-\n-    if b_positive:\n-        x = torch.abs(x)\n-\n-    return x\n-\n-\n-@pytest.mark.parametrize('expr, output_type, input0_type',\n-                         [('log', 'float32', 'float32'),\n-                          ('log', 'float64', 'float64'),\n-                             ('cos', 'float32', 'float32'),\n-                             ('cos', 'float64', 'float64'),\n-                             ('sin', 'float32', 'float32'),\n-                             ('sin', 'float64', 'float64'),\n-                             ('sqrt', 'float32', 'float32'),\n-                             ('sqrt', 'float64', 'float64'),\n-                             ('abs', 'float32', 'float32'),\n-                             ('exp', 'float32', 'float32'),\n-                             ('exp', 'float64', 'float64'),\n-                             ('sigmoid', 'float32', 'float32'),\n-                          ])\n-def test_single_input(expr, output_type, input0_type):\n-    src = f\"\"\"\n-def kernel(X, Y, BLOCK: tl.constexpr):\n-    x = tl.load(X + tl.arange(0, BLOCK))\n-    y = tl.{expr}(x)\n-    tl.store(Y + tl.arange(0, BLOCK), y)\n-\"\"\"\n-    fp = tempfile.NamedTemporaryFile(mode='w', suffix=\".py\")\n-    fp.write(src)\n-    fp.flush()\n-\n-    def kernel(X, Y, BLOCK: tl.constexpr):\n-        pass\n-    kernel.__code__ = _testcapi.code_newempty(fp.name, \"kernel\", 1)\n-    parameters = []\n-    parameters.append(Parameter(\"X\", 1))\n-    parameters.append(Parameter(\"Y\", 1))\n-    parameters.append(Parameter(\"BLOCK\", 1))\n-    kernel.__signature__ = Signature(parameters=parameters)\n-    kernel = triton.jit(kernel)\n-\n-    shape = (128, )\n-    # limit the range of integers so that the sum does not overflow\n-    x = get_tensor(shape, input0_type, expr == 'log' or expr == 'sqrt')\n-    # triton result\n-    y = torch.zeros(shape, dtype=torch_type[output_type], device=\"cuda\")\n-    kernel[(1,)](\n-        x, y,\n-        BLOCK=shape[0],\n-        extern_libs={\"libdevice\": system_libdevice_path()},\n-    )\n-    # reference result\n-    y_ref = getattr(torch, torch_ops[expr])(x)\n-    # compare\n-    assert_close(y, y_ref)\n-\n-\n-@pytest.mark.parametrize('expr, output_type, input0_type, input1_type',\n-                         [('umulhi', 'int32', 'int32', 'int32'),\n-                          ('cdiv', 'int32', 'int32', 'int32'),\n-                             ('fdiv', 'float32', 'float32', 'float32'),\n-                             ('minimum', 'float32', 'float32', 'float32'),\n-                             ('maximum', 'float32', 'float32', 'float32'),\n-                          ])\n-def test_two_input(expr, output_type, input0_type, input1_type):\n-    src = f\"\"\"\n-def kernel(X0, X1, Y, BLOCK: tl.constexpr):\n-    x0 = tl.load(X0 + tl.arange(0, BLOCK))\n-    x1 = tl.load(X1 + tl.arange(0, BLOCK))\n-    y = tl.{expr}(x0, x1)\n-    tl.store(Y + tl.arange(0, BLOCK), y)\n-\"\"\"\n-    fp = tempfile.NamedTemporaryFile(mode='w', suffix=\".py\")\n-    fp.write(src)\n-    fp.flush()\n-\n-    def kernel(X0, X1, Y, BLOCK: tl.constexpr):\n-        pass\n-    kernel.__code__ = _testcapi.code_newempty(fp.name, \"kernel\", 1)\n-    parameters = []\n-    parameters.append(Parameter(\"X0\", 1))\n-    parameters.append(Parameter(\"X1\", 1))\n-    parameters.append(Parameter(\"Y\", 1))\n-    parameters.append(Parameter(\"BLOCK\", 1))\n-    kernel.__signature__ = Signature(parameters=parameters)\n-    kernel = triton.jit(kernel)\n-\n-    shape = (128, )\n-    # limit the range of integers so that the sum does not overflow\n-    x0 = get_tensor(shape, input0_type)\n-    x1 = get_tensor(shape, input1_type)\n-\n-    # triton result\n-    y = torch.zeros(shape, dtype=torch_type[output_type], device=\"cuda\")\n-    kernel[(1,)](\n-        x0, x1, y,\n-        BLOCK=shape[0],\n-        extern_libs={\"libdevice\": system_libdevice_path()},\n-    )\n-    # reference result\n-\n-    if expr == \"cdiv\":\n-        y_ref = torch.div(x0 + x1 - 1, x1, rounding_mode='trunc')\n-    elif expr == \"umulhi\":\n-        y_ref = ((x0.to(torch.int64) * x1) >> 32).to(torch.int32)\n-    else:\n-        y_ref = getattr(torch, torch_ops[expr])(x0, x1)\n-    # compare\n-    assert_close(y, y_ref)\n-\n-\n-@pytest.mark.parametrize('expr, output_type, input0_type, input1_type, input2_type',\n-                         [('where', \"int32\", \"bool\", \"int32\", \"int32\"), ])\n-def test_three_input(expr, output_type, input0_type, input1_type, input2_type):\n-    src = f\"\"\"\n-def kernel(X0, X1, X2, Y, BLOCK: tl.constexpr):\n-    x0 = tl.load(X0 + tl.arange(0, BLOCK))\n-    x1 = tl.load(X1 + tl.arange(0, BLOCK))\n-    x2 = tl.load(X2 + tl.arange(0, BLOCK))\n-    y = tl.{expr}(x0, x1, x2)\n-    tl.store(Y + tl.arange(0, BLOCK), y)\n-\"\"\"\n-    fp = tempfile.NamedTemporaryFile(mode='w', suffix=\".py\")\n-    fp.write(src)\n-    fp.flush()\n-\n-    def kernel(X0, X1, X2, Y, BLOCK: tl.constexpr):\n-        pass\n-    kernel.__code__ = _testcapi.code_newempty(fp.name, \"kernel\", 1)\n-    parameters = []\n-    parameters.append(Parameter(\"X0\", 1))\n-    parameters.append(Parameter(\"X1\", 1))\n-    parameters.append(Parameter(\"X2\", 1))\n-    parameters.append(Parameter(\"Y\", 1))\n-    parameters.append(Parameter(\"BLOCK\", 1))\n-    kernel.__signature__ = Signature(parameters=parameters)\n-    kernel = triton.jit(kernel)\n-\n-    shape = (128, )\n-    # limit the range of integers so that the sum does not overflow\n-    x0 = get_tensor(shape, input0_type)\n-    x1 = get_tensor(shape, input1_type)\n-    x2 = get_tensor(shape, input1_type)\n-\n-    # triton result\n-    y = torch.zeros(shape, dtype=torch_type[output_type], device=\"cuda\")\n-    kernel[(1,)](\n-        x0, x1, x2, y,\n-        BLOCK=shape[0],\n-        extern_libs={\"libdevice\": system_libdevice_path()},\n-    )\n-    # reference result\n-\n-    y_ref = getattr(torch, torch_ops[expr])(x0, x1, x2)\n-    # compare\n-    assert_close(y, y_ref)"}, {"filename": "python/tests/test_ext_elemwise.py", "status": "removed", "additions": 0, "deletions": 179, "changes": 179, "file_content_changes": "@@ -1,179 +0,0 @@\n-\n-import pytest\n-import torch\n-from torch.testing import assert_close\n-\n-import triton\n-import triton.language as tl\n-from tests.libdevice_testutil import system_libdevice_path\n-\n-\n-@pytest.mark.parametrize('num_warps, block_size, iter_size', [\n-    [4, 256, 1],\n-    [4, 1024, 256],\n-])\n-def test_sin_no_mask(num_warps, block_size, iter_size):\n-    @triton.jit\n-    def kernel(x_ptr,\n-               y_ptr,\n-               block_size,\n-               iter_size: tl.constexpr):\n-        pid = tl.program_id(axis=0)\n-        for i in range(0, block_size, iter_size):\n-            offset = pid * block_size + tl.arange(0, iter_size)\n-            x_ptrs = x_ptr + offset\n-            x = tl.load(x_ptrs)\n-            y = tl.libdevice.sin(x)\n-            y_ptrs = y_ptr + offset\n-            tl.store(y_ptrs, y)\n-\n-            x_ptr += iter_size\n-            y_ptr += iter_size\n-\n-    x = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n-    y = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n-\n-    grid = lambda EA: (x.shape.numel() // (block_size),)\n-    kernel[grid](x_ptr=x, y_ptr=y,\n-                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps)\n-\n-    golden_y = torch.sin(x)\n-    assert_close(y, golden_y, rtol=1e-7, atol=1e-7)\n-\n-\n-@pytest.mark.parametrize('num_warps, block_size, iter_size', [\n-    [4, 256, 1],\n-    [4, 1024, 256],\n-])\n-def test_fmin_no_mask(num_warps, block_size, iter_size):\n-    @triton.jit\n-    def kernel(x_ptr,\n-               y_ptr,\n-               z_ptr,\n-               block_size,\n-               iter_size: tl.constexpr):\n-        pid = tl.program_id(axis=0)\n-        for i in range(0, block_size, iter_size):\n-            offset = pid * block_size + tl.arange(0, iter_size)\n-            x_ptrs = x_ptr + offset\n-            y_ptrs = y_ptr + offset\n-\n-            x = tl.load(x_ptrs)\n-            y = tl.load(y_ptrs)\n-            z = tl.libdevice.min(x, y)\n-            z_ptrs = z_ptr + offset\n-            tl.store(z_ptrs, z)\n-\n-            x_ptr += iter_size\n-            y_ptr += iter_size\n-            z_ptr += iter_size\n-\n-    x = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n-    y = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n-    z = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n-\n-    grid = lambda EA: (x.shape.numel() // (block_size),)\n-    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z,\n-                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps)\n-\n-    golden_z = torch.minimum(x, y)\n-    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n-\n-\n-@pytest.mark.parametrize('num_warps, block_size, iter_size', [\n-    [4, 256, 1],\n-    [4, 1024, 256],\n-])\n-def test_fmad_rn_no_mask(num_warps, block_size, iter_size):\n-    @triton.jit\n-    def kernel(x_ptr,\n-               y_ptr,\n-               z_ptr,\n-               w_ptr,\n-               block_size,\n-               iter_size: tl.constexpr):\n-        pid = tl.program_id(axis=0)\n-        for i in range(0, block_size, iter_size):\n-            offset = pid * block_size + tl.arange(0, iter_size)\n-            x_ptrs = x_ptr + offset\n-            y_ptrs = y_ptr + offset\n-            z_ptrs = z_ptr + offset\n-\n-            x = tl.load(x_ptrs)\n-            y = tl.load(y_ptrs)\n-            z = tl.load(z_ptrs)\n-\n-            w = tl.libdevice.fma_rn(x, y, z)\n-            w_ptrs = w_ptr + offset\n-            tl.store(w_ptrs, w)\n-\n-            x_ptr += iter_size\n-            y_ptr += iter_size\n-            z_ptr += iter_size\n-            w_ptr += iter_size\n-\n-    x = torch.randn((block_size,), device='cuda', dtype=torch.float64)\n-    y = torch.randn((block_size,), device='cuda', dtype=torch.float64)\n-    z = torch.randn((block_size,), device='cuda', dtype=torch.float64)\n-    w = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n-\n-    grid = lambda EA: (x.shape.numel() // (block_size),)\n-    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, w_ptr=w,\n-                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps)\n-\n-    golden_w = x * y + z\n-    assert_close(w, golden_w, rtol=1e-7, atol=1e-7)\n-\n-\n-@pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n-                         [('int32', 'libdevice.ffs', system_libdevice_path()),\n-                          ('int32', 'libdevice.ffs', '')])\n-def test_libdevice(dtype_str, expr, lib_path):\n-    src = f\"\"\"\n-def kernel(X, Y, BLOCK: tl.constexpr):\n-    x = tl.load(X + tl.arange(0, BLOCK))\n-    y = tl.{expr}(x)\n-    tl.store(Y + tl.arange(0, BLOCK), y)\n-\"\"\"\n-    import tempfile\n-    from inspect import Parameter, Signature\n-\n-    import _testcapi\n-\n-    fp = tempfile.NamedTemporaryFile(mode='w', suffix=\".py\")\n-    fp.write(src)\n-    fp.flush()\n-\n-    def kernel(X, Y, BLOCK: tl.constexpr):\n-        pass\n-    kernel.__code__ = _testcapi.code_newempty(fp.name, \"kernel\", 1)\n-    parameters = []\n-    parameters.append(Parameter(\"X\", 1))\n-    parameters.append(Parameter(\"Y\", 1))\n-    parameters.append(Parameter(\"BLOCK\", 1))\n-    kernel.__signature__ = Signature(parameters=parameters)\n-    kernel = triton.jit(kernel)\n-\n-    torch_type = {\n-        \"int32\": torch.int32,\n-        \"float32\": torch.float32,\n-        \"float64\": torch.float64\n-    }\n-\n-    shape = (128, )\n-    # limit the range of integers so that the sum does not overflow\n-    x = None\n-    if dtype_str == \"int32\":\n-        x = torch.randint(2**31 - 1, shape, dtype=torch_type[dtype_str], device=\"cuda\")\n-    else:\n-        x = torch.randn(shape, dtype=torch_type[dtype_str], device=\"cuda\")\n-    if expr == 'libdevice.ffs':\n-        y_ref = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n-        for i in range(shape[0]):\n-            y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n-\n-    # triton result\n-    y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n-    kernel[(1,)](x, y, BLOCK=shape[0], extern_libs={\"libdevice\": lib_path})\n-    # compare\n-    assert_close(y, y_ref)"}, {"filename": "python/tests/test_gemm.py", "status": "removed", "additions": 0, "deletions": 314, "changes": 314, "file_content_changes": "@@ -1,314 +0,0 @@\n-import pytest\n-import torch\n-from torch.testing import assert_close\n-\n-import triton\n-import triton.language as tl\n-\n-\n-@triton.jit\n-def matmul_no_scf_kernel(\n-    a_ptr, b_ptr, c_ptr,\n-    stride_am, stride_ak,\n-    stride_bk, stride_bn,\n-    stride_cm, stride_cn,\n-    M: tl.constexpr, N: tl.constexpr, K: tl.constexpr\n-):\n-    offs_m = tl.arange(0, M)\n-    offs_n = tl.arange(0, N)\n-    offs_k = tl.arange(0, K)\n-    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n-    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n-    a = tl.load(a_ptrs)\n-    b = tl.load(b_ptrs)\n-\n-    c = tl.dot(a, b)\n-\n-    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n-    tl.store(c_ptrs, c)\n-\n-\n-@pytest.mark.parametrize('SHAPE,NUM_WARPS,TRANS_A,TRANS_B', [\n-    (shape, num_warps, trans_a, trans_b)\n-    for shape in [\n-        [128, 256, 32],\n-        # [256, 128, 16],\n-        [128, 16, 32],\n-        [32, 128, 64],\n-        [128, 128, 64],\n-        [64, 128, 128],\n-    ]\n-    for num_warps in [2, 4]\n-    for trans_a in [False, True]\n-    for trans_b in [False, True]\n-])\n-def test_gemm_no_scf(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n-    SIZE_M, SIZE_N, SIZE_K = SHAPE\n-    if (TRANS_A):\n-        a = torch.randn((SIZE_K, SIZE_M), device='cuda', dtype=torch.float16).T\n-    else:\n-        a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n-\n-    if (TRANS_B):\n-        b = torch.randn((SIZE_N, SIZE_K), device='cuda', dtype=torch.float16).T\n-    else:\n-        b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n-\n-    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n-    grid = lambda META: (1, )\n-    matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n-                               stride_am=a.stride(0), stride_ak=a.stride(1),\n-                               stride_bk=b.stride(0), stride_bn=b.stride(1),\n-                               stride_cm=c.stride(0), stride_cn=c.stride(1),\n-                               M=SIZE_M, N=SIZE_N, K=SIZE_K,\n-                               num_warps=NUM_WARPS)\n-    golden = torch.matmul(a, b)\n-    torch.set_printoptions(profile=\"full\")\n-    assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n-\n-\n-@pytest.mark.parametrize('SHAPE,NUM_WARPS,TRANS_A,TRANS_B', [\n-    (shape, num_warps, trans_a, trans_b)\n-    for shape in [\n-        [64, 128, 128],\n-        [128, 128, 128],\n-        [16, 16, 32],\n-        [32, 16, 64],\n-        [32, 16, 64],\n-    ]\n-    for num_warps in [1, 2, 4]\n-    for trans_a in [False, True]\n-    for trans_b in [False, True]\n-])\n-def test_gemm_no_scf_int8(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n-    guard_for_volta(is_int8=True)\n-\n-    SIZE_M, SIZE_N, SIZE_K = SHAPE\n-\n-    if (TRANS_A):\n-        a = torch.randint(-5, 5, (SIZE_K, SIZE_M), device='cuda', dtype=torch.int8).T\n-    else:\n-        a = torch.randint(-5, 5, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n-\n-    if (TRANS_B):\n-        b = torch.randint(-5, 5, (SIZE_N, SIZE_K), device='cuda', dtype=torch.int8).T\n-    else:\n-        b = torch.randint(-5, 5, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n-\n-    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.int32)\n-\n-    grid = lambda META: (1, )\n-    matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n-                               stride_am=a.stride(0), stride_ak=a.stride(1),\n-                               stride_bk=b.stride(0), stride_bn=b.stride(1),\n-                               stride_cm=c.stride(0), stride_cn=c.stride(1),\n-                               M=SIZE_M, N=SIZE_N, K=SIZE_K,\n-                               num_warps=NUM_WARPS)\n-\n-    aa = a.cpu()\n-    bb = b.cpu()\n-    golden = torch.matmul(aa.float(), bb.float()).int()\n-    torch.set_printoptions(profile=\"full\")\n-    torch.testing.assert_close(c.cpu(), golden, check_dtype=False)\n-\n-\n-@triton.jit\n-def matmul_kernel(\n-    a_ptr, b_ptr, c_ptr,\n-    stride_am, stride_ak,\n-    stride_bk, stride_bn,\n-    stride_cm, stride_cn,\n-    M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n-    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n-):\n-    offs_m = tl.arange(0, BLOCK_SIZE_M)\n-    offs_n = tl.arange(0, BLOCK_SIZE_N)\n-    offs_k = tl.arange(0, BLOCK_SIZE_K)\n-    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n-    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n-    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-    for k in range(0, K, BLOCK_SIZE_K):\n-        a = tl.load(a_ptrs)\n-        b = tl.load(b_ptrs)\n-        accumulator += tl.dot(a, b)\n-        a_ptrs += BLOCK_SIZE_K * stride_ak\n-        b_ptrs += BLOCK_SIZE_K * stride_bk\n-\n-    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n-    tl.store(c_ptrs, accumulator)\n-\n-\n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K)).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K)).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N)).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N)).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-# It's not easy to get a proper error threshold in different size\n-# Here the gemm calculation is padded to a different size in order to get\n-# a variant version of the golden result. And the error between golden and\n-# golden_variant provide reference on selecting the proper rtol / atol.\n-\n-\n-def get_proper_err(a, b, golden):\n-    golden_variant = get_variant_golden(a, b)\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n-    return (golden_abs_err, golden_rel_err)\n-\n-\n-@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,TRANS_A,TRANS_B', [\n-    # Non-forloop\n-    [64, 32, 64, 4, 64, 32, 64, False, False],\n-    [128, 64, 128, 4, 128, 64, 128, False, False],\n-    [16, 16, 16, 16, 16, 16, 16, False, False],  # wpt overflow issue\n-    # K-Forloop\n-    # [16, 16, 64, 4, 8, 8, 8, False, False],  # Wrap threads\n-    [32, 32, 64, 4, 32, 32, 32, False, False],  # Single shared encoding\n-    [16, 16, 128, 4, 16, 16, 16, False, False],  # Single shared encoding and small k\n-    [64, 32, 128, 4, 64, 32, 64, False, False],\n-    [128, 16, 128, 4, 128, 16, 32, False, False],\n-    [32, 16, 128, 4, 32, 16, 32, False, False],\n-    [32, 64, 128, 4, 32, 64, 32, False, False],\n-    [32, 128, 256, 4, 32, 128, 64, False, False],\n-    [64, 128, 64, 4, 64, 128, 32, False, False],\n-    [64, 64, 128, 4, 64, 64, 32, False, False],\n-    [128, 128, 64, 4, 128, 128, 32, False, False],\n-    [128, 128, 128, 4, 128, 128, 32, False, False],\n-    [128, 128, 256, 4, 128, 128, 64, False, False],\n-    [128, 256, 128, 4, 128, 256, 32, False, False],\n-    [256, 128, 64, 4, 256, 128, 16, False, False],\n-    [128, 64, 128, 4, 128, 64, 32, False, False],\n-    [16, 16, 64, 4, 16, 16, 16, False, False],\n-    [32, 32, 64, 4, 32, 32, 32, False, False],\n-    # trans\n-    [128, 64, 128, 4, 128, 64, 32, True, False],\n-    [128, 64, 128, 4, 128, 64, 32, False, True],\n-])\n-def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n-\n-    if (TRANS_A):\n-        a = torch.randn((SIZE_K, SIZE_M), device='cuda', dtype=torch.float16).T\n-    else:\n-        a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n-\n-    if (TRANS_B):\n-        b = torch.randn((SIZE_N, SIZE_K), device='cuda', dtype=torch.float16).T\n-    else:\n-        b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n-\n-    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n-    grid = lambda META: (1, )\n-    matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n-                        stride_am=a.stride(0), stride_ak=a.stride(1),\n-                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n-                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n-                        M=a.shape[0], N=b.shape[1], K=a.shape[1],\n-                        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n-                        num_warps=NUM_WARPS)\n-    golden = torch.matmul(a, b)\n-    golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n-    torch.set_printoptions(profile=\"full\")\n-    assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n-\n-\n-@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K,allow_tf32', [\n-    [32, 32, 16, 4, 32, 32, 16, False],\n-    [32, 32, 16, 4, 32, 32, 16, True],\n-    [32, 16, 16, 4, 32, 32, 16, False],\n-    [32, 16, 16, 4, 32, 32, 16, True],\n-    [127, 41, 43, 4, 32, 32, 16, False],\n-    [127, 41, 43, 4, 32, 32, 16, True],\n-    [128, 8, 8, 4, 32, 32, 16, False],\n-    [128, 8, 8, 4, 32, 32, 16, True]\n-])\n-def test_gemm_fp32(M, N, K, num_warps, block_M, block_N, block_K, allow_tf32):\n-    @triton.jit\n-    def matmul_kernel(\n-        a_ptr, b_ptr, c_ptr,\n-        M, N, K,\n-        stride_am, stride_ak,\n-        stride_bk, stride_bn,\n-        stride_cm, stride_cn,\n-        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n-        ALLOW_TF32: tl.constexpr\n-    ):\n-        pid = tl.program_id(axis=0)\n-        # num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n-        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-        pid_m = pid // num_pid_n\n-        pid_n = pid % num_pid_n\n-\n-        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-        offs_k = tl.arange(0, BLOCK_SIZE_K)\n-        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n-        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n-\n-        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-        for k in range(0, K, BLOCK_SIZE_K):\n-            a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n-            b_mask = (offs_k[:, None] < K) & (offs_bn[None, :] < N)\n-            a = tl.load(a_ptrs, a_mask, other=0.0)\n-            b = tl.load(b_ptrs, b_mask, other=0.0)\n-            accumulator += tl.dot(a, b, allow_tf32=ALLOW_TF32)\n-            a_ptrs += BLOCK_SIZE_K * stride_ak\n-            b_ptrs += BLOCK_SIZE_K * stride_bk\n-            offs_k += BLOCK_SIZE_K\n-\n-        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-        c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n-        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n-        tl.store(c_ptrs, accumulator, c_mask)\n-\n-    guard_for_volta(is_tf32=allow_tf32)\n-\n-    # Configure the pytorch counterpart\n-    torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n-\n-    a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n-    b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n-    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n-\n-    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n-    matmul_kernel[grid](a, b, c,\n-                        M, N, K,\n-                        stride_am=a.stride(0), stride_ak=a.stride(1),\n-                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n-                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n-                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K, ALLOW_TF32=allow_tf32)\n-\n-    golden = torch.matmul(a, b)\n-    golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n-    if allow_tf32:\n-        # TF32 is not accurate enough\n-        torch.testing.assert_close(c, golden, rtol=max(1e-2, 1.5 * golden_rel_err), atol=max(1e-2, 1.5 * golden_abs_err))\n-    else:\n-        torch.testing.assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err))\n-\n-\n-def guard_for_volta(is_int8=False, is_tf32=False):\n-    '''\n-    Tell whether the test case is valid on Volta GPU.\n-    Some features are WIP, so the corresponding support are missing.\n-    '''\n-    capability = torch.cuda.get_device_capability()\n-    is_on_Volta = capability[0] < 8\n-    # TODO[Superjomn]: Remove the constraints below when features are ready\n-    is_feature_supported = not (is_int8 or is_tf32)\n-\n-    if is_on_Volta:\n-        if (not is_feature_supported):\n-            pytest.skip(\"Not valid on Volta\")\n\\ No newline at end of file"}, {"filename": "python/tests/test_matmul.py", "status": "removed", "additions": 0, "deletions": 101, "changes": 101, "file_content_changes": "@@ -1,101 +0,0 @@\n-import itertools\n-\n-import pytest\n-import torch\n-\n-import triton\n-import triton._C.libtriton.triton as _triton\n-\n-\n-@pytest.mark.parametrize(\n-    \"BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, DTYPE\",\n-    itertools.chain(\n-        *[\n-            [\n-                # 1 warp\n-                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (16, 32, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (16, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (16, 32, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (16, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (64, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (16, 64, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                # 2 warp\n-                (64, 32, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 64, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n-                (64, 32, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 64, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n-                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 128, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n-                # 4 warp\n-                (128, 64, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (64, 128, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (128, 32, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 128, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (128, 32, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 128, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n-                # 8 warp\n-                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE),\n-                (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE),\n-                (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE),\n-                # split-k\n-                (64, 64, 16, 2, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (64, 64, 16, 4, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (64, 64, 16, 8, 4, 2, None, None, None, AT, BT, DTYPE),\n-                # variable input\n-                (128, 128, 32, 1, 4, 2, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 384, 128, 640, AT, BT, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE),\n-            ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n-        ],\n-        # n-stage\n-        *[\n-            [\n-                (16, 16, 16, 1, 1, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (64, 32, 64, 1, 2, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (128, 64, 16, 1, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (256, 128, 32, 1, 8, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (128, 128, 32, 1, 4, STAGES, 384, 128, 640, AT, BT, DTYPE),\n-                # split-k\n-                (64, 64, 16, 8, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (64, 64, 16, 8, 4, STAGES, 1024, 1024, 32, AT, BT, DTYPE),\n-            ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [2, 3, 4]\n-        ]\n-    ),\n-)\n-def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, DTYPE):\n-    capability = torch.cuda.get_device_capability()\n-    if capability[0] < 7:\n-        pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n-    if capability[0] < 8 and DTYPE == \"bfloat16\":\n-        pytest.skip(\"Only test bfloat16 on devices with sm >= 80\")\n-    #if DTYPE == \"bfloat16\" and SPLIT_K != 1:\n-    #    pytest.skip(\"bfloat16 matmuls don't allow split_k for now\")\n-    if DTYPE == \"bfloat16\":\n-        pytest.skip(\"bfloat16 matmuls doesn't support for now\")\n-    torch.manual_seed(0)\n-    # nuke kernel decorators -- will set meta-parameters manually\n-    kwargs = {'BLOCK_M': BLOCK_M, 'BLOCK_N': BLOCK_N, 'BLOCK_K': BLOCK_K, 'SPLIT_K': SPLIT_K}\n-    pre_hook = None if SPLIT_K == 1 else lambda nargs: nargs['C'].zero_()\n-    configs = [triton.Config(kwargs=kwargs, num_warps=NWARP, num_stages=NSTAGE, pre_hook=pre_hook)]\n-    kernel = triton.ops._matmul.kernel\n-    kernel.configs = configs\n-    # kernel.run = kernel.run.run.run\n-\n-    # get matrix shape\n-    M = BLOCK_M if M is None else M\n-    N = BLOCK_N if N is None else N\n-    K = BLOCK_K * SPLIT_K if K is None else K\n-    # allocate/transpose inputs\n-    DTYPE = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[DTYPE]\n-    a = .1 * torch.randn((K, M) if AT else (M, K), device=\"cuda\", dtype=DTYPE)\n-    b = .1 * torch.randn((N, K) if BT else (K, N), device=\"cuda\", dtype=DTYPE)\n-    a = a.t() if AT else a\n-    b = b.t() if BT else b\n-    # run test\n-    th_c = torch.matmul(a, b)\n-    tt_c = triton.testing.catch_oor(lambda: triton.ops.matmul(a, b), pytest)\n-    triton.testing.assert_almost_equal(th_c, tt_c)"}, {"filename": "python/tests/test_random.py", "status": "removed", "additions": 0, "deletions": 198, "changes": 198, "file_content_changes": "@@ -1,198 +0,0 @@\n-import numpy as np\n-import pytest\n-import scipy.stats\n-import torch\n-\n-import triton\n-import triton.language as tl\n-\n-#####################################\n-# Reference Philox Implementation\n-#####################################\n-\n-\n-class PhiloxConfig:\n-    def __init__(self, PHILOX_ROUND_A, PHILOX_ROUND_B, PHILOX_KEY_A, PHILOX_KEY_B, DTYPE):\n-        self.PHILOX_ROUND_A = np.array(PHILOX_ROUND_A, dtype=DTYPE)\n-        self.PHILOX_ROUND_B = np.array(PHILOX_ROUND_B, dtype=DTYPE)\n-        self.PHILOX_KEY_A = np.array(PHILOX_KEY_A, dtype=DTYPE)\n-        self.PHILOX_KEY_B = np.array(PHILOX_KEY_B, dtype=DTYPE)\n-        self.DTYPE = DTYPE\n-\n-\n-# This is better for GPU\n-PHILOX_32 = PhiloxConfig(\n-    PHILOX_KEY_A=0x9E3779B9,\n-    PHILOX_KEY_B=0xBB67AE85,\n-    PHILOX_ROUND_A=0xD2511F53,\n-    PHILOX_ROUND_B=0xCD9E8D57,\n-    DTYPE=np.uint32,\n-)\n-\n-# This is what numpy implements\n-PHILOX_64 = PhiloxConfig(\n-    PHILOX_KEY_A=0x9E3779B97F4A7C15,\n-    PHILOX_KEY_B=0xBB67AE8584CAA73B,\n-    PHILOX_ROUND_A=0xD2E7470EE14C6C93,\n-    PHILOX_ROUND_B=0xCA5A826395121157,\n-    DTYPE=np.uint64,\n-)\n-\n-\n-class CustomPhilox4x:\n-    def __init__(self, seed, config):\n-        self._config = config\n-        seed = self._into_pieces(seed)\n-        self._key = np.array(seed[:2], dtype=self._dtype)\n-        self._counter = np.array((0, 0) + seed[2:], dtype=self._dtype)\n-\n-    @property\n-    def _dtype(self):\n-        return self._config.DTYPE\n-\n-    def _into_pieces(self, n, pad=4):\n-        res = []\n-        while len(res) < pad:\n-            res.append(np.array(n, dtype=self._dtype))\n-            n >>= (np.dtype(self._dtype).itemsize * 8)\n-        assert n == 0\n-        return tuple(res)\n-\n-    def _multiply_low_high(self, a, b):\n-        low = a * b\n-        high = int(a) * int(b)\n-        high = np.array(high >> (np.dtype(self._dtype).itemsize * 8), dtype=self._dtype)\n-        return low, high\n-\n-    def _single_round(self, counter, key):\n-        lo0, hi0 = self._multiply_low_high(self._config.PHILOX_ROUND_A, counter[0])\n-        lo1, hi1 = self._multiply_low_high(self._config.PHILOX_ROUND_B, counter[2])\n-        ret0 = hi1 ^ counter[1] ^ key[0]\n-        ret1 = lo1\n-        ret2 = hi0 ^ counter[3] ^ key[1]\n-        ret3 = lo0\n-        return np.array([ret0, ret1, ret2, ret3], dtype=self._dtype)\n-\n-    def _raise_key(self, key):\n-        pk = [self._config.PHILOX_KEY_A, self._config.PHILOX_KEY_B]\n-        return key + np.array(pk, dtype=self._dtype)\n-\n-    def random_raw(self):\n-        counter = self._counter\n-        key = self._key\n-        for _ in range(10):\n-            counter = self._single_round(counter, key)\n-            key = self._raise_key(key)\n-        self.advance(1)\n-        return counter\n-\n-    def advance(self, n_steps):\n-        self._counter[0] += n_steps\n-        assert self._counter[0] < 2**32, \"FIXME: doesn't work for large offsets\"\n-\n-\n-class CustomPhilox(CustomPhilox4x):\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-        self.buffer = []\n-\n-    def random_raw(self):\n-        if len(self.buffer) == 0:\n-            self.buffer = list(super().random_raw())[::-1]\n-        return int(self.buffer.pop())\n-\n-\n-#####################################\n-# Unit Tests\n-#####################################\n-\n-BLOCK = 1024\n-\n-# test generation of random uint32\n-\n-\n-@pytest.mark.parametrize('size, seed',\n-                         [(size, seed) for size in ['10', '4,53', '10000']\n-                          for seed in [0, 42, 124, 54, 0xffffffff, 0xdeadbeefcafeb0ba]]\n-                         )\n-def test_randint(size, seed, device='cuda'):\n-    size = list(map(int, size.split(',')))\n-\n-    @triton.jit\n-    def kernel(X, N, seed):\n-        offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n-        rand = tl.randint(seed, offset)\n-        tl.store(X + offset, rand, mask=offset < N)\n-    # triton result\n-    x = torch.empty(size, dtype=torch.int32, device=device)\n-    N = x.numel()\n-    grid = (triton.cdiv(N, BLOCK),)\n-    kernel[grid](x, N, seed)\n-    out_tri = x.cpu().numpy().astype(np.uint32).flatten().tolist()\n-    # reference result\n-    gen = CustomPhilox4x(seed, config=PHILOX_32)\n-    out_ref = [gen.random_raw()[0] for _ in out_tri]\n-    assert out_tri == out_ref\n-\n-# test uniform PRNG\n-\n-\n-@pytest.mark.parametrize('size, seed',\n-                         [(size, seed) for size in [1000000]\n-                          for seed in [0, 42, 124, 54]]\n-                         )\n-def test_rand(size, seed, device='cuda'):\n-    @triton.jit\n-    def kernel(X, N, seed):\n-        offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n-        rand = tl.rand(seed, offset)\n-        tl.store(X + offset, rand, mask=offset < N)\n-    # triton result\n-    x = torch.empty(size, dtype=torch.float32, device=device)\n-    N = x.numel()\n-    grid = (triton.cdiv(N, BLOCK),)\n-    kernel[grid](x, N, seed)\n-    assert all((x >= 0) & (x <= 1))\n-    assert scipy.stats.kstest(x.tolist(), 'uniform', args=(0, 1)).statistic < 0.01\n-\n-# test normal PRNG\n-\n-\n-@pytest.mark.parametrize('size, seed',\n-                         [(size, seed) for size in [1000000]\n-                          for seed in [0, 42, 124, 54]]\n-                         )\n-def test_randn(size, seed, device='cuda'):\n-    @triton.jit\n-    def kernel(X, N, seed):\n-        offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n-        rand = tl.randn(seed, offset)\n-        tl.store(X + offset, rand, mask=offset < N)\n-    # triton result\n-    x = torch.empty(size, dtype=torch.float32, device=device)\n-    N = x.numel()\n-    grid = (triton.cdiv(N, BLOCK),)\n-    kernel[grid](x, N, seed)\n-    assert abs(x.mean()) < 1e-2\n-    assert abs(x.std() - 1) < 1e-2\n-\n-\n-# tl.rand() should never produce >=1.0\n-\n-def test_rand_limits():\n-    @triton.jit\n-    def kernel(input, output, n: tl.constexpr):\n-        idx = tl.arange(0, n)\n-        x = tl.load(input + idx)\n-        y = tl.random.uint32_to_uniform_float(x)\n-        tl.store(output + idx, y)\n-\n-    min_max_int32 = torch.tensor([\n-        torch.iinfo(torch.int32).min,\n-        torch.iinfo(torch.int32).max,\n-    ], dtype=torch.int32, device='cuda')\n-    output = torch.empty(2, dtype=torch.float32, device='cuda')\n-    kernel[(1,)](min_max_int32, output, 2)\n-\n-    assert output[0] == output[1]\n-    assert 1.0 - torch.finfo(torch.float32).eps <= output[0].item() < 1.0"}, {"filename": "python/tests/test_reduce.py", "status": "removed", "additions": 0, "deletions": 164, "changes": 164, "file_content_changes": "@@ -1,164 +0,0 @@\n-import pytest\n-import numpy as np\n-import torch\n-from torch.testing import assert_close\n-\n-import triton\n-import triton.language as tl\n-\n-int_dtypes = ['int8', 'int16', 'int32', 'int64']\n-uint_dtypes = ['uint8']  # PyTorch does not support uint16/uint32/uint64\n-float_dtypes = ['float16', 'float32', 'float64']\n-dtypes = int_dtypes + uint_dtypes + float_dtypes\n-dtypes_with_bfloat16 = int_dtypes + uint_dtypes + float_dtypes\n-dtype_mapping = {dtype_str: torch.__dict__[dtype_str] for dtype_str in dtypes}\n-\n-\n-def get_reduced_dtype(op, dtype):\n-    if op in ['argmin', 'argmax']:\n-        return torch.int32\n-    if dtype in [torch.int8, torch.int16, torch.uint8]:\n-        return torch.int32\n-    if dtype in [torch.bfloat16]:\n-        return torch.float32\n-    return dtype\n-\n-\n-def patch_kernel(template, to_replace):\n-    kernel = triton.JITFunction(template.fn)\n-    for key, value in to_replace.items():\n-        kernel.src = kernel.src.replace(key, value)\n-    return kernel\n-\n-\n-@triton.jit\n-def reduce1d_kernel(x_ptr, z_ptr, block: tl.constexpr):\n-    x = tl.load(x_ptr + tl.arange(0, block))\n-    tl.store(z_ptr, tl.OP(x, axis=0))\n-\n-\n-@triton.jit\n-def reduce2d_kernel(x_ptr, z_ptr, axis: tl.constexpr, block_m: tl.constexpr, block_n: tl.constexpr):\n-    range_m = tl.arange(0, block_m)\n-    range_n = tl.arange(0, block_n)\n-    x = tl.load(x_ptr + range_m[:, None] * block_n + range_n[None, :])\n-    z = tl.OP(x, axis=axis)\n-    if axis == 0:\n-        tl.store(z_ptr + range_n, z)\n-    else:\n-        tl.store(z_ptr + range_m, z)\n-\n-\n-reduce1d_configs = [\n-    (op, dtype, shape)\n-    for op in ['sum', 'min', 'max', 'argmin', 'argmax', 'xor_sum']\n-    for dtype in dtypes\n-    for shape in [4, 8, 16, 32, 64, 128, 512, 1024]\n-]\n-\n-\n-@pytest.mark.parametrize('op, dtype, shape', reduce1d_configs)\n-def test_reduce1d(op, dtype, shape):\n-    if op == 'xor_sum' and dtype in float_dtypes:\n-        return\n-\n-    dtype = dtype_mapping[dtype]\n-    reduced_dtype = get_reduced_dtype(op, dtype)\n-\n-    if dtype.is_floating_point:\n-        x = torch.randn((shape,), device='cuda', dtype=dtype)\n-    elif dtype is torch.uint8:\n-        x = torch.randint(0, 20, (shape,), device='cuda', dtype=dtype)\n-    else:\n-        x = torch.randint(-20, 20, (shape,), device='cuda', dtype=dtype)\n-    z = torch.empty(\n-        tuple(),\n-        device=x.device,\n-        dtype=reduced_dtype,\n-    )\n-\n-    kernel = patch_kernel(reduce1d_kernel, {'OP': op})\n-    grid = (1,)\n-    kernel[grid](x_ptr=x, z_ptr=z, block=shape)\n-\n-    if op == 'sum':\n-        golden_z = torch.sum(x, dtype=reduced_dtype)\n-    elif op == 'min':\n-        golden_z = torch.min(x).to(reduced_dtype)\n-    elif op == 'max':\n-        golden_z = torch.max(x).to(reduced_dtype)\n-    elif op == 'argmin':\n-        golden_z = torch.argmin(x).to(reduced_dtype)\n-    elif op == 'argmax':\n-        golden_z = torch.argmax(x).to(reduced_dtype)\n-    elif op == 'xor_sum':\n-        sum_npy = np.bitwise_xor.reduce(x.cpu().numpy())\n-        golden_z = torch.tensor(sum_npy, dtype=reduced_dtype).cuda()\n-    else:\n-        raise RuntimeError(f'Unknwon reduce op {op}')\n-\n-    if dtype.is_floating_point and op == 'sum':\n-        if shape >= 256:\n-            assert_close(z, golden_z, rtol=0.05, atol=0.1)\n-        elif shape >= 32:\n-            assert_close(z, golden_z, rtol=0.05, atol=0.02)\n-        else:\n-            assert_close(z, golden_z, rtol=0.01, atol=0.01)\n-    else:\n-        assert_close(z, golden_z, rtol=0.001, atol=0.001)\n-\n-\n-reduce2d_configs = [\n-    (op, dtype, shape, axis)\n-    for op in ['sum', 'min', 'max', 'argmin', 'argmax', 'xor_sum']\n-    for dtype in dtypes\n-    for shape in [(1, 4), (1, 8), (1, 16), (1, 32), (2, 32), (4, 32), (4, 128), (32, 64)]\n-    for axis in [0, 1]\n-]\n-\n-\n-@pytest.mark.parametrize('op, dtype, shape, axis', reduce2d_configs)\n-def test_reduce2d(op, dtype, shape, axis):\n-    if op == 'xor_sum' and dtype in float_dtypes:\n-        return\n-\n-    dtype = dtype_mapping[dtype]\n-    reduced_dtype = get_reduced_dtype(op, dtype)\n-    reduced_shape = (shape[1 - axis],)\n-\n-    if dtype.is_floating_point:\n-        x = torch.randn(shape, device='cuda', dtype=dtype)\n-    elif dtype is torch.uint8:\n-        x = torch.randint(0, 20, shape, device='cuda', dtype=dtype)\n-    else:\n-        x = torch.randint(-20, 20, shape, device='cuda', dtype=dtype)\n-    z = torch.empty(reduced_shape, device=x.device, dtype=reduced_dtype)\n-\n-    kernel = patch_kernel(reduce2d_kernel, {'OP': op})\n-    kernel[(1,)](x_ptr=x, z_ptr=z, axis=axis, block_m=shape[0], block_n=shape[1])\n-\n-    if op == 'sum':\n-        golden_z = torch.sum(x, dim=axis, keepdim=False, dtype=reduced_dtype)\n-    elif op == 'min':\n-        golden_z = torch.min(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n-    elif op == 'max':\n-        golden_z = torch.max(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n-    elif op == 'argmin':\n-        golden_z = torch.argmin(x, dim=axis, keepdim=False).to(reduced_dtype)\n-    elif op == 'argmax':\n-        golden_z = torch.argmax(x, dim=axis, keepdim=False).to(reduced_dtype)\n-    elif op == 'xor_sum':\n-        sum_npy = np.bitwise_xor.reduce(x.cpu().numpy(), axis=axis, keepdims=False)\n-        golden_z = torch.tensor(sum_npy, dtype=reduced_dtype).cuda()\n-    else:\n-        raise RuntimeError(f'Unknwon reduce op {op}')\n-\n-    if dtype.is_floating_point and op == 'sum':\n-        if shape[axis] >= 256:\n-            assert_close(z, golden_z, rtol=0.05, atol=0.1)\n-        elif shape[axis] >= 32:\n-            assert_close(z, golden_z, rtol=0.05, atol=0.02)\n-        else:\n-            assert_close(z, golden_z, rtol=0.01, atol=0.01)\n-    else:\n-        assert_close(z, golden_z, rtol=0.001, atol=0.001)"}, {"filename": "python/tests/test_transpose.py", "status": "removed", "additions": 0, "deletions": 47, "changes": 47, "file_content_changes": "@@ -1,47 +0,0 @@\n-import pytest\n-import torch\n-from torch.testing import assert_close\n-\n-import triton\n-import triton.language as tl\n-\n-\n-@triton.jit\n-def kernel(x_ptr, stride_xm,\n-           z_ptr, stride_zn,\n-           SIZE_M: tl.constexpr, SIZE_N: tl.constexpr):\n-    off_m = tl.arange(0, SIZE_M)\n-    off_n = tl.arange(0, SIZE_N)\n-    Xs = x_ptr + off_m[:, None] * stride_xm + off_n[None, :] * 1\n-    Zs = z_ptr + off_m[:, None] * 1 + off_n[None, :] * stride_zn\n-    tl.store(Zs, tl.load(Xs))\n-\n-# These sizes cover the case of:\n-# - blocked layout and sliced layout with block parent\n-#  -- blocked layout in which sizePerThread/threadsPerWarp/warpsPerCTA\n-#     need/need not to be wrapped\n-#  -- sliced layout incase sizePerThread need to be wrapped\n-#  -- different orders\n-# - LayoutConversion from blocked -> blocked\n-# - tt.Broadcast which requires for broadcast in either/both of\n-#   CTA/perThread level\n-\n-# What is not covered and requires for TODO:\n-# - vectorization load/store of shared memory\n-# - multiple replication of layout conversion\n-\n-\n-@pytest.mark.parametrize('NUM_WARPS,SIZE_M,SIZE_N', [\n-    [1, 16, 16],\n-    [1, 32, 32],\n-    [1, 32, 64],\n-    [2, 64, 128],\n-    [2, 128, 64]\n-])\n-def test_convert_layout_impl(NUM_WARPS, SIZE_M, SIZE_N):\n-    grid = lambda META: (1, )\n-    x = torch.randn((SIZE_M, SIZE_N), device='cuda', dtype=torch.float32)\n-    z = torch.empty((SIZE_N, SIZE_M), device=x.device, dtype=x.dtype)\n-    kernel[grid](x_ptr=x, stride_xm=x.stride(0), z_ptr=z, stride_zn=z.stride(0), SIZE_M=SIZE_M, SIZE_N=SIZE_N, num_warps=NUM_WARPS)\n-    golden_z = torch.t(x)\n-    assert_close(z, golden_z, rtol=1e-7, atol=1e-7, check_dtype=False)"}, {"filename": "python/tests/test_vecadd.py", "status": "removed", "additions": 0, "deletions": 215, "changes": 215, "file_content_changes": "@@ -1,215 +0,0 @@\n-import math\n-import random\n-\n-import pytest\n-import torch\n-from torch.testing import assert_close\n-\n-import triton\n-import triton.language as tl\n-\n-\n-@pytest.mark.parametrize('num_warps, block_size, iter_size', [\n-    [4, 256, 1],\n-    [4, 1024, 256],\n-])\n-def test_vecadd_scf_no_mask(num_warps, block_size, iter_size):\n-\n-    @triton.jit\n-    def kernel(x_ptr,\n-               y_ptr,\n-               z_ptr,\n-               block_size,\n-               iter_size: tl.constexpr):\n-        pid = tl.program_id(axis=0)\n-        for i in range(0, block_size, iter_size):\n-            offset = pid * block_size + tl.arange(0, iter_size)\n-            x_ptrs = x_ptr + offset\n-            y_ptrs = y_ptr + offset\n-\n-            x = tl.load(x_ptrs)\n-            y = tl.load(y_ptrs)\n-            z = x + y\n-            z_ptrs = z_ptr + offset\n-            tl.store(z_ptrs, z)\n-\n-            x_ptr += iter_size\n-            y_ptr += iter_size\n-            z_ptr += iter_size\n-\n-    x = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n-    y = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n-    z = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n-\n-    grid = lambda EA: (x.shape.numel() // (block_size),)\n-    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z,\n-                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps)\n-\n-    golden_z = x + y\n-    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n-\n-\n-@pytest.mark.parametrize('shape, num_warps, block_size, iter_size', [\n-    [(127, 3), 2, 128, 1],\n-    [(127, 3), 2, 128, 32],\n-])\n-def test_vecadd_scf_mask(shape, num_warps, block_size, iter_size):\n-    @triton.jit\n-    def kernel(x_ptr,\n-               y_ptr,\n-               z_ptr,\n-               num_elements,\n-               block_size: tl.constexpr,\n-               iter_size: tl.constexpr\n-               ):\n-        '''\n-        @block_size: size of a block\n-        @iter_size: size of the iteration, a block has multiple iterations\n-        @num_elements: number of elements\n-        '''\n-        pid = tl.program_id(axis=0)\n-        for i in range(tl.cdiv(block_size, iter_size)):\n-            # TODO: a bug here, if put the offset outside the forloop, there will be a GPU mis-aligned error.\n-            offset = pid * block_size + tl.arange(0, iter_size)\n-            x_ptrs = x_ptr + offset\n-            y_ptrs = y_ptr + offset\n-\n-            x = tl.load(x_ptrs, mask=offset < num_elements)\n-            y = tl.load(y_ptrs, mask=offset < num_elements)\n-            z = x + y\n-            z_ptrs = z_ptr + offset\n-            tl.store(z_ptrs, z, mask=offset < num_elements)\n-\n-            x_ptr += iter_size\n-            y_ptr += iter_size\n-            z_ptr += iter_size\n-\n-    x = torch.randn(shape, device='cuda', dtype=torch.float32)\n-    y = torch.randn(shape, device='cuda', dtype=torch.float32)\n-    z = torch.empty(shape, device=x.device, dtype=x.dtype)\n-\n-    grid = lambda EA: (math.ceil(x.numel() / block_size),)\n-    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z,\n-                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps,\n-                 num_elements=x.numel())\n-\n-    golden_z = x + y\n-    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n-\n-\n-def vecadd_no_scf_tester(num_warps, block_size, shape):\n-    @triton.jit\n-    def kernel(x_ptr,\n-               y_ptr,\n-               z_ptr,\n-               n_elements,\n-               block_size_N: tl.constexpr):\n-        pid = tl.program_id(axis=0)\n-\n-        offset = pid * block_size_N + tl.arange(0, block_size_N)\n-        x_ptrs = x_ptr + offset\n-        y_ptrs = y_ptr + offset\n-\n-        mask = offset < n_elements\n-\n-        x = tl.load(x_ptrs, mask=mask)\n-        y = tl.load(y_ptrs, mask=mask)\n-        z = x + y\n-        z_ptrs = z_ptr + offset\n-        tl.store(z_ptrs, z, mask=mask)\n-\n-    x = torch.randn(shape, device='cuda', dtype=torch.float32)\n-    y = torch.randn(shape, device='cuda', dtype=torch.float32)\n-    z = torch.empty(shape, device=x.device, dtype=x.dtype)\n-\n-    grid = lambda EA: (math.ceil(x.shape.numel() / block_size),)\n-    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, n_elements=x.shape.numel(), block_size_N=block_size, num_warps=num_warps)\n-\n-    golden_z = x + y\n-    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n-\n-\n-def vecadd_fcmp_no_scf_tester(num_warps, block_size, shape):\n-    '''\n-    vecadd tester with float comparison as load/store mask.\n-    '''\n-    @triton.jit\n-    def kernel(x_ptr,\n-               y_ptr,\n-               z_ptr,\n-               n_elements,\n-               block_size_N: tl.constexpr):\n-        pid = tl.program_id(axis=0)\n-\n-        offset = pid * block_size_N + tl.arange(0, block_size_N)\n-        x_ptrs = x_ptr + offset\n-        y_ptrs = y_ptr + offset\n-\n-        io_mask = offset < n_elements\n-        x = tl.load(x_ptrs, mask=io_mask)\n-        y = tl.load(y_ptrs, mask=io_mask)\n-\n-        z = x + y\n-        val_mask = offset < n_elements and (z < 0. or z > 1.)\n-\n-        z_ptrs = z_ptr + offset\n-        tl.store(z_ptrs, z, mask=val_mask)\n-\n-    x = torch.randn(shape, device='cuda', dtype=torch.float32)\n-    y = torch.randn(shape, device='cuda', dtype=torch.float32)\n-    z = torch.zeros(shape, device=x.device, dtype=x.dtype)\n-\n-    grid = lambda EA: (math.ceil(x.shape.numel() / block_size),)\n-    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, n_elements=x.shape.numel(), block_size_N=block_size, num_warps=num_warps)\n-\n-    golden_z: torch.Tensor = x + y\n-    gz_data = torch.flatten(golden_z)\n-    for i in range(golden_z.numel()):\n-        gz_data[i] = gz_data[i] if gz_data[i] < 0. or gz_data[i] > 1. else 0.\n-\n-    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n-\n-\n-@pytest.mark.parametrize('num_warps, block_size, shape', [\n-    [4, 256, (256,)],\n-    [2, 256, (256,)],\n-    [1, 256, (256,)],\n-    [4, 16, (256,)],\n-    [2, 64, (256,)],\n-    [1, 128, (256,)],\n-])\n-def test_vecadd_no_scf(num_warps, block_size, shape):\n-    vecadd_no_scf_tester(num_warps, block_size, shape)\n-\n-\n-@pytest.mark.parametrize('num_warps, block_size, shape', [\n-    [1, 128, (256 + 1,)],\n-    [1, 256, (256 + 1,)],\n-    [2, 256, (3, 256 + 7)],\n-    [4, 256, (3, 256 + 7)],\n-])\n-def test_vecadd_no_scf_masked(num_warps, block_size, shape):\n-    vecadd_no_scf_tester(num_warps, block_size, shape)\n-\n-\n-def test_vecadd_no_scf_masked_randomly():\n-    random.seed(0)  # fix seed to make random test reproducible\n-    for i in range(10):\n-        num_elements = random.randint(128, 2048)\n-        shape = (num_elements,)\n-        max_warps = num_elements // 32  # floor div\n-        for num_warps in range(1, max_warps):\n-            is_power2 = num_warps & (num_warps - 1) == 0 and num_warps != 0\n-            if not is_power2: continue\n-            block_size = min(32, num_warps * 32)\n-            vecadd_no_scf_tester(num_warps, block_size, shape)\n-\n-\n-@pytest.mark.parametrize('num_warps, block_size, shape', [\n-    [1, 128, (256 + 1,)],\n-    [1, 256, (256 + 1,)],\n-    [2, 256, (3, 256 + 7)],\n-    [4, 256, (3, 256 + 7)],\n-])\n-def test_vecadd_fcmp_no_scf_masked(num_warps, block_size, shape):\n-    vecadd_fcmp_no_scf_tester(num_warps, block_size, shape)"}]