[{"filename": "python/triton/compiler.py", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -867,11 +867,11 @@ def optimize_triton_ir(mod):\n     pm.run(mod)\n     return mod\n \n-def make_triton_ir(fn, signature, specialization, constants):\n+def ast_to_ttir(fn, signature, specialization, constants):\n     mod, _ = build_triton_ir(fn, signature, specialization, constants)\n     return optimize_triton_ir(mod)\n \n-def make_tritongpu_ir(mod, num_warps, num_stages):\n+def ttir_to_ttgir(mod, num_warps, num_stages):\n     pm = _triton.ir.pass_manager(mod.context)\n     pm.add_convert_triton_to_tritongpu_pass(num_warps)\n     pm.enable_debug()\n@@ -897,13 +897,13 @@ def add_external_libs(mod, libs):\n     _triton.add_external_libs(mod, list(libs.keys()), list(libs.values()))\n \n \n-def make_llvm_ir(mod, extern_libs):\n+def ttgir_to_llir(mod, extern_libs):\n     if extern_libs:\n         add_external_libs(mod, extern_libs)\n     return _triton.translate_triton_gpu_to_llvmir(mod)\n \n \n-def make_ptx(mod: Any, **kwargs) -> Tuple[str, int]:\n+def llir_to_ptx(mod: Any, **kwargs) -> Tuple[str, int]:\n     '''\n     Translate TritonGPU module to PTX code.\n     :param mod: a TritonGPU dialect module\n@@ -927,7 +927,7 @@ def make_ptx(mod: Any, **kwargs) -> Tuple[str, int]:\n \n \n \n-def make_cubin(ptx: str, device: int):\n+def ptx_to_cubin(ptx: str, device: int):\n     '''\n     Compile TritonGPU module to cubin.\n     :param ptx: ptx code\n@@ -1338,27 +1338,27 @@ def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: i\n     # ast -> triton-ir (or read from cache)\n     ttir, ttir_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.ttir\", metadata,\n                            run_if_found = lambda path: _triton.ir.parse_mlir_module(path, context),\n-                           run_if_not_found = lambda: make_triton_ir(fn, signature, configs[0], constants))\n+                           run_if_not_found = lambda: ast_to_ttir(fn, signature, configs[0], constants))\n     # triton-ir -> triton-gpu-ir (or read from cache)\n     ttgir, ttgir_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.ttgir\", metadata,\n                             run_if_found = lambda path: _triton.ir.parse_mlir_module(path, context),\n-                            run_if_not_found = lambda: make_tritongpu_ir(ttir, num_warps, num_stages))\n+                            run_if_not_found = lambda: ttir_to_ttgir(ttir, num_warps, num_stages))\n     # triton-gpu-ir -> llvm-ir (or read from cache)\n     llir, llir_md5, force_compile, llvm_cached = read_or_execute(fn_cache_manager, force_compile, f\"{name}.llir\", metadata,\n                            run_if_found = lambda path: Path(path).read_bytes(),\n-                           run_if_not_found = lambda: make_llvm_ir(ttgir, extern_libs))\n+                           run_if_not_found = lambda: ttgir_to_llir(ttgir, extern_libs))\n     if llvm_cached:\n         shmem_size = metadata[\"shared\"]\n     else:\n         shmem_size = _triton.get_shared_memory_size(ttgir)\n     # llvm-ir -> ptx (or read from cache)\n     ptx, ptx_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.ptx\", metadata,\n                           run_if_found = lambda path: Path(path).read_text(),\n-                          run_if_not_found = lambda: make_ptx(llir, device))\n+                          run_if_not_found = lambda: llir_to_ptx(llir, device=device))\n     # ptx -> cubin (or read from cache)\n     cubin, cubin_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.cubin\", metadata,\n                             run_if_found = lambda path: Path(path).read_bytes(),      \n-                            run_if_not_found= lambda: make_cubin(ptx, device))\n+                            run_if_not_found= lambda: ptx_to_cubin(ptx, device))\n     # dump new metadata\n     kernel_name = ptx_get_kernel_name(ptx)\n     metadata = {\"name\": kernel_name, \"shared\": shmem_size, \"num_warps\": num_warps, \"num_stages\": num_stages,"}, {"filename": "python/triton/tools/aot.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -38,13 +38,13 @@\n         exit(0)\n \n     # triton-ir -> triton-gpu-ir\n-    module = triton.compiler.make_tritongpu_ir(module, num_warps=4, num_stages=3)\n+    module = triton.compiler.ttir_to_ttgir(module, num_warps=4, num_stages=3)\n     if args.target == 'triton-gpu-ir':\n         print(module.str())\n         exit(0)\n \n     # triton-gpu-ir -> llvm-ir\n-    module = triton.compiler.make_llvm_ir(module, extern_libs=None)\n+    module = triton.compiler.ttgir_to_llir(module, extern_libs=None)\n     if args.target == 'llvm-ir':\n         print(module)\n         exit(0)\n@@ -55,6 +55,6 @@\n         raise argparse.ArgumentError(None, \"Must specify --ptx-version for PTX compilation\")\n \n     # llvm-ir -> ptx\n-    module = triton.compiler.make_ptx(module, compute_capability=args.sm, ptx_version=args.ptx_version)\n+    module = triton.compiler.llir_to_ptx(module, compute_capability=args.sm, ptx_version=args.ptx_version)\n     assert args.target == 'ptx'\n     print(module)"}]