[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 39, "deletions": 4, "changes": 43, "file_content_changes": "@@ -2707,6 +2707,7 @@ struct ConvertLayoutOpConversion\n       return lowerDistributedToDistributed(op, adaptor, rewriter);\n     }\n     // TODO: to be implemented\n+    llvm_unreachable(\"unsupported layout conversion\");\n     return failure();\n   }\n \n@@ -5763,6 +5764,36 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n \n class ConvertTritonGPUToLLVM\n     : public ConvertTritonGPUToLLVMBase<ConvertTritonGPUToLLVM> {\n+\n+private:\n+  void decomposeBlockedToDotOperand(ModuleOp mod) {\n+\n+    // replace blocked -> dot_op with\n+    // blocked -> shared -> dot_op in order to\n+    mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n+      OpBuilder builder(cvtOp);\n+      auto srcType = cvtOp.getOperand().getType().cast<RankedTensorType>();\n+      auto dstType = cvtOp.getType().cast<RankedTensorType>();\n+      auto srcBlocked =\n+          srcType.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>();\n+      auto dstDotOp =\n+          dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+      if (srcBlocked && dstDotOp) {\n+        auto tmpType = RankedTensorType::get(\n+            dstType.getShape(), dstType.getElementType(),\n+            triton::gpu::SharedEncodingAttr::get(mod.getContext(), dstDotOp,\n+                                                 srcType.getShape(),\n+                                                 srcType.getElementType()));\n+        auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n+            cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n+        auto newConvert = builder.create<triton::gpu::ConvertLayoutOp>(\n+            cvtOp.getLoc(), dstType, tmp);\n+        cvtOp.replaceAllUsesWith(newConvert.getResult());\n+        cvtOp.erase();\n+      }\n+    });\n+  }\n+\n public:\n   ConvertTritonGPUToLLVM() = default;\n \n@@ -5779,15 +5810,19 @@ class ConvertTritonGPUToLLVM\n \n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n \n-    // step 1: Allocate shared memories and insert barriers\n-    // step 2: Convert SCF to CFG\n-    // step 3: Convert FuncOp to LLVMFuncOp via partial conversion\n-    // step 4: Convert the rest of ops via partial conversion\n+    // step 1: Decompose unoptimized layout conversions to use shared memory\n+    // step 2: Allocate shared memories and insert barriers\n+    // step 3: Convert SCF to CFG\n+    // step 4: Convert FuncOp to LLVMFuncOp via partial conversion\n+    // step 5: Convert the rest of ops via partial conversion\n     // The reason for putting step 1 before step 2 is that the membar analysis\n     // currently only supports SCF but not CFG.\n     // The reason for a separation between 1/4 is that, step 3 is out of\n     // the scope of Dialect Conversion, thus we need to make sure the smem\n     // is not revised during the conversion of step 4.\n+\n+    decomposeBlockedToDotOperand(mod);\n+\n     Allocation allocation(mod);\n     MembarAnalysis membar(&allocation);\n "}]