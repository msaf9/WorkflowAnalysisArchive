[{"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 0, "deletions": 151, "changes": 151, "file_content_changes": "@@ -1124,154 +1124,3 @@ void populateElementwiseOpToLLVMPatterns(\n   // __nv_expf for higher-precision calculation\n   patterns.add<ExpOpConversionApprox>(typeConverter, benefit);\n }\n-\n-struct FPExtOpConversion\n-    : ElementwiseOpConversionBase<LLVM::FPExtOp, FPExtOpConversion> {\n-  using Base = ElementwiseOpConversionBase<LLVM::FPExtOp, FPExtOpConversion>;\n-  using Base::Base;\n-  using Adaptor = typename Base::OpAdaptor;\n-\n-  static bool isLegalOp(LLVM::FPExtOp op) {\n-    auto retTy = op.getResult().getType();\n-    auto srcTy = op.getOperand().getType();\n-    if (retTy.isF32() && srcTy.isF16()) {\n-      return false;\n-    }\n-    return true;\n-  }\n-\n-  Value createDestOp(LLVM::FPExtOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n-    return FpToFpOpConversion::convertFp16ToFp32(loc, rewriter, operands[0]);\n-  }\n-};\n-\n-struct FPTruncOpConversion\n-    : ElementwiseOpConversionBase<LLVM::FPTruncOp, FPTruncOpConversion> {\n-  using Base =\n-      ElementwiseOpConversionBase<LLVM::FPTruncOp, FPTruncOpConversion>;\n-  using Base::Base;\n-  using Adaptor = typename Base::OpAdaptor;\n-\n-  static bool isLegalOp(LLVM::FPTruncOp op) {\n-    auto retTy = op.getResult().getType();\n-    auto srcTy = op.getOperand().getType();\n-    if (retTy.isF16() && srcTy.isF32()) {\n-      return false;\n-    }\n-    return true;\n-  }\n-\n-  Value createDestOp(LLVM::FPTruncOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n-    return FpToFpOpConversion::convertFp32ToFp16(loc, rewriter, operands[0]);\n-  }\n-};\n-\n-struct TruncOpConversion\n-    : ElementwiseOpConversionBase<LLVM::TruncOp, TruncOpConversion> {\n-  using Base = ElementwiseOpConversionBase<LLVM::TruncOp, TruncOpConversion>;\n-  using Base::Base;\n-  using Adaptor = typename Base::OpAdaptor;\n-\n-  static bool isLegalOp(LLVM::TruncOp op) {\n-    auto retTy = op.getResult().getType();\n-    auto srcTy = op.getOperand().getType();\n-    if (retTy.isInteger(16) && srcTy.isInteger(32)) {\n-      return false;\n-    }\n-    return true;\n-  }\n-\n-  Value createDestOp(LLVM::TruncOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n-    PTXBuilder builder;\n-    auto &cvt = *builder.create(\"cvt.u16.u32\");\n-    auto res = builder.newOperand(\"=h\");\n-    auto operand = builder.newOperand(operands[0], \"r\");\n-    cvt(res, operand);\n-    return builder.launch(rewriter, loc, i16_ty, false);\n-  }\n-};\n-\n-struct SExtOpConversion\n-    : ElementwiseOpConversionBase<LLVM::SExtOp, SExtOpConversion> {\n-  using Base = ElementwiseOpConversionBase<LLVM::SExtOp, SExtOpConversion>;\n-  using Base::Base;\n-  using Adaptor = typename Base::OpAdaptor;\n-\n-  static bool isLegalOp(LLVM::SExtOp op) {\n-    auto retTy = op.getResult().getType();\n-    auto srcTy = op.getOperand().getType();\n-    if (retTy.isInteger(32) && srcTy.isInteger(16)) {\n-      return false;\n-    }\n-    return true;\n-  }\n-\n-  Value createDestOp(LLVM::SExtOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n-    PTXBuilder builder;\n-    auto &cvt = *builder.create(\"cvt.s32.s16\");\n-    auto res = builder.newOperand(\"=r\");\n-    auto operand = builder.newOperand(operands[0], \"h\");\n-    cvt(res, operand);\n-    return builder.launch(rewriter, loc, i32_ty, false);\n-  }\n-};\n-\n-struct ZExtOpConversion\n-    : ElementwiseOpConversionBase<LLVM::ZExtOp, ZExtOpConversion> {\n-  using Base = ElementwiseOpConversionBase<LLVM::ZExtOp, ZExtOpConversion>;\n-  using Base::Base;\n-  using Adaptor = typename Base::OpAdaptor;\n-\n-  static bool isLegalOp(LLVM::ZExtOp op) {\n-    auto retTy = op.getResult().getType();\n-    auto srcTy = op.getOperand().getType();\n-    if (retTy.isInteger(32) && srcTy.isInteger(16)) {\n-      return false;\n-    }\n-    return true;\n-  }\n-\n-  Value createDestOp(LLVM::ZExtOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n-    PTXBuilder builder;\n-    auto &cvt = *builder.create(\"cvt.u32.u16\");\n-    auto res = builder.newOperand(\"=r\");\n-    auto operand = builder.newOperand(operands[0], \"h\");\n-    cvt(res, operand);\n-    return builder.launch(rewriter, loc, i32_ty, false);\n-  }\n-};\n-\n-bool isLegalElementwiseOp(Operation *op) {\n-  if (isa<LLVM::FPExtOp>(op)) {\n-    return FPExtOpConversion::isLegalOp(cast<LLVM::FPExtOp>(op));\n-  } else if (isa<LLVM::FPTruncOp>(op)) {\n-    return FPTruncOpConversion::isLegalOp(cast<LLVM::FPTruncOp>(op));\n-  } else if (isa<LLVM::TruncOp>(op)) {\n-    return TruncOpConversion::isLegalOp(cast<LLVM::TruncOp>(op));\n-  } else if (isa<LLVM::SExtOp>(op)) {\n-    return SExtOpConversion::isLegalOp(cast<LLVM::SExtOp>(op));\n-  } else if (isa<LLVM::ZExtOp>(op)) {\n-    return ZExtOpConversion::isLegalOp(cast<LLVM::ZExtOp>(op));\n-  }\n-  return true;\n-}\n-\n-void populateElementwiseOpToPTXPatterns(\n-    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    PatternBenefit benefit) {\n-  patterns.add<FPExtOpConversion>(typeConverter, benefit);\n-  patterns.add<FPTruncOpConversion>(typeConverter, benefit);\n-  patterns.add<TruncOpConversion>(typeConverter, benefit);\n-  patterns.add<SExtOpConversion>(typeConverter, benefit);\n-  patterns.add<ZExtOpConversion>(typeConverter, benefit);\n-}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.h", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -13,8 +13,4 @@ void populateElementwiseOpToLLVMPatterns(\n \n bool isLegalElementwiseOp(Operation *op);\n \n-void populateElementwiseOpToPTXPatterns(\n-    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    PatternBenefit benefit);\n-\n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 0, "deletions": 45, "changes": 45, "file_content_changes": "@@ -56,28 +56,6 @@ class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n   }\n };\n \n-class TritonPTXConversionTarget : public ConversionTarget {\n-public:\n-  explicit TritonPTXConversionTarget(MLIRContext &ctx) : ConversionTarget(ctx) {\n-    addDynamicallyLegalDialect<LLVM::LLVMDialect>(\n-        [&](Operation *op) { return isLegalElementwiseOp(op); });\n-\n-    addLegalDialect<NVVM::NVVMDialect>();\n-    addLegalOp<mlir::UnrealizedConversionCastOp>();\n-  }\n-};\n-\n-class TritonGCNConversionTarget : public ConversionTarget {\n-public:\n-  explicit TritonGCNConversionTarget(MLIRContext &ctx) : ConversionTarget(ctx) {\n-    addDynamicallyLegalDialect<LLVM::LLVMDialect>(\n-        [&](Operation *op) { return isLegalElementwiseOp(op); });\n-\n-    addLegalDialect<ROCDL::ROCDLDialect>();\n-    addLegalOp<mlir::UnrealizedConversionCastOp>();\n-  }\n-};\n-\n struct ReturnOpConversion : public ConvertOpToLLVMPattern<func::ReturnOp> {\n   using ConvertOpToLLVMPattern<func::ReturnOp>::ConvertOpToLLVMPattern;\n \n@@ -236,29 +214,6 @@ class ConvertTritonGPUToLLVM\n                                                           patterns);\n     if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n       return signalPassFailure();\n-\n-    if (isROCM) {\n-      TritonGCNConversionTarget gcnTarget(*context);\n-      RewritePatternSet gcnPatterns(context);\n-      populateElementwiseOpToPTXPatterns(typeConverter, gcnPatterns,\n-                                         /*benefits=*/10);\n-      if (failed(\n-              applyPartialConversion(mod, gcnTarget, std::move(gcnPatterns))))\n-        return signalPassFailure();\n-    } else {\n-      // Use our custom converters to convert some operations to PTX to avoid\n-      // using NVPTX for two reasons:\n-      // 1. NVPTX backend is flaky on data types like float16 and bfloat16\n-      // 2. In some cases, we may generate faster PTX code than NVPTX backend\n-      TritonPTXConversionTarget ptxTarget(*context);\n-      RewritePatternSet ptxPatterns(context);\n-      // Add patterns to convert LLVM to PTX\n-      populateElementwiseOpToPTXPatterns(typeConverter, ptxPatterns,\n-                                         /*benefits=*/10);\n-      if (failed(\n-              applyPartialConversion(mod, ptxTarget, std::move(ptxPatterns))))\n-        return signalPassFailure();\n-    }\n   }\n \n private:"}, {"filename": "python/MANIFEST.in", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1 +1,2 @@\n graft src\n+graft triton/third_party"}, {"filename": "python/setup.py", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -221,16 +221,18 @@ def build_extension(self, ext):\n     packages=[\n         \"triton\",\n         \"triton/_C\",\n-        \"triton/language\",\n-        \"triton/tools\",\n         \"triton/common\",\n+        \"triton/compiler\",\n+        \"triton/language\",\n         \"triton/ops\",\n+        \"triton/ops/blocksparse\",\n         \"triton/runtime\",\n-        \"triton/ops/blocksparse\"],\n+        \"triton/runtime/driver\",\n+        \"triton/tools\",\n+    ],\n     install_requires=[\n         \"filelock\",\n     ],\n-    package_data={\"triton\": [\"third_party/**/*\"]},\n     include_package_data=True,\n     ext_modules=[CMakeExtension(\"triton\", \"triton/_C/\")],\n     cmdclass={\"build_ext\": CMakeBuild},"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 31, "deletions": 2, "changes": 33, "file_content_changes": "@@ -408,6 +408,35 @@ def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n     _test_binary(dtype_x, dtype_y, expr, numpy_expr, mode_x=mode_x, mode_y=mode_y, device=device)\n \n \n+# ---------------\n+# test broadcast\n+# ---------------\n+@pytest.mark.parametrize(\"dtype\", dtypes_with_bfloat16)\n+def test_broadcast(dtype):\n+    @triton.jit\n+    def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.constexpr):\n+        offset1 = tl.arange(0, M)\n+        offset2 = tl.arange(0, N)\n+        x = tl.load(x_ptr + N * offset1[:, None] + offset2[None, :])\n+        y = tl.load(y_ptr + offset2)\n+        _, y_broadcasted = tl.broadcast(x, y)\n+        tl.store(y_broadcasted_ptr + N * offset1[:, None] + offset2[None, :], y_broadcasted)\n+\n+    M = 32\n+    N = 64\n+    rs = RandomState(17)\n+    x = numpy_random((M, N), dtype_str=dtype, rs=rs)\n+    y = numpy_random(N, dtype_str=dtype, rs=rs)\n+    _, y_broadcasted_np = np.broadcast_arrays(x, y)\n+\n+    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n+    y_tri = to_triton(y, device='cuda', dst_type=dtype)\n+    y_broadcasted_tri = to_triton(np.empty((M, N), dtype=y_broadcasted_np.dtype), device='cuda', dst_type=dtype)\n+\n+    broadcast_kernel[(1,)](x_tri, y_tri, y_broadcasted_tri, M=M, N=N)\n+    assert (y_broadcasted_np == to_numpy(y_broadcasted_tri)).all()\n+\n+\n # ---------------\n # test where\n # ---------------\n@@ -514,7 +543,7 @@ def test_unary_op(dtype_x, expr, device='cuda'):\n # ----------------\n \n \n-@pytest.mark.parametrize(\"dtype_x, expr\", [(dtype_x, expr) for dtype_x in float_dtypes for expr in ['exp', 'log', 'cos', 'sin']])\n+@pytest.mark.parametrize(\"dtype_x, expr\", [(dtype_x, expr) for dtype_x in [\"float32\", \"float64\"] for expr in ['exp', 'log', 'cos', 'sin']])\n def test_math_op(dtype_x, expr, device='cuda'):\n     _test_unary(dtype_x, f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n \n@@ -1345,7 +1374,7 @@ def kernel(X, stride_xm, stride_xk,\n         if DO_SOFTMAX:\n             max = tl.max(z, 1)\n             z = z - max[:, None]\n-            num = tl.exp(z)\n+            num = tl.exp(z.to(tl.float32)).to(max.dtype)\n             den = tl.sum(num, 1)\n             z = num / den[:, None]\n         if CHAIN_DOT:"}, {"filename": "python/test/unit/operators/test_blocksparse.py", "status": "modified", "additions": 8, "deletions": 11, "changes": 19, "file_content_changes": "@@ -81,17 +81,14 @@ def test_matmul(MODE, TRANS_A, TRANS_B, BLOCK, DTYPE, Z=3, H=2, M=512, N=384, K=\n     a_tri.retain_grad()\n     b_tri.retain_grad()\n     op = triton.ops.blocksparse.matmul(layout, BLOCK, MODE, trans_a=TRANS_A, trans_b=TRANS_B, device=\"cuda\")\n-    try:\n-        c_tri = op(a_tri, b_tri)\n-        c_tri.backward(dc_tri)\n-        da_tri = a_tri.grad\n-        db_tri = b_tri.grad\n-        # compare\n-        torch.testing.assert_allclose(c_ref, c_tri)\n-        torch.testing.assert_allclose(da_ref, da_tri)\n-        torch.testing.assert_allclose(db_ref, db_tri)\n-    except triton.OutOfResourcesError as e:\n-        pytest.skip(str(e))\n+    c_tri = op(a_tri, b_tri)\n+    c_tri.backward(dc_tri)\n+    da_tri = a_tri.grad\n+    db_tri = b_tri.grad\n+    # compare\n+    torch.testing.assert_allclose(c_ref, c_tri)\n+    torch.testing.assert_allclose(da_ref, da_tri)\n+    torch.testing.assert_allclose(db_ref, db_tri)\n \n \n configs = ["}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 38, "deletions": 22, "changes": 60, "file_content_changes": "@@ -17,7 +17,7 @@\n import triton._C.libtriton.triton as _triton\n # TODO: runtime.errors\n from ..runtime.autotuner import OutOfResources\n-from ..runtime.cache import CacheManager\n+from ..runtime.cache import get_cache_manager\n from ..runtime.driver import get_cuda_utils, get_hip_utils\n from ..tools.disasm import extract\n from .code_generator import ast_to_ttir\n@@ -410,7 +410,7 @@ def compile(fn, **kwargs):\n     # cache manager\n     so_path = make_stub(name, signature, constants)\n     # create cache manager\n-    fn_cache_manager = CacheManager(make_hash(fn, **kwargs))\n+    fn_cache_manager = get_cache_manager(make_hash(fn, **kwargs))\n     # determine name and extension type of provided function\n     if isinstance(fn, triton.runtime.JITFunction):\n         name, ext = fn.__name__, \"ast\"\n@@ -419,14 +419,22 @@ def compile(fn, **kwargs):\n \n     # load metadata if any\n     metadata = None\n-    if fn_cache_manager.has_file(f'{name}.json'):\n-        with open(fn_cache_manager._make_path(f\"{name}.json\")) as f:\n+    metadata_filename = f\"{name}.json\"\n+\n+    # The group is addressed by the metadata\n+    metadata_group = fn_cache_manager.get_group(\n+        metadata_filename\n+    ) or {}\n+\n+    metadata_path = metadata_group.get(metadata_filename)\n+\n+    if metadata_path is not None:\n+        with open(metadata_path) as f:\n             metadata = json.load(f)\n     else:\n         metadata = {\"num_warps\": num_warps,\n                     \"num_stages\": num_stages,\n                     \"constants\": _get_jsonable_constants(constants),\n-                    \"ctime\": dict(),\n                     \"debug\": debug}\n         if ext == \"ptx\":\n             assert \"shared\" in kwargs, \"ptx compilation must provide shared memory size\"\n@@ -437,25 +445,30 @@ def compile(fn, **kwargs):\n     module = fn\n     # run compilation pipeline  and populate metadata\n     for ir, (parse, compile_kernel) in list(stages.items())[first_stage:]:\n-        path = fn_cache_manager._make_path(f\"{name}.{ir}\")\n+        ir_filename = f\"{name}.{ir}\"\n+\n         if ir == ext:\n             next_module = parse(fn)\n-        elif os.path.exists(path) and\\\n-                ir in metadata[\"ctime\"] and\\\n-                os.path.getctime(path) == metadata[\"ctime\"][ir]:\n-            if ir == \"amdgcn\":\n-                next_module = (parse(path), parse(fn_cache_manager._make_path(f\"{name}.hsaco_path\")))\n-            else:\n-                next_module = parse(path)\n         else:\n-            next_module = compile_kernel(module)\n-            if ir == \"amdgcn\":\n-                fn_cache_manager.put(next_module[0], f\"{name}.{ir}\")\n-                fn_cache_manager.put(next_module[1], f\"{name}.hsaco_path\")\n+            path = metadata_group.get(ir_filename)\n+            if path is None:\n+                next_module = compile_kernel(module)\n+                if ir == \"amdgcn\":\n+                    extra_file_name = f\"{name}.hsaco_path\"\n+                    metadata_group[ir_filename] = fn_cache_manager.put(next_module[0], ir_filename)\n+                    metadata_group[extra_file_name] = fn_cache_manager.put(next_module[1], extra_file_name)\n+                else:\n+                    metadata_group[ir_filename] = fn_cache_manager.put(next_module, ir_filename)\n+                    fn_cache_manager.put(next_module, ir_filename)\n             else:\n-                fn_cache_manager.put(next_module, f\"{name}.{ir}\")\n-        if os.path.exists(path):\n-            metadata[\"ctime\"][ir] = os.path.getctime(path)\n+                if ir == \"amdgcn\":\n+                    extra_file_name = f\"{name}.hsaco_path\"\n+                    hasco_path = metadata_group.get(extra_file_name)\n+                    assert hasco_path is not None, \"Expected to have hsaco_path in metadata when we have the amdgcn\"\n+                    next_module = (parse(path), parse(hasco_path))\n+                else:\n+                    next_module = parse(path)\n+\n         if ir == \"cubin\":\n             asm[ir] = next_module\n         elif ir == \"amdgcn\":\n@@ -470,8 +483,11 @@ def compile(fn, **kwargs):\n             metadata[\"name\"] = get_kernel_name(next_module[0], pattern='.globl')\n             asm[\"hsaco_path\"] = next_module[1]\n         module = next_module\n-    # write-back metadata\n-    fn_cache_manager.put(json.dumps(metadata), f\"{name}.json\", binary=False)\n+    # write-back metadata, if it didn't come from the cache\n+    if metadata_path is None:\n+        metadata_group[metadata_filename] = fn_cache_manager.put(json.dumps(metadata), metadata_filename, binary=False)\n+        fn_cache_manager.put_group(metadata_filename, metadata_group)\n+\n     # return handle to compiled kernel\n     return CompiledKernel(fn, so_path, metadata, asm)\n "}, {"filename": "python/triton/compiler/make_launcher.py", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "file_content_changes": "@@ -3,7 +3,7 @@\n import tempfile\n \n from ..common import _build\n-from ..runtime.cache import CacheManager\n+from ..runtime.cache import get_cache_manager\n from ..runtime.jit import version_key\n \n \n@@ -26,19 +26,21 @@ def make_so_cache_key(version_hash, signature, constants):\n def make_stub(name, signature, constants):\n     # name of files that are cached\n     so_cache_key = make_so_cache_key(version_key(), signature, constants)\n-    so_cache_manager = CacheManager(so_cache_key)\n+    so_cache_manager = get_cache_manager(so_cache_key)\n     so_name = f\"{name}.so\"\n     # retrieve stub from cache if it exists\n-    if not so_cache_manager.has_file(so_name):\n+    cache_path = so_cache_manager.get_file(so_name)\n+    if cache_path is None:\n         with tempfile.TemporaryDirectory() as tmpdir:\n             src = generate_launcher(constants, signature)\n             src_path = os.path.join(tmpdir, \"main.c\")\n             with open(src_path, \"w\") as f:\n                 f.write(src)\n             so = _build(name, src_path, tmpdir)\n             with open(so, \"rb\") as f:\n-                so_cache_manager.put(f.read(), so_name, binary=True)\n-    return so_cache_manager._make_path(so_name)\n+                return so_cache_manager.put(f.read(), so_name, binary=True)\n+    else:\n+        return cache_path\n \n # ----- source code generation --------\n "}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -13,9 +13,6 @@\n \n TRITON_MAX_TENSOR_NUMEL = 131072\n \n-\n-T = TypeVar(\"T\")\n-\n TRITON_BUILTIN = \"__triton_builtin__\"\n \n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 35, "deletions": 3, "changes": 38, "file_content_changes": "@@ -1,12 +1,16 @@\n from __future__ import annotations  # remove after python 3.11\n \n-from typing import List, Optional, Sequence, Tuple\n+from functools import wraps\n+from typing import List, Optional, Sequence, Tuple, TypeVar\n \n from . import core as tl\n from triton._C.libtriton.triton import ir\n \n+T = TypeVar('T')\n \n # Create custom exception that prints message \"hello\"\n+\n+\n class IncompatibleTypeErrorImpl(Exception):\n     def __init__(self, type_a, type_b):\n         self.type_a = type_a\n@@ -599,13 +603,13 @@ def broadcast_impl_value(lhs: tl.tensor,\n         if len(lhs_shape) < len(rhs_shape):\n             # Add new axes to lhs\n             for dim in range(len(lhs_shape), len(rhs_shape)):\n-                lhs = tl.tensor(builder.create_expand_dims(lhs.handle, dim), tl.block_type(lhs_ty.scalar, lhs_shape + [1]))\n+                lhs = tl.tensor(builder.create_expand_dims(lhs.handle, 0), tl.block_type(lhs_ty.scalar, [1] + lhs_shape))\n                 lhs_ty = lhs.type\n                 lhs_shape = lhs_ty.get_block_shapes()\n         elif len(rhs_shape) < len(lhs_shape):\n             # Add new axes to rhs\n             for dim in range(len(rhs_shape), len(lhs_shape)):\n-                rhs = tl.tensor(builder.create_expand_dims(rhs.handle, dim), tl.block_type(rhs_ty.scalar, rhs_shape + [1]))\n+                rhs = tl.tensor(builder.create_expand_dims(rhs.handle, 0), tl.block_type(rhs_ty.scalar, [1] + rhs_shape))\n                 rhs_ty = rhs.type\n                 rhs_shape = rhs_ty.get_block_shapes()\n         assert len(rhs_shape) == len(lhs_shape)\n@@ -1260,35 +1264,63 @@ def wrap_tensor(x, scalar_ty):\n #                               Math\n # ===----------------------------------------------------------------------===\n \n+def _check_dtype(dtypes: List[str]) -> T:\n+    \"\"\"\n+    We following libdevice's convention to check accepted data types for math functions.\n+    It is not a good practice to support all data types as accelerators/GPUs don't support\n+    many float16 and bfloat16 math operations.\n+    We should let the users know that they are using and invoke explicit cast to convert\n+    the data type to the supported one.\n+    \"\"\"\n+    def wrapper(fn):\n+        @wraps(fn)\n+        def check(*args, **kwargs):\n+            # concatenate args and kwargs\n+            all_args = list(args) + list(kwargs.values())\n+            for arg in [a for a in all_args if isinstance(a, tl.tensor)]:\n+                if arg.type.scalar.name not in dtypes:\n+                    raise ValueError(f\"Expected dtype {dtypes} but got {arg.type.scalar.name}\")\n+            return fn(*args, **kwargs)\n+        return check\n+\n+    return wrapper\n+\n+\n def umulhi(x: tl.tensor, y: tl.tensor, builder: ir.builder) -> tl.tensor:\n     x, y = binary_op_type_checking_impl(x, y, builder)\n     # FIXME(Keren): not portable, should be fixed\n     from . import math\n     return math.mulhi(x, y, _builder=builder)\n \n \n+@_check_dtype(dtypes=[\"fp32\", \"fp64\"])\n def floor(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     # FIXME(Keren): not portable, should be fixed\n     from . import math\n     return math.floor(x, _builder=builder)\n \n \n+@_check_dtype(dtypes=[\"fp32\", \"fp64\"])\n def exp(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_exp(x.handle), x.type)\n \n \n+@_check_dtype(dtypes=[\"fp32\", \"fp64\"])\n def log(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_log(x.handle), x.type)\n \n \n+@_check_dtype(dtypes=[\"fp32\", \"fp64\"])\n def cos(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_cos(x.handle), x.type)\n \n \n+@_check_dtype(dtypes=[\"fp32\", \"fp64\"])\n def sin(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_sin(x.handle), x.type)\n \n \n+@_check_dtype(dtypes=[\"fp32\", \"fp64\"])\n def sqrt(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_sqrt(x.handle), x.type)\n "}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -60,7 +60,7 @@ def _fwd_kernel(\n         l_curr = tl.sum(p, 1) + l_prev\n         # rescale operands of matmuls\n         l_rcp = 1. / l_curr\n-        p *= l_rcp\n+        p *= l_rcp[:, None]\n         acc *= (l_prev * l_rcp)[:, None]\n         # update acc\n         p = p.to(Q.dtype.element_ty)"}, {"filename": "python/triton/runtime/__init__.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -1,5 +1,6 @@\n from . import driver\n-from .autotuner import Config, Heuristics, OutOfResources, autotune, heuristics\n+from .autotuner import (Autotuner, Config, Heuristics, OutOfResources, autotune,\n+                        heuristics)\n from .jit import (JITFunction, KernelInterface, MockTensor, TensorWrapper, reinterpret,\n                   version_key)\n \n@@ -16,4 +17,5 @@\n     \"TensorWrapper\",\n     \"OutOfResources\",\n     \"MockTensor\",\n+    \"Autotuner\",\n ]"}, {"filename": "python/triton/runtime/cache.py", "status": "modified", "additions": 85, "deletions": 3, "changes": 88, "file_content_changes": "@@ -1,5 +1,8 @@\n+import json\n import os\n+from abc import ABC, abstractmethod\n from pathlib import Path\n+from typing import Dict, Optional\n \n from filelock import FileLock\n \n@@ -8,8 +11,32 @@ def default_cache_dir():\n     return os.path.join(Path.home(), \".triton\", \"cache\")\n \n \n-class CacheManager:\n+class CacheManager(ABC):\n+    def __init__(self, key):\n+        pass\n+\n+    @abstractmethod\n+    def get_file(self, filename) -> Optional[str]:\n+        pass\n+\n+    @abstractmethod\n+    def has_file(self, filename) -> bool:\n+        pass\n+\n+    @abstractmethod\n+    def put(self, data, filename, binary=True) -> str:\n+        pass\n+\n+    @abstractmethod\n+    def get_group(self, filename: str) -> Optional[Dict[str, str]]:\n+        pass\n \n+    @abstractmethod\n+    def put_group(self, filename: str, group: Dict[str, str]):\n+        pass\n+\n+\n+class FileCacheManager(CacheManager):\n     def __init__(self, key):\n         self.key = key\n         self.lock_path = None\n@@ -20,15 +47,48 @@ def __init__(self, key):\n             self.lock_path = os.path.join(self.cache_dir, \"lock\")\n             os.makedirs(self.cache_dir, exist_ok=True)\n \n-    def _make_path(self, filename):\n+    def _make_path(self, filename) -> str:\n         return os.path.join(self.cache_dir, filename)\n \n     def has_file(self, filename):\n         if not self.cache_dir:\n             return False\n         return os.path.exists(self._make_path(filename))\n \n-    def put(self, data, filename, binary=True):\n+    def get_file(self, filename) -> Optional[str]:\n+        if self.has_file(filename):\n+            return self._make_path(filename)\n+        else:\n+            return None\n+\n+    def get_group(self, filename: str) -> Optional[Dict[str, str]]:\n+        grp_filename = f\"__grp__{filename}\"\n+        if not self.has_file(grp_filename):\n+            return None\n+        grp_filepath = self._make_path(grp_filename)\n+        with open(grp_filepath) as f:\n+            grp_data = json.load(f)\n+        child_paths = grp_data.get(\"child_paths\", None)\n+        # Invalid group data.\n+        if child_paths is None:\n+            return None\n+        result = {}\n+        for c in child_paths:\n+            p = self._make_path(c)\n+            if not os.path.exists(p):\n+                raise Exception(f\"Group file {p} does not exist from group {grp_filename} \")\n+            result[c] = p\n+        return result\n+\n+    # Note a group of pushed files as being part of a group\n+    def put_group(self, filename: str, group: Dict[str, str]):\n+        if not self.cache_dir:\n+            return\n+        grp_contents = json.dumps({\"child_paths\": sorted(list(group.keys()))})\n+        grp_filename = f\"__grp__{filename}\"\n+        return self.put(grp_contents, grp_filename, binary=False)\n+\n+    def put(self, data, filename, binary=True) -> str:\n         if not self.cache_dir:\n             return\n         binary = isinstance(data, bytes)\n@@ -42,3 +102,25 @@ def put(self, data, filename, binary=True):\n             with open(filepath + \".tmp\", mode) as f:\n                 f.write(data)\n             os.rename(filepath + \".tmp\", filepath)\n+        return filepath\n+\n+\n+__cache_cls = FileCacheManager\n+__cache_cls_nme = \"DEFAULT\"\n+\n+\n+def get_cache_manager(key) -> CacheManager:\n+    import os\n+\n+    user_cache_manager = os.environ.get(\"TRITON_CACHE_MANAGER\", None)\n+    global __cache_cls\n+    global __cache_cls_nme\n+\n+    if user_cache_manager is not None and user_cache_manager != __cache_cls_nme:\n+        import importlib\n+        module_path, clz_nme = user_cache_manager.split(\":\")\n+        module = importlib.import_module(module_path)\n+        __cache_cls = getattr(module, clz_nme)\n+        __cache_cls_nme = user_cache_manager\n+\n+    return __cache_cls(key)"}, {"filename": "python/triton/runtime/driver/cuda.py", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -3,7 +3,7 @@\n import tempfile\n \n from ...common.build import _build\n-from ..cache import CacheManager\n+from ..cache import get_cache_manager\n \n \n def get_cuda_utils():\n@@ -140,18 +140,19 @@ def _generate_src():\n     def __init__(self):\n         src = self._generate_src()\n         key = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n-        cache = CacheManager(key)\n+        cache = get_cache_manager(key)\n         fname = \"cuda_utils.so\"\n-        if not cache.has_file(fname):\n+        cache_path = cache.get_file(fname)\n+        if cache_path is None:\n             with tempfile.TemporaryDirectory() as tmpdir:\n                 src_path = os.path.join(tmpdir, \"main.c\")\n                 with open(src_path, \"w\") as f:\n                     f.write(src)\n                 so = _build(\"cuda_utils\", src_path, tmpdir)\n                 with open(so, \"rb\") as f:\n-                    cache.put(f.read(), fname, binary=True)\n+                    cache_path = cache.put(f.read(), fname, binary=True)\n         import importlib.util\n-        spec = importlib.util.spec_from_file_location(\"cuda_utils\", cache._make_path(fname))\n+        spec = importlib.util.spec_from_file_location(\"cuda_utils\", cache_path)\n         mod = importlib.util.module_from_spec(spec)\n         spec.loader.exec_module(mod)\n         self.load_binary = mod.load_binary"}, {"filename": "python/triton/runtime/driver/hip.py", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -3,7 +3,7 @@\n import tempfile\n \n from ...common.build import _build\n-from ..cache import CacheManager\n+from ..cache import get_cache_manager\n \n \n def get_hip_utils():\n@@ -139,18 +139,19 @@ def _generate_src(self):\n     def __init__(self):\n         src = self._generate_src()\n         key = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n-        cache = CacheManager(key)\n+        cache = get_cache_manager(key)\n         fname = \"hip_utils.so\"\n-        if not cache.has_file(fname):\n+        cache_path = cache.get_file(fname)\n+        if cache_path is None:\n             with tempfile.TemporaryDirectory() as tmpdir:\n                 src_path = os.path.join(tmpdir, \"main.c\")\n                 with open(src_path, \"w\") as f:\n                     f.write(src)\n                 so = _build(\"hip_utils\", src_path, tmpdir)\n                 with open(so, \"rb\") as f:\n-                    cache.put(f.read(), fname, binary=True)\n+                    cache_path = cache.put(f.read(), fname, binary=True)\n         import importlib.util\n-        spec = importlib.util.spec_from_file_location(\"hip_utils\", cache._make_path(fname))\n+        spec = importlib.util.spec_from_file_location(\"hip_utils\", cache_path)\n         mod = importlib.util.module_from_spec(spec)\n         spec.loader.exec_module(mod)\n         self.load_binary = mod.load_binary"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 37, "deletions": 0, "changes": 37, "file_content_changes": "@@ -89,6 +89,43 @@ def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n         return torch.mean(times).item()\n \n \n+def assert_close(x, y, atol=None, rtol=None, err_msg=''):\n+    import numpy as np\n+    import torch\n+\n+    # canonicalize arguments to be tensors\n+    if not isinstance(x, torch.Tensor):\n+        x = torch.tensor(x)\n+    if not isinstance(y, torch.Tensor):\n+        y = torch.tensor(y)\n+    # absolute tolerance\n+    if atol is None:\n+        atol = 1e-2\n+    atol = atol(x.dtype) if callable(atol) else atol\n+    # relative tolerance hook\n+    if rtol is None:\n+        rtol = 0.\n+    rtol = rtol(x.dtype) if callable(rtol) else rtol\n+    # we use numpy instead of pytorch\n+    # as it seems more memory efficient\n+    # pytorch tends to oom on large tensors\n+    if isinstance(x, torch.Tensor):\n+        if x.dtype == torch.bfloat16:\n+            x = x.float()\n+        x = x.cpu().detach().numpy()\n+    if isinstance(y, torch.Tensor):\n+        if y.dtype == torch.bfloat16:\n+            y = y.float()\n+        y = y.cpu().detach().numpy()\n+    # we handle size==1 case separately as we can\n+    # provide better error message there\n+    if x.size > 1 or y.size > 1:\n+        np.testing.assert_allclose(x, y, atol=atol, rtol=rtol, equal_nan=True)\n+        return\n+    if not np.allclose(x, y, atol=atol, rtol=rtol):\n+        raise AssertionError(f'{err_msg} {x} is not close to {y} (atol={atol}, rtol={rtol})')\n+\n+\n class Benchmark:\n     \"\"\"\n     This class is used by the :code:`perf_report` function to generate line plots with a concise API."}]