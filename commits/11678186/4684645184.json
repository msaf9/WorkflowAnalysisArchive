[{"filename": "python/triton/ops/matmul_perf_model.py", "status": "modified", "additions": 5, "deletions": 9, "changes": 14, "file_content_changes": "@@ -4,24 +4,22 @@\n \n import triton\n import triton._C.libtriton.triton as _triton\n-from triton.runtime.driver.cuda import get_cuda_utils\n+from triton.runtime import driver\n from triton.testing import get_dram_gbps, get_max_simd_tflops, get_max_tensorcore_tflops\n \n \n def get_tensorcore_tflops(backend, device, num_ctas, num_warps, dtype):\n     ''' return compute throughput in TOPS '''\n     total_warps = num_ctas * min(num_warps, 4)\n-    cuda_utils = get_cuda_utils()\n-    num_subcores = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n+    num_subcores = driver.utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n     tflops = min(num_subcores, total_warps) / num_subcores * get_max_tensorcore_tflops(dtype, backend, device)\n     return tflops\n \n \n def get_simd_tflops(backend, device, num_ctas, num_warps, dtype):\n     ''' return compute throughput in TOPS '''\n     total_warps = num_ctas * min(num_warps, 4)\n-    cuda_utils = get_cuda_utils()\n-    num_subcores = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n+    num_subcores = driver.utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n     tflops = min(num_subcores, total_warps) / num_subcores * get_max_simd_tflops(dtype, backend, device)\n     return tflops\n \n@@ -62,8 +60,7 @@ def estimate_matmul_time(\n     compute_ms = total_ops / tput\n \n     # time to load data\n-    cuda_utils = get_cuda_utils()\n-    num_sm = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"]\n+    num_sm = driver.utils.get_device_properties(device)[\"multiprocessor_count\"]\n     active_cta_ratio = min(1, num_ctas / num_sm)\n     active_cta_ratio_bw1 = min(1, num_ctas / 32)  # 32 active ctas are enough to saturate\n     active_cta_ratio_bw2 = max(min(1, (num_ctas - 32) / (108 - 32)), 0)  # 32-108, remaining 5%\n@@ -115,8 +112,7 @@ def early_config_prune(configs, named_args):\n             kw['BLOCK_M'], kw['BLOCK_N'], kw['BLOCK_K'], config.num_stages\n \n         # TODO: move to `cuda_utils` submodule\n-        cuda_utils = get_cuda_utils()\n-        max_shared_memory = cuda_utils.get_device_properties(device)[\"max_shared_mem\"]\n+        max_shared_memory = driver.utils.cuda_utils.get_device_properties(device)[\"max_shared_mem\"]\n         required_shared_memory = (BLOCK_M + BLOCK_N) * BLOCK_K * num_stages * dtsize\n         if required_shared_memory <= max_shared_memory:\n             pruned_configs.append(config)"}]