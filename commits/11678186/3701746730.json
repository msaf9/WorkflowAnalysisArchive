[{"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -710,7 +710,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n \n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n #shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n-#mma0 = #triton_gpu.mma<{version=2, warpsPerCTA=[1,1]}>\n+#mma0 = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[1,1]}>\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n@@ -923,7 +923,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // -----\n \n-#mma = #triton_gpu.mma<{version=2, warpsPerCTA=[2, 2]}>\n+#mma = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[2, 2]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>"}]