[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 7, "deletions": 8, "changes": 15, "file_content_changes": "@@ -53,9 +53,6 @@ void storeBlockedToShared(Value src, Value llSrc, ArrayRef<Value> srcStrides,\n   auto wordTy = vec_ty(elemTy, minVec);\n   auto elemPtrTy = ptr_ty(elemTy);\n \n-  // TODO: [goostavz] We should make a cache for the calculation of\n-  // emitBaseIndexForBlockedLayout in case backend compiler not being able to\n-  // optimize that\n   SmallVector<unsigned> srcShapePerCTA = getShapePerCTA(srcBlockedLayout);\n   SmallVector<unsigned> reps{ceil<unsigned>(srcShape[0], srcShapePerCTA[0]),\n                              ceil<unsigned>(srcShape[1], srcShapePerCTA[1])};\n@@ -182,7 +179,7 @@ struct ConvertLayoutOpConversion\n     unsigned rank = shape.size();\n     if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n       auto multiDimOffsetFirstElem =\n-          emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n+          emitBaseIndexForLayout(loc, rewriter, blockedLayout, shape);\n       SmallVector<Value> multiDimOffset(rank);\n       SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n           elemId, getSizePerThread(layout), getOrder(layout));\n@@ -501,8 +498,8 @@ struct ConvertLayoutOpConversion\n \n     auto srcStrides =\n         getStridesFromShapeAndOrder(srcShape, inOrd, loc, rewriter);\n-    auto srcIndices = emitBaseIndexForBlockedLayout(loc, rewriter,\n-                                                    srcBlockedLayout, srcShape);\n+    auto srcIndices =\n+        emitBaseIndexForLayout(loc, rewriter, srcBlockedLayout, srcShape);\n     storeBlockedToShared(src, adaptor.src(), srcStrides, srcIndices, dst,\n                          smemBase, elemTy, loc, rewriter);\n \n@@ -680,7 +677,9 @@ struct ConvertLayoutOpConversion\n void populateConvertLayoutOpToLLVMPatterns(\n     mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem, PatternBenefit benefit) {\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n-                                          benefit);\n+                                          indexCacheInfo, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.h", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -19,6 +19,8 @@ void storeBlockedToShared(Value src, Value llSrc, ArrayRef<Value> srcStrides,\n void populateConvertLayoutOpToLLVMPatterns(\n     mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem, PatternBenefit benefit);\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 16, "deletions": 15, "changes": 31, "file_content_changes": "@@ -14,7 +14,7 @@ using ::mlir::triton::gpu::getElemsPerThread;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n // Contains some helper functions for both Load and Store conversions.\n-struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n+struct LoadStoreConversionBase {\n   explicit LoadStoreConversionBase(AxisInfoAnalysis &axisAnalysisPass)\n       : axisAnalysisPass(axisAnalysisPass) {}\n \n@@ -640,7 +640,7 @@ struct InsertSliceOpConversion\n \n     auto llSrc = adaptor.source();\n     auto srcIndices =\n-        emitBaseIndexForBlockedLayout(loc, rewriter, srcLayout, srcShape);\n+        emitBaseIndexForLayout(loc, rewriter, srcLayout, srcShape);\n     storeBlockedToShared(src, llSrc, srcStrides, srcIndices, dst, smemBase,\n                          elemTy, loc, rewriter);\n     // Barrier is not necessary.\n@@ -657,12 +657,12 @@ struct InsertSliceAsyncOpConversion\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::gpu::InsertSliceAsyncOp>::ConvertTritonGPUOpToLLVMPattern;\n \n-  InsertSliceAsyncOpConversion(LLVMTypeConverter &converter,\n-                               const Allocation *allocation, Value smem,\n-                               AxisInfoAnalysis &axisAnalysisPass,\n-                               PatternBenefit benefit)\n+  InsertSliceAsyncOpConversion(\n+      LLVMTypeConverter &converter, const Allocation *allocation, Value smem,\n+      ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+      AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::gpu::InsertSliceAsyncOp>(\n-            converter, allocation, smem, benefit),\n+            converter, allocation, smem, indexCacheInfo, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n \n   LogicalResult\n@@ -865,20 +865,21 @@ struct InsertSliceAsyncOpConversion\n   }\n };\n \n-void populateLoadStoreOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                       RewritePatternSet &patterns,\n-                                       int numWarps,\n-                                       AxisInfoAnalysis &axisInfoAnalysis,\n-                                       const Allocation *allocation, Value smem,\n-                                       PatternBenefit benefit) {\n+void populateLoadStoreOpToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n   patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<StoreOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<AtomicCASOpConversion>(typeConverter, allocation, smem,\n                                       axisInfoAnalysis, benefit);\n   patterns.add<AtomicRMWOpConversion>(typeConverter, allocation, smem,\n                                       axisInfoAnalysis, benefit);\n   patterns.add<InsertSliceOpConversion>(typeConverter, allocation, smem,\n-                                        benefit);\n+                                        indexCacheInfo, benefit);\n   patterns.add<InsertSliceAsyncOpConversion>(typeConverter, allocation, smem,\n-                                             axisInfoAnalysis, benefit);\n+                                             indexCacheInfo, axisInfoAnalysis,\n+                                             benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.h", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -6,11 +6,11 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-void populateLoadStoreOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                       RewritePatternSet &patterns,\n-                                       int numWarps,\n-                                       AxisInfoAnalysis &axisInfoAnalysis,\n-                                       const Allocation *allocation, Value smem,\n-                                       PatternBenefit benefit);\n+void populateLoadStoreOpToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 9, "deletions": 7, "changes": 16, "file_content_changes": "@@ -164,7 +164,7 @@ struct ReduceOpConversion\n     auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n \n     SmallVector<SmallVector<unsigned>> offset =\n-        emitOffsetForBlockedLayout(srcLayout, srcShape);\n+        emitOffsetForLayout(srcLayout, srcShape);\n \n     std::map<SmallVector<unsigned>, Value> accs;\n     std::map<SmallVector<unsigned>, Value> accIndices;\n@@ -479,10 +479,12 @@ struct ReduceOpConversion\n   }\n };\n \n-void populateReduceOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                    RewritePatternSet &patterns, int numWarps,\n-                                    AxisInfoAnalysis &axisInfoAnalysis,\n-                                    const Allocation *allocation, Value smem,\n-                                    PatternBenefit benefit) {\n-  patterns.add<ReduceOpConversion>(typeConverter, allocation, smem, benefit);\n+void populateReduceOpToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n+  patterns.add<ReduceOpConversion>(typeConverter, allocation, smem,\n+                                   indexCacheInfo, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.h", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -6,10 +6,11 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-void populateReduceOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                    RewritePatternSet &patterns, int numWarps,\n-                                    AxisInfoAnalysis &axisInfoAnalysis,\n-                                    const Allocation *allocation, Value smem,\n-                                    PatternBenefit benefit);\n+void populateReduceOpToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n \n #endif\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 15, "deletions": 9, "changes": 24, "file_content_changes": "@@ -63,6 +63,7 @@ struct BroadcastOpConversion\n     auto srcShape = srcTy.getShape();\n     auto resultShape = resultTy.getShape();\n     unsigned rank = srcTy.getRank();\n+\n     assert(rank == resultTy.getRank());\n     auto order = triton::gpu::getOrder(srcLayout);\n     auto srcOffsets = emitOffsetForLayout(srcLayout, srcShape);\n@@ -272,9 +273,13 @@ struct PrintfOpConversion\n struct MakeRangeOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp> {\n \n-  MakeRangeOpConversion(LLVMTypeConverter &converter, PatternBenefit benefit)\n-      : ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp>(converter,\n-                                                             benefit) {}\n+  MakeRangeOpConversion(\n+      LLVMTypeConverter &converter,\n+      ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+      PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp>(\n+            converter, /*Allocation*/ nullptr, Value{}, indexCacheInfo,\n+            benefit) {}\n \n   LogicalResult\n   matchAndRewrite(triton::MakeRangeOp op, OpAdaptor adaptor,\n@@ -500,11 +505,12 @@ struct AsyncWaitOpConversion\n   }\n };\n \n-void populateTritonGPUToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                     RewritePatternSet &patterns, int numWarps,\n-                                     AxisInfoAnalysis &axisInfoAnalysis,\n-                                     const Allocation *allocation, Value smem,\n-                                     PatternBenefit benefit) {\n+void populateTritonGPUToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n   patterns.add<AddPtrOpConversion>(typeConverter, benefit);\n   patterns.add<AllocTensorOpConversion>(typeConverter, allocation, smem,\n                                         benefit);\n@@ -515,7 +521,7 @@ void populateTritonGPUToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n   patterns.add<GetNumProgramsOpConversion>(typeConverter, benefit);\n-  patterns.add<MakeRangeOpConversion>(typeConverter, benefit);\n+  patterns.add<MakeRangeOpConversion>(typeConverter, indexCacheInfo, benefit);\n   patterns.add<ReturnOpConversion>(typeConverter, benefit);\n   patterns.add<PrintfOpConversion>(typeConverter, benefit);\n }\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -6,10 +6,11 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-void populateTritonGPUToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                     RewritePatternSet &patterns, int numWarps,\n-                                     AxisInfoAnalysis &axisInfoAnalysis,\n-                                     const Allocation *allocation, Value smem,\n-                                     PatternBenefit benefit);\n+void populateTritonGPUToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 216, "deletions": 105, "changes": 321, "file_content_changes": "@@ -18,7 +18,6 @@ using ::mlir::LLVM::SharedMemoryObject;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;\n-\n // FuncOpConversion/FuncOpConversionBase is borrowed from\n // https://github.com/llvm/llvm-project/blob/fae656b2dd80246c3c6f01e9c77c49560368752c/mlir/lib/Conversion/FuncToLLVM/FuncToLLVM.cpp#L276\n // since it is not exposed on header files in mlir v14\n@@ -128,7 +127,60 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<FuncOp> {\n   }\n };\n \n-struct ConvertTritonGPUOpToLLVMPatternBase {\n+using IndexCacheKeyT = std::pair<Attribute, SmallVector<int64_t>>;\n+\n+struct CacheKeyDenseMapInfo {\n+  static IndexCacheKeyT getEmptyKey() {\n+    auto *pointer = llvm::DenseMapInfo<void *>::getEmptyKey();\n+    return std::make_pair(\n+        mlir::Attribute(static_cast<mlir::Attribute::ImplType *>(pointer)),\n+        SmallVector<int64_t>{});\n+  }\n+  static IndexCacheKeyT getTombstoneKey() {\n+    auto *pointer = llvm::DenseMapInfo<void *>::getTombstoneKey();\n+    return std::make_pair(\n+        mlir::Attribute(static_cast<mlir::Attribute::ImplType *>(pointer)),\n+        SmallVector<int64_t>{std::numeric_limits<int64_t>::max()});\n+  }\n+  static unsigned getHashValue(IndexCacheKeyT key) {\n+    return llvm::hash_combine(\n+        mlir::hash_value(key.first),\n+        llvm::hash_combine_range(key.second.begin(), key.second.end()));\n+  }\n+  static bool isEqual(IndexCacheKeyT LHS, IndexCacheKeyT RHS) {\n+    return LHS == RHS;\n+  }\n+};\n+\n+class ConvertTritonGPUOpToLLVMPatternBase {\n+public:\n+  // Two levels of value cache in emitting indices calculation:\n+  // Key: pair<layout, shape>\n+  struct IndexCacheInfo {\n+    DenseMap<IndexCacheKeyT, SmallVector<Value>, CacheKeyDenseMapInfo>\n+        *baseIndexCache;\n+    DenseMap<IndexCacheKeyT, SmallVector<SmallVector<Value>>,\n+             CacheKeyDenseMapInfo> *indexCache;\n+    OpBuilder::InsertPoint *indexInsertPoint;\n+  };\n+\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(LLVMTypeConverter &typeConverter)\n+      : converter(&typeConverter) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(LLVMTypeConverter &typeConverter,\n+                                               const Allocation *allocation,\n+                                               Value smem)\n+      : converter(&typeConverter), allocation(allocation), smem(smem) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(LLVMTypeConverter &typeConverter,\n+                                               const Allocation *allocation,\n+                                               Value smem,\n+                                               IndexCacheInfo indexCacheInfo)\n+      : converter(&typeConverter), indexCacheInfo(indexCacheInfo),\n+        allocation(allocation), smem(smem) {}\n+\n+  LLVMTypeConverter *getTypeConverter() const { return converter; }\n+\n   static Value\n   getStructFromSharedMemoryObject(Location loc,\n                                   const SharedMemoryObject &smemObj,\n@@ -139,25 +191,6 @@ struct ConvertTritonGPUOpToLLVMPatternBase {\n         LLVM::LLVMStructType::getLiteral(rewriter.getContext(), types);\n     return getStructFromElements(loc, elems, rewriter, structTy);\n   }\n-};\n-\n-template <typename SourceOp>\n-class ConvertTritonGPUOpToLLVMPattern\n-    : public ConvertOpToLLVMPattern<SourceOp>,\n-      public ConvertTritonGPUOpToLLVMPatternBase {\n-public:\n-  using OpAdaptor = typename SourceOp::Adaptor;\n-\n-  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n-                                           PatternBenefit benefit = 1)\n-      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n-\n-  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n-                                           const Allocation *allocation,\n-                                           Value smem,\n-                                           PatternBenefit benefit = 1)\n-      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n-        allocation(allocation), smem(smem) {}\n \n   Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n     auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n@@ -169,6 +202,23 @@ class ConvertTritonGPUOpToLLVMPattern\n     return threadId;\n   }\n \n+  // -----------------------------------------------------------------------\n+  // Shared memory utilities\n+  // -----------------------------------------------------------------------\n+  template <typename T>\n+  Value getSharedMemoryBase(Location loc, ConversionPatternRewriter &rewriter,\n+                            T value) const {\n+\n+    auto ptrTy = LLVM::LLVMPointerType::get(\n+        this->getTypeConverter()->convertType(rewriter.getI8Type()), 3);\n+    auto bufferId = allocation->getBufferId(value);\n+    assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n+    size_t offset = allocation->getOffset(bufferId);\n+    Value offVal = idx_val(offset);\n+    Value base = gep(ptrTy, smem, offVal);\n+    return base;\n+  }\n+\n   // -----------------------------------------------------------------------\n   // Utilities\n   // -----------------------------------------------------------------------\n@@ -242,6 +292,116 @@ class ConvertTritonGPUOpToLLVMPattern\n     return ret;\n   }\n \n+  struct SmallVectorKeyInfo {\n+    static unsigned getHashValue(const SmallVector<unsigned> &key) {\n+      return llvm::hash_combine_range(key.begin(), key.end());\n+    }\n+    static bool isEqual(const SmallVector<unsigned> &lhs,\n+                        const SmallVector<unsigned> &rhs) {\n+      return lhs == rhs;\n+    }\n+    static SmallVector<unsigned> getEmptyKey() {\n+      return SmallVector<unsigned>();\n+    }\n+    static SmallVector<unsigned> getTombstoneKey() {\n+      return {std::numeric_limits<unsigned>::max()};\n+    }\n+  };\n+\n+  // -----------------------------------------------------------------------\n+  // Get offsets / indices for any layout\n+  // -----------------------------------------------------------------------\n+\n+  SmallVector<Value> emitBaseIndexForLayout(Location loc,\n+                                            ConversionPatternRewriter &rewriter,\n+                                            const Attribute &layout,\n+                                            ArrayRef<int64_t> shape) const {\n+    IndexCacheKeyT key = std::make_pair(layout, llvm::to_vector(shape));\n+    auto cache = indexCacheInfo.baseIndexCache;\n+    assert(cache && \"baseIndexCache is nullptr\");\n+    auto insertPt = indexCacheInfo.indexInsertPoint;\n+    if (cache->count(key) > 0) {\n+      return cache->lookup(key);\n+    } else {\n+      ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+      restoreInsertionPointIfSet(insertPt, rewriter);\n+      SmallVector<Value> result;\n+      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+        result =\n+            emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n+      } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+        if (mmaLayout.isVolta())\n+          result = emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, shape);\n+        if (mmaLayout.isAmpere())\n+          result = emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, shape);\n+      } else {\n+        llvm_unreachable(\"unsupported emitBaseIndexForLayout\");\n+      }\n+      cache->insert(std::make_pair(key, result));\n+      *insertPt = rewriter.saveInsertionPoint();\n+      return result;\n+    }\n+  }\n+\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForLayout(const Attribute &layout, ArrayRef<int64_t> shape) const {\n+    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n+      return emitOffsetForBlockedLayout(blockedLayout, shape);\n+    if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+      if (mmaLayout.isVolta())\n+        return emitOffsetForMmaLayoutV1(mmaLayout, shape);\n+      if (mmaLayout.isAmpere())\n+        return emitOffsetForMmaLayoutV2(mmaLayout, shape);\n+    }\n+    llvm_unreachable(\"unsupported emitOffsetForLayout\");\n+  }\n+\n+  // -----------------------------------------------------------------------\n+  // Emit indices\n+  // -----------------------------------------------------------------------\n+  SmallVector<SmallVector<Value>> emitIndices(Location loc,\n+                                              ConversionPatternRewriter &b,\n+                                              const Attribute &layout,\n+                                              ArrayRef<int64_t> shape) const {\n+    IndexCacheKeyT key(layout, llvm::to_vector(shape));\n+    auto cache = indexCacheInfo.indexCache;\n+    assert(cache && \"indexCache is nullptr\");\n+    auto insertPt = indexCacheInfo.indexInsertPoint;\n+    if (cache->count(key) > 0) {\n+      return cache->lookup(key);\n+    } else {\n+      ConversionPatternRewriter::InsertionGuard guard(b);\n+      restoreInsertionPointIfSet(insertPt, b);\n+      SmallVector<SmallVector<Value>> result;\n+      if (auto blocked = layout.dyn_cast<BlockedEncodingAttr>()) {\n+        result = emitIndicesForDistributedLayout(loc, b, blocked, shape);\n+      } else if (auto mma = layout.dyn_cast<MmaEncodingAttr>()) {\n+        result = emitIndicesForDistributedLayout(loc, b, mma, shape);\n+      } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n+        result = emitIndicesForSliceLayout(loc, b, slice, shape);\n+      } else {\n+        llvm_unreachable(\n+            \"emitIndices for layouts other than blocked & slice not \"\n+            \"implemented yet\");\n+      }\n+      cache->insert(std::make_pair(key, result));\n+      *insertPt = b.saveInsertionPoint();\n+      return result;\n+    }\n+  }\n+\n+private:\n+  void restoreInsertionPointIfSet(OpBuilder::InsertPoint *insertPt,\n+                                  ConversionPatternRewriter &rewriter) const {\n+    if (insertPt->isSet()) {\n+      rewriter.restoreInsertionPoint(*insertPt);\n+    } else {\n+      auto func =\n+          rewriter.getInsertionPoint()->getParentOfType<LLVM::LLVMFuncOp>();\n+      rewriter.setInsertionPointToStart(&func.getBody().front());\n+    }\n+  }\n+\n   // -----------------------------------------------------------------------\n   // Blocked layout indices\n   // -----------------------------------------------------------------------\n@@ -411,38 +571,6 @@ class ConvertTritonGPUOpToLLVMPattern\n     return ret;\n   }\n \n-  // -----------------------------------------------------------------------\n-  // Get offsets / indices for any layout\n-  // -----------------------------------------------------------------------\n-\n-  SmallVector<Value> emitBaseIndexForLayout(Location loc,\n-                                            ConversionPatternRewriter &rewriter,\n-                                            const Attribute &layout,\n-                                            ArrayRef<int64_t> shape) const {\n-    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n-      return emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n-    if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-      if (mmaLayout.isVolta())\n-        return emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, shape);\n-      if (mmaLayout.isAmpere())\n-        return emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, shape);\n-    }\n-    llvm_unreachable(\"unsupported emitBaseIndexForLayout\");\n-  }\n-\n-  SmallVector<SmallVector<unsigned>>\n-  emitOffsetForLayout(const Attribute &layout, ArrayRef<int64_t> shape) const {\n-    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n-      return emitOffsetForBlockedLayout(blockedLayout, shape);\n-    if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-      if (mmaLayout.isVolta())\n-        return emitOffsetForMmaLayoutV1(mmaLayout, shape);\n-      if (mmaLayout.isAmpere())\n-        return emitOffsetForMmaLayoutV2(mmaLayout, shape);\n-    }\n-    llvm_unreachable(\"unsupported emitOffsetForLayout\");\n-  }\n-\n   // Emit indices calculation within each ConversionPattern, and returns a\n   // [elemsPerThread X rank] index matrix.\n \n@@ -470,22 +598,6 @@ class ConvertTritonGPUOpToLLVMPattern\n     return multiDimIdx;\n   }\n \n-  struct SmallVectorKeyInfo {\n-    static unsigned getHashValue(const SmallVector<unsigned> &key) {\n-      return llvm::hash_combine_range(key.begin(), key.end());\n-    }\n-    static bool isEqual(const SmallVector<unsigned> &lhs,\n-                        const SmallVector<unsigned> &rhs) {\n-      return lhs == rhs;\n-    }\n-    static SmallVector<unsigned> getEmptyKey() {\n-      return SmallVector<unsigned>();\n-    }\n-    static SmallVector<unsigned> getTombstoneKey() {\n-      return {std::numeric_limits<unsigned>::max()};\n-    }\n-  };\n-\n   SmallVector<SmallVector<Value>>\n   emitIndicesForSliceLayout(Location loc, ConversionPatternRewriter &rewriter,\n                             const SliceEncodingAttr &sliceLayout,\n@@ -505,46 +617,45 @@ class ConvertTritonGPUOpToLLVMPattern\n     return resultIndices;\n   }\n \n-  // -----------------------------------------------------------------------\n-  // Emit indices\n-  // -----------------------------------------------------------------------\n-  SmallVector<SmallVector<Value>> emitIndices(Location loc,\n-                                              ConversionPatternRewriter &b,\n-                                              const Attribute &layout,\n-                                              ArrayRef<int64_t> shape) const {\n-    if (auto blocked = layout.dyn_cast<BlockedEncodingAttr>()) {\n-      return emitIndicesForDistributedLayout(loc, b, blocked, shape);\n-    } else if (auto mma = layout.dyn_cast<MmaEncodingAttr>()) {\n-      return emitIndicesForDistributedLayout(loc, b, mma, shape);\n-    } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n-      return emitIndicesForSliceLayout(loc, b, slice, shape);\n-    } else {\n-      assert(0 && \"emitIndices for layouts other than blocked & slice not \"\n-                  \"implemented yet\");\n-      return {};\n-    }\n-  }\n+protected:\n+  LLVMTypeConverter *converter;\n+  const Allocation *allocation;\n+  Value smem;\n+  IndexCacheInfo indexCacheInfo;\n+};\n \n-  // -----------------------------------------------------------------------\n-  // Shared memory utilities\n-  // -----------------------------------------------------------------------\n-  template <typename T>\n-  Value getSharedMemoryBase(Location loc, ConversionPatternRewriter &rewriter,\n-                            T value) const {\n+template <typename SourceOp>\n+class ConvertTritonGPUOpToLLVMPattern\n+    : public ConvertOpToLLVMPattern<SourceOp>,\n+      public ConvertTritonGPUOpToLLVMPatternBase {\n+public:\n+  using OpAdaptor = typename SourceOp::Adaptor;\n \n-    auto ptrTy = LLVM::LLVMPointerType::get(\n-        this->getTypeConverter()->convertType(rewriter.getI8Type()), 3);\n-    auto bufferId = allocation->getBufferId(value);\n-    assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n-    size_t offset = allocation->getOffset(bufferId);\n-    Value offVal = idx_val(offset);\n-    Value base = gep(ptrTy, smem, offVal);\n-    return base;\n-  }\n+  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n+                                           PatternBenefit benefit = 1)\n+      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n+                                           const Allocation *allocation,\n+                                           Value smem,\n+                                           PatternBenefit benefit = 1)\n+      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation, smem) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n+                                           const Allocation *allocation,\n+                                           Value smem,\n+                                           IndexCacheInfo indexCacheInfo,\n+                                           PatternBenefit benefit = 1)\n+      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation, smem,\n+                                            indexCacheInfo) {}\n \n protected:\n-  const Allocation *allocation;\n-  Value smem;\n+  LLVMTypeConverter *getTypeConverter() const {\n+    return ((ConvertTritonGPUOpToLLVMPatternBase *)this)->getTypeConverter();\n+  }\n };\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 15, "deletions": 4, "changes": 19, "file_content_changes": "@@ -170,16 +170,20 @@ class ConvertTritonGPUToLLVM\n     // We set a higher benefit here to ensure triton's patterns runs before\n     // arith patterns for some encoding not supported by the community\n     // patterns.\n+    OpBuilder::InsertPoint indexInsertPoint;\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo indexCacheInfo{\n+        &baseIndexCache, &indexCache, &indexInsertPoint};\n+\n     RewritePatternSet patterns(context);\n \n     // Normal conversions\n     populateTritonGPUToLLVMPatterns(typeConverter, patterns, numWarps,\n                                     axisInfoAnalysis, &allocation, smem,\n-                                    /*benefit=*/10);\n+                                    indexCacheInfo, /*benefit=*/10);\n     // ConvertLayoutOp\n     populateConvertLayoutOpToLLVMPatterns(typeConverter, patterns, numWarps,\n                                           axisInfoAnalysis, &allocation, smem,\n-                                          /*benefit=*/10);\n+                                          indexCacheInfo, /*benefit=*/10);\n     // DotOp\n     populateDotOpToLLVMPatterns(typeConverter, patterns, numWarps,\n                                 axisInfoAnalysis, &allocation, smem,\n@@ -191,11 +195,11 @@ class ConvertTritonGPUToLLVM\n     // LoadStoreOp\n     populateLoadStoreOpToLLVMPatterns(typeConverter, patterns, numWarps,\n                                       axisInfoAnalysis, &allocation, smem,\n-                                      /*benefit=*/10);\n+                                      indexCacheInfo, /*benefit=*/10);\n     // ReduceOp\n     populateReduceOpToLLVMPatterns(typeConverter, patterns, numWarps,\n                                    axisInfoAnalysis, &allocation, smem,\n-                                   /*benefit=*/10);\n+                                   indexCacheInfo, /*benefit=*/10);\n     // ViewOp\n     populateViewOpToLLVMPatterns(typeConverter, patterns, numWarps,\n                                  axisInfoAnalysis, &allocation, smem,\n@@ -215,6 +219,13 @@ class ConvertTritonGPUToLLVM\n private:\n   Value smem;\n \n+  using IndexCacheKeyT = std::pair<Attribute, SmallVector<int64_t>>;\n+  DenseMap<IndexCacheKeyT, SmallVector<Value>, CacheKeyDenseMapInfo>\n+      baseIndexCache;\n+  DenseMap<IndexCacheKeyT, SmallVector<SmallVector<Value>>,\n+           CacheKeyDenseMapInfo>\n+      indexCache;\n+\n   int computeCapability{};\n \n   void initSharedMemory(size_t size,"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 54, "deletions": 13, "changes": 67, "file_content_changes": "@@ -997,20 +997,61 @@ func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n // -----\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n+    // CHECK: nvvm.read.ptx.sreg.nctaid.x\n+    // CHECK: nvvm.read.ptx.sreg.nctaid.y\n+    // CHECK: nvvm.read.ptx.sreg.nctaid.z\n+    %blockdimx = tt.get_num_programs {axis=0:i32} : i32\n+    %blockdimy = tt.get_num_programs {axis=1:i32} : i32\n+    %blockdimz = tt.get_num_programs {axis=2:i32} : i32\n+    %v0 = arith.addi %blockdimx, %blockdimy : i32\n+    %v1 = arith.addi %v0, %blockdimz : i32\n+    %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n+    tt.store %a, %0 : tensor<32xi32, #blocked0>\n+  \n+    return\n+  }\n+}\n \n-func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n-  // CHECK: nvvm.read.ptx.sreg.nctaid.x\n-  // CHECK: nvvm.read.ptx.sreg.nctaid.y\n-  // CHECK: nvvm.read.ptx.sreg.nctaid.z\n-  %blockdimx = tt.get_num_programs {axis=0:i32} : i32\n-  %blockdimy = tt.get_num_programs {axis=1:i32} : i32\n-  %blockdimz = tt.get_num_programs {axis=2:i32} : i32\n-  %v0 = arith.addi %blockdimx, %blockdimy : i32\n-  %v1 = arith.addi %v0, %blockdimz : i32\n-  %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n-  tt.store %a, %0 : tensor<32xi32, #blocked0>\n-\n-  return\n+// -----\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: test_index_cache \n+  func @test_index_cache() {\n+    // CHECK: nvvm.read.ptx.sreg.tid.x\n+    %0 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n+    // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n+    %1 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n+    return\n+  }\n }\n \n+// -----\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: test_base_index_cache \n+  func @test_base_index_cache(%arg0: tensor<128x32xf32, #blocked0>) {\n+    // CHECK: nvvm.read.ptx.sreg.tid.x\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n+    // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n+    %1 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n+    return\n+  }\n }\n+\n+// -----\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: test_index_cache_different_block\n+  func @test_index_cache_different_block(%arg0: tensor<128x32xf32, #blocked0>, %arg1: i1) {\n+    // CHECK: nvvm.read.ptx.sreg.tid.x\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n+    scf.if %arg1 {\n+      // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n+      %1 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n+    }\n+    return\n+  }\n+}\n\\ No newline at end of file"}]