[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 42, "deletions": 30, "changes": 72, "file_content_changes": "@@ -225,40 +225,52 @@ jobs:\n           GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n         run: |\n           OWNER_REPO=\"${{ github.repository }}\"\n-          PR_NUMBER=$(gh api --method GET repos/$OWNER_REPO/pulls -f state=closed | jq \".[] | select(.merged_at != null) | .number\" | head -1)\n-          echo \"Last merged PR number: $PR_NUMBER\"\n-\n-          BRANCH_NAME=$(gh api repos/$OWNER_REPO/pulls/$PR_NUMBER --jq '.head.ref')\n-          echo \"BRANCH_NAME: $BRANCH_NAME\"\n-\n-          page=1\n-          while true; do\n-            run_id=$(gh api --method GET \"repos/$OWNER_REPO/actions/runs?page=$page&per_page=100\" | jq --arg branch_name \"$BRANCH_NAME\" '.workflow_runs[] | select(.head_branch == $branch_name)' | jq '.id' | head -1)\n-            if [ \"$run_id\" != \"\" ]; then\n-              echo \"First run ID on branch $BRANCH_NAME is: $run_id\"\n-              WORKFLOW_RUN_ID=$run_id\n+          echo \"OWNER_REPO: $OWNER_REPO\"\n+          PR_NUMBERS=($(gh api --method GET repos/$OWNER_REPO/pulls -f state=closed | jq -r \".[] | select(.merged_at != null) | .number\"))\n+\n+          # Not all PRs go through integration tests\n+          success=0\n+          for PR_NUMBER in \"${PR_NUMBERS[@]}\"\n+          do\n+            echo \"Last merged PR number: $PR_NUMBER\"\n+            BRANCH_NAME=$(gh api repos/$OWNER_REPO/pulls/$PR_NUMBER --jq '.head.ref')\n+            echo \"BRANCH_NAME: $BRANCH_NAME\"\n+            USER_ID=$(gh api repos/$OWNER_REPO/pulls/$PR_NUMBER --jq '.user.id')\n+            echo \"USER_ID: $USER_ID\"\n+\n+            page=1\n+            while true; do\n+              run_id=$(gh api --method GET \"repos/$OWNER_REPO/actions/runs?page=$page&per_page=100\" | jq --arg branch_name \"$BRANCH_NAME\" --arg run_name \"Integration Tests\" --arg user_id \"$USER_ID\" '.workflow_runs[] | select(.head_branch == $branch_name and .name == $run_name and .actor.id == ($user_id | tonumber))' | jq '.id' | head -1)\n+              if [ \"$run_id\" != \"\" ]; then\n+                echo \"First run ID on branch $BRANCH_NAME is: $run_id\"\n+                WORKFLOW_RUN_ID=$run_id\n+                break\n+              fi\n+\n+              ((page++))\n+            done\n+\n+            echo \"WORKFLOW_RUN_ID: $WORKFLOW_RUN_ID\"\n+            ARTIFACT_URL=$(gh api repos/$OWNER_REPO/actions/runs/$WORKFLOW_RUN_ID/artifacts | jq --arg artifact_name \"$ARTIFACT_NAME\" '.artifacts[] | select(.name == $artifact_name).archive_download_url' --raw-output)\n+            echo \"ARTIFACT_URL: $ARTIFACT_URL\"\n+\n+            if [ -n \"$ARTIFACT_URL\" ]; then\n+              echo \"Downloading artifact: $ARTIFACT_URL\"\n+              curl --location --remote-header-name -H \"Authorization: token $GH_TOKEN\" -o reference.zip \"$ARTIFACT_URL\"\n+              # Print the size of the downloaded artifact\n+              echo \"Artifact size (stat): $(stat --printf=\"%s bytes\" reference.zip)\"\n+              echo \"Artifact size (du): $(du -sh reference.zip)\"\n+              unzip reference.zip\n+              tar -xzf artifacts.tar.gz\n+              rm reference.zip\n+              rm artifacts.tar.gz\n+              mv cache reference\n+              success=1\n               break\n             fi\n-\n-            ((page++))\n           done\n \n-          echo \"WORKFLOW_RUN_ID: $WORKFLOW_RUN_ID\"\n-          ARTIFACT_URL=$(gh api repos/$OWNER_REPO/actions/runs/$WORKFLOW_RUN_ID/artifacts | jq --arg artifact_name \"$ARTIFACT_NAME\" '.artifacts[] | select(.name == $artifact_name).archive_download_url' --raw-output)\n-          echo \"ARTIFACT_URL: $ARTIFACT_URL\"\n-\n-          if [ -n \"$ARTIFACT_URL\" ]; then\n-            echo \"Downloading artifact: $ARTIFACT_URL\"\n-            curl --location --remote-header-name -H \"Authorization: token $GH_TOKEN\" -o reference.zip \"$ARTIFACT_URL\"\n-            # Print the size of the downloaded artifact\n-            echo \"Artifact size (stat): $(stat --printf=\"%s bytes\" reference.zip)\"\n-            echo \"Artifact size (du): $(du -sh reference.zip)\"\n-            unzip reference.zip\n-            tar -xzf artifacts.tar.gz\n-            rm reference.zip\n-            rm artifacts.tar.gz\n-            mv cache reference\n-          else\n+          if [ $success -eq 0 ]; then\n             echo \"No artifact found with the name: $ARTIFACT_NAME\"\n             exit 1\n           fi"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 41, "deletions": 28, "changes": 69, "file_content_changes": "@@ -65,7 +65,7 @@ enum backend_t {\n \n void init_triton_runtime(py::module &&m) {\n   // wrap backend_t\n-  py::enum_<backend_t>(m, \"backend\")\n+  py::enum_<backend_t>(m, \"backend\", py::module_local())\n       .value(\"HOST\", HOST)\n       .value(\"CUDA\", CUDA)\n       .value(\"ROCM\", ROCM)\n@@ -164,12 +164,14 @@ void init_triton_ir(py::module &&m) {\n   using ret = py::return_value_policy;\n   using namespace pybind11::literals;\n \n-  py::enum_<mlir::triton::PaddingOption>(m, \"PADDING_OPTION\")\n+  py::enum_<mlir::triton::PaddingOption>(m, \"PADDING_OPTION\",\n+                                         py::module_local())\n       .value(\"PAD_ZERO\", mlir::triton::PaddingOption::PAD_ZERO)\n       .value(\"PAD_NAN\", mlir::triton::PaddingOption::PAD_NAN)\n       .export_values();\n \n-  py::enum_<mlir::triton::CacheModifier>(m, \"CACHE_MODIFIER\")\n+  py::enum_<mlir::triton::CacheModifier>(m, \"CACHE_MODIFIER\",\n+                                         py::module_local())\n       .value(\"NONE\", mlir::triton::CacheModifier::NONE)\n       .value(\"CA\", mlir::triton::CacheModifier::CA)\n       .value(\"CG\", mlir::triton::CacheModifier::CG)\n@@ -178,20 +180,21 @@ void init_triton_ir(py::module &&m) {\n       .value(\"WT\", mlir::triton::CacheModifier::WT)\n       .export_values();\n \n-  py::enum_<mlir::triton::MemSemantic>(m, \"MEM_SEMANTIC\")\n+  py::enum_<mlir::triton::MemSemantic>(m, \"MEM_SEMANTIC\", py::module_local())\n       .value(\"ACQUIRE_RELEASE\", mlir::triton::MemSemantic::ACQUIRE_RELEASE)\n       .value(\"ACQUIRE\", mlir::triton::MemSemantic::ACQUIRE)\n       .value(\"RELEASE\", mlir::triton::MemSemantic::RELEASE)\n       .value(\"RELAXED\", mlir::triton::MemSemantic::RELAXED)\n       .export_values();\n \n-  py::enum_<mlir::triton::EvictionPolicy>(m, \"EVICTION_POLICY\")\n+  py::enum_<mlir::triton::EvictionPolicy>(m, \"EVICTION_POLICY\",\n+                                          py::module_local())\n       .value(\"NORMAL\", mlir::triton::EvictionPolicy::NORMAL)\n       .value(\"EVICT_FIRST\", mlir::triton::EvictionPolicy::EVICT_FIRST)\n       .value(\"EVICT_LAST\", mlir::triton::EvictionPolicy::EVICT_LAST)\n       .export_values();\n \n-  py::enum_<mlir::triton::RMWOp>(m, \"ATOMIC_OP\")\n+  py::enum_<mlir::triton::RMWOp>(m, \"ATOMIC_OP\", py::module_local())\n       .value(\"ADD\", mlir::triton::RMWOp::ADD)\n       .value(\"FADD\", mlir::triton::RMWOp::FADD)\n       .value(\"AND\", mlir::triton::RMWOp::AND)\n@@ -203,7 +206,7 @@ void init_triton_ir(py::module &&m) {\n       .value(\"UMIN\", mlir::triton::RMWOp::UMIN)\n       .value(\"UMAX\", mlir::triton::RMWOp::UMAX);\n \n-  py::class_<mlir::MLIRContext>(m, \"context\")\n+  py::class_<mlir::MLIRContext>(m, \"context\", py::module_local())\n       .def(py::init<>())\n       .def(\"load_triton\", [](mlir::MLIRContext &self) {\n         self.getOrLoadDialect<mlir::triton::TritonDialect>();\n@@ -259,7 +262,7 @@ void init_triton_ir(py::module &&m) {\n   // // py::class_<ir::undef_value, ir::constant>(m, \"undef\")\n   // //     .def(\"get\", &ir::undef_value::get, ret::reference);\n \n-  py::class_<mlir::Type>(m, \"type\")\n+  py::class_<mlir::Type>(m, \"type\", py::module_local())\n       .def(\"is_integer\", &mlir::Type::isInteger)\n       .def(\"is_fp16\", &mlir::Type::isF16)\n       .def(\"__str__\", [](mlir::Type &self) {\n@@ -269,21 +272,21 @@ void init_triton_ir(py::module &&m) {\n         return os.str();\n       });\n \n-  py::class_<mlir::FunctionType>(m, \"function_type\")\n+  py::class_<mlir::FunctionType>(m, \"function_type\", py::module_local())\n       .def(\"param_types\", [](mlir::FunctionType &self) {\n         return std::vector<mlir::Type>(self.getInputs().begin(),\n                                        self.getInputs().end());\n       });\n \n-  py::class_<mlir::Location>(m, \"location\")\n+  py::class_<mlir::Location>(m, \"location\", py::module_local())\n       .def(\"__str__\", [](mlir::Location &self) {\n         std::string str;\n         llvm::raw_string_ostream os(str);\n         self.print(os);\n         return os.str();\n       });\n \n-  py::class_<mlir::Value>(m, \"value\")\n+  py::class_<mlir::Value>(m, \"value\", py::module_local())\n       .def(\"set_attr\",\n            [](mlir::Value &self, std::string &name,\n               mlir::Attribute &attr) -> void {\n@@ -307,14 +310,15 @@ void init_triton_ir(py::module &&m) {\n            })\n       .def(\"get_type\", &mlir::Value::getType);\n \n-  py::class_<mlir::BlockArgument, mlir::Value>(m, \"block_argument\");\n+  py::class_<mlir::BlockArgument, mlir::Value>(m, \"block_argument\",\n+                                               py::module_local());\n \n-  py::class_<mlir::Region>(m, \"region\")\n+  py::class_<mlir::Region>(m, \"region\", py::module_local())\n       .def(\"get_parent_region\", &mlir::Region::getParentRegion, ret::reference)\n       .def(\"size\", [](mlir::Region &self) { return self.getBlocks().size(); })\n       .def(\"empty\", &mlir::Region::empty);\n \n-  py::class_<mlir::Block>(m, \"block\")\n+  py::class_<mlir::Block>(m, \"block\", py::module_local())\n       .def(\"arg\",\n            [](mlir::Block &self, int index) -> mlir::BlockArgument {\n              return self.getArgument(index);\n@@ -383,12 +387,14 @@ void init_triton_ir(py::module &&m) {\n   //     .value(\"retune\", eattr::retune)\n   //     .value(\"not_implemented\", eattr::not_implemented);\n \n-  py::class_<mlir::Attribute>(m, \"attribute\");\n-  py::class_<mlir::IntegerAttr, mlir::Attribute>(m, \"integer_attr\");\n-  py::class_<mlir::BoolAttr, mlir::Attribute>(m, \"bool_attr\");\n+  py::class_<mlir::Attribute>(m, \"attribute\", py::module_local());\n+  py::class_<mlir::IntegerAttr, mlir::Attribute>(m, \"integer_attr\",\n+                                                 py::module_local());\n+  py::class_<mlir::BoolAttr, mlir::Attribute>(m, \"bool_attr\",\n+                                              py::module_local());\n \n   // Ops\n-  py::class_<mlir::OpState>(m, \"OpState\")\n+  py::class_<mlir::OpState>(m, \"OpState\", py::module_local())\n       .def(\"set_attr\",\n            [](mlir::OpState &self, std::string &name,\n               mlir::Attribute &attr) -> void { self->setAttr(name, attr); })\n@@ -427,23 +433,27 @@ void init_triton_ir(py::module &&m) {\n         return mlir::succeeded(mlir::verify(self.getOperation()));\n       });\n   // scf Ops\n-  py::class_<mlir::scf::ForOp, mlir::OpState>(m, \"ForOp\")\n+  py::class_<mlir::scf::ForOp, mlir::OpState>(m, \"ForOp\", py::module_local())\n       .def(\"get_induction_var\", &mlir::scf::ForOp::getInductionVar);\n \n-  py::class_<mlir::scf::IfOp, mlir::OpState>(m, \"IfOp\")\n+  py::class_<mlir::scf::IfOp, mlir::OpState>(m, \"IfOp\", py::module_local())\n       .def(\"get_then_block\", &mlir::scf::IfOp::thenBlock, ret::reference)\n       .def(\"get_else_block\", &mlir::scf::IfOp::elseBlock, ret::reference)\n       .def(\"get_then_yield\", &mlir::scf::IfOp::thenYield)\n       .def(\"get_else_yield\", &mlir::scf::IfOp::elseYield);\n-  py::class_<mlir::scf::YieldOp, mlir::OpState>(m, \"YieldOp\");\n-  py::class_<mlir::scf::WhileOp, mlir::OpState>(m, \"WhileOp\")\n+  py::class_<mlir::scf::YieldOp, mlir::OpState>(m, \"YieldOp\",\n+                                                py::module_local());\n+  py::class_<mlir::scf::WhileOp, mlir::OpState>(m, \"WhileOp\",\n+                                                py::module_local())\n       .def(\"get_before\", &mlir::scf::WhileOp::getBefore, ret::reference)\n       .def(\"get_after\", &mlir::scf::WhileOp::getAfter, ret::reference);\n-  py::class_<mlir::scf::ConditionOp, mlir::OpState>(m, \"ConditionOp\");\n+  py::class_<mlir::scf::ConditionOp, mlir::OpState>(m, \"ConditionOp\",\n+                                                    py::module_local());\n \n   // dynamic_attr is used to transfer ownership of the MLIR context to the\n   // module\n-  py::class_<mlir::ModuleOp, mlir::OpState>(m, \"module\", py::dynamic_attr())\n+  py::class_<mlir::ModuleOp, mlir::OpState>(m, \"module\", py::module_local(),\n+                                            py::dynamic_attr())\n       .def(\"dump\", &mlir::ModuleOp::dump)\n       .def(\"str\",\n            [](mlir::ModuleOp &self) -> std::string {\n@@ -523,7 +533,8 @@ void init_triton_ir(py::module &&m) {\n       },\n       ret::take_ownership);\n \n-  py::class_<mlir::triton::FuncOp, mlir::OpState>(m, \"function\")\n+  py::class_<mlir::triton::FuncOp, mlir::OpState>(m, \"function\",\n+                                                  py::module_local())\n       // .def_property_readonly(\"attrs\", &ir::function::attrs)\n       // .def(\"add_attr\", &ir::function::add_attr);\n       .def(\"args\",\n@@ -571,9 +582,11 @@ void init_triton_ir(py::module &&m) {\n       .def_property_readonly(\"type\", &mlir::triton::FuncOp::getFunctionType)\n       .def(\"reset_type\", &mlir::triton::FuncOp::setType);\n \n-  py::class_<mlir::OpBuilder::InsertPoint>(m, \"InsertPoint\");\n+  py::class_<mlir::OpBuilder::InsertPoint>(m, \"InsertPoint\",\n+                                           py::module_local());\n \n-  py::class_<TritonOpBuilder>(m, \"builder\", py::dynamic_attr())\n+  py::class_<TritonOpBuilder>(m, \"builder\", py::module_local(),\n+                              py::dynamic_attr())\n       .def(py::init<mlir::MLIRContext *>())\n       // getters\n       .def(\"create_module\",\n@@ -1507,7 +1520,7 @@ void init_triton_ir(py::module &&m) {\n                                                          offsets);\n            });\n \n-  py::class_<mlir::PassManager>(m, \"pass_manager\")\n+  py::class_<mlir::PassManager>(m, \"pass_manager\", py::module_local())\n       .def(py::init<mlir::MLIRContext *>())\n       .def(\"enable_debug\",\n            [](mlir::PassManager &self) {"}, {"filename": "python/test/kernel_comparison/kernels.yml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -23,7 +23,7 @@ name_and_extension:\n     extension: ptx\n   - name: _kernel_0d1d2d345d6d7c89c1011c\n     extension: ptx\n-  - name: _bwd_kernel_0d1d2d34d5d6d7d8d9d10d11d12d13d14d15c16d17d18d19c20d21d22d23c2425d26d27\n+  - name: _bwd_kernel_0d1d2d34d5d6d7d8d9d10d11d12d13d14d15d16c17d18d19d20c21d22d23d24c2526d27d\n     extension: ptx\n   - name: _fwd_kernel_0d1d2d34d5d6d7d8d9d10c11d12d13d14c15d16d17d18c19d20d21d22c2324d25d\n     extension: ptx"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -2691,7 +2691,7 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # -------------\n \n \n-@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and\"])\n+@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and_dynamic\", \"if_and_static\"])\n def test_if(if_type, device):\n \n     @triton.jit"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -663,6 +663,11 @@ def bitcast(input: tl.tensor,\n                      dst_ty)\n \n \n+# TODO: architecture descriptor class\n+def _is_cuda(arch):\n+    return isinstance(arch, int)\n+\n+\n def cast(input: tl.tensor,\n          dst_ty: tl.dtype,\n          builder: ir.builder) -> tl.tensor:\n@@ -677,7 +682,7 @@ def cast(input: tl.tensor,\n     src_sca_ty = src_ty.scalar\n     dst_sca_ty = dst_ty.scalar\n \n-    if builder.arch < 89 and \\\n+    if _is_cuda(builder.arch) and builder.arch < 89 and \\\n        (src_sca_ty.is_fp8e4() or dst_sca_ty.is_fp8e4()):\n         warnings.warn(\"Standard tl.float8e4 format will be deprecated on SM < 89. \"\n                       \"Please use tl.float8e4b15.\", DeprecationWarning)"}]