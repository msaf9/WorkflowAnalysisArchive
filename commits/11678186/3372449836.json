[{"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 13, "deletions": 5, "changes": 18, "file_content_changes": "@@ -29,12 +29,20 @@ def TT_RedOpAttr : I32EnumAttr<\n     /*case*/\n     [\n         I32EnumAttrCase</*sym*/\"ADD\", 1, /*str*/\"add\">,\n-        I32EnumAttrCase<\"MAX\", 2, \"max\">,\n+        I32EnumAttrCase<\"FADD\", 2, \"fadd\">,\n         I32EnumAttrCase<\"MIN\", 3, \"min\">,\n-        I32EnumAttrCase<\"FADD\", 4, \"fadd\">,\n-        I32EnumAttrCase<\"FMAX\", 5, \"fmax\">,\n-        I32EnumAttrCase<\"FMIN\", 6, \"fmin\">,\n-        I32EnumAttrCase<\"XOR\", 7, \"xor\">\n+        I32EnumAttrCase<\"MAX\", 4, \"max\">,\n+        I32EnumAttrCase<\"UMIN\", 5, \"umin\">,\n+        I32EnumAttrCase<\"UMAX\", 6, \"umax\">,\n+        I32EnumAttrCase<\"ARGMIN\", 7, \"argmin\">,\n+        I32EnumAttrCase<\"ARGMAX\", 8, \"argmax\">,\n+        I32EnumAttrCase<\"ARGUMIN\", 9, \"argumin\">,\n+        I32EnumAttrCase<\"ARGUMAX\", 10, \"argumax\">,\n+        I32EnumAttrCase<\"FMIN\", 11, \"fmin\">,\n+        I32EnumAttrCase<\"FMAX\", 12, \"fmax\">,\n+        I32EnumAttrCase<\"ARGFMIN\", 13, \"argfmin\">,\n+        I32EnumAttrCase<\"ARGFMAX\", 14, \"argfmax\">,\n+        I32EnumAttrCase<\"XOR\", 15, \"xor\">\n     ]> {\n     let cppNamespace = \"::mlir::triton\";\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 661, "deletions": 101, "changes": 762, "file_content_changes": "@@ -56,6 +56,12 @@ Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v) {\n                                            IntegerAttr::get(i32ty, v));\n }\n \n+Value createConstantF32(Location loc, PatternRewriter &rewriter, float v) {\n+  auto type = type::f32Ty(rewriter.getContext());\n+  return rewriter.create<LLVM::ConstantOp>(loc, type,\n+                                           rewriter.getF32FloatAttr(v));\n+}\n+\n // Create a index type constant.\n Value createIndexConstant(OpBuilder &builder, Location loc,\n \n@@ -90,7 +96,8 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n #define fmin(...) rewriter.create<LLVM::MinNumOp>(loc, __VA_ARGS__)\n #define and_(...) rewriter.create<LLVM::AndOp>(loc, __VA_ARGS__)\n #define xor_(...) rewriter.create<LLVM::XOrOp>(loc, __VA_ARGS__)\n-#define bitcast(...) rewriter.create<LLVM::BitcastOp>(loc, __VA_ARGS__)\n+#define bitcast(val__, type__)                                                 \\\n+  rewriter.create<LLVM::BitcastOp>(loc, type__, val__)\n #define gep(...) rewriter.create<LLVM::GEPOp>(loc, __VA_ARGS__)\n #define ptr_ty(...) LLVM::LLVMPointerType::get(__VA_ARGS__)\n #define insert_val(...) rewriter.create<LLVM::InsertValueOp>(loc, __VA_ARGS__)\n@@ -112,11 +119,13 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n #define barrier() rewriter.create<mlir::gpu::BarrierOp>(loc)\n #define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n #define i32_ty rewriter.getIntegerType(32)\n+#define f16_ty rewriter.getF16Type()\n #define i8_ty rewriter.getIntegerType(8)\n #define f32_ty rewriter.getF32Type()\n #define vec_ty(type, num) VectorType::get(num, type)\n+#define f32_val(...) LLVM::createConstantF32(loc, rewriter, __VA_ARGS__)\n #define void_ty(ctx) LLVM::LLVMVoidType::get(ctx)\n-#define struct_ty(...) LLVM::LLVMStructType::getLiteral(__VA_ARGS__)\n+#define struct_ty(...) LLVM::LLVMStructType::getLiteral(ctx, __VA_ARGS__)\n \n // Creator for constant\n #define i32_val(...) LLVM::createConstantI32(loc, rewriter, __VA_ARGS__)\n@@ -685,7 +694,7 @@ Value convertSplatLikeOp(Type elemType, Type resType, Value constVal,\n   if (tensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n     auto tensorTy = resType.cast<RankedTensorType>();\n     auto srcType = typeConverter->convertType(elemType);\n-    auto llSrc = bitcast(srcType, constVal);\n+    auto llSrc = bitcast(constVal, srcType);\n     size_t elemsPerThread = getElemsPerThread(tensorTy);\n     llvm::SmallVector<Value> elems(elemsPerThread, llSrc);\n     llvm::SmallVector<Type> elemTypes(elems.size(), srcType);\n@@ -964,7 +973,7 @@ struct LoadOpConversion\n                 rewriter, loc, this->getTypeConverter()->getIndexType(), s);\n             v = insert_element(vecTy, v, falseVal, sVal);\n           }\n-          v = bitcast(IntegerType::get(getContext(), width), v);\n+          v = bitcast(v, IntegerType::get(getContext(), width));\n \n           PTXInstr::Operand *opr{};\n           if (otherIsSplatConstInt)\n@@ -1001,9 +1010,8 @@ struct LoadOpConversion\n         } else {\n           curr = ret;\n         }\n-        curr = bitcast(\n-            LLVM::getFixedVectorType(valueElemTy, width / valueElemNbits),\n-            curr);\n+        curr = bitcast(curr, LLVM::getFixedVectorType(valueElemTy,\n+                                                      width / valueElemNbits));\n         rets.push_back(curr);\n       }\n       int tmp = width / valueElemNbits;\n@@ -1101,15 +1109,15 @@ struct StoreOpConversion\n           Value elem = valueElems[elemOffset];\n           if (elem.getType().isInteger(1))\n             elem = rewriter.create<LLVM::SExtOp>(loc, type::i8Ty(ctx), elem);\n-          elem = bitcast(valueElemTy, elem);\n+          elem = bitcast(elem, valueElemTy);\n \n           Type u32Ty = typeConverter->convertType(type::u32Ty(ctx));\n           llWord =\n               insert_element(wordTy, llWord, elem,\n                              rewriter.create<LLVM::ConstantOp>(\n                                  loc, u32Ty, IntegerAttr::get(u32Ty, elemIdx)));\n         }\n-        llWord = bitcast(valArgTy, llWord);\n+        llWord = bitcast(llWord, valArgTy);\n         std::string constraint =\n             (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n         asmArgs.emplace_back(llWord, constraint);\n@@ -1293,27 +1301,27 @@ void ReduceOpConversion::accumulate(ConversionPatternRewriter &rewriter,\n   case RedOp::ADD:\n     acc = add(acc, cur);\n     break;\n-  case RedOp::MAX:\n-    if (type.isUnsignedInteger())\n-      acc = umax(acc, cur);\n-    else\n-      acc = smax(acc, cur);\n+  case RedOp::FADD:\n+    acc = fadd(acc.getType(), acc, cur);\n     break;\n   case RedOp::MIN:\n-    if (type.isUnsignedInteger())\n-      acc = umin(acc, cur);\n-    else\n-      acc = smin(acc, cur);\n+    acc = smin(acc, cur);\n     break;\n-  case RedOp::FADD:\n-    acc = fadd(acc.getType(), acc, cur);\n+  case RedOp::MAX:\n+    acc = smax(acc, cur);\n     break;\n-  case RedOp::FMAX:\n-    acc = fmax(acc, cur);\n+  case RedOp::UMIN:\n+    acc = umin(acc, cur);\n+    break;\n+  case RedOp::UMAX:\n+    acc = umax(acc, cur);\n     break;\n   case RedOp::FMIN:\n     acc = fmin(acc, cur);\n     break;\n+  case RedOp::FMAX:\n+    acc = fmax(acc, cur);\n+    break;\n   case RedOp::XOR:\n     acc = xor_(acc, cur);\n     break;\n@@ -1328,15 +1336,15 @@ Value ReduceOpConversion::shflSync(ConversionPatternRewriter &rewriter,\n \n   if (bits == 64) {\n     Type vecTy = vec_ty(f32_ty, 2);\n-    Value vec = bitcast(vecTy, val);\n+    Value vec = bitcast(val, vecTy);\n     Value val0 = extract_element(f32_ty, vec, i32_val(0));\n     Value val1 = extract_element(f32_ty, vec, i32_val(1));\n     val0 = shflSync(rewriter, loc, val0, i);\n     val1 = shflSync(rewriter, loc, val1, i);\n     vec = undef(vecTy);\n     vec = insert_element(vecTy, vec, val0, i32_val(0));\n     vec = insert_element(vecTy, vec, val1, i32_val(1));\n-    return bitcast(val.getType(), vec);\n+    return bitcast(vec, val.getType());\n   }\n \n   PTXBuilder builder;\n@@ -1363,7 +1371,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n   auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n   auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n   Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n-  smemBase = bitcast(elemPtrTy, smemBase);\n+  smemBase = bitcast(smemBase, elemPtrTy);\n \n   auto smemShape = getScratchConfigForReduce(op);\n \n@@ -1430,7 +1438,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n \n     barrier();\n     SmallVector<Value> resultVals(resultElems);\n-    for (size_t i = 0; i < resultElems; i++) {\n+    for (unsigned i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       readIdx.insert(readIdx.begin() + axis, ints[0]);\n       Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n@@ -1469,7 +1477,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n   auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n   Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n-  smemBase = bitcast(elemPtrTy, smemBase);\n+  smemBase = bitcast(smemBase, elemPtrTy);\n \n   auto order = srcLayout.getOrder();\n   unsigned sizeIntraWarps = threadsPerWarp[axis];\n@@ -1569,7 +1577,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n \n     barrier();\n     SmallVector<Value> resultVals(resultElems);\n-    for (size_t i = 0; i < resultElems; i++) {\n+    for (size_t i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       readIdx.insert(readIdx.begin() + axis, i32_val(0));\n       Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n@@ -2136,7 +2144,7 @@ void ConvertLayoutOpConversion::processReplica(\n       auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n       Value ptr = gep(elemPtrTy, smemBase, offset);\n       auto vecTy = vec_ty(llvmElemTy, vec);\n-      ptr = bitcast(ptr_ty(vecTy, 3), ptr);\n+      ptr = bitcast(ptr, ptr_ty(vecTy, 3));\n       if (stNotRd) {\n         Value valVec = undef(vecTy);\n         for (unsigned v = 0; v < vec; ++v) {\n@@ -2175,7 +2183,7 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n   auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n   Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n   auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n-  smemBase = bitcast(elemPtrTy, smemBase);\n+  smemBase = bitcast(smemBase, elemPtrTy);\n   auto shape = dstTy.getShape();\n   unsigned rank = dstTy.getRank();\n   SmallVector<unsigned> numReplicates(rank);\n@@ -2235,7 +2243,8 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n   }\n \n   SmallVector<Type> types(outElems, llvmElemTy);\n-  Type structTy = struct_ty(getContext(), types);\n+  auto *ctx = llvmElemTy.getContext();\n+  Type structTy = struct_ty(types);\n   Value result = getStructFromElements(loc, outVals, rewriter, structTy);\n   rewriter.replaceOp(op, result);\n \n@@ -2295,7 +2304,7 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n   Value minVecVal = idx_val(minVec);\n   Value smemBase = getSharedMemoryBase(loc, rewriter, dst);\n   auto elemPtrTy = ptr_ty(getTypeConverter()->convertType(elemTy), 3);\n-  smemBase = bitcast(elemPtrTy, smemBase);\n+  smemBase = bitcast(smemBase, elemPtrTy);\n   unsigned numWordsEachRep = product<unsigned>(wordsInEachRep);\n   SmallVector<Value> wordVecs(numWordsEachRep);\n   // TODO: We should get less barriers if it is handled by membar pass\n@@ -2351,7 +2360,7 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n \n         // step 3: store\n         Value smemAddr = gep(elemPtrTy, smemBase, offset);\n-        smemAddr = bitcast(ptr_ty(wordTy, 3), smemAddr);\n+        smemAddr = bitcast(smemAddr, ptr_ty(wordTy, 3));\n         store(wordVecs[linearWordIdx], smemAddr);\n       }\n     }\n@@ -2694,7 +2703,7 @@ class MMA16816SmemLoader {\n           for (int e = 0; e < 4; ++e)\n             i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n                                           i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bitcast(i32_ty, i8v4Elems[m]);\n+          i32Elems[m] = bitcast(i8v4Elems[m], i32_ty);\n         }\n       } else { // k first\n         Value offset = i32_val(sOffsetElem);\n@@ -2712,7 +2721,7 @@ class MMA16816SmemLoader {\n           for (int e = 0; e < 4; ++e)\n             i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n                                           i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bitcast(i32_ty, i8v4Elems[m]);\n+          i32Elems[m] = bitcast(i8v4Elems[m], i32_ty);\n         }\n       }\n \n@@ -2824,10 +2833,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                                 ConversionPatternRewriter &rewriter) const;\n   /// Convert to mma.m8n8k4\n   LogicalResult convertMMA884(triton::DotOp op, OpAdaptor adaptor,\n-                              ConversionPatternRewriter &rewriter) const {\n-    assert(false && \"Not implemented yet.\");\n-    return failure();\n-  }\n+                              ConversionPatternRewriter &rewriter) const;\n \n   LogicalResult convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n                               ConversionPatternRewriter &rewriter) const {\n@@ -2836,57 +2842,136 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n   }\n };\n \n-struct DotOpConversionHelper {\n+// Helper for conversion of DotOp with mma<version=1>, that is sm<80\n+struct DotOpMmaV1ConversionHelper {\n+  MmaEncodingAttr mmaLayout;\n+  ArrayRef<unsigned> wpt;\n+\n+  using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+\n+  explicit DotOpMmaV1ConversionHelper(MmaEncodingAttr mmaLayout)\n+      : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()) {}\n+\n+  int getRepM(int M) const {\n+    return std::max<int>(M / (wpt[0] * instrShape[0]), 1);\n+  }\n+  int getRepN(int N) const {\n+    return std::max<int>(N / (wpt[1] * instrShape[1]), 1);\n+  }\n+  int getRepK(int K) const { return std::max<int>(K / instrShape[2], 1); }\n+\n+  static ArrayRef<unsigned> getMmaInstrShape() { return instrShape; }\n+\n+  static Type getMmaRetType(TensorType operand) {\n+    auto *ctx = operand.getContext();\n+    Type fp32Ty = type::f32Ty(ctx);\n+    // f16*f16+f32->f32\n+    return struct_ty(SmallVector<Type>{8, fp32Ty});\n+  }\n+\n+  // number of fp16x2 elements for $a.\n+  int numElemsPerThreadA(RankedTensorType tensorTy) const {\n+    auto shape = tensorTy.getShape();\n+    auto order = getOrder();\n+\n+    bool isARow = order[0] != 0;\n+    bool isAVec4 = !isARow && shape[order[0]] <= 16; // fp16*4 = 16bytes\n+    int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+\n+    SmallVector<int> fpw({2, 2, 1});\n+    int repM = 2 * packSize0;\n+    int repK = 1;\n+    int spwM = fpw[0] * 4 * repM;\n+    SmallVector<int> rep({repM, 0, repK}); // pad N with 0\n+    SmallVector<int> spw({spwM, 0, 1});    // pad N with 0\n+\n+    int NK = shape[1];\n+    unsigned numM = rep[0] * shape[0] / (spw[0] * wpt[0]);\n+\n+    // NOTE We cound't get the vec from the shared layout.\n+    // int vecA = sharedLayout.getVec();\n+    // TODO[Superjomn]: Consider the case when vecA > 4\n+    bool vecGt4 = false;\n+    int elemsPerLd = vecGt4 ? 4 : 2;\n+    return (numM / 2) * (NK / 4) * elemsPerLd;\n+  }\n+\n+  // number of fp16x2 elements for $b.\n+  int numElemsPerThreadB(RankedTensorType tensorTy) const {\n+    auto shape = tensorTy.getShape();\n+    auto order = getOrder();\n+    bool isBRow = order[0] != 0;\n+    bool isBVec4 = isBRow && shape[order[0]] <= 16;\n+    int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+    SmallVector<int> fpw({2, 2, 1});\n+    SmallVector<int> rep({0, 2 * packSize1, 1});       // pad M with 0\n+    SmallVector<int> spw({0, fpw[1] * 4 * rep[1], 1}); // pad M with 0\n+    // NOTE We cound't get the vec from the shared layout.\n+    // int vecB = sharedLayout.getVec();\n+    // TODO[Superjomn]: Consider the case when vecA > 4\n+    bool vecGt4 = false;\n+    int elemsPerLd = vecGt4 ? 4 : 2;\n+    int NK = shape[0];\n+\n+    unsigned numN = rep[1] * shape[1] / (spw[1] * wpt[0]);\n+    return (numN / 2) * (NK / 4) * elemsPerLd;\n+  }\n+\n+  // Loading $a from smem to registers, returns a LLVM::Struct.\n+  Value loadA(Value A, Value llA, Value thread, Value smem, Location loc,\n+              ConversionPatternRewriter &rewriter) const;\n+\n+  // Loading $b from smem to registers, returns a LLVM::Struct.\n+  Value loadB(Value B, Value llB, Value thread, Value smem, Location loc,\n+              ConversionPatternRewriter &rewriter) const;\n+\n+  // Loading $c to registers, returns a LLVM::Struct.\n+  Value loadC(Value C, Value llC, ConversionPatternRewriter &rewriter) const;\n+\n+  static ArrayRef<unsigned> getOrder() { return mmaOrder; }\n+\n+  // Compute the offset of the matrix to load.\n+  // Returns offsetAM, offsetAK, offsetBN, offsetBK.\n+  // NOTE, the information M(from $a) and N(from $b) couldn't be retrieved at\n+  // the same time in the usage in convert_layout[shared->dot_op], we leave the\n+  // noexist info to be 0 and only use the desired argument from the composed\n+  // result. In this way we want to retain the original code structure in\n+  // convert_mma884 method for easier debugging.\n+  std::tuple<Value, Value, Value, Value>\n+  computeOffsets(Value threadId, bool isARow, bool isBRow, ArrayRef<int> fpw,\n+                 ArrayRef<int> spw, ArrayRef<int> rep,\n+                 ConversionPatternRewriter &rewriter, Location loc) const;\n+\n+  // Extract values belong to $a or $b from a LLVMStruct, the shape is n0xn1.\n+  ValueTable extractLoadedOperand(Value llStruct, int n0, int n1,\n+                                  ConversionPatternRewriter &rewriter) const;\n+\n+private:\n+  static constexpr unsigned instrShape[] = {16, 16, 4};\n+  static constexpr unsigned mmaOrder[] = {0, 1};\n+};\n+\n+// Helper for conversion of DotOp with mma<version=2>, that is sm>=80\n+struct DotOpMmaV2ConversionHelper {\n   using TensorCoreType = DotOpConversion::TensorCoreType;\n \n   MmaEncodingAttr mmaLayout;\n   MLIRContext *ctx{};\n \n-  explicit DotOpConversionHelper(MmaEncodingAttr mmaLayout)\n+  explicit DotOpMmaV2ConversionHelper(MmaEncodingAttr mmaLayout)\n       : mmaLayout(mmaLayout) {\n     ctx = mmaLayout.getContext();\n   }\n \n-  // Load SplatLike C which contains a constVal. It simply returns 4 fp32\n-  // constVal.\n-  SmallVector<Value> loadSplatLikeC(Value C, Location loc,\n-                                    ConversionPatternRewriter &rewriter) const {\n-    assert(isSplatLike(C));\n-\n-    int numRes = getMmaInstrShape()[0] * getMmaInstrShape()[1] / 32;\n-    if (auto constv = llvm::dyn_cast<arith::ConstantOp>(C.getDefiningOp())) {\n-      if (auto attr = constv.getValue().dyn_cast<SplatElementsAttr>()) {\n-        Type elemType = attr.getElementType();\n-        if (elemType.isInteger(32)) {\n-          int v = attr.getSplatValue<int>();\n-          return SmallVector<Value>(numRes, i32_val(v));\n-        } else if (elemType.isInteger(8)) {\n-          int v = attr.getSplatValue<int8_t>();\n-          auto newv = rewriter.create<arith::ConstantOp>(\n-              loc, elemType, IntegerAttr::get(elemType, v));\n-          return SmallVector<Value>(numRes, newv);\n-        } else if (elemType.isF32()) {\n-          int v = attr.getSplatValue<float>();\n-          auto newv = rewriter.create<arith::ConstantOp>(\n-              loc, elemType, FloatAttr::get(elemType, v));\n-          return SmallVector<Value>(numRes, newv);\n-        }\n-      }\n-    }\n-\n-    assert(false && \"Not supported type.\");\n-    return {};\n-  }\n-\n   void deduceMmaType(DotOp op) const { mmaType = getMmaType(op); }\n   void deduceMmaType(Type operandTy) const {\n     mmaType = getTensorCoreTypeFromOperand(operandTy);\n   }\n \n   // Get the M and N of mat instruction shape.\n   static std::tuple<int, int> getMatShapeMN() {\n-    // According to DotOpConversionHelper::mmaMatShape, all the matrix shape's\n-    // M,N are {8,8}\n+    // According to DotOpMmaV2ConversionHelper::mmaMatShape, all the matrix\n+    // shape's M,N are {8,8}\n     return {8, 8};\n   }\n \n@@ -3144,7 +3229,7 @@ struct MMA16816ConversionHelper {\n \n   Value thread, lane, warp, warpMN, warpN, warpM;\n \n-  DotOpConversionHelper helper;\n+  DotOpMmaV2ConversionHelper helper;\n   ConversionPatternRewriter &rewriter;\n   TypeConverter *typeConverter;\n   Location loc;\n@@ -3204,22 +3289,25 @@ struct MMA16816ConversionHelper {\n \n   static int getNumRepM(Type operand, int M, int wpt) {\n     auto tensorCoreType =\n-        DotOpConversionHelper::getTensorCoreTypeFromOperand(operand);\n-    int mmaInstrM = DotOpConversionHelper::getMmaInstrShape(tensorCoreType)[0];\n+        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n+    int mmaInstrM =\n+        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[0];\n     return std::max<int>(M / (wpt * mmaInstrM), 1);\n   }\n \n   static int getNumRepN(Type operand, int N, int wpt) {\n     auto tensorCoreType =\n-        DotOpConversionHelper::getTensorCoreTypeFromOperand(operand);\n-    int mmaInstrN = DotOpConversionHelper::getMmaInstrShape(tensorCoreType)[1];\n+        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n+    int mmaInstrN =\n+        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[1];\n     return std::max<int>(N / (wpt * mmaInstrN), 1);\n   }\n \n   static int getNumRepK_(Type operand, int K) {\n     auto tensorCoreType =\n-        DotOpConversionHelper::getTensorCoreTypeFromOperand(operand);\n-    int mmaInstrK = DotOpConversionHelper::getMmaInstrShape(tensorCoreType)[2];\n+        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n+    int mmaInstrK =\n+        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[2];\n     return std::max<int>(K / mmaInstrK, 1);\n   }\n \n@@ -3305,7 +3393,7 @@ struct MMA16816ConversionHelper {\n   // Loading $c to registers, returns a Value.\n   Value loadC(Value tensor, Value llTensor) const {\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto [repM, repN] = DotOpConversionHelper::getRepMN(tensorTy);\n+    auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(tensorTy);\n     size_t fcSize = 4 * repM * repN;\n \n     assert(tensorTy.getEncoding().isa<MmaEncodingAttr>() &&\n@@ -3372,7 +3460,7 @@ struct MMA16816ConversionHelper {\n         return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n       };\n \n-      for (int i = 0; i < 4; i++)\n+      for (int i = 0; i < 4; ++i)\n         fc[m * colsPerThread + 4 * n + i] =\n             extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(i));\n     };\n@@ -3428,7 +3516,7 @@ struct MMA16816ConversionHelper {\n       Type smemPtrTy = helper.getShemPtrTy();\n       for (int i = 0; i < numPtrs; ++i) {\n         ptrs[i] =\n-            bitcast(smemPtrTy, gep(smemPtrTy, llTensor, ValueRange({offs[i]})));\n+            bitcast(gep(smemPtrTy, llTensor, ValueRange({offs[i]})), smemPtrTy);\n       }\n \n       auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n@@ -3493,7 +3581,7 @@ struct MMA16816ConversionHelper {\n \n     int offset{};\n     ValueTable vals;\n-    for (int i = 0; i < n0; i++) {\n+    for (int i = 0; i < n0; ++i) {\n       for (int j = 0; j < n1; j++) {\n         vals[{2 * i, 2 * j}] = elems[offset++];\n         vals[{2 * i, 2 * j + 1}] = elems[offset++];\n@@ -3515,20 +3603,37 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n \n   auto dotOperandLayout =\n       dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+\n   MmaEncodingAttr mmaLayout =\n       dotOperandLayout.getParent().dyn_cast_or_null<MmaEncodingAttr>();\n   assert(mmaLayout);\n \n-  MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n-                                     rewriter, getTypeConverter(), op.getLoc());\n-\n   Value res;\n-  if (dotOperandLayout.getOpIdx() == 0) {\n-    // operand $a\n-    res = mmaHelper.loadA(src, adaptor.src());\n-  } else if (dotOperandLayout.getOpIdx() == 1) {\n-    // operand $b\n-    res = mmaHelper.loadB(src, adaptor.src());\n+  if (mmaLayout.getVersion() == 2) {\n+    MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n+                                       rewriter, getTypeConverter(),\n+                                       op.getLoc());\n+\n+    if (dotOperandLayout.getOpIdx() == 0) {\n+      // operand $a\n+      res = mmaHelper.loadA(src, adaptor.src());\n+    } else if (dotOperandLayout.getOpIdx() == 1) {\n+      // operand $b\n+      res = mmaHelper.loadB(src, adaptor.src());\n+    }\n+  } else if (mmaLayout.getVersion() == 1) {\n+    DotOpMmaV1ConversionHelper helper(mmaLayout);\n+    if (dotOperandLayout.getOpIdx() == 0) {\n+      // operand $a\n+      res = helper.loadA(src, adaptor.src(), getThreadId(rewriter, loc),\n+                         adaptor.src(), loc, rewriter);\n+    } else if (dotOperandLayout.getOpIdx() == 1) {\n+      // operand $b\n+      res = helper.loadB(src, adaptor.src(), getThreadId(rewriter, loc),\n+                         adaptor.src(), loc, rewriter);\n+    }\n+  } else {\n+    assert(false && \"Unsupported mma layout found\");\n   }\n \n   rewriter.replaceOp(op, res);\n@@ -3572,6 +3677,424 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adaptor,\n                               adaptor);\n }\n \n+// Simply port the old code here to avoid large difference and make debugging\n+// and profiling easier.\n+LogicalResult\n+DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n+                               ConversionPatternRewriter &rewriter) const {\n+  auto *ctx = op.getContext();\n+  auto loc = op.getLoc();\n+\n+  Value A = op.a();\n+  Value B = op.b();\n+  Value D = op.getResult();\n+  auto mmaLayout = D.getType()\n+                       .cast<RankedTensorType>()\n+                       .getEncoding()\n+                       .cast<MmaEncodingAttr>();\n+\n+  auto ATensorTy = A.getType().cast<RankedTensorType>();\n+  auto BTensorTy = B.getType().cast<RankedTensorType>();\n+  auto DTensorTy = D.getType().cast<RankedTensorType>();\n+  auto AShape = ATensorTy.getShape();\n+  auto BShape = BTensorTy.getShape();\n+  auto DShape = DTensorTy.getShape();\n+  auto wpt = mmaLayout.getWarpsPerCTA();\n+\n+  bool transA = op.transA();\n+  bool transB = op.transB();\n+\n+  bool isARow = !transA;\n+  bool isBRow = !transB;\n+  bool isAVec4 = !isARow && AShape[isARow] <= 16; // fp16*4 = 16bytes\n+  bool isBVec4 = isBRow && BShape[isBRow] <= 16;\n+  int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+  int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+  SmallVector<int> fpw({2, 2, 1});\n+  SmallVector<int> rep({2 * packSize0, 2 * packSize1, 1});\n+  SmallVector<int> spw({fpw[0] * 4 * rep[0], fpw[1] * 4 * rep[1], 1});\n+\n+  Value loadedA = adaptor.a();\n+  Value loadedB = adaptor.b();\n+  Value loadedC = adaptor.c();\n+  DotOpMmaV1ConversionHelper helper(mmaLayout);\n+\n+  unsigned numM = rep[0] * DShape[0] / (spw[0] * wpt[0]);\n+  unsigned numN = rep[1] * DShape[1] / (spw[1] * wpt[0]);\n+  unsigned NK = AShape[1];\n+\n+  auto has = helper.extractLoadedOperand(loadedA, numM / 2, NK, rewriter);\n+  auto hbs = helper.extractLoadedOperand(loadedB, numN / 2, NK, rewriter);\n+\n+  size_t accSize = numM * numN;\n+\n+  // initialize accumulators\n+  SmallVector<Value> acc = getElementsFromStruct(loc, loadedC, rewriter);\n+\n+  auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n+    auto ha = has[{m, k}];\n+    auto hb = hbs[{n, k}];\n+    std::vector<size_t> idx{{\n+        (m * 2 + 0) + (n * 4 + 0) * numM, // row0\n+        (m * 2 + 0) + (n * 4 + 1) * numM,\n+        (m * 2 + 1) + (n * 4 + 0) * numM, // row1\n+        (m * 2 + 1) + (n * 4 + 1) * numM,\n+        (m * 2 + 0) + (n * 4 + 2) * numM, // row2\n+        (m * 2 + 0) + (n * 4 + 3) * numM,\n+        (m * 2 + 1) + (n * 4 + 2) * numM, // row3\n+        (m * 2 + 1) + (n * 4 + 3) * numM,\n+    }};\n+\n+    PTXBuilder builder;\n+\n+    auto *resOprs = builder.newListOperand(8, \"=f\");\n+    auto *AOprs = builder.newListOperand({\n+        {ha.first, \"f\"},\n+        {ha.second, \"f\"},\n+    });\n+\n+    auto *BOprs = builder.newListOperand({\n+        {hb.first, \"f\"},\n+        {hb.second, \"f\"},\n+    });\n+    auto *COprs = builder.newListOperand();\n+    for (int i = 0; i < 8; ++i)\n+      COprs->listAppend(builder.newOperand(acc[idx[i]], std::to_string(i)));\n+\n+    auto mma = builder.create(\"mma.sync.aligned.m8n8k4\")\n+                   ->o(isARow ? \"row\" : \"col\")\n+                   .o(isBRow ? \"row\" : \"col\")\n+                   .o(\".f32.f16.f16.f32\");\n+\n+    mma(resOprs, AOprs, BOprs, COprs);\n+\n+    Value res = builder.launch(rewriter, loc, helper.getMmaRetType(ATensorTy));\n+\n+    auto getIntAttr = [&](int v) {\n+      return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n+    };\n+    for (unsigned i = 0; i < 8; i++)\n+      acc[idx[i]] = extract_val(f32_ty, res, getIntAttr(i));\n+  };\n+\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      for (unsigned n = 0; n < numN / 2; ++n) {\n+        callMMA(m, n, k);\n+      }\n+\n+  // replace with new packed result\n+  Type structTy = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(acc.size(), type::f32Ty(ctx)));\n+  Value res = getStructFromElements(loc, acc, rewriter, structTy);\n+  rewriter.replaceOp(op, res);\n+\n+  return success();\n+}\n+\n+Value DotOpMmaV1ConversionHelper::loadA(\n+    Value tensor, Value llTensor, Value thread, Value smem, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto shape = tensorTy.getShape();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto order = sharedLayout.getOrder();\n+\n+  bool isARow = order[0] != 0;\n+  bool isAVec4 = !isARow && shape[order[0]] <= 16; // fp16*4 = 16bytes\n+  int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+\n+  SmallVector<int> fpw({2, 2, 1});\n+  int repM = 2 * packSize0;\n+  int repK = 1;\n+  int spwM = fpw[0] * 4 * repM;\n+  SmallVector<int> rep({repM, 0, repK}); // pad N with 0\n+  SmallVector<int> spw({spwM, 0, 1});    // pad N with 0\n+\n+  int vecA = sharedLayout.getVec();\n+\n+  int strideAM = isARow ? shape[1] : 1;\n+  int strideAK = isARow ? 1 : shape[0];\n+  int strideA0 = isARow ? strideAK : strideAM;\n+  int strideA1 = isARow ? strideAM : strideAK;\n+\n+  int strideRepM = wpt[0] * fpw[0] * 8;\n+  int strideRepK = 1;\n+\n+  auto [offsetAM, offsetAK, _0, _1] =\n+      computeOffsets(thread, isARow, false, fpw, spw, rep, rewriter, loc);\n+\n+  // swizzling\n+  int perPhaseA = sharedLayout.getPerPhase();\n+  int maxPhaseA = sharedLayout.getMaxPhase();\n+  int stepA0 = isARow ? strideRepK : strideRepM;\n+  int numPtrA = std::max(2 * perPhaseA * maxPhaseA / stepA0, 1);\n+  int NK = shape[1];\n+\n+  // pre-compute pointer lanes\n+  Value offA0 = isARow ? offsetAK : offsetAM;\n+  Value offA1 = isARow ? offsetAM : offsetAK;\n+  Value phaseA = urem(udiv(offA1, i32_val(perPhaseA)), i32_val(maxPhaseA));\n+  SmallVector<Value> offA(numPtrA);\n+\n+  for (int i = 0; i < numPtrA; i++) {\n+    Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));\n+    offA0I = udiv(offA0I, i32_val(vecA));\n+    offA0I = xor_(offA0I, phaseA);\n+    offA0I = xor_(offA0I, i32_val(vecA));\n+    offA[i] =\n+        add(mul(offA0I, i32_val(strideA0)), mul(offA1, i32_val(strideA1)));\n+  }\n+\n+  Type f16x2Ty = vec_ty(f16_ty, 2);\n+  // One thread get 8 elements as result\n+  Type retTy =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector(8, type::f32Ty(ctx)));\n+\n+  // prepare arguments\n+  SmallVector<Value> ptrA(numPtrA);\n+\n+  std::map<std::pair<int, int>, std::pair<Value, Value>> has;\n+  for (int i = 0; i < numPtrA; i++)\n+    ptrA[i] = gep(ptr_ty(f16_ty), smem, offA[i]);\n+\n+  auto instrShape = getMmaInstrShape();\n+  unsigned numM = rep[0] * shape[0] / (spw[0] * wpt[0]);\n+\n+  Type f16PtrTy = ptr_ty(f16_ty);\n+\n+  auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+  auto loadA = [&](int m, int k) {\n+    int offidx = (isARow ? k / 4 : m) % numPtrA;\n+    Value thePtrA = gep(f16PtrTy, smem, offA[offidx]);\n+\n+    int stepAM = isARow ? m : m / numPtrA * numPtrA;\n+    int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n+    Value pa = gep(f16PtrTy, thePtrA,\n+                   i32_val(stepAM * strideRepM * strideAM + stepAK * strideAK));\n+    Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n+    Value ha = load(bitcast(pa, aPtrTy));\n+    // record lds that needs to be moved\n+    Value ha00 = bitcast(extract_element(ha, i32_val(0)), f16x2Ty);\n+    Value ha01 = bitcast(extract_element(ha, i32_val(1)), f16x2Ty);\n+    ld(has, m, k, ha00, ha01);\n+\n+    if (vecA > 4) {\n+      Value ha10 = bitcast(extract_element(ha, i32_val(2)), f16x2Ty);\n+      Value ha11 = bitcast(extract_element(ha, i32_val(3)), f16x2Ty);\n+      if (isARow)\n+        ld(has, m, k + 4, ha10, ha11);\n+      else\n+        ld(has, m + 1, k, ha10, ha11);\n+    }\n+  };\n+\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      if (!has.count({m, k}))\n+        loadA(m, k);\n+\n+  SmallVector<Value> elems;\n+  elems.reserve(has.size() * 2);\n+  auto vecTy = vec_ty(f16_ty, 2);\n+  for (auto item : has) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+\n+  Type resTy = struct_ty(SmallVector<Type>(elems.size(), f16x2Ty));\n+  Value res = getStructFromElements(loc, elems, rewriter, resTy);\n+  return res;\n+}\n+\n+Value DotOpMmaV1ConversionHelper::loadB(\n+    Value tensor, Value llTensor, Value thread, Value smem, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto shape = tensorTy.getShape();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto order = sharedLayout.getOrder();\n+  bool isBRow = order[0] != 0;\n+  bool isBVec4 = isBRow && shape[order[0]] <= 16;\n+  int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+  SmallVector<int> fpw({2, 2, 1});\n+  SmallVector<int> rep({0, 2 * packSize1, 1});       // pad M with 0\n+  SmallVector<int> spw({0, fpw[1] * 4 * rep[1], 1}); // pad M with 0\n+  int vecB = sharedLayout.getVec();\n+  int strideBN = isBRow ? 1 : shape[0];\n+  int strideBK = isBRow ? shape[1] : 1;\n+  int strideB0 = isBRow ? strideBN : strideBK;\n+  int strideB1 = isBRow ? strideBK : strideBN;\n+  int strideRepN = wpt[1] * fpw[1] * 8;\n+  int strideRepK = 1;\n+\n+  // swizzling\n+  int perPhaseA = sharedLayout.getPerPhase();\n+  int maxPhaseA = sharedLayout.getMaxPhase();\n+  int perPhaseB = sharedLayout.getPerPhase();\n+  int maxPhaseB = sharedLayout.getMaxPhase();\n+  int stepB0 = isBRow ? strideRepN : strideRepK;\n+  int numPtrB = std::max(2 * perPhaseB * maxPhaseB / stepB0, 1);\n+  int NK = shape[0];\n+\n+  auto [_0, _1, offsetBN, offsetBK] =\n+      computeOffsets(thread, false, isBRow, fpw, spw, rep, rewriter, loc);\n+\n+  Value offB0 = isBRow ? offsetBN : offsetBK;\n+  Value offB1 = isBRow ? offsetBK : offsetBN;\n+  Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n+  SmallVector<Value> offB(numPtrB);\n+  for (int i = 0; i < numPtrB; ++i) {\n+    Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));\n+    offB0I = udiv(offB0I, i32_val(vecB));\n+    offB0I = xor_(offB0I, phaseB);\n+    offB0I = mul(offB0I, i32_val(vecB));\n+    offB[i] =\n+        add(mul(offB0I, i32_val(strideB0)), mul(offB1, i32_val(strideB1)));\n+  }\n+\n+  Type f16PtrTy = ptr_ty(f16_ty);\n+  Type f16x2Ty = vec_ty(f16_ty, 2);\n+\n+  SmallVector<Value> ptrB(numPtrB);\n+  ValueTable hbs;\n+  for (int i = 0; i < numPtrB; ++i)\n+    ptrB[i] = gep(ptr_ty(f16_ty), smem, offB[i]);\n+\n+  auto ld = [&](decltype(hbs) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+\n+  auto loadB = [&](int n, int K) {\n+    int offidx = (isBRow ? n : K / 4) % numPtrB;\n+    Value thePtrB = ptrB[offidx];\n+\n+    int stepBN = isBRow ? n / numPtrB * numPtrB : n;\n+    int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n+    Value pb = gep(f16PtrTy, thePtrB,\n+                   i32_val(stepBN * strideRepN * strideBN + stepBK * strideBK));\n+    Value hb =\n+        load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n+    // record lds that needs to be moved\n+    Value hb00 = bitcast(extract_element(hb, i32_val(0)), f16x2Ty);\n+    Value hb01 = bitcast(extract_element(hb, i32_val(1)), f16x2Ty);\n+    ld(hbs, n, K, hb00, hb01);\n+    if (vecB > 4) {\n+      Value hb10 = bitcast(extract_element(hb, i32_val(2)), f16x2Ty);\n+      Value hb11 = bitcast(extract_element(hb, i32_val(3)), f16x2Ty);\n+      if (isBRow)\n+        ld(hbs, n + 1, K, hb10, hb11);\n+      else\n+        ld(hbs, n, K + 4, hb10, hb11);\n+    }\n+  };\n+\n+  unsigned numN = rep[1] * shape[1] / (spw[1] * wpt[0]);\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned n = 0; n < numN / 2; ++n) {\n+      if (!hbs.count({n, k}))\n+        loadB(n, k);\n+    }\n+\n+  SmallVector<Value> elems;\n+  for (auto &item : hbs) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+  Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n+  Type resTy = struct_ty(SmallVector<Type>(elems.size(), fp16x2Ty));\n+  Value res = getStructFromElements(loc, elems, rewriter, resTy);\n+  return res;\n+}\n+\n+Value DotOpMmaV1ConversionHelper::loadC(\n+    Value tensor, Value llTensor, ConversionPatternRewriter &rewriter) const {\n+  return llTensor;\n+}\n+\n+std::tuple<Value, Value, Value, Value>\n+DotOpMmaV1ConversionHelper::computeOffsets(Value threadId, bool isARow,\n+                                           bool isBRow, ArrayRef<int> fpw,\n+                                           ArrayRef<int> spw, ArrayRef<int> rep,\n+                                           ConversionPatternRewriter &rewriter,\n+                                           Location loc) const {\n+  auto *ctx = rewriter.getContext();\n+  Value _1 = i32_val(1);\n+  Value _3 = i32_val(3);\n+  Value _4 = i32_val(4);\n+  Value _16 = i32_val(16);\n+  Value _32 = i32_val(32);\n+\n+  Value lane = urem(threadId, _32);\n+  Value warp = udiv(threadId, _32);\n+\n+  // warp offset\n+  Value warp0 = urem(warp, i32_val(wpt[0]));\n+  Value warp12 = udiv(warp, i32_val(wpt[0]));\n+  Value warp1 = urem(warp12, i32_val(wpt[1]));\n+  Value warpMOff = mul(warp0, i32_val(spw[0]));\n+  Value warpNOff = mul(warp1, i32_val(spw[1]));\n+  // Quad offset\n+  Value quadMOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[0]));\n+  Value quadNOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[1]));\n+  // Pair offset\n+  Value pairMOff = udiv(urem(lane, _16), _4);\n+  pairMOff = urem(pairMOff, i32_val(fpw[0]));\n+  pairMOff = mul(pairMOff, _4);\n+  Value pairNOff = udiv(urem(lane, _16), _4);\n+  pairNOff = udiv(pairNOff, i32_val(fpw[0]));\n+  pairNOff = urem(pairNOff, i32_val(fpw[1]));\n+  pairNOff = mul(pairNOff, _4);\n+  // scale\n+  pairMOff = mul(pairMOff, i32_val(rep[0] / 2));\n+  quadMOff = mul(quadMOff, i32_val(rep[0] / 2));\n+  pairNOff = mul(pairNOff, i32_val(rep[1] / 2));\n+  quadNOff = mul(quadNOff, i32_val(rep[1] / 2));\n+  // Quad pair offset\n+  Value laneMOff = add(pairMOff, quadMOff);\n+  Value laneNOff = add(pairNOff, quadNOff);\n+  // A offset\n+  Value offsetAM = add(warpMOff, laneMOff);\n+  Value offsetAK = and_(lane, _3);\n+  // B offset\n+  Value offsetBN = add(warpNOff, laneNOff);\n+  Value offsetBK = and_(lane, _3);\n+  // i indices\n+  Value offsetCM = add(and_(lane, _1), offsetAM);\n+  if (isARow) {\n+    offsetAM = add(offsetAM, urem(threadId, _4));\n+    offsetAK = i32_val(0);\n+  }\n+  if (!isBRow) {\n+    offsetBN = add(offsetBN, urem(threadId, _4));\n+    offsetBK = i32_val(0);\n+  }\n+\n+  return std::make_tuple(offsetAM, offsetAK, offsetBN, offsetBK);\n+}\n+\n+DotOpMmaV1ConversionHelper::ValueTable\n+DotOpMmaV1ConversionHelper::extractLoadedOperand(\n+    Value llStruct, int n0, int n1, ConversionPatternRewriter &rewriter) const {\n+  ValueTable rcds;\n+  SmallVector<Value> elems =\n+      ConvertTritonGPUOpToLLVMPatternBase::getElementsFromStruct(\n+          llStruct.getLoc(), llStruct, rewriter);\n+\n+  int offset = 0;\n+  for (int i = 0; i < n0; ++i)\n+    for (int k = 0; k < n1; k += 4) {\n+      rcds[{i, k}] = std::make_pair(elems[offset], elems[offset + 1]);\n+      offset += 2;\n+    }\n+\n+  return rcds;\n+}\n+\n /// ====================== mma codegen end ============================\n \n Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,\n@@ -3580,16 +4103,29 @@ Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,\n                                       TypeConverter *typeConverter,\n                                       ConversionPatternRewriter &rewriter,\n                                       Location loc) {\n+  auto tensorTy = resType.cast<RankedTensorType>();\n+  auto shape = tensorTy.getShape();\n   if (layout.getVersion() == 2) {\n-    auto tensorTy = resType.cast<RankedTensorType>();\n-    auto [repM, repN] = DotOpConversionHelper::getRepMN(tensorTy);\n+    auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(tensorTy);\n     size_t fcSize = 4 * repM * repN;\n \n     auto structTy = LLVM::LLVMStructType::getLiteral(\n         rewriter.getContext(), SmallVector<Type>(fcSize, elemType));\n     return getStructFromElements(loc, SmallVector<Value>(fcSize, constVal),\n                                  rewriter, structTy);\n   }\n+  if (layout.getVersion() == 1) {\n+    DotOpMmaV1ConversionHelper helper(layout);\n+    int repM = helper.getRepM(shape[0]);\n+    int repN = helper.getRepN(shape[1]);\n+    // According to mma layout of v1, each thread process 8 elements.\n+    int elems = 8 * repM * repN;\n+\n+    auto structTy = LLVM::LLVMStructType::getLiteral(\n+        rewriter.getContext(), SmallVector<Type>(elems, elemType));\n+    return getStructFromElements(loc, SmallVector<Value>(elems, constVal),\n+                                 rewriter, structTy);\n+  }\n \n   assert(false && \"Unsupported mma layout found\");\n }\n@@ -3621,6 +4157,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n   llvm::Optional<Type> convertTritonTensorType(RankedTensorType type) {\n     auto ctx = type.getContext();\n     Attribute layout = type.getEncoding();\n+    auto shape = type.getShape();\n     if (layout &&\n         (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n          layout.isa<MmaEncodingAttr>())) {\n@@ -3633,22 +4170,31 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n       return LLVM::LLVMPointerType::get(convertType(type.getElementType()), 3);\n     } else if (auto mmaLayout = layout.dyn_cast_or_null<MmaEncodingAttr>()) {\n       if (mmaLayout.getVersion() == 2) {\n-        auto [repM, repN] = DotOpConversionHelper::getRepMN(type);\n+        auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(type);\n         size_t fcSize = 4 * repM * repN;\n         return LLVM::LLVMStructType::getLiteral(\n             ctx, SmallVector<Type>(fcSize, type.getElementType()));\n       }\n \n+      if (mmaLayout.getVersion() == 1) {\n+        DotOpMmaV1ConversionHelper helper(mmaLayout);\n+        int repM = helper.getRepM(shape[0]);\n+        int repN = helper.getRepN(shape[1]);\n+        int elems = 8 * repM * repN;\n+        return LLVM::LLVMStructType::getLiteral(\n+            ctx, SmallVector<Type>(elems, type.getElementType()));\n+      }\n+\n       llvm::errs()\n           << \"Unexpected mma layout detected in TritonToLLVMTypeConverter\";\n       return llvm::None;\n \n     } else if (auto dot_op_layout =\n                    layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n       auto mmaLayout = dot_op_layout.getParent().cast<MmaEncodingAttr>();\n+      auto wpt = mmaLayout.getWarpsPerCTA();\n+      Type elemTy = type.getElementType();\n       if (mmaLayout.getVersion() == 2) {\n-        auto wpt = mmaLayout.getWarpsPerCTA();\n-        Type elemTy = type.getElementType();\n \n         if (dot_op_layout.getOpIdx() == 0) { // $a\n           int elems =\n@@ -3661,8 +4207,22 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n           int elems =\n               MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt);\n           Type x2Ty = vec_ty(elemTy, 2);\n-          return LLVM::LLVMStructType::getLiteral(\n-              ctx, SmallVector<Type>(elems, x2Ty));\n+          return struct_ty(SmallVector<Type>(elems, x2Ty));\n+        }\n+      }\n+\n+      if (mmaLayout.getVersion() == 1) {\n+        DotOpMmaV1ConversionHelper helper(mmaLayout);\n+\n+        if (dot_op_layout.getOpIdx() == 0) { // $a\n+          int elems = helper.numElemsPerThreadA(type);\n+          Type x2Ty = vec_ty(elemTy, 2);\n+          return struct_ty(SmallVector<Type>(elems, x2Ty));\n+        }\n+        if (dot_op_layout.getOpIdx() == 1) { // $b\n+          int elems = helper.numElemsPerThreadB(type);\n+          Type x2Ty = vec_ty(elemTy, 2);\n+          return struct_ty(SmallVector<Type>(elems, x2Ty));\n         }\n       }\n "}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 17, "deletions": 4, "changes": 21, "file_content_changes": "@@ -272,10 +272,23 @@ unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   size_t rank = shape.size();\n   assert(rank == 2 && \"Unexpected rank of mma layout\");\n-  assert(getVersion() == 2 && \"mmaLayout version = 1 is not implemented yet\");\n-  unsigned elemsCol = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]) * 2;\n-  unsigned elemsRow = ceil<unsigned>(shape[1], 8 * getWarpsPerCTA()[1]) * 2;\n-  return elemsCol * elemsRow;\n+  assert((getVersion() == 1 || getVersion() == 2) &&\n+         \"Only version 1 and 2 is supported\");\n+\n+  int res = 0;\n+  if (getVersion() == 1) {\n+    unsigned mmasRow = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]);\n+    unsigned mmasCol = ceil<unsigned>(shape[1], 16 * getWarpsPerCTA()[1]);\n+    // Each warp-level mma884 will perform a m16xn16xk4 mma, thus get a m16xn16\n+    // matrix as result.\n+    res = mmasRow * mmasCol * (16 * 16 / 32);\n+  } else if (getVersion() == 2) {\n+    unsigned elemsCol = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]) * 2;\n+    unsigned elemsRow = ceil<unsigned>(shape[1], 8 * getWarpsPerCTA()[1]) * 2;\n+    res = elemsCol * elemsRow;\n+  }\n+\n+  return res;\n }\n \n unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -87,8 +87,16 @@ void init_triton_ir(py::module &&m) {\n       .value(\"FADD\", mlir::triton::RedOp::FADD)\n       .value(\"MIN\", mlir::triton::RedOp::MIN)\n       .value(\"MAX\", mlir::triton::RedOp::MAX)\n+      .value(\"UMIN\", mlir::triton::RedOp::UMIN)\n+      .value(\"UMAX\", mlir::triton::RedOp::UMAX)\n+      .value(\"ARGMIN\", mlir::triton::RedOp::ARGMIN)\n+      .value(\"ARGMAX\", mlir::triton::RedOp::ARGMAX)\n+      .value(\"ARGUMIN\", mlir::triton::RedOp::ARGUMIN)\n+      .value(\"ARGUMAX\", mlir::triton::RedOp::ARGUMAX)\n       .value(\"FMIN\", mlir::triton::RedOp::FMIN)\n       .value(\"FMAX\", mlir::triton::RedOp::FMAX)\n+      .value(\"ARGFMIN\", mlir::triton::RedOp::ARGFMIN)\n+      .value(\"ARGFMAX\", mlir::triton::RedOp::ARGFMAX)\n       .value(\"XOR\", mlir::triton::RedOp::XOR);\n \n   py::enum_<mlir::triton::RMWOp>(m, \"ATOMIC_OP\")"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 134, "deletions": 113, "changes": 247, "file_content_changes": "@@ -874,121 +874,142 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n # # ---------------\n \n \n-# @pytest.mark.parametrize(\"op, dtype_str, shape\",\n-#                          [(op, dtype, shape)\n-#                           for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n-#                           for dtype in dtypes_with_bfloat16\n-#                           for shape in [32, 64, 128, 512]])\n-# def test_reduce1d(op, dtype_str, shape, device='cuda'):\n-#     check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+def get_reduced_dtype(dtype_str, op):\n+    if op == 'argmin' or op == 'argmax':\n+        return 'int32'\n+    if dtype_str in ['int8', 'uint8', 'int16', 'uint16']:\n+        return 'int32'\n+    if dtype_str == 'bfloat16':\n+        return 'float32'\n+    return dtype_str\n+\n+\n+# TODO: [Qingyi] Fix argmin / argmax\n+@pytest.mark.parametrize(\"op, dtype_str, shape\",\n+                         [(op, dtype, shape)\n+                          for op in ['min', 'max', 'sum']\n+                          for dtype in dtypes_with_bfloat16\n+                          for shape in [32, 64, 128, 512]])\n+def test_reduce1d(op, dtype_str, shape, device='cuda'):\n+    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n \n-#     # triton kernel\n-#     @triton.jit\n-#     def kernel(X, Z, BLOCK: tl.constexpr):\n-#         x = tl.load(X + tl.arange(0, BLOCK))\n-#         tl.store(Z, GENERATE_TEST_HERE)\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, Z, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        tl.store(Z, GENERATE_TEST_HERE)\n \n-#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=0)'})\n-#     # input\n-#     rs = RandomState(17)\n-#     # limit the range of integers so that the sum does not overflow\n-#     x = numpy_random((shape,), dtype_str=dtype_str, rs=rs)\n-#     x_tri = to_triton(x, device=device)\n-#     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n-#                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n-#     # numpy result\n-#     z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n-#     z_tri_dtype_str = z_dtype_str\n-#     if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n-#         z_dtype_str = 'float32'\n-#         z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n-#         # trunc mantissa for a fair comparison of accuracy\n-#         z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n-#         z_tri_dtype_str = 'bfloat16'\n-#     else:\n-#         z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n-#     # triton result\n-#     z_tri = to_triton(numpy_random((1,), dtype_str=z_dtype_str, rs=rs),\n-#                       device=device, dst_type=z_tri_dtype_str)\n-#     kernel[(1,)](x_tri, z_tri, BLOCK=shape)\n-#     z_tri = to_numpy(z_tri)\n-#     # compare\n-#     if op == 'sum':\n-#         np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n-#     else:\n-#         if op == 'argmin' or op == 'argmax':\n-#             # argmin and argmax can have multiple valid indices.\n-#             # so instead we compare the values pointed by indices\n-#             np.testing.assert_equal(x[z_ref], x[z_tri])\n-#         else:\n-#             np.testing.assert_equal(z_ref, z_tri)\n-\n-\n-# reduce_configs1 = [\n-#     (op, dtype, (1, 1024), axis) for dtype in dtypes_with_bfloat16\n-#     for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n-#     for axis in [1]\n-# ]\n-# reduce_configs2 = [\n-#     (op, 'float32', shape, axis)\n-#     for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n-#     for shape in [(2, 32), (4, 32), (4, 128), (32, 64), (64, 128), (128, 256), (32, 1024)]\n-#     for axis in [0, 1]\n-# ]\n-\n-\n-# @pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n-# def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n-#     # triton kernel\n-#     @triton.jit\n-#     def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n-#         range_m = tl.arange(0, BLOCK_M)\n-#         range_n = tl.arange(0, BLOCK_N)\n-#         x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n-#         z = GENERATE_TEST_HERE\n-#         if AXIS == 1:\n-#             tl.store(Z + range_m, z)\n-#         else:\n-#             tl.store(Z + range_n, z)\n-\n-#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=AXIS)'})\n-#     # input\n-#     rs = RandomState(17)\n-#     # limit the range of integers so that the sum does not overflow\n-#     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n-#     x_tri = to_triton(x)\n-#     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n-#                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n-#     z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n-#     z_tri_dtype_str = z_dtype_str\n-#     # numpy result\n-#     if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n-#         z_dtype_str = 'float32'\n-#         z_tri_dtype_str = 'bfloat16'\n-#         z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n-#         # trunc mantissa for a fair comparison of accuracy\n-#         z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n-#     else:\n-#         z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n-#     # triton result\n-#     z_tri = to_triton(numpy_random((shape[1 - axis],), dtype_str=z_dtype_str, rs=rs),\n-#                       device=device, dst_type=z_tri_dtype_str)\n-#     kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n-#     z_tri = to_numpy(z_tri)\n-#     # compare\n-#     if op == 'sum':\n-#         np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n-#     else:\n-#         if op == 'argmin' or op == 'argmax':\n-#             # argmin and argmax can have multiple valid indices.\n-#             # so instead we compare the values pointed by indices\n-#             z_ref_index = np.expand_dims(z_ref, axis=axis)\n-#             z_tri_index = np.expand_dims(z_tri, axis=axis)\n-#             z_ref_value = np.take_along_axis(x, z_ref_index, axis=axis)\n-#             z_tri_value = np.take_along_axis(x, z_tri_index, axis=axis)\n-#             np.testing.assert_equal(z_ref_value, z_tri_value)\n-#         else:\n-#             np.testing.assert_equal(z_ref, z_tri)\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=0)'})\n+    # input\n+    rs = RandomState(17)\n+    # limit the range of integers so that the sum does not overflow\n+    x = numpy_random((shape,), dtype_str=dtype_str, rs=rs)\n+    x_tri = to_triton(x, device=device)\n+    numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n+                'argmin': np.argmin, 'argmax': np.argmax}[op]\n+    # numpy result\n+    z_dtype_str = get_reduced_dtype(dtype_str, op)\n+    z_tri_dtype_str = z_dtype_str\n+    if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n+        z_dtype_str = 'float32'\n+        z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n+        # trunc mantissa for a fair comparison of accuracy\n+        z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n+        z_tri_dtype_str = 'bfloat16'\n+    else:\n+        z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n+    # triton result\n+    z_tri = to_triton(numpy_random((1,), dtype_str=z_dtype_str, rs=rs),\n+                      device=device, dst_type=z_tri_dtype_str)\n+    kernel[(1,)](x_tri, z_tri, BLOCK=shape)\n+    z_tri = to_numpy(z_tri)\n+    # compare\n+    if op == 'sum':\n+        np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n+    else:\n+        if op == 'argmin' or op == 'argmax':\n+            # argmin and argmax can have multiple valid indices.\n+            # so instead we compare the values pointed by indices\n+            np.testing.assert_equal(x[z_ref], x[z_tri])\n+        else:\n+            np.testing.assert_equal(z_ref, z_tri)\n+\n+\n+# TODO: [Qingyi] Fix argmin / argmax\n+reduce_configs1 = [\n+    (op, dtype, (1, 1024), axis) for dtype in dtypes_with_bfloat16\n+    for op in ['min', 'max', 'sum']\n+    for axis in [1]\n+]\n+\n+\n+# shape (128, 256) and (32, 1024) are not enabled on sm86 because the required shared memory\n+# exceeds the limit of 99KB\n+reduce2d_shapes = [(2, 32), (4, 32), (4, 128), (32, 64), (64, 128)]\n+if 'V100' in torch.cuda.get_device_name(0):\n+    reduce2d_shapes += [(128, 256) and (32, 1024)]\n+\n+\n+reduce_configs2 = [\n+    (op, 'float32', shape, axis)\n+    for op in ['min', 'max', 'sum']\n+    for shape in reduce2d_shapes\n+    for axis in [0, 1]\n+]\n+\n+\n+@pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n+def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n+        range_m = tl.arange(0, BLOCK_M)\n+        range_n = tl.arange(0, BLOCK_N)\n+        x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n+        z = GENERATE_TEST_HERE\n+        if AXIS == 1:\n+            tl.store(Z + range_m, z)\n+        else:\n+            tl.store(Z + range_n, z)\n+\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=AXIS)'})\n+    # input\n+    rs = RandomState(17)\n+    # limit the range of integers so that the sum does not overflow\n+    x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n+    x_tri = to_triton(x)\n+    numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n+                'argmin': np.argmin, 'argmax': np.argmax}[op]\n+    z_dtype_str = get_reduced_dtype(dtype_str, op)\n+    z_tri_dtype_str = z_dtype_str\n+    # numpy result\n+    if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n+        z_dtype_str = 'float32'\n+        z_tri_dtype_str = 'bfloat16'\n+        z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+        # trunc mantissa for a fair comparison of accuracy\n+        z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n+    else:\n+        z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+    # triton result\n+    z_tri = to_triton(numpy_random((shape[1 - axis],), dtype_str=z_dtype_str, rs=rs),\n+                      device=device, dst_type=z_tri_dtype_str)\n+    kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n+    z_tri = to_numpy(z_tri)\n+    # compare\n+    if op == 'sum':\n+        np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n+    else:\n+        if op == 'argmin' or op == 'argmax':\n+            # argmin and argmax can have multiple valid indices.\n+            # so instead we compare the values pointed by indices\n+            z_ref_index = np.expand_dims(z_ref, axis=axis)\n+            z_tri_index = np.expand_dims(z_tri, axis=axis)\n+            z_ref_value = np.take_along_axis(x, z_ref_index, axis=axis)\n+            z_tri_value = np.take_along_axis(x, z_tri_index, axis=axis)\n+            np.testing.assert_equal(z_ref_value, z_tri_value)\n+        else:\n+            np.testing.assert_equal(z_ref, z_tri)\n \n # # ---------------\n # # test permute"}, {"filename": "python/tests/test_math_ops.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -13,8 +13,8 @@ def math_kernel(x1_ptr, x2_ptr, x3_ptr, x4_ptr, n, BLOCK_SIZE: tl.constexpr):\n \n     y1 = tl.sin(x1)\n     y2 = tl.libdevice.sin(x2)\n-    y3 = tl.libdevice.fdiv_rn(x3, x3)\n-    y4 = tl.libdevice.fmaf_rd(x4, x4, x4)\n+    y3 = tl.libdevice.div_rn(x3, x3)\n+    y4 = tl.libdevice.fma_rd(x4, x4, x4)\n \n     tl.store(x1_ptr + offsets, y1, mask=offsets < n)\n     tl.store(x2_ptr + offsets, y2, mask=offsets < n)"}, {"filename": "python/tests/test_reduce.py", "status": "modified", "additions": 42, "deletions": 19, "changes": 61, "file_content_changes": "@@ -5,11 +5,20 @@\n import triton\n import triton.language as tl\n \n-dtype_mapping = {\n-    'float16': torch.float16,\n-    'float32': torch.float32,\n-    'float64': torch.float64,\n-}\n+int_dtypes = ['int8', 'int16', 'int32', 'int64']\n+uint_dtypes = ['uint8']  # PyTorch does not support uint16/uint32/uint64\n+float_dtypes = ['float16', 'float32', 'float64']\n+dtypes = int_dtypes + uint_dtypes + float_dtypes\n+dtypes_with_bfloat16 = int_dtypes + uint_dtypes + float_dtypes\n+dtype_mapping = {dtype_str: torch.__dict__[dtype_str] for dtype_str in dtypes}\n+\n+\n+def get_reduced_dtype(dtype):\n+    if dtype in [torch.int8, torch.int16, torch.uint8]:\n+        return torch.int32\n+    if dtype in [torch.bfloat16]:\n+        return torch.float32\n+    return dtype\n \n \n def patch_kernel(template, to_replace):\n@@ -40,33 +49,40 @@ def reduce2d_kernel(x_ptr, z_ptr, axis: tl.constexpr, block_m: tl.constexpr, blo\n reduce1d_configs = [\n     (op, dtype, shape)\n     for op in ['sum', 'min', 'max']\n-    for dtype in ['float16', 'float32', 'float64']\n+    for dtype in dtypes\n     for shape in [4, 8, 16, 32, 64, 128, 512, 1024]\n ]\n \n \n @pytest.mark.parametrize('op, dtype, shape', reduce1d_configs)\n def test_reduce1d(op, dtype, shape):\n     dtype = dtype_mapping[dtype]\n-    x = torch.randn((shape,), device='cuda', dtype=dtype)\n+    reduced_dtype = get_reduced_dtype(dtype)\n+\n+    if dtype.is_floating_point:\n+        x = torch.randn((shape,), device='cuda', dtype=dtype)\n+    elif dtype is torch.uint8:\n+        x = torch.randint(0, 20, (shape,), device='cuda', dtype=dtype)\n+    else:\n+        x = torch.randint(-20, 20, (shape,), device='cuda', dtype=dtype)\n     z = torch.empty(\n         tuple(),\n         device=x.device,\n-        dtype=dtype,\n+        dtype=reduced_dtype,\n     )\n \n     kernel = patch_kernel(reduce1d_kernel, {'OP': op})\n     grid = (1,)\n     kernel[grid](x_ptr=x, z_ptr=z, block=shape)\n \n     if op == 'sum':\n-        golden_z = torch.sum(x, dtype=dtype)\n+        golden_z = torch.sum(x, dtype=reduced_dtype)\n     elif op == 'min':\n-        golden_z = torch.min(x)\n+        golden_z = torch.min(x).to(reduced_dtype)\n     else:\n-        golden_z = torch.max(x)\n+        golden_z = torch.max(x).to(reduced_dtype)\n \n-    if op == 'sum':\n+    if dtype.is_floating_point and op == 'sum':\n         if shape >= 256:\n             assert_close(z, golden_z, rtol=0.05, atol=0.1)\n         elif shape >= 32:\n@@ -80,7 +96,7 @@ def test_reduce1d(op, dtype, shape):\n reduce2d_configs = [\n     (op, dtype, shape, axis)\n     for op in ['sum', 'min', 'max']\n-    for dtype in ['float16', 'float32', 'float64']\n+    for dtype in dtypes\n     for shape in [(1, 4), (1, 8), (1, 16), (1, 32), (2, 32), (4, 32), (4, 128), (32, 64)]\n     for axis in [0, 1]\n ]\n@@ -89,22 +105,29 @@ def test_reduce1d(op, dtype, shape):\n @pytest.mark.parametrize('op, dtype, shape, axis', reduce2d_configs)\n def test_reduce2d(op, dtype, shape, axis):\n     dtype = dtype_mapping[dtype]\n-    x = torch.randn(shape, device='cuda', dtype=dtype)\n+    reduced_dtype = get_reduced_dtype(dtype)\n     reduced_shape = (shape[1 - axis],)\n-    z = torch.empty(reduced_shape, device=x.device, dtype=dtype)\n+\n+    if dtype.is_floating_point:\n+        x = torch.randn(shape, device='cuda', dtype=dtype)\n+    elif dtype is torch.uint8:\n+        x = torch.randint(0, 20, shape, device='cuda', dtype=dtype)\n+    else:\n+        x = torch.randint(-20, 20, shape, device='cuda', dtype=dtype)\n+    z = torch.empty(reduced_shape, device=x.device, dtype=reduced_dtype)\n \n     kernel = patch_kernel(reduce2d_kernel, {'OP': op})\n     grid = (1,)\n     kernel[grid](x_ptr=x, z_ptr=z, axis=axis, block_m=shape[0], block_n=shape[1])\n \n     if op == 'sum':\n-        golden_z = torch.sum(x, dim=axis, keepdim=False, dtype=dtype)\n+        golden_z = torch.sum(x, dim=axis, keepdim=False, dtype=reduced_dtype)\n     elif op == 'min':\n-        golden_z = torch.min(x, dim=axis, keepdim=False)[0]\n+        golden_z = torch.min(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n     else:\n-        golden_z = torch.max(x, dim=axis, keepdim=False)[0]\n+        golden_z = torch.max(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n \n-    if op == 'sum':\n+    if dtype.is_floating_point and op == 'sum':\n         if shape[axis] >= 256:\n             assert_close(z, golden_z, rtol=0.05, atol=0.1)\n         elif shape[axis] >= 32:"}, {"filename": "python/triton/language/libdevice.py", "status": "modified", "additions": 167, "deletions": 303, "changes": 470, "file_content_changes": "@@ -58,13 +58,7 @@ def mulhi(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"int32\"), core.dtype(\"int32\"),): (\"__nv_mulhi\", core.dtype(\"int32\")),\n                                (core.dtype(\"uint32\"), core.dtype(\"uint32\"),): (\"__nv_umulhi\", core.dtype(\"uint32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def mul64hi(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"int64\"), core.dtype(\"int64\"),): (\"__nv_mul64hi\", core.dtype(\"int64\")),\n+                               (core.dtype(\"int64\"), core.dtype(\"int64\"),): (\"__nv_mul64hi\", core.dtype(\"int64\")),\n                                (core.dtype(\"uint64\"), core.dtype(\"uint64\"),): (\"__nv_umul64hi\", core.dtype(\"uint64\")),\n                                }, _builder)\n \n@@ -157,262 +151,138 @@ def saturatef(arg0, _builder=None):\n                                }, _builder)\n \n \n-@extern.extern\n-def fmaf_rn(arg0, arg1, arg2, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, arg2, ],\n-                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmaf_rn\", core.dtype(\"fp32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def fmaf_rz(arg0, arg1, arg2, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, arg2, ],\n-                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmaf_rz\", core.dtype(\"fp32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def fmaf_rd(arg0, arg1, arg2, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, arg2, ],\n-                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmaf_rd\", core.dtype(\"fp32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def fmaf_ru(arg0, arg1, arg2, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, arg2, ],\n-                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmaf_ru\", core.dtype(\"fp32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def fmaf_ieee_rn(arg0, arg1, arg2, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, arg2, ],\n-                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmaf_ieee_rn\", core.dtype(\"fp32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def fmaf_ieee_rz(arg0, arg1, arg2, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, arg2, ],\n-                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmaf_ieee_rz\", core.dtype(\"fp32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def fmaf_ieee_rd(arg0, arg1, arg2, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, arg2, ],\n-                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmaf_ieee_rd\", core.dtype(\"fp32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def fmaf_ieee_ru(arg0, arg1, arg2, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, arg2, ],\n-                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmaf_ieee_ru\", core.dtype(\"fp32\")),\n-                               }, _builder)\n-\n-\n @extern.extern\n def fma_rn(arg0, arg1, arg2, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, arg2, ],\n-                              {(core.dtype(\"fp64\"), core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_fma_rn\", core.dtype(\"fp64\")),\n+                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmaf_rn\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"), core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_fma_rn\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n def fma_rz(arg0, arg1, arg2, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, arg2, ],\n-                              {(core.dtype(\"fp64\"), core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_fma_rz\", core.dtype(\"fp64\")),\n+                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmaf_rz\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"), core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_fma_rz\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n def fma_rd(arg0, arg1, arg2, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, arg2, ],\n-                              {(core.dtype(\"fp64\"), core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_fma_rd\", core.dtype(\"fp64\")),\n+                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmaf_rd\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"), core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_fma_rd\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n def fma_ru(arg0, arg1, arg2, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, arg2, ],\n-                              {(core.dtype(\"fp64\"), core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_fma_ru\", core.dtype(\"fp64\")),\n+                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmaf_ru\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"), core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_fma_ru\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n-def fast_fdividef(arg0, arg1, _builder=None):\n+def fast_dividef(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fast_fdividef\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n @extern.extern\n-def fdiv_rn(arg0, arg1, _builder=None):\n+def div_rn(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fdiv_rn\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_ddiv_rn\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n-def fdiv_rz(arg0, arg1, _builder=None):\n+def div_rz(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fdiv_rz\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_ddiv_rz\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n-def fdiv_rd(arg0, arg1, _builder=None):\n+def div_rd(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fdiv_rd\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_ddiv_rd\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n-def fdiv_ru(arg0, arg1, _builder=None):\n+def div_ru(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fdiv_ru\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_ddiv_ru\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n-def frcp_rn(arg0, _builder=None):\n+def rcp_rn(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"fp32\"),): (\"__nv_frcp_rn\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"),): (\"__nv_drcp_rn\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n-def frcp_rz(arg0, _builder=None):\n+def rcp_rz(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"fp32\"),): (\"__nv_frcp_rz\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"),): (\"__nv_drcp_rz\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n-def frcp_rd(arg0, _builder=None):\n+def rcp_rd(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"fp32\"),): (\"__nv_frcp_rd\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"),): (\"__nv_drcp_rd\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n-def frcp_ru(arg0, _builder=None):\n+def rcp_ru(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"fp32\"),): (\"__nv_frcp_ru\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"),): (\"__nv_drcp_ru\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n-def fsqrt_rn(arg0, _builder=None):\n+def sqrt_rn(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"fp32\"),): (\"__nv_fsqrt_rn\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"),): (\"__nv_dsqrt_rn\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n-def fsqrt_rz(arg0, _builder=None):\n+def sqrt_rz(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"fp32\"),): (\"__nv_fsqrt_rz\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"),): (\"__nv_dsqrt_rz\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n-def fsqrt_rd(arg0, _builder=None):\n+def sqrt_rd(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"fp32\"),): (\"__nv_fsqrt_rd\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"),): (\"__nv_dsqrt_rd\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n-def fsqrt_ru(arg0, _builder=None):\n+def sqrt_ru(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"fp32\"),): (\"__nv_fsqrt_ru\", core.dtype(\"fp32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def ddiv_rn(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_ddiv_rn\", core.dtype(\"fp64\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def ddiv_rz(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_ddiv_rz\", core.dtype(\"fp64\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def ddiv_rd(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_ddiv_rd\", core.dtype(\"fp64\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def ddiv_ru(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_ddiv_ru\", core.dtype(\"fp64\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def drcp_rn(arg0, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n-                              {(core.dtype(\"fp64\"),): (\"__nv_drcp_rn\", core.dtype(\"fp64\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def drcp_rz(arg0, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n-                              {(core.dtype(\"fp64\"),): (\"__nv_drcp_rz\", core.dtype(\"fp64\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def drcp_rd(arg0, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n-                              {(core.dtype(\"fp64\"),): (\"__nv_drcp_rd\", core.dtype(\"fp64\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def drcp_ru(arg0, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n-                              {(core.dtype(\"fp64\"),): (\"__nv_drcp_ru\", core.dtype(\"fp64\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def dsqrt_rn(arg0, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n-                              {(core.dtype(\"fp64\"),): (\"__nv_dsqrt_rn\", core.dtype(\"fp64\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def dsqrt_rz(arg0, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n-                              {(core.dtype(\"fp64\"),): (\"__nv_dsqrt_rz\", core.dtype(\"fp64\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def dsqrt_rd(arg0, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n-                              {(core.dtype(\"fp64\"),): (\"__nv_dsqrt_rd\", core.dtype(\"fp64\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def dsqrt_ru(arg0, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n-                              {(core.dtype(\"fp64\"),): (\"__nv_dsqrt_ru\", core.dtype(\"fp64\")),\n+                               (core.dtype(\"fp64\"),): (\"__nv_dsqrt_ru\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n@@ -425,114 +295,66 @@ def sqrt(arg0, _builder=None):\n \n \n @extern.extern\n-def dadd_rn(arg0, arg1, _builder=None):\n+def add_rn(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_dadd_rn\", core.dtype(\"fp64\")),\n+                               (core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fadd_rn\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n @extern.extern\n-def dadd_rz(arg0, arg1, _builder=None):\n+def add_rz(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_dadd_rz\", core.dtype(\"fp64\")),\n+                               (core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fadd_rz\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n @extern.extern\n-def dadd_rd(arg0, arg1, _builder=None):\n+def add_rd(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_dadd_rd\", core.dtype(\"fp64\")),\n+                               (core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fadd_rd\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n @extern.extern\n-def dadd_ru(arg0, arg1, _builder=None):\n+def add_ru(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_dadd_ru\", core.dtype(\"fp64\")),\n+                               (core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fadd_ru\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n @extern.extern\n-def dmul_rn(arg0, arg1, _builder=None):\n+def mul_rn(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_dmul_rn\", core.dtype(\"fp64\")),\n+                               (core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmul_rn\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n @extern.extern\n-def dmul_rz(arg0, arg1, _builder=None):\n+def mul_rz(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_dmul_rz\", core.dtype(\"fp64\")),\n+                               (core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmul_rz\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n @extern.extern\n-def dmul_rd(arg0, arg1, _builder=None):\n+def mul_rd(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_dmul_rd\", core.dtype(\"fp64\")),\n+                               (core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmul_rd\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n @extern.extern\n-def dmul_ru(arg0, arg1, _builder=None):\n+def mul_ru(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_dmul_ru\", core.dtype(\"fp64\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def fadd_rd(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fadd_rd\", core.dtype(\"fp32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def fadd_ru(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fadd_ru\", core.dtype(\"fp32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def fmul_rd(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmul_rd\", core.dtype(\"fp32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def fmul_ru(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmul_ru\", core.dtype(\"fp32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def fadd_rn(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fadd_rn\", core.dtype(\"fp32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def fadd_rz(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fadd_rz\", core.dtype(\"fp32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def fmul_rn(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmul_rn\", core.dtype(\"fp32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def fmul_rz(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmul_rz\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmul_ru\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n@@ -624,7 +446,13 @@ def double2uint_ru(arg0, _builder=None):\n def int2double_rn(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"int32\"),): (\"__nv_int2double_rn\", core.dtype(\"fp64\")),\n-                               (core.dtype(\"uint32\"),): (\"__nv_uint2double_rn\", core.dtype(\"fp64\")),\n+                               }, _builder)\n+\n+\n+@extern.extern\n+def uint2double_rn(arg0, _builder=None):\n+    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n+                              {(core.dtype(\"uint32\"),): (\"__nv_uint2double_rn\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n@@ -688,31 +516,55 @@ def float2uint_ru(arg0, _builder=None):\n def int2float_rn(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"int32\"),): (\"__nv_int2float_rn\", core.dtype(\"fp32\")),\n-                               (core.dtype(\"uint32\"),): (\"__nv_uint2float_rn\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n @extern.extern\n def int2float_rz(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"int32\"),): (\"__nv_int2float_rz\", core.dtype(\"fp32\")),\n-                               (core.dtype(\"uint32\"),): (\"__nv_uint2float_rz\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n @extern.extern\n def int2float_rd(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"int32\"),): (\"__nv_int2float_rd\", core.dtype(\"fp32\")),\n-                               (core.dtype(\"uint32\"),): (\"__nv_uint2float_rd\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n @extern.extern\n def int2float_ru(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"int32\"),): (\"__nv_int2float_ru\", core.dtype(\"fp32\")),\n-                               (core.dtype(\"uint32\"),): (\"__nv_uint2float_ru\", core.dtype(\"fp32\")),\n+                               }, _builder)\n+\n+\n+@extern.extern\n+def uint2float_rn(arg0, _builder=None):\n+    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n+                              {(core.dtype(\"uint32\"),): (\"__nv_uint2float_rn\", core.dtype(\"fp32\")),\n+                               }, _builder)\n+\n+\n+@extern.extern\n+def uint2float_rz(arg0, _builder=None):\n+    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n+                              {(core.dtype(\"uint32\"),): (\"__nv_uint2float_rz\", core.dtype(\"fp32\")),\n+                               }, _builder)\n+\n+\n+@extern.extern\n+def uint2float_rd(arg0, _builder=None):\n+    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n+                              {(core.dtype(\"uint32\"),): (\"__nv_uint2float_rd\", core.dtype(\"fp32\")),\n+                               }, _builder)\n+\n+\n+@extern.extern\n+def uint2float_ru(arg0, _builder=None):\n+    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n+                              {(core.dtype(\"uint32\"),): (\"__nv_uint2float_ru\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n@@ -853,71 +705,118 @@ def double2ull_ru(arg0, _builder=None):\n def ll2float_rn(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"int64\"),): (\"__nv_ll2float_rn\", core.dtype(\"fp32\")),\n-                               (core.dtype(\"uint64\"),): (\"__nv_ull2float_rn\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n @extern.extern\n def ll2float_rz(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"int64\"),): (\"__nv_ll2float_rz\", core.dtype(\"fp32\")),\n-                               (core.dtype(\"uint64\"),): (\"__nv_ull2float_rz\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n @extern.extern\n def ll2float_rd(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"int64\"),): (\"__nv_ll2float_rd\", core.dtype(\"fp32\")),\n-                               (core.dtype(\"uint64\"),): (\"__nv_ull2float_rd\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n @extern.extern\n def ll2float_ru(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"int64\"),): (\"__nv_ll2float_ru\", core.dtype(\"fp32\")),\n-                               (core.dtype(\"uint64\"),): (\"__nv_ull2float_ru\", core.dtype(\"fp32\")),\n+                               }, _builder)\n+\n+\n+@extern.extern\n+def ull2float_rn(arg0, _builder=None):\n+    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n+                              {(core.dtype(\"uint64\"),): (\"__nv_ull2float_rn\", core.dtype(\"fp32\")),\n+                               }, _builder)\n+\n+\n+@extern.extern\n+def ull2float_rz(arg0, _builder=None):\n+    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n+                              {(core.dtype(\"uint64\"),): (\"__nv_ull2float_rz\", core.dtype(\"fp32\")),\n+                               }, _builder)\n+\n+\n+@extern.extern\n+def ull2float_rd(arg0, _builder=None):\n+    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n+                              {(core.dtype(\"uint64\"),): (\"__nv_ull2float_rd\", core.dtype(\"fp32\")),\n+                               }, _builder)\n+\n+\n+@extern.extern\n+def ull2float_ru(arg0, _builder=None):\n+    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n+                              {(core.dtype(\"uint64\"),): (\"__nv_ull2float_ru\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n @extern.extern\n def ll2double_rn(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"int64\"),): (\"__nv_ll2double_rn\", core.dtype(\"fp64\")),\n-                               (core.dtype(\"uint64\"),): (\"__nv_ull2double_rn\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n def ll2double_rz(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"int64\"),): (\"__nv_ll2double_rz\", core.dtype(\"fp64\")),\n-                               (core.dtype(\"uint64\"),): (\"__nv_ull2double_rz\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n def ll2double_rd(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"int64\"),): (\"__nv_ll2double_rd\", core.dtype(\"fp64\")),\n-                               (core.dtype(\"uint64\"),): (\"__nv_ull2double_rd\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n def ll2double_ru(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"int64\"),): (\"__nv_ll2double_ru\", core.dtype(\"fp64\")),\n-                               (core.dtype(\"uint64\"),): (\"__nv_ull2double_ru\", core.dtype(\"fp64\")),\n+                               }, _builder)\n+\n+\n+@extern.extern\n+def ull2double_rn(arg0, _builder=None):\n+    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n+                              {(core.dtype(\"uint64\"),): (\"__nv_ull2double_rn\", core.dtype(\"fp64\")),\n+                               }, _builder)\n+\n+\n+@extern.extern\n+def ull2double_rz(arg0, _builder=None):\n+    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n+                              {(core.dtype(\"uint64\"),): (\"__nv_ull2double_rz\", core.dtype(\"fp64\")),\n+                               }, _builder)\n+\n+\n+@extern.extern\n+def ull2double_rd(arg0, _builder=None):\n+    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n+                              {(core.dtype(\"uint64\"),): (\"__nv_ull2double_rd\", core.dtype(\"fp64\")),\n+                               }, _builder)\n+\n+\n+@extern.extern\n+def ull2double_ru(arg0, _builder=None):\n+    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n+                              {(core.dtype(\"uint64\"),): (\"__nv_ull2double_ru\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n def int_as_float(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"int32\"),): (\"__nv_int_as_float\", core.dtype(\"fp32\")),\n-                               (core.dtype(\"uint32\"),): (\"__nv_uint_as_float\", core.dtype(\"fp32\")),\n                                }, _builder)\n \n \n@@ -928,6 +827,13 @@ def float_as_int(arg0, _builder=None):\n                                }, _builder)\n \n \n+@extern.extern\n+def uint_as_float(arg0, _builder=None):\n+    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n+                              {(core.dtype(\"uint32\"),): (\"__nv_uint_as_float\", core.dtype(\"fp32\")),\n+                               }, _builder)\n+\n+\n @extern.extern\n def float_as_uint(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n@@ -1006,11 +912,9 @@ def fast_log10f(arg0, _builder=None):\n \n \n @extern.extern\n-def pow(arg0, arg1, _builder=None):\n+def fast_powf(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fast_powf\", core.dtype(\"fp32\")),\n-                               (core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_powf\", core.dtype(\"fp32\")),\n-                               (core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_pow\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n@@ -1031,35 +935,39 @@ def rhadd(arg0, arg1, _builder=None):\n \n \n @extern.extern\n-def fsub_rn(arg0, arg1, _builder=None):\n+def sub_rn(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fsub_rn\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_dsub_rn\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n-def fsub_rz(arg0, arg1, _builder=None):\n+def sub_rz(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fsub_rz\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_dsub_rz\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n-def fsub_rd(arg0, arg1, _builder=None):\n+def sub_rd(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fsub_rd\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_dsub_rd\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n-def fsub_ru(arg0, arg1, _builder=None):\n+def sub_ru(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fsub_ru\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_dsub_ru\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n @extern.extern\n-def frsqrt_rn(arg0, _builder=None):\n+def rsqrt_rn(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"fp32\"),): (\"__nv_frsqrt_rn\", core.dtype(\"fp32\")),\n                                }, _builder)\n@@ -1098,16 +1006,18 @@ def nearbyint(arg0, _builder=None):\n \n \n @extern.extern\n-def isnanf(arg0, _builder=None):\n+def isnan(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"fp32\"),): (\"__nv_isnanf\", core.dtype(\"int32\")),\n+                               (core.dtype(\"fp64\"),): (\"__nv_isnand\", core.dtype(\"int32\")),\n                                }, _builder)\n \n \n @extern.extern\n-def signbitf(arg0, _builder=None):\n+def signbit(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"fp32\"),): (\"__nv_signbitf\", core.dtype(\"int32\")),\n+                               (core.dtype(\"fp64\"),): (\"__nv_signbitd\", core.dtype(\"int32\")),\n                                }, _builder)\n \n \n@@ -1127,9 +1037,10 @@ def finitef(arg0, _builder=None):\n \n \n @extern.extern\n-def isinff(arg0, _builder=None):\n+def isinf(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"fp32\"),): (\"__nv_isinff\", core.dtype(\"int32\")),\n+                               (core.dtype(\"fp64\"),): (\"__nv_isinfd\", core.dtype(\"int32\")),\n                                }, _builder)\n \n \n@@ -1550,10 +1461,12 @@ def fma(arg0, arg1, arg2, _builder=None):\n \n \n @extern.extern\n-def powi(arg0, arg1, _builder=None):\n+def pow(arg0, arg1, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n                               {(core.dtype(\"fp32\"), core.dtype(\"int32\"),): (\"__nv_powif\", core.dtype(\"fp32\")),\n                                (core.dtype(\"fp64\"), core.dtype(\"int32\"),): (\"__nv_powi\", core.dtype(\"fp64\")),\n+                               (core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_powf\", core.dtype(\"fp32\")),\n+                               (core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_pow\", core.dtype(\"fp64\")),\n                                }, _builder)\n \n \n@@ -1605,57 +1518,8 @@ def logb(arg0, _builder=None):\n                                }, _builder)\n \n \n-@extern.extern\n-def signbitd(arg0, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n-                              {(core.dtype(\"fp64\"),): (\"__nv_signbitd\", core.dtype(\"int32\")),\n-                               }, _builder)\n-\n-\n @extern.extern\n def isfinited(arg0, _builder=None):\n     return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n                               {(core.dtype(\"fp64\"),): (\"__nv_isfinited\", core.dtype(\"int32\")),\n                                }, _builder)\n-\n-\n-@extern.extern\n-def isinfd(arg0, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n-                              {(core.dtype(\"fp64\"),): (\"__nv_isinfd\", core.dtype(\"int32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def isnand(arg0, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, ],\n-                              {(core.dtype(\"fp64\"),): (\"__nv_isnand\", core.dtype(\"int32\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def dsub_rn(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_dsub_rn\", core.dtype(\"fp64\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def dsub_rz(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_dsub_rz\", core.dtype(\"fp64\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def dsub_ru(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_dsub_ru\", core.dtype(\"fp64\")),\n-                               }, _builder)\n-\n-\n-@extern.extern\n-def dsub_rd(arg0, arg1, _builder=None):\n-    return extern.elementwise(\"libdevice\", LIBDEVICE_PATH, [arg0, arg1, ],\n-                              {(core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_dsub_rd\", core.dtype(\"fp64\")),\n-                               }, _builder)"}, {"filename": "python/triton/tools/build_extern.py", "status": "added", "additions": 348, "deletions": 0, "changes": 348, "file_content_changes": "@@ -0,0 +1,348 @@\n+import argparse\n+import subprocess\n+from abc import ABC, abstractmethod\n+\n+\n+class Symbol:\n+    def __init__(self, name: str, op_name: str, ret_type: str, arg_names: list, arg_types: list) -> None:\n+        '''\n+        A symbol is a function declaration.\n+        :param name: name of the symbol\n+        :param op_name: name of the operation\n+        :param ret_type: return type of the operation\n+        :param arg_names: names of the arguments\n+        :param arg_types: types of the arguments\n+        '''\n+        self._name = name\n+        self._op_name = op_name\n+        self._ret_type = ret_type\n+        self._arg_names = arg_names\n+        self._arg_types = arg_types\n+\n+    @property\n+    def name(self):\n+        return self._name\n+\n+    @property\n+    def op_name(self):\n+        return self._op_name\n+\n+    @property\n+    def ret_type(self):\n+        return self._ret_type\n+\n+    @property\n+    def arg_names(self):\n+        return self._arg_names\n+\n+    @property\n+    def arg_types(self):\n+        return self._arg_types\n+\n+\n+def convert_type(type_str):\n+    if type_str == \"i32\":\n+        return \"int32\"\n+    elif type_str == \"u32\":\n+        return \"uint32\"\n+    elif type_str == \"i64\":\n+        return \"int64\"\n+    elif type_str == \"u64\":\n+        return \"uint64\"\n+    elif type_str == \"float\":\n+        return \"fp32\"\n+    elif type_str == \"double\":\n+        return \"fp64\"\n+    else:\n+        # ignore other types, such as pointer types\n+        return None\n+\n+\n+def to_unsigned(type_str):\n+    if type_str == \"int32\":\n+        return \"uint32\"\n+    elif type_str == \"int64\":\n+        return \"uint64\"\n+    else:\n+        return type_str\n+\n+\n+class ExternLibrary(ABC):\n+    def __init__(self, name: str, path: str, format: bool = True, grouping: bool = True) -> None:\n+        '''\n+        Abstract class for extern library.\n+        :param name: name of the library\n+        :param path: path of the library\n+        :param format: whether to format the generated stub file\n+        '''\n+        self._name = name\n+        self._path = path\n+        self._symbols = {}\n+        self._format = True\n+        self._grouping = grouping\n+\n+    @property\n+    def name(self):\n+        return self._name\n+\n+    @property\n+    def path(self):\n+        return self._path\n+\n+    @property\n+    def symbols(self):\n+        return self._symbols\n+\n+    @property\n+    def grouping(self):\n+        return self._grouping\n+\n+    @abstractmethod\n+    def parse_symbols(self, input_file):\n+        pass\n+\n+    @abstractmethod\n+    def _output_stubs(self) -> str:\n+        pass\n+\n+    def generate_stub_file(self, output_dir):\n+        file_str = self._output_stubs()\n+        if file_str is None or len(file_str) == 0:\n+            raise Exception(\"file_str is empty\")\n+\n+        output_file = f\"{output_dir}/{self._name}.py\"\n+        with open(output_file, \"w\") as f:\n+            f.write(file_str)\n+            f.close()\n+            if self._format:\n+                subprocess.Popen([\"autopep8\", \"-a\", \"-r\", \"-i\", output_file],\n+                                 stdout=subprocess.PIPE).communicate()\n+                subprocess.Popen([\"isort\", output_file], stdout=subprocess.PIPE).communicate()\n+\n+\n+class Libdevice(ExternLibrary):\n+    def __init__(self, path) -> None:\n+        '''\n+        Constructor for Libdevice.\n+        :param path: path of the libdevice library\n+        '''\n+        super().__init__(\"libdevice\", path)\n+        self._symbol_groups = {}\n+\n+    def _extract_symbol(self, line):\n+        # Extract symbols from line in the following format:\n+        # \"define [internal] <ret_type> @<name>(<arg_types>,)\"\n+        entries = line.split(\"@\")\n+        ret_str = entries[0]\n+        func_str = entries[1]\n+        # Get ret_type, skip internal symbols\n+        ret_strs = ret_str.split()\n+        if ret_strs[1] == \"internal\":\n+            return None\n+        ret_type = convert_type(ret_strs[1])\n+        if ret_type is None:\n+            return None\n+        # Get function name\n+        func_strs = func_str.split(\"(\")\n+        func_name = func_strs[0].replace(\"@\", \"\")\n+        op_name = func_name.replace(\"__nv_\", \"\")\n+        if 'ieee' in op_name:\n+            return None\n+        # Get arg_types\n+        arg_strs = func_strs[1].split(\",\")\n+        arg_types = []\n+        arg_names = []\n+        for i, arg_str in enumerate(arg_strs):\n+            arg_type = convert_type(arg_str.split()[0])\n+            if arg_type is None:\n+                return None\n+            arg_name = 'arg' + str(i)\n+            arg_types.append(arg_type)\n+            arg_names.append(arg_name)\n+        if op_name == \"sad\":\n+            # Special case for sad, where the last argument is an unsigned int\n+            arg_types[-1] = to_unsigned(arg_types[-1])\n+        elif op_name.startswith(\"u\"):\n+            # LLVM does not differentiate between signed and unsigned integer type.\n+            # We have to convert the types to unsigned\n+            ret_type = to_unsigned(ret_type)\n+            for i, arg_type in enumerate(arg_types):\n+                arg_types[i] = to_unsigned(arg_type)\n+        return Symbol(func_name, op_name, ret_type, arg_names, arg_types)\n+\n+    def _group_symbols(self):\n+        symbol_set = {}\n+        for symbol in self._symbols.values():\n+            op_name = symbol.op_name\n+            symbol_set[op_name] = symbol\n+\n+        # Group functions together by renaming.\n+        renaming = {\n+            'llabs': 'abs', 'acosf': 'acos', 'acoshf': 'acosh',\n+            'dadd_rd': 'add_rd', 'fadd_rd': 'add_rd', 'dadd_rn': 'add_rn',\n+            'fadd_rn': 'add_rn', 'dadd_ru': 'add_ru', 'fadd_ru': 'add_ru',\n+            'dadd_rz': 'add_rz', 'fadd_rz': 'add_rz', 'asinf': 'asin',\n+            'asinhf': 'asinh', 'atanf': 'atan', 'atan2f': 'atan2',\n+            'atanhf': 'atanh', 'brevll': 'brev', 'cbrtf': 'cbrt',\n+            'ceilf': 'ceil', 'clzll': 'clz', 'copysignf': 'copysign',\n+            'cosf': 'cos', 'coshf': 'cosh', 'cospif': 'cospi',\n+            'cyl_bessel_i0f': 'cyl_bessel_i0', 'cyl_bessel_i1f': 'cyl_bessel_i1',\n+            'fdiv_rd': 'div_rd', 'ddiv_rd': 'div_rd', 'fdiv_rn': 'div_rn',\n+            'ddiv_rn': 'div_rn', 'fdiv_ru': 'div_ru', 'ddiv_ru': 'div_ru',\n+            'fdiv_rz': 'div_rz', 'ddiv_rz': 'div_rz', 'erff': 'erf',\n+            'erfcf': 'erfc', 'erfcinvf': 'erfcinv', 'erfcxf': 'erfcx',\n+            'erfinvf': 'erfinv', 'expf': 'exp', 'exp10f': 'exp10',\n+            'exp2f': 'exp2', 'expm1f': 'expm1', 'fabsf': 'abs',\n+            'fabs': 'abs', 'fast_fdividef': 'fast_dividef',\n+            'fdimf': 'fdim', 'ffsll': 'ffs', 'floorf': 'floor',\n+            'fmaf': 'fma', 'fmaf_rd': 'fma_rd', 'fmaf_rn': 'fma_rn',\n+            'fmaf_ru': 'fma_ru', 'fmaf_rz': 'fma_rz', 'fmodf': 'fmod',\n+            'uhadd': 'hadd', 'hypotf': 'hypot', 'ilogbf': 'ilogb',\n+            'isinff': 'isinf', 'isinfd': 'isinf', 'isnanf': 'isnan',\n+            'isnand': 'isnan', 'j0f': 'j0', 'j1f': 'j1', 'jnf': 'jn',\n+            'ldexpf': 'ldexp', 'lgammaf': 'lgamma', 'llrintf': 'llrint',\n+            'llroundf': 'llround', 'logf': 'log', 'log10f': 'log10',\n+            'log1pf': 'log1p', 'log2f': 'log2', 'logbf': 'logb',\n+            'umax': 'max', 'llmax': 'max', 'ullmax': 'max', 'fmaxf': 'max',\n+            'fmax': 'max', 'umin': 'min', 'llmin': 'min', 'ullmin': 'min',\n+            'fminf': 'min', 'fmin': 'min', 'dmul_rd': 'mul_rd', 'fmul_rd': 'mul_rd',\n+            'dmul_rn': 'mul_rn', 'fmul_rn': 'mul_rn', 'dmul_ru': 'mul_ru',\n+            'fmul_ru': 'mul_ru', 'dmul_rz': 'mul_rz', 'fmul_rz': 'mul_rz',\n+            'umul24': 'mul24', 'umulhi': 'mulhi', 'mul64hi': 'mulhi',\n+            'umul64hi': 'mulhi', 'nearbyintf': 'nearbyint', 'nextafterf': 'nextafter',\n+            'norm3df': 'norm3d', 'norm4df': 'norm4d', 'normcdff': 'normcdf',\n+            'normcdfinvf': 'normcdfinv', 'popcll': 'popc', 'powif': 'pow', 'powi': 'pow',\n+            'powf': 'pow', 'rcbrtf': 'rcbrt', 'frcp_rd': 'rcp_rd', 'drcp_rd': 'rcp_rd',\n+            'frcp_rn': 'rcp_rn', 'drcp_rn': 'rcp_rn', 'frcp_ru': 'rcp_ru',\n+            'drcp_ru': 'rcp_ru', 'frcp_rz': 'rcp_rz', 'drcp_rz': 'rcp_rz',\n+            'remainderf': 'remainder', 'urhadd': 'rhadd', 'rhypotf': 'rhypot',\n+            'rintf': 'rint', 'rnorm3df': 'rnorm3d', 'rnorm4df': 'rnorm4d',\n+            'roundf': 'round', 'rsqrtf': 'rsqrt', 'frsqrt_rn': 'rsqrt_rn',\n+            'usad': 'sad', 'scalbnf': 'scalbn', 'signbitf': 'signbit',\n+            'signbitd': 'signbit', 'sinf': 'sin', 'sinhf': 'sinh',\n+            'sinpif': 'sinpi', 'sqrtf': 'sqrt', 'fsqrt_rd': 'sqrt_rd',\n+            'dsqrt_rd': 'sqrt_rd', 'fsqrt_rn': 'sqrt_rn', 'dsqrt_rn': 'sqrt_rn',\n+            'fsqrt_ru': 'sqrt_ru', 'dsqrt_ru': 'sqrt_ru', 'fsqrt_rz': 'sqrt_rz',\n+            'dsqrt_rz': 'sqrt_rz', 'fsub_rd': 'sub_rd', 'dsub_rd': 'sub_rd',\n+            'fsub_rn': 'sub_rn', 'dsub_rn': 'sub_rn', 'fsub_ru': 'sub_ru',\n+            'dsub_ru': 'sub_ru', 'fsub_rz': 'sub_rz', 'dsub_rz': 'sub_rz',\n+            'tanf': 'tan', 'tanhf': 'tanh', 'tgammaf': 'tgamma', 'truncf': 'trunc',\n+            'y0f': 'y0', 'y1f': 'y1', 'ynf': 'yn'\n+        }\n+\n+        for symbol in self._symbols.values():\n+            op_name = symbol.op_name\n+            if op_name in renaming:\n+                op_name = renaming[op_name]\n+                symbol._op_name = op_name\n+            if op_name in self._symbol_groups:\n+                self._symbol_groups[op_name].append(symbol)\n+            else:\n+                self._symbol_groups[op_name] = [symbol]\n+\n+    def parse_symbols(self, input_file):\n+        if len(self.symbols) > 0:\n+            return\n+        output = subprocess.check_output([\"grep\", \"define\", input_file]).decode().splitlines()\n+        for line in output:\n+            symbol = self._extract_symbol(line)\n+            if symbol is None:\n+                continue\n+            self._symbols[symbol.name] = symbol\n+\n+        self._group_symbols()\n+\n+    def _output_stubs(self):\n+        # Generate python functions in the following format:\n+        # @extern.extern\n+        # def <op_name>(<args>, _builder=None):\n+        #   arg_type_symbol_dict = {[arg_type]: {(symbol, ret_type)}}\n+        #   return extern.dispatch(\"libdevice\", <path>, <args>, <arg_type_symbol_dict>, _builder)\n+        import_str = \"from . import core, extern\\n\"\n+        import_str += \"import os\\n\"\n+        header_str = \"LIBDEVICE_PATH = os.path.dirname(\\n\\tos.path.abspath(__file__)) + \\\"/libdevice.10.bc\\\"\\n\"\n+        func_str = \"\"\n+        for symbols in self._symbol_groups.values():\n+            func_str += \"@extern.extern\\n\"\n+            func_name_str = f\"def {symbols[0].op_name}(\"\n+            for arg_name in symbols[0].arg_names:\n+                func_name_str += f\"{arg_name}, \"\n+            func_name_str += \"_builder=None):\\n\"\n+\n+            return_str = f\"\\treturn extern.elementwise(\\\"{self._name}\\\", LIBDEVICE_PATH, [\"\n+            for arg_name in symbols[0].arg_names:\n+                return_str += f\"{arg_name}, \"\n+            return_str += \"], \\n\"\n+\n+            arg_type_symbol_dict_str = \"{\"\n+            for symbol in symbols:\n+                arg_type_symbol_dict_str += \"(\"\n+                for arg_type in symbol.arg_types:\n+                    arg_type_symbol_dict_str += f'core.dtype(\"{arg_type}\"),'\n+                ret_type = f'core.dtype(\"{symbol.ret_type}\")'\n+                arg_type_symbol_dict_str += \"): (\\\"\" + symbol.name + \"\\\", \" + ret_type + \"),\\n\"\n+            arg_type_symbol_dict_str += \"}\"\n+\n+            return_str += arg_type_symbol_dict_str\n+            return_str += \", _builder)\\n\"\n+\n+            func_str += func_name_str + return_str + \"\\n\"\n+        file_str = import_str + header_str + func_str\n+\n+        return file_str\n+\n+\n+class LLVMDisassembler:\n+    def __init__(self, path):\n+        '''\n+        Invoke llvm-dis to disassemble the given file.\n+        :param path: path to llvm-dis\n+        '''\n+        self._path = path\n+        self._ll_file = \"/tmp/extern_lib.ll\"\n+\n+    def disasm(self, lib_path):\n+        subprocess.Popen([self._path, lib_path, \"-o\", self.ll_file],\n+                         stdout=subprocess.PIPE).communicate()\n+\n+    @property\n+    def ll_file(self):\n+        return self._ll_file\n+\n+    @property\n+    def path(self):\n+        return self._path\n+\n+\n+extern_libs = [\"libdevice\"]\n+\n+\n+def build(llvm_dis_path, lib_path, lib_name, output_dir):\n+    '''\n+      Interface function to build the library file.\n+      :param llvm_dis_path: path to the llvm-dis binary\n+      :param lib_path: path to the external library file\n+      :param lib_name: name of the library\n+      :param output_dir: path to the output directory\n+    '''\n+    if lib_name == \"libdevice\":\n+        extern_lib = Libdevice(lib_path)\n+    else:\n+        raise Exception(f\"Unknown extern library: {lib_name}\")\n+\n+    llvm_disassembler = LLVMDisassembler(llvm_dis_path)\n+    llvm_disassembler.disasm(lib_path)\n+\n+    extern_lib.parse_symbols(llvm_disassembler.ll_file)\n+    extern_lib.generate_stub_file(output_dir)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--llvm-dis\", dest=\"llvm_dis_path\", help=\"Path to llvm-dis\", default=\"llvm-dis\")\n+    parser.add_argument(\"--lib-path\", dest=\"lib_path\", help=\"Path to the extern library\")\n+    parser.add_argument(\"--lib-name\", dest=\"lib_name\", help=\"Name of the extern library\")\n+    parser.add_argument(\"--output\", dest=\"output_dir\", help=\"Output file path\", default=\"/tmp/\")\n+    args = parser.parse_args()\n+\n+    build(args.llvm_dis_path, args.lib_path, args.lib_name, args.output_dir)"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 25, "deletions": 0, "changes": 25, "file_content_changes": "@@ -764,3 +764,28 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     return\n   }\n }\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n+#mma = #triton_gpu.mma<{version = 1, warpsPerCTA = [2, 2]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  func @matmul884_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  %a:tensor<128x32xf16, #shared>, %b:tensor<32x256xf16, #shared>) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<128x256xf32, #mma>\n+    // CHECK: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n+    %a_mat = triton_gpu.convert_layout %a : (tensor<128x32xf16, #shared>) -> tensor<128x32xf16, #dot_operand_a>\n+    %b_mat = triton_gpu.convert_layout %b : (tensor<32x256xf16, #shared>) -> tensor<32x256xf16, #dot_operand_b>\n+\n+    %28 = tt.dot %a_mat, %b_mat, %cst {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #dot_operand_a> * tensor<32x256xf16, #dot_operand_b> -> tensor<128x256xf32, #mma>\n+    // TODO[goostavz]: uncomment the following lines after convert_layout[mma<v1> -> blocked] is ready.\n+    // %38 = triton_gpu.convert_layout %28 : (tensor<128x256xf32, #mma>) -> tensor<128x256xf32, #blocked>\n+    // %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #blocked>\n+    // %36 = tt.broadcast %30 : (tensor<128x1x!tt.ptr<f32>, #blocked>) -> tensor<128x256x!tt.ptr<f32>, #blocked>\n+    // tt.store %36, %38 : tensor<128x256xf32, #blocked>\n+    return\n+  }\n+}"}]