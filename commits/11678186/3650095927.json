[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -88,9 +88,7 @@ jobs:\n       - name: Run python tests on V100\n         if: ${{matrix.runner[0] == 'self-hosted' && matrix.runner[1] == 'V100'}}\n         run: |\n-          # TODO[Superjomn]: Remove the forloop-unroll setting after pipeline pass works\n           cd python/tests\n-          export TRITON_STATIC_LOOP_UNROLLING=1\n           pytest test_gemm.py::test_gemm_for_mmav1\n \n       - name: Run CXX unittests"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 88, "deletions": 81, "changes": 169, "file_content_changes": "@@ -43,12 +43,51 @@ using ::mlir::triton::gpu::SharedEncodingAttr;\n struct DotOpMmaV1ConversionHelper {\n   MmaEncodingAttr mmaLayout;\n   ArrayRef<unsigned> wpt;\n+  static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n \n   using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n \n   explicit DotOpMmaV1ConversionHelper(MmaEncodingAttr mmaLayout)\n       : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()) {}\n \n+  // Help to share some variables across multiple functions for A.\n+  struct AParam {\n+    SmallVector<int> rep;\n+    SmallVector<int> spw;\n+\n+    // TODO[Superjomn]: Support the case when isAVec4=false later\n+    // Currently, we only support ld.v2, for the mma layout varies with\n+    // different ld vector width.\n+    // bool isAVec4 = !isARow && shapeTransed[orderTransed[0]] <= 16;\n+    const bool isAVec4{true};\n+\n+    explicit AParam(bool isARow) {\n+      int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+      int repM = 2 * packSize0;\n+      int repK = 1;\n+      int spwM = fpw[0] * 4 * repM;\n+      rep.assign({repM, 0, repK});\n+      spw.assign({spwM, 0, 1});\n+    }\n+  };\n+\n+  // Help to share some variables across multiple functions for A.\n+  struct BParam {\n+    SmallVector<int> rep;\n+    SmallVector<int> spw;\n+    // TODO[Superjomn]: Support the case when isBVec4=false later\n+    // Currently, we only support ld.v2, for the mma layout varies with\n+    // different ld vector width.\n+    // bool isBVec4 = isBRow && shapeTransed[orderTransed[0]] <= 16;\n+    const bool isBVec4{true};\n+\n+    explicit BParam(bool isBRow) {\n+      int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+      rep.assign({0, 2 * packSize1, 1});\n+      spw.assign({0, fpw[1] * 4 * rep[1], 1});\n+    }\n+  };\n+\n   int getRepM(int M) const {\n     return std::max<int>(M / (wpt[0] * instrShape[0]), 1);\n   }\n@@ -65,29 +104,34 @@ struct DotOpMmaV1ConversionHelper {\n     return struct_ty(SmallVector<Type>{8, fp32Ty});\n   }\n \n-  // number of fp16x2 elements for $a.\n-  int numElemsPerThreadA(RankedTensorType tensorTy) const {\n-    auto shape = tensorTy.getShape();\n-    auto order = getOrder();\n+  // Get the number of fp16x2 elements for $a.\n+  // \\param shapeTransed: the shape or reordered shape if transpose needed.\n+  // \\param orderTransed: the order or reordered order if transpose needed.\n+  unsigned getNumM(ArrayRef<int64_t> shapeTransed,\n+                   ArrayRef<unsigned> orderTransed) const {\n+    bool isARow = orderTransed[0] != 0;\n+    AParam param(isARow);\n \n-    bool isARow = order[0] != 0;\n-    bool isAVec4 = !isARow && shape[order[0]] <= 16; // fp16*4 = 16bytes\n-    // TODO[Superjomn]: Support the case when isAVec4=false later\n-    // Currently, we only support ld.v2, for the mma layout varies with\n-    // different ld vector width.\n-    isAVec4 = true;\n+    unsigned numM = param.rep[0] * shapeTransed[0] / (param.spw[0] * wpt[0]);\n+    return numM;\n+  }\n \n-    int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+  // Get the number of fp16x2 elements for $b.\n+  // \\param shapeTransed: the shape or reordered shape if transpose needed.\n+  // \\param orderTransed: the order or reordered order if transpose needed.\n+  unsigned getNumN(ArrayRef<int64_t> shapeTransed,\n+                   ArrayRef<unsigned> orderTransed) const {\n+    bool isBRow = orderTransed[0] != 0;\n+    BParam param(isBRow);\n \n-    SmallVector<int> fpw({2, 2, 1});\n-    int repM = 2 * packSize0;\n-    int repK = 1;\n-    int spwM = fpw[0] * 4 * repM;\n-    SmallVector<int> rep({repM, 0, repK}); // pad N with 0\n-    SmallVector<int> spw({spwM, 0, 1});    // pad N with 0\n+    unsigned numN = param.rep[1] * shapeTransed[1] / (param.spw[1] * wpt[1]);\n+    return numN;\n+  }\n \n-    int NK = shape[1];\n-    unsigned numM = rep[0] * shape[0] / (spw[0] * wpt[0]);\n+  int numElemsPerThreadA(ArrayRef<int64_t> shapeTransed,\n+                         ArrayRef<unsigned> orderTransed) const {\n+    int numM = getNumM(shapeTransed, orderTransed);\n+    int NK = shapeTransed[1];\n \n     // NOTE: We couldn't get the vec from the shared layout.\n     // int vecA = sharedLayout.getVec();\n@@ -97,39 +141,27 @@ struct DotOpMmaV1ConversionHelper {\n     return (numM / 2) * (NK / 4) * elemsPerLd;\n   }\n \n-  // number of fp16x2 elements for $b.\n-  int numElemsPerThreadB(RankedTensorType tensorTy) const {\n-    auto shape = tensorTy.getShape();\n-    auto order = getOrder();\n-    bool isBRow = order[0] != 0;\n-    bool isBVec4 = isBRow && shape[order[0]] <= 16;\n-    // TODO[Superjomn]: Support the case when isBVec4=false later\n-    // Currently, we only support ld.v2, for the mma layout varies with\n-    // different ld vector width.\n-    isBVec4 = true;\n-\n-    int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n-    SmallVector<int> fpw({2, 2, 1});\n-    SmallVector<int> rep({0, 2 * packSize1, 1});       // pad M with 0\n-    SmallVector<int> spw({0, fpw[1] * 4 * rep[1], 1}); // pad M with 0\n+  int numElemsPerThreadB(ArrayRef<int64_t> shapeTransed,\n+                         ArrayRef<unsigned> orderTransed) const {\n+    unsigned numN = getNumN(shapeTransed, orderTransed);\n+    int NK = shapeTransed[0];\n     // NOTE: We couldn't get the vec from the shared layout.\n     // int vecB = sharedLayout.getVec();\n     // TODO[Superjomn]: Consider the case when vecA > 4\n     bool vecGt4 = false;\n     int elemsPerLd = vecGt4 ? 4 : 2;\n-    int NK = shape[0];\n-\n-    unsigned numN = rep[1] * shape[1] / (spw[1] * wpt[0]);\n     return (numN / 2) * (NK / 4) * elemsPerLd;\n   }\n \n   // Loading $a from smem to registers, returns a LLVM::Struct.\n-  Value loadA(Value A, const SharedMemoryObject &smemObj, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const;\n+  Value loadA(Value A, bool transA, const SharedMemoryObject &smemObj,\n+              Value thread, Location loc,\n+              ConversionPatternRewriter &rewriter) const;\n \n   // Loading $b from smem to registers, returns a LLVM::Struct.\n-  Value loadB(Value B, const SharedMemoryObject &smemObj, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const;\n+  Value loadB(Value B, bool transB, const SharedMemoryObject &smemObj,\n+              Value thread, Location loc,\n+              ConversionPatternRewriter &rewriter) const;\n \n   static ArrayRef<unsigned> getOrder() { return mmaOrder; }\n \n@@ -1321,8 +1353,8 @@ struct DotOpFMAConversionHelper {\n };\n \n Value DotOpMmaV1ConversionHelper::loadA(\n-    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n-    ConversionPatternRewriter &rewriter) const {\n+    Value tensor, bool transA, const SharedMemoryObject &smemObj, Value thread,\n+    Location loc, ConversionPatternRewriter &rewriter) const {\n \n   auto *ctx = rewriter.getContext();\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n@@ -1336,24 +1368,11 @@ Value DotOpMmaV1ConversionHelper::loadA(\n   Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n \n   bool isARow = order[0] != 0;\n-  bool isAVec4 = !isARow && shape[order[0]] <= 16; // fp16*4 = 16bytes\n-  // TODO[Superjomn]: Support the case when isAVec4=false later\n-  // Currently, we only support ld.v2, for the mma layout varies with different\n-  // ld vector width.\n-  isAVec4 = true;\n-  int packSize0 = (isARow || isAVec4) ? 1 : 2;\n-\n-  SmallVector<int> fpw({2, 2, 1});\n-  int repM = 2 * packSize0;\n-  int repK = 1;\n-  int spwM = fpw[0] * 4 * repM;\n-  SmallVector<int> rep({repM, 0, repK}); // pad N with 0\n-  SmallVector<int> spw({spwM, 0, 1});    // pad N with 0\n-\n-  auto [offsetAM, offsetAK, _0, _1] =\n-      computeOffsets(thread, isARow, false, fpw, spw, rep, rewriter, loc);\n-  // TODO [Superjomn]: transA cannot be accessed in ConvertLayoutOp.\n-  bool transA = false;\n+  AParam param(isARow);\n+\n+  auto [offsetAM, offsetAK, _0, _1] = computeOffsets(\n+      thread, isARow, false, fpw, param.spw, param.rep, rewriter, loc);\n+\n   if (transA) {\n     std::swap(shape[0], shape[1]);\n     std::swap(offsetAM, offsetAK);\n@@ -1401,8 +1420,6 @@ Value DotOpMmaV1ConversionHelper::loadA(\n   for (int i = 0; i < numPtrA; i++)\n     ptrA[i] = gep(ptr_ty(f16_ty), smemBase, offA[i]);\n \n-  unsigned numM = std::max<int>(rep[0] * shape[0] / (spw[0] * wpt[0]), 1);\n-\n   Type f16PtrTy = ptr_ty(f16_ty);\n \n   auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n@@ -1434,6 +1451,7 @@ Value DotOpMmaV1ConversionHelper::loadA(\n     }\n   };\n \n+  unsigned numM = getNumM(shape, order);\n   for (unsigned k = 0; k < NK; k += 4)\n     for (unsigned m = 0; m < numM / 2; ++m)\n       loadA(m, k);\n@@ -1451,8 +1469,8 @@ Value DotOpMmaV1ConversionHelper::loadA(\n }\n \n Value DotOpMmaV1ConversionHelper::loadB(\n-    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n-    ConversionPatternRewriter &rewriter) const {\n+    Value tensor, bool transB, const SharedMemoryObject &smemObj, Value thread,\n+    Location loc, ConversionPatternRewriter &rewriter) const {\n   // smem\n   auto strides = smemObj.strides;\n \n@@ -1467,29 +1485,18 @@ Value DotOpMmaV1ConversionHelper::loadB(\n \n   Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n   bool isBRow = order[0] != 0;\n-  bool isBVec4 = isBRow && shape[order[0]] <= 16;\n-  // TODO[Superjomn]: Support the case when isBVec4=false later\n-  // Currently, we only support ld.v2, for the mma layout varies with different\n-  // ld vector width.\n-  isBVec4 = true;\n-  int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n-  SmallVector<int> fpw({2, 2, 1});\n-  SmallVector<int> rep({0, 2 * packSize1, 1});       // pad M with 0\n-  SmallVector<int> spw({0, fpw[1] * 4 * rep[1], 1}); // pad M with 0\n-  int vecB = sharedLayout.getVec();\n+  BParam param(isBRow);\n \n+  int vecB = sharedLayout.getVec();\n   Value strideBN = isBRow ? i32_val(1) : strides[1];\n   Value strideBK = isBRow ? strides[0] : i32_val(1);\n   Value strideB0 = isBRow ? strideBN : strideBK;\n   Value strideB1 = isBRow ? strideBK : strideBN;\n   int strideRepN = wpt[1] * fpw[1] * 8;\n   int strideRepK = 1;\n \n-  // TODO [Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n-  bool transB = false;\n-\n-  auto [_0, _1, offsetBN, offsetBK] =\n-      computeOffsets(thread, false, isBRow, fpw, spw, rep, rewriter, loc);\n+  auto [_0, _1, offsetBN, offsetBK] = computeOffsets(\n+      thread, false, isBRow, fpw, param.spw, param.rep, rewriter, loc);\n   if (transB) {\n     std::swap(order[0], order[1]);\n     std::swap(shape[0], shape[1]);\n@@ -1556,7 +1563,7 @@ Value DotOpMmaV1ConversionHelper::loadB(\n     }\n   };\n \n-  unsigned numN = rep[1] * shape[1] / (spw[1] * wpt[0]);\n+  unsigned numN = getNumN(shape, order);\n   for (unsigned k = 0; k < NK; k += 4)\n     for (unsigned n = 0; n < numN / 2; ++n) {\n       if (!hbs.count({n, k}))"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 34, "deletions": 16, "changes": 50, "file_content_changes": "@@ -1730,9 +1730,9 @@ struct CatOpConversion : public ConvertTritonGPUOpToLLVMPattern<CatOp> {\n     auto rhsVals = getElementsFromStruct(loc, adaptor.rhs(), rewriter);\n     // concatenate (and potentially reorder) values\n     SmallVector<Value> retVals;\n-    for(Value v: lhsVals)\n+    for (Value v : lhsVals)\n       retVals.push_back(v);\n-    for(Value v: rhsVals)\n+    for (Value v : rhsVals)\n       retVals.push_back(v);\n     // pack and replace\n     Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n@@ -3426,14 +3426,16 @@ Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n   } else if (!isOuter && mmaLayout.getVersion() == 1 &&\n              isHMMA) { // tensor core v1\n     DotOpMmaV1ConversionHelper helper(mmaLayout);\n-    if (dotOperandLayout.getOpIdx() == 0) {\n-      // operand $a\n-      res =\n-          helper.loadA(src, smemObj, getThreadId(rewriter, loc), loc, rewriter);\n-    } else if (dotOperandLayout.getOpIdx() == 1) {\n-      // operand $b\n-      res =\n-          helper.loadB(src, smemObj, getThreadId(rewriter, loc), loc, rewriter);\n+    if (dotOperandLayout.getOpIdx() == 0) { // operand $a\n+      // TODO[Superjomn]: transA is not available here.\n+      bool transA = false;\n+      res = helper.loadA(src, transA, smemObj, getThreadId(rewriter, loc), loc,\n+                         rewriter);\n+    } else if (dotOperandLayout.getOpIdx() == 1) { // operand $b\n+      // TODO[Superjomn]: transB is not available here.\n+      bool transB = false;\n+      res = helper.loadB(src, transB, smemObj, getThreadId(rewriter, loc), loc,\n+                         rewriter);\n     }\n   } else {\n     assert(false && \"Unsupported mma layout found\");\n@@ -3555,6 +3557,10 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n   bool isBRow = BOrder[0] != 0;\n   bool isAVec4 = !isARow && AShape[isARow] <= 16; // fp16*4 = 16bytes\n   bool isBVec4 = isBRow && BShape[isBRow] <= 16;\n+  // TODO[Superjomn]: ld.v4 is not supported.\n+  isAVec4 = true;\n+  isBVec4 = true;\n+\n   int packSize0 = (isARow || isAVec4) ? 1 : 2;\n   int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n   SmallVector<int> fpw({2, 2, 1});\n@@ -3567,7 +3573,7 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n   DotOpMmaV1ConversionHelper helper(mmaLayout);\n \n   unsigned numM = rep[0] * DShape[0] / (spw[0] * wpt[0]);\n-  unsigned numN = rep[1] * DShape[1] / (spw[1] * wpt[0]);\n+  unsigned numN = rep[1] * DShape[1] / (spw[1] * wpt[1]);\n   unsigned NK = AShape[1];\n \n   auto has = helper.extractLoadedOperand(loadedA, NK, rewriter);\n@@ -3854,7 +3860,8 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n   llvm::Optional<Type> convertTritonTensorType(RankedTensorType type) {\n     auto ctx = type.getContext();\n     Attribute layout = type.getEncoding();\n-    auto shape = type.getShape();\n+    SmallVector<int64_t> shape(type.getShape().begin(), type.getShape().end());\n+\n     if (layout &&\n         (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n          layout.isa<MmaEncodingAttr>())) {\n@@ -3917,13 +3924,22 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n         if (mmaLayout.getVersion() == 1) {\n           DotOpMmaV1ConversionHelper helper(mmaLayout);\n \n+          // TODO[Superjomn]: Both transA and transB are not available here.\n+          bool trans = false;\n+          // TODO[Superjomn]: The order of A and B are not available here.\n+          SmallVector<unsigned> order({1, 0});\n+          if (trans) {\n+            std::swap(shape[0], shape[1]);\n+            std::swap(order[0], order[1]);\n+          }\n+\n           if (dotOpLayout.getOpIdx() == 0) { // $a\n-            int elems = helper.numElemsPerThreadA(type);\n+            int elems = helper.numElemsPerThreadA(shape, order);\n             Type x2Ty = vec_ty(elemTy, 2);\n             return struct_ty(SmallVector<Type>(elems, x2Ty));\n           }\n           if (dotOpLayout.getOpIdx() == 1) { // $b\n-            int elems = helper.numElemsPerThreadB(type);\n+            int elems = helper.numElemsPerThreadB(shape, order);\n             Type x2Ty = vec_ty(elemTy, 2);\n             return struct_ty(SmallVector<Type>(elems, x2Ty));\n           }\n@@ -4422,6 +4438,7 @@ struct AtomicRMWOpConversion\n             rewriter, loc, getTypeConverter()->getIndexType(), ii);\n         rmwVal = insert_element(vecTy, rmwVal, valElements[i + ii], iiVal);\n       }\n+\n       Value rmwPtr = ptrElements[i];\n       Value rmwMask = maskElements[i];\n       rmwMask = and_(rmwMask, mask);\n@@ -4479,9 +4496,10 @@ struct AtomicRMWOpConversion\n       atom.o(rmwOp).o(sTy);\n       if (valueTy) {\n         atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n-        auto ret = ptxBuilderAtomicRMW.launch(rewriter, loc, valueElemTy);\n+        auto retType = vec == 1 ? valueElemTy : vecTy;\n+        auto ret = ptxBuilderAtomicRMW.launch(rewriter, loc, retType);\n         for (int ii = 0; ii < vec; ++ii) {\n-          resultVals[i * vec + ii] =\n+          resultVals[i + ii] =\n               vec == 1 ? ret : extract_element(valueElemTy, ret, idx_val(ii));\n         }\n       } else {"}]