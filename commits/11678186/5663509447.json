[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -338,7 +338,8 @@ def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n \n \n try:\n-    from flash_attn.flash_attn_interface import flash_attn_qkvpacked_func as flash_attn_func\n+    from flash_attn.flash_attn_interface import \\\n+        flash_attn_qkvpacked_func as flash_attn_func\n     FLASH_VER = 2\n except BaseException:\n     try:"}]