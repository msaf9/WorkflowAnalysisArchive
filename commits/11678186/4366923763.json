[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -683,6 +683,22 @@ def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n \n+def test_tensor_atomic_rmw_block(device=\"cuda\"):\n+    shape = (8, 8)\n+\n+    @triton.jit\n+    def kernel(X, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n+        off0 = tl.arange(0, SHAPE0)\n+        off1 = tl.arange(0, SHAPE1)\n+        offs = off0[:, None] * SHAPE1 + off1[None, :]\n+        val = offs.to(tl.float32)\n+        x = X + offs\n+        tl.atomic_min(x, val)\n+    x = torch.ones((8, 8), device=device, dtype=torch.float32)\n+    kernel[(2,)](x, shape[0], shape[1])\n+    assert torch.min(x).item() == 0.0\n+\n+\n def test_atomic_cas():\n     # 1. make sure that atomic_cas changes the original value (Lock)\n     @triton.jit"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -641,7 +641,7 @@ def bitcast(input: tl.tensor,\n             builder: ir.builder) -> tl.tensor:\n     src_ty = input.type\n     if src_ty.is_block():\n-        dst_ty = tl.block_type(dst_ty, input.type.get_block_shapes())\n+        dst_ty = tl.block_type(dst_ty.scalar, input.type.get_block_shapes())\n     if src_ty == dst_ty:\n         return input\n     src_sca_ty = src_ty.scalar\n@@ -665,7 +665,7 @@ def cast(input: tl.tensor,\n     if isinstance(dst_ty, tl.constexpr):\n         dst_ty = dst_ty.value\n     if src_ty.is_block():\n-        dst_ty = tl.block_type(dst_ty, input.type.get_block_shapes())\n+        dst_ty = tl.block_type(dst_ty.scalar, input.type.get_block_shapes())\n     if src_ty == dst_ty:\n         return input\n "}]