[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "file_content_changes": "@@ -334,6 +334,24 @@ std::unique_ptr<DataFlowSolver> createDataFlowSolver();\n \n triton::MakeTensorPtrOp getMakeTensorPtrOp(Value v);\n \n+SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n+                                                ArrayRef<int64_t> shape,\n+                                                StringRef inputType);\n+SmallVector<unsigned, 3>\n+mmaVersionToInstrShape(int version, ArrayRef<int64_t> shape, Type type);\n+SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n+                                                ArrayRef<int64_t> shape,\n+                                                Type type, int opIdx);\n+SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n+                                                ArrayRef<int64_t> shape,\n+                                                StringRef inputType, int opIdx);\n+SmallVector<unsigned, 3>\n+mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n+                       ArrayRef<int64_t> shape);\n+SmallVector<unsigned, 3>\n+mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n+                       ArrayRef<int64_t> shape, int opIdx);\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_UTILITY_H"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -516,6 +516,7 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n     \"unsigned\":$versionMinor,\n     ArrayRefParameter<\"unsigned\">:$warpsPerCTA,\n     \"CTALayoutAttr\":$CTALayout,\n+    StringRefParameter<\"\">:$inputType,\n     ArrayRefParameter<\"unsigned\">:$instrShape\n   );\n \n@@ -562,7 +563,7 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n           wpt[1] = std::clamp<int>(wpt[1] * 2, 1, shapeC[1] / spw[1]);\n       } while (wpt_nm1 != wpt);\n \n-      return $_get(context, versionMajor, versionMinor, wpt, CTALayout, instrShape);\n+      return $_get(context, versionMajor, versionMinor, wpt, CTALayout, \"\", instrShape);\n     }]>,\n \n "}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -21,10 +21,6 @@ class SharedEncodingAttr;\n \n LogicalResult fixupLoops(ModuleOp mod);\n \n-SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n-                                                const ArrayRef<int64_t> &shape,\n-                                                RankedTensorType type);\n-\n /// Returns true if the Load is for TMA\n bool isLoadFromTensorPtr(triton::LoadOp op);\n "}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 69, "deletions": 0, "changes": 69, "file_content_changes": "@@ -710,4 +710,73 @@ triton::MakeTensorPtrOp getMakeTensorPtrOp(Value v) {\n   llvm_unreachable(\"Unable to getMakeTensorPtr()\");\n }\n \n+SmallVector<unsigned, 3>\n+mmaVersionToInstrShape(int version, ArrayRef<int64_t> shape, Type eltType) {\n+  if (version == 1)\n+    return {16, 16};\n+  else if (version == 2)\n+    return {16, 8};\n+  else if (version == 3) {\n+    unsigned k = 256 / eltType.getIntOrFloatBitWidth();\n+    if (shape[0] % 64 != 0 || shape[1] % 8 != 0) {\n+      assert(false && \"type not supported\");\n+      return {0, 0, 0};\n+    }\n+\n+    SmallVector<unsigned> validN;\n+\n+    // MMAv3 with larger instruction shape is preferred.\n+    if (eltType.isFloat8E5M2() || eltType.isFloat8E4M3FN() || eltType.isF16() ||\n+        eltType.isBF16() || eltType.isF32()) {\n+      validN.assign({256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176,\n+                     168, 160, 152, 144, 136, 128, 120, 112, 104, 96,  88,\n+                     80,  72,  64,  56,  48,  40,  32,  24,  16,  8});\n+    }\n+\n+    if (eltType.isInteger(8)) {\n+      validN.assign({224, 208, 192, 176, 160, 144, 128, 112, 96, 80, 64, 48, 32,\n+                     24, 16, 8});\n+    }\n+\n+    for (auto n : validN) {\n+      if (shape[1] % n == 0) {\n+        return {16, n, k};\n+      }\n+    }\n+\n+    assert(false && \"type not supported\");\n+    return {0, 0, 0};\n+  } else {\n+    assert(false && \"version not supported\");\n+    return {0, 0};\n+  }\n+}\n+\n+SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n+                                                ArrayRef<int64_t> shape,\n+                                                StringRef inputType) {\n+  return {0};\n+}\n+SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n+                                                ArrayRef<int64_t> shape,\n+                                                Type type, int opIdx) {\n+  return {0};\n+}\n+SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n+                                                ArrayRef<int64_t> shape,\n+                                                StringRef inputType,\n+                                                int opIdx) {\n+  return {0};\n+}\n+SmallVector<unsigned, 3>\n+mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n+                       ArrayRef<int64_t> shape) {\n+  return {0};\n+}\n+\n+SmallVector<unsigned, 3>\n+mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n+                       ArrayRef<int64_t> shape, int opIdx) {\n+  return {0};\n+}\n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -154,7 +154,7 @@ struct ConvertLayoutOpConversion\n     }\n     if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n       auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n-      auto instrShape = mmaLayout.getInstrShape();\n+      auto instrShape = mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n       SmallVector<Value> mmaColIdx(4);\n       SmallVector<Value> mmaRowIdx(2);\n       Value threadId = getThreadId(rewriter, loc);\n@@ -741,7 +741,7 @@ struct ConvertLayoutOpConversion\n                                                          rewriter, srcTy);\n \n       auto srcShapePerCTA = getShapePerCTA(mmaLayout, srcShape);\n-      auto instrShape = mmaLayout.getInstrShape();\n+      auto instrShape = mmaVersionToInstrShape(mmaLayout, srcShapePerCTA);\n       auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n       uint32_t repM =\n           ceil<unsigned>(srcShapePerCTA[0], instrShape[0] * warpsPerCTA[0]);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/WGMMA.cpp", "status": "modified", "additions": 9, "deletions": 7, "changes": 16, "file_content_changes": "@@ -160,11 +160,11 @@ DotOpMmaV3SmemLoader loadA(TritonGPUToLLVMTypeConverter *typeConverter,\n   auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n   auto aSharedLayout = aTensorTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n   assert(aSharedLayout && \"only support load dot operand from shared.\");\n-  auto instrShape = mmaEncoding.getInstrShape();\n+  auto shapePerCTA = getShapePerCTA(aTensorTy);\n+  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA, 0);\n   auto wpt = mmaEncoding.getWarpsPerCTA();\n   auto aOrd = aSharedLayout.getOrder();\n   bool transA = aOrd[0] == 0;\n-  auto shapePerCTA = getShapePerCTA(aTensorTy);\n \n   int numRepM = ceil<unsigned>(shapePerCTA[0], instrShape[0] * wpt[0]);\n   int numRepK = ceil<unsigned>(shapePerCTA[1], instrShape[2]);\n@@ -191,11 +191,12 @@ DotOpMmaV3SmemLoader loadB(TritonGPUToLLVMTypeConverter *typeConverter,\n   auto bTensorTy = tensor.getType().cast<RankedTensorType>();\n   auto bSharedLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n   assert(bSharedLayout && \"only support load B from shared.\");\n-  auto instrShape = mmaEncoding.getInstrShape();\n+  auto shapePerCTA = triton::gpu::getShapePerCTA(bTensorTy);\n+  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA, 1);\n+\n   auto wpt = mmaEncoding.getWarpsPerCTA();\n   auto bOrd = bSharedLayout.getOrder();\n   bool transB = bOrd[0] == 1;\n-  auto shapePerCTA = triton::gpu::getShapePerCTA(bTensorTy);\n \n   int numRepK = ceil<unsigned>(shapePerCTA[0], instrShape[2]);\n   int numRepN = ceil<unsigned>(shapePerCTA[1], instrShape[1] * wpt[1]);\n@@ -278,7 +279,8 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n   bool transA = aOrd[0] == 0;\n   bool transB = bOrd[0] == 1;\n   auto dShapePerCTA = getShapePerCTA(dTensorTy);\n-  auto instrShape = mmaEncoding.getInstrShape();\n+  auto instrShape = mmaVersionToInstrShape(mmaEncoding, dShapePerCTA);\n+\n   auto accSize = 2 * (instrShape[1] / 4);\n   int M = 4 * instrShape[0];\n   int N = instrShape[1];\n@@ -359,9 +361,9 @@ Value loadC(Value tensor, Value llTensor) {\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n   auto mmaEncoding = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n   assert(mmaEncoding && \"Currently, we only support $c with a mma layout.\");\n-  auto instrShape = mmaEncoding.getInstrShape();\n-  auto wpt = mmaEncoding.getWarpsPerCTA();\n   auto shapePerCTA = getShapePerCTA(tensorTy);\n+  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA);\n+  auto wpt = mmaEncoding.getWarpsPerCTA();\n   auto shapePerCTATile = getShapePerCTATile(mmaEncoding);\n \n   int numRepM = ceil<unsigned>(shapePerCTA[0], shapePerCTATile[0]);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -1013,10 +1013,11 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto _warpsPerCTA = mmaLayout.getWarpsPerCTA();\n     assert(_warpsPerCTA.size() == 2);\n     auto order = triton::gpu::getOrder(mmaLayout);\n-    ArrayRef<unsigned int> instrShape = mmaLayout.getInstrShape();\n+    auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n+    ArrayRef<unsigned int> instrShape =\n+        mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n     SmallVector<Value> warpsPerCTA = {i32_val(_warpsPerCTA[0]),\n                                       i32_val(_warpsPerCTA[1])};\n-    auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n \n     Value threadId = getThreadId(rewriter, loc);\n     Value warpSize = i32_val(32);\n@@ -1068,7 +1069,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto shape = type.getShape();\n     auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n     SmallVector<SmallVector<unsigned>> ret;\n-    ArrayRef<unsigned int> instrShape = mmaLayout.getInstrShape();\n+    ArrayRef<unsigned int> instrShape =\n+        mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n \n     for (unsigned i = 0; i < shapePerCTA[0];\n          i += getShapePerCTATile(mmaLayout)[0]) {"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 20, "deletions": 8, "changes": 28, "file_content_changes": "@@ -663,6 +663,17 @@ static LogicalResult parseBool(AsmParser &parser, const NamedAttribute &attr,\n   return parseBoolAttrValue(parser, attr.getValue(), value, desc);\n };\n \n+static LogicalResult parseStrAttr(AsmParser &parser, const NamedAttribute &attr,\n+                                  StringRef &res, StringRef desc) {\n+  auto strAttr = attr.getValue().dyn_cast<StringAttr>();\n+  if (!strAttr) {\n+    parser.emitError(parser.getNameLoc(), \"expected an string for \") << desc;\n+    return failure();\n+  }\n+  res = strAttr.strref();\n+  return success();\n+};\n+\n //===----------------------------------------------------------------------===//\n // Attribute methods\n //===----------------------------------------------------------------------===//\n@@ -762,7 +773,8 @@ MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape, Type eltTy) const {\n     elemsPerThread[1] = elemsCol;\n   } else if (isHopper()) {\n     auto wpt = getWarpsPerCTA();\n-    auto instrMNK = getInstrShape();\n+    auto instrMNK =\n+        mmaVersionToInstrShape(getVersionMajor(), shape, getInputType());\n     int repM = ceil<unsigned>(shapePerCTA[0], instrMNK[0] * wpt[0]);\n     int repN = ceil<unsigned>(shapePerCTA[1], instrMNK[1] * wpt[1]);\n     elemsPerThread[0] = 2 * repM;\n@@ -789,12 +801,12 @@ MmaEncodingAttr::getElemsPerThreadOfOperand(int opIdx,\n         \"getElemsPerThreadOfOperand() not supported for version 2\");\n   } else if (isHopper()) {\n     auto wpt = getWarpsPerCTA();\n-    auto instrMNK = getInstrShape();\n+    auto instrMNK = mmaVersionToInstrShape(getVersionMajor(), shapePerCTA,\n+                                           getInputType(), opIdx);\n     if (opIdx == 0) {\n       int repM = ceil<unsigned>(shapePerCTA[0], instrMNK[0] * wpt[0]);\n       int repK = ceil<unsigned>(shapePerCTA[1], instrMNK[2]);\n       return 8 * repM * repK;\n-\n     } else if (opIdx == 1) {\n       int repK = ceil<unsigned>(shapePerCTA[0], instrMNK[2]);\n       int repN = ceil<unsigned>(shapePerCTA[1], instrMNK[1] * wpt[1]);\n@@ -1044,6 +1056,7 @@ Attribute MmaEncodingAttr::parse(AsmParser &parser, Type type) {\n   SmallVector<unsigned> CTAsPerCGA;\n   SmallVector<unsigned> CTASplitNum;\n   SmallVector<unsigned> CTAOrder;\n+  StringRef inputType;\n   SmallVector<unsigned> instrShape;\n \n   for (const NamedAttribute &attr : dict) {\n@@ -1071,8 +1084,8 @@ Attribute MmaEncodingAttr::parse(AsmParser &parser, Type type) {\n       if (parseIntArrayAttr(parser, attr, CTAOrder, \"CTAOrder\").failed())\n         return {};\n     }\n-    if (attr.getName() == \"instrShape\") {\n-      if (parseIntArrayAttr(parser, attr, instrShape, \"instrShape\").failed()) {\n+    if (attr.getName() == \"inputType\") {\n+      if (parseStrAttr(parser, attr, inputType, \"inputType\").failed()) {\n         return {};\n       }\n     }\n@@ -1083,7 +1096,7 @@ Attribute MmaEncodingAttr::parse(AsmParser &parser, Type type) {\n \n   return parser.getChecked<MmaEncodingAttr>(parser.getContext(), versionMajor,\n                                             versionMinor, warpsPerCTA,\n-                                            CTALayout, instrShape);\n+                                            CTALayout, inputType, instrShape);\n }\n \n void MmaEncodingAttr::print(AsmPrinter &printer) const {\n@@ -1093,8 +1106,7 @@ void MmaEncodingAttr::print(AsmPrinter &printer) const {\n           << \"warpsPerCTA = [\" << getWarpsPerCTA() << \"], \"\n           << \"CTAsPerCGA = [\" << getCTALayout().getCTAsPerCGA() << \"], \"\n           << \"CTASplitNum = [\" << getCTALayout().getCTASplitNum() << \"], \"\n-          << \"CTAOrder = [\" << getCTALayout().getCTAOrder() << \"], \"\n-          << \"instrShape = [\" << getInstrShape() << \"]\"\n+          << \"CTAOrder = [\" << getCTALayout().getCTAOrder() << \"]\"\n           << \"}>\";\n }\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -212,8 +212,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n     if (!versionMajor)\n       return failure();\n \n-    auto instrShape =\n-        mmaVersionToInstrShape(versionMajor, retShapePerCTA, AType);\n+    auto instrShape = mmaVersionToInstrShape(versionMajor, retShapePerCTA,\n+                                             AType.getElementType());\n \n     // operands\n     Value a = dotOp.getA();\n@@ -259,7 +259,7 @@ class BlockedToMMA : public mlir::RewritePattern {\n                                           numWarps, instrShape);\n       mmaEnc = ttg::MmaEncodingAttr::get(oldRetType.getContext(), versionMajor,\n                                          0 /*versionMinor*/, warpsPerTile,\n-                                         CTALayout, instrShape);\n+                                         CTALayout, \"\", instrShape);\n     }\n     auto newRetType = RankedTensorType::get(\n         oldRetType.getShape(), oldRetType.getElementType(), mmaEnc);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 0, "deletions": 43, "changes": 43, "file_content_changes": "@@ -64,49 +64,6 @@ LogicalResult fixupLoops(ModuleOp mod) {\n   return success();\n }\n \n-SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n-                                                const ArrayRef<int64_t> &shape,\n-                                                RankedTensorType type) {\n-  if (version == 1)\n-    return {16, 16};\n-  else if (version == 2)\n-    return {16, 8};\n-  else if (version == 3) {\n-    unsigned k = 256 / type.getElementTypeBitWidth();\n-    if (shape[0] % 64 != 0 || shape[1] % 8 != 0) {\n-      assert(false && \"type not supported\");\n-      return {0, 0, 0};\n-    }\n-    auto eltType = type.getElementType();\n-    SmallVector<unsigned> validN;\n-\n-    // MMAv3 with larger instruction shape is preferred.\n-    if (eltType.isFloat8E5M2() || eltType.isFloat8E4M3FN() || eltType.isF16() ||\n-        eltType.isBF16() || eltType.isF32()) {\n-      validN.assign({256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176,\n-                     168, 160, 152, 144, 136, 128, 120, 112, 104, 96,  88,\n-                     80,  72,  64,  56,  48,  40,  32,  24,  16,  8});\n-    }\n-\n-    if (eltType.isInteger(8)) {\n-      validN.assign({224, 208, 192, 176, 160, 144, 128, 112, 96, 80, 64, 48, 32,\n-                     24, 16, 8});\n-    }\n-\n-    for (auto n : validN) {\n-      if (shape[1] % n == 0) {\n-        return {16, n, k};\n-      }\n-    }\n-\n-    assert(false && \"type not supported\");\n-    return {0, 0, 0};\n-  } else {\n-    assert(false && \"version not supported\");\n-    return {0, 0};\n-  }\n-}\n-\n bool isLoadFromTensorPtr(triton::LoadOp op) {\n   return mlir::triton::isTensorPointerType(op.getPtr().getType());\n }"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/EmitIndicesTest.cpp", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -122,7 +122,7 @@ class EmitIndicesTest : public ::testing::Test {\n                        const std::string &refStr) {\n     auto layout =\n         MmaEncodingAttr::get(&context, versionMajor, versionMinor, warpsPerCTA,\n-                             getSingleCTALayout2d(), instrShape);\n+                             getSingleCTALayout2d(), \"\", instrShape);\n     runDistributed2d(row, col, layout, /*multiCTA=*/false, refStr);\n   }\n \n@@ -133,7 +133,7 @@ class EmitIndicesTest : public ::testing::Test {\n                          const std::string &refStr) {\n     auto parent =\n         MmaEncodingAttr::get(&context, versionMajor, versionMinor, warpsPerCTA,\n-                             getSingleCTALayout2d(), instrShape);\n+                             getSingleCTALayout2d(), \"\", instrShape);\n     auto layout = DotOperandEncodingAttr::get(&context, opIdx, parent, 0);\n     runDistributed2d(row, col, layout, /*multiCTA=*/false, refStr);\n   }\n@@ -638,7 +638,8 @@ TEST_F(EmitIndicesTest, LayoutVisualizer_Mma) {\n \n   Attribute mmaLayout = MmaEncodingAttr::get(\n       /*context=*/&context, /*versionMajor=*/2, /*versionMinor=*/1,\n-      /*warpsPerCTA=*/{1, 1}, /*CTALayout=*/CTALayout, /*instrShape=*/{16, 8});\n+      /*warpsPerCTA=*/{1, 1}, /*CTALayout=*/CTALayout, \"\",\n+      /*instrShape=*/{16, 8});\n \n   llvm::SmallVector<int64_t> shape = {/*row=*/16, /*col=*/8};\n "}, {"filename": "unittest/Dialect/TritonGPU/SwizzleTest.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -33,7 +33,7 @@ TEST_P(SwizzleDotOperandTestFixture, DotOperands) {\n \n   // create encoding\n   auto parent = triton::gpu::MmaEncodingAttr::get(&ctx, 2, 0, {1, 1}, CTALayout,\n-                                                  {16, 64, 16});\n+                                                  \"\", {16, 64, 16});\n   auto encoding = triton::gpu::DotOperandEncodingAttr::get(\n       &ctx, params.opIdx, parent, 32 / params.typeWidth);\n "}]