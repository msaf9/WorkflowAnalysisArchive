[{"filename": ".pre-commit-config.yaml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -13,7 +13,7 @@ repos:\n       - id: autopep8\n         name: autopep8\n         entry: autopep8\n-        args: [\"-a\", \"-r\", \"-i\", \"./python\"]\n+        args: [\"-a\", \"-r\", \"-i\", \"./python/\"]\n         language: python\n         types: [python]\n         additional_dependencies:"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 74, "deletions": 24, "changes": 98, "file_content_changes": "@@ -144,19 +144,21 @@ struct FpToFpOpConversion\n   convertFp8E5M2x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n                            const Value &v0, const Value &v1, const Value &v2,\n                            const Value &v3) {\n-    auto *ptxAsm = \"{                                      \\n\"\n-                   \".reg .b32 a<2>, b<2>;                  \\n\"\n-                   \"prmt.b32 a0, 0, $2, 0x5140;            \\n\"\n-                   \"prmt.b32 a1, 0, $2, 0x7362;            \\n\"\n-                   \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"shr.b32  b0, b0, 3;                    \\n\"\n-                   \"shr.b32  b1, b1, 3;                    \\n\"\n-                   \"add.u32  b0, b0, 0x30003000;           \\n\"\n-                   \"add.u32  b1, b1, 0x30003000;           \\n\"\n-                   \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\"\n-                   \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\"\n-                   \"}\";\n+    auto *ptxAsm = // WARN: subnormal (0bs00000xx) are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+        \"prmt.b32 a0, 0, $2, 0x5140;            \\n\" // a0 = 0xf300f400\n+        \"prmt.b32 a1, 0, $2, 0x7362;            \\n\" // a1 = 0xf100f200\n+        \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n+        \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+        \"shr.b32  b0, b0, 3;                    \\n\" // b0 >>= 3\n+        \"shr.b32  b1, b1, 3;                    \\n\" // shift into bf16 position\n+        \"add.u32  b0, b0, 0x38003800;           \\n\" // b0.exp += 2**7-2**4\n+                                                    // exponent compensate = 112\n+        \"add.u32  b1, b1, 0x38003800;           \\n\" // b1 += 112<<7 | 112<<7<<16\n+        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+        \"}\";\n     return convertFp8x4ToBf16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n   };\n \n@@ -222,9 +224,17 @@ struct FpToFpOpConversion\n   convertFp16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n                            const Value &v0, const Value &v1, const Value &v2,\n                            const Value &v3) {\n-    auto *ptxAsm = \"{                                      \\n\"\n-                   \"prmt.b32 $0, $1, $2, 0x7531; \\n\\t\"\n-                   \"}\";\n+    auto *ptxAsm =\n+        \"{                            \\n\"\n+        \".reg .b32 a<2>;              \\n\"\n+        \"and.b32 a0, $1, 0x7fff7fff;  \\n\"           // a0 &= 0x7fff7fff\n+        \"and.b32 a1, $2, 0x7fff7fff;  \\n\"           // (strip sign)\n+        \"add.u32 a0, a0, 0x00800080;  \\n\"           // a0 += 0x00800080\n+        \"add.u32 a1, a1, 0x00800080;  \\n\"           // (round to nearest)\n+        \"lop3.b32 a0, $1, 0x80008000, a0, 0xea; \\n\" // a0 = a0|(0x80008000&in0)\n+        \"lop3.b32 a1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n+        \"prmt.b32 $0, a0, a1, 0x7531; \\n\\t\"         // output = a1a0\n+        \"}\";\n     return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n   }\n \n@@ -336,12 +346,53 @@ struct FpToFpOpConversion\n     return convertBf16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n   };\n \n-  // TODO:\n-  // static SmallVector<Value>\n-  // convertBf16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n-  //                          const Value &v0, const Value &v1, const Value &v2,\n-  //                          const Value &v3) {\n-  // }\n+  static SmallVector<Value>\n+  convertBf16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // bf16 is clamped firstly to fp8 min/max\n+        \"{                                           \\n\" // bf16=fp8>>3 + 112<<7\n+        \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n+        \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n+        \"mov.u32 fp8_min, 0x38003800;                \\n\" // so bf16_min = 0x3800\n+        \"mov.u32 fp8_max, 0x57e057e0;                \\n\" // so bf16_max = 0x57e0\n+        \"mov.u32 rn_, 0x00100010;                    \\n\" // round to nearest\n+        \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n+        \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n+        \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n+        \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n+        \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n+\n+        // nosign = clamp(nosign, min, max)\n+        \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n+        \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_0_0, nosign_0_0, 0x38000000; \\n\"\n+        \"min.u32 nosign_0_0, nosign_0_0, 0x57e00000; \\n\"\n+        \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_0_1, nosign_0_1, 0x3800;     \\n\"\n+        \"min.u32 nosign_0_1, nosign_0_1, 0x57e0;     \\n\"\n+        \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n+        \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_1_0, nosign_1_0, 0x38000000; \\n\"\n+        \"min.u32 nosign_1_0, nosign_1_0, 0x57e00000; \\n\"\n+        \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_1_1, nosign_1_1, 0x3800;     \\n\"\n+        \"min.u32 nosign_1_1, nosign_1_1, 0x57e0;     \\n\"\n+        \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n+\n+        \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n+        \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n+        \"sub.u32 nosign0, nosign0, 0x38003800;       \\n\" // nosign0-=0x38003800\n+        \"sub.u32 nosign1, nosign1, 0x38003800;       \\n\" // (compensate offset)\n+        \"shl.b32 nosign0, nosign0, 3;                \\n\" // nosign0 <<= 3\n+        \"shl.b32 nosign1, nosign1, 3;                \\n\" // shift into to fp8e4\n+        \"prmt.b32 nosign, nosign0, nosign1, 0x7531;  \\n\" // nosign0 = 0xf100f200\n+                                                         // nosign1 = 0xf300f400\n+                                                         // nosign = 0xf3f4f1f2\n+        \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n+        \"}\";\n+    return convertBf16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n \n   /* ------------------ */\n   // FP8 -> FP32\n@@ -479,8 +530,7 @@ struct FpToFpOpConversion\n         {{F8E5M2TyID, BF16TyID}, convertFp8E5M2x4ToBf16x4},\n         // BF16 -> F8\n         {{BF16TyID, F8E4M3TyID}, convertBf16x4ToFp8E4M3x4},\n-        // TODO:\n-        // {{BF16TyID, F8E5M2TyID}, convertBf16x4ToFp8E5M2x4},\n+        {{BF16TyID, F8E5M2TyID}, convertBf16x4ToFp8E5M2x4},\n         // F8 -> F32\n         {{F8E4M3TyID, F32TyID}, convertFp8E4M3x4ToFp32x4},\n         {{F8E5M2TyID, F32TyID}, convertFp8E5M2x4ToFp32x4},"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -199,9 +199,12 @@ struct LoadOpConversion\n           v = bitcast(v, IntegerType::get(getContext(), width));\n \n           PTXInstr::Operand *opr{};\n-          if (otherIsSplatConstInt)\n+\n+          if (otherIsSplatConstInt) {\n+            for (size_t s = 0; s < 32; s += valueElemNbits)\n+              splatVal |= splatVal << valueElemNbits;\n             opr = ptxBuilder.newConstantOperand(splatVal);\n-          else\n+          } else\n             opr = ptxBuilder.newOperand(v, readConstraint);\n \n           mov(dstsOpr->listGet(ii), opr).predicateNot(pred, \"b\");"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -6,6 +6,7 @@\n \n import triton\n import triton.language as tl\n+import triton.ops\n from triton.testing import get_dram_gbps, get_max_tensorcore_tflops\n \n DEVICE_NAME = {7: 'v100', 8: 'a100'}[torch.cuda.get_device_capability()[0]]\n@@ -96,7 +97,7 @@ def test_matmul(M, N, K, dtype_str):\n     ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n \n #######################\n@@ -152,7 +153,7 @@ def test_elementwise(N):\n     ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=500)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n #######################\n # Flash-Attention\n@@ -200,4 +201,4 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n-    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 16, "deletions": 13, "changes": 29, "file_content_changes": "@@ -783,7 +783,7 @@ def serialized_add(data, Lock):\n     data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n     ref = torch.full((128,), 64.0)\n     serialized_add[(64,)](data, Lock)\n-    triton.testing.assert_almost_equal(data, ref)\n+    np.testing.assert_allclose(to_numpy(data), to_numpy(ref))\n \n \n # ---------------\n@@ -932,7 +932,7 @@ def test_convert_float16_to_float32(in_dtype):\n \n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n-@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.float32])\n+@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16, torch.float32])\n def test_f8_xf16_roundtrip(in_dtype, out_dtype):\n     \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n     check_type_supported(out_dtype)\n@@ -970,7 +970,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     assert torch.all(f8_tensor == f8_output_tensor)\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4])\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16])\n def test_f16_to_f8_rounding(in_dtype, out_dtype):\n     \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n@@ -1017,8 +1017,10 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n \n     # WARN: only normalized numbers are handled\n     f8_normal_min = 1 << in_dtype.fp_mantissa_width  # 0b00001000 for float8e4\n-    f8_normal_max = 0b01111110\n+    f8_normal_max = 0b01111110 if in_dtype == tl.float8e4 else 0b01111011\n     f16_min, f16_max, f16_max_minus_1 = convert_float_to_float32(torch.tensor([f8_normal_min, f8_normal_max, f8_normal_max - 1], dtype=torch.int8), in_dtype)\n+    assert torch.all(torch.isfinite(f16_min))\n+    assert torch.all(torch.isfinite(f16_max))\n     thres_error = f16_max - f16_max_minus_1\n     mismatch = torch.logical_and(\n         torch.logical_or(abs_error != min_error, abs_error > thres_error), torch.logical_and(torch.isfinite(f16_input), torch.logical_and(torch.abs(f16_input) <= f16_max, torch.abs(f16_input) >= f16_min))\n@@ -1212,8 +1214,8 @@ def kernel(X, stride_xm, stride_xn,\n     # numpy result\n     z_ref = x.transpose(*perm)\n     # compare\n-    triton.testing.assert_almost_equal(z_tri, z_ref)\n-    triton.testing.assert_almost_equal(z_tri_contiguous, z_ref)\n+    np.testing.assert_allclose(to_numpy(z_tri), z_ref)\n+    np.testing.assert_allclose(to_numpy(z_tri_contiguous), z_ref)\n     # parse ptx to make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']\n     assert 'ld.global.v4' in ptx\n@@ -1475,7 +1477,7 @@ def _kernel(z, BLOCK: tl.constexpr,\n         tl.store(z + off, val)\n     _kernel[(1,)](z_tri, START=start, END=start + BLOCK, BLOCK=BLOCK)\n     z_ref = torch.arange(start, BLOCK + start, dtype=torch.int32, device=device)\n-    triton.testing.assert_almost_equal(z_tri, z_ref)\n+    np.testing.assert_allclose(to_numpy(z_tri), to_numpy(z_ref))\n \n # ---------------\n # test load\n@@ -1511,7 +1513,8 @@ def _kernel(in_ptr, out_ptr, in_size: tl.constexpr, out_size: tl.constexpr):\n     kernel[(1,)](input, output, input_size, output_size)\n \n     reference_out = torch.cat((input, torch.ones((size_diff,), dtype=dtype, device=device)))\n-    triton.testing.allclose(output, reference_out)\n+    # print((output - reference_out).nonzero())\n+    torch.testing.assert_allclose(output, reference_out)\n \n # Testing masked loads with an intermate copy to shared memory run.\n \n@@ -1542,15 +1545,15 @@ def _kernel(in1_ptr, in2_ptr, output_ptr,\n         in2_offsets = K_offsets[:, None] * in2_stride + N_offsets[None, :]\n \n         # Load inputs.\n-        x = tl.load(in1_ptr + in_offsets, mask=in_offsets < in_numel)\n-        w = tl.load(in2_ptr + in2_offsets, mask=in2_offsets < in2_numel)\n+        x = tl.load(in1_ptr + in_offsets, mask=in_offsets < M * K)\n+        w = tl.load(in2_ptr + in2_offsets, mask=in2_offsets < K * N)\n \n         # Without a dot product the memory doesn't get promoted to shared.\n         o = tl.dot(x, w, out_dtype=tl.float32)\n \n         # Store output\n         output_offsets = M_offsets[:, None] * out_stride + N_offsets[None, :]\n-        tl.store(output_ptr + output_offsets, o, mask=output_offsets < in2_numel)\n+        tl.store(output_ptr + output_offsets, o, mask=output_offsets < M * N)\n \n     pgm = _kernel[(1,)](in1, in2, out,\n                         in1.stride()[0],\n@@ -1562,7 +1565,7 @@ def _kernel(in1_ptr, in2_ptr, output_ptr,\n                         M=M, N=N, K=K)\n \n     reference_out = torch.matmul(in1, in2)\n-    triton.testing.allclose(out, reference_out)\n+    torch.testing.assert_allclose(out, reference_out, atol=1e-2, rtol=0)\n \n \n @pytest.mark.parametrize(\"cache\", [\"\", \".ca\", \".cg\"])\n@@ -1605,7 +1608,7 @@ def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n         assert \"ld.global.v4.b32\" in ptx\n     else:\n         assert \"ld.global.b32\" in ptx\n-    # triton.testing.assert_almost_equal(dst, src[:N])\n+    # np.testing.assert_allclose(dst, src[:N])\n \n \n @pytest.mark.parametrize(\"has_hints\", [False, True])"}, {"filename": "python/test/unit/operators/test_blocksparse.py", "status": "modified", "additions": 47, "deletions": 19, "changes": 66, "file_content_changes": "@@ -2,6 +2,34 @@\n import torch\n \n import triton\n+import triton.ops\n+\n+\n+def sparsify_tensor(x, mask, block):\n+    ret = torch.empty((x.size(0), mask.sum(), block, block), dtype=x.dtype, device=x.device)\n+    for idx, (h, i, j) in enumerate(zip(*mask.nonzero(as_tuple=True))):\n+        ret[:, idx, :, :] = x[:, h, i * block:(i + 1) * block, j * block:(j + 1) * block]\n+    return ret\n+\n+\n+def make_pair(shape, device=\"cuda\", alpha=1e-2, beta=0., trans=False, data=None, dtype=torch.float32):\n+    if data is None:\n+        data = torch.randn(shape, dtype=torch.float32, requires_grad=True, device=device)\n+    ref_ret = data\n+    ref_ret = ref_ret * alpha + beta\n+    ref_ret = ref_ret.half().to(dtype)\n+    if trans:\n+        ref_ret = ref_ret.t().requires_grad_()\n+    ref_ret = ref_ret.detach().requires_grad_()\n+    tri_ret = ref_ret.clone().detach().requires_grad_()\n+    return ref_ret, tri_ret\n+\n+\n+def mask_tensor(x, mask, block, value=0):\n+    ret = x.clone()\n+    for h, i, j in zip(*(mask == 0).nonzero(as_tuple=True)):\n+        ret[:, h, i * block:(i + 1) * block, j * block:(j + 1) * block] = value\n+    return ret\n \n \n @pytest.mark.parametrize(\"MODE\", [\"sdd\", \"dds\", \"dsd\"])\n@@ -16,8 +44,8 @@ def test_matmul(MODE, TRANS_A, TRANS_B, BLOCK, DTYPE, Z=3, H=2, M=512, N=384, K=\n     is_sdd = MODE == \"sdd\"\n     is_dsd = MODE == \"dsd\"\n     is_dds = MODE == \"dds\"\n-    do_sparsify = lambda x: triton.testing.sparsify_tensor(x, layout, BLOCK)\n-    do_mask = lambda x: triton.testing.mask_tensor(x, layout, BLOCK)\n+    do_sparsify = lambda x: sparsify_tensor(x, layout, BLOCK)\n+    do_mask = lambda x: mask_tensor(x, layout, BLOCK)\n     # create inputs\n     # create op\n     a_shape = (Z, H, K, M) if TRANS_A else (Z, H, M, K)\n@@ -32,9 +60,9 @@ def test_matmul(MODE, TRANS_A, TRANS_B, BLOCK, DTYPE, Z=3, H=2, M=512, N=384, K=\n     layout[1, 2, :] = 0\n     layout[1, :, 1] = 0\n     # create data\n-    a_ref, a_tri = triton.testing.make_pair(a_shape, alpha=.1, dtype=DTYPE)\n-    b_ref, b_tri = triton.testing.make_pair(b_shape, alpha=.1, dtype=DTYPE)\n-    dc_ref, dc_tri = triton.testing.make_pair(c_shape, dtype=DTYPE)\n+    a_ref, a_tri = make_pair(a_shape, alpha=.1, dtype=DTYPE)\n+    b_ref, b_tri = make_pair(b_shape, alpha=.1, dtype=DTYPE)\n+    dc_ref, dc_tri = make_pair(c_shape, dtype=DTYPE)\n     # compute [torch]\n     dc_ref = do_mask(dc_ref) if is_sdd else dc_ref\n     a_ref = do_mask(a_ref) if is_dsd else a_ref\n@@ -59,9 +87,9 @@ def test_matmul(MODE, TRANS_A, TRANS_B, BLOCK, DTYPE, Z=3, H=2, M=512, N=384, K=\n     da_tri = a_tri.grad\n     db_tri = b_tri.grad\n     # compare\n-    triton.testing.assert_almost_equal(c_ref, c_tri)\n-    triton.testing.assert_almost_equal(da_ref, da_tri)\n-    triton.testing.assert_almost_equal(db_ref, db_tri)\n+    torch.testing.assert_allclose(c_ref, c_tri)\n+    torch.testing.assert_allclose(da_ref, da_tri)\n+    torch.testing.assert_allclose(db_ref, db_tri)\n \n \n configs = [\n@@ -88,10 +116,10 @@ def test_softmax(BLOCK, WIDTH, is_dense, Z=2, H=2, is_causal=True, scale=0.4):\n         layout[1, :, 1] = 0\n     # initialize data\n     a_shape = (Z, H, M, N)\n-    a_ref, a_tri = triton.testing.make_pair(a_shape)\n-    dout_ref, dout_tri = triton.testing.make_pair(a_shape)\n+    a_ref, a_tri = make_pair(a_shape)\n+    dout_ref, dout_tri = make_pair(a_shape)\n     # compute [torch]\n-    a_ref = triton.testing.mask_tensor(a_ref, layout, BLOCK, value=float(\"-inf\"))\n+    a_ref = mask_tensor(a_ref, layout, BLOCK, value=float(\"-inf\"))\n     a_ref.retain_grad()\n     at_mask = torch.ones((M, N), device=\"cuda\")\n     if is_causal:\n@@ -100,19 +128,19 @@ def test_softmax(BLOCK, WIDTH, is_dense, Z=2, H=2, is_causal=True, scale=0.4):\n     a_ref[M == 0] = float(\"-inf\")\n     out_ref = torch.softmax(a_ref * scale, -1)\n     out_ref.backward(dout_ref)\n-    out_ref = triton.testing.sparsify_tensor(out_ref, layout, BLOCK)\n-    da_ref = triton.testing.sparsify_tensor(a_ref.grad, layout, BLOCK)\n+    out_ref = sparsify_tensor(out_ref, layout, BLOCK)\n+    da_ref = sparsify_tensor(a_ref.grad, layout, BLOCK)\n     # compute [triton]\n-    a_tri = triton.testing.sparsify_tensor(a_tri, layout, BLOCK)\n+    a_tri = sparsify_tensor(a_tri, layout, BLOCK)\n     a_tri.retain_grad()\n-    dout_tri = triton.testing.sparsify_tensor(dout_tri, layout, BLOCK)\n+    dout_tri = sparsify_tensor(dout_tri, layout, BLOCK)\n     op = triton.ops.blocksparse.softmax(layout, BLOCK, device=\"cuda\", is_dense=is_dense)\n     out_tri = op(a_tri, scale=scale, is_causal=is_causal)\n     out_tri.backward(dout_tri)\n     da_tri = a_tri.grad\n     # compare\n-    triton.testing.assert_almost_equal(out_tri, out_ref)\n-    triton.testing.assert_almost_equal(da_tri, da_ref)\n+    torch.testing.assert_allclose(out_tri, out_ref)\n+    torch.testing.assert_allclose(da_tri, da_ref)\n \n \n @pytest.mark.parametrize(\"block\", [16, 32, 64])\n@@ -168,9 +196,9 @@ def test_attention_fwd_bwd(\n \n     # comparison\n     # print(f\"Triton loss {loss} and torch loss {torch_loss}.  Also checking grads...\")\n-    triton.testing.assert_almost_equal(loss, torch_loss)\n+    torch.testing.assert_allclose(loss, torch_loss, atol=1e-3, rtol=0)\n     for g1, g2 in zip(grads, torch_grads):\n-        triton.testing.assert_almost_equal(g1, g2)\n+        torch.testing.assert_allclose(g1, g2)\n \n \n @pytest.mark.parametrize(\"block\", [16, 32, 64])"}, {"filename": "python/test/unit/operators/test_cross_entropy.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -2,6 +2,7 @@\n import torch\n \n import triton\n+import triton.ops\n \n \n @pytest.mark.parametrize(\"M, N, dtype, mode\",\n@@ -24,7 +25,7 @@ def test_op(M, N, dtype, mode):\n     tt_y = triton.ops.cross_entropy(x, idx)\n     th_y = torch.nn.CrossEntropyLoss(reduction=\"none\")(x, idx)\n     if mode == 'forward':\n-        triton.testing.assert_almost_equal(th_y, tt_y)\n+        torch.testing.assert_allclose(th_y, tt_y)\n     # backward pass\n     elif mode == 'backward':\n         dy = torch.randn_like(tt_y)\n@@ -35,4 +36,4 @@ def test_op(M, N, dtype, mode):\n         x.grad.zero_()\n         th_y.backward(dy)\n         th_dx = x.grad.clone()\n-        triton.testing.assert_almost_equal(th_dx, tt_dx)\n+        torch.testing.assert_allclose(th_dx, tt_dx)"}, {"filename": "python/test/unit/operators/test_flash_attention.py", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -2,6 +2,7 @@\n import torch\n \n import triton\n+import triton.ops\n \n \n @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n@@ -38,8 +39,8 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype):\n     tri_dk, k.grad = k.grad.clone(), None\n     tri_dq, q.grad = q.grad.clone(), None\n     # compare\n-    triton.testing.assert_almost_equal(ref_out, tri_out)\n-    decimal = 1 if dtype == torch.bfloat16 else 2\n-    triton.testing.assert_almost_equal(ref_dv, tri_dv, decimal=decimal)\n-    triton.testing.assert_almost_equal(ref_dk, tri_dk)\n-    triton.testing.assert_almost_equal(ref_dq, tri_dq)\n+    atol = 1e-1 if dtype == torch.bfloat16 else 1e-2\n+    torch.testing.assert_allclose(ref_out, tri_out, atol=atol, rtol=0)\n+    torch.testing.assert_allclose(ref_dv, tri_dv, atol=atol, rtol=0)\n+    torch.testing.assert_allclose(ref_dk, tri_dk, atol=atol, rtol=0)\n+    torch.testing.assert_allclose(ref_dq, tri_dq, atol=atol, rtol=0)"}, {"filename": "python/test/unit/operators/test_inductor.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -52,4 +52,4 @@ def triton_(in_out_ptr0, in_out_ptr1, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel\n     arg8_1 = torch.rand(64, device=\"cuda\")\n     arg9_1 = torch.rand(64, device=\"cuda\")\n     triton_[(512,)](buf14, buf16, arg114_1, arg115_1, arg8_1, arg9_1, 512, 4096, 1, 2048)\n-    triton.testing.allclose(buf16.mean(), buf14.mean().item(), atol=1e-7, rtol=0)\n+    torch.testing.assert_allclose(buf16.mean().item(), buf14.mean().item(), atol=1e-7, rtol=0)"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -4,6 +4,7 @@\n import torch\n \n import triton\n+import triton.ops\n \n \n @pytest.mark.parametrize(\n@@ -95,4 +96,4 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n     # run test\n     th_c = torch.matmul(a, b)\n     tt_c = triton.testing.catch_oor(lambda: triton.ops.matmul(a, b), pytest)\n-    triton.testing.assert_almost_equal(th_c, tt_c)\n+    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)"}, {"filename": "python/triton/__init__.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -28,7 +28,6 @@\n from .compiler import compile, CompilationError\n from . import language\n from . import testing\n-from . import ops\n \n __all__ = [\n     \"autotune\","}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "file_content_changes": "@@ -19,7 +19,6 @@\n from typing import Any, Callable, Dict, Optional, Tuple, Type, Union\n \n import setuptools\n-import torch\n from filelock import FileLock\n \n import triton\n@@ -1669,8 +1668,8 @@ def _is_jsonable(x):\n def compile(fn, **kwargs):\n     capability = kwargs.get(\"cc\", None)\n     if capability is None:\n-        device = torch.cuda.current_device()\n-        capability = torch.cuda.get_device_capability(device)\n+        device = triton.runtime.jit.get_current_device()\n+        capability = triton.runtime.jit.get_device_capability(device)\n         capability = capability[0] * 10 + capability[1]\n     # we get the kernel, i.e. the first function generated in the module\n     # if fn is not a JITFunction, then it\n@@ -1805,7 +1804,7 @@ def __init__(self, fn, so_path, metadata, asm):\n     def _init_handles(self):\n         if self.cu_module is not None:\n             return\n-        device = torch.cuda.current_device()\n+        device = triton.runtime.jit.get_current_device()\n         global cuda_utils\n         init_cuda_utils()\n         max_shared = cuda_utils.get_device_properties(device)[\"max_shared_mem\"]\n@@ -1827,7 +1826,7 @@ def __getitem__(self, grid):\n \n         def runner(*args, stream=None):\n             if stream is None:\n-                stream = torch.cuda.current_stream().cuda_stream\n+                stream = triton.runtime.jit.get_cuda_stream()\n             self.c_wrapper(grid[0], grid[1], grid[2], self.num_warps, self.shared, stream, self.cu_function,\n                            CompiledKernel.launch_enter_hook, CompiledKernel.launch_exit_hook, self, *args)\n         return runner"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 58, "deletions": 39, "changes": 97, "file_content_changes": "@@ -10,15 +10,34 @@\n from collections import defaultdict, namedtuple\n from typing import Callable, Generic, Iterable, Optional, TypeVar, Union, cast, overload\n \n-import torch\n-\n import triton\n from triton.utils import MockTensor\n \n-try:\n-    from torch._C import _cuda_getCurrentRawStream as get_cuda_stream\n-except ImportError:\n-    get_cuda_stream = lambda dev_idx: torch.cuda.current_stream(dev_idx).cuda_stream\n+\n+def get_cuda_stream(idx=None):\n+    if idx is None:\n+        idx = get_current_device()\n+    try:\n+        from torch._C import _cuda_getCurrentRawStream\n+        return _cuda_getCurrentRawStream(idx)\n+    except ImportError:\n+        import torch\n+        return torch.cuda.current_stream(idx).cuda_stream\n+\n+\n+def get_current_device():\n+    import torch\n+    return torch.cuda.current_device()\n+\n+\n+def set_current_device(idx):\n+    import torch\n+    torch.cuda.set_device(idx)\n+\n+\n+def get_device_capability(idx):\n+    import torch\n+    return torch.cuda.get_device_capability(idx)\n \n \n T = TypeVar('T')\n@@ -160,34 +179,31 @@ def is_divisible_by_16(x):\n \n     @staticmethod\n     def _type_of(key):\n-        if isinstance(key, (torch.dtype, triton.language.dtype)):\n-            ty = {\n-                torch.bool: 'i1',\n-                torch.float16: 'fp16',\n-                torch.bfloat16: 'bf16',\n-                torch.float32: 'fp32',\n-                torch.float64: 'fp64',\n-                torch.uint8: 'u8',\n-                torch.int8: 'i8',\n-                torch.int16: 'i16',\n-                torch.int32: 'i32',\n-                torch.int64: 'i64',\n-\n-                triton.language.uint8: 'u8',\n-                triton.language.uint16: 'u16',\n-                triton.language.uint32: 'u32',\n-                triton.language.uint64: 'u64',\n-                triton.language.float8e5: 'fp8e5',\n-                triton.language.float8e4: 'fp8e4',\n-                triton.language.float16: 'fp16',\n-                triton.language.bfloat16: 'bf16',\n-                triton.language.float32: 'fp32',\n-            }[key]\n-            return f'*{ty}'\n+        # None are nullptr -- implicitly converted to *i8\n         if key is None:\n             return '*i8'\n-        assert isinstance(key, str)\n-        return key\n+        dtype_str = str(key).split(\".\")[-1]\n+        tys = {\n+            \"bool\": \"i1\",\n+            \"float8e5\": \"fp8e5\",\n+            \"float8e4\": \"fp8e4\",\n+            \"float16\": \"fp16\",\n+            \"bfloat16\": \"bf16\",\n+            \"float32\": \"fp32\",\n+            \"float64\": \"fp64\",\n+            \"int8\": \"i8\",\n+            \"int16\": \"i16\",\n+            \"int32\": \"i32\",\n+            \"int64\": \"i64\",\n+            \"uint8\": \"u8\",\n+            \"uint16\": \"u16\",\n+            \"uint32\": \"u32\",\n+            \"uint64\": \"u64\",\n+        }\n+        # reinterpret can create triton type\n+        for v in list(tys.values()):\n+            tys[v] = v\n+        return key if isinstance(key, str) else f\"*{tys[dtype_str]}\"\n \n     def _make_signature(self, sig_key):\n         signature = \",\".join([self._type_of(k) for i, k in enumerate(sig_key)])\n@@ -252,8 +268,8 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     grid_0 = grid[0]\n     grid_1 = grid[1] if grid_size > 1 else 1\n     grid_2 = grid[2] if grid_size > 2 else 1\n-    device = torch.cuda.current_device()\n-    torch.cuda.set_device(device)\n+    device = get_current_device()\n+    set_current_device(device)\n     if stream is None and not warmup:\n       stream = get_cuda_stream(device)\n     try:\n@@ -286,7 +302,9 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n \"\"\"\n         scope = {\"version_key\": version_key(), \"get_cuda_stream\": get_cuda_stream,\n                  \"self\": self, \"_spec_of\": self._spec_of, \"_key_of\": self._key_of,\n-                 \"cache\": self.cache, \"triton\": triton, \"torch\": torch}\n+                 \"cache\": self.cache, \"triton\": triton,\n+                 \"get_current_device\": get_current_device,\n+                 \"set_current_device\": set_current_device}\n         exec(src, scope)\n         return scope[self.fn.__name__]\n \n@@ -397,8 +415,9 @@ def jit(\n     \"\"\"\n     Decorator for JIT-compiling a function using the Triton compiler.\n \n-    :note: When a jit'd function is called, :code:`torch.tensor` arguments are\n-        implicitly converted to pointers using the :code:`.data_ptr()` method.\n+    :note: When a jit'd function is called, arguments are\n+        implicitly converted to pointers if they have a :code:`.data_ptr()` method\n+        and a `.dtype` attribute.\n \n     :note: This function will be compiled and run on the GPU. It will only have access to:\n \n@@ -449,8 +468,8 @@ def reinterpret(tensor, dtype):\n         else:\n             # Reinterpreting a wrapped tensor to a different type.\n             return TensorWrapper(tensor.base, dtype)\n-    elif isinstance(tensor, torch.Tensor):\n+    elif hasattr(tensor, \"data_ptr\"):\n         # A new wrapper is needed around an unwrapped tensor.\n         return TensorWrapper(tensor, dtype)\n     else:\n-        raise TypeError(f'Cannot reinterpret a {type(tensor)}.')\n+        raise TypeError(f'Cannot reinterpret a {type(tensor)}. Does not contain `data_ptr` method.')"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 7, "deletions": 95, "changes": 102, "file_content_changes": "@@ -4,21 +4,10 @@\n import sys\n from contextlib import contextmanager\n \n-import torch\n-\n+import triton\n import triton._C.libtriton.triton as _triton\n from .compiler import OutOfResources\n \n-try:\n-    import triton._C.libtriton.cutlass as _cutlass\n-    has_cutlass = True\n-except ImportError:\n-    _cutlass = None\n-    has_cutlass = False\n-\n-# TODO: move to separate module\n-import triton\n-\n \n def catch_oor(kernel, pytest_handle=None):\n     try:\n@@ -30,86 +19,6 @@ def catch_oor(kernel, pytest_handle=None):\n     return res\n \n \n-def sparsify_tensor(x, mask, block):\n-    ret = torch.empty((x.size(0), mask.sum(), block, block), dtype=x.dtype, device=x.device)\n-    for idx, (h, i, j) in enumerate(zip(*mask.nonzero(as_tuple=True))):\n-        ret[:, idx, :, :] = x[:, h, i * block:(i + 1) * block, j * block:(j + 1) * block]\n-    return ret\n-\n-\n-def make_pair(shape, device=\"cuda\", alpha=1e-2, beta=0., trans=False, data=None, dtype=torch.float32):\n-    if data is None:\n-        data = torch.randn(shape, dtype=torch.float32, requires_grad=True, device=device)\n-    ref_ret = data\n-    ref_ret = ref_ret * alpha + beta\n-    ref_ret = ref_ret.half().to(dtype)\n-    if trans:\n-        ref_ret = ref_ret.t().requires_grad_()\n-    ref_ret = ref_ret.detach().requires_grad_()\n-    tri_ret = ref_ret.clone().detach().requires_grad_()\n-    return ref_ret, tri_ret\n-\n-\n-def cutlass_matmul(a, b):\n-    if _cutlass is None:\n-        raise RuntimeError(\"Cannot find cutlass library\")\n-    M, N = a.shape[0], b.shape[1]\n-    Ka, Kb = a.shape[1], b.shape[0]\n-    assert Ka == Kb\n-    assert a.dtype == b.dtype\n-    assert a.device == b.device\n-    # allocate output\n-    c = torch.empty_strided((M, N), (1, M), dtype=a.dtype, device=a.device)\n-    # run function\n-    dtype = str(a.dtype).split('.')[-1]\n-    _cutlass.matmul(a.data_ptr(), b.data_ptr(), c.data_ptr(),\n-                    M, N, Ka,\n-                    a.stride(0), a.stride(1),\n-                    b.stride(0), b.stride(1),\n-                    c.stride(0), c.stride(1),\n-                    dtype, dtype, dtype,\n-                    a.device.index, torch.cuda.current_stream(a.device).cuda_stream)\n-\n-    return c\n-\n-\n-def mask_tensor(x, mask, block, value=0):\n-    ret = x.clone()\n-    for h, i, j in zip(*(mask == 0).nonzero(as_tuple=True)):\n-        ret[:, h, i * block:(i + 1) * block, j * block:(j + 1) * block] = value\n-    return ret\n-\n-\n-def assert_almost_equal(x, y, decimal=2, err_msg=''):\n-    import numpy.testing as npt\n-    if isinstance(x, torch.Tensor):\n-        if x.dtype == torch.bfloat16:\n-            x = x.float()\n-        x = x.cpu().detach().numpy()\n-    if isinstance(y, torch.Tensor):\n-        if y.dtype == torch.bfloat16:\n-            y = y.float()\n-        y = y.cpu().detach().numpy()\n-    npt.assert_array_almost_equal(x, y, err_msg=err_msg, decimal=decimal)\n-\n-\n-def allclose(x, y, atol=0, rtol=1e-2):\n-    if not isinstance(x, torch.Tensor):\n-        x = torch.tensor(x)\n-    if not isinstance(y, torch.Tensor):\n-        y = torch.tensor(y)\n-    if x.dtype != y.dtype:\n-        raise RuntimeError(f'{x.dtype} did not match with {x.dtype}')\n-    if x.shape != y.shape:\n-        raise RuntimeError(f'{x.shape} did not match with {y.shape}')\n-    if x.dtype == torch.bool:\n-        return torch.sum(x ^ y) == 0\n-    if x.dtype in [torch.int8, torch.int16, torch.int32, torch.int64]:\n-        rtol = 0\n-        atol = 0\n-    return torch.allclose(x, y, rtol=rtol, atol=atol)\n-\n-\n def nvsmi(attrs):\n     attrs = ','.join(attrs)\n     cmd = ['nvidia-smi', '-i', '0', '--query-gpu=' + attrs, '--format=csv,noheader,nounits']\n@@ -122,6 +31,7 @@ def nvsmi(attrs):\n def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n              percentiles=(0.5, 0.2, 0.8),\n              record_clocks=False, fast_flush=False):\n+    import torch\n     \"\"\"\n     Benchmark the runtime of the provided function. By default, return the median runtime of :code:`fn` along with\n     the 20-th and 80-th performance percentile.\n@@ -335,7 +245,7 @@ def perf_report(benchmarks):\n \n def get_dram_gbps(backend=None, device=None):\n     ''' return DRAM bandwidth in GB/s '''\n-    # assert backend == CUDA\n+    import torch\n     if not backend:\n         backend = _triton.runtime.backend.CUDA\n     if not device:\n@@ -346,7 +256,8 @@ def get_dram_gbps(backend=None, device=None):\n     return bw_gbps\n \n \n-def get_max_tensorcore_tflops(dtype: torch.dtype, backend=None, device=None, clock_rate=None):\n+def get_max_tensorcore_tflops(dtype, backend=None, device=None, clock_rate=None):\n+    import torch\n     if not backend:\n         backend = _triton.runtime.backend.CUDA\n     if not device:\n@@ -447,7 +358,8 @@ def set_gpu_clock(ref_sm_clock=1350, ref_mem_clock=1215):\n         subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-rmc\"])\n \n \n-def get_max_simd_tflops(dtype: torch.dtype, backend=None, device=None):\n+def get_max_simd_tflops(dtype, backend=None, device=None):\n+    import torch\n     if not backend:\n         backend = _triton.runtime.backend.CUDA\n     if not device:"}, {"filename": "python/triton/utils.py", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -1,7 +1,5 @@\n from __future__ import annotations\n \n-import torch\n-\n \n def cdiv(x, y):\n     return (x + y - 1) // y\n@@ -26,7 +24,8 @@ class MockTensor:\n     \"\"\"\n     @staticmethod\n     def wrap_dtype(arg):\n-        if isinstance(arg, torch.dtype):\n+        if arg.__class__.__name__ == \"dtype\" and\\\n+           arg.__module__ == \"torch\":\n             return MockTensor(arg)\n         return arg\n \n@@ -60,7 +59,7 @@ def reinterpret(tensor, dtype):\n         else:\n             # Reinterpreting a wrapped tensor to a different type.\n             return TensorWrapper(tensor.base, dtype)\n-    elif isinstance(tensor, torch.Tensor):\n+    elif hasattr(tensor, \"data_ptr\"):\n         # A new wrapper is needed around an unwrapped tensor.\n         return TensorWrapper(tensor, dtype)\n     else:"}]