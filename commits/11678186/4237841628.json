[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -83,10 +83,7 @@ jobs:\n         if: ${{ contains(matrix.runner, 'A100') }}\n         run: |\n           cd python/test/regression\n-          sudo nvidia-smi -i 0 -pm 1\n-          sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n           pytest -vs .\n-          sudo nvidia-smi -i 0 -rgc\n \n       # - name: Run lit tests\n       #   run: |"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "file_content_changes": "@@ -58,7 +58,7 @@ def nvsmi(attrs):\n         (1024, 1024, 1024): {'float16': 0.33, 'float32': 0.35, 'int8': 0.169},\n         (2048, 2048, 2048): {'float16': 0.64, 'float32': 0.57, 'int8': 0.34},\n         (4096, 4096, 4096): {'float16': 0.79, 'float32': 0.75, 'int8': 0.46},\n-        (8192, 8192, 8192): {'float16': 0.79, 'float32': 0.85, 'int8': 0.51},\n+        (8192, 8192, 8192): {'float16': 0.80, 'float32': 0.85, 'int8': 0.51},\n         # tall-skinny\n         (16, 1024, 1024): {'float16': 0.0077, 'float32': 0.0127, 'int8': 0.005},\n         (16, 4096, 4096): {'float16': 0.0363, 'float32': 0.0457, 'int8': 0.0259},\n@@ -84,9 +84,7 @@ def test_matmul(M, N, K, dtype_str):\n     torch.manual_seed(0)\n     ref_gpu_util = matmul_data[DEVICE_NAME][(M, N, K)][dtype_str]\n     cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n-    ref_sm_clock = sm_clocks[DEVICE_NAME]\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n-    assert abs(cur_sm_clock - ref_sm_clock) < 15, f'GPU SMs must run at {ref_sm_clock} MHz'\n     if dtype == torch.int8:\n         a = torch.randint(-128, 127, (M, K), dtype=dtype, device='cuda')\n         b = torch.randint(-128, 127, (N, K), dtype=dtype, device='cuda')\n@@ -95,7 +93,7 @@ def test_matmul(M, N, K, dtype_str):\n         a = torch.randn((M, K), dtype=dtype, device='cuda')\n         b = torch.randn((K, N), dtype=dtype, device='cuda')\n     fn = lambda: triton.ops.matmul(a, b)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=25, rep=200)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=1000)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)\n@@ -151,7 +149,7 @@ def test_elementwise(N):\n     y = torch.randn_like(z)\n     grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n     fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=25, rep=200)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=1000)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)"}]