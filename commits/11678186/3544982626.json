[{"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -59,7 +59,8 @@ def TT_AtomicRMWAttr : I32EnumAttr<\n         I32EnumAttrCase<\"MAX\", 6, \"max\">,\n         I32EnumAttrCase<\"MIN\", 7, \"min\">,\n         I32EnumAttrCase<\"UMAX\", 8, \"umax\">,\n-        I32EnumAttrCase<\"UMIN\", 9, \"umin\">\n+        I32EnumAttrCase<\"UMIN\", 9, \"umin\">,\n+        I32EnumAttrCase<\"XCHG\", 10, \"exch\">\n     ]> {\n     let cppNamespace = \"::mlir::triton\";\n }"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 18, "deletions": 3, "changes": 21, "file_content_changes": "@@ -115,9 +115,9 @@ SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op) {\n // TODO: extend beyond scalars\n SmallVector<unsigned> getScratchConfigForAtomicRMW(triton::AtomicRMWOp op) {\n   SmallVector<unsigned> smemShape;\n-  auto ptrTy = op.ptr().getType();\n-  if (auto tensorType = ptrTy.dyn_cast<RankedTensorType>()) {\n-    // do nothing or just assert because shared memory is not used in tensor\n+  if (op.ptr().getType().isa<RankedTensorType>()) {\n+    // do nothing or just assert because shared memory is not used in tensor up\n+    // to now\n   } else {\n     // need only bytes for scalar\n     // always vec = 1 and elemsPerThread = 1 for scalar?\n@@ -126,6 +126,10 @@ SmallVector<unsigned> getScratchConfigForAtomicRMW(triton::AtomicRMWOp op) {\n   return smemShape;\n }\n \n+SmallVector<unsigned> getScratchConfigForAtomicCAS(triton::AtomicCASOp op) {\n+  return SmallVector<unsigned>{1};\n+}\n+\n class AllocationAnalysis {\n public:\n   AllocationAnalysis(Operation *operation, Allocation *allocation)\n@@ -230,6 +234,17 @@ class AllocationAnalysis {\n                          : elems * elemTy.getIntOrFloatBitWidth() / 8;\n         allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n       }\n+    } else if (auto atomicCASOp = dyn_cast<triton::AtomicCASOp>(op)) {\n+      auto value = op->getOperand(0);\n+      auto smemShape = getScratchConfigForAtomicCAS(atomicCASOp);\n+      unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n+                                       std::multiplies{});\n+      auto elemTy =\n+          value.getType().cast<triton::PointerType>().getPointeeType();\n+      auto bytes = elemTy.isa<triton::PointerType>()\n+                       ? elems * kPtrBitWidth / 8\n+                       : elems * elemTy.getIntOrFloatBitWidth() / 8;\n+      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     }\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 93, "deletions": 7, "changes": 100, "file_content_changes": "@@ -6036,6 +6036,82 @@ struct ExpOpConversionApprox\n     return ptxBuilder.launch(rewriter, loc, f32_ty, false);\n   }\n };\n+/// ====================== atomic_cas codegen begin ==========================\n+struct AtomicCASOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::AtomicCASOp>,\n+      public LoadStoreConversionBase {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::AtomicCASOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  AtomicCASOpConversion(LLVMTypeConverter &converter,\n+                        const Allocation *allocation, Value smem,\n+                        AxisInfoAnalysis &axisAnalysisPass,\n+                        PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::AtomicCASOp>(\n+            converter, allocation, smem, benefit),\n+        LoadStoreConversionBase(axisAnalysisPass) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::AtomicCASOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op.getLoc();\n+    MLIRContext *ctx = rewriter.getContext();\n+    Value ptr = op.ptr();\n+\n+    Value llPtr = adaptor.ptr();\n+    Value llCmp = adaptor.cmp();\n+    Value llVal = adaptor.val();\n+\n+    auto ptrElements = getElementsFromStruct(loc, llPtr, rewriter);\n+    auto cmpElements = getElementsFromStruct(loc, llCmp, rewriter);\n+    auto valElements = getElementsFromStruct(loc, llVal, rewriter);\n+\n+    auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    Type valueElemTy =\n+        valueTy ? getTypeConverter()->convertType(valueTy.getElementType())\n+                : op.getResult().getType();\n+    auto tid = tid_val();\n+    Value pred = icmp_eq(tid, i32_val(0));\n+    PTXBuilder ptxBuilderMemfence;\n+    auto memfenc = ptxBuilderMemfence.create<PTXInstr>(\"membar\")->o(\"gl\");\n+    memfenc();\n+    auto ASMReturnTy = void_ty(ctx);\n+    ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n+\n+    Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+    atomPtr = bitcast(atomPtr, ptr_ty(valueElemTy, 3));\n+\n+    Value casPtr = ptrElements[0];\n+    Value casCmp = cmpElements[0];\n+    Value casVal = valElements[0];\n+\n+    PTXBuilder ptxBuilderAtomicCAS;\n+    auto *dstOpr = ptxBuilderAtomicCAS.newOperand(\"=r\");\n+    auto *ptrOpr = ptxBuilderAtomicCAS.newAddrOperand(casPtr, \"l\");\n+    auto *cmpOpr = ptxBuilderAtomicCAS.newOperand(casCmp, \"r\");\n+    auto *valOpr = ptxBuilderAtomicCAS.newOperand(casVal, \"r\");\n+    auto &atom = *ptxBuilderAtomicCAS.create<PTXInstr>(\"atom\");\n+    atom.global().o(\"cas\").o(\"b32\");\n+    atom(dstOpr, ptrOpr, cmpOpr, valOpr).predicate(pred);\n+    auto old = ptxBuilderAtomicCAS.launch(rewriter, loc, valueElemTy);\n+    barrier();\n+\n+    PTXBuilder ptxBuilderStore;\n+    auto *dstOprStore = ptxBuilderStore.newAddrOperand(atomPtr, \"l\");\n+    auto *valOprStore = ptxBuilderStore.newOperand(old, \"r\");\n+    auto &st = *ptxBuilderStore.create<PTXInstr>(\"st\");\n+    st.shared().o(\"b32\");\n+    st(dstOprStore, valOprStore).predicate(pred);\n+    ptxBuilderStore.launch(rewriter, loc, ASMReturnTy);\n+    ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n+    barrier();\n+    Value ret = load(atomPtr);\n+    barrier();\n+    rewriter.replaceOp(op, {ret});\n+    return success();\n+  }\n+};\n+/// ====================== atomic_cas codegen end ==========================\n \n /// ====================== atomic_rmw codegen begin ==========================\n struct AtomicRMWOpConversion\n@@ -6105,15 +6181,15 @@ struct AtomicRMWOpConversion\n       Value rmwMask = maskElements[i];\n       rmwMask = and_(rmwMask, mask);\n       std::string sTy;\n-      PTXBuilder ptxBuilder;\n+      PTXBuilder ptxBuilderAtomicRMW;\n       std::string tyId = valueElemNbits * vec == 64\n                              ? \"l\"\n                              : (valueElemNbits * vec == 32 ? \"r\" : \"h\");\n-      auto *dstOpr = ptxBuilder.newOperand(\"=\" + tyId);\n-      auto *ptrOpr = ptxBuilder.newAddrOperand(rmwPtr, \"l\");\n-      auto *valOpr = ptxBuilder.newOperand(rmwVal, tyId);\n+      auto *dstOpr = ptxBuilderAtomicRMW.newOperand(\"=\" + tyId);\n+      auto *ptrOpr = ptxBuilderAtomicRMW.newAddrOperand(rmwPtr, \"l\");\n+      auto *valOpr = ptxBuilderAtomicRMW.newOperand(rmwVal, tyId);\n \n-      auto &atom = ptxBuilder.create<>(\"atom\")->global().o(\"gpu\");\n+      auto &atom = ptxBuilderAtomicRMW.create<>(\"atom\")->global().o(\"gpu\");\n       auto rmwOp = stringifyRMWOp(atomicRmwAttr).str();\n       auto sBits = std::to_string(valueElemNbits);\n       switch (atomicRmwAttr) {\n@@ -6149,21 +6225,29 @@ struct AtomicRMWOpConversion\n         rmwOp = \"min\";\n         sTy = \"u\" + sBits;\n         break;\n+      case RMWOp::XCHG:\n+        sTy = \"b\" + sBits;\n+        break;\n       default:\n         return failure();\n       }\n       atom.o(rmwOp).o(sTy);\n       if (valueTy) {\n         atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n-        auto ret = ptxBuilder.launch(rewriter, loc, valueElemTy);\n+        auto ret = ptxBuilderAtomicRMW.launch(rewriter, loc, valueElemTy);\n         for (int ii = 0; ii < vec; ++ii) {\n           resultVals[i * vec + ii] =\n               vec == 1 ? ret : extract_element(valueElemTy, ret, idx_val(ii));\n         }\n       } else {\n+        PTXBuilder ptxBuilderMemfence;\n+        auto memfenc = ptxBuilderMemfence.create<PTXInstr>(\"membar\")->o(\"gl\");\n+        memfenc();\n+        auto ASMReturnTy = void_ty(ctx);\n+        ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n         rmwMask = and_(rmwMask, icmp_eq(tid, i32_val(0)));\n         atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n-        auto old = ptxBuilder.launch(rewriter, loc, valueElemTy);\n+        auto old = ptxBuilderAtomicRMW.launch(rewriter, loc, valueElemTy);\n         Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());\n         atomPtr = bitcast(atomPtr, ptr_ty(valueElemTy, 3));\n         store(old, atomPtr);\n@@ -6264,6 +6348,8 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<ReduceOpConversion>(typeConverter, allocation, smem, benefit);\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n                                           benefit);\n+  patterns.add<AtomicCASOpConversion>(typeConverter, allocation, smem,\n+                                      axisInfoAnalysis, benefit);\n   patterns.add<AtomicRMWOpConversion>(typeConverter, allocation, smem,\n                                       axisInfoAnalysis, benefit);\n   patterns.add<ExtractSliceOpConversion>(typeConverter, allocation, smem,"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -278,6 +278,20 @@ struct TritonStorePattern : public OpConversionPattern<triton::StoreOp> {\n   }\n };\n \n+struct TritonAtomicCASPattern\n+    : public OpConversionPattern<triton::AtomicCASOp> {\n+  using OpConversionPattern<triton::AtomicCASOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::AtomicCASOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    rewriter.replaceOpWithNewOp<triton::AtomicCASOp>(\n+        op, typeConverter->convertType(op.getType()), \n+        adaptor.ptr(), adaptor.cmp(), adaptor.val());\n+    return success();\n+  }\n+};\n+\n struct TritonAtomicRMWPattern\n     : public OpConversionPattern<triton::AtomicRMWOp> {\n   using OpConversionPattern<triton::AtomicRMWOp>::OpConversionPattern;"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 13, "deletions": 4, "changes": 17, "file_content_changes": "@@ -105,7 +105,7 @@ void init_triton_ir(py::module &&m) {\n       .value(\"AND\", mlir::triton::RMWOp::AND)\n       .value(\"OR\", mlir::triton::RMWOp::OR)\n       .value(\"XOR\", mlir::triton::RMWOp::XOR)\n-      // .value(\"XCHG\", mlir::triton::RMWOp::Xchg)\n+      .value(\"XCHG\", mlir::triton::RMWOp::XCHG)\n       .value(\"MAX\", mlir::triton::RMWOp::MAX)\n       .value(\"MIN\", mlir::triton::RMWOp::MIN)\n       .value(\"UMIN\", mlir::triton::RMWOp::UMIN)\n@@ -1095,9 +1095,18 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::OpBuilder &self, mlir::Value &ptr, mlir::Value &cmp,\n               mlir::Value &val) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             auto ptrType = mlir::getElementTypeOrSelf(ptr)\n-                                .cast<mlir::triton::PointerType>();\n-             mlir::Type dstType = ptrType.getPointeeType();\n+             mlir::Type dstType;\n+             if (auto srcTensorType = ptr.getType().dyn_cast<mlir::RankedTensorType>()) {\n+               mlir::Type dstElemType = srcTensorType.getElementType()\n+                                            .cast<mlir::triton::PointerType>()\n+                                            .getPointeeType();\n+               dstType = mlir::RankedTensorType::get(srcTensorType.getShape(),\n+                                                     dstElemType);\n+             } else {\n+               auto ptrType = mlir::getElementTypeOrSelf(ptr)\n+                                  .cast<mlir::triton::PointerType>();\n+               dstType = ptrType.getPointeeType();\n+             }\n              return self.create<mlir::triton::AtomicCASOp>(loc, dstType, ptr,\n                                                            cmp, val);\n            })"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -700,6 +700,16 @@ def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr)\n #     serialized_add[(64,)](data, Lock)\n #     triton.testing.assert_almost_equal(data, ref)\n \n+def test_simple_atomic_cas():\n+    # 1. make sure that atomic_cas changes the original value (Lock)\n+    @triton.jit\n+    def change_value(Lock):\n+        tl.atomic_cas(Lock, 0, 1)\n+\n+    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+    change_value[(1,)](Lock)\n+\n+    assert (Lock[0] == 1)\n \n # # ---------------\n # # test cast"}]