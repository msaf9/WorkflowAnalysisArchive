[{"filename": "CONTRIBUTING.md", "status": "modified", "additions": 69, "deletions": 0, "changes": 69, "file_content_changes": "@@ -51,3 +51,72 @@ A well-structured RFC should include:\n ## New backends\n \n Due to limited resources, we need to prioritize the number of targets we support. We are committed to providing upstream support for Nvidia and AMD GPUs. However, if you wish to contribute support for other backends, please start your project in a fork. If your backend proves to be useful and meets our performance requirements, we will discuss the possibility of upstreaming it.\n+\n+\n+## Project Structure\n+```\n+triton\n+\u251c\u2500\u2500 lib : C++ code for python library\n+\u2502   \u251c\u2500\u2500Analysis\n+\u2502   \u2502\tMemory barrier analysis\n+\u2502   \u2502\tclass to extract axis information from MLIR ops\n+\u2502   \u2502\timplementation of the shared memory allocation analysis for Triton dialect\n+\u2502   \u2502\n+\u2502   \u251c\u2500\u2500Conversion\n+\u2502   \u2502\t\u251c\u2500\u2500TritonGPUToLLVM:  Transforms TritonGPU  to LLVM;\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500TritonToTritonGPU: Transforms ops to TritonGPU ops; loading, storing, arithmetic, casting, and tensor operations.\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u2502\n+\u2502   \u251c\u2500\u2500Dialect\n+\u2502   \u2502\t\u251c\u2500\u2500Triton\n+\u2502   \u2502\t\u2502\tDefines core IR for Triton compiler\n+\u2502   \u2502\t\u251c\u2500\u2500TritonGPU\n+\u2502   \u2502\t    Defines TritonGPU operation for IR\n+\u2502   \u2502\n+\u2502   \u251c\u2500\u2500Target: contains Triton targets for converting to PTX, LLVMIR and HSACO IR targets\n+\u2502   \u2502\n+\u251c\u2500\u2500 bin\n+\u251c\u2500\u2500 cmake\n+\u251c\u2500\u2500 docs \u251c\u2500\u2500 Documentation regarding using triton\n+\u251c\u2500\u2500 include\n+\u2502   CMakelists.txt\n+\u2502   \u251c\u2500\u2500triton\n+\u2502   \u2502   \u251c\u2500\u2500\n+\u251c\u2500\u2500 python\n+\u2502   \u251c\u2500\u2500\n+\u2502   \u251c\u2500\u2500 MANIFEST.in\n+\u2502   \u251c\u2500\u2500 README.md\n+\u2502   \u251c\u2500\u2500 build\n+\u2502   \u251c\u2500\u2500 examples\n+\u2502   \u251c\u2500\u2500 pyproject.toml\n+\u2502   \u251c\u2500\u2500 setup.py: pip install for python package\n+\u2502   \u251c\u2500\u2500 src\n+\u2502   \u251c\u2500\u2500 test\n+\u2502   \u251c\u2500\u2500 triton\n+\u2502   \u2502\t\u251c\u2500\u2500 _C: Includes header files and compiled .so file for C library\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500common: Has interface for CUDA hardware backend\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500compiler: contains code for compiling source code to IR and lauching GPU kernels\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500interpreter: memory-map for tensors, converting primitives to tensors, and arethmetic ops for tensors\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500language: core of triton language, load tensors to SRAM, language logic, etc.\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500ops: contains functions for flash-attn, softmax, cross-entropy and other torch.nn.F functions\n+\u2502   \u2502\t\u251c\u2500\u2500runtime: contains impl jit compilation, autotuning, backend drivers,cahcing, error handles, etc.\n+\u2502   \u2502\t\u251c\u2500\u2500third_party\n+\u2502   \u2502\t\u251c\u2500\u2500tools\n+\u2502   \u251c\u2500\u2500 triton.egg-info\n+\u2502   \u251c\u2500\u2500 tutorials: contains tutorials for various use-cases\n+\u251c\u2500\u2500 test\n+\u2502   \u251c\u2500\u2500Analysis\n+\u2502   \u251c\u2500\u2500Conversion\n+\u2502   \u251c\u2500\u2500Dialect\n+\u2502   \u251c\u2500\u2500Target\n+\u251c\u2500\u2500 third_party\n+\u251c\u2500\u2500 unittest\n+\u2514\u2500\u2500 utils\n+```"}, {"filename": "README.md", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -42,6 +42,8 @@ pip install cmake; # build-time dependency\n pip install -e .\n ```\n \n+\n+\n # Changelog\n \n Version 2.0 is out! New features include:\n@@ -56,6 +58,9 @@ Community contributions are more than welcome, whether it be to fix bugs or to a\n \n If you\u2019re interested in joining our team and working on Triton & GPU kernels, [we\u2019re hiring](https://openai.com/jobs/#acceleration)!\n \n+\n+\n+\n # Compatibility\n \n Supported Platforms:"}, {"filename": "python/setup.py", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "file_content_changes": "@@ -66,10 +66,14 @@ def get_pybind11_package_info():\n \n \n def get_llvm_package_info():\n-    # download if nothing is installed\n+    # added statement for Apple Silicon\n     system = platform.system()\n+    arch = 'x86_64'\n     if system == \"Darwin\":\n         system_suffix = \"apple-darwin\"\n+        cpu_type = os.popen('sysctl machdep.cpu.brand_string').read()\n+        if \"apple\" in cpu_type.lower():\n+            arch = 'arm64'\n     elif system == \"Linux\":\n         vglibc = tuple(map(int, platform.libc_ver()[1].split('.')))\n         vglibc = vglibc[0] * 100 + vglibc[1]\n@@ -79,7 +83,7 @@ def get_llvm_package_info():\n         return Package(\"llvm\", \"LLVM-C.lib\", \"\", \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n     use_assert_enabled_llvm = check_env_flag(\"TRITON_USE_ASSERT_ENABLED_LLVM\", \"False\")\n     release_suffix = \"assert\" if use_assert_enabled_llvm else \"release\"\n-    name = f'llvm+mlir-17.0.0-x86_64-{system_suffix}-{release_suffix}'\n+    name = f'llvm+mlir-17.0.0-{arch}-{system_suffix}-{release_suffix}'\n     version = \"llvm-17.0.0-c5dede880d17\"\n     url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n     return Package(\"llvm\", name, url, \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n@@ -118,6 +122,7 @@ def get_thirdparty_packages(triton_cache_path):\n \n \n def download_and_copy_ptxas():\n+\n     base_dir = os.path.dirname(__file__)\n     src_path = \"bin/ptxas\"\n     version = \"12.1.105\""}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 41, "deletions": 28, "changes": 69, "file_content_changes": "@@ -65,7 +65,7 @@ enum backend_t {\n \n void init_triton_runtime(py::module &&m) {\n   // wrap backend_t\n-  py::enum_<backend_t>(m, \"backend\")\n+  py::enum_<backend_t>(m, \"backend\", py::module_local())\n       .value(\"HOST\", HOST)\n       .value(\"CUDA\", CUDA)\n       .value(\"ROCM\", ROCM)\n@@ -164,12 +164,14 @@ void init_triton_ir(py::module &&m) {\n   using ret = py::return_value_policy;\n   using namespace pybind11::literals;\n \n-  py::enum_<mlir::triton::PaddingOption>(m, \"PADDING_OPTION\")\n+  py::enum_<mlir::triton::PaddingOption>(m, \"PADDING_OPTION\",\n+                                         py::module_local())\n       .value(\"PAD_ZERO\", mlir::triton::PaddingOption::PAD_ZERO)\n       .value(\"PAD_NAN\", mlir::triton::PaddingOption::PAD_NAN)\n       .export_values();\n \n-  py::enum_<mlir::triton::CacheModifier>(m, \"CACHE_MODIFIER\")\n+  py::enum_<mlir::triton::CacheModifier>(m, \"CACHE_MODIFIER\",\n+                                         py::module_local())\n       .value(\"NONE\", mlir::triton::CacheModifier::NONE)\n       .value(\"CA\", mlir::triton::CacheModifier::CA)\n       .value(\"CG\", mlir::triton::CacheModifier::CG)\n@@ -178,20 +180,21 @@ void init_triton_ir(py::module &&m) {\n       .value(\"WT\", mlir::triton::CacheModifier::WT)\n       .export_values();\n \n-  py::enum_<mlir::triton::MemSemantic>(m, \"MEM_SEMANTIC\")\n+  py::enum_<mlir::triton::MemSemantic>(m, \"MEM_SEMANTIC\", py::module_local())\n       .value(\"ACQUIRE_RELEASE\", mlir::triton::MemSemantic::ACQUIRE_RELEASE)\n       .value(\"ACQUIRE\", mlir::triton::MemSemantic::ACQUIRE)\n       .value(\"RELEASE\", mlir::triton::MemSemantic::RELEASE)\n       .value(\"RELAXED\", mlir::triton::MemSemantic::RELAXED)\n       .export_values();\n \n-  py::enum_<mlir::triton::EvictionPolicy>(m, \"EVICTION_POLICY\")\n+  py::enum_<mlir::triton::EvictionPolicy>(m, \"EVICTION_POLICY\",\n+                                          py::module_local())\n       .value(\"NORMAL\", mlir::triton::EvictionPolicy::NORMAL)\n       .value(\"EVICT_FIRST\", mlir::triton::EvictionPolicy::EVICT_FIRST)\n       .value(\"EVICT_LAST\", mlir::triton::EvictionPolicy::EVICT_LAST)\n       .export_values();\n \n-  py::enum_<mlir::triton::RMWOp>(m, \"ATOMIC_OP\")\n+  py::enum_<mlir::triton::RMWOp>(m, \"ATOMIC_OP\", py::module_local())\n       .value(\"ADD\", mlir::triton::RMWOp::ADD)\n       .value(\"FADD\", mlir::triton::RMWOp::FADD)\n       .value(\"AND\", mlir::triton::RMWOp::AND)\n@@ -203,7 +206,7 @@ void init_triton_ir(py::module &&m) {\n       .value(\"UMIN\", mlir::triton::RMWOp::UMIN)\n       .value(\"UMAX\", mlir::triton::RMWOp::UMAX);\n \n-  py::class_<mlir::MLIRContext>(m, \"context\")\n+  py::class_<mlir::MLIRContext>(m, \"context\", py::module_local())\n       .def(py::init<>())\n       .def(\"load_triton\", [](mlir::MLIRContext &self) {\n         self.getOrLoadDialect<mlir::triton::TritonDialect>();\n@@ -259,7 +262,7 @@ void init_triton_ir(py::module &&m) {\n   // // py::class_<ir::undef_value, ir::constant>(m, \"undef\")\n   // //     .def(\"get\", &ir::undef_value::get, ret::reference);\n \n-  py::class_<mlir::Type>(m, \"type\")\n+  py::class_<mlir::Type>(m, \"type\", py::module_local())\n       .def(\"is_integer\", &mlir::Type::isInteger)\n       .def(\"is_fp16\", &mlir::Type::isF16)\n       .def(\"__str__\", [](mlir::Type &self) {\n@@ -269,21 +272,21 @@ void init_triton_ir(py::module &&m) {\n         return os.str();\n       });\n \n-  py::class_<mlir::FunctionType>(m, \"function_type\")\n+  py::class_<mlir::FunctionType>(m, \"function_type\", py::module_local())\n       .def(\"param_types\", [](mlir::FunctionType &self) {\n         return std::vector<mlir::Type>(self.getInputs().begin(),\n                                        self.getInputs().end());\n       });\n \n-  py::class_<mlir::Location>(m, \"location\")\n+  py::class_<mlir::Location>(m, \"location\", py::module_local())\n       .def(\"__str__\", [](mlir::Location &self) {\n         std::string str;\n         llvm::raw_string_ostream os(str);\n         self.print(os);\n         return os.str();\n       });\n \n-  py::class_<mlir::Value>(m, \"value\")\n+  py::class_<mlir::Value>(m, \"value\", py::module_local())\n       .def(\"set_attr\",\n            [](mlir::Value &self, std::string &name,\n               mlir::Attribute &attr) -> void {\n@@ -307,14 +310,15 @@ void init_triton_ir(py::module &&m) {\n            })\n       .def(\"get_type\", &mlir::Value::getType);\n \n-  py::class_<mlir::BlockArgument, mlir::Value>(m, \"block_argument\");\n+  py::class_<mlir::BlockArgument, mlir::Value>(m, \"block_argument\",\n+                                               py::module_local());\n \n-  py::class_<mlir::Region>(m, \"region\")\n+  py::class_<mlir::Region>(m, \"region\", py::module_local())\n       .def(\"get_parent_region\", &mlir::Region::getParentRegion, ret::reference)\n       .def(\"size\", [](mlir::Region &self) { return self.getBlocks().size(); })\n       .def(\"empty\", &mlir::Region::empty);\n \n-  py::class_<mlir::Block>(m, \"block\")\n+  py::class_<mlir::Block>(m, \"block\", py::module_local())\n       .def(\"arg\",\n            [](mlir::Block &self, int index) -> mlir::BlockArgument {\n              return self.getArgument(index);\n@@ -383,12 +387,14 @@ void init_triton_ir(py::module &&m) {\n   //     .value(\"retune\", eattr::retune)\n   //     .value(\"not_implemented\", eattr::not_implemented);\n \n-  py::class_<mlir::Attribute>(m, \"attribute\");\n-  py::class_<mlir::IntegerAttr, mlir::Attribute>(m, \"integer_attr\");\n-  py::class_<mlir::BoolAttr, mlir::Attribute>(m, \"bool_attr\");\n+  py::class_<mlir::Attribute>(m, \"attribute\", py::module_local());\n+  py::class_<mlir::IntegerAttr, mlir::Attribute>(m, \"integer_attr\",\n+                                                 py::module_local());\n+  py::class_<mlir::BoolAttr, mlir::Attribute>(m, \"bool_attr\",\n+                                              py::module_local());\n \n   // Ops\n-  py::class_<mlir::OpState>(m, \"OpState\")\n+  py::class_<mlir::OpState>(m, \"OpState\", py::module_local())\n       .def(\"set_attr\",\n            [](mlir::OpState &self, std::string &name,\n               mlir::Attribute &attr) -> void { self->setAttr(name, attr); })\n@@ -427,23 +433,27 @@ void init_triton_ir(py::module &&m) {\n         return mlir::succeeded(mlir::verify(self.getOperation()));\n       });\n   // scf Ops\n-  py::class_<mlir::scf::ForOp, mlir::OpState>(m, \"ForOp\")\n+  py::class_<mlir::scf::ForOp, mlir::OpState>(m, \"ForOp\", py::module_local())\n       .def(\"get_induction_var\", &mlir::scf::ForOp::getInductionVar);\n \n-  py::class_<mlir::scf::IfOp, mlir::OpState>(m, \"IfOp\")\n+  py::class_<mlir::scf::IfOp, mlir::OpState>(m, \"IfOp\", py::module_local())\n       .def(\"get_then_block\", &mlir::scf::IfOp::thenBlock, ret::reference)\n       .def(\"get_else_block\", &mlir::scf::IfOp::elseBlock, ret::reference)\n       .def(\"get_then_yield\", &mlir::scf::IfOp::thenYield)\n       .def(\"get_else_yield\", &mlir::scf::IfOp::elseYield);\n-  py::class_<mlir::scf::YieldOp, mlir::OpState>(m, \"YieldOp\");\n-  py::class_<mlir::scf::WhileOp, mlir::OpState>(m, \"WhileOp\")\n+  py::class_<mlir::scf::YieldOp, mlir::OpState>(m, \"YieldOp\",\n+                                                py::module_local());\n+  py::class_<mlir::scf::WhileOp, mlir::OpState>(m, \"WhileOp\",\n+                                                py::module_local())\n       .def(\"get_before\", &mlir::scf::WhileOp::getBefore, ret::reference)\n       .def(\"get_after\", &mlir::scf::WhileOp::getAfter, ret::reference);\n-  py::class_<mlir::scf::ConditionOp, mlir::OpState>(m, \"ConditionOp\");\n+  py::class_<mlir::scf::ConditionOp, mlir::OpState>(m, \"ConditionOp\",\n+                                                    py::module_local());\n \n   // dynamic_attr is used to transfer ownership of the MLIR context to the\n   // module\n-  py::class_<mlir::ModuleOp, mlir::OpState>(m, \"module\", py::dynamic_attr())\n+  py::class_<mlir::ModuleOp, mlir::OpState>(m, \"module\", py::module_local(),\n+                                            py::dynamic_attr())\n       .def(\"dump\", &mlir::ModuleOp::dump)\n       .def(\"str\",\n            [](mlir::ModuleOp &self) -> std::string {\n@@ -523,7 +533,8 @@ void init_triton_ir(py::module &&m) {\n       },\n       ret::take_ownership);\n \n-  py::class_<mlir::triton::FuncOp, mlir::OpState>(m, \"function\")\n+  py::class_<mlir::triton::FuncOp, mlir::OpState>(m, \"function\",\n+                                                  py::module_local())\n       // .def_property_readonly(\"attrs\", &ir::function::attrs)\n       // .def(\"add_attr\", &ir::function::add_attr);\n       .def(\"args\",\n@@ -571,9 +582,11 @@ void init_triton_ir(py::module &&m) {\n       .def_property_readonly(\"type\", &mlir::triton::FuncOp::getFunctionType)\n       .def(\"reset_type\", &mlir::triton::FuncOp::setType);\n \n-  py::class_<mlir::OpBuilder::InsertPoint>(m, \"InsertPoint\");\n+  py::class_<mlir::OpBuilder::InsertPoint>(m, \"InsertPoint\",\n+                                           py::module_local());\n \n-  py::class_<TritonOpBuilder>(m, \"builder\", py::dynamic_attr())\n+  py::class_<TritonOpBuilder>(m, \"builder\", py::module_local(),\n+                              py::dynamic_attr())\n       .def(py::init<mlir::MLIRContext *>())\n       // getters\n       .def(\"create_module\",\n@@ -1507,7 +1520,7 @@ void init_triton_ir(py::module &&m) {\n                                                          offsets);\n            });\n \n-  py::class_<mlir::PassManager>(m, \"pass_manager\")\n+  py::class_<mlir::PassManager>(m, \"pass_manager\", py::module_local())\n       .def(py::init<mlir::MLIRContext *>())\n       .def(\"enable_debug\",\n            [](mlir::PassManager &self) {"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -44,7 +44,6 @@ def nvsmi(attrs):\n         (512, 512, 512): {'float16': 0.061, 'float32': 0.097, 'int8': 0.05},\n         (1024, 1024, 1024): {'float16': 0.283, 'float32': 0.313, 'int8': 0.169},\n         (2048, 2048, 2048): {'float16': 0.618, 'float32': 0.532, 'int8': 0.34},\n-        (4096, 4096, 4096): {'float16': 0.751, 'float32': 0.726, 'int8': 0.46},\n         (8192, 8192, 8192): {'float16': 0.786, 'float32': 0.754, 'int8': 0.51},\n         # tall-skinny\n         (16, 1024, 1024): {'float16': 0.006, 'float32': 0.009, 'int8': 0.005},\n@@ -56,6 +55,8 @@ def nvsmi(attrs):\n         (1024, 64, 1024): {'float16': 0.029, 'float32': 0.046, 'int8': 0.017},\n         (4096, 64, 4096): {'float16': 0.179, 'float32': 0.214, 'int8': 0.102},\n         (8192, 64, 8192): {'float16': 0.278, 'float32': 0.000, 'int8': 0.177},\n+        # test EVEN_K==False\n+        (8192, 8192, 8176): {'float16': 0.786, 'float32': 0.696, 'int8': 0.51},\n     }\n }\n \n@@ -115,7 +116,6 @@ def _add(x_ptr, y_ptr, output_ptr, n_elements,\n         1024 * 64: {'float16': 0.013, 'float32': 0.026},\n         1024 * 256: {'float16': 0.053, 'float32': 0.105},\n         1024 * 1024: {'float16': 0.212, 'float32': 0.420},\n-        1024 * 4096: {'float16': 0.791, 'float32': 0.668},\n         1024 * 16384: {'float16': 0.762, 'float32': 0.812},\n         1024 * 65536: {'float16': 0.846, 'float32': 0.869},\n         # Non pow 2\n@@ -162,7 +162,7 @@ def test_elementwise(N, dtype_str):\n         (4, 48, 4096, 64, True, True, 'backward', 'bfloat16'): 0.202,\n         (4, 48, 1024, 16, True, True, 'backward', 'float32'): 0.089,\n         (4, 48, 4096, 64, True, False, 'forward', 'float16'): 0.242,\n-        (4, 48, 4096, 64, True, False, 'forward', 'bfloat16'): 0.248,\n+        (4, 48, 4096, 64, True, False, 'forward', 'bfloat16'): 0.220,\n         (4, 48, 1024, 16, True, False, 'forward', 'float32'): 0.069,\n         (4, 48, 4096, 64, True, False, 'backward', 'float16'): 0.136,\n         (4, 48, 4096, 64, True, False, 'backward', 'bfloat16'): 0.135,\n@@ -173,8 +173,8 @@ def test_elementwise(N, dtype_str):\n         (4, 48, 4096, 64, False, True, 'backward', 'float16'): 0.265,\n         (4, 48, 4096, 64, False, True, 'backward', 'bfloat16'): 0.257,\n         (4, 48, 1024, 16, False, True, 'backward', 'float32'): 0.128,\n-        (4, 48, 4096, 64, False, False, 'forward', 'float16'): 0.242,\n-        (4, 48, 4096, 64, False, False, 'forward', 'bfloat16'): 0.248,\n+        (4, 48, 4096, 64, False, False, 'forward', 'float16'): 0.251,\n+        (4, 48, 4096, 64, False, False, 'forward', 'bfloat16'): 0.220,\n         (4, 48, 1024, 16, False, False, 'forward', 'float32'): 0.069,\n         (4, 48, 4096, 64, False, False, 'backward', 'float16'): 0.159,\n         (4, 48, 4096, 64, False, False, 'backward', 'bfloat16'): 0.138,"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -2691,7 +2691,7 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # -------------\n \n \n-@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and\"])\n+@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and_dynamic\", \"if_and_static\"])\n def test_if(if_type, device):\n \n     @triton.jit"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -663,6 +663,11 @@ def bitcast(input: tl.tensor,\n                      dst_ty)\n \n \n+# TODO: architecture descriptor class\n+def _is_cuda(arch):\n+    return isinstance(arch, int)\n+\n+\n def cast(input: tl.tensor,\n          dst_ty: tl.dtype,\n          builder: ir.builder) -> tl.tensor:\n@@ -677,7 +682,7 @@ def cast(input: tl.tensor,\n     src_sca_ty = src_ty.scalar\n     dst_sca_ty = dst_ty.scalar\n \n-    if builder.arch < 89 and \\\n+    if _is_cuda(builder.arch) and builder.arch < 89 and \\\n        (src_sca_ty.is_fp8e4() or dst_sca_ty.is_fp8e4()):\n         warnings.warn(\"Standard tl.float8e4 format will be deprecated on SM < 89. \"\n                       \"Please use tl.float8e4b15.\", DeprecationWarning)"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -94,11 +94,14 @@ def _fwd_kernel(\n     # load q: it will stay in SRAM throughout\n     q = tl.load(Q_block_ptr)\n     q = (q * qk_scale).to(K.dtype.element_ty)\n+    # advance block pointers to first iteration of the loop\n+    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n+    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n     # loop over k, v and update accumulator\n     for start_n in range(lo, hi, BLOCK_N):\n         start_n = tl.multiple_of(start_n, BLOCK_N)\n         # -- compute qk ----\n-        k = tl.load(tl.advance(K_block_ptr, (0, start_n)))\n+        k = tl.load(K_block_ptr)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         qk += tl.dot(q, k, allow_tf32=True)\n         if MODE == 1 or MODE == 3:\n@@ -120,12 +123,15 @@ def _fwd_kernel(\n         acc_scale = l_i / l_i_new\n         acc = acc * acc_scale[:, None]\n         # update acc\n-        v = tl.load(tl.advance(V_block_ptr, (start_n, 0)))\n+        v = tl.load(V_block_ptr)\n         p = p.to(V.dtype.element_ty)\n         acc += tl.dot(p, v, allow_tf32=True)\n         # update m_i and l_i\n         l_i = l_i_new\n         m_i = m_i_new\n+        # update pointers\n+        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n+        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n     # write back l and m\n     l_ptrs = L + off_hz * N_CTX + offs_m\n     m_ptrs = M + off_hz * N_CTX + offs_m"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -93,11 +93,14 @@ def _fwd_kernel(\n     # load q: it will stay in SRAM throughout\n     q = tl.load(Q_block_ptr)\n     q = (q * qk_scale).to(tl.float16)\n+    # advance block pointers to first iteration of the loop\n+    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n+    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n     # loop over k, v and update accumulator\n     for start_n in range(lo, hi, BLOCK_N):\n         start_n = tl.multiple_of(start_n, BLOCK_N)\n         # -- compute qk ----\n-        k = tl.load(tl.advance(K_block_ptr, (0, start_n)))\n+        k = tl.load(K_block_ptr)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         qk += tl.dot(q, k)\n         if MODE == 1 or MODE == 3:\n@@ -119,12 +122,15 @@ def _fwd_kernel(\n         acc_scale = l_i / l_i_new\n         acc = acc * acc_scale[:, None]\n         # update acc\n-        v = tl.load(tl.advance(V_block_ptr, (start_n, 0)))\n+        v = tl.load(V_block_ptr)\n         p = p.to(tl.float16)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n         l_i = l_i_new\n         m_i = m_i_new\n+        # update pointers\n+        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n+        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n     # write back l and m\n     l_ptrs = L + off_hz * N_CTX + offs_m\n     m_ptrs = M + off_hz * N_CTX + offs_m"}]