[{"filename": "python/tests/test_core.py", "status": "modified", "additions": 13, "deletions": 38, "changes": 51, "file_content_changes": "@@ -976,34 +976,19 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n @pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n-    # we use triton-gpu IR directly so we can test different layouts\n-    kernel_str = \"\"\"\n-#layout = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n-#slice = #triton_gpu.slice<{dim = 1, parent = #layout}>\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func public @kernel_0d1d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n-    %cst = arith.constant dense<32> : tensor<4x1xi32, #layout>\n-    %0 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #slice>\n-    %1 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #triton_gpu.slice<{dim = 1, parent = #layout}>>\n-    %2 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<4x1x!tt.ptr<f32>, #layout>\n-    %3 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #layout}>>\n-    %4 = tt.expand_dims %1 {axis = 1 : i32} : (tensor<4xi32, #triton_gpu.slice<{dim = 1, parent = #layout}>>) -> tensor<4x1xi32, #layout>\n-    %5 = arith.muli %4, %cst : tensor<4x1xi32, #layout>\n-    %6 = tt.addptr %2, %5 : tensor<4x1x!tt.ptr<f32>, #layout>\n-    %7 = tt.broadcast %6 : (tensor<4x1x!tt.ptr<f32>, #layout>) -> tensor<4x32x!tt.ptr<f32>, #layout>\n-    %8 = tt.expand_dims %3 {axis = 0 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #layout}>>) -> tensor<1x32xi32, #layout>\n-    %9 = tt.broadcast %8 : (tensor<1x32xi32, #layout>) -> tensor<4x32xi32, #layout>\n-    %10 = tt.addptr %7, %9 : tensor<4x32x!tt.ptr<f32>, #layout>\n-    %11 = tt.load %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<4x32xf32, #layout>\n-    %13 = tt.reduce %11 {axis = 1 : i32, redOp = 2 : i32} : tensor<4x32xf32, #layout> -> tensor<4xf32, #slice>\n-    %15 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<4x!tt.ptr<f32>, #slice>\n-    %16 = tt.addptr %15, %0 : tensor<4x!tt.ptr<f32>, #slice>\n-    tt.store %16, %13 : tensor<4xf32, #slice>\n-    return\n-  }\n-}    \n-\"\"\"\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n+        range_m = tl.arange(0, BLOCK_M)\n+        range_n = tl.arange(0, BLOCK_N)\n+        x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n+        z = GENERATE_TEST_HERE\n+        if AXIS == 1:\n+            tl.store(Z + range_m, z)\n+        else:\n+            tl.store(Z + range_n, z)\n \n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=AXIS)'})\n     # input\n     rs = RandomState(17)\n     # limit the range of integers so that the sum does not overflow\n@@ -1025,18 +1010,8 @@ def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n     # triton result\n     z_tri = to_triton(numpy_random((shape[1 - axis],), dtype_str=z_dtype_str, rs=rs),\n                       device=device, dst_type=z_tri_dtype_str)\n-\n-    import tempfile\n-    import os\n-    with tempfile.TemporaryDirectory() as tmpdir:\n-        src = os.path.join(tmpdir, 'reduce.ttgir')\n-        with open(src, 'w') as f:\n-            f.write(kernel_str)\n-        kernel = triton.compile(src, signature=\"*f32,*f32\", device=torch.cuda.current_device(), constants={2:shape[0], 3:shape[1], 4:axis})\n-        kernel[(1,1,1)](x_tri.data_ptr(), z_tri.data_ptr())\n-\n+    kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n     z_tri = to_numpy(z_tri)\n-\n     # compare\n     if op == 'sum':\n         np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)"}]