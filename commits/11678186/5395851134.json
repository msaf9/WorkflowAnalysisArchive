[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 0, "deletions": 45, "changes": 45, "file_content_changes": "@@ -3039,51 +3039,6 @@ def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device):\n     assert torch.equal(z, x)\n \n \n-def test_swizzling():\n-    M, N = 128, 128\n-    dtype = 'float16'\n-    ir = f\"\"\"\n-    #blocked = #triton_gpu.blocked<{{sizePerThread = [1, 1], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}}>\n-    #mma = #triton_gpu.mma<{{versionMajor=2, versionMinor=0, warpsPerCTA=[2, 2]}}>\n-    module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n-  tt.func public @kernel_0d1d(%arg0: !tt.ptr<f16> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<f16> {{tt.divisibility = 16 : i32}}) {{\n-    %cst = arith.constant dense<{M}> : tensor<{M}x1xi32, #blocked>\n-    %0 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>\n-    %1 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>\n-    %2 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<{M}x{N}x!tt.ptr<f16>, #blocked>\n-    %4 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>\n-    %5 = arith.muli %4, %cst : tensor<{M}x1xi32, #blocked>\n-    %6 = tt.expand_dims %1 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x{N}xi32, #blocked>\n-    %7 = tt.broadcast %6 : (tensor<1x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #blocked>\n-    %8 = tt.broadcast %5 : (tensor<{M}x1xi32, #blocked>) -> tensor<{M}x{N}xi32, #blocked>\n-    %9 = arith.addi %8, %7 : tensor<{M}x{N}xi32, #blocked>\n-    %10 = tt.addptr %2, %9 : tensor<{M}x{N}x!tt.ptr<f16>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n-    %11 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xf16, #blocked>\n-    %3 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<{M}x{N}x!tt.ptr<f16>, #mma>\n-    %12 = triton_gpu.convert_layout %9 : (tensor<{M}x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #mma>\n-    %13 = triton_gpu.convert_layout %11 : (tensor<{M}x{N}xf16, #blocked>) -> tensor<{M}x{N}xf16, #mma>\n-    %14 = tt.addptr %3, %12 : tensor<{M}x{N}x!tt.ptr<f16>, #mma>, tensor<{M}x{N}xi32, #mma>\n-    tt.store %14, %13 : tensor<{M}x{N}xf16, #mma>\n-    tt.return\n-  }}\n-}}\n-\"\"\"\n-    x_list = [[j * N + i for i in range(N)] for j in range(M)]\n-    x_np = np.array(x_list, dtype=dtype)\n-    x = to_triton(x_np)\n-    z = torch.ones((M, N), dtype=torch.float16, device='cuda')\n-    z = to_triton(numpy_random((M, N), dtype_str=\"float16\", rs=RandomState(17)))\n-    # write the IR to a temporary file using mkstemp\n-    import tempfile\n-    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n-        f.write(ir)\n-        f.flush()\n-        kernel = triton.compile(f.name)\n-    kernel[(1, 1, 1)](x.data_ptr(), z.data_ptr())\n-\n-    assert torch.equal(z, x)\n-\n-\n def test_load_scalar_with_mask(device):\n     @triton.jit\n     def kernel(Input, Index, Out, N: int):"}]