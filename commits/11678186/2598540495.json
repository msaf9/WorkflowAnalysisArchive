[{"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -802,6 +802,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n     int width = std::min(tot_width, max_word_width);\n     int n_words = std::max(1, tot_width / width);\n     bool has_l2_evict_policy = (x->get_eviction_policy() != ir::load_inst::NORMAL) && tgt_->as_nvidia()->sm() >= 80;\n+    has_l2_evict_policy = false;\n     // has_evict_policy = false; // currently disable until supported in `store`\n     // -----\n     // create inline asm string\n@@ -966,6 +967,7 @@ void generator::visit_store_inst(ir::store_inst * x){\n       vec = std::min<size_t>(2, aln);\n   }\n   bool has_l2_evict_policy = (x->get_eviction_policy() != ir::load_inst::NORMAL) && tgt_->as_nvidia()->sm() >= 80;\n+  has_l2_evict_policy = false;\n   auto idxs    = idxs_.at(val_op);\n   Type *ty = cvt(val_op->get_type()->get_scalar_ty());\n   if (ty->isBFloatTy()) // llvm11-nvptx cannot select bf16 store"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 25, "deletions": 7, "changes": 32, "file_content_changes": "@@ -385,6 +385,8 @@ def test_index1d(expr, dtype_str, device='cuda'):\n     rank_y = expr.count(',') + 1\n     shape_x = [32 for _ in range(rank_x)]\n     shape_z = [32 for _ in range(rank_y)]\n+    shape_z_rank_mismatch = [32 for _ in range(rank_y + 1)]\n+    shape_z_dim_mismatch = [64 for _ in range(rank_y)]\n \n     # Triton kernel\n     @triton.jit\n@@ -395,12 +397,17 @@ def kernel(Z, X, SIZE: tl.constexpr):\n         z = GENERATE_TEST_HERE\n         tl.store(Z_PTR_EXPR, z)\n \n-    to_replace = {\n-        'X_PTR_EXPR': make_ptr_str('X', shape_x),\n-        'Z_PTR_EXPR': make_ptr_str('Z', shape_z),\n-        'GENERATE_TEST_HERE': expr,\n-    }\n-    kernel = patch_kernel(kernel, to_replace)\n+    def generate_kernel(shape_x, shape_z):\n+        to_replace = {\n+            'X_PTR_EXPR': make_ptr_str('X', shape_x),\n+            'Z_PTR_EXPR': make_ptr_str('Z', shape_z),\n+            'GENERATE_TEST_HERE': expr,\n+        }\n+        return patch_kernel(kernel, to_replace)\n+\n+    kernel_match = generate_kernel(shape_x, shape_z)\n+    kernel_dim_mismatch = generate_kernel(shape_x, shape_z_dim_mismatch)\n+    kernel_rank_mismatch = generate_kernel(shape_x, shape_z_rank_mismatch)\n \n     # torch result\n     x = numpy_random(shape_x, dtype_str=dtype_str)\n@@ -409,10 +416,21 @@ def kernel(Z, X, SIZE: tl.constexpr):\n     # triton result\n     z_tri = to_triton(np.empty_like(z_ref), device=device)\n     x_tri = to_triton(x)\n-    kernel[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n+    kernel_match[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n     # compare\n     assert (z_ref == to_numpy(z_tri)).all()\n \n+    def catch_compilation_error(kernel):\n+        try:\n+            kernel[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n+        except triton.code_gen.CompilationError as e:\n+            np.testing.assert_(True)\n+        except BaseException:\n+            np.testing.assert_(False)\n+\n+    catch_compilation_error(kernel_dim_mismatch)\n+    catch_compilation_error(kernel_rank_mismatch)\n+\n \n # ---------------\n # test tuples"}, {"filename": "python/triton/code_gen.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -956,7 +956,7 @@ def add_to_cache(self, key, wargs, device_idx, num_warps, num_stages):\n         return self.fn._warmup(key, arg_types=arg_types, device=device_idx, attributes=attributes, constants=constants, num_warps=num_warps, num_stages=num_stages, is_manual_warmup=False)\n \n     def __call__(self, *wargs, grid, num_warps=4, num_stages=2, **kwargs):\n-        assert num_warps != 0 and (num_warps & (num_warps - 1)) == 0, f\"{num_warps=} must be a power of 2.\"\n+        assert num_warps != 0 and (num_warps & (num_warps - 1)) == 0, f\"num_warps={num_warps} must be a power of 2.\"\n         # handle arguments passed by name\n         kwargs = {self.fn.arg_names.index(name): value for name, value in kwargs.items()}\n         wargs = list(wargs)"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -482,6 +482,11 @@ def broadcast_impl_shape(input: tl.tensor,\n         raise ValueError(f\"Cannot broadcast, rank mismatch: {src_shape}, {shape}\")\n     if shape == src_shape:\n         return input\n+    for i in range(len(src_shape)):\n+        if shape[i] != src_shape[i] and src_shape[i] != 1:\n+            raise ValueError(f\"Cannot broadcast, the expanded size of the tensor ({shape[i]})\"\n+                             f\" must match the existing size ({src_shape[1]}) at non-singleton dimension\"\n+                             f\" {i}: {src_shape}, {shape}\")\n     ret_ty = tl.block_type(input.type.scalar, shape)\n     return tl.tensor(builder.create_broadcast(input.handle, shape), ret_ty)\n "}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 26, "deletions": 22, "changes": 48, "file_content_changes": "@@ -128,17 +128,19 @@ def _layer_norm_bwd_dwdb(\n     cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n     dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n     db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-    for i in range(0, M, BLOCK_SIZE_M):\n-        rows = i + tl.arange(0, BLOCK_SIZE_M)\n-        mask = (rows[:, None] < M) & (cols[None, :] < N)\n-        offs = rows[:, None] * N + cols[None, :]\n-        a = tl.load(A + offs, mask=mask, other=0.).to(tl.float32)\n-        dout = tl.load(DOut + offs, mask=mask, other=0.).to(tl.float32)\n-        mean = tl.load(Mean + rows, mask=rows < M, other=0.)\n-        rstd = tl.load(Var + rows, mask=rows < M, other=0.)\n-        a_hat = (a - mean[:, None]) * rstd[:, None]\n-        dw += dout * a_hat\n-        db += dout\n+    UNROLL: tl.constexpr = 4\n+    for i in range(0, M, BLOCK_SIZE_M * UNROLL):\n+        for j in range(UNROLL):\n+            rows = i + j * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+            mask = (rows[:, None] < M) & (cols[None, :] < N)\n+            offs = rows[:, None] * N + cols[None, :]\n+            a = tl.load(A + offs, mask=mask, other=0.).to(tl.float32)\n+            dout = tl.load(DOut + offs, mask=mask, other=0.).to(tl.float32)\n+            mean = tl.load(Mean + rows, mask=rows < M, other=0.)\n+            rstd = tl.load(Var + rows, mask=rows < M, other=0.)\n+            a_hat = (a - mean[:, None]) * rstd[:, None]\n+            dw += dout * a_hat\n+            db += dout\n     sum_dw = tl.sum(dw, axis=0)\n     sum_db = tl.sum(db, axis=0)\n     tl.store(DW + cols, sum_dw, mask=cols < N)\n@@ -211,7 +213,15 @@ def backward(ctx, dout):\n             BLOCK_SIZE_N=ctx.BLOCK_SIZE,\n             num_warps=ctx.num_warps,\n         )\n-        # accumulate partial sums in separate kernel\n+        if N > 10240:\n+            BLOCK_SIZE_N = 128\n+            BLOCK_SIZE_M = 32\n+            num_warps = 4\n+        else:\n+            # maximize occupancy for small N\n+            BLOCK_SIZE_N = 16\n+            BLOCK_SIZE_M = 16\n+            num_warps = 8\n         grid = lambda meta: [triton.cdiv(N, meta[\"BLOCK_SIZE_N\"])]\n         _layer_norm_bwd_dwdb[grid](\n             a, dout,\n@@ -220,17 +230,11 @@ def backward(ctx, dout):\n             dbias,\n             M,\n             N,\n-            BLOCK_SIZE_M=32,\n-            BLOCK_SIZE_N=128,\n+            BLOCK_SIZE_M=BLOCK_SIZE_M,\n+            BLOCK_SIZE_N=BLOCK_SIZE_N,\n+            num_warps=num_warps\n         )\n-        return (da, None, dweight, dbias, None, None,\n-                None, None, None, None,\n-                None,\n-                None, None, None,\n-                None,\n-                None, None, None,\n-                None, None, None,\n-                None, None, None)\n+        return (da, None, dweight, dbias, None)\n \n \n def layer_norm(a, normalized_shape, weight, bias, eps):"}]