[{"filename": ".github/workflows/torchinductor/data/huggingface.csv", "status": "modified", "additions": 36, "deletions": 36, "changes": 72, "file_content_changes": "@@ -1,37 +1,37 @@\n dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio\n-cuda,AlbertForMaskedLM,4,1.5637,196.1976,36.7633,1.2637\n-cuda,AlbertForQuestionAnswering,4,1.5697,193.9925,22.3314,1.3133\n-cuda,BartForCausalLM,4,1.4704,88.2529,36.2264,0.9750\n-cuda,BertForMaskedLM,16,1.5146,83.3326,42.1532,1.0497\n-cuda,BertForQuestionAnswering,16,1.6353,65.9640,30.3219,1.1693\n-cuda,BlenderbotSmallForCausalLM,64,1.1694,62.6029,28.2282,0.9127\n-cuda,BlenderbotSmallForConditionalGeneration,64,1.3153,116.4360,51.6582,0.9804\n-cuda,CamemBert,16,1.4672,93.1047,33.8399,1.0462\n-cuda,DebertaForMaskedLM,4,0.8610,94.2568,41.9968,1.0406\n-cuda,DebertaForQuestionAnswering,8,0.9550,94.5806,42.3991,1.1506\n-cuda,DebertaV2ForMaskedLM,1,0.7829,214.8802,64.2595,0.9772\n-cuda,DistilBertForMaskedLM,128,1.2346,80.7496,30.1995,0.9625\n-cuda,DistilBertForQuestionAnswering,256,1.3784,86.2125,32.3333,1.1469\n-cuda,DistillGPT2,16,1.6558,72.8982,21.3507,1.0639\n-cuda,ElectraForCausalLM,32,1.5242,67.9513,57.7771,0.9719\n-cuda,ElectraForQuestionAnswering,64,1.9230,67.2341,45.5223,1.1624\n-cuda,GPT2ForSequenceClassification,4,2.0511,53.7014,31.2262,1.2305\n-cuda,LayoutLMForMaskedLM,16,1.5055,84.8592,40.6248,1.0491\n-cuda,LayoutLMForSequenceClassification,16,1.6464,66.9082,35.1418,1.1401\n-cuda,MBartForCausalLM,4,1.4704,88.7321,29.1925,0.9831\n-cuda,MegatronBertForCausalLM,4,1.1061,136.3322,79.4501,1.0946\n-cuda,MegatronBertForQuestionAnswering,8,1.2551,133.9124,75.9488,1.1147\n-cuda,MobileBertForMaskedLM,64,0.9569,333.3552,130.7601,1.0135\n-cuda,MobileBertForQuestionAnswering,128,0.9634,331.8111,126.0109,0.8400\n-cuda,PLBartForCausalLM,8,1.5155,83.1783,24.3849,0.9886\n-cuda,PLBartForConditionalGeneration,4,1.4414,93.6038,52.3630,1.0496\n-cuda,PegasusForCausalLM,32,1.1225,79.1829,36.5687,0.9737\n-cuda,PegasusForConditionalGeneration,32,1.1506,175.4371,59.1006,1.0686\n-cuda,RobertaForCausalLM,16,1.5780,83.6565,33.7543,1.0491\n-cuda,RobertaForQuestionAnswering,16,1.6336,66.3454,29.9597,1.1698\n-cuda,Speech2Text2ForCausalLM,256,1.5464,41.4059,25.6908,0.8768\n-cuda,T5ForConditionalGeneration,4,1.2736,96.9787,54.8479,1.1802\n-cuda,T5Small,4,1.2861,98.4766,32.1507,1.1802\n-cuda,TrOCRForCausalLM,32,1.2573,127.5731,36.5153,0.9584\n-cuda,XLNetLMHeadModel,8,1.6924,177.0149,83.7423,1.1026\n-cuda,YituTechConvBert,16,1.4142,107.2519,68.8073,1.0363\n\\ No newline at end of file\n+cuda,AlbertForMaskedLM,4,1.5511,164.3373,26.8523,1.2647\n+cuda,AlbertForQuestionAnswering,4,1.5501,163.5580,25.7983,1.3145\n+cuda,BartForCausalLM,4,1.5080,71.7230,32.8907,0.9749\n+cuda,BertForMaskedLM,16,1.5350,67.9451,35.3286,1.0494\n+cuda,BertForQuestionAnswering,16,1.6735,53.2963,34.3754,1.1710\n+cuda,BlenderbotSmallForCausalLM,64,1.2106,46.6466,23.8058,0.9120\n+cuda,BlenderbotSmallForConditionalGeneration,64,1.3616,77.3013,55.3546,0.9803\n+cuda,CamemBert,16,1.4779,76.1809,35.3883,1.0469\n+cuda,DebertaForMaskedLM,4,0.8415,62.3395,35.9657,1.0418\n+cuda,DebertaForQuestionAnswering,8,1.0609,67.5151,35.7728,1.1528\n+cuda,DebertaV2ForMaskedLM,1,0.6026,134.6517,66.1783,0.9773\n+cuda,DistilBertForMaskedLM,128,1.2460,66.9382,18.3089,0.9624\n+cuda,DistilBertForQuestionAnswering,256,1.3997,72.4126,18.1956,1.1486\n+cuda,DistillGPT2,16,1.6656,60.5455,17.2280,1.0641\n+cuda,ElectraForCausalLM,32,1.8299,45.4841,37.0944,0.9717\n+cuda,ElectraForQuestionAnswering,64,2.0289,52.6890,35.9632,1.1928\n+cuda,GPT2ForSequenceClassification,4,2.2567,38.2969,30.0527,1.2323\n+cuda,LayoutLMForMaskedLM,16,1.5423,68.8018,36.5562,1.0495\n+cuda,LayoutLMForSequenceClassification,16,1.7058,53.9355,35.2225,1.1659\n+cuda,MBartForCausalLM,4,1.4945,71.4649,32.8653,0.9830\n+cuda,MegatronBertForCausalLM,4,1.4328,58.4404,70.6226,1.0951\n+cuda,MegatronBertForQuestionAnswering,8,1.5886,85.2533,69.1219,1.1152\n+cuda,MobileBertForMaskedLM,64,0.9007,131.7379,107.5275,1.0136\n+cuda,MobileBertForQuestionAnswering,128,0.8435,167.9066,106.7049,0.8579\n+cuda,PLBartForCausalLM,8,1.5261,68.9224,19.5826,0.9887\n+cuda,PLBartForConditionalGeneration,4,1.5298,71.2811,45.6902,1.0495\n+cuda,PegasusForCausalLM,32,1.2212,57.5436,33.3863,0.9736\n+cuda,PegasusForConditionalGeneration,32,1.2822,106.4678,69.8825,1.0689\n+cuda,RobertaForCausalLM,16,1.6128,67.5706,34.7355,1.0496\n+cuda,RobertaForQuestionAnswering,16,1.6800,53.6267,33.8527,1.1704\n+cuda,Speech2Text2ForCausalLM,256,1.8230,32.9145,18.7201,0.8760\n+cuda,T5ForConditionalGeneration,4,1.6592,59.5324,39.4406,1.1814\n+cuda,T5Small,4,1.6581,59.5930,37.0471,1.1814\n+cuda,TrOCRForCausalLM,32,1.2586,106.2633,32.5330,0.9583\n+cuda,XLNetLMHeadModel,8,1.8108,142.8795,84.8197,1.1240\n+cuda,YituTechConvBert,16,1.5207,81.4595,53.1565,1.0362"}, {"filename": ".github/workflows/torchinductor/data/timm_models.csv", "status": "modified", "additions": 53, "deletions": 53, "changes": 106, "file_content_changes": "@@ -1,54 +1,54 @@\n dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio\n-cuda,adv_inception_v3,128,1.5315,126.8003,157.5622,1.0179\n-cuda,beit_base_patch16_224,64,1.3308,91.0230,35.2707,0.9891\n-cuda,coat_lite_mini,128,2.0396,63.6890,87.8844,1.0199\n-cuda,convmixer_768_32,32,1.0443,336.7657,32.3711,0.9999\n-cuda,convnext_base,64,1.4920,97.2042,76.9539,1.0388\n-cuda,crossvit_9_240,128,1.2019,87.2138,86.0260,1.0032\n-cuda,cspdarknet53,64,1.4388,90.4760,88.7691,1.0140\n-cuda,deit_base_distilled_patch16_224,64,1.2471,81.1739,45.5359,0.9527\n-cuda,dla102,128,1.5027,147.5660,82.0541,1.0331\n-cuda,dm_nfnet_f0,128,1.4283,98.4167,67.5993,1.0713\n-cuda,dpn107,32,1.2356,100.9084,91.1994,0.9651\n-cuda,eca_botnext26ts_256,128,1.5012,86.6883,79.4803,1.0043\n-cuda,ese_vovnet19b_dw,128,1.3848,52.7837,47.5720,0.9915\n-cuda,fbnetc_100,128,1.5364,64.6275,121.7965,0.9548\n-cuda,gernet_l,128,1.1577,76.1686,56.8719,0.9712\n-cuda,ghostnet_100,128,1.7220,65.4748,209.2949,1.0223\n-cuda,gluon_inception_v3,128,1.5343,126.3718,52.4958,1.0466\n-cuda,gmixer_24_224,128,1.6468,84.2051,51.3789,1.1584\n-cuda,gmlp_s16_224,128,1.5873,94.6600,58.2645,1.2023\n-cuda,hrnet_w18,128,1.3546,263.2673,198.3349,0.9923\n-cuda,inception_v3,128,1.5238,126.7710,51.6400,1.0466\n-cuda,jx_nest_base,32,1.2384,103.6627,78.7322,0.9607\n-cuda,lcnet_050,128,1.7771,21.2319,34.1647,0.9458\n-cuda,mixer_b16_224,128,1.2902,108.2678,27.6445,0.9948\n-cuda,mixnet_l,128,1.2122,182.1990,118.8907,0.9908\n-cuda,mnasnet_100,128,1.6199,47.7919,48.7633,0.9408\n-cuda,mobilenetv2_100,128,1.5661,50.4129,42.9730,1.1166\n-cuda,mobilenetv3_large_100,128,1.5888,46.3295,51.9647,0.9704\n-cuda,mobilevit_s,64,1.3031,82.1195,111.3226,1.0065\n-cuda,nfnet_l0,128,1.4895,81.4742,58.5768,0.9691\n-cuda,pit_b_224,64,1.4120,97.3046,43.7878,1.0241\n-cuda,pnasnet5large,16,1.0523,239.1102,145.2293,1.2797\n-cuda,poolformer_m36,64,1.2154,138.0360,93.0292,1.1927\n-cuda,regnety_002,128,1.2659,38.8745,68.1799,0.8660\n-cuda,repvgg_a2,128,1.2185,73.6415,32.3085,0.9735\n-cuda,res2net101_26w_4s,64,1.0443,116.1409,144.6286,0.9491\n-cuda,res2net50_14w_8s,128,1.3212,130.1624,102.7642,0.9609\n-cuda,res2next50,128,1.2159,157.3657,46.9827,0.9756\n-cuda,resmlp_12_224,128,1.2970,54.6888,40.0312,1.0342\n-cuda,resnest101e,64,1.4079,134.5610,119.5467,1.0831\n-cuda,rexnet_100,128,1.4427,65.3909,222.1865,1.0439\n-cuda,selecsls42b,128,1.4015,53.4159,31.3161,0.9731\n-cuda,spnasnet_100,128,1.5507,54.3208,34.4102,1.0045\n-cuda,swin_base_patch4_window7_224,64,1.5038,115.4018,104.8326,0.9043\n-cuda,swsl_resnext101_32x16d,32,0.9981,136.5238,49.8939,0.9833\n-cuda,tf_efficientnet_b0,128,1.4894,67.2972,57.0583,1.0725\n-cuda,tf_mixnet_l,128,1.2179,189.8781,68.3717,1.0676\n-cuda,tinynet_a,128,1.3548,64.3571,117.1189,1.0718\n-cuda,tnt_s_patch16_224,128,3.0069,126.5317,67.8712,1.0505\n-cuda,twins_pcpvt_base,64,1.2016,154.8390,144.5083,1.0541\n-cuda,visformer_small,128,1.1935,87.3201,42.3853,1.0220\n-cuda,vit_base_patch16_224,64,1.2207,85.2031,39.3641,0.9551\n-cuda,volo_d1_224,0,0.0000\n\\ No newline at end of file\n+cuda,adv_inception_v3,128,1.5923,102.5292,51.6032,1.0472\n+cuda,beit_base_patch16_224,64,1.3390,75.3027,29.7471,1.0156\n+cuda,coat_lite_mini,128,2.0579,53.3689,37.1856,1.0437\n+cuda,convmixer_768_32,32,1.0470,275.5328,23.8037,0.9999\n+cuda,convnext_base,64,1.5084,80.1811,42.5659,1.0373\n+cuda,crossvit_9_240,128,1.5392,37.1806,44.9986,0.9193\n+cuda,cspdarknet53,64,1.4721,75.0403,35.2882,1.0547\n+cuda,deit_base_distilled_patch16_224,64,1.1432,55.9737,23.4038,0.9816\n+cuda,dla102,128,1.5282,123.7284,49.3612,1.0430\n+cuda,dm_nfnet_f0,128,1.4354,79.7518,34.8994,1.1038\n+cuda,dpn107,32,1.2412,83.8921,58.9111,0.9952\n+cuda,eca_botnext26ts_256,128,1.5425,71.2406,28.8920,1.0270\n+cuda,ese_vovnet19b_dw,128,1.4647,42.4837,18.0285,1.0135\n+cuda,fbnetc_100,128,1.5795,53.8033,33.0222,1.0082\n+cuda,gernet_l,128,1.1684,63.4230,26.8687,1.0053\n+cuda,ghostnet_100,128,1.7812,54.4211,47.6168,1.0484\n+cuda,gluon_inception_v3,128,1.5952,102.5018,50.0857,1.0469\n+cuda,gmixer_24_224,128,1.6749,69.2430,42.0841,1.1921\n+cuda,gmlp_s16_224,128,1.5886,79.2132,43.0142,1.2343\n+cuda,hrnet_w18,128,1.3743,221.5304,134.2573,1.0100\n+cuda,inception_v3,128,1.5847,102.8333,49.7648,1.0472\n+cuda,jx_nest_base,32,1.3747,71.4190,61.4053,0.9905\n+cuda,lcnet_050,128,1.8159,18.0047,18.8249,1.0005\n+cuda,mixer_b16_224,128,1.2795,90.9229,21.0438,1.0133\n+cuda,mixnet_l,128,1.2273,149.9722,47.7482,1.0129\n+cuda,mnasnet_100,128,1.6594,40.0512,26.5165,1.0047\n+cuda,mobilenetv2_100,128,1.6085,41.1217,27.4450,1.1731\n+cuda,mobilenetv3_large_100,128,1.6610,37.9995,29.8185,1.0052\n+cuda,mobilevit_s,64,1.5212,55.4152,53.6475,1.0258\n+cuda,nfnet_l0,128,1.4927,65.7078,32.4067,0.9980\n+cuda,pit_b_224,64,1.2286,57.9484,26.5321,0.9606\n+cuda,pnasnet5large,16,1.0000,198.2494,93.4641,1.3184\n+cuda,poolformer_m36,64,1.3486,103.9235,62.3196,1.1942\n+cuda,regnety_002,128,1.3030,32.4968,27.2439,1.0014\n+cuda,repvgg_a2,128,1.2485,59.7729,26.9209,1.0185\n+cuda,res2net101_26w_4s,64,1.0813,94.1773,86.6520,0.9655\n+cuda,res2net50_14w_8s,128,1.3251,109.5258,79.9578,0.9830\n+cuda,res2next50,128,1.2518,125.5008,43.9754,0.9756\n+cuda,resmlp_12_224,128,1.3060,45.2373,19.3709,1.1048\n+cuda,resnest101e,64,1.4346,108.1945,78.1993,1.1037\n+cuda,rexnet_100,128,1.4637,55.0121,41.2075,1.0862\n+cuda,selecsls42b,128,1.4284,44.6645,23.3892,1.0139\n+cuda,spnasnet_100,128,1.5908,45.3189,32.0148,1.0048\n+cuda,swin_base_patch4_window7_224,64,1.6164,89.5854,75.5848,0.9299\n+cuda,swsl_resnext101_32x16d,32,1.0175,110.0041,45.7853,1.0003\n+cuda,tf_efficientnet_b0,128,1.5271,55.7361,34.5551,1.1079\n+cuda,tf_mixnet_l,128,1.2369,155.9027,48.6695,1.0921\n+cuda,tinynet_a,128,1.3792,53.0640,40.6346,1.1108\n+cuda,tnt_s_patch16_224,128,3.1078,104.8486,59.6028,1.0660\n+cuda,twins_pcpvt_base,64,1.5921,67.4600,84.4977,1.0909\n+cuda,visformer_small,128,1.1952,72.8705,23.7303,1.0410\n+cuda,vit_base_patch16_224,64,1.1309,56.4866,22.0208,0.9804\n+cuda,volo_d1_224,64,1.6868,72.0957,65.3011,0.9729"}, {"filename": ".github/workflows/torchinductor/data/torchbench.csv", "status": "modified", "additions": 50, "deletions": 48, "changes": 98, "file_content_changes": "@@ -1,51 +1,53 @@\n dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio\n-cuda,BERT_pytorch,16,1.3446,65.5196,59.4176,1.1679\n-cuda,LearningToPaint,96,1.0376,12.7398,35.9770,0.7613\n-cuda,Super_SloMo,6,1.3132,73.6570,39.9019,1.2390\n-cuda,alexnet,128,1.1653,10.1614,10.5925,0.9408\n-cuda,attention_is_all_you_need_pytorch,256,1.2514,82.7756,66.7768,1.1459\n-cuda,dcgan,32,0.8947,2.6217,5.9728,1.0082\n-cuda,densenet121,4,0.8777,65.7564,123.8633,0.8292\n-cuda,drq,1,1.0291,4.9372,7.8125,0.9849\n-cuda,fastNLP_Bert,6,1.5073,73.2808,42.6538,1.1547\n-cuda,functorch_dp_cifar10,64,1.4043,9.3820,51.7668,0.4986\n-cuda,functorch_maml_omniglot,1,1.0998,3.0214,12.6407,0.2181\n-cuda,hf_Albert,8,1.3968,56.3755,38.3806,1.2603\n-cuda,hf_Bart,4,1.1020,91.5587,53.6117,1.0087\n-cuda,hf_Bert,4,1.1458,65.0555,43.0593,1.0261\n-cuda,hf_Bert_large,4,1.1683,131.1134,59.7228,1.0909\n-cuda,hf_DistilBert,8,1.2007,34.2069,22.8917,1.0228\n-cuda,hf_GPT2,4,1.2689,52.0180,31.7956,1.1540\n+cuda,BERT_pytorch,16,1.7111,24.2741,35.7065,1.3212\n+cuda,LearningToPaint,96,1.0513,10.7557,11.1879,0.9896\n+cuda,Super_SloMo,6,1.3267,60.4328,28.2097,1.2392\n+cuda,alexnet,128,1.1754,8.3246,5.3319,1.0003\n+cuda,attention_is_all_you_need_pytorch,256,1.3416,36.4401,39.5927,1.1774\n+cuda,dcgan,32,0.9151,2.6249,3.2964,1.0082\n+cuda,densenet121,4,0.9225,51.3747,68.5841,0.9930\n+cuda,doctr_det_predictor,0,0.0000\n+cuda,doctr_reco_predictor,0,0.0000\n+cuda,drq,1,0.9500,3.4884,4.8028,0.9687\n+cuda,fastNLP_Bert,6,1.4328,34.7753,35.4863,1.2368\n+cuda,functorch_dp_cifar10,64,1.2015,8.1625,12.9040,1.0609\n+cuda,functorch_maml_omniglot,1,0.9322,2.5844,3.8640,1.0000\n+cuda,hf_Albert,8,2.1228,30.3377,26.8282,1.2676\n+cuda,hf_Bart,4,1.2899,39.1935,47.2373,1.0080\n+cuda,hf_Bert,4,1.3262,26.1063,35.0281,1.0656\n+cuda,hf_Bert_large,4,1.4163,55.1021,67.2825,1.0915\n+cuda,hf_DistilBert,8,1.4051,21.7191,18.0399,1.0242\n+cuda,hf_GPT2,4,1.6661,26.9039,29.9473,1.1555\n cuda,hf_Longformer,0,0.0000\n-cuda,hf_Reformer,4,1.0903,82.9519,28.0343,0.9289\n-cuda,hf_T5_large,2,1.3534,332.3302,172.6140,1.1666\n-cuda,lennard_jones,1000,0.9952,3.8690,4.8521,1.0000\n-cuda,maml_omniglot,32,1.0328,3.3367,8.2772,0.2181\n-cuda,mnasnet1_0,32,1.0162,25.4638,69.8684,0.8356\n-cuda,mobilenet_v2,96,1.5212,38.4276,100.4918,1.1011\n-cuda,nvidia_deeprecommender,256,1.0517,11.1245,7.3804,0.9715\n-cuda,phlippe_densenet,128,1.0043,33.4096,108.0736,0.8774\n-cuda,phlippe_resnet,128,1.0229,14.0998,21.7420,0.4147\n-cuda,pytorch_CycleGAN_and_pix2pix,1,1.3815,9.3944,32.3602,0.6135\n-cuda,pytorch_stargan,16,1.1625,14.4103,41.3705,0.8893\n-cuda,pytorch_unet,1,1.3638,35.7120,51.2342,0.9525\n-cuda,resnet152,32,0.9568,76.3876,70.2073,0.9997\n-cuda,resnet18,16,0.9193,12.1360,23.4287,0.6492\n-cuda,resnet50,32,1.0230,29.6914,26.1574,1.0010\n-cuda,resnext50_32x4d,8,0.8679,25.7775,39.3170,0.8524\n-cuda,shufflenet_v2_x1_0,128,1.1374,31.2127,62.0057,0.9590\n-cuda,soft_actor_critic,256,0.9754,3.1737,5.5626,0.9998\n-cuda,speech_transformer,32,1.1390,94.3465,74.5561,0.8732\n-cuda,squeezenet1_1,32,1.1572,9.2585,19.0393,0.9243\n-cuda,timm_efficientdet,1,1.3338,95.3918,255.9148,1.0310\n-cuda,timm_efficientnet,32,1.1237,34.3466,80.1230,0.9445\n-cuda,timm_nfnet,128,1.4441,95.5148,36.3090,1.1050\n-cuda,timm_regnet,32,1.0374,65.3419,57.6930,0.9528\n-cuda,timm_resnest,32,1.5878,18.2585,54.0304,0.9636\n-cuda,timm_vision_transformer,8,1.0850,51.7360,50.3927,0.7429\n-cuda,timm_vision_transformer_large,0,0.0000\n-cuda,timm_vovnet,32,1.1318,27.3068,27.8668,0.8884\n+cuda,hf_Reformer,4,1.1709,64.6979,15.7035,0.9267\n+cuda,hf_T5_large,2,1.7215,107.0798,148.8805,1.1684\n+cuda,lennard_jones,1000,0.8428,1.8488,3.0609,1.0001\n+cuda,maml_omniglot,32,0.9648,2.6869,3.9775,0.9999\n+cuda,mnasnet1_0,32,1.0469,21.6251,25.8232,0.9996\n+cuda,mobilenet_v2,96,1.5604,31.9572,27.0225,1.1734\n+cuda,nvidia_deeprecommender,256,1.0605,9.2080,4.1318,0.9711\n+cuda,phlippe_densenet,128,1.0237,27.5988,28.0400,1.0023\n+cuda,phlippe_resnet,128,1.0493,10.9751,10.2485,1.0092\n+cuda,pytorch_CycleGAN_and_pix2pix,1,1.3724,8.2225,11.9561,1.0219\n+cuda,pytorch_stargan,16,1.1835,11.9178,10.0507,1.0868\n+cuda,pytorch_unet,1,1.3787,29.7543,13.7711,1.0100\n+cuda,resnet152,32,0.9834,63.2446,67.7935,0.9991\n+cuda,resnet18,16,0.9451,9.4977,11.7663,0.9948\n+cuda,resnet50,32,1.0513,24.5141,24.6629,1.0021\n+cuda,resnext50_32x4d,8,0.9216,22.2460,24.3420,0.9984\n+cuda,shufflenet_v2_x1_0,128,1.1943,25.4520,28.8611,1.0951\n+cuda,soft_actor_critic,256,0.8691,1.9637,3.3716,0.9996\n+cuda,speech_transformer,32,1.2718,35.2922,46.9957,1.0897\n+cuda,squeezenet1_1,32,1.1302,8.4540,7.9625,1.0771\n+cuda,timm_efficientdet,1,1.3370,80.0377,120.1814,1.2713\n+cuda,timm_efficientnet,32,1.1874,27.6302,33.9059,1.0971\n+cuda,timm_nfnet,128,1.4525,77.3461,34.3270,1.1056\n+cuda,timm_regnet,32,1.0644,50.6953,35.7562,1.0000\n+cuda,timm_resnest,32,1.6200,14.7763,17.2245,1.0906\n+cuda,timm_vision_transformer,32,1.0800,19.4188,22.0255,0.9966\n+cuda,timm_vision_transformer_large,32,1.0081,393.1742,127.8083,0.9735\n+cuda,timm_vovnet,32,1.1472,22.4727,22.7328,1.0120\n cuda,torchrec_dlrm,0,0.0000\n-cuda,tts_angular,64,0.8185,10.2896,5.1774,1.0015\n-cuda,vgg16,64,1.2931,61.1714,10.9558,0.9828\n-cuda,yolov3,16,1.2202,68.8346,86.5149,1.0437\n\\ No newline at end of file\n+cuda,tts_angular,64,0.8974,6.5057,2.5555,0.9973\n+cuda,vgg16,64,1.2909,50.7405,6.1510,0.9828\n+cuda,yolov3,16,1.2930,54.8069,41.9269,1.0563"}, {"filename": ".github/workflows/torchinductor/scripts/check_perf.py", "status": "modified", "additions": 25, "deletions": 12, "changes": 37, "file_content_changes": "@@ -4,47 +4,60 @@\n \n # Create a named tuple for the output of the benchmark\n BenchmarkOutput = namedtuple(\n-    'BenchmarkOutput', ['dev', 'name', 'batch_size', 'speedup'])\n+    'BenchmarkOutput', ['dev', 'name', 'batch_size', 'speedup', 'latency'])\n \n \n def parse_output(file_path: str) -> dict:\n     entries = {}\n     with open(file_path) as f:\n         reader = csv.reader(f)\n         for i, row in enumerate(reader):\n-            if i == 0:\n+            if i == 0 or len(row) < 5:\n                 continue\n             dev = row[0]\n             name = row[1]\n             batch_size = row[2]\n             speedup = float(row[3])\n-            entries[name] = BenchmarkOutput(dev, name, batch_size, speedup)\n+            latency = float(row[4])\n+            entries[name] = BenchmarkOutput(dev, name, batch_size, speedup, latency)\n     return entries\n \n \n-def compare(baseline: dict, new: dict, threshold: float) -> bool:\n+def compare(baseline: dict, new: dict, threshold: float, geomean_threshold: float) -> bool:\n+    baseline_geomean = 1.0\n+    new_geomean = 1.0\n     for key in new:\n         if key not in baseline:\n             print(f\"New benchmark {key} not found in baseline\")\n-        baseline_speedup = baseline[key].speedup\n-        new_speedup = new[key].speedup\n-        if new_speedup < baseline_speedup * (1 - threshold):\n+        baseline_latency = baseline[key].latency\n+        new_latency = new[key].latency\n+        if new_latency < baseline_latency * (1 - threshold):\n             print(\n-                f\"New benchmark {key} is slower than baseline: {new_speedup} vs {baseline_speedup}\")\n-        elif new_speedup > baseline_speedup * (1 + threshold):\n+                f\"New benchmark {key} is faster than baseline: {new_latency} vs {baseline_latency}\")\n+        elif new_latency > baseline_latency * (1 + threshold):\n             print(\n-                f\"New benchmark {key} is faster than baseline: {new_speedup} vs {baseline_speedup}\")\n+                f\"New benchmark {key} is slower than baseline: {new_latency} vs {baseline_latency}\")\n+        baseline_geomean *= baseline[key].speedup\n+        new_geomean *= new[key].speedup\n+\n+    baseline_geomean = baseline_geomean ** (1 / len(baseline))\n+    new_geomean = new_geomean ** (1 / len(new))\n+    print(f\"Baseline geomean: {baseline_geomean}\")\n+    print(f\"New geomean: {new_geomean}\")\n+    assert new_geomean > baseline_geomean * (1 - geomean_threshold), \\\n+        f\"New geomean is slower than baseline: {new_geomean} vs {baseline_geomean}\"\n \n \n def main():\n     parser = argparse.ArgumentParser()\n     parser.add_argument('--baseline', required=True)\n     parser.add_argument('--new', required=True)\n-    parser.add_argument('--threshold', type=float, default=0.02)\n+    parser.add_argument('--threshold', type=float, default=0.1)\n+    parser.add_argument('--geomean-threshold', type=float, default=0.02)\n     args = parser.parse_args()\n     baseline = parse_output(args.baseline)\n     new = parse_output(args.new)\n-    compare(baseline, new, args.threshold)\n+    compare(baseline, new, args.threshold, args.geomean_threshold)\n \n \n main()"}, {"filename": ".github/workflows/torchinductor/scripts/run_torchinductor_perf.sh", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -27,6 +27,11 @@ cd \"$ROOT\" || exit\n for model in \"${MODELS[@]}\"; do\n   echo \"Checking performance test for $model\"\n   python3 \"$INDUCTOR\"/scripts/check_perf.py --new \"$TEST_REPORTS_DIR\"/\"$model\".csv --baseline \"$INDUCTOR\"/data/\"$model\".csv\n+  EXIT_STATUS=$?\n+  if [ \"$EXIT_STATUS\" -ne 0 ]; then\n+    echo \"Performance test for $model failed\"\n+    exit \"$EXIT_STATUS\"\n+  fi\n done\n \n # unlock GPU clocks"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 21, "deletions": 21, "changes": 42, "file_content_changes": "@@ -174,27 +174,27 @@ inline bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n   // same\n   if (isSingleValue(op->getOperand(0)))\n     return false;\n-  auto ptr = op->getOperand(0);\n-  // Case 2: We assume that `evict_last` loads/stores have high hit rate\n-  if (auto load = dyn_cast<triton::LoadOp>(op))\n-    if (load.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n-      return false;\n-  if (auto store = dyn_cast<triton::StoreOp>(op))\n-    if (store.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n-      return false;\n-  if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n-    auto encoding = tensorTy.getEncoding();\n-    // Case 3: Different type conversion is expensive (e.g., mma <-> block)\n-    if (encoding.getTypeID() != targetEncoding.getTypeID())\n-      return true;\n-    auto sizePerThread = triton::gpu::getSizePerThread(encoding);\n-    auto targetSizePerThread = triton::gpu::getSizePerThread(targetEncoding);\n-    auto order = triton::gpu::getOrder(encoding);\n-    auto targetOrder = triton::gpu::getOrder(targetEncoding);\n-    // Case 4: The targeEncoding may expose more vectorization opportunities\n-    return sizePerThread[order[0]] >= targetSizePerThread[targetOrder[0]];\n-  }\n-  return false;\n+  // auto ptr = op->getOperand(0);\n+  //// Case 2: We assume that `evict_last` loads/stores have high hit rate\n+  // if (auto load = dyn_cast<triton::LoadOp>(op))\n+  //   if (load.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n+  //     return false;\n+  // if (auto store = dyn_cast<triton::StoreOp>(op))\n+  //   if (store.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n+  //     return false;\n+  // if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n+  //   auto encoding = tensorTy.getEncoding();\n+  //   // Case 3: Different type conversion is expensive (e.g., mma <-> block)\n+  //   if (encoding.getTypeID() != targetEncoding.getTypeID())\n+  //     return true;\n+  //   auto sizePerThread = triton::gpu::getSizePerThread(encoding);\n+  //   auto targetSizePerThread = triton::gpu::getSizePerThread(targetEncoding);\n+  //   auto order = triton::gpu::getOrder(encoding);\n+  //   auto targetOrder = triton::gpu::getOrder(targetEncoding);\n+  //   // Case 4: The targeEncoding may expose more vectorization opportunities\n+  //   return sizePerThread[order[0]] >= targetSizePerThread[targetOrder[0]];\n+  // }\n+  return true;\n }\n \n inline bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 0, "deletions": 43, "changes": 43, "file_content_changes": "@@ -55,49 +55,6 @@ func.func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   // CHECK: return %6 : tensor<1024xi32, [[$target_layout]]>\n }\n \n-// CHECK-LABEL: remat_load_store\n-func.func @remat_load_store(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n-  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout0>\n-  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout0>\n-  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout0>, tensor<64xi32, #layout0>\n-  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout0>\n-  // CHECK-NOT: triton_gpu.convert_layout\n-  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout0>) -> tensor<64xi32, #layout1>\n-  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout0>) -> tensor<64x!tt.ptr<i32>, #layout1>\n-  tt.store %5, %4 : tensor<64xi32, #layout1>\n-  return\n-}\n-\n-// Don't rematerialize vectorized loads\n-// CHECK-LABEL: remat_expensive\n-func.func @remat_expensive(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n-  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout1>\n-  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout1>\n-  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout1>, tensor<64xi32, #layout1>\n-  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout1>\n-  // CHECK: triton_gpu.convert_layout\n-  // CHECK-NOT: triton_gpu.convert_layout\n-  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout1>) -> tensor<64xi32, #layout0>\n-  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout1>) -> tensor<64x!tt.ptr<i32>, #layout0>\n-  tt.store %5, %4 : tensor<64xi32, #layout0>\n-  return\n-}\n-\n-// Don't rematerialize loads when original and target layouts are different\n-// CHECK-LABEL: remat_multi_layout\n-func.func @remat_multi_layout(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n-  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout0>\n-  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout0>\n-  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout0>, tensor<64xi32, #layout0>\n-  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout0>\n-  // CHECK: triton_gpu.convert_layout\n-  // CHECK-NOT: triton_gpu.convert_layout\n-  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout0>) -> tensor<64xi32, #layout2>\n-  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout0>) -> tensor<64x!tt.ptr<i32>, #layout2>\n-  tt.store %5, %4 : tensor<64xi32, #layout2>\n-  return\n-}\n-\n // Always rematerialize single value loads\n // CHECK-LABEL: remat_single_value\n func.func @remat_single_value(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {"}]