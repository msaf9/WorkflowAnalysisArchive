[{"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -986,9 +986,11 @@ void generator::visit_load_inst(ir::load_inst* x){\n         ir::value *false_val = mx->get_false_value_operand();\n         v = insert_elt(v, vals_[false_val][idxs[i + ii*size + s]], s);\n       }\n+      // PTX doesn't support mov.u8, so we need to use mov.u16\n       v = bit_cast(v, IntegerType::get(*ctx_, width));\n+      auto mov_width = width < 16 ? 16 : width;\n       asm_oss << \"\\n        \";\n-      asm_oss << \"@!$\" << n_words << \" mov.u\" << width;\n+      asm_oss << \"@!$\" << n_words << \" mov.u\" << mov_width;\n       asm_oss << \" $\" << ii << \", \";\n       std::ios_base::fmtflags flags(asm_oss.flags());\n       if(ConstantInt* cst = dyn_cast<ConstantInt>(v))"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 34, "deletions": 1, "changes": 35, "file_content_changes": "@@ -18,6 +18,7 @@\n float_dtypes = ['float16', 'float32', 'float64']\n dtypes = int_dtypes + uint_dtypes + float_dtypes\n dtypes_with_bfloat16 = dtypes + ['bfloat16']\n+torch_dtypes = ['bool'] + int_dtypes + ['uint8'] + float_dtypes + ['bfloat16']\n \n \n def _bitwidth(dtype: str) -> int:\n@@ -1188,10 +1189,42 @@ def _kernel(z, BLOCK: tl.constexpr,\n # ---------------\n # test load\n # ---------------\n+\n+\n+@pytest.mark.parametrize(\"dtype_str\", torch_dtypes)\n+def test_masked_load(dtype_str, device='cuda'):\n+    dtype = getattr(torch, dtype_str)\n+    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+\n+    input_size = 126\n+    output_size = 128\n+    if dtype_str == 'bool':\n+        input = torch.randint(0, 2, (input_size,), dtype=dtype, device=device)\n+    elif dtype_str in int_dtypes or dtype_str in uint_dtypes:\n+        input = torch.randint(0, 127, (input_size,), dtype=dtype, device=device)\n+    else:\n+        input = torch.rand(input_size, dtype=dtype, device=device)\n+    output = torch.zeros((output_size,), dtype=dtype, device=device)\n+\n+    @triton.jit\n+    def _kernel(in_ptr, out_ptr, in_size: tl.constexpr, out_size: tl.constexpr):\n+        in_offsets = tl.arange(0, out_size)\n+        # Load inputs.\n+        x = tl.load(in_ptr + in_offsets, mask=in_offsets < in_size, other=1.0)\n+        # Store output\n+        output_offsets = tl.arange(0, out_size)\n+        tl.store(out_ptr + output_offsets, x)\n+\n+    _kernel[(1,)](input, output, input_size, output_size)\n+\n+    reference_out = input\n+    reference_out = torch.cat((reference_out, torch.ones((2,), dtype=dtype, device=device)))\n+    triton.testing.allclose(output, reference_out)\n+\n+\n # 'bfloat16': torch.bfloat16,\n # Testing masked loads with an intermate copy to shared memory run.\n \n-\n @pytest.mark.parametrize(\"dtype\", [torch.bfloat16, torch.float16, torch.float32])\n def test_masked_load_shared_memory(dtype, device='cuda'):\n     check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested"}]