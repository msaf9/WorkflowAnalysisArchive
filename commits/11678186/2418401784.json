[{"filename": "include/triton/codegen/analysis/liveness.h", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "file_content_changes": "@@ -1,12 +1,14 @@\n #ifndef TDL_INCLUDE_IR_CODEGEN_LIVENESS_H\n #define TDL_INCLUDE_IR_CODEGEN_LIVENESS_H\n \n-#include <map>\n-#include <set>\n-#include <vector>\n #include \"triton/codegen/analysis/layout.h\"\n #include \"triton/tools/graph.h\"\n \n+#include \"llvm/ADT/MapVector.h\"\n+\n+#include <set>\n+#include <vector>\n+\n namespace triton{\n \n namespace ir{\n@@ -42,14 +44,14 @@ struct segment {\n \n class liveness {\n private:\n-  typedef std::map<shared_layout*, segment>    intervals_map_t;\n+  typedef llvm::MapVector<shared_layout*, segment>    intervals_map_t;\n \n public:\n   // constructor\n   liveness(layouts *l): layouts_(l){ }\n   // accessors\n   const intervals_map_t& get()  const { return intervals_; }\n-  segment get(shared_layout* v)  const { return intervals_.at(v); }\n+  segment get(shared_layout* v)  const { return intervals_.lookup(v); }\n   // run\n   void run(ir::module &mod);\n "}, {"filename": "include/triton/tools/graph.h", "status": "modified", "additions": 8, "deletions": 7, "changes": 15, "file_content_changes": "@@ -3,8 +3,9 @@\n #ifndef _TRITON_TOOLS_THREAD_GRAPH_H_\n #define _TRITON_TOOLS_THREAD_GRAPH_H_\n \n+#include \"llvm/ADT/SetVector.h\"\n+\n #include <map>\n-#include <set>\n #include <vector>\n #include <iostream>\n \n@@ -13,21 +14,21 @@ namespace tools{\n \n template<class node_t>\n class graph {\n-  typedef std::map<node_t, std::set<node_t>> edges_t;\n+  typedef std::map<node_t, llvm::SetVector<node_t>> edges_t;\n \n public:\n   typedef std::map<size_t, std::vector<node_t>> cmap_t;\n   typedef std::map<node_t, size_t> nmap_t;\n \n private:\n-  void connected_components_impl(node_t x, std::set<node_t> &nodes,\n+  void connected_components_impl(node_t x, llvm::SetVector<node_t> &nodes,\n                                  nmap_t* nmap, cmap_t* cmap, int id) const {\n     if(nmap)\n       (*nmap)[x] = id;\n     if(cmap)\n       (*cmap)[id].push_back(x);\n-    if(nodes.find(x) != nodes.end()) {\n-      nodes.erase(x);\n+    if (nodes.count(x)) {\n+      nodes.remove(x);\n       for(const node_t &y: edges_.at(x))\n         connected_components_impl(y, nodes, nmap, cmap, id);\n     }\n@@ -39,7 +40,7 @@ class graph {\n       cmap->clear();\n     if(nmap)\n       nmap->clear();\n-    std::set<node_t> nodes = nodes_;\n+    llvm::SetVector<node_t> nodes = nodes_;\n     unsigned id = 0;\n     while(!nodes.empty()){\n       connected_components_impl(*nodes.begin(), nodes, nmap, cmap, id++);\n@@ -59,7 +60,7 @@ class graph {\n   }\n \n private:\n-  std::set<node_t> nodes_;\n+  llvm::SetVector<node_t> nodes_;\n   edges_t edges_;\n };\n "}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -840,6 +840,7 @@ void init_triton_ir(py::module &&m) {\n       .def(\"create_fp_trunc\", &ir::builder::create_fp_trunc, ret::reference)\n       .def(\"create_int_cast\", &ir::builder::create_int_cast, ret::reference)\n       .def(\"create_downcast\", &ir::builder::create_downcast, ret::reference)\n+      .def(\"create_int_to_ptr\", &ir::builder::create_int_to_ptr, ret::reference)\n       // phi\n       .def(\"create_phi\", &ir::builder::create_phi, ret::reference)\n       // Binary instructions"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 33, "deletions": 1, "changes": 34, "file_content_changes": "@@ -514,9 +514,41 @@ def kernel(X, Z):\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n \n \n+def test_atomic_cas():\n+    # 1. make sure that atomic_cas changes the original value (Lock)\n+    @triton.jit\n+    def change_value(Lock):\n+        tl.atomic_cas(Lock, 0, 1)\n+\n+    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+    change_value[(1,)](Lock)\n+\n+    assert(Lock[0] == 1)\n+\n+    # 2. only one block enters the critical section\n+    @triton.jit\n+    def serialized_add(data, Lock):\n+        ptrs = data + tl.arange(0, 128)\n+        while tl.atomic_cas(Lock, 0, 1) == 1:\n+            pass\n+\n+        tl.store(ptrs, tl.load(ptrs) + 1.0)\n+\n+        # release lock\n+        tl.atomic_xchg(Lock, 0)\n+\n+    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+    data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n+    ref = torch.full((128,), 64.0)\n+    serialized_add[(64,)](data, Lock)\n+    triton.testing.assert_almost_equal(data, ref)\n+\n+\n # ---------------\n # test cast\n # ---------------\n+\n+\n @pytest.mark.parametrize(\"dtype_x, dtype_z, bitcast\", [\n     (dtype_x, dtype_z, False)\n     for dtype_x in dtypes\n@@ -879,7 +911,7 @@ def _kernel(z, BLOCK: tl.constexpr,\n def test_masked_load_shared_memory(dtype, device='cuda'):\n     M = 32\n     N = 32\n-    K = 8\n+    K = 16\n \n     in1 = torch.rand((M, K), dtype=dtype, device=device)\n     in2 = torch.rand((K, N), dtype=dtype, device=device)"}, {"filename": "python/triton/code_gen.py", "status": "modified", "additions": 65, "deletions": 27, "changes": 92, "file_content_changes": "@@ -11,6 +11,7 @@\n import sys\n import tempfile\n import textwrap\n+import threading\n import time\n import warnings\n from typing import Dict, Set, Tuple, Union\n@@ -22,12 +23,17 @@\n import triton._C.libtriton.triton as _triton\n from .tools.disasm import extract\n \n+try:\n+    from torch._C import _cuda_getCurrentRawStream as get_cuda_stream\n+except ImportError:\n+    get_cuda_stream = lambda dev_idx: torch.cuda.current_stream(dev_idx).cuda_stream\n+\n \n def current_cuda_stream(device_idx=0):\n     # Torch's torch.cuda.current_stream() is slow. We provide this\n     # function to give the user an opportunity to monkey-patch their\n     # own faster current stream lookup.\n-    return torch.cuda.current_stream().cuda_stream\n+    return get_cuda_stream(device_idx)\n \n \n def mangle_ty(ty):\n@@ -578,13 +584,17 @@ def visit_For(self, node):\n                     for stmt in node.orelse:\n                         ast.NodeVisitor.generic_visit(self, stmt)\n                 return\n+\n         # create nodes\n         st_target = ast.Name(id=node.target.id, ctx=ast.Store())\n         ld_target = ast.Name(id=node.target.id, ctx=ast.Load())\n         arg_0 = node.iter.args[0] if len(node.iter.args) > 1 else ast.Num(0)\n         arg_1 = node.iter.args[1] if len(node.iter.args) > 1 else node.iter.args[0]\n         arg_2 = node.iter.args[2] if len(node.iter.args) > 2 else ast.Num(1)\n+        # init node\n         init_node = ast.Assign(targets=[st_target], value=arg_0)\n+\n+        # step node\n         pos_cond_node = ast.Compare(ld_target, [ast.Lt()], [arg_1])\n         neg_cond_node = ast.Compare(ld_target, [ast.Gt()], [arg_1])\n         pos_step_node = ast.Compare(arg_2, [ast.Gt()], [ast.Num(0)])\n@@ -604,7 +614,17 @@ def continue_fn():\n             cond = build_cond()\n             return self.builder.cond_br(cond.handle, loop_bb, next_bb)\n \n+        # init loop induction variable\n         self.visit(init_node)\n+        # promote it to right type\n+        init_val = self.value_constructor.get_value(node.target.id)\n+        promote = lambda a, b: triton.language.semantic.computation_type_impl(a, b, False)\n+        start_ty = triton.language.core._to_tensor(iter_args[0], self.builder).type\n+        stop_ty = triton.language.core._to_tensor(iter_args[1], self.builder).type if len(iter_args) > 1 else None\n+        ty = promote(start_ty, stop_ty) if len(iter_args) > 1 else start_ty\n+        casted = triton.language.semantic.cast(init_val, ty, self.builder)\n+        self.value_constructor.set_value(node.target.id, casted)\n+        # create cond\n         cond = build_cond()\n         self.builder.cond_br(cond.handle, loop_bb, next_bb)\n         self.builder.set_insert_block(loop_bb)\n@@ -909,6 +929,7 @@ def pow2_divisor(N):\n \n     def __init__(self, fn):\n         self.fn = fn\n+        self.cache_key = {}\n \n     def add_to_cache(self, key, wargs, device_idx, num_warps, num_stages):\n         tensor_idxs = [i for i, arg in enumerate(wargs) if hasattr(arg, 'data_ptr')]\n@@ -950,12 +971,11 @@ def __call__(self, *wargs, grid, num_warps=4, num_stages=2, **kwargs):\n         #         assert arg.is_cuda, \"All tensors must be on GPU!\"\n         # set device (i.e., make sure torch has the context initialized)\n         device = torch.cuda.current_device()\n-        torch.cuda.set_device(device)\n-        # query compute capability\n-        cc = torch.cuda.get_device_capability(device)\n-        cc = str(cc[0]) + '-' + str(cc[1])\n-        cache_key = self.fn.cache_key + cc\n-        # query current stream\n+        if device not in self.cache_key:\n+            cc = torch.cuda.get_device_capability(device)\n+            cc = str(cc[0]) + '-' + str(cc[1])\n+            self.cache_key[device] = self.fn.cache_key + cc\n+        cache_key = self.cache_key[device]\n         stream = current_cuda_stream(device)\n         return _triton.runtime.launch(wargs, self.fn.do_not_specialize, cache_key, self.fn.arg_names,\n                                       device, stream, self.fn.bin_cache, num_warps, num_stages, self.add_to_cache,\n@@ -1058,27 +1078,40 @@ def __call__(self, *args, **kwargs):\n         return self.kernel(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n \n \n-@functools.lru_cache()\n+_version_key_lock = threading.Lock()\n+_version_key = None\n+\n+\n def version_key():\n-    import pkgutil\n-    contents = []\n-    # frontend\n-    with open(triton.code_gen.__file__, \"rb\") as f:\n-        contents += [hashlib.md5(f.read()).hexdigest()]\n-    # backend\n-    with open(triton._C.libtriton.__file__, \"rb\") as f:\n-        contents += [hashlib.md5(f.read()).hexdigest()]\n-    # language\n-    language_path = os.path.join(*triton.__path__, 'language')\n-    for lib in pkgutil.iter_modules([language_path]):\n-        with open(lib.module_finder.find_spec(lib.name).origin, \"rb\") as f:\n+    global _version_key\n+\n+    if _version_key is not None:\n+        return _version_key\n+\n+    with _version_key_lock:\n+        if _version_key is not None:\n+            return _version_key\n+\n+        import pkgutil\n+        contents = []\n+        # frontend\n+        with open(triton.code_gen.__file__, \"rb\") as f:\n+            contents += [hashlib.md5(f.read()).hexdigest()]\n+        # backend\n+        with open(triton._C.libtriton.__file__, \"rb\") as f:\n             contents += [hashlib.md5(f.read()).hexdigest()]\n-    # ptxas version\n-    try:\n-        ptxas_version = hashlib.md5(subprocess.check_output([\"ptxas\", \"--version\"])).hexdigest()\n-    except Exception:\n-        ptxas_version = ''\n-    return '-'.join(triton.__version__) + '-' + ptxas_version + '-' + '-'.join(contents)\n+        # language\n+        language_path = os.path.join(*triton.__path__, 'language')\n+        for lib in pkgutil.iter_modules([language_path]):\n+            with open(lib.module_finder.find_spec(lib.name).origin, \"rb\") as f:\n+                contents += [hashlib.md5(f.read()).hexdigest()]\n+        # ptxas version\n+        try:\n+            ptxas_version = hashlib.md5(subprocess.check_output([\"ptxas\", \"--version\"])).hexdigest()\n+        except Exception:\n+            ptxas_version = ''\n+        _version_key = '-'.join(triton.__version__) + '-' + ptxas_version + '-' + '-'.join(contents)\n+        return _version_key\n \n \n class DependenciesFinder(ast.NodeVisitor):\n@@ -1117,6 +1150,11 @@ def visit_Call(self, node):\n         self.ret = hashlib.md5(self.ret).hexdigest()\n \n \n+def default_cache_dir():\n+    import getpass\n+    return f'/tmp/triton_{getpass.getuser()}'\n+\n+\n class JITFunction:\n \n     cache_hook = None\n@@ -1202,7 +1240,7 @@ def _warmup(self, key, arg_types, device, attributes, constants, num_warps, num_\n         hashed_key = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n \n         # create cache directory\n-        cache_dir = os.environ.get('TRITON_CACHE_DIR', '/tmp/triton/')\n+        cache_dir = os.environ.get('TRITON_CACHE_DIR', default_cache_dir())\n         if cache_dir:\n             os.makedirs(cache_dir, exist_ok=True)\n "}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 21, "deletions": 1, "changes": 22, "file_content_changes": "@@ -344,6 +344,26 @@ def __repr__(self) -> str:\n     def __bool__(self):\n         return bool(self.value)\n \n+    def __ge__(self, other):\n+        other = other.value if isinstance(other, constexpr) else other\n+        return self.value >= other\n+\n+    def __gt__(self, other):\n+        other = other.value if isinstance(other, constexpr) else other\n+        return self.value > other\n+\n+    def __le__(self, other):\n+        other = other.value if isinstance(other, constexpr) else other\n+        return self.value <= other\n+\n+    def __lt__(self, other):\n+        other = other.value if isinstance(other, constexpr) else other\n+        return self.value < other\n+\n+    def __eq__(self, other):\n+        other = other.value if isinstance(other, constexpr) else other\n+        return self.value == other\n+\n     def __call__(self, *args, **kwds):\n         return self.value(*args, **kwds)\n \n@@ -808,7 +828,7 @@ def _decorator(func):\n @_add_atomic_docstr(\"compare-and-swap\")\n def atomic_cas(pointer, cmp, val, _builder=None):\n     cmp = _to_tensor(cmp, _builder)\n-    val = _to_tensor(cmp, _builder)\n+    val = _to_tensor(val, _builder)\n     return semantic.atomic_cas(pointer, cmp, val, _builder)\n \n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -905,6 +905,10 @@ def dot(lhs: tl.tensor,\n         allow_tf32: bool,\n         builder: ir.builder) -> tl.tensor:\n     assert lhs.type.is_block() and rhs.type.is_block()\n+    assert len(lhs.shape) == 2 and len(rhs.shape) == 2\n+    assert lhs.shape[-1] == rhs.shape[0]\n+    assert lhs.shape[0] >= 16 and lhs.shape[1] >= 16 and rhs.shape[1] >= 16,\\\n+        \"small blocks not supported!\"\n     if lhs.type.scalar.is_int():\n         _0 = builder.get_int32(0)\n         ret_scalar_ty = tl.int32"}, {"filename": "python/triton/tools/compare_asm.py", "status": "added", "additions": 76, "deletions": 0, "changes": 76, "file_content_changes": "@@ -0,0 +1,76 @@\n+'''\n+Compare cached triton kernels in 2 directories.\n+\n+example:\n+python compare_asm.py --dir0=triton-works/ --dir1=triton-fails/ --asm=ttir \\\n+       --diff-out0=diff-works.ll --diff-out1=diff-fails.ll\n+'''\n+import argparse\n+import os\n+import pickle\n+\n+parser = argparse.ArgumentParser(description=\"unpickle\")\n+parser.add_argument('--dir0', dest='dir0', required=True,\n+                    help=\"Triton cache dir 0\")\n+parser.add_argument('--dir1', dest='dir1', required=True,\n+                    help=\"Triton cache dir 1\")\n+parser.add_argument('--asm', dest='asm',\n+                    choices=['ttir', 'llir', 'ptx', 'cubin'], required=True)\n+parser.add_argument('--early-stop', dest='early_stop', action='store_true',\n+                    help=\"Stop after first diff\")\n+parser.set_defaults(early_stop=True)\n+parser.add_argument('--diff-out0', dest='diff_out0', required=True,\n+                    help=\"output file path for kernels in dir0\")\n+parser.add_argument('--diff-out1', dest='diff_out1', required=True,\n+                    help=\"output file path for kernels in dir1\")\n+args = parser.parse_args()\n+dir0 = args.dir0\n+dir1 = args.dir1\n+asm = args.asm\n+\n+dir0_files = {}\n+dir1_files = {}\n+for root, _, files in os.walk(dir0):\n+    for file in files:\n+        if not file.endswith('.lock'):\n+            path = os.path.join(root, file)\n+            with open(path, 'rb') as f:\n+                loaded_file = pickle.load(f)\n+                bin = loaded_file['binary']\n+                key = loaded_file['key']\n+                info = key.split('-')[-3:]  # num_warps, num_stages, signature\n+                dict_key = bin.name + '-'.join(info)\n+                dir0_files[dict_key] = bin.asm\n+\n+for root, _, files in os.walk(dir1):\n+    for file in files:\n+        if not file.endswith('.lock'):\n+            path = os.path.join(root, file)\n+            with open(path, 'rb') as f:\n+                loaded_file = pickle.load(f)\n+                bin = loaded_file['binary']\n+                key = loaded_file['key']\n+                info = key.split('-')[-3:]  # num_warps, num_stages, signature\n+                dict_key = bin.name + '-'.join(info)\n+                dir1_files[dict_key] = bin.asm\n+\n+diff_keys = []\n+for key in dir0_files:\n+    asm0 = dir0_files[key]\n+    if key not in dir1_files:\n+        continue\n+    asm1 = dir1_files[key]\n+    if asm0[asm] != asm1[asm]:\n+        diff_keys.append(key)\n+\n+if args.early_stops:\n+    diff_keys = diff_keys[:1]\n+if diff_keys:\n+    with open(args.diff_out0, 'w') as f0, open(args.diff_out1, 'w') as f1:\n+        for key in diff_keys:\n+            f0.write(f'{asm} mismatch at {key}')\n+            f0.write(dir0_files[key][asm])\n+            f0.write('\\n')\n+            f1.write(f'{asm} mismatch at {key}')\n+            f1.write(dir1_files[key][asm])\n+            f1.write('\\n')"}]