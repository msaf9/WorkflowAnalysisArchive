[{"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -19,8 +19,6 @@ class SharedEncodingAttr;\n }\n } // namespace triton\n \n-LogicalResult fixupLoops(ModuleOp mod);\n-\n SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n                                                 const ArrayRef<int64_t> &shape,\n                                                 RankedTensorType type);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -248,8 +248,6 @@ class TritonGPUOptimizeDotOperandsPass\n     patterns.add<FuseTransHopper>(context);\n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n       signalPassFailure();\n-    if (fixupLoops(m).failed())\n-      signalPassFailure();\n   }\n };\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeEpilogue.cpp", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -127,9 +127,6 @@ class TritonGPUOptimizeEpiloguePass\n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n       signalPassFailure();\n     }\n-    if (fixupLoops(m).failed()) {\n-      signalPassFailure();\n-    }\n   }\n };\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 12, "deletions": 30, "changes": 42, "file_content_changes": "@@ -262,12 +262,12 @@ class MoveConvertOutOfIf : public mlir::RewritePattern {\n \n     IRMapping mapping;\n     for (size_t i = 0; i < numOps; i++) {\n-      auto thenCvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n-          thenYield.getOperand(i).getDefiningOp());\n+      auto thenCvt =\n+          thenYield.getOperand(i).getDefiningOp<triton::gpu::ConvertLayoutOp>();\n       if (hasElse) {\n         auto elseYield = ifOp.elseYield();\n-        auto elseCvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n-            elseYield.getOperand(i).getDefiningOp());\n+        auto elseCvt = elseYield.getOperand(i)\n+                           .getDefiningOp<triton::gpu::ConvertLayoutOp>();\n         if (thenCvt && elseCvt &&\n             std::distance(elseCvt->user_begin(), elseCvt->user_end()) == 1 &&\n             std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1 &&\n@@ -585,40 +585,26 @@ class ConvertDotConvert : public mlir::RewritePattern {\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n     auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto dotOp =\n-        dyn_cast_or_null<triton::DotOp>(dstOp.getSrc().getDefiningOp());\n+    auto dotOp = dstOp.getSrc().getDefiningOp<triton::DotOp>();\n     if (!dotOp)\n       return mlir::failure();\n     if (std::distance(dstOp->user_begin(), dstOp->user_end()) != 1 ||\n         std::distance(dotOp->user_begin(), dotOp->user_end()) != 1)\n       return mlir::failure();\n-    auto cvtOp = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n-        dotOp.getOperand(2).getDefiningOp());\n+    auto cvtOp =\n+        dotOp.getOperand(2).getDefiningOp<triton::gpu::ConvertLayoutOp>();\n     if (!cvtOp)\n       return mlir::failure();\n-    auto loadOp =\n-        dyn_cast_or_null<triton::LoadOp>(cvtOp.getSrc().getDefiningOp());\n-    if (!loadOp)\n-      return mlir::failure();\n+    if (!cvtOp.getSrc().getDefiningOp<triton::LoadOp>())\n+      return failure();\n     auto dstTy = dstOp.getResult().getType().cast<RankedTensorType>();\n     auto srcTy = cvtOp.getOperand().getType().cast<RankedTensorType>();\n     if (dstTy != srcTy)\n       return mlir::failure();\n \n-    // TODO: int tensor cores\n-    auto out_dtype = dstTy.getElementType().cast<FloatType>();\n-    APFloat value(0.0f);\n-    if (out_dtype.isBF16())\n-      value = APFloat(APFloat::IEEEhalf(), APInt(16, 0));\n-    else if (out_dtype.isF16())\n-      value = APFloat(APFloat::IEEEhalf(), APInt(16, 0));\n-    else if (out_dtype.isF32())\n-      value = APFloat(0.0f);\n-    else\n-      llvm_unreachable(\"unsupported data type\");\n-\n-    auto _0f =\n-        rewriter.create<arith::ConstantFloatOp>(op->getLoc(), value, out_dtype);\n+    auto _0f = rewriter.create<arith::ConstantOp>(\n+        op->getLoc(), dstTy.getElementType(),\n+        rewriter.getZeroAttr(dstTy.getElementType()));\n     auto _0 = rewriter.create<triton::SplatOp>(\n         op->getLoc(), dotOp.getResult().getType(), _0f);\n     auto newDot = rewriter.create<triton::DotOp>(\n@@ -660,10 +646,6 @@ class TritonGPURemoveLayoutConversionsPass\n     if (mlir::applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n       signalPassFailure();\n     }\n-\n-    if (fixupLoops(m).failed()) {\n-      signalPassFailure();\n-    }\n   }\n };\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 0, "deletions": 55, "changes": 55, "file_content_changes": "@@ -9,61 +9,6 @@\n \n namespace mlir {\n \n-namespace {\n-\n-class FixupLoop : public mlir::RewritePattern {\n-\n-public:\n-  explicit FixupLoop(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(scf::ForOp::getOperationName(), 2, context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto forOp = cast<scf::ForOp>(op);\n-\n-    // Rewrite init argument\n-    SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n-    bool shouldRematerialize = false;\n-    for (size_t i = 0; i < newInitArgs.size(); i++) {\n-      if (newInitArgs[i].getType() != forOp.getRegionIterArgs()[i].getType() ||\n-          newInitArgs[i].getType() != forOp.getResultTypes()[i]) {\n-        shouldRematerialize = true;\n-        break;\n-      }\n-    }\n-    if (!shouldRematerialize)\n-      return failure();\n-\n-    scf::ForOp newForOp = rewriter.create<scf::ForOp>(\n-        forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n-        forOp.getStep(), newInitArgs);\n-    newForOp->moveBefore(forOp);\n-    rewriter.setInsertionPointToStart(newForOp.getBody());\n-    IRMapping mapping;\n-    for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n-      mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n-    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n-\n-    for (Operation &op : forOp.getBody()->getOperations()) {\n-      rewriter.clone(op, mapping);\n-    }\n-    rewriter.replaceOp(forOp, newForOp.getResults());\n-    return success();\n-  }\n-};\n-\n-} // namespace\n-\n-LogicalResult fixupLoops(ModuleOp mod) {\n-  auto *ctx = mod.getContext();\n-  mlir::RewritePatternSet patterns(ctx);\n-  patterns.add<FixupLoop>(ctx);\n-  if (applyPatternsAndFoldGreedily(mod, std::move(patterns)).failed())\n-    return failure();\n-  return success();\n-}\n-\n SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n                                                 const ArrayRef<int64_t> &shape,\n                                                 RankedTensorType type) {"}]