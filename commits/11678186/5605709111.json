[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 8, "deletions": 19, "changes": 27, "file_content_changes": "@@ -1297,7 +1297,7 @@ def deserialize_fp8(np_data, in_dtype):\n \n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n-@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16, torch.float32])\n+@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.float32])\n def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     \"\"\"\n     For all possible float8 values (ref_fp8 = range(0, 256)), test that:\n@@ -1306,13 +1306,6 @@ def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     this is only possible if both conversions are correct\n     \"\"\"\n     check_type_supported(out_dtype, device)\n-    from contextlib import nullcontext as does_not_raise\n-    expectation = does_not_raise()\n-    err_msg = None\n-    if (in_dtype == tl.float8e4b15 and out_dtype != torch.float16) or\\\n-       (in_dtype != torch.float16 and out_dtype == tl.float8e4b15):\n-        expectation = pytest.raises(triton.CompilationError)\n-        err_msg = \"fp8e4b15 can only be converted to/from fp16\"\n \n     @triton.jit\n     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n@@ -1331,19 +1324,15 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     ref_fp8[is_subnormal] = 0\n     tri_fp8 = torch.from_numpy(serialize_fp8(ref_fp8, in_dtype)).cuda()\n     tri_fp16 = torch.empty(256, dtype=out_dtype, device=\"cuda\")\n-    with expectation as e:\n-        copy_kernel[(1,)](triton.reinterpret(tri_fp8, in_dtype), tri_fp16, tri_fp16.shape[0], BLOCK_SIZE=1024)\n+    copy_kernel[(1,)](triton.reinterpret(tri_fp8, in_dtype), tri_fp16, tri_fp16.shape[0], BLOCK_SIZE=1024)\n \n-        ref_fp8 = torch.from_numpy(ref_fp8).cuda()\n-        ref_fp16 = convert_float_to_float32(ref_fp8, in_dtype)\n-        assert torch.all(tri_fp16[~is_subnormal] == ref_fp16[~is_subnormal])\n+    ref_fp8 = torch.from_numpy(ref_fp8).cuda()\n+    ref_fp16 = convert_float_to_float32(ref_fp8, in_dtype)\n+    assert torch.all(tri_fp16[~is_subnormal] == ref_fp16[~is_subnormal])\n \n-        ref_fp8 = torch.empty_like(tri_fp16, dtype=torch.int8)\n-        copy_kernel[(1,)](tri_fp16, triton.reinterpret(ref_fp8, in_dtype), tri_fp16.shape[0], BLOCK_SIZE=1024)\n-        assert torch.all(tri_fp8 == ref_fp8)\n-\n-    if err_msg is not None:\n-        assert err_msg in str(e)\n+    ref_fp8 = torch.empty_like(tri_fp16, dtype=torch.int8)\n+    copy_kernel[(1,)](tri_fp16, triton.reinterpret(ref_fp8, in_dtype), tri_fp16.shape[0], BLOCK_SIZE=1024)\n+    assert torch.all(tri_fp8 == ref_fp8)\n \n # ---------------\n # test reduce"}]