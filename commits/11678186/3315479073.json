[{"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -83,8 +83,8 @@ class SimplifyConversion : public mlir::RewritePattern {\n     auto dstType = convert.getType().cast<RankedTensorType>();\n     // we don't handle conversions to DotOperandEncodingAttr\n     // this is a heuristics to accomodate fused attention\n-    if (dstType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n-      return mlir::failure();\n+    // if (dstType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n+    //   return mlir::failure();\n     // convert to the same layout -- we can delete\n     if (op->getResultTypes() == op->getOperandTypes()) {\n       rewriter.replaceOp(op, op->getOperands());\n@@ -550,8 +550,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n     a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), newAType, a);\n     b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), newBType, b);\n     auto newDot = rewriter.create<triton::DotOp>(\n-        dotOp.getLoc(), newRetType, dotOp.getOperand(0), dotOp.getOperand(1),\n-        newAcc, dotOp.allowTF32(), dotOp.transA(), dotOp.transB());\n+        dotOp.getLoc(), newRetType, a, b, newAcc, dotOp.allowTF32(),\n+        dotOp.transA(), dotOp.transB());\n \n     rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(\n         op, oldRetType, newDot.getResult());\n@@ -574,7 +574,7 @@ class TritonGPUCombineOpsPass\n     mlir::RewritePatternSet patterns(context);\n \n     patterns.add<SimplifyConversion>(context);\n-    patterns.add<DecomposeDotOperand>(context);\n+    // patterns.add<DecomposeDotOperand>(context);\n     patterns.add<RematerializeBackward>(context);\n     patterns.add<RematerializeForward>(context);\n     patterns.add<MoveConvertOutOfLoop>(context);"}]