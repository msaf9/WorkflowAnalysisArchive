[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 4, "deletions": 6, "changes": 10, "file_content_changes": "@@ -69,9 +69,11 @@ bool supportMMA(triton::DotOp op, int version);\n \n bool supportMMA(Value value, int version);\n \n-Type getElementType(Value value);\n+bool isSingleValue(Value value);\n \n-std::string getValueOperandName(Value value, AsmState &state);\n+bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy);\n+\n+Type getElementType(Value value);\n \n template <typename T_OUT, typename T_IN>\n inline SmallVector<T_OUT> convertType(ArrayRef<T_IN> in) {\n@@ -120,10 +122,6 @@ template <typename T> T nextPowOf2(T n) {\n   return n + 1;\n }\n \n-bool isSingleValue(Value value);\n-\n-bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy);\n-\n /// Multi-root DAG topological sort.\n /// Performs a topological sort of the Operation in the `toSort` SetVector.\n /// Returns a topologically sorted SetVector."}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "file_content_changes": "@@ -73,15 +73,13 @@ getShapePerCTA(Attribute layout,\n \n SmallVector<unsigned> getOrder(Attribute layout);\n \n-bool isaDistributedLayout(Attribute layout);\n+bool isDistributedLayout(Attribute layout);\n \n-bool expensiveCat(triton::CatOp cat, Attribute &targetEncoding);\n+bool isSharedEncoding(Value value);\n \n } // namespace gpu\n } // namespace triton\n \n-bool isSharedEncoding(Value value);\n-\n } // namespace mlir\n \n #endif // TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "renamed", "additions": 11, "deletions": 5, "changes": 16, "file_content_changes": "@@ -1,9 +1,13 @@\n-#ifndef TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n-#define TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#ifndef TRITON_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#define TRITON_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"llvm/ADT/MapVector.h\"\n \n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+\n namespace mlir {\n \n LogicalResult fixupLoops(ModuleOp mod);\n@@ -12,9 +16,11 @@ LogicalResult fixupLoops(ModuleOp mod);\n LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n                              Attribute &ret);\n \n-bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n+bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n+\n+bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding);\n \n-bool expensiveToRemat(Operation *op, Attribute &targetEncoding);\n+bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding);\n \n // skipInit is True when we only consider the operands of the initOp but\n // not the initOp itself.\n@@ -36,4 +42,4 @@ LogicalResult canMoveOutOfLoop(BlockArgument arg,\n \n } // namespace mlir\n \n-#endif // TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#endif // TRITON_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "include/triton/Target/AMDGCN/AMDGCNTranslation.h", "status": "removed", "additions": 0, "deletions": 19, "changes": 19, "file_content_changes": "@@ -1,19 +0,0 @@\n-#ifndef TRITON_TARGET_AMDGCNTRANSLATION_H\n-#define TRITON_TARGET_AMDGCNTRANSLATION_H\n-\n-#include <string>\n-#include <tuple>\n-\n-namespace llvm {\n-class Module;\n-} // namespace llvm\n-\n-namespace triton {\n-\n-// Translate LLVM IR to AMDGCN code.\n-std::tuple<std::string, std::string>\n-translateLLVMIRToAMDGCN(llvm::Module &module, std::string cc);\n-\n-} // namespace triton\n-\n-#endif"}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -38,7 +38,7 @@ void SharedMemoryAliasAnalysis::visitOperation(\n       // insert_slice %src into %dst[%offsets]\n       aliasInfo = AliasInfo(operands[1]->getValue());\n       pessimistic = false;\n-    } else if (isSharedEncoding(result)) {\n+    } else if (triton::gpu::isSharedEncoding(result)) {\n       aliasInfo.insert(result);\n       pessimistic = false;\n     }"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -151,7 +151,7 @@ class AllocationAnalysis {\n     }\n \n     for (Value result : op->getResults()) {\n-      if (isSharedEncoding(result)) {\n+      if (triton::gpu::isSharedEncoding(result)) {\n         // Bytes could be a different value once we support padding or other\n         // allocation policies.\n         auto tensorType = result.getType().dyn_cast<RankedTensorType>();"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 0, "deletions": 7, "changes": 7, "file_content_changes": "@@ -163,13 +163,6 @@ Type getElementType(Value value) {\n   return type;\n }\n \n-std::string getValueOperandName(Value value, AsmState &state) {\n-  std::string opName;\n-  llvm::raw_string_ostream ss(opName);\n-  value.printAsOperand(ss, state);\n-  return opName;\n-}\n-\n bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy) {\n   // dot_op<opIdx=0, parent=#mma> = #mma\n   // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 4, "deletions": 6, "changes": 10, "file_content_changes": "@@ -9,7 +9,7 @@ using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n-using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::isDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n // Forward declarations\n@@ -62,23 +62,21 @@ struct ConvertLayoutOpConversion\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n     Attribute dstLayout = dstTy.getEncoding();\n-    if (isaDistributedLayout(srcLayout) &&\n-        dstLayout.isa<SharedEncodingAttr>()) {\n+    if (isDistributedLayout(srcLayout) && dstLayout.isa<SharedEncodingAttr>()) {\n       return lowerDistributedToShared(op, adaptor, rewriter);\n     }\n     if (srcLayout.isa<SharedEncodingAttr>() &&\n         dstLayout.isa<DotOperandEncodingAttr>()) {\n       return lowerSharedToDotOperand(op, adaptor, rewriter);\n     }\n-    if (isaDistributedLayout(srcLayout) && isaDistributedLayout(dstLayout)) {\n+    if (isDistributedLayout(srcLayout) && isDistributedLayout(dstLayout)) {\n       return lowerDistributedToDistributed(op, adaptor, rewriter);\n     }\n     if (srcLayout.isa<MmaEncodingAttr>() &&\n         dstLayout.isa<DotOperandEncodingAttr>()) {\n       return lowerMmaToDotOperand(op, adaptor, rewriter);\n     }\n-    if (srcLayout.isa<SharedEncodingAttr>() &&\n-        isaDistributedLayout(dstLayout)) {\n+    if (srcLayout.isa<SharedEncodingAttr>() && isDistributedLayout(dstLayout)) {\n       return lowerSharedToDistributed(op, adaptor, rewriter);\n     }\n     // TODO: to be implemented"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -10,7 +10,7 @@ using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n-using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::isDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n SmallVector<Value>"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -11,7 +11,7 @@ using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n-using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::isDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n // Compute the offset of the matrix to load."}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -12,7 +12,7 @@ using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n-using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::isDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n // Data loader for mma.16816 instruction."}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 14, "deletions": 25, "changes": 39, "file_content_changes": "@@ -7,6 +7,7 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.cpp.inc\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"llvm/ADT/TypeSwitch.h\"\n \n using namespace mlir;\n@@ -349,27 +350,11 @@ SmallVector<unsigned> getOrder(Attribute layout) {\n   }\n }\n \n-bool isaDistributedLayout(Attribute layout) {\n+bool isDistributedLayout(Attribute layout) {\n   return layout.isa<BlockedEncodingAttr>() || layout.isa<MmaEncodingAttr>() ||\n          layout.isa<SliceEncodingAttr>();\n }\n \n-bool expensiveCat(triton::CatOp cat, Attribute &targetEncoding) {\n-  // If the new elements per thread is less than the old one, we will need to do\n-  // convert encoding that goes through shared memory anyway. So we consider it\n-  // as expensive.\n-  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n-  auto totalElemsPerThread = triton::gpu::getTotalElemsPerThread(tensorTy);\n-  auto shape = tensorTy.getShape();\n-  auto elemTy = tensorTy.getElementType();\n-  auto newTotalElemsPerThread =\n-      triton::gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n-  return newTotalElemsPerThread < totalElemsPerThread;\n-}\n-\n-} // namespace gpu\n-} // namespace triton\n-\n bool isSharedEncoding(Value value) {\n   auto type = value.getType();\n   if (auto tensorType = type.dyn_cast<RankedTensorType>()) {\n@@ -379,6 +364,9 @@ bool isSharedEncoding(Value value) {\n   return false;\n }\n \n+} // namespace gpu\n+} // namespace triton\n+\n } // namespace mlir\n \n static LogicalResult parseIntAttrValue(AsmParser &parser, Attribute attr,\n@@ -1142,7 +1130,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   if (auto cat = dyn_cast<triton::CatOp>(arg)) {\n     auto encoding =\n         op->getResult(0).getType().cast<RankedTensorType>().getEncoding();\n-    if (triton::gpu::expensiveCat(cat, encoding))\n+    if (isExpensiveCat(cat, encoding))\n       return mlir::failure();\n     rewriter.replaceOpWithNewOp<triton::CatOp>(op, op->getResult(0).getType(),\n                                                cat.getOperands());\n@@ -1151,7 +1139,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   // cvt(alloc_tensor(x), type2) -> alloc_tensor(x, type2)\n   auto alloc_tensor = dyn_cast<triton::gpu::AllocTensorOp>(arg);\n   if (alloc_tensor) {\n-    if (!isSharedEncoding(op->getResult(0))) {\n+    if (!triton::gpu::isSharedEncoding(op->getResult(0))) {\n       return mlir::failure();\n     }\n     rewriter.replaceOpWithNewOp<triton::gpu::AllocTensorOp>(\n@@ -1161,7 +1149,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   // cvt(insert_slice(x), type2) -> insert_slice(cvt(x, type2))\n   auto insert_slice = dyn_cast<triton::gpu::InsertSliceAsyncOp>(arg);\n   if (insert_slice) {\n-    if (!isSharedEncoding(op->getResult(0))) {\n+    if (!triton::gpu::isSharedEncoding(op->getResult(0))) {\n       return mlir::failure();\n     }\n     auto newType = op->getResult(0).getType().cast<RankedTensorType>();\n@@ -1183,7 +1171,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   // cvt(extract_slice(x), type2) -> extract_slice(cvt(x, type2))\n   auto extract_slice = dyn_cast<triton::gpu::ExtractSliceOp>(arg);\n   if (extract_slice) {\n-    if (!isSharedEncoding(op->getResult(0))) {\n+    if (!triton::gpu::isSharedEncoding(op->getResult(0))) {\n       return mlir::failure();\n     }\n     auto origType =\n@@ -1213,12 +1201,13 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   // cvt(cvt(x, type1), type2) -> cvt(x, type2)\n   if (llvm::isa<triton::gpu::ConvertLayoutOp>(arg)) {\n     if (arg->getOperand(0).getDefiningOp() &&\n-        !isSharedEncoding(arg->getOperand(0)) &&\n-        isSharedEncoding(op.getOperand()) &&\n-        !isSharedEncoding(op.getResult())) {\n+        !triton::gpu::isSharedEncoding(arg->getOperand(0)) &&\n+        triton::gpu::isSharedEncoding(op.getOperand()) &&\n+        !triton::gpu::isSharedEncoding(op.getResult())) {\n       return mlir::failure();\n     }\n-    if (isSharedEncoding(op.getOperand()) && isSharedEncoding(op.getResult())) {\n+    if (triton::gpu::isSharedEncoding(op.getOperand()) &&\n+        triton::gpu::isSharedEncoding(op.getResult())) {\n       return mlir::failure();\n     }\n     auto srcType = op.getOperand().getType().cast<RankedTensorType>();"}, {"filename": "lib/Dialect/TritonGPU/IR/Traits.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -7,7 +7,7 @@ mlir::OpTrait::impl::verifyResultsAreSharedEncoding(Operation *op) {\n     return failure();\n \n   for (auto result : op->getResults())\n-    if (!isSharedEncoding(result))\n+    if (!triton::gpu::isSharedEncoding(result))\n       return op->emitOpError() << \"requires all results to be shared encoding\";\n \n   return success();"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,11 +1,11 @@\n-#include \"Utility.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Support/LogicalResult.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"mlir/Transforms/Passes.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include <memory>\n \n using namespace mlir;"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,3 @@\n-#include \"Utility.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/IRMapping.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n@@ -8,6 +7,7 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"llvm/ADT/MapVector.h\"\n \n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -172,7 +172,7 @@ LogicalResult Prefetcher::initialize() {\n         break;\n       rets.push_back(op->getOperand(0));\n       if (auto cvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(op))\n-        if (isSharedEncoding(cvt.getOperand())) {\n+        if (triton::gpu::isSharedEncoding(cvt.getOperand())) {\n           foundConvertFromShared = true;\n           break;\n         }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1,4 +1,3 @@\n-#include \"Utility.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n@@ -16,6 +15,7 @@\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n \n #include <memory>\n \n@@ -359,7 +359,7 @@ class RematerializeForward : public mlir::RewritePattern {\n \n     for (Operation *op : cvtSlices) {\n       // don't rematerialize anything expensive\n-      if (expensiveToRemat(op, srcEncoding))\n+      if (isExpensiveToRemat(op, srcEncoding))\n         return failure();\n       // don't rematerialize non-element-wise\n       if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n@@ -408,8 +408,8 @@ class RematerializeBackward : public mlir::RewritePattern {\n     if (!op)\n       return mlir::failure();\n     // we don't want to rematerialize any conversion to/from shared\n-    if (isSharedEncoding(cvt->getResults()[0]) ||\n-        isSharedEncoding(cvt->getOperand(0)))\n+    if (triton::gpu::isSharedEncoding(cvt->getResults()[0]) ||\n+        triton::gpu::isSharedEncoding(cvt->getOperand(0)))\n       return mlir::failure();\n     // we don't handle conversions to DotOperandEncodingAttr\n     // this is a heuristics to accommodate fused attention"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 26, "deletions": 16, "changes": 42, "file_content_changes": "@@ -1,10 +1,10 @@\n-#include \"Utility.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/IR/IRMapping.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n-#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n \n namespace mlir {\n \n@@ -88,32 +88,42 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n   return success();\n }\n \n-bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n+bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n   // Case 1: A size 1 tensor is not expensive since all threads will load the\n   // same\n   if (isSingleValue(op->getOperand(0)))\n     return false;\n   // Case 2: Tensor of pointers has more threads than elements\n   // we can presume a high hit-rate that makes it cheap to load\n   auto ptrType = op->getOperand(0).getType().cast<RankedTensorType>();\n-  IntegerAttr numWarps =\n-      op->getParentOfType<ModuleOp>()->getAttrOfType<IntegerAttr>(\n-          \"triton_gpu.num-warps\");\n-  if (numWarps) {\n-    int sizePerThread = triton::gpu::getTotalElemsPerThread(ptrType);\n-    if (ptrType.getNumElements() < numWarps.getInt() * 32)\n-      return false;\n-  }\n+  auto mod = op->getParentOfType<ModuleOp>();\n+  int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+  int threadsPerWarp = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n+  if (ptrType.getNumElements() < numWarps * threadsPerWarp)\n+    return false;\n   return true;\n }\n \n-bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n+bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding) {\n+  // If the new elements per thread is less than the old one, we will need to do\n+  // convert encoding that goes through shared memory anyway. So we consider it\n+  // as expensive.\n+  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n+  auto totalElemsPerThread = triton::gpu::getTotalElemsPerThread(tensorTy);\n+  auto shape = tensorTy.getShape();\n+  auto elemTy = tensorTy.getElementType();\n+  auto newTotalElemsPerThread =\n+      triton::gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n+  return newTotalElemsPerThread < totalElemsPerThread;\n+}\n+\n+bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   if (!op)\n     return true;\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n-    return expensiveLoadOrStore(op, targetEncoding);\n+    return isExpensiveLoadOrStore(op, targetEncoding);\n   if (isa<triton::CatOp>(op))\n-    return triton::gpu::expensiveCat(cast<triton::CatOp>(op), targetEncoding);\n+    return isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n           triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n           triton::AtomicCASOp, triton::DotOp>(op))\n@@ -126,7 +136,7 @@ bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n \n bool canFoldConversion(Operation *op, Attribute &targetEncoding) {\n   if (isa<triton::CatOp>(op))\n-    return !triton::gpu::expensiveCat(cast<triton::CatOp>(op), targetEncoding);\n+    return !isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n   return isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n              triton::MakeRangeOp, triton::SplatOp, triton::ViewOp>(op);\n }\n@@ -148,7 +158,7 @@ int simulateBackwardRematerialization(\n     queue.pop_back();\n     // If the current operation is expensive to rematerialize,\n     // we stop everything\n-    if (expensiveToRemat(currOp, currLayout))\n+    if (isExpensiveToRemat(currOp, currLayout))\n       break;\n     // A conversion will be removed here (i.e. transferred to operands)\n     numCvts -= 1;"}, {"filename": "test/lib/Analysis/TestAlias.cpp", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -13,6 +13,13 @@ struct TestAliasPass\n \n   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAliasPass);\n \n+  static std::string getValueOperandName(Value value, AsmState &state) {\n+    std::string opName;\n+    llvm::raw_string_ostream ss(opName);\n+    value.printAsOperand(ss, state);\n+    return opName;\n+  }\n+\n   static void print(StringRef name, SmallVector<std::string, 4> &vals,\n                     raw_ostream &os) {\n     if (vals.empty())"}]