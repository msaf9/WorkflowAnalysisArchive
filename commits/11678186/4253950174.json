[{"filename": "python/test/unit/operators/test_flash_attention.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -5,7 +5,8 @@\n \n \n @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n-def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n+@pytest.mark.parametrize('dtype', [torch.float16, torch.bfloat16])\n+def test_op(Z, H, N_CTX, D_HEAD, dtype):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8:\n         pytest.skip(\"Flash attention only supported for compute capability < 80\")\n@@ -21,7 +22,7 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     for z in range(Z):\n         for h in range(H):\n             p[:, :, M == 0] = float(\"-inf\")\n-    p = torch.softmax(p.float(), dim=-1).half()\n+    p = torch.softmax(p.float(), dim=-1).to(dtype)\n     # p = torch.exp(p)\n     ref_out = torch.matmul(p, v)\n     ref_out.backward(dout)"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -63,7 +63,7 @@ def _fwd_kernel(\n         p *= l_rcp\n         acc *= (l_prev * l_rcp)[:, None]\n         # update acc\n-        p = p.to(tl.float16)\n+        p = p.to(Q.dtype.element_ty)\n         v = tl.load(v_ptrs)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n@@ -167,18 +167,18 @@ def _bwd_kernel(\n             p = tl.exp(qk * sm_scale - m[:, None])\n             # compute dv\n             do = tl.load(do_ptrs)\n-            dv += tl.dot(tl.trans(p.to(tl.float16)), do)\n+            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n             # compute dp = dot(v, do)\n             Di = tl.load(D_ptrs + offs_m_curr)\n             dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n             dp += tl.dot(do, tl.trans(v))\n             # compute ds = p * (dp - delta[:, None])\n             ds = p * dp * sm_scale\n             # compute dk = dot(ds.T, q)\n-            dk += tl.dot(tl.trans(ds.to(tl.float16)), q)\n+            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\n             # compute dq\n             dq = tl.load(dq_ptrs)\n-            dq += tl.dot(ds.to(tl.float16), k)\n+            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\n             tl.store(dq_ptrs, dq)\n             # increment pointers\n             dq_ptrs += BLOCK_M * stride_qm"}]