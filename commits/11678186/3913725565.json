[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 1, "deletions": 32, "changes": 33, "file_content_changes": "@@ -192,15 +192,8 @@ def _bwd_kernel(\n         tl.store(dk_ptrs, dk)\n \n \n-# _bwd_kernel = triton.compile(\"./slow.ttgir\", num_warps=8)\n-# _bwd_kernel = triton.compile(\"./unoptimized.ttgir\", num_warps=8)\n-# _bwd_kernel = triton.compile(\"./bwd.ttgir\", num_warps=8)\n-# _fwd_kernel = triton.compile(\"./fails.ptx\", num_warps=4, shared=18432)\n-\n-\n empty = torch.empty(128, device=\"cuda\")\n \n-\n class _attention(torch.autograd.Function):\n \n     @staticmethod\n@@ -229,15 +222,6 @@ def forward(ctx, q, k, v, sm_scale):\n             BLOCK_DMODEL=Lk, num_warps=num_warps,\n             num_stages=1,\n         )\n-        # _fwd_kernel[grid](\n-        #       q.data_ptr(), k.data_ptr(), v.data_ptr(), sm_scale,\n-        #       L.data_ptr(), m.data_ptr(),\n-        #       o.data_ptr(),\n-        #       q.stride(0), q.stride(1), q.stride(2),\n-        #       k.stride(0), k.stride(1), k.stride(2),\n-        #       v.stride(0), v.stride(1), v.stride(2),\n-        #       o.stride(0), o.stride(1), o.stride(2),\n-        #       q.shape[0], q.shape[1], q.shape[2])\n \n         ctx.save_for_backward(q, k, v, o, L, m)\n         ctx.BLOCK = BLOCK\n@@ -261,20 +245,7 @@ def backward(ctx, do):\n             BLOCK_M=ctx.BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n         )\n \n-        # _bwd_kernel[(ctx.grid[1], 1, 1)](\n-        #     q.data_ptr(), k.data_ptr(), v.data_ptr(), ctx.sm_scale,\n-        #     o.data_ptr(), do_scaled.data_ptr(),\n-        #     dq.data_ptr(), dk.data_ptr(), dv.data_ptr(),\n-        #     l.data_ptr(), m.data_ptr(),\n-        #     delta.data_ptr(),\n-        #     q.stride(0), q.stride(1), q.stride(2),\n-        #     k.stride(0), k.stride(1), k.stride(2),\n-        #     v.stride(0), v.stride(1), v.stride(2),\n-        #     q.shape[0], q.shape[1], q.shape[2],\n-        #     ctx.grid[0]\n-        # )\n-\n-        pgm = _bwd_kernel[(ctx.grid[1],)](\n+        _bwd_kernel[(ctx.grid[1],)](\n             q, k, v, ctx.sm_scale,\n             o, do_scaled,\n             dq, dk, dv,\n@@ -289,8 +260,6 @@ def backward(ctx, do):\n             BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n             num_stages=1,\n         )\n-        # print(pgm.asm[\"ttgir\"])\n-        # exit()\n         return dq, dk, dv, None\n \n "}]