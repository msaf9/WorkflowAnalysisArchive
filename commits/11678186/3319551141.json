[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 91, "deletions": 29, "changes": 120, "file_content_changes": "@@ -622,6 +622,13 @@ class ConvertTritonGPUOpToLLVMPattern\n   Value smem;\n };\n \n+Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,\n+                                      Type resType, Type elemType,\n+                                      Value constVal,\n+                                      TypeConverter *typeConverter,\n+                                      ConversionPatternRewriter &rewriter,\n+                                      Location loc);\n+\n // Convert SplatOp or arith::ConstantOp with SplatElementsAttr to a\n // LLVM::StructType value.\n //\n@@ -632,16 +639,26 @@ Value convertSplatLikeOp(Type elemType, Type resType, Value constVal,\n                          TypeConverter *typeConverter,\n                          ConversionPatternRewriter &rewriter, Location loc) {\n   auto tensorTy = resType.cast<RankedTensorType>();\n-  auto layout = tensorTy.getEncoding();\n-  auto srcType = typeConverter->convertType(elemType);\n-  auto llSrc = bitcast(srcType, constVal);\n-  size_t elemsPerThread = getElemsPerThread(layout, tensorTy.getShape());\n-  llvm::SmallVector<Value, 4> elems(elemsPerThread, llSrc);\n-  llvm::SmallVector<Type, 4> elemTypes(elems.size(), srcType);\n-  auto structTy =\n-      LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);\n-\n-  return getStructFromElements(loc, elems, rewriter, structTy);\n+  if (tensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n+    auto tensorTy = resType.cast<RankedTensorType>();\n+    auto layout = tensorTy.getEncoding();\n+    auto srcType = typeConverter->convertType(elemType);\n+    auto llSrc = bitcast(srcType, constVal);\n+    size_t elemsPerThread = getElemsPerThread(layout, tensorTy.getShape());\n+    llvm::SmallVector<Value, 4> elems(elemsPerThread, llSrc);\n+    llvm::SmallVector<Type, 4> elemTypes(elems.size(), srcType);\n+    auto structTy =\n+        LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);\n+\n+    return getStructFromElements(loc, elems, rewriter, structTy);\n+  } else if (auto mmaLayout =\n+                 tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>()) {\n+    return convertSplatLikeOpWithMmaLayout(\n+        mmaLayout, resType, elemType, constVal, typeConverter, rewriter, loc);\n+  } else\n+    assert(false && \"Unsupported layout found in ConvertSplatLikeOp\");\n+\n+  return Value{};\n }\n \n struct SplatOpConversion\n@@ -2358,8 +2375,6 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     MLIRContext *ctx = op->getContext();\n     bool allowTF32 = op.allowTF32();\n \n-    assert(isSplatLike(C) && \"Currently only splat-like C is supported now\");\n-\n     // Here we assume the DotOp's operands always comes from shared memory.\n     auto AShape = A.getType().cast<RankedTensorType>().getShape();\n     size_t reduceAxis = 1;\n@@ -2458,6 +2473,31 @@ struct DotOpConversionHelper {\n     mmaType = getTensorCoreTypeFromOperand(operandTy);\n   }\n \n+  // Deduce the M and N from either $c or $d type.\n+  static std::tuple<int, int> getMatShapeMN() {\n+    // According to DotOpConversionHelper::mmaMatShape, all the matrix shape's\n+    // M,N are {8,8}\n+    return {8, 8};\n+  }\n+\n+  // Get the M and N of mma instruction shape.\n+  static std::tuple<int, int> getInstrShapeMN() {\n+    // According to DotOpConversionHelper::mmaInstrShape, all the M,N are {16,8}\n+    return {16, 8};\n+  }\n+\n+  static std::tuple<int, int> getRepMN(const RankedTensorType &tensorTy) {\n+    auto mmaLayout = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto wpt = mmaLayout.getWarpsPerCTA();\n+\n+    int M = tensorTy.getShape()[0];\n+    int N = tensorTy.getShape()[1];\n+    auto [instrM, instrN] = getInstrShapeMN();\n+    int repM = std::max<int>(M / (wpt[0] * instrM), 1);\n+    int repN = std::max<int>(N / (wpt[1] * instrN), 1);\n+    return {repM, repN};\n+  }\n+\n   Type getShemPtrTy() const {\n     switch (mmaType) {\n     case TensorCoreType::FP32_FP16_FP16_FP32:\n@@ -2816,15 +2856,21 @@ struct MMA16816ConversionHelper {\n     return result;\n   }\n \n-  // Loading $c from smem(?) to registers, returns a Value.\n-  // NOTE Only SplatLike tensor is supported now.\n-  Value loadC(Value tensor) const {\n-    // Currently, we only support a SplatLike C. For the other cases, e.g., C in\n-    // shared layout or blocked layout, we will support them by expanding\n-    // convert_layout.\n-    auto hc = helper.loadSplatLikeC(tensor, loc, rewriter);\n-    assert(hc.size() == 4UL && \"Only splat-like C is supported now\");\n-    return hc[0];\n+  // Loading $c to registers, returns a Value.\n+  Value loadC(Value tensor, Value llTensor) const {\n+    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+    auto [repM, repN] = DotOpConversionHelper::getRepMN(tensorTy);\n+    size_t fcSize = 4 * repM * repN;\n+\n+    assert(tensorTy.getEncoding().isa<MmaEncodingAttr>() &&\n+           \"Currently, we only support $c with a mma layout.\");\n+    // Load a normal C tensor with mma layout, that should be a\n+    // LLVM::struct with fcSize elements.\n+    auto structTy = llTensor.getType().cast<LLVM::LLVMStructType>();\n+    assert(structTy.getBody().size() == fcSize &&\n+           \"DotOp's $c operand should pass the same number of values as $d in \"\n+           \"mma layout.\");\n+    return llTensor;\n   }\n \n   // Conduct the Dot conversion.\n@@ -2856,9 +2902,8 @@ struct MMA16816ConversionHelper {\n         getValuesFromDotOperandLayoutStruct(loadedA, numRepM, numRepK);\n     ValueTable hb = getValuesFromDotOperandLayoutStruct(\n         loadedB, std::max(numRepN / 2, 1), numRepK);\n-\n-    const int fcSize = 4 * numRepM * numRepN;\n-    SmallVector<Value> fc(fcSize, loadedC);\n+    auto fc = ConvertTritonGPUOpToLLVMPatternBase::getElementsFromStruct(\n+        loc, loadedC, rewriter);\n \n     auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n       unsigned colsPerThread = numRepN * 2;\n@@ -3047,7 +3092,7 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n     res = mmaHelper.loadB(src, adaptor.src());\n   } else if (dotOperandLayout.getOpIdx() == 2) {\n     // operand $c\n-    res = mmaHelper.loadC(src);\n+    res = mmaHelper.loadC(src, adaptor.src());\n   }\n \n   rewriter.replaceOp(op, res);\n@@ -3085,17 +3130,34 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adaptor,\n     loadedB = mmaHelper.loadB(op.b(), adaptor.b());\n   }\n \n-  // TODO[Superjomn]: Process C as a mma layout.\n-  // Currently, C is simply treated as a Splat Op, and the data layout is not\n-  // mattered.\n-  loadedC = mmaHelper.loadC(op.c());\n+  loadedC = mmaHelper.loadC(op.c(), adaptor.c());\n \n   return mmaHelper.convertDot(A, B, C, op.d(), loadedA, loadedB, loadedC, op,\n                               adaptor);\n }\n \n /// ====================== mma codegen end ============================\n \n+Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,\n+                                      Type resType, Type elemType,\n+                                      Value constVal,\n+                                      TypeConverter *typeConverter,\n+                                      ConversionPatternRewriter &rewriter,\n+                                      Location loc) {\n+  if (layout.getVersion() == 2) {\n+    auto tensorTy = resType.cast<RankedTensorType>();\n+    auto [repM, repN] = DotOpConversionHelper::getRepMN(tensorTy);\n+    size_t fcSize = 4 * repM * repN;\n+\n+    auto structTy = LLVM::LLVMStructType::getLiteral(\n+        rewriter.getContext(), SmallVector<Type>(fcSize, elemType));\n+    return getStructFromElements(loc, SmallVector<Value>(fcSize, constVal),\n+                                 rewriter, structTy);\n+  }\n+\n+  assert(false && \"Unsupported mma layout found\");\n+}\n+\n class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n public:\n   using TypeConverter::convertType;"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -140,6 +140,9 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n   pm.addPass(createConvertTritonGPUToLLVMPass());\n   // Conanicalize to eliminate the remaining UnrealizedConversionCastOp\n   pm.addPass(mlir::createCanonicalizerPass());\n+  pm.addPass(mlir::createCSEPass()); // Simplify the IR to improve readability.\n+  pm.addPass(mlir::createSymbolDCEPass());\n+  pm.addPass(mlir::createCanonicalizerPass());\n \n   if (failed(pm.run(module))) {\n     llvm::errs() << \"Pass execution failed\";"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 42, "deletions": 21, "changes": 63, "file_content_changes": "@@ -35,6 +35,9 @@ def matmul_no_scf_kernel(\n     [256, 128, 16, 4],\n     [128, 16, 32, 4],\n     [32, 128, 64, 4],\n+    [128, 128, 64, 4],\n+    [64, 128, 128, 4],\n+    [64, 128, 128, 2],\n ])\n def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n     a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n@@ -78,24 +81,42 @@ def matmul_kernel(\n     tl.store(c_ptrs, accumulator)\n \n # TODO: DotConversion in TritonGPUToLLVM cannot support non-splat C for the moment\n-# @pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K', [\n-#    [128, 256, 128, 4, 128, 256, 32],\n-#    # [256, 128, 64, 4, 256, 128, 16],\n-#    # [128, 16, 128, 4, 128, 16, 32],\n-#    # [32, 128, 256, 4, 32, 128, 64],\n-# ])\n-# def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K):\n-#    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n-#    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n-#    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n-#    grid = lambda META: (1, )\n-#    matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n-#                        stride_am=a.stride(0), stride_ak=a.stride(1),\n-#                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n-#                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n-#                        M=a.shape[0], N=b.shape[1], K=a.shape[1],\n-#                        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n-#                        num_warps=NUM_WARPS)\n-#    golden = torch.matmul(a, b)\n-#    torch.set_printoptions(profile=\"full\")\n-#    assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n+\n+\n+@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K', [\n+    # No forloop\n+    [64, 32, 64, 4, 64, 32, 64],\n+    [128, 64, 128, 4, 128, 64, 128],\n+    # Simple forloop\n+    [64, 32, 128, 4, 64, 32, 64],\n+    [128, 16, 128, 4, 128, 16, 32],\n+    [32, 16, 128, 4, 32, 16, 32],\n+    [32, 64, 128, 4, 32, 64, 32],\n+    [32, 128, 256, 4, 32, 128, 64],\n+    [64, 128, 64, 4, 64, 128, 32],  # n can't be 128\n+    [128, 128, 64, 4, 128, 128, 32],\n+\n+    # The following cases failed\n+    # TODO[Superjomn]: Fix later.\n+    #[64, 64, 128, 4, 64, 64, 32],\n+    #[128, 128, 128, 4, 128, 128, 32],\n+    #[128, 128, 256, 4, 128, 128, 64],\n+    #[128, 256, 128, 4, 128, 256, 32],\n+    #[256, 128, 64, 4, 256, 128, 16],\n+    #[128, 64, 128, 4, 128, 64, 32],\n+])\n+def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K):\n+    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n+    grid = lambda META: (1, )\n+    matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n+                        stride_am=a.stride(0), stride_ak=a.stride(1),\n+                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                        M=a.shape[0], N=b.shape[1], K=a.shape[1],\n+                        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n+                        num_warps=NUM_WARPS)\n+    golden = torch.matmul(a, b)\n+    torch.set_printoptions(profile=\"full\")\n+    assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -872,7 +872,9 @@ def make_tritongpu_ir(mod, num_warps):\n def optimize_tritongpu_ir(mod, num_stages):\n     pm = _triton.ir.pass_manager(mod.context)\n     pm.enable_debug()\n-    pm.add_tritongpu_pipeline_pass(num_stages)\n+    # Get error in backend due to wrong conversion in expanding async-related instruction.\n+    # TODO[Superjomn]: Open it when fixed.\n+    # pm.add_tritongpu_pipeline_pass(num_stages)\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n     pm.add_coalesce_pass()"}]