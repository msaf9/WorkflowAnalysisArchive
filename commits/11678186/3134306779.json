[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 151, "deletions": 1, "changes": 152, "file_content_changes": "@@ -360,6 +360,13 @@ class ConvertTritonGPUOpToLLVMPattern\n     return threadId;\n   }\n \n+  Value createIndexConst(ConversionPatternRewriter &rewriter, Location loc,\n+                         int64_t value) const {\n+    return rewriter.create<LLVM::ConstantOp>(\n+        loc, this->getTypeConverter()->getIndexType(),\n+        rewriter.getIntegerAttr(rewriter.getIndexType(), value));\n+  }\n+\n   // Convert an \\param index to a multi-dim coordinate given \\param shape and\n   // \\param order.\n   SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n@@ -1427,12 +1434,15 @@ struct ConvertLayoutOpConversion\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n     Attribute dstLayout = dstTy.getEncoding();\n+    if (srcLayout.isa<BlockedEncodingAttr>() &&\n+        dstLayout.isa<SharedEncodingAttr>()) {\n+      return lowerBlockedToShared(op, adaptor, rewriter);\n+    }\n     if ((!srcLayout.isa<BlockedEncodingAttr>() &&\n          !srcLayout.isa<MmaEncodingAttr>()) ||\n         (!dstLayout.isa<BlockedEncodingAttr>() &&\n          !dstLayout.isa<MmaEncodingAttr>())) {\n       // TODO: to be implemented\n-      llvm::errs() << \"Unsupported ConvertLayout found\";\n       return failure();\n     }\n     auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n@@ -1629,6 +1639,146 @@ struct ConvertLayoutOpConversion\n       }\n     }\n   }\n+\n+  LogicalResult\n+  lowerBlockedToShared(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+                       ConversionPatternRewriter &rewriter) const {\n+    auto loc = op.getLoc();\n+    Value src = op.src();\n+    Value dst = op.result();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto srcShape = srcTy.getShape();\n+    assert(srcShape.size() == 2 &&\n+           \"Unexpected rank of ConvertLayout(blocked->shared)\");\n+    auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+    auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n+    auto inOrd = srcBlockedLayout.getOrder();\n+    auto outOrd = dstSharedLayout.getOrder();\n+    unsigned inVec =\n+        inOrd == outOrd ? srcBlockedLayout.getSizePerThread()[inOrd[0]] : 1;\n+    unsigned outVec = dstSharedLayout.getVec();\n+    unsigned minVec = std::min(outVec, inVec);\n+    unsigned perPhase = dstSharedLayout.getPerPhase();\n+    unsigned maxPhase = dstSharedLayout.getMaxPhase();\n+    unsigned numElems = getElemsPerThread(srcBlockedLayout, srcShape);\n+    auto inVals = getElementsFromStruct(loc, adaptor.src(), numElems, rewriter);\n+    unsigned srcAccumSizeInThreads =\n+        product<unsigned>(srcBlockedLayout.getSizePerThread());\n+    auto elemTy = srcTy.getElementType();\n+    auto wordTy = VectorType::get(minVec, elemTy);\n+\n+    // TODO: [goostavz] We should make a cache for the calculation of\n+    // emitBaseIndexForBlockedLayout in case backend compiler not being able to\n+    // optimize that\n+    SmallVector<Value> multiDimOffsetFirstElem = emitBaseIndexForBlockedLayout(\n+        loc, rewriter, srcBlockedLayout, srcShape);\n+    SmallVector<unsigned> srcShapePerCTA = getShapePerCTA(srcBlockedLayout);\n+    SmallVector<unsigned> reps{ceil<unsigned>(srcShape[0], srcShapePerCTA[0]),\n+                               ceil<unsigned>(srcShape[1], srcShapePerCTA[1])};\n+\n+    // Visit each input value in the order they are placed in inVals\n+    //\n+    // Please note that the order was not awaring of blockLayout.getOrder(),\n+    // thus the adjacent elems may not belong to a same word. This could be\n+    // improved if we update the elements order by emitIndicesForBlockedLayout()\n+    SmallVector<unsigned> wordsInEachRep(2);\n+    wordsInEachRep[0] = inOrd[0] == 0\n+                            ? srcBlockedLayout.getSizePerThread()[0] / minVec\n+                            : srcBlockedLayout.getSizePerThread()[0];\n+    wordsInEachRep[1] = inOrd[0] == 0\n+                            ? srcBlockedLayout.getSizePerThread()[1]\n+                            : srcBlockedLayout.getSizePerThread()[1] / minVec;\n+    Value outVecVal = createIndexConst(rewriter, loc, outVec);\n+    Value minVecVal = createIndexConst(rewriter, loc, minVec);\n+    Value smemBase = getSharedMemoryBase(loc, rewriter, dst);\n+    auto elemPtrTy =\n+        LLVM::LLVMPointerType::get(getTypeConverter()->convertType(elemTy), 3);\n+    smemBase = rewriter.create<LLVM::BitcastOp>(loc, elemPtrTy, smemBase);\n+    unsigned numWordsEachRep = product<unsigned>(wordsInEachRep);\n+    SmallVector<Value> wordVecs(numWordsEachRep);\n+    for (unsigned i = 0; i < numElems; ++i) {\n+      if (i % srcAccumSizeInThreads == 0) {\n+        // start of a replication\n+        for (unsigned w = 0; w < numWordsEachRep; ++w) {\n+          wordVecs[w] = rewriter.create<LLVM::UndefOp>(loc, wordTy);\n+        }\n+      }\n+      unsigned linearIdxInNanoTile = i % srcAccumSizeInThreads;\n+      auto multiDimIdxInNanoTile = getMultiDimIndex<unsigned>(\n+          i % srcAccumSizeInThreads, srcBlockedLayout.getSizePerThread());\n+      multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n+      unsigned pos = multiDimIdxInNanoTile[inOrd[0]] % minVec;\n+      unsigned wordVecIdx =\n+          getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep);\n+      wordVecs[wordVecIdx] = rewriter.create<LLVM::InsertElementOp>(\n+          loc, wordTy, wordVecs[wordVecIdx], inVals[i],\n+          createIndexConst(rewriter, loc, pos));\n+\n+      if (i % srcAccumSizeInThreads == srcAccumSizeInThreads - 1) {\n+        // end of replication, store the vectors into shared memory\n+        unsigned linearRepIdx = i / srcAccumSizeInThreads;\n+        auto multiDimRepIdx = getMultiDimIndex<unsigned>(linearRepIdx, reps);\n+        for (unsigned linearWordIdx = 0; linearWordIdx < numWordsEachRep;\n+             ++linearWordIdx) {\n+          // step 1: recover the multidim_index from the index of input_elements\n+          // row = multiDimOffsetFirstElem[0] +\n+          //       multiDimRepIdx[0] * shapePerCTA[0] + multiDimWordIdx[0] *\n+          //       (inOrd[0] == 0) ? minVec : 1\n+          // col = multiDimOffsetFirstElem[1] +\n+          //       multiDimRepIdx[1] * shapePerCTA[1] + multiDimWordIdx[1] *\n+          //       (inOrd[0] == 1) ? minVec : 1\n+          auto multiDimWordIdx =\n+              getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep);\n+          SmallVector<Value> multiDimIdx(2);\n+          multiDimIdx[0] = rewriter.create<LLVM::AddOp>(\n+              loc, multiDimOffsetFirstElem[0],\n+              createIndexConst(rewriter, loc,\n+                               multiDimRepIdx[0] * srcShapePerCTA[0] +\n+                                   multiDimWordIdx[0] *\n+                                       (inOrd[0] == 0 ? minVec : 1)));\n+          multiDimIdx[1] = rewriter.create<LLVM::AddOp>(\n+              loc, multiDimOffsetFirstElem[1],\n+              createIndexConst(rewriter, loc,\n+                               multiDimRepIdx[1] * srcShapePerCTA[1] +\n+                                   multiDimWordIdx[1] *\n+                                       (inOrd[0] == 1 ? minVec : 1)));\n+\n+          // step 2: do swizzling\n+          Value remained = rewriter.create<LLVM::URemOp>(\n+              loc, multiDimIdx[inOrd[0]], outVecVal);\n+          multiDimIdx[inOrd[0]] = rewriter.create<LLVM::UDivOp>(\n+              loc, multiDimIdx[inOrd[0]], outVecVal);\n+          Value off_1 = rewriter.create<LLVM::MulOp>(\n+              loc, multiDimIdx[inOrd[1]],\n+              createIndexConst(rewriter, loc, srcShape[inOrd[0]]));\n+          Value phaseId = rewriter.create<LLVM::UDivOp>(\n+              loc, multiDimIdx[inOrd[1]],\n+              createIndexConst(rewriter, loc, perPhase));\n+          phaseId = rewriter.create<LLVM::URemOp>(\n+              loc, phaseId, createIndexConst(rewriter, loc, maxPhase));\n+          Value off_0 =\n+              rewriter.create<LLVM::XOrOp>(loc, multiDimIdx[inOrd[0]], phaseId);\n+          off_0 = rewriter.create<LLVM::MulOp>(loc, off_0, outVecVal);\n+          remained = rewriter.create<LLVM::UDivOp>(loc, remained, minVecVal);\n+          off_0 = rewriter.create<LLVM::AddOp>(\n+              loc, off_0,\n+              rewriter.create<LLVM::MulOp>(loc, remained, minVecVal));\n+          Value offset = rewriter.create<LLVM::AddOp>(loc, off_1, off_0);\n+\n+          // step 3: store\n+          Value smemAddr =\n+              rewriter.create<LLVM::GEPOp>(loc, elemPtrTy, smemBase, offset);\n+          smemAddr = rewriter.create<LLVM::BitcastOp>(\n+              loc, LLVM::LLVMPointerType::get(wordTy, 3), smemAddr);\n+          rewriter.create<LLVM::StoreOp>(loc, wordVecs[linearWordIdx],\n+                                         smemAddr);\n+        }\n+      }\n+    }\n+    rewriter.replaceOp(op, smemBase);\n+    return success();\n+  }\n };\n \n /// ====================== dot codegen begin =========================="}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "file_content_changes": "@@ -87,7 +87,6 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n   }\n-\n   return shape;\n }\n \n@@ -104,7 +103,7 @@ SmallVector<unsigned> getOrder(const Attribute &layout) {\n     assert(0 && \"Unimplemented usage of getOrder\");\n     return {};\n   }\n-}\n+};\n \n } // namespace gpu\n } // namespace triton\n@@ -215,9 +214,12 @@ unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n }\n \n unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n-  int threads = product(getWarpsPerCTA());\n-  int numElem = product(shape);\n-  return numElem / threads;\n+  size_t rank = shape.size();\n+  assert(rank == 2 && \"Unexpected rank of mma layout\");\n+  assert(getVersion() == 2 && \"mmaLayout version = 1 is not implemented yet\");\n+  unsigned elemsCol = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]) * 2;\n+  unsigned elemsRow = ceil<unsigned>(shape[1], 8 * getWarpsPerCTA()[1]) * 2;\n+  return elemsCol * elemsRow;\n }\n \n unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 17, "deletions": 0, "changes": 17, "file_content_changes": "@@ -515,3 +515,20 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     return\n   }\n }\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<16384 x i8>\n+  // CHECK-LABEL: convert_layout_blocked_shared\n+  func @convert_layout_blocked_shared(%arg0: tensor<128x32xf32, #blocked0>) {\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<8xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<8xf32>, 3>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n+    return\n+  }\n+}\n\\ No newline at end of file"}]