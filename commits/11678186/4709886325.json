[{"filename": ".clang-format", "status": "added", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -0,0 +1 @@\n+BasedOnStyle: LLVM"}, {"filename": ".editorconfig", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "file_content_changes": "@@ -0,0 +1,41 @@\n+# https://editorconfig.org/\n+\n+root = true\n+\n+[*]\n+charset = utf-8\n+end_of_line = lf\n+indent_style = space\n+indent_size = 4\n+trim_trailing_whitespace = true\n+insert_final_newline = true\n+\n+[*.py]\n+indent_size = 4\n+src_paths=python\n+\n+[*.{yaml,yml}]\n+indent_size = 2\n+\n+[*.md]\n+indent_size = 2\n+x-soft-wrap-text = true\n+\n+[*.rst]\n+indent_size = 4\n+x-soft-wrap-text = true\n+\n+[CMakeLists.txt,*.cmake]\n+indent_size = 2\n+\n+[Makefile]\n+indent_style = tab\n+\n+[*.{c,cc,cpp,h,hpp,cu,cuh}]\n+indent_size = 2\n+\n+[*.mlir]\n+indent_size = 2\n+\n+[*.td]\n+indent_size = 4"}, {"filename": ".flake8", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -0,0 +1,2 @@\n+[flake8]\n+ignore = E501,E701,E731"}, {"filename": ".github/CODEOWNERS", "status": "added", "additions": 57, "deletions": 0, "changes": 57, "file_content_changes": "@@ -0,0 +1,57 @@\n+# These owners will be the default owners for everything in\n+# the repo. Unless a later match takes precedence,\n+# @global-owner1 and @global-owner2 will be requested for\n+# review when someone opens a pull request.\n+*       @ptillet\n+\n+# --------\n+# Analyses\n+# --------\n+# Alias analysis\n+include/triton/Analysis/Alias.h @Jokeren\n+lib/Analysis/Alias.cpp @Jokeren\n+# Allocation analysis\n+include/triton/Analysis/Allocation.h @Jokeren\n+lib/Analysis/Allocation.cpp @Jokeren\n+# Membar analysis\n+include/triton/Analysis/Membar.h @Jokeren\n+lib/Analysis/Membar.cpp @Jokeren\n+# AxisInfo analysis\n+include/triton/Analysis/AxisInfo.h @ptillet\n+lib/Analysis/AxisInfo.cpp @ptillet\n+# Utilities\n+include/triton/Analysis/Utility.h @Jokeren\n+lib/Analysis/Utility.cpp @Jokeren\n+\n+# ----------\n+# Dialects\n+# ----------\n+# Pipeline pass\n+lib/Dialect/TritonGPU/Transforms/Pipeline.cpp @daadaada\n+# Prefetch pass\n+lib/Dialect/TritonGPU/Transforms/Prefetch.cpp @daadaada\n+# Coalesce pass\n+lib/Dialect/TritonGPU/Transforms/Coalesce.cpp @ptillet\n+# Layout simplification pass\n+lib/Dialect/TritonGPU/Transforms/Combine.cpp @ptillet\n+\n+# -----------\n+# Conversions\n+# -----------\n+# TritonGPUToLLVM\n+include/triton/Conversion/TritonGPUToLLVM/ @goostavz @Superjomn\n+lib/Conversions/TritonGPUToLLVM @goostavz @Superjomn\n+# TritonToTritonGPU\n+include/triton/Conversion/TritonToTritonGPU/ @daadaada\n+lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp @daadaada\n+\n+\n+# -------\n+# Targets\n+# -------\n+# LLVMIR\n+include/triton/Target/LLVMIR/ @goostavz @Superjomn\n+lib/Target/LLVMIR @goostavz @Superjomn\n+# PTX\n+include/triton/Target/PTX/ @goostavz @Superjomn\n+lib/Target/PTX @goostavz @Superjomn"}, {"filename": ".github/workflows/documentation.yml", "status": "modified", "additions": 22, "deletions": 24, "changes": 46, "file_content_changes": "@@ -1,55 +1,53 @@\n name: Documentation\n on:\n   workflow_dispatch:\n-  schedule:    \n+  schedule:\n     - cron: \"0 0 * * *\"\n \n jobs:\n-\n   Build-Documentation:\n-    \n-    runs-on: self-hosted\n+    runs-on: [self-hosted, A100]\n \n     steps:\n-\n-\n-      - name: Checkout gh-pages\n-        uses: actions/checkout@v1\n+      - name: Checkout branch\n+        uses: actions/checkout@v2\n         with:\n-          ref: 'gh-pages'\n+          token: ${{ secrets.CI_PAT }}\n+          fetch-depth: 0\n \n       - name: Clear docs\n         run: |\n           rm -r /tmp/triton-docs\n         continue-on-error: true\n \n-      - name: Checkout branch\n-        uses: actions/checkout@v1\n+      - name: Install dependent packages\n+        run: |\n+          pip3 install tabulate\n+          pip3 install cmake\n+\n+      #- name: Fetch dependent branches\n+      #  run: |\n+      #    git fetch origin main:main\n \n       - name: Build docs\n         run: |\n-          git fetch origin master:master\n           cd docs\n-          sphinx-multiversion . _build/html/\n+          export PATH=$(python3 -c \"import cmake; print(cmake.CMAKE_BIN_DIR)\"):$PATH\n+          python3 -m sphinx_multiversion . _build/html/\n \n-      - name: Publish docs\n+      - name: Update docs\n         run: |\n-          git branch\n-          # update docs\n-          mkdir /tmp/triton-docs;\n+          mkdir /tmp/triton-docs\n           mv docs/_build/html/* /tmp/triton-docs/\n           git checkout gh-pages\n           cp -r CNAME /tmp/triton-docs/\n           cp -r index.html /tmp/triton-docs/\n           cp -r .nojekyll /tmp/triton-docs/\n           rm -r *\n           cp -r /tmp/triton-docs/* .\n-          # ln -s master/index.html .\n-          # mv master docs\n           git add .\n           git commit -am \"[GH-PAGES] Updated website\"\n-          # publish docs\n-          eval `ssh-agent -s`\n-          DISPLAY=:0 SSH_ASKPASS=~/.ssh/give_pass.sh ssh-add ${{ secrets.SSH_KEY }} <<< ${{ secrets.SSH_PASS }}\n-          git remote set-url origin git@github.com:openai/triton.git\n-          git push\n+\n+      - name: Publish docs\n+        run: |\n+          git push origin gh-pages"}, {"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 91, "deletions": 22, "changes": 113, "file_content_changes": "@@ -3,53 +3,122 @@ name: Integration Tests\n on:\n   workflow_dispatch:\n   pull_request:\n-    branches:\n-      - master\n-      - v2.0\n+    branches: [main]\n+  merge_group:\n+    branches: [main]\n+    types: [checks_requested]\n \n+concurrency:\n+  group: ${{ github.ref }}\n+  cancel-in-progress: ${{ github.ref != 'refs/heads/master' }}\n+\n+env:\n+  TRITON_USE_ASSERT_ENABLED_LLVM: \"TRUE\"\n \n jobs:\n+  Runner-Preparation:\n+    runs-on: ubuntu-latest\n+    outputs:\n+      matrix: ${{ steps.set-matrix.outputs.matrix }}\n+    steps:\n+      - name: Prepare runner matrix\n+        id: set-matrix\n+        run: |\n+          if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n+            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], [\"self-hosted\", \"gfx908\"], \"macos-10.15\"]'\n+          else\n+            echo '::set-output name=matrix::[\"ubuntu-latest\", \"macos-10.15\"]'\n+          fi\n \n   Integration-Tests:\n-    \n-    runs-on: self-hosted\n+    needs: Runner-Preparation\n \n-    steps:\n+    runs-on: ${{ matrix.runner }}\n+\n+    strategy:\n+      matrix:\n+        runner: ${{fromJson(needs.Runner-Preparation.outputs.matrix)}}\n \n+    steps:\n       - name: Checkout\n         uses: actions/checkout@v2\n \n+      - name: Set CUDA ENV\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100')}}\n+        run: |\n+          echo \"BACKEND=CUDA\" >> \"${GITHUB_ENV}\"\n+\n+      - name: Set ROCM ENV\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'gfx908')}}\n+        run: |\n+          echo \"BACKEND=ROCM\" >> \"${GITHUB_ENV}\"\n+\n       - name: Clear cache\n         run: |\n-          rm -r /tmp/triton/\n-        continue-on-error: true\n+          rm -rf ~/.triton\n+\n+      - name: Update PATH\n+        run: |\n+          echo \"PATH=${HOME}/.local/bin:${PATH}\" >> \"${GITHUB_ENV}\"\n+\n+      - name: Check pre-commit\n+        if: ${{ matrix.runner != 'macos-10.15' }}\n+        run: |\n+          python3 -m pip install --upgrade pre-commit\n+          python3 -m pre_commit run --all-files\n \n       - name: Install Triton\n+        if: ${{ env.BACKEND != 'ROCM'}}\n         run: |\n-          alias python='python3'\n           cd python\n-          pip3 install -e '.[tests]'\n-\n-      - name: Check imports\n-        run: \"isort -c ./python || ( echo '::error title=Imports not sorted::Please run \\\"isort ./python\\\"' ; exit 1 )\"\n+          python3 -m pip install --upgrade pip\n+          python3 -m pip install cmake==3.24\n+          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n-      - name: Check style\n-        run: \"autopep8 -a -r -d --exit-code ./python || ( echo '::error title=Style issues::Please run \\\"autopep8 -a -r -i ./python\\\"' ; exit 1 )\"\n+      - name: Install Triton on ROCM\n+        if: ${{ env.BACKEND == 'ROCM'}}\n+        run: |\n+          cd python\n+          python3 -m pip install --upgrade pip\n+          python3 -m pip install cmake==3.24\n+          python3 -m pip install torch==1.13.1 --index-url https://download.pytorch.org/whl/rocm5.2\n+          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n-      - name: Flake8\n-        run: \"flake8 --config ./python/setup.cfg ./python || ( echo '::error::Flake8 failed; see logs for errors.' ; exit 1 )\"\n+      - name: Run lit tests\n+        if: ${{ env.BACKEND != 'ROCM'}}\n+        run: |\n+          python3 -m pip install lit\n+          cd python\n+          LIT_TEST_DIR=\"build/$(ls build | grep -i temp)/test\"\n+          if [ ! -d \"${LIT_TEST_DIR}\" ]; then\n+            echo \"Coult not find '${LIT_TEST_DIR}'\" ; exit -1\n+          fi\n+          lit -v \"${LIT_TEST_DIR}\"\n \n-      - name: Unit tests\n+      - name: Run python tests on CUDA\n+        if: ${{ env.BACKEND == 'CUDA'}}\n         run: |\n           cd python/test/unit\n-          pytest -vs .\n+          python3 -m pytest\n+\n+      - name: Run CXX unittests\n+        if: ${{ env.BACKEND != 'ROCM'}}\n+        run: |\n+          cd python\n+          cd \"build/$(ls build | grep -i temp)\"\n+          ctest\n+\n+      - name: Run python tests on ROCM\n+        if: ${{ env.BACKEND == 'ROCM'}}\n+        run: |\n+          cd python/test/unit/language\n+          python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py::test_empty_kernel\"\n \n       - name: Regression tests\n+        if: ${{ contains(matrix.runner, 'A100') }}\n         run: |\n           cd python/test/regression\n           sudo nvidia-smi -i 0 -pm 1\n           sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n-          sudo nvidia-smi -i 0 --lock-memory-clocks=877,877\n-          pytest -vs .\n+          python3 -m pytest -vs .\n           sudo nvidia-smi -i 0 -rgc\n-          sudo nvidia-smi -i 0 -rmc"}, {"filename": ".github/workflows/torch-inductor-tests.yml", "status": "added", "additions": 39, "deletions": 0, "changes": 39, "file_content_changes": "@@ -0,0 +1,39 @@\n+name: Torchinductor\n+\n+on:\n+  workflow_dispatch:\n+\n+jobs:\n+  Runner-Preparation:\n+    runs-on: ubuntu-latest\n+    outputs:\n+      matrix: ${{ steps.set-matrix.outputs.matrix }}\n+    steps:\n+      - name: Prepare runner matrix\n+        id: set-matrix\n+        run: |\n+          echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"]]'\n+\n+  Integration-Tests:\n+    needs: Runner-Preparation\n+    runs-on: ${{ matrix.runner }}\n+    strategy:\n+      matrix:\n+        runner: ${{fromJson(needs.Runner-Preparation.outputs.matrix)}}\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v2\n+      #- name: Packages\n+      #  run: |\n+      #    ./.github/workflows/torchinductor/scripts/install_torchinductor.sh\n+      - name: Environment\n+        run: |\n+          source /opt/torchinductor_venv/bin/activate\n+          ./.github/workflows/torchinductor/scripts/install_triton.sh\n+      - name: Performance\n+        run: |\n+          ./.github/workflows/torchinductor/scripts/run_torchinductor_perf.sh\n+      # Runs too long time\n+      #- name: Accuracy\n+      #  run: |\n+      #    ./.github/workflows/torchinductor/scripts/run_torchinductor_acc.sh"}, {"filename": ".github/workflows/torchinductor/data/huggingface.csv", "status": "added", "additions": 37, "deletions": 0, "changes": 37, "file_content_changes": "@@ -0,0 +1,37 @@\n+dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio\n+cuda,AlbertForMaskedLM,4,1.5511,164.3373,26.8523,1.2647\n+cuda,AlbertForQuestionAnswering,4,1.5501,163.5580,25.7983,1.3145\n+cuda,BartForCausalLM,4,1.5080,71.7230,32.8907,0.9749\n+cuda,BertForMaskedLM,16,1.5350,67.9451,35.3286,1.0494\n+cuda,BertForQuestionAnswering,16,1.6735,53.2963,34.3754,1.1710\n+cuda,BlenderbotSmallForCausalLM,64,1.2106,46.6466,23.8058,0.9120\n+cuda,BlenderbotSmallForConditionalGeneration,64,1.3616,77.3013,55.3546,0.9803\n+cuda,CamemBert,16,1.4779,76.1809,35.3883,1.0469\n+cuda,DebertaForMaskedLM,4,0.8415,62.3395,35.9657,1.0418\n+cuda,DebertaForQuestionAnswering,8,1.0609,67.5151,35.7728,1.1528\n+cuda,DebertaV2ForMaskedLM,1,0.6026,134.6517,66.1783,0.9773\n+cuda,DistilBertForMaskedLM,128,1.2460,66.9382,18.3089,0.9624\n+cuda,DistilBertForQuestionAnswering,256,1.3997,72.4126,18.1956,1.1486\n+cuda,DistillGPT2,16,1.6656,60.5455,17.2280,1.0641\n+cuda,ElectraForCausalLM,32,1.8299,45.4841,37.0944,0.9717\n+cuda,ElectraForQuestionAnswering,64,2.0289,52.6890,35.9632,1.1928\n+cuda,GPT2ForSequenceClassification,4,2.2567,38.2969,30.0527,1.2323\n+cuda,LayoutLMForMaskedLM,16,1.5423,68.8018,36.5562,1.0495\n+cuda,LayoutLMForSequenceClassification,16,1.7058,53.9355,35.2225,1.1659\n+cuda,MBartForCausalLM,4,1.4945,71.4649,32.8653,0.9830\n+cuda,MegatronBertForCausalLM,4,1.4328,58.4404,70.6226,1.0951\n+cuda,MegatronBertForQuestionAnswering,8,1.5886,85.2533,69.1219,1.1152\n+cuda,MobileBertForMaskedLM,64,0.9007,131.7379,107.5275,1.0136\n+cuda,MobileBertForQuestionAnswering,128,0.8435,167.9066,106.7049,0.8579\n+cuda,PLBartForCausalLM,8,1.5261,68.9224,19.5826,0.9887\n+cuda,PLBartForConditionalGeneration,4,1.5298,71.2811,45.6902,1.0495\n+cuda,PegasusForCausalLM,32,1.2212,57.5436,33.3863,0.9736\n+cuda,PegasusForConditionalGeneration,32,1.2822,106.4678,69.8825,1.0689\n+cuda,RobertaForCausalLM,16,1.6128,67.5706,34.7355,1.0496\n+cuda,RobertaForQuestionAnswering,16,1.6800,53.6267,33.8527,1.1704\n+cuda,Speech2Text2ForCausalLM,256,1.8230,32.9145,18.7201,0.8760\n+cuda,T5ForConditionalGeneration,4,1.6592,59.5324,39.4406,1.1814\n+cuda,T5Small,4,1.6581,59.5930,37.0471,1.1814\n+cuda,TrOCRForCausalLM,32,1.2586,106.2633,32.5330,0.9583\n+cuda,XLNetLMHeadModel,8,1.8108,142.8795,84.8197,1.1240\n+cuda,YituTechConvBert,16,1.5207,81.4595,53.1565,1.0362"}, {"filename": ".github/workflows/torchinductor/data/timm_models.csv", "status": "added", "additions": 54, "deletions": 0, "changes": 54, "file_content_changes": "@@ -0,0 +1,54 @@\n+dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio\n+cuda,adv_inception_v3,128,1.5923,102.5292,51.6032,1.0472\n+cuda,beit_base_patch16_224,64,1.3390,75.3027,29.7471,1.0156\n+cuda,coat_lite_mini,128,2.0579,53.3689,37.1856,1.0437\n+cuda,convmixer_768_32,32,1.0470,275.5328,23.8037,0.9999\n+cuda,convnext_base,64,1.5084,80.1811,42.5659,1.0373\n+cuda,crossvit_9_240,128,1.5392,37.1806,44.9986,0.9193\n+cuda,cspdarknet53,64,1.4721,75.0403,35.2882,1.0547\n+cuda,deit_base_distilled_patch16_224,64,1.1432,55.9737,23.4038,0.9816\n+cuda,dla102,128,1.5282,123.7284,49.3612,1.0430\n+cuda,dm_nfnet_f0,128,1.4354,79.7518,34.8994,1.1038\n+cuda,dpn107,32,1.2412,83.8921,58.9111,0.9952\n+cuda,eca_botnext26ts_256,128,1.5425,71.2406,28.8920,1.0270\n+cuda,ese_vovnet19b_dw,128,1.4647,42.4837,18.0285,1.0135\n+cuda,fbnetc_100,128,1.5795,53.8033,33.0222,1.0082\n+cuda,gernet_l,128,1.1684,63.4230,26.8687,1.0053\n+cuda,ghostnet_100,128,1.7812,54.4211,47.6168,1.0484\n+cuda,gluon_inception_v3,128,1.5952,102.5018,50.0857,1.0469\n+cuda,gmixer_24_224,128,1.6749,69.2430,42.0841,1.1921\n+cuda,gmlp_s16_224,128,1.5886,79.2132,43.0142,1.2343\n+cuda,hrnet_w18,128,1.3743,221.5304,134.2573,1.0100\n+cuda,inception_v3,128,1.5847,102.8333,49.7648,1.0472\n+cuda,jx_nest_base,32,1.3747,71.4190,61.4053,0.9905\n+cuda,lcnet_050,128,1.8159,18.0047,18.8249,1.0005\n+cuda,mixer_b16_224,128,1.2795,90.9229,21.0438,1.0133\n+cuda,mixnet_l,128,1.2273,149.9722,47.7482,1.0129\n+cuda,mnasnet_100,128,1.6594,40.0512,26.5165,1.0047\n+cuda,mobilenetv2_100,128,1.6085,41.1217,27.4450,1.1731\n+cuda,mobilenetv3_large_100,128,1.6610,37.9995,29.8185,1.0052\n+cuda,mobilevit_s,64,1.5212,55.4152,53.6475,1.0258\n+cuda,nfnet_l0,128,1.4927,65.7078,32.4067,0.9980\n+cuda,pit_b_224,64,1.2286,57.9484,26.5321,0.9606\n+cuda,pnasnet5large,16,1.0000,198.2494,93.4641,1.3184\n+cuda,poolformer_m36,64,1.3486,103.9235,62.3196,1.1942\n+cuda,regnety_002,128,1.3030,32.4968,27.2439,1.0014\n+cuda,repvgg_a2,128,1.2485,59.7729,26.9209,1.0185\n+cuda,res2net101_26w_4s,64,1.0813,94.1773,86.6520,0.9655\n+cuda,res2net50_14w_8s,128,1.3251,109.5258,79.9578,0.9830\n+cuda,res2next50,128,1.2518,125.5008,43.9754,0.9756\n+cuda,resmlp_12_224,128,1.3060,45.2373,19.3709,1.1048\n+cuda,resnest101e,64,1.4346,108.1945,78.1993,1.1037\n+cuda,rexnet_100,128,1.4637,55.0121,41.2075,1.0862\n+cuda,selecsls42b,128,1.4284,44.6645,23.3892,1.0139\n+cuda,spnasnet_100,128,1.5908,45.3189,32.0148,1.0048\n+cuda,swin_base_patch4_window7_224,64,1.6164,89.5854,75.5848,0.9299\n+cuda,swsl_resnext101_32x16d,32,1.0175,110.0041,45.7853,1.0003\n+cuda,tf_efficientnet_b0,128,1.5271,55.7361,34.5551,1.1079\n+cuda,tf_mixnet_l,128,1.2369,155.9027,48.6695,1.0921\n+cuda,tinynet_a,128,1.3792,53.0640,40.6346,1.1108\n+cuda,tnt_s_patch16_224,128,3.1078,104.8486,59.6028,1.0660\n+cuda,twins_pcpvt_base,64,1.5921,67.4600,84.4977,1.0909\n+cuda,visformer_small,128,1.1952,72.8705,23.7303,1.0410\n+cuda,vit_base_patch16_224,64,1.1309,56.4866,22.0208,0.9804\n+cuda,volo_d1_224,64,1.6868,72.0957,65.3011,0.9729"}, {"filename": ".github/workflows/torchinductor/data/torchbench.csv", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -0,0 +1,53 @@\n+dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio\n+cuda,BERT_pytorch,16,1.7111,24.2741,35.7065,1.3212\n+cuda,LearningToPaint,96,1.0513,10.7557,11.1879,0.9896\n+cuda,Super_SloMo,6,1.3267,60.4328,28.2097,1.2392\n+cuda,alexnet,128,1.1754,8.3246,5.3319,1.0003\n+cuda,attention_is_all_you_need_pytorch,256,1.3416,36.4401,39.5927,1.1774\n+cuda,dcgan,32,0.9151,2.6249,3.2964,1.0082\n+cuda,densenet121,4,0.9225,51.3747,68.5841,0.9930\n+cuda,doctr_det_predictor,0,0.0000\n+cuda,doctr_reco_predictor,0,0.0000\n+cuda,drq,1,0.9500,3.4884,4.8028,0.9687\n+cuda,fastNLP_Bert,6,1.4328,34.7753,35.4863,1.2368\n+cuda,functorch_dp_cifar10,64,1.2015,8.1625,12.9040,1.0609\n+cuda,functorch_maml_omniglot,1,0.9322,2.5844,3.8640,1.0000\n+cuda,hf_Albert,8,2.1228,30.3377,26.8282,1.2676\n+cuda,hf_Bart,4,1.2899,39.1935,47.2373,1.0080\n+cuda,hf_Bert,4,1.3262,26.1063,35.0281,1.0656\n+cuda,hf_Bert_large,4,1.4163,55.1021,67.2825,1.0915\n+cuda,hf_DistilBert,8,1.4051,21.7191,18.0399,1.0242\n+cuda,hf_GPT2,4,1.6661,26.9039,29.9473,1.1555\n+cuda,hf_Longformer,0,0.0000\n+cuda,hf_Reformer,4,1.1709,64.6979,15.7035,0.9267\n+cuda,hf_T5_large,2,1.7215,107.0798,148.8805,1.1684\n+cuda,lennard_jones,1000,0.8428,1.8488,3.0609,1.0001\n+cuda,maml_omniglot,32,0.9648,2.6869,3.9775,0.9999\n+cuda,mnasnet1_0,32,1.0469,21.6251,25.8232,0.9996\n+cuda,mobilenet_v2,96,1.5604,31.9572,27.0225,1.1734\n+cuda,nvidia_deeprecommender,256,1.0605,9.2080,4.1318,0.9711\n+cuda,phlippe_densenet,128,1.0237,27.5988,28.0400,1.0023\n+cuda,phlippe_resnet,128,1.0493,10.9751,10.2485,1.0092\n+cuda,pytorch_CycleGAN_and_pix2pix,1,1.3724,8.2225,11.9561,1.0219\n+cuda,pytorch_stargan,16,1.1835,11.9178,10.0507,1.0868\n+cuda,pytorch_unet,1,1.3787,29.7543,13.7711,1.0100\n+cuda,resnet152,32,0.9834,63.2446,67.7935,0.9991\n+cuda,resnet18,16,0.9451,9.4977,11.7663,0.9948\n+cuda,resnet50,32,1.0513,24.5141,24.6629,1.0021\n+cuda,resnext50_32x4d,8,0.9216,22.2460,24.3420,0.9984\n+cuda,shufflenet_v2_x1_0,128,1.1943,25.4520,28.8611,1.0951\n+cuda,soft_actor_critic,256,0.8691,1.9637,3.3716,0.9996\n+cuda,speech_transformer,32,1.2718,35.2922,46.9957,1.0897\n+cuda,squeezenet1_1,32,1.1302,8.4540,7.9625,1.0771\n+cuda,timm_efficientdet,1,1.3370,80.0377,120.1814,1.2713\n+cuda,timm_efficientnet,32,1.1874,27.6302,33.9059,1.0971\n+cuda,timm_nfnet,128,1.4525,77.3461,34.3270,1.1056\n+cuda,timm_regnet,32,1.0644,50.6953,35.7562,1.0000\n+cuda,timm_resnest,32,1.6200,14.7763,17.2245,1.0906\n+cuda,timm_vision_transformer,32,1.0800,19.4188,22.0255,0.9966\n+cuda,timm_vision_transformer_large,32,1.0081,393.1742,127.8083,0.9735\n+cuda,timm_vovnet,32,1.1472,22.4727,22.7328,1.0120\n+cuda,torchrec_dlrm,0,0.0000\n+cuda,tts_angular,64,0.8974,6.5057,2.5555,0.9973\n+cuda,vgg16,64,1.2909,50.7405,6.1510,0.9828\n+cuda,yolov3,16,1.2930,54.8069,41.9269,1.0563"}, {"filename": ".github/workflows/torchinductor/scripts/check_acc.py", "status": "added", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -0,0 +1,11 @@\n+import csv\n+import sys\n+\n+file_path = sys.argv[1]\n+with open(file_path) as f:\n+    reader = csv.reader(f)\n+    for i, row in enumerate(reader):\n+        if i == 0:\n+            continue\n+        if row[3] != \"pass\":\n+            print(f\"{row[1]} failed on device {row[0]} with batch size {row[2]}\")"}, {"filename": ".github/workflows/torchinductor/scripts/check_perf.py", "status": "added", "additions": 65, "deletions": 0, "changes": 65, "file_content_changes": "@@ -0,0 +1,65 @@\n+import argparse\n+import csv\n+from collections import namedtuple\n+\n+# Create a named tuple for the output of the benchmark\n+BenchmarkOutput = namedtuple(\n+    'BenchmarkOutput', ['dev', 'name', 'batch_size', 'speedup', 'latency'])\n+\n+\n+def parse_output(file_path: str) -> dict:\n+    entries = {}\n+    with open(file_path) as f:\n+        reader = csv.reader(f)\n+        for i, row in enumerate(reader):\n+            if i == 0 or len(row) < 5:\n+                continue\n+            dev = row[0]\n+            name = row[1]\n+            batch_size = row[2]\n+            speedup = float(row[3])\n+            latency = float(row[4])\n+            entries[name] = BenchmarkOutput(\n+                dev, name, batch_size, speedup, latency)\n+    return entries\n+\n+\n+def compare(baseline: dict, new: dict, threshold: float,\n+            geomean_threshold: float) -> bool:\n+    baseline_geomean = 1.0\n+    new_geomean = 1.0\n+    for key in new:\n+        if key not in baseline:\n+            print(f\"New benchmark {key} not found in baseline\")\n+        baseline_latency = baseline[key].latency\n+        new_latency = new[key].latency\n+        if new_latency < baseline_latency * (1 - threshold):\n+            print(\n+                f\"New benchmark {key} is faster than baseline: {new_latency} vs {baseline_latency}\")\n+        elif new_latency > baseline_latency * (1 + threshold):\n+            print(\n+                f\"New benchmark {key} is slower than baseline: {new_latency} vs {baseline_latency}\")\n+        baseline_geomean *= baseline[key].speedup\n+        new_geomean *= new[key].speedup\n+\n+    baseline_geomean = baseline_geomean ** (1 / len(baseline))\n+    new_geomean = new_geomean ** (1 / len(new))\n+    print(f\"Baseline geomean: {baseline_geomean}\")\n+    print(f\"New geomean: {new_geomean}\")\n+    assert new_geomean > baseline_geomean * (1 - geomean_threshold), \\\n+        f\"New geomean is slower than baseline: {new_geomean} vs {baseline_geomean}\"\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument('--baseline', required=True)\n+    parser.add_argument('--new', required=True)\n+    parser.add_argument('--threshold', type=float, default=0.1)\n+    parser.add_argument('--geomean-threshold', type=float, default=0.02)\n+    args = parser.parse_args()\n+    baseline = parse_output(args.baseline)\n+    new = parse_output(args.new)\n+    compare(baseline, new, args.threshold, args.geomean_threshold)\n+\n+\n+main()"}, {"filename": ".github/workflows/torchinductor/scripts/common.sh", "status": "added", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -0,0 +1,9 @@\n+#!/bin/bash\n+\n+TEST_REPORTS_DIR=/opt/torchinductor_reports\n+PYTORCH_DIR=/opt/pytorch\n+MODELS=(timm_models huggingface torchbench)\n+\n+echo \"$TEST_REPORTS_DIR\"\n+echo \"$PYTORCH_DIR\"\n+echo \"${MODELS[@]}\""}, {"filename": ".github/workflows/torchinductor/scripts/install_torchinductor.sh", "status": "added", "additions": 54, "deletions": 0, "changes": 54, "file_content_changes": "@@ -0,0 +1,54 @@\n+#!/bin/bash\n+\n+# remember where we started\n+ROOT=\"$(pwd)\"\n+\n+# torchinductor venv\n+whoami\n+python3 -m venv /opt/torchinductor_venv\n+# shellcheck source=/dev/null\n+source /opt/torchinductor_venv/bin/activate\n+# shellcheck source=/dev/null\n+source ./.github/workflows/torchinductor/scripts/common.sh\n+\n+# pytorch nightly\n+pip3 install --force-reinstall --pre torch torchtext torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu118\n+# pytorch source to get torchbench for dynamo\n+cd /opt || exit\n+git clone --recursive https://github.com/pytorch/pytorch\n+cd pytorch || exit\n+# if you are updating an existing checkout\n+git submodule sync\n+git submodule update --init --recursive\n+cd ..\n+\n+# required packages\n+pip3 install expecttest psutil\n+\n+# torchbench\n+pip3 install pyyaml\n+git clone https://github.com/pytorch/benchmark.git\n+cd benchmark || exit\n+python3 install.py\n+cd ..\n+\n+# timm\n+git clone https://github.com/huggingface/pytorch-image-models.git\n+cd pytorch-image-models || exit\n+pip3 install -e .\n+cd ..\n+\n+# build our own triton\n+cd \"$ROOT\" || exit\n+cd python || exit\n+rm -rf build\n+pip3 install -e .\n+pip3 uninstall pytorch-triton -y\n+\n+# clean up cache\n+rm -rf /tmp/torchinductor_root/\n+rm -rf ~/.triton/cache\n+rm -rf \"$TEST_REPORTS_DIR\"\n+\n+# go back to where we started\n+cd \"$ROOT\" || exit"}, {"filename": ".github/workflows/torchinductor/scripts/install_triton.sh", "status": "added", "additions": 24, "deletions": 0, "changes": 24, "file_content_changes": "@@ -0,0 +1,24 @@\n+#!/bin/bash\n+\n+# remember where we started\n+ROOT=\"$(pwd)\"\n+\n+# shellcheck source=/dev/null\n+source /opt/torchinductor_venv/bin/activate\n+# shellcheck source=/dev/null\n+source ./.github/workflows/torchinductor/scripts/common.sh\n+\n+# build our own triton\n+cd python || exit\n+pip3 install --pre pytorch-triton --extra-index-url https://download.pytorch.org/whl/nightly/cu118\n+rm -rf build\n+pip3 install -e .\n+pip3 uninstall pytorch-triton -y\n+\n+# clean up cache\n+rm -rf /tmp/torchinductor_root/\n+rm -rf ~/.triton/cache\n+rm -rf \"$TEST_REPORTS_DIR\"\n+\n+# go back to where we started\n+cd \"$ROOT\" || exit"}, {"filename": ".github/workflows/torchinductor/scripts/run_torchinductor_acc.sh", "status": "added", "additions": 35, "deletions": 0, "changes": 35, "file_content_changes": "@@ -0,0 +1,35 @@\n+#!/bin/bash\n+\n+# remember where we started\n+ROOT=\"$(pwd)\"\n+INDUCTOR=\"$ROOT\"/.github/workflows/torchinductor\n+\n+# shellcheck source=/dev/null\n+source /opt/torchinductor_venv/bin/activate\n+# shellcheck source=/dev/null\n+source \"$INDUCTOR\"/scripts/common.sh\n+\n+cd \"$PYTORCH_DIR\" || exit\n+TEST_REPORTS_DIR=$TEST_REPORTS_DIR/acc\n+mkdir -p \"$TEST_REPORTS_DIR\"\n+\n+for model in \"${MODELS[@]}\"; do\n+  echo \"Running accuracy test for $model\"\n+  python3 benchmarks/dynamo/\"$model\".py --ci --accuracy --timing --explain --inductor --device cuda \\\n+    --output \"$TEST_REPORTS_DIR\"/inference_\"$model\".csv\n+  python3 benchmarks/dynamo/\"$model\".py --ci --accuracy --timing --explain --inductor --training --amp --device cuda \\\n+    --output \"$TEST_REPORTS_DIR\"/training_\"$model\".csv\n+  python3 benchmarks/dynamo/\"$model\".py --ci --accuracy --timing --explain --inductor --dynamic-shapes --device cuda \\\n+    --output \"$TEST_REPORTS_DIR\"/dynamic_shapes_\"$model\".csv\n+done\n+\n+cd \"$ROOT\" || exit\n+for model in \"${MODELS[@]}\"; do\n+  echo \"Checking accuracy test for $model\"\n+  python3 \"$INDUCTOR\"/scripts/check_acc.py \"$TEST_REPORTS_DIR\"/inference_\"$model\".csv\n+  python3 \"$INDUCTOR\"/scripts/check_acc.py \"$TEST_REPORTS_DIR\"/training_\"$model\".csv\n+  python3 \"$INDUCTOR\"/scripts/check_acc.py \"$TEST_REPORTS_DIR\"/dynamic_shapes_\"$model\".csv\n+done\n+\n+# go back to where we started\n+cd \"$ROOT\" || exit"}, {"filename": ".github/workflows/torchinductor/scripts/run_torchinductor_perf.sh", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "file_content_changes": "@@ -0,0 +1,41 @@\n+#!/bin/bash\n+\n+# remember where we started\n+ROOT=\"$(pwd)\"\n+INDUCTOR=\"$ROOT\"/.github/workflows/torchinductor\n+\n+# shellcheck source=/dev/null\n+source /opt/torchinductor_venv/bin/activate\n+# shellcheck source=/dev/null\n+source \"$INDUCTOR\"/scripts/common.sh\n+\n+# lock GPU clocks to 1350 MHz\n+sudo nvidia-smi -i 0 -pm 1\n+sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n+\n+cd \"$PYTORCH_DIR\" || exit\n+TEST_REPORTS_DIR=$TEST_REPORTS_DIR/perf\n+mkdir -p \"$TEST_REPORTS_DIR\"\n+\n+for model in \"${MODELS[@]}\"; do\n+  echo \"Running performance test for $model\"\n+  python3 benchmarks/dynamo/\"$model\".py --ci --training --performance --disable-cudagraphs\\\n+    --device cuda --inductor --amp --output \"$TEST_REPORTS_DIR\"/\"$model\".csv\n+done\n+\n+cd \"$ROOT\" || exit\n+for model in \"${MODELS[@]}\"; do\n+  echo \"Checking performance test for $model\"\n+  python3 \"$INDUCTOR\"/scripts/check_perf.py --new \"$TEST_REPORTS_DIR\"/\"$model\".csv --baseline \"$INDUCTOR\"/data/\"$model\".csv\n+  EXIT_STATUS=$?\n+  if [ \"$EXIT_STATUS\" -ne 0 ]; then\n+    echo \"Performance test for $model failed\"\n+    exit \"$EXIT_STATUS\"\n+  fi\n+done\n+\n+# unlock GPU clocks\n+sudo nvidia-smi -i 0 -rgc\n+\n+# go back to where we started\n+cd \"$ROOT\" || exit"}, {"filename": ".github/workflows/wheels.yml", "status": "modified", "additions": 9, "deletions": 10, "changes": 19, "file_content_changes": "@@ -1,14 +1,14 @@\n name: Wheels\n on:\n   workflow_dispatch:\n-  schedule:    \n-    - cron: \"0 0 * * *\"\n+  #schedule:\n+  #  - cron: \"0 0 * * *\"\n \n jobs:\n \n   Build-Wheels:\n-    \n-    runs-on: self-hosted\n+\n+    runs-on: [self-hosted, V100]\n \n     steps:\n \n@@ -18,23 +18,22 @@ jobs:\n       - name: Patch setup.py\n         run: |\n           #sed -i 's/name\\=\\\"triton\\\"/name=\"triton-nightly\"/g' python/setup.py\n-          export LATEST_DATE=$(git show -s --format=%ci `git rev-parse HEAD` | cut -d ' ' -f 1 | sed 's/-//g')\n+          export LATEST_DATE=$(TZ=UTC0 git show --quiet --date='format-local:%Y%m%d' --format=\"%cd\")\n           sed -i -r \"s/version\\=\\\"(.*)\\\"/version=\\\"\\1-dev\"$LATEST_DATE\"\\\"/g\" python/setup.py\n           echo \"\" >> python/setup.cfg\n           echo \"[build_ext]\" >> python/setup.cfg\n           echo \"base-dir=/project\" >> python/setup.cfg\n \n       - name: Build wheels\n         run: |\n-          export CIBW_MANYLINUX_X86_64_IMAGE=\"manylinux2014\"\n-          export CIBW_MANYLINUX_PYPY_X86_64_IMAGE=\"manylinux2014\"\n-          export CIBW_BEFORE_BUILD=\"pip install cmake;\\\n-                                    yum install -y llvm11 llvm11-devel llvm11-static llvm11-libs zlib-devel;\"\n+          export CIBW_MANYLINUX_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n+          #export CIBW_MANYLINUX_PYPY_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n+          export CIBW_BEFORE_BUILD=\"pip install cmake;\"\n           export CIBW_SKIP=\"{cp,pp}35-*\"\n           export CIBW_BUILD=\"{cp,pp}3*-manylinux_x86_64\"\n           python3 -m cibuildwheel python --output-dir wheelhouse\n \n \n       - name: Upload wheels to PyPI\n         run: |\n-          python3 -m twine upload wheelhouse/* --skip-existing\n\\ No newline at end of file\n+          python3 -m twine upload wheelhouse/* -u __token__ -p ${{ secrets.PYPY_API_TOKEN }}"}, {"filename": ".gitignore", "status": "modified", "additions": 20, "deletions": 3, "changes": 23, "file_content_changes": "@@ -1,9 +1,26 @@\n+# Triton builds\n build/\n \n-__pycache__\n-.pytest_cache\n-\n+# Triton Python module builds\n python/build/\n python/triton.egg-info/\n python/triton/_C/libtriton.pyd\n python/triton/_C/libtriton.so\n+\n+# Python caches\n+__pycache__/\n+*.py[cod]\n+.pytest_cache\n+\n+# Environments\n+.venv\n+venv/\n+venv.bak/\n+\n+# VS Code project files\n+.vscode\n+.vs\n+\n+# JetBrains project files\n+.idea\n+cmake-build-*"}, {"filename": ".pre-commit-config.yaml", "status": "added", "additions": 61, "deletions": 0, "changes": 61, "file_content_changes": "@@ -0,0 +1,61 @@\n+repos:\n+  - repo: https://github.com/pre-commit/pre-commit-hooks\n+    rev: v4.4.0\n+    hooks:\n+      - id: check-symlinks\n+      - id: destroyed-symlinks\n+      - id: trailing-whitespace\n+      - id: end-of-file-fixer\n+      - id: check-yaml\n+      - id: check-toml\n+      - id: check-ast\n+      - id: check-added-large-files\n+      - id: check-merge-conflict\n+      - id: check-executables-have-shebangs\n+      - id: check-shebang-scripts-are-executable\n+      - id: detect-private-key\n+      - id: debug-statements\n+  - repo: https://github.com/PyCQA/isort\n+    rev: 5.12.0\n+    hooks:\n+      - id: isort\n+        stages: [commit, push, manual]\n+  - repo: https://github.com/pre-commit/mirrors-autopep8\n+    rev: v1.6.0\n+    hooks:\n+      - id: autopep8\n+        args: [\"-i\"]\n+        stages: [commit, push, manual]\n+  - repo: https://github.com/pycqa/flake8\n+    rev: 6.0.0\n+    hooks:\n+      - id: flake8\n+        # TODO: uncomment this to enable more flake8 plugins\n+        # additional_dependencies:\n+        #   - flake8-bugbear\n+        #   - flake8-comprehensions\n+        #   - flake8-docstrings\n+        #   - flake8-pyi\n+        #   - flake8-simplify\n+        stages: [commit, push, manual]\n+        exclude: |\n+          (?x)(\n+            ^test/|\n+            ^docs/conf.py$\n+          )\n+  - repo: https://github.com/pre-commit/mirrors-clang-format\n+    rev: v14.0.6\n+    hooks:\n+      - id: clang-format\n+        stages: [commit, push, manual]\n+        exclude: |\n+          (?x)(\n+            ^include/triton/external/|\n+            ^python/triton/third_party/\n+          )\n+\n+exclude: |\n+  (?x)(\n+    ^include/triton/external/|\n+    ^python/triton/third_party/\n+  )"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 211, "deletions": 109, "changes": 320, "file_content_changes": "@@ -1,22 +1,34 @@\n-cmake_minimum_required(VERSION 3.6)\n+cmake_minimum_required(VERSION 3.18)\n+\n+if(POLICY CMP0116)\n+# Introduced in cmake 3.20\n+# https://cmake.org/cmake/help/latest/policy/CMP0116.html\n+  cmake_policy(SET CMP0116 OLD)\n+endif()\n+\n include(ExternalProject)\n \n set(CMAKE_CXX_STANDARD 17)\n \n-if(NOT TRITON_LLVM_BUILD_DIR)\n-    set(TRITON_LLVM_BUILD_DIR ${CMAKE_BINARY_DIR})\n-endif()\n-\n+set(CMAKE_INCLUDE_CURRENT_DIR ON)\n \n project(triton)\n include(CTest)\n+\n if(NOT WIN32)\n   list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_SOURCE_DIR}/cmake\")\n endif()\n \n # Options\n-option(BUILD_TUTORIALS \"Build C++ Triton tutorials\" ON)\n-option(BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n+option(TRITON_BUILD_TUTORIALS \"Build C++ Triton tutorials\" ON)\n+option(TRITON_BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n+\n+# Ensure Python3 vars are set correctly\n+# used conditionally in this file and by lit tests\n+\n+# Customized release build type with assertions: TritonRelBuildWithAsserts\n+set(CMAKE_C_FLAGS_TRITONRELBUILDWITHASSERTS \"-O2 -g\")\n+set(CMAKE_CXX_FLAGS_TRITONRELBUILDWITHASSERTS \"-O2 -g\")\n \n # Default build type\n if(NOT CMAKE_BUILD_TYPE)\n@@ -25,25 +37,32 @@ if(NOT CMAKE_BUILD_TYPE)\n endif()\n \n if(NOT WIN32)\n-    find_library(TERMINFO_LIBRARY tinfo)\n+  find_library(TERMINFO_LIBRARY tinfo)\n endif()\n \n # Compiler flags\n include_directories(${CMAKE_CURRENT_SOURCE_DIR}/include)\n \n+# Third-party\n+include_directories(${PYBIND11_INCLUDE_DIR})\n+\n if(WIN32)\n-    SET(BUILD_SHARED_LIBS OFF)\n-    include_directories(${CMAKE_CURRENT_SOURCE_DIR}/deps/dlfcn-win32/src)\n-    add_subdirectory(deps/dlfcn-win32/src ${CMAKE_BINARY_DIR}/dlfcn-win32)\n+  SET(BUILD_SHARED_LIBS OFF)\n+  find_package(dlfcn-win32 REQUIRED)\n+  set(CMAKE_DL_LIBS dlfcn-win32::dl)\n endif()\n \n-set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -D__STDC_FORMAT_MACROS  -std=gnu++17\")\n+set(CMAKE_CXX_FLAGS \"${CMAKE_C_FLAGS} -D__STDC_FORMAT_MACROS  -fPIC -std=gnu++17 -fvisibility=hidden -fvisibility-inlines-hidden\")\n \n+if(APPLE)\n+  set(CMAKE_OSX_DEPLOYMENT_TARGET 11.6)\n+endif()\n \n-##########\n+# #########\n # LLVM\n-##########\n-if(\"${LLVM_LIBRARY_DIR}\" STREQUAL \"\")\n+# #########\n+if(NOT MLIR_DIR)\n+  if(NOT LLVM_LIBRARY_DIR)\n     if(WIN32)\n       find_package(LLVM 13 REQUIRED COMPONENTS nvptx amdgpu)\n \n@@ -58,114 +77,197 @@ if(\"${LLVM_LIBRARY_DIR}\" STREQUAL \"\")\n     else()\n       find_package(LLVM 11 REQUIRED COMPONENTS \"nvptx;amdgpu\")\n     endif()\n+\n     message(STATUS \"Found LLVM ${LLVM_PACKAGE_VERSION}\")\n+\n+    # FindLLVM outputs LLVM_LIBRARY_DIRS but we expect LLVM_LIBRARY_DIR here\n+    set(LLVM_LIBRARY_DIR ${LLVM_LIBRARY_DIRS})\n+\n     if(APPLE)\n       set(CMAKE_OSX_DEPLOYMENT_TARGET \"10.14\")\n     endif()\n-# sometimes we don't want to use llvm-config, since it may have been downloaded for some specific linux distros\n-else()\n+\n+  # sometimes we don't want to use llvm-config, since it may have been downloaded for some specific linux distros\n+  else()\n     set(LLVM_LDFLAGS \"-L${LLVM_LIBRARY_DIR}\")\n-    set(LLVM_LIBRARIES \n-libLLVMNVPTXCodeGen.a\n-libLLVMNVPTXDesc.a\n-libLLVMNVPTXInfo.a\n-libLLVMAMDGPUDisassembler.a\n-libLLVMMCDisassembler.a\n-libLLVMAMDGPUCodeGen.a\n-libLLVMMIRParser.a\n-libLLVMGlobalISel.a\n-libLLVMSelectionDAG.a\n-libLLVMipo.a\n-libLLVMInstrumentation.a\n-libLLVMVectorize.a\n-libLLVMLinker.a\n-libLLVMIRReader.a\n-libLLVMAsmParser.a\n-libLLVMFrontendOpenMP.a\n-libLLVMAsmPrinter.a\n-libLLVMDebugInfoDWARF.a\n-libLLVMCodeGen.a\n-libLLVMTarget.a\n-libLLVMScalarOpts.a\n-libLLVMInstCombine.a\n-libLLVMAggressiveInstCombine.a\n-libLLVMTransformUtils.a\n-libLLVMBitWriter.a\n-libLLVMAnalysis.a\n-libLLVMProfileData.a\n-libLLVMObject.a\n-libLLVMTextAPI.a\n-libLLVMBitReader.a\n-libLLVMAMDGPUAsmParser.a\n-libLLVMMCParser.a\n-libLLVMAMDGPUDesc.a\n-libLLVMAMDGPUUtils.a\n-libLLVMMC.a\n-libLLVMDebugInfoCodeView.a\n-libLLVMDebugInfoMSF.a\n-libLLVMCore.a\n-libLLVMRemarks.a\n-libLLVMBitstreamReader.a\n-libLLVMBinaryFormat.a\n-libLLVMAMDGPUInfo.a\n-libLLVMSupport.a\n-libLLVMDemangle.a\n-libLLVMPasses.a\n-libLLVMAnalysis.a\n-libLLVMTransformUtils.a\n-libLLVMScalarOpts.a\n-libLLVMTransformUtils.a\n-libLLVMipo.a\n-libLLVMObjCARCOpts.a\n-libLLVMCoroutines.a\n-libLLVMAnalysis.a\n-)\n+    set(LLVM_LIBRARIES\n+      libLLVMNVPTXCodeGen.a\n+      libLLVMNVPTXDesc.a\n+      libLLVMNVPTXInfo.a\n+      libLLVMAMDGPUDisassembler.a\n+      libLLVMMCDisassembler.a\n+      libLLVMAMDGPUCodeGen.a\n+      libLLVMMIRParser.a\n+      libLLVMGlobalISel.a\n+      libLLVMSelectionDAG.a\n+      libLLVMipo.a\n+      libLLVMInstrumentation.a\n+      libLLVMVectorize.a\n+      libLLVMLinker.a\n+      libLLVMIRReader.a\n+      libLLVMAsmParser.a\n+      libLLVMFrontendOpenMP.a\n+      libLLVMAsmPrinter.a\n+      libLLVMDebugInfoDWARF.a\n+      libLLVMCodeGen.a\n+      libLLVMTarget.a\n+      libLLVMScalarOpts.a\n+      libLLVMInstCombine.a\n+      libLLVMAggressiveInstCombine.a\n+      libLLVMTransformUtils.a\n+      libLLVMBitWriter.a\n+      libLLVMAnalysis.a\n+      libLLVMProfileData.a\n+      libLLVMObject.a\n+      libLLVMTextAPI.a\n+      libLLVMBitReader.a\n+      libLLVMAMDGPUAsmParser.a\n+      libLLVMMCParser.a\n+      libLLVMAMDGPUDesc.a\n+      libLLVMAMDGPUUtils.a\n+      libLLVMMC.a\n+      libLLVMDebugInfoCodeView.a\n+      libLLVMDebugInfoMSF.a\n+      libLLVMCore.a\n+      libLLVMRemarks.a\n+      libLLVMBitstreamReader.a\n+      libLLVMBinaryFormat.a\n+      libLLVMAMDGPUInfo.a\n+      libLLVMSupport.a\n+      libLLVMDemangle.a\n+      libLLVMPasses.a\n+      libLLVMAnalysis.a\n+      libLLVMTransformUtils.a\n+      libLLVMScalarOpts.a\n+      libLLVMTransformUtils.a\n+      libLLVMipo.a\n+      libLLVMObjCARCOpts.a\n+      libLLVMCoroutines.a\n+      libLLVMAnalysis.a\n+    )\n+  endif()\n+\n+  set(MLIR_DIR ${LLVM_LIBRARY_DIR}/cmake/mlir)\n endif()\n-include_directories(\"${LLVM_INCLUDE_DIRS}\")\n \n # Python module\n-if(BUILD_PYTHON_MODULE)\n-    message(STATUS \"Adding Python module\")\n-    # Build CUTLASS python wrapper if requested\n-    set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n-    set(CUTLASS_INCLUDE_DIR \"$ENV{CUTLASS_INCLUDE_DIR}\")\n-    set(CUTLASS_LIBRARY_DIR \"$ENV{CUTLASS_LIBRARY_DIR}\")\n-    if(NOT(\"${CUTLASS_INCLUDE_DIR}\" STREQUAL \"\") AND NOT(\"${CUTLASS_LIBRARY_DIR}\" STREQUAL \"\"))\n-        set(CUTLASS_SRC ${PYTHON_SRC_PATH}/cutlass.cc)\n-        add_definitions(-DWITH_CUTLASS_BINDINGS)\n-        set(CUTLASS_LIBRARIES \"cutlass.a\")\n-    endif()\n-    include_directories(\".\" ${PYTHON_SRC_PATH} ${PYTHON_INCLUDE_DIRS} ${CUTLASS_INCLUDE_DIR})\n-    link_directories(${PYTHON_LINK_DIRS} ${CUTLASS_LIBRARY_DIR})\n-    set(PYTHON_SRC ${PYTHON_SRC_PATH}/main.cc ${PYTHON_SRC_PATH}/triton.cc  ${PYTHON_SRC_PATH}/superblock.cc ${CUTLASS_SRC})\n+if(TRITON_BUILD_PYTHON_MODULE)\n+  message(STATUS \"Adding Python module\")\n+  set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n+  set(PYTHON_SRC ${PYTHON_SRC_PATH}/main.cc ${PYTHON_SRC_PATH}/triton.cc)\n+  include_directories(\".\" ${PYTHON_SRC_PATH})\n+\n+  if(PYTHON_INCLUDE_DIRS)\n+    include_directories(${PYTHON_INCLUDE_DIRS})\n+  else()\n+    find_package(Python3 REQUIRED COMPONENTS Development Interpreter)\n+    include_directories(${Python3_INCLUDE_DIRS})\n+    link_directories(${Python3_LIBRARY_DIRS})\n+    link_libraries(${Python3_LIBRARIES})\n+    add_link_options(${Python3_LINK_OPTIONS})\n+  endif()\n endif()\n \n+# # Triton\n+# file(GLOB_RECURSE LIBTRITON_SRC lib/*.cc)\n+# if (WIN32 AND TRITON_BUILD_PYTHON_MODULE)\n+# Python3_add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n+# set_target_properties(triton PROPERTIES SUFFIX \".pyd\")\n+# set_target_properties(triton PROPERTIES PREFIX \"lib\")\n+# else()\n+# add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n+# endif()\n \n-# Triton\n-file(GLOB_RECURSE LIBTRITON_SRC lib/*.cc)\n-if (WIN32 AND BUILD_PYTHON_MODULE)\n-    find_package(Python3 REQUIRED COMPONENTS Development)\n-    Python3_add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n-    set_target_properties(triton PROPERTIES SUFFIX \".pyd\")\n-    set_target_properties(triton PROPERTIES PREFIX \"lib\")\n-else()\n-    add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n-endif()\n+# MLIR\n+find_package(MLIR REQUIRED CONFIG PATHS ${MLIR_DIR})\n \n-target_link_options(triton PRIVATE ${LLVM_LDFLAGS})\n+list(APPEND CMAKE_MODULE_PATH \"${MLIR_CMAKE_DIR}\")\n+list(APPEND CMAKE_MODULE_PATH \"${LLVM_CMAKE_DIR}\")\n \n-if(WIN32)\n-    target_link_libraries(triton PRIVATE ${LLVM_LIBRARIES} dl) # dl is from dlfcn-win32\n-else()\n-    target_link_libraries(triton ${LLVM_LIBRARIES} z)\n+include(TableGen) # required by AddMLIR\n+include(AddLLVM)\n+include(AddMLIR)\n+\n+# Disable warnings that show up in external code (gtest;pybind11)\n+set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror -Wno-covered-switch-default\")\n+\n+include_directories(${MLIR_INCLUDE_DIRS})\n+include_directories(${LLVM_INCLUDE_DIRS})\n+include_directories(${PROJECT_SOURCE_DIR}/include)\n+include_directories(${PROJECT_BINARY_DIR}/include) # Tablegen'd files\n+\n+# link_directories(${LLVM_LIBRARY_DIR})\n+add_subdirectory(include)\n+add_subdirectory(lib)\n+add_subdirectory(bin)\n+\n+# find_package(PythonLibs REQUIRED)\n+set(TRITON_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}\")\n+set(TRITON_BINARY_DIR \"${CMAKE_CURRENT_BINARY_DIR}\")\n+\n+get_property(dialect_libs GLOBAL PROPERTY MLIR_DIALECT_LIBS)\n+get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)\n+\n+if(TRITON_BUILD_PYTHON_MODULE)\n+  add_library(triton SHARED ${PYTHON_SRC})\n+  set(TRITON_LIBRARIES\n+    TritonAnalysis\n+    TritonTransforms\n+    TritonGPUTransforms\n+    TritonLLVMIR\n+    TritonPTX\n+    TritonHSACO\n+    ${dialect_libs}\n+    ${conversion_libs}\n+\n+    # optimizations\n+    MLIRPass\n+    MLIRTransforms\n+    MLIRLLVMDialect\n+    MLIRSupport\n+    MLIRTargetLLVMIRExport\n+    MLIRExecutionEngine\n+    MLIRMathToLLVM\n+    MLIRNVVMToLLVMIRTranslation\n+    MLIRROCDLToLLVMIRTranslation\n+    MLIRIR\n+  )\n+\n+  if(WIN32)\n+    target_link_libraries(triton PRIVATE ${LLVM_LIBRARIES} ${CMAKE_DL_LIBS}\n+      ${TRITON_LIBRARIES}\n+    )\n+  elseif(APPLE)\n+    target_link_libraries(triton ${LLVM_LIBRARIES} z\n+      ${TRITON_LIBRARIES}\n+    )\n+  else()\n+    target_link_libraries(triton ${LLVM_LIBRARIES} z\n+      ${TRITON_LIBRARIES}\n+    )\n+    # TODO: Figure out which target is sufficient to fix errors; triton is\n+    # apparently not enough\n+    link_libraries(stdc++fs)\n+  endif()\n+\n+  target_link_options(triton PRIVATE ${LLVM_LDFLAGS})\n endif()\n \n+if(UNIX AND NOT APPLE)\n+  set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -Wl,--exclude-libs,ALL\")\n+endif()\n \n-if(BUILD_PYTHON_MODULE AND NOT WIN32)\n-    set(CMAKE_SHARED_LIBRARY_SUFFIX \".so\")\n-    # Check if the platform is MacOS\n-    if(APPLE)\n-        set(PYTHON_LDFLAGS \"-undefined dynamic_lookup -flto\")\n-    endif()\n-    target_link_libraries(triton ${CUTLASS_LIBRARIES} ${PYTHON_LDFLAGS})\n+if(TRITON_BUILD_PYTHON_MODULE AND NOT WIN32)\n+  set(CMAKE_SHARED_LIBRARY_SUFFIX \".so\")\n+\n+  # Check if the platform is MacOS\n+  if(APPLE)\n+    set(PYTHON_LDFLAGS \"-undefined dynamic_lookup -flto\")\n+  endif()\n+\n+  target_link_libraries(triton ${CUTLASS_LIBRARIES} ${PYTHON_LDFLAGS})\n endif()\n+\n+add_subdirectory(test)\n+\n+add_subdirectory(unittest)"}, {"filename": "CONTRIBUTING.md", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -0,0 +1,53 @@\n+# Triton Programming Language Contributor's Guide\n+\n+First of all, thank you for considering contributing to the Triton programming language! We appreciate the time and effort you're willing to put into improving and expanding our project. In order to maintain a high standard of code and a welcoming atmosphere for collaboration, we kindly ask you to follow the guidelines outlined below.\n+\n+## General Guidelines\n+\n+1. **Quality Contributions:** We value meaningful contributions that aim to improve the project and help it grow. Please refrain from submitting low-effort pull requests (PR) -- such as minor formatting/typo fixes -- solely for the purpose of appearing in the commit history. Maintainers have limited bandwidth, and may decline to review such work.\n+\n+2. **Code Formatting:** Our Continuous Integration (CI) pipeline uses autopep8, isort, and clang-format to check code formatting. To avoid failing the CI workflow due to formatting issues, please utilize the provided `.pre-commit-config.yaml` pre-commit configuration file.\n+\n+3. **Unit Testing:** When contributing new functionalities, please also include appropriate tests. We aim to continuously improve and expand our CI pipeline to ensure the robustness and reliability of the project. PRs that add a large amount of untested code will be rejected.\n+\n+4. **Respectful Communication:** In all discussions related to PRs or other contributions, please maintain a courteous and civil tone. We strive to foster a collaborative environment that is inclusive and respectful to all contributors.\n+\n+\n+## Request for Comments (RFCs)\n+\n+RFCs are a crucial aspect of the collaborative development process, as they provide a structured way to propose and discuss significant changes or additions to the project. RFCs may encompass modifications to the language itself, extensive changes in the compiler backend, or other substantial updates that impact the Triton ecosystem.\n+\n+To ensure that RFCs are clear and easy to understand, consider the following guidelines when creating one:\n+\n+### Purpose\n+\n+The purpose of an RFC is to:\n+\n+- Clearly communicate your proposal to the Triton community\n+- Collect feedback from maintainers and other contributors\n+- Provide a platform for discussing and refining ideas\n+- Reach a consensus on the best approach for implementing the proposed changes\n+\n+### Structure\n+\n+A well-structured RFC should include:\n+\n+1. **Title:** A concise and descriptive title that reflects the main topic of the proposal.\n+\n+2. **Summary:** A brief overview of the proposed changes, including the motivation behind them and their intended impact on the project.\n+\n+3. **Detailed Design:** A thorough description of the proposed changes, including:\n+   - Technical details and implementation approach\n+   - Any new or modified components, functions, or data structures\n+   - Any potential challenges or limitations, as well as proposed solutions\n+\n+4. **Examples and Use Cases:** Provide examples of how the proposed changes would be used in real-world scenarios, as well as any use cases that demonstrate the benefits of the changes.\n+\n+5. **Performance Impact:** Discuss the expected performance impact of the proposed changes, including any potential bottlenecks or performance improvements.\n+\n+6. **Timeline and Milestones:** Outline a proposed timeline for implementing the changes, including any milestones or intermediate steps.\n+\n+\n+## New backends\n+\n+Due to limited resources, we need to prioritize the number of targets we support. We are committed to providing upstream support for Nvidia and AMD GPUs. However, if you wish to contribute support for other backends, please start your project in a fork. If your backend proves to be useful and meets our performance requirements, we will discuss the possibility of upstreaming it."}, {"filename": "LICENSE", "status": "modified", "additions": 17, "deletions": 17, "changes": 34, "file_content_changes": "@@ -1,23 +1,23 @@\n-/* \n+/*\n * Copyright 2018-2020 Philippe Tillet\n-* Copyright 2020-2021 OpenAI\n-* \n-* Permission is hereby granted, free of charge, to any person obtaining \n-* a copy of this software and associated documentation files \n-* (the \"Software\"), to deal in the Software without restriction, \n-* including without limitation the rights to use, copy, modify, merge, \n-* publish, distribute, sublicense, and/or sell copies of the Software, \n-* and to permit persons to whom the Software is furnished to do so, \n+* Copyright 2020-2022 OpenAI\n+*\n+* Permission is hereby granted, free of charge, to any person obtaining\n+* a copy of this software and associated documentation files\n+* (the \"Software\"), to deal in the Software without restriction,\n+* including without limitation the rights to use, copy, modify, merge,\n+* publish, distribute, sublicense, and/or sell copies of the Software,\n+* and to permit persons to whom the Software is furnished to do so,\n * subject to the following conditions:\n-* \n-* The above copyright notice and this permission notice shall be \n+*\n+* The above copyright notice and this permission notice shall be\n * included in all copies or substantial portions of the Software.\n-* \n-* THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, \n-* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF \n+*\n+* THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n-* CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, \n-* TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE \n+* CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+* TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n-*/\n\\ No newline at end of file\n+*/"}, {"filename": "README.md", "status": "modified", "additions": 17, "deletions": 14, "changes": 31, "file_content_changes": "@@ -2,7 +2,7 @@\n   <img src=\"https://cdn.openai.com/triton/assets/triton-logo.png\" alt=\"Triton logo\" width=\"88\" height=\"100\">\n </div>\n \n-[![Wheels](https://github.com/openai/triton/actions/workflows/wheels.yml/badge.svg)](https://github.com/openai/triton/actions/workflows/wheels.yml)\n+[![Wheels](https://github.com/openai/triton/actions/workflows/wheels.yml/badge.svg?branch=release/2.0.x)](https://github.com/openai/triton/actions/workflows/wheels.yml)\n \n \n **`Documentation`** |\n@@ -25,30 +25,37 @@ You can install the latest stable release of Triton from pip:\n ```bash\n pip install triton\n ```\n-Binary wheels are available for CPython 3.6-3.9 and PyPy 3.6-3.7.\n+Binary wheels are available for CPython 3.6-3.11 and PyPy 3.7-3.9.\n \n And the latest nightly release:\n \n ```bash\n pip install -U --pre triton\n ```\n \n+# Install from source\n+\n+```\n+git clone https://github.com/openai/triton.git;\n+cd triton/python;\n+pip install cmake; # build-time dependency\n+pip install -e .\n+```\n+\n # Changelog\n \n-Version 1.1 is out! New features include:\n-- Many, many bugfixes\n-- More documentation\n-- Automatic on-disk caching of compiled binary objects\n-- Random Number Generation\n-- Faster (up to 2x on A100), cleaner blocksparse ops\n+Version 2.0 is out! New features include:\n+- Many, many bug fixes\n+- Performance improvements\n+- Backend rewritten to use MLIR\n+- Support for kernels that contain back-to-back matmuls (e.g., flash attention)\n \n # Contributing\n \n-Community contributions are more than welcome, whether it be to fix bugs or to add new features. Feel free to open GitHub issues about your contribution ideas, and we will review them. A contributor's guide containing general guidelines is coming soon!\n+Community contributions are more than welcome, whether it be to fix bugs or to add new features at [github](https://github.com/openai/triton/). For more detailed instructions, please visit our [contributor's guide](CONTRIBUTING.md).\n \n If you\u2019re interested in joining our team and working on Triton & GPU kernels, [we\u2019re hiring](https://openai.com/jobs/#acceleration)!\n \n-\n # Compatibility\n \n Supported Platforms:\n@@ -57,7 +64,3 @@ Supported Platforms:\n Supported Hardware:\n   * NVIDIA GPUs (Compute Capability 7.0+)\n   * Under development: AMD GPUs, CPUs\n-\n-# Disclaimer\n-\n-Triton is a fairly recent project, and it is under active development. We expect it to be pretty useful in a wide variety of cases, but don't be surprised if it's a bit rough around the edges :)"}, {"filename": "bin/CMakeLists.txt", "status": "added", "additions": 79, "deletions": 0, "changes": 79, "file_content_changes": "@@ -0,0 +1,79 @@\n+get_property(dialect_libs GLOBAL PROPERTY MLIR_DIALECT_LIBS)\n+get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)\n+\n+add_llvm_executable(triton-opt triton-opt.cpp PARTIAL_SOURCES_INTENDED)\n+\n+# TODO: what's this?\n+llvm_update_compile_flags(triton-opt)\n+target_link_libraries(triton-opt PRIVATE\n+  TritonAnalysis\n+  TritonTransforms\n+  TritonGPUTransforms\n+  ${dialect_libs}\n+  ${conversion_libs}\n+  # tests\n+  TritonTestAnalysis\n+  # MLIR core\n+  MLIROptLib\n+  MLIRPass\n+  MLIRTransforms\n+)\n+\n+mlir_check_all_link_libraries(triton-opt)\n+\n+add_llvm_executable(triton-reduce triton-reduce.cpp PARTIAL_SOURCES_INTENDED)\n+mlir_check_all_link_libraries(triton-reduce)\n+\n+llvm_update_compile_flags(triton-reduce)\n+target_link_libraries(triton-reduce PRIVATE\n+  TritonAnalysis\n+  TritonTransforms\n+  TritonGPUTransforms\n+  ${dialect_libs}\n+  ${conversion_libs}\n+  # tests\n+  TritonTestAnalysis\n+  # MLIR core\n+  MLIRReduceLib\n+  MLIRPass\n+  MLIRTransforms\n+)\n+\n+mlir_check_all_link_libraries(triton-reduce)\n+\n+\n+add_llvm_executable(triton-translate triton-translate.cpp PARTIAL_SOURCES_INTENDED)\n+llvm_update_compile_flags(triton-translate)\n+ target_link_libraries(triton-translate PRIVATE\n+         TritonAnalysis\n+         TritonTransforms\n+         TritonGPUTransforms\n+         TritonLLVMIR\n+         TritonPTX\n+         TritonHSACO\n+         ${dialect_libs}\n+         ${conversion_libs}\n+         # tests\n+         TritonTestAnalysis\n+\n+         LLVMCore\n+         LLVMSupport\n+         LLVMOption\n+         LLVMCodeGen\n+         LLVMAsmParser\n+\n+         # MLIR core\n+         MLIROptLib\n+         MLIRIR\n+         MLIRLLVMDialect\n+         MLIRPass\n+         MLIRSupport\n+         MLIRTransforms\n+         MLIRExecutionEngine\n+         MLIRMathToLLVM\n+         MLIRTransformUtils\n+         MLIRLLVMToLLVMIRTranslation\n+         MLIRNVVMToLLVMIRTranslation\n+         MLIRROCDLToLLVMIRTranslation\n+         )\n+mlir_check_all_link_libraries(triton-translate)"}, {"filename": "bin/RegisterTritonDialects.h", "status": "added", "additions": 38, "deletions": 0, "changes": 38, "file_content_changes": "@@ -0,0 +1,38 @@\n+#pragma once\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+\n+#include \"triton/Dialect/Triton/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+\n+#include \"triton/Conversion/TritonGPUToLLVM/Passes.h\"\n+#include \"triton/Conversion/TritonToTritonGPU/Passes.h\"\n+\n+#include \"mlir/InitAllPasses.h\"\n+\n+namespace mlir {\n+namespace test {\n+void registerTestAliasPass();\n+void registerTestAlignmentPass();\n+void registerTestAllocationPass();\n+void registerTestMembarPass();\n+} // namespace test\n+} // namespace mlir\n+\n+inline void registerTritonDialects(mlir::DialectRegistry &registry) {\n+  mlir::registerAllPasses();\n+  mlir::registerTritonPasses();\n+  mlir::registerTritonGPUPasses();\n+  mlir::test::registerTestAliasPass();\n+  mlir::test::registerTestAlignmentPass();\n+  mlir::test::registerTestAllocationPass();\n+  mlir::test::registerTestMembarPass();\n+  mlir::triton::registerConvertTritonToTritonGPUPass();\n+  mlir::triton::registerConvertTritonGPUToLLVMPass();\n+\n+  // TODO: register Triton & TritonGPU passes\n+  registry.insert<mlir::triton::TritonDialect, mlir::cf::ControlFlowDialect,\n+                  mlir::triton::gpu::TritonGPUDialect, mlir::math::MathDialect,\n+                  mlir::arith::ArithDialect, mlir::scf::SCFDialect,\n+                  mlir::gpu::GPUDialect>();\n+}"}, {"filename": "bin/triton-opt.cpp", "status": "added", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -0,0 +1,11 @@\n+#include \"./RegisterTritonDialects.h\"\n+\n+#include \"mlir/Tools/mlir-opt/MlirOptMain.h\"\n+\n+int main(int argc, char **argv) {\n+  mlir::DialectRegistry registry;\n+  registerTritonDialects(registry);\n+\n+  return mlir::asMainReturnCode(mlir::MlirOptMain(\n+      argc, argv, \"Triton (GPU) optimizer driver\\n\", registry));\n+}"}, {"filename": "bin/triton-reduce.cpp", "status": "added", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -0,0 +1,11 @@\n+#include \"./RegisterTritonDialects.h\"\n+\n+#include \"mlir/Tools/mlir-reduce/MlirReduceMain.h\"\n+\n+int main(int argc, char **argv) {\n+  mlir::DialectRegistry registry;\n+  registerTritonDialects(registry);\n+\n+  mlir::MLIRContext context(registry);\n+  return mlir::failed(mlir::mlirReduceMain(argc, argv, context));\n+}"}, {"filename": "bin/triton-translate.cpp", "status": "added", "additions": 154, "deletions": 0, "changes": 154, "file_content_changes": "@@ -0,0 +1,154 @@\n+#include \"mlir/ExecutionEngine/ExecutionEngine.h\"\n+#include \"mlir/ExecutionEngine/OptUtils.h\"\n+#include \"mlir/IR/AsmState.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/Dialect.h\"\n+#include \"mlir/Parser/Parser.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Pass/PassManager.h\"\n+#include \"mlir/Support/FileUtilities.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Target/LLVMIR/Dialect/LLVMIR/LLVMToLLVMIRTranslation.h\"\n+#include \"mlir/Target/LLVMIR/Export.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n+#include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Target/HSACO/HSACOTranslation.h\"\n+#include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n+#include \"triton/Target/PTX/PTXTranslation.h\"\n+#include \"llvm/IR/LLVMContext.h\"\n+#include \"llvm/Support/CommandLine.h\"\n+#include \"llvm/Support/InitLLVM.h\"\n+#include \"llvm/Support/SourceMgr.h\"\n+#include \"llvm/Support/ToolOutputFile.h\"\n+\n+namespace mlir {\n+namespace triton {\n+\n+OwningOpRef<ModuleOp> loadMLIRModule(llvm::StringRef inputFilename,\n+                                     MLIRContext &context) {\n+  std::string errorMessage;\n+  auto input = openInputFile(inputFilename, &errorMessage);\n+  if (!input) {\n+    llvm::errs() << errorMessage << \"\\n\";\n+    return nullptr;\n+  }\n+\n+  mlir::DialectRegistry registry;\n+  registry\n+      .insert<TritonDialect, triton::gpu::TritonGPUDialect,\n+              mlir::math::MathDialect, arith::ArithDialect, scf::SCFDialect>();\n+\n+  context.appendDialectRegistry(registry);\n+\n+  auto processBuffer = [&](std::unique_ptr<llvm::MemoryBuffer> ownedBuffer)\n+      -> OwningOpRef<ModuleOp> {\n+    llvm::SourceMgr sourceMgr;\n+    sourceMgr.AddNewSourceBuffer(std::move(ownedBuffer), SMLoc());\n+\n+    context.loadAllAvailableDialects();\n+    context.allowUnregisteredDialects();\n+\n+    OwningOpRef<ModuleOp> module =\n+        parseSourceFile<ModuleOp>(sourceMgr, &context);\n+    if (!module) {\n+      llvm::errs() << \"Parse MLIR file failed.\";\n+      return nullptr;\n+    }\n+\n+    return module;\n+  };\n+\n+  auto module = processBuffer(std::move(input));\n+  if (!module) {\n+    return nullptr;\n+  }\n+\n+  return module;\n+}\n+\n+LogicalResult tritonTranslateMain(int argc, char **argv,\n+                                  llvm::StringRef toolName) {\n+  static llvm::cl::opt<std::string> inputFilename(\n+      llvm::cl::Positional, llvm::cl::desc(\"<input file>\"),\n+      llvm::cl::init(\"-\"));\n+\n+  static llvm::cl::opt<std::string> outputFilename(\n+      \"o\", llvm::cl::desc(\"Output filename\"), llvm::cl::value_desc(\"filename\"),\n+      llvm::cl::init(\"-\"));\n+\n+  static llvm::cl::opt<std::string> targetKind(\n+      \"target\",\n+      llvm::cl::desc(\"<translation target, options: llvmir/ptx/hsaco>\"),\n+      llvm::cl::value_desc(\"target\"), llvm::cl::init(\"llvmir\"));\n+\n+  static llvm::cl::opt<int> SMArch(\"sm\", llvm::cl::desc(\"sm arch\"),\n+                                   llvm::cl::init(80));\n+\n+  static llvm::cl::opt<int> ptxVersion(\n+      \"ptx-version\", llvm::cl::desc(\"PTX version\"), llvm::cl::init(10000));\n+\n+  static llvm::cl::opt<std::string> GCNArch(\n+      \"gfx\", llvm::cl::desc(\"AMDGCN target. e.g. '90a'\"),\n+      llvm::cl::value_desc(\"architecture\"), llvm::cl::init(\"90a\"));\n+\n+  static llvm::cl::opt<std::string> GCNTriple(\n+      \"amdgcn\", llvm::cl::desc(\"AMDGCN triple. e.g. '-amd-amdhsa'\"),\n+      llvm::cl::value_desc(\"target triple\"), llvm::cl::init(\"-amd-amdhsa\"));\n+\n+  static llvm::cl::opt<std::string> GCNFeatures(\n+      \"\", llvm::cl::desc(\"AMDGCN features. e.g. '+sramecc,-xnack'\"),\n+      llvm::cl::value_desc(\"features\"), llvm::cl::init(\"+sramecc,-xnack\"));\n+\n+  llvm::InitLLVM y(argc, argv);\n+\n+  registerAsmPrinterCLOptions();\n+  registerMLIRContextCLOptions();\n+  llvm::cl::ParseCommandLineOptions(argc, argv, toolName);\n+\n+  mlir::MLIRContext context;\n+  auto module = loadMLIRModule(inputFilename, context);\n+  if (!module) {\n+    return failure();\n+  }\n+\n+  std::string errorMessage;\n+  auto output = openOutputFile(outputFilename, &errorMessage);\n+  if (!output) {\n+    llvm::errs() << errorMessage << \"\\n\";\n+    return failure();\n+  }\n+\n+  llvm::LLVMContext llvmContext;\n+  auto llvmir = translateTritonGPUToLLVMIR(&llvmContext, *module,\n+                                           SMArch.getValue(), false /*isRocm*/);\n+  if (!llvmir) {\n+    llvm::errs() << \"Translate to LLVM IR failed\";\n+  }\n+\n+  if (targetKind == \"llvmir\")\n+    llvm::outs() << *llvmir << '\\n';\n+  else if (targetKind == \"ptx\")\n+    llvm::outs() << ::triton::translateLLVMIRToPTX(*llvmir, SMArch.getValue(),\n+                                                   ptxVersion.getValue());\n+  else if (targetKind == \"hsaco\") {\n+    auto [module, hsaco] = ::triton::translateLLVMIRToHSACO(\n+        *llvmir, GCNArch.getValue(), GCNTriple.getValue(),\n+        GCNFeatures.getValue());\n+    llvm::outs() << hsaco;\n+  } else {\n+    llvm::errs() << \"Error: Unknown target specified: \" << targetKind << \"\\n\";\n+    return failure();\n+  }\n+\n+  return success();\n+}\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+int main(int argc, char **argv) {\n+  return failed(mlir::triton::tritonTranslateMain(\n+      argc, argv, \"Triton Translate Testing Tool.\"));\n+}"}, {"filename": "cmake/FindLLVM.cmake", "status": "modified", "additions": 8, "deletions": 11, "changes": 19, "file_content_changes": "@@ -1,4 +1,3 @@\n-\n # - Find LLVM headers and libraries.\n # This module locates LLVM and adapts the llvm-config output for use with\n # CMake.\n@@ -25,22 +24,20 @@\n #  LLVM_VERSION_STRING - Full LLVM version string (e.g. 6.0.0svn).\n #  LLVM_VERSION_BASE_STRING - Base LLVM version string without git/svn suffix (e.g. 6.0.0).\n #\n-# Note: The variable names were chosen in conformance with the offical CMake\n+# Note: The variable names were chosen in conformance with the official CMake\n # guidelines, see ${CMAKE_ROOT}/Modules/readme.txt.\n \n # Try suffixed versions to pick up the newest LLVM install available on Debian\n # derivatives.\n # We also want an user-specified LLVM_ROOT_DIR to take precedence over the\n # system default locations such as /usr/local/bin. Executing find_program()\n # multiples times is the approach recommended in the docs.\n-set(llvm_config_names llvm-config-12.0 llvm-config120 llvm-config-12 llvm-config-12-64\n-                      llvm-config-11.0 llvm-config110 llvm-config-11 llvm-config-11-64\n-                      llvm-config-10.0 llvm-config100 llvm-config-10 llvm-config-10-64\n-                      llvm-config-9.0 llvm-config90 llvm-config-9 llvm-config-9-64\n-                      llvm-config-8.0 llvm-config80 llvm-config-8 llvm-config-8-64\n-                      llvm-config-7.0 llvm-config70 llvm-config-7 llvm-config-7-64\n-                      llvm-config-6.0 llvm-config60\n+set(llvm_config_names llvm-config-6.0 llvm-config60\n                       llvm-config)\n+foreach(v RANGE 7 17)\n+    # names like llvm-config-7.0 llvm-config70 llvm-config-7 llvm-config-7-64\n+    list(PREPEND llvm_config_names llvm-config-${v}.0 llvm-config${v}0 llvm-config-${v} llvm-config-${v}-64)\n+endforeach()\n find_program(LLVM_CONFIG\n     NAMES ${llvm_config_names}\n     PATHS ${LLVM_ROOT_DIR}/bin NO_DEFAULT_PATH\n@@ -111,7 +108,7 @@ else()\n         )\n         if(result_code)\n             _LLVM_FAIL(\"Failed to execute llvm-config ('${LLVM_CONFIG}', result code: '${result_code})'\")\n-        else()        \n+        else()\n             file(TO_CMAKE_PATH \"${tmplibs}\" tmplibs)\n             string(REGEX MATCHALL \"${pattern}[^ ]+\" LLVM_${var} ${tmplibs})\n         endif()\n@@ -196,4 +193,4 @@ include(FindPackageHandleStandardArgs)\n \n find_package_handle_standard_args(LLVM\n     REQUIRED_VARS LLVM_ROOT_DIR\n-    VERSION_VAR LLVM_VERSION_STRING)\n\\ No newline at end of file\n+    VERSION_VAR LLVM_VERSION_STRING)"}, {"filename": "deps/dlfcn-win32", "status": "removed", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1 +0,0 @@\n-Subproject commit 522c301ec366e9b42205ae21617780d37cc0e9f0"}, {"filename": "docs/Makefile", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -17,4 +17,4 @@ help:\n # Catch-all target: route all unknown targets to Sphinx using the new\n # \"make mode\" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).\n %: Makefile\n-\t@$(SPHINXBUILD) -M $@ \"$(SOURCEDIR)\" \"$(BUILDDIR)\" $(SPHINXOPTS) $(O)\n\\ No newline at end of file\n+\t@$(SPHINXBUILD) -M $@ \"$(SOURCEDIR)\" \"$(BUILDDIR)\" $(SPHINXOPTS) $(O)"}, {"filename": "docs/_templates/versions.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -24,4 +24,4 @@\n         {%- endif %}\n     </div>\n </div>\n-{%- endif %}\n\\ No newline at end of file\n+{%- endif %}"}, {"filename": "docs/conf.py", "status": "modified", "additions": 33, "deletions": 19, "changes": 52, "file_content_changes": "@@ -1,4 +1,3 @@\n-#!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n #\n # Triton documentation build configuration file, created by\n@@ -24,70 +23,84 @@\n # -- General configuration ------------------------------------------------\n \n \n+import os\n+import sys\n+\n+import sphinx_rtd_theme\n+from sphinx_gallery.sorting import FileNameSortKey\n \n \n def process_sig(app, what, name, obj, options, signature, return_annotation):\n     if signature and '_builder' in signature:\n-        signature = signature.split('_builder')[0] + \")\" \n+        signature = signature.split('_builder')[0] + \")\"\n     return (signature, return_annotation)\n \n+\n def setup(app):\n     \"\"\"Customize function args retrieving to get args under decorator.\"\"\"\n-    import sphinx\n     import os\n \n+    import sphinx\n+\n     app.connect(\"autodoc-process-signature\", process_sig)\n     os.system(\"pip install -e ../python\")\n \n-\n     def forward_jit_fn(func):\n         old = func\n \n         def wrapped(obj, **kwargs):\n             import triton\n-            if isinstance(obj, triton.code_gen.JITFunction):\n+            if isinstance(obj, triton.runtime.JITFunction):\n                 obj = obj.fn\n             return old(obj)\n \n         return wrapped\n \n-\n     old_documenter = sphinx.ext.autosummary.get_documenter\n \n     def documenter(app, obj, parent):\n         import triton\n-        if isinstance(obj, triton.code_gen.JITFunction):\n+        if isinstance(obj, triton.runtime.JITFunction):\n             obj = obj.fn\n         return old_documenter(app, obj, parent)\n \n     sphinx.ext.autosummary.get_documenter = documenter\n-    sphinx.util.inspect.unwrap_all = forward_jit_fn(sphinx.util.inspect.unwrap_all)\n-    sphinx.util.inspect.signature = forward_jit_fn(sphinx.util.inspect.signature)\n-    sphinx.util.inspect.object_description = forward_jit_fn(sphinx.util.inspect.object_description)\n+    sphinx.util.inspect.unwrap_all = forward_jit_fn(\n+        sphinx.util.inspect.unwrap_all)\n+    sphinx.util.inspect.signature = forward_jit_fn(\n+        sphinx.util.inspect.signature)\n+    sphinx.util.inspect.object_description = forward_jit_fn(\n+        sphinx.util.inspect.object_description)\n \n \n # Auto Doc\n-import sys\n-import os\n+\n sys.path.insert(0, os.path.abspath('../python/'))\n-extensions = ['sphinx.ext.autodoc', 'sphinx.ext.intersphinx', 'sphinx.ext.autosummary', 'sphinx.ext.coverage', 'sphinx.ext.napoleon', 'sphinx_multiversion']\n+extensions = [\n+    'sphinx.ext.autodoc',\n+    'sphinx.ext.intersphinx',\n+    'sphinx.ext.autosummary',\n+    'sphinx.ext.coverage',\n+    'sphinx.ext.napoleon',\n+    'sphinx_multiversion']\n autosummary_generate = True\n \n # versioning config\n-smv_tag_whitelist = r'^(v1.1.2)$'\n-smv_branch_whitelist = r'^master$'\n+smv_tag_whitelist = r'^(v2.1.0)$'\n+smv_branch_whitelist = r'^main$'\n smv_remote_whitelist = None\n smv_released_pattern = r'^tags/.*$'\n smv_outputdir_format = '{ref.name}'\n smv_prefer_remote_refs = False\n \n # Sphinx gallery\n extensions += ['sphinx_gallery.gen_gallery']\n-from sphinx_gallery.sorting import FileNameSortKey\n+\n sphinx_gallery_conf = {\n     'examples_dirs': '../python/tutorials/',\n     'gallery_dirs': 'getting-started/tutorials',\n     'filename_pattern': '',\n+    # XXX: Temporarily disable fused attention tutorial on V100\n     'ignore_pattern': r'__init__\\.py',\n     'within_subsection_order': FileNameSortKey,\n     'reference_url': {\n@@ -149,7 +162,7 @@ def documenter(app, obj, parent):\n # The theme to use for HTML and HTML Help pages.  See the documentation for\n # a list of builtin themes.\n #\n-import sphinx_rtd_theme\n+\n html_theme = 'sphinx_rtd_theme'\n html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n \n@@ -223,5 +236,6 @@ def documenter(app, obj, parent):\n # (source start file, target name, title, author,\n #  dir menu entry, description, category)\n texinfo_documents = [\n-    (master_doc, 'Triton', 'Triton Documentation', author, 'Triton', 'One line description of project.', 'Miscellaneous'),\n-]\n\\ No newline at end of file\n+    (master_doc, 'Triton', 'Triton Documentation', author,\n+     'Triton', 'One line description of project.', 'Miscellaneous'),\n+]"}, {"filename": "docs/getting-started/installation.rst", "status": "modified", "additions": 12, "deletions": 12, "changes": 24, "file_content_changes": "@@ -1,10 +1,10 @@\n-==============\n+============\n Installation\n-==============\n+============\n \n----------------------\n+--------------------\n Binary Distributions\n----------------------\n+--------------------\n \n You can install the latest stable release of Triton from pip:\n \n@@ -17,25 +17,25 @@ Binary wheels are available for CPython 3.6-3.9 and PyPy 3.6-3.7.\n And the latest nightly release:\n \n .. code-block:: bash\n-  \n+\n       pip install -U --pre triton\n \n \n---------------\n+-----------\n From Source\n---------------\n+-----------\n \n-+++++++++++++++\n+++++++++++++++\n Python Package\n-+++++++++++++++\n+++++++++++++++\n \n You can install the Python package from source by running the following commands:\n \n .. code-block:: bash\n \n       git clone https://github.com/openai/triton.git;\n       cd triton/python;\n-      pip install cmake; # build time dependency\n+      pip install cmake; # build-time dependency\n       pip install -e .\n \n Note that, if llvm-11 is not present on your system, the setup.py script will download the official LLVM11 static libraries link against that.\n@@ -50,6 +50,6 @@ You can then test your installation by running the unit tests:\n and the benchmarks\n \n .. code-block:: bash\n-      \n-      cd bench/\n+\n+      cd bench\n       python -m run --with-plots --result-dir /tmp/triton-bench"}, {"filename": "docs/getting-started/tutorials/parallel_reduction.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "docs/index.rst", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -3,6 +3,7 @@ Welcome to Triton's documentation!\n \n Triton is a language and compiler for parallel programming. It aims to provide a Python-based programming environment for productively writing custom DNN compute kernels capable of running at maximal throughput on modern GPU hardware.\n \n+\n Getting Started\n ---------------\n \n@@ -17,8 +18,9 @@ Getting Started\n    getting-started/installation\n    getting-started/tutorials/index\n \n+\n Python API\n--------------------\n+----------\n \n - :doc:`triton <python-api/triton>`\n - :doc:`triton.language <python-api/triton.language>`\n@@ -34,9 +36,9 @@ Python API\n    python-api/triton.language\n    python-api/triton.testing\n \n-   \n+\n Going Further\n-------------------\n+-------------\n \n Check out the following documents to learn more about Triton and how it compares against other DSLs for DNNs:\n "}, {"filename": "docs/programming-guide/chapter-1/introduction.rst", "status": "modified", "additions": 19, "deletions": 17, "changes": 36, "file_content_changes": "@@ -1,18 +1,18 @@\n-==============\n+============\n Introduction\n-==============\n+============\n \n---------------\n+-----------\n Motivations\n---------------\n+-----------\n \n-Over the past decade, Deep Neural Networks (DNNs) have emerged as an important class of Machine Learning (ML) models, capable of  achieving state-of-the-art performance across many domains ranging from natural language processing [SUTSKEVER2014]_ to computer vision [REDMON2016]_ to computational neuroscience [LEE2017]_. The strength of these models lies in their hierarchical structure, composed of a sequence of parametric (e.g., convolutional) and non-parametric (e.g., rectified linearity) *layers*. This pattern, though notoriously computationally expensive, also generates a large amount of highly parallelizable work particularly well suited for multi- and many- core processors.\n+Over the past decade, Deep Neural Networks (DNNs) have emerged as an important class of Machine Learning (ML) models, capable of achieving state-of-the-art performance across many domains ranging from natural language processing [SUTSKEVER2014]_ to computer vision [REDMON2016]_ to computational neuroscience [LEE2017]_. The strength of these models lies in their hierarchical structure, composed of a sequence of parametric (e.g., convolutional) and non-parametric (e.g., rectified linearity) *layers*. This pattern, though notoriously computationally expensive, also generates a large amount of highly parallelizable work particularly well suited for multi- and many- core processors.\n \n As a consequence, Graphics Processing Units (GPUs) have become a cheap and accessible resource for exploring and/or deploying novel research ideas in the field. This trend has been accelerated by the release of several frameworks for General-Purpose GPU (GPGPU) computing, such as CUDA and OpenCL, which have made the development of high-performance programs easier. Yet, GPUs remain incredibly challenging to optimize for locality and parallelism, especially for computations that cannot be efficiently implemented using a combination of pre-existing optimized primitives. To make matters worse, GPU architectures are also rapidly evolving and specializing, as evidenced by the addition of tensor cores to NVIDIA (and more recently AMD) micro-architectures.\n \n-This tension between the computational opportunities offered by DNNs and the practical difficulty of GPU programming has created substantial academic and industrial interest for Domain-Specific Languages (DSLs) and compilers. Regrettably, these systems -- whether they be based on  polyhedral machinery (*e.g.*, Tiramisu [BAGHDADI2021]_, Tensor Comprehensions [VASILACHE2018]_) or scheduling languages (*e.g.*, Halide [JRK2013]_, TVM [CHEN2018]_) -- remain less flexible and (for the same algorithm) markedly slower than the best handwritten compute kernels available in libraries like `cuBLAS <https://docs.nvidia.com/cuda/cublas/index.html>`_, `cuDNN <https://docs.nvidia.com/deeplearning/cudnn/api/index.html>`_ or `TensorRT <https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html>`_.\n+This tension between the computational opportunities offered by DNNs and the practical difficulty of GPU programming has created substantial academic and industrial interest for Domain-Specific Languages (DSLs) and compilers. Regrettably, these systems -- whether they be based on polyhedral machinery (e.g., Tiramisu [BAGHDADI2021]_, Tensor Comprehensions [VASILACHE2018]_) or scheduling languages (e.g., Halide [JRK2013]_, TVM [CHEN2018]_) -- remain less flexible and (for the same algorithm) markedly slower than the best handwritten compute kernels available in libraries like `cuBLAS <https://docs.nvidia.com/cuda/cublas/index.html>`_, `cuDNN <https://docs.nvidia.com/deeplearning/cudnn/api/index.html>`_ or `TensorRT <https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html>`_.\n \n-The main premise of this project is the following: programming paradigms based on blocked algorithms [LAM1991]_ can facilitate the construction of high-performance compute kernels for neural networks.  We specifically revisit traditional \"Single Program, Multiple Data\" (SPMD [AUGUIN1983]_) execution models for GPUs, and propose a variant in which programs -- rather than threads -- are blocked. For example, in the case of matrix multiplication, CUDA and Triton differ as follows:\n+The main premise of this project is the following: programming paradigms based on blocked algorithms [LAM1991]_ can facilitate the construction of high-performance compute kernels for neural networks. We specifically revisit traditional \"Single Program, Multiple Data\" (SPMD [AUGUIN1983]_) execution models for GPUs, and propose a variant in which programs -- rather than threads -- are blocked. For example, in the case of matrix multiplication, CUDA and Triton differ as follows:\n \n .. table::\n     :widths: 50 50\n@@ -27,14 +27,14 @@ The main premise of this project is the following: programming paradigms based o\n     |                                                     |   :force:                                           |\n     |                                                     |                                                     |\n     |   #pragma parallel                                  |   #pragma parallel                                  |\n-    |   for(int m = 0; i < M; m++)                        |   for(int m = 0; m < M; m += MB)                    |\n+    |   for(int m = 0; m < M; m++)                        |   for(int m = 0; m < M; m += MB)                    |\n     |   #pragma parallel                                  |   #pragma parallel                                  |\n-    |   for(int n = 0; j < N; n++){                       |   for(int n = 0; n < N; n += NB){                   |\n+    |   for(int n = 0; n < N; n++){                       |   for(int n = 0; n < N; n += NB){                   |\n     |     float acc = 0;                                  |     float acc[MB, NB] = 0;                          |\n-    |     for(int k = 0; k < K;k ++)                      |     for(int k = 0; k < K; k += KB)                  |\n-    |       acc += A[i, k]* B[k, j];                      |       acc +=  A[m:m+MB, k:k+KB]                     |\n+    |     for(int k = 0; k < K; k++)                      |     for(int k = 0; k < K; k += KB)                  |\n+    |       acc += A[m, k] * B[k, n];                     |       acc +=  A[m:m+MB, k:k+KB]                     |\n     |                                                     |             @ B[k:k+KB, n:n+NB];                    |\n-    |     C[i, j] = acc;                                  |     C[m:m+MB, n:n+NB] = acc;                        |\n+    |     C[m, n] = acc;                                  |     C[m:m+MB, n:n+NB] = acc;                        |\n     |   }                                                 |   }                                                 |\n     |                                                     |                                                     |\n     +-----------------------------------------------------+-----------------------------------------------------+\n@@ -48,15 +48,17 @@ The main premise of this project is the following: programming paradigms based o\n \n A key benefit of this approach is that it leads to block-structured iteration spaces that offer programmers more flexibility than existing DSLs when implementing sparse operations, all while allowing compilers to aggressively optimize programs for data locality and parallelism.\n \n---------------\n+\n+----------\n Challenges\n---------------\n+----------\n \n The main challenge posed by our proposed paradigm is that of work scheduling, i.e., how the work done by each program instance should be partitioned for efficient execution on modern GPUs. To address this issue, the Triton compiler makes heavy use of *block-level data-flow analysis*, a technique for scheduling iteration blocks statically based on the control- and data-flow structure of the target program. The resulting system actually works surprisingly well: our compiler manages to apply a broad range of interesting optimization automatically (e.g., automatic coalescing, thread swizzling, pre-fetching, automatic vectorization, tensor core-aware instruction selection, shared memory allocation/synchronization, asynchronous copy scheduling). Of course doing all this is not trivial; one of the purposes of this guide is to give you a sense of how it works.\n \n---------------\n+\n+----------\n References\n---------------\n+----------\n \n .. [SUTSKEVER2014] I. Sutskever et al., \"Sequence to Sequence Learning with Neural Networks\", NIPS 2014\n .. [REDMON2016] J. Redmon et al., \"You Only Look Once: Unified, Real-Time Object Detection\", CVPR 2016\n@@ -66,4 +68,4 @@ References\n .. [JRK2013] J. Ragan-Kelley et al., \"Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines\", PLDI 2013\n .. [CHEN2018] T. Chen et al., \"TVM: An Automated End-to-End Optimizing Compiler for Deep Learning\", OSDI 2018\n .. [LAM1991] M. Lam et al., \"The Cache Performance and Optimizations of Blocked Algorithms\", ASPLOS 1991\n-.. [AUGUIN1983] M. Auguin et al., \"Opsila: an advanced SIMD for numerical analysis and signal processing\", EUROMICRO 1983\n\\ No newline at end of file\n+.. [AUGUIN1983] M. Auguin et al., \"Opsila: an advanced SIMD for numerical analysis and signal processing\", EUROMICRO 1983"}, {"filename": "docs/programming-guide/chapter-2/related-work.rst", "status": "modified", "additions": 38, "deletions": 35, "changes": 73, "file_content_changes": "@@ -1,20 +1,21 @@\n-==============\n+============\n Related Work\n-==============\n+============\n \n At first sight, Triton may seem like just yet another DSL for DNNs. The purpose of this section is to contextualize Triton and highlight its differences with the two leading approaches in this domain: polyhedral compilation and scheduling languages.\n \n------------------------\n+\n+----------------------\n Polyhedral Compilation\n------------------------\n+----------------------\n \n Traditional compilers typically rely on intermediate representations, such as LLVM-IR [LATTNER2004]_, that encode control flow information using (un)conditional branches. This relatively low-level format makes it difficult to statically analyze the runtime behavior (e.g., cache misses) of input programs, and to  automatically optimize loops accordingly through the use of tiling [WOLFE1989]_, fusion [DARTE1999]_ and interchange [ALLEN1984]_. To solve this issue, polyhedral compilers [ANCOURT1991]_ rely on program representations that have statically predictable control flow, thereby enabling aggressive compile-time program transformations for data locality and parallelism. Though this strategy has been adopted by many languages and compilers for DNNs such as Tiramisu [BAGHDADI2021]_, Tensor Comprehensions [VASILACHE2018]_, Diesel [ELANGO2018]_ and the Affine dialect in MLIR [LATTNER2019]_, it also comes with a number of limitations that will be described later in this section.\n \n-+++++++++++++++++++++++\n+++++++++++++++++++++++\n Program Representation\n-+++++++++++++++++++++++\n+++++++++++++++++++++++\n \n-Polyhedral compilation is a vast area of research. In this section we only outline the most basic aspects of this topic, but readers interested in the solid mathematical foundations underneath may refer to the ample litterature on linear and integer programming.\n+Polyhedral compilation is a vast area of research. In this section we only outline the most basic aspects of this topic, but readers interested in the solid mathematical foundations underneath may refer to the ample literature on linear and integer programming.\n \n .. table::\n     :widths: 50 50\n@@ -105,19 +106,19 @@ Where :math:`\\Theta_S(\\mathbf{x})` is a p-dimensional vector representing the sl\n \n where :math:`i` and :math:`j` are respectively the slowest and fastest growing loop indices in the nest. If :math:`T_S` is a vector (resp. tensor), then :math:`\\Theta_S` is a said to be one-dimensional (resp. multi-dimensional).\n \n-+++++++++++\n+++++++++++\n Advantages\n-+++++++++++\n+++++++++++\n \n Programs amenable to polyhedral compilation can be aggressively transformed and optimized. Most of these transformations actually boil down to the production of  schedules and iteration domains that enable loop transformations promoting parallelism and spatial/temporal data locality (e.g., fusion, interchange, tiling, parallelization).\n \n Polyhedral compilers can also automatically go through complex verification processes to ensure that the semantics of their input program is preserved throughout this optimization phase. Note that polyhedral optimizers are not incompatible with more standard optimization techniques. In fact, it is not uncommon for these systems to be implemented as a set of LLVM passes that can be run ahead of more traditional compilation techniques [GROSSER2012]_.\n \n-All in all, polyhedral machinery is extremely powerful, when applicable. It has been shown to support most common loop transformations, and has indeed achieved performance comparable to state-of-the-art GPU libraries for dense matrix multiplication [ELANGO2018]_. Additionally, it is also fully automatic and doesn't require any hint from programmers apart from source-code in a C-like format. \n+All in all, polyhedral machinery is extremely powerful, when applicable. It has been shown to support most common loop transformations, and has indeed achieved performance comparable to state-of-the-art GPU libraries for dense matrix multiplication [ELANGO2018]_. Additionally, it is also fully automatic and doesn't require any hint from programmers apart from source-code in a C-like format.\n \n-++++++++++++\n++++++++++++\n Limitations\n-++++++++++++\n++++++++++++\n \n Unfortunately, polyhedral compilers suffer from two major limitations that have prevented its adoption as a universal method for code generation in neural networks.\n \n@@ -127,48 +128,49 @@ Second, the polyhedral framework is not very generally applicable; SCoPs are rel\n \n On the other hand, blocked program representations advocated by this dissertation are less restricted in scope and can achieve close to peak performance using standard dataflow analysis.\n \n------------------------\n+\n+--------------------\n Scheduling Languages\n------------------------\n+--------------------\n \n-Separation of concerns [DIJKSTRA82]_ is a well-known design principle in computer science: programs should be decomposed into modular layers of abstraction that separate the semantics of their algorithms from the details of their implementation. Systems like Halide and TVM push this philosophy one step further, and enforce this separation at the grammatical level through the use of a  **scheduling language**. The benefits of this methodology are particularly visible in the case of matrix multiplication, where, as one can see below, the definition of the algorithm (Line 1-7) is completely disjoint from its implementation (Line 8-16), meaning that both can be maintained, optimized and distributed independently. \n+Separation of concerns [DIJKSTRA82]_ is a well-known design principle in computer science: programs should be decomposed into modular layers of abstraction that separate the semantics of their algorithms from the details of their implementation. Systems like Halide and TVM push this philosophy one step further, and enforce this separation at the grammatical level through the use of a  **scheduling language**. The benefits of this methodology are particularly visible in the case of matrix multiplication, where, as one can see below, the definition of the algorithm (Line 1-7) is completely disjoint from its implementation (Line 8-16), meaning that both can be maintained, optimized and distributed independently.\n \n .. code-block:: python\n   :linenos:\n \n   // algorithm\n   Var x(\"x\"), y(\"y\");\n-  Func matmul(\"matmul\"); \n-  RDom k(0, matrix_size); \n-  RVar ki; \n-  matmul(x, y) = 0.0f; \n-  matmul(x, y) += A(k, y) * B(x, k); \n+  Func matmul(\"matmul\");\n+  RDom k(0, matrix_size);\n+  RVar ki;\n+  matmul(x, y) = 0.0f;\n+  matmul(x, y) += A(k, y) * B(x, k);\n   // schedule\n-  Var xi(\"xi\"), xo(\"xo\"), yo(\"yo\"), yi(\"yo\"), yii(\"yii\"), xii(\"xii\"); \n-  matmul.vectorize(x, 8); \n-  matmul.update(0) \n-      .split(x, x, xi, block_size).split(xi, xi, xii, 8) \n-      .split(y, y, yi, block_size).split(yi, yi, yii, 4) \n-      .split(k, k, ki, block_size) \n-      .reorder(xii, yii, xi, ki, yi, k, x, y) \n+  Var xi(\"xi\"), xo(\"xo\"), yo(\"yo\"), yi(\"yo\"), yii(\"yii\"), xii(\"xii\");\n+  matmul.vectorize(x, 8);\n+  matmul.update(0)\n+      .split(x, x, xi, block_size).split(xi, xi, xii, 8)\n+      .split(y, y, yi, block_size).split(yi, yi, yii, 4)\n+      .split(k, k, ki, block_size)\n+      .reorder(xii, yii, xi, ki, yi, k, x, y)\n       .parallel(y).vectorize(xii).unroll(xi).unroll(yii);\n \n \n The resulting code may however not be completely portable, as schedules can sometimes rely on execution models (e.g., SPMD) or hardware intrinsics (e.g., matrix-multiply-accumulate) that are not widely available. This issue can be mitigated by auto-scheduling mechanisms [MULLAPUDI2016]_.\n \n-+++++++++++\n+++++++++++\n Advantages\n-+++++++++++\n+++++++++++\n \n The main advantage of this approach is that it allows programmers to write an algorithm *only once*, and focus on performance optimization separately. It makes it possible to manually specify optimizations that a polyhedral compiler wouldn't be able to figure out automatically using static data-flow analysis.\n \n Scheduling languages are, without a doubt, one of the most popular approaches for neural network code generation. The most popular system for this purpose is probably TVM, which provides good performance across a wide range of platforms as well as built-in automatic scheduling mechanisms.\n \n-++++++++++++\n++++++++++++\n Limitations\n-++++++++++++\n++++++++++++\n \n-This ease-of-development comes at a cost. First of all, existing systems that follow this paradigm tend to be noticeably slower than Triton on modern hardware when applicable (e.g., V100/A100 tensor cores w/ equal tile sizes). I do believe that this is not a fundamental issue of scheduling languages -- in the sense that it could probably be solved with more efforts -- but it could mean that these systems are harder to engineer. More importantly, existing scheduling languages generate loops whose bounds and increments cannot depend on surrounding loop indice without at least imposing severe constraints on possible schedules -- if not breaking the system entirely. This is problematic for sparse computations, whose iteration spaces may be irregular.\n+This ease-of-development comes at a cost. First of all, existing systems that follow this paradigm tend to be noticeably slower than Triton on modern hardware when applicable (e.g., V100/A100 tensor cores w/ equal tile sizes). I do believe that this is not a fundamental issue of scheduling languages -- in the sense that it could probably be solved with more efforts -- but it could mean that these systems are harder to engineer. More importantly, existing scheduling languages generate loops whose bounds and increments cannot depend on surrounding loop indices without at least imposing severe constraints on possible schedules -- if not breaking the system entirely. This is problematic for sparse computations, whose iteration spaces may be irregular.\n \n .. table::\n     :widths: 50 50\n@@ -181,7 +183,7 @@ This ease-of-development comes at a cost. First of all, existing systems that fo\n     |   for(int j = 0; j < 4; j++)                        |                                                     |\n     |     float acc = 0;                                  |                                                     |\n     |     for(int k = 0; k < K[i]; k++)                   |                                                     |\n-    |       acc += A[i][col[i,k]]*B[k][j]                 |                                                     |\n+    |       acc += A[i][col[i, k]] * B[k][j]              |                                                     |\n     |     C[i][j] = acc;                                  |                                                     |\n     +-----------------------------------------------------+-----------------------------------------------------+\n \n@@ -190,9 +192,10 @@ This ease-of-development comes at a cost. First of all, existing systems that fo\n \n On the other hand, the block-based program representation that we advocate for through this work allows for block-structured iteration spaces and allows programmers to manually handle load-balancing as they wish.\n \n---------------\n+\n+----------\n References\n---------------\n+----------\n \n .. [LATTNER2004] C. Lattner et al., \"LLVM: a compilation framework for lifelong program analysis transformation\", CGO 2004\n .. [WOLFE1989] M. Wolfe, \"More Iteration Space Tiling\", SC 1989"}, {"filename": "docs/python-api/triton.language.rst", "status": "modified", "additions": 22, "deletions": 14, "changes": 36, "file_content_changes": "@@ -1,11 +1,11 @@\n triton.language\n-================\n+===============\n \n .. currentmodule:: triton.language\n \n \n Programming Model\n--------------------\n+-----------------\n \n .. autosummary::\n     :toctree: generated\n@@ -16,7 +16,7 @@ Programming Model\n \n \n Creation Ops\n--------------\n+------------\n \n .. autosummary::\n     :toctree: generated\n@@ -27,7 +27,7 @@ Creation Ops\n \n \n Shape Manipulation Ops\n------------------------\n+----------------------\n \n .. autosummary::\n     :toctree: generated\n@@ -40,16 +40,17 @@ Shape Manipulation Ops\n \n \n Linear Algebra Ops\n--------------------\n+------------------\n \n .. autosummary::\n     :toctree: generated\n     :nosignatures:\n \n     dot\n \n+\n Memory Ops\n---------------------\n+----------\n \n .. autosummary::\n     :toctree: generated\n@@ -62,7 +63,7 @@ Memory Ops\n \n \n Indexing Ops\n---------------\n+------------\n \n .. autosummary::\n     :toctree: generated\n@@ -72,12 +73,13 @@ Indexing Ops\n \n \n Math Ops\n-----------\n+--------\n \n .. autosummary::\n     :toctree: generated\n     :nosignatures:\n \n+    abs\n     exp\n     log\n     cos\n@@ -88,18 +90,23 @@ Math Ops\n \n \n Reduction Ops\n----------------\n+-------------\n \n .. autosummary::\n     :toctree: generated\n     :nosignatures:\n \n+    argmax\n+    argmin\n     max\n     min\n+    reduce\n     sum\n+    xor_sum\n+\n \n Atomic Ops\n----------------\n+----------\n \n .. autosummary::\n     :toctree: generated\n@@ -112,7 +119,7 @@ Atomic Ops\n \n \n Comparison ops\n----------------\n+--------------\n \n .. autosummary::\n     :toctree: generated\n@@ -124,7 +131,7 @@ Comparison ops\n .. _Random Number Generation:\n \n Random Number Generation\n--------------------------\n+------------------------\n \n .. autosummary::\n     :toctree: generated\n@@ -135,11 +142,12 @@ Random Number Generation\n     rand\n     randn\n \n+\n Compiler Hint Ops\n--------------------\n+-----------------\n \n .. autosummary::\n     :toctree: generated\n     :nosignatures:\n \n-    multiple_of\n\\ No newline at end of file\n+    multiple_of"}, {"filename": "docs/python-api/triton.rst", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n triton\n-========\n+======\n \n .. currentmodule:: triton\n \n@@ -10,4 +10,4 @@ triton\n     jit\n     autotune\n     heuristics\n-    Config\n\\ No newline at end of file\n+    Config"}, {"filename": "docs/python-api/triton.testing.rst", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n triton.testing\n-================\n+==============\n \n .. currentmodule:: triton.testing\n \n@@ -9,4 +9,4 @@ triton.testing\n \n     do_bench\n     Benchmark\n-    perf_report\n\\ No newline at end of file\n+    perf_report"}, {"filename": "include/CMakeLists.txt", "status": "added", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -0,0 +1 @@\n+add_subdirectory(triton)"}, {"filename": "include/triton/Analysis/Alias.h", "status": "added", "additions": 95, "deletions": 0, "changes": 95, "file_content_changes": "@@ -0,0 +1,95 @@\n+#ifndef TRITON_ANALYSIS_ALIAS_H\n+#define TRITON_ANALYSIS_ALIAS_H\n+\n+#include \"mlir/Analysis/AliasAnalysis.h\"\n+#include \"mlir/Analysis/DataFlow/SparseAnalysis.h\"\n+#include \"llvm/ADT/DenseSet.h\"\n+\n+namespace mlir {\n+\n+class AliasInfo {\n+public:\n+  AliasInfo() = default;\n+  AliasInfo(Value value) { insert(value); }\n+\n+  void insert(Value value) { allocs.insert(value); }\n+\n+  const DenseSet<Value> &getAllocs() const { return allocs; }\n+\n+  bool operator==(const AliasInfo &other) const {\n+    return allocs == other.allocs;\n+  }\n+\n+  /// The pessimistic value state of a value without alias\n+  static AliasInfo getPessimisticValueState(MLIRContext *context = nullptr) {\n+    return AliasInfo();\n+  }\n+  static AliasInfo getPessimisticValueState(Value value) { return AliasInfo(); }\n+\n+  /// The union of both arguments\n+  static AliasInfo join(const AliasInfo &lhs, const AliasInfo &rhs);\n+\n+  void print(raw_ostream &os) const {\n+    llvm::interleaveComma(allocs, os, [&](Value alloc) { alloc.print(os); });\n+  }\n+\n+private:\n+  /// The set of allocated values that are aliased by this lattice.\n+  /// For now, we only consider aliased value produced by the following\n+  /// situations:\n+  /// 1. values returned by scf.yield\n+  /// 2. block arguments in scf.for\n+  /// Example:\n+  ///    alloc v1                  alloc v2\n+  ///       |                         |\n+  ///    |--------------|   |------------|\n+  ///  scf.for v3     scf.for v4       scf.for v5\n+  ///    |\n+  /// scf.yield v6\n+  ///\n+  /// v1's alloc [v1]\n+  /// v2's alloc [v2]\n+  /// v3's alloc [v1]\n+  /// v4's alloc [v1, v2]\n+  /// v5's alloc [v2]\n+  /// v6's alloc [v1]\n+  ///\n+  /// Therefore, v1's liveness range is the union of v3, v4, and v6\n+  /// v2's liveness range is the union of v4 and v5.\n+  DenseSet<Value> allocs;\n+};\n+\n+//===----------------------------------------------------------------------===//\n+// Shared Memory Alias Analysis\n+//===----------------------------------------------------------------------===//\n+class SharedMemoryAliasAnalysis\n+    : public dataflow::SparseDataFlowAnalysis<dataflow::Lattice<AliasInfo>> {\n+public:\n+  using dataflow::SparseDataFlowAnalysis<\n+      dataflow::Lattice<AliasInfo>>::SparseDataFlowAnalysis;\n+  using dataflow::SparseDataFlowAnalysis<\n+      dataflow::Lattice<AliasInfo>>::getLatticeElement;\n+\n+  /// XXX(Keren): Compatible interface with MLIR AliasAnalysis for future use.\n+  /// Given two values, returns their aliasing behavior.\n+  AliasResult alias(Value lhs, Value rhs);\n+\n+  /// Returns the modify-reference behavior of `op` on `location`.\n+  ModRefResult getModRef(Operation *op, Value location);\n+\n+  void setToEntryState(dataflow::Lattice<AliasInfo> *lattice) override {\n+    propagateIfChanged(\n+        lattice, lattice->join(\n+                     AliasInfo::getPessimisticValueState(lattice->getPoint())));\n+  }\n+\n+  /// Computes if the alloc set of the results are changed.\n+  void\n+  visitOperation(Operation *op,\n+                 ArrayRef<const dataflow::Lattice<AliasInfo> *> operands,\n+                 ArrayRef<dataflow::Lattice<AliasInfo> *> results) override;\n+};\n+\n+} // namespace mlir\n+\n+#endif // TRITON_ANALYSIS_ALIAS_H"}, {"filename": "include/triton/Analysis/Allocation.h", "status": "added", "additions": 195, "deletions": 0, "changes": 195, "file_content_changes": "@@ -0,0 +1,195 @@\n+#ifndef TRITON_ANALYSIS_ALLOCATION_H\n+#define TRITON_ANALYSIS_ALLOCATION_H\n+\n+#include \"llvm/ADT/DenseMap.h\"\n+#include \"llvm/ADT/MapVector.h\"\n+#include \"llvm/ADT/SetVector.h\"\n+#include \"llvm/Support/raw_ostream.h\"\n+\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include <atomic>\n+#include <limits>\n+\n+namespace mlir {\n+\n+namespace triton {\n+class AllocationAnalysis;\n+\n+SmallVector<unsigned>\n+getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n+                             unsigned &outVec);\n+\n+} // namespace triton\n+\n+/// Modified from llvm-15.0: llvm/ADT/AddressRanges.h\n+/// A class that represents an interval, specified using a start and an end\n+/// values: [Start, End).\n+template <typename T> class Interval {\n+public:\n+  Interval() {}\n+  Interval(T S, T E) : Start(S), End(E) { assert(Start <= End); }\n+  T start() const { return Start; }\n+  T end() const { return End; }\n+  T size() const { return End - Start; }\n+  bool contains(T Addr) const { return Start <= Addr && Addr < End; }\n+  bool intersects(const Interval &R) const {\n+    return Start < R.End && R.Start < End;\n+  }\n+  bool operator==(const Interval &R) const {\n+    return Start == R.Start && End == R.End;\n+  }\n+  bool operator!=(const Interval &R) const { return !(*this == R); }\n+  bool operator<(const Interval &R) const {\n+    return std::make_pair(Start, End) < std::make_pair(R.Start, R.End);\n+  }\n+\n+private:\n+  T Start = std::numeric_limits<T>::min();\n+  T End = std::numeric_limits<T>::max();\n+};\n+\n+class Allocation {\n+public:\n+  /// A unique identifier for shared memory buffers\n+  using BufferId = size_t;\n+  using BufferIdSetT = DenseSet<BufferId>;\n+\n+  static constexpr BufferId InvalidBufferId =\n+      std::numeric_limits<BufferId>::max();\n+\n+  /// Creates a new Allocation analysis that computes the shared memory\n+  /// information for all associated shared memory values.\n+  Allocation(Operation *operation) : operation(operation) { run(); }\n+\n+  /// Returns the operation this analysis was constructed from.\n+  Operation *getOperation() const { return operation; }\n+\n+  /// Returns the offset of the given buffer in the shared memory.\n+  size_t getOffset(BufferId bufferId) const {\n+    return bufferSet.at(bufferId).offset;\n+  }\n+\n+  /// Returns the size of the given buffer in the shared memory.\n+  size_t getAllocatedSize(BufferId bufferId) const {\n+    return bufferSet.at(bufferId).size;\n+  }\n+\n+  /// Returns the buffer id of the given value.\n+  /// This interface only returns the allocated buffer id.\n+  /// If you want to get all the buffer ids that are associated with the given\n+  /// value, including alias buffers, use getBufferIds.\n+  BufferId getBufferId(Value value) const {\n+    if (valueBuffer.count(value)) {\n+      return valueBuffer.lookup(value)->id;\n+    } else {\n+      return InvalidBufferId;\n+    }\n+  }\n+\n+  /// Returns all the buffer ids of the given value, including alias buffers.\n+  BufferIdSetT getBufferIds(Value value) const {\n+    BufferIdSetT bufferIds;\n+    auto allocBufferId = getBufferId(value);\n+    if (allocBufferId != InvalidBufferId)\n+      bufferIds.insert(allocBufferId);\n+    for (auto *buffer : aliasBuffer.lookup(value)) {\n+      if (buffer->id != InvalidBufferId)\n+        bufferIds.insert(buffer->id);\n+    }\n+    return bufferIds;\n+  }\n+\n+  /// Returns the scratch buffer id of the given value.\n+  BufferId getBufferId(Operation *operation) const {\n+    if (opScratch.count(operation)) {\n+      return opScratch.lookup(operation)->id;\n+    } else {\n+      return InvalidBufferId;\n+    }\n+  }\n+\n+  /// Returns the size of total shared memory allocated\n+  size_t getSharedMemorySize() const { return sharedMemorySize; }\n+\n+  bool isIntersected(BufferId lhsId, BufferId rhsId) const {\n+    if (lhsId == InvalidBufferId || rhsId == InvalidBufferId)\n+      return false;\n+    auto lhsBuffer = bufferSet.at(lhsId);\n+    auto rhsBuffer = bufferSet.at(rhsId);\n+    return lhsBuffer.intersects(rhsBuffer);\n+  }\n+\n+private:\n+  /// A class that represents a shared memory buffer\n+  struct BufferT {\n+    enum class BufferKind { Explicit, Scratch };\n+\n+    /// MT: thread-safe\n+    inline static std::atomic<BufferId> nextId = 0;\n+\n+    BufferKind kind;\n+    BufferId id;\n+    size_t size;\n+    size_t offset;\n+\n+    bool operator==(const BufferT &other) const { return id == other.id; }\n+    bool operator<(const BufferT &other) const { return id < other.id; }\n+\n+    BufferT() : BufferT(BufferKind::Explicit) {}\n+    BufferT(BufferKind kind)\n+        : kind(kind), id(InvalidBufferId), size(0), offset(0) {}\n+    BufferT(BufferKind kind, size_t size) : BufferT(kind, size, 0) {}\n+    BufferT(BufferKind kind, size_t size, size_t offset)\n+        : kind(kind), id(nextId++), size(size), offset(offset) {}\n+\n+    bool intersects(const BufferT &other) const {\n+      return Interval<size_t>(offset, offset + size)\n+          .intersects(\n+              Interval<size_t>(other.offset, other.offset + other.size));\n+    }\n+  };\n+\n+  /// Op -> Scratch Buffer\n+  using OpScratchMapT = DenseMap<Operation *, BufferT *>;\n+  /// Value -> Explicit Buffer\n+  using ValueBufferMapT = llvm::MapVector<Value, BufferT *>;\n+  /// Value -> Alias Buffer\n+  using AliasBufferMapT = llvm::MapVector<Value, llvm::SetVector<BufferT *>>;\n+  /// BufferId -> Buffer\n+  using BufferSetT = std::map<BufferId, BufferT>;\n+  /// Runs allocation analysis on the given top-level operation.\n+  void run();\n+\n+private:\n+  template <BufferT::BufferKind Kind, typename KeyType, typename... Args>\n+  void addBuffer(KeyType &key, Args &&...args) {\n+    auto buffer = BufferT(Kind, std::forward<Args>(args)...);\n+    bufferSet[buffer.id] = std::move(buffer);\n+    if constexpr (Kind == BufferT::BufferKind::Explicit) {\n+      valueBuffer[key] = &bufferSet[buffer.id];\n+    } else {\n+      opScratch[key] = &bufferSet[buffer.id];\n+    }\n+  }\n+\n+  void addAlias(Value value, Value alloc) {\n+    aliasBuffer[value].insert(valueBuffer[alloc]);\n+  }\n+\n+private:\n+  Operation *operation;\n+  OpScratchMapT opScratch;\n+  ValueBufferMapT valueBuffer;\n+  AliasBufferMapT aliasBuffer;\n+  BufferSetT bufferSet;\n+  size_t sharedMemorySize = 0;\n+\n+  friend class triton::AllocationAnalysis;\n+};\n+\n+template <typename T> Interval(T, T) -> Interval<T>;\n+\n+} // namespace mlir\n+\n+#endif // TRITON_ANALYSIS_ALLOCATION_H"}, {"filename": "include/triton/Analysis/AxisInfo.h", "status": "added", "additions": 303, "deletions": 0, "changes": 303, "file_content_changes": "@@ -0,0 +1,303 @@\n+#ifndef TRITON_ANALYSIS_AXISINFO_H\n+#define TRITON_ANALYSIS_AXISINFO_H\n+\n+#include \"mlir/Analysis/DataFlow/SparseAnalysis.h\"\n+#include \"llvm/Support/raw_ostream.h\"\n+\n+#include \"mlir/Support/LLVM.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+\n+#include <optional>\n+#include <type_traits>\n+\n+namespace mlir {\n+\n+//===----------------------------------------------------------------------===//\n+// AxisInfo\n+//===----------------------------------------------------------------------===//\n+\n+/// This lattice value represents known information on the axes of a lattice.\n+class AxisInfo {\n+public:\n+  typedef SmallVector<int64_t, 4> DimVectorT;\n+\n+public:\n+  /// Default constructor\n+  AxisInfo() : AxisInfo({}, {}, {}) {}\n+  /// Construct contiguity info with known contiguity\n+  AxisInfo(DimVectorT knownContiguity, DimVectorT knownDivisibility,\n+           DimVectorT knownConstancy)\n+      : AxisInfo(knownContiguity, knownDivisibility, knownConstancy, {}) {}\n+  AxisInfo(DimVectorT knownContiguity, DimVectorT knownDivisibility,\n+           DimVectorT knownConstancy, std::optional<int64_t> knownConstantValue)\n+      : contiguity(knownContiguity), divisibility(knownDivisibility),\n+        constancy(knownConstancy), constantValue(knownConstantValue),\n+        rank(contiguity.size()) {\n+    assert(knownContiguity.size() == static_cast<size_t>(rank));\n+    assert(knownDivisibility.size() == static_cast<size_t>(rank));\n+    assert(knownConstancy.size() == static_cast<size_t>(rank));\n+  }\n+\n+  /// Accessors\n+  int64_t getContiguity(size_t dim) const { return contiguity[dim]; }\n+  const DimVectorT &getContiguity() const { return contiguity; }\n+\n+  int64_t getDivisibility(size_t dim) const { return divisibility[dim]; }\n+  const DimVectorT &getDivisibility() const { return divisibility; }\n+\n+  int64_t getConstancy(size_t dim) const { return constancy[dim]; }\n+  const DimVectorT &getConstancy() const { return constancy; }\n+\n+  int getRank() const { return rank; }\n+\n+  std::optional<int64_t> getConstantValue() const { return constantValue; }\n+\n+  template <class T>\n+  static void\n+  initPessimisticStateFromFunc(int argNumber, T funcOp, DimVectorT *contiguity,\n+                               DimVectorT *divisibility, DimVectorT *constancy);\n+  /// Comparison\n+  bool operator==(const AxisInfo &other) const {\n+    return (contiguity == other.contiguity) &&\n+           (divisibility == other.divisibility) &&\n+           (constancy == other.constancy) &&\n+           (constantValue == other.constantValue) && (rank == other.rank);\n+  }\n+\n+  /// The pessimistic value state of the contiguity is unknown.\n+  static AxisInfo getPessimisticValueState(MLIRContext *context = nullptr) {\n+    return AxisInfo();\n+  }\n+  static AxisInfo getPessimisticValueState(Value value);\n+\n+  /// The gcd of both arguments for each dimension\n+  static AxisInfo join(const AxisInfo &lhs, const AxisInfo &rhs);\n+\n+  void print(raw_ostream &os) const {\n+    auto print = [&](StringRef name, DimVectorT vec) {\n+      os << name << \" = [\";\n+      llvm::interleaveComma(vec, os);\n+      os << \"]\";\n+    };\n+    print(\"contiguity\", contiguity);\n+    print(\", divisibility\", divisibility);\n+    print(\", constancy\", constancy);\n+    os << \", constant_value = \";\n+    if (constantValue)\n+      os << *constantValue;\n+    else\n+      os << \"<none>\";\n+  }\n+\n+private:\n+  /// The _contiguity_ information maps the `d`-th\n+  /// dimension to the length of the shortest\n+  /// sequence of contiguous integers along it.\n+  /// Suppose we have an array of N elements,\n+  /// with a contiguity value C,\n+  /// the array can be divided into a list of\n+  /// N/C sequences of C contiguous elements.\n+  /// Since we have N = 2^k, C must be a power of two.\n+  /// For example:\n+  /// [10, 11, 12, 13, 18, 19, 20, 21]\n+  /// [20, 21, 22, 23, 28, 29, 30, 31]\n+  /// Would have contiguity [1, 4].\n+  /// and\n+  /// [12, 16, 20, 24]\n+  /// [13, 17, 21, 25]\n+  /// [14, 18, 22, 26]\n+  /// [15, 19, 23, 27]\n+  /// [18, 22, 26, 30]\n+  /// [19, 23, 27, 31]\n+  /// Would have contiguity [2, 1].\n+  DimVectorT contiguity;\n+\n+  /// The _divisibility_ information maps the `d`-th\n+  /// dimension to the largest power-of-two that\n+  /// divides the first element of all groups of\n+  // _contiguity_ values along it\n+  /// For example:\n+  /// [10, 11, 12, 13, 18, 19, 20, 21]\n+  /// [20, 21, 22, 23, 28, 29, 30, 31]\n+  //  would have divisibility [1, 2]\n+  //  and\n+  /// [12, 16, 20, 24]\n+  /// [13, 17, 21, 25]\n+  /// [14, 18, 22, 26]\n+  /// [15, 19, 23, 27]\n+  //  would have divisibility [4, 1]\n+  //  On the other hand:\n+  //  [0, 1, 2, 0, 4, 5, 6, 7]\n+  //  would have divisibility 1 because\n+  //  _contiguity_=1\n+  DimVectorT divisibility;\n+\n+  /// The _constancy_ information maps the `d`-th\n+  /// dimension to the length of the shortest\n+  /// sequence of constant integer along it. This is\n+  /// particularly useful to infer the contiguity\n+  /// of operations (e.g., add) involving a constant.\n+  /// Suppose we have an array of N elements,\n+  /// with a constancy value C,\n+  /// the array can be divided into a list of\n+  /// N/C sequences of C elements with the same value.\n+  /// Since we have N = 2^k, C must be a power of two.\n+  /// For example\n+  /// [8, 8, 8, 8, 12, 12, 12, 12]\n+  /// [16, 16, 16, 16, 20, 20, 20, 20]\n+  /// would have constancy [1, 4]\n+  DimVectorT constancy;\n+\n+  /// The constant value of the lattice if we can infer it.\n+  std::optional<int64_t> constantValue;\n+\n+  // number of dimensions of the lattice\n+  int rank{};\n+};\n+\n+class AxisInfoVisitor {\n+public:\n+  AxisInfoVisitor() = default;\n+  virtual ~AxisInfoVisitor() = default;\n+\n+  static bool isContiguousDim(const AxisInfo &info, ArrayRef<int64_t> shape,\n+                              int dim) {\n+    return info.getContiguity(dim) == shape[dim];\n+  }\n+\n+  static bool isConstantDim(const AxisInfo &info, ArrayRef<int64_t> shape,\n+                            int dim) {\n+    return info.getConstancy(dim) == shape[dim];\n+  }\n+\n+  virtual AxisInfo\n+  getAxisInfo(Operation *op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) = 0;\n+\n+  virtual bool match(Operation *op) = 0;\n+};\n+\n+/// Base class for all operations\n+template <typename OpTy> class AxisInfoVisitorImpl : public AxisInfoVisitor {\n+public:\n+  using AxisInfoVisitor::AxisInfoVisitor;\n+\n+  AxisInfo\n+  getAxisInfo(Operation *op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) final {\n+    return getAxisInfo(cast<OpTy>(op), operands);\n+  }\n+\n+  bool match(Operation *op) final { return isa<OpTy>(op); }\n+\n+  virtual AxisInfo\n+  getAxisInfo(OpTy op, ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) {\n+    llvm_unreachable(\"Unimplemented getAxisInfo\");\n+  }\n+};\n+\n+/// Binary operations\n+template <typename OpTy>\n+class BinaryOpVisitorImpl : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto lhsInfo = operands[0]->getValue();\n+    auto rhsInfo = operands[1]->getValue();\n+    auto rank = lhsInfo.getRank();\n+    assert(operands.size() == 2 && \"Expected two operands\");\n+    AxisInfo::DimVectorT contiguity;\n+    AxisInfo::DimVectorT divisibility;\n+    AxisInfo::DimVectorT constancy;\n+    auto constantValue = getConstantValue(op, lhsInfo, rhsInfo);\n+    for (auto d = 0; d < rank; ++d) {\n+      if (constantValue.has_value()) {\n+        contiguity.push_back(1);\n+        constancy.push_back(\n+            std::max(lhsInfo.getConstancy(d), rhsInfo.getConstancy(d)));\n+        divisibility.push_back(highestPowOf2Divisor(constantValue.value()));\n+      } else {\n+        contiguity.push_back(getContiguity(op, lhsInfo, rhsInfo, d));\n+        constancy.push_back(getConstancy(op, lhsInfo, rhsInfo, d));\n+        divisibility.push_back(getDivisibility(op, lhsInfo, rhsInfo, d));\n+      }\n+    }\n+    return AxisInfo(contiguity, divisibility, constancy, constantValue);\n+  }\n+\n+protected:\n+  virtual int64_t getContiguity(OpTy op, const AxisInfo &lhs,\n+                                const AxisInfo &rhs, int dim) {\n+    return 1;\n+  }\n+\n+  virtual int64_t getDivisibility(OpTy op, const AxisInfo &lhs,\n+                                  const AxisInfo &rhs, int dim) {\n+    return 1;\n+  }\n+\n+  virtual int64_t getConstancy(OpTy op, const AxisInfo &lhs,\n+                               const AxisInfo &rhs, int dim) {\n+    return 1;\n+  }\n+\n+  virtual std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                                  const AxisInfo &rhs) {\n+    return {};\n+  }\n+};\n+\n+class AxisInfoVisitorList {\n+public:\n+  template <typename... Ts, typename = std::enable_if_t<sizeof...(Ts) != 0>>\n+  void append() {\n+    (visitors.emplace_back(std::make_unique<Ts>()), ...);\n+  }\n+\n+  AxisInfo apply(Operation *op,\n+                 ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) {\n+    for (auto &visitor : visitors)\n+      if (visitor->match(op))\n+        return visitor->getAxisInfo(op, operands);\n+    return AxisInfo();\n+  }\n+\n+private:\n+  std::vector<std::unique_ptr<AxisInfoVisitor>> visitors;\n+};\n+\n+class AxisInfoAnalysis\n+    : public dataflow::SparseDataFlowAnalysis<dataflow::Lattice<AxisInfo>> {\n+private:\n+  AxisInfoVisitorList visitors;\n+\n+  void setToEntryState(dataflow::Lattice<AxisInfo> *lattice) override {\n+    propagateIfChanged(\n+        lattice,\n+        lattice->join(AxisInfo::getPessimisticValueState(lattice->getPoint())));\n+  }\n+\n+public:\n+  AxisInfoAnalysis(DataFlowSolver &solver);\n+  using dataflow::SparseDataFlowAnalysis<\n+      dataflow::Lattice<AxisInfo>>::getLatticeElement;\n+\n+  void visitOperation(Operation *op,\n+                      ArrayRef<const dataflow::Lattice<AxisInfo> *> operands,\n+                      ArrayRef<dataflow::Lattice<AxisInfo> *> results) override;\n+\n+  unsigned getPtrContiguity(Value ptr);\n+\n+  unsigned getPtrAlignment(Value ptr);\n+\n+  unsigned getMaskAlignment(Value mask);\n+};\n+\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Analysis/Membar.h", "status": "added", "additions": 128, "deletions": 0, "changes": 128, "file_content_changes": "@@ -0,0 +1,128 @@\n+#ifndef TRITON_ANALYSIS_MEMBAR_H\n+#define TRITON_ANALYSIS_MEMBAR_H\n+\n+#include \"Allocation.h\"\n+#include \"llvm/ADT/SmallPtrSet.h\"\n+\n+namespace mlir {\n+\n+class OpBuilder;\n+\n+//===----------------------------------------------------------------------===//\n+// Shared Memory Barrier Analysis\n+//===----------------------------------------------------------------------===//\n+class MembarAnalysis {\n+public:\n+  /// Creates a new Membar analysis that generates the shared memory barrier\n+  /// in the following circumstances:\n+  /// - RAW: If a shared memory write is followed by a shared memory read, and\n+  /// their addresses are intersected, a barrier is inserted.\n+  /// - WAR: If a shared memory read is followed by a shared memory read, and\n+  /// their addresses are intersected, a barrier is inserted.\n+  /// The following circumstances do not require a barrier:\n+  /// - WAW: not possible because overlapped memory allocation is not allowed.\n+  /// - RAR: no write is performed.\n+  /// Temporary storage of operations such as Reduce are considered as both\n+  /// a shared memory read. If the temporary storage is written but not read,\n+  /// it is considered as the problem of the operation itself but not the membar\n+  /// analysis.\n+  /// The following circumstances are not considered yet:\n+  /// - Double buffers\n+  /// - N buffers\n+  MembarAnalysis(Allocation *allocation) : allocation(allocation) {}\n+\n+  /// Runs the membar analysis to the given operation, inserts a barrier if\n+  /// necessary.\n+  void run();\n+\n+private:\n+  struct BlockInfo {\n+    using BufferIdSetT = Allocation::BufferIdSetT;\n+\n+    BufferIdSetT syncReadBuffers;\n+    BufferIdSetT syncWriteBuffers;\n+\n+    BlockInfo() = default;\n+    BlockInfo(const BufferIdSetT &syncReadBuffers,\n+              const BufferIdSetT &syncWriteBuffers)\n+        : syncReadBuffers(syncReadBuffers), syncWriteBuffers(syncWriteBuffers) {\n+    }\n+\n+    /// Unions two BlockInfo objects.\n+    BlockInfo &join(const BlockInfo &other) {\n+      syncReadBuffers.insert(other.syncReadBuffers.begin(),\n+                             other.syncReadBuffers.end());\n+      syncWriteBuffers.insert(other.syncWriteBuffers.begin(),\n+                              other.syncWriteBuffers.end());\n+      return *this;\n+    }\n+\n+    /// Returns true if buffers in two BlockInfo objects are intersected.\n+    bool isIntersected(const BlockInfo &other, Allocation *allocation) const {\n+      return /*RAW*/ isIntersected(syncWriteBuffers, other.syncReadBuffers,\n+                                   allocation) ||\n+             /*WAR*/\n+             isIntersected(syncReadBuffers, other.syncWriteBuffers,\n+                           allocation) ||\n+             /*WAW*/\n+             isIntersected(syncWriteBuffers, other.syncWriteBuffers,\n+                           allocation);\n+    }\n+\n+    /// Clears the buffers because a barrier is inserted.\n+    void sync() {\n+      syncReadBuffers.clear();\n+      syncWriteBuffers.clear();\n+    }\n+\n+    /// Compares two BlockInfo objects.\n+    bool operator==(const BlockInfo &other) const {\n+      return syncReadBuffers == other.syncReadBuffers &&\n+             syncWriteBuffers == other.syncWriteBuffers;\n+    }\n+\n+    bool operator!=(const BlockInfo &other) const { return !(*this == other); }\n+\n+  private:\n+    /// Returns true if buffers in two sets are intersected.\n+    bool isIntersected(const BufferIdSetT &lhs, const BufferIdSetT &rhs,\n+                       Allocation *allocation) const {\n+      return std::any_of(lhs.begin(), lhs.end(), [&](auto lhsId) {\n+        return std::any_of(rhs.begin(), rhs.end(), [&](auto rhsId) {\n+          return allocation->isIntersected(lhsId, rhsId);\n+        });\n+      });\n+    }\n+  };\n+\n+  /// Applies the barrier analysis based on the SCF dialect, in which each\n+  /// region has a single basic block only.\n+  /// Example:\n+  /// region1\n+  ///   op1\n+  ///   op2 (scf.if)\n+  ///      region2\n+  ///        op3\n+  ///        op4\n+  ///      region3\n+  ///        op5\n+  ///        op6\n+  ///   op7\n+  /// TODO: Explain why we don't use ForwardAnalysis:\n+  void resolve(Operation *operation, OpBuilder *builder);\n+\n+  /// Updates the BlockInfo operation based on the operation.\n+  void update(Operation *operation, BlockInfo *blockInfo, OpBuilder *builder);\n+\n+  /// Collects the successors of the terminator\n+  void visitTerminator(Operation *operation, SmallVector<Block *> &successors);\n+\n+private:\n+  Allocation *allocation;\n+  DenseMap<Block *, BlockInfo> inputBlockInfoMap;\n+  DenseMap<Block *, BlockInfo> outputBlockInfoMap;\n+};\n+\n+} // namespace mlir\n+\n+#endif // TRITON_ANALYSIS_MEMBAR_H"}, {"filename": "include/triton/Analysis/Utility.h", "status": "added", "additions": 129, "deletions": 0, "changes": 129, "file_content_changes": "@@ -0,0 +1,129 @@\n+#ifndef TRITON_ANALYSIS_UTILITY_H\n+#define TRITON_ANALYSIS_UTILITY_H\n+\n+#include \"mlir/Analysis/DataFlowFramework.h\"\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include <algorithm>\n+#include <numeric>\n+#include <string>\n+\n+namespace mlir {\n+\n+class ReduceOpHelper {\n+public:\n+  explicit ReduceOpHelper(triton::ReduceOp rop)\n+      : op(rop.getOperation()), axis(rop.getAxis()) {\n+    auto firstTy = rop.getOperands()[0].getType().cast<RankedTensorType>();\n+    srcShape = firstTy.getShape();\n+    srcEncoding = firstTy.getEncoding();\n+    srcElementTypes = rop.getElementTypes();\n+\n+    for (const auto &t : rop.getInputTypes()) {\n+      if (t.getShape() != srcShape) {\n+        rop.emitError() << \"shape mismatch\";\n+      }\n+      if (t.getEncoding() != srcEncoding) {\n+        rop.emitError() << \"encoding mismatch\";\n+      }\n+    }\n+  }\n+\n+  ArrayRef<int64_t> getSrcShape() { return srcShape; }\n+\n+  Attribute getSrcLayout() { return srcEncoding; }\n+\n+  bool isFastReduction();\n+\n+  unsigned getInterWarpSize();\n+\n+  unsigned getIntraWarpSize();\n+\n+  unsigned getThreadsReductionAxis();\n+\n+  SmallVector<unsigned> getScratchConfigBasic();\n+\n+  SmallVector<SmallVector<unsigned>> getScratchConfigsFast();\n+\n+  unsigned getScratchSizeInBytes();\n+\n+  bool isSupportedLayout();\n+\n+private:\n+  Operation *op;\n+  ArrayRef<int64_t> srcShape;\n+  Attribute srcEncoding;\n+  SmallVector<Type> srcElementTypes;\n+  int axis;\n+};\n+\n+bool isSharedEncoding(Value value);\n+\n+bool maybeSharedAllocationOp(Operation *op);\n+\n+bool maybeAliasOp(Operation *op);\n+\n+bool supportMMA(triton::DotOp op, int version);\n+\n+bool supportMMA(Value value, int version);\n+\n+Type getElementType(Value value);\n+\n+std::string getValueOperandName(Value value, AsmState &state);\n+\n+template <typename T_OUT, typename T_IN>\n+inline SmallVector<T_OUT> convertType(ArrayRef<T_IN> in) {\n+  SmallVector<T_OUT> out;\n+  for (const T_IN &i : in)\n+    out.push_back(T_OUT(i));\n+  return out;\n+}\n+\n+template <typename Int> Int product(llvm::ArrayRef<Int> arr) {\n+  return std::accumulate(arr.begin(), arr.end(), 1, std::multiplies{});\n+}\n+\n+template <typename Int> Int ceil(Int m, Int n) { return (m + n - 1) / n; }\n+\n+// output[i] = input[order[i]]\n+template <typename T, typename RES_T = T>\n+SmallVector<RES_T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n+  size_t rank = order.size();\n+  assert(input.size() == rank);\n+  SmallVector<RES_T> result(rank);\n+  for (auto it : llvm::enumerate(order)) {\n+    result[it.index()] = input[it.value()];\n+  }\n+  return result;\n+}\n+\n+template <typename T> T highestPowOf2Divisor(T n) {\n+  if (n == 0) {\n+    return (static_cast<T>(1) << (sizeof(T) * 8 - 2));\n+  }\n+  return (n & (~(n - 1)));\n+}\n+\n+bool isSingleValue(Value value);\n+\n+bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy);\n+\n+/// Multi-root DAG topological sort.\n+/// Performs a topological sort of the Operation in the `toSort` SetVector.\n+/// Returns a topologically sorted SetVector.\n+/// It is faster than mlir::topologicalSort because it prunes nodes that have\n+/// been visited before.\n+SetVector<Operation *>\n+multiRootTopologicalSort(const SetVector<Operation *> &toSort);\n+\n+// This uses the toplogicalSort above\n+SetVector<Operation *>\n+multiRootGetSlice(Operation *op, TransitiveFilter backwardFilter = nullptr,\n+                  TransitiveFilter forwardFilter = nullptr);\n+\n+// Create a basic DataFlowSolver with constant and dead code analysis included.\n+std::unique_ptr<DataFlowSolver> createDataFlowSolver();\n+\n+} // namespace mlir\n+\n+#endif // TRITON_ANALYSIS_UTILITY_H"}, {"filename": "include/triton/CMakeLists.txt", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -0,0 +1,2 @@\n+add_subdirectory(Conversion)\n+add_subdirectory(Dialect)"}, {"filename": "include/triton/Conversion/CMakeLists.txt", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -0,0 +1,2 @@\n+add_subdirectory(TritonToTritonGPU)\n+add_subdirectory(TritonGPUToLLVM)"}, {"filename": "include/triton/Conversion/MLIRTypes.h", "status": "added", "additions": 39, "deletions": 0, "changes": 39, "file_content_changes": "@@ -0,0 +1,39 @@\n+#ifndef TRITON_CONVERSION_MLIR_TYPES_H\n+#define TRITON_CONVERSION_MLIR_TYPES_H\n+\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+\n+// This file redefines some common MLIR types for easy usage.\n+namespace mlir {\n+namespace triton {\n+namespace type {\n+\n+// Integer types\n+inline Type i32Ty(MLIRContext *ctx) { return IntegerType::get(ctx, 32); }\n+inline Type i16Ty(MLIRContext *ctx) { return IntegerType::get(ctx, 16); }\n+inline Type i8Ty(MLIRContext *ctx) { return IntegerType::get(ctx, 8); }\n+inline Type u32Ty(MLIRContext *ctx) {\n+  return IntegerType::get(ctx, 32, IntegerType::Unsigned);\n+}\n+inline Type u1Ty(MLIRContext *ctx) {\n+  return IntegerType::get(ctx, 1, IntegerType::Unsigned);\n+}\n+\n+// Float types\n+inline Type f16Ty(MLIRContext *ctx) { return FloatType::getF16(ctx); }\n+inline Type f32Ty(MLIRContext *ctx) { return FloatType::getF32(ctx); }\n+inline Type f64Ty(MLIRContext *ctx) { return FloatType::getF64(ctx); }\n+inline Type bf16Ty(MLIRContext *ctx) { return FloatType::getBF16(ctx); }\n+\n+inline bool isFloat(Type type) {\n+  return type.isF32() || type.isF64() || type.isF16() || type.isF128();\n+}\n+\n+inline bool isInt(Type type) { return type.isIntOrFloat() && !isFloat(type); }\n+\n+} // namespace type\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif // TRITON_CONVERSION_MLIR_TYPES_H"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/AsmFormat.h", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -0,0 +1,27 @@\n+#ifndef TRITON_CONVERSION_TRITON_GPU_TO_LLVM_ASM_FORMAT_H_\n+#define TRITON_CONVERSION_TRITON_GPU_TO_LLVM_ASM_FORMAT_H_\n+\n+#include \"mlir/IR/Value.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/ADT/StringExtras.h\"\n+#include \"llvm/ADT/StringRef.h\"\n+#include <memory>\n+#include <string>\n+\n+namespace mlir {\n+class ConversionPatternRewriter;\n+class Location;\n+\n+namespace triton {\n+using llvm::StringRef;\n+\n+inline std::string strJoin(llvm::ArrayRef<std::string> strs,\n+                           llvm::StringRef delimiter) {\n+  return llvm::join(strs.begin(), strs.end(), delimiter);\n+}\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif // TRITON_CONVERSION_TRITON_GPU_TO_LLVM_ASM_FORMAT_H_"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -0,0 +1,3 @@\n+set(LLVM_TARGET_DEFINITIONS Passes.td)\n+mlir_tablegen(Passes.h.inc -gen-pass-decls --name TritonGPUToLLVM)\n+add_public_tablegen_target(TritonGPUConversionPassIncGen)"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/GCNAsmFormat.h", "status": "added", "additions": 381, "deletions": 0, "changes": 381, "file_content_changes": "@@ -0,0 +1,381 @@\n+#ifndef TRITON_CONVERSION_TRITON_GPU_TO_LLVM_GCN_FORMAT_H_\n+#define TRITON_CONVERSION_TRITON_GPU_TO_LLVM_GCN_FORMAT_H_\n+\n+#include \"mlir/IR/Value.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/ADT/StringRef.h\"\n+#include <memory>\n+#include <string>\n+\n+namespace mlir {\n+class ConversionPatternRewriter;\n+class Location;\n+\n+namespace triton {\n+using llvm::StringRef;\n+\n+class GCNInstr;\n+class GCNInstrCommon;\n+class GCNInstrExecution;\n+\n+// GCNBuilder helps to manage a GCN asm program consists of one or multiple\n+// instructions.\n+//\n+// A helper for building an ASM program, the objective of GCNBuilder is to give\n+// a thin encapsulation and make the ASM code for MLIR LLVM Dialect more clear.\n+// Currently, several factors are introduced to reduce the need for mixing\n+// string and C++ if-else code.\n+//\n+// Usage:\n+// To create a multiplcation operation\n+//\n+//\n+// GCNBuilder gcnBuilder;\n+// unsigned bitwidth = elemTy.getIntOrFloatBitWidth();\n+//\n+// const std::string readConstraint = \"v\";\n+// const std::string writeConstraint = \"=v\";\n+// auto res = gcnBuilder.newOperand(writeConstraint);\n+// auto lhs = gcnBuilder.newOperand(operands[0], readConstraint);\n+// auto rhs = gcnBuilder.newOperand(operands[1], readConstraint);\n+//\n+// create inst\n+// auto &mul_inst =\n+// gcnBuilder.create<GCNInstr>(\"v_mul\")->float_op_type(bitwidth);\n+//\n+// launch insts\n+// mul_inst(res, lhs, rhs);\n+//\n+// return result\n+// Value ret = gcnBuilder.launch(rewriter, loc, elemTy, false);\n+// return ret;\n+// To get the asm code:\n+// builder.dump()\n+//\n+// To get all the mlir::Value used in the GCN code,\n+//\n+// builder.getAllMlirArgs() // get {pVal, iVal, jVal, kVal}\n+//\n+// To get the string containing all the constraints with \",\" separated,\n+// builder.getConstraints() // get \"=v,v,v\"\n+//\n+// GCNBuilder can build a GCN asm with multiple instructions, sample code:\n+//\n+// GCNBuilder builder;\n+// auto &rcp = gcnBuilder.create<GCNInstr>(\"v_rcp\")->float_op_type(bitwidth);\n+// auto &mul_inst =\n+// gcnBuilder.create<GCNInstr>(\"v_mul\")->float_op_type(bitwidth);\n+//\n+// rcp(...);\n+// mul_inst(...);\n+// This will get a GCN code with two instructions.\n+//\n+// Similar to a C function, a declared GCNInstr instance can be launched\n+// multiple times with different operands, e.g.\n+//\n+//   auto &mul_inst =\n+//   gcnBuilder.create<GCNInstr>(\"v_mul\")->float_op_type(bitwidth); mul_inst(...\n+//   some operands ...); mul_inst(... some different operands ...);\n+//\n+// Finally, we will get a GCN code with two mov instructions.\n+//\n+// There are several derived instruction type for typical instructions, for\n+// example, the GCNIOInstr for ld and st instructions.\n+struct GCNBuilder {\n+  struct Operand {\n+    std::string constraint;\n+    Value value;\n+    int idx{-1};\n+    llvm::SmallVector<Operand *> list;\n+    std::function<std::string(int idx)> repr;\n+\n+    // for list\n+    Operand() = default;\n+    Operand(const Operation &) = delete;\n+    Operand(Value value, StringRef constraint)\n+        : value(value), constraint(constraint) {}\n+\n+    bool isList() const { return !value && constraint.empty(); }\n+\n+    Operand *listAppend(Operand *arg) {\n+      list.push_back(arg);\n+      return this;\n+    }\n+\n+    Operand *listGet(size_t nth) const {\n+      assert(nth < list.size());\n+      return list[nth];\n+    }\n+\n+    std::string dump() const;\n+  };\n+\n+  struct Modifier {\n+    Value value;\n+    std::string modifier;\n+    std::string arg;\n+    llvm::SmallVector<Modifier *> list;\n+\n+    Modifier() = default;\n+    Modifier(const Operation &) = delete;\n+    Modifier(Value value, StringRef arg) : value(value), arg(arg) {}\n+\n+    bool isList() const { return !value && modifier.empty(); }\n+\n+    Modifier *listAppend(Modifier *arg) {\n+      list.push_back(arg);\n+      return this;\n+    }\n+\n+    Modifier *listGet(size_t index) const {\n+      assert(index < list.size());\n+      return list[index];\n+    }\n+\n+    std::string to_str() const {\n+      std::string str = modifier;\n+      if (!arg.empty()) {\n+        str += \":\" + arg;\n+      }\n+      return str;\n+    }\n+\n+    std::string dump() const;\n+  };\n+\n+  template <typename INSTR = GCNInstr, typename... Args>\n+  INSTR *create(Args &&...args) {\n+    instrs.emplace_back(std::make_unique<INSTR>(this, args...));\n+    return static_cast<INSTR *>(instrs.back().get());\n+  }\n+\n+  // Create a list of operands.\n+  Operand *newListOperand() { return newOperand(); }\n+\n+  Operand *newListOperand(ArrayRef<std::pair<mlir::Value, std::string>> items) {\n+    auto *list = newOperand();\n+    for (auto &item : items) {\n+      list->listAppend(newOperand(item.first, item.second));\n+    }\n+    return list;\n+  }\n+\n+  Operand *newListOperand(unsigned count, mlir::Value val,\n+                          const std::string &constraint) {\n+    auto *list = newOperand();\n+    for (int i = 0; i < count; ++i) {\n+      list->listAppend(newOperand(val, constraint));\n+    }\n+    return list;\n+  }\n+\n+  Operand *newListOperand(unsigned count, const std::string &constraint) {\n+    auto *list = newOperand();\n+    for (int i = 0; i < count; ++i) {\n+      list->listAppend(newOperand(constraint));\n+    }\n+    return list;\n+  }\n+\n+  // Create a new operand. It will not add to operand list.\n+  // @value: the MLIR value bind to this operand.\n+  // @constraint: ASM operand constraint, .e.g. \"=r\"\n+  // @formatter: extra format to represent this operand in ASM code, default is\n+  //             \"%{0}\".format(operand.idx).\n+  Operand *newOperand(mlir::Value value, StringRef constraint,\n+                      std::function<std::string(int idx)> formatter = nullptr);\n+\n+  // Create a new operand which is written to, that is, the constraint starts\n+  // with \"=\", e.g. \"=r\".\n+  Operand *newOperand(StringRef constraint);\n+\n+  // Create a constant integer operand.\n+  Operand *newConstantOperand(int v);\n+  // Create a constant operand with explicit code specified.\n+  Operand *newConstantOperand(const std::string &v);\n+\n+  Operand *newAddrOperand(mlir::Value addr, StringRef constraint);\n+\n+  Modifier *newModifier(StringRef modifier, StringRef arg);\n+\n+  llvm::SmallVector<Operand *, 4> getAllArgs() const;\n+\n+  llvm::SmallVector<Value, 4> getAllMLIRArgs() const;\n+\n+  std::string getConstraints() const;\n+\n+  std::string dump() const;\n+\n+  mlir::Value launch(ConversionPatternRewriter &rewriter, Location loc,\n+                     Type resTy, bool hasSideEffect = true,\n+                     bool isAlignStack = false,\n+                     ArrayRef<Attribute> attrs = {}) const;\n+\n+private:\n+  Operand *newOperand() {\n+    argArchive.emplace_back(std::make_unique<Operand>());\n+    return argArchive.back().get();\n+  }\n+\n+  Modifier *newModifier() {\n+    modArchive.emplace_back(std::make_unique<Modifier>());\n+    return modArchive.back().get();\n+  }\n+\n+  friend class GCNInstr;\n+  friend class GCNInstrCommon;\n+\n+protected:\n+  llvm::SmallVector<std::unique_ptr<Operand>, 6> argArchive;\n+  llvm::SmallVector<std::unique_ptr<Modifier>, 2> modArchive;\n+  llvm::SmallVector<std::unique_ptr<GCNInstrCommon>, 2> instrs;\n+  llvm::SmallVector<std::unique_ptr<GCNInstrExecution>, 4> executions;\n+  int oprCounter{};\n+};\n+\n+// GCN instruction common interface.\n+// Put the generic logic for all the instructions here.\n+struct GCNInstrCommon {\n+  explicit GCNInstrCommon(GCNBuilder *builder) : builder(builder) {}\n+\n+  using Operand = GCNBuilder::Operand;\n+  using Modifier = GCNBuilder::Modifier;\n+\n+  // clang-format off\n+  GCNInstrExecution& operator()() { return call({}, {}); }\n+  GCNInstrExecution& operator()(Operand* a) { return call({a}, {}); }\n+  GCNInstrExecution& operator()(Operand* a, Operand* b) { return call({a, b}, {}); }\n+  GCNInstrExecution& operator()(Operand* a, Operand* b, Operand* c) { return call({a, b, c}, {}); }\n+  GCNInstrExecution& operator()(Operand* a, Operand* b, Operand* c, Operand* d) { return call({a, b, c, d}, {}); }\n+  GCNInstrExecution& operator()(Operand* a, Operand* b, Operand* c, Operand* d, Operand * e) { return call({a, b, c, d, e}, {}); }\n+  GCNInstrExecution& operator()(Operand* a, Operand* b, Operand* c, Operand* d, Operand * e, Operand* f) { return call({a, b, c, d, e, f}, {}); }\n+  GCNInstrExecution& operator()(Operand* a, Operand* b, Operand* c, Operand* d, Operand * e, Operand* f, Operand* g) { return call({a, b, c, d, e, f, g}, {}); }\n+  // clang-format on\n+\n+  // Set operands of this instruction.\n+  GCNInstrExecution &operator()(llvm::ArrayRef<Operand *> oprs,\n+                                llvm::ArrayRef<Modifier *> mods);\n+\n+protected:\n+  GCNInstrExecution &call(llvm::ArrayRef<Operand *> oprs,\n+                          ArrayRef<Modifier *> mods);\n+\n+  GCNBuilder *builder{};\n+  llvm::SmallVector<std::string, 4> instrParts;\n+\n+  friend class GCNInstrExecution;\n+};\n+\n+template <class ConcreteT> struct GCNInstrBase : public GCNInstrCommon {\n+  using Operand = GCNBuilder::Operand;\n+  using Modifier = GCNBuilder::Modifier;\n+\n+  explicit GCNInstrBase(GCNBuilder *builder, const std::string &name)\n+      : GCNInstrCommon(builder) {\n+    o(name);\n+  }\n+\n+  ConcreteT &o(const std::string &suffix, bool predicate = true) {\n+    if (predicate)\n+      instrParts.push_back(suffix);\n+    return *static_cast<ConcreteT *>(this);\n+  }\n+};\n+\n+enum VectorWidth { Byte = 8, Short = 16, Dword = 32, Qword = 64 };\n+\n+struct GCNInstr : public GCNInstrBase<GCNInstr> {\n+  using GCNInstrBase<GCNInstr>::GCNInstrBase;\n+\n+  GCNInstr &float_op_type(int width) {\n+    switch (width) {\n+    case Byte:\n+      assert(Byte != width);\n+      break;\n+    case Short:\n+      o(\"f16\");\n+      break;\n+    case Dword:\n+      o(\"f32\");\n+      break;\n+    case Qword:\n+      o(\"f64\");\n+      break;\n+    default:\n+      break;\n+    }\n+    return *this;\n+  }\n+};\n+\n+struct GCNInstrExecution {\n+  using Operand = GCNBuilder::Operand;\n+  using Modifier = GCNBuilder::Modifier;\n+\n+  llvm::SmallVector<Operand *> argsInOrder;\n+  llvm::SmallVector<Modifier *> mods;\n+\n+  GCNInstrExecution() = default;\n+  explicit GCNInstrExecution(GCNInstrCommon *instr,\n+                             llvm::ArrayRef<Operand *> oprs,\n+                             llvm::ArrayRef<Modifier *> modifiers)\n+      : instr(instr), argsInOrder(oprs.begin(), oprs.end()),\n+        mods(modifiers.begin(), modifiers.end()) {}\n+\n+  std::string dump() const;\n+\n+  SmallVector<Operand *> getArgList() const;\n+\n+  GCNInstrCommon *instr{};\n+};\n+\n+struct GCNMemInstr : public GCNInstrBase<GCNMemInstr> {\n+  using GCNInstrBase<GCNMemInstr>::GCNInstrBase;\n+  // Add specific type suffix to instruction\n+\n+  GCNMemInstr &load_type(int width) {\n+    switch (width) {\n+    case Byte:\n+      o(\"ubyte\");\n+      break;\n+    case Short:\n+      o(\"ushort\");\n+      break;\n+    case Dword:\n+      o(\"dword\");\n+      break;\n+    case Qword:\n+      o(\"dwordx2\");\n+      break;\n+    default:\n+      break;\n+    }\n+    return *this;\n+  }\n+\n+  GCNMemInstr &store_type(int width) {\n+    switch (width) {\n+    case Byte:\n+      o(\"byte\");\n+      break;\n+    case Short:\n+      o(\"short\");\n+      break;\n+    case Dword:\n+      o(\"dword\");\n+      break;\n+    case Qword:\n+      o(\"dwordx2\");\n+      break;\n+    default:\n+      break;\n+    }\n+    return *this;\n+  }\n+};\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif // TRITON_CONVERSION_TRITON_GPU_TO_LLVM_ASM_FORMAT_H_"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h", "status": "added", "additions": 332, "deletions": 0, "changes": 332, "file_content_changes": "@@ -0,0 +1,332 @@\n+#ifndef TRITON_CONVERSION_TRITON_GPU_TO_LLVM_PTX_ASM_FORMAT_H_\n+#define TRITON_CONVERSION_TRITON_GPU_TO_LLVM_PTX_ASM_FORMAT_H_\n+\n+#include \"mlir/IR/Value.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/ADT/StringRef.h\"\n+#include <memory>\n+#include <string>\n+\n+namespace mlir {\n+class ConversionPatternRewriter;\n+class Location;\n+\n+namespace triton {\n+using llvm::StringRef;\n+\n+struct PTXInstr;\n+struct PTXInstrCommon;\n+struct PTXInstrExecution;\n+\n+// PTXBuilder helps to manage a PTX asm program consists of one or multiple\n+// instructions.\n+//\n+// A helper for building an ASM program, the objective of PTXBuilder is to give\n+// a thin encapsulation and make the ASM code for MLIR LLVM Dialect more clear.\n+// Currently, several factors are introduced to reduce the need for mixing\n+// string and C++ if-else code.\n+//\n+// Usage:\n+// To build: @$3 asm(\"@%3 add.s32 %0, %1, %2;\" : \"=r\"(i) : \"r\"(j), \"r\"(k),\n+// \"b\"(p));\n+//\n+// PTXBuilder builder;\n+// auto& add = builder.create<>();\n+// add.predicate(pVal).o(\"lo\").o(\"u32\"); // add any suffix\n+// // predicate here binds %0 to pVal, pVal is a mlir::Value\n+//\n+// auto* iOpr = builder.newOperand(iVal, \"r\"); // %1 bind to iVal\n+// auto* jOpr = builder.newOperand(jVal, \"r\"); // %2 bind to jVal\n+// auto* kOpr = builder.newOperand(kVal, \"r\"); // %3 bind to kVal\n+// add(iOpr, jOpr, kOpr).predicate(predVal);   // set operands and predicate\n+//\n+// To get the asm code:\n+// builder.dump()\n+//\n+// To get all the mlir::Value used in the PTX code,\n+//\n+// builder.getAllMlirArgs() // get {pVal, iVal, jVal, kVal}\n+//\n+// To get the string containing all the constraints with \",\" separated,\n+// builder.getConstraints() // get \"=r,r,k\"\n+//\n+// PTXBuilder can build a PTX asm with multiple instructions, sample code:\n+//\n+// PTXBuilder builder;\n+// auto& mov = builder.create(\"mov\");\n+// auto& cp = builder.create(\"cp\");\n+// mov(...);\n+// cp(...);\n+// This will get a PTX code with two instructions.\n+//\n+// Similar to a C function, a declared PTXInstr instance can be launched\n+// multiple times with different operands, e.g.\n+//\n+//   auto& mov = builder.create(\"mov\");\n+//   mov(... some operands ...);\n+//   mov(... some different operands ...);\n+//\n+// Finally, we will get a PTX code with two mov instructions.\n+//\n+// There are several derived instruction type for typical instructions, for\n+// example, the PtxIOInstr for ld and st instructions.\n+struct PTXBuilder {\n+  struct Operand {\n+    std::string constraint;\n+    Value value;\n+    int idx{-1};\n+    llvm::SmallVector<Operand *> list;\n+    std::function<std::string(int idx)> repr;\n+\n+    // for list\n+    Operand() = default;\n+    Operand(const Operation &) = delete;\n+    Operand(Value value, StringRef constraint)\n+        : constraint(constraint), value(value) {}\n+\n+    bool isList() const { return !value && constraint.empty(); }\n+\n+    Operand *listAppend(Operand *arg) {\n+      list.push_back(arg);\n+      return this;\n+    }\n+\n+    Operand *listGet(size_t nth) const {\n+      assert(nth < list.size());\n+      return list[nth];\n+    }\n+\n+    std::string dump() const;\n+  };\n+\n+  template <typename INSTR = PTXInstr, typename... Args>\n+  INSTR *create(Args &&...args) {\n+    instrs.emplace_back(std::make_unique<INSTR>(this, args...));\n+    return static_cast<INSTR *>(instrs.back().get());\n+  }\n+\n+  // Create a list of operands.\n+  Operand *newListOperand() { return newOperand(); }\n+\n+  Operand *newListOperand(ArrayRef<std::pair<mlir::Value, std::string>> items) {\n+    auto *list = newOperand();\n+    for (auto &item : items) {\n+      list->listAppend(newOperand(item.first, item.second));\n+    }\n+    return list;\n+  }\n+\n+  Operand *newListOperand(unsigned count, mlir::Value val,\n+                          const std::string &constraint) {\n+    auto *list = newOperand();\n+    for (unsigned i = 0; i < count; ++i) {\n+      list->listAppend(newOperand(val, constraint));\n+    }\n+    return list;\n+  }\n+\n+  Operand *newListOperand(unsigned count, const std::string &constraint) {\n+    auto *list = newOperand();\n+    for (unsigned i = 0; i < count; ++i) {\n+      list->listAppend(newOperand(constraint));\n+    }\n+    return list;\n+  }\n+\n+  // Create a new operand. It will not add to operand list.\n+  // @value: the MLIR value bind to this operand.\n+  // @constraint: ASM operand constraint, .e.g. \"=r\"\n+  // @formatter: extra format to represent this operand in ASM code, default is\n+  //             \"%{0}\".format(operand.idx).\n+  Operand *newOperand(mlir::Value value, StringRef constraint,\n+                      std::function<std::string(int idx)> formatter = nullptr);\n+\n+  // Create a new operand which is written to, that is, the constraint starts\n+  // with \"=\", e.g. \"=r\".\n+  // If the operand will be used in predicated execution,\n+  // users may want to initialize it before use.\n+  // Otherwise if the register is only used in the true branch or the false\n+  // branch but not both, the register is undefined and ptxas can perform\n+  // aggressive optimizations that may lead to incorrect results.\n+  Operand *newOperand(StringRef constraint, bool init = false);\n+\n+  // Create a constant integer operand.\n+  Operand *newConstantOperand(int64_t v);\n+  // Create a constant operand with explicit code specified.\n+  Operand *newConstantOperand(const std::string &v);\n+\n+  Operand *newAddrOperand(mlir::Value addr, StringRef constraint, int off = 0);\n+\n+  llvm::SmallVector<Operand *, 4> getAllArgs() const;\n+\n+  llvm::SmallVector<Value, 4> getAllMLIRArgs() const;\n+\n+  std::string getConstraints() const;\n+\n+  std::string dump() const;\n+\n+  mlir::Value launch(OpBuilder &rewriter, Location loc, Type resTy,\n+                     bool hasSideEffect = true, bool isAlignStack = false,\n+                     ArrayRef<Attribute> attrs = {}) const;\n+\n+private:\n+  Operand *newOperand() {\n+    argArchive.emplace_back(std::make_unique<Operand>());\n+    return argArchive.back().get();\n+  }\n+\n+  void initOperand(Operand *opr);\n+\n+  // Make the operands in argArchive follow the provided \\param order.\n+  void reorderArgArchive(ArrayRef<Operand *> order) {\n+    assert(order.size() == argArchive.size());\n+    // The order in argArchive is unnecessary when onlyAttachMLIRArgs=false, but\n+    // it does necessary when onlyAttachMLIRArgs is true for the $0, $1... are\n+    // determined by PTX code snippet passed from external.\n+    sort(argArchive.begin(), argArchive.end(),\n+         [&](std::unique_ptr<Operand> &a, std::unique_ptr<Operand> &b) {\n+           auto ida = std::find(order.begin(), order.end(), a.get());\n+           auto idb = std::find(order.begin(), order.end(), b.get());\n+           assert(ida != order.end());\n+           assert(idb != order.end());\n+           return ida < idb;\n+         });\n+  }\n+\n+  friend struct PTXInstr;\n+  friend struct PTXInstrCommon;\n+\n+protected:\n+  llvm::SmallVector<std::unique_ptr<Operand>, 6> argArchive;\n+  llvm::SmallVector<std::unique_ptr<PTXInstrCommon>, 2> instrs;\n+  llvm::SmallVector<std::unique_ptr<PTXInstrExecution>, 4> executions;\n+  int oprCounter{};\n+};\n+\n+// PTX instruction common interface.\n+// Put the generic logic for all the instructions here.\n+struct PTXInstrCommon {\n+  explicit PTXInstrCommon(PTXBuilder *builder) : builder(builder) {}\n+\n+  using Operand = PTXBuilder::Operand;\n+\n+  // clang-format off\n+  PTXInstrExecution& operator()() { return call({}); }\n+  PTXInstrExecution& operator()(Operand* a) { return call({a}); }\n+  PTXInstrExecution& operator()(Operand* a, Operand* b) { return call({a, b}); }\n+  PTXInstrExecution& operator()(Operand* a, Operand* b, Operand* c) { return call({a, b, c}); }\n+  PTXInstrExecution& operator()(Operand* a, Operand* b, Operand* c, Operand* d) { return call({a, b, c, d}); }\n+  PTXInstrExecution& operator()(Operand* a, Operand* b, Operand* c, Operand* d, Operand * e) { return call({a, b, c, d, e}); }\n+  PTXInstrExecution& operator()(Operand* a, Operand* b, Operand* c, Operand* d, Operand * e, Operand* f) { return call({a, b, c, d, e, f}); }\n+  PTXInstrExecution& operator()(Operand* a, Operand* b, Operand* c, Operand* d, Operand * e, Operand* f, Operand* g) { return call({a, b, c, d, e, f, g}); }\n+  // clang-format on\n+\n+  // Set operands of this instruction.\n+  PTXInstrExecution &operator()(llvm::ArrayRef<Operand *> oprs,\n+                                bool onlyAttachMLIRArgs = false);\n+\n+protected:\n+  // \"Call\" the instruction with operands.\n+  // \\param oprs The operands of this instruction.\n+  // \\param onlyAttachMLIRArgs Indicate that it simply attach the MLIR Arguments\n+  // to the inline Asm without generating the operand ids(such as $0, $1) in PTX\n+  // code.\n+  PTXInstrExecution &call(llvm::ArrayRef<Operand *> oprs,\n+                          bool onlyAttachMLIRArgs = false);\n+\n+  PTXBuilder *builder{};\n+  llvm::SmallVector<std::string, 4> instrParts;\n+\n+  friend struct PTXInstrExecution;\n+};\n+\n+template <class ConcreteT> struct PTXInstrBase : public PTXInstrCommon {\n+  using Operand = PTXBuilder::Operand;\n+\n+  explicit PTXInstrBase(PTXBuilder *builder, const std::string &name)\n+      : PTXInstrCommon(builder) {\n+    o(name);\n+  }\n+\n+  // Append a suffix to the instruction.\n+  // e.g. PTXInstr(\"add\").o(\"s32\") get a add.s32.\n+  // A predicate is used to tell whether to apply the suffix, so that no if-else\n+  // code needed. e.g. `PTXInstr(\"add\").o(\"s32\", isS32).o(\"u32\", !isS32);` will\n+  // get a `add.s32` if isS32 is true.\n+  ConcreteT &o(const std::string &suffix, bool predicate = true) {\n+    if (predicate)\n+      instrParts.push_back(suffix);\n+    return *static_cast<ConcreteT *>(this);\n+  }\n+};\n+\n+struct PTXInstr : public PTXInstrBase<PTXInstr> {\n+  using PTXInstrBase<PTXInstr>::PTXInstrBase;\n+\n+  // Append a \".global\" to the instruction.\n+  PTXInstr &global();\n+\n+  // Append a \".shared\" to the instruction.\n+  PTXInstr &shared();\n+\n+  // Append a \".v[0-9]+\" to the instruction\n+  PTXInstr &v(int vecWidth, bool predicate = true);\n+\n+  // Append a\".b[0-9]+\" to the instruction\n+  PTXInstr &b(int width);\n+};\n+\n+// Record the operands and context for \"launching\" a PtxInstr.\n+struct PTXInstrExecution {\n+  using Operand = PTXBuilder::Operand;\n+\n+  llvm::SmallVector<Operand *> argsInOrder;\n+\n+  PTXInstrExecution() = default;\n+  explicit PTXInstrExecution(PTXInstrCommon *instr,\n+                             llvm::ArrayRef<Operand *> oprs,\n+                             bool onlyAttachMLIRArgs)\n+      : argsInOrder(oprs.begin(), oprs.end()), instr(instr),\n+        onlyAttachMLIRArgs(onlyAttachMLIRArgs) {}\n+\n+  // Prefix a predicate to the instruction.\n+  PTXInstrExecution &predicate(mlir::Value value, StringRef constraint = \"b\") {\n+    pred = instr->builder->newOperand(value, constraint);\n+    return *this;\n+  }\n+\n+  // Prefix a !predicate to the instruction.\n+  PTXInstrExecution &predicateNot(mlir::Value value, StringRef constraint) {\n+    pred = instr->builder->newOperand(value, constraint);\n+    pred->repr = [](int idx) { return \"@!$\" + std::to_string(idx); };\n+    return *this;\n+  }\n+\n+  std::string dump() const;\n+\n+  SmallVector<Operand *> getArgList() const;\n+\n+  PTXInstrCommon *instr{};\n+  Operand *pred{};\n+  bool onlyAttachMLIRArgs{};\n+};\n+\n+/// ====== Some instruction wrappers ======\n+// We add the wrappers to make the usage more intuitive by avoiding mixing the\n+// PTX code with some trivial C++ code.\n+\n+struct PTXCpAsyncLoadInstr : PTXInstrBase<PTXCpAsyncLoadInstr> {\n+  explicit PTXCpAsyncLoadInstr(PTXBuilder *builder,\n+                               triton::CacheModifier modifier)\n+      : PTXInstrBase(builder, \"cp.async\") {\n+    o(triton::stringifyCacheModifier(modifier).str());\n+    o(\"shared\");\n+    o(\"global\");\n+  }\n+};\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/Passes.h", "status": "added", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -0,0 +1,16 @@\n+#ifndef TRITONGPU_CONVERSION_PASSES_H\n+#define TRITONGPU_CONVERSION_PASSES_H\n+\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n+\n+namespace mlir {\n+namespace triton {\n+\n+#define GEN_PASS_REGISTRATION\n+#include \"triton/Conversion/TritonGPUToLLVM/Passes.h.inc\"\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/Passes.td", "status": "added", "additions": 35, "deletions": 0, "changes": 35, "file_content_changes": "@@ -0,0 +1,35 @@\n+#ifndef TRITONGPU_CONVERSION_PASSES\n+#define TRITONGPU_CONVERSION_PASSES\n+\n+include \"mlir/Pass/PassBase.td\"\n+\n+\n+def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"> {\n+    let summary = \"Convert TritonGPU to LLVM\";\n+    let description = [{\n+\n+    }];\n+    let constructor = \"mlir::triton::createConvertTritonGPUToLLVMPass()\";\n+\n+    let dependentDialects = [\"mlir::arith::ArithDialect\",\n+                             \"mlir::math::MathDialect\",\n+                             \"mlir::gpu::GPUDialect\",\n+                             \"mlir::scf::SCFDialect\",\n+                             \"mlir::LLVM::LLVMDialect\",\n+                             \"mlir::tensor::TensorDialect\",\n+                             \"mlir::triton::TritonDialect\",\n+                             \"mlir::triton::gpu::TritonGPUDialect\",\n+                             \"mlir::ROCDL::ROCDLDialect\",\n+                             \"mlir::NVVM::NVVMDialect\"];\n+\n+    let options = [\n+        Option<\"computeCapability\", \"compute-capability\",\n+               \"int32_t\", /*default*/\"80\",\n+               \"device compute capability\">,\n+        Option<\"isROCM\", \"is-rocm\",\n+               \"bool\", /*default*/\"false\",\n+               \"compile for ROCM-compatible LLVM\">,\n+    ];\n+}\n+\n+#endif"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h", "status": "added", "additions": 23, "deletions": 0, "changes": 23, "file_content_changes": "@@ -0,0 +1,23 @@\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_PASS_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_PASS_H\n+\n+#include \"mlir/Conversion/LLVMCommon/TypeConverter.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include <memory>\n+\n+namespace mlir {\n+\n+class ModuleOp;\n+template <typename T> class OperationPass;\n+\n+namespace triton {\n+\n+std::unique_ptr<OperationPass<ModuleOp>>\n+createConvertTritonGPUToLLVMPass(int computeCapability = 80,\n+                                 bool isROCM = false);\n+\n+} // namespace triton\n+\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Conversion/TritonToTritonGPU/CMakeLists.txt", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -0,0 +1,3 @@\n+set(LLVM_TARGET_DEFINITIONS Passes.td)\n+mlir_tablegen(Passes.h.inc -gen-pass-decls --name TritonToTritonGPU)\n+add_public_tablegen_target(TritonConversionPassIncGen)"}, {"filename": "include/triton/Conversion/TritonToTritonGPU/Passes.h", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -0,0 +1,15 @@\n+#ifndef TRITON_CONVERSION_PASSES_H\n+#define TRITON_CONVERSION_PASSES_H\n+\n+#include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n+\n+namespace mlir {\n+namespace triton {\n+\n+#define GEN_PASS_REGISTRATION\n+#include \"triton/Conversion/TritonToTritonGPU/Passes.h.inc\"\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Conversion/TritonToTritonGPU/Passes.td", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -0,0 +1,27 @@\n+#ifndef TRITON_CONVERSION_PASSES\n+#define TRITON_CONVERSION_PASSES\n+\n+include \"mlir/Pass/PassBase.td\"\n+\n+def ConvertTritonToTritonGPU: Pass<\"convert-triton-to-tritongpu\", \"mlir::ModuleOp\"> {\n+    let summary = \"Convert Triton to TritonGPU\";\n+    let description = [{\n+\n+    }];\n+    let constructor = \"mlir::triton::createConvertTritonToTritonGPUPass()\";\n+\n+    let dependentDialects = [\"mlir::arith::ArithDialect\",\n+                             \"mlir::math::MathDialect\",\n+                             // TODO: Does this pass depend on SCF?\n+                             \"mlir::scf::SCFDialect\",\n+                             \"mlir::triton::TritonDialect\",\n+                             \"mlir::triton::gpu::TritonGPUDialect\"];\n+\n+   let options = [\n+       Option<\"numWarps\", \"num-warps\",\n+              \"int32_t\", /*default*/\"4\",\n+              \"number of warps\">\n+   ];\n+}\n+\n+#endif"}, {"filename": "include/triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h", "status": "added", "additions": 25, "deletions": 0, "changes": 25, "file_content_changes": "@@ -0,0 +1,25 @@\n+#ifndef TRITON_CONVERSION_TRITONTOTRITONGPU_TRITONTOTRITONGPUPASS_H\n+#define TRITON_CONVERSION_TRITONTOTRITONGPU_TRITONTOTRITONGPUPASS_H\n+\n+#include <memory>\n+\n+namespace mlir {\n+\n+class ModuleOp;\n+template <typename T> class OperationPass;\n+\n+namespace triton {\n+\n+constexpr static char AttrNumWarpsName[] = \"triton_gpu.num-warps\";\n+\n+// Create the pass with numWarps passed from cl::opt.\n+std::unique_ptr<OperationPass<ModuleOp>> createConvertTritonToTritonGPUPass();\n+\n+// Create the pass with numWarps set explicitly.\n+std::unique_ptr<OperationPass<ModuleOp>>\n+createConvertTritonToTritonGPUPass(int numWarps);\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Dialect/CMakeLists.txt", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -0,0 +1,2 @@\n+add_subdirectory(Triton)\n+add_subdirectory(TritonGPU)"}, {"filename": "include/triton/Dialect/Triton/CMakeLists.txt", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -0,0 +1,2 @@\n+add_subdirectory(IR)\n+add_subdirectory(Transforms)"}, {"filename": "include/triton/Dialect/Triton/IR/CMakeLists.txt", "status": "added", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -0,0 +1,19 @@\n+set(LLVM_TARGET_DEFINITIONS TritonOps.td)\n+mlir_tablegen(Ops.h.inc -gen-op-decls)\n+mlir_tablegen(Ops.cpp.inc -gen-op-defs)\n+mlir_tablegen(OpsEnums.h.inc -gen-enum-decls)\n+mlir_tablegen(OpsEnums.cpp.inc -gen-enum-defs)\n+\n+set(LLVM_TARGET_DEFINITIONS TritonDialect.td)\n+mlir_tablegen(Dialect.h.inc -gen-dialect-decls)\n+mlir_tablegen(Dialect.cpp.inc -gen-dialect-defs)\n+\n+set(LLVM_TARGET_DEFINITIONS TritonTypes.td)\n+mlir_tablegen(Types.h.inc -gen-typedef-decls)\n+mlir_tablegen(Types.cpp.inc -gen-typedef-defs)\n+\n+set(LLVM_TARGET_DEFINITIONS TritonInterfaces.td)\n+mlir_tablegen(AttrInterfaces.h.inc -gen-attr-interface-decls)\n+mlir_tablegen(AttrInterfaces.cpp.inc -gen-attr-interface-defs)\n+\n+add_public_tablegen_target(TritonTableGen)"}, {"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -0,0 +1,53 @@\n+#ifndef TRITON_DIALECT_TRITON_IR_DIALECT_H_\n+#define TRITON_DIALECT_TRITON_IR_DIALECT_H_\n+\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlow.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/Math/IR/Math.h\"\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/Dialect.h\"\n+#include \"mlir/Interfaces/ControlFlowInterfaces.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h.inc\"\n+#include \"triton/Dialect/Triton/IR/OpsEnums.h.inc\"\n+#include \"triton/Dialect/Triton/IR/Traits.h\"\n+#include \"triton/Dialect/Triton/IR/Types.h\"\n+\n+#define GET_OP_CLASSES\n+#include \"triton/Dialect/Triton/IR/Ops.h.inc\"\n+\n+namespace mlir {\n+namespace triton {\n+\n+class DialectInferLayoutInterface\n+    : public DialectInterface::Base<DialectInferLayoutInterface> {\n+public:\n+  DialectInferLayoutInterface(Dialect *dialect) : Base(dialect) {}\n+\n+  virtual LogicalResult\n+  inferTransOpEncoding(Attribute operandEncoding,\n+                       Attribute &resultEncoding) const = 0;\n+\n+  virtual LogicalResult\n+  inferReduceOpEncoding(Attribute operandEncoding, unsigned axis,\n+                        Attribute &resultEncoding) const = 0;\n+\n+  virtual LogicalResult\n+  inferExpandDimsOpEncoding(Attribute operandEncoding, unsigned axis,\n+                            Attribute &resultEncoding,\n+                            std::optional<Location> location) const = 0;\n+\n+  // Note: this function only verify operand encoding but doesn't infer result\n+  // encoding\n+  virtual LogicalResult\n+  inferDotOpEncoding(Attribute operandEncoding, unsigned opIdx,\n+                     Attribute retEncoding,\n+                     std::optional<Location> location) const = 0;\n+};\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif // TRITON_IR_DIALECT_H_"}, {"filename": "include/triton/Dialect/Triton/IR/Interfaces.h", "status": "added", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -0,0 +1,9 @@\n+#ifndef TRITON_IR_INTERFACES_H_\n+#define TRITON_IR_INTERFACES_H_\n+\n+#include \"mlir/IR/OpDefinition.h\"\n+\n+#define GET_TYPEDEF_CLASSES\n+#include \"triton/Dialect/Triton/IR/AttrInterfaces.h.inc\"\n+\n+#endif // TRITON_IR_TYPES_H_"}, {"filename": "include/triton/Dialect/Triton/IR/Traits.h", "status": "added", "additions": 110, "deletions": 0, "changes": 110, "file_content_changes": "@@ -0,0 +1,110 @@\n+#ifndef TRITON_IR_TRAITS_H_\n+#define TRITON_IR_TRAITS_H_\n+\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/OpDefinition.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+\n+#include <iostream>\n+\n+namespace mlir {\n+namespace OpTrait {\n+\n+// These functions are out-of-line implementations of the methods in the\n+// corresponding trait classes. This avoids them being template\n+// instantiated/duplicated.\n+namespace impl {\n+// The rationale for this trait is to prevent users from creating programs\n+// that would have catastrophic register pressure and cause the compiler to\n+// hang.\n+// Since H100 has 256KB registers, we should allow users to create tensors\n+// of size up to 256K elements. It will spill for datatypes wider than 1B,\n+// but we probably should limit number of elements (rather than bytes) to\n+// keep specs simple\n+int constexpr maxTensorNumElements = 1048576;\n+\n+LogicalResult verifyTensorSize(Operation *op);\n+\n+LogicalResult verifySameOperandsEncoding(Operation *op,\n+                                         bool allowTensorPointerType = false);\n+\n+LogicalResult\n+verifySameOperandsAndResultEncoding(Operation *op,\n+                                    bool allowTensorPointerType = false);\n+\n+LogicalResult verifySameLoadStoreOperandsShape(Operation *op);\n+\n+LogicalResult verifySameLoadStoreOperandsAndResultShape(Operation *op);\n+\n+bool verifyLoadStorePointerAndValueType(Type valueType, Type ptrType);\n+\n+} // namespace impl\n+\n+template <class ConcreteType>\n+class TensorSizeTrait : public TraitBase<ConcreteType, TensorSizeTrait> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifyTensorSize(op);\n+  }\n+};\n+\n+template <typename ConcreteType>\n+class SameOperandsAndResultEncoding\n+    : public TraitBase<ConcreteType, SameOperandsAndResultEncoding> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifySameOperandsAndResultEncoding(op);\n+  }\n+};\n+\n+template <typename ConcreteType>\n+class SameOperandsEncoding\n+    : public TraitBase<ConcreteType, SameOperandsEncoding> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifySameOperandsEncoding(op);\n+  }\n+};\n+\n+template <typename ConcreteType>\n+class SameLoadStoreOperandsShape\n+    : public TraitBase<ConcreteType, SameLoadStoreOperandsShape> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifySameLoadStoreOperandsShape(op);\n+  }\n+};\n+\n+template <typename ConcreteType>\n+class SameLoadStoreOperandsAndResultShape\n+    : public TraitBase<ConcreteType, SameLoadStoreOperandsAndResultShape> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifySameLoadStoreOperandsAndResultShape(op);\n+  }\n+};\n+\n+template <typename ConcreteType>\n+class SameLoadStoreOperandsEncoding\n+    : public TraitBase<ConcreteType, SameLoadStoreOperandsEncoding> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifySameOperandsEncoding(op,\n+                                            /*allowTensorPointerType=*/true);\n+  }\n+};\n+\n+template <typename ConcreteType>\n+class SameLoadStoreOperandsAndResultEncoding\n+    : public TraitBase<ConcreteType, SameLoadStoreOperandsAndResultEncoding> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifySameOperandsAndResultEncoding(\n+        op, /*allowTensorPointerType=*/true);\n+  }\n+};\n+\n+} // namespace OpTrait\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "added", "additions": 55, "deletions": 0, "changes": 55, "file_content_changes": "@@ -0,0 +1,55 @@\n+#ifndef TRITON_ATTR_DEFS\n+#define TRITON_ATTR_DEFS\n+\n+include \"mlir/IR/EnumAttr.td\"\n+\n+// Attributes for LoadOp\n+def TT_CacheModifierAttr : I32EnumAttr<\n+    \"CacheModifier\", \"\",\n+    [\n+        I32EnumAttrCase<\"NONE\", 1, \"none\">,\n+        I32EnumAttrCase<\"CA\", 2, \"ca\">,\n+        I32EnumAttrCase<\"CG\", 3, \"cg\">,\n+    ]> {\n+    let cppNamespace = \"::mlir::triton\";\n+}\n+\n+def TT_EvictionPolicyAttr : I32EnumAttr<\n+    \"EvictionPolicy\", \"\",\n+    [\n+        I32EnumAttrCase<\"NORMAL\", 1, \"evict_normal\">,\n+        I32EnumAttrCase<\"EVICT_FIRST\", 2, \"evict_first\">,\n+        I32EnumAttrCase<\"EVICT_LAST\", 3, \"evict_last\">\n+    ]> {\n+    let cppNamespace = \"::mlir::triton\";\n+}\n+\n+def TT_PaddingOptionAttr : I32EnumAttr<\n+    \"PaddingOption\", \"\",\n+    [\n+        I32EnumAttrCase<\"PAD_ZERO\", 1, \"zero\">,\n+        // We can not set the string value to \"NAN\" because it is a keyword in C++\n+        I32EnumAttrCase<\"PAD_NAN\", 2, \"nan\">\n+    ]> {\n+    let cppNamespace = \"::mlir::triton\";\n+}\n+\n+// atomic\n+def TT_AtomicRMWAttr : I32EnumAttr<\n+    \"RMWOp\", \"\",\n+    [\n+        I32EnumAttrCase<\"AND\", 1, \"and\">,\n+        I32EnumAttrCase<\"OR\", 2, \"or\">,\n+        I32EnumAttrCase<\"XOR\", 3, \"xor\">,\n+        I32EnumAttrCase<\"ADD\", 4, \"add\">,\n+        I32EnumAttrCase<\"FADD\", 5, \"fadd\">,\n+        I32EnumAttrCase<\"MAX\", 6, \"max\">,\n+        I32EnumAttrCase<\"MIN\", 7, \"min\">,\n+        I32EnumAttrCase<\"UMAX\", 8, \"umax\">,\n+        I32EnumAttrCase<\"UMIN\", 9, \"umin\">,\n+        I32EnumAttrCase<\"XCHG\", 10, \"exch\">\n+    ]> {\n+    let cppNamespace = \"::mlir::triton\";\n+}\n+\n+#endif"}, {"filename": "include/triton/Dialect/Triton/IR/TritonDialect.td", "status": "added", "additions": 45, "deletions": 0, "changes": 45, "file_content_changes": "@@ -0,0 +1,45 @@\n+#ifndef TRITON_DIALECT\n+#define TRITON_DIALECT\n+\n+include \"mlir/IR/OpBase.td\"\n+\n+def Triton_Dialect : Dialect {\n+  let name = \"tt\";\n+\n+  let cppNamespace = \"::mlir::triton\";\n+\n+  let summary = \"The Triton IR in MLIR\";\n+\n+  let description = [{\n+    Triton Dialect.\n+\n+    Dependent Dialects:\n+      * Arith:\n+        * addf, addi, andi, cmpf, cmpi, divf, fptosi, ...\n+      * Math:\n+        * exp, sin, cos, log, ...\n+      * StructuredControlFlow:\n+        * for, if, while, yield, condition\n+      * ControlFlow:\n+        * br, cond_br\n+  }];\n+\n+  let dependentDialects = [\n+    \"arith::ArithDialect\",\n+    \"math::MathDialect\",\n+    \"scf::SCFDialect\",\n+    \"cf::ControlFlowDialect\"\n+  ];\n+\n+  let extraClassDeclaration = [{\n+    void registerTypes();\n+  }];\n+\n+  let hasConstantMaterializer = 1;\n+  let useDefaultTypePrinterParser = 1;\n+}\n+\n+include \"triton/Dialect/Triton/IR/TritonTypes.td\"\n+\n+\n+#endif // TRITON_DIALECT"}, {"filename": "include/triton/Dialect/Triton/IR/TritonInterfaces.td", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -0,0 +1,14 @@\n+#ifndef TRITON_INTERFACES\n+#define TRITON_INTERFACES\n+\n+include \"mlir/IR/OpBase.td\"\n+\n+def TensorSizeTrait : NativeOpTrait<\"TensorSizeTrait\">;\n+def SameOperandsEncoding : NativeOpTrait<\"SameOperandsEncoding\">;\n+def SameOperandsAndResultEncoding : NativeOpTrait<\"SameOperandsAndResultEncoding\">;\n+def SameLoadStoreOperandsShape : NativeOpTrait<\"SameLoadStoreOperandsShape\">;\n+def SameLoadStoreOperandsAndResultShape : NativeOpTrait<\"SameLoadStoreOperandsAndResultShape\">;\n+def SameLoadStoreOperandsEncoding : NativeOpTrait<\"SameLoadStoreOperandsEncoding\">;\n+def SameLoadStoreOperandsAndResultEncoding : NativeOpTrait<\"SameLoadStoreOperandsAndResultEncoding\">;\n+\n+#endif // TRITON_INTERFACES"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "added", "additions": 735, "deletions": 0, "changes": 735, "file_content_changes": "@@ -0,0 +1,735 @@\n+#ifndef TRITON_OPS\n+#define TRITON_OPS\n+\n+include \"triton/Dialect/Triton/IR/TritonDialect.td\"\n+include \"triton/Dialect/Triton/IR/TritonTypes.td\"\n+include \"triton/Dialect/Triton/IR/TritonAttrDefs.td\"\n+include \"triton/Dialect/Triton/IR/TritonInterfaces.td\"\n+include \"mlir/IR/OpBase.td\"\n+include \"mlir/IR/FunctionInterfaces.td\" // FunctionOpInterface\n+include \"mlir/IR/SymbolInterfaces.td\" // SymbolUserOpInterface\n+include \"mlir/IR/OpAsmInterface.td\" // OpAsmOpInterface\n+include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n+include \"mlir/Interfaces/ControlFlowInterfaces.td\" // BranchOpInterface\n+include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n+include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n+include \"mlir/Interfaces/CastInterfaces.td\" // CastOpInterface\n+include \"mlir/Interfaces/CallInterfaces.td\" // CallOpInterface\n+\n+//\n+// Op Base\n+//\n+class TT_Op<string mnemonic, list<Trait> traits = []> :\n+    Op<Triton_Dialect, mnemonic, !listconcat(traits, [TensorSizeTrait])> {\n+}\n+\n+//\n+// Cast Ops\n+//\n+// Use cast ops in arith:\n+//   bitcast\n+//   fptoui, fptosi, uitofp, sitofp,\n+//   extf, tructf,\n+//   extui, extsi, tructi\n+def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n+                                         SameOperandsAndResultEncoding,\n+                                         Pure,\n+                                         /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n+    let summary = \"Cast int64 to pointer\";\n+\n+    let arguments = (ins TT_I64Like:$from);\n+\n+    let results = (outs TT_PtrLike:$result);\n+\n+    let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n+}\n+\n+def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n+                                         SameOperandsAndResultEncoding,\n+                                         Pure,\n+                                         /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n+    let summary = \"Cast pointer to int64\";\n+\n+    let arguments = (ins TT_PtrLike:$from);\n+\n+    let results = (outs TT_I64Like:$result);\n+\n+    let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n+}\n+\n+// arith.bitcast doesn't support pointers\n+def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n+                                     SameOperandsAndResultEncoding,\n+                                     Pure,\n+                                     /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n+    let summary = \"Cast between types of the same bitwidth\";\n+\n+    let arguments = (ins TT_Type:$from);\n+\n+    let results = (outs TT_Type:$result);\n+\n+    let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n+\n+    // TODO: Add verifier\n+}\n+\n+def TT_FpToFpOp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n+                                     SameOperandsAndResultEncoding,\n+                                     Pure,\n+                                     /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n+    let summary = \"Floating point casting for custom types\";\n+\n+    let description = [{\n+        Floating point casting for custom types (F8).\n+\n+        F8 <-> FP16, BF16, FP32, FP64\n+    }];\n+\n+    let arguments = (ins TT_FloatLike:$from);\n+\n+    let results = (outs TT_FloatLike:$result);\n+\n+    let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n+\n+    // TODO: We need a verifier here.\n+}\n+\n+//\n+// Pointer Arith Ops\n+//\n+def TT_AddPtrOp : TT_Op<\"addptr\",\n+                     [Pure,\n+                      SameOperandsAndResultShape,\n+                      SameOperandsAndResultEncoding,\n+                      TypesMatchWith<\"result type matches ptr type\",\n+                                     \"result\", \"ptr\", \"$_self\">]> {\n+    let arguments = (ins TT_PtrLike:$ptr, TT_IntLike:$offset);\n+\n+    let results = (outs TT_PtrLike:$result);\n+\n+    let assemblyFormat = \"$ptr `,` $offset attr-dict `:` type($result) `,` type($offset)\";\n+}\n+\n+def TT_AdvanceOp : TT_Op<\"advance\",\n+                         [Pure,\n+                          TypesMatchWith<\"result type matches ptr type\",\n+                                         \"result\", \"ptr\", \"$_self\">]> {\n+    let summary = \"Advance a tensor pointer by offsets\";\n+\n+    let arguments = (ins TT_TensorPtr:$ptr, Variadic<I32>:$offsets);\n+\n+    let results = (outs TT_TensorPtr:$result);\n+\n+    let assemblyFormat = \"$ptr `,` `[` $offsets `]` attr-dict `:` type($result)\";\n+}\n+\n+//\n+// Load/Store Ops\n+//\n+def TT_LoadOp : TT_Op<\"load\",\n+                      [SameLoadStoreOperandsAndResultShape,\n+                       SameLoadStoreOperandsAndResultEncoding,\n+                       AttrSizedOperandSegments,\n+                       MemoryEffects<[MemRead]>,\n+                       TypesMatchWith<\"infer ptr type from result type\",\n+                                      \"result\", \"ptr\", \"$_self\",\n+                                      \"mlir::OpTrait::impl::verifyLoadStorePointerAndValueType\">,\n+                       TypesMatchWith<\"infer mask type from result type or none\",\n+                                      \"result\", \"mask\", \"getI1SameShape($_self)\",\n+                                      \"($_op.getOperands().size() <= 1) || std::equal_to<>()\">,\n+                       TypesMatchWith<\"infer other type from result type or none\",\n+                                      \"result\", \"other\", \"$_self\",\n+                                      \"($_op.getOperands().size() <= 2) || std::equal_to<>()\">]> {\n+    let summary = \"Load from a tensor of pointers or from a tensor pointer\";\n+\n+    let arguments = (ins AnyTypeOf<[TT_PtrLike, TT_TensorPtr]>:$ptr, Optional<TT_BoolLike>:$mask,\n+                         Optional<TT_Type>:$other, OptionalAttr<DenseI32ArrayAttr>:$boundaryCheck,\n+                         OptionalAttr<TT_PaddingOptionAttr>:$padding, TT_CacheModifierAttr:$cache,\n+                         TT_EvictionPolicyAttr:$evict, BoolAttr:$isVolatile);\n+\n+    let results = (outs TT_Type:$result);\n+\n+    let builders = [\n+        // A tensor of pointers or a pointer to a scalar\n+        OpBuilder<(ins \"Value\":$ptr, \"triton::CacheModifier\":$cache,\n+                       \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n+        // A tensor pointer with boundary check and padding\n+        OpBuilder<(ins \"Value\":$ptr, \"ArrayRef<int32_t>\":$boundaryCheck,\n+                       \"Optional<triton::PaddingOption>\":$padding, \"triton::CacheModifier\":$cache,\n+                       \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n+        // A tensor of pointers or a pointer to a scalar with mask\n+        OpBuilder<(ins \"Value\":$ptr, \"Value\":$mask, \"triton::CacheModifier\":$cache,\n+                       \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n+        // A tensor of pointers or a pointer to a scalar with mask and other\n+        OpBuilder<(ins \"Value\":$ptr, \"Value\":$mask, \"Value\":$other, \"triton::CacheModifier\":$cache,\n+                       \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n+        // A utility function to build the operation with all attributes\n+        OpBuilder<(ins \"Value\":$ptr, \"Value\":$mask, \"Value\":$other, \"Optional<ArrayRef<int32_t>>\":$boundaryCheck,\n+                       \"Optional<triton::PaddingOption>\":$padding, \"triton::CacheModifier\":$cache,\n+                       \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>\n+    ];\n+\n+    // Format: `tt.load operands attrs : optional(type(ptr)) -> type(result)`\n+    // We need an extra `optional(type(ptr))` for inferring the tensor pointer type with back compatibility\n+    let hasCustomAssemblyFormat = 1;\n+\n+    let hasCanonicalizer = 1;\n+}\n+\n+def TT_StoreOp : TT_Op<\"store\",\n+                       [SameLoadStoreOperandsShape,\n+                        SameLoadStoreOperandsEncoding,\n+                        MemoryEffects<[MemWrite]>,\n+                        TypesMatchWith<\"infer ptr type from value type\",\n+                                       \"value\", \"ptr\", \"$_self\",\n+                                       \"mlir::OpTrait::impl::verifyLoadStorePointerAndValueType\">,\n+                        TypesMatchWith<\"infer mask type from value type\",\n+                                       \"value\", \"mask\", \"getI1SameShape($_self)\",\n+                                       \"($_op.getOperands().size() <= 2) || std::equal_to<>()\">]> {\n+    let summary = \"Store by a tensor of pointers or by a tensor pointer\";\n+\n+    let arguments = (ins AnyTypeOf<[TT_PtrLike, TT_TensorPtr]>:$ptr, TT_Type:$value, Optional<TT_BoolLike>:$mask,\n+                         OptionalAttr<DenseI32ArrayAttr>:$boundaryCheck,\n+                         DefaultValuedAttr<TT_CacheModifierAttr, \"triton::CacheModifier::NONE\">:$cache,\n+                         DefaultValuedAttr<TT_EvictionPolicyAttr, \"triton::EvictionPolicy::NORMAL\">:$evict);\n+\n+    let builders = [\n+        // A tensor of pointers or a pointer to a scalar\n+        OpBuilder<(ins \"Value\":$ptr, \"Value\":$value, \"triton::CacheModifier\":$cache, \"triton::EvictionPolicy\":$evict)>,\n+        // A tensor of pointers or a pointer to a scalar with mask\n+        OpBuilder<(ins \"Value\":$ptr, \"Value\":$value, \"Value\":$mask, \"triton::CacheModifier\":$cache,\n+                       \"triton::EvictionPolicy\":$evict)>,\n+        // A tensor pointer with boundary check\n+        OpBuilder<(ins \"Value\":$ptr, \"Value\":$value, \"ArrayRef<int32_t>\":$boundaryCheck, \"triton::CacheModifier\":$cache,\n+                       \"triton::EvictionPolicy\":$evict)>\n+    ];\n+\n+    // Format: `tt.store operands attrs : optional(type(ptr)), type(val)\n+    // We need an extra `optional(type(ptr))` for inferring the tensor pointer type with back compatibility\n+    let hasCustomAssemblyFormat = 1;\n+\n+    let hasCanonicalizer = 1;\n+}\n+\n+//\n+// Atomic Ops\n+//\n+def TT_AtomicRMWOp : TT_Op<\"atomic_rmw\", [SameOperandsAndResultShape,\n+                                          SameOperandsAndResultEncoding,\n+                                          MemoryEffects<[MemRead]>,\n+                                          MemoryEffects<[MemWrite]>,\n+                                          TypesMatchWith<\"infer ptr type from value type\",\n+                                                         \"val\", \"ptr\",\n+                                                         \"getPointerTypeSameShape($_self)\">,\n+                                          TypesMatchWith<\"infer mask type from value type\",\n+                                                         \"val\", \"mask\", \"getI1SameShape($_self)\",\n+                                                       \"($_op.getOperands().size() <= 2) || std::equal_to<>()\">]> {\n+    let summary = \"atomic rmw\";\n+\n+    let description = [{\n+        load data at $ptr, do $rmw_op with $val, and store result to $ptr.\n+\n+        return old value at $ptr\n+    }];\n+\n+    let arguments = (ins TT_AtomicRMWAttr:$atomic_rmw_op, TT_PtrLike:$ptr,\n+                         TT_Type:$val, Optional<TT_BoolLike>:$mask);\n+\n+    let results = (outs TT_Type:$result);\n+}\n+\n+def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [MemoryEffects<[MemRead]>,\n+                                          MemoryEffects<[MemWrite]>,\n+                                          SameOperandsAndResultShape,\n+                                          SameOperandsAndResultEncoding]> {\n+    let summary = \"atomic cas\";\n+\n+    let description = [{\n+        compare $cmp with data $old at location $ptr,\n+\n+        if $old == $cmp, store $val to $ptr,\n+\n+        else store $old to $ptr,\n+\n+        return $old\n+    }];\n+\n+    let arguments = (ins TT_PtrLike:$ptr, TT_Type:$cmp, TT_Type:$val);\n+\n+    let results = (outs TT_Type:$result);\n+}\n+\n+//\n+// Shape Manipulation Ops\n+//\n+def TT_SplatOp : TT_Op<\"splat\", [Pure,\n+                                 SameOperandsAndResultElementType,\n+                                 SameOperandsAndResultEncoding]> {\n+    let summary = \"splat\";\n+\n+    let arguments = (ins TT_Type:$src);\n+\n+    let results = (outs TT_Tensor:$result);\n+\n+    let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n+\n+    let hasFolder = 1;\n+}\n+\n+def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [Pure,\n+                                            DeclareOpInterfaceMethods<InferTypeOpInterface>,\n+                                            SameOperandsAndResultElementType]> {\n+    let summary = \"expand_dims\";\n+\n+    let arguments = (ins TT_Tensor:$src, I32Attr:$axis);\n+\n+    let results = (outs TT_Tensor:$result);\n+\n+    let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n+\n+    let hasCanonicalizeMethod = 1;\n+    let hasFolder = 1;\n+}\n+\n+// view is not `pure` because it may reorder elements\n+def TT_ViewOp : TT_Op<\"view\", [NoMemoryEffect,\n+                               SameOperandsAndResultElementType]> {\n+    let summary = \"view\";\n+\n+    let arguments = (ins TT_Tensor:$src);\n+\n+    let results = (outs TT_Tensor:$result);\n+\n+    let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n+\n+    let hasCanonicalizeMethod = 1;\n+    let hasFolder = 1;\n+}\n+\n+def TT_BroadcastOp : TT_Op<\"broadcast\", [Pure,\n+                                         SameOperandsAndResultElementType,\n+                                         SameOperandsAndResultEncoding]> {\n+    let summary = \"broadcast. No left-padding as of now.\";\n+\n+    let arguments = (ins TT_Type:$src);\n+\n+    let results = (outs TT_Type:$result);\n+\n+    let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n+\n+    let hasCanonicalizeMethod = 1;\n+    let hasFolder = 1;\n+}\n+\n+// cat is not `pure` because it may reorder elements\n+def TT_CatOp : TT_Op<\"cat\", [NoMemoryEffect,\n+                             SameOperandsAndResultElementType]> {\n+    let summary = \"concatenate 2 tensors\";\n+\n+    let arguments = (ins TT_Tensor:$lhs, TT_Tensor:$rhs);\n+\n+    let results = (outs TT_Tensor:$result);\n+\n+    let assemblyFormat = \"$lhs `,` $rhs attr-dict `:` functional-type(operands, results)\";\n+}\n+\n+def TT_TransOp : TT_Op<\"trans\", [Pure,\n+                                 DeclareOpInterfaceMethods<InferTypeOpInterface>,\n+                                 SameOperandsAndResultElementType]> {\n+\n+    let summary = \"transpose a tensor\";\n+\n+    let arguments = (ins TT_Tensor:$src);\n+\n+    let results = (outs TT_Tensor:$result);\n+\n+    let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n+}\n+\n+//\n+// SPMD Ops\n+//\n+def TT_GetProgramIdOp : TT_Op<\"get_program_id\", [Pure]> {\n+    let arguments = (ins I32Attr:$axis);\n+\n+    let results = (outs I32:$result);\n+\n+    let assemblyFormat = \"attr-dict `:` type($result)\";\n+}\n+\n+def TT_GetNumProgramsOp : TT_Op<\"get_num_programs\", [Pure]> {\n+    let arguments = (ins I32Attr:$axis);\n+\n+    let results = (outs I32:$result);\n+\n+    let assemblyFormat = \"attr-dict `:` type($result)\";\n+}\n+\n+//\n+// Dot Op\n+//\n+def TT_DotOp : TT_Op<\"dot\", [Pure,\n+                             DeclareOpInterfaceMethods<InferTypeOpInterface>,\n+                             TypesMatchWith<\"result's type matches accumulator's type\",\n+                                            \"d\", \"c\", \"$_self\">]> {\n+    let summary = \"dot\";\n+\n+    let description = [{\n+        $d = matrix_multiply($a, $b) + $c\n+    }];\n+\n+    let arguments = (ins TT_FpIntTensor:$a, TT_FpIntTensor:$b, TT_FpIntTensor:$c, BoolAttr:$allowTF32);\n+\n+    let results = (outs TT_FpIntTensor:$d);\n+\n+    let assemblyFormat = \"$a`,` $b`,` $c attr-dict `:` type($a) `*` type($b) `->` type($d)\";\n+}\n+\n+//\n+// Reduce Op\n+//\n+def TT_ReduceOp: TT_Op<\"reduce\",\n+                       [Pure,\n+                        SameOperandsEncoding,\n+                        SingleBlock,\n+                        DeclareOpInterfaceMethods<InferTypeOpInterface>]> {\n+    let summary = \"Reduction using generic combination algorithm\";\n+    let arguments = (ins Variadic<TT_Tensor>:$operands, I32Attr:$axis);\n+    let results = (outs Variadic<TT_Type>:$result);\n+    let regions = (region SizedRegion<1>:$combineOp);\n+    let builders = [\n+        OpBuilder<(ins \"ValueRange\":$operands, \"int\":$axis)>,\n+    ];\n+    let hasVerifier = 1;\n+    let hasRegionVerifier = 1;\n+    let extraClassDeclaration = [{\n+      llvm::SmallVector<RankedTensorType> getInputTypes();\n+      llvm::SmallVector<Type> getElementTypes();\n+      unsigned getNumOperands();\n+    }];\n+}\n+\n+def TT_ReduceReturnOp: TT_Op<\"reduce.return\",\n+                             [HasParent<\"ReduceOp\">, Pure, Terminator, ReturnLike]> {\n+    let summary = \"terminator for reduce operator\";\n+    let arguments = (ins Variadic<AnyType>:$result);\n+    let assemblyFormat = \"$result attr-dict `:` type($result)\";\n+}\n+\n+\n+//\n+// External Elementwise op\n+//\n+class TT_ExternElementwiseOpBase<string mnemonic, list<Trait> traits = []> :\n+    TT_Op<mnemonic,\n+         traits # [SameOperandsAndResultEncoding,\n+                   SameVariadicOperandSize]> {\n+\n+    let description = [{\n+        call an external function $symbol implemented in $libpath/$libname with $args\n+        return $libpath/$libname:$symbol($args...)\n+    }];\n+\n+    let arguments = (ins Variadic<TT_Type>:$args, StrAttr:$libname, StrAttr:$libpath, StrAttr:$symbol);\n+\n+    let results = (outs TT_Type:$result);\n+\n+    let assemblyFormat = \"operands attr-dict `:` functional-type(operands, $result)\";\n+}\n+\n+\n+def TT_PureExternElementwiseOp : TT_ExternElementwiseOpBase<\"pure_extern_elementwise\", [Pure, Elementwise]> {\n+    let summary = \"FFI for pure element-wise extern LLVM bitcode functions\";\n+}\n+\n+def TT_ImpureExternElementwiseOp : TT_ExternElementwiseOpBase<\"impure_extern_elementwise\", [MemoryEffects<[MemRead]>,\n+                                                                                            MemoryEffects<[MemWrite]>]> {\n+    let summary = \"FFI for impure element-wise extern LLVM bitcode functions\";\n+}\n+\n+//\n+// Make Range Op\n+//\n+// TODO: should have ConstantLike as Trait\n+def TT_MakeRangeOp : TT_Op<\"make_range\", [Pure]> {\n+    let summary = \"make range\";\n+\n+    let description = [{\n+        Returns an 1D int32 tensor.\n+\n+        Values span from $start to $end (exclusive), with step = 1\n+    }];\n+\n+    let arguments = (ins I32Attr:$start, I32Attr:$end);\n+\n+    let results = (outs TT_IntTensor:$result);\n+\n+    let assemblyFormat = \"attr-dict `:` type($result)\";\n+}\n+\n+//\n+// Print Op\n+//\n+def TT_PrintOp : TT_Op<\"print\", [MemoryEffects<[MemWrite]>]>,\n+  Arguments<(ins StrAttr:$prefix, Variadic<AnyTypeOf<[TT_Type]>>:$args)> {\n+  let summary = \"Device-side print, as in CUDA for debugging\";\n+  let description = [{\n+    `tt.print` takes a literal string prefix and an arbitrary number of scalar or tensor arguments that should be printed.\n+    format are generated automatically from the arguments.\n+  }];\n+  let assemblyFormat = [{\n+    $prefix attr-dict `:` ($args^ `:` type($args))?\n+  }];\n+}\n+\n+//\n+// Assert Op\n+//\n+def TT_AssertOp : TT_Op<\"assert\", [MemoryEffects<[MemWrite]>]> {\n+  let summary = \"Device-side assert, as in CUDA for correctness checking\";\n+  let description = [{\n+    `tt.assert` takes a condition tensor, a message string, a file string, a function string, and a line number.\n+    If the condition is false, the message is printed, and the program is aborted.\n+  }];\n+  let arguments = (ins TT_Tensor:$condition, StrAttr:$message, StrAttr:$file, StrAttr:$func, I32Attr:$line);\n+  let assemblyFormat = \"$condition `,` $message `,` $file `,` $func `,` $line attr-dict `:` type($condition)\";\n+}\n+\n+//\n+// Make Tensor Pointer Op\n+//\n+def TT_MakeTensorPtrOp : TT_Op<\"make_tensor_ptr\",\n+                               [Pure,\n+                                SameVariadicOperandSize,\n+                                TypesMatchWith<\"infer pointer type from the result type\",\n+                                               \"result\", \"base\",\n+                                               \"getPointerType(getElementTypeOfTensorPointerType($_self))\">]> {\n+  let summary = \"Make a tensor pointer type with meta information of the parent tensor and the block specified\";\n+\n+  let description = [{\n+      `tt.make_tensor_ptr` takes both meta information of the parent tensor and the block tensor, then it returns a\n+      pointer to the block tensor, e.g. returns a type of `tt.ptr<tensor<8x8xf16>>`.\n+  }];\n+\n+  // TODO(Chenggang): unify the integer types. Currently we cannot do that due to hardware constraints.\n+  let arguments = (ins\n+    TT_Ptr:$base,\n+    Variadic<I64>:$shape,\n+    Variadic<I64>:$strides,\n+    Variadic<I32>:$offsets,\n+    DenseI32ArrayAttr:$order\n+  );\n+\n+  let results = (outs TT_TensorPtr:$result);\n+\n+  // Add additional `[]` to increase readability and split variadic lists\n+  let assemblyFormat = \"$base `,` `[` $shape `]` `,` `[` $strides `]` `,` `[` $offsets `]` attr-dict `:` type($result)\";\n+\n+  let builders = [\n+    OpBuilder<(ins\n+        \"Value\":$base,\n+        \"ValueRange\":$shape,\n+        \"ValueRange\":$strides,\n+        \"ValueRange\":$offsets,\n+        \"ArrayRef<int32_t>\":$tensorShape,\n+        \"ArrayRef<int32_t>\":$order\n+    )>\n+  ];\n+}\n+\n+// The following ops, including `call`, `func`, and `return` are copied and modified from\n+// https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Func/IR/FuncOps.td\n+// We could revert it back once MLIR has a better inliner interface.\n+//\n+// Function Ops\n+//\n+def CallOp : TT_Op<\"call\", [CallOpInterface, /*MemRefsNormalizable, */DeclareOpInterfaceMethods<SymbolUserOpInterface>]> {\n+  let summary = \"call operation\";\n+  let description = [{\n+    The `tt.call` operation represents a direct call to a function that is\n+    within the same symbol scope as the call. The operands and result types of\n+    the call must match the specified function type. The callee is encoded as a\n+    symbol reference attribute named \"callee\".\n+\n+    Example:\n+\n+    ```mlir\n+    %2 = tt.call @my_add(%0, %1) : (f32, f32) -> f32\n+    ```\n+  }];\n+\n+  let arguments = (ins FlatSymbolRefAttr:$callee, Variadic<AnyType>:$operands);\n+  let results = (outs Variadic<AnyType>);\n+\n+  let builders = [\n+    OpBuilder<(ins \"FuncOp\":$callee, CArg<\"ValueRange\", \"{}\">:$operands), [{\n+      $_state.addOperands(operands);\n+      $_state.addAttribute(\"callee\", SymbolRefAttr::get(callee));\n+      $_state.addTypes(callee.getFunctionType().getResults());\n+    }]>,\n+    OpBuilder<(ins \"SymbolRefAttr\":$callee, \"TypeRange\":$results,\n+      CArg<\"ValueRange\", \"{}\">:$operands), [{\n+      $_state.addOperands(operands);\n+      $_state.addAttribute(\"callee\", callee);\n+      $_state.addTypes(results);\n+    }]>,\n+    OpBuilder<(ins \"StringAttr\":$callee, \"TypeRange\":$results,\n+      CArg<\"ValueRange\", \"{}\">:$operands), [{\n+      build($_builder, $_state, SymbolRefAttr::get(callee), results, operands);\n+    }]>,\n+    OpBuilder<(ins \"StringRef\":$callee, \"TypeRange\":$results,\n+      CArg<\"ValueRange\", \"{}\">:$operands), [{\n+      build($_builder, $_state, StringAttr::get($_builder.getContext(), callee),\n+            results, operands);\n+    }]>];\n+\n+  let extraClassDeclaration = [{\n+    FunctionType getCalleeType() {\n+      return FunctionType::get(getContext(), getOperandTypes(), getResultTypes());\n+    }\n+\n+    /// Get the argument operands to the called function.\n+    operand_range getArgOperands() {\n+      return {arg_operand_begin(), arg_operand_end()};\n+    }\n+\n+    operand_iterator arg_operand_begin() { return operand_begin(); }\n+    operand_iterator arg_operand_end() { return operand_end(); }\n+\n+    /// Return the callee of this operation.\n+    CallInterfaceCallable getCallableForCallee() {\n+      return (*this)->getAttrOfType<SymbolRefAttr>(\"callee\");\n+    }\n+  }];\n+\n+  let assemblyFormat = [{\n+    $callee `(` $operands `)` attr-dict `:` functional-type($operands, results)\n+  }];\n+}\n+\n+def FuncOp : TT_Op<\"func\", [AffineScope, AutomaticAllocationScope, CallableOpInterface, FunctionOpInterface, IsolatedFromAbove, OpAsmOpInterface]> {\n+  let summary = \"An operation with a name containing a single `SSACFG` region\";\n+  let description = [{\n+    Operations within the function cannot implicitly capture values defined\n+    outside of the function, i.e. Functions are `IsolatedFromAbove`. All\n+    external references must use function arguments or attributes that establish\n+    a symbolic connection (e.g. symbols referenced by name via a string\n+    attribute like SymbolRefAttr). An external function declaration (used when\n+    referring to a function declared in some other module) has no body. While\n+    the MLIR textual form provides a nice inline syntax for function arguments,\n+    they are internally represented as \u201cblock arguments\u201d to the first block in\n+    the region.\n+\n+    Only dialect attribute names may be specified in the attribute dictionaries\n+    for function arguments, results, or the function itself.\n+\n+    Example:\n+\n+    ```mlir\n+    // External function definitions.\n+    tt.func @abort()\n+    tt.func @scribble(i32, i64, memref<? x 128 x f32, #layout_map0>) -> f64\n+\n+    // A function that returns its argument twice:\n+    tt.func @count(%x: i64) -> (i64, i64)\n+      attributes {fruit: \"banana\"} {\n+      return %x, %x: i64, i64\n+    }\n+\n+    // A function with an argument attribute\n+    tt.func @example_fn_arg(%x: i32 {swift.self = unit})\n+\n+    // A function with a result attribute\n+    tt.func @example_fn_result() -> (f64 {dialectName.attrName = 0 : i64})\n+\n+    // A function with an attribute\n+    tt.func @example_fn_attr() attributes {dialectName.attrName = false}\n+    ```\n+  }];\n+\n+  let arguments = (ins SymbolNameAttr:$sym_name,\n+                       TypeAttrOf<FunctionType>:$function_type,\n+                       OptionalAttr<StrAttr>:$sym_visibility,\n+                       OptionalAttr<DictArrayAttr>:$arg_attrs,\n+                       OptionalAttr<DictArrayAttr>:$res_attrs);\n+  let regions = (region AnyRegion:$body);\n+\n+  let builders = [OpBuilder<(ins\n+    \"StringRef\":$name, \"FunctionType\":$type,\n+    CArg<\"ArrayRef<NamedAttribute>\", \"{}\">:$attrs,\n+    CArg<\"ArrayRef<DictionaryAttr>\", \"{}\">:$argAttrs)\n+  >];\n+  let extraClassDeclaration = [{\n+    //===------------------------------------------------------------------===//\n+    // CallableOpInterface\n+    //===------------------------------------------------------------------===//\n+\n+    /// Returns the region on the current operation that is callable. This may\n+    /// return null in the case of an external callable object, e.g. an external\n+    /// function.\n+    ::mlir::Region *getCallableRegion() { return isExternal() ? nullptr : &getBody(); }\n+\n+    /// Returns the results types that the callable region produces when\n+    /// executed.\n+    ArrayRef<Type> getCallableResults() { return getFunctionType().getResults(); }\n+\n+    /// Returns the argument attributes for all callable region arguments or\n+    /// null if there are none.\n+    ::mlir::ArrayAttr getCallableArgAttrs() {\n+      return getArgAttrs().value_or(nullptr);\n+    }\n+\n+    /// Returns the result attributes for all callable region results or\n+    /// null if there are none.\n+    ::mlir::ArrayAttr getCallableResAttrs() {\n+      return getResAttrs().value_or(nullptr);\n+    }\n+\n+    //===------------------------------------------------------------------===//\n+    // FunctionOpInterface Methods\n+    //===------------------------------------------------------------------===//\n+\n+    /// Returns the argument types of this function.\n+    ArrayRef<Type> getArgumentTypes() { return getFunctionType().getInputs(); }\n+\n+    /// Returns the result types of this function.\n+    ArrayRef<Type> getResultTypes() { return getFunctionType().getResults(); }\n+\n+    //===------------------------------------------------------------------===//\n+    // SymbolOpInterface Methods\n+    //===------------------------------------------------------------------===//\n+\n+    bool isDeclaration() { return isExternal(); }\n+  }];\n+  let hasCustomAssemblyFormat = 1;\n+}\n+\n+def ReturnOp : TT_Op<\"return\", [Pure, HasParent<\"FuncOp\">, /*MemRefsNormalizable, */ReturnLike, Terminator]> {\n+  let summary = \"Function return operation\";\n+  let description = [{\n+    The `tt.return` operation represents a return operation within a function.\n+    The operation takes variable number of operands and produces no results.\n+    The operand number and types must match the signature of the function\n+    that contains the operation.\n+\n+    Example:\n+\n+    ```mlir\n+    tt.func @foo() : (i32, f8) {\n+      ...\n+      tt.return %0, %1 : i32, f8\n+    }\n+    ```\n+  }];\n+\n+  let arguments = (ins Variadic<AnyType>:$operands);\n+\n+  let builders = [OpBuilder<(ins), [{\n+    build($_builder, $_state, std::nullopt);\n+  }]>];\n+\n+  let assemblyFormat = \"attr-dict ($operands^ `:` type($operands))?\";\n+  let hasVerifier = 1;\n+}\n+\n+#endif // Triton_OPS"}, {"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "added", "additions": 93, "deletions": 0, "changes": 93, "file_content_changes": "@@ -0,0 +1,93 @@\n+#ifndef TRITON_TYPES\n+#define TRITON_TYPES\n+\n+include \"mlir/IR/AttrTypeBase.td\"\n+include \"triton/Dialect/Triton/IR/TritonDialect.td\"\n+\n+//\n+// Types\n+//\n+class TritonTypeDef<string name, string _mnemonic>\n+    : TypeDef<Triton_Dialect, name> {\n+    // Used by printer/parser\n+    let mnemonic = _mnemonic;\n+}\n+\n+// Floating-point Type\n+def TT_Float : AnyTypeOf<[F8E4M3FN, F8E5M2, F16, BF16, F32, F64], \"floating-point\">;\n+def TT_FloatTensor : TensorOf<[TT_Float]>;\n+def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;\n+\n+// Boolean Type\n+// TT_Bool -> I1\n+def TT_BoolTensor : TensorOf<[I1]>;\n+def TT_BoolLike : AnyTypeOf<[I1, TT_BoolTensor]>;\n+\n+// Integer Type\n+def TT_Int : AnyTypeOf<[I1, I8, I16, I32, I64], \"integer\">;\n+def TT_IntTensor : TensorOf<[TT_Int]>;\n+def TT_IntLike : AnyTypeOf<[TT_Int, TT_IntTensor]>;\n+\n+// I32 Type\n+// TT_I32 -> I32\n+// TT_I32Tensor -> I32Tensor\n+def TT_I32Like : AnyTypeOf<[I32, I32Tensor]>;\n+\n+// I64 Type\n+// TT_I64 -> I64\n+// TT_I64Tensor -> I64Tensor\n+def TT_I64Like : AnyTypeOf<[I64, I64Tensor]>;\n+\n+// Pointer Type in TableGen\n+class TT_PtrOf<list<Type> pointeeTypes> :\n+    DialectType<Triton_Dialect,\n+                And<[CPred<\"$_self.isa<::mlir::triton::PointerType>()\">,\n+                     Concat<\"[](::mlir::Type pointeeType) { return \",\n+                            SubstLeaves<\"$_self\", \"pointeeType\", AnyTypeOf<pointeeTypes>.predicate>,\n+                                        \"; }($_self.cast<::mlir::triton::PointerType>().getPointeeType())\">]>,\n+                \"ptr\", \"::mlir::triton::PointerType\">;\n+\n+// Pointer Type in C++ (corresponding to `TT_PtrOf`)\n+def TT_PtrType : TritonTypeDef<\"Pointer\", \"ptr\"> {\n+    let summary = \"Pointer type (`::mlir::triton::PointerType`) in Triton IR type system\";\n+\n+    let description = [{\n+        Pointer type in Triton IR type system, which could be pointing to scalars or tensors.\n+    }];\n+\n+    let parameters = (ins \"Type\":$pointeeType, \"int\":$addressSpace);\n+\n+    let builders = [\n+        TypeBuilderWithInferredContext<(ins\n+            \"Type\":$pointeeType,\n+            \"int\":$addressSpace\n+        ), [{\n+            return $_get(pointeeType.getContext(), pointeeType, addressSpace);\n+        }]>\n+    ];\n+\n+    let hasCustomAssemblyFormat = 1;\n+\n+    let skipDefaultBuilders = 1;\n+}\n+\n+// Scalar Pointer Type: `ptr<>`\n+def TT_Ptr : TT_PtrOf<[AnyType]>;\n+\n+// Tensor of Pointer Type\n+def TT_PtrTensor : TensorOf<[TT_Ptr]>;\n+\n+// Tensor of Pointer Type or Pointer type: `tensor<ptr<>>` or `ptr<>`\n+def TT_PtrLike : AnyTypeOf<[TT_Ptr, TT_PtrTensor]>;\n+\n+// Tensor Type\n+def TT_FpIntTensor : AnyTypeOf<[TT_FloatTensor, TT_IntTensor]>;\n+def TT_Tensor : AnyTypeOf<[TT_FpIntTensor, TT_PtrTensor]>;\n+\n+// Pointer Type to Tensor Type: `ptr<tensor<>>`\n+def TT_TensorPtr : TT_PtrOf<[TT_Tensor]>;\n+\n+// Any Type in Triton IR\n+def TT_Type : AnyTypeOf<[TT_FloatLike, TT_IntLike, TT_PtrLike, TT_TensorPtr]>;\n+\n+#endif"}, {"filename": "include/triton/Dialect/Triton/IR/Types.h", "status": "added", "additions": 35, "deletions": 0, "changes": 35, "file_content_changes": "@@ -0,0 +1,35 @@\n+#ifndef TRITON_IR_TYPES_H_\n+#define TRITON_IR_TYPES_H_\n+\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/TypeSupport.h\"\n+#include \"mlir/IR/Types.h\"\n+\n+#define GET_TYPEDEF_CLASSES\n+#include \"triton/Dialect/Triton/IR/Types.h.inc\"\n+\n+namespace mlir {\n+\n+namespace triton {\n+\n+bool isTensorPointerType(Type type);\n+\n+unsigned getPointeeBitWidth(Type type);\n+\n+Type getPointeeType(Type type);\n+\n+Type getPointerType(Type type);\n+\n+Type getElementTypeOfTensorPointerType(Type type);\n+\n+Type getI1SameShape(Type type);\n+\n+Type getI32SameShape(Type type);\n+\n+Type getPointerTypeSameShape(Type type);\n+\n+} // namespace triton\n+\n+} // namespace mlir\n+\n+#endif // TRITON_IR_TYPES_H_"}, {"filename": "include/triton/Dialect/Triton/Transforms/CMakeLists.txt", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -0,0 +1,3 @@\n+set(LLVM_TARGET_DEFINITIONS Passes.td)\n+mlir_tablegen(Passes.h.inc -gen-pass-decls -name Triton)\n+add_public_tablegen_target(TritonTransformsIncGen)"}, {"filename": "include/triton/Dialect/Triton/Transforms/Passes.h", "status": "added", "additions": 21, "deletions": 0, "changes": 21, "file_content_changes": "@@ -0,0 +1,21 @@\n+#ifndef TRITON_DIALECT_TRITON_TRANSFORMS_PASSES_H_\n+#define TRITON_DIALECT_TRITON_TRANSFORMS_PASSES_H_\n+\n+#include \"mlir/Pass/Pass.h\"\n+\n+namespace mlir {\n+namespace triton {\n+\n+std::unique_ptr<Pass> createCombineOpsPass();\n+\n+std::unique_ptr<Pass>\n+createRewriteTensorPointerPass(int computeCapability = 80);\n+\n+} // namespace triton\n+\n+#define GEN_PASS_REGISTRATION\n+#include \"triton/Dialect/Triton/Transforms/Passes.h.inc\"\n+\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Dialect/Triton/Transforms/Passes.td", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "file_content_changes": "@@ -0,0 +1,41 @@\n+#ifndef TRITON_PASSES\n+#define TRITON_PASSES\n+\n+include \"mlir/Pass/PassBase.td\"\n+\n+def TritonCombineOps : Pass</*cli-arg*/\"triton-combine\", /*Op*/\"mlir::ModuleOp\"> {\n+  let summary = \"combine ops\";\n+  let description = [{\n+    dot(a, b, 0) + c => dot(a, b, c)\n+\n+    addptr(addptr(ptr, idx0), idx1) => addptr(ptr, AddI(idx0, idx1))\n+\n+    select(cond, load(ptrs, broadcast(cond), ???), other) =>\n+        load(ptrs, broadcast(cond), other)\n+  }];\n+\n+  let constructor = \"mlir::triton::createCombineOpsPass()\";\n+\n+  let dependentDialects = [\"mlir::arith::ArithDialect\"];\n+}\n+\n+def TritonRewriteTensorPointer : Pass</*cli-arg*/\"triton-rewrite-tensor-pointer\", /*Op*/\"mlir::ModuleOp\"> {\n+  let summary = \"Rewrite load/stores with tensor pointers into legacy load/stores\";\n+  let description = [{\n+    This pass rewrites all load/store semantics initiated by a `tt.make_tensor_ptr` and `tt.advance` into legacy\n+    semantics. After this pass, `tt.make_tensor_ptr` and `tt.advance` will disappear, and it generates logics to compute\n+    the pointer/mask/other for each load/store.\n+  }];\n+\n+  let constructor = \"mlir::triton::createRewriteTensorPointerPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::TritonDialect\"];\n+\n+  let options = [\n+    Option<\"computeCapability\", \"compute-capability\",\n+           \"int32_t\", /*default*/\"80\",\n+           \"device compute capability\">\n+  ];\n+}\n+\n+#endif"}, {"filename": "include/triton/Dialect/TritonGPU/CMakeLists.txt", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -0,0 +1,2 @@\n+add_subdirectory(IR)\n+add_subdirectory(Transforms)"}, {"filename": "include/triton/Dialect/TritonGPU/IR/CMakeLists.txt", "status": "added", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -0,0 +1,11 @@\n+set(LLVM_TARGET_DEFINITIONS TritonGPUOps.td)\n+mlir_tablegen(Dialect.h.inc -gen-dialect-decls -dialect=triton_gpu)\n+mlir_tablegen(Dialect.cpp.inc -gen-dialect-defs -dialect=triton_gpu)\n+mlir_tablegen(Ops.h.inc -gen-op-decls)\n+mlir_tablegen(Ops.cpp.inc -gen-op-defs)\n+add_public_tablegen_target(TritonGPUTableGen)\n+\n+set(LLVM_TARGET_DEFINITIONS TritonGPUAttrDefs.td)\n+mlir_tablegen(TritonGPUAttrDefs.h.inc -gen-attrdef-decls)\n+mlir_tablegen(TritonGPUAttrDefs.cpp.inc -gen-attrdef-defs)\n+add_public_tablegen_target(TritonGPUAttrDefsIncGen)"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "added", "additions": 48, "deletions": 0, "changes": 48, "file_content_changes": "@@ -0,0 +1,48 @@\n+#ifndef TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_\n+#define TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_\n+\n+#include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/Dialect.h\"\n+\n+// TritonGPU depends on Triton\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h.inc\"\n+#include \"triton/Dialect/TritonGPU/IR/Traits.h\"\n+\n+#define GET_ATTRDEF_CLASSES\n+#include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.h.inc\"\n+\n+#define GET_OP_CLASSES\n+#include \"triton/Dialect/TritonGPU/IR/Ops.h.inc\"\n+\n+namespace mlir {\n+namespace triton {\n+namespace gpu {\n+\n+unsigned getElemsPerThread(Type type);\n+\n+SmallVector<unsigned> getThreadsPerWarp(Attribute layout);\n+\n+SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n+\n+SmallVector<unsigned> getSizePerThread(Attribute layout);\n+\n+SmallVector<unsigned> getContigPerThread(Attribute layout);\n+\n+SmallVector<unsigned> getThreadsPerCTA(Attribute layout);\n+\n+SmallVector<unsigned>\n+getShapePerCTA(Attribute layout,\n+               ArrayRef<int64_t> tensorShape = ArrayRef<int64_t>());\n+\n+SmallVector<unsigned> getOrder(Attribute layout);\n+\n+bool isaDistributedLayout(Attribute layout);\n+\n+} // namespace gpu\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif // TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Traits.h", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "file_content_changes": "@@ -0,0 +1,31 @@\n+#ifndef TRITON_GPU_IR_TRAITS_H_\n+#define TRITON_GPU_IR_TRAITS_H_\n+\n+#include \"mlir/IR/OpDefinition.h\"\n+\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+\n+namespace mlir {\n+namespace OpTrait {\n+\n+// These functions are out-of-line implementations of the methods in the\n+// corresponding trait classes.  This avoids them being template\n+// instantiated/duplicated.\n+namespace impl {\n+LogicalResult verifyResultsAreSharedEncoding(Operation *op);\n+} // namespace impl\n+\n+template <typename ConcreteType>\n+class ResultsAreSharedEncoding\n+    : public TraitBase<ConcreteType, ResultsAreSharedEncoding> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifyResultsAreSharedEncoding(op);\n+  }\n+};\n+\n+} // namespace OpTrait\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "added", "additions": 526, "deletions": 0, "changes": 526, "file_content_changes": "@@ -0,0 +1,526 @@\n+#ifndef TRITONGPU_ATTRDEFS\n+#define TRITONGPU_ATTRDEFS\n+\n+include \"mlir/IR/AttrTypeBase.td\"\n+include \"triton/Dialect/TritonGPU/IR/TritonGPUDialect.td\"\n+include \"triton/Dialect/Triton/IR/TritonInterfaces.td\"\n+\n+//===----------------------------------------------------------------------===//\n+// TritonGPU Attribute Definitions\n+//===----------------------------------------------------------------------===//\n+\n+class TritonGPU_Attr<string name, list<Trait> traits = [],\n+                     string baseCppClass = \"::mlir::Attribute\">\n+  : AttrDef<TritonGPU_Dialect, name, traits, baseCppClass> {\n+\n+  let description = [{\n+TritonGPU Tensors differ from usual tensors in that they contain a _layout_ attribute which determines\n+how the data should be partitioned across CUDA threads. Formally speaking, we define a layout as a function\n+\\mathcal{L} that maps a multi-dimensional tensor index $i \\in \\mathbb{Z}^d$ to a set of integers T corresponding\n+to the indices of the CUDA threads allowed to access some data at index $i$.\n+\n+For example, let us consider the layout function:\n+\\mathcal{L}(0, 0) = {0, 4}\n+\\mathcal{L}(0, 1) = {1, 5}\n+\\mathcal{L}(1, 0) = {2, 6}\n+\\mathcal{L}(1, 1) = {3, 7}\n+\n+Then, attaching $\\mathcal{L} to a tensor $T$ would mean that:\n+- T[0,0] is owned by both cuda thread 0 and 4\n+- T[0,1] is owned by both cuda thread 1 and 5\n+- T[1,0] is owned by both cuda thread 2 and 6\n+- T[1,1] is owned by both cuda thread 3 and 7\n+\n+Right now, Triton implements two classes of layouts: shared, and distributed.\n+  }];\n+\n+  code extraBaseClassDeclaration = [{\n+    unsigned getElemsPerThread(ArrayRef<int64_t> shape, Type eltTy) const;\n+    ::mlir::LogicalResult verifyLayoutForArg(::mlir::Operation* op, unsigned argNo) const;\n+  }];\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// Shared Layout Encoding\n+//===----------------------------------------------------------------------===//\n+\n+def SharedEncodingAttr : TritonGPU_Attr<\"SharedEncoding\"> {\n+  let mnemonic = \"shared\";\n+\n+  let description = [{\n+An encoding for tensors whose elements may be simultaneously accessed by\n+different cuda threads in the programs, via shared memory. In other words,\n+for all indices i \\in R^d, \\mathcal{L}(i) = {0, 1, ..., 32*num_warps - 1}.\n+\n+In order to avoid shared memory bank conflicts, elements may be swizzled\n+in memory. For example, a swizzled row-major layout could store its data\n+as follows:\n+\n+A_{0, 0}  A_{0, 1}  A_{0, 2}  A_{0, 3} ...   [phase 0] \\ per_phase = 2\n+A_{1, 0}  A_{1, 1}  A_{1, 2}  A_{1, 3} ...   [phase 0] /\n+groups of vec=2 elements\n+are stored contiguously\n+_ _ _ _ /\\_ _ _ _\n+A_{2, 2}  A_{2, 3}  A_{2, 0}  A_{2, 1} ...   [phase 1] \\ per phase = 2\n+A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n+  }];\n+\n+  let parameters = (\n+    ins\n+    // swizzle info\n+    \"unsigned\":$vec, \"unsigned\":$perPhase, \"unsigned\":$maxPhase,\n+    ArrayRefParameter<\"unsigned\", \"order of axes by the rate of changing\">:$order\n+  );\n+\n+  let builders = [\n+    AttrBuilder<(ins \"DotOperandEncodingAttr\":$dotOpEnc,\n+                     \"ArrayRef<int64_t>\":$shape,\n+                     \"ArrayRef<unsigned>\":$order,\n+                     \"Type\":$eltTy), [{\n+        auto mmaEnc = dotOpEnc.getParent().dyn_cast<MmaEncodingAttr>();\n+\n+        if(!mmaEnc)\n+          return $_get(context, 1, 1, 1, order);\n+\n+        int opIdx = dotOpEnc.getOpIdx();\n+\n+        // number of rows per phase\n+        int perPhase = 128 / (shape[order[0]] * (eltTy.getIntOrFloatBitWidth() / 8));\n+        perPhase = std::max<int>(perPhase, 1);\n+\n+        // index of the inner dimension in `order`\n+        unsigned inner = (opIdx == 0) ? 0 : 1;\n+\n+        // ---- begin Volta ----\n+        if (mmaEnc.isVolta()) {\n+          bool is_row = order[0] != 0;\n+          bool is_vec4 = opIdx == 0 ? !is_row && (shape[order[0]] <= 16) :\n+              is_row && (shape[order[0]] <= 16);\n+          int pack_size = opIdx == 0 ? ((is_row || is_vec4) ? 1 : 2) :\n+                                       ((is_row && !is_vec4) ? 2 : 1);\n+          int rep = 2 * pack_size;\n+          int maxPhase = (order[inner] == 1 ? 8 : 4) / perPhase;\n+          int vec = 2 * rep;\n+          return $_get(context, vec, perPhase, maxPhase, order);\n+        }\n+\n+        // ---- begin Ampere ----\n+        if (mmaEnc.isAmpere()) {\n+          std::vector<size_t> matShape = {8, 8,\n+                                          2 * 64 / eltTy.getIntOrFloatBitWidth()};\n+          // for now, disable swizzle when using transposed int8 tensor cores\n+          if (eltTy.isInteger(8) && order[0] == inner)\n+            return $_get(context, 1, 1, 1, order);\n+\n+          // --- handle A operand ---\n+          if (opIdx == 0) { // compute swizzling for A operand\n+              int vec = (order[0] == 1) ? matShape[2] : matShape[0]; // k : m\n+              int mmaStride = (order[0] == 1) ? matShape[0] : matShape[2];\n+              int maxPhase = mmaStride / perPhase;\n+              return $_get(context, vec, perPhase, maxPhase, order);\n+          }\n+\n+          // --- handle B operand ---\n+          if (opIdx == 1) {\n+              int vec = (order[0] == 1) ? matShape[1] : matShape[2]; // n : k\n+              int mmaStride = (order[0] == 1) ? matShape[2] : matShape[1];\n+              int maxPhase = mmaStride / perPhase;\n+              return $_get(context, vec, perPhase, maxPhase, order);\n+          }\n+\n+          llvm_unreachable(\"invalid operand index\");\n+        }\n+\n+        // ---- not implemented ----\n+        llvm_unreachable(\"unsupported swizzling for provided MMA version\");\n+    }]>\n+  ];\n+\n+  let extraClassDeclaration = extraBaseClassDeclaration;\n+  let hasCustomAssemblyFormat = 1;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// Distributed Layout Encoding\n+//===----------------------------------------------------------------------===//\n+\n+class DistributedEncoding<string name> : TritonGPU_Attr<name> {\n+  let description = [{\n+Distributed encodings have a layout function that is entirely characterized\n+by a d-dimensional tensor L. Note that L doesn't need to have the same shape\n+(or even the same rank) as the tensor it is encoding.\n+\n+The layout function \\mathcal{L} of this layout is then defined, for an\n+index `i` \\in R^D, as follows:\n+\n+\\mathcal{L}(A)[i_d] = L[(i_d + k_d*A.shape[d]) % L.shape[d]] \\forall k_d such as i_d + k_d*A.shape[d] < L.shape[d]\n+\n+For example, for a tensor/layout pair\n+A = [x  x  x  x  x  x  x  x]\n+    [x  x  x  x  x  x  x  x]\n+L = [0  1  2  3 ]\n+    [4  5  6  7 ]\n+    [8  9  10 11]\n+    [12 13 14 15]\n+\n+Then the data of A would be distributed as follow between the 16 CUDA threads:\n+L(A) = [ {0,8} , {1,9} , {2,10}, {3,11}, {0,8} , {1, 9} , {2, 10}, {3, 11},\n+         {4,12}, {5,13}, {6,14}, {7,15}, {4,12}, {5, 13}, {6, 14}, {7, 15} ]\n+  }];\n+\n+  let extraClassDeclaration = extraBaseClassDeclaration;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// Blocked Layout Encoding\n+//===----------------------------------------------------------------------===//\n+\n+def BlockedEncodingAttr : DistributedEncoding<\"BlockedEncoding\"> {\n+  let mnemonic = \"blocked\";\n+\n+  let description = [{\n+An encoding where each warp owns a contiguous portion of the target tensor. This is typically the kind of data layout\n+used to promote memory coalescing in LoadInst and StoreInst.\n+It is characterized by three tuples -- thread tile size, warp tile size, and block tile size -- which\n+specify the amount of elements owned by each CUDA thread, warp and CTA respectively.\n+\n+For example, a row-major coalesced layout may partition a 16x16 tensor over 2 warps (i.e. 64 threads) as follows.\n+\n+[ 0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35 ]\n+[ 0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35 ]\n+[ 4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39 ]\n+[ 4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39 ]\n+...\n+[ 28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63 ]\n+[ 28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63 ]\n+\n+for\n+\n+#triton_gpu.blocked_layout<{\n+  sizePerThread = {2, 2}\n+  threadsPerWarp = {8, 4}\n+  warpsPerCTA = {1, 2}\n+}>\n+}];\n+\n+\n+  let builders = [\n+    // Custom builder initializes sizePerWarp and sizePerCTA automatically\n+    // TODO: compiles on MacOS but not linux?\n+    // AttrBuilder<(ins \"ArrayRef<unsigned>\":$sizePerThread,\n+    //                  \"ArrayRef<unsigned>\":$threadsPerWarp,\n+    //                  \"ArrayRef<unsigned>\":$warpsPerCTA,\n+    //                  \"ArrayRef<unsigned>\":$order), [{\n+    //   int rank = threadsPerWarp.size();\n+    //   SmallVector<unsigned, 4> sizePerWarp(rank);\n+    //   SmallVector<unsigned, 4> sizePerCTA(rank);\n+    //   for (unsigned i = 0; i < rank; i++) {\n+    //     sizePerWarp.push_back(sizePerThread[i] * threadsPerWarp[i]);\n+    //     sizePerCTA.push_back(sizePerWarp[i] * warpsPerCTA[i]);\n+    //   }\n+    //   return $_get(context, sizePerThread, threadsPerWarp, warpsPerCTA, order, sizePerWarp, sizePerCTA);\n+    // }]>,\n+    // Custom builder initializes sizePerWarp and sizePerCTA automatically\n+    // Default builder takes sizePerThread, order and numWarps, and tries to\n+    // pack numWarps*32 threads in the provided order for use in a type\n+    // of the given shape.\n+    AttrBuilder<(ins \"ArrayRef<int64_t>\":$shape,\n+                     \"ArrayRef<unsigned>\":$sizePerThread,\n+                     \"ArrayRef<unsigned>\":$order,\n+                     \"unsigned\":$numWarps), [{\n+      int rank = sizePerThread.size();\n+      unsigned remainingLanes = 32;\n+      unsigned remainingThreads = numWarps*32;\n+      unsigned remainingWarps = numWarps;\n+      unsigned prevLanes = 1;\n+      unsigned prevWarps = 1;\n+      SmallVector<unsigned, 4> threadsPerWarp(rank);\n+      SmallVector<unsigned, 4> warpsPerCTA(rank);\n+      for (int _dim = 0; _dim < rank - 1; ++_dim) {\n+        int i = order[_dim];\n+        unsigned threadsPerCTA = std::clamp<unsigned>(remainingThreads, 1, shape[i] / sizePerThread[i]);\n+        threadsPerWarp[i] = std::clamp<unsigned>(threadsPerCTA, 1, remainingLanes);\n+        warpsPerCTA[i] = std::clamp<unsigned>(threadsPerCTA / threadsPerWarp[i], 1, remainingWarps);\n+        remainingWarps /= warpsPerCTA[i];\n+        remainingLanes /= threadsPerWarp[i];\n+        remainingThreads /= threadsPerCTA;\n+        prevLanes *= threadsPerWarp[i];\n+        prevWarps *= warpsPerCTA[i];\n+      }\n+      // Expand the last dimension to fill the remaining lanes and warps\n+      threadsPerWarp[order[rank-1]] = 32 / prevLanes;\n+      warpsPerCTA[order[rank-1]] = numWarps / prevWarps;\n+\n+      return $_get(context, sizePerThread, threadsPerWarp, warpsPerCTA, order);\n+\n+    }]>\n+  ];\n+\n+  let extraClassDeclaration = extraBaseClassDeclaration # [{\n+    SliceEncodingAttr squeeze(int axis);\n+  }];\n+\n+  let parameters = (\n+    ins\n+    ArrayRefParameter<\"unsigned\">:$sizePerThread,\n+    ArrayRefParameter<\"unsigned\">:$threadsPerWarp,\n+    ArrayRefParameter<\"unsigned\">:$warpsPerCTA,\n+    // fastest-changing axis first\n+    ArrayRefParameter<\n+      \"unsigned\",\n+      \"order of axes by the rate of changing\"\n+    >:$order\n+    // These attributes can be inferred from the rest\n+    // ArrayRefParameter<\"unsigned\">:$sizePerWarp,\n+    // ArrayRefParameter<\"unsigned\">:$sizePerCTA\n+  );\n+\n+  let hasCustomAssemblyFormat = 1;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// MMA Layout Encoding\n+//===----------------------------------------------------------------------===//\n+// TODO: MMAv1 and MMAv2 should be two instances of the same class\n+\n+def MmaEncodingAttr : DistributedEncoding<\"MmaEncoding\"> {\n+  let mnemonic = \"mma\";\n+\n+  let description = [{\n+An encoding for tensors that have been produced by tensor cores.\n+It is characterized by two parameters:\n+- A 'versionMajor' which specifies the generation the tensor cores\n+whose output is being partitioned: 1 for first-gen tensor cores (Volta),\n+and 2 for second-gen tensor cores (Turing/Ampere).\n+- A 'versionMinor' which indicates the specific layout of a tensor core\n+generation, e.g. for Volta, there might be multiple kinds of layouts annotated\n+by 0,1,2 and so on.\n+- A `blockTileSize` to indicate how data should be\n+partitioned between warps.\n+\n+// -------------------------------- version = 1 --------------------------- //\n+\n+For first-gen tensor cores, the implicit warpTileSize is [16, 16].\n+Note: the layout is different from the recommended in PTX ISA\n+https://docs.nvidia.com/cuda/parallel-thread-execution/index.html\n+(mma.884 section, FP32 accumulator).\n+\n+For example, when versionMinor=1, the matrix L corresponding to\n+blockTileSize=[32,16] is:\n+\n+                               warp 0\n+--------------------------------/\\-------------------------------\n+[ 0   0   2   2   8   8   10  10   0   0   2   2   8   8   10  10 ]\n+[ 1   1   3   3   9   9   11  11   1   1   3   3   9   9   11  11 ]\n+[ 0   0   2   2   8   8   10  10   0   0   2   2   8   8   10  10 ]\n+[ 1   1   3   3   9   9   11  11   1   1   3   3   9   9   11  11 ]\n+[ 4   4   6   6   12  12  14  14   4   4   6   6   12  12  14  14 ]\n+[ 5   5   7   7   13  13  15  15   5   5   7   7   13  13  15  15 ]\n+[ 4   4   6   6   12  12  14  14   4   4   6   6   12  12  14  14 ]\n+[ 5   5   7   7   13  13  15  15   5   5   7   7   13  13  15  15 ]\n+[ 16  16  18  18  20  20  22  22   16  16  18  18  20  20  22  22 ]\n+[ 17  17  19  19  21  21  23  23   17  17  19  19  21  21  23  23 ]\n+[ 16  16  18  18  20  20  22  22   16  16  18  18  20  20  22  22 ]\n+[ 17  17  19  19  21  21  23  23   17  17  19  19  21  21  23  23 ]\n+[ 24  24  26  26  28  28  30  30   24  24  26  26  28  28  30  30 ]\n+[ 25  25  27  27  29  29  31  31   25  25  27  27  29  29  31  31 ]\n+[ 24  24  26  26  28  28  30  30   24  24  26  26  28  28  30  30 ]\n+[ 25  25  27  27  29  29  31  31   25  25  27  27  29  29  31  31 ]\n+\n+                          warp 1 = warp0 + 32\n+--------------------------------/\\-------------------------------\n+[ 32  32  34  34  40  40  42  42   32  32  34  34  40  40  42  42 ]\n+[ 33  33  35  35  41  41  43  43   33  33  35  35  41  41  43  43 ]\n+[ ............................................................... ]\n+\n+\n+// -------------------------------- version = 2 --------------------------- //\n+\n+For second-gen tensor cores, the implicit warpTileSize is [16, 8].\n+Information about this layout can be found in the official PTX documentation\n+https://docs.nvidia.com/cuda/parallel-thread-execution/index.html\n+(mma.16816 section, FP32 accumulator).\n+\n+For example, the matrix L corresponding to blockTileSize=[32,16] is:\n+                warp 0                          warp 1\n+-----------------/\\-------------  ----------------/\\-------------\n+[ 0   0   1   1   2   2   3   3   32  32  33  33  34  34  35  35\n+[ 4   4   5   5   6   6   7   7   36  36  37  37  38  38  39  39\n+[ ..............................  ..............................\n+[ 28  28  29  29  30  30  31  31  60  60  61  61  62  62  63  63\n+[ 0   0   1   1   2   2   3   3   32  32  33  33  34  34  35  35\n+[ 4   4   5   5   6   6   7   7   36  36  37  37  38  38  39  39\n+[ ..............................  ..............................\n+[ 28  28  29  29  30  30  31  31  60  60  61  61  62  62  63  63\n+\n+              warp 3                           warp 4\n+----------------/\\-------------   ----------------/\\-------------\n+[ 64  64  65  65  66  66  67  67  96  96  97  97  98  98  99  99\n+[ 68  68  69  69  70  70  71  71  100 100 101 101 102 102 103 103\n+[ ..............................  ...............................\n+[ 92  92  93  93  94  94  95  95  124 124 125 125 126 126 127 127\n+[ 64  64  65  65  66  66  67  67  96  96  97  97  98  98  99  99\n+[ 68  68  69  69  70  70  71  71  100 100 101 101 102 102 103 103\n+[ ..............................  ...............................\n+[ 92  92  93  93  94  94  95  95  124 124 125 125 126 126 127 127\n+\n+}];\n+\n+  let parameters = (\n+    ins\n+    \"unsigned\":$versionMajor,\n+    \"unsigned\":$versionMinor,\n+    ArrayRefParameter<\"unsigned\">:$warpsPerCTA\n+  );\n+\n+  let builders = [\n+    // Specially for MMAV1(Volta)\n+    AttrBuilder<(ins \"int\":$versionMajor,\n+                     \"int\":$numWarps,\n+                     \"ArrayRef<int64_t>\":$shapeC,\n+                     \"bool\":$isARow,\n+                     \"bool\":$isBRow,\n+                     \"bool\":$isAVec4,\n+                     \"bool\":$isBVec4,\n+                     \"int\":$id), [{\n+      assert(versionMajor == 1 && \"This builder is specially for versionMajor==1\");\n+      // 4-bits to encode 4 booleans: [isARow, isBRow, isAVec4, isBVec4]\n+      int versionMinor = (isARow * (1<<0)) |\\\n+                         (isBRow * (1<<1)) |\\\n+                         (isAVec4 * (1<<2)) |\\\n+                         (isBVec4 * (1<<3));\n+\n+\n+      // TODO: Share code with\n+      // DotOpMmaV1ConversionHelper::AParam/BParam, since same code to compute the\n+      // rep,spw and fpw.\n+      SmallVector<unsigned> wpt({1, 1});\n+      SmallVector<unsigned> wpt_nm1;\n+\n+      SmallVector<int, 2> rep(2), spw(2);\n+      std::array<int, 3> fpw{{2, 2, 1}};\n+      int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+      rep[0] = 2 * packSize0;\n+      spw[0] = fpw[0] * 4 * rep[0];\n+\n+      int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+      rep[1] = 2 * packSize1;\n+      spw[1] = fpw[1] * 4 * rep[1];\n+\n+      do {\n+        wpt_nm1 = wpt;\n+        if (wpt[0] * wpt[1] < numWarps)\n+          wpt[0] = std::clamp<int>(wpt[0] * 2, 1, shapeC[0] / spw[0]);\n+        if (wpt[0] * wpt[1] < numWarps)\n+          wpt[1] = std::clamp<int>(wpt[1] * 2, 1, shapeC[1] / spw[1]);\n+      } while (wpt_nm1 != wpt);\n+\n+      return $_get(context, versionMajor, versionMinor, wpt);\n+    }]>,\n+\n+\n+    AttrBuilder<(ins \"int\":$versionMajor,\n+                     \"int\":$numWarps,\n+                     \"ArrayRef<int64_t>\":$shapeA,\n+                     \"ArrayRef<int64_t>\":$shapeB,\n+                     \"ArrayRef<int64_t>\":$shapeC,\n+                     \"bool\":$isARow,\n+                     \"bool\":$isBRow,\n+                     \"int\":$id), [{\n+      assert(versionMajor == 1 && \"This builder is specially for versionMajor==1\");\n+      bool isAVec4 = !isARow && (shapeA[isARow] <= 16);\n+      bool isBVec4 = isBRow && (shapeB[isBRow] <= 16);\n+      return get(context, versionMajor, numWarps, shapeC, isARow, isBRow, isAVec4, isBVec4, id);\n+    }]>\n+  ];\n+\n+  let extraClassDeclaration = extraBaseClassDeclaration # [{\n+    bool isVolta() const;\n+    bool isAmpere() const;\n+    // Get [isARow, isBRow, isAVec4, isBVec4, id] from versionMinor\n+    std::tuple<bool, bool, bool, bool, int> decodeVoltaLayoutStates() const;\n+    // Number of bits in versionMinor to hold the ID of the MMA encoding instance.\n+    // Here 5 bits can hold 32 IDs in a single module.\n+    static constexpr int numBitsToHoldMmaV1ID{5};\n+  }];\n+\n+  let hasCustomAssemblyFormat = 1;\n+}\n+\n+def SliceEncodingAttr : DistributedEncoding<\"SliceEncoding\"> {\n+  let mnemonic = \"slice\";\n+\n+  let description = [{\n+    TODO: improve docs\n+\n+    A = [x  x  x  x  x  x  x  x]\n+\n+    parent = [0  1  2  3 ]\n+             [4  5  6  7 ]\n+             [8  9  10 11]\n+             [12 13 14 15]\n+    dim = 0\n+\n+    Then the data of A would be distributed as follow between the 16 CUDA threads:\n+    L(A) = [ {0,4,8,12} , {1,5,9,13} , ... {3,7,11,15}, {0,4,8,12} , ..., {3,7,11,15} ]\n+\n+    This is useful for constructing the inverse layout of an expand_dims operation during some optimization passes.\n+\n+  }];\n+\n+  let parameters = (\n+    ins\n+    \"unsigned\":$dim,\n+    // TODO: constraint here to only take distributed encodings\n+    \"Attribute\":$parent\n+  );\n+\n+  let extraClassDeclaration = extraBaseClassDeclaration # [{\n+    template<class T>\n+    SmallVector<T> paddedShape(ArrayRef<T> shape) const;\n+  }];\n+\n+  let hasCustomAssemblyFormat = 1;\n+}\n+\n+def DotOperandEncodingAttr : DistributedEncoding<\"DotOperandEncoding\"> {\n+  let mnemonic = \"dot_op\";\n+\n+  let description = [{\n+In TritonGPU dialect, considering `d = tt.dot a, b, c`\n+tt.dot's operands a and b must be of DotOperandEncodingAttr layout.\n+a's opIdx is 0, b's opIdx is 1.\n+The parend field in DotOperandEncodingAttr is the layout of d.\n+\n+For MMA v1, an additional attribute `isMMAv1Row` determines whether e.g. the a operand is used\n+in the context of an mma.884.row.col or an mma.884.col.col operation. See the PTX ISA documentation\n+section 9.7.13.4.1 for more details.\n+  }];\n+\n+  let parameters = (\n+    ins\n+    \"unsigned\":$opIdx,\n+    \"Attribute\":$parent\n+  );\n+\n+  let builders = [\n+  ];\n+\n+  let hasCustomAssemblyFormat = 1;\n+  let extraClassDeclaration = extraBaseClassDeclaration # [{\n+    bool getMMAv1IsRow() const;\n+    bool getMMAv1IsVec4() const;\n+    SmallVector<int> getMMAv1Rep() const;\n+    SmallVector<int> getMMAv1ShapePerWarp() const;\n+    int getMMAv1Vec() const;\n+    int getMMAv1NumOuter(ArrayRef<int64_t> shape) const;\n+    //\n+    SmallVector<int64_t> getMMAv2Rep(ArrayRef<int64_t> shape,\n+                                     int bitwidth) const;\n+\n+  }];\n+}\n+\n+\n+\n+#endif"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUDialect.td", "status": "added", "additions": 36, "deletions": 0, "changes": 36, "file_content_changes": "@@ -0,0 +1,36 @@\n+#ifndef TRITONGPU_DIALECT\n+#define TRITONGPU_DIALECT\n+\n+include \"mlir/IR/OpBase.td\"\n+\n+def TritonGPU_Dialect : Dialect {\n+  let name = \"triton_gpu\";\n+\n+  let cppNamespace = \"::mlir::triton::gpu\";\n+\n+  let hasOperationAttrVerify = 1;\n+\n+  let description = [{\n+    Triton GPU Dialect.\n+  }];\n+\n+  let dependentDialects = [\n+    \"triton::TritonDialect\",\n+    \"mlir::gpu::GPUDialect\",\n+    \"tensor::TensorDialect\",\n+  ];\n+\n+  let extraClassDeclaration = [{\n+    static std::string getNumWarpsAttrName() { return \"triton_gpu.num-warps\"; }\n+    static int getNumWarps(ModuleOp mod) {\n+      if(!mod->hasAttr(\"triton_gpu.num-warps\"))\n+        llvm::report_fatal_error(\n+            \"TritonGPU module should contain a triton_gpu.num-warps attribute\");\n+      return mod->getAttr(\"triton_gpu.num-warps\").cast<IntegerAttr>().getInt();\n+    }\n+  }];\n+\n+  let useDefaultAttributePrinterParser = 1;\n+}\n+\n+#endif"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "added", "additions": 277, "deletions": 0, "changes": 277, "file_content_changes": "@@ -0,0 +1,277 @@\n+#ifndef TRITONGPU_OPS\n+#define TRITONGPU_OPS\n+\n+include \"triton/Dialect/TritonGPU/IR/TritonGPUDialect.td\"\n+include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td\"\n+include \"mlir/Dialect/Arith/IR/ArithBase.td\"\n+include \"triton/Dialect/Triton/IR/TritonTypes.td\"\n+include \"triton/Dialect/Triton/IR/TritonAttrDefs.td\"\n+include \"mlir/IR/OpBase.td\"\n+include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n+include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n+include \"mlir/Interfaces/DestinationStyleOpInterface.td\"\n+include \"mlir/Interfaces/ViewLikeInterface.td\"\n+\n+def ResultsAreSharedEncoding: NativeOpTrait<\"ResultsAreSharedEncoding\">;\n+\n+class TTG_Op<string mnemonic, list<Trait> traits = []> :\n+    Op<TritonGPU_Dialect, mnemonic, traits>;\n+\n+def TTG_ConvertLayoutOp : TTG_Op<\"convert_layout\",\n+                                 [SameOperandsAndResultShape,\n+                                  SameOperandsAndResultElementType,\n+                                  Pure]> {\n+  let summary = \"convert layout\";\n+\n+  let arguments = (ins TT_Tensor:$src);\n+\n+  let results = (outs TT_Tensor:$result);\n+\n+  let hasCanonicalizeMethod = 1;\n+\n+  let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n+}\n+\n+def TTG_AsyncWaitOp : TTG_Op<\"async_wait\"> {\n+  let summary = \"async wait\";\n+\n+  let arguments = (ins I32Attr:$num);\n+\n+  let assemblyFormat = \"attr-dict\";\n+\n+  let extraClassDeclaration = [{\n+    static bool isSupported(int computeCapability) {\n+      return computeCapability >= 80;\n+    }\n+  }];\n+}\n+\n+def TTG_AsyncCommitGroupOp : TTG_Op<\"async_commit_group\"> {\n+  let summary = \"async commit group\";\n+\n+  let assemblyFormat = \"attr-dict\";\n+\n+  let extraClassDeclaration = [{\n+    static bool isSupported(int computeCapability) {\n+      return computeCapability >= 80;\n+    }\n+  }];\n+}\n+\n+\n+// Port Arith_CmpIOp & Arith_CmpFOp & Std_SelectOp to TritonGPU.\n+// This is needed because these ops don't\n+// handle encodings\n+// e.g., https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Arith/IR/ArithOps.td#L111\n+def TTG_CmpIOp : TTG_Op<\"cmpi\", [Pure, Elementwise,\n+                                 SameOperandsAndResultShape,\n+                                 SameOperandsAndResultEncoding]> {\n+  let summary = \"integer comparison operation\";\n+\n+  let description = [{}];\n+\n+  let arguments = (ins Arith_CmpIPredicateAttr:$predicate,\n+                       TT_IntLike:$lhs,\n+                       TT_IntLike:$rhs);\n+\n+  let results = (outs TT_BoolLike:$result);\n+}\n+\n+def TTG_CmpFOp : TTG_Op<\"cmpf\", [Pure, Elementwise,\n+                                 SameOperandsAndResultShape,\n+                                 SameOperandsAndResultEncoding]> {\n+  let summary = \"floating-point comparison operation\";\n+\n+  let description = [{}];\n+\n+  let arguments = (ins Arith_CmpFPredicateAttr:$predicate,\n+                       TT_FloatLike:$lhs,\n+                       TT_FloatLike:$rhs);\n+\n+  let results = (outs TT_BoolLike:$result);\n+}\n+\n+// TODO: migrate to arith::SelectOp on LLVM16\n+def TTG_SelectOp : TTG_Op<\"select\", [Pure, Elementwise,\n+                                     SameOperandsAndResultShape,\n+                                     SameOperandsAndResultEncoding]> {\n+  let summary = \"select operation\";\n+\n+  let description = [{}];\n+\n+  let arguments = (ins TT_BoolLike:$condition,\n+                       TT_Tensor:$true_value,\n+                       TT_Tensor:$false_value);\n+\n+  let results = (outs TT_Type:$result);\n+}\n+\n+\n+\n+def TTG_ExtractSliceOp : TTG_Op<\"extract_slice\",\n+                                [AttrSizedOperandSegments,\n+                                 ResultsAreSharedEncoding,\n+                                 Pure,\n+                                 OffsetSizeAndStrideOpInterface\n+                                 ]> {\n+  let summary = \"extract slice operation\";\n+  let description = [{\n+    same as tensor.extract_slice, but with int32 index. The motivations for re-implementing it are:\n+    We reimplement ExtractSliceOp with int32 index, because:\n+    - we want to enforce int32 indexing on GPUs since Triton tensors fit in SRAM\n+    - we still want to use indexWidth = 64 when lowering to LLVM because our loops can have\n+      64-bit induction variables and scf.for uses indexType for bounds/ivs\n+  }];\n+\n+  let arguments = (ins\n+    AnyRankedTensor:$source,\n+    Variadic<I32>:$offsets,\n+    Variadic<I32>:$sizes,\n+    Variadic<I32>:$strides,\n+    DenseI64ArrayAttr:$static_offsets,\n+    DenseI64ArrayAttr:$static_sizes,\n+    DenseI64ArrayAttr:$static_strides\n+  );\n+  let results = (outs AnyRankedTensor:$result);\n+\n+  let builders = [\n+    // Build an ExtractSliceOp with mixed static and dynamic entries and custom\n+    // result type. If the type passed is nullptr, it is inferred.\n+    OpBuilder<(ins \"RankedTensorType\":$resultType, \"Value\":$source,\n+      \"ArrayRef<OpFoldResult>\":$offsets, \"ArrayRef<OpFoldResult>\":$sizes,\n+      \"ArrayRef<OpFoldResult>\":$strides,\n+      CArg<\"ArrayRef<NamedAttribute>\", \"{}\">:$attrs)>,\n+  ];\n+\n+  let extraClassDeclaration = [{\n+    /// Return the number of leading operands before the `offsets`, `sizes` and\n+    /// and `strides` operands.\n+    static unsigned getOffsetSizeAndStrideStartOperandIndex() { return 1; }\n+\n+    /// Returns the type of the base tensor operand.\n+    RankedTensorType getSourceType() {\n+      return getSource().getType().cast<RankedTensorType>();\n+    }\n+\n+    std::array<unsigned, 3> getArrayAttrMaxRanks() {\n+      unsigned rank = getSourceType().getRank();\n+      return {rank, rank, rank};\n+    }\n+  }];\n+\n+  let assemblyFormat = [{\n+    $source ``\n+    custom<DynamicIndexList>($offsets, $static_offsets)\n+    custom<DynamicIndexList>($sizes, $static_sizes)\n+    custom<DynamicIndexList>($strides, $static_strides)\n+    attr-dict `:` type($source) `to` type($result)\n+  }];\n+}\n+\n+//\n+\n+def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n+                                    [AttrSizedOperandSegments,\n+                                     ResultsAreSharedEncoding,\n+                                     MemoryEffects<[MemRead]>,\n+                                     TypesMatchWith<\"infer mask type from src type\",\n+                                                    \"src\", \"mask\", \"getI1SameShape($_self)\",\n+                                                    \"($_op.getOperands().size() <= 3) || std::equal_to<>()\">,\n+                                     TypesMatchWith<\"infer other type from src type\",\n+                                                    \"src\", \"other\", \"getPointeeType($_self)\",\n+                                                    \"($_op.getOperands().size() <= 4) || std::equal_to<>()\">]> {\n+  let summary = \"insert slice async\";\n+\n+  let description = [{\n+      This operation inserts a tensor `$src` into another tensor `$dst` as specified by the operation\u2019s\n+      `$index` argument and `$axis` attribute.\n+\n+      It returns a copy of `$dst` with the proper slice updated asynchronously with the value of `$src`.\n+      This operation is non-blocking, and `$results` will have the updated value after the corresponding async_wait.\n+\n+      When converting from `tt.load` to `triton_gpu.insert_slice_async`, the `$evict`, `$cache`, and `$isVolatile` fields\n+      might be ignored on certain hardware. For example, on NVIDIA GPUs, the cache policy is determined by the backend,\n+      and `$evict` and `$isVolatile` are ignored because they apply to L1 cache only.\n+\n+      The insert_slice_async operation supports the following arguments:\n+\n+      * src: the tensor that is inserted.\n+      * dst: the tensor into which the `$src` tensor is inserted.\n+      * index: the index of the `$src` tensor at the given `$axis` from which the `$dst` tensor is inserted into\n+      * mask: optional tensor-rank number of boolean masks which specify which\n+              elements of the `$src` tensor are inserted into the `$dst` tensor.\n+      * other: optional tensor-rank number of other tensors which specify what\n+              values are inserted into the `$dst` tensor if the corresponding\n+              element of the `$mask` tensor is false.\n+\n+      In the future, we may decompose this operation into a sequence of:\n+\n+      * `async` operation to specify a sequence of asynchronous operations\n+      * `load` operation to load a tensor from global memory\n+      * `insert_slice` operations to insert the `$src` tensor into the `$dst` tensor\n+\n+      Example:\n+\n+      ```\n+      %1 = triton_gpu.alloc_tensor : tensor<2x32xf32>\n+      %2 = triton_gpu.insert_slice_async %0, %1, %index { axis = 0 } : tensor<32x!tt.ptr<f32>, #AL> -> tensor<2x32xf32, #A>\n+      triiton_gpu.async_wait { num = 0 : i32 }\n+      ```\n+  }];\n+\n+  let arguments = (ins TT_PtrTensor:$src, TT_Tensor:$dst, I32:$index,\n+                       Optional<I1Tensor>:$mask, Optional<TT_Type>:$other,\n+                       TT_CacheModifierAttr:$cache, TT_EvictionPolicyAttr:$evict,\n+                       BoolAttr:$isVolatile, I32Attr:$axis);\n+\n+  let builders = [\n+      OpBuilder<(ins \"Value\":$src, \"Value\":$dst, \"Value\":$index,\n+                     \"triton::CacheModifier\":$cache,\n+                     \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile, \"int\":$axis)>,\n+      OpBuilder<(ins \"Value\":$src, \"Value\":$dst, \"Value\":$index, \"Value\":$mask,\n+                     \"triton::CacheModifier\":$cache,\n+                     \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile, \"int\":$axis)>,\n+      OpBuilder<(ins \"Value\":$src, \"Value\":$dst, \"Value\":$index,\n+                     \"Value\":$mask, \"Value\":$other,\n+                     \"triton::CacheModifier\":$cache,\n+                     \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile, \"int\":$axis)>,\n+  ];\n+\n+  let results = (outs TT_Tensor:$result);\n+\n+  //let assemblyFormat = [{\n+  //  $src `,` $dst ``\n+  //  $index, $mask, $other\n+  //  attr-dict `:` type($src) `->` type($dst)\n+  //}];\n+\n+  let extraClassDeclaration = [{\n+    static DenseSet<unsigned> getEligibleLoadByteWidth(int computeCapability) {\n+      DenseSet<unsigned> validLoadBytes;\n+      if (computeCapability >= 80) {\n+        validLoadBytes = {4, 8, 16};\n+      }\n+      return validLoadBytes;\n+    }\n+  }];\n+\n+  let hasCustomAssemblyFormat = 1;\n+}\n+\n+def TTG_AllocTensorOp : TTG_Op<\"alloc_tensor\", [MemoryEffects<[MemAlloc]>,  // Allocate shared memory\n+                                                ResultsAreSharedEncoding]> {\n+  let summary = \"allocate tensor\";\n+\n+  let description = [{\n+    This operation defines a tensor of a particular shape.\n+    The contents of the tensor are supposed to be in shared memory.\n+\n+    Note: This op can be repalced to a `bufferization.alloc_tensor` in LLVM 16.\n+  }];\n+\n+  let assemblyFormat = [{attr-dict `:` type($result)}];\n+\n+  let results = (outs TT_Tensor:$result);\n+}\n+\n+#endif"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/CMakeLists.txt", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -0,0 +1,3 @@\n+set(LLVM_TARGET_DEFINITIONS Passes.td)\n+mlir_tablegen(Passes.h.inc -gen-pass-decls -name TritonGPU)\n+add_public_tablegen_target(TritonGPUTransformsIncGen)"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.h", "status": "added", "additions": 33, "deletions": 0, "changes": 33, "file_content_changes": "@@ -0,0 +1,33 @@\n+#ifndef TRITON_DIALECT_TRITONGPU_TRANSFORMS_PASSES_H_\n+#define TRITON_DIALECT_TRITONGPU_TRANSFORMS_PASSES_H_\n+\n+#include \"mlir/Pass/Pass.h\"\n+\n+namespace mlir {\n+std::unique_ptr<Pass> createTritonGPUPipelinePass(int numStages = 2);\n+\n+std::unique_ptr<Pass>\n+createTritonGPUAccelerateMatmulPass(int computeCapability = 80);\n+\n+std::unique_ptr<Pass> createTritonGPUPrefetchPass();\n+\n+std::unique_ptr<Pass> createTritonGPUCanonicalizeLoopsPass();\n+\n+std::unique_ptr<Pass> createTritonGPUCoalescePass();\n+\n+std::unique_ptr<Pass> createTritonGPUReorderInstructionsPass();\n+\n+std::unique_ptr<Pass> createTritonGPUDecomposeConversionsPass();\n+\n+std::unique_ptr<Pass> createTritonGPURemoveLayoutConversionsPass();\n+\n+std::unique_ptr<Pass> createTritonGPUVerifier();\n+\n+std::unique_ptr<Pass> createTritonGPUOptimizeDotOperandsPass();\n+\n+/// Generate the code for registering passes.\n+#define GEN_PASS_REGISTRATION\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n+\n+} // namespace mlir\n+#endif"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "added", "additions": 126, "deletions": 0, "changes": 126, "file_content_changes": "@@ -0,0 +1,126 @@\n+#ifndef TRITONGPU_PASSES\n+#define TRITONGPU_PASSES\n+\n+include \"mlir/Pass/PassBase.td\"\n+\n+def TritonGPUPipeline : Pass<\"tritongpu-pipeline\", \"mlir::ModuleOp\"> {\n+  let summary = \"pipeline\";\n+\n+  let description = [{\n+    Replace `LoadOp` in loops by `InsertSliceAsyncOp` instructions that asynchronously construct the data\n+    needed at the next iteration\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPUPipelinePass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::scf::SCFDialect\",\n+                           \"mlir::arith::ArithDialect\"];\n+\n+  let options = [\n+    Option<\"numStages\", \"num-stages\",\n+           \"int32_t\", /*default*/\"2\",\n+           \"number of pipeline stages\">\n+  ];\n+}\n+\n+def TritonGPUPrefetch : Pass<\"tritongpu-prefetch\", \"mlir::ModuleOp\"> {\n+  let summary = \"prefetch\";\n+\n+  let description = [{\n+    Decompose `DotOp` instructions in loops into several finer-grained `DotOp`\n+    that may have their operands constructed at the end of the previous iteration\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPUPrefetchPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::scf::SCFDialect\",\n+                           \"mlir::arith::ArithDialect\"];\n+}\n+\n+def TritonGPUAccelerateMatmul : Pass<\"tritongpu-accelerate-matmul\", \"mlir::ModuleOp\"> {\n+  let summary = \"accelerate matmul\";\n+\n+  let description = [{\n+    Optimize the input/output layout of `dot` instruction to make them compatible hardware accelerators\n+    (e.g., Nvidia tensor cores)\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPUAccelerateMatmulPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::TritonDialect\"];\n+\n+  let options = [\n+    Option<\"computeCapability\", \"compute-capability\",\n+           \"int32_t\", /*default*/\"80\",\n+           \"device compute capability\">\n+  ];\n+\n+}\n+\n+def TritonGPUOptimizeDotOperands : Pass<\"tritongpu-optimize-dot-operands\", \"mlir::ModuleOp\"> {\n+  let summary = \"fuse transpositions\";\n+\n+  let description = [{\n+    Re-arranged layouts of tensors used as matrix multiplication operands so as to promote the use of\n+    hardware-accelerated transpositions.\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPUOptimizeDotOperandsPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::TritonDialect\"];\n+}\n+\n+def TritonGPUCoalesce: Pass<\"tritongpu-coalesce\", \"mlir::ModuleOp\"> {\n+  let summary = \"coalesce\";\n+\n+  let description = [{\n+    TODO\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPUCoalescePass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n+}\n+\n+\n+def TritonGPURemoveLayoutConversions : Pass<\"tritongpu-remove-layout-conversions\", \"mlir::ModuleOp\"> {\n+  let summary = \"remove superfluous layout conversions\";\n+\n+  let description = [{\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPURemoveLayoutConversionsPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::TritonDialect\"];\n+}\n+\n+def TritonGPUReorderInstructions: Pass<\"tritongpu-reorder-instructions\", \"mlir::ModuleOp\"> {\n+  let summary = \"Reorder instructions\";\n+\n+  let description = \"This pass reorder instructions so as to (1) decrease register pressure (e.g., by moving \"\n+                    \"conversions from shared memory before their first use) and (2) promote LLVM instruction \"\n+                    \"order more friendly to `ptxas`.\";\n+\n+  let constructor = \"mlir::createTritonGPUReorderInstructionsPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::TritonDialect\"];\n+}\n+\n+def TritonGPUDecomposeConversions: Pass<\"tritongpu-decompose-conversions\", \"mlir::ModuleOp\"> {\n+  let summary = \"Decompose convert[distributed -> dotOperand] into convert[distributed -> shared -> dotOperand]\";\n+\n+  let description = \"Decomposing conversions this way makes it possible to use CSE and re-use #shared tensors\";\n+\n+  let constructor = \"mlir::createTritonGPUDecomposeConversionsPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::TritonDialect\"];\n+}\n+\n+#endif"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h", "status": "added", "additions": 33, "deletions": 0, "changes": 33, "file_content_changes": "@@ -0,0 +1,33 @@\n+//===----------------------------------------------------------------------===//\n+//\n+// Defines utilities to use while converting to the TritonGPU dialect.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#ifndef TRITON_DIALECT_TRITONGPU_TRANSFORMS_TRITONGPUCONVERSION_H_\n+#define TRITON_DIALECT_TRITONGPU_TRANSFORMS_TRITONGPUCONVERSION_H_\n+\n+#include \"mlir/Transforms/DialectConversion.h\"\n+\n+namespace mlir {\n+\n+class TritonGPUTypeConverter : public TypeConverter {\n+public:\n+  TritonGPUTypeConverter(MLIRContext *context, int numWarps);\n+  int getNumWarps() const { return numWarps; }\n+\n+private:\n+  MLIRContext *context;\n+  int numWarps;\n+};\n+\n+class TritonGPUConversionTarget : public ConversionTarget {\n+\n+public:\n+  explicit TritonGPUConversionTarget(MLIRContext &ctx,\n+                                     TritonGPUTypeConverter &typeConverter);\n+};\n+\n+} // namespace mlir\n+\n+#endif // TRITON_DIALECT_TRITONGPU_TRANSFORMS_TRITONGPUCONVERSION_H_"}, {"filename": "include/triton/Target/AMDGCN/AMDGCNTranslation.h", "status": "added", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -0,0 +1,19 @@\n+#ifndef TRITON_TARGET_AMDGCNTRANSLATION_H\n+#define TRITON_TARGET_AMDGCNTRANSLATION_H\n+\n+#include <string>\n+#include <tuple>\n+\n+namespace llvm {\n+class Module;\n+} // namespace llvm\n+\n+namespace triton {\n+\n+// Translate LLVM IR to AMDGCN code.\n+std::tuple<std::string, std::string>\n+translateLLVMIRToAMDGCN(llvm::Module &module, std::string cc);\n+\n+} // namespace triton\n+\n+#endif"}, {"filename": "include/triton/Target/HSACO/HSACOTranslation.h", "status": "added", "additions": 21, "deletions": 0, "changes": 21, "file_content_changes": "@@ -0,0 +1,21 @@\n+#ifndef TRITON_TARGET_HSACOTRANSLATION_H\n+#define TRITON_TARGET_HSACOTRANSLATION_H\n+\n+#include <memory>\n+#include <string>\n+#include <tuple>\n+\n+namespace llvm {\n+class Module;\n+} // namespace llvm\n+\n+namespace triton {\n+\n+// Translate TritonGPU IR to HSACO code.\n+std::tuple<std::string, std::string>\n+translateLLVMIRToHSACO(llvm::Module &module, std::string gfx_arch,\n+                       std::string gfx_triple, std::string gfx_features);\n+\n+} // namespace triton\n+\n+#endif"}, {"filename": "include/triton/Target/LLVMIR/LLVMIRTranslation.h", "status": "added", "additions": 39, "deletions": 0, "changes": 39, "file_content_changes": "@@ -0,0 +1,39 @@\n+#ifndef TRITON_TARGET_LLVMIRTRANSLATION_H\n+#define TRITON_TARGET_LLVMIRTRANSLATION_H\n+#include \"llvm/ADT/StringRef.h\"\n+#include <memory>\n+#include <string>\n+#include <vector>\n+\n+namespace llvm {\n+class Module;\n+class LLVMContext;\n+} // namespace llvm\n+\n+namespace mlir {\n+class ModuleOp;\n+} // namespace mlir\n+\n+namespace mlir {\n+namespace triton {\n+\n+// add external dependent libs\n+void addExternalLibs(mlir::ModuleOp &module,\n+                     const std::vector<std::string> &names,\n+                     const std::vector<std::string> &paths);\n+\n+// Translate TritonGPU dialect to LLVMIR, return null if failed.\n+std::unique_ptr<llvm::Module>\n+translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n+                           mlir::ModuleOp module, int computeCapability,\n+                           bool isROCM);\n+\n+// Translate mlir LLVM dialect to LLVMIR, return null if failed.\n+std::unique_ptr<llvm::Module>\n+translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n+                      bool isROCM);\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif // TRITON_TARGET_LLVMIRTRANSLATION_H"}, {"filename": "include/triton/Target/PTX/PTXTranslation.h", "status": "added", "additions": 17, "deletions": 0, "changes": 17, "file_content_changes": "@@ -0,0 +1,17 @@\n+#ifndef TRITON_TARGET_PTXTRANSLATION_H\n+#define TRITON_TARGET_PTXTRANSLATION_H\n+\n+#include <string>\n+\n+namespace llvm {\n+class Module;\n+} // namespace llvm\n+\n+namespace triton {\n+\n+// Translate TritonGPU IR to PTX code.\n+std::string translateLLVMIRToPTX(llvm::Module &module, int cc, int version);\n+\n+} // namespace triton\n+\n+#endif"}, {"filename": "include/triton/Tools/Sys/GetEnv.hpp", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -0,0 +1,53 @@\n+/*\n+ * Copyright (c) 2015, PHILIPPE TILLET. All rights reserved.\n+ *\n+ * This file is part of ISAAC.\n+ *\n+ * ISAAC is free software; you can redistribute it and/or\n+ * modify it under the terms of the GNU Lesser General Public\n+ * License as published by the Free Software Foundation; either\n+ * version 2.1 of the License, or (at your option) any later version.\n+ *\n+ * This library is distributed in the hope that it will be useful,\n+ * but WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+ * Lesser General Public License for more details.\n+ *\n+ * You should have received a copy of the GNU Lesser General Public\n+ * License along with this library; if not, write to the Free Software\n+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston,\n+ * MA 02110-1301  USA\n+ */\n+\n+#ifndef TDL_TOOLS_SYS_GETENV_HPP\n+#define TDL_TOOLS_SYS_GETENV_HPP\n+\n+#include <algorithm>\n+#include <cstdlib>\n+#include <string>\n+\n+namespace triton {\n+\n+namespace tools {\n+\n+inline std::string getenv(const char *name) {\n+  const char *cstr = std::getenv(name);\n+  if (!cstr)\n+    return \"\";\n+  std::string result(cstr);\n+  return result;\n+}\n+\n+inline bool getBoolEnv(const std::string &env) {\n+  const char *s = std::getenv(env.c_str());\n+  std::string str(s ? s : \"\");\n+  std::transform(str.begin(), str.end(), str.begin(),\n+                 [](unsigned char c) { return std::tolower(c); });\n+  return (str == \"on\" || str == \"true\" || str == \"1\");\n+}\n+\n+} // namespace tools\n+\n+} // namespace triton\n+\n+#endif"}, {"filename": "include/triton/Tools/Sys/GetPlatform.hpp", "status": "renamed", "additions": 10, "deletions": 21, "changes": 31, "file_content_changes": "@@ -19,29 +19,18 @@\n  * MA 02110-1301  USA\n  */\n \n-#ifndef TDL_TOOLS_SYS_GETENV_HPP\n-#define TDL_TOOLS_SYS_GETENV_HPP\n+#ifndef TDL_TOOLS_SYS_GETPLATFORM_HPP\n+#define TDL_TOOLS_SYS_GETPLATFORM_HPP\n \n-#include <string>\n+#include <algorithm>\n #include <cstdlib>\n+#include <iostream>\n+#include <map>\n+#include <memory>\n+#include <string>\n \n-namespace triton\n-{\n-\n-namespace tools\n-{\n-\n-    inline std::string getenv(const char * name)\n-    {\n-        const char * cstr = std::getenv(name);\n-        if(!cstr)\n-            return \"\";\n-        std::string result(cstr);\n-        return result;\n-    }\n-\n-}\n-\n-}\n+// inline bool _isROCM = false;\n+// inline void setROCM() { _isROCM = true; }\n+// inline bool isROCM() { return _isROCM; }\n \n #endif"}, {"filename": "include/triton/codegen/analysis/align.h", "status": "removed", "additions": 0, "deletions": 83, "changes": 83, "file_content_changes": "@@ -1,83 +0,0 @@\n-#ifndef TDL_INCLUDE_CODEGEN_ALIGNMENT_INFO_PASS_H\n-#define TDL_INCLUDE_CODEGEN_ALIGNMENT_INFO_PASS_H\n-\n-#include <map>\n-#include <vector>\n-\n-namespace triton {\n-\n-namespace ir {\n-  class value;\n-  class module;\n-  class phi_node;\n-  class splat_inst;\n-  class cast_inst;\n-  class cmp_inst;\n-  class reshape_inst;\n-  class broadcast_inst;\n-  class binary_operator;\n-  class getelementptr_inst;\n-}\n-\n-namespace codegen{\n-namespace analysis{\n-\n-class align {\n-private:\n-  struct cst_info {\n-    unsigned num_cst;\n-    unsigned value;\n-  };\n-  // helpers\n-  std::vector<unsigned> get_shapes(ir::value *v);\n-  // populate is_constant\n-  std::vector<cst_info> populate_is_constant_phi(ir::phi_node* x);\n-  std::vector<cst_info> populate_is_constant_splat(ir::splat_inst* x);\n-  std::vector<cst_info> populate_is_constant_reshape(ir::reshape_inst* x);\n-  std::vector<cst_info> populate_is_constant_broadcast(ir::broadcast_inst* x);\n-  std::vector<cst_info> populate_is_constant_binop(ir::binary_operator* x);\n-  std::vector<cst_info> populate_is_constant_cmp(ir::cmp_inst* x);\n-  std::vector<cst_info> populate_is_constant_gep(ir::getelementptr_inst* x);\n-  std::vector<cst_info> populate_is_constant_default(ir::value* v);\n-  std::vector<cst_info> populate_is_constant(ir::value *v);\n-  // populate max_contiguous\n-  std::vector<unsigned> populate_max_contiguous_phi(ir::phi_node* x);\n-  std::vector<unsigned> populate_max_contiguous_splat(ir::splat_inst* x);\n-  std::vector<unsigned> populate_max_contiguous_reshape(ir::reshape_inst* x);\n-  std::vector<unsigned> populate_max_contiguous_broadcast(ir::broadcast_inst* x);\n-  std::vector<unsigned> populate_max_contiguous_binop(ir::binary_operator* x);\n-  std::vector<unsigned> populate_max_contiguous_gep(ir::getelementptr_inst* x);\n-  std::vector<unsigned> populate_max_contiguous_cast(ir::cast_inst* x);\n-  std::vector<unsigned> populate_max_contiguous_default(ir::value* v);\n-  std::vector<unsigned> populate_max_contiguous(ir::value *v);\n-  // populate starting_multiple\n-  std::vector<unsigned> populate_starting_multiple_phi(ir::phi_node* x);\n-  std::vector<unsigned> populate_starting_multiple_splat(ir::splat_inst* x);\n-  std::vector<unsigned> populate_starting_multiple_reshape(ir::reshape_inst* x);\n-  std::vector<unsigned> populate_starting_multiple_broadcast(ir::broadcast_inst* x);\n-  std::vector<unsigned> populate_starting_multiple_binop(ir::binary_operator* x);\n-  std::vector<unsigned> populate_starting_multiple_gep(ir::getelementptr_inst* x);\n-  std::vector<unsigned> populate_starting_multiple_cast(ir::cast_inst* x);\n-  std::vector<unsigned> populate_starting_multiple_default(ir::value* v);\n-  std::vector<unsigned> populate_starting_multiple(ir::value *v);\n-  // populate all maps\n-  void populate(ir::value *v);\n-\n-public:\n-  void run(ir::module &mod);\n-  unsigned get(ir::value* v, unsigned ax) const;\n-  std::vector<unsigned> contiguous(ir::value* v) const;\n-  std::vector<cst_info> get_cst_info(ir::value* v) const;\n-\n-private:\n-  std::map<ir::value*, std::vector<cst_info>> is_constant_;\n-  std::map<ir::value*, std::vector<unsigned>> max_contiguous_;\n-  std::map<ir::value*, std::vector<unsigned>> starting_multiple_;\n-};\n-\n-\n-}\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/codegen/analysis/allocation.h", "status": "removed", "additions": 0, "deletions": 47, "changes": 47, "file_content_changes": "@@ -1,47 +0,0 @@\n-#ifndef TDL_INCLUDE_IR_CODEGEN_STORAGE_ALLOC_H\n-#define TDL_INCLUDE_IR_CODEGEN_STORAGE_ALLOC_H\n-\n-#include <map>\n-#include <set>\n-#include <iostream>\n-#include \"triton/codegen/analysis/liveness.h\"\n-\n-namespace triton{\n-\n-namespace ir{\n-  class value;\n-  class function;\n-  class module;\n-}\n-\n-namespace codegen{\n-namespace analysis{\n-\n-class tiles;\n-\n-class liveness;\n-class cts;\n-\n-class allocation {\n-public:\n-  allocation(liveness *live)\n-    : liveness_(live) { }\n-  // accessors\n-  bool has_offset(const data_layout *x)    const { return offsets_.find(x) != offsets_.end(); }\n-  unsigned offset(const data_layout *x)    const { return offsets_.at(x); }\n-  unsigned allocated_size()        const { return allocated_size_; }\n-  // run\n-  void run(ir::module& mod);\n-\n-private:\n-  std::map<const data_layout*, unsigned> offsets_;\n-  size_t allocated_size_;\n-  // dependences\n-  liveness *liveness_;\n-};\n-\n-}\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/codegen/analysis/axes.h", "status": "removed", "additions": 0, "deletions": 52, "changes": 52, "file_content_changes": "@@ -1,52 +0,0 @@\n-#ifndef _TRITON_CODEGEN_ANALYSIS_AXES_H_\n-#define _TRITON_CODEGEN_ANALYSIS_AXES_H_\n-\n-#include \"triton/tools/graph.h\"\n-#include <map>\n-#include <vector>\n-\n-namespace triton{\n-\n-namespace ir{\n-  class value;\n-  class module;\n-  class instruction;\n-}\n-\n-namespace codegen{\n-namespace analysis{\n-\n-class axes {\n-  typedef std::pair<ir::value*, unsigned> node_t;\n-\n-private:\n-  // update graph\n-  void update_graph_store(ir::instruction *i);\n-  void update_graph_reduce(ir::instruction *i);\n-  void update_graph_reshape(ir::instruction *i);\n-  void update_graph_trans(ir::instruction *i);\n-  void update_graph_broadcast(ir::instruction *i);\n-  void update_graph_dot(ir::instruction *i);\n-  void update_graph_elementwise(ir::instruction *i,\n-                                bool is_masked_load_async=false);\n-  void update_graph_no_edge(ir::instruction *i);\n-  void update_graph(ir::instruction *i);\n-\n-public:\n-  axes();\n-  void run(ir::module &mod);\n-  // accessors\n-  int get(ir::value *value, unsigned dim);\n-  std::vector<int> get(ir::value *value);\n-\n-private:\n-  tools::graph<node_t> graph_;\n-  std::map<node_t, size_t> axes_;\n-};\n-\n-}\n-}\n-\n-}\n-\n-#endif"}, {"filename": "include/triton/codegen/analysis/layout.h", "status": "removed", "additions": 0, "deletions": 346, "changes": 346, "file_content_changes": "@@ -1,346 +0,0 @@\n-#ifndef _TRITON_CODEGEN_ANALYSIS_GRID_H_\n-#define _TRITON_CODEGEN_ANALYSIS_GRID_H_\n-\n-#include <map>\n-#include <set>\n-#include <vector>\n-#include <memory>\n-#include \"triton/tools/graph.h\"\n-#include \"triton/codegen/target.h\"\n-\n-namespace triton{\n-\n-namespace ir{\n-  class value;\n-  class type;\n-  class module;\n-  class instruction;\n-  class phi_node;\n-}\n-\n-namespace codegen{\n-namespace analysis{\n-\n-class axes;\n-class align;\n-class layout_visitor;\n-class data_layout;\n-class mma_layout;\n-class scanline_layout;\n-class shared_layout;\n-\n-\n-class layout_visitor {\n-public:\n-  virtual void visit_layout(data_layout *);\n-  virtual void visit_layout_mma(mma_layout*) = 0;\n-  virtual void visit_layout_scanline(scanline_layout*) = 0;\n-  virtual void visit_layout_shared(shared_layout*) = 0;\n-};\n-\n-class data_layout {\n-protected:\n-  enum id_t {\n-    MMA,\n-    SCANLINE,\n-    SHARED\n-  };\n-\n-  typedef std::vector<int> axes_t;\n-  typedef std::vector<unsigned> shape_t;\n-  typedef std::vector<int> order_t;\n-  typedef std::vector<ir::value*> values_t;\n-\n-private:\n-  template<typename T>\n-  T* downcast(id_t id) {\n-    if(id_ == id)\n-      return static_cast<T*>(this);\n-    return nullptr;\n-  }\n-\n-public:\n-  data_layout(id_t id,\n-             const std::vector<int>& axes,\n-             const std::vector<unsigned> &shape,\n-             const std::vector<ir::value *> &values,\n-             analysis::align* align);\n-  // visitor\n-  virtual void accept(layout_visitor* vst) = 0;\n-  // downcast\n-  mma_layout* to_mma()          { return downcast<mma_layout>(MMA); }\n-  scanline_layout* to_scanline()      { return downcast<scanline_layout>(SCANLINE); }\n-  shared_layout* to_shared()          { return downcast<shared_layout>(SHARED); }\n-  // accessors\n-  size_t get_rank()                   { return shape_.size(); }\n-  const shape_t& get_shape() const    { return shape_; }\n-  const order_t& get_order() const    { return order_; }\n-  const values_t& get_values() const  { return values_;}\n-  int get_axis(size_t k) const        { return axes_.at(k); }\n-  std::vector<int> get_axes() const\t\t{ return axes_; }\n-  const int get_order(size_t k) const { return order_.at(k); }\n-  // find the position of given axis\n-  int find_axis(int to_find) const;\n-\n-\n-private:\n-  id_t id_;\n-  axes_t axes_;\n-  values_t values_;\n-\n-protected:\n-  order_t order_;\n-  shape_t shape_;\n-};\n-\n-class distributed_layout: public data_layout{\n-public:\n-  distributed_layout(id_t id,\n-                     const std::vector<int>& axes,\n-                     const std::vector<unsigned>& shape,\n-                     const std::vector<ir::value*>& values,\n-                     analysis::align* align);\n-\n-  int shape_per_cta(size_t k) { return shape_per_cta_.at(k); }\n-  int rep_per_cta(size_t k) { return shape_[k] / shape_per_cta_[k]; }\n-  virtual int contig_per_thread(size_t k) = 0;\n-\n-protected:\n-  std::vector<int> shape_per_cta_;\n-};\n-\n-class mma_layout: public distributed_layout {\n-public:\n-  enum TensorCoreType : uint8_t {\n-    // floating-point tensor core instr\n-    FP32_FP16_FP16_FP32 = 0, // default\n-    FP32_BF16_BF16_FP32,\n-    FP32_TF32_TF32_FP32,\n-    // integer tensor core instr\n-    INT32_INT1_INT1_INT32, // Not implemented\n-    INT32_INT4_INT4_INT32, // Not implemented\n-    INT32_INT8_INT8_INT32, // Not implemented\n-    //\n-    NOT_APPLICABLE,    \n-  };\n-\n-  // Used on nvidia GPUs with sm >= 80\n-  inline static const std::map<TensorCoreType, std::vector<int>> mma_instr_shape_ = {\n-    {FP32_FP16_FP16_FP32, {16, 8, 16}}, \n-    {FP32_BF16_BF16_FP32, {16, 8, 16}},\n-    {FP32_TF32_TF32_FP32, {16, 8, 8}},\n-\n-    {INT32_INT1_INT1_INT32, {16, 8, 256}},\n-    {INT32_INT4_INT4_INT32, {16, 8, 64}},\n-    {INT32_INT8_INT8_INT32, {16, 8, 32}},\n-  };\n-\n-  // shape of matrices loaded by ldmatrix (m-n-k, for mxk & kxn matrices)\n-  inline static const std::map<TensorCoreType, std::vector<int>> mma_mat_shape_ = {\n-    {FP32_FP16_FP16_FP32, {8, 8, 8}}, \n-    {FP32_BF16_BF16_FP32, {8, 8, 8}},\n-    {FP32_TF32_TF32_FP32, {8, 8, 4}},\n-\n-    {INT32_INT1_INT1_INT32, {8, 8, 64}},\n-    {INT32_INT4_INT4_INT32, {8, 8, 32}},\n-    {INT32_INT8_INT8_INT32, {8, 8, 16}},\n-  };\n-\n-  inline static const std::map<TensorCoreType, std::string> mma_instr_ptx_ = {\n-    {FP32_FP16_FP16_FP32, \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"}, \n-    {FP32_BF16_BF16_FP32, \"mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\"},\n-    {FP32_TF32_TF32_FP32, \"mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\"},\n-\n-    {INT32_INT1_INT1_INT32, \"mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc\"},\n-    {INT32_INT4_INT4_INT32, \"mma.sync.aligned.m16n8k64.row.col.satfinite.s32.s4.s4.s32\"},\n-    {INT32_INT8_INT8_INT32, \"mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\"},\n-  };\n-\n-  // vector length per ldmatrix (16*8/elelment_size_in_bits)\n-  inline static const std::map<TensorCoreType, int> mma_instr_vec_ = {\n-    {FP32_FP16_FP16_FP32, 8},\n-    {FP32_BF16_BF16_FP32, 8},\n-    {FP32_TF32_TF32_FP32, 4},\n-\n-    {INT32_INT1_INT1_INT32, 128},\n-    {INT32_INT4_INT4_INT32, 32},\n-    {INT32_INT8_INT8_INT32, 16},\n-  };\n-\n-public:\n-  mma_layout(size_t num_warps,\n-                const std::vector<int>& axes,\n-                const std::vector<unsigned>& shapes,\n-                const std::vector<ir::value *> &values,\n-                analysis::align* align, target *tgt,\n-             shared_layout* layout_a,\n-             shared_layout* layout_b,\n-             ir::value *dot);\n-  void accept(layout_visitor* vst) { vst->visit_layout_mma(this); }\n-  // accessor\n-  int fpw(size_t k) { return fpw_.at(k); }\n-  int wpt(size_t k) { return wpt_.at(k); }\n-  int spw(size_t k) { return spw_.at(k); }\n-  int rep(size_t k) { return rep_.at(k); }\n-  int contig_per_thread(size_t k) { return contig_per_thread_.at(k); }\n-\n-  // helpers for generator.cc\n-  std::string get_ptx_instr() const { return mma_instr_ptx_.at(tensor_core_type_); }\n-  std::vector<int> get_mma_instr_shape() const { return mma_instr_shape_.at(tensor_core_type_); }\n-  std::vector<int> get_mma_mat_shape() const { return mma_mat_shape_.at(tensor_core_type_); }\n-  int get_vec_a() const { return mma_instr_vec_.at(tensor_core_type_); }\n-  int get_vec_b() const { return mma_instr_vec_.at(tensor_core_type_); }\n-\n-  // setter\n-  void set_tensor_core_type(TensorCoreType type) { tensor_core_type_ = type; }\n-\n-private:\n-  // fragment per warp\n-  std::vector<int> fpw_;\n-  // shape per warp\n-  std::vector<int> spw_;\n-  // warp per tile\n-  std::vector<int> wpt_;\n-  // shape per tile\n-  std::vector<int> spt_;\n-  // repetitions\n-  std::vector<int> rep_;\n-  // contiguous per thread\n-  std::vector<int> contig_per_thread_;\n-\n-  TensorCoreType tensor_core_type_ = FP32_FP16_FP16_FP32;\n-};\n-\n-struct scanline_layout: public distributed_layout {\n-  scanline_layout(size_t num_warps,\n-                    const std::vector<int>& axes,\n-                    const std::vector<unsigned>& shape,\n-                    const std::vector<ir::value *> &values,\n-                    analysis::align* align,\n-                    target* tgt);\n-  void accept(layout_visitor* vst) { vst->visit_layout_scanline(this); }\n-  // accessor\n-  int mts(size_t k) { return mts_.at(k); }\n-  int nts(size_t k) { return nts_.at(k); }\n-  int contig_per_thread(size_t k) { return nts_.at(k); }\n-\n-  int per_thread(size_t k) { return contig_per_thread(k) * shape_[k] / shape_per_cta(k);}\n-public:\n-  // micro tile size. The size of a tile held by a thread block.\n-  std::vector<int> mts_;\n-  // nano tile size. The size of a tile held by a thread.\n-  std::vector<int> nts_;\n-};\n-\n-struct double_buffer_info_t {\n-  ir::value* first;\n-  ir::value* latch;\n-  ir::phi_node* phi;\n-};\n-\n-struct N_buffer_info_t {\n-  std::vector<ir::value*> firsts; // not necessarily ordered as input order\n-  ir::value* latch;\n-  ir::phi_node* phi;\n-  std::map<ir::value*, int> firsts_idx;\n-};\n-\n-// abstract for dot and coresponding smem values\n-class shared_layout: public data_layout {\n-private:\n-  static bool is_loop_latch(ir::phi_node *phi, ir::instruction *terminator);\n-  static void extract_double_bufferable(ir::value *v, std::shared_ptr<double_buffer_info_t>& res);\n-  static void extract_N_bufferable(ir::value *v, std::shared_ptr<N_buffer_info_t>& res, int &prev_stages);\n-\n-public:\n-  shared_layout(data_layout *arg,\n-                const std::vector<int>& axes,\n-                const std::vector<unsigned>& shapes,\n-                const std::vector<ir::value *> &values_,\n-                ir::type *ty,\n-                analysis::align* align, target *tgt);\n-  void accept(layout_visitor* vst) { vst->visit_layout_shared(this); }\n-  // accessors\n-  size_t get_size()                         { return size_; }\n-  ir::type* get_type()                      { return ty_; }\n-  double_buffer_info_t* get_double_buffer() { return double_buffer_.get(); }\n-  N_buffer_info_t* get_N_buffer()           { return N_buffer_.get(); }\n-  int get_num_stages() const;\n-  size_t get_per_stage_size() const         { return size_ / get_num_stages(); }\n-  size_t get_per_stage_elements() const;\n-  size_t get_num_per_phase()                { return num_per_phase_; }\n-  ir::value* hmma_dot_a()                      { return hmma_dot_a_; }\n-  ir::value* hmma_dot_b()                      { return hmma_dot_b_; }\n-  void set_mma_vec(int mma_vec)             { mma_vec_ = mma_vec; }\n-  int  get_mma_vec()                        { return mma_vec_;}\n-  int  get_mma_strided()                    { return mma_strided_; }\n-  bool allow_swizzle() const                { return allow_swizzle_; }\n-  data_layout* get_arg_layout()             { return arg_layout_; }\n-\n-private:\n-  size_t size_;\n-  ir::type *ty_;\n-  std::shared_ptr<double_buffer_info_t> double_buffer_;\n-  std::shared_ptr<N_buffer_info_t>      N_buffer_;\n-  size_t num_per_phase_;\n-  ir::value* hmma_dot_a_;\n-  ir::value* hmma_dot_b_;\n-  data_layout* arg_layout_;\n-  int mma_vec_;\n-  int mma_strided_;\n-  bool allow_swizzle_ = true;\n-  target *tgt_;\n-};\n-\n-\n-\n-class layouts {\n-  typedef ir::value* node_t;\n-  typedef std::map <node_t, std::set<node_t>> graph_t;\n-\n-private:\n-  // graph creation\n-  void connect(ir::value *x, ir::value *y);\n-  void make_graph(ir::instruction *i);\n-\n-  void init_hmma_tile(data_layout& layouts);\n-  void init_scanline_tile(data_layout &layouts);\n-\n-  void create(size_t id, const std::vector<ir::value*>& values);\n-\n-public:\n-  // constructor\n-  layouts(analysis::axes *axes, analysis::align *align, size_t num_warps, target* tgt);\n-\n-  // accessors\n-  unsigned layout_of(ir::value *value) const                  { return groups_.at(value); }\n-  bool has(ir::value* value) const { return groups_.find(value) != groups_.end(); }\n-  const std::vector<ir::value*>& values_of(unsigned id) const { return values_.at(id); }\n-  size_t num_layouts() const                                  { return values_.size();}\n-  data_layout* get(size_t id)                                 { return layouts_.at(id); }\n-  data_layout* get(ir::value *v)                              { return get(layout_of(v));}\n-  std::map<size_t, data_layout*> &get_all()                   { return layouts_; }\n-  bool has_tmp(ir::value* i)                                  { return tmp_.find(i) != tmp_.end(); }\n-  int tmp(ir::value* i)                                       { return tmp_.at(i);}\n-  void copy(ir::value* dst, ir::value* src)                   { groups_[dst] = groups_[src]; }\n-  // execution\n-  void run(ir::module &mod);\n-\n-private:\n-  analysis::axes* axes_;\n-  analysis::align* align_;\n-  size_t num_warps_;\n-  target* tgt_;\n-  tools::graph<ir::value*> graph_;\n-  std::map<ir::value*, size_t> groups_;\n-  std::map<size_t, std::vector<ir::value*>> values_;\n-  std::map<size_t, data_layout*> layouts_;\n-  std::map<ir::value*, size_t> tmp_;\n-};\n-\n-}\n-}\n-\n-}\n-\n-#endif"}, {"filename": "include/triton/codegen/analysis/liveness.h", "status": "removed", "additions": 0, "deletions": 69, "changes": 69, "file_content_changes": "@@ -1,69 +0,0 @@\n-#ifndef TDL_INCLUDE_IR_CODEGEN_LIVENESS_H\n-#define TDL_INCLUDE_IR_CODEGEN_LIVENESS_H\n-\n-#include \"triton/codegen/analysis/layout.h\"\n-#include \"triton/tools/graph.h\"\n-\n-#include \"llvm/ADT/MapVector.h\"\n-\n-#include <set>\n-#include <vector>\n-\n-namespace triton{\n-\n-namespace ir{\n-  class value;\n-  class phi_node;\n-  class function;\n-  class module;\n-  class instruction;\n-}\n-\n-namespace codegen{\n-namespace analysis{\n-\n-typedef unsigned slot_index;\n-\n-class tiles;\n-class layouts;\n-class data_layout;\n-\n-struct segment {\n-  slot_index start;\n-  slot_index end;\n-\n-  bool contains(slot_index idx) const {\n-    return start <= idx && idx < end;\n-  }\n-\n-  bool intersect(const segment &Other){\n-    return contains(Other.start) || Other.contains(start);\n-  }\n-};\n-\n-\n-class liveness {\n-private:\n-  typedef llvm::MapVector<shared_layout*, segment>    intervals_map_t;\n-\n-public:\n-  // constructor\n-  liveness(layouts *l): layouts_(l){ }\n-  // accessors\n-  const intervals_map_t& get()  const { return intervals_; }\n-  segment get(shared_layout* v)  const { return intervals_.lookup(v); }\n-  // run\n-  void run(ir::module &mod);\n-\n-private:\n-  // analysis\n-  layouts *layouts_;\n-  intervals_map_t intervals_;\n-};\n-\n-}\n-}\n-}\n-\n-\n-#endif"}, {"filename": "include/triton/codegen/analysis/swizzle.h", "status": "removed", "additions": 0, "deletions": 43, "changes": 43, "file_content_changes": "@@ -1,43 +0,0 @@\n-#ifndef TRITON_INCLUDE_IR_CODEGEN_SWIZZLE_H\n-#define TRITON_INCLUDE_IR_CODEGEN_SWIZZLE_H\n-\n-#include <map>\n-\n-namespace triton{\n-\n-namespace ir{\n-  class module;\n-}\n-\n-namespace codegen{\n-class target;\n-\n-namespace analysis{\n-\n-class layouts;\n-class data_layout;\n-\n-class swizzle {\n-public:\n-  // constructor\n-  swizzle(layouts *l, target* tgt): layouts_(l), tgt_(tgt){ }\n-  // accessors\n-  int get_per_phase(data_layout* layout) { return per_phase_.at(layout); }\n-  int get_max_phase(data_layout* layout) { return max_phase_.at(layout); }\n-  int get_vec  (data_layout* layout)     { return vec_.at(layout); }\n-  // run\n-  void run(ir::module &mod);\n-private:\n-  layouts* layouts_;\n-  target* tgt_;\n-  std::map<data_layout*, int> per_phase_;\n-  std::map<data_layout*, int> max_phase_;\n-  std::map<data_layout*, int> vec_;\n-};\n-\n-}\n-}\n-}\n-\n-\n-#endif"}, {"filename": "include/triton/codegen/pass.h", "status": "removed", "additions": 0, "deletions": 42, "changes": 42, "file_content_changes": "@@ -1,42 +0,0 @@\n-#ifndef _TRITON_CODEGEN_PASS_H_\n-#define _TRITON_CODEGEN_PASS_H_\n-\n-\n-#include <memory>\n-\n-namespace llvm{\n-  class Module;\n-  class LLVMContext;\n-}\n-\n-namespace triton{\n-\n-namespace codegen {\n-  class target;\n-}\n-\n-namespace ir{\n-  class module;\n-}\n-namespace driver{\n-  class device;\n-  class module;\n-  class kernel;\n-}\n-}\n-\n-namespace triton{\n-namespace codegen{\n-\n-// TODO:\n-// There should be a proper pass manager there!\n-std::unique_ptr<llvm::Module> add_passes_to_emit_bin(ir::module &ir, llvm::LLVMContext& ctx,\n-                                                     codegen::target* target,\n-                                                     int sm, int num_warps,\n-                                                     int num_stages, int &shared_static);\n-\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/codegen/selection/generator.h", "status": "removed", "additions": 0, "deletions": 274, "changes": 274, "file_content_changes": "@@ -1,274 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_SELECTION_GENERATOR_H_\n-#define _TRITON_SELECTION_GENERATOR_H_\n-\n-#include \"triton/ir/visitor.h\"\n-#include \"triton/ir/instructions.h\"\n-#include \"triton/codegen/analysis/layout.h\"\n-#include <functional>\n-\n-// forward\n-namespace llvm{\n-  class Type;\n-  class Value;\n-  class PHINode;\n-  class BasicBlock;\n-  class Attribute;\n-  class Instruction;\n-  class Constant;\n-  class LLVMContext;\n-  class Module;\n-  class ConstantFolder;\n-  class IRBuilderDefaultInserter;\n-  template <typename T, typename Inserter>\n-  class IRBuilder;\n-  class ArrayType;\n-  class Function;\n-  class StructType;\n-}\n-\n-namespace triton{\n-\n-namespace ir{\n-class attribute;\n-class load_inst;\n-class store_inst;\n-}\n-\n-namespace codegen{\n-\n-// forward\n-namespace analysis{\n-class liveness;\n-class tiles;\n-class align;\n-class allocation;\n-class cts;\n-class axes;\n-class layouts;\n-class swizzle;\n-}\n-// typedef\n-typedef llvm::IRBuilder<llvm::ConstantFolder,\n-                        llvm::IRBuilderDefaultInserter> Builder;\n-typedef llvm::LLVMContext LLVMContext;\n-typedef llvm::Type Type;\n-typedef llvm::Value Value;\n-typedef llvm::Attribute Attribute;\n-typedef llvm::BasicBlock BasicBlock;\n-typedef llvm::Module Module;\n-typedef llvm::Instruction Instruction;\n-typedef llvm::Constant Constant;\n-typedef llvm::ArrayType ArrayType;\n-typedef llvm::Function Function;\n-typedef std::vector<Value*> indices_t;\n-class target;\n-\n-}\n-}\n-\n-namespace triton{\n-namespace codegen{\n-\n-struct distributed_axis {\n-  int contiguous;\n-  std::vector<Value*> values;\n-  Value* thread_id;\n-};\n-\n-class adder{\n-public:\n-  adder(Builder** builder): builder_(builder) { }\n-  Value* operator()(Value* x, Value* y, const std::string& name = \"\");\n-\n-private:\n-  Builder** builder_;\n-};\n-\n-class multiplier{\n-public:\n-  multiplier(Builder** builder): builder_(builder) { }\n-  Value* operator()(Value* x, Value* y, const std::string& name = \"\");\n-private:\n-  Builder** builder_;\n-};\n-\n-class geper{\n-public:\n-  geper(Builder** builder): builder_(builder) { }\n-  Value* operator()(Value *ptr, Value* off, const std::string& name = \"\");\n-  Value* operator()(Type* ty, Value*ptr, std::vector<Value*> vals, const std::string& name = \"\");\n-\n-private:\n-  Builder** builder_;\n-};\n-\n-class generator: public ir::visitor, public analysis::layout_visitor {\n-private:\n-  void init_idx(ir::value *x);\n-  Instruction* add_barrier();\n-  Value* shared_off(const std::vector<unsigned>& shapes, const std::vector<int>& order, indices_t idx);\n-  void finalize_shared_layout(analysis::shared_layout*);\n-  void finalize_function(ir::function*);\n-  void finalize_phi_node(ir::phi_node*);\n-\n-private:\n-  Type *cvt(ir::type *ty);\n-  llvm::Attribute cvt(ir::attribute attr);\n-  void packed_type(ir::value* i);\n-  void forward_declare(ir::function* fn);\n-\n-public:\n-  generator(analysis::axes *a_axes,\n-            analysis::layouts *layouts,\n-            analysis::align *alignment,\n-            analysis::allocation *alloc,\n-            analysis::swizzle *swizzle,\n-            target *tgt,\n-            unsigned num_warps);\n-\n-  void visit_value(ir::value* v);\n-  void visit_call_inst(ir::call_inst*);\n-  void visit_launch_inst(ir::launch_inst *);\n-  void visit_phi_node(ir::phi_node*);\n-  void visit_binary_operator(ir::binary_operator*);\n-  void visit_getelementptr_inst(ir::getelementptr_inst*);\n-  void visit_icmp_inst(ir::icmp_inst*);\n-  void visit_fcmp_inst(ir::fcmp_inst*);\n-  std::tuple<Value*, Value*, Value*, Value*> fp8x4_to_fp32x4(Value *in0, Value *in1, Value *in2, Value *in3);\n-  std::tuple<Value*, Value*, Value*, Value*> fp32x4_to_fp8x4(Value *in0, Value *in1, Value *in2, Value *in3);\n-  std::tuple<Value*, Value*, Value*, Value*> fp8x4_to_fp16x4(Value *in0, Value *in1, Value *in2, Value *in3);\n-  std::tuple<Value*, Value*, Value*, Value*> fp16x4_to_fp8x4(Value *in0, Value *in1, Value *in2, Value *in3);\n-  Value* bf16_to_fp32(Value *in0);\n-  Value* fp32_to_bf16(Value *in0);\n-\n-  void visit_cast_inst(ir::cast_inst*);\n-  void visit_return_inst(ir::return_inst*);\n-  void visit_cond_branch_inst(ir::cond_branch_inst*);\n-  void visit_uncond_branch_inst(ir::uncond_branch_inst*);\n-  void visit_load_inst(ir::load_inst*);\n-  void visit_unmasked_load_inst(ir::unmasked_load_inst*);\n-  void visit_masked_load_inst(ir::masked_load_inst*);\n-  void visit_store_inst(ir::store_inst*);\n-  void visit_unmasked_store_inst(ir::unmasked_store_inst*);\n-  void visit_masked_store_inst(ir::masked_store_inst*);\n-  void visit_cat_inst(ir::cat_inst*);\n-  void visit_extract_value_inst(ir::extract_value_inst *);\n-  void visit_insert_value_inst(ir::insert_value_inst *);\n-  void visit_reshape_inst(ir::reshape_inst*);\n-  void visit_splat_inst(ir::splat_inst*);\n-  void visit_broadcast_inst(ir::broadcast_inst*);\n-  void visit_downcast_inst(ir::downcast_inst*);\n-  void visit_exp_inst(ir::exp_inst*);\n-  void visit_cos_inst(ir::cos_inst*);\n-  void visit_umulhi_inst(ir::umulhi_inst* x);\n-  void visit_sin_inst(ir::sin_inst*);\n-  void visit_log_inst(ir::log_inst*);\n-  void visit_get_program_id_inst(ir::get_program_id_inst*);\n-  void visit_get_num_programs_inst(ir::get_num_programs_inst*);\n-  void visit_atomic_cas_inst(ir::atomic_cas_inst*);\n-  void visit_atomic_rmw_inst(ir::atomic_rmw_inst*);\n-  void visit_mma884(ir::dot_inst*, ir::value *A, ir::value *B, ir::value *D, unsigned NK);\n-  void visit_mma16816(ir::dot_inst*, ir::value *A, ir::value *B, ir::value *D, unsigned NK);\n-  void visit_fmadot(ir::dot_inst*, ir::value *A, ir::value *B, ir::value *D, unsigned NK, Type *c_ty, Function *f_mul_add);\n-  void visit_dot_inst(ir::dot_inst*);\n-  void visit_trans_inst(ir::trans_inst*);\n-  void visit_sqrt_inst(ir::sqrt_inst*);\n-  Value* shfl_sync(Value* acc, int32_t i);\n-  void visit_reduce1d_inst(ir::reduce_inst*, std::function<Value*(Value*,Value*)>, Value*);\n-  void visit_reducend_inst_fast(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral);\n-  void visit_reducend_inst(ir::reduce_inst*, std::function<Value*(Value*,Value*)>, Value*);\n-  void visit_reduce_inst(ir::reduce_inst*);\n-  void visit_select_inst(ir::select_inst*);\n-  void visit_layout_convert(ir::value *out, ir::value *in);\n-  void visit_cvt_layout_inst(ir::cvt_layout_inst*);\n-  void visit_masked_load_async_inst(ir::masked_load_async_inst*);\n-  void visit_copy_to_shared_inst(ir::copy_to_shared_inst*);\n-  void visit_copy_from_shared_inst(ir::copy_from_shared_inst*);\n-  void visit_barrier_inst(ir::barrier_inst*);\n-  void visit_prefetch_s_inst(ir::prefetch_s_inst*);\n-  void visit_async_wait_inst(ir::async_wait_inst*);\n-//  void visit_make_range_dyn(ir::make_range_dyn*);\n-  void visit_make_range(ir::make_range*);\n-  void visit_clock_inst(ir::clock_inst*);\n-  void visit_globaltimer_inst(ir::globaltimer_inst*);\n-//  void visit_make_range_sta(ir::make_range_sta*);\n-  void visit_undef_value(ir::undef_value*);\n-  void visit_constant_int(ir::constant_int*);\n-  void visit_constant_fp(ir::constant_fp*);\n-  void visit_alloc_const(ir::alloc_const*);\n-  void visit_function(ir::function*);\n-  void visit_basic_block(ir::basic_block*);\n-  void visit_argument(ir::argument*);\n-  void visit(ir::module &, llvm::Module &);\n-\n-\n-  // layouts\n-  void visit_layout_mma(analysis::mma_layout*);\n-  void visit_layout_scanline(analysis::scanline_layout*);\n-  void visit_layout_shared(analysis::shared_layout*);\n-\n-\n-private:\n-  LLVMContext *ctx_;\n-  Builder* builder_;\n-  Module *mod_;\n-\n-  analysis::axes *a_axes_;\n-  analysis::swizzle *swizzle_;\n-  std::map<unsigned, distributed_axis> axes_;\n-  target *tgt_;\n-  analysis::layouts *layouts_;\n-  analysis::align *alignment_;\n-  analysis::allocation *alloc_;\n-  Value *shmem_;\n-  std::set<ir::value*> seen_;\n-\n-  unsigned num_warps_;\n-\n-  std::map<analysis::data_layout*, Value*> offset_a_m_;\n-  std::map<analysis::data_layout*, Value*> offset_a_k_;\n-  std::map<analysis::data_layout*, Value*> offset_b_k_;\n-  std::map<analysis::data_layout*, Value*> offset_b_n_;\n-\n-  /// layout -> base ptr\n-  std::map<analysis::data_layout*, Value*> shared_ptr_;\n-  std::map<analysis::data_layout*, Value*> shared_pre_ptr_;\n-  std::map<analysis::data_layout*, Value*> shared_next_ptr_;\n-  /// offset for double-buffered layout\n-  std::map<analysis::data_layout*, Value*> shared_off_;\n-\n-  /// Base shmem pointer of ir value\n-  std::map<ir::value*, Value*> shmems_;\n-  std::map<ir::value*, Value*> shoffs_;\n-  std::map<ir::value*, std::vector<indices_t>> idxs_;\n-  std::map<ir::value*, std::map<indices_t, Value*>> vals_;\n-  /// idx for multi-stage pipeline\n-  std::map<analysis::data_layout*, Value*> read_smem_idx_;\n-  std::map<analysis::data_layout*, Value*> write_smem_idx_;\n-  \n-  /// triton bb -> llvm bb\n-  std::map<ir::value*, BasicBlock *> bbs_;\n-  std::map<ir::value*, std::vector<int>> ords_;\n-  std::map<ir::value*, Function*> fns_;\n-\n-  // helper for creating llvm values\n-  adder add;\n-  multiplier mul;\n-  geper gep;\n-\n-  /// PHI nodes\n-  std::vector<std::tuple<llvm::PHINode*, Value*, ir::basic_block*>> lazy_phi_incs_;\n-\n-  /// Record prefetch instrs that needs to be moved\n-  std::map<ir::value*, std::vector<Value*>> prefetch_latch_to_bb_;\n-\n-  // Eviction policies\n-  std::map<ir::load_inst::EVICTION_POLICY, Value*> policies_;\n-};\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/codegen/target.h", "status": "removed", "additions": 0, "deletions": 105, "changes": 105, "file_content_changes": "@@ -1,105 +0,0 @@\n-#ifndef TDL_INCLUDE_IR_CODEGEN_TARGET_H\n-#define TDL_INCLUDE_IR_CODEGEN_TARGET_H\n-\n-namespace llvm{\n-  class Type;\n-  class Value;\n-  class Instruction;\n-  class Constant;\n-  class LLVMContext;\n-  class Module;\n-  class ConstantFolder;\n-  class IRBuilderDefaultInserter;\n-  template <typename T, typename Inserter>\n-  class IRBuilder;\n-  class ArrayType;\n-  class Function;\n-}\n-\n-// typedefs\n-namespace triton{\n-namespace codegen{\n-  typedef llvm::IRBuilder<llvm::ConstantFolder,\n-                          llvm::IRBuilderDefaultInserter> Builder;\n-  typedef llvm::LLVMContext LLVMContext;\n-  typedef llvm::Type Type;\n-  typedef llvm::Value Value;\n-  typedef llvm::Module Module;\n-  typedef llvm::Instruction Instruction;\n-  typedef llvm::Constant Constant;\n-  typedef llvm::ArrayType ArrayType;\n-  typedef llvm::Function Function;\n-}\n-}\n-\n-namespace triton{\n-namespace codegen{\n-\n-class nvidia_cu_target;\n-\n-class target {\n-public:\n-  target(bool is_gpu): is_gpu_(is_gpu){}\n-  virtual ~target() {}\n-  virtual void set_kernel(Builder& builder, LLVMContext &ctx, Module *module, Function* fn) = 0;\n-  virtual Instruction* add_barrier(Module *module, Builder& builder) = 0;\n-  virtual Instruction* add_memfence(Module *module, Builder& builder) = 0;\n-  virtual Value* get_global_offset(Module *module, Builder& builder, unsigned stride, unsigned ax) = 0;\n-  virtual Value* get_local_id(Module *module, Builder& builder, unsigned ax) = 0;\n-  virtual Value* get_block_id(Module *module, Builder& builder, unsigned ax) = 0;\n-  virtual Value* get_num_blocks(Module *module, Builder& builder, unsigned ax) = 0;\n-  virtual unsigned guaranteed_alignment() = 0;\n-  nvidia_cu_target* as_nvidia();\n-  bool is_gpu() const;\n-\n-private:\n-  bool is_gpu_;\n-};\n-\n-class amd_cl_target: public target {\n-public:\n-  amd_cl_target(): target(true){}\n-  void set_kernel(Builder& builder, LLVMContext &ctx, Module *module, Function* fn);\n-  Instruction* add_barrier(Module *module, Builder& builder);\n-  Instruction* add_memfence(Module *module, Builder& builder);\n-  Value* get_global_offset(Module *module, Builder& builder, unsigned stride, unsigned ax);\n-  Value* get_local_id(Module *module, Builder& builder, unsigned ax);\n-  Value* get_block_id(Module *module, Builder& builder, unsigned ax);\n-  Value* get_num_blocks(Module *module, Builder& builder, unsigned ax);\n-  unsigned guaranteed_alignment() { return 16; }\n-};\n-\n-class nvidia_cu_target: public target {\n-public:\n-  nvidia_cu_target(int sm): target(true), sm_(sm){}\n-  void set_kernel(Builder& builder, LLVMContext &ctx, Module *module, Function* fn);\n-  Instruction* add_barrier(Module *module, Builder& builder);\n-  Instruction* add_memfence(Module *module, Builder& builder);\n-  Value* get_global_offset(Module *module, Builder& builder, unsigned stride, unsigned ax);\n-  Value* get_local_id(Module *module, Builder& builder, unsigned ax);\n-  Value* get_block_id(Module *module, Builder& builder, unsigned ax);\n-  Value* get_num_blocks(Module *module, Builder& builder, unsigned ax);\n-  int sm() { return sm_; }\n-  unsigned guaranteed_alignment() { return 16; }\n-\n-private:\n-  int sm_;\n-};\n-\n-class cpu_target: public target {\n-public:\n-  cpu_target(): target(false){}\n-  void set_kernel(Builder& builder, LLVMContext &ctx, Module *module, Function* fn);\n-  Instruction* add_barrier(Module *module, Builder& builder);\n-  Instruction* add_memfence(Module *module, Builder& builder);\n-  Value* get_global_offset(Module *module, Builder& builder, unsigned stride, unsigned ax);\n-  Value* get_local_id(Module *module, Builder& builder, unsigned ax);\n-  Value* get_block_id(Module *module, Builder& builder, unsigned ax);\n-  Value* get_num_blocks(Module *module, Builder& builder, unsigned ax);\n-  unsigned guaranteed_alignment() { return 1; }\n-};\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/codegen/transform/coalesce.h", "status": "removed", "additions": 0, "deletions": 48, "changes": 48, "file_content_changes": "@@ -1,48 +0,0 @@\n-#ifndef TDL_INCLUDE_CODEGEN_OPTIMIZE_REORDER_H\n-#define TDL_INCLUDE_CODEGEN_OPTIMIZE_REORDER_H\n-\n-#include <map>\n-#include <set>\n-#include <vector>\n-\n-namespace triton {\n-\n-namespace ir {\n-  class module;\n-  class value;\n-  class io_inst;\n-  class instruction;\n-  class builder;\n-}\n-\n-namespace codegen{\n-\n-namespace analysis{\n-  class align;\n-  class layouts;\n-  class cts;\n-}\n-\n-namespace transform{\n-\n-class coalesce {\n-private:\n-  void extract_io_use(ir::value *v, std::set<ir::io_inst*>& result);\n-  void extract_ld(ir::io_inst *i, std::map<int, std::vector<triton::ir::io_inst *> > &result);\n-  ir::value* rematerialize(ir::value *v, ir::builder& builder, std::map<ir::value*, ir::value*>& seen);\n-\n-public:\n-  coalesce(analysis::align* align, triton::codegen::analysis::layouts *layouts);\n-  triton::ir::value *simplify(ir::instruction* i, triton::ir::builder &builder);\n-  void run(ir::module &mod);\n-\n-private:\n-  analysis::align* align_;\n-  analysis::layouts* layout_;\n-};\n-\n-}\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/codegen/transform/cts.h", "status": "removed", "additions": 0, "deletions": 36, "changes": 36, "file_content_changes": "@@ -1,36 +0,0 @@\n-#ifndef TDL_INCLUDE_CODEGEN_BUFFER_INFO_PASS_H\n-#define TDL_INCLUDE_CODEGEN_BUFFER_INFO_PASS_H\n-\n-#include <set>\n-#include <map>\n-\n-namespace triton {\n-\n-namespace ir {\n-  class module;\n-  class value;\n-  class phi_node;\n-  class instruction;\n-  class builder;\n-}\n-\n-namespace codegen{\n-namespace transform{\n-\n-class cts {\n-private:\n-  void add_copy(ir::instruction *parent, ir::value *x, ir::builder &builder, bool to_shared);\n-\n-public:\n-  cts(bool use_async = false): use_async_(use_async) {}\n-  void run(ir::module &mod);\n-\n-private:\n-  bool use_async_;\n-};\n-\n-}\n-}\n-}\n-\n-#endif\n\\ No newline at end of file"}, {"filename": "include/triton/codegen/transform/dce.h", "status": "removed", "additions": 0, "deletions": 24, "changes": 24, "file_content_changes": "@@ -1,24 +0,0 @@\n-#ifndef TDL_INCLUDE_CODEGEN_OPTIMIZE_CSE_H\n-#define TDL_INCLUDE_CODEGEN_OPTIMIZE_CSE_H\n-\n-\n-namespace triton {\n-\n-namespace ir {\n-  class module;\n-}\n-\n-namespace codegen{\n-namespace transform{\n-\n-class dce {\n-public:\n-  dce() {}\n-  void run(ir::module &mod);\n-};\n-\n-}\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/codegen/transform/disassociate.h", "status": "removed", "additions": 0, "deletions": 22, "changes": 22, "file_content_changes": "@@ -1,22 +0,0 @@\n-#ifndef _TRITON_SELECTION_TRANSFORM_DISASSOCIATE_H_\n-#define _TRITON_SELECTION_TRANSFORM_DISASSOCIATE_H_\n-\n-\n-namespace triton {\n-namespace ir {\n-  class module;\n-}\n-\n-namespace codegen{\n-namespace transform{\n-\n-class disassociate {\n-public:\n-  void run(ir::module &mod);\n-};\n-\n-}\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/codegen/transform/inline.h", "status": "removed", "additions": 0, "deletions": 31, "changes": 31, "file_content_changes": "@@ -1,31 +0,0 @@\n-#pragma once\n-\n-#include <list>\n-\n-namespace triton {\n-\n-namespace ir {\n-  class module;\n-  class function;\n-  class call_inst;\n-  class builder;\n-}\n-\n-namespace codegen{\n-namespace transform{\n-\n-struct fncmp {\n-  bool operator()(ir::function* x, ir::function* y) const;\n-};\n-\n-class inliner {\n-public:\n-  inliner() {}\n-  void do_inline(ir::function* fn, ir::call_inst* callsite, ir::builder& builder, std::list<ir::call_inst*>& callsites);\n-  void run(ir::module &mod);\n-};\n-\n-\n-}\n-}\n-}"}, {"filename": "include/triton/codegen/transform/membar.h", "status": "removed", "additions": 0, "deletions": 72, "changes": 72, "file_content_changes": "@@ -1,72 +0,0 @@\n-#ifndef TDL_INCLUDE_CODEGEN_BARRIERS_H\n-#define TDL_INCLUDE_CODEGEN_BARRIERS_H\n-\n-#include <vector>\n-#include <map>\n-#include <list>\n-#include <set>\n-#include \"triton/codegen/target.h\"\n-\n-namespace triton {\n-\n-namespace ir {\n-  class module;\n-  class basic_block;\n-  class instruction;\n-  class masked_load_async_inst;\n-  class value;\n-  class builder;\n-}\n-\n-namespace codegen{\n-\n-namespace analysis{\n-\n-class allocation;\n-class liveness;\n-class layouts;\n-class cts;\n-class shared_layout;\n-\n-}\n-\n-namespace transform{\n-\n-class prefetch;\n-\n-class membar {\n-private:\n-  typedef std::pair<unsigned, unsigned> interval_t;\n-  typedef std::set<ir::value*> val_set_t;\n-  typedef std::vector<ir::value*> val_vec_t;\n-\n-private:\n-  bool intersect(const val_set_t &X, const val_set_t &Y);\n-  bool check_safe_war(ir::instruction* i);\n-  int group_of(triton::ir::value *i, std::vector<triton::ir::value *> &async_write);\n-  bool intersect_with(analysis::shared_layout* a_layout, analysis::shared_layout* b_layout);\n-  val_set_t intersect_with(const val_set_t& as, const val_set_t& bs);\n-  void transfer(ir::basic_block *block, val_vec_t &async_write, val_set_t &sync_write, val_set_t &sync_read,\n-                std::set<triton::ir::value *> &safe_war, bool &inserted, ir::builder &builder);\n-\n-public:\n-  membar(analysis::liveness *liveness, analysis::layouts *layouts, analysis::allocation *alloc, \n-         transform::prefetch *prefetch, target* tgt):\n-    liveness_(liveness), layouts_(layouts), alloc_(alloc), prefetch_(prefetch), tgt_(tgt) {}\n-  void run(ir::module &mod);\n-\n-private:\n-  analysis::liveness *liveness_;\n-  analysis::layouts *layouts_;\n-  analysis::allocation *alloc_;\n-  transform::prefetch *prefetch_;\n-\n-  target* tgt_;\n-};\n-\n-\n-}\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/codegen/transform/peephole.h", "status": "removed", "additions": 0, "deletions": 56, "changes": 56, "file_content_changes": "@@ -1,56 +0,0 @@\n-#ifndef TDL_INCLUDE_CODEGEN_OPTIMIZE_TRANS_H\n-#define TDL_INCLUDE_CODEGEN_OPTIMIZE_TRANS_H\n-\n-#include \"triton/codegen/target.h\"\n-\n-namespace triton {\n-\n-namespace ir {\n-  class module;\n-  class value;\n-  class instruction;\n-  class trans_inst;\n-  class builder;\n-  class constant_int;\n-  class dot_inst;\n-}\n-\n-namespace codegen{\n-namespace analysis{\n-class layouts;\n-}\n-\n-namespace transform{\n-\n-class peephole {\n-private:\n-//  bool rewrite_cts_cfs(ir::instruction *value, ir::builder &builder);\n-  bool rewrite_trans_phi(ir::instruction* value, ir::builder &builder);\n-  bool rewrite_dot_fp32(ir::dot_inst *dot, ir::builder& builder, bool trans_a, bool trans_b, ir::value *A, ir::value *B, ir::value *D);\n-  bool rewrite_dot_hmma(ir::dot_inst *dot, ir::builder& builder, bool trans_a, bool trans_b, ir::value *A, ir::value *B, ir::value *D);\n-  bool rewrite_dot(ir::instruction *value, ir::builder& builder);\n-  bool rewrite_mult(ir::instruction *value, ir::builder& builder);\n-  bool rewrite_insert_extract(ir::instruction *value, ir::builder& builder);\n-\n-\n-  bool rewrite_unit_red(ir::instruction *value, ir::builder& builder);\n-  bool rewrite_gep_ptr_min_off_plus_off(ir::instruction *value, ir::builder& builder);\n-  bool rewrite_select_masked_load(ir::instruction *value, ir::builder& builder);\n-  bool rewrite_load_to_shared(ir::instruction *value, ir::builder& builder);\n-  bool rewrite_cvt_layout(ir::instruction *value, ir::builder& builder);\n- \n-public:\n-  peephole(target* tgt, analysis::layouts* layouts): tgt_(tgt), layouts_(layouts) {}\n-  void run(ir::module &mod);\n-\n-private:\n-  target* tgt_;\n-  analysis::layouts* layouts_;\n-};\n-\n-\n-}\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/codegen/transform/pipeline.h", "status": "removed", "additions": 0, "deletions": 30, "changes": 30, "file_content_changes": "@@ -1,30 +0,0 @@\n-#ifndef TRITON_INCLUDE_IR_CODEGEN_PIPELINE_H\n-#define TRITON_INCLUDE_IR_CODEGEN_PIPELINE_H\n-\n-// forward declaration\n-namespace triton {\n-namespace ir {\n-class module;\n-}\n-} // namespace triton\n-\n-namespace triton {\n-namespace codegen {\n-namespace transform {\n-\n-class pipeline {\n-public:\n-  pipeline(bool has_copy_async, int num_stages)\n-      : has_copy_async_(has_copy_async), num_stages_(num_stages) {}\n-  void run(ir::module &module);\n-\n-private:\n-  bool has_copy_async_;\n-  int num_stages_;\n-};\n-\n-} // namespace transform\n-} // namespace codegen\n-} // namespace triton\n-\n-#endif"}, {"filename": "include/triton/codegen/transform/prefetch.h", "status": "removed", "additions": 0, "deletions": 27, "changes": 27, "file_content_changes": "@@ -1,27 +0,0 @@\n-#ifndef TRITON_INCLUDE_TRITON_CODEGEN_TRANSFORM_PREFETCH_H\n-#define TRITON_INCLUDE_TRITON_CODEGEN_TRANSFORM_PREFETCH_H\n-\n-#include <set>\n-\n-// forward dclaration\n-namespace triton::ir{\n-class module;\n-class value;\n-}\n-\n-namespace triton::codegen {\n-class target;\n-}\n-\n-namespace triton::codegen::transform {\n-class prefetch {\n-  target* tgt_;\n-  std::set<ir::value*> prefetched_vals_;\n-public:\n-  prefetch(target *tgt) : tgt_(tgt) {}\n-  void run(ir::module &module);\n-  bool is_prefetched(ir::value* v) { return prefetched_vals_.find(v) != prefetched_vals_.end(); }\n-};\n-}\n-\n-#endif\n\\ No newline at end of file"}, {"filename": "include/triton/codegen/transform/reorder.h", "status": "removed", "additions": 0, "deletions": 26, "changes": 26, "file_content_changes": "@@ -1,26 +0,0 @@\n-#ifndef TRITON_INCLUDE_IR_CODEGEN_REORDER_H\n-#define TRITON_INCLUDE_IR_CODEGEN_REORDER_H\n-\n-namespace triton {\n-\n-// forward declaration\n-namespace ir {\n-class module;\n-}\n-\n-namespace codegen{\n-\n-namespace transform{\n-\n-class reorder {\n-public:\n-  void run(ir::module& module);\n-};\n-\n-}\n-\n-}\n-\n-}\n-\n-#endif"}, {"filename": "include/triton/driver/dispatch.h", "status": "removed", "additions": 0, "deletions": 318, "changes": 318, "file_content_changes": "@@ -1,318 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_DRIVER_DISPATCH_H_\n-#define _TRITON_DRIVER_DISPATCH_H_\n-\n-#include <type_traits>\n-#include <dlfcn.h>\n-\n-//CUDA Backend\n-#include \"triton/external/CUDA/cuda.h\"\n-#include \"triton/external/CUDA/nvml.h\"\n-\n-//// HIP backend\n-//#define __HIP_PLATFORM_AMD__\n-#include \"triton/external/hip.h\"\n-\n-//Exceptions\n-#include <iostream>\n-#include <stdexcept>\n-\n-namespace llvm {\n-class PassRegistry;\n-class Module;\n-}\n-\n-namespace triton\n-{\n-namespace driver\n-{\n-\n-class cu_context;\n-\n-template<class T> void check(T){}\n-void check(CUresult err);\n-void check(hipError_t err);\n-\n-class dispatch\n-{\n-protected:\n-  template <class F>\n-  struct return_type;\n-\n-  template <class R, class... A>\n-  struct return_type<R (*)(A...)>\n-  { typedef R type; };\n-\n-  typedef bool (*f_init_t)();\n-\n-  template<f_init_t initializer, typename FunPtrT, typename... Args>\n-  static typename return_type<FunPtrT>::type f_impl(void*& lib_h, FunPtrT, void*& cache, const char * name, Args... args)\n-  {\n-    initializer();\n-    if(cache == nullptr){\n-      cache = dlsym(lib_h, name);\n-\t\t\tif(cache == 0)\n-\t\t\t\tthrow std::runtime_error(\"dlsym unable to load function\");\n-\t\t}\n-    FunPtrT fptr;\n-    *reinterpret_cast<void **>(&fptr) = cache;\n-    typename return_type<FunPtrT>::type res = (*fptr)(args...);\n-    check(res);\n-    return res;\n-  }\n-\n-public:\n-  static void release();\n-  // Nvidia\n-  static bool nvmlinit();\n-  static bool cuinit();\n-  // AMD\n-  static bool hipinit();\n-\n-  /* ------------------- *\n-   * CUDA\n-   * ------------------- */\n-  // context management\n-  static CUresult cuInit(unsigned int Flags);\n-  static CUresult cuCtxDestroy_v2(CUcontext ctx);\n-  static CUresult cuCtxCreate_v2(CUcontext *pctx, unsigned int flags, CUdevice dev);\n-  static CUresult cuCtxPushCurrent_v2(CUcontext ctx);\n-  static CUresult cuCtxPopCurrent_v2(CUcontext *pctx);\n-  static CUresult cuCtxGetDevice(CUdevice* result);\n-  static CUresult cuCtxEnablePeerAccess(CUcontext peerContext, unsigned int flags);\n-  static CUresult cuDriverGetVersion(int *driverVersion);\n-  // device management\n-  static CUresult cuDeviceGet(CUdevice *device, int ordinal);\n-  static CUresult cuDeviceGetName(char *name, int len, CUdevice dev);\n-  static CUresult cuDeviceGetPCIBusId(char *id, int len, CUdevice dev);\n-  static CUresult cuDeviceGetAttribute(int *pi, CUdevice_attribute attrib, CUdevice dev);\n-  static CUresult cuDeviceGetCount(int *count);\n-  // link management\n-  static CUresult cuLinkAddFile_v2(CUlinkState state, CUjitInputType type, const char *path, unsigned int numOptions, CUjit_option *options, void **optionValues);\n-  static CUresult cuLinkAddData_v2(CUlinkState state, CUjitInputType type, void* data, size_t size, const char* name, unsigned int numOptions, CUjit_option* options, void** optionValues);\n-  static CUresult cuLinkCreate_v2(unsigned int  numOptions, CUjit_option* options, void** optionValues, CUlinkState* stateOut);\n-  static CUresult cuLinkComplete(CUlinkState state, void** cubinOut, size_t* sizeOut);\n-  static CUresult cuLinkDestroy(CUlinkState state);\n-  // module management\n-  static CUresult cuModuleGetGlobal_v2(CUdeviceptr *dptr, size_t* bytes, CUmodule hmod, const char *name);\n-  static CUresult cuModuleLoad(CUmodule *module, const char *fname);\n-  static CUresult cuModuleLoadData(CUmodule* module, const void* image);\n-  static CUresult cuModuleUnload(CUmodule hmod);\n-  static CUresult cuModuleLoadDataEx(CUmodule *module, const void *image, unsigned int numOptions, CUjit_option *options, void **optionValues);\n-  static CUresult cuModuleGetFunction(CUfunction *hfunc, CUmodule hmod, const char *name);\n-  // stream management\n-  static CUresult cuStreamCreate(CUstream *phStream, unsigned int Flags);\n-  static CUresult cuStreamSynchronize(CUstream hStream);\n-  static CUresult cuStreamGetCtx(CUstream hStream, CUcontext* pctx);\n-  static CUresult cuStreamDestroy_v2(CUstream hStream);\n-  static CUresult cuLaunchKernel(CUfunction f, unsigned int gridDimX, unsigned int gridDimY, unsigned int gridDimZ, unsigned int blockDimX, unsigned int blockDimY, unsigned int blockDimZ, unsigned int sharedMemBytes, CUstream hStream, void **kernelParams, void **extra);\n-  // function management\n-  static CUresult cuFuncGetAttribute(int* pi, CUfunction_attribute attrib, CUfunction hfunc);\n-  static CUresult cuFuncSetAttribute(CUfunction hfunc, CUfunction_attribute attrib, int value);\n-  static CUresult cuFuncSetCacheConfig(CUfunction hfunc, CUfunc_cache config);\n-  // memory management\n-  static CUresult cuMemAlloc_v2(CUdeviceptr *dptr, size_t bytesize);\n-  static CUresult cuPointerGetAttribute(void * data, CUpointer_attribute attribute, CUdeviceptr ptr);\n-  static CUresult cuMemsetD8Async(CUdeviceptr dst, unsigned char x, size_t N, CUstream stream);\n-  static CUresult cuMemcpyDtoH_v2(void *dstHost, CUdeviceptr srcDevice, size_t ByteCount);\n-  static CUresult cuMemFree_v2(CUdeviceptr dptr);\n-  static CUresult cuMemcpyDtoHAsync_v2(void *dstHost, CUdeviceptr srcDevice, size_t ByteCount, CUstream hStream);\n-  static CUresult cuMemcpyHtoDAsync_v2(CUdeviceptr dstDevice, const void *srcHost, size_t ByteCount, CUstream hStream);\n-  static CUresult cuMemcpyHtoD_v2(CUdeviceptr dstDevice, const void *srcHost, size_t ByteCount);\n-  // event management\n-  static CUresult cuEventCreate(CUevent *phEvent, unsigned int Flags);\n-  static CUresult cuEventElapsedTime(float *pMilliseconds, CUevent hStart, CUevent hEnd);\n-  static CUresult cuEventRecord(CUevent hEvent, CUstream hStream);\n-  static CUresult cuEventDestroy_v2(CUevent hEvent);\n-\n-\n-  /* ------------------- *\n-   * NVML\n-   * ------------------- */\n-  static nvmlReturn_t nvmlDeviceGetHandleByPciBusId_v2( const char* pciBusId, nvmlDevice_t* device);\n-  static nvmlReturn_t nvmlDeviceGetClockInfo(nvmlDevice_t device, nvmlClockType_t type, unsigned int *clock);\n-  static nvmlReturn_t nvmlDeviceGetMaxClockInfo(nvmlDevice_t device, nvmlClockType_t type, unsigned int *clock);\n-  static nvmlReturn_t nvmlDeviceSetApplicationsClocks(nvmlDevice_t device, unsigned int mem_clock, unsigned int sm_clock);\n-\n-  /* ------------------- *\n-   * HIP\n-   * ------------------- */\n-  // context management\n-  static hipError_t hipInit(unsigned int Flags);\n-  static hipError_t hipCtxDestroy(hipCtx_t ctx);\n-  static hipError_t hipCtxCreate(hipCtx_t *pctx, unsigned int flags, hipDevice_t dev);\n-  static hipError_t hipCtxPushCurrent(hipCtx_t ctx);\n-  static hipError_t hipCtxPopCurrent(hipCtx_t *pctx);\n-  static hipError_t hipCtxGetDevice(hipDevice_t* result);\n-  static hipError_t hipCtxEnablePeerAccess(hipCtx_t peerContext, unsigned int flags);\n-  static hipError_t hipDriverGetVersion(int *driverVersion);\n-  // device management\n-  static hipError_t hipGetDevice(hipDevice_t *device, int ordinal);\n-  static hipError_t hipDeviceGetName(char *name, int len, hipDevice_t dev);\n-  static hipError_t hipDeviceGetPCIBusId(char *id, int len, hipDevice_t dev);\n-  static hipError_t hipDeviceGetAttribute(int *pi, hipDeviceAttribute_t attrib, hipDevice_t dev);\n-  static hipError_t hipGetDeviceCount(int *count);\n-  // module management\n-  static hipError_t hipModuleGetGlobal(hipDeviceptr_t *dptr, size_t* bytes, hipModule_t hmod, const char *name);\n-  static hipError_t hipModuleLoad(hipModule_t *module, const char *fname);\n-  static hipError_t hipModuleLoadData(hipModule_t* module, const void* image);\n-  static hipError_t hipModuleUnload(hipModule_t hmod);\n-  static hipError_t hipModuleLoadDataEx(hipModule_t *module, const void *image, unsigned int numOptions, hipJitOption *options, void **optionValues);\n-  static hipError_t hipModuleGetFunction(hipFunction_t *hfunc, hipModule_t hmod, const char *name);\n-  // stream management\n-  static hipError_t hipStreamCreate(hipStream_t *phStream, unsigned int Flags);\n-  static hipError_t hipStreamSynchronize(hipStream_t hStream);\n-  static hipError_t hipStreamDestroy(hipStream_t hStream);\n-  static hipError_t hipModuleLaunchKernel(hipFunction_t f, unsigned int gridDimX, unsigned int gridDimY, unsigned int gridDimZ, unsigned int blockDimX, unsigned int blockDimY, unsigned int blockDimZ, unsigned int sharedMemBytes, hipStream_t hStream, void **kernelParams, void **extra);\n-  // function management\n-  static hipError_t hipFuncGetAttributes(hipFuncAttributes* attrib, void* hfunc);\n-  static hipError_t hipFuncSetAttribute(hipFunction_t hfunc, hipFuncAttribute attrib, int value);\n-  static hipError_t hipFuncSetCacheConfig(hipFunction_t hfunc, hipFuncCache_t config);\n-  // memory management\n-  static hipError_t hipMalloc(hipDeviceptr_t *dptr, size_t bytesize);\n-  static hipError_t hipPointerGetAttribute(void * data, CUpointer_attribute attribute, hipDeviceptr_t ptr);\n-  static hipError_t hipMemsetD8Async(hipDeviceptr_t dst, unsigned char x, size_t N, hipStream_t stream);\n-  static hipError_t hipMemcpyDtoH(void *dstHost, hipDeviceptr_t srcDevice, size_t ByteCount);\n-  static hipError_t hipFree(hipDeviceptr_t dptr);\n-  static hipError_t hipMemcpyDtoHAsync(void *dstHost, hipDeviceptr_t srcDevice, size_t ByteCount, hipStream_t hStream);\n-  static hipError_t hipMemcpyHtoDAsync(hipDeviceptr_t dstDevice, const void *srcHost, size_t ByteCount, hipStream_t hStream);\n-  static hipError_t hipMemcpyHtoD(hipDeviceptr_t dstDevice, const void *srcHost, size_t ByteCount);\n-  // event management\n-  static hipError_t hipEventCreate(hipEvent_t *phEvent, unsigned int Flags);\n-  static hipError_t hipEventElapsedTime(float *pMilliseconds, hipEvent_t hStart, hipEvent_t hEnd);\n-  static hipError_t hipEventRecord(hipEvent_t hEvent, hipStream_t hStream);\n-  static hipError_t hipEventDestroy(hipEvent_t hEvent);\n-\n-\n-\n-private:\n-\n-  // Libraries\n-  static void* cuda_;\n-  static void* nvml_;\n-  static void* hip_;\n-\n-\n-  /* ------------------- *\n-   * CUDA\n-   * ------------------- */\n-  // context management\n-  static void* cuCtxGetCurrent_;\n-  static void* cuCtxSetCurrent_;\n-  static void* cuCtxDestroy_v2_;\n-  static void* cuCtxCreate_v2_;\n-  static void* cuCtxGetDevice_;\n-  static void* cuCtxPushCurrent_v2_;\n-  static void* cuCtxPopCurrent_v2_;\n-  static void* cuCtxEnablePeerAccess_;\n-  static void* cuDriverGetVersion_;\n-  static void* cuInit_;\n-  // device management\n-  static void* cuDeviceGet_;\n-  static void* cuDeviceGetName_;\n-  static void* cuDeviceGetPCIBusId_;\n-  static void* cuDeviceGetAttribute_;\n-  static void* cuDeviceGetCount_;\n-  // link management\n-  static void* cuLinkAddFile_v2_;\n-  static void* cuLinkAddData_v2_;\n-  static void* cuLinkCreate_v2_;\n-  static void* cuLinkDestroy_;\n-  static void* cuLinkComplete_;\n-  // module management\n-  static void* cuModuleGetGlobal_v2_;\n-  static void* cuModuleLoad_;\n-  static void* cuModuleUnload_;\n-  static void* cuModuleLoadDataEx_;\n-  static void* cuModuleLoadData_;\n-  static void* cuModuleGetFunction_;\n-  // stream management\n-  static void* cuStreamCreate_;\n-  static void* cuStreamSynchronize_;\n-  static void* cuStreamDestroy_v2_;\n-  static void* cuStreamGetCtx_;\n-  static void* cuLaunchKernel_;\n-  // function management\n-  static void* cuFuncGetAttribute_;\n-  static void* cuFuncSetAttribute_;\n-  static void* cuFuncSetCacheConfig_;\n-  // memory management\n-  static void* cuMemcpyDtoH_v2_;\n-  static void* cuMemFree_v2_;\n-  static void* cuMemcpyDtoHAsync_v2_;\n-  static void* cuMemcpyHtoDAsync_v2_;\n-  static void* cuMemcpyHtoD_v2_;\n-  static void* cuMemAlloc_v2_;\n-  static void* cuMemsetD8Async_;\n-  static void* cuPointerGetAttribute_;\n-  // event management\n-  static void* cuEventCreate_;\n-  static void* cuEventElapsedTime_;\n-  static void* cuEventRecord_;\n-  static void* cuEventDestroy_v2_;\n-\n-  /* ------------------- *\n-   * NVML\n-   * ------------------- */\n-  static void* nvmlInit_v2_;\n-  static void* nvmlDeviceGetHandleByPciBusId_v2_;\n-  static void* nvmlDeviceGetClockInfo_;\n-  static void* nvmlDeviceGetMaxClockInfo_;\n-  static void* nvmlDeviceSetApplicationsClocks_;\n-\n-  /* ------------------- *\n-   * HIP\n-   * ------------------- */\n-  // context management\n-  static void* hipInit_;\n-  static void* hipCtxDestroy_;\n-  static void* hipCtxCreate_;\n-  static void* hipCtxPushCurrent_;\n-  static void* hipCtxPopCurrent_;\n-  static void* hipCtxGetDevice_;\n-  static void* hipCtxEnablePeerAccess_;\n-  static void* hipDriverGetVersion_;\n-  // device management\n-  static void* hipGetDevice_;\n-  static void* hipDeviceGetName_;\n-  static void* hipDeviceGetPCIBusId_;\n-  static void* hipDeviceGetAttribute_;\n-  static void* hipGetDeviceCount_;\n-  // module management\n-  static void* hipModuleGetGlobal_;\n-  static void* hipModuleLoad_;\n-  static void* hipModuleLoadData_;\n-  static void* hipModuleUnload_;\n-  static void* hipModuleLoadDataEx_;\n-  static void* hipModuleGetFunction_;\n-  // stream management\n-  static void* hipStreamCreate_;\n-  static void* hipStreamSynchronize_;\n-  static void* hipStreamDestroy_;\n-  static void* hipModuleLaunchKernel_;;\n-  // function management\n-  static void* hipFuncGetAttributes_;\n-  static void* hipFuncSetAttribute_;\n-  static void* hipFuncSetCacheConfig_;\n-  // memory management\n-  static void* hipMalloc_;\n-  static void* hipPointerGetAttribute_;\n-  static void* hipMemsetD8Async_;\n-  static void* hipMemcpyDtoH_;\n-  static void* hipFree_;\n-  static void* hipMemcpyDtoHAsync_;\n-  static void* hipMemcpyHtoDAsync_;\n-  static void* hipMemcpyHtoD_;\n-  // event management\n-  static void* hipEventCreate_;\n-  static void* hipEventElapsedTime_;\n-  static void* hipEventRecord_;\n-  static void* hipEventDestroy_;\n-};\n-\n-}\n-}\n-\n-\n-#endif"}, {"filename": "include/triton/driver/error.h", "status": "removed", "additions": 0, "deletions": 220, "changes": 220, "file_content_changes": "@@ -1,220 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_DRIVER_ERROR_H_\n-#define _TRITON_DRIVER_ERROR_H_\n-\n-#include <exception>\n-#include \"triton/driver/dispatch.h\"\n-\n-\n-namespace triton\n-{\n-\n-  namespace driver\n-  {\n-\n-  namespace exception\n-  {\n-\n-  namespace nvrtc\n-  {\n-\n-#define TRITON_CREATE_NVRTC_EXCEPTION(name, msg) class name: public std::exception { public: const char * what() const throw(){ return \"NVRTC: Error- \" msg; } }\n-\n-  TRITON_CREATE_NVRTC_EXCEPTION(out_of_memory              ,\"out of memory\");\n-  TRITON_CREATE_NVRTC_EXCEPTION(program_creation_failure   ,\"program creation failure\");\n-  TRITON_CREATE_NVRTC_EXCEPTION(invalid_input              ,\"invalid input\");\n-  TRITON_CREATE_NVRTC_EXCEPTION(invalid_program            ,\"invalid program\");\n-  TRITON_CREATE_NVRTC_EXCEPTION(invalid_option             ,\"invalid option\");\n-  TRITON_CREATE_NVRTC_EXCEPTION(compilation                ,\"compilation\");\n-  TRITON_CREATE_NVRTC_EXCEPTION(builtin_operation_failure  ,\"builtin operation failure\");\n-  TRITON_CREATE_NVRTC_EXCEPTION(unknown_error              ,\"unknown error\");\n-\n-#undef TRITON_CREATE_NVRTC_EXCEPTION\n-  }\n-\n-\n-  namespace cuda\n-  {\n-  class base: public std::exception{};\n-\n-#define TRITON_CREATE_CUDA_EXCEPTION(name, msg) class name: public base { public:const char * what() const throw(){ return \"CUDA: Error- \" msg; } }\n-\n-\n-  TRITON_CREATE_CUDA_EXCEPTION(invalid_value                   ,\"invalid value\");\n-  TRITON_CREATE_CUDA_EXCEPTION(out_of_memory                   ,\"out of memory\");\n-  TRITON_CREATE_CUDA_EXCEPTION(not_initialized                 ,\"not initialized\");\n-  TRITON_CREATE_CUDA_EXCEPTION(deinitialized                   ,\"deinitialized\");\n-  TRITON_CREATE_CUDA_EXCEPTION(profiler_disabled               ,\"profiler disabled\");\n-  TRITON_CREATE_CUDA_EXCEPTION(profiler_not_initialized        ,\"profiler not initialized\");\n-  TRITON_CREATE_CUDA_EXCEPTION(profiler_already_started        ,\"profiler already started\");\n-  TRITON_CREATE_CUDA_EXCEPTION(profiler_already_stopped        ,\"profiler already stopped\");\n-  TRITON_CREATE_CUDA_EXCEPTION(no_device                       ,\"no device\");\n-  TRITON_CREATE_CUDA_EXCEPTION(invalid_device                  ,\"invalid device\");\n-  TRITON_CREATE_CUDA_EXCEPTION(invalid_image                   ,\"invalid image\");\n-  TRITON_CREATE_CUDA_EXCEPTION(invalid_context                 ,\"invalid context\");\n-  TRITON_CREATE_CUDA_EXCEPTION(context_already_current         ,\"context already current\");\n-  TRITON_CREATE_CUDA_EXCEPTION(map_failed                      ,\"map failed\");\n-  TRITON_CREATE_CUDA_EXCEPTION(unmap_failed                    ,\"unmap failed\");\n-  TRITON_CREATE_CUDA_EXCEPTION(array_is_mapped                 ,\"array is mapped\");\n-  TRITON_CREATE_CUDA_EXCEPTION(already_mapped                  ,\"already mapped\");\n-  TRITON_CREATE_CUDA_EXCEPTION(no_binary_for_gpu               ,\"no binary for gpu\");\n-  TRITON_CREATE_CUDA_EXCEPTION(already_acquired                ,\"already acquired\");\n-  TRITON_CREATE_CUDA_EXCEPTION(not_mapped                      ,\"not mapped\");\n-  TRITON_CREATE_CUDA_EXCEPTION(not_mapped_as_array             ,\"not mapped as array\");\n-  TRITON_CREATE_CUDA_EXCEPTION(not_mapped_as_pointer           ,\"not mapped as pointer\");\n-  TRITON_CREATE_CUDA_EXCEPTION(ecc_uncorrectable               ,\"ecc uncorrectable\");\n-  TRITON_CREATE_CUDA_EXCEPTION(unsupported_limit               ,\"unsupported limit\");\n-  TRITON_CREATE_CUDA_EXCEPTION(context_already_in_use          ,\"context already in use\");\n-  TRITON_CREATE_CUDA_EXCEPTION(peer_access_unsupported         ,\"peer access unsupported\");\n-  TRITON_CREATE_CUDA_EXCEPTION(invalid_ptx                     ,\"invalid ptx\");\n-  TRITON_CREATE_CUDA_EXCEPTION(invalid_graphics_context        ,\"invalid graphics context\");\n-  TRITON_CREATE_CUDA_EXCEPTION(invalid_source                  ,\"invalid source\");\n-  TRITON_CREATE_CUDA_EXCEPTION(file_not_found                  ,\"file not found\");\n-  TRITON_CREATE_CUDA_EXCEPTION(shared_object_symbol_not_found  ,\"shared object symbol not found\");\n-  TRITON_CREATE_CUDA_EXCEPTION(shared_object_init_failed       ,\"shared object init failed\");\n-  TRITON_CREATE_CUDA_EXCEPTION(operating_system                ,\"operating system\");\n-  TRITON_CREATE_CUDA_EXCEPTION(invalid_handle                  ,\"invalid handle\");\n-  TRITON_CREATE_CUDA_EXCEPTION(not_found                       ,\"not found\");\n-  TRITON_CREATE_CUDA_EXCEPTION(not_ready                       ,\"not ready\");\n-  TRITON_CREATE_CUDA_EXCEPTION(illegal_address                 ,\"illegal address\");\n-  TRITON_CREATE_CUDA_EXCEPTION(launch_out_of_resources         ,\"launch out of resources\");\n-  TRITON_CREATE_CUDA_EXCEPTION(launch_timeout                  ,\"launch timeout\");\n-  TRITON_CREATE_CUDA_EXCEPTION(launch_incompatible_texturing   ,\"launch incompatible texturing\");\n-  TRITON_CREATE_CUDA_EXCEPTION(peer_access_already_enabled     ,\"peer access already enabled\");\n-  TRITON_CREATE_CUDA_EXCEPTION(peer_access_not_enabled         ,\"peer access not enabled\");\n-  TRITON_CREATE_CUDA_EXCEPTION(primary_context_active          ,\"primary context active\");\n-  TRITON_CREATE_CUDA_EXCEPTION(context_is_destroyed            ,\"context is destroyed\");\n-  TRITON_CREATE_CUDA_EXCEPTION(assert_error                    ,\"assert\");\n-  TRITON_CREATE_CUDA_EXCEPTION(too_many_peers                  ,\"too many peers\");\n-  TRITON_CREATE_CUDA_EXCEPTION(host_memory_already_registered  ,\"host memory already registered\");\n-  TRITON_CREATE_CUDA_EXCEPTION(host_memory_not_registered      ,\"hot memory not registered\");\n-  TRITON_CREATE_CUDA_EXCEPTION(hardware_stack_error            ,\"hardware stack error\");\n-  TRITON_CREATE_CUDA_EXCEPTION(illegal_instruction             ,\"illegal instruction\");\n-  TRITON_CREATE_CUDA_EXCEPTION(misaligned_address              ,\"misaligned address\");\n-  TRITON_CREATE_CUDA_EXCEPTION(invalid_address_space           ,\"invalid address space\");\n-  TRITON_CREATE_CUDA_EXCEPTION(invalid_pc                      ,\"invalid pc\");\n-  TRITON_CREATE_CUDA_EXCEPTION(launch_failed                   ,\"launch failed\");\n-  TRITON_CREATE_CUDA_EXCEPTION(not_permitted                   ,\"not permitted\");\n-  TRITON_CREATE_CUDA_EXCEPTION(not_supported                   ,\"not supported\");\n-  TRITON_CREATE_CUDA_EXCEPTION(unknown                         ,\"unknown\");\n-\n-#undef TRITON_CREATE_CUDA_EXCEPTION\n-  }\n-\n-  namespace cublas\n-  {\n-  class base: public std::exception{};\n-\n-#define TRITON_CREATE_CUBLAS_EXCEPTION(name, msg) class name: public base { public: const char * what() const throw(){ return \"CUBLAS: Error- \" msg; } }\n-\n-  TRITON_CREATE_CUBLAS_EXCEPTION(not_initialized              ,\"not initialized\");\n-  TRITON_CREATE_CUBLAS_EXCEPTION(alloc_failed                 ,\"alloc failed\");\n-  TRITON_CREATE_CUBLAS_EXCEPTION(invalid_value                ,\"invalid value\");\n-  TRITON_CREATE_CUBLAS_EXCEPTION(arch_mismatch                ,\"arch mismatch\");\n-  TRITON_CREATE_CUBLAS_EXCEPTION(mapping_error                ,\"mapping error\");\n-  TRITON_CREATE_CUBLAS_EXCEPTION(execution_failed             ,\"execution failed\");\n-  TRITON_CREATE_CUBLAS_EXCEPTION(internal_error               ,\"internal error\");\n-  TRITON_CREATE_CUBLAS_EXCEPTION(not_supported                ,\"not supported\");\n-  TRITON_CREATE_CUBLAS_EXCEPTION(license_error                ,\"license error\");\n-  TRITON_CREATE_CUBLAS_EXCEPTION(unknown                      ,\"unknown\");\n-\n-#undef TRITON_CREATE_CUBLAS_EXCEPTION\n-  }\n-\n-  namespace cudnn\n-  {\n-#define TRITON_CREATE_CUDNN_EXCEPTION(name, msg) class name: public std::exception { public: const char * what() const throw(){ return \"CUDNN: Error- \" msg; } }\n-\n-  TRITON_CREATE_CUDNN_EXCEPTION(not_initialized              ,\"not initialized\");\n-  TRITON_CREATE_CUDNN_EXCEPTION(alloc_failed                 ,\"allocation failed\");\n-  TRITON_CREATE_CUDNN_EXCEPTION(bad_param                    ,\"bad param\");\n-  TRITON_CREATE_CUDNN_EXCEPTION(internal_error               ,\"internal error\");\n-  TRITON_CREATE_CUDNN_EXCEPTION(invalid_value                ,\"invalid value\");\n-  TRITON_CREATE_CUDNN_EXCEPTION(arch_mismatch                ,\"arch mismatch\");\n-  TRITON_CREATE_CUDNN_EXCEPTION(mapping_error                ,\"mapping error\");\n-  TRITON_CREATE_CUDNN_EXCEPTION(execution_failed             ,\"execution failed\");\n-  TRITON_CREATE_CUDNN_EXCEPTION(not_supported                ,\"not supported\");\n-  TRITON_CREATE_CUDNN_EXCEPTION(license_error                ,\"license error\");\n-  TRITON_CREATE_CUDNN_EXCEPTION(runtime_prerequisite_missing ,\"prerequisite missing\");\n-  TRITON_CREATE_CUDNN_EXCEPTION(runtime_in_progress          ,\"runtime in progress\");\n-  TRITON_CREATE_CUDNN_EXCEPTION(runtime_fp_overflow          ,\"runtime fp overflow\");\n-  }\n-\n-\n-\n-\n-  namespace hip\n-  {\n-  class base: public std::exception{};\n-\n-#define TRITON_CREATE_HIP_EXCEPTION(name, msg) class name: public base { public:const char * what() const throw(){ return \"HIP: Error- \" msg; } }\n-\n-\n-  TRITON_CREATE_HIP_EXCEPTION(invalid_value                   ,\"invalid value\");\n-  TRITON_CREATE_HIP_EXCEPTION(out_of_memory                   ,\"out of memory\");\n-  TRITON_CREATE_HIP_EXCEPTION(not_initialized                 ,\"not initialized\");\n-  TRITON_CREATE_HIP_EXCEPTION(deinitialized                   ,\"deinitialized\");\n-  TRITON_CREATE_HIP_EXCEPTION(profiler_disabled               ,\"profiler disabled\");\n-  TRITON_CREATE_HIP_EXCEPTION(profiler_not_initialized        ,\"profiler not initialized\");\n-  TRITON_CREATE_HIP_EXCEPTION(profiler_already_started        ,\"profiler already started\");\n-  TRITON_CREATE_HIP_EXCEPTION(profiler_already_stopped        ,\"profiler already stopped\");\n-  TRITON_CREATE_HIP_EXCEPTION(no_device                       ,\"no device\");\n-  TRITON_CREATE_HIP_EXCEPTION(invalid_device                  ,\"invalid device\");\n-  TRITON_CREATE_HIP_EXCEPTION(invalid_image                   ,\"invalid image\");\n-  TRITON_CREATE_HIP_EXCEPTION(invalid_context                 ,\"invalid context\");\n-  TRITON_CREATE_HIP_EXCEPTION(context_already_current         ,\"context already current\");\n-  TRITON_CREATE_HIP_EXCEPTION(map_failed                      ,\"map failed\");\n-  TRITON_CREATE_HIP_EXCEPTION(unmap_failed                    ,\"unmap failed\");\n-  TRITON_CREATE_HIP_EXCEPTION(array_is_mapped                 ,\"array is mapped\");\n-  TRITON_CREATE_HIP_EXCEPTION(already_mapped                  ,\"already mapped\");\n-  TRITON_CREATE_HIP_EXCEPTION(no_binary_for_gpu               ,\"no binary for gpu\");\n-  TRITON_CREATE_HIP_EXCEPTION(already_acquired                ,\"already acquired\");\n-  TRITON_CREATE_HIP_EXCEPTION(not_mapped                      ,\"not mapped\");\n-  TRITON_CREATE_HIP_EXCEPTION(not_mapped_as_array             ,\"not mapped as array\");\n-  TRITON_CREATE_HIP_EXCEPTION(not_mapped_as_pointer           ,\"not mapped as pointer\");\n-  TRITON_CREATE_HIP_EXCEPTION(ecc_uncorrectable               ,\"ecc uncorrectable\");\n-  TRITON_CREATE_HIP_EXCEPTION(unsupported_limit               ,\"unsupported limit\");\n-  TRITON_CREATE_HIP_EXCEPTION(context_already_in_use          ,\"context already in use\");\n-  TRITON_CREATE_HIP_EXCEPTION(peer_access_unsupported         ,\"peer access unsupported\");\n-  TRITON_CREATE_HIP_EXCEPTION(invalid_ptx                     ,\"invalid ptx\");\n-  TRITON_CREATE_HIP_EXCEPTION(invalid_graphics_context        ,\"invalid graphics context\");\n-  TRITON_CREATE_HIP_EXCEPTION(invalid_source                  ,\"invalid source\");\n-  TRITON_CREATE_HIP_EXCEPTION(file_not_found                  ,\"file not found\");\n-  TRITON_CREATE_HIP_EXCEPTION(shared_object_symbol_not_found  ,\"shared object symbol not found\");\n-  TRITON_CREATE_HIP_EXCEPTION(shared_object_init_failed       ,\"shared object init failed\");\n-  TRITON_CREATE_HIP_EXCEPTION(operating_system                ,\"operating system\");\n-  TRITON_CREATE_HIP_EXCEPTION(invalid_handle                  ,\"invalid handle\");\n-  TRITON_CREATE_HIP_EXCEPTION(not_found                       ,\"not found\");\n-  TRITON_CREATE_HIP_EXCEPTION(not_ready                       ,\"not ready\");\n-  TRITON_CREATE_HIP_EXCEPTION(illegal_address                 ,\"illegal address\");\n-  TRITON_CREATE_HIP_EXCEPTION(launch_out_of_resources         ,\"launch out of resources\");\n-  TRITON_CREATE_HIP_EXCEPTION(launch_timeout                  ,\"launch timeout\");\n-  TRITON_CREATE_HIP_EXCEPTION(launch_incompatible_texturing   ,\"launch incompatible texturing\");\n-  TRITON_CREATE_HIP_EXCEPTION(peer_access_already_enabled     ,\"peer access already enabled\");\n-  TRITON_CREATE_HIP_EXCEPTION(peer_access_not_enabled         ,\"peer access not enabled\");\n-  TRITON_CREATE_HIP_EXCEPTION(primary_context_active          ,\"primary context active\");\n-  TRITON_CREATE_HIP_EXCEPTION(context_is_destroyed            ,\"context is destroyed\");\n-  TRITON_CREATE_HIP_EXCEPTION(assert_error                    ,\"assert\");\n-  TRITON_CREATE_HIP_EXCEPTION(too_many_peers                  ,\"too many peers\");\n-  TRITON_CREATE_HIP_EXCEPTION(host_memory_already_registered  ,\"host memory already registered\");\n-  TRITON_CREATE_HIP_EXCEPTION(host_memory_not_registered      ,\"hot memory not registered\");\n-  TRITON_CREATE_HIP_EXCEPTION(hardware_stack_error            ,\"hardware stack error\");\n-  TRITON_CREATE_HIP_EXCEPTION(illegal_instruction             ,\"illegal instruction\");\n-  TRITON_CREATE_HIP_EXCEPTION(misaligned_address              ,\"misaligned address\");\n-  TRITON_CREATE_HIP_EXCEPTION(invalid_address_space           ,\"invalid address space\");\n-  TRITON_CREATE_HIP_EXCEPTION(invalid_pc                      ,\"invalid pc\");\n-  TRITON_CREATE_HIP_EXCEPTION(launch_failed                   ,\"launch failed\");\n-  TRITON_CREATE_HIP_EXCEPTION(not_permitted                   ,\"not permitted\");\n-  TRITON_CREATE_HIP_EXCEPTION(not_supported                   ,\"not supported\");\n-  TRITON_CREATE_HIP_EXCEPTION(invalid_symbol                   ,\"invalid symbol\");\n-  TRITON_CREATE_HIP_EXCEPTION(unknown                         ,\"unknown\");\n-\n-#undef TRITON_CREATE_CUDA_EXCEPTION\n-  }\n-\n-  }\n-  }\n-}\n-\n-#endif"}, {"filename": "include/triton/driver/llvm.h", "status": "removed", "additions": 0, "deletions": 20, "changes": 20, "file_content_changes": "@@ -1,20 +0,0 @@\n-#include <string>\n-#include \"triton/driver/dispatch.h\"\n-\n-namespace llvm{\n-class Module;\n-}\n-\n-namespace triton{\n-namespace driver{\n-\n-void init_llvm();\n-std::string path_to_ptxas(int& version);\n-std::string llir_to_ptx(llvm::Module* module, int cc, int version);\n-std::string ptx_to_cubin(const std::string& ptx, const std::string& ptxas_path, int cc);\n-CUmodule ptx_to_cumodule(const std::string& ptx, int cc);\n-std::string llir_to_amdgpu(llvm::Module* module, const std::string& proc);\n-hipModule_t amdgpu_to_hipmodule(const std::string& path);\n-\n-}\n-}"}, {"filename": "include/triton/external/CUDA/nvml.h", "status": "removed", "additions": 0, "deletions": 6281, "changes": 6281, "file_content_changes": "N/A"}, {"filename": "include/triton/external/half.hpp", "status": "removed", "additions": 0, "deletions": 3067, "changes": 3067, "file_content_changes": "N/A"}, {"filename": "include/triton/external/hip.h", "status": "removed", "additions": 0, "deletions": 288, "changes": 288, "file_content_changes": "@@ -1,288 +0,0 @@\n-/*\n- * @brief hipError_t\n- * @enum\n- * @ingroup Enumerations\n- */\n-// Developer note - when updating these, update the hipErrorName and hipErrorString functions in\n-// NVCC and HCC paths Also update the hipCUDAErrorTohipError function in NVCC path.\n-\n-// Ignoring error-code return values from hip APIs is discouraged. On C++17,\n-// we can make that yield a warning\n-\n-/*\n- * @brief hipError_t\n- * @enum\n- * @ingroup Enumerations\n- */\n-// Developer note - when updating these, update the hipErrorName and hipErrorString functions in\n-// NVCC and HCC paths Also update the hipCUDAErrorTohipError function in NVCC path.\n-\n-#include <cstddef>\n-\n-typedef enum hipError_t {\n-    hipSuccess = 0,  ///< Successful completion.\n-    hipErrorInvalidValue = 1,  ///< One or more of the parameters passed to the API call is NULL\n-                               ///< or not in an acceptable range.\n-    hipErrorOutOfMemory = 2,\n-    // Deprecated\n-    hipErrorMemoryAllocation = 2,  ///< Memory allocation error.\n-    hipErrorNotInitialized = 3,\n-    // Deprecated\n-    hipErrorInitializationError = 3,\n-    hipErrorDeinitialized = 4,\n-    hipErrorProfilerDisabled = 5,\n-    hipErrorProfilerNotInitialized = 6,\n-    hipErrorProfilerAlreadyStarted = 7,\n-    hipErrorProfilerAlreadyStopped = 8,\n-    hipErrorInvalidConfiguration = 9,\n-    hipErrorInvalidPitchValue = 12,\n-    hipErrorInvalidSymbol = 13,\n-    hipErrorInvalidDevicePointer = 17,  ///< Invalid Device Pointer\n-    hipErrorInvalidMemcpyDirection = 21,  ///< Invalid memory copy direction\n-    hipErrorInsufficientDriver = 35,\n-    hipErrorMissingConfiguration = 52,\n-    hipErrorPriorLaunchFailure = 53,\n-    hipErrorInvalidDeviceFunction = 98,\n-    hipErrorNoDevice = 100,  ///< Call to hipGetDeviceCount returned 0 devices\n-    hipErrorInvalidDevice = 101,  ///< DeviceID must be in range 0...#compute-devices.\n-    hipErrorInvalidImage = 200,\n-    hipErrorInvalidContext = 201,  ///< Produced when input context is invalid.\n-    hipErrorContextAlreadyCurrent = 202,\n-    hipErrorMapFailed = 205,\n-    // Deprecated\n-    hipErrorMapBufferObjectFailed = 205,  ///< Produced when the IPC memory attach failed from ROCr.\n-    hipErrorUnmapFailed = 206,\n-    hipErrorArrayIsMapped = 207,\n-    hipErrorAlreadyMapped = 208,\n-    hipErrorNoBinaryForGpu = 209,\n-    hipErrorAlreadyAcquired = 210,\n-    hipErrorNotMapped = 211,\n-    hipErrorNotMappedAsArray = 212,\n-    hipErrorNotMappedAsPointer = 213,\n-    hipErrorECCNotCorrectable = 214,\n-    hipErrorUnsupportedLimit = 215,\n-    hipErrorContextAlreadyInUse = 216,\n-    hipErrorPeerAccessUnsupported = 217,\n-    hipErrorInvalidKernelFile = 218,  ///< In CUDA DRV, it is CUDA_ERROR_INVALID_PTX\n-    hipErrorInvalidGraphicsContext = 219,\n-    hipErrorInvalidSource = 300,\n-    hipErrorFileNotFound = 301,\n-    hipErrorSharedObjectSymbolNotFound = 302,\n-    hipErrorSharedObjectInitFailed = 303,\n-    hipErrorOperatingSystem = 304,\n-    hipErrorInvalidHandle = 400,\n-    // Deprecated\n-    hipErrorInvalidResourceHandle = 400,  ///< Resource handle (hipEvent_t or hipStream_t) invalid.\n-    hipErrorNotFound = 500,\n-    hipErrorNotReady = 600,  ///< Indicates that asynchronous operations enqueued earlier are not\n-                             ///< ready.  This is not actually an error, but is used to distinguish\n-                             ///< from hipSuccess (which indicates completion).  APIs that return\n-                             ///< this error include hipEventQuery and hipStreamQuery.\n-    hipErrorIllegalAddress = 700,\n-    hipErrorLaunchOutOfResources = 701,  ///< Out of resources error.\n-    hipErrorLaunchTimeOut = 702,\n-    hipErrorPeerAccessAlreadyEnabled =\n-        704,  ///< Peer access was already enabled from the current device.\n-    hipErrorPeerAccessNotEnabled =\n-        705,  ///< Peer access was never enabled from the current device.\n-    hipErrorSetOnActiveProcess = 708,\n-    hipErrorAssert = 710,  ///< Produced when the kernel calls assert.\n-    hipErrorHostMemoryAlreadyRegistered =\n-        712,  ///< Produced when trying to lock a page-locked memory.\n-    hipErrorHostMemoryNotRegistered =\n-        713,  ///< Produced when trying to unlock a non-page-locked memory.\n-    hipErrorLaunchFailure =\n-        719,  ///< An exception occurred on the device while executing a kernel.\n-    hipErrorCooperativeLaunchTooLarge =\n-        720,  ///< This error indicates that the number of blocks launched per grid for a kernel\n-              ///< that was launched via cooperative launch APIs exceeds the maximum number of\n-              ///< allowed blocks for the current device\n-    hipErrorNotSupported = 801,  ///< Produced when the hip API is not supported/implemented\n-    hipErrorUnknown = 999,  //< Unknown error.\n-    // HSA Runtime Error Codes start here.\n-    hipErrorRuntimeMemory = 1052,  ///< HSA runtime memory call returned error.  Typically not seen\n-                                   ///< in production systems.\n-    hipErrorRuntimeOther = 1053,  ///< HSA runtime call other than memory returned error.  Typically\n-                                  ///< not seen in production systems.\n-    hipErrorTbd  ///< Marker that more error codes are needed.\n-} hipError_t;\n-\n-\n-typedef struct ihipCtx_t* hipCtx_t;\n-\n-// Note many APIs also use integer deviceIds as an alternative to the device pointer:\n-typedef int hipDevice_t;\n-\n-typedef enum hipDeviceP2PAttr {\n-  hipDevP2PAttrPerformanceRank = 0,\n-  hipDevP2PAttrAccessSupported,\n-  hipDevP2PAttrNativeAtomicSupported,\n-  hipDevP2PAttrHipArrayAccessSupported\n-} hipDeviceP2PAttr;\n-\n-typedef struct ihipStream_t* hipStream_t;\n-\n-#define hipIpcMemLazyEnablePeerAccess 0\n-\n-#define HIP_IPC_HANDLE_SIZE 64\n-\n-typedef struct hipIpcMemHandle_st {\n-    char reserved[HIP_IPC_HANDLE_SIZE];\n-} hipIpcMemHandle_t;\n-\n-typedef struct hipIpcEventHandle_st {\n-    char reserved[HIP_IPC_HANDLE_SIZE];\n-} hipIpcEventHandle_t;\n-\n-typedef struct ihipModule_t* hipModule_t;\n-\n-typedef struct ihipModuleSymbol_t* hipFunction_t;\n-\n-typedef struct hipFuncAttributes {\n-    int binaryVersion;\n-    int cacheModeCA;\n-    size_t constSizeBytes;\n-    size_t localSizeBytes;\n-    int maxDynamicSharedSizeBytes;\n-    int maxThreadsPerBlock;\n-    int numRegs;\n-    int preferredShmemCarveout;\n-    int ptxVersion;\n-    size_t sharedSizeBytes;\n-} hipFuncAttributes;\n-\n-typedef struct ihipEvent_t* hipEvent_t;\n-\n-/*\n- * @brief hipDeviceAttribute_t\n- * @enum\n- * @ingroup Enumerations\n- */\n-typedef enum hipDeviceAttribute_t {\n-    hipDeviceAttributeMaxThreadsPerBlock,       ///< Maximum number of threads per block.\n-    hipDeviceAttributeMaxBlockDimX,             ///< Maximum x-dimension of a block.\n-    hipDeviceAttributeMaxBlockDimY,             ///< Maximum y-dimension of a block.\n-    hipDeviceAttributeMaxBlockDimZ,             ///< Maximum z-dimension of a block.\n-    hipDeviceAttributeMaxGridDimX,              ///< Maximum x-dimension of a grid.\n-    hipDeviceAttributeMaxGridDimY,              ///< Maximum y-dimension of a grid.\n-    hipDeviceAttributeMaxGridDimZ,              ///< Maximum z-dimension of a grid.\n-    hipDeviceAttributeMaxSharedMemoryPerBlock,  ///< Maximum shared memory available per block in\n-                                                ///< bytes.\n-    hipDeviceAttributeTotalConstantMemory,      ///< Constant memory size in bytes.\n-    hipDeviceAttributeWarpSize,                 ///< Warp size in threads.\n-    hipDeviceAttributeMaxRegistersPerBlock,  ///< Maximum number of 32-bit registers available to a\n-                                             ///< thread block. This number is shared by all thread\n-                                             ///< blocks simultaneously resident on a\n-                                             ///< multiprocessor.\n-    hipDeviceAttributeClockRate,             ///< Peak clock frequency in kilohertz.\n-    hipDeviceAttributeMemoryClockRate,       ///< Peak memory clock frequency in kilohertz.\n-    hipDeviceAttributeMemoryBusWidth,        ///< Global memory bus width in bits.\n-    hipDeviceAttributeMultiprocessorCount,   ///< Number of multiprocessors on the device.\n-    hipDeviceAttributeComputeMode,           ///< Compute mode that device is currently in.\n-    hipDeviceAttributeL2CacheSize,  ///< Size of L2 cache in bytes. 0 if the device doesn't have L2\n-                                    ///< cache.\n-    hipDeviceAttributeMaxThreadsPerMultiProcessor,  ///< Maximum resident threads per\n-                                                    ///< multiprocessor.\n-    hipDeviceAttributeComputeCapabilityMajor,       ///< Major compute capability version number.\n-    hipDeviceAttributeComputeCapabilityMinor,       ///< Minor compute capability version number.\n-    hipDeviceAttributeConcurrentKernels,  ///< Device can possibly execute multiple kernels\n-                                          ///< concurrently.\n-    hipDeviceAttributePciBusId,           ///< PCI Bus ID.\n-    hipDeviceAttributePciDeviceId,        ///< PCI Device ID.\n-    hipDeviceAttributeMaxSharedMemoryPerMultiprocessor,  ///< Maximum Shared Memory Per\n-                                                         ///< Multiprocessor.\n-    hipDeviceAttributeIsMultiGpuBoard,                   ///< Multiple GPU devices.\n-    hipDeviceAttributeIntegrated,                        ///< iGPU\n-    hipDeviceAttributeCooperativeLaunch,                 ///< Support cooperative launch\n-    hipDeviceAttributeCooperativeMultiDeviceLaunch,      ///< Support cooperative launch on multiple devices\n-    hipDeviceAttributeMaxTexture1DWidth,    ///< Maximum number of elements in 1D images\n-    hipDeviceAttributeMaxTexture2DWidth,    ///< Maximum dimension width of 2D images in image elements\n-    hipDeviceAttributeMaxTexture2DHeight,   ///< Maximum dimension height of 2D images in image elements\n-    hipDeviceAttributeMaxTexture3DWidth,    ///< Maximum dimension width of 3D images in image elements\n-    hipDeviceAttributeMaxTexture3DHeight,   ///< Maximum dimensions height of 3D images in image elements\n-    hipDeviceAttributeMaxTexture3DDepth,    ///< Maximum dimensions depth of 3D images in image elements\n-\n-    hipDeviceAttributeHdpMemFlushCntl,      ///< Address of the HDP_MEM_COHERENCY_FLUSH_CNTL register\n-    hipDeviceAttributeHdpRegFlushCntl,      ///< Address of the HDP_REG_COHERENCY_FLUSH_CNTL register\n-\n-    hipDeviceAttributeMaxPitch,             ///< Maximum pitch in bytes allowed by memory copies\n-    hipDeviceAttributeTextureAlignment,     ///<Alignment requirement for textures\n-    hipDeviceAttributeTexturePitchAlignment, ///<Pitch alignment requirement for 2D texture references bound to pitched memory;\n-    hipDeviceAttributeKernelExecTimeout,    ///<Run time limit for kernels executed on the device\n-    hipDeviceAttributeCanMapHostMemory,     ///<Device can map host memory into device address space\n-    hipDeviceAttributeEccEnabled,           ///<Device has ECC support enabled\n-\n-    hipDeviceAttributeCooperativeMultiDeviceUnmatchedFunc,        ///< Supports cooperative launch on multiple\n-                                                                  ///devices with unmatched functions\n-    hipDeviceAttributeCooperativeMultiDeviceUnmatchedGridDim,     ///< Supports cooperative launch on multiple\n-                                                                  ///devices with unmatched grid dimensions\n-    hipDeviceAttributeCooperativeMultiDeviceUnmatchedBlockDim,    ///< Supports cooperative launch on multiple\n-                                                                  ///devices with unmatched block dimensions\n-    hipDeviceAttributeCooperativeMultiDeviceUnmatchedSharedMem,   ///< Supports cooperative launch on multiple\n-                                                                  ///devices with unmatched shared memories\n-    hipDeviceAttributeAsicRevision,         ///< Revision of the GPU in this device\n-    hipDeviceAttributeManagedMemory,        ///< Device supports allocating managed memory on this system\n-    hipDeviceAttributeDirectManagedMemAccessFromHost, ///< Host can directly access managed memory on\n-                                                      /// the device without migration\n-    hipDeviceAttributeConcurrentManagedAccess,  ///< Device can coherently access managed memory\n-                                                /// concurrently with the CPU\n-    hipDeviceAttributePageableMemoryAccess,     ///< Device supports coherently accessing pageable memory\n-                                                /// without calling hipHostRegister on it\n-    hipDeviceAttributePageableMemoryAccessUsesHostPageTables, ///< Device accesses pageable memory via\n-                                                              /// the host's page tables\n-    hipDeviceAttributeCanUseStreamWaitValue ///< '1' if Device supports hipStreamWaitValue32() and\n-                                            ///< hipStreamWaitValue64() , '0' otherwise.\n-\n-} hipDeviceAttribute_t;\n-\n-typedef void* hipDeviceptr_t;\n-\n-/*\n- * @brief hipJitOption\n- * @enum\n- * @ingroup Enumerations\n- */\n-typedef enum hipJitOption {\n-    hipJitOptionMaxRegisters = 0,\n-    hipJitOptionThreadsPerBlock,\n-    hipJitOptionWallTime,\n-    hipJitOptionInfoLogBuffer,\n-    hipJitOptionInfoLogBufferSizeBytes,\n-    hipJitOptionErrorLogBuffer,\n-    hipJitOptionErrorLogBufferSizeBytes,\n-    hipJitOptionOptimizationLevel,\n-    hipJitOptionTargetFromContext,\n-    hipJitOptionTarget,\n-    hipJitOptionFallbackStrategy,\n-    hipJitOptionGenerateDebugInfo,\n-    hipJitOptionLogVerbose,\n-    hipJitOptionGenerateLineInfo,\n-    hipJitOptionCacheMode,\n-    hipJitOptionSm3xOpt,\n-    hipJitOptionFastCompile,\n-    hipJitOptionNumOptions\n-} hipJitOption;\n-\n-/**\n- * @warning On AMD devices and some Nvidia devices, these hints and controls are ignored.\n- */\n-typedef enum hipFuncAttribute {\n-    hipFuncAttributeMaxDynamicSharedMemorySize = 8,\n-    hipFuncAttributePreferredSharedMemoryCarveout = 9,\n-    hipFuncAttributeMax\n-} hipFuncAttribute;\n-\n-/**\n- * @warning On AMD devices and some Nvidia devices, these hints and controls are ignored.\n- */\n-typedef enum hipFuncCache_t {\n-    hipFuncCachePreferNone,    ///< no preference for shared memory or L1 (default)\n-    hipFuncCachePreferShared,  ///< prefer larger shared memory and smaller L1 cache\n-    hipFuncCachePreferL1,      ///< prefer larger L1 cache and smaller shared memory\n-    hipFuncCachePreferEqual,   ///< prefer equal size L1 cache and shared memory\n-} hipFuncCache_t;\n-\n-\n-#define HIP_LAUNCH_PARAM_BUFFER_POINTER ((void*)0x01)\n-#define HIP_LAUNCH_PARAM_BUFFER_SIZE ((void*)0x02)\n-#define HIP_LAUNCH_PARAM_END ((void*)0x03)"}, {"filename": "include/triton/ir/basic_block.h", "status": "removed", "additions": 0, "deletions": 92, "changes": 92, "file_content_changes": "@@ -1,92 +0,0 @@\n-\ufeff#pragma once\n-\n-#ifndef _TRITON_IR_BASIC_BLOCK_H_\n-#define _TRITON_IR_BASIC_BLOCK_H_\n-\n-#include <string>\n-#include <list>\n-#include \"value.h\"\n-#include \"visitor.h\"\n-\n-namespace triton{\n-namespace ir{\n-\n-class context;\n-class function;\n-class instruction;\n-\n-/* Basic Block */\n-class basic_block: public value{\n-public:\n-  // instruction iterator types\n-  typedef std::list<instruction*>                inst_list_t;\n-  typedef inst_list_t::iterator                  iterator;\n-  typedef inst_list_t::const_iterator            const_iterator;\n-  typedef inst_list_t::reverse_iterator          reverse_iterator;\n-  typedef inst_list_t::const_reverse_iterator    const_reverse_iterator;\n-\n-private:\n-  // constructors\n-  basic_block(context &ctx, const std::string &name, function *parent, basic_block *next);\n-\n-public:\n-  // accessors\n-  function* get_parent() { return parent_; }\n-  context& get_context() { return ctx_; }\n-\n-  // get iterator to first instruction that is not a phi\n-  void replace_phi_uses_with(basic_block* before, basic_block* after);\n-  iterator get_first_non_phi();\n-\n-  // get instruction list\n-  inst_list_t           &get_inst_list()       { return inst_list_; }\n-  const inst_list_t     &get_inst_list() const { return inst_list_; }\n-  void  erase(instruction *i)                  {  inst_list_.remove(i); }\n-\n-  // instruction iterator functions\n-  inline iterator                begin()       { return inst_list_.begin(); }\n-  inline const_iterator          begin() const { return inst_list_.begin(); }\n-  inline iterator                end  ()       { return inst_list_.end();   }\n-  inline const_iterator          end  () const { return inst_list_.end();   }\n-\n-  inline reverse_iterator        rbegin()       { return inst_list_.rbegin(); }\n-  inline const_reverse_iterator  rbegin() const { return inst_list_.rbegin(); }\n-  inline reverse_iterator        rend  ()       { return inst_list_.rend();   }\n-  inline const_reverse_iterator  rend  () const { return inst_list_.rend();   }\n-\n-  inline size_t                   size() const { return inst_list_.size();  }\n-  inline bool                    empty() const { return inst_list_.empty(); }\n-  inline const instruction      &front() const { return *inst_list_.front(); }\n-  inline       instruction      &front()       { return *inst_list_.front(); }\n-  inline const instruction       &back() const { return *inst_list_.back();  }\n-  inline       instruction       &back()       { return *inst_list_.back();  }\n-\n-  void append_instruction(ir::instruction* i);\n-  // split\n-  basic_block* split_before(ir::instruction* loc, const std::string& name);\n-\n-  // predecessors\n-  std::vector<basic_block*> get_predecessors() const;\n-  std::vector<basic_block*> get_successors() const;\n-\n-  // factory functions\n-  static basic_block* create(context &ctx, const std::string &name, function *parent, basic_block *next = nullptr);\n-\n-  void print(std::ostream &os);\n-\n-  // visitor\n-  void accept(visitor *v) { v->visit_basic_block(this); }\n-\n-private:\n-  context &ctx_;\n-  std::string name_;\n-  function *parent_;\n-  std::vector<basic_block*> preds_;\n-  std::vector<basic_block*> succs_;\n-  inst_list_t inst_list_;\n-};\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/ir/builder.h", "status": "removed", "additions": 0, "deletions": 204, "changes": 204, "file_content_changes": "@@ -1,204 +0,0 @@\n-\ufeff#pragma once\n-\n-#ifndef _TRITON_IR_BUILDER_H_\n-#define _TRITON_IR_BUILDER_H_\n-\n-#include <vector>\n-#include <string>\n-#include \"instructions.h\"\n-#include \"basic_block.h\"\n-#include \"type.h\"\n-\n-namespace triton{\n-namespace ir{\n-\n-class basic_block;\n-class value;\n-class type;\n-class constant_int;\n-class instruction;\n-class context;\n-class phi_node;\n-\n-/* Builder */\n-class builder{\n-public:\n-  typedef basic_block::iterator iterator;\n-\n-public:\n-  // Constructor\n-  builder(context &ctx);\n-  // Getters\n-  // const context& get_context() const { return ctx_; }\n-  context& get_context() { return ctx_; }\n-\n-  // Setters\n-  void set_insert_point(iterator instr);\n-  void set_insert_point(instruction* i);\n-  void set_insert_point_after(instruction* i);\n-  void set_insert_point(basic_block* block);\n-  basic_block* get_insert_block() { return block_; }\n-  iterator get_insert_point() { return insert_point_;}\n-  // Constants\n-  value *get_int1(bool val);\n-  value *get_int32(uint32_t val);\n-  value *get_int64(uint64_t val);\n-  value *get_float16(float val);\n-  value *get_float32(float val);\n-  value *get_range(int32_t lo, int32_t hi);\n-  // Types\n-  type *get_void_ty();\n-  type *get_int1_ty();\n-  type *get_int8_ty();\n-  type *get_int16_ty();\n-  type *get_int32_ty();\n-  type *get_int64_ty();\n-  type *get_fp8_ty();\n-  type *get_half_ty();\n-  type *get_bf16_ty();\n-  type *get_float_ty();\n-  type *get_double_ty();\n-  // Insert\n-  template<typename InstTy>\n-  InstTy* insert(InstTy *inst){\n-    assert(block_);\n-    block_->get_inst_list().insert(insert_point_, inst);\n-    inst->set_parent(block_);\n-//    for(ir::value* op: inst->ops())\n-//      op->add_use(inst);\n-    return inst;\n-  }\n-  // terminator instructions\n-  value* create_br(basic_block *dest);\n-  value* create_cond_br(value *cond, basic_block* if_dest, basic_block* else_dest);\n-  value* create_ret_void();\n-  value* create_ret(value *ret);\n-  // Cast instructions\n-  value* create_bitcast(value *src, type *dest_ty);\n-  value *create_cast(cast_op_t op, value *v, type *dst_ty);\n-  value* create_int_to_ptr(value *src, type *dst_ty);\n-  value* create_ptr_to_int(value *src, type *dst_ty);\n-  value* create_si_to_fp(value *src, type *dst_ty);\n-  value* create_ui_to_fp(value *src, type *dst_ty);\n-  value* create_fp_to_si(value *src, type *dst_ty);\n-  value* create_fp_to_ui(value *src, type *dst_ty);\n-  value* create_fp_ext(value *src, type *dst_ty);\n-  value* create_fp_trunc(value *src, type *dst_ty);\n-  value* create_int_cast(value *src, type *dst_ty, bool is_signed);\n-  value *create_downcast(value *arg);\n-  // Call instruction\n-  value* create_call(function* fn, const std::vector<value*>& args);\n-  value* create_launch(function* fn, const std::vector<value*>& args, const std::vector<value*>& grid, value* num_warps);\n-  // Phi instruction\n-  phi_node* create_phi(type *ty, unsigned num_reserved);\n-  // Binary instructions\n-  value *create_insert_nuwnswb_binop(binary_op_t op, value *lhs, value *rhs, bool has_nuw, bool has_nsw);\n-  value *create_fmul(value *lhs, value *rhs);\n-  value *create_fdiv(value *lhs, value *rhs);\n-  value *create_frem(value *lhs, value *rhs);\n-  value *create_fadd(value *lhs, value *rhs);\n-  value *create_fsub(value *lhs, value *rhs);\n-  value *create_sdiv(value *lhs, value *rhs);\n-  value *create_udiv(value *lhs, value *rhs);\n-  value *create_srem(value *lhs, value *rhs);\n-  value *create_urem(value *lhs, value *rhs);\n-  value *create_mul(value *lhs, value *rhs, bool has_nuw = false, bool has_nsw = false);\n-  value *create_add(value *lhs, value *rhs, bool has_nuw = false, bool has_nsw = false);\n-  value *create_sub(value *lhs, value *rhs, bool has_nuw = false, bool has_nsw = false);\n-  value *create_shl(value *lhs, value *rhs, bool has_nuw = false, bool has_nsw = false);\n-  value *create_lshr(value *lhs, value *rhs, bool has_nuw = false, bool has_nsw = false);\n-  value *create_ashr(value *lhs, value *rhs, bool has_nuw = false, bool has_nsw = false);\n-  // GEP\n-  value *create_gep(value *ptr, const std::vector<value*>& idx_list);\n-  // Comparison (int)\n-  value *create_icmp(cmp_pred_t pred, value *lhs, value *rhs);\n-  value *create_icmpSLE(value *lhs, value *rhs);\n-  value *create_icmpSLT(value *lhs, value *rhs);\n-  value *create_icmpSGE(value *lhs, value *rhs);\n-  value *create_icmpSGT(value *lhs, value *rhs);\n-  value *create_icmpULE(value *lhs, value *rhs);\n-  value *create_icmpULT(value *lhs, value *rhs);\n-  value *create_icmpUGE(value *lhs, value *rhs);\n-  value *create_icmpUGT(value *lhs, value *rhs);\n-  value *create_icmpEQ(value *lhs, value *rhs);\n-  value *create_icmpNE(value *lhs, value *rhs);\n-  // Comparison (float)\n-  value *create_fcmp(cmp_pred_t pred, value *lhs, value *rhs);\n-  value *create_fcmpOLT(value *lhs, value *rhs);\n-  value *create_fcmpOGT(value *lhs, value *rhs);\n-  value *create_fcmpOLE(value *lhs, value *rhs);\n-  value *create_fcmpOGE(value *lhs, value *rhs);\n-  value *create_fcmpOEQ(value *lhs, value *rhs);\n-  value *create_fcmpONE(value *lhs, value *rhs);\n-  value *create_fcmpULT(value *lhs, value *rhs);\n-  value *create_fcmpUGT(value *lhs, value *rhs);\n-  value *create_fcmpULE(value *lhs, value *rhs);\n-  value *create_fcmpUGE(value *lhs, value *rhs);\n-  value *create_fcmpUEQ(value *lhs, value *rhs);\n-  value *create_fcmpUNE(value *lhs, value *rhs);\n-  // Logical\n-  value *create_and(value *lhs, value *rhs);\n-  value *create_xor(value *lhs, value *rhs);\n-  value *create_or(value *lhs, value *rhs);\n-  // Input/Output\n-  value *create_load(value *arg, load_inst::CACHE_MODIFIER cache, load_inst::EVICTION_POLICY eviction, bool is_volatile);\n-  value *create_store(value *ptr, value *val);\n-  value *create_masked_load(value *arg, value *mask, value *false_value, load_inst::CACHE_MODIFIER cache, load_inst::EVICTION_POLICY eviction, bool is_volatile);\n-  value *create_masked_store(value *ptr, value *val, value *mask);\n-  // Struct instructions\n-  value *create_insert_value(value* val, value *elt, size_t idx);\n-  value *create_extract_value(value* val, size_t idx);\n-  // Block instruction\n-  value *create_splat(value *arg, const type::block_shapes_t &shapes);\n-  value *create_reshape(value *arg, const type::block_shapes_t &shapes);\n-  value *create_cat(value *lhs, value *rhs);\n-  value *create_broadcast(value *arg, const type::block_shapes_t &shapes);\n-  // Atomic instruction\n-  value *create_atomic_cas(value *ptr, value *cmp, value *val);\n-  value *create_atomic_rmw(atomic_rmw_op_t op, value *ptr, value *val, value *msk);\n-  value *create_atomic_max(value *ptr, value *val, value *msk);\n-  value *create_atomic_umax(value *ptr, value *val, value *msk);\n-  value *create_atomic_min(value *ptr, value *val, value *msk);\n-  value *create_atomic_umin(value *ptr, value *val, value *msk);\n-  value *create_atomic_fadd(value *ptr, value *val, value *msk);\n-  value *create_atomic_add(value *ptr, value *val, value *msk);\n-  value *create_atomic_and(value *ptr, value *val, value *msk);\n-  value *create_atomic_or(value *ptr, value *val, value *msk);\n-  value *create_atomic_xor(value *ptr, value *val, value *msk);\n-  value *create_atomic_xchg(value *ptr, value *val, value *msk);\n-  // Utilities\n-  value *create_clock();\n-  value *create_globaltimer();\n-  // Built-in instruction\n-  value *create_get_program_id(unsigned axis);\n-  value *create_get_num_programs(unsigned axis);\n-  value *create_exp(value* arg);\n-  value *create_cos(value* arg);\n-  value *create_sin(value* arg);\n-  value *create_log(value* arg);\n-  value *create_dot(value *A, value *B, value *C, bool allow_tf32);\n-  value *create_trans(value *A, const std::vector<int> &perm = {});\n-  value *create_sqrt(value *A);\n-  value *create_reduce(value *A, reduce_inst::op_t op, unsigned axis);\n-  value *create_select(value *pred, value *if_value, value *else_value);\n-  // Intrinsics\n-  // These have no place in the IR, and hopefully they can be removed at some point\n-  value *create_umulhi(value* lhs, value* rhs);\n-  value *create_copy_to_shared(value *arg);\n-  value *create_masked_load_async(value *arg, value *mask, value *false_value, load_inst::CACHE_MODIFIER cache, load_inst::EVICTION_POLICY);\n-  value *create_copy_from_shared(value *arg);\n-  value *create_barrier(const std::string &name = \"\");\n-  value *create_async_wait(int N);\n-  value *create_prefetch_s(value *arg, int inc);\n-\n-private:\n-  context &ctx_;\n-  basic_block *block_;\n-  iterator insert_point_;\n-};\n-\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/ir/constant.h", "status": "removed", "additions": 0, "deletions": 113, "changes": 113, "file_content_changes": "@@ -1,113 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_IR_CONSTANT_H_\n-#define _TRITON_IR_CONSTANT_H_\n-\n-#include \"enums.h\"\n-#include \"value.h\"\n-#include <cassert>\n-#include \"visitor.h\"\n-\n-namespace triton{\n-namespace ir{\n-\n-class type;\n-class context;\n-\n-/* Constant */\n-class constant: public user{\n-protected:\n-  using user::user;\n-\n-public:\n-  static constant* get_all_ones_value(type *ty);\n-  static constant* get_null_value(type *ty);\n-  virtual std::string repr() const = 0;\n-};\n-\n-/* Undef value */\n-class undef_value: public constant{\n-private:\n-  undef_value(type *ty);\n-\n-public:\n-  static undef_value* get(type* ty);\n-  std::string repr() const { return \"undef\"; }\n-  void accept(visitor* vst) { vst->visit_undef_value(this); }\n-};\n-\n-\n-/* Constant int */\n-class constant_int: public constant{\n-protected:\n-  constant_int(type *ty, uint64_t value);\n-\n-public:\n-  virtual uint64_t get_value() const { return value_; }\n-  static constant_int *get(type *ty, uint64_t value);\n-  std::string repr() const { return std::to_string(value_); }\n-  void accept(visitor* vst) { vst->visit_constant_int(this); }\n-\n-protected:\n-  uint64_t value_;\n-};\n-\n-/* Constant fp */\n-class constant_fp: public constant{\n-  constant_fp(type *ty, double value);\n-\n-public:\n-  double get_value() { return value_; }\n-  static constant* get_negative_zero(type *ty);\n-  static constant* get_zero_value_for_negation(type *ty);\n-  static constant* get(context &ctx, double v);\n-  static constant* get(type *ty, double v);\n-  std::string repr() const { return std::to_string(value_); }\n-  void accept(visitor* vst) { vst->visit_constant_fp(this); }\n-\n-private:\n-  double value_;\n-};\n-\n-\n-/* Global Value */\n-class global_value: public constant {\n-public:\n-  enum linkage_types_t {\n-    external\n-  };\n-\n-public:\n-  global_value(type *ty, unsigned num_ops,\n-               linkage_types_t linkage, const std::string &name,\n-               unsigned addr_space);\n-  std::string repr() const { return get_name(); }\n-\n-private:\n-  linkage_types_t linkage_;\n-};\n-\n-/* global object */\n-class global_object: public global_value {\n-public:\n-  global_object(type *ty, unsigned num_ops,\n-               linkage_types_t linkage, const std::string &name,\n-               unsigned addr_space = 0);\n-  std::string repr() const { return get_name(); }\n-};\n-\n-/* global variable */\n-class alloc_const: public global_object {\n-public:\n-  alloc_const(type *ty, constant_int *size,\n-              const std::string &name = \"\");\n-  std::string repr() const { return get_name(); }\n-  void accept(visitor* vst) { vst->visit_alloc_const(this); }\n-\n-\n-};\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/ir/context.h", "status": "removed", "additions": 0, "deletions": 29, "changes": 29, "file_content_changes": "@@ -1,29 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_IR_CONTEXT_H_\n-#define _TRITON_IR_CONTEXT_H_\n-\n-#include <memory>\n-#include \"triton/ir/type.h\"\n-\n-namespace triton{\n-namespace ir{\n-\n-class type;\n-class context_impl;\n-\n-/* Context */\n-class context {\n-public:\n-  context();\n-  context(const context&) = delete;\n-  context& operator=(const context&) = delete;\n-\n-public:\n-  std::shared_ptr<context_impl> p_impl;\n-};\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/ir/context_impl.h", "status": "removed", "additions": 0, "deletions": 47, "changes": 47, "file_content_changes": "@@ -1,47 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_IR_CONTEXT_IMPL_H_\n-#define _TRITON_IR_CONTEXT_IMPL_H_\n-\n-#include \"triton/ir/type.h\"\n-#include \"triton/ir/constant.h\"\n-#include <map>\n-#include <memory>\n-\n-namespace triton{\n-namespace ir{\n-\n-class context;\n-\n-/* Context impl */\n-class context_impl {\n-public:\n-  // constructors\n-  context_impl(context &ctx);\n-\n-public:\n-  // non-numeric types\n-  type void_ty, label_ty;\n-  // floating point types\n-  type fp8_ty, fp16_ty, bf16_ty, fp32_ty, fp64_ty;\n-  // integer types\n-  integer_type int1_ty, int8_ty, int16_ty, int32_ty, int64_ty, int128_ty;\n-  // Pointer types\n-  std::map<std::pair<type*, unsigned>, std::unique_ptr<pointer_type>> ptr_tys;\n-  // Block types\n-  std::map<std::pair<type*, type::block_shapes_t>, std::unique_ptr<block_type>> block_tys;\n-  // Struct types\n-  std::map<type::contained_tys_vec_t, struct_type*> struct_tys;\n-  // Int constants\n-  std::map<std::pair<type*, uint64_t>, std::unique_ptr<constant_int>> int_constants_;\n-  // Float constants\n-  std::map<std::pair<type*, double>, std::unique_ptr<constant_fp>> fp_constants_;\n-  // undef values\n-  std::map<type*, std::unique_ptr<undef_value>> uv_constants_;\n-\n-};\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/ir/enums.h", "status": "removed", "additions": 0, "deletions": 183, "changes": 183, "file_content_changes": "@@ -1,183 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_IR_ENUMS_H_\n-#define _TRITON_IR_ENUMS_H_\n-\n-namespace triton{\n-namespace ir{\n-\n-\n-enum binary_op_t: unsigned int{\n-  Add,\n-  FAdd,\n-  Sub,\n-  FSub,\n-  Mul,\n-  FMul,\n-  UDiv,\n-  SDiv,\n-  FDiv,\n-  URem,\n-  SRem,\n-  FRem,\n-  Shl,\n-  LShr,\n-  AShr,\n-  And,\n-  Or,\n-  Xor\n-};\n-\n-enum class atomic_rmw_op_t: unsigned int{\n-  And,\n-  Or,\n-  Xor,\n-  Add,\n-  Max,\n-  Min,\n-  UMax,\n-  UMin,\n-  FAdd,\n-  Xchg,\n-};\n-\n-enum cast_op_t: unsigned int {\n-  Trunc,\n-  ZExt,\n-  SExt,\n-  FPTrunc,\n-  FPExt,\n-  UIToFP,\n-  SIToFP,\n-  FPToUI,\n-  FPToSI,\n-  PtrToInt,\n-  IntToPtr,\n-  BitCast,\n-  AddrSpaceCast\n-};\n-\n-enum cmp_pred_t: unsigned int {\n-  FIRST_FCMP_PREDICATE,\n-  FCMP_FALSE,\n-  FCMP_OEQ,\n-  FCMP_OGT,\n-  FCMP_OGE,\n-  FCMP_OLT,\n-  FCMP_OLE,\n-  FCMP_ONE,\n-  FCMP_ORD,\n-  FCMP_UNO,\n-  FCMP_UEQ,\n-  FCMP_UGT,\n-  FCMP_UGE,\n-  FCMP_ULT,\n-  FCMP_ULE,\n-  FCMP_UNE,\n-  FCMP_TRUE,\n-  LAST_FCMP_PREDICATE,\n-  FIRST_ICMP_PREDICATE,\n-  ICMP_EQ,\n-  ICMP_NE,\n-  ICMP_UGT,\n-  ICMP_UGE,\n-  ICMP_ULT,\n-  ICMP_ULE,\n-  ICMP_SGT,\n-  ICMP_SGE,\n-  ICMP_SLT,\n-  ICMP_SLE,\n-  LAST_ICMP_PREDICATE\n-};\n-\n-enum value_id_t: unsigned {\n-  /* ------------ *\n-    INSTRUCTIONS\n-   * ------------ */\n-  INST_BEGIN,\n-  // call\n-  INST_CALL,\n-  INST_LAUNCH,\n-  // phi\n-  INST_PHI,\n-  // arithmetic\n-  INST_BINOP,\n-  INST_GETELEMENTPTR,\n-  INST_SELECT,\n-  INST_SQRT,\n-  // cmp\n-  INST_ICMP,\n-  INST_FCMP,\n-  // cast\n-  INST_CAST_TRUNC,\n-  INST_CAST_ZEXT,\n-  INST_CAST_SEXT,\n-  INST_CAST_FP_TRUNC,\n-  INST_CAST_FP_EXT,\n-  INST_CAST_UI_TO_FP,\n-  INST_CAST_SI_TO_FP,\n-  INST_CAST_FP_TO_UI,\n-  INST_CAST_FP_TO_SI,\n-  INST_CAST_PTR_TO_INT,\n-  INST_CAST_INT_TO_PTR,\n-  INST_CAST_BIT_CAST,\n-  INST_CAST_ADDR_SPACE_CAST,\n-  // terminators\n-  INST_RETURN,\n-  INST_COND_BRANCH,\n-  INST_UNCOND_BRANCH,\n-  // io\n-  INST_UNMASKED_LOAD,\n-  INST_MASKED_LOAD,\n-  INST_MASKED_LOAD_ASYNC,\n-  INST_UNMASKED_STORE,\n-  INST_MASKED_STORE,\n-  // struct\n-  INST_EXTRACT_VALUE,\n-  INST_INSERT_VALUE,\n-  // retile\n-  INST_RESHAPE,\n-  INST_SPLAT,\n-  INST_CAT,\n-  INST_BROADCAST,\n-  INST_DOWNCAST,\n-  // builtin\n-  INST_GET_PROGRAM_ID,\n-  INST_GET_NUM_PROGRAMS,\n-  // atomics\n-  INST_ATOMIC_CAS,\n-  INST_ATOMIC_EXCH,\n-  INST_ATOMIC_RMW,\n-  // math\n-  INST_UMULHI,\n-  INST_EXP,\n-  INST_COS,\n-  INST_SIN,\n-  INST_LOG,\n-  // array arithmetic\n-  INST_TRANS,\n-  INST_REDUCE,\n-  INST_DOT,\n-  // intrinsics\n-  INST_COPY_TO_SHARED,\n-  INST_COPY_FROM_SHARED,\n-  INST_CVT_LAYOUT,\n-  INST_CVT_SCANLINE,\n-  INST_DECOALESCE,\n-  INST_RECOALESCE,\n-  INST_BARRIER,\n-  INST_ASYNC_WAIT,\n-  INST_MAKE_RANGE_DYN,\n-  INST_MAKE_RANGE_STA,\n-  INST_MAKE_RANGE,\n-  INST_PREFETCH_S,\n-  INST_GLOBALTIMER,\n-  INST_CLOCK,\n-};\n-\n-\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/ir/function.h", "status": "removed", "additions": 0, "deletions": 145, "changes": 145, "file_content_changes": "@@ -1,145 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_IR_FUNCTION_H_\n-#define _TRITON_IR_FUNCTION_H_\n-\n-#include <string>\n-#include <map>\n-#include \"value.h\"\n-#include \"constant.h\"\n-\n-namespace triton{\n-namespace ir{\n-\n-class function;\n-class function_type;\n-class module;\n-class basic_block;\n-\n-/* Argument */\n-class argument: public value{\n-  argument(type *ty, const std::string &name, function *parent, unsigned arg_no);\n-\n-public:\n-  static argument* create(type *ty, const std::string &name,\n-                          function *parent = nullptr, unsigned arg_no = 0);\n-  function* get_parent() const;\n-  unsigned  get_arg_no() const;\n-\n-  void accept(visitor *v);\n-\n-private:\n-  function *parent_;\n-  unsigned arg_no_;\n-};\n-\n-/* Attribute */\n-enum attribute_kind_t {\n-  readonly = 0,\n-  writeonly,\n-  noalias,\n-  aligned,\n-  multiple_of,\n-  retune,\n-  not_implemented\n-};\n-\n-class attribute {\n-public:\n-  attribute(attribute_kind_t kind, unsigned value = 0):\n-    kind_(kind), value_(value){}\n-\n-  bool operator<(const attribute& other) const {\n-    return std::make_pair(kind_, value_) < std::make_pair(other.kind_, other.value_);\n-  }\n-\n-  attribute_kind_t get_kind() const {\n-    return kind_;\n-  }\n-\n-  unsigned get_value() const {\n-    return value_;\n-  }\n-\n-  bool is_llvm_attr() const {\n-    return kind_ != multiple_of;\n-  }\n-\n-  std::string repr() const {\n-    switch(kind_){\n-      case readonly: return \".readonly\";\n-      case writeonly: return \".writeonly\";\n-      case noalias: return \".noalias\";\n-      case aligned: return \".aligned(\" + std::to_string(value_) + \")\";\n-      case multiple_of: return \".multipleof(\" + std::to_string(value_) + \")\";\n-      case retune: return \".retunr\";\n-      default: break;\n-    }\n-    assert(false);\n-    return \"\";\n-  }\n-\n-private:\n-  attribute_kind_t kind_;\n-  unsigned value_;\n-};\n-\n-/* Function */\n-class function: public global_object{\n-  typedef std::vector<argument*> args_t;\n-  typedef args_t::iterator       arg_iterator;\n-  typedef args_t::const_iterator const_arg_iterator;\n-\n-  typedef std::vector<basic_block*> blocks_t;\n-  typedef blocks_t::iterator        block_iterator;\n-  typedef blocks_t::const_iterator  const_block_iterator;\n-\n-  typedef std::map<unsigned, std::set<attribute>> attr_map_t;\n-\n-private:\n-  function(function_type *ty, linkage_types_t linkage,\n-           const std::string &name = \"\", module *parent = nullptr);\n-\n-public:\n-  // accessors\n-  const args_t &args() const { return args_; }\n-  function_type* get_fn_type() { return fn_ty_; }\n-  const function_type* get_fn_type() const { return fn_ty_; }\n-  module *get_parent() { return parent_; }\n-  const module *get_parent() const { return parent_; }\n-\n-  // factory methods\n-  static function *create(function_type *ty, linkage_types_t linkage,\n-                          const std::string &name, module *mod);\n-  // blocks\n-  const blocks_t &blocks() { return blocks_; }\n-  const blocks_t &blocks() const { return blocks_; }\n-  void insert_block(basic_block* block, basic_block *next = nullptr);\n-\n-  // attributes\n-  void add_attr(unsigned arg_id, attribute attr) { attrs_[arg_id].insert(attr); }\n-  const attr_map_t &attrs() { return attrs_; }\n-  bool has_attr(unsigned arg_id) const { return  attrs_.find(arg_id) != attrs_.end(); }\n-  std::set<attribute> get_attributes(const argument* arg) { return attrs_[arg->get_arg_no() + 1]; }\n-  void set_is_kernel(bool new_val) { is_kernel_ = new_val; }\n-  bool get_is_kernel() { return is_kernel_; }\n-\n-  void print(std::ostream &os);\n-\n-  // visitor\n-  void accept(visitor *v) { v->visit_function(this); }\n-\n-private:\n-  module *parent_;\n-  bool init_;\n-  function_type *fn_ty_;\n-  args_t args_;\n-  blocks_t blocks_;\n-  attr_map_t attrs_;\n-  bool is_kernel_;\n-};\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/ir/instructions.h", "status": "removed", "additions": 0, "deletions": 1083, "changes": 1083, "file_content_changes": "@@ -1,1083 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_IR_INSTRUCTIONS_H_\n-#define _TRITON_IR_INSTRUCTIONS_H_\n-\n-#include <vector>\n-#include <map>\n-#include \"triton/ir/enums.h\"\n-#include \"triton/ir/constant.h\"\n-#include \"triton/ir/value.h\"\n-#include \"triton/ir/type.h\"\n-#include \"triton/ir/metadata.h\"\n-#include \"triton/ir/visitor.h\"\n-\n-#define _TRITON_DEFINE_CLONE(name) \\\n-  ir::instruction* clone_impl() const { return new name(*this); }\n-\n-#define _TRITON_DEFINE_ACCEPT(name) \\\n-  void accept(visitor* v) { v->visit_ ## name (this); }\n-\n-namespace triton{\n-namespace ir{\n-\n-class constant_int;\n-class constant;\n-class make_range;\n-class basic_block;\n-class context;\n-class visitor;\n-\n-//===----------------------------------------------------------------------===//\n-//                               instruction classes\n-//===----------------------------------------------------------------------===//\n-\n-class result_reference;\n-\n-\n-class instruction: public user{\n-public:\n-  virtual std::string repr_impl() const = 0;\n-\n-private:\n-  virtual ir::instruction* clone_impl() const = 0;\n-\n-protected:\n-  // constructors\n-  instruction(type *ty, value_id_t ity, unsigned num_ops,\n-              const std::string &name = \"\", instruction *next = nullptr);\n-\n-public:\n-  // parent\n-  void set_parent(basic_block *block)                         { parent_ = block; }\n-  const basic_block *get_parent() const                       { return parent_;  }\n-  basic_block *get_parent()                                   { return parent_;  }\n-  void erase_from_parent();\n-  // helpers\n-  bool has_tile_result_or_op();\n-  // repr\n-  std::string repr() const                                    { return repr_impl(); }\n-  // metadata\n-  void set_metadata(ir::metadata::kind_t kind,\n-                    unsigned value)                           { metadatas_[kind] = value;}\n-  unsigned get_metadata(ir::metadata::kind_t kind)            { return metadatas_[kind];}\n-  // cloning\n-  ir::instruction* clone() {\n-    ir::instruction* res = clone_impl();\n-//    for(auto it = op_begin(); it != op_end(); it++)\n-//      (*it)->add_use(res);\n-    res->parent_ = nullptr;\n-    res->users_.clear();\n-    return res;\n-  }\n-  // instruction id\n-  value_id_t get_id() const { return id_; }\n-\n-  void print(std::ostream &os);\n-\n-private:\n-  basic_block *parent_;\n-  std::map<ir::metadata::kind_t, unsigned> metadatas_;\n-  value_id_t id_;\n-};\n-\n-//===----------------------------------------------------------------------===//\n-//                               call_inst classes\n-//===----------------------------------------------------------------------===//\n-\n-class call_inst: public instruction {\n-private:\n-  std::string repr_impl() const;\n-  call_inst(ir::function* fn, const std::vector<ir::value*>& values, const std::string& name, instruction* next);\n-\n-public:\n-  static call_inst* create(ir::function* fn, const std::vector<ir::value*>& values, const std::string &name = \"\", instruction *next = nullptr);\n-  ir::function* get_fn() { return fn_; }\n-\n-  _TRITON_DEFINE_CLONE(call_inst)\n-  _TRITON_DEFINE_ACCEPT(call_inst)\n-\n-private:\n-  ir::function* fn_;\n-};\n-\n-class launch_inst: public instruction {\n-private:\n-  std::string repr_impl() const { return \"launch\"; }\n-  launch_inst(ir::function* fn, const std::vector<ir::value*>& values, const std::vector<ir::value*>& grid, ir::value* num_warps,\n-              const std::string &name = \"\", instruction *next = nullptr);\n-\n-public:\n-  static launch_inst* create(ir::function* fn, const std::vector<ir::value*>& values, const std::vector<ir::value*>& grid, ir::value* num_warps,\n-                             const std::string& name = \"\", instruction* next = nullptr);\n-\n-  ir::function* get_fn();\n-  std::vector<ir::value*> get_values();\n-  std::vector<ir::value*> get_grid();\n-  ir::value* get_num_warps();\n-\n-\n-  _TRITON_DEFINE_CLONE(launch_inst)\n-  _TRITON_DEFINE_ACCEPT(launch_inst)\n-\n-private:\n-  unsigned val_begin;\n-  unsigned val_end;\n-  unsigned grid_begin;\n-  unsigned grid_end;\n-};\n-\n-//===----------------------------------------------------------------------===//\n-//                               phi_node classes\n-//===----------------------------------------------------------------------===//\n-\n-class phi_node: public instruction {\n-private:\n-  phi_node(type *ty, unsigned num_reserved, const std::string &name, instruction *next);\n-  std::string repr_impl() const { return \"phi\"; }\n-\n-public:\n-  void set_incoming_value(unsigned i, value *v);\n-  void set_incoming_block(unsigned i, basic_block *block);\n-  value *get_value_for_block(basic_block *block);\n-  value *get_incoming_value(unsigned i) { return get_operand(i); }\n-  basic_block *get_incoming_block(unsigned i) { return blocks_[i]; }\n-  unsigned get_num_incoming() { return get_num_operands(); }\n-  void add_incoming(value *v, basic_block *block);\n-\n-  // Type\n-  void set_type(type *ty) { ty_ = ty; }\n-\n-  // Factory methods\n-  static phi_node* create(type *ty, unsigned num_reserved, const std::string &name = \"\", instruction *next = nullptr);\n-\n-  _TRITON_DEFINE_CLONE(phi_node)\n-  _TRITON_DEFINE_ACCEPT(phi_node)\n-\n-private:\n-  unsigned num_reserved_;\n-  std::vector<basic_block*> blocks_;\n-};\n-\n-//===----------------------------------------------------------------------===//\n-//                               binary_operator classes\n-//===----------------------------------------------------------------------===//\n-\n-class binary_operator: public instruction {\n-public:\n-  typedef binary_op_t op_t;\n-\n-private:\n-  std::string repr_impl() const;\n-\n-protected:\n-  // Constructors\n-  binary_operator(binary_op_t op, value *lhs, value *rhs, type *ty, const std::string &name, instruction *next);\n-\n-public:\n-  // Get operand\n-  binary_op_t get_op() const { return op_; }\n-\n-  // Bool\n-  bool is_terminator()  const;\n-  bool is_binary_op()   const;\n-  bool is_int_div_rem() const;\n-  bool is_shift()       const;\n-  bool is_cast()        const;\n-  bool is_int_mult()    const;\n-  bool is_int_add_sub() const;\n-  bool is_int_div()     const;\n-  bool is_int_rem()     const;\n-  bool is_shl()         const;\n-  bool is_shr()         const;\n-\n-  // Approx\n-  void set_fdiv_ieee_rounding(bool rnd) { fdiv_ieee_rnd_ = rnd; }\n-  bool get_fdiv_ieee_rounding() { return fdiv_ieee_rnd_; }\n-\n-  // Wraps\n-  void set_has_no_unsigned_wrap(bool b = true) { has_no_unsigned_wrap_ = b; }\n-  void set_has_no_signed_wrap(bool b = true)   { has_no_signed_wrap_ = b; }\n-\n-  // Factory methods\n-  static binary_operator *create(binary_op_t op, value *lhs, value *rhs,\n-                                 const std::string &name = \"\", instruction *next = nullptr);\n-//  static binary_operator *create_fneg(value *arg, const std::string &name = \"\", instruction *next = nullptr);\n-//  static binary_operator *create_neg(value *arg, const std::string &name = \"\", instruction *next = nullptr);\n-//  static binary_operator *create_not(value *arg, const std::string &name = \"\", instruction *next = nullptr);\n-\n-  _TRITON_DEFINE_CLONE(binary_operator)\n-  _TRITON_DEFINE_ACCEPT(binary_operator)\n-\n-public:\n-  binary_op_t op_;\n-  bool has_no_unsigned_wrap_;\n-  bool has_no_signed_wrap_;\n-\n-  bool fdiv_ieee_rnd_;\n-};\n-\n-\n-//===----------------------------------------------------------------------===//\n-//                               cmp_inst classes\n-//===----------------------------------------------------------------------===//\n-\n-class cmp_inst: public instruction{\n-public:\n-  typedef cmp_pred_t pred_t;\n-\n-private:\n-  std::string repr_impl() const;\n-\n-protected:\n-  cmp_inst(type *ty, value_id_t id, cmp_pred_t pred,\n-           value *lhs, value *rhs, const std::string &name, instruction *next);\n-  static bool is_fp_predicate(cmp_pred_t pred);\n-  static bool is_int_predicate(cmp_pred_t pred);\n-  static type* make_cmp_result_type(type *ty);\n-\n-public:\n-  cmp_pred_t get_pred() const { return pred_; }\n-\n-private:\n-  cmp_pred_t pred_;\n-};\n-\n-class icmp_inst: public cmp_inst {\n-  icmp_inst(type *ty, cmp_pred_t pred,\n-            value *lhs, value *rhs, const std::string &name, instruction *next);\n-\n-public:\n-  static icmp_inst* create(cmp_pred_t pred, value *lhs, value *rhs,\n-                    const std::string &name = \"\", instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(icmp_inst)\n-  _TRITON_DEFINE_ACCEPT(icmp_inst)\n-};\n-\n-class fcmp_inst: public cmp_inst {\n-  fcmp_inst(type *ty, cmp_pred_t pred,\n-            value *lhs, value *rhs, const std::string &name, instruction *next);\n-\n-public:\n-  static fcmp_inst* create(cmp_pred_t pred, value *lhs, value *rhs,\n-                    const std::string &name = \"\", instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(fcmp_inst)\n-  _TRITON_DEFINE_ACCEPT(fcmp_inst)\n-};\n-\n-//===----------------------------------------------------------------------===//\n-//                               unary_inst classes\n-//===----------------------------------------------------------------------===//\n-\n-class unary_inst: public instruction {\n-protected:\n-  unary_inst(type *ty, value_id_t id, value *v, const std::string &name, instruction *next);\n-};\n-\n-\n-//===----------------------------------------------------------------------===//\n-//                               cast_inst classes\n-//===----------------------------------------------------------------------===//\n-\n-class cast_inst: public unary_inst{\n-private:\n-  std::string repr_impl() const;\n-\n-protected:\n-  cast_inst(type *ty, value_id_t id, value *v, const std::string &name, instruction *next, cast_op_t op)\n-    : unary_inst(ty, id, v, name, next), op_(op) { }\n-\n-private:\n-  static bool is_valid(cast_op_t op, value *arg, type *ty);\n-\n-public:\n-  // accessors\n-  cast_op_t get_op() const { return op_; }\n-\n-  // factory methods\n-  static cast_inst *create(cast_op_t op, value *arg, type *ty,\n-                           const std::string &name = \"\", instruction *next = nullptr);\n-  static cast_inst *create_integer_cast(value *arg, type *ty, bool is_signed,\n-                           const std::string &name = \"\", instruction *next = nullptr);\n-\n-  _TRITON_DEFINE_ACCEPT(cast_inst)\n-\n-private:\n-  cast_op_t op_;\n-};\n-\n-#define TRITON_IR_DECLARE_CAST_INST_SIMPL(name, id, op) \\\n-class name : public cast_inst { \\\n-  _TRITON_DEFINE_CLONE(name) \\\n-  friend class cast_inst; \\\n-  name(type *ty, value *v, const std::string &name, instruction *next) \\\n-    : cast_inst(ty, id, v, name, next, op){ } \\\n-};\n-\n-TRITON_IR_DECLARE_CAST_INST_SIMPL(trunc_inst, INST_CAST_TRUNC, cast_op_t::Trunc)\n-TRITON_IR_DECLARE_CAST_INST_SIMPL(z_ext_inst, INST_CAST_ZEXT, cast_op_t::ZExt)\n-TRITON_IR_DECLARE_CAST_INST_SIMPL(s_ext_inst, INST_CAST_SEXT, cast_op_t::SExt)\n-TRITON_IR_DECLARE_CAST_INST_SIMPL(fp_trunc_inst, INST_CAST_FP_TRUNC, cast_op_t::FPTrunc)\n-TRITON_IR_DECLARE_CAST_INST_SIMPL(fp_ext_inst, INST_CAST_FP_EXT, cast_op_t::FPExt)\n-TRITON_IR_DECLARE_CAST_INST_SIMPL(ui_to_fp_inst, INST_CAST_UI_TO_FP, cast_op_t::UIToFP)\n-TRITON_IR_DECLARE_CAST_INST_SIMPL(si_to_fp_inst, INST_CAST_SI_TO_FP, cast_op_t::SIToFP)\n-TRITON_IR_DECLARE_CAST_INST_SIMPL(fp_to_ui_inst, INST_CAST_FP_TO_UI, cast_op_t::FPToUI)\n-TRITON_IR_DECLARE_CAST_INST_SIMPL(fp_to_si_inst, INST_CAST_FP_TO_SI, cast_op_t::FPToSI)\n-TRITON_IR_DECLARE_CAST_INST_SIMPL(ptr_to_int_inst, INST_CAST_PTR_TO_INT, cast_op_t::PtrToInt)\n-TRITON_IR_DECLARE_CAST_INST_SIMPL(int_to_ptr_inst, INST_CAST_INT_TO_PTR, cast_op_t::IntToPtr)\n-TRITON_IR_DECLARE_CAST_INST_SIMPL(bit_cast_inst, INST_CAST_BIT_CAST, cast_op_t::BitCast)\n-TRITON_IR_DECLARE_CAST_INST_SIMPL(addr_space_cast_inst, INST_CAST_ADDR_SPACE_CAST, cast_op_t::AddrSpaceCast)\n-\n-//===----------------------------------------------------------------------===//\n-//                               terminator_inst classes\n-//===----------------------------------------------------------------------===//\n-\n-class terminator_inst: public instruction{\n-  using instruction::instruction;\n-};\n-\n-// return instruction\n-class return_inst: public terminator_inst {\n-private:\n-  std::string repr_impl() const { return \"ret\"; }\n-  return_inst(context &ctx, value *ret_val, instruction *next);\n-\n-public:\n-  // accessors\n-  value *get_return_value()\n-  { return get_num_operands() ? get_operand(0) : nullptr; }\n-\n-  unsigned get_num_successors() const { return 0; }\n-\n-  // factory methods\n-  static return_inst* create(context &ctx, value *ret_val = nullptr, instruction *next = nullptr);\n-\n-  _TRITON_DEFINE_CLONE(return_inst)\n-  _TRITON_DEFINE_ACCEPT(return_inst)\n-};\n-\n-// base branch instruction\n-class branch_inst: public terminator_inst{\n-private:\n-  std::string repr_impl() const { return \"br\"; }\n-\n-protected:\n-  using terminator_inst::terminator_inst;\n-\n-public:\n-  static branch_inst* create(basic_block *dest,\n-                             instruction *next = nullptr);\n-  static branch_inst* create(value *cond, basic_block *if_dest, basic_block *else_dest,\n-                             instruction *next = nullptr);\n-};\n-\n-// conditional branch\n-class cond_branch_inst: public branch_inst {\n-private:\n-  friend class branch_inst;\n-  cond_branch_inst(basic_block *if_dst, basic_block *else_dst, value *cond, instruction *next);\n-\n-public:\n-  basic_block *get_true_dest()  { return (basic_block*)get_operand(0); }\n-  basic_block *get_false_dest() { return (basic_block*)get_operand(1); }\n-  value *get_cond()             { return get_operand(2); }\n-  _TRITON_DEFINE_CLONE(cond_branch_inst)\n-  _TRITON_DEFINE_ACCEPT(cond_branch_inst)\n-};\n-\n-// unconditional branch\n-class uncond_branch_inst: public branch_inst {\n-private:\n-  friend class branch_inst;\n-  uncond_branch_inst(basic_block *dst, instruction *next);\n-\n-public:\n-  basic_block *get_dest()  { return (basic_block*)get_operand(0); }\n-  _TRITON_DEFINE_CLONE(uncond_branch_inst)\n-  _TRITON_DEFINE_ACCEPT(uncond_branch_inst)\n-};\n-\n-\n-//===----------------------------------------------------------------------===//\n-//                               getelementptr_inst classes\n-//===----------------------------------------------------------------------===//\n-\n-class getelementptr_inst: public instruction {\n-private:\n-  std::string repr_impl() const { return \"getelementptr\"; }\n-  getelementptr_inst(type *pointee_ty, value *ptr, const std::vector<value*> &idx, const std::string &name, instruction *next);\n-\n-private:\n-  static type *get_return_type(type *ty, value *ptr, const std::vector<value*> &idx);\n-  static type *get_indexed_type_impl(type *ty, const std::vector<value *> &idx);\n-  static type *get_indexed_type(type *ty, const std::vector<value*> &idx);\n-\n-public:\n-  // accessors\n-  type *get_source_elt_ty() { return source_elt_ty; }\n-  op_iterator idx_begin()       { return op_begin() + 1; }\n-  op_iterator idx_end()         { return op_end(); }\n-  value *get_pointer_operand()  { return *op_begin(); }\n-\n-  // factory methods\n-  static getelementptr_inst* create(value *ptr, const std::vector<value*> &idx,\n-                                    const std::string &name = \"\", instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(getelementptr_inst)\n-  _TRITON_DEFINE_ACCEPT(getelementptr_inst)\n-\n-private:\n-  type *source_elt_ty;\n-  type *res_elt_ty;\n-};\n-\n-//===----------------------------------------------------------------------===//\n-//                          load_inst/store_inst classes\n-//===----------------------------------------------------------------------===//\n-\n-class io_inst: public instruction {\n-protected:\n-  io_inst(type *ty, value_id_t id, unsigned num_ops,\n-          const std::string &name = \"\", instruction *next = nullptr);\n-\n-public:\n-  // accessors\n-  value *get_pointer_operand() { return get_operand(0); }\n-};\n-\n-// load\n-class load_inst: public io_inst {\n-public:\n-  enum CACHE_MODIFIER : uint32_t {\n-    NONE=0,\n-    CA,\n-    CG,\n-  };\n-\n-  enum EVICTION_POLICY : uint32_t {\n-    NORMAL=0,\n-    EVICT_FIRST,\n-    EVICT_LAST,\n-  };\n-\n-  CACHE_MODIFIER get_cache_modifier() const { return cache_; }\n-  EVICTION_POLICY get_eviction_policy() const { return eviction_; }\n-  bool get_is_volatile() const { return is_volatile_; }\n-\n-protected:\n-  load_inst(value *ptr, value_id_t id, unsigned num_ops, CACHE_MODIFIER cache, EVICTION_POLICY eviction,\n-          bool is_volatile,\n-          const std::string &name = \"\", instruction *next = nullptr);\n-  std::string get_cache_modifier_repr() const {\n-    if (cache_ == CA) return \".ca\";\n-    if (cache_ == CG) return \".cg\";\n-    return \"\"; \n-  }\n-  std::string get_eviction_policy_repr() const {\n-    if (eviction_ == EVICT_FIRST) return \".L1::evict_first\";\n-    if (eviction_ == EVICT_LAST) return \".L2::evict_last\";\n-    return \"\";\n-  }\n-  EVICTION_POLICY eviction_;\n-  CACHE_MODIFIER cache_;\n-\n-  std::string get_volatile_repr() {\n-    return is_volatile_ ? \".volatile\" : \"\";\n-  }\n-  bool is_volatile_;\n-\n-private:\n-  static type *get_pointee_type(type *ty);\n-};\n-\n-// unmasked load\n-class unmasked_load_inst: public load_inst {\n-private:\n-  std::string repr_impl() const { return \"unmasked_load\" + get_cache_modifier_repr(); }\n-  unmasked_load_inst(value *ptr, load_inst::CACHE_MODIFIER cache, load_inst::EVICTION_POLICY eviction, bool is_volatile, const std::string &name, instruction *next);\n-\n-public:\n-  static unmasked_load_inst* create(value *ptr,\n-                                    CACHE_MODIFIER cache, EVICTION_POLICY eviction,\n-                                    bool is_volatile,\n-                                    const std::string &name = \"\",\n-                                    instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(unmasked_load_inst)\n-  _TRITON_DEFINE_ACCEPT(unmasked_load_inst)\n-};\n-\n-// masked load\n-class masked_load_inst: public load_inst {\n-private:\n-  std::string repr_impl() const { return \"masked_load\" + get_cache_modifier_repr(); }\n-  masked_load_inst(value *ptr, value *mask, value *false_value, load_inst::CACHE_MODIFIER cache, load_inst::EVICTION_POLICY eviction, bool is_volatile,\n-                   const std::string &name, instruction *next);\n-\n-public:\n-  // accessors\n-  value *get_mask_operand() { return get_operand(1); }\n-  value *get_false_value_operand() { return get_operand(2); }\n-  // factory method\n-  static masked_load_inst* create(value *ptr, value *mask, value *false_value,\n-                                  CACHE_MODIFIER cache, EVICTION_POLICY eviction,\n-                                  bool is_volatile,\n-                                  const std::string &name = \"\",\n-                                  instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(masked_load_inst)\n-  _TRITON_DEFINE_ACCEPT(masked_load_inst)\n-};\n-\n-// masked load async\n-class masked_load_async_inst: public load_inst {\n-private:\n-  std::string repr_impl() const { return \"masked_load_async\" + get_cache_modifier_repr(); }\n-  masked_load_async_inst(value *ptr, value *mask, value *false_value,\n-                         CACHE_MODIFIER cache, EVICTION_POLICY eviction,\n-                         const std::string &name, instruction *next);\n-\n-public:\n-  // accessors\n-  value *get_mask_operand() { return get_operand(1); }\n-  value *get_false_value_operand() { return get_operand(2); }\n-  // factory method\n-  static masked_load_async_inst* create(value *ptr, value *mask, value *false_value,\n-                                  load_inst::CACHE_MODIFIER cache,\n-                                  EVICTION_POLICY eviction,\n-                                  const std::string &name = \"\",\n-                                  instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(masked_load_async_inst)\n-  _TRITON_DEFINE_ACCEPT(masked_load_async_inst)\n-};\n-\n-\n-\n-// store\n-class store_inst: public io_inst {\n-protected:\n-  store_inst(value *ptr, value_id_t id, unsigned num_ops,\n-            const std::string &name = \"\", instruction *next = nullptr);\n-\n-public:\n-  value *get_value_operand() { return get_operand(1); }\n-};\n-\n-// unmasked_store\n-class unmasked_store_inst: public store_inst{\n-private:\n-  std::string repr_impl() const { return \"unmasked_store\"; }\n-  unmasked_store_inst(value *ptr, value *v, const std::string &name, instruction *next);\n-\n-public:\n-  // factory method\n-  static unmasked_store_inst* create(value* ptr, value *v,\n-                                    const std::string &name = \"\",\n-                                    instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(unmasked_store_inst)\n-  _TRITON_DEFINE_ACCEPT(unmasked_store_inst)\n-};\n-\n-class masked_store_inst: public store_inst{\n-private:\n-  std::string repr_impl() const { return \"masked_store\"; }\n-  masked_store_inst(value *ptr, value *v, value *mask,\n-                    const std::string &name, instruction *next);\n-\n-public:\n-  // accessors\n-  value *get_mask_operand() { return get_operand(2); }\n-  // factory method\n-  static masked_store_inst* create(value *ptr, value *v, value *mask,\n-                                   const std::string &name = \"\",\n-                                   instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(masked_store_inst)\n-  _TRITON_DEFINE_ACCEPT(masked_store_inst)\n-};\n-\n-//===----------------------------------------------------------------------===//\n-//                               struct classes\n-//===----------------------------------------------------------------------===//\n-\n-// insert_value\n-\n-class insert_value_inst: public instruction {\n-private:\n-  std::string repr_impl() const { return \"insertvalue\"; }\n-  insert_value_inst(value *val, value *elt, size_t idx, const std::string &name, instruction *next);\n-\n-public:\n-  static insert_value_inst* create(value *val, value* elt, size_t idx, const std::string &name = \"\", instruction *next = nullptr);\n-  size_t get_idx() { return idx_; }\n-  _TRITON_DEFINE_CLONE(insert_value_inst)\n-  _TRITON_DEFINE_ACCEPT(insert_value_inst)\n-\n-private:\n-  size_t idx_;\n-};\n-\n-// extract_value\n-\n-class extract_value_inst: public instruction {\n-private:\n-  std::string repr_impl() const { return \"extractvalue\"; }\n-  extract_value_inst(value *val, size_t idx, const std::string &name, instruction *next);\n-\n-public:\n-  static extract_value_inst* create(value *val, size_t idx, const std::string &name = \"\", instruction *next = nullptr);\n-  size_t get_idx() { return idx_; }\n-  _TRITON_DEFINE_CLONE(extract_value_inst)\n-  _TRITON_DEFINE_ACCEPT(extract_value_inst)\n-\n-private:\n-  size_t idx_;\n-};\n-\n-//===----------------------------------------------------------------------===//\n-//                               retile_inst classes\n-//===----------------------------------------------------------------------===//\n-\n-// cat\n-\n-class cat_inst: public instruction {\n-private:\n-  std::string repr_impl() const { return \"cat\"; }\n-  cat_inst(value *x, value *y, const std::string &name, instruction *next);\n-\n-public:\n-  static instruction* create(value *lhs, value *rhs,\n-                             const std::string &name = \"\",\n-                             instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(cat_inst)\n-  _TRITON_DEFINE_ACCEPT(cat_inst)\n-};\n-\n-// retile\n-\n-class retile_inst: public unary_inst {\n-protected:\n-  retile_inst(value *arg, value_id_t id, const type::block_shapes_t &shapes, const std::string &name, instruction *next);\n-};\n-\n-// reshape\n-\n-class reshape_inst: public retile_inst {\n-private:\n-  using retile_inst::retile_inst;\n-  std::string repr_impl() const { return \"reshape\"; }\n-\n-public:\n-  static instruction* create(value *arg, const type::block_shapes_t &shape_suffix,\n-                      const std::string &name = \"\", instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(reshape_inst)\n-  _TRITON_DEFINE_ACCEPT(reshape_inst)\n-};\n-\n-// splat\n-\n-class splat_inst: public retile_inst {\n-private:\n-  using retile_inst::retile_inst;\n-  std::string repr_impl() const { return \"splat\"; }\n-\n-public:\n-  static instruction* create(value *arg, const type::block_shapes_t &shape_suffix,\n-                      const std::string &name = \"\", instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(splat_inst)\n-  _TRITON_DEFINE_ACCEPT(splat_inst)\n-};\n-\n-// broadcast\n-\n-class broadcast_inst: public retile_inst {\n-private:\n-  using retile_inst::retile_inst;\n-  std::string repr_impl() const { return \"broadcast\"; }\n-\n-public:\n-  static instruction* create(value *arg, const type::block_shapes_t &shape_suffix,\n-                      const std::string &name = \"\", instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(broadcast_inst)\n-  _TRITON_DEFINE_ACCEPT(broadcast_inst)\n-};\n-\n-\n-// downcast\n-\n-class downcast_inst: public unary_inst {\n-private:\n-  using unary_inst::unary_inst;\n-  std::string repr_impl() const { return \"downcast\"; }\n-\n-public:\n-  static instruction* create(value *arg, const std::string &name = \"\", instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(downcast_inst)\n-  _TRITON_DEFINE_ACCEPT(downcast_inst)\n-};\n-\n-//===----------------------------------------------------------------------===//\n-//                               builtin_inst classes\n-//===----------------------------------------------------------------------===//\n-\n-class builtin_inst: public instruction{\n-protected:\n-  using instruction::instruction;\n-};\n-\n-class get_program_id_inst: public builtin_inst {\n-private:\n-  get_program_id_inst(type *ty, unsigned axis, const std::string &name, instruction *next);\n-  std::string repr_impl() const { return \"get_program_id(\" + std::to_string(axis_) + \")\"; }\n-\n-public:\n-  static instruction* create(context &ctx, unsigned axis, const std::string &name = \"\", instruction *next = nullptr);\n-  unsigned get_axis() const { return axis_; }\n-  _TRITON_DEFINE_CLONE(get_program_id_inst)\n-  _TRITON_DEFINE_ACCEPT(get_program_id_inst)\n-\n-private:\n-  unsigned axis_;\n-};\n-\n-class get_num_programs_inst: public builtin_inst {\n-private:\n-  get_num_programs_inst(type *ty, unsigned axis, const std::string &name, instruction *next);\n-  std::string repr_impl() const { return \"get_num_programs(\" + std::to_string(axis_) + \")\"; }\n-\n-public:\n-  static instruction* create(context &ctx, unsigned axis, const std::string &name = \"\", instruction *next = nullptr);\n-  unsigned get_axis() const { return axis_; }\n-  _TRITON_DEFINE_CLONE(get_num_programs_inst)\n-  _TRITON_DEFINE_ACCEPT(get_num_programs_inst)\n-\n-private:\n-  unsigned axis_;\n-};\n-\n-\n-class atomic_inst: public io_inst {\n-public:\n-  using io_inst::io_inst;\n-};\n-\n-class atomic_rmw_inst: public atomic_inst {\n-private:\n-  atomic_rmw_inst(atomic_rmw_op_t op, value *ptr, value *val, value *msk, const std::string &name = \"\", instruction *next = nullptr);\n-  std::string repr_impl() const { return \"atomic_rmw\"; }\n-  _TRITON_DEFINE_CLONE(atomic_rmw_inst)\n-  _TRITON_DEFINE_ACCEPT(atomic_rmw_inst)\n-\n-public:\n-  static instruction* create(atomic_rmw_op_t op, value *ptr, value *val, value *msk, const std::string &name = \"\", instruction *next = nullptr);\n-  atomic_rmw_op_t get_op() { return op_; }\n-\n-private:\n-  atomic_rmw_op_t op_;\n-};\n-\n-class atomic_cas_inst: public atomic_inst {\n-private:\n-  atomic_cas_inst(value *ptr, value *cmp, value *val, const std::string &name, instruction *next);\n-  std::string repr_impl() const { return \"atomic_cas\"; }\n-  _TRITON_DEFINE_CLONE(atomic_cas_inst)\n-  _TRITON_DEFINE_ACCEPT(atomic_cas_inst)\n-\n-public:\n-  static instruction* create(value *ptr, value *cmp, value *val, const std::string &name = \"\", instruction *next = nullptr);\n-};\n-\n-class umulhi_inst: public builtin_inst {\n-private:\n-  umulhi_inst(value *lhs, value *rhs, const std::string &name = \"\", instruction *next = nullptr);\n-  std::string repr_impl() const { return \"umulhi\"; }\n-  _TRITON_DEFINE_CLONE(umulhi_inst)\n-  _TRITON_DEFINE_ACCEPT(umulhi_inst)\n-\n-public:\n-  static instruction* create(value *lhs, value *rhs, const std::string &name = \"\", instruction *next = nullptr);\n-};\n-\n-class exp_inst: public builtin_inst {\n-private:\n-  exp_inst(value *val, const std::string &name = \"\", instruction *next = nullptr);\n-  std::string repr_impl() const { return \"exp\"; }\n-  _TRITON_DEFINE_CLONE(exp_inst)\n-  _TRITON_DEFINE_ACCEPT(exp_inst)\n-\n-public:\n-  static instruction* create(value *val, const std::string &name = \"\", instruction *next = nullptr);\n-};\n-\n-class cos_inst: public builtin_inst {\n-private:\n-  cos_inst(value *val, const std::string &name = \"\", instruction *next = nullptr);\n-  std::string repr_impl() const { return \"cos\"; }\n-  _TRITON_DEFINE_CLONE(cos_inst)\n-  _TRITON_DEFINE_ACCEPT(cos_inst)\n-\n-public:\n-  static instruction* create(value *val, const std::string &name = \"\", instruction *next = nullptr);\n-};\n-\n-class sin_inst: public builtin_inst {\n-private:\n-  sin_inst(value *val, const std::string &name = \"\", instruction *next = nullptr);\n-  std::string repr_impl() const { return \"sin\"; }\n-  _TRITON_DEFINE_CLONE(sin_inst)\n-  _TRITON_DEFINE_ACCEPT(sin_inst)\n-\n-public:\n-  static instruction* create(value *val, const std::string &name = \"\", instruction *next = nullptr);\n-};\n-\n-class log_inst: public builtin_inst {\n-private:\n-  log_inst(value *val, const std::string &name = \"\", instruction *next = nullptr);\n-  std::string repr_impl() const { return \"log\"; }\n-  _TRITON_DEFINE_CLONE(log_inst)\n-  _TRITON_DEFINE_ACCEPT(log_inst)\n-\n-public:\n-  static instruction* create(value *val, const std::string &name = \"\", instruction *next = nullptr);\n-};\n-\n-\n-class dot_inst: public builtin_inst {\n-public:\n-  enum TransT { NoTrans, Trans };\n-  enum DataType { \n-    FP8, FP16, BF16, TF32, FP32, \n-    INT1, INT4, INT8, INT32, \n-    UNKNOWN,\n-  };\n-\n-private:\n-  dot_inst(value *A, value *B, value *C, TransT AT, TransT BT, bool allow_tf32, const std::string &name, instruction *next);\n-  std::string repr_impl() const { return \"dot\"; }\n-  \n-public:\n-  bool is_prefetched() const { return is_prefetched_; }\n-  void set_prefetched(bool is_prefetched) { is_prefetched_ = is_prefetched; }\n-  bool allow_tf32() const { return allow_tf32_; }\n-\n-public:\n-  static instruction *create(value *A, value *B, value *C, bool AT, bool BT, bool allow_tf32, const std::string &name = \"\", instruction *next = nullptr);\n-  static instruction* create_nn(value *A, value *B, value *C, bool allow_tf32, const std::string &name = \"\", instruction *next = nullptr);\n-  static instruction* create_nt(value *A, value *B, value *C, bool allow_tf32, const std::string &name = \"\", instruction *next = nullptr);\n-  static instruction* create_tn(value *A, value *B, value *C, bool allow_tf32, const std::string &name = \"\", instruction *next = nullptr);\n-  static instruction* create_tt(value *A, value *B, value *C, bool allow_tf32, const std::string &name = \"\", instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(dot_inst)\n-  _TRITON_DEFINE_ACCEPT(dot_inst)\n-\n-private:\n-  bool is_prefetched_ = false;\n-  bool allow_tf32_ = false;\n-  DataType C_type_ = DataType::FP32;\n-  DataType A_type_ = DataType::FP16;\n-  DataType B_type_ = DataType::FP16;\n-};\n-\n-//class outer_inst: public builtin_inst {\n-//private:\n-//  outer_inst(value *A, value *B, value *C, const std::string &name, instruction *next);\n-//public:\n-//  static instruction* create(value *A, value *B, value *C, const std::string &name = \"\", instruction *next = nullptr);\n-//};\n-\n-class trans_inst: public builtin_inst {\n-public:\n-  ir::type* get_res_ty(ir::type* in, std::vector<int> perm);\n-  std::vector<int> init_perm(ir::type* ty, const std::vector<int>& perm);\n-\n-private:\n-  trans_inst(value *arg, const std::vector<int>& perm, const std::string& name, instruction* next);\n-  std::string repr_impl() const { return \"trans\"; }\n-\n-public:\n-  static instruction* create(value *arg, const std::vector<int> &perm = {}, const std::string &name = \"\", instruction *next = nullptr);\n-  const std::vector<int> get_perm() const;\n-  _TRITON_DEFINE_CLONE(trans_inst)\n-  _TRITON_DEFINE_ACCEPT(trans_inst)\n-\n-private:\n-  std::vector<int> perm_;\n-};\n-\n-class sqrt_inst: public builtin_inst {\n-private:\n-  sqrt_inst(value *arg, const std::string& name, instruction* next);\n-  std::string repr_impl() const { return \"sqrt\"; }\n-public:\n-  static instruction* create(value *arg, const std::string &name = \"\", instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(sqrt_inst)\n-  _TRITON_DEFINE_ACCEPT(sqrt_inst)\n-};\n-\n-class reduce_inst: public builtin_inst {\n-public:\n-  enum op_t{\n-    ADD, SUB, MAX, MIN,\n-    FADD, FSUB, FMAX, FMIN,\n-    XOR\n-  };\n-\n-private:\n-  static type* get_res_type(value *arg, unsigned axis);\n-  static std::string to_str(op_t op);\n-\n-private:\n-  reduce_inst(value* arg, op_t op, unsigned axis, const std::string& name, instruction* next);\n-  std::string repr_impl() const { return \"reduce\"; }\n-  _TRITON_DEFINE_CLONE(reduce_inst)\n-  _TRITON_DEFINE_ACCEPT(reduce_inst)\n-\n-public:\n-  static instruction* create(value *arg, op_t op, unsigned axis, const std::string &name = \"\", instruction *next = nullptr);\n-  unsigned get_axis() const { return axis_; }\n-  op_t get_op() const { return op_; }\n-\n-private:\n-  unsigned axis_;\n-  op_t op_;\n-};\n-\n-class select_inst: public builtin_inst {\n-private:\n-  select_inst(value *pred, value *if_value, value *else_value, const std::string& name, instruction* next);\n-  std::string repr_impl() const { return \"select\"; }\n-  _TRITON_DEFINE_CLONE(select_inst)\n-  _TRITON_DEFINE_ACCEPT(select_inst)\n-\n-public:\n-  static instruction* create(value *pred, value *if_value, value *else_value, const std::string &name = \"\", instruction *next = nullptr);\n-  value* get_pred_op() { return get_operand(0); }\n-  value* get_if_value_op() { return get_operand(1); }\n-  value* get_else_value_op() { return get_operand(2); }\n-};\n-\n-//===----------------------------------------------------------------------===//\n-//                               intrinsics classes\n-//===----------------------------------------------------------------------===//\n-\n-\n-class copy_to_shared_inst: public unary_inst{\n-private:\n-  using unary_inst::unary_inst;\n-  std::string repr_impl() const { return \"copy_to_shared\"; }\n-\n-public:\n-  static copy_to_shared_inst* create(value *arg, const std::string &name = \"\",\n-                                     instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(copy_to_shared_inst)\n-  _TRITON_DEFINE_ACCEPT(copy_to_shared_inst)\n-};\n-\n-class copy_from_shared_inst: public unary_inst{\n-private:\n-  using unary_inst::unary_inst;\n-  std::string repr_impl() const { return \"copy_from_shared\"; }\n-\n-public:\n-  static copy_from_shared_inst* create(value *arg, const std::string &name = \"\",\n-                                     instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(copy_from_shared_inst)\n-  _TRITON_DEFINE_ACCEPT(copy_from_shared_inst)\n-};\n-\n-class cvt_layout_inst: public unary_inst {\n-private:\n-  using unary_inst::unary_inst;\n-  std::string repr_impl() const { return \"cvt_layout_inst\"; }\n-\n-public:\n-  static cvt_layout_inst* create(value *arg, const std::string &name = \"\", instruction *next = nullptr);\n-  _TRITON_DEFINE_CLONE(cvt_layout_inst)\n-  _TRITON_DEFINE_ACCEPT(cvt_layout_inst)\n-};\n-\n-class barrier_inst: public instruction{\n-private:\n-  barrier_inst(context &ctx, const std::string &name, instruction *next);\n-  std::string repr_impl() const { return \"barrier\"; }\n-  _TRITON_DEFINE_CLONE(barrier_inst)\n-  _TRITON_DEFINE_ACCEPT(barrier_inst)\n-\n-public:\n-  static barrier_inst* create(context &ctx, const std::string &name = \"\",\n-                                            instruction *next = nullptr);\n-};\n-\n-class async_wait_inst: public instruction{\n-private:\n-  async_wait_inst(context &ctx, int N, const std::string &name, instruction *next);\n-  std::string repr_impl() const { return \"async_wait_group \" + std::to_string(N_) ; }\n-  _TRITON_DEFINE_CLONE(async_wait_inst)\n-  _TRITON_DEFINE_ACCEPT(async_wait_inst)\n-\n-public:\n-  static async_wait_inst* create(context &ctx, int N,\n-                                 const std::string &name = \"\", instruction *next = nullptr);\n-  int get_N() { return N_; }\n-  void set_N(int n) { N_ = n; }\n-\n-private:\n-  int N_;\n-};\n-\n-class prefetch_s_inst : public instruction {\n-  std::string repr_impl() const { return \"prefetch_s\"; }\n-  _TRITON_DEFINE_CLONE(prefetch_s_inst)\n-  _TRITON_DEFINE_ACCEPT(prefetch_s_inst)\n-  \n-  /// inc_: 0->first, 1->latch\n-  int inc_ = 0;\n-public:\n-  prefetch_s_inst(context &ctx, value *arg, int inc, const std::string &name, instruction *next) \n-    : instruction(type::get_void_ty(ctx), INST_PREFETCH_S, 1, name, next), inc_(inc) {\n-    set_operand(0, arg);\n-  }\n-  int get_inc() const { return inc_; }\n-  static prefetch_s_inst *create(context &ctx, value *arg, int inc, const std::string &name = \"\",\n-   instruction *next=nullptr);\n-};\n-\n-/* constant range */\n-class make_range: public instruction{\n-  make_range(type *ty, constant_int* first, constant_int* last);\n-  std::string repr_impl() const { return \"make_range[\" + first_->repr() + \" : \" + last_->repr() + \"]\"; }\n-  _TRITON_DEFINE_CLONE(make_range)\n-  _TRITON_DEFINE_ACCEPT(make_range)\n-\n-public:\n-  static make_range *create(constant_int *first, constant_int *last);\n-  const constant_int* get_first() const;\n-  const constant_int* get_last() const;\n-\n-private:\n-  constant_int* first_;\n-  constant_int* last_;\n-};\n-\n-/* timing utilities */\n-class clock_inst: public instruction{\n-  clock_inst(context &ctx, const std::string &name, instruction *next);\n-  std::string repr_impl() const { return \"clock\"; }\n-  _TRITON_DEFINE_CLONE(clock_inst)\n-  _TRITON_DEFINE_ACCEPT(clock_inst)\n-\n-public:\n-  static clock_inst* create(context &ctx, const std::string &name = \"\", instruction *next = nullptr);\n-};\n-\n-class globaltimer_inst: public instruction{\n-  globaltimer_inst(context &ctx, const std::string &name, instruction *next);\n-  std::string repr_impl() const { return \"globaltimer\"; }\n-  _TRITON_DEFINE_CLONE(globaltimer_inst)\n-  _TRITON_DEFINE_ACCEPT(globaltimer_inst)\n-\n-public:\n-  static globaltimer_inst* create(context &ctx, const std::string &name = \"\", instruction *next = nullptr);\n-};\n-\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/ir/metadata.h", "status": "removed", "additions": 0, "deletions": 32, "changes": 32, "file_content_changes": "@@ -1,32 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_IR_METADATA_H_\n-#define _TRITON_IR_METADATA_H_\n-\n-namespace triton{\n-namespace ir{\n-\n-\n-/* Metadata */\n-class metadata{\n-public:\n-  enum kind_t{\n-    multiple_of,\n-    max_contiguous\n-  };\n-\n-private:\n-  metadata(kind_t kind, unsigned value);\n-\n-public:\n-  static metadata* get(kind_t kind, unsigned value);\n-\n-private:\n-  kind_t kind_;\n-  unsigned value_;\n-};\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/ir/module.h", "status": "removed", "additions": 0, "deletions": 130, "changes": 130, "file_content_changes": "@@ -1,130 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_IR_MODULE_H_\n-#define _TRITON_IR_MODULE_H_\n-\n-#include <map>\n-#include <set>\n-#include <stack>\n-#include <string>\n-#include <functional>\n-#include \"triton/ir/builder.h\"\n-#include \"triton/ir/metadata.h\"\n-#include \"triton/ir/context.h\"\n-\n-namespace triton{\n-\n-namespace lang{\n-\n-class iteration_statement;\n-class compound_statement;\n-\n-}\n-\n-namespace ir{\n-\n-class basic_block;\n-class phi_node;\n-class value;\n-class context;\n-class function;\n-class attribute;\n-class function_type;\n-class constant;\n-class global_value;\n-class alloc_const;\n-\n-class value_constructor {\n-  typedef std::pair<std::string, basic_block*> val_key_t;\n-\n-private:\n-  phi_node *make_phi(type *ty, unsigned num_values, basic_block *block);\n-  value *try_remove_trivial_phis(ir::phi_node *&phi);\n-  value *add_phi_operands(const std::string& name, phi_node *&phi);\n-  value *get_value_recursive(const std::string& name, basic_block *block);\n-\n-public:\n-  value_constructor(builder &builder);\n-\n-  void set_value(const std::string& name, basic_block* block, value *x);\n-  void set_value(const std::string& name, value* x);\n-  const std::map<val_key_t, value*>& get_values() { return values_; }\n-  void set_values(const std::map<val_key_t, value*>& values) { values_ = values; }\n-  value *get_value(const std::string& name, basic_block* block);\n-  value *get_value(const std::string& name);\n-  void set_type(const std::string& name, ir::type* ty) { types_[name] = ty; }\n-  // Seal block -- no more predecessors will be added\n-  void seal_block(basic_block *block);\n-  // Metadata\n-\n-private:\n-  ir::builder& builder_;\n-  std::map<val_key_t, value*> values_;\n-  std::map<std::string, type*> types_;\n-  std::set<basic_block*> sealed_blocks_;\n-  std::map<basic_block*, std::map<std::string, phi_node*>> incomplete_phis_;\n-  std::map<value*, value**> current_phi_;\n-};\n-\n-/* Module */\n-\n-class module {\n-  typedef std::pair<std::string, basic_block*> val_key_t;\n-  typedef std::pair<ir::metadata::kind_t, unsigned> md_pair_t;\n-  friend class function;\n-\n-public:\n-  typedef std::map<std::string, global_value*> symbols_map_t;\n-  typedef std::vector<function*> functions_list_t;\n-\n-private:\n-  void push_function(function *fn) { functions_.push_back(fn); }\n-\n-public:\n-  module(const std::string &name, builder &builder): name_(name), builder_(builder) {}\n-  builder &get_builder() { return builder_; };\n-  const std::string& get_name() { return name_; };\n-\n-  // Functions\n-  const functions_list_t &get_function_list() const { return functions_; }\n-  functions_list_t &get_function_list()             { return functions_; }\n-  function *get_function(const std::string& name)   {\n-    if(symbols_.find(name) == symbols_.end())\n-      throw std::runtime_error(\"function \" + name + \" is not declared\");\n-    return (function*)symbols_.at(name);\n-  }\n-  function *get_or_insert_function(const std::string &name, function_type *ty);\n-  bool has_function(const std::string& name){\n-    return symbols_.find(name) != symbols_.end();\n-  }\n-  void remove_function(ir::function* fn){\n-    functions_.erase(std::remove(functions_.begin(), functions_.end(), fn), functions_.end());\n-  }\n-\n-  void reset_ret_ty(const std::string& name, type* ty);\n-\n-  // Const allocation\n-  void add_alloc(ir::alloc_const* x)                          { allocs_.push_back(x); }\n-  const std::vector<ir::alloc_const*>& allocs()               { return allocs_; }\n-  // Register global\n-  void register_global(const std::string& name, ir::value *x) { globals_[name] = x; }\n-  const std::map<std::string, ir::value*>& globals() const    { return globals_; }\n-  // Metadata\n-  void print(std::ostream &os);\n-  void add_metadata(const std::string &name, md_pair_t x)     { metadatas_[name] = x; }\n-  const std::map<std::string, md_pair_t> &get_metadatas() const { return metadatas_; }\n-\n-private:\n-  std::string name_;\n-  builder &builder_;\n-  functions_list_t functions_;\n-  symbols_map_t symbols_;\n-  std::vector<ir::alloc_const*> allocs_;\n-  std::map<std::string, ir::value*> globals_;\n-  std::map<std::string, md_pair_t> metadatas_;\n-};\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/ir/print.h", "status": "removed", "additions": 0, "deletions": 22, "changes": 22, "file_content_changes": "@@ -1,22 +0,0 @@\n-#ifndef _TRITON_IR_PRINT_H_\n-#define _TRITON_IR_PRINT_H_\n-\n-#include \"builder.h\"\n-\n-namespace triton{\n-namespace ir{\n-\n-class module;\n-class function;\n-class basic_block;\n-class instruction;\n-\n-void print(module &mod, std::ostream& os);\n-void print(function &func, std::ostream& os);\n-void print(basic_block &bb, std::ostream& os);\n-void print(instruction &instr, std::ostream& os);\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/ir/type.h", "status": "removed", "additions": 0, "deletions": 252, "changes": 252, "file_content_changes": "@@ -1,252 +0,0 @@\n-\ufeff#pragma once\n-\n-#ifndef _TRITON_IR_TYPE_H_\n-#define _TRITON_IR_TYPE_H_\n-\n-#include <cassert>\n-#include <vector>\n-#include <string>\n-#include <stdexcept>\n-\n-namespace triton{\n-namespace ir{\n-\n-class context;\n-class value;\n-class integer_type;\n-class constant_int;\n-\n-/* Type */\n-class type {\n-public:\n-  typedef std::vector<unsigned>\t         block_shapes_t;\n-\n-  typedef std::vector<type*>                  contained_tys_vec_t;\n-  typedef contained_tys_vec_t::iterator       ty_iterator;\n-  typedef contained_tys_vec_t::const_iterator const_ty_iterator;\n-\n-public:\n-  enum id_t {\n-    // primitive types\n-    VoidTyID = 0,    ///< type with no size\n-    FP8TyID,         ///< 8-bit floating point type (3 bits mantissa)\n-    FP16TyID,        ///< 16-bit floating point type (10 bits mantissa)\n-    BF16TyID,        ///< 16-bit floating point type (7 bits mantissa)\n-    FP32TyID,        ///< 32-bit floating point type\n-    FP64TyID,        ///< 64-bit floating point type\n-    LabelTyID,       ///< Labels\n-    MetadataTyID,    ///< Metadata\n-    TokenTyID,       ///< Token\n-    // derived types\n-    IntegerTyID,     ///< Arbitrary bit width integers\n-    FunctionTyID,    ///< Functions\n-    PointerTyID,     ///< Pointers\n-    StructTyID,      ///< Struct\n-    BlockTyID,       ///< Block\n-  };\n-\n-public:\n-  //constructors\n-  type(context &ctx, id_t id) : ctx_(ctx), id_(id) { }\n-\n-  //destructor\n-  virtual ~type(){}\n-\n-  // accessors\n-  context &get_context() const { return ctx_; }\n-  id_t get_type_id() const     { return id_;  }\n-  // type attributes\n-  unsigned get_fp_mantissa_width() const;\n-  unsigned get_integer_bitwidth() const;\n-  unsigned get_tile_bitwidth() const;\n-  unsigned get_primitive_size_in_bits() const;\n-  type *get_scalar_ty() const;\n-  block_shapes_t get_block_shapes() const;\n-  const size_t get_tile_rank() const;\n-  const size_t get_tile_ranks1() const;\n-  unsigned get_tile_num_elements() const;\n-  type *get_tile_element_ty() const;\n-  unsigned get_pointer_address_space() const;\n-  type *get_pointer_element_ty() const;\n-  unsigned get_struct_numel() const { return contained_tys_.size(); }\n-  type *get_struct_type(unsigned int i) const { return contained_tys_[i]; }\n-\n-  // primitive predicates\n-  bool is_void_ty() const               { return id_ == VoidTyID; }\n-  bool is_fp8_ty() const                { return id_ == FP8TyID; }\n-  bool is_fp16_ty() const               { return id_ == FP16TyID; }\n-  bool is_bf16_ty() const               { return id_ == BF16TyID; }\n-  bool is_fp32_ty() const               { return id_ == FP32TyID; }\n-  bool is_fp64_ty() const               { return id_ == FP64TyID; }\n-  bool is_label_ty()  const             { return id_ == LabelTyID;}\n-  bool is_metadata_ty() const           { return id_ == MetadataTyID; }\n-  bool is_token_ty() const              { return id_ == TokenTyID; }\n-  bool is_integer_ty() const            { return id_ == IntegerTyID; }\n-  bool is_bool_ty() const               { return is_integer_ty(1); }\n-  bool is_pointer_ty() const            { return id_ == PointerTyID; }\n-  bool is_block_ty() const               { return id_ == BlockTyID; }\n-  bool is_struct_ty() const             { return id_ == StructTyID; }\n-\n-  // Composite predicates\n-  bool is_int_or_tileint_ty();\n-  bool is_integer_ty(unsigned width) const;\n-  bool is_floating_point_ty() const;\n-  bool is_sized() const ;\n-\n-  // Factory methods\n-  // primitive types\n-  static type *get_void_ty(context &ctx);\n-  static type *get_label_ty(context &ctx);\n-  // half\n-  static type *get_fp8_ty(context &ctx);\n-  static type *get_fp16_ty(context &ctx);\n-  static type *get_bf16_ty(context &ctx);\n-  static type *get_fp32_ty(context &ctx);\n-  static type *get_fp64_ty(context &ctx);\n-  // integer types\n-  static integer_type *get_int1_ty(context &ctx);\n-  static integer_type *get_int8_ty(context &ctx);\n-  static integer_type *get_int16_ty(context &ctx);\n-  static integer_type *get_int32_ty(context &ctx);\n-  static integer_type *get_int64_ty(context &ctx);\n-  static integer_type *get_int128_ty(context &ctx);\n-\n-  // repr\n-  std::string tile_repr() const {\n-    std::string res = get_tile_element_ty()->repr();\n-    auto shapes = get_block_shapes();\n-    res += \"<\";\n-    for(size_t i = 0; i < shapes.size(); i++){\n-      if(i > 0)\n-        res += \", \";\n-      res += std::to_string(shapes[i]);\n-    }\n-    res+= \">\";\n-    return res;\n-  }\n-\n-  std::string repr() const {\n-    switch(id_) {\n-      case VoidTyID: return \"void\";\n-      case FP8TyID: return \"fp8\";\n-      case BF16TyID: return \"bf16\";\n-      case FP16TyID: return \"f16\";\n-      case FP32TyID: return \"f32\";\n-      case FP64TyID: return \"f64\";\n-      case LabelTyID: return \"label\";\n-      case MetadataTyID: return \"md\";\n-      case TokenTyID: return \"tok\";\n-      case IntegerTyID: return (\"i\") + std::to_string(get_integer_bitwidth());\n-      case FunctionTyID: return \"fn\";\n-      case PointerTyID: return get_pointer_element_ty()->repr() + \"*\";\n-      case StructTyID: return \"struct\";\n-      case BlockTyID: return tile_repr();\n-      default: break;\n-    }\n-    throw std::logic_error(\"unknown type id '\" + std::to_string(id_) + \"'\");\n-  };\n-\n-private:\n-  context &ctx_;\n-  id_t id_;\n-\n-protected:\n-  contained_tys_vec_t contained_tys_;\n-};\n-\n-class integer_type: public type {\n-  friend class context_impl;\n-\n-private:\n-  // constructors\n-  integer_type(context &ctx, unsigned bitwidth)\n-    : type(ctx, IntegerTyID), bitwidth_(bitwidth) {}\n-\n-public:\n-  // accessors\n-  unsigned get_bitwidth() const { return bitwidth_; }\n-\n-  // factory methods\n-  static integer_type* get(context &ctx, unsigned width);\n-\n-private:\n-  unsigned bitwidth_;\n-};\n-\n-class composite_type: public type{\n-protected:\n-  using type::type;\n-\n-public:\n-  bool index_valid(value *idx) const;\n-  type* get_type_at_index(value *idx) const;\n-};\n-\n-class struct_type: public composite_type {\n-public:\n-  struct_type(const contained_tys_vec_t& tys, bool is_packed);\n-  unsigned get_num_types() const { return contained_tys_.size(); }\n-  static struct_type* get(const contained_tys_vec_t& tys, bool is_packed);\n-\n-private:\n-  bool is_packed_;\n-};\n-\n-class block_type: public composite_type {\n-private:\n-  block_type(type *ty, const block_shapes_t &shapes);\n-  static bool is_valid_elt_ty(type *ty);\n-\n-public:\n-  // accessors\n-  const block_shapes_t& get_shapes() const { return shapes_; }\n-  unsigned get_num_elements() const;\n-  unsigned get_bitwidth() const;\n-\n-  // factory methods\n-  static block_type* get(type *ty, const block_shapes_t &shapes);\n-  static block_type* get_same_shapes(type *ty, type *ref);\n-\n-private:\n-  block_shapes_t shapes_;\n-};\n-\n-class pointer_type: public type {\n-private:\n-  pointer_type(type *ty, unsigned address_space);\n-  static bool is_valid_elt_ty(type *ty);\n-\n-public:\n-  // accessors\n-  unsigned get_address_space()               const { return address_space_; }\n-  type *get_element_ty()                     const { return contained_tys_[0]; }\n-  // factory methods\n-  static pointer_type* get(type *ty, unsigned address_space);\n-\n-private:\n-  unsigned address_space_;\n-};\n-\n-class function_type: public type {\n-private:\n-  function_type(type *ret_ty, const std::vector<type *> &param_tys);\n-\n-public:\n-  // accessors\n-  unsigned get_num_params()         const { return contained_tys_.size() - 1;  }\n-  const_ty_iterator params_begin() const { return contained_tys_.begin() + 1; }\n-  const_ty_iterator params_end()   const { return contained_tys_.end(); }\n-  ty_iterator       params_begin()       { return contained_tys_.begin() + 1; }\n-  ty_iterator       params_end()         { return contained_tys_.end(); }\n-  type*    get_param_ty(unsigned i) const { return contained_tys_.at(1 + i);   }\n-  type*    get_return_ty()          const { return contained_tys_.at(0);       }\n-  void     reset_ret_ty(type* ty)         { contained_tys_[0] = ty;}\n-  // factory methods\n-  static function_type* get(type *ret_ty, const std::vector<type*>& param_tys);\n-};\n-\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/ir/utils.h", "status": "removed", "additions": 0, "deletions": 30, "changes": 30, "file_content_changes": "@@ -1,30 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_IR_CFG_H_\n-#define _TRITON_IR_CFG_H_\n-\n-#include <vector>\n-#include <functional>\n-\n-namespace triton{\n-namespace ir{\n-\n-class module;\n-class function;\n-class basic_block;\n-class instruction;\n-class value;\n-\n-class cfg {\n-public:\n-  static std::vector<basic_block *> post_order(function* fn);\n-  static std::vector<basic_block *> reverse_post_order(function* fn);\n-};\n-\n-void for_each_instruction(ir::module& mod, const std::function<void(triton::ir::instruction*)> &fn);\n-void for_each_value(ir::module& mod, const std::function<void(triton::ir::value *)> &fn);\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/ir/value.h", "status": "removed", "additions": 0, "deletions": 95, "changes": 95, "file_content_changes": "@@ -1,95 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_IR_VALUE_H_\n-#define _TRITON_IR_VALUE_H_\n-\n-#include <string>\n-#include <vector>\n-#include <set>\n-\n-namespace triton{\n-namespace ir{\n-\n-class type;\n-class use;\n-class user;\n-class visitor;\n-\n-//===----------------------------------------------------------------------===//\n-//                               value class\n-//===----------------------------------------------------------------------===//\n-\n-class value {\n-public:\n-  typedef std::vector<user*> users_t;\n-\n-public:\n-  // constructor\n-  value(type *ty, const std::string &name = \"\");\n-  virtual ~value(){ }\n-  // uses\n-  void add_use(user* arg);\n-  users_t::iterator erase_use(user* arg);\n-  const std::vector<user*> &get_users() { return users_; }\n-  void replace_all_uses_with(value *target);\n-  // name\n-  void set_name(const std::string &name);\n-  const std::string &get_name() const { return name_; }\n-  bool has_name() const { return !name_.empty(); }\n-  type* get_type() const { return ty_; }\n-  // visitor\n-  virtual void accept(visitor *v) = 0;\n-\n-private:\n-  std::string name_;\n-\n-protected:\n-  type *ty_;\n-  users_t users_;\n-};\n-\n-//===----------------------------------------------------------------------===//\n-//                               user class\n-//===----------------------------------------------------------------------===//\n-\n-class user: public value{\n-public:\n-  typedef std::vector<value*>      ops_t;\n-  typedef ops_t::iterator       op_iterator;\n-  typedef ops_t::const_iterator const_op_iterator;\n-\n-protected:\n-  void resize_ops(unsigned num_ops) { ops_.resize(num_ops + num_hidden_); num_ops_ = num_ops; }\n-  void resize_hidden(unsigned num_hidden) { ops_.resize(num_ops_ + num_hidden); num_hidden_ = num_hidden; }\n-\n-public:\n-  // Constructor\n-  user(type *ty, unsigned num_ops, const std::string &name = \"\")\n-      : value(ty, name), ops_(num_ops), num_ops_(num_ops), num_hidden_(0){\n-  }\n-  virtual ~user() { }\n-\n-  // Operands\n-  const ops_t& ops() { return ops_; }\n-  const ops_t& ops() const { return ops_; }\n-  op_iterator op_begin() { return ops_.begin(); }\n-  op_iterator op_end()   { return ops_.end(); }\n-  void     set_operand(unsigned i, value *x);\n-  value   *get_operand(unsigned i) const;\n-  unsigned get_num_operands() const ;\n-  unsigned get_num_hidden() const;\n-\n-  // Utils\n-  value::users_t::iterator replace_uses_of_with(value *before, value *after);\n-\n-\n-private:\n-  ops_t ops_;\n-  unsigned num_ops_;\n-  unsigned num_hidden_;\n-};\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/ir/visitor.h", "status": "removed", "additions": 0, "deletions": 185, "changes": 185, "file_content_changes": "@@ -1,185 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_IR_VISITOR_H_\n-#define _TRITON_IR_VISITOR_H_\n-\n-\n-namespace triton{\n-namespace ir{\n-\n-class value;\n-\n-class instruction;\n-\n-class call_inst;\n-class launch_inst;\n-\n-class phi_node;\n-class binary_operator;\n-class getelementptr_inst;\n-\n-class icmp_inst;\n-class fcmp_inst;\n-class cast_inst;\n-class trunc_inst;\n-class z_ext_inst;\n-class s_ext_inst;\n-class fp_trunc_inst;\n-class fp_ext_inst;\n-class ui_to_fp_inst;\n-class si_to_fp_inst;\n-class fp_to_ui_inst;\n-class fp_to_si_inst;\n-class ptr_to_int_inst;\n-class int_to_ptr_inst;\n-class bit_cast_inst;\n-class addr_space_cast_inst;\n-\n-class return_inst;\n-class cond_branch_inst;\n-class uncond_branch_inst;\n-\n-\n-class unmasked_load_inst;\n-class masked_load_inst;\n-class unmasked_store_inst;\n-class masked_store_inst;\n-\n-class extract_value_inst;\n-class insert_value_inst;\n-\n-class retile_inst;\n-class reshape_inst;\n-class splat_inst;\n-class cat_inst;\n-class broadcast_inst;\n-class downcast_inst;\n-\n-class umulhi_inst;\n-class exp_inst;\n-class cos_inst;\n-class sin_inst;\n-class log_inst;\n-\n-class get_program_id_inst;\n-class get_num_programs_inst;\n-class atomic_inst;\n-class atomic_cas_inst;\n-class atomic_rmw_inst;\n-class dot_inst;\n-class trans_inst;\n-class sqrt_inst;\n-class reduce_inst;\n-class select_inst;\n-\n-class cvt_layout_inst;\n-class copy_to_shared_inst;\n-class copy_from_shared_inst;\n-class masked_load_async_inst;\n-class barrier_inst;\n-class async_wait_inst;\n-class make_range_dyn;\n-class make_range;\n-class prefetch_s_inst;\n-class clock_inst;\n-class globaltimer_inst;\n-\n-class make_range_sta;\n-class undef_value;\n-class constant_int;\n-class constant_fp;\n-class global_value;\n-class global_object;\n-class alloc_const;\n-\n-class constant_fp;\n-class undef_value;\n-class constant_int;\n-class constant_fp;\n-class global_value;\n-class global_object;\n-class alloc_const;\n-\n-class function;\n-\n-class basic_block;\n-\n-class argument;\n-\n-class visitor {\n-public:\n-  virtual ~visitor() {}\n-\n-  virtual void visit_value(ir::value*);\n-  virtual void visit_call_inst(ir::call_inst*) = 0;\n-  virtual void visit_launch_inst(ir::launch_inst*) = 0;\n-\n-  virtual void visit_basic_block(basic_block*) = 0;\n-  virtual void visit_argument(argument*) = 0;\n-  virtual void visit_phi_node(phi_node*) = 0;\n-  virtual void visit_binary_operator(binary_operator*) = 0;\n-  virtual void visit_getelementptr_inst(getelementptr_inst*) = 0;\n-\n-  virtual void visit_icmp_inst(icmp_inst*) = 0;\n-  virtual void visit_fcmp_inst(fcmp_inst*) = 0;\n-  virtual void visit_cast_inst(cast_inst*) = 0;\n-\n-  virtual void visit_return_inst(return_inst*) = 0;\n-  virtual void visit_cond_branch_inst(cond_branch_inst*) = 0;\n-  virtual void visit_uncond_branch_inst(uncond_branch_inst*) = 0;\n-\n-\n-  virtual void visit_unmasked_load_inst(unmasked_load_inst*) = 0;\n-  virtual void visit_masked_load_inst(masked_load_inst*) = 0;\n-  virtual void visit_unmasked_store_inst(unmasked_store_inst*) = 0;\n-  virtual void visit_masked_store_inst(masked_store_inst*) = 0;\n-\n-  virtual void visit_umulhi_inst(umulhi_inst*) = 0;\n-  virtual void visit_exp_inst(exp_inst*) = 0;\n-  virtual void visit_cos_inst(cos_inst*) = 0;\n-  virtual void visit_sin_inst(sin_inst*) = 0;\n-  virtual void visit_log_inst(log_inst*) = 0;\n-\n-  virtual void visit_extract_value_inst(extract_value_inst*) = 0;\n-  virtual void visit_insert_value_inst(insert_value_inst*) = 0;\n-\n-  virtual void visit_reshape_inst(reshape_inst*) = 0;\n-  virtual void visit_splat_inst(splat_inst*) = 0;\n-  virtual void visit_cat_inst(cat_inst*) = 0;\n-  virtual void visit_broadcast_inst(broadcast_inst*) = 0;\n-  virtual void visit_downcast_inst(downcast_inst*) = 0;\n-\n-  virtual void visit_get_program_id_inst(get_program_id_inst*) = 0;\n-  virtual void visit_get_num_programs_inst(get_num_programs_inst*) = 0;\n-  virtual void visit_atomic_cas_inst(atomic_cas_inst*) = 0;\n-  virtual void visit_atomic_rmw_inst(atomic_rmw_inst*) = 0;\n-  virtual void visit_dot_inst(dot_inst*) = 0;\n-  virtual void visit_trans_inst(trans_inst*) = 0;\n-  virtual void visit_sqrt_inst(sqrt_inst*) = 0;\n-  virtual void visit_reduce_inst(reduce_inst*) = 0;\n-  virtual void visit_select_inst(select_inst*) = 0;\n-\n-  virtual void visit_cvt_layout_inst(cvt_layout_inst*) = 0;\n-  virtual void visit_copy_to_shared_inst(copy_to_shared_inst*) = 0;\n-  virtual void visit_copy_from_shared_inst(copy_from_shared_inst*) = 0;\n-\n-\n-  virtual void visit_masked_load_async_inst(masked_load_async_inst*)= 0;\n-  virtual void visit_barrier_inst(barrier_inst*) = 0;\n-  virtual void visit_async_wait_inst(async_wait_inst*) = 0;\n-  virtual void visit_make_range(make_range*) = 0;\n-  virtual void visit_prefetch_s_inst(prefetch_s_inst*) = 0;\n-  virtual void visit_function(function*) = 0;\n-  virtual void visit_clock_inst(clock_inst*) = 0;\n-  virtual void visit_globaltimer_inst(globaltimer_inst*) = 0;\n-\n-  virtual void visit_undef_value(undef_value*) = 0;\n-  virtual void visit_constant_int(constant_int*) = 0;\n-  virtual void visit_constant_fp(constant_fp*) = 0;\n-  virtual void visit_alloc_const(alloc_const*) = 0;\n-};\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/tools/bench.hpp", "status": "removed", "additions": 0, "deletions": 54, "changes": 54, "file_content_changes": "@@ -1,54 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_TOOLS_BENCH_H_\n-#define _TRITON_TOOLS_BENCH_H_\n-\n-#include <chrono>\n-#include <functional>\n-#include <algorithm>\n-#include \"triton/driver/device.h\"\n-#include \"triton/driver/stream.h\"\n-\n-namespace triton{\n-namespace tools{\n-\n-class timer{\n-    typedef std::chrono::high_resolution_clock high_resolution_clock;\n-    typedef std::chrono::nanoseconds nanoseconds;\n-\n-public:\n-    explicit timer(bool run = false)\n-    { if (run) start(); }\n-\n-    void start()\n-    { _start = high_resolution_clock::now(); }\n-\n-    nanoseconds get() const\n-    { return std::chrono::duration_cast<nanoseconds>(high_resolution_clock::now() - _start); }\n-\n-private:\n-    high_resolution_clock::time_point _start;\n-};\n-\n-inline double bench(std::function<void()> const & op, driver::stream * stream, size_t warmup = 10, size_t repeat = 200)\n-{\n-  timer tmr;\n-  std::vector<size_t> times;\n-  double total_time = 0;\n-  for(size_t i = 0; i < warmup; i++)\n-    op();\n-  stream->synchronize();\n-  tmr.start();\n-  for(size_t i = 0; i < repeat; i++){\n-    op();\n-  }\n-  stream->synchronize();\n-  return (float)tmr.get().count() / repeat;\n-\n-//  return *std::min_element(times.begin(), times.end());\n-}\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/tools/graph.h", "status": "removed", "additions": 0, "deletions": 70, "changes": 70, "file_content_changes": "@@ -1,70 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_TOOLS_THREAD_GRAPH_H_\n-#define _TRITON_TOOLS_THREAD_GRAPH_H_\n-\n-#include \"llvm/ADT/SetVector.h\"\n-\n-#include <map>\n-#include <vector>\n-#include <iostream>\n-\n-namespace triton {\n-namespace tools{\n-\n-template<class node_t>\n-class graph {\n-  typedef std::map<node_t, llvm::SetVector<node_t>> edges_t;\n-\n-public:\n-  typedef std::map<size_t, std::vector<node_t>> cmap_t;\n-  typedef std::map<node_t, size_t> nmap_t;\n-\n-private:\n-  void connected_components_impl(node_t x, llvm::SetVector<node_t> &nodes,\n-                                 nmap_t* nmap, cmap_t* cmap, int id) const {\n-    if(nmap)\n-      (*nmap)[x] = id;\n-    if(cmap)\n-      (*cmap)[id].push_back(x);\n-    if (nodes.count(x)) {\n-      nodes.remove(x);\n-      for(const node_t &y: edges_.at(x))\n-        connected_components_impl(y, nodes, nmap, cmap, id);\n-    }\n-  }\n-\n-public:\n-  void connected_components(cmap_t *cmap, nmap_t *nmap) const {\n-    if(cmap)\n-      cmap->clear();\n-    if(nmap)\n-      nmap->clear();\n-    llvm::SetVector<node_t> nodes = nodes_;\n-    unsigned id = 0;\n-    while(!nodes.empty()){\n-      connected_components_impl(*nodes.begin(), nodes, nmap, cmap, id++);\n-    }\n-  }\n-\n-  void add_edge(node_t x, node_t y) {\n-    nodes_.insert(x);\n-    nodes_.insert(y);\n-    edges_[x].insert(y);\n-    edges_[y].insert(x);\n-  }\n-\n-  void clear() {\n-    nodes_.clear();\n-    edges_.clear();\n-  }\n-\n-private:\n-  llvm::SetVector<node_t> nodes_;\n-  edges_t edges_;\n-};\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/tools/sha1.hpp", "status": "removed", "additions": 0, "deletions": 186, "changes": 186, "file_content_changes": "@@ -1,186 +0,0 @@\n-/*\n- Copyright (c) 2011, Micael Hildenborg\n- All rights reserved.\n- Redistribution and use in source and binary forms, with or without\n- modification, are permitted provided that the following conditions are met:\n-    * Redistributions of source code must retain the above copyright\n-      notice, this list of conditions and the following disclaimer.\n-    * Redistributions in binary form must reproduce the above copyright\n-      notice, this list of conditions and the following disclaimer in the\n-      documentation and/or other materials provided with the distribution.\n-    * Neither the name of Micael Hildenborg nor the\n-      names of its contributors may be used to endorse or promote products\n-      derived from this software without specific prior written permission.\n- THIS SOFTWARE IS PROVIDED BY Micael Hildenborg ''AS IS'' AND ANY\n- EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n- WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n- DISCLAIMED. IN NO EVENT SHALL Micael Hildenborg BE LIABLE FOR ANY\n- DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n- (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n- LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n- ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n- (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n- SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n- */\n-\n-/*\n- Contributors:\n- Gustav\n- Several members in the gamedev.se forum.\n- Gregory Petrosyan\n- */\n-\n-#ifndef _TRITON_TOOLS_SHA1_HPP_\n-#define _TRITON_TOOLS_SHA1_HPP_\n-\n-namespace sha1\n-{\n-    namespace // local\n-    {\n-        // Rotate an integer value to left.\n-        inline unsigned int rol(const unsigned int value,\n-                const unsigned int steps)\n-        {\n-            return ((value << steps) | (value >> (32 - steps)));\n-        }\n-\n-        // Sets the first 16 integers in the buffert to zero.\n-        // Used for clearing the W buffert.\n-        inline void clearWBuffert(unsigned int* buffert)\n-        {\n-            for (int pos = 16; --pos >= 0;)\n-            {\n-                buffert[pos] = 0;\n-            }\n-        }\n-\n-        inline void innerHash(unsigned int* result, unsigned int* w)\n-        {\n-            unsigned int a = result[0];\n-            unsigned int b = result[1];\n-            unsigned int c = result[2];\n-            unsigned int d = result[3];\n-            unsigned int e = result[4];\n-\n-            int round = 0;\n-\n-            #define sha1macro(func,val) \\\n-      { \\\n-                const unsigned int t = rol(a, 5) + (func) + e + val + w[round]; \\\n-        e = d; \\\n-        d = c; \\\n-        c = rol(b, 30); \\\n-        b = a; \\\n-        a = t; \\\n-      }\n-\n-            while (round < 16)\n-            {\n-                sha1macro((b & c) | (~b & d), 0x5a827999)\n-                ++round;\n-            }\n-            while (round < 20)\n-            {\n-                w[round] = rol((w[round - 3] ^ w[round - 8] ^ w[round - 14] ^ w[round - 16]), 1);\n-                sha1macro((b & c) | (~b & d), 0x5a827999)\n-                ++round;\n-            }\n-            while (round < 40)\n-            {\n-                w[round] = rol((w[round - 3] ^ w[round - 8] ^ w[round - 14] ^ w[round - 16]), 1);\n-                sha1macro(b ^ c ^ d, 0x6ed9eba1)\n-                ++round;\n-            }\n-            while (round < 60)\n-            {\n-                w[round] = rol((w[round - 3] ^ w[round - 8] ^ w[round - 14] ^ w[round - 16]), 1);\n-                sha1macro((b & c) | (b & d) | (c & d), 0x8f1bbcdc)\n-                ++round;\n-            }\n-            while (round < 80)\n-            {\n-                w[round] = rol((w[round - 3] ^ w[round - 8] ^ w[round - 14] ^ w[round - 16]), 1);\n-                sha1macro(b ^ c ^ d, 0xca62c1d6)\n-                ++round;\n-            }\n-\n-            #undef sha1macro\n-\n-            result[0] += a;\n-            result[1] += b;\n-            result[2] += c;\n-            result[3] += d;\n-            result[4] += e;\n-        }\n-    } // namespace\n-\n-    inline void calc(const void* src, const int bytelength, unsigned char* hash)\n-    {\n-        // Init the result array.\n-        unsigned int result[5] = { 0x67452301, 0xefcdab89, 0x98badcfe, 0x10325476, 0xc3d2e1f0 };\n-\n-        // Cast the void src pointer to be the byte array we can work with.\n-        const unsigned char* sarray = (const unsigned char*) src;\n-\n-        // The reusable round buffer\n-        unsigned int w[80];\n-\n-        // Loop through all complete 64byte blocks.\n-        const int endOfFullBlocks = bytelength - 64;\n-        int endCurrentBlock;\n-        int currentBlock = 0;\n-\n-        while (currentBlock <= endOfFullBlocks)\n-        {\n-            endCurrentBlock = currentBlock + 64;\n-\n-            // Init the round buffer with the 64 byte block data.\n-            for (int roundPos = 0; currentBlock < endCurrentBlock; currentBlock += 4)\n-            {\n-                // This line will swap endian on big endian and keep endian on little endian.\n-                w[roundPos++] = (unsigned int) sarray[currentBlock + 3]\n-                        | (((unsigned int) sarray[currentBlock + 2]) << 8)\n-                        | (((unsigned int) sarray[currentBlock + 1]) << 16)\n-                        | (((unsigned int) sarray[currentBlock]) << 24);\n-            }\n-            innerHash(result, w);\n-        }\n-\n-        // Handle the last and not full 64 byte block if existing.\n-        endCurrentBlock = bytelength - currentBlock;\n-        clearWBuffert(w);\n-        int lastBlockBytes = 0;\n-        for (;lastBlockBytes < endCurrentBlock; ++lastBlockBytes)\n-        {\n-            w[lastBlockBytes >> 2] |= (unsigned int) sarray[lastBlockBytes + currentBlock] << ((3 - (lastBlockBytes & 3)) << 3);\n-        }\n-        w[lastBlockBytes >> 2] |= 0x80 << ((3 - (lastBlockBytes & 3)) << 3);\n-        if (endCurrentBlock >= 56)\n-        {\n-            innerHash(result, w);\n-            clearWBuffert(w);\n-        }\n-        w[15] = bytelength << 3;\n-        innerHash(result, w);\n-\n-        // Store hash in result pointer, and make sure we get in in the correct order on both endian models.\n-        for (int hashByte = 20; --hashByte >= 0;)\n-        {\n-            hash[hashByte] = (result[hashByte >> 2] >> (((3 - hashByte) & 0x3) << 3)) & 0xff;\n-        }\n-    }\n-\n-    inline void toHexString(const unsigned char* hash, char* hexstring)\n-    {\n-        const char hexDigits[] = { \"0123456789abcdef\" };\n-\n-        for (int hashByte = 20; --hashByte >= 0;)\n-        {\n-            hexstring[hashByte << 1] = hexDigits[(hash[hashByte] >> 4) & 0xf];\n-            hexstring[(hashByte << 1) + 1] = hexDigits[hash[hashByte] & 0xf];\n-        }\n-        hexstring[40] = 0;\n-    }\n-} // namespace sha1\n-\n-#endif"}, {"filename": "include/triton/tools/sys/exec.hpp", "status": "removed", "additions": 0, "deletions": 46, "changes": 46, "file_content_changes": "@@ -1,46 +0,0 @@\n-#ifndef TRITON_TOOLS_SYS_EXEC_HPP\n-#define TRITON_TOOLS_SYS_EXEC_HPP\n-\n-#include <cstdio>\n-#include <iostream>\n-#include <memory>\n-#include <stdexcept>\n-#include <string>\n-\n-namespace triton\n-{\n-namespace tools\n-{\n-\n-\n-#ifdef _WIN32\n-#define popen _popen\n-#define pclose _pclose\n-#endif\n-\n-#ifndef WEXITSTATUS\n-#define WEXITSTATUS(stat_val) ((unsigned)(stat_val) & 255)\n-#endif\n-\n-int exec(const std::string& cmd, std::string& result) {\n-  char buffer[128];\n-  FILE* pipe = popen(cmd.c_str(), \"r\");\n-  if (!pipe)\n-    return 0;\n-  result.clear();\n-  try {\n-    while (fgets(buffer, sizeof buffer, pipe) != NULL)\n-      result += buffer;\n-  } catch (...) {\n-    pclose(pipe);\n-    return 0;\n-  }\n-  int status = pclose(pipe);\n-  return WEXITSTATUS(status);\n-\n-}\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/tools/sys/mkdir.hpp", "status": "removed", "additions": 0, "deletions": 76, "changes": 76, "file_content_changes": "@@ -1,76 +0,0 @@\n-/*\n- * Copyright (c) 2015, PHILIPPE TILLET. All rights reserved.\n- *\n- * This file is part of ISAAC.\n- *\n- * ISAAC is free software; you can redistribute it and/or\n- * modify it under the terms of the GNU Lesser General Public\n- * License as published by the Free Software Foundation; either\n- * version 2.1 of the License, or (at your option) any later version.\n- *\n- * This library is distributed in the hope that it will be useful,\n- * but WITHOUT ANY WARRANTY; without even the implied warranty of\n- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n- * Lesser General Public License for more details.\n- *\n- * You should have received a copy of the GNU Lesser General Public\n- * License along with this library; if not, write to the Free Software\n- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston,\n- * MA 02110-1301  USA\n- */\n-\n-#ifndef TDL_TOOLS_SYS_MKDIR_HPP\n-#define TDL_TOOLS_SYS_MKDIR_HPP\n-\n-#include <cstring>\n-#include <string>\n-#include <cstdlib>\n-#include <sys/stat.h>\n-#include <errno.h>\n-#if defined(_WIN32)\n-  #include <direct.h>\n-#endif\n-\n-namespace triton\n-{\n-\n-namespace tools\n-{\n-\n-    inline int mkdir(std::string const & path)\n-    {\n-        #if defined(_WIN32)\n-            return _mkdir(path.c_str());\n-        #else\n-            return ::mkdir(path.c_str(), 0777);\n-        #endif\n-    }\n-\n-    inline int mkpath(std::string const & path)\n-    {\n-        int status = 0;\n-        size_t pp = 0;\n-        size_t sp;\n-        while ((sp = path.find('/', pp)) != std::string::npos)\n-        {\n-            if (sp != pp){\n-                status = mkdir(path.substr(0, sp));\n-            }\n-            pp = sp + 1;\n-        }\n-        return (status==0 || errno==EEXIST)?0:-1;\n-    }\n-\n-    inline int mtime(std::string const & path)\n-    {\n-      struct stat st;\n-      if(stat(path.c_str(), &st) != 0)\n-        return 0;\n-      return st.st_mtime;\n-    }\n-\n-}\n-\n-}\n-\n-#endif"}, {"filename": "include/triton/tools/thread_pool.h", "status": "removed", "additions": 0, "deletions": 90, "changes": 90, "file_content_changes": "@@ -1,90 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_TOOLS_THREAD_POOL_H_\n-#define _TRITON_TOOLS_THREAD_POOL_H_\n-\n-#include <vector>\n-#include <queue>\n-#include <memory>\n-#include <thread>\n-#include <mutex>\n-#include <condition_variable>\n-#include <future>\n-#include <functional>\n-#include <stdexcept>\n-\n-class ThreadPool {\n-public:\n-    ThreadPool(size_t threads)\n-        :   stop(false) {\n-      for(size_t i = 0;i < threads;++i)\n-          workers.emplace_back(\n-              [this] {\n-                for(;;){\n-                  std::function<void()> task;\n-                  {\n-                    std::unique_lock<std::mutex> lock(this->queue_mutex);\n-                    this->condition.wait(lock,\n-                      [this]{ return this->stop || !this->tasks.empty(); });\n-                    if(this->stop && this->tasks.empty())\n-                      return;\n-                    task = std::move(this->tasks.front());\n-                    this->tasks.pop();\n-                  }\n-                  task();\n-                }\n-              }\n-          );\n-    }\n-\n-\n-    template<class F, class... Args>\n-    auto enqueue(F&& f, Args&&... args)\n-        -> std::future<typename std::result_of<F(Args...)>::type>\n-    {\n-        using return_type = typename std::result_of<F(Args...)>::type;\n-\n-        auto task = std::make_shared< std::packaged_task<return_type()> >(\n-                std::bind(std::forward<F>(f), std::forward<Args>(args)...)\n-            );\n-\n-        std::future<return_type> res = task->get_future();\n-        {\n-            std::unique_lock<std::mutex> lock(queue_mutex);\n-\n-            // don't allow enqueueing after stopping the pool\n-            if(stop)\n-                throw std::runtime_error(\"enqueue on stopped ThreadPool\");\n-\n-            tasks.emplace([task](){ (*task)(); });\n-        }\n-        condition.notify_one();\n-        return res;\n-    }\n-\n-\n-    ~ThreadPool() {\n-        {\n-          std::unique_lock<std::mutex> lock(queue_mutex);\n-          stop = true;\n-        }\n-        condition.notify_all();\n-        for(std::thread &worker: workers)\n-          worker.join();\n-    }\n-\n-\n-private:\n-    // need to keep track of threads so we can join them\n-    std::vector< std::thread > workers;\n-    // the task queue\n-    std::queue< std::function<void()> > tasks;\n-\n-    // synchronization\n-    std::mutex queue_mutex;\n-    std::condition_variable condition;\n-    bool stop;\n-};\n-\n-\n-#endif"}, {"filename": "lib/Analysis/Alias.cpp", "status": "added", "additions": 66, "deletions": 0, "changes": 66, "file_content_changes": "@@ -0,0 +1,66 @@\n+#include \"triton/Analysis/Alias.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+\n+namespace mlir {\n+\n+AliasInfo AliasInfo::join(const AliasInfo &lhs, const AliasInfo &rhs) {\n+  if (lhs == rhs)\n+    return lhs;\n+  AliasInfo ret;\n+  for (auto value : lhs.allocs) {\n+    ret.insert(value);\n+  }\n+  for (auto value : rhs.allocs) {\n+    ret.insert(value);\n+  }\n+  return ret;\n+}\n+\n+void SharedMemoryAliasAnalysis::visitOperation(\n+    Operation *op, ArrayRef<const dataflow::Lattice<AliasInfo> *> operands,\n+    ArrayRef<dataflow::Lattice<AliasInfo> *> results) {\n+  AliasInfo aliasInfo;\n+  bool pessimistic = true;\n+  if (maybeSharedAllocationOp(op)) {\n+    // These ops may allocate a new shared memory buffer.\n+    auto result = op->getResult(0);\n+    // XXX(Keren): the following ops are always aliasing for now\n+    if (isa<triton::gpu::ExtractSliceOp, triton::TransOp>(op)) {\n+      // extract_slice %src\n+      // trans %src\n+      aliasInfo = AliasInfo(operands[0]->getValue());\n+      pessimistic = false;\n+    } else if (isa<tensor::InsertSliceOp, triton::gpu::InsertSliceAsyncOp>(\n+                   op)) {\n+      // insert_slice_async %src, %dst, %index\n+      // insert_slice %src into %dst[%offsets]\n+      aliasInfo = AliasInfo(operands[1]->getValue());\n+      pessimistic = false;\n+    } else if (isSharedEncoding(result)) {\n+      aliasInfo.insert(result);\n+      pessimistic = false;\n+    }\n+  }\n+\n+  if (pessimistic) {\n+    return setAllToEntryStates(results);\n+  }\n+  // Join all lattice elements\n+  for (auto *result : results)\n+    propagateIfChanged(result, result->join(aliasInfo));\n+}\n+\n+AliasResult SharedMemoryAliasAnalysis::alias(Value lhs, Value rhs) {\n+  // TODO: implement\n+  return AliasResult::MayAlias;\n+}\n+\n+ModRefResult SharedMemoryAliasAnalysis::getModRef(Operation *op,\n+                                                  Value location) {\n+  // TODO: implement\n+  return ModRefResult::getModAndRef();\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "added", "additions": 509, "deletions": 0, "changes": 509, "file_content_changes": "@@ -0,0 +1,509 @@\n+#include \"triton/Analysis/Allocation.h\"\n+#include \"mlir/Analysis/DataFlowFramework.h\"\n+#include \"mlir/Analysis/Liveness.h\"\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+#include \"triton/Analysis/Alias.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+\n+#include <algorithm>\n+#include <limits>\n+#include <numeric>\n+\n+using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n+using ::mlir::triton::gpu::getOrder;\n+using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+using ::mlir::triton::gpu::SliceEncodingAttr;\n+\n+namespace mlir {\n+\n+//===----------------------------------------------------------------------===//\n+// Shared Memory Allocation Analysis\n+//===----------------------------------------------------------------------===//\n+namespace triton {\n+\n+// Bitwidth of pointers\n+constexpr int kPtrBitWidth = 64;\n+\n+static std::pair<SmallVector<unsigned>, SmallVector<unsigned>>\n+getCvtOrder(Attribute srcLayout, Attribute dstLayout) {\n+  auto srcMmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n+  auto srcDotLayout = srcLayout.dyn_cast<DotOperandEncodingAttr>();\n+  auto dstMmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>();\n+  auto dstDotLayout = dstLayout.dyn_cast<DotOperandEncodingAttr>();\n+  assert(!(srcMmaLayout && dstMmaLayout) &&\n+         \"Unexpected mma -> mma layout conversion\");\n+  // mma or dot layout does not have an order, so the order depends on the\n+  // layout of the other operand.\n+  auto inOrd = (srcMmaLayout || srcDotLayout) ? getOrder(dstLayout)\n+                                              : getOrder(srcLayout);\n+  auto outOrd = (dstMmaLayout || dstDotLayout) ? getOrder(srcLayout)\n+                                               : getOrder(dstLayout);\n+\n+  return {inOrd, outOrd};\n+}\n+\n+SmallVector<unsigned>\n+getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n+                             unsigned &outVec) {\n+  auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n+  auto dstTy = op.getResult().getType().cast<RankedTensorType>();\n+  Attribute srcLayout = srcTy.getEncoding();\n+  Attribute dstLayout = dstTy.getEncoding();\n+\n+  // MmaToDotShortcut doesn't use shared mem\n+  if (srcLayout.isa<MmaEncodingAttr>() &&\n+      dstLayout.isa<DotOperandEncodingAttr>())\n+    if (isMmaToDotShortcut(srcTy, dstTy))\n+      return {};\n+\n+  assert(srcLayout && dstLayout &&\n+         \"Unexpected layout in getScratchConfigForCvtLayout()\");\n+  auto [inOrd, outOrd] = getCvtOrder(srcLayout, dstLayout);\n+  unsigned srcContigPerThread = getContigPerThread(srcLayout)[inOrd[0]];\n+  unsigned dstContigPerThread = getContigPerThread(dstLayout)[outOrd[0]];\n+  // TODO: Fix the legacy issue that ourOrd[0] == 0 always means\n+  //       that we cannot do vectorization.\n+  inVec = outOrd[0] == 0 ? 1 : inOrd[0] == 0 ? 1 : srcContigPerThread;\n+  outVec = outOrd[0] == 0 ? 1 : dstContigPerThread;\n+\n+  auto srcShape = srcTy.getShape();\n+  auto dstShape = dstTy.getShape();\n+  auto srcShapePerCTA = getShapePerCTA(srcLayout, srcShape);\n+  auto dstShapePerCTA = getShapePerCTA(dstLayout, dstShape);\n+\n+  unsigned rank = dstTy.getRank();\n+  SmallVector<unsigned> paddedRepShape(rank);\n+  unsigned pad = std::max(inVec, outVec);\n+  for (unsigned d = 0; d < rank; ++d) {\n+    paddedRepShape[d] =\n+        std::max(std::min<unsigned>(srcTy.getShape()[d], srcShapePerCTA[d]),\n+                 std::min<unsigned>(dstTy.getShape()[d], dstShapePerCTA[d]));\n+  }\n+  if (rank == 1)\n+    return paddedRepShape;\n+  unsigned paddedDim = 1;\n+  if (auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>()) {\n+    paddedDim = dstBlockedLayout.getOrder()[0];\n+  }\n+  paddedRepShape[paddedDim] += pad;\n+  return paddedRepShape;\n+}\n+\n+// TODO: extend beyond scalars\n+SmallVector<unsigned> getScratchConfigForAtomicRMW(triton::AtomicRMWOp op) {\n+  SmallVector<unsigned> smemShape;\n+  if (op.getPtr().getType().isa<RankedTensorType>()) {\n+    // do nothing or just assert because shared memory is not used in tensor up\n+    // to now\n+  } else {\n+    // need only bytes for scalar\n+    // always vec = 1 and elemsPerThread = 1 for scalar?\n+    smemShape.push_back(1);\n+  }\n+  return smemShape;\n+}\n+\n+SmallVector<unsigned> getScratchConfigForAtomicCAS(triton::AtomicCASOp op) {\n+  return SmallVector<unsigned>{1};\n+}\n+\n+class AllocationAnalysis {\n+public:\n+  AllocationAnalysis(Operation *operation, Allocation *allocation)\n+      : operation(operation), allocation(allocation) {\n+    run();\n+  }\n+\n+private:\n+  using BufferT = Allocation::BufferT;\n+\n+  /// Value -> Liveness Range\n+  /// Use MapVector to ensure determinism.\n+  using BufferRangeMapT = llvm::MapVector<BufferT *, Interval<size_t>>;\n+  /// Nodes -> Nodes\n+  using GraphT = DenseMap<BufferT *, DenseSet<BufferT *>>;\n+\n+  void run() {\n+    getValuesAndSizes();\n+    resolveLiveness();\n+    computeOffsets();\n+  }\n+\n+  /// Initializes explicitly defined shared memory values for a given operation.\n+  void getExplicitValueSize(Operation *op) {\n+    // Values returned from scf.yield will not be allocated even though they\n+    // have the shared encoding.\n+    // For example: %a = scf.if -> yield\n+    // %a must be allocated elsewhere by other operations.\n+    // FIXME(Keren): extract and insert are always alias for now\n+    if (!maybeSharedAllocationOp(op) || maybeAliasOp(op)) {\n+      return;\n+    }\n+\n+    for (Value result : op->getResults()) {\n+      if (isSharedEncoding(result)) {\n+        // Bytes could be a different value once we support padding or other\n+        // allocation policies.\n+        auto tensorType = result.getType().dyn_cast<RankedTensorType>();\n+        auto bytes = tensorType.getNumElements() *\n+                     tensorType.getElementTypeBitWidth() / 8;\n+        allocation->addBuffer<BufferT::BufferKind::Explicit>(result, bytes);\n+      }\n+    }\n+  }\n+\n+  /// Initializes temporary shared memory for a given operation.\n+  void getScratchValueSize(Operation *op) {\n+    if (auto reduceOp = dyn_cast<triton::ReduceOp>(op)) {\n+      ReduceOpHelper helper(reduceOp);\n+      unsigned bytes = helper.getScratchSizeInBytes();\n+      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+    } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n+      auto srcTy = cvtLayout.getSrc().getType().cast<RankedTensorType>();\n+      auto dstTy = cvtLayout.getResult().getType().cast<RankedTensorType>();\n+      auto srcEncoding = srcTy.getEncoding();\n+      auto dstEncoding = dstTy.getEncoding();\n+      if (srcEncoding.isa<SharedEncodingAttr>() ||\n+          dstEncoding.isa<SharedEncodingAttr>()) {\n+        // Conversions from/to shared memory do not need scratch memory.\n+        return;\n+      }\n+      // ConvertLayoutOp with both input/output non-shared_layout\n+      // TODO: Besides of implementing ConvertLayoutOp via shared memory, it's\n+      //       also possible to realize it with other approaches in restricted\n+      //       conditions, such as warp-shuffle\n+      unsigned inVec = 0;\n+      unsigned outVec = 0;\n+      auto smemShape = getScratchConfigForCvtLayout(cvtLayout, inVec, outVec);\n+      unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n+                                       std::multiplies{});\n+      auto bytes =\n+          srcTy.getElementType().isa<triton::PointerType>()\n+              ? elems * kPtrBitWidth / 8\n+              : elems * std::max<int>(8, srcTy.getElementTypeBitWidth()) / 8;\n+      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+    } else if (auto atomicRMWOp = dyn_cast<triton::AtomicRMWOp>(op)) {\n+      auto value = op->getOperand(0);\n+      // only scalar requires scratch memory\n+      // make it explicit for readability\n+      if (value.getType().dyn_cast<RankedTensorType>()) {\n+        // nothing to do\n+      } else {\n+        auto smemShape = getScratchConfigForAtomicRMW(atomicRMWOp);\n+        unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n+                                         std::multiplies{});\n+        auto elemTy =\n+            value.getType().cast<triton::PointerType>().getPointeeType();\n+        auto bytes =\n+            elemTy.isa<triton::PointerType>()\n+                ? elems * kPtrBitWidth / 8\n+                : elems * std::max<int>(8, elemTy.getIntOrFloatBitWidth()) / 8;\n+        allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+      }\n+    } else if (auto atomicCASOp = dyn_cast<triton::AtomicCASOp>(op)) {\n+      auto value = op->getOperand(0);\n+      auto smemShape = getScratchConfigForAtomicCAS(atomicCASOp);\n+      unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n+                                       std::multiplies{});\n+      auto elemTy =\n+          value.getType().cast<triton::PointerType>().getPointeeType();\n+      auto bytes = elemTy.isa<triton::PointerType>()\n+                       ? elems * kPtrBitWidth / 8\n+                       : elems * elemTy.getIntOrFloatBitWidth() / 8;\n+      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+    }\n+  }\n+\n+  void getValueAlias(Value value, SharedMemoryAliasAnalysis &analysis) {\n+    dataflow::Lattice<AliasInfo> *latticeElement =\n+        analysis.getLatticeElement(value);\n+    if (latticeElement) {\n+      AliasInfo &info = latticeElement->getValue();\n+      if (!info.getAllocs().empty()) {\n+        for (auto alloc : info.getAllocs()) {\n+          allocation->addAlias(value, alloc);\n+        }\n+      }\n+    }\n+  }\n+\n+  /// Extract all shared memory values and their sizes\n+  void getValuesAndSizes() {\n+    // Get the alloc values\n+    operation->walk<WalkOrder::PreOrder>([&](Operation *op) {\n+      getExplicitValueSize(op);\n+      getScratchValueSize(op);\n+    });\n+    // Get the alias values\n+    std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n+    SharedMemoryAliasAnalysis *aliasAnalysis =\n+        solver->load<SharedMemoryAliasAnalysis>();\n+    if (failed(solver->initializeAndRun(operation))) {\n+      // TODO: return error instead of bailing out..\n+      llvm_unreachable(\"failed to run SharedMemoryAliasAnalysis\");\n+    }\n+    operation->walk<WalkOrder::PreOrder>([&](Operation *op) {\n+      for (auto operand : op->getOperands()) {\n+        getValueAlias(operand, *aliasAnalysis);\n+      }\n+      for (auto value : op->getResults()) {\n+        getValueAlias(value, *aliasAnalysis);\n+      }\n+    });\n+  }\n+\n+  /// Computes the liveness range of the allocated value.\n+  /// Each buffer is allocated only once.\n+  void resolveExplicitBufferLiveness(\n+      function_ref<Interval<size_t>(Value value)> getLiveness) {\n+    for (auto valueBufferIter : allocation->valueBuffer) {\n+      auto value = valueBufferIter.first;\n+      auto *buffer = valueBufferIter.second;\n+      bufferRange[buffer] = getLiveness(value);\n+    }\n+  }\n+\n+  /// Extends the liveness range by unionizing the liveness range of the aliased\n+  /// values because each allocated buffer could be an alias of others, if block\n+  /// arguments are involved.\n+  void resolveAliasBufferLiveness(\n+      function_ref<Interval<size_t>(Value value)> getLiveness) {\n+    for (auto aliasBufferIter : allocation->aliasBuffer) {\n+      auto value = aliasBufferIter.first;\n+      auto buffers = aliasBufferIter.second;\n+      auto range = getLiveness(value);\n+      for (auto *buffer : buffers) {\n+        auto minId = range.start();\n+        auto maxId = range.end();\n+        if (bufferRange.count(buffer)) {\n+          // Extend the allocated buffer's range\n+          minId = std::min(minId, bufferRange[buffer].start());\n+          maxId = std::max(maxId, bufferRange[buffer].end());\n+        }\n+        bufferRange[buffer] = Interval(minId, maxId);\n+      }\n+    }\n+  }\n+\n+  /// Computes the liveness range of scratched buffers.\n+  /// Some operations may have a temporary buffer that is not explicitly\n+  /// allocated, but is used to store intermediate results.\n+  void resolveScratchBufferLiveness(\n+      const DenseMap<Operation *, size_t> &operationId) {\n+    // Analyze liveness of scratch buffers\n+    for (auto opScratchIter : allocation->opScratch) {\n+      // Any scratch memory's live range is the current operation's live\n+      // range.\n+      auto *op = opScratchIter.first;\n+      auto *buffer = opScratchIter.second;\n+      bufferRange.insert({buffer, Interval(operationId.lookup(op),\n+                                           operationId.lookup(op) + 1)});\n+    }\n+  }\n+\n+  /// Resolves liveness of all values involved under the root operation.\n+  void resolveLiveness() {\n+    // Assign an ID to each operation using post-order traversal.\n+    // To achieve the correct liveness range, the parent operation's ID\n+    // should be greater than each of its child operation's ID .\n+    // Example:\n+    //     ...\n+    //     %5 = triton.convert_layout %4\n+    //     %6 = scf.for ... iter_args(%arg0 = %0) -> (i32) {\n+    //       %2 = triton.convert_layout %5\n+    //       ...\n+    //       scf.yield %arg0\n+    //     }\n+    // For example, %5 is defined in the parent region and used in\n+    // the child region, and is not passed as a block argument.\n+    // %6 should should have an ID greater than its child operations,\n+    // otherwise %5 liveness range ends before the child operation's liveness\n+    // range ends.\n+    DenseMap<Operation *, size_t> operationId;\n+    operation->walk<WalkOrder::PostOrder>(\n+        [&](Operation *op) { operationId[op] = operationId.size(); });\n+\n+    // Analyze liveness of explicit buffers\n+    Liveness liveness(operation);\n+    auto getValueLivenessRange = [&](Value value) {\n+      auto liveOperations = liveness.resolveLiveness(value);\n+      auto minId = std::numeric_limits<size_t>::max();\n+      auto maxId = std::numeric_limits<size_t>::min();\n+      std::for_each(liveOperations.begin(), liveOperations.end(),\n+                    [&](Operation *liveOp) {\n+                      if (operationId[liveOp] < minId) {\n+                        minId = operationId[liveOp];\n+                      }\n+                      if ((operationId[liveOp] + 1) > maxId) {\n+                        maxId = operationId[liveOp] + 1;\n+                      }\n+                    });\n+      return Interval(minId, maxId);\n+    };\n+\n+    resolveExplicitBufferLiveness(getValueLivenessRange);\n+    resolveAliasBufferLiveness(getValueLivenessRange);\n+    resolveScratchBufferLiveness(operationId);\n+  }\n+\n+  /// Computes the shared memory offsets for all related values.\n+  /// Paper: Algorithms for Compile-Time Memory Optimization\n+  /// (https://www.cs.utexas.edu/users/harrison/papers/compile-time.pdf)\n+  void computeOffsets() {\n+    SmallVector<BufferT *> buffers;\n+    for (auto bufferIter : bufferRange) {\n+      buffers.emplace_back(bufferIter.first);\n+    }\n+\n+    DenseMap<BufferT *, size_t> bufferStart;\n+    calculateStarts(buffers, bufferStart);\n+\n+    GraphT interference;\n+    buildInterferenceGraph(buffers, bufferStart, interference);\n+\n+    allocate(buffers, bufferStart, interference);\n+  }\n+\n+  /// Computes the initial shared memory offsets.\n+  void calculateStarts(const SmallVector<BufferT *> &buffers,\n+                       DenseMap<BufferT *, size_t> &bufferStart) {\n+    //  v = values in shared memory\n+    //  t = triplet of (size, start, end)\n+    //  shared memory space\n+    //  -\n+    //  |         *******t4\n+    //  | /|\\ v2 inserts t4, t5, and t6\n+    //  |  |\n+    //  | ******t5         ************t6\n+    //  | ^^^^^v2^^^^^^\n+    //  |  |      *********************t2\n+    //  | \\|/ v2 erases t1\n+    //  | ******t1 ^^^^^^^^^v1^^^^^^^^^ ************t3\n+    //  |---------------------------------------------| liveness range\n+    //    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ...\n+    // If the available triple's range is less than a given buffer range,\n+    // we won't know if there has been an overlap without using graph coloring.\n+    // Start -> Liveness Range\n+    using TripleMapT = std::multimap<size_t, Interval<size_t>>;\n+    TripleMapT tripleMap;\n+    tripleMap.insert(std::make_pair(0, Interval<size_t>()));\n+    SmallVector<BufferT *> xBuffers = buffers;\n+    while (!xBuffers.empty()) {\n+      auto tripleIt = tripleMap.begin();\n+      auto size = tripleIt->first;\n+      auto range = tripleIt->second;\n+      tripleMap.erase(tripleIt);\n+      auto bufferIt =\n+          std::find_if(xBuffers.begin(), xBuffers.end(), [&](auto *buffer) {\n+            auto xRange = bufferRange[buffer];\n+            bool res = xRange.intersects(range);\n+            for (auto val : tripleMap)\n+              res = res && !val.second.intersects(xRange);\n+            return res;\n+          });\n+      if (bufferIt != xBuffers.end()) {\n+        auto buffer = *bufferIt;\n+        auto xSize = buffer->size;\n+        auto xRange = bufferRange.lookup(buffer);\n+        bufferStart[buffer] = size;\n+        tripleMap.insert(\n+            {size + xSize, Interval{std::max(range.start(), xRange.start()),\n+                                    std::min(range.end(), xRange.end())}});\n+        // We could either insert (range.start, xRange.start) or (range.start,\n+        // xRange.end), both are correct and determine the potential buffer\n+        // offset, and the graph coloring algorithm will solve the interference,\n+        // if any\n+        if (range.start() < xRange.start())\n+          tripleMap.insert({size, Interval{range.start(), xRange.end()}});\n+        if (xRange.end() < range.end())\n+          tripleMap.insert({size, Interval{xRange.start(), range.end()}});\n+        xBuffers.erase(bufferIt);\n+      }\n+    }\n+  }\n+\n+  /// Builds a graph of all shared memory values. Edges are created between\n+  /// shared memory values that are overlapping.\n+  void buildInterferenceGraph(const SmallVector<BufferT *> &buffers,\n+                              const DenseMap<BufferT *, size_t> &bufferStart,\n+                              GraphT &interference) {\n+    for (auto x : buffers) {\n+      for (auto y : buffers) {\n+        if (x == y)\n+          continue;\n+        auto xStart = bufferStart.lookup(x);\n+        auto yStart = bufferStart.lookup(y);\n+        auto xSize = x->size;\n+        auto ySize = y->size;\n+        Interval xSizeRange = {xStart, xStart + xSize};\n+        Interval ySizeRange = {yStart, yStart + ySize};\n+        auto xOpRange = bufferRange.lookup(x);\n+        auto yOpRange = bufferRange.lookup(y);\n+        if (xOpRange.intersects(yOpRange) &&\n+            xSizeRange.intersects(ySizeRange)) {\n+          interference[x].insert(y);\n+        }\n+      }\n+    }\n+  }\n+\n+  /// Finalizes shared memory offsets considering interference.\n+  void allocate(const SmallVector<BufferT *> &buffers,\n+                const DenseMap<BufferT *, size_t> &bufferStart,\n+                const GraphT &interference) {\n+    // First-fit graph coloring\n+    // Neighbors are nodes that interfere with each other.\n+    // We color a node by finding the index of the first available\n+    // non-neighboring node or the first neighboring node without any color.\n+    // Nodes with the same color do not interfere with each other.\n+    DenseMap<BufferT *, int> colors;\n+    for (auto value : buffers) {\n+      colors[value] = (value == buffers[0]) ? 0 : -1;\n+    }\n+    SmallVector<bool> available(buffers.size());\n+    for (auto x : buffers) {\n+      std::fill(available.begin(), available.end(), true);\n+      for (auto y : interference.lookup(x)) {\n+        int color = colors[y];\n+        if (color >= 0) {\n+          available[color] = false;\n+        }\n+      }\n+      auto it = std::find(available.begin(), available.end(), true);\n+      colors[x] = std::distance(available.begin(), it);\n+    }\n+    // Finalize allocation\n+    // color0: [0, 7), [0, 8), [0, 15) -> [0, 7), [0, 8), [0, 15)\n+    // color1: [7, 9) -> [0 + 1 * 15, 9 + 1 * 15) -> [15, 24)\n+    // color2: [8, 12) -> [8 + 2 * 15, 12 + 2 * 15) -> [38, 42)\n+    // TODO(Keren): We are wasting memory here.\n+    // Nodes with color2 can actually start with 24.\n+    for (auto x : buffers) {\n+      size_t adj = 0;\n+      for (auto y : interference.lookup(x)) {\n+        adj = std::max(adj, bufferStart.lookup(y) + y->size);\n+      }\n+      x->offset = bufferStart.lookup(x) + colors.lookup(x) * adj;\n+      allocation->sharedMemorySize =\n+          std::max(allocation->sharedMemorySize, x->offset + x->size);\n+    }\n+  }\n+\n+private:\n+  Operation *operation;\n+  Allocation *allocation;\n+  BufferRangeMapT bufferRange;\n+};\n+} // namespace triton\n+\n+void Allocation::run() { triton::AllocationAnalysis(getOperation(), this); }\n+\n+} // namespace mlir"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "added", "additions": 961, "deletions": 0, "changes": 961, "file_content_changes": "@@ -0,0 +1,961 @@\n+#include \"mlir/Analysis/DataFlowFramework.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"llvm/Support/raw_ostream.h\"\n+\n+#include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+\n+namespace mlir {\n+\n+// Function for extended Euclidean Algorithm\n+static int64_t gcdImpl(int64_t a, int64_t b, int64_t *x, int64_t *y) {\n+  // Base Case\n+  if (a == 0) {\n+    *x = 0;\n+    *y = 1;\n+    return b;\n+  }\n+  int64_t x1, y1; // To store results of recursive call\n+  int64_t gcd = gcdImpl(b % a, a, &x1, &y1);\n+  // Update x and y using results of\n+  // recursive call\n+  *x = y1 - (b / a) * x1;\n+  *y = x1;\n+  return gcd;\n+}\n+\n+static int64_t gcd(int64_t a, int64_t b) {\n+  if (a == 0)\n+    return b;\n+  if (b == 0)\n+    return a;\n+  int64_t x, y;\n+  return gcdImpl(a, b, &x, &y);\n+}\n+\n+static constexpr int log2Int(int64_t num) {\n+  return (num > 1) ? 1 + log2Int(num / 2) : 0;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// AxisInfo\n+//===----------------------------------------------------------------------===//\n+\n+template <class T>\n+void AxisInfo::initPessimisticStateFromFunc(int argNumber, T funcOp,\n+                                            DimVectorT *contiguity,\n+                                            DimVectorT *divisibility,\n+                                            DimVectorT *constancy) {\n+  // liast of attributes that we care about\n+  SmallVector<std::pair<DimVectorT *, std::string>> retVecs;\n+  retVecs.push_back({contiguity, \"tt.contiguity\"});\n+  retVecs.push_back({divisibility, \"tt.divisibility\"});\n+  retVecs.push_back({constancy, \"tt.constancy\"});\n+  // initialize attributes one by one\n+  for (auto [vec, attrName] : retVecs) {\n+    Attribute attr = funcOp.getArgAttr(argNumber, attrName);\n+    if (auto int_attr = attr.dyn_cast_or_null<IntegerAttr>())\n+      *vec = DimVectorT(contiguity->size(), int_attr.getValue().getZExtValue());\n+    if (auto dense_attr = attr.dyn_cast_or_null<DenseElementsAttr>()) {\n+      auto vals = dense_attr.getValues<int>();\n+      *vec = DimVectorT(vals.begin(), vals.end());\n+    }\n+  }\n+}\n+\n+AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n+  auto rank = 1;\n+  if (TensorType ty = value.getType().dyn_cast<TensorType>())\n+    rank = ty.getRank();\n+\n+  DimVectorT knownContiguity(rank, 1);\n+  DimVectorT knownDivisibility(rank, 1);\n+  DimVectorT knownConstancy(rank, 1);\n+\n+  BlockArgument blockArg = value.dyn_cast<BlockArgument>();\n+\n+  if (blockArg && blockArg.getOwner()->isEntryBlock()) {\n+    Operation *op = blockArg.getOwner()->getParentOp();\n+    if (auto fun = dyn_cast<triton::FuncOp>(op))\n+      initPessimisticStateFromFunc(blockArg.getArgNumber(), fun,\n+                                   &knownContiguity, &knownDivisibility,\n+                                   &knownConstancy);\n+    // llvm codegen check alignment to generate vector load/store\n+    // would be nice if this wasn't the case\n+    else if (auto fun = dyn_cast<LLVM::LLVMFuncOp>(op))\n+      initPessimisticStateFromFunc(blockArg.getArgNumber(), fun,\n+                                   &knownContiguity, &knownDivisibility,\n+                                   &knownConstancy);\n+    else {\n+      // Derive the divisibility of the induction variable only when\n+      // the step and the lower bound are both constants\n+      if (auto forOp = dyn_cast<scf::ForOp>(op)) {\n+        if (blockArg == forOp.getInductionVar()) {\n+          if (auto lowerBound =\n+                  forOp.getLowerBound().getDefiningOp<arith::ConstantOp>()) {\n+            if (auto step =\n+                    forOp.getStep().getDefiningOp<arith::ConstantOp>()) {\n+              auto lowerBoundVal = lowerBound.getValue()\n+                                       .cast<IntegerAttr>()\n+                                       .getValue()\n+                                       .getZExtValue();\n+              auto stepVal =\n+                  step.getValue().cast<IntegerAttr>().getValue().getZExtValue();\n+              auto k = gcd(lowerBoundVal, stepVal);\n+              if (k != 0)\n+                knownDivisibility = DimVectorT(rank, k);\n+            }\n+          }\n+        }\n+      }\n+    }\n+  } else if (Operation *op = value.getDefiningOp()) {\n+    if (Attribute attr = op->getAttr(\"tt.divisibility\")) {\n+      auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+      knownDivisibility = DimVectorT(vals.begin(), vals.end());\n+    }\n+    if (Attribute attr = op->getAttr(\"tt.contiguity\")) {\n+      auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+      knownContiguity = DimVectorT(vals.begin(), vals.end());\n+    }\n+    if (Attribute attr = op->getAttr(\"tt.constancy\")) {\n+      auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+      knownConstancy = DimVectorT(vals.begin(), vals.end());\n+    }\n+  }\n+\n+  return AxisInfo(knownContiguity, knownDivisibility, knownConstancy);\n+}\n+\n+// The gcd of both arguments for each dimension\n+AxisInfo AxisInfo::join(const AxisInfo &lhs, const AxisInfo &rhs) {\n+  // If one argument is not initialized, return the other.\n+  if (lhs.getRank() == 0)\n+    return rhs;\n+  if (rhs.getRank() == 0)\n+    return lhs;\n+  DimVectorT contiguity;\n+  DimVectorT divisibility;\n+  DimVectorT constancy;\n+  for (auto d = 0; d < lhs.getRank(); ++d) {\n+    contiguity.push_back(gcd(lhs.getContiguity(d), rhs.getContiguity(d)));\n+    divisibility.push_back(gcd(lhs.getDivisibility(d), rhs.getDivisibility(d)));\n+    constancy.push_back(gcd(lhs.getConstancy(d), rhs.getConstancy(d)));\n+  }\n+  std::optional<int64_t> constantValue;\n+  if (lhs.getConstantValue().has_value() &&\n+      rhs.getConstantValue().has_value() &&\n+      lhs.getConstantValue() == rhs.getConstantValue())\n+    constantValue = lhs.getConstantValue();\n+  return AxisInfo(contiguity, divisibility, constancy, constantValue);\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// AxisInfoVisitor\n+//===----------------------------------------------------------------------===//\n+\n+template <typename OpTy>\n+class CastOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    return operands[0]->getValue();\n+  }\n+};\n+\n+class MakeRangeOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::MakeRangeOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::MakeRangeOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(triton::MakeRangeOp op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto start = op.getStart();\n+    auto end = op.getEnd();\n+    return AxisInfo(/*contiguity=*/{end - start},\n+                    /*divisibility=*/{highestPowOf2Divisor(start)},\n+                    /*constancy=*/{1});\n+  }\n+};\n+\n+template <typename OpTy>\n+class ConstantOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto intAttr = op.getValue().template dyn_cast<IntegerAttr>();\n+    auto boolAttr = op.getValue().template dyn_cast<BoolAttr>();\n+    if (intAttr || boolAttr) {\n+      int64_t value{};\n+      if (intAttr)\n+        value = intAttr.getValue().getZExtValue();\n+      else\n+        value = boolAttr.getValue() ? 1 : 0;\n+      return AxisInfo(/*contiguity=*/{1},\n+                      /*divisibility=*/{highestPowOf2Divisor(value)},\n+                      /*constancy=*/{1},\n+                      /*knownConstantValue=*/{value});\n+    }\n+    // TODO: generalize to dense attr\n+    auto splatAttr = op.getValue().template dyn_cast<SplatElementsAttr>();\n+    if (splatAttr && splatAttr.getElementType().isIntOrIndex()) {\n+      int64_t value = splatAttr.template getSplatValue<APInt>().getZExtValue();\n+      TensorType ty = splatAttr.getType().template cast<TensorType>();\n+      return AxisInfo(\n+          /*contiguity=*/AxisInfo::DimVectorT(ty.getRank(), 1),\n+          /*divisibility=*/\n+          AxisInfo::DimVectorT(ty.getRank(), highestPowOf2Divisor(value)),\n+          /*constancy=*/\n+          AxisInfo::DimVectorT(ty.getShape().begin(), ty.getShape().end()),\n+          /*knownConstantValue=*/{value});\n+    }\n+    return AxisInfo();\n+  }\n+};\n+\n+template <typename OpTy>\n+class AddSubOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    return std::max(gcd(lhs.getConstancy(dim), rhs.getContiguity(dim)),\n+                    gcd(lhs.getContiguity(dim), rhs.getConstancy(dim)));\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    // lhs = k * d_lhs = k * k' * gcd(d_lhs, d_rhs)\n+    // rhs = p * d_rhs = p * p' * gcd(d_lhs, d_rhs)\n+    // lhs + rhs = k * d_lhs + p * d_rhs = (k * d_lhs + p * d_rhs) *\n+    // gcd(d_lhs, d_rhs)\n+    auto elemSize = 1;\n+    if constexpr (std::is_same_v<OpTy, triton::AddPtrOp>) {\n+      //  %ptr = addptr %lhs, %rhs\n+      // is equivalent to\n+      //  %0 = mul %lhs, %elemSize\n+      //  %ptr = add %0, %rhs\n+      elemSize = std::max<unsigned int>(\n+          1, triton::getPointeeBitWidth(op.getPtr().getType()) / 8);\n+    }\n+    return gcd(lhs.getDivisibility(dim), rhs.getDivisibility(dim) * elemSize);\n+  }\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value()) {\n+      if constexpr (std::is_same_v<OpTy, arith::AddIOp> ||\n+                    std::is_same_v<OpTy, triton::AddPtrOp> ||\n+                    std::is_same_v<OpTy, LLVM::AddOp>) {\n+        return {lhs.getConstantValue().value() +\n+                rhs.getConstantValue().value()};\n+      } else if constexpr (std::is_same_v<OpTy, arith::SubIOp>) {\n+        return {lhs.getConstantValue().value() -\n+                rhs.getConstantValue().value()};\n+      }\n+    }\n+    return {};\n+  }\n+};\n+\n+class MulIOpAxisInfoVisitor final : public BinaryOpVisitorImpl<arith::MulIOp> {\n+public:\n+  using BinaryOpVisitorImpl<arith::MulIOp>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(arith::MulIOp op, const AxisInfo &lhs,\n+                        const AxisInfo &rhs, int dim) override {\n+    // lhs * 1 = lhs\n+    auto lhsContiguity =\n+        rhs.getConstantValue().has_value() && rhs.getConstantValue() == 1\n+            ? lhs.getContiguity(dim)\n+            : 1;\n+    // 1 * rhs = rhs\n+    auto rhsContiguity =\n+        lhs.getConstantValue().has_value() && lhs.getConstantValue() == 1\n+            ? rhs.getContiguity(dim)\n+            : 1;\n+    return std::max(lhsContiguity, rhsContiguity);\n+  }\n+\n+  int64_t getConstancy(arith::MulIOp op, const AxisInfo &lhs,\n+                       const AxisInfo &rhs, int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  int64_t getDivisibility(arith::MulIOp op, const AxisInfo &lhs,\n+                          const AxisInfo &rhs, int dim) override {\n+    // lhs = k * d_lhs\n+    // rhs = p * d_rhs\n+    // lhs * rhs = k * d_lhs * p * d_rhs = k * p * d_lhs * d_rhs\n+    return lhs.getDivisibility(dim) * rhs.getDivisibility(dim);\n+  }\n+\n+  std::optional<int64_t> getConstantValue(arith::MulIOp op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() * rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class DivOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    // lhs / 1 = lhs\n+    return rhs.getConstantValue().has_value() &&\n+                   rhs.getConstantValue().value() == 1\n+               ? lhs.getContiguity(dim)\n+               : 1;\n+  }\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return BinaryOpVisitorImpl<OpTy>::getConstancy(op, lhs, rhs, dim);\n+    auto shape = resTy.getShape();\n+    // Case 1: both lhs and rhs are constants.\n+    auto constancy = gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+    // Case 2: lhs contiguous, rhs constant.\n+    // lhs: d_lhs * k, d_lhs * k + 1, ..., d_lhs * k + n\n+    // rhs: d_rhs * p, d_rhs * p, ..., d_rhs * p\n+    // lhs / rhs = d_lhs * k / (d_rhs * p), (d_lhs * k + 1) / (d_rhs * p),\n+    // ..., (d_lhs * k + n) / (d_rhs * p)\n+    // Because d_lhs % d_rhs = 0 || d_rhs % d_lhs = 0,\n+    // the minimal constancy is gcd(d_lhs, d_rhs).\n+    // Since gcd(d_lhs, d_rhs) maybe > len(lhs),\n+    // we need to use another gcd to get the actual constancy.\n+    if (AxisInfoVisitor::isContiguousDim(lhs, shape, dim) &&\n+        AxisInfoVisitor::isConstantDim(rhs, shape, dim)) {\n+      constancy = std::max(constancy, gcd(lhs.getContiguity(dim),\n+                                          gcd(lhs.getDivisibility(dim),\n+                                              rhs.getDivisibility(dim))));\n+    }\n+    return constancy;\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    // Case 1: lhs is 0\n+    if (lhs.getConstantValue().has_value() &&\n+        lhs.getConstantValue().value() == 0)\n+      return lhs.getDivisibility(dim);\n+    // Case 2: rhs is 1\n+    if (rhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().value() == 1)\n+      return lhs.getDivisibility(dim);\n+    // otherwise: return 1\n+    return 1;\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() / rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class RemOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return BinaryOpVisitorImpl<OpTy>::getContiguity(op, lhs, rhs, dim);\n+    auto shape = resTy.getShape();\n+    int64_t contiguity = 1;\n+    // lhs contiguous, rhs constant\n+    // lhs: d_lhs * k, d_lhs * k + 1, ..., d_lhs * k + n\n+    // rhs: d_rhs * p, d_rhs * p, ..., d_rhs * p\n+    // lhs % rhs = d_lhs * k % (d_rhs * p), (d_lhs * k + 1) % (d_rhs * p),\n+    // ..., (d_lhs * k + n) % (d_rhs * p)\n+    // Because d_lhs % d_rhs = 0 || d_rhs % d_lhs = 0,\n+    // The minimal contiguity is gcd(d_lhs, d_rhs).\n+    // Since gcd(d_lhs, d_rhs) maybe > len(lhs),\n+    // we need to use another gcd to get the actual contiguity.\n+    if (AxisInfoVisitor::isContiguousDim(lhs, shape, dim) &&\n+        AxisInfoVisitor::isConstantDim(rhs, shape, dim)) {\n+      contiguity = std::max(contiguity, gcd(lhs.getContiguity(dim),\n+                                            gcd(lhs.getDivisibility(dim),\n+                                                rhs.getDivisibility(dim))));\n+    }\n+    return contiguity;\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    // lhs: d_lhs * k = gcd(d_lhs, d_rhs) * k' * k = gcd(d_lhs, d_rhs) * k''\n+    // rhs: d_rhs * p = gcd(d_lhs, d_rhs) * p' * p = gcd(d_lhs, d_rhs) * p''\n+    // lhs = gcd(d_lhs, d_rhs) * k'' = gcd(d_lhs, d_rhs) * d + r\n+    // r must be divisible by gcd(d_lhs, d_rhs)\n+    return gcd(lhs.getDivisibility(dim), rhs.getDivisibility(dim));\n+  };\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return BinaryOpVisitorImpl<OpTy>::getConstancy(op, lhs, rhs, dim);\n+    auto shape = resTy.getShape();\n+    // lhs % 1 = 0\n+    return rhs.getConstantValue().has_value() &&\n+                   rhs.getConstantValue().value() == 1\n+               ? shape[dim]\n+               : gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() % rhs.getConstantValue().value()};\n+    else if (rhs.getConstantValue().has_value() &&\n+             rhs.getConstantValue().value() == 1)\n+      return {0};\n+    return {};\n+  }\n+};\n+\n+class SplatOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::SplatOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::SplatOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(triton::SplatOp op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    Type _retTy = *op->result_type_begin();\n+    TensorType retTy = _retTy.cast<TensorType>();\n+    AxisInfo opInfo = operands[0]->getValue();\n+    AxisInfo::DimVectorT contiguity;\n+    AxisInfo::DimVectorT divisibility;\n+    AxisInfo::DimVectorT constancy;\n+    for (int d = 0; d < retTy.getRank(); ++d) {\n+      contiguity.push_back(1);\n+      divisibility.push_back(opInfo.getDivisibility(0));\n+      constancy.push_back(retTy.getShape()[d]);\n+    }\n+    return AxisInfo(contiguity, divisibility, constancy,\n+                    operands[0]->getValue().getConstantValue());\n+  }\n+};\n+\n+class ExpandDimsOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::ExpandDimsOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::ExpandDimsOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(triton::ExpandDimsOp op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    AxisInfo opInfo = operands[0]->getValue();\n+    AxisInfo::DimVectorT contiguity = opInfo.getContiguity();\n+    AxisInfo::DimVectorT divisibility = opInfo.getDivisibility();\n+    AxisInfo::DimVectorT constancy = opInfo.getConstancy();\n+    contiguity.insert(contiguity.begin() + op.getAxis(), 1);\n+    divisibility.insert(divisibility.begin() + op.getAxis(), 1);\n+    constancy.insert(constancy.begin() + op.getAxis(), 1);\n+    return AxisInfo(contiguity, divisibility, constancy,\n+                    operands[0]->getValue().getConstantValue());\n+  }\n+};\n+\n+class BroadcastOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::BroadcastOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::BroadcastOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(triton::BroadcastOp op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    Type _retTy = *op->result_type_begin();\n+    Type _opTy = *op->operand_type_begin();\n+    TensorType retTy = _retTy.cast<TensorType>();\n+    TensorType opTy = _opTy.cast<TensorType>();\n+    ArrayRef<int64_t> retShape = retTy.getShape();\n+    ArrayRef<int64_t> opShape = opTy.getShape();\n+    AxisInfo opInfo = operands[0]->getValue();\n+    AxisInfo::DimVectorT contiguity;\n+    AxisInfo::DimVectorT divisibility;\n+    AxisInfo::DimVectorT constancy;\n+    for (int d = 0; d < retTy.getRank(); ++d) {\n+      contiguity.push_back(opShape[d] == 1 ? 1 : opInfo.getContiguity(d));\n+      divisibility.push_back(opInfo.getDivisibility(d));\n+      constancy.push_back(opShape[d] == 1 ? retShape[d]\n+                                          : opInfo.getConstancy(d));\n+    }\n+    return AxisInfo(contiguity, divisibility, constancy,\n+                    operands[0]->getValue().getConstantValue());\n+  }\n+};\n+\n+template <typename OpTy>\n+class CmpOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return AxisInfo();\n+    auto shape = resTy.getShape();\n+    short rank = resTy.getRank();\n+    auto lhsInfo = operands[0]->getValue();\n+    auto rhsInfo = operands[1]->getValue();\n+\n+    AxisInfo::DimVectorT contiguity, divisibility, constancy;\n+    std::optional<int64_t> constantValue;\n+    for (short d = 0; d < rank; ++d) {\n+      int64_t constHint = 1;\n+      if (lhsInfo.getConstantValue().has_value() &&\n+          rhsInfo.getConstantValue().has_value()) {\n+        constHint = lhsInfo.getConstancy(d);\n+        constantValue =\n+            compare(getPredicate(op), lhsInfo.getConstantValue().value(),\n+                    rhsInfo.getConstantValue().value())\n+                ? 1\n+                : 0;\n+      } else {\n+        // Case 1: lhs and rhs are both partial constants\n+        constHint = gcd(lhsInfo.getConstancy(d), rhsInfo.getConstancy(d));\n+        // Case 2: lhs all constant, rhs all contiguous\n+        // NOTE:\n+        // lhs: 4 4 4 4\n+        // rhs: 4 5 6 7\n+        // lhs ge rhs: 1, 0, 0, 0\n+        // Case 3: lhs all contiguous, rhs all constant\n+        // NOTE\n+        // lhs: 4 5 6 7\n+        // rhs: 4 4 4 4\n+        // lhs sle rhs: 1, 0, 0, 0\n+        if (/*Case 2=*/(\n+                notGePredicate(getPredicate(op)) &&\n+                (AxisInfoVisitor::isConstantDim(lhsInfo, shape, d) &&\n+                 AxisInfoVisitor::isContiguousDim(rhsInfo, shape, d))) ||\n+            /*Case 3=*/(notLePredicate(getPredicate(op)) &&\n+                        (AxisInfoVisitor::isContiguousDim(lhsInfo, shape, d) &&\n+                         AxisInfoVisitor::isConstantDim(rhsInfo, shape, d)))) {\n+          constHint = std::max(constHint, gcd(lhsInfo.getContiguity(d),\n+                                              gcd(lhsInfo.getDivisibility(d),\n+                                                  rhsInfo.getDivisibility(d))));\n+        }\n+      }\n+\n+      constancy.push_back(constHint);\n+      divisibility.push_back(1);\n+      contiguity.push_back(1);\n+    }\n+\n+    return AxisInfo(contiguity, divisibility, constancy, constantValue);\n+  }\n+\n+private:\n+  static arith::CmpIPredicate getPredicate(triton::gpu::CmpIOp op) {\n+    return op.getPredicate();\n+  }\n+\n+  static arith::CmpIPredicate getPredicate(arith::CmpIOp op) {\n+    return op.getPredicate();\n+  }\n+\n+  static bool notGePredicate(arith::CmpIPredicate predicate) {\n+    return predicate != arith::CmpIPredicate::sge &&\n+           predicate != arith::CmpIPredicate::uge;\n+  }\n+\n+  static bool notLePredicate(arith::CmpIPredicate predicate) {\n+    return predicate != arith::CmpIPredicate::sle &&\n+           predicate != arith::CmpIPredicate::ule;\n+  }\n+\n+  static bool compare(arith::CmpIPredicate predicate, int64_t lhs,\n+                      int64_t rhs) {\n+    switch (predicate) {\n+    case arith::CmpIPredicate::eq:\n+      return lhs == rhs;\n+    case arith::CmpIPredicate::ne:\n+      return lhs != rhs;\n+    case arith::CmpIPredicate::slt:\n+      return lhs < rhs;\n+    case arith::CmpIPredicate::sle:\n+      return lhs <= rhs;\n+    case arith::CmpIPredicate::sgt:\n+      return lhs > rhs;\n+    case arith::CmpIPredicate::sge:\n+      return lhs >= rhs;\n+    case arith::CmpIPredicate::ult:\n+      return (uint64_t)lhs < (uint64_t)rhs;\n+    case arith::CmpIPredicate::ule:\n+      return (uint64_t)lhs <= (uint64_t)rhs;\n+    case arith::CmpIPredicate::ugt:\n+      return (uint64_t)lhs > (uint64_t)rhs;\n+    case arith::CmpIPredicate::uge:\n+      return (uint64_t)lhs >= (uint64_t)rhs;\n+    default:\n+      break;\n+    }\n+    llvm_unreachable(\"unknown comparison predicate\");\n+  }\n+};\n+\n+template <typename OpTy>\n+class SelectOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return AxisInfo();\n+    auto shape = resTy.getShape();\n+    auto rank = shape.size();\n+    auto condConstancy = operands[0]->getValue().getConstancy();\n+    auto lhsInfo = operands[1]->getValue();\n+    auto rhsInfo = operands[2]->getValue();\n+\n+    AxisInfo::DimVectorT contiguity, divisibility, constancy;\n+    std::optional<int64_t> constantValue;\n+    if (operands[0]->getValue().getConstantValue().has_value()) {\n+      if (operands[0]->getValue().getConstantValue() == 0) {\n+        contiguity = rhsInfo.getContiguity();\n+        divisibility = rhsInfo.getDivisibility();\n+        constancy = rhsInfo.getConstancy();\n+        constantValue = rhsInfo.getConstantValue();\n+      } else {\n+        contiguity = lhsInfo.getContiguity();\n+        divisibility = lhsInfo.getDivisibility();\n+        constancy = lhsInfo.getConstancy();\n+        constantValue = lhsInfo.getConstantValue();\n+      }\n+    } else {\n+      for (auto d = 0; d < rank; ++d) {\n+        constancy.push_back(\n+            std::min(gcd(lhsInfo.getConstancy(d), condConstancy[d]),\n+                     gcd(rhsInfo.getConstancy(d), condConstancy[d])));\n+        divisibility.push_back(\n+            std::min(lhsInfo.getDivisibility(d), rhsInfo.getDivisibility(d)));\n+        contiguity.push_back(\n+            std::min(gcd(lhsInfo.getContiguity(d), condConstancy[d]),\n+                     gcd(rhsInfo.getContiguity(d), condConstancy[d])));\n+      }\n+      if (lhsInfo.getConstantValue().has_value() &&\n+          rhsInfo.getConstantValue().has_value() &&\n+          lhsInfo.getConstantValue() == rhsInfo.getConstantValue())\n+        constantValue = lhsInfo.getConstantValue();\n+    }\n+\n+    return AxisInfo(contiguity, divisibility, constancy, constantValue);\n+  }\n+};\n+\n+template <typename OpTy>\n+class LogicalOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value()) {\n+      if constexpr (std::is_same<OpTy, arith::AndIOp>::value) {\n+        return {lhs.getConstantValue().value() &\n+                rhs.getConstantValue().value()};\n+      } else if constexpr (std::is_same<OpTy, arith::OrIOp>::value) {\n+        return {lhs.getConstantValue().value() |\n+                rhs.getConstantValue().value()};\n+      } else if constexpr (std::is_same<OpTy, arith::XOrIOp>::value) {\n+        return {lhs.getConstantValue().value() ^\n+                rhs.getConstantValue().value()};\n+      }\n+    }\n+    return {};\n+  }\n+};\n+\n+class ShLIOpAxisInfoVisitor final : public BinaryOpVisitorImpl<arith::ShLIOp> {\n+public:\n+  using BinaryOpVisitorImpl<arith::ShLIOp>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(arith::ShLIOp op, const AxisInfo &lhs,\n+                        const AxisInfo &rhs, int dim) override {\n+    if (rhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().value() == 0)\n+      return lhs.getContiguity(dim);\n+    else\n+      return 1;\n+  }\n+\n+  int64_t getDivisibility(arith::ShLIOp op, const AxisInfo &lhs,\n+                          const AxisInfo &rhs, int dim) override {\n+    auto shift = rhs.getConstantValue().has_value()\n+                     ? rhs.getConstantValue().value()\n+                     : rhs.getDivisibility(dim);\n+    auto numBits = log2Int(lhs.getDivisibility(dim));\n+    auto maxBits = log2Int(highestPowOf2Divisor<int64_t>(0));\n+    // Make sure the return value doesn't exceed highestPowOf2Divisor<int64>(0)\n+    if (shift + numBits > maxBits)\n+      return highestPowOf2Divisor<int64_t>(0);\n+    return lhs.getDivisibility(dim) << shift;\n+  }\n+\n+  int64_t getConstancy(arith::ShLIOp op, const AxisInfo &lhs,\n+                       const AxisInfo &rhs, int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(arith::ShLIOp op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() << rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class ShROpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    if (rhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().value() == 0)\n+      return lhs.getContiguity(dim);\n+    else\n+      return 1;\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    if (rhs.getConstantValue().has_value())\n+      return std::max<int64_t>(1, lhs.getDivisibility(dim) /\n+                                      (1 << rhs.getConstantValue().value()));\n+    else\n+      return std::max<int64_t>(1, lhs.getDivisibility(dim) /\n+                                      (1 << rhs.getDivisibility(dim)));\n+  }\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() >> rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class MaxMinOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(OpTy op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    auto lhsInfo = operands[0]->getValue();\n+    auto rhsInfo = operands[1]->getValue();\n+    std::optional<int64_t> constantValue;\n+    if (lhsInfo.getConstantValue().has_value() &&\n+        rhsInfo.getConstantValue().has_value()) {\n+      if constexpr (std::is_same_v<OpTy, arith::MaxSIOp> ||\n+                    std::is_same_v<OpTy, arith::MaxUIOp>) {\n+        constantValue = {std::max(lhsInfo.getConstantValue().value(),\n+                                  rhsInfo.getConstantValue().value())};\n+      } else if constexpr (std::is_same_v<OpTy, arith::MinSIOp> ||\n+                           std::is_same_v<OpTy, arith::MinUIOp>) {\n+        constantValue = {std::min(lhsInfo.getConstantValue().value(),\n+                                  rhsInfo.getConstantValue().value())};\n+      }\n+    }\n+    auto rank = lhsInfo.getRank();\n+    return AxisInfo(/*knownContiguity=*/AxisInfo::DimVectorT(rank, 1),\n+                    /*knownDivisibility=*/AxisInfo::DimVectorT(rank, 1),\n+                    /*knownConstancy=*/AxisInfo::DimVectorT(rank, 1),\n+                    /*constantValue=*/constantValue);\n+  }\n+};\n+\n+//===----------------------------------------------------------------------===//\n+// AxisInfoAnalysis\n+//===----------------------------------------------------------------------===//\n+\n+AxisInfoAnalysis::AxisInfoAnalysis(DataFlowSolver &solver)\n+    : dataflow::SparseDataFlowAnalysis<dataflow::Lattice<AxisInfo>>(solver) {\n+  // UnrealizedConversionCast:\n+  // This is needed by TritonGPUToLLVM, to get AxisInfo when the graph is\n+  // in the process of a PartialConversion, where UnrealizedConversionCast\n+  // may exist\n+  visitors.append<CastOpAxisInfoVisitor<arith::ExtSIOp>,\n+                  CastOpAxisInfoVisitor<arith::ExtUIOp>,\n+                  CastOpAxisInfoVisitor<arith::TruncIOp>,\n+                  CastOpAxisInfoVisitor<arith::IndexCastOp>,\n+                  CastOpAxisInfoVisitor<triton::PtrToIntOp>,\n+                  CastOpAxisInfoVisitor<triton::IntToPtrOp>,\n+                  CastOpAxisInfoVisitor<triton::gpu::ConvertLayoutOp>,\n+                  CastOpAxisInfoVisitor<mlir::UnrealizedConversionCastOp>,\n+                  CastOpAxisInfoVisitor<triton::BitcastOp>>();\n+  // TODO: Remove rules for LLVM::ConstantOp, LLVM::AddOp\n+  // when scf.for supports integers induction variable\n+  visitors.append<MakeRangeOpAxisInfoVisitor>();\n+  visitors.append<ConstantOpAxisInfoVisitor<arith::ConstantOp>,\n+                  ConstantOpAxisInfoVisitor<LLVM::ConstantOp>>();\n+  visitors.append<AddSubOpAxisInfoVisitor<triton::AddPtrOp>,\n+                  AddSubOpAxisInfoVisitor<arith::AddIOp>,\n+                  AddSubOpAxisInfoVisitor<arith::SubIOp>,\n+                  AddSubOpAxisInfoVisitor<LLVM::AddOp>>();\n+  visitors.append<MulIOpAxisInfoVisitor>();\n+  visitors.append<DivOpAxisInfoVisitor<arith::DivSIOp>,\n+                  DivOpAxisInfoVisitor<arith::DivUIOp>>();\n+  visitors.append<RemOpAxisInfoVisitor<arith::RemSIOp>,\n+                  RemOpAxisInfoVisitor<arith::RemUIOp>>();\n+  visitors.append<BroadcastOpAxisInfoVisitor>();\n+  visitors.append<SplatOpAxisInfoVisitor>();\n+  visitors.append<ExpandDimsOpAxisInfoVisitor>();\n+  visitors.append<CmpOpAxisInfoVisitor<arith::CmpIOp>,\n+                  CmpOpAxisInfoVisitor<triton::gpu::CmpIOp>>();\n+  visitors.append<LogicalOpAxisInfoVisitor<arith::AndIOp>,\n+                  LogicalOpAxisInfoVisitor<arith::OrIOp>,\n+                  LogicalOpAxisInfoVisitor<arith::XOrIOp>>();\n+  visitors.append<SelectOpAxisInfoVisitor<mlir::arith::SelectOp>,\n+                  SelectOpAxisInfoVisitor<triton::gpu::SelectOp>>();\n+  visitors.append<ShLIOpAxisInfoVisitor, ShROpAxisInfoVisitor<arith::ShRUIOp>,\n+                  ShROpAxisInfoVisitor<arith::ShRSIOp>>();\n+  visitors.append<MaxMinOpAxisInfoVisitor<arith::MaxSIOp>,\n+                  MaxMinOpAxisInfoVisitor<arith::MaxUIOp>,\n+                  MaxMinOpAxisInfoVisitor<arith::MinSIOp>,\n+                  MaxMinOpAxisInfoVisitor<arith::MinUIOp>>();\n+}\n+\n+void AxisInfoAnalysis::visitOperation(\n+    Operation *op, ArrayRef<const dataflow::Lattice<AxisInfo> *> operands,\n+    ArrayRef<dataflow::Lattice<AxisInfo> *> results) {\n+  // TODO: For sure not the right way to do this\n+  // but why is scf.if not initialized otherwise?\n+  for (auto op : operands)\n+    if (op->getValue().getRank() == 0)\n+      setToEntryState((dataflow::Lattice<AxisInfo> *)op);\n+  AxisInfo curr = visitors.apply(op, operands);\n+  if (curr.getRank() == 0)\n+    return setAllToEntryStates(results);\n+  // override with hint\n+  auto newContiguity = curr.getContiguity();\n+  auto newDivisibility = curr.getDivisibility();\n+  auto newConstancy = curr.getConstancy();\n+  if (Attribute attr = op->getAttr(\"tt.contiguity\")) {\n+    auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+    newContiguity = AxisInfo::DimVectorT(vals.begin(), vals.end());\n+  }\n+  if (Attribute attr = op->getAttr(\"tt.divisibility\")) {\n+    auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+    newDivisibility = AxisInfo::DimVectorT(vals.begin(), vals.end());\n+  }\n+  if (Attribute attr = op->getAttr(\"tt.constancy\")) {\n+    auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n+    newConstancy = AxisInfo::DimVectorT(vals.begin(), vals.end());\n+  }\n+  curr = mlir::AxisInfo(newContiguity, newDivisibility, newConstancy,\n+                        curr.getConstantValue());\n+  // join all lattice elements\n+  for (auto *result : results)\n+    propagateIfChanged(result, result->join(curr));\n+}\n+\n+unsigned AxisInfoAnalysis::getPtrContiguity(Value ptr) {\n+  auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+  if (!tensorTy)\n+    return 1;\n+  auto layout = tensorTy.getEncoding();\n+  auto shape = tensorTy.getShape();\n+\n+  // Here order should be ordered by contiguous first, so the first element\n+  // should have the largest contiguous.\n+  auto order = triton::gpu::getOrder(layout);\n+  unsigned align = getPtrAlignment(ptr);\n+\n+  unsigned contigPerThread = triton::gpu::getSizePerThread(layout)[order[0]];\n+  contigPerThread = std::min(align, contigPerThread);\n+  contigPerThread = std::min<unsigned>(shape[order[0]], contigPerThread);\n+\n+  return contigPerThread;\n+}\n+\n+unsigned AxisInfoAnalysis::getPtrAlignment(Value ptr) {\n+  auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+  if (!tensorTy)\n+    return 1;\n+  dataflow::Lattice<AxisInfo> *latticeElement = getLatticeElement(ptr);\n+  if (!latticeElement)\n+    return 1;\n+  auto axisInfo = latticeElement->getValue();\n+  auto layout = tensorTy.getEncoding();\n+  auto order = triton::gpu::getOrder(layout);\n+  auto maxMultipleBytes = axisInfo.getDivisibility(order[0]);\n+  auto maxContig = axisInfo.getContiguity(order[0]);\n+  auto elemNumBits = triton::getPointeeBitWidth(tensorTy);\n+  auto elemNumBytes = std::max<unsigned>(elemNumBits / 8, 1);\n+  auto maxMultiple = std::max<int64_t>(maxMultipleBytes / elemNumBytes, 1);\n+  unsigned alignment = std::min(maxMultiple, maxContig);\n+  return alignment;\n+}\n+\n+unsigned AxisInfoAnalysis::getMaskAlignment(Value mask) {\n+  auto tensorTy = mask.getType().dyn_cast<RankedTensorType>();\n+  if (!tensorTy)\n+    return 1;\n+  dataflow::Lattice<AxisInfo> *latticeElement = getLatticeElement(mask);\n+  if (!latticeElement)\n+    return 1;\n+  auto maskAxis = latticeElement->getValue();\n+  auto maskOrder = triton::gpu::getOrder(tensorTy.getEncoding());\n+  auto alignment = std::max<unsigned>(maskAxis.getConstancy(maskOrder[0]), 1);\n+  return alignment;\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Analysis/CMakeLists.txt", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -0,0 +1,14 @@\n+add_mlir_library(TritonAnalysis\n+  AxisInfo.cpp\n+  Allocation.cpp\n+  Membar.cpp\n+  Alias.cpp\n+  Utility.cpp\n+\n+  DEPENDS\n+  TritonTableGen\n+  TritonGPUAttrDefsIncGen\n+\n+  LINK_LIBS PUBLIC\n+  MLIRAnalysis\n+)"}, {"filename": "lib/Analysis/Membar.cpp", "status": "added", "additions": 151, "deletions": 0, "changes": 151, "file_content_changes": "@@ -0,0 +1,151 @@\n+#include \"triton/Analysis/Membar.h\"\n+#include \"triton/Analysis/Alias.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+#include <deque>\n+\n+namespace mlir {\n+\n+void MembarAnalysis::run() {\n+  auto *operation = allocation->getOperation();\n+  OpBuilder builder(operation);\n+  resolve(operation, &builder);\n+}\n+\n+void MembarAnalysis::resolve(Operation *operation, OpBuilder *builder) {\n+  // Initialize the blockList\n+  std::deque<Block *> blockList;\n+  operation->walk<WalkOrder::PreOrder>([&](Block *block) {\n+    for (auto &op : block->getOperations()) {\n+      // Check if the operation belongs to scf dialect, if so, we need to\n+      // throw an error\n+      if (op.getDialect()->getNamespace() == \"scf\") {\n+        llvm::report_fatal_error(\n+            \"scf dialect is not supported in membar. Please lower it \"\n+            \"to cf dialect first.\");\n+        return;\n+      }\n+    }\n+    if (block->isEntryBlock())\n+      blockList.emplace_back(block);\n+  });\n+\n+  // A fixed point algorithm\n+  while (!blockList.empty()) {\n+    auto *block = blockList.front();\n+    blockList.pop_front();\n+    // Make a copy of the inputblockInfo but not update\n+    auto inputBlockInfo = inputBlockInfoMap.lookup(block);\n+    SmallVector<Block *> successors;\n+    for (auto &op : block->getOperations()) {\n+      if (op.hasTrait<OpTrait::IsTerminator>()) {\n+        visitTerminator(&op, successors);\n+      } else {\n+        update(&op, &inputBlockInfo, builder);\n+      }\n+    }\n+    // Get the reference because we want to update if it changed\n+    if (outputBlockInfoMap.count(block) &&\n+        inputBlockInfo == outputBlockInfoMap[block]) {\n+      // If we have seen the block before and the inputBlockInfo is the same as\n+      // the outputBlockInfo, we skip the successors\n+      continue;\n+    }\n+    // Update the current block\n+    outputBlockInfoMap[block].join(inputBlockInfo);\n+    // Update the successors\n+    for (auto *successor : successors) {\n+      inputBlockInfoMap[successor].join(outputBlockInfoMap[block]);\n+      blockList.emplace_back(successor);\n+    }\n+  }\n+}\n+\n+void MembarAnalysis::visitTerminator(Operation *op,\n+                                     SmallVector<Block *> &successors) {\n+  if (auto branchInterface = dyn_cast<BranchOpInterface>(op)) {\n+    Block *parentBlock = branchInterface->getBlock();\n+    for (Block *successor : parentBlock->getSuccessors()) {\n+      successors.push_back(successor);\n+    }\n+    return;\n+  }\n+  // Otherwise, it could be a return op\n+  if (isa<triton::ReduceReturnOp>(op) || isa<triton::ReturnOp>(op)) {\n+    return;\n+  }\n+  llvm_unreachable(\"Unknown terminator encountered in membar analysis\");\n+}\n+\n+void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n+                            OpBuilder *builder) {\n+  if (isa<triton::gpu::ExtractSliceOp>(op) ||\n+      isa<triton::gpu::AllocTensorOp>(op) || isa<triton::TransOp>(op)) {\n+    // alloc is an allocation op without memory write.\n+    // FIXME(Keren): extract_slice is always alias for now\n+    return;\n+  }\n+\n+  if (isa<gpu::BarrierOp>(op)) {\n+    // If the current op is a barrier, we sync previous reads and writes\n+    blockInfo->sync();\n+    return;\n+  }\n+\n+  if (isa<triton::gpu::AsyncWaitOp>(op) &&\n+      !isa<gpu::BarrierOp>(op->getNextNode())) {\n+    // If the current op is an async wait and the next op is not a barrier we\n+    // insert a barrier op and sync\n+    blockInfo->sync();\n+    OpBuilder::InsertionGuard g(*builder);\n+    builder->setInsertionPointAfter(op);\n+    builder->create<gpu::BarrierOp>(op->getLoc());\n+    blockInfo->sync();\n+    return;\n+  }\n+\n+  BlockInfo curBlockInfo;\n+  for (Value value : op->getOperands()) {\n+    for (auto bufferId : allocation->getBufferIds(value)) {\n+      if (bufferId != Allocation::InvalidBufferId) {\n+        if (isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n+            isa<tensor::InsertSliceOp>(op)) {\n+          // FIXME(Keren): insert_slice and insert_slice_async are always\n+          // alias for now\n+          curBlockInfo.syncWriteBuffers.insert(bufferId);\n+        } else {\n+          // ConvertLayoutOp: shared memory -> registers\n+          curBlockInfo.syncReadBuffers.insert(bufferId);\n+        }\n+      }\n+    }\n+  }\n+  for (Value value : op->getResults()) {\n+    // ConvertLayoutOp: registers -> shared memory\n+    auto bufferId = allocation->getBufferId(value);\n+    if (bufferId != Allocation::InvalidBufferId) {\n+      curBlockInfo.syncWriteBuffers.insert(bufferId);\n+    }\n+  }\n+  // Scratch buffer is considered as both shared memory write & read\n+  auto bufferId = allocation->getBufferId(op);\n+  if (bufferId != Allocation::InvalidBufferId) {\n+    curBlockInfo.syncWriteBuffers.insert(bufferId);\n+    curBlockInfo.syncReadBuffers.insert(bufferId);\n+  }\n+\n+  if (blockInfo->isIntersected(curBlockInfo, allocation)) {\n+    OpBuilder::InsertionGuard g(*builder);\n+    builder->setInsertionPoint(op);\n+    builder->create<gpu::BarrierOp>(op->getLoc());\n+    blockInfo->sync();\n+  }\n+  // Update the region info, even if barrier is inserted, we have to maintain\n+  // the current op's read/write buffers.\n+  blockInfo->join(curBlockInfo);\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Analysis/Utility.cpp", "status": "added", "additions": 393, "deletions": 0, "changes": 393, "file_content_changes": "@@ -0,0 +1,393 @@\n+#include \"triton/Analysis/Utility.h\"\n+#include \"mlir/Analysis/DataFlow/ConstantPropagationAnalysis.h\"\n+#include \"mlir/Analysis/DataFlow/DeadCodeAnalysis.h\"\n+#include \"mlir/IR/Dialect.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include <deque>\n+\n+namespace mlir {\n+\n+bool ReduceOpHelper::isFastReduction() {\n+  return axis == triton::gpu::getOrder(getSrcLayout())[0];\n+}\n+\n+unsigned ReduceOpHelper::getInterWarpSize() {\n+  auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n+  unsigned sizeIntraWarps = getIntraWarpSize();\n+  return std::min(srcReduceDimSize / sizeIntraWarps,\n+                  triton::gpu::getWarpsPerCTA(getSrcLayout())[axis]);\n+}\n+\n+unsigned ReduceOpHelper::getIntraWarpSize() {\n+  auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n+  return std::min(srcReduceDimSize,\n+                  triton::gpu::getThreadsPerWarp(getSrcLayout())[axis]);\n+}\n+\n+unsigned ReduceOpHelper::getThreadsReductionAxis() {\n+  auto srcLayout = getSrcLayout();\n+  return triton::gpu::getThreadsPerWarp(srcLayout)[axis] *\n+         triton::gpu::getWarpsPerCTA(srcLayout)[axis];\n+}\n+\n+SmallVector<unsigned> ReduceOpHelper::getScratchConfigBasic() {\n+  auto smemShape = convertType<unsigned>(getSrcShape());\n+  smemShape[axis] = std::min(smemShape[axis], getThreadsReductionAxis());\n+  return smemShape;\n+}\n+\n+SmallVector<SmallVector<unsigned>> ReduceOpHelper::getScratchConfigsFast() {\n+  SmallVector<SmallVector<unsigned>> smemShapes(3);\n+\n+  auto argLayout = getSrcLayout();\n+  auto argLayoutMma = argLayout.dyn_cast<triton::gpu::MmaEncodingAttr>();\n+  if (argLayoutMma && argLayoutMma.getVersionMajor() == 2 &&\n+      triton::gpu::getWarpsPerCTA(argLayout)[axis] == 1)\n+    return {{1, 1}, {1, 1}};\n+\n+  /// shared memory block0\n+  smemShapes[0] = convertType<unsigned>(getSrcShape());\n+  smemShapes[0][axis] = getInterWarpSize();\n+\n+  /// FIXME(Qingyi): This size is actually larger than required.\n+  /// shared memory block1:\n+  auto mod = op->getParentOfType<ModuleOp>();\n+  unsigned numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+  smemShapes[1].push_back(numWarps * 32);\n+\n+  return smemShapes;\n+}\n+\n+unsigned ReduceOpHelper::getScratchSizeInBytes() {\n+  unsigned elems = 0;\n+  if (isFastReduction()) {\n+    auto smemShapes = getScratchConfigsFast();\n+    for (const auto &smemShape : smemShapes)\n+      elems = std::max(elems, product<unsigned>(smemShape));\n+  } else {\n+    auto smemShape = getScratchConfigBasic();\n+    elems = product<unsigned>(smemShape);\n+  }\n+\n+  unsigned bytesPerElem = 0;\n+  for (const auto &ty : srcElementTypes) {\n+    bytesPerElem += ty.getIntOrFloatBitWidth() / 8;\n+  }\n+  return bytesPerElem * elems;\n+}\n+\n+bool ReduceOpHelper::isSupportedLayout() {\n+  auto srcLayout = getSrcLayout();\n+  if (srcLayout.isa<triton::gpu::BlockedEncodingAttr>()) {\n+    return true;\n+  }\n+  if (auto mmaLayout = srcLayout.dyn_cast<triton::gpu::MmaEncodingAttr>()) {\n+    if (mmaLayout.isAmpere()) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool isSharedEncoding(Value value) {\n+  auto type = value.getType();\n+  if (auto tensorType = type.dyn_cast<RankedTensorType>()) {\n+    auto encoding = tensorType.getEncoding();\n+    return encoding && encoding.isa<triton::gpu::SharedEncodingAttr>();\n+  }\n+  return false;\n+}\n+\n+bool maybeSharedAllocationOp(Operation *op) {\n+  // TODO(Keren): This function can be replaced by adding\n+  // MemoryEffectOpInterface. We can then use the MemoryEffectOpInterface to\n+  // query the memory effects of the op.\n+  auto *dialect = op->getDialect();\n+  return dialect &&\n+         (dialect->getTypeID() ==\n+              mlir::TypeID::get<triton::gpu::TritonGPUDialect>() ||\n+          dialect->getTypeID() == mlir::TypeID::get<triton::TritonDialect>() ||\n+          dialect->getTypeID() == mlir::TypeID::get<arith::ArithDialect>() ||\n+          dialect->getTypeID() == mlir::TypeID::get<tensor::TensorDialect>());\n+}\n+\n+bool maybeAliasOp(Operation *op) {\n+  return isa<triton::gpu::ExtractSliceOp>(op) || isa<triton::TransOp>(op) ||\n+         isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n+         isa<tensor::InsertSliceOp>(op);\n+}\n+\n+bool supportMMA(triton::DotOp op, int version) {\n+  // Refer to mma section for the data type supported by Volta and Hopper\n+  // Tensor Core in\n+  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-fragment-mma-884-f16\n+  auto aElemTy = op.getA().getType().cast<RankedTensorType>().getElementType();\n+  auto bElemTy = op.getB().getType().cast<RankedTensorType>().getElementType();\n+  if (aElemTy.isF32() && bElemTy.isF32()) {\n+    return op.getAllowTF32() && version >= 2;\n+  }\n+  return supportMMA(op.getA(), version) && supportMMA(op.getB(), version);\n+}\n+\n+bool supportMMA(Value value, int version) {\n+  // Tell whether a DotOp support HMMA by the operand type(either $a or $b).\n+  // We cannot get both the operand types(in TypeConverter), here we assume the\n+  // types of both the operands are identical here.\n+  assert((version == 1 || version == 2) &&\n+         \"Unexpected MMA layout version found\");\n+  auto elemTy = value.getType().cast<RankedTensorType>().getElementType();\n+  return elemTy.isF16() || elemTy.isBF16() ||\n+         (elemTy.isF32() && version >= 2) ||\n+         (elemTy.isInteger(8) && version >= 2);\n+}\n+\n+Type getElementType(Value value) {\n+  auto type = value.getType();\n+  if (auto tensorType = type.dyn_cast<RankedTensorType>())\n+    return tensorType.getElementType();\n+  return type;\n+}\n+\n+std::string getValueOperandName(Value value, AsmState &state) {\n+  std::string opName;\n+  llvm::raw_string_ostream ss(opName);\n+  value.printAsOperand(ss, state);\n+  return opName;\n+}\n+\n+bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy) {\n+  // dot_op<opIdx=0, parent=#mma> = #mma\n+  // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n+  auto srcLayout = srcTy.getEncoding();\n+  auto dstLayout = dstTy.getEncoding();\n+  auto mmaLayout = srcLayout.cast<triton::gpu::MmaEncodingAttr>();\n+  auto dotOperandLayout = dstLayout.cast<triton::gpu::DotOperandEncodingAttr>();\n+  return mmaLayout.getVersionMajor() == 2 &&\n+         mmaLayout.getWarpsPerCTA()[1] == 1 &&\n+         dotOperandLayout.getOpIdx() == 0 &&\n+         dotOperandLayout.getParent() == mmaLayout &&\n+         !srcTy.getElementType().isF32();\n+}\n+\n+bool isSingleValue(Value value) {\n+  // Don't consider load as expensive if it is loading a scalar.\n+  if (auto tensorTy = value.getType().dyn_cast<RankedTensorType>())\n+    return tensorTy.getNumElements() == 1;\n+  // TODO: Handle other cases.\n+  // For example, when ptr is a tensor of single value.\n+  // It means that ptr is a resultant of broadcast or generated through\n+  // a chain of broadcast and other operations.\n+  // Rematerialize it without considering contiguous memory access pattern is\n+  // fine.\n+  return true;\n+}\n+\n+namespace {\n+\n+/// A data structure similar to SetVector but maintains\n+/// a deque instead of a vector to allow for efficient\n+/// push_back and pop_front operations.\n+/// Using SetVector doesn't suffice our needs because\n+/// it only pushes and pops from the back.\n+/// For example, if we have a queue like this:\n+/// 0->4 1->2->3\n+///    ^--------\n+/// where 3 depends on 4, once we pop 3, we found\n+/// 4 is not ready, so we check 2 and push 3 back\n+/// to the queue.\n+struct DFSSubgraphState {\n+  DFSSubgraphState() : set(), deque() {}\n+  DenseSet<Operation *> set;\n+  std::deque<Operation *> deque;\n+\n+  bool push_back(Operation *op) {\n+    if (set.insert(op).second) {\n+      deque.push_back(op);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  Operation *pop_front() {\n+    Operation *op = deque.front();\n+    deque.pop_front();\n+    set.erase(op);\n+    return op;\n+  }\n+\n+  bool empty() { return deque.empty(); }\n+};\n+\n+/// DFS post-order implementation that maintains a global count to work across\n+/// multiple invocations, to help implement topological sort on multi-root DAGs.\n+/// We traverse all operations but only record the ones that appear in\n+/// `toSort` for the final result.\n+struct DFSState {\n+  DFSState(const SetVector<Operation *> &set) : toSort(set), seen() {}\n+  const SetVector<Operation *> &toSort;\n+  SmallVector<Operation *, 16> topologicalCounts;\n+  DenseSet<Operation *> seen;\n+\n+  /// We mark each op as ready if all its operands are seen. If an op is ready,\n+  /// we add it to the queue. Otherwise, we keep adding its operands to the\n+  /// ancestors set.\n+  void addToReadyQueue(Operation *op, DFSSubgraphState &subGraph,\n+                       SmallVector<Operation *, 4> &readyQueue) {\n+    bool ready = true;\n+    for (Value operand : op->getOperands()) {\n+      auto def = operand.getDefiningOp();\n+      if (def && !seen.count(def)) {\n+        subGraph.push_back(def);\n+        ready = false;\n+      }\n+    }\n+    if (ready)\n+      readyQueue.push_back(op);\n+  }\n+};\n+\n+void dfsPostorder(Operation *root, DFSState *state) {\n+  DFSSubgraphState subGraph;\n+  subGraph.push_back(root);\n+  SmallVector<Operation *> ops;\n+  while (!subGraph.empty()) {\n+    // Nodes in the ready queue are ready to be processed.\n+    // Meaning that either their operands are all seen or they have null\n+    // operands.\n+    SmallVector<Operation *, 4> readyQueue;\n+    auto *current = subGraph.pop_front();\n+    state->addToReadyQueue(current, subGraph, readyQueue);\n+    while (!readyQueue.empty()) {\n+      Operation *current = readyQueue.pop_back_val();\n+      if (!state->seen.insert(current).second)\n+        continue;\n+      ops.push_back(current);\n+      for (Value result : current->getResults()) {\n+        for (Operation *op : result.getUsers())\n+          state->addToReadyQueue(op, subGraph, readyQueue);\n+      }\n+      for (Region &region : current->getRegions()) {\n+        for (Operation &op : region.getOps())\n+          state->addToReadyQueue(&op, subGraph, readyQueue);\n+      }\n+    }\n+  }\n+\n+  for (Operation *op : llvm::reverse(ops)) {\n+    if (state->toSort.count(op) > 0)\n+      state->topologicalCounts.push_back(op);\n+  }\n+}\n+\n+} // namespace\n+\n+SetVector<Operation *>\n+multiRootTopologicalSort(const SetVector<Operation *> &toSort) {\n+  if (toSort.empty()) {\n+    return toSort;\n+  }\n+\n+  // Run from each root with global count and `seen` set.\n+  DFSState state(toSort);\n+  for (auto *s : toSort) {\n+    assert(toSort.count(s) == 1 && \"NYI: multi-sets not supported\");\n+    dfsPostorder(s, &state);\n+  }\n+\n+  // Reorder and return.\n+  SetVector<Operation *> res;\n+  for (auto it = state.topologicalCounts.rbegin(),\n+            eit = state.topologicalCounts.rend();\n+       it != eit; ++it) {\n+    res.insert(*it);\n+  }\n+  return res;\n+}\n+\n+SetVector<Operation *> multiRootGetSlice(Operation *op,\n+                                         TransitiveFilter backwardFilter,\n+                                         TransitiveFilter forwardFilter) {\n+  SetVector<Operation *> slice;\n+  slice.insert(op);\n+\n+  unsigned currentIndex = 0;\n+  SetVector<Operation *> backwardSlice;\n+  SetVector<Operation *> forwardSlice;\n+  while (currentIndex != slice.size()) {\n+    auto *currentOp = (slice)[currentIndex];\n+    // Compute and insert the backwardSlice starting from currentOp.\n+    backwardSlice.clear();\n+    getBackwardSlice(currentOp, &backwardSlice, backwardFilter);\n+    slice.insert(backwardSlice.begin(), backwardSlice.end());\n+\n+    // Compute and insert the forwardSlice starting from currentOp.\n+    forwardSlice.clear();\n+    getForwardSlice(currentOp, &forwardSlice, forwardFilter);\n+    slice.insert(forwardSlice.begin(), forwardSlice.end());\n+    ++currentIndex;\n+  }\n+  return multiRootTopologicalSort(slice);\n+}\n+\n+namespace {\n+// Copied from TestDeadCodeAnalysis.cpp, because some dead code analysis\n+// interacts with constant propagation, but SparseConstantPropagation\n+// doesn't seem to be sufficient.\n+class ConstantAnalysis : public DataFlowAnalysis {\n+public:\n+  using DataFlowAnalysis::DataFlowAnalysis;\n+\n+  LogicalResult initialize(Operation *top) override {\n+    WalkResult result = top->walk([&](Operation *op) {\n+      if (failed(visit(op)))\n+        return WalkResult::interrupt();\n+      return WalkResult::advance();\n+    });\n+    return success(!result.wasInterrupted());\n+  }\n+\n+  LogicalResult visit(ProgramPoint point) override {\n+    Operation *op = point.get<Operation *>();\n+    Attribute value;\n+    if (matchPattern(op, m_Constant(&value))) {\n+      auto *constant = getOrCreate<dataflow::Lattice<dataflow::ConstantValue>>(\n+          op->getResult(0));\n+      propagateIfChanged(constant, constant->join(dataflow::ConstantValue(\n+                                       value, op->getDialect())));\n+      return success();\n+    }\n+    // Dead code analysis requires every operands has initialized ConstantValue\n+    // state before it is visited.\n+    // https://github.com/llvm/llvm-project/blob/2ec1aba2b69faa1de5f71832a48e25aa3b5d5314/mlir/lib/Analysis/DataFlow/DeadCodeAnalysis.cpp#L322\n+    // That's why we need to set all operands to unknown constants.\n+    setAllToUnknownConstants(op->getResults());\n+    for (Region &region : op->getRegions()) {\n+      for (Block &block : region.getBlocks())\n+        setAllToUnknownConstants(block.getArguments());\n+    }\n+    return success();\n+  }\n+\n+private:\n+  /// Set all given values as not constants.\n+  void setAllToUnknownConstants(ValueRange values) {\n+    dataflow::ConstantValue unknownConstant(nullptr, nullptr);\n+    for (Value value : values) {\n+      auto *constant =\n+          getOrCreate<dataflow::Lattice<dataflow::ConstantValue>>(value);\n+      propagateIfChanged(constant, constant->join(unknownConstant));\n+    }\n+  }\n+};\n+} // namespace\n+\n+std::unique_ptr<DataFlowSolver> createDataFlowSolver() {\n+  auto solver = std::make_unique<DataFlowSolver>();\n+  solver->load<dataflow::DeadCodeAnalysis>();\n+  solver->load<ConstantAnalysis>();\n+  return solver;\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/CMakeLists.txt", "status": "added", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -0,0 +1,5 @@\n+# add_subdirectory(codegen)\n+add_subdirectory(Analysis)\n+add_subdirectory(Conversion)\n+add_subdirectory(Dialect)\n+add_subdirectory(Target)"}, {"filename": "lib/Conversion/CMakeLists.txt", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -0,0 +1,2 @@\n+add_subdirectory(TritonToTritonGPU)\n+add_subdirectory(TritonGPUToLLVM)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "added", "additions": 45, "deletions": 0, "changes": 45, "file_content_changes": "@@ -0,0 +1,45 @@\n+add_mlir_conversion_library(TritonGPUToLLVM\n+    TritonGPUToLLVM.cpp\n+    GCNAsmFormat.cpp\n+    PTXAsmFormat.cpp\n+    TritonGPUToLLVMPass.cpp\n+    ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp\n+    ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp\n+    ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp\n+    ConvertLayoutOpToLLVM.cpp\n+    DotOpToLLVM/FMA.cpp\n+    DotOpToLLVM/MMAv1.cpp\n+    DotOpToLLVM/MMAv2.cpp\n+    DotOpToLLVM.cpp\n+    ElementwiseOpToLLVM.cpp\n+    LoadStoreOpToLLVM.cpp\n+    TritonGPUToLLVM.cpp\n+    TritonGPUToLLVMPass.cpp\n+    PTXAsmFormat.cpp\n+    ReduceOpToLLVM.cpp\n+    Utility.cpp\n+    TypeConverter.cpp\n+    ViewOpToLLVM.cpp\n+\n+    ADDITIONAL_HEADER_DIRS\n+    ${PROJECT_SOURCE_DIR}/include/triton/Conversion/TritonGPUToLLVM\n+    ${PROJECT_BINARY_DIR}/include/triton/Conversion/TritonGPUToLLVM\n+\n+    DEPENDS\n+    TritonGPUConversionPassIncGen\n+\n+    LINK_COMPONENTS\n+    Core\n+\n+    LINK_LIBS PUBLIC\n+    MLIRIR\n+    MLIRPass\n+    MLIRGPUOps\n+    MLIRGPUToNVVMTransforms\n+    MLIRGPUToROCDLTransforms\n+    MLIRGPUTransforms\n+    TritonAnalysis\n+    TritonIR\n+    TritonGPUIR\n+    TritonGPUTransforms\n+)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "added", "additions": 684, "deletions": 0, "changes": 684, "file_content_changes": "@@ -0,0 +1,684 @@\n+#include \"ConvertLayoutOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getStridesFromShapeAndOrder;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n+using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getOrder;\n+using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+// Forward declarations\n+\n+namespace SharedToDotOperandMMAv1 {\n+using CoordTy = SmallVector<Value>;\n+using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+\n+SmallVector<CoordTy> getMNCoords(Value thread,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 ArrayRef<unsigned int> wpt,\n+                                 const MmaEncodingAttr &mmaLayout,\n+                                 ArrayRef<int64_t> shape, bool isARow,\n+                                 bool isBRow, bool isAVec4, bool isBVec4);\n+\n+Value convertLayout(int opIdx, Value tensor, const SharedMemoryObject &smemObj,\n+                    Value thread, Location loc,\n+                    TritonGPUToLLVMTypeConverter *typeConverter,\n+                    ConversionPatternRewriter &rewriter, Type resultTy);\n+\n+} // namespace SharedToDotOperandMMAv1\n+\n+namespace SharedToDotOperandMMAv2 {\n+Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,\n+                    Location loc, Value tensor,\n+                    DotOperandEncodingAttr bEncoding,\n+                    const SharedMemoryObject &smemObj,\n+                    TritonGPUToLLVMTypeConverter *typeConverter, Value thread);\n+}\n+\n+namespace SharedToDotOperandFMA {\n+Value convertLayout(int opIdx, Value B, Value llB, BlockedEncodingAttr dLayout,\n+                    Value thread, Location loc,\n+                    TritonGPUToLLVMTypeConverter *typeConverter,\n+                    ConversionPatternRewriter &rewriter);\n+}\n+\n+struct ConvertLayoutOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {\n+public:\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::gpu::ConvertLayoutOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    Attribute srcLayout = srcTy.getEncoding();\n+    Attribute dstLayout = dstTy.getEncoding();\n+    if (isaDistributedLayout(srcLayout) &&\n+        dstLayout.isa<SharedEncodingAttr>()) {\n+      return lowerDistributedToShared(op, adaptor, rewriter);\n+    }\n+    if (srcLayout.isa<SharedEncodingAttr>() &&\n+        dstLayout.isa<DotOperandEncodingAttr>()) {\n+      return lowerSharedToDotOperand(op, adaptor, rewriter);\n+    }\n+    if (isaDistributedLayout(srcLayout) && isaDistributedLayout(dstLayout)) {\n+      return lowerDistributedToDistributed(op, adaptor, rewriter);\n+    }\n+    if (srcLayout.isa<MmaEncodingAttr>() &&\n+        dstLayout.isa<DotOperandEncodingAttr>()) {\n+      return lowerMmaToDotOperand(op, adaptor, rewriter);\n+    }\n+    // TODO: to be implemented\n+    llvm_unreachable(\"unsupported layout conversion\");\n+    return failure();\n+  }\n+\n+private:\n+  SmallVector<Value> getMultiDimOffset(Attribute layout, Location loc,\n+                                       ConversionPatternRewriter &rewriter,\n+                                       unsigned elemId, RankedTensorType type,\n+                                       ArrayRef<unsigned> multiDimCTAInRepId,\n+                                       ArrayRef<unsigned> shapePerCTA) const {\n+    auto shape = type.getShape();\n+    unsigned rank = shape.size();\n+    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+      auto multiDimOffsetFirstElem =\n+          emitBaseIndexForLayout(loc, rewriter, blockedLayout, type);\n+      SmallVector<Value> multiDimOffset(rank);\n+      SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n+          elemId, getSizePerThread(layout), getOrder(layout));\n+      for (unsigned d = 0; d < rank; ++d) {\n+        multiDimOffset[d] = add(multiDimOffsetFirstElem[d],\n+                                i32_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n+                                        multiDimElemId[d]));\n+      }\n+      return multiDimOffset;\n+    }\n+    if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+      unsigned dim = sliceLayout.getDim();\n+      auto parentEncoding = sliceLayout.getParent();\n+      auto parentShape = sliceLayout.paddedShape(shape);\n+      auto parentTy = RankedTensorType::get(parentShape, type.getElementType(),\n+                                            parentEncoding);\n+      auto multiDimOffsetParent =\n+          getMultiDimOffset(parentEncoding, loc, rewriter, elemId, parentTy,\n+                            sliceLayout.paddedShape(multiDimCTAInRepId),\n+                            sliceLayout.paddedShape(shapePerCTA));\n+      SmallVector<Value> multiDimOffset(rank);\n+      for (unsigned d = 0; d < rank + 1; ++d) {\n+        if (d == dim)\n+          continue;\n+        unsigned slicedD = d < dim ? d : (d - 1);\n+        multiDimOffset[slicedD] = multiDimOffsetParent[d];\n+      }\n+      return multiDimOffset;\n+    }\n+    if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+      SmallVector<Value> mmaColIdx(4);\n+      SmallVector<Value> mmaRowIdx(2);\n+      Value threadId = getThreadId(rewriter, loc);\n+      Value warpSize = i32_val(32);\n+      Value laneId = urem(threadId, warpSize);\n+      Value warpId = udiv(threadId, warpSize);\n+      // TODO: fix the bug in MMAEncodingAttr document\n+      SmallVector<Value> multiDimWarpId(2);\n+      multiDimWarpId[0] = urem(warpId, i32_val(mmaLayout.getWarpsPerCTA()[0]));\n+      multiDimWarpId[1] = udiv(warpId, i32_val(mmaLayout.getWarpsPerCTA()[0]));\n+      Value _1 = i32_val(1);\n+      Value _2 = i32_val(2);\n+      Value _4 = i32_val(4);\n+      Value _8 = i32_val(8);\n+      Value _16 = i32_val(16);\n+      if (mmaLayout.isAmpere()) {\n+        multiDimWarpId[0] = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n+        multiDimWarpId[1] = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n+        Value mmaGrpId = udiv(laneId, _4);\n+        Value mmaGrpIdP8 = add(mmaGrpId, _8);\n+        Value mmaThreadIdInGrp = urem(laneId, _4);\n+        Value mmaThreadIdInGrpM2 = mul(mmaThreadIdInGrp, _2);\n+        Value mmaThreadIdInGrpM2P1 = add(mmaThreadIdInGrpM2, _1);\n+        Value rowWarpOffset = mul(multiDimWarpId[0], _16);\n+        mmaRowIdx[0] = add(mmaGrpId, rowWarpOffset);\n+        mmaRowIdx[1] = add(mmaGrpIdP8, rowWarpOffset);\n+        Value colWarpOffset = mul(multiDimWarpId[1], _8);\n+        mmaColIdx[0] = add(mmaThreadIdInGrpM2, colWarpOffset);\n+        mmaColIdx[1] = add(mmaThreadIdInGrpM2P1, colWarpOffset);\n+      } else if (mmaLayout.isVolta()) {\n+        // Volta doesn't follow the pattern here.\"\n+      } else {\n+        llvm_unreachable(\"Unexpected MMALayout version\");\n+      }\n+\n+      assert(rank == 2);\n+      SmallVector<Value> multiDimOffset(rank);\n+      if (mmaLayout.isAmpere()) {\n+        multiDimOffset[0] = elemId < 2 ? mmaRowIdx[0] : mmaRowIdx[1];\n+        multiDimOffset[1] = elemId % 2 == 0 ? mmaColIdx[0] : mmaColIdx[1];\n+        multiDimOffset[0] = add(\n+            multiDimOffset[0], i32_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n+        multiDimOffset[1] = add(\n+            multiDimOffset[1], i32_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n+      } else if (mmaLayout.isVolta()) {\n+        auto [isARow, isBRow, isAVec4, isBVec4, _] =\n+            mmaLayout.decodeVoltaLayoutStates();\n+        auto coords = SharedToDotOperandMMAv1::getMNCoords(\n+            threadId, rewriter, mmaLayout.getWarpsPerCTA(), mmaLayout, shape,\n+            isARow, isBRow, isAVec4, isBVec4);\n+        return coords[elemId];\n+      } else {\n+        llvm_unreachable(\"Unexpected MMALayout version\");\n+      }\n+      return multiDimOffset;\n+    }\n+    llvm_unreachable(\"unexpected layout in getMultiDimOffset\");\n+  }\n+\n+  // shared memory rd/st for blocked or mma layout with data padding\n+  void processReplica(Location loc, ConversionPatternRewriter &rewriter,\n+                      bool stNotRd, RankedTensorType type,\n+                      ArrayRef<unsigned> numCTAsEachRep,\n+                      ArrayRef<unsigned> multiDimRepId, unsigned vec,\n+                      ArrayRef<unsigned> paddedRepShape,\n+                      ArrayRef<unsigned> outOrd, SmallVector<Value> &vals,\n+                      Value smemBase) const {\n+    auto accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n+    auto layout = type.getEncoding();\n+    auto rank = type.getRank();\n+    auto sizePerThread = getSizePerThread(layout);\n+    auto accumSizePerThread = product<unsigned>(sizePerThread);\n+    SmallVector<unsigned> numCTAs(rank);\n+    auto shapePerCTA = getShapePerCTA(layout, type.getShape());\n+    auto order = getOrder(layout);\n+    for (unsigned d = 0; d < rank; ++d) {\n+      numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n+    }\n+    auto elemTy = type.getElementType();\n+    bool isInt1 = elemTy.isInteger(1);\n+    bool isPtr = elemTy.isa<triton::PointerType>();\n+    auto llvmElemTyOrig = getTypeConverter()->convertType(elemTy);\n+    if (isInt1)\n+      elemTy = IntegerType::get(elemTy.getContext(), 8);\n+    else if (isPtr)\n+      elemTy = IntegerType::get(elemTy.getContext(), 64);\n+\n+    auto llvmElemTy = getTypeConverter()->convertType(elemTy);\n+\n+    for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n+      auto multiDimCTAInRepId =\n+          getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep, order);\n+      SmallVector<unsigned> multiDimCTAId(rank);\n+      for (const auto &it : llvm::enumerate(multiDimCTAInRepId)) {\n+        auto d = it.index();\n+        multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n+      }\n+\n+      auto linearCTAId =\n+          getLinearIndex<unsigned>(multiDimCTAId, numCTAs, order);\n+      // TODO: This is actually redundant index calculation, we should\n+      //       consider of caching the index calculation result in case\n+      //       of performance issue observed.\n+      for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n+        SmallVector<Value> multiDimOffset =\n+            getMultiDimOffset(layout, loc, rewriter, elemId, type,\n+                              multiDimCTAInRepId, shapePerCTA);\n+        Value offset =\n+            linearize(rewriter, loc, multiDimOffset, paddedRepShape, outOrd);\n+\n+        auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n+        Value ptr = gep(elemPtrTy, smemBase, offset);\n+        auto vecTy = vec_ty(llvmElemTy, vec);\n+        ptr = bitcast(ptr, ptr_ty(vecTy, 3));\n+        if (stNotRd) {\n+          Value valVec = undef(vecTy);\n+          for (unsigned v = 0; v < vec; ++v) {\n+            auto currVal = vals[elemId + linearCTAId * accumSizePerThread + v];\n+            if (isInt1)\n+              currVal = zext(llvmElemTy, currVal);\n+            else if (isPtr)\n+              currVal = ptrtoint(llvmElemTy, currVal);\n+            valVec = insert_element(vecTy, valVec, currVal, i32_val(v));\n+          }\n+          store(valVec, ptr);\n+        } else {\n+          Value valVec = load(ptr);\n+          for (unsigned v = 0; v < vec; ++v) {\n+            Value currVal = extract_element(llvmElemTy, valVec, i32_val(v));\n+            if (isInt1)\n+              currVal = icmp_ne(currVal,\n+                                rewriter.create<LLVM::ConstantOp>(\n+                                    loc, i8_ty, rewriter.getI8IntegerAttr(0)));\n+            else if (isPtr)\n+              currVal = inttoptr(llvmElemTyOrig, currVal);\n+            vals[elemId + linearCTAId * accumSizePerThread + v] = currVal;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  // The MMAV1's result is quite different from the existing \"Replica\"\n+  // structure, add a new simple but clear implementation for it to avoid\n+  // modifying the logic of the existing one.\n+  void processReplicaForMMAV1(Location loc, ConversionPatternRewriter &rewriter,\n+                              bool stNotRd, RankedTensorType type,\n+                              ArrayRef<unsigned> multiDimRepId, unsigned vec,\n+                              ArrayRef<unsigned> paddedRepShape,\n+                              ArrayRef<unsigned> outOrd,\n+                              SmallVector<Value> &vals, Value smemBase,\n+                              ArrayRef<int64_t> shape,\n+                              bool isDestMma = false) const {\n+    unsigned accumNumCTAsEachRep = 1;\n+    auto layout = type.getEncoding();\n+    MmaEncodingAttr mma = layout.dyn_cast<MmaEncodingAttr>();\n+    auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>();\n+    if (sliceLayout)\n+      mma = sliceLayout.getParent().cast<MmaEncodingAttr>();\n+\n+    auto order = getOrder(layout);\n+    auto rank = type.getRank();\n+    int accumSizePerThread = vals.size();\n+\n+    SmallVector<unsigned> numCTAs(rank, 1);\n+    SmallVector<unsigned> numCTAsEachRep(rank, 1);\n+    SmallVector<unsigned> shapePerCTA = getShapePerCTA(layout, shape);\n+    auto elemTy = type.getElementType();\n+\n+    int ctaId = 0;\n+\n+    auto multiDimCTAInRepId =\n+        getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep, order);\n+    SmallVector<unsigned> multiDimCTAId(rank);\n+    for (const auto &it : llvm::enumerate(multiDimCTAInRepId)) {\n+      auto d = it.index();\n+      multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n+    }\n+\n+    std::vector<std::pair<SmallVector<Value>, Value>> coord2valT(\n+        accumSizePerThread);\n+    bool needTrans = outOrd[0] != 0;\n+    if (sliceLayout || isDestMma)\n+      needTrans = false;\n+\n+    vec = needTrans ? 2 : 1;\n+    {\n+      // We need to transpose the coordinates and values here to enable vec=2\n+      // when store to smem.\n+      std::vector<std::pair<SmallVector<Value>, Value>> coord2val(\n+          accumSizePerThread);\n+      for (unsigned elemId = 0; elemId < accumSizePerThread; ++elemId) {\n+        // TODO[Superjomn]: Move the coordinate computation out of loop, it is\n+        // duplicate in Volta.\n+        SmallVector<Value> multiDimOffset =\n+            getMultiDimOffset(layout, loc, rewriter, elemId, type,\n+                              multiDimCTAInRepId, shapePerCTA);\n+        coord2val[elemId] = std::make_pair(multiDimOffset, vals[elemId]);\n+      }\n+\n+      if (needTrans) {\n+        // do transpose\n+        auto aEncoding = DotOperandEncodingAttr::get(mma.getContext(), 0, mma);\n+        int numM = aEncoding.getMMAv1NumOuter(shape);\n+        int numN = accumSizePerThread / numM;\n+\n+        for (int r = 0; r < numM; r++) {\n+          for (int c = 0; c < numN; c++) {\n+            coord2valT[r * numN + c] = std::move(coord2val[c * numM + r]);\n+          }\n+        }\n+      } else {\n+        coord2valT = std::move(coord2val);\n+      }\n+    }\n+\n+    // Now the coord2valT has the transposed and contiguous elements(with\n+    // vec=2), the original vals is not needed.\n+    for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n+      auto coord = coord2valT[elemId].first;\n+      Value offset = linearize(rewriter, loc, coord, paddedRepShape, outOrd);\n+      auto elemPtrTy = ptr_ty(elemTy, 3);\n+      Value ptr = gep(elemPtrTy, smemBase, offset);\n+      auto vecTy = vec_ty(elemTy, vec);\n+      ptr = bitcast(ptr, ptr_ty(vecTy, 3));\n+      if (stNotRd) {\n+        Value valVec = undef(vecTy);\n+        for (unsigned v = 0; v < vec; ++v) {\n+          auto currVal = coord2valT[elemId + v].second;\n+          valVec = insert_element(vecTy, valVec, currVal, i32_val(v));\n+        }\n+        store(valVec, ptr);\n+      } else {\n+        Value valVec = load(ptr);\n+        for (unsigned v = 0; v < vec; ++v) {\n+          Value currVal = extract_element(elemTy, valVec, i32_val(v));\n+          vals[elemId + v] = currVal;\n+        }\n+      }\n+    }\n+  }\n+\n+  // blocked/mma -> blocked/mma.\n+  // Data padding in shared memory to avoid bank conflict.\n+  LogicalResult\n+  lowerDistributedToDistributed(triton::gpu::ConvertLayoutOp op,\n+                                OpAdaptor adaptor,\n+                                ConversionPatternRewriter &rewriter) const {\n+    auto loc = op.getLoc();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    Attribute srcLayout = srcTy.getEncoding();\n+    Attribute dstLayout = dstTy.getEncoding();\n+    auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n+    Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+    auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n+    smemBase = bitcast(smemBase, elemPtrTy);\n+    auto shape = dstTy.getShape();\n+    unsigned rank = dstTy.getRank();\n+    SmallVector<unsigned> numReplicates(rank);\n+    SmallVector<unsigned> inNumCTAsEachRep(rank);\n+    SmallVector<unsigned> outNumCTAsEachRep(rank);\n+    SmallVector<unsigned> inNumCTAs(rank);\n+    SmallVector<unsigned> outNumCTAs(rank);\n+    auto srcShapePerCTA = getShapePerCTA(srcLayout, srcTy.getShape());\n+    auto dstShapePerCTA = getShapePerCTA(dstLayout, shape);\n+\n+    // For Volta, all the coords for a CTA are calculated.\n+    bool isSrcMmaV1{}, isDstMmaV1{};\n+    if (auto mmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>()) {\n+      isSrcMmaV1 = mmaLayout.isVolta();\n+    }\n+    if (auto sliceLayout = srcLayout.dyn_cast<SliceEncodingAttr>()) {\n+      isSrcMmaV1 = sliceLayout.getParent().isa<MmaEncodingAttr>() &&\n+                   sliceLayout.getParent().cast<MmaEncodingAttr>().isVolta();\n+    }\n+    if (auto mmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>()) {\n+      isDstMmaV1 = mmaLayout.isVolta();\n+    }\n+    if (auto sliceLayout = dstLayout.dyn_cast<SliceEncodingAttr>()) {\n+      isDstMmaV1 = sliceLayout.getParent().isa<MmaEncodingAttr>() &&\n+                   sliceLayout.getParent().cast<MmaEncodingAttr>().isVolta();\n+    }\n+\n+    for (unsigned d = 0; d < rank; ++d) {\n+      unsigned inPerCTA = std::min<unsigned>(shape[d], srcShapePerCTA[d]);\n+      unsigned outPerCTA = std::min<unsigned>(shape[d], dstShapePerCTA[d]);\n+      unsigned maxPerCTA = std::max(inPerCTA, outPerCTA);\n+      numReplicates[d] = ceil<unsigned>(shape[d], maxPerCTA);\n+      inNumCTAsEachRep[d] = maxPerCTA / inPerCTA;\n+      outNumCTAsEachRep[d] = maxPerCTA / outPerCTA;\n+      assert(maxPerCTA % inPerCTA == 0 && maxPerCTA % outPerCTA == 0);\n+      inNumCTAs[d] = ceil<unsigned>(shape[d], inPerCTA);\n+      outNumCTAs[d] = ceil<unsigned>(shape[d], outPerCTA);\n+    }\n+    // Potentially we need to store for multiple CTAs in this replication\n+    auto accumNumReplicates = product<unsigned>(numReplicates);\n+    // unsigned elems = getElemsPerThread(srcTy);\n+    auto vals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n+                                                     rewriter, srcTy);\n+    unsigned inVec = 0;\n+    unsigned outVec = 0;\n+    auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n+\n+    unsigned outElems = getElemsPerThread(dstTy);\n+    auto outOrd = getOrder(dstLayout);\n+    SmallVector<Value> outVals(outElems);\n+\n+    for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n+      auto multiDimRepId =\n+          getMultiDimIndex<unsigned>(repId, numReplicates, outOrd);\n+      if (repId != 0)\n+        barrier();\n+      if (srcLayout.isa<BlockedEncodingAttr>() ||\n+          srcLayout.isa<SliceEncodingAttr>() ||\n+          srcLayout.isa<MmaEncodingAttr>()) {\n+        if (isSrcMmaV1)\n+          processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ true, srcTy,\n+                                 multiDimRepId, inVec, paddedRepShape, outOrd,\n+                                 vals, smemBase, shape);\n+        else\n+          processReplica(loc, rewriter, /*stNotRd*/ true, srcTy,\n+                         inNumCTAsEachRep, multiDimRepId, inVec, paddedRepShape,\n+                         outOrd, vals, smemBase);\n+      } else {\n+        assert(0 && \"ConvertLayout with input layout not implemented\");\n+        return failure();\n+      }\n+\n+      barrier();\n+      if (dstLayout.isa<BlockedEncodingAttr>() ||\n+          dstLayout.isa<SliceEncodingAttr>() ||\n+          dstLayout.isa<MmaEncodingAttr>()) {\n+        if (isDstMmaV1)\n+          processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ false, dstTy,\n+                                 multiDimRepId, outVec, paddedRepShape, outOrd,\n+                                 outVals, smemBase, shape, /*isDestMma=*/true);\n+        else\n+          processReplica(loc, rewriter, /*stNotRd*/ false, dstTy,\n+                         outNumCTAsEachRep, multiDimRepId, outVec,\n+                         paddedRepShape, outOrd, outVals, smemBase);\n+      } else {\n+        assert(0 && \"ConvertLayout with output layout not implemented\");\n+        return failure();\n+      }\n+    }\n+\n+    SmallVector<Type> types(outElems, llvmElemTy);\n+    auto *ctx = llvmElemTy.getContext();\n+    Type structTy = struct_ty(types);\n+    Value result =\n+        getTypeConverter()->packLLElements(loc, outVals, rewriter, dstTy);\n+    rewriter.replaceOp(op, result);\n+\n+    return success();\n+  }\n+\n+  // blocked -> shared.\n+  // Swizzling in shared memory to avoid bank conflict. Normally used for\n+  // A/B operands of dots.\n+  LogicalResult\n+  lowerDistributedToShared(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+                           ConversionPatternRewriter &rewriter) const {\n+    auto loc = op.getLoc();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto srcShape = srcTy.getShape();\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto dstShape = dstTy.getShape();\n+    assert(srcShape.size() == 2 &&\n+           \"Unexpected rank of ConvertLayout(blocked->shared)\");\n+    auto srcLayout = srcTy.getEncoding();\n+    auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n+    auto inOrd = getOrder(srcLayout);\n+    auto outOrd = dstSharedLayout.getOrder();\n+    Value smemBase = getSharedMemoryBase(loc, rewriter, dst);\n+    auto elemTy = getTypeConverter()->convertType(srcTy.getElementType());\n+    auto elemPtrTy = ptr_ty(getTypeConverter()->convertType(elemTy), 3);\n+    smemBase = bitcast(smemBase, elemPtrTy);\n+\n+    auto dstStrides =\n+        getStridesFromShapeAndOrder(dstShape, outOrd, loc, rewriter);\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy);\n+    storeDistributedToShared(src, adaptor.getSrc(), dstStrides, srcIndices, dst,\n+                             smemBase, elemTy, loc, rewriter);\n+    auto smemObj =\n+        SharedMemoryObject(smemBase, dstShape, outOrd, loc, rewriter);\n+    auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n+    rewriter.replaceOp(op, retVal);\n+    return success();\n+  }\n+\n+  // shared -> mma_operand\n+  LogicalResult\n+  lowerSharedToDotOperand(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+                          ConversionPatternRewriter &rewriter) const {\n+    auto loc = op.getLoc();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n+    auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n+    auto srcTensorTy = src.getType().cast<RankedTensorType>();\n+    auto dotOperandLayout =\n+        dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+    auto sharedLayout = srcTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+\n+    bool isOuter{};\n+    int K{};\n+    if (dotOperandLayout.getOpIdx() == 0) // $a\n+      K = dstTensorTy.getShape()[sharedLayout.getOrder()[0]];\n+    else // $b\n+      K = dstTensorTy.getShape()[sharedLayout.getOrder()[1]];\n+    isOuter = K == 1;\n+\n+    Value res;\n+    if (auto mmaLayout =\n+            dotOperandLayout.getParent().dyn_cast_or_null<MmaEncodingAttr>()) {\n+      res = lowerSharedToDotOperandMMA(op, adaptor, rewriter, mmaLayout,\n+                                       dotOperandLayout, isOuter);\n+    } else if (auto blockedLayout =\n+                   dotOperandLayout.getParent()\n+                       .dyn_cast_or_null<BlockedEncodingAttr>()) {\n+      auto dotOpLayout =\n+          dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+      auto thread = getThreadId(rewriter, loc);\n+      res = SharedToDotOperandFMA::convertLayout(\n+          dotOpLayout.getOpIdx(), src, adaptor.getSrc(), blockedLayout, thread,\n+          loc, getTypeConverter(), rewriter);\n+    } else {\n+      assert(false && \"Unsupported dot operand layout found\");\n+    }\n+\n+    rewriter.replaceOp(op, res);\n+    return success();\n+  }\n+\n+  // mma -> dot_operand\n+  LogicalResult\n+  lowerMmaToDotOperand(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+                       ConversionPatternRewriter &rewriter) const {\n+    auto loc = op.getLoc();\n+    auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n+    auto dstTy = op.getResult().getType().cast<RankedTensorType>();\n+    if (isMmaToDotShortcut(srcTy, dstTy)) {\n+      // get source values\n+      auto vals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n+                                                       rewriter, srcTy);\n+      unsigned elems = getElemsPerThread(srcTy);\n+      Type elemTy =\n+          this->getTypeConverter()->convertType(srcTy.getElementType());\n+      // for the destination type, we need to pack values together\n+      // so they can be consumed by tensor core operations\n+      SmallVector<Value> vecVals;\n+      SmallVector<Type> types;\n+      // For some reasons, LLVM's NVPTX backend inserts unnecessary (?) integer\n+      // instructions to pack & unpack sub-word integers. A workaround is to\n+      // store the results of ldmatrix in i32\n+      auto elemSize = elemTy.getIntOrFloatBitWidth();\n+      if (auto intTy = elemTy.dyn_cast<IntegerType>() && elemSize <= 16) {\n+        auto fold = 32 / elemSize;\n+        for (unsigned i = 0; i < elems; i += fold) {\n+          Value val = i32_val(0);\n+          for (unsigned j = 0; j < fold; j++) {\n+            auto ext =\n+                shl(i32_ty, zext(i32_ty, vals[i + j]), i32_val(elemSize * j));\n+            val = or_(i32_ty, val, ext);\n+          }\n+          vecVals.push_back(val);\n+        }\n+        elems = elems / (32 / elemSize);\n+        types = SmallVector<Type>(elems, i32_ty);\n+      } else {\n+        unsigned vecSize = std::max<unsigned>(32 / elemSize, 1);\n+        Type vecTy = vec_ty(elemTy, vecSize);\n+        types = SmallVector<Type>(elems / vecSize, vecTy);\n+        for (unsigned i = 0; i < elems; i += vecSize) {\n+          Value packed = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n+          for (unsigned j = 0; j < vecSize; j++)\n+            packed = insert_element(vecTy, packed, vals[i + j], i32_val(j));\n+          vecVals.push_back(packed);\n+        }\n+      }\n+\n+      // This needs to be ordered the same way that\n+      // ldmatrix.x4 would order it\n+      // TODO: this needs to be refactor so we don't\n+      // implicitly depends on how emitOffsetsForMMAV2\n+      // is implemented\n+      SmallVector<Value> reorderedVals;\n+      for (unsigned i = 0; i < vecVals.size(); i += 4) {\n+        reorderedVals.push_back(vecVals[i]);\n+        reorderedVals.push_back(vecVals[i + 2]);\n+        reorderedVals.push_back(vecVals[i + 1]);\n+        reorderedVals.push_back(vecVals[i + 3]);\n+      }\n+\n+      Value view = getTypeConverter()->packLLElements(loc, reorderedVals,\n+                                                      rewriter, dstTy);\n+      rewriter.replaceOp(op, view);\n+      return success();\n+    }\n+    return failure();\n+  }\n+\n+  // shared -> dot_operand if the result layout is mma\n+  Value lowerSharedToDotOperandMMA(\n+      triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+      ConversionPatternRewriter &rewriter, const MmaEncodingAttr &mmaLayout,\n+      const DotOperandEncodingAttr &dotOperandLayout, bool isOuter) const {\n+    auto loc = op.getLoc();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n+    bool isHMMA = supportMMA(dst, mmaLayout.getVersionMajor());\n+\n+    auto smemObj =\n+        getSharedMemoryObjectFromStruct(loc, adaptor.getSrc(), rewriter);\n+    Value res;\n+\n+    if (!isOuter && mmaLayout.isAmpere() && isHMMA) { // tensor core v2\n+\n+      res = SharedToDotOperandMMAv2::convertLayout(\n+          dotOperandLayout.getOpIdx(), rewriter, loc, src, dotOperandLayout,\n+          smemObj, getTypeConverter(), tid_val());\n+\n+    } else if (!isOuter && mmaLayout.isVolta() && isHMMA) { // tensor core v1\n+      bool isMMAv1Row = dotOperandLayout.getMMAv1IsRow();\n+      auto srcSharedLayout = src.getType()\n+                                 .cast<RankedTensorType>()\n+                                 .getEncoding()\n+                                 .cast<SharedEncodingAttr>();\n+\n+      // Can only convert [1, 0] to row or [0, 1] to col for now\n+      if ((srcSharedLayout.getOrder()[0] == 1 && !isMMAv1Row) ||\n+          (srcSharedLayout.getOrder()[0] == 0 && isMMAv1Row)) {\n+        llvm::errs() << \"Unsupported Shared -> DotOperand[MMAv1] conversion\\n\";\n+        return Value();\n+      }\n+\n+      res = SharedToDotOperandMMAv1::convertLayout(\n+          dotOperandLayout.getOpIdx(), src, smemObj, getThreadId(rewriter, loc),\n+          loc, getTypeConverter(), rewriter, dst.getType());\n+    } else {\n+      assert(false && \"Unsupported mma layout found\");\n+    }\n+    return res;\n+  }\n+}; // namespace triton::gpu::ConvertLayoutOp>\n+\n+void populateConvertLayoutOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n+  patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n+                                          indexCacheInfo, benefit);\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.h", "status": "added", "additions": 18, "deletions": 0, "changes": 18, "file_content_changes": "@@ -0,0 +1,18 @@\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_CONVERT_LAYOUT_OP_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_CONVERT_LAYOUT_OP_H\n+\n+#include \"TritonGPUToLLVMBase.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+\n+void populateConvertLayoutOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n+\n+#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp", "status": "added", "additions": 227, "deletions": 0, "changes": 227, "file_content_changes": "@@ -0,0 +1,227 @@\n+#include \"../ConvertLayoutOpToLLVM.h\"\n+#include \"../Utility.h\"\n+\n+using ValueTable = std::map<std::pair<int, int>, Value>;\n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getStridesFromShapeAndOrder;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n+using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getOrder;\n+using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+SmallVector<Value>\n+getThreadIds(Value threadId, ArrayRef<unsigned int> shapePerCTA,\n+             ArrayRef<unsigned int> sizePerThread, ArrayRef<unsigned int> order,\n+             ConversionPatternRewriter &rewriter, Location loc) {\n+  int dim = order.size();\n+  SmallVector<Value> threadIds(dim);\n+  for (unsigned k = 0; k < dim - 1; k++) {\n+    Value dimK = i32_val(shapePerCTA[order[k]] / sizePerThread[order[k]]);\n+    Value rem = urem(threadId, dimK);\n+    threadId = udiv(threadId, dimK);\n+    threadIds[order[k]] = rem;\n+  }\n+  Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n+  threadIds[order[dim - 1]] = urem(threadId, dimK);\n+  return threadIds;\n+}\n+\n+int getShapePerCTAForMN(BlockedEncodingAttr layout, bool isM) {\n+  auto order = layout.getOrder();\n+  auto shapePerCTA = getShapePerCTA(layout);\n+\n+  int mShapePerCTA =\n+      order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  int nShapePerCTA =\n+      order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  return isM ? mShapePerCTA : nShapePerCTA;\n+}\n+\n+// Get sizePerThread for M or N axis.\n+int getSizePerThreadForMN(BlockedEncodingAttr layout, bool isM) {\n+  auto order = layout.getOrder();\n+  auto sizePerThread = getSizePerThread(layout);\n+\n+  int mSizePerThread =\n+      order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+  int nSizePerThread =\n+      order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+  return isM ? mSizePerThread : nSizePerThread;\n+}\n+\n+Value getStructFromValueTable(ArrayRef<Value> vals,\n+                              ConversionPatternRewriter &rewriter, Location loc,\n+                              TritonGPUToLLVMTypeConverter *typeConverter,\n+                              Type elemTy) {\n+  SmallVector<Type> elemTypes(vals.size(), elemTy);\n+  SmallVector<Value> elems;\n+  elems.reserve(vals.size());\n+  for (auto &val : vals) {\n+    elems.push_back(val);\n+  }\n+  MLIRContext *ctx = elemTy.getContext();\n+  Type structTy = struct_ty(elemTypes);\n+  return typeConverter->packLLElements(loc, elems, rewriter, structTy);\n+}\n+\n+ValueTable getValueTableFromStruct(Value val, int K, int n0, int shapePerCTA,\n+                                   int sizePerThread,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Location loc,\n+                                   TritonGPUToLLVMTypeConverter *typeConverter,\n+                                   Type type) {\n+  ValueTable res;\n+  auto elems = typeConverter->unpackLLElements(loc, val, rewriter, type);\n+  int index = 0;\n+  for (unsigned k = 0; k < K; ++k) {\n+    for (unsigned m = 0; m < n0; m += shapePerCTA)\n+      for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n+        res[{m + mm, k}] = elems[index++];\n+      }\n+  }\n+  return res;\n+}\n+\n+Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n+               Location loc, TritonGPUToLLVMTypeConverter *typeConverter,\n+               ConversionPatternRewriter &rewriter) {\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto aShape = aTensorTy.getShape();\n+\n+  auto aOrder = aLayout.getOrder();\n+  auto order = dLayout.getOrder();\n+\n+  bool isARow = aOrder[0] == 1;\n+\n+  auto aSmem = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n+  Value strideAM = aSmem.strides[0];\n+  Value strideAK = aSmem.strides[1];\n+  Value strideA0 = isARow ? strideAK : strideAM;\n+  Value strideA1 = isARow ? strideAM : strideAK;\n+  int aNumPtr = 8;\n+  int K = aShape[1];\n+  int M = aShape[0];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value mContig = i32_val(sizePerThread[order[1]]);\n+\n+  // threadId in blocked layout\n+  auto threadIds =\n+      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n+  Value threadIdM = threadIds[0];\n+\n+  Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n+  Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n+  SmallVector<Value> aOff(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i) {\n+    aOff[i] = add(mul(offA0, strideA0), mul(offA1, strideA1));\n+  }\n+  auto elemTy = A.getType().cast<RankedTensorType>().getElementType();\n+\n+  Type ptrTy = ptr_ty(elemTy, 3);\n+  SmallVector<Value> aPtrs(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i)\n+    aPtrs[i] = gep(ptrTy, aSmem.base, aOff[i]);\n+\n+  SmallVector<Value> vas;\n+\n+  int mShapePerCTA = getShapePerCTAForMN(dLayout, true /*isM*/);\n+  int mSizePerThread = getSizePerThreadForMN(dLayout, true /*isM*/);\n+\n+  for (unsigned k = 0; k < K; ++k)\n+    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+      for (unsigned mm = 0; mm < mSizePerThread; ++mm) {\n+        Value offset =\n+            add(mul(i32_val(m + mm), strideAM), mul(i32_val(k), strideAK));\n+        Value pa = gep(ptrTy, aPtrs[0], offset);\n+        Value va = load(pa);\n+        vas.emplace_back(va);\n+      }\n+\n+  return getStructFromValueTable(vas, rewriter, loc, typeConverter, elemTy);\n+}\n+\n+Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n+               Location loc, TritonGPUToLLVMTypeConverter *typeConverter,\n+               ConversionPatternRewriter &rewriter) {\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto bShape = bTensorTy.getShape();\n+\n+  auto bOrder = bLayout.getOrder();\n+  auto order = dLayout.getOrder();\n+\n+  bool isBRow = bOrder[0] == 1;\n+\n+  auto bSmem = getSharedMemoryObjectFromStruct(loc, llB, rewriter);\n+  Value strideBN = bSmem.strides[1];\n+  Value strideBK = bSmem.strides[0];\n+  Value strideB0 = isBRow ? strideBN : strideBK;\n+  Value strideB1 = isBRow ? strideBK : strideBN;\n+  int bNumPtr = 8;\n+  int K = bShape[0];\n+  int N = bShape[1];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value nContig = i32_val(sizePerThread[order[0]]);\n+\n+  // threadId in blocked layout\n+  auto threadIds =\n+      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n+  Value threadIdN = threadIds[1];\n+\n+  Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n+  Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n+  SmallVector<Value> bOff(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i) {\n+    bOff[i] = add(mul(offB0, strideB0), mul(offB1, strideB1));\n+  }\n+  auto elemTy = B.getType().cast<RankedTensorType>().getElementType();\n+\n+  Type ptrTy = ptr_ty(elemTy, 3);\n+  SmallVector<Value> bPtrs(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i)\n+    bPtrs[i] = gep(ptrTy, bSmem.base, bOff[i]);\n+\n+  SmallVector<Value> vbs;\n+\n+  int nShapePerCTA = getShapePerCTAForMN(dLayout, false /*isM*/);\n+  int nSizePerThread = getSizePerThreadForMN(dLayout, false /*isM*/);\n+\n+  for (unsigned k = 0; k < K; ++k)\n+    for (unsigned n = 0; n < N; n += nShapePerCTA)\n+      for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n+        Value offset =\n+            add(mul(i32_val(n + nn), strideBN), mul(i32_val(k), strideBK));\n+        Value pb = gep(ptrTy, bPtrs[0], offset);\n+        Value vb = load(pb);\n+        vbs.emplace_back(vb);\n+      }\n+\n+  return getStructFromValueTable(vbs, rewriter, loc, typeConverter, elemTy);\n+}\n+\n+namespace SharedToDotOperandFMA {\n+Value convertLayout(int opIdx, Value val, Value llVal,\n+                    BlockedEncodingAttr dLayout, Value thread, Location loc,\n+                    TritonGPUToLLVMTypeConverter *typeConverter,\n+                    ConversionPatternRewriter &rewriter) {\n+  if (opIdx == 0)\n+    return loadAFMA(val, llVal, dLayout, thread, loc, typeConverter, rewriter);\n+  else\n+    return loadBFMA(val, llVal, dLayout, thread, loc, typeConverter, rewriter);\n+}\n+} // namespace SharedToDotOperandFMA"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp", "status": "added", "additions": 460, "deletions": 0, "changes": 460, "file_content_changes": "@@ -0,0 +1,460 @@\n+#include \"../ConvertLayoutOpToLLVM.h\"\n+#include \"../Utility.h\"\n+\n+using CoordTy = SmallVector<Value>;\n+using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getStridesFromShapeAndOrder;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n+using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getOrder;\n+using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+// Compute the offset of the matrix to load.\n+// Returns offsetAM, offsetAK, offsetBN, offsetBK.\n+// NOTE, the information M(from $a) and N(from $b) couldn't be retrieved at\n+// the same time in the usage in convert_layout[shared->dot_op], we leave\n+// the noexist info to be 0 and only use the desired argument from the\n+// composed result. In this way we want to retain the original code\n+// structure in convert_mma884 method for easier debugging.\n+static std::tuple<Value, Value, Value, Value>\n+computeOffsets(Value threadId, bool isARow, bool isBRow, ArrayRef<int> fpw,\n+               ArrayRef<int> spw, ArrayRef<int> rep,\n+               ConversionPatternRewriter &rewriter, Location loc,\n+               Type resultTy) {\n+  auto *ctx = rewriter.getContext();\n+  auto wpt = resultTy.cast<RankedTensorType>()\n+                 .getEncoding()\n+                 .cast<DotOperandEncodingAttr>()\n+                 .getParent()\n+                 .cast<MmaEncodingAttr>()\n+                 .getWarpsPerCTA();\n+\n+  Value _1 = i32_val(1);\n+  Value _3 = i32_val(3);\n+  Value _4 = i32_val(4);\n+  Value _16 = i32_val(16);\n+  Value _32 = i32_val(32);\n+\n+  Value lane = urem(threadId, _32);\n+  Value warp = udiv(threadId, _32);\n+\n+  // warp offset\n+  Value warp0 = urem(warp, i32_val(wpt[0]));\n+  Value warp12 = udiv(warp, i32_val(wpt[0]));\n+  Value warp1 = urem(warp12, i32_val(wpt[1]));\n+  Value warpMOff = mul(warp0, i32_val(spw[0]));\n+  Value warpNOff = mul(warp1, i32_val(spw[1]));\n+  // Quad offset\n+  Value quadMOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[0]));\n+  Value quadNOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[1]));\n+  // Pair offset\n+  Value pairMOff = udiv(urem(lane, _16), _4);\n+  pairMOff = urem(pairMOff, i32_val(fpw[0]));\n+  pairMOff = mul(pairMOff, _4);\n+  Value pairNOff = udiv(urem(lane, _16), _4);\n+  pairNOff = udiv(pairNOff, i32_val(fpw[0]));\n+  pairNOff = urem(pairNOff, i32_val(fpw[1]));\n+  pairNOff = mul(pairNOff, _4);\n+  // scale\n+  pairMOff = mul(pairMOff, i32_val(rep[0] / 2));\n+  quadMOff = mul(quadMOff, i32_val(rep[0] / 2));\n+  pairNOff = mul(pairNOff, i32_val(rep[1] / 2));\n+  quadNOff = mul(quadNOff, i32_val(rep[1] / 2));\n+  // Quad pair offset\n+  Value laneMOff = add(pairMOff, quadMOff);\n+  Value laneNOff = add(pairNOff, quadNOff);\n+  // A offset\n+  Value offsetAM = add(warpMOff, laneMOff);\n+  Value offsetAK = and_(lane, _3);\n+  // B offset\n+  Value offsetBN = add(warpNOff, laneNOff);\n+  Value offsetBK = and_(lane, _3);\n+  // i indices\n+  Value offsetCM = add(and_(lane, _1), offsetAM);\n+  if (isARow) {\n+    offsetAM = add(offsetAM, urem(threadId, _4));\n+    offsetAK = i32_val(0);\n+  }\n+  if (!isBRow) {\n+    offsetBN = add(offsetBN, urem(threadId, _4));\n+    offsetBK = i32_val(0);\n+  }\n+\n+  return std::make_tuple(offsetAM, offsetAK, offsetBN, offsetBK);\n+}\n+\n+static Value loadA(Value tensor, const SharedMemoryObject &smemObj,\n+                   Value thread, Location loc,\n+                   TritonGPUToLLVMTypeConverter *typeConverter,\n+                   ConversionPatternRewriter &rewriter, Type resultTy) {\n+  static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n+  auto wpt = resultTy.cast<RankedTensorType>()\n+                 .getEncoding()\n+                 .cast<DotOperandEncodingAttr>()\n+                 .getParent()\n+                 .cast<MmaEncodingAttr>()\n+                 .getWarpsPerCTA();\n+\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto shape = tensorTy.getShape();\n+  auto order = sharedLayout.getOrder();\n+\n+  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+  Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+\n+  bool isARow = order[0] != 0;\n+  auto resultEncoding = resultTy.cast<RankedTensorType>()\n+                            .getEncoding()\n+                            .cast<DotOperandEncodingAttr>();\n+  auto [offsetAM, offsetAK, _3, _4] = computeOffsets(\n+      thread, isARow, false, fpw, resultEncoding.getMMAv1ShapePerWarp(),\n+      resultEncoding.getMMAv1Rep(), rewriter, loc, resultTy);\n+\n+  int vecA = sharedLayout.getVec();\n+\n+  auto strides = smemObj.strides;\n+  Value strideAM = isARow ? strides[0] : i32_val(1);\n+  Value strideAK = isARow ? i32_val(1) : strides[1];\n+  Value strideA0 = isARow ? strideAK : strideAM;\n+  Value strideA1 = isARow ? strideAM : strideAK;\n+\n+  int strideRepM = wpt[0] * fpw[0] * 8;\n+  int strideRepK = 1;\n+\n+  // swizzling\n+  int perPhaseA = sharedLayout.getPerPhase();\n+  int maxPhaseA = sharedLayout.getMaxPhase();\n+  int stepA0 = isARow ? strideRepK : strideRepM;\n+  int numPtrA = std::max(2 * perPhaseA * maxPhaseA / stepA0, 1);\n+  int NK = shape[1];\n+\n+  // pre-compute pointer lanes\n+  Value offA0 = isARow ? offsetAK : offsetAM;\n+  Value offA1 = isARow ? offsetAM : offsetAK;\n+  Value phaseA = urem(udiv(offA1, i32_val(perPhaseA)), i32_val(maxPhaseA));\n+  offA0 = add(offA0, cSwizzleOffset);\n+  SmallVector<Value> offA(numPtrA);\n+  for (int i = 0; i < numPtrA; i++) {\n+    Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));\n+    offA0I = udiv(offA0I, i32_val(vecA));\n+    offA0I = xor_(offA0I, phaseA);\n+    offA0I = mul(offA0I, i32_val(vecA));\n+    offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n+  }\n+\n+  Type elemX2Ty = vec_ty(f16_ty, 2);\n+  Type elemPtrTy = ptr_ty(f16_ty, 3);\n+  if (tensorTy.getElementType().isBF16()) {\n+    elemX2Ty = vec_ty(i16_ty, 2);\n+    elemPtrTy = ptr_ty(i16_ty, 3);\n+  }\n+\n+  // prepare arguments\n+  SmallVector<Value> ptrA(numPtrA);\n+\n+  std::map<std::pair<int, int>, std::pair<Value, Value>> has;\n+  for (int i = 0; i < numPtrA; i++)\n+    ptrA[i] = gep(ptr_ty(f16_ty, 3), smemBase, offA[i]);\n+\n+  auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+  auto loadA = [&](int m, int k) {\n+    int offidx = (isARow ? k / 4 : m) % numPtrA;\n+    Value thePtrA = gep(elemPtrTy, smemBase, offA[offidx]);\n+\n+    int stepAM = isARow ? m : m / numPtrA * numPtrA;\n+    int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n+    Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),\n+                       mul(i32_val(stepAK), strideAK));\n+    Value pa = gep(elemPtrTy, thePtrA, offset);\n+    Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n+    Value ha = load(bitcast(pa, aPtrTy));\n+    // record lds that needs to be moved\n+    Value ha00 = bitcast(extract_element(ha, i32_val(0)), elemX2Ty);\n+    Value ha01 = bitcast(extract_element(ha, i32_val(1)), elemX2Ty);\n+    ld(has, m, k, ha00, ha01);\n+\n+    if (vecA > 4) {\n+      Value ha10 = bitcast(extract_element(ha, i32_val(2)), elemX2Ty);\n+      Value ha11 = bitcast(extract_element(ha, i32_val(3)), elemX2Ty);\n+      if (isARow)\n+        ld(has, m, k + 4, ha10, ha11);\n+      else\n+        ld(has, m + 1, k, ha10, ha11);\n+    }\n+  };\n+\n+  bool isARow_ = resultEncoding.getMMAv1IsRow();\n+  bool isAVec4 = resultEncoding.getMMAv1IsVec4();\n+  unsigned numM = resultEncoding.getMMAv1NumOuter(shape);\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      if (!has.count({m, k}))\n+        loadA(m, k);\n+\n+  SmallVector<Value> elems;\n+  elems.reserve(has.size() * 2);\n+  for (auto item : has) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+\n+  Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);\n+  return res;\n+}\n+\n+static Value loadB(Value tensor, const SharedMemoryObject &smemObj,\n+                   Value thread, Location loc,\n+                   TritonGPUToLLVMTypeConverter *typeConverter,\n+                   ConversionPatternRewriter &rewriter, Type resultTy) {\n+  static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n+  auto wpt = resultTy.cast<RankedTensorType>()\n+                 .getEncoding()\n+                 .cast<DotOperandEncodingAttr>()\n+                 .getParent()\n+                 .cast<MmaEncodingAttr>()\n+                 .getWarpsPerCTA();\n+  // smem\n+  auto strides = smemObj.strides;\n+\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+\n+  auto shape = tensorTy.getShape();\n+  auto order = sharedLayout.getOrder();\n+\n+  Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+  bool isBRow = order[0] != 0; // is row-major in shared memory layout\n+  // isBRow_ indicates whether B is row-major in DotOperand layout\n+  auto resultEncoding = resultTy.cast<RankedTensorType>()\n+                            .getEncoding()\n+                            .cast<DotOperandEncodingAttr>();\n+\n+  int vecB = sharedLayout.getVec();\n+  Value strideBN = isBRow ? i32_val(1) : strides[1];\n+  Value strideBK = isBRow ? strides[0] : i32_val(1);\n+  Value strideB0 = isBRow ? strideBN : strideBK;\n+  Value strideB1 = isBRow ? strideBK : strideBN;\n+  int strideRepN = wpt[1] * fpw[1] * 8;\n+  int strideRepK = 1;\n+\n+  auto [_3, _4, offsetBN, offsetBK] = computeOffsets(\n+      thread, false, isBRow, fpw, resultEncoding.getMMAv1ShapePerWarp(),\n+      resultEncoding.getMMAv1Rep(), rewriter, loc, resultTy);\n+\n+  // swizzling\n+  int perPhaseB = sharedLayout.getPerPhase();\n+  int maxPhaseB = sharedLayout.getMaxPhase();\n+  int stepB0 = isBRow ? strideRepN : strideRepK;\n+  int numPtrB = std::max(2 * perPhaseB * maxPhaseB / stepB0, 1);\n+  int NK = shape[0];\n+\n+  Value offB0 = isBRow ? offsetBN : offsetBK;\n+  Value offB1 = isBRow ? offsetBK : offsetBN;\n+  Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n+  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+\n+  offB0 = add(offB0, cSwizzleOffset);\n+  SmallVector<Value> offB(numPtrB);\n+  for (int i = 0; i < numPtrB; ++i) {\n+    Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));\n+    offB0I = udiv(offB0I, i32_val(vecB));\n+    offB0I = xor_(offB0I, phaseB);\n+    offB0I = mul(offB0I, i32_val(vecB));\n+    offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));\n+  }\n+\n+  Type elemPtrTy = ptr_ty(f16_ty, 3);\n+  Type elemX2Ty = vec_ty(f16_ty, 2);\n+  if (tensorTy.getElementType().isBF16()) {\n+    elemPtrTy = ptr_ty(i16_ty, 3);\n+    elemX2Ty = vec_ty(i16_ty, 2);\n+  }\n+\n+  SmallVector<Value> ptrB(numPtrB);\n+  ValueTable hbs;\n+  for (int i = 0; i < numPtrB; ++i)\n+    ptrB[i] = gep(ptr_ty(f16_ty, 3), smem, offB[i]);\n+\n+  auto ld = [&](decltype(hbs) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+\n+  auto loadB = [&](int n, int K) {\n+    int offidx = (isBRow ? n : K / 4) % numPtrB;\n+    Value thePtrB = ptrB[offidx];\n+\n+    int stepBN = isBRow ? n / numPtrB * numPtrB : n;\n+    int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n+    Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n+                       mul(i32_val(stepBK), strideBK));\n+    Value pb = gep(elemPtrTy, thePtrB, offset);\n+\n+    Value hb =\n+        load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n+    // record lds that needs to be moved\n+    Value hb00 = bitcast(extract_element(hb, i32_val(0)), elemX2Ty);\n+    Value hb01 = bitcast(extract_element(hb, i32_val(1)), elemX2Ty);\n+    ld(hbs, n, K, hb00, hb01);\n+    if (vecB > 4) {\n+      Value hb10 = bitcast(extract_element(hb, i32_val(2)), elemX2Ty);\n+      Value hb11 = bitcast(extract_element(hb, i32_val(3)), elemX2Ty);\n+      if (isBRow)\n+        ld(hbs, n + 1, K, hb10, hb11);\n+      else\n+        ld(hbs, n, K + 4, hb10, hb11);\n+    }\n+  };\n+\n+  bool isBRow_ = resultEncoding.getMMAv1IsRow();\n+  assert(isBRow == isBRow_ && \"B need smem isRow\");\n+  bool isBVec4 = resultEncoding.getMMAv1IsVec4();\n+  unsigned numN = resultEncoding.getMMAv1NumOuter(shape);\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned n = 0; n < numN / 2; ++n) {\n+      if (!hbs.count({n, k}))\n+        loadB(n, k);\n+    }\n+\n+  SmallVector<Value> elems;\n+  for (auto &item : hbs) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+\n+  Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);\n+  return res;\n+}\n+\n+namespace SharedToDotOperandMMAv1 {\n+using CoordTy = SmallVector<Value>;\n+using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+\n+SmallVector<CoordTy> getMNCoords(Value thread,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 ArrayRef<unsigned int> wpt,\n+                                 const MmaEncodingAttr &mmaLayout,\n+                                 ArrayRef<int64_t> shape, bool isARow,\n+                                 bool isBRow, bool isAVec4, bool isBVec4) {\n+  static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n+\n+  auto *ctx = thread.getContext();\n+  auto loc = UnknownLoc::get(ctx);\n+  Value _1 = i32_val(1);\n+  Value _2 = i32_val(2);\n+  Value _4 = i32_val(4);\n+  Value _16 = i32_val(16);\n+  Value _32 = i32_val(32);\n+  Value _fpw0 = i32_val(fpw[0]);\n+  Value _fpw1 = i32_val(fpw[1]);\n+\n+  // A info\n+  auto aEncoding = DotOperandEncodingAttr::get(ctx, 0, mmaLayout);\n+  auto aRep = aEncoding.getMMAv1Rep();\n+  auto aSpw = aEncoding.getMMAv1ShapePerWarp();\n+  // B info\n+  auto bEncoding = DotOperandEncodingAttr::get(ctx, 1, mmaLayout);\n+  auto bSpw = bEncoding.getMMAv1ShapePerWarp();\n+  auto bRep = bEncoding.getMMAv1Rep();\n+\n+  SmallVector<int, 2> rep({aRep[0], bRep[1]});\n+  SmallVector<int, 2> spw({aSpw[0], bSpw[1]});\n+  SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n+\n+  Value lane = urem(thread, _32);\n+  Value warp = udiv(thread, _32);\n+\n+  Value warp0 = urem(warp, i32_val(wpt[0]));\n+  Value warp12 = udiv(warp, i32_val(wpt[0]));\n+  Value warp1 = urem(warp12, i32_val(wpt[1]));\n+\n+  // warp offset\n+  Value offWarpM = mul(warp0, i32_val(spw[0]));\n+  Value offWarpN = mul(warp1, i32_val(spw[1]));\n+  // quad offset\n+  Value offQuadM = mul(udiv(and_(lane, _16), _4), _fpw0);\n+  Value offQuadN = mul(udiv(and_(lane, _16), _4), _fpw1);\n+  // pair offset\n+  Value offPairM = udiv(urem(lane, _16), _4);\n+  offPairM = urem(offPairM, _fpw0);\n+  offPairM = mul(offPairM, _4);\n+  Value offPairN = udiv(urem(lane, _16), _4);\n+  offPairN = udiv(offPairN, _fpw0);\n+  offPairN = urem(offPairN, _fpw1);\n+  offPairN = mul(offPairN, _4);\n+\n+  // sclare\n+  offPairM = mul(offPairM, i32_val(rep[0] / 2));\n+  offQuadM = mul(offQuadM, i32_val(rep[0] / 2));\n+  offPairN = mul(offPairN, i32_val(rep[1] / 2));\n+  offQuadN = mul(offQuadN, i32_val(rep[1] / 2));\n+\n+  // quad pair offset\n+  Value offLaneM = add(offPairM, offQuadM);\n+  Value offLaneN = add(offPairN, offQuadN);\n+  // a, b offset\n+  Value offsetAM = add(offWarpM, offLaneM);\n+  Value offsetBN = add(offWarpN, offLaneN);\n+  // m indices\n+  Value offsetCM = add(and_(lane, _1), offsetAM);\n+  SmallVector<Value> idxM;\n+  for (unsigned m = 0; m < shape[0]; m += shapePerCTA[0])\n+    for (unsigned mm = 0; mm < rep[0]; ++mm)\n+      idxM.push_back(add(offsetCM, i32_val(m + mm * 2)));\n+\n+  // n indices\n+  Value offsetCN = add((and_(lane, _2)), (add(offWarpN, offPairN)));\n+  SmallVector<Value> idxN;\n+  for (int n = 0; n < shape[1]; n += shapePerCTA[1]) {\n+    for (int nn = 0; nn < rep[1]; ++nn) {\n+      idxN.push_back(add(\n+          offsetCN, i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1])));\n+      idxN.push_back(\n+          add(offsetCN,\n+              i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1] + 1)));\n+    }\n+  }\n+\n+  SmallVector<SmallVector<Value>> axes({idxM, idxN});\n+\n+  // product the axis M and axis N to get coords, ported from\n+  // generator::init_idx method from triton2.0\n+\n+  // TODO[Superjomn]: check the order.\n+  SmallVector<CoordTy> coords;\n+  for (Value x1 : axes[1]) {   // N\n+    for (Value x0 : axes[0]) { // M\n+      SmallVector<Value, 2> idx(2);\n+      idx[0] = x0; // M\n+      idx[1] = x1; // N\n+      coords.push_back(std::move(idx));\n+    }\n+  }\n+\n+  return coords; // {M,N} in row-major\n+}\n+\n+Value convertLayout(int opIdx, Value tensor, const SharedMemoryObject &smemObj,\n+                    Value thread, Location loc,\n+                    TritonGPUToLLVMTypeConverter *typeConverter,\n+                    ConversionPatternRewriter &rewriter, Type resultTy) {\n+  if (opIdx == 0)\n+    return loadA(tensor, smemObj, thread, loc, typeConverter, rewriter,\n+                 resultTy);\n+  else {\n+    assert(opIdx == 1);\n+    return loadB(tensor, smemObj, thread, loc, typeConverter, rewriter,\n+                 resultTy);\n+  }\n+}\n+\n+} // namespace SharedToDotOperandMMAv1"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "added", "additions": 690, "deletions": 0, "changes": 690, "file_content_changes": "@@ -0,0 +1,690 @@\n+#include \"../ConvertLayoutOpToLLVM.h\"\n+#include \"../Utility.h\"\n+\n+using ValueTable = std::map<std::pair<unsigned, unsigned>, Value>;\n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getStridesFromShapeAndOrder;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n+using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getOrder;\n+using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+// Data loader for mma.16816 instruction.\n+class MMA16816SmemLoader {\n+public:\n+  MMA16816SmemLoader(int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n+                     ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n+                     ArrayRef<int> instrShape, ArrayRef<int> matShape,\n+                     int perPhase, int maxPhase, int elemBytes,\n+                     ConversionPatternRewriter &rewriter,\n+                     TritonGPUToLLVMTypeConverter *typeConverter,\n+                     const Location &loc);\n+\n+  // lane = thread % 32\n+  // warpOff = (thread/32) % wpt(0)\n+  llvm::SmallVector<Value> computeOffsets(Value warpOff, Value lane,\n+                                          Value cSwizzleOffset) {\n+    if (canUseLdmatrix)\n+      return computeLdmatrixMatOffs(warpOff, lane, cSwizzleOffset);\n+    else if (elemBytes == 4 && needTrans)\n+      return computeB32MatOffs(warpOff, lane, cSwizzleOffset);\n+    else if (elemBytes == 1 && needTrans)\n+      return computeB8MatOffs(warpOff, lane, cSwizzleOffset);\n+    else\n+      llvm::report_fatal_error(\"Invalid smem load config\");\n+\n+    return {};\n+  }\n+\n+  int getNumPtrs() const { return numPtrs; }\n+\n+  // Compute the offset to the matrix this thread(indexed by warpOff and lane)\n+  // mapped to.\n+  SmallVector<Value> computeLdmatrixMatOffs(Value warpId, Value lane,\n+                                            Value cSwizzleOffset);\n+\n+  // Compute 32-bit matrix offsets.\n+  SmallVector<Value> computeB32MatOffs(Value warpOff, Value lane,\n+                                       Value cSwizzleOffset);\n+\n+  // compute 8-bit matrix offset.\n+  SmallVector<Value> computeB8MatOffs(Value warpOff, Value lane,\n+                                      Value cSwizzleOffset);\n+\n+  // Load 4 matrices and returns 4 vec<2> elements.\n+  std::tuple<Value, Value, Value, Value>\n+  loadX4(int mat0, int mat1, ArrayRef<Value> offs, ArrayRef<Value> ptrs,\n+         Type matTy, Type shemPtrTy) const;\n+\n+private:\n+  SmallVector<uint32_t> order;\n+  int kOrder;\n+  SmallVector<int64_t> tileShape;\n+  SmallVector<int> instrShape;\n+  SmallVector<int> matShape;\n+  int perPhase;\n+  int maxPhase;\n+  int elemBytes;\n+  ConversionPatternRewriter &rewriter;\n+  const Location &loc;\n+  MLIRContext *ctx{};\n+\n+  int cMatShape;\n+  int sMatShape;\n+\n+  Value sStride;\n+\n+  bool needTrans;\n+  bool canUseLdmatrix;\n+\n+  int numPtrs;\n+\n+  int pLoadStrideInMat;\n+  int sMatStride;\n+\n+  int matArrStride;\n+  int warpOffStride;\n+};\n+\n+SmallVector<Value>\n+MMA16816SmemLoader::computeLdmatrixMatOffs(Value warpId, Value lane,\n+                                           Value cSwizzleOffset) {\n+  // 4x4 matrices\n+  Value c = urem(lane, i32_val(8));\n+  Value s = udiv(lane, i32_val(8)); // sub-warp-id\n+\n+  // Decompose s => s_0, s_1, that is the coordinate in 2x2 matrices in a\n+  // warp\n+  Value s0 = urem(s, i32_val(2));\n+  Value s1 = udiv(s, i32_val(2));\n+\n+  // We use different orders for a and b for better performance.\n+  Value kMatArr = kOrder == 1 ? s1 : s0;\n+  Value nkMatArr = kOrder == 1 ? s0 : s1;\n+\n+  // Matrix coordinates inside a CTA,\n+  // the matrix layout is [2wpt[0], 2] for A and [2, 2wpt[1]] for B.\n+  // e.g., Setting wpt=4, the data layout for A(kOrder=1) is\n+  //   |0 0|  -> 0,1,2,3 are the warpids\n+  //   |0 0|\n+  //   |1 1|\n+  //   |1 1|\n+  //   |2 2|\n+  //   |2 2|\n+  //   |3 3|\n+  //   |3 3|\n+  //\n+  // for B(kOrder=0) is\n+  //   |0 1 2 3 0 1 2 3| -> 0,1,2,3 are the warpids\n+  //   |0 1 2 3 0 1 2 3|\n+  // Note, for each warp, it handles a 2x2 matrices, that is the coordinate\n+  // address (s0,s1) annotates.\n+\n+  Value matOff[2];\n+  matOff[kOrder ^ 1] =\n+      add(mul(warpId, i32_val(warpOffStride)), // warp offset (kOrder=1)\n+          mul(nkMatArr,\n+              i32_val(matArrStride))); // matrix offset inside a warp (kOrder=1)\n+  matOff[kOrder] = kMatArr;\n+\n+  // Physical offset (before swizzling)\n+  Value cMatOff = matOff[order[0]];\n+  Value sMatOff = matOff[order[1]];\n+  Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n+  cMatOff = add(cMatOff, cSwizzleMatOff);\n+\n+  // row offset inside a matrix, each matrix has 8 rows.\n+  Value sOffInMat = c;\n+\n+  SmallVector<Value> offs(numPtrs);\n+  Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n+  // To prevent out-of-bound access of B when wpt * 16 > tile_size.\n+  // In such a case, we need to wrap around the offset of B.\n+  // |0 1 2 3 0 1 2 3| -> | 0(0) 1(1) 2(2) 3(3) |\n+  // |0 1 2 3 0 1 2 3|    | 0(0) 1(1) 2(2) 3(3) |\n+  //          ~~~~~~~ out-of-bound access\n+  Value sOff = urem(add(sOffInMat, mul(sMatOff, i32_val(sMatShape))),\n+                    i32_val(tileShape[order[1]]));\n+  for (int i = 0; i < numPtrs; ++i) {\n+    Value cMatOffI = add(cMatOff, i32_val(i * pLoadStrideInMat));\n+    cMatOffI = xor_(cMatOffI, phase);\n+    offs[i] = add(mul(cMatOffI, i32_val(cMatShape)), mul(sOff, sStride));\n+  }\n+\n+  return offs;\n+}\n+\n+SmallVector<Value> MMA16816SmemLoader::computeB32MatOffs(Value warpOff,\n+                                                         Value lane,\n+                                                         Value cSwizzleOffset) {\n+  assert(needTrans && \"Only used in transpose mode.\");\n+  // Load tf32 matrices with lds32\n+  Value cOffInMat = udiv(lane, i32_val(4));\n+  Value sOffInMat = urem(lane, i32_val(4));\n+\n+  Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n+  SmallVector<Value> offs(numPtrs);\n+\n+  for (int mat = 0; mat < 4; ++mat) { // Load 4 mats each time\n+    int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n+    int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n+    if (kMatArrInt > 0) // we don't need pointers for k\n+      continue;\n+    Value kMatArr = i32_val(kMatArrInt);\n+    Value nkMatArr = i32_val(nkMatArrInt);\n+\n+    Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n+                        mul(nkMatArr, i32_val(matArrStride)));\n+    Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n+    cMatOff = add(cMatOff, cSwizzleMatOff);\n+\n+    Value sMatOff = kMatArr;\n+    Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n+    // FIXME: (kOrder == 1?) is really dirty hack\n+    for (int i = 0; i < numPtrs / 2; ++i) {\n+      Value cMatOffI =\n+          add(cMatOff, i32_val(i * pLoadStrideInMat * (kOrder == 1 ? 1 : 2)));\n+      cMatOffI = xor_(cMatOffI, phase);\n+      Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n+      cOff = urem(cOff, i32_val(tileShape[order[0]]));\n+      sOff = urem(sOff, i32_val(tileShape[order[1]]));\n+      offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, sStride));\n+    }\n+  }\n+  return offs;\n+}\n+\n+SmallVector<Value> MMA16816SmemLoader::computeB8MatOffs(Value warpOff,\n+                                                        Value lane,\n+                                                        Value cSwizzleOffset) {\n+  assert(needTrans && \"Only used in transpose mode.\");\n+  Value cOffInMat = udiv(lane, i32_val(4));\n+  Value sOffInMat =\n+      mul(urem(lane, i32_val(4)), i32_val(4)); // each thread load 4 cols\n+\n+  SmallVector<Value> offs(numPtrs);\n+  for (int mat = 0; mat < 4; ++mat) {\n+    int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n+    int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n+    if (kMatArrInt > 0) // we don't need pointers for k\n+      continue;\n+    Value kMatArr = i32_val(kMatArrInt);\n+    Value nkMatArr = i32_val(nkMatArrInt);\n+\n+    Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n+                        mul(nkMatArr, i32_val(matArrStride)));\n+    Value sMatOff = kMatArr;\n+\n+    for (int loadx4Off = 0; loadx4Off < numPtrs / 8; ++loadx4Off) {\n+      for (int elemOff = 0; elemOff < 4; ++elemOff) {\n+        int ptrOff = loadx4Off * 8 + nkMatArrInt * 4 + elemOff;\n+        Value cMatOffI = add(cMatOff, i32_val(loadx4Off * pLoadStrideInMat *\n+                                              (kOrder == 1 ? 1 : 2)));\n+        Value sOffInMatElem = add(sOffInMat, i32_val(elemOff));\n+\n+        // disable swizzling ...\n+\n+        Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n+        Value sOff = add(sOffInMatElem, mul(sMatOff, i32_val(sMatShape)));\n+        // To prevent out-of-bound access when tile is too small.\n+        cOff = urem(cOff, i32_val(tileShape[order[0]]));\n+        sOff = urem(sOff, i32_val(tileShape[order[1]]));\n+        offs[ptrOff] = add(cOff, mul(sOff, sStride));\n+      }\n+    }\n+  }\n+  return offs;\n+}\n+\n+std::tuple<Value, Value, Value, Value>\n+MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> offs,\n+                           ArrayRef<Value> ptrs, Type matTy,\n+                           Type shemPtrTy) const {\n+  assert(mat0 % 2 == 0 && mat1 % 2 == 0 && \"smem matrix load must be aligned\");\n+  int matIdx[2] = {mat0, mat1};\n+\n+  int ptrIdx{-1};\n+\n+  if (canUseLdmatrix)\n+    ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n+  else if (elemBytes == 4 && needTrans)\n+    ptrIdx = matIdx[order[0]];\n+  else if (elemBytes == 1 && needTrans)\n+    ptrIdx = matIdx[order[0]] * 4;\n+  else\n+    llvm::report_fatal_error(\"unsupported mma type found\");\n+\n+  // The main difference with the original triton code is we removed the\n+  // prefetch-related logic here for the upstream optimizer phase should\n+  // take care with it, and that is transparent in dot conversion.\n+  auto getPtr = [&](int idx) { return ptrs[idx]; };\n+\n+  Value ptr = getPtr(ptrIdx);\n+\n+  // The struct should have exactly the same element types.\n+  auto resTy = matTy.cast<LLVM::LLVMStructType>();\n+  Type elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n+\n+  // For some reasons, LLVM's NVPTX backend inserts unnecessary (?) integer\n+  // instructions to pack & unpack sub-word integers. A workaround is to\n+  // store the results of ldmatrix in i32\n+  if (auto vecElemTy = elemTy.dyn_cast<VectorType>()) {\n+    Type elemElemTy = vecElemTy.getElementType();\n+    if (auto intTy = elemElemTy.dyn_cast<IntegerType>()) {\n+      if (intTy.getWidth() <= 16) {\n+        elemTy = rewriter.getI32Type();\n+        resTy =\n+            LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, elemTy));\n+      }\n+    }\n+  }\n+\n+  if (canUseLdmatrix) {\n+    Value sOffset =\n+        mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sStride);\n+    Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n+\n+    PTXBuilder builder;\n+    // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n+    // thread.\n+    auto resArgs = builder.newListOperand(4, \"=r\");\n+    auto addrArg = builder.newAddrOperand(sOffsetPtr, \"r\");\n+\n+    auto ldmatrix = builder.create(\"ldmatrix.sync.aligned.m8n8.x4\")\n+                        ->o(\"trans\", needTrans /*predicate*/)\n+                        .o(\"shared.b16\");\n+    ldmatrix(resArgs, addrArg);\n+\n+    // The result type is 4xi32, each i32 is composed of 2xf16\n+    // elements (adjacent two columns in a row) or a single f32 element.\n+    Value resV4 = builder.launch(rewriter, loc, resTy);\n+    return {extract_val(elemTy, resV4, 0), extract_val(elemTy, resV4, 1),\n+            extract_val(elemTy, resV4, 2), extract_val(elemTy, resV4, 3)};\n+  } else if (elemBytes == 4 && needTrans) { // Use lds.32 to load tf32 matrices\n+    Value ptr2 = getPtr(ptrIdx + 1);\n+    assert(sMatStride == 1);\n+    int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n+    Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n+    int sOffsetArrElem = sMatStride * sMatShape;\n+    Value sOffsetArrElemVal =\n+        add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n+\n+    Value elems[4];\n+    if (kOrder == 1) {\n+      elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n+      elems[1] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n+      elems[2] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n+      elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n+    } else {\n+      elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n+      elems[2] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n+      elems[1] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n+      elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n+    }\n+    std::array<Value, 4> retElems;\n+    retElems.fill(undef(elemTy));\n+    for (auto i = 0; i < 4; ++i) {\n+      retElems[i] = insert_element(elemTy, retElems[i], elems[i], i32_val(0));\n+    }\n+    return {retElems[0], retElems[1], retElems[2], retElems[3]};\n+  } else if (elemBytes == 1 && needTrans) { // work with int8\n+    // Can't use i32 here. Use LLVM's VectorType\n+    elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n+    std::array<std::array<Value, 4>, 2> ptrs;\n+    ptrs[0] = {\n+        getPtr(ptrIdx),\n+        getPtr(ptrIdx + 1),\n+        getPtr(ptrIdx + 2),\n+        getPtr(ptrIdx + 3),\n+    };\n+\n+    ptrs[1] = {\n+        getPtr(ptrIdx + 4),\n+        getPtr(ptrIdx + 5),\n+        getPtr(ptrIdx + 6),\n+        getPtr(ptrIdx + 7),\n+    };\n+\n+    assert(sMatStride == 1);\n+    int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n+    Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n+    int sOffsetArrElem = 1 * (sMatStride * sMatShape);\n+    Value sOffsetArrElemVal =\n+        add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n+\n+    std::array<Value, 4> i8v4Elems;\n+    i8v4Elems.fill(undef(elemTy));\n+\n+    Value i8Elems[4][4];\n+    if (kOrder == 1) {\n+      for (int i = 0; i < 2; ++i)\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[i][j] = load(gep(shemPtrTy, ptrs[i][j], sOffsetElemVal));\n+\n+      for (int i = 2; i < 4; ++i)\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[i][j] =\n+              load(gep(shemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n+\n+      for (int m = 0; m < 4; ++m) {\n+        for (int e = 0; e < 4; ++e)\n+          i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n+                                        i8Elems[m][e], i32_val(e));\n+      }\n+    } else { // k first\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[0][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetElemVal));\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[2][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetElemVal));\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[1][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[3][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n+\n+      for (int m = 0; m < 4; ++m) {\n+        for (int e = 0; e < 4; ++e)\n+          i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n+                                        i8Elems[m][e], i32_val(e));\n+      }\n+    }\n+\n+    return {bitcast(i8v4Elems[0], i32_ty), bitcast(i8v4Elems[1], i32_ty),\n+            bitcast(i8v4Elems[2], i32_ty), bitcast(i8v4Elems[3], i32_ty)};\n+  }\n+\n+  assert(false && \"Invalid smem load\");\n+  return {Value{}, Value{}, Value{}, Value{}};\n+}\n+\n+MMA16816SmemLoader::MMA16816SmemLoader(\n+    int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n+    ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n+    ArrayRef<int> instrShape, ArrayRef<int> matShape, int perPhase,\n+    int maxPhase, int elemBytes, ConversionPatternRewriter &rewriter,\n+    TritonGPUToLLVMTypeConverter *typeConverter, const Location &loc)\n+    : order(order.begin(), order.end()), kOrder(kOrder),\n+      tileShape(tileShape.begin(), tileShape.end()),\n+      instrShape(instrShape.begin(), instrShape.end()),\n+      matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n+      maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter), loc(loc),\n+      ctx(rewriter.getContext()) {\n+  cMatShape = matShape[order[0]];\n+  sMatShape = matShape[order[1]];\n+\n+  sStride = smemStrides[order[1]];\n+\n+  // rule: k must be the fast-changing axis.\n+  needTrans = kOrder != order[0];\n+  canUseLdmatrix = elemBytes == 2 || (!needTrans); // b16\n+\n+  if (canUseLdmatrix) {\n+    // Each CTA, the warps is arranged as [1xwpt] if not transposed,\n+    // otherwise [wptx1], and each warp will perform a mma.\n+    numPtrs =\n+        tileShape[order[0]] / (needTrans ? wpt : 1) / instrShape[order[0]];\n+  } else {\n+    numPtrs = tileShape[order[0]] / wpt / matShape[order[0]];\n+  }\n+  numPtrs = std::max<int>(numPtrs, 2);\n+\n+  // Special rule for i8/u8, 4 ptrs for each matrix\n+  if (!canUseLdmatrix && elemBytes == 1)\n+    numPtrs *= 4;\n+\n+  int loadStrideInMat[2];\n+  loadStrideInMat[kOrder] =\n+      2; // instrShape[kOrder] / matShape[kOrder], always 2\n+  loadStrideInMat[kOrder ^ 1] =\n+      wpt * (instrShape[kOrder ^ 1] / matShape[kOrder ^ 1]);\n+\n+  pLoadStrideInMat = loadStrideInMat[order[0]];\n+  sMatStride =\n+      loadStrideInMat[order[1]] / (instrShape[order[1]] / matShape[order[1]]);\n+\n+  // Each matArr contains warpOffStride matrices.\n+  matArrStride = kOrder == 1 ? 1 : wpt;\n+  warpOffStride = instrShape[kOrder ^ 1] / matShape[kOrder ^ 1];\n+}\n+\n+Type getShemPtrTy(Type argType) {\n+  MLIRContext *ctx = argType.getContext();\n+  if (argType.isF16())\n+    return ptr_ty(type::f16Ty(ctx), 3);\n+  else if (argType.isBF16())\n+    return ptr_ty(type::i16Ty(ctx), 3);\n+  else if (argType.isF32())\n+    return ptr_ty(type::f32Ty(ctx), 3);\n+  else if (argType.isInteger(8))\n+    return ptr_ty(type::i8Ty(ctx), 3);\n+  else\n+    llvm::report_fatal_error(\"mma16816 data type not supported\");\n+}\n+\n+Type getMatType(Type argType) {\n+  MLIRContext *ctx = argType.getContext();\n+  // floating point types\n+  Type fp32x1Ty = vec_ty(type::f32Ty(ctx), 1);\n+  Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n+  Type i16x2Ty = vec_ty(type::i16Ty(ctx), 2);\n+  Type fp16x2Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n+  // LLVM 14.0 does not support bf16 type, so we use i16 instead.\n+  Type bf16x2Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i16x2Ty));\n+  Type fp32Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32x1Ty));\n+  // integer types\n+  Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n+  Type i8x4Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i8x4Ty));\n+\n+  if (argType.isF16())\n+    return fp16x2Pack4Ty;\n+  else if (argType.isBF16())\n+    return bf16x2Pack4Ty;\n+  else if (argType.isF32())\n+    return fp32Pack4Ty;\n+  else if (argType.isInteger(8))\n+    return i8x4Pack4Ty;\n+  else\n+    llvm::report_fatal_error(\"mma16816 data type not supported\");\n+}\n+\n+Value composeValuesToDotOperandLayoutStruct(\n+    const ValueTable &vals, int n0, int n1,\n+    TritonGPUToLLVMTypeConverter *typeConverter, Location loc,\n+    ConversionPatternRewriter &rewriter) {\n+  std::vector<Value> elems;\n+  for (int m = 0; m < n0; ++m)\n+    for (int k = 0; k < n1; ++k) {\n+      elems.push_back(vals.at({2 * m, 2 * k}));\n+      elems.push_back(vals.at({2 * m, 2 * k + 1}));\n+      elems.push_back(vals.at({2 * m + 1, 2 * k}));\n+      elems.push_back(vals.at({2 * m + 1, 2 * k + 1}));\n+    }\n+\n+  assert(!elems.empty());\n+\n+  Type elemTy = elems[0].getType();\n+  MLIRContext *ctx = elemTy.getContext();\n+  Type structTy = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(elems.size(), elemTy));\n+  auto result = typeConverter->packLLElements(loc, elems, rewriter, structTy);\n+  return result;\n+}\n+\n+std::function<void(int, int)>\n+getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n+                MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder,\n+                SmallVector<int> instrShape, SmallVector<int> matShape,\n+                Value warpId, Value lane, ValueTable &vals, bool isA,\n+                TritonGPUToLLVMTypeConverter *typeConverter,\n+                ConversionPatternRewriter &rewriter, Location loc) {\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  Type eltTy = tensorTy.getElementType();\n+  // We assumes that the input operand of Dot should be from shared layout.\n+  // TODO(Superjomn) Consider other layouts if needed later.\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  const int perPhase = sharedLayout.getPerPhase();\n+  const int maxPhase = sharedLayout.getMaxPhase();\n+  const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n+  auto order = sharedLayout.getOrder();\n+\n+  // the original register_lds2, but discard the prefetch logic.\n+  auto ld2 = [](ValueTable &vals, int mn, int k, Value val) {\n+    vals[{mn, k}] = val;\n+  };\n+\n+  // (a, b) is the coordinate.\n+  auto load = [=, &rewriter, &vals, &ld2](int a, int b) {\n+    MMA16816SmemLoader loader(\n+        wpt, sharedLayout.getOrder(), kOrder, smemObj.strides,\n+        tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n+        maxPhase, elemBytes, rewriter, typeConverter, loc);\n+    Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+    SmallVector<Value> offs =\n+        loader.computeOffsets(warpId, lane, cSwizzleOffset);\n+    const int numPtrs = loader.getNumPtrs();\n+    SmallVector<Value> ptrs(numPtrs);\n+\n+    Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+\n+    Type smemPtrTy = getShemPtrTy(eltTy);\n+    for (int i = 0; i < numPtrs; ++i) {\n+      ptrs[i] =\n+          bitcast(gep(smemPtrTy, smemBase, ValueRange({offs[i]})), smemPtrTy);\n+    }\n+\n+    auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n+        (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n+        ptrs, getMatType(eltTy), getShemPtrTy(eltTy));\n+\n+    if (isA) {\n+      ld2(vals, a, b, ha0);\n+      ld2(vals, a + 1, b, ha1);\n+      ld2(vals, a, b + 1, ha2);\n+      ld2(vals, a + 1, b + 1, ha3);\n+    } else {\n+      ld2(vals, a, b, ha0);\n+      ld2(vals, a + 1, b, ha2);\n+      ld2(vals, a, b + 1, ha1);\n+      ld2(vals, a + 1, b + 1, ha3);\n+    }\n+  };\n+\n+  return load;\n+}\n+\n+Value loadA(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n+            DotOperandEncodingAttr aEncoding, const SharedMemoryObject &smemObj,\n+            TritonGPUToLLVMTypeConverter *typeConverter, Value thread) {\n+  auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n+  int bitwidth = aTensorTy.getElementTypeBitWidth();\n+  auto mmaLayout = aEncoding.getParent().cast<MmaEncodingAttr>();\n+\n+  SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n+                             aTensorTy.getShape().end());\n+\n+  ValueTable ha;\n+  std::function<void(int, int)> loadFn;\n+  int mmaInstrM = 16, mmaInstrN = 8, mmaInstrK = 4 * 64 / bitwidth;\n+  int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n+\n+  auto numRep = aEncoding.getMMAv2Rep(aTensorTy.getShape(), bitwidth);\n+  int numRepM = numRep[0];\n+  int numRepK = numRep[1];\n+\n+  if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n+    int wpt0 = mmaLayout.getWarpsPerCTA()[0];\n+    Value warp = udiv(thread, i32_val(32));\n+    Value lane = urem(thread, i32_val(32));\n+    Value warpM = urem(urem(warp, i32_val(wpt0)), i32_val(shape[0] / 16));\n+    // load from smem\n+    // we use ldmatrix.x4 so each warp processes 16x16 elements.\n+    int wpt = std::min<int>(wpt0, shape[0] / 16);\n+    loadFn = getLoadMatrixFn(\n+        tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/,\n+        {mmaInstrM, mmaInstrK} /*instrShape*/,\n+        {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, lane /*laneId*/,\n+        ha /*vals*/, true /*isA*/, typeConverter /* typeConverter */,\n+        rewriter /*rewriter*/, loc /*loc*/);\n+  } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n+    // load from registers, used in gemm fuse\n+    // TODO(Superjomn) Port the logic.\n+    assert(false && \"Loading A from register is not supported yet.\");\n+  } else {\n+    assert(false && \"A's layout is not supported.\");\n+  }\n+\n+  // step1. Perform loading.\n+  for (int m = 0; m < numRepM; ++m)\n+    for (int k = 0; k < numRepK; ++k)\n+      loadFn(2 * m, 2 * k);\n+\n+  // step2. Format the values to LLVM::Struct to passing to mma codegen.\n+  return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK,\n+                                               typeConverter, loc, rewriter);\n+}\n+\n+Value loadB(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n+            DotOperandEncodingAttr bEncoding, const SharedMemoryObject &smemObj,\n+            TritonGPUToLLVMTypeConverter *typeConverter, Value thread) {\n+  ValueTable hb;\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  int bitwidth = tensorTy.getElementTypeBitWidth();\n+  auto mmaLayout = bEncoding.getParent().cast<MmaEncodingAttr>();\n+\n+  SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n+                             tensorTy.getShape().end());\n+\n+  int mmaInstrM = 16, mmaInstrN = 8, mmaInstrK = 4 * 64 / bitwidth;\n+  int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n+\n+  auto numRep = bEncoding.getMMAv2Rep(tensorTy.getShape(), bitwidth);\n+  int numRepK = numRep[0];\n+  int numRepN = numRep[1];\n+\n+  int wpt0 = mmaLayout.getWarpsPerCTA()[0];\n+  int wpt1 = mmaLayout.getWarpsPerCTA()[1];\n+  Value warp = udiv(thread, i32_val(32));\n+  Value lane = urem(thread, i32_val(32));\n+  Value warpMN = udiv(warp, i32_val(wpt0));\n+  Value warpN = urem(urem(warpMN, i32_val(wpt1)), i32_val(shape[1] / 8));\n+  // we use ldmatrix.x4 so each warp processes 16x16 elements.\n+  int wpt = std::min<int>(wpt1, shape[1] / 16);\n+  auto loadFn = getLoadMatrixFn(\n+      tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/,\n+      {mmaInstrK, mmaInstrN} /*instrShape*/,\n+      {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, lane /*laneId*/,\n+      hb /*vals*/, false /*isA*/, typeConverter /* typeConverter */,\n+      rewriter /*rewriter*/, loc /*loc*/);\n+\n+  for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n+    for (int k = 0; k < numRepK; ++k)\n+      loadFn(2 * n, 2 * k);\n+  }\n+\n+  Value result = composeValuesToDotOperandLayoutStruct(\n+      hb, std::max(numRepN / 2, 1), numRepK, typeConverter, loc, rewriter);\n+  return result;\n+}\n+\n+namespace SharedToDotOperandMMAv2 {\n+Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,\n+                    Location loc, Value tensor, DotOperandEncodingAttr encoding,\n+                    const SharedMemoryObject &smemObj,\n+                    TritonGPUToLLVMTypeConverter *typeConverter, Value thread) {\n+  if (opIdx == 0)\n+    return loadA(rewriter, loc, tensor, encoding, smemObj, typeConverter,\n+                 thread);\n+  else {\n+    assert(opIdx == 1);\n+    return loadB(rewriter, loc, tensor, encoding, smemObj, typeConverter,\n+                 thread);\n+  }\n+}\n+} // namespace SharedToDotOperandMMAv2"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "added", "additions": 70, "deletions": 0, "changes": 70, "file_content_changes": "@@ -0,0 +1,70 @@\n+#include \"DotOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+\n+LogicalResult convertFMADot(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                            TritonGPUToLLVMTypeConverter *typeConverter,\n+                            ConversionPatternRewriter &rewriter);\n+\n+LogicalResult convertMMA884(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                            TritonGPUToLLVMTypeConverter *typeConverter,\n+                            ConversionPatternRewriter &rewriter);\n+\n+LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                              TritonGPUToLLVMTypeConverter *typeConverter,\n+                              ConversionPatternRewriter &rewriter);\n+\n+struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::DotOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::DotOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    // D = A * B + C\n+    Value A = op.getA();\n+    Value D = op.getResult();\n+\n+    // Here we assume the DotOp's operands always comes from shared memory.\n+    auto AShape = A.getType().cast<RankedTensorType>().getShape();\n+    size_t reduceAxis = 1;\n+    unsigned K = AShape[reduceAxis];\n+    bool isOuter = K == 1;\n+\n+    MmaEncodingAttr mmaLayout = D.getType()\n+                                    .cast<RankedTensorType>()\n+                                    .getEncoding()\n+                                    .dyn_cast<MmaEncodingAttr>();\n+    if (!isOuter && mmaLayout && supportMMA(op, mmaLayout.getVersionMajor())) {\n+      if (mmaLayout.isVolta())\n+        return convertMMA884(op, adaptor, getTypeConverter(), rewriter);\n+      if (mmaLayout.isAmpere())\n+        return convertMMA16816(op, adaptor, getTypeConverter(), rewriter);\n+\n+      llvm::report_fatal_error(\n+          \"Unsupported MMA kind found when converting DotOp to LLVM.\");\n+    }\n+\n+    if (D.getType()\n+            .cast<RankedTensorType>()\n+            .getEncoding()\n+            .isa<BlockedEncodingAttr>())\n+      return convertFMADot(op, adaptor, getTypeConverter(), rewriter);\n+\n+    llvm::report_fatal_error(\n+        \"Unsupported DotOp found when converting TritonGPU to LLVM.\");\n+  }\n+};\n+\n+void populateDotOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n+                                 RewritePatternSet &patterns, int numWarps,\n+                                 AxisInfoAnalysis &axisInfoAnalysis,\n+                                 const Allocation *allocation, Value smem,\n+                                 PatternBenefit benefit) {\n+  patterns.add<DotOpConversion>(typeConverter, allocation, smem, benefit);\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.h", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -0,0 +1,15 @@\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_DOT_OP_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_DOT_OP_H\n+\n+#include \"TritonGPUToLLVMBase.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+void populateDotOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n+                                 RewritePatternSet &patterns, int numWarps,\n+                                 AxisInfoAnalysis &axisInfoAnalysis,\n+                                 const Allocation *allocation, Value smem,\n+                                 PatternBenefit benefit);\n+\n+#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/FMA.cpp", "status": "added", "additions": 100, "deletions": 0, "changes": 100, "file_content_changes": "@@ -0,0 +1,100 @@\n+#include \"../DotOpToLLVM.h\"\n+#include \"../Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+\n+using ValueTableFMA = std::map<std::pair<int, int>, Value>;\n+\n+static ValueTableFMA getValueTableFromStructFMA(\n+    Value val, int K, int n0, int shapePerCTA, int sizePerThread,\n+    ConversionPatternRewriter &rewriter, Location loc,\n+    TritonGPUToLLVMTypeConverter *typeConverter, Type type) {\n+  ValueTableFMA res;\n+  auto elems = typeConverter->unpackLLElements(loc, val, rewriter, type);\n+  int index = 0;\n+  for (unsigned k = 0; k < K; ++k) {\n+    for (unsigned m = 0; m < n0; m += shapePerCTA)\n+      for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n+        res[{m + mm, k}] = elems[index++];\n+      }\n+  }\n+  return res;\n+}\n+\n+LogicalResult convertFMADot(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                            TritonGPUToLLVMTypeConverter *typeConverter,\n+                            ConversionPatternRewriter &rewriter) {\n+  auto *ctx = rewriter.getContext();\n+  auto loc = op.getLoc();\n+\n+  auto A = op.getA();\n+  auto B = op.getB();\n+  auto C = op.getC();\n+  auto D = op.getResult();\n+\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto dTensorTy = D.getType().cast<RankedTensorType>();\n+\n+  auto aShape = aTensorTy.getShape();\n+  auto bShape = bTensorTy.getShape();\n+\n+  BlockedEncodingAttr dLayout =\n+      dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto order = dLayout.getOrder();\n+  auto cc =\n+      typeConverter->unpackLLElements(loc, adaptor.getC(), rewriter, dTensorTy);\n+\n+  Value llA = adaptor.getA();\n+  Value llB = adaptor.getB();\n+\n+  auto sizePerThread = getSizePerThread(dLayout);\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+\n+  int K = aShape[1];\n+  int M = aShape[0];\n+  int N = bShape[1];\n+\n+  int mShapePerCTA =\n+      order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  int mSizePerThread =\n+      order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+  int nShapePerCTA =\n+      order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  int nSizePerThread =\n+      order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+\n+  auto has =\n+      getValueTableFromStructFMA(llA, K, M, mShapePerCTA, mSizePerThread,\n+                                 rewriter, loc, typeConverter, aTensorTy);\n+  auto hbs =\n+      getValueTableFromStructFMA(llB, K, N, nShapePerCTA, nSizePerThread,\n+                                 rewriter, loc, typeConverter, bTensorTy);\n+\n+  SmallVector<Value> ret = cc;\n+  bool isCRow = order[0] == 1;\n+\n+  for (unsigned k = 0; k < K; k++) {\n+    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+      for (unsigned n = 0; n < N; n += nShapePerCTA)\n+        for (unsigned mm = 0; mm < mSizePerThread; ++mm)\n+          for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n+            int mIdx = m / mShapePerCTA * mSizePerThread + mm;\n+            int nIdx = n / nShapePerCTA * nSizePerThread + nn;\n+\n+            int z = isCRow ? mIdx * N / nShapePerCTA * mSizePerThread + nIdx\n+                           : nIdx * M / mShapePerCTA * nSizePerThread + mIdx;\n+            ret[z] = rewriter.create<LLVM::FMulAddOp>(loc, has[{m + mm, k}],\n+                                                      hbs[{n + nn, k}], ret[z]);\n+          }\n+  }\n+\n+  auto res = typeConverter->packLLElements(loc, ret, rewriter, dTensorTy);\n+  rewriter.replaceOp(op, res);\n+\n+  return success();\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv1.cpp", "status": "added", "additions": 161, "deletions": 0, "changes": 161, "file_content_changes": "@@ -0,0 +1,161 @@\n+#include \"../DotOpToLLVM.h\"\n+#include \"../Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+\n+using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+\n+static Type getMmaRetType(TensorType operand) {\n+  auto *ctx = operand.getContext();\n+  Type fp32Ty = type::f32Ty(ctx);\n+  // f16*f16+f32->f32\n+  return struct_ty(SmallVector<Type>{8, fp32Ty});\n+}\n+\n+static ValueTable\n+extractLoadedOperand(Value llStruct, int NK,\n+                     ConversionPatternRewriter &rewriter,\n+                     TritonGPUToLLVMTypeConverter *typeConverter, Type type) {\n+  ValueTable rcds;\n+  SmallVector<Value> elems = typeConverter->unpackLLElements(\n+      llStruct.getLoc(), llStruct, rewriter, type);\n+\n+  int offset = 0;\n+  for (int i = 0; offset < elems.size(); ++i) {\n+    for (int k = 0; k < NK; k += 4) {\n+      rcds[{i, k}] = std::make_pair(elems[offset], elems[offset + 1]);\n+      offset += 2;\n+    }\n+  }\n+\n+  return rcds;\n+}\n+\n+LogicalResult convertMMA884(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                            TritonGPUToLLVMTypeConverter *typeConverter,\n+                            ConversionPatternRewriter &rewriter) {\n+  auto *ctx = op.getContext();\n+  auto loc = op.getLoc();\n+\n+  Value A = op.getA();\n+  Value B = op.getB();\n+  Value D = op.getResult();\n+  auto mmaLayout = D.getType()\n+                       .cast<RankedTensorType>()\n+                       .getEncoding()\n+                       .cast<MmaEncodingAttr>();\n+  auto ALayout = A.getType()\n+                     .cast<RankedTensorType>()\n+                     .getEncoding()\n+                     .cast<DotOperandEncodingAttr>();\n+  auto BLayout = B.getType()\n+                     .cast<RankedTensorType>()\n+                     .getEncoding()\n+                     .cast<DotOperandEncodingAttr>();\n+\n+  auto ATensorTy = A.getType().cast<RankedTensorType>();\n+  auto BTensorTy = B.getType().cast<RankedTensorType>();\n+  auto DTensorTy = D.getType().cast<RankedTensorType>();\n+  auto AShape = ATensorTy.getShape();\n+  auto BShape = BTensorTy.getShape();\n+\n+  bool isARow = ALayout.getMMAv1IsRow();\n+  bool isBRow = BLayout.getMMAv1IsRow();\n+  auto [isARow_, isBRow_, isAVec4_, isBVec4_, _] =\n+      mmaLayout.decodeVoltaLayoutStates();\n+  assert(isARow == isARow_);\n+  assert(isBRow == isBRow_);\n+\n+  unsigned numM = ALayout.getMMAv1NumOuter(AShape);\n+  unsigned numN = BLayout.getMMAv1NumOuter(BShape);\n+  unsigned NK = AShape[1];\n+\n+  auto has = extractLoadedOperand(adaptor.getA(), NK, rewriter, typeConverter,\n+                                  ATensorTy);\n+  auto hbs = extractLoadedOperand(adaptor.getB(), NK, rewriter, typeConverter,\n+                                  BTensorTy);\n+\n+  // Initialize accumulators with external values, the acc holds the\n+  // accumulator value that is shared between the MMA instructions inside a\n+  // DotOp, we can call the order of the values the accumulator-internal\n+  // order.\n+  SmallVector<Value> acc =\n+      typeConverter->unpackLLElements(loc, adaptor.getC(), rewriter, DTensorTy);\n+  size_t resSize = acc.size();\n+\n+  // The resVals holds the final result of the DotOp.\n+  // NOTE The current order of resVals is different from acc, we call it the\n+  // accumulator-external order. and\n+  SmallVector<Value> resVals(resSize);\n+\n+  auto getIdx = [&](int m, int n) {\n+    std::vector<size_t> idx{{\n+        (m * 2 + 0) + (n * 4 + 0) * numM, // row0\n+        (m * 2 + 0) + (n * 4 + 1) * numM,\n+        (m * 2 + 1) + (n * 4 + 0) * numM, // row1\n+        (m * 2 + 1) + (n * 4 + 1) * numM,\n+        (m * 2 + 0) + (n * 4 + 2) * numM, // row2\n+        (m * 2 + 0) + (n * 4 + 3) * numM,\n+        (m * 2 + 1) + (n * 4 + 2) * numM, // row3\n+        (m * 2 + 1) + (n * 4 + 3) * numM,\n+    }};\n+    return idx;\n+  };\n+\n+  auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n+    auto ha = has.at({m, k});\n+    auto hb = hbs.at({n, k});\n+\n+    PTXBuilder builder;\n+    auto idx = getIdx(m, n);\n+\n+    // note: using \"=f\" for float leads to cleaner PTX\n+    bool isIntMMA = DTensorTy.getElementType().isInteger(32);\n+    auto *resOprs = builder.newListOperand(8, isIntMMA ? \"=r\" : \"=f\");\n+    auto *AOprs = builder.newListOperand({\n+        {ha.first, \"r\"},\n+        {ha.second, \"r\"},\n+    });\n+\n+    auto *BOprs = builder.newListOperand({\n+        {hb.first, \"r\"},\n+        {hb.second, \"r\"},\n+    });\n+    auto *COprs = builder.newListOperand();\n+    for (int i = 0; i < 8; ++i)\n+      COprs->listAppend(builder.newOperand(acc[idx[i]], std::to_string(i)));\n+\n+    auto mma = builder.create(\"mma.sync.aligned.m8n8k4\")\n+                   ->o(isARow ? \"row\" : \"col\")\n+                   .o(isBRow ? \"row\" : \"col\")\n+                   .o(\"f32.f16.f16.f32\");\n+\n+    mma(resOprs, AOprs, BOprs, COprs);\n+\n+    Value res = builder.launch(rewriter, loc, getMmaRetType(ATensorTy));\n+\n+    for (auto i = 0; i < 8; i++) {\n+      Value elem = extract_val(f32_ty, res, i);\n+      acc[idx[i]] = elem;\n+    }\n+  };\n+\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      for (unsigned n = 0; n < numN / 2; ++n) {\n+        callMMA(m, n, k);\n+      }\n+\n+  // res holds the same layout of acc\n+  for (size_t i = 0; i < acc.size(); ++i) {\n+    resVals[i] = acc[i];\n+  }\n+\n+  Value res = typeConverter->packLLElements(loc, resVals, rewriter, DTensorTy);\n+  rewriter.replaceOp(op, res);\n+  return success();\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv2.cpp", "status": "added", "additions": 291, "deletions": 0, "changes": 291, "file_content_changes": "@@ -0,0 +1,291 @@\n+#include \"../DotOpToLLVM.h\"\n+#include \"../Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+\n+using ValueTableV2 = std::map<std::pair<unsigned, unsigned>, Value>;\n+\n+Value loadC(Value tensor, Value llTensor,\n+            TritonGPUToLLVMTypeConverter *typeConverter, Location loc,\n+            ConversionPatternRewriter &rewriter) {\n+  MLIRContext *ctx = tensor.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  size_t fcSize = triton::gpu::getElemsPerThread(tensor.getType());\n+\n+  assert(tensorTy.getEncoding().isa<MmaEncodingAttr>() &&\n+         \"Currently, we only support $c with a mma layout.\");\n+  // Load a normal C tensor with mma layout, that should be a\n+  // LLVM::struct with fcSize elements.\n+  auto structTy = llTensor.getType().cast<LLVM::LLVMStructType>();\n+  assert(structTy.getBody().size() == fcSize &&\n+         \"DotOp's $c operand should pass the same number of values as $d in \"\n+         \"mma layout.\");\n+\n+  auto numMmaRets = tensorTy.getElementType().getIntOrFloatBitWidth() / 8;\n+  assert(numMmaRets == 4 || numMmaRets == 2);\n+  if (numMmaRets == 4) {\n+    return llTensor;\n+  } else if (numMmaRets == 2) {\n+    auto cPack = SmallVector<Value>();\n+    auto cElemTy = tensorTy.getElementType();\n+    int numCPackedElem = 4 / numMmaRets;\n+    Type cPackTy = vec_ty(cElemTy, numCPackedElem);\n+    for (int i = 0; i < fcSize; i += numCPackedElem) {\n+      Value pack = rewriter.create<LLVM::UndefOp>(loc, cPackTy);\n+      for (int j = 0; j < numCPackedElem; ++j) {\n+        pack = insert_element(\n+            cPackTy, pack, extract_val(cElemTy, llTensor, i + j), i32_val(j));\n+      }\n+      cPack.push_back(pack);\n+    }\n+\n+    Type structTy = LLVM::LLVMStructType::getLiteral(\n+        ctx, SmallVector<Type>(cPack.size(), cPackTy));\n+    Value result =\n+        typeConverter->packLLElements(loc, cPack, rewriter, structTy);\n+    return result;\n+  }\n+\n+  return llTensor;\n+}\n+\n+ValueTableV2 getValuesFromDotOperandLayoutStruct(\n+    TritonGPUToLLVMTypeConverter *typeConverter, Location loc,\n+    ConversionPatternRewriter &rewriter, Value value, int n0, int n1,\n+    RankedTensorType type) {\n+\n+  auto elems = typeConverter->unpackLLElements(loc, value, rewriter, type);\n+  int offset{};\n+  ValueTableV2 vals;\n+  for (int i = 0; i < n0; ++i) {\n+    for (int j = 0; j < n1; j++) {\n+      vals[{2 * i, 2 * j}] = elems[offset++];\n+      vals[{2 * i, 2 * j + 1}] = elems[offset++];\n+      vals[{2 * i + 1, 2 * j}] = elems[offset++];\n+      vals[{2 * i + 1, 2 * j + 1}] = elems[offset++];\n+    }\n+  }\n+  return vals;\n+}\n+\n+enum class TensorCoreType : uint8_t {\n+  // floating-point tensor core instr\n+  FP32_FP16_FP16_FP32 = 0, // default\n+  FP32_BF16_BF16_FP32,\n+  FP32_TF32_TF32_FP32,\n+  FP16_FP16_FP16_FP16,\n+  // integer tensor core instr\n+  INT32_INT1_INT1_INT32, // Not implemented\n+  INT32_INT4_INT4_INT32, // Not implemented\n+  INT32_INT8_INT8_INT32, // Not implemented\n+  //\n+  NOT_APPLICABLE,\n+};\n+\n+Type getMmaRetType(TensorCoreType mmaType, MLIRContext *ctx) {\n+  Type fp32Ty = type::f32Ty(ctx);\n+  Type fp16Ty = type::f16Ty(ctx);\n+  Type i32Ty = type::i32Ty(ctx);\n+  Type fp32x4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n+  Type i32x4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i32Ty));\n+  Type fp16x2Pack2Ty = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(2, vec_ty(fp16Ty, 2)));\n+  switch (mmaType) {\n+  case TensorCoreType::FP32_FP16_FP16_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::FP32_BF16_BF16_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::FP32_TF32_TF32_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::FP16_FP16_FP16_FP16:\n+    return fp16x2Pack2Ty;\n+  case TensorCoreType::INT32_INT8_INT8_INT32:\n+    return i32x4Ty;\n+  default:\n+    llvm::report_fatal_error(\"Unsupported mma type found\");\n+  }\n+\n+  return Type{};\n+}\n+\n+TensorCoreType getMmaType(triton::DotOp op) {\n+  Value A = op.getA();\n+  Value B = op.getB();\n+  auto aTy = A.getType().cast<RankedTensorType>();\n+  auto bTy = B.getType().cast<RankedTensorType>();\n+  // d = a*b + c\n+  auto dTy = op.getD().getType().cast<RankedTensorType>();\n+\n+  if (dTy.getElementType().isF32()) {\n+    if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n+      return TensorCoreType::FP32_FP16_FP16_FP32;\n+    if (aTy.getElementType().isBF16() && bTy.getElementType().isBF16())\n+      return TensorCoreType::FP32_BF16_BF16_FP32;\n+    if (aTy.getElementType().isF32() && bTy.getElementType().isF32() &&\n+        op.getAllowTF32())\n+      return TensorCoreType::FP32_TF32_TF32_FP32;\n+  } else if (dTy.getElementType().isInteger(32)) {\n+    if (aTy.getElementType().isInteger(8) && bTy.getElementType().isInteger(8))\n+      return TensorCoreType::INT32_INT8_INT8_INT32;\n+  } else if (dTy.getElementType().isF16()) {\n+    if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n+      return TensorCoreType::FP16_FP16_FP16_FP16;\n+  }\n+\n+  return TensorCoreType::NOT_APPLICABLE;\n+}\n+\n+inline static const std::map<TensorCoreType, std::string> mmaInstrPtx = {\n+    {TensorCoreType::FP32_FP16_FP16_FP32,\n+     \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"},\n+    {TensorCoreType::FP32_BF16_BF16_FP32,\n+     \"mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\"},\n+    {TensorCoreType::FP32_TF32_TF32_FP32,\n+     \"mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\"},\n+\n+    {TensorCoreType::INT32_INT1_INT1_INT32,\n+     \"mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc\"},\n+    {TensorCoreType::INT32_INT4_INT4_INT32,\n+     \"mma.sync.aligned.m16n8k64.row.col.satfinite.s32.s4.s4.s32\"},\n+    {TensorCoreType::INT32_INT8_INT8_INT32,\n+     \"mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\"},\n+\n+    {TensorCoreType::FP16_FP16_FP16_FP16,\n+     \"mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\"},\n+};\n+\n+LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n+                         ConversionPatternRewriter &rewriter, Location loc,\n+                         Value a, Value b, Value c, Value d, Value loadedA,\n+                         Value loadedB, Value loadedC, DotOp op,\n+                         DotOpAdaptor adaptor) {\n+  MLIRContext *ctx = c.getContext();\n+  auto aTensorTy = a.getType().cast<RankedTensorType>();\n+  auto bTensorTy = b.getType().cast<RankedTensorType>();\n+  auto dTensorTy = d.getType().cast<RankedTensorType>();\n+\n+  SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n+                              aTensorTy.getShape().end());\n+  auto dShape = dTensorTy.getShape();\n+  int bitwidth = aTensorTy.getElementType().getIntOrFloatBitWidth();\n+  auto repA =\n+      aTensorTy.getEncoding().cast<DotOperandEncodingAttr>().getMMAv2Rep(\n+          aTensorTy.getShape(), bitwidth);\n+  auto repB =\n+      bTensorTy.getEncoding().cast<DotOperandEncodingAttr>().getMMAv2Rep(\n+          bTensorTy.getShape(), bitwidth);\n+\n+  assert(repA[1] == repB[0]);\n+  int repM = repA[0], repN = repB[1], repK = repA[1];\n+\n+  // shape / shape_per_cta\n+  auto ha = getValuesFromDotOperandLayoutStruct(typeConverter, loc, rewriter,\n+                                                loadedA, repM, repK, aTensorTy);\n+  auto hb = getValuesFromDotOperandLayoutStruct(typeConverter, loc, rewriter,\n+                                                loadedB, std::max(repN / 2, 1),\n+                                                repK, bTensorTy);\n+  auto fc = typeConverter->unpackLLElements(loc, loadedC, rewriter, dTensorTy);\n+  auto numMmaRets = dTensorTy.getElementType().getIntOrFloatBitWidth() / 8;\n+  int numCPackedElem = 4 / numMmaRets;\n+\n+  auto mmaType = getMmaType(op);\n+\n+  auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n+    unsigned colsPerThread = repN * 2;\n+    PTXBuilder builder;\n+    auto &mma = *builder.create(mmaInstrPtx.at(mmaType));\n+    // using =r for float32 works but leads to less readable ptx.\n+    bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n+    bool isAccF16 = dTensorTy.getElementType().isF16();\n+    auto retArgs =\n+        builder.newListOperand(numMmaRets, isIntMMA || isAccF16 ? \"=r\" : \"=f\");\n+    auto aArgs = builder.newListOperand({\n+        {ha[{m, k}], \"r\"},\n+        {ha[{m + 1, k}], \"r\"},\n+        {ha[{m, k + 1}], \"r\"},\n+        {ha[{m + 1, k + 1}], \"r\"},\n+    });\n+    auto bArgs =\n+        builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n+    auto cArgs = builder.newListOperand();\n+    for (int i = 0; i < numMmaRets; ++i) {\n+      cArgs->listAppend(builder.newOperand(\n+          fc[(m * colsPerThread + 4 * n) / numCPackedElem + i],\n+          std::to_string(i)));\n+      // reuse the output registers\n+    }\n+\n+    mma(retArgs, aArgs, bArgs, cArgs);\n+    Value mmaOut =\n+        builder.launch(rewriter, loc, getMmaRetType(mmaType, op.getContext()));\n+\n+    Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n+    for (int i = 0; i < numMmaRets; ++i) {\n+      fc[(m * colsPerThread + 4 * n) / numCPackedElem + i] =\n+          extract_val(elemTy, mmaOut, i);\n+    }\n+  };\n+\n+  for (int k = 0; k < repK; ++k)\n+    for (int m = 0; m < repM; ++m)\n+      for (int n = 0; n < repN; ++n)\n+        callMma(2 * m, n, 2 * k);\n+\n+  Type resElemTy = dTensorTy.getElementType();\n+\n+  // replace with new packed result\n+  Type structTy = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(fc.size() * numCPackedElem, resElemTy));\n+  SmallVector<Value> results(fc.size() * numCPackedElem);\n+  for (int i = 0; i < fc.size(); ++i) {\n+    for (int j = 0; j < numCPackedElem; ++j) {\n+      results[i * numCPackedElem + j] =\n+          numCPackedElem > 1\n+              ? bitcast(extract_element(fc[i], i32_val(j)), resElemTy)\n+              : bitcast(fc[i], resElemTy);\n+    }\n+  }\n+  Value res = typeConverter->packLLElements(loc, results, rewriter, structTy);\n+\n+  rewriter.replaceOp(op, res);\n+\n+  return success();\n+}\n+\n+// Convert to mma.m16n8k16\n+LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                              TritonGPUToLLVMTypeConverter *typeConverter,\n+                              ConversionPatternRewriter &rewriter) {\n+  auto loc = op.getLoc();\n+  auto mmaLayout = op.getResult()\n+                       .getType()\n+                       .cast<RankedTensorType>()\n+                       .getEncoding()\n+                       .cast<MmaEncodingAttr>();\n+\n+  Value A = op.getA();\n+  Value B = op.getB();\n+  Value C = op.getC();\n+\n+  auto ATensorTy = A.getType().cast<RankedTensorType>();\n+  auto BTensorTy = B.getType().cast<RankedTensorType>();\n+\n+  assert(ATensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n+         BTensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n+         \"Both $a and %b should be DotOperand layout.\");\n+\n+  Value loadedA, loadedB, loadedC;\n+  loadedA = adaptor.getA();\n+  loadedB = adaptor.getB();\n+  loadedC =\n+      loadC(op.getC(), adaptor.getC(), typeConverter, op.getLoc(), rewriter);\n+\n+  return convertDot(typeConverter, rewriter, op.getLoc(), A, B, C, op.getD(),\n+                    loadedA, loadedB, loadedC, op, adaptor);\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "added", "additions": 1130, "deletions": 0, "changes": 1130, "file_content_changes": "@@ -0,0 +1,1130 @@\n+#include \"ElementwiseOpToLLVM.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+using ::mlir::triton::gpu::getElemsPerThread;\n+\n+struct FpToFpOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::FpToFpOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::FpToFpOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  /* ------------------ */\n+  // FP8 -> FP16\n+  /* ------------------ */\n+\n+  static SmallVector<Value>\n+  convertFp8x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const char *ptxAsm, const Value &v0, const Value &v1,\n+                       const Value &v2, const Value &v3) {\n+    auto ctx = rewriter.getContext();\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    Value fp8x4Vec = undef(fp8x4VecTy);\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n+    fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto &ptxOp = *builder.create(ptxAsm);\n+\n+    auto *o0 = builder.newOperand(\"=r\");\n+    auto *o1 = builder.newOperand(\"=r\");\n+    auto *i = builder.newOperand(fp8x4Vec, \"r\");\n+    ptxOp({o0, o1, i}, /*onlyAttachMLIRArgs=*/true);\n+\n+    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n+    auto fp16x2x2StructTy =\n+        struct_ty(SmallVector<Type>{fp16x2VecTy, fp16x2VecTy});\n+    auto fp16x2x2Struct =\n+        builder.launch(rewriter, loc, fp16x2x2StructTy, false);\n+    auto fp16x2Vec0 = extract_val(fp16x2VecTy, fp16x2x2Struct, 0);\n+    auto fp16x2Vec1 = extract_val(fp16x2VecTy, fp16x2x2Struct, 1);\n+    return {extract_element(f16_ty, fp16x2Vec0, i32_val(0)),\n+            extract_element(f16_ty, fp16x2Vec0, i32_val(1)),\n+            extract_element(f16_ty, fp16x2Vec1, i32_val(0)),\n+            extract_element(f16_ty, fp16x2Vec1, i32_val(1))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp8E4M3x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // WARN: subnormal (0bs0000xxx) are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+        \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n+        \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n+        \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n+        \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+        \"shr.b32  b0, b0, 1;                    \\n\" // b0 >>= 1\n+        \"shr.b32  b1, b1, 1;                    \\n\" // shift into fp16 position\n+        \"add.u32  b0, b0, 0x20002000;           \\n\" // b0.exp += 2**4-2**3\n+                                                    // exponent compensate = 8\n+        \"add.u32  b1, b1, 0x20002000;           \\n\" // b1 += 8<<10 | 8<<10<<16\n+        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+        \"}\";\n+    return convertFp8x4ToFp16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp8E5M2x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    // exponent bias of Fp8E5M2 and Fp16 are the same\n+    auto *ptxAsm = \"{                           \\n\"\n+                   \"prmt.b32 $0, 0, $2, 0x5140; \\n\\t\"\n+                   \"prmt.b32 $1, 0, $2, 0x7362; \\n\\t\"\n+                   \"}\";\n+    return convertFp8x4ToFp16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n+\n+  /* ------------------ */\n+  // FP8 -> BF16\n+  /* ------------------ */\n+  static SmallVector<Value>\n+  convertFp8x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const char *ptxAsm, const Value &v0, const Value &v1,\n+                       const Value &v2, const Value &v3) {\n+    auto ctx = rewriter.getContext();\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    Value fp8x4Vec = undef(fp8x4VecTy);\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n+    fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto &ptxOp = *builder.create(ptxAsm);\n+\n+    auto *o0 = builder.newOperand(\"=r\");\n+    auto *o1 = builder.newOperand(\"=r\");\n+    auto *i = builder.newOperand(fp8x4Vec, \"r\");\n+    ptxOp({o0, o1, i}, /* onlyAttachMLIRArgs */ true);\n+\n+    auto bf16x2VecTy = vec_ty(i16_ty, 2);\n+    auto bf16x2x2StructTy =\n+        struct_ty(SmallVector<Type>{bf16x2VecTy, bf16x2VecTy});\n+    auto bf16x2x2Struct =\n+        builder.launch(rewriter, loc, bf16x2x2StructTy, false);\n+    auto bf16x2Vec0 = extract_val(bf16x2VecTy, bf16x2x2Struct, 0);\n+    auto bf16x2Vec1 = extract_val(bf16x2VecTy, bf16x2x2Struct, 1);\n+    return {extract_element(i16_ty, bf16x2Vec0, i32_val(0)),\n+            extract_element(i16_ty, bf16x2Vec0, i32_val(1)),\n+            extract_element(i16_ty, bf16x2Vec1, i32_val(0)),\n+            extract_element(i16_ty, bf16x2Vec1, i32_val(1))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp8E4M3x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // WARN: subnormal (0bs0000xxx) are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+        \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n+        \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n+        \"and.b32 b0, a0, 0x7fff7fff;            \\n\" // b0 = a0 & 0x7fff7fff\n+        \"and.b32 b1, a1, 0x7fff7fff;            \\n\" // (strip sign)\n+        \"shr.b32 b0, b0, 4;                     \\n\" // b0 >>= 4\n+        \"shr.b32 b1, b1, 4;                     \\n\" // shift into fp16 position\n+        \"add.u32 b0, b0, 0x3c003c00;            \\n\" // b0.exp += 2**7-2**3\n+                                                    // exponent compensate = 120\n+        \"add.u32 b1, b1, 0x3c003c00;            \\n\" // b1 += 120<<7 | 120<<7<<16\n+        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+        \"}\";\n+    return convertFp8x4ToBf16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  };\n+\n+  static SmallVector<Value>\n+  convertFp8E5M2x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // WARN: subnormal (0bs00000xx) are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+        \"prmt.b32 a0, 0, $2, 0x5140;            \\n\" // a0 = 0xf300f400\n+        \"prmt.b32 a1, 0, $2, 0x7362;            \\n\" // a1 = 0xf100f200\n+        \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n+        \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+        \"shr.b32  b0, b0, 3;                    \\n\" // b0 >>= 3\n+        \"shr.b32  b1, b1, 3;                    \\n\" // shift into bf16 position\n+        \"add.u32  b0, b0, 0x38003800;           \\n\" // b0.exp += 2**7-2**4\n+                                                    // exponent compensate = 112\n+        \"add.u32  b1, b1, 0x38003800;           \\n\" // b1 += 112<<7 | 112<<7<<16\n+        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+        \"}\";\n+    return convertFp8x4ToBf16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  };\n+\n+  /* ------------------ */\n+  // FP16 -> FP8\n+  /* ------------------ */\n+\n+  static SmallVector<Value>\n+  convertFp16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const char *ptxAsm, const Value &v0, const Value &v1,\n+                       const Value &v2, const Value &v3) {\n+    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n+    Value fp16x2Vec0 = undef(fp16x2VecTy);\n+    Value fp16x2Vec1 = undef(fp16x2VecTy);\n+    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v0, i32_val(0));\n+    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v1, i32_val(1));\n+    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v2, i32_val(0));\n+    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v3, i32_val(1));\n+    fp16x2Vec0 = bitcast(fp16x2Vec0, i32_ty);\n+    fp16x2Vec1 = bitcast(fp16x2Vec1, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto &ptxOp = *builder.create(ptxAsm);\n+\n+    auto *o = builder.newOperand(\"=r\");\n+    auto *i0 = builder.newOperand(fp16x2Vec0, \"r\");\n+    auto *i1 = builder.newOperand(fp16x2Vec1, \"r\");\n+    ptxOp({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n+\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n+    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp16x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // WARN: subnormal Fp8s are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // see Fp8E4M3x4ToFp16x4\n+        \"sub.u32 a0, $1, 0x20002000;            \\n\" // a0 = input0 - 0x20002000\n+                                                    // (compensate offset)\n+        \"sub.u32 a1, $2, 0x20002000;            \\n\" // a1 = input1 - 0x20002000\n+                                                    // (8 << 10 | 8 << 10 << 16)\n+        \"shl.b32 a0, a0, 1;                     \\n\" // a0 <<= 1\n+        \"shl.b32 a1, a1, 1;                     \\n\" // shift into fp8e4 position\n+        \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // a0 &= 0x7fff7fff\n+        \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+        \"add.u32 a0, a0, 0x00800080;            \\n\" // a0 += 0x00800080\n+        \"add.u32 a1, a1, 0x00800080;            \\n\" // (round to nearest)\n+        \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\" // b0 = a0|(0x80008000&in0)\n+        \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n+        \"prmt.b32 $0, b0, b1, 0x7531;           \\n\" // output = b1b0\n+        \"}\";\n+    return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm =\n+        \"{                            \\n\"\n+        \".reg .b32 a<2>;              \\n\"\n+        \"and.b32 a0, $1, 0x7fff7fff;  \\n\"           // a0 &= 0x7fff7fff\n+        \"and.b32 a1, $2, 0x7fff7fff;  \\n\"           // (strip sign)\n+        \"add.u32 a0, a0, 0x00800080;  \\n\"           // a0 += 0x00800080\n+        \"add.u32 a1, a1, 0x00800080;  \\n\"           // (round to nearest)\n+        \"lop3.b32 a0, $1, 0x80008000, a0, 0xea; \\n\" // a0 = a0|(0x80008000&in0)\n+        \"lop3.b32 a1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n+        \"prmt.b32 $0, a0, a1, 0x7531; \\n\\t\"         // output = a1a0\n+        \"}\";\n+    return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n+\n+  /* ------------------ */\n+  // FP32 -> FP8\n+  /* ------------------ */\n+\n+  static SmallVector<Value>\n+  convertFp32x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8E4M3x4(loc, rewriter, c0, c1, c2, c3);\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp32x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8E5M2x4(loc, rewriter, c0, c1, c2, c3);\n+  }\n+\n+  /* ------------------ */\n+  // BF16 -> FP8\n+  /* ------------------ */\n+\n+  static SmallVector<Value>\n+  convertBf16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const char *ptxAsm, const Value &v0, const Value &v1,\n+                       const Value &v2, const Value &v3) {\n+    auto bf16x2VecTy = vec_ty(i16_ty, 2);\n+    Value bf16x2Vec0 = undef(bf16x2VecTy);\n+    Value bf16x2Vec1 = undef(bf16x2VecTy);\n+    bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v0, i32_val(0));\n+    bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v1, i32_val(1));\n+    bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v2, i32_val(0));\n+    bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v3, i32_val(1));\n+    bf16x2Vec0 = bitcast(bf16x2Vec0, i32_ty);\n+    bf16x2Vec1 = bitcast(bf16x2Vec1, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto &ptxOp = *builder.create(ptxAsm);\n+\n+    auto *o = builder.newOperand(\"=r\");\n+    auto *i0 = builder.newOperand(bf16x2Vec0, \"r\");\n+    auto *i1 = builder.newOperand(bf16x2Vec1, \"r\");\n+    ptxOp({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n+\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n+    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertBf16x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // bf16 is clamped firstly to fp8 min/max\n+        \"{                                           \\n\" // bf16=fp8>>4 + 120<<7\n+        \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n+        \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n+        \"mov.u32 fp8_min, 0x3c003c00;                \\n\" // so bf16_min = 0x3c00\n+        \"mov.u32 fp8_max, 0x43f043f0;                \\n\" // so bf16_max = 0x43f0\n+        \"mov.u32 rn_, 0x80008;                       \\n\" // round to nearest\n+        \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n+        \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n+        \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n+        \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n+        \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n+\n+        // nosign = clamp(nosign, min, max)\n+        \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n+        \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_0_0, nosign_0_0, 0x3c000000; \\n\"\n+        \"min.u32 nosign_0_0, nosign_0_0, 0x43f00000; \\n\"\n+        \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_0_1, nosign_0_1, 0x3c00;     \\n\"\n+        \"min.u32 nosign_0_1, nosign_0_1, 0x43f0;     \\n\"\n+        \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n+        \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_1_0, nosign_1_0, 0x3c000000; \\n\"\n+        \"min.u32 nosign_1_0, nosign_1_0, 0x43f00000; \\n\"\n+        \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_1_1, nosign_1_1, 0x3c00;     \\n\"\n+        \"min.u32 nosign_1_1, nosign_1_1, 0x43f0;     \\n\"\n+        \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n+\n+        \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n+        \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n+        \"sub.u32 nosign0, nosign0, 0x3c003c00;       \\n\" // nosign0-=0x3c003c00\n+        \"sub.u32 nosign1, nosign1, 0x3c003c00;       \\n\" // (compensate offset)\n+        \"shr.u32 nosign0, nosign0, 4;                \\n\" // nosign0 >>= 4\n+        \"shr.u32 nosign1, nosign1, 4;                \\n\" // shift into to fp8e4\n+        \"prmt.b32 nosign, nosign0, nosign1, 0x6420;  \\n\" // nosign0 = 0x00f100f2\n+                                                         // nosign1 = 0x00f300f4\n+                                                         // nosign = 0xf3f4f1f2\n+        \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n+        \"}\";\n+    return convertBf16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  };\n+\n+  static SmallVector<Value>\n+  convertBf16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // bf16 is clamped firstly to fp8 min/max\n+        \"{                                           \\n\" // bf16=fp8>>3 + 112<<7\n+        \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n+        \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n+        \"mov.u32 fp8_min, 0x38003800;                \\n\" // so bf16_min = 0x3800\n+        \"mov.u32 fp8_max, 0x57e057e0;                \\n\" // so bf16_max = 0x57e0\n+        \"mov.u32 rn_, 0x00100010;                    \\n\" // round to nearest\n+        \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n+        \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n+        \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n+        \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n+        \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n+\n+        // nosign = clamp(nosign, min, max)\n+        \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n+        \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_0_0, nosign_0_0, 0x38000000; \\n\"\n+        \"min.u32 nosign_0_0, nosign_0_0, 0x57e00000; \\n\"\n+        \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_0_1, nosign_0_1, 0x3800;     \\n\"\n+        \"min.u32 nosign_0_1, nosign_0_1, 0x57e0;     \\n\"\n+        \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n+        \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_1_0, nosign_1_0, 0x38000000; \\n\"\n+        \"min.u32 nosign_1_0, nosign_1_0, 0x57e00000; \\n\"\n+        \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_1_1, nosign_1_1, 0x3800;     \\n\"\n+        \"min.u32 nosign_1_1, nosign_1_1, 0x57e0;     \\n\"\n+        \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n+\n+        \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n+        \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n+        \"sub.u32 nosign0, nosign0, 0x38003800;       \\n\" // nosign0-=0x38003800\n+        \"sub.u32 nosign1, nosign1, 0x38003800;       \\n\" // (compensate offset)\n+        \"shl.b32 nosign0, nosign0, 3;                \\n\" // nosign0 <<= 3\n+        \"shl.b32 nosign1, nosign1, 3;                \\n\" // shift into to fp8e4\n+        \"prmt.b32 nosign, nosign0, nosign1, 0x7531;  \\n\" // nosign0 = 0xf100f200\n+                                                         // nosign1 = 0xf300f400\n+                                                         // nosign = 0xf3f4f1f2\n+        \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n+        \"}\";\n+    return convertBf16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n+\n+  /* ------------------ */\n+  // FP8 -> FP32\n+  /* ------------------ */\n+\n+  static SmallVector<Value>\n+  convertFp8E4M3x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto fp16Values = convertFp8E4M3x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+    return {convertFp16ToFp32(loc, rewriter, fp16Values[0]),\n+            convertFp16ToFp32(loc, rewriter, fp16Values[1]),\n+            convertFp16ToFp32(loc, rewriter, fp16Values[2]),\n+            convertFp16ToFp32(loc, rewriter, fp16Values[3])};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp8E5M2x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto fp16Values = convertFp8E5M2x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+    return {rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[0]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[1]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[2]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[3])};\n+  }\n+\n+  //\n+\n+  static SmallVector<Value>\n+  convertFp8E4M3x4ToFp64x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto fp16Values = convertFp8E4M3x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+    return {rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[0]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[1]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[2]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[3])};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp64x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8E4M3x4(loc, rewriter, c0, c1, c2, c3);\n+  }\n+\n+  static Value convertBf16ToFp32(Location loc,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 const Value &v) {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.f32.bf16\");\n+    auto res = builder.newOperand(\"=r\");\n+    auto operand = builder.newOperand(v, \"h\");\n+    cvt(res, operand);\n+    return builder.launch(rewriter, loc, f32_ty, false);\n+  }\n+\n+  static Value convertFp16ToFp32(Location loc,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 const Value &v) {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.f32.f16\");\n+    auto res = builder.newOperand(\"=r\");\n+    auto operand = builder.newOperand(v, \"h\");\n+    cvt(res, operand);\n+    return builder.launch(rewriter, loc, f32_ty, false);\n+  }\n+\n+  static Value convertFp32ToBf16(Location loc,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 const Value &v) {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.rn.bf16.f32\");\n+    auto res = builder.newOperand(\"=h\");\n+    auto operand = builder.newOperand(v, \"r\");\n+    cvt(res, operand);\n+    // TODO: This is a hack to get the right type. We should be able to invoke\n+    // the type converter\n+    return builder.launch(rewriter, loc, i16_ty, false);\n+  }\n+\n+  static Value convertFp32ToFp16(Location loc,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 const Value &v) {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.rn.f16.f32\");\n+    auto res = builder.newOperand(\"=h\");\n+    auto operand = builder.newOperand(v, \"r\");\n+    cvt(res, operand);\n+    return builder.launch(rewriter, loc, f16_ty, false);\n+  }\n+\n+  LogicalResult\n+  matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto srcTensorType = op.getFrom().getType().cast<mlir::RankedTensorType>();\n+    auto dstTensorType =\n+        op.getResult().getType().cast<mlir::RankedTensorType>();\n+    auto srcEltType = srcTensorType.getElementType();\n+    auto dstEltType = dstTensorType.getElementType();\n+    auto loc = op->getLoc();\n+    auto elems = getElemsPerThread(dstTensorType);\n+    SmallVector<Value> resultVals;\n+    bool isSrcFP8 =\n+        srcEltType.isa<mlir::Float8E4M3FNType, mlir::Float8E5M2Type>();\n+    bool isDstFP8 =\n+        dstEltType.isa<mlir::Float8E4M3FNType, mlir::Float8E5M2Type>();\n+\n+    // Select convertor\n+    typedef std::function<SmallVector<Value>(\n+        Location, ConversionPatternRewriter &, const Value &, const Value &,\n+        const Value &, const Value &)>\n+        ConvertorT;\n+\n+    auto F8E4M3TyID = TypeID::get<mlir::Float8E4M3FNType>();\n+    auto F8E5M2TyID = TypeID::get<mlir::Float8E5M2Type>();\n+    auto F16TyID = TypeID::get<mlir::Float16Type>();\n+    auto BF16TyID = TypeID::get<mlir::BFloat16Type>();\n+    auto F32TyID = TypeID::get<mlir::Float32Type>();\n+    auto F64TyID = TypeID::get<mlir::Float64Type>();\n+    DenseMap<std::pair<TypeID, TypeID>, ConvertorT> convertorMap = {\n+        // F8 -> F16\n+        {{F8E4M3TyID, F16TyID}, convertFp8E4M3x4ToFp16x4},\n+        {{F8E5M2TyID, F16TyID}, convertFp8E5M2x4ToFp16x4},\n+        // F16 -> F8\n+        {{F16TyID, F8E4M3TyID}, convertFp16x4ToFp8E4M3x4},\n+        {{F16TyID, F8E5M2TyID}, convertFp16x4ToFp8E5M2x4},\n+        // F8 -> BF16\n+        {{F8E4M3TyID, BF16TyID}, convertFp8E4M3x4ToBf16x4},\n+        {{F8E5M2TyID, BF16TyID}, convertFp8E5M2x4ToBf16x4},\n+        // BF16 -> F8\n+        {{BF16TyID, F8E4M3TyID}, convertBf16x4ToFp8E4M3x4},\n+        {{BF16TyID, F8E5M2TyID}, convertBf16x4ToFp8E5M2x4},\n+        // F8 -> F32\n+        {{F8E4M3TyID, F32TyID}, convertFp8E4M3x4ToFp32x4},\n+        {{F8E5M2TyID, F32TyID}, convertFp8E5M2x4ToFp32x4},\n+        // F32 -> F8\n+        {{F32TyID, F8E4M3TyID}, convertFp32x4ToFp8E4M3x4},\n+        {{F32TyID, F8E5M2TyID}, convertFp32x4ToFp8E5M2x4},\n+    };\n+\n+    std::pair<TypeID, TypeID> key = {srcEltType.getTypeID(),\n+                                     dstEltType.getTypeID()};\n+    if (convertorMap.count(key) == 0) {\n+      llvm::errs() << \"Unsupported conversion from \" << srcEltType << \" to \"\n+                   << dstEltType << \"\\n\";\n+      llvm_unreachable(\"\");\n+    }\n+    auto convertor = convertorMap.lookup(key);\n+\n+    // Vectorized casting\n+    assert(elems % 4 == 0 &&\n+           \"FP8 casting only support tensors with 4-aligned sizes\");\n+    auto elements = getTypeConverter()->unpackLLElements(\n+        loc, adaptor.getFrom(), rewriter, srcTensorType);\n+    for (size_t i = 0; i < elems; i += 4) {\n+      auto converted = convertor(loc, rewriter, elements[i], elements[i + 1],\n+                                 elements[i + 2], elements[i + 3]);\n+      resultVals.append(converted);\n+    }\n+\n+    assert(resultVals.size() == elems);\n+    auto result = getTypeConverter()->packLLElements(loc, resultVals, rewriter,\n+                                                     dstTensorType);\n+    rewriter.replaceOp(op, result);\n+    return success();\n+  }\n+};\n+\n+template <typename SourceOp, typename ConcreteT>\n+class ElementwiseOpConversionBase\n+    : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n+public:\n+  using OpAdaptor = typename SourceOp::Adaptor;\n+\n+  explicit ElementwiseOpConversionBase(\n+      TritonGPUToLLVMTypeConverter &typeConverter, PatternBenefit benefit = 1)\n+      : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n+\n+  LogicalResult\n+  matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto resultTy = op.getType();\n+    Location loc = op->getLoc();\n+\n+    unsigned elems = getElemsPerThread(resultTy);\n+    auto resultElementTy = getElementTypeOrSelf(resultTy);\n+    Type elemTy = this->getTypeConverter()->convertType(resultElementTy);\n+    SmallVector<Type> types(elems, elemTy);\n+    Type structTy = this->getTypeConverter()->convertType(resultTy);\n+\n+    auto *concreteThis = static_cast<const ConcreteT *>(this);\n+    auto operands = getOperands(rewriter, adaptor, resultTy, elems, loc);\n+    SmallVector<Value> resultVals(elems);\n+    for (unsigned i = 0; i < elems; ++i) {\n+      resultVals[i] = concreteThis->createDestOp(op, adaptor, rewriter, elemTy,\n+                                                 operands[i], loc);\n+      if (!bool(resultVals[i]))\n+        return failure();\n+    }\n+    Value view = this->getTypeConverter()->packLLElements(loc, resultVals,\n+                                                          rewriter, resultTy);\n+    rewriter.replaceOp(op, view);\n+\n+    return success();\n+  }\n+\n+protected:\n+  SmallVector<SmallVector<Value>>\n+  getOperands(ConversionPatternRewriter &rewriter, OpAdaptor adaptor,\n+              Type operandTy, const unsigned elems, Location loc) const {\n+    SmallVector<SmallVector<Value>> operands(elems);\n+    for (auto operand : adaptor.getOperands()) {\n+      auto sub_operands = this->getTypeConverter()->unpackLLElements(\n+          loc, operand, rewriter, operandTy);\n+      for (size_t i = 0; i < elems; ++i) {\n+        operands[i].push_back(sub_operands[i]);\n+      }\n+    }\n+    return operands;\n+  }\n+};\n+\n+template <typename SourceOp, typename DestOp>\n+struct ElementwiseOpConversion\n+    : public ElementwiseOpConversionBase<\n+          SourceOp, ElementwiseOpConversion<SourceOp, DestOp>> {\n+  using Base =\n+      ElementwiseOpConversionBase<SourceOp,\n+                                  ElementwiseOpConversion<SourceOp, DestOp>>;\n+  using Base::Base;\n+  using OpAdaptor = typename Base::OpAdaptor;\n+\n+  explicit ElementwiseOpConversion(LLVMTypeConverter &typeConverter,\n+                                   PatternBenefit benefit = 1)\n+      : ElementwiseOpConversionBase<SourceOp, ElementwiseOpConversion>(\n+            typeConverter, benefit) {}\n+\n+  // An interface to support variant DestOp builder.\n+  DestOp createDestOp(SourceOp op, OpAdaptor adaptor,\n+                      ConversionPatternRewriter &rewriter, Type elemTy,\n+                      ValueRange operands, Location loc) const {\n+    return rewriter.create<DestOp>(loc, elemTy, operands,\n+                                   adaptor.getAttributes().getValue());\n+  }\n+};\n+\n+struct CmpIOpConversion\n+    : public ElementwiseOpConversionBase<triton::gpu::CmpIOp,\n+                                         CmpIOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<triton::gpu::CmpIOp, CmpIOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  // An interface to support variant DestOp builder.\n+  LLVM::ICmpOp createDestOp(triton::gpu::CmpIOp op, OpAdaptor adaptor,\n+                            ConversionPatternRewriter &rewriter, Type elemTy,\n+                            ValueRange operands, Location loc) const {\n+    return rewriter.create<LLVM::ICmpOp>(\n+        loc, elemTy, ArithCmpIPredicateToLLVM(op.getPredicate()), operands[0],\n+        operands[1]);\n+  }\n+\n+  static LLVM::ICmpPredicate\n+  ArithCmpIPredicateToLLVM(arith::CmpIPredicate predicate) {\n+    switch (predicate) {\n+#define __PRED_ENUM(item__)                                                    \\\n+  case arith::CmpIPredicate::item__:                                           \\\n+    return LLVM::ICmpPredicate::item__\n+\n+      __PRED_ENUM(eq);\n+      __PRED_ENUM(ne);\n+      __PRED_ENUM(sgt);\n+      __PRED_ENUM(sge);\n+      __PRED_ENUM(slt);\n+      __PRED_ENUM(sle);\n+      __PRED_ENUM(ugt);\n+      __PRED_ENUM(uge);\n+      __PRED_ENUM(ult);\n+      __PRED_ENUM(ule);\n+\n+#undef __PRED_ENUM\n+    }\n+    llvm_unreachable(\"Unknown arith::CmpIPredicate\");\n+  }\n+};\n+\n+struct CmpFOpConversion\n+    : public ElementwiseOpConversionBase<triton::gpu::CmpFOp,\n+                                         CmpFOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<triton::gpu::CmpFOp, CmpFOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  // An interface to support variant DestOp builder.\n+  static LLVM::FCmpOp createDestOp(triton::gpu::CmpFOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, ValueRange operands,\n+                                   Location loc) {\n+    return rewriter.create<LLVM::FCmpOp>(\n+        loc, elemTy, ArithCmpFPredicateToLLVM(op.getPredicate()), operands[0],\n+        operands[1]);\n+  }\n+\n+  static LLVM::FCmpPredicate\n+  ArithCmpFPredicateToLLVM(arith::CmpFPredicate predicate) {\n+    switch (predicate) {\n+#define __PRED_ENUM(item__, item1__)                                           \\\n+  case arith::CmpFPredicate::item__:                                           \\\n+    return LLVM::FCmpPredicate::item1__\n+\n+      __PRED_ENUM(OEQ, oeq);\n+      __PRED_ENUM(ONE, one);\n+      __PRED_ENUM(OGT, ogt);\n+      __PRED_ENUM(OGE, oge);\n+      __PRED_ENUM(OLT, olt);\n+      __PRED_ENUM(OLE, ole);\n+      __PRED_ENUM(ORD, ord);\n+      __PRED_ENUM(UEQ, ueq);\n+      __PRED_ENUM(UGT, ugt);\n+      __PRED_ENUM(UGE, uge);\n+      __PRED_ENUM(ULT, ult);\n+      __PRED_ENUM(ULE, ule);\n+      __PRED_ENUM(UNE, une);\n+      __PRED_ENUM(UNO, uno);\n+      __PRED_ENUM(AlwaysTrue, _true);\n+      __PRED_ENUM(AlwaysFalse, _false);\n+\n+#undef __PRED_ENUM\n+    }\n+    llvm_unreachable(\"Unknown arith::CmpFPredicate\");\n+  }\n+};\n+\n+template <class T>\n+struct ExternElementwiseOpConversion\n+    : public ElementwiseOpConversionBase<T, ExternElementwiseOpConversion<T>> {\n+  using Base = ElementwiseOpConversionBase<T, ExternElementwiseOpConversion<T>>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+  typedef typename Base::OpAdaptor OpAdaptor;\n+\n+  Value createDestOp(T op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    StringRef funcName = op.getSymbol();\n+    if (funcName.empty())\n+      llvm::errs() << \"ExternElementwiseOpConversion\";\n+\n+    Type funcType = getFunctionType(elemTy, operands);\n+    LLVM::LLVMFuncOp funcOp =\n+        appendOrGetFuncOp(rewriter, op, funcName, funcType);\n+    return rewriter.create<LLVM::CallOp>(loc, funcOp, operands).getResult();\n+  }\n+\n+private:\n+  Type getFunctionType(Type resultType, ValueRange operands) const {\n+    SmallVector<Type> operandTypes(operands.getTypes());\n+    return LLVM::LLVMFunctionType::get(resultType, operandTypes);\n+  }\n+\n+  LLVM::LLVMFuncOp appendOrGetFuncOp(ConversionPatternRewriter &rewriter, T op,\n+                                     StringRef funcName, Type funcType) const {\n+    using LLVM::LLVMFuncOp;\n+\n+    auto funcAttr = StringAttr::get(op->getContext(), funcName);\n+    Operation *funcOp = SymbolTable::lookupNearestSymbolFrom(op, funcAttr);\n+    if (funcOp)\n+      return cast<LLVMFuncOp>(*funcOp);\n+\n+    auto parent = ((Operation *)op)->getParentOfType<mlir::LLVM::LLVMFuncOp>();\n+    mlir::OpBuilder b(parent);\n+    auto ret = b.create<LLVMFuncOp>(op->getLoc(), funcName, funcType);\n+    ret.getOperation()->setAttr(\n+        \"libname\", StringAttr::get(op->getContext(), op.getLibname()));\n+    ret.getOperation()->setAttr(\n+        \"libpath\", StringAttr::get(op->getContext(), op.getLibpath()));\n+    return ret;\n+  }\n+};\n+\n+struct FDivOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::DivFOp, FDivOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::arith::DivFOp, FDivOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::DivFOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    PTXBuilder ptxBuilder;\n+    auto &fdiv = *ptxBuilder.create<PTXInstr>(\"div\");\n+    unsigned bitwidth = elemTy.getIntOrFloatBitWidth();\n+    if (32 == bitwidth) {\n+      fdiv.o(\"full\").o(\"f32\");\n+    } else if (64 == bitwidth) {\n+      fdiv.o(\"rn\").o(\"f64\");\n+    } else {\n+      assert(0 && bitwidth && \"not supported\");\n+    }\n+\n+    auto res = ptxBuilder.newOperand(bitwidth == 32 ? \"=r\" : \"=l\");\n+    auto lhs = ptxBuilder.newOperand(operands[0], bitwidth == 32 ? \"r\" : \"l\");\n+    auto rhs = ptxBuilder.newOperand(operands[1], bitwidth == 32 ? \"r\" : \"l\");\n+    fdiv(res, lhs, rhs);\n+\n+    Value ret = ptxBuilder.launch(rewriter, loc, elemTy, false);\n+    return ret;\n+  }\n+};\n+\n+struct FMulOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::MulFOp, FMulOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::arith::MulFOp, FMulOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::MulFOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto lhsElemTy = getElementType(op.getLhs());\n+    auto rhsElemTy = getElementType(op.getRhs());\n+    if (lhsElemTy.isBF16() && rhsElemTy.isBF16()) {\n+      PTXBuilder builder;\n+      auto ptxAsm = \" { .reg .b16 c;        \\n\"\n+                    \"    mov.b16 c, 0x8000U; \\n\" // 0.0\n+                    \"    fma.rn.bf16 $0, $1, $2, c; } \\n\";\n+      auto &fMul = *builder.create<PTXInstr>(ptxAsm);\n+      auto res = builder.newOperand(\"=h\");\n+      auto lhs = builder.newOperand(operands[0], \"h\");\n+      auto rhs = builder.newOperand(operands[1], \"h\");\n+      fMul({res, lhs, rhs}, /*onlyAttachMLIRArgs=*/true);\n+      return builder.launch(rewriter, loc, i16_ty, false);\n+    } else {\n+      return rewriter.create<LLVM::FMulOp>(loc, elemTy, operands[0],\n+                                           operands[1]);\n+    }\n+  }\n+};\n+\n+struct FAddOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::AddFOp, FAddOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::arith::AddFOp, FAddOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::AddFOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto lhsElemTy = getElementType(op.getLhs());\n+    auto rhsElemTy = getElementType(op.getRhs());\n+    if (lhsElemTy.isBF16() && rhsElemTy.isBF16()) {\n+      PTXBuilder builder;\n+      auto ptxAsm = \"{ .reg .b16 c;         \\n\"\n+                    \"   mov.b16 c, 0x3f80U; \\n\" // 1.0\n+                    \"   fma.rn.bf16 $0, $1, c, $2; } \\n\";\n+      auto &fAdd = *builder.create<PTXInstr>(ptxAsm);\n+      auto res = builder.newOperand(\"=h\");\n+      auto lhs = builder.newOperand(operands[0], \"h\");\n+      auto rhs = builder.newOperand(operands[1], \"h\");\n+      fAdd({res, lhs, rhs}, /*onlyAttachMLIRArgs=*/true);\n+      return builder.launch(rewriter, loc, i16_ty, false);\n+    } else {\n+      return rewriter.create<LLVM::FAddOp>(loc, elemTy, operands[0],\n+                                           operands[1]);\n+    }\n+  }\n+};\n+\n+struct FSubOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::SubFOp, FSubOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::arith::SubFOp, FSubOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::SubFOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto lhsElemTy = getElementType(op.getLhs());\n+    auto rhsElemTy = getElementType(op.getRhs());\n+    if (lhsElemTy.isBF16() && rhsElemTy.isBF16()) {\n+      PTXBuilder builder;\n+      auto ptxAsm = \" { .reg .b16 c;         \\n\"\n+                    \"    mov.b16 c, 0xbf80U; \\n\" // -1.0\n+                    \"    fma.rn.bf16 $0, $2, c, $1;} \\n\";\n+      auto &fSub = *builder.create<PTXInstr>(ptxAsm);\n+      auto res = builder.newOperand(\"=h\");\n+      auto lhs = builder.newOperand(operands[0], \"h\");\n+      auto rhs = builder.newOperand(operands[1], \"h\");\n+      fSub({res, lhs, rhs}, /*onlyAttachMLIRArgs=*/true);\n+      return builder.launch(rewriter, loc, i16_ty, false);\n+    } else {\n+      return rewriter.create<LLVM::FSubOp>(loc, elemTy, operands[0],\n+                                           operands[1]);\n+    }\n+  }\n+};\n+\n+struct SIToFPOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::SIToFPOp, SIToFPOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::arith::SIToFPOp, SIToFPOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::SIToFPOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto outElemTy = getElementType(op.getOut());\n+    if (outElemTy.isBF16()) {\n+      auto value = rewriter.create<LLVM::SIToFPOp>(loc, f32_ty, operands[0]);\n+      return FpToFpOpConversion::convertFp32ToBf16(loc, rewriter, value);\n+    } else {\n+      return rewriter.create<LLVM::SIToFPOp>(loc, elemTy, operands[0]);\n+    }\n+  }\n+};\n+\n+struct FPToSIOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::FPToSIOp, FPToSIOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::arith::FPToSIOp, FPToSIOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::FPToSIOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto inElemTy = getElementType(op.getIn());\n+    if (inElemTy.isBF16()) {\n+      auto value =\n+          FpToFpOpConversion::convertBf16ToFp32(loc, rewriter, operands[0]);\n+      return rewriter.create<LLVM::FPToSIOp>(loc, elemTy, value);\n+    } else {\n+      return rewriter.create<LLVM::FPToSIOp>(loc, elemTy, operands[0]);\n+    }\n+  }\n+};\n+\n+struct ExtFOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::ExtFOp, ExtFOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::arith::ExtFOp, ExtFOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::ExtFOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto inElemTy = getElementType(op.getIn());\n+    if (inElemTy.isBF16()) {\n+      auto outElemTy = getElementType(op.getOut());\n+      assert(outElemTy.isF32() && \"unsupported conversion\");\n+      return FpToFpOpConversion::convertBf16ToFp32(loc, rewriter, operands[0]);\n+    } else {\n+      return rewriter.create<LLVM::FPExtOp>(loc, elemTy, operands[0]);\n+    }\n+  }\n+};\n+\n+struct TruncFOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::TruncFOp, TruncFOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::arith::TruncFOp, TruncFOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::TruncFOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto outElemTy = getElementType(op.getOut());\n+    if (outElemTy.isBF16()) {\n+      auto inElemTy = getElementType(op.getIn());\n+      assert(inElemTy.isF32() && \"unsupported conversion\");\n+      return FpToFpOpConversion::convertFp32ToBf16(loc, rewriter, operands[0]);\n+    } else {\n+      return rewriter.create<LLVM::FPTruncOp>(loc, elemTy, operands[0]);\n+    }\n+  }\n+};\n+\n+struct ExpOpConversionApprox\n+    : ElementwiseOpConversionBase<mlir::math::ExpOp, ExpOpConversionApprox> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::math::ExpOp, ExpOpConversionApprox>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::math::ExpOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    // For non-FP32 input, call __nv_expf for higher-precision calculation\n+    if (elemTy.getIntOrFloatBitWidth() != 32)\n+      return {};\n+\n+    const double log2e = 1.4426950408889634;\n+    Value prod = fmul(f32_ty, operands[0], f32_val(log2e));\n+\n+    PTXBuilder ptxBuilder;\n+    auto &exp2 = ptxBuilder.create<PTXInstr>(\"ex2\")->o(\"approx\").o(\"f32\");\n+    auto output = ptxBuilder.newOperand(\"=f\");\n+    auto input = ptxBuilder.newOperand(prod, \"f\");\n+    exp2(output, input);\n+    return ptxBuilder.launch(rewriter, loc, f32_ty, false);\n+  }\n+};\n+\n+struct AbsIOpConversion\n+    : ElementwiseOpConversionBase<mlir::math::AbsIOp, AbsIOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::math::AbsIOp, AbsIOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::math::AbsIOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto boolFalse = rewriter.getBoolAttr(false);\n+    auto constFalse = rewriter.create<LLVM::ConstantOp>(loc, boolFalse);\n+    return rewriter.create<LLVM::AbsOp>(loc, elemTy, operands[0],\n+                                        /*is_int_min_poison=*/constFalse);\n+  }\n+};\n+\n+struct AbsFOpConversion\n+    : ElementwiseOpConversionBase<mlir::math::AbsFOp, AbsFOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::math::AbsFOp, AbsFOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::math::AbsFOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    if (llvm::isa<IntegerType>(elemTy)) {\n+      // Mask out the sign bit\n+      auto num_bits =\n+          getElementTypeOrSelf(op.getType()).getIntOrFloatBitWidth();\n+      assert(num_bits <= 16);\n+      auto mask = (1u << (num_bits - 1u)) - 1u;\n+      auto maskAttr = rewriter.getIntegerAttr(elemTy, mask);\n+      auto maskConst = rewriter.create<LLVM::ConstantOp>(loc, maskAttr);\n+      return and_(operands[0], maskConst);\n+    }\n+\n+    return rewriter.create<LLVM::FAbsOp>(loc, elemTy, operands[0]);\n+  }\n+};\n+\n+void populateElementwiseOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem, PatternBenefit benefit) {\n+#define POPULATE_TERNARY_OP(SRC_OP, DST_OP)                                    \\\n+  patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n+  POPULATE_TERNARY_OP(triton::gpu::SelectOp, LLVM::SelectOp)\n+  POPULATE_TERNARY_OP(arith::SelectOp, LLVM::SelectOp)\n+#undef POPULATE_TERNARY_OP\n+\n+#define POPULATE_BINARY_OP(SRC_OP, DST_OP)                                     \\\n+  patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n+  POPULATE_BINARY_OP(arith::SubIOp, LLVM::SubOp) // -\n+  POPULATE_BINARY_OP(arith::AddIOp, LLVM::AddOp) // +\n+  POPULATE_BINARY_OP(arith::MulIOp, LLVM::MulOp) // *\n+  POPULATE_BINARY_OP(arith::DivSIOp, LLVM::SDivOp)\n+  POPULATE_BINARY_OP(arith::DivUIOp, LLVM::UDivOp)\n+  POPULATE_BINARY_OP(arith::RemFOp, LLVM::FRemOp) // %\n+  POPULATE_BINARY_OP(arith::RemSIOp, LLVM::SRemOp)\n+  POPULATE_BINARY_OP(arith::RemUIOp, LLVM::URemOp)\n+  POPULATE_BINARY_OP(arith::AndIOp, LLVM::AndOp)    // &\n+  POPULATE_BINARY_OP(arith::OrIOp, LLVM::OrOp)      // |\n+  POPULATE_BINARY_OP(arith::XOrIOp, LLVM::XOrOp)    // ^\n+  POPULATE_BINARY_OP(arith::ShLIOp, LLVM::ShlOp)    // <<\n+  POPULATE_BINARY_OP(arith::ShRSIOp, LLVM::AShrOp)  // >>\n+  POPULATE_BINARY_OP(arith::ShRUIOp, LLVM::LShrOp)  // >>\n+  POPULATE_BINARY_OP(arith::MinFOp, LLVM::MinNumOp) // fmin\n+  POPULATE_BINARY_OP(arith::MinSIOp, LLVM::SMinOp)  // smin\n+#undef POPULATE_BINARY_OP\n+\n+#define POPULATE_UNARY_OP(SRC_OP, DST_OP)                                      \\\n+  patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n+  POPULATE_UNARY_OP(arith::TruncIOp, LLVM::TruncOp)\n+  POPULATE_UNARY_OP(arith::ExtSIOp, LLVM::SExtOp)\n+  POPULATE_UNARY_OP(arith::ExtUIOp, LLVM::ZExtOp)\n+  POPULATE_UNARY_OP(arith::FPToUIOp, LLVM::FPToUIOp)\n+  POPULATE_UNARY_OP(arith::UIToFPOp, LLVM::UIToFPOp)\n+  POPULATE_UNARY_OP(math::LogOp, math::LogOp)\n+  POPULATE_UNARY_OP(math::CosOp, math::CosOp)\n+  POPULATE_UNARY_OP(math::SinOp, math::SinOp)\n+  POPULATE_UNARY_OP(math::SqrtOp, math::SqrtOp)\n+  POPULATE_UNARY_OP(math::ExpOp, math::ExpOp)\n+  POPULATE_UNARY_OP(triton::BitcastOp, LLVM::BitcastOp)\n+  POPULATE_UNARY_OP(triton::IntToPtrOp, LLVM::IntToPtrOp)\n+  POPULATE_UNARY_OP(triton::PtrToIntOp, LLVM::PtrToIntOp)\n+#undef POPULATE_UNARY_OP\n+\n+  patterns.add<AbsIOpConversion>(typeConverter, benefit);\n+  patterns.add<AbsFOpConversion>(typeConverter, benefit);\n+  patterns.add<CmpIOpConversion>(typeConverter, benefit);\n+  patterns.add<CmpFOpConversion>(typeConverter, benefit);\n+\n+  patterns.add<FDivOpConversion>(typeConverter, benefit);\n+  patterns.add<FSubOpConversion>(typeConverter, benefit);\n+  patterns.add<FAddOpConversion>(typeConverter, benefit);\n+  patterns.add<FMulOpConversion>(typeConverter, benefit);\n+\n+  patterns.add<ExtFOpConversion>(typeConverter, benefit);\n+  patterns.add<TruncFOpConversion>(typeConverter, benefit);\n+  patterns.add<FPToSIOpConversion>(typeConverter, benefit);\n+  patterns.add<SIToFPOpConversion>(typeConverter, benefit);\n+\n+  patterns.add<FpToFpOpConversion>(typeConverter, benefit);\n+\n+  patterns.add<ExternElementwiseOpConversion<triton::PureExternElementwiseOp>>(\n+      typeConverter, benefit);\n+  patterns\n+      .add<ExternElementwiseOpConversion<triton::ImpureExternElementwiseOp>>(\n+          typeConverter, benefit);\n+  // ExpOpConversionApprox will try using ex2.approx if the input type is\n+  // FP32. For other input types, ExpOpConversionApprox will return failure and\n+  // ElementwiseOpConversion<math::ExpOp, math::ExpOp> defined below will call\n+  // __nv_expf for higher-precision calculation\n+  patterns.add<ExpOpConversionApprox>(typeConverter, benefit);\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.h", "status": "added", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -0,0 +1,16 @@\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_ELEMENTWISE_OP_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_ELEMENTWISE_OP_H\n+\n+#include \"TritonGPUToLLVMBase.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+void populateElementwiseOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem, PatternBenefit benefit);\n+\n+bool isLegalElementwiseOp(Operation *op);\n+\n+#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/GCNAsmFormat.cpp", "status": "added", "additions": 191, "deletions": 0, "changes": 191, "file_content_changes": "@@ -0,0 +1,191 @@\n+#include \"triton/Conversion/TritonGPUToLLVM/GCNAsmFormat.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/AsmFormat.h\"\n+#include \"llvm/Support/raw_ostream.h\"\n+#include <sstream> // unify to llvm::raw_string_ostream ?\n+\n+namespace mlir {\n+namespace triton {\n+\n+GCNInstr::Operand *\n+GCNBuilder::newOperand(mlir::Value value, StringRef constraint,\n+                       std::function<std::string(int)> formatter) {\n+  argArchive.emplace_back(std::make_unique<Operand>(value, constraint));\n+  auto *opr = argArchive.back().get();\n+  opr->repr = formatter;\n+  opr->idx = oprCounter++;\n+  return opr;\n+}\n+\n+GCNBuilder::Operand *GCNBuilder::newOperand(StringRef constraint) {\n+  // Constraint should be something like \"=r\"\n+  assert(!constraint.empty() && constraint[0] == '=');\n+  auto *opr = newOperand();\n+  opr->idx = oprCounter++;\n+  opr->constraint = constraint;\n+  return opr;\n+}\n+\n+GCNBuilder::Modifier *GCNBuilder::newModifier(StringRef modifier,\n+                                              StringRef arg) {\n+  assert(!modifier.empty());\n+  auto *mod = newModifier();\n+  mod->modifier = modifier;\n+  mod->arg = arg;\n+  return mod;\n+}\n+\n+GCNBuilder::Operand *GCNBuilder::newConstantOperand(const std::string &v) {\n+  argArchive.emplace_back(std::make_unique<Operand>());\n+  argArchive.back()->repr = [v](int idx) { return v; };\n+  return argArchive.back().get();\n+}\n+\n+GCNBuilder::Operand *GCNBuilder::newConstantOperand(int v) {\n+  std::stringstream ss;\n+  ss << \"0x\" << std::hex << v;\n+  return newConstantOperand(ss.str());\n+}\n+\n+std::string GCNBuilder::getConstraints() const {\n+  auto args = getAllArgs();\n+  llvm::SmallVector<std::string, 4> argReprs;\n+  for (auto arg : args)\n+    argReprs.push_back(arg->constraint);\n+  return strJoin(argReprs, \",\");\n+}\n+\n+llvm::SmallVector<Value, 4> GCNBuilder::getAllMLIRArgs() const {\n+  llvm::SmallVector<Value, 4> res;\n+  for (auto &arg : argArchive) {\n+    if (!arg->isList() && arg->value)\n+      res.push_back(arg->value);\n+  }\n+  return res;\n+}\n+\n+SmallVector<GCNBuilder::Operand *, 4> GCNBuilder::getAllArgs() const {\n+  llvm::SmallVector<Operand *, 4> res;\n+  for (auto &x : argArchive)\n+    if (!x->isList())\n+      res.push_back(x.get());\n+  return res;\n+}\n+\n+mlir::Value GCNBuilder::launch(ConversionPatternRewriter &rewriter,\n+                               Location loc, Type resTy, bool hasSideEffect,\n+                               bool isAlignStack,\n+                               ArrayRef<Attribute> attrs) const {\n+  auto *ctx = rewriter.getContext();\n+  auto inlineAsm = rewriter.create<LLVM::InlineAsmOp>(\n+      loc, resTy, getAllMLIRArgs(), // operands\n+      dump(),                       // asm_string\n+      getConstraints(),             // constraints\n+      hasSideEffect,                // has_side_effects\n+      isAlignStack,                 // is_align_stack\n+      LLVM::AsmDialectAttr::get(ctx,\n+                                LLVM::AsmDialect::AD_ATT), // asm_dialect\n+      ArrayAttr::get(ctx, attrs)                           // operand_attrs\n+  );\n+\n+  return inlineAsm.getRes();\n+}\n+\n+std::string GCNInstr::Operand::dump() const {\n+  if (repr)\n+    return repr(idx);\n+  if (!isList())\n+    return \"$\" + std::to_string(idx);\n+\n+  llvm::SmallVector<std::string> oprs;\n+  for (auto *opr : list)\n+    oprs.push_back(opr->dump());\n+  return strJoin(oprs, \", \");\n+}\n+\n+std::string GCNInstr::Modifier::dump() const {\n+  if (!isList())\n+    return to_str();\n+\n+  llvm::SmallVector<std::string> mods;\n+  for (auto *mod : list)\n+    mods.push_back(mod->dump());\n+  return strJoin(mods, \" \");\n+}\n+\n+GCNInstr::Operand *GCNBuilder::newAddrOperand(mlir::Value addr,\n+                                              StringRef constraint) {\n+  auto *opr = newOperand(addr, constraint);\n+  opr->repr = [](int idx) -> std::string {\n+    std::stringstream ss;\n+    ss << \"$\" << idx;\n+    return ss.str();\n+  };\n+\n+  return opr;\n+}\n+\n+std::string GCNBuilder::dump() const {\n+  llvm::SmallVector<std::string> lines;\n+  for (auto &exec : executions) {\n+    lines.push_back(exec->dump());\n+  }\n+\n+  return strJoin(lines, \"\\n\\t\");\n+}\n+\n+GCNInstrExecution &GCNInstrCommon::call(ArrayRef<Operand *> oprs,\n+                                        ArrayRef<Modifier *> mods) {\n+  builder->executions.emplace_back(\n+      std::make_unique<GCNInstrExecution>(this, oprs, mods));\n+  return *builder->executions.back();\n+}\n+\n+GCNInstrExecution &GCNInstrCommon::operator()(ArrayRef<Operand *> oprs,\n+                                              ArrayRef<Modifier *> mods) {\n+  return call(oprs, mods);\n+}\n+\n+std::string GCNInstrExecution::dump() const {\n+  std::string osStr;\n+  llvm::raw_string_ostream os(osStr);\n+\n+  std::string instrRepr = strJoin(instr->instrParts, \"_\");\n+\n+  llvm::SmallVector<std::string, 4> argReprs;\n+  for (auto *arg : argsInOrder) {\n+    argReprs.push_back(arg->dump());\n+  }\n+\n+  std::string argsRepr = strJoin(argReprs, \", \");\n+\n+  llvm::SmallVector<std::string, 4> modReprs;\n+  for (auto *mod : mods) {\n+    modReprs.push_back(mod->dump());\n+  }\n+\n+  std::string modsRepr = strJoin(modReprs, \" \");\n+  if (!modsRepr.empty()) {\n+    os << instrRepr << \" \" << argsRepr << \" \" << modsRepr;\n+  } else {\n+    os << instrRepr << \" \" << argsRepr;\n+  }\n+  os.flush();\n+  return osStr;\n+}\n+\n+SmallVector<GCNInstrExecution::Operand *>\n+GCNInstrExecution::getArgList() const {\n+  SmallVector<Operand *> args;\n+  for (auto *arg : argsInOrder) {\n+    if (arg->isList())\n+      args.insert(args.end(), arg->list.begin(), arg->list.end());\n+    else\n+      args.push_back(arg);\n+  }\n+  return args;\n+}\n+\n+} // namespace triton\n+} // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "added", "additions": 865, "deletions": 0, "changes": 865, "file_content_changes": "@@ -0,0 +1,865 @@\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/TypeUtilities.h\"\n+\n+#include \"ConvertLayoutOpToLLVM.h\"\n+#include \"LoadStoreOpToLLVM.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+// Contains some helper functions for both Load and Store conversions.\n+struct LoadStoreConversionBase {\n+  explicit LoadStoreConversionBase(AxisInfoAnalysis &axisAnalysisPass)\n+      : axisAnalysisPass(axisAnalysisPass) {}\n+\n+  unsigned getContiguity(Value ptr) const {\n+    auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return 1;\n+    return axisAnalysisPass.getPtrContiguity(ptr);\n+  }\n+\n+  unsigned getVectorSize(Value ptr) const {\n+    auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return 1;\n+    auto contiguity = getContiguity(ptr);\n+    auto pointeeBitWidth = triton::getPointeeBitWidth(tensorTy);\n+    // The maximum vector size is 128 bits on NVIDIA GPUs.\n+    return std::min<unsigned>(128 / pointeeBitWidth, contiguity);\n+  }\n+\n+  unsigned getMaskAlignment(Value mask) const {\n+    return axisAnalysisPass.getMaskAlignment(mask);\n+  }\n+\n+protected:\n+  AxisInfoAnalysis &axisAnalysisPass;\n+};\n+\n+struct LoadOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>,\n+      public LoadStoreConversionBase {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::LoadOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LoadOpConversion(TritonGPUToLLVMTypeConverter &converter,\n+                   AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>(converter, benefit),\n+        LoadStoreConversionBase(axisAnalysisPass) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op->getLoc();\n+\n+    // original values\n+    Value ptr = op.getPtr();\n+    Value mask = op.getMask();\n+    Value other = op.getOther();\n+\n+    // adaptor values\n+    Value llPtr = adaptor.getPtr();\n+    Value llMask = adaptor.getMask();\n+    Value llOther = adaptor.getOther();\n+\n+    // Determine the vectorization size\n+    Type valueTy = op.getResult().getType();\n+    Type valueElemTy =\n+        typeConverter->convertType(getElementTypeOrSelf(valueTy));\n+    unsigned vec = getVectorSize(ptr);\n+    unsigned numElems = getElemsPerThread(ptr.getType());\n+    if (llMask)\n+      vec = std::min<size_t>(vec, getMaskAlignment(mask));\n+\n+    // Get the LLVM values for pointers\n+    auto ptrElems = getTypeConverter()->unpackLLElements(loc, llPtr, rewriter,\n+                                                         ptr.getType());\n+    assert(ptrElems.size() == numElems);\n+\n+    // Get the LLVM values for mask\n+    SmallVector<Value> maskElems;\n+    if (llMask) {\n+      maskElems = getTypeConverter()->unpackLLElements(loc, llMask, rewriter,\n+                                                       mask.getType());\n+      assert(maskElems.size() == numElems);\n+    }\n+\n+    // Get the LLVM values for `other`\n+    // TODO: (goostavz) handle when other is const but not splat, which\n+    //       should be rarely seen\n+    bool otherIsSplatConstInt = false;\n+    DenseElementsAttr constAttr;\n+    int64_t splatVal = 0;\n+    if (other && valueElemTy.isa<IntegerType>() &&\n+        matchPattern(other, m_Constant(&constAttr)) && constAttr.isSplat() &&\n+        constAttr.getElementType().isa<IntegerType>()) {\n+      otherIsSplatConstInt = true;\n+      splatVal = constAttr.getSplatValue<APInt>().getSExtValue();\n+    }\n+    SmallVector<Value> otherElems;\n+    if (other) {\n+      otherElems = getTypeConverter()->unpackLLElements(loc, llOther, rewriter,\n+                                                        other.getType());\n+    }\n+\n+    // vectorized iteration through all the pointer/mask/other elements\n+    const int valueElemNBits =\n+        std::max(8u, valueElemTy.getIntOrFloatBitWidth());\n+    const int numVecs = numElems / vec;\n+\n+    SmallVector<Value> loadedVals;\n+    for (size_t vecStart = 0; vecStart < numElems; vecStart += vec) {\n+      // TODO: optimization when ptr is GEP with constant offset\n+      size_t in_off = 0;\n+\n+      const size_t maxWordWidth = std::max<size_t>(32, valueElemNBits);\n+      const size_t totalWidth = valueElemNBits * vec;\n+      const size_t width = std::min(totalWidth, maxWordWidth);\n+      const size_t nWords = std::max<size_t>(1, totalWidth / width);\n+      const size_t wordNElems = width / valueElemNBits;\n+      const size_t movWidth = width < 16 ? 16 : width;\n+      assert(wordNElems * nWords * numVecs == numElems);\n+\n+      // TODO(Superjomn) Add cache policy fields to StoreOp.\n+      // TODO(Superjomn) Deal with cache policy here.\n+      const bool hasL2EvictPolicy = false;\n+\n+      PTXBuilder ptxBuilder;\n+\n+      Value pred = mask ? maskElems[vecStart] : int_val(1, 1);\n+\n+      const std::string readConstraint =\n+          (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n+      const std::string writeConstraint =\n+          (width == 64) ? \"=l\" : ((width == 32) ? \"=r\" : \"=c\");\n+\n+      // prepare asm operands\n+      auto *dstsOpr = ptxBuilder.newListOperand();\n+      for (size_t wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n+        auto *opr = ptxBuilder.newOperand(writeConstraint,\n+                                          /*init=*/true); // =r operations\n+        dstsOpr->listAppend(opr);\n+      }\n+\n+      auto *addrOpr =\n+          ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n+\n+      // Define the instruction opcode\n+      auto &ld = ptxBuilder.create<>(\"ld\")\n+                     ->o(\"volatile\", op.getIsVolatile())\n+                     .global()\n+                     .o(\"ca\", op.getCache() == triton::CacheModifier::CA)\n+                     .o(\"cg\", op.getCache() == triton::CacheModifier::CG)\n+                     .o(\"L1::evict_first\",\n+                        op.getEvict() == triton::EvictionPolicy::EVICT_FIRST)\n+                     .o(\"L1::evict_last\",\n+                        op.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n+                     .o(\"L1::cache_hint\", hasL2EvictPolicy)\n+                     .v(nWords)\n+                     .b(width);\n+\n+      PTXBuilder::Operand *evictOpr{};\n+\n+      // Here lack a mlir::Value to bind to this operation, so disabled.\n+      // if (has_l2_evict_policy)\n+      //   evictOpr = ptxBuilder.newOperand(l2Evict, \"l\");\n+\n+      if (!evictOpr)\n+        ld(dstsOpr, addrOpr).predicate(pred, \"b\");\n+      else\n+        ld(dstsOpr, addrOpr, evictOpr).predicate(pred, \"b\");\n+\n+      if (other) {\n+        for (size_t ii = 0; ii < nWords; ++ii) {\n+          // PTX doesn't support mov.u8, so we need to use mov.u16\n+          PTXInstr &mov =\n+              ptxBuilder.create<>(\"mov\")->o(\"u\" + std::to_string(movWidth));\n+\n+          size_t size = width / valueElemNBits;\n+\n+          auto vecTy = LLVM::getFixedVectorType(valueElemTy, size);\n+          Value v = undef(vecTy);\n+          for (size_t s = 0; s < size; ++s) {\n+            Value falseVal = otherElems[vecStart + ii * size + s];\n+            Value sVal = createIndexAttrConstant(\n+                rewriter, loc, this->getTypeConverter()->getIndexType(), s);\n+            v = insert_element(vecTy, v, falseVal, sVal);\n+          }\n+          v = bitcast(v, IntegerType::get(getContext(), width));\n+\n+          PTXInstr::Operand *opr{};\n+\n+          if (otherIsSplatConstInt) {\n+            for (size_t s = 0; s < 32; s += valueElemNBits)\n+              splatVal |= splatVal << valueElemNBits;\n+            opr = ptxBuilder.newConstantOperand(splatVal);\n+          } else\n+            opr = ptxBuilder.newOperand(v, readConstraint);\n+\n+          mov(dstsOpr->listGet(ii), opr).predicateNot(pred, \"b\");\n+        }\n+      }\n+\n+      // Create inline ASM signature\n+      SmallVector<Type> retTys(nWords, IntegerType::get(getContext(), width));\n+      Type retTy = retTys.size() > 1\n+                       ? LLVM::LLVMStructType::getLiteral(getContext(), retTys)\n+                       : retTys[0];\n+\n+      // TODO: if (has_l2_evict_policy)\n+      // auto asmDialectAttr =\n+      // LLVM::AsmDialectAttr::get(rewriter.getContext(),\n+      //                                                 LLVM::AsmDialect::AD_ATT);\n+      Value ret = ptxBuilder.launch(rewriter, loc, retTy);\n+\n+      // Extract and store return values\n+      SmallVector<Value> rets;\n+      for (unsigned int ii = 0; ii < nWords; ++ii) {\n+        Value curr;\n+        if (retTy.isa<LLVM::LLVMStructType>()) {\n+          curr = extract_val(IntegerType::get(getContext(), width), ret, ii);\n+        } else {\n+          curr = ret;\n+        }\n+        curr = bitcast(curr, LLVM::getFixedVectorType(valueElemTy,\n+                                                      width / valueElemNBits));\n+        rets.push_back(curr);\n+      }\n+      int tmp = width / valueElemNBits;\n+      for (size_t ii = 0; ii < vec; ++ii) {\n+        Value vecIdx = createIndexAttrConstant(\n+            rewriter, loc, this->getTypeConverter()->getIndexType(), ii % tmp);\n+        Value loaded = extract_element(valueElemTy, rets[ii / tmp], vecIdx);\n+        loadedVals.push_back(loaded);\n+      }\n+    } // end vec\n+\n+    Type llvmResultStructTy = getTypeConverter()->convertType(valueTy);\n+    Value resultStruct = getTypeConverter()->packLLElements(\n+        loc, loadedVals, rewriter, llvmResultStructTy);\n+    rewriter.replaceOp(op, {resultStruct});\n+    return success();\n+  }\n+};\n+\n+struct StoreOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::StoreOp>,\n+      public LoadStoreConversionBase {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::StoreOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  StoreOpConversion(TritonGPUToLLVMTypeConverter &converter,\n+                    AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::StoreOp>(converter, benefit),\n+        LoadStoreConversionBase(axisAnalysisPass) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::StoreOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Value ptr = op.getPtr();\n+    Value value = op.getValue();\n+\n+    Value llPtr = adaptor.getPtr();\n+    Value llMask = adaptor.getMask();\n+    Value llValue = adaptor.getValue();\n+\n+    auto loc = op->getLoc();\n+    MLIRContext *ctx = rewriter.getContext();\n+\n+    auto valueTy = value.getType();\n+    Type valueElemTy =\n+        typeConverter->convertType(getElementTypeOrSelf(valueTy));\n+\n+    unsigned vec = getVectorSize(ptr);\n+    unsigned elemsPerThread = getElemsPerThread(ptr.getType());\n+\n+    auto ptrElems = getTypeConverter()->unpackLLElements(loc, llPtr, rewriter,\n+                                                         ptr.getType());\n+    auto valueElems = getTypeConverter()->unpackLLElements(\n+        loc, llValue, rewriter, value.getType());\n+    assert(ptrElems.size() == valueElems.size());\n+\n+    // Determine the vectorization size\n+    SmallVector<Value> maskElems;\n+    if (llMask) {\n+      Value mask = op.getMask();\n+      maskElems = getTypeConverter()->unpackLLElements(loc, llMask, rewriter,\n+                                                       mask.getType());\n+      assert(valueElems.size() == maskElems.size());\n+\n+      unsigned maskAlign = getMaskAlignment(mask);\n+      vec = std::min(vec, maskAlign);\n+    }\n+\n+    // numElements = 1 for scalar\n+    auto tensorTy = valueTy.dyn_cast<RankedTensorType>();\n+    auto numElems = tensorTy ? tensorTy.getNumElements() : 1;\n+    Value mask = int_val(1, 1);\n+    auto tid = tid_val();\n+    mask = and_(mask,\n+                icmp_slt(mul(tid, i32_val(elemsPerThread)), i32_val(numElems)));\n+\n+    const size_t dtsize =\n+        std::max<int>(1, valueElemTy.getIntOrFloatBitWidth() / 8);\n+    const size_t valueElemNBits = dtsize * 8;\n+\n+    const int numVecs = elemsPerThread / vec;\n+    for (size_t vecStart = 0; vecStart < elemsPerThread; vecStart += vec) {\n+      // TODO: optimization when ptr is AddPtr with constant offset\n+      size_t in_off = 0;\n+\n+      const size_t maxWordWidth = std::max<size_t>(32, valueElemNBits);\n+      const size_t totalWidth = valueElemNBits * vec;\n+      const size_t width = std::min(totalWidth, maxWordWidth);\n+      const size_t nWords = std::max<size_t>(1, totalWidth / width);\n+      const size_t wordNElems = width / valueElemNBits;\n+      assert(wordNElems * nWords * numVecs == elemsPerThread);\n+\n+      // TODO(Superjomn) Add cache policy fields to StoreOp.\n+      // TODO(Superjomn) Deal with cache policy here.\n+\n+      Type valArgTy = IntegerType::get(ctx, width);\n+      auto wordTy = vec_ty(valueElemTy, wordNElems);\n+\n+      SmallVector<std::pair<Value, std::string>> asmArgs;\n+      for (size_t wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n+        // llWord is a width-len composition\n+        Value llWord = undef(wordTy);\n+        // Insert each value element to the composition\n+        for (size_t elemIdx = 0; elemIdx < wordNElems; ++elemIdx) {\n+          const size_t elemOffset = vecStart + wordIdx * wordNElems + elemIdx;\n+          assert(elemOffset < valueElems.size());\n+          Value elem = valueElems[elemOffset];\n+          if (elem.getType().isInteger(1))\n+            elem = sext(i8_ty, elem);\n+          elem = bitcast(elem, valueElemTy);\n+\n+          llWord = insert_element(wordTy, llWord, elem, i32_val(elemIdx));\n+        }\n+        llWord = bitcast(llWord, valArgTy);\n+        std::string constraint =\n+            (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n+        asmArgs.emplace_back(llWord, constraint);\n+      }\n+\n+      // Prepare the PTX inline asm.\n+      PTXBuilder ptxBuilder;\n+      auto *asmArgList = ptxBuilder.newListOperand(asmArgs);\n+\n+      Value maskVal = llMask ? and_(mask, maskElems[vecStart]) : mask;\n+\n+      auto *asmAddr =\n+          ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n+\n+      auto &ptxStoreInstr =\n+          ptxBuilder.create<>(\"st\")->global().v(nWords).b(width);\n+      ptxStoreInstr(asmAddr, asmArgList).predicate(maskVal, \"b\");\n+\n+      Type boolTy = getTypeConverter()->convertType(rewriter.getIntegerType(1));\n+      llvm::SmallVector<Type> argTys({boolTy, ptr.getType()});\n+      argTys.insert(argTys.end(), nWords, valArgTy);\n+\n+      auto asmReturnTy = void_ty(ctx);\n+\n+      ptxBuilder.launch(rewriter, loc, asmReturnTy);\n+    }\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+};\n+\n+struct AtomicCASOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::AtomicCASOp>,\n+      public LoadStoreConversionBase {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::AtomicCASOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  AtomicCASOpConversion(TritonGPUToLLVMTypeConverter &converter,\n+                        const Allocation *allocation, Value smem,\n+                        AxisInfoAnalysis &axisAnalysisPass,\n+                        PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::AtomicCASOp>(\n+            converter, allocation, smem, benefit),\n+        LoadStoreConversionBase(axisAnalysisPass) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::AtomicCASOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op.getLoc();\n+    MLIRContext *ctx = rewriter.getContext();\n+\n+    Value llPtr = adaptor.getPtr();\n+    Value llCmp = adaptor.getCmp();\n+    Value llVal = adaptor.getVal();\n+\n+    auto ptrElements = getTypeConverter()->unpackLLElements(\n+        loc, llPtr, rewriter, op.getPtr().getType());\n+    auto cmpElements = getTypeConverter()->unpackLLElements(\n+        loc, llCmp, rewriter, op.getCmp().getType());\n+    auto valElements = getTypeConverter()->unpackLLElements(\n+        loc, llVal, rewriter, op.getVal().getType());\n+\n+    auto TensorTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    Type valueElemTy =\n+        TensorTy ? getTypeConverter()->convertType(TensorTy.getElementType())\n+                 : op.getResult().getType();\n+    auto valueElemNBits = valueElemTy.getIntOrFloatBitWidth();\n+    auto tid = tid_val();\n+    Value pred = icmp_eq(tid, i32_val(0));\n+    PTXBuilder ptxBuilderMemfence;\n+    auto memfence = ptxBuilderMemfence.create<PTXInstr>(\"membar\")->o(\"gl\");\n+    memfence();\n+    auto ASMReturnTy = void_ty(ctx);\n+    ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n+\n+    Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+    atomPtr = bitcast(atomPtr, ptr_ty(valueElemTy, 3));\n+    Value casPtr = ptrElements[0];\n+    Value casCmp = cmpElements[0];\n+    Value casVal = valElements[0];\n+\n+    PTXBuilder ptxBuilderAtomicCAS;\n+    auto *dstOpr = ptxBuilderAtomicCAS.newOperand(\"=r\", /*init=*/true);\n+    auto *ptrOpr = ptxBuilderAtomicCAS.newAddrOperand(casPtr, \"l\");\n+    auto *cmpOpr = ptxBuilderAtomicCAS.newOperand(casCmp, \"r\");\n+    auto *valOpr = ptxBuilderAtomicCAS.newOperand(casVal, \"r\");\n+    auto &atom = *ptxBuilderAtomicCAS.create<PTXInstr>(\"atom\");\n+    atom.global().o(\"cas\").o(\"b32\");\n+    atom(dstOpr, ptrOpr, cmpOpr, valOpr).predicate(pred);\n+    auto old = ptxBuilderAtomicCAS.launch(rewriter, loc, valueElemTy);\n+    barrier();\n+\n+    PTXBuilder ptxBuilderStore;\n+    auto *dstOprStore = ptxBuilderStore.newAddrOperand(atomPtr, \"r\");\n+    auto *valOprStore = ptxBuilderStore.newOperand(old, \"r\");\n+    auto &st = *ptxBuilderStore.create<PTXInstr>(\"st\");\n+    st.shared().o(\"b32\");\n+    st(dstOprStore, valOprStore).predicate(pred);\n+    ptxBuilderStore.launch(rewriter, loc, ASMReturnTy);\n+    ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n+    barrier();\n+    Value ret = load(atomPtr);\n+    barrier();\n+    rewriter.replaceOp(op, {ret});\n+    return success();\n+  }\n+};\n+\n+struct AtomicRMWOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>,\n+      public LoadStoreConversionBase {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::AtomicRMWOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  AtomicRMWOpConversion(TritonGPUToLLVMTypeConverter &converter,\n+                        const Allocation *allocation, Value smem,\n+                        AxisInfoAnalysis &axisAnalysisPass,\n+                        PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>(\n+            converter, allocation, smem, benefit),\n+        LoadStoreConversionBase(axisAnalysisPass) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::AtomicRMWOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op.getLoc();\n+    MLIRContext *ctx = rewriter.getContext();\n+\n+    auto atomicRmwAttr = op.getAtomicRmwOp();\n+\n+    Value val = op.getVal();\n+    Value ptr = op.getPtr();\n+\n+    Value llPtr = adaptor.getPtr();\n+    Value llVal = adaptor.getVal();\n+    Value llMask = adaptor.getMask();\n+\n+    auto valElements = getTypeConverter()->unpackLLElements(\n+        loc, llVal, rewriter, val.getType());\n+    auto ptrElements = getTypeConverter()->unpackLLElements(\n+        loc, llPtr, rewriter, ptr.getType());\n+    SmallVector<Value> maskElements;\n+    if (llMask)\n+      maskElements = getTypeConverter()->unpackLLElements(\n+          loc, llMask, rewriter, op.getMask().getType());\n+\n+    auto tensorTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    Type valueElemTy =\n+        tensorTy ? getTypeConverter()->convertType(tensorTy.getElementType())\n+                 : op.getResult().getType();\n+    const size_t valueElemNBits = valueElemTy.getIntOrFloatBitWidth();\n+    auto elemsPerThread = getElemsPerThread(val.getType());\n+    // vec = 1, numElements = 1 for scalar\n+    auto vec = getVectorSize(ptr);\n+    int numElems = 1;\n+    // tensor\n+    if (tensorTy) {\n+      auto valTy = val.getType().cast<RankedTensorType>();\n+      vec = std::min<unsigned>(vec, valTy.getElementType().isF16() ? 2 : 1);\n+      // mask\n+      numElems = tensorTy.getNumElements();\n+    }\n+    Value mask = int_val(1, 1);\n+    auto tid = tid_val();\n+    mask = and_(mask,\n+                icmp_slt(mul(tid, i32_val(elemsPerThread)), i32_val(numElems)));\n+\n+    auto vecTy = vec_ty(valueElemTy, vec);\n+    SmallVector<Value> resultVals(elemsPerThread);\n+    for (size_t i = 0; i < elemsPerThread; i += vec) {\n+      Value rmwVal = undef(vecTy);\n+      for (int ii = 0; ii < vec; ++ii) {\n+        Value iiVal = createIndexAttrConstant(\n+            rewriter, loc, getTypeConverter()->getIndexType(), ii);\n+        rmwVal = insert_element(vecTy, rmwVal, valElements[i + ii], iiVal);\n+      }\n+\n+      Value rmwPtr = ptrElements[i];\n+      Value rmwMask = llMask ? and_(mask, maskElements[i]) : mask;\n+      std::string sTy;\n+      PTXBuilder ptxBuilderAtomicRMW;\n+      std::string tyId = valueElemNBits * vec == 64\n+                             ? \"l\"\n+                             : (valueElemNBits * vec == 32 ? \"r\" : \"h\");\n+      auto *dstOpr = ptxBuilderAtomicRMW.newOperand(\"=\" + tyId, /*init=*/true);\n+      auto *ptrOpr = ptxBuilderAtomicRMW.newAddrOperand(rmwPtr, \"l\");\n+      auto *valOpr = ptxBuilderAtomicRMW.newOperand(rmwVal, tyId);\n+\n+      auto &atom = ptxBuilderAtomicRMW.create<>(\"atom\")->global().o(\"gpu\");\n+      auto rmwOp = stringifyRMWOp(atomicRmwAttr).str();\n+      auto sBits = std::to_string(valueElemNBits);\n+      switch (atomicRmwAttr) {\n+      case RMWOp::AND:\n+        sTy = \"b\" + sBits;\n+        break;\n+      case RMWOp::OR:\n+        sTy = \"b\" + sBits;\n+        break;\n+      case RMWOp::XOR:\n+        sTy = \"b\" + sBits;\n+        break;\n+      case RMWOp::ADD:\n+        sTy = \"s\" + sBits;\n+        break;\n+      case RMWOp::FADD:\n+        rmwOp = \"add\";\n+        rmwOp += (valueElemNBits == 16 ? \".noftz\" : \"\");\n+        sTy = \"f\" + sBits;\n+        sTy += (vec == 2 && valueElemNBits == 16) ? \"x2\" : \"\";\n+        break;\n+      case RMWOp::MAX:\n+        sTy = \"s\" + sBits;\n+        break;\n+      case RMWOp::MIN:\n+        sTy = \"s\" + sBits;\n+        break;\n+      case RMWOp::UMAX:\n+        rmwOp = \"max\";\n+        sTy = \"u\" + sBits;\n+        break;\n+      case RMWOp::UMIN:\n+        rmwOp = \"min\";\n+        sTy = \"u\" + sBits;\n+        break;\n+      case RMWOp::XCHG:\n+        sTy = \"b\" + sBits;\n+        break;\n+      default:\n+        return failure();\n+      }\n+      atom.o(rmwOp).o(sTy);\n+      if (tensorTy) {\n+        atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n+        auto retType = vec == 1 ? valueElemTy : vecTy;\n+        auto ret = ptxBuilderAtomicRMW.launch(rewriter, loc, retType);\n+        for (int ii = 0; ii < vec; ++ii) {\n+          resultVals[i + ii] =\n+              vec == 1 ? ret : extract_element(valueElemTy, ret, i32_val(ii));\n+        }\n+      } else {\n+        PTXBuilder ptxBuilderMemfence;\n+        auto memfenc = ptxBuilderMemfence.create<PTXInstr>(\"membar\")->o(\"gl\");\n+        memfenc();\n+        auto ASMReturnTy = void_ty(ctx);\n+        ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n+        rmwMask = and_(rmwMask, icmp_eq(tid, i32_val(0)));\n+        atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n+        auto old = ptxBuilderAtomicRMW.launch(rewriter, loc, valueElemTy);\n+        Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+        atomPtr = bitcast(atomPtr, ptr_ty(valueElemTy, 3));\n+        // Only threads with rmwMask = True store the result\n+        PTXBuilder ptxBuilderStore;\n+        auto &storeShared =\n+            ptxBuilderStore.create<>(\"st\")->shared().o(\"b\" + sBits);\n+        auto *ptrOpr = ptxBuilderStore.newAddrOperand(atomPtr, \"r\");\n+        auto *valOpr = ptxBuilderStore.newOperand(old, tyId);\n+        storeShared(ptrOpr, valOpr).predicate(rmwMask);\n+        ptxBuilderStore.launch(rewriter, loc, void_ty(ctx));\n+        barrier();\n+        Value ret = load(atomPtr);\n+        barrier();\n+        rewriter.replaceOp(op, {ret});\n+      }\n+    }\n+    if (tensorTy) {\n+      Type structTy = getTypeConverter()->convertType(tensorTy);\n+      Value resultStruct = getTypeConverter()->packLLElements(\n+          loc, resultVals, rewriter, structTy);\n+      rewriter.replaceOp(op, {resultStruct});\n+    }\n+    return success();\n+  }\n+};\n+\n+struct InsertSliceOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<tensor::InsertSliceOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      tensor::InsertSliceOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(tensor::InsertSliceOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    // %dst = insert_slice %src into %dst[%offsets]\n+    Location loc = op->getLoc();\n+    Value dst = op.getDest();\n+    Value src = op.getSource();\n+    Value res = op.getResult();\n+    assert(allocation->getBufferId(res) == Allocation::InvalidBufferId &&\n+           \"Only support in-place insert_slice for now\");\n+\n+    auto srcTy = src.getType().dyn_cast<RankedTensorType>();\n+    auto srcLayout = srcTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n+    auto srcShape = srcTy.getShape();\n+    assert(srcLayout && \"Unexpected srcLayout in InsertSliceOpConversion\");\n+\n+    auto dstTy = dst.getType().dyn_cast<RankedTensorType>();\n+    auto dstLayout = dstTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n+    auto llDst = adaptor.getDest();\n+    assert(dstLayout && \"Unexpected dstLayout in InsertSliceOpConversion\");\n+    assert(op.hasUnitStride() &&\n+           \"Only unit stride supported by InsertSliceOpConversion\");\n+\n+    // newBase = base + offset\n+    // Triton support either static and dynamic offsets\n+    auto smemObj = getSharedMemoryObjectFromStruct(loc, llDst, rewriter);\n+    SmallVector<Value, 4> offsets;\n+    SmallVector<Value, 4> srcStrides;\n+    auto mixedOffsets = op.getMixedOffsets();\n+    for (auto i = 0; i < mixedOffsets.size(); ++i) {\n+      if (op.isDynamicOffset(i)) {\n+        offsets.emplace_back(adaptor.getOffsets()[i]);\n+      } else {\n+        offsets.emplace_back(i32_val(op.getStaticOffset(i)));\n+      }\n+      // Like insert_slice_async, we only support slice from one dimension,\n+      // which has a slice size of 1\n+      if (op.getStaticSize(i) != 1) {\n+        srcStrides.emplace_back(smemObj.strides[i]);\n+      }\n+    }\n+\n+    // Compute the offset based on the original strides of the shared memory\n+    // object\n+    auto offset = dot(rewriter, loc, offsets, smemObj.strides);\n+    auto elemTy = getTypeConverter()->convertType(dstTy.getElementType());\n+    auto elemPtrTy = ptr_ty(elemTy, 3);\n+    auto smemBase = gep(elemPtrTy, smemObj.base, offset);\n+\n+    auto llSrc = adaptor.getSource();\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy);\n+    storeDistributedToShared(src, llSrc, srcStrides, srcIndices, dst, smemBase,\n+                             elemTy, loc, rewriter);\n+    // Barrier is not necessary.\n+    // The membar pass knows that it writes to shared memory and will handle it\n+    // properly.\n+    rewriter.replaceOp(op, llDst);\n+    return success();\n+  }\n+};\n+\n+struct InsertSliceAsyncOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::InsertSliceAsyncOp>,\n+      public LoadStoreConversionBase {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::gpu::InsertSliceAsyncOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  InsertSliceAsyncOpConversion(\n+      TritonGPUToLLVMTypeConverter &converter, const Allocation *allocation,\n+      Value smem,\n+      ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+      AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::gpu::InsertSliceAsyncOp>(\n+            converter, allocation, smem, indexCacheInfo, benefit),\n+        LoadStoreConversionBase(axisAnalysisPass) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::gpu::InsertSliceAsyncOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    // insert_slice_async %src, %dst, %index, %mask, %other\n+    auto loc = op.getLoc();\n+    Value src = op.getSrc();\n+    Value dst = op.getDst();\n+    Value res = op.getResult();\n+    Value mask = op.getMask();\n+    Value other = op.getOther();\n+    assert(allocation->getBufferId(res) == Allocation::InvalidBufferId &&\n+           \"Only support in-place insert_slice_async for now\");\n+\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto resTy = dst.getType().cast<RankedTensorType>();\n+    auto resElemTy = getTypeConverter()->convertType(resTy.getElementType());\n+    auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+    auto resSharedLayout = resTy.getEncoding().cast<SharedEncodingAttr>();\n+    auto srcShape = srcTy.getShape();\n+    assert(srcShape.size() == 2 &&\n+           \"insert_slice_async: Unexpected rank of %src\");\n+\n+    Value llDst = adaptor.getDst();\n+    Value llSrc = adaptor.getSrc();\n+    Value llMask = adaptor.getMask();\n+    Value llOther = adaptor.getOther();\n+    Value llIndex = adaptor.getIndex();\n+\n+    // %src\n+    auto srcElems = getTypeConverter()->unpackLLElements(loc, llSrc, rewriter,\n+                                                         src.getType());\n+\n+    // %dst\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto dstShape = dstTy.getShape();\n+    auto smemObj = getSharedMemoryObjectFromStruct(loc, llDst, rewriter);\n+    auto axis = op->getAttrOfType<IntegerAttr>(\"axis\").getInt();\n+    SmallVector<Value, 4> offsetVals;\n+    SmallVector<Value, 4> srcStrides;\n+    for (auto i = 0; i < dstShape.size(); ++i) {\n+      if (i == axis) {\n+        offsetVals.emplace_back(llIndex);\n+      } else {\n+        offsetVals.emplace_back(i32_val(0));\n+        srcStrides.emplace_back(smemObj.strides[i]);\n+      }\n+    }\n+    // Compute the offset based on the original dimensions of the shared\n+    // memory object\n+    auto dstOffset = dot(rewriter, loc, offsetVals, smemObj.strides);\n+    auto dstPtrTy = ptr_ty(resElemTy, 3);\n+    Value dstPtrBase = gep(dstPtrTy, smemObj.base, dstOffset);\n+\n+    // %mask\n+    SmallVector<Value> maskElems;\n+    if (llMask) {\n+      maskElems = getTypeConverter()->unpackLLElements(loc, llMask, rewriter,\n+                                                       mask.getType());\n+      assert(srcElems.size() == maskElems.size());\n+    }\n+\n+    // %other\n+    SmallVector<Value> otherElems;\n+    if (llOther) {\n+      // FIXME(Keren): always assume other is 0 for now\n+      // It's not necessary for now because the pipeline pass will skip\n+      // generating insert_slice_async if the load op has any \"other\" tensor.\n+      // assert(false && \"insert_slice_async: Other value not supported yet\");\n+      otherElems = getTypeConverter()->unpackLLElements(loc, llOther, rewriter,\n+                                                        other.getType());\n+      assert(srcElems.size() == otherElems.size());\n+    }\n+\n+    // We don't use getVec() here because we are copying from memory to memory.\n+    // If contiguity > vector size, we can have one pointer maintaining the\n+    // start of the vector and the other pointer moving to the next vector.\n+    unsigned inVec = getContiguity(src);\n+    unsigned outVec = resSharedLayout.getVec();\n+    unsigned minVec = std::min(outVec, inVec);\n+    unsigned numElems = getElemsPerThread(srcTy);\n+    unsigned perPhase = resSharedLayout.getPerPhase();\n+    unsigned maxPhase = resSharedLayout.getMaxPhase();\n+    auto sizePerThread = srcBlockedLayout.getSizePerThread();\n+    auto threadsPerCTA = getThreadsPerCTA(srcBlockedLayout);\n+    auto inOrder = srcBlockedLayout.getOrder();\n+    DenseMap<unsigned, Value> sharedPtrs =\n+        getSwizzledSharedPtrs(loc, inVec, srcTy, resSharedLayout, resElemTy,\n+                              smemObj, rewriter, offsetVals, srcStrides);\n+\n+    // If perPhase * maxPhase > threadsPerCTA, we will have elements\n+    // that share the same tile indices. The index calculation will\n+    // be cached.\n+    auto numSwizzleRows = std::max<unsigned>(\n+        (perPhase * maxPhase) / threadsPerCTA[inOrder[1]], 1);\n+    // A sharedLayout encoding has a \"vec\" parameter.\n+    // On the column dimension, if inVec > outVec, it means we have to divide\n+    // single vector read into multiple ones\n+    auto numVecCols = std::max<unsigned>(inVec / outVec, 1);\n+\n+    auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcTy);\n+\n+    for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n+      // 16 * 8 = 128bits\n+      auto maxBitWidth =\n+          std::max<unsigned>(128, resElemTy.getIntOrFloatBitWidth());\n+      auto vecBitWidth = resElemTy.getIntOrFloatBitWidth() * minVec;\n+      auto bitWidth = std::min<unsigned>(maxBitWidth, vecBitWidth);\n+      auto numWords = vecBitWidth / bitWidth;\n+      auto numWordElems = bitWidth / resElemTy.getIntOrFloatBitWidth();\n+\n+      // Tune CG and CA here.\n+      auto byteWidth = bitWidth / 8;\n+      CacheModifier srcCacheModifier =\n+          byteWidth == 16 ? CacheModifier::CG : CacheModifier::CA;\n+      assert(byteWidth == 16 || byteWidth == 8 || byteWidth == 4);\n+      auto resByteWidth = resElemTy.getIntOrFloatBitWidth() / 8;\n+\n+      Value basePtr = sharedPtrs[elemIdx];\n+      for (size_t wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n+        PTXBuilder ptxBuilder;\n+        auto wordElemIdx = wordIdx * numWordElems;\n+        auto &copyAsyncOp =\n+            *ptxBuilder.create<PTXCpAsyncLoadInstr>(srcCacheModifier);\n+        auto *dstOperand =\n+            ptxBuilder.newAddrOperand(basePtr, \"r\", wordElemIdx * resByteWidth);\n+        auto *srcOperand =\n+            ptxBuilder.newAddrOperand(srcElems[elemIdx + wordElemIdx], \"l\");\n+        auto *copySize = ptxBuilder.newConstantOperand(byteWidth);\n+        auto *srcSize = copySize;\n+        if (op.getMask()) {\n+          // We don't use predicate in this case, setting src-size to 0\n+          // if there's any mask. cp.async will automatically fill the\n+          // remaining slots with 0 if cp-size > src-size.\n+          // XXX(Keren): Always assume other = 0 for now.\n+          auto selectOp = select(maskElems[elemIdx + wordElemIdx],\n+                                 i32_val(byteWidth), i32_val(0));\n+          srcSize = ptxBuilder.newOperand(selectOp, \"r\");\n+        }\n+        copyAsyncOp(dstOperand, srcOperand, copySize, srcSize);\n+        ptxBuilder.launch(rewriter, loc, void_ty(getContext()));\n+      }\n+    }\n+\n+    rewriter.replaceOp(op, llDst);\n+    return success();\n+  }\n+};\n+\n+void populateLoadStoreOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n+  patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n+  patterns.add<StoreOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n+  patterns.add<AtomicCASOpConversion>(typeConverter, allocation, smem,\n+                                      axisInfoAnalysis, benefit);\n+  patterns.add<AtomicRMWOpConversion>(typeConverter, allocation, smem,\n+                                      axisInfoAnalysis, benefit);\n+  patterns.add<InsertSliceOpConversion>(typeConverter, allocation, smem,\n+                                        indexCacheInfo, benefit);\n+  patterns.add<InsertSliceAsyncOpConversion>(typeConverter, allocation, smem,\n+                                             indexCacheInfo, axisInfoAnalysis,\n+                                             benefit);\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.h", "status": "added", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -0,0 +1,16 @@\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_LOAD_STORE_OP_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_LOAD_STORE_OP_H\n+\n+#include \"TritonGPUToLLVMBase.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+void populateLoadStoreOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n+\n+#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/PTXAsmFormat.cpp", "status": "added", "additions": 225, "deletions": 0, "changes": 225, "file_content_changes": "@@ -0,0 +1,225 @@\n+#include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/AsmFormat.h\"\n+#include \"llvm/Support/raw_ostream.h\"\n+// TODO(Superjomn): unify to llvm::raw_string_ostream\n+#include <sstream>\n+\n+namespace mlir {\n+namespace triton {\n+\n+PTXInstr::Operand *\n+PTXBuilder::newOperand(mlir::Value value, StringRef constraint,\n+                       std::function<std::string(int)> formatter) {\n+  argArchive.emplace_back(std::make_unique<Operand>(value, constraint));\n+  auto *opr = argArchive.back().get();\n+  opr->repr = formatter;\n+  opr->idx = oprCounter++;\n+  return opr;\n+}\n+\n+void PTXBuilder::initOperand(Operand *opr) {\n+  auto numBits = 0;\n+  // Derive numBits from the constraint.\n+  if (opr->constraint[1] == 'c' || opr->constraint[1] == 'h')\n+    numBits = 16;\n+  else if (opr->constraint[1] == 'r')\n+    numBits = 32;\n+  else if (opr->constraint[1] == 'l')\n+    numBits = 64;\n+  else\n+    llvm_unreachable((\"Unknown constraint: \" + opr->constraint).c_str());\n+  // If numBits is less than 16, we use 16 as default because PTX does not\n+  // support 8-bit mov.\n+  numBits = numBits < 16 ? 16 : numBits;\n+  auto *zero = newConstantOperand(0);\n+  auto &init = create<>(\"mov\")->o(\"u\" + std::to_string(numBits));\n+  init(opr, zero);\n+}\n+\n+PTXBuilder::Operand *PTXBuilder::newOperand(StringRef constraint, bool init) {\n+  // Constraint should be something like \"=r\"\n+  assert(constraint.size() == 2 && constraint[0] == '=');\n+  auto *opr = newOperand();\n+  opr->idx = oprCounter++;\n+  opr->constraint = constraint;\n+  if (init) {\n+    initOperand(opr);\n+  }\n+  return opr;\n+}\n+\n+PTXBuilder::Operand *PTXBuilder::newConstantOperand(const std::string &v) {\n+  argArchive.emplace_back(std::make_unique<Operand>());\n+  argArchive.back()->repr = [v](int idx) { return v; };\n+  return argArchive.back().get();\n+}\n+\n+PTXBuilder::Operand *PTXBuilder::newConstantOperand(int64_t v) {\n+  std::stringstream ss;\n+  ss << \"0x\" << std::hex << v;\n+  return newConstantOperand(ss.str());\n+}\n+\n+std::string PTXBuilder::getConstraints() const {\n+  auto args = getAllArgs();\n+  llvm::SmallVector<std::string, 4> argReprs;\n+  for (auto arg : args)\n+    argReprs.push_back(arg->constraint);\n+  return strJoin(argReprs, \",\");\n+}\n+\n+llvm::SmallVector<Value, 4> PTXBuilder::getAllMLIRArgs() const {\n+  llvm::SmallVector<Value, 4> res;\n+  for (auto &arg : argArchive) {\n+    if (!arg->isList() && arg->value)\n+      res.push_back(arg->value);\n+  }\n+  return res;\n+}\n+\n+SmallVector<PTXBuilder::Operand *, 4> PTXBuilder::getAllArgs() const {\n+  llvm::SmallVector<Operand *, 4> res;\n+  for (auto &x : argArchive)\n+    if (!x->isList())\n+      res.push_back(x.get());\n+  return res;\n+}\n+\n+mlir::Value PTXBuilder::launch(OpBuilder &rewriter, Location loc, Type resTy,\n+                               bool hasSideEffect, bool isAlignStack,\n+                               ArrayRef<Attribute> attrs) const {\n+  auto *ctx = rewriter.getContext();\n+  auto inlineAsm = rewriter.create<LLVM::InlineAsmOp>(\n+      loc, resTy, getAllMLIRArgs(), // operands\n+      dump(),                       // asm_string\n+      getConstraints(),             // constraints\n+      hasSideEffect,                // has_side_effects\n+      isAlignStack,                 // is_align_stack\n+      LLVM::AsmDialectAttr::get(ctx,\n+                                LLVM::AsmDialect::AD_ATT), // asm_dialect\n+      ArrayAttr::get(ctx, attrs)                           // operand_attrs\n+  );\n+\n+  return inlineAsm.getRes();\n+}\n+\n+std::string PTXInstr::Operand::dump() const {\n+  if (repr)\n+    return repr(idx);\n+  if (!isList())\n+    return \"$\" + std::to_string(idx);\n+\n+  llvm::SmallVector<std::string> oprs;\n+  for (auto *opr : list)\n+    oprs.push_back(opr->dump());\n+  return \"{ \" + strJoin(oprs, \", \") + \" }\";\n+}\n+\n+PTXInstr::Operand *PTXBuilder::newAddrOperand(mlir::Value addr,\n+                                              StringRef constraint, int off) {\n+  auto *opr = newOperand(addr, constraint);\n+  opr->repr = [off](int idx) -> std::string {\n+    std::stringstream ss;\n+    ss << \"[ $\" << idx << \" + \" << off << \" ]\";\n+    return ss.str();\n+  };\n+\n+  return opr;\n+}\n+\n+std::string PTXBuilder::dump() const {\n+  llvm::SmallVector<std::string> lines;\n+  for (auto &exec : executions) {\n+    lines.push_back(exec->dump());\n+  }\n+\n+  return strJoin(lines, \"\\n\\t\");\n+}\n+\n+PTXInstrExecution &PTXInstrCommon::call(ArrayRef<Operand *> oprs,\n+                                        bool onlyAttachMLIRArgs) {\n+  if (onlyAttachMLIRArgs) {\n+    // Nearly impossible to make the $0,$1 in two PTX code snippets to point to\n+    // the same MLIR values in onlyAttachMLIRArgs mode.\n+    assert(builder->executions.empty() &&\n+           \"builder can only hold a single execution when onlyAttachMIIRArgs \"\n+           \"is true.\");\n+    builder->reorderArgArchive(oprs);\n+  }\n+\n+  builder->executions.emplace_back(\n+      std::make_unique<PTXInstrExecution>(this, oprs, onlyAttachMLIRArgs));\n+\n+  return *builder->executions.back();\n+}\n+\n+PTXInstrExecution &PTXInstrCommon::operator()(ArrayRef<Operand *> oprs,\n+                                              bool onlyAttachMLIRArgs) {\n+  return call(oprs, onlyAttachMLIRArgs);\n+}\n+\n+std::string PTXInstrExecution::dump() const {\n+  std::string osStr;\n+  llvm::raw_string_ostream os(osStr);\n+\n+  std::string instrRepr = strJoin(instr->instrParts, \".\");\n+  if (onlyAttachMLIRArgs)\n+    return instrRepr;\n+\n+  if (pred) {\n+    if (!pred->repr)\n+      os << \"@\" << pred->dump() << \" \";\n+    else\n+      os << pred->repr(pred->idx) << \" \";\n+  }\n+\n+  llvm::SmallVector<std::string, 4> argReprs;\n+  for (auto *arg : argsInOrder) {\n+    argReprs.push_back(arg->dump());\n+  }\n+\n+  std::string argsRepr = strJoin(argReprs, \", \");\n+\n+  os << instrRepr << \" \" << argsRepr << \";\";\n+  os.flush();\n+  return osStr;\n+}\n+\n+SmallVector<PTXInstrExecution::Operand *>\n+PTXInstrExecution::getArgList() const {\n+  SmallVector<Operand *> args;\n+  for (auto *arg : argsInOrder) {\n+    if (arg->isList())\n+      args.insert(args.end(), arg->list.begin(), arg->list.end());\n+    else\n+      args.push_back(arg);\n+  }\n+  return args;\n+}\n+\n+PTXInstr &PTXInstr::global() {\n+  o(\"global\");\n+  return *this;\n+}\n+\n+PTXInstr &PTXInstr::shared() {\n+  o(\"shared\");\n+  return *this;\n+}\n+\n+PTXInstr &PTXInstr::v(int vecWidth, bool predicate) {\n+  if (vecWidth > 1) {\n+    o(\"v\" + std::to_string(vecWidth), predicate);\n+  }\n+  return *this;\n+}\n+\n+PTXInstr &PTXInstr::b(int width) {\n+  o(\"b\" + std::to_string(width));\n+  return *this;\n+}\n+\n+} // namespace triton\n+} // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.h", "status": "added", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "added", "additions": 635, "deletions": 0, "changes": 635, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h", "status": "added", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "added", "additions": 942, "deletions": 0, "changes": 942, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "added", "additions": 427, "deletions": 0, "changes": 427, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "added", "additions": 150, "deletions": 0, "changes": 150, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.h", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.cpp", "status": "added", "additions": 150, "deletions": 0, "changes": 150, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "added", "additions": 272, "deletions": 0, "changes": 272, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "added", "additions": 175, "deletions": 0, "changes": 175, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.h", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonToTritonGPU/CMakeLists.txt", "status": "added", "additions": 20, "deletions": 0, "changes": 20, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "added", "additions": 829, "deletions": 0, "changes": 829, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/CMakeLists.txt", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/CMakeLists.txt", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/IR/CMakeLists.txt", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/IR/Dialect.cpp", "status": "added", "additions": 87, "deletions": 0, "changes": 87, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/IR/Interfaces.cpp", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "added", "additions": 710, "deletions": 0, "changes": 710, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/IR/Traits.cpp", "status": "added", "additions": 143, "deletions": 0, "changes": 143, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/IR/Types.cpp", "status": "added", "additions": 111, "deletions": 0, "changes": 111, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/Transforms/CMakeLists.txt", "status": "added", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "added", "additions": 220, "deletions": 0, "changes": 220, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.td", "status": "added", "additions": 48, "deletions": 0, "changes": 48, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/Triton/Transforms/RewriteTensorPointer.cpp", "status": "added", "additions": 504, "deletions": 0, "changes": 504, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/CMakeLists.txt", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/IR/CMakeLists.txt", "status": "added", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "added", "additions": 1113, "deletions": 0, "changes": 1113, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/IR/Traits.cpp", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "added", "additions": 217, "deletions": 0, "changes": 217, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/CMakeLists.txt", "status": "added", "additions": 20, "deletions": 0, "changes": 20, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "added", "additions": 196, "deletions": 0, "changes": 196, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/DecomposeConversions.cpp", "status": "added", "additions": 68, "deletions": 0, "changes": 68, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "added", "additions": 105, "deletions": 0, "changes": 105, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "added", "additions": 796, "deletions": 0, "changes": 796, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "added", "additions": 319, "deletions": 0, "changes": 319, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "added", "additions": 655, "deletions": 0, "changes": 655, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/ReorderInstructions.cpp", "status": "added", "additions": 108, "deletions": 0, "changes": 108, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "added", "additions": 109, "deletions": 0, "changes": 109, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "added", "additions": 275, "deletions": 0, "changes": 275, "file_content_changes": "N/A"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.h", "status": "added", "additions": 33, "deletions": 0, "changes": 33, "file_content_changes": "N/A"}, {"filename": "lib/Target/CMakeLists.txt", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "N/A"}, {"filename": "lib/Target/HSACO/CMakeLists.txt", "status": "added", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "N/A"}, {"filename": "lib/Target/HSACO/HSACOTranslation.cpp", "status": "added", "additions": 182, "deletions": 0, "changes": 182, "file_content_changes": "N/A"}, {"filename": "lib/Target/LLVMIR/CMakeLists.txt", "status": "added", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "N/A"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "added", "additions": 371, "deletions": 0, "changes": 371, "file_content_changes": "N/A"}, {"filename": "lib/Target/PTX/CMakeLists.txt", "status": "added", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "N/A"}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "added", "additions": 113, "deletions": 0, "changes": 113, "file_content_changes": "N/A"}, {"filename": "lib/codegen/analysis/align.cc", "status": "removed", "additions": 0, "deletions": 582, "changes": 582, "file_content_changes": "N/A"}, {"filename": "lib/codegen/analysis/allocation.cc", "status": "removed", "additions": 0, "deletions": 101, "changes": 101, "file_content_changes": "N/A"}, {"filename": "lib/codegen/analysis/axes.cc", "status": "removed", "additions": 0, "deletions": 162, "changes": 162, "file_content_changes": "N/A"}, {"filename": "lib/codegen/analysis/layout.cc", "status": "removed", "additions": 0, "deletions": 657, "changes": 657, "file_content_changes": "N/A"}, {"filename": "lib/codegen/analysis/liveness.cc", "status": "removed", "additions": 0, "deletions": 59, "changes": 59, "file_content_changes": "N/A"}, {"filename": "lib/codegen/analysis/swizzle.cc", "status": "removed", "additions": 0, "deletions": 61, "changes": 61, "file_content_changes": "N/A"}, {"filename": "lib/codegen/pass.cc", "status": "removed", "additions": 0, "deletions": 98, "changes": 98, "file_content_changes": "N/A"}, {"filename": "lib/codegen/selection/generator.cc", "status": "removed", "additions": 0, "deletions": 3435, "changes": 3435, "file_content_changes": "N/A"}, {"filename": "lib/codegen/target.cc", "status": "removed", "additions": 0, "deletions": 173, "changes": 173, "file_content_changes": "N/A"}, {"filename": "lib/codegen/transform/coalesce.cc", "status": "removed", "additions": 0, "deletions": 152, "changes": 152, "file_content_changes": "N/A"}, {"filename": "lib/codegen/transform/cts.cc", "status": "removed", "additions": 0, "deletions": 97, "changes": 97, "file_content_changes": "N/A"}, {"filename": "lib/codegen/transform/dce.cc", "status": "removed", "additions": 0, "deletions": 79, "changes": 79, "file_content_changes": "N/A"}, {"filename": "lib/codegen/transform/disassociate.cc", "status": "removed", "additions": 0, "deletions": 62, "changes": 62, "file_content_changes": "N/A"}, {"filename": "lib/codegen/transform/inline.cc", "status": "removed", "additions": 0, "deletions": 127, "changes": 127, "file_content_changes": "N/A"}, {"filename": "lib/codegen/transform/membar.cc", "status": "removed", "additions": 0, "deletions": 244, "changes": 244, "file_content_changes": "N/A"}, {"filename": "lib/codegen/transform/peephole.cc", "status": "removed", "additions": 0, "deletions": 331, "changes": 331, "file_content_changes": "N/A"}, {"filename": "lib/codegen/transform/pipeline.cc", "status": "removed", "additions": 0, "deletions": 331, "changes": 331, "file_content_changes": "N/A"}, {"filename": "lib/codegen/transform/prefetch.cc", "status": "removed", "additions": 0, "deletions": 133, "changes": 133, "file_content_changes": "N/A"}, {"filename": "lib/codegen/transform/reorder.cc", "status": "removed", "additions": 0, "deletions": 51, "changes": 51, "file_content_changes": "N/A"}, {"filename": "lib/driver/dispatch.cc", "status": "removed", "additions": 0, "deletions": 302, "changes": 302, "file_content_changes": "N/A"}, {"filename": "lib/driver/error.cc", "status": "removed", "additions": 0, "deletions": 166, "changes": 166, "file_content_changes": "N/A"}, {"filename": "lib/driver/llvm.cc", "status": "removed", "additions": 0, "deletions": 365, "changes": 365, "file_content_changes": "N/A"}, {"filename": "lib/ir/basic_block.cc", "status": "removed", "additions": 0, "deletions": 88, "changes": 88, "file_content_changes": "N/A"}, {"filename": "lib/ir/builder.cc", "status": "removed", "additions": 0, "deletions": 470, "changes": 470, "file_content_changes": "N/A"}, {"filename": "lib/ir/constant.cc", "status": "removed", "additions": 0, "deletions": 118, "changes": 118, "file_content_changes": "N/A"}, {"filename": "lib/ir/context.cc", "status": "removed", "additions": 0, "deletions": 40, "changes": 40, "file_content_changes": "N/A"}, {"filename": "lib/ir/function.cc", "status": "removed", "additions": 0, "deletions": 66, "changes": 66, "file_content_changes": "N/A"}, {"filename": "lib/ir/instructions.cc", "status": "removed", "additions": 0, "deletions": 1019, "changes": 1019, "file_content_changes": "N/A"}, {"filename": "lib/ir/metadata.cc", "status": "removed", "additions": 0, "deletions": 14, "changes": 14, "file_content_changes": "N/A"}, {"filename": "lib/ir/module.cc", "status": "removed", "additions": 0, "deletions": 27, "changes": 27, "file_content_changes": "N/A"}, {"filename": "lib/ir/print.cc", "status": "removed", "additions": 0, "deletions": 450, "changes": 450, "file_content_changes": "N/A"}, {"filename": "lib/ir/type.cc", "status": "removed", "additions": 0, "deletions": 252, "changes": 252, "file_content_changes": "N/A"}, {"filename": "lib/ir/utils.cc", "status": "removed", "additions": 0, "deletions": 68, "changes": 68, "file_content_changes": "N/A"}, {"filename": "lib/ir/value.cc", "status": "removed", "additions": 0, "deletions": 82, "changes": 82, "file_content_changes": "N/A"}, {"filename": "python/MANIFEST.in", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "N/A"}, {"filename": "python/bench/README.md", "status": "removed", "additions": 0, "deletions": 5, "changes": 5, "file_content_changes": "N/A"}, {"filename": "python/bench/bench_blocksparse.py", "status": "removed", "additions": 0, "deletions": 92, "changes": 92, "file_content_changes": "N/A"}, {"filename": "python/bench/bench_cross_entropy.py", "status": "removed", "additions": 0, "deletions": 41, "changes": 41, "file_content_changes": "N/A"}, {"filename": "python/bench/bench_matmul.py", "status": "removed", "additions": 0, "deletions": 67, "changes": 67, "file_content_changes": "N/A"}, {"filename": "python/bench/requirements-bench.txt", "status": "removed", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "N/A"}, {"filename": "python/bench/run.py", "status": "removed", "additions": 0, "deletions": 44, "changes": 44, "file_content_changes": "N/A"}, {"filename": "python/examples/copy_strided.py", "status": "added", "additions": 18, "deletions": 0, "changes": 18, "file_content_changes": "N/A"}, {"filename": "python/examples/empty.py", "status": "added", "additions": 13, "deletions": 0, "changes": 13, "file_content_changes": "N/A"}, {"filename": "python/pyproject.toml", "status": "added", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "N/A"}, {"filename": "python/setup.cfg", "status": "removed", "additions": 0, "deletions": 8, "changes": 8, "file_content_changes": "N/A"}, {"filename": "python/setup.py", "status": "modified", "additions": 165, "deletions": 56, "changes": 221, "file_content_changes": "N/A"}, {"filename": "python/src/cutlass.cc", "status": "removed", "additions": 0, "deletions": 202, "changes": 202, "file_content_changes": "N/A"}, {"filename": "python/src/extra/cuda.ll", "status": "added", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "N/A"}, {"filename": "python/src/functions.h", "status": "removed", "additions": 0, "deletions": 676, "changes": 676, "file_content_changes": "N/A"}, {"filename": "python/src/main.cc", "status": "modified", "additions": 1, "deletions": 5, "changes": 6, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/attr.h", "status": "removed", "additions": 0, "deletions": 493, "changes": 493, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/buffer_info.h", "status": "removed", "additions": 0, "deletions": 108, "changes": 108, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/cast.h", "status": "removed", "additions": 0, "deletions": 2128, "changes": 2128, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/chrono.h", "status": "removed", "additions": 0, "deletions": 162, "changes": 162, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/common.h", "status": "removed", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/complex.h", "status": "removed", "additions": 0, "deletions": 65, "changes": 65, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/detail/class.h", "status": "removed", "additions": 0, "deletions": 623, "changes": 623, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/detail/common.h", "status": "removed", "additions": 0, "deletions": 807, "changes": 807, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/detail/descr.h", "status": "removed", "additions": 0, "deletions": 100, "changes": 100, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/detail/init.h", "status": "removed", "additions": 0, "deletions": 335, "changes": 335, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/detail/internals.h", "status": "removed", "additions": 0, "deletions": 293, "changes": 293, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/detail/typeid.h", "status": "removed", "additions": 0, "deletions": 55, "changes": 55, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/eigen.h", "status": "removed", "additions": 0, "deletions": 607, "changes": 607, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/embed.h", "status": "removed", "additions": 0, "deletions": 200, "changes": 200, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/eval.h", "status": "removed", "additions": 0, "deletions": 117, "changes": 117, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/functional.h", "status": "removed", "additions": 0, "deletions": 108, "changes": 108, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/iostream.h", "status": "removed", "additions": 0, "deletions": 207, "changes": 207, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/numpy.h", "status": "removed", "additions": 0, "deletions": 1610, "changes": 1610, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/operators.h", "status": "removed", "additions": 0, "deletions": 168, "changes": 168, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/options.h", "status": "removed", "additions": 0, "deletions": 65, "changes": 65, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/pybind11.h", "status": "removed", "additions": 0, "deletions": 2162, "changes": 2162, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/pytypes.h", "status": "removed", "additions": 0, "deletions": 1471, "changes": 1471, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/stl.h", "status": "removed", "additions": 0, "deletions": 386, "changes": 386, "file_content_changes": "N/A"}, {"filename": "python/src/pybind11/stl_bind.h", "status": "removed", "additions": 0, "deletions": 630, "changes": 630, "file_content_changes": "N/A"}, {"filename": "python/src/superblock.cc", "status": "removed", "additions": 0, "deletions": 119, "changes": 119, "file_content_changes": "N/A"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 1620, "deletions": 908, "changes": 2528, "file_content_changes": "N/A"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 85, "deletions": 37, "changes": 122, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/assert_helper.py", "status": "added", "additions": 45, "deletions": 0, "changes": 45, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/print_helper.py", "status": "added", "additions": 46, "deletions": 0, "changes": 46, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/test_block_pointer.py", "status": "added", "additions": 102, "deletions": 0, "changes": 102, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 1459, "deletions": 157, "changes": 1616, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/test_random.py", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/test_subprocess.py", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "N/A"}, {"filename": "python/test/unit/operators/test_blocksparse.py", "status": "modified", "additions": 53, "deletions": 21, "changes": 74, "file_content_changes": "N/A"}, {"filename": "python/test/unit/operators/test_cross_entropy.py", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "N/A"}, {"filename": "python/test/unit/operators/test_flash_attention.py", "status": "added", "additions": 46, "deletions": 0, "changes": 46, "file_content_changes": "N/A"}]