[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -2447,7 +2447,7 @@ struct DotOpConversionHelper {\n     return {};\n   }\n \n-  void deduceMmaType(DotOp op) { mmaType = getMmaType(op); }\n+  void deduceMmaType(DotOp op) const { mmaType = getMmaType(op); }\n \n   Type getShemPtrTy() const {\n     switch (mmaType) {\n@@ -2591,7 +2591,7 @@ struct DotOpConversionHelper {\n   }\n \n private:\n-  TensorCoreType mmaType;\n+  mutable TensorCoreType mmaType;\n \n   // Used on nvidia GPUs mma layout .version == 2\n   // Refer to\n@@ -2684,7 +2684,6 @@ struct MMA16816ConversionHelper {\n       : mmaLayout(mmaLayout), helper(mmaLayout), rewriter(rewriter),\n         typeConverter(typeConverter), loc(loc), ctx(mmaLayout.getContext()),\n         thread(thread) {\n-\n     wpt = mmaLayout.getWarpsPerCTA();\n \n     auto mmaInstrShape = helper.getMmaInstrShape();\n@@ -2784,6 +2783,8 @@ struct MMA16816ConversionHelper {\n     Value c = op.c();\n     Value d = op.d();\n \n+    helper.deduceMmaType(op);\n+\n     auto aTensorTy = a.getType().cast<RankedTensorType>();\n     auto bTensorTy = b.getType().cast<RankedTensorType>();\n     auto cTensorTy = c.getType().cast<RankedTensorType>();"}]