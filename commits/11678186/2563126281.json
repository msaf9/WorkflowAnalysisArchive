[{"filename": "include/triton/codegen/transform/coalesce.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -32,11 +32,12 @@ class coalesce {\n   ir::value* rematerialize(ir::value *v, ir::builder& builder, std::map<ir::value*, ir::value*>& seen);\n \n public:\n-  coalesce(analysis::align* align, triton::codegen::analysis::layouts *layouts);\n+  coalesce(analysis::align* align, triton::codegen::analysis::layouts *layouts, bool has_sm80);\n   triton::ir::value *simplify(ir::instruction* i, triton::ir::builder &builder);\n   void run(ir::module &mod);\n \n private:\n+  bool has_sm80_;\n   analysis::align* align_;\n   analysis::layouts* layout_;\n };"}, {"filename": "lib/codegen/analysis/swizzle.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -37,7 +37,7 @@ void swizzle::run(ir::module &) {\n       if(tgt_->as_nvidia() && tgt_->as_nvidia()->sm() < 80){\n         int inner = mma_dot_a ? 0 : 1;\n         per_phase_[layout] = per_phase;\n-        max_phase_[layout] = (ord[inner] == 1 ? 4 : 4) / per_phase_[layout];\n+        max_phase_[layout] = (ord[inner] == 1 ? 8 : 4) / per_phase_[layout];\n         if(mma_dot_a)\n           vec_[layout] = 2*layouts_->get(mma_dot_a)->to_mma()->rep(0);\n         else"}, {"filename": "lib/codegen/pass.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -45,7 +45,7 @@ std::unique_ptr<llvm::Module> add_passes_to_emit_bin(ir::module &ir, llvm::LLVMC\n   codegen::analysis::allocation allocation(&liveness);\n   codegen::transform::dce dce;\n   codegen::transform::peephole peephole(target, &layouts);\n-  codegen::transform::coalesce coalesce(&align, &layouts);\n+  codegen::transform::coalesce coalesce(&align, &layouts, has_sm80);\n   codegen::transform::prefetch prefetch_s(target);\n   codegen::transform::membar barriers(&liveness, &layouts, &allocation, &prefetch_s, target);\n   codegen::generator isel(&axes, &layouts, &align, &allocation, &swizzle, target, num_warps);"}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 8, "deletions": 4, "changes": 12, "file_content_changes": "@@ -955,8 +955,14 @@ void generator::visit_store_inst(ir::store_inst * x){\n       max_eq = std::max<size_t>(max_eq, 1);\n       aln = std::min(aln, max_eq);\n     }\n-    vec  = std::min(nts, aln);\n-    if(x->get_eviction_policy() != ir::store_inst::EVICTION_POLICY::NORMAL)\n+    analysis::distributed_layout* layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(ptr_op));\n+    assert(layout);\n+    // vec  = std::min(nts, aln);\n+    vec = std::min<size_t>(layout->contig_per_thread(ord[0]), aln);\n+    // TODO: generalize\n+    bool is_mma_first_row = (ord.size() >= 1) && layout->to_mma() && \n+                       (a_axes_->get(ptr_op, ord[0]) == layouts_->get(ptr_op)->get_axis(1));\n+    if(is_mma_first_row)\n       vec = std::min<size_t>(2, aln);\n   }\n   bool has_l2_evict_policy = (x->get_eviction_policy() != ir::load_inst::NORMAL) && tgt_->as_nvidia()->sm() >= 80;\n@@ -1640,8 +1646,6 @@ void generator::visit_mma884(ir::dot_inst* C, ir::value *A, ir::value *B, ir::va\n   // write back accumulators\n   for(size_t i = 0; i < idxs_.at(C).size(); i++)\n     vals_[C][idxs_[C][i]] = acc[i];\n-  \n-  std::cout << \"done\" << std::endl;\n }\n \n namespace {"}, {"filename": "lib/codegen/transform/coalesce.cc", "status": "modified", "additions": 19, "deletions": 17, "changes": 36, "file_content_changes": "@@ -12,8 +12,8 @@ namespace triton {\n namespace codegen{\n namespace transform{\n \n-coalesce::coalesce(analysis::align* align, analysis::layouts *layouts)\n-  : align_(align), layout_(layouts) { }\n+coalesce::coalesce(analysis::align* align, analysis::layouts *layouts, bool has_sm80)\n+  : align_(align), layout_(layouts), has_sm80_(has_sm80) { }\n \n \n // simplify layout conversions using the following simple rules:\n@@ -72,26 +72,28 @@ void coalesce::run(ir::module &mod) {\n       i->replace_uses_of_with(op, new_op);\n     }\n     // coalesce before copy_to_shared\n-    // It's dirty, but the backend is being rewritten from scratch. :)\n-    // if(dynamic_cast<ir::copy_to_shared_inst*>(i))\n-    // if(ir::value* op = i->get_operand(0))\n-    // if(op->get_type()->is_block_ty())\n-    // if(op->get_type()->get_tile_rank() == 2)\n-    // if(invalidated.find(layout_->get(op)) == invalidated.end())\n-    // if(layout_->get(op)->to_mma()){\n-    //   ir::instruction* new_op = ir::cvt_layout_inst::create(op);\n-    //   builder.set_insert_point(i);\n-    //   builder.insert(new_op);\n-    //   op->replace_all_uses_with(new_op);\n-    //   new_op->replace_uses_of_with(new_op, op);\n-    //   invalidated.insert(layout_->get(op));\n-    // }\n+    // only necessary for sm < 80 as Ampere+ can handle reduction\n+    // on MMA layout\n+    if(!has_sm80_)\n+    if(dynamic_cast<ir::copy_to_shared_inst*>(i) || dynamic_cast<ir::reduce_inst*>(i))\n+    if(ir::value* op = i->get_operand(0))\n+    if(op->get_type()->is_block_ty())\n+    if(op->get_type()->get_tile_rank() == 2)\n+    if(invalidated.find(layout_->get(op)) == invalidated.end())\n+    if(layout_->get(op)->to_mma()){\n+      ir::instruction* new_op = ir::cvt_layout_inst::create(op);\n+      builder.set_insert_point(i);\n+      builder.insert(new_op);\n+      op->replace_all_uses_with(new_op);\n+      new_op->replace_uses_of_with(new_op, op);\n+      invalidated.insert(layout_->get(op));\n+    }\n     // uncoalesce after load\n     if(auto x = dynamic_cast<ir::load_inst*>(i))\n     if(x->get_type()->is_block_ty())\n     if(x->get_type()->get_tile_rank()==2)\n     if(layout_->get(x)->to_mma())\n-    if(dynamic_cast<ir::io_inst*>(i)->get_eviction_policy()==ir::io_inst::NORMAL){\n+    if(!has_sm80_ || dynamic_cast<ir::io_inst*>(i)->get_eviction_policy()==ir::io_inst::NORMAL){\n         builder.set_insert_point_after(x);\n         ir::instruction* new_x = ir::cvt_layout_inst::create(x);\n         builder.insert(new_x);"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -25,7 +25,7 @@ def _fwd_kernel(\n     offs_n = tl.arange(0, BLOCK_N)\n     offs_d = tl.arange(0, BLOCK_DMODEL)\n     off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\n+    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n     off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n     # Initialize pointers to Q, K, V\n     q_ptrs = Q + off_q\n@@ -42,7 +42,7 @@ def _fwd_kernel(\n     for start_n in range(0, start_qm + 1):\n         # -- compute qk ----\n         k = tl.load(k_ptrs)\n-        qk = tl.dot(q, k, trans_b=True)\n+        qk = tl.dot(q, k)\n         qk += tl.where(offs_m[:, None] >= (start_n * BLOCK_N + offs_n[None, :]), 0, float(\"-inf\"))\n         # -- compute m_ij, p, l_ij\n         m_ij = tl.max(qk, 1)\n@@ -142,12 +142,12 @@ def test_op(Z, H, N_CTX, D_MODEL, dtype=torch.float16):\n \n \n try:\n-    from src.flash_attn_interface import flash_attn_func\n+    from flash_attn.flash_attn_interface import flash_attn_func\n     HAS_FLASH = True\n except BaseException:\n     HAS_FLASH = False\n \n-BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n+BATCH, N_HEADS, N_CTX, D_HEAD = 4, 64, 2048, 64\n # vary batch size for fixed heads / seq\n batch_bench = triton.testing.Benchmark(\n     x_names=['BATCH'],"}]