[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 139, "deletions": 49, "changes": 188, "file_content_changes": "@@ -84,8 +84,11 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, int width,\n   rewriter.create<LLVM::InsertElementOp>(loc, __VA_ARGS__)\n #define extract_element(...)                                                   \\\n   rewriter.create<LLVM::ExtractElementOp>(loc, __VA_ARGS__)\n+#define load(...) rewriter.create<LLVM::LoadOp>(loc, __VA_ARGS__)\n+#define store(val, ptr) rewriter.create<LLVM::StoreOp>(loc, val, ptr)\n #define address_of(...) rewriter.create<LLVM::AddressOfOp>(loc, __VA_ARGS__)\n-#define i32_ty() rewriter.getIntegerType(32)\n+#define i32_ty rewriter.getIntegerType(32)\n+#define vec_ty(type, num) VectorType::get(num, type)\n \n // Creator for constant\n #define i32_val(...) LLVM::createConstantI32(loc, rewriter, __VA_ARGS__)\n@@ -237,7 +240,7 @@ struct FuncOpConversion : public FuncOpConversionBase {\n     // Set an attribute for maxntidx, it could be used in latter LLVM codegen\n     // for `nvvm.annotation` metadata.\n     newFuncOp->setAttr(NVVMMetadataField::MaxNTid,\n-                       rewriter.getIntegerAttr(i32_ty(), 32 * NumWarps));\n+                       rewriter.getIntegerAttr(i32_ty, 32 * NumWarps));\n \n     rewriter.eraseOp(funcOp);\n     return success();\n@@ -505,7 +508,8 @@ class ConvertTritonGPUOpToLLVMPattern\n     }\n   }\n \n-  // Emit indices calculation within each ConversionPattern\n+  // Emit indices calculation within each ConversionPattern, and returns a\n+  // [elemsPerThread X rank] index matrix.\n   // TODO: [goostavz] Double confirm the redundant indices calculations will\n   //       be eliminated in the consequent MLIR/LLVM optimization. We might\n   //       implement a indiceCache if necessary.\n@@ -544,26 +548,26 @@ class ConvertTritonGPUOpToLLVMPattern\n                                   threadOffset * sizePerThread[k] + elemOffset);\n     }\n     // step 3, add offset to base, and reorder the sequence of indices to\n-    // guarantee that elems in a same sizePerThread are adjacent in order\n-    SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread);\n-    unsigned accumSizePerThread = product<unsigned>(sizePerThread);\n+    // guarantee that elems in the same sizePerThread are adjacent in order\n+    SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread,\n+                                                SmallVector<Value>(rank));\n+    unsigned totalSizePerThread = product<unsigned>(sizePerThread);\n     SmallVector<unsigned> threadsPerDim(rank);\n-    for (unsigned k = 0; k < rank; ++k) {\n+    for (unsigned k = 0; k < rank; ++k)\n       threadsPerDim[k] = ceil<unsigned>(shape[k], sizePerThread[k]);\n-    }\n+\n     for (unsigned n = 0; n < elemsPerThread; ++n) {\n-      unsigned linearNanoTileId = n / accumSizePerThread;\n-      unsigned linearNanoTileElemId = n % accumSizePerThread;\n+      unsigned linearNanoTileId = n / totalSizePerThread;\n+      unsigned linearNanoTileElemId = n % totalSizePerThread;\n       SmallVector<unsigned> multiDimNanoTileId =\n           getMultiDimIndex<unsigned>(linearNanoTileId, threadsPerDim);\n-      SmallVector<unsigned> multiElemsInNanoTileId =\n+      SmallVector<unsigned> multiDimNanoTileElemId =\n           getMultiDimIndex<unsigned>(linearNanoTileElemId, sizePerThread);\n-      multiDimIdx[n].resize(rank);\n       for (unsigned k = 0; k < rank; ++k) {\n         unsigned reorderedMultiDimId =\n             multiDimNanoTileId[k] *\n                 (sizePerThread[k] * threadsPerWarp[k] * warpsPerCTA[k]) +\n-            multiElemsInNanoTileId[k];\n+            multiDimNanoTileElemId[k];\n         multiDimIdx[n][k] =\n             add(multiDimBase[k], idx_val(offset[k][reorderedMultiDimId]));\n       }\n@@ -600,19 +604,17 @@ class ConvertTritonGPUOpToLLVMPattern\n Value convertSplatLikeOp(Type elemType, Type resType, Value constVal,\n                          TypeConverter *typeConverter,\n                          ConversionPatternRewriter &rewriter, Location loc) {\n-\n   auto tensorTy = resType.cast<RankedTensorType>();\n   auto layout = tensorTy.getEncoding();\n   auto srcType = typeConverter->convertType(elemType);\n   auto llSrc = bit_cast(srcType, constVal);\n-  size_t numElemsPerThread = getElemsPerThread(layout, tensorTy.getShape());\n-  llvm::SmallVector<Value, 4> elems(numElemsPerThread, llSrc);\n+  size_t elemsPerThread = getElemsPerThread(layout, tensorTy.getShape());\n+  llvm::SmallVector<Value, 4> elems(elemsPerThread, llSrc);\n   llvm::SmallVector<Type, 4> elemTypes(elems.size(), srcType);\n   auto structTy =\n       LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);\n \n-  auto llStruct = getStructFromElements(loc, elems, rewriter, structTy);\n-  return llStruct;\n+  return getStructFromElements(loc, elems, rewriter, structTy);\n }\n \n struct SplatOpConversion\n@@ -709,7 +711,7 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n     assert(layout && \"unexpected layout in getLayout\");\n     auto shape = ty.getShape();\n     unsigned valueElems = layout.getElemsPerThread(shape);\n-    return std::make_tuple(layout, valueElems);\n+    return {layout, valueElems};\n   }\n \n   unsigned getAlignment(Value val, const BlockedEncodingAttr &layout) const {\n@@ -828,7 +830,7 @@ struct StoreOpConversion\n       llvm::SmallVector<std::string> asmArgs;\n \n       Type valArgTy = IntegerType::get(ctx, width);\n-      auto wordTy = VectorType::get(wordNElems, valueElemTy);\n+      auto wordTy = vec_ty(valueElemTy, wordNElems);\n \n       auto *asmArgList = ptxBuilder.newListOperand();\n       for (int wordIdx = 0; wordIdx < nWords; wordIdx++) {\n@@ -1419,6 +1421,7 @@ struct ConvertLayoutOpConversion\n     if ((!srcLayout.isa<BlockedEncodingAttr>()) ||\n         (!dstLayout.isa<BlockedEncodingAttr>())) {\n       // TODO: not implemented\n+      llvm::errs() << \"Unsupported ConvertLayout found\";\n       return failure();\n     }\n     auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n@@ -1453,6 +1456,7 @@ struct ConvertLayoutOpConversion\n         return {};\n       }\n     };\n+\n     SmallVector<unsigned> numReplicates(rank);\n     SmallVector<unsigned> inNumCTAsEachRep(rank);\n     SmallVector<unsigned> outNumCTAsEachRep(rank);\n@@ -1461,8 +1465,8 @@ struct ConvertLayoutOpConversion\n     auto srcShapePerCTA = getShapePerCTA(srcLayout.cast<BlockedEncodingAttr>());\n     auto dstShapePerCTA = getShapePerCTA(dstLayout.cast<BlockedEncodingAttr>());\n     for (unsigned d = 0; d < rank; ++d) {\n-      unsigned inPerCTA = std::min(unsigned(shape[d]), srcShapePerCTA[d]);\n-      unsigned outPerCTA = std::min(unsigned(shape[d]), dstShapePerCTA[d]);\n+      unsigned inPerCTA = std::min<unsigned>(shape[d], srcShapePerCTA[d]);\n+      unsigned outPerCTA = std::min<unsigned>(shape[d], dstShapePerCTA[d]);\n       unsigned maxPerCTA = std::max(inPerCTA, outPerCTA);\n       numReplicates[d] = ceil<unsigned>(shape[d], maxPerCTA);\n       inNumCTAsEachRep[d] = maxPerCTA / inPerCTA;\n@@ -1569,7 +1573,7 @@ struct ConvertLayoutOpConversion\n                       reorder<unsigned>(paddedRepShape, outOrd));\n         auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n         Value ptr = gep(elemPtrTy, smemBase, offset);\n-        auto vecTy = VectorType::get(vec, llvmElemTy);\n+        auto vecTy = vec_ty(llvmElemTy, vec);\n         ptr = bit_cast(LLVM::LLVMPointerType::get(vecTy, 3), ptr);\n         if (stNotRd) {\n           Value valVec = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n@@ -1580,9 +1584,9 @@ struct ConvertLayoutOpConversion\n                 vecTy, valVec,\n                 vals[elemId + linearCTAId * accumSizePerThread + v], vVal);\n           }\n-          rewriter.create<LLVM::StoreOp>(loc, valVec, ptr);\n+          store(valVec, ptr);\n         } else {\n-          Value valVec = rewriter.create<LLVM::LoadOp>(loc, ptr);\n+          Value valVec = load(ptr);\n           for (unsigned v = 0; v < vec; ++v) {\n             Value vVal = createIndexAttrConstant(\n                 rewriter, loc, getTypeConverter()->getIndexType(), v);\n@@ -1597,6 +1601,7 @@ struct ConvertLayoutOpConversion\n \n /// ====================== dot codegen begin ==========================\n \n+// Data loader for mma.16816 instruction.\n class MMA16816SmemLoader {\n public:\n   MMA16816SmemLoader(int wpt, ArrayRef<uint32_t> order, int kOrder,\n@@ -1637,7 +1642,7 @@ class MMA16816SmemLoader {\n     loadStrideInMat[kOrder] =\n         2; // instrShape[kOrder] / matShape[kOrder], always 2\n     loadStrideInMat[kOrder ^ 1] =\n-        wpt * (instrShape[order[1]] / matShape[order[1]]);\n+        wpt * (instrShape[kOrder ^ 1] / matShape[kOrder ^ 1]);\n \n     pLoadStrideInMat = loadStrideInMat[order[0]];\n     sMatStride =\n@@ -1668,8 +1673,6 @@ class MMA16816SmemLoader {\n   // Compute the offset to the matrix this thread(indexed by warpOff and lane)\n   // mapped to.\n   SmallVector<Value> computeLdmatrixMatOffs(Value warpId, Value lane) {\n-    MLIRContext *ctx = warpId.getContext();\n-\n     // 4x4 matrices\n     Value c = urem(lane, i32_val(8));\n     Value s = udiv(lane, i32_val(8)); // sub-warp-id\n@@ -1819,7 +1822,9 @@ class MMA16816SmemLoader {\n     else\n       llvm::report_fatal_error(\"unsupported mma type found\");\n \n-    // prefetch logic removed here.\n+    // The main difference with the original triton code is we removed the\n+    // prefetch-related logic here for the upstream optimizer phase should take\n+    // care with it, and that is transparent in dot conversion.\n     auto getPtr = [&](int idx) { return ptrs[idx]; };\n \n     Value ptr = getPtr(ptrIdx);\n@@ -1830,39 +1835,124 @@ class MMA16816SmemLoader {\n           matIdx[order[1]] * sMatStride * sMatShape * sTileStride * elemBytes;\n       PTXBuilder builder;\n \n-      auto resArgs = builder.newListOperand();\n-\n       // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a thread.\n-      for (int i = 0; i < 4; i++)\n-        resArgs->listAppend(builder.newOperand(\"=r\"));\n+      auto resArgs = builder.newListOperand(4, \"=r\");\n       auto addrArg = builder.newAddrOperand(ptr, \"r\", sOffset);\n \n       auto ldmatrix = builder.create(\"ldmatrix.sync.aligned.m8n8.x4\")\n                           ->o(\"trans\", needTrans /*predicate*/)\n                           .o(\"shared.b16\");\n       ldmatrix(resArgs, addrArg);\n \n-      Value resV4 = builder.launch(\n-          rewriter, loc, ldmatrixRetTy); // 4xi32, each is composed of 2xf16\n-      // elements(adjacent columns in a row)\n+      // The result type is 4xi32, each i32 is composed of 2xf16\n+      // elements(adjacent two columns in a row)\n+      Value resV4 = builder.launch(rewriter, loc, ldmatrixRetTy);\n \n       auto getIntAttr = [&](int v) {\n-        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty(), v)});\n+        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n       };\n \n-      Type fp16x2Ty = VectorType::get({2}, type::f16Ty(ctx));\n+      Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n \n-      return std::make_tuple(extract_val(fp16x2Ty, resV4, getIntAttr(0)),\n-                             extract_val(fp16x2Ty, resV4, getIntAttr(1)),\n-                             extract_val(fp16x2Ty, resV4, getIntAttr(2)),\n-                             extract_val(fp16x2Ty, resV4, getIntAttr(3)));\n+      return {extract_val(fp16x2Ty, resV4, getIntAttr(0)),\n+              extract_val(fp16x2Ty, resV4, getIntAttr(1)),\n+              extract_val(fp16x2Ty, resV4, getIntAttr(2)),\n+              extract_val(fp16x2Ty, resV4, getIntAttr(3))};\n     } else if (elemBytes == 4 &&\n                needTrans) { // Use lds.32 to load tf32 matrices\n-      assert(false && \"Not implemented yet\");\n+      Value ptr2 = getPtr(ptrIdx + 1);\n+      assert(sMatStride == 1);\n+      int sOffsetElem =\n+          matIdx[order[1]] * (sMatStride * sMatShape) * sTileStride;\n+      int sOffsetArrElem = 1 * (sMatStride * sMatShape) * sTileStride;\n+\n+      Value elems[4];\n+      if (kOrder == 1) {\n+        elems[0] = load(gep(type::f32Ty(ctx), ptr, i32_val(sOffsetElem)));\n+        elems[1] = load(gep(type::f32Ty(ctx), ptr2, i32_val(sOffsetElem)));\n+        elems[2] = load(\n+            gep(type::f32Ty(ctx), ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+        elems[3] = load(\n+            gep(type::f32Ty(ctx), ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n+      } else {\n+        elems[0] = load(gep(type::f32Ty(ctx), ptr, i32_val(sOffsetElem)));\n+        elems[2] = load(gep(type::f32Ty(ctx), ptr2, i32_val(sOffsetElem)));\n+        elems[1] = load(\n+            gep(type::f32Ty(ctx), ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+        elems[3] = load(\n+            gep(type::f32Ty(ctx), ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n+      }\n+\n+      return {elems[0], elems[1], elems[2], elems[3]};\n     } else if (elemBytes == 1 && needTrans) {\n-      assert(false && \"Not implemented yet\");\n+      std::array<std::array<Value, 4>, 2> ptrs;\n+      ptrs[0] = {\n+          getPtr(ptrIdx),\n+          getPtr(ptrIdx + 1),\n+          getPtr(ptrIdx + 2),\n+          getPtr(ptrIdx + 3),\n+      };\n+\n+      ptrs[1] = {\n+          getPtr(ptrIdx + 4),\n+          getPtr(ptrIdx + 5),\n+          getPtr(ptrIdx + 6),\n+          getPtr(ptrIdx + 7),\n+      };\n+\n+      assert(sMatStride == 1);\n+      int sOffsetElem =\n+          matIdx[order[1]] * (sMatStride * sMatShape) * sTileStride;\n+      int sOffsetArrElem = 1 * (sMatStride * sMatShape) * sTileStride;\n+\n+      std::array<Value, 4> i8v4Elems;\n+      std::array<Value, 4> i32Elems;\n+      i8v4Elems.fill(\n+          rewriter.create<LLVM::UndefOp>(loc, vec_ty(type::i8Ty(ctx), 4)));\n+\n+      Value i8Elems[4][4];\n+      Type elemTy = type::i8Ty(ctx);\n+      if (kOrder == 1) {\n+        Value offset = i32_val(sOffsetElem);\n+\n+        for (int i = 0; i < 2; i++)\n+          for (int j = 0; j < 4; j++)\n+            i8Elems[i][j] = load(gep(elemTy, ptrs[i][j], offset));\n+\n+        offset = i32_val(sOffsetElem + sOffsetArrElem);\n+        for (int i = 2; i < 4; i++)\n+          for (int j = 0; j < 4; j++)\n+            i8Elems[i][j] = load(gep(elemTy, ptrs[i - 2][j], offset));\n+\n+        for (int m = 0; m < 4; ++m) {\n+          for (int e = 0; e < 4; ++e)\n+            i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n+                                          i8Elems[m][e], i32_val(e));\n+          i32Elems[m] = bit_cast(i32_ty, i8v4Elems[m]);\n+        }\n+      } else { // k first\n+        Value offset = i32_val(sOffsetElem);\n+        for (int j = 0; j < 4; j++)\n+          i8Elems[0][j] = load(gep(elemTy, ptrs[0][j], offset));\n+        for (int j = 0; j < 4; j++)\n+          i8Elems[2][j] = load(gep(elemTy, ptrs[1][j], offset));\n+        offset = i32_val(sOffsetElem + sOffsetArrElem);\n+        for (int j = 0; j < 4; j++)\n+          i8Elems[1][j] = load(gep(elemTy, ptrs[0][j], offset));\n+        for (int j = 0; j < 4; j++)\n+          i8Elems[3][j] = load(gep(elemTy, ptrs[1][j], offset));\n+\n+        for (int m = 0; m < 4; ++m) {\n+          for (int e = 0; e < 4; ++e)\n+            i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n+                                          i8Elems[m][e], i32_val(e));\n+          i32Elems[m] = bit_cast(i32_ty, i8v4Elems[m]);\n+        }\n+      }\n     }\n-    return std::make_tuple(Value{}, Value{}, Value{}, Value{});\n+\n+    assert(false && \"Invalid smem load\");\n+    return {Value{}, Value{}, Value{}, Value{}};\n   }\n \n private:\n@@ -2062,8 +2152,8 @@ struct DotOpConversionHelper {\n   // The type of a matrix that loaded by either a ldmatrix or composed lds.\n   Type getMatType() const {\n     Type fp32Ty = type::f32Ty(ctx);\n-    Type fp16x2Ty = VectorType::get({2}, type::f16Ty(ctx));\n-    Type bf16x2Ty = VectorType::get({2}, type::bf16Ty(ctx));\n+    Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n+    Type bf16x2Ty = vec_ty(type::bf16Ty(ctx), 2);\n     // floating point types\n     Type fp16x2Pack4Ty =\n         LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n@@ -2072,7 +2162,7 @@ struct DotOpConversionHelper {\n     Type fp32Pack4Ty =\n         LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n     // integer types\n-    Type i8x4Ty = VectorType::get({4}, type::i8Ty(ctx));\n+    Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n     Type i8x4Pack4Ty =\n         LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i8x4Ty));\n     Type i32Pack4Ty = LLVM::LLVMStructType::getLiteral(\n@@ -2386,7 +2476,7 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n     Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n \n     auto getIntAttr = [&](int v) {\n-      return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty(), v)});\n+      return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n     };\n \n     fc[(m + 0) * mStride + (n * 2 + 0)] ="}]