[{"filename": ".pre-commit-config.yaml", "status": "added", "additions": 43, "deletions": 0, "changes": 43, "file_content_changes": "@@ -0,0 +1,43 @@\n+repos:\n+  - repo: local\n+    hooks:\n+      - id: isort\n+        name: isort\n+        entry: isort\n+        args: [\"./python\"]\n+        language: python\n+        types: [python]\n+        additional_dependencies:\n+          - isort==5.10.1\n+\n+      - id: autopep8\n+        name: autopep8\n+        entry: autopep8\n+        args: [\"-a\", \"-r\", \"-i\", \"./python\"]\n+        language: python\n+        types: [python]\n+        additional_dependencies:\n+          - autopep8==1.6.0\n+\n+      - id: clang-format\n+        name: clang-format\n+        entry: python -m clang_format\n+        args:\n+          [\n+            \"-regex\",\n+            \".*\\\\.(cpp|hpp|h|cc)\",\n+            \"-not\",\n+            \"-path\",\n+            \"./python/triton/*\",\n+            \"-not\",\n+            \"-path\",\n+            \"./python/build/*\",\n+            \"-not\",\n+            \"-path\",\n+            \"./include/triton/external/*\",\n+            \"-i\",\n+          ]\n+        language: python\n+        types: [c, c++]\n+        additional_dependencies:\n+          - clang-format==14.0.6"}, {"filename": "CONTRIBUTING.md", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -0,0 +1,53 @@\n+# Triton Programming Language Contributor's Guide\n+\n+First of all, thank you for considering contributing to the Triton programming language! We appreciate the time and effort you're willing to put into improving and expanding our project. In order to maintain a high standard of code and a welcoming atmosphere for collaboration, we kindly ask you to follow the guidelines outlined below.\n+\n+## General Guidelines\n+\n+1. **Quality Contributions:** We value meaningful contributions that aim to improve the project and help it grow. Please refrain from submitting low-effort pull requests (PR) -- such as minor formatting/typo fixes -- solely for the purpose of appearing in the commit history. Maintainers have limited bandwidth, and may decline to review such work.\n+\n+2. **Code Formatting:** Our Continuous Integration (CI) pipeline uses autopep8, isort, and clang-format to check code formatting. To avoid failing the CI workflow due to formatting issues, please utilize the provided `.pre-commit-config.yaml` pre-commit configuration file.\n+\n+3. **Unit Testing:** When contributing new functionalities, please also include appropriate tests. We aim to continuously improve and expand our CI pipeline to ensure the robustness and reliability of the project. PRs that add a large amount of untested code will be rejected. \n+\n+4. **Respectful Communication:** In all discussions related to PRs or other contributions, please maintain a courteous and civil tone. We strive to foster a collaborative environment that is inclusive and respectful to all contributors.\n+\n+\n+## Request for Comments (RFCs)\n+\n+RFCs are a crucial aspect of the collaborative development process, as they provide a structured way to propose and discuss significant changes or additions to the project. RFCs may encompass modifications to the language itself, extensive changes in the compiler backend, or other substantial updates that impact the Triton ecosystem.\n+\n+To ensure that RFCs are clear and easy to understand, consider the following guidelines when creating one:\n+\n+### Purpose\n+\n+The purpose of an RFC is to:\n+\n+- Clearly communicate your proposal to the Triton community\n+- Collect feedback from maintainers and other contributors\n+- Provide a platform for discussing and refining ideas\n+- Reach a consensus on the best approach for implementing the proposed changes\n+\n+### Structure\n+\n+A well-structured RFC should include:\n+\n+1. **Title:** A concise and descriptive title that reflects the main topic of the proposal.\n+\n+2. **Summary:** A brief overview of the proposed changes, including the motivation behind them and their intended impact on the project.\n+\n+3. **Detailed Design:** A thorough description of the proposed changes, including:\n+   - Technical details and implementation approach\n+   - Any new or modified components, functions, or data structures\n+   - Any potential challenges or limitations, as well as proposed solutions\n+\n+4. **Examples and Use Cases:** Provide examples of how the proposed changes would be used in real-world scenarios, as well as any use cases that demonstrate the benefits of the changes.\n+\n+5. **Performance Impact:** Discuss the expected performance impact of the proposed changes, including any potential bottlenecks or performance improvements.\n+\n+6. **Timeline and Milestones:** Outline a proposed timeline for implementing the changes, including any milestones or intermediate steps.\n+\n+\n+## New backends\n+\n+Due to limited resources, we need to prioritize the number of targets we support. We are committed to providing upstream support for Nvidia and AMD GPUs. However, if you wish to contribute support for other backends, please start your project in a fork. If your backend proves to be useful and meets our performance requirements, we will discuss the possibility of upstreaming it.\n\\ No newline at end of file"}, {"filename": "README.md", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "file_content_changes": "@@ -52,11 +52,9 @@ Version 2.0 is out! New features include:\n \n # Contributing\n \n-Community contributions are more than welcome, whether it be to fix bugs or to add new features. Feel free to open GitHub issues about your contribution ideas, and we will review them. Please do not submit PRs that simply fix simple typos in our documentation -- unless they can lead to confusion.\n+Community contributions are more than welcome, whether it be to fix bugs or to add new features. For more detailed instructions, please visit our [contributor's guide](CONTRIBUTING.md).\n \n-Note 1: A more detailed contributor's guide containing general guidelines is coming soon!\n-\n-Note 2: If you\u2019re interested in joining our team and working on Triton & GPU kernels, [we\u2019re hiring](https://openai.com/jobs/#acceleration)!\n+If you\u2019re interested in joining our team and working on Triton & GPU kernels, [we\u2019re hiring](https://openai.com/jobs/#acceleration)!\n \n # Compatibility\n "}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h", "status": "removed", "additions": 0, "deletions": 20, "changes": 20, "file_content_changes": "@@ -1,20 +0,0 @@\n-#ifndef TRITON_CONVERSION_ARITH_TO_INDEX_H\n-#define TRITON_CONVERSION_ARITH_TO_INDEX_H\n-\n-#include \"mlir/Conversion/LLVMCommon/TypeConverter.h\"\n-#include \"mlir/Transforms/DialectConversion.h\"\n-#include <memory>\n-\n-namespace mlir {\n-\n-class ModuleOp;\n-template <typename T> class OperationPass;\n-\n-namespace triton {\n-\n-std::unique_ptr<OperationPass<ModuleOp>> createTritonConvertArithToIndexPass();\n-\n-}\n-} // namespace mlir\n-\n-#endif\n\\ No newline at end of file"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/Passes.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -2,7 +2,6 @@\n #define TRITONGPU_CONVERSION_PASSES_H\n \n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n \n namespace mlir {"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/Passes.td", "status": "modified", "additions": 0, "deletions": 14, "changes": 14, "file_content_changes": "@@ -29,18 +29,4 @@ def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"\n     ];\n }\n \n-def TritonConvertArithToIndex : Pass<\"triton-convert-arith-to-index\", \"mlir::ModuleOp\"> {\n-\n-    let summary = \"Convert arith to index\";\n-    \n-    let constructor = \"mlir::triton::createTritonConvertArithToIndexPass()\";\n-\n-    let description = [{\n-      Convert arith operation on index values to corresponding ops in the index dialect.\n-      We need this because SCFToCF conversion currently generates arith ops on indices.\n-    }];\n-\n-    let dependentDialects = [\"mlir::index::IndexDialect\"];\n-}\n-\n #endif"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -514,6 +514,10 @@ section 9.7.13.4.1 for more details.\n     SmallVector<int> getMMAv1ShapePerWarp() const;\n     int getMMAv1Vec() const;\n     int getMMAv1NumOuter(ArrayRef<int64_t> shape) const;\n+    //\n+    SmallVector<int64_t> getMMAv2Rep(ArrayRef<int64_t> shape,\n+                                     int bitwidth) const;\n+\n   }];\n }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ArithToIndexPass.cpp", "status": "removed", "additions": 0, "deletions": 92, "changes": 92, "file_content_changes": "@@ -1,92 +0,0 @@\n-#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n-#include \"mlir/Analysis/DataFlowFramework.h\"\n-#include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n-#include \"mlir/Conversion/ControlFlowToLLVM//ControlFlowToLLVM.h\"\n-#include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n-#include \"mlir/Conversion/GPUToROCDL/GPUToROCDLPass.h\"\n-#include \"mlir/Conversion/LLVMCommon/VectorPattern.h\"\n-#include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n-#include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n-#include \"mlir/Dialect/Index/IR/IndexDialect.h\"\n-#include \"mlir/Dialect/Index/IR/IndexOps.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n-#include \"mlir/Dialect/LLVMIR/ROCDLDialect.h\"\n-#include \"mlir/Pass/Pass.h\"\n-#include \"triton/Dialect/Triton/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-\n-using namespace mlir;\n-using namespace mlir::triton;\n-\n-#define GEN_PASS_CLASSES\n-#include \"triton/Conversion/TritonGPUToLLVM/Passes.h.inc\"\n-\n-namespace {\n-class TritonArithToIndexConversionTarget : public mlir::ConversionTarget {\n-public:\n-  static bool hasIndexResultOrOperand(Operation *op) {\n-    if (!op)\n-      return false;\n-    bool hasRetIndex = llvm::find_if(op->getResultTypes(), [](Type type) {\n-                         return type.isIndex();\n-                       }) != op->getResultTypes().end();\n-    bool hasArgIndex = llvm::find_if(op->getOperandTypes(), [](Type type) {\n-                         return type.isIndex();\n-                       }) != op->getOperandTypes().end();\n-    return !hasRetIndex && !hasArgIndex;\n-  }\n-\n-  explicit TritonArithToIndexConversionTarget(MLIRContext &ctx)\n-      : ConversionTarget(ctx) {\n-    addLegalDialect<index::IndexDialect>();\n-    addDynamicallyLegalDialect<arith::ArithDialect>(hasIndexResultOrOperand);\n-  }\n-};\n-\n-template <class SrcOp, class DstOp>\n-LogicalResult replaceArithWithIndex(SrcOp op, PatternRewriter &rewriter) {\n-  // if (!hasIndexResultOrOperand(&*op))\n-  //   return failure();\n-  rewriter.replaceOpWithNewOp<DstOp>(op, op->getResultTypes(),\n-                                     op->getOperands(), op->getAttrs());\n-  return success();\n-}\n-\n-LogicalResult replaceArithCmpWithIndexCmp(arith::CmpIOp op,\n-                                          PatternRewriter &rewriter) {\n-  // if (!hasIndexResultOrOperand(&*op))\n-  //   return failure();\n-  rewriter.replaceOpWithNewOp<index::CmpOp>(\n-      op, op.getResult().getType(), (index::IndexCmpPredicate)op.getPredicate(),\n-      op.getOperand(0), op.getOperand(1));\n-  return success();\n-}\n-\n-class ArithToIndex : public TritonConvertArithToIndexBase<ArithToIndex> {\n-public:\n-  void runOnOperation() override {\n-    MLIRContext *context = &getContext();\n-    ModuleOp mod = getOperation();\n-    TritonArithToIndexConversionTarget target(*context);\n-    RewritePatternSet patterns(context);\n-    patterns.add(replaceArithWithIndex<arith::IndexCastOp, index::CastSOp>);\n-    patterns.add(replaceArithWithIndex<arith::ConstantOp, index::ConstantOp>);\n-    patterns.add(replaceArithWithIndex<arith::AddIOp, index::AddOp>);\n-    patterns.add(replaceArithCmpWithIndexCmp);\n-    if (failed(applyPartialConversion(mod, target, std::move(patterns)))) {\n-      return signalPassFailure();\n-    }\n-  }\n-};\n-} // namespace\n-\n-namespace mlir {\n-namespace triton {\n-\n-std::unique_ptr<OperationPass<ModuleOp>> createTritonConvertArithToIndexPass() {\n-  return std::make_unique<::ArithToIndex>();\n-}\n-\n-} // namespace triton\n-} // namespace mlir\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -1,10 +1,15 @@\n add_mlir_conversion_library(TritonGPUToLLVM\n-    ArithToIndexPass.cpp\n     TritonGPUToLLVM.cpp\n     GCNAsmFormat.cpp\n     PTXAsmFormat.cpp\n     TritonGPUToLLVMPass.cpp\n+    ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp\n+    ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp\n+    ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp\n     ConvertLayoutOpToLLVM.cpp\n+    DotOpToLLVM/FMA.cpp\n+    DotOpToLLVM/MMAv1.cpp\n+    DotOpToLLVM/MMAv2.cpp\n     DotOpToLLVM.cpp\n     ElementwiseOpToLLVM.cpp\n     LoadStoreOpToLLVM.cpp\n@@ -15,7 +20,6 @@ add_mlir_conversion_library(TritonGPUToLLVM\n     Utility.cpp\n     TypeConverter.cpp\n     ViewOpToLLVM.cpp\n-    DotOpHelpers.cpp\n \n     ADDITIONAL_HEADER_DIRS\n     ${PROJECT_SOURCE_DIR}/include/triton/Conversion/TritonGPUToLLVM"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 49, "deletions": 38, "changes": 87, "file_content_changes": "@@ -1,12 +1,8 @@\n #include \"ConvertLayoutOpToLLVM.h\"\n-#include \"DotOpHelpers.h\"\n #include \"Utility.h\"\n \n-using ::mlir::LLVM::DotOpFMAConversionHelper;\n-using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStridesFromShapeAndOrder;\n-using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getElemsPerThread;\n@@ -16,6 +12,41 @@ using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::isaDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n+// Forward declarations\n+\n+namespace SharedToDotOperandMMAv1 {\n+using CoordTy = SmallVector<Value>;\n+using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+\n+SmallVector<CoordTy> getMNCoords(Value thread,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 ArrayRef<unsigned int> wpt,\n+                                 const MmaEncodingAttr &mmaLayout,\n+                                 ArrayRef<int64_t> shape, bool isARow,\n+                                 bool isBRow, bool isAVec4, bool isBVec4);\n+\n+Value convertLayout(int opIdx, Value tensor, const SharedMemoryObject &smemObj,\n+                    Value thread, Location loc,\n+                    TritonGPUToLLVMTypeConverter *typeConverter,\n+                    ConversionPatternRewriter &rewriter, Type resultTy);\n+\n+} // namespace SharedToDotOperandMMAv1\n+\n+namespace SharedToDotOperandMMAv2 {\n+Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,\n+                    Location loc, Value tensor,\n+                    DotOperandEncodingAttr bEncoding,\n+                    const SharedMemoryObject &smemObj,\n+                    TritonGPUToLLVMTypeConverter *typeConverter, Value thread);\n+}\n+\n+namespace SharedToDotOperandFMA {\n+Value convertLayout(int opIdx, Value B, Value llB, BlockedEncodingAttr dLayout,\n+                    Value thread, Location loc,\n+                    TritonGPUToLLVMTypeConverter *typeConverter,\n+                    ConversionPatternRewriter &rewriter);\n+}\n+\n struct ConvertLayoutOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {\n public:\n@@ -139,10 +170,10 @@ struct ConvertLayoutOpConversion\n       } else if (mmaLayout.isVolta()) {\n         auto [isARow, isBRow, isAVec4, isBVec4, _] =\n             mmaLayout.decodeVoltaLayoutStates();\n-        auto coords = DotOpMmaV1ConversionHelper::getMNCoords(\n+        auto coords = SharedToDotOperandMMAv1::getMNCoords(\n             threadId, rewriter, mmaLayout.getWarpsPerCTA(), mmaLayout, shape,\n             isARow, isBRow, isAVec4, isBVec4);\n-        return DotOpMmaV1ConversionHelper::getCoord(elemId, coords);\n+        return coords[elemId];\n       } else {\n         llvm_unreachable(\"Unexpected MMALayout version\");\n       }\n@@ -518,15 +549,10 @@ struct ConvertLayoutOpConversion\n                        .dyn_cast_or_null<BlockedEncodingAttr>()) {\n       auto dotOpLayout =\n           dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n-      DotOpFMAConversionHelper helper(blockedLayout);\n       auto thread = getThreadId(rewriter, loc);\n-      if (dotOpLayout.getOpIdx() == 0) { // $a\n-        res = helper.loadA(src, adaptor.getSrc(), blockedLayout, thread, loc,\n-                           getTypeConverter(), rewriter);\n-      } else { // $b\n-        res = helper.loadB(src, adaptor.getSrc(), blockedLayout, thread, loc,\n-                           getTypeConverter(), rewriter);\n-      }\n+      res = SharedToDotOperandFMA::convertLayout(\n+          dotOpLayout.getOpIdx(), src, adaptor.getSrc(), blockedLayout, thread,\n+          loc, getTypeConverter(), rewriter);\n     } else {\n       assert(false && \"Unsupported dot operand layout found\");\n     }\n@@ -622,19 +648,12 @@ struct ConvertLayoutOpConversion\n     Value res;\n \n     if (!isOuter && mmaLayout.isAmpere() && isHMMA) { // tensor core v2\n-      MMA16816ConversionHelper mmaHelper(src.getType(), mmaLayout,\n-                                         getThreadId(rewriter, loc), rewriter,\n-                                         getTypeConverter(), op.getLoc());\n-\n-      if (dotOperandLayout.getOpIdx() == 0) {\n-        // operand $a\n-        res = mmaHelper.loadA(src, smemObj);\n-      } else if (dotOperandLayout.getOpIdx() == 1) {\n-        // operand $b\n-        res = mmaHelper.loadB(src, smemObj);\n-      }\n+\n+      res = SharedToDotOperandMMAv2::convertLayout(\n+          dotOperandLayout.getOpIdx(), rewriter, loc, src, dotOperandLayout,\n+          smemObj, getTypeConverter(), tid_val());\n+\n     } else if (!isOuter && mmaLayout.isVolta() && isHMMA) { // tensor core v1\n-      DotOpMmaV1ConversionHelper helper(mmaLayout);\n       bool isMMAv1Row = dotOperandLayout.getMMAv1IsRow();\n       auto srcSharedLayout = src.getType()\n                                  .cast<RankedTensorType>()\n@@ -648,23 +667,15 @@ struct ConvertLayoutOpConversion\n         return Value();\n       }\n \n-      if (dotOperandLayout.getOpIdx() == 0) { // operand $a\n-        // TODO[Superjomn]: transA is not available here.\n-        bool transA = false;\n-        res = helper.loadA(src, smemObj, getThreadId(rewriter, loc), loc,\n-                           getTypeConverter(), rewriter, dst.getType());\n-      } else if (dotOperandLayout.getOpIdx() == 1) { // operand $b\n-        // TODO[Superjomn]: transB is not available here.\n-        bool transB = false;\n-        res = helper.loadB(src, smemObj, getThreadId(rewriter, loc), loc,\n-                           getTypeConverter(), rewriter, dst.getType());\n-      }\n+      res = SharedToDotOperandMMAv1::convertLayout(\n+          dotOperandLayout.getOpIdx(), src, smemObj, getThreadId(rewriter, loc),\n+          loc, getTypeConverter(), rewriter, dst.getType());\n     } else {\n       assert(false && \"Unsupported mma layout found\");\n     }\n     return res;\n   }\n-};\n+}; // namespace triton::gpu::ConvertLayoutOp>\n \n void populateConvertLayoutOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp", "status": "added", "additions": 227, "deletions": 0, "changes": 227, "file_content_changes": "@@ -0,0 +1,227 @@\n+#include \"ConvertLayoutOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+using ValueTable = std::map<std::pair<int, int>, Value>;\n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getStridesFromShapeAndOrder;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n+using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getOrder;\n+using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+SmallVector<Value>\n+getThreadIds(Value threadId, ArrayRef<unsigned int> shapePerCTA,\n+             ArrayRef<unsigned int> sizePerThread, ArrayRef<unsigned int> order,\n+             ConversionPatternRewriter &rewriter, Location loc) {\n+  int dim = order.size();\n+  SmallVector<Value> threadIds(dim);\n+  for (unsigned k = 0; k < dim - 1; k++) {\n+    Value dimK = i32_val(shapePerCTA[order[k]] / sizePerThread[order[k]]);\n+    Value rem = urem(threadId, dimK);\n+    threadId = udiv(threadId, dimK);\n+    threadIds[order[k]] = rem;\n+  }\n+  Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n+  threadIds[order[dim - 1]] = urem(threadId, dimK);\n+  return threadIds;\n+}\n+\n+int getShapePerCTAForMN(BlockedEncodingAttr layout, bool isM) {\n+  auto order = layout.getOrder();\n+  auto shapePerCTA = getShapePerCTA(layout);\n+\n+  int mShapePerCTA =\n+      order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  int nShapePerCTA =\n+      order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  return isM ? mShapePerCTA : nShapePerCTA;\n+}\n+\n+// Get sizePerThread for M or N axis.\n+int getSizePerThreadForMN(BlockedEncodingAttr layout, bool isM) {\n+  auto order = layout.getOrder();\n+  auto sizePerThread = getSizePerThread(layout);\n+\n+  int mSizePerThread =\n+      order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+  int nSizePerThread =\n+      order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+  return isM ? mSizePerThread : nSizePerThread;\n+}\n+\n+Value getStructFromValueTable(ArrayRef<Value> vals,\n+                              ConversionPatternRewriter &rewriter, Location loc,\n+                              TritonGPUToLLVMTypeConverter *typeConverter,\n+                              Type elemTy) {\n+  SmallVector<Type> elemTypes(vals.size(), elemTy);\n+  SmallVector<Value> elems;\n+  elems.reserve(vals.size());\n+  for (auto &val : vals) {\n+    elems.push_back(val);\n+  }\n+  MLIRContext *ctx = elemTy.getContext();\n+  Type structTy = struct_ty(elemTypes);\n+  return typeConverter->packLLElements(loc, elems, rewriter, structTy);\n+}\n+\n+ValueTable getValueTableFromStruct(Value val, int K, int n0, int shapePerCTA,\n+                                   int sizePerThread,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Location loc,\n+                                   TritonGPUToLLVMTypeConverter *typeConverter,\n+                                   Type type) {\n+  ValueTable res;\n+  auto elems = typeConverter->unpackLLElements(loc, val, rewriter, type);\n+  int index = 0;\n+  for (unsigned k = 0; k < K; ++k) {\n+    for (unsigned m = 0; m < n0; m += shapePerCTA)\n+      for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n+        res[{m + mm, k}] = elems[index++];\n+      }\n+  }\n+  return res;\n+}\n+\n+Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n+               Location loc, TritonGPUToLLVMTypeConverter *typeConverter,\n+               ConversionPatternRewriter &rewriter) {\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto aShape = aTensorTy.getShape();\n+\n+  auto aOrder = aLayout.getOrder();\n+  auto order = dLayout.getOrder();\n+\n+  bool isARow = aOrder[0] == 1;\n+\n+  auto aSmem = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n+  Value strideAM = aSmem.strides[0];\n+  Value strideAK = aSmem.strides[1];\n+  Value strideA0 = isARow ? strideAK : strideAM;\n+  Value strideA1 = isARow ? strideAM : strideAK;\n+  int aNumPtr = 8;\n+  int K = aShape[1];\n+  int M = aShape[0];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value mContig = i32_val(sizePerThread[order[1]]);\n+\n+  // threadId in blocked layout\n+  auto threadIds =\n+      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n+  Value threadIdM = threadIds[0];\n+\n+  Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n+  Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n+  SmallVector<Value> aOff(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i) {\n+    aOff[i] = add(mul(offA0, strideA0), mul(offA1, strideA1));\n+  }\n+  auto elemTy = A.getType().cast<RankedTensorType>().getElementType();\n+\n+  Type ptrTy = ptr_ty(elemTy, 3);\n+  SmallVector<Value> aPtrs(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i)\n+    aPtrs[i] = gep(ptrTy, aSmem.base, aOff[i]);\n+\n+  SmallVector<Value> vas;\n+\n+  int mShapePerCTA = getShapePerCTAForMN(dLayout, true /*isM*/);\n+  int mSizePerThread = getSizePerThreadForMN(dLayout, true /*isM*/);\n+\n+  for (unsigned k = 0; k < K; ++k)\n+    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+      for (unsigned mm = 0; mm < mSizePerThread; ++mm) {\n+        Value offset =\n+            add(mul(i32_val(m + mm), strideAM), mul(i32_val(k), strideAK));\n+        Value pa = gep(ptrTy, aPtrs[0], offset);\n+        Value va = load(pa);\n+        vas.emplace_back(va);\n+      }\n+\n+  return getStructFromValueTable(vas, rewriter, loc, typeConverter, elemTy);\n+}\n+\n+Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n+               Location loc, TritonGPUToLLVMTypeConverter *typeConverter,\n+               ConversionPatternRewriter &rewriter) {\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto bShape = bTensorTy.getShape();\n+\n+  auto bOrder = bLayout.getOrder();\n+  auto order = dLayout.getOrder();\n+\n+  bool isBRow = bOrder[0] == 1;\n+\n+  auto bSmem = getSharedMemoryObjectFromStruct(loc, llB, rewriter);\n+  Value strideBN = bSmem.strides[1];\n+  Value strideBK = bSmem.strides[0];\n+  Value strideB0 = isBRow ? strideBN : strideBK;\n+  Value strideB1 = isBRow ? strideBK : strideBN;\n+  int bNumPtr = 8;\n+  int K = bShape[0];\n+  int N = bShape[1];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value nContig = i32_val(sizePerThread[order[0]]);\n+\n+  // threadId in blocked layout\n+  auto threadIds =\n+      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n+  Value threadIdN = threadIds[1];\n+\n+  Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n+  Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n+  SmallVector<Value> bOff(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i) {\n+    bOff[i] = add(mul(offB0, strideB0), mul(offB1, strideB1));\n+  }\n+  auto elemTy = B.getType().cast<RankedTensorType>().getElementType();\n+\n+  Type ptrTy = ptr_ty(elemTy, 3);\n+  SmallVector<Value> bPtrs(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i)\n+    bPtrs[i] = gep(ptrTy, bSmem.base, bOff[i]);\n+\n+  SmallVector<Value> vbs;\n+\n+  int nShapePerCTA = getShapePerCTAForMN(dLayout, false /*isM*/);\n+  int nSizePerThread = getSizePerThreadForMN(dLayout, false /*isM*/);\n+\n+  for (unsigned k = 0; k < K; ++k)\n+    for (unsigned n = 0; n < N; n += nShapePerCTA)\n+      for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n+        Value offset =\n+            add(mul(i32_val(n + nn), strideBN), mul(i32_val(k), strideBK));\n+        Value pb = gep(ptrTy, bPtrs[0], offset);\n+        Value vb = load(pb);\n+        vbs.emplace_back(vb);\n+      }\n+\n+  return getStructFromValueTable(vbs, rewriter, loc, typeConverter, elemTy);\n+}\n+\n+namespace SharedToDotOperandFMA {\n+Value convertLayout(int opIdx, Value val, Value llVal,\n+                    BlockedEncodingAttr dLayout, Value thread, Location loc,\n+                    TritonGPUToLLVMTypeConverter *typeConverter,\n+                    ConversionPatternRewriter &rewriter) {\n+  if (opIdx == 0)\n+    return loadAFMA(val, llVal, dLayout, thread, loc, typeConverter, rewriter);\n+  else\n+    return loadBFMA(val, llVal, dLayout, thread, loc, typeConverter, rewriter);\n+}\n+} // namespace SharedToDotOperandFMA\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp", "status": "added", "additions": 460, "deletions": 0, "changes": 460, "file_content_changes": "@@ -0,0 +1,460 @@\n+#include \"ConvertLayoutOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+using CoordTy = SmallVector<Value>;\n+using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getStridesFromShapeAndOrder;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n+using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getOrder;\n+using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+// Compute the offset of the matrix to load.\n+// Returns offsetAM, offsetAK, offsetBN, offsetBK.\n+// NOTE, the information M(from $a) and N(from $b) couldn't be retrieved at\n+// the same time in the usage in convert_layout[shared->dot_op], we leave\n+// the noexist info to be 0 and only use the desired argument from the\n+// composed result. In this way we want to retain the original code\n+// structure in convert_mma884 method for easier debugging.\n+static std::tuple<Value, Value, Value, Value>\n+computeOffsets(Value threadId, bool isARow, bool isBRow, ArrayRef<int> fpw,\n+               ArrayRef<int> spw, ArrayRef<int> rep,\n+               ConversionPatternRewriter &rewriter, Location loc,\n+               Type resultTy) {\n+  auto *ctx = rewriter.getContext();\n+  auto wpt = resultTy.cast<RankedTensorType>()\n+                 .getEncoding()\n+                 .cast<DotOperandEncodingAttr>()\n+                 .getParent()\n+                 .cast<MmaEncodingAttr>()\n+                 .getWarpsPerCTA();\n+\n+  Value _1 = i32_val(1);\n+  Value _3 = i32_val(3);\n+  Value _4 = i32_val(4);\n+  Value _16 = i32_val(16);\n+  Value _32 = i32_val(32);\n+\n+  Value lane = urem(threadId, _32);\n+  Value warp = udiv(threadId, _32);\n+\n+  // warp offset\n+  Value warp0 = urem(warp, i32_val(wpt[0]));\n+  Value warp12 = udiv(warp, i32_val(wpt[0]));\n+  Value warp1 = urem(warp12, i32_val(wpt[1]));\n+  Value warpMOff = mul(warp0, i32_val(spw[0]));\n+  Value warpNOff = mul(warp1, i32_val(spw[1]));\n+  // Quad offset\n+  Value quadMOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[0]));\n+  Value quadNOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[1]));\n+  // Pair offset\n+  Value pairMOff = udiv(urem(lane, _16), _4);\n+  pairMOff = urem(pairMOff, i32_val(fpw[0]));\n+  pairMOff = mul(pairMOff, _4);\n+  Value pairNOff = udiv(urem(lane, _16), _4);\n+  pairNOff = udiv(pairNOff, i32_val(fpw[0]));\n+  pairNOff = urem(pairNOff, i32_val(fpw[1]));\n+  pairNOff = mul(pairNOff, _4);\n+  // scale\n+  pairMOff = mul(pairMOff, i32_val(rep[0] / 2));\n+  quadMOff = mul(quadMOff, i32_val(rep[0] / 2));\n+  pairNOff = mul(pairNOff, i32_val(rep[1] / 2));\n+  quadNOff = mul(quadNOff, i32_val(rep[1] / 2));\n+  // Quad pair offset\n+  Value laneMOff = add(pairMOff, quadMOff);\n+  Value laneNOff = add(pairNOff, quadNOff);\n+  // A offset\n+  Value offsetAM = add(warpMOff, laneMOff);\n+  Value offsetAK = and_(lane, _3);\n+  // B offset\n+  Value offsetBN = add(warpNOff, laneNOff);\n+  Value offsetBK = and_(lane, _3);\n+  // i indices\n+  Value offsetCM = add(and_(lane, _1), offsetAM);\n+  if (isARow) {\n+    offsetAM = add(offsetAM, urem(threadId, _4));\n+    offsetAK = i32_val(0);\n+  }\n+  if (!isBRow) {\n+    offsetBN = add(offsetBN, urem(threadId, _4));\n+    offsetBK = i32_val(0);\n+  }\n+\n+  return std::make_tuple(offsetAM, offsetAK, offsetBN, offsetBK);\n+}\n+\n+static Value loadA(Value tensor, const SharedMemoryObject &smemObj,\n+                   Value thread, Location loc,\n+                   TritonGPUToLLVMTypeConverter *typeConverter,\n+                   ConversionPatternRewriter &rewriter, Type resultTy) {\n+  static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n+  auto wpt = resultTy.cast<RankedTensorType>()\n+                 .getEncoding()\n+                 .cast<DotOperandEncodingAttr>()\n+                 .getParent()\n+                 .cast<MmaEncodingAttr>()\n+                 .getWarpsPerCTA();\n+\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto shape = tensorTy.getShape();\n+  auto order = sharedLayout.getOrder();\n+\n+  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+  Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+\n+  bool isARow = order[0] != 0;\n+  auto resultEncoding = resultTy.cast<RankedTensorType>()\n+                            .getEncoding()\n+                            .cast<DotOperandEncodingAttr>();\n+  auto [offsetAM, offsetAK, _3, _4] = computeOffsets(\n+      thread, isARow, false, fpw, resultEncoding.getMMAv1ShapePerWarp(),\n+      resultEncoding.getMMAv1Rep(), rewriter, loc, resultTy);\n+\n+  int vecA = sharedLayout.getVec();\n+\n+  auto strides = smemObj.strides;\n+  Value strideAM = isARow ? strides[0] : i32_val(1);\n+  Value strideAK = isARow ? i32_val(1) : strides[1];\n+  Value strideA0 = isARow ? strideAK : strideAM;\n+  Value strideA1 = isARow ? strideAM : strideAK;\n+\n+  int strideRepM = wpt[0] * fpw[0] * 8;\n+  int strideRepK = 1;\n+\n+  // swizzling\n+  int perPhaseA = sharedLayout.getPerPhase();\n+  int maxPhaseA = sharedLayout.getMaxPhase();\n+  int stepA0 = isARow ? strideRepK : strideRepM;\n+  int numPtrA = std::max(2 * perPhaseA * maxPhaseA / stepA0, 1);\n+  int NK = shape[1];\n+\n+  // pre-compute pointer lanes\n+  Value offA0 = isARow ? offsetAK : offsetAM;\n+  Value offA1 = isARow ? offsetAM : offsetAK;\n+  Value phaseA = urem(udiv(offA1, i32_val(perPhaseA)), i32_val(maxPhaseA));\n+  offA0 = add(offA0, cSwizzleOffset);\n+  SmallVector<Value> offA(numPtrA);\n+  for (int i = 0; i < numPtrA; i++) {\n+    Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));\n+    offA0I = udiv(offA0I, i32_val(vecA));\n+    offA0I = xor_(offA0I, phaseA);\n+    offA0I = mul(offA0I, i32_val(vecA));\n+    offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n+  }\n+\n+  Type elemX2Ty = vec_ty(f16_ty, 2);\n+  Type elemPtrTy = ptr_ty(f16_ty, 3);\n+  if (tensorTy.getElementType().isBF16()) {\n+    elemX2Ty = vec_ty(i16_ty, 2);\n+    elemPtrTy = ptr_ty(i16_ty, 3);\n+  }\n+\n+  // prepare arguments\n+  SmallVector<Value> ptrA(numPtrA);\n+\n+  std::map<std::pair<int, int>, std::pair<Value, Value>> has;\n+  for (int i = 0; i < numPtrA; i++)\n+    ptrA[i] = gep(ptr_ty(f16_ty, 3), smemBase, offA[i]);\n+\n+  auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+  auto loadA = [&](int m, int k) {\n+    int offidx = (isARow ? k / 4 : m) % numPtrA;\n+    Value thePtrA = gep(elemPtrTy, smemBase, offA[offidx]);\n+\n+    int stepAM = isARow ? m : m / numPtrA * numPtrA;\n+    int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n+    Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),\n+                       mul(i32_val(stepAK), strideAK));\n+    Value pa = gep(elemPtrTy, thePtrA, offset);\n+    Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n+    Value ha = load(bitcast(pa, aPtrTy));\n+    // record lds that needs to be moved\n+    Value ha00 = bitcast(extract_element(ha, i32_val(0)), elemX2Ty);\n+    Value ha01 = bitcast(extract_element(ha, i32_val(1)), elemX2Ty);\n+    ld(has, m, k, ha00, ha01);\n+\n+    if (vecA > 4) {\n+      Value ha10 = bitcast(extract_element(ha, i32_val(2)), elemX2Ty);\n+      Value ha11 = bitcast(extract_element(ha, i32_val(3)), elemX2Ty);\n+      if (isARow)\n+        ld(has, m, k + 4, ha10, ha11);\n+      else\n+        ld(has, m + 1, k, ha10, ha11);\n+    }\n+  };\n+\n+  bool isARow_ = resultEncoding.getMMAv1IsRow();\n+  bool isAVec4 = resultEncoding.getMMAv1IsVec4();\n+  unsigned numM = resultEncoding.getMMAv1NumOuter(shape);\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      if (!has.count({m, k}))\n+        loadA(m, k);\n+\n+  SmallVector<Value> elems;\n+  elems.reserve(has.size() * 2);\n+  for (auto item : has) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+\n+  Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);\n+  return res;\n+}\n+\n+static Value loadB(Value tensor, const SharedMemoryObject &smemObj,\n+                   Value thread, Location loc,\n+                   TritonGPUToLLVMTypeConverter *typeConverter,\n+                   ConversionPatternRewriter &rewriter, Type resultTy) {\n+  static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n+  auto wpt = resultTy.cast<RankedTensorType>()\n+                 .getEncoding()\n+                 .cast<DotOperandEncodingAttr>()\n+                 .getParent()\n+                 .cast<MmaEncodingAttr>()\n+                 .getWarpsPerCTA();\n+  // smem\n+  auto strides = smemObj.strides;\n+\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+\n+  auto shape = tensorTy.getShape();\n+  auto order = sharedLayout.getOrder();\n+\n+  Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+  bool isBRow = order[0] != 0; // is row-major in shared memory layout\n+  // isBRow_ indicates whether B is row-major in DotOperand layout\n+  auto resultEncoding = resultTy.cast<RankedTensorType>()\n+                            .getEncoding()\n+                            .cast<DotOperandEncodingAttr>();\n+\n+  int vecB = sharedLayout.getVec();\n+  Value strideBN = isBRow ? i32_val(1) : strides[1];\n+  Value strideBK = isBRow ? strides[0] : i32_val(1);\n+  Value strideB0 = isBRow ? strideBN : strideBK;\n+  Value strideB1 = isBRow ? strideBK : strideBN;\n+  int strideRepN = wpt[1] * fpw[1] * 8;\n+  int strideRepK = 1;\n+\n+  auto [_3, _4, offsetBN, offsetBK] = computeOffsets(\n+      thread, false, isBRow, fpw, resultEncoding.getMMAv1ShapePerWarp(),\n+      resultEncoding.getMMAv1Rep(), rewriter, loc, resultTy);\n+\n+  // swizzling\n+  int perPhaseB = sharedLayout.getPerPhase();\n+  int maxPhaseB = sharedLayout.getMaxPhase();\n+  int stepB0 = isBRow ? strideRepN : strideRepK;\n+  int numPtrB = std::max(2 * perPhaseB * maxPhaseB / stepB0, 1);\n+  int NK = shape[0];\n+\n+  Value offB0 = isBRow ? offsetBN : offsetBK;\n+  Value offB1 = isBRow ? offsetBK : offsetBN;\n+  Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n+  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+\n+  offB0 = add(offB0, cSwizzleOffset);\n+  SmallVector<Value> offB(numPtrB);\n+  for (int i = 0; i < numPtrB; ++i) {\n+    Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));\n+    offB0I = udiv(offB0I, i32_val(vecB));\n+    offB0I = xor_(offB0I, phaseB);\n+    offB0I = mul(offB0I, i32_val(vecB));\n+    offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));\n+  }\n+\n+  Type elemPtrTy = ptr_ty(f16_ty, 3);\n+  Type elemX2Ty = vec_ty(f16_ty, 2);\n+  if (tensorTy.getElementType().isBF16()) {\n+    elemPtrTy = ptr_ty(i16_ty, 3);\n+    elemX2Ty = vec_ty(i16_ty, 2);\n+  }\n+\n+  SmallVector<Value> ptrB(numPtrB);\n+  ValueTable hbs;\n+  for (int i = 0; i < numPtrB; ++i)\n+    ptrB[i] = gep(ptr_ty(f16_ty, 3), smem, offB[i]);\n+\n+  auto ld = [&](decltype(hbs) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+\n+  auto loadB = [&](int n, int K) {\n+    int offidx = (isBRow ? n : K / 4) % numPtrB;\n+    Value thePtrB = ptrB[offidx];\n+\n+    int stepBN = isBRow ? n / numPtrB * numPtrB : n;\n+    int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n+    Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n+                       mul(i32_val(stepBK), strideBK));\n+    Value pb = gep(elemPtrTy, thePtrB, offset);\n+\n+    Value hb =\n+        load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n+    // record lds that needs to be moved\n+    Value hb00 = bitcast(extract_element(hb, i32_val(0)), elemX2Ty);\n+    Value hb01 = bitcast(extract_element(hb, i32_val(1)), elemX2Ty);\n+    ld(hbs, n, K, hb00, hb01);\n+    if (vecB > 4) {\n+      Value hb10 = bitcast(extract_element(hb, i32_val(2)), elemX2Ty);\n+      Value hb11 = bitcast(extract_element(hb, i32_val(3)), elemX2Ty);\n+      if (isBRow)\n+        ld(hbs, n + 1, K, hb10, hb11);\n+      else\n+        ld(hbs, n, K + 4, hb10, hb11);\n+    }\n+  };\n+\n+  bool isBRow_ = resultEncoding.getMMAv1IsRow();\n+  assert(isBRow == isBRow_ && \"B need smem isRow\");\n+  bool isBVec4 = resultEncoding.getMMAv1IsVec4();\n+  unsigned numN = resultEncoding.getMMAv1NumOuter(shape);\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned n = 0; n < numN / 2; ++n) {\n+      if (!hbs.count({n, k}))\n+        loadB(n, k);\n+    }\n+\n+  SmallVector<Value> elems;\n+  for (auto &item : hbs) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+\n+  Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);\n+  return res;\n+}\n+\n+namespace SharedToDotOperandMMAv1 {\n+using CoordTy = SmallVector<Value>;\n+using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+\n+SmallVector<CoordTy> getMNCoords(Value thread,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 ArrayRef<unsigned int> wpt,\n+                                 const MmaEncodingAttr &mmaLayout,\n+                                 ArrayRef<int64_t> shape, bool isARow,\n+                                 bool isBRow, bool isAVec4, bool isBVec4) {\n+  static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n+\n+  auto *ctx = thread.getContext();\n+  auto loc = UnknownLoc::get(ctx);\n+  Value _1 = i32_val(1);\n+  Value _2 = i32_val(2);\n+  Value _4 = i32_val(4);\n+  Value _16 = i32_val(16);\n+  Value _32 = i32_val(32);\n+  Value _fpw0 = i32_val(fpw[0]);\n+  Value _fpw1 = i32_val(fpw[1]);\n+\n+  // A info\n+  auto aEncoding = DotOperandEncodingAttr::get(ctx, 0, mmaLayout);\n+  auto aRep = aEncoding.getMMAv1Rep();\n+  auto aSpw = aEncoding.getMMAv1ShapePerWarp();\n+  // B info\n+  auto bEncoding = DotOperandEncodingAttr::get(ctx, 1, mmaLayout);\n+  auto bSpw = bEncoding.getMMAv1ShapePerWarp();\n+  auto bRep = bEncoding.getMMAv1Rep();\n+\n+  SmallVector<int, 2> rep({aRep[0], bRep[1]});\n+  SmallVector<int, 2> spw({aSpw[0], bSpw[1]});\n+  SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n+\n+  Value lane = urem(thread, _32);\n+  Value warp = udiv(thread, _32);\n+\n+  Value warp0 = urem(warp, i32_val(wpt[0]));\n+  Value warp12 = udiv(warp, i32_val(wpt[0]));\n+  Value warp1 = urem(warp12, i32_val(wpt[1]));\n+\n+  // warp offset\n+  Value offWarpM = mul(warp0, i32_val(spw[0]));\n+  Value offWarpN = mul(warp1, i32_val(spw[1]));\n+  // quad offset\n+  Value offQuadM = mul(udiv(and_(lane, _16), _4), _fpw0);\n+  Value offQuadN = mul(udiv(and_(lane, _16), _4), _fpw1);\n+  // pair offset\n+  Value offPairM = udiv(urem(lane, _16), _4);\n+  offPairM = urem(offPairM, _fpw0);\n+  offPairM = mul(offPairM, _4);\n+  Value offPairN = udiv(urem(lane, _16), _4);\n+  offPairN = udiv(offPairN, _fpw0);\n+  offPairN = urem(offPairN, _fpw1);\n+  offPairN = mul(offPairN, _4);\n+\n+  // sclare\n+  offPairM = mul(offPairM, i32_val(rep[0] / 2));\n+  offQuadM = mul(offQuadM, i32_val(rep[0] / 2));\n+  offPairN = mul(offPairN, i32_val(rep[1] / 2));\n+  offQuadN = mul(offQuadN, i32_val(rep[1] / 2));\n+\n+  // quad pair offset\n+  Value offLaneM = add(offPairM, offQuadM);\n+  Value offLaneN = add(offPairN, offQuadN);\n+  // a, b offset\n+  Value offsetAM = add(offWarpM, offLaneM);\n+  Value offsetBN = add(offWarpN, offLaneN);\n+  // m indices\n+  Value offsetCM = add(and_(lane, _1), offsetAM);\n+  SmallVector<Value> idxM;\n+  for (unsigned m = 0; m < shape[0]; m += shapePerCTA[0])\n+    for (unsigned mm = 0; mm < rep[0]; ++mm)\n+      idxM.push_back(add(offsetCM, i32_val(m + mm * 2)));\n+\n+  // n indices\n+  Value offsetCN = add((and_(lane, _2)), (add(offWarpN, offPairN)));\n+  SmallVector<Value> idxN;\n+  for (int n = 0; n < shape[1]; n += shapePerCTA[1]) {\n+    for (int nn = 0; nn < rep[1]; ++nn) {\n+      idxN.push_back(add(\n+          offsetCN, i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1])));\n+      idxN.push_back(\n+          add(offsetCN,\n+              i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1] + 1)));\n+    }\n+  }\n+\n+  SmallVector<SmallVector<Value>> axes({idxM, idxN});\n+\n+  // product the axis M and axis N to get coords, ported from\n+  // generator::init_idx method from triton2.0\n+\n+  // TODO[Superjomn]: check the order.\n+  SmallVector<CoordTy> coords;\n+  for (Value x1 : axes[1]) {   // N\n+    for (Value x0 : axes[0]) { // M\n+      SmallVector<Value, 2> idx(2);\n+      idx[0] = x0; // M\n+      idx[1] = x1; // N\n+      coords.push_back(std::move(idx));\n+    }\n+  }\n+\n+  return coords; // {M,N} in row-major\n+}\n+\n+Value convertLayout(int opIdx, Value tensor, const SharedMemoryObject &smemObj,\n+                    Value thread, Location loc,\n+                    TritonGPUToLLVMTypeConverter *typeConverter,\n+                    ConversionPatternRewriter &rewriter, Type resultTy) {\n+  if (opIdx == 0)\n+    return loadA(tensor, smemObj, thread, loc, typeConverter, rewriter,\n+                 resultTy);\n+  else {\n+    assert(opIdx == 1);\n+    return loadB(tensor, smemObj, thread, loc, typeConverter, rewriter,\n+                 resultTy);\n+  }\n+}\n+\n+} // namespace SharedToDotOperandMMAv1\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "added", "additions": 685, "deletions": 0, "changes": 685, "file_content_changes": "@@ -0,0 +1,685 @@\n+#include \"ConvertLayoutOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+using ValueTable = std::map<std::pair<unsigned, unsigned>, Value>;\n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getStridesFromShapeAndOrder;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n+using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getOrder;\n+using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+// Data loader for mma.16816 instruction.\n+class MMA16816SmemLoader {\n+public:\n+  MMA16816SmemLoader(int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n+                     ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n+                     ArrayRef<int> instrShape, ArrayRef<int> matShape,\n+                     int perPhase, int maxPhase, int elemBytes,\n+                     ConversionPatternRewriter &rewriter,\n+                     TritonGPUToLLVMTypeConverter *typeConverter,\n+                     const Location &loc);\n+\n+  // lane = thread % 32\n+  // warpOff = (thread/32) % wpt(0)\n+  llvm::SmallVector<Value> computeOffsets(Value warpOff, Value lane,\n+                                          Value cSwizzleOffset) {\n+    if (canUseLdmatrix)\n+      return computeLdmatrixMatOffs(warpOff, lane, cSwizzleOffset);\n+    else if (elemBytes == 4 && needTrans)\n+      return computeB32MatOffs(warpOff, lane, cSwizzleOffset);\n+    else if (elemBytes == 1 && needTrans)\n+      return computeB8MatOffs(warpOff, lane, cSwizzleOffset);\n+    else\n+      llvm::report_fatal_error(\"Invalid smem load config\");\n+\n+    return {};\n+  }\n+\n+  int getNumPtrs() const { return numPtrs; }\n+\n+  // Compute the offset to the matrix this thread(indexed by warpOff and lane)\n+  // mapped to.\n+  SmallVector<Value> computeLdmatrixMatOffs(Value warpId, Value lane,\n+                                            Value cSwizzleOffset);\n+\n+  // Compute 32-bit matrix offsets.\n+  SmallVector<Value> computeB32MatOffs(Value warpOff, Value lane,\n+                                       Value cSwizzleOffset);\n+\n+  // compute 8-bit matrix offset.\n+  SmallVector<Value> computeB8MatOffs(Value warpOff, Value lane,\n+                                      Value cSwizzleOffset);\n+\n+  // Load 4 matrices and returns 4 vec<2> elements.\n+  std::tuple<Value, Value, Value, Value>\n+  loadX4(int mat0, int mat1, ArrayRef<Value> offs, ArrayRef<Value> ptrs,\n+         Type matTy, Type shemPtrTy) const;\n+\n+private:\n+  SmallVector<uint32_t> order;\n+  int kOrder;\n+  SmallVector<int64_t> tileShape;\n+  SmallVector<int> instrShape;\n+  SmallVector<int> matShape;\n+  int perPhase;\n+  int maxPhase;\n+  int elemBytes;\n+  ConversionPatternRewriter &rewriter;\n+  const Location &loc;\n+  MLIRContext *ctx{};\n+\n+  int cMatShape;\n+  int sMatShape;\n+\n+  Value sStride;\n+\n+  bool needTrans;\n+  bool canUseLdmatrix;\n+\n+  int numPtrs;\n+\n+  int pLoadStrideInMat;\n+  int sMatStride;\n+\n+  int matArrStride;\n+  int warpOffStride;\n+};\n+\n+SmallVector<Value>\n+MMA16816SmemLoader::computeLdmatrixMatOffs(Value warpId, Value lane,\n+                                           Value cSwizzleOffset) {\n+  // 4x4 matrices\n+  Value c = urem(lane, i32_val(8));\n+  Value s = udiv(lane, i32_val(8)); // sub-warp-id\n+\n+  // Decompose s => s_0, s_1, that is the coordinate in 2x2 matrices in a\n+  // warp\n+  Value s0 = urem(s, i32_val(2));\n+  Value s1 = udiv(s, i32_val(2));\n+\n+  // We use different orders for a and b for better performance.\n+  Value kMatArr = kOrder == 1 ? s1 : s0;\n+  Value nkMatArr = kOrder == 1 ? s0 : s1;\n+\n+  // matrix coordinate inside a CTA, the matrix layout is [2x2wpt] for A and\n+  // [2wptx2] for B. e.g. Setting wpt=3, The data layout for A(kOrder=1) is\n+  //   |0 0 1 1 2 2| -> 0,1,2 are the warpids\n+  //   |0 0 1 1 2 2|\n+  //\n+  // for B(kOrder=0) is\n+  //   |0 0|  -> 0,1,2 are the warpids\n+  //   |1 1|\n+  //   |2 2|\n+  //   |0 0|\n+  //   |1 1|\n+  //   |2 2|\n+  // Note, for each warp, it handles a 2x2 matrices, that is the coordinate\n+  // address (s0,s1) annotates.\n+\n+  Value matOff[2];\n+  matOff[kOrder ^ 1] =\n+      add(mul(warpId, i32_val(warpOffStride)),   // warp offset\n+          mul(nkMatArr, i32_val(matArrStride))); // matrix offset inside a warp\n+  matOff[kOrder] = kMatArr;\n+\n+  // Physical offset (before swizzling)\n+  Value cMatOff = matOff[order[0]];\n+  Value sMatOff = matOff[order[1]];\n+  Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n+  cMatOff = add(cMatOff, cSwizzleMatOff);\n+\n+  // row offset inside a matrix, each matrix has 8 rows.\n+  Value sOffInMat = c;\n+\n+  SmallVector<Value> offs(numPtrs);\n+  Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n+  Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n+  for (int i = 0; i < numPtrs; ++i) {\n+    Value cMatOffI = add(cMatOff, i32_val(i * pLoadStrideInMat));\n+    cMatOffI = xor_(cMatOffI, phase);\n+    offs[i] = add(mul(cMatOffI, i32_val(cMatShape)), mul(sOff, sStride));\n+  }\n+\n+  return offs;\n+}\n+\n+SmallVector<Value> MMA16816SmemLoader::computeB32MatOffs(Value warpOff,\n+                                                         Value lane,\n+                                                         Value cSwizzleOffset) {\n+  assert(needTrans && \"Only used in transpose mode.\");\n+  // Load tf32 matrices with lds32\n+  Value cOffInMat = udiv(lane, i32_val(4));\n+  Value sOffInMat = urem(lane, i32_val(4));\n+\n+  Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n+  SmallVector<Value> offs(numPtrs);\n+\n+  for (int mat = 0; mat < 4; ++mat) { // Load 4 mats each time\n+    int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n+    int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n+    if (kMatArrInt > 0) // we don't need pointers for k\n+      continue;\n+    Value kMatArr = i32_val(kMatArrInt);\n+    Value nkMatArr = i32_val(nkMatArrInt);\n+\n+    Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n+                        mul(nkMatArr, i32_val(matArrStride)));\n+    Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n+    cMatOff = add(cMatOff, cSwizzleMatOff);\n+\n+    Value sMatOff = kMatArr;\n+    Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n+    // FIXME: (kOrder == 1?) is really dirty hack\n+    for (int i = 0; i < numPtrs / 2; ++i) {\n+      Value cMatOffI =\n+          add(cMatOff, i32_val(i * pLoadStrideInMat * (kOrder == 1 ? 1 : 2)));\n+      cMatOffI = xor_(cMatOffI, phase);\n+      Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n+      cOff = urem(cOff, i32_val(tileShape[order[0]]));\n+      sOff = urem(sOff, i32_val(tileShape[order[1]]));\n+      offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, sStride));\n+    }\n+  }\n+  return offs;\n+}\n+\n+SmallVector<Value> MMA16816SmemLoader::computeB8MatOffs(Value warpOff,\n+                                                        Value lane,\n+                                                        Value cSwizzleOffset) {\n+  assert(needTrans && \"Only used in transpose mode.\");\n+  Value cOffInMat = udiv(lane, i32_val(4));\n+  Value sOffInMat =\n+      mul(urem(lane, i32_val(4)), i32_val(4)); // each thread load 4 cols\n+\n+  SmallVector<Value> offs(numPtrs);\n+  for (int mat = 0; mat < 4; ++mat) {\n+    int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n+    int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n+    if (kMatArrInt > 0) // we don't need pointers for k\n+      continue;\n+    Value kMatArr = i32_val(kMatArrInt);\n+    Value nkMatArr = i32_val(nkMatArrInt);\n+\n+    Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n+                        mul(nkMatArr, i32_val(matArrStride)));\n+    Value sMatOff = kMatArr;\n+\n+    for (int loadx4Off = 0; loadx4Off < numPtrs / 8; ++loadx4Off) {\n+      for (int elemOff = 0; elemOff < 4; ++elemOff) {\n+        int ptrOff = loadx4Off * 8 + nkMatArrInt * 4 + elemOff;\n+        Value cMatOffI = add(cMatOff, i32_val(loadx4Off * pLoadStrideInMat *\n+                                              (kOrder == 1 ? 1 : 2)));\n+        Value sOffInMatElem = add(sOffInMat, i32_val(elemOff));\n+\n+        // disable swizzling ...\n+\n+        Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n+        Value sOff = add(sOffInMatElem, mul(sMatOff, i32_val(sMatShape)));\n+        // To prevent out-of-bound access when tile is too small.\n+        cOff = urem(cOff, i32_val(tileShape[order[0]]));\n+        sOff = urem(sOff, i32_val(tileShape[order[1]]));\n+        offs[ptrOff] = add(cOff, mul(sOff, sStride));\n+      }\n+    }\n+  }\n+  return offs;\n+}\n+\n+std::tuple<Value, Value, Value, Value>\n+MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> offs,\n+                           ArrayRef<Value> ptrs, Type matTy,\n+                           Type shemPtrTy) const {\n+  assert(mat0 % 2 == 0 && mat1 % 2 == 0 && \"smem matrix load must be aligned\");\n+  int matIdx[2] = {mat0, mat1};\n+\n+  int ptrIdx{-1};\n+\n+  if (canUseLdmatrix)\n+    ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n+  else if (elemBytes == 4 && needTrans)\n+    ptrIdx = matIdx[order[0]];\n+  else if (elemBytes == 1 && needTrans)\n+    ptrIdx = matIdx[order[0]] * 4;\n+  else\n+    llvm::report_fatal_error(\"unsupported mma type found\");\n+\n+  // The main difference with the original triton code is we removed the\n+  // prefetch-related logic here for the upstream optimizer phase should\n+  // take care with it, and that is transparent in dot conversion.\n+  auto getPtr = [&](int idx) { return ptrs[idx]; };\n+\n+  Value ptr = getPtr(ptrIdx);\n+\n+  // The struct should have exactly the same element types.\n+  auto resTy = matTy.cast<LLVM::LLVMStructType>();\n+  Type elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n+\n+  // For some reasons, LLVM's NVPTX backend inserts unnecessary (?) integer\n+  // instructions to pack & unpack sub-word integers. A workaround is to\n+  // store the results of ldmatrix in i32\n+  if (auto vecElemTy = elemTy.dyn_cast<VectorType>()) {\n+    Type elemElemTy = vecElemTy.getElementType();\n+    if (auto intTy = elemElemTy.dyn_cast<IntegerType>()) {\n+      if (intTy.getWidth() <= 16) {\n+        elemTy = rewriter.getI32Type();\n+        resTy =\n+            LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, elemTy));\n+      }\n+    }\n+  }\n+\n+  if (canUseLdmatrix) {\n+    Value sOffset =\n+        mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sStride);\n+    Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n+\n+    PTXBuilder builder;\n+    // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n+    // thread.\n+    auto resArgs = builder.newListOperand(4, \"=r\");\n+    auto addrArg = builder.newAddrOperand(sOffsetPtr, \"r\");\n+\n+    auto ldmatrix = builder.create(\"ldmatrix.sync.aligned.m8n8.x4\")\n+                        ->o(\"trans\", needTrans /*predicate*/)\n+                        .o(\"shared.b16\");\n+    ldmatrix(resArgs, addrArg);\n+\n+    // The result type is 4xi32, each i32 is composed of 2xf16\n+    // elements (adjacent two columns in a row) or a single f32 element.\n+    Value resV4 = builder.launch(rewriter, loc, resTy);\n+    return {extract_val(elemTy, resV4, 0), extract_val(elemTy, resV4, 1),\n+            extract_val(elemTy, resV4, 2), extract_val(elemTy, resV4, 3)};\n+  } else if (elemBytes == 4 && needTrans) { // Use lds.32 to load tf32 matrices\n+    Value ptr2 = getPtr(ptrIdx + 1);\n+    assert(sMatStride == 1);\n+    int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n+    Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n+    int sOffsetArrElem = sMatStride * sMatShape;\n+    Value sOffsetArrElemVal =\n+        add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n+\n+    Value elems[4];\n+    if (kOrder == 1) {\n+      elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n+      elems[1] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n+      elems[2] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n+      elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n+    } else {\n+      elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n+      elems[2] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n+      elems[1] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n+      elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n+    }\n+    std::array<Value, 4> retElems;\n+    retElems.fill(undef(elemTy));\n+    for (auto i = 0; i < 4; ++i) {\n+      retElems[i] = insert_element(elemTy, retElems[i], elems[i], i32_val(0));\n+    }\n+    return {retElems[0], retElems[1], retElems[2], retElems[3]};\n+  } else if (elemBytes == 1 && needTrans) { // work with int8\n+    // Can't use i32 here. Use LLVM's VectorType\n+    elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n+    std::array<std::array<Value, 4>, 2> ptrs;\n+    ptrs[0] = {\n+        getPtr(ptrIdx),\n+        getPtr(ptrIdx + 1),\n+        getPtr(ptrIdx + 2),\n+        getPtr(ptrIdx + 3),\n+    };\n+\n+    ptrs[1] = {\n+        getPtr(ptrIdx + 4),\n+        getPtr(ptrIdx + 5),\n+        getPtr(ptrIdx + 6),\n+        getPtr(ptrIdx + 7),\n+    };\n+\n+    assert(sMatStride == 1);\n+    int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n+    Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n+    int sOffsetArrElem = 1 * (sMatStride * sMatShape);\n+    Value sOffsetArrElemVal =\n+        add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n+\n+    std::array<Value, 4> i8v4Elems;\n+    i8v4Elems.fill(undef(elemTy));\n+\n+    Value i8Elems[4][4];\n+    if (kOrder == 1) {\n+      for (int i = 0; i < 2; ++i)\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[i][j] = load(gep(shemPtrTy, ptrs[i][j], sOffsetElemVal));\n+\n+      for (int i = 2; i < 4; ++i)\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[i][j] =\n+              load(gep(shemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n+\n+      for (int m = 0; m < 4; ++m) {\n+        for (int e = 0; e < 4; ++e)\n+          i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n+                                        i8Elems[m][e], i32_val(e));\n+      }\n+    } else { // k first\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[0][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetElemVal));\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[2][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetElemVal));\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[1][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[3][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n+\n+      for (int m = 0; m < 4; ++m) {\n+        for (int e = 0; e < 4; ++e)\n+          i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n+                                        i8Elems[m][e], i32_val(e));\n+      }\n+    }\n+\n+    return {bitcast(i8v4Elems[0], i32_ty), bitcast(i8v4Elems[1], i32_ty),\n+            bitcast(i8v4Elems[2], i32_ty), bitcast(i8v4Elems[3], i32_ty)};\n+  }\n+\n+  assert(false && \"Invalid smem load\");\n+  return {Value{}, Value{}, Value{}, Value{}};\n+}\n+\n+MMA16816SmemLoader::MMA16816SmemLoader(\n+    int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n+    ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n+    ArrayRef<int> instrShape, ArrayRef<int> matShape, int perPhase,\n+    int maxPhase, int elemBytes, ConversionPatternRewriter &rewriter,\n+    TritonGPUToLLVMTypeConverter *typeConverter, const Location &loc)\n+    : order(order.begin(), order.end()), kOrder(kOrder),\n+      tileShape(tileShape.begin(), tileShape.end()),\n+      instrShape(instrShape.begin(), instrShape.end()),\n+      matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n+      maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter), loc(loc),\n+      ctx(rewriter.getContext()) {\n+  cMatShape = matShape[order[0]];\n+  sMatShape = matShape[order[1]];\n+\n+  sStride = smemStrides[order[1]];\n+\n+  // rule: k must be the fast-changing axis.\n+  needTrans = kOrder != order[0];\n+  canUseLdmatrix = elemBytes == 2 || (!needTrans); // b16\n+\n+  if (canUseLdmatrix) {\n+    // Each CTA, the warps is arranged as [1xwpt] if not transposed,\n+    // otherwise [wptx1], and each warp will perform a mma.\n+    numPtrs =\n+        tileShape[order[0]] / (needTrans ? wpt : 1) / instrShape[order[0]];\n+  } else {\n+    numPtrs = tileShape[order[0]] / wpt / matShape[order[0]];\n+  }\n+  numPtrs = std::max<int>(numPtrs, 2);\n+\n+  // Special rule for i8/u8, 4 ptrs for each matrix\n+  if (!canUseLdmatrix && elemBytes == 1)\n+    numPtrs *= 4;\n+\n+  int loadStrideInMat[2];\n+  loadStrideInMat[kOrder] =\n+      2; // instrShape[kOrder] / matShape[kOrder], always 2\n+  loadStrideInMat[kOrder ^ 1] =\n+      wpt * (instrShape[kOrder ^ 1] / matShape[kOrder ^ 1]);\n+\n+  pLoadStrideInMat = loadStrideInMat[order[0]];\n+  sMatStride =\n+      loadStrideInMat[order[1]] / (instrShape[order[1]] / matShape[order[1]]);\n+\n+  // Each matArr contains warpOffStride matrices.\n+  matArrStride = kOrder == 1 ? 1 : wpt;\n+  warpOffStride = instrShape[kOrder ^ 1] / matShape[kOrder ^ 1];\n+}\n+\n+Type getShemPtrTy(Type argType) {\n+  MLIRContext *ctx = argType.getContext();\n+  if (argType.isF16())\n+    return ptr_ty(type::f16Ty(ctx), 3);\n+  else if (argType.isBF16())\n+    return ptr_ty(type::i16Ty(ctx), 3);\n+  else if (argType.isF32())\n+    return ptr_ty(type::f32Ty(ctx), 3);\n+  else if (argType.isInteger(8))\n+    return ptr_ty(type::i8Ty(ctx), 3);\n+  else\n+    llvm::report_fatal_error(\"mma16816 data type not supported\");\n+}\n+\n+Type getMatType(Type argType) {\n+  MLIRContext *ctx = argType.getContext();\n+  // floating point types\n+  Type fp32x1Ty = vec_ty(type::f32Ty(ctx), 1);\n+  Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n+  Type i16x2Ty = vec_ty(type::i16Ty(ctx), 2);\n+  Type fp16x2Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n+  // LLVM 14.0 does not support bf16 type, so we use i16 instead.\n+  Type bf16x2Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i16x2Ty));\n+  Type fp32Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32x1Ty));\n+  // integer types\n+  Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n+  Type i8x4Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i8x4Ty));\n+\n+  if (argType.isF16())\n+    return fp16x2Pack4Ty;\n+  else if (argType.isBF16())\n+    return bf16x2Pack4Ty;\n+  else if (argType.isF32())\n+    return fp32Pack4Ty;\n+  else if (argType.isInteger(8))\n+    return i8x4Pack4Ty;\n+  else\n+    llvm::report_fatal_error(\"mma16816 data type not supported\");\n+}\n+\n+Value composeValuesToDotOperandLayoutStruct(\n+    const ValueTable &vals, int n0, int n1,\n+    TritonGPUToLLVMTypeConverter *typeConverter, Location loc,\n+    ConversionPatternRewriter &rewriter) {\n+  std::vector<Value> elems;\n+  for (int m = 0; m < n0; ++m)\n+    for (int k = 0; k < n1; ++k) {\n+      elems.push_back(vals.at({2 * m, 2 * k}));\n+      elems.push_back(vals.at({2 * m, 2 * k + 1}));\n+      elems.push_back(vals.at({2 * m + 1, 2 * k}));\n+      elems.push_back(vals.at({2 * m + 1, 2 * k + 1}));\n+    }\n+\n+  assert(!elems.empty());\n+\n+  Type elemTy = elems[0].getType();\n+  MLIRContext *ctx = elemTy.getContext();\n+  Type structTy = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(elems.size(), elemTy));\n+  auto result = typeConverter->packLLElements(loc, elems, rewriter, structTy);\n+  return result;\n+}\n+\n+std::function<void(int, int)>\n+getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n+                MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder,\n+                SmallVector<int> instrShape, SmallVector<int> matShape,\n+                Value warpId, Value lane, ValueTable &vals, bool isA,\n+                TritonGPUToLLVMTypeConverter *typeConverter,\n+                ConversionPatternRewriter &rewriter, Location loc) {\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  Type eltTy = tensorTy.getElementType();\n+  // We assumes that the input operand of Dot should be from shared layout.\n+  // TODO(Superjomn) Consider other layouts if needed later.\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  const int perPhase = sharedLayout.getPerPhase();\n+  const int maxPhase = sharedLayout.getMaxPhase();\n+  const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n+  auto order = sharedLayout.getOrder();\n+\n+  // the original register_lds2, but discard the prefetch logic.\n+  auto ld2 = [](ValueTable &vals, int mn, int k, Value val) {\n+    vals[{mn, k}] = val;\n+  };\n+\n+  // (a, b) is the coordinate.\n+  auto load = [=, &rewriter, &vals, &ld2](int a, int b) {\n+    MMA16816SmemLoader loader(\n+        wpt, sharedLayout.getOrder(), kOrder, smemObj.strides,\n+        tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n+        maxPhase, elemBytes, rewriter, typeConverter, loc);\n+    Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+    SmallVector<Value> offs =\n+        loader.computeOffsets(warpId, lane, cSwizzleOffset);\n+    const int numPtrs = loader.getNumPtrs();\n+    SmallVector<Value> ptrs(numPtrs);\n+\n+    Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+\n+    Type smemPtrTy = getShemPtrTy(eltTy);\n+    for (int i = 0; i < numPtrs; ++i) {\n+      ptrs[i] =\n+          bitcast(gep(smemPtrTy, smemBase, ValueRange({offs[i]})), smemPtrTy);\n+    }\n+\n+    auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n+        (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n+        ptrs, getMatType(eltTy), getShemPtrTy(eltTy));\n+\n+    if (isA) {\n+      ld2(vals, a, b, ha0);\n+      ld2(vals, a + 1, b, ha1);\n+      ld2(vals, a, b + 1, ha2);\n+      ld2(vals, a + 1, b + 1, ha3);\n+    } else {\n+      ld2(vals, a, b, ha0);\n+      ld2(vals, a + 1, b, ha2);\n+      ld2(vals, a, b + 1, ha1);\n+      ld2(vals, a + 1, b + 1, ha3);\n+    }\n+  };\n+\n+  return load;\n+}\n+\n+Value loadA(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n+            DotOperandEncodingAttr aEncoding, const SharedMemoryObject &smemObj,\n+            TritonGPUToLLVMTypeConverter *typeConverter, Value thread) {\n+  auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n+  int bitwidth = aTensorTy.getElementTypeBitWidth();\n+  auto mmaLayout = aEncoding.getParent().cast<MmaEncodingAttr>();\n+\n+  SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n+                             aTensorTy.getShape().end());\n+\n+  ValueTable ha;\n+  std::function<void(int, int)> loadFn;\n+  int mmaInstrM = 16, mmaInstrN = 8, mmaInstrK = 4 * 64 / bitwidth;\n+  int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n+\n+  auto numRep = aEncoding.getMMAv2Rep(aTensorTy.getShape(), bitwidth);\n+  int numRepM = numRep[0], numRepK = numRep[1];\n+\n+  if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n+    int wpt0 = mmaLayout.getWarpsPerCTA()[0];\n+    Value warp = udiv(thread, i32_val(32));\n+    Value lane = urem(thread, i32_val(32));\n+    Value warpM = urem(urem(warp, i32_val(wpt0)), i32_val(shape[0] / 16));\n+    // load from smem\n+    // we use ldmatrix.x4 so each warp processes 16x16 elements.\n+    int wpt = std::min<int>(wpt0, shape[0] / 16);\n+    loadFn = getLoadMatrixFn(\n+        tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/,\n+        {mmaInstrM, mmaInstrK} /*instrShape*/,\n+        {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, lane /*laneId*/,\n+        ha /*vals*/, true /*isA*/, typeConverter /* typeConverter */,\n+        rewriter /*rewriter*/, loc /*loc*/);\n+  } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n+    // load from registers, used in gemm fuse\n+    // TODO(Superjomn) Port the logic.\n+    assert(false && \"Loading A from register is not supported yet.\");\n+  } else {\n+    assert(false && \"A's layout is not supported.\");\n+  }\n+\n+  // step1. Perform loading.\n+  for (int m = 0; m < numRepM; ++m)\n+    for (int k = 0; k < numRepK; ++k)\n+      loadFn(2 * m, 2 * k);\n+\n+  // step2. Format the values to LLVM::Struct to passing to mma codegen.\n+  return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK,\n+                                               typeConverter, loc, rewriter);\n+}\n+\n+Value loadB(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n+            DotOperandEncodingAttr bEncoding, const SharedMemoryObject &smemObj,\n+            TritonGPUToLLVMTypeConverter *typeConverter, Value thread) {\n+  ValueTable hb;\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  int bitwidth = tensorTy.getElementTypeBitWidth();\n+  auto mmaLayout = bEncoding.getParent().cast<MmaEncodingAttr>();\n+\n+  SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n+                             tensorTy.getShape().end());\n+\n+  // TODO[Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n+  bool transB = false;\n+  if (transB) {\n+    std::swap(shape[0], shape[1]);\n+  }\n+\n+  int mmaInstrM = 16, mmaInstrN = 8, mmaInstrK = 4 * 64 / bitwidth;\n+  int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n+\n+  auto numRep = bEncoding.getMMAv2Rep(tensorTy.getShape(), bitwidth);\n+  int numRepK = numRep[0];\n+  int numRepN = numRep[1];\n+\n+  int wpt0 = mmaLayout.getWarpsPerCTA()[0];\n+  int wpt1 = mmaLayout.getWarpsPerCTA()[1];\n+  Value warp = udiv(thread, i32_val(32));\n+  Value lane = urem(thread, i32_val(32));\n+  Value warpMN = udiv(warp, i32_val(wpt0));\n+  Value warpN = urem(urem(warpMN, i32_val(wpt1)), i32_val(shape[1] / 8));\n+  // we use ldmatrix.x4 so each warp processes 16x16 elements.\n+  int wpt = std::min<int>(wpt1, shape[1] / 16);\n+  auto loadFn = getLoadMatrixFn(\n+      tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/,\n+      {mmaInstrK, mmaInstrN} /*instrShape*/,\n+      {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, lane /*laneId*/,\n+      hb /*vals*/, false /*isA*/, typeConverter /* typeConverter */,\n+      rewriter /*rewriter*/, loc /*loc*/);\n+\n+  for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n+    for (int k = 0; k < numRepK; ++k)\n+      loadFn(2 * n, 2 * k);\n+  }\n+\n+  Value result = composeValuesToDotOperandLayoutStruct(\n+      hb, std::max(numRepN / 2, 1), numRepK, typeConverter, loc, rewriter);\n+  return result;\n+}\n+\n+namespace SharedToDotOperandMMAv2 {\n+Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,\n+                    Location loc, Value tensor, DotOperandEncodingAttr encoding,\n+                    const SharedMemoryObject &smemObj,\n+                    TritonGPUToLLVMTypeConverter *typeConverter, Value thread) {\n+  if (opIdx == 0)\n+    return loadA(rewriter, loc, tensor, encoding, smemObj, typeConverter,\n+                 thread);\n+  else {\n+    assert(opIdx == 1);\n+    return loadB(rewriter, loc, tensor, encoding, smemObj, typeConverter,\n+                 thread);\n+  }\n+}\n+} // namespace SharedToDotOperandMMAv2\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.cpp", "status": "removed", "additions": 0, "deletions": 1370, "changes": 1370, "file_content_changes": "@@ -1,1370 +0,0 @@\n-#include \"DotOpHelpers.h\"\n-#include \"TypeConverter.h\"\n-\n-namespace mlir {\n-namespace LLVM {\n-\n-Value DotOpMmaV1ConversionHelper::loadA(\n-    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n-    TritonGPUToLLVMTypeConverter *typeConverter,\n-    ConversionPatternRewriter &rewriter, Type resultTy) const {\n-  auto *ctx = rewriter.getContext();\n-  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto shape = tensorTy.getShape();\n-  auto order = sharedLayout.getOrder();\n-\n-  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n-  Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-\n-  bool isARow = order[0] != 0;\n-  auto resultEncoding = resultTy.cast<RankedTensorType>()\n-                            .getEncoding()\n-                            .cast<DotOperandEncodingAttr>();\n-  auto [offsetAM, offsetAK, _3, _4] = computeOffsets(\n-      thread, isARow, false, fpw, resultEncoding.getMMAv1ShapePerWarp(),\n-      resultEncoding.getMMAv1Rep(), rewriter, loc);\n-\n-  int vecA = sharedLayout.getVec();\n-\n-  auto strides = smemObj.strides;\n-  Value strideAM = isARow ? strides[0] : i32_val(1);\n-  Value strideAK = isARow ? i32_val(1) : strides[1];\n-  Value strideA0 = isARow ? strideAK : strideAM;\n-  Value strideA1 = isARow ? strideAM : strideAK;\n-\n-  int strideRepM = wpt[0] * fpw[0] * 8;\n-  int strideRepK = 1;\n-\n-  // swizzling\n-  int perPhaseA = sharedLayout.getPerPhase();\n-  int maxPhaseA = sharedLayout.getMaxPhase();\n-  int stepA0 = isARow ? strideRepK : strideRepM;\n-  int numPtrA = std::max(2 * perPhaseA * maxPhaseA / stepA0, 1);\n-  int NK = shape[1];\n-\n-  // pre-compute pointer lanes\n-  Value offA0 = isARow ? offsetAK : offsetAM;\n-  Value offA1 = isARow ? offsetAM : offsetAK;\n-  Value phaseA = urem(udiv(offA1, i32_val(perPhaseA)), i32_val(maxPhaseA));\n-  offA0 = add(offA0, cSwizzleOffset);\n-  SmallVector<Value> offA(numPtrA);\n-  for (int i = 0; i < numPtrA; i++) {\n-    Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));\n-    offA0I = udiv(offA0I, i32_val(vecA));\n-    offA0I = xor_(offA0I, phaseA);\n-    offA0I = mul(offA0I, i32_val(vecA));\n-    offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n-  }\n-\n-  Type elemX2Ty = vec_ty(f16_ty, 2);\n-  Type elemPtrTy = ptr_ty(f16_ty, 3);\n-  if (tensorTy.getElementType().isBF16()) {\n-    elemX2Ty = vec_ty(i16_ty, 2);\n-    elemPtrTy = ptr_ty(i16_ty, 3);\n-  }\n-\n-  // prepare arguments\n-  SmallVector<Value> ptrA(numPtrA);\n-\n-  std::map<std::pair<int, int>, std::pair<Value, Value>> has;\n-  for (int i = 0; i < numPtrA; i++)\n-    ptrA[i] = gep(ptr_ty(f16_ty, 3), smemBase, offA[i]);\n-\n-  auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n-    vals[{m, k}] = {val0, val1};\n-  };\n-  auto loadA = [&](int m, int k) {\n-    int offidx = (isARow ? k / 4 : m) % numPtrA;\n-    Value thePtrA = gep(elemPtrTy, smemBase, offA[offidx]);\n-\n-    int stepAM = isARow ? m : m / numPtrA * numPtrA;\n-    int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n-    Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),\n-                       mul(i32_val(stepAK), strideAK));\n-    Value pa = gep(elemPtrTy, thePtrA, offset);\n-    Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n-    Value ha = load(bitcast(pa, aPtrTy));\n-    // record lds that needs to be moved\n-    Value ha00 = bitcast(extract_element(ha, i32_val(0)), elemX2Ty);\n-    Value ha01 = bitcast(extract_element(ha, i32_val(1)), elemX2Ty);\n-    ld(has, m, k, ha00, ha01);\n-\n-    if (vecA > 4) {\n-      Value ha10 = bitcast(extract_element(ha, i32_val(2)), elemX2Ty);\n-      Value ha11 = bitcast(extract_element(ha, i32_val(3)), elemX2Ty);\n-      if (isARow)\n-        ld(has, m, k + 4, ha10, ha11);\n-      else\n-        ld(has, m + 1, k, ha10, ha11);\n-    }\n-  };\n-\n-  bool isARow_ = resultEncoding.getMMAv1IsRow();\n-  bool isAVec4 = resultEncoding.getMMAv1IsVec4();\n-  unsigned numM = resultEncoding.getMMAv1NumOuter(shape);\n-  for (unsigned k = 0; k < NK; k += 4)\n-    for (unsigned m = 0; m < numM / 2; ++m)\n-      if (!has.count({m, k}))\n-        loadA(m, k);\n-\n-  SmallVector<Value> elems;\n-  elems.reserve(has.size() * 2);\n-  for (auto item : has) { // has is a map, the key should be ordered.\n-    elems.push_back(item.second.first);\n-    elems.push_back(item.second.second);\n-  }\n-\n-  Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);\n-  return res;\n-}\n-\n-Value DotOpMmaV1ConversionHelper::loadB(\n-    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n-    TritonGPUToLLVMTypeConverter *typeConverter,\n-    ConversionPatternRewriter &rewriter, Type resultTy) const {\n-  // smem\n-  auto strides = smemObj.strides;\n-\n-  auto *ctx = rewriter.getContext();\n-  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-\n-  auto shape = tensorTy.getShape();\n-  auto order = sharedLayout.getOrder();\n-\n-  Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-  bool isBRow = order[0] != 0; // is row-major in shared memory layout\n-  // isBRow_ indicates whether B is row-major in DotOperand layout\n-  auto resultEncoding = resultTy.cast<RankedTensorType>()\n-                            .getEncoding()\n-                            .cast<DotOperandEncodingAttr>();\n-\n-  int vecB = sharedLayout.getVec();\n-  Value strideBN = isBRow ? i32_val(1) : strides[1];\n-  Value strideBK = isBRow ? strides[0] : i32_val(1);\n-  Value strideB0 = isBRow ? strideBN : strideBK;\n-  Value strideB1 = isBRow ? strideBK : strideBN;\n-  int strideRepN = wpt[1] * fpw[1] * 8;\n-  int strideRepK = 1;\n-\n-  auto [_3, _4, offsetBN, offsetBK] = computeOffsets(\n-      thread, false, isBRow, fpw, resultEncoding.getMMAv1ShapePerWarp(),\n-      resultEncoding.getMMAv1Rep(), rewriter, loc);\n-\n-  // swizzling\n-  int perPhaseB = sharedLayout.getPerPhase();\n-  int maxPhaseB = sharedLayout.getMaxPhase();\n-  int stepB0 = isBRow ? strideRepN : strideRepK;\n-  int numPtrB = std::max(2 * perPhaseB * maxPhaseB / stepB0, 1);\n-  int NK = shape[0];\n-\n-  Value offB0 = isBRow ? offsetBN : offsetBK;\n-  Value offB1 = isBRow ? offsetBK : offsetBN;\n-  Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n-  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n-\n-  offB0 = add(offB0, cSwizzleOffset);\n-  SmallVector<Value> offB(numPtrB);\n-  for (int i = 0; i < numPtrB; ++i) {\n-    Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));\n-    offB0I = udiv(offB0I, i32_val(vecB));\n-    offB0I = xor_(offB0I, phaseB);\n-    offB0I = mul(offB0I, i32_val(vecB));\n-    offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));\n-  }\n-\n-  Type elemPtrTy = ptr_ty(f16_ty, 3);\n-  Type elemX2Ty = vec_ty(f16_ty, 2);\n-  if (tensorTy.getElementType().isBF16()) {\n-    elemPtrTy = ptr_ty(i16_ty, 3);\n-    elemX2Ty = vec_ty(i16_ty, 2);\n-  }\n-\n-  SmallVector<Value> ptrB(numPtrB);\n-  ValueTable hbs;\n-  for (int i = 0; i < numPtrB; ++i)\n-    ptrB[i] = gep(ptr_ty(f16_ty, 3), smem, offB[i]);\n-\n-  auto ld = [&](decltype(hbs) &vals, int m, int k, Value val0, Value val1) {\n-    vals[{m, k}] = {val0, val1};\n-  };\n-\n-  auto loadB = [&](int n, int K) {\n-    int offidx = (isBRow ? n : K / 4) % numPtrB;\n-    Value thePtrB = ptrB[offidx];\n-\n-    int stepBN = isBRow ? n / numPtrB * numPtrB : n;\n-    int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n-    Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n-                       mul(i32_val(stepBK), strideBK));\n-    Value pb = gep(elemPtrTy, thePtrB, offset);\n-\n-    Value hb =\n-        load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n-    // record lds that needs to be moved\n-    Value hb00 = bitcast(extract_element(hb, i32_val(0)), elemX2Ty);\n-    Value hb01 = bitcast(extract_element(hb, i32_val(1)), elemX2Ty);\n-    ld(hbs, n, K, hb00, hb01);\n-    if (vecB > 4) {\n-      Value hb10 = bitcast(extract_element(hb, i32_val(2)), elemX2Ty);\n-      Value hb11 = bitcast(extract_element(hb, i32_val(3)), elemX2Ty);\n-      if (isBRow)\n-        ld(hbs, n + 1, K, hb10, hb11);\n-      else\n-        ld(hbs, n, K + 4, hb10, hb11);\n-    }\n-  };\n-\n-  bool isBRow_ = resultEncoding.getMMAv1IsRow();\n-  assert(isBRow == isBRow_ && \"B need smem isRow\");\n-  bool isBVec4 = resultEncoding.getMMAv1IsVec4();\n-  unsigned numN = resultEncoding.getMMAv1NumOuter(shape);\n-  for (unsigned k = 0; k < NK; k += 4)\n-    for (unsigned n = 0; n < numN / 2; ++n) {\n-      if (!hbs.count({n, k}))\n-        loadB(n, k);\n-    }\n-\n-  SmallVector<Value> elems;\n-  for (auto &item : hbs) { // has is a map, the key should be ordered.\n-    elems.push_back(item.second.first);\n-    elems.push_back(item.second.second);\n-  }\n-\n-  Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);\n-  return res;\n-}\n-\n-std::tuple<Value, Value, Value, Value>\n-DotOpMmaV1ConversionHelper::computeOffsets(Value threadId, bool isARow,\n-                                           bool isBRow, ArrayRef<int> fpw,\n-                                           ArrayRef<int> spw, ArrayRef<int> rep,\n-                                           ConversionPatternRewriter &rewriter,\n-                                           Location loc) const {\n-  auto *ctx = rewriter.getContext();\n-  Value _1 = i32_val(1);\n-  Value _3 = i32_val(3);\n-  Value _4 = i32_val(4);\n-  Value _16 = i32_val(16);\n-  Value _32 = i32_val(32);\n-\n-  Value lane = urem(threadId, _32);\n-  Value warp = udiv(threadId, _32);\n-\n-  // warp offset\n-  Value warp0 = urem(warp, i32_val(wpt[0]));\n-  Value warp12 = udiv(warp, i32_val(wpt[0]));\n-  Value warp1 = urem(warp12, i32_val(wpt[1]));\n-  Value warpMOff = mul(warp0, i32_val(spw[0]));\n-  Value warpNOff = mul(warp1, i32_val(spw[1]));\n-  // Quad offset\n-  Value quadMOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[0]));\n-  Value quadNOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[1]));\n-  // Pair offset\n-  Value pairMOff = udiv(urem(lane, _16), _4);\n-  pairMOff = urem(pairMOff, i32_val(fpw[0]));\n-  pairMOff = mul(pairMOff, _4);\n-  Value pairNOff = udiv(urem(lane, _16), _4);\n-  pairNOff = udiv(pairNOff, i32_val(fpw[0]));\n-  pairNOff = urem(pairNOff, i32_val(fpw[1]));\n-  pairNOff = mul(pairNOff, _4);\n-  // scale\n-  pairMOff = mul(pairMOff, i32_val(rep[0] / 2));\n-  quadMOff = mul(quadMOff, i32_val(rep[0] / 2));\n-  pairNOff = mul(pairNOff, i32_val(rep[1] / 2));\n-  quadNOff = mul(quadNOff, i32_val(rep[1] / 2));\n-  // Quad pair offset\n-  Value laneMOff = add(pairMOff, quadMOff);\n-  Value laneNOff = add(pairNOff, quadNOff);\n-  // A offset\n-  Value offsetAM = add(warpMOff, laneMOff);\n-  Value offsetAK = and_(lane, _3);\n-  // B offset\n-  Value offsetBN = add(warpNOff, laneNOff);\n-  Value offsetBK = and_(lane, _3);\n-  // i indices\n-  Value offsetCM = add(and_(lane, _1), offsetAM);\n-  if (isARow) {\n-    offsetAM = add(offsetAM, urem(threadId, _4));\n-    offsetAK = i32_val(0);\n-  }\n-  if (!isBRow) {\n-    offsetBN = add(offsetBN, urem(threadId, _4));\n-    offsetBK = i32_val(0);\n-  }\n-\n-  return std::make_tuple(offsetAM, offsetAK, offsetBN, offsetBK);\n-}\n-\n-DotOpMmaV1ConversionHelper::ValueTable\n-DotOpMmaV1ConversionHelper::extractLoadedOperand(\n-    Value llStruct, int NK, ConversionPatternRewriter &rewriter,\n-    TritonGPUToLLVMTypeConverter *typeConverter, Type type) const {\n-  ValueTable rcds;\n-  SmallVector<Value> elems = typeConverter->unpackLLElements(\n-      llStruct.getLoc(), llStruct, rewriter, type);\n-\n-  int offset = 0;\n-  for (int i = 0; offset < elems.size(); ++i) {\n-    for (int k = 0; k < NK; k += 4) {\n-      rcds[{i, k}] = std::make_pair(elems[offset], elems[offset + 1]);\n-      offset += 2;\n-    }\n-  }\n-\n-  return rcds;\n-}\n-\n-// TODO: Mostly a duplicate of TritonGPUToLLVMBase::emitBaseIndexforMMaLayoutV1\n-SmallVector<DotOpMmaV1ConversionHelper::CoordTy>\n-DotOpMmaV1ConversionHelper::getMNCoords(Value thread,\n-                                        ConversionPatternRewriter &rewriter,\n-                                        ArrayRef<unsigned int> wpt,\n-                                        const MmaEncodingAttr &mmaLayout,\n-                                        ArrayRef<int64_t> shape, bool isARow,\n-                                        bool isBRow, bool isAVec4,\n-                                        bool isBVec4) {\n-\n-  auto *ctx = thread.getContext();\n-  auto loc = UnknownLoc::get(ctx);\n-  Value _1 = i32_val(1);\n-  Value _2 = i32_val(2);\n-  Value _4 = i32_val(4);\n-  Value _16 = i32_val(16);\n-  Value _32 = i32_val(32);\n-  Value _fpw0 = i32_val(fpw[0]);\n-  Value _fpw1 = i32_val(fpw[1]);\n-\n-  // A info\n-  auto aEncoding = DotOperandEncodingAttr::get(ctx, 0, mmaLayout);\n-  auto aRep = aEncoding.getMMAv1Rep();\n-  auto aSpw = aEncoding.getMMAv1ShapePerWarp();\n-  // B info\n-  auto bEncoding = DotOperandEncodingAttr::get(ctx, 1, mmaLayout);\n-  auto bSpw = bEncoding.getMMAv1ShapePerWarp();\n-  auto bRep = bEncoding.getMMAv1Rep();\n-\n-  SmallVector<int, 2> rep({aRep[0], bRep[1]});\n-  SmallVector<int, 2> spw({aSpw[0], bSpw[1]});\n-  SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n-\n-  Value lane = urem(thread, _32);\n-  Value warp = udiv(thread, _32);\n-\n-  Value warp0 = urem(warp, i32_val(wpt[0]));\n-  Value warp12 = udiv(warp, i32_val(wpt[0]));\n-  Value warp1 = urem(warp12, i32_val(wpt[1]));\n-\n-  // warp offset\n-  Value offWarpM = mul(warp0, i32_val(spw[0]));\n-  Value offWarpN = mul(warp1, i32_val(spw[1]));\n-  // quad offset\n-  Value offQuadM = mul(udiv(and_(lane, _16), _4), _fpw0);\n-  Value offQuadN = mul(udiv(and_(lane, _16), _4), _fpw1);\n-  // pair offset\n-  Value offPairM = udiv(urem(lane, _16), _4);\n-  offPairM = urem(offPairM, _fpw0);\n-  offPairM = mul(offPairM, _4);\n-  Value offPairN = udiv(urem(lane, _16), _4);\n-  offPairN = udiv(offPairN, _fpw0);\n-  offPairN = urem(offPairN, _fpw1);\n-  offPairN = mul(offPairN, _4);\n-\n-  // sclare\n-  offPairM = mul(offPairM, i32_val(rep[0] / 2));\n-  offQuadM = mul(offQuadM, i32_val(rep[0] / 2));\n-  offPairN = mul(offPairN, i32_val(rep[1] / 2));\n-  offQuadN = mul(offQuadN, i32_val(rep[1] / 2));\n-\n-  // quad pair offset\n-  Value offLaneM = add(offPairM, offQuadM);\n-  Value offLaneN = add(offPairN, offQuadN);\n-  // a, b offset\n-  Value offsetAM = add(offWarpM, offLaneM);\n-  Value offsetBN = add(offWarpN, offLaneN);\n-  // m indices\n-  Value offsetCM = add(and_(lane, _1), offsetAM);\n-  SmallVector<Value> idxM;\n-  for (unsigned m = 0; m < shape[0]; m += shapePerCTA[0])\n-    for (unsigned mm = 0; mm < rep[0]; ++mm)\n-      idxM.push_back(add(offsetCM, i32_val(m + mm * 2)));\n-\n-  // n indices\n-  Value offsetCN = add((and_(lane, _2)), (add(offWarpN, offPairN)));\n-  SmallVector<Value> idxN;\n-  for (int n = 0; n < shape[1]; n += shapePerCTA[1]) {\n-    for (int nn = 0; nn < rep[1]; ++nn) {\n-      idxN.push_back(add(\n-          offsetCN, i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1])));\n-      idxN.push_back(\n-          add(offsetCN,\n-              i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1] + 1)));\n-    }\n-  }\n-\n-  SmallVector<SmallVector<Value>> axes({idxM, idxN});\n-\n-  // product the axis M and axis N to get coords, ported from\n-  // generator::init_idx method from triton2.0\n-\n-  // TODO[Superjomn]: check the order.\n-  SmallVector<CoordTy> coords;\n-  for (Value x1 : axes[1]) {   // N\n-    for (Value x0 : axes[0]) { // M\n-      SmallVector<Value, 2> idx(2);\n-      idx[0] = x0; // M\n-      idx[1] = x1; // N\n-      coords.push_back(std::move(idx));\n-    }\n-  }\n-\n-  return coords; // {M,N} in row-major\n-}\n-\n-std::tuple<int, int>\n-DotOpMmaV2ConversionHelper::getRepMN(const RankedTensorType &tensorTy) {\n-  auto mmaLayout = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n-  auto wpt = mmaLayout.getWarpsPerCTA();\n-\n-  int M = tensorTy.getShape()[0];\n-  int N = tensorTy.getShape()[1];\n-  auto [instrM, instrN] = getInstrShapeMN();\n-  int repM = std::max<int>(M / (wpt[0] * instrM), 1);\n-  int repN = std::max<int>(N / (wpt[1] * instrN), 1);\n-  return {repM, repN};\n-}\n-\n-Type DotOpMmaV2ConversionHelper::getShemPtrTy() const {\n-  switch (mmaType) {\n-  case TensorCoreType::FP32_FP16_FP16_FP32:\n-    return ptr_ty(type::f16Ty(ctx), 3);\n-  case TensorCoreType::FP32_BF16_BF16_FP32:\n-    return ptr_ty(type::i16Ty(ctx), 3);\n-  case TensorCoreType::FP32_TF32_TF32_FP32:\n-    return ptr_ty(type::f32Ty(ctx), 3);\n-  case TensorCoreType::INT32_INT8_INT8_INT32:\n-    return ptr_ty(type::i8Ty(ctx), 3);\n-  default:\n-    llvm::report_fatal_error(\"mma16816 data type not supported\");\n-  }\n-  return Type{};\n-}\n-\n-Type DotOpMmaV2ConversionHelper::getMatType() const {\n-  // floating point types\n-  Type fp32x1Ty = vec_ty(type::f32Ty(ctx), 1);\n-  Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n-  Type i16x2Ty = vec_ty(type::i16Ty(ctx), 2);\n-  Type fp16x2Pack4Ty =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n-  // LLVM 14.0 does not support bf16 type, so we use i16 instead.\n-  Type bf16x2Pack4Ty =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i16x2Ty));\n-  Type fp32Pack4Ty =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32x1Ty));\n-  // integer types\n-  Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n-  Type i8x4Pack4Ty =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i8x4Ty));\n-\n-  switch (mmaType) {\n-  case TensorCoreType::FP32_FP16_FP16_FP32:\n-    return fp16x2Pack4Ty;\n-  case TensorCoreType::FP32_BF16_BF16_FP32:\n-    return bf16x2Pack4Ty;\n-  case TensorCoreType::FP32_TF32_TF32_FP32:\n-    return fp32Pack4Ty;\n-  case TensorCoreType::INT32_INT8_INT8_INT32:\n-    return i8x4Pack4Ty;\n-  default:\n-    llvm::report_fatal_error(\"Unsupported mma type found\");\n-  }\n-\n-  return Type{};\n-}\n-\n-Type DotOpMmaV2ConversionHelper::getLoadElemTy() {\n-  switch (mmaType) {\n-  case TensorCoreType::FP32_FP16_FP16_FP32:\n-    return vec_ty(type::f16Ty(ctx), 2);\n-  case TensorCoreType::FP32_BF16_BF16_FP32:\n-    return vec_ty(type::bf16Ty(ctx), 2);\n-  case TensorCoreType::FP32_TF32_TF32_FP32:\n-    return type::f32Ty(ctx);\n-  case TensorCoreType::INT32_INT8_INT8_INT32:\n-    return type::i32Ty(ctx);\n-  default:\n-    llvm::report_fatal_error(\"Unsupported mma type found\");\n-  }\n-\n-  return Type{};\n-}\n-\n-Type DotOpMmaV2ConversionHelper::getMmaRetType() const {\n-  Type fp32Ty = type::f32Ty(ctx);\n-  Type i32Ty = type::i32Ty(ctx);\n-  Type fp32x4Ty =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n-  Type i32x4Ty =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i32Ty));\n-  switch (mmaType) {\n-  case TensorCoreType::FP32_FP16_FP16_FP32:\n-    return fp32x4Ty;\n-  case TensorCoreType::FP32_BF16_BF16_FP32:\n-    return fp32x4Ty;\n-  case TensorCoreType::FP32_TF32_TF32_FP32:\n-    return fp32x4Ty;\n-  case TensorCoreType::INT32_INT8_INT8_INT32:\n-    return i32x4Ty;\n-  default:\n-    llvm::report_fatal_error(\"Unsupported mma type found\");\n-  }\n-\n-  return Type{};\n-}\n-\n-DotOpMmaV2ConversionHelper::TensorCoreType\n-DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(Type operandTy) {\n-  auto tensorTy = operandTy.cast<RankedTensorType>();\n-  auto elemTy = tensorTy.getElementType();\n-  if (elemTy.isF16())\n-    return TensorCoreType::FP32_FP16_FP16_FP32;\n-  if (elemTy.isF32())\n-    return TensorCoreType::FP32_TF32_TF32_FP32;\n-  if (elemTy.isBF16())\n-    return TensorCoreType::FP32_BF16_BF16_FP32;\n-  if (elemTy.isInteger(8))\n-    return TensorCoreType::INT32_INT8_INT8_INT32;\n-  return TensorCoreType::NOT_APPLICABLE;\n-}\n-\n-DotOpMmaV2ConversionHelper::TensorCoreType\n-DotOpMmaV2ConversionHelper::getMmaType(triton::DotOp op) {\n-  Value A = op.getA();\n-  Value B = op.getB();\n-  auto aTy = A.getType().cast<RankedTensorType>();\n-  auto bTy = B.getType().cast<RankedTensorType>();\n-  // d = a*b + c\n-  auto dTy = op.getD().getType().cast<RankedTensorType>();\n-\n-  if (dTy.getElementType().isF32()) {\n-    if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n-      return TensorCoreType::FP32_FP16_FP16_FP32;\n-    if (aTy.getElementType().isBF16() && bTy.getElementType().isBF16())\n-      return TensorCoreType::FP32_BF16_BF16_FP32;\n-    if (aTy.getElementType().isF32() && bTy.getElementType().isF32() &&\n-        op.getAllowTF32())\n-      return TensorCoreType::FP32_TF32_TF32_FP32;\n-  } else if (dTy.getElementType().isInteger(32)) {\n-    if (aTy.getElementType().isInteger(8) && bTy.getElementType().isInteger(8))\n-      return TensorCoreType::INT32_INT8_INT8_INT32;\n-  }\n-\n-  return TensorCoreType::NOT_APPLICABLE;\n-}\n-\n-SmallVector<Value>\n-MMA16816SmemLoader::computeLdmatrixMatOffs(Value warpId, Value lane,\n-                                           Value cSwizzleOffset) {\n-  // 4x4 matrices\n-  Value c = urem(lane, i32_val(8));\n-  Value s = udiv(lane, i32_val(8)); // sub-warp-id\n-\n-  // Decompose s => s_0, s_1, that is the coordinate in 2x2 matrices in a\n-  // warp\n-  Value s0 = urem(s, i32_val(2));\n-  Value s1 = udiv(s, i32_val(2));\n-\n-  // We use different orders for a and b for better performance.\n-  Value kMatArr = kOrder == 1 ? s1 : s0;\n-  Value nkMatArr = kOrder == 1 ? s0 : s1;\n-\n-  // matrix coordinate inside a CTA, the matrix layout is [2x2wpt] for A and\n-  // [2wptx2] for B. e.g. Setting wpt=3, The data layout for A(kOrder=1) is\n-  //   |0 0 1 1 2 2| -> 0,1,2 are the warpids\n-  //   |0 0 1 1 2 2|\n-  //\n-  // for B(kOrder=0) is\n-  //   |0 0|  -> 0,1,2 are the warpids\n-  //   |1 1|\n-  //   |2 2|\n-  //   |0 0|\n-  //   |1 1|\n-  //   |2 2|\n-  // Note, for each warp, it handles a 2x2 matrices, that is the coordinate\n-  // address (s0,s1) annotates.\n-\n-  Value matOff[2];\n-  matOff[kOrder ^ 1] =\n-      add(mul(warpId, i32_val(warpOffStride)),   // warp offset\n-          mul(nkMatArr, i32_val(matArrStride))); // matrix offset inside a warp\n-  matOff[kOrder] = kMatArr;\n-\n-  // Physical offset (before swizzling)\n-  Value cMatOff = matOff[order[0]];\n-  Value sMatOff = matOff[order[1]];\n-  Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n-  cMatOff = add(cMatOff, cSwizzleMatOff);\n-\n-  // row offset inside a matrix, each matrix has 8 rows.\n-  Value sOffInMat = c;\n-\n-  SmallVector<Value> offs(numPtrs);\n-  Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n-  Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n-  for (int i = 0; i < numPtrs; ++i) {\n-    Value cMatOffI = add(cMatOff, i32_val(i * pLoadStrideInMat));\n-    cMatOffI = xor_(cMatOffI, phase);\n-    offs[i] = add(mul(cMatOffI, i32_val(cMatShape)), mul(sOff, sStride));\n-  }\n-\n-  return offs;\n-}\n-\n-SmallVector<Value> MMA16816SmemLoader::computeB32MatOffs(Value warpOff,\n-                                                         Value lane,\n-                                                         Value cSwizzleOffset) {\n-  assert(needTrans && \"Only used in transpose mode.\");\n-  // Load tf32 matrices with lds32\n-  Value cOffInMat = udiv(lane, i32_val(4));\n-  Value sOffInMat = urem(lane, i32_val(4));\n-\n-  Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n-  SmallVector<Value> offs(numPtrs);\n-\n-  for (int mat = 0; mat < 4; ++mat) { // Load 4 mats each time\n-    int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n-    int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n-    if (kMatArrInt > 0) // we don't need pointers for k\n-      continue;\n-    Value kMatArr = i32_val(kMatArrInt);\n-    Value nkMatArr = i32_val(nkMatArrInt);\n-\n-    Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n-                        mul(nkMatArr, i32_val(matArrStride)));\n-    Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n-    cMatOff = add(cMatOff, cSwizzleMatOff);\n-\n-    Value sMatOff = kMatArr;\n-    Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n-    // FIXME: (kOrder == 1?) is really dirty hack\n-    for (int i = 0; i < numPtrs / 2; ++i) {\n-      Value cMatOffI =\n-          add(cMatOff, i32_val(i * pLoadStrideInMat * (kOrder == 1 ? 1 : 2)));\n-      cMatOffI = xor_(cMatOffI, phase);\n-      Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n-      cOff = urem(cOff, i32_val(tileShape[order[0]]));\n-      sOff = urem(sOff, i32_val(tileShape[order[1]]));\n-      offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, sStride));\n-    }\n-  }\n-  return offs;\n-}\n-\n-SmallVector<Value> MMA16816SmemLoader::computeB8MatOffs(Value warpOff,\n-                                                        Value lane,\n-                                                        Value cSwizzleOffset) {\n-  assert(needTrans && \"Only used in transpose mode.\");\n-  Value cOffInMat = udiv(lane, i32_val(4));\n-  Value sOffInMat =\n-      mul(urem(lane, i32_val(4)), i32_val(4)); // each thread load 4 cols\n-\n-  SmallVector<Value> offs(numPtrs);\n-  for (int mat = 0; mat < 4; ++mat) {\n-    int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n-    int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n-    if (kMatArrInt > 0) // we don't need pointers for k\n-      continue;\n-    Value kMatArr = i32_val(kMatArrInt);\n-    Value nkMatArr = i32_val(nkMatArrInt);\n-\n-    Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n-                        mul(nkMatArr, i32_val(matArrStride)));\n-    Value sMatOff = kMatArr;\n-\n-    for (int loadx4Off = 0; loadx4Off < numPtrs / 8; ++loadx4Off) {\n-      for (int elemOff = 0; elemOff < 4; ++elemOff) {\n-        int ptrOff = loadx4Off * 8 + nkMatArrInt * 4 + elemOff;\n-        Value cMatOffI = add(cMatOff, i32_val(loadx4Off * pLoadStrideInMat *\n-                                              (kOrder == 1 ? 1 : 2)));\n-        Value sOffInMatElem = add(sOffInMat, i32_val(elemOff));\n-\n-        // disable swizzling ...\n-\n-        Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n-        Value sOff = add(sOffInMatElem, mul(sMatOff, i32_val(sMatShape)));\n-        // To prevent out-of-bound access when tile is too small.\n-        cOff = urem(cOff, i32_val(tileShape[order[0]]));\n-        sOff = urem(sOff, i32_val(tileShape[order[1]]));\n-        offs[ptrOff] = add(cOff, mul(sOff, sStride));\n-      }\n-    }\n-  }\n-  return offs;\n-}\n-\n-std::tuple<Value, Value, Value, Value>\n-MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> offs,\n-                           ArrayRef<Value> ptrs, Type matTy,\n-                           Type shemPtrTy) const {\n-  assert(mat0 % 2 == 0 && mat1 % 2 == 0 && \"smem matrix load must be aligned\");\n-  int matIdx[2] = {mat0, mat1};\n-\n-  int ptrIdx{-1};\n-\n-  if (canUseLdmatrix)\n-    ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n-  else if (elemBytes == 4 && needTrans)\n-    ptrIdx = matIdx[order[0]];\n-  else if (elemBytes == 1 && needTrans)\n-    ptrIdx = matIdx[order[0]] * 4;\n-  else\n-    llvm::report_fatal_error(\"unsupported mma type found\");\n-\n-  // The main difference with the original triton code is we removed the\n-  // prefetch-related logic here for the upstream optimizer phase should\n-  // take care with it, and that is transparent in dot conversion.\n-  auto getPtr = [&](int idx) { return ptrs[idx]; };\n-\n-  Value ptr = getPtr(ptrIdx);\n-\n-  // The struct should have exactly the same element types.\n-  auto resTy = matTy.cast<LLVM::LLVMStructType>();\n-  Type elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n-\n-  // For some reasons, LLVM's NVPTX backend inserts unnecessary (?) integer\n-  // instructions to pack & unpack sub-word integers. A workaround is to\n-  // store the results of ldmatrix in i32\n-  if (auto vecElemTy = elemTy.dyn_cast<VectorType>()) {\n-    Type elemElemTy = vecElemTy.getElementType();\n-    if (auto intTy = elemElemTy.dyn_cast<IntegerType>()) {\n-      if (intTy.getWidth() <= 16) {\n-        elemTy = rewriter.getI32Type();\n-        resTy =\n-            LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, elemTy));\n-      }\n-    }\n-  }\n-\n-  if (canUseLdmatrix) {\n-    Value sOffset =\n-        mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sStride);\n-    Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n-\n-    PTXBuilder builder;\n-    // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n-    // thread.\n-    auto resArgs = builder.newListOperand(4, \"=r\");\n-    auto addrArg = builder.newAddrOperand(sOffsetPtr, \"r\");\n-\n-    auto ldmatrix = builder.create(\"ldmatrix.sync.aligned.m8n8.x4\")\n-                        ->o(\"trans\", needTrans /*predicate*/)\n-                        .o(\"shared.b16\");\n-    ldmatrix(resArgs, addrArg);\n-\n-    // The result type is 4xi32, each i32 is composed of 2xf16\n-    // elements (adjacent two columns in a row) or a single f32 element.\n-    Value resV4 = builder.launch(rewriter, loc, resTy);\n-    return {extract_val(elemTy, resV4, 0), extract_val(elemTy, resV4, 1),\n-            extract_val(elemTy, resV4, 2), extract_val(elemTy, resV4, 3)};\n-  } else if (elemBytes == 4 && needTrans) { // Use lds.32 to load tf32 matrices\n-    Value ptr2 = getPtr(ptrIdx + 1);\n-    assert(sMatStride == 1);\n-    int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n-    Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n-    int sOffsetArrElem = sMatStride * sMatShape;\n-    Value sOffsetArrElemVal =\n-        add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n-\n-    Value elems[4];\n-    if (kOrder == 1) {\n-      elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n-      elems[1] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n-      elems[2] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n-      elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n-    } else {\n-      elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n-      elems[2] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n-      elems[1] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n-      elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n-    }\n-    std::array<Value, 4> retElems;\n-    retElems.fill(undef(elemTy));\n-    for (auto i = 0; i < 4; ++i) {\n-      retElems[i] = insert_element(elemTy, retElems[i], elems[i], i32_val(0));\n-    }\n-    return {retElems[0], retElems[1], retElems[2], retElems[3]};\n-  } else if (elemBytes == 1 && needTrans) { // work with int8\n-    // Can't use i32 here. Use LLVM's VectorType\n-    elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n-    std::array<std::array<Value, 4>, 2> ptrs;\n-    ptrs[0] = {\n-        getPtr(ptrIdx),\n-        getPtr(ptrIdx + 1),\n-        getPtr(ptrIdx + 2),\n-        getPtr(ptrIdx + 3),\n-    };\n-\n-    ptrs[1] = {\n-        getPtr(ptrIdx + 4),\n-        getPtr(ptrIdx + 5),\n-        getPtr(ptrIdx + 6),\n-        getPtr(ptrIdx + 7),\n-    };\n-\n-    assert(sMatStride == 1);\n-    int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n-    Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n-    int sOffsetArrElem = 1 * (sMatStride * sMatShape);\n-    Value sOffsetArrElemVal =\n-        add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n-\n-    std::array<Value, 4> i8v4Elems;\n-    i8v4Elems.fill(undef(elemTy));\n-\n-    Value i8Elems[4][4];\n-    if (kOrder == 1) {\n-      for (int i = 0; i < 2; ++i)\n-        for (int j = 0; j < 4; ++j)\n-          i8Elems[i][j] = load(gep(shemPtrTy, ptrs[i][j], sOffsetElemVal));\n-\n-      for (int i = 2; i < 4; ++i)\n-        for (int j = 0; j < 4; ++j)\n-          i8Elems[i][j] =\n-              load(gep(shemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n-\n-      for (int m = 0; m < 4; ++m) {\n-        for (int e = 0; e < 4; ++e)\n-          i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n-                                        i8Elems[m][e], i32_val(e));\n-      }\n-    } else { // k first\n-      for (int j = 0; j < 4; ++j)\n-        i8Elems[0][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetElemVal));\n-      for (int j = 0; j < 4; ++j)\n-        i8Elems[2][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetElemVal));\n-      for (int j = 0; j < 4; ++j)\n-        i8Elems[1][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n-      for (int j = 0; j < 4; ++j)\n-        i8Elems[3][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n-\n-      for (int m = 0; m < 4; ++m) {\n-        for (int e = 0; e < 4; ++e)\n-          i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n-                                        i8Elems[m][e], i32_val(e));\n-      }\n-    }\n-\n-    return {bitcast(i8v4Elems[0], i32_ty), bitcast(i8v4Elems[1], i32_ty),\n-            bitcast(i8v4Elems[2], i32_ty), bitcast(i8v4Elems[3], i32_ty)};\n-  }\n-\n-  assert(false && \"Invalid smem load\");\n-  return {Value{}, Value{}, Value{}, Value{}};\n-}\n-\n-MMA16816SmemLoader::MMA16816SmemLoader(\n-    int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n-    ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n-    ArrayRef<int> instrShape, ArrayRef<int> matShape, int perPhase,\n-    int maxPhase, int elemBytes, ConversionPatternRewriter &rewriter,\n-    TypeConverter *typeConverter, const Location &loc)\n-    : order(order.begin(), order.end()), kOrder(kOrder),\n-      tileShape(tileShape.begin(), tileShape.end()),\n-      instrShape(instrShape.begin(), instrShape.end()),\n-      matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n-      maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter), loc(loc),\n-      ctx(rewriter.getContext()) {\n-  cMatShape = matShape[order[0]];\n-  sMatShape = matShape[order[1]];\n-\n-  sStride = smemStrides[order[1]];\n-\n-  // rule: k must be the fast-changing axis.\n-  needTrans = kOrder != order[0];\n-  canUseLdmatrix = elemBytes == 2 || (!needTrans); // b16\n-\n-  if (canUseLdmatrix) {\n-    // Each CTA, the warps is arranged as [1xwpt] if not transposed,\n-    // otherwise [wptx1], and each warp will perform a mma.\n-    numPtrs =\n-        tileShape[order[0]] / (needTrans ? wpt : 1) / instrShape[order[0]];\n-  } else {\n-    numPtrs = tileShape[order[0]] / wpt / matShape[order[0]];\n-  }\n-  numPtrs = std::max<int>(numPtrs, 2);\n-\n-  // Special rule for i8/u8, 4 ptrs for each matrix\n-  if (!canUseLdmatrix && elemBytes == 1)\n-    numPtrs *= 4;\n-\n-  int loadStrideInMat[2];\n-  loadStrideInMat[kOrder] =\n-      2; // instrShape[kOrder] / matShape[kOrder], always 2\n-  loadStrideInMat[kOrder ^ 1] =\n-      wpt * (instrShape[kOrder ^ 1] / matShape[kOrder ^ 1]);\n-\n-  pLoadStrideInMat = loadStrideInMat[order[0]];\n-  sMatStride =\n-      loadStrideInMat[order[1]] / (instrShape[order[1]] / matShape[order[1]]);\n-\n-  // Each matArr contains warpOffStride matrices.\n-  matArrStride = kOrder == 1 ? 1 : wpt;\n-  warpOffStride = instrShape[kOrder ^ 1] / matShape[kOrder ^ 1];\n-}\n-Value MMA16816ConversionHelper::loadA(Value tensor,\n-                                      const SharedMemoryObject &smemObj) const {\n-  auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n-\n-  SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n-                             aTensorTy.getShape().end());\n-\n-  ValueTable ha;\n-  std::function<void(int, int)> loadFn;\n-  auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n-  auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(aTensorTy);\n-\n-  int numRepM = getNumRepM(aTensorTy, shape[0]);\n-  int numRepK = getNumRepK(aTensorTy, shape[1]);\n-\n-  if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n-    Value warpM = getWarpM(shape[0]);\n-    // load from smem\n-    // we use ldmatrix.x4 so each warp processes 16x16 elements.\n-    int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / 16);\n-    loadFn =\n-        getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/,\n-                        {mmaInstrM, mmaInstrK} /*instrShape*/,\n-                        {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/,\n-                        ha /*vals*/, true /*isA*/);\n-  } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n-    // load from registers, used in gemm fuse\n-    // TODO(Superjomn) Port the logic.\n-    assert(false && \"Loading A from register is not supported yet.\");\n-  } else {\n-    assert(false && \"A's layout is not supported.\");\n-  }\n-\n-  // step1. Perform loading.\n-  for (int m = 0; m < numRepM; ++m)\n-    for (int k = 0; k < numRepK; ++k)\n-      loadFn(2 * m, 2 * k);\n-\n-  // step2. Format the values to LLVM::Struct to passing to mma codegen.\n-  return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n-}\n-Value MMA16816ConversionHelper::loadB(Value tensor,\n-                                      const SharedMemoryObject &smemObj) {\n-  ValueTable hb;\n-  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-\n-  SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n-                             tensorTy.getShape().end());\n-\n-  // TODO[Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n-  bool transB = false;\n-  if (transB) {\n-    std::swap(shape[0], shape[1]);\n-  }\n-\n-  auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(tensorTy);\n-  auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(tensorTy);\n-  int numRepK = getNumRepK(tensorTy, shape[0]);\n-  int numRepN = getNumRepN(tensorTy, shape[1]);\n-\n-  Value warpN = getWarpN(shape[1]);\n-  // we use ldmatrix.x4 so each warp processes 16x16 elements.\n-  int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / 16);\n-  auto loadFn =\n-      getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/,\n-                      {mmaInstrK, mmaInstrN} /*instrShape*/,\n-                      {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/,\n-                      hb /*vals*/, false /*isA*/);\n-\n-  for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n-    for (int k = 0; k < numRepK; ++k)\n-      loadFn(2 * n, 2 * k);\n-  }\n-\n-  Value result = composeValuesToDotOperandLayoutStruct(\n-      hb, std::max(numRepN / 2, 1), numRepK);\n-  return result;\n-}\n-Value MMA16816ConversionHelper::loadC(Value tensor, Value llTensor) const {\n-  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-  auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(tensorTy);\n-  size_t fcSize = 4 * repM * repN;\n-\n-  assert(tensorTy.getEncoding().isa<MmaEncodingAttr>() &&\n-         \"Currently, we only support $c with a mma layout.\");\n-  // Load a normal C tensor with mma layout, that should be a\n-  // LLVM::struct with fcSize elements.\n-  auto structTy = llTensor.getType().cast<LLVM::LLVMStructType>();\n-  assert(structTy.getBody().size() == fcSize &&\n-         \"DotOp's $c operand should pass the same number of values as $d in \"\n-         \"mma layout.\");\n-  return llTensor;\n-}\n-LogicalResult MMA16816ConversionHelper::convertDot(Value a, Value b, Value c,\n-                                                   Value d, Value loadedA,\n-                                                   Value loadedB, Value loadedC,\n-                                                   DotOp op,\n-                                                   DotOpAdaptor adaptor) const {\n-  helper.deduceMmaType(op);\n-\n-  auto aTensorTy = a.getType().cast<RankedTensorType>();\n-  auto bTensorTy = b.getType().cast<RankedTensorType>();\n-  auto dTensorTy = d.getType().cast<RankedTensorType>();\n-\n-  SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n-                              aTensorTy.getShape().end());\n-\n-  auto dShape = dTensorTy.getShape();\n-\n-  // shape / shape_per_cta\n-  int numRepM = getNumRepM(aTensorTy, dShape[0]);\n-  int numRepN = getNumRepN(aTensorTy, dShape[1]);\n-  int numRepK = getNumRepK(aTensorTy, aShape[1]);\n-\n-  ValueTable ha =\n-      getValuesFromDotOperandLayoutStruct(loadedA, numRepM, numRepK, aTensorTy);\n-  ValueTable hb = getValuesFromDotOperandLayoutStruct(\n-      loadedB, std::max(numRepN / 2, 1), numRepK, bTensorTy);\n-  auto fc = typeConverter->unpackLLElements(loc, loadedC, rewriter, dTensorTy);\n-\n-  auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n-    unsigned colsPerThread = numRepN * 2;\n-    PTXBuilder builder;\n-    auto &mma = *builder.create(helper.getMmaInstr().str());\n-    // using =r for float32 works but leads to less readable ptx.\n-    bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n-    auto retArgs = builder.newListOperand(4, isIntMMA ? \"=r\" : \"=f\");\n-    auto aArgs = builder.newListOperand({\n-        {ha[{m, k}], \"r\"},\n-        {ha[{m + 1, k}], \"r\"},\n-        {ha[{m, k + 1}], \"r\"},\n-        {ha[{m + 1, k + 1}], \"r\"},\n-    });\n-    auto bArgs =\n-        builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n-    auto cArgs = builder.newListOperand();\n-    for (int i = 0; i < 4; ++i) {\n-      cArgs->listAppend(builder.newOperand(fc[m * colsPerThread + 4 * n + i],\n-                                           std::to_string(i)));\n-      // reuse the output registers\n-    }\n-\n-    mma(retArgs, aArgs, bArgs, cArgs);\n-    Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n-\n-    Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n-    for (int i = 0; i < 4; ++i)\n-      fc[m * colsPerThread + 4 * n + i] = extract_val(elemTy, mmaOut, i);\n-  };\n-\n-  for (int k = 0; k < numRepK; ++k)\n-    for (int m = 0; m < numRepM; ++m)\n-      for (int n = 0; n < numRepN; ++n)\n-        callMma(2 * m, n, 2 * k);\n-\n-  Type resElemTy = dTensorTy.getElementType();\n-\n-  for (auto &elem : fc) {\n-    elem = bitcast(elem, resElemTy);\n-  }\n-\n-  // replace with new packed result\n-  Type structTy = LLVM::LLVMStructType::getLiteral(\n-      ctx, SmallVector<Type>(fc.size(), resElemTy));\n-  Value res = typeConverter->packLLElements(loc, fc, rewriter, structTy);\n-  rewriter.replaceOp(op, res);\n-\n-  return success();\n-}\n-std::function<void(int, int)> MMA16816ConversionHelper::getLoadMatrixFn(\n-    Value tensor, const SharedMemoryObject &smemObj, MmaEncodingAttr mmaLayout,\n-    int wpt, uint32_t kOrder, SmallVector<int> instrShape,\n-    SmallVector<int> matShape, Value warpId,\n-    MMA16816ConversionHelper::ValueTable &vals, bool isA) const {\n-  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-  // We assumes that the input operand of Dot should be from shared layout.\n-  // TODO(Superjomn) Consider other layouts if needed later.\n-  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  const int perPhase = sharedLayout.getPerPhase();\n-  const int maxPhase = sharedLayout.getMaxPhase();\n-  const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n-  auto order = sharedLayout.getOrder();\n-\n-  // the original register_lds2, but discard the prefetch logic.\n-  auto ld2 = [](ValueTable &vals, int mn, int k, Value val) {\n-    vals[{mn, k}] = val;\n-  };\n-\n-  // (a, b) is the coordinate.\n-  auto load = [=, &vals, &ld2](int a, int b) {\n-    MMA16816SmemLoader loader(\n-        wpt, sharedLayout.getOrder(), kOrder, smemObj.strides,\n-        tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n-        maxPhase, elemBytes, rewriter, typeConverter, loc);\n-    Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n-    SmallVector<Value> offs =\n-        loader.computeOffsets(warpId, lane, cSwizzleOffset);\n-    const int numPtrs = loader.getNumPtrs();\n-    SmallVector<Value> ptrs(numPtrs);\n-\n-    Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-\n-    Type smemPtrTy = helper.getShemPtrTy();\n-    for (int i = 0; i < numPtrs; ++i) {\n-      ptrs[i] =\n-          bitcast(gep(smemPtrTy, smemBase, ValueRange({offs[i]})), smemPtrTy);\n-    }\n-\n-    auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n-        (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n-        ptrs, helper.getMatType(), helper.getShemPtrTy());\n-\n-    if (isA) {\n-      ld2(vals, a, b, ha0);\n-      ld2(vals, a + 1, b, ha1);\n-      ld2(vals, a, b + 1, ha2);\n-      ld2(vals, a + 1, b + 1, ha3);\n-    } else {\n-      ld2(vals, a, b, ha0);\n-      ld2(vals, a + 1, b, ha2);\n-      ld2(vals, a, b + 1, ha1);\n-      ld2(vals, a + 1, b + 1, ha3);\n-    }\n-  };\n-\n-  return load;\n-}\n-Value MMA16816ConversionHelper::composeValuesToDotOperandLayoutStruct(\n-    const MMA16816ConversionHelper::ValueTable &vals, int n0, int n1) const {\n-  std::vector<Value> elems;\n-  for (int m = 0; m < n0; ++m)\n-    for (int k = 0; k < n1; ++k) {\n-      elems.push_back(vals.at({2 * m, 2 * k}));\n-      elems.push_back(vals.at({2 * m, 2 * k + 1}));\n-      elems.push_back(vals.at({2 * m + 1, 2 * k}));\n-      elems.push_back(vals.at({2 * m + 1, 2 * k + 1}));\n-    }\n-\n-  assert(!elems.empty());\n-\n-  Type elemTy = elems[0].getType();\n-  Type structTy = LLVM::LLVMStructType::getLiteral(\n-      ctx, SmallVector<Type>(elems.size(), elemTy));\n-  auto result = typeConverter->packLLElements(loc, elems, rewriter, structTy);\n-  return result;\n-}\n-MMA16816ConversionHelper::ValueTable\n-MMA16816ConversionHelper::getValuesFromDotOperandLayoutStruct(Value value,\n-                                                              int n0, int n1,\n-                                                              Type type) const {\n-  auto elems = typeConverter->unpackLLElements(loc, value, rewriter, type);\n-\n-  int offset{};\n-  ValueTable vals;\n-  for (int i = 0; i < n0; ++i) {\n-    for (int j = 0; j < n1; j++) {\n-      vals[{2 * i, 2 * j}] = elems[offset++];\n-      vals[{2 * i, 2 * j + 1}] = elems[offset++];\n-      vals[{2 * i + 1, 2 * j}] = elems[offset++];\n-      vals[{2 * i + 1, 2 * j + 1}] = elems[offset++];\n-    }\n-  }\n-  return vals;\n-}\n-SmallVector<Value> DotOpFMAConversionHelper::getThreadIds(\n-    Value threadId, ArrayRef<unsigned int> shapePerCTA,\n-    ArrayRef<unsigned int> sizePerThread, ArrayRef<unsigned int> order,\n-    ConversionPatternRewriter &rewriter, Location loc) const {\n-  int dim = order.size();\n-  SmallVector<Value> threadIds(dim);\n-  for (unsigned k = 0; k < dim - 1; k++) {\n-    Value dimK = i32_val(shapePerCTA[order[k]] / sizePerThread[order[k]]);\n-    Value rem = urem(threadId, dimK);\n-    threadId = udiv(threadId, dimK);\n-    threadIds[order[k]] = rem;\n-  }\n-  Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n-  threadIds[order[dim - 1]] = urem(threadId, dimK);\n-  return threadIds;\n-}\n-Value DotOpFMAConversionHelper::loadA(\n-    Value A, Value llA, BlockedEncodingAttr dLayout, Value thread, Location loc,\n-    TritonGPUToLLVMTypeConverter *typeConverter,\n-    ConversionPatternRewriter &rewriter) const {\n-  auto aTensorTy = A.getType().cast<RankedTensorType>();\n-  auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto aShape = aTensorTy.getShape();\n-\n-  auto aOrder = aLayout.getOrder();\n-  auto order = dLayout.getOrder();\n-\n-  bool isARow = aOrder[0] == 1;\n-\n-  auto aSmem = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n-  Value strideAM = aSmem.strides[0];\n-  Value strideAK = aSmem.strides[1];\n-  Value strideA0 = isARow ? strideAK : strideAM;\n-  Value strideA1 = isARow ? strideAM : strideAK;\n-  int aNumPtr = 8;\n-  int K = aShape[1];\n-  int M = aShape[0];\n-\n-  auto shapePerCTA = getShapePerCTA(dLayout);\n-  auto sizePerThread = getSizePerThread(dLayout);\n-\n-  Value _0 = i32_val(0);\n-\n-  Value mContig = i32_val(sizePerThread[order[1]]);\n-\n-  // threadId in blocked layout\n-  auto threadIds =\n-      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n-  Value threadIdM = threadIds[0];\n-\n-  Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n-  Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n-  SmallVector<Value> aOff(aNumPtr);\n-  for (int i = 0; i < aNumPtr; ++i) {\n-    aOff[i] = add(mul(offA0, strideA0), mul(offA1, strideA1));\n-  }\n-  auto elemTy = A.getType().cast<RankedTensorType>().getElementType();\n-\n-  Type ptrTy = ptr_ty(elemTy, 3);\n-  SmallVector<Value> aPtrs(aNumPtr);\n-  for (int i = 0; i < aNumPtr; ++i)\n-    aPtrs[i] = gep(ptrTy, aSmem.base, aOff[i]);\n-\n-  SmallVector<Value> vas;\n-\n-  int mShapePerCTA = getShapePerCTAForMN(dLayout, true /*isM*/);\n-  int mSizePerThread = getSizePerThreadForMN(dLayout, true /*isM*/);\n-\n-  for (unsigned k = 0; k < K; ++k)\n-    for (unsigned m = 0; m < M; m += mShapePerCTA)\n-      for (unsigned mm = 0; mm < mSizePerThread; ++mm) {\n-        Value offset =\n-            add(mul(i32_val(m + mm), strideAM), mul(i32_val(k), strideAK));\n-        Value pa = gep(ptrTy, aPtrs[0], offset);\n-        Value va = load(pa);\n-        vas.emplace_back(va);\n-      }\n-\n-  return getStructFromValueTable(vas, rewriter, loc, typeConverter, elemTy);\n-}\n-Value DotOpFMAConversionHelper::loadB(\n-    Value B, Value llB, BlockedEncodingAttr dLayout, Value thread, Location loc,\n-    TritonGPUToLLVMTypeConverter *typeConverter,\n-    ConversionPatternRewriter &rewriter) const {\n-  auto bTensorTy = B.getType().cast<RankedTensorType>();\n-  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto bShape = bTensorTy.getShape();\n-\n-  auto bOrder = bLayout.getOrder();\n-  auto order = dLayout.getOrder();\n-\n-  bool isBRow = bOrder[0] == 1;\n-\n-  auto bSmem = getSharedMemoryObjectFromStruct(loc, llB, rewriter);\n-  Value strideBN = bSmem.strides[1];\n-  Value strideBK = bSmem.strides[0];\n-  Value strideB0 = isBRow ? strideBN : strideBK;\n-  Value strideB1 = isBRow ? strideBK : strideBN;\n-  int bNumPtr = 8;\n-  int K = bShape[0];\n-  int N = bShape[1];\n-\n-  auto shapePerCTA = getShapePerCTA(dLayout);\n-  auto sizePerThread = getSizePerThread(dLayout);\n-\n-  Value _0 = i32_val(0);\n-\n-  Value nContig = i32_val(sizePerThread[order[0]]);\n-\n-  // threadId in blocked layout\n-  auto threadIds =\n-      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n-  Value threadIdN = threadIds[1];\n-\n-  Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n-  Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n-  SmallVector<Value> bOff(bNumPtr);\n-  for (int i = 0; i < bNumPtr; ++i) {\n-    bOff[i] = add(mul(offB0, strideB0), mul(offB1, strideB1));\n-  }\n-  auto elemTy = B.getType().cast<RankedTensorType>().getElementType();\n-\n-  Type ptrTy = ptr_ty(elemTy, 3);\n-  SmallVector<Value> bPtrs(bNumPtr);\n-  for (int i = 0; i < bNumPtr; ++i)\n-    bPtrs[i] = gep(ptrTy, bSmem.base, bOff[i]);\n-\n-  SmallVector<Value> vbs;\n-\n-  int nShapePerCTA = getShapePerCTAForMN(dLayout, false /*isM*/);\n-  int nSizePerThread = getSizePerThreadForMN(dLayout, false /*isM*/);\n-\n-  for (unsigned k = 0; k < K; ++k)\n-    for (unsigned n = 0; n < N; n += nShapePerCTA)\n-      for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n-        Value offset =\n-            add(mul(i32_val(n + nn), strideBN), mul(i32_val(k), strideBK));\n-        Value pb = gep(ptrTy, bPtrs[0], offset);\n-        Value vb = load(pb);\n-        vbs.emplace_back(vb);\n-      }\n-\n-  return getStructFromValueTable(vbs, rewriter, loc, typeConverter, elemTy);\n-}\n-DotOpFMAConversionHelper::ValueTable\n-DotOpFMAConversionHelper::getValueTableFromStruct(\n-    Value val, int K, int n0, int shapePerCTA, int sizePerThread,\n-    ConversionPatternRewriter &rewriter, Location loc,\n-    TritonGPUToLLVMTypeConverter *typeConverter, Type type) const {\n-  ValueTable res;\n-  auto elems = typeConverter->unpackLLElements(loc, val, rewriter, type);\n-  int index = 0;\n-  for (unsigned k = 0; k < K; ++k) {\n-    for (unsigned m = 0; m < n0; m += shapePerCTA)\n-      for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n-        res[{m + mm, k}] = elems[index++];\n-      }\n-  }\n-  return res;\n-}\n-Value DotOpFMAConversionHelper::getStructFromValueTable(\n-    ArrayRef<Value> vals, ConversionPatternRewriter &rewriter, Location loc,\n-    TritonGPUToLLVMTypeConverter *typeConverter, Type elemTy) const {\n-  SmallVector<Type> elemTypes(vals.size(), elemTy);\n-  SmallVector<Value> elems;\n-  elems.reserve(vals.size());\n-  for (auto &val : vals) {\n-    elems.push_back(val);\n-  }\n-\n-  Type structTy = struct_ty(elemTypes);\n-  return typeConverter->packLLElements(loc, elems, rewriter, structTy);\n-}\n-int DotOpFMAConversionHelper::getNumElemsPerThread(\n-    ArrayRef<int64_t> shape, DotOperandEncodingAttr dotOpLayout) {\n-  auto blockedLayout = dotOpLayout.getParent().cast<BlockedEncodingAttr>();\n-  auto shapePerCTA = getShapePerCTA(blockedLayout);\n-  auto sizePerThread = getSizePerThread(blockedLayout);\n-\n-  // TODO[Superjomn]: we assume the k axis is fixed for $a and $b here, fix it\n-  // if not.\n-  int K = dotOpLayout.getOpIdx() == 0 ? shape[1] : shape[0];\n-  int otherDim = dotOpLayout.getOpIdx() == 1 ? shape[1] : shape[0];\n-\n-  bool isM = dotOpLayout.getOpIdx() == 0;\n-  int shapePerCTAMN = getShapePerCTAForMN(blockedLayout, isM);\n-  int sizePerThreadMN = getSizePerThreadForMN(blockedLayout, isM);\n-  return K * std::max<int>(otherDim / shapePerCTAMN, 1) * sizePerThreadMN;\n-}\n-} // namespace LLVM\n-} // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "removed", "additions": 0, "deletions": 569, "changes": 569, "file_content_changes": "@@ -1,569 +0,0 @@\n-#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_DOT_OP_HELPERS_H\n-#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_DOT_OP_HELPERS_H\n-\n-#include \"mlir/Analysis/SliceAnalysis.h\"\n-#include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n-#include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n-#include \"mlir/Conversion/LLVMCommon/LoweringOptions.h\"\n-#include \"mlir/Conversion/LLVMCommon/Pattern.h\"\n-#include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n-#include \"mlir/Dialect/Arith/IR/Arith.h\"\n-#include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n-#include \"mlir/IR/Matchers.h\"\n-#include \"mlir/IR/TypeUtilities.h\"\n-#include \"mlir/Transforms/DialectConversion.h\"\n-#include \"triton/Analysis/AxisInfo.h\"\n-#include \"triton/Analysis/Utility.h\"\n-#include \"triton/Conversion/MLIRTypes.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n-#include \"triton/Dialect/Triton/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-#include \"llvm/Support/Format.h\"\n-#include \"llvm/Support/FormatVariadic.h\"\n-\n-#include \"Utility.h\"\n-\n-class TritonGPUToLLVMTypeConverter;\n-\n-namespace mlir {\n-namespace LLVM {\n-using namespace mlir::triton;\n-using ::mlir::triton::gpu::BlockedEncodingAttr;\n-using ::mlir::triton::gpu::DotOperandEncodingAttr;\n-using ::mlir::triton::gpu::MmaEncodingAttr;\n-using ::mlir::triton::gpu::SharedEncodingAttr;\n-\n-// Helper for conversion of DotOp with mma<version=1>, that is sm<80\n-struct DotOpMmaV1ConversionHelper {\n-  MmaEncodingAttr mmaLayout;\n-  ArrayRef<unsigned> wpt;\n-  static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n-\n-  using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n-\n-  explicit DotOpMmaV1ConversionHelper(MmaEncodingAttr mmaLayout)\n-      : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()) {}\n-\n-  static ArrayRef<unsigned> getMmaInstrShape() { return instrShape; }\n-\n-  static Type getMmaRetType(TensorType operand) {\n-    auto *ctx = operand.getContext();\n-    Type fp32Ty = type::f32Ty(ctx);\n-    // f16*f16+f32->f32\n-    return struct_ty(SmallVector<Type>{8, fp32Ty});\n-  }\n-\n-  static Type getMatType(TensorType operand) {\n-    auto *ctx = operand.getContext();\n-    Type fp16Ty = type::f16Ty(ctx);\n-    Type vecTy = vec_ty(fp16Ty, 2);\n-    return struct_ty(SmallVector<Type>{vecTy});\n-  }\n-\n-  // Loading $a from smem to registers, returns a LLVM::Struct.\n-  Value loadA(Value tensor, const SharedMemoryObject &smemObj, Value thread,\n-              Location loc, TritonGPUToLLVMTypeConverter *converter,\n-              ConversionPatternRewriter &rewriter, Type resultTy) const;\n-\n-  // Loading $b from smem to registers, returns a LLVM::Struct.\n-  Value loadB(Value tensor, const SharedMemoryObject &smemObj, Value thread,\n-              Location loc, TritonGPUToLLVMTypeConverter *converter,\n-              ConversionPatternRewriter &rewriter, Type resultTy) const;\n-\n-  static ArrayRef<unsigned> getOrder() { return mmaOrder; }\n-\n-  // Compute the offset of the matrix to load.\n-  // Returns offsetAM, offsetAK, offsetBN, offsetBK.\n-  // NOTE, the information M(from $a) and N(from $b) couldn't be retrieved at\n-  // the same time in the usage in convert_layout[shared->dot_op], we leave\n-  // the noexist info to be 0 and only use the desired argument from the\n-  // composed result. In this way we want to retain the original code\n-  // structure in convert_mma884 method for easier debugging.\n-  std::tuple<Value, Value, Value, Value>\n-  computeOffsets(Value threadId, bool isARow, bool isBRow, ArrayRef<int> fpw,\n-                 ArrayRef<int> spw, ArrayRef<int> rep,\n-                 ConversionPatternRewriter &rewriter, Location loc) const;\n-\n-  // Extract values belong to $a or $b from a LLVMStruct, the shape is n0xn1.\n-  DotOpMmaV1ConversionHelper::ValueTable extractLoadedOperand(\n-      Value llStruct, int NK, ConversionPatternRewriter &rewriter,\n-      TritonGPUToLLVMTypeConverter *typeConverter, Type type) const;\n-\n-  using CoordTy = SmallVector<Value>;\n-  // Get the coordinates(m,n) of the elements emit by a thread in accumulator.\n-  static SmallVector<CoordTy>\n-  getMNCoords(Value thread, ConversionPatternRewriter &rewriter,\n-              ArrayRef<unsigned> wpt, const MmaEncodingAttr &mmaLayout,\n-              ArrayRef<int64_t> shape, bool isARow, bool isBRow, bool isAVec4,\n-              bool isBVec4);\n-\n-  // \\param elemId the offset of the element in a thread\n-  static CoordTy getCoord(int elemId, ArrayRef<CoordTy> coords) {\n-    return coords[elemId];\n-  }\n-\n-private:\n-  static constexpr unsigned instrShape[] = {16, 16, 4};\n-  static constexpr unsigned mmaOrder[] = {0, 1};\n-};\n-\n-// Helper for conversion of DotOp with mma<version=2>, that is sm>=80\n-struct DotOpMmaV2ConversionHelper {\n-  enum class TensorCoreType : uint8_t {\n-    // floating-point tensor core instr\n-    FP32_FP16_FP16_FP32 = 0, // default\n-    FP32_BF16_BF16_FP32,\n-    FP32_TF32_TF32_FP32,\n-    // integer tensor core instr\n-    INT32_INT1_INT1_INT32, // Not implemented\n-    INT32_INT4_INT4_INT32, // Not implemented\n-    INT32_INT8_INT8_INT32, // Not implemented\n-    //\n-    NOT_APPLICABLE,\n-  };\n-\n-  MmaEncodingAttr mmaLayout;\n-  MLIRContext *ctx{};\n-\n-  explicit DotOpMmaV2ConversionHelper(MmaEncodingAttr mmaLayout)\n-      : mmaLayout(mmaLayout) {\n-    ctx = mmaLayout.getContext();\n-  }\n-\n-  void deduceMmaType(DotOp op) const { mmaType = getMmaType(op); }\n-  void deduceMmaType(Type operandTy) const {\n-    mmaType = getTensorCoreTypeFromOperand(operandTy);\n-  }\n-\n-  // Get the M and N of mma instruction shape.\n-  static std::tuple<int, int> getInstrShapeMN() {\n-    // According to DotOpConversionHelper::mmaInstrShape, all the M,N are\n-    // {16,8}\n-    return {16, 8};\n-  }\n-\n-  static std::tuple<int, int> getRepMN(const RankedTensorType &tensorTy);\n-\n-  Type getShemPtrTy() const;\n-\n-  // The type of matrix that loaded by either a ldmatrix or composed lds.\n-  Type getMatType() const;\n-\n-  Type getLoadElemTy();\n-\n-  Type getMmaRetType() const;\n-\n-  ArrayRef<int> getMmaInstrShape() const {\n-    assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n-           \"Unknown mma type found.\");\n-    return mmaInstrShape.at(mmaType);\n-  }\n-\n-  static ArrayRef<int> getMmaInstrShape(TensorCoreType tensorCoreType) {\n-    assert(tensorCoreType != TensorCoreType::NOT_APPLICABLE &&\n-           \"Unknown mma type found.\");\n-    return mmaInstrShape.at(tensorCoreType);\n-  }\n-\n-  ArrayRef<int> getMmaMatShape() const {\n-    assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n-           \"Unknown mma type found.\");\n-    return mmaMatShape.at(mmaType);\n-  }\n-\n-  // Deduce the TensorCoreType from either $a or $b's type.\n-  static TensorCoreType getTensorCoreTypeFromOperand(Type operandTy);\n-\n-  int getVec() const {\n-    assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n-           \"Unknown mma type found.\");\n-    return mmaInstrVec.at(mmaType);\n-  }\n-\n-  StringRef getMmaInstr() const {\n-    assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n-           \"Unknown mma type found.\");\n-    return mmaInstrPtx.at(mmaType);\n-  }\n-\n-  static TensorCoreType getMmaType(triton::DotOp op);\n-\n-private:\n-  mutable TensorCoreType mmaType{TensorCoreType::NOT_APPLICABLE};\n-\n-  // Used on nvidia GPUs mma layout .version == 2\n-  // Refer to\n-  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-storage\n-  // for more details.\n-  inline static const std::map<TensorCoreType, llvm::SmallVector<int>>\n-      mmaInstrShape = {\n-          {TensorCoreType::FP32_FP16_FP16_FP32, {16, 8, 16}},\n-          {TensorCoreType::FP32_BF16_BF16_FP32, {16, 8, 16}},\n-          {TensorCoreType::FP32_TF32_TF32_FP32, {16, 8, 8}},\n-\n-          {TensorCoreType::INT32_INT1_INT1_INT32, {16, 8, 256}},\n-          {TensorCoreType::INT32_INT4_INT4_INT32, {16, 8, 64}},\n-          {TensorCoreType::INT32_INT8_INT8_INT32, {16, 8, 32}},\n-  };\n-\n-  // shape of matrices loaded by ldmatrix (m-n-k, for mxk & kxn matrices)\n-  // Refer to\n-  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix\n-  // for more details.\n-  inline static const std::map<TensorCoreType, llvm::SmallVector<int>>\n-      mmaMatShape = {\n-          {TensorCoreType::FP32_FP16_FP16_FP32, {8, 8, 8}},\n-          {TensorCoreType::FP32_BF16_BF16_FP32, {8, 8, 8}},\n-          {TensorCoreType::FP32_TF32_TF32_FP32, {8, 8, 4}},\n-\n-          {TensorCoreType::INT32_INT1_INT1_INT32, {8, 8, 64}},\n-          {TensorCoreType::INT32_INT4_INT4_INT32, {8, 8, 32}},\n-          {TensorCoreType::INT32_INT8_INT8_INT32, {8, 8, 16}},\n-  };\n-\n-  // Supported mma instruction in PTX.\n-  // Refer to\n-  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-for-mma\n-  // for more details.\n-  inline static const std::map<TensorCoreType, std::string> mmaInstrPtx = {\n-      {TensorCoreType::FP32_FP16_FP16_FP32,\n-       \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"},\n-      {TensorCoreType::FP32_BF16_BF16_FP32,\n-       \"mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\"},\n-      {TensorCoreType::FP32_TF32_TF32_FP32,\n-       \"mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\"},\n-\n-      {TensorCoreType::INT32_INT1_INT1_INT32,\n-       \"mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc\"},\n-      {TensorCoreType::INT32_INT4_INT4_INT32,\n-       \"mma.sync.aligned.m16n8k64.row.col.satfinite.s32.s4.s4.s32\"},\n-      {TensorCoreType::INT32_INT8_INT8_INT32,\n-       \"mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\"},\n-  };\n-\n-  // vector length per ldmatrix (16*8/element_size_in_bits)\n-  inline static const std::map<TensorCoreType, uint8_t> mmaInstrVec = {\n-      {TensorCoreType::FP32_FP16_FP16_FP32, 8},\n-      {TensorCoreType::FP32_BF16_BF16_FP32, 8},\n-      {TensorCoreType::FP32_TF32_TF32_FP32, 4},\n-\n-      {TensorCoreType::INT32_INT1_INT1_INT32, 128},\n-      {TensorCoreType::INT32_INT4_INT4_INT32, 32},\n-      {TensorCoreType::INT32_INT8_INT8_INT32, 16},\n-  };\n-};\n-\n-// Data loader for mma.16816 instruction.\n-class MMA16816SmemLoader {\n-public:\n-  MMA16816SmemLoader(int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n-                     ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n-                     ArrayRef<int> instrShape, ArrayRef<int> matShape,\n-                     int perPhase, int maxPhase, int elemBytes,\n-                     ConversionPatternRewriter &rewriter,\n-                     TypeConverter *typeConverter, const Location &loc);\n-\n-  // lane = thread % 32\n-  // warpOff = (thread/32) % wpt(0)\n-  llvm::SmallVector<Value> computeOffsets(Value warpOff, Value lane,\n-                                          Value cSwizzleOffset) {\n-    if (canUseLdmatrix)\n-      return computeLdmatrixMatOffs(warpOff, lane, cSwizzleOffset);\n-    else if (elemBytes == 4 && needTrans)\n-      return computeB32MatOffs(warpOff, lane, cSwizzleOffset);\n-    else if (elemBytes == 1 && needTrans)\n-      return computeB8MatOffs(warpOff, lane, cSwizzleOffset);\n-    else\n-      llvm::report_fatal_error(\"Invalid smem load config\");\n-\n-    return {};\n-  }\n-\n-  int getNumPtrs() const { return numPtrs; }\n-\n-  // Compute the offset to the matrix this thread(indexed by warpOff and lane)\n-  // mapped to.\n-  SmallVector<Value> computeLdmatrixMatOffs(Value warpId, Value lane,\n-                                            Value cSwizzleOffset);\n-\n-  // Compute 32-bit matrix offsets.\n-  SmallVector<Value> computeB32MatOffs(Value warpOff, Value lane,\n-                                       Value cSwizzleOffset);\n-\n-  // compute 8-bit matrix offset.\n-  SmallVector<Value> computeB8MatOffs(Value warpOff, Value lane,\n-                                      Value cSwizzleOffset);\n-\n-  // Load 4 matrices and returns 4 vec<2> elements.\n-  std::tuple<Value, Value, Value, Value>\n-  loadX4(int mat0, int mat1, ArrayRef<Value> offs, ArrayRef<Value> ptrs,\n-         Type matTy, Type shemPtrTy) const;\n-\n-private:\n-  SmallVector<uint32_t> order;\n-  int kOrder;\n-  SmallVector<int64_t> tileShape;\n-  SmallVector<int> instrShape;\n-  SmallVector<int> matShape;\n-  int perPhase;\n-  int maxPhase;\n-  int elemBytes;\n-  ConversionPatternRewriter &rewriter;\n-  const Location &loc;\n-  MLIRContext *ctx{};\n-\n-  int cMatShape;\n-  int sMatShape;\n-\n-  Value sStride;\n-\n-  bool needTrans;\n-  bool canUseLdmatrix;\n-\n-  int numPtrs;\n-\n-  int pLoadStrideInMat;\n-  int sMatStride;\n-\n-  int matArrStride;\n-  int warpOffStride;\n-};\n-\n-// This class helps to adapt the existing DotOpConversion to the latest\n-// DotOpOperand layout design. It decouples the existing implementation to two\n-// parts:\n-// 1. loading the specific operand matrix(for $a, $b, $c) from smem\n-// 2. passing the loaded value and perform the mma codegen\n-struct MMA16816ConversionHelper {\n-  MmaEncodingAttr mmaLayout;\n-  ArrayRef<unsigned int> wpt;\n-  SmallVector<unsigned int> properWpt;\n-\n-  Value thread, lane, warp;\n-\n-  DotOpMmaV2ConversionHelper helper;\n-  ConversionPatternRewriter &rewriter;\n-  TritonGPUToLLVMTypeConverter *typeConverter;\n-  Location loc;\n-  MLIRContext *ctx{};\n-\n-  using ValueTable = std::map<std::pair<unsigned, unsigned>, Value>;\n-\n-  // dotOperand: type of either one operand of dotOp.\n-  MMA16816ConversionHelper(Type dotOperand, MmaEncodingAttr mmaLayout,\n-                           Value thread, ConversionPatternRewriter &rewriter,\n-                           TritonGPUToLLVMTypeConverter *typeConverter,\n-                           Location loc)\n-      : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()), thread(thread),\n-        helper(mmaLayout), rewriter(rewriter), typeConverter(typeConverter),\n-        loc(loc), ctx(mmaLayout.getContext()) {\n-    helper.deduceMmaType(dotOperand);\n-\n-    Value _32 = i32_val(32);\n-    lane = urem(thread, _32);\n-    warp = udiv(thread, _32);\n-  }\n-\n-  // Get a warpId for M axis.\n-  Value getWarpM(int M) const {\n-    auto matInstrShape = helper.getMmaInstrShape();\n-    return urem(urem(warp, i32_val(wpt[0])), i32_val(M / matInstrShape[0]));\n-  }\n-\n-  // Get a warpId for N axis.\n-  Value getWarpN(int N) const {\n-    auto matInstrShape = helper.getMmaInstrShape();\n-    Value warpMN = udiv(warp, i32_val(wpt[0]));\n-    return urem(urem(warpMN, i32_val(wpt[1])), i32_val(N / matInstrShape[1]));\n-  }\n-\n-  // Get the mmaInstrShape deducing either from $a or $b.\n-  std::tuple<int, int, int> getMmaInstrShape(Type operand) const {\n-    helper.deduceMmaType(operand);\n-    auto mmaInstrShape = helper.getMmaInstrShape();\n-    int mmaInstrM = mmaInstrShape[0];\n-    int mmaInstrN = mmaInstrShape[1];\n-    int mmaInstrK = mmaInstrShape[2];\n-    return std::make_tuple(mmaInstrM, mmaInstrN, mmaInstrK);\n-  }\n-\n-  // Get the mmaMatShape deducing either from $a or $b.\n-  std::tuple<int, int, int> getMmaMatShape(Type operand) const {\n-    helper.deduceMmaType(operand);\n-    auto matShape = helper.getMmaMatShape();\n-    int matShapeM = matShape[0];\n-    int matShapeN = matShape[1];\n-    int matShapeK = matShape[2];\n-    return std::make_tuple(matShapeM, matShapeN, matShapeK);\n-  }\n-\n-  // \\param operand is either $a or $b's type.\n-  inline int getNumRepM(Type operand, int M) const {\n-    return getNumRepM(operand, M, wpt[0]);\n-  }\n-\n-  // \\param operand is either $a or $b's type.\n-  inline int getNumRepN(Type operand, int N) const {\n-    return getNumRepN(operand, N, wpt[1]);\n-  }\n-\n-  // \\param operand is either $a or $b's type.\n-  inline int getNumRepK(Type operand, int K) const {\n-    return getNumRepK_(operand, K);\n-  }\n-\n-  static int getNumRepM(Type operand, int M, int wpt) {\n-    auto tensorCoreType =\n-        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n-    int mmaInstrM =\n-        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[0];\n-    return std::max<int>(M / (wpt * mmaInstrM), 1);\n-  }\n-\n-  static int getNumRepN(Type operand, int N, int wpt) {\n-    auto tensorCoreType =\n-        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n-    int mmaInstrN =\n-        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[1];\n-    return std::max<int>(N / (wpt * mmaInstrN), 1);\n-  }\n-\n-  static int getNumRepK_(Type operand, int K) {\n-    auto tensorCoreType =\n-        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n-    int mmaInstrK =\n-        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[2];\n-    return std::max<int>(K / mmaInstrK, 1);\n-  }\n-\n-  // Get number of elements per thread for $a operand.\n-  static size_t getANumElemsPerThread(RankedTensorType operand, int wpt) {\n-    auto shape = operand.getShape();\n-    int repM = getNumRepM(operand, shape[0], wpt);\n-    int repK = getNumRepK_(operand, shape[1]);\n-    return 4 * repM * repK;\n-  }\n-\n-  // Get number of elements per thread for $b operand.\n-  static size_t getBNumElemsPerThread(RankedTensorType operand, int wpt) {\n-    auto shape = operand.getShape();\n-    int repK = getNumRepK_(operand, shape[0]);\n-    int repN = getNumRepN(operand, shape[1], wpt);\n-    return 4 * std::max(repN / 2, 1) * repK;\n-  }\n-\n-  // Loading $a from smem to registers, returns a LLVM::Struct.\n-  Value loadA(Value tensor, const SharedMemoryObject &smemObj) const;\n-\n-  // Loading $b from smem to registers, returns a LLVM::Struct.\n-  Value loadB(Value tensor, const SharedMemoryObject &smemObj);\n-\n-  // Loading $c to registers, returns a Value.\n-  Value loadC(Value tensor, Value llTensor) const;\n-\n-  // Conduct the Dot conversion.\n-  // \\param a, \\param b, \\param c and \\param d are DotOp operands.\n-  // \\param loadedA, \\param loadedB, \\param loadedC, all of them are result of\n-  // loading.\n-  LogicalResult convertDot(Value a, Value b, Value c, Value d, Value loadedA,\n-                           Value loadedB, Value loadedC, DotOp op,\n-                           DotOpAdaptor adaptor) const;\n-\n-private:\n-  std::function<void(int, int)>\n-  getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n-                  MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder,\n-                  SmallVector<int> instrShape, SmallVector<int> matShape,\n-                  Value warpId, ValueTable &vals, bool isA) const;\n-\n-  // Compose a map of Values to a LLVM::Struct.\n-  // The layout is a list of Value with coordinate of (i,j), the order is as\n-  // the follows:\n-  // [\n-  //  (0,0), (0,1), (1,0), (1,1), # i=0, j=0\n-  //  (0,2), (0,3), (1,2), (1,3), # i=0, j=1\n-  //  (0,4), (0,5), (1,4), (1,5), # i=0, j=2\n-  //  ...\n-  //  (2,0), (2,1), (3,0), (3,1), # i=1, j=0\n-  //  (2,2), (2,3), (3,2), (3,3), # i=1, j=1\n-  //  (2,4), (2,5), (3,4), (3,5), # i=1, j=2\n-  //  ...\n-  // ]\n-  // i \\in [0, n0) and j \\in [0, n1)\n-  // There should be \\param n0 * \\param n1 elements in the output Struct.\n-  Value composeValuesToDotOperandLayoutStruct(const ValueTable &vals, int n0,\n-                                              int n1) const;\n-\n-  ValueTable getValuesFromDotOperandLayoutStruct(Value value, int n0, int n1,\n-                                                 Type type) const;\n-};\n-\n-// Helper for conversion of FMA DotOp.\n-struct DotOpFMAConversionHelper {\n-  Attribute layout;\n-  MLIRContext *ctx{};\n-\n-  using ValueTable = std::map<std::pair<int, int>, Value>;\n-\n-  explicit DotOpFMAConversionHelper(Attribute layout)\n-      : layout(layout), ctx(layout.getContext()) {}\n-\n-  SmallVector<Value>\n-  getThreadIds(Value threadId, ArrayRef<unsigned> shapePerCTA,\n-               ArrayRef<unsigned> sizePerThread, ArrayRef<unsigned> order,\n-               ConversionPatternRewriter &rewriter, Location loc) const;\n-\n-  Value loadA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n-              Location loc, TritonGPUToLLVMTypeConverter *typeConverter,\n-              ConversionPatternRewriter &rewriter) const;\n-\n-  Value loadB(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n-              Location loc, TritonGPUToLLVMTypeConverter *typeConverter,\n-              ConversionPatternRewriter &rewriter) const;\n-\n-  ValueTable getValueTableFromStruct(\n-      Value val, int K, int n0, int shapePerCTA, int sizePerThread,\n-      ConversionPatternRewriter &rewriter, Location loc,\n-      TritonGPUToLLVMTypeConverter *typeConverter, Type type) const;\n-\n-  Value getStructFromValueTable(ArrayRef<Value> vals,\n-                                ConversionPatternRewriter &rewriter,\n-                                Location loc,\n-                                TritonGPUToLLVMTypeConverter *typeConverter,\n-                                Type elemTy) const;\n-\n-  // get number of elements per thread for $a or $b.\n-  static int getNumElemsPerThread(ArrayRef<int64_t> shape,\n-                                  DotOperandEncodingAttr dotOpLayout);\n-\n-  // Get shapePerCTA for M or N axis.\n-  static int getShapePerCTAForMN(BlockedEncodingAttr layout, bool isM) {\n-    auto order = layout.getOrder();\n-    auto shapePerCTA = getShapePerCTA(layout);\n-\n-    int mShapePerCTA =\n-        order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-    int nShapePerCTA =\n-        order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-    return isM ? mShapePerCTA : nShapePerCTA;\n-  }\n-\n-  // Get sizePerThread for M or N axis.\n-  static int getSizePerThreadForMN(BlockedEncodingAttr layout, bool isM) {\n-    auto order = layout.getOrder();\n-    auto sizePerThread = getSizePerThread(layout);\n-\n-    int mSizePerThread =\n-        order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n-    int nSizePerThread =\n-        order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n-    return isM ? mSizePerThread : nSizePerThread;\n-  }\n-};\n-\n-} // namespace LLVM\n-} // namespace mlir\n-\n-#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 15, "deletions": 245, "changes": 260, "file_content_changes": "@@ -1,16 +1,24 @@\n #include \"DotOpToLLVM.h\"\n-#include \"DotOpHelpers.h\"\n #include \"Utility.h\"\n \n using namespace mlir;\n using namespace mlir::triton;\n \n-using ::mlir::LLVM::DotOpFMAConversionHelper;\n-using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n-using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n \n+LogicalResult convertFMADot(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                            TritonGPUToLLVMTypeConverter *typeConverter,\n+                            ConversionPatternRewriter &rewriter);\n+\n+LogicalResult convertMMA884(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                            TritonGPUToLLVMTypeConverter *typeConverter,\n+                            ConversionPatternRewriter &rewriter);\n+\n+LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                              TritonGPUToLLVMTypeConverter *typeConverter,\n+                              ConversionPatternRewriter &rewriter);\n+\n struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::DotOp>::ConvertTritonGPUOpToLLVMPattern;\n@@ -34,9 +42,9 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                                     .dyn_cast<MmaEncodingAttr>();\n     if (!isOuter && mmaLayout && supportMMA(op, mmaLayout.getVersionMajor())) {\n       if (mmaLayout.isVolta())\n-        return convertMMA884(op, adaptor, rewriter);\n+        return convertMMA884(op, adaptor, getTypeConverter(), rewriter);\n       if (mmaLayout.isAmpere())\n-        return convertMMA16816(op, adaptor, rewriter);\n+        return convertMMA16816(op, adaptor, getTypeConverter(), rewriter);\n \n       llvm::report_fatal_error(\n           \"Unsupported MMA kind found when converting DotOp to LLVM.\");\n@@ -46,249 +54,11 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n             .cast<RankedTensorType>()\n             .getEncoding()\n             .isa<BlockedEncodingAttr>())\n-      return convertFMADot(op, adaptor, rewriter);\n+      return convertFMADot(op, adaptor, getTypeConverter(), rewriter);\n \n     llvm::report_fatal_error(\n         \"Unsupported DotOp found when converting TritonGPU to LLVM.\");\n   }\n-\n-private:\n-  // Convert to mma.m16n8k16\n-  LogicalResult convertMMA16816(triton::DotOp op, OpAdaptor adaptor,\n-                                ConversionPatternRewriter &rewriter) const {\n-    auto loc = op.getLoc();\n-    auto mmaLayout = op.getResult()\n-                         .getType()\n-                         .cast<RankedTensorType>()\n-                         .getEncoding()\n-                         .cast<MmaEncodingAttr>();\n-\n-    Value A = op.getA();\n-    Value B = op.getB();\n-    Value C = op.getC();\n-\n-    MMA16816ConversionHelper mmaHelper(A.getType(), mmaLayout,\n-                                       getThreadId(rewriter, loc), rewriter,\n-                                       getTypeConverter(), loc);\n-\n-    auto ATensorTy = A.getType().cast<RankedTensorType>();\n-    auto BTensorTy = B.getType().cast<RankedTensorType>();\n-\n-    assert(ATensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n-           BTensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n-           \"Both $a and %b should be DotOperand layout.\");\n-\n-    Value loadedA, loadedB, loadedC;\n-    loadedA = adaptor.getA();\n-    loadedB = adaptor.getB();\n-    loadedC = mmaHelper.loadC(op.getC(), adaptor.getC());\n-\n-    return mmaHelper.convertDot(A, B, C, op.getD(), loadedA, loadedB, loadedC,\n-                                op, adaptor);\n-  }\n-  /// Convert to mma.m8n8k4\n-  LogicalResult convertMMA884(triton::DotOp op, OpAdaptor adaptor,\n-                              ConversionPatternRewriter &rewriter) const {\n-    auto *ctx = op.getContext();\n-    auto loc = op.getLoc();\n-\n-    Value A = op.getA();\n-    Value B = op.getB();\n-    Value D = op.getResult();\n-    auto mmaLayout = D.getType()\n-                         .cast<RankedTensorType>()\n-                         .getEncoding()\n-                         .cast<MmaEncodingAttr>();\n-    auto ALayout = A.getType()\n-                       .cast<RankedTensorType>()\n-                       .getEncoding()\n-                       .cast<DotOperandEncodingAttr>();\n-    auto BLayout = B.getType()\n-                       .cast<RankedTensorType>()\n-                       .getEncoding()\n-                       .cast<DotOperandEncodingAttr>();\n-\n-    auto ATensorTy = A.getType().cast<RankedTensorType>();\n-    auto BTensorTy = B.getType().cast<RankedTensorType>();\n-    auto DTensorTy = D.getType().cast<RankedTensorType>();\n-    auto AShape = ATensorTy.getShape();\n-    auto BShape = BTensorTy.getShape();\n-\n-    bool isARow = ALayout.getMMAv1IsRow();\n-    bool isBRow = BLayout.getMMAv1IsRow();\n-    auto [isARow_, isBRow_, isAVec4_, isBVec4_, _] =\n-        mmaLayout.decodeVoltaLayoutStates();\n-    assert(isARow == isARow_);\n-    assert(isBRow == isBRow_);\n-\n-    DotOpMmaV1ConversionHelper helper(mmaLayout);\n-\n-    unsigned numM = ALayout.getMMAv1NumOuter(AShape);\n-    unsigned numN = BLayout.getMMAv1NumOuter(BShape);\n-    unsigned NK = AShape[1];\n-\n-    auto has = helper.extractLoadedOperand(adaptor.getA(), NK, rewriter,\n-                                           getTypeConverter(), ATensorTy);\n-    auto hbs = helper.extractLoadedOperand(adaptor.getB(), NK, rewriter,\n-                                           getTypeConverter(), BTensorTy);\n-\n-    // Initialize accumulators with external values, the acc holds the\n-    // accumulator value that is shared between the MMA instructions inside a\n-    // DotOp, we can call the order of the values the accumulator-internal\n-    // order.\n-    SmallVector<Value> acc = getTypeConverter()->unpackLLElements(\n-        loc, adaptor.getC(), rewriter, DTensorTy);\n-    size_t resSize = acc.size();\n-\n-    // The resVals holds the final result of the DotOp.\n-    // NOTE The current order of resVals is different from acc, we call it the\n-    // accumulator-external order. and\n-    SmallVector<Value> resVals(resSize);\n-\n-    auto getIdx = [&](int m, int n) {\n-      std::vector<size_t> idx{{\n-          (m * 2 + 0) + (n * 4 + 0) * numM, // row0\n-          (m * 2 + 0) + (n * 4 + 1) * numM,\n-          (m * 2 + 1) + (n * 4 + 0) * numM, // row1\n-          (m * 2 + 1) + (n * 4 + 1) * numM,\n-          (m * 2 + 0) + (n * 4 + 2) * numM, // row2\n-          (m * 2 + 0) + (n * 4 + 3) * numM,\n-          (m * 2 + 1) + (n * 4 + 2) * numM, // row3\n-          (m * 2 + 1) + (n * 4 + 3) * numM,\n-      }};\n-      return idx;\n-    };\n-\n-    auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n-      auto ha = has.at({m, k});\n-      auto hb = hbs.at({n, k});\n-\n-      PTXBuilder builder;\n-      auto idx = getIdx(m, n);\n-\n-      // note: using \"=f\" for float leads to cleaner PTX\n-      bool isIntMMA = DTensorTy.getElementType().isInteger(32);\n-      auto *resOprs = builder.newListOperand(8, isIntMMA ? \"=r\" : \"=f\");\n-      auto *AOprs = builder.newListOperand({\n-          {ha.first, \"r\"},\n-          {ha.second, \"r\"},\n-      });\n-\n-      auto *BOprs = builder.newListOperand({\n-          {hb.first, \"r\"},\n-          {hb.second, \"r\"},\n-      });\n-      auto *COprs = builder.newListOperand();\n-      for (int i = 0; i < 8; ++i)\n-        COprs->listAppend(builder.newOperand(acc[idx[i]], std::to_string(i)));\n-\n-      auto mma = builder.create(\"mma.sync.aligned.m8n8k4\")\n-                     ->o(isARow ? \"row\" : \"col\")\n-                     .o(isBRow ? \"row\" : \"col\")\n-                     .o(\"f32.f16.f16.f32\");\n-\n-      mma(resOprs, AOprs, BOprs, COprs);\n-\n-      Value res =\n-          builder.launch(rewriter, loc, helper.getMmaRetType(ATensorTy));\n-\n-      for (auto i = 0; i < 8; i++) {\n-        Value elem = extract_val(f32_ty, res, i);\n-        acc[idx[i]] = elem;\n-      }\n-    };\n-\n-    for (unsigned k = 0; k < NK; k += 4)\n-      for (unsigned m = 0; m < numM / 2; ++m)\n-        for (unsigned n = 0; n < numN / 2; ++n) {\n-          callMMA(m, n, k);\n-        }\n-\n-    // res holds the same layout of acc\n-    for (size_t i = 0; i < acc.size(); ++i) {\n-      resVals[i] = acc[i];\n-    }\n-\n-    Value res =\n-        getTypeConverter()->packLLElements(loc, resVals, rewriter, DTensorTy);\n-    rewriter.replaceOp(op, res);\n-    return success();\n-  }\n-\n-  LogicalResult convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n-                              ConversionPatternRewriter &rewriter) const {\n-    auto *ctx = rewriter.getContext();\n-    auto loc = op.getLoc();\n-\n-    auto A = op.getA();\n-    auto B = op.getB();\n-    auto C = op.getC();\n-    auto D = op.getResult();\n-\n-    auto aTensorTy = A.getType().cast<RankedTensorType>();\n-    auto bTensorTy = B.getType().cast<RankedTensorType>();\n-    auto dTensorTy = D.getType().cast<RankedTensorType>();\n-\n-    auto aShape = aTensorTy.getShape();\n-    auto bShape = bTensorTy.getShape();\n-\n-    BlockedEncodingAttr dLayout =\n-        dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n-    auto order = dLayout.getOrder();\n-    auto cc = getTypeConverter()->unpackLLElements(loc, adaptor.getC(),\n-                                                   rewriter, dTensorTy);\n-\n-    DotOpFMAConversionHelper helper(dLayout);\n-    Value llA = adaptor.getA();\n-    Value llB = adaptor.getB();\n-\n-    auto sizePerThread = getSizePerThread(dLayout);\n-    auto shapePerCTA = getShapePerCTA(dLayout);\n-\n-    int K = aShape[1];\n-    int M = aShape[0];\n-    int N = bShape[1];\n-\n-    int mShapePerCTA =\n-        order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-    int mSizePerThread =\n-        order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n-    int nShapePerCTA =\n-        order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-    int nSizePerThread =\n-        order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n-\n-    auto has = helper.getValueTableFromStruct(llA, K, M, mShapePerCTA,\n-                                              mSizePerThread, rewriter, loc,\n-                                              getTypeConverter(), aTensorTy);\n-    auto hbs = helper.getValueTableFromStruct(llB, K, N, nShapePerCTA,\n-                                              nSizePerThread, rewriter, loc,\n-                                              getTypeConverter(), bTensorTy);\n-\n-    SmallVector<Value> ret = cc;\n-    bool isCRow = order[0] == 1;\n-\n-    for (unsigned k = 0; k < K; k++) {\n-      for (unsigned m = 0; m < M; m += mShapePerCTA)\n-        for (unsigned n = 0; n < N; n += nShapePerCTA)\n-          for (unsigned mm = 0; mm < mSizePerThread; ++mm)\n-            for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n-              int mIdx = m / mShapePerCTA * mSizePerThread + mm;\n-              int nIdx = n / nShapePerCTA * nSizePerThread + nn;\n-\n-              int z = isCRow ? mIdx * N / nShapePerCTA * mSizePerThread + nIdx\n-                             : nIdx * M / mShapePerCTA * nSizePerThread + mIdx;\n-              ret[z] = rewriter.create<LLVM::FMulAddOp>(\n-                  loc, has[{m + mm, k}], hbs[{n + nn, k}], ret[z]);\n-            }\n-    }\n-\n-    auto res =\n-        getTypeConverter()->packLLElements(loc, ret, rewriter, dTensorTy);\n-    rewriter.replaceOp(op, res);\n-\n-    return success();\n-  }\n };\n \n void populateDotOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/FMA.cpp", "status": "added", "additions": 100, "deletions": 0, "changes": 100, "file_content_changes": "@@ -0,0 +1,100 @@\n+#include \"DotOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+\n+using ValueTableFMA = std::map<std::pair<int, int>, Value>;\n+\n+static ValueTableFMA getValueTableFromStructFMA(\n+    Value val, int K, int n0, int shapePerCTA, int sizePerThread,\n+    ConversionPatternRewriter &rewriter, Location loc,\n+    TritonGPUToLLVMTypeConverter *typeConverter, Type type) {\n+  ValueTableFMA res;\n+  auto elems = typeConverter->unpackLLElements(loc, val, rewriter, type);\n+  int index = 0;\n+  for (unsigned k = 0; k < K; ++k) {\n+    for (unsigned m = 0; m < n0; m += shapePerCTA)\n+      for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n+        res[{m + mm, k}] = elems[index++];\n+      }\n+  }\n+  return res;\n+}\n+\n+LogicalResult convertFMADot(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                            TritonGPUToLLVMTypeConverter *typeConverter,\n+                            ConversionPatternRewriter &rewriter) {\n+  auto *ctx = rewriter.getContext();\n+  auto loc = op.getLoc();\n+\n+  auto A = op.getA();\n+  auto B = op.getB();\n+  auto C = op.getC();\n+  auto D = op.getResult();\n+\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto dTensorTy = D.getType().cast<RankedTensorType>();\n+\n+  auto aShape = aTensorTy.getShape();\n+  auto bShape = bTensorTy.getShape();\n+\n+  BlockedEncodingAttr dLayout =\n+      dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto order = dLayout.getOrder();\n+  auto cc =\n+      typeConverter->unpackLLElements(loc, adaptor.getC(), rewriter, dTensorTy);\n+\n+  Value llA = adaptor.getA();\n+  Value llB = adaptor.getB();\n+\n+  auto sizePerThread = getSizePerThread(dLayout);\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+\n+  int K = aShape[1];\n+  int M = aShape[0];\n+  int N = bShape[1];\n+\n+  int mShapePerCTA =\n+      order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  int mSizePerThread =\n+      order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+  int nShapePerCTA =\n+      order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  int nSizePerThread =\n+      order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+\n+  auto has =\n+      getValueTableFromStructFMA(llA, K, M, mShapePerCTA, mSizePerThread,\n+                                 rewriter, loc, typeConverter, aTensorTy);\n+  auto hbs =\n+      getValueTableFromStructFMA(llB, K, N, nShapePerCTA, nSizePerThread,\n+                                 rewriter, loc, typeConverter, bTensorTy);\n+\n+  SmallVector<Value> ret = cc;\n+  bool isCRow = order[0] == 1;\n+\n+  for (unsigned k = 0; k < K; k++) {\n+    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+      for (unsigned n = 0; n < N; n += nShapePerCTA)\n+        for (unsigned mm = 0; mm < mSizePerThread; ++mm)\n+          for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n+            int mIdx = m / mShapePerCTA * mSizePerThread + mm;\n+            int nIdx = n / nShapePerCTA * nSizePerThread + nn;\n+\n+            int z = isCRow ? mIdx * N / nShapePerCTA * mSizePerThread + nIdx\n+                           : nIdx * M / mShapePerCTA * nSizePerThread + mIdx;\n+            ret[z] = rewriter.create<LLVM::FMulAddOp>(loc, has[{m + mm, k}],\n+                                                      hbs[{n + nn, k}], ret[z]);\n+          }\n+  }\n+\n+  auto res = typeConverter->packLLElements(loc, ret, rewriter, dTensorTy);\n+  rewriter.replaceOp(op, res);\n+\n+  return success();\n+}\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv1.cpp", "status": "added", "additions": 161, "deletions": 0, "changes": 161, "file_content_changes": "@@ -0,0 +1,161 @@\n+#include \"DotOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+\n+using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+\n+static Type getMmaRetType(TensorType operand) {\n+  auto *ctx = operand.getContext();\n+  Type fp32Ty = type::f32Ty(ctx);\n+  // f16*f16+f32->f32\n+  return struct_ty(SmallVector<Type>{8, fp32Ty});\n+}\n+\n+static ValueTable\n+extractLoadedOperand(Value llStruct, int NK,\n+                     ConversionPatternRewriter &rewriter,\n+                     TritonGPUToLLVMTypeConverter *typeConverter, Type type) {\n+  ValueTable rcds;\n+  SmallVector<Value> elems = typeConverter->unpackLLElements(\n+      llStruct.getLoc(), llStruct, rewriter, type);\n+\n+  int offset = 0;\n+  for (int i = 0; offset < elems.size(); ++i) {\n+    for (int k = 0; k < NK; k += 4) {\n+      rcds[{i, k}] = std::make_pair(elems[offset], elems[offset + 1]);\n+      offset += 2;\n+    }\n+  }\n+\n+  return rcds;\n+}\n+\n+LogicalResult convertMMA884(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                            TritonGPUToLLVMTypeConverter *typeConverter,\n+                            ConversionPatternRewriter &rewriter) {\n+  auto *ctx = op.getContext();\n+  auto loc = op.getLoc();\n+\n+  Value A = op.getA();\n+  Value B = op.getB();\n+  Value D = op.getResult();\n+  auto mmaLayout = D.getType()\n+                       .cast<RankedTensorType>()\n+                       .getEncoding()\n+                       .cast<MmaEncodingAttr>();\n+  auto ALayout = A.getType()\n+                     .cast<RankedTensorType>()\n+                     .getEncoding()\n+                     .cast<DotOperandEncodingAttr>();\n+  auto BLayout = B.getType()\n+                     .cast<RankedTensorType>()\n+                     .getEncoding()\n+                     .cast<DotOperandEncodingAttr>();\n+\n+  auto ATensorTy = A.getType().cast<RankedTensorType>();\n+  auto BTensorTy = B.getType().cast<RankedTensorType>();\n+  auto DTensorTy = D.getType().cast<RankedTensorType>();\n+  auto AShape = ATensorTy.getShape();\n+  auto BShape = BTensorTy.getShape();\n+\n+  bool isARow = ALayout.getMMAv1IsRow();\n+  bool isBRow = BLayout.getMMAv1IsRow();\n+  auto [isARow_, isBRow_, isAVec4_, isBVec4_, _] =\n+      mmaLayout.decodeVoltaLayoutStates();\n+  assert(isARow == isARow_);\n+  assert(isBRow == isBRow_);\n+\n+  unsigned numM = ALayout.getMMAv1NumOuter(AShape);\n+  unsigned numN = BLayout.getMMAv1NumOuter(BShape);\n+  unsigned NK = AShape[1];\n+\n+  auto has = extractLoadedOperand(adaptor.getA(), NK, rewriter, typeConverter,\n+                                  ATensorTy);\n+  auto hbs = extractLoadedOperand(adaptor.getB(), NK, rewriter, typeConverter,\n+                                  BTensorTy);\n+\n+  // Initialize accumulators with external values, the acc holds the\n+  // accumulator value that is shared between the MMA instructions inside a\n+  // DotOp, we can call the order of the values the accumulator-internal\n+  // order.\n+  SmallVector<Value> acc =\n+      typeConverter->unpackLLElements(loc, adaptor.getC(), rewriter, DTensorTy);\n+  size_t resSize = acc.size();\n+\n+  // The resVals holds the final result of the DotOp.\n+  // NOTE The current order of resVals is different from acc, we call it the\n+  // accumulator-external order. and\n+  SmallVector<Value> resVals(resSize);\n+\n+  auto getIdx = [&](int m, int n) {\n+    std::vector<size_t> idx{{\n+        (m * 2 + 0) + (n * 4 + 0) * numM, // row0\n+        (m * 2 + 0) + (n * 4 + 1) * numM,\n+        (m * 2 + 1) + (n * 4 + 0) * numM, // row1\n+        (m * 2 + 1) + (n * 4 + 1) * numM,\n+        (m * 2 + 0) + (n * 4 + 2) * numM, // row2\n+        (m * 2 + 0) + (n * 4 + 3) * numM,\n+        (m * 2 + 1) + (n * 4 + 2) * numM, // row3\n+        (m * 2 + 1) + (n * 4 + 3) * numM,\n+    }};\n+    return idx;\n+  };\n+\n+  auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n+    auto ha = has.at({m, k});\n+    auto hb = hbs.at({n, k});\n+\n+    PTXBuilder builder;\n+    auto idx = getIdx(m, n);\n+\n+    // note: using \"=f\" for float leads to cleaner PTX\n+    bool isIntMMA = DTensorTy.getElementType().isInteger(32);\n+    auto *resOprs = builder.newListOperand(8, isIntMMA ? \"=r\" : \"=f\");\n+    auto *AOprs = builder.newListOperand({\n+        {ha.first, \"r\"},\n+        {ha.second, \"r\"},\n+    });\n+\n+    auto *BOprs = builder.newListOperand({\n+        {hb.first, \"r\"},\n+        {hb.second, \"r\"},\n+    });\n+    auto *COprs = builder.newListOperand();\n+    for (int i = 0; i < 8; ++i)\n+      COprs->listAppend(builder.newOperand(acc[idx[i]], std::to_string(i)));\n+\n+    auto mma = builder.create(\"mma.sync.aligned.m8n8k4\")\n+                   ->o(isARow ? \"row\" : \"col\")\n+                   .o(isBRow ? \"row\" : \"col\")\n+                   .o(\"f32.f16.f16.f32\");\n+\n+    mma(resOprs, AOprs, BOprs, COprs);\n+\n+    Value res = builder.launch(rewriter, loc, getMmaRetType(ATensorTy));\n+\n+    for (auto i = 0; i < 8; i++) {\n+      Value elem = extract_val(f32_ty, res, i);\n+      acc[idx[i]] = elem;\n+    }\n+  };\n+\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      for (unsigned n = 0; n < numN / 2; ++n) {\n+        callMMA(m, n, k);\n+      }\n+\n+  // res holds the same layout of acc\n+  for (size_t i = 0; i < acc.size(); ++i) {\n+    resVals[i] = acc[i];\n+  }\n+\n+  Value res = typeConverter->packLLElements(loc, resVals, rewriter, DTensorTy);\n+  rewriter.replaceOp(op, res);\n+  return success();\n+}\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv2.cpp", "status": "added", "additions": 291, "deletions": 0, "changes": 291, "file_content_changes": "@@ -0,0 +1,291 @@\n+#include \"DotOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+\n+using ValueTableV2 = std::map<std::pair<unsigned, unsigned>, Value>;\n+\n+Value loadC(Value tensor, Value llTensor,\n+            TritonGPUToLLVMTypeConverter *typeConverter, Location loc,\n+            ConversionPatternRewriter &rewriter) {\n+  MLIRContext *ctx = tensor.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  size_t fcSize = triton::gpu::getElemsPerThread(tensor.getType());\n+\n+  assert(tensorTy.getEncoding().isa<MmaEncodingAttr>() &&\n+         \"Currently, we only support $c with a mma layout.\");\n+  // Load a normal C tensor with mma layout, that should be a\n+  // LLVM::struct with fcSize elements.\n+  auto structTy = llTensor.getType().cast<LLVM::LLVMStructType>();\n+  assert(structTy.getBody().size() == fcSize &&\n+         \"DotOp's $c operand should pass the same number of values as $d in \"\n+         \"mma layout.\");\n+\n+  auto numMmaRets = tensorTy.getElementType().getIntOrFloatBitWidth() / 8;\n+  assert(numMmaRets == 4 || numMmaRets == 2);\n+  if (numMmaRets == 4) {\n+    return llTensor;\n+  } else if (numMmaRets == 2) {\n+    auto cPack = SmallVector<Value>();\n+    auto cElemTy = tensorTy.getElementType();\n+    int numCPackedElem = 4 / numMmaRets;\n+    Type cPackTy = vec_ty(cElemTy, numCPackedElem);\n+    for (int i = 0; i < fcSize; i += numCPackedElem) {\n+      Value pack = rewriter.create<LLVM::UndefOp>(loc, cPackTy);\n+      for (int j = 0; j < numCPackedElem; ++j) {\n+        pack = insert_element(\n+            cPackTy, pack, extract_val(cElemTy, llTensor, i + j), i32_val(j));\n+      }\n+      cPack.push_back(pack);\n+    }\n+\n+    Type structTy = LLVM::LLVMStructType::getLiteral(\n+        ctx, SmallVector<Type>(cPack.size(), cPackTy));\n+    Value result =\n+        typeConverter->packLLElements(loc, cPack, rewriter, structTy);\n+    return result;\n+  }\n+\n+  return llTensor;\n+}\n+\n+ValueTableV2 getValuesFromDotOperandLayoutStruct(\n+    TritonGPUToLLVMTypeConverter *typeConverter, Location loc,\n+    ConversionPatternRewriter &rewriter, Value value, int n0, int n1,\n+    RankedTensorType type) {\n+\n+  auto elems = typeConverter->unpackLLElements(loc, value, rewriter, type);\n+  int offset{};\n+  ValueTableV2 vals;\n+  for (int i = 0; i < n0; ++i) {\n+    for (int j = 0; j < n1; j++) {\n+      vals[{2 * i, 2 * j}] = elems[offset++];\n+      vals[{2 * i, 2 * j + 1}] = elems[offset++];\n+      vals[{2 * i + 1, 2 * j}] = elems[offset++];\n+      vals[{2 * i + 1, 2 * j + 1}] = elems[offset++];\n+    }\n+  }\n+  return vals;\n+}\n+\n+enum class TensorCoreType : uint8_t {\n+  // floating-point tensor core instr\n+  FP32_FP16_FP16_FP32 = 0, // default\n+  FP32_BF16_BF16_FP32,\n+  FP32_TF32_TF32_FP32,\n+  FP16_FP16_FP16_FP16,\n+  // integer tensor core instr\n+  INT32_INT1_INT1_INT32, // Not implemented\n+  INT32_INT4_INT4_INT32, // Not implemented\n+  INT32_INT8_INT8_INT32, // Not implemented\n+  //\n+  NOT_APPLICABLE,\n+};\n+\n+Type getMmaRetType(TensorCoreType mmaType, MLIRContext *ctx) {\n+  Type fp32Ty = type::f32Ty(ctx);\n+  Type fp16Ty = type::f16Ty(ctx);\n+  Type i32Ty = type::i32Ty(ctx);\n+  Type fp32x4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n+  Type i32x4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i32Ty));\n+  Type fp16x2Pack2Ty = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(2, vec_ty(fp16Ty, 2)));\n+  switch (mmaType) {\n+  case TensorCoreType::FP32_FP16_FP16_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::FP32_BF16_BF16_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::FP32_TF32_TF32_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::FP16_FP16_FP16_FP16:\n+    return fp16x2Pack2Ty;\n+  case TensorCoreType::INT32_INT8_INT8_INT32:\n+    return i32x4Ty;\n+  default:\n+    llvm::report_fatal_error(\"Unsupported mma type found\");\n+  }\n+\n+  return Type{};\n+}\n+\n+TensorCoreType getMmaType(triton::DotOp op) {\n+  Value A = op.getA();\n+  Value B = op.getB();\n+  auto aTy = A.getType().cast<RankedTensorType>();\n+  auto bTy = B.getType().cast<RankedTensorType>();\n+  // d = a*b + c\n+  auto dTy = op.getD().getType().cast<RankedTensorType>();\n+\n+  if (dTy.getElementType().isF32()) {\n+    if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n+      return TensorCoreType::FP32_FP16_FP16_FP32;\n+    if (aTy.getElementType().isBF16() && bTy.getElementType().isBF16())\n+      return TensorCoreType::FP32_BF16_BF16_FP32;\n+    if (aTy.getElementType().isF32() && bTy.getElementType().isF32() &&\n+        op.getAllowTF32())\n+      return TensorCoreType::FP32_TF32_TF32_FP32;\n+  } else if (dTy.getElementType().isInteger(32)) {\n+    if (aTy.getElementType().isInteger(8) && bTy.getElementType().isInteger(8))\n+      return TensorCoreType::INT32_INT8_INT8_INT32;\n+  } else if (dTy.getElementType().isF16()) {\n+    if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n+      return TensorCoreType::FP16_FP16_FP16_FP16;\n+  }\n+\n+  return TensorCoreType::NOT_APPLICABLE;\n+}\n+\n+inline static const std::map<TensorCoreType, std::string> mmaInstrPtx = {\n+    {TensorCoreType::FP32_FP16_FP16_FP32,\n+     \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"},\n+    {TensorCoreType::FP32_BF16_BF16_FP32,\n+     \"mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\"},\n+    {TensorCoreType::FP32_TF32_TF32_FP32,\n+     \"mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\"},\n+\n+    {TensorCoreType::INT32_INT1_INT1_INT32,\n+     \"mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc\"},\n+    {TensorCoreType::INT32_INT4_INT4_INT32,\n+     \"mma.sync.aligned.m16n8k64.row.col.satfinite.s32.s4.s4.s32\"},\n+    {TensorCoreType::INT32_INT8_INT8_INT32,\n+     \"mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\"},\n+\n+    {TensorCoreType::FP16_FP16_FP16_FP16,\n+     \"mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\"},\n+};\n+\n+LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n+                         ConversionPatternRewriter &rewriter, Location loc,\n+                         Value a, Value b, Value c, Value d, Value loadedA,\n+                         Value loadedB, Value loadedC, DotOp op,\n+                         DotOpAdaptor adaptor) {\n+  MLIRContext *ctx = c.getContext();\n+  auto aTensorTy = a.getType().cast<RankedTensorType>();\n+  auto bTensorTy = b.getType().cast<RankedTensorType>();\n+  auto dTensorTy = d.getType().cast<RankedTensorType>();\n+\n+  SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n+                              aTensorTy.getShape().end());\n+  auto dShape = dTensorTy.getShape();\n+  int bitwidth = aTensorTy.getElementType().getIntOrFloatBitWidth();\n+  auto repA =\n+      aTensorTy.getEncoding().cast<DotOperandEncodingAttr>().getMMAv2Rep(\n+          aTensorTy.getShape(), bitwidth);\n+  auto repB =\n+      bTensorTy.getEncoding().cast<DotOperandEncodingAttr>().getMMAv2Rep(\n+          bTensorTy.getShape(), bitwidth);\n+\n+  assert(repA[1] == repB[0]);\n+  int repM = repA[0], repN = repB[1], repK = repA[1];\n+\n+  // shape / shape_per_cta\n+  auto ha = getValuesFromDotOperandLayoutStruct(typeConverter, loc, rewriter,\n+                                                loadedA, repM, repK, aTensorTy);\n+  auto hb = getValuesFromDotOperandLayoutStruct(typeConverter, loc, rewriter,\n+                                                loadedB, std::max(repN / 2, 1),\n+                                                repK, bTensorTy);\n+  auto fc = typeConverter->unpackLLElements(loc, loadedC, rewriter, dTensorTy);\n+  auto numMmaRets = dTensorTy.getElementType().getIntOrFloatBitWidth() / 8;\n+  int numCPackedElem = 4 / numMmaRets;\n+\n+  auto mmaType = getMmaType(op);\n+\n+  auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n+    unsigned colsPerThread = repN * 2;\n+    PTXBuilder builder;\n+    auto &mma = *builder.create(mmaInstrPtx.at(mmaType));\n+    // using =r for float32 works but leads to less readable ptx.\n+    bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n+    bool isAccF16 = dTensorTy.getElementType().isF16();\n+    auto retArgs =\n+        builder.newListOperand(numMmaRets, isIntMMA || isAccF16 ? \"=r\" : \"=f\");\n+    auto aArgs = builder.newListOperand({\n+        {ha[{m, k}], \"r\"},\n+        {ha[{m + 1, k}], \"r\"},\n+        {ha[{m, k + 1}], \"r\"},\n+        {ha[{m + 1, k + 1}], \"r\"},\n+    });\n+    auto bArgs =\n+        builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n+    auto cArgs = builder.newListOperand();\n+    for (int i = 0; i < numMmaRets; ++i) {\n+      cArgs->listAppend(builder.newOperand(\n+          fc[(m * colsPerThread + 4 * n) / numCPackedElem + i],\n+          std::to_string(i)));\n+      // reuse the output registers\n+    }\n+\n+    mma(retArgs, aArgs, bArgs, cArgs);\n+    Value mmaOut =\n+        builder.launch(rewriter, loc, getMmaRetType(mmaType, op.getContext()));\n+\n+    Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n+    for (int i = 0; i < numMmaRets; ++i) {\n+      fc[(m * colsPerThread + 4 * n) / numCPackedElem + i] =\n+          extract_val(elemTy, mmaOut, i);\n+    }\n+  };\n+\n+  for (int k = 0; k < repK; ++k)\n+    for (int m = 0; m < repM; ++m)\n+      for (int n = 0; n < repN; ++n)\n+        callMma(2 * m, n, 2 * k);\n+\n+  Type resElemTy = dTensorTy.getElementType();\n+\n+  // replace with new packed result\n+  Type structTy = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(fc.size() * numCPackedElem, resElemTy));\n+  SmallVector<Value> results(fc.size() * numCPackedElem);\n+  for (int i = 0; i < fc.size(); ++i) {\n+    for (int j = 0; j < numCPackedElem; ++j) {\n+      results[i * numCPackedElem + j] =\n+          numCPackedElem > 1\n+              ? bitcast(extract_element(fc[i], i32_val(j)), resElemTy)\n+              : bitcast(fc[i], resElemTy);\n+    }\n+  }\n+  Value res = typeConverter->packLLElements(loc, results, rewriter, structTy);\n+\n+  rewriter.replaceOp(op, res);\n+\n+  return success();\n+}\n+\n+// Convert to mma.m16n8k16\n+LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                              TritonGPUToLLVMTypeConverter *typeConverter,\n+                              ConversionPatternRewriter &rewriter) {\n+  auto loc = op.getLoc();\n+  auto mmaLayout = op.getResult()\n+                       .getType()\n+                       .cast<RankedTensorType>()\n+                       .getEncoding()\n+                       .cast<MmaEncodingAttr>();\n+\n+  Value A = op.getA();\n+  Value B = op.getB();\n+  Value C = op.getC();\n+\n+  auto ATensorTy = A.getType().cast<RankedTensorType>();\n+  auto BTensorTy = B.getType().cast<RankedTensorType>();\n+\n+  assert(ATensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n+         BTensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n+         \"Both $a and %b should be DotOperand layout.\");\n+\n+  Value loadedA, loadedB, loadedC;\n+  loadedA = adaptor.getA();\n+  loadedB = adaptor.getB();\n+  loadedC =\n+      loadC(op.getC(), adaptor.getC(), typeConverter, op.getLoc(), rewriter);\n+\n+  return convertDot(typeConverter, rewriter, op.getLoc(), A, B, C, op.getD(),\n+                    loadedA, loadedB, loadedC, op, adaptor);\n+}\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 89, "deletions": 74, "changes": 163, "file_content_changes": "@@ -51,24 +51,29 @@ struct FpToFpOpConversion\n   convertFp8E4M3x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n                            const Value &v0, const Value &v1, const Value &v2,\n                            const Value &v3) {\n-    auto *ptxAsm = \"{                                      \\n\"\n-                   \".reg .b32 a<2>, b<2>;                  \\n\"\n-                   \"prmt.b32 a0, 0, $2, 0x5040;            \\n\"\n-                   \"prmt.b32 a1, 0, $2, 0x7060;            \\n\"\n-                   \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"shr.b32  b0, b0, 1;                    \\n\"\n-                   \"shr.b32  b1, b1, 1;                    \\n\"\n-                   \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\"\n-                   \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\"\n-                   \"}\";\n+    auto *ptxAsm = // WARN: subnormal (0bs0000xxx) are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+        \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n+        \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n+        \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n+        \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+        \"shr.b32  b0, b0, 1;                    \\n\" // b0 >>= 1\n+        \"shr.b32  b1, b1, 1;                    \\n\" // shift into fp16 position\n+        \"add.u32  b0, b0, 0x20002000;           \\n\" // b0.exp += 2**4-2**3\n+                                                    // exponent compensate = 8\n+        \"add.u32  b1, b1, 0x20002000;           \\n\" // b1 += 8<<10 | 8<<10<<16\n+        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+        \"}\";\n     return convertFp8x4ToFp16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n   }\n \n   static SmallVector<Value>\n   convertFp8E5M2x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n                            const Value &v0, const Value &v1, const Value &v2,\n                            const Value &v3) {\n+    // exponent bias of Fp8E5M2 and Fp16 are the same\n     auto *ptxAsm = \"{                           \\n\"\n                    \"prmt.b32 $0, 0, $2, 0x5140; \\n\\t\"\n                    \"prmt.b32 $1, 0, $2, 0x7362; \\n\\t\"\n@@ -117,21 +122,21 @@ struct FpToFpOpConversion\n   convertFp8E4M3x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n                            const Value &v0, const Value &v1, const Value &v2,\n                            const Value &v3) {\n-    auto *ptxAsm = \"{                                          \\n\"\n-                   \".reg .b32 a<2>, sign<2>, nosign<2>, b<2>;  \\n\"\n-                   \"prmt.b32 a0, 0, $2, 0x5040;                \\n\"\n-                   \"prmt.b32 a1, 0, $2, 0x7060;                \\n\"\n-                   \"and.b32 sign0, a0, 0x80008000;             \\n\"\n-                   \"and.b32 sign1, a1, 0x80008000;             \\n\"\n-                   \"and.b32 nosign0, a0, 0x7fff7fff;           \\n\"\n-                   \"and.b32 nosign1, a1, 0x7fff7fff;           \\n\"\n-                   \"shr.b32 nosign0, nosign0, 4;               \\n\"\n-                   \"shr.b32 nosign1, nosign1, 4;               \\n\"\n-                   \"add.u32 nosign0, nosign0, 0x38003800;      \\n\"\n-                   \"add.u32 nosign1, nosign1, 0x38003800;      \\n\"\n-                   \"or.b32 $0, sign0, nosign0;                 \\n\"\n-                   \"or.b32 $1, sign1, nosign1;                 \\n\"\n-                   \"}\";\n+    auto *ptxAsm = // WARN: subnormal (0bs0000xxx) are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+        \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n+        \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n+        \"and.b32 b0, a0, 0x7fff7fff;            \\n\" // b0 = a0 & 0x7fff7fff\n+        \"and.b32 b1, a1, 0x7fff7fff;            \\n\" // (strip sign)\n+        \"shr.b32 b0, b0, 4;                     \\n\" // b0 >>= 4\n+        \"shr.b32 b1, b1, 4;                     \\n\" // shift into fp16 position\n+        \"add.u32 b0, b0, 0x3c003c00;            \\n\" // b0.exp += 2**7-2**3\n+                                                    // exponent compensate = 120\n+        \"add.u32 b1, b1, 0x3c003c00;            \\n\" // b1 += 120<<7 | 120<<7<<16\n+        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+        \"}\";\n     return convertFp8x4ToBf16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n   };\n \n@@ -193,18 +198,23 @@ struct FpToFpOpConversion\n   convertFp16x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n                            const Value &v0, const Value &v1, const Value &v2,\n                            const Value &v3) {\n-    auto *ptxAsm = \"{                                      \\n\"\n-                   \".reg .b32 a<2>, b<2>;                  \\n\"\n-                   \"shl.b32 a0, $1, 1;                     \\n\"\n-                   \"shl.b32 a1, $2, 1;                     \\n\"\n-                   \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"add.u32 a0, a0, 0x00800080;            \\n\"\n-                   \"add.u32 a1, a1, 0x00800080;            \\n\"\n-                   \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n-                   \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n-                   \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n-                   \"}\";\n+    auto *ptxAsm = // WARN: subnormal Fp8s are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // see Fp8E4M3x4ToFp16x4\n+        \"sub.u32 a0, $1, 0x20002000;            \\n\" // a0 = input0 - 0x20002000\n+                                                    // (compensate offset)\n+        \"sub.u32 a1, $2, 0x20002000;            \\n\" // a1 = input1 - 0x20002000\n+                                                    // (8 << 10 | 8 << 10 << 16)\n+        \"shl.b32 a0, a0, 1;                     \\n\" // a0 <<= 1\n+        \"shl.b32 a1, a1, 1;                     \\n\" // shift into fp8e4 position\n+        \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // a0 &= 0x7fff7fff\n+        \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+        \"add.u32 a0, a0, 0x00800080;            \\n\" // a0 += 0x00800080\n+        \"add.u32 a1, a1, 0x00800080;            \\n\" // (round to nearest)\n+        \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\" // b0 = a0|(0x80008000&in0)\n+        \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n+        \"prmt.b32 $0, b0, b1, 0x7531;           \\n\" // output = b1b0\n+        \"}\";\n     return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n   }\n \n@@ -282,42 +292,47 @@ struct FpToFpOpConversion\n   convertBf16x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n                            const Value &v0, const Value &v1, const Value &v2,\n                            const Value &v3) {\n-    auto *ptxAsm = \"{                                            \\n\"\n-                   \".reg .u32 sign, sign<2>, nosign, nosign<2>;  \\n\"\n-                   \".reg .u32 fp8_min, fp8_max, rn_, zero;       \\n\"\n-                   \"mov.u32 fp8_min, 0x38003800;                 \\n\"\n-                   \"mov.u32 fp8_max, 0x3ff03ff0;                 \\n\"\n-                   \"mov.u32 rn_, 0x80008;                        \\n\"\n-                   \"mov.u32 zero, 0;                             \\n\"\n-                   \"and.b32 sign0, $1, 0x80008000;               \\n\"\n-                   \"and.b32 sign1, $2, 0x80008000;               \\n\"\n-                   \"prmt.b32 sign, sign0, sign1, 0x7531;         \\n\"\n-                   \"and.b32 nosign0, $1, 0x7fff7fff;             \\n\"\n-                   \"and.b32 nosign1, $2, 0x7fff7fff;             \\n\"\n-                   \".reg .u32 nosign_0_<2>, nosign_1_<2>;        \\n\"\n-                   \"and.b32 nosign_0_0, nosign0, 0xffff0000;     \\n\"\n-                   \"max.u32 nosign_0_0, nosign_0_0, 0x38000000;  \\n\"\n-                   \"min.u32 nosign_0_0, nosign_0_0, 0x3ff00000;  \\n\"\n-                   \"and.b32 nosign_0_1, nosign0, 0x0000ffff;     \\n\"\n-                   \"max.u32 nosign_0_1, nosign_0_1, 0x3800;      \\n\"\n-                   \"min.u32 nosign_0_1, nosign_0_1, 0x3ff0;      \\n\"\n-                   \"or.b32 nosign0, nosign_0_0, nosign_0_1;      \\n\"\n-                   \"and.b32 nosign_1_0, nosign1, 0xffff0000;     \\n\"\n-                   \"max.u32 nosign_1_0, nosign_1_0, 0x38000000;  \\n\"\n-                   \"min.u32 nosign_1_0, nosign_1_0, 0x3ff00000;  \\n\"\n-                   \"and.b32 nosign_1_1, nosign1, 0x0000ffff;     \\n\"\n-                   \"max.u32 nosign_1_1, nosign_1_1, 0x3800;      \\n\"\n-                   \"min.u32 nosign_1_1, nosign_1_1, 0x3ff0;      \\n\"\n-                   \"or.b32 nosign1, nosign_1_0, nosign_1_1;      \\n\"\n-                   \"add.u32 nosign0, nosign0, rn_;               \\n\"\n-                   \"add.u32 nosign1, nosign1, rn_;               \\n\"\n-                   \"sub.u32 nosign0, nosign0, 0x38003800;        \\n\"\n-                   \"sub.u32 nosign1, nosign1, 0x38003800;        \\n\"\n-                   \"shr.u32 nosign0, nosign0, 4;                 \\n\"\n-                   \"shr.u32 nosign1, nosign1, 4;                 \\n\"\n-                   \"prmt.b32 nosign, nosign0, nosign1, 0x6420;   \\n\"\n-                   \"or.b32 $0, nosign, sign;                     \\n\"\n-                   \"}\";\n+    auto *ptxAsm = // bf16 is clamped firstly to fp8 min/max\n+        \"{                                           \\n\" // bf16=fp8>>4 + 120<<7\n+        \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n+        \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n+        \"mov.u32 fp8_min, 0x3c003c00;                \\n\" // so bf16_min = 0x3c00\n+        \"mov.u32 fp8_max, 0x43f043f0;                \\n\" // so bf16_max = 0x43f0\n+        \"mov.u32 rn_, 0x80008;                       \\n\" // round to nearest\n+        \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n+        \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n+        \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n+        \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n+        \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n+\n+        // nosign = clamp(nosign, min, max)\n+        \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n+        \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_0_0, nosign_0_0, 0x3c000000; \\n\"\n+        \"min.u32 nosign_0_0, nosign_0_0, 0x43f00000; \\n\"\n+        \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_0_1, nosign_0_1, 0x3c00;     \\n\"\n+        \"min.u32 nosign_0_1, nosign_0_1, 0x43f0;     \\n\"\n+        \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n+        \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_1_0, nosign_1_0, 0x3c000000; \\n\"\n+        \"min.u32 nosign_1_0, nosign_1_0, 0x43f00000; \\n\"\n+        \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_1_1, nosign_1_1, 0x3c00;     \\n\"\n+        \"min.u32 nosign_1_1, nosign_1_1, 0x43f0;     \\n\"\n+        \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n+\n+        \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n+        \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n+        \"sub.u32 nosign0, nosign0, 0x3c003c00;       \\n\" // nosign0-=0x3c003c00\n+        \"sub.u32 nosign1, nosign1, 0x3c003c00;       \\n\" // (compensate offset)\n+        \"shr.u32 nosign0, nosign0, 4;                \\n\" // nosign0 >>= 4\n+        \"shr.u32 nosign1, nosign1, 4;                \\n\" // shift into to fp8e4\n+        \"prmt.b32 nosign, nosign0, nosign1, 0x6420;  \\n\" // nosign0 = 0x00f100f2\n+                                                         // nosign1 = 0x00f300f4\n+                                                         // nosign = 0xf3f4f1f2\n+        \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n+        \"}\";\n     return convertBf16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n   };\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,5 +1,4 @@\n #include \"TritonGPUToLLVM.h\"\n-#include \"DotOpHelpers.h\"\n #include \"Utility.h\"\n #include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -9,7 +9,6 @@\n \n #include \"TypeConverter.h\"\n //\n-#include \"DotOpHelpers.h\"\n #include \"Utility.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n@@ -701,7 +700,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto shape = type.getShape();\n \n     auto wpt = mmaLayout.getWarpsPerCTA();\n-    auto fpw = LLVM::DotOpMmaV1ConversionHelper::fpw;\n+    static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n     auto [isARow, isBRow, isAVec4, isBVec4, _] =\n         mmaLayout.decodeVoltaLayoutStates();\n \n@@ -787,7 +786,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto bRep = bEncoding.getMMAv1Rep();\n \n     auto wpt = mmaLayout.getWarpsPerCTA();\n-    auto fpw = LLVM::DotOpMmaV1ConversionHelper::fpw;\n+    static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n     SmallVector<int, 2> rep({aRep[0], bRep[1]});\n     SmallVector<int, 2> spw({aSpw[0], bSpw[1]});\n     SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "modified", "additions": 38, "deletions": 66, "changes": 104, "file_content_changes": "@@ -1,15 +1,11 @@\n #include \"TypeConverter.h\"\n-#include \"DotOpHelpers.h\"\n #include \"Utility.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"triton/Conversion/MLIRTypes.h\"\n \n using namespace mlir;\n using namespace mlir::triton;\n \n-using ::mlir::LLVM::DotOpFMAConversionHelper;\n-using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n-using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getElemsPerThread;\n@@ -82,25 +78,47 @@ SmallVector<Value> TritonGPUToLLVMTypeConverter::unpackLLElements(\n   return results;\n }\n \n-llvm::Optional<Type>\n-TritonGPUToLLVMTypeConverter::convertTritonTensorType(RankedTensorType type) {\n+Type TritonGPUToLLVMTypeConverter::getElementTypeForStruct(\n+    RankedTensorType type) {\n+  auto ctx = type.getContext();\n+  Attribute layout = type.getEncoding();\n+  Type elemTy = convertType(type.getElementType());\n+  auto dotOpLayout = layout.dyn_cast<DotOperandEncodingAttr>();\n+  if (!dotOpLayout)\n+    return elemTy;\n+  auto mmaParent = dotOpLayout.getParent().dyn_cast<MmaEncodingAttr>();\n+  if (!mmaParent)\n+    return elemTy;\n+  if (mmaParent.isAmpere()) {\n+    int bitwidth = elemTy.getIntOrFloatBitWidth();\n+    // sub-word integer types need to be packed for perf reasons\n+    if (elemTy.isa<IntegerType>() && bitwidth < 32)\n+      return IntegerType::get(ctx, 32);\n+    // TODO: unify everything to use packed integer-types\n+    // otherwise, vector types are ok\n+    const llvm::DenseMap<int, Type> elemTyMap = {\n+        {32, vec_ty(elemTy, 1)},\n+        {16, vec_ty(elemTy, 2)},\n+        {8, vec_ty(elemTy, 4)},\n+    };\n+    return elemTyMap.lookup(bitwidth);\n+  } else {\n+    assert(mmaParent.isVolta());\n+    return vec_ty(elemTy, 2);\n+  }\n+}\n+\n+Type TritonGPUToLLVMTypeConverter::convertTritonTensorType(\n+    RankedTensorType type) {\n   auto ctx = type.getContext();\n   Attribute layout = type.getEncoding();\n   SmallVector<int64_t> shape(type.getShape().begin(), type.getShape().end());\n+  Type eltType = getElementTypeForStruct(type);\n \n-  if (layout &&\n-      (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n-       layout.isa<MmaEncodingAttr>())) {\n-    unsigned numElementsPerThread = getElemsPerThread(type);\n-    SmallVector<Type, 4> types(numElementsPerThread,\n-                               convertType(type.getElementType()));\n-    return LLVM::LLVMStructType::getLiteral(ctx, types);\n-  } else if (auto shared_layout =\n-                 layout.dyn_cast_or_null<SharedEncodingAttr>()) {\n+  if (auto shared_layout = layout.dyn_cast<SharedEncodingAttr>()) {\n     SmallVector<Type, 4> types;\n     // base ptr\n-    auto ptrType =\n-        LLVM::LLVMPointerType::get(convertType(type.getElementType()), 3);\n+    auto ptrType = LLVM::LLVMPointerType::get(eltType, 3);\n     types.push_back(ptrType);\n     // shape dims\n     auto rank = type.getRank();\n@@ -109,55 +127,9 @@ TritonGPUToLLVMTypeConverter::convertTritonTensorType(RankedTensorType type) {\n       types.push_back(IntegerType::get(ctx, 32));\n     }\n     return LLVM::LLVMStructType::getLiteral(ctx, types);\n-  } else if (auto dotOpLayout =\n-                 layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n-    if (dotOpLayout.getParent()\n-            .isa<BlockedEncodingAttr>()) { // for parent is blocked layout\n-      int numElemsPerThread =\n-          DotOpFMAConversionHelper::getNumElemsPerThread(shape, dotOpLayout);\n-\n-      return LLVM::LLVMStructType::getLiteral(\n-          ctx, SmallVector<Type>(numElemsPerThread, type::f32Ty(ctx)));\n-    } else { // for parent is MMA layout\n-      auto mmaLayout = dotOpLayout.getParent().cast<MmaEncodingAttr>();\n-      auto wpt = mmaLayout.getWarpsPerCTA();\n-      Type elemTy = convertType(type.getElementType());\n-      if (mmaLayout.isAmpere()) {\n-        const llvm::DenseMap<int, Type> targetTyMap = {\n-            {32, vec_ty(elemTy, 1)},\n-            {16, vec_ty(elemTy, 2)},\n-            {8, vec_ty(elemTy, 4)},\n-        };\n-        Type targetTy;\n-        if (targetTyMap.count(elemTy.getIntOrFloatBitWidth())) {\n-          targetTy = targetTyMap.lookup(elemTy.getIntOrFloatBitWidth());\n-          // <2xi16>/<4xi8> => i32\n-          // We are doing this because NVPTX inserts extra integer instrs to\n-          // pack & unpack vectors of sub-word integers\n-          // Note: this needs to be synced with\n-          //       DotOpMmaV2ConversionHelper::loadX4\n-          if (elemTy.isa<IntegerType>() &&\n-              (elemTy.getIntOrFloatBitWidth() == 8 ||\n-               elemTy.getIntOrFloatBitWidth() == 16))\n-            targetTy = IntegerType::get(ctx, 32);\n-        } else {\n-          assert(false && \"Unsupported element type\");\n-        }\n-        auto elems = getElemsPerThread(type);\n-        return struct_ty(SmallVector<Type>(elems, targetTy));\n-      }\n-\n-      if (mmaLayout.isVolta()) {\n-        int elems = getElemsPerThread(type);\n-        Type x2Ty = vec_ty(elemTy, 2);\n-        return struct_ty(SmallVector<Type>(elems, x2Ty));\n-      }\n-    }\n-\n-    llvm::errs() << \"Unexpected dot operand layout detected in \"\n-                    \"TritonToLLVMTypeConverter\";\n-    return std::nullopt;\n   }\n \n-  return std::nullopt;\n+  unsigned numElementsPerThread = getElemsPerThread(type);\n+  SmallVector<Type, 4> types(numElementsPerThread, eltType);\n+  return LLVM::LLVMStructType::getLiteral(ctx, types);\n }\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -15,6 +15,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n   TritonGPUToLLVMTypeConverter(MLIRContext *ctx, LowerToLLVMOptions &option,\n                                const DataLayoutAnalysis *analysis = nullptr);\n \n+  Type getElementTypeForStruct(RankedTensorType type);\n   Type convertTritonPointerType(triton::PointerType type);\n \n   Value packLLElements(Location loc, ValueRange resultVals,\n@@ -24,7 +25,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n                                       ConversionPatternRewriter &rewriter,\n                                       Type type);\n \n-  llvm::Optional<Type> convertTritonTensorType(RankedTensorType type);\n+  Type convertTritonTensorType(RankedTensorType type);\n };\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 0, "deletions": 5, "changes": 5, "file_content_changes": "@@ -1,14 +1,9 @@\n #include \"ViewOpToLLVM.h\"\n-#include \"DotOpHelpers.h\"\n \n using namespace mlir;\n using namespace mlir::triton;\n \n-using ::mlir::LLVM::DotOpFMAConversionHelper;\n-using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n-using ::mlir::LLVM::DotOpMmaV2ConversionHelper;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n-using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::getElemsPerThread;\n \n struct SplatOpConversion"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 29, "deletions": 4, "changes": 33, "file_content_changes": "@@ -684,9 +684,12 @@ class CFBranchPattern : public OpConversionPattern<cf::BranchOp> {\n   LogicalResult\n   matchAndRewrite(cf::BranchOp op, cf::BranchOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    addNamedAttrs(rewriter.replaceOpWithNewOp<cf::BranchOp>(\n-                      op, op.getSuccessor(), adaptor.getOperands()),\n-                  adaptor.getAttributes());\n+    auto converter = getTypeConverter();\n+    auto newOp = rewriter.replaceOpWithNewOp<cf::BranchOp>(\n+        op, op.getSuccessor(), adaptor.getOperands());\n+    if (failed(rewriter.convertRegionTypes(newOp.getSuccessor()->getParent(),\n+                                           *converter)))\n+      return failure();\n     return success();\n   }\n };\n@@ -711,14 +714,36 @@ class CFCondBranchPattern : public OpConversionPattern<cf::CondBranchOp> {\n     if (failed(rewriter.convertRegionTypes(newOp.getFalseDest()->getParent(),\n                                            *converter)))\n       return failure();\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+};\n+\n+class FuncOpPattern : public OpConversionPattern<func::FuncOp> {\n+public:\n+  using OpConversionPattern<func::FuncOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(func::FuncOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto converter = getTypeConverter();\n+    auto newOp = rewriter.replaceOpWithNewOp<func::FuncOp>(\n+        op, op.getName(), op.getFunctionType());\n+    addNamedAttrs(newOp, adaptor.getAttributes());\n+    rewriter.inlineRegionBefore(op.getBody(), newOp.getBody(),\n+                                newOp.getBody().end());\n+    if (failed(rewriter.convertRegionTypes(&newOp.getBody(), *converter)))\n+      return failure();\n+\n     return success();\n   }\n };\n \n void populateCFPatterns(TritonGPUTypeConverter &typeConverter,\n                         RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n-  patterns.add<CFBranchPattern, CFCondBranchPattern>(typeConverter, context);\n+  patterns.add<FuncOpPattern, CFCondBranchPattern, CFBranchPattern>(\n+      typeConverter, context);\n }\n //\n "}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 54, "deletions": 18, "changes": 72, "file_content_changes": "@@ -411,30 +411,36 @@ unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n   return 0;\n }\n \n+SmallVector<int64_t>\n+DotOperandEncodingAttr::getMMAv2Rep(ArrayRef<int64_t> shape,\n+                                    int bitwidth) const {\n+  auto mmaParent = getParent().cast<MmaEncodingAttr>();\n+  SmallVector<int> shapePerWarp = {16, 8, 4 * 64 / bitwidth};\n+  auto warpsPerCTA = getParent().cast<MmaEncodingAttr>().getWarpsPerCTA();\n+  assert(mmaParent.isAmpere());\n+  if (getOpIdx() == 0)\n+    return {std::max<int64_t>(1, shape[0] / (shapePerWarp[0] * warpsPerCTA[0])),\n+            shape[1] / shapePerWarp[2]};\n+  else {\n+    assert(getOpIdx() == 1);\n+    return {\n+        shape[0] / shapePerWarp[2],\n+        std::max<int64_t>(1, shape[1] / (shapePerWarp[1] * warpsPerCTA[1]))};\n+  }\n+}\n+\n unsigned DotOperandEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n                                                    Type eltTy) const {\n   if (auto mmaParent = getParent().dyn_cast<MmaEncodingAttr>()) {\n     int warpsPerCTAM = mmaParent.getWarpsPerCTA()[0];\n     int warpsPerCTAN = mmaParent.getWarpsPerCTA()[1];\n     // A100\n     if (mmaParent.isAmpere()) {\n-      int bitwidth = eltTy.getIntOrFloatBitWidth();\n-      int shapePerWarpM = 16;\n-      int shapePerWarpN = 8;\n-      int shapePerWarpK = 4 * 64 / bitwidth;\n-      int shapePerCTAM = shapePerWarpM * warpsPerCTAM;\n-      int shapePerCTAN = shapePerWarpN * warpsPerCTAN;\n-\n-      if (getOpIdx() == 0) {\n-        int repM = std::max<int>(1, shape[0] / shapePerCTAM);\n-        int repK = std::max<int>(1, shape[1] / shapePerWarpK);\n-        return 4 * repM * repK;\n-      }\n-      if (getOpIdx() == 1) {\n-        int repN = std::max<int>(1, shape[1] / shapePerCTAN);\n-        int repK = std::max<int>(1, shape[0] / shapePerWarpK);\n-        return 4 * std::max(repN / 2, 1) * repK;\n-      }\n+      auto rep = getMMAv2Rep(shape, eltTy.getIntOrFloatBitWidth());\n+      if (getOpIdx() == 0)\n+        return 4 * rep[0] * rep[1];\n+      if (getOpIdx() == 1)\n+        return 4 * rep[0] * std::max<int>(rep[1] / 2, 1);\n     }\n     // V100\n     if (mmaParent.isVolta()) {\n@@ -497,7 +503,31 @@ unsigned DotOperandEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n       }\n     }\n   }\n-  llvm_unreachable(\"unknown mma version\");\n+  if (auto blockedLayout = getParent().dyn_cast<BlockedEncodingAttr>()) {\n+    auto shapePerCTA = getShapePerCTA(blockedLayout);\n+    auto order = blockedLayout.getOrder();\n+    auto sizePerThread = getSizePerThread(blockedLayout);\n+\n+    int K = getOpIdx() == 0 ? shape[1] : shape[0];\n+    int otherDim = getOpIdx() == 1 ? shape[1] : shape[0];\n+\n+    bool isM = getOpIdx() == 0;\n+\n+    int mSizePerThread =\n+        order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+    int nSizePerThread =\n+        order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+    int sizePerThreadMN = isM ? mSizePerThread : nSizePerThread;\n+\n+    int mShapePerCTA =\n+        order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+    int nShapePerCTA =\n+        order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+    int shapePerCTAMN = isM ? mShapePerCTA : nShapePerCTA;\n+\n+    return K * std::max<int>(otherDim / shapePerCTAMN, 1) * sizePerThreadMN;\n+  }\n+  llvm_unreachable(\"unknown dot operand parent layout\");\n   return 0;\n }\n \n@@ -946,6 +976,12 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n                                                 view.getResult());\n     return mlir::success();\n   }\n+  // cvt(cat) -> cat\n+  if (auto cat = dyn_cast<triton::CatOp>(arg)) {\n+    rewriter.replaceOpWithNewOp<triton::CatOp>(op, op->getResult(0).getType(),\n+                                               cat.getOperands());\n+    return mlir::success();\n+  }\n   // cvt(alloc_tensor(x), type2) -> alloc_tensor(x, type2)\n   auto alloc_tensor = dyn_cast<triton::gpu::AllocTensorOp>(arg);\n   if (alloc_tensor) {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 126, "deletions": 43, "changes": 169, "file_content_changes": "@@ -25,9 +25,17 @@ namespace ttg = triton::gpu;\n static Type getI1SameShape(Value v) {\n   Type vType = v.getType();\n   auto i1Type = IntegerType::get(vType.getContext(), 1);\n-  auto tensorType = vType.cast<RankedTensorType>();\n-  return RankedTensorType::get(tensorType.getShape(), i1Type,\n-                               tensorType.getEncoding());\n+  if (auto tensorType = vType.dyn_cast<RankedTensorType>())\n+    return RankedTensorType::get(tensorType.getShape(), i1Type,\n+                                 tensorType.getEncoding());\n+  return i1Type;\n+}\n+\n+// pass named attrs (e.g., tt.contiguity) from Triton to Triton\n+static void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n+  for (const NamedAttribute attr : dictAttrs.getValue())\n+    if (!op->hasAttr(attr.getName()))\n+      op->setAttr(attr.getName(), attr.getValue());\n }\n \n #define int_attr(num) builder.getI64IntegerAttr(num)\n@@ -69,6 +77,23 @@ class LoopPipeliner {\n   /// Block arguments that loads depend on\n   SetVector<BlockArgument> depArgs;\n \n+  /// If we have a load that immediately depends on a block argument in the\n+  /// current iteration, it is an immediate dependency. Otherwise, it is a\n+  /// non-immediate dependency, which means the load depends on a block argument\n+  /// in the previous iterations.\n+  /// For example:\n+  /// scf.for (%arg0, %arg1, %arg2) {\n+  ///   %0 = load %arg0  <--- immediate dep, this address is initialized at\n+  ///   numStages-2\n+  ///   %1 = load %arg1\n+  ///   %2 = add %1, %arg2\n+  ///   %3 = load %2  <--- non-immediate dep, %arg1 must be an update-to-date\n+  ///   value\n+  /// }\n+  SetVector<BlockArgument> immedidateDepArgs;\n+\n+  SetVector<BlockArgument> nonImmedidateDepArgs;\n+\n   /// Operations (inside the loop body) that loads depend on\n   SetVector<Operation *> depOps;\n \n@@ -79,6 +104,9 @@ class LoopPipeliner {\n \n   Value lookupOrDefault(Value origin, int stage);\n \n+  Value getLoadMask(triton::LoadOp loadOp, Value mappedMask, Value loopCond,\n+                    OpBuilder &builder);\n+\n   /// Returns a empty buffer of size <numStages, ...>\n   ttg::AllocTensorOp allocateEmptyBuffer(Operation *op, OpBuilder &builder);\n \n@@ -170,7 +198,7 @@ LogicalResult LoopPipeliner::initialize() {\n   }\n \n   // can we use forOp.walk(...) here?\n-  SmallVector<triton::LoadOp, 2> allLoads;\n+  SmallVector<triton::LoadOp, 2> validLoads;\n   for (Operation &op : *loop)\n     if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n       auto ptr = loadOp.getPtr();\n@@ -184,30 +212,31 @@ LogicalResult LoopPipeliner::initialize() {\n                     .cast<triton::PointerType>()\n                     .getPointeeType();\n       unsigned width = vec * ty.getIntOrFloatBitWidth();\n+      // cp.async's cp-size can only be 4, 8 and 16.\n       if (width >= 32)\n-        allLoads.push_back(loadOp);\n+        validLoads.push_back(loadOp);\n     }\n \n   // Early stop: no need to continue if there is no load in the loop.\n-  if (allLoads.empty())\n+  if (validLoads.empty())\n     return failure();\n \n   // load => values that it depends on\n   DenseMap<Value, SetVector<Value>> loadDeps;\n-  for (triton::LoadOp loadOp : allLoads) {\n+  for (triton::LoadOp loadOp : validLoads) {\n     SetVector<Value> deps;\n     for (Value op : loadOp->getOperands())\n       collectDeps(op, numStages - 1, deps);\n     loadDeps[loadOp] = deps;\n   }\n \n-  // Don't pipeline loads that depend on other loads\n-  // (Because if a load depends on another load, this load needs to wait on the\n-  //  other load in the prologue, which is against the point of the pipeline\n-  //  pass)\n-  for (triton::LoadOp loadOp : allLoads) {\n+  // Don't pipeline valid loads that depend on other valid loads\n+  // (Because if a valid load depends on another valid load, this load needs to\n+  // wait on the other load in the prologue, which is against the point of the\n+  // pipeline pass)\n+  for (triton::LoadOp loadOp : validLoads) {\n     bool isCandidate = true;\n-    for (triton::LoadOp other : allLoads) {\n+    for (triton::LoadOp other : validLoads) {\n       if (loadDeps[loadOp].contains(other)) {\n         isCandidate = false;\n         break;\n@@ -264,20 +293,54 @@ LogicalResult LoopPipeliner::initialize() {\n   if (!loads.empty()) {\n     // Update depArgs & depOps\n     for (Value loadOp : loads) {\n-      for (Value dep : loadDeps[loadOp]) {\n-        // TODO: we should record the stage that the value is depended on\n-        if (auto arg = dep.dyn_cast<BlockArgument>())\n+      auto &deps = loadDeps[loadOp];\n+      for (auto &dep : deps) {\n+        if (auto arg = dep.dyn_cast<BlockArgument>()) {\n           depArgs.insert(arg);\n-        else\n+          if (deps.front().isa<BlockArgument>()) {\n+            immedidateDepArgs.insert(arg);\n+          } else {\n+            nonImmedidateDepArgs.insert(arg);\n+          }\n+        } else\n           depOps.insert(dep.getDefiningOp());\n       }\n     }\n     return success();\n   }\n \n+  // Check if immedidateDepArgs and nonImmedidateDepArgs are disjoint\n+  // If yes, we cannot pipeline the loop for now\n+  for (BlockArgument arg : immedidateDepArgs)\n+    if (nonImmedidateDepArgs.contains(arg)) {\n+      return failure();\n+    }\n+\n   return failure();\n }\n \n+Value LoopPipeliner::getLoadMask(triton::LoadOp loadOp, Value mappedMask,\n+                                 Value loopCond, OpBuilder &builder) {\n+  Type maskType = getI1SameShape(loadOp);\n+  Value mask = loadOp.getMask();\n+  Value newMask;\n+  if (mask) {\n+    Value cond = loopCond;\n+    if (isa<RankedTensorType>(maskType)) {\n+      cond = builder.create<triton::SplatOp>(mask.getLoc(), maskType, loopCond);\n+    }\n+    newMask = builder.create<arith::AndIOp>(mask.getLoc(), mappedMask, cond);\n+  } else {\n+    if (isa<RankedTensorType>(maskType)) {\n+      newMask = builder.create<triton::SplatOp>(loopCond.getLoc(), maskType,\n+                                                loopCond);\n+    } else {\n+      newMask = loopCond;\n+    }\n+  }\n+  return newMask;\n+}\n+\n void LoopPipeliner::emitPrologue() {\n   // llvm::errs() << \"loads to pipeline...:\\n\";\n   // for (Value load : loads)\n@@ -322,17 +385,9 @@ void LoopPipeliner::emitPrologue() {\n         }\n         // load => copy async\n         if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n-          Value mask = lookupOrDefault(loadOp.getMask(), stage);\n-          Value newMask;\n-          if (mask) {\n-            Value splatCond = builder.create<triton::SplatOp>(\n-                mask.getLoc(), mask.getType(), loopCond);\n-            newMask =\n-                builder.create<arith::AndIOp>(mask.getLoc(), mask, splatCond);\n-          } else {\n-            newMask = builder.create<triton::SplatOp>(\n-                loopCond.getLoc(), getI1SameShape(loadOp), loopCond);\n-          }\n+          Value newMask =\n+              getLoadMask(loadOp, lookupOrDefault(loadOp.getMask(), stage),\n+                          loopCond, builder);\n           // TODO: check if the hardware supports async copy\n           newOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n               op->getLoc(), loadsBuffer[loadOp].getType(),\n@@ -345,7 +400,19 @@ void LoopPipeliner::emitPrologue() {\n         } else\n           llvm_unreachable(\"This should be LoadOp\");\n       } else {\n-        newOp = builder.clone(*op);\n+        if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n+          Value newMask =\n+              getLoadMask(loadOp, lookupOrDefault(loadOp.getMask(), stage),\n+                          loopCond, builder);\n+          newOp = builder.create<triton::LoadOp>(\n+              loadOp.getLoc(), loadOp.getResult().getType(),\n+              lookupOrDefault(loadOp.getPtr(), stage), newMask,\n+              lookupOrDefault(loadOp.getOther(), stage), loadOp.getCache(),\n+              loadOp.getEvict(), loadOp.getIsVolatile());\n+          addNamedAttrs(newOp, op->getAttrDictionary());\n+        } else {\n+          newOp = builder.clone(*op);\n+        }\n         // Update loop-carried uses\n         for (unsigned opIdx = 0; opIdx < op->getNumOperands(); ++opIdx) {\n           auto it = valueMapping.find(op->getOperand(opIdx));\n@@ -429,7 +496,10 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   //   (original args)\n   //   (insertSliceAsync buffer at stage numStages - 1) for each load\n   //   (extracted tensor) for each load\n-  //   (depArgs at stage numStages - 2)\n+  //   (depArgs at stage numStages - 1):\n+  //   for each dep arg that is not an immediate block argument\n+  //   (depArgs at stage numStages - 2):\n+  //   for each dep arg that is an immediate block argument\n   //   (iv at stage numStages - 2)\n   //   (pipeline iteration index)\n   //   (loop iteration index)\n@@ -450,7 +520,10 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   size_t depArgsBeginIdx = newLoopArgs.size();\n   for (BlockArgument depArg : depArgs) {\n     depArgsIdx[depArg] = newLoopArgs.size();\n-    newLoopArgs.push_back(valueMapping[depArg][numStages - 2]);\n+    if (immedidateDepArgs.contains(depArg)) {\n+      newLoopArgs.push_back(valueMapping[depArg][numStages - 2]);\n+    } else\n+      newLoopArgs.push_back(valueMapping[depArg][numStages - 1]);\n   }\n \n   size_t nextIVIdx = newLoopArgs.size();\n@@ -549,7 +622,21 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   for (Operation *op : orderedDeps)\n     if (!loads.contains(op->getResult(0))) {\n-      Operation *nextOp = builder.clone(*op, nextMapping);\n+      Operation *nextOp;\n+      if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n+        auto newMask =\n+            getLoadMask(loadOp, nextMapping.lookupOrDefault(loadOp.getMask()),\n+                        nextLoopCond, builder);\n+        nextOp = builder.create<triton::LoadOp>(\n+            loadOp.getLoc(), loadOp.getResult().getType(),\n+            nextMapping.lookupOrDefault(loadOp.getPtr()), newMask,\n+            nextMapping.lookupOrDefault(loadOp.getOther()), loadOp.getCache(),\n+            loadOp.getEvict(), loadOp.getIsVolatile());\n+        addNamedAttrs(nextOp, op->getAttrDictionary());\n+        nextMapping.map(loadOp.getResult(), nextOp->getResult(0));\n+      } else {\n+        nextOp = builder.clone(*op, nextMapping);\n+      }\n \n       auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n@@ -571,21 +658,17 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n     // Update loading mask\n     if (loads.contains(op->getResult(0))) {\n       auto loadOp = llvm::cast<triton::LoadOp>(op);\n-      Value mask = loadOp.getMask();\n-      Value newMask;\n+      auto mask = loadOp.getMask();\n+      auto newMask =\n+          getLoadMask(loadOp, nextMapping.lookupOrDefault(loadOp.getMask()),\n+                      nextLoopCond, builder);\n       if (mask) {\n-        Value splatCond = builder.create<triton::SplatOp>(\n-            mask.getLoc(), mask.getType(), nextLoopCond);\n-        newMask = builder.create<arith::AndIOp>(\n-            mask.getLoc(), splatCond, nextMapping.lookupOrDefault(mask));\n         // If mask is defined outside the loop, don't update the map more than\n         // once\n         if (!(forOp.isDefinedOutsideOfLoop(mask) && nextMapping.contains(mask)))\n-          nextMapping.map(mask, newMask);\n-        newMask = nextMapping.lookupOrDefault(loadOp.getMask());\n-      } else\n-        newMask = builder.create<triton::SplatOp>(\n-            loadOp.getLoc(), getI1SameShape(loadOp), nextLoopCond);\n+          nextMapping.map(loadOp.getMask(), newMask);\n+        newMask = nextMapping.lookupOrDefault(mask);\n+      }\n       Value insertAsyncOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n           op->getLoc(), loadsBuffer[loadOp].getType(),\n           nextMapping.lookupOrDefault(loadOp.getPtr()),"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 36, "deletions": 21, "changes": 57, "file_content_changes": "@@ -84,41 +84,46 @@ class DecomposeDotOperand : public mlir::RewritePattern {\n   }\n };\n \n+// It's beneficial to move the conversion\n+// to after the reduce if necessary since it will be\n+// done on a rank-reduced tensor hence cheaper\n class SimplifyReduceCvt : public mlir::RewritePattern {\n public:\n   explicit SimplifyReduceCvt(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::ReduceOp::getOperationName(), 2, context) {\n-  }\n+      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n+                             2, context) {}\n \n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n-    auto reduce = cast<triton::ReduceOp>(*op);\n-    auto reduceArg = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n-        reduce.getOperand().getDefiningOp());\n-    if (!reduceArg)\n+    if (!llvm::isa<triton::gpu::ConvertLayoutOp>(op))\n+      return mlir::failure();\n+    auto convert = llvm::cast<triton::gpu::ConvertLayoutOp>(op);\n+    triton::ReduceOp reduce;\n+    for (auto &use : convert.getResult().getUses()) {\n+      auto owner = use.getOwner();\n+      if (llvm::isa_and_nonnull<triton::ReduceOp>(owner)) {\n+        reduce = llvm::cast<triton::ReduceOp>(owner);\n+        break;\n+      }\n+    }\n+    if (!reduce)\n       return mlir::failure();\n     // this may generate unsupported conversions in the LLVM codegen\n-    if (reduceArg.getOperand()\n+    if (convert.getOperand()\n             .getType()\n             .cast<RankedTensorType>()\n             .getEncoding()\n             .isa<triton::gpu::MmaEncodingAttr>())\n       return mlir::failure();\n     auto newReduce = rewriter.create<triton::ReduceOp>(\n-        op->getLoc(), reduce.getRedOp(), reduceArg.getOperand(),\n+        op->getLoc(), reduce.getRedOp(), convert.getOperand(),\n         reduce.getAxis());\n-    if (isa_and_nonnull<triton::gpu::ConvertLayoutOp>(\n-            *reduceArg.getOperand().getDefiningOp()))\n-      return mlir::failure();\n     Value newRet = newReduce.getResult();\n-    // it's still beneficial to move the conversion\n-    // to after the reduce if necessary since it will be\n-    // done on a rank-reduced tensor hence cheaper\n     if (newRet.getType() != reduce.getResult().getType())\n       newRet = rewriter.create<triton::gpu::ConvertLayoutOp>(\n           op->getLoc(), reduce.getResult().getType(), newRet);\n-    rewriter.replaceOp(op, newRet);\n+    rewriter.replaceAllUsesWith(reduce, newRet);\n \n     return success();\n   }\n@@ -327,8 +332,7 @@ class RematerializeForward : public mlir::RewritePattern {\n         return failure();\n       }\n       // don't rematerialize non-element-wise\n-      if (!isa<triton::ViewOp, triton::CatOp>(op) &&\n-          !op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n+      if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n           !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n           !isa<triton::StoreOp>(op)) {\n         return failure();\n@@ -363,7 +367,7 @@ class RematerializeBackward : public mlir::RewritePattern {\n public:\n   explicit RematerializeBackward(mlir::MLIRContext *context)\n       : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             2, context) {}\n+                             3, context) {}\n \n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *cvt,\n@@ -555,8 +559,19 @@ class ConvertDotConvert : public mlir::RewritePattern {\n       return mlir::failure();\n \n     // TODO: int tensor cores\n-    auto _0f = rewriter.create<arith::ConstantFloatOp>(\n-        op->getLoc(), APFloat(0.0f), dstTy.getElementType().cast<FloatType>());\n+    auto out_dtype = dstTy.getElementType().cast<FloatType>();\n+    APFloat value(0.0f);\n+    if (out_dtype.isBF16())\n+      value = APFloat(APFloat::IEEEhalf(), APInt(16, 0));\n+    else if (out_dtype.isF16())\n+      value = APFloat(APFloat::IEEEhalf(), APInt(16, 0));\n+    else if (out_dtype.isF32())\n+      value = APFloat(0.0f);\n+    else\n+      llvm_unreachable(\"unsupported data type\");\n+\n+    auto _0f =\n+        rewriter.create<arith::ConstantFloatOp>(op->getLoc(), value, out_dtype);\n     auto _0 = rewriter.create<triton::SplatOp>(\n         op->getLoc(), dotOp.getResult().getType(), _0f);\n     auto newDot = rewriter.create<triton::DotOp>(\n@@ -595,7 +610,7 @@ class TritonGPURemoveLayoutConversionsPass\n     patterns.add<DecomposeDotOperand>(context);\n     patterns.add<ConvertDotConvert>(context);\n \n-    if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n+    if (mlir::applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n       signalPassFailure();\n     }\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 16, "deletions": 11, "changes": 27, "file_content_changes": "@@ -14,11 +14,10 @@ using namespace mlir::triton::gpu;\n TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n                                                int numWarps)\n     : context(context), numWarps(numWarps) {\n-  // TODO: how does MLIR pick the right conversion?\n   addConversion([](Type type) { return type; });\n   addConversion([this](RankedTensorType tensorType) -> RankedTensorType {\n     // types with encoding are already in the right format\n-    // TODO: check for layout encodings specifically\n+    // TODO: check for layout encodings more specifically\n     if (tensorType.getEncoding())\n       return tensorType;\n     // pessimistic values for attributes:\n@@ -41,16 +40,19 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n   // This will create newArg, and map(origArg, newArg)\n   addArgumentMaterialization([&](OpBuilder &builder,\n                                  RankedTensorType tensorType, ValueRange inputs,\n-                                 Location loc) {\n-    llvm_unreachable(\"Argument rematerialization not implemented\");\n+                                 Location loc) -> llvm::Optional<Value> {\n+    llvm_unreachable(\"Argument rematerialization should not happen in Triton \"\n+                     \"-> TritonGPU conversion\");\n     return std::nullopt;\n   });\n \n   // If the origValue still has live user(s), use this to\n   // convert origValue to newValue\n   addSourceMaterialization([&](OpBuilder &builder, RankedTensorType tensorType,\n-                               ValueRange inputs, Location loc) {\n-    llvm_unreachable(\"Source rematerialization not implemented\");\n+                               ValueRange inputs,\n+                               Location loc) -> llvm::Optional<Value> {\n+    llvm_unreachable(\"Source rematerialization should not happen in Triton -> \"\n+                     \"TritonGPU Conversion\");\n     return std::nullopt;\n   });\n \n@@ -62,9 +64,6 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n     auto cast =\n         builder.create<triton::gpu::ConvertLayoutOp>(loc, tensorType, inputs);\n     return Optional<Value>(cast.getResult());\n-    // return Optional<Value>(cast.getResult(0));\n-    // llvm_unreachable(\"Not implemented\");\n-    // return std::nullopt;\n   });\n }\n \n@@ -82,10 +81,16 @@ TritonGPUConversionTarget::TritonGPUConversionTarget(\n                scf::ReduceReturnOp>();\n \n   addDynamicallyLegalDialect<arith::ArithDialect, math::MathDialect,\n-                             triton::TritonDialect, scf::SCFDialect>(\n+                             func::FuncDialect, triton::TritonDialect,\n+                             cf::ControlFlowDialect, scf::SCFDialect>(\n       [&](Operation *op) {\n-        if (typeConverter.isLegal(op))\n+        bool hasLegalRegions = true;\n+        for (auto &region : op->getRegions()) {\n+          hasLegalRegions = hasLegalRegions && typeConverter.isLegal(&region);\n+        }\n+        if (hasLegalRegions && typeConverter.isLegal(op)) {\n           return true;\n+        }\n         return false;\n       });\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -179,6 +179,8 @@ int simulateBackwardRematerialization(\n       if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n               triton::MakeRangeOp, triton::SplatOp>(*opArgI))\n         continue;\n+      if (isa<triton::ViewOp, triton::CatOp>(opArgI))\n+        continue;\n \n       // We add one expensive conversion for the current operand\n       numCvts += 1;\n@@ -229,6 +231,7 @@ void rematerializeConversionChain(\n     sortedValues.push_back(op->getResult(0));\n \n   for (Value currOperand : sortedValues) {\n+    Value origOperand = currOperand;\n     // unpack information\n     Attribute targetLayout = toConvert.lookup(currOperand);\n     // rematerialize the operand if necessary\n@@ -250,9 +253,9 @@ void rematerializeConversionChain(\n       newOperand->moveAfter(currOperation);\n     else {\n       Block *block = currOperand.cast<BlockArgument>().getOwner();\n-      newOperand->moveAfter(block, block->begin());\n+      newOperand->moveBefore(block, block->begin());\n     }\n-    mapping.map(currOperand, newOperand);\n+    mapping.map(origOperand, newOperand);\n   }\n }\n "}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -13,7 +13,6 @@\n #include \"mlir/Target/LLVMIR/Export.h\"\n #include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n #include \"mlir/Transforms/Passes.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Tools/Sys/GetEnv.hpp\"\n #include \"triton/Tools/Sys/GetPlatform.hpp\"\n@@ -273,7 +272,7 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module) {\n   }\n \n   auto optPipeline = mlir::makeOptimizingTransformer(\n-      /*optLevel=*/0, /*sizeLevel=*/0,\n+      /*optLevel=*/3, /*sizeLevel=*/0,\n       /*targetMachine=*/nullptr);\n \n   if (auto err = optPipeline(llvmModule.get())) {\n@@ -308,7 +307,6 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n       /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n   pm.addPass(mlir::createConvertSCFToCFPass());\n-  pm.addPass(createTritonConvertArithToIndexPass());\n   pm.addPass(mlir::createConvertIndexToLLVMPass());\n   pm.addPass(createConvertTritonGPUToLLVMPass(computeCapability));\n   pm.addPass(mlir::createArithToLLVMConversionPass());"}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 10, "deletions": 5, "changes": 15, "file_content_changes": "@@ -1,6 +1,5 @@\n #include \"triton/Target/PTX/PTXTranslation.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n-#include <optional>\n \n #include \"llvm/IR/IRBuilder.h\"\n #include \"llvm/IR/LegacyPassManager.h\"\n@@ -12,13 +11,19 @@\n #include \"llvm/Support/TargetSelect.h\"\n #include \"llvm/Target/TargetMachine.h\"\n \n+#include <mutex>\n+#include <optional>\n+\n namespace triton {\n \n static void initLLVM() {\n-  LLVMInitializeNVPTXTargetInfo();\n-  LLVMInitializeNVPTXTarget();\n-  LLVMInitializeNVPTXTargetMC();\n-  LLVMInitializeNVPTXAsmPrinter();\n+  static std::once_flag init_flag;\n+  std::call_once(init_flag, []() {\n+    LLVMInitializeNVPTXTargetInfo();\n+    LLVMInitializeNVPTXTarget();\n+    LLVMInitializeNVPTXTargetMC();\n+    LLVMInitializeNVPTXAsmPrinter();\n+  });\n }\n \n static bool findAndReplace(std::string &str, const std::string &begin,"}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -69,7 +69,7 @@ def get_llvm_package_info():\n     use_assert_enabled_llvm = check_env_flag(\"TRITON_USE_ASSERT_ENABLED_LLVM\", \"False\")\n     release_suffix = \"assert\" if use_assert_enabled_llvm else \"release\"\n     name = f'llvm+mlir-17.0.0-x86_64-{system_suffix}-{release_suffix}'\n-    version = \"llvm-17.0.0-8e5a41e8271f\"\n+    version = \"llvm-17.0.0-2538e550420f\"\n     url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n     return Package(\"llvm\", name, url, \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n "}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -56,7 +56,7 @@ def nvsmi(attrs):\n     'a100': {\n         (512, 512, 512): {'float16': 0.08, 'float32': 0.13, 'int8': 0.05},\n         (1024, 1024, 1024): {'float16': 0.33, 'float32': 0.35, 'int8': 0.169},\n-        (2048, 2048, 2048): {'float16': 0.59, 'float32': 0.57, 'int8': 0.34},\n+        (2048, 2048, 2048): {'float16': 0.62, 'float32': 0.57, 'int8': 0.34},\n         (4096, 4096, 4096): {'float16': 0.81, 'float32': 0.75, 'int8': 0.46},\n         (8192, 8192, 8192): {'float16': 0.77, 'float32': 0.85, 'int8': 0.51},\n         # tall-skinny"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 155, "deletions": 62, "changes": 217, "file_content_changes": "@@ -520,6 +520,17 @@ def test_unary_op(dtype_x, expr, device='cuda'):\n def test_math_op(expr, device='cuda'):\n     _test_unary('float32', f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n \n+# ----------------\n+# test abs\n+# ----------------\n+\n+\n+@pytest.mark.parametrize(\"dtype_x\", [\n+    (dtype_x)\n+    for dtype_x in dtypes_with_bfloat16\n+])\n+def test_abs(dtype_x, device='cuda'):\n+    _test_unary(dtype_x, 'tl.abs(x)', 'np.abs(x) ', device=device)\n \n # ----------------\n # test indexing\n@@ -874,6 +885,52 @@ def kernel(in_out_ptr):\n         assert torch.all(x == 2)\n \n \n+def convert_float_to_float32(fp: torch.tensor, dtype=None):\n+    if not dtype:\n+        dtype = getattr(tl, torch_dtype_name(fp.dtype))\n+\n+    fp = fp.view(getattr(torch, f\"int{dtype.primitive_bitwidth}\"))\n+    exp_width = dtype.primitive_bitwidth - dtype.fp_mantissa_width - 1\n+    exp_bias = 2 ** (exp_width - 1) - 1\n+    sign = ((fp >> (dtype.primitive_bitwidth - 1)) & 0x01).int()\n+    exp = ((fp >> dtype.fp_mantissa_width) & ((1 << exp_width) - 1)).int()\n+    frac = (fp & ((1 << dtype.fp_mantissa_width) - 1)).int()\n+\n+    output = torch.where(exp == 0,\n+                         # subnormal\n+                         ((-1.0) ** sign) * (2.0 ** (1 - exp_bias)) * (frac / (2.0 ** dtype.fp_mantissa_width)),\n+                         # normal\n+                         ((-1.0) ** sign) * (2.0 ** (exp - exp_bias)) * (1.0 + frac / (2.0 ** dtype.fp_mantissa_width))).float()\n+\n+    extended_exp = ((1 << (tl.float32.primitive_bitwidth - tl.float32.fp_mantissa_width - 1)) - 1) << tl.float32.fp_mantissa_width\n+    # special cases, exp is 0b11..1\n+    if dtype == tl.float8e4:\n+        # float8e4m3 does not have infinities\n+        output[fp == torch.tensor(0b01111111, dtype=torch.int8)] = torch.nan\n+        output[fp == torch.tensor(0b11111111, dtype=torch.int8)] = torch.nan\n+    else:\n+        output = torch.where(exp == (1 << exp_width) - 1,\n+                             ((sign << (tl.float32.primitive_bitwidth - 1)) | extended_exp | (frac << (tl.float32.fp_mantissa_width - dtype.fp_mantissa_width))).view(torch.float32),\n+                             output)\n+    return output\n+\n+\n+@pytest.mark.parametrize(\"in_dtype\", [torch.float16, torch.bfloat16])\n+def test_convert_float16_to_float32(in_dtype):\n+    \"\"\"Tests that check convert_float_to_float32 function\"\"\"\n+    check_type_supported(in_dtype)\n+\n+    f16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16).view(in_dtype)\n+    f32_output = convert_float_to_float32(f16_input)\n+\n+    nan = f16_input.isnan()\n+    assert torch.all(f32_output[nan].isnan())\n+    inf = f16_input.isinf()\n+    assert torch.all(f32_output[inf].isinf())\n+    other = torch.logical_not(torch.logical_or(nan, inf))\n+    assert torch.all(f16_input[other] == f32_output[other])\n+\n+\n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.float32])\n def test_f8_xf16_roundtrip(in_dtype, out_dtype):\n@@ -898,6 +955,14 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     copy_kernel[grid](f8, xf16, n_elements, BLOCK_SIZE=1024)\n \n+    # exponent_mask = 0b01111100 for float8e5\n+    # exponent_mask = 0b01111000 for float8e4\n+    exponent_mask = 0b01111111 ^ ((1 << in_dtype.fp_mantissa_width) - 1)\n+    normal = torch.logical_and((f8_tensor & exponent_mask) != 0, (f8_tensor & exponent_mask) != exponent_mask)\n+    ref16 = convert_float_to_float32(f8_tensor, in_dtype)\n+    # WARN: currently only normal float8s are handled\n+    assert torch.all(xf16[normal] == ref16[normal])\n+\n     f8_output_tensor = torch.empty_like(xf16, dtype=torch.int8)\n     f8_output = triton.reinterpret(f8_output_tensor, in_dtype)\n     copy_kernel[grid](xf16, f8_output, n_elements, BLOCK_SIZE=1024)\n@@ -906,7 +971,8 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4])\n-def test_f16_to_f8_rounding(in_dtype):\n+@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16])\n+def test_f16_to_f8_rounding(in_dtype, out_dtype):\n     \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n     error is the minimum over all float8.\n     Or the same explanation a bit mathier:\n@@ -919,28 +985,22 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         output = input\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n-    # torch.view with a dtype isn't supported in triton's torch yet so use numpy's view\n-    f16_input_np = (\n-        np.array(\n-            range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=np.int16,\n-        )\n-        .view(np.float16)\n-    )\n-    f16_input = torch.tensor(f16_input_np, dtype=torch.float16, device='cuda')\n+    i16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16, device='cuda')\n+    f16_input = i16_input.view(out_dtype)\n     n_elements = f16_input.numel()\n     f8_output_tensor = torch.empty_like(f16_input, dtype=torch.int8)\n     f8_output = triton.reinterpret(f8_output_tensor, in_dtype)\n     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     copy_kernel[grid](f16_input, f8_output, n_elements, BLOCK_SIZE=1024)\n \n-    f16_output = torch.empty_like(f16_input, dtype=torch.float16)\n+    f16_output = torch.empty_like(f16_input, dtype=out_dtype)\n     copy_kernel[grid](f8_output, f16_output, n_elements, BLOCK_SIZE=1024)\n \n     abs_error = torch.abs(f16_input - f16_output)\n \n     all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device='cuda')\n     all_f8_vals = triton.reinterpret(all_f8_vals_tensor, in_dtype)\n-    all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=torch.float16)\n+    all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=out_dtype)\n     copy_kernel[grid](all_f8_vals, all_f8_vals_in_f16, n_elements=256, BLOCK_SIZE=1024)\n \n     all_finite_f8_vals_in_f16 = all_f8_vals_in_f16[\n@@ -954,9 +1014,14 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         ),\n         dim=1,\n     )[0]\n-    # 1.9375 is float8 max\n+\n+    # WARN: only normalized numbers are handled\n+    f8_normal_min = 1 << in_dtype.fp_mantissa_width  # 0b00001000 for float8e4\n+    f8_normal_max = 0b01111110\n+    f16_min, f16_max, f16_max_minus_1 = convert_float_to_float32(torch.tensor([f8_normal_min, f8_normal_max, f8_normal_max - 1], dtype=torch.int8), in_dtype)\n+    thres_error = f16_max - f16_max_minus_1\n     mismatch = torch.logical_and(\n-        abs_error != min_error, torch.logical_and(torch.isfinite(f16_input), torch.abs(f16_input) < 1.9375)\n+        torch.logical_or(abs_error != min_error, abs_error > thres_error), torch.logical_and(torch.isfinite(f16_input), torch.logical_and(torch.abs(f16_input) <= f16_max, torch.abs(f16_input) >= f16_min))\n     )\n     assert torch.all(\n         torch.logical_not(mismatch)\n@@ -1162,15 +1227,17 @@ def kernel(X, stride_xm, stride_xn,\n # ---------------\n \n \n-@pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype\",\n-                         [(*shape, 4, False, False, epilogue, allow_tf32, dtype)\n+@pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype\",\n+                         [(*shape, 4, False, False, epilogue, allow_tf32, in_dtype, out_dtype)\n                           for shape in [(64, 64, 64), (16, 16, 16)]\n                           for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n                           for allow_tf32 in [True, False]\n-                          for dtype in ['float16', 'float32']\n-                          if not (allow_tf32 and (dtype in ['float16']))] +\n+                          for in_dtype, out_dtype in [('float16', 'float16'),\n+                                                      ('float16', 'float32'),\n+                                                      ('float32', 'float32')]\n+                          if not (allow_tf32 and (in_dtype in ['float16']))] +\n \n-                         [(*shape_nw, col_a, col_b, 'none', allow_tf32, dtype)\n+                         [(*shape_nw, col_a, col_b, 'none', allow_tf32, in_dtype, out_dtype)\n                           for shape_nw in [[128, 256, 32, 8],\n                                            [128, 16, 32, 4],\n                                            [32, 128, 64, 4],\n@@ -1183,19 +1250,25 @@ def kernel(X, stride_xm, stride_xn,\n                           for allow_tf32 in [True]\n                           for col_a in [True, False]\n                           for col_b in [True, False]\n-                          for dtype in ['int8', 'float16', 'float32']])\n-def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype, device='cuda'):\n+                          for in_dtype, out_dtype in [('int8', 'int8'),\n+                                                      ('float16', 'float16'),\n+                                                      ('float16', 'float32'),\n+                                                      ('float32', 'float32')]])\n+def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype, device='cuda'):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if capability[0] < 8:\n-        if dtype == 'int8':\n+        if in_dtype == 'int8':\n             pytest.skip(\"Only test int8 on devices with sm >= 80\")\n-        elif dtype == 'float32' and allow_tf32:\n+        elif in_dtype == 'float32' and allow_tf32:\n             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n     if capability[0] == 7:\n         if (M, N, K, num_warps) == (128, 256, 32, 8):\n             pytest.skip(\"shared memory out of resource\")\n+        if out_dtype == 'float16':\n+            # TODO: support out_dtype=float16 for tl.dot on V100\n+            pytest.skip(\"Only test out_dtype=float16 on devices with sm >=80\")\n \n     torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n \n@@ -1205,6 +1278,7 @@ def kernel(X, stride_xm, stride_xk,\n                Y, stride_yk, stride_yn,\n                W, stride_wn, stride_wl,\n                Z, stride_zm, stride_zn,\n+               out_dtype: tl.constexpr,\n                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n                ALLOW_TF32: tl.constexpr,\n@@ -1220,7 +1294,7 @@ def kernel(X, stride_xm, stride_xk,\n         Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n         x = tl.load(Xs)\n         y = tl.load(Ys)\n-        z = tl.dot(x, y, allow_tf32=ALLOW_TF32)\n+        z = tl.dot(x, y, allow_tf32=ALLOW_TF32, out_dtype=out_dtype)\n         if ADD_MATRIX:\n             z += tl.load(Zs)\n         if ADD_ROWS:\n@@ -1237,42 +1311,54 @@ def kernel(X, stride_xm, stride_xk,\n             z = num / den[:, None]\n         if CHAIN_DOT:\n             w = tl.load(Ws)\n-            z = tl.dot(z.to(w.dtype), w)\n+            z = tl.dot(z.to(w.dtype), w, out_dtype=out_dtype)\n         tl.store(Zs, z)\n     # input\n     rs = RandomState(17)\n     if col_a:\n-        x = numpy_random((K, M), dtype_str=dtype, rs=rs).T\n+        x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T\n     else:\n-        x = numpy_random((M, K), dtype_str=dtype, rs=rs)\n+        x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)\n     if col_b:\n-        y = numpy_random((N, K), dtype_str=dtype, rs=rs).T\n+        y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T\n     else:\n-        y = numpy_random((K, N), dtype_str=dtype, rs=rs)\n-    w = numpy_random((N, N), dtype_str=dtype, rs=rs)\n-    if 'int' not in dtype:\n+        y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)\n+    w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)\n+    if 'int' not in in_dtype:\n         x *= .1\n         y *= .1\n-    if dtype == 'float32' and allow_tf32:\n+    if in_dtype == 'float32' and allow_tf32:\n         x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n         y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n         w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n     x_tri = to_triton(x, device=device)\n     y_tri = to_triton(y, device=device)\n     w_tri = to_triton(w, device=device)\n     # triton result\n-    if dtype == 'int8':\n+    if out_dtype == 'int8':\n         z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)\n     else:\n-        z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs) * .1\n+        z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1\n \n     z_tri = to_triton(z, device=device)\n     if epilogue == 'trans':\n         z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])\n+\n+    if out_dtype == 'int8':\n+        out_dtype = tl.int8\n+    elif out_dtype == 'float16' and epilogue != 'softmax':\n+        # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will\n+        # fail with the following error: 'llvm.fmul' op requires the same type\n+        # for all operands and results\n+        out_dtype = tl.float16\n+    else:\n+        out_dtype = tl.float32\n+\n     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n                          y_tri, y_tri.stride(0), y_tri.stride(1),\n                          w_tri, w_tri.stride(0), w_tri.stride(1),\n                          z_tri, z_tri.stride(0), z_tri.stride(1),\n+                         out_dtype,\n                          COL_A=col_a, COL_B=col_b,\n                          BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n                          ADD_MATRIX=epilogue == 'add-matrix',\n@@ -1283,7 +1369,7 @@ def kernel(X, stride_xm, stride_xk,\n                          ALLOW_TF32=allow_tf32,\n                          num_warps=num_warps)\n     # torch result\n-    if dtype == 'int8':\n+    if in_dtype == 'int8':\n         z_ref = np.matmul(x.astype(np.float32),\n                           y.astype(np.float32())).astype(np.int32)\n     else:\n@@ -1303,9 +1389,11 @@ def kernel(X, stride_xm, stride_xk,\n         z_ref = np.matmul(z_ref, w)\n     # compare\n     # print(z_ref[:,0], z_tri[:,0])\n-    if dtype == 'float32':\n+    if in_dtype == 'float32':\n         # XXX: Somehow there's a larger difference when we use float32\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n+    elif out_dtype == tl.float16:\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n     else:\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n     # make sure ld/st are vectorized\n@@ -1314,12 +1402,14 @@ def kernel(X, stride_xm, stride_xk,\n         # XXX: skip small sizes because they are not vectorized\n         assert 'ld.global.v4' in ptx\n         assert 'st.global.v4' in ptx\n-    if dtype == 'float32' and allow_tf32:\n+    if in_dtype == 'float32' and allow_tf32:\n         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n-    elif dtype == 'float32' and allow_tf32:\n+    elif in_dtype == 'float32' and allow_tf32:\n         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' not in ptx\n-    elif dtype == 'int8':\n+    elif in_dtype == 'int8':\n         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n+    elif out_dtype == tl.float16:\n+        assert 'mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16' in ptx\n \n \n @pytest.mark.parametrize(\"dtype_str\", int_dtypes + float_dtypes + ['bfloat16'])\n@@ -1456,7 +1546,7 @@ def _kernel(in1_ptr, in2_ptr, output_ptr,\n         w = tl.load(in2_ptr + in2_offsets, mask=in2_offsets < in2_numel)\n \n         # Without a dot product the memory doesn't get promoted to shared.\n-        o = tl.dot(x, w)\n+        o = tl.dot(x, w, out_dtype=tl.float32)\n \n         # Store output\n         output_offsets = M_offsets[:, None] * out_stride + N_offsets[None, :]\n@@ -1791,11 +1881,11 @@ def _kernel(dst):\n \n \n @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n-                         [('int32', 'libdevice.ffs', ''),\n-                          ('float32', 'libdevice.log2', ''),\n-                          ('float32', 'libdevice.pow', tl.libdevice.LIBDEVICE_PATH),\n-                          ('float64', 'libdevice.norm4d', '')])\n-def test_libdevice_tensor(dtype_str, expr, lib_path):\n+                         [('int32', 'math.ffs', ''),\n+                          ('float32', 'math.log2', ''),\n+                          ('float32', 'math.pow', tl.math.LIBDEVICE_PATH),\n+                          ('float64', 'math.norm4d', '')])\n+def test_math_tensor(dtype_str, expr, lib_path):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -1808,37 +1898,37 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n \n-    if expr == 'libdevice.log2':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.broadcast_to(tl.libdevice.log2(5.0), x.shape)'})\n+    if expr == 'math.log2':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.broadcast_to(tl.math.log2(5.0), x.shape)'})\n         y_ref = np.log2(5.0)\n-    elif expr == 'libdevice.ffs':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.ffs(x)'})\n+    elif expr == 'math.ffs':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.ffs(x)'})\n         y_ref = np.zeros(shape, dtype=x.dtype)\n         for i in range(shape[0]):\n             y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n-    elif expr == 'libdevice.pow':\n+    elif expr == 'math.pow':\n         # numpy does not allow negative factors in power, so we use abs()\n         x = np.abs(x)\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.pow(x, x)'})\n         y_ref = np.power(x, x)\n-    elif expr == 'libdevice.norm4d':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.norm4d(x, x, x, x)'})\n+    elif expr == 'math.norm4d':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.norm4d(x, x, x, x)'})\n         y_ref = np.sqrt(4 * np.power(x, 2))\n \n     x_tri = to_triton(x)\n     # triton result\n     y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n     # compare\n-    if expr == 'libdevice.ffs':\n+    if expr == 'math.ffs':\n         np.testing.assert_equal(y_ref, to_numpy(y_tri))\n     else:\n         np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n \n \n @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n-                         [('float32', 'libdevice.pow', '')])\n-def test_libdevice_scalar(dtype_str, expr, lib_path):\n+                         [('float32', 'math.pow', '')])\n+def test_math_scalar(dtype_str, expr, lib_path):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -1854,13 +1944,13 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n \n     # numpy does not allow negative factors in power, so we use abs()\n     x = np.abs(x)\n-    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.pow(x, x)'})\n     y_ref[:] = np.power(x, x)\n \n     # triton result\n     x_tri = to_triton(x)[0].item()\n     y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n-    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'math': lib_path})\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n \n@@ -1869,21 +1959,24 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n # -----------------------\n \n \n-def test_for_iv_int64():\n+@pytest.mark.parametrize(\"lo, hi, iv\", [(2**35, 2**35 + 20, 1), (2**35, 2**35 + 20, 2), (2**35, 2**35 + 20, 3),\n+                                        (15, -16, -1), (15, -16, -2), (15, -16, -3),\n+                                        (-18, -22, -1), (22, 18, -1)])\n+def test_for_iv(lo, hi, iv):\n \n     @triton.jit\n-    def kernel(Out, lo, hi):\n+    def kernel(Out, lo, hi, iv: tl.constexpr):\n         acc = 0\n         acc = acc.to(tl.int64)\n-        for i in range(lo, hi):\n+        for i in range(lo, hi, iv):\n             acc += i\n         tl.store(Out, acc)\n \n     lo = 2**35\n     hi = 2**35 + 20\n     out = to_triton(np.zeros((1,), dtype=np.int64), device='cuda')\n-    kernel[(1,)](out, lo, hi)\n-    assert out[0] == sum(range(lo, hi))\n+    kernel[(1,)](out, lo, hi, iv)\n+    assert out[0] == sum(range(lo, hi, iv))\n \n \n def test_if_else():"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 124, "deletions": 125, "changes": 249, "file_content_changes": "@@ -16,7 +16,7 @@\n import warnings\n from collections import namedtuple\n from pathlib import Path\n-from typing import Any, Callable, Dict, Optional, Tuple, Union\n+from typing import Any, Callable, Dict, Optional, Tuple, Type, Union\n \n import setuptools\n import torch\n@@ -97,6 +97,21 @@ def mangle_fn(name, arg_tys, constants):\n     return ret\n \n \n+def _is_triton_tensor(o: Any) -> bool:\n+    return isinstance(o, triton.language.tensor)\n+\n+\n+def _is_constexpr(o: Any) -> bool:\n+    return isinstance(o, triton.language.constexpr)  # TODO: fetch triton.language.constexpr to a global after circular imports untangled, saving getattr\n+\n+\n+def _unwrap_if_constexpr(o: Any):\n+    return o.value if isinstance(o, triton.language.constexpr) else o\n+\n+\n+_condition_types = {bool, int}  # Python types accepted for conditionals inside kernels\n+\n+\n class enter_sub_region:\n     def __init__(self, generator: CodeGenerator):\n         self.generator = generator\n@@ -158,10 +173,10 @@ def local_lookup(name: str, absent):\n                 self.global_uses[name] = value\n             return value\n \n-        lookup_order = local_lookup, self.gscope.get, self.builtin_namespace.get\n         absent_marker = object()\n \n         def name_lookup(name: str) -> Any:\n+            lookup_order = local_lookup, self.gscope.get, self.builtin_namespace.get\n             absent = absent_marker\n             for lookup_function in lookup_order:\n                 value = lookup_function(name, absent)\n@@ -181,9 +196,6 @@ def set_value(self, name: str,\n         self.lscope[name] = value\n         self.local_defs[name] = value\n \n-    def is_triton_tensor(self, value):\n-        return isinstance(value, triton.language.tensor)\n-\n     #\n     # AST visitor\n     #\n@@ -209,7 +221,11 @@ def contains_return_op(self, node):\n         elif isinstance(node, ast.Call):\n             fn = self.visit(node.func)\n             if isinstance(fn, triton.JITFunction):\n-                return self.contains_return_op(fn.parse())\n+                old_gscope = self.gscope\n+                self.gscope = sys.modules[fn.fn.__module__].__dict__\n+                ret = self.contains_return_op(fn.parse())\n+                self.gscope = old_gscope\n+                return ret\n             return False\n         elif isinstance(node, ast.If):\n             pred = lambda s: self.contains_return_op(s)\n@@ -275,7 +291,7 @@ def visit_FunctionDef(self, node):\n         for i, arg_name in enumerate(arg_names):\n             if i in self.constants:\n                 cst = self.constants[i]\n-                if not isinstance(cst, triton.language.constexpr):\n+                if not _is_constexpr(cst):\n                     cst = triton.language.constexpr(self.constants[i])\n                 arg_values.append(cst)\n                 continue\n@@ -326,7 +342,7 @@ def visit_AnnAssign(self, node):\n             if target in self.lscope:\n                 raise ValueError(f'{target} is already defined.'\n                                  f' constexpr cannot be reassigned.')\n-            if not isinstance(value, triton.language.constexpr):\n+            if not _is_constexpr(value):\n                 value = triton.language.constexpr(value)\n             self.lscope[target] = value\n             return self.lscope[target]\n@@ -338,18 +354,19 @@ def visit_Assign(self, node):\n         for target in node.targets:\n             _names += [self.visit(target)]\n         if len(_names) > 1:\n-            raise NotImplementedError(\"Multiple assignment is not supported.\")\n+            raise UnsupportedLanguageConstruct(None, node, \"simultaneous multiple assignment is not supported.\")\n         names = _names[0]\n         values = self.visit(node.value)\n         if not isinstance(names, tuple):\n             names = [names]\n         if not isinstance(values, tuple):\n             values = [values]\n+        native_nontensor_types = (triton.language.dtype, )\n         for name, value in zip(names, values):\n             # by default, constexpr are assigned into python variable\n-            if isinstance(value, triton.language.constexpr):\n-                value = value.value\n-            if not isinstance(value, triton.language.tensor):\n+            value = _unwrap_if_constexpr(value)\n+            if not _is_triton_tensor(value) and \\\n+               not isinstance(value, native_nontensor_types):\n                 value = triton.language.core._to_tensor(value, self.builder)\n             self.set_value(name, value)\n \n@@ -376,30 +393,27 @@ def visit_Tuple(self, node):\n         args = [self.visit(x) for x in node.elts]\n         return tuple(args)\n \n+    def _apply_binary_method(self, method_name, lhs, rhs):\n+        # TODO: raise something meaningful if getattr fails below, esp for reverse method\n+        if _is_triton_tensor(lhs):\n+            return getattr(lhs, method_name)(rhs, _builder=self.builder)\n+        if _is_triton_tensor(rhs):\n+            reverse_method_name = re.sub(r\"__(.*)__\", r\"__r\\1__\", method_name)\n+            return getattr(rhs, reverse_method_name)(lhs, _builder=self.builder)\n+        return getattr(lhs, method_name)(rhs)\n+\n     def visit_BinOp(self, node):\n         lhs = self.visit(node.left)\n         rhs = self.visit(node.right)\n-        fn = {\n-            ast.Add: '__add__',\n-            ast.Sub: '__sub__',\n-            ast.Mult: '__mul__',\n-            ast.Div: '__truediv__',\n-            ast.FloorDiv: '__floordiv__',\n-            ast.Mod: '__mod__',\n-            ast.Pow: '__pow__',\n-            ast.LShift: '__lshift__',\n-            ast.RShift: '__rshift__',\n-            ast.BitAnd: '__and__',\n-            ast.BitOr: '__or__',\n-            ast.BitXor: '__xor__',\n-        }[type(node.op)]\n-        if self.is_triton_tensor(lhs):\n-            return getattr(lhs, fn)(rhs, _builder=self.builder)\n-        elif self.is_triton_tensor(rhs):\n-            fn = fn[:2] + 'r' + fn[2:]\n-            return getattr(rhs, fn)(lhs, _builder=self.builder)\n-        else:\n-            return getattr(lhs, fn)(rhs)\n+        method_name = self._method_name_for_bin_op.get(type(node.op))\n+        if method_name is None:\n+            raise UnsupportedLanguageConstruct(None, node, \"AST binary operator '{}' is not (currently) implemented.\".format(node.op.__name__))\n+        return self._apply_binary_method(method_name, lhs, rhs)\n+    _method_name_for_bin_op: Dict[Type[ast.operator], str] = {\n+        ast.Add: '__add__', ast.Sub: '__sub__', ast.Mult: '__mul__', ast.Div: '__truediv__',\n+        ast.FloorDiv: '__floordiv__', ast.Mod: '__mod__', ast.Pow: '__pow__',\n+        ast.LShift: '__lshift__', ast.RShift: '__rshift__', ast.BitAnd: '__and__', ast.BitOr: '__or__', ast.BitXor: '__xor__',\n+    }\n \n     def visit_then_else_blocks(self, node, liveins, then_block, else_block):\n         # then block\n@@ -515,15 +529,18 @@ def visit_if_scf(self, cond, node):\n \n     def visit_If(self, node):\n         cond = self.visit(node.test)\n-        if isinstance(cond, triton.language.tensor):\n+        if _is_triton_tensor(cond):\n             cond = cond.to(triton.language.int1, _builder=self.builder)\n             if self.scf_stack or not self.contains_return_op(node):\n                 self.visit_if_scf(cond, node)\n             else:\n                 self.visit_if_top_level(cond, node)\n         else:\n-            if isinstance(cond, triton.language.constexpr):\n-                cond = cond.value\n+            cond = _unwrap_if_constexpr(cond)\n+            if type(cond) not in _condition_types:  # not isinstance - we insist the real thing, no subclasses and no ducks\n+                raise UnsupportedLanguageConstruct(\n+                    None, node, \"`if` conditionals can only accept values of type {{{}}}, not objects of type {}\".format(\n+                        ', '.join(_.__name__ for _ in _condition_types), type(cond).__name__))\n             if cond:\n                 self.visit_compound_statement(node.body)\n             else:\n@@ -540,45 +557,31 @@ def visit_Pass(self, node):\n         pass\n \n     def visit_Compare(self, node):\n-        assert len(node.comparators) == 1\n-        assert len(node.ops) == 1\n-        lhs = self.visit(node.left)\n-        rhs = self.visit(node.comparators[0])\n-        if isinstance(lhs, triton.language.constexpr):\n-            lhs = lhs.value\n-        if isinstance(rhs, triton.language.constexpr):\n-            rhs = rhs.value\n+        if not (len(node.comparators) == 1 and len(node.ops) == 1):\n+            raise UnsupportedLanguageConstruct(None, node, \"simultaneous multiple comparison is not supported\")\n+        lhs = _unwrap_if_constexpr(self.visit(node.left))\n+        rhs = _unwrap_if_constexpr(self.visit(node.comparators[0]))\n         if type(node.ops[0]) == ast.Is:\n             return triton.language.constexpr(lhs is rhs)\n         if type(node.ops[0]) == ast.IsNot:\n             return triton.language.constexpr(lhs is not rhs)\n-        fn = {\n-            ast.Eq: '__eq__',\n-            ast.NotEq: '__ne__',\n-            ast.Lt: '__lt__',\n-            ast.LtE: '__le__',\n-            ast.Gt: '__gt__',\n-            ast.GtE: '__ge__',\n-        }[type(node.ops[0])]\n-        if self.is_triton_tensor(lhs):\n-            return getattr(lhs, fn)(rhs, _builder=self.builder)\n-        elif self.is_triton_tensor(rhs):\n-            fn = fn[:2] + 'r' + fn[2:]\n-            return getattr(rhs, fn)(lhs, _builder=self.builder)\n-        else:\n-            return getattr(lhs, fn)(rhs)\n+        method_name = self._method_name_for_comp_op.get(type(node.ops[0]))\n+        if method_name is None:\n+            raise UnsupportedLanguageConstruct(None, node, \"AST comparison operator '{}' is not (currently) implemented.\".format(node.ops[0].__name__))\n+        return self._apply_binary_method(method_name, lhs, rhs)\n+    _method_name_for_comp_op: Dict[Type[ast.cmpop], str] = {\n+        ast.Eq: '__eq__', ast.NotEq: '__ne__', ast.Lt: '__lt__', ast.LtE: '__le__', ast.Gt: '__gt__', ast.GtE: '__ge__'\n+    }\n \n     def visit_UnaryOp(self, node):\n         op = self.visit(node.operand)\n-        fn = {\n-            ast.USub: '__neg__',\n-            ast.UAdd: '__pos__',\n-            ast.Not: '__not__',\n-            ast.Invert: '__invert__',\n-        }[type(node.op)]\n-        if self.is_triton_tensor(op):\n+        fn = self._method_name_for_unary_op.get(type(node.op))\n+        if fn is None:\n+            raise UnsupportedLanguageConstruct(None, node, \"AST unary operator '{}' is not (currently) implemented.\".format(node.op.__name__))\n+        if _is_triton_tensor(op):\n             return getattr(op, fn)(_builder=self.builder)\n         return getattr(op, fn)()\n+    _method_name_for_unary_op: Dict[Type[ast.unaryop], str] = {ast.USub: '__neg__', ast.UAdd: '__pos__', ast.Not: '__not__', ast.Invert: '__invert__'}\n \n     def visit_While(self, node):\n         with enter_sub_region(self) as sr:\n@@ -600,8 +603,8 @@ def visit_While(self, node):\n             for name in loop_defs:\n                 if name in liveins:\n                     # We should not def new constexpr\n-                    assert self.is_triton_tensor(loop_defs[name])\n-                    assert self.is_triton_tensor(liveins[name])\n+                    assert _is_triton_tensor(loop_defs[name])\n+                    assert _is_triton_tensor(liveins[name])\n                     assert loop_defs[name].type == liveins[name].type\n                     # these are loop-carried values\n                     names.append(name)\n@@ -659,7 +662,7 @@ def visit_Subscript(self, node):\n         assert node.ctx.__class__.__name__ == \"Load\"\n         lhs = self.visit(node.value)\n         slices = self.visit(node.slice)\n-        if self.is_triton_tensor(lhs):\n+        if _is_triton_tensor(lhs):\n             return lhs.__getitem__(slices, _builder=self.builder)\n         return lhs[slices]\n \n@@ -692,7 +695,7 @@ def visit_For(self, node):\n         step = iter_args[2] if len(iter_args) > 2 else self.visit(ast.Num(1))\n         # handle negative constant step (not supported by scf.for in MLIR)\n         negative_step = False\n-        if isinstance(step, triton.language.constexpr) and step.value < 0:\n+        if _is_constexpr(step) and step.value < 0:\n             step = triton.language.constexpr(-step.value)\n             negative_step = True\n             lb, ub = ub, lb\n@@ -703,14 +706,15 @@ def visit_For(self, node):\n         iv_type = triton.language.semantic.integer_promote_impl(lb.dtype, ub.dtype)\n         iv_type = triton.language.semantic.integer_promote_impl(iv_type, step.dtype)\n         iv_ir_type = iv_type.to_ir(self.builder)\n+        iv_is_signed = iv_type.int_signedness == triton.language.core.dtype.SIGNEDNESS.SIGNED\n         # lb/ub/step might be constexpr, we need to cast them to tensor\n         lb = lb.handle\n         ub = ub.handle\n         step = step.handle\n         # ForOp can only accept IndexType as lb/ub/step. Cast integer to Index\n-        lb = self.builder.create_to_index(lb)\n-        ub = self.builder.create_to_index(ub)\n-        step = self.builder.create_to_index(step)\n+        lb = self.builder.create_int_cast(lb, iv_ir_type, iv_is_signed)\n+        ub = self.builder.create_int_cast(ub, iv_ir_type, iv_is_signed)\n+        step = self.builder.create_int_cast(step, iv_ir_type, iv_is_signed)\n         # Create placeholder for the loop induction variable\n         iv = self.builder.create_undef(iv_ir_type)\n         self.set_value(node.target.id, triton.language.core.tensor(iv, iv_type))\n@@ -735,8 +739,8 @@ def visit_For(self, node):\n             names = []\n             for name in self.local_defs:\n                 if name in liveins:\n-                    assert self.is_triton_tensor(self.local_defs[name]), f'{name} is not tensor'\n-                    assert self.is_triton_tensor(liveins[name])\n+                    assert _is_triton_tensor(self.local_defs[name]), f'{name} is not tensor'\n+                    assert _is_triton_tensor(liveins[name])\n                     assert self.local_defs[name].type == liveins[name].type,\\\n                         f'Loop-carried variable {name} has initial type {liveins[name].type} '\\\n                         f'but is re-assigned to {self.local_defs[name].type} in loop! '\\\n@@ -769,12 +773,10 @@ def visit_For(self, node):\n \n             # update induction variable with actual value, and replace all uses\n             self.builder.set_insertion_point_to_start(for_op.get_body(0))\n-            iv = self.builder.create_index_to_si(for_op.get_induction_var())\n-            iv = self.builder.create_int_cast(iv, iv_ir_type, True)\n+            iv = for_op.get_induction_var()\n             if negative_step:\n-                ub_si = self.builder.create_index_to_si(ub)\n-                ub_si = self.builder.create_int_cast(ub_si, iv_ir_type, True)\n-                iv = self.builder.create_sub(ub_si, iv)\n+                iv = self.builder.create_sub(ub, iv)\n+                iv = self.builder.create_add(iv, lb)\n             self.lscope[node.target.id].handle.replace_all_uses_with(iv)\n             self.set_value(node.target.id, triton.language.core.tensor(iv, iv_type))\n \n@@ -807,9 +809,7 @@ def visit_Assert(self, node) -> Any:\n         return triton.language.core.device_assert(test, msg, _builder=self.builder)\n \n     def visit_Call(self, node):\n-        fn = self.visit(node.func)\n-        if isinstance(fn, triton.language.constexpr):\n-            fn = fn.value\n+        fn = _unwrap_if_constexpr(self.visit(node.func))\n \n         static_implementation = self.statically_implemented_functions.get(fn)\n         if static_implementation is not None:\n@@ -824,11 +824,11 @@ def visit_Call(self, node):\n             from inspect import getcallargs\n             args = getcallargs(fn.fn, *args, **kws)\n             args = [args[name] for name in fn.arg_names]\n-            args = [arg if isinstance(arg, triton.language.tensor)\n+            args = [arg if _is_triton_tensor(arg)\n                     else triton.language.constexpr(arg) for arg in args]\n             # generate function def\n             attributes = dict()\n-            constexprs = [i for i, arg in enumerate(args) if isinstance(arg, triton.language.constexpr)]\n+            constexprs = [i for i, arg in enumerate(args) if _is_constexpr(arg)]\n             constants = {i: args[i] for i in constexprs}\n             # generate call\n             args = [None if i in constexprs else arg for i, arg in enumerate(args)]\n@@ -857,32 +857,25 @@ def visit_Call(self, node):\n                 for i in range(call_op.get_num_results()):\n                     results.append(triton.language.tensor(call_op.get_result(i), callee_ret_type[i]))\n                 return tuple(results)\n-        if (hasattr(fn, '__self__') and self.is_triton_tensor(fn.__self__)) or impl.is_builtin(fn):\n+        if (hasattr(fn, '__self__') and _is_triton_tensor(fn.__self__)) or impl.is_builtin(fn):\n             return fn(*args, _builder=self.builder, **kws)\n         if fn in self.builtin_namespace.values():\n-            args = [arg.value if isinstance(arg, triton.language.constexpr) else arg for arg in args]\n+            args = map(_unwrap_if_constexpr, args)\n         return fn(*args, **kws)\n \n     def visit_Constant(self, node):\n         return triton.language.constexpr(node.value)\n \n     def visit_BoolOp(self, node: ast.BoolOp):\n-        assert len(node.values) == 2\n+        if len(node.values) != 2:\n+            raise UnsupportedLanguageConstruct(None, node, \"chained boolean operators (A or B or C) are not supported; use parentheses to split the chain.\")\n         lhs = self.visit(node.values[0])\n         rhs = self.visit(node.values[1])\n-\n-        fn = {\n-            ast.And: 'logical_and',\n-            ast.Or: 'logical_or',\n-        }[type(node.op)]\n-\n-        if self.is_triton_tensor(lhs):\n-            return getattr(lhs, fn)(rhs, _builder=self.builder)\n-        elif self.is_triton_tensor(rhs):\n-            fn = fn[:2] + 'r' + fn[2:]\n-            return getattr(rhs, fn)(lhs, _builder=self.builder)\n-        else:\n-            return getattr(lhs, fn)(rhs)\n+        method_name = self._method_name_for_bool_op.get(type(node.op))\n+        if method_name is None:\n+            raise UnsupportedLanguageConstruct(None, node, \"AST boolean operator '{}' is not (currently) implemented.\".format(node.op.__name__))\n+        return self._apply_binary_method(method_name, lhs, rhs)\n+    _method_name_for_bool_op: Dict[Type[ast.boolop], str] = {ast.And: 'logical_and', ast.Or: 'logical_or'}\n \n     if sys.version_info < (3, 8):\n         def visit_NameConstant(self, node):\n@@ -896,7 +889,7 @@ def visit_Str(self, node):\n \n     def visit_Attribute(self, node):\n         lhs = self.visit(node.value)\n-        if isinstance(lhs, triton.language.tensor):\n+        if _is_triton_tensor(lhs):\n             if node.attr == \"T\":\n                 return triton.language.semantic.trans(lhs, builder=self.builder)\n         return getattr(lhs, node.attr)\n@@ -915,9 +908,9 @@ def visit_JoinedStr(self, node):\n             elif isinstance(value, ast.FormattedValue):\n                 conversion_code = value.conversion\n                 evaluated = self.visit(value.value)\n-                if not isinstance(evaluated, triton.language.constexpr):\n-                    raise NotImplementedError(\"Cannot evaluate f-string containing non-constexpr conversion values,\"\n-                                              \" found conversion of type \" + str(type(evaluated)))\n+                if not _is_constexpr(evaluated):\n+                    raise UnsupportedLanguageConstruct(\n+                        None, node, \"Cannot evaluate f-string containing non-constexpr conversion values, found conversion of type \" + str(type(evaluated)))\n                 values[i] = (\"{}\" if conversion_code < 0 else \"{!\" + chr(conversion_code) + \"}\").format(evaluated.value)\n             else:\n                 raise AssertionError(\"encountered unexpected node of type {} in a JoinedStr node\".format(type(value)))\n@@ -934,19 +927,16 @@ def visit(self, node):\n             return super().visit(node)\n \n     def generic_visit(self, node):\n-        typename = type(node).__name__\n-        raise NotImplementedError(\"Unsupported node: {}\".format(typename))\n+        raise UnsupportedLanguageConstruct(None, node, \"unsupported AST node type: {}\".format(type(node).__name__))\n \n     # TODO: populate this here (rather than inside `_define_name_lookup`) once cyclic imports resolved\n     statically_implemented_functions: Dict[object, Callable[[ast.Call], Any]] = {}\n \n     def execute_static_print(self, node: ast.Call) -> None:\n         # TODO: too simplistic? Perhaps do something else with non-constexpr\n-        def unwrap(_):\n-            return _.value if isinstance(_, triton.language.constexpr) else _\n \n-        kws = {name: unwrap(value) for name, value in (self.visit(keyword) for keyword in node.keywords)}\n-        args = [unwrap(self.visit(arg)) for arg in node.args]\n+        kws = {name: _unwrap_if_constexpr(value) for name, value in (self.visit(keyword) for keyword in node.keywords)}\n+        args = [_unwrap_if_constexpr(self.visit(arg)) for arg in node.args]\n         print(*args, **kws)\n \n     def execute_static_assert(self, node: ast.Call) -> None:\n@@ -975,20 +965,29 @@ class CompilationError(Exception):\n \n     def _format_message(self) -> str:\n         node = self.node\n-        message = f'at {node.lineno}:{node.col_offset}:'\n         if self.src is None:\n-            message += \" <source unavailable>\"\n+            source_excerpt = \" <source unavailable>\"\n         else:\n-            message += '\\n'.join(self.src.split('\\n')[:node.lineno][-self.source_line_count_max_in_message:])\n-            message += '\\n' + ' ' * node.col_offset + '^'\n+            source_excerpt = self.src.split('\\n')[:node.lineno][-self.source_line_count_max_in_message:]\n+            if source_excerpt:\n+                source_excerpt.append(' ' * node.col_offset + '^')\n+                source_excerpt = '\\n'.join(source_excerpt)\n+            else:\n+                source_excerpt = \" <source empty>\"\n+\n+        message = \"at {}:{}:{}\".format(node.lineno, node.col_offset, source_excerpt)\n         if self.error_message:\n             message += '\\n' + self.error_message\n         return message\n \n-    def __init__(self, src: Optional[str], node: ast.AST, error_message: Optional[str]):\n+    def __init__(self, src: Optional[str], node: ast.AST, error_message: Union[str, 'triton.language.core.constexpr', None]):\n         self.src = src\n         self.node = node\n-        self.error_message = error_message\n+        self.error_message = _unwrap_if_constexpr(error_message)\n+        self.message = self._format_message()\n+\n+    def set_source_code(self, src: Optional[str]):\n+        self.src = src\n         self.message = self._format_message()\n \n     def __str__(self):\n@@ -1004,10 +1003,11 @@ def __reduce__(self):\n \n class CompileTimeAssertionFailure(CompilationError):\n     \"\"\"Specific exception for failed tests in `static_assert` invocations\"\"\"\n+    pass\n \n-    def set_source_code(self, src: Optional[str]):\n-        self.src = src\n-        self.message = self._format_message()\n+\n+class UnsupportedLanguageConstruct(CompilationError):\n+    pass\n \n \n class OutOfResources(Exception):\n@@ -1072,11 +1072,10 @@ def build_triton_ir(fn, signature, specialization, constants, debug=False):\n     generator = CodeGenerator(context, prototype, gscope=gscope, constants=all_constants, function_name=function_name, attributes=new_attrs, is_kernel=True, debug=debug)\n     try:\n         generator.visit(fn.parse())\n-    except CompileTimeAssertionFailure as e:\n-        e.set_source_code(fn.src)\n+    except CompilationError as e:\n+        if e.src is None:\n+            e.set_source_code(fn.src)\n         raise\n-    except CompilationError:  # (can this ever happen? nobody has access to fn.src except here)\n-        raise  # unchanged\n     except Exception as e:\n         node = generator.last_node\n         if node is None:\n@@ -1591,7 +1590,7 @@ def format_of(ty):\n \n \n def default_cache_dir():\n-    return os.path.join(os.environ[\"HOME\"], \".triton\", \"cache\")\n+    return os.path.join(Path.home(), \".triton\", \"cache\")\n \n \n class CacheManager:"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -5,7 +5,7 @@\n     ir,\n     builtin,\n )\n-from . import libdevice\n+from . import math\n from .core import (\n     abs,\n     arange,\n@@ -141,7 +141,7 @@\n     \"int64\",\n     \"int8\",\n     \"ir\",\n-    \"libdevice\",\n+    \"math\",\n     \"load\",\n     \"log\",\n     \"max\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 13, "deletions": 3, "changes": 16, "file_content_changes": "@@ -862,7 +862,7 @@ def reshape(input, shape, _builder=None):\n \n \n @builtin\n-def dot(input, other, allow_tf32=True, _builder=None):\n+def dot(input, other, allow_tf32=True, out_dtype=float32, _builder=None):\n     \"\"\"\n     Returns the matrix product of two blocks.\n \n@@ -874,7 +874,8 @@ def dot(input, other, allow_tf32=True, _builder=None):\n     :type other: 2D tensor of scalar-type in {:code:`float16`, :code:`bfloat16`, :code:`float32`}\n     \"\"\"\n     allow_tf32 = _constexpr_to_value(allow_tf32)\n-    return semantic.dot(input, other, allow_tf32, _builder)\n+    out_dtype = _constexpr_to_value(out_dtype)\n+    return semantic.dot(input, other, allow_tf32, out_dtype, _builder)\n \n \n # -----------------------\n@@ -1215,7 +1216,16 @@ def max_contiguous(input, values, _builder=None):\n \n @triton.jit\n def abs(x):\n-    return where(x >= 0, x, -x)\n+    x_dtype = x.dtype\n+    if x_dtype.is_floating():\n+        num_bits: constexpr = x.dtype.primitive_bitwidth\n+        int_dtype = dtype(f'int{num_bits}')\n+        mask = 2 ** (num_bits - 1) - 1\n+        ret = x.to(int_dtype, bitcast=True) & mask.to(int_dtype)\n+        ret = ret.to(x_dtype, bitcast=True)\n+    else:\n+        ret = where(x >= 0, x, -x)\n+    return ret\n \n \n @triton.jit"}, {"filename": "python/triton/language/math.py", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "python/triton/language/libdevice.py"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 10, "deletions": 5, "changes": 15, "file_content_changes": "@@ -1051,6 +1051,7 @@ def atomic_xchg(ptr: tl.tensor,\n def dot(lhs: tl.tensor,\n         rhs: tl.tensor,\n         allow_tf32: bool,\n+        out_dtype: tl.dtype,\n         builder: ir.builder) -> tl.tensor:\n     assert lhs.type.is_block() and rhs.type.is_block()\n     assert lhs.dtype == rhs.dtype, \"lhs and rhs must have the same dtype!\"\n@@ -1062,9 +1063,13 @@ def dot(lhs: tl.tensor,\n     if lhs.type.scalar.is_int():\n         _0 = builder.get_int32(0)\n         ret_scalar_ty = tl.int32\n-    else:\n+    elif lhs.type.scalar.is_fp32() or lhs.type.scalar.is_bf16():\n         _0 = builder.get_fp32(0)\n         ret_scalar_ty = tl.float32\n+    else:\n+        _0 = builder.get_fp16(0) if out_dtype.is_fp16() else builder.get_fp32(0)\n+        ret_scalar_ty = out_dtype\n+\n     M = lhs.type.shape[0]\n     N = rhs.type.shape[1]\n     _0 = builder.create_splat(_0, [M, N])\n@@ -1188,14 +1193,14 @@ def xor_sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n def umulhi(x: tl.tensor, y: tl.tensor, builder: ir.builder) -> tl.tensor:\n     x, y = binary_op_type_checking_impl(x, y, builder)\n     # FIXME(Keren): not portable, should be fixed\n-    from . import libdevice\n-    return libdevice.mulhi(x, y, _builder=builder)\n+    from . import math\n+    return math.mulhi(x, y, _builder=builder)\n \n \n def floor(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     # FIXME(Keren): not portable, should be fixed\n-    from . import libdevice\n-    return libdevice.floor(x, _builder=builder)\n+    from . import math\n+    return math.floor(x, _builder=builder)\n \n \n def exp(x: tl.tensor, builder: ir.builder) -> tl.tensor:"}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -64,7 +64,7 @@ def _sdd_kernel(\n         else:\n             a = tl.load(a_ptrs, mask=offs_ak[None, :] < k, other=0.)\n             b = tl.load(b_ptrs, mask=offs_bk[:, None] < k, other=0.)\n-        acc += tl.dot(a, b)\n+        acc += tl.dot(a, b, out_dtype=tl.float32)\n         a_ptrs += TILE_K * stride_ak\n         b_ptrs += TILE_K * stride_bk\n     c = acc.to(C.dtype.element_ty)\n@@ -183,7 +183,7 @@ def _dsd_kernel(\n     for k in range(K, 0, -TILE_K):\n         a = tl.load(pa)\n         b = tl.load(pb)\n-        acc += tl.dot(a, b)\n+        acc += tl.dot(a, b, out_dtype=tl.float32)\n         pa += inc_a\n         pb += inc_b * stride_bk\n         pinc += 2"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 21, "deletions": 9, "changes": 30, "file_content_changes": "@@ -64,9 +64,9 @@ def _kernel(A, B, C, M, N, K,\n             stride_am, stride_ak,\n             stride_bk, stride_bn,\n             stride_cm, stride_cn,\n+            dot_out_dtype: tl.constexpr,\n             BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n             GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr,\n-            ACC_TYPE: tl.constexpr\n             ):\n     # matrix multiplication\n     pid = tl.program_id(0)\n@@ -88,7 +88,7 @@ def _kernel(A, B, C, M, N, K,\n     # pointers\n     A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n     B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n-    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n+    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=dot_out_dtype)\n     for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n         if EVEN_K:\n             a = tl.load(A)\n@@ -97,7 +97,7 @@ def _kernel(A, B, C, M, N, K,\n             k_remaining = K - k * (BLOCK_K * SPLIT_K)\n             a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.)\n             b = tl.load(B, mask=rk[:, None] < k_remaining, other=0.)\n-        acc += tl.dot(a, b)\n+        acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n         A += BLOCK_K * SPLIT_K * stride_ak\n         B += BLOCK_K * SPLIT_K * stride_bk\n     acc = acc.to(C.dtype.element_ty)\n@@ -119,7 +119,7 @@ class _matmul(torch.autograd.Function):\n     _locks = {}\n \n     @staticmethod\n-    def _call(a, b):\n+    def _call(a, b, dot_out_dtype):\n         device = a.device\n         # handle non-contiguous inputs if necessary\n         if a.stride(0) > 1 and a.stride(1) > 1:\n@@ -132,20 +132,32 @@ def _call(a, b):\n         _, N = b.shape\n         # allocates output\n         c = torch.empty((M, N), device=device, dtype=a.dtype)\n-        # accumulator types\n-        ACC_TYPE = tl.float32 if a.dtype in [torch.float16, torch.bfloat16, torch.float32] else tl.int32\n+        if dot_out_dtype is None:\n+            if a.dtype in [torch.float16, torch.float32, torch.bfloat16]:\n+                dot_out_dtype = tl.float32\n+            else:\n+                dot_out_dtype = tl.int32\n+        else:\n+            assert isinstance(dot_out_dtype, torch.dtype), \"dot_out_dtype must be a torch.dtype\"\n+            if dot_out_dtype == torch.float16:\n+                dot_out_dtype = tl.float16\n+            elif dot_out_dtype in [torch.float32, torch.bfloat16]:\n+                dot_out_dtype = tl.float32\n+            else:\n+                dot_out_dtype = tl.int32\n         # launch kernel\n         grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']), META['SPLIT_K'])\n         _kernel[grid](a, b, c, M, N, K,\n                       a.stride(0), a.stride(1),\n                       b.stride(0), b.stride(1),\n                       c.stride(0), c.stride(1),\n-                      GROUP_M=8, ACC_TYPE=ACC_TYPE)\n+                      dot_out_dtype=dot_out_dtype,\n+                      GROUP_M=8)\n         return c\n \n     @staticmethod\n-    def forward(ctx, a, b):\n-        return _matmul._call(a, b)\n+    def forward(ctx, a, b, dot_out_dtype=None):\n+        return _matmul._call(a, b, dot_out_dtype=dot_out_dtype)\n \n \n matmul = _matmul.apply"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 110, "deletions": 0, "changes": 110, "file_content_changes": "@@ -956,6 +956,7 @@ func.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !\n   return\n }\n \n+// -----\n \n // Just make sure it doesn't crash on non-tensor types.\n // CHECK-LABEL: if_no_tensor\n@@ -982,3 +983,112 @@ func.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   tt.store %9, %8 {cache = 1 : i32, evict = 1 : i32} : f32\n   return\n }\n+\n+// -----\n+\n+// Check if the SimplifyReduceCvt rewriter pattern doesn't hang.\n+// CHECK-LABEL: reduce_cvt\n+// CHECK-NOT: triton_gpu.convert_layout\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 2], order = [0, 1]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [2, 1], order = [0, 1]}>\n+#blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [16, 2], warpsPerCTA = [2, 1], order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n+  func.func public @reduce_cvt1(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg1: i32, %arg2: i32) {\n+    %cst = arith.constant dense<0> : tensor<1x2xi32, #blocked>\n+    %cst_0 = arith.constant dense<2> : tensor<1x2xi32, #blocked>\n+    %0 = tt.make_range {end = 2 : i32, start = 0 : i32} : tensor<2xi32, #blocked1>\n+    %1 = triton_gpu.convert_layout %0 : (tensor<2xi32, #blocked1>) -> tensor<2xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+    %2 = tt.expand_dims %1 {axis = 0 : i32} : (tensor<2xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x2xi32, #blocked>\n+    %3 = \"triton_gpu.cmpi\"(%2, %cst_0) {predicate = 2 : i64} : (tensor<1x2xi32, #blocked>, tensor<1x2xi32, #blocked>) -> tensor<1x2xi1, #blocked>\n+    %4 = tt.reduce %cst {axis = 1 : i32, redOp = 1 : i32} : tensor<1x2xi32, #blocked> -> tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+    %5 = triton_gpu.convert_layout %4 : (tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<1xi32, #blocked1>\n+    %6 = triton_gpu.convert_layout %5 : (tensor<1xi32, #blocked1>) -> tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+    %7 = tt.expand_dims %6 {axis = 1 : i32} : (tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<1x1xi32, #blocked2>\n+    %8 = triton_gpu.convert_layout %7 : (tensor<1x1xi32, #blocked2>) -> tensor<1x1xi32, #blocked>\n+    %9 = tt.splat %arg0 : (!tt.ptr<i64>) -> tensor<1x2x!tt.ptr<i64>, #blocked>\n+    %10 = tt.addptr %9, %2 : tensor<1x2x!tt.ptr<i64>, #blocked>, tensor<1x2xi32, #blocked>\n+    %11 = tt.broadcast %8 : (tensor<1x1xi32, #blocked>) -> tensor<1x2xi32, #blocked>\n+    %12 = arith.extsi %11 : tensor<1x2xi32, #blocked> to tensor<1x2xi64, #blocked>\n+    %13 = triton_gpu.convert_layout %10 : (tensor<1x2x!tt.ptr<i64>, #blocked>) -> tensor<1x2x!tt.ptr<i64>, #blocked3>\n+    %14 = triton_gpu.convert_layout %12 : (tensor<1x2xi64, #blocked>) -> tensor<1x2xi64, #blocked3>\n+    %15 = triton_gpu.convert_layout %3 : (tensor<1x2xi1, #blocked>) -> tensor<1x2xi1, #blocked3>\n+    tt.store %13, %14, %15 {cache = 1 : i32, evict = 1 : i32} : tensor<1x2xi64, #blocked3>\n+    return\n+  }\n+}\n+\n+// -----\n+\n+// Check if the SimplifyReduceCvt handles convert_layout lifted from the for loop.\n+// CHECK-LABEL: reduce_cvt2\n+// CHECK: tt.reduce\n+// CHECK-NEXT: triton_gpu.convert_layout\n+// CHECK-NOT: triton_gpu.convert_layout\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n+#blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  func.func public @reduce_cvt2(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<1x256xf32, #blocked>\n+    %c3136_i32 = arith.constant 3136 : i32\n+    %c256_i32 = arith.constant 256 : i32\n+    %c0_i32 = arith.constant 0 : i32\n+    %cst_0 = arith.constant dense<3.136000e+03> : tensor<1x1xf32, #blocked>\n+    %cst_1 = arith.constant dense<50176> : tensor<1x256xi32, #blocked>\n+    %cst_2 = arith.constant dense<196> : tensor<1x1xi32, #blocked>\n+    %cst_3 = arith.constant dense<196> : tensor<1x256xi32, #blocked>\n+    %cst_4 = arith.constant dense<3136> : tensor<1x256xi32, #blocked>\n+    %cst_5 = arith.constant dense<256> : tensor<1x1xi32, #blocked>\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = tt.make_range {end = 1 : i32, start = 0 : i32} : tensor<1xi32, #blocked1>\n+    %2 = triton_gpu.convert_layout %1 : (tensor<1xi32, #blocked1>) -> tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+    %3 = tt.expand_dims %2 {axis = 1 : i32} : (tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<1x1xi32, #blocked2>\n+    %4 = triton_gpu.convert_layout %3 : (tensor<1x1xi32, #blocked2>) -> tensor<1x1xi32, #blocked>\n+    %5 = tt.splat %0 : (i32) -> tensor<1x1xi32, #blocked>\n+    %6 = arith.addi %5, %4 : tensor<1x1xi32, #blocked>\n+    %7 = \"triton_gpu.cmpi\"(%6, %cst_5) {predicate = 2 : i64} : (tensor<1x1xi32, #blocked>, tensor<1x1xi32, #blocked>) -> tensor<1x1xi1, #blocked>\n+    %8 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked1>\n+    %9 = triton_gpu.convert_layout %8 : (tensor<256xi32, #blocked1>) -> tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+    %10 = tt.expand_dims %9 {axis = 0 : i32} : (tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x256xi32, #blocked>\n+    %11 = arith.muli %6, %cst_2 : tensor<1x1xi32, #blocked>\n+    %12 = tt.broadcast %11 : (tensor<1x1xi32, #blocked>) -> tensor<1x256xi32, #blocked>\n+    %13 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<1x256x!tt.ptr<f32>, #blocked>\n+    %14 = tt.broadcast %7 : (tensor<1x1xi1, #blocked>) -> tensor<1x256xi1, #blocked>\n+    %15 = scf.for %arg5 = %c0_i32 to %c3136_i32 step %c256_i32 iter_args(%arg6 = %cst) -> (tensor<1x256xf32, #blocked>)  : i32 {\n+      %43 = tt.splat %arg5 : (i32) -> tensor<1x256xi32, #blocked>\n+      %44 = arith.addi %43, %10 : tensor<1x256xi32, #blocked>\n+      %45 = \"triton_gpu.cmpi\"(%44, %cst_4) {predicate = 2 : i64} : (tensor<1x256xi32, #blocked>, tensor<1x256xi32, #blocked>) -> tensor<1x256xi1, #blocked>\n+      %46 = arith.remsi %44, %cst_3 : tensor<1x256xi32, #blocked>\n+      %47 = arith.divsi %44, %cst_3 : tensor<1x256xi32, #blocked>\n+      %48 = arith.addi %46, %12 : tensor<1x256xi32, #blocked>\n+      %49 = arith.muli %47, %cst_1 : tensor<1x256xi32, #blocked>\n+      %50 = arith.addi %48, %49 : tensor<1x256xi32, #blocked>\n+      %51 = tt.addptr %13, %50 : tensor<1x256x!tt.ptr<f32>, #blocked>, tensor<1x256xi32, #blocked>\n+      %52 = arith.andi %45, %14 : tensor<1x256xi1, #blocked>\n+      %53 = triton_gpu.convert_layout %51 : (tensor<1x256x!tt.ptr<f32>, #blocked>) -> tensor<1x256x!tt.ptr<f32>, #blocked3>\n+      %54 = triton_gpu.convert_layout %52 : (tensor<1x256xi1, #blocked>) -> tensor<1x256xi1, #blocked3>\n+      %55 = triton_gpu.convert_layout %cst : (tensor<1x256xf32, #blocked>) -> tensor<1x256xf32, #blocked3>\n+      %56 = tt.load %53, %54, %55 {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<1x256xf32, #blocked3>\n+      %57 = triton_gpu.convert_layout %56 : (tensor<1x256xf32, #blocked3>) -> tensor<1x256xf32, #blocked>\n+      %58 = arith.addf %arg6, %57 : tensor<1x256xf32, #blocked>\n+      %59 = \"triton_gpu.select\"(%52, %58, %arg6) : (tensor<1x256xi1, #blocked>, tensor<1x256xf32, #blocked>, tensor<1x256xf32, #blocked>) -> tensor<1x256xf32, #blocked>\n+      scf.yield %59 : tensor<1x256xf32, #blocked>\n+    }\n+    %16 = tt.reduce %15 {axis = 1 : i32, redOp = 2 : i32} : tensor<1x256xf32, #blocked> -> tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+    %17 = triton_gpu.convert_layout %16 : (tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<1xf32, #blocked1>\n+    %18 = triton_gpu.convert_layout %17 : (tensor<1xf32, #blocked1>) -> tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+    %19 = tt.expand_dims %18 {axis = 1 : i32} : (tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<1x1xf32, #blocked2>\n+    %20 = triton_gpu.convert_layout %19 : (tensor<1x1xf32, #blocked2>) -> tensor<1x1xf32, #blocked>\n+    %21 = arith.divf %20, %cst_0 : tensor<1x1xf32, #blocked>\n+    %22 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<1x1x!tt.ptr<f32>, #blocked>\n+    %23 = tt.addptr %22, %6 : tensor<1x1x!tt.ptr<f32>, #blocked>, tensor<1x1xi32, #blocked>\n+    %24 = triton_gpu.convert_layout %23 : (tensor<1x1x!tt.ptr<f32>, #blocked>) -> tensor<1x1x!tt.ptr<f32>, #blocked>\n+    %25 = triton_gpu.convert_layout %21 : (tensor<1x1xf32, #blocked>) -> tensor<1x1xf32, #blocked>\n+    %26 = triton_gpu.convert_layout %7 : (tensor<1x1xi1, #blocked>) -> tensor<1x1xi1, #blocked>\n+    tt.store %24, %25, %26 {cache = 1 : i32, evict = 1 : i32} : tensor<1x1xf32, #blocked>\n+    return\n+  }\n+}\n+"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 129, "deletions": 1, "changes": 130, "file_content_changes": "@@ -222,4 +222,132 @@ func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n     scf.yield %next_b_ptr, %c : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return %loop#1 : tensor<128x128xf32, #C>\n-}\n\\ No newline at end of file\n+}\n+\n+// CHECK: func.func @lut_bmm\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: %[[LUT_PTR:.*]] = tt.addptr\n+// CHECK: %arg27 = %[[LUT_PTR]]\n+// CHECK: %[[LUT_BUFFER_0:.*]] = tt.load %arg27, {{.*}}\n+// CHECK: %[[LUT_BUFFER_1:.*]] = arith.muli {{.*}}, %[[LUT_BUFFER_0]]\n+// CHECK: %[[LUT_BUFFER_2:.*]] = tt.splat %[[LUT_BUFFER_1]]\n+// CHECK: %[[NEXT_BUFFER_0:.*]] = tt.addptr {{.*}}, %[[LUT_BUFFER_2]]\n+// CHECK: %[[NEXT_BUFFER_1:.*]] = tt.addptr %arg26, {{.*}}\n+// CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_1]]\n+// CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_0]]\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}> \n+func.func @lut_bmm(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32 {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}, %arg17: i32 {tt.divisibility = 16 : i32}) { \n+  %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma> \n+  %c4_i32 = arith.constant 4 : i32\n+  %c1 = arith.constant 1 : index\n+  %c0 = arith.constant 0 : index \n+  %c0_i64 = arith.constant 0 : i64\n+  %c1_i32 = arith.constant 1 : i32\n+  %0 = tt.get_program_id {axis = 2 : i32} : i32\n+  %1 = tt.get_program_id {axis = 0 : i32} : i32\n+  %2 = tt.get_program_id {axis = 1 : i32} : i32 \n+  %3 = tt.get_num_programs {axis = 0 : i32} : i32 \n+  %4 = tt.get_num_programs {axis = 1 : i32} : i32 \n+  %5 = arith.muli %1, %4 : i32\n+  %6 = arith.addi %5, %2 : i32\n+  %7 = arith.muli %4, %c4_i32 : i32\n+  %8 = arith.divsi %6, %7 : i32\n+  %9 = arith.muli %8, %c4_i32 : i32 \n+  %10 = arith.subi %3, %9 : i32 \n+  %11 = arith.cmpi slt, %10, %c4_i32 : i32 \n+  %12 = arith.select %11, %10, %c4_i32 : i32 \n+  %13 = arith.remsi %6, %12 : i32 \n+  %14 = arith.addi %9, %13 : i32 \n+  %15 = arith.remsi %6, %7 : i32 \n+  %16 = arith.divsi %15, %12 : i32 \n+  %17 = arith.muli %arg5, %0 : i32 \n+  %18 = tt.addptr %arg4, %17 : !tt.ptr<i64>, i32\n+  %19 = tt.addptr %18, %14 : !tt.ptr<i64>, i32\n+  %20 = tt.load %19 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64 \n+  %21 = tt.addptr %19, %c1_i32 : !tt.ptr<i64>, i32\n+  %22 = tt.load %21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64 \n+  %23 = arith.subi %22, %20 : i64 \n+  %24 = arith.cmpi eq, %23, %c0_i64 : i64 \n+  cf.cond_br %24, ^bb1, ^bb2\n+^bb1:  // pred: ^bb0\n+  return\n+^bb2:  // pred: ^bb0\n+  %25 = arith.muli %arg1, %0 : i32 \n+  %26 = tt.addptr %arg0, %25 : !tt.ptr<f16>, i32\n+  %27 = arith.extsi %arg2 : i32 to i64\n+  %28 = arith.muli %27, %20 : i64 \n+  %29 = tt.addptr %26, %28 : !tt.ptr<f16>, i64\n+  %30 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+  %31 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %32 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+  %33 = tt.expand_dims %30 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<16x1xi32, #blocked>\n+  %34 = tt.expand_dims %31 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<16x1xi32, #blocked1>\n+  %35 = tt.expand_dims %32 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<16x1xi32, #blocked>\n+  %36 = tt.splat %arg3 : (i32) -> tensor<16x1xi32, #blocked>\n+  %37 = arith.muli %36, %33 : tensor<16x1xi32, #blocked>\n+  %38 = tt.splat %29 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked>\n+  %39 = tt.addptr %38, %37 : tensor<16x1x!tt.ptr<f16>, #blocked>, tensor<16x1xi32, #blocked>\n+  %40 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+  %41 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>\n+  %42 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+  %43 = tt.expand_dims %40 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x16xi32, #blocked>\n+  %44 = tt.expand_dims %41 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>) -> tensor<1x16xi32, #blocked1>\n+  %45 = tt.expand_dims %42 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x16xi32, #blocked>\n+  %46 = tt.broadcast %39 : (tensor<16x1x!tt.ptr<f16>, #blocked>) -> tensor<16x16x!tt.ptr<f16>, #blocked>\n+  %47 = tt.broadcast %43 : (tensor<1x16xi32, #blocked>) -> tensor<16x16xi32, #blocked>\n+  %48 = tt.broadcast %45 : (tensor<1x16xi32, #blocked>) -> tensor<16x16xi32, #blocked>\n+  %49 = tt.addptr %46, %47 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n+  %50 = arith.muli %arg9, %0 : i32 \n+  %51 = tt.addptr %arg8, %50 : !tt.ptr<f16>, i32\n+  %52 = arith.muli %arg11, %16 : i32 \n+  %53 = tt.addptr %51, %52 : !tt.ptr<f16>, i32\n+  %54 = tt.splat %53 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked1>\n+  %55 = tt.addptr %54, %34 : tensor<16x1x!tt.ptr<f16>, #blocked1>, tensor<16x1xi32, #blocked1>\n+  %56 = tt.splat %arg12 : (i32) -> tensor<1x16xi32, #blocked1>\n+  %57 = arith.muli %56, %44 : tensor<1x16xi32, #blocked1>\n+  %58 = tt.broadcast %55 : (tensor<16x1x!tt.ptr<f16>, #blocked1>) -> tensor<16x16x!tt.ptr<f16>, #blocked1>\n+  %59 = tt.broadcast %57 : (tensor<1x16xi32, #blocked1>) -> tensor<16x16xi32, #blocked1>\n+  %60 = tt.addptr %58, %59 : tensor<16x16x!tt.ptr<f16>, #blocked1>, tensor<16x16xi32, #blocked1>\n+  %61 = arith.muli %arg14, %0 : i32\n+  %62 = tt.addptr %arg13, %61 : !tt.ptr<f16>, i32\n+  %63 = arith.muli %arg15, %14 : i32\n+  %64 = tt.addptr %62, %63 : !tt.ptr<f16>, i32\n+  %65 = arith.muli %arg16, %16 : i32\n+  %66 = tt.addptr %64, %65 : !tt.ptr<f16>, i32\n+  %67 = tt.splat %arg17 : (i32) -> tensor<16x1xi32, #blocked>\n+  %68 = arith.muli %67, %35 : tensor<16x1xi32, #blocked>\n+  %69 = tt.splat %66 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked>\n+  %70 = tt.addptr %69, %68 : tensor<16x1x!tt.ptr<f16>, #blocked>, tensor<16x1xi32, #blocked>\n+  %71 = tt.broadcast %70 : (tensor<16x1x!tt.ptr<f16>, #blocked>) -> tensor<16x16x!tt.ptr<f16>, #blocked>\n+  %72 = tt.addptr %71, %48 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n+  %73 = arith.muli %arg7, %0 : i32\n+  %74 = tt.addptr %arg6, %73 : !tt.ptr<i64>, i32\n+  %75 = tt.addptr %74, %20 : !tt.ptr<i64>, i64\n+  %76 = arith.index_cast %23 : i64 to index\n+  %77 = arith.extsi %arg10 : i32 to i64\n+  %78 = tt.splat %arg2 : (i32) -> tensor<16x16xi32, #blocked>\n+  %79:3 = scf.for %arg18 = %c0 to %76 step %c1 iter_args(%arg19 = %cst, %arg20 = %49, %arg21 = %75) -> (tensor<16x16xf32, #mma>, tensor<16x16x!tt.ptr<f16>, #blocked>, !tt.ptr<i64>) {\n+    %82 = tt.load %arg20 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blocked>\n+    %83 = tt.load %arg21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64\n+    %84 = arith.muli %77, %83 : i64\n+    %85 = tt.splat %84 : (i64) -> tensor<16x16xi64, #blocked1>\n+    %86 = tt.addptr %60, %85 : tensor<16x16x!tt.ptr<f16>, #blocked1>, tensor<16x16xi64, #blocked1>\n+    %87 = tt.load %86 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blocked1>\n+    %88 = triton_gpu.convert_layout %82 : (tensor<16x16xf16, #blocked>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma}>>\n+    %89 = triton_gpu.convert_layout %87 : (tensor<16x16xf16, #blocked1>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma}>>\n+    %90 = tt.dot %88, %89, %arg19 {allowTF32 = true} : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma}>> -> tensor<16x16xf32, #mma>\n+    %91 = tt.addptr %arg20, %78 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n+    %92 = tt.addptr %arg21, %c1_i32 : !tt.ptr<i64>, i32\n+    scf.yield %90, %91, %92 : tensor<16x16xf32, #mma>, tensor<16x16x!tt.ptr<f16>, #blocked>, !tt.ptr<i64>\n+  }\n+  %80 = arith.truncf %79#0 : tensor<16x16xf32, #mma> to tensor<16x16xf16, #mma>\n+  %81 = triton_gpu.convert_layout %80 : (tensor<16x16xf16, #mma>) -> tensor<16x16xf16, #blocked>\n+  tt.store %72, %81 {cache = 1 : i32, evict = 1 : i32} : tensor<16x16xf16, #blocked>\n+  return\n+}"}]