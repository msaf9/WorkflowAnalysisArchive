[{"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -262,13 +262,14 @@ def make_hash(fn, arch, **kwargs):\n         configs = kwargs[\"configs\"]\n         signature = kwargs[\"signature\"]\n         constants = kwargs.get(\"constants\", dict())\n+        constants_items = [(k, v if not isinstance(v, JITFunction) else v.literal_hash) for k, v in constants.items()]\n         num_warps = kwargs.get(\"num_warps\", 4)\n         num_stages = kwargs.get(\"num_stages\", 3)\n         debug = kwargs.get(\"debug\", False)\n         # Get unique key for the compiled code\n         get_conf_key = lambda conf: (sorted(conf.divisible_by_16), sorted(conf.equal_to_1))\n         configs_key = [get_conf_key(conf) for conf in configs]\n-        key = f\"{fn.cache_key}-{''.join(signature.values())}-{configs_key}-{constants}-{num_warps}-{num_stages}-{debug}-{arch}\"\n+        key = f\"{fn.cache_key}-{''.join(signature.values())}-{configs_key}-{constants_items}-{num_warps}-{num_stages}-{debug}-{arch}\"\n         return hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n     assert isinstance(fn, str)\n     return hashlib.md5((Path(fn).read_text() + version_key()).encode(\"utf-8\")).hexdigest()"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 19, "deletions": 15, "changes": 34, "file_content_changes": "@@ -62,8 +62,9 @@ def __init__(self, globals, src) -> None:\n         self.ret = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n         self.globals = globals\n \n-    def visit_Name(self, node):\n-        return self.globals.get(node.id, None)\n+    def visit_Name(self, node: ast.Name):\n+        x = self.globals.get(node.id, None)\n+        return x\n \n     def visit_Attribute(self, node):\n         lhs = self.visit(node.value)\n@@ -82,13 +83,9 @@ def visit_Call(self, node):\n         if func.__module__ and (func.__module__.startswith('triton.') or '.triton.' in func.__module__):\n             return\n         assert isinstance(func, JITFunction), f\"Function \\\"{func.__name__}\\\" is being called from a Triton function but is not a Triton function itself. Decorate it with @triton.jit to fix this\"\n-        if func.hash is None:\n-            tree = ast.parse(func.src)\n-            finder = DependenciesFinder(func.__globals__, func.src)\n-            finder.visit(tree)\n-            func.hash = finder.ret\n+\n         noinline = str(getattr(func, 'noinline', False))\n-        self.ret = (self.ret + func.hash + noinline).encode(\"utf-8\")\n+        self.ret = (self.ret + func.literal_hash + noinline).encode(\"utf-8\")\n         self.ret = hashlib.md5(self.ret).hexdigest()\n \n # -----------------------------------------------------------------------------\n@@ -412,7 +409,8 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         self.src = self.src[self.src.find(\"def\"):]\n         # cache of just-in-time compiled kernels\n         self.cache = defaultdict(dict)\n-        self.hash = None\n+        self._hash = None\n+        self._literal_hash = None\n         # JITFunction can be instantiated as kernel\n         # when called with a grid using __getitem__\n         self.kernel_decorators = []\n@@ -433,13 +431,19 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         self.__module__ = fn.__module__\n \n     @property\n-    def cache_key(self):\n-        # TODO : hash should be attribute of `self`\n-        if self.hash is None:\n+    def literal_hash(self):\n+        if self._literal_hash is None:\n             dependencies_finder = DependenciesFinder(globals=self.__globals__, src=self.src)\n             dependencies_finder.visit(self.parse())\n-            self.hash = dependencies_finder.ret + version_key()\n-        return self.hash\n+            self._literal_hash = dependencies_finder.ret\n+        return self._literal_hash\n+\n+    @property\n+    def cache_key(self):\n+        # TODO : hash should be attribute of `self`\n+        if self._hash is None:\n+            self._hash = self.literal_hash + version_key()\n+        return self._hash\n \n     def warmup(self, *args, **kwargs):\n         return self.run(*map(MockTensor.wrap_dtype, args), **kwargs, warmup=True)\n@@ -466,7 +470,7 @@ def __setattr__(self, name, value):\n         # - when `.src` attribute is set, cache path needs\n         #   to be reinitialized\n         if name == 'src':\n-            self.hash = None\n+            self._hash = None\n \n     def __repr__(self):\n         return f\"JITFunction({self.module}:{self.fn.__name__})\""}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "file_content_changes": "@@ -325,11 +325,11 @@ def matmul(a, b, activation=None):\n         ],  # Different possible values for `x_name`\n         line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\n         # Possible values for `line_arg`\n-        line_vals=['cublas', 'triton', 'triton+relu'],\n+        line_vals=['cublas', 'triton', 'cublas+relu', 'triton+relu'],\n         # Label name for the lines\n-        line_names=[\"cuBLAS\", \"Triton\", 'Triton+Relu'],\n+        line_names=[\"cuBLAS\", \"Triton\", \"cuBLAS+Relu\", \"Triton+Relu\"],\n         # Line styles\n-        styles=[('green', '-'), ('blue', '-'), ('orange', '-')],\n+        styles=[('green', '-'), ('blue', '-'), ('orange', '-'), ('yellow', '-')],\n         ylabel=\"TFLOPS\",  # Label name for the y-axis\n         plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n         args={},\n@@ -343,6 +343,9 @@ def benchmark(M, N, K, provider):\n         ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n     if provider == 'triton':\n         ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n+    if provider == 'cublas+relu':\n+        torch_relu = torch.nn.ReLU(inplace=True)\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch_relu(torch.matmul(a, b)), quantiles=quantiles)\n     if provider == 'triton+relu':\n         ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b, activation=leaky_relu), quantiles=quantiles)\n "}]