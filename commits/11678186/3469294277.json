[{"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -27,6 +27,9 @@ namespace mlir {\n //===----------------------------------------------------------------------===//\n namespace triton {\n \n+// Bitwidth of pointers\n+constexpr int kPtrBitWidth = 64; \n+\n static std::pair<SmallVector<unsigned>, SmallVector<unsigned>>\n getCvtOrder(const Attribute &srcLayout, const Attribute &dstLayout) {\n   auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>();\n@@ -193,7 +196,9 @@ class AllocationAnalysis {\n       auto smemShape = getScratchConfigForCvtLayout(cvtLayout, inVec, outVec);\n       unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n                                        std::multiplies{});\n-      auto bytes = elems * srcTy.getElementTypeBitWidth() / 8;\n+      auto bytes = srcTy.getElementType().isa<triton::PointerType>()? \n+                   elems * c_PtrBitWidth / 8 :\n+                   elems * srcTy.getElementTypeBitWidth() / 8;\n       allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     }\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -93,6 +93,8 @@ void llPrintf(StringRef msg, ValueRange args,\n               ConversionPatternRewriter &rewriter);\n \n // Shortcuts for some commonly used LLVM ops to keep code simple and intuitive\n+#define inttoptr(...) rewriter.create<LLVM::IntToPtrOp>(loc, __VA_ARGS__)\n+#define ptrtoint(...) rewriter.create<LLVM::PtrToIntOp>(loc, __VA_ARGS__)\n #define zext(...) rewriter.create<LLVM::ZExtOp>(loc, __VA_ARGS__)\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n #define urem(...) rewriter.create<LLVM::URemOp>(loc, __VA_ARGS__)\n@@ -2945,8 +2947,13 @@ void ConvertLayoutOpConversion::processReplica(\n   }\n   auto elemTy = type.getElementType();\n   bool isInt1 = elemTy.isInteger(1);\n+  bool isPtr = elemTy.isa<triton::PointerType>();\n+  auto llvmElemTyOrig = getTypeConverter()->convertType(elemTy);\n   if (isInt1)\n     elemTy = IntegerType::get(elemTy.getContext(), 8);\n+  else if (isPtr)\n+    elemTy = IntegerType::get(elemTy.getContext(), 64);\n+\n   auto llvmElemTy = getTypeConverter()->convertType(elemTy);\n \n   for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n@@ -2978,6 +2985,8 @@ void ConvertLayoutOpConversion::processReplica(\n           auto currVal = vals[elemId + linearCTAId * accumSizePerThread + v];\n           if (isInt1)\n             currVal = zext(llvmElemTy, currVal);\n+          else if (isPtr)\n+            currVal = ptrtoint(llvmElemTy, currVal);\n \n           valVec = insert_element(vecTy, valVec, currVal, idx_val(v));\n         }\n@@ -2990,6 +2999,8 @@ void ConvertLayoutOpConversion::processReplica(\n             currVal =\n                 icmp_ne(currVal, rewriter.create<LLVM::ConstantOp>(\n                                      loc, i8_ty, rewriter.getI8IntegerAttr(0)));\n+          else if (isPtr)\n+            currVal = inttoptr(llvmElemTyOrig, currVal);\n           vals[elemId + linearCTAId * accumSizePerThread + v] = currVal;\n         }\n       }"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 20, "deletions": 1, "changes": 21, "file_content_changes": "@@ -742,6 +742,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n }\n \n // -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n@@ -753,8 +754,8 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   }\n }\n \n-\n // -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n@@ -766,8 +767,25 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   }\n }\n \n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: convert_blocked_to_blocked_ptr\n+  func @convert_blocked_to_blocked_ptr(%src:tensor<32x!tt.ptr<f32>, #blocked0>) {\n+    // CHECK: llvm.ptrtoint\n+    // CHECK: llvm.store\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.inttoptr\n+    // CHECK-COUNT-4: llvm.insertvalue\n+    %cvt = triton_gpu.convert_layout %src : (tensor<32x!tt.ptr<f32>, #blocked0>) -> tensor<32x!tt.ptr<f32>, #blocked1>\n+    return\n+  }\n+}\n \n // -----\n+\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n #mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n@@ -839,6 +857,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n }\n \n // -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32"}]