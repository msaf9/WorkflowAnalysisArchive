[{"filename": "python/triton/compiler.py", "status": "modified", "additions": 96, "deletions": 0, "changes": 96, "file_content_changes": "@@ -4,6 +4,7 @@\n import contextlib\n import functools\n import hashlib\n+import importlib\n import io\n import json\n import os\n@@ -13,6 +14,7 @@\n import sys\n import sysconfig\n import tempfile\n+import textwrap\n import warnings\n from collections import namedtuple\n from pathlib import Path\n@@ -1887,9 +1889,101 @@ def _is_jsonable(x):\n \n # def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, configs=None):\n \n+# --------------------\n+# Brainfuck\n+# --------------------\n+\n+\n+class _GetchUnix:\n+    def __init__(self):\n+        pass\n+\n+    def __call__(self):\n+        import sys\n+        import termios\n+        import tty\n+        fd = sys.stdin.fileno()\n+        old_settings = termios.tcgetattr(fd)\n+        try:\n+            tty.setraw(sys.stdin.fileno())\n+            ch = sys.stdin.read(1)\n+        finally:\n+            termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)\n+        return ch\n+\n+\n+getch = _GetchUnix()\n+\n+\n+def evaluate(code):\n+    code = cleanup(list(code))\n+    bracemap = buildbracemap(code)\n+\n+    cells, codeptr, cellptr = [0], 0, 0\n+    ret = \"\"\n+    while codeptr < len(code):\n+        command = code[codeptr]\n+\n+        if command == \">\":\n+            cellptr += 1\n+            if cellptr == len(cells): cells.append(0)\n+\n+        if command == \"<\":\n+            cellptr = 0 if cellptr <= 0 else cellptr - 1\n+\n+        if command == \"+\":\n+            cells[cellptr] = cells[cellptr] + 1 if cells[cellptr] < 255 else 0\n+\n+        if command == \"-\":\n+            cells[cellptr] = cells[cellptr] - 1 if cells[cellptr] > 0 else 255\n+\n+        if command == \"[\" and cells[cellptr] == 0: codeptr = bracemap[codeptr]\n+        if command == \"]\" and cells[cellptr] != 0: codeptr = bracemap[codeptr]\n+        if command == \".\": ret += (chr(cells[cellptr]))\n+        if command == \",\": cells[cellptr] = ord(getch.getch())\n+\n+        codeptr += 1\n+\n+    return ret\n+\n+\n+def cleanup(code):\n+    return ''.join(filter(lambda x: x in ['.', ',', '[', ']', '<', '>', '+', '-'], code))\n+\n+\n+def buildbracemap(code):\n+    temp_bracestack, bracemap = [], {}\n+\n+    for position, command in enumerate(code):\n+        if command == \"[\": temp_bracestack.append(position)\n+        if command == \"]\":\n+            start = temp_bracestack.pop()\n+            bracemap[start] = position\n+            bracemap[position] = start\n+    return bracemap\n+\n+\n+def decode_brainfuck_src(fn_src):\n+    fn_src = evaluate(fn_src)\n+    with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".py\") as f:\n+        f.write(textwrap.dedent(fn_src))\n+        f.flush()\n+        module_name = os.path.splitext(os.path.basename(f.name))[0]\n+        spec = importlib.util.spec_from_file_location(module_name, f.name)\n+        module = importlib.util.module_from_spec(spec)\n+        module.__dict__.update({\"triton\": triton, \"tl\": triton.language})\n+        spec.loader.exec_module(module)\n+        callables = list(filter(callable, module.__dict__.values()))\n+        assert len(callables) == 1\n+        ret = triton.JITFunction(callables[0])\n+    return ret\n+\n \n @static_vars(discovered_gfx_arch_fulldetails=get_amdgpu_arch_fulldetails())\n def compile(fn, **kwargs):\n+    if \"brainfuck\" in kwargs:\n+        fn = decode_brainfuck_src(fn)\n+\n     capability = kwargs.get(\"cc\", None)\n     if capability is None:\n         device = triton.runtime.jit.get_current_device()\n@@ -1975,6 +2069,8 @@ def compile(fn, **kwargs):\n         signature = {k: v for k, v in enumerate(param_tys)}\n         first_stage = list(stages.keys()).index(ir)\n \n+    print(constants)\n+    print(signature)\n     # cache manager\n     so_path = make_stub(name, signature, constants)\n     # create cache manager"}, {"filename": "python/tutorials/08-brainfuck-matmul.py", "status": "modified", "additions": 9, "deletions": 49, "changes": 58, "file_content_changes": "@@ -1,7 +1,6 @@\n import torch\n \n import triton\n-import triton.language as tl\n \n src = \"\"\"\n --[----->+<]>--.+.+.[--->+<]>--.+[----->+<]>.------------.--[--->+<]>-.-------.++++++++.---------.-------------.++++++++\n@@ -288,9 +287,11 @@\n BLOCK_M = 128\n BLOCK_N = 128\n BLOCK_K = 32\n-matmul_kernel = triton.compile(src, signature=\"*f16,*f16,*f16,i32,i32,i32,i32,i32,i32\",\n-                                constants={7: 1, 9: 1, 11: 1, 12: BLOCK_M, 13: BLOCK_N, 14: BLOCK_K, \n-                                           15: 8, 16: None})\n+matmul_kernel = triton.compile(src,\n+                               constants={12: 128, 13: 128, 14: 32, 15: 8, 16: None, 9: 1, 11: 1, 7: 1},\n+                               signature={0: '*fp16', 1: '*fp16', 2: '*fp16', 3: 'i32', 4: 'i32', 5: 'i32', 6: 'i32', 7: 'i32', 8: 'i32', 9: 'i32', 10: 'i32', 11: 'i32'},\n+                               num_warps=4,\n+                               brainfuck=True)\n \n \n # %%\n@@ -311,13 +312,13 @@ def matmul(a, b, activation=None):\n     # allocates output\n     c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n     # 1D launch kernel where each block gets its own program.\n-    grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N), )\n+    grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N), 1, 1)\n     matmul_kernel[grid](\n         a, b, c,\n         M, N, K,\n-        a.stride(0),\n-        b.stride(0),\n-        c.stride(0),\n+        a.stride(0), a.stride(1),\n+        b.stride(0), b.stride(1),\n+        c.stride(0), c.stride(1),\n     )\n     return c\n \n@@ -339,44 +340,3 @@ def matmul(a, b, activation=None):\n     print(\"\u2705 Triton and Torch match\")\n else:\n     print(\"\u274c Triton and Torch differ\")\n-\n-# %%\n-# Benchmark\n-# ---------\n-#\n-# Square Matrix Performance\n-# ~~~~~~~~~~~~~~~~~~~~~~~~~~\n-#\n-# We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices, but feel free to arrange this script as you wish to benchmark any other matrix shape.\n-\n-\n-@triton.testing.perf_report(\n-    triton.testing.Benchmark(\n-        x_names=['M', 'N', 'K'],  # argument names to use as an x-axis for the plot\n-        x_vals=[\n-            8192\n-        ],  # different possible values for `x_name`\n-        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n-        # possible values for `line_arg``\n-        line_vals=['cublas', 'triton'],\n-        # label name for the lines\n-        line_names=[\"cuBLAS\", \"Triton\"],\n-        # line styles\n-        styles=[('green', '-'), ('green', '--'), ('blue', '-'), ('blue', '--')],\n-        ylabel=\"TFLOPS\",  # label name for the y-axis\n-        plot_name=\"matmul-performance\",  # name for the plot. Used also as a file name for saving the plot.\n-        args={},\n-    )\n-)\n-def benchmark(M, N, K, provider):\n-    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n-    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n-    if provider == 'cublas':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), rep=100)\n-    if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), rep=100)\n-    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n-    return perf(ms), perf(max_ms), perf(min_ms)\n-\n-\n-benchmark.run(show_plots=True, print_data=True)"}]