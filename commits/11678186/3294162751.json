[{"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -972,6 +972,7 @@ def _compile(fn, signature: str, device: int = -1, constants=dict(), specializat\n     module = optimize_triton_ir(module)\n     if output == \"ttir\":\n         return module.str()\n+    \n \n     # tritongpu-ir\n     module = make_tritongpu_ir(module, num_warps)"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 20, "deletions": 20, "changes": 40, "file_content_changes": "@@ -75,8 +75,8 @@ def _fwd_kernel(\n         p = p.to(tl.float16)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n-        # l_i = l_i_new\n-        # m_i = m_i_new\n+        l_i = l_i_new\n+        m_i = m_i_new\n     # rematerialize offsets to save registers\n     start_m = tl.program_id(0)\n     offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n@@ -212,19 +212,19 @@ def forward(ctx, q, k, v, sm_scale):\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         num_warps = 4 if Lk <= 64 else 8\n \n-        # _fwd_kernel[grid](\n-        #     q, k, v, sm_scale,\n-        #     tmp, L, m,\n-        #     o,\n-        #     q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n-        #     k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n-        #     v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-        #     o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n-        #     q.shape[0], q.shape[1], q.shape[2],\n-        #     BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n-        #     BLOCK_DMODEL=Lk, num_warps=num_warps,\n-        #     num_stages=1,\n-        # )\n+        _fwd_kernel[grid](\n+            q, k, v, sm_scale,\n+            tmp, L, m,\n+            o,\n+            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+            q.shape[0], q.shape[1], q.shape[2],\n+            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+            BLOCK_DMODEL=Lk, num_warps=num_warps,\n+            num_stages=1,\n+        )\n         ctx.save_for_backward(q, k, v, o, L, m)\n         ctx.BLOCK = BLOCK\n         ctx.grid = grid\n@@ -241,11 +241,11 @@ def backward(ctx, do):\n         dv = torch.empty_like(v)\n         do_scaled = torch.empty_like(do)\n         delta = torch.empty_like(l)\n-        # _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n-        #     o, do, l,\n-        #     do_scaled, delta,\n-        #     BLOCK_M=ctx.BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n-        # )\n+        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n+            o, do, l,\n+            do_scaled, delta,\n+            BLOCK_M=ctx.BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n+        )\n \n         num_warps = 4 if ctx.BLOCK_DMODEL <= 64 else 8\n         _bwd_kernel[(ctx.grid[1],)]("}]