[{"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 37, "deletions": 126, "changes": 163, "file_content_changes": "@@ -150,110 +150,27 @@\n # Final Result\n # ------------\n \n-import argparse\n-\n import torch\n \n import triton\n import triton.language as tl\n \n-parser = argparse.ArgumentParser(\n-    prog=\"03-matrix-multiplication\",\n-    description=\"Matrix multiplication kernel via Triton\")\n-parser.add_argument('-c', '--config', default=\"FNFN\",\n-                    help=\"configuration of matmul data types and transposition options\")\n-parser.add_argument('-s', '--stype', default=\"int8\",\n-                    help=\"which type should be used as the 'S' type in config\")\n-args = parser.parse_args()\n-\n-print(\"config:\", args.config, \"stype:\", args.stype)\n-\n-\n-def f8_to_f16(x):\n-    assert x.is_contiguous(), \"Kernel only works for contiguous tensors\"\n-\n-    @triton.jit\n-    def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n-        pid = tl.program_id(0)\n-        offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n-        mask = offs < N\n-        x = tl.load(X + offs, mask=mask)\n-        y = x.to(tl.float16)\n-        tl.store(Y + offs, y, mask=mask)\n-    ret = torch.empty(x.shape, dtype=torch.float16, device=x.device)\n-    grid = lambda META: (triton.cdiv(x.numel(), META['BLOCK_SIZE']),)\n-    kernel[grid](ret, triton.reinterpret(x, tl.float8e5), ret.numel(), BLOCK_SIZE=1024)\n-    return ret\n-\n-\n-def f16_to_f8(x):\n-    assert x.is_contiguous(), \"Kernel only works for contiguous tensors\"\n-\n-    @triton.jit\n-    def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n-        pid = tl.program_id(0)\n-        offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n-        mask = offs < N\n-        x = tl.load(X + offs, mask=mask)\n-        y = x.to(tl.float8e5)\n-        tl.store(Y + offs, y, mask=mask)\n-    ret = torch.empty(x.shape, dtype=torch.int8, device=x.device)\n-    grid = lambda META: (triton.cdiv(x.numel(), META['BLOCK_SIZE']),)\n-    kernel[grid](triton.reinterpret(ret, tl.float8e5), x, x.numel(), BLOCK_SIZE=1024)\n-    return ret\n-\n-\n-def transform_impl(c, x):\n-    t = x\n-    if c[0] == \"S\":\n-        if args.stype == \"int8\":\n-            t = x.to(torch.int8)\n-        elif args.stype == \"float8\":\n-            t = f16_to_f8(x)\n-        else:\n-            raise Exception(\"unexpected --stype value\")\n-    return t.T if c[1] == \"T\" else t\n-\n-\n-def transform_lhs(x):\n-    return transform_impl(args.config[0:2], x)\n-\n-\n-def transform_rhs(x):\n-    return transform_impl(args.config[2:4], x)\n-\n-\n-def maybe_upcast_impl(p, x):\n-    if not x.is_contiguous():\n-        return maybe_upcast_impl(p, x.T).T\n-    return f8_to_f16(x) if p == \"S\" and args.stype == \"float8\" else x.to(torch.float16)\n-\n-\n-def maybe_upcast_lhs(x):\n-    return maybe_upcast_impl(args.config[0], x)\n-\n-\n-def maybe_upcast_rhs(x):\n-    return maybe_upcast_impl(args.config[2], x)\n \n # `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n #   - A list of `triton.Config` objects that define different configurations of\n #       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n #   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n #       provided configs\n-\n-\n @triton.autotune(\n     configs=[\n-        # triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=4),\n-        # triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        # triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        # triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        # triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        # triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n-        # triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n-        # triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n     ],\n     key=['M', 'N', 'K'],\n )\n@@ -272,8 +189,7 @@ def matmul_kernel(\n     # Meta-parameters\n     BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n     GROUP_SIZE_M: tl.constexpr,\n-    IS_LHS_FP8: tl.constexpr,\n-    IS_RHS_FP8: tl.constexpr,\n+    ACTIVATION: tl.constexpr,\n ):\n     \"\"\"Kernel for computing the matmul C = A x B.\n     A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n@@ -315,18 +231,16 @@ def matmul_kernel(\n         # Load the next block of A and B, generate a mask by checking the K dimension.\n         # If it is out of bounds, set it to 0.\n         a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n-        if IS_LHS_FP8:\n-            a = a.to(tl.float8e5, bitcast=True)\n-        a = a.to(tl.float16)\n         b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n-        if IS_RHS_FP8:\n-            b = b.to(tl.float8e5, bitcast=True)\n-        b = b.to(tl.float16)\n         # We accumulate along the K dimension.\n         accumulator += tl.dot(a, b)\n         # Advance the ptrs to the next K block.\n         a_ptrs += BLOCK_SIZE_K * stride_ak\n         b_ptrs += BLOCK_SIZE_K * stride_bk\n+    # You can fuse arbitrary activation functions here\n+    # while the accumulator is still in FP32!\n+    if ACTIVATION == \"leaky_relu\":\n+        accumulator = leaky_relu(accumulator)\n     c = accumulator.to(tl.float16)\n \n     # -----------------------------------------------------------\n@@ -338,6 +252,13 @@ def matmul_kernel(\n     tl.store(c_ptrs, c, mask=c_mask)\n \n \n+# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n+@triton.jit\n+def leaky_relu(x):\n+    x = x + 1\n+    return tl.where(x >= 0, x, 0.01 * x)\n+\n+\n # %%\n # We can now create a convenience wrapper function that only takes two input tensors,\n # and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n@@ -346,10 +267,12 @@ def matmul_kernel(\n def matmul(a, b, activation=\"\"):\n     # Check constraints.\n     assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n+    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n+    assert b.is_contiguous(), \"Matrix B must be contiguous\"\n     M, K = a.shape\n     K, N = b.shape\n     # Allocates output.\n-    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n+    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n     # 1D launch kernel where each block gets its own program.\n     grid = lambda META: (\n         triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n@@ -360,8 +283,7 @@ def matmul(a, b, activation=\"\"):\n         a.stride(0), a.stride(1),\n         b.stride(0), b.stride(1),\n         c.stride(0), c.stride(1),\n-        IS_LHS_FP8=(a.dtype == torch.int8 and args.stype == \"float8\"),\n-        IS_RHS_FP8=(b.dtype == torch.int8 and args.stype == \"float8\")\n+        ACTIVATION=activation\n     )\n     return c\n \n@@ -373,27 +295,17 @@ def matmul(a, b, activation=\"\"):\n # We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS).\n \n torch.manual_seed(0)\n-M = N = K = 512\n-a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n-# print(f\"before_lhs={a}\")\n-a = transform_lhs(a)\n-# print(f\"after_lhs={maybe_upcast_lhs(a)}\")\n-b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n-# print(f\"before_rhs={b}\")\n-b = transform_rhs(b)\n-# print(f\"after_rhs={maybe_upcast_rhs(b)}\")\n-# print((a - b).abs().max())\n+a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n triton_output = matmul(a, b)\n-torch_output = torch.matmul(maybe_upcast_lhs(a), maybe_upcast_rhs(b))\n-if torch.allclose(triton_output, torch_output, atol=1e-1, rtol=0):\n+torch_output = torch.matmul(a, b)\n+print(f\"triton_output={triton_output}\")\n+print(f\"torch_output={torch_output}\")\n+if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n     print(\"\u2705 Triton and Torch match\")\n else:\n-    print(f\"triton_output={triton_output}\")\n-    print(f\"torch_output={torch_output}\")\n     print(\"\u274c Triton and Torch differ\")\n \n-print(((triton_output[0, :] - torch_output[0, :]).abs() > 1e-1).nonzero())\n-\n # %%\n # Benchmark\n # ---------\n@@ -409,7 +321,7 @@ def matmul(a, b, activation=\"\"):\n     triton.testing.Benchmark(\n         x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot\n         x_vals=[\n-            128 * i for i in range(32, 33)\n+            128 * i for i in range(2, 33)\n         ],  # Different possible values for `x_name`\n         line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\n         # Possible values for `line_arg`\n@@ -424,16 +336,15 @@ def matmul(a, b, activation=\"\"):\n     )\n )\n def benchmark(M, N, K, provider):\n-    a = transform_lhs(torch.randn((M, K), device='cuda', dtype=torch.float16))\n-    b = transform_rhs(torch.randn((K, N), device='cuda', dtype=torch.float16))\n+    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n+    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n     quantiles = [0.5, 0.2, 0.8]\n     if provider == 'cublas':\n-        fn = lambda: torch.matmul(maybe_upcast_lhs(a), maybe_upcast_rhs(b))\n-    elif provider == 'triton':\n-        fn = lambda: matmul(a, b)\n-    ms, min_ms, max_ms = triton.testing.do_bench(fn, quantiles=quantiles, warmup=100, rep=1000)\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n+    if provider == 'triton':\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n     perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n     return perf(ms), perf(max_ms), perf(min_ms)\n \n \n-# benchmark.run(show_plots=False, print_data=True)\n+benchmark.run(show_plots=True, print_data=True)"}]