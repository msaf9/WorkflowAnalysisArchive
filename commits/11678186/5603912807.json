[{"filename": "CONTRIBUTING.md", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -106,7 +106,7 @@ triton\n \u2502   \u2502\t\u251c\u2500\u2500language: core of triton language, load tensors to SRAM, language logic, etc.\n \u2502   \u2502\t\u2502\n \u2502   \u2502\t\u251c\u2500\u2500ops: contains functions for flash-attn, softmax, cross-entropy and other torch.nn.F functions\n-\u2502   \u2502\t\u251c\u2500\u2500runtime: contains impl jit compilation, autotuning, backend drivers,cahcing, error handles, etc.\n+\u2502   \u2502\t\u251c\u2500\u2500runtime: contains impl jit compilation, autotuning, backend drivers, caching, error handles, etc.\n \u2502   \u2502\t\u251c\u2500\u2500third_party\n \u2502   \u2502\t\u251c\u2500\u2500tools\n \u2502   \u251c\u2500\u2500 triton.egg-info"}, {"filename": "docs/meetups/07-18-2023.md", "status": "modified", "additions": 25, "deletions": 0, "changes": 25, "file_content_changes": "@@ -17,3 +17,28 @@\n    - Sm75.\n    - device functions. How hard is this to support while Triton frontend traverses AST?\n    - remove torch dependencies from the frontend. (it sounds like there is already progress on this but could be worth discussing)\n+\n+##### Minutes\n+Recording link [here](https://drive.google.com/file/d/1uMlIvih_E5FITwPnNHwTYzo-UKqtey2c/view)\n+\n+1. Backend plans/broader roadmap:\n+   - Plan is for major updates to come in the Triton development meetup which will happen mid-September. For major design changes, currently the plan is to not upstream them directly but have a staging state and different backends can be integrated through a plugin mechanism where Triton provides a layer at the Triton IR layer that is generic and other backends can plug into that.\n+   - Short term roadmap plans are very focused on things like improving all FP8 things on Ampere and Hopper support (end of August). After Hopper support lands, priorities will include refactoring codebase to increase maintainability.\n+   - Linalg \u2013 upstreaming on hold due to limited dev bandwidth. Want to build an ecosystem where others can leverage Linalg like passes developed in their backend.\n+   - For now, peak performance on Nvidia GPUs needs Nvidia specific things, but the convergence of programming models for different backends will allow convergence of hardware backend support in Triton.\n+2. Documentation:\n+   - OpenAI has included comments in the backend code.\n+   - Seek community involvement to improve tutorials, based on new users knowing what is missing.\n+   - Seek community involvement for signature changes and doc updates.\n+   - Thread created in slack for suggestions on areas needing doc updates. Ian Bearman and his team may have bandwidth to update certain documentation.\n+3. Discussion channels:\n+   - Preferred #dev channel in slack for technical discussions.\n+   - Between GitHub and Slack it would be good to post links into places so folks know discussions are happening elsewhere\n+4. CI/testing:\n+   - Pretty liberal in terms of accepting regression tests and integration tests for Nvidia.\n+   - Plugin interface tested like everything else, and regressions there would block merges into main.\n+   - Correctness/Performance of external backends are tested nightly, but regressions do not prevent wheels from being built.\n+5. Language improvements:\n+   - Have added location information support into Triton codegen.\n+   - Feel free to bring up pain points in slack.\n+7. Windows Support: Technically not difficult to get a preliminary version. Most of the maintenance burden would come from having to support it when it breaks."}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -439,12 +439,15 @@ struct ConvertLayoutOpConversion\n     }\n     // Potentially we need to store for multiple CTAs in this replication\n     auto accumNumReplicates = product<unsigned>(numReplicates);\n-    // unsigned elems = getTotalElemsPerThread(srcTy);\n     auto vals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n                                                      rewriter, srcTy);\n     unsigned inVec = 0;\n     unsigned outVec = 0;\n     auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n+    if (getElementTypeOrSelf(op.getType()).isa<mlir::Float8E4M3B11FNUZType>()) {\n+      assert(inVec % 4 == 0 && \"conversion not supported for FP8E4M3B15\");\n+      assert(outVec % 4 == 0 && \"conversion not supported for FP8E4M3B15\");\n+    }\n \n     unsigned outElems = getTotalElemsPerThread(dstTy);\n     auto outOrd = getOrder(dstLayout);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "file_content_changes": "@@ -313,6 +313,7 @@ class ConvertTritonGPUToLLVM\n     int threadsPerWarp = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n \n     // Preprocess\n+    decomposeFp8e4b15Convert(mod);\n     decomposeMmaToDotOperand(mod, numWarps, threadsPerWarp);\n     decomposeBlockedToDotOperand(mod);\n     decomposeInsertSliceAsyncOp(mod);\n@@ -442,6 +443,33 @@ class ConvertTritonGPUToLLVM\n                                         allocation.getSharedMemorySize()));\n   }\n \n+  void decomposeFp8e4b15Convert(ModuleOp mod) const {\n+    mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n+      OpBuilder builder(cvtOp);\n+      if (!getElementTypeOrSelf(cvtOp).isa<mlir::Float8E4M3B11FNUZType>())\n+        return;\n+      auto shape = cvtOp.getType().cast<RankedTensorType>().getShape();\n+      auto argEncoding =\n+          cvtOp.getOperand().getType().cast<RankedTensorType>().getEncoding();\n+      auto cvtEncoding = cvtOp.getType().cast<RankedTensorType>().getEncoding();\n+      if (argEncoding.isa<triton::gpu::DotOperandEncodingAttr>() ||\n+          cvtEncoding.isa<triton::gpu::DotOperandEncodingAttr>())\n+        return;\n+      auto F16Ty = builder.getF16Type();\n+\n+      auto newArgType = RankedTensorType::get(shape, F16Ty, argEncoding);\n+      auto newCvtType = RankedTensorType::get(shape, F16Ty, cvtEncoding);\n+      auto newArg = builder.create<mlir::triton::FpToFpOp>(\n+          cvtOp.getLoc(), newArgType, cvtOp.getOperand());\n+      auto newCvt = builder.create<mlir::triton::gpu::ConvertLayoutOp>(\n+          cvtOp.getLoc(), newCvtType, newArg);\n+      auto newRet = builder.create<mlir::triton::FpToFpOp>(\n+          cvtOp.getLoc(), cvtOp.getType(), newCvt.getResult());\n+      cvtOp.replaceAllUsesWith(newRet.getResult());\n+      cvtOp.erase();\n+    });\n+  }\n+\n   void decomposeMmaToDotOperand(ModuleOp mod, int numWarps,\n                                 int threadsPerWarp) const {\n     // Replace `mma -> dot_op` with `mma -> blocked -> dot_op`"}, {"filename": "python/README.md", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 27, "deletions": 2, "changes": 29, "file_content_changes": "@@ -44,6 +44,9 @@ def numpy_random(shape, dtype_str, rs: Optional[RandomState] = None, low=None, h\n         x = rs.randint(low, high, shape, dtype=dtype)\n         x[x == 0] = 1  # Hack. Never return zero so tests of division don't error out.\n         return x\n+    elif dtype_str and 'float8' in dtype_str:\n+        x = rs.randint(20, 40, shape, dtype=np.int8)\n+        return x\n     elif dtype_str in float_dtypes:\n         return rs.normal(0, 1, shape).astype(dtype_str)\n     elif dtype_str == 'bfloat16':\n@@ -67,6 +70,8 @@ def to_triton(x: np.ndarray, device='cuda', dst_type=None) -> Union[TensorWrappe\n         x_signed = x.astype(getattr(np, signed_type_name))\n         return reinterpret(torch.tensor(x_signed, device=device), getattr(tl, t))\n     else:\n+        if dst_type and 'float8' in dst_type:\n+            return reinterpret(torch.tensor(x, device=device), getattr(tl, dst_type))\n         if t == 'float32' and dst_type == 'bfloat16':\n             return torch.tensor(x, device=device).bfloat16()\n         return torch.tensor(x, device=device)\n@@ -1276,6 +1281,20 @@ def serialize_fp8(np_data, in_dtype):\n     else:\n         return np_data\n \n+# inverse of `serialize_fp8`\n+\n+\n+def deserialize_fp8(np_data, in_dtype):\n+    if in_dtype == tl.float8e4b15:\n+        f8x4 = np_data.view(np.uint32)\n+        s = [(f8x4 & (0x80000000 >> i)) << i for i in [0, 16, 1, 17]]\n+        b = [(f8x4 & (0x7f000000 >> i)) << i for i in [1, 17, 8, 24]]\n+        signs = (s[0] >> 0) | (s[1] >> 8) | (s[2] >> 16) | (s[3] >> 24)\n+        bits = (b[0] >> 0) | (b[1] >> 8) | (b[2] >> 16) | (b[3] >> 24)\n+        return (signs | bits).view(np.int8)\n+    else:\n+        return np_data\n+\n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16, torch.float32])\n@@ -1901,7 +1920,7 @@ def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n @pytest.mark.parametrize(\"dtype_str, shape, perm\",\n                          [(dtype, shape, perm)\n                           # TODO: bfloat16\n-                          for dtype in ['float16', 'float32']\n+                          for dtype in ['float8e4b15', 'float16', 'float32']\n                              for shape in [(64, 64), (128, 128)]\n                              for perm in [(1, 0)]])\n def test_permute(dtype_str, shape, perm, device):\n@@ -1930,7 +1949,13 @@ def kernel(X, stride_xm, stride_xn,\n                                     z_tri_contiguous, z_tri_contiguous.stride(0), z_tri_contiguous.stride(1),\n                                     BLOCK_M=shape[0], BLOCK_N=shape[1])\n     # numpy result\n-    z_ref = x.transpose(*perm)\n+    if dtype_str == 'float8e4b15':\n+        ty = tl.float8e4b15\n+        z_ref = serialize_fp8(deserialize_fp8(x, ty).T.copy(), ty)\n+        z_tri = z_tri.base\n+        z_tri_contiguous = z_tri_contiguous.base\n+    else:\n+        z_ref = x.transpose(*perm)\n     # compare\n     np.testing.assert_allclose(to_numpy(z_tri), z_ref)\n     np.testing.assert_allclose(to_numpy(z_tri_contiguous), z_ref)"}, {"filename": "test/Triton/combine.mlir", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,4 +1,3 @@\n-// RUN: triton-opt %s -split-input-file -canonicalize -triton-combine\n // RUN: triton-opt %s -split-input-file -canonicalize -triton-combine | FileCheck %s\n \n // CHECK-LABEL: @test_combine_dot_add_pattern"}]