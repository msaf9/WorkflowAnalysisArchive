[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -70,16 +70,16 @@ jobs:\n           python3 -m pip install --no-build-isolation -vvv '.[tests]'\n           python3 -m pip install pytest-xdist\n \n-      # - name: Run lit tests\n-      #   if: ${{ env.BACKEND == 'CUDA'}}\n-      #   run: |\n-      #     python3 -m pip install lit\n-      #     cd python\n-      #     LIT_TEST_DIR=\"build/$(ls build | grep -i cmake)/test\"\n-      #     if [ ! -d \"${LIT_TEST_DIR}\" ]; then\n-      #       echo \"Coult not find '${LIT_TEST_DIR}'\" ; exit -1\n-      #     fi\n-      #     lit -v \"${LIT_TEST_DIR}\"\n+      - name: Run lit tests\n+        if: ${{ env.BACKEND == 'CUDA'}}\n+        run: |\n+          python3 -m pip install lit\n+          cd python\n+          LIT_TEST_DIR=\"build/$(ls build | grep -i cmake)/test\"\n+          if [ ! -d \"${LIT_TEST_DIR}\" ]; then\n+            echo \"Coult not find '${LIT_TEST_DIR}'\" ; exit -1\n+          fi\n+          lit -v \"${LIT_TEST_DIR}\"\n \n       - name: Enable MMAV3 and TMA\n         if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'H100')}}"}, {"filename": "test/NVGPU/test_cga.mlir", "status": "modified", "additions": 13, "deletions": 13, "changes": 26, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-translate %s | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --convert-nv-gpu-to-llvm | FileCheck %s\n #SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 2 : i32} {\n   tt.func @test_mbarrier() {\n@@ -7,24 +7,24 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 2 :\n     %pred = arith.constant 1 : i1\n     %id0 = arith.constant 0 : i32\n     %id1 = arith.constant 1 : i32\n-    // CHECK: call void @__nv_cga_barrier_sync()\n-    // CHECK: call void @__nv_cga_barrier_arrive()\n-    // CHECK: call void @__nv_cga_barrier_wait()\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n     nvgpu.cga_barrier_sync\n     nvgpu.cga_barrier_arrive\n     nvgpu.cga_barrier_wait\n \n     %ptr = llvm.mlir.null : !llvm.ptr<i32, 3>\n \n-    // CHECK: %[[X:.+]] = tail call i32 asm \"mov.u32 $0, %cluster_ctaid.x;\", \"=r\"()\n-    // CHECK: %[[Y:.+]] = tail call i32 asm \"mov.u32 $0, %cluster_ctaid.y;\", \"=r\"()\n-    // CHECK: %[[Z:.+]] = tail call i32 asm \"mov.u32 $0, %cluster_ctaid.z;\", \"=r\"()\n-    // CHECK: %[[NX:.+]] = tail call i32 asm \"mov.u32 $0, %cluster_nctaid.x;\", \"=r\"()\n-    // CHECK: %[[NY:.+]] = tail call i32 asm \"mov.u32 $0, %cluster_nctaid.y;\", \"=r\"()\n-    // CHECK: %[[A0:.+]] = mul i32 %[[NY]], %[[Z]]\n-    // CHECK: %[[A1:.+]] = add i32 %[[A0]], %[[Y]]\n-    // CHECK: %[[A2:.+]] = mul i32 %[[A1]], %[[NX]]\n-    // CHECK: add i32 %[[A2]], %[[X]]\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.mul\n+    // CHECK: llvm.add\n+    // CHECK: llvm.mul\n+    // CHECK: llvm.add\n     %v = nvgpu.cluster_id\n     llvm.store %v, %ptr : !llvm.ptr<i32, 3>\n "}, {"filename": "test/NVGPU/test_mbarrier.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -1,18 +1,18 @@\n-// RUN: triton-translate %s | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --convert-nv-gpu-to-llvm | FileCheck %s\n #SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32,  \"triton_gpu.num-ctas\" = 2 : i32} {\n   tt.func @test_mbarrier() {\n     %mbarrier = llvm.mlir.null : !llvm.ptr<i64, 3>\n     %pred = arith.constant 1 : i1\n-    // CHECK: call void @__nv_mbarrier_init\n+    // CHECK: llvm.inline_asm\n     nvgpu.mbarrier_init %mbarrier, %pred { count = 32 : i32 } : !llvm.ptr<i64, 3>\n-    // CHECK: call void @__nv_mbarrier_arrive_cp_async\n+    // CHECK: llvm.inline_asm\n     nvgpu.mbarrier_arrive %mbarrier, %pred {arriveType = 1 : i32}: !llvm.ptr<i64, 3>\n-    // CHECK: call void @__nv_mbarrier_arrive_normal\n+    // CHECK: llvm.inline_asm\n     nvgpu.mbarrier_arrive %mbarrier, %pred {arriveType = 0 : i32}: !llvm.ptr<i64, 3>\n-    // CHECK: call void @__nv_mbarrier_arrive_expect_tx\n+    // CHECK: llvm.inline_asm\n     nvgpu.mbarrier_arrive %mbarrier, %pred {arriveType = 2 : i32, txCount = 128 : i32}: !llvm.ptr<i64, 3>\n-    // CHECK: call void @__nv_mbarrier_wait\n+    // CHECK: llvm.inline_asm\n     nvgpu.mbarrier_wait %mbarrier, %pred : !llvm.ptr<i64, 3>, i1\n     tt.return\n   }"}, {"filename": "test/NVGPU/test_tma.mlir", "status": "modified", "additions": 6, "deletions": 21, "changes": 27, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-translate %s | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --convert-nv-gpu-to-llvm | FileCheck %s\n #SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32,  \"triton_gpu.num-ctas\" = 2 : i32} {\n   tt.func @test_tma(%im2colOffsets0 : !llvm.struct<(i16, i16)>, %im2colOffsets1 : !llvm.struct<(i16, i16, i16)>) {\n@@ -14,30 +14,15 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32,  \"triton_gpu.num-ctas\" = 2\n     %pred = arith.constant 1 : i1\n     %mask = arith.constant 15 : i16\n \n-    // CHECK: void @__nv_tma_load_tiled_3d\n-    // CHECK: void @__nv_tma_load_tiled_5d\n+    // CHECK: llvm.inline_asm {{.*}} cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n+    // CHECK: llvm.inline_asm {{.*}} cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n     nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1 {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 2, 0>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32\n-    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2 {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 3, 0>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32\n     nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %c3 {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 4, 0>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i32\n-    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %c3, %c4 {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 5, 0>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i32, i32\n \n-    // CHECK: void @__nv_tma_load_tiled_mcast_3d\n-    // CHECK: void @__nv_tma_load_tiled_mcast_4d\n-    // CHECK: void @__nv_tma_load_tiled_mcast_5d\n+    // CHECK: llvm.inline_asm {{.*}} cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster.L2::cache_hint\n+    // CHECK: llvm.inline_asm {{.*}} cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n     nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %mask {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 2, 1>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i16\n-    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %mask {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 3, 1>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i16\n-    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %c3, %mask {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 4, 1>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i32, i16\n-    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %c3, %c4, %mask {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 5, 1>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i32, i32, i16\n-\n-    // CHECK: tail call void @__nv_tma_load_im2col_4d\n-    // CHECK: tail call void @__nv_tma_load_im2col_5d\n-    // CHECK: tail call void @__nv_tma_load_im2col_mcast_4d\n-    // CHECK: tail call void @__nv_tma_load_im2col_mcast_5d\n-    nvgpu.tma_load_im2col %dst, %mbarrier, %tmaDesc, %l2desc, %im2colOffsets0, %pred, %c0, %c1, %c2, %c3 {mcastMask = 0 : i16}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, !llvm.struct<(i16, i16)>, i1, i32, i32, i32, i32\n-    nvgpu.tma_load_im2col %dst, %mbarrier, %tmaDesc, %l2desc, %im2colOffsets1, %pred, %c0, %c1, %c2, %c3, %c4 {mcastMask = 0 : i16}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, !llvm.struct<(i16, i16, i16)>, i1, i32, i32, i32, i32, i32\n-\n-    nvgpu.tma_load_im2col %dst, %mbarrier, %tmaDesc, %l2desc, %im2colOffsets0, %pred, %c0, %c1, %c2, %c3 {mcastMask = 1 : i16}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, !llvm.struct<(i16, i16)>, i1, i32, i32, i32, i32\n-    nvgpu.tma_load_im2col %dst, %mbarrier, %tmaDesc, %l2desc, %im2colOffsets1, %pred, %c0, %c1, %c2, %c3, %c4 {mcastMask = 1 : i16}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, !llvm.struct<(i16, i16, i16)>, i1, i32, i32, i32, i32, i32\n+    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %c3 {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 4, 0>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i32\n \n     tt.return\n   }"}, {"filename": "test/NVGPU/test_wgmma.mlir", "status": "modified", "additions": 34, "deletions": 5, "changes": 39, "file_content_changes": "@@ -1,16 +1,45 @@\n-// RUN: triton-translate %s | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --convert-nv-gpu-to-llvm | FileCheck %s\n #SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32,  \"triton_gpu.num-ctas\" = 2 : i32} {\n   tt.func @test_tma(%opC : !llvm.struct<(f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32)>) {\n     %buffer = llvm.mlir.null : !llvm.ptr<i64, 3>\n     %height = arith.constant 16 : i32\n-    // CHECK: call i64 @__nv_get_wgmma_desc\n+    // CHECK: llvm.ptrtoint\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.zext\n+    // CHECK: llvm.mul\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.or\n     %descA = nvgpu.wgmma_desc_create %buffer, %height {mode = 2 : i32}: (!llvm.ptr<i64, 3>, i32) -> (i64)\n-    // CHECK: call i64 @__nv_get_wgmma_desc\n+    // CHECK: llvm.ptrtoint\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.zext\n+    // CHECK: llvm.mul\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.or\n     %descB = nvgpu.wgmma_desc_create %buffer, %height {mode = 2 : i32}: (!llvm.ptr<i64, 3>, i32) -> (i64)\n \n-    // CHECK: wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16\n-    %acc0 = nvgpu.wgmma %descA, %descA, %opC {m=64:i32, n=64:i32, k=16:i32, eltTypeC=7:i32, eltTypeA=4:i32, eltTypeB=4:i32, layoutA=0:i32, layoutB=0:i32} : (i64, i64, !llvm.struct<(f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32)>) -> (!llvm.struct<(f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32)>)\n+    // CHECK-COUNT-32: llvm.extractvalue\n+    // CHECK: llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {$0,$1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$20,$21,$22,$23,$24,$25,$26,$27,$28,$29,$30,$31}, $64, $65, 1, 1, 1, 0, 1;\"\n+    %acc0 = nvgpu.wgmma %descA, %descB, %opC {m=64:i32, n=64:i32, k=16:i32, eltTypeC=7:i32, eltTypeA=4:i32, eltTypeB=4:i32, layoutA=0:i32, layoutB=0:i32} : (i64, i64, !llvm.struct<(f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32)>) -> (!llvm.struct<(f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32)>)\n     tt.return\n   }\n } // end module"}]