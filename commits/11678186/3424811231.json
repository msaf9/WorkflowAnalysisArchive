[{"filename": "CMakeLists.txt", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -15,6 +15,10 @@ endif()\n option(TRITON_BUILD_TUTORIALS \"Build C++ Triton tutorials\" ON)\n option(TRITON_BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n \n+# Ensure Python3 vars are set correctly\n+#  used conditionally in this file and by lit tests\n+find_package(Python3 REQUIRED COMPONENTS Development Interpreter)\n+\n # Default build type\n if(NOT CMAKE_BUILD_TYPE)\n   message(STATUS \"Default build type: Release\")\n@@ -133,24 +137,22 @@ endif()\n if(TRITON_BUILD_PYTHON_MODULE)\n     message(STATUS \"Adding Python module\")\n     set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n+    set(PYTHON_SRC ${PYTHON_SRC_PATH}/main.cc ${PYTHON_SRC_PATH}/triton.cc)\n     include_directories(\".\" ${PYTHON_SRC_PATH})\n     if (PYTHON_INCLUDE_DIRS)\n       include_directories(${PYTHON_INCLUDE_DIRS})\n     else()\n-      find_package(Python3 REQUIRED COMPONENTS Development)\n       include_directories(${Python3_INCLUDE_DIRS})\n       link_directories(${Python3_LIBRARY_DIRS})\n       link_libraries(${Python3_LIBRARIES})\n       add_link_options(${Python3_LINK_OPTIONS})\n     endif()\n-    set(PYTHON_SRC ${PYTHON_SRC_PATH}/main.cc ${PYTHON_SRC_PATH}/triton.cc)\n endif()\n \n \n # # Triton\n # file(GLOB_RECURSE LIBTRITON_SRC lib/*.cc)\n # if (WIN32 AND TRITON_BUILD_PYTHON_MODULE)\n-#     find_package(Python3 REQUIRED COMPONENTS Development)\n #     Python3_add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n #     set_target_properties(triton PROPERTIES SUFFIX \".pyd\")\n #     set_target_properties(triton PROPERTIES PREFIX \"lib\")"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -21,11 +21,11 @@ template <typename Int> Int product(llvm::ArrayRef<Int> arr) {\n template <typename Int> Int ceil(Int m, Int n) { return (m + n - 1) / n; }\n \n // output[i] = input[order[i]]\n-template <typename T>\n-SmallVector<T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n+template <typename T, typename RES_T = T>\n+SmallVector<RES_T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n   size_t rank = order.size();\n   assert(input.size() == rank);\n-  SmallVector<T> result(rank);\n+  SmallVector<RES_T> result(rank);\n   for (auto it : llvm::enumerate(order)) {\n     result[it.index()] = input[it.value()];\n   }"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -77,7 +77,7 @@ SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op) {\n   auto srcShape = srcTy.getShape();\n   auto axis = op.axis();\n \n-  bool fastReduce = axis == 1; // FIXME(Qingyi): The fastest-changing dimension\n+  bool fastReduce = axis == srcLayout.getOrder()[0];\n \n   SmallVector<unsigned> smemShape;\n   for (auto d : srcShape)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 290, "deletions": 245, "changes": 535, "file_content_changes": "@@ -83,6 +83,11 @@ static Value createLLVMIntegerConstant(OpBuilder &builder, Location loc,\n \n } // namespace\n \n+// A helper function for using printf in LLVM conversion.\n+void llPrintf(StringRef msg, ValueRange args,\n+              ConversionPatternRewriter &rewriter);\n+\n+// Shortcuts for some commonly used LLVM ops to keep code simple and intuitive//\n // Shortcuts for some commonly used LLVM ops to keep code simple and intuitive\n #define zext(...) rewriter.create<LLVM::ZExtOp>(loc, __VA_ARGS__)\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n@@ -139,6 +144,10 @@ static Value createLLVMIntegerConstant(OpBuilder &builder, Location loc,\n   LLVM::createIndexConstant(rewriter, loc, this->getTypeConverter(),           \\\n                             __VA_ARGS__)\n \n+// Helper function\n+#define tid_val() getThreadId(rewriter, loc)\n+#define llprintf(fmt, ...) LLVM::llPrintf(fmt, {__VA_ARGS__}, rewriter)\n+\n } // namespace LLVM\n } // namespace mlir\n \n@@ -338,6 +347,7 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n   return llvmStruct;\n }\n \n+// Delinearize on compile-time consts, assuming the order is [n, .. 2, 1, 0]\n template <typename T>\n static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n   // shape: {a, b, c, d}  ->  accMul: {b*c*d, c*d, d, 1}\n@@ -355,6 +365,7 @@ static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n   return multiDimIndex;\n }\n \n+// Linearize on compile-time consts, assuming the order is [n, .. 2, 1, 0]\n template <typename T>\n static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   assert(multiDimIndex.size() == shape.size());\n@@ -510,12 +521,12 @@ class ConvertTritonGPUOpToLLVMPattern\n       multiDim[0] = linear;\n     } else {\n       Value remained = linear;\n-      for (auto &&en : llvm::enumerate(llvm::reverse(shape.drop_front()))) {\n+      for (auto &&en : llvm::enumerate(shape.drop_back())) {\n         Value dimSize = idx_val(en.value());\n-        multiDim[rank - 1 - en.index()] = urem(remained, dimSize);\n+        multiDim[en.index()] = urem(remained, dimSize);\n         remained = udiv(remained, dimSize);\n       }\n-      multiDim[0] = remained;\n+      multiDim[rank - 1] = remained;\n     }\n     return multiDim;\n   }\n@@ -525,9 +536,9 @@ class ConvertTritonGPUOpToLLVMPattern\n     int rank = multiDim.size();\n     Value linear = idx_val(0);\n     if (rank > 0) {\n-      linear = multiDim.front();\n+      linear = multiDim.back();\n       for (auto [dim, shape] :\n-           llvm::zip(multiDim.drop_front(), shape.drop_front())) {\n+           llvm::reverse(llvm::zip(multiDim.drop_back(), shape.drop_back()))) {\n         Value dimSize = idx_val(shape);\n         linear = add(mul(linear, dimSize), dim);\n       }\n@@ -566,6 +577,7 @@ class ConvertTritonGPUOpToLLVMPattern\n         delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n     SmallVector<Value> multiDimThreadId =\n         delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n+\n     SmallVector<Value> multiDimBase(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n       // Wrap around multiDimWarpId/multiDimThreadId incase\n@@ -1362,7 +1374,9 @@ struct ReduceOpConversion\n LogicalResult\n ReduceOpConversion::matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter) const {\n-  if (op.axis() == 1) // FIXME(Qingyi): The fastest-changing dimension\n+  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+  auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  if (op.axis() == srcLayout.getOrder()[0])\n     return matchAndRewriteFast(op, adaptor, rewriter);\n   return matchAndRewriteBasic(op, adaptor, rewriter);\n }\n@@ -1444,6 +1458,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n \n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n   auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto srcOrd = srcLayout.getOrder();\n   auto srcShape = srcTy.getShape();\n \n   auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n@@ -1487,16 +1502,21 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     SmallVector<Value> writeIdx = indices[key];\n \n     writeIdx[axis] = udiv(writeIdx[axis], sizePerThread);\n-    Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n+    Value writeOffset =\n+        linearize(rewriter, loc, reorder<Value>(writeIdx, srcOrd),\n+                  reorder<unsigned>(smemShape, srcOrd));\n     Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n     store(acc, writePtr);\n \n     SmallVector<Value> readIdx(writeIdx.size(), ints[0]);\n     for (int N = smemShape[axis] / 2; N > 0; N >>= 1) {\n       readIdx[axis] = ints[N];\n       Value readMask = icmp_slt(writeIdx[axis], ints[N]);\n-      Value readOffset = select(\n-          readMask, linearize(rewriter, loc, readIdx, smemShape), ints[0]);\n+      Value readOffset =\n+          select(readMask,\n+                 linearize(rewriter, loc, reorder<Value>(readIdx, srcOrd),\n+                           reorder<unsigned>(smemShape, srcOrd)),\n+                 ints[0]);\n       Value readPtr = gep(elemPtrTy, writePtr, readOffset);\n       barrier();\n       accumulate(rewriter, loc, op.redOp(), acc, load(readPtr), false);\n@@ -1519,7 +1539,9 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     for (unsigned i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       readIdx.insert(readIdx.begin() + axis, ints[0]);\n-      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n+      Value readOffset =\n+          linearize(rewriter, loc, reorder<Value>(readIdx, srcOrd),\n+                    reorder<unsigned>(smemShape, srcOrd));\n       Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n       resultVals[i] = load(readPtr);\n     }\n@@ -1548,6 +1570,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n   auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n   auto srcShape = srcTy.getShape();\n+  auto srcRank = srcTy.getRank();\n \n   auto threadsPerWarp = srcLayout.getThreadsPerWarp();\n   auto warpsPerCTA = srcLayout.getWarpsPerCTA();\n@@ -1592,6 +1615,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n       delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n   SmallVector<Value> multiDimWarpId =\n       delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+\n   Value laneIdAxis = multiDimLaneId[axis];\n   Value warpIdAxis = multiDimWarpId[axis];\n \n@@ -1609,56 +1633,77 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n       accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n     }\n \n-    if (sizeInterWarps == 1) {\n-      SmallVector<Value> writeIdx = indices[key];\n-      writeIdx[axis] = zero;\n-      Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n-      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n-      storeShared(rewriter, loc, writePtr, acc, laneZero);\n-    } else {\n-      SmallVector<Value> writeIdx = indices[key];\n-      writeIdx[axis] =\n-          warpIdAxis; // axis must be the fastest-changing dimension\n-      Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n-      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n-      storeShared(rewriter, loc, writePtr, acc, laneZero);\n-      barrier();\n+    SmallVector<Value> writeIdx = indices[key];\n+    writeIdx[axis] = (sizeInterWarps == 1) ? zero : warpIdAxis;\n+    Value writeOffset =\n+        linearize(rewriter, loc, reorder<Value>(writeIdx, order),\n+                  reorder<unsigned>(smemShape, order));\n+    Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+    storeShared(rewriter, loc, writePtr, acc, laneZero);\n+  }\n \n-      SmallVector<Value> readIdx = writeIdx;\n-      readIdx[axis] = urem(laneId, i32_val(sizeInterWarps));\n-      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n-      Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n-      acc = load(readPtr);\n+  barrier();\n \n-      // reduce across warps\n-      for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n-        Value shfl = shflSync(rewriter, loc, acc, N);\n-        accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n-      }\n+  // the second round of shuffle reduction\n+  //   now the problem size: sizeInterWarps, s1, s2, .. , sn  =>\n+  //                                      1, s1, s2, .. , sn\n+  //   where sizeInterWarps is 2^m\n+  //\n+  // each thread needs to process:\n+  //   elemsPerThread = sizeInterWarps * s1 * s2 .. Sn / numThreads\n+  unsigned elems = product<unsigned>(smemShape);\n+  unsigned numThreads = product<unsigned>(srcLayout.getWarpsPerCTA()) * 32;\n+  unsigned elemsPerThread = std::max<unsigned>(elems / numThreads, 1);\n+  Value readOffset = threadId;\n+  for (unsigned round = 0; round < elemsPerThread; ++round) {\n+    Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n+    Value acc = load(readPtr);\n+\n+    for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n+      Value shfl = shflSync(rewriter, loc, acc, N);\n+      accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n+    }\n \n-      writeIdx[axis] = zero;\n-      writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n-      writePtr = gep(elemPtrTy, smemBase, writeOffset);\n-      storeShared(rewriter, loc, writePtr, acc, and_(laneZero, warpZero));\n+    Value writeOffset = udiv(readOffset, i32_val(sizeInterWarps));\n+    Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+    Value threadIsNeeded = icmp_slt(threadId, i32_val(elems));\n+    Value laneIdModSizeInterWarps = urem(laneId, i32_val(sizeInterWarps));\n+    Value laneIdModSizeInterWarpsIsZero =\n+        icmp_eq(laneIdModSizeInterWarps, zero);\n+    storeShared(rewriter, loc, writePtr, acc,\n+                and_(threadIsNeeded, laneIdModSizeInterWarpsIsZero));\n+\n+    if (round != elemsPerThread - 1) {\n+      readOffset = add(readOffset, i32_val(numThreads));\n     }\n   }\n \n+  // We could avoid this barrier in some of the layouts, however this is not\n+  // the general case. TODO: optimize the barrier incase the layouts are\n+  // accepted.\n+  barrier();\n+\n   // set output values\n   if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n     // nd-tensor where n >= 1\n     auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n     auto resultShape = resultTy.getShape();\n+    SmallVector<unsigned> resultOrd;\n+    for (auto ord : order) {\n+      if (ord != 0)\n+        resultOrd.push_back(ord - 1);\n+    }\n \n     unsigned resultElems = getElemsPerThread(resultTy);\n     auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultShape);\n     assert(resultIndices.size() == resultElems);\n \n-    barrier();\n     SmallVector<Value> resultVals(resultElems);\n     for (size_t i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n-      readIdx.insert(readIdx.begin() + axis, i32_val(0));\n-      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n+      Value readOffset =\n+          linearize(rewriter, loc, reorder<Value>(readIdx, resultOrd),\n+                    reorder<int64_t, unsigned>(resultShape, resultOrd));\n       Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n       resultVals[i] = load(readPtr);\n     }\n@@ -1670,7 +1715,6 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n     rewriter.replaceOp(op, ret);\n   } else {\n     // 0d-tensor -> scalar\n-    barrier();\n     Value resultVal = load(smemBase);\n     rewriter.replaceOp(op, resultVal);\n   }\n@@ -1707,6 +1751,191 @@ struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n   }\n };\n \n+struct PrintfOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintfOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::PrintfOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::PrintfOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op->getLoc();\n+    SmallVector<Value, 16> operands;\n+    for (auto operand : adaptor.getOperands()) {\n+      auto sub_operands = this->getElementsFromStruct(loc, operand, rewriter);\n+      for (auto elem : sub_operands) {\n+        operands.push_back(elem);\n+      }\n+    }\n+    std::string formatStr;\n+    llvm::raw_string_ostream os(formatStr);\n+    os << op.prefix();\n+    if (operands.size() > 0) {\n+      os << getFormatSubstr(operands[0]);\n+    }\n+\n+    for (size_t i = 1; i < operands.size(); ++i) {\n+      os << \", \" << getFormatSubstr(operands[i]);\n+    }\n+    llPrintf(formatStr, operands, rewriter);\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+  // get format specific for each input value\n+  // currently support pointer, i8, i16, i32, i64, f16, bf16, f32, f64\n+  std::string getFormatSubstr(Value value) const {\n+    Type type = value.getType();\n+    unsigned width = type.getIntOrFloatBitWidth();\n+\n+    if (type.isa<LLVM::LLVMPointerType>()) {\n+      return \"%p\";\n+    } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n+      return \"%f\";\n+    } else if (type.isSignedInteger()) {\n+      return \"%i\";\n+    } else if (type.isUnsignedInteger() || type.isSignlessInteger()) {\n+      return \"%u\";\n+    }\n+    assert(false && \"not supported type\");\n+  }\n+\n+  // declare vprintf(i8*, i8*) as external function\n+  static LLVM::LLVMFuncOp\n+  getVprintfDeclaration(ConversionPatternRewriter &rewriter) {\n+    auto moduleOp =\n+        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    StringRef funcName(\"vprintf\");\n+    Operation *funcOp = moduleOp.lookupSymbol(funcName);\n+    if (funcOp)\n+      return cast<LLVM::LLVMFuncOp>(*funcOp);\n+\n+    auto *context = rewriter.getContext();\n+\n+    SmallVector<Type> argsType{ptr_ty(IntegerType::get(context, 8)),\n+                               ptr_ty(IntegerType::get(context, 8))};\n+    auto funcType = LLVM::LLVMFunctionType::get(i32_ty, argsType);\n+\n+    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+    rewriter.setInsertionPointToStart(moduleOp.getBody());\n+\n+    return rewriter.create<LLVM::LLVMFuncOp>(UnknownLoc::get(context), funcName,\n+                                             funcType);\n+  }\n+\n+  // extend integer to int32, extend float to float64\n+  // this comes from vprintf alignment requirements.\n+  static std::pair<Type, Value>\n+  promoteValue(ConversionPatternRewriter &rewriter, Value value) {\n+    auto *context = rewriter.getContext();\n+    auto type = value.getType();\n+    type.dump();\n+    unsigned width = type.getIntOrFloatBitWidth();\n+    Value newOp = value;\n+    Type newType = type;\n+\n+    bool bUnsigned = type.isUnsignedInteger();\n+    if (type.isIntOrIndex() && width < 32) {\n+      if (bUnsigned) {\n+        newType = ui32_ty;\n+        newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n+                                              value);\n+      } else {\n+        newType = i32_ty;\n+        newOp = rewriter.create<LLVM::SExtOp>(UnknownLoc::get(context), newType,\n+                                              value);\n+      }\n+    } else if (type.isBF16() || type.isF16() || type.isF32()) {\n+      newType = f64_ty;\n+      newOp = rewriter.create<LLVM::FPExtOp>(UnknownLoc::get(context), newType,\n+                                             value);\n+    }\n+\n+    return {newType, newOp};\n+  }\n+\n+  static void llPrintf(StringRef msg, ValueRange args,\n+                       ConversionPatternRewriter &rewriter) {\n+    static const char formatStringPrefix[] = \"printfFormat_\";\n+    assert(!msg.empty() && \"printf with empty string not support\");\n+    Type int8Ptr = ptr_ty(i8_ty);\n+\n+    auto *context = rewriter.getContext();\n+    auto moduleOp =\n+        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    auto funcOp = getVprintfDeclaration(rewriter);\n+\n+    Value one = rewriter.create<LLVM::ConstantOp>(\n+        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(1));\n+    Value zero = rewriter.create<LLVM::ConstantOp>(\n+        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(0));\n+\n+    unsigned stringNumber = 0;\n+    SmallString<16> stringConstName;\n+    do {\n+      stringConstName.clear();\n+      (formatStringPrefix + Twine(stringNumber++)).toStringRef(stringConstName);\n+    } while (moduleOp.lookupSymbol(stringConstName));\n+\n+    llvm::SmallString<64> formatString(msg);\n+    formatString.push_back('\\n');\n+    formatString.push_back('\\0');\n+    size_t formatStringSize = formatString.size_in_bytes();\n+    auto globalType = LLVM::LLVMArrayType::get(i8_ty, formatStringSize);\n+\n+    LLVM::GlobalOp global;\n+    {\n+      ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+      rewriter.setInsertionPointToStart(moduleOp.getBody());\n+      global = rewriter.create<LLVM::GlobalOp>(\n+          UnknownLoc::get(context), globalType,\n+          /*isConstant=*/true, LLVM::Linkage::Internal, stringConstName,\n+          rewriter.getStringAttr(formatString));\n+    }\n+\n+    Value globalPtr =\n+        rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(context), global);\n+    Value stringStart =\n+        rewriter.create<LLVM::GEPOp>(UnknownLoc::get(context), int8Ptr,\n+                                     globalPtr, mlir::ValueRange({zero, zero}));\n+\n+    Value bufferPtr =\n+        rewriter.create<LLVM::NullOp>(UnknownLoc::get(context), int8Ptr);\n+\n+    SmallVector<Value, 16> newArgs;\n+    if (args.size() >= 1) {\n+      SmallVector<Type> argTypes;\n+      for (auto arg : args) {\n+        Type newType;\n+        Value newArg;\n+        std::tie(newType, newArg) = promoteValue(rewriter, arg);\n+        argTypes.push_back(newType);\n+        newArgs.push_back(newArg);\n+      }\n+\n+      Type structTy = LLVM::LLVMStructType::getLiteral(context, argTypes);\n+      auto allocated = rewriter.create<LLVM::AllocaOp>(UnknownLoc::get(context),\n+                                                       ptr_ty(structTy), one,\n+                                                       /*alignment=*/0);\n+\n+      for (const auto &entry : llvm::enumerate(newArgs)) {\n+        auto index = rewriter.create<LLVM::ConstantOp>(\n+            UnknownLoc::get(context), i32_ty,\n+            rewriter.getI32IntegerAttr(entry.index()));\n+        auto fieldPtr = rewriter.create<LLVM::GEPOp>(\n+            UnknownLoc::get(context), ptr_ty(argTypes[entry.index()]),\n+            allocated, ArrayRef<Value>{zero, index});\n+        rewriter.create<LLVM::StoreOp>(UnknownLoc::get(context), entry.value(),\n+                                       fieldPtr);\n+      }\n+      bufferPtr = rewriter.create<LLVM::BitcastOp>(UnknownLoc::get(context),\n+                                                   int8Ptr, allocated);\n+    }\n+\n+    ValueRange operands{stringStart, bufferPtr};\n+    rewriter.create<LLVM::CallOp>(UnknownLoc::get(context), funcOp, operands);\n+  }\n+};\n+\n struct MakeRangeOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp> {\n \n@@ -2070,17 +2299,6 @@ struct ConvertLayoutOpConversion\n   }\n \n private:\n-  template <typename T>\n-  SmallVector<T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) const {\n-    size_t rank = order.size();\n-    assert(input.size() == rank);\n-    SmallVector<T> result(rank);\n-    for (auto it : llvm::enumerate(order)) {\n-      result[rank - 1 - it.value()] = input[it.index()];\n-    }\n-    return result;\n-  };\n-\n   // shared memory rd/st for blocked or mma layout with data padding\n   void processReplica(Location loc, ConversionPatternRewriter &rewriter,\n                       bool stNotRd, RankedTensorType type,\n@@ -4707,9 +4925,9 @@ struct InsertSliceAsyncOpConversion\n     auto threadsPerCTA = getThreadsPerCTA(srcBlockedLayout);\n     auto inOrder = srcBlockedLayout.getOrder();\n \n-    // If perPhase * maxPhase > threadsPerCTA, we need to swizzle over\n-    // elements across phases. If perPhase * maxPhase <= threadsPerCTA,\n-    // swizzle is not allowd\n+    // If perPhase * maxPhase > threadsPerCTA, we will have elements\n+    // that share the same tile indices. The index calculation will\n+    // be cached.\n     auto numSwizzleRows = std::max<unsigned>(\n         (perPhase * maxPhase) / threadsPerCTA[inOrder[1]], 1);\n     // A sharedLayout encoding has a \"vec\" parameter.\n@@ -4718,7 +4936,7 @@ struct InsertSliceAsyncOpConversion\n     auto numVecCols = std::max<unsigned>(inVec / outVec, 1);\n \n     auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcShape);\n-    // <<tileVecIdxRow, tileVecIdxCol>, TileOffset>\n+    //  <<tileVecIdxRow, tileVecIdxCol>, TileOffset>\n     DenseMap<std::pair<unsigned, unsigned>, Value> tileOffsetMap;\n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n       // minVec = 2, inVec = 4, outVec = 2\n@@ -4748,12 +4966,14 @@ struct InsertSliceAsyncOpConversion\n         // Example1:\n         // outVec = 2, inVec = 2, minVec = 2\n         // outVec = 2, inVec = 4, minVec = 2\n-        //     | [1 2] [3 4]  ... [15 16] |\n-        //     | [3 4] [5 6]  ... [1 2]   |\n+        //     | [1 2] [3 4] [5 6] ... |\n+        //     | [3 4] [1 2] [7 8] ... |\n+        //     | [5 6] [7 8] [1 2] ... |\n         // Example2:\n         // outVec = 4, inVec = 2, minVec = 2\n-        //     | [1 2 3 4] [5 6 7 8] ... [13 14 15 16] |\n-        //     | [5 6 7 8] [9 10 11 12] ... [1 2 3 4]  |\n+        //     | [1 2 3 4] [5 6 7 8] [9 10 11 12] ... |\n+        //     | [5 6 7 8] [1 2 3 4] [13 14 15 16] ... |\n+        //     | [9 10 11 12] [13 14 15 16] [1 2 3 4] ... |\n         auto srcIdx = srcIndices[tileVecIdxRow * sizePerThread[inOrder[0]]];\n         Value phase = urem(udiv(srcIdx[inOrder[1]], i32_val(perPhase)),\n                            i32_val(maxPhase));\n@@ -4909,190 +5129,6 @@ struct FDivOpConversion\n   }\n };\n \n-struct PrintfOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintfOp> {\n-  using ConvertTritonGPUOpToLLVMPattern<\n-      triton::PrintfOp>::ConvertTritonGPUOpToLLVMPattern;\n-\n-  LogicalResult\n-  matchAndRewrite(triton::PrintfOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    auto loc = op->getLoc();\n-    SmallVector<Value, 16> operands;\n-    for (auto operand : adaptor.getOperands()) {\n-      auto sub_operands = this->getElementsFromStruct(loc, operand, rewriter);\n-      for (auto elem : sub_operands) {\n-        operands.push_back(elem);\n-      }\n-    }\n-    std::string formatStr;\n-    llvm::raw_string_ostream os(formatStr);\n-    os << op.prefix();\n-    if (operands.size() > 0) {\n-      os << getFormatSubstr(operands[0]);\n-    }\n-\n-    for (size_t i = 1; i < operands.size(); ++i) {\n-      os << \", \" << getFormatSubstr(operands[i]);\n-    }\n-    llPrintf(formatStr, operands, rewriter);\n-    rewriter.eraseOp(op);\n-    return success();\n-  }\n-  // get format specific for each input value\n-  // currently support pointer, i8, i16, i32, i64, f16, bf16, f32, f64\n-  std::string getFormatSubstr(Value value) const {\n-    Type type = value.getType();\n-    unsigned width = type.getIntOrFloatBitWidth();\n-\n-    if (type.isa<LLVM::LLVMPointerType>()) {\n-      return \"%p\";\n-    } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n-      return \"%f\";\n-    } else if (type.isSignedInteger()) {\n-      return \"%i\";\n-    } else if (type.isUnsignedInteger() || type.isSignlessInteger()) {\n-      return \"%u\";\n-    }\n-    assert(false && \"not supported type\");\n-  }\n-\n-  // declare vprintf(i8*, i8*) as external function\n-  LLVM::LLVMFuncOp\n-  getVprintfDeclaration(ConversionPatternRewriter &rewriter) const {\n-    auto moduleOp =\n-        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n-    StringRef funcName(\"vprintf\");\n-    Operation *funcOp = moduleOp.lookupSymbol(funcName);\n-    if (funcOp)\n-      return cast<LLVM::LLVMFuncOp>(*funcOp);\n-\n-    auto *context = rewriter.getContext();\n-\n-    SmallVector<Type> argsType{ptr_ty(IntegerType::get(context, 8)),\n-                               ptr_ty(IntegerType::get(context, 8))};\n-    auto funcType = LLVM::LLVMFunctionType::get(i32_ty, argsType);\n-\n-    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n-    rewriter.setInsertionPointToStart(moduleOp.getBody());\n-\n-    return rewriter.create<LLVM::LLVMFuncOp>(UnknownLoc::get(context), funcName,\n-                                             funcType);\n-  }\n-\n-  // extend integer to int32, extend float to float64\n-  // this comes from vprintf alignment requirements.\n-  std::pair<Type, Value> promoteValue(ConversionPatternRewriter &rewriter,\n-                                      Value value) const {\n-    auto *context = rewriter.getContext();\n-    auto type = value.getType();\n-    unsigned width = type.getIntOrFloatBitWidth();\n-    Value newOp = value;\n-    Type newType = type;\n-\n-    bool bUnsigned = type.isUnsignedInteger();\n-    if (type.isIntOrIndex() && width < 32) {\n-      if (bUnsigned) {\n-        newType = ui32_ty;\n-        newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n-                                              value);\n-      } else {\n-        newType = i32_ty;\n-        newOp = rewriter.create<LLVM::SExtOp>(UnknownLoc::get(context), newType,\n-                                              value);\n-      }\n-    } else if (type.isBF16() || type.isF16() || type.isF32()) {\n-      newType = f64_ty;\n-      newOp = rewriter.create<LLVM::FPExtOp>(UnknownLoc::get(context), newType,\n-                                             value);\n-    }\n-\n-    return {newType, newOp};\n-  }\n-\n-  void llPrintf(StringRef msg, ValueRange args,\n-                ConversionPatternRewriter &rewriter) const {\n-    static const char formatStringPrefix[] = \"printfFormat_\";\n-    assert(!msg.empty() && \"printf with empty string not support\");\n-    Type int8Ptr = ptr_ty(i8_ty);\n-\n-    auto *context = rewriter.getContext();\n-    auto moduleOp =\n-        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n-    auto funcOp = getVprintfDeclaration(rewriter);\n-\n-    Value one = rewriter.create<LLVM::ConstantOp>(\n-        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(1));\n-    Value zero = rewriter.create<LLVM::ConstantOp>(\n-        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(0));\n-\n-    unsigned stringNumber = 0;\n-    SmallString<16> stringConstName;\n-    do {\n-      stringConstName.clear();\n-      (formatStringPrefix + Twine(stringNumber++)).toStringRef(stringConstName);\n-    } while (moduleOp.lookupSymbol(stringConstName));\n-\n-    llvm::SmallString<64> formatString(msg);\n-    formatString.push_back('\\n');\n-    formatString.push_back('\\0');\n-    size_t formatStringSize = formatString.size_in_bytes();\n-    auto globalType = LLVM::LLVMArrayType::get(i8_ty, formatStringSize);\n-\n-    LLVM::GlobalOp global;\n-    {\n-      ConversionPatternRewriter::InsertionGuard guard(rewriter);\n-      rewriter.setInsertionPointToStart(moduleOp.getBody());\n-      global = rewriter.create<LLVM::GlobalOp>(\n-          UnknownLoc::get(context), globalType,\n-          /*isConstant=*/true, LLVM::Linkage::Internal, stringConstName,\n-          rewriter.getStringAttr(formatString));\n-    }\n-\n-    Value globalPtr =\n-        rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(context), global);\n-    Value stringStart =\n-        rewriter.create<LLVM::GEPOp>(UnknownLoc::get(context), int8Ptr,\n-                                     globalPtr, mlir::ValueRange({zero, zero}));\n-\n-    Value bufferPtr =\n-        rewriter.create<LLVM::NullOp>(UnknownLoc::get(context), int8Ptr);\n-\n-    SmallVector<Value, 16> newArgs;\n-    if (args.size() >= 1) {\n-      SmallVector<Type> argTypes;\n-      for (auto arg : args) {\n-        Type newType;\n-        Value newArg;\n-        std::tie(newType, newArg) = promoteValue(rewriter, arg);\n-        argTypes.push_back(newType);\n-        newArgs.push_back(newArg);\n-      }\n-\n-      Type structTy = LLVM::LLVMStructType::getLiteral(context, argTypes);\n-      auto allocated = rewriter.create<LLVM::AllocaOp>(UnknownLoc::get(context),\n-                                                       ptr_ty(structTy), one,\n-                                                       /*alignment=*/0);\n-\n-      for (const auto &entry : llvm::enumerate(newArgs)) {\n-        auto index = rewriter.create<LLVM::ConstantOp>(\n-            UnknownLoc::get(context), i32_ty,\n-            rewriter.getI32IntegerAttr(entry.index()));\n-        auto fieldPtr = rewriter.create<LLVM::GEPOp>(\n-            UnknownLoc::get(context), ptr_ty(argTypes[entry.index()]),\n-            allocated, ArrayRef<Value>{zero, index});\n-        rewriter.create<LLVM::StoreOp>(UnknownLoc::get(context), entry.value(),\n-                                       fieldPtr);\n-      }\n-      bufferPtr = rewriter.create<LLVM::BitcastOp>(UnknownLoc::get(context),\n-                                                   int8Ptr, allocated);\n-    }\n-\n-    ValueRange operands{stringStart, bufferPtr};\n-    rewriter.create<LLVM::CallOp>(UnknownLoc::get(context), funcOp, operands);\n-  }\n-};\n-\n void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n                                   AxisInfoAnalysis &axisInfoAnalysis,\n@@ -5297,6 +5333,15 @@ void ConvertTritonGPUToLLVM::initSharedMemory(\n \n namespace mlir {\n \n+namespace LLVM {\n+\n+void llPrintf(StringRef msg, ValueRange args,\n+              ConversionPatternRewriter &rewriter) {\n+  PrintfOpConversion::llPrintf(msg, args, rewriter);\n+}\n+\n+} // namespace LLVM\n+\n TritonLLVMConversionTarget::TritonLLVMConversionTarget(\n     MLIRContext &ctx, mlir::LLVMTypeConverter &typeConverter)\n     : ConversionTarget(ctx) {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 29, "deletions": 35, "changes": 64, "file_content_changes": "@@ -15,6 +15,14 @@ using namespace mlir;\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n+static Type getI1SameShape(Value v) {\n+  Type vType = v.getType();\n+  auto i1Type = IntegerType::get(vType.getContext(), 1);\n+  auto tensorType = vType.cast<RankedTensorType>();\n+  return RankedTensorType::get(tensorType.getShape(), i1Type,\n+                               tensorType.getEncoding());\n+}\n+\n namespace {\n class LoopPipeliner {\n   /// cache forOp we are working on\n@@ -262,13 +270,23 @@ void LoopPipeliner::emitPrologue() {\n           loadStageBuffer[op->getResult(0)] = {loadsBuffer[op->getResult(0)]};\n         }\n         // load => copy async\n-        // TODO: check if the hardware supports async copy\n         if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n+          Value mask = lookupOrDefault(loadOp.mask(), stage);\n+          Value newMask;\n+          if (mask) {\n+            Value splatCond = builder.create<triton::SplatOp>(\n+                mask.getLoc(), mask.getType(), loopCond);\n+            newMask =\n+                builder.create<arith::AndIOp>(mask.getLoc(), mask, splatCond);\n+          } else {\n+            newMask = builder.create<triton::SplatOp>(\n+                loopCond.getLoc(), getI1SameShape(loadOp), loopCond);\n+          }\n+          // TODO: check if the hardware supports async copy\n           newOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n               op->getLoc(), loadsBuffer[loadOp].getType(),\n               lookupOrDefault(loadOp.ptr(), stage),\n-              loadStageBuffer[loadOp][stage], pipelineIterIdx,\n-              lookupOrDefault(loadOp.mask(), stage),\n+              loadStageBuffer[loadOp][stage], pipelineIterIdx, newMask,\n               lookupOrDefault(loadOp.other(), stage), loadOp.cache(),\n               loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n           loadStageBuffer[loadOp].push_back(newOp->getResult(0));\n@@ -287,33 +305,6 @@ void LoopPipeliner::emitPrologue() {\n         }\n       }\n \n-      // If this is a load/async_copy, we need to update the mask\n-      if (Value mask = [&]() {\n-            if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(newOp)) {\n-              return loadOp.mask();\n-            } else if (auto insertSliceAsyncOp =\n-                           llvm::dyn_cast<triton::gpu::InsertSliceAsyncOp>(\n-                               newOp)) {\n-              return insertSliceAsyncOp.mask();\n-            } else {\n-              return mlir::Value();\n-            }\n-          }()) {\n-        // assert(I1 or TensorOf<[I1]>);\n-        OpBuilder::InsertionGuard g(builder);\n-        // TODO: move this out of the loop\n-        builder.setInsertionPoint(newOp);\n-        Value splatCond = builder.create<triton::SplatOp>(\n-            mask.getLoc(), mask.getType(), loopCond);\n-        Value newMask =\n-            builder.create<arith::AndIOp>(mask.getLoc(), mask, splatCond);\n-        // TODO: better way to do this?\n-        if (llvm::isa<triton::LoadOp>(newOp))\n-          newOp->setOperand(1, newMask);\n-        else // InsertSliceAsyncOp\n-          newOp->setOperand(3, newMask);\n-      }\n-\n       // update mapping of results\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n         Value originalResult = op->getResult(dstIdx);\n@@ -332,7 +323,7 @@ void LoopPipeliner::emitPrologue() {\n                 newOp->getResult(dstIdx), stage + 1);\n         }\n       }\n-    }\n+    } // for (Operation *op : orderedDeps)\n \n     pipelineIterIdx = builder.create<arith::AddIOp>(\n         iv.getLoc(), pipelineIterIdx,\n@@ -490,26 +481,29 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n-    // TODO(da): does this work if loadOp has no mask?\n     // update loading mask\n     if (loads.contains(op->getResult(0))) {\n       auto loadOp = llvm::cast<triton::LoadOp>(op);\n       Value mask = loadOp.mask();\n+      Value newMask;\n       if (mask) {\n         Value splatCond = builder.create<triton::SplatOp>(\n             mask.getLoc(), mask.getType(), nextLoopCond);\n-        Value newMask = builder.create<arith::AndIOp>(\n+        newMask = builder.create<arith::AndIOp>(\n             mask.getLoc(), splatCond, nextMapping.lookupOrDefault(mask));\n         // if mask is defined outside the loop, don't update the map more than\n         // once\n         if (!(forOp.isDefinedOutsideOfLoop(mask) && nextMapping.contains(mask)))\n           nextMapping.map(mask, newMask);\n-      }\n+        newMask = nextMapping.lookupOrDefault(loadOp.mask());\n+      } else\n+        newMask = builder.create<triton::SplatOp>(\n+            loadOp.getLoc(), getI1SameShape(loadOp), nextLoopCond);\n       Value insertAsyncOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n           op->getLoc(), loadsBuffer[loadOp].getType(),\n           nextMapping.lookupOrDefault(loadOp.ptr()),\n           newForOp.getRegionIterArgs()[bufferIdx + nextBuffers.size()],\n-          insertSliceIndex, nextMapping.lookupOrDefault(loadOp.mask()),\n+          insertSliceIndex, newMask,\n           nextMapping.lookupOrDefault(loadOp.other()), loadOp.cache(),\n           loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n       nextBuffers.push_back(insertAsyncOp);"}, {"filename": "python/tests/test_reduce.py", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "file_content_changes": "@@ -97,9 +97,7 @@ def test_reduce1d(op, dtype, shape):\n     (op, dtype, shape, axis)\n     for op in ['sum', 'min', 'max']\n     for dtype in dtypes\n-    for shape in [(1, 4), (1, 8), (1, 16), (1, 32), (2, 32), (4, 32)]\n-    # TODO: fix and uncomment\n-    #, (4, 128), (32, 64)]\n+    for shape in [(1, 4), (1, 8), (1, 16), (1, 32), (2, 32), (4, 32), (4, 128), (32, 64)]\n     for axis in [0, 1]\n ]\n \n@@ -128,7 +126,6 @@ def test_reduce2d(op, dtype, shape, axis):\n         golden_z = torch.min(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n     else:\n         golden_z = torch.max(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n-\n     if dtype.is_floating_point and op == 'sum':\n         if shape[axis] >= 256:\n             assert_close(z, golden_z, rtol=0.05, atol=0.1)"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -156,7 +156,7 @@\n \n @triton.autotune(\n     configs=[\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n     ],\n     key=['M', 'N', 'K'],\n )"}, {"filename": "test/Target/tritongpu_to_llvmir.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: python3 -m triton.tools.aot %s --target=llvm-ir | FileCheck %s\n+// RUN: %PYTHON -m triton.tools.aot %s --target=llvm-ir | FileCheck %s\n \n // == LLVM IR check begin ==\n // CHECK-LABEL: ; ModuleID = 'LLVMDialectModule'"}, {"filename": "test/Target/tritongpu_to_ptx.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: python3 -m triton.tools.aot %s --target=ptx --sm=80 --ptx-version=63 | FileCheck %s\n+// RUN: %PYTHON -m triton.tools.aot %s --target=ptx --sm=80 --ptx-version=63 | FileCheck %s\n // CHECK-LABEL: // Generated by LLVM NVPTX Back-End\n // CHECK: .version 6.3\n // CHECK: .target sm_80"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 12, "deletions": 5, "changes": 17, "file_content_changes": "@@ -13,12 +13,19 @@\n // CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n // CHECK-DAG: %[[CONSTANT_2:.*]] = arith.constant 2 : i32\n // CHECK-DAG: %[[CONSTANT_3:.*]] = arith.constant 3 : i32\n+// CHECK-DAG: %[[LOOP_COND_0:.*]] = arith.cmpi slt, %[[LB:.*]], %[[UB:.*]]\n // CHECK: %[[ABUFFER:.*]] = triton_gpu.alloc_tensor\n-// CHECK: %[[A0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]]\n+// CHECK-DAG: %[[LOOP_COND_0_SPLAT_A:.*]] = tt.splat %[[LOOP_COND_0]]\n+// CHECK: %[[A0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]], %[[LOOP_COND_0_SPLAT_A]]\n // CHECK: %[[BBUFFER:.*]] = triton_gpu.alloc_tensor\n-// CHECK: %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]]\n-// CHECK: %[[A1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n-// CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n+// CHECK-DAG: %[[LOOP_COND_0_SPLAT_B:.*]] = tt.splat %[[LOOP_COND_0]]\n+// CHECK: %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]], %[[LOOP_COND_0_SPLAT_B]]\n+// CHECK-DAG: %[[IV_1:.*]] = arith.addi %[[LB]], %[[STEP:.*]]\n+// CHECK-DAG: %[[LOOP_COND_1:.*]] = arith.cmpi slt, %[[IV_1]], %[[UB]]\n+// CHECK-DAG: %[[LOOP_COND_1_SPLAT_A:.*]] = tt.splat %[[LOOP_COND_1]]\n+// CHECK: %[[A1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]], %[[LOOP_COND_1_SPLAT_A]]\n+// CHECK-DAG: %[[LOOP_COND_1_SPLAT_B:.*]] = tt.splat %[[LOOP_COND_1]]\n+// CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]], %[[LOOP_COND_1_SPLAT_B]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n // CHECK: %[[A0:.*]] = tensor.extract_slice %[[A1BUFFER]][0, 0, 0]\n // CHECK: %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n@@ -49,7 +56,7 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n \n   scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n-    %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n+    %a_ = tt.load %a_ptr {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n     %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>"}]