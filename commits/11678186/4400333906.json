[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1148,7 +1148,6 @@ def kernel(X, stride_xm, stride_xn,\n                           for dtype in ['int8', 'float16', 'float32']])\n def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype, device='cuda'):\n     capability = torch.cuda.get_device_capability()\n-    capability = (7, 0)\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if capability[0] < 8:"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1598,9 +1598,9 @@ def compile(fn, **kwargs):\n         \"ttir\": (lambda path: parse_mlir_module(path, context),\n                  lambda src: ast_to_ttir(src, signature, configs[0], constants, debug)),\n         \"ttgir\": (lambda path: parse_mlir_module(path, context),\n-                  lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps), num_stages, 70)),\n+                  lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps), num_stages, capability)),\n         \"llir\": (lambda path: Path(path).read_text(),\n-                 lambda src: ttgir_to_llir(src, extern_libs, 70)),\n+                 lambda src: ttgir_to_llir(src, extern_libs, capability)),\n         \"ptx\": (lambda path: Path(path).read_text(),\n                 lambda src: llir_to_ptx(src, capability)),\n         \"cubin\": (lambda path: Path(path).read_bytes(),"}]