[{"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 10, "deletions": 13, "changes": 23, "file_content_changes": "@@ -155,12 +155,14 @@ struct DotOpMmaV1ConversionHelper {\n   }\n \n   // Loading $a from smem to registers, returns a LLVM::Struct.\n-  Value loadA(Value A, const SharedMemoryObject &smemObj, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const;\n+  Value loadA(Value A, bool transA, const SharedMemoryObject &smemObj,\n+              Value thread, Location loc,\n+              ConversionPatternRewriter &rewriter) const;\n \n   // Loading $b from smem to registers, returns a LLVM::Struct.\n-  Value loadB(Value B, const SharedMemoryObject &smemObj, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const;\n+  Value loadB(Value B, bool transB, const SharedMemoryObject &smemObj,\n+              Value thread, Location loc,\n+              ConversionPatternRewriter &rewriter) const;\n \n   static ArrayRef<unsigned> getOrder() { return mmaOrder; }\n \n@@ -1352,8 +1354,8 @@ struct DotOpFMAConversionHelper {\n };\n \n Value DotOpMmaV1ConversionHelper::loadA(\n-    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n-    ConversionPatternRewriter &rewriter) const {\n+    Value tensor, bool transA, const SharedMemoryObject &smemObj, Value thread,\n+    Location loc, ConversionPatternRewriter &rewriter) const {\n \n   auto *ctx = rewriter.getContext();\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n@@ -1372,8 +1374,6 @@ Value DotOpMmaV1ConversionHelper::loadA(\n   auto [offsetAM, offsetAK, _0, _1] = computeOffsets(\n       thread, isARow, false, fpw, param.spw, param.rep, rewriter, loc);\n \n-  // TODO [Superjomn]: transA cannot be accessed in ConvertLayoutOp.\n-  bool transA = false;\n   if (transA) {\n     std::swap(shape[0], shape[1]);\n     std::swap(offsetAM, offsetAK);\n@@ -1470,8 +1470,8 @@ Value DotOpMmaV1ConversionHelper::loadA(\n }\n \n Value DotOpMmaV1ConversionHelper::loadB(\n-    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n-    ConversionPatternRewriter &rewriter) const {\n+    Value tensor, bool transB, const SharedMemoryObject &smemObj, Value thread,\n+    Location loc, ConversionPatternRewriter &rewriter) const {\n   // smem\n   auto strides = smemObj.strides;\n \n@@ -1496,9 +1496,6 @@ Value DotOpMmaV1ConversionHelper::loadB(\n   int strideRepN = wpt[1] * fpw[1] * 8;\n   int strideRepK = 1;\n \n-  // TODO [Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n-  bool transB = false;\n-\n   auto [_0, _1, offsetBN, offsetBK] = computeOffsets(\n       thread, false, isBRow, fpw, param.spw, param.rep, rewriter, loc);\n   if (transB) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 10, "deletions": 8, "changes": 18, "file_content_changes": "@@ -3408,14 +3408,16 @@ Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n   } else if (!isOuter && mmaLayout.getVersion() == 1 &&\n              isHMMA) { // tensor core v1\n     DotOpMmaV1ConversionHelper helper(mmaLayout);\n-    if (dotOperandLayout.getOpIdx() == 0) {\n-      // operand $a\n-      res =\n-          helper.loadA(src, smemObj, getThreadId(rewriter, loc), loc, rewriter);\n-    } else if (dotOperandLayout.getOpIdx() == 1) {\n-      // operand $b\n-      res =\n-          helper.loadB(src, smemObj, getThreadId(rewriter, loc), loc, rewriter);\n+    if (dotOperandLayout.getOpIdx() == 0) { // operand $a\n+      // TODO[Superjomn]: transA is not available here.\n+      bool transA = false;\n+      res = helper.loadA(src, transA, smemObj, getThreadId(rewriter, loc), loc,\n+                         rewriter);\n+    } else if (dotOperandLayout.getOpIdx() == 1) { // operand $b\n+      // TODO[Superjomn]: transB is not available here.\n+      bool transB = false;\n+      res = helper.loadB(src, transB, smemObj, getThreadId(rewriter, loc), loc,\n+                         rewriter);\n     }\n   } else {\n     assert(false && \"Unsupported mma layout found\");"}]