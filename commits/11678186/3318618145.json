[{"filename": "lib/Dialect/Triton/IR/Traits.cpp", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -46,10 +46,10 @@ mlir::LogicalResult mlir::OpTrait::impl::verifyTensorSize(Operation *op) {\n         return op->emitError(\"Maximum allowed number of elements is \")\n                << maxTensorNumElements << \", but \" << *op\n                << \" has more than that\";\n-      // if ((numElements & (numElements - 1)) != 0)\n-      //   return op->emitError(\"Number of elements must be power-of-two, but \")\n-      //          << *op << \" doesn't follow the rule (\" << numElements << \")\"\n-      //          << \" elements\";\n+      if ((numElements & (numElements - 1)) != 0)\n+        return op->emitError(\"Number of elements must be power-of-two, but \")\n+               << *op << \" doesn't follow the rule (\" << numElements << \")\"\n+               << \" elements\";\n     }\n   }\n   for (auto opType : op->getResultTypes()) {\n@@ -61,10 +61,10 @@ mlir::LogicalResult mlir::OpTrait::impl::verifyTensorSize(Operation *op) {\n         return op->emitError(\"Maximum allowed number of elements is \")\n                << maxTensorNumElements << \", but \" << *op\n                << \" has more than that\";\n-      // if ((numElements & (numElements - 1)) != 0)\n-      //   return op->emitError(\"Number of elements must be power-of-two, but \")\n-      //          << *op << \" doesn't follow the rule (\" << numElements << \")\"\n-      //          << \" elements\";\n+      if ((numElements & (numElements - 1)) != 0)\n+        return op->emitError(\"Number of elements must be power-of-two, but \")\n+               << *op << \" doesn't follow the rule (\" << numElements << \")\"\n+               << \" elements\";\n     }\n   }\n   return success();"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 36, "deletions": 10, "changes": 46, "file_content_changes": "@@ -61,7 +61,9 @@ class Prefetcher {\n   LogicalResult isForOpOperand(Value v);\n \n   Value generatePrefetch(Value v, unsigned opIdx, bool isPrefetch,\n-                         Attribute dotEncoding, OpBuilder &builder);\n+                         Attribute dotEncoding, OpBuilder &builder,\n+                         llvm::Optional<int64_t> offsetK = llvm::None,\n+                         llvm::Optional<int64_t> shapeK = llvm::None);\n \n public:\n   Prefetcher() = delete;\n@@ -78,7 +80,9 @@ class Prefetcher {\n };\n \n Value Prefetcher::generatePrefetch(Value v, unsigned opIdx, bool isPrefetch,\n-                                   Attribute dotEncoding, OpBuilder &builder) {\n+                                   Attribute dotEncoding, OpBuilder &builder,\n+                                   llvm::Optional<int64_t> offsetK,\n+                                   llvm::Optional<int64_t> shapeK) {\n   // opIdx: 0 => a, 1 => b\n   auto type = v.getType().cast<RankedTensorType>();\n   SmallVector<int64_t> shape{type.getShape().begin(), type.getShape().end()};\n@@ -89,9 +93,15 @@ Value Prefetcher::generatePrefetch(Value v, unsigned opIdx, bool isPrefetch,\n \n   // k => (prefetchWidth, k - prefetchWidth)\n   int64_t kIdx = opIdx == 0 ? 1 : 0;\n+\n   offset[kIdx] = isPrefetch ? 0 : prefetchWidth;\n   shape[kIdx] = isPrefetch ? prefetchWidth : (shape[kIdx] - prefetchWidth);\n \n+  if (shapeK)\n+    shape[kIdx] = *shapeK;\n+  if (offsetK)\n+    offset[kIdx] = *offsetK;\n+\n   Value newSmem = builder.create<tensor::ExtractSliceOp>(\n       v.getLoc(),\n       // TODO: encoding?\n@@ -195,6 +205,12 @@ scf::ForOp Prefetcher::createNewForOp() {\n       forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n       forOp.getStep(), loopArgs);\n \n+  auto largestPow2 = [](int64_t n) -> int64_t {\n+    while ((n & (n - 1)) != 0)\n+      n = n & (n - 1);\n+    return n;\n+  };\n+\n   builder.setInsertionPointToStart(newForOp.getBody());\n   BlockAndValueMapping mapping;\n   for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n@@ -216,14 +232,24 @@ scf::ForOp Prefetcher::createNewForOp() {\n             1, newForOp.getRegionIterArgForOpOperand(*b.use_begin()));\n \n       // remaining part\n-      Value aRem = generatePrefetch(mapping.lookup(dot2aLoopArg[dot]), 0, false,\n-                                    dotEncoding, builder);\n-      Value bRem = generatePrefetch(mapping.lookup(dot2bLoopArg[dot]), 1, false,\n-                                    dotEncoding, builder);\n-      newOp = builder.clone(*dot, mapping);\n-      newOp->setOperand(0, aRem);\n-      newOp->setOperand(1, bRem);\n-      newOp->setOperand(2, firstDot->getResult(0));\n+      int64_t kOff = prefetchWidth;\n+      int64_t kRem = dot.a().getType().cast<RankedTensorType>().getShape()[1] -\n+                     prefetchWidth;\n+      Operation *prevDot = firstDot;\n+      while (kRem != 0) {\n+        int64_t kShape = largestPow2(kRem);\n+        Value aRem = generatePrefetch(mapping.lookup(dot2aLoopArg[dot]), 0, false,\n+                                      dotEncoding, builder, kOff, kShape);\n+        Value bRem = generatePrefetch(mapping.lookup(dot2bLoopArg[dot]), 1, false,\n+                                      dotEncoding, builder, kOff, kShape);\n+        newOp = builder.clone(*dot, mapping);\n+        newOp->setOperand(0, aRem);\n+        newOp->setOperand(1, bRem);\n+        newOp->setOperand(2, prevDot->getResult(0));\n+        prevDot = newOp;\n+        kOff += kShape;\n+        kRem -= kShape;\n+      }\n     } else {\n       newOp = builder.clone(op, mapping);\n     }"}]