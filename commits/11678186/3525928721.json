[{"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -86,9 +86,10 @@ void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n       curRegionInfo.syncWriteBuffers.insert(bufferId);\n     }\n   }\n-  // Scratch buffer is considered as a shared memory read\n+  // Scratch buffer is considered as both shared memory write & read\n   auto bufferId = allocation->getBufferId(op);\n   if (bufferId != Allocation::InvalidBufferId) {\n+    curRegionInfo.syncWriteBuffers.insert(bufferId);\n     curRegionInfo.syncReadBuffers.insert(bufferId);\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -3077,7 +3077,8 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n   for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n     auto multiDimRepId =\n         getMultiDimIndex<unsigned>(repId, numReplicates, outOrd);\n-    barrier();\n+    if (repId != 0)\n+      barrier();\n     if (srcLayout.isa<BlockedEncodingAttr>() ||\n         srcLayout.isa<SliceEncodingAttr>() ||\n         srcLayout.isa<MmaEncodingAttr>()) {\n@@ -3169,11 +3170,6 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n   auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n   auto numWordsEachRep = product<unsigned>(wordsInEachRep);\n   SmallVector<Value> wordVecs(numWordsEachRep);\n-  // TODO: We should get less barriers if it is handled by membar pass\n-  //       instead of the backend, since the later can only handle it in\n-  //       the most conservative way. However just keep for now and revisit\n-  //       in the future in case necessary.\n-  barrier();\n   for (unsigned i = 0; i < numElems; ++i) {\n     if (i % srcAccumSizeInThreads == 0) {\n       // start of a replication"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -577,7 +577,6 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_layout_blocked_blocked\n   func @convert_layout_blocked_blocked(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n-    // CHECK: nvvm.barrier0\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n     // CHECK: llvm.store\n@@ -625,7 +624,6 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_layout_blocked_blocked_vec\n   func @convert_layout_blocked_blocked_vec(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n-    // CHECK: nvvm.barrier0\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n     // CHECK: llvm.store\n@@ -649,7 +647,6 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_layout_blocked_blocked_multi_rep\n   func @convert_layout_blocked_blocked_multi_rep(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n-    // CHECK: nvvm.barrier0\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n     // CHECK: nvvm.barrier0\n@@ -717,7 +714,6 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mma_block\n   func @convert_layout_mma_blocked(%arg0: tensor<32x16xf32, #mma>) {\n-    // CHECK: nvvm.barrier0\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n     // CHECK: llvm.store"}]