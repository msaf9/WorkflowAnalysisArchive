[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 13, "deletions": 16, "changes": 29, "file_content_changes": "@@ -8,20 +8,19 @@ on:\n       - triton-mlir\n \n jobs:\n-\n   Runner-Preparation:\n     runs-on: ubuntu-latest\n     outputs:\n       matrix: ${{ steps.set-matrix.outputs.matrix }}\n     steps:\n-    - name: Prepare runner matrix\n-      id: set-matrix\n-      run: |\n-        if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n-          echo '::set-output name=matrix::[[\"self-hosted\", \"A10\"], \"macos-latest\"]'\n-        else\n-          echo '::set-output name=matrix::[\"ubuntu-latest\", \"macos-latest\"]'\n-        fi\n+      - name: Prepare runner matrix\n+        id: set-matrix\n+        run: |\n+          if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n+            echo '::set-output name=matrix::[[\"self-hosted\", \"A10\"], \"macos-10.15\"]'\n+          else\n+            echo '::set-output name=matrix::[\"ubuntu-latest\", \"macos-10.15\"]'\n+          fi\n \n   Integration-Tests:\n     needs: Runner-Preparation\n@@ -33,7 +32,6 @@ jobs:\n         runner: ${{fromJson(needs.Runner-Preparation.outputs.matrix)}}\n \n     steps:\n-\n       - name: Checkout\n         uses: actions/checkout@v2\n \n@@ -42,33 +40,32 @@ jobs:\n           rm -rf ~/.triton/cache/\n \n       - name: Check imports\n-        if: ${{ matrix.runner != 'macos-latest' }}\n+        if: startsWith(matrix.runner, 'ubuntu')\n         run: |\n           pip install isort\n           isort -c ./python || ( echo '::error title=Imports not sorted::Please run \\\"isort ./python\\\"' ; exit 1 )\n \n       - name: Check python style\n-        if: ${{ matrix.runner != 'macos-latest' }}\n+        if: startsWith(matrix.runner, 'ubuntu')\n         run: |\n           pip install autopep8\n           autopep8 -a -r -d --exit-code ./python || ( echo '::error title=Style issues::Please run \\\"autopep8 -a -r -i ./python\\\"' ; exit 1 )\n \n       - name: Check cpp style\n-        if: ${{ matrix.runner != 'macos-latest' }}\n+        if: startsWith(matrix.runner, 'ubuntu')\n         run: |\n           pip install clang-format\n           find . -regex '.*\\.\\(cpp\\|hpp\\|h\\|cc\\)' -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file --dry-run -Werror -i ||\n           (echo '::error title=Style issues:: Please run `find . -regex \".*\\.\\(cpp\\|hpp\\|h\\|cc\\)\" -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file -i`' ; exit 1)\n \n       - name: Flake8\n-        if: ${{ matrix.runner != 'macos-latest' }}\n+        if: startsWith(matrix.runner, 'ubuntu')\n         run: |\n           pip install flake8\n           flake8 --config ./python/setup.cfg ./python || ( echo '::error::Flake8 failed; see logs for errors.' ; exit 1 )\n \n       - name: Install Triton\n         run: |\n-          alias python='python3'\n           cd python\n           TRITON_USE_ASSERT_ENABLED_LLVM=TRUE pip3 install -e '.[tests]'\n \n@@ -82,7 +79,7 @@ jobs:\n           lit -v \"$LIT_TEST_DIR\"\n \n       - name: Run python tests\n-        if: ${{ matrix.runner[0] == 'self-hosted' }}\n+        if: ${{matrix.runner[0] == 'self-hosted'}}\n         run: |\n           cd python/tests\n           pytest"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 31, "deletions": 16, "changes": 47, "file_content_changes": "@@ -30,8 +30,8 @@ class TT_Op<string mnemonic, list<Trait> traits = []> :\n //   fptoui, fptosi, uitofp, sitofp,\n //   extf, tructf,\n //   extui, extsi, tructi\n-def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape, \n-                                         SameOperandsAndResultEncoding, \n+def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n+                                         SameOperandsAndResultEncoding,\n                                          NoSideEffect,\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Cast int64 to pointer\";\n@@ -43,7 +43,7 @@ def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n     let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n }\n \n-def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape, \n+def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n                                          SameOperandsAndResultEncoding,\n                                          NoSideEffect,\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n@@ -57,7 +57,7 @@ def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n }\n \n // arith.bitcast doesn't support pointers\n-def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape, \n+def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n                                      SameOperandsAndResultEncoding,\n                                      NoSideEffect,\n                                      /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n@@ -72,7 +72,7 @@ def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n     // TODO: Add verifier\n }\n \n-def TT_FpToFp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape, \n+def TT_FpToFp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n                                    SameOperandsAndResultEncoding,\n                                    NoSideEffect,\n                                    /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n@@ -99,7 +99,7 @@ def TT_FpToFp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n //\n \n def TT_AddPtrOp : TT_Op<\"addptr\",\n-                     [NoSideEffect, \n+                     [NoSideEffect,\n                      SameOperandsAndResultShape,\n                      SameOperandsAndResultEncoding,\n                       TypesMatchWith<\"result type matches ptr type\",\n@@ -224,7 +224,7 @@ def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [SameOperandsAndResultShape,\n //\n // Shape Manipulation Ops\n //\n-def TT_SplatOp : TT_Op<\"splat\", [NoSideEffect, \n+def TT_SplatOp : TT_Op<\"splat\", [NoSideEffect,\n                                  SameOperandsAndResultElementType]> {\n     let summary = \"splat\";\n \n@@ -237,8 +237,8 @@ def TT_SplatOp : TT_Op<\"splat\", [NoSideEffect,\n     let hasFolder = 1;\n }\n \n-def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect, \n-                                            DeclareOpInterfaceMethods<InferTypeOpInterface>, \n+def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect,\n+                                            DeclareOpInterfaceMethods<InferTypeOpInterface>,\n                                             SameOperandsAndResultElementType]> {\n     let summary = \"expand_dims\";\n \n@@ -249,7 +249,7 @@ def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect,\n     let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n }\n \n-def TT_ViewOp : TT_Op<\"view\", [NoSideEffect, \n+def TT_ViewOp : TT_Op<\"view\", [NoSideEffect,\n                                SameOperandsAndResultElementType]> {\n     let summary = \"view\";\n \n@@ -261,7 +261,7 @@ def TT_ViewOp : TT_Op<\"view\", [NoSideEffect,\n \n }\n \n-def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect, \n+def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect,\n                                          SameOperandsAndResultElementType]> {\n     let summary = \"broadcast. No left-padding as of now.\";\n \n@@ -274,7 +274,7 @@ def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect,\n     let hasFolder = 1;\n }\n \n-def TT_CatOp : TT_Op<\"cat\", [NoSideEffect, \n+def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n                              SameOperandsAndResultElementType]> {\n     let summary = \"concatenate 2 tensors\";\n \n@@ -307,7 +307,7 @@ def TT_GetNumProgramsOp : TT_Op<\"get_num_programs\", [NoSideEffect]> {\n //\n // Dot Op\n //\n-def TT_DotOp : TT_Op<\"dot\", [NoSideEffect, \n+def TT_DotOp : TT_Op<\"dot\", [NoSideEffect,\n                              DeclareOpInterfaceMethods<InferTypeOpInterface>,\n                              TypesMatchWith<\"result's type matches accumulator's type\",\n                                             \"d\", \"c\", \"$_self\">]> {\n@@ -357,12 +357,11 @@ def TT_ExtElemwiseOp : TT_Op<\"ext_elemwise\", [NoSideEffect, Elementwise, SameOpe\n         return $libpath/$libname:$symbol($args...)\n     }];\n \n-    let arguments = (ins Variadic<TT_Tensor>:$args, StrAttr:$libname, StrAttr:$libpath, StrAttr:$symbol);\n+    let arguments = (ins Variadic<TT_Type>:$args, StrAttr:$libname, StrAttr:$libpath, StrAttr:$symbol);\n \n-    let results = (outs TT_Tensor:$result);\n+    let results = (outs TT_Type:$result);\n \n     let assemblyFormat = \"operands attr-dict `:` type(operands) `->` type($result)\";\n-\n }\n \n //\n@@ -385,4 +384,20 @@ def TT_MakeRangeOp : TT_Op<\"make_range\", [NoSideEffect]> {\n     let assemblyFormat = \"attr-dict `:` type($result)\";\n }\n \n+//\n+// Make PrintfOp\n+//\n+def TT_PrintfOp : TT_Op<\"printf\", [MemoryEffects<[MemWrite]>]>,\n+  Arguments<(ins StrAttr:$prefix,\n+                Variadic<AnyTypeOf<[TT_Type]>>:$args)> {\n+  let summary = \"Device-side printf, as in CUDA for debugging\";\n+  let description = [{\n+    `tt.printf` takes a literal string prefix and an arbitrary number of scalar or tensor arguments that should be printed.\n+    format are generated automatically from the arguments.\n+  }];\n+  let assemblyFormat = [{\n+    $prefix attr-dict ($args^ `:` type($args))?\n+  }];\n+}\n+\n #endif // Triton_OPS"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 12, "deletions": 1, "changes": 13, "file_content_changes": "@@ -159,6 +159,16 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n                          newContiguity, newDivisibility, newConstancy);\n   }\n+  // TODO: All other binary ops\n+  if (llvm::isa<arith::AndIOp, arith::OrIOp>(op)) {\n+    auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };\n+    auto newDivisibility = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };\n+    auto newConstancy = [](AxisInfo lhs, AxisInfo rhs, int d) {\n+      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n+    };\n+    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n+                         newContiguity, newDivisibility, newConstancy);\n+  }\n   // Splat\n   if (llvm::isa<triton::SplatOp>(op)) {\n     Type _retTy = *op->result_type_begin();\n@@ -200,7 +210,8 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     for (int d = 0; d < retTy.getRank(); ++d) {\n       contiguity.push_back(opShape[d] == 1 ? 1 : opInfo.getContiguity(d));\n       divisibility.push_back(opInfo.getDivisibility(d));\n-      constancy.push_back(opShape[d] == 1 ? retShape[d] : 1);\n+      constancy.push_back(opShape[d] == 1 ? retShape[d]\n+                                          : opInfo.getConstancy(d));\n     }\n     curr = AxisInfo(contiguity, divisibility, constancy);\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 190, "deletions": 1, "changes": 191, "file_content_changes": "@@ -119,9 +119,11 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n #define barrier() rewriter.create<mlir::gpu::BarrierOp>(loc)\n #define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n #define i32_ty rewriter.getIntegerType(32)\n+#define ui32_ty rewriter.getIntegerType(32, false)\n #define f16_ty rewriter.getF16Type()\n #define i8_ty rewriter.getIntegerType(8)\n #define f32_ty rewriter.getF32Type()\n+#define f64_ty rewriter.getF64Type()\n #define vec_ty(type, num) VectorType::get(num, type)\n #define f32_val(...) LLVM::createConstantF32(loc, rewriter, __VA_ARGS__)\n #define void_ty(ctx) LLVM::LLVMVoidType::get(ctx)\n@@ -691,7 +693,8 @@ Value convertSplatLikeOp(Type elemType, Type resType, Value constVal,\n                          TypeConverter *typeConverter,\n                          ConversionPatternRewriter &rewriter, Location loc) {\n   auto tensorTy = resType.cast<RankedTensorType>();\n-  if (tensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n+  if (tensorTy.getEncoding().isa<BlockedEncodingAttr>() ||\n+      tensorTy.getEncoding().isa<SliceEncodingAttr>()) {\n     auto tensorTy = resType.cast<RankedTensorType>();\n     auto srcType = typeConverter->convertType(elemType);\n     auto llSrc = bitcast(constVal, srcType);\n@@ -1807,6 +1810,7 @@ class ElementwiseOpConversionBase\n     }\n     Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n+\n     return success();\n   }\n \n@@ -4731,6 +4735,190 @@ struct FDivOpConversion\n   }\n };\n \n+struct PrintfOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintfOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::PrintfOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::PrintfOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op->getLoc();\n+    SmallVector<Value, 16> operands;\n+    for (auto operand : adaptor.getOperands()) {\n+      auto sub_operands = this->getElementsFromStruct(loc, operand, rewriter);\n+      for (auto elem : sub_operands) {\n+        operands.push_back(elem);\n+      }\n+    }\n+    std::string formatStr;\n+    llvm::raw_string_ostream os(formatStr);\n+    os << op.prefix();\n+    if (operands.size() > 0) {\n+      os << getFormatSubstr(operands[0]);\n+    }\n+\n+    for (size_t i = 1; i < operands.size(); ++i) {\n+      os << \", \" << getFormatSubstr(operands[i]);\n+    }\n+    llPrintf(formatStr, operands, rewriter);\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+  // get format specific for each input value\n+  // currently support pointer, i8, i16, i32, i64, f16, bf16, f32, f64\n+  std::string getFormatSubstr(Value value) const {\n+    Type type = value.getType();\n+    unsigned width = type.getIntOrFloatBitWidth();\n+\n+    if (type.isa<LLVM::LLVMPointerType>()) {\n+      return \"%p\";\n+    } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n+      return \"%f\";\n+    } else if (type.isSignedInteger()) {\n+      return \"%i\";\n+    } else if (type.isUnsignedInteger() || type.isSignlessInteger()) {\n+      return \"%u\";\n+    }\n+    assert(false && \"not supported type\");\n+  }\n+\n+  // declare vprintf(i8*, i8*) as external function\n+  LLVM::LLVMFuncOp\n+  getVprintfDeclaration(ConversionPatternRewriter &rewriter) const {\n+    auto moduleOp =\n+        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    StringRef funcName(\"vprintf\");\n+    Operation *funcOp = moduleOp.lookupSymbol(funcName);\n+    if (funcOp)\n+      return cast<LLVM::LLVMFuncOp>(*funcOp);\n+\n+    auto *context = rewriter.getContext();\n+\n+    SmallVector<Type> argsType{ptr_ty(IntegerType::get(context, 8)),\n+                               ptr_ty(IntegerType::get(context, 8))};\n+    auto funcType = LLVM::LLVMFunctionType::get(i32_ty, argsType);\n+\n+    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+    rewriter.setInsertionPointToStart(moduleOp.getBody());\n+\n+    return rewriter.create<LLVM::LLVMFuncOp>(UnknownLoc::get(context), funcName,\n+                                             funcType);\n+  }\n+\n+  // extend integer to int32, extend float to float64\n+  // this comes from vprintf alignment requirements.\n+  std::pair<Type, Value> promoteValue(ConversionPatternRewriter &rewriter,\n+                                      Value value) const {\n+    auto *context = rewriter.getContext();\n+    auto type = value.getType();\n+    unsigned width = type.getIntOrFloatBitWidth();\n+    Value newOp = value;\n+    Type newType = type;\n+\n+    bool bUnsigned = type.isUnsignedInteger();\n+    if (type.isIntOrIndex() && width < 32) {\n+      if (bUnsigned) {\n+        newType = ui32_ty;\n+        newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n+                                              value);\n+      } else {\n+        newType = i32_ty;\n+        newOp = rewriter.create<LLVM::SExtOp>(UnknownLoc::get(context), newType,\n+                                              value);\n+      }\n+    } else if (type.isBF16() || type.isF16() || type.isF32()) {\n+      newType = f64_ty;\n+      newOp = rewriter.create<LLVM::FPExtOp>(UnknownLoc::get(context), newType,\n+                                             value);\n+    }\n+\n+    return {newType, newOp};\n+  }\n+\n+  void llPrintf(StringRef msg, ValueRange args,\n+                ConversionPatternRewriter &rewriter) const {\n+    static const char formatStringPrefix[] = \"printfFormat_\";\n+    assert(!msg.empty() && \"printf with empty string not support\");\n+    Type int8Ptr = ptr_ty(i8_ty);\n+\n+    auto *context = rewriter.getContext();\n+    auto moduleOp =\n+        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    auto funcOp = getVprintfDeclaration(rewriter);\n+\n+    Value one = rewriter.create<LLVM::ConstantOp>(\n+        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(1));\n+    Value zero = rewriter.create<LLVM::ConstantOp>(\n+        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(0));\n+\n+    unsigned stringNumber = 0;\n+    SmallString<16> stringConstName;\n+    do {\n+      stringConstName.clear();\n+      (formatStringPrefix + Twine(stringNumber++)).toStringRef(stringConstName);\n+    } while (moduleOp.lookupSymbol(stringConstName));\n+\n+    llvm::SmallString<64> formatString(msg);\n+    formatString.push_back('\\n');\n+    formatString.push_back('\\0');\n+    size_t formatStringSize = formatString.size_in_bytes();\n+    auto globalType = LLVM::LLVMArrayType::get(i8_ty, formatStringSize);\n+\n+    LLVM::GlobalOp global;\n+    {\n+      ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+      rewriter.setInsertionPointToStart(moduleOp.getBody());\n+      global = rewriter.create<LLVM::GlobalOp>(\n+          UnknownLoc::get(context), globalType,\n+          /*isConstant=*/true, LLVM::Linkage::Internal, stringConstName,\n+          rewriter.getStringAttr(formatString));\n+    }\n+\n+    Value globalPtr =\n+        rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(context), global);\n+    Value stringStart =\n+        rewriter.create<LLVM::GEPOp>(UnknownLoc::get(context), int8Ptr,\n+                                     globalPtr, mlir::ValueRange({zero, zero}));\n+\n+    Value bufferPtr =\n+        rewriter.create<LLVM::NullOp>(UnknownLoc::get(context), int8Ptr);\n+\n+    SmallVector<Value, 16> newArgs;\n+    if (args.size() >= 1) {\n+      SmallVector<Type> argTypes;\n+      for (auto arg : args) {\n+        Type newType;\n+        Value newArg;\n+        std::tie(newType, newArg) = promoteValue(rewriter, arg);\n+        argTypes.push_back(newType);\n+        newArgs.push_back(newArg);\n+      }\n+\n+      Type structTy = LLVM::LLVMStructType::getLiteral(context, argTypes);\n+      auto allocated = rewriter.create<LLVM::AllocaOp>(UnknownLoc::get(context),\n+                                                       ptr_ty(structTy), one,\n+                                                       /*alignment=*/0);\n+\n+      for (const auto &entry : llvm::enumerate(newArgs)) {\n+        auto index = rewriter.create<LLVM::ConstantOp>(\n+            UnknownLoc::get(context), i32_ty,\n+            rewriter.getI32IntegerAttr(entry.index()));\n+        auto fieldPtr = rewriter.create<LLVM::GEPOp>(\n+            UnknownLoc::get(context), ptr_ty(argTypes[entry.index()]),\n+            allocated, ArrayRef<Value>{zero, index});\n+        rewriter.create<LLVM::StoreOp>(UnknownLoc::get(context), entry.value(),\n+                                       fieldPtr);\n+      }\n+      bufferPtr = rewriter.create<LLVM::BitcastOp>(UnknownLoc::get(context),\n+                                                   int8Ptr, allocated);\n+    }\n+\n+    ValueRange operands{stringStart, bufferPtr};\n+    rewriter.create<LLVM::CallOp>(UnknownLoc::get(context), funcOp, operands);\n+  }\n+};\n+\n void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n                                   AxisInfoAnalysis &axisInfoAnalysis,\n@@ -4817,6 +5005,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<ViewLikeOpConversion<triton::ExpandDimsOp>>(typeConverter,\n                                                            benefit);\n   patterns.add<DotOpConversion>(typeConverter, allocation, smem, benefit);\n+  patterns.add<PrintfOpConversion>(typeConverter, benefit);\n }\n \n class ConvertTritonGPUToLLVM"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 14, "deletions": 2, "changes": 16, "file_content_changes": "@@ -339,6 +339,18 @@ struct TritonReducePattern : public OpConversionPattern<triton::ReduceOp> {\n   }\n };\n \n+struct TritonPrintfPattern : public OpConversionPattern<triton::PrintfOp> {\n+  using OpConversionPattern<PrintfOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(PrintfOp op, typename PrintfOp::Adaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    rewriter.replaceOpWithNewOp<triton::PrintfOp>(op, op.prefixAttr(),\n+                                                  adaptor.getOperands());\n+    return success();\n+  }\n+};\n+\n void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n                             RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n@@ -350,8 +362,8 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n       TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n       TritonGenericPattern<triton::AddPtrOp>, TritonReducePattern,\n       TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n-      TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern>(\n-      typeConverter, context);\n+      TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern,\n+      TritonPrintfPattern>(typeConverter, context);\n }\n \n //"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 40, "deletions": 4, "changes": 44, "file_content_changes": "@@ -533,6 +533,35 @@ class BlockedToMMA : public mlir::RewritePattern {\n   BlockedToMMA(mlir::MLIRContext *context)\n       : mlir::RewritePattern(triton::DotOp::getOperationName(), 2, context) {}\n \n+  static SmallVector<unsigned, 2>\n+  getWarpsPerTile(const ArrayRef<int64_t> &shape, int version, int numWarps) {\n+    assert(version == 2);\n+    // TODO: Handle one warp per row for fused matmuls\n+    // TODO: unsigned -> int64_t to keep things uniform\n+    SmallVector<unsigned, 2> ret = {1, 1};\n+    SmallVector<int64_t, 2> shapePerWarp = {16, 8};\n+    bool changed = false;\n+    // TODO (@daadaada): double-check.\n+    // original logic in\n+    // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L252\n+    // seems buggy for shape = [32, 16] ?\n+    do {\n+      changed = false;\n+      if (ret[0] * ret[1] >= numWarps)\n+        break;\n+      if (shape[0] / shapePerWarp[0] / ret[0] >=\n+          shape[1] / (shapePerWarp[1] * 2) / ret[1]) {\n+        if (ret[0] < shape[0] / shapePerWarp[0]) {\n+          ret[0] *= 2;\n+        } else\n+          ret[1] *= 2;\n+      } else {\n+        ret[1] *= 2;\n+      }\n+    } while (true);\n+    return ret;\n+  }\n+\n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n@@ -541,13 +570,20 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto oldRetType = dotOp.getResult().getType().cast<RankedTensorType>();\n     if (oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n       return failure();\n-    // TODO: compute warpsPerCTA\n-    auto newRetType = RankedTensorType::get(\n-        oldRetType.getShape(), oldRetType.getElementType(),\n-        triton::gpu::MmaEncodingAttr::get(oldRetType.getContext(), 2, {2, 2}));\n+    // get MMA encoding for the given number of warps\n+    auto retShape = oldRetType.getShape();\n+    auto mod = op->getParentOfType<mlir::ModuleOp>();\n+    int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+    auto newRetType =\n+        RankedTensorType::get(retShape, oldRetType.getElementType(),\n+                              triton::gpu::MmaEncodingAttr::get(\n+                                  oldRetType.getContext(), 2,\n+                                  getWarpsPerTile(retShape, 2, numWarps)));\n+    // convert accumulator\n     auto oldAcc = dotOp.getOperand(2);\n     auto newAcc = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         oldAcc.getLoc(), newRetType, oldAcc);\n+    // convert output\n     auto newDot = rewriter.create<triton::DotOp>(\n         dotOp.getLoc(), newRetType, dotOp.getOperand(0), dotOp.getOperand(1),\n         newAcc, dotOp.allowTF32(), dotOp.transA(), dotOp.transB());"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -197,7 +197,8 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n     ext_mod->setTargetTriple(llvmir->getTargetTriple());\n     ext_mod->setDataLayout(llvmir->getDataLayout());\n \n-    if (llvm::Linker::linkModules(*llvmir, std::move(ext_mod))) {\n+    if (llvm::Linker::linkModules(*llvmir, std::move(ext_mod),\n+                                  llvm::Linker::Flags::LinkOnlyNeeded)) {\n       llvm::errs() << \"Failed to link extern lib \" << lib.first;\n       return nullptr;\n     }"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -1185,6 +1185,16 @@ void init_triton_ir(py::module &&m) {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::SelectOp>(loc, condition, trueValue,\n                                                 falseValue);\n+           })\n+      .def(\"create_printf\",\n+           [](mlir::OpBuilder &self, const std::string &prefix,\n+              const std::vector<mlir::Value> &values) -> void {\n+             auto loc = self.getUnknownLoc();\n+             self.create<mlir::triton::PrintfOp>(\n+                 loc,\n+                 mlir::StringAttr::get(self.getContext(),\n+                                       llvm::StringRef(prefix)),\n+                 values);\n            });\n \n   py::class_<mlir::PassManager>(m, \"pass_manager\")"}, {"filename": "python/tests/printf_helper.py", "status": "added", "additions": 56, "deletions": 0, "changes": 56, "file_content_changes": "@@ -0,0 +1,56 @@\n+import torch\n+from torch.testing import assert_close\n+\n+import triton\n+import triton.language as tl\n+\n+torch_type = {\n+    \"bool\": torch.bool,\n+    'int8': torch.int8,\n+    'uint8': torch.uint8,\n+    'int16': torch.int16,\n+    \"int32\": torch.int32,\n+    'int64': torch.long,\n+    'float16': torch.float16,\n+    'bfloat16': torch.bfloat16,\n+    \"float32\": torch.float32,\n+    \"float64\": torch.float64\n+}\n+\n+\n+def get_tensor(shape, data_type, b_positive=False):\n+    x = None\n+    if data_type.startswith('int'):\n+        x = torch.arange(0, shape[0], dtype=torch_type[data_type], device='cuda')\n+    else:\n+        x = torch.arange(0, shape[0], dtype=torch_type[data_type], device='cuda')\n+\n+    return x\n+\n+# @pytest.mark.parametrize('data_type',\n+#                          [(\"int8\"),\n+#                           ('int16'),\n+#                           ('int32'),\n+#                           (\"int64\"),\n+#                           ('float16'),\n+#                           (\"float32\"),\n+#                           (\"float64\")])\n+\n+\n+def printf(data_type):\n+    @triton.jit\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        tl.printf(\"\", x)\n+        tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+    shape = (128, )\n+    # limit the range of integers so that the sum does not overflow\n+    x = get_tensor(shape, data_type)\n+    y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n+    kernel[(1,)](x, y, BLOCK=shape[0])\n+    assert_close(y, x)\n+\n+\n+printf(\"float16\")\n+printf(\"int8\")"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 72, "deletions": 48, "changes": 120, "file_content_changes": "@@ -144,7 +144,7 @@ def kernel(Z, X, SIZE: tl.constexpr):\n     # triton result\n     x_tri = to_triton(x, device=device, dst_type=dtype_x)\n     z_tri = to_triton(np.empty_like(z_ref), device=device, dst_type=dtype_x)\n-    kernel[(1, )](z_tri, x_tri, SIZE=SIZE, num_warps=4)\n+    kernel[(1, )](z_tri, x_tri, SIZE=SIZE, num_warps=4, extern_libs={\"libdevice\": \"/usr/local/cuda/nvvm/libdevice/libdevice.10.bc\"})\n     # compare\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n \n@@ -463,17 +463,12 @@ def test_unary_op(dtype_x, expr, device='cuda'):\n # # test math ops\n # # ----------------\n \n-# TODO: Math module\n-# # @pytest.mark.parametrize(\"expr\", [\n-# #     'exp', 'log', 'cos', 'sin'\n-# # ])\n \n-\n-# @pytest.mark.parametrize(\"expr\", [\n-#     'exp', 'log', 'cos', 'sin'\n-# ])\n-# def test_math_op(expr, device='cuda'):\n-#     _test_unary('float32', f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n+@pytest.mark.parametrize(\"expr\", [\n+    'exp', 'log', 'cos', 'sin'\n+])\n+def test_math_op(expr, device='cuda'):\n+    _test_unary('float32', f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n \n \n # # ----------------\n@@ -1545,43 +1540,72 @@ def _kernel(dst):\n # # -------------\n \n \n-# @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n-#                          [('int32', 'libdevice.ffs', ''),\n-#                           ('float32', 'libdevice.pow', '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'),\n-#                           ('float64', 'libdevice.norm4d', '')])\n-# def test_libdevice(dtype_str, expr, lib_path):\n+@pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n+                         [('int32', 'libdevice.ffs', ''),\n+                          ('float32', 'libdevice.pow', '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'),\n+                          ('float64', 'libdevice.norm4d', '')])\n+def test_libdevice_tensor(dtype_str, expr, lib_path):\n \n-#     @triton.jit\n-#     def kernel(X, Y, BLOCK: tl.constexpr):\n-#         x = tl.load(X + tl.arange(0, BLOCK))\n-#         y = GENERATE_TEST_HERE\n-#         tl.store(Y + tl.arange(0, BLOCK), y)\n+    @triton.jit\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        y = GENERATE_TEST_HERE\n+        tl.store(Y + tl.arange(0, BLOCK), y)\n \n-#     shape = (128, )\n-#     rs = RandomState(17)\n-#     # limit the range of integers so that the sum does not overflow\n-#     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n-\n-#     if expr == 'libdevice.ffs':\n-#         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.ffs(x)'})\n-#         y_ref = np.zeros(shape, dtype=x.dtype)\n-#         for i in range(shape[0]):\n-#             y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n-#     elif expr == 'libdevice.pow':\n-#         # numpy does not allow negative factors in power, so we use abs()\n-#         x = np.abs(x)\n-#         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n-#         y_ref = np.power(x, x)\n-#     elif expr == 'libdevice.norm4d':\n-#         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.norm4d(x, x, x, x)'})\n-#         y_ref = np.sqrt(4 * np.power(x, 2))\n+    shape = (128, )\n+    rs = RandomState(17)\n+    # limit the range of integers so that the sum does not overflow\n+    x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n \n-#     x_tri = to_triton(x)\n-#     # triton result\n-#     y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n-#     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n-#     # compare\n-#     if expr == 'libdevice.ffs':\n-#         np.testing.assert_equal(y_ref, to_numpy(y_tri))\n-#     else:\n-#         np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n+    if expr == 'libdevice.ffs':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.ffs(x)'})\n+        y_ref = np.zeros(shape, dtype=x.dtype)\n+        for i in range(shape[0]):\n+            y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n+    elif expr == 'libdevice.pow':\n+        # numpy does not allow negative factors in power, so we use abs()\n+        x = np.abs(x)\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+        y_ref = np.power(x, x)\n+    elif expr == 'libdevice.norm4d':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.norm4d(x, x, x, x)'})\n+        y_ref = np.sqrt(4 * np.power(x, 2))\n+\n+    x_tri = to_triton(x)\n+    # triton result\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n+    # compare\n+    if expr == 'libdevice.ffs':\n+        np.testing.assert_equal(y_ref, to_numpy(y_tri))\n+    else:\n+        np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n+\n+\n+@pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n+                         [('float32', 'libdevice.pow', '')])\n+def test_libdevice_scalar(dtype_str, expr, lib_path):\n+\n+    @triton.jit\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        x = X\n+        y = GENERATE_TEST_HERE\n+        tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+    shape = (128, )\n+    rs = RandomState(17)\n+    # limit the range of integers so that the sum does not overflow\n+    x = numpy_random((1,), dtype_str=dtype_str, rs=rs)\n+    y_ref = np.zeros(shape, dtype=x.dtype)\n+\n+    # numpy does not allow negative factors in power, so we use abs()\n+    x = np.abs(x)\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+    y_ref[:] = np.power(x, x)\n+\n+    # triton result\n+    x_tri = to_triton(x)[0].item()\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n+    # compare\n+    np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)"}, {"filename": "python/tests/test_printf.py", "status": "added", "additions": 21, "deletions": 0, "changes": 21, "file_content_changes": "@@ -0,0 +1,21 @@\n+import os\n+import subprocess\n+\n+dir_path = os.path.dirname(os.path.realpath(__file__))\n+printf_path = os.path.join(dir_path, \"printf_helper.py\")\n+\n+\n+def test_printf():\n+    proc = subprocess.Popen([\"python\", printf_path], stdout=subprocess.PIPE, shell=False)\n+    (outs, err) = proc.communicate()\n+    outs = outs.split()\n+    new_lines = set()\n+    for line in outs:\n+        try:\n+            value = int(float(line))\n+            new_lines.add(value)\n+        except Exception as e:\n+            print(e)\n+    for i in range(128):\n+        assert i in new_lines\n+    assert len(new_lines) == 128"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -1197,3 +1197,22 @@ def swizzle2d(i, j, size_i, size_j, size_g):\n @triton.jit\n def zeros_like(input):\n     return zeros(input.shape, input.dtype)\n+\n+\n+@builtin\n+def printf(prefix, *args, _builder=None):\n+    import string\n+    new_prefix = prefix\n+    if isinstance(prefix, constexpr):\n+        new_prefix = prefix.value\n+    assert isinstance(new_prefix, str), f\"{new_prefix} is not string\"\n+    b_ascii = True\n+    for ch in new_prefix:\n+        if ch not in string.printable:\n+            b_ascii = False\n+            break\n+    assert b_ascii, f\"{new_prefix} is not an ascii string\"\n+    new_args = []\n+    for arg in args:\n+        new_args.append(_to_tensor(arg, _builder))\n+    return semantic.printf(new_prefix, new_args, _builder)"}, {"filename": "python/triton/language/extern.py", "status": "modified", "additions": 28, "deletions": 22, "changes": 50, "file_content_changes": "@@ -56,28 +56,34 @@ def elementwise(lib_name: str, lib_path: str, args: list, arg_type_symbol_dict:\n         :return: the return value of the function\n     '''\n     dispatch_args = args.copy()\n-    if len(args) == 1:\n-        dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n-        ret_shape = dispatch_args[0].shape\n-    elif len(args) == 2:\n-        dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n-        dispatch_args[1] = core._to_tensor(dispatch_args[1], _builder)\n-        dispatch_args[0], dispatch_args[1] = semantic.binary_op_type_checking_impl(\n-            dispatch_args[0], dispatch_args[1], _builder)\n-        ret_shape = dispatch_args[0].shape\n-    else:\n-        for i in range(len(dispatch_args)):\n-            dispatch_args[i] = core._to_tensor(dispatch_args[i], _builder)\n-        broadcast_arg = dispatch_args[0]\n-        # Get the broadcast shape over all the arguments\n-        for i in range(len(dispatch_args)):\n-            _, broadcast_arg = semantic.binary_op_type_checking_impl(\n-                dispatch_args[i], broadcast_arg, _builder)\n-        # Change the shape of each argument based on the broadcast shape\n-        for i in range(len(dispatch_args)):\n-            dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n-                dispatch_args[i], broadcast_arg, _builder)\n-        ret_shape = broadcast_arg.shape\n+    all_scalar = True\n+    ret_shape = None\n+    for dispatch_arg in dispatch_args:\n+        if dispatch_arg.type.is_block():\n+            all_scalar = False\n+    if not all_scalar:\n+        if len(args) == 1:\n+            dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n+            ret_shape = dispatch_args[0].shape\n+        elif len(args) == 2:\n+            dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n+            dispatch_args[1] = core._to_tensor(dispatch_args[1], _builder)\n+            dispatch_args[0], dispatch_args[1] = semantic.binary_op_type_checking_impl(\n+                dispatch_args[0], dispatch_args[1], _builder)\n+            ret_shape = dispatch_args[0].shape\n+        else:\n+            for i in range(len(dispatch_args)):\n+                dispatch_args[i] = core._to_tensor(dispatch_args[i], _builder)\n+            broadcast_arg = dispatch_args[0]\n+            # Get the broadcast shape over all the arguments\n+            for i in range(len(dispatch_args)):\n+                _, broadcast_arg = semantic.binary_op_type_checking_impl(\n+                    dispatch_args[i], broadcast_arg, _builder)\n+            # Change the shape of each argument based on the broadcast shape\n+            for i in range(len(dispatch_args)):\n+                dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n+                    dispatch_args[i], broadcast_arg, _builder)\n+            ret_shape = broadcast_arg.shape\n     func = getattr(_builder, \"create_external_elementwise\")\n     return dispatch(func, lib_name, lib_path, dispatch_args, arg_type_symbol_dict, ret_shape, _builder)\n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -1123,3 +1123,10 @@ def max_contiguous(x: tl.tensor, values: List[int]) -> tl.tensor:\n \n def debug_barrier(builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_barrier(''), tl.void)\n+\n+\n+def printf(prefix: str, args: List[tl.tensor], builder: ir.builder) -> tl.tensor:\n+    new_args = []\n+    for arg in args:\n+        new_args.append(arg.handle)\n+    return tl.tensor(builder.create_printf(prefix, new_args), tl.void)"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 5, "deletions": 23, "changes": 28, "file_content_changes": "@@ -157,15 +157,6 @@\n @triton.autotune(\n     configs=[\n         triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n-        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n-        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n-        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n     ],\n     key=['M', 'N', 'K'],\n )\n@@ -318,13 +309,13 @@ def matmul(a, b, activation=None):\n     triton.testing.Benchmark(\n         x_names=['M', 'N', 'K'],  # argument names to use as an x-axis for the plot\n         x_vals=[\n-            128 * i for i in range(2, 33)\n+            8192\n         ],  # different possible values for `x_name`\n         line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n         # possible values for `line_arg``\n-        line_vals=['cublas', 'cublas + relu', 'triton', 'triton + relu'],\n+        line_vals=['cublas', 'triton'],\n         # label name for the lines\n-        line_names=[\"cuBLAS\", \"cuBLAS (+ torch.nn.LeakyReLU)\", \"Triton\", \"Triton (+ LeakyReLU)\"],\n+        line_names=[\"cuBLAS\", \"Triton\"],\n         # line styles\n         styles=[('green', '-'), ('green', '--'), ('blue', '-'), ('blue', '--')],\n         ylabel=\"TFLOPS\",  # label name for the y-axis\n@@ -336,18 +327,9 @@ def benchmark(M, N, K, provider):\n     a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n     b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n     if provider == 'cublas':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), rep=100)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b))\n-    if provider == 'cublas + relu':\n-        torch_relu = torch.nn.ReLU(inplace=True)\n-        ms, min_ms, max_ms = triton.testing.do_bench(\n-            lambda: torch_relu(torch.matmul(a, b))\n-        )\n-    if provider == 'triton + relu':\n-        ms, min_ms, max_ms = triton.testing.do_bench(\n-            lambda: matmul(a, b, activation=leaky_relu)\n-        )\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), rep=100)\n     perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n     return perf(ms), perf(max_ms), perf(min_ms)\n "}]