[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 215, "deletions": 216, "changes": 431, "file_content_changes": "@@ -759,6 +759,17 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n     return vec;\n   }\n \n+  unsigned getMaskAlignment(Value mask) const {\n+    auto maskOrder = mask.getType()\n+                         .cast<RankedTensorType>()\n+                         .getEncoding()\n+                         .cast<BlockedEncodingAttr>()\n+                         .getOrder();\n+\n+    auto maskAxis = getAxisInfo(mask);\n+    return std::max<int>(maskAxis->getConstancy(maskOrder[0]), 1);\n+  }\n+\n   llvm::Optional<AxisInfo> getAxisInfo(Value val) const {\n     if (auto it = AxisAnalysisPass.lookupLatticeElement(val)) {\n       return it->getValue();\n@@ -771,6 +782,208 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n   AxisInfoAnalysis &AxisAnalysisPass;\n };\n \n+struct LoadOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>,\n+      public LoadStoreConversionBase {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::LoadOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LoadOpConversion(LLVMTypeConverter &converter,\n+                   AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>(converter, benefit),\n+        LoadStoreConversionBase(axisAnalysisPass) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Value ptr = op.ptr();\n+    Value mask = op.mask();\n+    Value other = op.other();\n+\n+    Value llPtr = adaptor.ptr();\n+    Value llMask = adaptor.mask();\n+    Value llOther = adaptor.other();\n+\n+    auto loc = op->getLoc();\n+    MLIRContext *ctx = rewriter.getContext();\n+\n+    auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    if (!valueTy)\n+      return failure();\n+    Type valueElemTy =\n+        getTypeConverter()->convertType(valueTy.getElementType());\n+\n+    auto [layout, numElems] = getLayout(ptr);\n+\n+    auto ptrElems = getLLVMElems(ptr, llPtr, layout, rewriter, loc);\n+    assert(ptrElems.size() == numElems);\n+    // Determine the vectorization size\n+    size_t vec = getVectorizeSize(ptr, layout);\n+\n+    SmallVector<Value> maskElems;\n+    if (llMask) {\n+      unsigned maskAlignment = getMaskAlignment(mask);\n+      maskElems = getLLVMElems(mask, llMask, layout, rewriter, loc);\n+      assert(ptrElems.size() == maskElems.size());\n+\n+      size_t maskAlign = getMaskAlignment(mask);\n+      vec = std::min(vec, maskAlign);\n+    }\n+\n+    const size_t dtsize =\n+        std::max<int>(1, valueElemTy.getIntOrFloatBitWidth() / 8);\n+    const size_t valueElemNbits = dtsize * 8;\n+\n+    const int numVecs = numElems / vec;\n+\n+    // TODO: (goostavz) handle when other is const but not splat, which\n+    //       should be rarely seen\n+    bool otherIsSplatConstInt = false;\n+    DenseElementsAttr constAttr;\n+    int64_t splatVal = 0;\n+    if (valueElemTy.isa<IntegerType>() &&\n+        matchPattern(op.other(), m_Constant(&constAttr)) &&\n+        constAttr.isSplat()) {\n+      otherIsSplatConstInt = true;\n+      splatVal = constAttr.getSplatValue<APInt>().getSExtValue();\n+    }\n+\n+    auto otherElems = getLLVMElems(other, llOther, layout, rewriter, loc);\n+\n+    SmallVector<Value> loadedVals;\n+    for (size_t vecStart = 0; vecStart < numElems; vecStart += vec) {\n+      // TODO: optimization when ptr is GEP with constant offset\n+      size_t in_off = 0;\n+\n+      const int maxWordWidth = std::max<int>(32, valueElemNbits);\n+      const int totalWidth = valueElemNbits * vec;\n+      const int width = std::min(totalWidth, maxWordWidth);\n+      const int nWords = std::max(1, totalWidth / width);\n+      const int wordNElems = width / valueElemNbits;\n+      const int vecNElems = totalWidth / valueElemNbits;\n+      assert(wordNElems * nWords * numVecs == numElems);\n+\n+      // TODO(Superjomn) Add cache policy fields to StoreOp.\n+      // TODO(Superjomn) Deal with cache policy here.\n+      const bool hasL2EvictPolicy = false;\n+\n+      PTXBuilder ptxBuilder;\n+      auto &ld = *ptxBuilder.create<PTXIOInstr>(\"ld\");\n+\n+      Value pred = mask ? maskElems[vecStart] : int_val(1, 1);\n+\n+      const std::string readConstraint =\n+          (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n+      const std::string writeConstraint =\n+          (width == 64) ? \"=l\" : ((width == 32) ? \"=r\" : \"=c\");\n+\n+      // prepare asm operands\n+      auto *dstsOpr = ptxBuilder.newListOperand();\n+      for (int wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n+        auto *opr = ptxBuilder.newOperand(writeConstraint); // =r operations\n+        dstsOpr->listAppend(opr);\n+      }\n+\n+      auto *addrOpr =\n+          ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n+\n+      // Define the instruction opcode\n+      ld.o(\"volatile\", op.isVolatile())\n+          .global()\n+          .o(\"ca\", op.cache() == triton::CacheModifier::CA)\n+          .o(\"cg\", op.cache() == triton::CacheModifier::CG)\n+          .o(\"L1::evict_first\",\n+             op.evict() == triton::EvictionPolicy::EVICT_FIRST)\n+          .o(\"L1::evict_last\", op.evict() == triton::EvictionPolicy::EVICT_LAST)\n+          .o(\"L1::cache_hint\", hasL2EvictPolicy)\n+          .v(nWords)\n+          .b(width);\n+\n+      PTXBuilder::Operand *evictOpr{};\n+\n+      // Here lack a mlir::Value to bind to this operation, so disabled.\n+      // if (has_l2_evict_policy)\n+      //   evictOpr = ptxBuilder.newOperand(l2Evict, \"l\");\n+\n+      if (!evictOpr)\n+        ld(dstsOpr, addrOpr).predicate(pred, \"b\");\n+      else\n+        ld(dstsOpr, addrOpr, evictOpr).predicate(pred, \"b\");\n+\n+      if (other) {\n+        for (size_t ii = 0; ii < nWords; ++ii) {\n+          PTXInstr &mov = *ptxBuilder.create<>(\"mov\");\n+          mov.o(\"u\", width);\n+\n+          size_t size = width / valueElemNbits;\n+\n+          auto vecTy = LLVM::getFixedVectorType(valueElemTy, size);\n+          Value v = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n+          for (size_t s = 0; s < size; ++s) {\n+            Value falseVal = otherElems[vecStart + ii * size + s];\n+            Value sVal = createIndexAttrConstant(\n+                rewriter, loc, this->getTypeConverter()->getIndexType(), s);\n+            v = insert_element(vecTy, v, falseVal, sVal);\n+          }\n+          v = bitcast(IntegerType::get(getContext(), width), v);\n+\n+          PTXInstr::Operand *opr{};\n+          if (otherIsSplatConstInt)\n+            opr = ptxBuilder.newConstantOperand(splatVal);\n+          else\n+            opr = ptxBuilder.newOperand(v, readConstraint);\n+\n+          mov(dstsOpr->listGet(ii), opr).predicateNot(pred, \"b\");\n+        }\n+      }\n+\n+      // ---\n+      // create inline ASM signature\n+      // ---\n+      SmallVector<Type> retTys(nWords, IntegerType::get(getContext(), width));\n+      Type retTy = retTys.size() > 1\n+                       ? LLVM::LLVMStructType::getLiteral(getContext(), retTys)\n+                       : retTys[0];\n+\n+      // TODO: if (has_l2_evict_policy)\n+      auto asmDialectAttr = LLVM::AsmDialectAttr::get(rewriter.getContext(),\n+                                                      LLVM::AsmDialect::AD_ATT);\n+      Value ret = ptxBuilder.launch(rewriter, loc, retTy);\n+\n+      // ---\n+      // extract and store return values\n+      // ---\n+      SmallVector<Value> rets;\n+      for (unsigned int ii = 0; ii < nWords; ++ii) {\n+        Value curr;\n+        if (retTy.isa<LLVM::LLVMStructType>()) {\n+          curr = extract_val(IntegerType::get(getContext(), width), ret,\n+                             rewriter.getI64ArrayAttr(ii));\n+        } else {\n+          curr = ret;\n+        }\n+        curr = bitcast(\n+            LLVM::getFixedVectorType(valueElemTy, width / valueElemNbits),\n+            curr);\n+        rets.push_back(curr);\n+      }\n+      int tmp = width / valueElemNbits;\n+      for (size_t ii = 0; ii < vec; ++ii) {\n+        Value vecIdx = createIndexAttrConstant(\n+            rewriter, loc, this->getTypeConverter()->getIndexType(), ii % tmp);\n+        Value loaded = extract_element(valueElemTy, rets[ii / tmp], vecIdx);\n+        loadedVals.push_back(loaded);\n+      }\n+    } // end vec\n+\n+    Type llvmResultStructTy = getTypeConverter()->convertType(valueTy);\n+    Value resultStruct =\n+        getStructFromElements(loc, loadedVals, rewriter, llvmResultStructTy);\n+    rewriter.replaceOp(op, {resultStruct});\n+    return success();\n+  }\n+};\n+\n struct StoreOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::StoreOp>,\n       public LoadStoreConversionBase {\n@@ -814,14 +1027,8 @@ struct StoreOpConversion\n     if (llMask) {\n       maskElems = getLLVMElems(mask, llMask, layout, rewriter, loc);\n       assert(valueElems.size() == maskElems.size());\n-      auto maskOrder = mask.getType()\n-                           .cast<RankedTensorType>()\n-                           .getEncoding()\n-                           .cast<BlockedEncodingAttr>()\n-                           .getOrder();\n-\n-      auto maskAxis = getAxisInfo(mask);\n-      size_t maskAlign = std::max<int>(maskAxis->getConstancy(maskOrder[0]), 1);\n+\n+      size_t maskAlign = getMaskAlignment(mask);\n       vec = std::min(vec, maskAlign);\n     }\n \n@@ -1062,214 +1269,6 @@ struct MakeRangeOpConversion\n   }\n };\n \n-struct LoadOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>,\n-      public LoadStoreConversionBase {\n-  using ConvertTritonGPUOpToLLVMPattern<\n-      triton::LoadOp>::ConvertTritonGPUOpToLLVMPattern;\n-\n-  LoadOpConversion(LLVMTypeConverter &converter,\n-                   AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n-      : ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>(converter, benefit),\n-        LoadStoreConversionBase(axisAnalysisPass) {}\n-\n-  LogicalResult\n-  matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    Value ptr = op.ptr();\n-    Value mask = op.mask();\n-    Value other = op.other();\n-\n-    Value llPtr = adaptor.ptr();\n-    Value llMask = adaptor.mask();\n-    Value llOther = adaptor.other();\n-\n-    auto loc = op->getLoc();\n-    MLIRContext *ctx = rewriter.getContext();\n-\n-    auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n-    if (!valueTy)\n-      return failure();\n-    Type valueElemTy =\n-        getTypeConverter()->convertType(valueTy.getElementType());\n-\n-    auto [layout, numElems] = getLayout(ptr);\n-\n-    auto ptrElems = getLLVMElems(ptr, llPtr, layout, rewriter, loc);\n-    assert(ptrElems.size() == numElems);\n-    // Determine the vectorization size\n-    size_t vec = getVectorizeSize(ptr, layout);\n-\n-    SmallVector<Value> maskElems;\n-    if (llMask) {\n-      maskElems = getLLVMElems(mask, llMask, layout, rewriter, loc);\n-      assert(ptrElems.size() == maskElems.size());\n-      auto maskOrder = mask.getType()\n-                           .cast<RankedTensorType>()\n-                           .getEncoding()\n-                           .cast<BlockedEncodingAttr>()\n-                           .getOrder();\n-\n-      auto maskAxis = getAxisInfo(mask);\n-      size_t maskAlign = std::max<int>(maskAxis->getConstancy(maskOrder[0]), 1);\n-      vec = std::min(vec, maskAlign);\n-    }\n-\n-    const size_t dtsize =\n-        std::max<int>(1, valueElemTy.getIntOrFloatBitWidth() / 8);\n-    const size_t valueElemNbits = dtsize * 8;\n-\n-    const int numVecs = numElems / vec;\n-\n-    // TODO: (goostavz) handle when other is const but not splat, which\n-    //       should be rarely seen\n-    bool otherIsSplatConstInt = false;\n-    DenseElementsAttr constAttr;\n-    int64_t splatVal = 0;\n-    if (valueElemTy.isa<IntegerType>() &&\n-        matchPattern(op.other(), m_Constant(&constAttr)) &&\n-        constAttr.isSplat()) {\n-      otherIsSplatConstInt = true;\n-      splatVal = constAttr.getSplatValue<APInt>().getSExtValue();\n-    }\n-\n-    auto otherElems = getLLVMElems(other, llOther, layout, rewriter, loc);\n-\n-    SmallVector<Value> loadedVals;\n-    for (size_t vecStart = 0; vecStart < numElems; vecStart += vec) {\n-      // TODO: optimization when ptr is GEP with constant offset\n-      size_t in_off = 0;\n-\n-      const int maxWordWidth = std::max<int>(32, valueElemNbits);\n-      const int totalWidth = valueElemNbits * vec;\n-      const int width = std::min(totalWidth, maxWordWidth);\n-      const int nWords = std::max(1, totalWidth / width);\n-      const int wordNElems = width / valueElemNbits;\n-      const int vecNElems = totalWidth / valueElemNbits;\n-      assert(wordNElems * nWords * numVecs == numElems);\n-\n-      // TODO(Superjomn) Add cache policy fields to StoreOp.\n-      // TODO(Superjomn) Deal with cache policy here.\n-      const bool hasL2EvictPolicy = false;\n-\n-      PTXBuilder ptxBuilder;\n-      auto &ld = *ptxBuilder.create<PTXIOInstr>(\"ld\");\n-\n-      Value pred = mask ? maskElems[vecStart] : int_val(1, 1);\n-\n-      const std::string readConstraint =\n-          (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n-      const std::string writeConstraint =\n-          (width == 64) ? \"=l\" : ((width == 32) ? \"=r\" : \"=c\");\n-\n-      // prepare asm operands\n-      auto *dstsOpr = ptxBuilder.newListOperand();\n-      for (int wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n-        auto *opr = ptxBuilder.newOperand(writeConstraint); // =r operations\n-        dstsOpr->listAppend(opr);\n-      }\n-\n-      auto *addrOpr =\n-          ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n-\n-      // Define the instruction opcode\n-      ld.o(\"volatile\", op.isVolatile())\n-          .global()\n-          .o(\"ca\", op.cache() == triton::CacheModifier::CA)\n-          .o(\"cg\", op.cache() == triton::CacheModifier::CG)\n-          .o(\"L1::evict_first\",\n-             op.evict() == triton::EvictionPolicy::EVICT_FIRST)\n-          .o(\"L1::evict_last\", op.evict() == triton::EvictionPolicy::EVICT_LAST)\n-          .o(\"L1::cache_hint\", hasL2EvictPolicy)\n-          .v(nWords)\n-          .b(width);\n-\n-      PTXBuilder::Operand *evictOpr{};\n-\n-      // Here lack a mlir::Value to bind to this operation, so disabled.\n-      // if (has_l2_evict_policy)\n-      //   evictOpr = ptxBuilder.newOperand(l2Evict, \"l\");\n-\n-      if (!evictOpr)\n-        ld(dstsOpr, addrOpr).predicate(pred, \"b\");\n-      else\n-        ld(dstsOpr, addrOpr, evictOpr).predicate(pred, \"b\");\n-\n-      if (other) {\n-        for (size_t ii = 0; ii < nWords; ++ii) {\n-          PTXInstr &mov = *ptxBuilder.create<>(\"mov\");\n-          mov.o(\"u\", width);\n-\n-          size_t size = width / valueElemNbits;\n-\n-          auto vecTy = LLVM::getFixedVectorType(valueElemTy, size);\n-          Value v = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n-          for (size_t s = 0; s < size; ++s) {\n-            Value falseVal = otherElems[vecStart + ii * size + s];\n-            Value sVal = createIndexAttrConstant(\n-                rewriter, loc, this->getTypeConverter()->getIndexType(), s);\n-            v = insert_element(vecTy, v, falseVal, sVal);\n-          }\n-          v = bitcast(IntegerType::get(getContext(), width), v);\n-\n-          PTXInstr::Operand *opr{};\n-          if (otherIsSplatConstInt) {\n-            opr = ptxBuilder.newConstantOperand(splatVal);\n-          } else {\n-            opr = ptxBuilder.newOperand(v, readConstraint);\n-          }\n-\n-          mov(dstsOpr->listGet(ii), opr).predicateNot(pred, \"b\");\n-        }\n-      }\n-\n-      // ---\n-      // create inline ASM signature\n-      // ---\n-      SmallVector<Type> retTys(nWords, IntegerType::get(getContext(), width));\n-      Type retTy = retTys.size() > 1\n-                       ? LLVM::LLVMStructType::getLiteral(getContext(), retTys)\n-                       : retTys[0];\n-\n-      // TODO: if (has_l2_evict_policy)\n-      auto asmDialectAttr = LLVM::AsmDialectAttr::get(rewriter.getContext(),\n-                                                      LLVM::AsmDialect::AD_ATT);\n-      Value ret = ptxBuilder.launch(rewriter, loc, retTy);\n-\n-      // ---\n-      // extract and store return values\n-      // ---\n-      SmallVector<Value> rets;\n-      for (unsigned int ii = 0; ii < nWords; ++ii) {\n-        Value curr;\n-        if (retTy.isa<LLVM::LLVMStructType>()) {\n-          curr = extract_val(IntegerType::get(getContext(), width), ret,\n-                             rewriter.getI64ArrayAttr(ii));\n-        } else {\n-          curr = ret;\n-        }\n-        curr = bitcast(\n-            LLVM::getFixedVectorType(valueElemTy, width / valueElemNbits),\n-            curr);\n-        rets.push_back(curr);\n-      }\n-      int tmp = width / valueElemNbits;\n-      for (size_t ii = 0; ii < vec; ++ii) {\n-        Value vecIdx = createIndexAttrConstant(\n-            rewriter, loc, this->getTypeConverter()->getIndexType(), ii % tmp);\n-        Value loaded = extract_element(valueElemTy, rets[ii / tmp], vecIdx);\n-        loadedVals.push_back(loaded);\n-      }\n-    } // end vec\n-\n-    Type llvmResultStructTy = getTypeConverter()->convertType(valueTy);\n-    Value resultStruct =\n-        getStructFromElements(loc, loadedVals, rewriter, llvmResultStructTy);\n-    rewriter.replaceOp(op, {resultStruct});\n-    return success();\n-  }\n-};\n-\n struct GetProgramIdOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::GetProgramIdOp> {\n   using ConvertTritonGPUOpToLLVMPattern<"}, {"filename": "python/tests/test_vecadd.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -188,7 +188,7 @@ def test_vecadd_no_scf(num_warps, block_size, shape):\n     [2, 256, (3, 256 + 7)],\n     [4, 256, (3, 256 + 7)],\n ])\n-def test_vecadd__no_scf_masked(num_warps, block_size, shape):\n+def test_vecadd_no_scf_masked(num_warps, block_size, shape):\n     vecadd_no_scf_tester(num_warps, block_size, shape)\n \n "}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 49, "deletions": 68, "changes": 117, "file_content_changes": "@@ -55,7 +55,7 @@ func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {t\n \n module {\n \n-// CHECK-LABEL: store_constant_align\n+// This is a tiny test for verifying StoreOp-related alignment, It simply store a constant to a buffer.\n func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1]\n   %pid = tt.get_program_id {axis = 0 : i32} : i32\n@@ -87,74 +87,55 @@ func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n:\n \n // -----\n \n-// module {\n-//   func @kernel_0d1d2d3d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n-//     %c128_i32 = arith.constant 128 : i32\n-//     %0 = tt.get_program_id {axis = 0 : i32} : i32\n-//     %1 = arith.muli %0, %c128_i32 : i32\n-//     %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n-//     %3 = tt.splat %1 : (i32) -> tensor<128xi32>\n-//     %4 = arith.addi %3, %2 : tensor<128xi32>\n-//     %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n-//     %6 = tt.addptr %5, %4 : tensor<128x!tt.ptr<f32>>\n-//     %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n-//     %8 = tt.addptr %7, %4 : tensor<128x!tt.ptr<f32>>\n-//     %9 = tt.splat %arg3 : (i32) -> tensor<128xi32>\n-//     %10 = arith.cmpi slt, %4, %9 : tensor<128xi32>\n-//     %11 = tt.load %6, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128xf32>\n-//     %12 = tt.load %8, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128xf32>\n-//     %13 = arith.addf %11, %12 : tensor<128xf32>\n-//     %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n-//     %15 = tt.addptr %14, %4 : tensor<128x!tt.ptr<f32>>\n-//     tt.store %15, %13, %10 : tensor<128xf32>\n-//     return\n-//   }\n-\n-// }\n-\n+// This IR is dumped from vecadd test.\n+// Note, the hint {tt.divisibility = 16 : i32} for %n_elements affects the alignment of mask.\n+func @kernel_0d1d2d3d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32 {tt.divisibility = 16 : i32}) {\n+  %c64_i32 = arith.constant 64 : i32\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = arith.muli %0, %c64_i32 : i32\n+  %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+  %3 = tt.splat %1 : (i32) -> tensor<64xi32>\n+  %4 = arith.addi %3, %2 : tensor<64xi32>\n+  %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>>\n+  %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>\n+  %9 = tt.splat %n_elements : (i32) -> tensor<64xi32>\n+  // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [16] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n+  %mask = arith.cmpi slt, %4, %9 : tensor<64xi32>\n+  %11 = tt.load %6, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+  %12 = tt.load %8, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+  %13 = arith.addf %11, %12 : tensor<64xf32>\n+  %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  // CHECK: Contiguity: [64] ; Divisibility: [16] ; Constancy: [1] ( %{{.*}} = tt.addptr %{{.*}}, %{{.*}} : tensor<64x!tt.ptr<f32>> )\n+  %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>\n+  tt.store %15, %13, %mask : tensor<64xf32>\n+  return\n+}\n \n // -----\n \n-#blocked = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n-module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n-  func @kernel_0d1d2d3(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n-    %c128_i32 = arith.constant 128 : i32\n-    %0 = tt.get_program_id {axis = 0 : i32} : i32\n-    %1 = arith.muli %0, %c128_i32 : i32\n-    %2 = tt.splat %1 : (i32) -> tensor<128xi32, #blocked>\n-    %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked>\n-    %4 = tt.splat %1 : (i32) -> tensor<128xi32, #blocked>\n-    %5 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked>\n-    %6 = tt.splat %1 : (i32) -> tensor<128xi32, #blocked>\n-    %7 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked>\n-    %8 = tt.splat %1 : (i32) -> tensor<128xi32, #blocked>\n-    %9 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked>\n-    %10 = tt.splat %1 : (i32) -> tensor<128xi32, #blocked>\n-    %11 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked>\n-    %12 = tt.splat %1 : (i32) -> tensor<128xi32, #blocked>\n-    %13 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked>\n-    %14 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>, #blocked>\n-    %15 = arith.addi %12, %13 : tensor<128xi32, #blocked>\n-    %16 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>, #blocked>\n-    %17 = arith.addi %8, %9 : tensor<128xi32, #blocked>\n-    %18 = arith.addi %2, %3 : tensor<128xi32, #blocked>\n-    %19 = tt.splat %arg3 : (i32) -> tensor<128xi32, #blocked>\n-    %20 = arith.addi %6, %7 : tensor<128xi32, #blocked>\n-    %21 = tt.splat %arg3 : (i32) -> tensor<128xi32, #blocked>\n-    %22 = arith.addi %10, %11 : tensor<128xi32, #blocked>\n-    %23 = tt.splat %arg3 : (i32) -> tensor<128xi32, #blocked>\n-    %24 = tt.addptr %14, %15 : tensor<128x!tt.ptr<f32>, #blocked>\n-    %25 = \"triton_gpu.cmpi\"(%22, %23) {predicate = 2 : i64} : (tensor<128xi32, #blocked>, tensor<128xi32, #blocked>) -> tensor<128xi1, #blocked>\n-    %26 = tt.load %24, %25 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128xf32, #blocked>\n-    %27 = tt.addptr %16, %17 : tensor<128x!tt.ptr<f32>, #blocked>\n-    %28 = \"triton_gpu.cmpi\"(%20, %21) {predicate = 2 : i64} : (tensor<128xi32, #blocked>, tensor<128xi32, #blocked>) -> tensor<128xi1, #blocked>\n-    %29 = tt.load %27, %28 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128xf32, #blocked>\n-    %30 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>, #blocked>\n-    %31 = arith.addi %4, %5 : tensor<128xi32, #blocked>\n-    %32 = tt.addptr %30, %31 : tensor<128x!tt.ptr<f32>, #blocked>\n-    %33 = arith.addf %26, %29 : tensor<128xf32, #blocked>\n-    %34 = \"triton_gpu.cmpi\"(%18, %19) {predicate = 2 : i64} : (tensor<128xi32, #blocked>, tensor<128xi32, #blocked>) -> tensor<128xi1, #blocked>\n-    tt.store %32, %33, %34 : tensor<128xf32, #blocked>\n-    return\n-  }\n+// This IR is dumped from vecadd test.\n+// Note, there is no divisibility hint for %n_elements, triton should assume its divisibility to be 1 by default.\n+func @kernel_0d1d2d3(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n+  %c64_i32 = arith.constant 64 : i32\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = arith.muli %0, %c64_i32 : i32\n+  %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+  %3 = tt.splat %1 : (i32) -> tensor<64xi32>\n+  %4 = arith.addi %3, %2 : tensor<64xi32>\n+  %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>>\n+  %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>\n+  %9 = tt.splat %n_elements : (i32) -> tensor<64xi32>\n+  // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [1] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n+  %10 = arith.cmpi slt, %4, %9 : tensor<64xi32>\n+  %11 = tt.load %6, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+  %12 = tt.load %8, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+  %13 = arith.addf %11, %12 : tensor<64xf32>\n+  %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>\n+  tt.store %15, %13, %10 : tensor<64xf32>\n+  return\n }"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 32, "deletions": 1, "changes": 33, "file_content_changes": "@@ -161,6 +161,37 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n \n // -----\n \n+// This test verifies the vectorization of Load and Store Ops.\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n+// Note, the %n_elements doesn't have a \"tt.divisibility\" hint, so Triton assumes it's divisibility is 1, this should effect the mask's alignment and further restrict the load/store ops' vector width to be 1.\n+module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n+  func @vecadd_masked_vec1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n+    %c64_i32 = arith.constant 64 : i32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %c64_i32 : i32\n+    %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked>\n+    %3 = tt.splat %1 : (i32) -> tensor<64xi32, #blocked>\n+    %4 = arith.addi %3, %2 : tensor<64xi32, #blocked>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n+    %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>, #blocked>\n+    %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n+    %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>, #blocked>\n+    %9 = tt.splat %n_elements : (i32) -> tensor<64xi32, #blocked>\n+    %10 = \"triton_gpu.cmpi\"(%4, %9) {predicate = 2 : i64} : (tensor<64xi32, #blocked>, tensor<64xi32, #blocked>) -> tensor<64xi1, #blocked>\n+    // load op has a vector width = 1 due to the %mask's alignment\n+    // CHECK: ld.global.b32\n+    %11 = tt.load %6, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32, #blocked>\n+    %12 = tt.load %8, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32, #blocked>\n+    %13 = arith.addf %11, %12 : tensor<64xf32, #blocked>\n+    %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n+    %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>, #blocked>\n+    tt.store %15, %13, %10 : tensor<64xf32, #blocked>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: global_load_store_vec8\n@@ -682,4 +713,4 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n     return\n   }\n-}\n\\ No newline at end of file\n+}"}]