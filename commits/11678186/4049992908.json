[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 9, "deletions": 1, "changes": 10, "file_content_changes": "@@ -65,7 +65,7 @@ template <typename Int> Int product(llvm::ArrayRef<Int> arr) {\n \n template <typename Int> Int ceil(Int m, Int n) { return (m + n - 1) / n; }\n \n-// output[i] = input[order[i]]\n+/// output[i] = input[order[i]]\n template <typename T, typename RES_T = T>\n SmallVector<RES_T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n   size_t rank = order.size();\n@@ -80,6 +80,14 @@ SmallVector<RES_T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n                         triton::gpu::DotOperandEncodingAttr &dotOperandLayout);\n \n+/// Multi-root DAG topological sort.\n+/// Performs a topological sort of the Operation in the `toSort` SetVector.\n+/// Returns a topologically sorted SetVector.\n+/// It is faster than mlir::topologicalSort because it prunes nodes that have\n+/// been visited before.\n+SetVector<Operation *>\n+multiRootTopologicalSort(const SetVector<Operation *> &toSort);\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_UTILITY_H"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 61, "deletions": 0, "changes": 61, "file_content_changes": "@@ -164,4 +164,65 @@ bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n          dotOperandLayout.getParent() == mmaLayout;\n }\n \n+namespace {\n+/// DFS post-order implementation that maintains a global count to work across\n+/// multiple invocations, to help implement topological sort on multi-root DAGs.\n+/// We traverse all operations but only record the ones that appear in\n+/// `toSort` for the final result.\n+struct DFSState {\n+  DFSState(const SetVector<Operation *> &set) : toSort(set), seen() {}\n+  const SetVector<Operation *> &toSort;\n+  SmallVector<Operation *, 16> topologicalCounts;\n+  DenseSet<Operation *> seen;\n+};\n+\n+void dfsPostorder(Operation *root, DFSState *state) {\n+  SmallVector<Operation *> queue(1, root);\n+  std::vector<Operation *> ops;\n+  while (!queue.empty()) {\n+    Operation *current = queue.pop_back_val();\n+    if (!state->seen.insert(current).second)\n+      continue;\n+    ops.push_back(current);\n+    for (Value result : current->getResults()) {\n+      for (Operation *op : result.getUsers())\n+        queue.push_back(op);\n+    }\n+    for (Region &region : current->getRegions()) {\n+      for (Operation &op : region.getOps())\n+        queue.push_back(&op);\n+    }\n+  }\n+\n+  for (Operation *op : llvm::reverse(ops)) {\n+    if (state->toSort.count(op) > 0)\n+      state->topologicalCounts.push_back(op);\n+  }\n+}\n+\n+} // namespace\n+\n+SetVector<Operation *>\n+multiRootTopologicalSort(const SetVector<Operation *> &toSort) {\n+  if (toSort.empty()) {\n+    return toSort;\n+  }\n+\n+  // Run from each root with global count and `seen` set.\n+  DFSState state(toSort);\n+  for (auto *s : toSort) {\n+    assert(toSort.count(s) == 1 && \"NYI: multi-sets not supported\");\n+    dfsPostorder(s, &state);\n+  }\n+\n+  // Reorder and return.\n+  SetVector<Operation *> res;\n+  for (auto it = state.topologicalCounts.rbegin(),\n+            eit = state.topologicalCounts.rend();\n+       it != eit; ++it) {\n+    res.insert(*it);\n+  }\n+  return res;\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -635,7 +635,7 @@ class RematerializeBackward : public mlir::RewritePattern {\n       else\n         sortedValues.push_back(v);\n     }\n-    tmp = mlir::topologicalSort(tmp);\n+    tmp = mlir::multiRootTopologicalSort(tmp);\n     for (Operation *op : tmp)\n       sortedValues.push_back(op->getResult(0));\n "}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -110,7 +110,7 @@ def check_type_supported(dtype):\n         pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n \n \n-@pytest.mark.parametrize(\"dtype_x\", [dtype_x for dtype_x in dtypes] + [\"bfloat16\"])\n+@pytest.mark.parametrize(\"dtype_x\", list(dtypes) + [\"bfloat16\"])\n def test_empty_kernel(dtype_x, device='cuda'):\n     SIZE = 128\n \n@@ -773,7 +773,7 @@ def kernel(X, Z, BITCAST: tl.constexpr):\n         assert to_numpy(z_tri) == z_ref\n \n \n-@pytest.mark.parametrize(\"dtype_str\", [dtype_str for dtype_str in torch_dtypes])\n+@pytest.mark.parametrize(\"dtype_str\", list(torch_dtypes))\n def test_store_constant(dtype_str):\n     check_type_supported(dtype_str)\n "}, {"filename": "python/triton/language/extern.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -65,9 +65,9 @@ def elementwise(lib_name: str, lib_path: str, args: list, arg_type_symbol_dict:\n     if not all_scalar:\n         broadcast_arg = dispatch_args[0]\n         # Get the broadcast shape over all the arguments\n-        for i in range(len(dispatch_args)):\n+        for i, item in enumerate(dispatch_args):\n             _, broadcast_arg = semantic.binary_op_type_checking_impl(\n-                dispatch_args[i], broadcast_arg, _builder)\n+                item, broadcast_arg, _builder)\n         # Change the shape of each argument based on the broadcast shape\n         for i in range(len(dispatch_args)):\n             dispatch_args[i], _ = semantic.binary_op_type_checking_impl("}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 5, "deletions": 6, "changes": 11, "file_content_changes": "@@ -502,7 +502,7 @@ def view(input: tl.tensor,\n \n \n def expand_dims(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n-    dst_shape = [s for s in input.type.shape]\n+    dst_shape = list(input.type.shape)\n     dst_shape.insert(axis, 1)\n     ret_ty = tl.block_type(input.type.scalar, dst_shape)\n     return tl.tensor(builder.create_expand_dims(input.handle, axis), ret_ty)\n@@ -533,10 +533,10 @@ def broadcast_impl_shape(input: tl.tensor,\n         raise ValueError(f\"Cannot broadcast, rank mismatch: {src_shape}, {shape}\")\n     if shape == src_shape:\n         return input\n-    for i in range(len(src_shape)):\n-        if shape[i] != src_shape[i] and src_shape[i] != 1:\n+    for i, item in enumerate(src_shape):\n+        if shape[i] != item and item != 1:\n             raise ValueError(f\"Cannot broadcast, the expanded size of the tensor ({shape[i]})\"\n-                             f\" must match the existing size ({src_shape[i]}) at non-singleton dimension\"\n+                             f\" must match the existing size ({item}) at non-singleton dimension\"\n                              f\" {i}: {src_shape}, {shape}\")\n     ret_ty = tl.block_type(input.type.scalar, shape)\n     return tl.tensor(builder.create_broadcast(input.handle, shape), ret_ty)\n@@ -576,8 +576,7 @@ def broadcast_impl_value(lhs: tl.tensor,\n         assert len(rhs_shape) == len(lhs_shape)\n \n         ret_shape = []\n-        for i in range(len(lhs_shape)):\n-            left = lhs_shape[i]\n+        for i, left in enumerate(lhs_shape):\n             right = rhs_shape[i]\n             if left == 1:\n                 ret_shape.append(right)"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -69,7 +69,7 @@ def kernel_call():\n     def run(self, *args, **kwargs):\n         self.nargs = dict(zip(self.arg_names, args))\n         if len(self.configs) > 1:\n-            key = tuple([args[i] for i in self.key_idx])\n+            key = tuple(args[i] for i in self.key_idx)\n             if key not in self.cache:\n                 # prune configs\n                 pruned_configs = self.prune_configs(kwargs)"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -195,7 +195,7 @@ def _make_signature(self, sig_key):\n         return signature\n \n     def _make_constants(self, constexpr_key):\n-        constants = {i: k for i, k in zip(self.constexprs, constexpr_key)}\n+        constants = dict(zip(self.constexprs, constexpr_key))\n         return constants\n \n     def _call_hook(self, key, signature, device, constants, num_warps, num_stages, extern_libs, configs):\n@@ -298,10 +298,10 @@ def __init__(self, fn, version=None, do_not_specialize=None):\n         # function signature information\n         signature = inspect.signature(fn)\n         self.arg_names = [v.name for v in signature.parameters.values()]\n-        self.has_defaults = any([v.default != inspect._empty for v in signature.parameters.values()])\n+        self.has_defaults = any(v.default != inspect._empty for v in signature.parameters.values())\n         # specialization hints\n         self.do_not_specialize = [] if do_not_specialize is None else do_not_specialize\n-        self.do_not_specialize = set([self.arg_names.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize])\n+        self.do_not_specialize = {self.arg_names.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize}\n         # function source code (without decorators)\n         self.src = textwrap.dedent(inspect.getsource(fn))\n         self.src = self.src[self.src.find(\"def\"):]"}]