[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -210,7 +210,7 @@ def forward(ctx, q, k, v, sm_scale):\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         num_warps = 4 if Lk <= 64 else 8\n \n-        h = _fwd_kernel[grid](\n+        _fwd_kernel[grid](\n             q, k, v, sm_scale,\n             L, m,\n             o,\n@@ -246,7 +246,7 @@ def backward(ctx, do):\n             do_scaled, delta,\n             BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n         )\n-        h = _bwd_kernel[(ctx.grid[1],)](\n+        _bwd_kernel[(ctx.grid[1],)](\n             q, k, v, ctx.sm_scale,\n             o, do_scaled,\n             dq, dk, dv,"}]