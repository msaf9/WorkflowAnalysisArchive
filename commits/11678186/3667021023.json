[{"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 2, "deletions": 5, "changes": 7, "file_content_changes": "@@ -199,7 +199,7 @@ def get_proper_err(a, b, golden):\n     [128, 64, 128, 4, 128, 64, 32, False, True],\n ])\n def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n-    guard_for_volta(NUM_WARPS, TRANS_A, TRANS_B):\n+    guard_for_volta(NUM_WARPS, TRANS_A, TRANS_B)\n \n     if (TRANS_A):\n         a = torch.randn((SIZE_K, SIZE_M), device='cuda', dtype=torch.float16).T\n@@ -307,13 +307,10 @@ def guard_for_volta(num_warps, trans_a, trans_b, is_int8=False, is_tf32=False):\n     Tell whether the test case is valid on Volta GPU.\n     Some features are WIP, so the corresponding support are missing.\n     '''\n-    if is_int8 or is_tf32:\n-        return False\n-\n     capability = torch.cuda.get_device_capability()\n     is_on_Volta = capability[0] < 8\n     # TODO[Superjomn]: Remove the constraints below when features are ready\n-    is_feature_ready = not (trans_a or trans_b)\n+    is_feature_ready = not (is_int8 or is_tf32 or trans_a or trans_b)\n \n     if is_on_Volta:\n         if not is_feature_ready:"}]