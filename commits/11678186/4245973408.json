[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 22, "deletions": 8, "changes": 30, "file_content_changes": "@@ -3,9 +3,10 @@ name: Integration Tests\n on:\n   workflow_dispatch:\n   pull_request:\n-    branches:\n-      - main\n-      - triton-mlir\n+    branches: [main]\n+  merge_group:\n+    branches: [main]\n+    types: [checks_requested]\n \n concurrency:\n   group: ${{ github.ref }}\n@@ -21,7 +22,7 @@ jobs:\n         id: set-matrix\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n-            echo '::set-output name=matrix::[[\"self-hosted\", \"A10\"], [\"self-hosted\", \"V100\"], \"macos-10.15\"]'\n+            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], \"macos-10.15\"]'\n           else\n             echo '::set-output name=matrix::[\"ubuntu-latest\", \"macos-10.15\"]'\n           fi\n@@ -43,29 +44,33 @@ jobs:\n         run: |\n           rm -rf ~/.triton/cache/\n \n+      - name: Update path\n+        run: |\n+          echo \"$HOME/.local/bin/\" >> $GITHUB_PATH\n+\n       - name: Check imports\n         if: ${{ matrix.runner != 'macos-10.15' }}\n         run: |\n-          pip install isort\n+          pip3 install isort\n           isort -c ./python || ( echo '::error title=Imports not sorted::Please run \\\"isort ./python\\\"' ; exit 1 )\n \n       - name: Check python style\n         if: ${{ matrix.runner != 'macos-10.15' }}\n         run: |\n-          pip install autopep8\n+          pip3 install autopep8\n           autopep8 -a -r -d --exit-code ./python || ( echo '::error title=Style issues::Please run \\\"autopep8 -a -r -i ./python\\\"' ; exit 1 )\n \n       - name: Check cpp style\n         if: ${{ matrix.runner != 'macos-10.15' }}\n         run: |\n-          pip install clang-format\n+          pip3 install clang-format\n           find . -regex '.*\\.\\(cpp\\|hpp\\|h\\|cc\\)' -not -path \"./python/triton/*\" -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file --dry-run -Werror -i ||\n           (echo '::error title=Style issues:: Please run `find . -regex \".*\\.\\(cpp\\|hpp\\|h\\|cc\\)\" -not -path \"./python/triton/*\" -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file -i`' ; exit 1)\n \n       - name: Flake8\n         if: ${{ matrix.runner != 'macos-10.15' }}\n         run: |\n-          pip install flake8\n+          pip3 install flake8\n           flake8 --config ./python/setup.cfg ./python || ( echo '::error::Flake8 failed; see logs for errors.' ; exit 1 )\n \n       - name: Install Triton\n@@ -94,3 +99,12 @@ jobs:\n           cd python/\n           cd \"build/$(ls build)\"\n           ctest\n+\n+      - name: Regression tests\n+        if: ${{ contains(matrix.runner, 'A100') }}\n+        run: |\n+          cd python/test/regression\n+          sudo nvidia-smi -i 0 -pm 1\n+          sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n+          pytest -vs .\n+          sudo nvidia-smi -i 0 -rgc\n\\ No newline at end of file"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -136,6 +136,12 @@ static std::map<std::string, std::string> getExternLibs(mlir::ModuleOp module) {\n \n   if (!funcs.empty()) {\n     static const std::string libdevice = \"libdevice\";\n+    // first search for environmental path\n+    std::string env_path = ::triton::tools::getenv(\"TRITON_LIBDEVICE_PATH\");\n+    if (!env_path.empty()) {\n+      externLibs.try_emplace(libdevice, env_path);\n+      return externLibs;\n+    }\n     namespace fs = std::filesystem;\n     // Search for libdevice relative to its library path if used from Python\n     // Then native code is in `triton/_C/libtriton.so` and libdevice in"}, {"filename": "python/src/main.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-\ufeff#include <pybind11/pybind11.h>\n+#include <pybind11/pybind11.h>\n \n void init_superblocking(pybind11::module &m);\n void init_torch_utils(pybind11::module &m);"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 66, "deletions": 27, "changes": 93, "file_content_changes": "@@ -8,7 +8,7 @@\n import triton.language as tl\n from triton.testing import get_dram_gbps, get_max_tensorcore_tflops\n \n-DEVICE_NAME = 'v100'\n+DEVICE_NAME = {7: 'v100', 8: 'a100'}[torch.cuda.get_device_capability()[0]]\n \n #######################\n # Utilities\n@@ -34,7 +34,6 @@ def nvsmi(attrs):\n matmul_data = {\n     'v100': {\n         # square\n-        (256, 256, 256): {'float16': 0.027},\n         (512, 512, 512): {'float16': 0.158},\n         (1024, 1024, 1024): {'float16': 0.466},\n         (2048, 2048, 2048): {'float16': 0.695},\n@@ -51,29 +50,26 @@ def nvsmi(attrs):\n         (4096, 64, 4096): {'float16': 0.264},\n         (8192, 64, 8192): {'float16': 0.452},\n     },\n+    # NOTE:\n+    # A100 in the CI server is slow-ish for some reason.\n+    # On some other servers, we are getting about 90% peak for 8kx8x8k float16\n     'a100': {\n-        (256, 256, 256): {'float16': 0.010, 'float32': 0.0214, 'int8': 0.006},\n-        (512, 512, 512): {'float16': 0.061, 'float32': 0.109, 'int8': 0.030},\n-        (1024, 1024, 1024): {'float16': 0.287, 'float32': 0.331, 'int8': 0.169},\n-        (2048, 2048, 2048): {'float16': 0.604, 'float32': 0.599, 'int8': 0.385},\n-        (4096, 4096, 4096): {'float16': 0.842, 'float32': 0.862, 'int8': 0.711},\n-        (8192, 8192, 8192): {'float16': 0.896, 'float32': 0.932, 'int8': 0.860},\n+        (512, 512, 512): {'float16': 0.08, 'float32': 0.13, 'int8': 0.05},\n+        (1024, 1024, 1024): {'float16': 0.33, 'float32': 0.35, 'int8': 0.169},\n+        (2048, 2048, 2048): {'float16': 0.64, 'float32': 0.57, 'int8': 0.34},\n+        (4096, 4096, 4096): {'float16': 0.81, 'float32': 0.75, 'int8': 0.46},\n+        (8192, 8192, 8192): {'float16': 0.77, 'float32': 0.85, 'int8': 0.51},\n         # tall-skinny\n         (16, 1024, 1024): {'float16': 0.0077, 'float32': 0.0127, 'int8': 0.005},\n         (16, 4096, 4096): {'float16': 0.0363, 'float32': 0.0457, 'int8': 0.0259},\n-        (16, 8192, 8192): {'float16': 0.0564, 'float32': 0.0648, 'int8': 0.0431},\n+        (16, 8192, 8192): {'float16': 0.07, 'float32': 0.0648, 'int8': 0.0431},\n         (64, 1024, 1024): {'float16': 0.0271, 'float32': 0.0509, 'int8': 0.0169},\n-        (64, 4096, 4096): {'float16': 0.141, 'float32': 0.162, 'int8': 0.097},\n-        (64, 8192, 8192): {'float16': 0.244, 'float32': 0.257, 'int8': 0.174},\n+        (64, 4096, 4096): {'float16': 0.16, 'float32': 0.162, 'int8': 0.097},\n+        (64, 8192, 8192): {'float16': 0.30, 'float32': 0.257, 'int8': 0.174},\n         (1024, 64, 1024): {'float16': 0.0263, 'float32': 0.0458, 'int8': 0.017},\n-        (4096, 64, 4096): {'float16': 0.135, 'float32': 0.177, 'int8': 0.102},\n-        (8192, 64, 8192): {'float16': 0.216, 'float32': 0.230, 'int8': 0.177},\n+        (4096, 64, 4096): {'float16': 0.16, 'float32': 0.177, 'int8': 0.102},\n+        (8192, 64, 8192): {'float16': 0.25, 'float32': 0.230, 'int8': 0.177},\n     }\n-    #   # deep reductions\n-    #   (64  , 64  , 16384) : {'a100': 0.},\n-    #   (64  , 64  , 65536) : {'a100': 0.},\n-    #   (256 , 256 , 8192 ) : {'a100': 0.},\n-    #   (256 , 256 , 32768) : {'a100': 0.},\n }\n \n \n@@ -88,9 +84,7 @@ def test_matmul(M, N, K, dtype_str):\n     torch.manual_seed(0)\n     ref_gpu_util = matmul_data[DEVICE_NAME][(M, N, K)][dtype_str]\n     cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n-    ref_sm_clock = sm_clocks[DEVICE_NAME]\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n-    assert abs(cur_sm_clock - ref_sm_clock) < 10, f'GPU SMs must run at {ref_sm_clock} MHz'\n     if dtype == torch.int8:\n         a = torch.randint(-128, 127, (M, K), dtype=dtype, device='cuda')\n         b = torch.randint(-128, 127, (N, K), dtype=dtype, device='cuda')\n@@ -99,10 +93,10 @@ def test_matmul(M, N, K, dtype_str):\n         a = torch.randn((M, K), dtype=dtype, device='cuda')\n         b = torch.randn((K, N), dtype=dtype, device='cuda')\n     fn = lambda: triton.ops.matmul(a, b)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=25, rep=1000)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)\n+    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n \n #######################\n@@ -149,16 +143,61 @@ def _add(x_ptr, y_ptr, output_ptr, n_elements,\n def test_elementwise(N):\n     torch.manual_seed(0)\n     ref_gpu_util = elementwise_data[DEVICE_NAME][N]\n-    cur_mem_clock = nvsmi(['clocks.current.memory'])[0]\n-    ref_mem_clock = mem_clocks[DEVICE_NAME]\n     max_gpu_perf = get_dram_gbps()\n-    assert abs(cur_mem_clock - ref_mem_clock) < 10, f'GPU memory must run at {ref_mem_clock} MHz'\n     z = torch.empty((N, ), dtype=torch.float16, device='cuda')\n     x = torch.randn_like(z)\n     y = torch.randn_like(z)\n     grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n     fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=25, rep=250)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=500)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)\n+    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+\n+#######################\n+# Flash-Attention\n+#######################\n+\n+\n+flash_attention_data = {\n+    \"a100\": {\n+        (4, 48, 4096, 64, 'forward', 'float16'): 0.37,\n+        (4, 48, 4096, 64, 'backward', 'float16'): 0.25,\n+    }\n+}\n+\n+\n+@pytest.mark.parametrize(\"Z, H, N_CTX, D_HEAD\", [[4, 48, 4096, 64]])\n+@pytest.mark.parametrize(\"mode\", ['forward', 'backward'])\n+@pytest.mark.parametrize(\"dtype_str\", ['float16'])\n+def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n+    is_backward = mode == 'backward'\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] < 8:\n+        pytest.skip(\"Flash attention only supported for compute capability < 80\")\n+    torch.manual_seed(20)\n+    dtype = {'float16': torch.float16, 'float32': torch.float32, 'int8': torch.int8}[dtype_str]\n+    # init data\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n+    sm_scale = 0.2\n+    # benchmark\n+    fn = lambda: triton.ops.attention(q, k, v, sm_scale)\n+    if is_backward:\n+        o = fn()\n+        do = torch.randn_like(o)\n+        fn = lambda: o.backward(do, retain_graph=True)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=500)\n+    # compute flops\n+    flops_per_matmul = 2. * Z * H * N_CTX * N_CTX * D_HEAD * 0.5\n+    total_flops = 2 * flops_per_matmul\n+    if is_backward:\n+        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n+    cur_gpu_perf = total_flops / ms * 1e-9\n+    # maximum flops\n+    cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n+    max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n+    cur_gpu_util = cur_gpu_perf / max_gpu_perf\n+    ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n+    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 13, "deletions": 8, "changes": 21, "file_content_changes": "@@ -16,7 +16,6 @@\n import warnings\n from collections import namedtuple\n from pathlib import Path\n-from sysconfig import get_paths\n from typing import Any, Callable, Dict, Tuple, Union\n \n import setuptools\n@@ -1314,11 +1313,6 @@ def default_cache_dir():\n     return os.path.join(os.environ[\"HOME\"], \".triton\", \"cache\")\n \n \n-def default_cuda_dir():\n-    default_dir = \"/usr/local/cuda\"\n-    return os.getenv(\"CUDA_HOME\", default=default_dir)\n-\n-\n class CacheManager:\n \n     def __init__(self, key):\n@@ -1376,7 +1370,9 @@ def quiet():\n \n def _build(name, src, srcdir):\n     cuda_lib_dirs = libcuda_dirs()\n-    cuda_path = os.environ.get('CUDA_PATH', default_cuda_dir())\n+    base_dir = os.path.dirname(__file__)\n+    cuda_path = os.path.join(base_dir, \"third_party\", \"cuda\")\n+\n     cu_include_dir = os.path.join(cuda_path, \"include\")\n     triton_include_dir = os.path.join(os.path.dirname(__file__), \"include\")\n     cuda_header = os.path.join(cu_include_dir, \"cuda.h\")\n@@ -1394,7 +1390,16 @@ def _build(name, src, srcdir):\n         cc = gcc if gcc is not None else clang\n         if cc is None:\n             raise RuntimeError(\"Failed to find C compiler. Please specify via CC environment variable.\")\n-    py_include_dir = get_paths()[\"include\"]\n+    # This function was renamed and made public in Python 3.10\n+    if hasattr(sysconfig, 'get_default_scheme'):\n+        scheme = sysconfig.get_default_scheme()\n+    else:\n+        scheme = sysconfig._get_default_scheme()\n+    # 'posix_local' is a custom scheme on Debian. However, starting Python 3.10, the default install\n+    # path changes to include 'local'. This change is required to use triton with system-wide python.\n+    if scheme == 'posix_local':\n+        scheme = 'posix_prefix'\n+    py_include_dir = sysconfig.get_paths(scheme=scheme)[\"include\"]\n \n     cc_cmd = [cc, src, \"-O3\", f\"-I{cu_include_dir}\", f\"-I{py_include_dir}\", f\"-I{srcdir}\", \"-shared\", \"-fPIC\", \"-lcuda\", \"-o\", so]\n     cc_cmd += [f\"-L{dir}\" for dir in cuda_lib_dirs]"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 9, "deletions": 4, "changes": 13, "file_content_changes": "@@ -9,6 +9,8 @@\n \n T = TypeVar('T')\n \n+TRITON_MAX_TENSOR_NUMEL = 131072\n+\n \n def _to_tensor(x, builder):\n     if isinstance(x, bool):\n@@ -254,6 +256,8 @@ def __init__(self, element_ty: dtype, shape: List):\n         self.numel = 1\n         for s in self.shape:\n             self.numel *= s\n+        if self.numel > TRITON_MAX_TENSOR_NUMEL:\n+            raise ValueError(f\"numel ({self.numel}) exceeds triton maximum tensor numel ({TRITON_MAX_TENSOR_NUMEL})\")\n \n         self.name = self.__str__()\n \n@@ -702,12 +706,13 @@ def num_programs(axis, _builder=None):\n @builtin\n def arange(start, end, _builder=None):\n     \"\"\"\n-    Returns contiguous values within the open interval [:code:`start`, :code:`end`).\n+    Returns contiguous values within the left-closed and right-open interval [:code:`start`, :code:`end`). \\\n+    End - Start must be less than or equal to TRITON_MAX_TENSOR_NUMEL = 131072\n \n     :param start: Start of the interval. Must be a power of two.\n-    :type start: int\n-    :param stop: End of the interval. Must be a power of two >= start.\n-    :type stop: int\n+    :type start: int32\n+    :param end: End of the interval. Must be a power of two > start.\n+    :type end: int32\n     \"\"\"\n     start = _constexpr_to_value(start)\n     end = _constexpr_to_value(end)"}, {"filename": "python/triton/language/libdevice.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -3,7 +3,8 @@\n from .. import impl\n from . import core, extern\n \n-LIBDEVICE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\", \"cuda\", \"lib\", \"libdevice.10.bc\")\n+LOCAL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\", \"cuda\", \"lib\", \"libdevice.10.bc\")\n+LIBDEVICE_PATH = os.getenv(\"TRITON_LIBDEVICE_PATH\", LOCAL_PATH)\n \n \n @impl.extern"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -478,6 +478,12 @@ def not_equal(input: tl.tensor,\n def arange(start: int, end: int, builder: ir.builder) -> tl.tensor:\n     if not isinstance(start, int) or not isinstance(end, int):\n         raise ValueError(\"arange's arguments must be of type tl.constexpr\")\n+    is_start_int64 = bool(start >> 32)\n+    is_end_int64 = bool(end >> 32)\n+    if is_start_int64 or is_end_int64:\n+        raise ValueError(\"arange must fit in int32\")\n+    if end <= start:\n+        raise ValueError(\"arange's end argument must be greater than the start argument\")\n \n     shape = [end - start]\n     ret_ty = tl.block_type(tl.int32, shape)\n@@ -647,6 +653,8 @@ def cast(input: tl.tensor,\n          dst_ty: tl.dtype,\n          builder: ir.builder) -> tl.tensor:\n     src_ty = input.type\n+    if isinstance(dst_ty, tl.constexpr):\n+        dst_ty = dst_ty.value\n     if src_ty.is_block():\n         dst_ty = tl.block_type(dst_ty, input.type.get_block_shapes())\n     if src_ty == dst_ty:"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -93,20 +93,23 @@ def assert_almost_equal(x, y, decimal=2, err_msg=''):\n     npt.assert_array_almost_equal(x, y, err_msg=err_msg, decimal=decimal)\n \n \n-def allclose(x, y, tol=1e-2):\n+def allclose(x, y, atol=0, rtol=1e-2):\n+    if not isinstance(x, torch.Tensor):\n+        x = torch.tensor(x)\n+    if not isinstance(y, torch.Tensor):\n+        y = torch.tensor(y)\n     if x.dtype != y.dtype:\n         raise RuntimeError(f'{x.dtype} did not match with {x.dtype}')\n     if x.shape != y.shape:\n         raise RuntimeError(f'{x.shape} did not match with {y.shape}')\n     if x.dtype == torch.bool:\n         return torch.sum(x ^ y) == 0\n     if x.dtype in [torch.int8, torch.int16, torch.int32, torch.int64]:\n-        tol = 0\n+        rtol = 0\n     diff = abs(x - y)\n     x_max = torch.max(x)\n     y_max = torch.max(y)\n-    err = torch.max(diff) / torch.max(x_max, y_max)\n-    return err <= tol\n+    return torch.max(diff) <= atol + rtol * torch.max(x_max, y_max)\n \n \n def nvsmi(attrs):"}, {"filename": "python/triton/tools/build_extern.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -289,7 +289,8 @@ def _output_stubs(self) -> str:\n         #   return extern.dispatch(\"libdevice\", <path>, <args>, <arg_type_symbol_dict>, _builder)\n         import_str = \"from . import core, extern\\n\"\n         import_str += \"import os\\n\"\n-        header_str = \"LIBDEVICE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \\\"..\\\", \\\"third_party\\\", \\\"cuda\\\", \\\"lib\\\", \\\"libdevice.10.bc\\\")\"\n+        header_str = \"LOCAL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \\\"..\\\", \\\"third_party\\\", \\\"cuda\\\", \\\"lib\\\", \\\"libdevice.10.bc\\\")\\n\"\n+        header_str += \"LIBDEVICE_PATH = os.getenv(\\\"TRITON_LIBDEVICE_PATH\\\", LOCAL_PATH)\\n\"\n         func_str = \"\"\n         for symbols in self._symbol_groups.values():\n             func_str += \"@extern.extern\\n\""}]