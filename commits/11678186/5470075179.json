[{"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -3,6 +3,9 @@\n ===============\n This is a Triton implementation of the Flash Attention algorithm\n (see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n+\n+Sequence Parallel implementation inspired by HazyResearch\n+(see https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attn_triton.py)\n \"\"\"\n \n import torch"}]