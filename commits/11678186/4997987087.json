[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 64, "deletions": 64, "changes": 128, "file_content_changes": "@@ -130,6 +130,17 @@ def __str__(self):\n         return f\"#triton_gpu.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}}}>\"\n \n \n+class SharedLayout:\n+    def __init__(self, vec, per_phase, max_phase, order):\n+        self.vec = str(vec)\n+        self.per_phase = str(per_phase)\n+        self.max_phase = str(max_phase)\n+        self.order = str(order)\n+\n+    def __str__(self):\n+        return f\"#triton_gpu.shared<{{vec={self.vec}, perPhase={self.per_phase}, maxPhase={self.max_phase}, order={self.order}}}>\"\n+\n+\n @pytest.mark.parametrize(\"dtype_x\", list(dtypes) + [\"bfloat16\"])\n def test_empty_kernel(dtype_x, device='cuda'):\n     SIZE = 128\n@@ -1593,6 +1604,59 @@ def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device='cuda'):\n     np.testing.assert_allclose(y_ref, y_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n \n \n+layouts = [\n+    [BlockedLayout([1, 1], [16, 2], [1, 4], [0, 1]), SharedLayout(1, 1, 1, [1, 0])],\n+    [BlockedLayout([1, 1], [16, 2], [1, 4], [0, 1]), SharedLayout(4, 2, 4, [1, 0])],\n+    [BlockedLayout([1, 1], [16, 2], [1, 4], [0, 1]), SharedLayout(2, 2, 4, [1, 0])],\n+    [BlockedLayout([1, 1], [8, 4], [2, 2], [0, 1]), SharedLayout(4, 2, 4, [1, 0])],\n+]\n+\n+\n+@pytest.mark.parametrize(\"M, N\", [[64, 64], [64, 128], [128, 64], [128, 128]])\n+@pytest.mark.parametrize(\"src, dst\", layouts)\n+def test_convert_shared(M, N, src, dst, device='cuda'):\n+    ir = f\"\"\"\n+    #src = {src}\n+    #dst = {dst}\n+    module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n+        tt.func public @kernel(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg2: i32 {{tt.divisibility = 16 : i32}}) {{\n+            %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+            %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n+            %2 = tt.splat %arg2 : (i32) -> tensor<{M}x1xi32, #src>\n+            %3 = arith.muli %1, %2 : tensor<{M}x1xi32, #src>\n+            %4 = tt.broadcast %3 : (tensor<{M}x1xi32, #src>) -> tensor<{M}x{N}xi32, #src>\n+            %5 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #src}}>>\n+            %6 = tt.expand_dims %5 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #src}}>>) -> tensor<1x{N}xi32, #src>\n+            %7 = tt.broadcast %6 : (tensor<1x{N}xi32, #src>) -> tensor<{M}x{N}xi32, #src>\n+            %8 = arith.addi %4, %7 : tensor<{M}x{N}xi32, #src>\n+            %9 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x{N}x!tt.ptr<i32>, #src>\n+            %10 = tt.addptr %9, %8 : tensor<{M}x{N}x!tt.ptr<i32>, #src>, tensor<{M}x{N}xi32, #src>\n+            %11 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #src>\n+            %12 = triton_gpu.convert_layout %11 : (tensor<{M}x{N}xi32, #src>) -> tensor<{M}x{N}xi32, #dst>\n+            %13 = triton_gpu.convert_layout %12 : (tensor<{M}x{N}xi32, #dst>) -> tensor<{M}x{N}xi32, #src>\n+            %15 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<{M}x{N}x!tt.ptr<i32>, #src>\n+            %16 = tt.addptr %15, %8 : tensor<{M}x{N}x!tt.ptr<i32>, #src>, tensor<{M}x{N}xi32, #src>\n+            tt.store %16, %13 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{M}x{N}xi32, #src>\n+            tt.return\n+        }}\n+    }}\n+    \"\"\"\n+    import tempfile\n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n+        f.write(ir)\n+        f.flush()\n+        kernel = triton.compile(f.name)\n+\n+    rs = RandomState(17)\n+    x = rs.randint(0, 4, (M, N)).astype('int32')\n+    y = np.zeros((M, N), dtype='int32')\n+    x_tri = torch.tensor(x, device=device)\n+    y_tri = torch.tensor(y, device=device)\n+    kernel[(1, 1, 1)](x_tri, y_tri, x_tri.stride(0))\n+    y_ref = x\n+    np.testing.assert_allclose(y_ref, y_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+\n+\n @triton.jit\n def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n     delta = mean_2 - mean_1\n@@ -1981,70 +2045,6 @@ def kernel_dynamic(out, val, dtype: tl.constexpr):\n # test arange\n # ---------------\n \n-def test_vecmat():\n-    @triton.jit\n-    def batched_vecmat(\n-        # inputs\n-        A,  # shape: [dim_m, dim_k]\n-        B,  # shape: [dim_m, dim_n, dim_k]\n-        # dimensions\n-        dim_m, dim_n, dim_k,\n-        # outputs\n-        output,\n-        # block information\n-        block_m: tl.constexpr, block_n: tl.constexpr, block_k: tl.constexpr\n-    ):\n-        m_index = tl.program_id(0)\n-        n_index = tl.program_id(1)\n-        # Output tile\n-        output_tile = (m_index * block_m + tl.arange(0, block_m))[:, None] * dim_n \\\n-            + (n_index * block_n + tl.arange(0, block_n))[None, :]\n-\n-        vecmat = tl.zeros([block_m, block_n], dtype=A.dtype.element_ty)\n-        for k_index in range(2):\n-            # Load A tile\n-            a_tile = (m_index * block_m + tl.arange(0, block_m))[:, None] * dim_k \\\n-                + (k_index * block_k + tl.arange(0, block_k))[None, :]\n-            a = tl.load(A + a_tile)\n-\n-            # Load B tile, transposed to [n, m, k] in order to broadcast A on a\n-            # leading dimension.\n-            b_tile = (m_index * block_m + tl.arange(0, block_m))[None, :, None] * dim_n * dim_k \\\n-                + (n_index * block_n + tl.arange(0, block_n))[:, None, None] * dim_k \\\n-                + (k_index * block_k + tl.arange(0, block_k))[None, None, :]\n-            b = tl.load(B + b_tile)\n-\n-            expanded_a, _ = tl.broadcast(a, b)\n-            vecmat += tl.trans(tl.sum(expanded_a * b, axis=2))\n-\n-        tl.store(output + output_tile, vecmat)\n-\n-    M, N, K = 128, 128, 128\n-    block_m, block_n, block_k = 16, 32, 64\n-\n-    rs = RandomState(17)\n-    A_vec = rs.randint(0, 4, (M, K)).astype('float32')\n-    B_vec = rs.randint(0, 4, (M, N, K)).astype('float32')\n-    A = A_vec\n-    B = B_vec\n-\n-    A_tri = torch.tensor(A, device='cuda')\n-    B_tri = torch.tensor(B, device='cuda')\n-    C_tri = torch.zeros((M, N), dtype=torch.float32, device='cuda')\n-\n-    grid = (M // block_m, N // block_n)\n-\n-    batched_vecmat[grid](A_tri, B_tri, M, N, K, C_tri,\n-                         block_m=block_m, block_n=block_n, block_k=block_k,\n-                         num_warps=4, num_stages=1)\n-\n-    A_expanded = A[:, np.newaxis, :]\n-    A_broadcasted = np.broadcast_to(A_expanded, (M, N, K))\n-    AB = A_broadcasted * B\n-    C_ref = np.sum(AB, axis=2)\n-\n-    np.testing.assert_allclose(C_ref, C_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n-\n \n @pytest.mark.parametrize(\"start\", [0, 1, 7, 16])\n def test_arange(start, device='cuda'):"}]