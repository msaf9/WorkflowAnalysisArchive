[{"filename": "python/tests/test_random.py", "status": "added", "additions": 198, "deletions": 0, "changes": 198, "file_content_changes": "@@ -0,0 +1,198 @@\n+import numpy as np\n+import pytest\n+import scipy.stats\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+#####################################\n+# Reference Philox Implementation\n+#####################################\n+\n+\n+class PhiloxConfig:\n+    def __init__(self, PHILOX_ROUND_A, PHILOX_ROUND_B, PHILOX_KEY_A, PHILOX_KEY_B, DTYPE):\n+        self.PHILOX_ROUND_A = np.array(PHILOX_ROUND_A, dtype=DTYPE)\n+        self.PHILOX_ROUND_B = np.array(PHILOX_ROUND_B, dtype=DTYPE)\n+        self.PHILOX_KEY_A = np.array(PHILOX_KEY_A, dtype=DTYPE)\n+        self.PHILOX_KEY_B = np.array(PHILOX_KEY_B, dtype=DTYPE)\n+        self.DTYPE = DTYPE\n+\n+\n+# This is better for GPU\n+PHILOX_32 = PhiloxConfig(\n+    PHILOX_KEY_A=0x9E3779B9,\n+    PHILOX_KEY_B=0xBB67AE85,\n+    PHILOX_ROUND_A=0xD2511F53,\n+    PHILOX_ROUND_B=0xCD9E8D57,\n+    DTYPE=np.uint32,\n+)\n+\n+# This is what numpy implements\n+PHILOX_64 = PhiloxConfig(\n+    PHILOX_KEY_A=0x9E3779B97F4A7C15,\n+    PHILOX_KEY_B=0xBB67AE8584CAA73B,\n+    PHILOX_ROUND_A=0xD2E7470EE14C6C93,\n+    PHILOX_ROUND_B=0xCA5A826395121157,\n+    DTYPE=np.uint64,\n+)\n+\n+\n+class CustomPhilox4x:\n+    def __init__(self, seed, config):\n+        self._config = config\n+        seed = self._into_pieces(seed)\n+        self._key = np.array(seed[:2], dtype=self._dtype)\n+        self._counter = np.array((0, 0) + seed[2:], dtype=self._dtype)\n+\n+    @property\n+    def _dtype(self):\n+        return self._config.DTYPE\n+\n+    def _into_pieces(self, n, pad=4):\n+        res = []\n+        while len(res) < pad:\n+            res.append(np.array(n, dtype=self._dtype))\n+            n >>= (np.dtype(self._dtype).itemsize * 8)\n+        assert n == 0\n+        return tuple(res)\n+\n+    def _multiply_low_high(self, a, b):\n+        low = a * b\n+        high = int(a) * int(b)\n+        high = np.array(high >> (np.dtype(self._dtype).itemsize * 8), dtype=self._dtype)\n+        return low, high\n+\n+    def _single_round(self, counter, key):\n+        lo0, hi0 = self._multiply_low_high(self._config.PHILOX_ROUND_A, counter[0])\n+        lo1, hi1 = self._multiply_low_high(self._config.PHILOX_ROUND_B, counter[2])\n+        ret0 = hi1 ^ counter[1] ^ key[0]\n+        ret1 = lo1\n+        ret2 = hi0 ^ counter[3] ^ key[1]\n+        ret3 = lo0\n+        return np.array([ret0, ret1, ret2, ret3], dtype=self._dtype)\n+\n+    def _raise_key(self, key):\n+        pk = [self._config.PHILOX_KEY_A, self._config.PHILOX_KEY_B]\n+        return key + np.array(pk, dtype=self._dtype)\n+\n+    def random_raw(self):\n+        counter = self._counter\n+        key = self._key\n+        for _ in range(10):\n+            counter = self._single_round(counter, key)\n+            key = self._raise_key(key)\n+        self.advance(1)\n+        return counter\n+\n+    def advance(self, n_steps):\n+        self._counter[0] += n_steps\n+        assert self._counter[0] < 2**32, \"FIXME: doesn't work for large offsets\"\n+\n+\n+class CustomPhilox(CustomPhilox4x):\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.buffer = []\n+\n+    def random_raw(self):\n+        if len(self.buffer) == 0:\n+            self.buffer = list(super().random_raw())[::-1]\n+        return int(self.buffer.pop())\n+\n+\n+#####################################\n+# Unit Tests\n+#####################################\n+\n+BLOCK = 1024\n+\n+# test generation of random uint32\n+\n+\n+@pytest.mark.parametrize('size, seed',\n+                         [(size, seed) for size in ['10', '4,53', '10000']\n+                          for seed in [0, 42, 124, 54, 0xffffffff, 0xdeadbeefcafeb0ba]]\n+                         )\n+def test_randint(size, seed, device='cuda'):\n+    size = list(map(int, size.split(',')))\n+\n+    @triton.jit\n+    def kernel(X, N, seed):\n+        offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n+        rand = tl.randint(seed, offset)\n+        tl.store(X + offset, rand, mask=offset < N)\n+    # triton result\n+    x = torch.empty(size, dtype=torch.int32, device=device)\n+    N = x.numel()\n+    grid = (triton.cdiv(N, BLOCK),)\n+    kernel[grid](x, N, seed)\n+    out_tri = x.cpu().numpy().astype(np.uint32).flatten().tolist()\n+    # reference result\n+    gen = CustomPhilox4x(seed, config=PHILOX_32)\n+    out_ref = [gen.random_raw()[0] for _ in out_tri]\n+    assert out_tri == out_ref\n+\n+# test uniform PRNG\n+\n+\n+@pytest.mark.parametrize('size, seed',\n+                         [(size, seed) for size in [1000000]\n+                          for seed in [0, 42, 124, 54]]\n+                         )\n+def test_rand(size, seed, device='cuda'):\n+    @triton.jit\n+    def kernel(X, N, seed):\n+        offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n+        rand = tl.rand(seed, offset)\n+        tl.store(X + offset, rand, mask=offset < N)\n+    # triton result\n+    x = torch.empty(size, dtype=torch.float32, device=device)\n+    N = x.numel()\n+    grid = (triton.cdiv(N, BLOCK),)\n+    kernel[grid](x, N, seed)\n+    assert all((x >= 0) & (x <= 1))\n+    assert scipy.stats.kstest(x.tolist(), 'uniform', args=(0, 1)).statistic < 0.01\n+\n+# test normal PRNG\n+\n+\n+@pytest.mark.parametrize('size, seed',\n+                         [(size, seed) for size in [1000000]\n+                          for seed in [0, 42, 124, 54]]\n+                         )\n+def test_randn(size, seed, device='cuda'):\n+    @triton.jit\n+    def kernel(X, N, seed):\n+        offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n+        rand = tl.randn(seed, offset)\n+        tl.store(X + offset, rand, mask=offset < N)\n+    # triton result\n+    x = torch.empty(size, dtype=torch.float32, device=device)\n+    N = x.numel()\n+    grid = (triton.cdiv(N, BLOCK),)\n+    kernel[grid](x, N, seed)\n+    assert abs(x.mean()) < 1e-2\n+    assert abs(x.std() - 1) < 1e-2\n+\n+\n+# tl.rand() should never produce >=1.0\n+\n+def test_rand_limits():\n+    @triton.jit\n+    def kernel(input, output, n: tl.constexpr):\n+        idx = tl.arange(0, n)\n+        x = tl.load(input + idx)\n+        y = tl.random.uint32_to_uniform_float(x)\n+        tl.store(output + idx, y)\n+\n+    min_max_int32 = torch.tensor([\n+        torch.iinfo(torch.int32).min,\n+        torch.iinfo(torch.int32).max,\n+    ], dtype=torch.int32, device='cuda')\n+    output = torch.empty(2, dtype=torch.float32, device='cuda')\n+    kernel[(1,)](min_max_int32, output, 2)\n+\n+    assert output[0] == output[1]\n+    assert 1.0 - torch.finfo(torch.float32).eps <= output[0].item() < 1.0"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -625,10 +625,12 @@ def visit_For(self, node):\n                 if name in liveins:\n                     assert self.is_triton_tensor(self.local_defs[name]), f'{name} is not tensor'\n                     assert self.is_triton_tensor(liveins[name])\n-                    if self.local_defs[name].type == liveins[name].type:\n-                        names.append(name)\n-                        init_args.append(triton.language.core._to_tensor(liveins[name], self.builder))\n-                        yields.append(triton.language.core._to_tensor(self.local_defs[name], self.builder))\n+                    if self.local_defs[name].type != liveins[name].type:\n+                        local_value = self.local_defs[name]\n+                        self.local_defs[name] = local_value.to(liveins[name].dtype, _builder=self.builder)\n+                    names.append(name)\n+                    init_args.append(triton.language.core._to_tensor(liveins[name], self.builder))\n+                    yields.append(triton.language.core._to_tensor(self.local_defs[name], self.builder))\n \n             # create ForOp\n             self.builder.set_insertion_point_to_end(insert_block)"}, {"filename": "python/triton/language/random.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1,10 +1,10 @@\n import triton\n from . import core as tl\n \n-PHILOX_KEY_A: tl.constexpr = -1640531527  # 0x9E3779B9\n-PHILOX_KEY_B: tl.constexpr = -1150833019  # 0xBB67AE85\n-PHILOX_ROUND_A: tl.constexpr = -766435501  # 0xD2511F53\n-PHILOX_ROUND_B: tl.constexpr = -845247145  # 0xCD9E8D57\n+PHILOX_KEY_A: tl.constexpr = 0x9E3779B9\n+PHILOX_KEY_B: tl.constexpr = 0xBB67AE85\n+PHILOX_ROUND_A: tl.constexpr = 0xD2511F53\n+PHILOX_ROUND_B: tl.constexpr = 0xCD9E8D57\n N_ROUNDS_DEFAULT = 10  # Default number of rounds for philox\n \n # -------------------"}]