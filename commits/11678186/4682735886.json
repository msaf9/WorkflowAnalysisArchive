[{"filename": ".github/workflows/documentation.yml", "status": "added", "additions": 52, "deletions": 0, "changes": 52, "file_content_changes": "@@ -0,0 +1,52 @@\n+name: Documentation\n+on:\n+  workflow_dispatch:\n+  schedule:\n+    - cron: \"0 0 * * *\"\n+\n+jobs:\n+\n+  Build-Documentation:\n+\n+    runs-on: [self-hosted, V100]\n+\n+    steps:\n+\n+      - name: Checkout gh-pages\n+        uses: actions/checkout@v2\n+        with:\n+          ref: 'gh-pages'\n+\n+      - name: Clear docs\n+        run: |\n+          rm -r /tmp/triton-docs\n+        continue-on-error: true\n+\n+      - name: Checkout branch\n+        uses: actions/checkout@v2\n+\n+      - name: Build docs\n+        run: |\n+          git fetch origin main\n+          cd docs\n+          sphinx-multiversion . _build/html/\n+\n+      - name: Publish docs\n+        run: |\n+          git branch\n+          # update docs\n+          mkdir /tmp/triton-docs;\n+          mv docs/_build/html/* /tmp/triton-docs/\n+          git checkout gh-pages\n+          cp -r CNAME /tmp/triton-docs/\n+          cp -r index.html /tmp/triton-docs/\n+          cp -r .nojekyll /tmp/triton-docs/\n+          rm -r *\n+          cp -r /tmp/triton-docs/* .\n+          git add .\n+          git commit -am \"[GH-PAGES] Updated website\"\n+          # publish docs\n+          eval `ssh-agent -s`\n+          DISPLAY=:0 SSH_ASKPASS=~/.ssh/give_pass.sh ssh-add ${{ secrets.SSH_KEY }} <<< ${{ secrets.SSH_PASS }}\n+          git remote set-url origin git@github.com:openai/triton.git\n+          git push"}, {"filename": "README.md", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -2,7 +2,7 @@\n   <img src=\"https://cdn.openai.com/triton/assets/triton-logo.png\" alt=\"Triton logo\" width=\"88\" height=\"100\">\n </div>\n \n-[![Wheels](https://github.com/openai/triton/actions/workflows/wheels.yml/badge.svg)](https://github.com/openai/triton/actions/workflows/wheels.yml)\n+[![Wheels](https://github.com/openai/triton/actions/workflows/wheels.yml/badge.svg?branch=release/2.0.x)](https://github.com/openai/triton/actions/workflows/wheels.yml)\n \n \n **`Documentation`** |\n@@ -52,7 +52,7 @@ Version 2.0 is out! New features include:\n \n # Contributing\n \n-Community contributions are more than welcome, whether it be to fix bugs or to add new features. For more detailed instructions, please visit our [contributor's guide](CONTRIBUTING.md).\n+Community contributions are more than welcome, whether it be to fix bugs or to add new features at [github](https://github.com/openai/triton/). For more detailed instructions, please visit our [contributor's guide](CONTRIBUTING.md).\n \n If you\u2019re interested in joining our team and working on Triton & GPU kernels, [we\u2019re hiring](https://openai.com/jobs/#acceleration)!\n "}, {"filename": "bin/CMakeLists.txt", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "file_content_changes": "@@ -21,6 +21,26 @@ target_link_libraries(triton-opt PRIVATE\n \n mlir_check_all_link_libraries(triton-opt)\n \n+add_llvm_executable(triton-reduce triton-reduce.cpp PARTIAL_SOURCES_INTENDED)\n+mlir_check_all_link_libraries(triton-reduce)\n+\n+llvm_update_compile_flags(triton-reduce)\n+target_link_libraries(triton-reduce PRIVATE\n+  TritonAnalysis\n+  TritonTransforms\n+  TritonGPUTransforms\n+  ${dialect_libs}\n+  ${conversion_libs}\n+  # tests\n+  TritonTestAnalysis\n+  # MLIR core\n+  MLIRReduceLib\n+  MLIRPass\n+  MLIRTransforms\n+)\n+\n+mlir_check_all_link_libraries(triton-reduce)\n+\n \n add_llvm_executable(triton-translate triton-translate.cpp PARTIAL_SOURCES_INTENDED)\n llvm_update_compile_flags(triton-translate)"}, {"filename": "bin/RegisterTritonDialects.h", "status": "added", "additions": 38, "deletions": 0, "changes": 38, "file_content_changes": "@@ -0,0 +1,38 @@\n+#pragma once\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+\n+#include \"triton/Dialect/Triton/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+\n+#include \"triton/Conversion/TritonGPUToLLVM/Passes.h\"\n+#include \"triton/Conversion/TritonToTritonGPU/Passes.h\"\n+\n+#include \"mlir/InitAllPasses.h\"\n+\n+namespace mlir {\n+namespace test {\n+void registerTestAliasPass();\n+void registerTestAlignmentPass();\n+void registerTestAllocationPass();\n+void registerTestMembarPass();\n+} // namespace test\n+} // namespace mlir\n+\n+inline void registerTritonDialects(mlir::DialectRegistry &registry) {\n+  mlir::registerAllPasses();\n+  mlir::registerTritonPasses();\n+  mlir::registerTritonGPUPasses();\n+  mlir::test::registerTestAliasPass();\n+  mlir::test::registerTestAlignmentPass();\n+  mlir::test::registerTestAllocationPass();\n+  mlir::test::registerTestMembarPass();\n+  mlir::triton::registerConvertTritonToTritonGPUPass();\n+  mlir::triton::registerConvertTritonGPUToLLVMPass();\n+\n+  // TODO: register Triton & TritonGPU passes\n+  registry.insert<mlir::triton::TritonDialect, mlir::cf::ControlFlowDialect,\n+                  mlir::triton::gpu::TritonGPUDialect, mlir::math::MathDialect,\n+                  mlir::arith::ArithDialect, mlir::scf::SCFDialect,\n+                  mlir::gpu::GPUDialect>();\n+}"}, {"filename": "bin/triton-opt.cpp", "status": "modified", "additions": 2, "deletions": 34, "changes": 36, "file_content_changes": "@@ -1,42 +1,10 @@\n-#include \"triton/Dialect/Triton/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"./RegisterTritonDialects.h\"\n \n-#include \"triton/Dialect/Triton/Transforms/Passes.h\"\n-#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n-\n-#include \"triton/Conversion/TritonGPUToLLVM/Passes.h\"\n-#include \"triton/Conversion/TritonToTritonGPU/Passes.h\"\n-\n-#include \"mlir/IR/Dialect.h\"\n-#include \"mlir/InitAllPasses.h\"\n #include \"mlir/Tools/mlir-opt/MlirOptMain.h\"\n \n-namespace mlir {\n-namespace test {\n-void registerTestAliasPass();\n-void registerTestAlignmentPass();\n-void registerTestAllocationPass();\n-void registerTestMembarPass();\n-} // namespace test\n-} // namespace mlir\n-\n int main(int argc, char **argv) {\n-  mlir::registerAllPasses();\n-  mlir::registerTritonPasses();\n-  mlir::registerTritonGPUPasses();\n-  mlir::test::registerTestAliasPass();\n-  mlir::test::registerTestAlignmentPass();\n-  mlir::test::registerTestAllocationPass();\n-  mlir::test::registerTestMembarPass();\n-  mlir::triton::registerConvertTritonToTritonGPUPass();\n-  mlir::triton::registerConvertTritonGPUToLLVMPass();\n-\n-  // TODO: register Triton & TritonGPU passes\n   mlir::DialectRegistry registry;\n-  registry.insert<mlir::triton::TritonDialect, mlir::cf::ControlFlowDialect,\n-                  mlir::triton::gpu::TritonGPUDialect, mlir::func::FuncDialect,\n-                  mlir::math::MathDialect, mlir::arith::ArithDialect,\n-                  mlir::scf::SCFDialect, mlir::gpu::GPUDialect>();\n+  registerTritonDialects(registry);\n \n   return mlir::asMainReturnCode(mlir::MlirOptMain(\n       argc, argv, \"Triton (GPU) optimizer driver\\n\", registry));"}, {"filename": "bin/triton-reduce.cpp", "status": "added", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -0,0 +1,11 @@\n+#include \"./RegisterTritonDialects.h\"\n+\n+#include \"mlir/Tools/mlir-reduce/MlirReduceMain.h\"\n+\n+int main(int argc, char **argv) {\n+  mlir::DialectRegistry registry;\n+  registerTritonDialects(registry);\n+\n+  mlir::MLIRContext context(registry);\n+  return mlir::failed(mlir::mlirReduceMain(argc, argv, context));\n+}"}, {"filename": "docs/conf.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -50,7 +50,7 @@ def forward_jit_fn(func):\n \n         def wrapped(obj, **kwargs):\n             import triton\n-            if isinstance(obj, triton.code_gen.JITFunction):\n+            if isinstance(obj, triton.runtime.JITFunction):\n                 obj = obj.fn\n             return old(obj)\n \n@@ -60,7 +60,7 @@ def wrapped(obj, **kwargs):\n \n     def documenter(app, obj, parent):\n         import triton\n-        if isinstance(obj, triton.code_gen.JITFunction):\n+        if isinstance(obj, triton.runtime.JITFunction):\n             obj = obj.fn\n         return old_documenter(app, obj, parent)\n "}, {"filename": "docs/programming-guide/chapter-1/introduction.rst", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -27,14 +27,14 @@ The main premise of this project is the following: programming paradigms based o\n     |                                                     |   :force:                                           |\n     |                                                     |                                                     |\n     |   #pragma parallel                                  |   #pragma parallel                                  |\n-    |   for(int m = 0; i < M; m++)                        |   for(int m = 0; m < M; m += MB)                    |\n+    |   for(int m = 0; m < M; m++)                        |   for(int m = 0; m < M; m += MB)                    |\n     |   #pragma parallel                                  |   #pragma parallel                                  |\n-    |   for(int n = 0; j < N; n++){                       |   for(int n = 0; n < N; n += NB){                   |\n+    |   for(int n = 0; n < N; n++){                       |   for(int n = 0; n < N; n += NB){                   |\n     |     float acc = 0;                                  |     float acc[MB, NB] = 0;                          |\n     |     for(int k = 0; k < K; k++)                      |     for(int k = 0; k < K; k += KB)                  |\n-    |       acc += A[i, k] * B[k, j];                     |       acc +=  A[m:m+MB, k:k+KB]                     |\n+    |       acc += A[m, k] * B[k, n];                     |       acc +=  A[m:m+MB, k:k+KB]                     |\n     |                                                     |             @ B[k:k+KB, n:n+NB];                    |\n-    |     C[i, j] = acc;                                  |     C[m:m+MB, n:n+NB] = acc;                        |\n+    |     C[m, n] = acc;                                  |     C[m:m+MB, n:n+NB] = acc;                        |\n     |   }                                                 |   }                                                 |\n     |                                                     |                                                     |\n     +-----------------------------------------------------+-----------------------------------------------------+"}, {"filename": "include/triton/Analysis/AxisInfo.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -54,6 +54,10 @@ class AxisInfo {\n \n   std::optional<int64_t> getConstantValue() const { return constantValue; }\n \n+  template <class T>\n+  static void\n+  initPessimisticStateFromFunc(int argNumber, T funcOp, DimVectorT *contiguity,\n+                               DimVectorT *divisibility, DimVectorT *constancy);\n   /// Comparison\n   bool operator==(const AxisInfo &other) const {\n     return (contiguity == other.contiguity) &&"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -34,6 +34,8 @@ class ReduceOpHelper {\n \n   unsigned getScratchSizeInBytes();\n \n+  bool isSupportedLayout();\n+\n private:\n   triton::ReduceOp op;\n   RankedTensorType srcTy{};\n@@ -88,8 +90,7 @@ template <typename T> T highestPowOf2Divisor(T n) {\n \n bool isSingleValue(Value value);\n \n-bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n-                        triton::gpu::DotOperandEncodingAttr &dotOperandLayout);\n+bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy);\n \n /// Multi-root DAG topological sort.\n /// Performs a topological sort of the Operation in the `toSort` SetVector."}, {"filename": "include/triton/Dialect/Triton/IR/TritonDialect.td", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -19,15 +19,16 @@ def Triton_Dialect : Dialect {\n       * Math:\n         * exp, sin, cos, log, ...\n       * StructuredControlFlow:\n-        * ForOp, IfOp, WhileOp, YieldOp, ConditionOp\n+        * for, if, while, yield, condition\n+      * ControlFlow:\n+        * br, cond_br\n   }];\n \n   let dependentDialects = [\n     \"arith::ArithDialect\",\n     \"math::MathDialect\",\n     \"scf::SCFDialect\",\n-    \"cf::ControlFlowDialect\",\n-    \"func::FuncDialect\"\n+    \"cf::ControlFlowDialect\"\n   ];\n \n   let extraClassDeclaration = [{"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 205, "deletions": 8, "changes": 213, "file_content_changes": "@@ -6,11 +6,15 @@ include \"triton/Dialect/Triton/IR/TritonTypes.td\"\n include \"triton/Dialect/Triton/IR/TritonAttrDefs.td\"\n include \"triton/Dialect/Triton/IR/TritonInterfaces.td\"\n include \"mlir/IR/OpBase.td\"\n+include \"mlir/IR/FunctionInterfaces.td\" // FunctionOpInterface\n+include \"mlir/IR/SymbolInterfaces.td\" // SymbolUserOpInterface\n+include \"mlir/IR/OpAsmInterface.td\" // OpAsmOpInterface\n include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/ControlFlowInterfaces.td\" // BranchOpInterface\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/CastInterfaces.td\" // CastOpInterface\n+include \"mlir/Interfaces/CallInterfaces.td\" // CallOpInterface\n \n //\n // Op Base\n@@ -20,7 +24,7 @@ class TT_Op<string mnemonic, list<Trait> traits = []> :\n }\n \n //\n-// CastOps\n+// Cast Ops\n //\n // Use cast ops in arith:\n //   bitcast\n@@ -93,7 +97,6 @@ def TT_FpToFpOp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n //\n // Pointer Arith Ops\n //\n-\n def TT_AddPtrOp : TT_Op<\"addptr\",\n                      [Pure,\n                       SameOperandsAndResultShape,\n@@ -209,7 +212,7 @@ def TT_StoreOp : TT_Op<\"store\",\n }\n \n //\n-// Atomic Op\n+// Atomic Ops\n //\n def TT_AtomicRMWOp : TT_Op<\"atomic_rmw\", [SameOperandsAndResultShape,\n                                           SameOperandsAndResultEncoding,\n@@ -256,7 +259,6 @@ def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [MemoryEffects<[MemRead]>,\n     let results = (outs TT_Type:$result);\n }\n \n-\n //\n // Shape Manipulation Ops\n //\n@@ -408,7 +410,7 @@ def TT_ReduceOp : TT_Op<\"reduce\", [Pure,\n }\n \n //\n-// External elementwise op\n+// External Elementwise op\n //\n def TT_ExtElemwiseOp : TT_Op<\"ext_elemwise\", [Pure, Elementwise, SameOperandsAndResultShape,\n                                               SameOperandsAndResultEncoding,\n@@ -449,7 +451,7 @@ def TT_MakeRangeOp : TT_Op<\"make_range\", [Pure]> {\n }\n \n //\n-// Make PrintOp\n+// Print Op\n //\n def TT_PrintOp : TT_Op<\"print\", [MemoryEffects<[MemWrite]>]>,\n   Arguments<(ins StrAttr:$prefix, Variadic<AnyTypeOf<[TT_Type]>>:$args)> {\n@@ -464,7 +466,7 @@ def TT_PrintOp : TT_Op<\"print\", [MemoryEffects<[MemWrite]>]>,\n }\n \n //\n-// Make AssertOp\n+// Assert Op\n //\n def TT_AssertOp : TT_Op<\"assert\", [MemoryEffects<[MemWrite]>]> {\n   let summary = \"Device-side assert, as in CUDA for correctness checking\";\n@@ -477,7 +479,7 @@ def TT_AssertOp : TT_Op<\"assert\", [MemoryEffects<[MemWrite]>]> {\n }\n \n //\n-// Make a Tensor Pointer\n+// Make Tensor Pointer Op\n //\n def TT_MakeTensorPtrOp : TT_Op<\"make_tensor_ptr\",\n                                [Pure,\n@@ -518,4 +520,199 @@ def TT_MakeTensorPtrOp : TT_Op<\"make_tensor_ptr\",\n   ];\n }\n \n+// The following ops, including `call`, `func`, and `return` are copied and modified from\n+// https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Func/IR/FuncOps.td\n+// We could revert it back once MLIR has a better inliner interface.\n+//\n+// Function Ops\n+//\n+def CallOp : TT_Op<\"call\", [CallOpInterface, /*MemRefsNormalizable, */DeclareOpInterfaceMethods<SymbolUserOpInterface>]> {\n+  let summary = \"call operation\";\n+  let description = [{\n+    The `tt.call` operation represents a direct call to a function that is\n+    within the same symbol scope as the call. The operands and result types of\n+    the call must match the specified function type. The callee is encoded as a\n+    symbol reference attribute named \"callee\".\n+\n+    Example:\n+\n+    ```mlir\n+    %2 = tt.call @my_add(%0, %1) : (f32, f32) -> f32\n+    ```\n+  }];\n+\n+  let arguments = (ins FlatSymbolRefAttr:$callee, Variadic<AnyType>:$operands);\n+  let results = (outs Variadic<AnyType>);\n+\n+  let builders = [\n+    OpBuilder<(ins \"FuncOp\":$callee, CArg<\"ValueRange\", \"{}\">:$operands), [{\n+      $_state.addOperands(operands);\n+      $_state.addAttribute(\"callee\", SymbolRefAttr::get(callee));\n+      $_state.addTypes(callee.getFunctionType().getResults());\n+    }]>,\n+    OpBuilder<(ins \"SymbolRefAttr\":$callee, \"TypeRange\":$results,\n+      CArg<\"ValueRange\", \"{}\">:$operands), [{\n+      $_state.addOperands(operands);\n+      $_state.addAttribute(\"callee\", callee);\n+      $_state.addTypes(results);\n+    }]>,\n+    OpBuilder<(ins \"StringAttr\":$callee, \"TypeRange\":$results,\n+      CArg<\"ValueRange\", \"{}\">:$operands), [{\n+      build($_builder, $_state, SymbolRefAttr::get(callee), results, operands);\n+    }]>,\n+    OpBuilder<(ins \"StringRef\":$callee, \"TypeRange\":$results,\n+      CArg<\"ValueRange\", \"{}\">:$operands), [{\n+      build($_builder, $_state, StringAttr::get($_builder.getContext(), callee),\n+            results, operands);\n+    }]>];\n+\n+  let extraClassDeclaration = [{\n+    FunctionType getCalleeType() {\n+      return FunctionType::get(getContext(), getOperandTypes(), getResultTypes());\n+    }\n+\n+    /// Get the argument operands to the called function.\n+    operand_range getArgOperands() {\n+      return {arg_operand_begin(), arg_operand_end()};\n+    }\n+\n+    operand_iterator arg_operand_begin() { return operand_begin(); }\n+    operand_iterator arg_operand_end() { return operand_end(); }\n+\n+    /// Return the callee of this operation.\n+    CallInterfaceCallable getCallableForCallee() {\n+      return (*this)->getAttrOfType<SymbolRefAttr>(\"callee\");\n+    }\n+  }];\n+\n+  let assemblyFormat = [{\n+    $callee `(` $operands `)` attr-dict `:` functional-type($operands, results)\n+  }];\n+}\n+\n+def FuncOp : TT_Op<\"func\", [AffineScope, AutomaticAllocationScope, CallableOpInterface, FunctionOpInterface, IsolatedFromAbove, OpAsmOpInterface]> {\n+  let summary = \"An operation with a name containing a single `SSACFG` region\";\n+  let description = [{\n+    Operations within the function cannot implicitly capture values defined\n+    outside of the function, i.e. Functions are `IsolatedFromAbove`. All\n+    external references must use function arguments or attributes that establish\n+    a symbolic connection (e.g. symbols referenced by name via a string\n+    attribute like SymbolRefAttr). An external function declaration (used when\n+    referring to a function declared in some other module) has no body. While\n+    the MLIR textual form provides a nice inline syntax for function arguments,\n+    they are internally represented as \u201cblock arguments\u201d to the first block in\n+    the region.\n+\n+    Only dialect attribute names may be specified in the attribute dictionaries\n+    for function arguments, results, or the function itself.\n+\n+    Example:\n+\n+    ```mlir\n+    // External function definitions.\n+    tt.func @abort()\n+    tt.func @scribble(i32, i64, memref<? x 128 x f32, #layout_map0>) -> f64\n+\n+    // A function that returns its argument twice:\n+    tt.func @count(%x: i64) -> (i64, i64)\n+      attributes {fruit: \"banana\"} {\n+      return %x, %x: i64, i64\n+    }\n+\n+    // A function with an argument attribute\n+    tt.func @example_fn_arg(%x: i32 {swift.self = unit})\n+\n+    // A function with a result attribute\n+    tt.func @example_fn_result() -> (f64 {dialectName.attrName = 0 : i64})\n+\n+    // A function with an attribute\n+    tt.func @example_fn_attr() attributes {dialectName.attrName = false}\n+    ```\n+  }];\n+\n+  let arguments = (ins SymbolNameAttr:$sym_name,\n+                       TypeAttrOf<FunctionType>:$function_type,\n+                       OptionalAttr<StrAttr>:$sym_visibility,\n+                       OptionalAttr<DictArrayAttr>:$arg_attrs,\n+                       OptionalAttr<DictArrayAttr>:$res_attrs);\n+  let regions = (region AnyRegion:$body);\n+\n+  let builders = [OpBuilder<(ins\n+    \"StringRef\":$name, \"FunctionType\":$type,\n+    CArg<\"ArrayRef<NamedAttribute>\", \"{}\">:$attrs,\n+    CArg<\"ArrayRef<DictionaryAttr>\", \"{}\">:$argAttrs)\n+  >];\n+  let extraClassDeclaration = [{\n+    //===------------------------------------------------------------------===//\n+    // CallableOpInterface\n+    //===------------------------------------------------------------------===//\n+\n+    /// Returns the region on the current operation that is callable. This may\n+    /// return null in the case of an external callable object, e.g. an external\n+    /// function.\n+    ::mlir::Region *getCallableRegion() { return isExternal() ? nullptr : &getBody(); }\n+\n+    /// Returns the results types that the callable region produces when\n+    /// executed.\n+    ArrayRef<Type> getCallableResults() { return getFunctionType().getResults(); }\n+\n+    /// Returns the argument attributes for all callable region arguments or\n+    /// null if there are none.\n+    ::mlir::ArrayAttr getCallableArgAttrs() {\n+      return getArgAttrs().value_or(nullptr);\n+    }\n+\n+    /// Returns the result attributes for all callable region results or\n+    /// null if there are none.\n+    ::mlir::ArrayAttr getCallableResAttrs() {\n+      return getResAttrs().value_or(nullptr);\n+    }\n+\n+    //===------------------------------------------------------------------===//\n+    // FunctionOpInterface Methods\n+    //===------------------------------------------------------------------===//\n+\n+    /// Returns the argument types of this function.\n+    ArrayRef<Type> getArgumentTypes() { return getFunctionType().getInputs(); }\n+\n+    /// Returns the result types of this function.\n+    ArrayRef<Type> getResultTypes() { return getFunctionType().getResults(); }\n+\n+    //===------------------------------------------------------------------===//\n+    // SymbolOpInterface Methods\n+    //===------------------------------------------------------------------===//\n+\n+    bool isDeclaration() { return isExternal(); }\n+  }];\n+  let hasCustomAssemblyFormat = 1;\n+}\n+\n+def ReturnOp : TT_Op<\"return\", [Pure, HasParent<\"FuncOp\">, /*MemRefsNormalizable, */ReturnLike, Terminator]> {\n+  let summary = \"Function return operation\";\n+  let description = [{\n+    The `tt.return` operation represents a return operation within a function.\n+    The operation takes variable number of operands and produces no results.\n+    The operand number and types must match the signature of the function\n+    that contains the operation.\n+\n+    Example:\n+\n+    ```mlir\n+    tt.func @foo() : (i32, f8) {\n+      ...\n+      tt.return %0, %1 : i32, f8\n+    }\n+    ```\n+  }];\n+\n+  let arguments = (ins Variadic<AnyType>:$operands);\n+\n+  let builders = [OpBuilder<(ins), [{\n+    build($_builder, $_state, std::nullopt);\n+  }]>];\n+\n+  let assemblyFormat = \"attr-dict ($operands^ `:` type($operands))?\";\n+  let hasVerifier = 1;\n+}\n+\n #endif // Triton_OPS"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -23,23 +23,23 @@ namespace gpu {\n \n unsigned getElemsPerThread(Type type);\n \n-SmallVector<unsigned> getThreadsPerWarp(const Attribute &layout);\n+SmallVector<unsigned> getThreadsPerWarp(Attribute layout);\n \n-SmallVector<unsigned> getWarpsPerCTA(const Attribute &layout);\n+SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n \n-SmallVector<unsigned> getSizePerThread(const Attribute &layout);\n+SmallVector<unsigned> getSizePerThread(Attribute layout);\n \n-SmallVector<unsigned> getContigPerThread(const Attribute &layout);\n+SmallVector<unsigned> getContigPerThread(Attribute layout);\n \n-SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout);\n+SmallVector<unsigned> getThreadsPerCTA(Attribute layout);\n \n SmallVector<unsigned>\n-getShapePerCTA(const Attribute &layout,\n+getShapePerCTA(Attribute layout,\n                ArrayRef<int64_t> tensorShape = ArrayRef<int64_t>());\n \n-SmallVector<unsigned> getOrder(const Attribute &layout);\n+SmallVector<unsigned> getOrder(Attribute layout);\n \n-bool isaDistributedLayout(const Attribute &layout);\n+bool isaDistributedLayout(Attribute layout);\n \n } // namespace gpu\n } // namespace triton"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 12, "deletions": 6, "changes": 18, "file_content_changes": "@@ -33,7 +33,7 @@ namespace triton {\n constexpr int kPtrBitWidth = 64;\n \n static std::pair<SmallVector<unsigned>, SmallVector<unsigned>>\n-getCvtOrder(const Attribute &srcLayout, const Attribute &dstLayout) {\n+getCvtOrder(Attribute srcLayout, Attribute dstLayout) {\n   auto srcMmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n   auto srcDotLayout = srcLayout.dyn_cast<DotOperandEncodingAttr>();\n   auto dstMmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>();\n@@ -59,10 +59,10 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   Attribute dstLayout = dstTy.getEncoding();\n \n   // MmaToDotShortcut doesn't use shared mem\n-  if (auto mmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>())\n-    if (auto dotOperandLayout = dstLayout.dyn_cast<DotOperandEncodingAttr>())\n-      if (isMmaToDotShortcut(mmaLayout, dotOperandLayout))\n-        return {};\n+  if (srcLayout.isa<MmaEncodingAttr>() &&\n+      dstLayout.isa<DotOperandEncodingAttr>())\n+    if (isMmaToDotShortcut(srcTy, dstTy))\n+      return {};\n \n   assert(srcLayout && dstLayout &&\n          \"Unexpected layout in getScratchConfigForCvtLayout()\");\n@@ -389,7 +389,9 @@ class AllocationAnalysis {\n     //  | ******t1 ^^^^^^^^^v1^^^^^^^^^ ************t3\n     //  |---------------------------------------------| liveness range\n     //    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ...\n-    /// Start -> Liveness Range\n+    // If the available triple's range is less than a given buffer range,\n+    // we won't know if there has been an overlap without using graph coloring.\n+    // Start -> Liveness Range\n     using TripleMapT = std::multimap<size_t, Interval<size_t>>;\n     TripleMapT tripleMap;\n     tripleMap.insert(std::make_pair(0, Interval<size_t>()));\n@@ -415,6 +417,10 @@ class AllocationAnalysis {\n         tripleMap.insert(\n             {size + xSize, Interval{std::max(range.start(), xRange.start()),\n                                     std::min(range.end(), xRange.end())}});\n+        // We could either insert (range.start, xRange.start) or (range.start,\n+        // xRange.end), both are correct and determine the potential buffer\n+        // offset, and the graph coloring algorithm will solve the interference,\n+        // if any\n         if (range.start() < xRange.start())\n           tripleMap.insert({size, Interval{range.start(), xRange.end()}});\n         if (xRange.end() < range.end())"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 41, "deletions": 22, "changes": 63, "file_content_changes": "@@ -42,27 +42,52 @@ static constexpr int log2Int(int64_t num) {\n // AxisInfo\n //===----------------------------------------------------------------------===//\n \n+template <class T>\n+void AxisInfo::initPessimisticStateFromFunc(int argNumber, T funcOp,\n+                                            DimVectorT *contiguity,\n+                                            DimVectorT *divisibility,\n+                                            DimVectorT *constancy) {\n+  // liast of attributes that we care about\n+  SmallVector<std::pair<DimVectorT *, std::string>> retVecs;\n+  retVecs.push_back({contiguity, \"tt.contiguity\"});\n+  retVecs.push_back({divisibility, \"tt.divisibility\"});\n+  retVecs.push_back({constancy, \"tt.constancy\"});\n+  // initialize attributes one by one\n+  for (auto [vec, attrName] : retVecs) {\n+    Attribute attr = funcOp.getArgAttr(argNumber, attrName);\n+    if (auto int_attr = attr.dyn_cast_or_null<IntegerAttr>())\n+      *vec = DimVectorT(contiguity->size(), int_attr.getValue().getZExtValue());\n+    if (auto dense_attr = attr.dyn_cast_or_null<DenseElementsAttr>()) {\n+      auto vals = dense_attr.getValues<int>();\n+      *vec = DimVectorT(vals.begin(), vals.end());\n+    }\n+  }\n+}\n+\n AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n   auto rank = 1;\n   if (TensorType ty = value.getType().dyn_cast<TensorType>())\n     rank = ty.getRank();\n-  auto contiHint = 1;\n-  auto divHint = 1;\n-  auto constHint = 1;\n+\n+  DimVectorT knownContiguity(rank, 1);\n+  DimVectorT knownDivisibility(rank, 1);\n+  DimVectorT knownConstancy(rank, 1);\n+\n   BlockArgument blockArg = value.dyn_cast<BlockArgument>();\n+\n   if (blockArg && blockArg.getOwner()->isEntryBlock()) {\n     Operation *op = blockArg.getOwner()->getParentOp();\n-    if (func::FuncOp fun = dyn_cast<func::FuncOp>(op)) {\n-      Attribute attr =\n-          fun.getArgAttr(blockArg.getArgNumber(), \"tt.divisibility\");\n-      if (attr)\n-        divHint = attr.cast<IntegerAttr>().getValue().getZExtValue();\n-    } else if (auto fun = dyn_cast<LLVM::LLVMFuncOp>(op)) {\n-      Attribute attr =\n-          fun.getArgAttr(blockArg.getArgNumber(), \"tt.divisibility\");\n-      if (attr)\n-        divHint = attr.cast<IntegerAttr>().getValue().getZExtValue();\n-    } else {\n+    if (auto fun = dyn_cast<triton::FuncOp>(op))\n+      initPessimisticStateFromFunc(blockArg.getArgNumber(), fun,\n+                                   &knownContiguity, &knownDivisibility,\n+                                   &knownConstancy);\n+    // llvm codegen check alignment to generate vector load/store\n+    // would be nice if this wasn't the case\n+    else if (auto fun = dyn_cast<LLVM::LLVMFuncOp>(op))\n+      initPessimisticStateFromFunc(blockArg.getArgNumber(), fun,\n+                                   &knownContiguity, &knownDivisibility,\n+                                   &knownConstancy);\n+    else {\n       // Derive the divisibility of the induction variable only when\n       // the step and the lower bound are both constants\n       if (auto forOp = dyn_cast<scf::ForOp>(op)) {\n@@ -79,16 +104,13 @@ AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n                   step.getValue().cast<IntegerAttr>().getValue().getZExtValue();\n               auto k = gcd(lowerBoundVal, stepVal);\n               if (k != 0)\n-                divHint = k;\n+                knownDivisibility = DimVectorT(rank, k);\n             }\n           }\n         }\n       }\n     }\n   } else if (Operation *op = value.getDefiningOp()) {\n-    DimVectorT knownContiguity(rank, 1);\n-    DimVectorT knownDivisibility(rank, 1);\n-    DimVectorT knownConstancy(rank, 1);\n     if (Attribute attr = op->getAttr(\"tt.divisibility\")) {\n       auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n       knownDivisibility = DimVectorT(vals.begin(), vals.end());\n@@ -101,12 +123,9 @@ AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n       auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n       knownConstancy = DimVectorT(vals.begin(), vals.end());\n     }\n-    return AxisInfo(knownContiguity, knownDivisibility, knownConstancy);\n   }\n \n-  return AxisInfo(/*knownContiguity=*/DimVectorT(rank, contiHint),\n-                  /*knownDivisibility=*/DimVectorT(rank, divHint),\n-                  /*knownConstancy=*/DimVectorT(rank, constHint));\n+  return AxisInfo(knownContiguity, knownDivisibility, knownConstancy);\n }\n \n // The gcd of both arguments for each dimension"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -74,7 +74,7 @@ void MembarAnalysis::visitTerminator(Operation *op,\n     return;\n   }\n   // Otherwise, it could be a return op\n-  assert(isa<func::ReturnOp>(op) && \"Unknown terminator\");\n+  assert(isa<triton::ReturnOp>(op) && \"Unknown terminator\");\n }\n \n void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 20, "deletions": 3, "changes": 23, "file_content_changes": "@@ -91,6 +91,19 @@ unsigned ReduceOpHelper::getScratchSizeInBytes() {\n   return bytes;\n }\n \n+bool ReduceOpHelper::isSupportedLayout() {\n+  auto srcLayout = srcTy.getEncoding();\n+  if (srcLayout.isa<triton::gpu::BlockedEncodingAttr>()) {\n+    return true;\n+  }\n+  if (auto mmaLayout = srcLayout.dyn_cast<triton::gpu::MmaEncodingAttr>()) {\n+    if (mmaLayout.isAmpere()) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n bool isSharedEncoding(Value value) {\n   auto type = value.getType();\n   if (auto tensorType = type.dyn_cast<RankedTensorType>()) {\n@@ -157,14 +170,18 @@ std::string getValueOperandName(Value value, AsmState &state) {\n   return opName;\n }\n \n-bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n-                        triton::gpu::DotOperandEncodingAttr &dotOperandLayout) {\n+bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy) {\n   // dot_op<opIdx=0, parent=#mma> = #mma\n   // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n+  auto srcLayout = srcTy.getEncoding();\n+  auto dstLayout = dstTy.getEncoding();\n+  auto mmaLayout = srcLayout.cast<triton::gpu::MmaEncodingAttr>();\n+  auto dotOperandLayout = dstLayout.cast<triton::gpu::DotOperandEncodingAttr>();\n   return mmaLayout.getVersionMajor() == 2 &&\n          mmaLayout.getWarpsPerCTA()[1] == 1 &&\n          dotOperandLayout.getOpIdx() == 0 &&\n-         dotOperandLayout.getParent() == mmaLayout;\n+         dotOperandLayout.getParent() == mmaLayout &&\n+         !srcTy.getElementType().isF32();\n }\n \n bool isSingleValue(Value value) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 5, "changes": 6, "file_content_changes": "@@ -568,11 +568,7 @@ struct ConvertLayoutOpConversion\n     auto loc = op.getLoc();\n     auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n     auto dstTy = op.getResult().getType().cast<RankedTensorType>();\n-    auto srcLayout = srcTy.getEncoding();\n-    auto dstLayout = dstTy.getEncoding();\n-    auto srcMmaLayout = srcLayout.cast<MmaEncodingAttr>();\n-    auto dstDotLayout = dstLayout.cast<DotOperandEncodingAttr>();\n-    if (isMmaToDotShortcut(srcMmaLayout, dstDotLayout)) {\n+    if (isMmaToDotShortcut(srcTy, dstTy)) {\n       // get source values\n       auto vals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n                                                        rewriter, srcTy);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 87, "deletions": 10, "changes": 97, "file_content_changes": "@@ -131,40 +131,92 @@ struct ReduceOpConversion\n     }\n   }\n \n+  // Calculates the write index in the shared memory where we would be writing\n+  // the within-thread accumulations before we start doing across-threads\n+  // accumulations. `index` is the index of the within-thread accumulations in\n+  // the full tensor, whereas `writeIdx` is the mapped-to index in the shared\n+  // memory\n+  void getWriteIndexBasic(ConversionPatternRewriter &rewriter, Location loc,\n+                          Attribute layout, SmallVector<Value> &index,\n+                          SmallVector<Value> &writeIdx,\n+                          std::map<int, Value> &ints, unsigned axis) const {\n+    writeIdx = index;\n+    auto sizePerThread = triton::gpu::getSizePerThread(layout);\n+    Value axisSizePerThread = ints[sizePerThread[axis]];\n+    Value _8 = ints[8];\n+    Value _16 = ints[16];\n+    if (layout.isa<BlockedEncodingAttr>()) {\n+      // A single thread owns axisSizePerThread contiguous values\n+      // on the reduction axis. After within thread reduction,\n+      // we would have a single accumulation every `axisSizePerThread`\n+      // contiguous values in the original tensor, so we would need\n+      // to map every `axisSizePerThread` to 1 value in smem as:\n+      // writeIdx[axis] = index[axis] / axisSizePerThread\n+      writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n+    }\n+    auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>();\n+    if (mmaLayout && mmaLayout.isAmpere()) {\n+      if (axis == 0) {\n+        // Because warpTileSize = [16, 8] and threadsPerWarp = [8, 4], each 8\n+        // rows in smem would correspond to a warp. The mapping\n+        // is: (warp_index) x 8 + (row index within warp)\n+        writeIdx[axis] =\n+            add(mul(udiv(index[axis], _16), _8), urem(index[axis], _8));\n+      } else {\n+        // Same as BlockedEncodingAttr case\n+        writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n+      }\n+    }\n+    if (mmaLayout && !mmaLayout.isAmpere()) {\n+      llvm::report_fatal_error(\"Unsupported layout\");\n+    }\n+  }\n+\n   // Use shared memory for reduction within warps and across warps\n   LogicalResult\n   matchAndRewriteBasic(triton::ReduceOp op, OpAdaptor adaptor,\n                        ConversionPatternRewriter &rewriter) const {\n+    ReduceOpHelper helper(op);\n     Location loc = op->getLoc();\n     unsigned axis = op.getAxis();\n+    // Specifies whether the reduce operation returns an index\n+    // rather than a value, e.g. argmax, argmin, .. etc\n     bool withIndex = triton::ReduceOp::withIndex(op.getRedOp());\n \n     auto srcTy = op.getOperand().getType().cast<RankedTensorType>();\n-    auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n-    auto srcOrd = srcLayout.getOrder();\n+    auto srcLayout = srcTy.getEncoding();\n+    if (!helper.isSupportedLayout()) {\n+      assert(false && \"Unexpected srcLayout in ReduceOpConversion\");\n+    }\n+    // The order of the axes for the the threads within the warp\n+    auto srcOrd = triton::gpu::getOrder(srcLayout);\n+    auto sizePerThread = triton::gpu::getSizePerThread(srcLayout);\n     auto srcShape = srcTy.getShape();\n \n     auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n     auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n     auto indexPtrTy = LLVM::LLVMPointerType::get(llvmIndexTy, 3);\n+\n     Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n     smemBase = bitcast(smemBase, elemPtrTy);\n \n-    ReduceOpHelper helper(op);\n     auto smemShape = helper.getScratchConfigBasic();\n     unsigned elems = product<unsigned>(smemShape);\n     Value indexSmemBase = gep(elemPtrTy, smemBase, i32_val(elems));\n     indexSmemBase = bitcast(indexSmemBase, indexPtrTy);\n \n     unsigned srcElems = getElemsPerThread(srcTy);\n+    // Emits indices of the original tensor that each thread\n+    // would own\n     auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy);\n     auto srcValues = getTypeConverter()->unpackLLElements(\n         loc, adaptor.getOperand(), rewriter, srcTy);\n-\n+    // Emits offsets (the offset from the base index)\n+    // of the original tensor that each thread would own\n     SmallVector<SmallVector<unsigned>> offset =\n         emitOffsetForLayout(srcLayout, srcTy);\n-\n+    // Keep track of accumulations and their indices\n     std::map<SmallVector<unsigned>, Value> accs;\n     std::map<SmallVector<unsigned>, Value> accIndices;\n     std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n@@ -191,7 +243,9 @@ struct ReduceOpConversion\n     ints[0] = i32_val(0);\n     for (int N = smemShape[axis] / 2; N > 0; N >>= 1)\n       ints[N] = i32_val(N);\n-    Value sizePerThread = i32_val(srcLayout.getSizePerThread()[axis]);\n+    ints[sizePerThread[axis]] = i32_val(sizePerThread[axis]);\n+    ints[8] = i32_val(8);\n+    ints[16] = i32_val(16);\n \n     // reduce across threads\n     for (auto it : accs) {\n@@ -200,29 +254,49 @@ struct ReduceOpConversion\n       Value accIndex;\n       if (withIndex)\n         accIndex = accIndices[key];\n-      SmallVector<Value> writeIdx = indices[key];\n-\n-      writeIdx[axis] = udiv(writeIdx[axis], sizePerThread);\n+      // get the writeIdx at which to write in smem\n+      SmallVector<Value> writeIdx;\n+      getWriteIndexBasic(rewriter, loc, srcLayout, indices[key], writeIdx, ints,\n+                         axis);\n+      // calculate the offset in smem for that writeIdx\n       Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, srcOrd);\n+      // Get element pointers for the value and index\n       Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n       Value indexWritePtr = gep(indexPtrTy, indexSmemBase, writeOffset);\n+      // Store the within-thread accumulated value at writePtr\n       store(acc, writePtr);\n+      // Store the index of within-thread accumulation at indexWritePtr\n       if (withIndex)\n         store(accIndex, indexWritePtr);\n \n       SmallVector<Value> readIdx(writeIdx.size(), ints[0]);\n+      // Perform parallel reduction with sequential addressing\n+      // E.g. We reduce `smemShape[axis]` elements into `smemShape[axis]/2`\n+      // elements using `smemShape[axis]/2` threads where each thread\n+      // would accumalte values that are `smemShape[axis]/2` apart\n+      // to avoid bank conflicts. Then we repeat with `smemShape[axis]/4`\n+      // threads, .. etc.\n       for (int N = smemShape[axis] / 2; N > 0; N >>= 1) {\n+        // The readIdx will be N elements away on the reduction axis\n         readIdx[axis] = ints[N];\n+        // If the writeIdx is greater or equal to N, do nothing\n         Value readMask = icmp_slt(writeIdx[axis], ints[N]);\n+        // Calculate the readOffset, if readMask is False, readOffset=0\n+        // meaning we reduce the value at writeIdx with itself\n         Value readOffset = select(\n             readMask, linearize(rewriter, loc, readIdx, smemShape, srcOrd),\n             ints[0]);\n+        // The readPtr is readOffset away from writePtr\n         Value readPtr = gep(elemPtrTy, writePtr, readOffset);\n         barrier();\n+        // If we do not care about the index, i.e. this is not an argmax,\n+        // argmin, .. etc\n         if (!withIndex) {\n+          // The value at the readPtr, whereas acc is the value at writePtr\n           Value cur = load(readPtr);\n           accumulate(rewriter, loc, op.getRedOp(), acc, cur, false);\n           barrier();\n+          // Update writePtr value\n           store(acc, writePtr);\n         } else {\n           Value cur = load(readPtr);\n@@ -274,12 +348,16 @@ struct ReduceOpConversion\n   // exchange across warps\n   LogicalResult matchAndRewriteFast(triton::ReduceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter) const {\n+    ReduceOpHelper helper(op);\n     Location loc = op->getLoc();\n     unsigned axis = adaptor.getAxis();\n     bool withIndex = triton::ReduceOp::withIndex(op.getRedOp());\n \n     auto srcTy = op.getOperand().getType().cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding();\n+    if (!helper.isSupportedLayout()) {\n+      assert(false && \"Unexpected srcLayout in ReduceOpConversion\");\n+    }\n     auto srcShape = srcTy.getShape();\n     auto order = getOrder(srcLayout);\n \n@@ -293,7 +371,6 @@ struct ReduceOpConversion\n     Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n     smemBase = bitcast(smemBase, elemPtrTy);\n \n-    ReduceOpHelper helper(op);\n     auto smemShapes = helper.getScratchConfigsFast();\n     unsigned elems = product<unsigned>(smemShapes[0]);\n     unsigned maxElems = std::max(elems, product<unsigned>(smemShapes[1]));"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -9,11 +9,11 @@ using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::triton::gpu::getElemsPerThread;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n-struct ReturnOpConversion : public ConvertOpToLLVMPattern<func::ReturnOp> {\n-  using ConvertOpToLLVMPattern<func::ReturnOp>::ConvertOpToLLVMPattern;\n+struct ReturnOpConversion : public ConvertOpToLLVMPattern<triton::ReturnOp> {\n+  using ConvertOpToLLVMPattern<triton::ReturnOp>::ConvertOpToLLVMPattern;\n \n   LogicalResult\n-  matchAndRewrite(func::ReturnOp op, OpAdaptor adaptor,\n+  matchAndRewrite(triton::ReturnOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     unsigned numArguments = op.getNumOperands();\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 10, "deletions": 11, "changes": 21, "file_content_changes": "@@ -4,7 +4,6 @@\n // TODO: refactor so that it doesn't fail if Allocation.h\n // is included after utility.h (due to conflict in `store` macro\n // and <atomic>\n-#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"triton/Analysis/Allocation.h\"\n \n #include \"TypeConverter.h\"\n@@ -41,12 +40,12 @@ void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n // TODO(Superjomn): remove the code when MLIR v15.0 is included.\n // All the rights are reserved by the LLVM community.\n \n-struct FuncOpConversionBase : public ConvertOpToLLVMPattern<func::FuncOp> {\n+struct FuncOpConversionBase : public ConvertOpToLLVMPattern<triton::FuncOp> {\n private:\n   /// Only retain those attributes that are not constructed by\n   /// `LLVMFuncOp::build`. If `filterArgAttrs` is set, also filter out argument\n   /// attributes.\n-  static void filterFuncAttributes(func::FuncOp op, bool filterArgAttrs,\n+  static void filterFuncAttributes(triton::FuncOp op, bool filterArgAttrs,\n                                    SmallVectorImpl<NamedAttribute> &result) {\n \n     for (const auto &attr : op->getAttrs()) {\n@@ -66,19 +65,19 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<func::FuncOp> {\n   }\n \n protected:\n-  using ConvertOpToLLVMPattern<func::FuncOp>::ConvertOpToLLVMPattern;\n+  using ConvertOpToLLVMPattern<triton::FuncOp>::ConvertOpToLLVMPattern;\n \n   // Convert input FuncOp to LLVMFuncOp by using the LLVMTypeConverter provided\n   // to this legalization pattern.\n   LLVM::LLVMFuncOp\n-  convertFuncOpToLLVMFuncOp(func::FuncOp funcOp,\n+  convertFuncOpToLLVMFuncOp(triton::FuncOp funcOp,\n                             ConversionPatternRewriter &rewriter) const {\n     // Convert the original function arguments. They are converted using the\n     // LLVMTypeConverter provided to this legalization pattern.\n     auto varargsAttr = funcOp->getAttrOfType<BoolAttr>(\"func.varargs\");\n     TypeConverter::SignatureConversion result(funcOp.getNumArguments());\n     auto llvmType = getTypeConverter()->convertFunctionSignature(\n-        funcOp.getFunctionType(), varargsAttr && varargsAttr.getValue(),\n+        funcOp.getFunctionType(), varargsAttr && varargsAttr.getValue(), false,\n         result);\n     if (!llvmType)\n       return nullptr;\n@@ -502,7 +501,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n   SmallVector<Value> emitBaseIndexForLayout(Location loc,\n                                             ConversionPatternRewriter &rewriter,\n-                                            const Attribute &layout,\n+                                            Attribute layout,\n                                             RankedTensorType type) const {\n     IndexCacheKeyT key = std::make_pair(layout, type);\n     auto cache = indexCacheInfo.baseIndexCache;\n@@ -532,7 +531,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   }\n \n   SmallVector<SmallVector<unsigned>>\n-  emitOffsetForLayout(const Attribute &layout, RankedTensorType type) const {\n+  emitOffsetForLayout(Attribute layout, RankedTensorType type) const {\n     if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n       return emitOffsetForBlockedLayout(blockedLayout, type);\n     if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n@@ -549,7 +548,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // -----------------------------------------------------------------------\n   SmallVector<SmallVector<Value>> emitIndices(Location loc,\n                                               ConversionPatternRewriter &b,\n-                                              const Attribute &layout,\n+                                              Attribute layout,\n                                               RankedTensorType type) const {\n     IndexCacheKeyT key(layout, type);\n     auto cache = indexCacheInfo.indexCache;\n@@ -861,8 +860,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // Emit indices calculation within each ConversionPattern, and returns a\n   // [elemsPerThread X rank] index matrix.\n   SmallVector<SmallVector<Value>> emitIndicesForDistributedLayout(\n-      Location loc, ConversionPatternRewriter &rewriter,\n-      const Attribute &layout, RankedTensorType type) const {\n+      Location loc, ConversionPatternRewriter &rewriter, Attribute layout,\n+      RankedTensorType type) const {\n     // step 1, delinearize threadId to get the base index\n     auto multiDimBase = emitBaseIndexForLayout(loc, rewriter, layout, type);\n     // step 2, get offset of each element"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 5, "deletions": 6, "changes": 11, "file_content_changes": "@@ -51,16 +51,15 @@ class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n     } else {\n       addLegalDialect<NVVM::NVVMDialect>();\n     }\n-    addIllegalOp<mlir::func::FuncOp>();\n     addLegalOp<mlir::UnrealizedConversionCastOp>();\n   }\n };\n \n-struct ReturnOpConversion : public ConvertOpToLLVMPattern<func::ReturnOp> {\n-  using ConvertOpToLLVMPattern<func::ReturnOp>::ConvertOpToLLVMPattern;\n+struct ReturnOpConversion : public ConvertOpToLLVMPattern<triton::ReturnOp> {\n+  using ConvertOpToLLVMPattern<triton::ReturnOp>::ConvertOpToLLVMPattern;\n \n   LogicalResult\n-  matchAndRewrite(func::ReturnOp op, OpAdaptor adaptor,\n+  matchAndRewrite(triton::ReturnOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     unsigned numArguments = op.getNumOperands();\n \n@@ -86,7 +85,7 @@ struct FuncOpConversion : public FuncOpConversionBase {\n       : FuncOpConversionBase(converter, benefit), numWarps(numWarps) {}\n \n   LogicalResult\n-  matchAndRewrite(func::FuncOp funcOp, OpAdaptor adaptor,\n+  matchAndRewrite(triton::FuncOp funcOp, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     auto newFuncOp = convertFuncOpToLLVMFuncOp(funcOp, rewriter);\n     if (!newFuncOp) {\n@@ -265,7 +264,7 @@ class ConvertTritonGPUToLLVM\n           srcType.getEncoding().dyn_cast<triton::gpu::MmaEncodingAttr>();\n       auto dstDotOp =\n           dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n-      if (srcMma && dstDotOp && !isMmaToDotShortcut(srcMma, dstDotOp)) {\n+      if (srcMma && dstDotOp && !isMmaToDotShortcut(srcType, dstType)) {\n         auto tmpType = RankedTensorType::get(\n             dstType.getShape(), dstType.getElementType(),\n             triton::gpu::BlockedEncodingAttr::get("}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -163,8 +163,8 @@ void populateStdPatternsAndLegality(TritonGPUTypeConverter &typeConverter,\n   MLIRContext *context = patterns.getContext();\n   // Rewrite rule\n   patterns.add<StdSelectPattern>(typeConverter, context);\n-  target.addLegalOp<func::ReturnOp>(); // this is ok because all functions are\n-                                       // inlined by the frontend\n+  target.addLegalOp<triton::ReturnOp>(); // this is ok because all functions are\n+                                         // inlined by the frontend\n }\n \n void populateMathPatternsAndLegality(TritonGPUTypeConverter &typeConverter,\n@@ -721,15 +721,15 @@ class CFCondBranchPattern : public OpConversionPattern<cf::CondBranchOp> {\n   }\n };\n \n-class FuncOpPattern : public OpConversionPattern<func::FuncOp> {\n+class FuncOpPattern : public OpConversionPattern<triton::FuncOp> {\n public:\n-  using OpConversionPattern<func::FuncOp>::OpConversionPattern;\n+  using OpConversionPattern<triton::FuncOp>::OpConversionPattern;\n \n   LogicalResult\n-  matchAndRewrite(func::FuncOp op, OpAdaptor adaptor,\n+  matchAndRewrite(triton::FuncOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     auto converter = getTypeConverter();\n-    auto newOp = rewriter.replaceOpWithNewOp<func::FuncOp>(\n+    auto newOp = rewriter.replaceOpWithNewOp<triton::FuncOp>(\n         op, op.getName(), op.getFunctionType());\n     addNamedAttrs(newOp, adaptor.getAttributes());\n     rewriter.inlineRegionBefore(op.getBody(), newOp.getBody(),"}, {"filename": "lib/Dialect/Triton/IR/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -12,5 +12,4 @@ add_mlir_dialect_library(TritonIR\n   MLIRIR\n   MLIRArithDialect\n   MLIRSCFDialect\n-  MLIRFuncDialect\n )"}, {"filename": "lib/Dialect/Triton/IR/Dialect.cpp", "status": "modified", "additions": 36, "deletions": 0, "changes": 36, "file_content_changes": "@@ -1,6 +1,7 @@\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n \n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n #include \"triton/Dialect/Triton/IR/AttrInterfaces.h.inc\"\n #include \"llvm/ADT/StringSwitch.h\"\n #include \"llvm/ADT/TypeSwitch.h\"\n@@ -21,6 +22,10 @@ using namespace mlir::triton;\n namespace {\n struct TritonInlinerInterface : public DialectInlinerInterface {\n   using DialectInlinerInterface::DialectInlinerInterface;\n+  bool isLegalToInline(Operation *call, Operation *callable,\n+                       bool wouldBeCloned) const final {\n+    return true;\n+  }\n   bool isLegalToInline(Region *dest, Region *src, bool wouldBeCloned,\n                        IRMapping &valueMapping) const final {\n     return true;\n@@ -29,6 +34,37 @@ struct TritonInlinerInterface : public DialectInlinerInterface {\n                        IRMapping &) const final {\n     return true;\n   }\n+  //===--------------------------------------------------------------------===//\n+  // Transformation Hooks\n+  //===--------------------------------------------------------------------===//\n+\n+  /// Handle the given inlined terminator by replacing it with a new operation\n+  /// as necessary.\n+  void handleTerminator(Operation *op, Block *newDest) const final {\n+    // Only return needs to be handled here.\n+    auto returnOp = dyn_cast<triton::ReturnOp>(op);\n+    if (!returnOp)\n+      return;\n+\n+    // Replace the return with a branch to the dest.\n+    OpBuilder builder(op);\n+    builder.create<mlir::cf::BranchOp>(op->getLoc(), newDest,\n+                                       returnOp.getOperands());\n+    op->erase();\n+  }\n+\n+  /// Handle the given inlined terminator by replacing it with a new operation\n+  /// as necessary.\n+  void handleTerminator(Operation *op,\n+                        ArrayRef<Value> valuesToRepl) const final {\n+    // Only return needs to be handled here.\n+    auto returnOp = cast<triton::ReturnOp>(op);\n+\n+    // Replace the values directly with the return operands.\n+    assert(returnOp.getNumOperands() == valuesToRepl.size());\n+    for (const auto &it : llvm::enumerate(returnOp.getOperands()))\n+      valuesToRepl[it.index()].replaceAllUsesWith(it.value());\n+  }\n };\n } // namespace\n "}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 102, "deletions": 0, "changes": 102, "file_content_changes": "@@ -1,6 +1,8 @@\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/FunctionImplementation.h\"\n+#include \"mlir/IR/FunctionInterfaces.h\"\n #include \"mlir/IR/OperationSupport.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n@@ -516,5 +518,105 @@ void MakeTensorPtrOp::build(::mlir::OpBuilder &builder,\n                builder.getDenseI32ArrayAttr(order));\n }\n \n+// The following ops, including `call`, `func`, and `return` are copied and\n+// modified from\n+// https://github.com/llvm/llvm-project/blob/main/mlir/lib/Dialect/Func/IR/FuncOps.cpp\n+// We could revert it back once MLIR has a better inliner interface.\n+//-- FuncOp --\n+void triton::FuncOp::build(OpBuilder &builder, OperationState &state,\n+                           StringRef name, FunctionType type,\n+                           ArrayRef<NamedAttribute> attrs,\n+                           ArrayRef<DictionaryAttr> argAttrs) {\n+  state.addAttribute(SymbolTable::getSymbolAttrName(),\n+                     builder.getStringAttr(name));\n+  state.addAttribute(getFunctionTypeAttrName(state.name), TypeAttr::get(type));\n+  state.attributes.append(attrs.begin(), attrs.end());\n+  state.addRegion();\n+\n+  if (argAttrs.empty())\n+    return;\n+  assert(type.getNumInputs() == argAttrs.size());\n+  function_interface_impl::addArgAndResultAttrs(\n+      builder, state, argAttrs, /*resultAttrs=*/std::nullopt,\n+      getArgAttrsAttrName(state.name), getResAttrsAttrName(state.name));\n+}\n+\n+ParseResult triton::FuncOp::parse(OpAsmParser &parser, OperationState &result) {\n+  auto buildFuncType =\n+      [](Builder &builder, ArrayRef<Type> argTypes, ArrayRef<Type> results,\n+         function_interface_impl::VariadicFlag,\n+         std::string &) { return builder.getFunctionType(argTypes, results); };\n+\n+  return function_interface_impl::parseFunctionOp(\n+      parser, result, /*allowVariadic=*/false,\n+      getFunctionTypeAttrName(result.name), buildFuncType,\n+      getArgAttrsAttrName(result.name), getResAttrsAttrName(result.name));\n+}\n+\n+void triton::FuncOp::print(OpAsmPrinter &printer) {\n+  function_interface_impl::printFunctionOp(\n+      printer, *this, /*isVariadic=*/false, getFunctionTypeAttrName(),\n+      getArgAttrsAttrName(), getResAttrsAttrName());\n+}\n+\n+// -- CallOp --\n+LogicalResult\n+triton::CallOp::verifySymbolUses(mlir::SymbolTableCollection &symbolTable) {\n+  // Check that the callee attribute was specified.\n+  auto fnAttr = (*this)->getAttrOfType<FlatSymbolRefAttr>(\"callee\");\n+  if (!fnAttr)\n+    return emitOpError(\"requires a 'callee' symbol reference attribute\");\n+  FuncOp fn = symbolTable.lookupNearestSymbolFrom<FuncOp>(*this, fnAttr);\n+  if (!fn)\n+    return emitOpError() << \"'\" << fnAttr.getValue()\n+                         << \"' does not reference a valid function\";\n+\n+  // Verify that the operand and result types match the callee.\n+  auto fnType = fn.getFunctionType();\n+  if (fnType.getNumInputs() != getNumOperands())\n+    return emitOpError(\"incorrect number of operands for callee\");\n+\n+  for (unsigned i = 0, e = fnType.getNumInputs(); i != e; ++i)\n+    if (getOperand(i).getType() != fnType.getInput(i))\n+      return emitOpError(\"operand type mismatch: expected operand type \")\n+             << fnType.getInput(i) << \", but provided \"\n+             << getOperand(i).getType() << \" for operand number \" << i;\n+\n+  if (fnType.getNumResults() != getNumResults())\n+    return emitOpError(\"incorrect number of results for callee\");\n+\n+  for (unsigned i = 0, e = fnType.getNumResults(); i != e; ++i)\n+    if (getResult(i).getType() != fnType.getResult(i)) {\n+      auto diag = emitOpError(\"result type mismatch at index \") << i;\n+      diag.attachNote() << \"      op result types: \" << getResultTypes();\n+      diag.attachNote() << \"function result types: \" << fnType.getResults();\n+      return diag;\n+    }\n+\n+  return success();\n+}\n+\n+// -- ReturnOp --\n+LogicalResult triton::ReturnOp::verify() {\n+  auto function = cast<triton::FuncOp>((*this)->getParentOp());\n+\n+  // The operand number and types must match the function signature.\n+  const auto &results = function.getFunctionType().getResults();\n+  if (getNumOperands() != results.size())\n+    return emitOpError(\"has \")\n+           << getNumOperands() << \" operands, but enclosing function (@\"\n+           << function.getName() << \") returns \" << results.size();\n+\n+  for (unsigned i = 0, e = results.size(); i != e; ++i)\n+    if (getOperand(i).getType() != results[i])\n+      return emitError() << \"type of return operand \" << i << \" (\"\n+                         << getOperand(i).getType()\n+                         << \") doesn't match function result type (\"\n+                         << results[i] << \")\"\n+                         << \" in function @\" << function.getName();\n+\n+  return success();\n+}\n+\n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -48,7 +48,7 @@ unsigned getElemsPerThread(Type type) {\n                            tensorType.getElementType());\n }\n \n-SmallVector<unsigned> getThreadsPerWarp(const Attribute &layout) {\n+SmallVector<unsigned> getThreadsPerWarp(Attribute layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getThreadsPerWarp().begin(),\n                                  blockedLayout.getThreadsPerWarp().end());\n@@ -63,7 +63,7 @@ SmallVector<unsigned> getThreadsPerWarp(const Attribute &layout) {\n   return {};\n }\n \n-SmallVector<unsigned> getWarpsPerCTA(const Attribute &layout) {\n+SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getWarpsPerCTA().begin(),\n                                  blockedLayout.getWarpsPerCTA().end());\n@@ -76,7 +76,7 @@ SmallVector<unsigned> getWarpsPerCTA(const Attribute &layout) {\n   return {};\n }\n \n-SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n+SmallVector<unsigned> getSizePerThread(Attribute layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n                                  blockedLayout.getSizePerThread().end());\n@@ -120,7 +120,7 @@ SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n   }\n }\n \n-SmallVector<unsigned> getContigPerThread(const Attribute &layout) {\n+SmallVector<unsigned> getContigPerThread(Attribute layout) {\n   if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     assert(mmaLayout.isVolta() || mmaLayout.isAmpere());\n     return {1, 2};\n@@ -129,7 +129,7 @@ SmallVector<unsigned> getContigPerThread(const Attribute &layout) {\n   }\n }\n \n-SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout) {\n+SmallVector<unsigned> getThreadsPerCTA(Attribute layout) {\n   SmallVector<unsigned> threads;\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     for (int d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n@@ -148,7 +148,7 @@ SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout) {\n   return threads;\n }\n \n-SmallVector<unsigned> getShapePerCTA(const Attribute &layout,\n+SmallVector<unsigned> getShapePerCTA(Attribute layout,\n                                      ArrayRef<int64_t> tensorShape) {\n   SmallVector<unsigned> shape;\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n@@ -202,7 +202,7 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout,\n   return shape;\n }\n \n-SmallVector<unsigned> getOrder(const Attribute &layout) {\n+SmallVector<unsigned> getOrder(Attribute layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getOrder().begin(),\n                                  blockedLayout.getOrder().end());\n@@ -232,7 +232,7 @@ SmallVector<unsigned> getOrder(const Attribute &layout) {\n   }\n };\n \n-bool isaDistributedLayout(const Attribute &layout) {\n+bool isaDistributedLayout(Attribute layout) {\n   return layout.isa<BlockedEncodingAttr>() || layout.isa<MmaEncodingAttr>() ||\n          layout.isa<SliceEncodingAttr>();\n }\n@@ -241,7 +241,7 @@ bool isaDistributedLayout(const Attribute &layout) {\n } // namespace triton\n } // namespace mlir\n \n-static LogicalResult parseIntAttrValue(AsmParser &parser, const Attribute &attr,\n+static LogicalResult parseIntAttrValue(AsmParser &parser, Attribute attr,\n                                        unsigned &value, StringRef desc) {\n   auto intAttr = attr.dyn_cast<IntegerAttr>();\n   if (!intAttr) {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -194,10 +194,13 @@ LogicalResult LoopPipeliner::initialize() {\n     if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n       auto ptr = loadOp.getPtr();\n       unsigned vec = axisInfoAnalysis->getPtrContiguity(ptr);\n+\n       if (auto mask = loadOp.getMask())\n         vec = std::min<unsigned>(vec, axisInfoAnalysis->getMaskAlignment(mask));\n+\n+      auto lattice = axisInfoAnalysis->getLatticeElement(ptr)->getValue();\n       auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n-      if (!tensorTy)\n+      if (!tensorTy || tensorTy.getRank() < 2)\n         continue;\n       auto ty = tensorTy.getElementType()\n                     .cast<triton::PointerType>()"}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 11, "deletions": 12, "changes": 23, "file_content_changes": "@@ -81,18 +81,17 @@ TritonGPUConversionTarget::TritonGPUConversionTarget(\n                scf::ReduceReturnOp>();\n \n   addDynamicallyLegalDialect<arith::ArithDialect, math::MathDialect,\n-                             func::FuncDialect, triton::TritonDialect,\n-                             cf::ControlFlowDialect, scf::SCFDialect>(\n-      [&](Operation *op) {\n-        bool hasLegalRegions = true;\n-        for (auto &region : op->getRegions()) {\n-          hasLegalRegions = hasLegalRegions && typeConverter.isLegal(&region);\n-        }\n-        if (hasLegalRegions && typeConverter.isLegal(op)) {\n-          return true;\n-        }\n-        return false;\n-      });\n+                             triton::TritonDialect, cf::ControlFlowDialect,\n+                             scf::SCFDialect>([&](Operation *op) {\n+    bool hasLegalRegions = true;\n+    for (auto &region : op->getRegions()) {\n+      hasLegalRegions = hasLegalRegions && typeConverter.isLegal(&region);\n+    }\n+    if (hasLegalRegions && typeConverter.isLegal(op)) {\n+      return true;\n+    }\n+    return false;\n+  });\n \n   // We have requirements for the data layouts\n   addDynamicallyLegalOp<triton::DotOp>([](triton::DotOp dotOp) -> bool {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 22, "deletions": 9, "changes": 31, "file_content_changes": "@@ -89,10 +89,21 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n }\n \n bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n-  // Case 1: A size 1 tensor is not expensive since all threads will load the\n+  // Case 1a: A size 1 tensor is not expensive since all threads will load the\n   // same\n   if (isSingleValue(op->getOperand(0)))\n     return false;\n+  // Case 1b: Tensor of pointers has more threads than elements\n+  // we can presume a high hit-rate that makes it cheap to load\n+  auto ptrType = op->getOperand(0).getType().cast<RankedTensorType>();\n+  IntegerAttr numWarps =\n+      op->getParentOfType<ModuleOp>()->getAttrOfType<IntegerAttr>(\n+          \"triton_gpu.num-warps\");\n+  if (numWarps) {\n+    int sizePerThread = triton::gpu::getElemsPerThread(ptrType);\n+    if (ptrType.getNumElements() < numWarps.getInt() * 32)\n+      return false;\n+  }\n   // auto ptr = op->getOperand(0);\n   //// Case 2: We assume that `evict_last` loads/stores have high hit rate\n   // if (auto load = dyn_cast<triton::LoadOp>(op))\n@@ -103,15 +114,17 @@ bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n   //     return false;\n   // if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n   //   auto encoding = tensorTy.getEncoding();\n-  //   // Case 3: Different type conversion is expensive (e.g., mma <-> block)\n-  //   if (encoding.getTypeID() != targetEncoding.getTypeID())\n+  //   // Case 3: Different type conversion is expensive (e.g., mma <->\n+  //   block) if (encoding.getTypeID() != targetEncoding.getTypeID())\n   //     return true;\n   //   auto sizePerThread = triton::gpu::getSizePerThread(encoding);\n-  //   auto targetSizePerThread = triton::gpu::getSizePerThread(targetEncoding);\n-  //   auto order = triton::gpu::getOrder(encoding);\n-  //   auto targetOrder = triton::gpu::getOrder(targetEncoding);\n-  //   // Case 4: The targeEncoding may expose more vectorization opportunities\n-  //   return sizePerThread[order[0]] >= targetSizePerThread[targetOrder[0]];\n+  //   auto targetSizePerThread =\n+  //   triton::gpu::getSizePerThread(targetEncoding); auto order =\n+  //   triton::gpu::getOrder(encoding); auto targetOrder =\n+  //   triton::gpu::getOrder(targetEncoding);\n+  //   // Case 4: The targeEncoding may expose more vectorization\n+  //   opportunities return sizePerThread[order[0]] >=\n+  //   targetSizePerThread[targetOrder[0]];\n   // }\n   return true;\n }\n@@ -134,7 +147,7 @@ bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n int simulateBackwardRematerialization(\n     Operation *initOp, SetVector<Operation *> &processed,\n     SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n-    const Attribute &targetEncoding) {\n+    Attribute targetEncoding) {\n   // DFS\n   std::vector<std::pair<Operation *, Attribute>> queue;\n   queue.emplace_back(initOp, targetEncoding);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -19,7 +19,7 @@ bool expensiveToRemat(Operation *op, Attribute &targetEncoding);\n int simulateBackwardRematerialization(\n     Operation *initOp, SetVector<Operation *> &processed,\n     SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n-    const Attribute &targetEncoding);\n+    Attribute targetEncoding);\n \n Operation *cloneWithInferType(mlir::PatternRewriter &rewriter, Operation *op,\n                               IRMapping &mapping);"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 7, "deletions": 1, "changes": 8, "file_content_changes": "@@ -7,6 +7,7 @@\n #include \"mlir/IR/Dialect.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Pass/PassManager.h\"\n+#include \"mlir/Target/LLVMIR/Dialect/Builtin/BuiltinToLLVMIRTranslation.h\"\n #include \"mlir/Target/LLVMIR/Dialect/LLVMIR/LLVMToLLVMIRTranslation.h\"\n #include \"mlir/Target/LLVMIR/Dialect/NVVM/NVVMToLLVMIRTranslation.h\"\n #include \"mlir/Target/LLVMIR/Dialect/ROCDL/ROCDLToLLVMIRTranslation.h\"\n@@ -245,6 +246,7 @@ std::unique_ptr<llvm::Module>\n translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n                       bool isROCM) {\n   DialectRegistry registry;\n+  mlir::registerBuiltinDialectTranslation(registry);\n   mlir::registerLLVMDialectTranslation(registry);\n   mlir::registerROCDLDialectTranslation(registry);\n   mlir::registerNVVMDialectTranslation(registry);\n@@ -296,7 +298,11 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n                            mlir::ModuleOp module, int computeCapability,\n                            bool isROCM) {\n   mlir::PassManager pm(module->getContext());\n-  applyPassManagerCLOptions(pm);\n+  mlir::registerPassManagerCLOptions();\n+  if (failed(applyPassManagerCLOptions(pm))) {\n+    llvm::errs() << \"failed to apply pass manager CL options\\n\";\n+    return nullptr;\n+  }\n   auto printingFlags = mlir::OpPrintingFlags();\n   printingFlags.elideLargeElementsAttrs(16);\n   pm.enableIRPrinting("}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -68,7 +68,7 @@ def get_llvm_package_info():\n     use_assert_enabled_llvm = check_env_flag(\"TRITON_USE_ASSERT_ENABLED_LLVM\", \"False\")\n     release_suffix = \"assert\" if use_assert_enabled_llvm else \"release\"\n     name = f'llvm+mlir-17.0.0-x86_64-{system_suffix}-{release_suffix}'\n-    version = \"llvm-17.0.0-2538e550420f\"\n+    version = \"llvm-17.0.0-f733b4fb9b8b\"\n     url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n     return Package(\"llvm\", name, url, \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n "}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 26, "deletions": 26, "changes": 52, "file_content_changes": "@@ -208,7 +208,7 @@ void init_triton_ir(py::module &&m) {\n                std::string attrName = name + \"_arg\" + std::to_string(id);\n                mlir::Block *owner = arg.getOwner();\n                if (owner->isEntryBlock() &&\n-                   !mlir::isa<mlir::func::FuncOp>(owner->getParentOp())) {\n+                   !mlir::isa<mlir::triton::FuncOp>(owner->getParentOp())) {\n                  owner->getParentOp()->setAttr(attrName, attr);\n                }\n              }\n@@ -361,7 +361,7 @@ void init_triton_ir(py::module &&m) {\n              return str;\n            })\n       .def(\"push_back\",\n-           [](mlir::ModuleOp &self, mlir::func::FuncOp &funcOp) -> void {\n+           [](mlir::ModuleOp &self, mlir::triton::FuncOp &funcOp) -> void {\n              self.push_back(funcOp);\n            })\n       .def(\"has_function\",\n@@ -372,13 +372,14 @@ void init_triton_ir(py::module &&m) {\n            })\n       .def(\"get_function\",\n            [](mlir::ModuleOp &self,\n-              std::string &funcName) -> mlir::func::FuncOp {\n-             return self.lookupSymbol<mlir::func::FuncOp>(funcName);\n+              std::string &funcName) -> mlir::triton::FuncOp {\n+             return self.lookupSymbol<mlir::triton::FuncOp>(funcName);\n            })\n       .def(\"get_single_function\",\n-           [](mlir::ModuleOp &self) -> mlir::func::FuncOp {\n-             llvm::SmallVector<mlir::func::FuncOp> funcs;\n-             self.walk([&](mlir::func::FuncOp func) { funcs.push_back(func); });\n+           [](mlir::ModuleOp &self) -> mlir::triton::FuncOp {\n+             llvm::SmallVector<mlir::triton::FuncOp> funcs;\n+             self.walk(\n+                 [&](mlir::triton::FuncOp func) { funcs.push_back(func); });\n              if (funcs.size() != 1)\n                throw std::runtime_error(\"Expected a single function\");\n              return funcs[0];\n@@ -400,12 +401,11 @@ void init_triton_ir(py::module &&m) {\n         // initialize registry\n         // note: we initialize llvm for undef\n         mlir::DialectRegistry registry;\n-        registry.insert<mlir::triton::TritonDialect,\n-                        mlir::triton::gpu::TritonGPUDialect,\n-                        mlir::math::MathDialect, mlir::arith::ArithDialect,\n-                        mlir::index::IndexDialect, mlir::func::FuncDialect,\n-                        mlir::scf::SCFDialect, mlir::cf::ControlFlowDialect,\n-                        mlir::LLVM::LLVMDialect>();\n+        registry.insert<\n+            mlir::triton::TritonDialect, mlir::triton::gpu::TritonGPUDialect,\n+            mlir::math::MathDialect, mlir::arith::ArithDialect,\n+            mlir::index::IndexDialect, mlir::scf::SCFDialect,\n+            mlir::cf::ControlFlowDialect, mlir::LLVM::LLVMDialect>();\n         context.appendDialectRegistry(registry);\n         context.loadAllAvailableDialects();\n \n@@ -423,30 +423,30 @@ void init_triton_ir(py::module &&m) {\n       },\n       ret::take_ownership);\n \n-  py::class_<mlir::func::FuncOp, mlir::OpState>(m, \"function\")\n+  py::class_<mlir::triton::FuncOp, mlir::OpState>(m, \"function\")\n       // .def_property_readonly(\"attrs\", &ir::function::attrs)\n       // .def(\"add_attr\", &ir::function::add_attr);\n       .def(\"args\",\n-           [](mlir::func::FuncOp &self, unsigned idx) -> mlir::BlockArgument {\n+           [](mlir::triton::FuncOp &self, unsigned idx) -> mlir::BlockArgument {\n              return self.getArgument(idx);\n            })\n       .def(\n           \"add_entry_block\",\n-          [](mlir::func::FuncOp &self) -> mlir::Block * {\n+          [](mlir::triton::FuncOp &self) -> mlir::Block * {\n             return self.addEntryBlock();\n           },\n           ret::reference)\n       .def(\n           \"set_arg_attr\",\n-          [](mlir::func::FuncOp &self, int arg_no, const std::string &name,\n+          [](mlir::triton::FuncOp &self, int arg_no, const std::string &name,\n              int val) {\n             // set arg attributes \"name\" to value \"val\"\n             auto attrTy = mlir::IntegerType::get(self.getContext(), 32);\n             self.setArgAttr(arg_no, name, mlir::IntegerAttr::get(attrTy, val));\n           },\n           ret::reference)\n-      .def_property_readonly(\"type\", &mlir::func::FuncOp::getFunctionType)\n-      .def(\"reset_type\", &mlir::func::FuncOp::setType);\n+      .def_property_readonly(\"type\", &mlir::triton::FuncOp::getFunctionType)\n+      .def(\"reset_type\", &mlir::triton::FuncOp::setType);\n \n   py::class_<mlir::OpBuilder::InsertPoint>(m, \"InsertPoint\");\n \n@@ -463,13 +463,13 @@ void init_triton_ir(py::module &&m) {\n       .def(\"ret\",\n            [](mlir::OpBuilder &self, std::vector<mlir::Value> &vals) -> void {\n              auto loc = self.getUnknownLoc();\n-             self.create<mlir::func::ReturnOp>(loc, vals);\n+             self.create<mlir::triton::ReturnOp>(loc, vals);\n            })\n       .def(\"call\",\n-           [](mlir::OpBuilder &self, mlir::func::FuncOp &func,\n+           [](mlir::OpBuilder &self, mlir::triton::FuncOp &func,\n               std::vector<mlir::Value> &args) -> mlir::OpState {\n              auto loc = self.getUnknownLoc();\n-             return self.create<mlir::func::CallOp>(loc, func, args);\n+             return self.create<mlir::triton::CallOp>(loc, func, args);\n            })\n       // insertion block/point\n       .def(\"set_insertion_point_to_start\",\n@@ -651,16 +651,16 @@ void init_triton_ir(py::module &&m) {\n       .def(\"get_or_insert_function\",\n            [](mlir::OpBuilder &self, mlir::ModuleOp &module,\n               std::string &funcName, mlir::Type &funcType,\n-              std::string &visibility) -> mlir::func::FuncOp {\n+              std::string &visibility) -> mlir::triton::FuncOp {\n              if (mlir::Operation *funcOperation = module.lookupSymbol(funcName))\n-               return llvm::dyn_cast<mlir::func::FuncOp>(funcOperation);\n+               return llvm::dyn_cast<mlir::triton::FuncOp>(funcOperation);\n              auto loc = self.getUnknownLoc();\n              if (auto funcTy = funcType.dyn_cast<mlir::FunctionType>()) {\n                llvm::SmallVector<mlir::NamedAttribute> attrs = {\n                    mlir::NamedAttribute(self.getStringAttr(\"sym_visibility\"),\n                                         self.getStringAttr(visibility))};\n-               return self.create<mlir::func::FuncOp>(loc, funcName, funcTy,\n-                                                      attrs);\n+               return self.create<mlir::triton::FuncOp>(loc, funcName, funcTy,\n+                                                        attrs);\n              }\n              throw std::runtime_error(\"invalid function type\");\n            })"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -67,7 +67,7 @@ def nvsmi(attrs):\n         (64, 1024, 1024): {'float16': 0.0271, 'float32': 0.0509, 'int8': 0.0169},\n         (64, 4096, 4096): {'float16': 0.16, 'float32': 0.162, 'int8': 0.097},\n         (64, 8192, 8192): {'float16': 0.30, 'float32': 0.257, 'int8': 0.174},\n-        (1024, 64, 1024): {'float16': 0.0263, 'float32': 0.0458, 'int8': 0.017},\n+        (1024, 64, 1024): {'float16': 0.037, 'float32': 0.0458, 'int8': 0.017},\n         (4096, 64, 4096): {'float16': 0.16, 'float32': 0.177, 'int8': 0.102},\n         (8192, 64, 8192): {'float16': 0.25, 'float32': 0.230, 'int8': 0.177},\n     }\n@@ -94,10 +94,10 @@ def test_matmul(M, N, K, dtype_str):\n         a = torch.randn((M, K), dtype=dtype, device='cuda')\n         b = torch.randn((K, N), dtype=dtype, device='cuda')\n     fn = lambda: triton.ops.matmul(a, b)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n+    ms = triton.testing.do_bench(fn, warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n \n #######################\n@@ -131,8 +131,8 @@ def _add(x_ptr, y_ptr, output_ptr, n_elements,\n     'a100': {\n         1024 * 16: 0.008,\n         1024 * 64: 0.034,\n-        1024 * 256: 0.114,\n-        1024 * 1024: 0.315,\n+        1024 * 256: 0.132,\n+        1024 * 1024: 0.352,\n         1024 * 4096: 0.580,\n         1024 * 16384: 0.782,\n         1024 * 65536: 0.850,\n@@ -150,10 +150,10 @@ def test_elementwise(N):\n     y = torch.randn_like(z)\n     grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n     fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=500)\n+    ms = triton.testing.do_bench(fn, warmup=100, rep=500)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n #######################\n # Flash-Attention\n@@ -189,7 +189,7 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n         o = fn()\n         do = torch.randn_like(o)\n         fn = lambda: o.backward(do, retain_graph=True)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=500)\n+    ms = triton.testing.do_bench(fn, warmup=100, rep=500)\n     # compute flops\n     flops_per_matmul = 2. * Z * H * N_CTX * N_CTX * D_HEAD * 0.5\n     total_flops = 2 * flops_per_matmul\n@@ -201,4 +201,4 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n-    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 122, "deletions": 25, "changes": 147, "file_content_changes": "@@ -110,6 +110,26 @@ def check_type_supported(dtype):\n         pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n \n \n+class MmaLayout:\n+    def __init__(self, version, warps_per_cta):\n+        self.version = version\n+        self.warps_per_cta = str(warps_per_cta)\n+\n+    def __str__(self):\n+        return f\"#triton_gpu.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}}}>\"\n+\n+\n+class BlockedLayout:\n+    def __init__(self, size_per_thread, threads_per_warp, warps_per_cta, order):\n+        self.sz_per_thread = str(size_per_thread)\n+        self.threads_per_warp = str(threads_per_warp)\n+        self.warps_per_cta = str(warps_per_cta)\n+        self.order = str(order)\n+\n+    def __str__(self):\n+        return f\"#triton_gpu.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}}}>\"\n+\n+\n @pytest.mark.parametrize(\"dtype_x\", list(dtypes) + [\"bfloat16\"])\n def test_empty_kernel(dtype_x, device='cuda'):\n     SIZE = 128\n@@ -408,6 +428,35 @@ def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n     _test_binary(dtype_x, dtype_y, expr, numpy_expr, mode_x=mode_x, mode_y=mode_y, device=device)\n \n \n+# ---------------\n+# test broadcast\n+# ---------------\n+@pytest.mark.parametrize(\"dtype\", dtypes_with_bfloat16)\n+def test_broadcast(dtype):\n+    @triton.jit\n+    def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.constexpr):\n+        offset1 = tl.arange(0, M)\n+        offset2 = tl.arange(0, N)\n+        x = tl.load(x_ptr + N * offset1[:, None] + offset2[None, :])\n+        y = tl.load(y_ptr + offset2)\n+        _, y_broadcasted = tl.broadcast(x, y)\n+        tl.store(y_broadcasted_ptr + N * offset1[:, None] + offset2[None, :], y_broadcasted)\n+\n+    M = 32\n+    N = 64\n+    rs = RandomState(17)\n+    x = numpy_random((M, N), dtype_str=dtype, rs=rs)\n+    y = numpy_random(N, dtype_str=dtype, rs=rs)\n+    _, y_broadcasted_np = np.broadcast_arrays(x, y)\n+\n+    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n+    y_tri = to_triton(y, device='cuda', dst_type=dtype)\n+    y_broadcasted_tri = to_triton(np.empty((M, N), dtype=y_broadcasted_np.dtype), device='cuda', dst_type=dtype)\n+\n+    broadcast_kernel[(1,)](x_tri, y_tri, y_broadcasted_tri, M=M, N=N)\n+    assert (y_broadcasted_np == to_numpy(y_broadcasted_tri)).all()\n+\n+\n # ---------------\n # test where\n # ---------------\n@@ -514,7 +563,7 @@ def test_unary_op(dtype_x, expr, device='cuda'):\n # ----------------\n \n \n-@pytest.mark.parametrize(\"dtype_x, expr\", [(dtype_x, expr) for dtype_x in float_dtypes for expr in ['exp', 'log', 'cos', 'sin']])\n+@pytest.mark.parametrize(\"dtype_x, expr\", [(dtype_x, expr) for dtype_x in [\"float32\", \"float64\"] for expr in ['exp', 'log', 'cos', 'sin']])\n def test_math_op(dtype_x, expr, device='cuda'):\n     _test_unary(dtype_x, f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n \n@@ -1211,6 +1260,75 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n         else:\n             np.testing.assert_equal(z_ref, z_tri)\n \n+\n+layouts = [\n+    BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n+    BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1]),\n+    MmaLayout(version=(2, 0), warps_per_cta=[4, 1])\n+]\n+\n+\n+@pytest.mark.parametrize(\"M, N\", [[128, 16], [128, 128], [32, 128]])\n+@pytest.mark.parametrize(\"src_layout\", layouts)\n+@pytest.mark.parametrize(\"axis\", [0, 1])\n+def test_reduce_layouts(M, N, src_layout, axis, device='cuda'):\n+    rdims_2d = f\"1x{N}\" if axis == 0 else f\"{M}x1\"\n+    rdims_1d = f\"{N}\" if axis == 0 else f\"{M}\"\n+    store_range = \"%7\" if axis == 0 else \"%1\"\n+    ir = f\"\"\"\n+    #blocked = #triton_gpu.blocked<{{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}}>\n+    #src = {src_layout}\n+    module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n+    tt.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}, %arg1: i32 {{tt.divisibility = 16 : i32}}, %arg2: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}) {{\n+        %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>\n+        %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>\n+        %2 = tt.splat %arg1 : (i32) -> tensor<{M}x1xi32, #blocked>\n+        %3 = arith.muli %1, %2 : tensor<{M}x1xi32, #blocked>\n+        %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<{M}x1x!tt.ptr<f32>, #blocked>\n+        %5 = tt.addptr %4, %3 : tensor<{M}x1x!tt.ptr<f32>, #blocked>, tensor<{M}x1xi32, #blocked>\n+        %6 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>\n+        %7 = tt.expand_dims %6 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x{N}xi32, #blocked>\n+        %8 = tt.broadcast %5 : (tensor<{M}x1x!tt.ptr<f32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<f32>, #blocked>\n+        %9 = tt.broadcast %7 : (tensor<1x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #blocked>\n+        %10 = tt.addptr %8, %9 : tensor<{M}x{N}x!tt.ptr<f32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n+        %11 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<{rdims_2d}x!tt.ptr<f32>, #blocked>\n+        %12 = tt.addptr %11, {store_range} : tensor<{rdims_2d}x!tt.ptr<f32>, #blocked>, tensor<{rdims_2d}xi32, #blocked>\n+        %13 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xf32, #blocked>\n+        %14 = triton_gpu.convert_layout %13 : (tensor<{M}x{N}xf32, #blocked>) -> tensor<{M}x{N}xf32, #src>\n+        %15 = tt.reduce %14 {{axis = {axis} : i32, redOp = 12 : i32}} : tensor<{M}x{N}xf32, #src> -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n+        %16 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n+        %17 = tt.expand_dims %16 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xf32, #blocked>\n+        tt.store %12, %17 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xf32, #blocked>\n+        tt.return\n+    }}\n+    }}\n+    \"\"\"\n+\n+    import tempfile\n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n+        f.write(ir)\n+        f.flush()\n+        kernel = triton.compile(f.name)\n+\n+    rs = RandomState(17)\n+    x = rs.randint(0, 4, (M, N)).astype('float32')\n+    x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+\n+    if axis == 0:\n+        z = np.zeros((1, N)).astype('float32')\n+    else:\n+        z = np.zeros((M, 1)).astype('float32')\n+\n+    x_tri = torch.tensor(x, device=device)\n+    z_tri = torch.tensor(z, device=device)\n+\n+    pgm = kernel[(1, 1, 4)](x_tri, x_tri.stride(0), z_tri)\n+\n+    z_ref = np.max(x, axis=axis, keepdims=True)\n+\n+    np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+\n+\n # ---------------\n # test permute\n # ---------------\n@@ -1345,7 +1463,7 @@ def kernel(X, stride_xm, stride_xk,\n         if DO_SOFTMAX:\n             max = tl.max(z, 1)\n             z = z - max[:, None]\n-            num = tl.exp(z)\n+            num = tl.exp(z.to(tl.float32)).to(max.dtype)\n             den = tl.sum(num, 1)\n             z = num / den[:, None]\n         if CHAIN_DOT:\n@@ -2158,27 +2276,6 @@ def kernel(InitI, Bound, CutOff, OutI, OutJ):\n # -----------------------\n # TODO: backend should be tested separately\n \n-\n-class MmaLayout:\n-    def __init__(self, version, warps_per_cta):\n-        self.version = version\n-        self.warps_per_cta = str(warps_per_cta)\n-\n-    def __str__(self):\n-        return f\"#triton_gpu.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}}}>\"\n-\n-\n-class BlockedLayout:\n-    def __init__(self, size_per_thread, threads_per_warp, warps_per_cta, order):\n-        self.sz_per_thread = str(size_per_thread)\n-        self.threads_per_warp = str(threads_per_warp)\n-        self.warps_per_cta = str(warps_per_cta)\n-        self.order = str(order)\n-\n-    def __str__(self):\n-        return f\"#triton_gpu.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}}}>\"\n-\n-\n layouts = [\n     # MmaLayout(version=1, warps_per_cta=[1, 4]),\n     MmaLayout(version=(2, 0), warps_per_cta=[1, 4]),\n@@ -2209,7 +2306,7 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n #dst = {dst_layout}\n \"\"\" + \"\"\"\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func.func public @kernel_0d1d(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n+  tt.func public @kernel_0d1d(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n     %cst = arith.constant dense<128> : tensor<128x1xi32, #src>\n     %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #src}>>\n     %1 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #src}>>\n@@ -2227,7 +2324,7 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n     %13 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n     %14 = tt.addptr %3, %12 : tensor<128x128x!tt.ptr<f16>, #dst>, tensor<128x128xi32, #dst>\n     tt.store %14, %13 : tensor<128x128xf16, #dst>\n-    return\n+    tt.return\n   }\n }\n \"\"\""}, {"filename": "python/test/unit/operators/test_blocksparse.py", "status": "modified", "additions": 8, "deletions": 11, "changes": 19, "file_content_changes": "@@ -81,17 +81,14 @@ def test_matmul(MODE, TRANS_A, TRANS_B, BLOCK, DTYPE, Z=3, H=2, M=512, N=384, K=\n     a_tri.retain_grad()\n     b_tri.retain_grad()\n     op = triton.ops.blocksparse.matmul(layout, BLOCK, MODE, trans_a=TRANS_A, trans_b=TRANS_B, device=\"cuda\")\n-    try:\n-        c_tri = op(a_tri, b_tri)\n-        c_tri.backward(dc_tri)\n-        da_tri = a_tri.grad\n-        db_tri = b_tri.grad\n-        # compare\n-        torch.testing.assert_allclose(c_ref, c_tri)\n-        torch.testing.assert_allclose(da_ref, da_tri)\n-        torch.testing.assert_allclose(db_ref, db_tri)\n-    except triton.OutOfResourcesError as e:\n-        pytest.skip(str(e))\n+    c_tri = op(a_tri, b_tri)\n+    c_tri.backward(dc_tri)\n+    da_tri = a_tri.grad\n+    db_tri = b_tri.grad\n+    # compare\n+    torch.testing.assert_allclose(c_ref, c_tri)\n+    torch.testing.assert_allclose(da_ref, da_tri)\n+    torch.testing.assert_allclose(db_ref, db_tri)\n \n \n configs = ["}, {"filename": "python/test/unit/runtime/test_launch.py", "status": "modified", "additions": 68, "deletions": 67, "changes": 135, "file_content_changes": "@@ -1,19 +1,18 @@\n import gc\n-import importlib\n-import os\n-import sys\n-import tempfile\n-import textwrap\n-import time\n+# import importlib\n+# import os\n+# import sys\n+# import tempfile\n+# import textwrap\n+# import time\n import tracemalloc\n-from typing import Tuple\n \n import torch\n \n import triton\n import triton.language as tl\n \n-LATENCY_THRESHOLD_US = 46\n+# from typing import Tuple\n \n \n def test_memory_leak() -> None:\n@@ -44,62 +43,64 @@ def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n         tracemalloc.stop()\n \n \n-def test_kernel_launch_latency() -> None:\n-    def define_kernel(kernel_name: str, num_tensor_args: int) -> str:\n-        arg_str = \",\".join([f\"arg{i}: torch.Tensor\" for i in range(num_tensor_args)])\n-        arg_str += \", n_elements: int, BLOCK_SIZE: tl.constexpr\"\n-        func_str = f\"\"\"\n-        import torch\n-\n-        import triton\n-        import triton.language as tl\n-\n-        @triton.jit\n-        def {kernel_name}({arg_str}):\n-            pass\n-        \"\"\"\n-        with tempfile.NamedTemporaryFile(mode=\"w+t\", suffix=\".py\", delete=False) as temp_file:\n-            temp_file.write(textwrap.dedent(func_str))\n-            temp_file_path = temp_file.name\n-\n-        return temp_file_path\n-\n-    def import_kernel(file_path, kernel_name):\n-        directory, filename = os.path.split(file_path)\n-        module_name, _ = os.path.splitext(filename)\n-        sys.path.insert(0, directory)\n-\n-        module = importlib.import_module(module_name)\n-        kernel = getattr(module, kernel_name)\n-        return kernel\n-\n-    def empty(*kernel_args: Tuple[torch.Tensor]):\n-        first_arg = kernel_args[0]\n-        n_elements = first_arg.numel()\n-        grid = (triton.cdiv(n_elements, 1024),)\n-        device = torch.cuda.current_device()\n-        # Warmup\n-        empty_kernel[grid](*kernel_args, n_elements, BLOCK_SIZE=1024, device=device)\n-        torch.cuda.synchronize()\n-        # Measure launch overhead at steady state\n-        num_runs = 1000\n-        start_time = time.time()\n-        for i in range(num_runs):\n-            empty_kernel[grid](*kernel_args, n_elements, BLOCK_SIZE=1024, device=device)\n-        end_time = time.time()\n-        latency_us = (end_time - start_time) / num_runs * 1e6\n-\n-        assert latency_us < LATENCY_THRESHOLD_US, \"Kernel launch time has increased!\"\n-\n-    num_tensor_args = 40\n-    kernel_name = 'empty_kernel'\n-    file_path = define_kernel(kernel_name, num_tensor_args)\n-    empty_kernel = import_kernel(file_path, kernel_name)\n-\n-    # Initialize random tensors for the empty_kernel\n-    torch.manual_seed(0)\n-    size = 1024\n-    kernel_args = (torch.rand(size, device='cuda') for i in range(num_tensor_args))\n-\n-    # Run empty, which would run empty_kernel internally\n-    empty(*kernel_args)\n+# LATENCY_THRESHOLD_US = 46\n+\n+# def test_kernel_launch_latency() -> None:\n+#     def define_kernel(kernel_name: str, num_tensor_args: int) -> str:\n+#         arg_str = \",\".join([f\"arg{i}: torch.Tensor\" for i in range(num_tensor_args)])\n+#         arg_str += \", n_elements: int, BLOCK_SIZE: tl.constexpr\"\n+#         func_str = f\"\"\"\n+#         import torch\n+\n+#         import triton\n+#         import triton.language as tl\n+\n+#         @triton.jit\n+#         def {kernel_name}({arg_str}):\n+#             pass\n+#         \"\"\"\n+#         with tempfile.NamedTemporaryFile(mode=\"w+t\", suffix=\".py\", delete=False) as temp_file:\n+#             temp_file.write(textwrap.dedent(func_str))\n+#             temp_file_path = temp_file.name\n+\n+#         return temp_file_path\n+\n+#     def import_kernel(file_path, kernel_name):\n+#         directory, filename = os.path.split(file_path)\n+#         module_name, _ = os.path.splitext(filename)\n+#         sys.path.insert(0, directory)\n+\n+#         module = importlib.import_module(module_name)\n+#         kernel = getattr(module, kernel_name)\n+#         return kernel\n+\n+#     def empty(*kernel_args: Tuple[torch.Tensor]):\n+#         first_arg = kernel_args[0]\n+#         n_elements = first_arg.numel()\n+#         grid = (triton.cdiv(n_elements, 1024),)\n+#         device = torch.cuda.current_device()\n+#         # Warmup\n+#         empty_kernel[grid](*kernel_args, n_elements, BLOCK_SIZE=1024, device=device)\n+#         torch.cuda.synchronize()\n+#         # Measure launch overhead at steady state\n+#         num_runs = 1000\n+#         start_time = time.time()\n+#         for i in range(num_runs):\n+#             empty_kernel[grid](*kernel_args, n_elements, BLOCK_SIZE=1024, device=device)\n+#         end_time = time.time()\n+#         latency_us = (end_time - start_time) / num_runs * 1e6\n+\n+#         assert latency_us < LATENCY_THRESHOLD_US, \"Kernel launch time has increased!\"\n+\n+#     num_tensor_args = 40\n+#     kernel_name = 'empty_kernel'\n+#     file_path = define_kernel(kernel_name, num_tensor_args)\n+#     empty_kernel = import_kernel(file_path, kernel_name)\n+\n+#     # Initialize random tensors for the empty_kernel\n+#     torch.manual_seed(0)\n+#     size = 1024\n+#     kernel_args = (torch.rand(size, device='cuda') for i in range(num_tensor_args))\n+\n+#     # Run empty, which would run empty_kernel internally\n+#     empty(*kernel_args)"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 32, "deletions": 37, "changes": 69, "file_content_changes": "@@ -5,6 +5,7 @@\n from typing import Any, Callable, Dict, Optional, Tuple, Type, Union\n \n from .. import language\n+from ..language import constexpr, tensor\n # ideally we wouldn't need any runtime component\n from ..runtime import JITFunction\n from .errors import (CompilationError, CompileTimeAssertionFailure,\n@@ -49,15 +50,15 @@ def mangle_fn(name, arg_tys, constants):\n \n \n def _is_triton_tensor(o: Any) -> bool:\n-    return isinstance(o, language.tensor)\n+    return isinstance(o, tensor)\n \n \n def _is_constexpr(o: Any) -> bool:\n-    return isinstance(o, language.constexpr)  # TODO: fetch language.constexpr to a global after circular imports untangled, saving getattr\n+    return isinstance(o, constexpr)\n \n \n def _unwrap_if_constexpr(o: Any):\n-    return o.value if isinstance(o, language.constexpr) else o\n+    return o.value if isinstance(o, constexpr) else o\n \n \n _condition_types = {bool, int, type(None)}  # Python types accepted for conditionals inside kernels\n@@ -100,24 +101,17 @@ def __init__(self, context, prototype, gscope, attributes, constants, function_n\n         self.scf_stack = []\n         # SSA-construction\n         # name => language.tensor\n-        self.local_defs: Dict[str, language.tensor] = {}\n-        self.global_uses: Dict[str, language.tensor] = {}\n+        self.local_defs: Dict[str, tensor] = {}\n+        self.global_uses: Dict[str, tensor] = {}\n         self.dereference_name: Callable[[str], Any] = self._define_name_lookup()\n \n     builtin_namespace: Dict[str, Any] = {_.__name__: _ for _ in (range, float, int, isinstance, getattr)}\n+    builtin_namespace.update((\n+        ('print', language.core.device_print),\n+        ('min', language.minimum),\n+    ))\n \n     def _define_name_lookup(self):\n-        # TODO: this needs to be moved to class scope when cyclic imports untangled and `language` can be imported at module level\n-        self.builtin_namespace.update((\n-            ('print', language.core.device_print),\n-            ('min', language.minimum),  # TODO: why `min`? if `min`, why not `max`? `sum`? `all`?\n-        ))\n-        # TODO: this needs to be moved to class scope when cyclic imports untangled and `language` can be imported at module level\n-        self.statically_implemented_functions.update((\n-            (language.core.static_assert, CodeGenerator.execute_static_assert),\n-            (language.core.static_print, CodeGenerator.execute_static_print),\n-        ))\n-\n         def local_lookup(name: str, absent):\n             value = self.lscope.get(name, absent)  # this needs to be re-fetched from `self` every time, because it gets switched occasionally\n             if value is not absent and name not in self.local_defs:\n@@ -127,9 +121,8 @@ def local_lookup(name: str, absent):\n         absent_marker = object()\n \n         def name_lookup(name: str) -> Any:\n-            lookup_order = local_lookup, self.gscope.get, self.builtin_namespace.get\n             absent = absent_marker\n-            for lookup_function in lookup_order:\n+            for lookup_function in local_lookup, self.gscope.get, self.builtin_namespace.get:\n                 value = lookup_function(name, absent)\n                 if value is not absent:\n                     return value\n@@ -138,7 +131,7 @@ def name_lookup(name: str) -> Any:\n         return name_lookup\n \n     def set_value(self, name: str,\n-                  value: Union[language.tensor, language.constexpr]) -> None:\n+                  value: Union[tensor, constexpr]) -> None:\n         ''' This function:\n           called by visit_Assign() & visit_FuncDef() to store left value (lvalue)\n         1. record local defined name (FIXME: should consider control flow)\n@@ -243,13 +236,13 @@ def visit_FunctionDef(self, node):\n             if i in self.constants:\n                 cst = self.constants[i]\n                 if not _is_constexpr(cst):\n-                    cst = language.constexpr(self.constants[i])\n+                    cst = constexpr(self.constants[i])\n                 arg_values.append(cst)\n                 continue\n             else:\n                 if i in self.attributes:\n                     fn.set_arg_attr(idx, \"tt.divisibility\", self.attributes[i][1])\n-                arg_values.append(language.tensor(fn.args(idx), self.prototype.param_types[idx]))\n+                arg_values.append(tensor(fn.args(idx), self.prototype.param_types[idx]))\n                 idx += 1\n \n         insert_pt = self.builder.get_insertion_block()\n@@ -289,12 +282,12 @@ def visit_AnnAssign(self, node):\n         target = self.visit(node.target)\n         value = self.visit(node.value)\n         # constexpr\n-        if annotation == language.constexpr:\n+        if annotation == constexpr:\n             if target in self.lscope:\n                 raise ValueError(f'{target} is already defined.'\n                                  f' constexpr cannot be reassigned.')\n             if not _is_constexpr(value):\n-                value = language.constexpr(value)\n+                value = constexpr(value)\n             self.lscope[target] = value\n             return self.lscope[target]\n         # default: call visit_Assign\n@@ -515,9 +508,9 @@ def visit_Compare(self, node):\n         lhs = _unwrap_if_constexpr(self.visit(node.left))\n         rhs = _unwrap_if_constexpr(self.visit(node.comparators[0]))\n         if type(node.ops[0]) == ast.Is:\n-            return language.constexpr(lhs is rhs)\n+            return constexpr(lhs is rhs)\n         if type(node.ops[0]) == ast.IsNot:\n-            return language.constexpr(lhs is not rhs)\n+            return constexpr(lhs is not rhs)\n         method_name = self._method_name_for_comp_op.get(type(node.ops[0]))\n         if method_name is None:\n             raise UnsupportedLanguageConstruct(None, node, \"AST comparison operator '{}' is not (currently) implemented.\".format(node.ops[0].__name__))\n@@ -631,7 +624,7 @@ def visit_For(self, node):\n                                  iterator.end.value,\n                                  iterator.step.value)\n             for i in static_range:\n-                self.lscope[node.target.id] = language.constexpr(i)\n+                self.lscope[node.target.id] = constexpr(i)\n                 self.visit_compound_statement(node.body)\n                 for stmt in node.orelse:\n                     ast.NodeVisitor.generic_visit(self, stmt)\n@@ -649,7 +642,7 @@ def visit_For(self, node):\n         # handle negative constant step (not supported by scf.for in MLIR)\n         negative_step = False\n         if _is_constexpr(step) and step.value < 0:\n-            step = language.constexpr(-step.value)\n+            step = constexpr(-step.value)\n             negative_step = True\n             lb, ub = ub, lb\n         lb = language.core._to_tensor(lb, self.builder)\n@@ -779,7 +772,7 @@ def visit_Call(self, node):\n             args = getcallargs(fn.fn, *args, **kws)\n             args = [args[name] for name in fn.arg_names]\n             args = [arg if _is_triton_tensor(arg)\n-                    else language.constexpr(arg) for arg in args]\n+                    else constexpr(arg) for arg in args]\n             # generate function def\n             attributes = dict()\n             constexprs = [i for i, arg in enumerate(args) if _is_constexpr(arg)]\n@@ -804,12 +797,12 @@ def visit_Call(self, node):\n             if call_op.get_num_results() == 0 or callee_ret_type is None:\n                 return None\n             elif call_op.get_num_results() == 1:\n-                return language.tensor(call_op.get_result(0), callee_ret_type)\n+                return tensor(call_op.get_result(0), callee_ret_type)\n             else:\n                 # should return a tuple of tl.tensor\n                 results = []\n                 for i in range(call_op.get_num_results()):\n-                    results.append(language.tensor(call_op.get_result(i), callee_ret_type[i]))\n+                    results.append(tensor(call_op.get_result(i), callee_ret_type[i]))\n                 return tuple(results)\n         if (hasattr(fn, '__self__') and _is_triton_tensor(fn.__self__)) or language.core.is_builtin(fn):\n             return fn(*args, _builder=self.builder, **kws)\n@@ -818,7 +811,7 @@ def visit_Call(self, node):\n         return fn(*args, **kws)\n \n     def visit_Constant(self, node):\n-        return language.constexpr(node.value)\n+        return constexpr(node.value)\n \n     def visit_BoolOp(self, node: ast.BoolOp):\n         if len(node.values) != 2:\n@@ -833,13 +826,13 @@ def visit_BoolOp(self, node: ast.BoolOp):\n \n     if sys.version_info < (3, 8):\n         def visit_NameConstant(self, node):\n-            return language.constexpr(node.value)\n+            return constexpr(node.value)\n \n         def visit_Num(self, node):\n-            return language.constexpr(node.n)\n+            return constexpr(node.n)\n \n         def visit_Str(self, node):\n-            return language.constexpr(ast.literal_eval(node))\n+            return constexpr(ast.literal_eval(node))\n \n     def visit_Attribute(self, node):\n         lhs = self.visit(node.value)\n@@ -883,9 +876,6 @@ def visit(self, node):\n     def generic_visit(self, node):\n         raise UnsupportedLanguageConstruct(None, node, \"unsupported AST node type: {}\".format(type(node).__name__))\n \n-    # TODO: populate this here (rather than inside `_define_name_lookup`) once cyclic imports resolved\n-    statically_implemented_functions: Dict[object, Callable[[ast.Call], Any]] = {}\n-\n     def execute_static_print(self, node: ast.Call) -> None:\n         # TODO: too simplistic? Perhaps do something else with non-constexpr\n \n@@ -913,6 +903,11 @@ def execute_static_assert(self, node: ast.Call) -> None:\n             raise CompileTimeAssertionFailure(None, node, _unwrap_if_constexpr(message))\n         return None\n \n+    statically_implemented_functions: Dict[object, Callable[[ast.Call], Any]] = {\n+        language.core.static_assert: execute_static_assert,\n+        language.core.static_print: execute_static_print,\n+    }\n+\n \n def str_to_ty(name):\n     if name[0] == \"*\":"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 40, "deletions": 24, "changes": 64, "file_content_changes": "@@ -17,7 +17,7 @@\n import triton._C.libtriton.triton as _triton\n # TODO: runtime.errors\n from ..runtime.autotuner import OutOfResources\n-from ..runtime.cache import CacheManager\n+from ..runtime.cache import get_cache_manager\n from ..runtime.driver import get_cuda_utils, get_hip_utils\n from ..tools.disasm import extract\n from .code_generator import ast_to_ttir\n@@ -267,14 +267,14 @@ def make_hash(fn, **kwargs):\n     return hashlib.md5((Path(fn).read_text() + triton.runtime.jit.version_key()).encode(\"utf-8\")).hexdigest()\n \n \n-# - ^\\s*func\\.func\\s+ : match the start of the string, any leading whitespace, the keyword func,\n+# - ^\\s*tt\\.func\\s+ : match the start of the string, any leading whitespace, the keyword func,\n #    and any following whitespace\n # - (public\\s+)? : optionally match the keyword public and any following whitespace\n # - (@\\w+) : match an @ symbol followed by one or more word characters\n #   (letters, digits, or underscores), and capture it as group 1 (the function name)\n # - (\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\)) : match a pair of parentheses enclosing\n #   zero or more arguments separated by commas, and capture it as group 2 (the argument list)\n-mlir_prototype_pattern = r'^\\s*func\\.func\\s+(?:public\\s+)?(@\\w+)(\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\))\\s*\\{\\s*$'\n+mlir_prototype_pattern = r'^\\s*tt\\.func\\s+(?:public\\s+)?(@\\w+)(\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\))\\s*\\{\\s*$'\n ptx_prototype_pattern = r\"\\.(?:visible|extern)\\s+\\.(?:entry|func)\\s+(\\w+)\\s*\\(([^)]*)\\)\"\n prototype_pattern = {\n     \"ttir\": mlir_prototype_pattern,\n@@ -410,7 +410,7 @@ def compile(fn, **kwargs):\n     # cache manager\n     so_path = make_stub(name, signature, constants)\n     # create cache manager\n-    fn_cache_manager = CacheManager(make_hash(fn, **kwargs))\n+    fn_cache_manager = get_cache_manager(make_hash(fn, **kwargs))\n     # determine name and extension type of provided function\n     if isinstance(fn, triton.runtime.JITFunction):\n         name, ext = fn.__name__, \"ast\"\n@@ -419,14 +419,22 @@ def compile(fn, **kwargs):\n \n     # load metadata if any\n     metadata = None\n-    if fn_cache_manager.has_file(f'{name}.json'):\n-        with open(fn_cache_manager._make_path(f\"{name}.json\")) as f:\n+    metadata_filename = f\"{name}.json\"\n+\n+    # The group is addressed by the metadata\n+    metadata_group = fn_cache_manager.get_group(\n+        metadata_filename\n+    ) or {}\n+\n+    metadata_path = metadata_group.get(metadata_filename)\n+\n+    if metadata_path is not None:\n+        with open(metadata_path) as f:\n             metadata = json.load(f)\n     else:\n         metadata = {\"num_warps\": num_warps,\n                     \"num_stages\": num_stages,\n                     \"constants\": _get_jsonable_constants(constants),\n-                    \"ctime\": dict(),\n                     \"debug\": debug}\n         if ext == \"ptx\":\n             assert \"shared\" in kwargs, \"ptx compilation must provide shared memory size\"\n@@ -437,25 +445,30 @@ def compile(fn, **kwargs):\n     module = fn\n     # run compilation pipeline  and populate metadata\n     for ir, (parse, compile_kernel) in list(stages.items())[first_stage:]:\n-        path = fn_cache_manager._make_path(f\"{name}.{ir}\")\n+        ir_filename = f\"{name}.{ir}\"\n+\n         if ir == ext:\n             next_module = parse(fn)\n-        elif os.path.exists(path) and\\\n-                ir in metadata[\"ctime\"] and\\\n-                os.path.getctime(path) == metadata[\"ctime\"][ir]:\n-            if ir == \"amdgcn\":\n-                next_module = (parse(path), parse(fn_cache_manager._make_path(f\"{name}.hsaco_path\")))\n-            else:\n-                next_module = parse(path)\n         else:\n-            next_module = compile_kernel(module)\n-            if ir == \"amdgcn\":\n-                fn_cache_manager.put(next_module[0], f\"{name}.{ir}\")\n-                fn_cache_manager.put(next_module[1], f\"{name}.hsaco_path\")\n+            path = metadata_group.get(ir_filename)\n+            if path is None:\n+                next_module = compile_kernel(module)\n+                if ir == \"amdgcn\":\n+                    extra_file_name = f\"{name}.hsaco_path\"\n+                    metadata_group[ir_filename] = fn_cache_manager.put(next_module[0], ir_filename)\n+                    metadata_group[extra_file_name] = fn_cache_manager.put(next_module[1], extra_file_name)\n+                else:\n+                    metadata_group[ir_filename] = fn_cache_manager.put(next_module, ir_filename)\n+                    fn_cache_manager.put(next_module, ir_filename)\n             else:\n-                fn_cache_manager.put(next_module, f\"{name}.{ir}\")\n-        if os.path.exists(path):\n-            metadata[\"ctime\"][ir] = os.path.getctime(path)\n+                if ir == \"amdgcn\":\n+                    extra_file_name = f\"{name}.hsaco_path\"\n+                    hasco_path = metadata_group.get(extra_file_name)\n+                    assert hasco_path is not None, \"Expected to have hsaco_path in metadata when we have the amdgcn\"\n+                    next_module = (parse(path), parse(hasco_path))\n+                else:\n+                    next_module = parse(path)\n+\n         if ir == \"cubin\":\n             asm[ir] = next_module\n         elif ir == \"amdgcn\":\n@@ -470,8 +483,11 @@ def compile(fn, **kwargs):\n             metadata[\"name\"] = get_kernel_name(next_module[0], pattern='.globl')\n             asm[\"hsaco_path\"] = next_module[1]\n         module = next_module\n-    # write-back metadata\n-    fn_cache_manager.put(json.dumps(metadata), f\"{name}.json\", binary=False)\n+    # write-back metadata, if it didn't come from the cache\n+    if metadata_path is None:\n+        metadata_group[metadata_filename] = fn_cache_manager.put(json.dumps(metadata), metadata_filename, binary=False)\n+        fn_cache_manager.put_group(metadata_filename, metadata_group)\n+\n     # return handle to compiled kernel\n     return CompiledKernel(fn, so_path, metadata, asm)\n "}, {"filename": "python/triton/compiler/make_launcher.py", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "file_content_changes": "@@ -3,7 +3,7 @@\n import tempfile\n \n from ..common import _build\n-from ..runtime.cache import CacheManager\n+from ..runtime.cache import get_cache_manager\n from ..runtime.jit import version_key\n \n \n@@ -26,19 +26,21 @@ def make_so_cache_key(version_hash, signature, constants):\n def make_stub(name, signature, constants):\n     # name of files that are cached\n     so_cache_key = make_so_cache_key(version_key(), signature, constants)\n-    so_cache_manager = CacheManager(so_cache_key)\n+    so_cache_manager = get_cache_manager(so_cache_key)\n     so_name = f\"{name}.so\"\n     # retrieve stub from cache if it exists\n-    if not so_cache_manager.has_file(so_name):\n+    cache_path = so_cache_manager.get_file(so_name)\n+    if cache_path is None:\n         with tempfile.TemporaryDirectory() as tmpdir:\n             src = generate_launcher(constants, signature)\n             src_path = os.path.join(tmpdir, \"main.c\")\n             with open(src_path, \"w\") as f:\n                 f.write(src)\n             so = _build(name, src_path, tmpdir)\n             with open(so, \"rb\") as f:\n-                so_cache_manager.put(f.read(), so_name, binary=True)\n-    return so_cache_manager._make_path(so_name)\n+                return so_cache_manager.put(f.read(), so_name, binary=True)\n+    else:\n+        return cache_path\n \n # ----- source code generation --------\n "}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -12,9 +12,6 @@\n \n TRITON_MAX_TENSOR_NUMEL = 131072\n \n-\n-T = TypeVar(\"T\")\n-\n TRITON_BUILTIN = \"__triton_builtin__\"\n \n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 35, "deletions": 3, "changes": 38, "file_content_changes": "@@ -1,12 +1,16 @@\n from __future__ import annotations  # remove after python 3.11\n \n-from typing import List, Optional, Tuple\n+from functools import wraps\n+from typing import List, Optional, Tuple, TypeVar\n \n from . import core as tl\n from triton._C.libtriton.triton import ir\n \n+T = TypeVar('T')\n \n # Create custom exception that prints message \"hello\"\n+\n+\n class IncompatibleTypeErrorImpl(Exception):\n     def __init__(self, type_a, type_b):\n         self.type_a = type_a\n@@ -599,13 +603,13 @@ def broadcast_impl_value(lhs: tl.tensor,\n         if len(lhs_shape) < len(rhs_shape):\n             # Add new axes to lhs\n             for dim in range(len(lhs_shape), len(rhs_shape)):\n-                lhs = tl.tensor(builder.create_expand_dims(lhs.handle, dim), tl.block_type(lhs_ty.scalar, lhs_shape + [1]))\n+                lhs = tl.tensor(builder.create_expand_dims(lhs.handle, 0), tl.block_type(lhs_ty.scalar, [1] + lhs_shape))\n                 lhs_ty = lhs.type\n                 lhs_shape = lhs_ty.get_block_shapes()\n         elif len(rhs_shape) < len(lhs_shape):\n             # Add new axes to rhs\n             for dim in range(len(rhs_shape), len(lhs_shape)):\n-                rhs = tl.tensor(builder.create_expand_dims(rhs.handle, dim), tl.block_type(rhs_ty.scalar, rhs_shape + [1]))\n+                rhs = tl.tensor(builder.create_expand_dims(rhs.handle, 0), tl.block_type(rhs_ty.scalar, [1] + rhs_shape))\n                 rhs_ty = rhs.type\n                 rhs_shape = rhs_ty.get_block_shapes()\n         assert len(rhs_shape) == len(lhs_shape)\n@@ -1315,35 +1319,63 @@ def xor_sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n #                               Math\n # ===----------------------------------------------------------------------===\n \n+def _check_dtype(dtypes: List[str]) -> T:\n+    \"\"\"\n+    We following libdevice's convention to check accepted data types for math functions.\n+    It is not a good practice to support all data types as accelerators/GPUs don't support\n+    many float16 and bfloat16 math operations.\n+    We should let the users know that they are using and invoke explicit cast to convert\n+    the data type to the supported one.\n+    \"\"\"\n+    def wrapper(fn):\n+        @wraps(fn)\n+        def check(*args, **kwargs):\n+            # concatenate args and kwargs\n+            all_args = list(args) + list(kwargs.values())\n+            for arg in [a for a in all_args if isinstance(a, tl.tensor)]:\n+                if arg.type.scalar.name not in dtypes:\n+                    raise ValueError(f\"Expected dtype {dtypes} but got {arg.type.scalar.name}\")\n+            return fn(*args, **kwargs)\n+        return check\n+\n+    return wrapper\n+\n+\n def umulhi(x: tl.tensor, y: tl.tensor, builder: ir.builder) -> tl.tensor:\n     x, y = binary_op_type_checking_impl(x, y, builder)\n     # FIXME(Keren): not portable, should be fixed\n     from . import math\n     return math.mulhi(x, y, _builder=builder)\n \n \n+@_check_dtype(dtypes=[\"fp32\", \"fp64\"])\n def floor(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     # FIXME(Keren): not portable, should be fixed\n     from . import math\n     return math.floor(x, _builder=builder)\n \n \n+@_check_dtype(dtypes=[\"fp32\", \"fp64\"])\n def exp(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_exp(x.handle), x.type)\n \n \n+@_check_dtype(dtypes=[\"fp32\", \"fp64\"])\n def log(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_log(x.handle), x.type)\n \n \n+@_check_dtype(dtypes=[\"fp32\", \"fp64\"])\n def cos(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_cos(x.handle), x.type)\n \n \n+@_check_dtype(dtypes=[\"fp32\", \"fp64\"])\n def sin(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_sin(x.handle), x.type)\n \n \n+@_check_dtype(dtypes=[\"fp32\", \"fp64\"])\n def sqrt(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_sqrt(x.handle), x.type)\n "}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -29,7 +29,7 @@ def _sdd_kernel(\n     # ------------ #\n     # - Prologue - #\n     # ------------ #\n-    block_id = tl.program_id(1) + grid_offset\n+    block_id = tl.program_id(0) + grid_offset\n     lut += block_id * 3\n     # offsets\n     off_z = tl.program_id(2)  # batch\n@@ -102,7 +102,7 @@ def sdd_matmul(a, b, trans_a, trans_b, trans_c, spdims, block, lut, widths, out=\n     else:\n         assert out.shape == (a.shape[0], lut.shape[0], block, block)\n         c = out\n-    grid = [1, c.shape[1], c.shape[0]]\n+    grid = [c.shape[1], 1, c.shape[0]]\n     _sdd_kernel[grid](\n         a, b, c,\n         a.stride(0), a.stride(1), a.stride(3 if trans_a else 2), a.stride(2 if trans_a else 3),"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -60,7 +60,7 @@ def _fwd_kernel(\n         l_curr = tl.sum(p, 1) + l_prev\n         # rescale operands of matmuls\n         l_rcp = 1. / l_curr\n-        p *= l_rcp\n+        p *= l_rcp[:, None]\n         acc *= (l_prev * l_rcp)[:, None]\n         # update acc\n         p = p.to(Q.dtype.element_ty)"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -77,7 +77,7 @@ def kernel_call():\n             self.hook(args)\n             self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **current)\n         try:\n-            return do_bench(kernel_call, percentiles=(0.5, 0.2, 0.8))\n+            return do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n         except OutOfResources:\n             return (float('inf'), float('inf'), float('inf'))\n "}, {"filename": "python/triton/runtime/cache.py", "status": "modified", "additions": 85, "deletions": 3, "changes": 88, "file_content_changes": "@@ -1,5 +1,8 @@\n+import json\n import os\n+from abc import ABC, abstractmethod\n from pathlib import Path\n+from typing import Dict, Optional\n \n from filelock import FileLock\n \n@@ -8,8 +11,32 @@ def default_cache_dir():\n     return os.path.join(Path.home(), \".triton\", \"cache\")\n \n \n-class CacheManager:\n+class CacheManager(ABC):\n+    def __init__(self, key):\n+        pass\n+\n+    @abstractmethod\n+    def get_file(self, filename) -> Optional[str]:\n+        pass\n+\n+    @abstractmethod\n+    def has_file(self, filename) -> bool:\n+        pass\n+\n+    @abstractmethod\n+    def put(self, data, filename, binary=True) -> str:\n+        pass\n+\n+    @abstractmethod\n+    def get_group(self, filename: str) -> Optional[Dict[str, str]]:\n+        pass\n \n+    @abstractmethod\n+    def put_group(self, filename: str, group: Dict[str, str]):\n+        pass\n+\n+\n+class FileCacheManager(CacheManager):\n     def __init__(self, key):\n         self.key = key\n         self.lock_path = None\n@@ -20,15 +47,48 @@ def __init__(self, key):\n             self.lock_path = os.path.join(self.cache_dir, \"lock\")\n             os.makedirs(self.cache_dir, exist_ok=True)\n \n-    def _make_path(self, filename):\n+    def _make_path(self, filename) -> str:\n         return os.path.join(self.cache_dir, filename)\n \n     def has_file(self, filename):\n         if not self.cache_dir:\n             return False\n         return os.path.exists(self._make_path(filename))\n \n-    def put(self, data, filename, binary=True):\n+    def get_file(self, filename) -> Optional[str]:\n+        if self.has_file(filename):\n+            return self._make_path(filename)\n+        else:\n+            return None\n+\n+    def get_group(self, filename: str) -> Optional[Dict[str, str]]:\n+        grp_filename = f\"__grp__{filename}\"\n+        if not self.has_file(grp_filename):\n+            return None\n+        grp_filepath = self._make_path(grp_filename)\n+        with open(grp_filepath) as f:\n+            grp_data = json.load(f)\n+        child_paths = grp_data.get(\"child_paths\", None)\n+        # Invalid group data.\n+        if child_paths is None:\n+            return None\n+        result = {}\n+        for c in child_paths:\n+            p = self._make_path(c)\n+            if not os.path.exists(p):\n+                raise Exception(f\"Group file {p} does not exist from group {grp_filename} \")\n+            result[c] = p\n+        return result\n+\n+    # Note a group of pushed files as being part of a group\n+    def put_group(self, filename: str, group: Dict[str, str]):\n+        if not self.cache_dir:\n+            return\n+        grp_contents = json.dumps({\"child_paths\": sorted(list(group.keys()))})\n+        grp_filename = f\"__grp__{filename}\"\n+        return self.put(grp_contents, grp_filename, binary=False)\n+\n+    def put(self, data, filename, binary=True) -> str:\n         if not self.cache_dir:\n             return\n         binary = isinstance(data, bytes)\n@@ -42,3 +102,25 @@ def put(self, data, filename, binary=True):\n             with open(filepath + \".tmp\", mode) as f:\n                 f.write(data)\n             os.rename(filepath + \".tmp\", filepath)\n+        return filepath\n+\n+\n+__cache_cls = FileCacheManager\n+__cache_cls_nme = \"DEFAULT\"\n+\n+\n+def get_cache_manager(key) -> CacheManager:\n+    import os\n+\n+    user_cache_manager = os.environ.get(\"TRITON_CACHE_MANAGER\", None)\n+    global __cache_cls\n+    global __cache_cls_nme\n+\n+    if user_cache_manager is not None and user_cache_manager != __cache_cls_nme:\n+        import importlib\n+        module_path, clz_nme = user_cache_manager.split(\":\")\n+        module = importlib.import_module(module_path)\n+        __cache_cls = getattr(module, clz_nme)\n+        __cache_cls_nme = user_cache_manager\n+\n+    return __cache_cls(key)"}, {"filename": "python/triton/runtime/driver/cuda.py", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -3,7 +3,7 @@\n import tempfile\n \n from ...common.build import _build\n-from ..cache import CacheManager\n+from ..cache import get_cache_manager\n \n \n def get_cuda_utils():\n@@ -140,18 +140,19 @@ def _generate_src():\n     def __init__(self):\n         src = self._generate_src()\n         key = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n-        cache = CacheManager(key)\n+        cache = get_cache_manager(key)\n         fname = \"cuda_utils.so\"\n-        if not cache.has_file(fname):\n+        cache_path = cache.get_file(fname)\n+        if cache_path is None:\n             with tempfile.TemporaryDirectory() as tmpdir:\n                 src_path = os.path.join(tmpdir, \"main.c\")\n                 with open(src_path, \"w\") as f:\n                     f.write(src)\n                 so = _build(\"cuda_utils\", src_path, tmpdir)\n                 with open(so, \"rb\") as f:\n-                    cache.put(f.read(), fname, binary=True)\n+                    cache_path = cache.put(f.read(), fname, binary=True)\n         import importlib.util\n-        spec = importlib.util.spec_from_file_location(\"cuda_utils\", cache._make_path(fname))\n+        spec = importlib.util.spec_from_file_location(\"cuda_utils\", cache_path)\n         mod = importlib.util.module_from_spec(spec)\n         spec.loader.exec_module(mod)\n         self.load_binary = mod.load_binary"}, {"filename": "python/triton/runtime/driver/hip.py", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -3,7 +3,7 @@\n import tempfile\n \n from ...common.build import _build\n-from ..cache import CacheManager\n+from ..cache import get_cache_manager\n \n \n def get_hip_utils():\n@@ -139,18 +139,19 @@ def _generate_src(self):\n     def __init__(self):\n         src = self._generate_src()\n         key = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n-        cache = CacheManager(key)\n+        cache = get_cache_manager(key)\n         fname = \"hip_utils.so\"\n-        if not cache.has_file(fname):\n+        cache_path = cache.get_file(fname)\n+        if cache_path is None:\n             with tempfile.TemporaryDirectory() as tmpdir:\n                 src_path = os.path.join(tmpdir, \"main.c\")\n                 with open(src_path, \"w\") as f:\n                     f.write(src)\n                 so = _build(\"hip_utils\", src_path, tmpdir)\n                 with open(so, \"rb\") as f:\n-                    cache.put(f.read(), fname, binary=True)\n+                    cache_path = cache.put(f.read(), fname, binary=True)\n         import importlib.util\n-        spec = importlib.util.spec_from_file_location(\"hip_utils\", cache._make_path(fname))\n+        spec = importlib.util.spec_from_file_location(\"hip_utils\", cache_path)\n         mod = importlib.util.module_from_spec(spec)\n         spec.loader.exec_module(mod)\n         self.load_binary = mod.load_binary"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 36, "deletions": 25, "changes": 61, "file_content_changes": "@@ -18,8 +18,10 @@ def nvsmi(attrs):\n \n \n def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n-             percentiles=(0.5, 0.2, 0.8),\n-             record_clocks=False, fast_flush=False):\n+             quantiles=None,\n+             fast_flush=True,\n+             return_mode=\"mean\"):\n+    assert return_mode in [\"min\", \"max\", \"mean\", \"median\"]\n     import torch\n     \"\"\"\n     Benchmark the runtime of the provided function. By default, return the median runtime of :code:`fn` along with\n@@ -33,8 +35,8 @@ def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n     :type rep: int\n     :param grad_to_none: Reset the gradient of the provided tensor to None\n     :type grad_to_none: torch.tensor, optional\n-    :param percentiles: Performance percentile to return in addition to the median.\n-    :type percentiles: list[float]\n+    :param quantiles: Performance percentile to return in addition to the median.\n+    :type quantiles: list[float]\n     :param fast_flush: Use faster kernel to flush L2 between measurements\n     :type fast_flush: bool\n     \"\"\"\n@@ -82,39 +84,48 @@ def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n     # Record clocks\n     torch.cuda.synchronize()\n     times = torch.tensor([s.elapsed_time(e) for s, e in zip(start_event, end_event)])\n-    if percentiles:\n-        percentiles = torch.quantile(times, torch.tensor(percentiles)).tolist()\n-        return tuple(percentiles)\n-    else:\n-        return torch.mean(times).item()\n+    if quantiles is not None:\n+        ret = torch.quantile(times, torch.tensor(quantiles)).tolist()\n+        if len(ret) == 1:\n+            ret = ret[0]\n+        return ret\n+    return getattr(torch, return_mode)(times).item()\n \n \n def assert_close(x, y, atol=None, rtol=None, err_msg=''):\n+    import numpy as np\n     import torch\n \n-    def default_atol(dtype):\n-        return 1e-2\n-\n-    def default_rtol(dtype):\n-        return 0.\n-    if atol is None:\n-        atol = default_atol\n-    if rtol is None:\n-        rtol = default_rtol\n+    # canonicalize arguments to be tensors\n     if not isinstance(x, torch.Tensor):\n         x = torch.tensor(x)\n     if not isinstance(y, torch.Tensor):\n         y = torch.tensor(y)\n+    # absolute tolerance\n+    if atol is None:\n+        atol = 1e-2\n     atol = atol(x.dtype) if callable(atol) else atol\n+    # relative tolerance hook\n+    if rtol is None:\n+        rtol = 0.\n     rtol = rtol(x.dtype) if callable(rtol) else rtol\n-    if x.numel() > 1 or y.numel() > 1:\n-        # we could use a fused kernel for fast `isclose`\n-        # if x.numel()*16 > torch.cuda.mem_get_info()[0]:\n-        torch.testing.assert_close(x.cpu(), y.cpu(), atol=atol, rtol=rtol, equal_nan=True)\n-        # else:\n-        #     torch.testing.assert_close(x, y, atol=atol, rtol=rtol, equal_nan=True)\n+    # we use numpy instead of pytorch\n+    # as it seems more memory efficient\n+    # pytorch tends to oom on large tensors\n+    if isinstance(x, torch.Tensor):\n+        if x.dtype == torch.bfloat16:\n+            x = x.float()\n+        x = x.cpu().detach().numpy()\n+    if isinstance(y, torch.Tensor):\n+        if y.dtype == torch.bfloat16:\n+            y = y.float()\n+        y = y.cpu().detach().numpy()\n+    # we handle size==1 case separately as we can\n+    # provide better error message there\n+    if x.size > 1 or y.size > 1:\n+        np.testing.assert_allclose(x, y, atol=atol, rtol=rtol, equal_nan=True)\n         return\n-    if not torch.isclose(x, y, atol=atol, rtol=rtol):\n+    if not np.allclose(x, y, atol=atol, rtol=rtol):\n         raise AssertionError(f'{err_msg} {x} is not close to {y} (atol={atol}, rtol={rtol})')\n \n "}, {"filename": "python/tutorials/01-vector-add.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -120,10 +120,11 @@ def add(x: torch.Tensor, y: torch.Tensor):\n def benchmark(size, provider):\n     x = torch.rand(size, device='cuda', dtype=torch.float32)\n     y = torch.rand(size, device='cuda', dtype=torch.float32)\n+    quantiles = [0.5, 0.2, 0.8]\n     if provider == 'torch':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y)\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y), quantiles=quantiles)\n     gbps = lambda ms: 12 * size / ms * 1e-6\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n "}, {"filename": "python/tutorials/02-fused-softmax.py", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -177,12 +177,13 @@ def softmax(x):\n )\n def benchmark(M, N, provider):\n     x = torch.randn(M, N, device='cuda', dtype=torch.float32)\n+    quantiles = [0.5, 0.2, 0.8]\n     if provider == 'torch-native':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1), quantiles=quantiles)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x), quantiles=quantiles)\n     if provider == 'torch-jit':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x), quantiles=quantiles)\n     gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n "}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -329,10 +329,11 @@ def matmul(a, b, activation=None):\n def benchmark(M, N, K, provider):\n     a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n     b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n+    quantiles = [0.5, 0.2, 0.8]\n     if provider == 'cublas':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), rep=100)\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), rep=100)\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n     perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n     return perf(ms), perf(max_ms), perf(min_ms)\n "}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -339,6 +339,7 @@ def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='c\n     x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\n     dy = .1 * torch.randn_like(x)\n     x.requires_grad_(True)\n+    quantiles = [0.5, 0.2, 0.8]\n     # utility functions\n     if provider == 'triton':\n         y_fwd = lambda: layer_norm(x, w_shape, weight, bias, eps)\n@@ -350,13 +351,13 @@ def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='c\n     # forward pass\n     if mode == 'forward':\n         gbps = lambda ms: 2 * x.numel() * x.element_size() / ms * 1e-6\n-        ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, rep=500)\n+        ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, quantiles=quantiles, rep=500)\n     # backward pass\n     if mode == 'backward':\n         gbps = lambda ms: 3 * x.numel() * x.element_size() / ms * 1e-6\n         y = y_fwd()\n         ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True),\n-                                                     grad_to_none=[x], rep=500)\n+                                                     quantiles=quantiles, grad_to_none=[x], rep=500)\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n \n "}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -341,7 +341,7 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n             o = fn()\n             do = torch.randn_like(o)\n             fn = lambda: o.backward(do, retain_graph=True)\n-        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n+        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n         return ms\n     if provider == \"flash\":\n         lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n@@ -353,7 +353,7 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n             o = fn()\n             do = torch.randn_like(o)\n             fn = lambda: o.backward(do, retain_graph=True)\n-        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n+        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n         return ms\n \n "}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 26, "deletions": 26, "changes": 52, "file_content_changes": "@@ -11,7 +11,7 @@\n \n // CHECK-LABEL: matmul_loop\n // There shouldn't be any aliasing with the dot op encoding.\n-func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n   %a_mask = arith.constant dense<true> : tensor<128x32xi1, #AL>\n@@ -32,38 +32,38 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: alloc\n-func.func @alloc(%A : !tt.ptr<f16>) {\n+tt.func @alloc(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n   // CHECK: %0 -> %0\n   %cst2 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: convert\n-func.func @convert(%A : !tt.ptr<f16>) {\n+tt.func @convert(%A : !tt.ptr<f16>) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   // CHECK: %0 -> %0\n   %cst1 = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: trans\n-func.func @trans(%A : !tt.ptr<f16>) {\n+tt.func @trans(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %tensor = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n   // CHECK: %0 -> %cst\n   %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: insert_slice_async\n-func.func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n@@ -72,11 +72,11 @@ func.func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %index = arith.constant 0 : i32\n   // CHECK: %2 -> %cst_0\n   %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: insert_slice\n-func.func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n@@ -86,21 +86,21 @@ func.func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a = tt.load %a_ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #AL>\n   // CHECK: %inserted_slice -> %cst_0\n   %b = tensor.insert_slice %a into %tensor[%index, 0, 0][1, 16, 16][1, 1, 1]: tensor<16x16xf16, #AL> into tensor<1x16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: extract_slice\n-func.func @extract_slice(%A : !tt.ptr<f16>) {\n+tt.func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n   // CHECK-NEXT: %0 -> %cst\n   %cst1 = triton_gpu.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: if_cat\n-func.func @if_cat(%i1 : i1) {\n+tt.func @if_cat(%i1 : i1) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK: %cst_0 -> %cst_0\n@@ -115,11 +115,11 @@ func.func @if_cat(%i1 : i1) {\n     %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield %b : tensor<32x16xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: if_alias\n-func.func @if_alias(%i1 : i1) {\n+tt.func @if_alias(%i1 : i1) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n@@ -130,11 +130,11 @@ func.func @if_alias(%i1 : i1) {\n   } else {\n     scf.yield %cst1 : tensor<16x16xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: for\n-func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n@@ -150,11 +150,11 @@ func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B :\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: for_if\n-func.func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: %cst -> %cst\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n@@ -176,11 +176,11 @@ func.func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n     }\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: for_for_if\n-func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: %cst -> %cst\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n@@ -211,11 +211,11 @@ func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>\n     }\n     scf.yield %a_shared, %b_shared, %c_shared_next : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: cf_for\n-func.func @cf_for(%arg0: index, %arg1: index, %arg2: index, %arg3: !tt.ptr<f16>, %arg4: !tt.ptr<f16>) {\n+tt.func @cf_for(%arg0: index, %arg1: index, %arg2: index, %arg3: !tt.ptr<f16>, %arg4: !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %cst = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n@@ -242,5 +242,5 @@ func.func @cf_for(%arg0: index, %arg1: index, %arg2: index, %arg3: !tt.ptr<f16>,\n   gpu.barrier\n   // CHECK-NEXT: %9 -> %9\n   %9 = tt.cat %0, %0 {axis = 0 : i64} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n-  return\n+  tt.return\n }"}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 38, "deletions": 38, "changes": 76, "file_content_changes": "@@ -1,7 +1,7 @@\n // RUN: triton-opt %s -test-print-alignment -split-input-file -o %t 2>&1 | FileCheck %s\n \n // CHECK-LABEL: @cast\n-func.func @cast() {\n+tt.func @cast() {\n   // CHECK: contiguity = [1], divisibility = [1], constancy = [1], constant_value = 1\n   %cst = arith.constant 1 : i32\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = 1\n@@ -10,13 +10,13 @@ func.func @cast() {\n   %cst_tensor = arith.constant dense<1> : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n   %1 = tt.bitcast %cst_tensor : tensor<128xi32> -> tensor<128xi64>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @add\n-func.func @add() {\n+tt.func @add() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n@@ -27,13 +27,13 @@ func.func @add() {\n   %3 = arith.constant dense<127> : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [128], constant_value = 128\n   %4 = arith.addi %1, %3 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @addptr\n-func.func @addptr(%arg0: !tt.ptr<i1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<i16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}) {\n+tt.func @addptr(%arg0: !tt.ptr<i1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<i16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}) {\n   // CHECK: contiguity = [1], divisibility = [1], constancy = [1], constant_value = 1\n   %cst1 = arith.constant 1 : i32\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n@@ -84,13 +84,13 @@ func.func @addptr(%arg0: !tt.ptr<i1> {tt.divisibility = 16 : i32}, %arg1: !tt.pt\n   %21 = tt.addptr %16, %12 : tensor<128x128x!tt.ptr<i32>>, tensor<128x128xi32>\n   // CHECK-NEXT: contiguity = [1, 128], divisibility = [8, 16], constancy = [128, 1], constant_value = <none>\n   %22 = tt.addptr %17, %12 : tensor<128x128x!tt.ptr<i64>>, tensor<128x128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @sub\n-func.func @sub() {\n+tt.func @sub() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n@@ -101,13 +101,13 @@ func.func @sub() {\n   %3 = arith.constant dense<129> : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [128], constant_value = 128\n   %4 = arith.subi %3, %1 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @mul\n-func.func @mul() {\n+tt.func @mul() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n@@ -122,13 +122,13 @@ func.func @mul() {\n   %5 = arith.constant dense<2> : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [256], constancy = [128], constant_value = 256\n   %6 = arith.muli %4, %5 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @div\n-func.func @div() {\n+tt.func @div() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n@@ -153,14 +153,14 @@ func.func @div() {\n   %10 = tt.make_range {end = 8320 : i32, start = 8192 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [64], constant_value = <none>\n   %11 = arith.divsi %10, %4 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n \n // -----\n \n // CHECK-LABEL: @rem\n-func.func @rem() {\n+tt.func @rem() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n@@ -179,35 +179,35 @@ func.func @rem() {\n   %7 = arith.constant dense<66> : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [2], divisibility = [2], constancy = [1], constant_value = <none>\n   %8 = arith.remui %0, %7 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @broadcast\n-func.func @broadcast() {\n+tt.func @broadcast() {\n   // CHECK: contiguity = [1], divisibility = [64], constancy = [128], constant_value = 64\n   %0 = arith.constant dense<64> : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1, 1], divisibility = [64, 1], constancy = [128, 1], constant_value = 64\n   %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n   // CHECK-NEXT: contiguity = [1, 1], divisibility = [64, 1], constancy = [128, 128], constant_value = 64\n   %2 = tt.broadcast %1 : (tensor<128x1xi32>) -> tensor<128x128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @splat\n-func.func @splat(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n+tt.func @splat(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   // CHECK: contiguity = [1, 1], divisibility = [16, 16], constancy = [128, 128], constant_value = <none>\n   %0 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @cmp\n-func.func @cmp() {\n+tt.func @cmp() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [4611686018427387904], constancy = [128], constant_value = 0\n@@ -226,13 +226,13 @@ func.func @cmp() {\n   %7 = arith.cmpi sgt, %0, %6 : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 0\n   %8 = arith.cmpi sgt, %1, %6 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @logic\n-func.func @logic() {\n+tt.func @logic() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [64], constancy = [128], constant_value = 64\n@@ -255,13 +255,13 @@ func.func @logic() {\n   %9 = arith.ori %2, %4 : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [8], constant_value = <none>\n   %10 = arith.xori %2, %4 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @select\n-func.func @select() {\n+tt.func @select() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [4611686018427387904], constancy = [128], constant_value = 0\n@@ -278,12 +278,12 @@ func.func @select() {\n   %5 = arith.select %4, %3, %7 : tensor<128xi1>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = <none>\n   %8 = \"triton_gpu.select\"(%7, %3, %2) : (tensor<128xi1>, tensor<128xi1>, tensor<128xi1>) -> tensor<128xi1>\n-  return\n+  tt.return\n }\n \n // -----\n \n-func.func @shift() {\n+tt.func @shift() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [8], constancy = [128], constant_value = 8\n@@ -296,12 +296,12 @@ func.func @shift() {\n   %4 = arith.shrsi %0, %2 : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [128], constant_value = 128\n   %5 = arith.shli %1, %2 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n-func.func @max_min() {\n+tt.func @max_min() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [128], divisibility = [64], constancy = [1], constant_value = <none>\n@@ -316,13 +316,13 @@ func.func @max_min() {\n   %5 = arith.constant dense<4> : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = 8\n   %6 = arith.maxsi %4, %5 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @for\n-func.func @for() {\n+tt.func @for() {\n   // CHECK: contiguity = [1, 1], divisibility = [4611686018427387904, 4611686018427387904], constancy = [128, 32], constant_value = 0\n   %a_init = arith.constant dense<0> : tensor<128x32xi32>\n   // CHECK-NEXT: contiguity = [1, 1], divisibility = [1, 1], constancy = [128, 32], constant_value = 1\n@@ -343,13 +343,13 @@ func.func @for() {\n     // CHECK: contiguity = [1, 1], divisibility = [4, 4], constancy = [128, 32], constant_value = 4\n     scf.yield %b, %a, %c : tensor<128x32xi32>, tensor<128x32xi32>, tensor<128x32xi32>\n   }\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @permute_2d\n-func.func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n+tt.func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK: contiguity = [1, 1], divisibility = [1, 1], constancy = [128, 128], constant_value = 1\n   %cst = arith.constant dense<true> : tensor<128x128xi1>\n   // CHECK-NEXT: contiguity = [1, 1], divisibility = [1, 1], constancy = [1, 1], constant_value = <none>\n@@ -397,7 +397,7 @@ func.func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i\n   // CHECK-NEXT: contiguity = [1, 1], divisibility = [1, 1], constancy = [1, 1], constant_value = <none>\n   %20 = tt.load %10, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf32>\n   tt.store %19, %20, %cst : tensor<128x128xf32>\n-  return\n+  tt.return\n }\n \n // -----\n@@ -406,7 +406,7 @@ module {\n \n // This is a tiny test for verifying StoreOp-related alignment, It simply store a constant to a buffer.\n // CHECK-LABEL: @store_constant_align\n-func.func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n: i32 {tt.divisibility = 16 : i32}) {\n+tt.func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n   %pid = tt.get_program_id {axis = 0 : i32} : i32\n   // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [1], constant_value = 128\n@@ -430,7 +430,7 @@ func.func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n   %cst = arith.constant dense<0.0> : tensor<128xf32>\n   tt.store %5, %cst, %mask : tensor<128xf32>\n-  return\n+  tt.return\n }\n \n }\n@@ -440,7 +440,7 @@ func.func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}\n // This IR is dumped from vecadd test.\n // Note, the hint {tt.divisibility = 16 : i32} for %n_elements affects the alignment of mask.\n // CHECK-LABEL: @vecadd_mask_align_16\n-func.func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32 {tt.divisibility = 16 : i32}) {\n+tt.func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32 {tt.divisibility = 16 : i32}) {\n   %c64_i32 = arith.constant 64 : i32\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n   %1 = arith.muli %0, %c64_i32 : i32\n@@ -461,15 +461,15 @@ func.func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}\n   // CHECK: tt.addptr %{{.*}} => contiguity = [64], divisibility = [16], constancy = [1], constant_value = <none>\n   %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   tt.store %15, %13, %mask : tensor<64xf32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // This IR is dumped from vecadd test.\n // Note, there is no divisibility hint for %n_elements, Triton should assume its divisibility to be 1 by default.\n // CHECK-LABEL: @vecadd_mask_align_1\n-func.func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n+tt.func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n   %c64_i32 = arith.constant 64 : i32\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n   %1 = arith.muli %0, %c64_i32 : i32\n@@ -489,5 +489,5 @@ func.func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n   %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   tt.store %15, %13, %10 : tensor<64xf32>\n-  return\n+  tt.return\n }"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 79, "deletions": 32, "changes": 111, "file_content_changes": "@@ -13,7 +13,7 @@\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // CHECK-LABEL: matmul_loop\n-func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n \n@@ -40,13 +40,13 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 4608\n }\n \n // Shared memory is available after a tensor's liveness range ends\n // CHECK-LABEL: reusable\n-func.func @reusable(%A : !tt.ptr<f16>) {\n+tt.func @reusable(%A : !tt.ptr<f16>) {\n   %cst1 = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n   %cst3 = arith.constant dense<true> : tensor<32x128xi1, #AL>\n@@ -69,7 +69,7 @@ func.func @reusable(%A : !tt.ptr<f16>) {\n   // CHECK-NEXT: offset = 0, size = 1152\n   %a4 = triton_gpu.convert_layout %a4_ : (tensor<32x128xf16, #AL>) -> tensor<32x128xf16, #B_DOT>\n   %c1 = tt.dot %a3, %a4, %c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 4608\n }\n \n@@ -78,7 +78,7 @@ func.func @reusable(%A : !tt.ptr<f16>) {\n // %cst1->%cst4\n // %cst3->%g->%h->%i\n // CHECK-LABEL: preallocate\n-func.func @preallocate(%A : !tt.ptr<f16>) {\n+tt.func @preallocate(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 512\n@@ -107,13 +107,13 @@ func.func @preallocate(%A : !tt.ptr<f16>) {\n   %h = tt.cat %d, %cst5 {axis = 0} : (tensor<64x16xf16, #A_SHARED>, tensor<64x16xf16, #A_SHARED>) -> tensor<128x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 2048, size = 4096\n   %i = tt.cat %f, %cst5 {axis = 0} : (tensor<64x16xf16, #A_SHARED>, tensor<64x16xf16, #A_SHARED>) -> tensor<128x16xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 12288\n }\n \n // Unused tensors are immediately released\n // CHECK-LABEL: unused\n-func.func @unused(%A : !tt.ptr<f16>) {\n+tt.func @unused(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 1024\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 0, size = 512\n@@ -122,13 +122,13 @@ func.func @unused(%A : !tt.ptr<f16>) {\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 1024\n   %a = tt.cat %cst1, %cst2 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK: size = 2048\n }\n \n // cst0 is alive through the entire function, it cannot be released before the end of the function\n // CHECK-LABEL: longlive\n-func.func @longlive(%A : !tt.ptr<f16>) {\n+tt.func @longlive(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n@@ -151,65 +151,112 @@ func.func @longlive(%A : !tt.ptr<f16>) {\n   %c = tt.cat %cst3, %cst4 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 1024\n   %d = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 2560\n }\n \n+// This example triggers graph coloring with > 1 colors.\n+// CHECK-LABEL: multi_color\n+tt.func @multi_color(%A : !tt.ptr<f16>) {\n+  // CHECK: offset = 0, size = 64\n+  %cst = arith.constant dense<0.000000e+00> : tensor<4x8xf16, #A_SHARED>\n+  // CHECK-NEXT: offset = 1216, size = 32\n+  %cst_0 = arith.constant dense<0.000000e+00> : tensor<4x4xf16, #A_SHARED>\n+  // CHECK-NEXT: offset = 1248, size = 128\n+  %cst_1 = arith.constant dense<0.000000e+00> : tensor<16x4xf16, #A_SHARED>\n+  %cst_2 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n+  // CHECK-NEXT: scratch offset = 64, size = 1152\n+  %0 = triton_gpu.convert_layout %cst_2 : (tensor<16x32xf16, #AL>) -> tensor<16x32xf16, #AL>\n+  %1 = triton_gpu.convert_layout %cst : (tensor<4x8xf16, #A_SHARED>) -> tensor<4x8xf16, #AL>\n+  // CHECK-NEXT: offset = 0, size = 128\n+  %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x16xf16, #A_SHARED>\n+  %2 = triton_gpu.convert_layout %cst_0 : (tensor<4x4xf16, #A_SHARED>) -> tensor<4x4xf16, #AL>\n+  // CHECK-NEXT: scratch offset = 0, size = 1152\n+  %3 = triton_gpu.convert_layout %cst_2 : (tensor<16x32xf16, #AL>) -> tensor<16x32xf16, #AL>\n+  // CHECK-NEXT: offset = 0, size = 256\n+  %cst_4 = arith.constant dense<0.000000e+00> : tensor<4x32xf16, #A_SHARED>\n+  // CHECK-NEXT: offset = 256, size = 64\n+  %cst_5 = arith.constant dense<0.000000e+00> : tensor<4x8xf16, #A_SHARED>\n+  %4 = triton_gpu.convert_layout %cst_5 : (tensor<4x8xf16, #A_SHARED>) -> tensor<4x8xf16, #AL>\n+  %5 = triton_gpu.convert_layout %cst_5 : (tensor<4x8xf16, #A_SHARED>) -> tensor<4x8xf16, #AL>\n+  // CHECK-NEXT: offset = 256, size = 512\n+  %cst_6 = arith.constant dense<0.000000e+00> : tensor<8x32xf16, #A_SHARED>\n+  // CHECK-NEXT: offset = 2528, size = 128\n+  %cst_7 = arith.constant dense<0.000000e+00> : tensor<2x32xf16, #A_SHARED>\n+  %6 = triton_gpu.convert_layout %cst_0 : (tensor<4x4xf16, #A_SHARED>) -> tensor<4x4xf16, #AL>\n+  // CHECK-NEXT: offset = 256, size = 512\n+  %cst_8 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  // CHECK-NEXT: offset = 256, size = 32\n+  %cst_9 = arith.constant dense<0.000000e+00> : tensor<4x4xf16, #A_SHARED>\n+  // CHECK-NEXT: offset = 256, size = 512\n+  %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %7 = triton_gpu.convert_layout %cst_1 : (tensor<16x4xf16, #A_SHARED>) -> tensor<16x4xf16, #AL>\n+  %8 = triton_gpu.convert_layout %cst_4 : (tensor<4x32xf16, #A_SHARED>) -> tensor<4x32xf16, #AL>\n+  // CHECK-NEXT: scratch offset = 0, size = 1152\n+  %9 = triton_gpu.convert_layout %cst_2 : (tensor<16x32xf16, #AL>) -> tensor<16x32xf16, #AL>\n+  %cst_11 = arith.constant dense<0.000000e+00> : tensor<4x4xf16, #AL>\n+  %10 = triton_gpu.convert_layout %cst_7 : (tensor<2x32xf16, #A_SHARED>) -> tensor<2x32xf16, #AL>\n+  %cst_12 = arith.constant dense<0.000000e+00> : tensor<4x16xf16, #AL>\n+  %cst_13 = arith.constant dense<0.000000e+00> : tensor<8x32xf16, #AL>\n+  // CHECK-NEXT: size = 2656\n+  tt.return\n+}\n+\n // CHECK-LABEL: alloc\n-func.func @alloc(%A : !tt.ptr<f16>) {\n+tt.func @alloc(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n   // CHECK-NEXT: offset = 0, size = 512\n   %cst2 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 512\n }\n \n // CHECK-LABEL: scratch\n-func.func @scratch() {\n+tt.func @scratch() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   // CHECK: scratch offset = 0, size = 512\n   %b = tt.reduce %cst0 {redOp = 1 : i32, axis = 0 : i32} : tensor<16x16xf16, #AL> -> tensor<16xf16, #sliceAd0>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 512\n }\n \n // CHECK-LABEL: trans\n-func.func @trans(%A : !tt.ptr<f16>) {\n+tt.func @trans(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 1024\n   %tensor = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n   %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: insert_slice_async\n-func.func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   // CHECK: offset = 0, size = 512\n   %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n   %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 512\n }\n \n // CHECK-LABEL: extract_slice\n-func.func @extract_slice(%A : !tt.ptr<f16>) {\n+tt.func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n   %cst1 = triton_gpu.extract_slice %cst0[%index, 0, 0][1, 16, 16][1,1,1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 512\n }\n \n // B0 -> (B1) -> B0\n // Memory used by B1 can be reused by B0.\n // CHECK-LABEL: if\n-func.func @if(%i1 : i1) {\n+tt.func @if(%i1 : i1) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n@@ -226,14 +273,14 @@ func.func @if(%i1 : i1) {\n   %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 1024\n   %a = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 2048\n }\n \n // B0 -> (B1) -> (B2) -> B0\n // Memory used by B0 cannot be reused by B1 or B2.\n // CHECK-LABEL: if_else\n-func.func @if_else(%i1 : i1) {\n+tt.func @if_else(%i1 : i1) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n@@ -253,14 +300,14 @@ func.func @if_else(%i1 : i1) {\n   }\n   // CHECK-NEXT: offset = 1024, size = 1024\n   %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 3072\n }\n \n // Block arguments and yields are memory aliases that do not trigger a new\n // allocation.\n // CHECK-LABEL: for\n-func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 8192\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n@@ -270,12 +317,12 @@ func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B :\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 24576\n }\n \n // CHECK-LABEL: for_if_slice\n-func.func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: offset = 0, size = 8192\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n@@ -290,13 +337,13 @@ func.func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f1\n     }\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 24576\n }\n \n // c0 cannot be released in the loop\n // CHECK-LABEL: for_use_ancestor\n-func.func @for_use_ancestor(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @for_use_ancestor(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: offset = 0, size = 8192\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n@@ -309,14 +356,14 @@ func.func @for_use_ancestor(%lb : index, %ub : index, %step : index, %A : !tt.pt\n     %c1 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n     scf.yield %b_shared, %a_shared: tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 32768\n }\n \n // a_shared_init, b_shared_init, and c_shared_init's liveness ranges are span over the entire function before cst2.\n // So they cannot be reused by cst0 and cst1, but can be reused by cst2.\n // CHECK-LABEL: for_for_if\n-func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: offset = 0, size = 8192\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n@@ -340,7 +387,7 @@ func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>\n   }\n   // CHECK-NEXT: offset = 0, size = 8192\n   %cst2 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 40960\n }\n "}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 51, "deletions": 51, "changes": 102, "file_content_changes": "@@ -14,7 +14,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // CHECK-LABEL: matmul_loop\n // There shouldn't be any membar with the dot op encoding.\n-func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n \n@@ -38,11 +38,11 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: raw_single_block\n-func.func @raw_single_block(%A : !tt.ptr<f16>) {\n+tt.func @raw_single_block(%A : !tt.ptr<f16>) {\n   %cst1 = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n   %0 = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n@@ -51,11 +51,11 @@ func.func @raw_single_block(%A : !tt.ptr<f16>) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %3 = triton_gpu.convert_layout %2 : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: war_single_block\n-func.func @war_single_block(%A : !tt.ptr<f16>) {\n+tt.func @war_single_block(%A : !tt.ptr<f16>) {\n   %cst1 = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n   %0 = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n@@ -67,11 +67,11 @@ func.func @war_single_block(%A : !tt.ptr<f16>) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: %4 = triton_gpu.convert_layout\n   %4 = triton_gpu.convert_layout %1 : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: scratch\n-func.func @scratch() {\n+tt.func @scratch() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n@@ -80,11 +80,11 @@ func.func @scratch() {\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %1 = triton_gpu.convert_layout %0 : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n   %2 = tt.reduce %1 {redOp = 1 : i32, axis = 0 : i32} : tensor<32x16xf16, #AL> -> tensor<16xf16, #sliceAd0>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: async_wait\n-func.func @async_wait() {\n+tt.func @async_wait() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n@@ -93,21 +93,21 @@ func.func @async_wait() {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %1 = triton_gpu.convert_layout %0 : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: alloc\n-func.func @alloc() {\n+tt.func @alloc() {\n   %0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n   %1 = tt.cat %0, %0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %2 = triton_gpu.convert_layout %1 : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: extract_slice\n-func.func @extract_slice() {\n+tt.func @extract_slice() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n   %0 = triton_gpu.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n@@ -117,19 +117,19 @@ func.func @extract_slice() {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %2 = triton_gpu.convert_layout %1 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: trans\n-func.func @trans() {\n+tt.func @trans() {\n   // CHECK-NOT: gpu.barrier\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n   %b = tt.trans %cst0 : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: insert_slice_async_op\n-func.func @insert_slice_async_op(%A : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @insert_slice_async_op(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n@@ -142,11 +142,11 @@ func.func @insert_slice_async_op(%A : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %5 = tt.cat %4, %4 {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: insert_slice_op\n-func.func @insert_slice_op(%A : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @insert_slice_op(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n@@ -162,12 +162,12 @@ func.func @insert_slice_op(%A : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %5 = tt.cat %4, %4 {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // If branch inserted a barrier for %cst0 and %cst1, but else didn't, then the barrier should be inserted in the parent region\n // CHECK-LABEL: multi_blocks\n-func.func @multi_blocks(%i1 : i1) {\n+tt.func @multi_blocks(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n@@ -186,12 +186,12 @@ func.func @multi_blocks(%i1 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %2 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // Both branches inserted a barrier for %cst0 and %cst1, then the barrier doesn't need to be inserted in the parent region\n // CHECK-LABEL: multi_blocks_join_barrier\n-func.func @multi_blocks_join_barrier(%i1 : i1) {\n+tt.func @multi_blocks_join_barrier(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n@@ -206,12 +206,12 @@ func.func @multi_blocks_join_barrier(%i1 : i1) {\n     scf.yield\n   }\n   %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n-  return\n+  tt.return\n }\n \n // Read yielded tensor requires a barrier\n // CHECK-LABEL: multi_blocks_yield\n-func.func @multi_blocks_yield(%i1 : i1) {\n+tt.func @multi_blocks_yield(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %a = scf.if %i1 -> (tensor<32x16xf16, #A_SHARED>) {\n@@ -229,12 +229,12 @@ func.func @multi_blocks_yield(%i1 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %4 = tt.cat %a, %a {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // Even though the entry block doesn't have a barrier, the successors should have barriers\n // CHECK-LABEL: multi_blocks_entry_no_shared\n-func.func @multi_blocks_entry_no_shared(%i1 : i1) {\n+tt.func @multi_blocks_entry_no_shared(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   %a = scf.if %i1 -> (tensor<32x16xf16, #A_SHARED>) {\n     // CHECK: gpu.barrier\n@@ -251,12 +251,12 @@ func.func @multi_blocks_entry_no_shared(%i1 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %1 = tt.cat %a, %a {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // Conservatively add a barrier as if the branch (%i1) is never taken\n // CHECK-LABEL: multi_blocks_noelse\n-func.func @multi_blocks_noelse(%i1 : i1) {\n+tt.func @multi_blocks_noelse(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n@@ -268,12 +268,12 @@ func.func @multi_blocks_noelse(%i1 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %1 = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n-  return\n+  tt.return\n }\n \n // Conservatively add a barrier as if the branch (%i2) is never taken\n // CHECK-LABEL: multi_blocks_nested_scf\n-func.func @multi_blocks_nested_scf(%i1 : i1, %i2 : i1) {\n+tt.func @multi_blocks_nested_scf(%i1 : i1, %i2 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n@@ -293,11 +293,11 @@ func.func @multi_blocks_nested_scf(%i1 : i1, %i2 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %2 = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: for\n-func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n@@ -307,13 +307,13 @@ func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B :\n     %5 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n }\n \n // Although a_shared and b_shared are synced before entering the loop,\n // they are reassociated with aliases (c_shared) and thus require a barrier.\n // CHECK-LABEL: for_alias\n-func.func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK: gpu.barrier\n@@ -330,13 +330,13 @@ func.func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>,\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %9 = tt.cat %0, %0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // Although cst2 is not an argument of scf.yield, its memory is reused by cst1.\n // So we need a barrier both before and after cst1\n // CHECK-LABEL: for_reuse\n-func.func @for_reuse(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @for_reuse(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK: gpu.barrier\n@@ -355,11 +355,11 @@ func.func @for_reuse(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>,\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %9 = tt.cat %0, %0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: for_reuse_nested\n-func.func @for_reuse_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @for_reuse_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK: gpu.barrier\n@@ -381,12 +381,12 @@ func.func @for_reuse_nested(%lb : index, %ub : index, %step : index, %A : !tt.pt\n   // CHECK: gpu.barrier\n   // CHECK-NEXT:  tt.cat\n   %15 = tt.cat %0, %0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // repeatedly write to the same shared memory addresses\n // CHECK-LABEL: for_for_if\n-func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n@@ -407,12 +407,12 @@ func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>\n     }\n     scf.yield %a_shared, %b_shared, %c_shared_next : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n }\n \n // c_block_next can either be converted from c_shared_init or c_shared_next_next\n // CHECK-LABEL: for_if_for\n-func.func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n@@ -438,11 +438,11 @@ func.func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>\n     %b_blocked_next = triton_gpu.convert_layout %b_shared: (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL>\n     scf.yield %a_shared, %b_shared, %c_shared_next_next : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: cf_if\n-func.func @cf_if(%i1 : i1) {\n+tt.func @cf_if(%i1 : i1) {\n   %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n   %cst_0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n   cf.cond_br %i1, ^bb1, ^bb2\n@@ -455,10 +455,10 @@ func.func @cf_if(%i1 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %1 = triton_gpu.convert_layout %cst : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<16x16xf16, #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>>\n-  return\n+  tt.return\n }\n \n-func.func @cf_if_else(%i1 : i1) {\n+tt.func @cf_if_else(%i1 : i1) {\n   %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n   %cst_0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n   cf.cond_br %i1, ^bb1, ^bb2\n@@ -479,23 +479,23 @@ func.func @cf_if_else(%i1 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %4 = tt.cat %2, %2 {axis = 0 : i64} : (tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<64x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n-  return\n+  tt.return\n }\n \n-func.func @cf_if_else_return(%i1 : i1) {\n+tt.func @cf_if_else_return(%i1 : i1) {\n   %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n   %cst_0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n   cf.cond_br %i1, ^bb1, ^bb2\n ^bb1:  // pred: ^bb0\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %0 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n-  return\n+  tt.return\n ^bb2:  // pred: ^bb0\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %1 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n-  return\n+  tt.return\n }\n \n }"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -1,6 +1,6 @@\n // RUN: triton-opt %s | FileCheck %s\n \n-func.func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n+tt.func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n   // scalar -> scalar\n   // CHECK:  i64 -> !tt.ptr<f32>\n   %0 = tt.int_to_ptr %scalar_i64 : i64 -> !tt.ptr<f32>\n@@ -32,10 +32,10 @@ func.func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i6\n   %7 = tt.ptr_to_int %tensor_ptr_1d : tensor<16x!tt.ptr<f32>> -> tensor<16xi64>\n   // CHECK: tensor<16xf32> to tensor<16xf16>\n   %8 = arith.truncf %tensor_f32_1d : tensor<16xf32> to tensor<16xf16>\n-  return\n+  tt.return\n }\n \n-func.func @addptr_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_i32: i32) {\n+tt.func @addptr_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_i32: i32) {\n   // scalar -> scalar\n   // CHECK: !tt.ptr<f32>\n   %0 = tt.addptr %scalar_ptr, %scalar_i32 : !tt.ptr<f32>, i32\n@@ -51,10 +51,10 @@ func.func @addptr_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_i32: i32) {\n   %tensor_i32_1d = tt.splat %scalar_i32 : (i32) -> tensor<16xi32>\n   // CHECK: tensor<16x!tt.ptr<f32>>\n   %2 = tt.addptr %tensor_ptr_1d, %tensor_i32_1d : tensor<16x!tt.ptr<f32>>, tensor<16xi32>\n-  return\n+  tt.return\n }\n \n-func.func @load_store_ops_scalar(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %mask : i1) {\n+tt.func @load_store_ops_scalar(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %mask : i1) {\n   // Test if Load/Store ops can handle scalar values\n   %other = arith.constant 0.0e+0 : f32\n \n@@ -73,10 +73,10 @@ func.func @load_store_ops_scalar(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}\n   tt.store %ptr, %b, %mask : f32\n   // CHECK: tt.store %{{.*}}, %[[L2]], %{{.*}} {cache = 1 : i32, evict = 1 : i32} : f32\n   tt.store %ptr, %c, %mask : f32\n-  return\n+  tt.return\n }\n \n-func.func @reduce_ops_infer(%ptr: !tt.ptr<f32>, %v : tensor<1x2x4xf32>) {\n+tt.func @reduce_ops_infer(%ptr: !tt.ptr<f32>, %v : tensor<1x2x4xf32>) {\n   // Test if reduce ops infer types correctly\n \n   // CHECK: %{{.*}} = tt.reduce %{{.*}} -> tensor<2x4xf32>\n@@ -98,10 +98,10 @@ func.func @reduce_ops_infer(%ptr: !tt.ptr<f32>, %v : tensor<1x2x4xf32>) {\n   tt.store %ptr1x2, %c : tensor<1x2xf32>\n   tt.store %ptr1, %e : tensor<1xf32>\n   tt.store %ptr, %g : f32\n-  return\n+  tt.return\n }\n \n-func.func @dot_ops_infer(%ptr: !tt.ptr<f32>, %v : f32) {\n+tt.func @dot_ops_infer(%ptr: !tt.ptr<f32>, %v : f32) {\n   // Test if reduce ops infer types correctly\n   %v128x32 = tt.splat %v : (f32) -> tensor<128x32xf32>\n   %v32x128 = tt.splat %v : (f32) -> tensor<32x128xf32>\n@@ -128,5 +128,5 @@ func.func @dot_ops_infer(%ptr: !tt.ptr<f32>, %v : f32) {\n   tt.store %ptr32x32, %r2 : tensor<32x32xf32>\n   tt.store %ptr128x128, %r3 : tensor<128x128xf32>\n   tt.store %ptr1x1, %r4 : tensor<1x1xf32>\n-  return\n+  tt.return\n }"}, {"filename": "test/Conversion/triton_to_tritongpu.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -1,17 +1,17 @@\n // RUN: triton-opt %s -split-input-file -convert-triton-to-tritongpu=num-warps=2 | FileCheck %s\n \n-func.func @ops() {\n+tt.func @ops() {\n   // CHECK: module attributes {\"triton_gpu.num-warps\" = 2 : i32} {{.*}}\n   %a = arith.constant dense<1.00e+00> : tensor<128x32xf16>\n   %b = arith.constant dense<2.00e+00> : tensor<32x128xf16>\n   %c = arith.constant dense<3.00e+00> : tensor<128x128xf32>\n   %0 = tt.dot %a, %b, %c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16> * tensor<32x128xf16> -> tensor<128x128xf32>\n-  return\n+  tt.return\n }\n \n // -----\n \n-func.func @load_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n+tt.func @load_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   // Test if LoadOp is lowered properly (see #771)\n   %ptrs = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n   %mask = arith.constant dense<true> : tensor<128xi1>\n@@ -25,12 +25,12 @@ func.func @load_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   tt.store %ptrs, %a : tensor<128xf32>\n   tt.store %ptrs, %b : tensor<128xf32>\n   tt.store %ptrs, %c : tensor<128xf32>\n-  return\n+  tt.return\n }\n \n // -----\n \n-func.func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n+tt.func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   // Test if the total number of threadsPerWarp is 32\n   // Test if the total number of warps is 2\n   // CHECK: #[[blocked0:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 2], order = [0, 1]}>\n@@ -49,5 +49,5 @@ func.func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   // CHECK: tensor<16x16xf32, #[[blocked2]]> -> tensor<16xf32, #triton_gpu.slice<{dim = 0, parent = #[[blocked2]]}>>\n   %c3_ = tt.reduce %c2 {redOp = 1 : i32, axis = 0 : i32} : tensor<16x16xf32> -> tensor<16xf32>\n \n-  return\n+  tt.return\n }"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 99, "deletions": 99, "changes": 198, "file_content_changes": "@@ -4,9 +4,9 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: llvm.func @test_empty_kernel(%arg0: i64, %arg1: !llvm.ptr<f16, 1>)\n   // Here the 128 comes from the 4 in module attribute multiples 32\n   // CHECK:  attributes {nvvm.kernel = 1 : ui1, nvvm.maxntid = [128 : i32]} {{.*}}\n-  func.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n+  tt.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n     // CHECK:  llvm.return\n-    return\n+    tt.return\n   }\n } // end module\n \n@@ -15,11 +15,11 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_load\n-  func.func @basic_load(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf32, #blocked0>) {\n+  tt.func @basic_load(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf32, #blocked0>) {\n     // CHECK: llvm.inline_asm\n     // CHECK: llvm.inline_asm\n     %1 = tt.load %a_ptr_init, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -28,13 +28,13 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: vectorized_load\n-  func.func @vectorized_load(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf32, #blocked0>) {\n+  tt.func @vectorized_load(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf32, #blocked0>) {\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ld.global.b32\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ld.global.b32\n     %1 = tt.load %a_ptr_init, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -43,13 +43,13 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: vectorized_load_f16\n-  func.func @vectorized_load_f16(%a_ptr_init: tensor<256x!tt.ptr<f16>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf16, #blocked0>) {\n+  tt.func @vectorized_load_f16(%a_ptr_init: tensor<256x!tt.ptr<f16>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf16, #blocked0>) {\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ld.global.b16\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ld.global.b16\n     %1 = tt.load %a_ptr_init, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf16, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -59,10 +59,10 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: masked_load_const_other\n-  func.func @masked_load_const_other(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>) {\n+  tt.func @masked_load_const_other(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>) {\n     %cst_0 = arith.constant dense<0.000000e+00> : tensor<256xf32, #blocked0>\n     %1 = tt.load %a_ptr_init, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -72,10 +72,10 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: masked_load_const_other_vec\n-  func.func @masked_load_const_other_vec(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>) {\n+  tt.func @masked_load_const_other_vec(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>) {\n     %cst_0 = arith.constant dense<0.000000e+00> : tensor<256xf32, #blocked0>\n     %1 = tt.load %a_ptr_init, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -84,7 +84,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   // CHECK-LABEL: global_load_store_no_vec\n-  func.func @global_load_store_no_vec(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+  tt.func @global_load_store_no_vec(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -127,7 +127,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     tt.store %13, %11 : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -136,7 +136,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   // CHECK-LABEL: global_load_store_vec4\n-  func.func @global_load_store_vec4(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n+  tt.func @global_load_store_vec4(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -163,7 +163,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     // Store 4 elements to global with single one vectorized store instruction\n     // CHECK: @$5 st.global.v4.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n     tt.store %13, %11 : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -173,7 +173,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n #blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n // Note, the %n_elements doesn't have a \"tt.divisibility\" hint, so Triton assumes it's divisibility is 1, this should effect the mask's alignment and further restrict the load/store ops' vector width to be 1.\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n-  func.func @vecadd_masked_vec1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n+  tt.func @vecadd_masked_vec1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n     %c64_i32 = arith.constant 64 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c64_i32 : i32\n@@ -194,7 +194,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n     %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>, #blocked>, tensor<64xi32, #blocked>\n     tt.store %15, %13, %10 : tensor<64xf32, #blocked>\n-    return\n+    tt.return\n   }\n }\n \n@@ -203,7 +203,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: global_load_store_vec2\n-    func.func @global_load_store_vec2(%arg0: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg3: i32) {\n+    tt.func @global_load_store_vec2(%arg0: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -239,7 +239,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: @${{.*}} st.global.v2.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}} };\n     // CHECK: @${{.*}} st.global.v2.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}} };\n     tt.store %13, %11 : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -248,7 +248,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: global_load_store_vec8\n-    func.func @global_load_store_vec8(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n+    tt.func @global_load_store_vec8(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -278,7 +278,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: @$5 st.global.v4.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n     // CHECK: @$5 st.global.v4.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n     tt.store %13, %11 : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -291,7 +291,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_view_broadcast\n-  func.func @basic_view_broadcast(%arg : tensor<256xf32,#blocked0>) {\n+  tt.func @basic_view_broadcast(%arg : tensor<256xf32,#blocked0>) {\n     // CHECK: llvm.mlir.undef\n     // CHECK: %[[T0:.*]] = llvm.extractvalue\n     // CHECK: %[[T1:.*]] = llvm.extractvalue\n@@ -306,7 +306,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK: llvm.insertvalue %[[T0]]\n     // CHECK: llvm.insertvalue %[[T1]]\n     %1 = tt.broadcast %0 : (tensor<256x1xf32,#blocked2>) -> tensor<256x4xf32, #blocked2>\n-    return\n+    tt.return\n   }\n }\n \n@@ -315,13 +315,13 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_make_range\n-  func.func @basic_make_range() {\n+  tt.func @basic_make_range() {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n     // CHECK: llvm.mlir.undef\n     // CHECK: llvm.insertvalue\n     // CHECK: llvm.insertvalue\n     %0 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -330,11 +330,11 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_addf\n-  func.func @basic_addf(%arg0 : tensor<256xf32,#blocked0>, %arg1 : tensor<256xf32,#blocked0>) {\n+  tt.func @basic_addf(%arg0 : tensor<256xf32,#blocked0>, %arg1 : tensor<256xf32,#blocked0>) {\n     // CHECK: llvm.fadd\n     // CHECK: llvm.fadd\n     %1 = arith.addf %arg0, %arg1 : tensor<256xf32,#blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -343,22 +343,22 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_addi\n-  func.func @basic_addi(%arg0 : tensor<256xi32,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n+  tt.func @basic_addi(%arg0 : tensor<256xi32,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n     // CHECK: llvm.add\n     // CHECK: llvm.add\n     %1 = arith.addi %arg0, %arg1 : tensor<256xi32,#blocked0>\n-    return\n+    tt.return\n   }\n }\n \n // -----\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_program_id\n-  func.func @basic_program_id() {\n+  tt.func @basic_program_id() {\n     // CHECK: nvvm.read.ptx.sreg.ctaid.x : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n-    return\n+    tt.return\n   }\n }\n \n@@ -367,11 +367,11 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_addptr\n-  func.func @basic_addptr(%arg0 : tensor<256x!tt.ptr<f32>,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n+  tt.func @basic_addptr(%arg0 : tensor<256x!tt.ptr<f32>,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n     // CHECK: llvm.getelementptr\n     // CHECK: llvm.getelementptr\n     %0 = tt.addptr %arg0, %arg1 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -381,14 +381,14 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem\n   // CHECK-LABEL: basic_alloc_tensor\n-  func.func @basic_alloc_tensor() {\n+  tt.func @basic_alloc_tensor() {\n     // CHECK: llvm.mlir.addressof @global_smem\n     // CHECK-NEXT: llvm.bitcast\n     // CHECK-NEXT: llvm.mlir.constant\n     // CHECK-NEXT: llvm.getelementptr\n     // CHECK-NEXT: llvm.bitcast\n     %0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #shared0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -398,7 +398,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem\n   // CHECK-LABEL: basic_extract_slice\n-  func.func @basic_extract_slice() {\n+  tt.func @basic_extract_slice() {\n     // CHECK: llvm.mlir.addressof @global_smem\n     // CHECK: llvm.extractvalue\n     // CHECK-NEXT: llvm.extractvalue\n@@ -423,18 +423,18 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %index = arith.constant 1 : i32\n     %0 = triton_gpu.alloc_tensor : tensor<128x16x32xf32, #shared0>\n     %1 = triton_gpu.extract_slice %0[%index, 0, 0][1, 16, 32][1, 1, 1] : tensor<128x16x32xf32, #shared0> to tensor<16x32xf32, #shared0>\n-    return\n+    tt.return\n   }\n }\n \n // -----\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_async_wait\n-  func.func @basic_async_wait() {\n+  tt.func @basic_async_wait() {\n     // CHECK: cp.async.wait_group 0x4\n     triton_gpu.async_wait {num = 4: i32}\n-    return\n+    tt.return\n   }\n }\n \n@@ -450,7 +450,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #A = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_fallback\n-  func.func @basic_insert_slice_async_fallback(%arg0: !tt.ptr<f16> {tt.divisibility = 1 : i32}) {\n+  tt.func @basic_insert_slice_async_fallback(%arg0: !tt.ptr<f16> {tt.divisibility = 1 : i32}) {\n     %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<64xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #slice2d1>) -> tensor<16x1xi32, #block2>\n@@ -473,7 +473,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<8xi32>, 3>\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x64x!tt.ptr<f16>, #AL> -> tensor<2x16x64xf16, #A>\n-    return\n+    tt.return\n   }\n }\n \n@@ -489,7 +489,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #A = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_v4\n-  func.func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 32 : i32}) {\n+  tt.func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 32 : i32}) {\n     %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<64xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #slice2d1>) -> tensor<16x1xi32, #block2>\n@@ -515,7 +515,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x64x!tt.ptr<f32>, #AL> -> tensor<2x16x64xf32, #A>\n     triton_gpu.async_commit_group\n-    return\n+    tt.return\n   }\n }\n \n@@ -531,7 +531,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #A = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_v1\n-  func.func @basic_insert_slice_async_v1(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n+  tt.func @basic_insert_slice_async_v1(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n     %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #slice2d1>) -> tensor<16x1xi32, #block2>\n@@ -561,7 +561,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x32x!tt.ptr<f32>, #AL> -> tensor<2x16x32xf32, #A>\n     triton_gpu.async_commit_group\n-    return\n+    tt.return\n   }\n }\n \n@@ -576,7 +576,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #A = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_v1_multictas\n-  func.func @basic_insert_slice_async_v1_multictas(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n+  tt.func @basic_insert_slice_async_v1_multictas(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n     %off0_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<32xi32, #slice2d1>) -> tensor<32x1xi32, #block2>\n@@ -618,7 +618,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32x!tt.ptr<f32>, #AL> -> tensor<2x32x32xf32, #A>\n     triton_gpu.async_commit_group\n-    return\n+    tt.return\n   }\n }\n \n@@ -627,12 +627,12 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: basic_splat\n-  func.func @basic_splat(%ptr: !tt.ptr<f32>) {\n+  tt.func @basic_splat(%ptr: !tt.ptr<f32>) {\n     // CHECK: llvm.mlir.undef\n     // CHECK: llvm.insertvalue\n     // CHECK: llvm.insertvalue\n     %0 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>,#blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -641,13 +641,13 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_store\n-  func.func @basic_store(%ptrs: tensor<256x!tt.ptr<f32>, #blocked0>, %vals: tensor<256xf32, #blocked0>, %mask: tensor<256xi1, #blocked0>) {\n+  tt.func @basic_store(%ptrs: tensor<256x!tt.ptr<f32>, #blocked0>, %vals: tensor<256xf32, #blocked0>, %mask: tensor<256xi1, #blocked0>) {\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     tt.store %ptrs, %vals, %mask : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -658,7 +658,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked\n-  func.func @convert_layout_blocked_blocked(%arg0: tensor<16x16xf32, #blocked0>) {\n+  tt.func @convert_layout_blocked_blocked(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n@@ -694,7 +694,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #blocked1>\n-    return\n+    tt.return\n   }\n }\n \n@@ -705,7 +705,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked_vec\n-  func.func @convert_layout_blocked_blocked_vec(%arg0: tensor<16x16xf32, #blocked0>) {\n+  tt.func @convert_layout_blocked_blocked_vec(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n@@ -717,7 +717,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #blocked1>\n-    return\n+    tt.return\n   }\n }\n \n@@ -728,7 +728,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked_multi_rep\n-  func.func @convert_layout_blocked_blocked_multi_rep(%arg0: tensor<16x16xf32, #blocked0>) {\n+  tt.func @convert_layout_blocked_blocked_multi_rep(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n@@ -746,7 +746,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #blocked1>\n-    return\n+    tt.return\n   }\n }\n \n@@ -759,7 +759,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_dot\n-  func.func @convert_dot(%A: tensor<16x16xf16, #blocked0>, %B: tensor<16x16xf16, #blocked0>) {\n+  tt.func @convert_dot(%A: tensor<16x16xf16, #blocked0>, %B: tensor<16x16xf16, #blocked0>) {\n     %AA = triton_gpu.convert_layout %A : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #shared0>\n     %BB = triton_gpu.convert_layout %B : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #shared0>\n     // CHECK: llvm.inline_asm\n@@ -776,14 +776,14 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK-SAME: mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\n     %D = tt.dot %AA_DOT, %BB_DOT, %cst0 {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n \n-    return\n+    tt.return\n   }\n }\n \n // TODO: problems in MLIR's parser on slice layout\n // #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n // module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-//   func.func @make_range_sliced_layout() {\n+//   tt.func @make_range_sliced_layout() {\n //     %0 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked0}>>\n //     return\n //   }\n@@ -796,7 +796,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mmav2_block\n-  func.func @convert_layout_mmav2_blocked(%arg0: tensor<32x16xf32, #mma>) {\n+  tt.func @convert_layout_mmav2_blocked(%arg0: tensor<32x16xf32, #mma>) {\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n     // CHECK: llvm.store\n@@ -805,7 +805,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<32x16xf32, #mma>) -> tensor<32x16xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -816,7 +816,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mmav1_block\n-  func.func @convert_layout_mmav1_blocked(%arg0: tensor<32x64xf32, #mma>) {\n+  tt.func @convert_layout_mmav1_blocked(%arg0: tensor<32x64xf32, #mma>) {\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n     // CHECK: llvm.store\n@@ -829,7 +829,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<32x64xf32, #mma>) -> tensor<32x64xf32, #blocked>\n-    return\n+    tt.return\n   }\n }\n \n@@ -839,13 +839,13 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_shared\n-  func.func @convert_layout_blocked_shared(%arg0: tensor<128x32xf32, #blocked0>) {\n+  tt.func @convert_layout_blocked_shared(%arg0: tensor<128x32xf32, #blocked0>) {\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<8xf32>, 3>\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<8xf32>, 3>\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -855,10 +855,10 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_blocked1d_to_slice0\n-  func.func @convert_blocked1d_to_slice0(%src:tensor<32xi32, #blocked0>) {\n+  tt.func @convert_blocked1d_to_slice0(%src:tensor<32xi32, #blocked0>) {\n     // CHECK-COUNT-4: llvm.load {{.*}} : !llvm.ptr<vector<1xi32>, 3>\n     %cvt = triton_gpu.convert_layout %src : (tensor<32xi32, #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>\n-    return\n+    tt.return\n   }\n }\n \n@@ -868,10 +868,10 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_blocked1d_to_slice1\n-  func.func @convert_blocked1d_to_slice1(%src:tensor<32xi32, #blocked0>) {\n+  tt.func @convert_blocked1d_to_slice1(%src:tensor<32xi32, #blocked0>) {\n     // CHECK-COUNT-32: llvm.load {{.*}} : !llvm.ptr<vector<1xi32>, 3>\n     %cvt = triton_gpu.convert_layout %src : (tensor<32xi32, #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n-    return\n+    tt.return\n   }\n }\n \n@@ -881,14 +881,14 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_blocked_to_blocked_ptr\n-  func.func @convert_blocked_to_blocked_ptr(%src:tensor<32x!tt.ptr<f32>, #blocked0>) {\n+  tt.func @convert_blocked_to_blocked_ptr(%src:tensor<32x!tt.ptr<f32>, #blocked0>) {\n     // CHECK: llvm.ptrtoint\n     // CHECK: llvm.store\n     // CHECK: nvvm.barrier0\n     // CHECK: llvm.inttoptr\n     // CHECK-COUNT-4: llvm.insertvalue\n     %cvt = triton_gpu.convert_layout %src : (tensor<32x!tt.ptr<f32>, #blocked0>) -> tensor<32x!tt.ptr<f32>, #blocked1>\n-    return\n+    tt.return\n   }\n }\n \n@@ -900,7 +900,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func.func @matmul_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  tt.func @matmul_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<128x32xf16, #shared>, %b:tensor<32x256xf16, #shared>) {\n     %cst = arith.constant dense<0.000000e+00> : tensor<128x256xf32, #mma>\n     // CHECK: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n@@ -913,7 +913,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #blocked>\n     %36 = tt.broadcast %30 : (tensor<128x1x!tt.ptr<f32>, #blocked>) -> tensor<128x256x!tt.ptr<f32>, #blocked>\n     tt.store %36, %38 : tensor<128x256xf32, #blocked>\n-    return\n+    tt.return\n   }\n }\n \n@@ -926,7 +926,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma, isMMAv1Row=true}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma, isMMAv1Row=true}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func.func @matmul884_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  tt.func @matmul884_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<32x64xf16, #shared0>, %b:tensor<64x64xf16, #shared1>) {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x64xf32, #mma>\n     // CHECK: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n@@ -938,7 +938,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n     %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x64x!tt.ptr<f32>, #blocked>\n     tt.store %36, %38 : tensor<32x64xf32, #blocked>\n-    return\n+    tt.return\n   }\n }\n \n@@ -949,7 +949,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#blocked}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#blocked}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func.func @matmul_fmadot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  tt.func @matmul_fmadot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #blocked>\n     // CHECK: llvm.intr.fmuladd\n@@ -960,7 +960,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n     %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n     tt.store %36, %28 : tensor<32x32xf32, #blocked>\n-    return\n+    tt.return\n   }\n }\n \n@@ -973,7 +973,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: matmul_tf32dot\n-  func.func @matmul_tf32dot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  tt.func @matmul_tf32dot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n     // CHECK: llvm.inline_asm\n@@ -999,7 +999,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n     %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n     tt.store %36, %38 : tensor<32x32xf32, #blocked>\n-    return\n+    tt.return\n   }\n }\n \n@@ -1008,27 +1008,27 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32\n-  func.func @atomic_add_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xi1, #blocked0>, %arg2 : tensor<256xf32, #blocked0>) {\n+  tt.func @atomic_add_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xi1, #blocked0>, %arg2 : tensor<256xf32, #blocked0>) {\n     // CHECK: llvm.icmp \"slt\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$3 atom.global.gpu.add.f32\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$3 atom.global.gpu.add.f32\n     %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xf32, #blocked0>, tensor<256xi1, #blocked0>) -> tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n // -----\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32_scalar\n-  func.func @atomic_add_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : i1, %arg2 : f32) {\n+  tt.func @atomic_add_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : i1, %arg2 : f32) {\n     // CHECK: llvm.icmp \"eq\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$3 atom.global.gpu.add.f32\n     %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (!tt.ptr<f32>, f32, i1) -> f32\n-    return\n+    tt.return\n   }\n }\n \n@@ -1037,27 +1037,27 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: store_f32\n-  func.func @store_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xf32, #blocked0>) {\n+  tt.func @store_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xf32, #blocked0>) {\n     // CHECK: llvm.icmp \"slt\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$2 st.global.b32\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$2 st.global.b32\n     tt.store %arg0, %arg1 : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n // -----\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: store_f32_scalar\n-  func.func @store_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : f32) {\n+  tt.func @store_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : f32) {\n     // CHECK: llvm.icmp \"slt\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$2 st.global.b32\n     tt.store %arg0, %arg1 : f32\n-    return\n+    tt.return\n   }\n }\n \n@@ -1066,7 +1066,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n-func.func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n+tt.func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n   %blockidx = tt.get_program_id {axis=0:i32} : i32\n   %blockidy = tt.get_program_id {axis=1:i32} : i32\n   %blockidz = tt.get_program_id {axis=2:i32} : i32\n@@ -1078,15 +1078,15 @@ func.func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n   %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n   tt.store %a, %0 : tensor<32xi32, #blocked0>\n \n-  return\n+  tt.return\n }\n \n }\n \n // -----\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func.func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n+  tt.func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n     // CHECK: nvvm.read.ptx.sreg.nctaid.x\n     // CHECK: nvvm.read.ptx.sreg.nctaid.y\n     // CHECK: nvvm.read.ptx.sreg.nctaid.z\n@@ -1098,20 +1098,20 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n     tt.store %a, %0 : tensor<32xi32, #blocked0>\n \n-    return\n+    tt.return\n   }\n }\n \n // -----\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: test_index_cache\n-  func.func @test_index_cache() {\n+  tt.func @test_index_cache() {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n     %0 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n     // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n     %1 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -1120,12 +1120,12 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: test_base_index_cache\n-  func.func @test_base_index_cache(%arg0: tensor<128x32xf32, #blocked0>) {\n+  tt.func @test_base_index_cache(%arg0: tensor<128x32xf32, #blocked0>) {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n     // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n     %1 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -1134,7 +1134,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: test_index_cache_different_block\n-  func.func @test_index_cache_different_block(%arg0: tensor<128x32xf32, #blocked0>, %arg1: i1) {\n+  tt.func @test_index_cache_different_block(%arg0: tensor<128x32xf32, #blocked0>, %arg1: i1) {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n     // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n@@ -1143,6 +1143,6 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n       %1 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n       cf.br ^bb2\n     ^bb2:  // 2 preds: ^bb0, ^bb1\n-      return\n+      tt.return\n   }\n }"}, {"filename": "test/Target/tritongpu_to_llvmir.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -8,9 +8,9 @@\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n-func.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n+tt.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n \n-  return\n+  tt.return\n }\n \n }"}, {"filename": "test/Target/tritongpu_to_ptx.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -6,9 +6,9 @@\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n-func.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n+tt.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n \n-  return\n+  tt.return\n }\n \n }"}, {"filename": "test/Triton/combine.mlir", "status": "modified", "additions": 30, "deletions": 30, "changes": 60, "file_content_changes": "@@ -2,7 +2,7 @@\n // RUN: triton-opt %s -split-input-file -canonicalize -triton-combine | FileCheck %s\n \n // CHECK-LABEL: @test_combine_dot_add_pattern\n-func.func @test_combine_dot_add_pattern() -> (tensor<128x128xf32>, tensor<128x128xf32>) {\n+tt.func @test_combine_dot_add_pattern() -> (tensor<128x128xf32>, tensor<128x128xf32>) {\n     // CHECK-DAG: %[[d:.*]] = arith.constant dense<3.000000e+00> : tensor<128x128xf32>\n     // CHECK-DAG: %[[b:.*]] = arith.constant dense<2.000000e+00> : tensor<128x128xf32>\n     // CHECK-DAG: %[[a:.*]] = arith.constant dense<1.000000e+00> : tensor<128x128xf32>\n@@ -19,12 +19,12 @@ func.func @test_combine_dot_add_pattern() -> (tensor<128x128xf32>, tensor<128x12\n     // CHECK-NEXT: %[[res1:.*]] = tt.dot %[[a]], %[[b]], %[[d]] {allowTF32 = true} : tensor<128x128xf32> * tensor<128x128xf32> -> tensor<128x128xf32>\n     %res1 = arith.addf %d, %dot_out : tensor<128x128xf32>\n \n-    return %res0, %res1 : tensor<128x128xf32>, tensor<128x128xf32>\n+    tt.return %res0, %res1 : tensor<128x128xf32>, tensor<128x128xf32>\n }\n \n \n // COM: CHECK-LABEL: @test_combine_addptr_pattern\n-func.func @test_combine_addptr_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>> {\n+tt.func @test_combine_addptr_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>> {\n     %off0 = arith.constant 10 : i32\n     %off1 = arith.constant 15 : i32\n \n@@ -42,12 +42,12 @@ func.func @test_combine_addptr_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<\n     %ptr0 = tt.addptr %base_, %idx0 : tensor<8x!tt.ptr<f32>>, tensor<8xi32>\n     %ptr1 = tt.addptr %ptr0, %idx1 : tensor<8x!tt.ptr<f32>>, tensor<8xi32>\n \n-    return %ptr1 : tensor<8x!tt.ptr<f32>>\n+    tt.return %ptr1 : tensor<8x!tt.ptr<f32>>\n }\n \n \n // CHECK-LABEL: @test_combine_select_masked_load_pattern\n-func.func @test_combine_select_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %cond: i1) -> (tensor<8xf32>, tensor<8xf32>) {\n+tt.func @test_combine_select_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %cond: i1) -> (tensor<8xf32>, tensor<8xf32>) {\n     %mask = tt.broadcast %cond : (i1) -> tensor<8xi1>\n     %false_val = arith.constant dense<0.0> : tensor<8xf32>\n \n@@ -59,12 +59,12 @@ func.func @test_combine_select_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>,\n     %y = tt.load %ptr, %mask, %false_val {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n     %1 = arith.select %cond, %y, %false_val : tensor<8xf32>\n \n-    // CHECK: return %[[res1]], %[[res2]] : tensor<8xf32>, tensor<8xf32>\n-    return %0, %1 : tensor<8xf32>, tensor<8xf32>\n+    // CHECK: tt.return %[[res1]], %[[res2]] : tensor<8xf32>, tensor<8xf32>\n+    tt.return %0, %1 : tensor<8xf32>, tensor<8xf32>\n }\n \n // CHECK-LABEL: @test_combine_select_masked_load_fail_pattern\n-func.func @test_combine_select_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %dummy_load: tensor<8xf32>, %dummy_broadcast: tensor<8xi1>, %cond0: i1, %cond1: i1) -> (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) {\n+tt.func @test_combine_select_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %dummy_load: tensor<8xf32>, %dummy_broadcast: tensor<8xi1>, %cond0: i1, %cond1: i1) -> (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) {\n     %false_val = arith.constant dense<0.0> : tensor<8xf32>\n \n     // Case 1: value at the \"load\" position is not an \"op\".  Select should not be canonicalized.\n@@ -82,21 +82,21 @@ func.func @test_combine_select_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f\n     // CHECK: %{{.*}} = arith.select %{{.*}}, %{{.*}}, %{{.*}} : tensor<8xf32>\n     %2 = arith.select %cond1, %real_load1, %false_val : tensor<8xf32>\n \n-    return %0, %1, %2 : tensor<8xf32>, tensor<8xf32>, tensor<8xf32>\n+    tt.return %0, %1, %2 : tensor<8xf32>, tensor<8xf32>, tensor<8xf32>\n }\n \n // CHECK-LABEL: @test_combine_broadcast_constant_pattern\n-func.func @test_combine_broadcast_constant_pattern(%cst : f32) -> tensor<8x2xf32> {\n+tt.func @test_combine_broadcast_constant_pattern(%cst : f32) -> tensor<8x2xf32> {\n     // CHECK: %[[cst:.*]] = arith.constant dense<1.000000e+00> : tensor<8x2xf32>\n     %const = arith.constant dense<1.0> : tensor<8xf32>\n     %bst_out = tt.broadcast %const : (tensor<8xf32>) -> tensor<8x2xf32>\n \n-    // CHECK-NEXT: return %[[cst]] : tensor<8x2xf32>\n-    return %bst_out : tensor<8x2xf32>\n+    // CHECK-NEXT: tt.return %[[cst]] : tensor<8x2xf32>\n+    tt.return %bst_out : tensor<8x2xf32>\n }\n \n // CHECK-LABEL: @test_canonicalize_masked_load_pattern\n-func.func @test_canonicalize_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>) -> (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) {\n+tt.func @test_canonicalize_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>) -> (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) {\n     %true_mask = arith.constant dense<true> : tensor<8xi1>\n     %false_mask = arith.constant dense<false> : tensor<8xi1>\n     %other_val = arith.constant dense<0.0> : tensor<8xf32>\n@@ -112,12 +112,12 @@ func.func @test_canonicalize_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>) -\n     // false_mask with other. It should become \"other\" (i.e., %y)\n     %z = tt.load %ptr, %false_mask, %y {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n \n-    // CHECK: return %[[res1]], %[[res2]], %[[res2]] : tensor<8xf32>, tensor<8xf32>, tensor<8xf32>\n-    return %x, %y, %z: tensor<8xf32>, tensor<8xf32>, tensor<8xf32>\n+    // CHECK: tt.return %[[res1]], %[[res2]], %[[res2]] : tensor<8xf32>, tensor<8xf32>, tensor<8xf32>\n+    tt.return %x, %y, %z: tensor<8xf32>, tensor<8xf32>, tensor<8xf32>\n }\n \n // CHECK-LABEL: @test_canonicalize_masked_load_fail_pattern\n-func.func @test_canonicalize_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %mask: tensor<8xi1>) -> (tensor<8xf32>, tensor<8xf32>) {\n+tt.func @test_canonicalize_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %mask: tensor<8xi1>) -> (tensor<8xf32>, tensor<8xf32>) {\n     %other_val = arith.constant dense<0.0> : tensor<8xf32>\n \n     // Case: value at the \"mask\" position is not an \"op\".  Load should not be canonicalized.\n@@ -126,43 +126,43 @@ func.func @test_canonicalize_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32\n     // CHECK: %[[res1:.*]] = tt.load %{{.*}}, %{{.*}}, %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n     %y = tt.load %ptr, %mask, %other_val {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n \n-    return %x, %y: tensor<8xf32>, tensor<8xf32>\n+    tt.return %x, %y: tensor<8xf32>, tensor<8xf32>\n }\n \n // CHECK-LABEL: @test_canonicalize_masked_store_pattern\n-func.func @test_canonicalize_masked_store_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %val: tensor<8xf32>) {\n+tt.func @test_canonicalize_masked_store_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %val: tensor<8xf32>) {\n     %true_mask = arith.constant dense<true> : tensor<8xi1>\n     %false_mask = arith.constant dense<false> : tensor<8xi1>\n \n     // CHECK: tt.store %{{.*}}, %{{.*}} : tensor<8xf32>\n     tt.store %ptr, %val, %true_mask : tensor<8xf32>\n \n     // The following store should disappear.\n-    // CHECK-NEXT: return\n+    // CHECK-NEXT: tt.return\n     tt.store %ptr, %val, %false_mask : tensor<8xf32>\n-    return\n+    tt.return\n }\n \n // CHECK-LABEL: @test_canonicalize_masked_store_fail_pattern\n-func.func @test_canonicalize_masked_store_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %val: tensor<8xf32>, %mask: tensor<8xi1>) {\n+tt.func @test_canonicalize_masked_store_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %val: tensor<8xf32>, %mask: tensor<8xi1>) {\n     // Case: value at the \"mask\" position is not an \"op\".  Store should not be canonicalized.\n     // CHECK: tt.store %{{.*}}, %{{.*}}, %{{.*}} : tensor<8xf32>\n     tt.store %ptr, %val, %mask : tensor<8xf32>\n-    return\n+    tt.return\n }\n \n // CHECK-LABEL: @test_canonicalize_expand_dims\n-func.func @test_canonicalize_expand_dims(%arg0: tensor<f32>) -> (tensor<1x8xf32>) {\n+tt.func @test_canonicalize_expand_dims(%arg0: tensor<f32>) -> (tensor<1x8xf32>) {\n     %splat = tt.splat %arg0 : (tensor<f32>) -> tensor<8xf32>\n     // CHECK: %{{.*}} = tt.splat %arg0 : (tensor<f32>) -> tensor<1x8xf32>\n     %ed = tt.expand_dims %splat {axis = 0 : i32} : (tensor<8xf32>) -> tensor<1x8xf32>\n \n-    return %ed : tensor<1x8xf32>\n+    tt.return %ed : tensor<1x8xf32>\n }\n \n \n // CHECK-LABEL: @test_canonicalize_view\n-func.func @test_canonicalize_view(%arg0: tensor<8xf32>, %arg1: tensor<f32>) -> (tensor<4x2xf32>, tensor<2x2x2xf32>, tensor<8xf32>) {\n+tt.func @test_canonicalize_view(%arg0: tensor<8xf32>, %arg1: tensor<f32>) -> (tensor<4x2xf32>, tensor<2x2x2xf32>, tensor<8xf32>) {\n     %view0 = tt.view %arg0 : (tensor<8xf32>) -> tensor<2x4xf32>\n     // CHECK: %{{.*}} = tt.view %arg0 : (tensor<8xf32>) -> tensor<4x2xf32>\n     %view1 = tt.view %view0 : (tensor<2x4xf32>) -> tensor<4x2xf32>\n@@ -175,11 +175,11 @@ func.func @test_canonicalize_view(%arg0: tensor<8xf32>, %arg1: tensor<f32>) -> (\n     // CHECK: %{{.*}} = arith.addf %arg0, %arg0 : tensor<8xf32>\n     %add = arith.addf %view3, %arg0 : tensor<8xf32>\n \n-    return %view1, %view2, %add : tensor<4x2xf32>, tensor<2x2x2xf32>, tensor<8xf32>\n+    tt.return %view1, %view2, %add : tensor<4x2xf32>, tensor<2x2x2xf32>, tensor<8xf32>\n }\n \n // CHECK-LABEL: @test_canonicalize_broadcast\n-func.func @test_canonicalize_broadcast(%arg0: tensor<1x1x8xf32>, %arg1: tensor<f32>) -> (tensor<4x2x8xf32>, tensor<8x8xf32>, tensor<1x1x8xf32>) {\n+tt.func @test_canonicalize_broadcast(%arg0: tensor<1x1x8xf32>, %arg1: tensor<f32>) -> (tensor<4x2x8xf32>, tensor<8x8xf32>, tensor<1x1x8xf32>) {\n     %broadcast0 = tt.broadcast %arg0 : (tensor<1x1x8xf32>) -> tensor<1x2x8xf32>\n     // CHECK: %{{.*}} = tt.broadcast %arg0 : (tensor<1x1x8xf32>) -> tensor<4x2x8xf32>\n     %broadcast1 = tt.broadcast %broadcast0 : (tensor<1x2x8xf32>) -> tensor<4x2x8xf32>\n@@ -192,11 +192,11 @@ func.func @test_canonicalize_broadcast(%arg0: tensor<1x1x8xf32>, %arg1: tensor<f\n     // CHECK: %{{.*}} = arith.addf %arg0, %arg0 : tensor<1x1x8xf32>\n     %add = arith.addf %broadcast3, %arg0 : tensor<1x1x8xf32>\n \n-    return %broadcast1, %broadcast2, %add : tensor<4x2x8xf32>, tensor<8x8xf32>, tensor<1x1x8xf32>\n+    tt.return %broadcast1, %broadcast2, %add : tensor<4x2x8xf32>, tensor<8x8xf32>, tensor<1x1x8xf32>\n }\n \n // CHECK-LABEL: @test_fold_views\n-func.func @test_fold_views() -> (tensor<16x8xf32>, tensor<16x128xf32>, tensor<1x1x128xf32>) {\n+tt.func @test_fold_views() -> (tensor<16x8xf32>, tensor<16x128xf32>, tensor<1x1x128xf32>) {\n     %a = arith.constant dense<1.0> : tensor<1x128xf32>\n \n     // CHECK-DAG: %{{.*}} = arith.constant dense<1.{{.*}}> : tensor<16x8xf32>\n@@ -208,5 +208,5 @@ func.func @test_fold_views() -> (tensor<16x8xf32>, tensor<16x128xf32>, tensor<1x\n     // CHECK-DAG: %{{.*}} = arith.constant dense<1.{{.*}}> : tensor<1x1x128xf32>\n     %d = tt.expand_dims %a {axis = 0: i32} : (tensor<1x128xf32>) -> tensor<1x1x128xf32>\n \n-    return %b, %c, %d : tensor<16x8xf32>, tensor<16x128xf32>, tensor<1x1x128xf32>\n+    tt.return %b, %c, %d : tensor<16x8xf32>, tensor<16x128xf32>, tensor<1x1x128xf32>\n }"}, {"filename": "test/Triton/rewrite-tensor-pointer.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n // RUN: triton-opt %s -triton-rewrite-tensor-pointer | FileCheck %s\n-func.func public @matmul_kernel(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}) {\n+tt.func public @matmul_kernel(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}) {\n   %c31_i32 = arith.constant 31 : i32\n   %c127_i32 = arith.constant 127 : i32\n   %c1 = arith.constant 1 : index\n@@ -79,5 +79,5 @@ func.func public @matmul_kernel(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}\n   %53 = tt.broadcast %51 : (tensor<1x32xi1>) -> tensor<128x32xi1>\n   %54 = arith.andi %52, %53 : tensor<128x32xi1>\n   tt.store %45, %30, %54 {cache = 1 : i32, evict = 1 : i32} : tensor<128x32xf16>\n-  return\n+  tt.return\n }"}, {"filename": "test/Triton/vecadd.mlir", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1,7 +1,7 @@\n // RUN: triton-opt %s -verify-diagnostics\n \n module {\n-  func.func @add_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32__(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32) {\n+  tt.func @add_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32__(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32) {\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %c256_i32 = arith.constant 256 : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -39,11 +39,11 @@ module {\n     %16 = tt.broadcast %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n     %17 = tt.addptr %16, %4 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n     tt.store %17, %15#0, %6 : tensor<256xf32>\n-    return\n+    tt.return\n   }\n }\n // module {\n-//   func.func @add_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32__(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32) {\n+//   tt.func @add_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32__(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32) {\n //     %c64 = arith.constant 64 : index\n //     %c32 = arith.constant 32 : index\n //     %c0 = arith.constant 0 : index\n@@ -125,6 +125,6 @@ module {\n //     %53 = tt.broadcast %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %54 = tt.addptr %53, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     tt.store %54, %52#0, %6 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     return\n+//     tt.return\n //   }\n // }"}, {"filename": "test/TritonGPU/coalesce.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -19,7 +19,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n // CHECK: [[store_val:%.*]] = triton_gpu.convert_layout {{.*}} -> tensor<64x64xf32, [[col_layout]]>\n // CHECK: [[store_mask:%.*]] = triton_gpu.convert_layout {{.*}} -> tensor<64x64xi1, [[col_layout]]>\n // CHECK: tt.store [[store_ptr]], [[store_val]], [[store_mask]]\n-func.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+tt.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n                 %arg1: i32 {tt.divisibility = 16 : i32},\n                 %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n                 %arg3: i32 {tt.divisibility = 16 : i32}) {\n@@ -47,7 +47,7 @@ func.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %18 = tt.addptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n   %19 = tt.load %10, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked1>\n   tt.store %18, %19, %cst : tensor<64x64xf32, #blocked1>\n-  return\n+  tt.return\n }\n \n }"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 107, "deletions": 92, "changes": 199, "file_content_changes": "@@ -9,66 +9,81 @@\n // CHECK: [[$col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK: [[$col_layout_novec:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK-LABEL: cst\n-func.func @cst() -> tensor<1024xi32, #layout1> {\n+tt.func @cst() -> tensor<1024xi32, #layout1> {\n   %cst = arith.constant dense<0> : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %cst : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: return %cst : tensor<1024xi32, [[$target_layout]]>\n-  return %1: tensor<1024xi32, #layout1>\n+  // CHECK: tt.return %cst : tensor<1024xi32, [[$target_layout]]>\n+  tt.return %1: tensor<1024xi32, #layout1>\n }\n \n // CHECK-LABEL: range\n-func.func @range() -> tensor<1024xi32, #layout1> {\n+tt.func @range() -> tensor<1024xi32, #layout1> {\n   %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %0 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: return %0 : tensor<1024xi32, [[$target_layout]]>\n-  return %1: tensor<1024xi32, #layout1>\n+  // CHECK: tt.return %0 : tensor<1024xi32, [[$target_layout]]>\n+  tt.return %1: tensor<1024xi32, #layout1>\n }\n \n // CHECK-LABEL: splat\n-func.func @splat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n+tt.func @splat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   %0 = tt.splat %arg0 : (i32) -> tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %0 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: return %0 : tensor<1024xi32, [[$target_layout]]>\n-  return %1: tensor<1024xi32, #layout1>\n+  // CHECK: tt.return %0 : tensor<1024xi32, [[$target_layout]]>\n+  tt.return %1: tensor<1024xi32, #layout1>\n }\n \n // CHECK-LABEL: remat\n-func.func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n+tt.func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #layout0>\n   %1 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #layout0>\n   %2 = arith.muli %0, %1 : tensor<1024xi32, #layout0>\n   %3 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   %4 = tt.splat %arg0 : (i32) -> tensor<1024xi32, #layout0>\n   %5 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   %6 = arith.addi %3, %5 : tensor<1024xi32, #layout1>\n-  return %6: tensor<1024xi32, #layout1>\n+  tt.return %6: tensor<1024xi32, #layout1>\n   // CHECK: %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>\n   // CHECK: %1 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>\n   // CHECK: %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>\n   // CHECK: %3 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>\n   // CHECK: %4 = arith.muli %0, %2 : tensor<1024xi32, [[$target_layout]]>\n   // CHECK: %5 = arith.muli %1, %3 : tensor<1024xi32, [[$target_layout]]>\n   // CHECK: %6 = arith.addi %4, %5 : tensor<1024xi32, [[$target_layout]]>\n-  // CHECK: return %6 : tensor<1024xi32, [[$target_layout]]>\n+  // CHECK: tt.return %6 : tensor<1024xi32, [[$target_layout]]>\n }\n \n // Always rematerialize single value loads\n // CHECK-LABEL: remat_single_value\n-func.func @remat_single_value(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+tt.func @remat_single_value(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   %0 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<1x!tt.ptr<i32>, #layout1>\n   %1 = tt.load %0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1xi32, #layout1>\n   // CHECK-NOT: triton_gpu.convert_layout\n   %2 = triton_gpu.convert_layout %1 : (tensor<1xi32, #layout1>) -> tensor<1xi32, #layout0>\n   %3 = triton_gpu.convert_layout %0 : (tensor<1x!tt.ptr<i32>, #layout1>) -> tensor<1x!tt.ptr<i32>, #layout0>\n   tt.store %3, %2 : tensor<1xi32, #layout0>\n-  return\n+  tt.return\n+}\n+\n+\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+tt.func @remat_fast_load(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %0 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<16x!tt.ptr<i32>, #layout1>\n+  %1 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #layout1>\n+  %2 = tt.addptr %0, %1 : tensor<16x!tt.ptr<i32>, #layout1>, tensor<16xi32, #layout1>\n+  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16xi32, #layout1>\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %4 = triton_gpu.convert_layout %3 : (tensor<16xi32, #layout1>) -> tensor<16xi32, #layout0>\n+  %5 = triton_gpu.convert_layout %2 : (tensor<16x!tt.ptr<i32>, #layout1>) -> tensor<16x!tt.ptr<i32>, #layout0>\n+  tt.store %5, %4 : tensor<16xi32, #layout0>\n+  tt.return\n+}\n }\n \n // CHECK-LABEL: if\n-func.func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+tt.func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout1>\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n@@ -81,11 +96,11 @@ func.func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n     %6 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout1>) -> tensor<1024xi32, #layout0>\n     tt.store %5, %6 : tensor<1024xi32, #layout0>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: if_convert_else_not\n-func.func @if_convert_else_not(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+tt.func @if_convert_else_not(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n   %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n@@ -102,11 +117,11 @@ func.func @if_convert_else_not(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility\n   }\n   // CHECK-NOT: triton_gpu.convert_layout\n   tt.store %5, %8 : tensor<1024xi32, #layout1>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: if_not_else_convert\n-func.func @if_not_else_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+tt.func @if_not_else_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n   %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n@@ -123,11 +138,11 @@ func.func @if_not_else_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility\n   }\n   // CHECK-NOT: triton_gpu.convert_layout\n   tt.store %5, %8 : tensor<1024xi32, #layout1>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: if_else_both_convert\n-func.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+tt.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n   %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n@@ -146,7 +161,7 @@ func.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility\n   // disabledCHECK: triton_gpu.convert_layout\n   // CHECK-NOT: triton_gpu.convert_layout\n   tt.store %5, %8 : tensor<1024xi32, #layout1>\n-  return\n+  tt.return\n }\n \n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n@@ -158,12 +173,12 @@ func.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility\n #blocked4 = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n \n // CHECK-LABEL: transpose\n-func.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n+tt.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, {{%cst.*}}, {{%cst.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, [[$row_layout]]>\n   // CHECK: [[cvt_val:%.*]] = triton_gpu.convert_layout [[loaded_val]] : (tensor<64x64xf32, [[$row_layout]]>) -> tensor<64x64xf32, [[$col_layout]]>\n   // CHECK: tt.store {{.*}}, [[cvt_val]], {{%cst.*}} : tensor<64x64xf32, [[$col_layout]]>\n-  // CHECK: return\n+  // CHECK: tt.return\n   %cst = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked1>\n   %cst_0 = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n   %00 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #slice1dim1>\n@@ -195,65 +210,65 @@ func.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i3\n   %25 = triton_gpu.convert_layout %23 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked4>\n   %26 = triton_gpu.convert_layout %cst_0 : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked4>\n   tt.store %24, %25, %26 : tensor<64x64xf32, #blocked4>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: loop\n-func.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n-    // CHECK-NOT: triton_gpu.convert_layout\n-    // CHECK: [[loop_ret:%.*]]:2 = scf.for {{.*}} -> (tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>)\n-    // CHECK-NEXT: {{.*}} = tt.load {{.*}} : tensor<64x64xf32, [[$row_layout]]>\n-    // CHECK-NEXT: {{.*}} = arith.addf {{.*}} : tensor<64x64xf32, [[$row_layout]]>\n-    // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>, tensor<64x64xi32, [[$row_layout]]>\n-    // CHECK-NEXT: scf.yield {{.*}} : tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>\n-    // CHECK-NEXT: }\n-    // CHECK-NEXT: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[$row_layout]]>) -> tensor<64x64xf32, [[$col_layout_novec]]>\n-    // CHECK-NOT: triton_gpu.convert_layout\n-    %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n-    %cst_0 = arith.constant dense<64> : tensor<64x64xi32, #blocked1>\n-    %c1 = arith.constant 1 : index\n-    %c32 = arith.constant 32 : index\n-    %c0 = arith.constant 0 : index\n-    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked1>\n-    %00 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #slice1dim1>\n-    %01 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #slice2dim0>\n-    %1 = tt.expand_dims %00 {axis = 1 : i32} : (tensor<64xi32, #slice1dim1>) -> tensor<64x1xi32, #blocked1>\n-    %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n-    %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n-    %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-    %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n-    %6 = tt.expand_dims %01 {axis = 0 : i32} : (tensor<64xi32, #slice2dim0>) -> tensor<1x64xi32, #blocked2>\n-    %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n-    %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n-    %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-    %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n-    %11:2 = scf.for %arg5 = %c0 to %c32 step %c1 iter_args(%arg6 = %cst_1, %arg7 = %10) -> (tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>) {\n-      %23 = triton_gpu.convert_layout %arg7 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n-      %24 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n-      %25 = triton_gpu.convert_layout %cst_1 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n-      %26 = tt.load %23, %24, %25 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked3>\n-      %27 = triton_gpu.convert_layout %26 : (tensor<64x64xf32, #blocked3>) -> tensor<64x64xf32, #blocked1>\n-      %28 = arith.addf %arg6, %27 : tensor<64x64xf32, #blocked1>\n-      %29 = tt.addptr %arg7, %cst_0 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n-      scf.yield %28, %29 : tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>\n-    }\n-    %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-    %13 = tt.addptr %12, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n-    %14 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n-    %15 = arith.muli %6, %14 : tensor<1x64xi32, #blocked2>\n-    %16 = tt.broadcast %13 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n-    %17 = tt.broadcast %15 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n-    %18 = triton_gpu.convert_layout %17 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-    %19 = tt.addptr %16, %18 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n-    %20 = triton_gpu.convert_layout %19 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n-    %21 = triton_gpu.convert_layout %11#0 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked1>\n-    %22 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked1>\n-    tt.store %20, %21, %22 : tensor<64x64xf32, #blocked1>\n-    return\n+tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  // CHECK: [[loop_ret:%.*]]:2 = scf.for {{.*}} -> (tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>)\n+  // CHECK-NEXT: {{.*}} = tt.load {{.*}} : tensor<64x64xf32, [[$row_layout]]>\n+  // CHECK-NEXT: {{.*}} = arith.addf {{.*}} : tensor<64x64xf32, [[$row_layout]]>\n+  // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>, tensor<64x64xi32, [[$row_layout]]>\n+  // CHECK-NEXT: scf.yield {{.*}} : tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>\n+  // CHECK-NEXT: }\n+  // CHECK-NEXT: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[$row_layout]]>) -> tensor<64x64xf32, [[$col_layout_novec]]>\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n+  %cst_0 = arith.constant dense<64> : tensor<64x64xi32, #blocked1>\n+  %c1 = arith.constant 1 : index\n+  %c32 = arith.constant 32 : index\n+  %c0 = arith.constant 0 : index\n+  %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked1>\n+  %00 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #slice1dim1>\n+  %01 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #slice2dim0>\n+  %1 = tt.expand_dims %00 {axis = 1 : i32} : (tensor<64xi32, #slice1dim1>) -> tensor<64x1xi32, #blocked1>\n+  %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n+  %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n+  %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n+  %6 = tt.expand_dims %01 {axis = 0 : i32} : (tensor<64xi32, #slice2dim0>) -> tensor<1x64xi32, #blocked2>\n+  %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n+  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n+  %11:2 = scf.for %arg5 = %c0 to %c32 step %c1 iter_args(%arg6 = %cst_1, %arg7 = %10) -> (tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>) {\n+    %23 = triton_gpu.convert_layout %arg7 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n+    %24 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n+    %25 = triton_gpu.convert_layout %cst_1 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n+    %26 = tt.load %23, %24, %25 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked3>\n+    %27 = triton_gpu.convert_layout %26 : (tensor<64x64xf32, #blocked3>) -> tensor<64x64xf32, #blocked1>\n+    %28 = arith.addf %arg6, %27 : tensor<64x64xf32, #blocked1>\n+    %29 = tt.addptr %arg7, %cst_0 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n+    scf.yield %28, %29 : tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  }\n+  %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %13 = tt.addptr %12, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n+  %14 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n+  %15 = arith.muli %6, %14 : tensor<1x64xi32, #blocked2>\n+  %16 = tt.broadcast %13 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %17 = tt.broadcast %15 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %18 = triton_gpu.convert_layout %17 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n+  %19 = tt.addptr %16, %18 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n+  %20 = triton_gpu.convert_layout %19 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %21 = triton_gpu.convert_layout %11#0 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked1>\n+  %22 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked1>\n+  tt.store %20, %21, %22 : tensor<64x64xf32, #blocked1>\n+  tt.return\n }\n \n // CHECK-LABEL: vecadd\n-func.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n+tt.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %c256_i32 = arith.constant 256 : i32\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n@@ -280,12 +295,12 @@ func.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.p\n   %21 = tt.addptr %19, %20 : tensor<256x!tt.ptr<f32>, #layout1>, tensor<256xi32, #layout1>\n   %22 = triton_gpu.convert_layout %18 : (tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>) -> tensor<256xf32, #layout1>\n   tt.store %21, %22 : tensor<256xf32, #layout1>\n-  return\n+  tt.return\n }\n \n // Select has args with different element types\n // CHECK-LABEL: select\n-func.func @select(%arg0: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}) {\n+tt.func @select(%arg0: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %cst = arith.constant dense<30000> : tensor<1x1xi32, #blocked2>\n   %cst_0 = arith.constant dense<30000> : tensor<1x512xi32, #blocked2>\n@@ -331,12 +346,12 @@ func.func @select(%arg0: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg1: !tt.p\n     tt.store %31, %32, %33 : tensor<1x512xf64, #blocked3>\n     scf.yield %30 : tensor<1x512xf64, #blocked2>\n   }\n-  return\n+  tt.return\n }\n \n // Make sure the following IR doesn't hang the compiler.\n // CHECK-LABEL: long_func\n-func.func public @long_func(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg5: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg7: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg9: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg10: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg11: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg12: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg13: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg14: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg15: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}) {\n+tt.func public @long_func(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg5: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg7: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg9: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg10: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg11: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg12: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg13: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg14: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg15: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}) {\n   %cst = arith.constant dense<1.000000e+00> : tensor<1024xf32, #blocked0>\n   %cst_0 = arith.constant dense<5.000000e-04> : tensor<1024xf32, #blocked0>\n   %cst_1 = arith.constant dense<0.999499976> : tensor<1024xf32, #blocked0>\n@@ -727,13 +742,13 @@ func.func public @long_func(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %a\n   %365 = triton_gpu.convert_layout %364 : (tensor<1024x!tt.ptr<f64>, #blocked0>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n   %366 = triton_gpu.convert_layout %343 : (tensor<1024xf64, #blocked0>) -> tensor<1024xf64, #blocked0>\n   tt.store %365, %366 : tensor<1024xf64, #blocked0>\n-  return\n+  tt.return\n }\n \n // A mnist model from torch inductor.\n // Check if topological sort is working correct and there's no unnecessary convert\n // CHECK-LABEL: mnist\n-func.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: i32) {\n+tt.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %cst = arith.constant dense<10> : tensor<16x1xi32, #blocked2>\n   %cst_0 = arith.constant dense<10> : tensor<1x16xi32, #blocked3>\n@@ -807,7 +822,7 @@ func.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1:\n   %62 = triton_gpu.convert_layout %58 : (tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked4>\n   %63 = triton_gpu.convert_layout %22 : (tensor<16x16xi1, #blocked2>) -> tensor<16x16xi1, #blocked4>\n   tt.store %61, %62, %63 : tensor<16x16xf32, #blocked4>\n-  return\n+  tt.return\n }\n \n // -----\n@@ -820,7 +835,7 @@ func.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1:\n #blocked5 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [8, 1], order = [1, 0]}>\n // cmpf and cmpi have different operands and result types\n // CHECK-LABEL: cmp\n-func.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) {\n+tt.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) {\n   %c64 = arith.constant 64 : index\n   %c2048 = arith.constant 2048 : index\n   %c0 = arith.constant 0 : index\n@@ -953,14 +968,14 @@ func.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !\n     %82 = triton_gpu.convert_layout %54 : (tensor<64x64xi1, #blocked2>) -> tensor<64x64xi1, #blocked4>\n     tt.store %80, %81, %82 : tensor<64x64xf16, #blocked4>\n   }\n-  return\n+  tt.return\n }\n \n // -----\n \n // Just make sure it doesn't crash on non-tensor types.\n // CHECK-LABEL: if_no_tensor\n-func.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}) {\n+tt.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %c-1_i64 = arith.constant -1 : i64\n   %cst = arith.constant 0.000000e+00 : f32\n@@ -981,7 +996,7 @@ func.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %8 = tt.load %5, %7, %cst {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : f32\n   %9 = tt.addptr %arg1, %0 : !tt.ptr<f32>, i32\n   tt.store %9, %8 {cache = 1 : i32, evict = 1 : i32} : f32\n-  return\n+  tt.return\n }\n \n // -----\n@@ -994,7 +1009,7 @@ func.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [2, 1], order = [0, 1]}>\n #blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [16, 2], warpsPerCTA = [2, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n-  func.func public @reduce_cvt1(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg1: i32, %arg2: i32) {\n+  tt.func public @reduce_cvt1(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg1: i32, %arg2: i32) {\n     %cst = arith.constant dense<0> : tensor<1x2xi32, #blocked>\n     %cst_0 = arith.constant dense<2> : tensor<1x2xi32, #blocked>\n     %0 = tt.make_range {end = 2 : i32, start = 0 : i32} : tensor<2xi32, #blocked1>\n@@ -1014,7 +1029,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %14 = triton_gpu.convert_layout %12 : (tensor<1x2xi64, #blocked>) -> tensor<1x2xi64, #blocked3>\n     %15 = triton_gpu.convert_layout %3 : (tensor<1x2xi1, #blocked>) -> tensor<1x2xi1, #blocked3>\n     tt.store %13, %14, %15 {cache = 1 : i32, evict = 1 : i32} : tensor<1x2xi64, #blocked3>\n-    return\n+    tt.return\n   }\n }\n \n@@ -1030,7 +1045,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n #blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func.func public @reduce_cvt2(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}) {\n+  tt.func public @reduce_cvt2(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}) {\n     %cst = arith.constant dense<0.000000e+00> : tensor<1x256xf32, #blocked>\n     %c3136_i32 = arith.constant 3136 : index\n     %c256_i32 = arith.constant 256 : index\n@@ -1089,6 +1104,6 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %25 = triton_gpu.convert_layout %21 : (tensor<1x1xf32, #blocked>) -> tensor<1x1xf32, #blocked>\n     %26 = triton_gpu.convert_layout %7 : (tensor<1x1xi1, #blocked>) -> tensor<1x1xi1, #blocked>\n     tt.store %24, %25, %26 {cache = 1 : i32, evict = 1 : i32} : tensor<1x1xf32, #blocked>\n-    return\n+    tt.return\n   }\n }"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 78, "deletions": 116, "changes": 194, "file_content_changes": "@@ -6,11 +6,12 @@\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #ALs0 = #triton_gpu.slice<{parent=#AL, dim=0}>\n #BLs0 = #triton_gpu.slice<{parent=#BL, dim=0}>\n+#BLs1 = #triton_gpu.slice<{parent=#BL, dim=1}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n-// CHECK: func.func @matmul_loop\n+// CHECK: tt.func @matmul_loop\n // CHECK-DAG: %[[CONSTANT_0:.*]] = arith.constant 0 : i32\n // CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n // CHECK-DAG: %[[CONSTANT_2:.*]] = arith.constant 2 : i32\n@@ -46,7 +47,7 @@\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:   scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n-func.func @matmul_loop(%lb : index, %ub : index, %step : index,\n+tt.func @matmul_loop(%lb : index, %ub : index, %step : index,\n                   %A : !tt.ptr<f16> {tt.divisibility = 16 : i32},\n                   %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) -> tensor<128x128xf32, #C> {\n   // A ptrs\n@@ -87,10 +88,10 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index,\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n-  return %loop#2: tensor<128x128xf32, #C>\n+  tt.return %loop#2: tensor<128x128xf32, #C>\n }\n \n-// CHECK: func.func @matmul_loop_nested\n+// CHECK: tt.func @matmul_loop_nested\n // CHECK-DAG: %[[CONSTANT_0:.*]] = arith.constant 0 : i32\n // CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n // CHECK-DAG: %[[CONSTANT_2:.*]] = arith.constant 2 : i32\n@@ -119,7 +120,7 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index,\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n-func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n+tt.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n                          %A : !tt.ptr<f16> {tt.divisibility = 16 : i32},\n                          %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) -> tensor<128x128xf32, #C>{\n \n@@ -161,11 +162,11 @@ func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n \n     scf.yield %loop2#2 : tensor<128x128xf32, #C>\n   }\n-  return %loop1#0 : tensor<128x128xf32, #C>\n+  tt.return %loop1#0 : tensor<128x128xf32, #C>\n }\n \n \n-// CHECK: func.func @matmul_loop_single_pipeline\n+// CHECK: tt.func @matmul_loop_single_pipeline\n // CHECK-DAG: %[[CONSTANT_0:.*]] = arith.constant 0 : i32\n // CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n // CHECK-DAG: %[[CONSTANT_2:.*]] = arith.constant 2 : i32\n@@ -186,7 +187,7 @@ func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:   scf.yield {{.*}}, {{.*}}, %[[NEXT_B_BUFFER]], %[[NEXT_B]], {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n-func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n+tt.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n                                   %A : !tt.ptr<f16> {tt.divisibility = 16 : i32},\n                                   %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) -> tensor<128x128xf32, #C> {\n   // A ptrs\n@@ -221,133 +222,94 @@ func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_b_ptr, %c : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n-  return %loop#1 : tensor<128x128xf32, #C>\n+  tt.return %loop#1 : tensor<128x128xf32, #C>\n }\n \n-// CHECK: func.func @lut_bmm\n+// CHECK: tt.func @lut_bmm_scalar\n // CHECK: triton_gpu.insert_slice_async\n // CHECK: triton_gpu.insert_slice_async\n // CHECK: triton_gpu.insert_slice_async\n // CHECK: triton_gpu.insert_slice_async\n // CHECK: triton_gpu.async_commit_group\n-// CHECK: %[[LUT_PTR:.*]] = tt.addptr\n-// CHECK: %arg27 = %[[LUT_PTR]]\n-// CHECK: %[[LUT_BUFFER_0:.*]] = tt.load %arg27, {{.*}}\n+// CHECK: %[[LUT_BUFFER_0:.*]] = tt.load %arg15, {{.*}}\n // CHECK: %[[LUT_BUFFER_1:.*]] = arith.muli {{.*}}, %[[LUT_BUFFER_0]]\n // CHECK: %[[LUT_BUFFER_2:.*]] = tt.splat %[[LUT_BUFFER_1]]\n // CHECK: %[[NEXT_BUFFER_0:.*]] = tt.addptr {{.*}}, %[[LUT_BUFFER_2]]\n-// CHECK: %[[NEXT_BUFFER_1:.*]] = tt.addptr %arg26, {{.*}}\n+// CHECK: %[[NEXT_BUFFER_1:.*]] = tt.addptr %arg14, {{.*}}\n // CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_1]]\n // CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_0]]\n-#blocked = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n-#blocked1 = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n-#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n-func.func @lut_bmm(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32 {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}, %arg17: i32 {tt.divisibility = 16 : i32}) {\n-  %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma>\n+// CHECK: triton_gpu.async_wait {num = 2 : i32}\n+tt.func @lut_bmm_scalar(%77: i64 {tt.divisibility=16: i32},\n+                   %76: index,\n+                   %49: tensor<16x16x!tt.ptr<f16>, #AL> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %75: !tt.ptr<i64>,\n+                   %78: tensor<16x16xi32, #AL> {tt.constancy=16: i32, tt.divisibility=16: i32},\n+                   %60: tensor<16x16x!tt.ptr<f16>, #BL> {tt.divisibility=16: i32, tt.contiguity=16 : i32}) -> tensor<16x16xf32, #C>{\n+  %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #C>\n   %c4_i32 = arith.constant 4 : i32\n   %c1 = arith.constant 1 : index\n   %c0 = arith.constant 0 : index\n   %c0_i64 = arith.constant 0 : i64\n   %c1_i32 = arith.constant 1 : i32\n-  %0 = tt.get_program_id {axis = 2 : i32} : i32\n-  %1 = tt.get_program_id {axis = 0 : i32} : i32\n-  %2 = tt.get_program_id {axis = 1 : i32} : i32\n-  %3 = tt.get_num_programs {axis = 0 : i32} : i32\n-  %4 = tt.get_num_programs {axis = 1 : i32} : i32\n-  %5 = arith.muli %1, %4 : i32\n-  %6 = arith.addi %5, %2 : i32\n-  %7 = arith.muli %4, %c4_i32 : i32\n-  %8 = arith.divsi %6, %7 : i32\n-  %9 = arith.muli %8, %c4_i32 : i32\n-  %10 = arith.subi %3, %9 : i32\n-  %11 = arith.cmpi slt, %10, %c4_i32 : i32\n-  %12 = arith.select %11, %10, %c4_i32 : i32\n-  %13 = arith.remsi %6, %12 : i32\n-  %14 = arith.addi %9, %13 : i32\n-  %15 = arith.remsi %6, %7 : i32\n-  %16 = arith.divsi %15, %12 : i32\n-  %17 = arith.muli %arg5, %0 : i32\n-  %18 = tt.addptr %arg4, %17 : !tt.ptr<i64>, i32\n-  %19 = tt.addptr %18, %14 : !tt.ptr<i64>, i32\n-  %20 = tt.load %19 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64\n-  %21 = tt.addptr %19, %c1_i32 : !tt.ptr<i64>, i32\n-  %22 = tt.load %21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64\n-  %23 = arith.subi %22, %20 : i64\n-  %24 = arith.cmpi eq, %23, %c0_i64 : i64\n-  cf.cond_br %24, ^bb1, ^bb2\n-^bb1:  // pred: ^bb0\n-  return\n-^bb2:  // pred: ^bb0\n-  %25 = arith.muli %arg1, %0 : i32\n-  %26 = tt.addptr %arg0, %25 : !tt.ptr<f16>, i32\n-  %27 = arith.extsi %arg2 : i32 to i64\n-  %28 = arith.muli %27, %20 : i64\n-  %29 = tt.addptr %26, %28 : !tt.ptr<f16>, i64\n-  %30 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n-  %31 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n-  %32 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n-  %33 = tt.expand_dims %30 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<16x1xi32, #blocked>\n-  %34 = tt.expand_dims %31 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<16x1xi32, #blocked1>\n-  %35 = tt.expand_dims %32 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<16x1xi32, #blocked>\n-  %36 = tt.splat %arg3 : (i32) -> tensor<16x1xi32, #blocked>\n-  %37 = arith.muli %36, %33 : tensor<16x1xi32, #blocked>\n-  %38 = tt.splat %29 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked>\n-  %39 = tt.addptr %38, %37 : tensor<16x1x!tt.ptr<f16>, #blocked>, tensor<16x1xi32, #blocked>\n-  %40 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n-  %41 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>\n-  %42 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n-  %43 = tt.expand_dims %40 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x16xi32, #blocked>\n-  %44 = tt.expand_dims %41 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>) -> tensor<1x16xi32, #blocked1>\n-  %45 = tt.expand_dims %42 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x16xi32, #blocked>\n-  %46 = tt.broadcast %39 : (tensor<16x1x!tt.ptr<f16>, #blocked>) -> tensor<16x16x!tt.ptr<f16>, #blocked>\n-  %47 = tt.broadcast %43 : (tensor<1x16xi32, #blocked>) -> tensor<16x16xi32, #blocked>\n-  %48 = tt.broadcast %45 : (tensor<1x16xi32, #blocked>) -> tensor<16x16xi32, #blocked>\n-  %49 = tt.addptr %46, %47 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n-  %50 = arith.muli %arg9, %0 : i32\n-  %51 = tt.addptr %arg8, %50 : !tt.ptr<f16>, i32\n-  %52 = arith.muli %arg11, %16 : i32\n-  %53 = tt.addptr %51, %52 : !tt.ptr<f16>, i32\n-  %54 = tt.splat %53 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked1>\n-  %55 = tt.addptr %54, %34 : tensor<16x1x!tt.ptr<f16>, #blocked1>, tensor<16x1xi32, #blocked1>\n-  %56 = tt.splat %arg12 : (i32) -> tensor<1x16xi32, #blocked1>\n-  %57 = arith.muli %56, %44 : tensor<1x16xi32, #blocked1>\n-  %58 = tt.broadcast %55 : (tensor<16x1x!tt.ptr<f16>, #blocked1>) -> tensor<16x16x!tt.ptr<f16>, #blocked1>\n-  %59 = tt.broadcast %57 : (tensor<1x16xi32, #blocked1>) -> tensor<16x16xi32, #blocked1>\n-  %60 = tt.addptr %58, %59 : tensor<16x16x!tt.ptr<f16>, #blocked1>, tensor<16x16xi32, #blocked1>\n-  %61 = arith.muli %arg14, %0 : i32\n-  %62 = tt.addptr %arg13, %61 : !tt.ptr<f16>, i32\n-  %63 = arith.muli %arg15, %14 : i32\n-  %64 = tt.addptr %62, %63 : !tt.ptr<f16>, i32\n-  %65 = arith.muli %arg16, %16 : i32\n-  %66 = tt.addptr %64, %65 : !tt.ptr<f16>, i32\n-  %67 = tt.splat %arg17 : (i32) -> tensor<16x1xi32, #blocked>\n-  %68 = arith.muli %67, %35 : tensor<16x1xi32, #blocked>\n-  %69 = tt.splat %66 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked>\n-  %70 = tt.addptr %69, %68 : tensor<16x1x!tt.ptr<f16>, #blocked>, tensor<16x1xi32, #blocked>\n-  %71 = tt.broadcast %70 : (tensor<16x1x!tt.ptr<f16>, #blocked>) -> tensor<16x16x!tt.ptr<f16>, #blocked>\n-  %72 = tt.addptr %71, %48 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n-  %73 = arith.muli %arg7, %0 : i32\n-  %74 = tt.addptr %arg6, %73 : !tt.ptr<i64>, i32\n-  %75 = tt.addptr %74, %20 : !tt.ptr<i64>, i64\n-  %76 = arith.index_cast %23 : i64 to index\n-  %77 = arith.extsi %arg10 : i32 to i64\n-  %78 = tt.splat %arg2 : (i32) -> tensor<16x16xi32, #blocked>\n-  %79:3 = scf.for %arg18 = %c0 to %76 step %c1 iter_args(%arg19 = %cst, %arg20 = %49, %arg21 = %75) -> (tensor<16x16xf32, #mma>, tensor<16x16x!tt.ptr<f16>, #blocked>, !tt.ptr<i64>) {\n-    %82 = tt.load %arg20 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blocked>\n+  %79:3 = scf.for %arg18 = %c0 to %76 step %c1 iter_args(%arg19 = %cst, %arg20 = %49, %arg21 = %75) -> (tensor<16x16xf32, #C>, tensor<16x16x!tt.ptr<f16>, #AL>, !tt.ptr<i64>) {\n+    %82 = tt.load %arg20 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #AL>\n     %83 = tt.load %arg21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64\n     %84 = arith.muli %77, %83 : i64\n-    %85 = tt.splat %84 : (i64) -> tensor<16x16xi64, #blocked1>\n-    %86 = tt.addptr %60, %85 : tensor<16x16x!tt.ptr<f16>, #blocked1>, tensor<16x16xi64, #blocked1>\n-    %87 = tt.load %86 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blocked1>\n-    %88 = triton_gpu.convert_layout %82 : (tensor<16x16xf16, #blocked>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma}>>\n-    %89 = triton_gpu.convert_layout %87 : (tensor<16x16xf16, #blocked1>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma}>>\n-    %90 = tt.dot %88, %89, %arg19 {allowTF32 = true} : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma}>> -> tensor<16x16xf32, #mma>\n-    %91 = tt.addptr %arg20, %78 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n+    %85 = tt.splat %84 : (i64) -> tensor<16x16xi64, #BL>\n+    %86 = tt.addptr %60, %85 : tensor<16x16x!tt.ptr<f16>, #BL>, tensor<16x16xi64, #BL>\n+    %87 = tt.load %86 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BL>\n+    %88 = triton_gpu.convert_layout %82 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A>\n+    %89 = triton_gpu.convert_layout %87 : (tensor<16x16xf16, #BL>) -> tensor<16x16xf16, #B>\n+    %90 = tt.dot %88, %89, %arg19 {allowTF32 = true} : tensor<16x16xf16, #A> * tensor<16x16xf16, #B> -> tensor<16x16xf32, #C>\n+    %91 = tt.addptr %arg20, %78 : tensor<16x16x!tt.ptr<f16>, #AL>, tensor<16x16xi32, #AL>\n     %92 = tt.addptr %arg21, %c1_i32 : !tt.ptr<i64>, i32\n-    scf.yield %90, %91, %92 : tensor<16x16xf32, #mma>, tensor<16x16x!tt.ptr<f16>, #blocked>, !tt.ptr<i64>\n+    scf.yield %90, %91, %92 : tensor<16x16xf32, #C>, tensor<16x16x!tt.ptr<f16>, #AL>, !tt.ptr<i64>\n   }\n-  %80 = arith.truncf %79#0 : tensor<16x16xf32, #mma> to tensor<16x16xf16, #mma>\n-  %81 = triton_gpu.convert_layout %80 : (tensor<16x16xf16, #mma>) -> tensor<16x16xf16, #blocked>\n-  tt.store %72, %81 {cache = 1 : i32, evict = 1 : i32} : tensor<16x16xf16, #blocked>\n-  return\n+  tt.return %79#0 : tensor<16x16xf32, #C>\n+}\n+\n+// CHECK: tt.func @lut_bmm_vector\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: %[[LUT_BUFFER_0:.*]] = tt.load %arg15, {{.*}}\n+// CHECK: %[[LUT_BUFFER_1:.*]] = tt.expand_dims %[[LUT_BUFFER_0]] {axis = 1 : i32}\n+// CHECK: %[[LUT_BUFFER_2:.*]] = tt.broadcast %[[LUT_BUFFER_1]]\n+// CHECK: %[[LUT_BUFFER_3:.*]] = arith.muli {{.*}}, %[[LUT_BUFFER_2]]\n+// CHECK: %[[NEXT_BUFFER_0:.*]] = tt.addptr {{.*}}, %[[LUT_BUFFER_3]]\n+// CHECK: %[[NEXT_BUFFER_1:.*]] = tt.addptr %arg14, {{.*}}\n+// CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_1]]\n+// CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_0]]\n+// CHECK: triton_gpu.async_wait {num = 2 : i32}\n+tt.func @lut_bmm_vector(%77: tensor<16x16xi64, #BL> {tt.divisibility=16: i32, tt.constancy=16: i32},\n+                   %76: index,\n+                   %49: tensor<16x16x!tt.ptr<f16>, #AL> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %75: tensor<16x!tt.ptr<i64>, #BLs1>,\n+                   %78: tensor<16x16xi32, #AL> {tt.constancy=16: i32, tt.divisibility=16: i32},\n+                   %60: tensor<16x16x!tt.ptr<f16>, #BL> {tt.divisibility=16: i32, tt.contiguity=16 : i32}) -> tensor<16x16xf32, #C>{\n+  %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #C>\n+  %c4_i32 = arith.constant 4 : i32\n+  %c1 = arith.constant 1 : index\n+  %c0 = arith.constant 0 : index\n+  %c0_i64 = arith.constant 0 : i64\n+  %c1_i32 = arith.constant 1 : i32\n+  %c1_i32_splat = tt.splat %c1_i32 : (i32) -> tensor<16xi32, #BLs1>\n+  %79:3 = scf.for %arg18 = %c0 to %76 step %c1 iter_args(%arg19 = %cst, %arg20 = %49, %arg21 = %75) -> (tensor<16x16xf32, #C>, tensor<16x16x!tt.ptr<f16>, #AL>, tensor<16x!tt.ptr<i64>, #BLs1>) {\n+    %82 = tt.load %arg20 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #AL>\n+    %83 = tt.load %arg21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16xi64, #BLs1>\n+    %84 = tt.expand_dims %83 {axis=1: i32}: (tensor<16xi64, #BLs1>) -> tensor<16x1xi64, #BL>\n+    %850 = tt.broadcast %84 : (tensor<16x1xi64, #BL>) -> tensor<16x16xi64, #BL>\n+    %85 = arith.muli %77, %850 : tensor<16x16xi64, #BL>\n+    %86 = tt.addptr %60, %85 : tensor<16x16x!tt.ptr<f16>, #BL>, tensor<16x16xi64, #BL>\n+    %87 = tt.load %86 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BL>\n+    %88 = triton_gpu.convert_layout %82 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A>\n+    %89 = triton_gpu.convert_layout %87 : (tensor<16x16xf16, #BL>) -> tensor<16x16xf16, #B>\n+    %90 = tt.dot %88, %89, %arg19 {allowTF32 = true} : tensor<16x16xf16, #A> * tensor<16x16xf16, #B> -> tensor<16x16xf32, #C>\n+    %91 = tt.addptr %arg20, %78 : tensor<16x16x!tt.ptr<f16>, #AL>, tensor<16x16xi32, #AL>\n+    %92 = tt.addptr %arg21, %c1_i32_splat : tensor<16x!tt.ptr<i64>, #BLs1>, tensor<16xi32, #BLs1>\n+    scf.yield %90, %91, %92 : tensor<16x16xf32, #C>, tensor<16x16x!tt.ptr<f16>, #AL>, tensor<16x!tt.ptr<i64>, #BLs1>\n+  }\n+  tt.return %79#0 : tensor<16x16xf32, #C>\n }"}, {"filename": "test/TritonGPU/matmul.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -4,7 +4,7 @@\n // CHECK: offset = 49152, size = 49152\n // CHECK: size = 98304\n module {\n-func.func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c64_13c64_14c64_15c8(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32) {\n+tt.func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c64_13c64_14c64_15c8(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32) {\n     %cst = arith.constant dense<true> : tensor<64x64xi1>\n     %c64 = arith.constant 64 : index\n     %c0 = arith.constant 0 : index\n@@ -101,6 +101,6 @@ func.func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32_\n     %74 = tt.broadcast %72 : (tensor<1x64xi1>) -> tensor<64x64xi1>\n     %75 = arith.andi %73, %74 : tensor<64x64xi1>\n     tt.store %66, %47#0, %75 : tensor<64x64xf32>\n-    return\n+    tt.return\n   }\n }"}, {"filename": "test/TritonGPU/prefetch.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -11,7 +11,7 @@\n #B_OP = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n \n-// CHECK: func.func @matmul_loop\n+// CHECK: tt.func @matmul_loop\n // CHECK-DAG: %[[A0_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice %[[A0:.*]][0, 0] [128, 16]\n // CHECK-DAG: %[[A0_PREFETCH:.*]] = triton_gpu.convert_layout %[[A0_PREFETCH_SMEM]]\n // CHECK-DAG: %[[B0_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice %[[B0:.*]][0, 0] [16, 128]\n@@ -28,7 +28,7 @@\n // CHECK-DAG:   %[[NEXT_B_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice {{.*}}[0, 0] [16, 128]\n // CHECK-DAG:   %[[NEXT_B_PREFETCH:.*]] = triton_gpu.convert_layout %[[NEXT_B_PREFETCH_SMEM]]\n // CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_PREFETCH]], %[[NEXT_B_PREFETCH]]\n-func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n \n@@ -60,5 +60,5 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16\n \n     scf.yield %next_a_ptr, %next_b_ptr, %next_a, %next_b, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x32xf16, #A>, tensor<32x128xf16, #B>, tensor<128x128xf32, #C>\n   }\n-  return\n+  tt.return\n }"}, {"filename": "test/lib/Analysis/TestAlias.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -9,7 +9,7 @@ using namespace mlir;\n namespace {\n \n struct TestAliasPass\n-    : public PassWrapper<TestAliasPass, OperationPass<func::FuncOp>> {\n+    : public PassWrapper<TestAliasPass, OperationPass<triton::FuncOp>> {\n \n   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAliasPass);\n "}, {"filename": "test/lib/Analysis/TestAllocation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -6,7 +6,7 @@ using namespace mlir;\n namespace {\n \n struct TestAllocationPass\n-    : public PassWrapper<TestAllocationPass, OperationPass<func::FuncOp>> {\n+    : public PassWrapper<TestAllocationPass, OperationPass<triton::FuncOp>> {\n \n   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAllocationPass);\n "}, {"filename": "test/lib/Analysis/TestAxisInfo.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -7,7 +7,7 @@ using namespace mlir;\n namespace {\n \n struct TestAxisInfoPass\n-    : public PassWrapper<TestAxisInfoPass, OperationPass<func::FuncOp>> {\n+    : public PassWrapper<TestAxisInfoPass, OperationPass<triton::FuncOp>> {\n \n   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAxisInfoPass);\n "}, {"filename": "test/lib/Analysis/TestMembar.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -11,7 +11,7 @@ using namespace mlir;\n namespace {\n \n struct TestMembarPass\n-    : public PassWrapper<TestMembarPass, OperationPass<func::FuncOp>> {\n+    : public PassWrapper<TestMembarPass, OperationPass<triton::FuncOp>> {\n \n   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestMembarPass);\n "}]