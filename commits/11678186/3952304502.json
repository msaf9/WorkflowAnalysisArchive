[{"filename": "CMakeLists.txt", "status": "modified", "additions": 32, "deletions": 29, "changes": 61, "file_content_changes": "@@ -188,8 +188,6 @@ add_subdirectory(include)\n add_subdirectory(lib)\n add_subdirectory(bin)\n \n-add_library(triton SHARED ${PYTHON_SRC})\n-\n # find_package(PythonLibs REQUIRED)\n \n set(TRITON_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}\")\n@@ -198,35 +196,40 @@ set(TRITON_BINARY_DIR \"${CMAKE_CURRENT_BINARY_DIR}\")\n get_property(dialect_libs GLOBAL PROPERTY MLIR_DIALECT_LIBS)\n get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)\n \n-target_link_libraries(triton\n-  TritonAnalysis\n-  TritonTransforms\n-  TritonGPUTransforms\n-  TritonLLVMIR\n-  TritonPTX\n-  ${dialect_libs}\n-  ${conversion_libs}\n-  # optimizations\n-  MLIRPass\n-  MLIRTransforms\n-  MLIRLLVMIR\n-  MLIRSupport\n-  MLIRTargetLLVMIRExport\n-  MLIRExecutionEngine\n-  MLIRMathToLLVM\n-  MLIRNVVMToLLVMIRTranslation\n-  MLIRIR\n-)\n-\n-target_link_options(triton PRIVATE ${LLVM_LDFLAGS})\n-\n-if(WIN32)\n-    target_link_libraries(triton PRIVATE ${LLVM_LIBRARIES} dl) # dl is from dlfcn-win32\n-else()\n-    target_link_libraries(triton ${LLVM_LIBRARIES} z)\n+if(TRITON_BUILD_PYTHON_MODULE)\n+  add_library(triton SHARED ${PYTHON_SRC})\n+\n+  target_link_libraries(triton\n+    TritonAnalysis\n+    TritonTransforms\n+    TritonGPUTransforms\n+    TritonLLVMIR\n+    TritonPTX\n+    ${dialect_libs}\n+    ${conversion_libs}\n+    # optimizations\n+    MLIRPass\n+    MLIRTransforms\n+    MLIRLLVMIR\n+    MLIRSupport\n+    MLIRTargetLLVMIRExport\n+    MLIRExecutionEngine\n+    MLIRMathToLLVM\n+    MLIRNVVMToLLVMIRTranslation\n+    MLIRIR\n+  )\n+\n+  target_link_options(triton PRIVATE ${LLVM_LDFLAGS})\n+\n+  if(WIN32)\n+      target_link_libraries(triton PRIVATE ${LLVM_LIBRARIES} dl) # dl is from dlfcn-win32\n+  elseif(APPLE)\n+      target_link_libraries(triton ${LLVM_LIBRARIES} z)\n+  else()\n+      target_link_libraries(triton ${LLVM_LIBRARIES} z stdc++fs)\n+  endif()\n endif()\n \n-\n if(TRITON_BUILD_PYTHON_MODULE AND NOT WIN32)\n     set(CMAKE_SHARED_LIBRARY_SUFFIX \".so\")\n     # Check if the platform is MacOS"}, {"filename": "bin/CMakeLists.txt", "status": "modified", "additions": 33, "deletions": 32, "changes": 65, "file_content_changes": "@@ -26,35 +26,36 @@ target_link_libraries(triton-opt PRIVATE\n mlir_check_all_link_libraries(triton-opt)\n \n \n-# add_llvm_executable(triton-translate triton-translate.cpp PARTIAL_SOURCES_INTENDED)\n-#llvm_update_compile_flags(triton-translate)\n-# target_link_libraries(triton-translate PRIVATE\n-#         TritonAnalysis\n-#         TritonTransforms\n-#         TritonGPUTransforms\n-#         TritonLLVMIR\n-#         TritonDriver\n-#         ${dialect_libs}\n-#         ${conversion_libs}\n-#         # tests\n-#         TritonTestAnalysis\n-\n-#         LLVMCore\n-#         LLVMSupport\n-#         LLVMOption\n-#         LLVMCodeGen\n-#         LLVMAsmParser\n-\n-#         # MLIR core\n-#         MLIROptLib\n-#         MLIRIR\n-#         MLIRPass\n-#         MLIRSupport\n-#         MLIRTransforms\n-#         MLIRExecutionEngine\n-#         MLIRMathToLLVM\n-#         MLIRTransformUtils\n-#         MLIRLLVMToLLVMIRTranslation\n-#         MLIRNVVMToLLVMIRTranslation\n-#         )\n-# mlir_check_all_link_libraries(triton-translate)\n+add_llvm_executable(triton-translate triton-translate.cpp PARTIAL_SOURCES_INTENDED)\n+llvm_update_compile_flags(triton-translate)\n+ target_link_libraries(triton-translate PRIVATE\n+         TritonAnalysis\n+         TritonTransforms\n+         TritonGPUTransforms\n+         TritonLLVMIR\n+         TritonPTX\n+         ${dialect_libs}\n+         ${conversion_libs}\n+         # tests\n+         TritonTestAnalysis\n+\n+         LLVMCore\n+         LLVMSupport\n+         LLVMOption\n+         LLVMCodeGen\n+         LLVMAsmParser\n+\n+         # MLIR core\n+         MLIROptLib\n+         MLIRIR\n+         MLIRLLVMIR\n+         MLIRPass\n+         MLIRSupport\n+         MLIRTransforms\n+         MLIRExecutionEngine\n+         MLIRMathToLLVM\n+         MLIRTransformUtils\n+         MLIRLLVMToLLVMIRTranslation\n+         MLIRNVVMToLLVMIRTranslation\n+         )\n+mlir_check_all_link_libraries(triton-translate)"}, {"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -15,7 +15,7 @@\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n-#include \"triton/driver/llvm.h\"\n+#include \"triton/Target/PTX/PTXTranslation.h\"\n #include \"llvm/IR/LLVMContext.h\"\n #include \"llvm/Support/CommandLine.h\"\n #include \"llvm/Support/InitLLVM.h\"\n@@ -116,8 +116,8 @@ LogicalResult tritonTranslateMain(int argc, char **argv,\n   if (targetKind == \"llvmir\")\n     llvm::outs() << *llvmir << '\\n';\n   else if (targetKind == \"ptx\")\n-    llvm::outs() << ::triton::driver::llir_to_ptx(\n-        llvmir.get(), SMArch.getValue(), ptxVersion.getValue());\n+    llvm::outs() << ::triton::translateLLVMIRToPTX(*llvmir, SMArch.getValue(),\n+                                                   ptxVersion.getValue());\n \n   return success();\n }"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -161,9 +161,8 @@ struct PTXBuilder {\n \n   std::string dump() const;\n \n-  mlir::Value launch(ConversionPatternRewriter &rewriter, Location loc,\n-                     Type resTy, bool hasSideEffect = true,\n-                     bool isAlignStack = false,\n+  mlir::Value launch(OpBuilder &rewriter, Location loc, Type resTy,\n+                     bool hasSideEffect = true, bool isAlignStack = false,\n                      ArrayRef<Attribute> attrs = {}) const;\n \n private:"}, {"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -25,6 +25,10 @@ class DialectInferLayoutInterface\n public:\n   DialectInferLayoutInterface(Dialect *dialect) : Base(dialect) {}\n \n+  virtual LogicalResult\n+  inferTransOpEncoding(Attribute operandEncoding,\n+                       Attribute &resultEncoding) const = 0;\n+\n   virtual LogicalResult\n   inferReduceOpEncoding(Attribute operandEncoding, unsigned axis,\n                         Attribute &resultEncoding) const = 0;"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -289,7 +289,7 @@ def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n }\n \n def TT_TransOp : TT_Op<\"trans\", [NoSideEffect,\n-                               SameOperandsAndResultElementType]> {\n+                                 DeclareOpInterfaceMethods<InferTypeOpInterface>]> {\n \n     let summary = \"transpose a tensor\";\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -39,6 +39,8 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout);\n \n SmallVector<unsigned> getOrder(const Attribute &layout);\n \n+bool isaDistributedLayout(const Attribute &layout);\n+\n } // namespace gpu\n } // namespace triton\n } // namespace mlir"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 34, "deletions": 6, "changes": 40, "file_content_changes": "@@ -376,31 +376,59 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n   );\n \n   let builders = [\n-    // specific for MMAV1(Volta)\n+     // Specially for MMAV1(Volta)\n+    AttrBuilder<(ins \"int\":$versionMajor,\n+                     \"int\":$numWarps,\n+                     \"int\":$id), [{\n+      assert(versionMajor == 1 && \"This builder is specially for versionMajor==1\");\n+      SmallVector<unsigned> wpt({static_cast<unsigned>(numWarps), 1});\n+      int versionMinor = 0;\n+\n+      assert(id < (1<<numBitsToHoldMmaV1ID) && \"MMAv1 ID exceeds the maximum\");\n+      for (int i = 0; i < numBitsToHoldMmaV1ID; ++i)\n+        versionMinor |= static_cast<bool>((1<<i) & id) * (1<<(4+i));\n+\n+      return $_get(context, versionMajor, versionMinor, wpt);\n+    }]>,\n+\n+    // Specially for MMAV1(Volta)\n     AttrBuilder<(ins \"int\":$versionMajor,\n                      \"ArrayRef<unsigned>\":$warpsPerCTA,\n                      \"ArrayRef<int64_t>\":$shapeA,\n                      \"ArrayRef<int64_t>\":$shapeB,\n                      \"bool\":$isARow,\n-                     \"bool\":$isBRow), [{\n-      assert(versionMajor == 1 && \"Only MMAv1 has multiple versionMinor.\");\n+                     \"bool\":$isBRow,\n+                     \"int\":$id), [{\n+      assert(versionMajor == 1 && \"This builder is specially for versionMajor==1\");\n       bool isAVec4 = !isARow && (shapeA[isARow] <= 16);\n       bool isBVec4 = isBRow && (shapeB[isBRow] <= 16);\n       // 4-bits to encode 4 booleans: [isARow, isBRow, isAVec4, isBVec4]\n       int versionMinor = (isARow * (1<<0)) |\\\n                          (isBRow * (1<<1)) |\\\n                          (isAVec4 * (1<<2)) |\\\n                          (isBVec4 * (1<<3));\n+\n+      assert(id < (1<<numBitsToHoldMmaV1ID) && \"MMAv1 ID exceeds the maximum\");\n+      for (int i = 0; i < numBitsToHoldMmaV1ID; ++i)\n+        versionMinor |= static_cast<bool>((1<<i) & id) * (1<<(4+i));\n+\n       return $_get(context, versionMajor, versionMinor, warpsPerCTA);\n     }]>\n-\n   ];\n \n   let extraClassDeclaration = extraBaseClassDeclaration # [{\n     bool isVolta() const;\n     bool isAmpere() const;\n-    // Get [isARow, isBRow, isAVec4, isBVec4] from versionMinor\n-    std::tuple<bool, bool, bool, bool> decodeVoltaLayoutStates() const;\n+    // Get [isARow, isBRow, isAVec4, isBVec4, id] from versionMinor\n+    std::tuple<bool, bool, bool, bool, int> decodeVoltaLayoutStates() const;\n+    // Number of bits in versionMinor to hold the ID of the MMA encoding instance.\n+    // Here 5 bits can hold 32 IDs in a single module.\n+    static constexpr int numBitsToHoldMmaV1ID{5};\n+\n+    // Here is a temporary flag that indicates whether we need to update the warpsPerCTA for MMAv1, since the current backend cannot support the updated wpt.\n+    // The mmav1's wpt-related logic is separated into multiple files, so a global flag is added here for universal coordination.\n+    // TODO[Superjomn]: Remove this flag once the MMAv1 backend is ready.\n+    static constexpr bool _mmaV1UpdateWpt{false};\n   }];\n \n }"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "file_content_changes": "@@ -40,6 +40,19 @@ def TTG_AsyncWaitOp : TTG_Op<\"async_wait\"> {\n   }];\n }\n \n+def TTG_AsyncCommitGroupOp : TTG_Op<\"async_commit_group\"> {\n+  let summary = \"async commit group\";\n+\n+  let assemblyFormat = \"attr-dict\";\n+\n+  let extraClassDeclaration = [{\n+    static bool isSupported(int computeCapability) {\n+      return computeCapability >= 80;\n+    }\n+  }];\n+}\n+\n+\n // Port Arith_CmpIOp & Arith_CmpFOp & Std_SelectOp to TritonGPU.\n // This is needed because these ops don't\n // handle encodings"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -13,10 +13,16 @@ std::unique_ptr<Pass> createTritonGPUCanonicalizeLoopsPass();\n \n std::unique_ptr<Pass> createTritonGPUCoalescePass();\n \n+std::unique_ptr<Pass> createTritonGPUReorderInstructionsPass();\n+\n+std::unique_ptr<Pass> createTritonGPUDecomposeConversionsPass();\n+\n std::unique_ptr<Pass> createTritonGPUCombineOpsPass(int computeCapability = 80);\n \n std::unique_ptr<Pass> createTritonGPUVerifier();\n \n+std::unique_ptr<Pass> createTritonGPUUpdateMmaForVoltaPass();\n+\n /// Generate the code for registering passes.\n #define GEN_PASS_REGISTRATION\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\""}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 36, "deletions": 0, "changes": 36, "file_content_changes": "@@ -71,6 +71,30 @@ def TritonGPUCombineOps : Pass<\"tritongpu-combine\", \"mlir::ModuleOp\"> {\n   ];\n }\n \n+def TritonGPUReorderInstructions: Pass<\"tritongpu-reorder-instructions\", \"mlir::ModuleOp\"> {\n+  let summary = \"Reorder instructions\";\n+\n+  let description = \"This pass reorder instructions so as to (1) decrease register pressure (e.g., by moving \"\n+                    \"conversions from shared memory before their first use) and (2) promote LLVM instruction \"\n+                    \"order more friendly to `ptxas`.\";\n+\n+  let constructor = \"mlir::createTritonGPUReorderInstructionsPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::TritonDialect\"];\n+}\n+\n+def TritonGPUDecomposeConversions: Pass<\"tritongpu-decompose-conversions\", \"mlir::ModuleOp\"> {\n+  let summary = \"Decompose convert[distributed -> dotOperand] into convert[distributed -> shared -> dotOperand]\";\n+\n+  let description = \"Decomposing conversions this way makes it possible to use CSE and re-use #shared tensors\";\n+\n+  let constructor = \"mlir::createTritonGPUDecomposeConversionsPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::TritonDialect\"];\n+}\n+\n def TritonGPUCanonicalizeLoops: Pass<\"tritongpu-canonicalize-loops\", \"mlir::ModuleOp\"> {\n   let summary = \"canonicalize scf.ForOp ops\";\n \n@@ -84,4 +108,16 @@ def TritonGPUCanonicalizeLoops: Pass<\"tritongpu-canonicalize-loops\", \"mlir::Modu\n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n }\n \n+def UpdateMmaForVolta : Pass<\"tritongpu-update-mma-for-volta\", \"mlir::ModuleOp\"> {\n+  let summary = \"Update mma encodings for Volta\";\n+\n+  let description = [{\n+    This helps to update the mma encodings for Volta.\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPUUpdateMmaForVoltaPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n+}\n+\n #endif"}, {"filename": "include/triton/Target/LLVMIR/LLVMIRTranslation.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -31,8 +31,6 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n std::unique_ptr<llvm::Module>\n translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module);\n \n-bool linkExternLib(llvm::Module &module, llvm::StringRef path);\n-\n } // namespace triton\n } // namespace mlir\n "}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -25,13 +25,14 @@ ChangeResult SharedMemoryAliasAnalysis::visitOperation(\n   if (maybeSharedAllocationOp(op)) {\n     // These ops may allocate a new shared memory buffer.\n     auto result = op->getResult(0);\n-    // FIXME(Keren): extract and insert are always alias for now\n+    // XXX(Keren): the following ops are always aliasing for now\n     if (isa<tensor::ExtractSliceOp, triton::TransOp>(op)) {\n       // extract_slice %src\n+      // trans %src\n       aliasInfo = AliasInfo(operands[0]->getValue());\n       pessimistic = false;\n-    } else if (isa<tensor::InsertSliceOp>(op) ||\n-               isa<triton::gpu::InsertSliceAsyncOp>(op)) {\n+    } else if (isa<tensor::InsertSliceOp, triton::gpu::InsertSliceAsyncOp>(\n+                   op)) {\n       // insert_slice_async %src, %dst, %index\n       // insert_slice %src into %dst[%offsets]\n       aliasInfo = AliasInfo(operands[1]->getValue());"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 17, "deletions": 3, "changes": 20, "file_content_changes": "@@ -298,10 +298,24 @@ class AllocationAnalysis {\n \n   /// Resolves liveness of all values involved under the root operation.\n   void resolveLiveness() {\n-    // In the SCF dialect, we always have a sequentially nested structure of\n-    // blocks\n+    // Assign an ID to each operation using post-order traversal.\n+    // To achieve the correct liveness range, the parent operation's ID\n+    // should be greater than each of its child operation's ID .\n+    // Example:\n+    //     ...\n+    //     %5 = triton.convert_layout %4\n+    //     %6 = scf.for ... iter_args(%arg0 = %0) -> (i32) {\n+    //       %2 = triton.convert_layout %5\n+    //       ...\n+    //       scf.yield %arg0\n+    //     }\n+    // For example, %5 is defined in the parent region and used in\n+    // the child region, and is not passed as a block argument.\n+    // %6 should should have an ID greater than its child operations,\n+    // otherwise %5 liveness range ends before the child operation's liveness\n+    // range ends.\n     DenseMap<Operation *, size_t> operationId;\n-    operation->walk<WalkOrder::PreOrder>(\n+    operation->walk<WalkOrder::PostOrder>(\n         [&](Operation *op) { operationId[op] = operationId.size(); });\n \n     // Analyze liveness of explicit buffers"}, {"filename": "lib/Analysis/CMakeLists.txt", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -6,5 +6,6 @@ add_mlir_library(TritonAnalysis\n   Utility.cpp\n \n   DEPENDS\n+  TritonTableGen\n   TritonGPUAttrDefsIncGen\n-)\n\\ No newline at end of file\n+)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 19, "deletions": 131, "changes": 150, "file_content_changes": "@@ -9,10 +9,12 @@ using ::mlir::LLVM::getStridesFromShapeAndOrder;\n using ::mlir::LLVM::getStructFromElements;\n using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getElemsPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::isaDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n@@ -24,115 +26,6 @@ bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n          dotOperandLayout.getParent() == mmaLayout;\n }\n \n-void storeBlockedToShared(Value src, Value llSrc, ArrayRef<Value> srcStrides,\n-                          ArrayRef<Value> srcIndices, Value dst, Value smemBase,\n-                          Type elemTy, Location loc,\n-                          ConversionPatternRewriter &rewriter) {\n-  auto srcTy = src.getType().cast<RankedTensorType>();\n-  auto srcShape = srcTy.getShape();\n-  assert(srcShape.size() == 2 && \"Unexpected rank of insertSlice\");\n-\n-  auto dstTy = dst.getType().cast<RankedTensorType>();\n-  auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n-  auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto inOrd = srcBlockedLayout.getOrder();\n-  auto outOrd = dstSharedLayout.getOrder();\n-  if (inOrd != outOrd)\n-    llvm_unreachable(\n-        \"blocked -> shared with different order not yet implemented\");\n-  unsigned inVec =\n-      inOrd == outOrd ? srcBlockedLayout.getSizePerThread()[inOrd[0]] : 1;\n-  unsigned outVec = dstSharedLayout.getVec();\n-  unsigned minVec = std::min(outVec, inVec);\n-  unsigned perPhase = dstSharedLayout.getPerPhase();\n-  unsigned maxPhase = dstSharedLayout.getMaxPhase();\n-  unsigned numElems = getElemsPerThread(srcTy);\n-  auto inVals = getElementsFromStruct(loc, llSrc, rewriter);\n-  auto srcAccumSizeInThreads =\n-      product<unsigned>(srcBlockedLayout.getSizePerThread());\n-  auto wordTy = vec_ty(elemTy, minVec);\n-  auto elemPtrTy = ptr_ty(elemTy);\n-\n-  // TODO: [goostavz] We should make a cache for the calculation of\n-  // emitBaseIndexForBlockedLayout in case backend compiler not being able to\n-  // optimize that\n-  SmallVector<unsigned> srcShapePerCTA = getShapePerCTA(srcBlockedLayout);\n-  SmallVector<unsigned> reps{ceil<unsigned>(srcShape[0], srcShapePerCTA[0]),\n-                             ceil<unsigned>(srcShape[1], srcShapePerCTA[1])};\n-\n-  // Visit each input value in the order they are placed in inVals\n-  //\n-  // Please note that the order was not awaring of blockLayout.getOrder(),\n-  // thus the adjacent elems may not belong to a same word. This could be\n-  // improved if we update the elements order by emitIndicesForBlockedLayout()\n-  SmallVector<unsigned> wordsInEachRep(2);\n-  wordsInEachRep[0] = inOrd[0] == 0\n-                          ? srcBlockedLayout.getSizePerThread()[0] / minVec\n-                          : srcBlockedLayout.getSizePerThread()[0];\n-  wordsInEachRep[1] = inOrd[0] == 0\n-                          ? srcBlockedLayout.getSizePerThread()[1]\n-                          : srcBlockedLayout.getSizePerThread()[1] / minVec;\n-  Value outVecVal = i32_val(outVec);\n-  Value minVecVal = i32_val(minVec);\n-  auto numWordsEachRep = product<unsigned>(wordsInEachRep);\n-  SmallVector<Value> wordVecs(numWordsEachRep);\n-  for (unsigned i = 0; i < numElems; ++i) {\n-    if (i % srcAccumSizeInThreads == 0) {\n-      // start of a replication\n-      for (unsigned w = 0; w < numWordsEachRep; ++w) {\n-        wordVecs[w] = undef(wordTy);\n-      }\n-    }\n-    unsigned linearIdxInNanoTile = i % srcAccumSizeInThreads;\n-    auto multiDimIdxInNanoTile = getMultiDimIndex<unsigned>(\n-        linearIdxInNanoTile, srcBlockedLayout.getSizePerThread(), inOrd);\n-    unsigned pos = multiDimIdxInNanoTile[inOrd[0]] % minVec;\n-    multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n-    auto wordVecIdx =\n-        getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep, inOrd);\n-    wordVecs[wordVecIdx] =\n-        insert_element(wordTy, wordVecs[wordVecIdx], inVals[i], i32_val(pos));\n-\n-    if (i % srcAccumSizeInThreads == srcAccumSizeInThreads - 1) {\n-      // end of replication, store the vectors into shared memory\n-      unsigned linearRepIdx = i / srcAccumSizeInThreads;\n-      auto multiDimRepIdx =\n-          getMultiDimIndex<unsigned>(linearRepIdx, reps, inOrd);\n-      for (unsigned linearWordIdx = 0; linearWordIdx < numWordsEachRep;\n-           ++linearWordIdx) {\n-        // step 1: recover the multidim_index from the index of\n-        // input_elements\n-        auto multiDimWordIdx =\n-            getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep, inOrd);\n-        SmallVector<Value> multiDimIdx(2);\n-        auto wordOffset0 = multiDimRepIdx[0] * srcShapePerCTA[0] +\n-                           multiDimWordIdx[0] * (inOrd[0] == 0 ? minVec : 1);\n-        auto wordOffset1 = multiDimRepIdx[1] * srcShapePerCTA[1] +\n-                           multiDimWordIdx[1] * (inOrd[0] == 1 ? minVec : 1);\n-        multiDimIdx[0] = add(srcIndices[0], i32_val(wordOffset0));\n-        multiDimIdx[1] = add(srcIndices[1], i32_val(wordOffset1));\n-\n-        // step 2: do swizzling\n-        Value remained = urem(multiDimIdx[outOrd[0]], outVecVal);\n-        multiDimIdx[outOrd[0]] = udiv(multiDimIdx[outOrd[0]], outVecVal);\n-        Value off_1 = mul(multiDimIdx[outOrd[1]], srcStrides[outOrd[1]]);\n-        Value phaseId = udiv(multiDimIdx[outOrd[1]], i32_val(perPhase));\n-        phaseId = urem(phaseId, i32_val(maxPhase));\n-        Value off_0 = xor_(multiDimIdx[outOrd[0]], phaseId);\n-        off_0 = mul(off_0, outVecVal);\n-        remained = udiv(remained, minVecVal);\n-        off_0 = add(off_0, mul(remained, minVecVal));\n-        Value offset = add(off_1, off_0);\n-\n-        // step 3: store\n-        Value smemAddr = gep(elemPtrTy, smemBase, offset);\n-        smemAddr = bitcast(smemAddr, ptr_ty(wordTy, 3));\n-        store(wordVecs[linearWordIdx], smemAddr);\n-      }\n-    }\n-  }\n-}\n-\n struct ConvertLayoutOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {\n public:\n@@ -148,20 +41,15 @@ struct ConvertLayoutOpConversion\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n     Attribute dstLayout = dstTy.getEncoding();\n-    if (srcLayout.isa<BlockedEncodingAttr>() &&\n+    if (isaDistributedLayout(srcLayout) &&\n         dstLayout.isa<SharedEncodingAttr>()) {\n-      return lowerBlockedToShared(op, adaptor, rewriter);\n+      return lowerDistributedToShared(op, adaptor, rewriter);\n     }\n     if (srcLayout.isa<SharedEncodingAttr>() &&\n         dstLayout.isa<DotOperandEncodingAttr>()) {\n       return lowerSharedToDotOperand(op, adaptor, rewriter);\n     }\n-    if ((srcLayout.isa<BlockedEncodingAttr>() ||\n-         srcLayout.isa<MmaEncodingAttr>() ||\n-         srcLayout.isa<SliceEncodingAttr>()) &&\n-        (dstLayout.isa<BlockedEncodingAttr>() ||\n-         dstLayout.isa<MmaEncodingAttr>() ||\n-         dstLayout.isa<SliceEncodingAttr>())) {\n+    if (isaDistributedLayout(srcLayout) && isaDistributedLayout(dstLayout)) {\n       return lowerDistributedToDistributed(op, adaptor, rewriter);\n     }\n     if (srcLayout.isa<MmaEncodingAttr>() &&\n@@ -182,7 +70,7 @@ struct ConvertLayoutOpConversion\n     unsigned rank = shape.size();\n     if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n       auto multiDimOffsetFirstElem =\n-          emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n+          emitBaseIndexForLayout(loc, rewriter, blockedLayout, shape);\n       SmallVector<Value> multiDimOffset(rank);\n       SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n           elemId, getSizePerThread(layout), getOrder(layout));\n@@ -479,8 +367,8 @@ struct ConvertLayoutOpConversion\n   // Swizzling in shared memory to avoid bank conflict. Normally used for\n   // A/B operands of dots.\n   LogicalResult\n-  lowerBlockedToShared(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n-                       ConversionPatternRewriter &rewriter) const {\n+  lowerDistributedToShared(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+                           ConversionPatternRewriter &rewriter) const {\n     auto loc = op.getLoc();\n     Value src = op.src();\n     Value dst = op.result();\n@@ -490,22 +378,20 @@ struct ConvertLayoutOpConversion\n     auto dstShape = dstTy.getShape();\n     assert(srcShape.size() == 2 &&\n            \"Unexpected rank of ConvertLayout(blocked->shared)\");\n-    auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+    auto srcLayout = srcTy.getEncoding();\n     auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n-    auto inOrd = srcBlockedLayout.getOrder();\n+    auto inOrd = getOrder(srcLayout);\n     auto outOrd = dstSharedLayout.getOrder();\n     Value smemBase = getSharedMemoryBase(loc, rewriter, dst);\n     auto elemTy = getTypeConverter()->convertType(srcTy.getElementType());\n     auto elemPtrTy = ptr_ty(getTypeConverter()->convertType(elemTy), 3);\n     smemBase = bitcast(smemBase, elemPtrTy);\n \n-    auto srcStrides =\n-        getStridesFromShapeAndOrder(srcShape, inOrd, loc, rewriter);\n-    auto srcIndices = emitBaseIndexForBlockedLayout(loc, rewriter,\n-                                                    srcBlockedLayout, srcShape);\n-    storeBlockedToShared(src, adaptor.src(), srcStrides, srcIndices, dst,\n-                         smemBase, elemTy, loc, rewriter);\n-\n+    auto dstStrides =\n+        getStridesFromShapeAndOrder(dstShape, outOrd, loc, rewriter);\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n+    storeDistributedToShared(src, adaptor.src(), dstStrides, srcIndices, dst,\n+                             smemBase, elemTy, loc, rewriter);\n     auto smemObj =\n         SharedMemoryObject(smemBase, dstShape, outOrd, loc, rewriter);\n     auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n@@ -680,7 +566,9 @@ struct ConvertLayoutOpConversion\n void populateConvertLayoutOpToLLVMPatterns(\n     mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem, PatternBenefit benefit) {\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n-                                          benefit);\n+                                          indexCacheInfo, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.h", "status": "modified", "additions": 3, "deletions": 6, "changes": 9, "file_content_changes": "@@ -11,14 +11,11 @@ using ::mlir::triton::gpu::DotOperandEncodingAttr;\n bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n                         DotOperandEncodingAttr &dotOperandLayout);\n \n-void storeBlockedToShared(Value src, Value llSrc, ArrayRef<Value> srcStrides,\n-                          ArrayRef<Value> srcIndices, Value dst, Value smemBase,\n-                          Type elemPtrTy, Location loc,\n-                          ConversionPatternRewriter &rewriter);\n-\n void populateConvertLayoutOpToLLVMPatterns(\n     mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n-    const Allocation *allocation, Value smem, PatternBenefit benefit);\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 71, "deletions": 70, "changes": 141, "file_content_changes": "@@ -93,6 +93,13 @@ struct DotOpMmaV1ConversionHelper {\n \n   static ArrayRef<unsigned> getMmaInstrShape() { return instrShape; }\n \n+  static Type getMatType(TensorType operand) {\n+    auto *ctx = operand.getContext();\n+    Type fp16Ty = type::f16Ty(ctx);\n+    Type vecTy = vec_ty(fp16Ty, 2);\n+    return struct_ty(SmallVector<Type>{vecTy});\n+  }\n+\n   static Type getMmaRetType(TensorType operand) {\n     auto *ctx = operand.getContext();\n     Type fp32Ty = type::f32Ty(ctx);\n@@ -204,7 +211,12 @@ struct DotOpMmaV1ConversionHelper {\n       offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n     }\n \n-    Type f16x2Ty = vec_ty(f16_ty, 2);\n+    Type elemX2Ty = vec_ty(f16_ty, 2);\n+    Type elemPtrTy = ptr_ty(f16_ty);\n+    if (tensorTy.getElementType().isBF16()) {\n+      elemX2Ty = vec_ty(i16_ty, 2);\n+      elemPtrTy = ptr_ty(i16_ty);\n+    }\n \n     // prepare arguments\n     SmallVector<Value> ptrA(numPtrA);\n@@ -213,30 +225,28 @@ struct DotOpMmaV1ConversionHelper {\n     for (int i = 0; i < numPtrA; i++)\n       ptrA[i] = gep(ptr_ty(f16_ty), smemBase, offA[i]);\n \n-    Type f16PtrTy = ptr_ty(f16_ty);\n-\n     auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n       vals[{m, k}] = {val0, val1};\n     };\n     auto loadA = [&](int m, int k) {\n       int offidx = (isARow ? k / 4 : m) % numPtrA;\n-      Value thePtrA = gep(f16PtrTy, smemBase, offA[offidx]);\n+      Value thePtrA = gep(elemPtrTy, smemBase, offA[offidx]);\n \n       int stepAM = isARow ? m : m / numPtrA * numPtrA;\n       int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n       Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),\n                          mul(i32_val(stepAK), strideAK));\n-      Value pa = gep(f16PtrTy, thePtrA, offset);\n+      Value pa = gep(elemPtrTy, thePtrA, offset);\n       Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n       Value ha = load(bitcast(pa, aPtrTy));\n       // record lds that needs to be moved\n-      Value ha00 = bitcast(extract_element(ha, i32_val(0)), f16x2Ty);\n-      Value ha01 = bitcast(extract_element(ha, i32_val(1)), f16x2Ty);\n+      Value ha00 = bitcast(extract_element(ha, i32_val(0)), elemX2Ty);\n+      Value ha01 = bitcast(extract_element(ha, i32_val(1)), elemX2Ty);\n       ld(has, m, k, ha00, ha01);\n \n       if (vecA > 4) {\n-        Value ha10 = bitcast(extract_element(ha, i32_val(2)), f16x2Ty);\n-        Value ha11 = bitcast(extract_element(ha, i32_val(3)), f16x2Ty);\n+        Value ha10 = bitcast(extract_element(ha, i32_val(2)), elemX2Ty);\n+        Value ha11 = bitcast(extract_element(ha, i32_val(3)), elemX2Ty);\n         if (isARow)\n           ld(has, m, k + 4, ha10, ha11);\n         else\n@@ -256,7 +266,7 @@ struct DotOpMmaV1ConversionHelper {\n       elems.push_back(item.second.second);\n     }\n \n-    Type resTy = struct_ty(SmallVector<Type>(elems.size(), f16x2Ty));\n+    Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n     Value res = getStructFromElements(loc, elems, rewriter, resTy);\n     return res;\n   }\n@@ -319,8 +329,12 @@ struct DotOpMmaV1ConversionHelper {\n       offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));\n     }\n \n-    Type f16PtrTy = ptr_ty(f16_ty);\n-    Type f16x2Ty = vec_ty(f16_ty, 2);\n+    Type elemPtrTy = ptr_ty(f16_ty);\n+    Type elemX2Ty = vec_ty(f16_ty, 2);\n+    if (tensorTy.getElementType().isBF16()) {\n+      elemPtrTy = ptr_ty(i16_ty);\n+      elemX2Ty = vec_ty(i16_ty, 2);\n+    }\n \n     SmallVector<Value> ptrB(numPtrB);\n     ValueTable hbs;\n@@ -339,17 +353,17 @@ struct DotOpMmaV1ConversionHelper {\n       int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n       Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n                          mul(i32_val(stepBK), strideBK));\n-      Value pb = gep(f16PtrTy, thePtrB, offset);\n+      Value pb = gep(elemPtrTy, thePtrB, offset);\n \n       Value hb =\n           load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n       // record lds that needs to be moved\n-      Value hb00 = bitcast(extract_element(hb, i32_val(0)), f16x2Ty);\n-      Value hb01 = bitcast(extract_element(hb, i32_val(1)), f16x2Ty);\n+      Value hb00 = bitcast(extract_element(hb, i32_val(0)), elemX2Ty);\n+      Value hb01 = bitcast(extract_element(hb, i32_val(1)), elemX2Ty);\n       ld(hbs, n, K, hb00, hb01);\n       if (vecB > 4) {\n-        Value hb10 = bitcast(extract_element(hb, i32_val(2)), f16x2Ty);\n-        Value hb11 = bitcast(extract_element(hb, i32_val(3)), f16x2Ty);\n+        Value hb10 = bitcast(extract_element(hb, i32_val(2)), elemX2Ty);\n+        Value hb11 = bitcast(extract_element(hb, i32_val(3)), elemX2Ty);\n         if (isBRow)\n           ld(hbs, n + 1, K, hb10, hb11);\n         else\n@@ -369,8 +383,7 @@ struct DotOpMmaV1ConversionHelper {\n       elems.push_back(item.second.first);\n       elems.push_back(item.second.second);\n     }\n-    Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n-    Type resTy = struct_ty(SmallVector<Type>(elems.size(), fp16x2Ty));\n+    Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n     Value res = getStructFromElements(loc, elems, rewriter, resTy);\n     return res;\n   }\n@@ -532,17 +545,17 @@ struct DotOpMmaV2ConversionHelper {\n \n   // The type of matrix that loaded by either a ldmatrix or composed lds.\n   Type getMatType() const {\n-    Type fp32Ty = type::f32Ty(ctx);\n+    // floating point types\n+    Type fp32x1Ty = vec_ty(type::f32Ty(ctx), 1);\n     Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n     Type i16x2Ty = vec_ty(type::i16Ty(ctx), 2);\n-    // floating point types\n     Type fp16x2Pack4Ty =\n         LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n     // LLVM 14.0 does not support bf16 type, so we use i16 instead.\n     Type bf16x2Pack4Ty =\n         LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i16x2Ty));\n     Type fp32Pack4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n+        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32x1Ty));\n     // integer types\n     Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n     Type i8x4Pack4Ty =\n@@ -954,7 +967,7 @@ class MMA16816SmemLoader {\n   // Load 4 matrices and returns 4 vec<2> elements.\n   std::tuple<Value, Value, Value, Value>\n   loadX4(int mat0, int mat1, ArrayRef<Value> offs, ArrayRef<Value> ptrs,\n-         Type ldmatrixRetTy, Type shemPtrTy) const {\n+         Type matTy, Type shemPtrTy) const {\n     assert(mat0 % 2 == 0 && mat1 % 2 == 0 &&\n            \"smem matrix load must be aligned\");\n     int matIdx[2] = {mat0, mat1};\n@@ -977,6 +990,9 @@ class MMA16816SmemLoader {\n \n     Value ptr = getPtr(ptrIdx);\n \n+    // The struct should have exactly the same element types.\n+    Type elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n+\n     if (canUseLdmatrix) {\n       Value sOffset =\n           mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sStride);\n@@ -994,20 +1010,12 @@ class MMA16816SmemLoader {\n       ldmatrix(resArgs, addrArg);\n \n       // The result type is 4xi32, each i32 is composed of 2xf16\n-      // elements(adjacent two columns in a row)\n-      Value resV4 = builder.launch(rewriter, loc, ldmatrixRetTy);\n-\n-      auto getIntAttr = [&](int v) {\n-        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n-      };\n-\n-      // The struct should have exactly the same element types.\n-      Type elemType = resV4.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n-\n-      return {extract_val(elemType, resV4, getIntAttr(0)),\n-              extract_val(elemType, resV4, getIntAttr(1)),\n-              extract_val(elemType, resV4, getIntAttr(2)),\n-              extract_val(elemType, resV4, getIntAttr(3))};\n+      // elements (adjacent two columns in a row) or a single f32 element.\n+      Value resV4 = builder.launch(rewriter, loc, matTy);\n+      return {extract_val(elemTy, resV4, i32_arr_attr(0)),\n+              extract_val(elemTy, resV4, i32_arr_attr(1)),\n+              extract_val(elemTy, resV4, i32_arr_attr(2)),\n+              extract_val(elemTy, resV4, i32_arr_attr(3))};\n     } else if (elemBytes == 4 &&\n                needTrans) { // Use lds.32 to load tf32 matrices\n       Value ptr2 = getPtr(ptrIdx + 1);\n@@ -1019,21 +1027,23 @@ class MMA16816SmemLoader {\n           add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n \n       Value elems[4];\n-      Type elemTy = type::f32Ty(ctx);\n-      Type elemPtrTy = ptr_ty(elemTy);\n       if (kOrder == 1) {\n-        elems[0] = load(gep(elemPtrTy, ptr, sOffsetElemVal));\n-        elems[1] = load(gep(elemPtrTy, ptr2, sOffsetElemVal));\n-        elems[2] = load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n-        elems[3] = load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n+        elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n+        elems[1] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n+        elems[2] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n+        elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n       } else {\n-        elems[0] = load(gep(elemPtrTy, ptr, sOffsetElemVal));\n-        elems[2] = load(gep(elemPtrTy, ptr2, sOffsetElemVal));\n-        elems[1] = load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n-        elems[3] = load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n+        elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n+        elems[2] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n+        elems[1] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n+        elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n       }\n-      return {elems[0], elems[1], elems[2], elems[3]};\n-\n+      std::array<Value, 4> retElems;\n+      retElems.fill(undef(elemTy));\n+      for (auto i = 0; i < 4; ++i) {\n+        retElems[i] = insert_element(elemTy, retElems[i], elems[i], i32_val(0));\n+      }\n+      return {retElems[0], retElems[1], retElems[2], retElems[3]};\n     } else if (elemBytes == 1 && needTrans) { // work with int8\n       std::array<std::array<Value, 4>, 2> ptrs;\n       ptrs[0] = {\n@@ -1058,49 +1068,42 @@ class MMA16816SmemLoader {\n           add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n \n       std::array<Value, 4> i8v4Elems;\n-      std::array<Value, 4> i32Elems;\n-      i8v4Elems.fill(\n-          rewriter.create<LLVM::UndefOp>(loc, vec_ty(type::i8Ty(ctx), 4)));\n+      i8v4Elems.fill(undef(elemTy));\n \n       Value i8Elems[4][4];\n-      Type elemTy = type::i8Ty(ctx);\n-      Type elemPtrTy = ptr_ty(elemTy);\n-      Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n       if (kOrder == 1) {\n         for (int i = 0; i < 2; ++i)\n           for (int j = 0; j < 4; ++j)\n-            i8Elems[i][j] = load(gep(elemPtrTy, ptrs[i][j], sOffsetElemVal));\n+            i8Elems[i][j] = load(gep(shemPtrTy, ptrs[i][j], sOffsetElemVal));\n \n         for (int i = 2; i < 4; ++i)\n           for (int j = 0; j < 4; ++j)\n             i8Elems[i][j] =\n-                load(gep(elemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n+                load(gep(shemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n \n         for (int m = 0; m < 4; ++m) {\n           for (int e = 0; e < 4; ++e)\n             i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n                                           i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bitcast(i8v4Elems[m], i8x4Ty);\n         }\n       } else { // k first\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[0][j] = load(gep(elemPtrTy, ptrs[0][j], sOffsetElemVal));\n+          i8Elems[0][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[2][j] = load(gep(elemPtrTy, ptrs[1][j], sOffsetElemVal));\n+          i8Elems[2][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[1][j] = load(gep(elemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n+          i8Elems[1][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[3][j] = load(gep(elemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n+          i8Elems[3][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n \n         for (int m = 0; m < 4; ++m) {\n           for (int e = 0; e < 4; ++e)\n             i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n                                           i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bitcast(i8v4Elems[m], i8x4Ty);\n         }\n       }\n \n-      return {i32Elems[0], i32Elems[1], i32Elems[2], i32Elems[3]};\n+      return {i8v4Elems[0], i8v4Elems[1], i8v4Elems[2], i8v4Elems[3]};\n     }\n \n     assert(false && \"Invalid smem load\");\n@@ -1388,7 +1391,9 @@ struct MMA16816ConversionHelper {\n       unsigned colsPerThread = numRepN * 2;\n       PTXBuilder builder;\n       auto &mma = *builder.create(helper.getMmaInstr().str());\n-      auto retArgs = builder.newListOperand(4, \"=r\");\n+      // using =r for float32 works but leads to less readable ptx.\n+      bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n+      auto retArgs = builder.newListOperand(4, isIntMMA ? \"=r\" : \"=f\");\n       auto aArgs = builder.newListOperand({\n           {ha[{m, k}], \"r\"},\n           {ha[{m + 1, k}], \"r\"},\n@@ -1407,14 +1412,10 @@ struct MMA16816ConversionHelper {\n       mma(retArgs, aArgs, bArgs, cArgs);\n       Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n \n-      auto getIntAttr = [&](int v) {\n-        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n-      };\n-\n       Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n       for (int i = 0; i < 4; ++i)\n         fc[m * colsPerThread + 4 * n + i] =\n-            extract_val(elemTy, mmaOut, getIntAttr(i));\n+            extract_val(elemTy, mmaOut, i32_arr_attr(i));\n     };\n \n     for (int k = 0; k < numRepK; ++k)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 5, "deletions": 7, "changes": 12, "file_content_changes": "@@ -177,7 +177,9 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n       PTXBuilder builder;\n       auto idx = getIdx(m, n);\n \n-      auto *resOprs = builder.newListOperand(8, \"=f\");\n+      // note: using \"=f\" for float leads to cleaner PTX\n+      bool isIntMMA = DTensorTy.getElementType().isInteger(32);\n+      auto *resOprs = builder.newListOperand(8, isIntMMA ? \"=r\" : \"=f\");\n       auto *AOprs = builder.newListOperand({\n           {ha.first, \"r\"},\n           {ha.second, \"r\"},\n@@ -201,12 +203,8 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n       Value res =\n           builder.launch(rewriter, loc, helper.getMmaRetType(ATensorTy));\n \n-      auto getIntAttr = [&](int v) {\n-        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n-      };\n-\n-      for (unsigned i = 0; i < 8; i++) {\n-        Value elem = extract_val(f32_ty, res, getIntAttr(i));\n+      for (auto i = 0; i < 8; i++) {\n+        Value elem = extract_val(f32_ty, res, i32_arr_attr(i));\n         acc[idx[i]] = elem;\n         resVals[(m * numN / 2 + n) * 8 + i] = elem;\n       }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 4, "deletions": 8, "changes": 12, "file_content_changes": "@@ -49,10 +49,8 @@ struct FpToFpOpConversion\n         struct_ty(SmallVector<Type>{fp16x2VecTy, fp16x2VecTy});\n     auto fp16x2x2Struct =\n         builder.launch(rewriter, loc, fp16x2x2StructTy, false);\n-    auto fp16x2Vec0 =\n-        extract_val(fp16x2VecTy, fp16x2x2Struct, rewriter.getI32ArrayAttr({0}));\n-    auto fp16x2Vec1 =\n-        extract_val(fp16x2VecTy, fp16x2x2Struct, rewriter.getI32ArrayAttr({1}));\n+    auto fp16x2Vec0 = extract_val(fp16x2VecTy, fp16x2x2Struct, i32_arr_attr(0));\n+    auto fp16x2Vec1 = extract_val(fp16x2VecTy, fp16x2x2Struct, i32_arr_attr(1));\n     return {extract_element(f16_ty, fp16x2Vec0, i32_val(0)),\n             extract_element(f16_ty, fp16x2Vec0, i32_val(1)),\n             extract_element(f16_ty, fp16x2Vec1, i32_val(0)),\n@@ -143,10 +141,8 @@ struct FpToFpOpConversion\n         struct_ty(SmallVector<Type>{bf16x2VecTy, bf16x2VecTy});\n     auto bf16x2x2Struct =\n         builder.launch(rewriter, loc, bf16x2x2StructTy, false);\n-    auto bf16x2Vec0 =\n-        extract_val(bf16x2VecTy, bf16x2x2Struct, rewriter.getI32ArrayAttr({0}));\n-    auto bf16x2Vec1 =\n-        extract_val(bf16x2VecTy, bf16x2x2Struct, rewriter.getI32ArrayAttr({1}));\n+    auto bf16x2Vec0 = extract_val(bf16x2VecTy, bf16x2x2Struct, i32_arr_attr(0));\n+    auto bf16x2Vec1 = extract_val(bf16x2VecTy, bf16x2x2Struct, i32_arr_attr(1));\n     return {extract_element(i16_ty, bf16x2Vec0, i32_val(0)),\n             extract_element(i16_ty, bf16x2Vec0, i32_val(1)),\n             extract_element(i16_ty, bf16x2Vec1, i32_val(0)),"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 24, "deletions": 81, "changes": 105, "file_content_changes": "@@ -14,7 +14,7 @@ using ::mlir::triton::gpu::getElemsPerThread;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n // Contains some helper functions for both Load and Store conversions.\n-struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n+struct LoadStoreConversionBase {\n   explicit LoadStoreConversionBase(AxisInfoAnalysis &axisAnalysisPass)\n       : axisAnalysisPass(axisAnalysisPass) {}\n \n@@ -214,7 +214,7 @@ struct LoadOpConversion\n         Value curr;\n         if (retTy.isa<LLVM::LLVMStructType>()) {\n           curr = extract_val(IntegerType::get(getContext(), width), ret,\n-                             rewriter.getI64ArrayAttr(ii));\n+                             i64_arr_attr(ii));\n         } else {\n           curr = ret;\n         }\n@@ -639,10 +639,9 @@ struct InsertSliceOpConversion\n     auto smemBase = gep(elemPtrTy, smemObj.base, offset);\n \n     auto llSrc = adaptor.source();\n-    auto srcIndices =\n-        emitBaseIndexForBlockedLayout(loc, rewriter, srcLayout, srcShape);\n-    storeBlockedToShared(src, llSrc, srcStrides, srcIndices, dst, smemBase,\n-                         elemTy, loc, rewriter);\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n+    storeDistributedToShared(src, llSrc, srcStrides, srcIndices, dst, smemBase,\n+                             elemTy, loc, rewriter);\n     // Barrier is not necessary.\n     // The membar pass knows that it writes to shared memory and will handle it\n     // properly.\n@@ -657,12 +656,12 @@ struct InsertSliceAsyncOpConversion\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::gpu::InsertSliceAsyncOp>::ConvertTritonGPUOpToLLVMPattern;\n \n-  InsertSliceAsyncOpConversion(LLVMTypeConverter &converter,\n-                               const Allocation *allocation, Value smem,\n-                               AxisInfoAnalysis &axisAnalysisPass,\n-                               PatternBenefit benefit)\n+  InsertSliceAsyncOpConversion(\n+      LLVMTypeConverter &converter, const Allocation *allocation, Value smem,\n+      ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+      AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::gpu::InsertSliceAsyncOp>(\n-            converter, allocation, smem, benefit),\n+            converter, allocation, smem, indexCacheInfo, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n \n   LogicalResult\n@@ -744,6 +743,9 @@ struct InsertSliceAsyncOpConversion\n     auto sizePerThread = srcBlockedLayout.getSizePerThread();\n     auto threadsPerCTA = getThreadsPerCTA(srcBlockedLayout);\n     auto inOrder = srcBlockedLayout.getOrder();\n+    DenseMap<unsigned, Value> sharedPtrs =\n+        getSwizzledSharedPtrs(loc, inVec, srcTy, resSharedLayout, resElemTy,\n+                              smemObj, rewriter, offsetVals, srcStrides);\n \n     // If perPhase * maxPhase > threadsPerCTA, we will have elements\n     // that share the same tile indices. The index calculation will\n@@ -756,61 +758,8 @@ struct InsertSliceAsyncOpConversion\n     auto numVecCols = std::max<unsigned>(inVec / outVec, 1);\n \n     auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcShape);\n-    //  <<tileVecIdxRow, tileVecIdxCol>, TileOffset>\n-    DenseMap<std::pair<unsigned, unsigned>, Value> tileOffsetMap;\n+\n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n-      // minVec = 2, inVec = 4, outVec = 2\n-      //   baseOffsetCol = 0   baseOffsetCol = 0\n-      //   tileVecIdxCol = 0   tileVecIdxCol = 1\n-      //                -/\\-   -/\\-\n-      //               [|x x| |x x| x x x x x]\n-      //               [|x x| |x x| x x x x x]\n-      // baseOffsetRow [|x x| |x x| x x x x x]\n-      //               [|x x| |x x| x x x x x]\n-      auto vecIdx = elemIdx / minVec;\n-      auto vecIdxCol = vecIdx % (sizePerThread[inOrder[0]] / minVec);\n-      auto vecIdxRow = vecIdx / (sizePerThread[inOrder[0]] / minVec);\n-      auto baseOffsetCol =\n-          vecIdxCol / numVecCols * numVecCols * threadsPerCTA[inOrder[0]];\n-      auto baseOffsetRow = vecIdxRow / numSwizzleRows * numSwizzleRows *\n-                           threadsPerCTA[inOrder[1]];\n-      auto tileVecIdxCol = vecIdxCol % numVecCols;\n-      auto tileVecIdxRow = vecIdxRow % numSwizzleRows;\n-\n-      if (!tileOffsetMap.count({tileVecIdxRow, tileVecIdxCol})) {\n-        // Swizzling\n-        // Since the swizzling index is related to outVec, and we know minVec\n-        // already, inVec doesn't matter\n-        //\n-        // (Numbers represent row indices)\n-        // Example1:\n-        // outVec = 2, inVec = 2, minVec = 2\n-        // outVec = 2, inVec = 4, minVec = 2\n-        //     | [1 2] [3 4] [5 6] ... |\n-        //     | [3 4] [1 2] [7 8] ... |\n-        //     | [5 6] [7 8] [1 2] ... |\n-        // Example2:\n-        // outVec = 4, inVec = 2, minVec = 2\n-        //     | [1 2 3 4] [5 6 7 8] [9 10 11 12] ... |\n-        //     | [5 6 7 8] [1 2 3 4] [13 14 15 16] ... |\n-        //     | [9 10 11 12] [13 14 15 16] [1 2 3 4] ... |\n-        auto srcIdx = srcIndices[tileVecIdxRow * sizePerThread[inOrder[0]]];\n-        Value phase = urem(udiv(srcIdx[inOrder[1]], i32_val(perPhase)),\n-                           i32_val(maxPhase));\n-        // srcShape and smemObj.shape maybe different if smemObj is a\n-        // slice of the original shared memory object.\n-        // So we need to use the original shape to compute the offset\n-        Value rowOffset = mul(srcIdx[inOrder[1]], srcStrides[inOrder[1]]);\n-        Value colOffset =\n-            add(srcIdx[inOrder[0]], i32_val(tileVecIdxCol * minVec));\n-        Value swizzleIdx = udiv(colOffset, i32_val(outVec));\n-        Value swizzleColOffset =\n-            add(mul(xor_(swizzleIdx, phase), i32_val(outVec)),\n-                urem(colOffset, i32_val(outVec)));\n-        Value tileOffset = add(rowOffset, swizzleColOffset);\n-        tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}] =\n-            gep(dstPtrTy, dstPtrBase, tileOffset);\n-      }\n \n       // 16 * 8 = 128bits\n       auto maxBitWidth =\n@@ -827,11 +776,7 @@ struct InsertSliceAsyncOpConversion\n       assert(byteWidth == 16 || byteWidth == 8 || byteWidth == 4);\n       auto resByteWidth = resElemTy.getIntOrFloatBitWidth() / 8;\n \n-      Value tileOffset = tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}];\n-      Value baseOffset =\n-          add(mul(i32_val(baseOffsetRow), srcStrides[inOrder[1]]),\n-              i32_val(baseOffsetCol));\n-      Value basePtr = gep(dstPtrTy, tileOffset, baseOffset);\n+      Value basePtr = sharedPtrs[elemIdx];\n       for (size_t wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n         PTXBuilder ptxBuilder;\n         auto wordElemIdx = wordIdx * numWordElems;\n@@ -857,28 +802,26 @@ struct InsertSliceAsyncOpConversion\n       }\n     }\n \n-    PTXBuilder ptxBuilder;\n-    ptxBuilder.create<>(\"cp.async.commit_group\")->operator()();\n-    ptxBuilder.launch(rewriter, loc, void_ty(getContext()));\n     rewriter.replaceOp(op, llDst);\n     return success();\n   }\n };\n \n-void populateLoadStoreOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                       RewritePatternSet &patterns,\n-                                       int numWarps,\n-                                       AxisInfoAnalysis &axisInfoAnalysis,\n-                                       const Allocation *allocation, Value smem,\n-                                       PatternBenefit benefit) {\n+void populateLoadStoreOpToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n   patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<StoreOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<AtomicCASOpConversion>(typeConverter, allocation, smem,\n                                       axisInfoAnalysis, benefit);\n   patterns.add<AtomicRMWOpConversion>(typeConverter, allocation, smem,\n                                       axisInfoAnalysis, benefit);\n   patterns.add<InsertSliceOpConversion>(typeConverter, allocation, smem,\n-                                        benefit);\n+                                        indexCacheInfo, benefit);\n   patterns.add<InsertSliceAsyncOpConversion>(typeConverter, allocation, smem,\n-                                             axisInfoAnalysis, benefit);\n+                                             indexCacheInfo, axisInfoAnalysis,\n+                                             benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.h", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -6,11 +6,11 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-void populateLoadStoreOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                       RewritePatternSet &patterns,\n-                                       int numWarps,\n-                                       AxisInfoAnalysis &axisInfoAnalysis,\n-                                       const Allocation *allocation, Value smem,\n-                                       PatternBenefit benefit);\n+void populateLoadStoreOpToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/PTXAsmFormat.cpp", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -78,9 +78,8 @@ SmallVector<PTXBuilder::Operand *, 4> PTXBuilder::getAllArgs() const {\n   return res;\n }\n \n-mlir::Value PTXBuilder::launch(ConversionPatternRewriter &rewriter,\n-                               Location loc, Type resTy, bool hasSideEffect,\n-                               bool isAlignStack,\n+mlir::Value PTXBuilder::launch(OpBuilder &rewriter, Location loc, Type resTy,\n+                               bool hasSideEffect, bool isAlignStack,\n                                ArrayRef<Attribute> attrs) const {\n   auto *ctx = rewriter.getContext();\n   auto inlineAsm = rewriter.create<LLVM::InlineAsmOp>("}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 9, "deletions": 7, "changes": 16, "file_content_changes": "@@ -164,7 +164,7 @@ struct ReduceOpConversion\n     auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n \n     SmallVector<SmallVector<unsigned>> offset =\n-        emitOffsetForBlockedLayout(srcLayout, srcShape);\n+        emitOffsetForLayout(srcLayout, srcShape);\n \n     std::map<SmallVector<unsigned>, Value> accs;\n     std::map<SmallVector<unsigned>, Value> accIndices;\n@@ -479,10 +479,12 @@ struct ReduceOpConversion\n   }\n };\n \n-void populateReduceOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                    RewritePatternSet &patterns, int numWarps,\n-                                    AxisInfoAnalysis &axisInfoAnalysis,\n-                                    const Allocation *allocation, Value smem,\n-                                    PatternBenefit benefit) {\n-  patterns.add<ReduceOpConversion>(typeConverter, allocation, smem, benefit);\n+void populateReduceOpToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n+  patterns.add<ReduceOpConversion>(typeConverter, allocation, smem,\n+                                   indexCacheInfo, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.h", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -6,10 +6,11 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-void populateReduceOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                    RewritePatternSet &patterns, int numWarps,\n-                                    AxisInfoAnalysis &axisInfoAnalysis,\n-                                    const Allocation *allocation, Value smem,\n-                                    PatternBenefit benefit);\n+void populateReduceOpToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n \n #endif\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 34, "deletions": 9, "changes": 43, "file_content_changes": "@@ -63,6 +63,7 @@ struct BroadcastOpConversion\n     auto srcShape = srcTy.getShape();\n     auto resultShape = resultTy.getShape();\n     unsigned rank = srcTy.getRank();\n+\n     assert(rank == resultTy.getRank());\n     auto order = triton::gpu::getOrder(srcLayout);\n     auto srcOffsets = emitOffsetForLayout(srcLayout, srcShape);\n@@ -272,9 +273,13 @@ struct PrintfOpConversion\n struct MakeRangeOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp> {\n \n-  MakeRangeOpConversion(LLVMTypeConverter &converter, PatternBenefit benefit)\n-      : ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp>(converter,\n-                                                             benefit) {}\n+  MakeRangeOpConversion(\n+      LLVMTypeConverter &converter,\n+      ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+      PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp>(\n+            converter, /*Allocation*/ nullptr, Value{}, indexCacheInfo,\n+            benefit) {}\n \n   LogicalResult\n   matchAndRewrite(triton::MakeRangeOp op, OpAdaptor adaptor,\n@@ -500,22 +505,42 @@ struct AsyncWaitOpConversion\n   }\n };\n \n-void populateTritonGPUToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                     RewritePatternSet &patterns, int numWarps,\n-                                     AxisInfoAnalysis &axisInfoAnalysis,\n-                                     const Allocation *allocation, Value smem,\n-                                     PatternBenefit benefit) {\n+struct AsyncCommitGroupOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::AsyncCommitGroupOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::gpu::AsyncCommitGroupOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::gpu::AsyncCommitGroupOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+\n+    PTXBuilder ptxBuilder;\n+    ptxBuilder.create<>(\"cp.async.commit_group\")->operator()();\n+    ptxBuilder.launch(rewriter, op.getLoc(), void_ty(op.getContext()));\n+    // Safe to remove the op since it doesn't have any return value.\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+};\n+\n+void populateTritonGPUToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n   patterns.add<AddPtrOpConversion>(typeConverter, benefit);\n   patterns.add<AllocTensorOpConversion>(typeConverter, allocation, smem,\n                                         benefit);\n+  patterns.add<AsyncCommitGroupOpConversion>(typeConverter, benefit);\n   patterns.add<AsyncWaitOpConversion>(typeConverter, benefit);\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n \n   patterns.add<ExtractSliceOpConversion>(typeConverter, allocation, smem,\n                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n   patterns.add<GetNumProgramsOpConversion>(typeConverter, benefit);\n-  patterns.add<MakeRangeOpConversion>(typeConverter, benefit);\n+  patterns.add<MakeRangeOpConversion>(typeConverter, indexCacheInfo, benefit);\n   patterns.add<ReturnOpConversion>(typeConverter, benefit);\n   patterns.add<PrintfOpConversion>(typeConverter, benefit);\n }\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -6,10 +6,11 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-void populateTritonGPUToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                     RewritePatternSet &patterns, int numWarps,\n-                                     AxisInfoAnalysis &axisInfoAnalysis,\n-                                     const Allocation *allocation, Value smem,\n-                                     PatternBenefit benefit);\n+void populateTritonGPUToLLVMPatterns(\n+    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 397, "deletions": 105, "changes": 502, "file_content_changes": "@@ -18,7 +18,6 @@ using ::mlir::LLVM::SharedMemoryObject;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;\n-\n // FuncOpConversion/FuncOpConversionBase is borrowed from\n // https://github.com/llvm/llvm-project/blob/fae656b2dd80246c3c6f01e9c77c49560368752c/mlir/lib/Conversion/FuncToLLVM/FuncToLLVM.cpp#L276\n // since it is not exposed on header files in mlir v14\n@@ -128,7 +127,60 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<FuncOp> {\n   }\n };\n \n-struct ConvertTritonGPUOpToLLVMPatternBase {\n+using IndexCacheKeyT = std::pair<Attribute, SmallVector<int64_t>>;\n+\n+struct CacheKeyDenseMapInfo {\n+  static IndexCacheKeyT getEmptyKey() {\n+    auto *pointer = llvm::DenseMapInfo<void *>::getEmptyKey();\n+    return std::make_pair(\n+        mlir::Attribute(static_cast<mlir::Attribute::ImplType *>(pointer)),\n+        SmallVector<int64_t>{});\n+  }\n+  static IndexCacheKeyT getTombstoneKey() {\n+    auto *pointer = llvm::DenseMapInfo<void *>::getTombstoneKey();\n+    return std::make_pair(\n+        mlir::Attribute(static_cast<mlir::Attribute::ImplType *>(pointer)),\n+        SmallVector<int64_t>{std::numeric_limits<int64_t>::max()});\n+  }\n+  static unsigned getHashValue(IndexCacheKeyT key) {\n+    return llvm::hash_combine(\n+        mlir::hash_value(key.first),\n+        llvm::hash_combine_range(key.second.begin(), key.second.end()));\n+  }\n+  static bool isEqual(IndexCacheKeyT LHS, IndexCacheKeyT RHS) {\n+    return LHS == RHS;\n+  }\n+};\n+\n+class ConvertTritonGPUOpToLLVMPatternBase {\n+public:\n+  // Two levels of value cache in emitting indices calculation:\n+  // Key: pair<layout, shape>\n+  struct IndexCacheInfo {\n+    DenseMap<IndexCacheKeyT, SmallVector<Value>, CacheKeyDenseMapInfo>\n+        *baseIndexCache;\n+    DenseMap<IndexCacheKeyT, SmallVector<SmallVector<Value>>,\n+             CacheKeyDenseMapInfo> *indexCache;\n+    OpBuilder::InsertPoint *indexInsertPoint;\n+  };\n+\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(LLVMTypeConverter &typeConverter)\n+      : converter(&typeConverter) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(LLVMTypeConverter &typeConverter,\n+                                               const Allocation *allocation,\n+                                               Value smem)\n+      : converter(&typeConverter), allocation(allocation), smem(smem) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(LLVMTypeConverter &typeConverter,\n+                                               const Allocation *allocation,\n+                                               Value smem,\n+                                               IndexCacheInfo indexCacheInfo)\n+      : converter(&typeConverter), indexCacheInfo(indexCacheInfo),\n+        allocation(allocation), smem(smem) {}\n+\n+  LLVMTypeConverter *getTypeConverter() const { return converter; }\n+\n   static Value\n   getStructFromSharedMemoryObject(Location loc,\n                                   const SharedMemoryObject &smemObj,\n@@ -139,25 +191,6 @@ struct ConvertTritonGPUOpToLLVMPatternBase {\n         LLVM::LLVMStructType::getLiteral(rewriter.getContext(), types);\n     return getStructFromElements(loc, elems, rewriter, structTy);\n   }\n-};\n-\n-template <typename SourceOp>\n-class ConvertTritonGPUOpToLLVMPattern\n-    : public ConvertOpToLLVMPattern<SourceOp>,\n-      public ConvertTritonGPUOpToLLVMPatternBase {\n-public:\n-  using OpAdaptor = typename SourceOp::Adaptor;\n-\n-  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n-                                           PatternBenefit benefit = 1)\n-      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n-\n-  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n-                                           const Allocation *allocation,\n-                                           Value smem,\n-                                           PatternBenefit benefit = 1)\n-      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n-        allocation(allocation), smem(smem) {}\n \n   Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n     auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n@@ -169,6 +202,204 @@ class ConvertTritonGPUOpToLLVMPattern\n     return threadId;\n   }\n \n+  // -----------------------------------------------------------------------\n+  // Shared memory utilities\n+  // -----------------------------------------------------------------------\n+  template <typename T>\n+  Value getSharedMemoryBase(Location loc, ConversionPatternRewriter &rewriter,\n+                            T value) const {\n+\n+    auto ptrTy = LLVM::LLVMPointerType::get(\n+        this->getTypeConverter()->convertType(rewriter.getI8Type()), 3);\n+    auto bufferId = allocation->getBufferId(value);\n+    assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n+    size_t offset = allocation->getOffset(bufferId);\n+    Value offVal = idx_val(offset);\n+    Value base = gep(ptrTy, smem, offVal);\n+    return base;\n+  }\n+\n+  DenseMap<unsigned, Value>\n+  getSwizzledSharedPtrs(Location loc, unsigned inVec, RankedTensorType srcTy,\n+                        triton::gpu::SharedEncodingAttr resSharedLayout,\n+                        Type resElemTy, SharedMemoryObject smemObj,\n+                        ConversionPatternRewriter &rewriter,\n+                        SmallVectorImpl<Value> &offsetVals,\n+                        SmallVectorImpl<Value> &srcStrides) const {\n+    // This utililty computes the pointers for accessing the provided swizzled\n+    // shared memory layout `resSharedLayout`. More specifically, it computes,\n+    // for all indices (row, col) of `srcEncoding` such that idx % inVec = 0,\n+    // the pointer: ptr[(row, col)] = base + (rowOff * strides[ord[1]] + colOff)\n+    // where :\n+    //   compute phase = (row // perPhase) % maxPhase\n+    //   rowOff = row\n+    //   colOff = colOffSwizzled + colOffOrdered\n+    //     colOffSwizzled = ((col // outVec) ^ phase) * outVec\n+    //     colOffOrdered = (col % outVec) // minVec * minVec\n+    //\n+    // Note 1:\n+    // -------\n+    // Because swizzling happens at a granularity of outVec, we need to\n+    // decompose the offset into a swizzled factor and a non-swizzled (ordered)\n+    // factor\n+    //\n+    // Note 2:\n+    // -------\n+    // If we have x, y, z of the form:\n+    // x = 0b00000xxxx\n+    // y = 0byyyyy0000\n+    // z = 0b00000zzzz\n+    // then (x + y) XOR z = 0byyyyxxxx XOR 0b00000zzzz = (x XOR z) + y\n+    // This means that we can use some immediate offsets for shared memory\n+    // operations.\n+    auto dstPtrTy = ptr_ty(resElemTy, 3);\n+    auto dstOffset = dot(rewriter, loc, offsetVals, smemObj.strides);\n+    Value dstPtrBase = gep(dstPtrTy, smemObj.base, dstOffset);\n+\n+    auto srcEncoding = srcTy.getEncoding();\n+    auto srcShape = srcTy.getShape();\n+    unsigned numElems = triton::gpu::getElemsPerThread(srcTy);\n+    // swizzling params as described in TritonGPUAttrDefs.td\n+    unsigned outVec = resSharedLayout.getVec();\n+    unsigned perPhase = resSharedLayout.getPerPhase();\n+    unsigned maxPhase = resSharedLayout.getMaxPhase();\n+    // order\n+    auto inOrder = triton::gpu::getOrder(srcEncoding);\n+    auto outOrder = triton::gpu::getOrder(resSharedLayout);\n+    // tensor indices held by the current thread, as LLVM values\n+    auto srcIndices = emitIndices(loc, rewriter, srcEncoding, srcShape);\n+    // return values\n+    DenseMap<unsigned, Value> ret;\n+    // cache for non-immediate offsets\n+    DenseMap<unsigned, Value> cacheCol, cacheRow;\n+    unsigned minVec = std::min(outVec, inVec);\n+    for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n+      // extract multi dimensional index for current element\n+      auto idx = srcIndices[elemIdx];\n+      Value idxCol = idx[inOrder[0]]; // contiguous dimension\n+      Value idxRow = idx[inOrder[1]]; // discontiguous dimension\n+      Value strideCol = srcStrides[inOrder[0]];\n+      Value strideRow = srcStrides[inOrder[1]];\n+      // extract dynamic/static offset for immediate offseting\n+      unsigned immedateOffCol = 0;\n+      if (auto add = dyn_cast_or_null<LLVM::AddOp>(idxCol.getDefiningOp()))\n+        if (auto _cst = dyn_cast_or_null<LLVM::ConstantOp>(\n+                add.getRhs().getDefiningOp())) {\n+          unsigned cst =\n+              _cst.getValue().cast<IntegerAttr>().getValue().getSExtValue();\n+          unsigned key = cst % (outVec * maxPhase);\n+          cacheCol.insert({key, idxCol});\n+          idxCol = cacheCol[key];\n+          immedateOffCol = cst / (outVec * maxPhase) * (outVec * maxPhase);\n+        }\n+      // extract dynamic/static offset for immediate offseting\n+      unsigned immedateOffRow = 0;\n+      if (auto add = dyn_cast_or_null<LLVM::AddOp>(idxRow.getDefiningOp()))\n+        if (auto _cst = dyn_cast_or_null<LLVM::ConstantOp>(\n+                add.getRhs().getDefiningOp())) {\n+          unsigned cst =\n+              _cst.getValue().cast<IntegerAttr>().getValue().getSExtValue();\n+          unsigned key = cst % (perPhase * maxPhase);\n+          cacheRow.insert({key, idxRow});\n+          idxRow = cacheRow[key];\n+          immedateOffRow = cst / (perPhase * maxPhase) * (perPhase * maxPhase);\n+        }\n+      // compute phase = (row // perPhase) % maxPhase\n+      Value phase = urem(udiv(idxRow, i32_val(perPhase)), i32_val(maxPhase));\n+      // row offset is simply row index\n+      Value rowOff = mul(idxRow, strideRow);\n+      // because swizzling happens at a granularity of outVec, we need to\n+      // decompose the offset into a swizzled factor and a non-swizzled\n+      // (ordered) factor: colOffSwizzled = ((col // outVec) ^ phase) * outVec\n+      // colOffOrdered = (col % outVec) // minVec * minVec\n+      Value colOffSwizzled = xor_(udiv(idxCol, i32_val(outVec)), phase);\n+      colOffSwizzled = mul(colOffSwizzled, i32_val(outVec));\n+      Value colOffOrdered = urem(idxCol, i32_val(outVec));\n+      colOffOrdered = udiv(colOffOrdered, i32_val(minVec));\n+      colOffOrdered = mul(colOffOrdered, i32_val(minVec));\n+      Value colOff = add(colOffSwizzled, colOffOrdered);\n+      // compute non-immediate offset\n+      Value offset = add(rowOff, mul(colOff, strideCol));\n+      Value currPtr = gep(dstPtrTy, dstPtrBase, offset);\n+      // compute immediate offset\n+      Value immedateOff =\n+          add(mul(i32_val(immedateOffRow), srcStrides[inOrder[1]]),\n+              i32_val(immedateOffCol));\n+      ret[elemIdx] = gep(dstPtrTy, currPtr, immedateOff);\n+    }\n+    return ret;\n+  }\n+\n+  bool isMmaToDotShortcut(\n+      MmaEncodingAttr &mmaLayout,\n+      triton::gpu::DotOperandEncodingAttr &dotOperandLayout) const {\n+    // dot_op<opIdx=0, parent=#mma> = #mma\n+    // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n+    return mmaLayout.getWarpsPerCTA()[1] == 1 &&\n+           dotOperandLayout.getOpIdx() == 0 &&\n+           dotOperandLayout.getParent() == mmaLayout;\n+  }\n+\n+  void storeDistributedToShared(Value src, Value llSrc,\n+                                ArrayRef<Value> dstStrides,\n+                                ArrayRef<SmallVector<Value>> srcIndices,\n+                                Value dst, Value smemBase, Type elemTy,\n+                                Location loc,\n+                                ConversionPatternRewriter &rewriter) const {\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto srcShape = srcTy.getShape();\n+    assert(srcShape.size() == 2 &&\n+           \"Unexpected rank of storeDistributedToShared\");\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto srcDistributedLayout = srcTy.getEncoding();\n+    if (auto mmaLayout = srcDistributedLayout.dyn_cast<MmaEncodingAttr>()) {\n+      assert((!mmaLayout.isVolta()) &&\n+             \"ConvertLayout MMAv1->Shared is not suppported yet\");\n+    }\n+    auto dstSharedLayout =\n+        dstTy.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n+    auto dstElemTy = dstTy.getElementType();\n+    auto inOrd = triton::gpu::getOrder(srcDistributedLayout);\n+    auto outOrd = dstSharedLayout.getOrder();\n+    unsigned inVec =\n+        inOrd == outOrd\n+            ? triton::gpu::getContigPerThread(srcDistributedLayout)[inOrd[0]]\n+            : 1;\n+    unsigned outVec = dstSharedLayout.getVec();\n+    unsigned minVec = std::min(outVec, inVec);\n+    unsigned perPhase = dstSharedLayout.getPerPhase();\n+    unsigned maxPhase = dstSharedLayout.getMaxPhase();\n+    unsigned numElems = triton::gpu::getElemsPerThread(srcTy);\n+    assert(numElems == srcIndices.size());\n+    auto inVals = LLVM::getElementsFromStruct(loc, llSrc, rewriter);\n+    auto wordTy = vec_ty(elemTy, minVec);\n+    auto elemPtrTy = ptr_ty(elemTy);\n+    Value outVecVal = i32_val(outVec);\n+    Value minVecVal = i32_val(minVec);\n+    Value word;\n+\n+    SmallVector<Value> srcStrides = {dstStrides[0], dstStrides[1]};\n+    SmallVector<Value> offsetVals = {i32_val(0), i32_val(0)};\n+    SharedMemoryObject smemObj(smemBase, srcStrides, offsetVals);\n+\n+    DenseMap<unsigned, Value> sharedPtrs =\n+        getSwizzledSharedPtrs(loc, inVec, srcTy, dstSharedLayout, dstElemTy,\n+                              smemObj, rewriter, offsetVals, srcStrides);\n+\n+    std::map<unsigned, Value> cache0;\n+    std::map<unsigned, Value> cache1;\n+    for (unsigned i = 0; i < numElems; ++i) {\n+      if (i % minVec == 0)\n+        word = undef(wordTy);\n+      word = insert_element(wordTy, word, inVals[i], i32_val(i % minVec));\n+      if (i % minVec == minVec - 1) {\n+        Value smemAddr = sharedPtrs[i / minVec * minVec];\n+        smemAddr = bitcast(smemAddr, ptr_ty(wordTy, 3));\n+        store(word, smemAddr);\n+      }\n+    }\n+  }\n+\n   // -----------------------------------------------------------------------\n   // Utilities\n   // -----------------------------------------------------------------------\n@@ -242,6 +473,116 @@ class ConvertTritonGPUOpToLLVMPattern\n     return ret;\n   }\n \n+  struct SmallVectorKeyInfo {\n+    static unsigned getHashValue(const SmallVector<unsigned> &key) {\n+      return llvm::hash_combine_range(key.begin(), key.end());\n+    }\n+    static bool isEqual(const SmallVector<unsigned> &lhs,\n+                        const SmallVector<unsigned> &rhs) {\n+      return lhs == rhs;\n+    }\n+    static SmallVector<unsigned> getEmptyKey() {\n+      return SmallVector<unsigned>();\n+    }\n+    static SmallVector<unsigned> getTombstoneKey() {\n+      return {std::numeric_limits<unsigned>::max()};\n+    }\n+  };\n+\n+  // -----------------------------------------------------------------------\n+  // Get offsets / indices for any layout\n+  // -----------------------------------------------------------------------\n+\n+  SmallVector<Value> emitBaseIndexForLayout(Location loc,\n+                                            ConversionPatternRewriter &rewriter,\n+                                            const Attribute &layout,\n+                                            ArrayRef<int64_t> shape) const {\n+    IndexCacheKeyT key = std::make_pair(layout, llvm::to_vector(shape));\n+    auto cache = indexCacheInfo.baseIndexCache;\n+    assert(cache && \"baseIndexCache is nullptr\");\n+    auto insertPt = indexCacheInfo.indexInsertPoint;\n+    if (cache->count(key) > 0) {\n+      return cache->lookup(key);\n+    } else {\n+      ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+      restoreInsertionPointIfSet(insertPt, rewriter);\n+      SmallVector<Value> result;\n+      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+        result =\n+            emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n+      } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+        if (mmaLayout.isVolta())\n+          result = emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, shape);\n+        if (mmaLayout.isAmpere())\n+          result = emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, shape);\n+      } else {\n+        llvm_unreachable(\"unsupported emitBaseIndexForLayout\");\n+      }\n+      cache->insert(std::make_pair(key, result));\n+      *insertPt = rewriter.saveInsertionPoint();\n+      return result;\n+    }\n+  }\n+\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForLayout(const Attribute &layout, ArrayRef<int64_t> shape) const {\n+    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n+      return emitOffsetForBlockedLayout(blockedLayout, shape);\n+    if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+      if (mmaLayout.isVolta())\n+        return emitOffsetForMmaLayoutV1(mmaLayout, shape);\n+      if (mmaLayout.isAmpere())\n+        return emitOffsetForMmaLayoutV2(mmaLayout, shape);\n+    }\n+    llvm_unreachable(\"unsupported emitOffsetForLayout\");\n+  }\n+\n+  // -----------------------------------------------------------------------\n+  // Emit indices\n+  // -----------------------------------------------------------------------\n+  SmallVector<SmallVector<Value>> emitIndices(Location loc,\n+                                              ConversionPatternRewriter &b,\n+                                              const Attribute &layout,\n+                                              ArrayRef<int64_t> shape) const {\n+    IndexCacheKeyT key(layout, llvm::to_vector(shape));\n+    auto cache = indexCacheInfo.indexCache;\n+    assert(cache && \"indexCache is nullptr\");\n+    auto insertPt = indexCacheInfo.indexInsertPoint;\n+    if (cache->count(key) > 0) {\n+      return cache->lookup(key);\n+    } else {\n+      ConversionPatternRewriter::InsertionGuard guard(b);\n+      restoreInsertionPointIfSet(insertPt, b);\n+      SmallVector<SmallVector<Value>> result;\n+      if (auto blocked = layout.dyn_cast<BlockedEncodingAttr>()) {\n+        result = emitIndicesForDistributedLayout(loc, b, blocked, shape);\n+      } else if (auto mma = layout.dyn_cast<MmaEncodingAttr>()) {\n+        result = emitIndicesForDistributedLayout(loc, b, mma, shape);\n+      } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n+        result = emitIndicesForSliceLayout(loc, b, slice, shape);\n+      } else {\n+        llvm_unreachable(\n+            \"emitIndices for layouts other than blocked & slice not \"\n+            \"implemented yet\");\n+      }\n+      cache->insert(std::make_pair(key, result));\n+      *insertPt = b.saveInsertionPoint();\n+      return result;\n+    }\n+  }\n+\n+private:\n+  void restoreInsertionPointIfSet(OpBuilder::InsertPoint *insertPt,\n+                                  ConversionPatternRewriter &rewriter) const {\n+    if (insertPt->isSet()) {\n+      rewriter.restoreInsertionPoint(*insertPt);\n+    } else {\n+      auto func =\n+          rewriter.getInsertionPoint()->getParentOfType<LLVM::LLVMFuncOp>();\n+      rewriter.setInsertionPointToStart(&func.getBody().front());\n+    }\n+  }\n+\n   // -----------------------------------------------------------------------\n   // Blocked layout indices\n   // -----------------------------------------------------------------------\n@@ -411,38 +752,6 @@ class ConvertTritonGPUOpToLLVMPattern\n     return ret;\n   }\n \n-  // -----------------------------------------------------------------------\n-  // Get offsets / indices for any layout\n-  // -----------------------------------------------------------------------\n-\n-  SmallVector<Value> emitBaseIndexForLayout(Location loc,\n-                                            ConversionPatternRewriter &rewriter,\n-                                            const Attribute &layout,\n-                                            ArrayRef<int64_t> shape) const {\n-    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n-      return emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n-    if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-      if (mmaLayout.isVolta())\n-        return emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, shape);\n-      if (mmaLayout.isAmpere())\n-        return emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, shape);\n-    }\n-    llvm_unreachable(\"unsupported emitBaseIndexForLayout\");\n-  }\n-\n-  SmallVector<SmallVector<unsigned>>\n-  emitOffsetForLayout(const Attribute &layout, ArrayRef<int64_t> shape) const {\n-    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n-      return emitOffsetForBlockedLayout(blockedLayout, shape);\n-    if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-      if (mmaLayout.isVolta())\n-        return emitOffsetForMmaLayoutV1(mmaLayout, shape);\n-      if (mmaLayout.isAmpere())\n-        return emitOffsetForMmaLayoutV2(mmaLayout, shape);\n-    }\n-    llvm_unreachable(\"unsupported emitOffsetForLayout\");\n-  }\n-\n   // Emit indices calculation within each ConversionPattern, and returns a\n   // [elemsPerThread X rank] index matrix.\n \n@@ -470,22 +779,6 @@ class ConvertTritonGPUOpToLLVMPattern\n     return multiDimIdx;\n   }\n \n-  struct SmallVectorKeyInfo {\n-    static unsigned getHashValue(const SmallVector<unsigned> &key) {\n-      return llvm::hash_combine_range(key.begin(), key.end());\n-    }\n-    static bool isEqual(const SmallVector<unsigned> &lhs,\n-                        const SmallVector<unsigned> &rhs) {\n-      return lhs == rhs;\n-    }\n-    static SmallVector<unsigned> getEmptyKey() {\n-      return SmallVector<unsigned>();\n-    }\n-    static SmallVector<unsigned> getTombstoneKey() {\n-      return {std::numeric_limits<unsigned>::max()};\n-    }\n-  };\n-\n   SmallVector<SmallVector<Value>>\n   emitIndicesForSliceLayout(Location loc, ConversionPatternRewriter &rewriter,\n                             const SliceEncodingAttr &sliceLayout,\n@@ -505,46 +798,45 @@ class ConvertTritonGPUOpToLLVMPattern\n     return resultIndices;\n   }\n \n-  // -----------------------------------------------------------------------\n-  // Emit indices\n-  // -----------------------------------------------------------------------\n-  SmallVector<SmallVector<Value>> emitIndices(Location loc,\n-                                              ConversionPatternRewriter &b,\n-                                              const Attribute &layout,\n-                                              ArrayRef<int64_t> shape) const {\n-    if (auto blocked = layout.dyn_cast<BlockedEncodingAttr>()) {\n-      return emitIndicesForDistributedLayout(loc, b, blocked, shape);\n-    } else if (auto mma = layout.dyn_cast<MmaEncodingAttr>()) {\n-      return emitIndicesForDistributedLayout(loc, b, mma, shape);\n-    } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n-      return emitIndicesForSliceLayout(loc, b, slice, shape);\n-    } else {\n-      assert(0 && \"emitIndices for layouts other than blocked & slice not \"\n-                  \"implemented yet\");\n-      return {};\n-    }\n-  }\n+protected:\n+  LLVMTypeConverter *converter;\n+  const Allocation *allocation;\n+  Value smem;\n+  IndexCacheInfo indexCacheInfo;\n+};\n \n-  // -----------------------------------------------------------------------\n-  // Shared memory utilities\n-  // -----------------------------------------------------------------------\n-  template <typename T>\n-  Value getSharedMemoryBase(Location loc, ConversionPatternRewriter &rewriter,\n-                            T value) const {\n+template <typename SourceOp>\n+class ConvertTritonGPUOpToLLVMPattern\n+    : public ConvertOpToLLVMPattern<SourceOp>,\n+      public ConvertTritonGPUOpToLLVMPatternBase {\n+public:\n+  using OpAdaptor = typename SourceOp::Adaptor;\n \n-    auto ptrTy = LLVM::LLVMPointerType::get(\n-        this->getTypeConverter()->convertType(rewriter.getI8Type()), 3);\n-    auto bufferId = allocation->getBufferId(value);\n-    assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n-    size_t offset = allocation->getOffset(bufferId);\n-    Value offVal = idx_val(offset);\n-    Value base = gep(ptrTy, smem, offVal);\n-    return base;\n-  }\n+  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n+                                           PatternBenefit benefit = 1)\n+      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n+                                           const Allocation *allocation,\n+                                           Value smem,\n+                                           PatternBenefit benefit = 1)\n+      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation, smem) {}\n+\n+  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n+                                           const Allocation *allocation,\n+                                           Value smem,\n+                                           IndexCacheInfo indexCacheInfo,\n+                                           PatternBenefit benefit = 1)\n+      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation, smem,\n+                                            indexCacheInfo) {}\n \n protected:\n-  const Allocation *allocation;\n-  Value smem;\n+  LLVMTypeConverter *getTypeConverter() const {\n+    return ((ConvertTritonGPUOpToLLVMPatternBase *)this)->getTypeConverter();\n+  }\n };\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 20, "deletions": 4, "changes": 24, "file_content_changes": "@@ -170,16 +170,20 @@ class ConvertTritonGPUToLLVM\n     // We set a higher benefit here to ensure triton's patterns runs before\n     // arith patterns for some encoding not supported by the community\n     // patterns.\n+    OpBuilder::InsertPoint indexInsertPoint;\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo indexCacheInfo{\n+        &baseIndexCache, &indexCache, &indexInsertPoint};\n+\n     RewritePatternSet patterns(context);\n \n     // Normal conversions\n     populateTritonGPUToLLVMPatterns(typeConverter, patterns, numWarps,\n                                     axisInfoAnalysis, &allocation, smem,\n-                                    /*benefit=*/10);\n+                                    indexCacheInfo, /*benefit=*/10);\n     // ConvertLayoutOp\n     populateConvertLayoutOpToLLVMPatterns(typeConverter, patterns, numWarps,\n                                           axisInfoAnalysis, &allocation, smem,\n-                                          /*benefit=*/10);\n+                                          indexCacheInfo, /*benefit=*/10);\n     // DotOp\n     populateDotOpToLLVMPatterns(typeConverter, patterns, numWarps,\n                                 axisInfoAnalysis, &allocation, smem,\n@@ -191,11 +195,11 @@ class ConvertTritonGPUToLLVM\n     // LoadStoreOp\n     populateLoadStoreOpToLLVMPatterns(typeConverter, patterns, numWarps,\n                                       axisInfoAnalysis, &allocation, smem,\n-                                      /*benefit=*/10);\n+                                      indexCacheInfo, /*benefit=*/10);\n     // ReduceOp\n     populateReduceOpToLLVMPatterns(typeConverter, patterns, numWarps,\n                                    axisInfoAnalysis, &allocation, smem,\n-                                   /*benefit=*/10);\n+                                   indexCacheInfo, /*benefit=*/10);\n     // ViewOp\n     populateViewOpToLLVMPatterns(typeConverter, patterns, numWarps,\n                                  axisInfoAnalysis, &allocation, smem,\n@@ -215,6 +219,13 @@ class ConvertTritonGPUToLLVM\n private:\n   Value smem;\n \n+  using IndexCacheKeyT = std::pair<Attribute, SmallVector<int64_t>>;\n+  DenseMap<IndexCacheKeyT, SmallVector<Value>, CacheKeyDenseMapInfo>\n+      baseIndexCache;\n+  DenseMap<IndexCacheKeyT, SmallVector<SmallVector<Value>>,\n+           CacheKeyDenseMapInfo>\n+      indexCache;\n+\n   int computeCapability{};\n \n   void initSharedMemory(size_t size,\n@@ -377,6 +388,11 @@ class ConvertTritonGPUToLLVM\n       decomposed = true;\n     });\n \n+    mod.walk([&](triton::gpu::AsyncCommitGroupOp asyncCommitGroupOp) -> void {\n+      if (!triton::gpu::AsyncCommitGroupOp::isSupported(computeCapability))\n+        asyncCommitGroupOp.erase();\n+    });\n+\n     mod.walk([&](triton::gpu::AsyncWaitOp asyncWaitOp) -> void {\n       if (!triton::gpu::AsyncWaitOp::isSupported(computeCapability)) {\n         // async wait is supported in Ampere and later"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.h", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -90,7 +90,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n         Type elemTy = convertType(type.getElementType());\n         if (mmaLayout.isAmpere()) {\n           const llvm::DenseMap<int, Type> targetTyMap = {\n-              {32, elemTy},\n+              {32, vec_ty(elemTy, 1)},\n               {16, vec_ty(elemTy, 2)},\n               {8, vec_ty(elemTy, 4)},\n           };\n@@ -103,8 +103,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n           if (dotOpLayout.getOpIdx() == 0) { // $a\n             auto elems =\n                 MMA16816ConversionHelper::getANumElemsPerThread(type, wpt[0]);\n-            return LLVM::LLVMStructType::getLiteral(\n-                ctx, SmallVector<Type>(elems, targetTy));\n+            return struct_ty(SmallVector<Type>(elems, targetTy));\n           }\n           if (dotOpLayout.getOpIdx() == 1) { // $b\n             auto elems ="}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -95,6 +95,10 @@\n                             __VA_ARGS__)\n #define tid_val() getThreadId(rewriter, loc)\n \n+// Attributes\n+#define i32_arr_attr(...) rewriter.getI32ArrayAttr({__VA_ARGS__})\n+#define i64_arr_attr(...) rewriter.getI64ArrayAttr({__VA_ARGS__})\n+\n namespace mlir {\n namespace triton {\n \n@@ -191,7 +195,7 @@ getElementsFromStruct(Location loc, Value llvmStruct,\n   SmallVector<Value> results(types.size());\n   for (unsigned i = 0; i < types.size(); ++i) {\n     Type type = types[i];\n-    results[i] = extract_val(type, llvmStruct, rewriter.getI64ArrayAttr(i));\n+    results[i] = extract_val(type, llvmStruct, i64_arr_attr(i));\n   }\n   return results;\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 19, "deletions": 2, "changes": 21, "file_content_changes": "@@ -62,28 +62,45 @@ struct SplatOpConversion\n     auto tensorTy = resType.cast<RankedTensorType>();\n     auto shape = tensorTy.getShape();\n     auto parent = layout.getParent();\n+    Value retVal = constVal;\n+    Type retTy = elemType;\n     int numElems{};\n     if (auto mmaLayout = parent.dyn_cast<MmaEncodingAttr>()) {\n+      Type matTy;\n       if (mmaLayout.isAmpere()) {\n         numElems = layout.getOpIdx() == 0\n                        ? MMA16816ConversionHelper::getANumElemsPerThread(\n                              tensorTy, mmaLayout.getWarpsPerCTA()[0])\n                        : MMA16816ConversionHelper::getBNumElemsPerThread(\n                              tensorTy, mmaLayout.getWarpsPerCTA()[1]);\n+        DotOpMmaV2ConversionHelper helper(mmaLayout);\n+        helper.deduceMmaType(tensorTy);\n+        matTy = helper.getMatType();\n       } else if (mmaLayout.isVolta()) {\n         DotOpMmaV1ConversionHelper helper(mmaLayout);\n         numElems = layout.getOpIdx() == 0\n                        ? helper.numElemsPerThreadA(shape, {0, 1})\n                        : helper.numElemsPerThreadB(shape, {0, 1});\n+        matTy = helper.getMatType(tensorTy);\n+      }\n+      auto numPackedElems = matTy.cast<LLVM::LLVMStructType>()\n+                                .getBody()[0]\n+                                .cast<VectorType>()\n+                                .getNumElements();\n+      retTy = vec_ty(elemType, numPackedElems);\n+      retVal = undef(retTy);\n+      for (auto i = 0; i < numPackedElems; ++i) {\n+        retVal = insert_element(retTy, retVal, constVal, i32_val(i));\n       }\n     } else if (auto blockedLayout = parent.dyn_cast<BlockedEncodingAttr>()) {\n       numElems = DotOpFMAConversionHelper::getNumElemsPerThread(shape, layout);\n     } else {\n       assert(false && \"Unsupported layout found\");\n     }\n+\n     auto structTy = LLVM::LLVMStructType::getLiteral(\n-        rewriter.getContext(), SmallVector<Type>(numElems, elemType));\n-    return getStructFromElements(loc, SmallVector<Value>(numElems, constVal),\n+        rewriter.getContext(), SmallVector<Type>(numElems, retTy));\n+    return getStructFromElements(loc, SmallVector<Value>(numElems, retVal),\n                                  rewriter, structTy);\n   }\n "}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -319,20 +319,7 @@ struct TritonTransPattern : public OpConversionPattern<triton::TransOp> {\n       src = rewriter.create<triton::gpu::ConvertLayoutOp>(src.getLoc(), srcType,\n                                                           src);\n     }\n-    auto srcSharedEncoding =\n-        srcEncoding.cast<triton::gpu::SharedEncodingAttr>();\n-    SmallVector<unsigned> retOrder(srcSharedEncoding.getOrder().begin(),\n-                                   srcSharedEncoding.getOrder().end());\n-    SmallVector<int64_t> retShapes(srcType.getShape().begin(),\n-                                   srcType.getShape().end());\n-    std::reverse(retOrder.begin(), retOrder.end());\n-    std::reverse(retShapes.begin(), retShapes.end());\n-    auto retEncoding =\n-        triton::gpu::SharedEncodingAttr::get(getContext(), 1, 1, 1, retOrder);\n-    auto retType =\n-        RankedTensorType::get(retShapes, srcType.getElementType(), retEncoding);\n-\n-    rewriter.replaceOpWithNewOp<triton::TransOp>(op, retType, src);\n+    rewriter.replaceOpWithNewOp<triton::TransOp>(op, src);\n     return success();\n   }\n };"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -206,6 +206,33 @@ void LoadOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n   state.addTypes({resultType});\n }\n \n+//-- TransOp --\n+mlir::LogicalResult mlir::triton::TransOp::inferReturnTypes(\n+    MLIRContext *context, Optional<Location> location, ValueRange operands,\n+    DictionaryAttr attributes, RegionRange regions,\n+    SmallVectorImpl<Type> &inferredReturnTypes) {\n+  // type is the same as the input\n+  auto argTy = operands[0].getType().cast<RankedTensorType>();\n+  SmallVector<int64_t> retShape(argTy.getShape().begin(),\n+                                argTy.getShape().end());\n+  std::reverse(retShape.begin(), retShape.end());\n+  auto retEltTy = argTy.getElementType();\n+  Attribute argEncoding = argTy.getEncoding();\n+  Attribute retEncoding;\n+  if (argEncoding) {\n+    Dialect &dialect = argEncoding.getDialect();\n+    auto inferLayoutInterface = dyn_cast<DialectInferLayoutInterface>(&dialect);\n+    if (inferLayoutInterface->inferTransOpEncoding(argEncoding, retEncoding)\n+            .failed()) {\n+      llvm::report_fatal_error(\"failed to infer layout for ReduceOp\");\n+      return mlir::failure();\n+    }\n+  }\n+  inferredReturnTypes.push_back(\n+      RankedTensorType::get(retShape, retEltTy, retEncoding));\n+  return mlir::success();\n+}\n+\n //-- DotOp --\n mlir::LogicalResult mlir::triton::DotOp::inferReturnTypes(\n     MLIRContext *context, Optional<Location> location, ValueRange operands,"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 37, "deletions": 5, "changes": 42, "file_content_changes": "@@ -104,7 +104,10 @@ SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n     return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n                                  blockedLayout.getSizePerThread().end());\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-    return getSizePerThread(sliceLayout.getParent());\n+    auto ret = getSizePerThread(sliceLayout.getParent());\n+    return ret;\n+    // ret.erase(ret.begin() + sliceLayout.getDim());\n+    return ret;\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     if (mmaLayout.isAmpere()) {\n       return {2, 2};\n@@ -158,7 +161,11 @@ SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout) {\n       threads.push_back(blockedLayout.getThreadsPerWarp()[d] *\n                         blockedLayout.getWarpsPerCTA()[d]);\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    assert(0 && \"Unimplemented usage of MmaEncodingAttr\");\n+    if (mmaLayout.getVersionMajor() == 2) {\n+      threads = {8 * mmaLayout.getWarpsPerCTA()[0],\n+                 4 * mmaLayout.getWarpsPerCTA()[1]};\n+    } else\n+      assert(0 && \"Unimplemented usage of MmaEncodingAttr\");\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n   }\n@@ -254,6 +261,11 @@ SmallVector<unsigned> getOrder(const Attribute &layout) {\n   }\n };\n \n+bool isaDistributedLayout(const Attribute &layout) {\n+  return layout.isa<BlockedEncodingAttr>() || layout.isa<MmaEncodingAttr>() ||\n+         layout.isa<SliceEncodingAttr>();\n+}\n+\n } // namespace gpu\n } // namespace triton\n } // namespace mlir\n@@ -588,15 +600,20 @@ bool MmaEncodingAttr::isVolta() const { return getVersionMajor() == 1; }\n \n bool MmaEncodingAttr::isAmpere() const { return getVersionMajor() == 2; }\n \n-// Get [isARow, isBRow, isAVec4, isBVec4] from versionMinor\n-std::tuple<bool, bool, bool, bool>\n+// Get [isARow, isBRow, isAVec4, isBVec4, id] from versionMinor\n+std::tuple<bool, bool, bool, bool, int>\n MmaEncodingAttr::decodeVoltaLayoutStates() const {\n   unsigned versionMinor = getVersionMinor();\n   bool isARow = versionMinor & (1 << 0);\n   bool isBRow = versionMinor & (1 << 1);\n   bool isAVec4 = versionMinor & (1 << 2);\n   bool isBVec4 = versionMinor & (1 << 3);\n-  return std::make_tuple(isARow, isBRow, isAVec4, isBVec4);\n+\n+  int id = 0;\n+  for (int i = numBitsToHoldMmaV1ID - 1; i >= 0; --i)\n+    id = (id << 1) + static_cast<bool>(versionMinor & (1 << (4 + i)));\n+\n+  return std::make_tuple(isARow, isBRow, isAVec4, isBVec4, id);\n }\n \n //===----------------------------------------------------------------------===//\n@@ -729,6 +746,21 @@ struct TritonGPUInferLayoutInterface\n     return success();\n   }\n \n+  LogicalResult inferTransOpEncoding(Attribute operandEncoding,\n+                                     Attribute &resultEncoding) const override {\n+    SharedEncodingAttr sharedEncoding =\n+        operandEncoding.dyn_cast<SharedEncodingAttr>();\n+    if (!sharedEncoding)\n+      return failure();\n+    SmallVector<unsigned> retOrder(sharedEncoding.getOrder().begin(),\n+                                   sharedEncoding.getOrder().end());\n+    std::reverse(retOrder.begin(), retOrder.end());\n+    resultEncoding = SharedEncodingAttr::get(\n+        getDialect()->getContext(), sharedEncoding.getVec(),\n+        sharedEncoding.getPerPhase(), sharedEncoding.getMaxPhase(), retOrder);\n+    return mlir::success();\n+  }\n+\n   LogicalResult\n   inferExpandDimsOpEncoding(Attribute operandEncoding, unsigned axis,\n                             Attribute &resultEncoding,"}, {"filename": "lib/Dialect/TritonGPU/Transforms/CMakeLists.txt", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -8,7 +8,11 @@ add_mlir_dialect_library(TritonGPUTransforms\n   Combine.cpp\n   Pipeline.cpp\n   Prefetch.cpp\n+  ReorderInstructions.cpp\n+  DecomposeConversions.cpp\n   TritonGPUConversion.cpp\n+  UpdateMmaForVolta.cpp\n+  Utility.cpp\n \n   DEPENDS\n   TritonGPUTransformsIncGen"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 139, "deletions": 61, "changes": 200, "file_content_changes": "@@ -1,3 +1,4 @@\n+#include \"Utility.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/SCF/SCF.h\"\n #include \"mlir/IR/BlockAndValueMapping.h\"\n@@ -22,6 +23,11 @@\n using namespace mlir;\n namespace {\n #include \"TritonGPUCombine.inc\"\n+using triton::DotOp;\n+using triton::gpu::ConvertLayoutOp;\n+using triton::gpu::DotOperandEncodingAttr;\n+using triton::gpu::MmaEncodingAttr;\n+using triton::gpu::SliceEncodingAttr;\n \n // -----------------------------------------------------------------------------\n //\n@@ -478,7 +484,7 @@ class FoldConvertAndReduce : public mlir::RewritePattern {\n       return op->getBlock() == cvt->getBlock() &&\n              !(isa<triton::ReduceOp>(op) &&\n                !op->getResult(0).getType().isa<RankedTensorType>()) &&\n-             !isa<scf::YieldOp>(op);\n+             !isa<triton::gpu::ConvertLayoutOp>(op) && !isa<scf::YieldOp>(op);\n     };\n     mlir::getForwardSlice(cvt.getResult(), &cvtSlices, filter);\n     if (cvtSlices.empty())\n@@ -881,24 +887,31 @@ SmallVector<int64_t, 2> mmaVersionToShapePerWarp(int version) {\n \n SmallVector<unsigned, 2> warpsPerTileV1(const ArrayRef<int64_t> shape,\n                                         int numWarps) {\n-  SmallVector<unsigned, 2> ret = {1, 1};\n-  SmallVector<int64_t, 2> shapePerWarp =\n-      mmaVersionToShapePerWarp(1 /*version*/);\n-  bool changed = false;\n-  do {\n-    changed = false;\n-    int pre = ret[0];\n-    if (ret[0] * ret[1] < numWarps) {\n-      ret[0] = std::clamp<unsigned>(ret[0] * 2, 1, shape[0] / shapePerWarp[0]);\n-      changed = pre != ret[0];\n-    }\n-    if (ret[0] * ret[1] < numWarps) {\n-      pre = ret[1];\n-      ret[1] = std::clamp<unsigned>(ret[1] * 2, 1, shape[1] / shapePerWarp[1]);\n-      changed = pre != ret[1];\n-    }\n-  } while (changed);\n-  return ret;\n+  if (!MmaEncodingAttr::_mmaV1UpdateWpt) {\n+    SmallVector<unsigned, 2> ret = {1, 1};\n+    SmallVector<int64_t, 2> shapePerWarp =\n+        mmaVersionToShapePerWarp(1 /*version*/);\n+    bool changed = false;\n+    do {\n+      changed = false;\n+      int pre = ret[0];\n+      if (ret[0] * ret[1] < numWarps) {\n+        ret[0] =\n+            std::clamp<unsigned>(ret[0] * 2, 1, shape[0] / shapePerWarp[0]);\n+        changed = pre != ret[0];\n+      }\n+      if (ret[0] * ret[1] < numWarps) {\n+        pre = ret[1];\n+        ret[1] =\n+            std::clamp<unsigned>(ret[1] * 2, 1, shape[1] / shapePerWarp[1]);\n+        changed = pre != ret[1];\n+      }\n+    } while (changed);\n+    return ret;\n+  } else {\n+    // Set a default value and ensure product of wpt equals numWarps\n+    return {static_cast<unsigned>(numWarps), 1};\n+  }\n }\n \n SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n@@ -1019,6 +1032,7 @@ class OptimizeConvertToDotOperand : public mlir::RewritePattern {\n         dstDotOperandLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n     if ((order[0] == 1 && isMMAv1Row) || (order[0] == 0 && !isMMAv1Row))\n       return failure();\n+\n     auto newIsRow = BoolAttr::get(op->getContext(), !isMMAv1Row);\n     auto newDstEncoding = triton::gpu::DotOperandEncodingAttr::get(\n         op->getContext(), dstDotOperandLayout.getOpIdx(),\n@@ -1034,6 +1048,7 @@ class OptimizeConvertToDotOperand : public mlir::RewritePattern {\n \n class BlockedToMMA : public mlir::RewritePattern {\n   int computeCapability;\n+  mutable int mmaV1Counter{}; // used to generate ID for MMAv1 encoding\n \n public:\n   BlockedToMMA(mlir::MLIRContext *context, int computeCapability)\n@@ -1060,7 +1075,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto dotOp = cast<triton::DotOp>(op);\n     // TODO: Check data-types and SM compatibility\n     auto oldRetType = dotOp.getResult().getType().cast<RankedTensorType>();\n-    if (oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n+    if (!oldRetType.getEncoding() ||\n+        oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n       return failure();\n \n     auto AType = dotOp.getOperand(0).getType().cast<RankedTensorType>();\n@@ -1091,13 +1107,13 @@ class BlockedToMMA : public mlir::RewritePattern {\n         getWarpsPerTile(dotOp, retShape, versionMajor, numWarps);\n     triton::gpu::MmaEncodingAttr mmaEnc;\n     if (versionMajor == 1) {\n-      auto shapeA = AType.getShape();\n-      auto shapeB = BType.getShape();\n-      bool isARow = AOrder[0] != 0;\n-      bool isBRow = BOrder[0] != 0;\n-      mmaEnc = triton::gpu::MmaEncodingAttr::get(\n-          oldRetType.getContext(), versionMajor, warpsPerTile, shapeA, shapeB,\n-          isARow, isBRow);\n+      if (MmaEncodingAttr::_mmaV1UpdateWpt)\n+        mmaEnc = triton::gpu::MmaEncodingAttr::get(\n+            oldRetType.getContext(), versionMajor, numWarps, mmaV1Counter++);\n+      else\n+        mmaEnc = triton::gpu::MmaEncodingAttr::get(\n+            dotOp->getContext(), versionMajor, 0 /*versionMinor*/,\n+            warpsPerTileV1(retShape, numWarps));\n     } else if (versionMajor == 2) {\n       mmaEnc = triton::gpu::MmaEncodingAttr::get(\n           oldRetType.getContext(), versionMajor, 0 /*versionMinor*/,\n@@ -1153,45 +1169,107 @@ class BlockedToMMA : public mlir::RewritePattern {\n   }\n };\n \n-class FixupLoop : public mlir::RewritePattern {\n+// Convert + trans + convert\n+// x = convert_layout distributed -> #shared_x\n+// y = trans x -> #shared_y\n+// z = convert_layout y -> #dot_operand\n+class ConvertTransConvert : public mlir::RewritePattern {\n \n public:\n-  explicit FixupLoop(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(scf::ForOp::getOperationName(), 2, context) {}\n+  ConvertTransConvert(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n+                             1, context) {}\n \n-  mlir::LogicalResult\n+  LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n-    auto forOp = cast<scf::ForOp>(op);\n+    auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto tmpOp = dyn_cast_or_null<triton::TransOp>(dstOp.src().getDefiningOp());\n+    if (!tmpOp)\n+      return mlir::failure();\n+    auto srcOp = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n+        tmpOp.src().getDefiningOp());\n+    if (!srcOp)\n+      return mlir::failure();\n+    auto arg = srcOp.src();\n+    auto X = tmpOp.src();\n+    auto Y = dstOp.src();\n+    // types\n+    auto argType = arg.getType().cast<RankedTensorType>();\n+    auto XType = X.getType().cast<RankedTensorType>();\n+    auto YType = Y.getType().cast<RankedTensorType>();\n+    auto ZType = dstOp.getResult().getType().cast<RankedTensorType>();\n+    // encodings\n+    auto argEncoding = argType.getEncoding();\n+    auto XEncoding =\n+        XType.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n+    auto YEncoding =\n+        YType.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n+    auto ZEncoding =\n+        ZType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+    if (!ZEncoding)\n+      return mlir::failure();\n+    // new X encoding\n+    auto newXOrder = triton::gpu::getOrder(argEncoding);\n+    auto newXEncoding = triton::gpu::SharedEncodingAttr::get(\n+        getContext(), ZEncoding, XType.getShape(), newXOrder,\n+        XType.getElementType());\n+    auto newXType = RankedTensorType::get(XType.getShape(),\n+                                          XType.getElementType(), newXEncoding);\n+    if (XEncoding == newXEncoding)\n+      return mlir::failure();\n \n-    // Rewrite init argument\n-    SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n-    bool shouldRematerialize = false;\n-    for (size_t i = 0; i < newInitArgs.size(); i++) {\n-      auto initArg = newInitArgs[i];\n-      auto regionArg = forOp.getRegionIterArgs()[i];\n-      if (newInitArgs[i].getType() != forOp.getRegionIterArgs()[i].getType()) {\n-        shouldRematerialize = true;\n-        break;\n-      }\n-    }\n-    if (!shouldRematerialize)\n-      return failure();\n+    auto newX = rewriter.create<triton::gpu::ConvertLayoutOp>(srcOp.getLoc(),\n+                                                              newXType, arg);\n+    auto newY = rewriter.create<triton::TransOp>(tmpOp.getLoc(), newX);\n+    rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(dstOp, ZType,\n+                                                              newY);\n+    return mlir::success();\n+  }\n+};\n \n-    scf::ForOp newForOp = rewriter.create<scf::ForOp>(\n-        forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n-        forOp.getStep(), newInitArgs);\n-    newForOp->moveBefore(forOp);\n-    rewriter.setInsertionPointToStart(newForOp.getBody());\n-    BlockAndValueMapping mapping;\n-    for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n-      mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n+//\n+class ConvertDotConvert : public mlir::RewritePattern {\n+public:\n+  ConvertDotConvert(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n+                             1, context) {}\n \n-    for (Operation &op : forOp.getBody()->getOperations()) {\n-      Operation *newOp = rewriter.clone(op, mapping);\n-    }\n-    rewriter.replaceOp(forOp, newForOp.getResults());\n-    return success();\n+  LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto dotOp = dyn_cast_or_null<triton::DotOp>(dstOp.src().getDefiningOp());\n+    if (!dotOp)\n+      return mlir::failure();\n+    if (std::distance(dstOp->user_begin(), dstOp->user_end()) != 1 ||\n+        std::distance(dotOp->user_begin(), dotOp->user_end()) != 1)\n+      return mlir::failure();\n+    auto cvtOp = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n+        dotOp.getOperand(2).getDefiningOp());\n+    if (!cvtOp)\n+      return mlir::failure();\n+    auto loadOp = dyn_cast_or_null<triton::LoadOp>(cvtOp.src().getDefiningOp());\n+    if (!loadOp)\n+      return mlir::failure();\n+    auto dstTy = dstOp.getResult().getType().cast<RankedTensorType>();\n+    auto srcTy = cvtOp.getOperand().getType().cast<RankedTensorType>();\n+    if (dstTy != srcTy)\n+      return mlir::failure();\n+\n+    // TODO: int tensor cores\n+    auto _0f = rewriter.create<arith::ConstantFloatOp>(\n+        op->getLoc(), APFloat(0.0f), dstTy.getElementType().cast<FloatType>());\n+    auto _0 = rewriter.create<triton::SplatOp>(\n+        op->getLoc(), dotOp.getResult().getType(), _0f);\n+    auto newDot = rewriter.create<triton::DotOp>(\n+        op->getLoc(), dotOp.getResult().getType(), dotOp.getOperand(0),\n+        dotOp.getOperand(1), _0, dotOp.allowTF32());\n+    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        op->getLoc(), dstTy, newDot.getResult());\n+    auto newAdd = rewriter.replaceOpWithNewOp<arith::AddFOp>(\n+        op, newCvt, cvtOp.getOperand());\n+    return mlir::success();\n   }\n };\n \n@@ -1224,14 +1302,14 @@ class TritonGPUCombineOpsPass\n     patterns.add<MoveConvertOutOfLoop>(context);\n     patterns.add<MoveConvertOutOfIf>(context);\n     patterns.add<BlockedToMMA>(context, computeCapability);\n+    patterns.add<ConvertTransConvert>(context);\n+    patterns.add<ConvertDotConvert>(context);\n \n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n       signalPassFailure();\n     }\n \n-    mlir::RewritePatternSet loopFixup(context);\n-    loopFixup.add<FixupLoop>(context);\n-    if (applyPatternsAndFoldGreedily(m, std::move(loopFixup)).failed()) {\n+    if (fixupLoops(m).failed()) {\n       signalPassFailure();\n     }\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/DecomposeConversions.cpp", "status": "added", "additions": 69, "deletions": 0, "changes": 69, "file_content_changes": "@@ -0,0 +1,69 @@\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n+#include \"mlir/Dialect/SCF/SCF.h\"\n+#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Verifier.h\"\n+#include \"mlir/Interfaces/InferTypeOpInterface.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Pass/PassManager.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"mlir/Transforms/Passes.h\"\n+#include \"mlir/Transforms/RegionUtils.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n+\n+using namespace mlir;\n+\n+class TritonGPUDecomposeConversionsPass\n+    : public TritonGPUDecomposeConversionsBase<\n+          TritonGPUDecomposeConversionsPass> {\n+public:\n+  TritonGPUDecomposeConversionsPass() = default;\n+\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp mod = getOperation();\n+    mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n+      OpBuilder builder(cvtOp);\n+      auto srcType = cvtOp.getOperand().getType().cast<RankedTensorType>();\n+      auto dstType = cvtOp.getType().cast<RankedTensorType>();\n+      auto srcEncoding = srcType.getEncoding();\n+      if (srcEncoding.isa<triton::gpu::SharedEncodingAttr>())\n+        return;\n+      auto dstDotOp =\n+          dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+      if (!dstDotOp)\n+        return;\n+      if (auto srcMmaEncoding =\n+              srcEncoding.dyn_cast<triton::gpu::MmaEncodingAttr>()) {\n+\n+        if (srcMmaEncoding.getVersionMajor() == 1 ||\n+            (srcMmaEncoding.getWarpsPerCTA()[1] == 1 &&\n+             dstDotOp.getParent() == srcMmaEncoding))\n+          return;\n+      }\n+      auto tmpType = RankedTensorType::get(\n+          dstType.getShape(), dstType.getElementType(),\n+          triton::gpu::SharedEncodingAttr::get(\n+              mod.getContext(), dstDotOp, srcType.getShape(),\n+              triton::gpu::getOrder(srcEncoding), srcType.getElementType()));\n+      auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n+          cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n+      auto newConvert = builder.create<triton::gpu::ConvertLayoutOp>(\n+          cvtOp.getLoc(), dstType, tmp);\n+      cvtOp.replaceAllUsesWith(newConvert.getResult());\n+      cvtOp.erase();\n+    });\n+  }\n+};\n+\n+std::unique_ptr<Pass> mlir::createTritonGPUDecomposeConversionsPass() {\n+  return std::make_unique<TritonGPUDecomposeConversionsPass>();\n+}"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 43, "deletions": 17, "changes": 60, "file_content_changes": "@@ -302,6 +302,7 @@ void LoopPipeliner::emitPrologue() {\n               loadStageBuffer[loadOp][stage], pipelineIterIdx, newMask,\n               lookupOrDefault(loadOp.other(), stage), loadOp.cache(),\n               loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n+          builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n           loadStageBuffer[loadOp].push_back(newOp->getResult(0));\n         } else\n           llvm_unreachable(\"This should be LoadOp\");\n@@ -319,6 +320,9 @@ void LoopPipeliner::emitPrologue() {\n       }\n \n       // Update mapping of results\n+      // if (stage == numStages - 2)\n+      //   continue;\n+\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n         Value originalResult = op->getResult(dstIdx);\n         // copy_async will update the value of its only use\n@@ -383,7 +387,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   //   (original args)\n   //   (insertSliceAsync buffer at stage numStages - 1) for each load\n   //   (extracted tensor) for each load\n-  //   (depArgs at stage numStages - 1)\n+  //   (depArgs at stage numStages - 2)\n   //   (iv at stage numStages - 2)\n   //   (pipeline iteration index)\n   //   (loop iteration index)\n@@ -404,7 +408,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   size_t depArgsBeginIdx = newLoopArgs.size();\n   for (BlockArgument depArg : depArgs) {\n     depArgsIdx[depArg] = newLoopArgs.size();\n-    newLoopArgs.push_back(valueMapping[depArg][numStages - 1]);\n+    newLoopArgs.push_back(valueMapping[depArg][numStages - 2]);\n   }\n \n   size_t nextIVIdx = newLoopArgs.size();\n@@ -463,10 +467,12 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   DenseMap<BlockArgument, Value> depArgsMapping;\n   size_t argIdx = 0;\n   for (BlockArgument arg : depArgs) {\n-    nextMapping.map(arg,\n-                    newForOp.getRegionIterArgs()[argIdx + depArgsBeginIdx]);\n+    BlockArgument nextArg =\n+        newForOp.getRegionIterArgs()[argIdx + depArgsBeginIdx];\n+    nextMapping.map(arg, nextArg);\n     ++argIdx;\n   }\n+\n   // Special handling for iv & loop condition\n   Value nextIV = builder.create<arith::AddIOp>(\n       newForOp.getInductionVar().getLoc(),\n@@ -491,6 +497,25 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   extractSliceIndex = builder.create<arith::IndexCastOp>(\n       extractSliceIndex.getLoc(), builder.getIndexType(), extractSliceIndex);\n \n+  for (Operation *op : orderedDeps)\n+    if (!loads.contains(op->getResult(0))) {\n+      Operation *nextOp = builder.clone(*op, nextMapping);\n+\n+      auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n+      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n+        for (OpOperand &operand : originYield->getOpOperands()) {\n+          if (operand.get() == op->getResult(dstIdx)) {\n+            size_t originIdx = operand.getOperandNumber();\n+            size_t newArgIdx = depArgsIdx[forOp.getRegionIterArgs()[originIdx]];\n+            BlockArgument newArg = newForOp.getRegionIterArgs()[newArgIdx];\n+            nextMapping.map(forOp.getRegionIterArgs()[originIdx],\n+                            nextOp->getResult(dstIdx));\n+            depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n+          }\n+        }\n+      }\n+    }\n+\n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n     // Update loading mask\n@@ -518,6 +543,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n           insertSliceIndex, newMask,\n           nextMapping.lookupOrDefault(loadOp.other()), loadOp.cache(),\n           loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n+      builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n       nextBuffers.push_back(insertAsyncOp);\n       auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n       sliceType = RankedTensorType::get(sliceType.getShape(),\n@@ -532,19 +558,19 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n                                     int_attr(sliceType.getShape()[1])},\n           SmallVector<OpFoldResult>{int_attr(1), int_attr(1), int_attr(1)});\n       extractSlices.push_back(nextOp->getResult(0));\n-    } else\n-      nextOp = builder.clone(*op, nextMapping);\n-    // Update mapping of results\n-    for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n-      nextMapping.map(op->getResult(dstIdx), nextOp->getResult(dstIdx));\n-      // If this is a loop-carried value, update the mapping for yield\n-      auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n-      for (OpOperand &operand : originYield->getOpOperands()) {\n-        if (operand.get() == op->getResult(dstIdx)) {\n-          size_t originIdx = operand.getOperandNumber();\n-          size_t newArgIdx = depArgsIdx[forOp.getRegionIterArgs()[originIdx]];\n-          BlockArgument newArg = newForOp.getRegionIterArgs()[newArgIdx];\n-          depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n+\n+      // Update mapping of results\n+      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n+        nextMapping.map(op->getResult(dstIdx), nextOp->getResult(dstIdx));\n+        // If this is a loop-carried value, update the mapping for yield\n+        auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n+        for (OpOperand &operand : originYield->getOpOperands()) {\n+          if (operand.get() == op->getResult(dstIdx)) {\n+            size_t originIdx = operand.getOperandNumber();\n+            size_t newArgIdx = depArgsIdx[forOp.getRegionIterArgs()[originIdx]];\n+            BlockArgument newArg = newForOp.getRegionIterArgs()[newArgIdx];\n+            depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n+          }\n         }\n       }\n     }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -248,13 +248,17 @@ scf::ForOp Prefetcher::createNewForOp() {\n                      prefetchWidth;\n       Operation *prevDot = firstDot;\n       while (kRem != 0) {\n-        int64_t kShape = largestPow2(kRem);\n+        // int64_t kShape = largestPow2(kRem);\n+        int64_t kShape = prefetchWidth;\n+        auto insertionPoint = builder.saveInsertionPoint();\n+        builder.setInsertionPoint(prevDot);\n         Value aRem =\n             generatePrefetch(mapping.lookup(dot2aLoopArg[dot]), 0, false,\n                              dotEncoding, builder, kOff, kShape);\n         Value bRem =\n             generatePrefetch(mapping.lookup(dot2bLoopArg[dot]), 1, false,\n                              dotEncoding, builder, kOff, kShape);\n+        builder.restoreInsertionPoint(insertionPoint);\n         newOp = builder.clone(*dot, mapping);\n         newOp->setOperand(0, aRem);\n         newOp->setOperand(1, bRem);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/ReorderInstructions.cpp", "status": "added", "additions": 109, "deletions": 0, "changes": 109, "file_content_changes": "@@ -0,0 +1,109 @@\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n+#include \"mlir/Dialect/SCF/SCF.h\"\n+#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/Verifier.h\"\n+#include \"mlir/Interfaces/InferTypeOpInterface.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Pass/PassManager.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"mlir/Transforms/Passes.h\"\n+#include \"mlir/Transforms/RegionUtils.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n+\n+using namespace mlir;\n+\n+static inline bool\n+willIncreaseRegisterPressure(triton::gpu::ConvertLayoutOp op) {\n+  auto srcType = op.getOperand().getType().cast<RankedTensorType>();\n+  auto dstType = op.getResult().getType().cast<RankedTensorType>();\n+  auto srcEncoding = srcType.getEncoding();\n+  auto dstEncoding = dstType.getEncoding();\n+  if (srcEncoding.isa<triton::gpu::SharedEncodingAttr>())\n+    return true;\n+  if (dstEncoding.isa<triton::gpu::DotOperandEncodingAttr>())\n+    return true;\n+  return false;\n+}\n+\n+class TritonGPUReorderInstructionsPass\n+    : public TritonGPUReorderInstructionsBase<\n+          TritonGPUReorderInstructionsPass> {\n+public:\n+  TritonGPUReorderInstructionsPass() = default;\n+\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp m = getOperation();\n+    // Sink conversions into loops when they will increase\n+    // register pressure\n+    DenseMap<Operation *, Operation *> opToMove;\n+    m.walk([&](triton::gpu::ConvertLayoutOp op) {\n+      if (!willIncreaseRegisterPressure(op))\n+        return;\n+      auto user_begin = op->user_begin();\n+      auto user_end = op->user_end();\n+      if (std::distance(user_begin, user_end) != 1)\n+        return;\n+      if (user_begin->getParentOfType<scf::ForOp>() ==\n+          op->getParentOfType<scf::ForOp>())\n+        return;\n+      opToMove.insert({op, *user_begin});\n+    });\n+    for (auto &kv : opToMove)\n+      kv.first->moveBefore(kv.second);\n+    // Move convert(load) immediately after dependent load\n+    m.walk([&](triton::gpu::ConvertLayoutOp op) {\n+      auto dstType = op.getResult().getType().cast<RankedTensorType>();\n+      auto dstEncoding = dstType.getEncoding();\n+      if (!dstEncoding.isa<triton::gpu::SharedEncodingAttr>())\n+        return;\n+      Operation *argOp = op.getOperand().getDefiningOp();\n+      if (!argOp)\n+        return;\n+      op->moveAfter(argOp);\n+    });\n+    // Move transpositions just after their definition\n+    opToMove.clear();\n+    m.walk([&](triton::TransOp op) {\n+      Operation *argOp = op.getOperand().getDefiningOp();\n+      if (!argOp)\n+        return;\n+      op->moveAfter(argOp);\n+    });\n+    // Move `dot` operand so that conversions to opIdx=0 happens before\n+    // conversions to opIdx=1\n+    m.walk([&](triton::gpu::ConvertLayoutOp op) {\n+      auto dstType = op.getResult().getType().cast<RankedTensorType>();\n+      auto dstEncoding =\n+          dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+      if (!dstEncoding)\n+        return;\n+      int opIdx = dstEncoding.getOpIdx();\n+      if (opIdx != 1)\n+        return;\n+      if (op->getUsers().empty())\n+        return;\n+      auto dotUser = dyn_cast<triton::DotOp>(*op->user_begin());\n+      if (!dotUser)\n+        return;\n+      auto BOp = dotUser.getOperand(1).getDefiningOp();\n+      if (!BOp)\n+        return;\n+      op->moveBefore(BOp);\n+    });\n+    return;\n+  }\n+};\n+\n+std::unique_ptr<Pass> mlir::createTritonGPUReorderInstructionsPass() {\n+  return std::make_unique<TritonGPUReorderInstructionsPass>();\n+}"}, {"filename": "lib/Dialect/TritonGPU/Transforms/UpdateMmaForVolta.cpp", "status": "added", "additions": 355, "deletions": 0, "changes": 355, "file_content_changes": "@@ -0,0 +1,355 @@\n+#include \"Utility.h\"\n+#include \"mlir/Dialect/SCF/SCF.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+\n+namespace mlir {\n+namespace {\n+using triton::DotOp;\n+using triton::gpu::ConvertLayoutOp;\n+using triton::gpu::DotOperandEncodingAttr;\n+using triton::gpu::MmaEncodingAttr;\n+using triton::gpu::SliceEncodingAttr;\n+\n+// This pattern collects the wrong Mma those need to update and create the right\n+// ones for each.\n+// TODO[Superjomn]: RewirtePattern is not needed here, Rewrite this to a method\n+class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n+  // Holds the mapping from old(wrong) mmaEncodingAttr to the new(correct)\n+  // mmaEncodingAttr.\n+  DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n+\n+public:\n+  CollectMmaToUpdateForVolta(\n+      mlir::MLIRContext *ctx,\n+      DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n+      : mlir::RewritePattern(triton::DotOp::getOperationName(), 1, ctx),\n+        mmaToUpdate(mmaToUpdate) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+\n+    auto dotOp = cast<triton::DotOp>(op);\n+    auto *ctx = dotOp->getContext();\n+    auto AT = dotOp.a().getType().cast<RankedTensorType>();\n+    auto BT = dotOp.b().getType().cast<RankedTensorType>();\n+    auto DT = dotOp.d().getType().cast<RankedTensorType>();\n+    if (!DT.getEncoding())\n+      return failure();\n+    auto mmaLayout = DT.getEncoding().dyn_cast<MmaEncodingAttr>();\n+    if (!(mmaLayout && mmaLayout.isVolta()))\n+      return failure();\n+\n+    // Has processed.\n+    if (mmaToUpdate.count(mmaLayout))\n+      return failure();\n+\n+    auto dotOperandA = AT.getEncoding().cast<DotOperandEncodingAttr>();\n+    auto dotOperandB = BT.getEncoding().cast<DotOperandEncodingAttr>();\n+    bool isARow = dotOperandA.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    bool isBRow = dotOperandB.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    auto [isARow_, isBRow_, isAVec4, isBVec4, mmaId] =\n+        mmaLayout.decodeVoltaLayoutStates();\n+\n+    // The wpt of MMAv1 is also determined by isARow, isBRow and shape, and it\n+    // could only be set here for those states might be updated by previous\n+    // patterns in the Combine Pass.\n+    if (isARow_ == isARow && isBRow_ == isBRow) {\n+      auto tgtWpt =\n+          getWarpsPerCTA(DT.getShape(), isARow, isBRow, isAVec4, isBVec4,\n+                         product(mmaLayout.getWarpsPerCTA()));\n+      // Check if the wpt should be updated.\n+      if (tgtWpt == mmaLayout.getWarpsPerCTA() ||\n+          !MmaEncodingAttr::_mmaV1UpdateWpt)\n+        return failure();\n+    }\n+\n+    MmaEncodingAttr newMmaLayout;\n+    {\n+      // Create a temporary MMA layout to obtain the isAVec4 and isBVec4\n+      auto tmpMmaLayout = MmaEncodingAttr::get(\n+          ctx, mmaLayout.getVersionMajor(), mmaLayout.getWarpsPerCTA(),\n+          AT.getShape(), BT.getShape(), isARow, isBRow, mmaId);\n+      auto [isARow_, isBRow_, isAVec4_, isBVec4_, _] =\n+          tmpMmaLayout.decodeVoltaLayoutStates();\n+\n+      // Recalculate the wpt, for here we could get the latest information, the\n+      // wpt should be updated.\n+      auto updatedWpt =\n+          getWarpsPerCTA(DT.getShape(), isARow_, isBRow_, isAVec4_, isBVec4_,\n+                         product(mmaLayout.getWarpsPerCTA()));\n+      auto newWpt = MmaEncodingAttr::_mmaV1UpdateWpt\n+                        ? updatedWpt\n+                        : mmaLayout.getWarpsPerCTA();\n+      newMmaLayout = MmaEncodingAttr::get(ctx, mmaLayout.getVersionMajor(),\n+                                          newWpt, AT.getShape(), BT.getShape(),\n+                                          isARow, isBRow, mmaId);\n+    }\n+\n+    // Collect the wrong MMA Layouts, and mark need to update.\n+    mmaToUpdate.try_emplace(mmaLayout, newMmaLayout);\n+\n+    return failure();\n+  }\n+\n+  // Get the wpt for MMAv1 using more information.\n+  // Reference the original logic here\n+  // https://github.com/openai/triton/blob/0e4691e6dd91e001a8d33b71badf8b3314325459/lib/codegen/analysis/layout.cc#L223\n+  SmallVector<unsigned, 2> getWarpsPerCTA(ArrayRef<int64_t> shape, bool isARow,\n+                                          bool isBRow, bool isAVec4,\n+                                          bool isBVec4, int numWarps) const {\n+    // TODO[Superjomn]: Share code with\n+    // DotOpMmaV1ConversionHelper::AParam/BParam, since same code to compute the\n+    // rep,spw and fpw.\n+    SmallVector<unsigned, 2> wpt({1, 1});\n+    SmallVector<unsigned, 2> wpt_nm1;\n+\n+    SmallVector<int, 2> rep(2), spw(2);\n+    std::array<int, 3> fpw{{2, 2, 1}};\n+    int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+    rep[0] = 2 * packSize0;\n+    spw[0] = fpw[0] * 4 * rep[0];\n+\n+    int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+    rep[1] = 2 * packSize1;\n+    spw[1] = fpw[1] * 4 * rep[1];\n+\n+    do {\n+      wpt_nm1 = wpt;\n+      if (wpt[0] * wpt[1] < numWarps)\n+        wpt[0] = std::clamp<int>(wpt[0] * 2, 1, shape[0] / spw[0]);\n+      if (wpt[0] * wpt[1] < numWarps)\n+        wpt[1] = std::clamp<int>(wpt[1] * 2, 1, shape[1] / spw[1]);\n+    } while (wpt_nm1 != wpt);\n+\n+    return wpt;\n+  }\n+};\n+\n+class UpdateMMAForMMAv1 : public mlir::RewritePattern {\n+  const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n+\n+public:\n+  UpdateMMAForMMAv1(\n+      MLIRContext *context,\n+      const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n+      : RewritePattern(MatchAnyOpTypeTag{}, 1, context),\n+        mmaToUpdate(mmaToUpdate) {}\n+\n+  LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    // Nothing to update\n+    if (mmaToUpdate.empty())\n+      return failure();\n+\n+    if (auto dotOp = llvm::dyn_cast<DotOp>(op))\n+      return rewriteDotOp(op, rewriter);\n+    else if (auto cvtOp = llvm::dyn_cast<ConvertLayoutOp>(op))\n+      return rewriteCvtOp(op, rewriter);\n+    else if (auto expandDimsOp = llvm::dyn_cast<triton::ExpandDimsOp>(op))\n+      return rewriteExpandDimsOp(op, rewriter);\n+    else if (auto constOp = llvm::dyn_cast<arith::ConstantOp>(op))\n+      return rewriteConstantOp(op, rewriter);\n+    else\n+      return rewriteElementwiseOp(op, rewriter);\n+    return failure();\n+  }\n+\n+  LogicalResult rewriteDotOp(Operation *op,\n+                             mlir::PatternRewriter &rewriter) const {\n+    auto dotOp = llvm::cast<DotOp>(op);\n+    auto tensorTy = dotOp->getResult(0).getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return failure();\n+\n+    auto mma = dotOp.d()\n+                   .getType()\n+                   .cast<RankedTensorType>()\n+                   .getEncoding()\n+                   .dyn_cast<MmaEncodingAttr>();\n+    if (!mma || !mmaToUpdate.count(mma))\n+      return failure();\n+\n+    auto newTensorTy = getUpdatedType(tensorTy);\n+    rewriter.replaceOpWithNewOp<DotOp>(op, newTensorTy, dotOp.a(), dotOp.b(),\n+                                       dotOp.c(), dotOp.allowTF32());\n+    return success();\n+  }\n+\n+  LogicalResult rewriteCvtOp(Operation *op,\n+                             mlir::PatternRewriter &rewriter) const {\n+    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n+    if (!needUpdate(cvt.getResult().getType()))\n+      return failure();\n+    auto tensorTy = cvt.result().getType().dyn_cast<RankedTensorType>();\n+\n+    auto newTensorTy = getUpdatedType(tensorTy);\n+    auto newOp = rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n+                                                              cvt.getOperand());\n+    return success();\n+  }\n+\n+  LogicalResult rewriteExpandDimsOp(Operation *op,\n+                                    mlir::PatternRewriter &rewriter) const {\n+    auto expandDims = llvm::cast<triton::ExpandDimsOp>(op);\n+    auto srcTy = expandDims.src().getType();\n+    auto resTy = expandDims.getResult().getType();\n+\n+    // the result type need to update\n+    if (!needUpdate(srcTy) && needUpdate(resTy)) {\n+      rewriter.replaceOpWithNewOp<triton::ExpandDimsOp>(op, expandDims.src(),\n+                                                        expandDims.axis());\n+      return success();\n+    }\n+\n+    return failure();\n+  }\n+\n+  LogicalResult rewriteConstantOp(Operation *op,\n+                                  mlir::PatternRewriter &rewriter) const {\n+    auto constant = llvm::cast<arith::ConstantOp>(op);\n+    auto resTy = constant.getResult().getType();\n+    if (!needUpdate(resTy))\n+      return failure();\n+\n+    auto tensorTy = constant.getResult().getType().cast<RankedTensorType>();\n+    auto mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n+    auto dot = tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>();\n+    if (!mma && !dot)\n+      return failure();\n+\n+    auto newTensorTy = getUpdatedType(tensorTy);\n+    if (auto attr = constant.getValue().dyn_cast<SplatElementsAttr>()) {\n+      auto newRet =\n+          SplatElementsAttr::get(newTensorTy, attr.getSplatValue<Attribute>());\n+      rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newRet);\n+      return success();\n+    }\n+\n+    return failure();\n+  }\n+\n+  LogicalResult rewriteElementwiseOp(Operation *op,\n+                                     mlir::PatternRewriter &rewriter) const {\n+    if (op->getNumOperands() != 1 || op->getNumResults() != 1)\n+      return failure();\n+\n+    auto srcTy = op->getOperand(0).getType();\n+    auto resTy = op->getResult(0).getType();\n+    if (!needUpdate(srcTy) && needUpdate(resTy)) {\n+      op->getResult(0).setType(\n+          getUpdatedType(resTy.dyn_cast<RankedTensorType>()));\n+      return success();\n+    }\n+    return failure();\n+  }\n+\n+  RankedTensorType getUpdatedType(RankedTensorType type) const {\n+    if (!needUpdate(type))\n+      return type;\n+    auto encoding = type.getEncoding();\n+    if (auto mma = encoding.dyn_cast<MmaEncodingAttr>()) {\n+      auto newMma = mmaToUpdate.lookup(mma);\n+      return RankedTensorType::get(type.getShape(), type.getElementType(),\n+                                   newMma);\n+    } else if (auto slice = encoding.dyn_cast<SliceEncodingAttr>()) {\n+      if (auto mma = slice.getParent().dyn_cast<MmaEncodingAttr>()) {\n+        auto newMma = mmaToUpdate.lookup(mma);\n+        auto newSlice =\n+            SliceEncodingAttr::get(slice.getContext(), slice.getDim(), newMma);\n+        return RankedTensorType::get(type.getShape(), type.getElementType(),\n+                                     newSlice);\n+      }\n+    } else if (auto dotOp = encoding.dyn_cast<DotOperandEncodingAttr>()) {\n+      if (auto mma = dotOp.getParent().dyn_cast<MmaEncodingAttr>()) {\n+        auto newMma = mmaToUpdate.lookup(mma);\n+        auto newDotOp =\n+            DotOperandEncodingAttr::get(dotOp.getContext(), dotOp.getOpIdx(),\n+                                        newMma, dotOp.getIsMMAv1Row());\n+        return RankedTensorType::get(type.getShape(), type.getElementType(),\n+                                     newDotOp);\n+      }\n+    }\n+    return type;\n+  }\n+\n+  // Tell if this type contains a wrong MMA encoding and need to update.\n+  bool needUpdate(Type type) const {\n+    auto tensorTy = type.dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return false;\n+    return needUpdate(tensorTy);\n+  }\n+\n+  // Tell if this type contains a wrong MMA encoding and need to update.\n+  bool needUpdate(RankedTensorType type) const {\n+    auto encoding = type.getEncoding();\n+    if (!encoding)\n+      return false;\n+\n+    MmaEncodingAttr mma;\n+    if ((mma = encoding.dyn_cast<MmaEncodingAttr>())) {\n+    } else if (auto slice = encoding.dyn_cast<SliceEncodingAttr>()) {\n+      mma = slice.getParent().dyn_cast<MmaEncodingAttr>();\n+    } else if (auto dotOp = encoding.dyn_cast<DotOperandEncodingAttr>()) {\n+      mma = dotOp.getParent().dyn_cast<MmaEncodingAttr>();\n+    }\n+\n+    return mma && mmaToUpdate.count(mma);\n+  }\n+};\n+\n+} // namespace\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n+\n+class UpdateMmaForVoltaPass\n+    : public UpdateMmaForVoltaBase<UpdateMmaForVoltaPass> {\n+public:\n+  UpdateMmaForVoltaPass() = default;\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp m = getOperation();\n+\n+    llvm::DenseMap<MmaEncodingAttr, MmaEncodingAttr> mmaToUpdate;\n+    {\n+      mlir::RewritePatternSet patterns(context);\n+      patterns.add<CollectMmaToUpdateForVolta>(context, mmaToUpdate);\n+\n+      GreedyRewriteConfig config;\n+      config.enableRegionSimplification =\n+          false; // The pattern doesn't modify the IR\n+      if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n+        signalPassFailure();\n+    }\n+\n+    if (!mmaToUpdate.empty()) {\n+      mlir::RewritePatternSet patterns(context);\n+      patterns.add<UpdateMMAForMMAv1>(context, mmaToUpdate);\n+\n+      mlir::GreedyRewriteConfig config;\n+      // Make sure the slice and dot_operand layouts' parent mma are updated\n+      // before updating DotOp or it will get a mismatch parent-encoding.\n+      config.useTopDownTraversal = true;\n+\n+      if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n+        signalPassFailure();\n+\n+      if (fixupLoops(m).failed())\n+        signalPassFailure();\n+    }\n+  }\n+};\n+\n+std::unique_ptr<Pass> createTritonGPUUpdateMmaForVoltaPass() {\n+  return std::make_unique<UpdateMmaForVoltaPass>();\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "added", "additions": 63, "deletions": 0, "changes": 63, "file_content_changes": "@@ -0,0 +1,63 @@\n+#include \"Utility.h\"\n+#include \"mlir/Dialect/SCF/SCF.h\"\n+#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+\n+namespace mlir {\n+\n+namespace {\n+\n+class FixupLoop : public mlir::RewritePattern {\n+\n+public:\n+  explicit FixupLoop(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(scf::ForOp::getOperationName(), 2, context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto forOp = cast<scf::ForOp>(op);\n+\n+    // Rewrite init argument\n+    SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n+    bool shouldRematerialize = false;\n+    for (size_t i = 0; i < newInitArgs.size(); i++) {\n+      if (newInitArgs[i].getType() != forOp.getRegionIterArgs()[i].getType() ||\n+          newInitArgs[i].getType() != forOp.getResultTypes()[i]) {\n+        shouldRematerialize = true;\n+        break;\n+      }\n+    }\n+    if (!shouldRematerialize)\n+      return failure();\n+\n+    scf::ForOp newForOp = rewriter.create<scf::ForOp>(\n+        forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n+        forOp.getStep(), newInitArgs);\n+    newForOp->moveBefore(forOp);\n+    rewriter.setInsertionPointToStart(newForOp.getBody());\n+    BlockAndValueMapping mapping;\n+    for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n+      mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n+    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n+\n+    for (Operation &op : forOp.getBody()->getOperations()) {\n+      rewriter.clone(op, mapping);\n+    }\n+    rewriter.replaceOp(forOp, newForOp.getResults());\n+    return success();\n+  }\n+};\n+\n+} // namespace\n+\n+LogicalResult fixupLoops(ModuleOp mod) {\n+  auto *ctx = mod.getContext();\n+  mlir::RewritePatternSet patterns(ctx);\n+  patterns.add<FixupLoop>(ctx);\n+  if (applyPatternsAndFoldGreedily(mod, std::move(patterns)).failed())\n+    return failure();\n+  return success();\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.h", "status": "added", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -0,0 +1,12 @@\n+#ifndef TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#define TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+\n+namespace mlir {\n+\n+LogicalResult fixupLoops(ModuleOp mod);\n+\n+} // namespace mlir\n+\n+#endif // TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "lib/Target/LLVMIR/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -7,6 +7,7 @@ add_mlir_translation_library(TritonLLVMIR\n         LINK_LIBS PUBLIC\n         MLIRIR\n         MLIRLLVMIR\n+        MLIRSCFToStandard\n         MLIRSupport\n         MLIRTargetLLVMIRExport\n         )"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 127, "deletions": 75, "changes": 202, "file_content_changes": "@@ -18,6 +18,7 @@\n #include \"llvm/IRReader/IRReader.h\"\n #include \"llvm/Linker/Linker.h\"\n #include \"llvm/Support/SourceMgr.h\"\n+#include <filesystem>\n \n namespace mlir {\n namespace triton {\n@@ -26,19 +27,18 @@ namespace triton {\n // information from mlir module.\n struct NVVMMetadata {\n   int maxntidx{-1};\n-  bool is_kernel{};\n+  bool isKernel{};\n   // Free to extend with other information.\n };\n \n // Add the nvvm related metadata to LLVM IR.\n-void amendLLVMFunc(llvm::Function *func, const NVVMMetadata &metadata) {\n+static void amendLLVMFunc(llvm::Function *func, const NVVMMetadata &metadata) {\n   auto *module = func->getParent();\n   auto &ctx = func->getContext();\n \n   if (metadata.maxntidx > 0) {\n-    auto i32_ty = llvm::IntegerType::get(ctx, 32);\n-    auto warps =\n-        llvm::ConstantInt::get(i32_ty, llvm::APInt(32, metadata.maxntidx));\n+    auto warps = llvm::ConstantInt::get(llvm::IntegerType::get(ctx, 32),\n+                                        llvm::APInt(32, metadata.maxntidx));\n \n     llvm::Metadata *md_args[] = {llvm::ValueAsMetadata::get(func),\n                                  llvm::MDString::get(ctx, \"maxntidx\"),\n@@ -48,18 +48,19 @@ void amendLLVMFunc(llvm::Function *func, const NVVMMetadata &metadata) {\n         ->addOperand(llvm::MDNode::get(ctx, md_args));\n   }\n \n-  if (metadata.is_kernel) {\n-    llvm::Metadata *md_args[] = {\n+  if (metadata.isKernel) {\n+    llvm::Metadata *mdArgs[] = {\n         llvm::ValueAsMetadata::get(func), llvm::MDString::get(ctx, \"kernel\"),\n         llvm::ValueAsMetadata::get(\n             llvm::ConstantInt::get(llvm::Type::getInt32Ty(ctx), 1))};\n     module->getOrInsertNamedMetadata(\"nvvm.annotations\")\n-        ->addOperand(llvm::MDNode::get(ctx, md_args));\n+        ->addOperand(llvm::MDNode::get(ctx, mdArgs));\n   }\n }\n \n-void extractNVVMMetadata(mlir::ModuleOp module,\n-                         llvm::DenseMap<llvm::StringRef, NVVMMetadata> *dic) {\n+static void\n+extractNVVMMetadata(mlir::ModuleOp module,\n+                    llvm::DenseMap<llvm::StringRef, NVVMMetadata> *dic) {\n   for (auto op : module.getOps<LLVM::LLVMFuncOp>()) {\n     NVVMMetadata meta;\n \n@@ -74,7 +75,7 @@ void extractNVVMMetadata(mlir::ModuleOp module,\n \n     // kernel\n     if (op->hasAttr(\"nvvm.kernel\")) {\n-      meta.is_kernel = true;\n+      meta.isKernel = true;\n       hasMetadata = true;\n     }\n \n@@ -83,13 +84,109 @@ void extractNVVMMetadata(mlir::ModuleOp module,\n   }\n }\n \n+static std::map<std::string, std::string> getExternLibs(mlir::ModuleOp module) {\n+  std::map<std::string, std::string> externLibs;\n+  SmallVector<LLVM::LLVMFuncOp> funcs;\n+  module.walk([&](LLVM::LLVMFuncOp func) {\n+    if (func.isExternal())\n+      funcs.push_back(func);\n+  });\n+\n+  for (auto &func : funcs) {\n+    if (func.getOperation()->hasAttr(\"libname\")) {\n+      auto name =\n+          func.getOperation()->getAttr(\"libname\").dyn_cast<StringAttr>();\n+      auto path =\n+          func.getOperation()->getAttr(\"libpath\").dyn_cast<StringAttr>();\n+      if (name) {\n+        std::string libName = name.str();\n+        externLibs[libName] = path.str();\n+      }\n+    }\n+  }\n+\n+  if (module.getOperation()->hasAttr(\"triton_gpu.externs\")) {\n+    auto dict = module.getOperation()\n+                    ->getAttr(\"triton_gpu.externs\")\n+                    .dyn_cast<DictionaryAttr>();\n+    for (auto &attr : dict) {\n+      externLibs[attr.getName().strref().trim().str()] =\n+          attr.getValue().dyn_cast<StringAttr>().strref().trim().str();\n+    }\n+  }\n+\n+  if (!funcs.empty()) {\n+    // When using the Math Dialect, it is possible that some ops (e.g., log) are\n+    // lowered to a function call. In this case, we need to link libdevice\n+    // using its default path:\n+    // [triton root dir]/python/triton/language/libdevice.10.bc\n+    // TODO(Keren): handle external linkage other than libdevice?\n+    namespace fs = std::filesystem;\n+    static const std::string libdevice = \"libdevice\";\n+    static const std::filesystem::path path = std::filesystem::path(__FILE__)\n+                                                  .parent_path()\n+                                                  .parent_path()\n+                                                  .parent_path()\n+                                                  .parent_path() /\n+                                              \"python\" / \"triton\" / \"language\" /\n+                                              \"libdevice.10.bc\";\n+    externLibs.try_emplace(libdevice, path.string());\n+  }\n+\n+  return externLibs;\n+}\n+\n+static void linkLibdevice(llvm::Module &module) {\n+  // please check https://llvm.org/docs/NVPTXUsage.html#reflection-parameters\n+  // this will enable fast math path in libdevice\n+  // for example, when enable nvvm-reflect-ftz, sqrt.approx.f32 will change to\n+  // sqrt.approx.ftz.f32\n+  auto &ctx = module.getContext();\n+  llvm::Type *i32 = llvm::Type::getInt32Ty(ctx);\n+  llvm::Metadata *mdFour =\n+      llvm::ConstantAsMetadata::get(llvm::ConstantInt::getSigned(i32, 4));\n+  llvm::Metadata *mdName = llvm::MDString::get(ctx, \"nvvm-reflect-ftz\");\n+  llvm::Metadata *mdOne =\n+      llvm::ConstantAsMetadata::get(llvm::ConstantInt::getSigned(i32, 1));\n+  llvm::MDNode *reflect = llvm::MDNode::get(ctx, {mdFour, mdName, mdOne});\n+  module.addModuleFlag(reflect);\n+}\n+\n+static bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n+                          llvm::StringRef path) {\n+  llvm::SMDiagnostic err;\n+  auto &ctx = module.getContext();\n+\n+  auto extMod = llvm::parseIRFile(path, err, ctx);\n+  if (!extMod) {\n+    llvm::errs() << \"Failed to load \" << path;\n+    return true;\n+  }\n+\n+  extMod->setTargetTriple(module.getTargetTriple());\n+  extMod->setDataLayout(module.getDataLayout());\n+\n+  if (llvm::Linker::linkModules(module, std::move(extMod),\n+                                llvm::Linker::Flags::LinkOnlyNeeded)) {\n+    llvm::errs() << \"Failed to link \" << path;\n+    return true;\n+  }\n+\n+  if (name == \"libdevice\") {\n+    linkLibdevice(module);\n+  } else {\n+    assert(false && \"unknown extern lib: \");\n+  }\n+\n+  return false;\n+}\n+\n std::unique_ptr<llvm::Module>\n translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module) {\n-  auto context = module->getContext();\n   DialectRegistry registry;\n   mlir::registerLLVMDialectTranslation(registry);\n   mlir::registerNVVMDialectTranslation(registry);\n-  context->appendDialectRegistry(registry);\n+  module->getContext()->appendDialectRegistry(registry);\n \n   llvm::DenseMap<llvm::StringRef, NVVMMetadata> nvvmMetadata;\n   extractNVVMMetadata(module, &nvvmMetadata);\n@@ -100,6 +197,20 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module) {\n     return nullptr;\n   }\n \n+  // Link external libraries before perform optimizations\n+  // Note from libdevice users guide:\n+  // https://docs.nvidia.com/cuda/libdevice-users-guide/basic-usage.html\n+  // The standard process for linking with libdevice is to first link it with\n+  // the target module, then run the standard LLVM optimization and code\n+  // generation passes. This allows the optimizers to inline and perform\n+  // analyses on the used library functions, and eliminate any used functions as\n+  // dead code.\n+  auto externLibs = getExternLibs(module);\n+  for (auto &lib : externLibs) {\n+    if (linkExternLib(*llvmModule, lib.first, lib.second))\n+      return nullptr;\n+  }\n+\n   auto optPipeline = mlir::makeOptimizingTransformer(\n       /*optLevel=*/3, /*sizeLevel=*/0,\n       /*targetMachine=*/nullptr);\n@@ -147,49 +258,12 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n     return nullptr;\n   }\n \n-  std::map<std::string, std::string> externLibs;\n-  SmallVector<LLVM::LLVMFuncOp> funcs;\n-  module.walk([&](LLVM::LLVMFuncOp func) {\n-    if (func.isExternal())\n-      funcs.push_back(func);\n-  });\n-\n-  for (auto &func : funcs) {\n-    if (func.getOperation()->hasAttr(\"libname\")) {\n-      auto name =\n-          func.getOperation()->getAttr(\"libname\").dyn_cast<StringAttr>();\n-      auto path =\n-          func.getOperation()->getAttr(\"libpath\").dyn_cast<StringAttr>();\n-      if (name) {\n-        std::string lib_name = name.str();\n-        externLibs[lib_name] = path.str();\n-      }\n-    }\n-  }\n-\n-  if (module.getOperation()->hasAttr(\"triton_gpu.externs\")) {\n-    auto dict = module.getOperation()\n-                    ->getAttr(\"triton_gpu.externs\")\n-                    .dyn_cast<DictionaryAttr>();\n-    for (auto &attr : dict) {\n-      externLibs[attr.getName().strref().trim().str()] =\n-          attr.getValue().dyn_cast<StringAttr>().strref().trim().str();\n-    }\n-  }\n-\n-  auto llvmir = translateLLVMToLLVMIR(llvmContext, module);\n-  if (!llvmir) {\n+  auto llvmIR = translateLLVMToLLVMIR(llvmContext, module);\n+  if (!llvmIR) {\n     llvm::errs() << \"Translate to LLVM IR failed\";\n     return nullptr;\n   }\n-\n-  llvm::SMDiagnostic err;\n-  for (auto &lib : externLibs) {\n-    if (linkExternLib(*llvmir, lib.second))\n-      return nullptr;\n-  }\n-\n-  return llvmir;\n+  return llvmIR;\n }\n \n void addExternalLibs(mlir::ModuleOp &module,\n@@ -211,27 +285,5 @@ void addExternalLibs(mlir::ModuleOp &module,\n   module.getOperation()->setAttr(\"triton_gpu.externs\", dict);\n }\n \n-bool linkExternLib(llvm::Module &module, llvm::StringRef path) {\n-  llvm::SMDiagnostic err;\n-  auto &ctx = module.getContext();\n-\n-  auto extMod = llvm::parseIRFile(path, err, ctx);\n-  if (!extMod) {\n-    llvm::errs() << \"Failed to load \" << path;\n-    return true;\n-  }\n-\n-  extMod->setTargetTriple(module.getTargetTriple());\n-  extMod->setDataLayout(module.getDataLayout());\n-\n-  if (llvm::Linker::linkModules(module, std::move(extMod),\n-                                llvm::Linker::Flags::LinkOnlyNeeded)) {\n-    llvm::errs() << \"Failed to link \" << path;\n-    return true;\n-  }\n-\n-  return false;\n-}\n-\n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 10, "deletions": 50, "changes": 60, "file_content_changes": "@@ -8,7 +8,6 @@\n #include \"llvm/MC/TargetRegistry.h\"\n #include \"llvm/Support/TargetSelect.h\"\n #include \"llvm/Target/TargetMachine.h\"\n-#include <filesystem>\n \n namespace triton {\n \n@@ -31,68 +30,29 @@ static bool findAndReplace(std::string &str, const std::string &begin,\n   return true;\n }\n \n-static void linkExternal(llvm::Module &module) {\n-  bool hasExternal = false;\n-  for (auto &func : module) {\n-    if (func.hasExternalLinkage()) {\n-      hasExternal = true;\n-      break;\n-    }\n-  }\n-\n-  if (hasExternal) {\n-    namespace fs = std::filesystem;\n-    // [triton root dir]/python/triton/language/libdevice.10.bc\n-    static const fs::path libdevice = fs::path(__FILE__)\n-                                          .parent_path()\n-                                          .parent_path()\n-                                          .parent_path()\n-                                          .parent_path() /\n-                                      \"python\" / \"triton\" / \"language\" /\n-                                      \"libdevice.10.bc\";\n-    if (mlir::triton::linkExternLib(module, libdevice.string()))\n-      llvm::errs() << \"link failed for: \" << libdevice.string();\n-\n-    // please check https://llvm.org/docs/NVPTXUsage.html#reflection-parameters\n-    // this will enable fast math path in libdevice\n-    // for example, when enable nvvm-reflect-ftz, sqrt.approx.f32 will change to\n-    // sqrt.approx.ftz.f32\n-    auto &ctx = module.getContext();\n-    llvm::Type *I32 = llvm::Type::getInt32Ty(ctx);\n-    llvm::Metadata *mdFour =\n-        llvm::ConstantAsMetadata::get(llvm::ConstantInt::getSigned(I32, 4));\n-    llvm::Metadata *mdName = llvm::MDString::get(ctx, \"nvvm-reflect-ftz\");\n-    llvm::Metadata *mdOne =\n-        llvm::ConstantAsMetadata::get(llvm::ConstantInt::getSigned(I32, 1));\n-    llvm::MDNode *reflect = llvm::MDNode::get(ctx, {mdFour, mdName, mdOne});\n-    module.addModuleFlag(reflect);\n-  }\n-}\n-\n std::string translateLLVMIRToPTX(llvm::Module &module, int cc, int version) {\n-  linkExternal(module);\n-\n-  // LLVM version in use may not officially support target hardware\n-  int maxNNVMCC = 75;\n+  // LLVM version in use may not officially support target hardware.\n+  // Supported versions for LLVM 14 are here:\n+  // https://github.com/llvm/llvm-project/blob/f28c006a5895fc0e329fe15fead81e37457cb1d1/clang/include/clang/Basic/BuiltinsNVPTX.def\n+  int maxPTX = std::min(75, version);\n+  int maxCC = std::min(86, cc);\n   // options\n   auto options = llvm::cl::getRegisteredOptions();\n   auto *shortPtr =\n       static_cast<llvm::cl::opt<bool> *>(options[\"nvptx-short-ptr\"]);\n   assert(shortPtr);\n   shortPtr->setValue(true);\n-  // compute capability\n-  std::string sm = \"sm_\" + std::to_string(cc);\n+  std::string sm = \"sm_\" + std::to_string(maxCC);\n   // max PTX version\n-  int ptxMajor = version / 10;\n-  int ptxMinor = version % 10;\n+  int ptxMajor = maxPTX / 10;\n+  int ptxMinor = maxPTX % 10;\n   // create\n   llvm::SmallVector<char, 0> buffer;\n   std::string triple = \"nvptx64-nvidia-cuda\";\n-  std::string proc = \"sm_\" + std::to_string(std::min(cc, maxNNVMCC));\n+  std::string proc = \"sm_\" + std::to_string(maxCC);\n   std::string layout = \"\";\n   std::string features = \"\";\n-  // std::string features = \"+ptx\" + std::to_string(std::min(ptx,\n-  // max_nvvm_ptx));\n+  // std::string features = \"+ptx\" + std::to_string(maxPTX);\n   initLLVM();\n   // verify and store llvm\n   llvm::legacy::PassManager pm;"}, {"filename": "python/examples/copy_strided.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -15,5 +15,5 @@ def kernel(X, stride_xm,\n     tl.store(Zs, tl.load(Xs))\n \n \n-ret = triton.compile(kernel, \"*fp32,i32,*fp32,i32\", constants={\"BLOCK_M\": 64, \"BLOCK_N\": 64}, output=\"ttgir\")\n+ret = triton.compile(kernel, signature=\"*fp32,i32,*fp32,i32\", constants={\"BLOCK_M\": 64, \"BLOCK_N\": 64}, output=\"ttgir\")\n print(ret)"}, {"filename": "python/setup.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -141,10 +141,10 @@ def build_extension(self, ext):\n             \"-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=\" + extdir,\n             \"-DTRITON_BUILD_TUTORIALS=OFF\",\n             \"-DTRITON_BUILD_PYTHON_MODULE=ON\",\n-            # '-DPYTHON_EXECUTABLE=' + sys.executable,\n-            '-DCMAKE_VERBOSE_MAKEFILE:BOOL=ON',\n+            \"-DPython3_EXECUTABLE:FILEPATH=\" + sys.executable,\n+            \"-DCMAKE_VERBOSE_MAKEFILE:BOOL=ON\",\n             \"-DPYTHON_INCLUDE_DIRS=\" + python_include_dir,\n-            \"-DLLVM_EXTERNAL_LIT=\" + lit_dir\n+            \"-DLLVM_EXTERNAL_LIT=\" + lit_dir,\n         ] + thirdparty_cmake_args\n \n         # configuration\n@@ -173,7 +173,7 @@ def build_extension(self, ext):\n     author_email=\"phil@openai.com\",\n     description=\"A language and compiler for custom Deep Learning operations\",\n     long_description=\"\",\n-    packages=[\"triton\", \"triton/_C\", \"triton/language\", \"triton/tools\", \"triton/ops\", \"triton/runtime\", \"triton/ops/blocksparse\"],\n+    packages=[\"triton\", \"triton/_C\", \"triton/language\", \"triton/tools\", \"triton/impl\", \"triton/ops\", \"triton/runtime\", \"triton/ops/blocksparse\"],\n     install_requires=[\n         \"cmake\",\n         \"filelock\","}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 42, "deletions": 7, "changes": 49, "file_content_changes": "@@ -468,18 +468,41 @@ void init_triton_ir(py::module &&m) {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n                  loc, v, self.getI1Type()));\n            })\n+      .def(\"get_int8\",\n+           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 loc, v, self.getI8Type()));\n+           })\n       .def(\"get_int32\",\n            [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n                  loc, v, self.getI32Type()));\n            })\n-      // .def(\"get_uint32\", &ir::builder::get_int32, ret::reference)\n-      // .def(\"get_int64\", [](ir::builder *self, int64_t v) { return\n-      // self->get_int64((uint64_t)v); }, ret::reference) .def(\"get_uint64\",\n-      // &ir::builder::get_int64, ret::reference) .def(\"get_float16\",\n-      // &ir::builder::get_float16, ret::reference)\n-      .def(\"get_float32\",\n+      .def(\"get_int64\",\n+           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 loc, v, self.getI64Type()));\n+           })\n+      // bfloat16 cannot be initialized as it is treated as int16 for now\n+      //.def(\"get_bf16\",\n+      //     [](mlir::OpBuilder &self, float v) -> mlir::Value {\n+      //       auto loc = self.getUnknownLoc();\n+      //       auto type = self.getBF16Type();\n+      //       return self.create<mlir::arith::ConstantFloatOp>(\n+      //           loc,\n+      //           mlir::APFloat(type.getFloatSemantics(), std::to_string(v)),\n+      //           type);\n+      //     })\n+      .def(\"get_fp16\",\n+           [](mlir::OpBuilder &self, float v) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<mlir::arith::ConstantOp>(\n+                 loc, self.getF16FloatAttr(v));\n+           })\n+      .def(\"get_fp32\",\n            [](mlir::OpBuilder &self, float v) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::ConstantOp>(\n@@ -1333,11 +1356,23 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::createTritonGPUPrefetchPass());\n            })\n-      .def(\"add_triton_gpu_combine_pass\",\n+      .def(\"add_tritongpu_combine_pass\",\n            [](mlir::PassManager &self, int computeCapability) {\n              self.addPass(\n                  mlir::createTritonGPUCombineOpsPass(computeCapability));\n            })\n+      .def(\"add_tritongpu_update_mma_for_volta_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::createTritonGPUUpdateMmaForVoltaPass());\n+           })\n+      .def(\"add_tritongpu_reorder_instructions_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::createTritonGPUReorderInstructionsPass());\n+           })\n+      .def(\"add_tritongpu_decompose_conversions_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::createTritonGPUDecomposeConversionsPass());\n+           })\n       .def(\"add_triton_gpu_to_llvm\",\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::triton::createConvertTritonGPUToLLVMPass());"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 52, "deletions": 31, "changes": 83, "file_content_changes": "@@ -773,23 +773,26 @@ def kernel(X, Z, BITCAST: tl.constexpr):\n         assert to_numpy(z_tri) == z_ref\n \n \n-def test_store_bool():\n+@pytest.mark.parametrize(\"dtype_str\", [dtype_str for dtype_str in torch_dtypes])\n+def test_store_constant(dtype_str):\n+    check_type_supported(dtype_str)\n+\n     \"\"\"Tests that boolean True is stored as 1\"\"\"\n     @triton.jit\n-    def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n+    def kernel(output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n         mask = offsets < n_elements\n-        input = tl.load(input_ptr + offsets, mask=mask)\n-        output = input\n+        output = GENERATE_TEST_HERE\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n-    src = torch.tensor([True, False], dtype=torch.bool, device='cuda')\n-    n_elements = src.numel()\n-    dst = torch.empty_like(src)\n-    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-    copy_kernel[grid](src, dst, n_elements, BLOCK_SIZE=1024)\n+    triton_dtype_str = 'uint8' if dtype_str == 'bool' else dtype_str\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.zeros([BLOCK_SIZE], dtype=tl.{triton_dtype_str}) + 1'})\n+    block_size = 128\n+    ref = torch.ones([block_size], dtype=getattr(torch, dtype_str), device='cuda')\n+    output = torch.zeros([block_size], dtype=getattr(torch, dtype_str), device='cuda')\n+    kernel[(1,)](output, block_size, BLOCK_SIZE=block_size)\n \n-    assert (to_numpy(src).view('uint8') == to_numpy(dst).view('uint8')).all()\n+    assert torch.all(output == ref)\n \n \n @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n@@ -882,7 +885,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n \n \n def get_reduced_dtype(dtype_str, op):\n-    if op == 'argmin' or op == 'argmax':\n+    if op in ('argmin', 'argmax'):\n         return 'int32'\n     if dtype_str in ['int8', 'uint8', 'int16', 'uint16']:\n         return 'int32'\n@@ -893,7 +896,7 @@ def get_reduced_dtype(dtype_str, op):\n \n @pytest.mark.parametrize(\"op, dtype_str, shape\",\n                          [(op, dtype, shape)\n-                          for op in ['min', 'max', 'sum']\n+                          for op in ['min', 'max', 'sum', 'argmin', 'argmax']\n                           for dtype in dtypes_with_bfloat16\n                           for shape in [32, 64, 128, 512]])\n def test_reduce1d(op, dtype_str, shape, device='cuda'):\n@@ -914,7 +917,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n     # numpy result\n-    z_dtype_str = get_reduced_dtype(dtype_str, op)\n+    z_dtype_str = 'int32' if op in ('argmin', 'argmax') else dtype_str\n     z_tri_dtype_str = z_dtype_str\n     if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n         z_dtype_str = 'float32'\n@@ -933,7 +936,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n     if op == 'sum':\n         np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n     else:\n-        if op == 'argmin' or op == 'argmax':\n+        if op in ('argmin', 'argmax'):\n             # argmin and argmax can have multiple valid indices.\n             # so instead we compare the values pointed by indices\n             np.testing.assert_equal(x[z_ref], x[z_tri])\n@@ -1010,7 +1013,7 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     if op == 'sum':\n         np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n     else:\n-        if op == 'argmin' or op == 'argmax':\n+        if op in ('argmin', 'argmax'):\n             # argmin and argmax can have multiple valid indices.\n             # so instead we compare the values pointed by indices\n             z_ref_index = np.expand_dims(z_ref, axis=axis)\n@@ -1228,19 +1231,23 @@ def kernel(X, stride_xm, stride_xk,\n         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n \n \n-def test_dot_without_load():\n+@pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n+def test_dot_without_load(dtype_str):\n     @triton.jit\n-    def kernel(out):\n-        pid = tl.program_id(axis=0)\n-        a = tl.zeros((32, 32), tl.float32)\n-        b = tl.zeros((32, 32), tl.float32)\n-        c = tl.zeros((32, 32), tl.float32)\n+    def _kernel(out):\n+        a = GENERATE_TEST_HERE\n+        b = GENERATE_TEST_HERE\n         c = tl.dot(a, b)\n-        pout = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n-        tl.store(pout, c)\n-\n-    out = torch.ones((32, 32), dtype=torch.float32, device=\"cuda\")\n+        out_ptr = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n+        tl.store(out_ptr, c)\n+\n+    kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.full((32, 32), 1.0, tl.{dtype_str})\"})\n+    a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+    b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+    out_ref = torch.matmul(a, b)\n+    out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n     kernel[(1,)](out)\n+    assert torch.all(out == out_ref)\n \n # ---------------\n # test arange\n@@ -1267,7 +1274,7 @@ def _kernel(z, BLOCK: tl.constexpr,\n # ---------------\n \n \n-@pytest.mark.parametrize(\"dtype_str, size, size_diff\", [(dtype_str, size, size_diff) for dtype_str in torch_dtypes for size in [128, 512] for size_diff in [1, 2, 3, 4]])\n+@pytest.mark.parametrize(\"dtype_str, size, size_diff\", [(dtype_str, size, size_diff) for dtype_str in torch_dtypes for size in [128, 512] for size_diff in [0, 1, 2, 3, 4]])\n def test_masked_load(dtype_str, size, size_diff, device='cuda'):\n     dtype = getattr(torch, dtype_str)\n     check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n@@ -1286,18 +1293,18 @@ def test_masked_load(dtype_str, size, size_diff, device='cuda'):\n     def _kernel(in_ptr, out_ptr, in_size: tl.constexpr, out_size: tl.constexpr):\n         in_offsets = tl.arange(0, out_size)\n         # Load inputs.\n-        x = tl.load(in_ptr + in_offsets, mask=in_offsets < in_size, other=1)\n+        x = GENERATE_TEST_HERE\n         # Store output\n         output_offsets = tl.arange(0, out_size)\n         tl.store(out_ptr + output_offsets, x)\n \n-    _kernel[(1,)](input, output, input_size, output_size)\n+    mask_str = \"mask=in_offsets < in_size, other=1\" if size_diff > 0 else \"None\"\n+    kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.load(in_ptr + in_offsets, {mask_str})\"})\n+    kernel[(1,)](input, output, input_size, output_size)\n \n-    reference_out = input\n-    reference_out = torch.cat((reference_out, torch.ones((size_diff,), dtype=dtype, device=device)))\n+    reference_out = torch.cat((input, torch.ones((size_diff,), dtype=dtype, device=device)))\n     triton.testing.allclose(output, reference_out)\n \n-# 'bfloat16': torch.bfloat16,\n # Testing masked loads with an intermate copy to shared memory run.\n \n \n@@ -1446,6 +1453,20 @@ def kernel(x):\n     kernel[(1, )](x)\n \n \n+@pytest.mark.parametrize(\"device\", ['cuda', 'cpu'])\n+def test_pointer_arguments(device):\n+    @triton.jit\n+    def kernel(x):\n+        pass\n+    x = torch.empty(1024, device=device)\n+    result = True\n+    try:\n+        kernel[(1,)](x)\n+    except ValueError:\n+        result = True if device == 'cpu' else False\n+    assert result\n+\n+\n @pytest.mark.parametrize(\"value, value_type\", [\n     (-1, 'i32'), (0, 'i32'), (-2**31, 'i32'), (2**31 - 1, 'i32'),\n     (2**31, 'u32'), (2**32 - 1, 'u32'), (2**32, 'i64'), (2**63 - 1, 'i64'),"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 53, "deletions": 33, "changes": 86, "file_content_changes": "@@ -734,10 +734,6 @@ def visit_BoolOp(self, node: ast.BoolOp):\n         assert len(node.values) == 2\n         lhs = self.visit(node.values[0])\n         rhs = self.visit(node.values[1])\n-        if isinstance(lhs, triton.language.constexpr):\n-            lhs = lhs.value\n-        if isinstance(rhs, triton.language.constexpr):\n-            rhs = rhs.value\n \n         fn = {\n             ast.And: 'logical_and',\n@@ -894,17 +890,24 @@ def ttir_to_ttgir(mod, num_warps, num_stages, compute_capability):\n     pm.add_coalesce_pass()\n     # The combine pass converts blocked layout to mma layout\n     # for dot ops so that pipeline can get shared memory swizzled correctly.\n-    pm.add_triton_gpu_combine_pass(compute_capability)\n+    pm.add_tritongpu_combine_pass(compute_capability)\n     pm.add_tritongpu_pipeline_pass(num_stages)\n     # Prefetch must be done after pipeline pass because pipeline pass\n     # extracts slices from the original tensor.\n     pm.add_tritongpu_prefetch_pass()\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n-    pm.add_triton_gpu_combine_pass(compute_capability)\n+    pm.add_tritongpu_combine_pass(compute_capability)\n     pm.add_licm_pass()\n-    pm.add_triton_gpu_combine_pass(compute_capability)\n+    pm.add_tritongpu_combine_pass(compute_capability)\n+    if compute_capability // 10 == 7:\n+        # The update_mma_for_volta pass helps to compute some information for MMA encoding specifically for MMAv1\n+        pm.add_tritongpu_update_mma_for_volta_pass()\n     pm.add_cse_pass()\n+    pm.add_tritongpu_decompose_conversions_pass()\n+    pm.add_cse_pass()\n+    pm.add_symbol_dce_pass()\n+    pm.add_tritongpu_reorder_instructions_pass()\n     pm.run(mod)\n     return mod\n \n@@ -967,23 +970,12 @@ def ptx_get_version(cuda_version) -> int:\n     '''\n     assert isinstance(cuda_version, str)\n     major, minor = map(int, cuda_version.split('.'))\n-    version = major * 1000 + minor * 10\n-    if version >= 11040:\n-        return 74\n-    if version >= 11030:\n-        return 73\n-    if version >= 11020:\n-        return 72\n-    if version >= 11010:\n-        return 71\n-    if version >= 11000:\n-        return 70\n-    if version >= 10020:\n-        return 65\n-    if version >= 10010:\n-        return 64\n-    if version >= 10000:\n-        return 63\n+    if major == 12:\n+        return 80 + minor\n+    if major == 11:\n+        return 70 + minor\n+    if major == 10:\n+        return 63 + minor\n     raise RuntimeError(\"Triton only support CUDA 10.0 or higher\")\n \n \n@@ -1082,6 +1074,7 @@ def format_of(ty):\n     # generate glue code\n     src = f\"\"\"\n #include \\\"cuda.h\\\"\n+#include <stdbool.h>\n #include <Python.h>\n \n static inline void gpuAssert(CUresult code, const char *file, int line)\n@@ -1107,12 +1100,22 @@ def format_of(ty):\n   }}\n }}\n \n-static inline CUdeviceptr getPointer(PyObject *obj, int idx) {{\n+typedef struct _DevicePtrInfo {{\n+    CUdeviceptr dev_ptr;\n+    bool valid;\n+}} DevicePtrInfo;\n+\n+static inline DevicePtrInfo getPointer(PyObject *obj, int idx) {{\n+  DevicePtrInfo ptr_info;\n+  ptr_info.dev_ptr = 0;\n+  ptr_info.valid = true;\n   if (PyLong_Check(obj)) {{\n-    return (CUdeviceptr)PyLong_AsUnsignedLongLong(obj);\n+    ptr_info.dev_ptr = PyLong_AsUnsignedLongLong(obj);\n+    return ptr_info;\n   }}\n   if (obj == Py_None) {{\n-    return (CUdeviceptr)0;\n+    // valid nullptr\n+    return ptr_info;\n   }}\n   PyObject *ptr = PyObject_GetAttrString(obj, \"data_ptr\");\n   if(ptr){{\n@@ -1122,11 +1125,23 @@ def format_of(ty):\n     Py_DECREF(ptr);\n     if (!PyLong_Check(ret)) {{\n       PyErr_SetString(PyExc_TypeError, \"data_ptr method of Pointer object must return 64-bit int\");\n+      ptr_info.valid = false;\n+      return ptr_info;\n     }}\n-    return (CUdeviceptr)PyLong_AsUnsignedLongLong(ret);\n+    ptr_info.dev_ptr = PyLong_AsUnsignedLongLong(ret);\n+    unsigned attr;\n+    CUresult status =\n+        cuPointerGetAttribute(&attr, CU_POINTER_ATTRIBUTE_MEMORY_TYPE, ptr_info.dev_ptr);\n+    if (!(attr == CU_MEMORYTYPE_DEVICE || attr == CU_MEMORYTYPE_UNIFIED) ||\n+        !(status == CUDA_SUCCESS)) {{\n+        PyErr_Format(PyExc_ValueError,\n+                     \"Pointer argument (at %d) cannot be accessed from Triton (cpu tensor?)\", idx);\n+        ptr_info.valid = false;\n+    }}\n+    return ptr_info;\n   }}\n   PyErr_SetString(PyExc_TypeError, \"Pointer argument must be either uint64 or have data_ptr method\");\n-  return (CUdeviceptr)0;\n+  return ptr_info;\n }}\n \n static PyObject* launch(PyObject* self, PyObject* args) {{\n@@ -1150,7 +1165,10 @@ def format_of(ty):\n     Py_DECREF(new_args);\n   }}\n \n-  _launch(gridX, gridY, gridZ, num_warps, shared_memory, (CUstream)_stream, (CUfunction)_function, {', '.join(f\"getPointer(_arg{i},{i})\" if ty[0]==\"*\" else f\"_arg{i}\"for i, ty in signature.items())});\n+\n+  // raise exception asap\n+  {\"; \".join([f\"DevicePtrInfo ptr_info{i} = getPointer(_arg{i}, {i}); if (!ptr_info{i}.valid) return NULL;\" if ty[0] == \"*\" else \"\" for i, ty in signature.items()])};\n+  _launch(gridX, gridY, gridZ, num_warps, shared_memory, (CUstream)_stream, (CUfunction)_function, {', '.join(f\"ptr_info{i}.dev_ptr\" if ty[0]==\"*\" else f\"_arg{i}\"for i, ty in signature.items())});\n \n   if (launch_exit_hook != Py_None) {{\n     PyObject *new_args = NULL;\n@@ -1469,9 +1487,9 @@ def compile(fn, **kwargs):\n         import re\n         match = re.search(prototype_pattern[ir], src, re.MULTILINE)\n         name, signature = match.group(1), match.group(2)\n-        print(name, signature)\n+        # print(name, signature)\n         types = re.findall(arg_type_pattern[ir], signature)\n-        print(types)\n+        # print(types)\n         param_tys = [convert_type_repr(ty) for ty in types]\n         signature = {k: v for k, v in enumerate(param_tys)}\n         first_stage = list(stages.keys()).index(ir)\n@@ -1562,6 +1580,7 @@ def _init_handles(self):\n         if self.shared > max_shared:\n             raise OutOfResources(self.shared, max_shared, \"shared memory\")\n         mod, func, n_regs, n_spills = cuda_utils.load_binary(self.metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n+        # print(self.shared, n_regs, n_spills)\n         self.cu_module = mod\n         self.cu_function = func\n \n@@ -1601,7 +1620,8 @@ def __new__(cls):\n             cls.instance = super(CudaUtils, cls).__new__(cls)\n         return cls.instance\n \n-    def _generate_src(self):\n+    @staticmethod\n+    def _generate_src():\n         return \"\"\"\n         #include <cuda.h>\n "}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -5,6 +5,7 @@\n     ir,\n     builtin,\n )\n+from . import libdevice\n from .core import (\n     abs,\n     arange,\n@@ -30,6 +31,7 @@\n     dot,\n     dtype,\n     exp,\n+    full,\n     fdiv,\n     float16,\n     float32,\n@@ -123,13 +125,15 @@\n     \"float32\",\n     \"float64\",\n     \"float8\",\n+    \"full\",\n     \"function_type\",\n     \"int1\",\n     \"int16\",\n     \"int32\",\n     \"int64\",\n     \"int8\",\n     \"ir\",\n+    \"libdevice\",\n     \"load\",\n     \"log\",\n     \"max\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 58, "deletions": 22, "changes": 80, "file_content_changes": "@@ -26,7 +26,7 @@ def _to_tensor(x, builder):\n         else:\n             raise RuntimeError(f'Nonrepresentable integer {x}.')\n     elif isinstance(x, float):\n-        return tensor(builder.get_float32(x), float32)\n+        return tensor(builder.get_fp32(x), float32)\n     elif isinstance(x, constexpr):\n         return _to_tensor(x.value, builder)\n     elif isinstance(x, tensor):\n@@ -139,13 +139,16 @@ def is_int(self):\n     def is_bool(self):\n         return self.is_int1()\n \n-    def is_void(self):\n+    @staticmethod\n+    def is_void():\n         raise RuntimeError(\"Not implemented\")\n \n-    def is_block(self):\n+    @staticmethod\n+    def is_block():\n         return False\n \n-    def is_ptr(self):\n+    @staticmethod\n+    def is_ptr():\n         return False\n \n     def __eq__(self, other: dtype):\n@@ -168,13 +171,13 @@ def to_ir(self, builder: ir.builder) -> ir.type:\n             return builder.get_void_ty()\n         elif self.name == 'int1':\n             return builder.get_int1_ty()\n-        elif self.name == 'int8' or self.name == 'uint8':\n+        elif self.name in ('int8', 'uint8'):\n             return builder.get_int8_ty()\n-        elif self.name == 'int16' or self.name == 'uint16':\n+        elif self.name in ('int16', 'uint16'):\n             return builder.get_int16_ty()\n-        elif self.name == 'int32' or self.name == 'uint32':\n+        elif self.name in ('int32', 'uint32'):\n             return builder.get_int32_ty()\n-        elif self.name == 'int64' or self.name == 'uint64':\n+        elif self.name in ('int64', 'uint64'):\n             return builder.get_int64_ty()\n         elif self.name == 'fp8':\n             return builder.get_fp8_ty()\n@@ -403,6 +406,18 @@ def __bool__(self):\n     def __neg__(self):\n         return constexpr(-self.value)\n \n+    def __and__(self, other):\n+        return constexpr(self.value & other.value)\n+\n+    def logical_and(self, other):\n+        return constexpr(self.value and other.value)\n+\n+    def __or__(self, other):\n+        return constexpr(self.value | other.value)\n+\n+    def logical_or(self, other):\n+        return constexpr(self.value or other.value)\n+\n     def __pos__(self):\n         return constexpr(+self.value)\n \n@@ -678,24 +693,31 @@ def arange(start, end, _builder=None):\n     return semantic.arange(start, end, _builder)\n \n \n+def _shape_check_impl(shape):\n+    shape = _constexpr_to_value(shape)\n+    for i, d in enumerate(shape):\n+        if not isinstance(d, constexpr):\n+            raise TypeError(f\"Shape element {i} must have type `constexpr`\")\n+        if not isinstance(d.value, int):\n+            raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n+    return [_constexpr_to_value(x) for x in shape]\n+\n+\n @builtin\n-def zeros(shape, dtype, _builder=None):\n+def full(shape, value, dtype, _builder=None):\n     \"\"\"\n-    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n+    Returns a tensor filled with the scalar value for the given :code:`shape` and :code:`dtype`.\n \n     :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n+    :value value: A scalar value to fill the array with\n     :type shape: tuple of ints\n     :param dtype: Data-type of the new array, e.g., :code:`tl.float16`\n     :type dtype: DType\n     \"\"\"\n-    for i, d in enumerate(shape):\n-        if not isinstance(d, constexpr):\n-            raise TypeError(f\"Shape element {i} must have type `constexpr`\")\n-        if not isinstance(d.value, int):\n-            raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n-    shape = [x.value for x in shape]\n+    shape = _shape_check_impl(shape)\n+    value = _constexpr_to_value(value)\n     dtype = _constexpr_to_value(dtype)\n-    return semantic.zeros(shape, dtype, _builder)\n+    return semantic.full(shape, value, dtype, _builder)\n \n \n # -----------------------\n@@ -726,6 +748,7 @@ def broadcast_to(input, shape, _builder=None):\n     :param shape: The desired shape.\n     :type shape: Tuple[int]\n     \"\"\"\n+    shape = _shape_check_impl(shape)\n     return semantic.broadcast_impl_shape(input, shape, _builder)\n \n \n@@ -763,14 +786,14 @@ def view(input, shape, _builder=None):\n     :type shape: Tuple[int]\n \n     \"\"\"\n-    shape = [x.value for x in shape]\n+    shape = _shape_check_impl(shape)\n     return semantic.view(input, shape, _builder)\n \n \n @builtin\n def reshape(input, shape, _builder=None):\n     # TODO: should be more than just a view\n-    shape = [x.value for x in shape]\n+    shape = _shape_check_impl(shape)\n     return semantic.view(input, shape, _builder)\n \n # -----------------------\n@@ -818,9 +841,9 @@ def load(pointer, mask=None, other=None, cache_modifier=\"\", eviction_policy=\"\",\n     'type cache_modifier: str, optional\n     \"\"\"\n     # mask, other can be constexpr\n-    if mask is not None:\n+    if _constexpr_to_value(mask) is not None:\n         mask = _to_tensor(mask, _builder)\n-    if other is not None:\n+    if _constexpr_to_value(other) is not None:\n         other = _to_tensor(other, _builder)\n     cache_modifier = _constexpr_to_value(cache_modifier)\n     eviction_policy = _constexpr_to_value(eviction_policy)\n@@ -844,7 +867,7 @@ def store(pointer, value, mask=None, _builder=None):\n     \"\"\"\n     # value can be constexpr\n     value = _to_tensor(value, _builder)\n-    if mask is not None:\n+    if _constexpr_to_value(mask) is not None:\n         mask = _to_tensor(mask, _builder)\n     return semantic.store(pointer, value, mask, _builder)\n \n@@ -1231,6 +1254,19 @@ def swizzle2d(i, j, size_i, size_j, size_g):\n     return new_i, new_j\n \n \n+@triton.jit\n+def zeros(shape, dtype):\n+    \"\"\"\n+    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n+\n+    :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n+    :type shape: tuple of ints\n+    :param dtype: Data-type of the new array, e.g., :code:`tl.float16`\n+    :type dtype: DType\n+    \"\"\"\n+    return full(shape, 0, dtype)\n+\n+\n @triton.jit\n def zeros_like(input):\n     return zeros(input.shape, input.dtype)"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 26, "deletions": 10, "changes": 36, "file_content_changes": "@@ -471,10 +471,15 @@ def arange(start: int, end: int, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_make_range(start, end), ret_ty)\n \n \n-def zeros(shape: List[int], dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n-    _0 = builder.get_null_value(dtype.to_ir(builder))\n+def full(shape: List[int], value, dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n+    if value == 0:\n+        _value = builder.get_null_value(dtype.to_ir(builder))\n+    else:\n+        get_value_fn = getattr(builder, f\"get_{dtype.name}\")\n+        _value = get_value_fn(value)\n     ret_ty = tl.block_type(dtype, shape)\n-    return tl.tensor(builder.create_splat(_0, shape), ret_ty)\n+    return tl.tensor(builder.create_splat(_value, shape), ret_ty)\n+\n \n # ===----------------------------------------------------------------------===//\n #                               Shape Manipulation\n@@ -891,8 +896,8 @@ def atomic_max(ptr: tl.tensor,\n     # return atomic_umin(i_ptr, i_val) if val < 0\n     i_val = bitcast(val, tl.int32, builder)\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n-    pos = greater_equal(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n-    neg = less_than(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n+    pos = greater_equal(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n+    neg = less_than(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n     pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX, i_ptr.handle, i_val.handle, and_(mask, pos, builder).handle), i_val.type)\n     neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN, i_ptr.handle, i_val.handle, and_(mask, neg, builder).handle), i_val.type)\n     return where(pos, pos_ret, neg_ret, builder)\n@@ -923,8 +928,8 @@ def atomic_min(ptr: tl.tensor,\n     # return atomic_umax(i_ptr, i_val) if val < 0\n     i_val = bitcast(val, tl.int32, builder)\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n-    pos = greater_equal(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n-    neg = less_than(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n+    pos = greater_equal(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n+    neg = less_than(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n     pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MIN,\n                                                   i_ptr.handle,\n                                                   i_val.handle,\n@@ -998,7 +1003,7 @@ def dot(lhs: tl.tensor,\n         _0 = builder.get_int32(0)\n         ret_scalar_ty = tl.int32\n     else:\n-        _0 = builder.get_float32(0)\n+        _0 = builder.get_fp32(0)\n         ret_scalar_ty = tl.float32\n     M = lhs.type.shape[0]\n     N = rhs.type.shape[1]\n@@ -1036,15 +1041,18 @@ def where(condition: tl.tensor,\n def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n                 FLOAT_OP: ir.REDUCE_OP, INT_OP: ir.REDUCE_OP) -> tl.tensor:\n     scalar_ty = input.type.scalar\n+    out_scalar_ty = scalar_ty\n     # input is extended to 32-bits if necessary\n     # this increases numerical accuracy and can be done pretty much for free\n     # on GPUs\n     if scalar_ty.is_int() and scalar_ty.int_bitwidth <= 32:\n         input = cast(input, tl.int32, builder)\n+        out_scalar_ty = tl.int32\n \n     # hardware doesn't support FMAX, FMIN, CMP for bfloat16\n     if scalar_ty is tl.bfloat16:\n         input = cast(input, tl.float32, builder)\n+        out_scalar_ty = tl.float32\n \n     # choose the right unsigned operation\n     if scalar_ty.is_int_unsigned():\n@@ -1057,17 +1065,23 @@ def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n         if INT_OP in int_op_to_unit:\n             INT_OP = int_op_to_unit[INT_OP]\n \n+    # If we are doing an argmin or argmax we want to use an int32 output type\n+    if FLOAT_OP is ir.REDUCE_OP.ARGFMAX or INT_OP is ir.REDUCE_OP.ARGMAX:\n+        out_scalar_ty = tl.int32\n+    elif FLOAT_OP is ir.REDUCE_OP.ARGFMIN or INT_OP is ir.REDUCE_OP.ARGMIN:\n+        out_scalar_ty = tl.int32\n+\n     # get result type\n     shape = input.type.shape\n     ret_shape = []\n     for i, s in enumerate(shape):\n         if i != axis:\n             ret_shape.append(s)\n     if ret_shape:\n-        res_ty = tl.block_type(scalar_ty, ret_shape)\n+        res_ty = tl.block_type(out_scalar_ty, ret_shape)\n     else:\n         # 0d-tensor -> scalar\n-        res_ty = scalar_ty\n+        res_ty = out_scalar_ty\n \n     if scalar_ty.is_floating():\n         return tl.tensor(builder.create_reduce(input.handle, FLOAT_OP, axis), res_ty)\n@@ -1109,11 +1123,13 @@ def xor_sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n \n def umulhi(x: tl.tensor, y: tl.tensor, builder: ir.builder) -> tl.tensor:\n     x, y = binary_op_type_checking_impl(x, y, builder)\n+    # FIXME(Keren): not portable, should be fixed\n     from . import libdevice\n     return libdevice.mulhi(x, y, _builder=builder)\n \n \n def floor(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n+    # FIXME(Keren): not portable, should be fixed\n     from . import libdevice\n     return libdevice.floor(x, _builder=builder)\n "}, {"filename": "python/triton/ops/blocksparse/softmax.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -115,8 +115,8 @@ def _blocksparse_softmax_bwd(\n     dout = tl.load(DOuts + lane_n, mask=mask, other=0.0)\n     dout = dout.to(tl.float32)\n     # compute\n+    a = tl.where((ns > m) & is_causal & (a == a), 0., a)\n     da = a * (dout - tl.sum(a * dout, 0))\n-    da = tl.where((ns > m) & is_causal, 0., da)\n     # apply relative attention\n     if DR is not None:\n         DR += z * stride_zr"}, {"filename": "python/triton/tools/aot.py", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -1,4 +1,5 @@\n import argparse\n+import sys\n \n import triton\n import triton._C.libtriton.triton as libtriton\n@@ -24,7 +25,7 @@\n     # check for validity of format arguments\n     if args.target not in VALID_FORMATS:\n         print(\"Invalid target format: \" + args.target)\n-        exit(0)\n+        sys.exit(0)\n \n     # parse source file to MLIR module\n     context = libtriton.ir.context()\n@@ -35,7 +36,7 @@\n     module = triton.compiler.optimize_triton_ir(module)\n     if args.target == 'triton-ir':\n         print(module.str())\n-        exit(0)\n+        sys.exit(0)\n \n     if not args.sm:\n         raise argparse.ArgumentError(None, \"Must specify --sm for PTX compilation\")\n@@ -44,13 +45,13 @@\n     module = triton.compiler.ttir_to_ttgir(module, num_warps=4, num_stages=3, compute_capability=args.sm)\n     if args.target == 'triton-gpu-ir':\n         print(module.str())\n-        exit(0)\n+        sys.exit(0)\n \n     # triton-gpu-ir -> llvm-ir\n     module = triton.compiler.ttgir_to_llir(module, extern_libs=None, compute_capability=args.sm)\n     if args.target == 'llvm-ir':\n         print(module)\n-        exit(0)\n+        sys.exit(0)\n \n     if not args.ptx_version:\n         raise argparse.ArgumentError(None, \"Must specify --ptx-version for PTX compilation\")"}, {"filename": "python/triton/tools/build_extern.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -157,7 +157,8 @@ def __init__(self, path) -> None:\n         super().__init__(\"libdevice\", path)\n         self._symbol_groups = {}\n \n-    def _extract_symbol(self, line) -> Optional[Symbol]:\n+    @staticmethod\n+    def _extract_symbol(line) -> Optional[Symbol]:\n         # Extract symbols from line in the following format:\n         # \"define [internal] <ret_type> @<name>(<arg_types>,)\"\n         entries = line.split(\"@\")"}, {"filename": "python/triton/utils.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -33,7 +33,8 @@ def wrap_dtype(arg):\n     def __init__(self, dtype):\n         self.dtype = dtype\n \n-    def data_ptr(self):\n+    @staticmethod\n+    def data_ptr():\n         return 0  # optimistically assumes multiple of 16\n \n "}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 6, "deletions": 9, "changes": 15, "file_content_changes": "@@ -225,18 +225,18 @@ def forward(ctx, q, k, v, sm_scale):\n             q.shape[0], q.shape[1], q.shape[2],\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n             BLOCK_DMODEL=Lk, num_warps=num_warps,\n-            num_stages=1,\n+            num_stages=2,\n         )\n \n         ctx.save_for_backward(q, k, v, o, L, m)\n-        ctx.BLOCK = BLOCK\n         ctx.grid = grid\n         ctx.sm_scale = sm_scale\n         ctx.BLOCK_DMODEL = Lk\n         return o\n \n     @staticmethod\n     def backward(ctx, do):\n+        BLOCK = 128\n         q, k, v, o, l, m = ctx.saved_tensors\n         do = do.contiguous()\n         dq = torch.zeros_like(q, dtype=torch.float32)\n@@ -247,11 +247,8 @@ def backward(ctx, do):\n         _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n             o, do, l,\n             do_scaled, delta,\n-            BLOCK_M=ctx.BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n+            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n         )\n-\n-        # NOTE: kernel currently buggy for other values of `num_warps`\n-        num_warps = 8\n         _bwd_kernel[(ctx.grid[1],)](\n             q, k, v, ctx.sm_scale,\n             o, do_scaled,\n@@ -263,8 +260,8 @@ def backward(ctx, do):\n             v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n             q.shape[0], q.shape[1], q.shape[2],\n             ctx.grid[0],\n-            BLOCK_M=ctx.BLOCK, BLOCK_N=ctx.BLOCK,\n-            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=num_warps,\n+            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n             num_stages=1,\n         )\n         return dq, dk, dv, None\n@@ -319,7 +316,7 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n # vary seq length for fixed head and batch=4\n configs = [triton.testing.Benchmark(\n     x_names=['N_CTX'],\n-    x_vals=[2**i for i in range(10, 16)],\n+    x_vals=[2**i for i in range(10, 15)],\n     line_arg='provider',\n     line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n     line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -3,6 +3,7 @@\n #AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#A_SHARED_T = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [0, 1]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n@@ -52,6 +53,15 @@ func @convert(%A : !tt.ptr<f16>) {\n   return\n }\n \n+// CHECK-LABEL: trans\n+func @trans(%A : !tt.ptr<f16>) {\n+  // CHECK: %cst -> %cst\n+  %tensor = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n+  // CHECK: %0 -> %cst\n+  %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n+  return\n+}\n+\n // CHECK-LABEL: insert_slice_async\n func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "file_content_changes": "@@ -4,6 +4,7 @@\n #sliceAd0 = #triton_gpu.slice<{dim = 0, parent = #AL}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#A_SHARED_T = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [0, 1]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n@@ -174,6 +175,14 @@ func @scratch() {\n   // CHECK-NEXT: size = 512\n }\n \n+// CHECK-LABEL: trans\n+func @trans(%A : !tt.ptr<f16>) {\n+  // CHECK: offset = 0, size = 1024\n+  %tensor = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n+  %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n+  return\n+}\n+\n // CHECK-LABEL: insert_slice_async\n func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n@@ -285,6 +294,25 @@ func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %\n   // CHECK-NEXT: size = 24576\n }\n \n+// c0 cannot be released in the loop\n+// CHECK-LABEL: for_use_ancestor\n+func @for_use_ancestor(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+  // CHECK: offset = 0, size = 8192\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK-NEXT: offset = 8192, size = 8192\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK-NEXT: offset = 16384, size = 8192\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    %c0 = tt.trans %c_shared_init : (tensor<128x32xf16, #A_SHARED>) -> tensor<32x128xf16, #A_SHARED_T>\n+    // CHECK-NEXT: offset = 24576, size = 8192\n+    %c1 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+    scf.yield %b_shared, %a_shared: tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+  }\n+  return\n+  // CHECK-NEXT: size = 32768\n+}\n+\n // a_shared_init, b_shared_init, and c_shared_init's liveness ranges are span over the entire function before cst2.\n // So they cannot be reused by cst0 and cst1, but can be reused by cst2.\n // CHECK-LABEL: for_if_for"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -4,6 +4,7 @@\n #sliceAd0 = #triton_gpu.slice<{dim = 0, parent = #AL}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#A_SHARED_T = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [0, 1]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n@@ -111,6 +112,13 @@ func @extract_slice() {\n   return\n }\n \n+// CHECK-LABEL: trans\n+func @trans() {\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n+  %b = tt.trans %cst0 : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n+  return\n+}\n+\n // CHECK-LABEL: insert_slice_async\n func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 61, "deletions": 29, "changes": 90, "file_content_changes": "@@ -461,6 +461,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x64x!tt.ptr<f32>, #AL> -> tensor<2x16x64xf32, #A>\n+    triton_gpu.async_commit_group\n     return\n   }\n }\n@@ -506,6 +507,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x32x!tt.ptr<f32>, #AL> -> tensor<2x16x32xf32, #A>\n+    triton_gpu.async_commit_group\n     return\n   }\n }\n@@ -540,40 +542,29 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %index = arith.constant 1 : i32\n \n     // CHECK: llvm.mlir.constant(0 : i32) : i32\n+    // CHECK: llvm.mlir.constant(16 : i32) : i32\n+    // CHECK: llvm.mul\n     // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n-    // CHECK: llvm.mlir.constant(0 : i32) : i32\n-    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n-    // CHECK: llvm.mlir.constant(0 : i32) : i32\n-    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n-    // CHECK: llvm.mlir.constant(0 : i32) : i32\n-    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n-    // CHECK: llvm.mlir.constant(16 : i32) : i32\n-    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n-    // CHECK: llvm.mlir.constant(16 : i32) : i32\n-    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n-    // CHECK: llvm.mlir.constant(16 : i32) : i32\n-    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n-    // CHECK: llvm.mlir.constant(16 : i32) : i32\n-    // CHECK: llvm.add\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32x!tt.ptr<f32>, #AL> -> tensor<2x32x32xf32, #A>\n+    triton_gpu.async_commit_group\n     return\n   }\n }\n@@ -935,10 +926,10 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    // CHECK-SAME: (f32, f32, f32, f32)\n+    // CHECK-SAME: (vector<1xf32>, vector<1xf32>, vector<1xf32>, vector<1xf32>)\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    // CHECK-SAME: (f32, f32, f32, f32)\n+    // CHECK-SAME: (vector<1xf32>, vector<1xf32>, vector<1xf32>, vector<1xf32>)\n     %a_mat = triton_gpu.convert_layout %a : (tensor<32x16xf32, #shared>) -> tensor<32x16xf32, #dot_operand_a>\n     %b_mat = triton_gpu.convert_layout %b : (tensor<16x32xf32, #shared>) -> tensor<16x32xf32, #dot_operand_b>\n \n@@ -997,20 +988,61 @@ func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n // -----\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n+    // CHECK: nvvm.read.ptx.sreg.nctaid.x\n+    // CHECK: nvvm.read.ptx.sreg.nctaid.y\n+    // CHECK: nvvm.read.ptx.sreg.nctaid.z\n+    %blockdimx = tt.get_num_programs {axis=0:i32} : i32\n+    %blockdimy = tt.get_num_programs {axis=1:i32} : i32\n+    %blockdimz = tt.get_num_programs {axis=2:i32} : i32\n+    %v0 = arith.addi %blockdimx, %blockdimy : i32\n+    %v1 = arith.addi %v0, %blockdimz : i32\n+    %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n+    tt.store %a, %0 : tensor<32xi32, #blocked0>\n+  \n+    return\n+  }\n+}\n \n-func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n-  // CHECK: nvvm.read.ptx.sreg.nctaid.x\n-  // CHECK: nvvm.read.ptx.sreg.nctaid.y\n-  // CHECK: nvvm.read.ptx.sreg.nctaid.z\n-  %blockdimx = tt.get_num_programs {axis=0:i32} : i32\n-  %blockdimy = tt.get_num_programs {axis=1:i32} : i32\n-  %blockdimz = tt.get_num_programs {axis=2:i32} : i32\n-  %v0 = arith.addi %blockdimx, %blockdimy : i32\n-  %v1 = arith.addi %v0, %blockdimz : i32\n-  %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n-  tt.store %a, %0 : tensor<32xi32, #blocked0>\n-\n-  return\n+// -----\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: test_index_cache \n+  func @test_index_cache() {\n+    // CHECK: nvvm.read.ptx.sreg.tid.x\n+    %0 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n+    // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n+    %1 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n+    return\n+  }\n }\n \n+// -----\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: test_base_index_cache \n+  func @test_base_index_cache(%arg0: tensor<128x32xf32, #blocked0>) {\n+    // CHECK: nvvm.read.ptx.sreg.tid.x\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n+    // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n+    %1 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n+    return\n+  }\n }\n+\n+// -----\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: test_index_cache_different_block\n+  func @test_index_cache_different_block(%arg0: tensor<128x32xf32, #blocked0>, %arg1: i1) {\n+    // CHECK: nvvm.read.ptx.sreg.tid.x\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n+    scf.if %arg1 {\n+      // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n+      %1 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n+    }\n+    return\n+  }\n+}\n\\ No newline at end of file"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -tritongpu-combine 2>&1 | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -tritongpu-combine 2>&1 | FileCheck %s\n \n #layout0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n #layout1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n@@ -7,7 +7,6 @@\n // CHECK: [[row_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n // CHECK: [[col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK: [[col_layout_novec:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n-\n func @cst() -> tensor<1024xi32, #layout1> {\n   %cst = arith.constant dense<0> : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %cst : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n@@ -62,9 +61,9 @@ func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n // CHECK-LABEL: transpose\n func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, [[row_layout]]>\n+  // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, {{%cst.*}}, {{%cst.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, [[row_layout]]>\n   // CHECK: [[cvt_val:%.*]] = triton_gpu.convert_layout [[loaded_val]] : (tensor<64x64xf32, [[row_layout]]>) -> tensor<64x64xf32, [[col_layout]]>\n-  // CHECK: tt.store {{.*}}, [[cvt_val]], %cst_1 : tensor<64x64xf32, [[col_layout]]>\n+  // CHECK: tt.store {{.*}}, [[cvt_val]], {{%cst.*}} : tensor<64x64xf32, [[col_layout]]>\n   // CHECK: return\n   %cst = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked1>\n   %cst_0 = arith.constant dense<true> : tensor<64x64xi1, #blocked1>"}, {"filename": "test/TritonGPU/prefetch.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -17,11 +17,11 @@\n // CHECK-DAG: %[[B0_PREFETCH_SMEM:.*]] = tensor.extract_slice %[[B0:.*]][0, 0] [16, 128]\n // CHECK-DAG: %[[B0_PREFETCH:.*]] = triton_gpu.convert_layout %[[B0_PREFETCH_SMEM]]\n // CHECK:     scf.for {{.*}} iter_args({{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, %[[a0_prefetch:.*]] = %[[A0_PREFETCH]], %[[b0_prefetch:.*]] = %[[B0_PREFETCH]]\n-// CHECK:       %[[D_FIRST:.*]] = tt.dot %[[a0_prefetch]], %[[b0_prefetch:.*]], {{.*}}\n // CHECK-DAG:   %[[A_REM_SMEM:.*]] = tensor.extract_slice %[[arg_a0]][0, 16] [128, 16]\n // CHECK-DAG:   %[[A_REM:.*]] = triton_gpu.convert_layout %[[A_REM_SMEM]]\n // CHECK-DAG:   %[[B_REM_SMEM:.*]] = tensor.extract_slice %[[arg_b0]][16, 0] [16, 128]\n // CHECK-DAG:   %[[B_REM:.*]] = triton_gpu.convert_layout %[[B_REM_SMEM]]\n+// CHECK:       %[[D_FIRST:.*]] = tt.dot %[[a0_prefetch]], %[[b0_prefetch:.*]], {{.*}}\n // CHECK:       tt.dot %[[A_REM]], %[[B_REM]], %[[D_FIRST:.*]]\n // CHECK-DAG:   %[[NEXT_A_PREFETCH_SMEM:.*]] = tensor.extract_slice {{.*}}[0, 0] [128, 16]\n // CHECK-DAG:   %[[NEXT_A_PREFETCH:.*]] = triton_gpu.convert_layout %[[NEXT_A_PREFETCH_SMEM]]"}, {"filename": "test/TritonGPU/update-mma-for-volta.mlir", "status": "added", "additions": 73, "deletions": 0, "changes": 73, "file_content_changes": "@@ -0,0 +1,73 @@\n+// RUN: triton-opt %s -split-input-file -tritongpu-combine -tritongpu-update-mma-for-volta 2>&1 | FileCheck %s\n+\n+// -----\n+\n+// check the UpdateMMAVersionMinorForVolta pattern\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [4, 4], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n+#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[4,4]}>\n+// Here, the isMMAv1Row of a and b's dot_operands mismatch #mma0's versionMinor,\n+// and the pattern should update the versionMinor.\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n+// It creates a new MMA layout to fit with $a and $b's dot_operand, and get the right warpsPerCTA\n+// The ID of this MMA instance should be 0.\n+// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 4]}>\n+module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n+  // CHECK-LABEL: dot_mmav1\n+  func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n+    %C = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked0>\n+    %AA = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a>\n+    %BB = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b>\n+    %CC = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma0>\n+\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, %cst {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma]]>\n+    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<64x64xf32, #mma0>\n+    %res = triton_gpu.convert_layout %D : (tensor<64x64xf32, #mma0>) -> tensor<64x64xf32, #blocked0>\n+\n+    return %res : tensor<64x64xf32, #blocked0>\n+  }\n+}\n+\n+\n+// -----\n+// Check id in multiple MMA layout instances\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [4, 4], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n+#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[4,4]}>\n+// mma id=1, with all other boolean flags be false, should get a versionMinor of 16(= 1 * 1<<4)\n+#mma1 = #triton_gpu.mma<{versionMajor=1, versionMinor=16, warpsPerCTA=[4,4]}>\n+\n+// Will still get two MMA layouts\n+// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 4]}>\n+// CHECK: [[new_mma1:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 19, warpsPerCTA = [4, 4]}>\n+\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n+#dot_operand_a1 = #triton_gpu.dot_op<{opIdx=0, parent=#mma1, isMMAv1Row=true}>\n+#dot_operand_b1 = #triton_gpu.dot_op<{opIdx=1, parent=#mma1, isMMAv1Row=false}>\n+\n+module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n+  // CHECK-LABEL: dot_mmav1\n+  func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n+    %C = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked0>\n+    %AA = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a>\n+    %BB = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b>\n+    %CC = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma0>\n+\n+    %AA1 = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a1>\n+    %BB1 = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b1>\n+    %CC1 = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma1>\n+\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma]]>\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma1]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma1]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma1]]>\n+    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<64x64xf32, #mma0>\n+    %D1 = tt.dot %AA1, %BB1, %CC1 {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a1> * tensor<64x64xf16, #dot_operand_b1> -> tensor<64x64xf32, #mma1>\n+    %res = triton_gpu.convert_layout %D : (tensor<64x64xf32, #mma0>) -> tensor<64x64xf32, #blocked0>\n+    %res1 = triton_gpu.convert_layout %D1 : (tensor<64x64xf32, #mma1>) -> tensor<64x64xf32, #blocked0>\n+    %sum = arith.addf %res, %res1 : tensor<64x64xf32, #blocked0>\n+\n+    return %sum : tensor<64x64xf32, #blocked0>\n+  }\n+}"}, {"filename": "unittest/Dialect/TritonGPU/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -1,6 +1,5 @@\n-\n add_triton_ut(\n \tNAME TestSwizzling\n \tSRCS SwizzleTest.cpp\n \tLIBS TritonGPUIR  ${dialect_libs} ${conversion_libs}\n-)\n\\ No newline at end of file\n+)"}]