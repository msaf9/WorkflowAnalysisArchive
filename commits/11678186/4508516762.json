[{"filename": "python/triton/compiler.py", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -19,6 +19,7 @@\n from typing import Any, Callable, Dict, Optional, Tuple, Type, Union\n \n import setuptools\n+import torch\n from filelock import FileLock\n \n import triton\n@@ -1136,7 +1137,7 @@ def ttgir_to_llir(mod, extern_libs, compute_capability):\n     return _triton.translate_triton_gpu_to_llvmir(mod, compute_capability)\n \n \n-def llir_to_ptx(mod: Any, compute_capability: int, ptx_version: int = None) -> Tuple[str, int]:\n+def llir_to_ptx(mod: Any, compute_capability: int, ptx_version: int = None) -> str:\n     '''\n     Translate TritonGPU module to PTX code.\n     :param mod: a TritonGPU dialect module\n@@ -1668,8 +1669,8 @@ def _is_jsonable(x):\n def compile(fn, **kwargs):\n     capability = kwargs.get(\"cc\", None)\n     if capability is None:\n-        device = triton.runtime.jit.get_current_device()\n-        capability = triton.runtime.jit.get_device_capability(device)\n+        device = torch.cuda.current_device()\n+        capability = torch.cuda.get_device_capability(device)\n         capability = capability[0] * 10 + capability[1]\n     # we get the kernel, i.e. the first function generated in the module\n     # if fn is not a JITFunction, then it\n@@ -1804,7 +1805,7 @@ def __init__(self, fn, so_path, metadata, asm):\n     def _init_handles(self):\n         if self.cu_module is not None:\n             return\n-        device = triton.runtime.jit.get_current_device()\n+        device = torch.cuda.current_device()\n         global cuda_utils\n         init_cuda_utils()\n         max_shared = cuda_utils.get_device_properties(device)[\"max_shared_mem\"]\n@@ -1826,7 +1827,7 @@ def __getitem__(self, grid):\n \n         def runner(*args, stream=None):\n             if stream is None:\n-                stream = triton.runtime.jit.get_cuda_stream()\n+                stream = torch.cuda.current_stream().cuda_stream\n             self.c_wrapper(grid[0], grid[1], grid[2], self.num_warps, self.shared, stream, self.cu_function,\n                            CompiledKernel.launch_enter_hook, CompiledKernel.launch_exit_hook, self, *args)\n         return runner"}]