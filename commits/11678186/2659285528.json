[{"filename": "include/triton/codegen/selection/generator.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -220,7 +220,6 @@ class generator: public ir::visitor, public analysis::layout_visitor {\n   void add_extern_lib(const std::string &lib_name, const std::string &lib_path);\n \n   // Get all external libraries\n-  // XXX: It is a hack to return unique_ptr<> here.\n   const ExternLibMap &get_extern_lib_map() {\n     return extern_lib_map_;\n   }"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,7 +1,6 @@\n # flake8: noqa: F821,F841\n import itertools\n import re\n-import sys\n from typing import Optional, Union\n \n import numpy as np"}, {"filename": "python/tutorials/07-libdevice-function.py", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -7,7 +7,7 @@\n \n In `trition/language/libdevice.py`, we try to aggregate functions with the same computation but different data types together.\n For example, both `__nv_asin` and `__nvasinf` calculate the principal value of the arc sine of the input, but `__nv_asin` operates on `double` and `__nv_asinf` operates on `float`.\n-Using triton, you can simply call `tl.libdevice.asinf`. \n+Using triton, you can simply call `tl.libdevice.asinf`.\n triton automatically selects the correct underlying device function to invoke based on input and output types.\n \"\"\"\n \n@@ -23,9 +23,9 @@\n \n @triton.jit\n def asin_kernel(\n-    x_ptr,  \n+    x_ptr,\n     y_ptr,\n-    n_elements, \n+    n_elements,\n     BLOCK_SIZE: tl.constexpr,\n ):\n     pid = tl.program_id(axis=0)\n@@ -34,14 +34,14 @@ def asin_kernel(\n     mask = offsets < n_elements\n     x = tl.load(x_ptr + offsets, mask=mask)\n     x = tl.libdevice.asin(x)\n-    # Write x + y back to DRAM\n     tl.store(y_ptr + offsets, x, mask=mask)\n \n # %%\n #  Using the default libdevice library path\n # --------------------------\n # We can use the default libdevice library path encoded in `triton/language/libdevice.py`\n \n+\n torch.manual_seed(0)\n size = 98432\n x = torch.rand(size, device='cuda')\n@@ -65,10 +65,10 @@ def asin_kernel(\n \n output_triton = torch.empty_like(x)\n asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024,\n-                 extern_libs={'libdevice': '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'})\n+                  extern_libs={'libdevice': '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'})\n print(output_torch)\n print(output_triton)\n print(\n     f'The maximum difference between torch and triton is '\n     f'{torch.max(torch.abs(output_torch - output_triton))}'\n-)\n\\ No newline at end of file\n+)"}]