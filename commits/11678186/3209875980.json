[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 29, "deletions": 17, "changes": 46, "file_content_changes": "@@ -6,6 +6,7 @@\n #include \"mlir/Conversion/LLVMCommon/LoweringOptions.h\"\n #include \"mlir/Conversion/LLVMCommon/Pattern.h\"\n #include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n+#include \"mlir/Conversion/StandardToLLVM/ConvertStandardToLLVM.h\"\n #include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n #include \"mlir/Dialect/GPU/GPUDialect.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n@@ -1287,24 +1288,33 @@ struct AddPtrOpConversion\n   matchAndRewrite(triton::AddPtrOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n-    auto resultTy = op.getType().dyn_cast<RankedTensorType>();\n-    auto resultLayout = resultTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n-    assert(resultLayout && \"Unexpected resultLayout in AddPtrOpConversion\");\n-    auto resultShape = resultTy.getShape();\n-    unsigned elems = resultLayout.getElemsPerThread(resultShape);\n-    Type elemTy =\n-        this->getTypeConverter()->convertType(resultTy.getElementType());\n-    SmallVector<Type> types(elems, elemTy);\n-    Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n-    auto ptrs = getElementsFromStruct(loc, adaptor.ptr(), elems, rewriter);\n-    auto offsets =\n-        getElementsFromStruct(loc, adaptor.offset(), elems, rewriter);\n-    SmallVector<Value> resultVals(elems);\n-    for (unsigned i = 0; i < elems; ++i) {\n-      resultVals[i] = gep(elemTy, ptrs[i], offsets[i]);\n+    auto resultTy = op.getType();\n+    auto resultTensorTy = resultTy.dyn_cast<RankedTensorType>();\n+    if (resultTensorTy) {\n+      auto resultLayout =\n+          resultTensorTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n+      assert(resultLayout && \"Unexpected resultLayout in AddPtrOpConversion\");\n+      auto resultShape = resultTensorTy.getShape();\n+      unsigned elems = resultLayout.getElemsPerThread(resultShape);\n+      Type elemTy =\n+          getTypeConverter()->convertType(resultTensorTy.getElementType());\n+      SmallVector<Type> types(elems, elemTy);\n+      Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n+      auto ptrs = getElementsFromStruct(loc, adaptor.ptr(), elems, rewriter);\n+      auto offsets =\n+          getElementsFromStruct(loc, adaptor.offset(), elems, rewriter);\n+      SmallVector<Value> resultVals(elems);\n+      for (unsigned i = 0; i < elems; ++i) {\n+        resultVals[i] = gep(elemTy, ptrs[i], offsets[i]);\n+      }\n+      Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n+      rewriter.replaceOp(op, view);\n+    } else {\n+      assert(resultTy.isa<triton::PointerType>());\n+      Type llResultTy = getTypeConverter()->convertType(resultTy);\n+      Value result = gep(llResultTy, adaptor.ptr(), adaptor.offset());\n+      rewriter.replaceOp(op, result);\n     }\n-    Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n-    rewriter.replaceOp(op, view);\n     return success();\n   }\n };\n@@ -3066,6 +3076,7 @@ class ConvertTritonGPUToLLVM\n     mlir::arith::populateArithmeticToLLVMConversionPatterns(typeConverter,\n                                                             patterns);\n     mlir::populateMathToLLVMConversionPatterns(typeConverter, patterns);\n+    mlir::populateStdToLLVMConversionPatterns(typeConverter, patterns);\n \n     mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n \n@@ -3122,6 +3133,7 @@ TritonLLVMConversionTarget::TritonLLVMConversionTarget(\n   // addIllegalDialect<triton::TritonDialect>();\n   // addIllegalDialect<triton::gpu::TritonGPUDialect>();\n   addIllegalDialect<mlir::gpu::GPUDialect>();\n+  addIllegalDialect<mlir::StandardOpsDialect>();\n   addLegalOp<mlir::UnrealizedConversionCastOp>();\n }\n "}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -1,4 +1,5 @@\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n+#include \"mlir/Conversion/Passes.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/ExecutionEngine/ExecutionEngine.h\"\n #include \"mlir/ExecutionEngine/OptUtils.h\"\n@@ -135,6 +136,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n       /*printAfterOnlyOnChange=*/true,\n       /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n+  pm.addPass(mlir::createLowerToCFGPass());\n   pm.addPass(createConvertTritonGPUToLLVMPass());\n   // Conanicalize to eliminate the remaining UnrealizedConversionCastOp\n   pm.addPass(mlir::createCanonicalizerPass());"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "file_content_changes": "@@ -3,6 +3,7 @@\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/Verifier.h\"\n \n+#include \"mlir/Conversion/Passes.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Transforms/Passes.h\"\n@@ -1185,8 +1186,12 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::createTritonGPUVerifier());\n            })\n-      .def(\"add_triton_gpu_to_llvm\", [](mlir::PassManager &self) {\n-        self.addPass(mlir::triton::createConvertTritonGPUToLLVMPass());\n+      .def(\"add_triton_gpu_to_llvm\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::triton::createConvertTritonGPUToLLVMPass());\n+           })\n+      .def(\"add_scf_to_cfg\", [](mlir::PassManager &self) {\n+        self.addPass(mlir::createLowerToCFGPass());\n       });\n }\n "}, {"filename": "python/tests/test_vecadd.py", "status": "added", "additions": 79, "deletions": 0, "changes": 79, "file_content_changes": "@@ -0,0 +1,79 @@\n+import pytest\n+import torch\n+from torch.testing import assert_allclose\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@pytest.mark.parametrize('NUM_WARPS, BLOCK_SIZE', [\n+    [4, 256],\n+    [2, 256],\n+    [1, 256],\n+])\n+def test_vecadd_no_mask(NUM_WARPS, BLOCK_SIZE):\n+\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               z_ptr,\n+               BLOCK_SIZE: tl.constexpr):\n+        pid = tl.program_id(axis=0)\n+        offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+        x_ptrs = x_ptr + offset\n+        y_ptrs = y_ptr + offset\n+        x = tl.load(x_ptrs)\n+        y = tl.load(y_ptrs)\n+        z = x + y\n+        z_ptrs = z_ptr + offset\n+        tl.store(z_ptrs, z)\n+\n+    x = torch.randn((BLOCK_SIZE,), device='cuda', dtype=torch.float32)\n+    y = torch.randn((BLOCK_SIZE,), device='cuda', dtype=torch.float32)\n+    z = torch.empty((BLOCK_SIZE,), device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (x.shape.numel() // BLOCK_SIZE,)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, BLOCK_SIZE=BLOCK_SIZE, num_warps=NUM_WARPS)\n+\n+    golden_z = x + y\n+    assert_allclose(z, golden_z, rtol=1e-7, atol=1e-7)\n+\n+\n+@pytest.mark.parametrize('NUM_WARPS, BLOCK_SIZE, ITER_SIZE', [\n+    [4, 256, 1],\n+    [4, 1024, 256],\n+])\n+def test_vecadd_scf_no_mask(NUM_WARPS, BLOCK_SIZE, ITER_SIZE):\n+\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               z_ptr,\n+               BLOCK_SIZE,\n+               ITER_SIZE: tl.constexpr):\n+        pid = tl.program_id(axis=0)\n+        for i in range(0, BLOCK_SIZE, ITER_SIZE):\n+            offset = pid * BLOCK_SIZE + tl.arange(0, ITER_SIZE)\n+            x_ptrs = x_ptr + offset\n+            y_ptrs = y_ptr + offset\n+            x = tl.load(x_ptrs)\n+            y = tl.load(y_ptrs)\n+            z = x + y\n+            z_ptrs = z_ptr + offset\n+            tl.store(z_ptrs, z)\n+            x_ptr += ITER_SIZE\n+            y_ptr += ITER_SIZE\n+            z_ptr += ITER_SIZE\n+\n+    x = torch.randn((BLOCK_SIZE,), device='cuda', dtype=torch.float32)\n+    y = torch.randn((BLOCK_SIZE,), device='cuda', dtype=torch.float32)\n+    z = torch.empty((BLOCK_SIZE,), device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (x.shape.numel() // (BLOCK_SIZE),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z,\n+                 BLOCK_SIZE=x.shape[0], ITER_SIZE=ITER_SIZE, num_warps=NUM_WARPS)\n+\n+    golden_z = x + y\n+    assert_allclose(z, golden_z, rtol=1e-7, atol=1e-7)\n+\n+# TODO: test_vecadd with mask"}, {"filename": "python/tests/test_vecadd_no_scf.py", "status": "removed", "additions": 0, "deletions": 42, "changes": 42, "file_content_changes": "@@ -1,42 +0,0 @@\n-import torch\n-from torch.testing import assert_allclose\n-\n-import triton\n-import triton.language as tl\n-\n-\n-def vecadd_no_scf_tester(num_warps, block_size):\n-    @triton.jit\n-    def kernel(x_ptr,\n-               y_ptr,\n-               z_ptr,\n-               BLOCK_SIZE_N: tl.constexpr):\n-        pid = tl.program_id(axis=0)\n-        offset = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-        x_ptrs = x_ptr + offset\n-        y_ptrs = y_ptr + offset\n-        x = tl.load(x_ptrs)\n-        y = tl.load(y_ptrs)\n-        z = x + y\n-        z_ptrs = z_ptr + offset\n-        tl.store(z_ptrs, z)\n-\n-    x = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n-    y = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n-    z = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n-\n-    grid = lambda EA: (x.shape.numel() // block_size,)\n-    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, BLOCK_SIZE_N=block_size, num_warps=num_warps)\n-\n-    golden_z = x + y\n-    assert_allclose(z, golden_z, rtol=1e-7, atol=1e-7)\n-\n-\n-def test_vecadd_no_scf():\n-    vecadd_no_scf_tester(num_warps=4, block_size=256)\n-    vecadd_no_scf_tester(num_warps=2, block_size=256)\n-    vecadd_no_scf_tester(num_warps=1, block_size=256)\n-\n-\n-if __name__ == '__main__':\n-    test_vecadd_no_scf()"}]