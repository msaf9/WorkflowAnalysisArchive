[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 28, "deletions": 20, "changes": 48, "file_content_changes": "@@ -1183,7 +1183,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n # TODO: [Qingyi] Fix argmin / argmax\n reduce_configs1 = [\n     (op, dtype, (1, 1024), axis) for dtype in dtypes_with_bfloat16\n-    for op in ['min', 'max', 'sum']\n+    for op in ['min', 'max', 'sum', 'argmin', 'argmax']\n     for axis in [1]\n ]\n \n@@ -1199,7 +1199,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n reduce_configs2 = [\n     (op, 'float32', shape, axis)\n-    for op in ['min', 'max', 'sum']\n+    for op in ['min', 'max', 'sum', 'argmin', 'argmax']\n     for shape in reduce2d_shapes\n     for axis in [0, 1]\n ]\n@@ -1426,20 +1426,33 @@ def kernel(X, stride_xm, stride_xn,\n \n \n @pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype\",\n+                         [(*shape, 4, False, False, epilogue, allow_tf32, in_dtype, out_dtype)\n+                          for shape in [(64, 64, 64), (16, 16, 16)]\n+                          for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n+                          for allow_tf32 in [True, False]\n+                          for in_dtype, out_dtype in [('float16', 'float16'),\n+                                                      ('float16', 'float32'),\n+                                                      ('float32', 'float32')]\n+                          if not (allow_tf32 and (in_dtype in ['float16']))] +\n+\n                          [(*shape_nw, col_a, col_b, 'none', allow_tf32, in_dtype, out_dtype)\n-                          for shape_nw in [\n-                             [16, 16, 32, 1],\n-                             [64, 64, 64, 4],\n-                             [128, 128, 128, 4],\n-                             [128, 128, 64, 2],\n-                             [128, 128, 128, 2],\n-                             [64, 64, 64, 1],\n-                             [128, 128, 128, 8],\n-                         ]\n-                             for allow_tf32 in [True]\n-                             for col_a in [True, False]\n-                             for col_b in [True, False]\n-                             for in_dtype, out_dtype in [('int8', 'int8'), ('float16', 'float16'), ('float32', 'float32')]])\n+                          for shape_nw in [[128, 256, 32, 8],\n+                                           [128, 16, 32, 4],\n+                                           [32, 128, 64, 4],\n+                                           [128, 128, 64, 4],\n+                                           [64, 128, 128, 4],\n+                                           [32, 128, 64, 2],\n+                                           [64, 64, 32, 4],\n+                                           [32, 32, 128, 16],\n+                                           [128, 128, 64, 2],\n+                                           [64, 128, 128, 2]]\n+                          for allow_tf32 in [True]\n+                          for col_a in [True, False]\n+                          for col_b in [True, False]\n+                          for in_dtype, out_dtype in [('int8', 'int8'),\n+                                                      ('float16', 'float16'),\n+                                                      ('float16', 'float32'),\n+                                                      ('float32', 'float32')]])\n def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype, device='cuda'):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n@@ -1517,10 +1530,6 @@ def kernel(X, stride_xm, stride_xk,\n         x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n         y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n         w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n-    # x = x*0 + 3\n-    # y = y*0 + 1\n-    # for i in range(K):\n-    #     y[i, :] = i\n     x_tri = to_triton(x, device=device)\n     y_tri = to_triton(y, device=device)\n     w_tri = to_triton(w, device=device)\n@@ -1586,7 +1595,6 @@ def kernel(X, stride_xm, stride_xk,\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n     else:\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n-        # np.testing.assert_allclose(z_ref[:16, 0], to_numpy(z_tri[:16, 0]), rtol=0.01)\n     # make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']\n     if (K > 16 or N > 16 or M > 16) and (M * N // (num_warps * 32) >= 4):"}, {"filename": "python/test/unit/operators/test_inductor.py", "status": "modified", "additions": 100, "deletions": 0, "changes": 100, "file_content_changes": "@@ -53,3 +53,103 @@ def triton_(in_out_ptr0, in_out_ptr1, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel\n     arg9_1 = torch.rand(64, device=\"cuda\")\n     triton_[(512,)](buf14, buf16, arg114_1, arg115_1, arg8_1, arg9_1, 512, 4096, 1, 2048)\n     torch.testing.assert_allclose(buf16.mean().item(), buf14.mean().item(), atol=1e-7, rtol=0)\n+\n+\n+def test_avg_pool_bw():\n+\n+    @triton.jit\n+    def triton_(in_ptr0, out_ptr0, XBLOCK: tl.constexpr):\n+        xoffset = tl.program_id(0) * XBLOCK\n+        xindex = xoffset + tl.arange(0, XBLOCK)[:]\n+        x1 = (xindex // 8) % 8\n+        x0 = xindex % 8\n+        x2 = (xindex // 64)\n+        x5 = xindex\n+        tmp0 = (-1) + x1\n+        tmp1 = (-1) + x0\n+        tmp2 = 2 + x1\n+        tmp3 = 2 + x0\n+        tmp4 = 0\n+        tmp5 = tl.where(tmp0 != tmp0, tmp0, tl.where(tmp0 > tmp4, tmp0, tmp4))\n+        tmp6 = tl.where(tmp1 != tmp1, tmp1, tl.where(tmp1 > tmp4, tmp1, tmp4))\n+        tmp7 = 8\n+        tmp8 = tl.where(tmp2 != tmp2, tmp2, tl.where(tmp2 < tmp7, tmp2, tmp7))\n+        tmp9 = tl.where(tmp3 != tmp3, tmp3, tl.where(tmp3 < tmp7, tmp3, tmp7))\n+        tmp10 = tmp5 + tmp4\n+        tmp11 = tmp6 + tmp4\n+        tmp12 = 1\n+        tmp13 = tmp8 - tmp12\n+        tmp14 = tl.where(tmp10 != tmp10, tmp10, tl.where(tmp10 < tmp13, tmp10, tmp13))\n+        tmp15 = tmp9 - tmp12\n+        tmp16 = tl.where(tmp11 != tmp11, tmp11, tl.where(tmp11 < tmp15, tmp11, tmp15))\n+        tmp17 = tl.load(in_ptr0 + (tmp16 + (8 * tmp14) + (64 * x2)), None).to(tl.float32)\n+        tmp18 = tmp17 / 9\n+        tmp19 = tmp10 < tmp8\n+        tmp20 = tmp11 < tmp9\n+        tmp21 = tmp19 & tmp20\n+        tmp22 = 0.0\n+        tmp23 = tl.where(tmp21, tmp18, tmp22)\n+        tmp24 = tmp6 + tmp12\n+        tmp25 = tl.where(tmp24 != tmp24, tmp24, tl.where(tmp24 < tmp15, tmp24, tmp15))\n+        tmp26 = tl.load(in_ptr0 + (tmp25 + (8 * tmp14) + (64 * x2)), None).to(tl.float32)\n+        tmp27 = tmp26 / 9\n+        tmp28 = tmp24 < tmp9\n+        tmp29 = tmp19 & tmp28\n+        tmp30 = tmp23 + tmp27\n+        tmp31 = tl.where(tmp29, tmp30, tmp23)\n+        tmp32 = 2\n+        tmp33 = tmp6 + tmp32\n+        tmp34 = tl.where(tmp33 != tmp33, tmp33, tl.where(tmp33 < tmp15, tmp33, tmp15))\n+        tmp35 = tl.load(in_ptr0 + (tmp34 + (8 * tmp14) + (64 * x2)), None).to(tl.float32)\n+        tmp36 = tmp35 / 9\n+        tmp37 = tmp33 < tmp9\n+        tmp38 = tmp19 & tmp37\n+        tmp39 = tmp31 + tmp36\n+        tmp40 = tl.where(tmp38, tmp39, tmp31)\n+        tmp41 = tmp5 + tmp12\n+        tmp42 = tl.where(tmp41 != tmp41, tmp41, tl.where(tmp41 < tmp13, tmp41, tmp13))\n+        tmp43 = tl.load(in_ptr0 + (tmp16 + (8 * tmp42) + (64 * x2)), None).to(tl.float32)\n+        tmp44 = tmp43 / 9\n+        tmp45 = tmp41 < tmp8\n+        tmp46 = tmp45 & tmp20\n+        tmp47 = tmp40 + tmp44\n+        tmp48 = tl.where(tmp46, tmp47, tmp40)\n+        tmp49 = tl.load(in_ptr0 + (tmp25 + (8 * tmp42) + (64 * x2)), None).to(tl.float32)\n+        tmp50 = tmp49 / 9\n+        tmp51 = tmp45 & tmp28\n+        tmp52 = tmp48 + tmp50\n+        tmp53 = tl.where(tmp51, tmp52, tmp48)\n+        tmp54 = tl.load(in_ptr0 + (tmp34 + (8 * tmp42) + (64 * x2)), None).to(tl.float32)\n+        tmp55 = tmp54 / 9\n+        tmp56 = tmp45 & tmp37\n+        tmp57 = tmp53 + tmp55\n+        tmp58 = tl.where(tmp56, tmp57, tmp53)\n+        tmp59 = tmp5 + tmp32\n+        tmp60 = tl.where(tmp59 != tmp59, tmp59, tl.where(tmp59 < tmp13, tmp59, tmp13))\n+        tmp61 = tl.load(in_ptr0 + (tmp16 + (8 * tmp60) + (64 * x2)), None).to(tl.float32)\n+        tmp62 = tmp61 / 9\n+        tmp63 = tmp59 < tmp8\n+        tmp64 = tmp63 & tmp20\n+        tmp65 = tmp58 + tmp62\n+        tmp66 = tl.where(tmp64, tmp65, tmp58)\n+        tmp67 = tl.load(in_ptr0 + (tmp25 + (8 * tmp60) + (64 * x2)), None).to(tl.float32)\n+        tmp68 = tmp67 / 9\n+        tmp69 = tmp63 & tmp28\n+        tmp70 = tmp66 + tmp68\n+        tmp71 = tl.where(tmp69, tmp70, tmp66)\n+        tmp72 = tl.load(in_ptr0 + (tmp34 + (8 * tmp60) + (64 * x2)), None).to(tl.float32)\n+        tmp73 = tmp72 / 9\n+        tmp74 = tmp63 & tmp37\n+        tmp75 = tmp71 + tmp73\n+        tmp76 = tl.where(tmp74, tmp75, tmp71)\n+        tl.store(out_ptr0 + (x5 + tl.zeros([XBLOCK], tl.int32)), tmp76, None)\n+\n+    inp = torch.ones(8, 2048, 8, 8, device=\"cuda\", dtype=torch.half)\n+    out = torch.ones_like(inp) * 3\n+    numel = inp.numel()\n+    triton_[(numel // 1024,)](inp, out, 1024)\n+    out_ref = torch.ones_like(inp)\n+    out_ref[:, :, 1:7, 0::7] = 2 / 3\n+    out_ref[:, :, 0::7, 1:7] = 2 / 3\n+    out_ref[:, :, 0::7, 0::7] = 4 / 9\n+    torch.testing.assert_allclose(out, out_ref)"}, {"filename": "python/test/unit/runtime/test_autotuner.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -13,10 +13,10 @@ def test_kwargs():\n \n     @triton.autotune(configs=configs, key=['N'])\n     @triton.jit\n-    def _kernel(dst: torch.Tensor, N: int, BLOCK_SIZE: tl.constexpr):\n+    def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n         offsets = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n         x = tl.load(src + offsets, mask=offsets < N)\n         tl.store(dst + offsets, x, mask=offsets < N)\n     grid = lambda META: (triton.cdiv(N, META['BLOCK_SIZE']),)\n-    _kernel[grid](dst, src, N, 32)\n-    _kernel[grid](dst, src, N, 32)\n+    _kernel[grid](dst, src, N)\n+    _kernel[grid](dst=dst, src=src, N=N)"}]