[{"filename": "docs/conf.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -101,7 +101,7 @@ def documenter(app, obj, parent):\n     'gallery_dirs': 'getting-started/tutorials',\n     'filename_pattern': '',\n     # XXX: Temporarily disable fused attention tutorial on V100\n-    'ignore_pattern': r'__init__\\.py',\n+    'ignore_pattern': r'(__init__\\.py|09.*\\.py|10.*\\.py)',\n     'within_subsection_order': FileNameSortKey,\n     'reference_url': {\n         'sphinx_gallery': None,"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -2172,12 +2172,12 @@ def kernel(X, stride_xm, stride_xk,\n                Y, stride_yk, stride_yn,\n                W, stride_wn, stride_wl,\n                Z, stride_zm, stride_zn,\n-               out_dtype: tl.constexpr,\n                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n                ALLOW_TF32: tl.constexpr,\n                DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n-               COL_A: tl.constexpr, COL_B: tl.constexpr):\n+               COL_A: tl.constexpr, COL_B: tl.constexpr,\n+               out_dtype: tl.constexpr = tl.float32):\n         off_m = tl.arange(0, BLOCK_M)\n         off_n = tl.arange(0, BLOCK_N)\n         off_l = tl.arange(0, BLOCK_N)\n@@ -2251,7 +2251,6 @@ def kernel(X, stride_xm, stride_xk,\n                          y_tri, y_tri.stride(0), y_tri.stride(1),\n                          w_tri, w_tri.stride(0), w_tri.stride(1),\n                          z_tri, z_tri.stride(0), z_tri.stride(1),\n-                         out_dtype,\n                          COL_A=col_a, COL_B=col_b,\n                          BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n                          ADD_MATRIX=epilogue == 'add-matrix',\n@@ -2260,7 +2259,8 @@ def kernel(X, stride_xm, stride_xk,\n                          DO_SOFTMAX=epilogue == 'softmax',\n                          CHAIN_DOT=epilogue == 'chain-dot',\n                          ALLOW_TF32=allow_tf32,\n-                         num_warps=num_warps, num_ctas=num_ctas)\n+                         num_warps=num_warps, num_ctas=num_ctas,\n+                         out_dtype=out_dtype)\n     if epilogue == 'softmax' and (in_dtype != 'float32' or allow_tf32):\n         ptx = pgm.asm[\"ptx\"]\n         start = ptx.find(\"shfl.sync\")"}, {"filename": "python/test/unit/language/test_line_info.py", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -116,5 +116,3 @@ def test_line_info(func: str):\n         assert (check_file_lines(file_lines, \"standard.py\", 33))\n         assert (check_file_lines(file_lines, \"standard.py\", 34))\n         assert (check_file_lines(file_lines, \"standard.py\", 36))\n-        # core.py is changed frequently, so we only check if it exists\n-        assert (check_file_lines(file_lines, \"core.py\", -1))"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -4,11 +4,21 @@\n from . import math\n from . import extra\n from .standard import (\n+    argmax,\n+    argmin,\n     cdiv,\n+    cumprod,\n+    cumsum,\n+    max,\n+    maximum,\n+    min,\n+    minimum,\n     sigmoid,\n     softmax,\n+    sum,\n     ravel,\n     swizzle2d,\n+    xor_sum,\n     zeros,\n     zeros_like,\n )\n@@ -17,8 +27,6 @@\n     abs,\n     advance,\n     arange,\n-    argmin,\n-    argmax,\n     associative_scan,\n     atomic_add,\n     atomic_and,\n@@ -35,8 +43,6 @@\n     cat,\n     constexpr,\n     cos,\n-    cumprod,\n-    cumsum,\n     debug_barrier,\n     device_assert,\n     device_print,\n@@ -63,12 +69,8 @@\n     load,\n     log,\n     make_block_ptr,\n-    max,\n     max_constancy,\n     max_contiguous,\n-    maximum,\n-    min,\n-    minimum,\n     multiple_of,\n     num_programs,\n     pi32_t,\n@@ -81,7 +83,6 @@\n     static_assert,\n     static_print,\n     store,\n-    sum,\n     static_range,\n     tensor,\n     trans,\n@@ -94,7 +95,6 @@\n     view,\n     void,\n     where,\n-    xor_sum,\n )\n from .random import (\n     pair_uniform_to_normal,"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 5, "deletions": 191, "changes": 196, "file_content_changes": "@@ -6,8 +6,7 @@\n from typing import Callable, List, Sequence, TypeVar\n \n from .._C.libtriton.triton import ir\n-from ..runtime.jit import jit\n-from . import math, semantic\n+from . import semantic\n \n T = TypeVar('T')\n \n@@ -205,6 +204,10 @@ def is_int(self):\n     def is_bool(self):\n         return self.is_int1()\n \n+    @staticmethod\n+    def is_dtype(type_str):\n+        return type_str in dtype.SINT_TYPES + dtype.UINT_TYPES + dtype.FP_TYPES + dtype.OTHER_TYPES\n+\n     @staticmethod\n     def is_void():\n         raise RuntimeError(\"Not implemented\")\n@@ -1380,170 +1383,6 @@ def _reduce_with_indices(input, axis, combine_fn, _builder=None, _generator=None\n     return rvalue, rindices\n \n \n-@jit\n-def minimum(x, y):\n-    \"\"\"\n-    Computes the element-wise minimum of :code:`x` and :code:`y`.\n-\n-    :param input: the first input tensor\n-    :type input: Block\n-    :param other: the second input tensor\n-    :type other: Block\n-    \"\"\"\n-    return math.min(x, y)\n-\n-\n-@jit\n-def maximum(x, y):\n-    \"\"\"\n-    Computes the element-wise maximum of :code:`x` and :code:`y`.\n-\n-    :param input: the first input tensor\n-    :type input: Block\n-    :param other: the second input tensor\n-    :type other: Block\n-    \"\"\"\n-    return math.max(x, y)\n-\n-# max and argmax\n-\n-\n-@jit\n-def _argmax_combine(value1, index1, value2, index2, tie_break_left):\n-    if tie_break_left:\n-        tie = value1 == value2 and index1 < index2\n-    else:\n-        tie = False\n-    gt = value1 > value2 or tie\n-    v_ret = where(gt, value1, value2)\n-    i_ret = where(gt, index1, index2)\n-    return v_ret, i_ret\n-\n-\n-@jit\n-def _argmax_combine_tie_break_left(value1, index1, value2, index2):\n-    return _argmax_combine(value1, index1, value2, index2, True)\n-\n-\n-@jit\n-def _argmax_combine_tie_break_fast(value1, index1, value2, index2):\n-    return _argmax_combine(value1, index1, value2, index2, False)\n-\n-\n-@jit\n-@_add_reduction_docstr(\"maximum\",\n-                       return_indices_arg=\"return_indices\",\n-                       tie_break_arg=\"return_indices_tie_break_left\")\n-def max(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n-    input = _promote_reduction_input(input)\n-    if return_indices:\n-        if return_indices_tie_break_left:\n-            return _reduce_with_indices(input, axis, _argmax_combine_tie_break_left)\n-        else:\n-            return _reduce_with_indices(input, axis, _argmax_combine_tie_break_fast)\n-    else:\n-        if constexpr(input.dtype.primitive_bitwidth) < 32:\n-            if constexpr(input.dtype.is_floating()):\n-                input = input.to(float32)\n-            else:\n-                assert input.dtype.is_integer_type()\n-                input = input.to(int32)\n-        return reduce(input, axis, maximum)\n-\n-\n-@jit\n-@_add_reduction_docstr(\"maximum index\", tie_break_arg=\"tie_break_left\")\n-def argmax(input, axis, tie_break_left=True):\n-    (_, ret) = max(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n-    return ret\n-\n-# min and argmin\n-\n-\n-@jit\n-def _argmin_combine(value1, index1, value2, index2, tie_break_left):\n-    if tie_break_left:\n-        tie = value1 == value2 and index1 < index2\n-    else:\n-        tie = False\n-    lt = value1 < value2 or tie\n-    value_ret = where(lt, value1, value2)\n-    index_ret = where(lt, index1, index2)\n-    return value_ret, index_ret\n-\n-\n-@jit\n-def _argmin_combine_tie_break_left(value1, index1, value2, index2):\n-    return _argmin_combine(value1, index1, value2, index2, True)\n-\n-\n-@jit\n-def _argmin_combine_tie_break_fast(value1, index1, value2, index2):\n-    return _argmin_combine(value1, index1, value2, index2, False)\n-\n-\n-@jit\n-@_add_reduction_docstr(\"minimum\",\n-                       return_indices_arg=\"return_indices\",\n-                       tie_break_arg=\"return_indices_tie_break_left\")\n-def min(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n-    input = _promote_reduction_input(input)\n-    if return_indices:\n-        if return_indices_tie_break_left:\n-            return _reduce_with_indices(input, axis, _argmin_combine_tie_break_left)\n-        else:\n-            return _reduce_with_indices(input, axis, _argmin_combine_tie_break_fast)\n-    else:\n-        if constexpr(input.dtype.primitive_bitwidth) < 32:\n-            if constexpr(input.dtype.is_floating()):\n-                input = input.to(float32)\n-            else:\n-                assert input.dtype.is_integer_type()\n-                input = input.to(int32)\n-        return reduce(input, axis, minimum)\n-\n-\n-@jit\n-@_add_reduction_docstr(\"minimum index\",\n-                       tie_break_arg=\"tie_break_left\")\n-def argmin(input, axis, tie_break_left=True):\n-    _, ret = min(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n-    return ret\n-\n-\n-@jit\n-def _sum_combine(a, b):\n-    return a + b\n-\n-# sum\n-\n-\n-@jit\n-@_add_reduction_docstr(\"sum\")\n-def sum(input, axis=None):\n-    input = _promote_reduction_input(input)\n-    return reduce(input, axis, _sum_combine)\n-\n-\n-@jit\n-def _xor_combine(a, b):\n-    return a ^ b\n-\n-\n-# xor sum\n-\n-@builtin\n-@_add_reduction_docstr(\"xor sum\")\n-def xor_sum(input, axis=None, _builder=None, _generator=None):\n-    scalar_ty = input.type.scalar\n-    if not scalar_ty.is_int():\n-        raise ValueError(\"xor_sum only supported for integers\")\n-\n-    input = _promote_reduction_input(input, _builder=_builder)\n-    return reduce(input, axis, _xor_combine,\n-                  _builder=_builder, _generator=_generator)\n-\n-\n # -----------------------\n # Scans\n # -----------------------\n@@ -1594,31 +1433,6 @@ def make_combine_region(scan_op):\n     axis = _constexpr_to_value(axis)\n     return semantic.associative_scan(input, axis, make_combine_region, _builder)\n \n-# cumsum\n-\n-\n-@jit\n-@_add_scan_docstr(\"cumsum\")\n-def cumsum(input, axis=0):\n-    # todo rename this to a generic function name\n-    input = _promote_reduction_input(input)\n-    return associative_scan(input, axis, _sum_combine)\n-\n-# cumprod\n-\n-\n-@jit\n-def _prod_combine(a, b):\n-    return a * b\n-\n-\n-@jit\n-@_add_scan_docstr(\"cumprod\")\n-def cumprod(input, axis=0):\n-    # todo rename this to a generic function name\n-    input = _promote_reduction_input(input)\n-    return associative_scan(input, axis, _prod_combine)\n-\n # -----------------------\n # Compiler Hint Ops\n # -----------------------"}, {"filename": "python/triton/language/random.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -1,5 +1,6 @@\n from ..runtime.jit import jit\n from . import core as tl\n+from . import standard\n \n PHILOX_KEY_A: tl.constexpr = 0x9E3779B9\n PHILOX_KEY_B: tl.constexpr = 0xBB67AE85\n@@ -141,7 +142,7 @@ def rand4x(seed, offsets, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n @jit\n def pair_uniform_to_normal(u1, u2):\n     \"\"\"Box-Muller transform\"\"\"\n-    u1 = tl.maximum(1.0e-7, u1)\n+    u1 = standard.maximum(1.0e-7, u1)\n     th = 6.283185307179586 * u2\n     r = tl.sqrt(-2.0 * tl.log(u1))\n     return r * tl.cos(th), r * tl.sin(th)"}, {"filename": "python/triton/language/standard.py", "status": "modified", "additions": 193, "deletions": 4, "changes": 197, "file_content_changes": "@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n from ..runtime.jit import jit\n-from . import core\n+from . import core, math\n \n # -----------------------\n # Standard library\n@@ -30,9 +30,9 @@ def sigmoid(x):\n @jit\n @core._add_math_1arg_docstr(\"softmax\")\n def softmax(x, ieee_rounding=False):\n-    z = x - core.max(x, 0)\n+    z = x - max(x, 0)\n     num = core.exp(z)\n-    den = core.sum(num, 0)\n+    den = sum(num, 0)\n     return core.fdiv(num, den, ieee_rounding)\n \n \n@@ -73,7 +73,7 @@ def swizzle2d(i, j, size_i, size_j, size_g):\n     # row-index of the first element of this group\n     off_i = group_id * size_g\n     # last group may have fewer rows\n-    size_g = core.minimum(size_i - off_i, size_g)\n+    size_g = minimum(size_i - off_i, size_g)\n     # new row and column indices\n     new_i = off_i + (ij % size_g)\n     new_j = (ij % size_gj) // size_g\n@@ -96,3 +96,192 @@ def zeros(shape, dtype):\n @jit\n def zeros_like(input):\n     return zeros(input.shape, input.dtype)\n+\n+\n+@jit\n+def minimum(x, y):\n+    \"\"\"\n+    Computes the element-wise minimum of :code:`x` and :code:`y`.\n+\n+    :param input: the first input tensor\n+    :type input: Block\n+    :param other: the second input tensor\n+    :type other: Block\n+    \"\"\"\n+    return math.min(x, y)\n+\n+\n+@jit\n+def maximum(x, y):\n+    \"\"\"\n+    Computes the element-wise maximum of :code:`x` and :code:`y`.\n+\n+    :param input: the first input tensor\n+    :type input: Block\n+    :param other: the second input tensor\n+    :type other: Block\n+    \"\"\"\n+    return math.max(x, y)\n+\n+# max and argmax\n+\n+\n+@jit\n+def _argmax_combine(value1, index1, value2, index2, tie_break_left):\n+    if tie_break_left:\n+        tie = value1 == value2 and index1 < index2\n+    else:\n+        tie = False\n+    gt = value1 > value2 or tie\n+    v_ret = core.where(gt, value1, value2)\n+    i_ret = core.where(gt, index1, index2)\n+    return v_ret, i_ret\n+\n+\n+@jit\n+def _argmax_combine_tie_break_left(value1, index1, value2, index2):\n+    return _argmax_combine(value1, index1, value2, index2, True)\n+\n+\n+@jit\n+def _argmax_combine_tie_break_fast(value1, index1, value2, index2):\n+    return _argmax_combine(value1, index1, value2, index2, False)\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"maximum\",\n+                            return_indices_arg=\"return_indices\",\n+                            tie_break_arg=\"return_indices_tie_break_left\")\n+def max(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n+    input = core._promote_reduction_input(input)\n+    if return_indices:\n+        if return_indices_tie_break_left:\n+            return core._reduce_with_indices(input, axis, _argmax_combine_tie_break_left)\n+        else:\n+            return core._reduce_with_indices(input, axis, _argmax_combine_tie_break_fast)\n+    else:\n+        if core.constexpr(input.dtype.primitive_bitwidth) < 32:\n+            if core.constexpr(input.dtype.is_floating()):\n+                input = input.to(core.float32)\n+            else:\n+                assert input.dtype.is_integer_type()\n+                input = input.to(core.int32)\n+        return core.reduce(input, axis, maximum)\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"maximum index\", tie_break_arg=\"tie_break_left\")\n+def argmax(input, axis, tie_break_left=True):\n+    (_, ret) = max(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n+    return ret\n+\n+# min and argmin\n+\n+\n+@jit\n+def _argmin_combine(value1, index1, value2, index2, tie_break_left):\n+    if tie_break_left:\n+        tie = value1 == value2 and index1 < index2\n+    else:\n+        tie = False\n+    lt = value1 < value2 or tie\n+    value_ret = core.where(lt, value1, value2)\n+    index_ret = core.where(lt, index1, index2)\n+    return value_ret, index_ret\n+\n+\n+@jit\n+def _argmin_combine_tie_break_left(value1, index1, value2, index2):\n+    return _argmin_combine(value1, index1, value2, index2, True)\n+\n+\n+@jit\n+def _argmin_combine_tie_break_fast(value1, index1, value2, index2):\n+    return _argmin_combine(value1, index1, value2, index2, False)\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"minimum\",\n+                            return_indices_arg=\"return_indices\",\n+                            tie_break_arg=\"return_indices_tie_break_left\")\n+def min(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n+    input = core._promote_reduction_input(input)\n+    if return_indices:\n+        if return_indices_tie_break_left:\n+            return core._reduce_with_indices(input, axis, _argmin_combine_tie_break_left)\n+        else:\n+            return core._reduce_with_indices(input, axis, _argmin_combine_tie_break_fast)\n+    else:\n+        if core.constexpr(input.dtype.primitive_bitwidth) < 32:\n+            if core.constexpr(input.dtype.is_floating()):\n+                input = input.to(core.float32)\n+            else:\n+                assert input.dtype.is_integer_type()\n+                input = input.to(core.int32)\n+        return core.reduce(input, axis, minimum)\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"minimum index\",\n+                            tie_break_arg=\"tie_break_left\")\n+def argmin(input, axis, tie_break_left=True):\n+    _, ret = min(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n+    return ret\n+\n+\n+@jit\n+def _sum_combine(a, b):\n+    return a + b\n+\n+# sum\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"sum\")\n+def sum(input, axis=None):\n+    input = core._promote_reduction_input(input)\n+    return core.reduce(input, axis, _sum_combine)\n+\n+\n+@jit\n+def _xor_combine(a, b):\n+    return a ^ b\n+\n+# xor sum\n+\n+\n+@core.builtin\n+@core._add_reduction_docstr(\"xor sum\")\n+def xor_sum(input, axis=None, _builder=None, _generator=None):\n+    scalar_ty = input.type.scalar\n+    if not scalar_ty.is_int():\n+        raise ValueError(\"xor_sum only supported for integers\")\n+\n+    input = core._promote_reduction_input(input, _builder=_builder)\n+    return core.reduce(input, axis, _xor_combine,\n+                       _builder=_builder, _generator=_generator)\n+\n+# cumsum\n+\n+\n+@jit\n+@core._add_scan_docstr(\"cumsum\")\n+def cumsum(input, axis=0):\n+    # todo rename this to a generic function name\n+    input = core._promote_reduction_input(input)\n+    return core.associative_scan(input, axis, _sum_combine)\n+\n+# cumprod\n+\n+\n+@jit\n+def _prod_combine(a, b):\n+    return a * b\n+\n+\n+@jit\n+@core._add_scan_docstr(\"cumprod\")\n+def cumprod(input, axis=0):\n+    # todo rename this to a generic function name\n+    input = core._promote_reduction_input(input)\n+    return core.associative_scan(input, axis, _prod_combine)"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -13,6 +13,7 @@\n \n from .._C.libtriton.triton import TMAInfos\n from ..common.backend import get_backend, path_to_ptxas\n+from ..language.core import dtype\n \n TRITON_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n TRITON_VERSION = \"2.1.0\"\n@@ -358,10 +359,11 @@ def _make_launcher(self):\n \n         spec_keys = ', '.join(specializations)\n         grid_args = ','.join([f'\"{arg}\": {arg}' for arg in self.arg_names])\n-        args_signature = ', '.join(name if dflt == inspect._empty else f'{name} = {dflt}' for name, dflt in zip(self.arg_names, self.arg_defaults))\n+        args_signature = ', '.join(name if dflt == inspect._empty else f'{name} = triton.language.dtype(\\'{dflt}\\')' if dtype.is_dtype(f'{dflt}') else f'{name} = {dflt}' for name, dflt in zip(self.arg_names, self.arg_defaults))\n         args_signature = args_signature + ', ' if len(args_signature) > 0 else ''\n \n         src = f\"\"\"\n+import triton\n def {self.fn.__name__}({args_signature}grid=None, num_warps=None, num_ctas=1, num_stages=None, enable_warp_specialization=False, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\n     from ..compiler import compile, CompiledKernel, get_arch_default_num_warps, get_arch_default_num_stages\n     sig_key = {f'{sig_keys},' if len(sig_keys) > 0 else ()}\n@@ -466,9 +468,6 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         self.arg_names = [v.name for v in signature.parameters.values()]\n         self.arg_defaults = [v.default for v in signature.parameters.values()]\n         self.has_defaults = any(v != inspect._empty for v in self.arg_defaults)\n-        # specialization hints\n-        self.do_not_specialize = [] if do_not_specialize is None else do_not_specialize\n-        self.do_not_specialize = {self.arg_names.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize}\n         # function source code (without decorators)\n         self.src = textwrap.dedent(inspect.getsource(fn))\n         self.src = self.src[self.src.find(\"def\"):]\n@@ -485,6 +484,10 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         self.__annotations__ = {name: _normalize_ty(ty) for name, ty in fn.__annotations__.items()}\n         # index of constexprs\n         self.constexprs = [self.arg_names.index(name) for name, ty in self.__annotations__.items() if 'constexpr' in ty]\n+        # specialization hints\n+        regular_args = [arg for i, arg in enumerate(self.arg_names) if i not in self.constexprs]\n+        self.do_not_specialize = [] if do_not_specialize is None else do_not_specialize\n+        self.do_not_specialize = {regular_args.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize}\n         # tma info\n         self.tensormaps_info = TMAInfos()\n         # launcher"}]