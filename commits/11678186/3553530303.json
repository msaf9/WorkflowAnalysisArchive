[{"filename": "include/triton/Analysis/Allocation.h", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "file_content_changes": "@@ -20,7 +20,14 @@ SmallVector<unsigned>\n getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n                              unsigned &outVec);\n \n-SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op);\n+bool enableFastReduce(triton::ReduceOp op);\n+\n+SmallVector<unsigned> getScratchConfigForReduceBasic(triton::ReduceOp op);\n+\n+SmallVector<SmallVector<unsigned>>\n+getScratchConfigsForReduceFast(triton::ReduceOp op);\n+\n+unsigned getScratchSizeInBytesForReduce(triton::ReduceOp op);\n \n } // namespace triton\n "}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -16,6 +16,14 @@ bool maybeAliasOp(Operation *op);\n \n std::string getValueOperandName(Value value, AsmState &state);\n \n+template <typename T_OUT, typename T_IN>\n+inline SmallVector<T_OUT> convertType(ArrayRef<T_IN> in) {\n+  SmallVector<T_OUT> out;\n+  for (const T_IN &i : in)\n+    out.push_back(T_OUT(i));\n+  return out;\n+}\n+\n template <typename Int> Int product(llvm::ArrayRef<Int> arr) {\n   return std::accumulate(arr.begin(), arr.end(), 1, std::multiplies{});\n }"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -351,6 +351,9 @@ def TT_ReduceOp : TT_Op<\"reduce\", [NoSideEffect,\n \n     let assemblyFormat = \"$operand attr-dict `:` type($operand) `->` type($result)\";\n \n+    let extraClassDeclaration = [{\n+        static bool withIndex(mlir::triton::RedOp redOp);\n+    }];\n }\n \n //"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 58, "deletions": 28, "changes": 86, "file_content_changes": "@@ -88,30 +88,74 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   return paddedRepShape;\n }\n \n-SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op) {\n+/// ====================== reduce op config begin ==========================\n+\n+bool enableFastReduce(triton::ReduceOp op) {\n+  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+  auto srcLayout = srcTy.getEncoding();\n+  return op.axis() == getOrder(srcLayout)[0];\n+}\n+\n+SmallVector<unsigned> getScratchConfigForReduceBasic(triton::ReduceOp op) {\n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n   auto srcLayout = srcTy.getEncoding();\n   auto srcShape = srcTy.getShape();\n   auto axis = op.axis();\n \n-  bool fastReduce = axis == getOrder(srcLayout)[0];\n+  auto smemShape = convertType<unsigned>(srcShape);\n+  smemShape[axis] = gpu::getThreadsPerWarp(srcLayout)[axis] *\n+                    gpu::getWarpsPerCTA(srcLayout)[axis];\n \n-  SmallVector<unsigned> smemShape;\n-  for (auto d : srcShape)\n-    smemShape.push_back(d);\n+  return smemShape;\n+}\n+\n+SmallVector<SmallVector<unsigned>>\n+getScratchConfigsForReduceFast(triton::ReduceOp op) {\n+  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+  auto srcLayout = srcTy.getEncoding();\n+  auto srcShape = srcTy.getShape();\n+  auto axis = op.axis();\n+\n+  SmallVector<SmallVector<unsigned>> smemShapes(3);\n+\n+  /// shared memory block0\n+  smemShapes[0] = convertType<unsigned>(srcShape);\n+  smemShapes[0][axis] = gpu::getWarpsPerCTA(srcLayout)[axis];\n \n-  if (fastReduce) {\n-    unsigned sizeInterWarps = gpu::getWarpsPerCTA(srcLayout)[axis];\n-    smemShape[axis] = sizeInterWarps;\n+  /// shared memory block1\n+  auto mod = op.getOperation()->getParentOfType<ModuleOp>();\n+  unsigned numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+  smemShapes[1].push_back(numWarps * 32);\n+\n+  /// shared memory block2\n+  smemShapes[2] = convertType<unsigned>(srcShape);\n+  smemShapes[2].erase(smemShapes[2].begin() + axis);\n+\n+  return smemShapes;\n+}\n+\n+unsigned getScratchSizeInBytesForReduce(triton::ReduceOp op) {\n+  unsigned elems = 0;\n+  if (enableFastReduce(op)) {\n+    auto smemShapes = getScratchConfigsForReduceFast(op);\n+    for (const auto &smemShape : smemShapes)\n+      elems = std::max(elems, product<unsigned>(smemShape));\n   } else {\n-    unsigned threadsPerCTAAxis = gpu::getThreadsPerWarp(srcLayout)[axis] *\n-                                 gpu::getWarpsPerCTA(srcLayout)[axis];\n-    smemShape[axis] = threadsPerCTAAxis;\n+    auto smemShape = getScratchConfigForReduceBasic(op);\n+    elems = product<unsigned>(smemShape);\n   }\n \n-  return smemShape;\n+  auto tensorType = op.operand().getType().cast<RankedTensorType>();\n+  unsigned bytes = elems * tensorType.getElementTypeBitWidth() / 8;\n+\n+  if (triton::ReduceOp::withIndex(op.redOp()))\n+    bytes += elems * sizeof(int32_t);\n+\n+  return bytes;\n }\n \n+/// ====================== reduce op config end ==========================\n+\n // TODO: extend beyond scalars\n SmallVector<unsigned> getScratchConfigForAtomicRMW(triton::AtomicRMWOp op) {\n   SmallVector<unsigned> smemShape;\n@@ -178,22 +222,8 @@ class AllocationAnalysis {\n   /// Initializes temporary shared memory for a given operation.\n   void getScratchValueSize(Operation *op) {\n     if (auto reduceOp = dyn_cast<triton::ReduceOp>(op)) {\n-      // TODO(Keren): Reduce with index is not supported yet.\n-      auto value = op->getOperand(0);\n-      if (auto tensorType = value.getType().dyn_cast<RankedTensorType>()) {\n-        auto srcLayout = tensorType.getEncoding();\n-        bool fastReduce = reduceOp.axis() == getOrder(srcLayout)[0];\n-        auto smemShape = getScratchConfigForReduce(reduceOp);\n-        unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n-                                         std::multiplies{});\n-        if (fastReduce) {\n-          auto mod = op->getParentOfType<ModuleOp>();\n-          unsigned numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n-          elems = std::max<unsigned>(elems, numWarps * 32);\n-        }\n-        auto bytes = elems * tensorType.getElementTypeBitWidth() / 8;\n-        allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n-      }\n+      unsigned bytes = getScratchSizeInBytesForReduce(reduceOp);\n+      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n       auto srcTy = cvtLayout.src().getType().cast<RankedTensorType>();\n       auto dstTy = cvtLayout.result().getType().cast<RankedTensorType>();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 193, "deletions": 29, "changes": 222, "file_content_changes": "@@ -121,12 +121,32 @@ void llPrintf(StringRef msg, ValueRange args,\n   rewriter.create<LLVM::ExtractElementOp>(loc, __VA_ARGS__)\n #define load(...) rewriter.create<LLVM::LoadOp>(loc, __VA_ARGS__)\n #define store(val, ptr) rewriter.create<LLVM::StoreOp>(loc, val, ptr)\n+#define fcmp_ogt(lhs, rhs)                                                     \\\n+  rewriter.create<LLVM::FCmpOp>(loc, rewriter.getI1Type(),                     \\\n+                                LLVM::FCmpPredicate::ogt, lhs, rhs)\n+#define fcmp_olt(lhs, rhs)                                                     \\\n+  rewriter.create<LLVM::FCmpOp>(loc, rewriter.getI1Type(),                     \\\n+                                LLVM::FCmpPredicate::olt, lhs, rhs)\n #define icmp_eq(...)                                                           \\\n   rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::eq, __VA_ARGS__)\n #define icmp_ne(...)                                                           \\\n   rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ne, __VA_ARGS__)\n #define icmp_slt(...)                                                          \\\n   rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::slt, __VA_ARGS__)\n+#define icmp_sle(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::sle, __VA_ARGS__)\n+#define icmp_sgt(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::sgt, __VA_ARGS__)\n+#define icmp_sge(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::sge, __VA_ARGS__)\n+#define icmp_ult(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ult, __VA_ARGS__)\n+#define icmp_ule(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ule, __VA_ARGS__)\n+#define icmp_ugt(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ugt, __VA_ARGS__)\n+#define icmp_uge(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::uge, __VA_ARGS__)\n #define select(...) rewriter.create<LLVM::SelectOp>(loc, __VA_ARGS__)\n #define address_of(...) rewriter.create<LLVM::AddressOfOp>(loc, __VA_ARGS__)\n #define barrier() rewriter.create<mlir::gpu::BarrierOp>(loc)\n@@ -1547,6 +1567,10 @@ struct ReduceOpConversion\n   void accumulate(ConversionPatternRewriter &rewriter, Location loc,\n                   RedOp redOp, Value &acc, Value cur, bool isFirst) const;\n \n+  void accumulateWithIndex(ConversionPatternRewriter &rewriter, Location loc,\n+                           RedOp redOp, Value &acc, Value &accIndex, Value cur,\n+                           Value curIndex, bool isFirst) const;\n+\n   Value shflSync(ConversionPatternRewriter &rewriter, Location loc, Value val,\n                  int i) const;\n \n@@ -1563,9 +1587,7 @@ struct ReduceOpConversion\n LogicalResult\n ReduceOpConversion::matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter) const {\n-  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n-  auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n-  if (op.axis() == srcLayout.getOrder()[0])\n+  if (enableFastReduce(op))\n     return matchAndRewriteFast(op, adaptor, rewriter);\n   return matchAndRewriteBasic(op, adaptor, rewriter);\n }\n@@ -1577,7 +1599,6 @@ void ReduceOpConversion::accumulate(ConversionPatternRewriter &rewriter,\n     acc = cur;\n     return;\n   }\n-  auto type = cur.getType();\n   switch (redOp) {\n   case RedOp::ADD:\n     acc = add(acc, cur);\n@@ -1606,6 +1627,75 @@ void ReduceOpConversion::accumulate(ConversionPatternRewriter &rewriter,\n   case RedOp::XOR:\n     acc = xor_(acc, cur);\n     break;\n+  case RedOp::ARGMIN:\n+  case RedOp::ARGMAX:\n+  case RedOp::ARGUMIN:\n+  case RedOp::ARGUMAX:\n+  case RedOp::ARGFMIN:\n+  case RedOp::ARGFMAX:\n+    llvm::report_fatal_error(\n+        \"This accumulate implementation is not for argmin / argmax\");\n+  default:\n+    llvm::report_fatal_error(\"Unsupported reduce op\");\n+  }\n+}\n+\n+void ReduceOpConversion::accumulateWithIndex(\n+    ConversionPatternRewriter &rewriter, Location loc, RedOp redOp, Value &acc,\n+    Value &accIndex, Value cur, Value curIndex, bool isFirst) const {\n+  if (isFirst) {\n+    acc = cur;\n+    accIndex = curIndex;\n+    return;\n+  }\n+  switch (redOp) {\n+  case RedOp::ARGMIN:\n+    accIndex =\n+        select(icmp_slt(acc, cur), accIndex,\n+               select(icmp_sgt(acc, cur), curIndex, smin(accIndex, curIndex)));\n+    acc = smin(acc, cur);\n+    break;\n+  case RedOp::ARGMAX:\n+    accIndex =\n+        select(icmp_sgt(acc, cur), accIndex,\n+               select(icmp_slt(acc, cur), curIndex, smin(accIndex, curIndex)));\n+    acc = smax(acc, cur);\n+    break;\n+  case RedOp::ARGUMIN:\n+    accIndex =\n+        select(icmp_ult(acc, cur), accIndex,\n+               select(icmp_ugt(acc, cur), curIndex, smin(accIndex, curIndex)));\n+    acc = umin(acc, cur);\n+    break;\n+  case RedOp::ARGUMAX:\n+    accIndex =\n+        select(icmp_ugt(acc, cur), accIndex,\n+               select(icmp_ult(acc, cur), curIndex, smin(accIndex, curIndex)));\n+    acc = umax(acc, cur);\n+    break;\n+  case RedOp::ARGFMIN:\n+    accIndex =\n+        select(fcmp_olt(acc, cur), accIndex,\n+               select(fcmp_ogt(acc, cur), curIndex, smin(accIndex, curIndex)));\n+    acc = fmin(acc, cur);\n+    break;\n+  case RedOp::ARGFMAX:\n+    accIndex =\n+        select(fcmp_ogt(acc, cur), accIndex,\n+               select(fcmp_olt(acc, cur), curIndex, smin(accIndex, curIndex)));\n+    acc = fmax(acc, cur);\n+    break;\n+  case RedOp::ADD:\n+  case RedOp::FADD:\n+  case RedOp::MIN:\n+  case RedOp::MAX:\n+  case RedOp::UMIN:\n+  case RedOp::UMAX:\n+  case RedOp::FMIN:\n+  case RedOp::FMAX:\n+  case RedOp::XOR:\n+    llvm::report_fatal_error(\n+        \"This accumulate implementation is only for argmin / argmax\");\n   default:\n     llvm::report_fatal_error(\"Unsupported reduce op\");\n   }\n@@ -1644,18 +1734,24 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     ConversionPatternRewriter &rewriter) const {\n   Location loc = op->getLoc();\n   unsigned axis = op.axis();\n+  bool withIndex = triton::ReduceOp::withIndex(op.redOp());\n \n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n   auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n   auto srcOrd = srcLayout.getOrder();\n   auto srcShape = srcTy.getShape();\n \n   auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n+  auto llvmIndexTy = getTypeConverter()->getIndexType();\n   auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+  auto indexPtrTy = LLVM::LLVMPointerType::get(llvmIndexTy, 3);\n   Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n   smemBase = bitcast(smemBase, elemPtrTy);\n \n-  auto smemShape = getScratchConfigForReduce(op);\n+  auto smemShape = getScratchConfigForReduceBasic(op);\n+  unsigned elems = product<unsigned>(smemShape);\n+  Value indexSmemBase = gep(elemPtrTy, smemBase, i32_val(elems));\n+  indexSmemBase = bitcast(indexSmemBase, indexPtrTy);\n \n   unsigned srcElems = getElemsPerThread(srcTy);\n   auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n@@ -1665,14 +1761,21 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n       emitOffsetForBlockedLayout(srcLayout, srcShape);\n \n   std::map<SmallVector<unsigned>, Value> accs;\n+  std::map<SmallVector<unsigned>, Value> accIndices;\n   std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n \n   // reduce within threads\n   for (unsigned i = 0; i < srcElems; ++i) {\n     SmallVector<unsigned> key = offset[i];\n     key[axis] = 0;\n     bool isFirst = accs.find(key) == accs.end();\n-    accumulate(rewriter, loc, op.redOp(), accs[key], srcValues[i], isFirst);\n+    if (!withIndex) {\n+      accumulate(rewriter, loc, op.redOp(), accs[key], srcValues[i], isFirst);\n+    } else {\n+      Value curIndex = srcIndices[i][axis];\n+      accumulateWithIndex(rewriter, loc, op.redOp(), accs[key], accIndices[key],\n+                          srcValues[i], curIndex, isFirst);\n+    }\n     if (isFirst)\n       indices[key] = srcIndices[i];\n   }\n@@ -1688,12 +1791,18 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n   for (auto it : accs) {\n     const SmallVector<unsigned> &key = it.first;\n     Value acc = it.second;\n+    Value accIndex;\n+    if (withIndex)\n+      accIndex = accIndices[key];\n     SmallVector<Value> writeIdx = indices[key];\n \n     writeIdx[axis] = udiv(writeIdx[axis], sizePerThread);\n     Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, srcOrd);\n     Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+    Value indexWritePtr = gep(indexPtrTy, indexSmemBase, writeOffset);\n     store(acc, writePtr);\n+    if (withIndex)\n+      store(accIndex, indexWritePtr);\n \n     SmallVector<Value> readIdx(writeIdx.size(), ints[0]);\n     for (int N = smemShape[axis] / 2; N > 0; N >>= 1) {\n@@ -1704,11 +1813,24 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n                  ints[0]);\n       Value readPtr = gep(elemPtrTy, writePtr, readOffset);\n       barrier();\n-      accumulate(rewriter, loc, op.redOp(), acc, load(readPtr), false);\n-      store(acc, writePtr);\n+      if (!withIndex) {\n+        Value cur = load(readPtr);\n+        accumulate(rewriter, loc, op.redOp(), acc, cur, false);\n+        store(acc, writePtr);\n+      } else {\n+        Value cur = load(readPtr);\n+        Value indexReadPtr = gep(indexPtrTy, indexWritePtr, readOffset);\n+        Value curIndex = load(indexReadPtr);\n+        accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, cur,\n+                            curIndex, false);\n+        store(acc, writePtr);\n+        store(accIndex, indexWritePtr);\n+      }\n     }\n   }\n \n+  barrier();\n+\n   // set output values\n   if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n     // nd-tensor where n >= 1\n@@ -1719,25 +1841,25 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultShape);\n     assert(resultIndices.size() == resultElems);\n \n-    barrier();\n     SmallVector<Value> resultVals(resultElems);\n     for (unsigned i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       readIdx.insert(readIdx.begin() + axis, ints[0]);\n       Value readOffset = linearize(rewriter, loc, readIdx, smemShape, srcOrd);\n       Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n-      resultVals[i] = load(readPtr);\n+      Value indexReadPtr = gep(indexPtrTy, indexSmemBase, readOffset);\n+      resultVals[i] = withIndex ? load(indexReadPtr) : load(readPtr);\n     }\n \n-    SmallVector<Type> resultTypes(resultElems, llvmElemTy);\n+    SmallVector<Type> resultTypes(resultElems,\n+                                  withIndex ? llvmIndexTy : llvmElemTy);\n     Type structTy =\n         LLVM::LLVMStructType::getLiteral(this->getContext(), resultTypes);\n     Value ret = getStructFromElements(loc, resultVals, rewriter, structTy);\n     rewriter.replaceOp(op, ret);\n   } else {\n     // 0d-tensor -> scalar\n-    barrier();\n-    Value resultVal = load(smemBase);\n+    Value resultVal = withIndex ? load(indexSmemBase) : load(smemBase);\n     rewriter.replaceOp(op, resultVal);\n   }\n \n@@ -1749,6 +1871,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n     ConversionPatternRewriter &rewriter) const {\n   Location loc = op->getLoc();\n   unsigned axis = adaptor.axis();\n+  bool withIndex = triton::ReduceOp::withIndex(op.redOp());\n \n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n   auto srcLayout = srcTy.getEncoding();\n@@ -1759,10 +1882,19 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcLayout);\n \n   auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n+  auto llvmIndexTy = getTypeConverter()->getIndexType();\n   auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+  auto indexPtrTy = LLVM::LLVMPointerType::get(llvmIndexTy, 3);\n   Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n   smemBase = bitcast(smemBase, elemPtrTy);\n \n+  auto smemShapes = getScratchConfigsForReduceFast(op);\n+  unsigned elems = product<unsigned>(smemShapes[0]);\n+  unsigned maxElems = std::max(elems, product<unsigned>(smemShapes[1]));\n+  maxElems = std::max(maxElems, product<unsigned>(smemShapes[2]));\n+  Value indexSmemBase = gep(elemPtrTy, smemBase, i32_val(maxElems));\n+  indexSmemBase = bitcast(indexSmemBase, indexPtrTy);\n+\n   auto order = getOrder(srcLayout);\n   unsigned sizeIntraWarps = threadsPerWarp[axis];\n   unsigned sizeInterWarps = warpsPerCTA[axis];\n@@ -1775,16 +1907,21 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n       emitOffsetForLayout(srcLayout, srcShape);\n \n   std::map<SmallVector<unsigned>, Value> accs;\n+  std::map<SmallVector<unsigned>, Value> accIndices;\n   std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n \n-  auto smemShape = getScratchConfigForReduce(op);\n-\n   // reduce within threads\n   for (unsigned i = 0; i < srcElems; ++i) {\n     SmallVector<unsigned> key = offset[i];\n     key[axis] = 0;\n     bool isFirst = accs.find(key) == accs.end();\n-    accumulate(rewriter, loc, op.redOp(), accs[key], srcValues[i], isFirst);\n+    if (!withIndex) {\n+      accumulate(rewriter, loc, op.redOp(), accs[key], srcValues[i], isFirst);\n+    } else {\n+      Value curIndex = srcIndices[i][axis];\n+      accumulateWithIndex(rewriter, loc, op.redOp(), accs[key], accIndices[key],\n+                          srcValues[i], curIndex, isFirst);\n+    }\n     if (isFirst)\n       indices[key] = srcIndices[i];\n   }\n@@ -1809,18 +1946,32 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   for (auto it : accs) {\n     const SmallVector<unsigned> &key = it.first;\n     Value acc = it.second;\n+    Value accIndex;\n+    if (withIndex)\n+      accIndex = accIndices[key];\n \n     // reduce within warps\n     for (unsigned N = sizeIntraWarps / 2; N > 0; N >>= 1) {\n       Value shfl = shflSync(rewriter, loc, acc, N);\n-      accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n+      if (!withIndex) {\n+        accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n+      } else {\n+        Value shflIndex = shflSync(rewriter, loc, accIndex, N);\n+        accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, shfl,\n+                            shflIndex, false);\n+      }\n     }\n \n     SmallVector<Value> writeIdx = indices[key];\n     writeIdx[axis] = (sizeInterWarps == 1) ? zero : warpIdAxis;\n-    Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, order);\n+    Value writeOffset =\n+        linearize(rewriter, loc, writeIdx, smemShapes[0], order);\n     Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n     storeShared(rewriter, loc, writePtr, acc, laneZero);\n+    if (withIndex) {\n+      Value indexWritePtr = gep(indexPtrTy, indexSmemBase, writeOffset);\n+      storeShared(rewriter, loc, indexWritePtr, accIndex, laneZero);\n+    }\n   }\n \n   barrier();\n@@ -1832,18 +1983,28 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   //\n   // each thread needs to process:\n   //   elemsPerThread = sizeInterWarps * s1 * s2 .. Sn / numThreads\n-  unsigned elems = product<unsigned>(smemShape);\n   unsigned numThreads =\n       product<unsigned>(triton::gpu::getWarpsPerCTA(srcLayout)) * 32;\n   unsigned elemsPerThread = std::max<unsigned>(elems / numThreads, 1);\n   Value readOffset = threadId;\n   for (unsigned round = 0; round < elemsPerThread; ++round) {\n     Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n     Value acc = load(readPtr);\n+    Value accIndex;\n+    if (withIndex) {\n+      Value readIndexPtr = gep(indexPtrTy, indexSmemBase, readOffset);\n+      accIndex = load(readIndexPtr);\n+    }\n \n     for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n       Value shfl = shflSync(rewriter, loc, acc, N);\n-      accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n+      if (!withIndex) {\n+        accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n+      } else {\n+        Value shflIndex = shflSync(rewriter, loc, accIndex, N);\n+        accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, shfl,\n+                            shflIndex, false);\n+      }\n     }\n \n     Value writeOffset = udiv(readOffset, i32_val(sizeInterWarps));\n@@ -1852,8 +2013,12 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n     Value laneIdModSizeInterWarps = urem(laneId, i32_val(sizeInterWarps));\n     Value laneIdModSizeInterWarpsIsZero =\n         icmp_eq(laneIdModSizeInterWarps, zero);\n-    storeShared(rewriter, loc, writePtr, acc,\n-                and_(threadIsNeeded, laneIdModSizeInterWarpsIsZero));\n+    Value pred = and_(threadIsNeeded, laneIdModSizeInterWarpsIsZero);\n+    storeShared(rewriter, loc, writePtr, acc, pred);\n+    if (withIndex) {\n+      Value writeIndexPtr = gep(indexPtrTy, indexSmemBase, writeOffset);\n+      storeShared(rewriter, loc, writeIndexPtr, accIndex, pred);\n+    }\n \n     if (round != elemsPerThread - 1) {\n       readOffset = add(readOffset, i32_val(numThreads));\n@@ -1881,25 +2046,24 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n     assert(resultIndices.size() == resultElems);\n \n     SmallVector<Value> resultVals(resultElems);\n-    SmallVector<unsigned> resultShape;\n-    std::copy(resultTy.getShape().begin(), resultTy.getShape().end(),\n-              std::back_inserter(resultShape));\n     for (size_t i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       Value readOffset =\n-          linearize(rewriter, loc, readIdx, resultShape, resultOrd);\n+          linearize(rewriter, loc, readIdx, smemShapes[2], resultOrd);\n       Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n-      resultVals[i] = load(readPtr);\n+      Value indexReadPtr = gep(indexPtrTy, indexSmemBase, readOffset);\n+      resultVals[i] = withIndex ? load(indexReadPtr) : load(readPtr);\n     }\n \n-    SmallVector<Type> resultTypes(resultElems, llvmElemTy);\n+    SmallVector<Type> resultTypes(resultElems,\n+                                  withIndex ? llvmIndexTy : llvmElemTy);\n     Type structTy =\n         LLVM::LLVMStructType::getLiteral(this->getContext(), resultTypes);\n     Value ret = getStructFromElements(loc, resultVals, rewriter, structTy);\n     rewriter.replaceOp(op, ret);\n   } else {\n     // 0d-tensor -> scalar\n-    Value resultVal = load(smemBase);\n+    Value resultVal = withIndex ? load(indexSmemBase) : load(smemBase);\n     rewriter.replaceOp(op, resultVal);\n   }\n "}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 15, "deletions": 2, "changes": 17, "file_content_changes": "@@ -240,12 +240,16 @@ mlir::LogicalResult mlir::triton::ReduceOp::inferReturnTypes(\n   Value arg = operands[0];\n   auto argTy = arg.getType().cast<RankedTensorType>();\n   auto argEltTy = argTy.getElementType();\n+  auto i32Ty = IntegerType::get(argEltTy.getContext(), 32);\n+  auto redOp = attributes.get(\"redOp\").cast<mlir::triton::RedOpAttr>().getValue();\n+  bool withIndex = mlir::triton::ReduceOp::withIndex(redOp);\n+  auto retEltTy = withIndex ? i32Ty : argEltTy;\n   auto retShape = argTy.getShape().vec();\n   int axis = attributes.get(\"axis\").cast<IntegerAttr>().getInt();\n   retShape.erase(retShape.begin() + axis);\n   if (retShape.empty()) {\n     // 0d-tensor -> scalar\n-    inferredReturnTypes.push_back(argEltTy);\n+    inferredReturnTypes.push_back(retEltTy);\n   } else {\n     // nd-tensor where n >= 1\n     // infer encoding\n@@ -264,11 +268,20 @@ mlir::LogicalResult mlir::triton::ReduceOp::inferReturnTypes(\n     }\n     // create type\n     inferredReturnTypes.push_back(\n-        RankedTensorType::get(retShape, argEltTy, retEncoding));\n+        RankedTensorType::get(retShape, retEltTy, retEncoding));\n   }\n   return mlir::success();\n }\n \n+bool mlir::triton::ReduceOp::withIndex(mlir::triton::RedOp redOp) {\n+  return redOp == mlir::triton::RedOp::ARGMIN ||\n+         redOp == mlir::triton::RedOp::ARGMAX ||\n+         redOp == mlir::triton::RedOp::ARGUMIN ||\n+         redOp == mlir::triton::RedOp::ARGUMAX ||\n+         redOp == mlir::triton::RedOp::ARGFMIN ||\n+         redOp == mlir::triton::RedOp::ARGFMAX;\n+}\n+\n //-- SplatOp --\n OpFoldResult SplatOp::fold(ArrayRef<Attribute> operands) {\n   auto constOperand = src().getDefiningOp<arith::ConstantOp>();"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -1195,10 +1195,11 @@ void init_triton_ir(py::module &&m) {\n                  operand.getType().dyn_cast<mlir::RankedTensorType>();\n              std::vector<int64_t> shape = inputTensorType.getShape();\n              shape.erase(shape.begin() + axis);\n-             mlir::Type resType = inputTensorType.getElementType();\n+             bool withIndex = mlir::triton::ReduceOp::withIndex(redOp);\n+             mlir::Type resType = withIndex ? self.getI32Type()\n+                                            : inputTensorType.getElementType();\n              if (!shape.empty()) {\n-               resType = mlir::RankedTensorType::get(\n-                   shape, inputTensorType.getElementType());\n+               resType = mlir::RankedTensorType::get(shape, resType);\n              }\n              return self.create<mlir::triton::ReduceOp>(loc, resType, redOp,\n                                                         operand, axis);"}, {"filename": "python/tests/test_reduce.py", "status": "modified", "additions": 35, "deletions": 7, "changes": 42, "file_content_changes": "@@ -1,4 +1,5 @@\n import pytest\n+import numpy as np\n import torch\n from torch.testing import assert_close\n \n@@ -13,7 +14,9 @@\n dtype_mapping = {dtype_str: torch.__dict__[dtype_str] for dtype_str in dtypes}\n \n \n-def get_reduced_dtype(dtype):\n+def get_reduced_dtype(op, dtype):\n+    if op in ['argmin', 'argmax']:\n+        return torch.int32\n     if dtype in [torch.int8, torch.int16, torch.uint8]:\n         return torch.int32\n     if dtype in [torch.bfloat16]:\n@@ -48,16 +51,19 @@ def reduce2d_kernel(x_ptr, z_ptr, axis: tl.constexpr, block_m: tl.constexpr, blo\n \n reduce1d_configs = [\n     (op, dtype, shape)\n-    for op in ['sum', 'min', 'max']\n+    for op in ['sum', 'min', 'max', 'argmin', 'argmax', 'xor_sum']\n     for dtype in dtypes\n     for shape in [4, 8, 16, 32, 64, 128, 512, 1024]\n ]\n \n \n @pytest.mark.parametrize('op, dtype, shape', reduce1d_configs)\n def test_reduce1d(op, dtype, shape):\n+    if op == 'xor_sum' and dtype in float_dtypes:\n+        return\n+\n     dtype = dtype_mapping[dtype]\n-    reduced_dtype = get_reduced_dtype(dtype)\n+    reduced_dtype = get_reduced_dtype(op, dtype)\n \n     if dtype.is_floating_point:\n         x = torch.randn((shape,), device='cuda', dtype=dtype)\n@@ -79,8 +85,17 @@ def test_reduce1d(op, dtype, shape):\n         golden_z = torch.sum(x, dtype=reduced_dtype)\n     elif op == 'min':\n         golden_z = torch.min(x).to(reduced_dtype)\n-    else:\n+    elif op == 'max':\n         golden_z = torch.max(x).to(reduced_dtype)\n+    elif op == 'argmin':\n+        golden_z = torch.argmin(x).to(reduced_dtype)\n+    elif op == 'argmax':\n+        golden_z = torch.argmax(x).to(reduced_dtype)\n+    elif op == 'xor_sum':\n+        sum_npy = np.bitwise_xor.reduce(x.cpu().numpy())\n+        golden_z = torch.tensor(sum_npy, dtype=reduced_dtype).cuda()\n+    else:\n+        raise RuntimeError(f'Unknwon reduce op {op}')\n \n     if dtype.is_floating_point and op == 'sum':\n         if shape >= 256:\n@@ -95,7 +110,7 @@ def test_reduce1d(op, dtype, shape):\n \n reduce2d_configs = [\n     (op, dtype, shape, axis)\n-    for op in ['sum', 'min', 'max']\n+    for op in ['sum', 'min', 'max', 'argmin', 'argmax', 'xor_sum']\n     for dtype in dtypes\n     for shape in [(1, 4), (1, 8), (1, 16), (1, 32), (2, 32), (4, 32), (4, 128), (32, 64)]\n     for axis in [0, 1]\n@@ -104,8 +119,11 @@ def test_reduce1d(op, dtype, shape):\n \n @pytest.mark.parametrize('op, dtype, shape, axis', reduce2d_configs)\n def test_reduce2d(op, dtype, shape, axis):\n+    if op == 'xor_sum' and dtype in float_dtypes:\n+        return\n+\n     dtype = dtype_mapping[dtype]\n-    reduced_dtype = get_reduced_dtype(dtype)\n+    reduced_dtype = get_reduced_dtype(op, dtype)\n     reduced_shape = (shape[1 - axis],)\n \n     if dtype.is_floating_point:\n@@ -123,8 +141,18 @@ def test_reduce2d(op, dtype, shape, axis):\n         golden_z = torch.sum(x, dim=axis, keepdim=False, dtype=reduced_dtype)\n     elif op == 'min':\n         golden_z = torch.min(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n-    else:\n+    elif op == 'max':\n         golden_z = torch.max(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n+    elif op == 'argmin':\n+        golden_z = torch.argmin(x, dim=axis, keepdim=False).to(reduced_dtype)\n+    elif op == 'argmax':\n+        golden_z = torch.argmax(x, dim=axis, keepdim=False).to(reduced_dtype)\n+    elif op == 'xor_sum':\n+        sum_npy = np.bitwise_xor.reduce(x.cpu().numpy(), axis=axis, keepdims=False)\n+        golden_z = torch.tensor(sum_npy, dtype=reduced_dtype).cuda()\n+    else:\n+        raise RuntimeError(f'Unknwon reduce op {op}')\n+\n     if dtype.is_floating_point and op == 'sum':\n         if shape[axis] >= 256:\n             assert_close(z, golden_z, rtol=0.05, atol=0.1)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -1041,13 +1041,27 @@ def max(input, axis, _builder=None):\n     return semantic.max(input, axis, _builder)\n \n \n+@builtin\n+@_add_reduction_docstr(\"maximum index\")\n+def argmax(input, axis, _builder=None):\n+    axis = _constexpr_to_value(axis)\n+    return semantic.argmax(input, axis, _builder)\n+\n+\n @builtin\n @_add_reduction_docstr(\"minimum\")\n def min(input, axis, _builder=None):\n     axis = _constexpr_to_value(axis)\n     return semantic.min(input, axis, _builder)\n \n \n+@builtin\n+@_add_reduction_docstr(\"minimum index\")\n+def argmin(input, axis, _builder=None):\n+    axis = _constexpr_to_value(axis)\n+    return semantic.argmin(input, axis, _builder)\n+\n+\n @builtin\n @_add_reduction_docstr(\"sum\")\n def sum(input, axis, _builder=None):"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -1061,10 +1061,18 @@ def min(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     return reduce_impl(input, axis, builder, \"min\", ir.REDUCE_OP.FMIN, ir.REDUCE_OP.MIN)\n \n \n+def argmin(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n+    return reduce_impl(input, axis, builder, \"argmin\", ir.REDUCE_OP.ARGFMIN, ir.REDUCE_OP.ARGMIN)\n+\n+\n def max(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     return reduce_impl(input, axis, builder, \"max\", ir.REDUCE_OP.FMAX, ir.REDUCE_OP.MAX)\n \n \n+def argmax(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n+    return reduce_impl(input, axis, builder, \"argmax\", ir.REDUCE_OP.ARGFMAX, ir.REDUCE_OP.ARGMAX)\n+\n+\n def sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     return reduce_impl(input, axis, builder, \"sum\", ir.REDUCE_OP.FADD, ir.REDUCE_OP.ADD)\n "}]