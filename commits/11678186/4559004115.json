[{"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 13, "deletions": 1, "changes": 14, "file_content_changes": "@@ -248,12 +248,24 @@ def _get_arg_specialization_key(self, arg) -> str:\n             return f'({arg} % {JITFunction.divisibility} == 0, {arg} == 1)'\n         else:\n             return '(False,)'\n+\n+    def _get_arg_sig_key(self, arg) -> str:\n+        arg_annotation = self.__annotations__.get(arg, None)\n+        if arg_annotation is torch.Tensor:\n+            return f'{arg}.dtype'\n+        elif arg_annotation is bool:\n+            return \"i1\"\n+        elif arg_annotation is float:\n+            return 'fp32'\n+        else:\n+            return f'_key_of({arg})'\n+\n     def _make_launcher(self):\n         regular_args = [f'{arg}' for i, arg in enumerate(self.arg_names) if i not in self.constexprs]\n         constexpr_args = [f'{arg}' for i, arg in enumerate(self.arg_names) if i in self.constexprs]\n         args = ', '.join(regular_args)\n         # cache key for regular argument type\n-        sig_keys = ', '.join([f'_key_of({arg})' for arg in regular_args])\n+        sig_keys = ', '.join([self._get_arg_sig_key(arg) for arg in regular_args])\n         # cache key for constexpr argument values\n         constexpr_keys = ', '.join(constexpr_args)\n         # cache key for argument specialization"}]