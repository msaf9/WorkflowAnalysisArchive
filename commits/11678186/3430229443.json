[{"filename": "include/triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -320,7 +320,7 @@ struct PTXInstrExecution {\n   // Prefix a !predicate to the instruction.\n   PTXInstrExecution &predicateNot(mlir::Value value, StringRef constraint) {\n     pred = instr->builder->newOperand(value, constraint);\n-    pred->repr = [](int idx) { return \"@!%\" + std::to_string(idx); };\n+    pred->repr = [](int idx) { return \"@!$\" + std::to_string(idx); };\n     return *this;\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/PtxAsmFormat.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -125,7 +125,7 @@ std::string PTXBuilder::dump() const {\n     lines.push_back(exec->dump());\n   }\n \n-  return strJoin(lines, \"\\r\\n\");\n+  return strJoin(lines, \"\\n\\t\");\n }\n \n PTXInstrExecution &PTXInstrCommon::call(ArrayRef<Operand *> oprs) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 262, "deletions": 40, "changes": 302, "file_content_changes": "@@ -87,7 +87,6 @@ static Value createLLVMIntegerConstant(OpBuilder &builder, Location loc,\n void llPrintf(StringRef msg, ValueRange args,\n               ConversionPatternRewriter &rewriter);\n \n-// Shortcuts for some commonly used LLVM ops to keep code simple and intuitive//\n // Shortcuts for some commonly used LLVM ops to keep code simple and intuitive\n #define zext(...) rewriter.create<LLVM::ZExtOp>(loc, __VA_ARGS__)\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n@@ -1051,7 +1050,7 @@ struct LoadOpConversion\n       if (other) {\n         for (size_t ii = 0; ii < nWords; ++ii) {\n           PTXInstr &mov = *ptxBuilder.create<>(\"mov\");\n-          mov.o(\"u\", width);\n+          mov.o(\"u\" + std::to_string(width));\n \n           size_t size = width / valueElemNbits;\n \n@@ -2923,8 +2922,8 @@ class MMA16816SmemLoader {\n       Value sOffset =\n           mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sTileStride);\n       Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n-      PTXBuilder builder;\n \n+      PTXBuilder builder;\n       // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n       // thread.\n       auto resArgs = builder.newListOperand(4, \"=r\");\n@@ -2943,12 +2942,13 @@ class MMA16816SmemLoader {\n         return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n       };\n \n-      Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n+      // The struct should have exactly the same element types.\n+      Type elemType = resV4.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n \n-      return {extract_val(fp16x2Ty, resV4, getIntAttr(0)),\n-              extract_val(fp16x2Ty, resV4, getIntAttr(1)),\n-              extract_val(fp16x2Ty, resV4, getIntAttr(2)),\n-              extract_val(fp16x2Ty, resV4, getIntAttr(3))};\n+      return {extract_val(elemType, resV4, getIntAttr(0)),\n+              extract_val(elemType, resV4, getIntAttr(1)),\n+              extract_val(elemType, resV4, getIntAttr(2)),\n+              extract_val(elemType, resV4, getIntAttr(3))};\n     } else if (elemBytes == 4 &&\n                needTrans) { // Use lds.32 to load tf32 matrices\n       Value ptr2 = getPtr(ptrIdx + 1);\n@@ -2961,20 +2961,25 @@ class MMA16816SmemLoader {\n \n       Value elems[4];\n       Type elemTy = type::f32Ty(ctx);\n+      Type elemPtrTy = ptr_ty(elemTy);\n       if (kOrder == 1) {\n-        elems[0] = load(gep(elemTy, ptr, sOffsetElemVal));\n-        elems[1] = load(gep(elemTy, ptr2, sOffsetElemVal));\n-        elems[2] = load(gep(elemTy, ptr, sOffsetArrElemVal));\n-        elems[3] = load(gep(elemTy, ptr2, sOffsetArrElemVal));\n+        elems[0] = load(gep(elemPtrTy, ptr, i32_val(sOffsetElem)));\n+        elems[1] = load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem)));\n+        elems[2] =\n+            load(gep(elemPtrTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+        elems[3] =\n+            load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n       } else {\n-        elems[0] = load(gep(elemTy, ptr, sOffsetElemVal));\n-        elems[2] = load(gep(elemTy, ptr2, sOffsetElemVal));\n-        elems[1] = load(gep(elemTy, ptr, sOffsetArrElemVal));\n-        elems[3] = load(gep(elemTy, ptr2, sOffsetArrElemVal));\n+        elems[0] = load(gep(elemPtrTy, ptr, i32_val(sOffsetElem)));\n+        elems[2] = load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem)));\n+        elems[1] =\n+            load(gep(elemPtrTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+        elems[3] =\n+            load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n       }\n-\n       return {elems[0], elems[1], elems[2], elems[3]};\n-    } else if (elemBytes == 1 && needTrans) {\n+\n+    } else if (elemBytes == 1 && needTrans) { // work with int8\n       std::array<std::array<Value, 4>, 2> ptrs;\n       ptrs[0] = {\n           getPtr(ptrIdx),\n@@ -3004,15 +3009,16 @@ class MMA16816SmemLoader {\n \n       Value i8Elems[4][4];\n       Type elemTy = type::i8Ty(ctx);\n+      Type elemPtrTy = ptr_ty(elemTy);\n       if (kOrder == 1) {\n         for (int i = 0; i < 2; ++i)\n           for (int j = 0; j < 4; ++j)\n-            i8Elems[i][j] = load(gep(elemTy, ptrs[i][j], sOffsetElemVal));\n+            i8Elems[i][j] = load(gep(elemPtrTy, ptrs[i][j], sOffsetElemVal));\n \n         for (int i = 2; i < 4; ++i)\n           for (int j = 0; j < 4; ++j)\n             i8Elems[i][j] =\n-                load(gep(elemTy, ptrs[i - 2][j], sOffsetArrElemVal));\n+                load(gep(elemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n \n         for (int m = 0; m < 4; ++m) {\n           for (int e = 0; e < 4; ++e)\n@@ -3022,13 +3028,13 @@ class MMA16816SmemLoader {\n         }\n       } else { // k first\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[0][j] = load(gep(elemTy, ptrs[0][j], sOffsetElemVal));\n+          i8Elems[0][j] = load(gep(elemPtrTy, ptrs[0][j], sOffsetElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[2][j] = load(gep(elemTy, ptrs[1][j], sOffsetElemVal));\n+          i8Elems[2][j] = load(gep(elemPtrTy, ptrs[1][j], sOffsetElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[1][j] = load(gep(elemTy, ptrs[0][j], sOffsetArrElemVal));\n+          i8Elems[1][j] = load(gep(elemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[3][j] = load(gep(elemTy, ptrs[1][j], sOffsetArrElemVal));\n+          i8Elems[3][j] = load(gep(elemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n \n         for (int m = 0; m < 4; ++m) {\n           for (int e = 0; e < 4; ++e)\n@@ -3112,6 +3118,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     size_t reduceAxis = 1;\n     unsigned K = AShape[reduceAxis];\n     bool isOuter = K == 1;\n+\n     bool isMMA = D.getType()\n                      .cast<RankedTensorType>()\n                      .getEncoding()\n@@ -3123,11 +3130,13 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                       .getEncoding()\n                       .cast<MmaEncodingAttr>();\n \n-    if (!isOuter && isMMA) {\n+    bool isHMMA = isDotHMMA(op);\n+    if (!isOuter && isMMA && isHMMA) {\n       if (mmaLayout.getVersion() == 1)\n         return convertMMA884(op, adaptor, rewriter);\n       if (mmaLayout.getVersion() == 2)\n         return convertMMA16816(op, adaptor, rewriter);\n+\n       llvm::report_fatal_error(\n           \"Unsupported MMA kind found when converting DotOp to LLVM.\");\n     }\n@@ -3140,6 +3149,49 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n         \"Unsupported DotOp found when converting TritonGPU to LLVM.\");\n   }\n \n+  // Tell whether a DotOp support HMMA.\n+  // This is port from the master branch, the original logic is retained.\n+  static bool isDotHMMA(DotOp op) {\n+    auto a = op.a();\n+    auto b = op.b();\n+    auto c = op.c();\n+    auto d = op.getResult();\n+    auto aTensorTy = a.getType().cast<RankedTensorType>();\n+    auto bTensorTy = b.getType().cast<RankedTensorType>();\n+    auto cTensorTy = c.getType().cast<RankedTensorType>();\n+    auto dTensorTy = d.getType().cast<RankedTensorType>();\n+\n+    if (!dTensorTy.getEncoding().isa<MmaEncodingAttr>())\n+      return false;\n+\n+    auto mmaLayout = dTensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto aElemTy = aTensorTy.getElementType();\n+    auto bElemTy = bTensorTy.getElementType();\n+\n+    assert((mmaLayout.getVersion() == 1 || mmaLayout.getVersion() == 2) &&\n+           \"Unexpected MMA layout version found\");\n+    // Refer to mma section for the data type supported by Volta and Hopper\n+    // Tensor Core in\n+    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-fragment-mma-884-f16\n+    return (aElemTy.isF16() && bElemTy.isF16()) ||\n+           (aElemTy.isBF16() && bElemTy.isBF16()) ||\n+           (aElemTy.isF32() && bElemTy.isF32() && op.allowTF32() &&\n+            mmaLayout.getVersion() >= 2) ||\n+           (aElemTy.isInteger(8) && bElemTy.isInteger(8) &&\n+            mmaLayout.getVersion() >= 2);\n+  }\n+\n+  // Tell whether a DotOp support HMMA by the operand type(either $a or $b).\n+  // We cannot get both the operand types(in TypeConverter), here we assume the\n+  // types of both the operands are identical here.\n+  // TODO[Superjomn]: Find a better way to implement it.\n+  static bool isDotHMMA(TensorType operand, bool allowTF32, int mmaVersion) {\n+    auto elemTy = operand.getElementType();\n+    return elemTy.isF16() || elemTy.isBF16() ||\n+           (elemTy.isF32() && allowTF32 && mmaVersion >= 2) ||\n+           (elemTy.isInteger(8) && mmaVersion >= 2);\n+  }\n+\n private:\n   // Convert to mma.m16n8k16\n   LogicalResult convertMMA16816(triton::DotOp a, OpAdaptor adaptor,\n@@ -3149,10 +3201,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                               ConversionPatternRewriter &rewriter) const;\n \n   LogicalResult convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n-                              ConversionPatternRewriter &rewriter) const {\n-    assert(false && \"Not implemented yet.\");\n-    return failure();\n-  }\n+                              ConversionPatternRewriter &rewriter) const;\n };\n \n // Helper for conversion of DotOp with mma<version=1>, that is sm<80\n@@ -3651,6 +3700,7 @@ struct MMA16816ConversionHelper {\n     std::function<void(int, int)> loadFn;\n     auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n     auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(aTensorTy);\n+\n     int numRepM = getNumRepM(aTensorTy, shape[0]);\n     int numRepK = getNumRepK(aTensorTy, shape[1]);\n \n@@ -3766,26 +3816,34 @@ struct MMA16816ConversionHelper {\n                                              std::to_string(i)));\n         // reuse the output registers\n       }\n+\n       mma(retArgs, aArgs, bArgs, cArgs);\n       Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n \n       auto getIntAttr = [&](int v) {\n         return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n       };\n \n+      Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n       for (int i = 0; i < 4; ++i)\n         fc[m * colsPerThread + 4 * n + i] =\n-            extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(i));\n+            extract_val(elemTy, mmaOut, getIntAttr(i));\n     };\n \n     for (int k = 0; k < numRepK; ++k)\n       for (int m = 0; m < numRepM; ++m)\n         for (int n = 0; n < numRepN; ++n)\n           callMma(2 * m, n, 2 * k);\n \n+    Type resElemTy = dTensorTy.getElementType();\n+\n+    for (auto &elem : fc) {\n+      elem = bitcast(elem, resElemTy);\n+    }\n+\n     // replace with new packed result\n     Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(fc.size(), type::f32Ty(ctx)));\n+        ctx, SmallVector<Type>(fc.size(), resElemTy));\n     Value res = getStructFromElements(loc, fc, rewriter, structTy);\n     rewriter.replaceOp(op, res);\n \n@@ -3821,9 +3879,7 @@ struct MMA16816ConversionHelper {\n           tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n           maxPhase, elemBytes, rewriter, typeConverter, loc);\n       SmallVector<Value> offs = loader.computeOffsets(warpId, lane);\n-\n       const int numPtrs = loader.getNumPtr();\n-\n       SmallVector<Value> ptrs(numPtrs);\n \n       Type smemPtrTy = helper.getShemPtrTy();\n@@ -3835,6 +3891,7 @@ struct MMA16816ConversionHelper {\n       auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n           (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n           ptrs, helper.getMatType(), helper.getShemPtrTy());\n+\n       if (!needTrans) {\n         ld2(vals, a, b, ha0);\n         ld2(vals, a + 1, b, ha1);\n@@ -3879,10 +3936,9 @@ struct MMA16816ConversionHelper {\n \n     assert(!elems.empty());\n \n-    Type fp16Ty = type::f16Ty(ctx);\n-    Type fp16x2Ty = vec_ty(fp16Ty, 2);\n+    Type elemTy = elems[0].getType();\n     Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(elems.size(), fp16x2Ty));\n+        ctx, SmallVector<Type>(elems.size(), elemTy));\n     auto result = getStructFromElements(loc, elems, rewriter, structTy);\n     return result;\n   }\n@@ -3921,9 +3977,25 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n       dotOperandLayout.getParent().dyn_cast_or_null<MmaEncodingAttr>();\n   assert(mmaLayout);\n \n+  bool isOuter{};\n+  {\n+    int K{};\n+    if (dotOperandLayout.getOpIdx() == 0) // $a\n+      K = dstTensorTy.getShape()[1];\n+    else // $b\n+      K = dstTensorTy.getShape()[0];\n+    isOuter = K == 1;\n+  }\n+\n+  // TODO[Superjomn]: the allowTF32 is not available in ConvertLayoutOp for it\n+  // is an attribute of DotOp.\n+  bool allowTF32 = false;\n+  bool isHMMA = DotOpConversion::isDotHMMA(dstTensorTy, allowTF32,\n+                                           mmaLayout.getVersion());\n+\n   auto smemObj = getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n   Value res;\n-  if (mmaLayout.getVersion() == 2) {\n+  if (!isOuter && mmaLayout.getVersion() == 2 && isHMMA) { // tensor core v2\n     MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n                                        rewriter, getTypeConverter(),\n                                        op.getLoc());\n@@ -3935,7 +4007,8 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n       // operand $b\n       res = mmaHelper.loadB(src, smemObj);\n     }\n-  } else if (mmaLayout.getVersion() == 1) {\n+  } else if (!isOuter && mmaLayout.getVersion() == 1 &&\n+             isHMMA) { // tensor core v1\n     DotOpMmaV1ConversionHelper helper(mmaLayout);\n     if (dotOperandLayout.getOpIdx() == 0) {\n       // operand $a\n@@ -4412,6 +4485,155 @@ DotOpMmaV1ConversionHelper::extractLoadedOperand(\n   return rcds;\n }\n \n+LogicalResult\n+DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n+                               ConversionPatternRewriter &rewriter) const {\n+  auto *ctx = rewriter.getContext();\n+  auto loc = op.getLoc();\n+  auto threadId = getThreadId(rewriter, loc);\n+\n+  using ValueTable = std::map<std::pair<int, int>, Value>;\n+\n+  auto A = op.a();\n+  auto B = op.b();\n+  auto C = op.c();\n+  auto D = op.getResult();\n+\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto cTensorTy = C.getType().cast<RankedTensorType>();\n+  auto dTensorTy = D.getType().cast<RankedTensorType>();\n+\n+  auto aShape = aTensorTy.getShape();\n+  auto bShape = bTensorTy.getShape();\n+  auto cShape = cTensorTy.getShape();\n+\n+  auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto cLayout = cTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto dLayout = dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+\n+  auto aOrder = aLayout.getOrder();\n+  auto bOrder = bLayout.getOrder();\n+\n+  auto order = dLayout.getOrder();\n+\n+  bool isARow = aOrder[0] == 1;\n+  bool isBRow = bOrder[0] == 1;\n+\n+  int strideAM = isARow ? aShape[1] : 1;\n+  int strideAK = isARow ? 1 : aShape[0];\n+  int strideBN = isBRow ? 1 : bShape[0];\n+  int strideBK = isBRow ? bShape[1] : 1;\n+  int strideA0 = isARow ? strideAK : strideAM;\n+  int strideA1 = isARow ? strideAM : strideAK;\n+  int strideB0 = isBRow ? strideBN : strideBK;\n+  int strideB1 = isBRow ? strideBK : strideBN;\n+  int lda = isARow ? strideAM : strideAK;\n+  int ldb = isBRow ? strideBK : strideBN;\n+  int aPerPhase = aLayout.getPerPhase();\n+  int aMaxPhase = aLayout.getMaxPhase();\n+  int bPerPhase = bLayout.getPerPhase();\n+  int bMaxPhase = bLayout.getMaxPhase();\n+  int aNumPtr = 8;\n+  int bNumPtr = 8;\n+  int NK = aShape[1];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value mContig = i32_val(sizePerThread[order[1]]);\n+  Value nContig = i32_val(sizePerThread[order[0]]);\n+\n+  // threadId in blocked layout\n+  SmallVector<Value> threadIds;\n+  {\n+    int dim = cShape.size();\n+    threadIds.resize(dim);\n+    for (unsigned k = 0; k < dim - 1; k++) {\n+      Value dimK = i32_val(shapePerCTA[order[k]]);\n+      Value rem = urem(threadId, dimK);\n+      threadId = udiv(threadId, dimK);\n+      threadIds[order[k]] = rem;\n+    }\n+    Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n+    threadIds[order[dim - 1]] = urem(threadId, dimK);\n+  }\n+\n+  Value threadIdM = threadIds[0];\n+  Value threadIdN = threadIds[1];\n+\n+  Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n+  Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n+  SmallVector<Value> aOff(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i) {\n+    aOff[i] = add(mul(offA0, i32_val(strideA0)), mul(offA1, i32_val(strideA1)));\n+  }\n+\n+  Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n+  Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n+  SmallVector<Value> bOff(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i) {\n+    bOff[i] = add(mul(offB0, i32_val(strideB0)), mul(offB1, i32_val(strideB1)));\n+  }\n+\n+  auto aSmem = getSharedMemoryObjectFromStruct(loc, adaptor.a(), rewriter);\n+  auto bSmem = getSharedMemoryObjectFromStruct(loc, adaptor.b(), rewriter);\n+\n+  Type f32PtrTy = ptr_ty(f32_ty);\n+  SmallVector<Value> aPtrs(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i)\n+    aPtrs[i] = gep(f32PtrTy, aSmem.base, aOff[i]);\n+\n+  SmallVector<Value> bPtrs(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i)\n+    bPtrs[i] = gep(f32PtrTy, bSmem.base, bOff[i]);\n+\n+  ValueTable has, hbs;\n+  auto cc = getElementsFromStruct(loc, adaptor.c(), rewriter);\n+  SmallVector<Value> ret = cc;\n+  // is this compatible with blocked layout?\n+\n+  for (unsigned k = 0; k < NK; k++) {\n+    int z = 0;\n+    for (unsigned i = 0; i < cShape[order[1]]; i += shapePerCTA[order[1]])\n+      for (unsigned j = 0; j < cShape[order[0]]; j += shapePerCTA[order[0]])\n+        for (unsigned ii = 0; ii < sizePerThread[order[1]]; ++ii)\n+          for (unsigned jj = 0; jj < sizePerThread[order[0]]; ++jj) {\n+            unsigned m = order[0] == 1 ? i : j;\n+            unsigned n = order[0] == 1 ? j : i;\n+            unsigned mm = order[0] == 1 ? ii : jj;\n+            unsigned nn = order[0] == 1 ? jj : ii;\n+            if (!has.count({m + mm, k})) {\n+              Value pa = gep(f32PtrTy, aPtrs[0],\n+                             i32_val((m + mm) * strideAM + k * strideAK));\n+              Value va = load(pa);\n+              has[{m + mm, k}] = va;\n+            }\n+            if (!hbs.count({n + nn, k})) {\n+              Value pb = gep(f32PtrTy, bPtrs[0],\n+                             i32_val((n + nn) * strideBN + k * strideBK));\n+              Value vb = load(pb);\n+              hbs[{n + nn, k}] = vb;\n+            }\n+\n+            ret[z] = rewriter.create<LLVM::FMulAddOp>(loc, has[{m + mm, k}],\n+                                                      hbs[{n + nn, k}], ret[z]);\n+            ++z;\n+          }\n+  }\n+\n+  auto res = getStructFromElements(\n+      loc, ret, rewriter,\n+      struct_ty(SmallVector<Type>(ret.size(), ret[0].getType())));\n+  rewriter.replaceOp(op, res);\n+\n+  return success();\n+}\n+\n /// ====================== mma codegen end ============================\n \n Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,\n@@ -5067,8 +5289,8 @@ void ConvertTritonGPUToLLVM::initSharedMemory(\n   OpBuilder b(mod.getBodyRegion());\n   auto loc = mod.getLoc();\n   auto elemTy = typeConverter.convertType(b.getIntegerType(8));\n-  // Set array size 0 and external linkage indicates that we use dynamic shared\n-  // allocation to allow a larger shared memory size for each kernel.\n+  // Set array size 0 and external linkage indicates that we use dynamic\n+  // shared allocation to allow a larger shared memory size for each kernel.\n   auto arrayTy = LLVM::LLVMArrayType::get(elemTy, 0);\n   auto global = b.create<LLVM::GlobalOp>(\n       loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::External,"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -140,10 +140,13 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n                   \"BlockedEncodingAttr not implemented\");\n     }\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    assert(mmaLayout.getVersion() == 2 &&\n-           \"mmaLayout version = 1 is not implemented yet\");\n-    return {16 * mmaLayout.getWarpsPerCTA()[0],\n-            8 * mmaLayout.getWarpsPerCTA()[1]};\n+    if (mmaLayout.getVersion() == 2)\n+      return {16 * mmaLayout.getWarpsPerCTA()[0],\n+              8 * mmaLayout.getWarpsPerCTA()[1]};\n+    if (mmaLayout.getVersion() == 1)\n+      return {16 * mmaLayout.getWarpsPerCTA()[0],\n+              16 * mmaLayout.getWarpsPerCTA()[1]};\n+    assert(0 && \"Unexpected MMA layout version found\");\n   } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n     auto parentLayout = dotLayout.getParent();\n     assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 9, "deletions": 1, "changes": 10, "file_content_changes": "@@ -597,6 +597,14 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto oldRetType = dotOp.getResult().getType().cast<RankedTensorType>();\n     if (oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n       return failure();\n+\n+    auto A = dotOp.getOperand(0).getType().cast<RankedTensorType>();\n+    auto B = dotOp.getOperand(1).getType().cast<RankedTensorType>();\n+    // for FMA, should retain the blocked layout.\n+    if (A.getElementType().isF32() && B.getElementType().isF32() &&\n+        !dotOp.allowTF32())\n+      return failure();\n+\n     // get MMA encoding for the given number of warps\n     auto retShape = oldRetType.getShape();\n     auto mod = op->getParentOfType<mlir::ModuleOp>();\n@@ -663,4 +671,4 @@ class TritonGPUCombineOpsPass\n \n std::unique_ptr<Pass> mlir::createTritonGPUCombineOpsPass() {\n   return std::make_unique<TritonGPUCombineOpsPass>();\n-}\n\\ No newline at end of file\n+}"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 89, "deletions": 2, "changes": 91, "file_content_changes": "@@ -55,6 +55,33 @@ def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n     assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n \n \n+@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n+    [64, 128, 128, 1],\n+    [128, 128, 128, 4],\n+    [16, 8, 32, 1],\n+    [32, 16, 64, 2],\n+    [32, 16, 64, 4],\n+])\n+def test_gemm_no_scf_int8(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n+    a = torch.randint(-5, 5, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n+    b = torch.randint(-5, 5, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n+    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.int32)\n+\n+    grid = lambda META: (1, )\n+    matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n+                               stride_am=a.stride(0), stride_ak=a.stride(1),\n+                               stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                               stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                               M=SIZE_M, N=SIZE_N, K=SIZE_K,\n+                               num_warps=NUM_WARPS)\n+\n+    aa = a.cpu()\n+    bb = b.cpu()\n+    golden = torch.matmul(aa.float(), bb.float()).int()\n+    torch.set_printoptions(profile=\"full\")\n+    torch.testing.assert_close(c.cpu(), golden, check_dtype=False)\n+\n+\n @triton.jit\n def matmul_kernel(\n     a_ptr, b_ptr, c_ptr,\n@@ -80,8 +107,6 @@ def matmul_kernel(\n     c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n     tl.store(c_ptrs, accumulator)\n \n-# TODO: DotConversion in TritonGPUToLLVM cannot support non-splat C for the moment\n-\n \n def get_variant_golden(a, b):\n     SIZE_M = a.shape[0]\n@@ -144,3 +169,65 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n \n     torch.set_printoptions(profile=\"full\")\n     assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n+\n+\n+@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n+    [32, 32, 16, 4, 32, 32, 16],\n+    [32, 16, 16, 4, 32, 32, 16],\n+    [128, 8, 8, 4, 32, 32, 16],\n+    [127, 41, 43, 4, 32, 32, 16],\n+])\n+def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n+    @triton.jit\n+    def matmul_kernel(\n+        a_ptr, b_ptr, c_ptr,\n+        M, N, K,\n+        stride_am, stride_ak,\n+        stride_bk, stride_bn,\n+        stride_cm, stride_cn,\n+        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+    ):\n+        pid = tl.program_id(axis=0)\n+        # num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+        pid_m = pid // num_pid_n\n+        pid_n = pid % num_pid_n\n+\n+        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        offs_k = tl.arange(0, BLOCK_SIZE_K)\n+        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+\n+        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        for k in range(0, K, BLOCK_SIZE_K):\n+            a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n+            b_mask = (offs_k[:, None] < K) & (offs_bn[None, :] < N)\n+            a = tl.load(a_ptrs, a_mask)\n+            b = tl.load(b_ptrs, b_mask)\n+            # NOTE the allow_tf32 should be false to force the dot op to do fmadot lowering\n+            accumulator += tl.dot(a, b, allow_tf32=False)\n+            a_ptrs += BLOCK_SIZE_K * stride_ak\n+            b_ptrs += BLOCK_SIZE_K * stride_bk\n+            offs_k += BLOCK_SIZE_K\n+\n+        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n+        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n+        tl.store(c_ptrs, accumulator, c_mask)\n+\n+    a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n+    b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n+    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n+\n+    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n+    matmul_kernel[grid](a, b, c,\n+                        M, N, K,\n+                        stride_am=a.stride(0), stride_ak=a.stride(1),\n+                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K)\n+\n+    golden = torch.matmul(a, b)\n+    torch.testing.assert_close(c, golden)"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 20, "deletions": 1, "changes": 21, "file_content_changes": "@@ -347,7 +347,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_extract_slice\n   func @basic_extract_slice() {\n     // CHECK: llvm.mlir.addressof @global_smem\n-    // CHECK: llvm.extractvalue \n+    // CHECK: llvm.extractvalue\n     // CHECK-NEXT: llvm.extractvalue\n     // CHECK-NEXT: llvm.extractvalue\n     // CHECK-NEXT: llvm.extractvalue\n@@ -785,3 +785,22 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     return\n   }\n }\n+\n+// -----\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n+#mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  func @matmul_fmadot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #blocked>\n+    // CHECK: llvm.intr.fmuladd\n+    %28 = tt.dot %a, %b, %cst {allowTF32 = false, transA = false, transB = false} : tensor<32x16xf32, #shared> * tensor<16x32xf32, #shared> -> tensor<32x32xf32, #blocked>\n+    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n+    %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n+    tt.store %36, %28 : tensor<32x32xf32, #blocked>\n+    return\n+  }\n+}"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/PtxAsmFormatTest.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -112,8 +112,8 @@ TEST_F(PtxAsmFormatTest, MultiLinePTX) {\n   mov(valVal1, constVal);\n   mov(valVal1, valVal0);\n \n-  EXPECT_EQ(builder.dump(), \"mov $0, 0x1;\\r\\n\"\n-                            \"mov $1, 0x1;\\r\\n\"\n+  EXPECT_EQ(builder.dump(), \"mov $0, 0x1;\\n\\t\"\n+                            \"mov $1, 0x1;\\n\\t\"\n                             \"mov $1, $0;\");\n \n   auto values = builder.getAllMLIRArgs();"}]