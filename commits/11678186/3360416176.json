[{"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 13, "deletions": 5, "changes": 18, "file_content_changes": "@@ -29,12 +29,20 @@ def TT_RedOpAttr : I32EnumAttr<\n     /*case*/\n     [\n         I32EnumAttrCase</*sym*/\"ADD\", 1, /*str*/\"add\">,\n-        I32EnumAttrCase<\"MAX\", 2, \"max\">,\n+        I32EnumAttrCase<\"FADD\", 2, \"fadd\">,\n         I32EnumAttrCase<\"MIN\", 3, \"min\">,\n-        I32EnumAttrCase<\"FADD\", 4, \"fadd\">,\n-        I32EnumAttrCase<\"FMAX\", 5, \"fmax\">,\n-        I32EnumAttrCase<\"FMIN\", 6, \"fmin\">,\n-        I32EnumAttrCase<\"XOR\", 7, \"xor\">\n+        I32EnumAttrCase<\"MAX\", 4, \"max\">,\n+        I32EnumAttrCase<\"UMIN\", 5, \"umin\">,\n+        I32EnumAttrCase<\"UMAX\", 6, \"umax\">,\n+        I32EnumAttrCase<\"ARGMIN\", 7, \"argmin\">,\n+        I32EnumAttrCase<\"ARGMAX\", 8, \"argmax\">,\n+        I32EnumAttrCase<\"ARGUMIN\", 9, \"argumin\">,\n+        I32EnumAttrCase<\"ARGUMAX\", 10, \"argumax\">,\n+        I32EnumAttrCase<\"FMIN\", 11, \"fmin\">,\n+        I32EnumAttrCase<\"FMAX\", 12, \"fmax\">,\n+        I32EnumAttrCase<\"ARGFMIN\", 13, \"argfmin\">,\n+        I32EnumAttrCase<\"ARGFMAX\", 14, \"argfmax\">,\n+        I32EnumAttrCase<\"XOR\", 15, \"xor\">\n     ]> {\n     let cppNamespace = \"::mlir::triton\";\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 13, "deletions": 13, "changes": 26, "file_content_changes": "@@ -1293,27 +1293,27 @@ void ReduceOpConversion::accumulate(ConversionPatternRewriter &rewriter,\n   case RedOp::ADD:\n     acc = add(acc, cur);\n     break;\n-  case RedOp::MAX:\n-    if (type.isUnsignedInteger())\n-      acc = umax(acc, cur);\n-    else\n-      acc = smax(acc, cur);\n+  case RedOp::FADD:\n+    acc = fadd(acc.getType(), acc, cur);\n     break;\n   case RedOp::MIN:\n-    if (type.isUnsignedInteger())\n-      acc = umin(acc, cur);\n-    else\n-      acc = smin(acc, cur);\n+    acc = smin(acc, cur);\n     break;\n-  case RedOp::FADD:\n-    acc = fadd(acc.getType(), acc, cur);\n+  case RedOp::MAX:\n+    acc = smax(acc, cur);\n     break;\n-  case RedOp::FMAX:\n-    acc = fmax(acc, cur);\n+  case RedOp::UMIN:\n+    acc = umin(acc, cur);\n+    break;\n+  case RedOp::UMAX:\n+    acc = umax(acc, cur);\n     break;\n   case RedOp::FMIN:\n     acc = fmin(acc, cur);\n     break;\n+  case RedOp::FMAX:\n+    acc = fmax(acc, cur);\n+    break;\n   case RedOp::XOR:\n     acc = xor_(acc, cur);\n     break;"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -87,8 +87,16 @@ void init_triton_ir(py::module &&m) {\n       .value(\"FADD\", mlir::triton::RedOp::FADD)\n       .value(\"MIN\", mlir::triton::RedOp::MIN)\n       .value(\"MAX\", mlir::triton::RedOp::MAX)\n+      .value(\"UMIN\", mlir::triton::RedOp::UMIN)\n+      .value(\"UMAX\", mlir::triton::RedOp::UMAX)\n+      .value(\"ARGMIN\", mlir::triton::RedOp::ARGMIN)\n+      .value(\"ARGMAX\", mlir::triton::RedOp::ARGMAX)\n+      .value(\"ARGUMIN\", mlir::triton::RedOp::ARGUMIN)\n+      .value(\"ARGUMAX\", mlir::triton::RedOp::ARGUMAX)\n       .value(\"FMIN\", mlir::triton::RedOp::FMIN)\n       .value(\"FMAX\", mlir::triton::RedOp::FMAX)\n+      .value(\"ARGFMIN\", mlir::triton::RedOp::ARGFMIN)\n+      .value(\"ARGFMAX\", mlir::triton::RedOp::ARGFMAX)\n       .value(\"XOR\", mlir::triton::RedOp::XOR);\n \n   py::enum_<mlir::triton::RMWOp>(m, \"ATOMIC_OP\")"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 124, "deletions": 112, "changes": 236, "file_content_changes": "@@ -870,121 +870,133 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n # # ---------------\n \n \n-# @pytest.mark.parametrize(\"op, dtype_str, shape\",\n-#                          [(op, dtype, shape)\n-#                           for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n-#                           for dtype in dtypes_with_bfloat16\n-#                           for shape in [32, 64, 128, 512]])\n-# def test_reduce1d(op, dtype_str, shape, device='cuda'):\n-#     check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+def get_reduced_dtype(dtype_str, op):\n+    if op == 'argmin' or op == 'argmax':\n+        return 'int32'\n+    if dtype_str in ['int8', 'uint8', 'int16', 'uint16']:\n+        return 'int32'\n+    if dtype_str == 'bfloat16':\n+        return 'float32'\n+    return dtype_str\n+\n+\n+# TODO: [Qingyi] Fix argmin / argmax\n+@pytest.mark.parametrize(\"op, dtype_str, shape\",\n+                         [(op, dtype, shape)\n+                          for op in ['min', 'max', 'sum']\n+                          for dtype in dtypes_with_bfloat16\n+                          for shape in [32, 64, 128, 512]])\n+def test_reduce1d(op, dtype_str, shape, device='cuda'):\n+    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n \n-#     # triton kernel\n-#     @triton.jit\n-#     def kernel(X, Z, BLOCK: tl.constexpr):\n-#         x = tl.load(X + tl.arange(0, BLOCK))\n-#         tl.store(Z, GENERATE_TEST_HERE)\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, Z, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        tl.store(Z, GENERATE_TEST_HERE)\n \n-#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=0)'})\n-#     # input\n-#     rs = RandomState(17)\n-#     # limit the range of integers so that the sum does not overflow\n-#     x = numpy_random((shape,), dtype_str=dtype_str, rs=rs)\n-#     x_tri = to_triton(x, device=device)\n-#     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n-#                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n-#     # numpy result\n-#     z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n-#     z_tri_dtype_str = z_dtype_str\n-#     if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n-#         z_dtype_str = 'float32'\n-#         z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n-#         # trunc mantissa for a fair comparison of accuracy\n-#         z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n-#         z_tri_dtype_str = 'bfloat16'\n-#     else:\n-#         z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n-#     # triton result\n-#     z_tri = to_triton(numpy_random((1,), dtype_str=z_dtype_str, rs=rs),\n-#                       device=device, dst_type=z_tri_dtype_str)\n-#     kernel[(1,)](x_tri, z_tri, BLOCK=shape)\n-#     z_tri = to_numpy(z_tri)\n-#     # compare\n-#     if op == 'sum':\n-#         np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n-#     else:\n-#         if op == 'argmin' or op == 'argmax':\n-#             # argmin and argmax can have multiple valid indices.\n-#             # so instead we compare the values pointed by indices\n-#             np.testing.assert_equal(x[z_ref], x[z_tri])\n-#         else:\n-#             np.testing.assert_equal(z_ref, z_tri)\n-\n-\n-# reduce_configs1 = [\n-#     (op, dtype, (1, 1024), axis) for dtype in dtypes_with_bfloat16\n-#     for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n-#     for axis in [1]\n-# ]\n-# reduce_configs2 = [\n-#     (op, 'float32', shape, axis)\n-#     for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n-#     for shape in [(2, 32), (4, 32), (4, 128), (32, 64), (64, 128), (128, 256), (32, 1024)]\n-#     for axis in [0, 1]\n-# ]\n-\n-\n-# @pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n-# def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n-#     # triton kernel\n-#     @triton.jit\n-#     def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n-#         range_m = tl.arange(0, BLOCK_M)\n-#         range_n = tl.arange(0, BLOCK_N)\n-#         x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n-#         z = GENERATE_TEST_HERE\n-#         if AXIS == 1:\n-#             tl.store(Z + range_m, z)\n-#         else:\n-#             tl.store(Z + range_n, z)\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=0)'})\n+    # input\n+    rs = RandomState(17)\n+    # limit the range of integers so that the sum does not overflow\n+    x = numpy_random((shape,), dtype_str=dtype_str, rs=rs)\n+    x_tri = to_triton(x, device=device)\n+    numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n+                'argmin': np.argmin, 'argmax': np.argmax}[op]\n+    # numpy result\n+    z_dtype_str = get_reduced_dtype(dtype_str, op)\n+    z_tri_dtype_str = z_dtype_str\n+    if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n+        z_dtype_str = 'float32'\n+        z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n+        # trunc mantissa for a fair comparison of accuracy\n+        z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n+        z_tri_dtype_str = 'bfloat16'\n+    else:\n+        z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n+    # triton result\n+    z_tri = to_triton(numpy_random((1,), dtype_str=z_dtype_str, rs=rs),\n+                      device=device, dst_type=z_tri_dtype_str)\n+    kernel[(1,)](x_tri, z_tri, BLOCK=shape)\n+    z_tri = to_numpy(z_tri)\n+    # compare\n+    if op == 'sum':\n+        np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n+    else:\n+        if op == 'argmin' or op == 'argmax':\n+            # argmin and argmax can have multiple valid indices.\n+            # so instead we compare the values pointed by indices\n+            np.testing.assert_equal(x[z_ref], x[z_tri])\n+        else:\n+            np.testing.assert_equal(z_ref, z_tri)\n \n-#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=AXIS)'})\n-#     # input\n-#     rs = RandomState(17)\n-#     # limit the range of integers so that the sum does not overflow\n-#     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n-#     x_tri = to_triton(x)\n-#     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n-#                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n-#     z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n-#     z_tri_dtype_str = z_dtype_str\n-#     # numpy result\n-#     if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n-#         z_dtype_str = 'float32'\n-#         z_tri_dtype_str = 'bfloat16'\n-#         z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n-#         # trunc mantissa for a fair comparison of accuracy\n-#         z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n-#     else:\n-#         z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n-#     # triton result\n-#     z_tri = to_triton(numpy_random((shape[1 - axis],), dtype_str=z_dtype_str, rs=rs),\n-#                       device=device, dst_type=z_tri_dtype_str)\n-#     kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n-#     z_tri = to_numpy(z_tri)\n-#     # compare\n-#     if op == 'sum':\n-#         np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n-#     else:\n-#         if op == 'argmin' or op == 'argmax':\n-#             # argmin and argmax can have multiple valid indices.\n-#             # so instead we compare the values pointed by indices\n-#             z_ref_index = np.expand_dims(z_ref, axis=axis)\n-#             z_tri_index = np.expand_dims(z_tri, axis=axis)\n-#             z_ref_value = np.take_along_axis(x, z_ref_index, axis=axis)\n-#             z_tri_value = np.take_along_axis(x, z_tri_index, axis=axis)\n-#             np.testing.assert_equal(z_ref_value, z_tri_value)\n-#         else:\n-#             np.testing.assert_equal(z_ref, z_tri)\n+\n+# TODO: [Qingyi] Fix argmin / argmax\n+reduce_configs1 = [\n+    (op, dtype, (1, 1024), axis) for dtype in dtypes_with_bfloat16\n+    for op in ['min', 'max', 'sum']\n+    for axis in [1]\n+]\n+reduce_configs2 = [\n+    (op, 'float32', shape, axis)\n+    for op in ['min', 'max', 'sum']\n+    for shape in [(2, 32), (4, 32), (4, 128), (32, 64), (64, 128), (128, 256), (32, 1024)]\n+    for axis in [0, 1]\n+]\n+\n+\n+@pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n+def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n+        range_m = tl.arange(0, BLOCK_M)\n+        range_n = tl.arange(0, BLOCK_N)\n+        x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n+        z = GENERATE_TEST_HERE\n+        if AXIS == 1:\n+            tl.store(Z + range_m, z)\n+        else:\n+            tl.store(Z + range_n, z)\n+\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=AXIS)'})\n+    # input\n+    rs = RandomState(17)\n+    # limit the range of integers so that the sum does not overflow\n+    x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n+    x_tri = to_triton(x)\n+    numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n+                'argmin': np.argmin, 'argmax': np.argmax}[op]\n+    z_dtype_str = get_reduced_dtype(dtype_str, op)\n+    z_tri_dtype_str = z_dtype_str\n+    # numpy result\n+    if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n+        z_dtype_str = 'float32'\n+        z_tri_dtype_str = 'bfloat16'\n+        z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+        # trunc mantissa for a fair comparison of accuracy\n+        z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n+    else:\n+        z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+    # triton result\n+    z_tri = to_triton(numpy_random((shape[1 - axis],), dtype_str=z_dtype_str, rs=rs),\n+                      device=device, dst_type=z_tri_dtype_str)\n+    kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n+    z_tri = to_numpy(z_tri)\n+    # compare\n+    if op == 'sum':\n+        np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n+    else:\n+        if op == 'argmin' or op == 'argmax':\n+            # argmin and argmax can have multiple valid indices.\n+            # so instead we compare the values pointed by indices\n+            z_ref_index = np.expand_dims(z_ref, axis=axis)\n+            z_tri_index = np.expand_dims(z_tri, axis=axis)\n+            z_ref_value = np.take_along_axis(x, z_ref_index, axis=axis)\n+            z_tri_value = np.take_along_axis(x, z_tri_index, axis=axis)\n+            np.testing.assert_equal(z_ref_value, z_tri_value)\n+        else:\n+            np.testing.assert_equal(z_ref, z_tri)\n \n # # ---------------\n # # test permute"}, {"filename": "python/tests/test_reduce.py", "status": "modified", "additions": 42, "deletions": 19, "changes": 61, "file_content_changes": "@@ -5,11 +5,20 @@\n import triton\n import triton.language as tl\n \n-dtype_mapping = {\n-    'float16': torch.float16,\n-    'float32': torch.float32,\n-    'float64': torch.float64,\n-}\n+int_dtypes = ['int8', 'int16', 'int32', 'int64']\n+uint_dtypes = ['uint8']  # PyTorch does not support uint16/uint32/uint64\n+float_dtypes = ['float16', 'float32', 'float64']\n+dtypes = int_dtypes + uint_dtypes + float_dtypes\n+dtypes_with_bfloat16 = int_dtypes + uint_dtypes + float_dtypes\n+dtype_mapping = {dtype_str: torch.__dict__[dtype_str] for dtype_str in dtypes}\n+\n+\n+def get_reduced_dtype(dtype):\n+    if dtype in [torch.int8, torch.int16, torch.uint8]:\n+        return torch.int32\n+    if dtype in [torch.bfloat16]:\n+        return torch.float32\n+    return dtype\n \n \n def patch_kernel(template, to_replace):\n@@ -40,33 +49,40 @@ def reduce2d_kernel(x_ptr, z_ptr, axis: tl.constexpr, block_m: tl.constexpr, blo\n reduce1d_configs = [\n     (op, dtype, shape)\n     for op in ['sum', 'min', 'max']\n-    for dtype in ['float16', 'float32', 'float64']\n+    for dtype in dtypes\n     for shape in [4, 8, 16, 32, 64, 128, 512, 1024]\n ]\n \n \n @pytest.mark.parametrize('op, dtype, shape', reduce1d_configs)\n def test_reduce1d(op, dtype, shape):\n     dtype = dtype_mapping[dtype]\n-    x = torch.randn((shape,), device='cuda', dtype=dtype)\n+    reduced_dtype = get_reduced_dtype(dtype)\n+\n+    if dtype.is_floating_point:\n+        x = torch.randn((shape,), device='cuda', dtype=dtype)\n+    elif dtype is torch.uint8:\n+        x = torch.randint(0, 20, (shape,), device='cuda', dtype=dtype)\n+    else:\n+        x = torch.randint(-20, 20, (shape,), device='cuda', dtype=dtype)\n     z = torch.empty(\n         tuple(),\n         device=x.device,\n-        dtype=dtype,\n+        dtype=reduced_dtype,\n     )\n \n     kernel = patch_kernel(reduce1d_kernel, {'OP': op})\n     grid = (1,)\n     kernel[grid](x_ptr=x, z_ptr=z, block=shape)\n \n     if op == 'sum':\n-        golden_z = torch.sum(x, dtype=dtype)\n+        golden_z = torch.sum(x, dtype=reduced_dtype)\n     elif op == 'min':\n-        golden_z = torch.min(x)\n+        golden_z = torch.min(x).to(reduced_dtype)\n     else:\n-        golden_z = torch.max(x)\n+        golden_z = torch.max(x).to(reduced_dtype)\n \n-    if op == 'sum':\n+    if dtype.is_floating_point and op == 'sum':\n         if shape >= 256:\n             assert_close(z, golden_z, rtol=0.05, atol=0.1)\n         elif shape >= 32:\n@@ -80,7 +96,7 @@ def test_reduce1d(op, dtype, shape):\n reduce2d_configs = [\n     (op, dtype, shape, axis)\n     for op in ['sum', 'min', 'max']\n-    for dtype in ['float16', 'float32', 'float64']\n+    for dtype in dtypes\n     for shape in [(1, 4), (1, 8), (1, 16), (1, 32), (2, 32), (4, 32), (4, 128), (32, 64)]\n     for axis in [0, 1]\n ]\n@@ -89,22 +105,29 @@ def test_reduce1d(op, dtype, shape):\n @pytest.mark.parametrize('op, dtype, shape, axis', reduce2d_configs)\n def test_reduce2d(op, dtype, shape, axis):\n     dtype = dtype_mapping[dtype]\n-    x = torch.randn(shape, device='cuda', dtype=dtype)\n+    reduced_dtype = get_reduced_dtype(dtype)\n     reduced_shape = (shape[1 - axis],)\n-    z = torch.empty(reduced_shape, device=x.device, dtype=dtype)\n+\n+    if dtype.is_floating_point:\n+        x = torch.randn(shape, device='cuda', dtype=dtype)\n+    elif dtype is torch.uint8:\n+        x = torch.randint(0, 20, shape, device='cuda', dtype=dtype)\n+    else:\n+        x = torch.randint(-20, 20, shape, device='cuda', dtype=dtype)\n+    z = torch.empty(reduced_shape, device=x.device, dtype=reduced_dtype)\n \n     kernel = patch_kernel(reduce2d_kernel, {'OP': op})\n     grid = (1,)\n     kernel[grid](x_ptr=x, z_ptr=z, axis=axis, block_m=shape[0], block_n=shape[1])\n \n     if op == 'sum':\n-        golden_z = torch.sum(x, dim=axis, keepdim=False, dtype=dtype)\n+        golden_z = torch.sum(x, dim=axis, keepdim=False, dtype=reduced_dtype)\n     elif op == 'min':\n-        golden_z = torch.min(x, dim=axis, keepdim=False)[0]\n+        golden_z = torch.min(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n     else:\n-        golden_z = torch.max(x, dim=axis, keepdim=False)[0]\n+        golden_z = torch.max(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n \n-    if op == 'sum':\n+    if dtype.is_floating_point and op == 'sum':\n         if shape[axis] >= 256:\n             assert_close(z, golden_z, rtol=0.05, atol=0.1)\n         elif shape[axis] >= 32:"}]