[{"filename": "CMakeLists.txt", "status": "modified", "additions": 32, "deletions": 31, "changes": 63, "file_content_changes": "@@ -188,8 +188,6 @@ add_subdirectory(include)\n add_subdirectory(lib)\n add_subdirectory(bin)\n \n-add_library(triton SHARED ${PYTHON_SRC})\n-\n # find_package(PythonLibs REQUIRED)\n \n set(TRITON_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}\")\n@@ -198,37 +196,40 @@ set(TRITON_BINARY_DIR \"${CMAKE_CURRENT_BINARY_DIR}\")\n get_property(dialect_libs GLOBAL PROPERTY MLIR_DIALECT_LIBS)\n get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)\n \n-target_link_libraries(triton\n-  TritonAnalysis\n-  TritonTransforms\n-  TritonGPUTransforms\n-  TritonLLVMIR\n-  TritonPTX\n-  ${dialect_libs}\n-  ${conversion_libs}\n-  # optimizations\n-  MLIRPass\n-  MLIRTransforms\n-  MLIRLLVMIR\n-  MLIRSupport\n-  MLIRTargetLLVMIRExport\n-  MLIRExecutionEngine\n-  MLIRMathToLLVM\n-  MLIRNVVMToLLVMIRTranslation\n-  MLIRIR\n-)\n-\n-target_link_options(triton PRIVATE ${LLVM_LDFLAGS})\n-\n-if(WIN32)\n-    target_link_libraries(triton PRIVATE ${LLVM_LIBRARIES} dl) # dl is from dlfcn-win32\n-elseif(APPLE)\n-    target_link_libraries(triton ${LLVM_LIBRARIES} z)\n-else()\n-    target_link_libraries(triton ${LLVM_LIBRARIES} z stdc++fs)\n+if(TRITON_BUILD_PYTHON_MODULE)\n+  add_library(triton SHARED ${PYTHON_SRC})\n+\n+  target_link_libraries(triton\n+    TritonAnalysis\n+    TritonTransforms\n+    TritonGPUTransforms\n+    TritonLLVMIR\n+    TritonPTX\n+    ${dialect_libs}\n+    ${conversion_libs}\n+    # optimizations\n+    MLIRPass\n+    MLIRTransforms\n+    MLIRLLVMIR\n+    MLIRSupport\n+    MLIRTargetLLVMIRExport\n+    MLIRExecutionEngine\n+    MLIRMathToLLVM\n+    MLIRNVVMToLLVMIRTranslation\n+    MLIRIR\n+  )\n+\n+  target_link_options(triton PRIVATE ${LLVM_LDFLAGS})\n+\n+  if(WIN32)\n+      target_link_libraries(triton PRIVATE ${LLVM_LIBRARIES} dl) # dl is from dlfcn-win32\n+  elseif(APPLE)\n+      target_link_libraries(triton ${LLVM_LIBRARIES} z)\n+  else()\n+      target_link_libraries(triton ${LLVM_LIBRARIES} z stdc++fs)\n+  endif()\n endif()\n \n-\n if(TRITON_BUILD_PYTHON_MODULE AND NOT WIN32)\n     set(CMAKE_SHARED_LIBRARY_SUFFIX \".so\")\n     # Check if the platform is MacOS"}, {"filename": "bin/CMakeLists.txt", "status": "modified", "additions": 33, "deletions": 32, "changes": 65, "file_content_changes": "@@ -26,35 +26,36 @@ target_link_libraries(triton-opt PRIVATE\n mlir_check_all_link_libraries(triton-opt)\n \n \n-# add_llvm_executable(triton-translate triton-translate.cpp PARTIAL_SOURCES_INTENDED)\n-#llvm_update_compile_flags(triton-translate)\n-# target_link_libraries(triton-translate PRIVATE\n-#         TritonAnalysis\n-#         TritonTransforms\n-#         TritonGPUTransforms\n-#         TritonLLVMIR\n-#         TritonDriver\n-#         ${dialect_libs}\n-#         ${conversion_libs}\n-#         # tests\n-#         TritonTestAnalysis\n-\n-#         LLVMCore\n-#         LLVMSupport\n-#         LLVMOption\n-#         LLVMCodeGen\n-#         LLVMAsmParser\n-\n-#         # MLIR core\n-#         MLIROptLib\n-#         MLIRIR\n-#         MLIRPass\n-#         MLIRSupport\n-#         MLIRTransforms\n-#         MLIRExecutionEngine\n-#         MLIRMathToLLVM\n-#         MLIRTransformUtils\n-#         MLIRLLVMToLLVMIRTranslation\n-#         MLIRNVVMToLLVMIRTranslation\n-#         )\n-# mlir_check_all_link_libraries(triton-translate)\n+add_llvm_executable(triton-translate triton-translate.cpp PARTIAL_SOURCES_INTENDED)\n+llvm_update_compile_flags(triton-translate)\n+ target_link_libraries(triton-translate PRIVATE\n+         TritonAnalysis\n+         TritonTransforms\n+         TritonGPUTransforms\n+         TritonLLVMIR\n+         TritonPTX\n+         ${dialect_libs}\n+         ${conversion_libs}\n+         # tests\n+         TritonTestAnalysis\n+\n+         LLVMCore\n+         LLVMSupport\n+         LLVMOption\n+         LLVMCodeGen\n+         LLVMAsmParser\n+\n+         # MLIR core\n+         MLIROptLib\n+         MLIRIR\n+         MLIRLLVMIR\n+         MLIRPass\n+         MLIRSupport\n+         MLIRTransforms\n+         MLIRExecutionEngine\n+         MLIRMathToLLVM\n+         MLIRTransformUtils\n+         MLIRLLVMToLLVMIRTranslation\n+         MLIRNVVMToLLVMIRTranslation\n+         )\n+mlir_check_all_link_libraries(triton-translate)"}, {"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -15,7 +15,7 @@\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n-#include \"triton/driver/llvm.h\"\n+#include \"triton/Target/PTX/PTXTranslation.h\"\n #include \"llvm/IR/LLVMContext.h\"\n #include \"llvm/Support/CommandLine.h\"\n #include \"llvm/Support/InitLLVM.h\"\n@@ -116,8 +116,8 @@ LogicalResult tritonTranslateMain(int argc, char **argv,\n   if (targetKind == \"llvmir\")\n     llvm::outs() << *llvmir << '\\n';\n   else if (targetKind == \"ptx\")\n-    llvm::outs() << ::triton::driver::llir_to_ptx(\n-        llvmir.get(), SMArch.getValue(), ptxVersion.getValue());\n+    llvm::outs() << ::triton::translateLLVMIRToPTX(*llvmir, SMArch.getValue(),\n+                                                   ptxVersion.getValue());\n \n   return success();\n }"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -161,9 +161,8 @@ struct PTXBuilder {\n \n   std::string dump() const;\n \n-  mlir::Value launch(ConversionPatternRewriter &rewriter, Location loc,\n-                     Type resTy, bool hasSideEffect = true,\n-                     bool isAlignStack = false,\n+  mlir::Value launch(OpBuilder &rewriter, Location loc, Type resTy,\n+                     bool hasSideEffect = true, bool isAlignStack = false,\n                      ArrayRef<Attribute> attrs = {}) const;\n \n private:"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "file_content_changes": "@@ -40,6 +40,19 @@ def TTG_AsyncWaitOp : TTG_Op<\"async_wait\"> {\n   }];\n }\n \n+def TTG_AsyncCommitGroupOp : TTG_Op<\"async_commit_group\"> {\n+  let summary = \"async commit group\";\n+\n+  let assemblyFormat = \"attr-dict\";\n+\n+  let extraClassDeclaration = [{\n+    static bool isSupported(int computeCapability) {\n+      return computeCapability >= 80;\n+    }\n+  }];\n+}\n+\n+\n // Port Arith_CmpIOp & Arith_CmpFOp & Std_SelectOp to TritonGPU.\n // This is needed because these ops don't\n // handle encodings"}, {"filename": "lib/Analysis/CMakeLists.txt", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -6,5 +6,6 @@ add_mlir_library(TritonAnalysis\n   Utility.cpp\n \n   DEPENDS\n+  TritonTableGen\n   TritonGPUAttrDefsIncGen\n-)\n\\ No newline at end of file\n+)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 46, "deletions": 51, "changes": 97, "file_content_changes": "@@ -100,6 +100,13 @@ struct DotOpMmaV1ConversionHelper {\n \n   static ArrayRef<unsigned> getMmaInstrShape() { return instrShape; }\n \n+  static Type getMatType(TensorType operand) {\n+    auto *ctx = operand.getContext();\n+    Type fp16Ty = type::f16Ty(ctx);\n+    Type vecTy = vec_ty(fp16Ty, 2);\n+    return struct_ty(SmallVector<Type>{vecTy});\n+  }\n+\n   static Type getMmaRetType(TensorType operand) {\n     auto *ctx = operand.getContext();\n     Type fp32Ty = type::f32Ty(ctx);\n@@ -708,17 +715,17 @@ struct DotOpMmaV2ConversionHelper {\n \n   // The type of matrix that loaded by either a ldmatrix or composed lds.\n   Type getMatType() const {\n-    Type fp32Ty = type::f32Ty(ctx);\n+    // floating point types\n+    Type fp32x1Ty = vec_ty(type::f32Ty(ctx), 1);\n     Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n     Type i16x2Ty = vec_ty(type::i16Ty(ctx), 2);\n-    // floating point types\n     Type fp16x2Pack4Ty =\n         LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n     // LLVM 14.0 does not support bf16 type, so we use i16 instead.\n     Type bf16x2Pack4Ty =\n         LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i16x2Ty));\n     Type fp32Pack4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n+        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32x1Ty));\n     // integer types\n     Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n     Type i8x4Pack4Ty =\n@@ -1130,7 +1137,7 @@ class MMA16816SmemLoader {\n   // Load 4 matrices and returns 4 vec<2> elements.\n   std::tuple<Value, Value, Value, Value>\n   loadX4(int mat0, int mat1, ArrayRef<Value> offs, ArrayRef<Value> ptrs,\n-         Type ldmatrixRetTy, Type shemPtrTy) const {\n+         Type matTy, Type shemPtrTy) const {\n     assert(mat0 % 2 == 0 && mat1 % 2 == 0 &&\n            \"smem matrix load must be aligned\");\n     int matIdx[2] = {mat0, mat1};\n@@ -1153,6 +1160,9 @@ class MMA16816SmemLoader {\n \n     Value ptr = getPtr(ptrIdx);\n \n+    // The struct should have exactly the same element types.\n+    Type elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n+\n     if (canUseLdmatrix) {\n       Value sOffset =\n           mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sStride);\n@@ -1170,20 +1180,12 @@ class MMA16816SmemLoader {\n       ldmatrix(resArgs, addrArg);\n \n       // The result type is 4xi32, each i32 is composed of 2xf16\n-      // elements(adjacent two columns in a row)\n-      Value resV4 = builder.launch(rewriter, loc, ldmatrixRetTy);\n-\n-      auto getIntAttr = [&](int v) {\n-        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n-      };\n-\n-      // The struct should have exactly the same element types.\n-      Type elemType = resV4.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n-\n-      return {extract_val(elemType, resV4, getIntAttr(0)),\n-              extract_val(elemType, resV4, getIntAttr(1)),\n-              extract_val(elemType, resV4, getIntAttr(2)),\n-              extract_val(elemType, resV4, getIntAttr(3))};\n+      // elements (adjacent two columns in a row) or a single f32 element.\n+      Value resV4 = builder.launch(rewriter, loc, matTy);\n+      return {extract_val(elemTy, resV4, i32_arr_attr(0)),\n+              extract_val(elemTy, resV4, i32_arr_attr(1)),\n+              extract_val(elemTy, resV4, i32_arr_attr(2)),\n+              extract_val(elemTy, resV4, i32_arr_attr(3))};\n     } else if (elemBytes == 4 &&\n                needTrans) { // Use lds.32 to load tf32 matrices\n       Value ptr2 = getPtr(ptrIdx + 1);\n@@ -1195,21 +1197,23 @@ class MMA16816SmemLoader {\n           add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n \n       Value elems[4];\n-      Type elemTy = type::f32Ty(ctx);\n-      Type elemPtrTy = ptr_ty(elemTy);\n       if (kOrder == 1) {\n-        elems[0] = load(gep(elemPtrTy, ptr, sOffsetElemVal));\n-        elems[1] = load(gep(elemPtrTy, ptr2, sOffsetElemVal));\n-        elems[2] = load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n-        elems[3] = load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n+        elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n+        elems[1] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n+        elems[2] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n+        elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n       } else {\n-        elems[0] = load(gep(elemPtrTy, ptr, sOffsetElemVal));\n-        elems[2] = load(gep(elemPtrTy, ptr2, sOffsetElemVal));\n-        elems[1] = load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n-        elems[3] = load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n+        elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n+        elems[2] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n+        elems[1] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n+        elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n       }\n-      return {elems[0], elems[1], elems[2], elems[3]};\n-\n+      std::array<Value, 4> retElems;\n+      retElems.fill(undef(elemTy));\n+      for (auto i = 0; i < 4; ++i) {\n+        retElems[i] = insert_element(elemTy, retElems[i], elems[i], i32_val(0));\n+      }\n+      return {retElems[0], retElems[1], retElems[2], retElems[3]};\n     } else if (elemBytes == 1 && needTrans) { // work with int8\n       std::array<std::array<Value, 4>, 2> ptrs;\n       ptrs[0] = {\n@@ -1234,49 +1238,42 @@ class MMA16816SmemLoader {\n           add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n \n       std::array<Value, 4> i8v4Elems;\n-      std::array<Value, 4> i32Elems;\n-      i8v4Elems.fill(\n-          rewriter.create<LLVM::UndefOp>(loc, vec_ty(type::i8Ty(ctx), 4)));\n+      i8v4Elems.fill(undef(elemTy));\n \n       Value i8Elems[4][4];\n-      Type elemTy = type::i8Ty(ctx);\n-      Type elemPtrTy = ptr_ty(elemTy);\n-      Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n       if (kOrder == 1) {\n         for (int i = 0; i < 2; ++i)\n           for (int j = 0; j < 4; ++j)\n-            i8Elems[i][j] = load(gep(elemPtrTy, ptrs[i][j], sOffsetElemVal));\n+            i8Elems[i][j] = load(gep(shemPtrTy, ptrs[i][j], sOffsetElemVal));\n \n         for (int i = 2; i < 4; ++i)\n           for (int j = 0; j < 4; ++j)\n             i8Elems[i][j] =\n-                load(gep(elemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n+                load(gep(shemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n \n         for (int m = 0; m < 4; ++m) {\n           for (int e = 0; e < 4; ++e)\n             i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n                                           i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bitcast(i8v4Elems[m], i8x4Ty);\n         }\n       } else { // k first\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[0][j] = load(gep(elemPtrTy, ptrs[0][j], sOffsetElemVal));\n+          i8Elems[0][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[2][j] = load(gep(elemPtrTy, ptrs[1][j], sOffsetElemVal));\n+          i8Elems[2][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[1][j] = load(gep(elemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n+          i8Elems[1][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[3][j] = load(gep(elemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n+          i8Elems[3][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n \n         for (int m = 0; m < 4; ++m) {\n           for (int e = 0; e < 4; ++e)\n             i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n                                           i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bitcast(i8v4Elems[m], i8x4Ty);\n         }\n       }\n \n-      return {i32Elems[0], i32Elems[1], i32Elems[2], i32Elems[3]};\n+      return {i8v4Elems[0], i8v4Elems[1], i8v4Elems[2], i8v4Elems[3]};\n     }\n \n     assert(false && \"Invalid smem load\");\n@@ -1564,7 +1561,9 @@ struct MMA16816ConversionHelper {\n       unsigned colsPerThread = numRepN * 2;\n       PTXBuilder builder;\n       auto &mma = *builder.create(helper.getMmaInstr().str());\n-      auto retArgs = builder.newListOperand(4, \"=r\");\n+      // using =r for float32 works but leads to less readable ptx.\n+      bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n+      auto retArgs = builder.newListOperand(4, isIntMMA ? \"=r\" : \"=f\");\n       auto aArgs = builder.newListOperand({\n           {ha[{m, k}], \"r\"},\n           {ha[{m + 1, k}], \"r\"},\n@@ -1583,14 +1582,10 @@ struct MMA16816ConversionHelper {\n       mma(retArgs, aArgs, bArgs, cArgs);\n       Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n \n-      auto getIntAttr = [&](int v) {\n-        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n-      };\n-\n       Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n       for (int i = 0; i < 4; ++i)\n         fc[m * colsPerThread + 4 * n + i] =\n-            extract_val(elemTy, mmaOut, getIntAttr(i));\n+            extract_val(elemTy, mmaOut, i32_arr_attr(i));\n     };\n \n     for (int k = 0; k < numRepK; ++k)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 5, "deletions": 12, "changes": 17, "file_content_changes": "@@ -176,7 +176,9 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n       PTXBuilder builder;\n       auto idx = getIdx(m, n);\n \n-      auto *resOprs = builder.newListOperand(8, \"=f\");\n+      // note: using \"=f\" for float leads to cleaner PTX\n+      bool isIntMMA = DTensorTy.getElementType().isInteger(32);\n+      auto *resOprs = builder.newListOperand(8, isIntMMA ? \"=r\" : \"=f\");\n       auto *AOprs = builder.newListOperand({\n           {ha.first, \"r\"},\n           {ha.second, \"r\"},\n@@ -200,17 +202,8 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n       Value res =\n           builder.launch(rewriter, loc, helper.getMmaRetType(ATensorTy));\n \n-      auto getIntAttr = [&](int v) {\n-        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n-      };\n-\n-      SmallVector<Value> C; // DEBUG\n-      for (int i = 0; i < 8; i++) {\n-        C.push_back(acc[idx[i]]);\n-      }\n-\n-      for (unsigned i = 0; i < 8; i++) {\n-        Value elem = extract_val(f32_ty, res, getIntAttr(i));\n+      for (auto i = 0; i < 8; i++) {\n+        Value elem = extract_val(f32_ty, res, i32_arr_attr(i));\n         acc[idx[i]] = elem;\n       }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 4, "deletions": 8, "changes": 12, "file_content_changes": "@@ -49,10 +49,8 @@ struct FpToFpOpConversion\n         struct_ty(SmallVector<Type>{fp16x2VecTy, fp16x2VecTy});\n     auto fp16x2x2Struct =\n         builder.launch(rewriter, loc, fp16x2x2StructTy, false);\n-    auto fp16x2Vec0 =\n-        extract_val(fp16x2VecTy, fp16x2x2Struct, rewriter.getI32ArrayAttr({0}));\n-    auto fp16x2Vec1 =\n-        extract_val(fp16x2VecTy, fp16x2x2Struct, rewriter.getI32ArrayAttr({1}));\n+    auto fp16x2Vec0 = extract_val(fp16x2VecTy, fp16x2x2Struct, i32_arr_attr(0));\n+    auto fp16x2Vec1 = extract_val(fp16x2VecTy, fp16x2x2Struct, i32_arr_attr(1));\n     return {extract_element(f16_ty, fp16x2Vec0, i32_val(0)),\n             extract_element(f16_ty, fp16x2Vec0, i32_val(1)),\n             extract_element(f16_ty, fp16x2Vec1, i32_val(0)),\n@@ -143,10 +141,8 @@ struct FpToFpOpConversion\n         struct_ty(SmallVector<Type>{bf16x2VecTy, bf16x2VecTy});\n     auto bf16x2x2Struct =\n         builder.launch(rewriter, loc, bf16x2x2StructTy, false);\n-    auto bf16x2Vec0 =\n-        extract_val(bf16x2VecTy, bf16x2x2Struct, rewriter.getI32ArrayAttr({0}));\n-    auto bf16x2Vec1 =\n-        extract_val(bf16x2VecTy, bf16x2x2Struct, rewriter.getI32ArrayAttr({1}));\n+    auto bf16x2Vec0 = extract_val(bf16x2VecTy, bf16x2x2Struct, i32_arr_attr(0));\n+    auto bf16x2Vec1 = extract_val(bf16x2VecTy, bf16x2x2Struct, i32_arr_attr(1));\n     return {extract_element(i16_ty, bf16x2Vec0, i32_val(0)),\n             extract_element(i16_ty, bf16x2Vec0, i32_val(1)),\n             extract_element(i16_ty, bf16x2Vec1, i32_val(0)),"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "file_content_changes": "@@ -214,7 +214,7 @@ struct LoadOpConversion\n         Value curr;\n         if (retTy.isa<LLVM::LLVMStructType>()) {\n           curr = extract_val(IntegerType::get(getContext(), width), ret,\n-                             rewriter.getI64ArrayAttr(ii));\n+                             i64_arr_attr(ii));\n         } else {\n           curr = ret;\n         }\n@@ -802,9 +802,6 @@ struct InsertSliceAsyncOpConversion\n       }\n     }\n \n-    PTXBuilder ptxBuilder;\n-    ptxBuilder.create<>(\"cp.async.commit_group\")->operator()();\n-    ptxBuilder.launch(rewriter, loc, void_ty(getContext()));\n     rewriter.replaceOp(op, llDst);\n     return success();\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/PTXAsmFormat.cpp", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -78,9 +78,8 @@ SmallVector<PTXBuilder::Operand *, 4> PTXBuilder::getAllArgs() const {\n   return res;\n }\n \n-mlir::Value PTXBuilder::launch(ConversionPatternRewriter &rewriter,\n-                               Location loc, Type resTy, bool hasSideEffect,\n-                               bool isAlignStack,\n+mlir::Value PTXBuilder::launch(OpBuilder &rewriter, Location loc, Type resTy,\n+                               bool hasSideEffect, bool isAlignStack,\n                                ArrayRef<Attribute> attrs) const {\n   auto *ctx = rewriter.getContext();\n   auto inlineAsm = rewriter.create<LLVM::InlineAsmOp>("}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -564,6 +564,24 @@ struct AsyncWaitOpConversion\n   }\n };\n \n+struct AsyncCommitGroupOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::AsyncCommitGroupOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::gpu::AsyncCommitGroupOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::gpu::AsyncCommitGroupOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+\n+    PTXBuilder ptxBuilder;\n+    ptxBuilder.create<>(\"cp.async.commit_group\")->operator()();\n+    ptxBuilder.launch(rewriter, op.getLoc(), void_ty(op.getContext()));\n+    // Safe to remove the op since it doesn't have any return value.\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+};\n+\n namespace mlir {\n namespace LLVM {\n \n@@ -598,6 +616,7 @@ void populateTritonGPUToLLVMPatterns(\n   patterns.add<AddPtrOpConversion>(typeConverter, benefit);\n   patterns.add<AllocTensorOpConversion>(typeConverter, allocation, smem,\n                                         benefit);\n+  patterns.add<AsyncCommitGroupOpConversion>(typeConverter, benefit);\n   patterns.add<AsyncWaitOpConversion>(typeConverter, benefit);\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -388,6 +388,11 @@ class ConvertTritonGPUToLLVM\n       decomposed = true;\n     });\n \n+    mod.walk([&](triton::gpu::AsyncCommitGroupOp asyncCommitGroupOp) -> void {\n+      if (!triton::gpu::AsyncCommitGroupOp::isSupported(computeCapability))\n+        asyncCommitGroupOp.erase();\n+    });\n+\n     mod.walk([&](triton::gpu::AsyncWaitOp asyncWaitOp) -> void {\n       if (!triton::gpu::AsyncWaitOp::isSupported(computeCapability)) {\n         // async wait is supported in Ampere and later"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.h", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -90,7 +90,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n         Type elemTy = convertType(type.getElementType());\n         if (mmaLayout.isAmpere()) {\n           const llvm::DenseMap<int, Type> targetTyMap = {\n-              {32, elemTy},\n+              {32, vec_ty(elemTy, 1)},\n               {16, vec_ty(elemTy, 2)},\n               {8, vec_ty(elemTy, 4)},\n           };\n@@ -103,8 +103,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n           if (dotOpLayout.getOpIdx() == 0) { // $a\n             auto elems =\n                 MMA16816ConversionHelper::getANumElemsPerThread(type, wpt[0]);\n-            return LLVM::LLVMStructType::getLiteral(\n-                ctx, SmallVector<Type>(elems, targetTy));\n+            return struct_ty(SmallVector<Type>(elems, targetTy));\n           }\n           if (dotOpLayout.getOpIdx() == 1) { // $b\n             auto elems ="}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -95,6 +95,10 @@\n                             __VA_ARGS__)\n #define tid_val() getThreadId(rewriter, loc)\n \n+// Attributes\n+#define i32_arr_attr(...) rewriter.getI32ArrayAttr({__VA_ARGS__})\n+#define i64_arr_attr(...) rewriter.getI64ArrayAttr({__VA_ARGS__})\n+\n namespace mlir {\n namespace triton {\n \n@@ -191,7 +195,7 @@ getElementsFromStruct(Location loc, Value llvmStruct,\n   SmallVector<Value> results(types.size());\n   for (unsigned i = 0; i < types.size(); ++i) {\n     Type type = types[i];\n-    results[i] = extract_val(type, llvmStruct, rewriter.getI64ArrayAttr(i));\n+    results[i] = extract_val(type, llvmStruct, i64_arr_attr(i));\n   }\n   return results;\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 9, "deletions": 2, "changes": 11, "file_content_changes": "@@ -64,14 +64,20 @@ struct SplatOpConversion\n     auto dotOperand =\n         tensorTy.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>();\n     auto parent = layout.getParent();\n+    Value retVal = constVal;\n+    Type retTy = elemType;\n     int numElems{};\n     if (auto mmaLayout = parent.dyn_cast<MmaEncodingAttr>()) {\n+      Type matTy;\n       if (mmaLayout.isAmpere()) {\n         numElems = layout.getOpIdx() == 0\n                        ? MMA16816ConversionHelper::getANumElemsPerThread(\n                              tensorTy, mmaLayout.getWarpsPerCTA()[0])\n                        : MMA16816ConversionHelper::getBNumElemsPerThread(\n                              tensorTy, mmaLayout.getWarpsPerCTA()[1]);\n+        DotOpMmaV2ConversionHelper helper(mmaLayout);\n+        helper.deduceMmaType(tensorTy);\n+        matTy = helper.getMatType();\n       } else if (mmaLayout.isVolta()) {\n         DotOpMmaV1ConversionHelper helper(mmaLayout);\n         bool isRow = layout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n@@ -92,9 +98,10 @@ struct SplatOpConversion\n     } else {\n       assert(false && \"Unsupported layout found\");\n     }\n+\n     auto structTy = LLVM::LLVMStructType::getLiteral(\n-        rewriter.getContext(), SmallVector<Type>(numElems, elemType));\n-    return getStructFromElements(loc, SmallVector<Value>(numElems, constVal),\n+        rewriter.getContext(), SmallVector<Type>(numElems, retTy));\n+    return getStructFromElements(loc, SmallVector<Value>(numElems, retVal),\n                                  rewriter, structTy);\n   }\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 43, "deletions": 17, "changes": 60, "file_content_changes": "@@ -302,6 +302,7 @@ void LoopPipeliner::emitPrologue() {\n               loadStageBuffer[loadOp][stage], pipelineIterIdx, newMask,\n               lookupOrDefault(loadOp.other(), stage), loadOp.cache(),\n               loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n+          builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n           loadStageBuffer[loadOp].push_back(newOp->getResult(0));\n         } else\n           llvm_unreachable(\"This should be LoadOp\");\n@@ -319,6 +320,9 @@ void LoopPipeliner::emitPrologue() {\n       }\n \n       // Update mapping of results\n+      // if (stage == numStages - 2)\n+      //   continue;\n+\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n         Value originalResult = op->getResult(dstIdx);\n         // copy_async will update the value of its only use\n@@ -383,7 +387,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   //   (original args)\n   //   (insertSliceAsync buffer at stage numStages - 1) for each load\n   //   (extracted tensor) for each load\n-  //   (depArgs at stage numStages - 1)\n+  //   (depArgs at stage numStages - 2)\n   //   (iv at stage numStages - 2)\n   //   (pipeline iteration index)\n   //   (loop iteration index)\n@@ -404,7 +408,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   size_t depArgsBeginIdx = newLoopArgs.size();\n   for (BlockArgument depArg : depArgs) {\n     depArgsIdx[depArg] = newLoopArgs.size();\n-    newLoopArgs.push_back(valueMapping[depArg][numStages - 1]);\n+    newLoopArgs.push_back(valueMapping[depArg][numStages - 2]);\n   }\n \n   size_t nextIVIdx = newLoopArgs.size();\n@@ -463,10 +467,12 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   DenseMap<BlockArgument, Value> depArgsMapping;\n   size_t argIdx = 0;\n   for (BlockArgument arg : depArgs) {\n-    nextMapping.map(arg,\n-                    newForOp.getRegionIterArgs()[argIdx + depArgsBeginIdx]);\n+    BlockArgument nextArg =\n+        newForOp.getRegionIterArgs()[argIdx + depArgsBeginIdx];\n+    nextMapping.map(arg, nextArg);\n     ++argIdx;\n   }\n+\n   // Special handling for iv & loop condition\n   Value nextIV = builder.create<arith::AddIOp>(\n       newForOp.getInductionVar().getLoc(),\n@@ -491,6 +497,25 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   extractSliceIndex = builder.create<arith::IndexCastOp>(\n       extractSliceIndex.getLoc(), builder.getIndexType(), extractSliceIndex);\n \n+  for (Operation *op : orderedDeps)\n+    if (!loads.contains(op->getResult(0))) {\n+      Operation *nextOp = builder.clone(*op, nextMapping);\n+\n+      auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n+      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n+        for (OpOperand &operand : originYield->getOpOperands()) {\n+          if (operand.get() == op->getResult(dstIdx)) {\n+            size_t originIdx = operand.getOperandNumber();\n+            size_t newArgIdx = depArgsIdx[forOp.getRegionIterArgs()[originIdx]];\n+            BlockArgument newArg = newForOp.getRegionIterArgs()[newArgIdx];\n+            nextMapping.map(forOp.getRegionIterArgs()[originIdx],\n+                            nextOp->getResult(dstIdx));\n+            depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n+          }\n+        }\n+      }\n+    }\n+\n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n     // Update loading mask\n@@ -518,6 +543,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n           insertSliceIndex, newMask,\n           nextMapping.lookupOrDefault(loadOp.other()), loadOp.cache(),\n           loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n+      builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n       nextBuffers.push_back(insertAsyncOp);\n       auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n       sliceType = RankedTensorType::get(sliceType.getShape(),\n@@ -532,19 +558,19 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n                                     int_attr(sliceType.getShape()[1])},\n           SmallVector<OpFoldResult>{int_attr(1), int_attr(1), int_attr(1)});\n       extractSlices.push_back(nextOp->getResult(0));\n-    } else\n-      nextOp = builder.clone(*op, nextMapping);\n-    // Update mapping of results\n-    for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n-      nextMapping.map(op->getResult(dstIdx), nextOp->getResult(dstIdx));\n-      // If this is a loop-carried value, update the mapping for yield\n-      auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n-      for (OpOperand &operand : originYield->getOpOperands()) {\n-        if (operand.get() == op->getResult(dstIdx)) {\n-          size_t originIdx = operand.getOperandNumber();\n-          size_t newArgIdx = depArgsIdx[forOp.getRegionIterArgs()[originIdx]];\n-          BlockArgument newArg = newForOp.getRegionIterArgs()[newArgIdx];\n-          depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n+\n+      // Update mapping of results\n+      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n+        nextMapping.map(op->getResult(dstIdx), nextOp->getResult(dstIdx));\n+        // If this is a loop-carried value, update the mapping for yield\n+        auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n+        for (OpOperand &operand : originYield->getOpOperands()) {\n+          if (operand.get() == op->getResult(dstIdx)) {\n+            size_t originIdx = operand.getOperandNumber();\n+            size_t newArgIdx = depArgsIdx[forOp.getRegionIterArgs()[originIdx]];\n+            BlockArgument newArg = newForOp.getRegionIterArgs()[newArgIdx];\n+            depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n+          }\n         }\n       }\n     }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -248,13 +248,17 @@ scf::ForOp Prefetcher::createNewForOp() {\n                      prefetchWidth;\n       Operation *prevDot = firstDot;\n       while (kRem != 0) {\n-        int64_t kShape = largestPow2(kRem);\n+        // int64_t kShape = largestPow2(kRem);\n+        int64_t kShape = prefetchWidth;\n+        auto insertionPoint = builder.saveInsertionPoint();\n+        builder.setInsertionPoint(prevDot);\n         Value aRem =\n             generatePrefetch(mapping.lookup(dot2aLoopArg[dot]), 0, false,\n                              dotEncoding, builder, kOff, kShape);\n         Value bRem =\n             generatePrefetch(mapping.lookup(dot2bLoopArg[dot]), 1, false,\n                              dotEncoding, builder, kOff, kShape);\n+        builder.restoreInsertionPoint(insertionPoint);\n         newOp = builder.clone(*dot, mapping);\n         newOp->setOperand(0, aRem);\n         newOp->setOperand(1, bRem);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/ReorderInstructions.cpp", "status": "modified", "additions": 10, "deletions": 2, "changes": 12, "file_content_changes": "@@ -53,6 +53,9 @@ class TritonGPUReorderInstructionsPass\n       auto user_end = op->user_end();\n       if (std::distance(user_begin, user_end) != 1)\n         return;\n+      if (user_begin->getParentOfType<scf::ForOp>() ==\n+          op->getParentOfType<scf::ForOp>())\n+        return;\n       opToMove.insert({op, *user_begin});\n     });\n     for (auto &kv : opToMove)\n@@ -89,8 +92,13 @@ class TritonGPUReorderInstructionsPass\n         return;\n       if (op->getUsers().empty())\n         return;\n-      auto user_begin = op->user_begin();\n-      op->moveBefore(*user_begin);\n+      auto dotUser = dyn_cast<triton::DotOp>(*op->user_begin());\n+      if (!dotUser)\n+        return;\n+      auto BOp = dotUser.getOperand(1).getDefiningOp();\n+      if (!BOp)\n+        return;\n+      op->moveBefore(BOp);\n     });\n     return;\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/UpdateMmaForVolta.cpp", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -217,14 +217,15 @@ class UpdateMMAForMMAv1 : public mlir::RewritePattern {\n \n     auto tensorTy = constant.getResult().getType().cast<RankedTensorType>();\n     auto mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n-    if ((!mma))\n+    auto dot = tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>();\n+    if (!mma && !dot)\n       return failure();\n \n     auto newTensorTy = getUpdatedType(tensorTy);\n     if (auto attr = constant.getValue().dyn_cast<SplatElementsAttr>()) {\n       auto newRet =\n           SplatElementsAttr::get(newTensorTy, attr.getSplatValue<Attribute>());\n-      rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newTensorTy, newRet);\n+      rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newRet);\n       return success();\n     }\n "}, {"filename": "lib/Target/LLVMIR/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -7,6 +7,7 @@ add_mlir_translation_library(TritonLLVMIR\n         LINK_LIBS PUBLIC\n         MLIRIR\n         MLIRLLVMIR\n+        MLIRSCFToStandard\n         MLIRSupport\n         MLIRTargetLLVMIRExport\n         )"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 23, "deletions": 3, "changes": 26, "file_content_changes": "@@ -468,6 +468,12 @@ void init_triton_ir(py::module &&m) {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n                  loc, v, self.getI1Type()));\n            })\n+      .def(\"get_int8\",\n+           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 loc, v, self.getI8Type()));\n+           })\n       .def(\"get_int32\",\n            [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n@@ -480,9 +486,23 @@ void init_triton_ir(py::module &&m) {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n                  loc, v, self.getI64Type()));\n            })\n-      // .def(\"get_uint32\", &ir::builder::get_int32, ret::reference)\n-      // .def(\"get_float16\", &ir::builder::get_float16, ret::reference)\n-      .def(\"get_float32\",\n+      // bfloat16 cannot be initialized as it is treated as int16 for now\n+      //.def(\"get_bf16\",\n+      //     [](mlir::OpBuilder &self, float v) -> mlir::Value {\n+      //       auto loc = self.getUnknownLoc();\n+      //       auto type = self.getBF16Type();\n+      //       return self.create<mlir::arith::ConstantFloatOp>(\n+      //           loc,\n+      //           mlir::APFloat(type.getFloatSemantics(), std::to_string(v)),\n+      //           type);\n+      //     })\n+      .def(\"get_fp16\",\n+           [](mlir::OpBuilder &self, float v) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<mlir::arith::ConstantOp>(\n+                 loc, self.getF16FloatAttr(v));\n+           })\n+      .def(\"get_fp32\",\n            [](mlir::OpBuilder &self, float v) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::ConstantOp>("}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 14, "deletions": 10, "changes": 24, "file_content_changes": "@@ -1235,19 +1235,23 @@ def kernel(X, stride_xm, stride_xk,\n         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n \n \n-def test_dot_without_load():\n+@pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n+def test_dot_without_load(dtype_str):\n     @triton.jit\n-    def kernel(out):\n-        pid = tl.program_id(axis=0)\n-        a = tl.zeros((32, 32), tl.float32)\n-        b = tl.zeros((32, 32), tl.float32)\n-        c = tl.zeros((32, 32), tl.float32)\n+    def _kernel(out):\n+        a = GENERATE_TEST_HERE\n+        b = GENERATE_TEST_HERE\n         c = tl.dot(a, b)\n-        pout = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n-        tl.store(pout, c)\n-\n-    out = torch.ones((32, 32), dtype=torch.float32, device=\"cuda\")\n+        out_ptr = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n+        tl.store(out_ptr, c)\n+\n+    kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.full((32, 32), 1.0, tl.{dtype_str})\"})\n+    a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+    b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+    out_ref = torch.matmul(a, b)\n+    out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n     kernel[(1,)](out)\n+    assert torch.all(out == out_ref)\n \n # ---------------\n # test arange"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -1488,9 +1488,9 @@ def compile(fn, **kwargs):\n         import re\n         match = re.search(prototype_pattern[ir], src, re.MULTILINE)\n         name, signature = match.group(1), match.group(2)\n-        print(name, signature)\n+        # print(name, signature)\n         types = re.findall(arg_type_pattern[ir], signature)\n-        print(types)\n+        # print(types)\n         param_tys = [convert_type_repr(ty) for ty in types]\n         signature = {k: v for k, v in enumerate(param_tys)}\n         first_stage = list(stages.keys()).index(ir)\n@@ -1581,6 +1581,7 @@ def _init_handles(self):\n         if self.shared > max_shared:\n             raise OutOfResources(self.shared, max_shared, \"shared memory\")\n         mod, func, n_regs, n_spills = cuda_utils.load_binary(self.metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n+        # print(self.shared, n_regs, n_spills)\n         self.cu_module = mod\n         self.cu_function = func\n \n@@ -1620,7 +1621,8 @@ def __new__(cls):\n             cls.instance = super(CudaUtils, cls).__new__(cls)\n         return cls.instance\n \n-    def _generate_src(self):\n+    @staticmethod\n+    def _generate_src():\n         return \"\"\"\n         #include <cuda.h>\n "}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -31,6 +31,7 @@\n     dot,\n     dtype,\n     exp,\n+    full,\n     fdiv,\n     float16,\n     float32,\n@@ -124,6 +125,7 @@\n     \"float32\",\n     \"float64\",\n     \"float8\",\n+    \"full\",\n     \"function_type\",\n     \"int1\",\n     \"int16\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 39, "deletions": 15, "changes": 54, "file_content_changes": "@@ -26,7 +26,7 @@ def _to_tensor(x, builder):\n         else:\n             raise RuntimeError(f'Nonrepresentable integer {x}.')\n     elif isinstance(x, float):\n-        return tensor(builder.get_float32(x), float32)\n+        return tensor(builder.get_fp32(x), float32)\n     elif isinstance(x, constexpr):\n         return _to_tensor(x.value, builder)\n     elif isinstance(x, tensor):\n@@ -139,13 +139,16 @@ def is_int(self):\n     def is_bool(self):\n         return self.is_int1()\n \n-    def is_void(self):\n+    @staticmethod\n+    def is_void():\n         raise RuntimeError(\"Not implemented\")\n \n-    def is_block(self):\n+    @staticmethod\n+    def is_block():\n         return False\n \n-    def is_ptr(self):\n+    @staticmethod\n+    def is_ptr():\n         return False\n \n     def __eq__(self, other: dtype):\n@@ -690,24 +693,31 @@ def arange(start, end, _builder=None):\n     return semantic.arange(start, end, _builder)\n \n \n+def _shape_check_impl(shape):\n+    shape = _constexpr_to_value(shape)\n+    for i, d in enumerate(shape):\n+        if not isinstance(d, constexpr):\n+            raise TypeError(f\"Shape element {i} must have type `constexpr`\")\n+        if not isinstance(d.value, int):\n+            raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n+    return [_constexpr_to_value(x) for x in shape]\n+\n+\n @builtin\n-def zeros(shape, dtype, _builder=None):\n+def full(shape, value, dtype, _builder=None):\n     \"\"\"\n-    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n+    Returns a tensor filled with the scalar value for the given :code:`shape` and :code:`dtype`.\n \n     :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n+    :value value: A scalar value to fill the array with\n     :type shape: tuple of ints\n     :param dtype: Data-type of the new array, e.g., :code:`tl.float16`\n     :type dtype: DType\n     \"\"\"\n-    for i, d in enumerate(shape):\n-        if not isinstance(d, constexpr):\n-            raise TypeError(f\"Shape element {i} must have type `constexpr`\")\n-        if not isinstance(d.value, int):\n-            raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n-    shape = [x.value for x in shape]\n+    shape = _shape_check_impl(shape)\n+    value = _constexpr_to_value(value)\n     dtype = _constexpr_to_value(dtype)\n-    return semantic.zeros(shape, dtype, _builder)\n+    return semantic.full(shape, value, dtype, _builder)\n \n \n # -----------------------\n@@ -738,6 +748,7 @@ def broadcast_to(input, shape, _builder=None):\n     :param shape: The desired shape.\n     :type shape: Tuple[int]\n     \"\"\"\n+    shape = _shape_check_impl(shape)\n     return semantic.broadcast_impl_shape(input, shape, _builder)\n \n \n@@ -775,14 +786,14 @@ def view(input, shape, _builder=None):\n     :type shape: Tuple[int]\n \n     \"\"\"\n-    shape = [x.value for x in shape]\n+    shape = _shape_check_impl(shape)\n     return semantic.view(input, shape, _builder)\n \n \n @builtin\n def reshape(input, shape, _builder=None):\n     # TODO: should be more than just a view\n-    shape = [x.value for x in shape]\n+    shape = _shape_check_impl(shape)\n     return semantic.view(input, shape, _builder)\n \n # -----------------------\n@@ -1243,6 +1254,19 @@ def swizzle2d(i, j, size_i, size_j, size_g):\n     return new_i, new_j\n \n \n+@triton.jit\n+def zeros(shape, dtype):\n+    \"\"\"\n+    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n+\n+    :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n+    :type shape: tuple of ints\n+    :param dtype: Data-type of the new array, e.g., :code:`tl.float16`\n+    :type dtype: DType\n+    \"\"\"\n+    return full(shape, 0, dtype)\n+\n+\n @triton.jit\n def zeros_like(input):\n     return zeros(input.shape, input.dtype)"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 13, "deletions": 8, "changes": 21, "file_content_changes": "@@ -471,10 +471,15 @@ def arange(start: int, end: int, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_make_range(start, end), ret_ty)\n \n \n-def zeros(shape: List[int], dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n-    _0 = builder.get_null_value(dtype.to_ir(builder))\n+def full(shape: List[int], value, dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n+    if value == 0:\n+        _value = builder.get_null_value(dtype.to_ir(builder))\n+    else:\n+        get_value_fn = getattr(builder, f\"get_{dtype.name}\")\n+        _value = get_value_fn(value)\n     ret_ty = tl.block_type(dtype, shape)\n-    return tl.tensor(builder.create_splat(_0, shape), ret_ty)\n+    return tl.tensor(builder.create_splat(_value, shape), ret_ty)\n+\n \n # ===----------------------------------------------------------------------===//\n #                               Shape Manipulation\n@@ -891,8 +896,8 @@ def atomic_max(ptr: tl.tensor,\n     # return atomic_umin(i_ptr, i_val) if val < 0\n     i_val = bitcast(val, tl.int32, builder)\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n-    pos = greater_equal(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n-    neg = less_than(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n+    pos = greater_equal(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n+    neg = less_than(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n     pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX, i_ptr.handle, i_val.handle, and_(mask, pos, builder).handle), i_val.type)\n     neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN, i_ptr.handle, i_val.handle, and_(mask, neg, builder).handle), i_val.type)\n     return where(pos, pos_ret, neg_ret, builder)\n@@ -923,8 +928,8 @@ def atomic_min(ptr: tl.tensor,\n     # return atomic_umax(i_ptr, i_val) if val < 0\n     i_val = bitcast(val, tl.int32, builder)\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n-    pos = greater_equal(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n-    neg = less_than(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n+    pos = greater_equal(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n+    neg = less_than(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n     pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MIN,\n                                                   i_ptr.handle,\n                                                   i_val.handle,\n@@ -998,7 +1003,7 @@ def dot(lhs: tl.tensor,\n         _0 = builder.get_int32(0)\n         ret_scalar_ty = tl.int32\n     else:\n-        _0 = builder.get_float32(0)\n+        _0 = builder.get_fp32(0)\n         ret_scalar_ty = tl.float32\n     M = lhs.type.shape[0]\n     N = rhs.type.shape[1]"}, {"filename": "python/triton/tools/aot.py", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -1,4 +1,5 @@\n import argparse\n+import sys\n \n import triton\n import triton._C.libtriton.triton as libtriton\n@@ -24,7 +25,7 @@\n     # check for validity of format arguments\n     if args.target not in VALID_FORMATS:\n         print(\"Invalid target format: \" + args.target)\n-        exit(0)\n+        sys.exit(0)\n \n     # parse source file to MLIR module\n     context = libtriton.ir.context()\n@@ -35,7 +36,7 @@\n     module = triton.compiler.optimize_triton_ir(module)\n     if args.target == 'triton-ir':\n         print(module.str())\n-        exit(0)\n+        sys.exit(0)\n \n     if not args.sm:\n         raise argparse.ArgumentError(None, \"Must specify --sm for PTX compilation\")\n@@ -44,13 +45,13 @@\n     module = triton.compiler.ttir_to_ttgir(module, num_warps=4, num_stages=3, compute_capability=args.sm)\n     if args.target == 'triton-gpu-ir':\n         print(module.str())\n-        exit(0)\n+        sys.exit(0)\n \n     # triton-gpu-ir -> llvm-ir\n     module = triton.compiler.ttgir_to_llir(module, extern_libs=None, compute_capability=args.sm)\n     if args.target == 'llvm-ir':\n         print(module)\n-        exit(0)\n+        sys.exit(0)\n \n     if not args.ptx_version:\n         raise argparse.ArgumentError(None, \"Must specify --ptx-version for PTX compilation\")"}, {"filename": "python/triton/tools/build_extern.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -157,7 +157,8 @@ def __init__(self, path) -> None:\n         super().__init__(\"libdevice\", path)\n         self._symbol_groups = {}\n \n-    def _extract_symbol(self, line) -> Optional[Symbol]:\n+    @staticmethod\n+    def _extract_symbol(line) -> Optional[Symbol]:\n         # Extract symbols from line in the following format:\n         # \"define [internal] <ret_type> @<name>(<arg_types>,)\"\n         entries = line.split(\"@\")"}, {"filename": "python/triton/utils.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -33,7 +33,8 @@ def wrap_dtype(arg):\n     def __init__(self, dtype):\n         self.dtype = dtype\n \n-    def data_ptr(self):\n+    @staticmethod\n+    def data_ptr():\n         return 0  # optimistically assumes multiple of 16\n \n "}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -461,6 +461,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x64x!tt.ptr<f32>, #AL> -> tensor<2x16x64xf32, #A>\n+    triton_gpu.async_commit_group\n     return\n   }\n }\n@@ -506,6 +507,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x32x!tt.ptr<f32>, #AL> -> tensor<2x16x32xf32, #A>\n+    triton_gpu.async_commit_group\n     return\n   }\n }\n@@ -562,6 +564,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32x!tt.ptr<f32>, #AL> -> tensor<2x32x32xf32, #A>\n+    triton_gpu.async_commit_group\n     return\n   }\n }\n@@ -923,10 +926,10 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    // CHECK-SAME: (f32, f32, f32, f32)\n+    // CHECK-SAME: (vector<1xf32>, vector<1xf32>, vector<1xf32>, vector<1xf32>)\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    // CHECK-SAME: (f32, f32, f32, f32)\n+    // CHECK-SAME: (vector<1xf32>, vector<1xf32>, vector<1xf32>, vector<1xf32>)\n     %a_mat = triton_gpu.convert_layout %a : (tensor<32x16xf32, #shared>) -> tensor<32x16xf32, #dot_operand_a>\n     %b_mat = triton_gpu.convert_layout %b : (tensor<16x32xf32, #shared>) -> tensor<16x32xf32, #dot_operand_b>\n "}, {"filename": "test/TritonGPU/prefetch.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -17,11 +17,11 @@\n // CHECK-DAG: %[[B0_PREFETCH_SMEM:.*]] = tensor.extract_slice %[[B0:.*]][0, 0] [16, 128]\n // CHECK-DAG: %[[B0_PREFETCH:.*]] = triton_gpu.convert_layout %[[B0_PREFETCH_SMEM]]\n // CHECK:     scf.for {{.*}} iter_args({{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, %[[a0_prefetch:.*]] = %[[A0_PREFETCH]], %[[b0_prefetch:.*]] = %[[B0_PREFETCH]]\n-// CHECK:       %[[D_FIRST:.*]] = tt.dot %[[a0_prefetch]], %[[b0_prefetch:.*]], {{.*}}\n // CHECK-DAG:   %[[A_REM_SMEM:.*]] = tensor.extract_slice %[[arg_a0]][0, 16] [128, 16]\n // CHECK-DAG:   %[[A_REM:.*]] = triton_gpu.convert_layout %[[A_REM_SMEM]]\n // CHECK-DAG:   %[[B_REM_SMEM:.*]] = tensor.extract_slice %[[arg_b0]][16, 0] [16, 128]\n // CHECK-DAG:   %[[B_REM:.*]] = triton_gpu.convert_layout %[[B_REM_SMEM]]\n+// CHECK:       %[[D_FIRST:.*]] = tt.dot %[[a0_prefetch]], %[[b0_prefetch:.*]], {{.*}}\n // CHECK:       tt.dot %[[A_REM]], %[[B_REM]], %[[D_FIRST:.*]]\n // CHECK-DAG:   %[[NEXT_A_PREFETCH_SMEM:.*]] = tensor.extract_slice {{.*}}[0, 0] [128, 16]\n // CHECK-DAG:   %[[NEXT_A_PREFETCH:.*]] = triton_gpu.convert_layout %[[NEXT_A_PREFETCH_SMEM]]"}]