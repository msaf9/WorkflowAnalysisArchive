[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 9, "deletions": 11, "changes": 20, "file_content_changes": "@@ -490,6 +490,7 @@ std::function<void(int, int)> getLoadMatrixFn(\n     bool isA, TritonGPUToLLVMTypeConverter *typeConverter,\n     ConversionPatternRewriter &rewriter, Location loc) {\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto shapePerCTA = getShapePerCTA(tensorTy);\n   Type eltTy = tensorTy.getElementType();\n   // We assumes that the input operand of Dot should be from shared layout.\n   // TODO(Superjomn) Consider other layouts if needed later.\n@@ -515,9 +516,8 @@ std::function<void(int, int)> getLoadMatrixFn(\n   auto load = [=, &rewriter, &vals](int a, int b) {\n     MMA16816SmemLoader loader(\n         warpsPerTile, sharedLayout.getOrder(), mmaLayout.getWarpsPerCTA(),\n-        kOrder, kWidth, smemObj.strides, tensorTy.getShape() /*tileShape*/,\n-        instrShape, matShape, perPhase, maxPhase, elemBytes, rewriter,\n-        typeConverter, loc);\n+        kOrder, kWidth, smemObj.strides, shapePerCTA /*tileShape*/, instrShape,\n+        matShape, perPhase, maxPhase, elemBytes, rewriter, typeConverter, loc);\n     // Offset of a slice within the original tensor in shared memory\n     Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n     SmallVector<Value> offs =\n@@ -559,17 +559,15 @@ Value loadArg(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n               TritonGPUToLLVMTypeConverter *typeConverter, Value thread,\n               bool isA) {\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto shapePerCTA = getShapePerCTA(tensorTy);\n   int bitwidth = tensorTy.getElementTypeBitWidth();\n   auto mmaLayout = encoding.getParent().cast<MmaEncodingAttr>();\n \n-  SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n-                             tensorTy.getShape().end());\n-\n   ValueTable vals;\n   int mmaInstrM = 16, mmaInstrN = 8, mmaInstrK = 4 * 64 / bitwidth;\n   int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n \n-  auto numRep = encoding.getMMAv2Rep(tensorTy.getShape(), bitwidth);\n+  auto numRep = encoding.getMMAv2Rep(shapePerCTA, bitwidth);\n   int kWidth = encoding.getMMAv2kWidth();\n \n   auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n@@ -579,14 +577,14 @@ Value loadArg(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n \n   SmallVector<Value> multiDimWarpId =\n       delinearize(rewriter, loc, warp, warpsPerCTA, order);\n-  Value warpM = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n-  Value warpN = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n+  Value warpM = urem(multiDimWarpId[0], i32_val(shapePerCTA[0] / 16));\n+  Value warpN = urem(multiDimWarpId[1], i32_val(shapePerCTA[1] / 8));\n \n   int warpsPerTile;\n   if (isA)\n-    warpsPerTile = std::min<int>(warpsPerCTA[0], shape[0] / 16);\n+    warpsPerTile = std::min<int>(warpsPerCTA[0], shapePerCTA[0] / 16);\n   else\n-    warpsPerTile = std::min<int>(warpsPerCTA[1], shape[1] / 16);\n+    warpsPerTile = std::min<int>(warpsPerCTA[1], shapePerCTA[1] / 16);\n \n   std::function<void(int, int)> loadFn;\n   if (isA)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "file_content_changes": "@@ -569,6 +569,28 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n         mask = and_(mask, icmp_slt(mul(threadDim, i32_val(sizePerThread[dim])),\n                                    i32_val(shape[dim])));\n       }\n+      // Do not write duplicated data when multicast is enabled\n+      if (triton::gpu::getNumCTAs(layout) > 1) {\n+        auto _0 = i32_val(0);\n+        auto CTAsPerCGA = triton::gpu::getCTAsPerCGA(layout);\n+        auto CTASplitNum = triton::gpu::getCTASplitNum(layout);\n+        auto CTAOrder = triton::gpu::getCTAOrder(layout);\n+\n+        auto clusterCTAId = getClusterCTAId(rewriter, loc);\n+        auto multiDimClusterCTAId =\n+            delinearize(rewriter, loc, clusterCTAId, CTAsPerCGA, CTAOrder);\n+\n+        for (unsigned dim = 0; dim < rank; ++dim) {\n+          // Skip when multicast is not enabled in this dimension\n+          if (CTAsPerCGA[dim] == CTASplitNum[dim])\n+            continue;\n+          // This wrapping rule must be consistent with emitCTAOffsetForLayout\n+          unsigned splitNum = std::min<unsigned>(shape[dim], CTASplitNum[dim]);\n+          multiDimClusterCTAId[dim] =\n+              urem(multiDimClusterCTAId[dim], i32_val(splitNum));\n+          mask = and_(mask, icmp_eq(multiDimClusterCTAId[dim], _0));\n+        }\n+      }\n     } else {\n       // If the tensor is not ranked, then it is a scalar and only thread 0 can\n       // write"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -849,11 +849,13 @@ DotOperandEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n \n unsigned DotOperandEncodingAttr::getTotalElemsPerThread(ArrayRef<int64_t> shape,\n                                                         Type eltTy) const {\n+  auto shapePerCTA = getShapePerCTA(*this, shape);\n   if (auto mmaParent = getParent().dyn_cast<MmaEncodingAttr>()) {\n     int warpsPerCTAM = mmaParent.getWarpsPerCTA()[0];\n     int warpsPerCTAN = mmaParent.getWarpsPerCTA()[1];\n+    // A100\n     if (mmaParent.isAmpere()) {\n-      auto rep = getMMAv2Rep(shape, eltTy.getIntOrFloatBitWidth());\n+      auto rep = getMMAv2Rep(shapePerCTA, eltTy.getIntOrFloatBitWidth());\n       if (getOpIdx() == 0)\n         return 4 * rep[0] * rep[1];\n       if (getOpIdx() == 1)\n@@ -925,8 +927,8 @@ unsigned DotOperandEncodingAttr::getTotalElemsPerThread(ArrayRef<int64_t> shape,\n     auto order = blockedLayout.getOrder();\n     auto sizePerThread = getSizePerThread(blockedLayout);\n \n-    int K = getOpIdx() == 0 ? shape[1] : shape[0];\n-    int otherDim = getOpIdx() == 1 ? shape[1] : shape[0];\n+    int K = getOpIdx() == 0 ? shapePerCTA[1] : shapePerCTA[0];\n+    int otherDim = getOpIdx() == 1 ? shapePerCTA[1] : shapePerCTA[0];\n \n     bool isM = getOpIdx() == 0;\n "}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/PlanCTA.cpp", "status": "modified", "additions": 11, "deletions": 1, "changes": 12, "file_content_changes": "@@ -325,8 +325,18 @@ bool CTAPlanner::processReduce(triton::FuncOp &funcOp) {\n       }\n     }\n \n+    llvm::SmallVector<unsigned> CTASplitNum = CTAsPerCGA;\n+\n+    // If numCTAs > 1 and the only dimension is the reduced dimension, after the\n+    // above two for-loops, CTAsPerCGA = [0] and remainingCTAs = numCTAs. We set\n+    // CTAsPerCGA[0] = numCTAs and keep CTASplitNum[0] = 1 to ensure that no\n+    // cross-CTA reduction is required, although this will introduce duplicated\n+    // calculation\n+    if (remainingCTAs > 0)\n+      CTAsPerCGA[order[rank - 1]] *= remainingCTAs;\n+\n     auto CTALayout =\n-        ttg::CTALayoutAttr::get(context, CTAsPerCGA, CTAsPerCGA, CTAOrder);\n+        ttg::CTALayoutAttr::get(context, CTAsPerCGA, CTASplitNum, CTAOrder);\n     if (!tiled)\n       setTiling(CTALayout.getCTAsPerCGA());\n     auto newSrcLayout = replaceCTALayout(srcLayout, srcShape, CTALayout);"}]