[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -155,7 +155,7 @@ def TT_LoadOp : TT_Op<\"load\",\n                        \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n         // A tensor pointer with boundary check and padding\n         OpBuilder<(ins \"Value\":$ptr, \"ArrayRef<int32_t>\":$boundaryCheck,\n-                       \"Optional<triton::PaddingOption>\":$padding, \"triton::CacheModifier\":$cache,\n+                       \"std::optional<triton::PaddingOption>\":$padding, \"triton::CacheModifier\":$cache,\n                        \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n         // A tensor of pointers or a pointer to a scalar with mask\n         OpBuilder<(ins \"Value\":$ptr, \"Value\":$mask, \"triton::CacheModifier\":$cache,\n@@ -164,8 +164,9 @@ def TT_LoadOp : TT_Op<\"load\",\n         OpBuilder<(ins \"Value\":$ptr, \"Value\":$mask, \"Value\":$other, \"triton::CacheModifier\":$cache,\n                        \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n         // A utility function to build the operation with all attributes\n-        OpBuilder<(ins \"Value\":$ptr, \"Value\":$mask, \"Value\":$other, \"Optional<ArrayRef<int32_t>>\":$boundaryCheck,\n-                       \"Optional<triton::PaddingOption>\":$padding, \"triton::CacheModifier\":$cache,\n+        OpBuilder<(ins \"Value\":$ptr, \"Value\":$mask, \"Value\":$other,\n+                       \"std::optional<ArrayRef<int32_t>>\":$boundaryCheck,\n+                       \"std::optional<triton::PaddingOption>\":$padding, \"triton::CacheModifier\":$cache,\n                        \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>\n     ];\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -31,10 +31,37 @@ SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n \n SmallVector<unsigned> getSizePerThread(Attribute layout);\n \n+// Returns the number of contiguous elements that each thread\n+// has access to, on each dimension of the tensor. E.g.\n+// for a blocked layout with sizePerThread = [1, 4], returns [1, 4],\n+// regardless of the shape of the tensor.\n SmallVector<unsigned> getContigPerThread(Attribute layout);\n \n+// Returns the number of non-replicated contiguous elements that each thread\n+// has access to, on each dimension of the tensor. For a blocked layout\n+// with sizePerThread = [1, 4] and tensor shape = [128, 1], the elements\n+// for thread 0 would be [A_{0, 0}, A_{0, 0}, A_{0, 0}, A_{0, 0}], returns [1,\n+// 1]. Whereas for a tensor shape [128, 128], the elements for thread 0 would be\n+// [A_{0, 0}, A_{0, 1}, A_{0, 2}, A_{0, 3}], returns [1, 4].\n SmallVector<unsigned> getUniqueContigPerThread(Type type);\n \n+// Returns the number of threads per warp that have access to non-replicated\n+// elements of the tensor. E.g. for a blocked layout with sizePerThread = [1,\n+// 1], threadsPerWarp = [2, 16] and tensor shape = [2, 2], threads 0, 1, 16, 17\n+// have access to the full tensor, whereas the other threads have access to\n+// replicated elements, so this function returns [2, 2].\n+SmallVector<unsigned>\n+getThreadsPerWarpWithUniqueData(Attribute layout,\n+                                ArrayRef<int64_t> tensorShape);\n+\n+// Returns the number of warps per CTA that have access to non-replicated\n+// elements of the tensor. E.g. for a blocked layout with sizePerThread = [1,\n+// 1], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4] and tensor shape = [2, 2],\n+// returns [1, 1], since the first warp has access to the full tensor, whereas\n+// the other warps have access to replicated elements.\n+SmallVector<unsigned>\n+getWarpsPerCTAWithUniqueData(Attribute layout, ArrayRef<int64_t> tensorShape);\n+\n SmallVector<unsigned> getThreadsPerCTA(Attribute layout);\n \n SmallVector<unsigned>"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 11, "deletions": 4, "changes": 15, "file_content_changes": "@@ -17,19 +17,23 @@ unsigned ReduceOpHelper::getInterWarpSize() {\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n   unsigned sizeIntraWarps = getIntraWarpSize();\n   return std::min(srcReduceDimSize / sizeIntraWarps,\n-                  triton::gpu::getWarpsPerCTA(getSrcLayout())[axis]);\n+                  triton::gpu::getWarpsPerCTAWithUniqueData(\n+                      getSrcLayout(), getSrcShape())[axis]);\n }\n \n unsigned ReduceOpHelper::getIntraWarpSize() {\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n   return std::min(srcReduceDimSize,\n-                  triton::gpu::getThreadsPerWarp(getSrcLayout())[axis]);\n+                  triton::gpu::getThreadsPerWarpWithUniqueData(\n+                      getSrcLayout(), getSrcShape())[axis]);\n }\n \n unsigned ReduceOpHelper::getThreadsReductionAxis() {\n   auto srcLayout = getSrcLayout();\n-  return triton::gpu::getThreadsPerWarp(srcLayout)[axis] *\n-         triton::gpu::getWarpsPerCTA(srcLayout)[axis];\n+  auto srcShape = getSrcShape();\n+  return triton::gpu::getThreadsPerWarpWithUniqueData(srcLayout,\n+                                                      srcShape)[axis] *\n+         triton::gpu::getWarpsPerCTAWithUniqueData(srcLayout, srcShape)[axis];\n }\n \n SmallVector<unsigned> ReduceOpHelper::getScratchConfigBasic() {\n@@ -88,6 +92,9 @@ bool ReduceOpHelper::isSupportedLayout() {\n       return true;\n     }\n   }\n+  if (auto sliceLayout = srcLayout.dyn_cast<triton::gpu::SliceEncodingAttr>()) {\n+    return true;\n+  }\n   return false;\n }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 14, "deletions": 5, "changes": 19, "file_content_changes": "@@ -87,6 +87,15 @@ struct ReduceOpConversion\n                           Attribute layout, SmallVector<Value> &index,\n                           SmallVector<Value> &writeIdx,\n                           std::map<int, Value> &ints, unsigned axis) const {\n+    if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+      auto dim = sliceLayout.getDim();\n+      assert(dim != axis && \"Reduction axis cannot be sliced\");\n+      auto parentLayout = sliceLayout.getParent();\n+      getWriteIndexBasic(rewriter, loc, parentLayout, index, writeIdx, ints,\n+                         axis);\n+      return;\n+    }\n+\n     writeIdx = index;\n     auto sizePerThread = triton::gpu::getSizePerThread(layout);\n     Value axisSizePerThread = ints[sizePerThread[axis]];\n@@ -100,9 +109,10 @@ struct ReduceOpConversion\n       // to map every `axisSizePerThread` to 1 value in smem as:\n       // writeIdx[axis] = index[axis] / axisSizePerThread\n       writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n-    }\n-    auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>();\n-    if (mmaLayout && mmaLayout.isAmpere()) {\n+    } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+      if (!mmaLayout.isAmpere()) {\n+        llvm::report_fatal_error(\"Unsupported layout\");\n+      }\n       if (axis == 0) {\n         // Because warpTileSize = [16, 8] and threadsPerWarp = [8, 4], each 8\n         // rows in smem would correspond to a warp. The mapping\n@@ -113,8 +123,7 @@ struct ReduceOpConversion\n         // Same as BlockedEncodingAttr case\n         writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n       }\n-    }\n-    if (mmaLayout && !mmaLayout.isAmpere()) {\n+    } else {\n       llvm::report_fatal_error(\"Unsupported layout\");\n     }\n   }"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 71, "deletions": 7, "changes": 78, "file_content_changes": "@@ -81,10 +81,41 @@ SmallVector<unsigned> getThreadsPerWarp(Attribute layout) {\n     if (mmaLayout.isAmpere())\n       return {8, 4};\n   }\n+  if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    auto parent = sliceLayout.getParent();\n+    auto parentThreadsPerWarp = getThreadsPerWarp(parent);\n+    SmallVector<unsigned> threadsPerWarp = parentThreadsPerWarp;\n+    threadsPerWarp.erase(threadsPerWarp.begin() + sliceLayout.getDim());\n+    for (unsigned i = 0; i < threadsPerWarp.size(); i++)\n+      threadsPerWarp[i] *= parentThreadsPerWarp[sliceLayout.getDim()];\n+    return threadsPerWarp;\n+  }\n   assert(0 && \"getThreadsPerWarp not implemented\");\n   return {};\n }\n \n+SmallVector<unsigned>\n+getThreadsPerWarpWithUniqueData(Attribute layout,\n+                                ArrayRef<int64_t> tensorShape) {\n+  if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    auto parentLayout = sliceLayout.getParent();\n+    auto parentShape = sliceLayout.paddedShape(tensorShape);\n+    auto parentThreadsPerWarp =\n+        getThreadsPerWarpWithUniqueData(parentLayout, parentShape);\n+    SmallVector<unsigned> threadsPerWarp = parentThreadsPerWarp;\n+    threadsPerWarp.erase(threadsPerWarp.begin() + sliceLayout.getDim());\n+    return threadsPerWarp;\n+  }\n+  auto threadsPerWarp = getThreadsPerWarp(layout);\n+  assert(threadsPerWarp.size() == tensorShape.size() &&\n+         \"layout and tensor shape must have the same rank\");\n+  for (unsigned i = 0; i < threadsPerWarp.size(); i++) {\n+    threadsPerWarp[i] = std::min<unsigned>(threadsPerWarp[i], tensorShape[i]);\n+  }\n+\n+  return threadsPerWarp;\n+}\n+\n SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getWarpsPerCTA().begin(),\n@@ -94,10 +125,43 @@ SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n     return SmallVector<unsigned>(mmaLayout.getWarpsPerCTA().begin(),\n                                  mmaLayout.getWarpsPerCTA().end());\n   }\n+  if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    auto parent = sliceLayout.getParent();\n+    auto parentWarpsPerCTA = getWarpsPerCTA(parent);\n+    SmallVector<unsigned> warpsPerCTA = parentWarpsPerCTA;\n+    warpsPerCTA.erase(warpsPerCTA.begin() + sliceLayout.getDim());\n+    for (unsigned i = 0; i < warpsPerCTA.size(); i++)\n+      warpsPerCTA[i] *= parentWarpsPerCTA[sliceLayout.getDim()];\n+    return warpsPerCTA;\n+  }\n   assert(0 && \"getWarpsPerCTA not implemented\");\n   return {};\n }\n \n+SmallVector<unsigned>\n+getWarpsPerCTAWithUniqueData(Attribute layout, ArrayRef<int64_t> tensorShape) {\n+  if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    auto parentLayout = sliceLayout.getParent();\n+    auto parentShape = sliceLayout.paddedShape(tensorShape);\n+    auto parentWarpsPerCTA =\n+        getWarpsPerCTAWithUniqueData(parentLayout, parentShape);\n+    SmallVector<unsigned> warpsPerCTA = parentWarpsPerCTA;\n+    warpsPerCTA.erase(warpsPerCTA.begin() + sliceLayout.getDim());\n+    return warpsPerCTA;\n+  }\n+  auto warpsPerCTA = getWarpsPerCTA(layout);\n+  assert(warpsPerCTA.size() == tensorShape.size() &&\n+         \"layout and tensor shape must have the same rank\");\n+  for (unsigned i = 0; i < warpsPerCTA.size(); i++) {\n+    auto sizePerWarp =\n+        getSizePerThread(layout)[i] * getThreadsPerWarp(layout)[i];\n+    auto maxWarpsPerDim = ceil<unsigned>(tensorShape[i], sizePerWarp);\n+    warpsPerCTA[i] = std::min<unsigned>(warpsPerCTA[i], maxWarpsPerDim);\n+  }\n+\n+  return warpsPerCTA;\n+}\n+\n SmallVector<unsigned> getSizePerThread(Attribute layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n@@ -189,7 +253,7 @@ SmallVector<unsigned> getThreadsPerCTA(Attribute layout) {\n       threads.push_back(blockedLayout.getThreadsPerWarp()[d] *\n                         blockedLayout.getWarpsPerCTA()[d]);\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    if (mmaLayout.getVersionMajor() == 2) {\n+    if (mmaLayout.isAmpere()) {\n       threads = {8 * mmaLayout.getWarpsPerCTA()[0],\n                  4 * mmaLayout.getWarpsPerCTA()[1]};\n     } else\n@@ -1074,9 +1138,9 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n       return mlir::failure();\n     }\n     auto newType = op->getResult(0).getType().cast<RankedTensorType>();\n-    // Ensure that the new insert_slice op is placed in the same place as the\n-    // old insert_slice op. Otherwise, the new insert_slice op may be placed\n-    // after the async_wait op, which is not allowed.\n+    // Ensure that the new insert_slice op is placed in the same place as\n+    // the old insert_slice op. Otherwise, the new insert_slice op may be\n+    // placed after the async_wait op, which is not allowed.\n     OpBuilder::InsertionGuard guard(rewriter);\n     rewriter.setInsertionPoint(insert_slice);\n     auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n@@ -1104,9 +1168,9 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n     auto resType = RankedTensorType::get(\n         origResType.getShape(), origResType.getElementType(),\n         extract_slice.getType().cast<RankedTensorType>().getEncoding());\n-    // Ensure that the new extract_slice op is placed in the same place as the\n-    // old extract_slice op. Otherwise, the new extract_slice op may be placed\n-    // after the async_wait op, which is not allowed.\n+    // Ensure that the new extract_slice op is placed in the same place as\n+    // the old extract_slice op. Otherwise, the new extract_slice op may be\n+    // placed after the async_wait op, which is not allowed.\n     OpBuilder::InsertionGuard guard(rewriter);\n     rewriter.setInsertionPoint(extract_slice);\n     auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>("}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 8, "deletions": 7, "changes": 15, "file_content_changes": "@@ -364,10 +364,6 @@ Value LoopPipeliner::getLoadMask(triton::LoadOp loadOp, Value mappedMask,\n }\n \n void LoopPipeliner::emitPrologue() {\n-  // llvm::errs() << \"loads to pipeline...:\\n\";\n-  // for (Value load : loads)\n-  //   llvm::errs() << load << \"\\n\";\n-\n   OpBuilder builder(forOp);\n   for (BlockArgument &arg : forOp.getRegionIterArgs()) {\n     OpOperand &operand = forOp.getOpOperandForRegionIterArg(arg);\n@@ -392,7 +388,7 @@ void LoopPipeliner::emitPrologue() {\n     for (Operation &op : forOp.getLoopBody().front()) {\n       if (depOps.contains(&op))\n         orderedDeps.push_back(&op);\n-      else if (loads.contains(op.getResult(0)))\n+      else if (op.getNumResults() > 0 && loads.contains(op.getResult(0)))\n         orderedDeps.push_back(&op);\n     }\n     assert(depOps.size() + loads.size() == orderedDeps.size() &&\n@@ -583,7 +579,12 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n     // we replace the use new load use with a convert layout\n     size_t i = std::distance(loads.begin(), it);\n     auto cvtDstTy = op.getResult(0).getType().cast<RankedTensorType>();\n-    auto cvtDstEnc = cvtDstTy.getEncoding().cast<ttg::DotOperandEncodingAttr>();\n+    auto cvtDstEnc =\n+        cvtDstTy.getEncoding().dyn_cast<ttg::DotOperandEncodingAttr>();\n+    if (!cvtDstEnc) {\n+      builder.clone(op, mapping);\n+      continue;\n+    }\n     auto newDstTy = RankedTensorType::get(\n         cvtDstTy.getShape(), cvtDstTy.getElementType(),\n         ttg::DotOperandEncodingAttr::get(\n@@ -601,7 +602,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   for (Operation &op : forOp.getLoopBody().front()) {\n     if (depOps.contains(&op))\n       orderedDeps.push_back(&op);\n-    else if (loads.contains(op.getResult(0)))\n+    else if (op.getNumResults() > 0 && loads.contains(op.getResult(0)))\n       orderedDeps.push_back(&op);\n   }\n   assert(depOps.size() + loads.size() == orderedDeps.size() &&"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 48, "deletions": 43, "changes": 91, "file_content_changes": "@@ -1586,54 +1586,59 @@ void init_triton_translation(py::module &m) {\n   m.def(\"compile_ptx_to_cubin\",\n         [](const std::string &ptxCode, const std::string &ptxasPath,\n            int capability) -> py::object {\n-          py::gil_scoped_release allow_threads;\n+          std::string cubin;\n+          {\n+            py::gil_scoped_release allow_threads;\n \n-          // compile ptx with ptxas\n-          llvm::SmallString<64> fsrc;\n-          llvm::SmallString<64> flog;\n-          llvm::sys::fs::createTemporaryFile(\"compile-ptx-src\", \"\", fsrc);\n-          llvm::sys::fs::createTemporaryFile(\"compile-ptx-log\", \"\", flog);\n-          std::string fbin = std::string(fsrc) + \".o\";\n-          llvm::FileRemover logRemover(flog);\n-          llvm::FileRemover binRemover(fbin);\n-          const char *_fsrc = fsrc.c_str();\n-          const char *_flog = flog.c_str();\n-          const char *_fbin = fbin.c_str();\n-          std::ofstream ofs(_fsrc);\n-          ofs << ptxCode << std::endl;\n-          ofs.close();\n-          std::string cmd;\n-          int err;\n-          cmd = ptxasPath + \" -v --gpu-name=sm_\" + std::to_string(capability) +\n-                (capability == 90 ? \"a \" : \" \") + _fsrc + \" -o \" + _fsrc +\n-                \".o 2> \" + _flog;\n+            // compile ptx with ptxas\n+            llvm::SmallString<64> fsrc;\n+            llvm::SmallString<64> flog;\n+            llvm::sys::fs::createTemporaryFile(\"compile-ptx-src\", \"\", fsrc);\n+            llvm::sys::fs::createTemporaryFile(\"compile-ptx-log\", \"\", flog);\n+            std::string fbin = std::string(fsrc) + \".o\";\n+            llvm::FileRemover logRemover(flog);\n+            llvm::FileRemover binRemover(fbin);\n+            const char *_fsrc = fsrc.c_str();\n+            const char *_flog = flog.c_str();\n+            const char *_fbin = fbin.c_str();\n+            std::ofstream ofs(_fsrc);\n+            ofs << ptxCode << std::endl;\n+            ofs.close();\n+            std::string cmd;\n+            int err;\n+            cmd = ptxasPath + \" -v --gpu-name=sm_\" +\n+                  std::to_string(capability) + (capability == 90 ? \"a \" : \" \") +\n+                  _fsrc + \" -o \" + _fsrc + \".o 2> \" + _flog;\n \n-          err = system(cmd.c_str());\n-          if (err != 0) {\n-            err >>= 8;\n-            std::ifstream _log(_flog);\n-            std::string log(std::istreambuf_iterator<char>(_log), {});\n-            if (err == 255) {\n-              throw std::runtime_error(\"Internal Triton PTX codegen error: \\n\" +\n-                                       log);\n-            } else if (err == 128 + SIGSEGV) {\n-              throw std::runtime_error(\"Please run `ptxas \" + fsrc.str().str() +\n-                                       \"` to confirm that this is a \"\n-                                       \"bug in `ptxas`\\n\" +\n-                                       log);\n+            err = system(cmd.c_str());\n+            if (err != 0) {\n+              err >>= 8;\n+              std::ifstream _log(_flog);\n+              std::string log(std::istreambuf_iterator<char>(_log), {});\n+              if (err == 255) {\n+                throw std::runtime_error(\n+                    \"Internal Triton PTX codegen error: \\n\" + log);\n+              } else if (err == 128 + SIGSEGV) {\n+                throw std::runtime_error(\"Please run `ptxas \" +\n+                                         fsrc.str().str() +\n+                                         \"` to confirm that this is a \"\n+                                         \"bug in `ptxas`\\n\" +\n+                                         log);\n+              } else {\n+                throw std::runtime_error(\"`ptxas` failed with error code \" +\n+                                         std::to_string(err) + \": \\n\" + log);\n+              }\n+              return {};\n             } else {\n-              throw std::runtime_error(\"`ptxas` failed with error code \" +\n-                                       std::to_string(err) + \": \\n\" + log);\n+              llvm::FileRemover srcRemover(fsrc);\n+              std::ifstream _cubin(_fbin, std::ios::binary);\n+              cubin = std::string(std::istreambuf_iterator<char>(_cubin), {});\n+              _cubin.close();\n+              // Do not return here, exit the gil scope and return below\n             }\n-            return {};\n-          } else {\n-            llvm::FileRemover srcRemover(fsrc);\n-            std::ifstream _cubin(_fbin, std::ios::binary);\n-            std::string cubin(std::istreambuf_iterator<char>(_cubin), {});\n-            _cubin.close();\n-            py::bytes bytes(cubin);\n-            return std::move(bytes);\n           }\n+          py::bytes bytes(cubin);\n+          return std::move(bytes);\n         });\n \n   m.def(\"add_external_libs\","}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 64, "deletions": 0, "changes": 64, "file_content_changes": "@@ -1420,6 +1420,70 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n     )\n \n \n+layouts = [\n+    BlockedLayout([1, 4], [1, 32], [4, 1], [1, 0]),\n+    BlockedLayout([1, 4], [1, 32], [2, 2], [1, 0]),\n+    BlockedLayout([1, 4], [1, 32], [1, 4], [1, 0]),\n+    BlockedLayout([1, 4], [8, 4], [2, 2], [0, 1])\n+]\n+\n+\n+@pytest.mark.parametrize(\"M, N\", [[32, 128], [128, 128], [128, 32]])\n+@pytest.mark.parametrize(\"src_layout\", layouts)\n+def test_reduce_2d(M, N, src_layout, device='cuda'):\n+    ir = f\"\"\"\n+    #src = {src_layout}\n+    module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n+    tt.func public @sum_kernel_0d1d(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n+        %cst = arith.constant dense<{M}> : tensor<{M}x1xi32, #src>\n+        %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+        %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n+        %2 = arith.muli %1, %cst : tensor<{M}x1xi32, #src>\n+        %3 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #src}}>>\n+        %4 = tt.expand_dims %3 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #src}}>>) -> tensor<1x{N}xi32, #src>\n+        %5 = tt.broadcast %2 : (tensor<{M}x1xi32, #src>) -> tensor<{M}x{N}xi32, #src>\n+        %6 = tt.broadcast %4 : (tensor<1x{N}xi32, #src>) -> tensor<{M}x{N}xi32, #src>\n+        %7 = arith.addi %5, %6 : tensor<{M}x{N}xi32, #src>\n+        %8 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x{N}x!tt.ptr<i32>, #src>\n+        %9 = tt.addptr %8, %7 : tensor<{M}x{N}x!tt.ptr<i32>, #src>, tensor<{M}x{N}xi32, #src>\n+        %10 = tt.load %9 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #src>\n+        %11 = \"tt.reduce\"(%10) ({{\n+        ^bb0(%arg2: i32, %arg3: i32):\n+        %13 = arith.addi %arg2, %arg3 : i32\n+        tt.reduce.return %13 : i32\n+        }}) {{axis = 1 : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+        %12 = \"tt.reduce\"(%11) ({{\n+        ^bb0(%arg2: i32, %arg3: i32):\n+        %13 = arith.addi %arg2, %arg3 : i32\n+        tt.reduce.return %13 : i32\n+        }}) {{axis = 0 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> i32\n+        tt.store %arg1, %12 {{cache = 1 : i32, evict = 1 : i32}} : i32\n+        tt.return\n+    }}\n+    }}\n+    \"\"\"\n+    import tempfile\n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n+        f.write(ir)\n+        f.flush()\n+        kernel = triton.compile(f.name)\n+\n+    rs = RandomState(17)\n+    x = rs.randint(0, 4, (M, N)).astype('int32')\n+    x = (x.view('uint32') & np.uint32(0xffffe000)).view('int32')\n+\n+    z = np.zeros((1,)).astype('int32')\n+\n+    x_tri = torch.tensor(x, device=device)\n+    z_tri = torch.tensor(z, device=device)\n+\n+    pgm = kernel[(1, 1, 1)](x_tri, z_tri)\n+\n+    z_ref = np.sum(x)\n+\n+    np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+\n+\n def test_generic_reduction(device='cuda'):\n \n     @triton.jit"}, {"filename": "python/triton/__init__.py", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -4,10 +4,6 @@\n # ---------------------------------------\n # Note: import order is significant here.\n \n-# TODO: torch needs to be imported first\n-# or pybind11 shows `munmap_chunk(): invalid pointer`\n-import torch  # noqa: F401\n-\n # submodules\n from .runtime import (\n     autotune,"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -11,8 +11,6 @@\n from pathlib import Path\n from typing import Any, Tuple\n \n-import torch\n-\n import triton\n import triton._C.libtriton.triton as _triton\n from ..runtime import driver\n@@ -324,6 +322,10 @@ def _is_cuda(arch):\n \n \n def get_architecture_descriptor(capability):\n+    try:\n+        import torch\n+    except ImportError:\n+        raise ImportError(\"Triton requires PyTorch to be installed\")\n     if capability is None:\n         if torch.version.hip is None:\n             device = triton.runtime.jit.get_current_device()"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -3,8 +3,6 @@\n from functools import wraps\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n-import torch\n-\n import triton\n from . import core as tl\n from triton._C.libtriton.triton import ir\n@@ -1183,6 +1181,10 @@ def dot(lhs: tl.tensor,\n         allow_tf32: bool,\n         out_dtype: tl.dtype,\n         builder: ir.builder) -> tl.tensor:\n+    try:\n+        import torch\n+    except ImportError:\n+        raise ImportError(\"Triton requires PyTorch to be installed\")\n     if torch.version.hip is None:\n         device = triton.runtime.jit.get_current_device()\n         capability = triton.runtime.jit.get_device_capability(device)"}]