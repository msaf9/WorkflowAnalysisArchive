[{"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 159, "deletions": 106, "changes": 265, "file_content_changes": "@@ -5,9 +5,13 @@\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"mlir/IR/Verifier.h\"\n+#include \"mlir/Interfaces/InferTypeOpInterface.h\"\n #include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Support/LogicalResult.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"mlir/Transforms/Passes.h\"\n+#include \"mlir/Transforms/RegionUtils.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n \n@@ -36,7 +40,7 @@ class SimplifyConversion : public mlir::RewritePattern {\n public:\n   SimplifyConversion(mlir::MLIRContext *context)\n       : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             2, context) {}\n+                             4, context) {}\n \n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n@@ -86,106 +90,165 @@ class SimplifyConversion : public mlir::RewritePattern {\n //\n // -----------------------------------------------------------------------------\n \n+static LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n+                                    Attribute &ret) {\n+  ret = targetEncoding;\n+  if (auto expand_dims = dyn_cast<triton::ExpandDimsOp>(op)) {\n+    ret = triton::gpu::SliceEncodingAttr::get(\n+        op->getContext(), expand_dims.axis(), targetEncoding);\n+  }\n+  if (auto reduce = dyn_cast<triton::ReduceOp>(op)) {\n+    auto sliceEncoding =\n+        targetEncoding.dyn_cast<triton::gpu::SliceEncodingAttr>();\n+    if (!sliceEncoding)\n+      return failure();\n+    ret = sliceEncoding.getParent();\n+  }\n+  return success();\n+}\n+\n+inline bool expensive_to_remat(Operation *op) {\n+  if (!op)\n+    return true;\n+  if (isa<triton::gpu::ExtractSliceOp, triton::gpu::AllocTensorOp,\n+          triton::gpu::InsertSliceAsyncOp, triton::LoadOp, triton::StoreOp,\n+          triton::DotOp>(op))\n+    return true;\n+  if (isa<scf::YieldOp, scf::ForOp>(op))\n+    return true;\n+  return false;\n+};\n+\n+Operation *cloneWithInferType(mlir::PatternRewriter &rewriter, Operation *op,\n+                              BlockAndValueMapping &mapping) {\n+  Operation *newOp = rewriter.clone(*op, mapping);\n+  auto origType = op->getResult(0).getType().cast<RankedTensorType>();\n+  auto newType = RankedTensorType::get(\n+      origType.getShape(), origType.getElementType(),\n+      newOp->getOperand(0).getType().cast<RankedTensorType>().getEncoding());\n+  newOp->getResult(0).setType(newType);\n+  auto typeInfer = dyn_cast<InferTypeOpInterface>(newOp);\n+  if (typeInfer) {\n+    SmallVector<Type, 1> newType;\n+    auto sucess = typeInfer.inferReturnTypes(\n+        newOp->getContext(), newOp->getLoc(), newOp->getOperands(),\n+        newOp->getAttrDictionary(), newOp->getRegions(), newType);\n+    if (success)\n+      newOp->getResult(0).setType(newType.front());\n+  }\n+  return newOp;\n+}\n+\n // Layout conversions are expensive. They require going through\n // shared memory, which is orders of magnitude slower than\n // other non-i/o operations in the dialect.\n // It therefore makes sense to remove them whenever possible,\n // even if it means rematerializing all values whose definitions\n // are reachable from it without passing through any memory operation.\n-class PullConversionToSource : public mlir::RewritePattern {\n+class RematerializeBackward : public mlir::RewritePattern {\n public:\n-  PullConversionToSource(mlir::MLIRContext *context)\n+  RematerializeBackward(mlir::MLIRContext *context)\n       : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n                              2, context) {}\n \n-  Attribute invertEncoding(Type targetType, Operation *op) const {\n-    RankedTensorType targetTensorType = targetType.cast<RankedTensorType>();\n-    if (auto expand_dims = dyn_cast<triton::ExpandDimsOp>(op)) {\n-      return targetTensorType.getEncoding()\n-          .cast<triton::gpu::BlockedEncodingAttr>()\n-          .squeeze(expand_dims.axis());\n-    }\n-    return targetTensorType.getEncoding();\n-  }\n-\n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *cvt,\n                   mlir::PatternRewriter &rewriter) const override {\n     if (!llvm::isa<triton::gpu::ConvertLayoutOp>(cvt))\n       return mlir::failure();\n-    // constants/splat are handled separately\n+    // we don't touch block arguments\n     Operation *op = cvt->getOperand(0).getDefiningOp();\n     if (!op)\n       return mlir::failure();\n-\n-    auto blacklist = [](Operation *op) {\n-      if (isa<triton::gpu::ExtractSliceOp, triton::gpu::AllocTensorOp,\n-              triton::gpu::InsertSliceAsyncOp, triton::LoadOp, triton::StoreOp,\n-              triton::DotOp>(op))\n-        return true;\n-      if (isa<scf::YieldOp, scf::ForOp>(op))\n-        return true;\n-      return false;\n-    };\n-\n-    // find all the ops that the conversion depends on\n-    SetVector<Operation *> depIntoOps;\n-    mlir::getBackwardSlice(cvt, &depIntoOps, [&](Operation *op) {\n-      return !blacklist(op) && op->getResult(0) &&\n-             (op->getResult(0).getParentBlock() ==\n-              cvt->getResult(0).getParentBlock());\n-    });\n-    // find all the ops that depend on something expensive\n-    // to rematerialize and break the dependency\n-    // chain there\n-    SetVector<Operation *> blacklistedOps;\n-    mlir::getBackwardSlice(cvt, &blacklistedOps, blacklist);\n-    for (Operation *op : blacklistedOps) {\n-      SetVector<Operation *> toRemove;\n-      mlir::getBackwardSlice(op, &toRemove);\n-      depIntoOps.set_subtract(toRemove);\n-    }\n-    // if there is nothing that can benefit from moving conversions\n-    // in the remaining op, we don't do anything\n-    auto it = llvm::find_if(depIntoOps, [&](Operation *op) {\n-      if (isa<triton::gpu::ConvertLayoutOp>(op)) {\n-        // conversions in for loops interfere with the\n-        // push-to-sink pass. Need a better cost model if how many conversions\n-        // we can actually remove by moving them to the beginning of the block\n-        auto forOp = dyn_cast<scf::ForOp>(cvt->getParentOp());\n-        if (!forOp &&\n-            (cvt->getResult(0).getType() == op->getOperand(0).getType()))\n-          return true;\n-      }\n-      if (isa<arith::ConstantOp, triton::MakeRangeOp, triton::SplatOp>(op))\n-        return true;\n-      return false;\n-    });\n-    if (it == depIntoOps.end()) {\n+    // we don't want to rematerialize any conversion to/from shared\n+    if (isSharedLayout(cvt->getResults()[0]) ||\n+        isSharedLayout(cvt->getOperand(0)))\n       return mlir::failure();\n+    auto targetType = cvt->getResultTypes()[0].cast<RankedTensorType>();\n+    // DFS\n+    SetVector<Operation *> processed;\n+    SetVector<Attribute> layout;\n+    std::vector<std::pair<Operation *, Attribute>> queue;\n+    std::vector<std::pair<Value, Attribute>> toConvert;\n+    queue.push_back({cvt, targetType.getEncoding()});\n+    int numCvts = 1;\n+    while (!queue.empty()) {\n+      Operation *currOp;\n+      Attribute currLayout;\n+      std::tie(currOp, currLayout) = queue.back();\n+      queue.pop_back();\n+      // If the current operation is expensive to rematerialize,\n+      // we stop everything\n+      if (expensive_to_remat(currOp))\n+        break;\n+      // a conversion will be removed here (i.e. transfered to operands)\n+      numCvts -= 1;\n+      // done processing\n+      processed.insert(currOp);\n+      layout.insert(currLayout);\n+      // add all operands to the queue\n+      for (Value argI : currOp->getOperands()) {\n+        Attribute newEncoding;\n+        if (failed(invertEncoding(currLayout, currOp, newEncoding)))\n+          return mlir::failure();\n+        toConvert.push_back({argI, newEncoding});\n+        Operation *opArgI = argI.getDefiningOp();\n+        if (!opArgI)\n+          continue;\n+        if (!opArgI || processed.contains(opArgI) ||\n+            (opArgI->getBlock() != cvt->getBlock()))\n+          continue;\n+        // if the conversion can be folded into opArgI then\n+        // we actually haven't added anny conversion\n+        if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n+                triton::MakeRangeOp, triton::SplatOp>(*opArgI))\n+          continue;\n+        // we add one expensive conversion for the current operand\n+        numCvts += 1;\n+        queue.push_back({opArgI, newEncoding});\n+      }\n     }\n+    // if rematerialization would add more conversions than it removes\n+    // then we don't do it\n+    if (numCvts > 0)\n+      return mlir::failure();\n+\n+    FuncOp parentFunc = cvt->getParentOfType<FuncOp>();\n+    bool test = cvt->getResult(0)\n+                    .getType()\n+                    .cast<RankedTensorType>()\n+                    .getEncoding()\n+                    .isa<triton::gpu::MmaEncodingAttr>();\n+    // if (test)\n+    //   llvm::outs() << \"--------\\nConverting \" << *cvt << \"\\n---------\\n\";\n \n-    // We convert cvt(op(arg_0, arg_1, ..., arg_n))\n-    // into op(cvt_0(arg_0), cvt_1(arg_1), ..., cvt_n(arg_n))\n     BlockAndValueMapping mapping;\n-    for (Value argI : op->getOperands()) {\n-      // Compute new argument types\n-      auto oldArgType = argI.getType().dyn_cast<RankedTensorType>();\n-      if (!oldArgType)\n-        continue;\n-      auto newEncoding = invertEncoding(cvt->getResultTypes()[0], op);\n-      auto newArgType = RankedTensorType::get(\n-          oldArgType.getShape(), oldArgType.getElementType(), newEncoding);\n-      // Create new argument\n-      auto cvtI = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          op->getLoc(), newArgType, argI);\n-      cvtI->moveBefore(op);\n-      mapping.map(argI, cvtI);\n+    for (int i = toConvert.size() - 1; i >= 0; i--) {\n+      // unpack information\n+      Value currOperand;\n+      Attribute targetLayout;\n+      std::tie(currOperand, targetLayout) = toConvert[i];\n+      // if (test)\n+      //   llvm::outs() << \"current \" << currOperand << \"\\n\";\n+      // rematerialize the operand if necessary\n+      Operation *currOperation = currOperand.getDefiningOp();\n+      if (processed.contains(currOperation)) {\n+        currOperation = cloneWithInferType(rewriter, currOperation, mapping);\n+        currOperand = currOperation->getResult(0);\n+      }\n+      if (i == 0)\n+        break;\n+      // compute target type for the layout cast\n+      auto currType = currOperand.getType().cast<RankedTensorType>();\n+      auto newType = RankedTensorType::get(\n+          currType.getShape(), currType.getElementType(), targetLayout);\n+      auto newOperand = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+          currOperand.getLoc(), newType, currOperand);\n+      if (currOperation)\n+        newOperand->moveAfter(currOperation);\n+      mapping.map(currOperand, newOperand);\n     }\n-    Operation *newOp = rewriter.clone(*op, mapping);\n-    newOp->getResult(0).setType(cvt->getResult(0).getType());\n-    rewriter.replaceOp(cvt, newOp->getResults());\n-\n+    rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n     return mlir::success();\n   }\n };\n@@ -226,6 +289,7 @@ bool tryLegalizeOp(Operation *op, DenseSet<Value> toPreserve,\n std::pair<SmallVector<Value, 4>, scf::ForOp>\n tryConvertIterArg(scf::ForOp &forOp, mlir::PatternRewriter &rewriter, size_t i,\n                   Type newType) {\n+  forOp.getInductionVar();\n   auto newEncoding = newType.cast<RankedTensorType>().getEncoding();\n   auto ctx = forOp.getContext();\n   auto isInLoop = [&](Operation *op) { return op->getParentOp() == forOp; };\n@@ -243,6 +307,7 @@ tryConvertIterArg(scf::ForOp &forOp, mlir::PatternRewriter &rewriter, size_t i,\n   BlockAndValueMapping mapping;\n   for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n+  mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n   // traverse all ops in the loop\n   for (Operation &op : forOp.getBody()->without_terminator()) {\n     // we clone the op\n@@ -278,9 +343,9 @@ tryConvertIterArg(scf::ForOp &forOp, mlir::PatternRewriter &rewriter, size_t i,\n   return {newResults, newForOp};\n }\n \n-class MoveArgConvertOutOfLoop : public mlir::RewritePattern {\n+class MoveConvertOutOfLoop : public mlir::RewritePattern {\n public:\n-  MoveArgConvertOutOfLoop(mlir::MLIRContext *context)\n+  MoveConvertOutOfLoop(mlir::MLIRContext *context)\n       : mlir::RewritePattern(scf::ForOp::getOperationName(), 1, context) {}\n \n   mlir::LogicalResult matchAndRewrite(mlir::Operation *op,\n@@ -290,27 +355,14 @@ class MoveArgConvertOutOfLoop : public mlir::RewritePattern {\n     auto isInLoop = [&](Operation *op) { return op->getParentOp() == forOp; };\n     auto iterArgs = forOp.getRegionIterArgs();\n     for (auto iterArg : llvm::enumerate(iterArgs)) {\n+      // skip non-tensor types\n+      if (!iterArg.value().getType().isa<RankedTensorType>())\n+        continue;\n+      // check\n       for (auto op : iterArg.value().getUsers()) {\n-        auto currOps = mlir::getSlice(op, isInLoop);\n-        auto pred = [&](Operation *op) {\n-          return isa<triton::LoadOp, triton::StoreOp>(op);\n-        };\n-        auto isCvt = [&](Operation *op) {\n-          return isa<triton::gpu::ConvertLayoutOp>(op);\n-        };\n-        auto isYield = [&](Operation *op) { return isa<scf::YieldOp>(op); };\n-        auto opIt = std::find(currOps.begin(), currOps.end(), op);\n-        auto yieldIt = std::find_if(currOps.begin(), currOps.end(), isYield);\n-        auto fwdEndIt = std::find_if(opIt, currOps.end(), pred);\n-        auto bwdBeginIt = std::find_if(currOps.begin(), opIt, pred);\n-        auto fwdCvtIt = std::find_if(opIt, fwdEndIt, isCvt);\n-        auto bwdCvtIt = std::find_if(bwdBeginIt, opIt, isCvt);\n-\n-        if (!iterArg.value().getType().isa<RankedTensorType>())\n-          continue;\n-        if (fwdCvtIt != fwdEndIt) {\n+        if (isa<triton::gpu::ConvertLayoutOp>(op)) {\n           auto newFor = tryConvertIterArg(forOp, rewriter, iterArg.index(),\n-                                          (*fwdCvtIt)->getResult(0).getType());\n+                                          op->getResult(0).getType());\n           rewriter.replaceOp(forOp, newFor.first);\n           return success();\n         }\n@@ -324,9 +376,9 @@ class MoveArgConvertOutOfLoop : public mlir::RewritePattern {\n //\n // -----------------------------------------------------------------------------\n \n-class PushConversionToSink : public mlir::RewritePattern {\n+class RematerializeForward : public mlir::RewritePattern {\n public:\n-  PushConversionToSink(mlir::MLIRContext *context)\n+  RematerializeForward(mlir::MLIRContext *context)\n       : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n                              2, context) {}\n \n@@ -430,16 +482,17 @@ class TritonGPUCombineOpsPass\n     mlir::RewritePatternSet patterns(context);\n \n     patterns.add<SimplifyConversion>(context);\n-    patterns.add<PullConversionToSource>(context);\n-    patterns.add<PushConversionToSink>(context);\n-    patterns.add<MoveArgConvertOutOfLoop>(context);\n+    patterns.add<RematerializeBackward>(context);\n+    patterns.add<RematerializeForward>(context);\n+    patterns.add<MoveConvertOutOfLoop>(context);\n     patterns.add<BlockedToMMA>(context);\n \n-    if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n+    if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n       signalPassFailure();\n+    }\n   }\n };\n \n std::unique_ptr<Pass> mlir::createTritonGPUCombineOpsPass() {\n   return std::make_unique<TritonGPUCombineOpsPass>();\n-}\n+}\n\\ No newline at end of file"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 6, "deletions": 30, "changes": 36, "file_content_changes": "@@ -45,8 +45,8 @@ func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   // CHECK: %1 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[target_layout]]>\n   // CHECK: %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[target_layout]]>\n   // CHECK: %3 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[target_layout]]>\n-  // CHECK: %4 = arith.muli %2, %3 : tensor<1024xi32, [[target_layout]]>\n-  // CHECK: %5 = arith.muli %0, %1 : tensor<1024xi32, [[target_layout]]>\n+  // CHECK: %4 = arith.muli %0, %2 : tensor<1024xi32, [[target_layout]]>\n+  // CHECK: %5 = arith.muli %1, %3 : tensor<1024xi32, [[target_layout]]>\n   // CHECK: %6 = arith.addi %4, %5 : tensor<1024xi32, [[target_layout]]>\n   // CHECK: return %6 : tensor<1024xi32, [[target_layout]]>\n }\n@@ -61,34 +61,10 @@ func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n \n // CHECK-LABEL: transpose\n func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n-  // CHECK: %cst = arith.constant dense<true> : tensor<64x64xi1, [[row_layout]]>\n-  // CHECK: %cst_0 = arith.constant dense<0.000000e+00> : tensor<64x64xf32, [[row_layout]]>\n-  // CHECK: %cst_1 = arith.constant dense<true> : tensor<64x64xi1, [[col_layout]]>\n-  // CHECK: %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = [[col_layout]]}>>\n-  // CHECK: %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = [[row_layout]]}>>\n-  // CHECK: %2 = tt.expand_dims %1 {axis = 1 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = [[row_layout]]}>>) -> tensor<64x1xi32, [[row_layout]]>\n-  // CHECK: %3 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, [[row_layout]]>\n-  // CHECK: %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, [[row_layout]]>\n-  // CHECK: %5 = arith.muli %2, %3 : tensor<64x1xi32, [[row_layout]]>\n-  // CHECK: %6 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = [[col_layout]]}>>\n-  // CHECK: %7 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = [[row_layout]]}>>\n-  // CHECK: %8 = tt.addptr %4, %5 : tensor<64x1x!tt.ptr<f32>, [[row_layout]]>\n-  // CHECK: %9 = tt.expand_dims %7 {axis = 0 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = [[row_layout]]}>>) -> tensor<1x64xi32, [[row_layout]]>\n-  // CHECK: %10 = tt.broadcast %8 : (tensor<64x1x!tt.ptr<f32>, [[row_layout]]>) -> tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n-  // CHECK: %11 = tt.broadcast %9 : (tensor<1x64xi32, [[row_layout]]>) -> tensor<64x64xi32, [[row_layout]]>\n-  // CHECK: %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, [[col_layout]]>\n-  // CHECK: %13 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = [[col_layout]]}>>) -> tensor<64x1xi32, [[col_layout]]>\n-  // CHECK: %14 = tt.expand_dims %6 {axis = 0 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = [[col_layout]]}>>) -> tensor<1x64xi32, [[col_layout]]>\n-  // CHECK: %15 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, [[col_layout]]>\n-  // CHECK: %16 = tt.addptr %12, %13 : tensor<64x1x!tt.ptr<f32>, [[col_layout]]>\n-  // CHECK: %17 = arith.muli %14, %15 : tensor<1x64xi32, [[col_layout]]>\n-  // CHECK: %18 = tt.broadcast %16 : (tensor<64x1x!tt.ptr<f32>, [[col_layout]]>) -> tensor<64x64x!tt.ptr<f32>, [[col_layout]]>\n-  // CHECK: %19 = tt.broadcast %17 : (tensor<1x64xi32, [[col_layout]]>) -> tensor<64x64xi32, [[col_layout]]>\n-  // CHECK: %20 = tt.addptr %10, %11 : tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n-  // CHECK: %21 = tt.load %20, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<64x64xf32, [[row_layout]]>\n-  // CHECK: %22 = tt.addptr %18, %19 : tensor<64x64x!tt.ptr<f32>, [[col_layout]]>\n-  // CHECK: %23 = triton_gpu.convert_layout %21 : (tensor<64x64xf32, [[row_layout]]>) -> tensor<64x64xf32, [[col_layout]]>\n-  // CHECK: tt.store %22, %23, %cst_1 : tensor<64x64xf32, [[col_layout]]>\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<64x64xf32, [[row_layout]]>\n+  // CHECK: [[cvt_val:%.*]] = triton_gpu.convert_layout [[loaded_val]] : (tensor<64x64xf32, [[row_layout]]>) -> tensor<64x64xf32, [[col_layout]]>\n+  // CHECK: tt.store {{.*}}, [[cvt_val]], %cst_1 : tensor<64x64xf32, [[col_layout]]>\n   // CHECK: return\n   %cst = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked1>\n   %cst_0 = arith.constant dense<true> : tensor<64x64xi1, #blocked1>"}]