[{"filename": "CMakeLists.txt", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -132,14 +132,13 @@ endif()\n # Python module\n if(TRITON_BUILD_PYTHON_MODULE)\n     message(STATUS \"Adding Python module\")\n+    set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n+    include_directories(\".\" ${PYTHON_SRC_PATH})\n     if (PYTHON_INCLUDE_DIRS)\n-      set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n-      include_directories(\".\" ${PYTHON_SRC_PATH} ${PYTHON_INCLUDE_DIRS})\n-      link_directories(${PYTHON_LINK_DIRS})\n+      include_directories(${PYTHON_INCLUDE_DIRS})\n     else()\n       find_package(Python3 REQUIRED COMPONENTS Development)\n-      set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n-      include_directories(\".\" ${PYTHON_SRC_PATH} ${Python3_INCLUDE_DIRS})\n+      include_directories(${Python3_INCLUDE_DIRS})\n       link_directories(${Python3_LIBRARY_DIRS})\n       link_libraries(${Python3_LIBRARIES})\n       add_link_options(${Python3_LINK_OPTIONS})\n@@ -169,7 +168,9 @@ list(APPEND CMAKE_MODULE_PATH \"${LLVM_CMAKE_DIR}\")\n include(TableGen) # required by AddMLIR\n include(AddLLVM)\n include(AddMLIR)\n-# include(HandleLLVMOptions) # human-friendly error message\n+\n+# Disable warnings that show up in external code (gtest;pybind11)\n+set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror -Wno-covered-switch-default\")\n \n include_directories(${MLIR_INCLUDE_DIRS})\n include_directories(${LLVM_INCLUDE_DIRS})\n@@ -192,7 +193,6 @@ get_property(dialect_libs GLOBAL PROPERTY MLIR_DIALECT_LIBS)\n get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)\n \n target_link_libraries(triton\n-  ${PYTHON_LIBRARIES}\n   TritonAnalysis\n   TritonTransforms\n   TritonGPUTransforms"}, {"filename": "include/triton/Analysis/Allocation.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -142,7 +142,7 @@ class Allocation {\n     BufferT(BufferKind kind) : BufferT(kind, 0, 0) {}\n     BufferT(BufferKind kind, size_t size) : BufferT(kind, size, 0) {}\n     BufferT(BufferKind kind, size_t size, size_t offset)\n-        : kind(kind), size(size), offset(offset), id(nextId++) {}\n+        : kind(kind), id(nextId++), size(size), offset(offset) {}\n \n     bool intersects(const BufferT &other) const {\n       return Interval<size_t>(offset, offset + size)"}, {"filename": "include/triton/Analysis/AxisInfo.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -28,8 +28,8 @@ class AxisInfo {\n            DimVectorT knownConstancy)\n       : contiguity(knownContiguity), divisibility(knownDivisibility),\n         constancy(knownConstancy), rank(contiguity.size()) {\n-    assert(knownDivisibility.size() == rank);\n-    assert(knownConstancy.size() == rank);\n+    assert(knownDivisibility.size() == (size_t)rank);\n+    assert(knownConstancy.size() == (size_t)rank);\n   }\n \n   // Accessors"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -15,9 +15,9 @@ class Location;\n namespace triton {\n using llvm::StringRef;\n \n-class PTXInstr;\n-class PTXInstrCommon;\n-class PTXInstrExecution;\n+struct PTXInstr;\n+struct PTXInstrCommon;\n+struct PTXInstrExecution;\n \n // PTXBuilder helps to manage a PTX asm program consists of one or multiple\n // instructions.\n@@ -83,7 +83,7 @@ struct PTXBuilder {\n     Operand() = default;\n     Operand(const Operation &) = delete;\n     Operand(Value value, StringRef constraint)\n-        : value(value), constraint(constraint) {}\n+        : constraint(constraint), value(value) {}\n \n     bool isList() const { return !value && constraint.empty(); }\n \n@@ -120,15 +120,15 @@ struct PTXBuilder {\n   Operand *newListOperand(unsigned count, mlir::Value val,\n                           const std::string &constraint) {\n     auto *list = newOperand();\n-    for (int i = 0; i < count; ++i) {\n+    for (unsigned i = 0; i < count; ++i) {\n       list->listAppend(newOperand(val, constraint));\n     }\n     return list;\n   }\n \n   Operand *newListOperand(unsigned count, const std::string &constraint) {\n     auto *list = newOperand();\n-    for (int i = 0; i < count; ++i) {\n+    for (unsigned i = 0; i < count; ++i) {\n       list->listAppend(newOperand(constraint));\n     }\n     return list;\n@@ -172,8 +172,8 @@ struct PTXBuilder {\n     return argArchive.back().get();\n   }\n \n-  friend class PTXInstr;\n-  friend class PTXInstrCommon;\n+  friend struct PTXInstr;\n+  friend struct PTXInstrCommon;\n \n protected:\n   llvm::SmallVector<std::unique_ptr<Operand>, 6> argArchive;\n@@ -209,7 +209,7 @@ struct PTXInstrCommon {\n   PTXBuilder *builder{};\n   llvm::SmallVector<std::string, 4> instrParts;\n \n-  friend class PTXInstrExecution;\n+  friend struct PTXInstrExecution;\n };\n \n template <class ConcreteT> struct PTXInstrBase : public PTXInstrCommon {\n@@ -309,7 +309,7 @@ struct PTXInstrExecution {\n   PTXInstrExecution() = default;\n   explicit PTXInstrExecution(PTXInstrCommon *instr,\n                              llvm::ArrayRef<Operand *> oprs)\n-      : instr(instr), argsInOrder(oprs.begin(), oprs.end()) {}\n+      : argsInOrder(oprs.begin(), oprs.end()), instr(instr) {}\n \n   // Prefix a predicate to the instruction.\n   PTXInstrExecution &predicate(mlir::Value value, StringRef constraint = \"b\") {"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -11,16 +11,12 @@ class ModuleOp;\n template <typename T> class OperationPass;\n \n class TritonLLVMConversionTarget : public ConversionTarget {\n-  mlir::LLVMTypeConverter &typeConverter;\n-\n public:\n   explicit TritonLLVMConversionTarget(MLIRContext &ctx,\n                                       mlir::LLVMTypeConverter &typeConverter);\n };\n \n class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n-  mlir::LLVMTypeConverter &typeConverter;\n-\n public:\n   explicit TritonLLVMFunctionConversionTarget(\n       MLIRContext &ctx, mlir::LLVMTypeConverter &typeConverter);"}, {"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -26,11 +26,11 @@ class DialectInferLayoutInterface\n   DialectInferLayoutInterface(Dialect *dialect) : Base(dialect) {}\n \n   virtual LogicalResult\n-  inferReduceOpEncoding(Attribute operandEncoding, int axis,\n+  inferReduceOpEncoding(Attribute operandEncoding, unsigned axis,\n                         Attribute &resultEncoding) const = 0;\n \n   virtual LogicalResult\n-  inferExpandDimsOpEncoding(Attribute operandEncoding, int axis,\n+  inferExpandDimsOpEncoding(Attribute operandEncoding, unsigned axis,\n                             Attribute &resultEncoding) const = 0;\n };\n "}, {"filename": "include/triton/Dialect/Triton/Transforms/Passes.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -8,7 +8,7 @@ namespace triton {\n \n std::unique_ptr<Pass> createCombineOpsPass();\n \n-}\n+} // namespace triton\n \n #define GEN_PASS_REGISTRATION\n #include \"triton/Dialect/Triton/Transforms/Passes.h.inc\""}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -21,7 +21,6 @@ class TritonGPUTypeConverter : public TypeConverter {\n };\n \n class TritonGPUConversionTarget : public ConversionTarget {\n-  TritonGPUTypeConverter &typeConverter;\n \n public:\n   explicit TritonGPUConversionTarget(MLIRContext &ctx,"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -82,7 +82,6 @@ SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op) {\n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n   auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n   auto srcShape = srcTy.getShape();\n-  auto rank = srcShape.size();\n   auto axis = op.axis();\n \n   bool fast_reduce = axis == 1; // FIXME(Qingyi): The fastest-changing dimension"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 4, "deletions": 10, "changes": 14, "file_content_changes": "@@ -66,7 +66,7 @@ AxisInfo AxisInfo::join(const AxisInfo &lhs, const AxisInfo &rhs) {\n   DimVectorT retContiguity;\n   DimVectorT retDivisibility;\n   DimVectorT retConstancy;\n-  for (size_t d = 0; d < lhs.getRank(); ++d) {\n+  for (int d = 0; d < lhs.getRank(); ++d) {\n     retContiguity.push_back(gcd(lhs.getContiguity(d), rhs.getContiguity(d)));\n     retDivisibility.push_back(\n         gcd(lhs.getDivisibility(d), rhs.getDivisibility(d)));\n@@ -88,7 +88,7 @@ AxisInfo AxisInfoAnalysis::visitBinaryOp(\n   AxisInfo::DimVectorT newContiguity;\n   AxisInfo::DimVectorT newDivisibility;\n   AxisInfo::DimVectorT newConstancy;\n-  for (size_t d = 0; d < rank; ++d) {\n+  for (int d = 0; d < rank; ++d) {\n     newContiguity.push_back(getContiguity(lhsInfo, rhsInfo, d));\n     newDivisibility.push_back(getDivisibility(lhsInfo, rhsInfo, d));\n     newConstancy.push_back(getConstancy(lhsInfo, rhsInfo, d));\n@@ -167,7 +167,7 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     AxisInfo::DimVectorT contiguity;\n     AxisInfo::DimVectorT divisibility;\n     AxisInfo::DimVectorT constancy;\n-    for (size_t d = 0; d < retTy.getRank(); ++d) {\n+    for (int d = 0; d < retTy.getRank(); ++d) {\n       contiguity.push_back(1);\n       divisibility.push_back(opInfo.getDivisibility(0));\n       constancy.push_back(retTy.getShape()[d]);\n@@ -176,12 +176,6 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n   }\n   // expandDims\n   if (auto expandDims = llvm::dyn_cast<triton::ExpandDimsOp>(op)) {\n-    Type _retTy = *op->result_type_begin();\n-    Type _opTy = *op->operand_type_begin();\n-    TensorType retTy = _retTy.cast<TensorType>();\n-    TensorType opTy = _opTy.cast<TensorType>();\n-    ArrayRef<int64_t> retShape = retTy.getShape();\n-    ArrayRef<int64_t> opShape = opTy.getShape();\n     AxisInfo opInfo = operands[0]->getValue();\n     AxisInfo::DimVectorT contiguity = opInfo.getContiguity();\n     AxisInfo::DimVectorT divisibility = opInfo.getDivisibility();\n@@ -203,7 +197,7 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     AxisInfo::DimVectorT contiguity;\n     AxisInfo::DimVectorT divisibility;\n     AxisInfo::DimVectorT constancy;\n-    for (size_t d = 0; d < retTy.getRank(); ++d) {\n+    for (int d = 0; d < retTy.getRank(); ++d) {\n       contiguity.push_back(opShape[d] == 1 ? 1 : opInfo.getContiguity(d));\n       divisibility.push_back(opInfo.getDivisibility(d));\n       constancy.push_back(opShape[d] == 1 ? retShape[d] : 1);"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -28,11 +28,10 @@ bool maybeSharedAllocationOp(Operation *op) {\n }\n \n std::string getValueOperandName(Value value, AsmState &state) {\n-  auto *op = value.getDefiningOp();\n   std::string opName;\n   llvm::raw_string_ostream ss(opName);\n   value.printAsOperand(ss, state);\n-  return std::move(opName);\n+  return opName;\n }\n \n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 63, "deletions": 86, "changes": 149, "file_content_changes": "@@ -82,6 +82,7 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n } // namespace\n \n // Shortcuts for some commonly used LLVM ops to keep code simple and intuitive\n+#define zext(...) rewriter.create<LLVM::ZExtOp>(loc, __VA_ARGS__)\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n #define urem(...) rewriter.create<LLVM::URemOp>(loc, __VA_ARGS__)\n #define add(...) rewriter.create<LLVM::AddOp>(loc, __VA_ARGS__)\n@@ -109,6 +110,8 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n #define store(val, ptr) rewriter.create<LLVM::StoreOp>(loc, val, ptr)\n #define icmp_eq(...)                                                           \\\n   rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::eq, __VA_ARGS__)\n+#define icmp_ne(...)                                                           \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ne, __VA_ARGS__)\n #define icmp_slt(...)                                                          \\\n   rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::slt, __VA_ARGS__)\n #define select(...) rewriter.create<LLVM::SelectOp>(loc, __VA_ARGS__)\n@@ -117,6 +120,7 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n #define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n #define i32_ty rewriter.getIntegerType(32)\n #define f16_ty rewriter.getF16Type()\n+#define i8_ty rewriter.getIntegerType(8)\n #define f32_ty rewriter.getF32Type()\n #define vec_ty(type, num) VectorType::get(num, type)\n #define f32_val(...) LLVM::createConstantF32(loc, rewriter, __VA_ARGS__)\n@@ -263,7 +267,6 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<FuncOp> {\n /// FuncOp legalization pattern that converts MemRef arguments to pointers to\n /// MemRef descriptors (LLVM struct data types) containing all the MemRef type\n /// information.\n-static constexpr StringRef kEmitIfaceAttrName = \"llvm.emit_c_interface\";\n struct FuncOpConversion : public FuncOpConversionBase {\n   FuncOpConversion(LLVMTypeConverter &converter, int numWarps,\n                    PatternBenefit benefit)\n@@ -301,7 +304,6 @@ struct ReturnOpConversion : public ConvertOpToLLVMPattern<::mlir::ReturnOp> {\n   LogicalResult\n   matchAndRewrite(ReturnOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    Location loc = op->getLoc();\n     unsigned numArguments = op.getNumOperands();\n \n     // Currently, Triton kernel function always return nothing.\n@@ -491,7 +493,6 @@ class ConvertTritonGPUOpToLLVMPattern\n                                 ConversionPatternRewriter &rewriter,\n                                 const BlockedEncodingAttr &blocked_layout,\n                                 ArrayRef<int64_t> shape) const {\n-    auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n     Value threadId = getThreadId(rewriter, loc);\n     Value warpSize = idx_val(32);\n     Value laneId = urem(threadId, warpSize);\n@@ -663,7 +664,6 @@ class ConvertTritonGPUOpToLLVMPattern\n     auto bufferId = allocation->getBufferId(value);\n     assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n     size_t offset = allocation->getOffset(bufferId);\n-    auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n     Value offVal = idx_val(offset);\n     Value base = gep(ptrTy, smem, offVal);\n     return base;\n@@ -693,7 +693,6 @@ Value convertSplatLikeOp(Type elemType, Type resType, Value constVal,\n   auto tensorTy = resType.cast<RankedTensorType>();\n   if (tensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n     auto tensorTy = resType.cast<RankedTensorType>();\n-    auto layout = tensorTy.getEncoding();\n     auto srcType = typeConverter->convertType(elemType);\n     auto llSrc = bitcast(constVal, srcType);\n     size_t elemsPerThread = getElemsPerThread(tensorTy);\n@@ -851,7 +850,6 @@ struct LoadOpConversion\n   LogicalResult\n   matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    MLIRContext *ctx = rewriter.getContext();\n     auto loc = op->getLoc();\n \n     // original values\n@@ -906,12 +904,11 @@ struct LoadOpConversion\n       // TODO: optimization when ptr is GEP with constant offset\n       size_t in_off = 0;\n \n-      const int maxWordWidth = std::max<int>(32, valueElemNbits);\n-      const int totalWidth = valueElemNbits * vec;\n-      const int width = std::min(totalWidth, maxWordWidth);\n-      const int nWords = std::max(1, totalWidth / width);\n-      const int wordNElems = width / valueElemNbits;\n-      const int vecNElems = totalWidth / valueElemNbits;\n+      const size_t maxWordWidth = std::max<size_t>(32, valueElemNbits);\n+      const size_t totalWidth = valueElemNbits * vec;\n+      const size_t width = std::min(totalWidth, maxWordWidth);\n+      const size_t nWords = std::max<size_t>(1, totalWidth / width);\n+      const size_t wordNElems = width / valueElemNbits;\n       assert(wordNElems * nWords * numVecs == numElems);\n \n       // TODO(Superjomn) Add cache policy fields to StoreOp.\n@@ -930,7 +927,7 @@ struct LoadOpConversion\n \n       // prepare asm operands\n       auto *dstsOpr = ptxBuilder.newListOperand();\n-      for (int wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n+      for (size_t wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n         auto *opr = ptxBuilder.newOperand(writeConstraint); // =r operations\n         dstsOpr->listAppend(opr);\n       }\n@@ -997,8 +994,8 @@ struct LoadOpConversion\n                        : retTys[0];\n \n       // TODO: if (has_l2_evict_policy)\n-      auto asmDialectAttr = LLVM::AsmDialectAttr::get(rewriter.getContext(),\n-                                                      LLVM::AsmDialect::AD_ATT);\n+      // auto asmDialectAttr = LLVM::AsmDialectAttr::get(rewriter.getContext(),\n+      //                                                 LLVM::AsmDialect::AD_ATT);\n       Value ret = ptxBuilder.launch(rewriter, loc, retTy);\n \n       // ---\n@@ -1088,27 +1085,25 @@ struct StoreOpConversion\n       // TODO: optimization when ptr is AddPtr with constant offset\n       size_t in_off = 0;\n \n-      const int maxWordWidth = std::max<int>(32, valueElemNbits);\n-      const int totalWidth = valueElemNbits * vec;\n-      const int width = std::min(totalWidth, maxWordWidth);\n-      const int nWords = std::max(1, totalWidth / width);\n-      const int wordNElems = width / valueElemNbits;\n-      const int vecNElems = totalWidth / valueElemNbits;\n+      const size_t maxWordWidth = std::max<size_t>(32, valueElemNbits);\n+      const size_t totalWidth = valueElemNbits * vec;\n+      const size_t width = std::min(totalWidth, maxWordWidth);\n+      const size_t nWords = std::max<size_t>(1, totalWidth / width);\n+      const size_t wordNElems = width / valueElemNbits;\n       assert(wordNElems * nWords * numVecs == numElems);\n \n       // TODO(Superjomn) Add cache policy fields to StoreOp.\n       // TODO(Superjomn) Deal with cache policy here.\n-      const bool hasL2EvictPolicy = false;\n \n       Type valArgTy = IntegerType::get(ctx, width);\n       auto wordTy = vec_ty(valueElemTy, wordNElems);\n \n       SmallVector<std::pair<Value, std::string>> asmArgs;\n-      for (int wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n+      for (size_t wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n         // llWord is a width-len composition\n         Value llWord = rewriter.create<LLVM::UndefOp>(loc, wordTy);\n         // Insert each value element to the composition\n-        for (int elemIdx = 0; elemIdx < wordNElems; ++elemIdx) {\n+        for (size_t elemIdx = 0; elemIdx < wordNElems; ++elemIdx) {\n           const size_t elemOffset = vecStart + wordIdx * wordNElems + elemIdx;\n           assert(elemOffset < valueElems.size());\n           Value elem = valueElems[elemOffset];\n@@ -1228,7 +1223,6 @@ struct BroadcastOpConversion\n     }\n \n     unsigned srcElems = getElemsPerThread(srcTy);\n-    auto elemTy = resultTy.getElementType();\n     auto srcVals = getElementsFromStruct(loc, src, rewriter);\n     unsigned resultElems = getElemsPerThread(resultTy);\n     SmallVector<Value> resultVals(resultElems);\n@@ -1290,8 +1284,6 @@ struct ReduceOpConversion\n LogicalResult\n ReduceOpConversion::matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter) const {\n-  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n-  auto rank = srcTy.getShape().size();\n   if (op.axis() == 1) // FIXME(Qingyi): The fastest-changing dimension\n     return matchAndRewriteFast(op, adaptor, rewriter);\n   return matchAndRewriteBasic(op, adaptor, rewriter);\n@@ -1340,7 +1332,6 @@ void ReduceOpConversion::accumulate(ConversionPatternRewriter &rewriter,\n \n Value ReduceOpConversion::shflSync(ConversionPatternRewriter &rewriter,\n                                    Location loc, Value val, int i) const {\n-  MLIRContext *ctx = rewriter.getContext();\n   unsigned bits = val.getType().getIntOrFloatBitWidth();\n \n   if (bits == 64) {\n@@ -1447,7 +1438,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n \n     barrier();\n     SmallVector<Value> resultVals(resultElems);\n-    for (int i = 0; i < resultElems; ++i) {\n+    for (unsigned i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       readIdx.insert(readIdx.begin() + axis, ints[0]);\n       Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n@@ -1479,7 +1470,6 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n   auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n   auto srcShape = srcTy.getShape();\n-  auto srcOrder = srcLayout.getOrder();\n \n   auto threadsPerWarp = srcLayout.getThreadsPerWarp();\n   auto warpsPerCTA = srcLayout.getWarpsPerCTA();\n@@ -1587,7 +1577,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n \n     barrier();\n     SmallVector<Value> resultVals(resultElems);\n-    for (int i = 0; i < resultElems; ++i) {\n+    for (size_t i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       readIdx.insert(readIdx.begin() + axis, i32_val(0));\n       Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n@@ -1627,7 +1617,6 @@ struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n     // due to MLIR's restrictions\n     Location loc = op->getLoc();\n     auto resultTy = op.getType().template cast<RankedTensorType>();\n-    auto resultShape = resultTy.getShape();\n     unsigned elems = getElemsPerThread(resultTy);\n     Type elemTy =\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n@@ -1706,7 +1695,6 @@ struct AddPtrOpConversion\n       auto resultLayout =\n           resultTensorTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n       assert(resultLayout && \"Unexpected resultLayout in AddPtrOpConversion\");\n-      auto resultShape = resultTensorTy.getShape();\n       unsigned elems = getElemsPerThread(resultTy);\n       Type elemTy =\n           getTypeConverter()->convertType(resultTensorTy.getElementType());\n@@ -1829,7 +1817,7 @@ class ElementwiseOpConversionBase\n     SmallVector<SmallVector<Value>> operands(elems);\n     for (auto operand : adaptor.getOperands()) {\n       auto sub_operands = this->getElementsFromStruct(loc, operand, rewriter);\n-      for (int i = 0; i < elems; ++i) {\n+      for (size_t i = 0; i < elems; ++i) {\n         operands[i].push_back(sub_operands[i]);\n       }\n     }\n@@ -1939,6 +1927,7 @@ struct CmpFOpConversion\n       __PRED_ENUM(ORD, ord);\n       __PRED_ENUM(UEQ, ueq);\n       __PRED_ENUM(UGT, ugt);\n+      __PRED_ENUM(UGE, uge);\n       __PRED_ENUM(ULT, ult);\n       __PRED_ENUM(ULE, ule);\n       __PRED_ENUM(UNE, une);\n@@ -2042,21 +2031,24 @@ void ConvertLayoutOpConversion::processReplica(\n   auto rank = type.getRank();\n   auto sizePerThread = getSizePerThread(layout);\n   auto accumSizePerThread = product<unsigned>(sizePerThread);\n-  auto llvmIndexTy = getTypeConverter()->getIndexType();\n   SmallVector<unsigned> numCTAs(rank);\n   auto shapePerCTA = getShapePerCTA(layout);\n   for (unsigned d = 0; d < rank; ++d) {\n     numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n   }\n-  auto llvmElemTy = getTypeConverter()->convertType(type.getElementType());\n+  auto elemTy = type.getElementType();\n+  bool isInt1 = elemTy.isInteger(1);\n+  if (isInt1)\n+    elemTy = IntegerType::get(elemTy.getContext(), 8);\n+  auto llvmElemTy = getTypeConverter()->convertType(elemTy);\n+\n   SmallVector<Value> multiDimOffsetFirstElem;\n   SmallVector<Value> mmaColIdx(2);\n   SmallVector<Value> mmaRowIdx(2);\n   if (blockedLayout) {\n     multiDimOffsetFirstElem = emitBaseIndexForBlockedLayout(\n         loc, rewriter, blockedLayout, type.getShape());\n   } else if (sliceLayout) {\n-    unsigned dim = sliceLayout.getDim();\n     auto parent = sliceLayout.getParent();\n     if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n       SmallVector<int64_t> paddedShape =\n@@ -2156,16 +2148,22 @@ void ConvertLayoutOpConversion::processReplica(\n       if (stNotRd) {\n         Value valVec = undef(vecTy);\n         for (unsigned v = 0; v < vec; ++v) {\n-          valVec = insert_element(\n-              vecTy, valVec,\n-              vals[elemId + linearCTAId * accumSizePerThread + v], idx_val(v));\n+          auto currVal = vals[elemId + linearCTAId * accumSizePerThread + v];\n+          if (isInt1)\n+            currVal = zext(llvmElemTy, currVal);\n+\n+          valVec = insert_element(vecTy, valVec, currVal, idx_val(v));\n         }\n         store(valVec, ptr);\n       } else {\n         Value valVec = load(ptr);\n         for (unsigned v = 0; v < vec; ++v) {\n-          vals[elemId + linearCTAId * accumSizePerThread + v] =\n-              extract_element(llvmElemTy, valVec, idx_val(v));\n+          Value currVal = extract_element(llvmElemTy, valVec, idx_val(v));\n+          if (isInt1)\n+            currVal =\n+                icmp_ne(currVal, rewriter.create<LLVM::ConstantOp>(\n+                                     loc, i8_ty, rewriter.getI8IntegerAttr(0)));\n+          vals[elemId + linearCTAId * accumSizePerThread + v] = currVal;\n         }\n       }\n     }\n@@ -2208,7 +2206,7 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n   }\n   // Potentially we need to store for multiple CTAs in this replication\n   unsigned accumNumReplicates = product<unsigned>(numReplicates);\n-  unsigned elems = getElemsPerThread(srcTy);\n+  // unsigned elems = getElemsPerThread(srcTy);\n   auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n   unsigned inVec = 0;\n   unsigned outVec = 0;\n@@ -2376,17 +2374,17 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n // Data loader for mma.16816 instruction.\n class MMA16816SmemLoader {\n public:\n-  MMA16816SmemLoader(int wpt, ArrayRef<uint32_t> order, int kOrder,\n+  MMA16816SmemLoader(int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n                      ArrayRef<int64_t> tileShape, ArrayRef<int> instrShape,\n                      ArrayRef<int> matShape, int perPhase, int maxPhase,\n                      int elemBytes, ConversionPatternRewriter &rewriter,\n                      TypeConverter *typeConverter, const Location &loc)\n-      : wpt(wpt), order(order.begin(), order.end()), kOrder(kOrder),\n+      : order(order.begin(), order.end()), kOrder(kOrder),\n         tileShape(tileShape.begin(), tileShape.end()),\n         instrShape(instrShape.begin(), instrShape.end()),\n         matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n-        maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter),\n-        typeConverter(typeConverter), loc(loc), ctx(rewriter.getContext()) {\n+        maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter), loc(loc),\n+        ctx(rewriter.getContext()) {\n     cMatShape = matShape[order[0]];\n     sMatShape = matShape[order[1]];\n \n@@ -2585,7 +2583,6 @@ class MMA16816SmemLoader {\n     assert(mat0 % 2 == 0 && mat1 % 2 == 0 &&\n            \"smem matrix load must be aligned\");\n     int matIdx[2] = {mat0, mat1};\n-    int k = matIdx[kOrder];\n \n     int ptrIdx{-1};\n \n@@ -2605,7 +2602,6 @@ class MMA16816SmemLoader {\n \n     Value ptr = getPtr(ptrIdx);\n \n-    Value resV4;\n     if (canUseLdmatrix) {\n       int sOffset =\n           matIdx[order[1]] * sMatStride * sMatShape * sTileStride * elemBytes;\n@@ -2736,7 +2732,6 @@ class MMA16816SmemLoader {\n   }\n \n private:\n-  int wpt;\n   SmallVector<uint32_t> order;\n   int kOrder;\n   SmallVector<int64_t> tileShape;\n@@ -2746,7 +2741,6 @@ class MMA16816SmemLoader {\n   int maxPhase;\n   int elemBytes;\n   ConversionPatternRewriter &rewriter;\n-  TypeConverter *typeConverter{};\n   const Location &loc;\n   MLIRContext *ctx{};\n \n@@ -2795,14 +2789,9 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n   LogicalResult\n   matchAndRewrite(triton::DotOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    Location loc = op->getLoc();\n     // D = A * B + C\n     Value A = op.a();\n-    Value B = op.b();\n-    Value C = op.c();\n     Value D = op.getResult();\n-    MLIRContext *ctx = op->getContext();\n-    bool allowTF32 = op.allowTF32();\n \n     // Here we assume the DotOp's operands always comes from shared memory.\n     auto AShape = A.getType().cast<RankedTensorType>().getShape();\n@@ -3036,8 +3025,6 @@ struct DotOpMmaV2ConversionHelper {\n     Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n     Type i8x4Pack4Ty =\n         LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i8x4Ty));\n-    Type i32Pack4Ty = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(4, type::i32Ty(ctx)));\n \n     switch (mmaType) {\n     case TensorCoreType::FP32_FP16_FP16_FP32:\n@@ -3147,7 +3134,6 @@ struct DotOpMmaV2ConversionHelper {\n     auto bTy = B.getType().cast<RankedTensorType>();\n     // d = a*b + c\n     auto dTy = op.d().getType().cast<RankedTensorType>();\n-    auto mmaLayout = dTy.getEncoding().cast<MmaEncodingAttr>();\n \n     if (dTy.getElementType().isF32()) {\n       if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n@@ -3253,9 +3239,9 @@ struct MMA16816ConversionHelper {\n   MMA16816ConversionHelper(MmaEncodingAttr mmaLayout, Value thread,\n                            ConversionPatternRewriter &rewriter,\n                            TypeConverter *typeConverter, Location loc)\n-      : mmaLayout(mmaLayout), helper(mmaLayout), rewriter(rewriter),\n-        typeConverter(typeConverter), loc(loc), ctx(mmaLayout.getContext()),\n-        thread(thread) {\n+      : mmaLayout(mmaLayout), thread(thread), helper(mmaLayout),\n+        rewriter(rewriter), typeConverter(typeConverter), loc(loc),\n+        ctx(mmaLayout.getContext()) {\n     wpt = mmaLayout.getWarpsPerCTA();\n \n     Value _32 = i32_val(32);\n@@ -3369,8 +3355,8 @@ struct MMA16816ConversionHelper {\n     }\n \n     // step1. Perform loading.\n-    for (unsigned m = 0; m < numRepM; ++m)\n-      for (unsigned k = 0; k < numRepK; ++k)\n+    for (int m = 0; m < numRepM; ++m)\n+      for (int k = 0; k < numRepK; ++k)\n         loadFn(2 * m, 2 * k);\n \n     // step2. Format the values to LLVM::Struct to passing to mma codegen.\n@@ -3393,8 +3379,8 @@ struct MMA16816ConversionHelper {\n         0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShpae*/,\n         {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/);\n \n-    for (unsigned n = 0; n < std::max(numRepN / 2, 1); ++n) {\n-      for (unsigned k = 0; k < numRepK; ++k)\n+    for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n+      for (int k = 0; k < numRepK; ++k)\n         loadFn(2 * n, 2 * k);\n     }\n \n@@ -3430,17 +3416,12 @@ struct MMA16816ConversionHelper {\n     helper.deduceMmaType(op);\n \n     auto aTensorTy = a.getType().cast<RankedTensorType>();\n-    auto bTensorTy = b.getType().cast<RankedTensorType>();\n-    auto cTensorTy = c.getType().cast<RankedTensorType>();\n     auto dTensorTy = d.getType().cast<RankedTensorType>();\n \n     auto aShape = aTensorTy.getShape();\n     auto dShape = dTensorTy.getShape();\n \n-    int NK = aShape[1];\n     // shape / shape_per_cta\n-    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n-    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(aTensorTy);\n     int numRepM = getNumRepM(aTensorTy, dShape[0]);\n     int numRepN = getNumRepN(aTensorTy, dShape[1]);\n     int numRepK = getNumRepK(aTensorTy, aShape[1]);\n@@ -3483,9 +3464,9 @@ struct MMA16816ConversionHelper {\n             extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(i));\n     };\n \n-    for (unsigned k = 0; k < numRepK; ++k)\n-      for (unsigned m = 0; m < numRepM; ++m)\n-        for (unsigned n = 0; n < numRepN; ++n)\n+    for (int k = 0; k < numRepK; ++k)\n+      for (int m = 0; m < numRepM; ++m)\n+        for (int n = 0; n < numRepN; ++n)\n           callMma(2 * m, n, 2 * k);\n \n     // replace with new packed result\n@@ -3500,7 +3481,7 @@ struct MMA16816ConversionHelper {\n private:\n   std::function<void(int, int)>\n   getLoadMatrixFn(Value tensor, Value llTensor, MmaEncodingAttr mmaLayout,\n-                  int wpt, int kOrder, ArrayRef<int> instrShape,\n+                  int wpt, uint32_t kOrder, ArrayRef<int> instrShape,\n                   ArrayRef<int> matShape, Value warpId,\n                   ValueTable &vals) const {\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n@@ -3574,8 +3555,8 @@ struct MMA16816ConversionHelper {\n   Value composeValuesToDotOperandLayoutStruct(const ValueTable &vals, int n0,\n                                               int n1) const {\n     std::vector<Value> elems;\n-    for (unsigned m = 0; m < n0; ++m)\n-      for (unsigned k = 0; k < n1; ++k) {\n+    for (int m = 0; m < n0; ++m)\n+      for (int k = 0; k < n1; ++k) {\n         elems.push_back(vals.at({2 * m, 2 * k}));\n         elems.push_back(vals.at({2 * m, 2 * k + 1}));\n         elems.push_back(vals.at({2 * m + 1, 2 * k}));\n@@ -3617,10 +3598,8 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n   auto loc = op.getLoc();\n   Value src = op.src();\n   Value dst = op.result();\n-  auto srcTensorTy = src.getType().cast<RankedTensorType>();\n   auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n \n-  auto sharedLayout = srcTensorTy.getEncoding().cast<SharedEncodingAttr>();\n   auto dotOperandLayout =\n       dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n \n@@ -4272,7 +4251,7 @@ struct AsyncWaitOpConversion\n     auto ctx = op.getContext();\n     auto loc = op.getLoc();\n     auto voidTy = void_ty(ctx);\n-    auto ret = ptxBuilder.launch(rewriter, loc, voidTy);\n+    ptxBuilder.launch(rewriter, loc, voidTy);\n \n     // Safe to remove the op since it doesn't have any return value.\n     rewriter.eraseOp(op);\n@@ -4361,12 +4340,10 @@ struct InsertSliceAsyncOpConversion\n     unsigned perPhase = resSharedLayout.getPerPhase();\n     unsigned maxPhase = resSharedLayout.getMaxPhase();\n     auto sizePerThread = srcBlockedLayout.getSizePerThread();\n-    auto threadsPerWarp = srcBlockedLayout.getThreadsPerWarp();\n-    auto warpsPerCTA = srcBlockedLayout.getWarpsPerCTA();\n     auto threadsPerCTA = getThreadsPerCTA(srcBlockedLayout);\n \n     auto inOrder = srcBlockedLayout.getOrder();\n-    auto outOrder = resSharedLayout.getOrder();\n+\n     // If perPhase * maxPhase > threadsPerCTA, we need to swizzle over\n     // elements across phases. If perPhase * maxPhase == threadsPerCTA,\n     // swizzle is not allowd\n@@ -4447,7 +4424,7 @@ struct InsertSliceAsyncOpConversion\n       auto resByteWidth = resElemTy.getIntOrFloatBitWidth() / 8;\n \n       auto tileOffset = tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}];\n-      for (unsigned wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n+      for (size_t wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n         PTXBuilder ptxBuilder;\n         auto wordElemIdx = wordIdx * numWordElems;\n         auto &copyAsyncOp =\n@@ -4769,7 +4746,7 @@ namespace mlir {\n \n TritonLLVMConversionTarget::TritonLLVMConversionTarget(\n     MLIRContext &ctx, mlir::LLVMTypeConverter &typeConverter)\n-    : ConversionTarget(ctx), typeConverter(typeConverter) {\n+    : ConversionTarget(ctx) {\n   addLegalDialect<LLVM::LLVMDialect>();\n   addLegalDialect<NVVM::NVVMDialect>();\n   // addIllegalDialect<triton::TritonDialect>();\n@@ -4781,7 +4758,7 @@ TritonLLVMConversionTarget::TritonLLVMConversionTarget(\n \n TritonLLVMFunctionConversionTarget::TritonLLVMFunctionConversionTarget(\n     MLIRContext &ctx, mlir::LLVMTypeConverter &typeConverter)\n-    : ConversionTarget(ctx), typeConverter(typeConverter) {\n+    : ConversionTarget(ctx) {\n   addLegalDialect<LLVM::LLVMDialect>();\n   // addLegalDialect<NVVM::NVVMDialect>();\n   addIllegalOp<mlir::FuncOp>();"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 9, "deletions": 16, "changes": 25, "file_content_changes": "@@ -21,9 +21,7 @@ template <class Op> class GenericOpPattern : public OpConversionPattern<Op> {\n   matchAndRewrite(Op op, typename Op::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    Op res =\n-        rewriter.replaceOpWithNewOp<Op>(op, retType, adaptor.getOperands());\n-\n+    rewriter.replaceOpWithNewOp<Op>(op, retType, adaptor.getOperands());\n     return success();\n   }\n };\n@@ -37,9 +35,8 @@ class ArithCmpPattern : public OpConversionPattern<SrcOp> {\n   matchAndRewrite(SrcOp op, typename SrcOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    DstOp res =\n-        rewriter.replaceOpWithNewOp<DstOp>(op, retType, adaptor.getPredicate(),\n-                                           adaptor.getLhs(), adaptor.getRhs());\n+    rewriter.replaceOpWithNewOp<DstOp>(op, retType, adaptor.getPredicate(),\n+                                       adaptor.getLhs(), adaptor.getRhs());\n     return success();\n   }\n };\n@@ -129,10 +126,9 @@ class StdSelectPattern : public OpConversionPattern<SelectOp> {\n   matchAndRewrite(SelectOp op, typename SelectOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    triton::gpu::SelectOp res =\n-        rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n-            op, retType, adaptor.getCondition(), adaptor.getTrueValue(),\n-            adaptor.getFalseValue());\n+    rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n+        op, retType, adaptor.getCondition(), adaptor.getTrueValue(),\n+        adaptor.getFalseValue());\n     return success();\n   }\n };\n@@ -204,9 +200,6 @@ struct TritonExpandDimsPattern\n         triton::gpu::BlockedEncodingAttr::get(getContext(), retSizePerThread,\n                                               retThreadsPerWarp, retWarpsPerCTA,\n                                               retOrder);\n-    // return type\n-    RankedTensorType retType =\n-        RankedTensorType::get(retShape, argType.getElementType(), retEncoding);\n     // convert operand to slice of return type\n     Attribute newArgEncoding = triton::gpu::SliceEncodingAttr::get(\n         getContext(), op.axis(), retEncoding);\n@@ -252,7 +245,7 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n                                            bType.getElementType(), encoding);\n       b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), dstType, b);\n     }\n-    auto newDot = rewriter.replaceOpWithNewOp<triton::DotOp>(\n+    rewriter.replaceOpWithNewOp<triton::DotOp>(\n         op, retType, a, b, adaptor.c(), adaptor.allowTF32(), adaptor.transA(),\n         adaptor.transB());\n     return success();\n@@ -279,7 +272,7 @@ struct TritonStorePattern : public OpConversionPattern<triton::StoreOp> {\n   LogicalResult\n   matchAndRewrite(triton::StoreOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto newOp = rewriter.replaceOpWithNewOp<triton::StoreOp>(\n+    rewriter.replaceOpWithNewOp<triton::StoreOp>(\n         op, adaptor.ptr(), adaptor.value(), adaptor.mask());\n     return success();\n   }\n@@ -340,7 +333,7 @@ struct TritonReducePattern : public OpConversionPattern<triton::ReduceOp> {\n   LogicalResult\n   matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto newOp = rewriter.replaceOpWithNewOp<triton::ReduceOp>(\n+    rewriter.replaceOpWithNewOp<triton::ReduceOp>(\n         op, adaptor.redOp(), adaptor.operand(), adaptor.axis());\n     return success();\n   }"}, {"filename": "lib/Dialect/Triton/IR/Dialect.cpp", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "file_content_changes": "@@ -8,11 +8,30 @@\n \n #include \"mlir/IR/DialectImplementation.h\"\n \n+#include \"mlir/Transforms/InliningUtils.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.cpp.inc\"\n \n using namespace mlir;\n using namespace mlir::triton;\n \n+//===----------------------------------------------------------------------===//\n+// TritonDialect Dialect Interfaces\n+//===----------------------------------------------------------------------===//\n+\n+namespace {\n+struct TritonInlinerInterface : public DialectInlinerInterface {\n+  using DialectInlinerInterface::DialectInlinerInterface;\n+  bool isLegalToInline(Region *dest, Region *src, bool wouldBeCloned,\n+                       BlockAndValueMapping &valueMapping) const final {\n+    return true;\n+  }\n+  bool isLegalToInline(Operation *, Region *, bool wouldBeCloned,\n+                       BlockAndValueMapping &) const final {\n+    return true;\n+  }\n+};\n+} // namespace\n+\n void TritonDialect::initialize() {\n   registerTypes();\n \n@@ -22,6 +41,7 @@ void TritonDialect::initialize() {\n       >();\n \n   // We can also add interface here.\n+  addInterfaces<TritonInlinerInterface>();\n }\n \n Operation *TritonDialect::materializeConstant(OpBuilder &builder,"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "file_content_changes": "@@ -49,7 +49,6 @@ unsigned getElemsPerThread(Type type) {\n   auto tensorType = type.cast<RankedTensorType>();\n   auto layout = tensorType.getEncoding();\n   auto shape = tensorType.getShape();\n-  size_t rank = shape.size();\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return blockedLayout.getElemsPerThread(shape);\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n@@ -109,15 +108,15 @@ SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout) {\n SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n   SmallVector<unsigned> shape;\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-    for (int d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n+    for (unsigned d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n       shape.push_back(blockedLayout.getSizePerThread()[d] *\n                       blockedLayout.getThreadsPerWarp()[d] *\n                       blockedLayout.getWarpsPerCTA()[d]);\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     unsigned dim = sliceLayout.getDim();\n     auto parent = sliceLayout.getParent();\n     if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n-      for (int d = 0, n = blockedParent.getOrder().size(); d < n; ++d) {\n+      for (unsigned d = 0, n = blockedParent.getOrder().size(); d < n; ++d) {\n         if (d == dim)\n           continue;\n         shape.push_back(blockedParent.getSizePerThread()[d] *\n@@ -258,7 +257,6 @@ SliceEncodingAttr::paddedShape(ArrayRef<int64_t> shape) const {\n unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   size_t rank = shape.size();\n   auto parent = getParent();\n-  unsigned dim = getDim();\n   if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n     assert(rank == blockedParent.getSizePerThread().size() - 1 &&\n            \"unexpected rank in SliceEncodingAttr::getElemsPerThread\");\n@@ -527,11 +525,11 @@ mlir::LogicalResult ExtractSliceOp::inferReturnTypes(\n   auto encoding = srcType.getEncoding();\n   auto srcShape = srcType.getShape();\n   auto axis = attributes.get(\"axis\").cast<IntegerAttr>().getInt();\n-  if (axis < 0 || axis > srcShape.size())\n+  if (axis < 0 || (size_t)axis > srcShape.size())\n     return failure();\n   SmallVector<int64_t, 4> dstShape;\n-  for (int i = 0; i < srcShape.size(); i++)\n-    if (i != axis)\n+  for (size_t i = 0; i < srcShape.size(); i++)\n+    if (i != (size_t)axis)\n       dstShape.push_back(srcShape[i]);\n   auto returnType =\n       RankedTensorType::get(dstShape, srcType.getElementType(), encoding);\n@@ -593,15 +591,17 @@ struct TritonGPUInferLayoutInterface\n     : public triton::DialectInferLayoutInterface {\n   using DialectInferLayoutInterface::DialectInferLayoutInterface;\n \n-  LogicalResult inferReduceOpEncoding(Attribute operandEncoding, int axis,\n-                                      Attribute &resultEncoding) const {\n+  LogicalResult\n+  inferReduceOpEncoding(Attribute operandEncoding, unsigned axis,\n+                        Attribute &resultEncoding) const override {\n     resultEncoding = SliceEncodingAttr::get(getDialect()->getContext(), axis,\n                                             operandEncoding);\n     return success();\n   }\n \n-  LogicalResult inferExpandDimsOpEncoding(Attribute operandEncoding, int axis,\n-                                          Attribute &resultEncoding) const {\n+  LogicalResult\n+  inferExpandDimsOpEncoding(Attribute operandEncoding, unsigned axis,\n+                            Attribute &resultEncoding) const override {\n     auto sliceEncoding = operandEncoding.dyn_cast<SliceEncodingAttr>();\n     if (!sliceEncoding) {\n       llvm::report_fatal_error("}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 5, "deletions": 12, "changes": 17, "file_content_changes": "@@ -87,7 +87,6 @@ class SimplifyConversion : public mlir::RewritePattern {\n     if (!llvm::isa<triton::gpu::ConvertLayoutOp>(op))\n       return mlir::failure();\n     auto convert = llvm::cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto srcType = convert.getOperand().getType().cast<RankedTensorType>();\n     auto dstType = convert.getType().cast<RankedTensorType>();\n     // we don't handle conversions to DotOperandEncodingAttr\n     // this is a heuristics to accomodate fused attention\n@@ -219,10 +218,10 @@ Operation *cloneWithInferType(mlir::PatternRewriter &rewriter, Operation *op,\n   auto typeInfer = dyn_cast<InferTypeOpInterface>(newOp);\n   if (typeInfer) {\n     SmallVector<Type, 1> newType;\n-    auto sucess = typeInfer.inferReturnTypes(\n+    auto success = typeInfer.inferReturnTypes(\n         newOp->getContext(), newOp->getLoc(), newOp->getOperands(),\n         newOp->getAttrDictionary(), newOp->getRegions(), newType);\n-    if (success)\n+    if (succeeded(success))\n       newOp->getResult(0).setType(newType.front());\n   }\n   return newOp;\n@@ -364,10 +363,6 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n   rematerializeForLoop(mlir::PatternRewriter &rewriter, scf::ForOp &forOp,\n                        size_t i, RankedTensorType newType,\n                        triton::gpu::ConvertLayoutOp origConversion) const {\n-\n-    auto newEncoding = newType.cast<RankedTensorType>().getEncoding();\n-    auto ctx = forOp.getContext();\n-    auto isInLoop = [&](Operation *op) { return op->getParentOp() == forOp; };\n     // Rewrite init argument\n     Type origType = forOp.getInitArgs()[i].getType();\n     SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n@@ -418,11 +413,10 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n     return newResults;\n   }\n \n-  mlir::LogicalResult matchAndRewrite(mlir::Operation *op,\n-                                      mlir::PatternRewriter &rewriter) const {\n-\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n     auto forOp = cast<scf::ForOp>(op);\n-    auto isInLoop = [&](Operation *op) { return op->getParentOp() == forOp; };\n     auto iterArgs = forOp.getRegionIterArgs();\n     for (auto iterArg : llvm::enumerate(iterArgs)) {\n       // if (iterArg.index() != 1)\n@@ -480,7 +474,6 @@ class RematerializeForward : public mlir::RewritePattern {\n     auto forOp = dyn_cast<scf::ForOp>(cvt->getParentOp());\n     if (!forOp)\n       return mlir::failure();\n-    auto yieldOp = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n     auto isInLoop = [&](Operation *op) { return op->getParentOp() == forOp; };\n \n     SetVector<Operation *> cvtSlices;"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 11, "deletions": 28, "changes": 39, "file_content_changes": "@@ -17,11 +17,6 @@ using namespace mlir;\n \n namespace {\n class LoopPipeliner {\n-  /// comments on numStages:\n-  ///   [0, numStages-1) are in the prologue\n-  ///   numStages-1 is appended after the loop body\n-  int numStages;\n-\n   /// cache forOp we are working on\n   scf::ForOp forOp;\n \n@@ -43,6 +38,11 @@ class LoopPipeliner {\n   ///\n   Value loopIterIdx;\n \n+  /// comments on numStages:\n+  ///   [0, numStages-1) are in the prologue\n+  ///   numStages-1 is appended after the loop body\n+  int numStages;\n+\n   /// value (in loop) => value at stage N\n   DenseMap<Value, SmallVector<Value>> valueMapping;\n \n@@ -58,9 +58,6 @@ class LoopPipeliner {\n \n   Value lookupOrDefault(Value origin, int stage);\n \n-  /// return true if this op uses any of `loads`\n-  bool isDirectUserOfAsyncLoad(Operation &op);\n-\n   /// returns a empty buffer of size <numStages, ...>\n   triton::gpu::AllocTensorOp allocateEmptyBuffer(Operation *op,\n                                                  OpBuilder &builder);\n@@ -84,7 +81,7 @@ class LoopPipeliner {\n   /// create the new ForOp (add new args & insert prefetched ops)\n   scf::ForOp createNewForOp();\n \n-  friend class PipelinePass;\n+  friend struct PipelinePass;\n };\n \n // helpers\n@@ -123,19 +120,6 @@ void LoopPipeliner::collectDeps(Value v, int stages, DenseSet<Value> &deps) {\n   }\n }\n \n-bool LoopPipeliner::isDirectUserOfAsyncLoad(Operation &op) {\n-  for (Value loadOp : loads) {\n-    assert(loadOp.hasOneUse() &&\n-           \"load should only have one use (ConvertLayout)\");\n-    Value loadUseResult = loadOp.getUsers().begin()->getResult(0);\n-    for (Value opOperand : op.getOperands()) {\n-      if (opOperand == loadUseResult)\n-        return true;\n-    }\n-  }\n-  return false;\n-}\n-\n triton::gpu::AllocTensorOp\n LoopPipeliner::allocateEmptyBuffer(Operation *op, OpBuilder &builder) {\n   // allocate a buffer for each pipelined tensor\n@@ -356,8 +340,8 @@ void LoopPipeliner::emitPrologue() {\n   } // for (int stage = 0; stage < numStages - 1; ++stage)\n \n   // async.wait & extract_slice\n-  Operation *asyncWait = builder.create<triton::gpu::AsyncWaitOp>(\n-      loads[0].getLoc(), loads.size() * (numStages - 2));\n+  builder.create<triton::gpu::AsyncWaitOp>(loads[0].getLoc(),\n+                                           loads.size() * (numStages - 2));\n   loopIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n   for (Value loadOp : loads) {\n     Value extractSlice = builder.create<triton::gpu::ExtractSliceOp>(\n@@ -380,8 +364,7 @@ void LoopPipeliner::emitEpilogue() {\n   OpBuilder builder(forOp);\n   OpBuilder::InsertionGuard g(builder);\n   builder.setInsertionPointAfter(forOp);\n-  Operation *asyncWait =\n-      builder.create<triton::gpu::AsyncWaitOp>(forOp.getLoc(), 0);\n+  builder.create<triton::gpu::AsyncWaitOp>(forOp.getLoc(), 0);\n }\n \n scf::ForOp LoopPipeliner::createNewForOp() {\n@@ -575,8 +558,8 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   yieldValues.push_back(loopIterIdx);\n \n   builder.setInsertionPointToEnd(newForOp.getBody());\n-  auto test = builder.create<scf::YieldOp>(\n-      forOp.getBody()->getTerminator()->getLoc(), yieldValues);\n+  builder.create<scf::YieldOp>(forOp.getBody()->getTerminator()->getLoc(),\n+                               yieldValues);\n   return newForOp;\n }\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Swizzle.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -30,7 +30,7 @@ struct SwizzlePass : public TritonGPUSwizzleBase<SwizzlePass> {\n                           (ty.getElementType().getIntOrFloatBitWidth() / 8));\n     perPhase = std::max<int>(perPhase, 1);\n     // index of the inner dimension in `order`\n-    int inner = (opIdx == 0) ? 0 : 1;\n+    size_t inner = (opIdx == 0) ? 0 : 1;\n     if (version == 1) {\n       int maxPhase = (order[inner] == 1 ? 8 : 4) / perPhase;\n       // TODO: handle rep (see\n@@ -67,7 +67,6 @@ struct SwizzlePass : public TritonGPUSwizzleBase<SwizzlePass> {\n \n   void runOnOperation() override {\n     Operation *op = getOperation();\n-    MLIRContext *context = &getContext();\n     op->walk([&](triton::DotOp dotOp) -> void {\n       OpBuilder builder(dotOp);\n       auto _retEncoding ="}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -73,7 +73,7 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n //\n TritonGPUConversionTarget::TritonGPUConversionTarget(\n     MLIRContext &context, TritonGPUTypeConverter &typeConverter)\n-    : ConversionTarget(context), typeConverter(typeConverter) {\n+    : ConversionTarget(context) {\n   // TODO: we should also verify ops of TritonGPUDialect\n   addLegalDialect<triton::gpu::TritonGPUDialect>();\n \n@@ -90,7 +90,7 @@ TritonGPUConversionTarget::TritonGPUConversionTarget(\n   });\n \n   // We have requirements for the data layouts\n-  addDynamicallyLegalOp<triton::DotOp>([this](triton::DotOp dotOp) -> bool {\n+  addDynamicallyLegalOp<triton::DotOp>([](triton::DotOp dotOp) -> bool {\n     Attribute aEncoding =\n         dotOp.a().getType().cast<RankedTensorType>().getEncoding();\n     Attribute bEncoding ="}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -63,7 +63,7 @@ static bool find_and_replace(std::string &str, const std::string &begin,\n static std::string llir_to_ptx(llvm::Module *module, int capability, int ptx) {\n   // LLVM version in use may not officially support target hardware\n   int max_nvvm_cc = 75;\n-  int max_nvvm_ptx = 74;\n+  // int max_nvvm_ptx = 74;\n   // options\n   auto options = llvm::cl::getRegisteredOptions();\n   auto *short_ptr ="}, {"filename": "python/setup.py", "status": "modified", "additions": 8, "deletions": 5, "changes": 13, "file_content_changes": "@@ -1,5 +1,4 @@\n import distutils\n-import distutils.spawn\n import os\n import platform\n import re\n@@ -39,12 +38,13 @@ class Package(NamedTuple):\n     test_file: str\n     include_flag: str\n     lib_flag: str\n+    syspath_var_name: str\n \n \n def get_pybind11_package_info():\n     name = \"pybind11-2.10.0\"\n     url = \"https://github.com/pybind/pybind11/archive/refs/tags/v2.10.0.tar.gz\"\n-    return Package(\"pybind11\", name, url, \"include/pybind11/pybind11.h\", \"PYBIND11_INCLUDE_DIR\", \"\")\n+    return Package(\"pybind11\", name, url, \"include/pybind11/pybind11.h\", \"PYBIND11_INCLUDE_DIR\", \"\", \"PYBIND11_SYSPATH\")\n \n \n def get_llvm_package_info():\n@@ -58,7 +58,7 @@ def get_llvm_package_info():\n     else:\n         name = 'clang+llvm-14.0.0-x86_64-{}'.format(system_suffix)\n         url = \"https://github.com/llvm/llvm-project/releases/download/llvmorg-14.0.0/{}.tar.xz\".format(name)\n-    return Package(\"llvm\", name, url, \"lib\", \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\")\n+    return Package(\"llvm\", name, url, \"lib\", \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n \n \n def get_thirdparty_packages(triton_cache_path):\n@@ -68,6 +68,8 @@ def get_thirdparty_packages(triton_cache_path):\n         package_root_dir = os.path.join(triton_cache_path, p.package)\n         package_dir = os.path.join(package_root_dir, p.name)\n         test_file_path = os.path.join(package_dir, p.test_file)\n+        if p.syspath_var_name in os.environ:\n+            package_dir = os.environ[p.syspath_var_name]\n         if not os.path.exists(test_file_path):\n             try:\n                 shutil.rmtree(package_root_dir)\n@@ -136,14 +138,15 @@ def build_extension(self, ext):\n         if not os.path.exists(llvm_build_dir):\n             os.makedirs(llvm_build_dir)\n         # python directories\n-        python_include_dirs = [distutils.sysconfig.get_python_inc()] + ['/usr/local/cuda/include']\n+        python_include_dir = distutils.sysconfig.get_python_inc()\n         cmake_args = [\n+            \"-DLLVM_ENABLE_WERROR=ON\",\n             \"-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=\" + extdir,\n             \"-DTRITON_BUILD_TUTORIALS=OFF\",\n             \"-DTRITON_BUILD_PYTHON_MODULE=ON\",\n             # '-DPYTHON_EXECUTABLE=' + sys.executable,\n             # '-DCMAKE_VERBOSE_MAKEFILE:BOOL=ON',\n-            \"-DPYTHON_INCLUDE_DIRS=\" + \";\".join(python_include_dirs),\n+            \"-DPYTHON_INCLUDE_DIRS=\" + python_include_dir,\n             \"-DLLVM_EXTERNAL_LIT=\" + lit_dir\n         ] + thirdparty_cmake_args\n "}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 29, "deletions": 27, "changes": 56, "file_content_changes": "@@ -26,6 +26,7 @@\n #include \"llvm/IR/Module.h\"\n #include \"llvm/IR/Verifier.h\"\n #include \"llvm/IRReader/IRReader.h\"\n+#include \"llvm/Support/FileUtilities.h\"\n #include \"llvm/Support/raw_ostream.h\"\n \n #include \"llvm/Support/SourceMgr.h\"\n@@ -169,9 +170,8 @@ void init_triton_ir(py::module &&m) {\n       .def(\"replace_all_uses_with\",\n            [](mlir::Value &self, mlir::Value &newValue) {\n              self.replaceAllUsesWith(newValue);\n-           })\n+           });\n \n-      ;\n   py::class_<mlir::BlockArgument, mlir::Value>(m, \"block_arguement\");\n \n   py::class_<mlir::Region>(m, \"region\")\n@@ -422,7 +422,12 @@ void init_triton_ir(py::module &&m) {\n       .def(\"get_int32_attr\", &mlir::OpBuilder::getI32IntegerAttr)\n       // Use arith.ConstantOp to create constants\n       // // Constants\n-      // .def(\"get_int1\", &ir::builder::get_int1, ret::reference)\n+      .def(\"get_int1\",\n+           [](mlir::OpBuilder &self, bool v) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 loc, v, self.getI1Type()));\n+           })\n       .def(\"get_int32\",\n            [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n@@ -659,13 +664,13 @@ void init_triton_ir(py::module &&m) {\n              auto loc = self.getUnknownLoc();\n              // get element type if necessary\n              mlir::Type srcType = src.getType();\n+             auto srcTensorType = srcType.dyn_cast<mlir::RankedTensorType>();\n+             auto dstTensorType = dstType.dyn_cast<mlir::RankedTensorType>();\n              mlir::Type srcEltType = srcType;\n              mlir::Type dstEltType = dstType;\n-             if (dstType.isa<mlir::RankedTensorType>()) {\n-               dstEltType =\n-                   dstType.cast<mlir::RankedTensorType>().getElementType();\n-               srcEltType =\n-                   srcType.cast<mlir::RankedTensorType>().getElementType();\n+             if (dstTensorType && srcTensorType) {\n+               dstEltType = dstTensorType.getElementType();\n+               srcEltType = srcTensorType.getElementType();\n              }\n              unsigned srcWidth = srcEltType.getIntOrFloatBitWidth();\n              unsigned dstWidth = dstEltType.getIntOrFloatBitWidth();\n@@ -1301,39 +1306,36 @@ void init_triton_translation(py::module &m) {\n           py::gil_scoped_release allow_threads;\n \n           // compile ptx with ptxas\n-          char _fsrc[L_tmpnam];\n-          char _flog[L_tmpnam];\n-          std::tmpnam(_fsrc);\n-          std::tmpnam(_flog);\n-          std::string fsrc = _fsrc;\n-          std::string flog = _flog;\n-          std::string fbin = fsrc + \".o\";\n+          llvm::SmallString<64> fsrc;\n+          llvm::SmallString<64> flog;\n+          llvm::sys::fs::createTemporaryFile(\"compile-ptx-src\", \"\", fsrc);\n+          llvm::sys::fs::createTemporaryFile(\"compile-ptx-log\", \"\", flog);\n+          std::string fbin = std::string(fsrc) + \".o\";\n+          llvm::FileRemover srcRemover(fsrc);\n+          llvm::FileRemover logRemover(flog);\n+          llvm::FileRemover binRemover(fbin);\n+          const char *_fsrc = fsrc.c_str();\n+          const char *_flog = flog.c_str();\n           const char *_fbin = fbin.c_str();\n-          std::ofstream ofs(fsrc);\n+          std::ofstream ofs(_fsrc);\n           ofs << ptxCode << std::endl;\n           ofs.close();\n           std::string cmd;\n           int err;\n           cmd = ptxasPath + \" -v --gpu-name=sm_\" + std::to_string(capability) +\n-                \" \" + fsrc + \" -o \" + fsrc + \".o 2> \" + flog;\n+                \" \" + _fsrc + \" -o \" + _fsrc + \".o 2> \" + _flog;\n           err = system(cmd.c_str());\n           if (err != 0) {\n             std::ifstream _log(_flog);\n             std::string log(std::istreambuf_iterator<char>(_log), {});\n-            unlink(_fsrc);\n-            unlink(_flog);\n             throw std::runtime_error(\"Internal Triton PTX codegen error: \\n\" +\n                                      log);\n           }\n           std::ifstream _cubin(_fbin, std::ios::binary);\n           std::string cubin(std::istreambuf_iterator<char>(_cubin), {});\n           _cubin.close();\n-          unlink(_fsrc);\n-          unlink(_flog);\n-          unlink(_fbin);\n-\n           py::bytes bytes(cubin);\n-          return bytes;\n+          return std::move(bytes);\n         });\n \n   m.def(\"add_external_libs\",\n@@ -1345,8 +1347,8 @@ void init_triton_translation(py::module &m) {\n \n void init_triton(py::module &m) {\n   py::module subm = m.def_submodule(\"triton\");\n-  // init_triton_codegen(std::move(subm.def_submodule(\"code_gen\")));\n-  init_triton_runtime(std::move(subm.def_submodule(\"runtime\")));\n-  init_triton_ir(std::move(subm.def_submodule(\"ir\")));\n+  // init_triton_codegen(subm.def_submodule(\"code_gen\"));\n+  init_triton_runtime(subm.def_submodule(\"runtime\"));\n+  init_triton_ir(subm.def_submodule(\"ir\"));\n   init_triton_translation(subm);\n }"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 227, "deletions": 224, "changes": 451, "file_content_changes": "@@ -411,41 +411,40 @@ def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n     assert (z == to_numpy(z_tri)).all()\n \n \n-# TODO: wrong result\n-# def test_where_broadcast():\n-#     @triton.jit\n-#     def where_kernel(cond_ptr, a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n-#         xoffsets = tl.arange(0, BLOCK_SIZE)[:, None]\n-#         yoffsets = tl.arange(0, BLOCK_SIZE)[None, :]\n+def test_where_broadcast():\n+    @triton.jit\n+    def where_kernel(cond_ptr, a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n+        xoffsets = tl.arange(0, BLOCK_SIZE)[:, None]\n+        yoffsets = tl.arange(0, BLOCK_SIZE)[None, :]\n \n-#         mask = tl.load(cond_ptr + yoffsets)\n-#         vals = tl.load(a_ptr + yoffsets + BLOCK_SIZE * xoffsets)\n-#         res = tl.where(mask, vals, 0.)\n-#         tl.store(out_ptr + yoffsets + BLOCK_SIZE * xoffsets, res)\n+        mask = tl.load(cond_ptr + yoffsets)\n+        vals = tl.load(a_ptr + yoffsets + BLOCK_SIZE * xoffsets)\n+        res = tl.where(mask, vals, 0.)\n+        tl.store(out_ptr + yoffsets + BLOCK_SIZE * xoffsets, res)\n \n-#     @triton.jit\n-#     def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n-#         xoffsets = tl.arange(0, BLOCK_SIZE)[:, None]\n-#         yoffsets = tl.arange(0, BLOCK_SIZE)[None, :]\n-#         mask = 0\n-#         vals = tl.load(a_ptr + yoffsets + BLOCK_SIZE * xoffsets)\n-#         res = tl.where(mask, vals, 0.)\n-#         tl.store(out_ptr + yoffsets + BLOCK_SIZE * xoffsets, res)\n-\n-#     SIZE = 32\n-#     dtype = 'float32'\n-#     rs = RandomState(17)\n-#     x = numpy_random((SIZE, SIZE), dtype_str=dtype, rs=rs)\n-#     mask = numpy_random(SIZE, 'bool', rs=rs)\n-#     z = np.where(mask, x, 0)\n-#     cond_tri = to_triton(mask, device=\"cuda\")\n-#     x_tri = to_triton(x, device='cuda', dst_type=dtype)\n-#     z_tri = to_triton(np.empty((SIZE, SIZE), dtype=z.dtype), device='cuda', dst_type=dtype)\n-#     where_kernel[(1,)](cond_tri, x_tri, z_tri, SIZE)\n-#     assert (z == to_numpy(z_tri)).all()\n-#     where_scalar_condition[(1,)](x_tri, z_tri, SIZE)\n-#     z = np.where(0, x, 0)\n-#     assert (z == to_numpy(z_tri)).all()\n+    @triton.jit\n+    def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n+        xoffsets = tl.arange(0, BLOCK_SIZE)[:, None]\n+        yoffsets = tl.arange(0, BLOCK_SIZE)[None, :]\n+        mask = 0\n+        vals = tl.load(a_ptr + yoffsets + BLOCK_SIZE * xoffsets)\n+        res = tl.where(mask, vals, 0.)\n+        tl.store(out_ptr + yoffsets + BLOCK_SIZE * xoffsets, res)\n+\n+    SIZE = 32\n+    dtype = 'float32'\n+    rs = RandomState(17)\n+    x = numpy_random((SIZE, SIZE), dtype_str=dtype, rs=rs)\n+    mask = numpy_random(SIZE, 'bool', rs=rs)\n+    z = np.where(mask, x, 0)\n+    cond_tri = to_triton(mask, device=\"cuda\")\n+    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n+    z_tri = to_triton(np.empty((SIZE, SIZE), dtype=z.dtype), device='cuda', dst_type=dtype)\n+    where_kernel[(1,)](cond_tri, x_tri, z_tri, SIZE)\n+    assert (z == to_numpy(z_tri)).all()\n+    where_scalar_condition[(1,)](x_tri, z_tri, SIZE)\n+    z = np.where(0, x, 0)\n+    assert (z == to_numpy(z_tri)).all()\n \n # # ---------------\n # # test unary ops\n@@ -719,7 +718,7 @@ def without_fn(X, Y, A, B, C):\n     # ('bfloat16', 'float32', False),\n     ('float32', 'int32', True),\n     # TODO:\n-    # ('float32', 'int1', False),\n+    ('float32', 'int1', False),\n ] + [\n     (f'uint{x}', f'int{x}', True) for x in [8, 16, 32, 64]\n ] + [\n@@ -1178,20 +1177,20 @@ def kernel(X, stride_xm, stride_xn,\n # # ---------------\n \n \n-# @pytest.mark.parametrize(\"start\", [0, 1, 7, 16])\n-# def test_arange(start, device='cuda'):\n-#     BLOCK = 128\n-#     z_tri = torch.empty(BLOCK, dtype=torch.int32, device=device)\n+@pytest.mark.parametrize(\"start\", [0, 1, 7, 16])\n+def test_arange(start, device='cuda'):\n+    BLOCK = 128\n+    z_tri = torch.empty(BLOCK, dtype=torch.int32, device=device)\n \n-#     @triton.jit\n-#     def _kernel(z, BLOCK: tl.constexpr,\n-#                 START: tl.constexpr, END: tl.constexpr):\n-#         off = tl.arange(0, BLOCK)\n-#         val = tl.arange(START, END)\n-#         tl.store(z + off, val)\n-#     _kernel[(1,)](z_tri, START=start, END=start + BLOCK, BLOCK=BLOCK)\n-#     z_ref = torch.arange(start, BLOCK + start, dtype=torch.int32, device=device)\n-#     triton.testing.assert_almost_equal(z_tri, z_ref)\n+    @triton.jit\n+    def _kernel(z, BLOCK: tl.constexpr,\n+                START: tl.constexpr, END: tl.constexpr):\n+        off = tl.arange(0, BLOCK)\n+        val = tl.arange(START, END)\n+        tl.store(z + off, val)\n+    _kernel[(1,)](z_tri, START=start, END=start + BLOCK, BLOCK=BLOCK)\n+    z_ref = torch.arange(start, BLOCK + start, dtype=torch.int32, device=device)\n+    triton.testing.assert_almost_equal(z_tri, z_ref)\n \n # # ---------------\n # # test load\n@@ -1249,47 +1248,47 @@ def kernel(X, stride_xm, stride_xn,\n #     triton.testing.allclose(out, reference_out)\n \n \n-# @pytest.mark.parametrize(\"cache\", [\"\", \".ca\", \".cg\"])\n-# def test_load_cache_modifier(cache):\n-#     src = torch.empty(128, device='cuda')\n-#     dst = torch.empty(128, device='cuda')\n+@pytest.mark.parametrize(\"cache\", [\"\", \".ca\", \".cg\"])\n+def test_load_cache_modifier(cache):\n+    src = torch.empty(128, device='cuda')\n+    dst = torch.empty(128, device='cuda')\n \n-#     @triton.jit\n-#     def _kernel(dst, src, CACHE: tl.constexpr):\n-#         offsets = tl.arange(0, 128)\n-#         x = tl.load(src + offsets, cache_modifier=CACHE)\n-#         tl.store(dst + offsets, x)\n+    @triton.jit\n+    def _kernel(dst, src, CACHE: tl.constexpr):\n+        offsets = tl.arange(0, 128)\n+        x = tl.load(src + offsets, cache_modifier=CACHE)\n+        tl.store(dst + offsets, x)\n \n-#     pgm = _kernel[(1,)](dst, src, CACHE=cache)\n-#     ptx = pgm.asm['ptx']\n-#     if cache == '':\n-#         assert 'ld.global.ca' not in ptx\n-#         assert 'ld.global.cg' not in ptx\n-#     if cache == '.cg':\n-#         assert 'ld.global.cg' in ptx\n-#         assert 'ld.global.ca' not in ptx\n-#     if cache == '.ca':\n-#         assert 'ld.global.ca' in ptx\n-#         assert 'ld.global.cg' not in ptx\n-\n-\n-# @pytest.mark.parametrize(\"N\", [16, 10, 11, 1024])\n-# def test_vectorization(N):\n-#     src = torch.empty(1024, device='cuda')\n-#     dst = torch.empty(1024, device='cuda')\n+    pgm = _kernel[(1,)](dst, src, CACHE=cache)\n+    ptx = pgm.asm['ptx']\n+    if cache == '':\n+        assert 'ld.global.ca' not in ptx\n+        assert 'ld.global.cg' not in ptx\n+    if cache == '.cg':\n+        assert 'ld.global.cg' in ptx\n+        assert 'ld.global.ca' not in ptx\n+    if cache == '.ca':\n+        assert 'ld.global.ca' in ptx\n+        assert 'ld.global.cg' not in ptx\n+\n+\n+@pytest.mark.parametrize(\"N\", [16, 10, 11, 1024])\n+def test_vectorization(N):\n+    src = torch.empty(1024, device='cuda')\n+    dst = torch.empty(1024, device='cuda')\n \n-#     @triton.jit\n-#     def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n-#         offsets = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n-#         x = tl.load(src + offsets, mask=offsets < N)\n-#         tl.store(dst + offsets, x, mask=offsets < N)\n-#     pgm = _kernel[(1,)](dst, src, N=N, BLOCK_SIZE=src.shape[0])\n-#     ptx = pgm.asm[\"ptx\"]\n-#     if N % 16 == 0:\n-#         assert \"ld.global.v4.b32\" in ptx\n-#     else:\n-#         assert \"ld.global.b32\" in ptx\n-#     # triton.testing.assert_almost_equal(dst, src[:N])\n+    @triton.jit\n+    def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n+        offsets = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+        x = tl.load(src + offsets, mask=offsets < N)\n+        tl.store(dst + offsets, x, mask=offsets < N)\n+    pgm = _kernel[(1,)](dst, src, N=N, BLOCK_SIZE=src.shape[0])\n+    ptx = pgm.asm[\"ptx\"]\n+    if N % 16 == 0:\n+        assert \"ld.global.v4.b32\" in ptx\n+    else:\n+        assert \"ld.global.b32\" in ptx\n+    # triton.testing.assert_almost_equal(dst, src[:N])\n # # ---------------\n # # test store\n # # ---------------\n@@ -1312,205 +1311,209 @@ def kernel(X, stride_xm, stride_xn,\n # # TODO: can't be local to test_default\n \n \n-# @triton.jit\n-# def _impl(value=10):\n-#     return value\n+@triton.jit\n+def _impl(value=10):\n+    return value\n \n \n-# def test_default():\n-#     value = 5\n-#     ret0 = torch.zeros(1, dtype=torch.int32, device='cuda')\n-#     ret1 = torch.zeros(1, dtype=torch.int32, device='cuda')\n+def test_default():\n+    value = 5\n+    ret0 = torch.zeros(1, dtype=torch.int32, device='cuda')\n+    ret1 = torch.zeros(1, dtype=torch.int32, device='cuda')\n \n-#     @triton.jit\n-#     def _kernel(ret0, ret1, value):\n-#         tl.store(ret0, _impl())\n-#         tl.store(ret1, _impl(value))\n+    @triton.jit\n+    def _kernel(ret0, ret1, value):\n+        tl.store(ret0, _impl())\n+        tl.store(ret1, _impl(value))\n \n-#     _kernel[(1,)](ret0, ret1, value)\n-#     assert ret0.item() == 10\n-#     assert ret1.item() == value\n+    _kernel[(1,)](ret0, ret1, value)\n+    assert ret0.item() == 10\n+    assert ret1.item() == value\n \n # # ---------------\n # # test noop\n # # ----------------\n \n \n-# def test_noop(device='cuda'):\n-#     @triton.jit\n-#     def kernel(x):\n-#         pass\n-#     x = to_triton(numpy_random((1,), dtype_str='int32'), device=device)\n-#     kernel[(1, )](x)\n+def test_noop(device='cuda'):\n+    @triton.jit\n+    def kernel(x):\n+        pass\n+    x = to_triton(numpy_random((1,), dtype_str='int32'), device=device)\n+    kernel[(1, )](x)\n \n \n-# @pytest.mark.parametrize(\"value, value_type\", [\n-#     (-1, 'i32'), (0, 'i32'), (-2**31, 'i32'), (2**31 - 1, 'i32'),\n-#     (2**31, 'u32'), (2**32 - 1, 'u32'), (2**32, 'i64'), (2**63 - 1, 'i64'),\n-#     (-2**63, 'i64'), (2**63, 'u64'), (2**64 - 1, 'u64')\n-# ])\n-# def test_value_specialization(value: int, value_type: str, device='cuda') -> None:\n-#     spec_type = None\n+@pytest.mark.parametrize(\"value, value_type\", [\n+    (-1, 'i32'), (0, 'i32'), (-2**31, 'i32'), (2**31 - 1, 'i32'),\n+    (2**31, 'u32'), (2**32 - 1, 'u32'), (2**32, 'i64'), (2**63 - 1, 'i64'),\n+    (-2**63, 'i64'), (2**63, 'u64'), (2**64 - 1, 'u64')\n+])\n+def test_value_specialization(value: int, value_type: str, device='cuda') -> None:\n+    spec_type = None\n \n-#     def cache_hook(*args, **kwargs):\n-#         nonlocal spec_type\n-#         spec_type = kwargs[\"compile\"][\"signature\"][0]\n-#     JITFunction.cache_hook = cache_hook\n+    def cache_hook(*args, **kwargs):\n+        nonlocal spec_type\n+        spec_type = kwargs[\"compile\"][\"signature\"][0]\n+    JITFunction.cache_hook = cache_hook\n \n-#     @triton.jit\n-#     def kernel(VALUE, X):\n-#         pass\n+    @triton.jit\n+    def kernel(VALUE, X):\n+        pass\n \n-#     x = torch.tensor([3.14159], device='cuda')\n-#     pgm = kernel[(1, )](value, x)\n+    x = torch.tensor([3.14159], device='cuda')\n+    pgm = kernel[(1, )](value, x)\n \n-#     JITFunction.cache_hook = None\n-#     assert spec_type == value_type\n+    JITFunction.cache_hook = None\n+    assert spec_type == value_type\n \n+# # --------------------\n+# # value specialization\n+# # --------------------\n \n-# @pytest.mark.parametrize(\n-#     \"value, overflow\",\n-#     [(2**64 - 1, False), (2**64, True), (-2**63, False), (-2**63 - 1, True)]\n-# )\n-# def test_value_specialization_overflow(value: int, overflow: bool, device='cuda') -> None:\n \n-#     @triton.jit\n-#     def kernel(VALUE, X):\n-#         pass\n+@pytest.mark.parametrize(\n+    \"value, overflow\",\n+    [(2**64 - 1, False), (2**64, True), (-2**63, False), (-2**63 - 1, True)]\n+)\n+def test_value_specialization_overflow(value: int, overflow: bool, device='cuda') -> None:\n \n-#     x = torch.tensor([3.14159], device='cuda')\n+    @triton.jit\n+    def kernel(VALUE, X):\n+        pass\n \n-#     if overflow:\n-#         with pytest.raises(OverflowError):\n-#             kernel[(1, )](value, x)\n-#     else:\n-#         kernel[(1, )](value, x)\n+    x = torch.tensor([3.14159], device='cuda')\n+\n+    if overflow:\n+        with pytest.raises(OverflowError):\n+            kernel[(1, )](value, x)\n+    else:\n+        kernel[(1, )](value, x)\n \n \n # # ----------------\n # # test constexpr\n # # ----------------\n \n-# @pytest.mark.parametrize(\"op\", ['+', '-', '*', '/', '%', '<', '>'])\n-# @pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n-# @pytest.mark.parametrize(\"is_rhs_constexpr\", [True, False])\n-# def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr):\n+@pytest.mark.parametrize(\"op\", ['+', '-', '*', '/', '%', '<', '>'])\n+@pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n+@pytest.mark.parametrize(\"is_rhs_constexpr\", [True, False])\n+def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr):\n \n-#     @triton.jit\n-#     def kernel(Z, X, Y):\n-#         x = tl.load(X)\n-#         y = tl.load(Y)\n-#         z = GENERATE_TEST_HERE\n-#         tl.store(Z, z)\n-\n-#     x_str = \"3.14\" if is_lhs_constexpr else \"x\"\n-#     y_str = \"4.13\" if is_rhs_constexpr else \"y\"\n-#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{x_str} {op} {y_str}\"})\n-#     x = numpy_random((1,), dtype_str=\"float32\")\n-#     y = numpy_random((1,), dtype_str=\"float32\")\n-#     z = np.array(eval(f\"{x_str} {op} {y_str}\"))\n-#     x_tri = to_triton(x)\n-#     y_tri = to_triton(y)\n-#     z_tri = to_triton(np.empty((1,), dtype=z.dtype))\n-#     kernel[(1,)](z_tri, x_tri, y_tri)\n-#     np.testing.assert_allclose(z, to_numpy(z_tri))\n+    @triton.jit\n+    def kernel(Z, X, Y):\n+        x = tl.load(X)\n+        y = tl.load(Y)\n+        z = GENERATE_TEST_HERE\n+        tl.store(Z, z)\n \n+    x_str = \"3.14\" if is_lhs_constexpr else \"x\"\n+    y_str = \"4.13\" if is_rhs_constexpr else \"y\"\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{x_str} {op} {y_str}\"})\n+    x = numpy_random((1,), dtype_str=\"float32\")\n+    y = numpy_random((1,), dtype_str=\"float32\")\n+    z = np.array(eval(f\"{x_str} {op} {y_str}\"))\n+    x_tri = to_triton(x)\n+    y_tri = to_triton(y)\n+    z_tri = to_triton(np.empty((1,), dtype=z.dtype))\n+    kernel[(1,)](z_tri, x_tri, y_tri)\n+    np.testing.assert_allclose(z, to_numpy(z_tri))\n \n-# def test_constexpr_shape():\n \n-#     @triton.jit\n-#     def kernel(X):\n-#         off = tl.arange(0, 128 + 128)\n-#         tl.store(X + off, off)\n+def test_constexpr_shape():\n \n-#     x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n-#     kernel[(1,)](x_tri)\n-#     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256))\n+    @triton.jit\n+    def kernel(X):\n+        off = tl.arange(0, 128 + 128)\n+        tl.store(X + off, off)\n \n+    x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+    kernel[(1,)](x_tri)\n+    np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256))\n \n-# def test_constexpr_scalar_shape():\n \n-#     @triton.jit\n-#     def kernel(X, s):\n-#         off = tl.arange(0, 256)\n-#         val = off % (256 // s)\n-#         tl.store(X + off, val)\n+def test_constexpr_scalar_shape():\n \n-#     x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n-#     kernel[(1,)](x_tri, 32)\n-#     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256) % 8)\n+    @triton.jit\n+    def kernel(X, s):\n+        off = tl.arange(0, 256)\n+        val = off % (256 // s)\n+        tl.store(X + off, val)\n+\n+    x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+    kernel[(1,)](x_tri, 32)\n+    np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256) % 8)\n \n # # -------------\n # # test call\n # # -------------\n \n \n-# @triton.jit\n-# def val_multiplier(val, i):\n-#     return val * i\n+@triton.jit\n+def val_multiplier(val, i):\n+    return val * i\n \n \n-# @triton.jit\n-# def vecmul_kernel(ptr, n_elements, rep):\n-#     pid = tl.program_id(axis=0)\n-#     offsets = pid * 128 + tl.arange(0, 128)\n-#     mask = offsets < n_elements\n-#     vec = tl.load(ptr + offsets, mask=mask)\n-#     for i in range(1, rep):\n-#         vec = val_multiplier(vec, i)\n-#     tl.store(ptr + offsets, vec, mask=mask)\n+@triton.jit\n+def vecmul_kernel(ptr, n_elements, rep):\n+    pid = tl.program_id(axis=0)\n+    offsets = pid * 128 + tl.arange(0, 128)\n+    mask = offsets < n_elements\n+    vec = tl.load(ptr + offsets, mask=mask)\n+    for i in range(1, rep):\n+        vec = val_multiplier(vec, i)\n+    tl.store(ptr + offsets, vec, mask=mask)\n \n \n-# def test_call():\n+def test_call():\n \n-#     @triton.jit\n-#     def kernel(ptr, n_elements, num1, num2):\n-#         vecmul_kernel(ptr, n_elements, num1)\n-#         vecmul_kernel(ptr, n_elements, num2)\n+    @triton.jit\n+    def kernel(ptr, n_elements, num1, num2):\n+        vecmul_kernel(ptr, n_elements, num1)\n+        vecmul_kernel(ptr, n_elements, num2)\n \n-#     size = 1024\n-#     rand_val = numpy_random((size,), dtype_str=\"float32\")\n-#     rand_val_tri = to_triton(rand_val, device='cuda')\n-#     kernel[(size // 128,)](rand_val_tri, size, 3, 5)\n+    size = 1024\n+    rand_val = numpy_random((size,), dtype_str=\"float32\")\n+    rand_val_tri = to_triton(rand_val, device='cuda')\n+    kernel[(size // 128,)](rand_val_tri, size, 3, 5)\n \n-#     ans = rand_val * 1 * 2 * 1 * 2 * 3 * 4\n-#     np.testing.assert_equal(to_numpy(rand_val_tri), ans)\n+    ans = rand_val * 1 * 2 * 1 * 2 * 3 * 4\n+    np.testing.assert_equal(to_numpy(rand_val_tri), ans)\n \n # # -------------\n # # test if\n # # -------------\n \n \n-# def test_if():\n+def test_if():\n \n-#     @triton.jit\n-#     def kernel(Cond, XTrue, XFalse, Ret):\n-#         pid = tl.program_id(0)\n-#         cond = tl.load(Cond)\n-#         if pid % 2:\n-#             tl.store(Ret, tl.load(XTrue))\n-#         else:\n-#             tl.store(Ret, tl.load(XFalse))\n+    @triton.jit\n+    def kernel(Cond, XTrue, XFalse, Ret):\n+        pid = tl.program_id(0)\n+        cond = tl.load(Cond)\n+        if pid % 2:\n+            tl.store(Ret, tl.load(XTrue))\n+        else:\n+            tl.store(Ret, tl.load(XFalse))\n \n-#     cond = torch.ones(1, dtype=torch.int32, device='cuda')\n-#     x_true = torch.tensor([3.14], dtype=torch.float32, device='cuda')\n-#     x_false = torch.tensor([1.51], dtype=torch.float32, device='cuda')\n-#     ret = torch.empty(1, dtype=torch.float32, device='cuda')\n-#     kernel[(1,)](cond, x_true, x_false, ret)\n+    cond = torch.ones(1, dtype=torch.int32, device='cuda')\n+    x_true = torch.tensor([3.14], dtype=torch.float32, device='cuda')\n+    x_false = torch.tensor([1.51], dtype=torch.float32, device='cuda')\n+    ret = torch.empty(1, dtype=torch.float32, device='cuda')\n+    kernel[(1,)](cond, x_true, x_false, ret)\n \n \n-# def test_num_warps_pow2():\n-#     dst = torch.empty(128, device='cuda')\n+def test_num_warps_pow2():\n+    dst = torch.empty(128, device='cuda')\n \n-#     @triton.jit\n-#     def _kernel(dst):\n-#         pass\n-\n-#     with pytest.raises(AssertionError, match='must be a power of 2'):\n-#         _kernel[(1,)](dst=dst, num_warps=3)\n-#     _kernel[(1,)](dst=dst, num_warps=1)\n-#     _kernel[(1,)](dst=dst, num_warps=2)\n-#     _kernel[(1,)](dst=dst, num_warps=4)\n+    @triton.jit\n+    def _kernel(dst):\n+        pass\n+\n+    with pytest.raises(AssertionError, match='must be a power of 2'):\n+        _kernel[(1,)](dst=dst, num_warps=3)\n+    _kernel[(1,)](dst=dst, num_warps=1)\n+    _kernel[(1,)](dst=dst, num_warps=2)\n+    _kernel[(1,)](dst=dst, num_warps=4)\n \n # # -------------\n # # test extern"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 10, "deletions": 7, "changes": 17, "file_content_changes": "@@ -196,7 +196,7 @@ def visit_Return(self, node):\n             return tuple(ret_types)\n         else:\n             ret = triton.language.core._to_tensor(ret_value, self.builder)\n-            self.builder.ret([ret_value.handle])\n+            self.builder.ret([ret.handle])\n             return ret.type\n \n     def visit_FunctionDef(self, node):\n@@ -399,13 +399,15 @@ def visit_If(self, node):\n                     if_op = self.builder.create_if_op([ty.to_ir(self.builder) for ty in ret_types], cond.handle, True)\n                     then_block.merge_block_before(if_op.get_then_block())\n                     self.builder.set_insertion_point_to_end(if_op.get_then_block())\n-                    self.builder.create_yield_op([then_defs[n].handle for n in names])\n+                    if len(names) > 0:\n+                        self.builder.create_yield_op([then_defs[n].handle for n in names])\n                     if not node.orelse:\n                         else_block = if_op.get_else_block()\n                     else:\n                         else_block.merge_block_before(if_op.get_else_block())\n                     self.builder.set_insertion_point_to_end(if_op.get_else_block())\n-                    self.builder.create_yield_op([else_defs[n].handle for n in names])\n+                    if len(names) > 0:\n+                        self.builder.create_yield_op([else_defs[n].handle for n in names])\n                 else:  # no else block\n                     if_op = self.builder.create_if_op([ty.to_ir(self.builder) for ty in ret_types], cond.handle, False)\n                     then_block.merge_block_before(if_op.get_then_block())\n@@ -526,7 +528,8 @@ def visit_While(self, node):\n                                                                 [ty.to_ir(self.builder) for ty in ret_types])\n             loop_block.merge_block_before(after_block)\n             self.builder.set_insertion_point_to_end(after_block)\n-            self.builder.create_yield_op([y.handle for y in yields])\n+            if len(yields) > 0:\n+                self.builder.create_yield_op([y.handle for y in yields])\n \n         # update global uses in while_op\n         for i, name in enumerate(names):\n@@ -685,8 +688,7 @@ def visit_Call(self, node):\n             fn_name = mangle_fn(fn.__name__, arg_types, constants)\n             # generate function def if necessary\n             if not self.module.has_function(fn_name):\n-                ret_type = triton.language.void\n-                prototype = triton.language.function_type([ret_type], arg_types)\n+                prototype = triton.language.function_type([], arg_types)\n                 gscope = sys.modules[fn.fn.__module__].__dict__\n                 generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module, function_name=fn_name, function_types=self.function_ret_types)\n                 generator.visit(fn.parse())\n@@ -696,7 +698,7 @@ def visit_Call(self, node):\n                 callee_ret_type = self.function_ret_types[fn_name]\n             symbol = self.module.get_function(fn_name)\n             call_op = self.builder.call(symbol, arg_vals)\n-            if call_op.get_num_results() == 0:\n+            if call_op.get_num_results() == 0 or callee_ret_type is None:\n                 return None\n             elif call_op.get_num_results() == 1:\n                 return triton.language.tensor(call_op.get_result(0), callee_ret_type)\n@@ -993,6 +995,7 @@ def _compile(fn, signature: str, device: int = -1, constants=dict(), specializat\n     module = optimize_tritongpu_ir(module, num_stages)\n     if output == \"ttgir\":\n         return module.str()\n+\n     if extern_libs:\n         add_external_libs(module, extern_libs)\n "}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -473,6 +473,11 @@ def __mod__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n         return semantic.mod(self, other, _builder)\n \n+    @builtin\n+    def __rmod__(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n+        return semantic.mod(other, self, _builder)\n+\n     # unary operators\n     @builtin\n     def __neg__(self, _builder=None):\n@@ -541,6 +546,7 @@ def __lt__(self, other, _builder=None):\n \n     @builtin\n     def __rlt__(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n         return semantic.less_than(other, self, _builder)\n \n     # <="}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 14, "deletions": 6, "changes": 20, "file_content_changes": "@@ -649,19 +649,27 @@ def cast(input: tl.tensor,\n     if src_sca_ty.is_int() and dst_sca_ty.is_int() and \\\n        (src_sca_ty.int_bitwidth != dst_sca_ty.int_bitwidth or src_sca_ty.int_signedness != dst_sca_ty.int_signedness):\n         sign_extend = src_sca_ty.is_int_signed() and not src_sca_ty.is_bool()\n-        return tl.tensor(builder.create_int_cast(input.handle,\n-                                                 dst_ty.to_ir(builder), sign_extend),\n-                         dst_ty)\n+        if dst_sca_ty.is_bool():\n+            ty = input.dtype.to_ir(builder)\n+            _0 = tl.tensor(builder.get_null_value(ty), input.dtype)\n+            return not_equal(input, _0, builder)\n+        else:\n+            return tl.tensor(builder.create_int_cast(input.handle,\n+                                                     dst_ty.to_ir(builder), sign_extend),\n+                             dst_ty)\n \n     # Float to Int\n     if src_sca_ty.is_floating() and dst_sca_ty.is_int():\n-        # TODO: is this correct?\n         if dst_sca_ty.is_bool():\n-            return tl.tensor(builder.create_fp_to_ui(input.handle,\n+            ty = input.dtype.to_ir(builder)\n+            _0 = tl.tensor(builder.get_null_value(ty), input.dtype)\n+            return not_equal(input, _0, builder)\n+        elif dst_sca_ty.is_int_signed():\n+            return tl.tensor(builder.create_fp_to_si(input.handle,\n                                                      dst_ty.to_ir(builder)),\n                              dst_ty)\n         else:\n-            return tl.tensor(builder.create_fp_to_si(input.handle,\n+            return tl.tensor(builder.create_fp_to_ui(input.handle,\n                                                      dst_ty.to_ir(builder)),\n                              dst_ty)\n "}, {"filename": "test/CMakeLists.txt", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -15,3 +15,12 @@ set(TRITON_TEST_DEPENDS\n   triton-opt\n   FileCheck\n )\n+\n+add_lit_testsuite(check-triton-lit-tests \"Running the triton regression tests\"\n+  ${CMAKE_CURRENT_BINARY_DIR}\n+  DEPENDS ${TRITON_TEST_DEPENDS}\n+  )\n+\n+set_target_properties(check-triton-lit-tests PROPERTIES FOLDER \"Tests\")\n+\n+add_lit_testsuites(TRITON-LIT-TESTS ${CMAKE_CURRENT_SOURCE_DIR} DEPENDS ${TRITON_TEST_DEPENDS})"}]