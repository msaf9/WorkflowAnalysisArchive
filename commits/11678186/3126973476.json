[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 78, "deletions": 31, "changes": 109, "file_content_changes": "@@ -1609,8 +1609,10 @@ class MMA16816SmemLoader {\n                      ArrayRef<int> matShape, int perPhase, int maxPhase,\n                      int elemBytes, ConversionPatternRewriter &rewriter,\n                      TypeConverter *typeConverter, const Location &loc)\n-      : wpt(wpt), order(order), kOrder(kOrder), tileShape(tileShape),\n-        instrShape(instrShape), matShape(matShape), perPhase(perPhase),\n+      : wpt(wpt), order(order.begin(), order.end()), kOrder(kOrder),\n+        tileShape(tileShape.begin(), tileShape.end()),\n+        instrShape(instrShape.begin(), instrShape.end()),\n+        matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n         maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter),\n         typeConverter(typeConverter), loc(loc), ctx(rewriter.getContext()) {\n     cMatShape = matShape[order[0]];\n@@ -1813,6 +1815,7 @@ class MMA16816SmemLoader {\n     int k = matIdx[kOrder];\n \n     int ptrIdx{-1};\n+\n     if (canUseLdmatrix)\n       ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n     else if (elemBytes == 4 && needTrans) // tf32 & trans\n@@ -1867,20 +1870,21 @@ class MMA16816SmemLoader {\n       int sOffsetArrElem = 1 * (sMatStride * sMatShape) * sTileStride;\n \n       Value elems[4];\n+      Type elemTy = type::f32Ty(ctx);\n       if (kOrder == 1) {\n-        elems[0] = load(gep(type::f32Ty(ctx), ptr, i32_val(sOffsetElem)));\n-        elems[1] = load(gep(type::f32Ty(ctx), ptr2, i32_val(sOffsetElem)));\n-        elems[2] = load(\n-            gep(type::f32Ty(ctx), ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n-        elems[3] = load(\n-            gep(type::f32Ty(ctx), ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n+        elems[0] = load(gep(elemTy, ptr, i32_val(sOffsetElem)));\n+        elems[1] = load(gep(elemTy, ptr2, i32_val(sOffsetElem)));\n+        elems[2] =\n+            load(gep(elemTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+        elems[3] =\n+            load(gep(elemTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n       } else {\n-        elems[0] = load(gep(type::f32Ty(ctx), ptr, i32_val(sOffsetElem)));\n-        elems[2] = load(gep(type::f32Ty(ctx), ptr2, i32_val(sOffsetElem)));\n-        elems[1] = load(\n-            gep(type::f32Ty(ctx), ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n-        elems[3] = load(\n-            gep(type::f32Ty(ctx), ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n+        elems[0] = load(gep(elemTy, ptr, i32_val(sOffsetElem)));\n+        elems[2] = load(gep(elemTy, ptr2, i32_val(sOffsetElem)));\n+        elems[1] =\n+            load(gep(elemTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+        elems[3] =\n+            load(gep(elemTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n       }\n \n       return {elems[0], elems[1], elems[2], elems[3]};\n@@ -1949,6 +1953,8 @@ class MMA16816SmemLoader {\n           i32Elems[m] = bit_cast(i32_ty, i8v4Elems[m]);\n         }\n       }\n+\n+      return {i32Elems[0], i32Elems[1], i32Elems[2], i32Elems[3]};\n     }\n \n     assert(false && \"Invalid smem load\");\n@@ -1957,11 +1963,11 @@ class MMA16816SmemLoader {\n \n private:\n   int wpt;\n-  ArrayRef<uint32_t> order;\n+  SmallVector<uint32_t> order;\n   int kOrder;\n-  ArrayRef<int64_t> tileShape;\n-  ArrayRef<int> instrShape;\n-  ArrayRef<int> matShape;\n+  SmallVector<int64_t> tileShape;\n+  SmallVector<int> instrShape;\n+  SmallVector<int> matShape;\n   int perPhase;\n   int maxPhase;\n   int elemBytes;\n@@ -2184,6 +2190,23 @@ struct DotOpConversionHelper {\n     return Type{};\n   }\n \n+  Type getLoadElemTy() {\n+    switch (mmaType) {\n+    case TensorCoreType::FP32_FP16_FP16_FP32:\n+      return vec_ty(type::f16Ty(ctx), 2);\n+    case TensorCoreType::FP32_BF16_BF16_FP32:\n+      return vec_ty(type::bf16Ty(ctx), 2);\n+    case TensorCoreType::FP32_TF32_TF32_FP32:\n+      return type::f32Ty(ctx);\n+    case TensorCoreType::INT32_INT8_INT8_INT32:\n+      return type::i32Ty(ctx);\n+    default:\n+      llvm::report_fatal_error(\"Unsupported mma type found\");\n+    }\n+\n+    return Type{};\n+  }\n+\n   Type getMmaRetType() const {\n     Type fp32Ty = type::f32Ty(ctx);\n     Type i32Ty = type::i32Ty(ctx);\n@@ -2370,9 +2393,10 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n   const int numRepN = std::max<int>(dShape[1] / (wpt[1] * mmaInstrN), 1);\n   const int numRepK = std::max<int>(NK / mmaInstrK, 1);\n \n-  Value head = getThreadId(rewriter, loc);\n-  Value lane = urem(head, i32_val(32));\n-  Value warp = udiv(head, i32_val(32));\n+  Value _32 = i32_val(32);\n+  Value thread = getThreadId(rewriter, loc);\n+  Value lane = urem(thread, _32);\n+  Value warp = udiv(thread, _32);\n   Value warpMN = udiv(warp, i32_val(wpt[0]));\n   Value warpM = urem(warp, i32_val(wpt[0]));\n   Value warpN = urem(warpMN, i32_val(wpt[1]));\n@@ -2384,7 +2408,7 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n   std::map<std::pair<unsigned, unsigned>, Value> hb;\n \n   // the original register_lds2, but discard the prefetch logic.\n-  auto ld2 = [&](decltype(ha) &vals, int mn, int k, Value val) {\n+  auto ld2 = [](decltype(ha) &vals, int mn, int k, Value val) {\n     vals[{mn, k}] = val;\n   };\n \n@@ -2400,6 +2424,7 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n     const int perPhase = sharedLayout.getPerPhase();\n     const int maxPhase = sharedLayout.getMaxPhase();\n     const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n+    auto order = sharedLayout.getOrder();\n \n     MMA16816SmemLoader loader(wpt, sharedLayout.getOrder(), kOrder,\n                               tensorTy.getShape() /*tileShape*/, instrShape,\n@@ -2417,29 +2442,51 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n           smemPtrTy, gep(smemBase.getType(), smemBase, ValueRange({offs[i]})));\n     }\n \n+    bool needTrans = kOrder != order[0];\n+\n     // (a, b) is the coordinate.\n-    auto load = [&, loader, ptrs, offs](int a, int b) {\n+    auto load = [&, loader, ptrs, offs, needTrans](int a, int b) {\n       auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n           (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n           ptrs, helper.getMatType(), helper.getShemPtrTy());\n-      ld2(vals, a, b, ha0);\n-      ld2(vals, a + 1, b, ha1);\n-      ld2(vals, a, b + 1, ha2);\n-      ld2(vals, a + 1, b + 1, ha3);\n+      if (!needTrans) {\n+        ld2(vals, a, b, ha0);\n+        ld2(vals, a + 1, b, ha1);\n+        ld2(vals, a, b + 1, ha2);\n+        ld2(vals, a + 1, b + 1, ha3);\n+      } else {\n+        ld2(vals, a, b, ha0);\n+        ld2(vals, a + 1, b, ha2);\n+        ld2(vals, a, b + 1, ha1);\n+        ld2(vals, a + 1, b + 1, ha3);\n+      }\n     };\n \n     return load;\n   };\n \n-  std::function<void(int, int)> loadA = getLoadMatrixFn(\n-      A, mmaLayout.getWarpsPerCTA()[0] /*wpt*/, 1 /*kOrder*/,\n-      {mmaInstrM, mmaInstrK} /*instrShpae*/,\n-      {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/);\n+  std::function<void(int, int)> loadA;\n   std::function<void(int, int)> loadB = getLoadMatrixFn(\n       B, mmaLayout.getWarpsPerCTA()[1] /*wpt*/, 0 /*kOrder*/,\n       {mmaInstrK, mmaInstrN} /*instrShpae*/,\n       {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/);\n \n+  if (aTensorTy.getEncoding()\n+          .dyn_cast<SharedEncodingAttr>()) { // load from smem\n+    loadA = getLoadMatrixFn(A, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n+                            1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShpae*/,\n+                            {matShapeM, matShapeK} /*matShape*/,\n+                            warpM /*warpId*/, ha /*vals*/);\n+  } else if (auto blockedLayout =\n+                 aTensorTy.getEncoding()\n+                     .dyn_cast<BlockedEncodingAttr>()) { // load from registers,\n+                                                         // used in gemm fuse\n+    // TODO(Superjomn) Port the logic.\n+    assert(false && \"Loading A from register is not supported yet.\");\n+  } else {\n+    assert(false && \"A's layout is not supported.\");\n+  }\n+\n   const unsigned mStride = numRepN * 2;\n   SmallVector<Value> fc(numRepM * mStride + numRepN * 2);\n   auto callMma = [&](unsigned m, unsigned n, unsigned k) {"}]