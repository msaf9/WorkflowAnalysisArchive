[{"filename": "CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -215,6 +215,7 @@ if(TRITON_BUILD_PYTHON_MODULE)\n     ${conversion_libs}\n \n     # optimizations\n+    MLIRBytecodeWriter\n     MLIRPass\n     MLIRTransforms\n     MLIRLLVMDialect"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -3,6 +3,8 @@\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/Verifier.h\"\n \n+#include \"mlir/Bytecode/BytecodeWriter.h\"\n+\n #include \"mlir/Conversion/Passes.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Pass/PassManager.h\"\n@@ -348,6 +350,14 @@ void init_triton_ir(py::module &&m) {\n              self.print(os);\n              return str;\n            })\n+      .def(\"bytecode\",\n+           [](mlir::ModuleOp &self) -> py::bytearray {\n+             std::string bytecode;\n+             llvm::raw_string_ostream os(bytecode);\n+             if (failed(mlir::writeBytecodeToFile(self, os)))\n+               throw std::runtime_error(\"Failed to write module bytecode\");\n+             return py::bytearray(bytecode);\n+           })\n       .def(\"push_back\",\n            [](mlir::ModuleOp &self, mlir::triton::FuncOp &funcOp) -> void {\n              self.push_back(funcOp);"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 23, "deletions": 2, "changes": 25, "file_content_changes": "@@ -467,6 +467,21 @@ def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.con\n     broadcast_kernel[(1,)](x_tri, y_tri, y_broadcasted_tri, M=M, N=N)\n     assert (y_broadcasted_np == to_numpy(y_broadcasted_tri)).all()\n \n+# ------------------\n+# test invalid slice\n+# ------------------\n+\n+\n+def test_invalid_slice():\n+    dst = torch.empty(128, device='cuda')\n+\n+    @triton.jit\n+    def _kernel(dst):\n+        dst[10:]\n+\n+    with pytest.raises(triton.CompilationError, match='unsupported tensor index'):\n+        _kernel[(1,)](dst=dst)\n+\n \n # ----------------\n # test expand_dims\n@@ -1379,6 +1394,9 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n     for op in ['min', 'max', 'sum', 'argmin', 'argmax']\n     for shape in reduce2d_shapes\n     for axis in [0, 1]\n+] + [\n+    (op, 'float32', [16, 32], None)\n+    for op in ['min', 'max', 'sum']\n ]\n \n \n@@ -1393,7 +1411,9 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n         range_n = tl.arange(0, BLOCK_N)\n         x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n         z = GENERATE_TEST_HERE\n-        if AXIS == 1:\n+        if AXIS is None:\n+            tl.store(Z, z)\n+        elif AXIS == 1:\n             tl.store(Z + range_m, z)\n         else:\n             tl.store(Z + range_n, z)\n@@ -1418,7 +1438,8 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     else:\n         z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n     # triton result\n-    z_tri = to_triton(numpy_random((shape[1 - axis],), dtype_str=z_dtype_str, rs=rs),\n+    ret_numel = 1 if axis is None else shape[1 - axis]\n+    z_tri = to_triton(numpy_random((ret_numel,), dtype_str=z_dtype_str, rs=rs),\n                       device=device, dst_type=z_tri_dtype_str)\n     kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n     z_tri = to_numpy(z_tri)"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -388,7 +388,8 @@ def visit_Assign(self, node):\n         for name, value in zip(names, values):\n             # by default, constexpr are assigned into python variable\n             value = _unwrap_if_constexpr(value)\n-            if not _is_triton_tensor(value) and \\\n+            if value is not None and \\\n+               not _is_triton_tensor(value) and \\\n                not isinstance(value, native_nontensor_types):\n                 value = language.core._to_tensor(value, self.builder)\n             self.set_value(name, value)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -711,7 +711,7 @@ def __getitem__(self, slices, _builder=None):\n         for dim, sl in enumerate(slices):\n             if isinstance(sl, constexpr) and sl.value is None:\n                 ret = semantic.expand_dims(ret, dim, _builder)\n-            elif sl == slice(None, None, None):\n+            elif isinstance(sl, slice) and sl.start is None and sl.stop is None and sl.step is None:\n                 pass\n             else:\n                 assert False, f\"unsupported tensor index: {sl}\"\n@@ -1297,8 +1297,8 @@ def make_combine_region(reduce_op):\n             else:\n                 handles = [r.handle for r in results]\n             _builder.create_reduce_ret(*handles)\n-\n-    axis = _constexpr_to_value(axis)\n+    if axis is not None:\n+        axis = _constexpr_to_value(axis)\n     return semantic.reduction(input, axis, make_combine_region, _builder)\n \n \n@@ -1369,7 +1369,7 @@ def _max_combine(a, b):\n \n @triton.jit\n @_add_reduction_docstr(\"maximum\")\n-def max(input, axis):\n+def max(input, axis=None):\n     input = _promote_reduction_input(input)\n     return reduce(input, axis, _max_combine)\n \n@@ -1399,7 +1399,7 @@ def _min_combine(a, b):\n \n @triton.jit\n @_add_reduction_docstr(\"minimum\")\n-def min(input, axis):\n+def min(input, axis=None):\n     input = _promote_reduction_input(input)\n     return reduce(input, axis, _min_combine)\n \n@@ -1428,7 +1428,7 @@ def _sum_combine(a, b):\n \n @triton.jit\n @_add_reduction_docstr(\"sum\")\n-def sum(input, axis):\n+def sum(input, axis=None):\n     input = _promote_reduction_input(input)\n     return reduce(input, axis, _sum_combine)\n \n@@ -1440,7 +1440,7 @@ def _xor_combine(a, b):\n \n @builtin\n @_add_reduction_docstr(\"xor sum\")\n-def xor_sum(input, axis, _builder=None, _generator=None):\n+def xor_sum(input, axis=None, _builder=None, _generator=None):\n     scalar_ty = input.type.scalar\n     if not scalar_ty.is_int():\n         raise ValueError(\"xor_sum only supported for integers\")"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -1236,6 +1236,13 @@ def where(condition: tl.tensor,\n def reduction(\n     inputs: Sequence[tl.tensor], axis: int, region_builder_fn, builder: ir.builder\n ) -> Tuple[tl.tensor, ...]:\n+    if axis is None:\n+        new_inputs = []\n+        for i in range(len(inputs)):\n+            new_shape = [inputs[i].numel.value]\n+            new_inputs.append(view(inputs[i], new_shape, builder))\n+        inputs = tuple(new_inputs)\n+        axis = 0\n     # get result shape\n     shape = inputs[0].type.shape\n     ret_shape = [s for i, s in enumerate(shape) if i != axis]"}]