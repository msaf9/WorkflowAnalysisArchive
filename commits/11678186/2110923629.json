[{"filename": "python/triton/code_gen.py", "status": "modified", "additions": 11, "deletions": 13, "changes": 24, "file_content_changes": "@@ -22,6 +22,7 @@\n import triton._C.libtriton.triton as _triton\n from .tools.disasm import extract\n \n+current_stream = lambda device: torch.cuda.current_stream(device).cuda_stream\n \n def mangle_ty(ty):\n     if ty.is_ptr():\n@@ -787,6 +788,7 @@ def __reduce__(self):\n \n \n class Kernel:\n+\n     @staticmethod\n     def _type_name(obj):\n         type_names = {\n@@ -915,28 +917,24 @@ def __call__(self, *wargs, grid, num_warps=4, num_stages=2, **kwargs):\n             raise TypeError(f\"Function takes {len(self.fn.arg_names)} positional arguments but {len(wargs)} were given\")\n         # handle annotations\n         for pos, _type in self.fn.annotations.items():\n+            assert _type == triton.language.constexpr, \"only constexpr annotations are supported for now\"\n             wargs[pos] = _type(wargs[pos])\n         # check that tensors are on GPU.\n         for arg in wargs:\n             if hasattr(arg, 'data_ptr'):\n                 assert arg.is_cuda, \"All tensors must be on GPU!\"\n-        # query device index and cuda stream\n+        # set device (i.e., make sure torch has the context initialized)\n         device = torch.cuda.current_device()\n         torch.cuda.set_device(device)\n+        # query compute capability\n         cc = torch.cuda.get_device_capability(device)\n         cc = str(cc[0]) + '-' + str(cc[1])\n-        # # query stream\n-        # # this is hacky but much faster than `torch.cuda.current_stream(device).cuda_stream`\n-        # # https://github.com/pytorch/pytorch/blob/master/c10/core/Stream.h#L154\n-        # # building a C wrapper to re-use the unpack function would add a build-time torch dependency\n-        # # and require different wheels for different torch versions -- undesirable!\n-        # bits = torch._C._cuda_getCurrentStream(device)\n-        # mask = 1 << 47\n-        # stream = ((bits & 0xFFFFFFFFFFFF) ^ mask) - mask\n-        stream = torch.cuda.current_stream(device).cuda_stream\n-        # make key for cache\n-        return _triton.runtime.launch(wargs, self.fn.do_not_specialize, self.fn.cache_key + cc, self.fn.arg_names, device, stream,\n-                                      self.fn.bin_cache, num_warps, num_stages, self.add_to_cache, grid)\n+        cache_key = self.fn.cache_key + cc\n+        # query current stream\n+        stream = current_stream(device)\n+        return _triton.runtime.launch(wargs, self.fn.do_not_specialize, cache_key, self.fn.arg_names, \n+                                      device, stream, self.fn.bin_cache, num_warps, num_stages, self.add_to_cache, \n+                                      grid)\n \n \n class Launcher:"}]