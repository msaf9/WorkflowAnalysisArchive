[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 18, "deletions": 2, "changes": 20, "file_content_changes": "@@ -11,6 +11,7 @@\n #include \"Utility.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n #include \"triton/Target/PTX/TmaMetadata.h\"\n #include <set>\n \n@@ -31,6 +32,7 @@ using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;\n using ::mlir::triton::gpu::TMAMetadataTy;\n+namespace ttng = ::mlir::triton::nvidia_gpu;\n \n typedef DenseMap<Operation *, triton::MakeTensorPtrOp> TensorPtrMapT;\n \n@@ -238,12 +240,26 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return llvmStruct;\n   }\n \n-  Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n-    auto tid = rewriter.create<::mlir::gpu::ThreadIdOp>(\n+  // Returns CTA level thread idx\n+  Value getThreadIdInCTA(ConversionPatternRewriter &rewriter,\n+                         Location loc) const {\n+    Value tid = rewriter.create<::mlir::gpu::ThreadIdOp>(\n         loc, ::mlir::gpu::Dimension::x);\n     return rewriter.create<arith::IndexCastOp>(loc, i32_ty, tid);\n   }\n \n+  // Returns CTA level thread idx for not ws mode.\n+  // Returns agent level thread idx for ws mode.\n+  Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n+    Value tid = getThreadIdInCTA(rewriter, loc);\n+    auto mod = rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    if (ttng::TritonNvidiaGPUDialect::getWSSupportedAttr(mod)) {\n+      Value _128 = rewriter.create<arith::ConstantIntOp>(loc, 128, 32);\n+      tid = rewriter.create<arith::RemSIOp>(loc, tid, _128);\n+    }\n+    return tid;\n+  }\n+\n   static Value getSRegValue(OpBuilder &b, Location loc,\n                             const std::string &sRegStr) {\n     PTXBuilder builder;"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 0, "deletions": 10, "changes": 10, "file_content_changes": "@@ -28,14 +28,6 @@\n import triton.language as tl\n \n \n-def isMMAV3OrTMAEnabled():\n-    import os\n-    for k in ('ENABLE_MMA_V3', 'ENABLE_TMA'):\n-        if os.environ.get(k, '0').lower() in ['1', 'on', 'true']:\n-            return True\n-    return False\n-\n-\n @triton.jit\n def static_persistent_matmul_kernel(\n     a_ptr, b_ptr, c_ptr,\n@@ -610,8 +602,6 @@ def static_persistent_matmul_no_scf_kernel(\n                              ]))\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n def test_static_persistent_matmul_no_scf_kernel(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_TYPE, USE_TMA_EPILOGUE, USE_TMA_LOAD):\n-    if isMMAV3OrTMAEnabled():\n-        pytest.skip(\"known failure\")\n     if (TRANS_A):\n         a = torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:"}]