[{"filename": "python/test/unit/debugger/test_debugger.py", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "file_content_changes": "@@ -49,3 +49,21 @@ def test_program_ids_from_grid():\n     first_run = list(program_ids_from_grid(grid))\n     second_run = list(program_ids_from_grid(grid))\n     assert first_run != second_run\n+\n+\n+def test_atomic():\n+    @triton.jit(interpret=True)\n+    def atomic(\n+        x_ptr,\n+    ):\n+        pid = tl.program_id(axis=0)\n+        tl.atomic_add(x_ptr + pid, 1)\n+        t = tl.atomic_xchg(x_ptr + pid, 3)\n+        t += 1  # 2\n+        tl.atomic_cas(x_ptr + pid, 3, t)  # match\n+        tl.atomic_cas(x_ptr + pid, 40, 9)  # no match\n+    nb_dim = 16\n+    a = torch.zeros((nb_dim, ), dtype=torch.int32, device=\"cuda\")\n+\n+    atomic[(nb_dim, )](a)\n+    assert torch.allclose(a, torch.full_like(a, 2))"}, {"filename": "python/triton/debugger/tl_lang.py", "status": "modified", "additions": 42, "deletions": 21, "changes": 63, "file_content_changes": "@@ -6,6 +6,9 @@\n \n \n def _primitive_to_tensor(x):\n+    \"\"\"\n+    Converts various Python primitive data types to PyTorch tensor.\n+    \"\"\"\n     tensor_args = {\"device\": \"cuda\"}\n     if isinstance(x, bool):\n         return torch.tensor([x], dtype=torch.bool, **tensor_args)\n@@ -28,10 +31,15 @@ def _primitive_to_tensor(x):\n         return _primitive_to_tensor(x.value)\n     elif x is None:\n         return None\n-    assert False, f\"cannot convert {x} to tensor\"\n+    assert False, f\"cannot convert {x} of type {type(x)} to tensor\"\n \n \n def _infer_tensor(func):\n+    \"\"\"\n+    A decorator function to harmonize function args:\n+        - converts primitives to PyTorch tensors\n+        - wraps PyTorch tensors with WrappedTensors\n+    \"\"\"\n     def wrapper(*args):\n         new_args = tuple(map(lambda v: _primitive_to_tensor(v), args))\n         new_args = tuple(map(lambda v: WrappedTensor(v) if torch.is_tensor(v) else v, new_args))\n@@ -42,9 +50,13 @@ def wrapper(*args):\n \n \n def _tensor_operation(func):\n+    \"\"\"\n+    A decorator function to unwrap WrappedTensors and debugger_constexpr before calling the function.\n+    Can be combined with _infer_tensor decorator to harmonize args (everything to torch tensor).\n+    \"\"\"\n     def wrapper(*args, **kwargs):\n         for arg in args:\n-            assert not torch.is_tensor(arg)\n+            assert not torch.is_tensor(arg), \"unexpected tensor argument\"\n \n         def unwrap_tensor(v):\n             if isinstance(v, WrappedTensor):\n@@ -332,16 +344,6 @@ def __init__(self, memory_map: MemoryMap, context: ExecutionContext):\n     # Types\n     # Removed void, int1, float8, uint16, uint32, uint64, pi32_t\n \n-    int8 = torch.int8\n-    int16 = torch.int16\n-    int32 = torch.int32\n-    int64 = torch.int64\n-    uint8 = torch.uint8\n-    bfloat16 = torch.bfloat16\n-    float32 = torch.float32\n-    float64 = torch.float64\n-    float16 = torch.float16\n-\n     # constexpr = debugger_constexpr\n \n     # Program functions\n@@ -402,7 +404,7 @@ def zeros(self, shape, dtype):\n         return torch.zeros(size=shape, dtype=dtype, device=\"cuda\")\n \n     @_tensor_operation\n-    def dequantize(self, input, scale, shift, nbit, dst_ty=float16):\n+    def dequantize(self, input, scale, shift, nbit, dst_ty=torch.float16):\n         raise NotImplementedError()\n \n     @_tensor_operation\n@@ -434,16 +436,20 @@ def dot(self, input, other, trans_a=False, trans_b=False, allow_tf32=True):\n     def atomic_cas(self, pointer, cmp, val):\n         stored = self._memory_map.load(pointer, None, 0.0)\n         if not isinstance(cmp, torch.Tensor):\n-            cmp = torch.tensor(cmp, dtype=stored.dtype, device=\"cuda\")\n+            cmp = torch.tensor([cmp], dtype=stored.dtype, device=\"cuda\")\n         if not isinstance(val, torch.Tensor):\n-            val = torch.tensor(val, dtype=stored.dtype, device=\"cuda\")\n+            val = torch.tensor([val], dtype=stored.dtype, device=\"cuda\")\n         if stored == cmp:\n             self._memory_map.store(pointer, val, None)\n         return stored\n \n     @_tensor_operation\n     def atomic_xchg(self, pointer, val, mask=None):\n-        raise NotImplementedError()\n+        if isinstance(val, int):\n+            val = torch.tensor([val], dtype=torch.int32, device=\"cuda\")\n+        stored = self._memory_map.load(pointer, mask, 0.0)\n+        self._memory_map.store(pointer, val, mask)\n+        return stored\n \n     @_tensor_operation\n     def atomic_add(self, pointer, val, mask=None):\n@@ -455,23 +461,38 @@ def atomic_add(self, pointer, val, mask=None):\n \n     @_tensor_operation\n     def atomic_max(self, pointer, val, mask=None):\n-        raise NotImplementedError()\n+        stored = self._memory_map.load(pointer, mask, 0.0)\n+        result = torch.maximum(stored, val)\n+        self._memory_map.store(pointer, result, mask)\n+        return stored\n \n     @_tensor_operation\n     def atomic_min(self, pointer, val, mask=None):\n-        raise NotImplementedError()\n+        stored = self._memory_map.load(pointer, mask, 0.0)\n+        result = torch.minimum(stored, val)\n+        self._memory_map.store(pointer, result, mask)\n+        return stored\n \n     @_tensor_operation\n     def atomic_and(self, pointer, val, mask=None):\n-        raise NotImplementedError()\n+        stored = self._memory_map.load(pointer, mask, 0)\n+        result = torch.bitwise_and(stored, val)\n+        self._memory_map.store(pointer, result, mask)\n+        return stored\n \n     @_tensor_operation\n     def atomic_or(self, pointer, val, mask=None):\n-        raise NotImplementedError()\n+        stored = self._memory_map.load(pointer, mask, 0)\n+        result = torch.bitwise_or(stored, val)\n+        self._memory_map.store(pointer, result, mask)\n+        return stored\n \n     @_tensor_operation\n     def atomic_xor(self, pointer, val, mask=None):\n-        raise NotImplementedError()\n+        stored = self._memory_map.load(pointer, mask, 0)\n+        result = torch.bitwise_xor(stored, val)\n+        self._memory_map.store(pointer, result, mask)\n+        return stored\n \n     @_tensor_operation\n     def where(self, condition, x, y):"}]