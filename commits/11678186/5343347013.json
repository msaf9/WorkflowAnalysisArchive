[{"filename": "python/test/unit/language/conftest.py", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -0,0 +1,14 @@\n+# content of conftest.py\n+\n+import pytest\n+\n+\n+def pytest_addoption(parser):\n+    parser.addoption(\n+        \"--device\", action=\"store\", default='cuda'\n+    )\n+\n+\n+@pytest.fixture\n+def device(request):\n+    return request.config.getoption(\"--device\")"}, {"filename": "python/test/unit/language/test_annotations.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -7,13 +7,13 @@\n import triton.language as tl\n \n \n-def test_annotations():\n+def test_annotations(device):\n \n     @triton.jit\n     def _kernel(X: torch.Tensor, N: int, BLOCK_SIZE: tl.constexpr):\n         pass\n \n-    x = torch.empty(1, device='cuda')\n+    x = torch.empty(1, device=device)\n     _kernel[(1,)](x, x.shape[0], 32)\n     try:\n         _kernel[(1,)](x.shape[0], x.shape[0], 32)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 204, "deletions": 195, "changes": 399, "file_content_changes": "@@ -101,13 +101,19 @@ def patch_kernel(template, to_replace):\n     return kernel\n \n \n-def check_type_supported(dtype):\n+def check_cuda_only(device):\n+    if device not in ['cuda']:\n+        pytest.skip(\"Only for cuda\")\n+\n+\n+def check_type_supported(dtype, device):\n     '''\n     skip test if dtype is not supported on the current device\n     '''\n-    cc = torch.cuda.get_device_capability()\n-    if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n-        pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n+    if device in ['cuda']:\n+        cc = torch.cuda.get_device_capability()\n+        if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n+            pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n \n \n class MmaLayout:\n@@ -142,20 +148,20 @@ def __str__(self):\n \n \n @pytest.mark.parametrize(\"dtype_x\", list(dtypes) + [\"bfloat16\"])\n-def test_empty_kernel(dtype_x, device='cuda'):\n+def test_empty_kernel(dtype_x, device):\n     SIZE = 128\n \n     @triton.jit\n     def kernel(X, SIZE: tl.constexpr):\n         pass\n-    check_type_supported(dtype_x)\n+    check_type_supported(dtype_x, device)\n     x = to_triton(numpy_random(SIZE, dtype_str=dtype_x), device=device, dst_type=dtype_x)\n     kernel[(1, )](x, SIZE=SIZE, num_warps=4)\n \n \n # generic test functions\n def _test_unary(dtype_x, expr, numpy_expr=None, device='cuda'):\n-    check_type_supported(dtype_x)  # early return if dtype_x is not supported\n+    check_type_supported(dtype_x, device)  # early return if dtype_x is not supported\n     SIZE = 128\n     # define the kernel / launch-grid\n \n@@ -213,8 +219,8 @@ def _binary_op_dtype_override(a: str, b: str) -> Optional[np.dtype]:\n \n \n def _test_binary(dtype_x, dtype_y, expr, numpy_expr=None, mode_x='real', mode_y='real', device='cuda', y_low=None, y_high=None):\n-    check_type_supported(dtype_x)  # early return if dtype_x is not supported\n-    check_type_supported(dtype_y)\n+    check_type_supported(dtype_x, device)  # early return if dtype_x is not supported\n+    check_type_supported(dtype_y, device)\n     SIZE = 128\n     # define the kernel / launch-grid\n \n@@ -284,7 +290,7 @@ def _mod_operation_ill_conditioned(dtype_x, dtype_y) -> bool:\n     for dtype_x in dtypes_with_bfloat16\n     for dtype_y in dtypes_with_bfloat16\n ])\n-def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n+def test_bin_op(dtype_x, dtype_y, op, device):\n     expr = f' x {op} y'\n     if op == '%' and dtype_x in int_dtypes + uint_dtypes and dtype_y in int_dtypes + uint_dtypes:\n         # LLVM has 'numpy.fmod', not 'numpy.remainder', semantics on integer remainders.\n@@ -317,7 +323,7 @@ def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n                          [(dtype_x, dtype_y) for dtype_x in int_dtypes for dtype_y in int_dtypes] +\n                          [(dtype_x, dtype_y) for dtype_x in uint_dtypes for dtype_y in uint_dtypes]\n                          )\n-def test_floordiv(dtype_x, dtype_y, device='cuda'):\n+def test_floordiv(dtype_x, dtype_y, device):\n     # Triton has IEEE, not numpy/torch, semantics for %, and those carry\n     # through to //, so we have to use a nonstandard expression to get a\n     # reference result for //.\n@@ -326,7 +332,7 @@ def test_floordiv(dtype_x, dtype_y, device='cuda'):\n     _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n \n \n-def test_unsigned_name_mangling(device='cuda'):\n+def test_unsigned_name_mangling(device):\n     # Test that uint32 and int32 are mangled differently by the compiler\n     SIZE = 128\n     # define the kernel / launch-grid\n@@ -372,7 +378,7 @@ def kernel(O1, O2, X, Y, SIZE: tl.constexpr):\n     for dtype_x in dtypes + dtypes_with_bfloat16\n     for dtype_y in dtypes + dtypes_with_bfloat16\n ])\n-def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n+def test_bitwise_op(dtype_x, dtype_y, op, device):\n     expr = f'x {op} y'\n     if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n         numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n@@ -395,7 +401,7 @@ def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n     for dtype_x in int_dtypes + uint_dtypes\n     for dtype_y in int_dtypes + uint_dtypes\n ])\n-def test_shift_op(dtype_x, dtype_y, op, device='cuda'):\n+def test_shift_op(dtype_x, dtype_y, op, device):\n     expr = f'x {op} y'\n     bw = max(_bitwidth(dtype_x), _bitwidth(dtype_y))\n     if dtype_x.startswith('int'):\n@@ -428,7 +434,7 @@ def test_shift_op(dtype_x, dtype_y, op, device='cuda'):\n                                                     ('nan', 'nan')]\n \n                           ])\n-def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n+def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device):\n     expr = f'x {op} y'\n     if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n         numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n@@ -443,7 +449,7 @@ def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n # test broadcast\n # ---------------\n @pytest.mark.parametrize(\"dtype\", dtypes_with_bfloat16)\n-def test_broadcast(dtype):\n+def test_broadcast(dtype, device):\n     @triton.jit\n     def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.constexpr):\n         offset1 = tl.arange(0, M)\n@@ -460,9 +466,9 @@ def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.con\n     y = numpy_random(N, dtype_str=dtype, rs=rs)\n     _, y_broadcasted_np = np.broadcast_arrays(x, y)\n \n-    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n-    y_tri = to_triton(y, device='cuda', dst_type=dtype)\n-    y_broadcasted_tri = to_triton(np.empty((M, N), dtype=y_broadcasted_np.dtype), device='cuda', dst_type=dtype)\n+    x_tri = to_triton(x, device=device, dst_type=dtype)\n+    y_tri = to_triton(y, device=device, dst_type=dtype)\n+    y_broadcasted_tri = to_triton(np.empty((M, N), dtype=y_broadcasted_np.dtype), device=device, dst_type=dtype)\n \n     broadcast_kernel[(1,)](x_tri, y_tri, y_broadcasted_tri, M=M, N=N)\n     assert (y_broadcasted_np == to_numpy(y_broadcasted_tri)).all()\n@@ -472,8 +478,8 @@ def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.con\n # ------------------\n \n \n-def test_invalid_slice():\n-    dst = torch.empty(128, device='cuda')\n+def test_invalid_slice(device):\n+    dst = torch.empty(128, device=device)\n \n     @triton.jit\n     def _kernel(dst):\n@@ -486,7 +492,7 @@ def _kernel(dst):\n # ----------------\n # test expand_dims\n # ----------------\n-def test_expand_dims():\n+def test_expand_dims(device):\n     @triton.jit\n     def expand_dims_kernel(dummy, N: tl.constexpr):\n         offset1 = tl.arange(0, N)\n@@ -516,11 +522,11 @@ def expand_dims_kernel(dummy, N: tl.constexpr):\n         tl.static_assert(t.shape == [N, 1, 1, 1])\n \n     N = 32\n-    dummy_tensor = torch.empty((), device=\"cuda\")\n+    dummy_tensor = torch.empty((), device=device)\n     expand_dims_kernel[(1,)](dummy_tensor, N)\n \n \n-def test_expand_dims_error_cases():\n+def test_expand_dims_error_cases(device):\n     @triton.jit\n     def dim_out_of_range1(dummy, N: tl.constexpr):\n         offset1 = tl.arange(0, N)\n@@ -548,7 +554,7 @@ def duplicate_dim2(dummy, N: tl.constexpr):\n         t = tl.expand_dims(offset1, (0, -3))\n \n     N = 32\n-    dummy_tensor = torch.empty((), device=\"cuda\")\n+    dummy_tensor = torch.empty((), device=device)\n \n     with pytest.raises(triton.CompilationError, match=\"invalid axis -3\"):\n         dim_out_of_range1[(1,)](dummy_tensor, N)\n@@ -566,8 +572,8 @@ def duplicate_dim2(dummy, N: tl.constexpr):\n # ----------------------------\n # test invalid program id axis\n # ----------------------------\n-def test_invalid_pid_axis():\n-    dst = torch.empty(128, device='cuda')\n+def test_invalid_pid_axis(device):\n+    dst = torch.empty(128, device=device)\n \n     @triton.jit\n     def _kernel(dst):\n@@ -581,12 +587,12 @@ def _kernel(dst):\n # test where\n # ---------------\n @pytest.mark.parametrize(\"dtype\", dtypes_with_bfloat16 + [\"*int32\"])\n-def test_where(dtype):\n+def test_where(dtype, device):\n     select_ptrs = False\n     if dtype == \"*int32\":\n         dtype = \"int64\"\n         select_ptrs = True\n-    check_type_supported(dtype)\n+    check_type_supported(dtype, device)\n \n     @triton.jit\n     def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n@@ -616,10 +622,10 @@ def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n     y = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n     z = np.where(cond, x, y)\n \n-    cond_tri = to_triton(cond, device='cuda')\n-    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n-    y_tri = to_triton(y, device='cuda', dst_type=dtype)\n-    z_tri = to_triton(np.empty(SIZE, dtype=z.dtype), device='cuda', dst_type=dtype)\n+    cond_tri = to_triton(cond, device=device)\n+    x_tri = to_triton(x, device=device, dst_type=dtype)\n+    y_tri = to_triton(y, device=device, dst_type=dtype)\n+    z_tri = to_triton(np.empty(SIZE, dtype=z.dtype), device=device, dst_type=dtype)\n \n     grid = lambda meta: (triton.cdiv(SIZE, meta['BLOCK_SIZE']),)\n     where_kernel[grid](cond_tri, x_tri, y_tri, z_tri, SIZE, BLOCK_SIZE=1024, TEST_POINTERS=select_ptrs, TEST_SCALAR_POINTERS=False)\n@@ -630,7 +636,7 @@ def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n         assert (z == to_numpy(z_tri)).all()\n \n \n-def test_where_broadcast():\n+def test_where_broadcast(device):\n     @triton.jit\n     def where_kernel(cond_ptr, a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n         xoffsets = tl.arange(0, BLOCK_SIZE)[:, None]\n@@ -656,9 +662,9 @@ def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n     x = numpy_random((SIZE, SIZE), dtype_str=dtype, rs=rs)\n     mask = numpy_random(SIZE, 'bool', rs=rs)\n     z = np.where(mask, x, 0)\n-    cond_tri = to_triton(mask, device=\"cuda\")\n-    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n-    z_tri = to_triton(np.empty((SIZE, SIZE), dtype=z.dtype), device='cuda', dst_type=dtype)\n+    cond_tri = to_triton(mask, device=device)\n+    x_tri = to_triton(x, device=device, dst_type=dtype)\n+    z_tri = to_triton(np.empty((SIZE, SIZE), dtype=z.dtype), device=device, dst_type=dtype)\n     where_kernel[(1,)](cond_tri, x_tri, z_tri, SIZE)\n     assert (z == to_numpy(z_tri)).all()\n     where_scalar_condition[(1,)](x_tri, z_tri, SIZE)\n@@ -675,7 +681,7 @@ def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n ] + [\n     (dtype_x, ' ~x') for dtype_x in int_dtypes\n ])\n-def test_unary_op(dtype_x, expr, device='cuda'):\n+def test_unary_op(dtype_x, expr, device):\n     _test_unary(dtype_x, expr, device=device)\n \n # ----------------\n@@ -684,7 +690,7 @@ def test_unary_op(dtype_x, expr, device='cuda'):\n \n \n @pytest.mark.parametrize(\"dtype_x, expr\", [(dtype_x, expr) for dtype_x in [\"float32\", \"float64\"] for expr in ['exp', 'log', 'cos', 'sin']])\n-def test_math_op(dtype_x, expr, device='cuda'):\n+def test_math_op(dtype_x, expr, device):\n     _test_unary(dtype_x, f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n \n # ----------------\n@@ -696,12 +702,12 @@ def test_math_op(dtype_x, expr, device='cuda'):\n     (dtype_x)\n     for dtype_x in dtypes_with_bfloat16\n ])\n-def test_abs(dtype_x, device='cuda'):\n+def test_abs(dtype_x, device):\n     _test_unary(dtype_x, 'tl.abs(x)', 'np.abs(x) ', device=device)\n \n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n-def test_abs_f8(in_dtype):\n+def test_abs_f8(in_dtype, device):\n \n     @triton.jit\n     def abs_kernel(Z, X, SIZE: tl.constexpr):\n@@ -710,7 +716,7 @@ def abs_kernel(Z, X, SIZE: tl.constexpr):\n         z = tl.abs(x)\n         tl.store(Z + off, z)\n \n-    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n+    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device=device)\n     # f32_to_f8 doesn't handle nan, so we make sure f8_tensor doesn't contain any nan\n     all_exp_ones = (f8_tensor & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n     f8_tensor[all_exp_ones] = 0\n@@ -750,7 +756,7 @@ def make_ptr_str(name, shape):\n               ':, :, None']\n     for d in ['int32', 'uint32', 'uint16']\n ])\n-def test_index1d(expr, dtype_str, device='cuda'):\n+def test_index1d(expr, dtype_str, device):\n     rank_x = expr.count(':')\n     rank_y = expr.count(',') + 1\n     shape_x = [32 for _ in range(rank_x)]\n@@ -785,7 +791,7 @@ def generate_kernel(shape_x, shape_z):\n     z_ref = eval(expr) + y\n     # triton result\n     z_tri = to_triton(np.empty_like(z_ref), device=device)\n-    x_tri = to_triton(x)\n+    x_tri = to_triton(x, device=device)\n     kernel_match[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n     # compare\n     assert (z_ref == to_numpy(z_tri)).all()\n@@ -814,9 +820,7 @@ def tuples_fn(a, b):\n         a * b\n \n \n-def test_tuples():\n-    device = 'cuda'\n-\n+def test_tuples(device):\n     @triton.jit\n     def with_fn(X, Y, A, B, C):\n         x = tl.load(X)\n@@ -907,9 +911,7 @@ def noinline_multi_values_fn(x, y, Z):\n \n \n @pytest.mark.parametrize(\"mode\", [\"simple\", \"call_graph\", \"shared\", \"dynamic\", \"multi_values\"])\n-def test_noinline(mode):\n-    device = 'cuda'\n-\n+def test_noinline(mode, device):\n     @triton.jit\n     def kernel(X, Y, Z):\n         x = tl.load(X)\n@@ -946,7 +948,9 @@ def kernel(X, Y, Z):\n     ]\n     for mode in ['all_neg', 'all_pos', 'min_neg', 'max_pos']\n     for sem in [None, 'acquire', 'release', 'acq_rel', 'relaxed']]))\n-def test_atomic_rmw(op, dtype_x_str, mode, sem, device='cuda'):\n+def test_atomic_rmw(op, dtype_x_str, mode, sem, device):\n+    check_cuda_only(device, device)\n+\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         if dtype_x_str == 'float16':\n@@ -996,7 +1000,7 @@ def kernel(X, Z):\n     assert f\"atom.global.gpu.{sem_str}\" in h.asm[\"ptx\"]\n \n \n-def test_atomic_rmw_predicate(device=\"cuda\"):\n+def test_atomic_rmw_predicate(device):\n     @triton.jit\n     def kernel(X):\n         val = tl.program_id(0)\n@@ -1009,7 +1013,7 @@ def kernel(X):\n \n @pytest.mark.parametrize(\"shape, axis\",\n                          [(shape, axis) for shape in [(2, 2), (2, 8), (8, 2), (8, 8), (32, 32)] for axis in [0, 1]])\n-def test_tensor_atomic_rmw(shape, axis, device=\"cuda\"):\n+def test_tensor_atomic_rmw(shape, axis, device):\n     shape0, shape1 = shape\n     # triton kernel\n \n@@ -1035,7 +1039,7 @@ def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n \n-def test_tensor_atomic_rmw_block(device=\"cuda\"):\n+def test_tensor_atomic_rmw_block(device):\n     shape = (8, 8)\n \n     @triton.jit\n@@ -1052,13 +1056,13 @@ def kernel(X, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"sem\", [None, 'acquire', 'release', 'acq_rel', 'relaxed'])\n-def test_atomic_cas(sem):\n+def test_atomic_cas(sem, device):\n     # 1. make sure that atomic_cas changes the original value (Lock)\n     @triton.jit\n     def change_value(Lock):\n         tl.atomic_cas(Lock, 0, 1)\n \n-    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+    Lock = torch.zeros((1,), device=device, dtype=torch.int32)\n     change_value[(1,)](Lock)\n \n     assert (Lock[0] == 1)\n@@ -1075,8 +1079,8 @@ def serialized_add(data, Lock, SEM: tl.constexpr):\n         # release lock\n         tl.atomic_xchg(Lock, 0)\n \n-    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n-    data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n+    Lock = torch.zeros((1,), device=device, dtype=torch.int32)\n+    data = torch.zeros((128,), device=device, dtype=torch.float32)\n     ref = torch.full((128,), 64.0)\n     h = serialized_add[(64,)](data, Lock, SEM=sem)\n     sem_str = \"acq_rel\" if sem is None else sem\n@@ -1103,10 +1107,10 @@ def serialized_add(data, Lock, SEM: tl.constexpr):\n ] + [\n     (f'int{x}', f'uint{x}', True) for x in [8, 16, 32, 64]\n ])\n-def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n+def test_cast(dtype_x, dtype_z, bitcast, device):\n     # bfloat16 on cc < 80 will not be tested\n-    check_type_supported(dtype_x)\n-    check_type_supported(dtype_z)\n+    check_type_supported(dtype_x, device)\n+    check_type_supported(dtype_z, device)\n \n     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n     x0 = 43 if dtype_x in int_dtypes else 43.5\n@@ -1116,7 +1120,7 @@ def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n         x_tri = torch.tensor([x0], dtype=getattr(torch, dtype_x), device=device)\n     else:\n         x = np.array([x0], dtype=getattr(np, dtype_x))\n-        x_tri = to_triton(x)\n+        x_tri = to_triton(x, device=device)\n \n     # triton kernel\n     @triton.jit\n@@ -1148,8 +1152,8 @@ def kernel(X, Z, BITCAST: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"dtype_str, num_warps\", [(dtype_str, num_warps) for dtype_str in int_dtypes + float_dtypes for num_warps in [4, 8]])\n-def test_cat(dtype_str, num_warps):\n-    check_type_supported(dtype_str)\n+def test_cat(dtype_str, num_warps, device):\n+    check_type_supported(dtype_str, device)\n \n     @triton.jit\n     def kernel(X, Y, Z, N: tl.constexpr):\n@@ -1159,19 +1163,19 @@ def kernel(X, Y, Z, N: tl.constexpr):\n         z = tl.cat(x, y, can_reorder=True)\n         tl.store(Z + tl.arange(0, 2 * N), z)\n \n-    x = torch.arange(0, 128, device='cuda').to(getattr(torch, dtype_str))\n-    y = torch.arange(-128, 0, device='cuda').to(getattr(torch, dtype_str))\n+    x = torch.arange(0, 128, device=device).to(getattr(torch, dtype_str))\n+    y = torch.arange(-128, 0, device=device).to(getattr(torch, dtype_str))\n     z_ref = torch.cat([x, y], dim=0).sum()\n-    z = torch.zeros((256,), dtype=getattr(torch, dtype_str), device='cuda')\n+    z = torch.zeros((256,), dtype=getattr(torch, dtype_str), device=device)\n     kernel[(1, )](x, y, z, N=128, num_warps=num_warps)\n     assert z.sum() == z_ref\n     # check if there's no duplicate value in z\n     assert z.unique().size(0) == z.size(0)\n \n \n @pytest.mark.parametrize(\"dtype_str\", list(torch_dtypes))\n-def test_store_constant(dtype_str):\n-    check_type_supported(dtype_str)\n+def test_store_constant(dtype_str, device):\n+    check_type_supported(dtype_str, device)\n \n     \"\"\"Tests that boolean True is stored as 1\"\"\"\n     @triton.jit\n@@ -1184,14 +1188,14 @@ def kernel(output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     triton_dtype_str = 'uint8' if dtype_str == 'bool' else dtype_str\n     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.zeros([BLOCK_SIZE], dtype=tl.{triton_dtype_str}) + 1'})\n     block_size = 128\n-    ref = torch.ones([block_size], dtype=getattr(torch, dtype_str), device='cuda')\n-    output = torch.zeros([block_size], dtype=getattr(torch, dtype_str), device='cuda')\n+    ref = torch.ones([block_size], dtype=getattr(torch, dtype_str), device=device)\n+    output = torch.zeros([block_size], dtype=getattr(torch, dtype_str), device=device)\n     kernel[(1,)](output, block_size, BLOCK_SIZE=block_size)\n \n     assert torch.all(output == ref)\n \n \n-def test_load_store_same_ptr():\n+def test_load_store_same_ptr(device):\n     @triton.jit()\n     def kernel(in_out_ptr):\n         pid = tl.program_id(axis=0)\n@@ -1200,7 +1204,7 @@ def kernel(in_out_ptr):\n         tl.store(in_out_ptr + pid, out)\n \n     for _ in range(1000):\n-        x = torch.ones((65536,), device=\"cuda\", dtype=torch.float32)\n+        x = torch.ones((65536,), device=device, dtype=torch.float32)\n         kernel[(65536,)](x, num_warps=32)\n         assert torch.all(x == 2)\n \n@@ -1236,9 +1240,9 @@ def convert_float_to_float32(fp: torch.tensor, dtype=None):\n \n \n @pytest.mark.parametrize(\"in_dtype\", [torch.float16, torch.bfloat16])\n-def test_convert_float16_to_float32(in_dtype):\n+def test_convert_float16_to_float32(in_dtype, device):\n     \"\"\"Tests that check convert_float_to_float32 function\"\"\"\n-    check_type_supported(in_dtype)\n+    check_type_supported(in_dtype, device)\n \n     f16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16).view(in_dtype)\n     f32_output = convert_float_to_float32(f16_input)\n@@ -1253,9 +1257,9 @@ def test_convert_float16_to_float32(in_dtype):\n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16, torch.float32])\n-def test_f8_xf16_roundtrip(in_dtype, out_dtype):\n+def test_f8_xf16_roundtrip(in_dtype, out_dtype, device):\n     \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n-    check_type_supported(out_dtype)\n+    check_type_supported(out_dtype, device)\n \n     @triton.jit\n     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n@@ -1265,7 +1269,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         output = input\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n-    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n+    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device=device)\n     # f32_to_f8 doesn't handle nan, so we make sure f8_tensor doesn't contain any nan\n     all_exp_ones = (f8_tensor & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n     f8_tensor[all_exp_ones] = 0\n@@ -1292,7 +1296,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16])\n-def test_f16_to_f8_rounding(in_dtype, out_dtype):\n+def test_f16_to_f8_rounding(in_dtype, out_dtype, device):\n     \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n     error is the minimum over all float8.\n     Or the same explanation a bit mathier:\n@@ -1305,7 +1309,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         output = input\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n-    i16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16, device='cuda')\n+    i16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16, device=device)\n     f16_input = i16_input.view(out_dtype)\n     n_elements = f16_input.numel()\n     f8_output_tensor = torch.empty_like(f16_input, dtype=torch.int8)\n@@ -1318,7 +1322,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n \n     abs_error = torch.abs(f16_input - f16_output)\n \n-    all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device='cuda')\n+    all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device=device)\n     all_f8_vals = triton.reinterpret(all_f8_vals_tensor, in_dtype)\n     all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=out_dtype)\n     copy_kernel[grid](all_f8_vals, all_f8_vals_in_f16, n_elements=256, BLOCK_SIZE=1024)\n@@ -1374,8 +1378,8 @@ def get_reduced_dtype(dtype_str, op):\n                                      'sum']\n                           for dtype in dtypes_with_bfloat16\n                           for shape in [32, 64, 128, 512]])\n-def test_reduce1d(op, dtype_str, shape, device='cuda'):\n-    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+def test_reduce1d(op, dtype_str, shape, device):\n+    check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n@@ -1439,7 +1443,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n reduce2d_shapes = [(2, 32), (4, 32), (4, 128)]\n # TODO: fix and uncomment\n # , (32, 64), (64, 128)]\n-if 'V100' in torch.cuda.get_device_name(0):\n+if torch.cuda.is_available() and 'V100' in torch.cuda.get_device_name(0):\n     reduce2d_shapes += [(128, 256) and (32, 1024)]\n \n \n@@ -1455,8 +1459,8 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n-def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n-    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+def test_reduce2d(op, dtype_str, shape, axis, device):\n+    check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n@@ -1477,7 +1481,7 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     rs = RandomState(17)\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n-    x_tri = to_triton(x)\n+    x_tri = to_triton(x, device=device)\n     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n     z_dtype_str = get_reduced_dtype(dtype_str, op)\n@@ -1523,7 +1527,7 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n @pytest.mark.parametrize(\"M, N\", [[128, 16], [128, 128], [32, 128]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"axis\", [0, 1])\n-def test_reduce_layouts(M, N, src_layout, axis, device='cuda'):\n+def test_reduce_layouts(M, N, src_layout, axis, device):\n     rdims_2d = f\"1x{N}\" if axis == 0 else f\"{M}x1\"\n     rdims_1d = f\"{N}\" if axis == 0 else f\"{M}\"\n     store_range = \"%7\" if axis == 0 else \"%1\"\n@@ -1595,7 +1599,7 @@ def test_reduce_layouts(M, N, src_layout, axis, device='cuda'):\n \n @pytest.mark.parametrize(\"M\", [32, 64, 128, 256])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n-def test_store_op(M, src_layout, device='cuda'):\n+def test_store_op(M, src_layout, device):\n     ir = f\"\"\"\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n@@ -1644,7 +1648,7 @@ def test_store_op(M, src_layout, device='cuda'):\n @pytest.mark.parametrize(\"dst_layout\", layouts)\n @pytest.mark.parametrize(\"src_dim\", [0, 1])\n @pytest.mark.parametrize(\"dst_dim\", [0, 1])\n-def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device='cuda'):\n+def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device):\n     ir = f\"\"\"\n     #dst = {dst_layout}\n     #src = {src_layout}\n@@ -1701,7 +1705,7 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n \n @pytest.mark.parametrize(\"M, N\", [[128, 128], [256, 128], [256, 256], [128, 256]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n-def test_chain_reduce(M, N, src_layout, device='cuda'):\n+def test_chain_reduce(M, N, src_layout, device):\n     ir = f\"\"\"\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n@@ -1753,7 +1757,7 @@ def test_chain_reduce(M, N, src_layout, device='cuda'):\n     np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n \n \n-def test_generic_reduction(device='cuda'):\n+def test_generic_reduction(device):\n \n     @triton.jit\n     def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n@@ -1789,8 +1793,8 @@ def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n                           for dtype in ['float16', 'float32']\n                              for shape in [(64, 64), (128, 128)]\n                              for perm in [(1, 0)]])\n-def test_permute(dtype_str, shape, perm, device='cuda'):\n-    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+def test_permute(dtype_str, shape, perm, device):\n+    check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n@@ -1860,7 +1864,9 @@ def kernel(X, stride_xm, stride_xn,\n                                                       ('float16', 'float16'),\n                                                       ('float16', 'float32'),\n                                                       ('float32', 'float32')]])\n-def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype, device='cuda'):\n+def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype, device):\n+    check_cuda_only(device)\n+\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n@@ -2019,9 +2025,9 @@ def kernel(X, stride_xm, stride_xk,\n \n \n @pytest.mark.parametrize(\"dtype_str\", int_dtypes + float_dtypes + ['bfloat16'])\n-def test_full(dtype_str):\n+def test_full(dtype_str, device):\n     dtype = getattr(torch, dtype_str)\n-    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+    check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     @triton.jit\n     def kernel_static(out):\n@@ -2036,9 +2042,9 @@ def kernel_dynamic(out, val, dtype: tl.constexpr):\n         tl.store(out_ptr, a)\n \n     kernel_static_patched = patch_kernel(kernel_static, {'GENERATE_TEST_HERE': f\"tl.full((128,), 2, tl.{dtype_str})\"})\n-    out_static = torch.zeros((128), dtype=dtype, device=\"cuda\")\n+    out_static = torch.zeros((128), dtype=dtype, device=device)\n     kernel_static_patched[(1,)](out_static)\n-    out_dynamic = torch.zeros((128), dtype=dtype, device=\"cuda\")\n+    out_dynamic = torch.zeros((128), dtype=dtype, device=device)\n     kernel_dynamic[(1,)](out_dynamic, 2, getattr(triton.language, dtype_str))\n     assert torch.all(out_static == 2)\n     assert torch.all(out_dynamic == 2)\n@@ -2050,20 +2056,20 @@ def kernel_dynamic(out, val, dtype: tl.constexpr):\n                           ('float(\"nan\")', \"f32\"), ('float(\"-nan\")', \"f32\"),\n                           (0., \"f32\"),\n                           (5, \"i32\"), (2**40, \"i64\"),])\n-def test_constexpr(literal, dtype_str):\n+def test_constexpr(literal, dtype_str, device):\n     @triton.jit\n     def kernel(out_ptr):\n         val = GENERATE_TEST_HERE\n         tl.store(out_ptr.to(tl.pointer_type(val.dtype)), val)\n \n     kernel_patched = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{literal}\"})\n-    out = torch.zeros((1,), dtype=torch.float32, device=\"cuda\")\n+    out = torch.zeros((1,), dtype=torch.float32, device=device)\n     h = kernel_patched[(1,)](out)\n     assert re.search(r\"arith.constant .* : \" + dtype_str, h.asm[\"ttir\"]) is not None\n \n # TODO: uncomment once DotOperandEncoding::getElemsPerThread is implemented\n # @pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n-# def test_dot_without_load(dtype_str):\n+# def test_dot_without_load(dtype_str, device=device):\n #     @triton.jit\n #     def _kernel(out):\n #         a = GENERATE_TEST_HERE\n@@ -2073,10 +2079,10 @@ def kernel(out_ptr):\n #         tl.store(out_ptr, c)\n \n #     kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.full((32, 32), 1.0, tl.{dtype_str})\"})\n-#     a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n-#     b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+#     a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n+#     b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n #     out_ref = torch.matmul(a, b)\n-#     out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+#     out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=device)\n #     kernel[(1,)](out)\n #     assert torch.all(out == out_ref)\n \n@@ -2086,7 +2092,7 @@ def kernel(out_ptr):\n \n \n @pytest.mark.parametrize(\"start\", [0, 1, 7, 16])\n-def test_arange(start, device='cuda'):\n+def test_arange(start, device):\n     BLOCK = 128\n     z_tri = torch.empty(BLOCK, dtype=torch.int32, device=device)\n \n@@ -2106,9 +2112,9 @@ def _kernel(z, BLOCK: tl.constexpr,\n \n \n @pytest.mark.parametrize(\"dtype_str, size, size_diff\", [(dtype_str, size, size_diff) for dtype_str in torch_dtypes for size in [128, 512] for size_diff in [0, 1, 2, 3, 4]])\n-def test_masked_load(dtype_str, size, size_diff, device='cuda'):\n+def test_masked_load(dtype_str, size, size_diff, device):\n     dtype = getattr(torch, dtype_str)\n-    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+    check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     input_size = size - size_diff\n     output_size = size\n@@ -2141,8 +2147,8 @@ def _kernel(in_ptr, out_ptr, in_size: tl.constexpr, out_size: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"dtype\", [torch.bfloat16, torch.float16, torch.float32])\n-def test_masked_load_shared_memory(dtype, device='cuda'):\n-    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+def test_masked_load_shared_memory(dtype, device):\n+    check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     M = 32\n     N = 32\n@@ -2190,9 +2196,9 @@ def _kernel(in1_ptr, in2_ptr, output_ptr,\n \n \n @pytest.mark.parametrize(\"cache\", [\"\", \".ca\", \".cg\"])\n-def test_load_cache_modifier(cache):\n-    src = torch.empty(128, device='cuda')\n-    dst = torch.empty(128, device='cuda')\n+def test_load_cache_modifier(cache, device):\n+    src = torch.empty(128, device=device)\n+    dst = torch.empty(128, device=device)\n \n     @triton.jit\n     def _kernel(dst, src, CACHE: tl.constexpr):\n@@ -2214,9 +2220,9 @@ def _kernel(dst, src, CACHE: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"N\", [16, 10, 11, 1024])\n-def test_vectorization(N):\n-    src = torch.empty(1024, device='cuda')\n-    dst = torch.empty(1024, device='cuda')\n+def test_vectorization(N, device):\n+    src = torch.empty(1024, device=device)\n+    dst = torch.empty(1024, device=device)\n \n     @triton.jit\n     def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n@@ -2233,10 +2239,10 @@ def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"has_hints\", [False, True])\n-def test_vectorization_hints(has_hints):\n-    src = torch.empty(1024, device='cuda')\n-    dst = torch.empty(1024, device='cuda')\n-    off = torch.zeros(1, device='cuda', dtype=torch.int32)\n+def test_vectorization_hints(has_hints, device):\n+    src = torch.empty(1024, device=device)\n+    dst = torch.empty(1024, device=device)\n+    off = torch.zeros(1, device=device, dtype=torch.int32)\n \n     @triton.jit\n     def _kernel(dst, src, off, N, BLOCK_SIZE: tl.constexpr, HINT: tl.constexpr):\n@@ -2280,10 +2286,10 @@ def _impl(value=10):\n     return value\n \n \n-def test_default():\n+def test_default(device):\n     value = 5\n-    ret0 = torch.zeros(1, dtype=torch.int32, device='cuda')\n-    ret1 = torch.zeros(1, dtype=torch.int32, device='cuda')\n+    ret0 = torch.zeros(1, dtype=torch.int32, device=device)\n+    ret1 = torch.zeros(1, dtype=torch.int32, device=device)\n \n     @triton.jit\n     def _kernel(ret0, ret1, value):\n@@ -2299,7 +2305,7 @@ def _kernel(ret0, ret1, value):\n # ----------------\n \n \n-def test_noop(device='cuda'):\n+def test_noop(device):\n     @triton.jit\n     def kernel(x):\n         pass\n@@ -2326,7 +2332,7 @@ def kernel(x):\n     (2**31, 'i64'), (2**32 - 1, 'i64'), (2**32, 'i64'), (2**63 - 1, 'i64'),\n     (-2**63, 'i64'), (2**63, 'u64'), (2**64 - 1, 'u64')\n ])\n-def test_value_specialization(value: int, value_type: str, device='cuda') -> None:\n+def test_value_specialization(value: int, value_type: str, device) -> None:\n     spec_type = None\n \n     def cache_hook(*args, **kwargs):\n@@ -2338,7 +2344,7 @@ def cache_hook(*args, **kwargs):\n     def kernel(VALUE, X):\n         pass\n \n-    x = torch.tensor([3.14159], device='cuda')\n+    x = torch.tensor([3.14159], device=device)\n     pgm = kernel[(1, )](value, x)\n \n     JITFunction.cache_hook = None\n@@ -2353,13 +2359,13 @@ def kernel(VALUE, X):\n     \"value, overflow\",\n     [(2**64 - 1, False), (2**64, True), (-2**63, False), (-2**63 - 1, True)]\n )\n-def test_value_specialization_overflow(value: int, overflow: bool, device='cuda') -> None:\n+def test_value_specialization_overflow(value: int, overflow: bool, device) -> None:\n \n     @triton.jit\n     def kernel(VALUE, X):\n         pass\n \n-    x = torch.tensor([3.14159], device='cuda')\n+    x = torch.tensor([3.14159], device=device)\n \n     if overflow:\n         with pytest.raises(OverflowError):\n@@ -2375,7 +2381,7 @@ def kernel(VALUE, X):\n @pytest.mark.parametrize(\"op\", ['+', '-', '*', '/', '%', '<', '>', '<<', '>>', '&', '^', '|'])\n @pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n @pytest.mark.parametrize(\"is_rhs_constexpr\", [True, False])\n-def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr):\n+def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr, device):\n \n     @triton.jit\n     def kernel(Z, X, Y):\n@@ -2396,34 +2402,34 @@ def kernel(Z, X, Y):\n         y = numpy_random((1,), dtype_str=\"float32\")\n     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{x_str} {op} {y_str}\"})\n     z = np.array(eval(f\"{x_str} {op} {y_str}\"))\n-    x_tri = to_triton(x)\n-    y_tri = to_triton(y)\n-    z_tri = to_triton(np.empty((1,), dtype=z.dtype))\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(y, device=device)\n+    z_tri = to_triton(np.empty((1,), dtype=z.dtype), device=device)\n     kernel[(1,)](z_tri, x_tri, y_tri)\n     np.testing.assert_allclose(z, to_numpy(z_tri))\n \n \n-def test_constexpr_shape():\n+def test_constexpr_shape(device):\n \n     @triton.jit\n     def kernel(X):\n         off = tl.arange(0, 128 + 128)\n         tl.store(X + off, off)\n \n-    x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+    x_tri = to_triton(np.empty((256, ), dtype=np.int32), device=device)\n     kernel[(1,)](x_tri)\n     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256))\n \n \n-def test_constexpr_scalar_shape():\n+def test_constexpr_scalar_shape(device):\n \n     @triton.jit\n     def kernel(X, s):\n         off = tl.arange(0, 256)\n         val = off % (256 // s)\n         tl.store(X + off, val)\n \n-    x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+    x_tri = to_triton(np.empty((256, ), dtype=np.int32), device=device)\n     kernel[(1,)](x_tri, 32)\n     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256) % 8)\n \n@@ -2457,7 +2463,7 @@ def vecmul_kernel(ptr, n_elements, rep, type: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"type\", [\"inline\", \"noinline\"])\n-def test_call(type):\n+def test_call(type, device):\n \n     @triton.jit\n     def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n@@ -2466,7 +2472,7 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n \n     size = 1024\n     rand_val = numpy_random((size,), dtype_str=\"float32\")\n-    rand_val_tri = to_triton(rand_val, device='cuda')\n+    rand_val_tri = to_triton(rand_val, device=device)\n     err_msg = \"\"\n     try:\n         kernel[(size // 128,)](rand_val_tri, size, 3, 5, type)\n@@ -2484,8 +2490,8 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # -------------\n \n \n-@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and_dynamic\", \"if_and_static\"])\n-def test_if(if_type):\n+@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and\"])\n+def test_if(if_type, device):\n \n     @triton.jit\n     def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr, StaticVaue: tl.constexpr):\n@@ -2509,16 +2515,17 @@ def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr\n             else:\n                 tl.store(Ret, tl.load(XFalse))\n \n-    cond = torch.ones(1, dtype=torch.int32, device='cuda')\n-    x_true = torch.tensor([3.14], dtype=torch.float32, device='cuda')\n-    x_false = torch.tensor([1.51], dtype=torch.float32, device='cuda')\n-    ret = torch.empty(1, dtype=torch.float32, device='cuda')\n+    cond = torch.ones(1, dtype=torch.int32, device=device)\n+    x_true = torch.tensor([3.14], dtype=torch.float32, device=device)\n+    x_false = torch.tensor([1.51], dtype=torch.float32, device=device)\n+    ret = torch.empty(1, dtype=torch.float32, device=device)\n+\n     kernel[(1,)](cond, x_true, x_false, ret, if_type, True, 1)\n     assert torch.equal(ret, x_true)\n \n \n-def test_num_warps_pow2():\n-    dst = torch.empty(128, device='cuda')\n+def test_num_warps_pow2(device):\n+    dst = torch.empty(128, device=device)\n \n     @triton.jit\n     def _kernel(dst):\n@@ -2542,7 +2549,7 @@ def _kernel(dst):\n                           ('float32', 'math.pow', tl.math.libdevice_path()),\n                           ('float64', 'math.pow_dtype', tl.math.libdevice_path()),\n                           ('float64', 'math.norm4d', '')])\n-def test_math_tensor(dtype_str, expr, lib_path):\n+def test_math_tensor(dtype_str, expr, lib_path, device):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -2583,9 +2590,9 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{expr}(x, x, x, x)'})\n         y_ref = np.sqrt(4 * np.power(x, 2))\n \n-    x_tri = to_triton(x)\n+    x_tri = to_triton(x, device=device)\n     # triton result\n-    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device=device)\n     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n     # compare\n     if expr == 'math.ffs':\n@@ -2598,7 +2605,7 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n                          [('float32', 'math.pow', ''),\n                           ('float64', 'math.pow_dtype', ''),\n                           ('float64', 'math.pow', tl.math.libdevice_path())])\n-def test_math_scalar(dtype_str, expr, lib_path):\n+def test_math_scalar(dtype_str, expr, lib_path, device):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -2623,8 +2630,8 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n         y_ref[:] = np.power(x, 0.5)\n \n     # triton result\n-    x_tri = to_triton(x)[0].item()\n-    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    x_tri = to_triton(x, device=device)[0].item()\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device=device)\n     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n@@ -2637,7 +2644,7 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n @pytest.mark.parametrize(\"lo, hi, iv\", [(2**35, 2**35 + 20, 1), (2**35, 2**35 + 20, 2), (2**35, 2**35 + 20, 3),\n                                         (15, -16, -1), (15, -16, -2), (15, -16, -3),\n                                         (-18, -22, -1), (22, 18, -1)])\n-def test_for_iv(lo, hi, iv):\n+def test_for_iv(lo, hi, iv, device):\n \n     @triton.jit\n     def kernel(Out, lo, hi, iv: tl.constexpr):\n@@ -2649,12 +2656,12 @@ def kernel(Out, lo, hi, iv: tl.constexpr):\n \n     lo = 2**35\n     hi = 2**35 + 20\n-    out = to_triton(np.zeros((1,), dtype=np.int64), device='cuda')\n+    out = to_triton(np.zeros((1,), dtype=np.int64), device=device)\n     kernel[(1,)](out, lo, hi, iv)\n     assert out[0] == sum(range(lo, hi, iv))\n \n \n-def test_if_else():\n+def test_if_else(device):\n \n     @triton.jit\n     def kernel(Cond, TrueVal, FalseVal, Out):\n@@ -2664,10 +2671,10 @@ def kernel(Cond, TrueVal, FalseVal, Out):\n             val = tl.load(FalseVal)\n         tl.store(Out, val)\n \n-    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-    true_val = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n-    false_val = to_triton(np.full((1,), 2, dtype=np.int32), device='cuda')\n-    cond = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+    true_val = to_triton(np.full((1,), 1, dtype=np.int32), device=device)\n+    false_val = to_triton(np.full((1,), 2, dtype=np.int32), device=device)\n+    cond = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     # True\n     cond[0] = True\n     kernel[(1,)](cond, true_val, false_val, out)\n@@ -2679,7 +2686,7 @@ def kernel(Cond, TrueVal, FalseVal, Out):\n \n \n @pytest.mark.parametrize(\"mode\", [\"dynamic\", \"static\"])\n-def test_if_return(mode):\n+def test_if_return(mode, device):\n \n     @triton.jit\n     def kernel(ExitEarly, Out, cond: tl.constexpr, mode: tl.constexpr):\n@@ -2693,8 +2700,8 @@ def kernel(ExitEarly, Out, cond: tl.constexpr, mode: tl.constexpr):\n                 return\n         tl.store(Out, 1)\n \n-    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-    exit_early = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+    exit_early = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     # exit early path taken\n     exit_early[0] = 1\n     kernel[(1,)](exit_early, out, True, mode)\n@@ -2739,7 +2746,7 @@ def add_fn_static_cond(x, cond: tl.constexpr):\n @pytest.mark.parametrize(\"call_type\", [\"attribute\", \"attribute_jit\",\n                                        \"jit\", \"jit_if\", \"jit_ifexp\", \"jit_expr\",\n                                        \"jit_static_cond\", \"jit_noinline\", \"jit_extern\"])\n-def test_if_call(call_type):\n+def test_if_call(call_type, device):\n     @triton.jit\n     def kernel(Out, call_type: tl.constexpr):\n         pid = tl.program_id(0)\n@@ -2798,15 +2805,15 @@ def kernel(Out, call_type: tl.constexpr):\n \n         tl.store(Out, o)\n \n-    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     kernel[(1,)](out, call_type)\n     assert to_numpy(out)[0] == 1\n \n \n @pytest.mark.parametrize(\"_cond1\", [True, False])\n @pytest.mark.parametrize(\"_cond2\", [True, False])\n @pytest.mark.parametrize(\"_cond3\", [True, False])\n-def test_nested_if_else_return(_cond1, _cond2, _cond3):\n+def test_nested_if_else_return(_cond1, _cond2, _cond3, device):\n \n     @triton.jit\n     def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n@@ -2823,13 +2830,13 @@ def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n                 val = tl.load(Val3)\n         tl.store(Out, val)\n \n-    out = to_triton(np.full((1,), -1, dtype=np.int32), device='cuda')\n-    cond1 = to_triton(np.full((1,), _cond1, dtype=np.int32), device='cuda')\n-    cond2 = to_triton(np.full((1,), _cond2, dtype=np.int32), device='cuda')\n-    cond3 = to_triton(np.full((1,), _cond3, dtype=np.int32), device='cuda')\n-    val1 = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n-    val2 = to_triton(np.full((1,), 2, dtype=np.int32), device='cuda')\n-    val3 = to_triton(np.full((1,), 3, dtype=np.int32), device='cuda')\n+    out = to_triton(np.full((1,), -1, dtype=np.int32), device=device)\n+    cond1 = to_triton(np.full((1,), _cond1, dtype=np.int32), device=device)\n+    cond2 = to_triton(np.full((1,), _cond2, dtype=np.int32), device=device)\n+    cond3 = to_triton(np.full((1,), _cond3, dtype=np.int32), device=device)\n+    val1 = to_triton(np.full((1,), 1, dtype=np.int32), device=device)\n+    val2 = to_triton(np.full((1,), 2, dtype=np.int32), device=device)\n+    val3 = to_triton(np.full((1,), 3, dtype=np.int32), device=device)\n     kernel[(1,)](cond1, cond2, cond3, val1, val2, val3, out)\n     targets = {\n         (True, True, True): val1[0],\n@@ -2844,7 +2851,7 @@ def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n     assert out[0] == targets[(_cond1, _cond2, _cond3)]\n \n \n-def test_while():\n+def test_while(device):\n \n     @triton.jit\n     def kernel(InitI, Bound, CutOff, OutI, OutJ):\n@@ -2857,16 +2864,16 @@ def kernel(InitI, Bound, CutOff, OutI, OutJ):\n         tl.store(OutI, curr_i)\n         tl.store(OutJ, j)\n \n-    out_i = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-    out_j = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-    init_i = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n-    bound = to_triton(np.full((1,), 10, dtype=np.int32), device='cuda')\n-    cut_off = to_triton(np.full((1,), 5, dtype=np.int32), device='cuda')\n+    out_i = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+    out_j = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+    init_i = to_triton(np.full((1,), 1, dtype=np.int32), device=device)\n+    bound = to_triton(np.full((1,), 10, dtype=np.int32), device=device)\n+    cut_off = to_triton(np.full((1,), 5, dtype=np.int32), device=device)\n     kernel[(1,)](init_i, bound, cut_off, out_i, out_j)\n     assert out_i[0] == init_i[0] + 1\n     assert out_j[0] == cut_off[0] + 1\n \n-# def test_for_if():\n+# def test_for_if(device):\n \n #     @triton.jit\n #     def kernel(bound, cutoff, M, N):\n@@ -2880,8 +2887,8 @@ def kernel(InitI, Bound, CutOff, OutI, OutJ):\n #         tl.store(M, m)\n #         tl.store(N, n)\n \n-#     m = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-#     n = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+#     m = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+#     n = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n #     kernel[(1,)](10, 7, m, n)\n #     print(m[0])\n #     print(n[0])\n@@ -2891,7 +2898,8 @@ def kernel(InitI, Bound, CutOff, OutI, OutJ):\n # -----------------------\n \n \n-def test_globaltimer():\n+def test_globaltimer(device):\n+    check_cuda_only(device)\n \n     @triton.jit\n     def kernel(Out1, Out2):\n@@ -2902,21 +2910,22 @@ def kernel(Out1, Out2):\n         end = tl.extra.cuda.globaltimer()\n         tl.store(Out2, end - start)\n \n-    out1 = to_triton(np.zeros((128,), dtype=np.int64), device='cuda')\n-    out2 = to_triton(np.zeros((1,), dtype=np.int64), device='cuda')\n+    out1 = to_triton(np.zeros((128,), dtype=np.int64), device=device)\n+    out2 = to_triton(np.zeros((1,), dtype=np.int64), device=device)\n     h = kernel[(1,)](out1, out2)\n     assert out2[0] > 0\n     # 2 inlined globaltimers + one extra in the wrapper extern function\n     assert h.asm[\"ptx\"].count(\"%globaltimer\") == 3\n \n \n-def test_smid():\n+def test_smid(device):\n+    check_cuda_only(device)\n \n     @triton.jit\n     def kernel(Out):\n         tl.store(Out + tl.program_id(0), tl.extra.cuda.smid())\n \n-    out = to_triton(np.zeros((1024,), dtype=np.int32), device='cuda')\n+    out = to_triton(np.zeros((1024,), dtype=np.int32), device=device)\n     h = kernel[(out.shape[0],)](out)\n     assert out.sort()[0].unique().shape[0] > 0\n     assert h.asm[\"ptx\"].count(\"%smid\") == 2\n@@ -2954,7 +2963,7 @@ def kernel(Out):\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"interm_layout\", intermediate_layouts)\n @pytest.mark.parametrize(\"dst_layout\", layouts)\n-def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device='cuda'):\n+def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device):\n     if str(src_layout) == str(dst_layout):\n         pytest.skip()\n     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):\n@@ -3006,7 +3015,7 @@ def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device='\n }\n \"\"\"\n \n-    x = to_triton(numpy_random(shape, dtype_str=dtype))\n+    x = to_triton(numpy_random(shape, dtype_str=dtype), device=device)\n     z = torch.empty_like(x)\n \n     # write the IR to a temporary file using mkstemp\n@@ -3020,23 +3029,23 @@ def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device='\n     assert torch.equal(z, x)\n \n \n-def test_load_scalar_with_mask():\n+def test_load_scalar_with_mask(device):\n     @triton.jit\n     def kernel(Input, Index, Out, N: int):\n         index = tl.load(Index)\n         scalar = tl.load(Input + index, mask=index < N, other=0)\n         tl.store(Out, scalar, mask=index < N)\n-    Index = torch.tensor([0], dtype=torch.int32, device='cuda')\n-    Input = torch.tensor([0], dtype=torch.int32, device='cuda')\n-    Out = torch.empty_like(Index, device='cuda')\n+    Index = torch.tensor([0], dtype=torch.int32, device=device)\n+    Input = torch.tensor([0], dtype=torch.int32, device=device)\n+    Out = torch.empty_like(Index, device=device)\n     kernel[(1,)](Input, Index, Out, Index.numel())\n     assert Out.data[0] == 0\n \n \n # This test is used to test our own PTX codegen for float16 and int16 conversions\n # maybe delete it later after ptxas has been fixed\n @pytest.mark.parametrize(\"dtype_str\", ['float16', 'int16'])\n-def test_ptx_cast(dtype_str):\n+def test_ptx_cast(dtype_str, device):\n     @triton.jit\n     def kernel(in_ptr0, out_ptr2, xnumel, rnumel, dtype: tl.constexpr, XBLOCK: tl.constexpr, RBLOCK: tl.constexpr):\n         xoffset = tl.program_id(0) * XBLOCK\n@@ -3066,7 +3075,7 @@ def kernel(in_ptr0, out_ptr2, xnumel, rnumel, dtype: tl.constexpr, XBLOCK: tl.co\n         triton_dtype = tl.float32\n \n     s0 = 4\n-    buf11 = -torch.ones((6 * s0, 197, 197), device='cuda', dtype=torch_dtype)\n-    buf14 = -torch.ones((s0, 6, 197, 197), device='cuda', dtype=torch_dtype)\n+    buf11 = -torch.ones((6 * s0, 197, 197), device=device, dtype=torch_dtype)\n+    buf14 = -torch.ones((s0, 6, 197, 197), device=device, dtype=torch_dtype)\n     kernel[(4728,)](buf11, buf14, 1182 * s0, 197, triton_dtype, 1, 256, num_warps=2)\n     assert buf14.to(torch.float32).mean() == -2.0"}, {"filename": "python/test/unit/language/test_random.py", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -115,7 +115,7 @@ def random_raw(self):\n                          [(size, seed) for size in ['10', '4,53', '10000']\n                           for seed in [0, 42, 124, 54, 0xffffffff, 0xdeadbeefcafeb0ba]]\n                          )\n-def test_randint(size, seed, device='cuda'):\n+def test_randint(size, seed, device):\n     size = list(map(int, size.split(',')))\n \n     @triton.jit\n@@ -141,7 +141,7 @@ def kernel(X, N, seed):\n                          [(size, seed) for size in [1000000]\n                           for seed in [0, 42, 124, 54]]\n                          )\n-def test_rand(size, seed, device='cuda'):\n+def test_rand(size, seed, device):\n     @triton.jit\n     def kernel(X, N, seed):\n         offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n@@ -162,7 +162,7 @@ def kernel(X, N, seed):\n                          [(size, seed) for size in [1000000]\n                           for seed in [0, 42, 124, 54]]\n                          )\n-def test_randn(size, seed, device='cuda'):\n+def test_randn(size, seed, device):\n     @triton.jit\n     def kernel(X, N, seed):\n         offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n@@ -179,7 +179,7 @@ def kernel(X, N, seed):\n \n # tl.rand() should never produce >=1.0\n \n-def test_rand_limits():\n+def test_rand_limits(device):\n     @triton.jit\n     def kernel(input, output, n: tl.constexpr):\n         idx = tl.arange(0, n)\n@@ -190,8 +190,8 @@ def kernel(input, output, n: tl.constexpr):\n     min_max_int32 = torch.tensor([\n         torch.iinfo(torch.int32).min,\n         torch.iinfo(torch.int32).max,\n-    ], dtype=torch.int32, device='cuda')\n-    output = torch.empty(2, dtype=torch.float32, device='cuda')\n+    ], dtype=torch.int32, device=device)\n+    output = torch.empty(2, dtype=torch.float32, device=device)\n     kernel[(1,)](min_max_int32, output, 2)\n \n     assert output[0] == output[1]"}]