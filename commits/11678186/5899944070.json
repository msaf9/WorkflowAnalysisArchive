[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 8, "deletions": 31, "changes": 39, "file_content_changes": "@@ -11,29 +11,6 @@\n #include <string>\n \n namespace mlir {\n-\n-struct BlockToCFOpsHelper {\n-  BlockToCFOpsHelper(Value v) {\n-    auto moduleOp =\n-        v.getParentBlock()->getParentOp()->getParentOfType<ModuleOp>();\n-    moduleOp.walk([&](Operation *op) {\n-      if (auto br = dyn_cast<cf::BranchOp>(op)) {\n-        Block *block = br.getDest();\n-        blockToCFOps[block].insert({op, -1});\n-      }\n-      if (auto condBr = dyn_cast<cf::CondBranchOp>(op)) {\n-        Block *blockT = condBr.getTrueDest();\n-        Block *blockF = condBr.getFalseDest();\n-        blockToCFOps[blockT].insert({condBr, 1});\n-        blockToCFOps[blockF].insert({condBr, 0});\n-      }\n-    });\n-  }\n-\n-  using BranchOps = llvm::SetVector<std::pair<Operation *, int>>;\n-  llvm::DenseMap<Block *, BranchOps> blockToCFOps;\n-};\n-\n class ReduceOpHelper {\n public:\n   explicit ReduceOpHelper(triton::ReduceOp op)\n@@ -97,7 +74,6 @@ class ScanLoweringHelper {\n   explicit ScanLoweringHelper(triton::ScanOp op) : scanOp(op) {\n     auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n     srcEncoding = type.getEncoding();\n-    srcShapePerCTA = triton::gpu::getShapePerCTA(type);\n   }\n   // Return true if the lowering of the scan op is supported.\n   bool isSupported();\n@@ -135,7 +111,6 @@ class ScanLoweringHelper {\n private:\n   triton::ScanOp scanOp;\n   Attribute srcEncoding;\n-  ArrayRef<int64_t> srcShapePerCTA;\n };\n \n bool maybeSharedAllocationOp(Operation *op);\n@@ -362,17 +337,19 @@ std::unique_ptr<DataFlowSolver> createDataFlowSolver();\n \n triton::MakeTensorPtrOp getMakeTensorPtrOp(Value v);\n \n-SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n-                                                ArrayRef<int64_t> shape,\n-                                                StringRef inputType);\n SmallVector<unsigned, 3>\n mmaVersionToInstrShape(int version, ArrayRef<int64_t> shape, Type type);\n-SmallVector<unsigned, 3>\n-mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n-                       ArrayRef<int64_t> shape);\n \n Value getParentValueWithSameEncoding(Attribute layout, Value value);\n \n+inline Type getInput0EltType(Value value) {\n+  return value.getDefiningOp()\n+      ->getOperands()[0]\n+      .getType()\n+      .cast<RankedTensorType>()\n+      .getElementType();\n+}\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_UTILITY_H"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 4, "deletions": 8, "changes": 12, "file_content_changes": "@@ -39,23 +39,20 @@ SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n SmallVector<unsigned> getSizePerThread(BlockedEncodingAttr layout);\n SmallVector<unsigned> getSizePerThread(Value value);\n \n-SmallVector<unsigned> getSizePerThread(Attribute layout,\n-                                       ArrayRef<int64_t> shapePerCTA);\n-\n // Returns the number of contiguous elements that each thread\n // has access to, on each dimension of the tensor. E.g.\n // for a blocked layout with sizePerThread = [1, 4], returns [1, 4],\n // regardless of the shape of the tensor.\n-SmallVector<unsigned> getContigPerThread(Attribute layout);\n+SmallVector<unsigned> getContigPerThread(BlockedEncodingAttr layout);\n+SmallVector<unsigned> getContigPerThread(Value value);\n \n // Returns the number of non-replicated contiguous elements that each thread\n // has access to, on each dimension of the tensor. For a blocked layout\n // with sizePerThread = [1, 4] and tensor shape = [128, 1], the elements\n // for thread 0 would be [A_{0, 0}, A_{0, 0}, A_{0, 0}, A_{0, 0}], returns [1,\n // 1]. Whereas for a tensor shape [128, 128], the elements for thread 0 would be\n // [A_{0, 0}, A_{0, 1}, A_{0, 2}, A_{0, 3}], returns [1, 4].\n-SmallVector<unsigned> getUniqueContigPerThread(Attribute layout,\n-                                               ArrayRef<int64_t> tensorShape);\n+SmallVector<unsigned> getUniqueContigPerThread(Value value);\n \n // Returns the number of threads per warp that have access to non-replicated\n // elements of the tensor. E.g. for a blocked layout with sizePerThread = [1,\n@@ -71,8 +68,7 @@ getThreadsPerWarpWithUniqueData(Attribute layout,\n // 1], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4] and tensor shape = [2, 2],\n // returns [1, 1], since the first warp has access to the full tensor, whereas\n // the other warps have access to replicated elements.\n-SmallVector<unsigned>\n-getWarpsPerCTAWithUniqueData(Attribute layout, ArrayRef<int64_t> tensorShape);\n+SmallVector<unsigned> getWarpsPerCTAWithUniqueData(Value value);\n \n SmallVector<unsigned> getThreadsPerCTA(Attribute layout);\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 2, "deletions": 5, "changes": 7, "file_content_changes": "@@ -515,8 +515,7 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n     \"unsigned\":$versionMajor,\n     \"unsigned\":$versionMinor,\n     ArrayRefParameter<\"unsigned\">:$warpsPerCTA,\n-    \"CTALayoutAttr\":$CTALayout,\n-    StringRefParameter<\"\">:$inputType\n+    \"CTALayoutAttr\":$CTALayout\n   );\n \n   let builders = [\n@@ -561,7 +560,7 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n           wpt[1] = std::clamp<int>(wpt[1] * 2, 1, shapeC[1] / spw[1]);\n       } while (wpt_nm1 != wpt);\n \n-      return $_get(context, versionMajor, versionMinor, wpt, CTALayout, \"\");\n+      return $_get(context, versionMajor, versionMinor, wpt, CTALayout);\n     }]>,\n \n \n@@ -586,8 +585,6 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n     bool isAmpere() const;\n     bool isHopper() const;\n \n-    unsigned getElemsPerThreadOfOperand(int opIdx, ArrayRef<int64_t> shape) const;\n-\n     // Get [isARow, isBRow, isAVec4, isBVec4, id] from versionMinor\n     std::tuple<bool, bool, bool, bool, int> decodeVoltaLayoutStates() const;\n "}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -78,8 +78,8 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   assert(srcLayout && dstLayout &&\n          \"Unexpected layout in getScratchConfigForCvtLayout()\");\n   auto [inOrd, outOrd] = getCvtOrder(srcLayout, dstLayout);\n-  unsigned srcContigPerThread = getContigPerThread(srcLayout)[inOrd[0]];\n-  unsigned dstContigPerThread = getContigPerThread(dstLayout)[outOrd[0]];\n+  unsigned srcContigPerThread = getContigPerThread(op.getSrc())[inOrd[0]];\n+  unsigned dstContigPerThread = getContigPerThread(op.getResult())[outOrd[0]];\n   // TODO: Fix the legacy issue that ourOrd[0] == 0 always means\n   //       that we cannot do vectorization.\n   inVec = outOrd[0] == 0 ? 1 : inOrd[0] == 0 ? 1 : srcContigPerThread;"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -949,8 +949,7 @@ unsigned ModuleAxisInfoAnalysis::getPtrContiguity(Value ptr) {\n   auto order = triton::gpu::getOrder(layout);\n   unsigned align = getPtrAlignment(ptr);\n \n-  auto uniqueContigPerThread =\n-      triton::gpu::getUniqueContigPerThread(layout, tensorTy.getShape());\n+  auto uniqueContigPerThread = triton::gpu::getUniqueContigPerThread(ptr);\n   assert(order[0] < uniqueContigPerThread.size() &&\n          \"Unxpected uniqueContigPerThread size\");\n   unsigned contiguity = uniqueContigPerThread[order[0]];"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 97, "deletions": 44, "changes": 141, "file_content_changes": "@@ -102,15 +102,15 @@ unsigned ReduceOpHelper::getIntraWarpSize() {\n unsigned ReduceOpHelper::getInterWarpSizeWithUniqueData() {\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n   unsigned sizeIntraWarps = getIntraWarpSizeWithUniqueData();\n-  return std::min(srcReduceDimSize / sizeIntraWarps,\n-                  triton::gpu::getWarpsPerCTAWithUniqueData(\n-                      getSrcLayout(), getSrcShape())[axis]);\n+  return std::min(\n+      srcReduceDimSize / sizeIntraWarps,\n+      triton::gpu::getWarpsPerCTAWithUniqueData(getSrcValue())[axis]);\n }\n \n unsigned ReduceOpHelper::getIntraWarpSizeWithUniqueData() {\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n-  unsigned elementPerThreads = triton::gpu::getUniqueContigPerThread(\n-      getSrcLayout(), getSrcShape())[axis];\n+  unsigned elementPerThreads =\n+      triton::gpu::getUniqueContigPerThread(getSrcValue())[axis];\n   return std::min(srcReduceDimSize / elementPerThreads,\n                   triton::gpu::getThreadsPerWarpWithUniqueData(\n                       getSrcLayout(), getSrcShape())[axis]);\n@@ -121,7 +121,7 @@ unsigned ReduceOpHelper::getThreadsReductionAxis() {\n   auto srcShape = getSrcShape();\n   return triton::gpu::getThreadsPerWarpWithUniqueData(srcLayout,\n                                                       srcShape)[axis] *\n-         triton::gpu::getWarpsPerCTAWithUniqueData(srcLayout, srcShape)[axis];\n+         triton::gpu::getWarpsPerCTAWithUniqueData(getSrcValue())[axis];\n }\n \n SmallVector<unsigned> ReduceOpHelper::getScratchConfigBasic() {\n@@ -232,8 +232,7 @@ unsigned ScanLoweringHelper::getAxisNumWarps() {\n \n unsigned ScanLoweringHelper::getAxisNumBlocks() {\n   auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n-  auto sizePerThreads =\n-      triton::gpu::getSizePerThread(srcEncoding, srcShapePerCTA);\n+  auto sizePerThreads = triton::gpu::getSizePerThread(scanOp.getOperand(0));\n   auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n   auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n   unsigned axis = getAxis();\n@@ -244,8 +243,7 @@ unsigned ScanLoweringHelper::getAxisNumBlocks() {\n \n unsigned ScanLoweringHelper::getNonAxisNumBlocks() {\n   auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n-  auto sizePerThreads =\n-      triton::gpu::getSizePerThread(srcEncoding, srcShapePerCTA);\n+  auto sizePerThreads = triton::gpu::getSizePerThread(scanOp.getOperand(0));\n   auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n   auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n   unsigned axis = getAxis();\n@@ -313,8 +311,7 @@ unsigned ScanLoweringHelper::getAxisBlockStride() {\n   auto order = triton::gpu::getOrder(srcEncoding);\n   unsigned stride = 1;\n   auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n-  auto sizePerThreads =\n-      triton::gpu::getSizePerThread(srcEncoding, srcShapePerCTA);\n+  auto sizePerThreads = triton::gpu::getSizePerThread(scanOp.getOperand(0));\n   auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n   auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n   for (unsigned dim : order) {\n@@ -636,14 +633,39 @@ std::unique_ptr<DataFlowSolver> createDataFlowSolver() {\n   return solver;\n }\n \n-static triton::MakeTensorPtrOp getMakeTensorPtrOpImpl(Operation *op, Value v) {\n+struct BlockToCFOpsHelper {\n+  BlockToCFOpsHelper(Value v) {\n+    auto moduleOp =\n+        v.getParentBlock()->getParentOp()->getParentOfType<ModuleOp>();\n+    moduleOp.walk([&](Operation *op) {\n+      if (auto br = dyn_cast<cf::BranchOp>(op)) {\n+        Block *block = br.getDest();\n+        blockToCFOps[block].insert({op, -1});\n+      }\n+      if (auto condBr = dyn_cast<cf::CondBranchOp>(op)) {\n+        Block *blockT = condBr.getTrueDest();\n+        Block *blockF = condBr.getFalseDest();\n+        blockToCFOps[blockT].insert({condBr, 1});\n+        blockToCFOps[blockF].insert({condBr, 0});\n+      }\n+    });\n+  }\n+\n+  using BranchOps = llvm::SetVector<std::pair<Operation *, int>>;\n+  llvm::DenseMap<Block *, BranchOps> blockToCFOps;\n+};\n+\n+static triton::MakeTensorPtrOp getMakeTensorPtrOp_(BlockToCFOpsHelper &helper,\n+                                                   Value v);\n+\n+static triton::MakeTensorPtrOp\n+getMakeTensorPtrOpImpl(BlockToCFOpsHelper &helper, Operation *op, Value v) {\n \n   if (auto makeTensorPtrOp = dyn_cast<triton::MakeTensorPtrOp>(op)) {\n     return makeTensorPtrOp;\n   }\n-\n   if (auto advanceOp = dyn_cast<triton::AdvanceOp>(op)) {\n-    return getMakeTensorPtrOp(advanceOp.getPtr());\n+    return getMakeTensorPtrOp_(helper, advanceOp.getPtr());\n   }\n \n   if (auto branch = dyn_cast<RegionBranchOpInterface>(op)) {\n@@ -656,47 +678,95 @@ static triton::MakeTensorPtrOp getMakeTensorPtrOpImpl(Operation *op, Value v) {\n \n     // benzh@ if multi yields, all yields operand should come from same arg.\n     Value newValue = yieldOps[0].getOperands()[idx];\n-    return getMakeTensorPtrOp(newValue);\n+    return getMakeTensorPtrOp_(helper, newValue);\n   }\n \n   llvm_unreachable(\"Unable to getMakeTensorPtr()\");\n }\n-\n-triton::MakeTensorPtrOp getMakeTensorPtrOp(Value v) {\n-  BlockToCFOpsHelper helper(v);\n-\n+static triton::MakeTensorPtrOp getMakeTensorPtrOp_(BlockToCFOpsHelper &helper,\n+                                                   Value v) {\n   if (Operation *definingOp = v.getDefiningOp()) {\n-    return getMakeTensorPtrOpImpl(definingOp, v);\n+    return getMakeTensorPtrOpImpl(helper, definingOp, v);\n   } else if (BlockArgument arg = v.cast<BlockArgument>()) {\n     unsigned argNum = arg.getArgNumber();\n     Operation *argOwner = arg.getOwner()->getParentOp();\n \n     if (auto forOp = dyn_cast<scf::ForOp>(argOwner)) {\n-      return getMakeTensorPtrOp(\n-          forOp.getOperand(argNum + forOp.getNumControlOperands() - 1));\n+      return getMakeTensorPtrOp_(\n+          helper, forOp.getOperand(argNum + forOp.getNumControlOperands() - 1));\n     } else if (auto funcOp = dyn_cast<mlir::triton::FuncOp>(argOwner)) {\n       Block *block = arg.getOwner();\n       Operation *op;\n       int tOrF;\n       std::tie(op, tOrF) = helper.blockToCFOps[block][0];\n       if (auto br = dyn_cast<cf::BranchOp>(op)) {\n-        return getMakeTensorPtrOp(br.getDestOperands()[argNum]);\n+        return getMakeTensorPtrOp_(helper, br.getDestOperands()[argNum]);\n       }\n       if (auto condBr = dyn_cast<cf::CondBranchOp>(op)) {\n         if (tOrF) {\n-          return getMakeTensorPtrOp(condBr.getTrueDestOperands()[argNum]);\n+          return getMakeTensorPtrOp_(helper,\n+                                     condBr.getTrueDestOperands()[argNum]);\n         } else {\n-          return getMakeTensorPtrOp(condBr.getFalseDestOperands()[argNum]);\n+          return getMakeTensorPtrOp_(helper,\n+                                     condBr.getFalseDestOperands()[argNum]);\n         }\n       }\n     } else {\n-      return getMakeTensorPtrOp(argOwner->getOperand(argNum));\n+      return getMakeTensorPtrOp_(helper, argOwner->getOperand(argNum));\n     }\n   }\n \n   llvm_unreachable(\"Unable to getMakeTensorPtr()\");\n }\n \n+triton::MakeTensorPtrOp getMakeTensorPtrOp(Value v) {\n+  BlockToCFOpsHelper helper(v);\n+  return getMakeTensorPtrOp_(helper, v);\n+}\n+\n+static Value getParentValueWithSameEncoding_(BlockToCFOpsHelper &helper,\n+                                             Attribute layout, Value value) {\n+  if (Operation *defOp = value.getDefiningOp()) {\n+    for (Value operand : defOp->getOperands()) {\n+      if (auto type = operand.getType().cast<RankedTensorType>()) {\n+        if (type.getEncoding() == layout)\n+          return operand;\n+      }\n+    }\n+    llvm_unreachable(\"no operand has same encoding\");\n+  } else if (BlockArgument arg = value.cast<BlockArgument>()) {\n+    unsigned argNum = arg.getArgNumber();\n+    Operation *argOwner = arg.getOwner()->getParentOp();\n+    Value newValue;\n+    if (auto forOp = dyn_cast<scf::ForOp>(argOwner))\n+      newValue = forOp.getOperand(argNum + forOp.getNumControlOperands() - 1);\n+    else if (auto funcOp = dyn_cast<mlir::triton::FuncOp>(argOwner)) {\n+      Block *block = arg.getOwner();\n+      Operation *op;\n+      int tOrF = 0;\n+      std::tie(op, tOrF) = helper.blockToCFOps[block][0];\n+      if (auto br = dyn_cast<cf::BranchOp>(op)) {\n+        newValue = br.getDestOperands()[argNum];\n+      }\n+      if (auto condBr = dyn_cast<cf::CondBranchOp>(op)) {\n+        if (tOrF)\n+          newValue = condBr.getTrueDestOperands()[argNum];\n+        else\n+          newValue = condBr.getFalseDestOperands()[argNum];\n+      }\n+    } else\n+      newValue = argOwner->getOperand(argNum);\n+    return getParentValueWithSameEncoding_(helper, layout, newValue);\n+  }\n+  llvm_unreachable(\"cannot find defOp and not block argument\");\n+  return {};\n+}\n+\n+Value getParentValueWithSameEncoding(Attribute layout, Value value) {\n+  BlockToCFOpsHelper helper(value);\n+  return getParentValueWithSameEncoding_(helper, layout, value);\n+}\n+\n SmallVector<unsigned, 3>\n mmaVersionToInstrShape(int version, ArrayRef<int64_t> shape, Type eltType) {\n   if (version == 1)\n@@ -730,28 +800,11 @@ mmaVersionToInstrShape(int version, ArrayRef<int64_t> shape, Type eltType) {\n         return {16, n, k};\n       }\n     }\n-\n     assert(false && \"type not supported\");\n     return {0, 0, 0};\n   } else {\n     assert(false && \"version not supported\");\n     return {0, 0};\n   }\n }\n-\n-SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n-                                                ArrayRef<int64_t> shape,\n-                                                StringRef inputType) {\n-  return {0};\n-}\n-\n-SmallVector<unsigned, 3>\n-mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n-                       ArrayRef<int64_t> shape) {\n-  return {0};\n-}\n-Value getParentValueWithSameEncoding(Attribute layout, Value value) {\n-  return {};\n-}\n-\n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -11,7 +11,6 @@ using ::mlir::LLVM::linearize;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStridesFromShapeAndOrder;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n-using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getShapePerCTATile;\n@@ -116,7 +115,7 @@ struct ConvertLayoutOpConversion\n           emitBaseIndexForLayout(loc, rewriter, value, false);\n       SmallVector<Value> multiDimOffset(rank);\n       SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n-          elemId, getSizePerThread(layout, {}), getOrder(layout));\n+          elemId, getSizePerThread(value), getOrder(layout));\n       for (unsigned d = 0; d < rank; ++d) {\n         multiDimOffset[d] =\n             add(multiDimOffsetFirstElem[d],\n@@ -152,7 +151,8 @@ struct ConvertLayoutOpConversion\n     }\n     if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n       auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n-      auto instrShape = mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n+      auto instrShape = mmaVersionToInstrShape(\n+          mmaLayout.getVersionMajor(), shapePerCTA, getInput0EltType(value));\n       SmallVector<Value> mmaColIdx(4);\n       SmallVector<Value> mmaRowIdx(2);\n       Value threadId = getThreadId(rewriter, loc);\n@@ -248,7 +248,7 @@ struct ConvertLayoutOpConversion\n     auto layout = type.getEncoding();\n     auto rank = type.getRank();\n     auto shapePerCTA = getShapePerCTA(layout, type.getShape());\n-    auto sizePerThread = getSizePerThread(layout, shapePerCTA);\n+    auto sizePerThread = getSizePerThread(value);\n     auto accumSizePerThread = product<unsigned>(sizePerThread);\n     SmallVector<unsigned> numCTATiles(rank);\n     auto shapePerCTATile = getShapePerCTATile(value);\n@@ -738,7 +738,8 @@ struct ConvertLayoutOpConversion\n                                                          rewriter, srcTy);\n \n       auto srcShapePerCTA = getShapePerCTA(mmaLayout, srcShape);\n-      auto instrShape = mmaVersionToInstrShape(mmaLayout, srcShapePerCTA);\n+      auto instrShape = mmaVersionToInstrShape(\n+          mmaLayout.getVersionMajor(), srcShapePerCTA, getInput0EltType(src));\n       auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n       uint32_t repM =\n           ceil<unsigned>(srcShapePerCTA[0], instrShape[0] * warpsPerCTA[0]);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -7,7 +7,6 @@ using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStridesFromShapeAndOrder;\n using ::mlir::LLVM::linearize;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n-using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getSizePerThread;\n@@ -47,7 +46,7 @@ int getShapePerCTATileForMN(BlockedEncodingAttr layout, bool isM) {\n // Get sizePerThread for M or N axis.\n int getSizePerThreadForMN(BlockedEncodingAttr layout, bool isM) {\n   auto order = layout.getOrder();\n-  auto sizePerThread = getSizePerThread(layout, {});\n+  auto sizePerThread = getSizePerThread(layout);\n \n   int mSizePerThread =\n       order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n@@ -111,7 +110,7 @@ Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n   int M = aShapePerCTA[0];\n \n   auto shapePerCTATile = getShapePerCTATile(dLayout);\n-  auto sizePerThread = getSizePerThread(dLayout, {});\n+  auto sizePerThread = getSizePerThread(dLayout);\n \n   Value _0 = i32_val(0);\n \n@@ -175,7 +174,7 @@ Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n   int N = bShapePerCTA[1];\n \n   auto shapePerCTATile = getShapePerCTATile(dLayout);\n-  auto sizePerThread = getSizePerThread(dLayout, {});\n+  auto sizePerThread = getSizePerThread(dLayout);\n \n   Value _0 = i32_val(0);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -6,7 +6,6 @@ using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStridesFromShapeAndOrder;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n-using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getTotalElemsPerThread;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -8,7 +8,6 @@ using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStridesFromShapeAndOrder;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n-using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getTotalElemsPerThread;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/FMA.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -53,8 +53,8 @@ LogicalResult convertFMADot(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n   Value llA = adaptor.getA();\n   Value llB = adaptor.getB();\n \n-  auto sizePerThread = getSizePerThread(dLayout, {});\n-  auto shapePerCTATile = mlir::triton::gpu::getShapePerCTATile(D);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+  auto shapePerCTATile = getShapePerCTATile(dLayout);\n \n   int K = aShapePerCTA[1];\n   int M = aShapePerCTA[0];"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/WGMMA.cpp", "status": "modified", "additions": 14, "deletions": 5, "changes": 19, "file_content_changes": "@@ -161,7 +161,11 @@ DotOpMmaV3SmemLoader loadA(TritonGPUToLLVMTypeConverter *typeConverter,\n   auto aSharedLayout = aTensorTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n   assert(aSharedLayout && \"only support load dot operand from shared.\");\n   auto shapePerCTA = getShapePerCTA(aTensorTy);\n-  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA);\n+  auto dotShapePerCTA =\n+      getShapePerCTA((*tensor.use_begin()).getOwner()->getResult(0).getType());\n+  auto instrShape =\n+      mmaVersionToInstrShape(mmaEncoding.getVersionMajor(), dotShapePerCTA,\n+                             aTensorTy.getElementType());\n   auto wpt = mmaEncoding.getWarpsPerCTA();\n   auto aOrd = aSharedLayout.getOrder();\n   bool transA = aOrd[0] == 0;\n@@ -192,7 +196,11 @@ DotOpMmaV3SmemLoader loadB(TritonGPUToLLVMTypeConverter *typeConverter,\n   auto bSharedLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n   assert(bSharedLayout && \"only support load B from shared.\");\n   auto shapePerCTA = triton::gpu::getShapePerCTA(bTensorTy);\n-  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA);\n+  auto dotShapePerCTA =\n+      getShapePerCTA((*tensor.use_begin()).getOwner()->getResult(0).getType());\n+  auto instrShape =\n+      mmaVersionToInstrShape(mmaEncoding.getVersionMajor(), dotShapePerCTA,\n+                             bTensorTy.getElementType());\n \n   auto wpt = mmaEncoding.getWarpsPerCTA();\n   auto bOrd = bSharedLayout.getOrder();\n@@ -279,7 +287,8 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n   bool transA = aOrd[0] == 0;\n   bool transB = bOrd[0] == 1;\n   auto dShapePerCTA = getShapePerCTA(dTensorTy);\n-  auto instrShape = mmaVersionToInstrShape(mmaEncoding, dShapePerCTA);\n+  auto instrShape = mmaVersionToInstrShape(\n+      mmaEncoding.getVersionMajor(), dShapePerCTA, aTensorTy.getElementType());\n \n   auto accSize = 2 * (instrShape[1] / 4);\n   int M = 4 * instrShape[0];\n@@ -362,9 +371,9 @@ Value loadC(Value dTensor, Value tensor, Value llTensor) {\n   auto mmaEncoding = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n   assert(mmaEncoding && \"Currently, we only support $c with a mma layout.\");\n   auto shapePerCTA = getShapePerCTA(tensorTy);\n-  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA);\n+  auto instrShape = mmaVersionToInstrShape(\n+      mmaEncoding.getVersionMajor(), shapePerCTA, getInput0EltType(dTensor));\n   auto wpt = mmaEncoding.getWarpsPerCTA();\n-  // benzh@need from ACC\n   auto shapePerCTATile = getShapePerCTATile(dTensor);\n \n   int numRepM = ceil<unsigned>(shapePerCTA[0], shapePerCTATile[0]);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 10, "deletions": 55, "changes": 65, "file_content_changes": "@@ -87,72 +87,30 @@ struct ReduceOpConversion\n     return srcValues;\n   }\n \n-  Value getParentValueWithSameLayout(BlockToCFOpsHelper &helper, Value value,\n-                                     Attribute attr) const {\n-    if (Operation *defOp = value.getDefiningOp()) {\n-      for (Value operand : defOp->getOperands()) {\n-        if (auto type = operand.getType().cast<RankedTensorType>()) {\n-          if (type.getEncoding() == attr)\n-            return operand;\n-        }\n-      }\n-      llvm_unreachable(\"no operand has same encoding\");\n-    } else if (BlockArgument arg = value.cast<BlockArgument>()) {\n-      unsigned argNum = arg.getArgNumber();\n-      Operation *argOwner = arg.getOwner()->getParentOp();\n-      Value newValue;\n-      if (auto forOp = dyn_cast<scf::ForOp>(argOwner))\n-        Value newValue =\n-            forOp.getOperand(argNum + forOp.getNumControlOperands() - 1);\n-      else if (auto funcOp = dyn_cast<mlir::triton::FuncOp>(argOwner)) {\n-        Block *block = arg.getOwner();\n-        Operation *op;\n-        int tOrF = 0;\n-        std::tie(op, tOrF) = helper.blockToCFOps[block][0];\n-        Value newValue;\n-        if (auto br = dyn_cast<cf::BranchOp>(op)) {\n-          newValue = br.getDestOperands()[argNum];\n-        }\n-        if (auto condBr = dyn_cast<cf::CondBranchOp>(op)) {\n-          if (tOrF)\n-            newValue = condBr.getTrueDestOperands()[argNum];\n-          else\n-            newValue = condBr.getFalseDestOperands()[argNum];\n-        }\n-      } else\n-        newValue = argOwner->getOperand(argNum);\n-      return getParentValueWithSameLayout(helper, value, attr);\n-    }\n-    llvm_unreachable(\"cannot find defOp and not block argument\");\n-    return {};\n-  }\n-\n   // Calculates the write index in the shared memory where we would be writing\n   // the within-thread accumulations before we start doing across-threads\n   // accumulations. `index` is the index of the within-thread accumulations in\n   // the full tensor, whereas `writeIdx` is the mapped-to index in the shared\n   // memory\n   void getWriteIndexBasic(ConversionPatternRewriter &rewriter, Location loc,\n-                          BlockToCFOpsHelper &helper, Value value,\n-                          Attribute layout, SmallVector<Value> &index,\n+                          Value value, Attribute layout,\n+                          SmallVector<Value> &index,\n                           SmallVector<Value> &writeIdx,\n                           std::map<int, Value> &ints, unsigned originalAxis,\n                           unsigned axis) const {\n     if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n       // Recover the axis in the parent layout\n       auto parentAxis = axis < sliceLayout.getDim() ? axis : axis + 1;\n       auto parentLayout = sliceLayout.getParent();\n-      auto parentValue =\n-          getParentValueWithSameLayout(helper, value, parentLayout);\n+      auto parentValue = getParentValueWithSameEncoding(parentLayout, value);\n \n-      getWriteIndexBasic(rewriter, loc, helper, parentValue, parentLayout,\n-                         index, writeIdx, ints, originalAxis, parentAxis);\n+      getWriteIndexBasic(rewriter, loc, parentValue, parentLayout, index,\n+                         writeIdx, ints, originalAxis, parentAxis);\n       return;\n     }\n \n     writeIdx = index;\n-    auto sizePerThread = triton::gpu::getSizePerThread(\n-        layout, triton::gpu::getShapePerCTA(value.getType()));\n+    auto sizePerThread = triton::gpu::getSizePerThread(value);\n     Value axisSizePerThread = ints[sizePerThread[axis]];\n     Value _8 = ints[8];\n     Value _16 = ints[16];\n@@ -199,8 +157,7 @@ struct ReduceOpConversion\n     // The order of the axes for the the threads within the warp\n     auto srcOrd = triton::gpu::getOrder(srcLayout);\n     auto srcShape = helper.getSrcShape();\n-    auto sizePerThread = triton::gpu::getSizePerThread(\n-        srcLayout, triton::gpu::getShapePerCTA(srcLayout, srcShape));\n+    auto sizePerThread = triton::gpu::getSizePerThread(helper.getSrcValue());\n \n     SmallVector<Type> elemPtrTys(srcTys.size());\n     for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n@@ -237,15 +194,13 @@ struct ReduceOpConversion\n     ints[16] = i32_val(16);\n \n     // reduce across threads\n-    BlockToCFOpsHelper cfHelper(helper.getSrcValue());\n-\n     for (auto it : accs) {\n       const SmallVector<unsigned> &key = it.first;\n       auto &acc = it.second;\n       // get the writeIdx at which to write in smem\n       SmallVector<Value> writeIdx;\n-      getWriteIndexBasic(rewriter, loc, cfHelper, helper.getSrcValue(),\n-                         srcLayout, indices[key], writeIdx, ints, axis, axis);\n+      getWriteIndexBasic(rewriter, loc, helper.getSrcValue(), srcLayout,\n+                         indices[key], writeIdx, ints, axis, axis);\n \n       // calculate the offset in smem for that writeIdx\n       Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, srcOrd);\n@@ -511,7 +466,7 @@ struct ReduceOpConversion\n     auto threadsPerWarp =\n         triton::gpu::getThreadsPerWarpWithUniqueData(srcLayout, srcShape);\n     auto warpsPerCTA =\n-        triton::gpu::getWarpsPerCTAWithUniqueData(srcLayout, srcShape);\n+        triton::gpu::getWarpsPerCTAWithUniqueData(helper.getSrcValue());\n     auto order = getOrder(srcLayout);\n     SmallVector<Value> multiDimLaneId =\n         delinearize(rewriter, loc, laneId, threadsPerWarp, order);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 8, "deletions": 17, "changes": 25, "file_content_changes": "@@ -451,9 +451,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto inOrd = triton::gpu::getOrder(srcSharedLayout);\n     auto outOrd = triton::gpu::getOrder(dstDistributedLayout);\n     unsigned outVec =\n-        inOrd == outOrd\n-            ? triton::gpu::getContigPerThread(dstDistributedLayout)[outOrd[0]]\n-            : 1;\n+        inOrd == outOrd ? triton::gpu::getContigPerThread(dst)[outOrd[0]] : 1;\n     unsigned inVec = srcSharedLayout.getVec();\n     unsigned minVec = std::min(outVec, inVec);\n     unsigned outElems = triton::gpu::getTotalElemsPerThread(dstTy);\n@@ -500,9 +498,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto inOrd = triton::gpu::getOrder(srcDistributedLayout);\n     auto outOrd = dstSharedLayout.getOrder();\n     unsigned inVec =\n-        inOrd == outOrd\n-            ? triton::gpu::getContigPerThread(srcDistributedLayout)[inOrd[0]]\n-            : 1;\n+        inOrd == outOrd ? triton::gpu::getContigPerThread(src)[inOrd[0]] : 1;\n     unsigned outVec = dstSharedLayout.getVec();\n     unsigned minVec = std::min(outVec, inVec);\n     unsigned numElems = triton::gpu::getTotalElemsPerThread(srcTy);\n@@ -546,7 +542,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       auto shape = tensorTy.getShape();\n       unsigned rank = shape.size();\n       auto shapePerCTA = triton::gpu::getShapePerCTA(tensorTy);\n-      auto sizePerThread = triton::gpu::getSizePerThread(layout, shapePerCTA);\n+      auto sizePerThread = triton::gpu::getSizePerThread(value);\n       auto threadsPerWarp = triton::gpu::getThreadsPerWarp(layout);\n       auto warpsPerCTA = triton::gpu::getWarpsPerCTA(layout);\n       auto order = triton::gpu::getOrder(layout);\n@@ -666,7 +662,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n           result = emitBaseIndexWithinCTAForMmaLayoutV1(loc, rewriter,\n                                                         mmaLayout, type);\n         if (mmaLayout.isAmpere() || mmaLayout.isHopper())\n-          result = emitBaseIndexWithinCTAForMmaLayoutV2V3(loc, rewriter,\n+          result = emitBaseIndexWithinCTAForMmaLayoutV2V3(loc, rewriter, value,\n                                                           mmaLayout, type);\n       } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n         auto parentLayout = sliceLayout.getParent();\n@@ -1009,15 +1005,15 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   }\n \n   SmallVector<Value> emitBaseIndexWithinCTAForMmaLayoutV2V3(\n-      Location loc, ConversionPatternRewriter &rewriter,\n+      Location loc, ConversionPatternRewriter &rewriter, Value value,\n       const MmaEncodingAttr &mmaLayout, RankedTensorType type) const {\n     auto shape = type.getShape();\n     auto _warpsPerCTA = mmaLayout.getWarpsPerCTA();\n     assert(_warpsPerCTA.size() == 2);\n     auto order = triton::gpu::getOrder(mmaLayout);\n     auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n-    ArrayRef<unsigned int> instrShape =\n-        mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n+    ArrayRef<unsigned int> instrShape = mmaVersionToInstrShape(\n+        mmaLayout.getVersionMajor(), shapePerCTA, getInput0EltType(value));\n     SmallVector<Value> warpsPerCTA = {i32_val(_warpsPerCTA[0]),\n                                       i32_val(_warpsPerCTA[1])};\n \n@@ -1071,13 +1067,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto shape = type.getShape();\n     auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n     SmallVector<SmallVector<unsigned>> ret;\n-    auto inputType = value.getDefiningOp()\n-                         ->getOperands()[0]\n-                         .getType()\n-                         .cast<RankedTensorType>()\n-                         .getElementType();\n     ArrayRef<unsigned int> instrShape = mmaVersionToInstrShape(\n-        mmaLayout.getVersionMajor(), shapePerCTA, inputType);\n+        mmaLayout.getVersionMajor(), shapePerCTA, getInput0EltType(value));\n \n     for (unsigned i = 0; i < shapePerCTA[0];\n          i += triton::gpu::getShapePerCTATile(value)[0]) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -655,7 +655,7 @@ class ConvertTritonGPUToLLVM\n             dstType.getShape(), dstType.getElementType(),\n             triton::gpu::BlockedEncodingAttr::get(\n                 mod.getContext(), srcType.getShape(),\n-                getSizePerThread(srcMma, triton::gpu::getShapePerCTA(srcType)),\n+                triton::gpu::getSizePerThread(cvtOp.getOperand()),\n                 getOrder(srcMma), numWarps, threadsPerWarp, numCTAs));\n         auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n             cvtOp.getLoc(), tmpType, cvtOp.getOperand());"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 65, "deletions": 88, "changes": 153, "file_content_changes": "@@ -159,38 +159,45 @@ SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n   return {};\n }\n \n-SmallVector<unsigned>\n-getWarpsPerCTAWithUniqueData(Attribute layout, ArrayRef<int64_t> tensorShape) {\n+SmallVector<unsigned> getWarpsPerCTAWithUniqueData(Value value) {\n+  auto type = value.getType().cast<RankedTensorType>();\n+  auto layout = type.getEncoding();\n+  auto tensorShape = type.getShape();\n   if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parentLayout = sliceLayout.getParent();\n-    auto parentShape = sliceLayout.paddedShape(tensorShape);\n-    auto parentWarpsPerCTA =\n-        getWarpsPerCTAWithUniqueData(parentLayout, parentShape);\n-    SmallVector<unsigned> warpsPerCTA = parentWarpsPerCTA;\n-    warpsPerCTA.erase(warpsPerCTA.begin() + sliceLayout.getDim());\n-    return warpsPerCTA;\n+    auto parentValue = getParentValueWithSameEncoding(parentLayout, value);\n+    auto parentWarpsPerCTA = getWarpsPerCTAWithUniqueData(parentValue);\n+    parentWarpsPerCTA.erase(parentWarpsPerCTA.begin() + sliceLayout.getDim());\n+    return parentWarpsPerCTA;\n   }\n   auto warpsPerCTA = getWarpsPerCTA(layout);\n   assert(warpsPerCTA.size() == tensorShape.size() &&\n          \"layout and tensor shape must have the same rank\");\n   for (unsigned i = 0; i < warpsPerCTA.size(); i++) {\n     auto sizePerWarp =\n-        getSizePerThread(layout, {})[i] * getThreadsPerWarp(layout)[i];\n+        getSizePerThread(value)[i] * getThreadsPerWarp(layout)[i];\n     auto maxWarpsPerDim = ceil<unsigned>(tensorShape[i], sizePerWarp);\n     warpsPerCTA[i] = std::min<unsigned>(warpsPerCTA[i], maxWarpsPerDim);\n   }\n \n   return warpsPerCTA;\n }\n \n-SmallVector<unsigned> getSizePerThread(Attribute layout,\n-                                       ArrayRef<int64_t> shapePerCTA) {\n+SmallVector<unsigned> getSizePerThread(BlockedEncodingAttr layout) {\n+  return SmallVector<unsigned>(layout.getSizePerThread().begin(),\n+                               layout.getSizePerThread().end());\n+}\n+\n+SmallVector<unsigned> getSizePerThread(Value value) {\n+  auto type = value.getType().cast<RankedTensorType>();\n+  auto layout = type.getEncoding();\n+\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-    return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n-                                 blockedLayout.getSizePerThread().end());\n+    return getSizePerThread(blockedLayout);\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-    // [benzh] should here provide parent shape???\n-    auto sizePerThread = getSizePerThread(sliceLayout.getParent(), shapePerCTA);\n+    auto parentLayout = sliceLayout.getParent();\n+    auto parentValue = getParentValueWithSameEncoding(parentLayout, value);\n+    auto sizePerThread = getSizePerThread(parentValue);\n     sizePerThread.erase(sizePerThread.begin() + sliceLayout.getDim());\n     return sizePerThread;\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n@@ -199,7 +206,9 @@ SmallVector<unsigned> getSizePerThread(Attribute layout,\n     } else if (mmaLayout.isVolta()) {\n       return {1, 2};\n     } else if (mmaLayout.isHopper()) {\n-      auto instrShape = mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n+      auto shapePerCTA = getShapePerCTA(type);\n+      auto instrShape = mmaVersionToInstrShape(\n+          mmaLayout.getVersionMajor(), shapePerCTA, getInput0EltType(value));\n       // TODO(thomas): what are those magic numbers?\n       return SmallVector<unsigned>{instrShape[0] * 4 / 32, instrShape[1] / 4};\n     } else {\n@@ -209,57 +218,66 @@ SmallVector<unsigned> getSizePerThread(Attribute layout,\n     auto parentLayout = dotLayout.getParent();\n     assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n     if (auto parentMmaLayout = parentLayout.dyn_cast<MmaEncodingAttr>()) {\n-      assert(parentMmaLayout.isAmpere() &&\n-             \"mmaLayout version = 1 is not implemented yet\");\n-      auto opIdx = dotLayout.getOpIdx();\n-      if (opIdx == 0) {\n+      if (parentMmaLayout.isAmpere()) {\n+        auto opIdx = dotLayout.getOpIdx();\n+        if (opIdx == 0) {\n+          return {2, 4};\n+        } else if (opIdx == 1) {\n+          return {4, 1};\n+        }\n+      } else if (parentMmaLayout.isHopper()) {\n+        auto opIdx = dotLayout.getOpIdx();\n+        assert(opIdx == 0 && \"hopper mma only support A from register\");\n         return {2, 4};\n-      } else if (opIdx == 1) {\n-        return {4, 1};\n       } else {\n-        assert(0 && \"DotOperandEncodingAttr opIdx must be 0 or 1\");\n-        return {};\n+        assert(0 && \"mmaLayout version = 1 is not implemented yet\");\n       }\n     } else {\n       assert(0 && \"DotOperandEncodingAttr non-MmaEncodingAttr parent not \"\n                   \"supported yet\");\n-      return {};\n     }\n   } else {\n     assert(0 && \"getSizePerThread not implemented\");\n-    return {};\n   }\n+  return {};\n+}\n+\n+SmallVector<unsigned> getContigPerThread(BlockedEncodingAttr layout) {\n+  return getSizePerThread(layout);\n }\n \n-SmallVector<unsigned> getContigPerThread(Attribute layout) {\n+SmallVector<unsigned> getContigPerThread(Value value) {\n+  auto layout = value.getType().cast<RankedTensorType>().getEncoding();\n   if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     assert(mmaLayout.isVolta() || mmaLayout.isAmpere() || mmaLayout.isHopper());\n     return {1, 2};\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parentLayout = sliceLayout.getParent();\n-    return getContigPerThread(parentLayout);\n+    auto parentValue = getParentValueWithSameEncoding(parentLayout, value);\n+    return getContigPerThread(parentValue);\n   } else {\n-    return getSizePerThread(layout, {});\n+    return getSizePerThread(value);\n   }\n }\n \n-SmallVector<unsigned> getUniqueContigPerThread(Attribute layout,\n-                                               ArrayRef<int64_t> shape) {\n+SmallVector<unsigned> getUniqueContigPerThread(Value value) {\n   // If slice layout, call recursively on parent layout, and drop\n   // sliced dim\n+  auto type = value.getType().cast<RankedTensorType>();\n+  auto shape = type.getShape();\n+  auto layout = type.getEncoding();\n   if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parentLayout = sliceLayout.getParent();\n-    auto parentShape = sliceLayout.paddedShape(shape);\n-    auto parentUniqueContigPerThread =\n-        getUniqueContigPerThread(parentLayout, parentShape);\n+    auto parentValue = getParentValueWithSameEncoding(parentLayout, value);\n+    auto parentUniqueContigPerThread = getUniqueContigPerThread(parentValue);\n     parentUniqueContigPerThread.erase(parentUniqueContigPerThread.begin() +\n                                       sliceLayout.getDim());\n     return parentUniqueContigPerThread;\n   }\n   // Base case\n   auto rank = shape.size();\n   SmallVector<unsigned> ret(rank);\n-  auto contigPerThread = getContigPerThread(layout);\n+  auto contigPerThread = getContigPerThread(value);\n   assert(contigPerThread.size() == rank && \"Unexpected contigPerThread size\");\n   for (int d = 0; d < rank; ++d) {\n     ret[d] = std::min<unsigned>(shape[d], contigPerThread[d]);\n@@ -301,7 +319,6 @@ static SmallVector<unsigned> getMMAShapePerCTATile_(MmaEncodingAttr mmaLayout,\n                                                     bool fromInput = false) {\n   SmallVector<unsigned> shape;\n   auto tensorShape = getShapePerCTA(value.getType());\n-  Type inputType;\n \n   if (mmaLayout.isAmpere())\n     return {16 * mmaLayout.getWarpsPerCTA()[0],\n@@ -315,8 +332,16 @@ static SmallVector<unsigned> getMMAShapePerCTATile_(MmaEncodingAttr mmaLayout,\n             static_cast<unsigned>(tensorShape[1])};\n   }\n   if (mmaLayout.isHopper()) {\n+    Type inputType;\n+    if (fromInput) {\n+      inputType = value.getType().cast<RankedTensorType>().getElementType();\n+      tensorShape = getShapePerCTA(\n+          (*value.use_begin()).getOwner()->getResult(0).getType());\n+    } else\n+      inputType = getInput0EltType(value);\n     auto instrShape = mmaVersionToInstrShape(mmaLayout.getVersionMajor(),\n                                              tensorShape, inputType);\n+\n     return {16 * mmaLayout.getWarpsPerCTA()[0],\n             instrShape[1] * mmaLayout.getWarpsPerCTA()[1]};\n   }\n@@ -687,17 +712,6 @@ static LogicalResult parseBool(AsmParser &parser, const NamedAttribute &attr,\n   return parseBoolAttrValue(parser, attr.getValue(), value, desc);\n };\n \n-static LogicalResult parseStrAttr(AsmParser &parser, const NamedAttribute &attr,\n-                                  StringRef &res, StringRef desc) {\n-  auto strAttr = attr.getValue().dyn_cast<StringAttr>();\n-  if (!strAttr) {\n-    parser.emitError(parser.getNameLoc(), \"expected an string for \") << desc;\n-    return failure();\n-  }\n-  res = strAttr.strref();\n-  return success();\n-};\n-\n //===----------------------------------------------------------------------===//\n // Attribute methods\n //===----------------------------------------------------------------------===//\n@@ -798,7 +812,7 @@ MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape, Type eltTy) const {\n   } else if (isHopper()) {\n     auto wpt = getWarpsPerCTA();\n     auto instrMNK =\n-        mmaVersionToInstrShape(getVersionMajor(), shape, getInputType());\n+        mmaVersionToInstrShape(getVersionMajor(), shapePerCTA, eltTy);\n     int repM = ceil<unsigned>(shapePerCTA[0], instrMNK[0] * wpt[0]);\n     int repN = ceil<unsigned>(shapePerCTA[1], instrMNK[1] * wpt[1]);\n     elemsPerThread[0] = 2 * repM;\n@@ -810,37 +824,6 @@ MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape, Type eltTy) const {\n   return elemsPerThread;\n }\n \n-unsigned\n-MmaEncodingAttr::getElemsPerThreadOfOperand(int opIdx,\n-                                            ArrayRef<int64_t> shape) const {\n-  size_t rank = shape.size();\n-  assert(rank == 2 && \"Unexpected rank of mma layout\");\n-  auto shapePerCTA = getShapePerCTA(*this, shape);\n-  int res = 0;\n-  if (isVolta()) {\n-    llvm_unreachable(\n-        \"getElemsPerThreadOfOperand() not supported for version 1\");\n-  } else if (isAmpere()) {\n-    llvm_unreachable(\n-        \"getElemsPerThreadOfOperand() not supported for version 2\");\n-  } else if (isHopper()) {\n-    auto wpt = getWarpsPerCTA();\n-    auto instrMNK =\n-        mmaVersionToInstrShape(getVersionMajor(), shapePerCTA, getInputType());\n-    if (opIdx == 0) {\n-      int repM = ceil<unsigned>(shapePerCTA[0], instrMNK[0] * wpt[0]);\n-      int repK = ceil<unsigned>(shapePerCTA[1], instrMNK[2]);\n-      return 8 * repM * repK;\n-    } else if (opIdx == 1) {\n-      int repK = ceil<unsigned>(shapePerCTA[0], instrMNK[2]);\n-      int repN = ceil<unsigned>(shapePerCTA[1], instrMNK[1] * wpt[1]);\n-      // benzh@ here need more check\n-      return 4 * std::max<int>(instrMNK[1] / 32, 1) * repK * repN;\n-    }\n-  }\n-  return res;\n-}\n-\n unsigned MmaEncodingAttr::getTotalElemsPerThread(ArrayRef<int64_t> shape,\n                                                  Type eltTy) const {\n   return product<unsigned>(getElemsPerThread(shape, eltTy));\n@@ -959,7 +942,7 @@ unsigned DotOperandEncodingAttr::getTotalElemsPerThread(ArrayRef<int64_t> shape,\n   if (auto blockedLayout = getParent().dyn_cast<BlockedEncodingAttr>()) {\n     auto shapePerCTATile = getShapePerCTATile(blockedLayout);\n     auto order = blockedLayout.getOrder();\n-    auto sizePerThread = getSizePerThread(blockedLayout, {});\n+    auto sizePerThread = getSizePerThread(blockedLayout);\n \n     int K = getOpIdx() == 0 ? shape[1] : shape[0];\n     int otherDim = getOpIdx() == 1 ? shape[1] : shape[0];\n@@ -1108,19 +1091,13 @@ Attribute MmaEncodingAttr::parse(AsmParser &parser, Type type) {\n       if (parseIntArrayAttr(parser, attr, CTAOrder, \"CTAOrder\").failed())\n         return {};\n     }\n-    if (attr.getName() == \"inputType\") {\n-      if (parseStrAttr(parser, attr, inputType, \"inputType\").failed()) {\n-        return {};\n-      }\n-    }\n   }\n \n   auto CTALayout = CTALayoutAttr::get(parser.getContext(), CTAsPerCGA,\n                                       CTASplitNum, CTAOrder);\n \n-  return parser.getChecked<MmaEncodingAttr>(parser.getContext(), versionMajor,\n-                                            versionMinor, warpsPerCTA,\n-                                            CTALayout, inputType);\n+  return parser.getChecked<MmaEncodingAttr>(\n+      parser.getContext(), versionMajor, versionMinor, warpsPerCTA, CTALayout);\n }\n \n void MmaEncodingAttr::print(AsmPrinter &printer) const {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -259,7 +259,7 @@ class BlockedToMMA : public mlir::RewritePattern {\n                                           numWarps, instrShape);\n       mmaEnc = ttg::MmaEncodingAttr::get(oldRetType.getContext(), versionMajor,\n                                          0 /*versionMinor*/, warpsPerTile,\n-                                         CTALayout, \"\");\n+                                         CTALayout);\n     }\n     auto newRetType = RankedTensorType::get(\n         oldRetType.getShape(), oldRetType.getElementType(), mmaEnc);"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/PlanCTA.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -301,8 +301,7 @@ bool CTAPlanner::processReduce(triton::FuncOp &funcOp) {\n \n     auto rank = srcShape.size();\n     auto order = ttg::getOrder(srcLayout);\n-    auto sizePerThread =\n-        ttg::getSizePerThread(srcLayout, ttg::getShapePerCTA(srcTy));\n+    auto sizePerThread = ttg::getSizePerThread(src);\n     auto CTAOrder = ttg::getCTAOrder(srcLayout);\n \n     llvm::SmallVector<unsigned> CTAsPerCGA(rank, 0);"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/EmitIndicesTest.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -121,7 +121,7 @@ class EmitIndicesTest : public ::testing::Test {\n                        llvm::ArrayRef<unsigned> instrShape,\n                        const std::string &refStr) {\n     auto layout = MmaEncodingAttr::get(&context, versionMajor, versionMinor,\n-                                       warpsPerCTA, getSingleCTALayout2d(), \"\");\n+                                       warpsPerCTA, getSingleCTALayout2d());\n     runDistributed2d(row, col, layout, /*multiCTA=*/false, refStr);\n   }\n \n@@ -131,7 +131,7 @@ class EmitIndicesTest : public ::testing::Test {\n                          llvm::ArrayRef<unsigned> instrShape, unsigned opIdx,\n                          const std::string &refStr) {\n     auto parent = MmaEncodingAttr::get(&context, versionMajor, versionMinor,\n-                                       warpsPerCTA, getSingleCTALayout2d(), \"\");\n+                                       warpsPerCTA, getSingleCTALayout2d());\n     auto layout = DotOperandEncodingAttr::get(&context, opIdx, parent, 0);\n     runDistributed2d(row, col, layout, /*multiCTA=*/false, refStr);\n   }\n@@ -636,7 +636,7 @@ TEST_F(EmitIndicesTest, LayoutVisualizer_Mma) {\n \n   Attribute mmaLayout = MmaEncodingAttr::get(\n       /*context=*/&context, /*versionMajor=*/2, /*versionMinor=*/1,\n-      /*warpsPerCTA=*/{1, 1}, /*CTALayout=*/CTALayout, \"\");\n+      /*warpsPerCTA=*/{1, 1}, /*CTALayout=*/CTALayout);\n \n   llvm::SmallVector<int64_t> shape = {/*row=*/16, /*col=*/8};\n "}, {"filename": "unittest/Dialect/TritonGPU/SwizzleTest.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -33,7 +33,7 @@ TEST_P(SwizzleDotOperandTestFixture, DotOperands) {\n \n   // create encoding\n   auto parent =\n-      triton::gpu::MmaEncodingAttr::get(&ctx, 2, 0, {1, 1}, CTALayout, \"\");\n+      triton::gpu::MmaEncodingAttr::get(&ctx, 2, 0, {1, 1}, CTALayout);\n   auto encoding = triton::gpu::DotOperandEncodingAttr::get(\n       &ctx, params.opIdx, parent, 32 / params.typeWidth);\n "}]