[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -1543,15 +1543,15 @@ def _kernel(in1_ptr, in2_ptr, output_ptr,\n         in2_offsets = K_offsets[:, None] * in2_stride + N_offsets[None, :]\n \n         # Load inputs.\n-        x = tl.load(in1_ptr + in_offsets, mask=in_offsets < M*K)\n-        w = tl.load(in2_ptr + in2_offsets, mask=in2_offsets < K*N)\n+        x = tl.load(in1_ptr + in_offsets, mask=in_offsets < M * K)\n+        w = tl.load(in2_ptr + in2_offsets, mask=in2_offsets < K * N)\n \n         # Without a dot product the memory doesn't get promoted to shared.\n         o = tl.dot(x, w, out_dtype=tl.float32)\n \n         # Store output\n         output_offsets = M_offsets[:, None] * out_stride + N_offsets[None, :]\n-        tl.store(output_ptr + output_offsets, o, mask=output_offsets < M*N)\n+        tl.store(output_ptr + output_offsets, o, mask=output_offsets < M * N)\n \n     pgm = _kernel[(1,)](in1, in2, out,\n                         in1.stride()[0],"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -14,7 +14,7 @@\n from triton.utils import MockTensor\n \n \n-def get_cuda_stream(idx = None):\n+def get_cuda_stream(idx=None):\n     if idx is None:\n         idx = get_current_device()\n     try:"}]