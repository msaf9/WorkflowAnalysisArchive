[{"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 81, "deletions": 116, "changes": 197, "file_content_changes": "@@ -34,12 +34,28 @@ unsigned getElementBitWidth(const Value &val) {\n   return typeForMem.getIntOrFloatBitWidth();\n }\n \n-typedef DenseMap<Value, std::function<Type(Type)>> LayoutMap;\n+static Value getMemAccessPtr(Operation *op) {\n+  if (auto ld = dyn_cast<triton::LoadOp>(op))\n+    return ld.getPtr();\n+  if (auto atomic = dyn_cast<triton::AtomicRMWOp>(op))\n+    return atomic.getPtr();\n+  if (auto atomic = dyn_cast<triton::AtomicCASOp>(op))\n+    return atomic.getPtr();\n+  if (auto insert = dyn_cast<triton::gpu::InsertSliceAsyncOp>(op))\n+    return insert.getSrc();\n+  if (auto store = dyn_cast<triton::StoreOp>(op))\n+    return store.getPtr();\n+  return nullptr;\n+}\n \n struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n-  Attribute getCoalescedEncoding(ModuleAxisInfoAnalysis &axisInfoAnalysis,\n-                                 Value ptr, Operation *op, int numWarps,\n-                                 int threadsPerWarp) {\n+  void\n+  setCoalescedEncoding(ModuleAxisInfoAnalysis &axisInfoAnalysis, Operation *op,\n+                       int numWarps, int threadsPerWarp,\n+                       llvm::MapVector<Operation *, Attribute> &layoutMap) {\n+    if (layoutMap.count(op))\n+      return;\n+    Value ptr = getMemAccessPtr(op);\n     auto refType = ptr.getType();\n     if (refType.isa<PointerType>())\n       refType = refType.cast<PointerType>().getPointeeType();\n@@ -88,7 +104,7 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n       order = argSort(queryAxisInfo(ptr).getContiguity());\n     }\n \n-    auto matchesOrder = [&refTensorType](const Value &val) {\n+    auto matchesShape = [&refTensorType](const Value &val) {\n       if (val.getType() == refTensorType) {\n         return true;\n       }\n@@ -104,17 +120,19 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     // among all dependent pointers who have the same order as\n     // `ptr`.\n     // We only do it for normal tensors of pointers, not tensor pointers.\n-    SetVector<Value> withSameOrder;\n-    withSameOrder.insert(ptr);\n+    llvm::SmallSetVector<Operation *, 32> memAccessesSameOrder;\n+    memAccessesSameOrder.insert(op);\n     if (refType.isa<RankedTensorType>() && ptr.getDefiningOp()) {\n-      for (Operation *op : mlir::multiRootGetSlice(ptr.getDefiningOp())) {\n-        for (Value val : op->getResults()) {\n-          if (!matchesOrder(val))\n-            continue;\n-          auto currOrder =\n-              argSort(axisInfoAnalysis.getAxisInfo(val)->getContiguity());\n-          if (order == currOrder)\n-            withSameOrder.insert(val);\n+      for (Operation *use : mlir::multiRootGetSlice(op)) {\n+        Value val = getMemAccessPtr(use);\n+        if (!val)\n+          continue;\n+        if (!matchesShape(val))\n+          continue;\n+        auto currOrder =\n+            argSort(axisInfoAnalysis.getAxisInfo(val)->getContiguity());\n+        if (order == currOrder) {\n+          memAccessesSameOrder.insert(use);\n         }\n       }\n     }\n@@ -133,10 +151,8 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n                                 .getPointeeType()\n                           : refTensorType.getElementType();\n \n-    // Thread tile size depends on memory alignment\n-    SmallVector<unsigned, 4> sizePerThread(refTensorType.getRank(), 1);\n-    unsigned perThread = 1;\n-    for (Value val : withSameOrder) {\n+    auto getNumElementPerThread = [&](Operation *op) {\n+      Value val = getMemAccessPtr(op);\n       auto valInfo = queryAxisInfo(val);\n       unsigned elemNumBits = getElementBitWidth(val);\n       unsigned elemNumBytes = std::max(elemNumBits / 8, 1u);\n@@ -146,73 +162,73 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n           std::min(valInfo.getContiguity(order[0]), shapePerCTA[order[0]]);\n       unsigned alignment = std::min(maxMultiple, maxContig);\n       unsigned currPerThread = std::min(alignment, 128 / elemNumBits);\n+      return currPerThread;\n+    };\n+    unsigned perThread = getNumElementPerThread(op);\n+    for (Operation *op : memAccessesSameOrder) {\n+      unsigned currPerThread = getNumElementPerThread(op);\n       perThread = std::max(perThread, currPerThread);\n     }\n \n-    perThread = std::min<int>(perThread, numElemsPerThread);\n+    auto getNewEncoding = [&](unsigned perThread) {\n+      // Thread tile size depends on memory alignment\n+      SmallVector<unsigned, 4> sizePerThread(refTensorType.getRank(), 1);\n+      sizePerThread[order[0]] = perThread;\n \n-    if (!dyn_cast<triton::LoadOp>(op)) {\n-      // For ops that can result in a global memory write, we should enforce\n-      // that each thread handles at most 128 bits, which is the widest\n-      // available vectorized store op; otherwise, the store will have \"gaps\"\n-      // in the memory write at the warp level, resulting in worse performance.\n-      // For loads, we can expect that the gaps won't matter due to the L1\n-      // cache.\n-      unsigned elemNumBits = getElementBitWidth(ptr);\n-      perThread = std::min<int>(perThread, 128 / elemNumBits);\n+      auto CTALayout = triton::gpu::getCTALayout(refTensorType.getEncoding());\n+      return triton::gpu::BlockedEncodingAttr::get(\n+          &getContext(), refTensorType.getShape(), sizePerThread, order,\n+          numWarps, threadsPerWarp, CTALayout);\n+    };\n+    // Set the encoding\n+    Attribute newLoadEncoding = getNewEncoding(perThread);\n+    for (Operation *op : memAccessesSameOrder) {\n+      Attribute newEncoding = newLoadEncoding;\n+      if (!isa<triton::LoadOp>(op)) {\n+        unsigned storeElPerThreads =\n+            std::min(perThread, getNumElementPerThread(op));\n+        newEncoding = getNewEncoding(storeElPerThreads);\n+      }\n+      layoutMap[op] = newEncoding;\n     }\n-    sizePerThread[order[0]] = perThread;\n-\n-    auto CTALayout = triton::gpu::getCTALayout(refTensorType.getEncoding());\n-    return triton::gpu::BlockedEncodingAttr::get(\n-        &getContext(), refTensorType.getShape(), sizePerThread, order, numWarps,\n-        threadsPerWarp, CTALayout);\n   }\n \n-  std::function<Type(Type)>\n-  getTypeConverter(ModuleAxisInfoAnalysis &axisInfoAnalysis, Value ptr,\n-                   Operation *op, int numWarps, int threadsPerWarp) {\n-    Attribute encoding = getCoalescedEncoding(axisInfoAnalysis, ptr, op,\n-                                              numWarps, threadsPerWarp);\n-    return [encoding](Type type) {\n-      RankedTensorType tensorType = type.cast<RankedTensorType>();\n-      return RankedTensorType::get(tensorType.getShape(),\n-                                   tensorType.getElementType(), encoding);\n-    };\n+  static Type getNewType(Type type, Attribute encoding) {\n+    RankedTensorType tensorType = type.cast<RankedTensorType>();\n+    return RankedTensorType::get(tensorType.getShape(),\n+                                 tensorType.getElementType(), encoding);\n   }\n \n-  template <class T>\n-  void coalesceOp(LayoutMap &layoutMap, Operation *op, Value ptr,\n-                  OpBuilder builder) {\n-    if (!layoutMap.count(ptr))\n-      return;\n-\n+  void coalesceOp(Attribute encoding, Operation *op) {\n+    OpBuilder builder(op);\n     // Convert operands\n     // For load/store with tensor pointers, we don't have to change the\n     // operands' type, we do this by changing the outputs' type of\n     // `make_tensor_ptr`\n-    auto convertType = layoutMap.lookup(ptr);\n     SmallVector<Value, 4> newArgs;\n     for (auto operand : op->getOperands()) {\n       auto tensorType = operand.getType().dyn_cast<RankedTensorType>();\n       if (tensorType &&\n-          !tensorType.getEncoding().isa<triton::gpu::SharedEncodingAttr>())\n+          !tensorType.getEncoding().isa<triton::gpu::SharedEncodingAttr>()) {\n+        Type newType = getNewType(tensorType, encoding);\n         newArgs.push_back(builder.create<triton::gpu::ConvertLayoutOp>(\n-            op->getLoc(), convertType(tensorType), operand));\n-      else\n+            op->getLoc(), newType, operand));\n+      } else {\n         newArgs.push_back(operand);\n+      }\n     }\n \n     // Convert output types\n     SmallVector<Type, 4> newTypes;\n     for (auto t : op->getResultTypes()) {\n-      bool isAsync = std::is_same<T, triton::gpu::InsertSliceAsyncOp>::value;\n-      newTypes.push_back(isAsync ? t : convertType(t));\n+      bool isAsync = isa<triton::gpu::InsertSliceAsyncOp>(op);\n+      newTypes.push_back(isAsync ? t : getNewType(t, encoding));\n     }\n \n     // Construct new op with the new encoding\n     Operation *newOp =\n-        builder.create<T>(op->getLoc(), newTypes, newArgs, op->getAttrs());\n+        builder.create(op->getLoc(), op->getName().getIdentifier(), newArgs,\n+                       newTypes, op->getAttrs());\n \n     // Cast the results back to the original layout\n     for (size_t i = 0; i < op->getNumResults(); i++) {\n@@ -226,45 +242,16 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     op->erase();\n   }\n \n-  void coalesceMakeTensorPtrOpResult(LayoutMap &layoutMap, Operation *op,\n-                                     Value ptr, OpBuilder builder) {\n-    if (!layoutMap.count(ptr))\n-      return;\n-\n-    // Convert result type\n-    auto convertType = layoutMap.lookup(ptr);\n-    auto ptrType = ptr.getType().cast<PointerType>();\n-    auto resultTensorType = convertType(ptrType.getPointeeType());\n-    auto newResultType =\n-        PointerType::get(resultTensorType, ptrType.getAddressSpace());\n-\n-    // Build new operation and replace\n-    Operation *newOp = builder.create<MakeTensorPtrOp>(\n-        op->getLoc(), newResultType, op->getOperands(), op->getAttrs());\n-    op->getResult(0).replaceAllUsesWith(newOp->getResult(0));\n-    op->erase();\n-  }\n-\n   void runOnOperation() override {\n     // Run axis info analysis\n     ModuleOp moduleOp = getOperation();\n     ModuleAxisInfoAnalysis axisInfoAnalysis(moduleOp);\n \n     // For each i/o operation, we determine what layout\n     // the pointers should have for best memory coalescing\n-    LayoutMap layoutMap;\n+    llvm::MapVector<Operation *, Attribute> layoutMap;\n     moduleOp.walk([&](Operation *curr) {\n-      Value ptr;\n-      if (auto op = dyn_cast<triton::LoadOp>(curr))\n-        ptr = op.getPtr();\n-      if (auto op = dyn_cast<triton::AtomicRMWOp>(curr))\n-        ptr = op.getPtr();\n-      if (auto op = dyn_cast<triton::AtomicCASOp>(curr))\n-        ptr = op.getPtr();\n-      if (auto op = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr))\n-        ptr = op.getSrc();\n-      if (auto op = dyn_cast<triton::StoreOp>(curr))\n-        ptr = op.getPtr();\n+      Value ptr = getMemAccessPtr(curr);\n       if (!ptr)\n         return;\n       // We only convert `tensor<tt.ptr<>>` or `tt.ptr<tensor<>>` load/store\n@@ -279,9 +266,8 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n       int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n       int threadsPerWarp =\n           triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n-      auto convertType = getTypeConverter(axisInfoAnalysis, ptr, curr, numWarps,\n-                                          threadsPerWarp);\n-      layoutMap[ptr] = convertType;\n+      setCoalescedEncoding(axisInfoAnalysis, curr, numWarps, threadsPerWarp,\n+                           layoutMap);\n     });\n \n     // For each memory op that has a layout L1:\n@@ -291,30 +277,9 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     //    produces a tensor with layout L2\n     // 4. Convert the output of this new memory op back to L1\n     // 5. Replace all the uses of the original memory op by the new one\n-    moduleOp.walk([&](Operation *curr) {\n-      OpBuilder builder(curr);\n-      if (auto load = dyn_cast<triton::LoadOp>(curr)) {\n-        coalesceOp<triton::LoadOp>(layoutMap, curr, load.getPtr(), builder);\n-        return;\n-      }\n-      if (auto op = dyn_cast<triton::AtomicRMWOp>(curr)) {\n-        coalesceOp<triton::AtomicRMWOp>(layoutMap, curr, op.getPtr(), builder);\n-        return;\n-      }\n-      if (auto op = dyn_cast<triton::AtomicCASOp>(curr)) {\n-        coalesceOp<triton::AtomicCASOp>(layoutMap, curr, op.getPtr(), builder);\n-        return;\n-      }\n-      if (auto load = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr)) {\n-        coalesceOp<triton::gpu::InsertSliceAsyncOp>(layoutMap, curr,\n-                                                    load.getSrc(), builder);\n-        return;\n-      }\n-      if (auto store = dyn_cast<triton::StoreOp>(curr)) {\n-        coalesceOp<triton::StoreOp>(layoutMap, curr, store.getPtr(), builder);\n-        return;\n-      }\n-    });\n+    for (auto &kv : layoutMap) {\n+      coalesceOp(kv.second, kv.first);\n+    }\n   }\n };\n "}]