[{"filename": "python/test/unit/operators/test_flash_attention.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -5,7 +5,9 @@\n import triton.ops\n \n \n-@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64),\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 16),\n+                                                 (4, 48, 1024, 32),\n+                                                 (4, 48, 1024, 64),\n                                                  (4, 48, 1024, 128)])\n @pytest.mark.parametrize('dtype', [torch.float16, torch.bfloat16])\n def test_op(Z, H, N_CTX, D_HEAD, dtype):"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -203,8 +203,7 @@ def forward(ctx, q, k, v, sm_scale):\n         # shape constraints\n         Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n         assert Lq == Lk and Lk == Lv\n-        # assert Lk in {16, 32, 64, 128}\n-        assert Lk in {64}  # TODO: fix other cases\n+        assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n         grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)"}]