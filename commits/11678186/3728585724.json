[{"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 57, "deletions": 12, "changes": 69, "file_content_changes": "@@ -932,27 +932,57 @@ class UpdateMMAVersionMinorForVolta : public mlir::RewritePattern {\n     auto newBTy = RankedTensorType::get(BT.getShape(), BT.getElementType(),\n                                         newDotOperandB);\n \n+    /*\n     // Here, we assume that A,B,C has only one consumer\n     auto newA =\n         rewriter.create<ConvertLayoutOp>(dotOp->getLoc(), newATy, dotOp.a())\n             .getResult();\n     auto newB =\n         rewriter.create<ConvertLayoutOp>(dotOp->getLoc(), newBTy, dotOp.b())\n             .getResult();\n+     */\n     Value newC;\n+    RankedTensorType newCTy;\n     if (dotOp.c()) {\n       auto CT = dotOp.c().getType().cast<RankedTensorType>();\n-      auto newCTy = RankedTensorType::get(CT.getShape(), CT.getElementType(),\n-                                          newMmaLayout);\n+      newCTy = RankedTensorType::get(CT.getShape(), CT.getElementType(),\n+                                     newMmaLayout);\n       newC =\n           rewriter.create<ConvertLayoutOp>(dotOp->getLoc(), newCTy, dotOp.c())\n               .getResult();\n     }\n+    if (!(dotOp.a().getDefiningOp() && dotOp.b().getDefiningOp()))\n+      return failure();\n+\n+    auto AOp = llvm::dyn_cast<ConvertLayoutOp>(dotOp.a().getDefiningOp());\n+    auto BOp = llvm::dyn_cast<ConvertLayoutOp>(dotOp.b().getDefiningOp());\n+    ConvertLayoutOp COp;\n+    if (dotOp.c())\n+      COp = llvm::dyn_cast<ConvertLayoutOp>(dotOp.c().getDefiningOp());\n+    if (!(AOp && BOp))\n+      return failure();\n+    if (dotOp.c() && !COp)\n+      return failure();\n+\n+    auto newAOp = rewriter.replaceOpWithNewOp<ConvertLayoutOp>(\n+        AOp, newATy, AOp.getOperand());\n+    auto newBOp = rewriter.replaceOpWithNewOp<ConvertLayoutOp>(\n+        BOp, newBTy, BOp.getOperand());\n+    ConvertLayoutOp newCOp;\n+    if (dotOp.c())\n+      newCOp = rewriter.replaceOpWithNewOp<ConvertLayoutOp>(COp, newCTy,\n+                                                            COp.getOperand());\n+\n+    auto newA = AOp.getResult();\n+    auto newB = BOp.getResult();\n+    if (dotOp.c())\n+      newC = COp.getResult();\n \n     auto newDotRetTy =\n         RankedTensorType::get(DT.getShape(), DT.getElementType(), newMmaLayout);\n-    rewriter.replaceOpWithNewOp<triton::DotOp>(dotOp, newDotRetTy, newA, newB,\n-                                               newC, dotOp.allowTF32());\n+    rewriter.replaceOpWithNewOp<triton::DotOp>(\n+        dotOp, newDotRetTy, newAOp.getResult(), newBOp.getResult(),\n+        newCOp.getResult(), dotOp.allowTF32());\n     return success();\n   }\n };\n@@ -1023,22 +1053,37 @@ class TritonGPUCombineOpsPass\n     patterns.add<DecomposeDotOperand>(context);\n     patterns.add<RematerializeBackward>(context);\n     patterns.add<RematerializeForward>(context);\n-    patterns.add<MoveConvertOutOfLoop>(context);\n     patterns.add<BlockedToMMA>(context, computeCapability);\n+    patterns.add<UpdateMMAVersionMinorForVolta>(context);\n+    patterns.add<MoveConvertOutOfLoop>(context);\n \n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n       signalPassFailure();\n     }\n \n-    mlir::RewritePatternSet updateMMAVersionMinorForVolta(context);\n-    updateMMAVersionMinorForVolta\n-        .add<UpdateMMAVersionMinorForVolta, SimplifyConversion>(context);\n+    /*\n+    {\n+      mlir::RewritePatternSet patternSet(context);\n+      //patternSet\n+          //.add<UpdateMMAVersionMinorForVolta, SimplifyConversion>(context);\n \n-    if (applyPatternsAndFoldGreedily(m,\n-                                     std::move(updateMMAVersionMinorForVolta))\n-            .failed()) {\n-      signalPassFailure();\n+      if (applyPatternsAndFoldGreedily(m,\n+                                       std::move(patternSet))\n+          .failed()) {\n+        signalPassFailure();\n+      }\n+    }\n+\n+    {\n+      mlir::RewritePatternSet patternSet(context);\n+      patternSet.add<MoveConvertOutOfLoop>(context);\n+      if (applyPatternsAndFoldGreedily(m,\n+                                       std::move(patternSet))\n+          .failed()) {\n+        signalPassFailure();\n+      }\n     }\n+     */\n \n     mlir::RewritePatternSet loopFixup(context);\n     loopFixup.add<FixupLoop>(context);"}]