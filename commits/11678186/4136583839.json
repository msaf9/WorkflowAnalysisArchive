[{"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 14, "deletions": 11, "changes": 25, "file_content_changes": "@@ -324,17 +324,20 @@ class DivOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n \n   int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n                           int dim) override {\n-    // lhs = k * d_lhs = k * k' * gcd(d_lhs, d_rhs)\n-    // rhs = p * d_rhs = p * p' * gcd(d_lhs, d_rhs)\n-    // lhs / rhs = k * k' * gcd(d_lhs, d_rhs) / (p * p' * gcd(d_lhs, d_rhs))\n-    //           = k / p * k' / p'\n-    // gcd(k', p') = divisibility(d_lhs / gcd(d_lhs, d_rhs), d_rhs / gcd(d_lhs,\n-    // d_rhs))\n-    auto lhsDivisibility = lhs.getDivisibility(dim);\n-    auto rhsDivisibility = rhs.getDivisibility(dim);\n-    auto initGcd = gcd(lhsDivisibility, rhsDivisibility);\n-    return std::max(lhsDivisibility / initGcd, rhsDivisibility / initGcd);\n-  };\n+    // Case 1: lhs is 0\n+    if (lhs.getConstantValue().has_value() &&\n+        lhs.getConstantValue().value() == 0)\n+      return lhs.getDivisibility(dim);\n+    // Case 2: rhs is constant\n+    if (rhs.getConstantValue().has_value()) {\n+      auto lhsDivisibility = lhs.getDivisibility(dim);\n+      auto rhsValue = rhs.getConstantValue().value();\n+      if (lhsDivisibility % rhsValue == 0)\n+        return lhsDivisibility / rhsValue;\n+    }\n+    // Case 3: both are not constant\n+    return 1;\n+  }\n \n   std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n                                           const AxisInfo &rhs) override {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -9,6 +9,7 @@ add_mlir_conversion_library(TritonGPUToLLVM\n     ReduceOpToLLVM.cpp\n     Utility.cpp\n     ViewOpToLLVM.cpp\n+    DotOpHelpers.cpp\n \n     ADDITIONAL_HEADER_DIRS\n     ${PROJECT_SOURCE_DIR}/include/triton/Conversion/TritonGPUToLLVM"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.cpp", "status": "added", "additions": 1425, "deletions": 0, "changes": 1425, "file_content_changes": "@@ -0,0 +1,1425 @@\n+#include \"DotOpHelpers.h\"\n+\n+namespace mlir {\n+namespace LLVM {\n+\n+int DotOpMmaV1ConversionHelper::numElemsPerThreadA(ArrayRef<int64_t> shape,\n+                                                   bool isARow, bool isAVec4,\n+                                                   int vec) const {\n+  int numM = getNumM(shape[0], isARow, isAVec4);\n+  int NK = shape[1];\n+  // Here we mimic the logic in loadA, the result cannot be calculated\n+  // directly.\n+  llvm::DenseSet<std::pair<int, int>> visited;\n+  auto ld = [&](int m, int k) {\n+    visited.insert({m, k});\n+    if (vec > 4) {\n+      if (isARow)\n+        visited.insert({m, k + 4});\n+      else\n+        visited.insert({m + 1, k});\n+    }\n+  };\n+\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      if (!visited.count({m, k}))\n+        ld(m, k);\n+\n+  return visited.size() * 2;\n+}\n+\n+int DotOpMmaV1ConversionHelper::numElemsPerThreadB(ArrayRef<int64_t> shape,\n+                                                   bool isBRow, bool isBVec4,\n+                                                   int vec) const {\n+  unsigned numN = getNumN(shape[1], isBRow, isBVec4);\n+  int NK = shape[0];\n+  // Here we mimic the logic in loadA, the result cannot be calculated\n+  // directly.\n+  llvm::DenseSet<std::pair<int, int>> visited;\n+  int elemsPerLd = vec > 4 ? 4 : 2;\n+  auto ld = [&](int n, int k) {\n+    visited.insert({n, k});\n+    if (vec > 4) {\n+      if (isBRow)\n+        visited.insert({n + 1, k});\n+      else\n+        visited.insert({n, k + 4});\n+    }\n+  };\n+\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned n = 0; n < numN / 2; ++n) {\n+      if (!visited.count({n, k}))\n+        ld(n, k);\n+    }\n+\n+  return visited.size() * 2;\n+}\n+\n+Value DotOpMmaV1ConversionHelper::loadA(\n+    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto shape = tensorTy.getShape();\n+  auto order = sharedLayout.getOrder();\n+\n+  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+  Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+\n+  bool isARow = order[0] != 0;\n+  auto [isARow_, _0, isAVec4, _1, _2] = mmaLayout.decodeVoltaLayoutStates();\n+\n+  AParam param(isARow_, isAVec4);\n+\n+  auto [offsetAM, offsetAK, _3, _4] = computeOffsets(\n+      thread, isARow, false, fpw, param.spw, param.rep, rewriter, loc);\n+\n+  int vecA = sharedLayout.getVec();\n+\n+  auto strides = smemObj.strides;\n+  Value strideAM = isARow ? strides[0] : i32_val(1);\n+  Value strideAK = isARow ? i32_val(1) : strides[1];\n+  Value strideA0 = isARow ? strideAK : strideAM;\n+  Value strideA1 = isARow ? strideAM : strideAK;\n+\n+  int strideRepM = wpt[0] * fpw[0] * 8;\n+  int strideRepK = 1;\n+\n+  // swizzling\n+  int perPhaseA = sharedLayout.getPerPhase();\n+  int maxPhaseA = sharedLayout.getMaxPhase();\n+  int stepA0 = isARow ? strideRepK : strideRepM;\n+  int numPtrA = std::max(2 * perPhaseA * maxPhaseA / stepA0, 1);\n+  int NK = shape[1];\n+\n+  // pre-compute pointer lanes\n+  Value offA0 = isARow ? offsetAK : offsetAM;\n+  Value offA1 = isARow ? offsetAM : offsetAK;\n+  Value phaseA = urem(udiv(offA1, i32_val(perPhaseA)), i32_val(maxPhaseA));\n+  offA0 = add(offA0, cSwizzleOffset);\n+  SmallVector<Value> offA(numPtrA);\n+  for (int i = 0; i < numPtrA; i++) {\n+    Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));\n+    offA0I = udiv(offA0I, i32_val(vecA));\n+    offA0I = xor_(offA0I, phaseA);\n+    offA0I = mul(offA0I, i32_val(vecA));\n+    offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n+  }\n+\n+  Type elemX2Ty = vec_ty(f16_ty, 2);\n+  Type elemPtrTy = ptr_ty(f16_ty);\n+  if (tensorTy.getElementType().isBF16()) {\n+    elemX2Ty = vec_ty(i16_ty, 2);\n+    elemPtrTy = ptr_ty(i16_ty);\n+  }\n+\n+  // prepare arguments\n+  SmallVector<Value> ptrA(numPtrA);\n+\n+  std::map<std::pair<int, int>, std::pair<Value, Value>> has;\n+  for (int i = 0; i < numPtrA; i++)\n+    ptrA[i] = gep(ptr_ty(f16_ty), smemBase, offA[i]);\n+\n+  auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+  auto loadA = [&](int m, int k) {\n+    int offidx = (isARow ? k / 4 : m) % numPtrA;\n+    Value thePtrA = gep(elemPtrTy, smemBase, offA[offidx]);\n+\n+    int stepAM = isARow ? m : m / numPtrA * numPtrA;\n+    int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n+    Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),\n+                       mul(i32_val(stepAK), strideAK));\n+    Value pa = gep(elemPtrTy, thePtrA, offset);\n+    Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n+    Value ha = load(bitcast(pa, aPtrTy));\n+    // record lds that needs to be moved\n+    Value ha00 = bitcast(extract_element(ha, i32_val(0)), elemX2Ty);\n+    Value ha01 = bitcast(extract_element(ha, i32_val(1)), elemX2Ty);\n+    ld(has, m, k, ha00, ha01);\n+\n+    if (vecA > 4) {\n+      Value ha10 = bitcast(extract_element(ha, i32_val(2)), elemX2Ty);\n+      Value ha11 = bitcast(extract_element(ha, i32_val(3)), elemX2Ty);\n+      if (isARow)\n+        ld(has, m, k + 4, ha10, ha11);\n+      else\n+        ld(has, m + 1, k, ha10, ha11);\n+    }\n+  };\n+\n+  unsigned numM = getNumM(shape[0], isARow, isAVec4);\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      if (!has.count({m, k}))\n+        loadA(m, k);\n+\n+  SmallVector<Value> elems;\n+  elems.reserve(has.size() * 2);\n+  for (auto item : has) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+\n+  Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n+  Value res = getStructFromElements(loc, elems, rewriter, resTy);\n+  return res;\n+}\n+\n+Value DotOpMmaV1ConversionHelper::loadB(\n+    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+  // smem\n+  auto strides = smemObj.strides;\n+\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+\n+  auto shape = tensorTy.getShape();\n+  auto order = sharedLayout.getOrder();\n+\n+  Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+  bool isBRow = order[0] != 0; // is row-major in shared memory layout\n+  // isBRow_ indicates whether B is row-major in DotOperand layout\n+  auto [_0, isBRow_, _1, isBVec4, _2] = mmaLayout.decodeVoltaLayoutStates();\n+  assert(isBRow == isBRow_ && \"B need smem isRow\");\n+\n+  BParam param(isBRow_, isBVec4);\n+\n+  int vecB = sharedLayout.getVec();\n+  Value strideBN = isBRow ? i32_val(1) : strides[1];\n+  Value strideBK = isBRow ? strides[0] : i32_val(1);\n+  Value strideB0 = isBRow ? strideBN : strideBK;\n+  Value strideB1 = isBRow ? strideBK : strideBN;\n+  int strideRepN = wpt[1] * fpw[1] * 8;\n+  int strideRepK = 1;\n+\n+  auto [_3, _4, offsetBN, offsetBK] = computeOffsets(\n+      thread, false, isBRow, fpw, param.spw, param.rep, rewriter, loc);\n+\n+  // swizzling\n+  int perPhaseB = sharedLayout.getPerPhase();\n+  int maxPhaseB = sharedLayout.getMaxPhase();\n+  int stepB0 = isBRow ? strideRepN : strideRepK;\n+  int numPtrB = std::max(2 * perPhaseB * maxPhaseB / stepB0, 1);\n+  int NK = shape[0];\n+\n+  Value offB0 = isBRow ? offsetBN : offsetBK;\n+  Value offB1 = isBRow ? offsetBK : offsetBN;\n+  Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n+  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+\n+  offB0 = add(offB0, cSwizzleOffset);\n+  SmallVector<Value> offB(numPtrB);\n+  for (int i = 0; i < numPtrB; ++i) {\n+    Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));\n+    offB0I = udiv(offB0I, i32_val(vecB));\n+    offB0I = xor_(offB0I, phaseB);\n+    offB0I = mul(offB0I, i32_val(vecB));\n+    offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));\n+  }\n+\n+  Type elemPtrTy = ptr_ty(f16_ty);\n+  Type elemX2Ty = vec_ty(f16_ty, 2);\n+  if (tensorTy.getElementType().isBF16()) {\n+    elemPtrTy = ptr_ty(i16_ty);\n+    elemX2Ty = vec_ty(i16_ty, 2);\n+  }\n+\n+  SmallVector<Value> ptrB(numPtrB);\n+  ValueTable hbs;\n+  for (int i = 0; i < numPtrB; ++i)\n+    ptrB[i] = gep(ptr_ty(f16_ty), smem, offB[i]);\n+\n+  auto ld = [&](decltype(hbs) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+\n+  auto loadB = [&](int n, int K) {\n+    int offidx = (isBRow ? n : K / 4) % numPtrB;\n+    Value thePtrB = ptrB[offidx];\n+\n+    int stepBN = isBRow ? n / numPtrB * numPtrB : n;\n+    int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n+    Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n+                       mul(i32_val(stepBK), strideBK));\n+    Value pb = gep(elemPtrTy, thePtrB, offset);\n+\n+    Value hb =\n+        load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n+    // record lds that needs to be moved\n+    Value hb00 = bitcast(extract_element(hb, i32_val(0)), elemX2Ty);\n+    Value hb01 = bitcast(extract_element(hb, i32_val(1)), elemX2Ty);\n+    ld(hbs, n, K, hb00, hb01);\n+    if (vecB > 4) {\n+      Value hb10 = bitcast(extract_element(hb, i32_val(2)), elemX2Ty);\n+      Value hb11 = bitcast(extract_element(hb, i32_val(3)), elemX2Ty);\n+      if (isBRow)\n+        ld(hbs, n + 1, K, hb10, hb11);\n+      else\n+        ld(hbs, n, K + 4, hb10, hb11);\n+    }\n+  };\n+\n+  unsigned numN = getNumN(shape[1], isBRow, isBVec4);\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned n = 0; n < numN / 2; ++n) {\n+      if (!hbs.count({n, k}))\n+        loadB(n, k);\n+    }\n+\n+  SmallVector<Value> elems;\n+  for (auto &item : hbs) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+\n+  Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n+  Value res = getStructFromElements(loc, elems, rewriter, resTy);\n+  return res;\n+}\n+\n+std::tuple<Value, Value, Value, Value>\n+DotOpMmaV1ConversionHelper::computeOffsets(Value threadId, bool isARow,\n+                                           bool isBRow, ArrayRef<int> fpw,\n+                                           ArrayRef<int> spw, ArrayRef<int> rep,\n+                                           ConversionPatternRewriter &rewriter,\n+                                           Location loc) const {\n+  auto *ctx = rewriter.getContext();\n+  Value _1 = i32_val(1);\n+  Value _3 = i32_val(3);\n+  Value _4 = i32_val(4);\n+  Value _16 = i32_val(16);\n+  Value _32 = i32_val(32);\n+\n+  Value lane = urem(threadId, _32);\n+  Value warp = udiv(threadId, _32);\n+\n+  // warp offset\n+  Value warp0 = urem(warp, i32_val(wpt[0]));\n+  Value warp12 = udiv(warp, i32_val(wpt[0]));\n+  Value warp1 = urem(warp12, i32_val(wpt[1]));\n+  Value warpMOff = mul(warp0, i32_val(spw[0]));\n+  Value warpNOff = mul(warp1, i32_val(spw[1]));\n+  // Quad offset\n+  Value quadMOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[0]));\n+  Value quadNOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[1]));\n+  // Pair offset\n+  Value pairMOff = udiv(urem(lane, _16), _4);\n+  pairMOff = urem(pairMOff, i32_val(fpw[0]));\n+  pairMOff = mul(pairMOff, _4);\n+  Value pairNOff = udiv(urem(lane, _16), _4);\n+  pairNOff = udiv(pairNOff, i32_val(fpw[0]));\n+  pairNOff = urem(pairNOff, i32_val(fpw[1]));\n+  pairNOff = mul(pairNOff, _4);\n+  // scale\n+  pairMOff = mul(pairMOff, i32_val(rep[0] / 2));\n+  quadMOff = mul(quadMOff, i32_val(rep[0] / 2));\n+  pairNOff = mul(pairNOff, i32_val(rep[1] / 2));\n+  quadNOff = mul(quadNOff, i32_val(rep[1] / 2));\n+  // Quad pair offset\n+  Value laneMOff = add(pairMOff, quadMOff);\n+  Value laneNOff = add(pairNOff, quadNOff);\n+  // A offset\n+  Value offsetAM = add(warpMOff, laneMOff);\n+  Value offsetAK = and_(lane, _3);\n+  // B offset\n+  Value offsetBN = add(warpNOff, laneNOff);\n+  Value offsetBK = and_(lane, _3);\n+  // i indices\n+  Value offsetCM = add(and_(lane, _1), offsetAM);\n+  if (isARow) {\n+    offsetAM = add(offsetAM, urem(threadId, _4));\n+    offsetAK = i32_val(0);\n+  }\n+  if (!isBRow) {\n+    offsetBN = add(offsetBN, urem(threadId, _4));\n+    offsetBK = i32_val(0);\n+  }\n+\n+  return std::make_tuple(offsetAM, offsetAK, offsetBN, offsetBK);\n+}\n+\n+DotOpMmaV1ConversionHelper::ValueTable\n+DotOpMmaV1ConversionHelper::extractLoadedOperand(\n+    Value llStruct, int NK, ConversionPatternRewriter &rewriter) const {\n+  ValueTable rcds;\n+  SmallVector<Value> elems =\n+      getElementsFromStruct(llStruct.getLoc(), llStruct, rewriter);\n+\n+  int offset = 0;\n+  for (int i = 0; offset < elems.size(); ++i) {\n+    for (int k = 0; k < NK; k += 4) {\n+      rcds[{i, k}] = std::make_pair(elems[offset], elems[offset + 1]);\n+      offset += 2;\n+    }\n+  }\n+\n+  return rcds;\n+}\n+\n+SmallVector<DotOpMmaV1ConversionHelper::CoordTy>\n+DotOpMmaV1ConversionHelper::getMNCoords(Value thread,\n+                                        ConversionPatternRewriter &rewriter,\n+                                        ArrayRef<unsigned int> wpt,\n+                                        ArrayRef<int64_t> shape, bool isARow,\n+                                        bool isBRow, bool isAVec4,\n+                                        bool isBVec4) {\n+\n+  auto *ctx = thread.getContext();\n+  auto loc = UnknownLoc::get(ctx);\n+  Value _1 = i32_val(1);\n+  Value _2 = i32_val(2);\n+  Value _4 = i32_val(4);\n+  Value _16 = i32_val(16);\n+  Value _32 = i32_val(32);\n+  Value _fpw0 = i32_val(fpw[0]);\n+  Value _fpw1 = i32_val(fpw[1]);\n+\n+  DotOpMmaV1ConversionHelper::AParam aParam(isARow, isAVec4);\n+  DotOpMmaV1ConversionHelper::BParam bParam(isBRow, isBVec4);\n+\n+  SmallVector<int, 2> rep({aParam.rep[0], bParam.rep[1]});\n+  SmallVector<int, 2> spw({aParam.spw[0], bParam.spw[1]});\n+  SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n+\n+  Value lane = urem(thread, _32);\n+  Value warp = udiv(thread, _32);\n+\n+  Value warp0 = urem(warp, i32_val(wpt[0]));\n+  Value warp12 = udiv(warp, i32_val(wpt[0]));\n+  Value warp1 = urem(warp12, i32_val(wpt[1]));\n+\n+  // warp offset\n+  Value offWarpM = mul(warp0, i32_val(spw[0]));\n+  Value offWarpN = mul(warp1, i32_val(spw[1]));\n+  // quad offset\n+  Value offQuadM = mul(udiv(and_(lane, _16), _4), _fpw0);\n+  Value offQuadN = mul(udiv(and_(lane, _16), _4), _fpw1);\n+  // pair offset\n+  Value offPairM = udiv(urem(lane, _16), _4);\n+  offPairM = urem(offPairM, _fpw0);\n+  offPairM = mul(offPairM, _4);\n+  Value offPairN = udiv(urem(lane, _16), _4);\n+  offPairN = udiv(offPairN, _fpw0);\n+  offPairN = urem(offPairN, _fpw1);\n+  offPairN = mul(offPairN, _4);\n+\n+  // sclare\n+  offPairM = mul(offPairM, i32_val(rep[0] / 2));\n+  offQuadM = mul(offQuadM, i32_val(rep[0] / 2));\n+  offPairN = mul(offPairN, i32_val(rep[1] / 2));\n+  offQuadN = mul(offQuadN, i32_val(rep[1] / 2));\n+\n+  // quad pair offset\n+  Value offLaneM = add(offPairM, offQuadM);\n+  Value offLaneN = add(offPairN, offQuadN);\n+  // a, b offset\n+  Value offsetAM = add(offWarpM, offLaneM);\n+  Value offsetBN = add(offWarpN, offLaneN);\n+  // m indices\n+  Value offsetCM = add(and_(lane, _1), offsetAM);\n+  SmallVector<Value> idxM;\n+  for (unsigned m = 0; m < shape[0]; m += shapePerCTA[0])\n+    for (unsigned mm = 0; mm < rep[0]; ++mm)\n+      idxM.push_back(add(offsetCM, i32_val(m + mm * 2)));\n+\n+  // n indices\n+  Value offsetCN = add((and_(lane, _2)), (add(offWarpN, offPairN)));\n+  SmallVector<Value> idxN;\n+  for (int n = 0; n < shape[1]; n += shapePerCTA[1]) {\n+    for (int nn = 0; nn < rep[1]; ++nn) {\n+      idxN.push_back(add(\n+          offsetCN, i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1])));\n+      idxN.push_back(\n+          add(offsetCN,\n+              i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1] + 1)));\n+    }\n+  }\n+\n+  SmallVector<SmallVector<Value>> axes({idxM, idxN});\n+\n+  // product the axis M and axis N to get coords, ported from\n+  // generator::init_idx method from triton2.0\n+\n+  // TODO[Superjomn]: check the order.\n+  SmallVector<CoordTy> coords;\n+  for (Value x1 : axes[1]) {   // N\n+    for (Value x0 : axes[0]) { // M\n+      SmallVector<Value, 2> idx(2);\n+      idx[0] = x0; // M\n+      idx[1] = x1; // N\n+      coords.push_back(std::move(idx));\n+    }\n+  }\n+\n+  return coords; // {M,N} in row-major\n+}\n+\n+void DotOpMmaV1ConversionHelper::AParam::build(bool isARow) {\n+  int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+  int repM = 2 * packSize0;\n+  int repK = 1;\n+  int spwM = fpw[0] * 4 * repM;\n+  rep.assign({repM, 0, repK});\n+  spw.assign({spwM, 0, 1});\n+  vec = 2 * rep[0];\n+}\n+\n+void DotOpMmaV1ConversionHelper::BParam::build(bool isBRow) {\n+  int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+  rep.assign({0, 2 * packSize1, 1});\n+  spw.assign({0, fpw[1] * 4 * rep[1], 1});\n+  vec = 2 * rep[1];\n+}\n+\n+std::tuple<int, int>\n+DotOpMmaV2ConversionHelper::getRepMN(const RankedTensorType &tensorTy) {\n+  auto mmaLayout = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n+  auto wpt = mmaLayout.getWarpsPerCTA();\n+\n+  int M = tensorTy.getShape()[0];\n+  int N = tensorTy.getShape()[1];\n+  auto [instrM, instrN] = getInstrShapeMN();\n+  int repM = std::max<int>(M / (wpt[0] * instrM), 1);\n+  int repN = std::max<int>(N / (wpt[1] * instrN), 1);\n+  return {repM, repN};\n+}\n+\n+Type DotOpMmaV2ConversionHelper::getShemPtrTy() const {\n+  switch (mmaType) {\n+  case TensorCoreType::FP32_FP16_FP16_FP32:\n+    return ptr_ty(type::f16Ty(ctx), 3);\n+  case TensorCoreType::FP32_BF16_BF16_FP32:\n+    return ptr_ty(type::i16Ty(ctx), 3);\n+  case TensorCoreType::FP32_TF32_TF32_FP32:\n+    return ptr_ty(type::f32Ty(ctx), 3);\n+  case TensorCoreType::INT32_INT8_INT8_INT32:\n+    return ptr_ty(type::i8Ty(ctx), 3);\n+  default:\n+    llvm::report_fatal_error(\"mma16816 data type not supported\");\n+  }\n+  return Type{};\n+}\n+\n+Type DotOpMmaV2ConversionHelper::getMatType() const {\n+  // floating point types\n+  Type fp32x1Ty = vec_ty(type::f32Ty(ctx), 1);\n+  Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n+  Type i16x2Ty = vec_ty(type::i16Ty(ctx), 2);\n+  Type fp16x2Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n+  // LLVM 14.0 does not support bf16 type, so we use i16 instead.\n+  Type bf16x2Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i16x2Ty));\n+  Type fp32Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32x1Ty));\n+  // integer types\n+  Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n+  Type i8x4Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i8x4Ty));\n+\n+  switch (mmaType) {\n+  case TensorCoreType::FP32_FP16_FP16_FP32:\n+    return fp16x2Pack4Ty;\n+  case TensorCoreType::FP32_BF16_BF16_FP32:\n+    return bf16x2Pack4Ty;\n+  case TensorCoreType::FP32_TF32_TF32_FP32:\n+    return fp32Pack4Ty;\n+  case TensorCoreType::INT32_INT8_INT8_INT32:\n+    return i8x4Pack4Ty;\n+  default:\n+    llvm::report_fatal_error(\"Unsupported mma type found\");\n+  }\n+\n+  return Type{};\n+}\n+\n+Type DotOpMmaV2ConversionHelper::getLoadElemTy() {\n+  switch (mmaType) {\n+  case TensorCoreType::FP32_FP16_FP16_FP32:\n+    return vec_ty(type::f16Ty(ctx), 2);\n+  case TensorCoreType::FP32_BF16_BF16_FP32:\n+    return vec_ty(type::bf16Ty(ctx), 2);\n+  case TensorCoreType::FP32_TF32_TF32_FP32:\n+    return type::f32Ty(ctx);\n+  case TensorCoreType::INT32_INT8_INT8_INT32:\n+    return type::i32Ty(ctx);\n+  default:\n+    llvm::report_fatal_error(\"Unsupported mma type found\");\n+  }\n+\n+  return Type{};\n+}\n+\n+Type DotOpMmaV2ConversionHelper::getMmaRetType() const {\n+  Type fp32Ty = type::f32Ty(ctx);\n+  Type i32Ty = type::i32Ty(ctx);\n+  Type fp32x4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n+  Type i32x4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i32Ty));\n+  switch (mmaType) {\n+  case TensorCoreType::FP32_FP16_FP16_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::FP32_BF16_BF16_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::FP32_TF32_TF32_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::INT32_INT8_INT8_INT32:\n+    return i32x4Ty;\n+  default:\n+    llvm::report_fatal_error(\"Unsupported mma type found\");\n+  }\n+\n+  return Type{};\n+}\n+\n+DotOpMmaV2ConversionHelper::TensorCoreType\n+DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(Type operandTy) {\n+  auto tensorTy = operandTy.cast<RankedTensorType>();\n+  auto elemTy = tensorTy.getElementType();\n+  if (elemTy.isF16())\n+    return TensorCoreType::FP32_FP16_FP16_FP32;\n+  if (elemTy.isF32())\n+    return TensorCoreType::FP32_TF32_TF32_FP32;\n+  if (elemTy.isBF16())\n+    return TensorCoreType::FP32_BF16_BF16_FP32;\n+  if (elemTy.isInteger(8))\n+    return TensorCoreType::INT32_INT8_INT8_INT32;\n+  return TensorCoreType::NOT_APPLICABLE;\n+}\n+\n+DotOpMmaV2ConversionHelper::TensorCoreType\n+DotOpMmaV2ConversionHelper::getMmaType(triton::DotOp op) {\n+  Value A = op.a();\n+  Value B = op.b();\n+  auto aTy = A.getType().cast<RankedTensorType>();\n+  auto bTy = B.getType().cast<RankedTensorType>();\n+  // d = a*b + c\n+  auto dTy = op.d().getType().cast<RankedTensorType>();\n+\n+  if (dTy.getElementType().isF32()) {\n+    if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n+      return TensorCoreType::FP32_FP16_FP16_FP32;\n+    if (aTy.getElementType().isBF16() && bTy.getElementType().isBF16())\n+      return TensorCoreType::FP32_BF16_BF16_FP32;\n+    if (aTy.getElementType().isF32() && bTy.getElementType().isF32() &&\n+        op.allowTF32())\n+      return TensorCoreType::FP32_TF32_TF32_FP32;\n+  } else if (dTy.getElementType().isInteger(32)) {\n+    if (aTy.getElementType().isInteger(8) && bTy.getElementType().isInteger(8))\n+      return TensorCoreType::INT32_INT8_INT8_INT32;\n+  }\n+\n+  return TensorCoreType::NOT_APPLICABLE;\n+}\n+\n+SmallVector<Value>\n+MMA16816SmemLoader::computeLdmatrixMatOffs(Value warpId, Value lane,\n+                                           Value cSwizzleOffset) {\n+  // 4x4 matrices\n+  Value c = urem(lane, i32_val(8));\n+  Value s = udiv(lane, i32_val(8)); // sub-warp-id\n+\n+  // Decompose s => s_0, s_1, that is the coordinate in 2x2 matrices in a\n+  // warp\n+  Value s0 = urem(s, i32_val(2));\n+  Value s1 = udiv(s, i32_val(2));\n+\n+  // We use different orders for a and b for better performance.\n+  Value kMatArr = kOrder == 1 ? s1 : s0;\n+  Value nkMatArr = kOrder == 1 ? s0 : s1;\n+\n+  // matrix coordinate inside a CTA, the matrix layout is [2x2wpt] for A and\n+  // [2wptx2] for B. e.g. Setting wpt=3, The data layout for A(kOrder=1) is\n+  //   |0 0 1 1 2 2| -> 0,1,2 are the warpids\n+  //   |0 0 1 1 2 2|\n+  //\n+  // for B(kOrder=0) is\n+  //   |0 0|  -> 0,1,2 are the warpids\n+  //   |1 1|\n+  //   |2 2|\n+  //   |0 0|\n+  //   |1 1|\n+  //   |2 2|\n+  // Note, for each warp, it handles a 2x2 matrices, that is the coordinate\n+  // address (s0,s1) annotates.\n+\n+  Value matOff[2];\n+  matOff[kOrder ^ 1] =\n+      add(mul(warpId, i32_val(warpOffStride)),   // warp offset\n+          mul(nkMatArr, i32_val(matArrStride))); // matrix offset inside a warp\n+  matOff[kOrder] = kMatArr;\n+\n+  // Physical offset (before swizzling)\n+  Value cMatOff = matOff[order[0]];\n+  Value sMatOff = matOff[order[1]];\n+  Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n+  cMatOff = add(cMatOff, cSwizzleMatOff);\n+\n+  // row offset inside a matrix, each matrix has 8 rows.\n+  Value sOffInMat = c;\n+\n+  SmallVector<Value> offs(numPtrs);\n+  Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n+  Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n+  for (int i = 0; i < numPtrs; ++i) {\n+    Value cMatOffI = add(cMatOff, i32_val(i * pLoadStrideInMat));\n+    cMatOffI = xor_(cMatOffI, phase);\n+    offs[i] = add(mul(cMatOffI, i32_val(cMatShape)), mul(sOff, sStride));\n+  }\n+\n+  return offs;\n+}\n+\n+SmallVector<Value> MMA16816SmemLoader::computeB32MatOffs(Value warpOff,\n+                                                         Value lane,\n+                                                         Value cSwizzleOffset) {\n+  assert(needTrans && \"Only used in transpose mode.\");\n+  // Load tf32 matrices with lds32\n+  Value cOffInMat = udiv(lane, i32_val(4));\n+  Value sOffInMat = urem(lane, i32_val(4));\n+\n+  Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n+  SmallVector<Value> offs(numPtrs);\n+\n+  for (int mat = 0; mat < 4; ++mat) { // Load 4 mats each time\n+    int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n+    int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n+    if (kMatArrInt > 0) // we don't need pointers for k\n+      continue;\n+    Value kMatArr = i32_val(kMatArrInt);\n+    Value nkMatArr = i32_val(nkMatArrInt);\n+\n+    Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n+                        mul(nkMatArr, i32_val(matArrStride)));\n+    Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n+    cMatOff = add(cMatOff, cSwizzleMatOff);\n+\n+    Value sMatOff = kMatArr;\n+    Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n+    // FIXME: (kOrder == 1?) is really dirty hack\n+    for (int i = 0; i < numPtrs / 2; ++i) {\n+      Value cMatOffI =\n+          add(cMatOff, i32_val(i * pLoadStrideInMat * (kOrder == 1 ? 1 : 2)));\n+      cMatOffI = xor_(cMatOffI, phase);\n+      Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n+      cOff = urem(cOff, i32_val(tileShape[order[0]]));\n+      sOff = urem(sOff, i32_val(tileShape[order[1]]));\n+      offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, sStride));\n+    }\n+  }\n+  return offs;\n+}\n+\n+SmallVector<Value> MMA16816SmemLoader::computeB8MatOffs(Value warpOff,\n+                                                        Value lane,\n+                                                        Value cSwizzleOffset) {\n+  assert(needTrans && \"Only used in transpose mode.\");\n+  Value cOffInMat = udiv(lane, i32_val(4));\n+  Value sOffInMat =\n+      mul(urem(lane, i32_val(4)), i32_val(4)); // each thread load 4 cols\n+\n+  SmallVector<Value> offs(numPtrs);\n+  for (int mat = 0; mat < 4; ++mat) {\n+    int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n+    int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n+    if (kMatArrInt > 0) // we don't need pointers for k\n+      continue;\n+    Value kMatArr = i32_val(kMatArrInt);\n+    Value nkMatArr = i32_val(nkMatArrInt);\n+\n+    Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n+                        mul(nkMatArr, i32_val(matArrStride)));\n+    Value sMatOff = kMatArr;\n+\n+    for (int loadx4Off = 0; loadx4Off < numPtrs / 8; ++loadx4Off) {\n+      for (int elemOff = 0; elemOff < 4; ++elemOff) {\n+        int ptrOff = loadx4Off * 8 + nkMatArrInt * 4 + elemOff;\n+        Value cMatOffI = add(cMatOff, i32_val(loadx4Off * pLoadStrideInMat *\n+                                              (kOrder == 1 ? 1 : 2)));\n+        Value sOffInMatElem = add(sOffInMat, i32_val(elemOff));\n+\n+        // disable swizzling ...\n+\n+        Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n+        Value sOff = add(sOffInMatElem, mul(sMatOff, i32_val(sMatShape)));\n+        // To prevent out-of-bound access when tile is too small.\n+        cOff = urem(cOff, i32_val(tileShape[order[0]]));\n+        sOff = urem(sOff, i32_val(tileShape[order[1]]));\n+        offs[ptrOff] = add(cOff, mul(sOff, sStride));\n+      }\n+    }\n+  }\n+  return offs;\n+}\n+\n+std::tuple<Value, Value, Value, Value>\n+MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> offs,\n+                           ArrayRef<Value> ptrs, Type matTy,\n+                           Type shemPtrTy) const {\n+  assert(mat0 % 2 == 0 && mat1 % 2 == 0 && \"smem matrix load must be aligned\");\n+  int matIdx[2] = {mat0, mat1};\n+\n+  int ptrIdx{-1};\n+\n+  if (canUseLdmatrix)\n+    ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n+  else if (elemBytes == 4 && needTrans)\n+    ptrIdx = matIdx[order[0]];\n+  else if (elemBytes == 1 && needTrans)\n+    ptrIdx = matIdx[order[0]] * 4;\n+  else\n+    llvm::report_fatal_error(\"unsupported mma type found\");\n+\n+  // The main difference with the original triton code is we removed the\n+  // prefetch-related logic here for the upstream optimizer phase should\n+  // take care with it, and that is transparent in dot conversion.\n+  auto getPtr = [&](int idx) { return ptrs[idx]; };\n+\n+  Value ptr = getPtr(ptrIdx);\n+\n+  // The struct should have exactly the same element types.\n+  auto resTy = matTy.cast<LLVM::LLVMStructType>();\n+  Type elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n+\n+  // For some reasons, LLVM's NVPTX backend inserts unnecessary (?) integer\n+  // instructions to pack & unpack sub-word integers. A workaround is to\n+  // store the results of ldmatrix in i32\n+  if (auto vecElemTy = elemTy.dyn_cast<VectorType>()) {\n+    Type elemElemTy = vecElemTy.getElementType();\n+    if (auto intTy = elemElemTy.dyn_cast<IntegerType>()) {\n+      if (intTy.getWidth() <= 16) {\n+        elemTy = rewriter.getI32Type();\n+        resTy =\n+            LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, elemTy));\n+      }\n+    }\n+  }\n+\n+  if (canUseLdmatrix) {\n+    Value sOffset =\n+        mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sStride);\n+    Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n+\n+    PTXBuilder builder;\n+    // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n+    // thread.\n+    auto resArgs = builder.newListOperand(4, \"=r\");\n+    auto addrArg = builder.newAddrOperand(sOffsetPtr, \"r\");\n+\n+    auto ldmatrix = builder.create(\"ldmatrix.sync.aligned.m8n8.x4\")\n+                        ->o(\"trans\", needTrans /*predicate*/)\n+                        .o(\"shared.b16\");\n+    ldmatrix(resArgs, addrArg);\n+\n+    // The result type is 4xi32, each i32 is composed of 2xf16\n+    // elements (adjacent two columns in a row) or a single f32 element.\n+    Value resV4 = builder.launch(rewriter, loc, resTy);\n+    return {extract_val(elemTy, resV4, i32_arr_attr(0)),\n+            extract_val(elemTy, resV4, i32_arr_attr(1)),\n+            extract_val(elemTy, resV4, i32_arr_attr(2)),\n+            extract_val(elemTy, resV4, i32_arr_attr(3))};\n+  } else if (elemBytes == 4 && needTrans) { // Use lds.32 to load tf32 matrices\n+    Value ptr2 = getPtr(ptrIdx + 1);\n+    assert(sMatStride == 1);\n+    int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n+    Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n+    int sOffsetArrElem = sMatStride * sMatShape;\n+    Value sOffsetArrElemVal =\n+        add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n+\n+    Value elems[4];\n+    if (kOrder == 1) {\n+      elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n+      elems[1] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n+      elems[2] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n+      elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n+    } else {\n+      elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n+      elems[2] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n+      elems[1] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n+      elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n+    }\n+    std::array<Value, 4> retElems;\n+    retElems.fill(undef(elemTy));\n+    for (auto i = 0; i < 4; ++i) {\n+      retElems[i] = insert_element(elemTy, retElems[i], elems[i], i32_val(0));\n+    }\n+    return {retElems[0], retElems[1], retElems[2], retElems[3]};\n+  } else if (elemBytes == 1 && needTrans) { // work with int8\n+    // Can't use i32 here. Use LLVM's VectorType\n+    elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n+    std::array<std::array<Value, 4>, 2> ptrs;\n+    ptrs[0] = {\n+        getPtr(ptrIdx),\n+        getPtr(ptrIdx + 1),\n+        getPtr(ptrIdx + 2),\n+        getPtr(ptrIdx + 3),\n+    };\n+\n+    ptrs[1] = {\n+        getPtr(ptrIdx + 4),\n+        getPtr(ptrIdx + 5),\n+        getPtr(ptrIdx + 6),\n+        getPtr(ptrIdx + 7),\n+    };\n+\n+    assert(sMatStride == 1);\n+    int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n+    Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n+    int sOffsetArrElem = 1 * (sMatStride * sMatShape);\n+    Value sOffsetArrElemVal =\n+        add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n+\n+    std::array<Value, 4> i8v4Elems;\n+    i8v4Elems.fill(undef(elemTy));\n+\n+    Value i8Elems[4][4];\n+    if (kOrder == 1) {\n+      for (int i = 0; i < 2; ++i)\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[i][j] = load(gep(shemPtrTy, ptrs[i][j], sOffsetElemVal));\n+\n+      for (int i = 2; i < 4; ++i)\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[i][j] =\n+              load(gep(shemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n+\n+      for (int m = 0; m < 4; ++m) {\n+        for (int e = 0; e < 4; ++e)\n+          i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n+                                        i8Elems[m][e], i32_val(e));\n+      }\n+    } else { // k first\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[0][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetElemVal));\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[2][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetElemVal));\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[1][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[3][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n+\n+      for (int m = 0; m < 4; ++m) {\n+        for (int e = 0; e < 4; ++e)\n+          i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n+                                        i8Elems[m][e], i32_val(e));\n+      }\n+    }\n+\n+    return {bitcast(i8v4Elems[0], i32_ty), bitcast(i8v4Elems[1], i32_ty),\n+            bitcast(i8v4Elems[2], i32_ty), bitcast(i8v4Elems[3], i32_ty)};\n+  }\n+\n+  assert(false && \"Invalid smem load\");\n+  return {Value{}, Value{}, Value{}, Value{}};\n+}\n+\n+MMA16816SmemLoader::MMA16816SmemLoader(\n+    int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n+    ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n+    ArrayRef<int> instrShape, ArrayRef<int> matShape, int perPhase,\n+    int maxPhase, int elemBytes, ConversionPatternRewriter &rewriter,\n+    TypeConverter *typeConverter, const Location &loc)\n+    : order(order.begin(), order.end()), kOrder(kOrder),\n+      tileShape(tileShape.begin(), tileShape.end()),\n+      instrShape(instrShape.begin(), instrShape.end()),\n+      matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n+      maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter), loc(loc),\n+      ctx(rewriter.getContext()) {\n+  cMatShape = matShape[order[0]];\n+  sMatShape = matShape[order[1]];\n+\n+  sStride = smemStrides[order[1]];\n+\n+  // rule: k must be the fast-changing axis.\n+  needTrans = kOrder != order[0];\n+  canUseLdmatrix = elemBytes == 2 || (!needTrans); // b16\n+\n+  if (canUseLdmatrix) {\n+    // Each CTA, the warps is arranged as [1xwpt] if not transposed,\n+    // otherwise [wptx1], and each warp will perform a mma.\n+    numPtrs =\n+        tileShape[order[0]] / (needTrans ? wpt : 1) / instrShape[order[0]];\n+  } else {\n+    numPtrs = tileShape[order[0]] / wpt / matShape[order[0]];\n+  }\n+  numPtrs = std::max<int>(numPtrs, 2);\n+\n+  // Special rule for i8/u8, 4 ptrs for each matrix\n+  if (!canUseLdmatrix && elemBytes == 1)\n+    numPtrs *= 4;\n+\n+  int loadStrideInMat[2];\n+  loadStrideInMat[kOrder] =\n+      2; // instrShape[kOrder] / matShape[kOrder], always 2\n+  loadStrideInMat[kOrder ^ 1] =\n+      wpt * (instrShape[kOrder ^ 1] / matShape[kOrder ^ 1]);\n+\n+  pLoadStrideInMat = loadStrideInMat[order[0]];\n+  sMatStride =\n+      loadStrideInMat[order[1]] / (instrShape[order[1]] / matShape[order[1]]);\n+\n+  // Each matArr contains warpOffStride matrices.\n+  matArrStride = kOrder == 1 ? 1 : wpt;\n+  warpOffStride = instrShape[kOrder ^ 1] / matShape[kOrder ^ 1];\n+}\n+Value MMA16816ConversionHelper::loadA(Value tensor,\n+                                      const SharedMemoryObject &smemObj) const {\n+  auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n+\n+  SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n+                             aTensorTy.getShape().end());\n+\n+  ValueTable ha;\n+  std::function<void(int, int)> loadFn;\n+  auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n+  auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(aTensorTy);\n+\n+  int numRepM = getNumRepM(aTensorTy, shape[0]);\n+  int numRepK = getNumRepK(aTensorTy, shape[1]);\n+\n+  if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n+    Value warpM = getWarpM(shape[0]);\n+    // load from smem\n+    // we use ldmatrix.x4 so each warp processes 16x16 elements.\n+    int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / 16);\n+    loadFn =\n+        getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/,\n+                        {mmaInstrM, mmaInstrK} /*instrShape*/,\n+                        {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/,\n+                        ha /*vals*/, true /*isA*/);\n+  } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n+    // load from registers, used in gemm fuse\n+    // TODO(Superjomn) Port the logic.\n+    assert(false && \"Loading A from register is not supported yet.\");\n+  } else {\n+    assert(false && \"A's layout is not supported.\");\n+  }\n+\n+  // step1. Perform loading.\n+  for (int m = 0; m < numRepM; ++m)\n+    for (int k = 0; k < numRepK; ++k)\n+      loadFn(2 * m, 2 * k);\n+\n+  // step2. Format the values to LLVM::Struct to passing to mma codegen.\n+  return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n+}\n+Value MMA16816ConversionHelper::loadB(Value tensor,\n+                                      const SharedMemoryObject &smemObj) {\n+  ValueTable hb;\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+\n+  SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n+                             tensorTy.getShape().end());\n+\n+  // TODO[Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n+  bool transB = false;\n+  if (transB) {\n+    std::swap(shape[0], shape[1]);\n+  }\n+\n+  auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(tensorTy);\n+  auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(tensorTy);\n+  int numRepK = getNumRepK(tensorTy, shape[0]);\n+  int numRepN = getNumRepN(tensorTy, shape[1]);\n+\n+  Value warpN = getWarpN(shape[1]);\n+  // we use ldmatrix.x4 so each warp processes 16x16 elements.\n+  int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / 16);\n+  auto loadFn =\n+      getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/,\n+                      {mmaInstrK, mmaInstrN} /*instrShape*/,\n+                      {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/,\n+                      hb /*vals*/, false /*isA*/);\n+\n+  for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n+    for (int k = 0; k < numRepK; ++k)\n+      loadFn(2 * n, 2 * k);\n+  }\n+\n+  Value result = composeValuesToDotOperandLayoutStruct(\n+      hb, std::max(numRepN / 2, 1), numRepK);\n+  return result;\n+}\n+Value MMA16816ConversionHelper::loadC(Value tensor, Value llTensor) const {\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(tensorTy);\n+  size_t fcSize = 4 * repM * repN;\n+\n+  assert(tensorTy.getEncoding().isa<MmaEncodingAttr>() &&\n+         \"Currently, we only support $c with a mma layout.\");\n+  // Load a normal C tensor with mma layout, that should be a\n+  // LLVM::struct with fcSize elements.\n+  auto structTy = llTensor.getType().cast<LLVM::LLVMStructType>();\n+  assert(structTy.getBody().size() == fcSize &&\n+         \"DotOp's $c operand should pass the same number of values as $d in \"\n+         \"mma layout.\");\n+  return llTensor;\n+}\n+LogicalResult MMA16816ConversionHelper::convertDot(Value a, Value b, Value c,\n+                                                   Value d, Value loadedA,\n+                                                   Value loadedB, Value loadedC,\n+                                                   DotOp op,\n+                                                   DotOpAdaptor adaptor) const {\n+  helper.deduceMmaType(op);\n+\n+  auto aTensorTy = a.getType().cast<RankedTensorType>();\n+  auto dTensorTy = d.getType().cast<RankedTensorType>();\n+\n+  SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n+                              aTensorTy.getShape().end());\n+\n+  auto dShape = dTensorTy.getShape();\n+\n+  // shape / shape_per_cta\n+  int numRepM = getNumRepM(aTensorTy, dShape[0]);\n+  int numRepN = getNumRepN(aTensorTy, dShape[1]);\n+  int numRepK = getNumRepK(aTensorTy, aShape[1]);\n+\n+  ValueTable ha =\n+      getValuesFromDotOperandLayoutStruct(loadedA, numRepM, numRepK);\n+  ValueTable hb = getValuesFromDotOperandLayoutStruct(\n+      loadedB, std::max(numRepN / 2, 1), numRepK);\n+  auto fc = getElementsFromStruct(loc, loadedC, rewriter);\n+\n+  auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n+    unsigned colsPerThread = numRepN * 2;\n+    PTXBuilder builder;\n+    auto &mma = *builder.create(helper.getMmaInstr().str());\n+    // using =r for float32 works but leads to less readable ptx.\n+    bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n+    auto retArgs = builder.newListOperand(4, isIntMMA ? \"=r\" : \"=f\");\n+    auto aArgs = builder.newListOperand({\n+        {ha[{m, k}], \"r\"},\n+        {ha[{m + 1, k}], \"r\"},\n+        {ha[{m, k + 1}], \"r\"},\n+        {ha[{m + 1, k + 1}], \"r\"},\n+    });\n+    auto bArgs =\n+        builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n+    auto cArgs = builder.newListOperand();\n+    for (int i = 0; i < 4; ++i) {\n+      cArgs->listAppend(builder.newOperand(fc[m * colsPerThread + 4 * n + i],\n+                                           std::to_string(i)));\n+      // reuse the output registers\n+    }\n+\n+    mma(retArgs, aArgs, bArgs, cArgs);\n+    Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n+\n+    Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n+    for (int i = 0; i < 4; ++i)\n+      fc[m * colsPerThread + 4 * n + i] =\n+          extract_val(elemTy, mmaOut, i32_arr_attr(i));\n+  };\n+\n+  for (int k = 0; k < numRepK; ++k)\n+    for (int m = 0; m < numRepM; ++m)\n+      for (int n = 0; n < numRepN; ++n)\n+        callMma(2 * m, n, 2 * k);\n+\n+  Type resElemTy = dTensorTy.getElementType();\n+\n+  for (auto &elem : fc) {\n+    elem = bitcast(elem, resElemTy);\n+  }\n+\n+  // replace with new packed result\n+  Type structTy = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(fc.size(), resElemTy));\n+  Value res = getStructFromElements(loc, fc, rewriter, structTy);\n+  rewriter.replaceOp(op, res);\n+\n+  return success();\n+}\n+std::function<void(int, int)> MMA16816ConversionHelper::getLoadMatrixFn(\n+    Value tensor, const SharedMemoryObject &smemObj, MmaEncodingAttr mmaLayout,\n+    int wpt, uint32_t kOrder, SmallVector<int> instrShape,\n+    SmallVector<int> matShape, Value warpId,\n+    MMA16816ConversionHelper::ValueTable &vals, bool isA) const {\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  // We assumes that the input operand of Dot should be from shared layout.\n+  // TODO(Superjomn) Consider other layouts if needed later.\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  const int perPhase = sharedLayout.getPerPhase();\n+  const int maxPhase = sharedLayout.getMaxPhase();\n+  const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n+  auto order = sharedLayout.getOrder();\n+\n+  // the original register_lds2, but discard the prefetch logic.\n+  auto ld2 = [](ValueTable &vals, int mn, int k, Value val) {\n+    vals[{mn, k}] = val;\n+  };\n+\n+  // (a, b) is the coordinate.\n+  auto load = [=, &vals, &ld2](int a, int b) {\n+    MMA16816SmemLoader loader(\n+        wpt, sharedLayout.getOrder(), kOrder, smemObj.strides,\n+        tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n+        maxPhase, elemBytes, rewriter, typeConverter, loc);\n+    Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+    SmallVector<Value> offs =\n+        loader.computeOffsets(warpId, lane, cSwizzleOffset);\n+    const int numPtrs = loader.getNumPtrs();\n+    SmallVector<Value> ptrs(numPtrs);\n+\n+    Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+\n+    Type smemPtrTy = helper.getShemPtrTy();\n+    for (int i = 0; i < numPtrs; ++i) {\n+      ptrs[i] =\n+          bitcast(gep(smemPtrTy, smemBase, ValueRange({offs[i]})), smemPtrTy);\n+    }\n+\n+    auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n+        (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n+        ptrs, helper.getMatType(), helper.getShemPtrTy());\n+\n+    if (isA) {\n+      ld2(vals, a, b, ha0);\n+      ld2(vals, a + 1, b, ha1);\n+      ld2(vals, a, b + 1, ha2);\n+      ld2(vals, a + 1, b + 1, ha3);\n+    } else {\n+      ld2(vals, a, b, ha0);\n+      ld2(vals, a + 1, b, ha2);\n+      ld2(vals, a, b + 1, ha1);\n+      ld2(vals, a + 1, b + 1, ha3);\n+    }\n+  };\n+\n+  return load;\n+}\n+Value MMA16816ConversionHelper::composeValuesToDotOperandLayoutStruct(\n+    const MMA16816ConversionHelper::ValueTable &vals, int n0, int n1) const {\n+  std::vector<Value> elems;\n+  for (int m = 0; m < n0; ++m)\n+    for (int k = 0; k < n1; ++k) {\n+      elems.push_back(vals.at({2 * m, 2 * k}));\n+      elems.push_back(vals.at({2 * m, 2 * k + 1}));\n+      elems.push_back(vals.at({2 * m + 1, 2 * k}));\n+      elems.push_back(vals.at({2 * m + 1, 2 * k + 1}));\n+    }\n+\n+  assert(!elems.empty());\n+\n+  Type elemTy = elems[0].getType();\n+  Type structTy = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(elems.size(), elemTy));\n+  auto result = getStructFromElements(loc, elems, rewriter, structTy);\n+  return result;\n+}\n+MMA16816ConversionHelper::ValueTable\n+MMA16816ConversionHelper::getValuesFromDotOperandLayoutStruct(Value value,\n+                                                              int n0,\n+                                                              int n1) const {\n+  auto elems = getElementsFromStruct(loc, value, rewriter);\n+\n+  int offset{};\n+  ValueTable vals;\n+  for (int i = 0; i < n0; ++i) {\n+    for (int j = 0; j < n1; j++) {\n+      vals[{2 * i, 2 * j}] = elems[offset++];\n+      vals[{2 * i, 2 * j + 1}] = elems[offset++];\n+      vals[{2 * i + 1, 2 * j}] = elems[offset++];\n+      vals[{2 * i + 1, 2 * j + 1}] = elems[offset++];\n+    }\n+  }\n+  return vals;\n+}\n+SmallVector<Value> DotOpFMAConversionHelper::getThreadIds(\n+    Value threadId, ArrayRef<unsigned int> shapePerCTA,\n+    ArrayRef<unsigned int> sizePerThread, ArrayRef<unsigned int> order,\n+    ConversionPatternRewriter &rewriter, Location loc) const {\n+  int dim = order.size();\n+  SmallVector<Value> threadIds(dim);\n+  for (unsigned k = 0; k < dim - 1; k++) {\n+    Value dimK = i32_val(shapePerCTA[order[k]] / sizePerThread[order[k]]);\n+    Value rem = urem(threadId, dimK);\n+    threadId = udiv(threadId, dimK);\n+    threadIds[order[k]] = rem;\n+  }\n+  Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n+  threadIds[order[dim - 1]] = urem(threadId, dimK);\n+  return threadIds;\n+}\n+Value DotOpFMAConversionHelper::loadA(\n+    Value A, Value llA, BlockedEncodingAttr dLayout, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto aShape = aTensorTy.getShape();\n+\n+  auto aOrder = aLayout.getOrder();\n+  auto order = dLayout.getOrder();\n+\n+  bool isARow = aOrder[0] == 1;\n+\n+  auto aSmem = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n+  Value strideAM = aSmem.strides[0];\n+  Value strideAK = aSmem.strides[1];\n+  Value strideA0 = isARow ? strideAK : strideAM;\n+  Value strideA1 = isARow ? strideAM : strideAK;\n+  int aNumPtr = 8;\n+  int K = aShape[1];\n+  int M = aShape[0];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value mContig = i32_val(sizePerThread[order[1]]);\n+\n+  // threadId in blocked layout\n+  auto threadIds =\n+      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n+  Value threadIdM = threadIds[0];\n+\n+  Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n+  Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n+  SmallVector<Value> aOff(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i) {\n+    aOff[i] = add(mul(offA0, strideA0), mul(offA1, strideA1));\n+  }\n+  auto elemTy = A.getType().cast<RankedTensorType>().getElementType();\n+\n+  Type ptrTy = ptr_ty(elemTy);\n+  SmallVector<Value> aPtrs(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i)\n+    aPtrs[i] = gep(ptrTy, aSmem.base, aOff[i]);\n+\n+  SmallVector<Value> vas;\n+\n+  int mShapePerCTA = getShapePerCTAForMN(dLayout, true /*isM*/);\n+  int mSizePerThread = getSizePerThreadForMN(dLayout, true /*isM*/);\n+\n+  for (unsigned k = 0; k < K; ++k)\n+    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+      for (unsigned mm = 0; mm < mSizePerThread; ++mm) {\n+        Value offset =\n+            add(mul(i32_val(m + mm), strideAM), mul(i32_val(k), strideAK));\n+        Value pa = gep(ptrTy, aPtrs[0], offset);\n+        Value va = load(pa);\n+        vas.emplace_back(va);\n+      }\n+\n+  return getStructFromValueTable(vas, rewriter, loc, elemTy);\n+}\n+Value DotOpFMAConversionHelper::loadB(\n+    Value B, Value llB, BlockedEncodingAttr dLayout, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto bShape = bTensorTy.getShape();\n+\n+  auto bOrder = bLayout.getOrder();\n+  auto order = dLayout.getOrder();\n+\n+  bool isBRow = bOrder[0] == 1;\n+\n+  auto bSmem = getSharedMemoryObjectFromStruct(loc, llB, rewriter);\n+  Value strideBN = bSmem.strides[1];\n+  Value strideBK = bSmem.strides[0];\n+  Value strideB0 = isBRow ? strideBN : strideBK;\n+  Value strideB1 = isBRow ? strideBK : strideBN;\n+  int bNumPtr = 8;\n+  int K = bShape[0];\n+  int N = bShape[1];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value nContig = i32_val(sizePerThread[order[0]]);\n+\n+  // threadId in blocked layout\n+  auto threadIds =\n+      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n+  Value threadIdN = threadIds[1];\n+\n+  Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n+  Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n+  SmallVector<Value> bOff(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i) {\n+    bOff[i] = add(mul(offB0, strideB0), mul(offB1, strideB1));\n+  }\n+  auto elemTy = B.getType().cast<RankedTensorType>().getElementType();\n+\n+  Type ptrTy = ptr_ty(elemTy);\n+  SmallVector<Value> bPtrs(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i)\n+    bPtrs[i] = gep(ptrTy, bSmem.base, bOff[i]);\n+\n+  SmallVector<Value> vbs;\n+\n+  int nShapePerCTA = getShapePerCTAForMN(dLayout, false /*isM*/);\n+  int nSizePerThread = getSizePerThreadForMN(dLayout, false /*isM*/);\n+\n+  for (unsigned k = 0; k < K; ++k)\n+    for (unsigned n = 0; n < N; n += nShapePerCTA)\n+      for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n+        Value offset =\n+            add(mul(i32_val(n + nn), strideBN), mul(i32_val(k), strideBK));\n+        Value pb = gep(ptrTy, bPtrs[0], offset);\n+        Value vb = load(pb);\n+        vbs.emplace_back(vb);\n+      }\n+\n+  return getStructFromValueTable(vbs, rewriter, loc, elemTy);\n+}\n+DotOpFMAConversionHelper::ValueTable\n+DotOpFMAConversionHelper::getValueTableFromStruct(\n+    Value val, int K, int n0, int shapePerCTA, int sizePerThread,\n+    ConversionPatternRewriter &rewriter, Location loc) const {\n+  ValueTable res;\n+  auto elems = getElementsFromStruct(loc, val, rewriter);\n+  int index = 0;\n+  for (unsigned k = 0; k < K; ++k) {\n+    for (unsigned m = 0; m < n0; m += shapePerCTA)\n+      for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n+        res[{m + mm, k}] = elems[index++];\n+      }\n+  }\n+  return res;\n+}\n+Value DotOpFMAConversionHelper::getStructFromValueTable(\n+    ArrayRef<Value> vals, ConversionPatternRewriter &rewriter, Location loc,\n+    Type elemTy) const {\n+  SmallVector<Type> elemTypes(vals.size(), elemTy);\n+  SmallVector<Value> elems;\n+  elems.reserve(vals.size());\n+  for (auto &val : vals) {\n+    elems.push_back(val);\n+  }\n+\n+  Type structTy = struct_ty(elemTypes);\n+  return getStructFromElements(loc, elems, rewriter, structTy);\n+}\n+int DotOpFMAConversionHelper::getNumElemsPerThread(\n+    ArrayRef<int64_t> shape, DotOperandEncodingAttr dotOpLayout) {\n+  auto blockedLayout = dotOpLayout.getParent().cast<BlockedEncodingAttr>();\n+  auto shapePerCTA = getShapePerCTA(blockedLayout);\n+  auto sizePerThread = getSizePerThread(blockedLayout);\n+\n+  // TODO[Superjomn]: we assume the k aixs is fixed for $a and $b here, fix it\n+  // if not.\n+  int K = dotOpLayout.getOpIdx() == 0 ? shape[1] : shape[0];\n+  int otherDim = dotOpLayout.getOpIdx() == 1 ? shape[1] : shape[0];\n+\n+  bool isM = dotOpLayout.getOpIdx() == 0;\n+  int shapePerCTAMN = getShapePerCTAForMN(blockedLayout, isM);\n+  int sizePerThreadMN = getSizePerThreadForMN(blockedLayout, isM);\n+  return K * std::max<int>(otherDim / shapePerCTAMN, 1) * sizePerThreadMN;\n+}\n+} // namespace LLVM\n+} // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 34, "deletions": 1336, "changes": 1370, "file_content_changes": "@@ -59,15 +59,7 @@ struct DotOpMmaV1ConversionHelper {\n     AParam(bool isARow, bool isAVec4) : isAVec4(isAVec4) { build(isARow); }\n \n   private:\n-    void build(bool isARow) {\n-      int packSize0 = (isARow || isAVec4) ? 1 : 2;\n-      int repM = 2 * packSize0;\n-      int repK = 1;\n-      int spwM = fpw[0] * 4 * repM;\n-      rep.assign({repM, 0, repK});\n-      spw.assign({spwM, 0, 1});\n-      vec = 2 * rep[0];\n-    }\n+    void build(bool isARow);\n   };\n \n   // Help to share some variables across multiple functions for A.\n@@ -83,12 +75,7 @@ struct DotOpMmaV1ConversionHelper {\n     BParam(bool isBRow, bool isBVec4) : isBVec4(isBVec4) { build(isBRow); }\n \n   private:\n-    void build(bool isBRow) {\n-      int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n-      rep.assign({0, 2 * packSize1, 1});\n-      spw.assign({0, fpw[1] * 4 * rep[1], 1});\n-      vec = 2 * rep[1];\n-    }\n+    void build(bool isBRow);\n   };\n \n   int getRepM(int M) const {\n@@ -131,283 +118,18 @@ struct DotOpMmaV1ConversionHelper {\n   }\n \n   int numElemsPerThreadA(ArrayRef<int64_t> shape, bool isARow, bool isAVec4,\n-                         int vec) const {\n-    int numM = getNumM(shape[0], isARow, isAVec4);\n-    int NK = shape[1];\n-    // Here we mimic the logic in loadA, the result cannot be calculated\n-    // directly.\n-    llvm::DenseSet<std::pair<int, int>> visited;\n-    auto ld = [&](int m, int k) {\n-      visited.insert({m, k});\n-      if (vec > 4) {\n-        if (isARow)\n-          visited.insert({m, k + 4});\n-        else\n-          visited.insert({m + 1, k});\n-      }\n-    };\n-\n-    for (unsigned k = 0; k < NK; k += 4)\n-      for (unsigned m = 0; m < numM / 2; ++m)\n-        if (!visited.count({m, k}))\n-          ld(m, k);\n-\n-    return visited.size() * 2;\n-  }\n+                         int vec) const;\n \n   int numElemsPerThreadB(ArrayRef<int64_t> shape, bool isBRow, bool isBVec4,\n-                         int vec) const {\n-    unsigned numN = getNumN(shape[1], isBRow, isBVec4);\n-    int NK = shape[0];\n-    // Here we mimic the logic in loadA, the result cannot be calculated\n-    // directly.\n-    llvm::DenseSet<std::pair<int, int>> visited;\n-    int elemsPerLd = vec > 4 ? 4 : 2;\n-    auto ld = [&](int n, int k) {\n-      visited.insert({n, k});\n-      if (vec > 4) {\n-        if (isBRow)\n-          visited.insert({n + 1, k});\n-        else\n-          visited.insert({n, k + 4});\n-      }\n-    };\n-\n-    for (unsigned k = 0; k < NK; k += 4)\n-      for (unsigned n = 0; n < numN / 2; ++n) {\n-        if (!visited.count({n, k}))\n-          ld(n, k);\n-      }\n-\n-    return visited.size() * 2;\n-  }\n+                         int vec) const;\n \n   // Loading $a from smem to registers, returns a LLVM::Struct.\n   Value loadA(Value tensor, const SharedMemoryObject &smemObj, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const {\n-    auto *ctx = rewriter.getContext();\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-    auto shape = tensorTy.getShape();\n-    auto order = sharedLayout.getOrder();\n-\n-    Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n-    Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-\n-    bool isARow = order[0] != 0;\n-    auto [isARow_, _0, isAVec4, _1, _2] = mmaLayout.decodeVoltaLayoutStates();\n-\n-    AParam param(isARow_, isAVec4);\n-\n-    auto [offsetAM, offsetAK, _3, _4] = computeOffsets(\n-        thread, isARow, false, fpw, param.spw, param.rep, rewriter, loc);\n-\n-    int vecA = sharedLayout.getVec();\n-\n-    auto strides = smemObj.strides;\n-    Value strideAM = isARow ? strides[0] : i32_val(1);\n-    Value strideAK = isARow ? i32_val(1) : strides[1];\n-    Value strideA0 = isARow ? strideAK : strideAM;\n-    Value strideA1 = isARow ? strideAM : strideAK;\n-\n-    int strideRepM = wpt[0] * fpw[0] * 8;\n-    int strideRepK = 1;\n-\n-    // swizzling\n-    int perPhaseA = sharedLayout.getPerPhase();\n-    int maxPhaseA = sharedLayout.getMaxPhase();\n-    int stepA0 = isARow ? strideRepK : strideRepM;\n-    int numPtrA = std::max(2 * perPhaseA * maxPhaseA / stepA0, 1);\n-    int NK = shape[1];\n-\n-    // pre-compute pointer lanes\n-    Value offA0 = isARow ? offsetAK : offsetAM;\n-    Value offA1 = isARow ? offsetAM : offsetAK;\n-    Value phaseA = urem(udiv(offA1, i32_val(perPhaseA)), i32_val(maxPhaseA));\n-    offA0 = add(offA0, cSwizzleOffset);\n-    SmallVector<Value> offA(numPtrA);\n-    for (int i = 0; i < numPtrA; i++) {\n-      Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));\n-      offA0I = udiv(offA0I, i32_val(vecA));\n-      offA0I = xor_(offA0I, phaseA);\n-      offA0I = mul(offA0I, i32_val(vecA));\n-      offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n-    }\n-\n-    Type elemX2Ty = vec_ty(f16_ty, 2);\n-    Type elemPtrTy = ptr_ty(f16_ty);\n-    if (tensorTy.getElementType().isBF16()) {\n-      elemX2Ty = vec_ty(i16_ty, 2);\n-      elemPtrTy = ptr_ty(i16_ty);\n-    }\n-\n-    // prepare arguments\n-    SmallVector<Value> ptrA(numPtrA);\n-\n-    std::map<std::pair<int, int>, std::pair<Value, Value>> has;\n-    for (int i = 0; i < numPtrA; i++)\n-      ptrA[i] = gep(ptr_ty(f16_ty), smemBase, offA[i]);\n-\n-    auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n-      vals[{m, k}] = {val0, val1};\n-    };\n-    auto loadA = [&](int m, int k) {\n-      int offidx = (isARow ? k / 4 : m) % numPtrA;\n-      Value thePtrA = gep(elemPtrTy, smemBase, offA[offidx]);\n-\n-      int stepAM = isARow ? m : m / numPtrA * numPtrA;\n-      int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n-      Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),\n-                         mul(i32_val(stepAK), strideAK));\n-      Value pa = gep(elemPtrTy, thePtrA, offset);\n-      Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n-      Value ha = load(bitcast(pa, aPtrTy));\n-      // record lds that needs to be moved\n-      Value ha00 = bitcast(extract_element(ha, i32_val(0)), elemX2Ty);\n-      Value ha01 = bitcast(extract_element(ha, i32_val(1)), elemX2Ty);\n-      ld(has, m, k, ha00, ha01);\n-\n-      if (vecA > 4) {\n-        Value ha10 = bitcast(extract_element(ha, i32_val(2)), elemX2Ty);\n-        Value ha11 = bitcast(extract_element(ha, i32_val(3)), elemX2Ty);\n-        if (isARow)\n-          ld(has, m, k + 4, ha10, ha11);\n-        else\n-          ld(has, m + 1, k, ha10, ha11);\n-      }\n-    };\n-\n-    unsigned numM = getNumM(shape[0], isARow, isAVec4);\n-    for (unsigned k = 0; k < NK; k += 4)\n-      for (unsigned m = 0; m < numM / 2; ++m)\n-        if (!has.count({m, k}))\n-          loadA(m, k);\n-\n-    SmallVector<Value> elems;\n-    elems.reserve(has.size() * 2);\n-    for (auto item : has) { // has is a map, the key should be ordered.\n-      elems.push_back(item.second.first);\n-      elems.push_back(item.second.second);\n-    }\n-\n-    Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n-    Value res = getStructFromElements(loc, elems, rewriter, resTy);\n-    return res;\n-  }\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n \n   // Loading $b from smem to registers, returns a LLVM::Struct.\n   Value loadB(Value tensor, const SharedMemoryObject &smemObj, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const {\n-    // smem\n-    auto strides = smemObj.strides;\n-\n-    auto *ctx = rewriter.getContext();\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-\n-    auto shape = tensorTy.getShape();\n-    auto order = sharedLayout.getOrder();\n-\n-    Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-    bool isBRow = order[0] != 0; // is row-major in shared memory layout\n-    // isBRow_ indicates whether B is row-major in DotOperand layout\n-    auto [_0, isBRow_, _1, isBVec4, _2] = mmaLayout.decodeVoltaLayoutStates();\n-    assert(isBRow == isBRow_ && \"B need smem isRow\");\n-\n-    BParam param(isBRow_, isBVec4);\n-\n-    int vecB = sharedLayout.getVec();\n-    Value strideBN = isBRow ? i32_val(1) : strides[1];\n-    Value strideBK = isBRow ? strides[0] : i32_val(1);\n-    Value strideB0 = isBRow ? strideBN : strideBK;\n-    Value strideB1 = isBRow ? strideBK : strideBN;\n-    int strideRepN = wpt[1] * fpw[1] * 8;\n-    int strideRepK = 1;\n-\n-    auto [_3, _4, offsetBN, offsetBK] = computeOffsets(\n-        thread, false, isBRow, fpw, param.spw, param.rep, rewriter, loc);\n-\n-    // swizzling\n-    int perPhaseB = sharedLayout.getPerPhase();\n-    int maxPhaseB = sharedLayout.getMaxPhase();\n-    int stepB0 = isBRow ? strideRepN : strideRepK;\n-    int numPtrB = std::max(2 * perPhaseB * maxPhaseB / stepB0, 1);\n-    int NK = shape[0];\n-\n-    Value offB0 = isBRow ? offsetBN : offsetBK;\n-    Value offB1 = isBRow ? offsetBK : offsetBN;\n-    Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n-    Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n-\n-    offB0 = add(offB0, cSwizzleOffset);\n-    SmallVector<Value> offB(numPtrB);\n-    for (int i = 0; i < numPtrB; ++i) {\n-      Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));\n-      offB0I = udiv(offB0I, i32_val(vecB));\n-      offB0I = xor_(offB0I, phaseB);\n-      offB0I = mul(offB0I, i32_val(vecB));\n-      offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));\n-    }\n-\n-    Type elemPtrTy = ptr_ty(f16_ty);\n-    Type elemX2Ty = vec_ty(f16_ty, 2);\n-    if (tensorTy.getElementType().isBF16()) {\n-      elemPtrTy = ptr_ty(i16_ty);\n-      elemX2Ty = vec_ty(i16_ty, 2);\n-    }\n-\n-    SmallVector<Value> ptrB(numPtrB);\n-    ValueTable hbs;\n-    for (int i = 0; i < numPtrB; ++i)\n-      ptrB[i] = gep(ptr_ty(f16_ty), smem, offB[i]);\n-\n-    auto ld = [&](decltype(hbs) &vals, int m, int k, Value val0, Value val1) {\n-      vals[{m, k}] = {val0, val1};\n-    };\n-\n-    auto loadB = [&](int n, int K) {\n-      int offidx = (isBRow ? n : K / 4) % numPtrB;\n-      Value thePtrB = ptrB[offidx];\n-\n-      int stepBN = isBRow ? n / numPtrB * numPtrB : n;\n-      int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n-      Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n-                         mul(i32_val(stepBK), strideBK));\n-      Value pb = gep(elemPtrTy, thePtrB, offset);\n-\n-      Value hb =\n-          load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n-      // record lds that needs to be moved\n-      Value hb00 = bitcast(extract_element(hb, i32_val(0)), elemX2Ty);\n-      Value hb01 = bitcast(extract_element(hb, i32_val(1)), elemX2Ty);\n-      ld(hbs, n, K, hb00, hb01);\n-      if (vecB > 4) {\n-        Value hb10 = bitcast(extract_element(hb, i32_val(2)), elemX2Ty);\n-        Value hb11 = bitcast(extract_element(hb, i32_val(3)), elemX2Ty);\n-        if (isBRow)\n-          ld(hbs, n + 1, K, hb10, hb11);\n-        else\n-          ld(hbs, n, K + 4, hb10, hb11);\n-      }\n-    };\n-\n-    unsigned numN = getNumN(shape[1], isBRow, isBVec4);\n-    for (unsigned k = 0; k < NK; k += 4)\n-      for (unsigned n = 0; n < numN / 2; ++n) {\n-        if (!hbs.count({n, k}))\n-          loadB(n, k);\n-      }\n-\n-    SmallVector<Value> elems;\n-    for (auto &item : hbs) { // has is a map, the key should be ordered.\n-      elems.push_back(item.second.first);\n-      elems.push_back(item.second.second);\n-    }\n-\n-    Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n-    Value res = getStructFromElements(loc, elems, rewriter, resTy);\n-    return res;\n-  }\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n \n   static ArrayRef<unsigned> getOrder() { return mmaOrder; }\n \n@@ -421,80 +143,12 @@ struct DotOpMmaV1ConversionHelper {\n   std::tuple<Value, Value, Value, Value>\n   computeOffsets(Value threadId, bool isARow, bool isBRow, ArrayRef<int> fpw,\n                  ArrayRef<int> spw, ArrayRef<int> rep,\n-                 ConversionPatternRewriter &rewriter, Location loc) const {\n-    auto *ctx = rewriter.getContext();\n-    Value _1 = i32_val(1);\n-    Value _3 = i32_val(3);\n-    Value _4 = i32_val(4);\n-    Value _16 = i32_val(16);\n-    Value _32 = i32_val(32);\n-\n-    Value lane = urem(threadId, _32);\n-    Value warp = udiv(threadId, _32);\n-\n-    // warp offset\n-    Value warp0 = urem(warp, i32_val(wpt[0]));\n-    Value warp12 = udiv(warp, i32_val(wpt[0]));\n-    Value warp1 = urem(warp12, i32_val(wpt[1]));\n-    Value warpMOff = mul(warp0, i32_val(spw[0]));\n-    Value warpNOff = mul(warp1, i32_val(spw[1]));\n-    // Quad offset\n-    Value quadMOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[0]));\n-    Value quadNOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[1]));\n-    // Pair offset\n-    Value pairMOff = udiv(urem(lane, _16), _4);\n-    pairMOff = urem(pairMOff, i32_val(fpw[0]));\n-    pairMOff = mul(pairMOff, _4);\n-    Value pairNOff = udiv(urem(lane, _16), _4);\n-    pairNOff = udiv(pairNOff, i32_val(fpw[0]));\n-    pairNOff = urem(pairNOff, i32_val(fpw[1]));\n-    pairNOff = mul(pairNOff, _4);\n-    // scale\n-    pairMOff = mul(pairMOff, i32_val(rep[0] / 2));\n-    quadMOff = mul(quadMOff, i32_val(rep[0] / 2));\n-    pairNOff = mul(pairNOff, i32_val(rep[1] / 2));\n-    quadNOff = mul(quadNOff, i32_val(rep[1] / 2));\n-    // Quad pair offset\n-    Value laneMOff = add(pairMOff, quadMOff);\n-    Value laneNOff = add(pairNOff, quadNOff);\n-    // A offset\n-    Value offsetAM = add(warpMOff, laneMOff);\n-    Value offsetAK = and_(lane, _3);\n-    // B offset\n-    Value offsetBN = add(warpNOff, laneNOff);\n-    Value offsetBK = and_(lane, _3);\n-    // i indices\n-    Value offsetCM = add(and_(lane, _1), offsetAM);\n-    if (isARow) {\n-      offsetAM = add(offsetAM, urem(threadId, _4));\n-      offsetAK = i32_val(0);\n-    }\n-    if (!isBRow) {\n-      offsetBN = add(offsetBN, urem(threadId, _4));\n-      offsetBK = i32_val(0);\n-    }\n-\n-    return std::make_tuple(offsetAM, offsetAK, offsetBN, offsetBK);\n-  }\n+                 ConversionPatternRewriter &rewriter, Location loc) const;\n \n   // Extract values belong to $a or $b from a LLVMStruct, the shape is n0xn1.\n   DotOpMmaV1ConversionHelper::ValueTable\n   extractLoadedOperand(Value llStruct, int NK,\n-                       ConversionPatternRewriter &rewriter) const {\n-    ValueTable rcds;\n-    SmallVector<Value> elems =\n-        getElementsFromStruct(llStruct.getLoc(), llStruct, rewriter);\n-\n-    int offset = 0;\n-    for (int i = 0; offset < elems.size(); ++i) {\n-      for (int k = 0; k < NK; k += 4) {\n-        rcds[{i, k}] = std::make_pair(elems[offset], elems[offset + 1]);\n-        offset += 2;\n-      }\n-    }\n-\n-    return rcds;\n-  }\n+                       ConversionPatternRewriter &rewriter) const;\n \n   // Get the number of elements of this thread in M axis. The N axis could be\n   // further deduced with the accSize / elemsM. \\param wpt: the wpt in M axis\n@@ -510,97 +164,7 @@ struct DotOpMmaV1ConversionHelper {\n   static SmallVector<CoordTy>\n   getMNCoords(Value thread, ConversionPatternRewriter &rewriter,\n               ArrayRef<unsigned> wpt, ArrayRef<int64_t> shape, bool isARow,\n-              bool isBRow, bool isAVec4, bool isBVec4) {\n-\n-    auto *ctx = thread.getContext();\n-    auto loc = UnknownLoc::get(ctx);\n-    Value _1 = i32_val(1);\n-    Value _2 = i32_val(2);\n-    Value _4 = i32_val(4);\n-    Value _16 = i32_val(16);\n-    Value _32 = i32_val(32);\n-    Value _fpw0 = i32_val(fpw[0]);\n-    Value _fpw1 = i32_val(fpw[1]);\n-\n-    DotOpMmaV1ConversionHelper::AParam aParam(isARow, isAVec4);\n-    DotOpMmaV1ConversionHelper::BParam bParam(isBRow, isBVec4);\n-\n-    SmallVector<int, 2> rep({aParam.rep[0], bParam.rep[1]});\n-    SmallVector<int, 2> spw({aParam.spw[0], bParam.spw[1]});\n-    SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n-\n-    Value lane = urem(thread, _32);\n-    Value warp = udiv(thread, _32);\n-\n-    Value warp0 = urem(warp, i32_val(wpt[0]));\n-    Value warp12 = udiv(warp, i32_val(wpt[0]));\n-    Value warp1 = urem(warp12, i32_val(wpt[1]));\n-\n-    // warp offset\n-    Value offWarpM = mul(warp0, i32_val(spw[0]));\n-    Value offWarpN = mul(warp1, i32_val(spw[1]));\n-    // quad offset\n-    Value offQuadM = mul(udiv(and_(lane, _16), _4), _fpw0);\n-    Value offQuadN = mul(udiv(and_(lane, _16), _4), _fpw1);\n-    // pair offset\n-    Value offPairM = udiv(urem(lane, _16), _4);\n-    offPairM = urem(offPairM, _fpw0);\n-    offPairM = mul(offPairM, _4);\n-    Value offPairN = udiv(urem(lane, _16), _4);\n-    offPairN = udiv(offPairN, _fpw0);\n-    offPairN = urem(offPairN, _fpw1);\n-    offPairN = mul(offPairN, _4);\n-\n-    // sclare\n-    offPairM = mul(offPairM, i32_val(rep[0] / 2));\n-    offQuadM = mul(offQuadM, i32_val(rep[0] / 2));\n-    offPairN = mul(offPairN, i32_val(rep[1] / 2));\n-    offQuadN = mul(offQuadN, i32_val(rep[1] / 2));\n-\n-    // quad pair offset\n-    Value offLaneM = add(offPairM, offQuadM);\n-    Value offLaneN = add(offPairN, offQuadN);\n-    // a, b offset\n-    Value offsetAM = add(offWarpM, offLaneM);\n-    Value offsetBN = add(offWarpN, offLaneN);\n-    // m indices\n-    Value offsetCM = add(and_(lane, _1), offsetAM);\n-    SmallVector<Value> idxM;\n-    for (unsigned m = 0; m < shape[0]; m += shapePerCTA[0])\n-      for (unsigned mm = 0; mm < rep[0]; ++mm)\n-        idxM.push_back(add(offsetCM, i32_val(m + mm * 2)));\n-\n-    // n indices\n-    Value offsetCN = add((and_(lane, _2)), (add(offWarpN, offPairN)));\n-    SmallVector<Value> idxN;\n-    for (int n = 0; n < shape[1]; n += shapePerCTA[1]) {\n-      for (int nn = 0; nn < rep[1]; ++nn) {\n-        idxN.push_back(add(offsetCN, i32_val(n + nn / 2 * 4 +\n-                                             (nn % 2) * 2 * fpw[1] * rep[1])));\n-        idxN.push_back(\n-            add(offsetCN,\n-                i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1] + 1)));\n-      }\n-    }\n-\n-    SmallVector<SmallVector<Value>> axes({idxM, idxN});\n-\n-    // product the axis M and axis N to get coords, ported from\n-    // generator::init_idx method from triton2.0\n-\n-    // TODO[Superjomn]: check the order.\n-    SmallVector<CoordTy> coords;\n-    for (Value x1 : axes[1]) {   // N\n-      for (Value x0 : axes[0]) { // M\n-        SmallVector<Value, 2> idx(2);\n-        idx[0] = x0; // M\n-        idx[1] = x1; // N\n-        coords.push_back(std::move(idx));\n-      }\n-    }\n-\n-    return coords; // {M,N} in row-major\n-  }\n+              bool isBRow, bool isAVec4, bool isBVec4);\n \n   // \\param elemId the offset of the element in a thread\n   static CoordTy getCoord(int elemId, ArrayRef<CoordTy> coords) {\n@@ -647,107 +211,16 @@ struct DotOpMmaV2ConversionHelper {\n     return {16, 8};\n   }\n \n-  static std::tuple<int, int> getRepMN(const RankedTensorType &tensorTy) {\n-    auto mmaLayout = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n-    auto wpt = mmaLayout.getWarpsPerCTA();\n-\n-    int M = tensorTy.getShape()[0];\n-    int N = tensorTy.getShape()[1];\n-    auto [instrM, instrN] = getInstrShapeMN();\n-    int repM = std::max<int>(M / (wpt[0] * instrM), 1);\n-    int repN = std::max<int>(N / (wpt[1] * instrN), 1);\n-    return {repM, repN};\n-  }\n+  static std::tuple<int, int> getRepMN(const RankedTensorType &tensorTy);\n \n-  Type getShemPtrTy() const {\n-    switch (mmaType) {\n-    case TensorCoreType::FP32_FP16_FP16_FP32:\n-      return ptr_ty(type::f16Ty(ctx), 3);\n-    case TensorCoreType::FP32_BF16_BF16_FP32:\n-      return ptr_ty(type::i16Ty(ctx), 3);\n-    case TensorCoreType::FP32_TF32_TF32_FP32:\n-      return ptr_ty(type::f32Ty(ctx), 3);\n-    case TensorCoreType::INT32_INT8_INT8_INT32:\n-      return ptr_ty(type::i8Ty(ctx), 3);\n-    default:\n-      llvm::report_fatal_error(\"mma16816 data type not supported\");\n-    }\n-    return Type{};\n-  }\n+  Type getShemPtrTy() const;\n \n   // The type of matrix that loaded by either a ldmatrix or composed lds.\n-  Type getMatType() const {\n-    // floating point types\n-    Type fp32x1Ty = vec_ty(type::f32Ty(ctx), 1);\n-    Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n-    Type i16x2Ty = vec_ty(type::i16Ty(ctx), 2);\n-    Type fp16x2Pack4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n-    // LLVM 14.0 does not support bf16 type, so we use i16 instead.\n-    Type bf16x2Pack4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i16x2Ty));\n-    Type fp32Pack4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32x1Ty));\n-    // integer types\n-    Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n-    Type i8x4Pack4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i8x4Ty));\n-\n-    switch (mmaType) {\n-    case TensorCoreType::FP32_FP16_FP16_FP32:\n-      return fp16x2Pack4Ty;\n-    case TensorCoreType::FP32_BF16_BF16_FP32:\n-      return bf16x2Pack4Ty;\n-    case TensorCoreType::FP32_TF32_TF32_FP32:\n-      return fp32Pack4Ty;\n-    case TensorCoreType::INT32_INT8_INT8_INT32:\n-      return i8x4Pack4Ty;\n-    default:\n-      llvm::report_fatal_error(\"Unsupported mma type found\");\n-    }\n-\n-    return Type{};\n-  }\n+  Type getMatType() const;\n \n-  Type getLoadElemTy() {\n-    switch (mmaType) {\n-    case TensorCoreType::FP32_FP16_FP16_FP32:\n-      return vec_ty(type::f16Ty(ctx), 2);\n-    case TensorCoreType::FP32_BF16_BF16_FP32:\n-      return vec_ty(type::bf16Ty(ctx), 2);\n-    case TensorCoreType::FP32_TF32_TF32_FP32:\n-      return type::f32Ty(ctx);\n-    case TensorCoreType::INT32_INT8_INT8_INT32:\n-      return type::i32Ty(ctx);\n-    default:\n-      llvm::report_fatal_error(\"Unsupported mma type found\");\n-    }\n-\n-    return Type{};\n-  }\n+  Type getLoadElemTy();\n \n-  Type getMmaRetType() const {\n-    Type fp32Ty = type::f32Ty(ctx);\n-    Type i32Ty = type::i32Ty(ctx);\n-    Type fp32x4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n-    Type i32x4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i32Ty));\n-    switch (mmaType) {\n-    case TensorCoreType::FP32_FP16_FP16_FP32:\n-      return fp32x4Ty;\n-    case TensorCoreType::FP32_BF16_BF16_FP32:\n-      return fp32x4Ty;\n-    case TensorCoreType::FP32_TF32_TF32_FP32:\n-      return fp32x4Ty;\n-    case TensorCoreType::INT32_INT8_INT8_INT32:\n-      return i32x4Ty;\n-    default:\n-      llvm::report_fatal_error(\"Unsupported mma type found\");\n-    }\n-\n-    return Type{};\n-  }\n+  Type getMmaRetType() const;\n \n   ArrayRef<int> getMmaInstrShape() const {\n     assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n@@ -768,19 +241,7 @@ struct DotOpMmaV2ConversionHelper {\n   }\n \n   // Deduce the TensorCoreType from either $a or $b's type.\n-  static TensorCoreType getTensorCoreTypeFromOperand(Type operandTy) {\n-    auto tensorTy = operandTy.cast<RankedTensorType>();\n-    auto elemTy = tensorTy.getElementType();\n-    if (elemTy.isF16())\n-      return TensorCoreType::FP32_FP16_FP16_FP32;\n-    if (elemTy.isF32())\n-      return TensorCoreType::FP32_TF32_TF32_FP32;\n-    if (elemTy.isBF16())\n-      return TensorCoreType::FP32_BF16_BF16_FP32;\n-    if (elemTy.isInteger(8))\n-      return TensorCoreType::INT32_INT8_INT8_INT32;\n-    return TensorCoreType::NOT_APPLICABLE;\n-  }\n+  static TensorCoreType getTensorCoreTypeFromOperand(Type operandTy);\n \n   int getVec() const {\n     assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n@@ -794,30 +255,7 @@ struct DotOpMmaV2ConversionHelper {\n     return mmaInstrPtx.at(mmaType);\n   }\n \n-  static TensorCoreType getMmaType(triton::DotOp op) {\n-    Value A = op.a();\n-    Value B = op.b();\n-    auto aTy = A.getType().cast<RankedTensorType>();\n-    auto bTy = B.getType().cast<RankedTensorType>();\n-    // d = a*b + c\n-    auto dTy = op.d().getType().cast<RankedTensorType>();\n-\n-    if (dTy.getElementType().isF32()) {\n-      if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n-        return TensorCoreType::FP32_FP16_FP16_FP32;\n-      if (aTy.getElementType().isBF16() && bTy.getElementType().isBF16())\n-        return TensorCoreType::FP32_BF16_BF16_FP32;\n-      if (aTy.getElementType().isF32() && bTy.getElementType().isF32() &&\n-          op.allowTF32())\n-        return TensorCoreType::FP32_TF32_TF32_FP32;\n-    } else if (dTy.getElementType().isInteger(32)) {\n-      if (aTy.getElementType().isInteger(8) &&\n-          bTy.getElementType().isInteger(8))\n-        return TensorCoreType::INT32_INT8_INT8_INT32;\n-    }\n-\n-    return TensorCoreType::NOT_APPLICABLE;\n-  }\n+  static TensorCoreType getMmaType(triton::DotOp op);\n \n private:\n   mutable TensorCoreType mmaType{TensorCoreType::NOT_APPLICABLE};\n@@ -892,50 +330,7 @@ class MMA16816SmemLoader {\n                      ArrayRef<int> instrShape, ArrayRef<int> matShape,\n                      int perPhase, int maxPhase, int elemBytes,\n                      ConversionPatternRewriter &rewriter,\n-                     TypeConverter *typeConverter, const Location &loc)\n-      : order(order.begin(), order.end()), kOrder(kOrder),\n-        tileShape(tileShape.begin(), tileShape.end()),\n-        instrShape(instrShape.begin(), instrShape.end()),\n-        matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n-        maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter), loc(loc),\n-        ctx(rewriter.getContext()) {\n-    cMatShape = matShape[order[0]];\n-    sMatShape = matShape[order[1]];\n-\n-    sStride = smemStrides[order[1]];\n-\n-    // rule: k must be the fast-changing axis.\n-    needTrans = kOrder != order[0];\n-    canUseLdmatrix = elemBytes == 2 || (!needTrans); // b16\n-\n-    if (canUseLdmatrix) {\n-      // Each CTA, the warps is arranged as [1xwpt] if not transposed,\n-      // otherwise [wptx1], and each warp will perform a mma.\n-      numPtrs =\n-          tileShape[order[0]] / (needTrans ? wpt : 1) / instrShape[order[0]];\n-    } else {\n-      numPtrs = tileShape[order[0]] / wpt / matShape[order[0]];\n-    }\n-    numPtrs = std::max<int>(numPtrs, 2);\n-\n-    // Special rule for i8/u8, 4 ptrs for each matrix\n-    if (!canUseLdmatrix && elemBytes == 1)\n-      numPtrs *= 4;\n-\n-    int loadStrideInMat[2];\n-    loadStrideInMat[kOrder] =\n-        2; // instrShape[kOrder] / matShape[kOrder], always 2\n-    loadStrideInMat[kOrder ^ 1] =\n-        wpt * (instrShape[kOrder ^ 1] / matShape[kOrder ^ 1]);\n-\n-    pLoadStrideInMat = loadStrideInMat[order[0]];\n-    sMatStride =\n-        loadStrideInMat[order[1]] / (instrShape[order[1]] / matShape[order[1]]);\n-\n-    // Each matArr contains warpOffStride matrices.\n-    matArrStride = kOrder == 1 ? 1 : wpt;\n-    warpOffStride = instrShape[kOrder ^ 1] / matShape[kOrder ^ 1];\n-  }\n+                     TypeConverter *typeConverter, const Location &loc);\n \n   // lane = thread % 32\n   // warpOff = (thread/32) % wpt(0)\n@@ -958,307 +353,20 @@ class MMA16816SmemLoader {\n   // Compute the offset to the matrix this thread(indexed by warpOff and lane)\n   // mapped to.\n   SmallVector<Value> computeLdmatrixMatOffs(Value warpId, Value lane,\n-                                            Value cSwizzleOffset) {\n-    // 4x4 matrices\n-    Value c = urem(lane, i32_val(8));\n-    Value s = udiv(lane, i32_val(8)); // sub-warp-id\n-\n-    // Decompose s => s_0, s_1, that is the coordinate in 2x2 matrices in a\n-    // warp\n-    Value s0 = urem(s, i32_val(2));\n-    Value s1 = udiv(s, i32_val(2));\n-\n-    // We use different orders for a and b for better performance.\n-    Value kMatArr = kOrder == 1 ? s1 : s0;\n-    Value nkMatArr = kOrder == 1 ? s0 : s1;\n-\n-    // matrix coordinate inside a CTA, the matrix layout is [2x2wpt] for A and\n-    // [2wptx2] for B. e.g. Setting wpt=3, The data layout for A(kOrder=1) is\n-    //   |0 0 1 1 2 2| -> 0,1,2 are the warpids\n-    //   |0 0 1 1 2 2|\n-    //\n-    // for B(kOrder=0) is\n-    //   |0 0|  -> 0,1,2 are the warpids\n-    //   |1 1|\n-    //   |2 2|\n-    //   |0 0|\n-    //   |1 1|\n-    //   |2 2|\n-    // Note, for each warp, it handles a 2x2 matrices, that is the coordinate\n-    // address (s0,s1) annotates.\n-\n-    Value matOff[2];\n-    matOff[kOrder ^ 1] = add(\n-        mul(warpId, i32_val(warpOffStride)),   // warp offset\n-        mul(nkMatArr, i32_val(matArrStride))); // matrix offset inside a warp\n-    matOff[kOrder] = kMatArr;\n-\n-    // Physical offset (before swizzling)\n-    Value cMatOff = matOff[order[0]];\n-    Value sMatOff = matOff[order[1]];\n-    Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n-    cMatOff = add(cMatOff, cSwizzleMatOff);\n-\n-    // row offset inside a matrix, each matrix has 8 rows.\n-    Value sOffInMat = c;\n-\n-    SmallVector<Value> offs(numPtrs);\n-    Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n-    Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n-    for (int i = 0; i < numPtrs; ++i) {\n-      Value cMatOffI = add(cMatOff, i32_val(i * pLoadStrideInMat));\n-      cMatOffI = xor_(cMatOffI, phase);\n-      offs[i] = add(mul(cMatOffI, i32_val(cMatShape)), mul(sOff, sStride));\n-    }\n-\n-    return offs;\n-  }\n+                                            Value cSwizzleOffset);\n \n   // Compute 32-bit matrix offsets.\n   SmallVector<Value> computeB32MatOffs(Value warpOff, Value lane,\n-                                       Value cSwizzleOffset) {\n-    assert(needTrans && \"Only used in transpose mode.\");\n-    // Load tf32 matrices with lds32\n-    Value cOffInMat = udiv(lane, i32_val(4));\n-    Value sOffInMat = urem(lane, i32_val(4));\n-\n-    Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n-    SmallVector<Value> offs(numPtrs);\n-\n-    for (int mat = 0; mat < 4; ++mat) { // Load 4 mats each time\n-      int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n-      int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n-      if (kMatArrInt > 0) // we don't need pointers for k\n-        continue;\n-      Value kMatArr = i32_val(kMatArrInt);\n-      Value nkMatArr = i32_val(nkMatArrInt);\n-\n-      Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n-                          mul(nkMatArr, i32_val(matArrStride)));\n-      Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n-      cMatOff = add(cMatOff, cSwizzleMatOff);\n-\n-      Value sMatOff = kMatArr;\n-      Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n-      // FIXME: (kOrder == 1?) is really dirty hack\n-      for (int i = 0; i < numPtrs / 2; ++i) {\n-        Value cMatOffI =\n-            add(cMatOff, i32_val(i * pLoadStrideInMat * (kOrder == 1 ? 1 : 2)));\n-        cMatOffI = xor_(cMatOffI, phase);\n-        Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n-        cOff = urem(cOff, i32_val(tileShape[order[0]]));\n-        sOff = urem(sOff, i32_val(tileShape[order[1]]));\n-        offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, sStride));\n-      }\n-    }\n-    return offs;\n-  }\n+                                       Value cSwizzleOffset);\n \n   // compute 8-bit matrix offset.\n   SmallVector<Value> computeB8MatOffs(Value warpOff, Value lane,\n-                                      Value cSwizzleOffset) {\n-    assert(needTrans && \"Only used in transpose mode.\");\n-    Value cOffInMat = udiv(lane, i32_val(4));\n-    Value sOffInMat =\n-        mul(urem(lane, i32_val(4)), i32_val(4)); // each thread load 4 cols\n-\n-    SmallVector<Value> offs(numPtrs);\n-    for (int mat = 0; mat < 4; ++mat) {\n-      int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n-      int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n-      if (kMatArrInt > 0) // we don't need pointers for k\n-        continue;\n-      Value kMatArr = i32_val(kMatArrInt);\n-      Value nkMatArr = i32_val(nkMatArrInt);\n-\n-      Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n-                          mul(nkMatArr, i32_val(matArrStride)));\n-      Value sMatOff = kMatArr;\n-\n-      for (int loadx4Off = 0; loadx4Off < numPtrs / 8; ++loadx4Off) {\n-        for (int elemOff = 0; elemOff < 4; ++elemOff) {\n-          int ptrOff = loadx4Off * 8 + nkMatArrInt * 4 + elemOff;\n-          Value cMatOffI = add(cMatOff, i32_val(loadx4Off * pLoadStrideInMat *\n-                                                (kOrder == 1 ? 1 : 2)));\n-          Value sOffInMatElem = add(sOffInMat, i32_val(elemOff));\n-\n-          // disable swizzling ...\n-\n-          Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n-          Value sOff = add(sOffInMatElem, mul(sMatOff, i32_val(sMatShape)));\n-          // To prevent out-of-bound access when tile is too small.\n-          cOff = urem(cOff, i32_val(tileShape[order[0]]));\n-          sOff = urem(sOff, i32_val(tileShape[order[1]]));\n-          offs[ptrOff] = add(cOff, mul(sOff, sStride));\n-        }\n-      }\n-    }\n-    return offs;\n-  }\n+                                      Value cSwizzleOffset);\n \n   // Load 4 matrices and returns 4 vec<2> elements.\n   std::tuple<Value, Value, Value, Value>\n   loadX4(int mat0, int mat1, ArrayRef<Value> offs, ArrayRef<Value> ptrs,\n-         Type matTy, Type shemPtrTy) const {\n-    assert(mat0 % 2 == 0 && mat1 % 2 == 0 &&\n-           \"smem matrix load must be aligned\");\n-    int matIdx[2] = {mat0, mat1};\n-\n-    int ptrIdx{-1};\n-\n-    if (canUseLdmatrix)\n-      ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n-    else if (elemBytes == 4 && needTrans)\n-      ptrIdx = matIdx[order[0]];\n-    else if (elemBytes == 1 && needTrans)\n-      ptrIdx = matIdx[order[0]] * 4;\n-    else\n-      llvm::report_fatal_error(\"unsupported mma type found\");\n-\n-    // The main difference with the original triton code is we removed the\n-    // prefetch-related logic here for the upstream optimizer phase should\n-    // take care with it, and that is transparent in dot conversion.\n-    auto getPtr = [&](int idx) { return ptrs[idx]; };\n-\n-    Value ptr = getPtr(ptrIdx);\n-\n-    // The struct should have exactly the same element types.\n-    auto resTy = matTy.cast<LLVM::LLVMStructType>();\n-    Type elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n-\n-    // For some reasons, LLVM's NVPTX backend inserts unnecessary (?) integer\n-    // instructions to pack & unpack sub-word integers. A workaround is to\n-    // store the results of ldmatrix in i32\n-    if (auto vecElemTy = elemTy.dyn_cast<VectorType>()) {\n-      Type elemElemTy = vecElemTy.getElementType();\n-      if (auto intTy = elemElemTy.dyn_cast<IntegerType>()) {\n-        if (intTy.getWidth() <= 16) {\n-          elemTy = rewriter.getI32Type();\n-          resTy = LLVM::LLVMStructType::getLiteral(\n-              ctx, SmallVector<Type>(4, elemTy));\n-        }\n-      }\n-    }\n-\n-    if (canUseLdmatrix) {\n-      Value sOffset =\n-          mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sStride);\n-      Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n-\n-      PTXBuilder builder;\n-      // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n-      // thread.\n-      auto resArgs = builder.newListOperand(4, \"=r\");\n-      auto addrArg = builder.newAddrOperand(sOffsetPtr, \"r\");\n-\n-      auto ldmatrix = builder.create(\"ldmatrix.sync.aligned.m8n8.x4\")\n-                          ->o(\"trans\", needTrans /*predicate*/)\n-                          .o(\"shared.b16\");\n-      ldmatrix(resArgs, addrArg);\n-\n-      // The result type is 4xi32, each i32 is composed of 2xf16\n-      // elements (adjacent two columns in a row) or a single f32 element.\n-      Value resV4 = builder.launch(rewriter, loc, resTy);\n-      return {extract_val(elemTy, resV4, i32_arr_attr(0)),\n-              extract_val(elemTy, resV4, i32_arr_attr(1)),\n-              extract_val(elemTy, resV4, i32_arr_attr(2)),\n-              extract_val(elemTy, resV4, i32_arr_attr(3))};\n-    } else if (elemBytes == 4 &&\n-               needTrans) { // Use lds.32 to load tf32 matrices\n-      Value ptr2 = getPtr(ptrIdx + 1);\n-      assert(sMatStride == 1);\n-      int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n-      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n-      int sOffsetArrElem = sMatStride * sMatShape;\n-      Value sOffsetArrElemVal =\n-          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n-\n-      Value elems[4];\n-      if (kOrder == 1) {\n-        elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n-        elems[1] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n-        elems[2] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n-        elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n-      } else {\n-        elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n-        elems[2] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n-        elems[1] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n-        elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n-      }\n-      std::array<Value, 4> retElems;\n-      retElems.fill(undef(elemTy));\n-      for (auto i = 0; i < 4; ++i) {\n-        retElems[i] = insert_element(elemTy, retElems[i], elems[i], i32_val(0));\n-      }\n-      return {retElems[0], retElems[1], retElems[2], retElems[3]};\n-    } else if (elemBytes == 1 && needTrans) { // work with int8\n-      // Can't use i32 here. Use LLVM's VectorType\n-      elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n-      std::array<std::array<Value, 4>, 2> ptrs;\n-      ptrs[0] = {\n-          getPtr(ptrIdx),\n-          getPtr(ptrIdx + 1),\n-          getPtr(ptrIdx + 2),\n-          getPtr(ptrIdx + 3),\n-      };\n-\n-      ptrs[1] = {\n-          getPtr(ptrIdx + 4),\n-          getPtr(ptrIdx + 5),\n-          getPtr(ptrIdx + 6),\n-          getPtr(ptrIdx + 7),\n-      };\n-\n-      assert(sMatStride == 1);\n-      int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n-      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n-      int sOffsetArrElem = 1 * (sMatStride * sMatShape);\n-      Value sOffsetArrElemVal =\n-          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n-\n-      std::array<Value, 4> i8v4Elems;\n-      i8v4Elems.fill(undef(elemTy));\n-\n-      Value i8Elems[4][4];\n-      if (kOrder == 1) {\n-        for (int i = 0; i < 2; ++i)\n-          for (int j = 0; j < 4; ++j)\n-            i8Elems[i][j] = load(gep(shemPtrTy, ptrs[i][j], sOffsetElemVal));\n-\n-        for (int i = 2; i < 4; ++i)\n-          for (int j = 0; j < 4; ++j)\n-            i8Elems[i][j] =\n-                load(gep(shemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n-\n-        for (int m = 0; m < 4; ++m) {\n-          for (int e = 0; e < 4; ++e)\n-            i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n-                                          i8Elems[m][e], i32_val(e));\n-        }\n-      } else { // k first\n-        for (int j = 0; j < 4; ++j)\n-          i8Elems[0][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetElemVal));\n-        for (int j = 0; j < 4; ++j)\n-          i8Elems[2][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetElemVal));\n-        for (int j = 0; j < 4; ++j)\n-          i8Elems[1][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n-        for (int j = 0; j < 4; ++j)\n-          i8Elems[3][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n-\n-        for (int m = 0; m < 4; ++m) {\n-          for (int e = 0; e < 4; ++e)\n-            i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n-                                          i8Elems[m][e], i32_val(e));\n-        }\n-      }\n-\n-      return {bitcast(i8v4Elems[0], i32_ty), bitcast(i8v4Elems[1], i32_ty),\n-              bitcast(i8v4Elems[2], i32_ty), bitcast(i8v4Elems[3], i32_ty)};\n-    }\n-\n-    assert(false && \"Invalid smem load\");\n-    return {Value{}, Value{}, Value{}, Value{}};\n-  }\n+         Type matTy, Type shemPtrTy) const;\n \n private:\n   SmallVector<uint32_t> order;\n@@ -1413,240 +521,28 @@ struct MMA16816ConversionHelper {\n   }\n \n   // Loading $a from smem to registers, returns a LLVM::Struct.\n-  Value loadA(Value tensor, const SharedMemoryObject &smemObj) const {\n-    auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n-\n-    SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n-                               aTensorTy.getShape().end());\n-\n-    ValueTable ha;\n-    std::function<void(int, int)> loadFn;\n-    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n-    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(aTensorTy);\n-\n-    int numRepM = getNumRepM(aTensorTy, shape[0]);\n-    int numRepK = getNumRepK(aTensorTy, shape[1]);\n-\n-    if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n-      Value warpM = getWarpM(shape[0]);\n-      // load from smem\n-      // we use ldmatrix.x4 so each warp processes 16x16 elements.\n-      int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / 16);\n-      loadFn =\n-          getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/,\n-                          {mmaInstrM, mmaInstrK} /*instrShape*/,\n-                          {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/,\n-                          ha /*vals*/, true /*isA*/);\n-    } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n-      // load from registers, used in gemm fuse\n-      // TODO(Superjomn) Port the logic.\n-      assert(false && \"Loading A from register is not supported yet.\");\n-    } else {\n-      assert(false && \"A's layout is not supported.\");\n-    }\n-\n-    // step1. Perform loading.\n-    for (int m = 0; m < numRepM; ++m)\n-      for (int k = 0; k < numRepK; ++k)\n-        loadFn(2 * m, 2 * k);\n-\n-    // step2. Format the values to LLVM::Struct to passing to mma codegen.\n-    return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n-  }\n+  Value loadA(Value tensor, const SharedMemoryObject &smemObj) const;\n \n   // Loading $b from smem to registers, returns a LLVM::Struct.\n-  Value loadB(Value tensor, const SharedMemoryObject &smemObj) {\n-    ValueTable hb;\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-\n-    SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n-                               tensorTy.getShape().end());\n-\n-    // TODO[Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n-    bool transB = false;\n-    if (transB) {\n-      std::swap(shape[0], shape[1]);\n-    }\n-\n-    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(tensorTy);\n-    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(tensorTy);\n-    int numRepK = getNumRepK(tensorTy, shape[0]);\n-    int numRepN = getNumRepN(tensorTy, shape[1]);\n-\n-    Value warpN = getWarpN(shape[1]);\n-    // we use ldmatrix.x4 so each warp processes 16x16 elements.\n-    int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / 16);\n-    auto loadFn =\n-        getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/,\n-                        {mmaInstrK, mmaInstrN} /*instrShape*/,\n-                        {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/,\n-                        hb /*vals*/, false /*isA*/);\n-\n-    for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n-      for (int k = 0; k < numRepK; ++k)\n-        loadFn(2 * n, 2 * k);\n-    }\n-\n-    Value result = composeValuesToDotOperandLayoutStruct(\n-        hb, std::max(numRepN / 2, 1), numRepK);\n-    return result;\n-  }\n+  Value loadB(Value tensor, const SharedMemoryObject &smemObj);\n \n   // Loading $c to registers, returns a Value.\n-  Value loadC(Value tensor, Value llTensor) const {\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(tensorTy);\n-    size_t fcSize = 4 * repM * repN;\n-\n-    assert(tensorTy.getEncoding().isa<MmaEncodingAttr>() &&\n-           \"Currently, we only support $c with a mma layout.\");\n-    // Load a normal C tensor with mma layout, that should be a\n-    // LLVM::struct with fcSize elements.\n-    auto structTy = llTensor.getType().cast<LLVM::LLVMStructType>();\n-    assert(structTy.getBody().size() == fcSize &&\n-           \"DotOp's $c operand should pass the same number of values as $d in \"\n-           \"mma layout.\");\n-    return llTensor;\n-  }\n+  Value loadC(Value tensor, Value llTensor) const;\n \n   // Conduct the Dot conversion.\n   // \\param a, \\param b, \\param c and \\param d are DotOp operands.\n   // \\param loadedA, \\param loadedB, \\param loadedC, all of them are result of\n   // loading.\n   LogicalResult convertDot(Value a, Value b, Value c, Value d, Value loadedA,\n                            Value loadedB, Value loadedC, DotOp op,\n-                           DotOpAdaptor adaptor) const {\n-    helper.deduceMmaType(op);\n-\n-    auto aTensorTy = a.getType().cast<RankedTensorType>();\n-    auto dTensorTy = d.getType().cast<RankedTensorType>();\n-\n-    SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n-                                aTensorTy.getShape().end());\n-\n-    auto dShape = dTensorTy.getShape();\n-\n-    // shape / shape_per_cta\n-    int numRepM = getNumRepM(aTensorTy, dShape[0]);\n-    int numRepN = getNumRepN(aTensorTy, dShape[1]);\n-    int numRepK = getNumRepK(aTensorTy, aShape[1]);\n-\n-    ValueTable ha =\n-        getValuesFromDotOperandLayoutStruct(loadedA, numRepM, numRepK);\n-    ValueTable hb = getValuesFromDotOperandLayoutStruct(\n-        loadedB, std::max(numRepN / 2, 1), numRepK);\n-    auto fc = getElementsFromStruct(loc, loadedC, rewriter);\n-\n-    auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n-      unsigned colsPerThread = numRepN * 2;\n-      PTXBuilder builder;\n-      auto &mma = *builder.create(helper.getMmaInstr().str());\n-      // using =r for float32 works but leads to less readable ptx.\n-      bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n-      auto retArgs = builder.newListOperand(4, isIntMMA ? \"=r\" : \"=f\");\n-      auto aArgs = builder.newListOperand({\n-          {ha[{m, k}], \"r\"},\n-          {ha[{m + 1, k}], \"r\"},\n-          {ha[{m, k + 1}], \"r\"},\n-          {ha[{m + 1, k + 1}], \"r\"},\n-      });\n-      auto bArgs =\n-          builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n-      auto cArgs = builder.newListOperand();\n-      for (int i = 0; i < 4; ++i) {\n-        cArgs->listAppend(builder.newOperand(fc[m * colsPerThread + 4 * n + i],\n-                                             std::to_string(i)));\n-        // reuse the output registers\n-      }\n-\n-      mma(retArgs, aArgs, bArgs, cArgs);\n-      Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n-\n-      Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n-      for (int i = 0; i < 4; ++i)\n-        fc[m * colsPerThread + 4 * n + i] =\n-            extract_val(elemTy, mmaOut, i32_arr_attr(i));\n-    };\n-\n-    for (int k = 0; k < numRepK; ++k)\n-      for (int m = 0; m < numRepM; ++m)\n-        for (int n = 0; n < numRepN; ++n)\n-          callMma(2 * m, n, 2 * k);\n-\n-    Type resElemTy = dTensorTy.getElementType();\n-\n-    for (auto &elem : fc) {\n-      elem = bitcast(elem, resElemTy);\n-    }\n-\n-    // replace with new packed result\n-    Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(fc.size(), resElemTy));\n-    Value res = getStructFromElements(loc, fc, rewriter, structTy);\n-    rewriter.replaceOp(op, res);\n-\n-    return success();\n-  }\n+                           DotOpAdaptor adaptor) const;\n \n private:\n   std::function<void(int, int)>\n   getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n                   MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder,\n                   SmallVector<int> instrShape, SmallVector<int> matShape,\n-                  Value warpId, ValueTable &vals, bool isA) const {\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    // We assumes that the input operand of Dot should be from shared layout.\n-    // TODO(Superjomn) Consider other layouts if needed later.\n-    auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-    const int perPhase = sharedLayout.getPerPhase();\n-    const int maxPhase = sharedLayout.getMaxPhase();\n-    const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n-    auto order = sharedLayout.getOrder();\n-\n-    // the original register_lds2, but discard the prefetch logic.\n-    auto ld2 = [](ValueTable &vals, int mn, int k, Value val) {\n-      vals[{mn, k}] = val;\n-    };\n-\n-    // (a, b) is the coordinate.\n-    auto load = [=, &vals, &ld2](int a, int b) {\n-      MMA16816SmemLoader loader(\n-          wpt, sharedLayout.getOrder(), kOrder, smemObj.strides,\n-          tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n-          maxPhase, elemBytes, rewriter, typeConverter, loc);\n-      Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n-      SmallVector<Value> offs =\n-          loader.computeOffsets(warpId, lane, cSwizzleOffset);\n-      const int numPtrs = loader.getNumPtrs();\n-      SmallVector<Value> ptrs(numPtrs);\n-\n-      Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-\n-      Type smemPtrTy = helper.getShemPtrTy();\n-      for (int i = 0; i < numPtrs; ++i) {\n-        ptrs[i] =\n-            bitcast(gep(smemPtrTy, smemBase, ValueRange({offs[i]})), smemPtrTy);\n-      }\n-\n-      auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n-          (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n-          ptrs, helper.getMatType(), helper.getShemPtrTy());\n-\n-      if (isA) {\n-        ld2(vals, a, b, ha0);\n-        ld2(vals, a + 1, b, ha1);\n-        ld2(vals, a, b + 1, ha2);\n-        ld2(vals, a + 1, b + 1, ha3);\n-      } else {\n-        ld2(vals, a, b, ha0);\n-        ld2(vals, a + 1, b, ha2);\n-        ld2(vals, a, b + 1, ha1);\n-        ld2(vals, a + 1, b + 1, ha3);\n-      }\n-    };\n-\n-    return load;\n-  }\n+                  Value warpId, ValueTable &vals, bool isA) const;\n \n   // Compose a map of Values to a LLVM::Struct.\n   // The layout is a list of Value with coordinate of (i,j), the order is as\n@@ -1664,41 +560,10 @@ struct MMA16816ConversionHelper {\n   // i \\in [0, n0) and j \\in [0, n1)\n   // There should be \\param n0 * \\param n1 elements in the output Struct.\n   Value composeValuesToDotOperandLayoutStruct(const ValueTable &vals, int n0,\n-                                              int n1) const {\n-    std::vector<Value> elems;\n-    for (int m = 0; m < n0; ++m)\n-      for (int k = 0; k < n1; ++k) {\n-        elems.push_back(vals.at({2 * m, 2 * k}));\n-        elems.push_back(vals.at({2 * m, 2 * k + 1}));\n-        elems.push_back(vals.at({2 * m + 1, 2 * k}));\n-        elems.push_back(vals.at({2 * m + 1, 2 * k + 1}));\n-      }\n-\n-    assert(!elems.empty());\n-\n-    Type elemTy = elems[0].getType();\n-    Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(elems.size(), elemTy));\n-    auto result = getStructFromElements(loc, elems, rewriter, structTy);\n-    return result;\n-  }\n+                                              int n1) const;\n \n   ValueTable getValuesFromDotOperandLayoutStruct(Value value, int n0,\n-                                                 int n1) const {\n-    auto elems = getElementsFromStruct(loc, value, rewriter);\n-\n-    int offset{};\n-    ValueTable vals;\n-    for (int i = 0; i < n0; ++i) {\n-      for (int j = 0; j < n1; j++) {\n-        vals[{2 * i, 2 * j}] = elems[offset++];\n-        vals[{2 * i, 2 * j + 1}] = elems[offset++];\n-        vals[{2 * i + 1, 2 * j}] = elems[offset++];\n-        vals[{2 * i + 1, 2 * j + 1}] = elems[offset++];\n-      }\n-    }\n-    return vals;\n-  }\n+                                                 int n1) const;\n };\n \n // Helper for conversion of FMA DotOp.\n@@ -1714,193 +579,26 @@ struct DotOpFMAConversionHelper {\n   SmallVector<Value>\n   getThreadIds(Value threadId, ArrayRef<unsigned> shapePerCTA,\n                ArrayRef<unsigned> sizePerThread, ArrayRef<unsigned> order,\n-               ConversionPatternRewriter &rewriter, Location loc) const {\n-    int dim = order.size();\n-    SmallVector<Value> threadIds(dim);\n-    for (unsigned k = 0; k < dim - 1; k++) {\n-      Value dimK = i32_val(shapePerCTA[order[k]] / sizePerThread[order[k]]);\n-      Value rem = urem(threadId, dimK);\n-      threadId = udiv(threadId, dimK);\n-      threadIds[order[k]] = rem;\n-    }\n-    Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n-    threadIds[order[dim - 1]] = urem(threadId, dimK);\n-    return threadIds;\n-  }\n+               ConversionPatternRewriter &rewriter, Location loc) const;\n \n   Value loadA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const {\n-    auto aTensorTy = A.getType().cast<RankedTensorType>();\n-    auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-    auto aShape = aTensorTy.getShape();\n-\n-    auto aOrder = aLayout.getOrder();\n-    auto order = dLayout.getOrder();\n-\n-    bool isARow = aOrder[0] == 1;\n-\n-    auto aSmem = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n-    Value strideAM = aSmem.strides[0];\n-    Value strideAK = aSmem.strides[1];\n-    Value strideA0 = isARow ? strideAK : strideAM;\n-    Value strideA1 = isARow ? strideAM : strideAK;\n-    int aNumPtr = 8;\n-    int K = aShape[1];\n-    int M = aShape[0];\n-\n-    auto shapePerCTA = getShapePerCTA(dLayout);\n-    auto sizePerThread = getSizePerThread(dLayout);\n-\n-    Value _0 = i32_val(0);\n-\n-    Value mContig = i32_val(sizePerThread[order[1]]);\n-\n-    // threadId in blocked layout\n-    auto threadIds =\n-        getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n-    Value threadIdM = threadIds[0];\n-\n-    Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n-    Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n-    SmallVector<Value> aOff(aNumPtr);\n-    for (int i = 0; i < aNumPtr; ++i) {\n-      aOff[i] = add(mul(offA0, strideA0), mul(offA1, strideA1));\n-    }\n-    auto elemTy = A.getType().cast<RankedTensorType>().getElementType();\n-\n-    Type ptrTy = ptr_ty(elemTy);\n-    SmallVector<Value> aPtrs(aNumPtr);\n-    for (int i = 0; i < aNumPtr; ++i)\n-      aPtrs[i] = gep(ptrTy, aSmem.base, aOff[i]);\n-\n-    SmallVector<Value> vas;\n-\n-    int mShapePerCTA = getShapePerCTAForMN(dLayout, true /*isM*/);\n-    int mSizePerThread = getSizePerThreadForMN(dLayout, true /*isM*/);\n-\n-    for (unsigned k = 0; k < K; ++k)\n-      for (unsigned m = 0; m < M; m += mShapePerCTA)\n-        for (unsigned mm = 0; mm < mSizePerThread; ++mm) {\n-          Value offset =\n-              add(mul(i32_val(m + mm), strideAM), mul(i32_val(k), strideAK));\n-          Value pa = gep(ptrTy, aPtrs[0], offset);\n-          Value va = load(pa);\n-          vas.emplace_back(va);\n-        }\n-\n-    return getStructFromValueTable(vas, rewriter, loc, elemTy);\n-  }\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n \n   Value loadB(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const {\n-    auto bTensorTy = B.getType().cast<RankedTensorType>();\n-    auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-    auto bShape = bTensorTy.getShape();\n-\n-    auto bOrder = bLayout.getOrder();\n-    auto order = dLayout.getOrder();\n-\n-    bool isBRow = bOrder[0] == 1;\n-\n-    auto bSmem = getSharedMemoryObjectFromStruct(loc, llB, rewriter);\n-    Value strideBN = bSmem.strides[1];\n-    Value strideBK = bSmem.strides[0];\n-    Value strideB0 = isBRow ? strideBN : strideBK;\n-    Value strideB1 = isBRow ? strideBK : strideBN;\n-    int bNumPtr = 8;\n-    int K = bShape[0];\n-    int N = bShape[1];\n-\n-    auto shapePerCTA = getShapePerCTA(dLayout);\n-    auto sizePerThread = getSizePerThread(dLayout);\n-\n-    Value _0 = i32_val(0);\n-\n-    Value nContig = i32_val(sizePerThread[order[0]]);\n-\n-    // threadId in blocked layout\n-    auto threadIds =\n-        getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n-    Value threadIdN = threadIds[1];\n-\n-    Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n-    Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n-    SmallVector<Value> bOff(bNumPtr);\n-    for (int i = 0; i < bNumPtr; ++i) {\n-      bOff[i] = add(mul(offB0, strideB0), mul(offB1, strideB1));\n-    }\n-    auto elemTy = B.getType().cast<RankedTensorType>().getElementType();\n-\n-    Type ptrTy = ptr_ty(elemTy);\n-    SmallVector<Value> bPtrs(bNumPtr);\n-    for (int i = 0; i < bNumPtr; ++i)\n-      bPtrs[i] = gep(ptrTy, bSmem.base, bOff[i]);\n-\n-    SmallVector<Value> vbs;\n-\n-    int nShapePerCTA = getShapePerCTAForMN(dLayout, false /*isM*/);\n-    int nSizePerThread = getSizePerThreadForMN(dLayout, false /*isM*/);\n-\n-    for (unsigned k = 0; k < K; ++k)\n-      for (unsigned n = 0; n < N; n += nShapePerCTA)\n-        for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n-          Value offset =\n-              add(mul(i32_val(n + nn), strideBN), mul(i32_val(k), strideBK));\n-          Value pb = gep(ptrTy, bPtrs[0], offset);\n-          Value vb = load(pb);\n-          vbs.emplace_back(vb);\n-        }\n-\n-    return getStructFromValueTable(vbs, rewriter, loc, elemTy);\n-  }\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n \n   ValueTable getValueTableFromStruct(Value val, int K, int n0, int shapePerCTA,\n                                      int sizePerThread,\n                                      ConversionPatternRewriter &rewriter,\n-                                     Location loc) const {\n-    ValueTable res;\n-    auto elems = getElementsFromStruct(loc, val, rewriter);\n-    int index = 0;\n-    for (unsigned k = 0; k < K; ++k) {\n-      for (unsigned m = 0; m < n0; m += shapePerCTA)\n-        for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n-          res[{m + mm, k}] = elems[index++];\n-        }\n-    }\n-    return res;\n-  }\n+                                     Location loc) const;\n \n   Value getStructFromValueTable(ArrayRef<Value> vals,\n                                 ConversionPatternRewriter &rewriter,\n-                                Location loc, Type elemTy) const {\n-    SmallVector<Type> elemTypes(vals.size(), elemTy);\n-    SmallVector<Value> elems;\n-    elems.reserve(vals.size());\n-    for (auto &val : vals) {\n-      elems.push_back(val);\n-    }\n-\n-    Type structTy = struct_ty(elemTypes);\n-    return getStructFromElements(loc, elems, rewriter, structTy);\n-  }\n+                                Location loc, Type elemTy) const;\n \n   // get number of elements per thread for $a or $b.\n   static int getNumElemsPerThread(ArrayRef<int64_t> shape,\n-                                  DotOperandEncodingAttr dotOpLayout) {\n-    auto blockedLayout = dotOpLayout.getParent().cast<BlockedEncodingAttr>();\n-    auto shapePerCTA = getShapePerCTA(blockedLayout);\n-    auto sizePerThread = getSizePerThread(blockedLayout);\n-\n-    // TODO[Superjomn]: we assume the k aixs is fixed for $a and $b here, fix it\n-    // if not.\n-    int K = dotOpLayout.getOpIdx() == 0 ? shape[1] : shape[0];\n-    int otherDim = dotOpLayout.getOpIdx() == 1 ? shape[1] : shape[0];\n-\n-    bool isM = dotOpLayout.getOpIdx() == 0;\n-    int shapePerCTAMN = getShapePerCTAForMN(blockedLayout, isM);\n-    int sizePerThreadMN = getSizePerThreadForMN(blockedLayout, isM);\n-    return K * std::max<int>(otherDim / shapePerCTAMN, 1) * sizePerThreadMN;\n-  }\n+                                  DotOperandEncodingAttr dotOpLayout);\n \n   // Get shapePerCTA for M or N axis.\n   static int getShapePerCTAForMN(BlockedEncodingAttr layout, bool isM) {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 73, "deletions": 26, "changes": 99, "file_content_changes": "@@ -1,3 +1,4 @@\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n@@ -10,36 +11,59 @@ using namespace mlir::triton;\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n+template <class T> SmallVector<unsigned, 4> argSort(const T &arr) {\n+  SmallVector<unsigned, 4> ret(arr.size());\n+  std::iota(ret.begin(), ret.end(), 0);\n+  std::sort(ret.begin(), ret.end(),\n+            [&](unsigned x, unsigned y) { return arr[x] > arr[y]; });\n+  return ret;\n+}\n+\n+typedef DenseMap<Value, std::function<Type(Type)>> LayoutMap;\n+\n struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n   Attribute getCoalescedEncoding(AxisInfoAnalysis &axisInfo, Value ptr,\n                                  int numWarps) {\n     auto origType = ptr.getType().cast<RankedTensorType>();\n     // Get the shape of the tensor.\n     size_t rank = origType.getRank();\n     AxisInfo info = axisInfo.lookupLatticeElement(ptr)->getValue();\n-    // Layout order in decreasing order of contiguity\n-    SmallVector<unsigned, 4> order(rank);\n-    std::iota(order.begin(), order.end(), 0);\n-    auto contiguity = info.getContiguity();\n-    std::sort(order.begin(), order.end(), [&](unsigned x, unsigned y) {\n-      return contiguity[x] > contiguity[y];\n-    });\n-\n+    // Get the contiguity order of `ptr`\n+    auto order = argSort(info.getContiguity());\n+    // The desired divisibility is the maximum divisibility\n+    // among all dependent pointers who have the same order as\n+    // `ptr`\n+    SetVector<Value> withSameOrder;\n+    withSameOrder.insert(ptr);\n+    if (ptr.getDefiningOp())\n+      for (Operation *op : mlir::getSlice(ptr.getDefiningOp())) {\n+        for (Value val : op->getResults()) {\n+          if (val.getType() != origType)\n+            continue;\n+          auto valInfo = axisInfo.lookupLatticeElement(val);\n+          auto currOrder = argSort(valInfo->getValue().getContiguity());\n+          if (order == currOrder)\n+            withSameOrder.insert(val);\n+        }\n+      }\n     int numElems = product(origType.getShape());\n     int numThreads = numWarps * 32;\n     int numElemsPerThread = std::max(numElems / numThreads, 1);\n-\n     // Thread tile size depends on memory alignment\n     SmallVector<unsigned, 4> sizePerThread(rank, 1);\n     unsigned elemNumBits = getPointeeBitWidth(origType);\n     unsigned elemNumBytes = std::max(elemNumBits / 8, 1u);\n-    unsigned maxMultipleBytes = info.getDivisibility(order[0]);\n-    unsigned maxMultiple = std::max(maxMultipleBytes / elemNumBytes, 1u);\n-    unsigned maxContig = info.getContiguity(order[0]);\n-    unsigned alignment = std::min(maxMultiple, maxContig);\n-    unsigned perThread = std::min(alignment, 128 / elemNumBits);\n+    unsigned perThread = 1;\n+    for (Value val : withSameOrder) {\n+      AxisInfo info = axisInfo.lookupLatticeElement(val)->getValue();\n+      unsigned maxMultipleBytes = info.getDivisibility(order[0]);\n+      unsigned maxMultiple = std::max(maxMultipleBytes / elemNumBytes, 1u);\n+      unsigned maxContig = info.getContiguity(order[0]);\n+      unsigned alignment = std::min(maxMultiple, maxContig);\n+      unsigned currPerThread = std::min(alignment, 128 / elemNumBits);\n+      perThread = std::max(perThread, currPerThread);\n+    }\n     sizePerThread[order[0]] = std::min<int>(perThread, numElemsPerThread);\n-\n     SmallVector<unsigned> dims(rank);\n     std::iota(dims.begin(), dims.end(), 0);\n     // create encoding\n@@ -59,16 +83,12 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n   }\n \n   template <class T>\n-  void coalesceOp(AxisInfoAnalysis &axisInfo, Operation *op, Value ptr,\n+  void coalesceOp(LayoutMap &layoutMap, Operation *op, Value ptr,\n                   OpBuilder builder) {\n     RankedTensorType ty = ptr.getType().template dyn_cast<RankedTensorType>();\n     if (!ty)\n       return;\n-    auto mod = op->getParentOfType<ModuleOp>();\n-    int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n-\n-    AxisInfo info = axisInfo.lookupLatticeElement(ptr)->getValue();\n-    auto convertType = getTypeConverter(axisInfo, ptr, numWarps);\n+    auto convertType = layoutMap.lookup(ptr);\n     // convert operands\n     SmallVector<Value, 4> newArgs;\n     for (auto v : op->getOperands()) {\n@@ -106,6 +126,33 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     AxisInfoAnalysis axisInfo(&getContext());\n     axisInfo.run(op);\n \n+    // For each i/o operation, we determine what layout\n+    // the pointers should have for best memory coalescing\n+    LayoutMap layoutMap;\n+    op->walk([&](Operation *curr) {\n+      Value ptr;\n+      if (auto op = dyn_cast<triton::LoadOp>(curr))\n+        ptr = op.ptr();\n+      if (auto op = dyn_cast<triton::AtomicRMWOp>(curr))\n+        ptr = op.ptr();\n+      if (auto op = dyn_cast<triton::AtomicCASOp>(curr))\n+        ptr = op.ptr();\n+      if (auto op = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr))\n+        ptr = op.src();\n+      if (auto op = dyn_cast<triton::StoreOp>(curr))\n+        ptr = op.ptr();\n+      if (!ptr)\n+        return;\n+      RankedTensorType ty = ptr.getType().template dyn_cast<RankedTensorType>();\n+      if (!ty || !ty.getElementType().isa<PointerType>())\n+        return;\n+      AxisInfo info = axisInfo.lookupLatticeElement(ptr)->getValue();\n+      auto mod = curr->getParentOfType<ModuleOp>();\n+      int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+      auto convertType = getTypeConverter(axisInfo, ptr, numWarps);\n+      layoutMap[ptr] = convertType;\n+    });\n+\n     // For each memory op that has a layout L1:\n     // 1. Create a coalesced memory layout L2 of the pointer operands\n     // 2. Convert all operands from layout L1 to layout L2\n@@ -116,24 +163,24 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     op->walk([&](Operation *curr) {\n       OpBuilder builder(curr);\n       if (auto load = dyn_cast<triton::LoadOp>(curr)) {\n-        coalesceOp<triton::LoadOp>(axisInfo, curr, load.ptr(), builder);\n+        coalesceOp<triton::LoadOp>(layoutMap, curr, load.ptr(), builder);\n         return;\n       }\n       if (auto op = dyn_cast<triton::AtomicRMWOp>(curr)) {\n-        coalesceOp<triton::AtomicRMWOp>(axisInfo, curr, op.ptr(), builder);\n+        coalesceOp<triton::AtomicRMWOp>(layoutMap, curr, op.ptr(), builder);\n         return;\n       }\n       if (auto op = dyn_cast<triton::AtomicCASOp>(curr)) {\n-        coalesceOp<triton::AtomicCASOp>(axisInfo, curr, op.ptr(), builder);\n+        coalesceOp<triton::AtomicCASOp>(layoutMap, curr, op.ptr(), builder);\n         return;\n       }\n       if (auto load = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr)) {\n-        coalesceOp<triton::gpu::InsertSliceAsyncOp>(axisInfo, curr, load.src(),\n+        coalesceOp<triton::gpu::InsertSliceAsyncOp>(layoutMap, curr, load.src(),\n                                                     builder);\n         return;\n       }\n       if (auto store = dyn_cast<triton::StoreOp>(curr)) {\n-        coalesceOp<triton::StoreOp>(axisInfo, curr, store.ptr(), builder);\n+        coalesceOp<triton::StoreOp>(layoutMap, curr, store.ptr(), builder);\n         return;\n       }\n     });"}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 7, "changes": 8, "file_content_changes": "@@ -200,12 +200,6 @@ def build_extension(self, ext):\n         subprocess.check_call([\"cmake\", \"--build\", \".\"] + build_args, cwd=self.build_temp)\n \n \n-package_data = {\n-    \"triton/ops\": [\"*.c\"],\n-    \"triton/ops/blocksparse\": [\"*.c\"],\n-    \"triton/language\": [\"*.bc\"],\n-}\n-\n download_and_copy_ptxas()\n \n setup(\n@@ -222,7 +216,7 @@ def build_extension(self, ext):\n         \"torch\",\n         \"lit\",\n     ],\n-    package_data={\"triton\": [\"third_party/*\"]},\n+    package_data={\"triton\": [\"third_party/**/*\"]},\n     include_package_data=True,\n     ext_modules=[CMakeExtension(\"triton\", \"triton/_C/\")],\n     cmdclass={\"build_ext\": CMakeBuild},"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1097,7 +1097,7 @@ def kernel(X, stride_xm, stride_xn,\n                                            [64, 128, 128, 4],\n                                            [32, 128, 64, 2],\n                                            [128, 128, 64, 2],\n-                                           [64, 128, 128, 4]]\n+                                           [64, 128, 128, 2]]\n                           for allow_tf32 in [True]\n                           for col_a in [True, False]\n                           for col_b in [True, False]"}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 11, "deletions": 5, "changes": 16, "file_content_changes": "@@ -78,18 +78,24 @@ func @div() {\n   %1 = arith.constant dense<1> : tensor<128xi32>\n   // CHECK-NEXT: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n   %2 = arith.divsi %0, %1 : tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n   %3 = arith.divui %1, %0 : tensor<128xi32>\n   // CHECK-NEXT: Contiguity: [1] ; Divisibility: [64] ; Constancy: [128] ; ConstantValue: [64]\n   %4 = arith.constant dense<64> : tensor<128xi32>\n   // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16777216] ; Constancy: [64] ; ConstantValue: [None]\n   %5 = arith.divsi %0, %4 : tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16777216] ; Constancy: [1] ; ConstantValue: [None]\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n   %6 = arith.divsi %4, %0 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [64] ; Constancy: [128] ; ConstantValue: [64]\n+  %7 = arith.divsi %4, %1 : tensor<128xi32>\n   // CHECK-NEXT: Contiguity: [1] ; Divisibility: [2] ; Constancy: [128] ; ConstantValue: [66]\n-  %7 = arith.constant dense<66> : tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [536870912] ; Constancy: [2] ; ConstantValue: [None]\n-  %8 = arith.divui %0, %7 : tensor<128xi32>\n+  %8 = arith.constant dense<66> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [2] ; ConstantValue: [None]\n+  %9 = arith.divui %0, %8 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [8192] ; Constancy: [1] ; ConstantValue: [None]\n+  %10 = tt.make_range {end = 8320 : i32, start = 8192 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [64] ; ConstantValue: [None]\n+  %11 = arith.divsi %10, %4 : tensor<128xi32>\n   return \n }\n "}]