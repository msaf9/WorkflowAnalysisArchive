[{"filename": "include/triton/Target/LLVMIR/LLVMIRTranslation.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -2,6 +2,7 @@\n #define TRITON_TARGET_LLVMIRTRANSLATION_H\n #include \"llvm/ADT/StringRef.h\"\n #include <memory>\n+#include <string>\n #include <vector>\n \n namespace llvm {"}, {"filename": "include/triton/Target/PTX/PTXTranslation.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,7 +1,6 @@\n #ifndef TRITON_TARGET_PTXTRANSLATION_H\n #define TRITON_TARGET_PTXTRANSLATION_H\n \n-#include <memory>\n #include <string>\n \n namespace llvm {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 135, "deletions": 66, "changes": 201, "file_content_changes": "@@ -99,6 +99,7 @@ void llPrintf(StringRef msg, ValueRange args,\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n #define urem(...) rewriter.create<LLVM::URemOp>(loc, __VA_ARGS__)\n #define add(...) rewriter.create<LLVM::AddOp>(loc, __VA_ARGS__)\n+#define sub(...) rewriter.create<LLVM::SubOp>(loc, __VA_ARGS__)\n #define fadd(...) rewriter.create<LLVM::FAddOp>(loc, __VA_ARGS__)\n #define mul(...) rewriter.create<LLVM::MulOp>(loc, __VA_ARGS__)\n #define smax(...) rewriter.create<LLVM::SMaxOp>(loc, __VA_ARGS__)\n@@ -441,34 +442,71 @@ struct SharedMemoryObject {\n   // if we want to support more optimizations.\n   SmallVector<Value>\n       strides; // i32 int. The strides of the shared memory object.\n+  SmallVector<Value> offsets; // i32 int. The offsets of the shared memory\n+                              // objects from the originally allocated object.\n \n-  SharedMemoryObject(Value base, ArrayRef<Value> strides)\n-      : base(base), strides(strides.begin(), strides.end()) {}\n+  SharedMemoryObject(Value base, ArrayRef<Value> strides,\n+                     ArrayRef<Value> offsets)\n+      : base(base), strides(strides.begin(), strides.end()),\n+        offsets(offsets.begin(), offsets.end()) {}\n \n+  SharedMemoryObject(Value base, ArrayRef<int64_t> shape,\n+                     ArrayRef<unsigned> order, Location loc,\n+                     ConversionPatternRewriter &rewriter)\n+      : base(base) {\n+    auto rank = shape.size();\n+    auto stride = 1;\n+    strides.resize(rank);\n+    for (auto idx : order) {\n+      strides[idx] = i32_val(stride);\n+      offsets.emplace_back(i32_val(0));\n+      stride *= shape[idx];\n+    }\n+  }\n+\n+  // XXX(Keren): a special allocator for 3d tensors. It's a workaround for\n+  // now since we don't have a correct way to encoding 3d tensors in the\n+  // pipeline pass.\n   SharedMemoryObject(Value base, ArrayRef<int64_t> shape, Location loc,\n                      ConversionPatternRewriter &rewriter)\n       : base(base) {\n     auto stride = 1;\n     for (auto dim : llvm::reverse(shape)) {\n-      this->strides.emplace_back(i32_val(stride));\n+      strides.emplace_back(i32_val(stride));\n+      offsets.emplace_back(i32_val(0));\n       stride *= dim;\n     }\n-    this->strides = llvm::to_vector<4>(llvm::reverse(this->strides));\n+    strides = llvm::to_vector<4>(llvm::reverse(strides));\n   }\n \n   SmallVector<Value> getElems() const {\n     SmallVector<Value> elems;\n     elems.push_back(base);\n     elems.append(strides.begin(), strides.end());\n+    elems.append(offsets.begin(), offsets.end());\n     return elems;\n   }\n \n   SmallVector<Type> getTypes() const {\n     SmallVector<Type> types;\n     types.push_back(base.getType());\n     types.append(strides.size(), IntegerType::get(base.getContext(), 32));\n+    types.append(offsets.size(), IntegerType::get(base.getContext(), 32));\n     return types;\n   }\n+\n+  Value getCSwizzleOffset(int order) const {\n+    assert(order >= 0 && order < strides.size());\n+    return offsets[order];\n+  }\n+\n+  Value getBaseBeforeSwizzle(int order, Location loc,\n+                             ConversionPatternRewriter &rewriter) const {\n+    Value cSwizzleOffset = getCSwizzleOffset(order);\n+    Value offset = sub(i32_val(0), cSwizzleOffset);\n+    Type type = base.getType();\n+    return gep(type, base, offset);\n+  }\n };\n \n struct ConvertTritonGPUOpToLLVMPatternBase {\n@@ -493,8 +531,11 @@ struct ConvertTritonGPUOpToLLVMPatternBase {\n   getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n                                   ConversionPatternRewriter &rewriter) {\n     auto elems = getElementsFromStruct(loc, llvmStruct, rewriter);\n-    return SharedMemoryObject(/*base=*/elems[0],\n-                              /*strides=*/{elems.begin() + 1, elems.end()});\n+    auto rank = (elems.size() - 1) / 2;\n+    return SharedMemoryObject(\n+        /*base=*/elems[0],\n+        /*strides=*/{elems.begin() + 1, elems.begin() + 1 + rank},\n+        /*offsets=*/{elems.begin() + 1 + rank, elems.end()});\n   }\n \n   static Value\n@@ -2238,31 +2279,34 @@ struct ExtractSliceOpConversion\n     // Triton support either static and dynamic offsets\n     auto smemObj =\n         getSharedMemoryObjectFromStruct(loc, adaptor.source(), rewriter);\n+    SmallVector<Value, 4> opOffsetVals;\n     SmallVector<Value, 4> offsetVals;\n     auto mixedOffsets = op.getMixedOffsets();\n     for (auto i = 0; i < mixedOffsets.size(); ++i) {\n       if (op.isDynamicOffset(i))\n-        offsetVals.emplace_back(adaptor.offsets()[i]);\n+        opOffsetVals.emplace_back(adaptor.offsets()[i]);\n       else\n-        offsetVals.emplace_back(i32_val(op.getStaticOffset(i)));\n+        opOffsetVals.emplace_back(i32_val(op.getStaticOffset(i)));\n+      offsetVals.emplace_back(add(smemObj.offsets[i], opOffsetVals[i]));\n     }\n     // Compute the offset based on the original strides of the shared memory\n     // object\n-    auto offset = dot(rewriter, loc, offsetVals, smemObj.strides);\n+    auto offset = dot(rewriter, loc, opOffsetVals, smemObj.strides);\n     // newShape = rank_reduce(shape)\n     // Triton only supports static tensor sizes\n     SmallVector<Value, 4> strideVals;\n-    auto staticSizes = op.static_sizes();\n     for (auto i = 0; i < op.static_sizes().size(); ++i) {\n-      if (op.getStaticSize(i) != 1) {\n+      if (op.getStaticSize(i) == 1) {\n+        offsetVals.erase(offsetVals.begin() + i);\n+      } else {\n         strideVals.emplace_back(smemObj.strides[i]);\n       }\n     }\n     auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n     auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n     auto resTy = op.getType().dyn_cast<RankedTensorType>();\n-    smemObj =\n-        SharedMemoryObject(gep(elemPtrTy, smemObj.base, offset), strideVals);\n+    smemObj = SharedMemoryObject(gep(elemPtrTy, smemObj.base, offset),\n+                                 strideVals, offsetVals);\n     auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n     rewriter.replaceOp(op, retVal);\n     return success();\n@@ -3128,7 +3172,7 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n   Value smemBase = getSharedMemoryBase(loc, rewriter, dst);\n   auto elemPtrTy = ptr_ty(getTypeConverter()->convertType(elemTy), 3);\n   smemBase = bitcast(smemBase, elemPtrTy);\n-  auto smemObj = SharedMemoryObject(smemBase, dstShape, loc, rewriter);\n+  auto smemObj = SharedMemoryObject(smemBase, dstShape, outOrd, loc, rewriter);\n   auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n   unsigned numWordsEachRep = product<unsigned>(wordsInEachRep);\n   SmallVector<Value> wordVecs(numWordsEachRep);\n@@ -3218,8 +3262,8 @@ class MMA16816SmemLoader {\n     cMatShape = matShape[order[0]];\n     sMatShape = matShape[order[1]];\n \n-    cTileStride = smemStrides[order[0]];\n-    sTileStride = smemStrides[order[1]];\n+    cStride = smemStrides[1];\n+    sStride = smemStrides[0];\n \n     // rule: k must be the fast-changing axis.\n     needTrans = kOrder != order[0];\n@@ -3228,17 +3272,16 @@ class MMA16816SmemLoader {\n     if (canUseLdmatrix) {\n       // Each CTA, the warps is arranged as [1xwpt] if not transposed,\n       // otherwise [wptx1], and each warp will perform a mma.\n-      numPtr =\n+      numPtrs =\n           tileShape[order[0]] / (needTrans ? wpt : 1) / instrShape[order[0]];\n     } else {\n-      numPtr = tileShape[order[0]] / wpt / matShape[order[0]];\n+      numPtrs = tileShape[order[0]] / wpt / matShape[order[0]];\n     }\n-\n-    numPtr = std::max<int>(numPtr, 2);\n+    numPtrs = std::max<int>(numPtrs, 2);\n \n     // Special rule for i8/u8, 4 ptrs for each matrix\n     if (!canUseLdmatrix && elemBytes == 1)\n-      numPtr *= 4;\n+      numPtrs *= 4;\n \n     int loadStrideInMat[2];\n     loadStrideInMat[kOrder] =\n@@ -3257,24 +3300,26 @@ class MMA16816SmemLoader {\n \n   // lane = thread % 32\n   // warpOff = (thread/32) % wpt(0)\n-  llvm::SmallVector<Value> computeOffsets(Value warpOff, Value lane) {\n+  llvm::SmallVector<Value> computeOffsets(Value warpOff, Value lane,\n+                                          Value cSwizzleOffset) {\n     if (canUseLdmatrix)\n-      return computeLdmatrixMatOffs(warpOff, lane);\n+      return computeLdmatrixMatOffs(warpOff, lane, cSwizzleOffset);\n     else if (elemBytes == 4 && needTrans)\n-      return computeB32MatOffs(warpOff, lane);\n+      return computeB32MatOffs(warpOff, lane, cSwizzleOffset);\n     else if (elemBytes == 1 && needTrans)\n-      return computeB8MatOffs(warpOff, lane);\n+      return computeB8MatOffs(warpOff, lane, cSwizzleOffset);\n     else\n       llvm::report_fatal_error(\"Invalid smem load config\");\n \n     return {};\n   }\n \n-  int getNumPtr() const { return numPtr; }\n+  int getNumPtrs() const { return numPtrs; }\n \n   // Compute the offset to the matrix this thread(indexed by warpOff and lane)\n   // mapped to.\n-  SmallVector<Value> computeLdmatrixMatOffs(Value warpId, Value lane) {\n+  SmallVector<Value> computeLdmatrixMatOffs(Value warpId, Value lane,\n+                                            Value cSwizzleOffset) {\n     // 4x4 matrices\n     Value c = urem(lane, i32_val(8));\n     Value s = udiv(lane, i32_val(8)); // sub-warp-id\n@@ -3312,31 +3357,34 @@ class MMA16816SmemLoader {\n     // Physical offset (before swizzling)\n     Value cMatOff = matOff[order[0]];\n     Value sMatOff = matOff[order[1]];\n+    Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n+    cMatOff = add(cMatOff, cSwizzleMatOff);\n \n     // row offset inside a matrix, each matrix has 8 rows.\n     Value sOffInMat = c;\n \n-    SmallVector<Value> offs(numPtr);\n+    SmallVector<Value> offs(numPtrs);\n     Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n     Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n-    for (int i = 0; i < numPtr; ++i) {\n+    for (int i = 0; i < numPtrs; ++i) {\n       Value cMatOffI = add(cMatOff, i32_val(i * pLoadStrideInMat));\n       cMatOffI = xor_(cMatOffI, phase);\n-      offs[i] = add(mul(cMatOffI, i32_val(cMatShape)), mul(sOff, sTileStride));\n+      offs[i] = add(mul(cMatOffI, i32_val(cMatShape)), mul(sOff, sStride));\n     }\n \n     return offs;\n   }\n \n   // Compute 32-bit matrix offsets.\n-  SmallVector<Value> computeB32MatOffs(Value warpOff, Value lane) {\n+  SmallVector<Value> computeB32MatOffs(Value warpOff, Value lane,\n+                                       Value cSwizzleOffset) {\n     assert(needTrans && \"Only used in transpose mode.\");\n     // Load tf32 matrices with lds32\n     Value cOffInMat = udiv(lane, i32_val(4));\n     Value sOffInMat = urem(lane, i32_val(4));\n \n     Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n-    SmallVector<Value> offs(numPtr);\n+    SmallVector<Value> offs(numPtrs);\n \n     for (int mat = 0; mat < 4; ++mat) { // Load 4 mats each time\n       int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n@@ -3348,30 +3396,34 @@ class MMA16816SmemLoader {\n \n       Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n                           mul(nkMatArr, i32_val(matArrStride)));\n+      Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n+      cMatOff = add(cMatOff, cSwizzleMatOff);\n+\n       Value sMatOff = kMatArr;\n       Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n       // FIXME: (kOrder == 1?) is really dirty hack\n-      for (int i = 0; i < numPtr / 2; ++i) {\n+      for (int i = 0; i < numPtrs / 2; ++i) {\n         Value cMatOffI =\n             add(cMatOff, i32_val(i * pLoadStrideInMat * (kOrder == 1 ? 1 : 2)));\n         cMatOffI = xor_(cMatOffI, phase);\n         Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n         cOff = urem(cOff, i32_val(tileShape[order[0]]));\n         sOff = urem(sOff, i32_val(tileShape[order[1]]));\n-        offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, sTileStride));\n+        offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, sStride));\n       }\n     }\n     return offs;\n   }\n \n   // compute 8-bit matrix offset.\n-  SmallVector<Value> computeB8MatOffs(Value warpOff, Value lane) {\n+  SmallVector<Value> computeB8MatOffs(Value warpOff, Value lane,\n+                                      Value cSwizzleOffset) {\n     assert(needTrans && \"Only used in transpose mode.\");\n     Value cOffInMat = udiv(lane, i32_val(4));\n     Value sOffInMat =\n         mul(urem(lane, i32_val(4)), i32_val(4)); // each thread load 4 cols\n \n-    SmallVector<Value> offs(numPtr);\n+    SmallVector<Value> offs(numPtrs);\n     for (int mat = 0; mat < 4; ++mat) {\n       int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n       int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n@@ -3384,7 +3436,7 @@ class MMA16816SmemLoader {\n                           mul(nkMatArr, i32_val(matArrStride)));\n       Value sMatOff = kMatArr;\n \n-      for (int loadx4Off = 0; loadx4Off < numPtr / 8; ++loadx4Off) {\n+      for (int loadx4Off = 0; loadx4Off < numPtrs / 8; ++loadx4Off) {\n         for (int elemOff = 0; elemOff < 4; ++elemOff) {\n           int ptrOff = loadx4Off * 8 + nkMatArrInt * 4 + elemOff;\n           Value cMatOffI = add(cMatOff, i32_val(loadx4Off * pLoadStrideInMat *\n@@ -3398,7 +3450,7 @@ class MMA16816SmemLoader {\n           // To prevent out-of-bound access when tile is too small.\n           cOff = urem(cOff, i32_val(tileShape[order[0]]));\n           sOff = urem(sOff, i32_val(tileShape[order[1]]));\n-          offs[ptrOff] = add(cOff, mul(sOff, sTileStride));\n+          offs[ptrOff] = add(cOff, mul(sOff, sStride));\n         }\n       }\n     }\n@@ -3433,7 +3485,7 @@ class MMA16816SmemLoader {\n \n     if (canUseLdmatrix) {\n       Value sOffset =\n-          mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sTileStride);\n+          mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sStride);\n       Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n \n       PTXBuilder builder;\n@@ -3467,10 +3519,10 @@ class MMA16816SmemLoader {\n       Value ptr2 = getPtr(ptrIdx + 1);\n       assert(sMatStride == 1);\n       int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n-      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sTileStride);\n+      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n       int sOffsetArrElem = sMatStride * sMatShape;\n       Value sOffsetArrElemVal =\n-          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sTileStride));\n+          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n \n       Value elems[4];\n       Type elemTy = type::f32Ty(ctx);\n@@ -3510,10 +3562,10 @@ class MMA16816SmemLoader {\n \n       assert(sMatStride == 1);\n       int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n-      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sTileStride);\n+      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n       int sOffsetArrElem = 1 * (sMatStride * sMatShape);\n       Value sOffsetArrElemVal =\n-          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sTileStride));\n+          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n \n       std::array<Value, 4> i8v4Elems;\n       std::array<Value, 4> i32Elems;\n@@ -3581,13 +3633,13 @@ class MMA16816SmemLoader {\n   int cMatShape;\n   int sMatShape;\n \n-  Value cTileStride;\n-  Value sTileStride;\n+  Value cStride;\n+  Value sStride;\n \n   bool needTrans;\n   bool canUseLdmatrix;\n \n-  int numPtr;\n+  int numPtrs;\n \n   int pLoadStrideInMat;\n   int sMatStride;\n@@ -4224,7 +4276,8 @@ struct MMA16816ConversionHelper {\n       loadFn = getLoadMatrixFn(\n           tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n           1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShape*/,\n-          {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/);\n+          {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/,\n+          true /*isA*/);\n     } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n       // load from registers, used in gemm fuse\n       // TODO(Superjomn) Port the logic.\n@@ -4255,7 +4308,8 @@ struct MMA16816ConversionHelper {\n     auto loadFn = getLoadMatrixFn(\n         tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n         0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShape*/,\n-        {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/);\n+        {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/,\n+        false /*isA*/);\n \n     for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n       for (int k = 0; k < numRepK; ++k)\n@@ -4369,7 +4423,7 @@ struct MMA16816ConversionHelper {\n   getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n                   MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder,\n                   ArrayRef<int> instrShape, ArrayRef<int> matShape,\n-                  Value warpId, ValueTable &vals) const {\n+                  Value warpId, ValueTable &vals, bool isA) const {\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n     // We assumes that the input operand of Dot should be from shared layout.\n     // TODO(Superjomn) Consider other layouts if needed later.\n@@ -4379,8 +4433,6 @@ struct MMA16816ConversionHelper {\n     const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n     auto order = sharedLayout.getOrder();\n \n-    bool needTrans = kOrder != order[0];\n-\n     // the original register_lds2, but discard the prefetch logic.\n     auto ld2 = [](ValueTable &vals, int mn, int k, Value val) {\n       vals[{mn, k}] = val;\n@@ -4392,21 +4444,24 @@ struct MMA16816ConversionHelper {\n           wpt, sharedLayout.getOrder(), kOrder, smemObj.strides,\n           tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n           maxPhase, elemBytes, rewriter, typeConverter, loc);\n-      SmallVector<Value> offs = loader.computeOffsets(warpId, lane);\n-      const int numPtrs = loader.getNumPtr();\n+      Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+      SmallVector<Value> offs =\n+          loader.computeOffsets(warpId, lane, cSwizzleOffset);\n+      const int numPtrs = loader.getNumPtrs();\n       SmallVector<Value> ptrs(numPtrs);\n \n+      Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n       Type smemPtrTy = helper.getShemPtrTy();\n       for (int i = 0; i < numPtrs; ++i) {\n-        ptrs[i] = bitcast(gep(smemPtrTy, smemObj.base, ValueRange({offs[i]})),\n-                          smemPtrTy);\n+        ptrs[i] =\n+            bitcast(gep(smemPtrTy, smemBase, ValueRange({offs[i]})), smemPtrTy);\n       }\n \n       auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n           (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n           ptrs, helper.getMatType(), helper.getShemPtrTy());\n \n-      if (!needTrans) {\n+      if (isA) {\n         ld2(vals, a, b, ha0);\n         ld2(vals, a + 1, b, ha1);\n         ld2(vals, a, b + 1, ha2);\n@@ -4432,7 +4487,7 @@ struct MMA16816ConversionHelper {\n   //  ...\n   //  (2,0), (2,1), (3,0), (3,1), # i=1, j=0\n   //  (2,2), (2,3), (3,2), (3,3), # i=1, j=1\n-  //  (2,4), (2,5), (2,4), (2,5), # i=1, j=2\n+  //  (2,4), (2,5), (3,4), (3,5), # i=1, j=2\n   //  ...\n   // ]\n   // i \\in [0, n0) and j \\in [0, n1)\n@@ -4811,15 +4866,13 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n Value DotOpMmaV1ConversionHelper::loadA(\n     Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n     ConversionPatternRewriter &rewriter) const {\n-  // smem\n-  Value smem = smemObj.base;\n-  auto strides = smemObj.strides;\n \n   auto *ctx = rewriter.getContext();\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n   auto shape = tensorTy.getShape();\n   auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n   auto order = sharedLayout.getOrder();\n+  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n \n   bool isARow = order[0] != 0;\n   bool isAVec4 = !isARow && shape[order[0]] <= 16; // fp16*4 = 16bytes\n@@ -4834,6 +4887,7 @@ Value DotOpMmaV1ConversionHelper::loadA(\n \n   int vecA = sharedLayout.getVec();\n \n+  auto strides = smemObj.strides;\n   Value strideAM = isARow ? strides[0] : i32_val(1);\n   Value strideAK = isARow ? i32_val(1) : strides[1];\n   Value strideA0 = isARow ? strideAK : strideAM;\n@@ -4856,8 +4910,8 @@ Value DotOpMmaV1ConversionHelper::loadA(\n   Value offA0 = isARow ? offsetAK : offsetAM;\n   Value offA1 = isARow ? offsetAM : offsetAK;\n   Value phaseA = urem(udiv(offA1, i32_val(perPhaseA)), i32_val(maxPhaseA));\n+  offA0 = add(offA0, cSwizzleOffset);\n   SmallVector<Value> offA(numPtrA);\n-\n   for (int i = 0; i < numPtrA; i++) {\n     Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));\n     offA0I = udiv(offA0I, i32_val(vecA));\n@@ -4875,6 +4929,7 @@ Value DotOpMmaV1ConversionHelper::loadA(\n   SmallVector<Value> ptrA(numPtrA);\n \n   std::map<std::pair<int, int>, std::pair<Value, Value>> has;\n+  auto smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n   for (int i = 0; i < numPtrA; i++)\n     ptrA[i] = gep(ptr_ty(f16_ty), smem, offA[i]);\n \n@@ -4971,6 +5026,8 @@ Value DotOpMmaV1ConversionHelper::loadB(\n   Value offB0 = isBRow ? offsetBN : offsetBK;\n   Value offB1 = isBRow ? offsetBK : offsetBN;\n   Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n+  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+  offB0 = add(offB0, cSwizzleOffset);\n   SmallVector<Value> offB(numPtrB);\n   for (int i = 0; i < numPtrB; ++i) {\n     Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));\n@@ -5480,7 +5537,8 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n       types.push_back(ptrType);\n       // shape dims\n       auto rank = type.getRank();\n-      for (auto i = 0; i < rank; i++) {\n+      // offsets + strides\n+      for (auto i = 0; i < rank * 2; i++) {\n         types.push_back(IntegerType::get(ctx, 32));\n       }\n       return LLVM::LLVMStructType::getLiteral(ctx, types);\n@@ -5921,11 +5979,13 @@ struct AtomicRMWOpConversion\n \n     Value llPtr = adaptor.ptr();\n     Value llVal = adaptor.val();\n+    Value llMask = adaptor.mask();\n \n     auto valElements = getElementsFromStruct(loc, llVal, rewriter);\n     auto ptrElements = getElementsFromStruct(loc, llPtr, rewriter);\n+    auto maskElements = getElementsFromStruct(loc, llMask, rewriter);\n \n-    // TODO[dongdongl]: Support mask and scalar\n+    // TODO[dongdongl]: Support scalar\n \n     auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n     if (!valueTy)\n@@ -5940,6 +6000,14 @@ struct AtomicRMWOpConversion\n \n     auto vecTy = vec_ty(valueElemTy, vec);\n     auto elemsPerThread = getElemsPerThread(val.getType());\n+    // mask\n+    Value mask = int_val(1, 1);\n+    auto shape = valueTy.getShape();\n+    auto numElements = product(shape);\n+    auto tid = tid_val();\n+    mask = and_(mask, icmp_slt(mul(tid, i32_val(elemsPerThread)),\n+                               i32_val(numElements)));\n+\n     SmallVector<Value> resultVals(elemsPerThread);\n     for (size_t i = 0; i < elemsPerThread; i += vec) {\n       Value rmwVal = undef(vecTy);\n@@ -5949,6 +6017,8 @@ struct AtomicRMWOpConversion\n         rmwVal = insert_element(vecTy, rmwVal, valElements[i + ii], iiVal);\n       }\n       Value rmwPtr = ptrElements[i];\n+      Value rmwMask = maskElements[i];\n+      rmwMask = and_(rmwMask, mask);\n       std::string sTy;\n       PTXBuilder ptxBuilder;\n \n@@ -5996,9 +6066,8 @@ struct AtomicRMWOpConversion\n         return failure();\n       }\n       atom.o(rmwOp).o(sTy);\n-      //TODO:[dongdongl] actual mask support\n-      Value pred = int_val(1, 1);\n-      atom(dstOpr, ptrOpr, valOpr).predicate(pred);\n+      atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n+\n       auto ret = ptxBuilder.launch(rewriter, loc, valueElemTy);\n       for (int ii = 0; ii < vec; ++ii) {\n         resultVals[i * vec + ii] ="}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -126,7 +126,7 @@ namespace triton {\n \n //-- FpToFpOp --\n bool FpToFpOp::areCastCompatible(::mlir::TypeRange inputs,\n-                               ::mlir::TypeRange outputs) {\n+                                 ::mlir::TypeRange outputs) {\n   if (inputs.size() != 1 || outputs.size() != 1)\n     return false;\n   auto srcEltType = inputs.front();\n@@ -143,8 +143,8 @@ bool FpToFpOp::areCastCompatible(::mlir::TypeRange inputs,\n     std::swap(srcEltType, dstEltType);\n   if (!srcEltType.dyn_cast<mlir::triton::Float8Type>())\n     return false;\n-  return dstEltType.isF16() || dstEltType.isBF16() ||\n-         dstEltType.isF32() || dstEltType.isF64();\n+  return dstEltType.isF16() || dstEltType.isBF16() || dstEltType.isF32() ||\n+         dstEltType.isF64();\n }\n \n //-- StoreOp --"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -33,9 +33,9 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     SmallVector<unsigned, 4> sizePerThread(rank, 1);\n     PointerType ptrType = origType.getElementType().cast<PointerType>();\n     auto pointeeType = ptrType.getPointeeType();\n-    unsigned numBits =\n-        pointeeType.isa<triton::Float8Type>() ?\n-        8 : pointeeType.getIntOrFloatBitWidth();\n+    unsigned numBits = pointeeType.isa<triton::Float8Type>()\n+                           ? 8\n+                           : pointeeType.getIntOrFloatBitWidth();\n     unsigned maxMultiple = info.getDivisibility(order[0]);\n     unsigned maxContig = info.getContiguity(order[0]);\n     unsigned alignment = std::min(maxMultiple, maxContig);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 14, "deletions": 8, "changes": 22, "file_content_changes": "@@ -78,8 +78,6 @@ class SimplifyConversion : public mlir::RewritePattern {\n     if (!llvm::isa<triton::gpu::ConvertLayoutOp>(op))\n       return mlir::failure();\n     auto convert = llvm::cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto srcType = convert.getOperand().getType().cast<RankedTensorType>();\n-    auto dstType = convert.getType().cast<RankedTensorType>();\n     // we don't handle conversions to DotOperandEncodingAttr\n     // this is a heuristics to accommodate fused attention\n     // if (dstType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n@@ -96,13 +94,19 @@ class SimplifyConversion : public mlir::RewritePattern {\n     // cvt(alloc_tensor(x), type2) -> alloc_tensor(x, type2)\n     auto alloc_tensor = dyn_cast<triton::gpu::AllocTensorOp>(arg);\n     if (alloc_tensor) {\n+      if (!isSharedEncoding(op->getResult(0))) {\n+        return mlir::failure();\n+      }\n       rewriter.replaceOpWithNewOp<triton::gpu::AllocTensorOp>(\n           op, op->getResult(0).getType());\n       return mlir::success();\n     }\n     // cvt(insert_slice(x), type2) -> insert_slice(cvt(x, type2))\n     auto insert_slice = dyn_cast<triton::gpu::InsertSliceAsyncOp>(arg);\n     if (insert_slice) {\n+      if (!isSharedEncoding(op->getResult(0))) {\n+        return mlir::failure();\n+      }\n       auto newType = op->getResult(0).getType().cast<RankedTensorType>();\n       // Ensure that the new insert_slice op is placed in the same place as the\n       // old insert_slice op. Otherwise, the new insert_slice op may be placed\n@@ -121,6 +125,9 @@ class SimplifyConversion : public mlir::RewritePattern {\n     // cvt(extract_slice(x), type2) -> extract_slice(cvt(x, type2))\n     auto extract_slice = dyn_cast<tensor::ExtractSliceOp>(arg);\n     if (extract_slice) {\n+      if (!isSharedEncoding(op->getResult(0))) {\n+        return mlir::failure();\n+      }\n       auto origType = extract_slice.source().getType().cast<RankedTensorType>();\n       auto newType = RankedTensorType::get(\n           origType.getShape(), origType.getElementType(),\n@@ -144,16 +151,15 @@ class SimplifyConversion : public mlir::RewritePattern {\n       return mlir::success();\n     }\n \n-    // cvt(type2, x)\n+    // cvt(cvt(x, type1), type2) -> cvt(x, type2)\n     if (llvm::isa<triton::gpu::ConvertLayoutOp>(arg)) {\n-      auto argType = arg->getOperand(0).getType().cast<RankedTensorType>();\n       if (arg->getOperand(0).getDefiningOp() &&\n-          !argType.getEncoding().isa<triton::gpu::SharedEncodingAttr>() &&\n-          srcType.getEncoding().isa<triton::gpu::SharedEncodingAttr>() &&\n-          !dstType.getEncoding().isa<triton::gpu::SharedEncodingAttr>()) {\n-\n+          !isSharedEncoding(arg->getOperand(0)) &&\n+          isSharedEncoding(convert.getOperand()) &&\n+          !isSharedEncoding(convert.getResult())) {\n         return mlir::failure();\n       }\n+      auto srcType = convert.getOperand().getType().cast<RankedTensorType>();\n       auto srcShared =\n           srcType.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n       if (srcShared && srcShared.getVec() > 1)"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 12, "deletions": 9, "changes": 21, "file_content_changes": "@@ -27,6 +27,7 @@\n //===----------------------------------------------------------------------===//\n \n #include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n \n@@ -60,7 +61,7 @@ class Prefetcher {\n \n   LogicalResult isForOpOperand(Value v);\n \n-  Value generatePrefetch(Value v, unsigned opIdx, bool isPrefetch,\n+  Value generatePrefetch(Value v, unsigned opIdx, bool isPrologue,\n                          Attribute dotEncoding, OpBuilder &builder,\n                          llvm::Optional<int64_t> offsetK = llvm::None,\n                          llvm::Optional<int64_t> shapeK = llvm::None);\n@@ -79,7 +80,7 @@ class Prefetcher {\n   scf::ForOp createNewForOp();\n };\n \n-Value Prefetcher::generatePrefetch(Value v, unsigned opIdx, bool isPrefetch,\n+Value Prefetcher::generatePrefetch(Value v, unsigned opIdx, bool isPrologue,\n                                    Attribute dotEncoding, OpBuilder &builder,\n                                    llvm::Optional<int64_t> offsetK,\n                                    llvm::Optional<int64_t> shapeK) {\n@@ -94,8 +95,8 @@ Value Prefetcher::generatePrefetch(Value v, unsigned opIdx, bool isPrefetch,\n   // k => (prefetchWidth, k - prefetchWidth)\n   int64_t kIdx = opIdx == 0 ? 1 : 0;\n \n-  offset[kIdx] = isPrefetch ? 0 : prefetchWidth;\n-  shape[kIdx] = isPrefetch ? prefetchWidth : (shape[kIdx] - prefetchWidth);\n+  offset[kIdx] = isPrologue ? 0 : prefetchWidth;\n+  shape[kIdx] = isPrologue ? prefetchWidth : (shape[kIdx] - prefetchWidth);\n \n   if (shapeK)\n     shape[kIdx] = *shapeK;\n@@ -132,9 +133,9 @@ LogicalResult Prefetcher::initialize() {\n \n   // returns source of cvt\n   auto getPrefetchSrc = [](Value v) -> Value {\n-    // TODO: Check if the layout of src is SharedEncodingAttr\n     if (auto cvt = v.getDefiningOp<triton::gpu::ConvertLayoutOp>())\n-      return cvt.src();\n+      if (isSharedEncoding(cvt.getOperand()))\n+        return cvt.src();\n     return Value();\n   };\n \n@@ -152,6 +153,10 @@ LogicalResult Prefetcher::initialize() {\n   };\n \n   for (triton::DotOp dot : dotsInFor) {\n+    auto kSize = dot.a().getType().cast<RankedTensorType>().getShape()[1];\n+    // Skip prefetching if kSize is less than prefetchWidth\n+    if (kSize < prefetchWidth)\n+      continue;\n     Value aSmem = getPrefetchSrc(dot.a());\n     Value bSmem = getPrefetchSrc(dot.b());\n     if (aSmem && bSmem) {\n@@ -217,7 +222,7 @@ scf::ForOp Prefetcher::createNewForOp() {\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n \n   for (Operation &op : forOp.getBody()->without_terminator()) {\n-    Operation *newOp = nullptr;\n+    Operation *newOp = builder.clone(op, mapping);\n     auto dot = dyn_cast<triton::DotOp>(&op);\n     if (dots.contains(dot)) {\n       Attribute dotEncoding =\n@@ -252,8 +257,6 @@ scf::ForOp Prefetcher::createNewForOp() {\n         kOff += kShape;\n         kRem -= kShape;\n       }\n-    } else {\n-      newOp = builder.clone(op, mapping);\n     }\n     // update mapping of results\n     for (unsigned dstIdx : llvm::seq(unsigned(0), op.getNumResults()))"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -3,11 +3,9 @@\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/ExecutionEngine/ExecutionEngine.h\"\n #include \"mlir/ExecutionEngine/OptUtils.h\"\n-#include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/Dialect.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Pass/PassManager.h\"\n-#include \"mlir/Support/LogicalResult.h\"\n #include \"mlir/Target/LLVMIR/Dialect/LLVMIR/LLVMToLLVMIRTranslation.h\"\n #include \"mlir/Target/LLVMIR/Dialect/NVVM/NVVMToLLVMIRTranslation.h\"\n #include \"mlir/Target/LLVMIR/Export.h\""}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 0, "deletions": 29, "changes": 29, "file_content_changes": "@@ -1,46 +1,17 @@\n #include \"triton/Target/PTX/PTXTranslation.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"mlir/ExecutionEngine/ExecutionEngine.h\"\n-#include \"mlir/ExecutionEngine/OptUtils.h\"\n-#include \"mlir/IR/BuiltinOps.h\"\n-#include \"mlir/IR/Dialect.h\"\n-#include \"mlir/Pass/Pass.h\"\n-#include \"mlir/Pass/PassManager.h\"\n-#include \"mlir/Support/LogicalResult.h\"\n-#include \"mlir/Target/LLVMIR/Dialect/LLVMIR/LLVMToLLVMIRTranslation.h\"\n-#include \"mlir/Target/LLVMIR/Export.h\"\n-#include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n \n-#include \"llvm/ExecutionEngine/ExecutionEngine.h\"\n-#include \"llvm/ExecutionEngine/SectionMemoryManager.h\"\n #include \"llvm/IR/IRBuilder.h\"\n-#include \"llvm/IR/IRPrintingPasses.h\"\n #include \"llvm/IR/LegacyPassManager.h\"\n #include \"llvm/IR/Module.h\"\n #include \"llvm/IR/Verifier.h\"\n #include \"llvm/MC/TargetRegistry.h\"\n-#include \"llvm/Support/CodeGen.h\"\n-#include \"llvm/Support/CommandLine.h\"\n-#include \"llvm/Support/SourceMgr.h\"\n #include \"llvm/Support/TargetSelect.h\"\n-#include \"llvm/Support/raw_ostream.h\"\n #include \"llvm/Target/TargetMachine.h\"\n-#include \"llvm/Target/TargetOptions.h\"\n-#include \"llvm/Transforms/Scalar.h\"\n-#include \"llvm/Transforms/Utils/Cloning.h\"\n #include <filesystem>\n-#include <regex>\n \n namespace triton {\n \n-extern \"C\" {\n-int set_curterm(char *nterm) { return 0; }\n-int del_curterm(char *nterm) { return 0; }\n-int tigetnum(char *capname) { return 0; }\n-int setupterm(char *term, int fildes, int *errret) { return 0; }\n-}\n-\n static void init_llvm() {\n   LLVMInitializeNVPTXTargetInfo();\n   LLVMInitializeNVPTXTarget();"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -672,7 +672,7 @@ def without_fn(X, Y, A, B, C):\n #     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n def test_tensor_atomic_rmw_add_elementwise(device=\"cuda\"):\n-    shape0, shape1 = 16, 16\n+    shape0, shape1 = 2, 8\n     @triton.jit\n     def kernel(Z, X, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n         off0 = tl.arange(0, SHAPE0)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 26, "deletions": 21, "changes": 47, "file_content_changes": "@@ -472,8 +472,6 @@ def visit_UnaryOp(self, node):\n         if type(node.op) == ast.Not:\n             assert isinstance(op, triton.language.constexpr), \"`not` only supported for constexpr at the moment\"\n             return triton.language.constexpr(not op)\n-        if isinstance(op, triton.language.constexpr):\n-            op = op.value\n         fn = {\n             ast.USub: '__neg__',\n             ast.UAdd: '__pos__',\n@@ -563,27 +561,30 @@ def visit_For(self, node):\n         iterator = self.visit(node.iter.func)\n         if iterator != self.builtins['range']:\n             raise RuntimeError('Only `range` iterator currently supported')\n-        # static for loops: all iterator arguments are constexpr\n+        # visit iterator arguments\n+        # note: only `range` iterator is supported now\n         iter_args = [self.visit(arg) for arg in node.iter.args]\n-        static_unrolling = os.environ.get('TRITON_STATIC_LOOP_UNROLLING', False)\n-        is_static = False\n-        if static_unrolling:\n-            is_static = all([isinstance(x, triton.language.constexpr) for x in iter_args])\n-        if is_static:\n-            iter_args = [arg.value for arg in iter_args]\n-            range = iterator(*iter_args)\n-            if len(range) <= 10:\n-                for i in iterator(*iter_args):\n+        # collect lower bound (lb), upper bound (ub), and step\n+        lb = iter_args[0] if len(iter_args) > 1 else self.visit(ast.Num(0))\n+        ub = iter_args[1] if len(iter_args) > 1 else self.visit(node.iter.args[0])\n+        step = iter_args[2] if len(iter_args) > 2 else self.visit(ast.Num(1))\n+        # static for loops: all iterator arguments are constexpr\n+        if isinstance(lb, triton.language.constexpr) and \\\n+           isinstance(ub, triton.language.constexpr) and \\\n+           isinstance(step, triton.language.constexpr):\n+            sta_range = iterator(lb.value, ub.value, step.value)\n+            static_unrolling = os.environ.get('TRITON_STATIC_LOOP_UNROLLING', False)\n+            if static_unrolling and len(range) <= 10:\n+                for i in sta_range:\n                     self.lscope[node.target.id] = triton.language.constexpr(i)\n                     self.visit_compound_statement(node.body)\n                     for stmt in node.orelse:\n                         ast.NodeVisitor.generic_visit(self, stmt)\n                 return\n-\n-        # collect lower bound (lb), upper bound (ub), and step\n-        lb = self.visit(node.iter.args[0] if len(node.iter.args) > 1 else ast.Num(0))\n-        ub = self.visit(node.iter.args[1] if len(node.iter.args) > 1 else node.iter.args[0])\n-        step = self.visit(node.iter.args[2] if len(node.iter.args) > 2 else ast.Num(1))\n+        # handle negative constant step (not supported by scf.for in MLIR)\n+        if isinstance(step, triton.language.constexpr) and step.value < 0:\n+            step = triton.language.constexpr(-step.value)\n+            lb, ub = ub, lb\n         # lb/ub/step might be constexpr, we need to cast them to tensor\n         lb = triton.language.core._to_tensor(lb, self.builder).handle\n         ub = triton.language.core._to_tensor(ub, self.builder).handle\n@@ -857,6 +858,7 @@ def build_triton_ir(fn, signature, specialization, constants):\n     ret.context = context\n     return ret, generator\n \n+\n def optimize_triton_ir(mod):\n     pm = _triton.ir.pass_manager(mod.context)\n     pm.enable_debug()\n@@ -868,10 +870,12 @@ def optimize_triton_ir(mod):\n     pm.run(mod)\n     return mod\n \n+\n def ast_to_ttir(fn, signature, specialization, constants):\n     mod, _ = build_triton_ir(fn, signature, specialization, constants)\n     return optimize_triton_ir(mod)\n \n+\n def ttir_to_ttgir(mod, num_warps, num_stages):\n     pm = _triton.ir.pass_manager(mod.context)\n     pm.add_convert_triton_to_tritongpu_pass(num_warps)\n@@ -881,6 +885,9 @@ def ttir_to_ttgir(mod, num_warps, num_stages):\n     pm.add_coalesce_pass()\n     pm.add_triton_gpu_combine_pass()\n     pm.add_tritongpu_pipeline_pass(num_stages)\n+    # Prefetch must be done after pipeline pass because pipeline pass\n+    # extracts slices from the original tensor.\n+    pm.add_tritongpu_prefetch_pass()\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n     pm.add_triton_gpu_combine_pass()\n@@ -922,7 +929,6 @@ def llir_to_ptx(mod: Any, compute_capability: int = None, ptx_version: int = Non\n     return _triton.translate_llvmir_to_ptx(mod, compute_capability, ptx_version)\n \n \n-\n def ptx_to_cubin(ptx: str, device: int):\n     '''\n     Compile TritonGPU module to cubin.\n@@ -992,8 +998,6 @@ def path_to_ptxas():\n instance_descriptor = namedtuple(\"instance_descriptor\", [\"divisible_by_16\", \"equal_to_1\"], defaults=[set(), set()])\n \n \n-\n-\n # ------------------------------------------------------------------------------\n # compiler\n # ------------------------------------------------------------------------------\n@@ -1319,7 +1323,7 @@ def make_stub(name, signature, constants):\n \n \n def convert_type_repr(x):\n-    match = re.search('!tt\\.ptr<(.*)>', x)\n+    match = re.search(r'!tt\\.ptr<(.*)>', x)\n     if match is not None:\n       return '*' + convert_type_repr(match.group(1))\n     return x\n@@ -1490,6 +1494,7 @@ def _generate_src(self):\n         #include <cuda.h>\n \n         #include \\\"cuda.h\\\"\n+        #define PY_SSIZE_T_CLEAN \n         #include <Python.h>\n \n         static inline void gpuAssert(CUresult code, const char *file, int line)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 30, "deletions": 21, "changes": 51, "file_content_changes": "@@ -345,67 +345,76 @@ def __repr__(self) -> str:\n         return f\"constexpr[{self.value}]\"\n \n     def __add__(self, other):\n-        return self.value + other.value\n+        return constexpr(self.value + other.value)\n \n     def __radd__(self, other):\n-        return other.value + self.value\n+        return constexpr(other.value + self.value)\n \n     def __sub__(self, other):\n-        return self.value - other.value\n+        return constexpr(self.value - other.value)\n \n     def __rsub__(self, other):\n-        return other.value - self.value\n+        return constexpr(other.value - self.value)\n \n     def __mul__(self, other):\n-        return self.value * other.value\n+        return constexpr(self.value * other.value)\n \n     def __rmul__(self, other):\n-        return other.value * self.value\n+        return constexpr(other.value * self.value)\n \n     def __truediv__(self, other):\n-        return self.value / other.value\n+        return constexpr(self.value / other.value)\n \n     def __rtruediv__(self, other):\n-        return other.value / self.value\n+        return constexpr(other.value / self.value)\n \n     def __floordiv__(self, other):\n-        return self.value // other.value\n+        return constexpr(self.value // other.value)\n \n     def __rfloordiv__(self, other):\n-        return other.value // self.value\n+        return constexpr(other.value // self.value)\n \n     def __gt__(self, other):\n-        return self.value > other.value\n+        return constexpr(self.value > other.value)\n \n     def __rgt__(self, other):\n-        return other.value > self.value\n+        return constexpr(other.value > self.value)\n \n     def __ge__(self, other):\n-        return self.value >= other.value\n+        return constexpr(self.value >= other.value)\n \n     def __rge__(self, other):\n-        return other.value >= self.value\n+        return constexpr(other.value >= self.value)\n \n     def __lt__(self, other):\n-        return self.value < other.value\n+        return constexpr(self.value < other.value)\n \n     def __rlt__(self, other):\n-        return other.value < self.value\n+        return constexpr(other.value < self.value)\n \n     def __le__(self, other):\n-        return self.value <= other.value\n+        return constexpr(self.value <= other.value)\n \n     def __rle__(self, other):\n-        return other.value <= self.value\n+        return constexpr(other.value <= self.value)\n \n     def __eq__(self, other):\n-        return self.value == other.value\n+        return constexpr(self.value == other.value)\n \n     def __ne__(self, other):\n-        return self.value != other.value\n+        return constexpr(self.value != other.value)\n \n     def __bool__(self):\n-        return bool(self.value)\n+        return constexpr(bool(self.value))\n+\n+    def __neg__(self):\n+        return constexpr(-self.value)\n+    \n+    def __pos__(self):\n+        return constexpr(+self.value)\n+    \n+    def __invert__(self):\n+        return constexpr(~self.value)\n \n     def __call__(self, *args, **kwds):\n         return self.value(*args, **kwds)"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -351,8 +351,14 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK-NEXT: llvm.extractvalue\n     // CHECK-NEXT: llvm.extractvalue\n     // CHECK-NEXT: llvm.extractvalue\n+    // CHECK-NEXT: llvm.extractvalue\n+    // CHECK-NEXT: llvm.extractvalue\n+    // CHECK-NEXT: llvm.extractvalue\n+    // CHECK-NEXT: llvm.add\n     // CHECK-NEXT: llvm.mlir.constant(0 : i32) : i32\n+    // CHECK-NEXT: llvm.add\n     // CHECK-NEXT: llvm.mlir.constant(0 : i32) : i32\n+    // CHECK-NEXT: llvm.add\n     // CHECK-NEXT: llvm.mlir.constant(0 : i32) : i32\n     // CHECK-NEXT: llvm.mul\n     // CHECK-NEXT: llvm.add"}]