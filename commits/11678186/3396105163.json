[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -357,12 +357,11 @@ def TT_ExtElemwiseOp : TT_Op<\"ext_elemwise\", [NoSideEffect, Elementwise, SameOpe\n         return $libpath/$libname:$symbol($args...)\n     }];\n \n-    let arguments = (ins Variadic<TT_Tensor>:$args, StrAttr:$libname, StrAttr:$libpath, StrAttr:$symbol);\n+    let arguments = (ins Variadic<TT_Type>:$args, StrAttr:$libname, StrAttr:$libpath, StrAttr:$symbol);\n \n-    let results = (outs TT_Tensor:$result);\n+    let results = (outs TT_Type:$result);\n \n     let assemblyFormat = \"operands attr-dict `:` type(operands) `->` type($result)\";\n-\n }\n \n //"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -261,7 +261,7 @@ void init_triton_ir(py::module &&m) {\n           },\n           ret::reference)\n       .def(\"dump\", [](mlir::OpState &self) { self->dump(); })\n-      .def(\"str\",\n+      .def(\"__str__\",\n            [](mlir::OpState &self) -> std::string {\n              std::string str;\n              llvm::raw_string_ostream os(str);\n@@ -1280,8 +1280,8 @@ void init_triton_translation(py::module &m) {\n   using ret = py::return_value_policy;\n \n   m.def(\"get_shared_memory_size\", [](mlir::ModuleOp module) {\n-    return module->getAttrOfType<mlir::IntegerAttr>(\"triton_gpu.shared\")\n-        .getInt();\n+    auto shared = module->getAttrOfType<mlir::IntegerAttr>(\"triton_gpu.shared\");\n+    return shared.getInt();\n   });\n \n   m.def("}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 30, "deletions": 1, "changes": 31, "file_content_changes": "@@ -1544,7 +1544,7 @@ def _kernel(dst):\n                          [('int32', 'libdevice.ffs', ''),\n                           ('float32', 'libdevice.pow', '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'),\n                           ('float64', 'libdevice.norm4d', '')])\n-def test_libdevice(dtype_str, expr, lib_path):\n+def test_libdevice_tensor(dtype_str, expr, lib_path):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -1580,3 +1580,32 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n         np.testing.assert_equal(y_ref, to_numpy(y_tri))\n     else:\n         np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n+\n+\n+@pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n+                         [('float32', 'libdevice.pow', '')])\n+def test_libdevice_scalar(dtype_str, expr, lib_path):\n+\n+    @triton.jit\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        x = X\n+        y = GENERATE_TEST_HERE\n+        tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+    shape = (128, )\n+    rs = RandomState(17)\n+    # limit the range of integers so that the sum does not overflow\n+    x = numpy_random((1,), dtype_str=dtype_str, rs=rs)\n+    y_ref = np.zeros(shape, dtype=x.dtype)\n+\n+    # numpy does not allow negative factors in power, so we use abs()\n+    x = np.abs(x)\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+    y_ref[:] = np.power(x, x)\n+\n+    # triton result\n+    x_tri = to_triton(x)[0].item()\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n+    # compare\n+    np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)"}, {"filename": "python/tests/test_math_ops.py", "status": "removed", "additions": 0, "deletions": 33, "changes": 33, "file_content_changes": "@@ -1,33 +0,0 @@\n-\n-import triton\n-import triton.language as tl\n-\n-\n-@triton.jit\n-def math_kernel(x1_ptr, x2_ptr, x3_ptr, x4_ptr, n, BLOCK_SIZE: tl.constexpr):\n-    offsets = tl.arange(0, BLOCK_SIZE)\n-    x1 = tl.load(x1_ptr + offsets, mask=offsets < n)\n-    x2 = tl.load(x2_ptr + offsets, mask=offsets < n)\n-    x3 = tl.load(x3_ptr + offsets, mask=offsets < n)\n-    x4 = tl.load(x4_ptr + offsets, mask=offsets < n)\n-\n-    y1 = tl.sin(x1)\n-    y2 = tl.libdevice.sin(x2)\n-    y3 = tl.libdevice.div_rn(x3, x3)\n-    y4 = tl.libdevice.fma_rd(x4, x4, x4)\n-\n-    tl.store(x1_ptr + offsets, y1, mask=offsets < n)\n-    tl.store(x2_ptr + offsets, y2, mask=offsets < n)\n-    tl.store(x3_ptr + offsets, y3, mask=offsets < n)\n-    tl.store(x4_ptr + offsets, y4, mask=offsets < n)\n-\n-\n-def test_empty_kernel_cubin_compile():\n-    kernel = triton.compiler._compile(math_kernel,\n-                                      \"*fp32,*fp32,*fp32,*fp32,i32\",\n-                                      device=0,\n-                                      constants={\"BLOCK_SIZE\": 256},\n-                                      output=\"ttgir\")  # \"cubin\"\n-    assert kernel\n-    # TODO: Check if the values are correct.\n-    # TODO: Cover all the math operators"}, {"filename": "python/tests/test_type.py", "status": "removed", "additions": 0, "deletions": 80, "changes": 80, "file_content_changes": "@@ -1,80 +0,0 @@\n-import triton\n-import triton.language as tl\n-\n-\n-# TODO: function with no arguments don't work\n-@triton.jit\n-def binop_type_check(X):\n-    # 0d-tensor is not allowed.\n-    # zero_0d = tl.zeros([], dtype=tl.float32)\n-    zero_1d = tl.zeros([2], dtype=tl.float32)\n-    zero_2d_21 = tl.zeros([2, 1], dtype=tl.float32)\n-    zero_2d_22 = tl.zeros([2, 2], dtype=tl.float32)\n-\n-    # scalar + scalar -> scalar\n-    a0 = 0.0 + 0.0\n-    # # scalar + 0D -> 0D\n-    # a1 = 0.0 + zero_0d\n-    # a2 = zero_0d + 0.0\n-    # scalar + 1D -> 1D\n-    a3 = 0.0 + zero_1d\n-    a4 = zero_1d + 0.0\n-    # scalar + 2D -> 2D\n-    a5 = 0.0 + zero_2d_22\n-    a6 = zero_2d_22 + 0.0\n-\n-    # # 0D + 0D -> 0D\n-    # b1 = zero_0d + zero_0d\n-    # # 0D + 1D -> 1D\n-    # b2 = zero_0d + zero_1d\n-    # b3 = zero_1d + zero_0d\n-    # # 0D + 2D -> 2D\n-    # b4 = zero_0d + zero_2d_22\n-    # b5 = zero_2d_22 + zero_0d\n-\n-    # 1D + 1D -> 1D\n-    c1 = zero_1d + zero_1d\n-    # 1D + 2D -> 2D\n-    c2 = zero_1d + zero_2d_21\n-    c3 = zero_1d + zero_2d_22\n-    c4 = zero_2d_21 + zero_1d\n-    c5 = zero_2d_22 + zero_1d\n-\n-    # 2D + 2D -> 2D\n-    d1 = zero_2d_21 + zero_2d_21\n-    d2 = zero_2d_22 + zero_2d_22\n-    d3 = zero_2d_21 + zero_2d_22\n-    d4 = zero_2d_22 + zero_2d_21\n-\n-    # return a0, a1, a2, a3, a4, a5, a6, b1, b2, b3, b4, b5, c1, c2, c3, c4, c5, d1, d2, d3, d4\n-    return a0, a3, a4, a5, a6, c1, c2, c3, c4, c5, d1, d2, d3, d4\n-\n-\n-def test_binop_type_check():\n-    kernel = triton.compiler._compile(binop_type_check,\n-                                      signature=\"*fp32\",\n-                                      device=0,\n-                                      output=\"ttir\")\n-    assert (kernel)\n-    # TODO: Check types of the results\n-\n-\n-@triton.jit\n-def reduce_type_check(ptr):\n-    v_32 = tl.load(ptr + tl.arange(0, 32))\n-    v_scalar = tl.min(v_32, axis=0)\n-    tl.store(ptr, v_scalar)\n-    v_64x128 = tl.load(ptr + tl.arange(0, 64)[:, None] + tl.arange(0, 128)[None, :])\n-    v_64 = tl.max(v_64x128, axis=1)\n-    tl.store(ptr + tl.arange(0, 64), v_64)\n-    v_128 = tl.max(v_64x128, axis=0)\n-    tl.store(ptr + tl.arange(0, 128), v_128)\n-\n-\n-def test_reduce_type_check():\n-    kernel = triton.compiler._compile(reduce_type_check,\n-                                      signature=\"*fp32\",\n-                                      device=0,\n-                                      output=\"ttir\")\n-    assert (kernel)\n-    # TODO: Check types of the results"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 87, "deletions": 77, "changes": 164, "file_content_changes": "@@ -16,8 +16,9 @@\n import warnings\n from collections import namedtuple\n from sysconfig import get_paths\n-from typing import Any, Dict, Tuple, Union\n+from typing import Any, Callable, Dict, Tuple, Union\n \n+from pathlib import Path\n import setuptools\n import torch\n from filelock import FileLock\n@@ -823,7 +824,10 @@ def kernel_suffix(signature, specialization):\n # ------------------------------------------------------------------------------\n \n \n-def make_triton_ir(fn, signature, specialization, constants):\n+def build_triton_ir(fn, signature, specialization, constants):\n+    # canonicalize signature\n+    if isinstance(signature, str):\n+      signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n     context = _triton.ir.context()\n     context.load_triton()\n     # create kernel prototype\n@@ -853,7 +857,6 @@ def make_triton_ir(fn, signature, specialization, constants):\n     ret.context = context\n     return ret, generator\n \n-\n def optimize_triton_ir(mod):\n     pm = _triton.ir.pass_manager(mod.context)\n     pm.enable_debug()\n@@ -865,16 +868,13 @@ def optimize_triton_ir(mod):\n     pm.run(mod)\n     return mod\n \n+def ast_to_ttir(fn, signature, specialization, constants):\n+    mod, _ = build_triton_ir(fn, signature, specialization, constants)\n+    return optimize_triton_ir(mod)\n \n-def make_tritongpu_ir(mod, num_warps):\n+def ttir_to_ttgir(mod, num_warps, num_stages):\n     pm = _triton.ir.pass_manager(mod.context)\n     pm.add_convert_triton_to_tritongpu_pass(num_warps)\n-    pm.run(mod)\n-    return mod\n-\n-\n-def optimize_tritongpu_ir(mod, num_stages):\n-    pm = _triton.ir.pass_manager(mod.context)\n     pm.enable_debug()\n     pm.add_tritongpu_pipeline_pass(num_stages)\n     pm.add_canonicalizer_pass()\n@@ -896,28 +896,41 @@ def add_external_libs(mod, libs):\n     _triton.add_external_libs(mod, list(libs.keys()), list(libs.values()))\n \n \n-def make_llvm_ir(mod):\n+def ttgir_to_llir(mod, extern_libs):\n+    if extern_libs:\n+        add_external_libs(mod, extern_libs)\n     return _triton.translate_triton_gpu_to_llvmir(mod)\n \n \n-def make_ptx(mod: Any, compute_capability: int, ptx_version: int) -> Tuple[str, int]:\n+def llir_to_ptx(mod: Any, compute_capability: int = None, ptx_version: int = None) -> Tuple[str, int]:\n     '''\n     Translate TritonGPU module to PTX code.\n     :param mod: a TritonGPU dialect module\n     :return:\n         - PTX code\n         - shared memory alloaction size\n     '''\n+    if compute_capability is None:\n+        device = torch.cuda.current_device()\n+        compute_capability = torch.cuda.get_device_capability(device)\n+        compute_capability = compute_capability[0] * 10 + compute_capability[1]\n+    if ptx_version is None:\n+        _, cuda_version = path_to_ptxas()\n+        ptx_version = ptx_get_version(cuda_version)\n     return _triton.translate_llvmir_to_ptx(mod, compute_capability, ptx_version)\n \n \n-def make_cubin(ptx: str, ptxas: str, compute_capability: int):\n+\n+def ptx_to_cubin(ptx: str, device: int):\n     '''\n     Compile TritonGPU module to cubin.\n     :param ptx: ptx code\n     :param device: CUDA device\n     :return: str\n     '''\n+    ptxas, _ = path_to_ptxas()\n+    compute_capability = torch.cuda.get_device_capability(device)\n+    compute_capability = compute_capability[0] * 10 + compute_capability[1]\n     return _triton.compile_ptx_to_cubin(ptx, ptxas, compute_capability)\n \n \n@@ -977,46 +990,6 @@ def path_to_ptxas():\n instance_descriptor = namedtuple(\"instance_descriptor\", [\"divisible_by_16\", \"equal_to_1\"], defaults=[set(), set()])\n \n \n-def _compile(fn, signature: str, device: int = -1, constants=dict(), specialization=instance_descriptor(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, output: str = \"ttgir\") -> Tuple[str, int, str]:\n-    if isinstance(signature, str):\n-        signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n-    valid_outputs = (\"ttir\", \"ttgir\", \"ptx\", \"cubin\")\n-    assert output in valid_outputs, \"output should be one of [%s], but get \\\"%s\\\"\" % (','.join(valid_outputs), output)\n-\n-    # triton-ir\n-    module, _ = make_triton_ir(fn, signature, specialization, constants)\n-    module = optimize_triton_ir(module)\n-    if output == \"ttir\":\n-        return module.str()\n-\n-    # tritongpu-ir\n-    module = make_tritongpu_ir(module, num_warps)\n-    module = optimize_tritongpu_ir(module, num_stages)\n-    if output == \"ttgir\":\n-        return module.str()\n-\n-    if extern_libs:\n-        add_external_libs(module, extern_libs)\n-\n-    # llvm-ir\n-    llvm_ir = make_llvm_ir(module)\n-\n-    assert device >= 0, \"device should be provided.\"\n-    ptxas, cuda_version = path_to_ptxas()\n-    compute_capability = torch.cuda.get_device_capability(device)\n-    compute_capability = compute_capability[0] * 10 + compute_capability[1]\n-    ptx_version = ptx_get_version(cuda_version)\n-    ptx = make_ptx(llvm_ir, compute_capability, ptx_version)\n-    shem_size = _triton.get_shared_memory_size(module)\n-    kernel_name = ptx_get_kernel_name(ptx)\n-    if output == \"ptx\":\n-        return ptx, shem_size, kernel_name\n-\n-    cubin = make_cubin(ptx, ptxas, compute_capability)\n-    if output == \"cubin\":\n-        return cubin, ptx, shem_size, kernel_name\n-\n-    assert False\n \n \n # ------------------------------------------------------------------------------\n@@ -1305,6 +1278,23 @@ def make_fn_cache_key(fn_hash, signature, configs, constants, num_warps, num_sta\n     return key\n \n \n+def read_or_execute(cache_manager, force_compile, file_name, metadata,\n+                    run_if_found: Callable[[str], bytes] = None,\n+                    run_if_not_found: Callable = None):\n+    if not force_compile and cache_manager.has_file(file_name):\n+      module = run_if_found(cache_manager._make_path(file_name))\n+      data = module if isinstance(module, bytes) else str(module).encode(\"utf-8\")\n+      md5 = hashlib.md5(data).hexdigest()\n+      suffix = file_name.split(\".\")[1]\n+      has_changed = metadata and md5 != metadata[\"md5\"][suffix]\n+      return module, md5, has_changed, True\n+    module = run_if_not_found()\n+    data = module if isinstance(module, bytes) else str(module).encode(\"utf-8\")\n+    md5 = hashlib.md5(data).hexdigest()\n+    cache_manager.put(data, file_name, True)\n+    return module, md5, True, False\n+\n+\n def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, configs=None):\n     if isinstance(signature, str):\n         signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n@@ -1328,48 +1318,68 @@ def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: i\n             so = _build(fn.__name__, src_path, tmpdir)\n             with open(so, \"rb\") as f:\n                 so_cache_manager.put(f.read(), so_name, binary=True)\n-\n-    # retrieve cached shared object if it exists\n+    so_path = so_cache_manager._make_path(so_name)\n+    # create cache manager\n     fn_cache_key = make_fn_cache_key(fn.cache_key, signature, configs, constants, num_warps, num_stages)\n     fn_cache_manager = CacheManager(fn_cache_key)\n-    ptx_name = f\"{name}.ptx\"\n-    cubin_name = f\"{name}.cubin\"\n-    data_name = f\"{name}.json\"\n-    if not fn_cache_manager.has_file(cubin_name) or \\\n-       not fn_cache_manager.has_file(data_name) or \\\n-       not fn_cache_manager.has_file(ptx_name):\n-        cubin, ptx, shared, kernel_name = _compile(fn, signature, device, constants, configs[0], num_warps, num_stages, extern_libs, \"cubin\")\n-        metadata = {\"name\": kernel_name, \"shared\": shared, \"num_warps\": num_warps, \"num_stages\": num_stages}\n-        fn_cache_manager.put(cubin, cubin_name)\n-        fn_cache_manager.put(ptx, ptx_name, binary=False)\n-        fn_cache_manager.put(json.dumps(metadata), data_name, binary=False)\n+    # load metadata if any\n+    metadata = None\n+    if fn_cache_manager.has_file(f'{name}.json'):\n+      with open(fn_cache_manager._make_path(f\"{name}.json\")) as f:\n+            metadata = json.load(f)\n+    context = _triton.ir.context()\n+    force_compile = False\n+    # ast -> triton-ir (or read from cache)\n+    ttir, ttir_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.ttir\", metadata,\n+                           run_if_found = lambda path: _triton.ir.parse_mlir_module(path, context),\n+                           run_if_not_found = lambda: ast_to_ttir(fn, signature, configs[0], constants))\n+    # triton-ir -> triton-gpu-ir (or read from cache)\n+    ttgir, ttgir_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.ttgir\", metadata,\n+                            run_if_found = lambda path: _triton.ir.parse_mlir_module(path, context),\n+                            run_if_not_found = lambda: ttir_to_ttgir(ttir, num_warps, num_stages))\n+    # triton-gpu-ir -> llvm-ir (or read from cache)\n+    llir, llir_md5, force_compile, llvm_cached = read_or_execute(fn_cache_manager, force_compile, f\"{name}.llir\", metadata,\n+                           run_if_found = lambda path: Path(path).read_bytes(),\n+                           run_if_not_found = lambda: ttgir_to_llir(ttgir, extern_libs))\n+    if llvm_cached:\n+        shmem_size = metadata[\"shared\"]\n+    else:\n+        shmem_size = _triton.get_shared_memory_size(ttgir)\n+    # llvm-ir -> ptx (or read from cache)\n+    ptx, ptx_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.ptx\", metadata,\n+                          run_if_found = lambda path: Path(path).read_text(),\n+                          run_if_not_found = lambda: llir_to_ptx(llir))\n+    # ptx -> cubin (or read from cache)\n+    cubin, cubin_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.cubin\", metadata,\n+                            run_if_found = lambda path: Path(path).read_bytes(),      \n+                            run_if_not_found= lambda: ptx_to_cubin(ptx, device))\n+    # dump new metadata\n+    kernel_name = ptx_get_kernel_name(ptx)\n+    metadata = {\"name\": kernel_name, \"shared\": shmem_size, \"num_warps\": num_warps, \"num_stages\": num_stages,\n+                \"md5\": {  \"cubin\": cubin_md5,  \"ptx\": ptx_md5,  \n+                          \"llir\": llir_md5,\n+                          \"ttir\": ttir_md5, \"ttgir\": ttgir_md5 }}\n+    fn_cache_manager.put(json.dumps(metadata), f\"{name}.json\", binary=False)\n \n-    return CompiledKernel(name, so_cache_manager._make_path(so_name), fn_cache_manager.cache_dir)\n+    asm = {\"ttir\": ttir, \"ttgir\": ttgir, \"llir\": llir, \"ptx\": ptx, \"cubin\": cubin}\n+    return CompiledKernel(so_path, metadata, asm)\n \n \n class CompiledKernel:\n \n-    def __init__(self, fn_name, so_path, cache_dir):\n-\n+    def __init__(self, so_path, metadata, asm):\n         # initialize launcher\n         import importlib.util\n         spec = importlib.util.spec_from_file_location(\"launcher\", so_path)\n         mod = importlib.util.module_from_spec(spec)\n         spec.loader.exec_module(mod)\n         self.c_wrapper = getattr(mod, \"launch\")\n         # initialize metadata\n-        with open(os.path.join(cache_dir, f\"{fn_name}.json\")) as f:\n-            metadata = json.load(f)\n         self.shared = metadata[\"shared\"]\n         self.num_warps = metadata[\"num_warps\"]\n         self.num_stages = metadata[\"num_stages\"]\n         # initialize asm dict\n-        self.asm = dict()\n-        with open(os.path.join(cache_dir, f\"{fn_name}.cubin\"), \"rb\") as f:\n-            self.asm[\"cubin\"] = f.read()\n-        with open(os.path.join(cache_dir, f\"{fn_name}.ptx\"), \"r\") as f:\n-            self.asm[\"ptx\"] = f.read()\n-\n+        self.asm = asm\n         device = torch.cuda.current_device()\n         global cuda_utils\n         if cuda_utils is None:"}, {"filename": "python/triton/language/extern.py", "status": "modified", "additions": 28, "deletions": 22, "changes": 50, "file_content_changes": "@@ -56,28 +56,34 @@ def elementwise(lib_name: str, lib_path: str, args: list, arg_type_symbol_dict:\n         :return: the return value of the function\n     '''\n     dispatch_args = args.copy()\n-    if len(args) == 1:\n-        dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n-        ret_shape = dispatch_args[0].shape\n-    elif len(args) == 2:\n-        dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n-        dispatch_args[1] = core._to_tensor(dispatch_args[1], _builder)\n-        dispatch_args[0], dispatch_args[1] = semantic.binary_op_type_checking_impl(\n-            dispatch_args[0], dispatch_args[1], _builder)\n-        ret_shape = dispatch_args[0].shape\n-    else:\n-        for i in range(len(dispatch_args)):\n-            dispatch_args[i] = core._to_tensor(dispatch_args[i], _builder)\n-        broadcast_arg = dispatch_args[0]\n-        # Get the broadcast shape over all the arguments\n-        for i in range(len(dispatch_args)):\n-            _, broadcast_arg = semantic.binary_op_type_checking_impl(\n-                dispatch_args[i], broadcast_arg, _builder)\n-        # Change the shape of each argument based on the broadcast shape\n-        for i in range(len(dispatch_args)):\n-            dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n-                dispatch_args[i], broadcast_arg, _builder)\n-        ret_shape = broadcast_arg.shape\n+    all_scalar = True\n+    ret_shape = None\n+    for dispatch_arg in dispatch_args:\n+        if dispatch_arg.type.is_block():\n+            all_scalar = False\n+    if not all_scalar:\n+        if len(args) == 1:\n+            dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n+            ret_shape = dispatch_args[0].shape\n+        elif len(args) == 2:\n+            dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n+            dispatch_args[1] = core._to_tensor(dispatch_args[1], _builder)\n+            dispatch_args[0], dispatch_args[1] = semantic.binary_op_type_checking_impl(\n+                dispatch_args[0], dispatch_args[1], _builder)\n+            ret_shape = dispatch_args[0].shape\n+        else:\n+            for i in range(len(dispatch_args)):\n+                dispatch_args[i] = core._to_tensor(dispatch_args[i], _builder)\n+            broadcast_arg = dispatch_args[0]\n+            # Get the broadcast shape over all the arguments\n+            for i in range(len(dispatch_args)):\n+                _, broadcast_arg = semantic.binary_op_type_checking_impl(\n+                    dispatch_args[i], broadcast_arg, _builder)\n+            # Change the shape of each argument based on the broadcast shape\n+            for i in range(len(dispatch_args)):\n+                dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n+                    dispatch_args[i], broadcast_arg, _builder)\n+            ret_shape = broadcast_arg.shape\n     func = getattr(_builder, \"create_external_elementwise\")\n     return dispatch(func, lib_name, lib_path, dispatch_args, arg_type_symbol_dict, ret_shape, _builder)\n "}, {"filename": "python/triton/tools/aot.py", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -38,14 +38,13 @@\n         exit(0)\n \n     # triton-ir -> triton-gpu-ir\n-    module = triton.compiler.make_tritongpu_ir(module, num_warps=4)\n-    module = triton.compiler.optimize_tritongpu_ir(module, num_stages=3)\n+    module = triton.compiler.ttir_to_ttgir(module, num_warps=4, num_stages=3)\n     if args.target == 'triton-gpu-ir':\n         print(module.str())\n         exit(0)\n \n     # triton-gpu-ir -> llvm-ir\n-    module = triton.compiler.make_llvm_ir(module)\n+    module = triton.compiler.ttgir_to_llir(module, extern_libs=None)\n     if args.target == 'llvm-ir':\n         print(module)\n         exit(0)\n@@ -56,6 +55,6 @@\n         raise argparse.ArgumentError(None, \"Must specify --ptx-version for PTX compilation\")\n \n     # llvm-ir -> ptx\n-    module = triton.compiler.make_ptx(module, compute_capability=args.sm, ptx_version=args.ptx_version)\n+    module = triton.compiler.llir_to_ptx(module, compute_capability=args.sm, ptx_version=args.ptx_version)\n     assert args.target == 'ptx'\n     print(module)"}]