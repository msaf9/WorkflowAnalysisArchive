[{"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 61, "deletions": 49, "changes": 110, "file_content_changes": "@@ -8,71 +8,79 @@\n \n \n @pytest.mark.parametrize(\n-    \"BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, DTYPE\",\n+    \"BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE\",\n     itertools.chain(\n         *[\n             [\n                 # 1 warp\n-                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (16, 32, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (16, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (16, 32, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (16, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (64, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (16, 64, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n+                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (32, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (16, 32, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (16, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (32, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (16, 32, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (16, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (64, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (16, 64, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 # 2 warp\n-                (64, 32, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 64, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n-                (64, 32, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 64, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n-                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 128, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n+                (64, 32, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (32, 64, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (64, 32, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (32, 64, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (32, 128, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 # 4 warp\n-                (128, 64, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (64, 128, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (128, 32, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 128, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (128, 32, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 128, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n+                (128, 64, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (64, 128, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (128, 32, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (32, 128, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (128, 32, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (32, 128, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 # 8 warp\n-                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE),\n-                (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE),\n-                (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE),\n+                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 # split-k\n-                (64, 64, 16, 2, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (64, 64, 16, 4, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (64, 64, 16, 8, 4, 2, None, None, None, AT, BT, DTYPE),\n+                (64, 64, 16, 2, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (64, 64, 16, 4, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (64, 64, 16, 8, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 # variable input\n-                (128, 128, 32, 1, 4, 2, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 384, 128, 640, AT, BT, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 384, 128, 640, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n         # n-stage\n         *[\n             [\n-                (16, 16, 16, 1, 1, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (64, 32, 64, 1, 2, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (128, 64, 16, 1, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (256, 128, 32, 1, 8, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (128, 128, 32, 1, 4, STAGES, 384, 128, 640, AT, BT, DTYPE),\n+                (16, 16, 16, 1, 1, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n+                (64, 32, 64, 1, 2, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n+                (128, 64, 16, 1, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n+                (256, 128, 32, 1, 8, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, STAGES, 384, 128, 640, AT, BT, DTYPE, DTYPE),\n                 # split-k\n-                (64, 64, 16, 8, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (64, 64, 16, 8, 4, STAGES, 1024, 1024, 32, AT, BT, DTYPE),\n+                (64, 64, 16, 8, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n+                (64, 64, 16, 8, 4, STAGES, 1024, 1024, 32, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [2, 3, 4]\n-        ]\n+        ],\n+        # mixed-precision\n+        *[\n+            [\n+                (32, 64, 16, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n+            ] for (ADTYPE, BDTYPE) in [(\"float16\", \"float32\"), (\"float32\", \"float16\"),\n+                                       (\"bfloat16\", \"float32\"), (\"float32\", \"bfloat16\")\n+                                      ] for AT in [False, True] for BT in [False, True]\n+        ],\n     ),\n )\n-def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, DTYPE):\n+def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n-    if capability[0] < 8 and DTYPE == \"bfloat16\":\n+    if capability[0] < 8 and (ADTYPE == \"bfloat16\" or BDTYPE == \"bfloat16\"):\n         pytest.skip(\"Only test bfloat16 on devices with sm >= 80\")\n-    if DTYPE == \"bfloat16\" and SPLIT_K != 1:\n+    if (ADTYPE == \"bfloat16\" or BDTYPE == \"bfloat16\") and SPLIT_K != 1:\n         pytest.skip(\"bfloat16 matmuls don't allow split_k for now\")\n     torch.manual_seed(0)\n     # nuke kernel decorators -- will set meta-parameters manually\n@@ -88,13 +96,17 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n     N = BLOCK_N if N is None else N\n     K = BLOCK_K * SPLIT_K if K is None else K\n     # allocate/transpose inputs\n-    DTYPE = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[DTYPE]\n-    a = .1 * torch.randn((K, M) if AT else (M, K), device=\"cuda\", dtype=DTYPE)\n-    b = .1 * torch.randn((N, K) if BT else (K, N), device=\"cuda\", dtype=DTYPE)\n-    a = a.t() if AT else a\n-    b = b.t() if BT else b\n+    def get_input(n, m, t, dtype):\n+      if t:\n+        return get_input(m, n, False, dtype).t()\n+\n+      dtype={\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[dtype]\n+      return .1 * torch.randn((n, m), device=\"cuda\", dtype=dtype)\n+\n+    a = get_input(M, K, AT, ADTYPE)\n+    b = get_input(K, N, BT, BDTYPE)\n     # run test\n-    th_c = torch.matmul(a, b)\n+    th_c = torch.matmul(a.to(torch.float32), b.to(torch.float32))\n     try:\n         tt_c = triton.ops.matmul(a, b)\n         torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 22, "deletions": 2, "changes": 24, "file_content_changes": "@@ -5,6 +5,23 @@\n from .matmul_perf_model import early_config_prune, estimate_matmul_time\n \n \n+_ordered_datatypes = [torch.float16, torch.bfloat16, torch.float32]\n+\n+\n+def get_higher_dtype(a, b):\n+    if a is b:\n+        return a\n+\n+    assert a in _ordered_datatypes\n+    assert b in _ordered_datatypes\n+\n+    for d in _ordered_datatypes:\n+        if a is d:\n+            return b\n+        if b is d:\n+            return a\n+\n+\n def init_to_zero(name):\n     return lambda nargs: nargs[name].zero_()\n \n@@ -97,6 +114,8 @@ def _kernel(A, B, C, M, N, K,\n             k_remaining = K - k * (BLOCK_K * SPLIT_K)\n             a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.)\n             b = tl.load(B, mask=rk[:, None] < k_remaining, other=0.)\n+        a = a.to(C.dtype.element_ty)\n+        b = b.to(C.dtype.element_ty)\n         acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n         A += BLOCK_K * SPLIT_K * stride_ak\n         B += BLOCK_K * SPLIT_K * stride_bk\n@@ -131,9 +150,10 @@ def _call(a, b, dot_out_dtype):\n         M, K = a.shape\n         _, N = b.shape\n         # allocates output\n-        c = torch.empty((M, N), device=device, dtype=a.dtype)\n+        c_dtype = get_higher_dtype(a.dtype, b.dtype)\n+        c = torch.empty((M, N), device=device, dtype=c_dtype)\n         if dot_out_dtype is None:\n-            if a.dtype in [torch.float16, torch.float32, torch.bfloat16]:\n+            if c_dtype in [torch.float16, torch.float32, torch.bfloat16]:\n                 dot_out_dtype = tl.float32\n             else:\n                 dot_out_dtype = tl.int32"}]