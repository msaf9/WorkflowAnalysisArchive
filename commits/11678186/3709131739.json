[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 41, "deletions": 10, "changes": 51, "file_content_changes": "@@ -81,7 +81,6 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n         if(!mmaEnc)\n           return $_get(context, 1, 1, 1, order);\n \n-        int version = mmaEnc.getVersion();\n         int opIdx = dotOpEnc.getOpIdx();\n \n         // number of rows per phase\n@@ -91,8 +90,8 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n         // index of the inner dimension in `order`\n         unsigned inner = (opIdx == 0) ? 0 : 1;\n \n-        // ---- begin version 1 ----\n-        if (version == 1) {\n+        // ---- begin Volta ----\n+        if (mmaEnc.isVolta()) {\n           bool is_row = order[0] != 0;\n           bool is_vec4 = opIdx == 0 ? !is_row && (shape[order[0]] <= 16) :\n               is_row && (shape[order[0]] <= 16);\n@@ -107,8 +106,8 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n           return $_get(context, vec, perPhase, maxPhase, order);\n         }\n \n-        // ---- begin version 2 ----\n-        if (version == 2) {\n+        // ---- begin Ampere ----\n+        if (mmaEnc.isAmpere()) {\n           std::vector<size_t> matShape = {8, 8,\n                                           2 * 64 / eltTy.getIntOrFloatBitWidth()};\n           // for now, disable swizzle when using transposed int8 tensor cores\n@@ -292,9 +291,12 @@ def MmaEncodingAttr : DistributedEncoding<\"MmaEncoding\"> {\n   let description = [{\n An encoding for tensors that have been produced by tensor cores.\n It is characterized by two parameters:\n-- A 'version' which specifies the generation the tensor cores\n+- A 'versionMajor' which specifies the generation the tensor cores\n whose output is being partitioned: 1 for first-gen tensor cores (Volta),\n and 2 for second-gen tensor cores (Turing/Ampere).\n+- A 'versionMinor' which indicates the specific layout of a tensor core\n+generation, e.g. for Volta, there might be multiple kinds of layouts annotated\n+by 0,1,2 and so on.\n - A `blockTileSize` to indicate how data should be\n partitioned between warps.\n \n@@ -305,7 +307,8 @@ Note: the layout is different from the recommended in PTX ISA\n https://docs.nvidia.com/cuda/parallel-thread-execution/index.html\n (mma.884 section, FP32 accumulator).\n \n-For example, the matrix L corresponding to blockTileSize=[32,16] is:\n+For example, when versionMinor=1, the matrix L corresponding to\n+blockTileSize=[32,16] is:\n \n                                warp 0\n --------------------------------/\\-------------------------------\n@@ -367,11 +370,39 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n \n   let parameters = (\n     ins\n-    \"unsigned\":$version,\n+    \"unsigned\":$versionMajor,\n+    \"unsigned\":$versionMinor,\n     ArrayRefParameter<\"unsigned\">:$warpsPerCTA\n   );\n \n-  let extraClassDeclaration = extraBaseClassDeclaration;\n+  let builders = [\n+    // specific for MMAV1(Volta)\n+    AttrBuilder<(ins \"int\":$versionMajor,\n+                     \"ArrayRef<unsigned>\":$warpsPerCTA,\n+                     \"ArrayRef<int64_t>\":$shapeA,\n+                     \"ArrayRef<int64_t>\":$shapeB,\n+                     \"bool\":$isARow,\n+                     \"bool\":$isBRow), [{\n+      assert(versionMajor == 1 && \"Only MMAv1 has multiple versionMinor.\");\n+      bool isAVec4 = !isARow && (shapeA[isARow] <= 16);\n+      bool isBVec4 = isBRow && (shapeB[isBRow] <= 16);\n+      // 4-bits to encode 4 booleans: [isARow, isBRow, isAVec4, isBVec4]\n+      int versionMinor = (isARow * (1<<0)) |\\\n+                         (isBRow * (1<<1)) |\\\n+                         (isAVec4 * (1<<2)) |\\\n+                         (isBVec4 * (1<<3));\n+      return $_get(context, versionMajor, versionMinor, warpsPerCTA);\n+    }]>\n+\n+  ];\n+\n+  let extraClassDeclaration = extraBaseClassDeclaration # [{\n+    bool isVolta() const;\n+    bool isAmpere() const;\n+    // Get [isARow, isBRow, isAVec4, isBVec4] from versionMinor\n+    std::tuple<bool, bool, bool, bool> decodeVoltaLayoutStates() const;\n+  }];\n+\n }\n \n def SliceEncodingAttr : DistributedEncoding<\"SliceEncoding\"> {\n@@ -434,7 +465,7 @@ section 9.7.13.4.1 for more details.\n                      \"Attribute\":$parent), [{\n       Attribute isMMAv1Row;\n       if(parent.isa<MmaEncodingAttr>() &&\n-         parent.cast<MmaEncodingAttr>().getVersion() == 1){\n+         parent.cast<MmaEncodingAttr>().isVolta()){\n         isMMAv1Row = BoolAttr::get(context, true);\n       }\n       return $_get(context, opIdx, parent, isMMAv1Row);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 16, "deletions": 16, "changes": 32, "file_content_changes": "@@ -609,9 +609,9 @@ class ConvertTritonGPUOpToLLVMPattern\n     if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n       return emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n     if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-      if (mmaLayout.getVersion() == 1)\n+      if (mmaLayout.isVolta())\n         return emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, shape);\n-      if (mmaLayout.getVersion() == 2)\n+      if (mmaLayout.isAmpere())\n         return emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, shape);\n     }\n     llvm_unreachable(\"unsupported emitBaseIndexForLayout\");\n@@ -622,9 +622,9 @@ class ConvertTritonGPUOpToLLVMPattern\n     if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n       return emitOffsetForBlockedLayout(blockedLayout, shape);\n     if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-      if (mmaLayout.getVersion() == 1)\n+      if (mmaLayout.isVolta())\n         return emitOffsetForMmaLayoutV1(mmaLayout, shape);\n-      if (mmaLayout.getVersion() == 2)\n+      if (mmaLayout.isAmpere())\n         return emitOffsetForMmaLayoutV2(mmaLayout, shape);\n     }\n     llvm_unreachable(\"unsupported emitOffsetForLayout\");\n@@ -2872,7 +2872,7 @@ struct ConvertLayoutOpConversion\n       Value _4 = idx_val(4);\n       Value _8 = idx_val(8);\n       Value _16 = idx_val(16);\n-      if (mmaLayout.getVersion() == 2) {\n+      if (mmaLayout.isAmpere()) {\n         multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n         multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 8));\n         Value mmaGrpId = udiv(laneId, _4);\n@@ -2886,7 +2886,7 @@ struct ConvertLayoutOpConversion\n         Value colWarpOffset = mul(multiDimWarpId[1], _8);\n         mmaColIdx[0] = add(mmaThreadIdInGrpM2, colWarpOffset);\n         mmaColIdx[1] = add(mmaThreadIdInGrpM2P1, colWarpOffset);\n-      } else if (mmaLayout.getVersion() == 1) {\n+      } else if (mmaLayout.isVolta()) {\n         multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n         multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 16));\n         Value laneIdDiv16 = udiv(laneId, _16);\n@@ -2914,14 +2914,14 @@ struct ConvertLayoutOpConversion\n \n       assert(rank == 2);\n       SmallVector<Value> multiDimOffset(rank);\n-      if (mmaLayout.getVersion() == 2) {\n+      if (mmaLayout.isAmpere()) {\n         multiDimOffset[0] = elemId < 2 ? mmaRowIdx[0] : mmaRowIdx[1];\n         multiDimOffset[1] = elemId % 2 == 0 ? mmaColIdx[0] : mmaColIdx[1];\n         multiDimOffset[0] = add(\n             multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n         multiDimOffset[1] = add(\n             multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n-      } else if (mmaLayout.getVersion() == 1) {\n+      } else if (mmaLayout.isVolta()) {\n         // the order of elements in a thread:\n         //   c0, c1, ...  c4, c5\n         //   c2, c3, ...  c6, c7\n@@ -3333,9 +3333,9 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                                     .getEncoding()\n                                     .dyn_cast<MmaEncodingAttr>();\n     if (!isOuter && mmaLayout && supportMMA(op, mmaLayout.getVersion())) {\n-      if (mmaLayout.getVersion() == 1)\n+      if (mmaLayout.isVolta())\n         return convertMMA884(op, adaptor, rewriter);\n-      if (mmaLayout.getVersion() == 2)\n+      if (mmaLayout.isAmpere())\n         return convertMMA16816(op, adaptor, rewriter);\n \n       llvm::report_fatal_error(\n@@ -3376,7 +3376,7 @@ Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n   auto smemObj = getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n   Value res;\n \n-  if (!isOuter && mmaLayout.getVersion() == 2 && isHMMA) { // tensor core v2\n+  if (!isOuter && mmaLayout.isAmpere() && isHMMA) { // tensor core v2\n     MMA16816ConversionHelper mmaHelper(src.getType(), mmaLayout,\n                                        getThreadId(rewriter, loc), rewriter,\n                                        getTypeConverter(), op.getLoc());\n@@ -3388,7 +3388,7 @@ Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n       // operand $b\n       res = mmaHelper.loadB(src, smemObj);\n     }\n-  } else if (!isOuter && mmaLayout.getVersion() == 1 &&\n+  } else if (!isOuter && mmaLayout.isVolta() &&\n              isHMMA) { // tensor core v1\n     DotOpMmaV1ConversionHelper helper(mmaLayout);\n     bool isMMAv1Row =\n@@ -3754,7 +3754,7 @@ Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,\n                                       Location loc) {\n   auto tensorTy = resType.cast<RankedTensorType>();\n   auto shape = tensorTy.getShape();\n-  if (layout.getVersion() == 2) {\n+  if (layout.isAmpere()) {\n     auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(tensorTy);\n     size_t fcSize = 4 * repM * repN;\n \n@@ -3763,7 +3763,7 @@ Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,\n     return getStructFromElements(loc, SmallVector<Value>(fcSize, constVal),\n                                  rewriter, structTy);\n   }\n-  if (layout.getVersion() == 1) {\n+  if (layout.isVolta()) {\n     DotOpMmaV1ConversionHelper helper(layout);\n     int repM = helper.getRepM(shape[0]);\n     int repN = helper.getRepN(shape[1]);\n@@ -3844,7 +3844,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n         auto mmaLayout = dotOpLayout.getParent().cast<MmaEncodingAttr>();\n         auto wpt = mmaLayout.getWarpsPerCTA();\n         Type elemTy = convertType(type.getElementType());\n-        if (mmaLayout.getVersion() == 2) {\n+        if (mmaLayout.isAmpere()) {\n           const llvm::DenseMap<int, Type> targetTyMap = {\n               {32, elemTy},\n               {16, vec_ty(elemTy, 2)},\n@@ -3869,7 +3869,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n           }\n         }\n \n-        if (mmaLayout.getVersion() == 1) {\n+        if (mmaLayout.isVolta()) {\n           DotOpMmaV1ConversionHelper helper(mmaLayout);\n \n           // TODO[Superjomn]: Both transA and transB are not available here."}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 50, "deletions": 26, "changes": 76, "file_content_changes": "@@ -77,9 +77,9 @@ SmallVector<unsigned> getThreadsPerWarp(const Attribute &layout) {\n                                  blockedLayout.getThreadsPerWarp().end());\n   }\n   if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    if (mmaLayout.getVersion() == 1)\n+    if (mmaLayout.isVolta())\n       return {4, 8};\n-    if (mmaLayout.getVersion() == 2)\n+    if (mmaLayout.isAmpere())\n       return {8, 4};\n   }\n   assert(0 && \"getThreadsPerWarp not implemented\");\n@@ -106,9 +106,9 @@ SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     return getSizePerThread(sliceLayout.getParent());\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    if (mmaLayout.getVersion() == 2) {\n+    if (mmaLayout.isAmpere()) {\n       return {2, 2};\n-    } else if (mmaLayout.getVersion() == 1) {\n+    } else if (mmaLayout.isVolta()) {\n       // Note: here the definition of sizePerThread is obscure, which doesn't\n       // mean vecSize=4 can be supported in the last dimension.\n       return {2, 4};\n@@ -119,7 +119,7 @@ SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n     auto parentLayout = dotLayout.getParent();\n     assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n     if (auto parentMmaLayout = parentLayout.dyn_cast<MmaEncodingAttr>()) {\n-      assert(parentMmaLayout.getVersion() == 2 &&\n+      assert(parentMmaLayout.isAmpere() &&\n              \"mmaLayout version = 1 is not implemented yet\");\n       auto parentShapePerCTA = getShapePerCTA(parentLayout);\n       auto opIdx = dotLayout.getOpIdx();\n@@ -144,7 +144,7 @@ SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n \n SmallVector<unsigned> getContigPerThread(Attribute layout) {\n   if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    assert(mmaLayout.getVersion() == 1 || mmaLayout.getVersion() == 2);\n+    assert(mmaLayout.isVolta() || mmaLayout.isAmpere());\n     return {1, 2};\n   } else {\n     return getSizePerThread(layout);\n@@ -182,18 +182,18 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n       shape.push_back(getShapePerCTA(parent)[d]);\n     }\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    if (mmaLayout.getVersion() == 2)\n+    if (mmaLayout.isAmpere())\n       return {16 * mmaLayout.getWarpsPerCTA()[0],\n               8 * mmaLayout.getWarpsPerCTA()[1]};\n-    if (mmaLayout.getVersion() == 1)\n+    if (mmaLayout.isVolta())\n       return {16 * mmaLayout.getWarpsPerCTA()[0],\n               16 * mmaLayout.getWarpsPerCTA()[1]};\n     assert(0 && \"Unexpected MMA layout version found\");\n   } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n     auto parentLayout = dotLayout.getParent();\n     assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n     if (auto parentMmaLayout = parentLayout.dyn_cast<MmaEncodingAttr>()) {\n-      assert(parentMmaLayout.getVersion() == 2 &&\n+      assert(parentMmaLayout.isAmpere() &&\n              \"mmaLayout version = 1 is not implemented yet\");\n       auto parentShapePerCTA = getShapePerCTA(parentLayout);\n       auto opIdx = dotLayout.getOpIdx();\n@@ -209,10 +209,10 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n                   \"supported yet\");\n     }\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    if (mmaLayout.getVersion() == 2) {\n+    if (mmaLayout.isAmpere()) {\n       return {16 * mmaLayout.getWarpsPerCTA()[0],\n               8 * mmaLayout.getWarpsPerCTA()[1]};\n-    } else if (mmaLayout.getVersion() == 1) {\n+    } else if (mmaLayout.isVolta()) {\n       return {16 * mmaLayout.getWarpsPerCTA()[0],\n               16 * mmaLayout.getWarpsPerCTA()[1]};\n     } else {\n@@ -368,17 +368,16 @@ unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   size_t rank = shape.size();\n   assert(rank == 2 && \"Unexpected rank of mma layout\");\n-  assert((getVersion() == 1 || getVersion() == 2) &&\n-         \"Only version 1 and 2 is supported\");\n+  assert((isVolta() || isAmpere()) && \"Only version 1 and 2 is supported\");\n \n   int res = 0;\n-  if (getVersion() == 1) {\n+  if (isVolta()) {\n     unsigned mmasRow = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]);\n     unsigned mmasCol = ceil<unsigned>(shape[1], 16 * getWarpsPerCTA()[1]);\n     // Each warp-level mma884 will perform a m16xn16xk4 mma, thus get a m16xn16\n     // matrix as result.\n     res = mmasRow * mmasCol * (16 * 16 / 32);\n-  } else if (getVersion() == 2) {\n+  } else if (isAmpere()) {\n     unsigned elemsCol = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]) * 2;\n     unsigned elemsRow = ceil<unsigned>(shape[1], 8 * getWarpsPerCTA()[1]) * 2;\n     res = elemsCol * elemsRow;\n@@ -476,12 +475,17 @@ Attribute MmaEncodingAttr::parse(AsmParser &parser, Type type) {\n   if (parser.parseGreater().failed())\n     return {};\n \n-  unsigned version = 0;\n+  unsigned versionMajor = 0;\n+  unsigned versionMinor = 0;\n   SmallVector<unsigned, 2> warpsPerCTA;\n \n   for (const NamedAttribute &attr : dict) {\n-    if (attr.getName() == \"version\") {\n-      if (parseUInt(parser, attr, version, \"version\").failed())\n+    if (attr.getName() == \"versionMajor\") {\n+      if (parseUInt(parser, attr, versionMajor, \"versionMajor\").failed())\n+        return {};\n+    }\n+    if (attr.getName() == \"versionMinor\") {\n+      if (parseUInt(parser, attr, versionMinor, \"versionMinor\").failed())\n         return {};\n     }\n     if (attr.getName() == \"warpsPerCTA\") {\n@@ -490,13 +494,14 @@ Attribute MmaEncodingAttr::parse(AsmParser &parser, Type type) {\n     }\n   }\n \n-  return parser.getChecked<MmaEncodingAttr>(parser.getContext(), version,\n-                                            warpsPerCTA);\n+  return parser.getChecked<MmaEncodingAttr>(parser.getContext(), versionMajor,\n+                                            versionMinor, warpsPerCTA);\n }\n \n void MmaEncodingAttr::print(AsmPrinter &printer) const {\n   printer << \"<{\"\n-          << \"version = \" << getVersion() << \", \"\n+          << \"versionMajor = \" << getVersionMajor() << \", \"\n+          << \"versionMinor = \" << getVersionMinor() << \", \"\n           << \"warpsPerCTA = [\" << getWarpsPerCTA() << \"]\"\n           << \"}>\";\n }\n@@ -575,6 +580,25 @@ void SharedEncodingAttr::print(AsmPrinter &printer) const {\n           << \"}>\";\n }\n \n+//===----------------------------------------------------------------------===//\n+// Mma encoding\n+//===----------------------------------------------------------------------===//\n+\n+bool MmaEncodingAttr::isVolta() const { return getVersionMajor() == 1; }\n+\n+bool MmaEncodingAttr::isAmpere() const { return getVersionMajor() == 2; }\n+\n+// Get [isARow, isBRow, isAVec4, isBVec4] from versionMinor\n+std::tuple<bool, bool, bool, bool>\n+MmaEncodingAttr::decodeVoltaLayoutStates() const {\n+  unsigned versionMinor = getVersionMinor();\n+  bool isARow = versionMinor & (1 << 0);\n+  bool isBRow = versionMinor & (1 << 1);\n+  bool isAVec4 = versionMinor & (1 << 2);\n+  bool isBVec4 = versionMinor & (1 << 3);\n+  return std::make_tuple(isARow, isBRow, isAVec4, isBVec4);\n+}\n+\n //===----------------------------------------------------------------------===//\n // DotOperand Encoding\n //===----------------------------------------------------------------------===//\n@@ -589,10 +613,10 @@ Attribute DotOperandEncodingAttr::parse(AsmParser &parser, Type type) {\n   unsigned opIdx = attrs.get(\"opIdx\").cast<IntegerAttr>().getInt();\n   Attribute parent = attrs.get(\"parent\");\n   Attribute isMMAv1Row;\n-  if(parent.isa<MmaEncodingAttr>() &&\n-     parent.cast<MmaEncodingAttr>().getVersion() == 1){\n+  if (parent.isa<MmaEncodingAttr>() &&\n+      parent.cast<MmaEncodingAttr>().isVolta()) {\n     isMMAv1Row = attrs.get(\"isMMAv1Row\");\n-    if(!isMMAv1Row)\n+    if (!isMMAv1Row)\n       llvm::report_fatal_error(\"isMMAv1Row attribute is missing\");\n   }\n   return parser.getChecked<DotOperandEncodingAttr>(parser.getContext(), opIdx,\n@@ -603,8 +627,8 @@ void DotOperandEncodingAttr::print(mlir::AsmPrinter &printer) const {\n   printer << \"<{\"\n           << \"opIdx = \" << getOpIdx() << \", \"\n           << \"parent = \" << getParent();\n-  if(getIsMMAv1Row())\n-    printer << \", isMMAv1Row = \" << getIsMMAv1Row();      \n+  if (getIsMMAv1Row())\n+    printer << \", isMMAv1Row = \" << getIsMMAv1Row();\n   printer << \"}>\";\n }\n "}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -1715,7 +1715,7 @@ def __init__(self, version, warps_per_cta):\n         self.warps_per_cta = str(warps_per_cta)\n \n     def __str__(self):\n-        return f\"#triton_gpu.mma<{{version={self.version}, warpsPerCTA={self.warps_per_cta}}}>\"\n+        return f\"#triton_gpu.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}}}>\"\n \n class BlockedLayout:\n     def __init__(self, size_per_thread, threads_per_warp, warps_per_cta, order):\n@@ -1729,9 +1729,9 @@ def __str__(self):\n \n layouts = [\n   # MmaLayout(version=1, warps_per_cta=[1, 4]),\n-  MmaLayout(version=2, warps_per_cta=[1, 4]),\n+  MmaLayout(version=(2,0), warps_per_cta=[1, 4]),\n   # MmaLayout(version=1, warps_per_cta=[4, 1]),\n-  MmaLayout(version=2, warps_per_cta=[4, 1]),\n+  MmaLayout(version=(2,0), warps_per_cta=[4, 1]),\n   BlockedLayout([1, 8], [2, 16], [4, 1], [1, 0]),\n   BlockedLayout([1, 4], [4, 8], [2, 2], [1, 0]),\n   BlockedLayout([1, 1], [1, 32], [2, 2], [1, 0]),"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -4,7 +4,7 @@\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n-#C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n+#C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n "}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -5,7 +5,7 @@\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n-#C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n+#C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n "}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -5,7 +5,7 @@\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n-#C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n+#C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n "}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -710,7 +710,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n \n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n #shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n-#mma0 = #triton_gpu.mma<{version=2, warpsPerCTA=[1,1]}>\n+#mma0 = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[1,1]}>\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n@@ -748,7 +748,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n // -----\n \n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [1, 0]}>\n-#mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [2, 2]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mmav2_block\n@@ -768,7 +768,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n // -----\n \n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [1, 0]}>\n-#mma = #triton_gpu.mma<{version = 1, warpsPerCTA = [2, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 1, warpsPerCTA = [2, 1]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mmav1_block\n@@ -853,7 +853,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n \n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n-#mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [2, 2]}>\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n@@ -878,7 +878,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n-#mma = #triton_gpu.mma<{version = 1, warpsPerCTA = [2, 2]}>\n+#mma = #triton_gpu.mma<{versionMajor = 1, warpsPerCTA = [2, 2]}>\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma, isMMAv1Row=true}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma, isMMAv1Row=true}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n@@ -923,7 +923,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // -----\n \n-#mma = #triton_gpu.mma<{version=2, warpsPerCTA=[2, 2]}>\n+#mma = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[2, 2]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -4,7 +4,7 @@\n // matmul: 128x32 @ 32x128 -> 128x128\n #AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n-#C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n+#C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n "}, {"filename": "unittest/Dialect/TritonGPU/SwizzleTest.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -28,7 +28,7 @@ TEST_P(SwizzleDotOperandTestFixture, DotOperands) {\n   MLIRContext ctx;\n   ctx.loadDialect<triton::gpu::TritonGPUDialect>();\n   // create encoding\n-  auto parent = triton::gpu::MmaEncodingAttr::get(&ctx, 2, {1, 1});\n+  auto parent = triton::gpu::MmaEncodingAttr::get(&ctx, 2, 0, {1, 1});\n   auto encoding =\n       triton::gpu::DotOperandEncodingAttr::get(&ctx, params.opIdx, parent);\n \n@@ -50,4 +50,4 @@ INSTANTIATE_TEST_SUITE_P(TestDotOperands, SwizzleDotOperandTestFixture,\n                                            ParamT{{32, 32}, 0, 16, {8, 2, 4}},\n                                            ParamT{{32, 32}, 1, 16, {8, 2, 4}},\n                                            ParamT{{16, 16}, 0, 16, {8, 4, 2}},\n-                                           ParamT{{16, 16}, 1, 16, {8, 4, 2}}));\n\\ No newline at end of file\n+                                           ParamT{{16, 16}, 1, 16, {8, 4, 2}}));"}]