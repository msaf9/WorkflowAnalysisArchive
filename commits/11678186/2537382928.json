[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 36, "deletions": 48, "changes": 84, "file_content_changes": "@@ -18,60 +18,48 @@ def _fwd_kernel(\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n ):\n-    off_hz = tl.program_id(0)\n-    off_z = off_hz // H\n-    off_h = off_hz % H\n-    start_qm = tl.num_programs(1) - 1 - tl.program_id(1)\n-    start_kn = 0\n-    # initialize pointers to Q\n-    offs_qm = start_qm * BLOCK_M + tl.arange(0, BLOCK_M)\n-    offs_qk = tl.arange(0, BLOCK_DMODEL)\n-    q_ptrs = Q + (off_z * stride_qz\n-                  + off_h * stride_qh\n-                  + offs_qm[:, None] * stride_qm\n-                  + offs_qk[None, :] * stride_qk)\n-    # initialize pointers to K\n-    offs_kk = tl.arange(0, BLOCK_DMODEL)\n-    offs_kn = start_kn * BLOCK_N + tl.arange(0, BLOCK_N)\n-    k_ptrs = K + (off_z * stride_kz\n-                  + off_h * stride_kh\n-                  + offs_kn[None, :] * stride_kn\n-                  + offs_kk[:, None] * stride_kk)\n-    # initialize pointers to V\n-    off_vk = tl.arange(0, BLOCK_N)\n-    off_vn = tl.arange(0, BLOCK_DMODEL)\n-    v_ptrs = V + off_z * stride_vz \\\n-        + off_h * stride_vh \\\n-        + off_vk[:, None] * stride_vk \\\n-        + off_vn[None, :] * stride_vn\n+    start_qm = tl.program_id(0)\n+    off_hz = tl.program_id(1)\n+    # initialize offsets\n+    offs_m = start_qm * BLOCK_M + tl.arange(0, BLOCK_M)\n+    offs_n = tl.arange(0, BLOCK_N)\n+    offs_d = tl.arange(0, BLOCK_DMODEL)\n+    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\n+    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    # Initialize pointers to Q, K, V\n+    q_ptrs = Q + off_q\n+    k_ptrs = K + off_k\n+    v_ptrs = V + off_v\n     # initialize pointer to m and l\n-    t_ptrs = TMP + off_hz * N_CTX + offs_qm\n+    t_ptrs = TMP + off_hz * N_CTX + offs_m\n \n     acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n     m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n     l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n \n-    num_blocks_for_row = start_qm + 1\n-    for start_n in range(0, num_blocks_for_row):\n-        q = tl.load(q_ptrs)  # BUG: fails when moved out of the loop\n+    q = tl.load(q_ptrs)\n+    for start_n in range(0, start_qm + 1):\n         # -- compute qk ----\n         k = tl.load(k_ptrs)\n-        qk = tl.dot(q, k)\n-        qk = tl.where(offs_qm[:, None] >= (start_n * BLOCK_N + offs_kn[None, :]), qk, float(\"-inf\"))\n+        qk = tl.dot(q, k, trans_b=True)\n+        qk += tl.where(offs_m[:, None] >= (start_n * BLOCK_N + offs_n[None, :]), 0, float(\"-inf\"))\n         # -- compute m_ij, p, l_ij\n         m_ij = tl.max(qk, 1)\n         p = tl.exp(qk - m_ij[:, None])\n         l_ij = tl.sum(p, 1)\n         # -- update m_i and l_i\n         m_i_new = tl.maximum(m_i, m_ij)\n-        l_i_new = tl.exp(m_i - m_i_new) * l_i + tl.exp(m_ij - m_i_new) * l_ij\n+        alpha = tl.exp(m_i - m_i_new)\n+        beta = tl.exp(m_ij - m_i_new)\n+        l_i_new = alpha * l_i + beta * l_ij\n         # -- update output accumulator --\n         # scale p\n-        p_scale = tl.exp(m_ij - m_i_new) / l_i_new\n+        p_scale = beta / l_i_new\n         p = p * p_scale[:, None]\n         p = p.to(tl.float16)\n         # scale acc\n-        acc_scale = l_i / l_i_new * tl.exp(m_i - m_i_new)\n+        acc_scale = l_i / l_i_new * alpha\n         tl.store(t_ptrs, acc_scale)\n         acc_scale = tl.load(t_ptrs)  # BUG: have to store and immediately load\n         acc = acc * acc_scale[:, None]\n@@ -80,21 +68,21 @@ def _fwd_kernel(\n         acc += tl.dot(p, v)\n         k_ptrs += BLOCK_N * stride_kn\n         v_ptrs += BLOCK_N * stride_vk\n+        # r_ptrs += BLOCK_N\n         l_i = l_i_new\n         m_i = m_i_new\n \n+    start_qm = tl.program_id(0)\n+    offs_m = start_qm * BLOCK_M + tl.arange(0, BLOCK_M)\n     # write back l and m\n-    l_ptrs = L + off_hz * N_CTX + offs_qm\n-    m_ptrs = M + off_hz * N_CTX + offs_qm\n+    l_ptrs = L + off_hz * N_CTX + offs_m\n+    m_ptrs = M + off_hz * N_CTX + offs_m\n     tl.store(l_ptrs, l_i)\n     tl.store(m_ptrs, m_i)\n     # initialize pointers to output\n-    offs_om = offs_qm\n-    offs_on = tl.arange(0, BLOCK_DMODEL)\n-    out_ptrs = Out + off_z * stride_oz \\\n-        + off_h * stride_oh \\\n-        + offs_om[:, None] * stride_om \\\n-        + offs_on[None, :] * stride_on\n+    offs_n = tl.arange(0, BLOCK_DMODEL)\n+    off_out = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n+    out_ptrs = Out + off_out\n     tl.store(out_ptrs, acc)\n \n \n@@ -107,13 +95,13 @@ def forward(ctx, q, k, v):\n         Lq, Lk = q.shape[-1], k.shape[-2]\n         assert Lq == Lk\n         o = torch.empty_like(q)\n-        grid = (q.shape[0] * q.shape[1], triton.cdiv(q.shape[2], BLOCK))\n+        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1])\n         tmp = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n-        l = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n-        pgm = _fwd_kernel[grid](\n+        _fwd_kernel[grid](\n             q, k, v,\n-            tmp, l, m,\n+            tmp, L, m,\n             o,\n             q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n             k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n@@ -124,7 +112,7 @@ def forward(ctx, q, k, v):\n             BLOCK_DMODEL=64, num_warps=4,\n             num_stages=1,\n         )\n-        ctx.save_for_backward(q, k, v, o, l, m)\n+        ctx.save_for_backward(q, k, v, o, L, m)\n         ctx.BLOCK = BLOCK\n         ctx.grid = grid\n         return o"}]