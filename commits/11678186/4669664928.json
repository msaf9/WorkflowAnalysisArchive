[{"filename": "bin/RegisterTritonDialects.h", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -32,7 +32,7 @@ inline void registerTritonDialects(mlir::DialectRegistry &registry) {\n \n   // TODO: register Triton & TritonGPU passes\n   registry.insert<mlir::triton::TritonDialect, mlir::cf::ControlFlowDialect,\n-                  mlir::triton::gpu::TritonGPUDialect, mlir::func::FuncDialect,\n-                  mlir::math::MathDialect, mlir::arith::ArithDialect,\n-                  mlir::scf::SCFDialect, mlir::gpu::GPUDialect>();\n+                  mlir::triton::gpu::TritonGPUDialect, mlir::math::MathDialect,\n+                  mlir::arith::ArithDialect, mlir::scf::SCFDialect,\n+                  mlir::gpu::GPUDialect>();\n }"}, {"filename": "include/triton/Dialect/Triton/IR/TritonDialect.td", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -19,15 +19,16 @@ def Triton_Dialect : Dialect {\n       * Math:\n         * exp, sin, cos, log, ...\n       * StructuredControlFlow:\n-        * ForOp, IfOp, WhileOp, YieldOp, ConditionOp\n+        * for, if, while, yield, condition\n+      * ControlFlow:\n+        * br, cond_br\n   }];\n \n   let dependentDialects = [\n     \"arith::ArithDialect\",\n     \"math::MathDialect\",\n     \"scf::SCFDialect\",\n-    \"cf::ControlFlowDialect\",\n-    \"func::FuncDialect\"\n+    \"cf::ControlFlowDialect\"\n   ];\n \n   let extraClassDeclaration = [{"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 205, "deletions": 8, "changes": 213, "file_content_changes": "@@ -6,11 +6,15 @@ include \"triton/Dialect/Triton/IR/TritonTypes.td\"\n include \"triton/Dialect/Triton/IR/TritonAttrDefs.td\"\n include \"triton/Dialect/Triton/IR/TritonInterfaces.td\"\n include \"mlir/IR/OpBase.td\"\n+include \"mlir/IR/FunctionInterfaces.td\" // FunctionOpInterface\n+include \"mlir/IR/SymbolInterfaces.td\" // SymbolUserOpInterface\n+include \"mlir/IR/OpAsmInterface.td\" // OpAsmOpInterface\n include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/ControlFlowInterfaces.td\" // BranchOpInterface\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/CastInterfaces.td\" // CastOpInterface\n+include \"mlir/Interfaces/CallInterfaces.td\" // CallOpInterface\n \n //\n // Op Base\n@@ -20,7 +24,7 @@ class TT_Op<string mnemonic, list<Trait> traits = []> :\n }\n \n //\n-// CastOps\n+// Cast Ops\n //\n // Use cast ops in arith:\n //   bitcast\n@@ -93,7 +97,6 @@ def TT_FpToFpOp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n //\n // Pointer Arith Ops\n //\n-\n def TT_AddPtrOp : TT_Op<\"addptr\",\n                      [Pure,\n                       SameOperandsAndResultShape,\n@@ -209,7 +212,7 @@ def TT_StoreOp : TT_Op<\"store\",\n }\n \n //\n-// Atomic Op\n+// Atomic Ops\n //\n def TT_AtomicRMWOp : TT_Op<\"atomic_rmw\", [SameOperandsAndResultShape,\n                                           SameOperandsAndResultEncoding,\n@@ -256,7 +259,6 @@ def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [MemoryEffects<[MemRead]>,\n     let results = (outs TT_Type:$result);\n }\n \n-\n //\n // Shape Manipulation Ops\n //\n@@ -408,7 +410,7 @@ def TT_ReduceOp : TT_Op<\"reduce\", [Pure,\n }\n \n //\n-// External elementwise op\n+// External Elementwise op\n //\n def TT_ExtElemwiseOp : TT_Op<\"ext_elemwise\", [Pure, Elementwise, SameOperandsAndResultShape,\n                                               SameOperandsAndResultEncoding,\n@@ -449,7 +451,7 @@ def TT_MakeRangeOp : TT_Op<\"make_range\", [Pure]> {\n }\n \n //\n-// Make PrintOp\n+// Print Op\n //\n def TT_PrintOp : TT_Op<\"print\", [MemoryEffects<[MemWrite]>]>,\n   Arguments<(ins StrAttr:$prefix, Variadic<AnyTypeOf<[TT_Type]>>:$args)> {\n@@ -464,7 +466,7 @@ def TT_PrintOp : TT_Op<\"print\", [MemoryEffects<[MemWrite]>]>,\n }\n \n //\n-// Make AssertOp\n+// Assert Op\n //\n def TT_AssertOp : TT_Op<\"assert\", [MemoryEffects<[MemWrite]>]> {\n   let summary = \"Device-side assert, as in CUDA for correctness checking\";\n@@ -477,7 +479,7 @@ def TT_AssertOp : TT_Op<\"assert\", [MemoryEffects<[MemWrite]>]> {\n }\n \n //\n-// Make a Tensor Pointer\n+// Make Tensor Pointer Op\n //\n def TT_MakeTensorPtrOp : TT_Op<\"make_tensor_ptr\",\n                                [Pure,\n@@ -518,4 +520,199 @@ def TT_MakeTensorPtrOp : TT_Op<\"make_tensor_ptr\",\n   ];\n }\n \n+// The following ops, including `call`, `func`, and `return` are copied and modified from\n+// https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Func/IR/FuncOps.td\n+// We could revert it back once MLIR has a better inliner interface.\n+//\n+// Function Ops\n+//\n+def CallOp : TT_Op<\"call\", [CallOpInterface, /*MemRefsNormalizable, */DeclareOpInterfaceMethods<SymbolUserOpInterface>]> {\n+  let summary = \"call operation\";\n+  let description = [{\n+    The `tt.call` operation represents a direct call to a function that is\n+    within the same symbol scope as the call. The operands and result types of\n+    the call must match the specified function type. The callee is encoded as a\n+    symbol reference attribute named \"callee\".\n+\n+    Example:\n+\n+    ```mlir\n+    %2 = tt.call @my_add(%0, %1) : (f32, f32) -> f32\n+    ```\n+  }];\n+\n+  let arguments = (ins FlatSymbolRefAttr:$callee, Variadic<AnyType>:$operands);\n+  let results = (outs Variadic<AnyType>);\n+\n+  let builders = [\n+    OpBuilder<(ins \"FuncOp\":$callee, CArg<\"ValueRange\", \"{}\">:$operands), [{\n+      $_state.addOperands(operands);\n+      $_state.addAttribute(\"callee\", SymbolRefAttr::get(callee));\n+      $_state.addTypes(callee.getFunctionType().getResults());\n+    }]>,\n+    OpBuilder<(ins \"SymbolRefAttr\":$callee, \"TypeRange\":$results,\n+      CArg<\"ValueRange\", \"{}\">:$operands), [{\n+      $_state.addOperands(operands);\n+      $_state.addAttribute(\"callee\", callee);\n+      $_state.addTypes(results);\n+    }]>,\n+    OpBuilder<(ins \"StringAttr\":$callee, \"TypeRange\":$results,\n+      CArg<\"ValueRange\", \"{}\">:$operands), [{\n+      build($_builder, $_state, SymbolRefAttr::get(callee), results, operands);\n+    }]>,\n+    OpBuilder<(ins \"StringRef\":$callee, \"TypeRange\":$results,\n+      CArg<\"ValueRange\", \"{}\">:$operands), [{\n+      build($_builder, $_state, StringAttr::get($_builder.getContext(), callee),\n+            results, operands);\n+    }]>];\n+\n+  let extraClassDeclaration = [{\n+    FunctionType getCalleeType() {\n+      return FunctionType::get(getContext(), getOperandTypes(), getResultTypes());\n+    }\n+\n+    /// Get the argument operands to the called function.\n+    operand_range getArgOperands() {\n+      return {arg_operand_begin(), arg_operand_end()};\n+    }\n+\n+    operand_iterator arg_operand_begin() { return operand_begin(); }\n+    operand_iterator arg_operand_end() { return operand_end(); }\n+\n+    /// Return the callee of this operation.\n+    CallInterfaceCallable getCallableForCallee() {\n+      return (*this)->getAttrOfType<SymbolRefAttr>(\"callee\");\n+    }\n+  }];\n+\n+  let assemblyFormat = [{\n+    $callee `(` $operands `)` attr-dict `:` functional-type($operands, results)\n+  }];\n+}\n+\n+def FuncOp : TT_Op<\"func\", [AffineScope, AutomaticAllocationScope, CallableOpInterface, FunctionOpInterface, IsolatedFromAbove, OpAsmOpInterface]> {\n+  let summary = \"An operation with a name containing a single `SSACFG` region\";\n+  let description = [{\n+    Operations within the function cannot implicitly capture values defined\n+    outside of the function, i.e. Functions are `IsolatedFromAbove`. All\n+    external references must use function arguments or attributes that establish\n+    a symbolic connection (e.g. symbols referenced by name via a string\n+    attribute like SymbolRefAttr). An external function declaration (used when\n+    referring to a function declared in some other module) has no body. While\n+    the MLIR textual form provides a nice inline syntax for function arguments,\n+    they are internally represented as \u201cblock arguments\u201d to the first block in\n+    the region.\n+\n+    Only dialect attribute names may be specified in the attribute dictionaries\n+    for function arguments, results, or the function itself.\n+\n+    Example:\n+\n+    ```mlir\n+    // External function definitions.\n+    tt.func @abort()\n+    tt.func @scribble(i32, i64, memref<? x 128 x f32, #layout_map0>) -> f64\n+\n+    // A function that returns its argument twice:\n+    tt.func @count(%x: i64) -> (i64, i64)\n+      attributes {fruit: \"banana\"} {\n+      return %x, %x: i64, i64\n+    }\n+\n+    // A function with an argument attribute\n+    tt.func @example_fn_arg(%x: i32 {swift.self = unit})\n+\n+    // A function with a result attribute\n+    tt.func @example_fn_result() -> (f64 {dialectName.attrName = 0 : i64})\n+\n+    // A function with an attribute\n+    tt.func @example_fn_attr() attributes {dialectName.attrName = false}\n+    ```\n+  }];\n+\n+  let arguments = (ins SymbolNameAttr:$sym_name,\n+                       TypeAttrOf<FunctionType>:$function_type,\n+                       OptionalAttr<StrAttr>:$sym_visibility,\n+                       OptionalAttr<DictArrayAttr>:$arg_attrs,\n+                       OptionalAttr<DictArrayAttr>:$res_attrs);\n+  let regions = (region AnyRegion:$body);\n+\n+  let builders = [OpBuilder<(ins\n+    \"StringRef\":$name, \"FunctionType\":$type,\n+    CArg<\"ArrayRef<NamedAttribute>\", \"{}\">:$attrs,\n+    CArg<\"ArrayRef<DictionaryAttr>\", \"{}\">:$argAttrs)\n+  >];\n+  let extraClassDeclaration = [{\n+    //===------------------------------------------------------------------===//\n+    // CallableOpInterface\n+    //===------------------------------------------------------------------===//\n+\n+    /// Returns the region on the current operation that is callable. This may\n+    /// return null in the case of an external callable object, e.g. an external\n+    /// function.\n+    ::mlir::Region *getCallableRegion() { return isExternal() ? nullptr : &getBody(); }\n+\n+    /// Returns the results types that the callable region produces when\n+    /// executed.\n+    ArrayRef<Type> getCallableResults() { return getFunctionType().getResults(); }\n+\n+    /// Returns the argument attributes for all callable region arguments or\n+    /// null if there are none.\n+    ::mlir::ArrayAttr getCallableArgAttrs() {\n+      return getArgAttrs().value_or(nullptr);\n+    }\n+\n+    /// Returns the result attributes for all callable region results or\n+    /// null if there are none.\n+    ::mlir::ArrayAttr getCallableResAttrs() {\n+      return getResAttrs().value_or(nullptr);\n+    }\n+\n+    //===------------------------------------------------------------------===//\n+    // FunctionOpInterface Methods\n+    //===------------------------------------------------------------------===//\n+\n+    /// Returns the argument types of this function.\n+    ArrayRef<Type> getArgumentTypes() { return getFunctionType().getInputs(); }\n+\n+    /// Returns the result types of this function.\n+    ArrayRef<Type> getResultTypes() { return getFunctionType().getResults(); }\n+\n+    //===------------------------------------------------------------------===//\n+    // SymbolOpInterface Methods\n+    //===------------------------------------------------------------------===//\n+\n+    bool isDeclaration() { return isExternal(); }\n+  }];\n+  let hasCustomAssemblyFormat = 1;\n+}\n+\n+def ReturnOp : TT_Op<\"return\", [Pure, HasParent<\"FuncOp\">, /*MemRefsNormalizable, */ReturnLike, Terminator]> {\n+  let summary = \"Function return operation\";\n+  let description = [{\n+    The `tt.return` operation represents a return operation within a function.\n+    The operation takes variable number of operands and produces no results.\n+    The operand number and types must match the signature of the function\n+    that contains the operation.\n+\n+    Example:\n+\n+    ```mlir\n+    tt.func @foo() : (i32, f8) {\n+      ...\n+      tt.return %0, %1 : i32, f8\n+    }\n+    ```\n+  }];\n+\n+  let arguments = (ins Variadic<AnyType>:$operands);\n+\n+  let builders = [OpBuilder<(ins), [{\n+    build($_builder, $_state, std::nullopt);\n+  }]>];\n+\n+  let assemblyFormat = \"attr-dict ($operands^ `:` type($operands))?\";\n+  let hasVerifier = 1;\n+}\n+\n #endif // Triton_OPS"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -77,7 +77,7 @@ AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n \n   if (blockArg && blockArg.getOwner()->isEntryBlock()) {\n     Operation *op = blockArg.getOwner()->getParentOp();\n-    if (auto fun = dyn_cast<func::FuncOp>(op))\n+    if (auto fun = dyn_cast<triton::FuncOp>(op))\n       initPessimisticStateFromFunc(blockArg.getArgNumber(), fun,\n                                    &knownContiguity, &knownDivisibility,\n                                    &knownConstancy);"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -74,7 +74,7 @@ void MembarAnalysis::visitTerminator(Operation *op,\n     return;\n   }\n   // Otherwise, it could be a return op\n-  assert(isa<func::ReturnOp>(op) && \"Unknown terminator\");\n+  assert(isa<triton::ReturnOp>(op) && \"Unknown terminator\");\n }\n \n void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -9,11 +9,11 @@ using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::triton::gpu::getElemsPerThread;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n-struct ReturnOpConversion : public ConvertOpToLLVMPattern<func::ReturnOp> {\n-  using ConvertOpToLLVMPattern<func::ReturnOp>::ConvertOpToLLVMPattern;\n+struct ReturnOpConversion : public ConvertOpToLLVMPattern<triton::ReturnOp> {\n+  using ConvertOpToLLVMPattern<triton::ReturnOp>::ConvertOpToLLVMPattern;\n \n   LogicalResult\n-  matchAndRewrite(func::ReturnOp op, OpAdaptor adaptor,\n+  matchAndRewrite(triton::ReturnOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     unsigned numArguments = op.getNumOperands();\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "file_content_changes": "@@ -4,7 +4,6 @@\n // TODO: refactor so that it doesn't fail if Allocation.h\n // is included after utility.h (due to conflict in `store` macro\n // and <atomic>\n-#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"triton/Analysis/Allocation.h\"\n \n #include \"TypeConverter.h\"\n@@ -41,12 +40,12 @@ void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n // TODO(Superjomn): remove the code when MLIR v15.0 is included.\n // All the rights are reserved by the LLVM community.\n \n-struct FuncOpConversionBase : public ConvertOpToLLVMPattern<func::FuncOp> {\n+struct FuncOpConversionBase : public ConvertOpToLLVMPattern<triton::FuncOp> {\n private:\n   /// Only retain those attributes that are not constructed by\n   /// `LLVMFuncOp::build`. If `filterArgAttrs` is set, also filter out argument\n   /// attributes.\n-  static void filterFuncAttributes(func::FuncOp op, bool filterArgAttrs,\n+  static void filterFuncAttributes(triton::FuncOp op, bool filterArgAttrs,\n                                    SmallVectorImpl<NamedAttribute> &result) {\n \n     for (const auto &attr : op->getAttrs()) {\n@@ -66,12 +65,12 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<func::FuncOp> {\n   }\n \n protected:\n-  using ConvertOpToLLVMPattern<func::FuncOp>::ConvertOpToLLVMPattern;\n+  using ConvertOpToLLVMPattern<triton::FuncOp>::ConvertOpToLLVMPattern;\n \n   // Convert input FuncOp to LLVMFuncOp by using the LLVMTypeConverter provided\n   // to this legalization pattern.\n   LLVM::LLVMFuncOp\n-  convertFuncOpToLLVMFuncOp(func::FuncOp funcOp,\n+  convertFuncOpToLLVMFuncOp(triton::FuncOp funcOp,\n                             ConversionPatternRewriter &rewriter) const {\n     // Convert the original function arguments. They are converted using the\n     // LLVMTypeConverter provided to this legalization pattern."}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "file_content_changes": "@@ -51,16 +51,15 @@ class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n     } else {\n       addLegalDialect<NVVM::NVVMDialect>();\n     }\n-    addIllegalOp<mlir::func::FuncOp>();\n     addLegalOp<mlir::UnrealizedConversionCastOp>();\n   }\n };\n \n-struct ReturnOpConversion : public ConvertOpToLLVMPattern<func::ReturnOp> {\n-  using ConvertOpToLLVMPattern<func::ReturnOp>::ConvertOpToLLVMPattern;\n+struct ReturnOpConversion : public ConvertOpToLLVMPattern<triton::ReturnOp> {\n+  using ConvertOpToLLVMPattern<triton::ReturnOp>::ConvertOpToLLVMPattern;\n \n   LogicalResult\n-  matchAndRewrite(func::ReturnOp op, OpAdaptor adaptor,\n+  matchAndRewrite(triton::ReturnOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     unsigned numArguments = op.getNumOperands();\n \n@@ -86,7 +85,7 @@ struct FuncOpConversion : public FuncOpConversionBase {\n       : FuncOpConversionBase(converter, benefit), numWarps(numWarps) {}\n \n   LogicalResult\n-  matchAndRewrite(func::FuncOp funcOp, OpAdaptor adaptor,\n+  matchAndRewrite(triton::FuncOp funcOp, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     auto newFuncOp = convertFuncOpToLLVMFuncOp(funcOp, rewriter);\n     if (!newFuncOp) {"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -163,8 +163,8 @@ void populateStdPatternsAndLegality(TritonGPUTypeConverter &typeConverter,\n   MLIRContext *context = patterns.getContext();\n   // Rewrite rule\n   patterns.add<StdSelectPattern>(typeConverter, context);\n-  target.addLegalOp<func::ReturnOp>(); // this is ok because all functions are\n-                                       // inlined by the frontend\n+  target.addLegalOp<triton::ReturnOp>(); // this is ok because all functions are\n+                                         // inlined by the frontend\n }\n \n void populateMathPatternsAndLegality(TritonGPUTypeConverter &typeConverter,\n@@ -721,15 +721,15 @@ class CFCondBranchPattern : public OpConversionPattern<cf::CondBranchOp> {\n   }\n };\n \n-class FuncOpPattern : public OpConversionPattern<func::FuncOp> {\n+class FuncOpPattern : public OpConversionPattern<triton::FuncOp> {\n public:\n-  using OpConversionPattern<func::FuncOp>::OpConversionPattern;\n+  using OpConversionPattern<triton::FuncOp>::OpConversionPattern;\n \n   LogicalResult\n-  matchAndRewrite(func::FuncOp op, OpAdaptor adaptor,\n+  matchAndRewrite(triton::FuncOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     auto converter = getTypeConverter();\n-    auto newOp = rewriter.replaceOpWithNewOp<func::FuncOp>(\n+    auto newOp = rewriter.replaceOpWithNewOp<triton::FuncOp>(\n         op, op.getName(), op.getFunctionType());\n     addNamedAttrs(newOp, adaptor.getAttributes());\n     rewriter.inlineRegionBefore(op.getBody(), newOp.getBody(),"}, {"filename": "lib/Dialect/Triton/IR/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -12,5 +12,4 @@ add_mlir_dialect_library(TritonIR\n   MLIRIR\n   MLIRArithDialect\n   MLIRSCFDialect\n-  MLIRFuncDialect\n )"}, {"filename": "lib/Dialect/Triton/IR/Dialect.cpp", "status": "modified", "additions": 36, "deletions": 0, "changes": 36, "file_content_changes": "@@ -1,6 +1,7 @@\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n \n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n #include \"triton/Dialect/Triton/IR/AttrInterfaces.h.inc\"\n #include \"llvm/ADT/StringSwitch.h\"\n #include \"llvm/ADT/TypeSwitch.h\"\n@@ -21,6 +22,10 @@ using namespace mlir::triton;\n namespace {\n struct TritonInlinerInterface : public DialectInlinerInterface {\n   using DialectInlinerInterface::DialectInlinerInterface;\n+  bool isLegalToInline(Operation *call, Operation *callable,\n+                       bool wouldBeCloned) const final {\n+    return true;\n+  }\n   bool isLegalToInline(Region *dest, Region *src, bool wouldBeCloned,\n                        IRMapping &valueMapping) const final {\n     return true;\n@@ -29,6 +34,37 @@ struct TritonInlinerInterface : public DialectInlinerInterface {\n                        IRMapping &) const final {\n     return true;\n   }\n+  //===--------------------------------------------------------------------===//\n+  // Transformation Hooks\n+  //===--------------------------------------------------------------------===//\n+\n+  /// Handle the given inlined terminator by replacing it with a new operation\n+  /// as necessary.\n+  void handleTerminator(Operation *op, Block *newDest) const final {\n+    // Only return needs to be handled here.\n+    auto returnOp = dyn_cast<triton::ReturnOp>(op);\n+    if (!returnOp)\n+      return;\n+\n+    // Replace the return with a branch to the dest.\n+    OpBuilder builder(op);\n+    builder.create<mlir::cf::BranchOp>(op->getLoc(), newDest,\n+                                       returnOp.getOperands());\n+    op->erase();\n+  }\n+\n+  /// Handle the given inlined terminator by replacing it with a new operation\n+  /// as necessary.\n+  void handleTerminator(Operation *op,\n+                        ArrayRef<Value> valuesToRepl) const final {\n+    // Only return needs to be handled here.\n+    auto returnOp = cast<triton::ReturnOp>(op);\n+\n+    // Replace the values directly with the return operands.\n+    assert(returnOp.getNumOperands() == valuesToRepl.size());\n+    for (const auto &it : llvm::enumerate(returnOp.getOperands()))\n+      valuesToRepl[it.index()].replaceAllUsesWith(it.value());\n+  }\n };\n } // namespace\n "}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 102, "deletions": 0, "changes": 102, "file_content_changes": "@@ -1,6 +1,8 @@\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/FunctionImplementation.h\"\n+#include \"mlir/IR/FunctionInterfaces.h\"\n #include \"mlir/IR/OperationSupport.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n@@ -516,5 +518,105 @@ void MakeTensorPtrOp::build(::mlir::OpBuilder &builder,\n                builder.getDenseI32ArrayAttr(order));\n }\n \n+// The following ops, including `call`, `func`, and `return` are copied and\n+// modified from\n+// https://github.com/llvm/llvm-project/blob/main/mlir/lib/Dialect/Func/IR/FuncOps.cpp\n+// We could revert it back once MLIR has a better inliner interface.\n+//-- FuncOp --\n+void triton::FuncOp::build(OpBuilder &builder, OperationState &state,\n+                           StringRef name, FunctionType type,\n+                           ArrayRef<NamedAttribute> attrs,\n+                           ArrayRef<DictionaryAttr> argAttrs) {\n+  state.addAttribute(SymbolTable::getSymbolAttrName(),\n+                     builder.getStringAttr(name));\n+  state.addAttribute(getFunctionTypeAttrName(state.name), TypeAttr::get(type));\n+  state.attributes.append(attrs.begin(), attrs.end());\n+  state.addRegion();\n+\n+  if (argAttrs.empty())\n+    return;\n+  assert(type.getNumInputs() == argAttrs.size());\n+  function_interface_impl::addArgAndResultAttrs(\n+      builder, state, argAttrs, /*resultAttrs=*/std::nullopt,\n+      getArgAttrsAttrName(state.name), getResAttrsAttrName(state.name));\n+}\n+\n+ParseResult triton::FuncOp::parse(OpAsmParser &parser, OperationState &result) {\n+  auto buildFuncType =\n+      [](Builder &builder, ArrayRef<Type> argTypes, ArrayRef<Type> results,\n+         function_interface_impl::VariadicFlag,\n+         std::string &) { return builder.getFunctionType(argTypes, results); };\n+\n+  return function_interface_impl::parseFunctionOp(\n+      parser, result, /*allowVariadic=*/false,\n+      getFunctionTypeAttrName(result.name), buildFuncType,\n+      getArgAttrsAttrName(result.name), getResAttrsAttrName(result.name));\n+}\n+\n+void triton::FuncOp::print(OpAsmPrinter &printer) {\n+  function_interface_impl::printFunctionOp(\n+      printer, *this, /*isVariadic=*/false, getFunctionTypeAttrName(),\n+      getArgAttrsAttrName(), getResAttrsAttrName());\n+}\n+\n+// -- CallOp --\n+LogicalResult\n+triton::CallOp::verifySymbolUses(mlir::SymbolTableCollection &symbolTable) {\n+  // Check that the callee attribute was specified.\n+  auto fnAttr = (*this)->getAttrOfType<FlatSymbolRefAttr>(\"callee\");\n+  if (!fnAttr)\n+    return emitOpError(\"requires a 'callee' symbol reference attribute\");\n+  FuncOp fn = symbolTable.lookupNearestSymbolFrom<FuncOp>(*this, fnAttr);\n+  if (!fn)\n+    return emitOpError() << \"'\" << fnAttr.getValue()\n+                         << \"' does not reference a valid function\";\n+\n+  // Verify that the operand and result types match the callee.\n+  auto fnType = fn.getFunctionType();\n+  if (fnType.getNumInputs() != getNumOperands())\n+    return emitOpError(\"incorrect number of operands for callee\");\n+\n+  for (unsigned i = 0, e = fnType.getNumInputs(); i != e; ++i)\n+    if (getOperand(i).getType() != fnType.getInput(i))\n+      return emitOpError(\"operand type mismatch: expected operand type \")\n+             << fnType.getInput(i) << \", but provided \"\n+             << getOperand(i).getType() << \" for operand number \" << i;\n+\n+  if (fnType.getNumResults() != getNumResults())\n+    return emitOpError(\"incorrect number of results for callee\");\n+\n+  for (unsigned i = 0, e = fnType.getNumResults(); i != e; ++i)\n+    if (getResult(i).getType() != fnType.getResult(i)) {\n+      auto diag = emitOpError(\"result type mismatch at index \") << i;\n+      diag.attachNote() << \"      op result types: \" << getResultTypes();\n+      diag.attachNote() << \"function result types: \" << fnType.getResults();\n+      return diag;\n+    }\n+\n+  return success();\n+}\n+\n+// -- ReturnOp --\n+LogicalResult triton::ReturnOp::verify() {\n+  auto function = cast<triton::FuncOp>((*this)->getParentOp());\n+\n+  // The operand number and types must match the function signature.\n+  const auto &results = function.getFunctionType().getResults();\n+  if (getNumOperands() != results.size())\n+    return emitOpError(\"has \")\n+           << getNumOperands() << \" operands, but enclosing function (@\"\n+           << function.getName() << \") returns \" << results.size();\n+\n+  for (unsigned i = 0, e = results.size(); i != e; ++i)\n+    if (getOperand(i).getType() != results[i])\n+      return emitError() << \"type of return operand \" << i << \" (\"\n+                         << getOperand(i).getType()\n+                         << \") doesn't match function result type (\"\n+                         << results[i] << \")\"\n+                         << \" in function @\" << function.getName();\n+\n+  return success();\n+}\n+\n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 11, "deletions": 12, "changes": 23, "file_content_changes": "@@ -81,18 +81,17 @@ TritonGPUConversionTarget::TritonGPUConversionTarget(\n                scf::ReduceReturnOp>();\n \n   addDynamicallyLegalDialect<arith::ArithDialect, math::MathDialect,\n-                             func::FuncDialect, triton::TritonDialect,\n-                             cf::ControlFlowDialect, scf::SCFDialect>(\n-      [&](Operation *op) {\n-        bool hasLegalRegions = true;\n-        for (auto &region : op->getRegions()) {\n-          hasLegalRegions = hasLegalRegions && typeConverter.isLegal(&region);\n-        }\n-        if (hasLegalRegions && typeConverter.isLegal(op)) {\n-          return true;\n-        }\n-        return false;\n-      });\n+                             triton::TritonDialect, cf::ControlFlowDialect,\n+                             scf::SCFDialect>([&](Operation *op) {\n+    bool hasLegalRegions = true;\n+    for (auto &region : op->getRegions()) {\n+      hasLegalRegions = hasLegalRegions && typeConverter.isLegal(&region);\n+    }\n+    if (hasLegalRegions && typeConverter.isLegal(op)) {\n+      return true;\n+    }\n+    return false;\n+  });\n \n   // We have requirements for the data layouts\n   addDynamicallyLegalOp<triton::DotOp>([](triton::DotOp dotOp) -> bool {"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 26, "deletions": 26, "changes": 52, "file_content_changes": "@@ -208,7 +208,7 @@ void init_triton_ir(py::module &&m) {\n                std::string attrName = name + \"_arg\" + std::to_string(id);\n                mlir::Block *owner = arg.getOwner();\n                if (owner->isEntryBlock() &&\n-                   !mlir::isa<mlir::func::FuncOp>(owner->getParentOp())) {\n+                   !mlir::isa<mlir::triton::FuncOp>(owner->getParentOp())) {\n                  owner->getParentOp()->setAttr(attrName, attr);\n                }\n              }\n@@ -361,7 +361,7 @@ void init_triton_ir(py::module &&m) {\n              return str;\n            })\n       .def(\"push_back\",\n-           [](mlir::ModuleOp &self, mlir::func::FuncOp &funcOp) -> void {\n+           [](mlir::ModuleOp &self, mlir::triton::FuncOp &funcOp) -> void {\n              self.push_back(funcOp);\n            })\n       .def(\"has_function\",\n@@ -372,13 +372,14 @@ void init_triton_ir(py::module &&m) {\n            })\n       .def(\"get_function\",\n            [](mlir::ModuleOp &self,\n-              std::string &funcName) -> mlir::func::FuncOp {\n-             return self.lookupSymbol<mlir::func::FuncOp>(funcName);\n+              std::string &funcName) -> mlir::triton::FuncOp {\n+             return self.lookupSymbol<mlir::triton::FuncOp>(funcName);\n            })\n       .def(\"get_single_function\",\n-           [](mlir::ModuleOp &self) -> mlir::func::FuncOp {\n-             llvm::SmallVector<mlir::func::FuncOp> funcs;\n-             self.walk([&](mlir::func::FuncOp func) { funcs.push_back(func); });\n+           [](mlir::ModuleOp &self) -> mlir::triton::FuncOp {\n+             llvm::SmallVector<mlir::triton::FuncOp> funcs;\n+             self.walk(\n+                 [&](mlir::triton::FuncOp func) { funcs.push_back(func); });\n              if (funcs.size() != 1)\n                throw std::runtime_error(\"Expected a single function\");\n              return funcs[0];\n@@ -400,12 +401,11 @@ void init_triton_ir(py::module &&m) {\n         // initialize registry\n         // note: we initialize llvm for undef\n         mlir::DialectRegistry registry;\n-        registry.insert<mlir::triton::TritonDialect,\n-                        mlir::triton::gpu::TritonGPUDialect,\n-                        mlir::math::MathDialect, mlir::arith::ArithDialect,\n-                        mlir::index::IndexDialect, mlir::func::FuncDialect,\n-                        mlir::scf::SCFDialect, mlir::cf::ControlFlowDialect,\n-                        mlir::LLVM::LLVMDialect>();\n+        registry.insert<\n+            mlir::triton::TritonDialect, mlir::triton::gpu::TritonGPUDialect,\n+            mlir::math::MathDialect, mlir::arith::ArithDialect,\n+            mlir::index::IndexDialect, mlir::scf::SCFDialect,\n+            mlir::cf::ControlFlowDialect, mlir::LLVM::LLVMDialect>();\n         context.appendDialectRegistry(registry);\n         context.loadAllAvailableDialects();\n \n@@ -423,30 +423,30 @@ void init_triton_ir(py::module &&m) {\n       },\n       ret::take_ownership);\n \n-  py::class_<mlir::func::FuncOp, mlir::OpState>(m, \"function\")\n+  py::class_<mlir::triton::FuncOp, mlir::OpState>(m, \"function\")\n       // .def_property_readonly(\"attrs\", &ir::function::attrs)\n       // .def(\"add_attr\", &ir::function::add_attr);\n       .def(\"args\",\n-           [](mlir::func::FuncOp &self, unsigned idx) -> mlir::BlockArgument {\n+           [](mlir::triton::FuncOp &self, unsigned idx) -> mlir::BlockArgument {\n              return self.getArgument(idx);\n            })\n       .def(\n           \"add_entry_block\",\n-          [](mlir::func::FuncOp &self) -> mlir::Block * {\n+          [](mlir::triton::FuncOp &self) -> mlir::Block * {\n             return self.addEntryBlock();\n           },\n           ret::reference)\n       .def(\n           \"set_arg_attr\",\n-          [](mlir::func::FuncOp &self, int arg_no, const std::string &name,\n+          [](mlir::triton::FuncOp &self, int arg_no, const std::string &name,\n              int val) {\n             // set arg attributes \"name\" to value \"val\"\n             auto attrTy = mlir::IntegerType::get(self.getContext(), 32);\n             self.setArgAttr(arg_no, name, mlir::IntegerAttr::get(attrTy, val));\n           },\n           ret::reference)\n-      .def_property_readonly(\"type\", &mlir::func::FuncOp::getFunctionType)\n-      .def(\"reset_type\", &mlir::func::FuncOp::setType);\n+      .def_property_readonly(\"type\", &mlir::triton::FuncOp::getFunctionType)\n+      .def(\"reset_type\", &mlir::triton::FuncOp::setType);\n \n   py::class_<mlir::OpBuilder::InsertPoint>(m, \"InsertPoint\");\n \n@@ -463,13 +463,13 @@ void init_triton_ir(py::module &&m) {\n       .def(\"ret\",\n            [](mlir::OpBuilder &self, std::vector<mlir::Value> &vals) -> void {\n              auto loc = self.getUnknownLoc();\n-             self.create<mlir::func::ReturnOp>(loc, vals);\n+             self.create<mlir::triton::ReturnOp>(loc, vals);\n            })\n       .def(\"call\",\n-           [](mlir::OpBuilder &self, mlir::func::FuncOp &func,\n+           [](mlir::OpBuilder &self, mlir::triton::FuncOp &func,\n               std::vector<mlir::Value> &args) -> mlir::OpState {\n              auto loc = self.getUnknownLoc();\n-             return self.create<mlir::func::CallOp>(loc, func, args);\n+             return self.create<mlir::triton::CallOp>(loc, func, args);\n            })\n       // insertion block/point\n       .def(\"set_insertion_point_to_start\",\n@@ -651,16 +651,16 @@ void init_triton_ir(py::module &&m) {\n       .def(\"get_or_insert_function\",\n            [](mlir::OpBuilder &self, mlir::ModuleOp &module,\n               std::string &funcName, mlir::Type &funcType,\n-              std::string &visibility) -> mlir::func::FuncOp {\n+              std::string &visibility) -> mlir::triton::FuncOp {\n              if (mlir::Operation *funcOperation = module.lookupSymbol(funcName))\n-               return llvm::dyn_cast<mlir::func::FuncOp>(funcOperation);\n+               return llvm::dyn_cast<mlir::triton::FuncOp>(funcOperation);\n              auto loc = self.getUnknownLoc();\n              if (auto funcTy = funcType.dyn_cast<mlir::FunctionType>()) {\n                llvm::SmallVector<mlir::NamedAttribute> attrs = {\n                    mlir::NamedAttribute(self.getStringAttr(\"sym_visibility\"),\n                                         self.getStringAttr(visibility))};\n-               return self.create<mlir::func::FuncOp>(loc, funcName, funcTy,\n-                                                      attrs);\n+               return self.create<mlir::triton::FuncOp>(loc, funcName, funcTy,\n+                                                        attrs);\n              }\n              throw std::runtime_error(\"invalid function type\");\n            })"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -2238,7 +2238,7 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n #dst = {dst_layout}\n \"\"\" + \"\"\"\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func.func public @kernel_0d1d(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n+  tt.func public @kernel_0d1d(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n     %cst = arith.constant dense<128> : tensor<128x1xi32, #src>\n     %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #src}>>\n     %1 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #src}>>\n@@ -2256,7 +2256,7 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n     %13 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n     %14 = tt.addptr %3, %12 : tensor<128x128x!tt.ptr<f16>, #dst>, tensor<128x128xi32, #dst>\n     tt.store %14, %13 : tensor<128x128xf16, #dst>\n-    return\n+    tt.return\n   }\n }\n \"\"\""}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -267,14 +267,14 @@ def make_hash(fn, **kwargs):\n     return hashlib.md5((Path(fn).read_text() + triton.runtime.jit.version_key()).encode(\"utf-8\")).hexdigest()\n \n \n-# - ^\\s*func\\.func\\s+ : match the start of the string, any leading whitespace, the keyword func,\n+# - ^\\s*tt\\.func\\s+ : match the start of the string, any leading whitespace, the keyword func,\n #    and any following whitespace\n # - (public\\s+)? : optionally match the keyword public and any following whitespace\n # - (@\\w+) : match an @ symbol followed by one or more word characters\n #   (letters, digits, or underscores), and capture it as group 1 (the function name)\n # - (\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\)) : match a pair of parentheses enclosing\n #   zero or more arguments separated by commas, and capture it as group 2 (the argument list)\n-mlir_prototype_pattern = r'^\\s*func\\.func\\s+(?:public\\s+)?(@\\w+)(\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\))\\s*\\{\\s*$'\n+mlir_prototype_pattern = r'^\\s*tt\\.func\\s+(?:public\\s+)?(@\\w+)(\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\))\\s*\\{\\s*$'\n ptx_prototype_pattern = r\"\\.(?:visible|extern)\\s+\\.(?:entry|func)\\s+(\\w+)\\s*\\(([^)]*)\\)\"\n prototype_pattern = {\n     \"ttir\": mlir_prototype_pattern,"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 26, "deletions": 26, "changes": 52, "file_content_changes": "@@ -11,7 +11,7 @@\n \n // CHECK-LABEL: matmul_loop\n // There shouldn't be any aliasing with the dot op encoding.\n-func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n   %a_mask = arith.constant dense<true> : tensor<128x32xi1, #AL>\n@@ -32,38 +32,38 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: alloc\n-func.func @alloc(%A : !tt.ptr<f16>) {\n+tt.func @alloc(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n   // CHECK: %0 -> %0\n   %cst2 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: convert\n-func.func @convert(%A : !tt.ptr<f16>) {\n+tt.func @convert(%A : !tt.ptr<f16>) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   // CHECK: %0 -> %0\n   %cst1 = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: trans\n-func.func @trans(%A : !tt.ptr<f16>) {\n+tt.func @trans(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %tensor = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n   // CHECK: %0 -> %cst\n   %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: insert_slice_async\n-func.func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n@@ -72,11 +72,11 @@ func.func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %index = arith.constant 0 : i32\n   // CHECK: %2 -> %cst_0\n   %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: insert_slice\n-func.func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n@@ -86,21 +86,21 @@ func.func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a = tt.load %a_ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #AL>\n   // CHECK: %inserted_slice -> %cst_0\n   %b = tensor.insert_slice %a into %tensor[%index, 0, 0][1, 16, 16][1, 1, 1]: tensor<16x16xf16, #AL> into tensor<1x16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: extract_slice\n-func.func @extract_slice(%A : !tt.ptr<f16>) {\n+tt.func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n   // CHECK-NEXT: %0 -> %cst\n   %cst1 = triton_gpu.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: if_cat\n-func.func @if_cat(%i1 : i1) {\n+tt.func @if_cat(%i1 : i1) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK: %cst_0 -> %cst_0\n@@ -115,11 +115,11 @@ func.func @if_cat(%i1 : i1) {\n     %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield %b : tensor<32x16xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: if_alias\n-func.func @if_alias(%i1 : i1) {\n+tt.func @if_alias(%i1 : i1) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n@@ -130,11 +130,11 @@ func.func @if_alias(%i1 : i1) {\n   } else {\n     scf.yield %cst1 : tensor<16x16xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: for\n-func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n@@ -150,11 +150,11 @@ func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B :\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: for_if\n-func.func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: %cst -> %cst\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n@@ -176,11 +176,11 @@ func.func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n     }\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: for_for_if\n-func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: %cst -> %cst\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n@@ -211,11 +211,11 @@ func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>\n     }\n     scf.yield %a_shared, %b_shared, %c_shared_next : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: cf_for\n-func.func @cf_for(%arg0: index, %arg1: index, %arg2: index, %arg3: !tt.ptr<f16>, %arg4: !tt.ptr<f16>) {\n+tt.func @cf_for(%arg0: index, %arg1: index, %arg2: index, %arg3: !tt.ptr<f16>, %arg4: !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %cst = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n@@ -242,5 +242,5 @@ func.func @cf_for(%arg0: index, %arg1: index, %arg2: index, %arg3: !tt.ptr<f16>,\n   gpu.barrier\n   // CHECK-NEXT: %9 -> %9\n   %9 = tt.cat %0, %0 {axis = 0 : i64} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n-  return\n+  tt.return\n }"}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 38, "deletions": 38, "changes": 76, "file_content_changes": "@@ -1,7 +1,7 @@\n // RUN: triton-opt %s -test-print-alignment -split-input-file -o %t 2>&1 | FileCheck %s\n \n // CHECK-LABEL: @cast\n-func.func @cast() {\n+tt.func @cast() {\n   // CHECK: contiguity = [1], divisibility = [1], constancy = [1], constant_value = 1\n   %cst = arith.constant 1 : i32\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = 1\n@@ -10,13 +10,13 @@ func.func @cast() {\n   %cst_tensor = arith.constant dense<1> : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n   %1 = tt.bitcast %cst_tensor : tensor<128xi32> -> tensor<128xi64>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @add\n-func.func @add() {\n+tt.func @add() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n@@ -27,13 +27,13 @@ func.func @add() {\n   %3 = arith.constant dense<127> : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [128], constant_value = 128\n   %4 = arith.addi %1, %3 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @addptr\n-func.func @addptr(%arg0: !tt.ptr<i1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<i16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}) {\n+tt.func @addptr(%arg0: !tt.ptr<i1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<i16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}) {\n   // CHECK: contiguity = [1], divisibility = [1], constancy = [1], constant_value = 1\n   %cst1 = arith.constant 1 : i32\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n@@ -84,13 +84,13 @@ func.func @addptr(%arg0: !tt.ptr<i1> {tt.divisibility = 16 : i32}, %arg1: !tt.pt\n   %21 = tt.addptr %16, %12 : tensor<128x128x!tt.ptr<i32>>, tensor<128x128xi32>\n   // CHECK-NEXT: contiguity = [1, 128], divisibility = [8, 16], constancy = [128, 1], constant_value = <none>\n   %22 = tt.addptr %17, %12 : tensor<128x128x!tt.ptr<i64>>, tensor<128x128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @sub\n-func.func @sub() {\n+tt.func @sub() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n@@ -101,13 +101,13 @@ func.func @sub() {\n   %3 = arith.constant dense<129> : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [128], constant_value = 128\n   %4 = arith.subi %3, %1 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @mul\n-func.func @mul() {\n+tt.func @mul() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n@@ -122,13 +122,13 @@ func.func @mul() {\n   %5 = arith.constant dense<2> : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [256], constancy = [128], constant_value = 256\n   %6 = arith.muli %4, %5 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @div\n-func.func @div() {\n+tt.func @div() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n@@ -153,14 +153,14 @@ func.func @div() {\n   %10 = tt.make_range {end = 8320 : i32, start = 8192 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [64], constant_value = <none>\n   %11 = arith.divsi %10, %4 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n \n // -----\n \n // CHECK-LABEL: @rem\n-func.func @rem() {\n+tt.func @rem() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 1\n@@ -179,35 +179,35 @@ func.func @rem() {\n   %7 = arith.constant dense<66> : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [2], divisibility = [2], constancy = [1], constant_value = <none>\n   %8 = arith.remui %0, %7 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @broadcast\n-func.func @broadcast() {\n+tt.func @broadcast() {\n   // CHECK: contiguity = [1], divisibility = [64], constancy = [128], constant_value = 64\n   %0 = arith.constant dense<64> : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1, 1], divisibility = [64, 1], constancy = [128, 1], constant_value = 64\n   %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n   // CHECK-NEXT: contiguity = [1, 1], divisibility = [64, 1], constancy = [128, 128], constant_value = 64\n   %2 = tt.broadcast %1 : (tensor<128x1xi32>) -> tensor<128x128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @splat\n-func.func @splat(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n+tt.func @splat(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   // CHECK: contiguity = [1, 1], divisibility = [16, 16], constancy = [128, 128], constant_value = <none>\n   %0 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @cmp\n-func.func @cmp() {\n+tt.func @cmp() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [4611686018427387904], constancy = [128], constant_value = 0\n@@ -226,13 +226,13 @@ func.func @cmp() {\n   %7 = arith.cmpi sgt, %0, %6 : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = 0\n   %8 = arith.cmpi sgt, %1, %6 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @logic\n-func.func @logic() {\n+tt.func @logic() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [64], constancy = [128], constant_value = 64\n@@ -255,13 +255,13 @@ func.func @logic() {\n   %9 = arith.ori %2, %4 : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [8], constant_value = <none>\n   %10 = arith.xori %2, %4 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @select\n-func.func @select() {\n+tt.func @select() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [4611686018427387904], constancy = [128], constant_value = 0\n@@ -278,12 +278,12 @@ func.func @select() {\n   %5 = arith.select %4, %3, %7 : tensor<128xi1>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [128], constant_value = <none>\n   %8 = \"triton_gpu.select\"(%7, %3, %2) : (tensor<128xi1>, tensor<128xi1>, tensor<128xi1>) -> tensor<128xi1>\n-  return\n+  tt.return\n }\n \n // -----\n \n-func.func @shift() {\n+tt.func @shift() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [8], constancy = [128], constant_value = 8\n@@ -296,12 +296,12 @@ func.func @shift() {\n   %4 = arith.shrsi %0, %2 : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [128], constant_value = 128\n   %5 = arith.shli %1, %2 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n-func.func @max_min() {\n+tt.func @max_min() {\n   // CHECK: contiguity = [128], divisibility = [1073741824], constancy = [1], constant_value = <none>\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [128], divisibility = [64], constancy = [1], constant_value = <none>\n@@ -316,13 +316,13 @@ func.func @max_min() {\n   %5 = arith.constant dense<4> : tensor<128xi32>\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = 8\n   %6 = arith.maxsi %4, %5 : tensor<128xi32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @for\n-func.func @for() {\n+tt.func @for() {\n   // CHECK: contiguity = [1, 1], divisibility = [4611686018427387904, 4611686018427387904], constancy = [128, 32], constant_value = 0\n   %a_init = arith.constant dense<0> : tensor<128x32xi32>\n   // CHECK-NEXT: contiguity = [1, 1], divisibility = [1, 1], constancy = [128, 32], constant_value = 1\n@@ -343,13 +343,13 @@ func.func @for() {\n     // CHECK: contiguity = [1, 1], divisibility = [4, 4], constancy = [128, 32], constant_value = 4\n     scf.yield %b, %a, %c : tensor<128x32xi32>, tensor<128x32xi32>, tensor<128x32xi32>\n   }\n-  return\n+  tt.return\n }\n \n // -----\n \n // CHECK-LABEL: @permute_2d\n-func.func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n+tt.func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK: contiguity = [1, 1], divisibility = [1, 1], constancy = [128, 128], constant_value = 1\n   %cst = arith.constant dense<true> : tensor<128x128xi1>\n   // CHECK-NEXT: contiguity = [1, 1], divisibility = [1, 1], constancy = [1, 1], constant_value = <none>\n@@ -397,7 +397,7 @@ func.func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i\n   // CHECK-NEXT: contiguity = [1, 1], divisibility = [1, 1], constancy = [1, 1], constant_value = <none>\n   %20 = tt.load %10, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf32>\n   tt.store %19, %20, %cst : tensor<128x128xf32>\n-  return\n+  tt.return\n }\n \n // -----\n@@ -406,7 +406,7 @@ module {\n \n // This is a tiny test for verifying StoreOp-related alignment, It simply store a constant to a buffer.\n // CHECK-LABEL: @store_constant_align\n-func.func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n: i32 {tt.divisibility = 16 : i32}) {\n+tt.func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n   %pid = tt.get_program_id {axis = 0 : i32} : i32\n   // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [1], constant_value = 128\n@@ -430,7 +430,7 @@ func.func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}\n   // CHECK-NEXT: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n   %cst = arith.constant dense<0.0> : tensor<128xf32>\n   tt.store %5, %cst, %mask : tensor<128xf32>\n-  return\n+  tt.return\n }\n \n }\n@@ -440,7 +440,7 @@ func.func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}\n // This IR is dumped from vecadd test.\n // Note, the hint {tt.divisibility = 16 : i32} for %n_elements affects the alignment of mask.\n // CHECK-LABEL: @vecadd_mask_align_16\n-func.func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32 {tt.divisibility = 16 : i32}) {\n+tt.func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32 {tt.divisibility = 16 : i32}) {\n   %c64_i32 = arith.constant 64 : i32\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n   %1 = arith.muli %0, %c64_i32 : i32\n@@ -461,15 +461,15 @@ func.func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}\n   // CHECK: tt.addptr %{{.*}} => contiguity = [64], divisibility = [16], constancy = [1], constant_value = <none>\n   %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   tt.store %15, %13, %mask : tensor<64xf32>\n-  return\n+  tt.return\n }\n \n // -----\n \n // This IR is dumped from vecadd test.\n // Note, there is no divisibility hint for %n_elements, Triton should assume its divisibility to be 1 by default.\n // CHECK-LABEL: @vecadd_mask_align_1\n-func.func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n+tt.func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n   %c64_i32 = arith.constant 64 : i32\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n   %1 = arith.muli %0, %c64_i32 : i32\n@@ -489,5 +489,5 @@ func.func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n   %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   tt.store %15, %13, %10 : tensor<64xf32>\n-  return\n+  tt.return\n }"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 34, "deletions": 34, "changes": 68, "file_content_changes": "@@ -13,7 +13,7 @@\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // CHECK-LABEL: matmul_loop\n-func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n \n@@ -40,13 +40,13 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 4608\n }\n \n // Shared memory is available after a tensor's liveness range ends\n // CHECK-LABEL: reusable\n-func.func @reusable(%A : !tt.ptr<f16>) {\n+tt.func @reusable(%A : !tt.ptr<f16>) {\n   %cst1 = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n   %cst3 = arith.constant dense<true> : tensor<32x128xi1, #AL>\n@@ -69,7 +69,7 @@ func.func @reusable(%A : !tt.ptr<f16>) {\n   // CHECK-NEXT: offset = 0, size = 1152\n   %a4 = triton_gpu.convert_layout %a4_ : (tensor<32x128xf16, #AL>) -> tensor<32x128xf16, #B_DOT>\n   %c1 = tt.dot %a3, %a4, %c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 4608\n }\n \n@@ -78,7 +78,7 @@ func.func @reusable(%A : !tt.ptr<f16>) {\n // %cst1->%cst4\n // %cst3->%g->%h->%i\n // CHECK-LABEL: preallocate\n-func.func @preallocate(%A : !tt.ptr<f16>) {\n+tt.func @preallocate(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 512\n@@ -107,13 +107,13 @@ func.func @preallocate(%A : !tt.ptr<f16>) {\n   %h = tt.cat %d, %cst5 {axis = 0} : (tensor<64x16xf16, #A_SHARED>, tensor<64x16xf16, #A_SHARED>) -> tensor<128x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 2048, size = 4096\n   %i = tt.cat %f, %cst5 {axis = 0} : (tensor<64x16xf16, #A_SHARED>, tensor<64x16xf16, #A_SHARED>) -> tensor<128x16xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 12288\n }\n \n // Unused tensors are immediately released\n // CHECK-LABEL: unused\n-func.func @unused(%A : !tt.ptr<f16>) {\n+tt.func @unused(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 1024\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 0, size = 512\n@@ -122,13 +122,13 @@ func.func @unused(%A : !tt.ptr<f16>) {\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 1024\n   %a = tt.cat %cst1, %cst2 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK: size = 2048\n }\n \n // cst0 is alive through the entire function, it cannot be released before the end of the function\n // CHECK-LABEL: longlive\n-func.func @longlive(%A : !tt.ptr<f16>) {\n+tt.func @longlive(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n@@ -151,13 +151,13 @@ func.func @longlive(%A : !tt.ptr<f16>) {\n   %c = tt.cat %cst3, %cst4 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 1024\n   %d = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 2560\n }\n \n // This example triggers graph coloring with > 1 colors.\n // CHECK-LABEL: multi_color\n-func.func @multi_color(%A : !tt.ptr<f16>) {\n+tt.func @multi_color(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 64\n   %cst = arith.constant dense<0.000000e+00> : tensor<4x8xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1216, size = 32\n@@ -199,64 +199,64 @@ func.func @multi_color(%A : !tt.ptr<f16>) {\n   %cst_12 = arith.constant dense<0.000000e+00> : tensor<4x16xf16, #AL>\n   %cst_13 = arith.constant dense<0.000000e+00> : tensor<8x32xf16, #AL>\n   // CHECK-NEXT: size = 2656\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: alloc\n-func.func @alloc(%A : !tt.ptr<f16>) {\n+tt.func @alloc(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n   // CHECK-NEXT: offset = 0, size = 512\n   %cst2 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 512\n }\n \n // CHECK-LABEL: scratch\n-func.func @scratch() {\n+tt.func @scratch() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   // CHECK: scratch offset = 0, size = 512\n   %b = tt.reduce %cst0 {redOp = 1 : i32, axis = 0 : i32} : tensor<16x16xf16, #AL> -> tensor<16xf16, #sliceAd0>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 512\n }\n \n // CHECK-LABEL: trans\n-func.func @trans(%A : !tt.ptr<f16>) {\n+tt.func @trans(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 1024\n   %tensor = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n   %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: insert_slice_async\n-func.func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   // CHECK: offset = 0, size = 512\n   %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n   %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 512\n }\n \n // CHECK-LABEL: extract_slice\n-func.func @extract_slice(%A : !tt.ptr<f16>) {\n+tt.func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n   %cst1 = triton_gpu.extract_slice %cst0[%index, 0, 0][1, 16, 16][1,1,1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 512\n }\n \n // B0 -> (B1) -> B0\n // Memory used by B1 can be reused by B0.\n // CHECK-LABEL: if\n-func.func @if(%i1 : i1) {\n+tt.func @if(%i1 : i1) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n@@ -273,14 +273,14 @@ func.func @if(%i1 : i1) {\n   %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 1024\n   %a = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 2048\n }\n \n // B0 -> (B1) -> (B2) -> B0\n // Memory used by B0 cannot be reused by B1 or B2.\n // CHECK-LABEL: if_else\n-func.func @if_else(%i1 : i1) {\n+tt.func @if_else(%i1 : i1) {\n   // CHECK: offset = 0, size = 512\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n@@ -300,14 +300,14 @@ func.func @if_else(%i1 : i1) {\n   }\n   // CHECK-NEXT: offset = 1024, size = 1024\n   %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 3072\n }\n \n // Block arguments and yields are memory aliases that do not trigger a new\n // allocation.\n // CHECK-LABEL: for\n-func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 8192\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n@@ -317,12 +317,12 @@ func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B :\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 24576\n }\n \n // CHECK-LABEL: for_if_slice\n-func.func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: offset = 0, size = 8192\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n@@ -337,13 +337,13 @@ func.func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f1\n     }\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 24576\n }\n \n // c0 cannot be released in the loop\n // CHECK-LABEL: for_use_ancestor\n-func.func @for_use_ancestor(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @for_use_ancestor(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: offset = 0, size = 8192\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n@@ -356,14 +356,14 @@ func.func @for_use_ancestor(%lb : index, %ub : index, %step : index, %A : !tt.pt\n     %c1 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n     scf.yield %b_shared, %a_shared: tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 32768\n }\n \n // a_shared_init, b_shared_init, and c_shared_init's liveness ranges are span over the entire function before cst2.\n // So they cannot be reused by cst0 and cst1, but can be reused by cst2.\n // CHECK-LABEL: for_for_if\n-func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: offset = 0, size = 8192\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n@@ -387,7 +387,7 @@ func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>\n   }\n   // CHECK-NEXT: offset = 0, size = 8192\n   %cst2 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n-  return\n+  tt.return\n   // CHECK-NEXT: size = 40960\n }\n "}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 51, "deletions": 51, "changes": 102, "file_content_changes": "@@ -14,7 +14,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // CHECK-LABEL: matmul_loop\n // There shouldn't be any membar with the dot op encoding.\n-func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n \n@@ -38,11 +38,11 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: raw_single_block\n-func.func @raw_single_block(%A : !tt.ptr<f16>) {\n+tt.func @raw_single_block(%A : !tt.ptr<f16>) {\n   %cst1 = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n   %0 = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n@@ -51,11 +51,11 @@ func.func @raw_single_block(%A : !tt.ptr<f16>) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %3 = triton_gpu.convert_layout %2 : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: war_single_block\n-func.func @war_single_block(%A : !tt.ptr<f16>) {\n+tt.func @war_single_block(%A : !tt.ptr<f16>) {\n   %cst1 = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n   %0 = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n@@ -67,11 +67,11 @@ func.func @war_single_block(%A : !tt.ptr<f16>) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: %4 = triton_gpu.convert_layout\n   %4 = triton_gpu.convert_layout %1 : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: scratch\n-func.func @scratch() {\n+tt.func @scratch() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n@@ -80,11 +80,11 @@ func.func @scratch() {\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %1 = triton_gpu.convert_layout %0 : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n   %2 = tt.reduce %1 {redOp = 1 : i32, axis = 0 : i32} : tensor<32x16xf16, #AL> -> tensor<16xf16, #sliceAd0>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: async_wait\n-func.func @async_wait() {\n+tt.func @async_wait() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n@@ -93,21 +93,21 @@ func.func @async_wait() {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %1 = triton_gpu.convert_layout %0 : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: alloc\n-func.func @alloc() {\n+tt.func @alloc() {\n   %0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n   %1 = tt.cat %0, %0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %2 = triton_gpu.convert_layout %1 : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: extract_slice\n-func.func @extract_slice() {\n+tt.func @extract_slice() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n   %0 = triton_gpu.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n@@ -117,19 +117,19 @@ func.func @extract_slice() {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %2 = triton_gpu.convert_layout %1 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: trans\n-func.func @trans() {\n+tt.func @trans() {\n   // CHECK-NOT: gpu.barrier\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n   %b = tt.trans %cst0 : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: insert_slice_async_op\n-func.func @insert_slice_async_op(%A : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @insert_slice_async_op(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n@@ -142,11 +142,11 @@ func.func @insert_slice_async_op(%A : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %5 = tt.cat %4, %4 {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: insert_slice_op\n-func.func @insert_slice_op(%A : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @insert_slice_op(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n@@ -162,12 +162,12 @@ func.func @insert_slice_op(%A : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %5 = tt.cat %4, %4 {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // If branch inserted a barrier for %cst0 and %cst1, but else didn't, then the barrier should be inserted in the parent region\n // CHECK-LABEL: multi_blocks\n-func.func @multi_blocks(%i1 : i1) {\n+tt.func @multi_blocks(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n@@ -186,12 +186,12 @@ func.func @multi_blocks(%i1 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %2 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // Both branches inserted a barrier for %cst0 and %cst1, then the barrier doesn't need to be inserted in the parent region\n // CHECK-LABEL: multi_blocks_join_barrier\n-func.func @multi_blocks_join_barrier(%i1 : i1) {\n+tt.func @multi_blocks_join_barrier(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n@@ -206,12 +206,12 @@ func.func @multi_blocks_join_barrier(%i1 : i1) {\n     scf.yield\n   }\n   %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n-  return\n+  tt.return\n }\n \n // Read yielded tensor requires a barrier\n // CHECK-LABEL: multi_blocks_yield\n-func.func @multi_blocks_yield(%i1 : i1) {\n+tt.func @multi_blocks_yield(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %a = scf.if %i1 -> (tensor<32x16xf16, #A_SHARED>) {\n@@ -229,12 +229,12 @@ func.func @multi_blocks_yield(%i1 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %4 = tt.cat %a, %a {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // Even though the entry block doesn't have a barrier, the successors should have barriers\n // CHECK-LABEL: multi_blocks_entry_no_shared\n-func.func @multi_blocks_entry_no_shared(%i1 : i1) {\n+tt.func @multi_blocks_entry_no_shared(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   %a = scf.if %i1 -> (tensor<32x16xf16, #A_SHARED>) {\n     // CHECK: gpu.barrier\n@@ -251,12 +251,12 @@ func.func @multi_blocks_entry_no_shared(%i1 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %1 = tt.cat %a, %a {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // Conservatively add a barrier as if the branch (%i1) is never taken\n // CHECK-LABEL: multi_blocks_noelse\n-func.func @multi_blocks_noelse(%i1 : i1) {\n+tt.func @multi_blocks_noelse(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n@@ -268,12 +268,12 @@ func.func @multi_blocks_noelse(%i1 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %1 = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n-  return\n+  tt.return\n }\n \n // Conservatively add a barrier as if the branch (%i2) is never taken\n // CHECK-LABEL: multi_blocks_nested_scf\n-func.func @multi_blocks_nested_scf(%i1 : i1, %i2 : i1) {\n+tt.func @multi_blocks_nested_scf(%i1 : i1, %i2 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n@@ -293,11 +293,11 @@ func.func @multi_blocks_nested_scf(%i1 : i1, %i2 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %2 = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: for\n-func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n@@ -307,13 +307,13 @@ func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B :\n     %5 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n }\n \n // Although a_shared and b_shared are synced before entering the loop,\n // they are reassociated with aliases (c_shared) and thus require a barrier.\n // CHECK-LABEL: for_alias\n-func.func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK: gpu.barrier\n@@ -330,13 +330,13 @@ func.func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>,\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %9 = tt.cat %0, %0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // Although cst2 is not an argument of scf.yield, its memory is reused by cst1.\n // So we need a barrier both before and after cst1\n // CHECK-LABEL: for_reuse\n-func.func @for_reuse(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @for_reuse(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK: gpu.barrier\n@@ -355,11 +355,11 @@ func.func @for_reuse(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>,\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %9 = tt.cat %0, %0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: for_reuse_nested\n-func.func @for_reuse_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @for_reuse_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK: gpu.barrier\n@@ -381,12 +381,12 @@ func.func @for_reuse_nested(%lb : index, %ub : index, %step : index, %A : !tt.pt\n   // CHECK: gpu.barrier\n   // CHECK-NEXT:  tt.cat\n   %15 = tt.cat %0, %0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n-  return\n+  tt.return\n }\n \n // repeatedly write to the same shared memory addresses\n // CHECK-LABEL: for_for_if\n-func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n@@ -407,12 +407,12 @@ func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>\n     }\n     scf.yield %a_shared, %b_shared, %c_shared_next : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n }\n \n // c_block_next can either be converted from c_shared_init or c_shared_next_next\n // CHECK-LABEL: for_if_for\n-func.func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+tt.func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n@@ -438,11 +438,11 @@ func.func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>\n     %b_blocked_next = triton_gpu.convert_layout %b_shared: (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL>\n     scf.yield %a_shared, %b_shared, %c_shared_next_next : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: cf_if\n-func.func @cf_if(%i1 : i1) {\n+tt.func @cf_if(%i1 : i1) {\n   %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n   %cst_0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n   cf.cond_br %i1, ^bb1, ^bb2\n@@ -455,10 +455,10 @@ func.func @cf_if(%i1 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %1 = triton_gpu.convert_layout %cst : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<16x16xf16, #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>>\n-  return\n+  tt.return\n }\n \n-func.func @cf_if_else(%i1 : i1) {\n+tt.func @cf_if_else(%i1 : i1) {\n   %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n   %cst_0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n   cf.cond_br %i1, ^bb1, ^bb2\n@@ -479,23 +479,23 @@ func.func @cf_if_else(%i1 : i1) {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %4 = tt.cat %2, %2 {axis = 0 : i64} : (tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<64x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n-  return\n+  tt.return\n }\n \n-func.func @cf_if_else_return(%i1 : i1) {\n+tt.func @cf_if_else_return(%i1 : i1) {\n   %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n   %cst_0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n   cf.cond_br %i1, ^bb1, ^bb2\n ^bb1:  // pred: ^bb0\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %0 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n-  return\n+  tt.return\n ^bb2:  // pred: ^bb0\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: tt.cat\n   %1 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n-  return\n+  tt.return\n }\n \n }"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -1,6 +1,6 @@\n // RUN: triton-opt %s | FileCheck %s\n \n-func.func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n+tt.func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n   // scalar -> scalar\n   // CHECK:  i64 -> !tt.ptr<f32>\n   %0 = tt.int_to_ptr %scalar_i64 : i64 -> !tt.ptr<f32>\n@@ -32,10 +32,10 @@ func.func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i6\n   %7 = tt.ptr_to_int %tensor_ptr_1d : tensor<16x!tt.ptr<f32>> -> tensor<16xi64>\n   // CHECK: tensor<16xf32> to tensor<16xf16>\n   %8 = arith.truncf %tensor_f32_1d : tensor<16xf32> to tensor<16xf16>\n-  return\n+  tt.return\n }\n \n-func.func @addptr_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_i32: i32) {\n+tt.func @addptr_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_i32: i32) {\n   // scalar -> scalar\n   // CHECK: !tt.ptr<f32>\n   %0 = tt.addptr %scalar_ptr, %scalar_i32 : !tt.ptr<f32>, i32\n@@ -51,10 +51,10 @@ func.func @addptr_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_i32: i32) {\n   %tensor_i32_1d = tt.splat %scalar_i32 : (i32) -> tensor<16xi32>\n   // CHECK: tensor<16x!tt.ptr<f32>>\n   %2 = tt.addptr %tensor_ptr_1d, %tensor_i32_1d : tensor<16x!tt.ptr<f32>>, tensor<16xi32>\n-  return\n+  tt.return\n }\n \n-func.func @load_store_ops_scalar(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %mask : i1) {\n+tt.func @load_store_ops_scalar(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %mask : i1) {\n   // Test if Load/Store ops can handle scalar values\n   %other = arith.constant 0.0e+0 : f32\n \n@@ -73,10 +73,10 @@ func.func @load_store_ops_scalar(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}\n   tt.store %ptr, %b, %mask : f32\n   // CHECK: tt.store %{{.*}}, %[[L2]], %{{.*}} {cache = 1 : i32, evict = 1 : i32} : f32\n   tt.store %ptr, %c, %mask : f32\n-  return\n+  tt.return\n }\n \n-func.func @reduce_ops_infer(%ptr: !tt.ptr<f32>, %v : tensor<1x2x4xf32>) {\n+tt.func @reduce_ops_infer(%ptr: !tt.ptr<f32>, %v : tensor<1x2x4xf32>) {\n   // Test if reduce ops infer types correctly\n \n   // CHECK: %{{.*}} = tt.reduce %{{.*}} -> tensor<2x4xf32>\n@@ -98,10 +98,10 @@ func.func @reduce_ops_infer(%ptr: !tt.ptr<f32>, %v : tensor<1x2x4xf32>) {\n   tt.store %ptr1x2, %c : tensor<1x2xf32>\n   tt.store %ptr1, %e : tensor<1xf32>\n   tt.store %ptr, %g : f32\n-  return\n+  tt.return\n }\n \n-func.func @dot_ops_infer(%ptr: !tt.ptr<f32>, %v : f32) {\n+tt.func @dot_ops_infer(%ptr: !tt.ptr<f32>, %v : f32) {\n   // Test if reduce ops infer types correctly\n   %v128x32 = tt.splat %v : (f32) -> tensor<128x32xf32>\n   %v32x128 = tt.splat %v : (f32) -> tensor<32x128xf32>\n@@ -128,5 +128,5 @@ func.func @dot_ops_infer(%ptr: !tt.ptr<f32>, %v : f32) {\n   tt.store %ptr32x32, %r2 : tensor<32x32xf32>\n   tt.store %ptr128x128, %r3 : tensor<128x128xf32>\n   tt.store %ptr1x1, %r4 : tensor<1x1xf32>\n-  return\n+  tt.return\n }"}, {"filename": "test/Conversion/triton_to_tritongpu.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -1,17 +1,17 @@\n // RUN: triton-opt %s -split-input-file -convert-triton-to-tritongpu=num-warps=2 | FileCheck %s\n \n-func.func @ops() {\n+tt.func @ops() {\n   // CHECK: module attributes {\"triton_gpu.num-warps\" = 2 : i32} {{.*}}\n   %a = arith.constant dense<1.00e+00> : tensor<128x32xf16>\n   %b = arith.constant dense<2.00e+00> : tensor<32x128xf16>\n   %c = arith.constant dense<3.00e+00> : tensor<128x128xf32>\n   %0 = tt.dot %a, %b, %c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16> * tensor<32x128xf16> -> tensor<128x128xf32>\n-  return\n+  tt.return\n }\n \n // -----\n \n-func.func @load_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n+tt.func @load_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   // Test if LoadOp is lowered properly (see #771)\n   %ptrs = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n   %mask = arith.constant dense<true> : tensor<128xi1>\n@@ -25,12 +25,12 @@ func.func @load_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   tt.store %ptrs, %a : tensor<128xf32>\n   tt.store %ptrs, %b : tensor<128xf32>\n   tt.store %ptrs, %c : tensor<128xf32>\n-  return\n+  tt.return\n }\n \n // -----\n \n-func.func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n+tt.func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   // Test if the total number of threadsPerWarp is 32\n   // Test if the total number of warps is 2\n   // CHECK: #[[blocked0:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 2], order = [0, 1]}>\n@@ -49,5 +49,5 @@ func.func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   // CHECK: tensor<16x16xf32, #[[blocked2]]> -> tensor<16xf32, #triton_gpu.slice<{dim = 0, parent = #[[blocked2]]}>>\n   %c3_ = tt.reduce %c2 {redOp = 1 : i32, axis = 0 : i32} : tensor<16x16xf32> -> tensor<16xf32>\n \n-  return\n+  tt.return\n }"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 99, "deletions": 99, "changes": 198, "file_content_changes": "@@ -4,9 +4,9 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: llvm.func @test_empty_kernel(%arg0: i64, %arg1: !llvm.ptr<f16, 1>)\n   // Here the 128 comes from the 4 in module attribute multiples 32\n   // CHECK:  attributes {nvvm.kernel = 1 : ui1, nvvm.maxntid = [128 : i32]} {{.*}}\n-  func.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n+  tt.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n     // CHECK:  llvm.return\n-    return\n+    tt.return\n   }\n } // end module\n \n@@ -15,11 +15,11 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_load\n-  func.func @basic_load(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf32, #blocked0>) {\n+  tt.func @basic_load(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf32, #blocked0>) {\n     // CHECK: llvm.inline_asm\n     // CHECK: llvm.inline_asm\n     %1 = tt.load %a_ptr_init, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -28,13 +28,13 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: vectorized_load\n-  func.func @vectorized_load(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf32, #blocked0>) {\n+  tt.func @vectorized_load(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf32, #blocked0>) {\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ld.global.b32\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ld.global.b32\n     %1 = tt.load %a_ptr_init, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -43,13 +43,13 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: vectorized_load_f16\n-  func.func @vectorized_load_f16(%a_ptr_init: tensor<256x!tt.ptr<f16>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf16, #blocked0>) {\n+  tt.func @vectorized_load_f16(%a_ptr_init: tensor<256x!tt.ptr<f16>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf16, #blocked0>) {\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ld.global.b16\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ld.global.b16\n     %1 = tt.load %a_ptr_init, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf16, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -59,10 +59,10 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: masked_load_const_other\n-  func.func @masked_load_const_other(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>) {\n+  tt.func @masked_load_const_other(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>) {\n     %cst_0 = arith.constant dense<0.000000e+00> : tensor<256xf32, #blocked0>\n     %1 = tt.load %a_ptr_init, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -72,10 +72,10 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: masked_load_const_other_vec\n-  func.func @masked_load_const_other_vec(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>) {\n+  tt.func @masked_load_const_other_vec(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>) {\n     %cst_0 = arith.constant dense<0.000000e+00> : tensor<256xf32, #blocked0>\n     %1 = tt.load %a_ptr_init, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -84,7 +84,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   // CHECK-LABEL: global_load_store_no_vec\n-  func.func @global_load_store_no_vec(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+  tt.func @global_load_store_no_vec(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -127,7 +127,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     tt.store %13, %11 : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -136,7 +136,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   // CHECK-LABEL: global_load_store_vec4\n-  func.func @global_load_store_vec4(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n+  tt.func @global_load_store_vec4(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -163,7 +163,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     // Store 4 elements to global with single one vectorized store instruction\n     // CHECK: @$5 st.global.v4.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n     tt.store %13, %11 : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -173,7 +173,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n #blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n // Note, the %n_elements doesn't have a \"tt.divisibility\" hint, so Triton assumes it's divisibility is 1, this should effect the mask's alignment and further restrict the load/store ops' vector width to be 1.\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n-  func.func @vecadd_masked_vec1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n+  tt.func @vecadd_masked_vec1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n     %c64_i32 = arith.constant 64 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c64_i32 : i32\n@@ -194,7 +194,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n     %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>, #blocked>, tensor<64xi32, #blocked>\n     tt.store %15, %13, %10 : tensor<64xf32, #blocked>\n-    return\n+    tt.return\n   }\n }\n \n@@ -203,7 +203,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: global_load_store_vec2\n-    func.func @global_load_store_vec2(%arg0: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg3: i32) {\n+    tt.func @global_load_store_vec2(%arg0: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -239,7 +239,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: @${{.*}} st.global.v2.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}} };\n     // CHECK: @${{.*}} st.global.v2.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}} };\n     tt.store %13, %11 : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -248,7 +248,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: global_load_store_vec8\n-    func.func @global_load_store_vec8(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n+    tt.func @global_load_store_vec8(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -278,7 +278,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: @$5 st.global.v4.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n     // CHECK: @$5 st.global.v4.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n     tt.store %13, %11 : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -291,7 +291,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_view_broadcast\n-  func.func @basic_view_broadcast(%arg : tensor<256xf32,#blocked0>) {\n+  tt.func @basic_view_broadcast(%arg : tensor<256xf32,#blocked0>) {\n     // CHECK: llvm.mlir.undef\n     // CHECK: %[[T0:.*]] = llvm.extractvalue\n     // CHECK: %[[T1:.*]] = llvm.extractvalue\n@@ -306,7 +306,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK: llvm.insertvalue %[[T0]]\n     // CHECK: llvm.insertvalue %[[T1]]\n     %1 = tt.broadcast %0 : (tensor<256x1xf32,#blocked2>) -> tensor<256x4xf32, #blocked2>\n-    return\n+    tt.return\n   }\n }\n \n@@ -315,13 +315,13 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_make_range\n-  func.func @basic_make_range() {\n+  tt.func @basic_make_range() {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n     // CHECK: llvm.mlir.undef\n     // CHECK: llvm.insertvalue\n     // CHECK: llvm.insertvalue\n     %0 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -330,11 +330,11 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_addf\n-  func.func @basic_addf(%arg0 : tensor<256xf32,#blocked0>, %arg1 : tensor<256xf32,#blocked0>) {\n+  tt.func @basic_addf(%arg0 : tensor<256xf32,#blocked0>, %arg1 : tensor<256xf32,#blocked0>) {\n     // CHECK: llvm.fadd\n     // CHECK: llvm.fadd\n     %1 = arith.addf %arg0, %arg1 : tensor<256xf32,#blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -343,22 +343,22 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_addi\n-  func.func @basic_addi(%arg0 : tensor<256xi32,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n+  tt.func @basic_addi(%arg0 : tensor<256xi32,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n     // CHECK: llvm.add\n     // CHECK: llvm.add\n     %1 = arith.addi %arg0, %arg1 : tensor<256xi32,#blocked0>\n-    return\n+    tt.return\n   }\n }\n \n // -----\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_program_id\n-  func.func @basic_program_id() {\n+  tt.func @basic_program_id() {\n     // CHECK: nvvm.read.ptx.sreg.ctaid.x : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n-    return\n+    tt.return\n   }\n }\n \n@@ -367,11 +367,11 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_addptr\n-  func.func @basic_addptr(%arg0 : tensor<256x!tt.ptr<f32>,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n+  tt.func @basic_addptr(%arg0 : tensor<256x!tt.ptr<f32>,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n     // CHECK: llvm.getelementptr\n     // CHECK: llvm.getelementptr\n     %0 = tt.addptr %arg0, %arg1 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -381,14 +381,14 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem\n   // CHECK-LABEL: basic_alloc_tensor\n-  func.func @basic_alloc_tensor() {\n+  tt.func @basic_alloc_tensor() {\n     // CHECK: llvm.mlir.addressof @global_smem\n     // CHECK-NEXT: llvm.bitcast\n     // CHECK-NEXT: llvm.mlir.constant\n     // CHECK-NEXT: llvm.getelementptr\n     // CHECK-NEXT: llvm.bitcast\n     %0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #shared0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -398,7 +398,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem\n   // CHECK-LABEL: basic_extract_slice\n-  func.func @basic_extract_slice() {\n+  tt.func @basic_extract_slice() {\n     // CHECK: llvm.mlir.addressof @global_smem\n     // CHECK: llvm.extractvalue\n     // CHECK-NEXT: llvm.extractvalue\n@@ -423,18 +423,18 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %index = arith.constant 1 : i32\n     %0 = triton_gpu.alloc_tensor : tensor<128x16x32xf32, #shared0>\n     %1 = triton_gpu.extract_slice %0[%index, 0, 0][1, 16, 32][1, 1, 1] : tensor<128x16x32xf32, #shared0> to tensor<16x32xf32, #shared0>\n-    return\n+    tt.return\n   }\n }\n \n // -----\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_async_wait\n-  func.func @basic_async_wait() {\n+  tt.func @basic_async_wait() {\n     // CHECK: cp.async.wait_group 0x4\n     triton_gpu.async_wait {num = 4: i32}\n-    return\n+    tt.return\n   }\n }\n \n@@ -450,7 +450,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #A = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_fallback\n-  func.func @basic_insert_slice_async_fallback(%arg0: !tt.ptr<f16> {tt.divisibility = 1 : i32}) {\n+  tt.func @basic_insert_slice_async_fallback(%arg0: !tt.ptr<f16> {tt.divisibility = 1 : i32}) {\n     %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<64xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #slice2d1>) -> tensor<16x1xi32, #block2>\n@@ -473,7 +473,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<8xi32>, 3>\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x64x!tt.ptr<f16>, #AL> -> tensor<2x16x64xf16, #A>\n-    return\n+    tt.return\n   }\n }\n \n@@ -489,7 +489,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #A = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_v4\n-  func.func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 32 : i32}) {\n+  tt.func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 32 : i32}) {\n     %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<64xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #slice2d1>) -> tensor<16x1xi32, #block2>\n@@ -515,7 +515,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x64x!tt.ptr<f32>, #AL> -> tensor<2x16x64xf32, #A>\n     triton_gpu.async_commit_group\n-    return\n+    tt.return\n   }\n }\n \n@@ -531,7 +531,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #A = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_v1\n-  func.func @basic_insert_slice_async_v1(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n+  tt.func @basic_insert_slice_async_v1(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n     %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #slice2d1>) -> tensor<16x1xi32, #block2>\n@@ -561,7 +561,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x32x!tt.ptr<f32>, #AL> -> tensor<2x16x32xf32, #A>\n     triton_gpu.async_commit_group\n-    return\n+    tt.return\n   }\n }\n \n@@ -576,7 +576,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #A = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_v1_multictas\n-  func.func @basic_insert_slice_async_v1_multictas(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n+  tt.func @basic_insert_slice_async_v1_multictas(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n     %off0_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<32xi32, #slice2d1>) -> tensor<32x1xi32, #block2>\n@@ -618,7 +618,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32x!tt.ptr<f32>, #AL> -> tensor<2x32x32xf32, #A>\n     triton_gpu.async_commit_group\n-    return\n+    tt.return\n   }\n }\n \n@@ -627,12 +627,12 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: basic_splat\n-  func.func @basic_splat(%ptr: !tt.ptr<f32>) {\n+  tt.func @basic_splat(%ptr: !tt.ptr<f32>) {\n     // CHECK: llvm.mlir.undef\n     // CHECK: llvm.insertvalue\n     // CHECK: llvm.insertvalue\n     %0 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>,#blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -641,13 +641,13 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_store\n-  func.func @basic_store(%ptrs: tensor<256x!tt.ptr<f32>, #blocked0>, %vals: tensor<256xf32, #blocked0>, %mask: tensor<256xi1, #blocked0>) {\n+  tt.func @basic_store(%ptrs: tensor<256x!tt.ptr<f32>, #blocked0>, %vals: tensor<256xf32, #blocked0>, %mask: tensor<256xi1, #blocked0>) {\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     tt.store %ptrs, %vals, %mask : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -658,7 +658,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked\n-  func.func @convert_layout_blocked_blocked(%arg0: tensor<16x16xf32, #blocked0>) {\n+  tt.func @convert_layout_blocked_blocked(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n@@ -694,7 +694,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #blocked1>\n-    return\n+    tt.return\n   }\n }\n \n@@ -705,7 +705,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked_vec\n-  func.func @convert_layout_blocked_blocked_vec(%arg0: tensor<16x16xf32, #blocked0>) {\n+  tt.func @convert_layout_blocked_blocked_vec(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n@@ -717,7 +717,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #blocked1>\n-    return\n+    tt.return\n   }\n }\n \n@@ -728,7 +728,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked_multi_rep\n-  func.func @convert_layout_blocked_blocked_multi_rep(%arg0: tensor<16x16xf32, #blocked0>) {\n+  tt.func @convert_layout_blocked_blocked_multi_rep(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n@@ -746,7 +746,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #blocked1>\n-    return\n+    tt.return\n   }\n }\n \n@@ -759,7 +759,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_dot\n-  func.func @convert_dot(%A: tensor<16x16xf16, #blocked0>, %B: tensor<16x16xf16, #blocked0>) {\n+  tt.func @convert_dot(%A: tensor<16x16xf16, #blocked0>, %B: tensor<16x16xf16, #blocked0>) {\n     %AA = triton_gpu.convert_layout %A : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #shared0>\n     %BB = triton_gpu.convert_layout %B : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #shared0>\n     // CHECK: llvm.inline_asm\n@@ -776,14 +776,14 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK-SAME: mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\n     %D = tt.dot %AA_DOT, %BB_DOT, %cst0 {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n \n-    return\n+    tt.return\n   }\n }\n \n // TODO: problems in MLIR's parser on slice layout\n // #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n // module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-//   func.func @make_range_sliced_layout() {\n+//   tt.func @make_range_sliced_layout() {\n //     %0 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked0}>>\n //     return\n //   }\n@@ -796,7 +796,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mmav2_block\n-  func.func @convert_layout_mmav2_blocked(%arg0: tensor<32x16xf32, #mma>) {\n+  tt.func @convert_layout_mmav2_blocked(%arg0: tensor<32x16xf32, #mma>) {\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n     // CHECK: llvm.store\n@@ -805,7 +805,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<32x16xf32, #mma>) -> tensor<32x16xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -816,7 +816,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mmav1_block\n-  func.func @convert_layout_mmav1_blocked(%arg0: tensor<32x64xf32, #mma>) {\n+  tt.func @convert_layout_mmav1_blocked(%arg0: tensor<32x64xf32, #mma>) {\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n     // CHECK: llvm.store\n@@ -829,7 +829,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<32x64xf32, #mma>) -> tensor<32x64xf32, #blocked>\n-    return\n+    tt.return\n   }\n }\n \n@@ -839,13 +839,13 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_shared\n-  func.func @convert_layout_blocked_shared(%arg0: tensor<128x32xf32, #blocked0>) {\n+  tt.func @convert_layout_blocked_shared(%arg0: tensor<128x32xf32, #blocked0>) {\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<8xf32>, 3>\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<8xf32>, 3>\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -855,10 +855,10 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_blocked1d_to_slice0\n-  func.func @convert_blocked1d_to_slice0(%src:tensor<32xi32, #blocked0>) {\n+  tt.func @convert_blocked1d_to_slice0(%src:tensor<32xi32, #blocked0>) {\n     // CHECK-COUNT-4: llvm.load {{.*}} : !llvm.ptr<vector<1xi32>, 3>\n     %cvt = triton_gpu.convert_layout %src : (tensor<32xi32, #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>\n-    return\n+    tt.return\n   }\n }\n \n@@ -868,10 +868,10 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_blocked1d_to_slice1\n-  func.func @convert_blocked1d_to_slice1(%src:tensor<32xi32, #blocked0>) {\n+  tt.func @convert_blocked1d_to_slice1(%src:tensor<32xi32, #blocked0>) {\n     // CHECK-COUNT-32: llvm.load {{.*}} : !llvm.ptr<vector<1xi32>, 3>\n     %cvt = triton_gpu.convert_layout %src : (tensor<32xi32, #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n-    return\n+    tt.return\n   }\n }\n \n@@ -881,14 +881,14 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_blocked_to_blocked_ptr\n-  func.func @convert_blocked_to_blocked_ptr(%src:tensor<32x!tt.ptr<f32>, #blocked0>) {\n+  tt.func @convert_blocked_to_blocked_ptr(%src:tensor<32x!tt.ptr<f32>, #blocked0>) {\n     // CHECK: llvm.ptrtoint\n     // CHECK: llvm.store\n     // CHECK: nvvm.barrier0\n     // CHECK: llvm.inttoptr\n     // CHECK-COUNT-4: llvm.insertvalue\n     %cvt = triton_gpu.convert_layout %src : (tensor<32x!tt.ptr<f32>, #blocked0>) -> tensor<32x!tt.ptr<f32>, #blocked1>\n-    return\n+    tt.return\n   }\n }\n \n@@ -900,7 +900,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func.func @matmul_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  tt.func @matmul_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<128x32xf16, #shared>, %b:tensor<32x256xf16, #shared>) {\n     %cst = arith.constant dense<0.000000e+00> : tensor<128x256xf32, #mma>\n     // CHECK: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n@@ -913,7 +913,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #blocked>\n     %36 = tt.broadcast %30 : (tensor<128x1x!tt.ptr<f32>, #blocked>) -> tensor<128x256x!tt.ptr<f32>, #blocked>\n     tt.store %36, %38 : tensor<128x256xf32, #blocked>\n-    return\n+    tt.return\n   }\n }\n \n@@ -926,7 +926,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma, isMMAv1Row=true}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma, isMMAv1Row=true}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func.func @matmul884_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  tt.func @matmul884_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<32x64xf16, #shared0>, %b:tensor<64x64xf16, #shared1>) {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x64xf32, #mma>\n     // CHECK: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n@@ -938,7 +938,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n     %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x64x!tt.ptr<f32>, #blocked>\n     tt.store %36, %38 : tensor<32x64xf32, #blocked>\n-    return\n+    tt.return\n   }\n }\n \n@@ -949,7 +949,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#blocked}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#blocked}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func.func @matmul_fmadot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  tt.func @matmul_fmadot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #blocked>\n     // CHECK: llvm.intr.fmuladd\n@@ -960,7 +960,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n     %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n     tt.store %36, %28 : tensor<32x32xf32, #blocked>\n-    return\n+    tt.return\n   }\n }\n \n@@ -973,7 +973,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: matmul_tf32dot\n-  func.func @matmul_tf32dot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  tt.func @matmul_tf32dot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n     // CHECK: llvm.inline_asm\n@@ -999,7 +999,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n     %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n     tt.store %36, %38 : tensor<32x32xf32, #blocked>\n-    return\n+    tt.return\n   }\n }\n \n@@ -1008,27 +1008,27 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32\n-  func.func @atomic_add_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xi1, #blocked0>, %arg2 : tensor<256xf32, #blocked0>) {\n+  tt.func @atomic_add_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xi1, #blocked0>, %arg2 : tensor<256xf32, #blocked0>) {\n     // CHECK: llvm.icmp \"slt\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$3 atom.global.gpu.add.f32\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$3 atom.global.gpu.add.f32\n     %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xf32, #blocked0>, tensor<256xi1, #blocked0>) -> tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n // -----\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32_scalar\n-  func.func @atomic_add_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : i1, %arg2 : f32) {\n+  tt.func @atomic_add_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : i1, %arg2 : f32) {\n     // CHECK: llvm.icmp \"eq\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$3 atom.global.gpu.add.f32\n     %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (!tt.ptr<f32>, f32, i1) -> f32\n-    return\n+    tt.return\n   }\n }\n \n@@ -1037,27 +1037,27 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: store_f32\n-  func.func @store_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xf32, #blocked0>) {\n+  tt.func @store_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xf32, #blocked0>) {\n     // CHECK: llvm.icmp \"slt\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$2 st.global.b32\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$2 st.global.b32\n     tt.store %arg0, %arg1 : tensor<256xf32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n // -----\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: store_f32_scalar\n-  func.func @store_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : f32) {\n+  tt.func @store_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : f32) {\n     // CHECK: llvm.icmp \"slt\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$2 st.global.b32\n     tt.store %arg0, %arg1 : f32\n-    return\n+    tt.return\n   }\n }\n \n@@ -1066,7 +1066,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n-func.func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n+tt.func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n   %blockidx = tt.get_program_id {axis=0:i32} : i32\n   %blockidy = tt.get_program_id {axis=1:i32} : i32\n   %blockidz = tt.get_program_id {axis=2:i32} : i32\n@@ -1078,15 +1078,15 @@ func.func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n   %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n   tt.store %a, %0 : tensor<32xi32, #blocked0>\n \n-  return\n+  tt.return\n }\n \n }\n \n // -----\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func.func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n+  tt.func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n     // CHECK: nvvm.read.ptx.sreg.nctaid.x\n     // CHECK: nvvm.read.ptx.sreg.nctaid.y\n     // CHECK: nvvm.read.ptx.sreg.nctaid.z\n@@ -1098,20 +1098,20 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n     tt.store %a, %0 : tensor<32xi32, #blocked0>\n \n-    return\n+    tt.return\n   }\n }\n \n // -----\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: test_index_cache\n-  func.func @test_index_cache() {\n+  tt.func @test_index_cache() {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n     %0 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n     // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n     %1 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -1120,12 +1120,12 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: test_base_index_cache\n-  func.func @test_base_index_cache(%arg0: tensor<128x32xf32, #blocked0>) {\n+  tt.func @test_base_index_cache(%arg0: tensor<128x32xf32, #blocked0>) {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n     // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n     %1 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n-    return\n+    tt.return\n   }\n }\n \n@@ -1134,7 +1134,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: test_index_cache_different_block\n-  func.func @test_index_cache_different_block(%arg0: tensor<128x32xf32, #blocked0>, %arg1: i1) {\n+  tt.func @test_index_cache_different_block(%arg0: tensor<128x32xf32, #blocked0>, %arg1: i1) {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n     // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n@@ -1143,6 +1143,6 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n       %1 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n       cf.br ^bb2\n     ^bb2:  // 2 preds: ^bb0, ^bb1\n-      return\n+      tt.return\n   }\n }"}, {"filename": "test/Target/tritongpu_to_llvmir.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -8,9 +8,9 @@\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n-func.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n+tt.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n \n-  return\n+  tt.return\n }\n \n }"}, {"filename": "test/Target/tritongpu_to_ptx.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -6,9 +6,9 @@\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n-func.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n+tt.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n \n-  return\n+  tt.return\n }\n \n }"}, {"filename": "test/Triton/combine.mlir", "status": "modified", "additions": 30, "deletions": 30, "changes": 60, "file_content_changes": "@@ -2,7 +2,7 @@\n // RUN: triton-opt %s -split-input-file -canonicalize -triton-combine | FileCheck %s\n \n // CHECK-LABEL: @test_combine_dot_add_pattern\n-func.func @test_combine_dot_add_pattern() -> (tensor<128x128xf32>, tensor<128x128xf32>) {\n+tt.func @test_combine_dot_add_pattern() -> (tensor<128x128xf32>, tensor<128x128xf32>) {\n     // CHECK-DAG: %[[d:.*]] = arith.constant dense<3.000000e+00> : tensor<128x128xf32>\n     // CHECK-DAG: %[[b:.*]] = arith.constant dense<2.000000e+00> : tensor<128x128xf32>\n     // CHECK-DAG: %[[a:.*]] = arith.constant dense<1.000000e+00> : tensor<128x128xf32>\n@@ -19,12 +19,12 @@ func.func @test_combine_dot_add_pattern() -> (tensor<128x128xf32>, tensor<128x12\n     // CHECK-NEXT: %[[res1:.*]] = tt.dot %[[a]], %[[b]], %[[d]] {allowTF32 = true} : tensor<128x128xf32> * tensor<128x128xf32> -> tensor<128x128xf32>\n     %res1 = arith.addf %d, %dot_out : tensor<128x128xf32>\n \n-    return %res0, %res1 : tensor<128x128xf32>, tensor<128x128xf32>\n+    tt.return %res0, %res1 : tensor<128x128xf32>, tensor<128x128xf32>\n }\n \n \n // COM: CHECK-LABEL: @test_combine_addptr_pattern\n-func.func @test_combine_addptr_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>> {\n+tt.func @test_combine_addptr_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>> {\n     %off0 = arith.constant 10 : i32\n     %off1 = arith.constant 15 : i32\n \n@@ -42,12 +42,12 @@ func.func @test_combine_addptr_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<\n     %ptr0 = tt.addptr %base_, %idx0 : tensor<8x!tt.ptr<f32>>, tensor<8xi32>\n     %ptr1 = tt.addptr %ptr0, %idx1 : tensor<8x!tt.ptr<f32>>, tensor<8xi32>\n \n-    return %ptr1 : tensor<8x!tt.ptr<f32>>\n+    tt.return %ptr1 : tensor<8x!tt.ptr<f32>>\n }\n \n \n // CHECK-LABEL: @test_combine_select_masked_load_pattern\n-func.func @test_combine_select_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %cond: i1) -> (tensor<8xf32>, tensor<8xf32>) {\n+tt.func @test_combine_select_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %cond: i1) -> (tensor<8xf32>, tensor<8xf32>) {\n     %mask = tt.broadcast %cond : (i1) -> tensor<8xi1>\n     %false_val = arith.constant dense<0.0> : tensor<8xf32>\n \n@@ -59,12 +59,12 @@ func.func @test_combine_select_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>,\n     %y = tt.load %ptr, %mask, %false_val {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n     %1 = arith.select %cond, %y, %false_val : tensor<8xf32>\n \n-    // CHECK: return %[[res1]], %[[res2]] : tensor<8xf32>, tensor<8xf32>\n-    return %0, %1 : tensor<8xf32>, tensor<8xf32>\n+    // CHECK: tt.return %[[res1]], %[[res2]] : tensor<8xf32>, tensor<8xf32>\n+    tt.return %0, %1 : tensor<8xf32>, tensor<8xf32>\n }\n \n // CHECK-LABEL: @test_combine_select_masked_load_fail_pattern\n-func.func @test_combine_select_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %dummy_load: tensor<8xf32>, %dummy_broadcast: tensor<8xi1>, %cond0: i1, %cond1: i1) -> (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) {\n+tt.func @test_combine_select_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %dummy_load: tensor<8xf32>, %dummy_broadcast: tensor<8xi1>, %cond0: i1, %cond1: i1) -> (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) {\n     %false_val = arith.constant dense<0.0> : tensor<8xf32>\n \n     // Case 1: value at the \"load\" position is not an \"op\".  Select should not be canonicalized.\n@@ -82,21 +82,21 @@ func.func @test_combine_select_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f\n     // CHECK: %{{.*}} = arith.select %{{.*}}, %{{.*}}, %{{.*}} : tensor<8xf32>\n     %2 = arith.select %cond1, %real_load1, %false_val : tensor<8xf32>\n \n-    return %0, %1, %2 : tensor<8xf32>, tensor<8xf32>, tensor<8xf32>\n+    tt.return %0, %1, %2 : tensor<8xf32>, tensor<8xf32>, tensor<8xf32>\n }\n \n // CHECK-LABEL: @test_combine_broadcast_constant_pattern\n-func.func @test_combine_broadcast_constant_pattern(%cst : f32) -> tensor<8x2xf32> {\n+tt.func @test_combine_broadcast_constant_pattern(%cst : f32) -> tensor<8x2xf32> {\n     // CHECK: %[[cst:.*]] = arith.constant dense<1.000000e+00> : tensor<8x2xf32>\n     %const = arith.constant dense<1.0> : tensor<8xf32>\n     %bst_out = tt.broadcast %const : (tensor<8xf32>) -> tensor<8x2xf32>\n \n-    // CHECK-NEXT: return %[[cst]] : tensor<8x2xf32>\n-    return %bst_out : tensor<8x2xf32>\n+    // CHECK-NEXT: tt.return %[[cst]] : tensor<8x2xf32>\n+    tt.return %bst_out : tensor<8x2xf32>\n }\n \n // CHECK-LABEL: @test_canonicalize_masked_load_pattern\n-func.func @test_canonicalize_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>) -> (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) {\n+tt.func @test_canonicalize_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>) -> (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) {\n     %true_mask = arith.constant dense<true> : tensor<8xi1>\n     %false_mask = arith.constant dense<false> : tensor<8xi1>\n     %other_val = arith.constant dense<0.0> : tensor<8xf32>\n@@ -112,12 +112,12 @@ func.func @test_canonicalize_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>) -\n     // false_mask with other. It should become \"other\" (i.e., %y)\n     %z = tt.load %ptr, %false_mask, %y {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n \n-    // CHECK: return %[[res1]], %[[res2]], %[[res2]] : tensor<8xf32>, tensor<8xf32>, tensor<8xf32>\n-    return %x, %y, %z: tensor<8xf32>, tensor<8xf32>, tensor<8xf32>\n+    // CHECK: tt.return %[[res1]], %[[res2]], %[[res2]] : tensor<8xf32>, tensor<8xf32>, tensor<8xf32>\n+    tt.return %x, %y, %z: tensor<8xf32>, tensor<8xf32>, tensor<8xf32>\n }\n \n // CHECK-LABEL: @test_canonicalize_masked_load_fail_pattern\n-func.func @test_canonicalize_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %mask: tensor<8xi1>) -> (tensor<8xf32>, tensor<8xf32>) {\n+tt.func @test_canonicalize_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %mask: tensor<8xi1>) -> (tensor<8xf32>, tensor<8xf32>) {\n     %other_val = arith.constant dense<0.0> : tensor<8xf32>\n \n     // Case: value at the \"mask\" position is not an \"op\".  Load should not be canonicalized.\n@@ -126,43 +126,43 @@ func.func @test_canonicalize_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32\n     // CHECK: %[[res1:.*]] = tt.load %{{.*}}, %{{.*}}, %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n     %y = tt.load %ptr, %mask, %other_val {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n \n-    return %x, %y: tensor<8xf32>, tensor<8xf32>\n+    tt.return %x, %y: tensor<8xf32>, tensor<8xf32>\n }\n \n // CHECK-LABEL: @test_canonicalize_masked_store_pattern\n-func.func @test_canonicalize_masked_store_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %val: tensor<8xf32>) {\n+tt.func @test_canonicalize_masked_store_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %val: tensor<8xf32>) {\n     %true_mask = arith.constant dense<true> : tensor<8xi1>\n     %false_mask = arith.constant dense<false> : tensor<8xi1>\n \n     // CHECK: tt.store %{{.*}}, %{{.*}} : tensor<8xf32>\n     tt.store %ptr, %val, %true_mask : tensor<8xf32>\n \n     // The following store should disappear.\n-    // CHECK-NEXT: return\n+    // CHECK-NEXT: tt.return\n     tt.store %ptr, %val, %false_mask : tensor<8xf32>\n-    return\n+    tt.return\n }\n \n // CHECK-LABEL: @test_canonicalize_masked_store_fail_pattern\n-func.func @test_canonicalize_masked_store_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %val: tensor<8xf32>, %mask: tensor<8xi1>) {\n+tt.func @test_canonicalize_masked_store_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %val: tensor<8xf32>, %mask: tensor<8xi1>) {\n     // Case: value at the \"mask\" position is not an \"op\".  Store should not be canonicalized.\n     // CHECK: tt.store %{{.*}}, %{{.*}}, %{{.*}} : tensor<8xf32>\n     tt.store %ptr, %val, %mask : tensor<8xf32>\n-    return\n+    tt.return\n }\n \n // CHECK-LABEL: @test_canonicalize_expand_dims\n-func.func @test_canonicalize_expand_dims(%arg0: tensor<f32>) -> (tensor<1x8xf32>) {\n+tt.func @test_canonicalize_expand_dims(%arg0: tensor<f32>) -> (tensor<1x8xf32>) {\n     %splat = tt.splat %arg0 : (tensor<f32>) -> tensor<8xf32>\n     // CHECK: %{{.*}} = tt.splat %arg0 : (tensor<f32>) -> tensor<1x8xf32>\n     %ed = tt.expand_dims %splat {axis = 0 : i32} : (tensor<8xf32>) -> tensor<1x8xf32>\n \n-    return %ed : tensor<1x8xf32>\n+    tt.return %ed : tensor<1x8xf32>\n }\n \n \n // CHECK-LABEL: @test_canonicalize_view\n-func.func @test_canonicalize_view(%arg0: tensor<8xf32>, %arg1: tensor<f32>) -> (tensor<4x2xf32>, tensor<2x2x2xf32>, tensor<8xf32>) {\n+tt.func @test_canonicalize_view(%arg0: tensor<8xf32>, %arg1: tensor<f32>) -> (tensor<4x2xf32>, tensor<2x2x2xf32>, tensor<8xf32>) {\n     %view0 = tt.view %arg0 : (tensor<8xf32>) -> tensor<2x4xf32>\n     // CHECK: %{{.*}} = tt.view %arg0 : (tensor<8xf32>) -> tensor<4x2xf32>\n     %view1 = tt.view %view0 : (tensor<2x4xf32>) -> tensor<4x2xf32>\n@@ -175,11 +175,11 @@ func.func @test_canonicalize_view(%arg0: tensor<8xf32>, %arg1: tensor<f32>) -> (\n     // CHECK: %{{.*}} = arith.addf %arg0, %arg0 : tensor<8xf32>\n     %add = arith.addf %view3, %arg0 : tensor<8xf32>\n \n-    return %view1, %view2, %add : tensor<4x2xf32>, tensor<2x2x2xf32>, tensor<8xf32>\n+    tt.return %view1, %view2, %add : tensor<4x2xf32>, tensor<2x2x2xf32>, tensor<8xf32>\n }\n \n // CHECK-LABEL: @test_canonicalize_broadcast\n-func.func @test_canonicalize_broadcast(%arg0: tensor<1x1x8xf32>, %arg1: tensor<f32>) -> (tensor<4x2x8xf32>, tensor<8x8xf32>, tensor<1x1x8xf32>) {\n+tt.func @test_canonicalize_broadcast(%arg0: tensor<1x1x8xf32>, %arg1: tensor<f32>) -> (tensor<4x2x8xf32>, tensor<8x8xf32>, tensor<1x1x8xf32>) {\n     %broadcast0 = tt.broadcast %arg0 : (tensor<1x1x8xf32>) -> tensor<1x2x8xf32>\n     // CHECK: %{{.*}} = tt.broadcast %arg0 : (tensor<1x1x8xf32>) -> tensor<4x2x8xf32>\n     %broadcast1 = tt.broadcast %broadcast0 : (tensor<1x2x8xf32>) -> tensor<4x2x8xf32>\n@@ -192,11 +192,11 @@ func.func @test_canonicalize_broadcast(%arg0: tensor<1x1x8xf32>, %arg1: tensor<f\n     // CHECK: %{{.*}} = arith.addf %arg0, %arg0 : tensor<1x1x8xf32>\n     %add = arith.addf %broadcast3, %arg0 : tensor<1x1x8xf32>\n \n-    return %broadcast1, %broadcast2, %add : tensor<4x2x8xf32>, tensor<8x8xf32>, tensor<1x1x8xf32>\n+    tt.return %broadcast1, %broadcast2, %add : tensor<4x2x8xf32>, tensor<8x8xf32>, tensor<1x1x8xf32>\n }\n \n // CHECK-LABEL: @test_fold_views\n-func.func @test_fold_views() -> (tensor<16x8xf32>, tensor<16x128xf32>, tensor<1x1x128xf32>) {\n+tt.func @test_fold_views() -> (tensor<16x8xf32>, tensor<16x128xf32>, tensor<1x1x128xf32>) {\n     %a = arith.constant dense<1.0> : tensor<1x128xf32>\n \n     // CHECK-DAG: %{{.*}} = arith.constant dense<1.{{.*}}> : tensor<16x8xf32>\n@@ -208,5 +208,5 @@ func.func @test_fold_views() -> (tensor<16x8xf32>, tensor<16x128xf32>, tensor<1x\n     // CHECK-DAG: %{{.*}} = arith.constant dense<1.{{.*}}> : tensor<1x1x128xf32>\n     %d = tt.expand_dims %a {axis = 0: i32} : (tensor<1x128xf32>) -> tensor<1x1x128xf32>\n \n-    return %b, %c, %d : tensor<16x8xf32>, tensor<16x128xf32>, tensor<1x1x128xf32>\n+    tt.return %b, %c, %d : tensor<16x8xf32>, tensor<16x128xf32>, tensor<1x1x128xf32>\n }"}, {"filename": "test/Triton/rewrite-tensor-pointer.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n // RUN: triton-opt %s -triton-rewrite-tensor-pointer | FileCheck %s\n-func.func public @matmul_kernel(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}) {\n+tt.func public @matmul_kernel(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}) {\n   %c31_i32 = arith.constant 31 : i32\n   %c127_i32 = arith.constant 127 : i32\n   %c1 = arith.constant 1 : index\n@@ -79,5 +79,5 @@ func.func public @matmul_kernel(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}\n   %53 = tt.broadcast %51 : (tensor<1x32xi1>) -> tensor<128x32xi1>\n   %54 = arith.andi %52, %53 : tensor<128x32xi1>\n   tt.store %45, %30, %54 {cache = 1 : i32, evict = 1 : i32} : tensor<128x32xf16>\n-  return\n+  tt.return\n }"}, {"filename": "test/Triton/vecadd.mlir", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1,7 +1,7 @@\n // RUN: triton-opt %s -verify-diagnostics\n \n module {\n-  func.func @add_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32__(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32) {\n+  tt.func @add_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32__(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32) {\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %c256_i32 = arith.constant 256 : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -39,11 +39,11 @@ module {\n     %16 = tt.broadcast %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n     %17 = tt.addptr %16, %4 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n     tt.store %17, %15#0, %6 : tensor<256xf32>\n-    return\n+    tt.return\n   }\n }\n // module {\n-//   func.func @add_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32__(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32) {\n+//   tt.func @add_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32__(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32) {\n //     %c64 = arith.constant 64 : index\n //     %c32 = arith.constant 32 : index\n //     %c0 = arith.constant 0 : index\n@@ -125,6 +125,6 @@ module {\n //     %53 = tt.broadcast %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %54 = tt.addptr %53, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     tt.store %54, %52#0, %6 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     return\n+//     tt.return\n //   }\n // }"}, {"filename": "test/TritonGPU/coalesce.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -19,7 +19,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n // CHECK: [[store_val:%.*]] = triton_gpu.convert_layout {{.*}} -> tensor<64x64xf32, [[col_layout]]>\n // CHECK: [[store_mask:%.*]] = triton_gpu.convert_layout {{.*}} -> tensor<64x64xi1, [[col_layout]]>\n // CHECK: tt.store [[store_ptr]], [[store_val]], [[store_mask]]\n-func.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+tt.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n                 %arg1: i32 {tt.divisibility = 16 : i32},\n                 %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n                 %arg3: i32 {tt.divisibility = 16 : i32}) {\n@@ -47,7 +47,7 @@ func.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %18 = tt.addptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n   %19 = tt.load %10, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked1>\n   tt.store %18, %19, %cst : tensor<64x64xf32, #blocked1>\n-  return\n+  tt.return\n }\n \n }"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 94, "deletions": 94, "changes": 188, "file_content_changes": "@@ -9,67 +9,67 @@\n // CHECK: [[$col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK: [[$col_layout_novec:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK-LABEL: cst\n-func.func @cst() -> tensor<1024xi32, #layout1> {\n+tt.func @cst() -> tensor<1024xi32, #layout1> {\n   %cst = arith.constant dense<0> : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %cst : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: return %cst : tensor<1024xi32, [[$target_layout]]>\n-  return %1: tensor<1024xi32, #layout1>\n+  // CHECK: tt.return %cst : tensor<1024xi32, [[$target_layout]]>\n+  tt.return %1: tensor<1024xi32, #layout1>\n }\n \n // CHECK-LABEL: range\n-func.func @range() -> tensor<1024xi32, #layout1> {\n+tt.func @range() -> tensor<1024xi32, #layout1> {\n   %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %0 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: return %0 : tensor<1024xi32, [[$target_layout]]>\n-  return %1: tensor<1024xi32, #layout1>\n+  // CHECK: tt.return %0 : tensor<1024xi32, [[$target_layout]]>\n+  tt.return %1: tensor<1024xi32, #layout1>\n }\n \n // CHECK-LABEL: splat\n-func.func @splat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n+tt.func @splat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   %0 = tt.splat %arg0 : (i32) -> tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %0 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: return %0 : tensor<1024xi32, [[$target_layout]]>\n-  return %1: tensor<1024xi32, #layout1>\n+  // CHECK: tt.return %0 : tensor<1024xi32, [[$target_layout]]>\n+  tt.return %1: tensor<1024xi32, #layout1>\n }\n \n // CHECK-LABEL: remat\n-func.func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n+tt.func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #layout0>\n   %1 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #layout0>\n   %2 = arith.muli %0, %1 : tensor<1024xi32, #layout0>\n   %3 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   %4 = tt.splat %arg0 : (i32) -> tensor<1024xi32, #layout0>\n   %5 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   %6 = arith.addi %3, %5 : tensor<1024xi32, #layout1>\n-  return %6: tensor<1024xi32, #layout1>\n+  tt.return %6: tensor<1024xi32, #layout1>\n   // CHECK: %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>\n   // CHECK: %1 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>\n   // CHECK: %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>\n   // CHECK: %3 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>\n   // CHECK: %4 = arith.muli %0, %2 : tensor<1024xi32, [[$target_layout]]>\n   // CHECK: %5 = arith.muli %1, %3 : tensor<1024xi32, [[$target_layout]]>\n   // CHECK: %6 = arith.addi %4, %5 : tensor<1024xi32, [[$target_layout]]>\n-  // CHECK: return %6 : tensor<1024xi32, [[$target_layout]]>\n+  // CHECK: tt.return %6 : tensor<1024xi32, [[$target_layout]]>\n }\n \n // Always rematerialize single value loads\n // CHECK-LABEL: remat_single_value\n-func.func @remat_single_value(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+tt.func @remat_single_value(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   %0 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<1x!tt.ptr<i32>, #layout1>\n   %1 = tt.load %0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1xi32, #layout1>\n   // CHECK-NOT: triton_gpu.convert_layout\n   %2 = triton_gpu.convert_layout %1 : (tensor<1xi32, #layout1>) -> tensor<1xi32, #layout0>\n   %3 = triton_gpu.convert_layout %0 : (tensor<1x!tt.ptr<i32>, #layout1>) -> tensor<1x!tt.ptr<i32>, #layout0>\n   tt.store %3, %2 : tensor<1xi32, #layout0>\n-  return\n+  tt.return\n }\n \n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-func.func @remat_fast_load(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+tt.func @remat_fast_load(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   %0 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<16x!tt.ptr<i32>, #layout1>\n   %1 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #layout1>\n   %2 = tt.addptr %0, %1 : tensor<16x!tt.ptr<i32>, #layout1>, tensor<16xi32, #layout1>\n@@ -78,12 +78,12 @@ func.func @remat_fast_load(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   %4 = triton_gpu.convert_layout %3 : (tensor<16xi32, #layout1>) -> tensor<16xi32, #layout0>\n   %5 = triton_gpu.convert_layout %2 : (tensor<16x!tt.ptr<i32>, #layout1>) -> tensor<16x!tt.ptr<i32>, #layout0>\n   tt.store %5, %4 : tensor<16xi32, #layout0>\n-  return\n+  tt.return\n }\n }\n \n // CHECK-LABEL: if\n-func.func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+tt.func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout1>\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n@@ -96,11 +96,11 @@ func.func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n     %6 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout1>) -> tensor<1024xi32, #layout0>\n     tt.store %5, %6 : tensor<1024xi32, #layout0>\n   }\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: if_convert_else_not\n-func.func @if_convert_else_not(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+tt.func @if_convert_else_not(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n   %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n@@ -117,11 +117,11 @@ func.func @if_convert_else_not(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility\n   }\n   // CHECK-NOT: triton_gpu.convert_layout\n   tt.store %5, %8 : tensor<1024xi32, #layout1>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: if_not_else_convert\n-func.func @if_not_else_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+tt.func @if_not_else_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n   %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n@@ -138,11 +138,11 @@ func.func @if_not_else_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility\n   }\n   // CHECK-NOT: triton_gpu.convert_layout\n   tt.store %5, %8 : tensor<1024xi32, #layout1>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: if_else_both_convert\n-func.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+tt.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n   %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n@@ -161,7 +161,7 @@ func.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility\n   // disabledCHECK: triton_gpu.convert_layout\n   // CHECK-NOT: triton_gpu.convert_layout\n   tt.store %5, %8 : tensor<1024xi32, #layout1>\n-  return\n+  tt.return\n }\n \n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n@@ -173,12 +173,12 @@ func.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility\n #blocked4 = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n \n // CHECK-LABEL: transpose\n-func.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n+tt.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, {{%cst.*}}, {{%cst.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, [[$row_layout]]>\n   // CHECK: [[cvt_val:%.*]] = triton_gpu.convert_layout [[loaded_val]] : (tensor<64x64xf32, [[$row_layout]]>) -> tensor<64x64xf32, [[$col_layout]]>\n   // CHECK: tt.store {{.*}}, [[cvt_val]], {{%cst.*}} : tensor<64x64xf32, [[$col_layout]]>\n-  // CHECK: return\n+  // CHECK: tt.return\n   %cst = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked1>\n   %cst_0 = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n   %00 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #slice1dim1>\n@@ -210,65 +210,65 @@ func.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i3\n   %25 = triton_gpu.convert_layout %23 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked4>\n   %26 = triton_gpu.convert_layout %cst_0 : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked4>\n   tt.store %24, %25, %26 : tensor<64x64xf32, #blocked4>\n-  return\n+  tt.return\n }\n \n // CHECK-LABEL: loop\n-func.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n-    // CHECK-NOT: triton_gpu.convert_layout\n-    // CHECK: [[loop_ret:%.*]]:2 = scf.for {{.*}} -> (tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>)\n-    // CHECK-NEXT: {{.*}} = tt.load {{.*}} : tensor<64x64xf32, [[$row_layout]]>\n-    // CHECK-NEXT: {{.*}} = arith.addf {{.*}} : tensor<64x64xf32, [[$row_layout]]>\n-    // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>, tensor<64x64xi32, [[$row_layout]]>\n-    // CHECK-NEXT: scf.yield {{.*}} : tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>\n-    // CHECK-NEXT: }\n-    // CHECK-NEXT: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[$row_layout]]>) -> tensor<64x64xf32, [[$col_layout_novec]]>\n-    // CHECK-NOT: triton_gpu.convert_layout\n-    %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n-    %cst_0 = arith.constant dense<64> : tensor<64x64xi32, #blocked1>\n-    %c1 = arith.constant 1 : index\n-    %c32 = arith.constant 32 : index\n-    %c0 = arith.constant 0 : index\n-    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked1>\n-    %00 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #slice1dim1>\n-    %01 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #slice2dim0>\n-    %1 = tt.expand_dims %00 {axis = 1 : i32} : (tensor<64xi32, #slice1dim1>) -> tensor<64x1xi32, #blocked1>\n-    %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n-    %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n-    %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-    %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n-    %6 = tt.expand_dims %01 {axis = 0 : i32} : (tensor<64xi32, #slice2dim0>) -> tensor<1x64xi32, #blocked2>\n-    %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n-    %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n-    %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-    %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n-    %11:2 = scf.for %arg5 = %c0 to %c32 step %c1 iter_args(%arg6 = %cst_1, %arg7 = %10) -> (tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>) {\n-      %23 = triton_gpu.convert_layout %arg7 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n-      %24 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n-      %25 = triton_gpu.convert_layout %cst_1 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n-      %26 = tt.load %23, %24, %25 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked3>\n-      %27 = triton_gpu.convert_layout %26 : (tensor<64x64xf32, #blocked3>) -> tensor<64x64xf32, #blocked1>\n-      %28 = arith.addf %arg6, %27 : tensor<64x64xf32, #blocked1>\n-      %29 = tt.addptr %arg7, %cst_0 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n-      scf.yield %28, %29 : tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>\n-    }\n-    %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-    %13 = tt.addptr %12, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n-    %14 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n-    %15 = arith.muli %6, %14 : tensor<1x64xi32, #blocked2>\n-    %16 = tt.broadcast %13 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n-    %17 = tt.broadcast %15 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n-    %18 = triton_gpu.convert_layout %17 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-    %19 = tt.addptr %16, %18 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n-    %20 = triton_gpu.convert_layout %19 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n-    %21 = triton_gpu.convert_layout %11#0 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked1>\n-    %22 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked1>\n-    tt.store %20, %21, %22 : tensor<64x64xf32, #blocked1>\n-    return\n+tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  // CHECK: [[loop_ret:%.*]]:2 = scf.for {{.*}} -> (tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>)\n+  // CHECK-NEXT: {{.*}} = tt.load {{.*}} : tensor<64x64xf32, [[$row_layout]]>\n+  // CHECK-NEXT: {{.*}} = arith.addf {{.*}} : tensor<64x64xf32, [[$row_layout]]>\n+  // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>, tensor<64x64xi32, [[$row_layout]]>\n+  // CHECK-NEXT: scf.yield {{.*}} : tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>\n+  // CHECK-NEXT: }\n+  // CHECK-NEXT: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[$row_layout]]>) -> tensor<64x64xf32, [[$col_layout_novec]]>\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n+  %cst_0 = arith.constant dense<64> : tensor<64x64xi32, #blocked1>\n+  %c1 = arith.constant 1 : index\n+  %c32 = arith.constant 32 : index\n+  %c0 = arith.constant 0 : index\n+  %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked1>\n+  %00 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #slice1dim1>\n+  %01 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #slice2dim0>\n+  %1 = tt.expand_dims %00 {axis = 1 : i32} : (tensor<64xi32, #slice1dim1>) -> tensor<64x1xi32, #blocked1>\n+  %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n+  %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n+  %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n+  %6 = tt.expand_dims %01 {axis = 0 : i32} : (tensor<64xi32, #slice2dim0>) -> tensor<1x64xi32, #blocked2>\n+  %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n+  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n+  %11:2 = scf.for %arg5 = %c0 to %c32 step %c1 iter_args(%arg6 = %cst_1, %arg7 = %10) -> (tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>) {\n+    %23 = triton_gpu.convert_layout %arg7 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n+    %24 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n+    %25 = triton_gpu.convert_layout %cst_1 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n+    %26 = tt.load %23, %24, %25 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked3>\n+    %27 = triton_gpu.convert_layout %26 : (tensor<64x64xf32, #blocked3>) -> tensor<64x64xf32, #blocked1>\n+    %28 = arith.addf %arg6, %27 : tensor<64x64xf32, #blocked1>\n+    %29 = tt.addptr %arg7, %cst_0 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n+    scf.yield %28, %29 : tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  }\n+  %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %13 = tt.addptr %12, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n+  %14 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n+  %15 = arith.muli %6, %14 : tensor<1x64xi32, #blocked2>\n+  %16 = tt.broadcast %13 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %17 = tt.broadcast %15 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %18 = triton_gpu.convert_layout %17 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n+  %19 = tt.addptr %16, %18 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n+  %20 = triton_gpu.convert_layout %19 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %21 = triton_gpu.convert_layout %11#0 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked1>\n+  %22 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked1>\n+  tt.store %20, %21, %22 : tensor<64x64xf32, #blocked1>\n+  tt.return\n }\n \n // CHECK-LABEL: vecadd\n-func.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n+tt.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %c256_i32 = arith.constant 256 : i32\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n@@ -295,12 +295,12 @@ func.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.p\n   %21 = tt.addptr %19, %20 : tensor<256x!tt.ptr<f32>, #layout1>, tensor<256xi32, #layout1>\n   %22 = triton_gpu.convert_layout %18 : (tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>) -> tensor<256xf32, #layout1>\n   tt.store %21, %22 : tensor<256xf32, #layout1>\n-  return\n+  tt.return\n }\n \n // Select has args with different element types\n // CHECK-LABEL: select\n-func.func @select(%arg0: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}) {\n+tt.func @select(%arg0: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %cst = arith.constant dense<30000> : tensor<1x1xi32, #blocked2>\n   %cst_0 = arith.constant dense<30000> : tensor<1x512xi32, #blocked2>\n@@ -346,12 +346,12 @@ func.func @select(%arg0: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg1: !tt.p\n     tt.store %31, %32, %33 : tensor<1x512xf64, #blocked3>\n     scf.yield %30 : tensor<1x512xf64, #blocked2>\n   }\n-  return\n+  tt.return\n }\n \n // Make sure the following IR doesn't hang the compiler.\n // CHECK-LABEL: long_func\n-func.func public @long_func(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg5: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg7: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg9: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg10: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg11: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg12: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg13: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg14: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg15: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}) {\n+tt.func public @long_func(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg5: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg7: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg9: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg10: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg11: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg12: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg13: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg14: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg15: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}) {\n   %cst = arith.constant dense<1.000000e+00> : tensor<1024xf32, #blocked0>\n   %cst_0 = arith.constant dense<5.000000e-04> : tensor<1024xf32, #blocked0>\n   %cst_1 = arith.constant dense<0.999499976> : tensor<1024xf32, #blocked0>\n@@ -742,13 +742,13 @@ func.func public @long_func(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %a\n   %365 = triton_gpu.convert_layout %364 : (tensor<1024x!tt.ptr<f64>, #blocked0>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n   %366 = triton_gpu.convert_layout %343 : (tensor<1024xf64, #blocked0>) -> tensor<1024xf64, #blocked0>\n   tt.store %365, %366 : tensor<1024xf64, #blocked0>\n-  return\n+  tt.return\n }\n \n // A mnist model from torch inductor.\n // Check if topological sort is working correct and there's no unnecessary convert\n // CHECK-LABEL: mnist\n-func.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: i32) {\n+tt.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %cst = arith.constant dense<10> : tensor<16x1xi32, #blocked2>\n   %cst_0 = arith.constant dense<10> : tensor<1x16xi32, #blocked3>\n@@ -822,7 +822,7 @@ func.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1:\n   %62 = triton_gpu.convert_layout %58 : (tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked4>\n   %63 = triton_gpu.convert_layout %22 : (tensor<16x16xi1, #blocked2>) -> tensor<16x16xi1, #blocked4>\n   tt.store %61, %62, %63 : tensor<16x16xf32, #blocked4>\n-  return\n+  tt.return\n }\n \n // -----\n@@ -835,7 +835,7 @@ func.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1:\n #blocked5 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [8, 1], order = [1, 0]}>\n // cmpf and cmpi have different operands and result types\n // CHECK-LABEL: cmp\n-func.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) {\n+tt.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) {\n   %c64 = arith.constant 64 : index\n   %c2048 = arith.constant 2048 : index\n   %c0 = arith.constant 0 : index\n@@ -968,14 +968,14 @@ func.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !\n     %82 = triton_gpu.convert_layout %54 : (tensor<64x64xi1, #blocked2>) -> tensor<64x64xi1, #blocked4>\n     tt.store %80, %81, %82 : tensor<64x64xf16, #blocked4>\n   }\n-  return\n+  tt.return\n }\n \n // -----\n \n // Just make sure it doesn't crash on non-tensor types.\n // CHECK-LABEL: if_no_tensor\n-func.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}) {\n+tt.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %c-1_i64 = arith.constant -1 : i64\n   %cst = arith.constant 0.000000e+00 : f32\n@@ -996,7 +996,7 @@ func.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %8 = tt.load %5, %7, %cst {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : f32\n   %9 = tt.addptr %arg1, %0 : !tt.ptr<f32>, i32\n   tt.store %9, %8 {cache = 1 : i32, evict = 1 : i32} : f32\n-  return\n+  tt.return\n }\n \n // -----\n@@ -1009,7 +1009,7 @@ func.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [2, 1], order = [0, 1]}>\n #blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [16, 2], warpsPerCTA = [2, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n-  func.func public @reduce_cvt1(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg1: i32, %arg2: i32) {\n+  tt.func public @reduce_cvt1(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg1: i32, %arg2: i32) {\n     %cst = arith.constant dense<0> : tensor<1x2xi32, #blocked>\n     %cst_0 = arith.constant dense<2> : tensor<1x2xi32, #blocked>\n     %0 = tt.make_range {end = 2 : i32, start = 0 : i32} : tensor<2xi32, #blocked1>\n@@ -1029,7 +1029,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %14 = triton_gpu.convert_layout %12 : (tensor<1x2xi64, #blocked>) -> tensor<1x2xi64, #blocked3>\n     %15 = triton_gpu.convert_layout %3 : (tensor<1x2xi1, #blocked>) -> tensor<1x2xi1, #blocked3>\n     tt.store %13, %14, %15 {cache = 1 : i32, evict = 1 : i32} : tensor<1x2xi64, #blocked3>\n-    return\n+    tt.return\n   }\n }\n \n@@ -1045,7 +1045,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n #blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func.func public @reduce_cvt2(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}) {\n+  tt.func public @reduce_cvt2(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}) {\n     %cst = arith.constant dense<0.000000e+00> : tensor<1x256xf32, #blocked>\n     %c3136_i32 = arith.constant 3136 : index\n     %c256_i32 = arith.constant 256 : index\n@@ -1104,6 +1104,6 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %25 = triton_gpu.convert_layout %21 : (tensor<1x1xf32, #blocked>) -> tensor<1x1xf32, #blocked>\n     %26 = triton_gpu.convert_layout %7 : (tensor<1x1xi1, #blocked>) -> tensor<1x1xi1, #blocked>\n     tt.store %24, %25, %26 {cache = 1 : i32, evict = 1 : i32} : tensor<1x1xf32, #blocked>\n-    return\n+    tt.return\n   }\n }"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 15, "deletions": 15, "changes": 30, "file_content_changes": "@@ -11,7 +11,7 @@\n #A = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n-// CHECK: func.func @matmul_loop\n+// CHECK: tt.func @matmul_loop\n // CHECK-DAG: %[[CONSTANT_0:.*]] = arith.constant 0 : i32\n // CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n // CHECK-DAG: %[[CONSTANT_2:.*]] = arith.constant 2 : i32\n@@ -47,7 +47,7 @@\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:   scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n-func.func @matmul_loop(%lb : index, %ub : index, %step : index,\n+tt.func @matmul_loop(%lb : index, %ub : index, %step : index,\n                   %A : !tt.ptr<f16> {tt.divisibility = 16 : i32},\n                   %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) -> tensor<128x128xf32, #C> {\n   // A ptrs\n@@ -88,10 +88,10 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index,\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n-  return %loop#2: tensor<128x128xf32, #C>\n+  tt.return %loop#2: tensor<128x128xf32, #C>\n }\n \n-// CHECK: func.func @matmul_loop_nested\n+// CHECK: tt.func @matmul_loop_nested\n // CHECK-DAG: %[[CONSTANT_0:.*]] = arith.constant 0 : i32\n // CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n // CHECK-DAG: %[[CONSTANT_2:.*]] = arith.constant 2 : i32\n@@ -120,7 +120,7 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index,\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n-func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n+tt.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n                          %A : !tt.ptr<f16> {tt.divisibility = 16 : i32},\n                          %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) -> tensor<128x128xf32, #C>{\n \n@@ -162,11 +162,11 @@ func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n \n     scf.yield %loop2#2 : tensor<128x128xf32, #C>\n   }\n-  return %loop1#0 : tensor<128x128xf32, #C>\n+  tt.return %loop1#0 : tensor<128x128xf32, #C>\n }\n \n \n-// CHECK: func.func @matmul_loop_single_pipeline\n+// CHECK: tt.func @matmul_loop_single_pipeline\n // CHECK-DAG: %[[CONSTANT_0:.*]] = arith.constant 0 : i32\n // CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n // CHECK-DAG: %[[CONSTANT_2:.*]] = arith.constant 2 : i32\n@@ -187,7 +187,7 @@ func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:   scf.yield {{.*}}, {{.*}}, %[[NEXT_B_BUFFER]], %[[NEXT_B]], {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n-func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n+tt.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n                                   %A : !tt.ptr<f16> {tt.divisibility = 16 : i32},\n                                   %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) -> tensor<128x128xf32, #C> {\n   // A ptrs\n@@ -222,10 +222,10 @@ func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_b_ptr, %c : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n-  return %loop#1 : tensor<128x128xf32, #C>\n+  tt.return %loop#1 : tensor<128x128xf32, #C>\n }\n \n-// CHECK: func.func @lut_bmm_scalar\n+// CHECK: tt.func @lut_bmm_scalar\n // CHECK: triton_gpu.insert_slice_async\n // CHECK: triton_gpu.insert_slice_async\n // CHECK: triton_gpu.insert_slice_async\n@@ -239,7 +239,7 @@ func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n // CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_1]]\n // CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_0]]\n // CHECK: triton_gpu.async_wait {num = 2 : i32}\n-func.func @lut_bmm_scalar(%77: i64 {tt.divisibility=16: i32},\n+tt.func @lut_bmm_scalar(%77: i64 {tt.divisibility=16: i32},\n                    %76: index,\n                    %49: tensor<16x16x!tt.ptr<f16>, #AL> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n                    %75: !tt.ptr<i64>,\n@@ -265,10 +265,10 @@ func.func @lut_bmm_scalar(%77: i64 {tt.divisibility=16: i32},\n     %92 = tt.addptr %arg21, %c1_i32 : !tt.ptr<i64>, i32\n     scf.yield %90, %91, %92 : tensor<16x16xf32, #C>, tensor<16x16x!tt.ptr<f16>, #AL>, !tt.ptr<i64>\n   }\n-  return %79#0 : tensor<16x16xf32, #C>\n+  tt.return %79#0 : tensor<16x16xf32, #C>\n }\n \n-// CHECK: func.func @lut_bmm_vector\n+// CHECK: tt.func @lut_bmm_vector\n // CHECK: triton_gpu.insert_slice_async\n // CHECK: triton_gpu.insert_slice_async\n // CHECK: triton_gpu.insert_slice_async\n@@ -283,7 +283,7 @@ func.func @lut_bmm_scalar(%77: i64 {tt.divisibility=16: i32},\n // CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_1]]\n // CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_0]]\n // CHECK: triton_gpu.async_wait {num = 2 : i32}\n-func.func @lut_bmm_vector(%77: tensor<16x16xi64, #BL> {tt.divisibility=16: i32, tt.constancy=16: i32},\n+tt.func @lut_bmm_vector(%77: tensor<16x16xi64, #BL> {tt.divisibility=16: i32, tt.constancy=16: i32},\n                    %76: index,\n                    %49: tensor<16x16x!tt.ptr<f16>, #AL> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n                    %75: tensor<16x!tt.ptr<i64>, #BLs1>,\n@@ -311,5 +311,5 @@ func.func @lut_bmm_vector(%77: tensor<16x16xi64, #BL> {tt.divisibility=16: i32,\n     %92 = tt.addptr %arg21, %c1_i32_splat : tensor<16x!tt.ptr<i64>, #BLs1>, tensor<16xi32, #BLs1>\n     scf.yield %90, %91, %92 : tensor<16x16xf32, #C>, tensor<16x16x!tt.ptr<f16>, #AL>, tensor<16x!tt.ptr<i64>, #BLs1>\n   }\n-  return %79#0 : tensor<16x16xf32, #C>\n+  tt.return %79#0 : tensor<16x16xf32, #C>\n }"}, {"filename": "test/TritonGPU/matmul.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -4,7 +4,7 @@\n // CHECK: offset = 49152, size = 49152\n // CHECK: size = 98304\n module {\n-func.func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c64_13c64_14c64_15c8(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32) {\n+tt.func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c64_13c64_14c64_15c8(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32) {\n     %cst = arith.constant dense<true> : tensor<64x64xi1>\n     %c64 = arith.constant 64 : index\n     %c0 = arith.constant 0 : index\n@@ -101,6 +101,6 @@ func.func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32_\n     %74 = tt.broadcast %72 : (tensor<1x64xi1>) -> tensor<64x64xi1>\n     %75 = arith.andi %73, %74 : tensor<64x64xi1>\n     tt.store %66, %47#0, %75 : tensor<64x64xf32>\n-    return\n+    tt.return\n   }\n }"}, {"filename": "test/TritonGPU/prefetch.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -11,7 +11,7 @@\n #B_OP = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n \n-// CHECK: func.func @matmul_loop\n+// CHECK: tt.func @matmul_loop\n // CHECK-DAG: %[[A0_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice %[[A0:.*]][0, 0] [128, 16]\n // CHECK-DAG: %[[A0_PREFETCH:.*]] = triton_gpu.convert_layout %[[A0_PREFETCH_SMEM]]\n // CHECK-DAG: %[[B0_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice %[[B0:.*]][0, 0] [16, 128]\n@@ -28,7 +28,7 @@\n // CHECK-DAG:   %[[NEXT_B_PREFETCH_SMEM:.*]] = triton_gpu.extract_slice {{.*}}[0, 0] [16, 128]\n // CHECK-DAG:   %[[NEXT_B_PREFETCH:.*]] = triton_gpu.convert_layout %[[NEXT_B_PREFETCH_SMEM]]\n // CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_PREFETCH]], %[[NEXT_B_PREFETCH]]\n-func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+tt.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n \n@@ -60,5 +60,5 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16\n \n     scf.yield %next_a_ptr, %next_b_ptr, %next_a, %next_b, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x32xf16, #A>, tensor<32x128xf16, #B>, tensor<128x128xf32, #C>\n   }\n-  return\n+  tt.return\n }"}, {"filename": "test/lib/Analysis/TestAlias.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -9,7 +9,7 @@ using namespace mlir;\n namespace {\n \n struct TestAliasPass\n-    : public PassWrapper<TestAliasPass, OperationPass<func::FuncOp>> {\n+    : public PassWrapper<TestAliasPass, OperationPass<triton::FuncOp>> {\n \n   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAliasPass);\n "}, {"filename": "test/lib/Analysis/TestAllocation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -6,7 +6,7 @@ using namespace mlir;\n namespace {\n \n struct TestAllocationPass\n-    : public PassWrapper<TestAllocationPass, OperationPass<func::FuncOp>> {\n+    : public PassWrapper<TestAllocationPass, OperationPass<triton::FuncOp>> {\n \n   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAllocationPass);\n "}, {"filename": "test/lib/Analysis/TestAxisInfo.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -7,7 +7,7 @@ using namespace mlir;\n namespace {\n \n struct TestAxisInfoPass\n-    : public PassWrapper<TestAxisInfoPass, OperationPass<func::FuncOp>> {\n+    : public PassWrapper<TestAxisInfoPass, OperationPass<triton::FuncOp>> {\n \n   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAxisInfoPass);\n "}, {"filename": "test/lib/Analysis/TestMembar.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -11,7 +11,7 @@ using namespace mlir;\n namespace {\n \n struct TestMembarPass\n-    : public PassWrapper<TestMembarPass, OperationPass<func::FuncOp>> {\n+    : public PassWrapper<TestMembarPass, OperationPass<triton::FuncOp>> {\n \n   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestMembarPass);\n "}]