[{"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 25, "deletions": 31, "changes": 56, "file_content_changes": "@@ -67,27 +67,21 @@ def nvsmi(attrs):\n     # On some other servers, we are getting about 90% peak for 8kx8x8k float16\n     'a100': {\n         # square\n-        (512, 512, 512): {'float16': 0.084, 'float32': 0.12, 'int8': 0.05},\n-        (1024, 1024, 1024): {'float16': 0.332, 'float32': 0.352, 'int8': 0.169},\n-        (2048, 2048, 2048): {'float16': 0.635, 'float32': 0.522, 'int8': 0.34},\n-        (4096, 4096, 4096): {'float16': 0.750, 'float32': 0.810, 'int8': 0.46},\n-        (8192, 8192, 8192): {'float16': 0.760, 'float32': 0.760, 'int8': 0.51},\n+        (512, 512, 512): {'float16': 0.059, 'float32': 0.094, 'int8': 0.05},\n+        (1024, 1024, 1024): {'float16': 0.281, 'float32': 0.316, 'int8': 0.169},\n+        (2048, 2048, 2048): {'float16': 0.598, 'float32': 0.534, 'int8': 0.34},\n+        (4096, 4096, 4096): {'float16': 0.741, 'float32': 0.752, 'int8': 0.46},\n+        (8192, 8192, 8192): {'float16': 0.772, 'float32': 0.934, 'int8': 0.51},\n         # tall-skinny\n-        (16, 1024, 1024): {'float16': 0.008, 'float32': 0.009, 'int8': 0.005},\n-        (16, 4096, 4096): {'float16': 0.036, 'float32': 0.038, 'int8': 0.026},\n-        (16, 8192, 8192): {'float16': 0.056, 'float32': 0.061, 'int8': 0.043},\n-        (64, 1024, 1024): {'float16': 0.020, 'float32': 0.030, 'int8': 0.017},\n-        (64, 4096, 4096): {'float16': 0.160, 'float32': 0.162, 'int8': 0.097},\n-        (64, 8192, 8192): {'float16': 0.280, 'float32': 0.257, 'int8': 0.174},\n-        (1024, 64, 1024): {'float16': 0.040, 'float32': 0.050, 'int8': 0.017},\n-        (4096, 64, 4096): {'float16': 0.160, 'float32': 0.200, 'int8': 0.102},\n-        (8192, 64, 8192): {'float16': 0.250, 'float32': 0.23, 'int8': 0.177},\n-        # Non pow 2 shapes\n-        (1000, 200, 100): {'float16': 0.011, 'float32': 0.017, 'int8': 0.05},\n-        (1000, 200, 700): {'float16': 0.027, 'float32': 0.047, 'int8': 0.05},\n-        (994, 136, 402): {'float16': 0.015, 'float32': 0.024, 'int8': 0.05},\n-        (995, 135, 409): {'float16': 0.015, 'float32': 0.025, 'int8': 0.05},\n-        (99, 1357, 409): {'float16': 0.011, 'float32': 0.036, 'int8': 0.05}\n+        (16, 1024, 1024): {'float16': 0.006, 'float32': 0.009, 'int8': 0.005},\n+        (16, 4096, 4096): {'float16': 0.055, 'float32': 0.045, 'int8': 0.026},\n+        (16, 8192, 8192): {'float16': 0.074, 'float32': 0.075, 'int8': 0.043},\n+        (64, 1024, 1024): {'float16': 0.018, 'float32': 0.023, 'int8': 0.017},\n+        (64, 4096, 4096): {'float16': 0.153, 'float32': 0.000, 'int8': 0.097},\n+        (64, 8192, 8192): {'float16': 0.324, 'float32': 0.000, 'int8': 0.174},\n+        (1024, 64, 1024): {'float16': 0.027, 'float32': 0.044, 'int8': 0.017},\n+        (4096, 64, 4096): {'float16': 0.180, 'float32': 0.216, 'int8': 0.102},\n+        (8192, 64, 8192): {'float16': 0.270, 'float32': 0.000, 'int8': 0.177},\n     }\n }\n \n@@ -156,17 +150,16 @@ def _add(x_ptr, y_ptr, output_ptr, n_elements,\n         10003 * 7007: {'float16': 0.010, 'float32': 0.010},\n     },\n     'a100': {\n-        1024 * 16: {'float16': 0.010, 'bfloat16': 0.010, 'float32': 0.020},\n-        1024 * 64: {'float16': 0.040, 'bfloat16': 0.040, 'float32': 0.066},\n-        1024 * 256: {'float16': 0.132, 'bfloat16': 0.132, 'float32': 0.227},\n-        1024 * 1024: {'float16': 0.353, 'bfloat16': 0.353, 'float32': 0.488},\n-        1024 * 4096: {'float16': 0.605, 'bfloat16': 0.605, 'float32': 0.705},\n-        1024 * 16384: {'float16': 0.758, 'bfloat16': 0.750, 'float32': 0.819},\n-        1024 * 65536: {'float16': 0.850, 'bfloat16': 0.850, 'float32': 0.870},\n+        1024 * 16: {'float16': 0.003, 'float32': 0.007},\n+        1024 * 64: {'float16': 0.013, 'float32': 0.026},\n+        1024 * 256: {'float16': 0.052, 'float32': 0.105},\n+        1024 * 1024: {'float16': 0.210, 'float32': 0.404},\n+        1024 * 4096: {'float16': 0.770, 'float32': 0.672},\n+        1024 * 16384: {'float16': 0.757, 'float32': 0.813},\n+        1024 * 65536: {'float16': 0.847, 'float32': 0.866},\n         # Non pow 2\n-        1020 * 100: {'float16': 0.051, 'bfloat16': 0.051, 'float32': 0.103},\n-        995 * 125: {'float16': 0.063, 'bfloat16': 0.063, 'float32': 0.126},\n-        10003 * 7007: {'float16': 0.544, 'bfloat16': 0.541, 'float32': 0.861},\n+        1020 * 100: {'float16': 0.020, 'float32': 0.041},\n+        10003 * 7007: {'float16': 0.539, 'float32': 0.861},\n     }\n }\n \n@@ -180,7 +173,8 @@ def test_elementwise(N, dtype_str):\n     if dtype_str in ['bfloat16'] and DEVICE_NAME != 'a100':\n         pytest.skip('Only test bfloat16 on a100')\n     dtype = {'float16': torch.float16, 'bfloat16': torch.bfloat16, 'float32': torch.float32}[dtype_str]\n-    ref_gpu_util = elementwise_data[DEVICE_NAME][N][dtype_str]\n+    ref_dtype_str = 'float16' if dtype_str == 'bfloat16' else dtype_str\n+    ref_gpu_util = elementwise_data[DEVICE_NAME][N][ref_dtype_str]\n     max_gpu_perf = get_dram_gbps()\n     z = torch.empty((N, ), dtype=dtype, device='cuda')\n     x = torch.randn_like(z)"}]