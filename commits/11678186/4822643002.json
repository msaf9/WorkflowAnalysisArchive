[{"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 13, "deletions": 8, "changes": 21, "file_content_changes": "@@ -1,4 +1,5 @@\n-import multiprocessing\n+from concurrent.futures import ThreadPoolExecutor\n+\n import os\n import shutil\n from collections import namedtuple\n@@ -217,20 +218,24 @@ def kernel_sub(a, b, o, N: tl.constexpr):\n         tuple(range(4)),\n         ())\n \n-    proc = multiprocessing.Process(\n-        target=triton.compile,\n-        kwargs=dict(\n+    # Define a function to run in the thread\n+    def compile_kernel():\n+        triton.compile(\n             fn=kernel_sub,\n             signature={0: \"*fp32\", 1: \"*fp32\", 2: \"*fp32\"},\n             device=0,\n             constants={3: 32},\n             configs=[config],\n             warm_cache_only=True,\n             cc=cc,\n-        ))\n-    proc.start()\n-    proc.join()\n-    assert proc.exitcode == 0\n+        )\n+\n+    # Create a ThreadPoolExecutor and submit the function to run in a separate thread\n+    with ThreadPoolExecutor(max_workers=1) as executor:\n+        future = executor.submit(compile_kernel)\n+        future.result()  # Wait for the function to complete and retrieve the result (if any)\n+\n+    # No need to check exit code as exceptions in the thread will be propagated to the main thread\n \n \n def test_memory_leak() -> None:"}]