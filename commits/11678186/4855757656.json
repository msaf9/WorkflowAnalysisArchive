[{"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 14, "deletions": 8, "changes": 22, "file_content_changes": "@@ -37,22 +37,28 @@ SmallVector<unsigned> getSizePerThread(Attribute layout);\n // regardless of the shape of the tensor.\n SmallVector<unsigned> getContigPerThread(Attribute layout);\n \n-// Returns the number of unique (distinct) contiguous elements that each thread\n-// has access to, on each dimension of the tensor. E.g. for a blocked layout\n-// with sizePerThread = [1, 4] and tensor shape = [128, 1], returns [1, 1].\n+// Returns the number of non-replicated contiguous elements that each thread\n+// has access to, on each dimension of the tensor. For a blocked layout\n+// with sizePerThread = [1, 4] and tensor shape = [128, 1], the elements\n+// for thread 0 would be [A_{0, 0}, A_{0, 0}, A_{0, 0}, A_{0, 0}], returns [1,\n+// 1]. Whereas for a tensor shape [128, 128], the elements for thread 0 would be\n+// [A_{0, 0}, A_{0, 1}, A_{0, 2}, A_{0, 3}], returns [1, 4].\n SmallVector<unsigned> getUniqueContigPerThread(Type type);\n \n-// Returns the number of threads per warp that have access to unique (distinct)\n-// elements of the tensor. E.g. for a blocked layout with threadsPerWarp = [2,\n-// 16] and tensor shape = [2, 2], returns [2, 1].\n+// Returns the number of threads per warp that have access to non-replicated\n+// elements of the tensor. E.g. for a blocked layout with sizePerThread = [1,\n+// 1], threadsPerWarp = [2, 16] and tensor shape = [2, 2], threads 0, 1, 16, 17\n+// have access to the full tensor, whereas the other threads have access to\n+// replicated elements, so this function returns [2, 2].\n SmallVector<unsigned>\n getThreadsPerWarpWithUniqueData(Attribute layout,\n                                 ArrayRef<int64_t> tensorShape);\n \n-// Returns the number of warps per CTA that have access to unique (distinct)\n+// Returns the number of warps per CTA that have access to non-replicated\n // elements of the tensor. E.g. for a blocked layout with sizePerThread = [1,\n // 1], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4] and tensor shape = [2, 2],\n-// returns [1, 1].\n+// returns [1, 1], since the first warp has access to the full tensor, whereas\n+// the other warps have access to replicated elements.\n SmallVector<unsigned>\n getWarpsPerCTAWithUniqueData(Attribute layout, ArrayRef<int64_t> tensorShape);\n "}]