[{"filename": "python/tests/test_ext_elemwise.py", "status": "modified", "additions": 26, "deletions": 23, "changes": 49, "file_content_changes": "@@ -125,28 +125,40 @@ def kernel(x_ptr,\n \n \n @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n-                         [('float32', 'libdevice.pow', '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'),\n-                          ('int32', 'libdevice.ffs', ''),\n-                          ('float64', 'libdevice.norm4d', '')])\n+                         [('int32', 'libdevice.ffs', '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'),\n+                          ('int32', 'libdevice.ffs', '')])\n def test_libdevice(dtype_str, expr, lib_path):\n-    def patch_kernel(template, to_replace):\n-        kernel = triton.JITFunction(template.fn)\n-        for key, value in to_replace.items():\n-            kernel.src = kernel.src.replace(key, value)\n-        return kernel\n+    src = f\"\"\"\n+def kernel(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    y = tl.{expr}(x)\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\"\"\"\n+    import tempfile\n+    from inspect import Parameter, Signature\n+\n+    import _testcapi\n+\n+    fp = tempfile.NamedTemporaryFile(mode='w', suffix=\".py\")\n+    fp.write(src)\n+    fp.flush()\n+\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        pass\n+    kernel.__code__ = _testcapi.code_newempty(fp.name, \"kernel\", 1)\n+    parameters = []\n+    parameters.append(Parameter(\"X\", 1))\n+    parameters.append(Parameter(\"Y\", 1))\n+    parameters.append(Parameter(\"BLOCK\", 1))\n+    kernel.__signature__ = Signature(parameters=parameters)\n+    kernel = triton.jit(kernel)\n \n     torch_type = {\n         \"int32\": torch.int32,\n         \"float32\": torch.float32,\n         \"float64\": torch.float64\n     }\n \n-    @triton.jit\n-    def kernel(X, Y, BLOCK: tl.constexpr):\n-        x = tl.load(X + tl.arange(0, BLOCK))\n-        y = GENERATE_TEST_HERE\n-        tl.store(Y + tl.arange(0, BLOCK), y)\n-\n     shape = (128, )\n     # limit the range of integers so that the sum does not overflow\n     x = None\n@@ -155,18 +167,9 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     else:\n         x = torch.randn(shape, dtype=torch_type[dtype_str], device=\"cuda\")\n     if expr == 'libdevice.ffs':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.ffs(x)'})\n         y_ref = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n         for i in range(shape[0]):\n             y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n-    elif expr == 'libdevice.pow':\n-        # numpy does not allow negative factors in power, so we use abs()\n-        x = torch.abs(x)\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n-        y_ref = torch.pow(x, x)\n-    elif expr == 'libdevice.norm4d':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.norm4d(x, x, x, x)'})\n-        y_ref = torch.sqrt(4 * torch.pow(x, 2))\n \n     # triton result\n     y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")"}]