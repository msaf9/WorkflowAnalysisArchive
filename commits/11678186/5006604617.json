[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -397,7 +397,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto wordTy = vec_ty(elemTy, minVec);\n     SmallVector<Value> outVals(outElems);\n     for (unsigned i = 0; i < numVecs; ++i) {\n-      Value smemAddr = sharedPtrs[i / minVec * minVec];\n+      Value smemAddr = sharedPtrs[i * minVec];\n       smemAddr = bitcast(smemAddr, ptr_ty(wordTy, 3));\n       Value valVec = load(smemAddr);\n       for (unsigned v = 0; v < minVec; ++v) {"}, {"filename": "python/test/regression/test_functional_regressions.py", "status": "modified", "additions": 68, "deletions": 0, "changes": 68, "file_content_changes": "@@ -1,4 +1,6 @@\n+import numpy as np\n import torch\n+from numpy.random import RandomState\n \n import triton\n import triton.language as tl\n@@ -66,3 +68,69 @@ def chained_matmul_kernel(\n                                 block_k=block_k)\n \n     assert (torch_result == triton_result).all()\n+\n+\n+def test_vecmat():\n+    @triton.jit\n+    def batched_vecmat(\n+        # inputs\n+        A,  # shape: [dim_m, dim_k]\n+        B,  # shape: [dim_m, dim_n, dim_k]\n+        # dimensions\n+        dim_m, dim_n, dim_k,\n+        # outputs\n+        output,\n+        # block information\n+        block_m: tl.constexpr, block_n: tl.constexpr, block_k: tl.constexpr\n+    ):\n+        m_index = tl.program_id(0)\n+        n_index = tl.program_id(1)\n+        # Output tile\n+        output_tile = (m_index * block_m + tl.arange(0, block_m))[:, None] * dim_n \\\n+            + (n_index * block_n + tl.arange(0, block_n))[None, :]\n+\n+        vecmat = tl.zeros([block_m, block_n], dtype=A.dtype.element_ty)\n+        k_blocks = dim_k // block_k\n+        for k_index in range(k_blocks):\n+            # Load A tile\n+            a_tile = (m_index * block_m + tl.arange(0, block_m))[:, None] * dim_k \\\n+                + (k_index * block_k + tl.arange(0, block_k))[None, :]\n+            a = tl.load(A + a_tile)\n+\n+            # Load B tile, transposed to [n, m, k] in order to broadcast A on a\n+            # leading dimension.\n+            b_tile = (m_index * block_m + tl.arange(0, block_m))[None, :, None] * dim_n * dim_k \\\n+                + (n_index * block_n + tl.arange(0, block_n))[:, None, None] * dim_k \\\n+                + (k_index * block_k + tl.arange(0, block_k))[None, None, :]\n+            b = tl.load(B + b_tile)\n+\n+            expanded_a, _ = tl.broadcast(a, b)\n+            vecmat += tl.trans(tl.sum(expanded_a * b, axis=2))\n+\n+        tl.store(output + output_tile, vecmat)\n+\n+    M, N, K = 128, 128, 128\n+    block_m, block_n, block_k = 16, 32, 64\n+\n+    rs = RandomState(17)\n+    A_vec = rs.randint(0, 4, (M, K)).astype('float32')\n+    B_vec = rs.randint(0, 4, (M, N, K)).astype('float32')\n+    A = A_vec\n+    B = B_vec\n+\n+    A_tri = torch.tensor(A, device='cuda')\n+    B_tri = torch.tensor(B, device='cuda')\n+    C_tri = torch.zeros((M, N), dtype=torch.float32, device='cuda')\n+\n+    grid = (M // block_m, N // block_n)\n+\n+    batched_vecmat[grid](A_tri, B_tri, M, N, K, C_tri,\n+                         block_m=block_m, block_n=block_n, block_k=block_k,\n+                         num_warps=4, num_stages=1)\n+\n+    A_expanded = A[:, np.newaxis, :]\n+    A_broadcasted = np.broadcast_to(A_expanded, (M, N, K))\n+    AB = A_broadcasted * B\n+    C_ref = np.sum(AB, axis=2)\n+\n+    np.testing.assert_allclose(C_ref, C_tri.cpu().numpy(), rtol=0.01, atol=1e-3)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 34, "deletions": 61, "changes": 95, "file_content_changes": "@@ -1604,59 +1604,6 @@ def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device='cuda'):\n     np.testing.assert_allclose(y_ref, y_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n \n \n-layouts = [\n-    [BlockedLayout([1, 1], [16, 2], [1, 4], [0, 1]), SharedLayout(1, 1, 1, [1, 0])],\n-    [BlockedLayout([1, 1], [16, 2], [1, 4], [0, 1]), SharedLayout(4, 2, 4, [1, 0])],\n-    [BlockedLayout([1, 1], [16, 2], [1, 4], [0, 1]), SharedLayout(2, 2, 4, [1, 0])],\n-    [BlockedLayout([1, 1], [8, 4], [2, 2], [0, 1]), SharedLayout(4, 2, 4, [1, 0])],\n-]\n-\n-\n-@pytest.mark.parametrize(\"M, N\", [[64, 64], [64, 128], [128, 64], [128, 128]])\n-@pytest.mark.parametrize(\"src, dst\", layouts)\n-def test_convert_shared(M, N, src, dst, device='cuda'):\n-    ir = f\"\"\"\n-    #src = {src}\n-    #dst = {dst}\n-    module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n-        tt.func public @kernel(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg2: i32 {{tt.divisibility = 16 : i32}}) {{\n-            %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-            %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n-            %2 = tt.splat %arg2 : (i32) -> tensor<{M}x1xi32, #src>\n-            %3 = arith.muli %1, %2 : tensor<{M}x1xi32, #src>\n-            %4 = tt.broadcast %3 : (tensor<{M}x1xi32, #src>) -> tensor<{M}x{N}xi32, #src>\n-            %5 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #src}}>>\n-            %6 = tt.expand_dims %5 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #src}}>>) -> tensor<1x{N}xi32, #src>\n-            %7 = tt.broadcast %6 : (tensor<1x{N}xi32, #src>) -> tensor<{M}x{N}xi32, #src>\n-            %8 = arith.addi %4, %7 : tensor<{M}x{N}xi32, #src>\n-            %9 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x{N}x!tt.ptr<i32>, #src>\n-            %10 = tt.addptr %9, %8 : tensor<{M}x{N}x!tt.ptr<i32>, #src>, tensor<{M}x{N}xi32, #src>\n-            %11 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #src>\n-            %12 = triton_gpu.convert_layout %11 : (tensor<{M}x{N}xi32, #src>) -> tensor<{M}x{N}xi32, #dst>\n-            %13 = triton_gpu.convert_layout %12 : (tensor<{M}x{N}xi32, #dst>) -> tensor<{M}x{N}xi32, #src>\n-            %15 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<{M}x{N}x!tt.ptr<i32>, #src>\n-            %16 = tt.addptr %15, %8 : tensor<{M}x{N}x!tt.ptr<i32>, #src>, tensor<{M}x{N}xi32, #src>\n-            tt.store %16, %13 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{M}x{N}xi32, #src>\n-            tt.return\n-        }}\n-    }}\n-    \"\"\"\n-    import tempfile\n-    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n-        f.write(ir)\n-        f.flush()\n-        kernel = triton.compile(f.name)\n-\n-    rs = RandomState(17)\n-    x = rs.randint(0, 4, (M, N)).astype('int32')\n-    y = np.zeros((M, N), dtype='int32')\n-    x_tri = torch.tensor(x, device=device)\n-    y_tri = torch.tensor(y, device=device)\n-    kernel[(1, 1, 1)](x_tri, y_tri, x_tri.stride(0))\n-    y_ref = x\n-    np.testing.assert_allclose(y_ref, y_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n-\n-\n @triton.jit\n def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n     delta = mean_2 - mean_1\n@@ -2868,22 +2815,49 @@ def kernel(Out):\n     BlockedLayout([4, 4], [1, 32], [4, 1], [1, 0])\n ]\n \n+intermediate_layouts = [\n+    None,\n+    SharedLayout(1, 1, 1, [1, 0]),\n+    SharedLayout(4, 2, 4, [1, 0]),\n+    SharedLayout(2, 2, 4, [1, 0]),\n+]\n+\n \n @pytest.mark.parametrize(\"shape\", [(128, 128)])\n @pytest.mark.parametrize(\"dtype\", ['float16'])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n+@pytest.mark.parametrize(\"interm_layout\", intermediate_layouts)\n @pytest.mark.parametrize(\"dst_layout\", layouts)\n-def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n+def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device='cuda'):\n     if str(src_layout) == str(dst_layout):\n         pytest.skip()\n     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):\n         pytest.skip()\n \n-    ir = f\"\"\"\n-#src = {src_layout}\n-#dst = {dst_layout}\n-\"\"\" + \"\"\"\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+    layouts = f\"\"\"\n+    #src = {src_layout}\n+    #dst = {dst_layout}\n+    \"\"\" if interm_layout is None else f\"\"\"\n+    #src = {src_layout}\n+    #interm = {interm_layout}\n+    #dst = {dst_layout}\n+    \"\"\"\n+\n+    conversion = f\"\"\"\n+    %12 = triton_gpu.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n+    %13 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n+    \"\"\" if interm_layout is None else f\"\"\"\n+    %15 = triton_gpu.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #interm>\n+    %16 = triton_gpu.convert_layout %15 : (tensor<128x128xi32, #interm>) -> tensor<128x128xi32, #src>\n+    %17 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #interm>\n+    %18 = triton_gpu.convert_layout %17 : (tensor<128x128xf16, #interm>) -> tensor<128x128xf16, #src>\n+\n+    %12 = triton_gpu.convert_layout %16 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n+    %13 = triton_gpu.convert_layout %18 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n+    \"\"\"\n+\n+    ir = layouts + \"\"\"\n+    module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   tt.func public @kernel_0d1d(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n     %cst = arith.constant dense<128> : tensor<128x1xi32, #src>\n     %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #src}>>\n@@ -2898,8 +2872,7 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n     %10 = tt.addptr %2, %9 : tensor<128x128x!tt.ptr<f16>, #src>, tensor<128x128xi32, #src>\n     %11 = tt.load %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf16, #src>\n     %3 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>, #dst>\n-    %12 = triton_gpu.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n-    %13 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n+    \"\"\" + conversion + \"\"\"\n     %14 = tt.addptr %3, %12 : tensor<128x128x!tt.ptr<f16>, #dst>, tensor<128x128xi32, #dst>\n     tt.store %14, %13 : tensor<128x128xf16, #dst>\n     tt.return"}]