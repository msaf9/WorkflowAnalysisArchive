[{"filename": "include/triton/Analysis/Allocation.h", "status": "modified", "additions": 1, "deletions": 5, "changes": 6, "file_content_changes": "@@ -19,11 +19,7 @@ class AllocationAnalysis;\n \n SmallVector<unsigned>\n getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n-                             unsigned &outVec);\n-\n-SmallVector<int64_t>\n-getScratchConfigForCvtLayoutSwizzled(triton::gpu::ConvertLayoutOp op,\n-                                     unsigned &inVec, unsigned &outVec);\n+                             unsigned &outVec, bool withPadding);\n \n } // namespace triton\n "}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 14, "deletions": 50, "changes": 64, "file_content_changes": "@@ -49,48 +49,9 @@ getCvtOrder(Attribute srcLayout, Attribute dstLayout) {\n   return {inOrd, outOrd};\n }\n \n-SmallVector<int64_t>\n-getScratchConfigForCvtLayoutSwizzled(triton::gpu::ConvertLayoutOp op,\n-                                     unsigned &inVec, unsigned &outVec) {\n-  auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n-  auto dstTy = op.getResult().getType().cast<RankedTensorType>();\n-  Attribute srcLayout = srcTy.getEncoding();\n-  Attribute dstLayout = dstTy.getEncoding();\n-\n-  // MmaToDotShortcut doesn't use shared mem\n-  if (srcLayout.isa<MmaEncodingAttr>() &&\n-      dstLayout.isa<DotOperandEncodingAttr>())\n-    if (isMmaToDotShortcut(srcTy, dstTy))\n-      return {};\n-\n-  assert(srcLayout && dstLayout &&\n-         \"Unexpected layout in getScratchConfigForCvtLayout()\");\n-  auto [inOrd, outOrd] = getCvtOrder(srcLayout, dstLayout);\n-  unsigned srcContigPerThread = getContigPerThread(srcLayout)[inOrd[0]];\n-  unsigned dstContigPerThread = getContigPerThread(dstLayout)[outOrd[0]];\n-  // TODO: Fix the legacy issue that ourOrd[0] == 0 always means\n-  //       that we cannot do vectorization.\n-  inVec = outOrd[0] == 0 ? 1 : inOrd[0] == 0 ? 1 : srcContigPerThread;\n-  outVec = outOrd[0] == 0 ? 1 : dstContigPerThread;\n-\n-  auto srcShape = srcTy.getShape();\n-  auto dstShape = dstTy.getShape();\n-  auto srcShapePerCTA = getShapePerCTA(srcLayout, srcShape);\n-  auto dstShapePerCTA = getShapePerCTA(dstLayout, dstShape);\n-\n-  unsigned rank = dstTy.getRank();\n-  SmallVector<int64_t> paddedRepShape(rank);\n-  for (unsigned d = 0; d < rank; ++d) {\n-    paddedRepShape[d] =\n-        std::max(std::min<int64_t>(srcTy.getShape()[d], srcShapePerCTA[d]),\n-                 std::min<int64_t>(dstTy.getShape()[d], dstShapePerCTA[d]));\n-  }\n-  return paddedRepShape;\n-}\n-\n SmallVector<unsigned>\n getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n-                             unsigned &outVec) {\n+                             unsigned &outVec, bool withPadding) {\n   auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n   auto dstTy = op.getResult().getType().cast<RankedTensorType>();\n   Attribute srcLayout = srcTy.getEncoding();\n@@ -118,21 +79,23 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   auto dstShapePerCTA = getShapePerCTA(dstLayout, dstShape);\n \n   unsigned rank = dstTy.getRank();\n-  SmallVector<unsigned> paddedRepShape(rank);\n-  unsigned pad = std::max(inVec, outVec);\n+  SmallVector<unsigned> repShape(rank);\n   for (unsigned d = 0; d < rank; ++d) {\n-    paddedRepShape[d] =\n+    repShape[d] =\n         std::max(std::min<unsigned>(srcTy.getShape()[d], srcShapePerCTA[d]),\n                  std::min<unsigned>(dstTy.getShape()[d], dstShapePerCTA[d]));\n   }\n   if (rank == 1)\n-    return paddedRepShape;\n-  unsigned paddedDim = 1;\n-  if (auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>()) {\n-    paddedDim = dstBlockedLayout.getOrder()[0];\n+    return repShape;\n+  if (withPadding) {\n+    unsigned paddedDim = 1;\n+    if (auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>()) {\n+      paddedDim = dstBlockedLayout.getOrder()[0];\n+    }\n+    unsigned pad = std::max(inVec, outVec);\n+    repShape[paddedDim] += pad;\n   }\n-  paddedRepShape[paddedDim] += pad;\n-  return paddedRepShape;\n+  return repShape;\n }\n \n // TODO: extend beyond scalars\n@@ -223,7 +186,8 @@ class AllocationAnalysis {\n       //       conditions, such as warp-shuffle\n       unsigned inVec = 0;\n       unsigned outVec = 0;\n-      auto smemShape = getScratchConfigForCvtLayout(cvtLayout, inVec, outVec);\n+      auto smemShape =\n+          getScratchConfigForCvtLayout(cvtLayout, inVec, outVec, true);\n       unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n                                        std::multiplies{});\n       auto bytes ="}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 71, "deletions": 226, "changes": 297, "file_content_changes": "@@ -203,9 +203,8 @@ struct ConvertLayoutOpConversion\n                       bool stNotRd, RankedTensorType type,\n                       ArrayRef<unsigned> numCTAsEachRep,\n                       ArrayRef<unsigned> multiDimRepId, unsigned vec,\n-                      ArrayRef<int64_t> paddedRepShape,\n-                      ArrayRef<unsigned> outOrd, SmallVector<Value> &vals,\n-                      Value smemBase,\n+                      ArrayRef<unsigned> repShape, ArrayRef<unsigned> outOrd,\n+                      SmallVector<Value> &vals, Value smemBase,\n                       triton::gpu::SharedEncodingAttr sharedLayout) const {\n     auto accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n     auto layout = type.getEncoding();\n@@ -229,22 +228,21 @@ struct ConvertLayoutOpConversion\n       elemTy = IntegerType::get(elemTy.getContext(), 64);\n \n     auto llvmElemTy = getTypeConverter()->convertType(elemTy);\n-\n+    SmallVector<int64_t> repShapeLong;\n+    for (unsigned i = 0; i < repShape.size(); ++i) {\n+      repShapeLong.push_back(repShape[i]);\n+    }\n+    auto smemStrides = getStridesFromShapeAndOrder(\n+        repShapeLong, getOrder(sharedLayout), loc, rewriter);\n+    SmallVector<Value> smemOffsetVals(rank, i32_val(0));\n+\n+    SharedMemoryObject smemObj(smemBase, smemStrides, smemOffsetVals);\n+    auto sharedVec = sharedLayout.getVec();\n+    unsigned minVec = std::min(vec, sharedVec);\n+    DenseMap<unsigned, Value> sharedPtrs =\n+        getSwizzledSharedPtrs(loc, vec, type, sharedLayout, llvmElemTy, smemObj,\n+                              rewriter, smemOffsetVals, smemStrides);\n     for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n-      auto smemStrides = getStridesFromShapeAndOrder(\n-          paddedRepShape, getOrder(sharedLayout), loc, rewriter);\n-      SmallVector<Value> smemOffsetVals;\n-      for (unsigned i = 0; i < rank; ++i)\n-        smemOffsetVals.push_back(i32_val(0));\n-\n-      SharedMemoryObject smemObj(smemBase, smemStrides, smemOffsetVals);\n-      auto sharedVec = sharedLayout.getVec();\n-      unsigned minVec = std::min(vec, sharedVec);\n-      DenseMap<unsigned, Value> sharedPtrs =\n-          getSwizzledSharedPtrs(loc, vec, type, sharedLayout, llvmElemTy,\n-                                smemObj, rewriter, smemOffsetVals, smemStrides);\n-      auto wordTy = vec_ty(llvmElemTy, minVec);\n-\n       auto multiDimCTAInRepId =\n           getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep, order);\n       SmallVector<unsigned> multiDimCTAId(rank);\n@@ -258,15 +256,18 @@ struct ConvertLayoutOpConversion\n       auto linearCTAIdInRep =\n           getLinearIndex<unsigned>(multiDimCTAInRepId, numCTAs, order);\n \n-      auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n       Value word;\n+      auto wordTy = vec_ty(llvmElemTy, minVec);\n       if (stNotRd) {\n         for (unsigned i = 0; i < accumSizePerThread; ++i) {\n           if (i % minVec == 0)\n             word = undef(wordTy);\n-          word = insert_element(wordTy, word,\n-                                vals[linearCTAId * accumSizePerThread + i],\n-                                i32_val(i % minVec));\n+          auto currVal = vals[linearCTAId * accumSizePerThread + i];\n+          if (isInt1)\n+            currVal = zext(llvmElemTy, currVal);\n+          else if (isPtr)\n+            currVal = ptrtoint(llvmElemTy, currVal);\n+          word = insert_element(wordTy, word, currVal, i32_val(i % minVec));\n \n           if (i % minVec == minVec - 1) {\n \n@@ -288,90 +289,14 @@ struct ConvertLayoutOpConversion\n \n           Value valVec = load(smemAddr);\n           for (unsigned v = 0; v < minVec; ++v) {\n-            Value currVal = extract_element(llvmElemTy, valVec, i32_val(v));\n-            vals[linearCTAId * accumSizePerThread + i * minVec + v] = currVal;\n-          }\n-        }\n-      }\n-    }\n-  }\n-\n-  void processReplicaOriginal(Location loc, ConversionPatternRewriter &rewriter,\n-                              bool stNotRd, RankedTensorType type,\n-                              ArrayRef<unsigned> numCTAsEachRep,\n-                              ArrayRef<unsigned> multiDimRepId, unsigned vec,\n-                              ArrayRef<unsigned> paddedRepShape,\n-                              ArrayRef<unsigned> outOrd,\n-                              SmallVector<Value> &vals, Value smemBase) const {\n-    auto accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n-    auto layout = type.getEncoding();\n-    auto rank = type.getRank();\n-    auto sizePerThread = getSizePerThread(layout);\n-    auto accumSizePerThread = product<unsigned>(sizePerThread);\n-    SmallVector<unsigned> numCTAs(rank);\n-    auto shapePerCTA = getShapePerCTA(layout, type.getShape());\n-    auto order = getOrder(layout);\n-    for (unsigned d = 0; d < rank; ++d) {\n-      numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n-    }\n-    auto elemTy = type.getElementType();\n-    bool isInt1 = elemTy.isInteger(1);\n-    bool isPtr = elemTy.isa<triton::PointerType>();\n-    auto llvmElemTyOrig = getTypeConverter()->convertType(elemTy);\n-    if (isInt1)\n-      elemTy = IntegerType::get(elemTy.getContext(), 8);\n-    else if (isPtr)\n-      elemTy = IntegerType::get(elemTy.getContext(), 64);\n-\n-    auto llvmElemTy = getTypeConverter()->convertType(elemTy);\n-\n-    for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n-      auto multiDimCTAInRepId =\n-          getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep, order);\n-      SmallVector<unsigned> multiDimCTAId(rank);\n-      for (const auto &it : llvm::enumerate(multiDimCTAInRepId)) {\n-        auto d = it.index();\n-        multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n-      }\n-\n-      auto linearCTAId =\n-          getLinearIndex<unsigned>(multiDimCTAId, numCTAs, order);\n-      // TODO: This is actually redundant index calculation, we should\n-      //       consider of caching the index calculation result in case\n-      //       of performance issue observed.\n-      for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n-        SmallVector<Value> multiDimOffset =\n-            getMultiDimOffset(layout, loc, rewriter, elemId, type,\n-                              multiDimCTAInRepId, shapePerCTA);\n-        Value offset =\n-            linearize(rewriter, loc, multiDimOffset, paddedRepShape, outOrd);\n-\n-        auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n-        Value ptr = gep(elemPtrTy, smemBase, offset);\n-        auto vecTy = vec_ty(llvmElemTy, vec);\n-        ptr = bitcast(ptr, ptr_ty(vecTy, 3));\n-        if (stNotRd) {\n-          Value valVec = undef(vecTy);\n-          for (unsigned v = 0; v < vec; ++v) {\n-            auto currVal = vals[elemId + linearCTAId * accumSizePerThread + v];\n-            if (isInt1)\n-              currVal = zext(llvmElemTy, currVal);\n-            else if (isPtr)\n-              currVal = ptrtoint(llvmElemTy, currVal);\n-            valVec = insert_element(vecTy, valVec, currVal, i32_val(v));\n-          }\n-          store(valVec, ptr);\n-        } else {\n-          Value valVec = load(ptr);\n-          for (unsigned v = 0; v < vec; ++v) {\n             Value currVal = extract_element(llvmElemTy, valVec, i32_val(v));\n             if (isInt1)\n               currVal = icmp_ne(currVal,\n                                 rewriter.create<LLVM::ConstantOp>(\n                                     loc, i8_ty, rewriter.getI8IntegerAttr(0)));\n             else if (isPtr)\n               currVal = inttoptr(llvmElemTyOrig, currVal);\n-            vals[elemId + linearCTAId * accumSizePerThread + v] = currVal;\n+            vals[linearCTAId * accumSizePerThread + i * minVec + v] = currVal;\n           }\n         }\n       }\n@@ -485,20 +410,20 @@ struct ConvertLayoutOpConversion\n   lowerDistributedToDistributed(triton::gpu::ConvertLayoutOp op,\n                                 OpAdaptor adaptor,\n                                 ConversionPatternRewriter &rewriter) const {\n-    bool enabled = true;\n     auto loc = op.getLoc();\n     Value src = op.getSrc();\n     Value dst = op.getResult();\n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n     Attribute dstLayout = dstTy.getEncoding();\n-    if (!srcLayout.isa<BlockedEncodingAttr>() ||\n-        !dstLayout.isa<MmaEncodingAttr>() //||\n-        // triton::gpu::getWarpsPerCTA(dstLayout)[1] != 1 ||\n-        // dstTy.getElementType().isF32()\n-    ) {\n-      enabled = false;\n+    if (!isaDistributedLayout(srcLayout)) {\n+      assert(0 && \"ConvertLayout with input layout not implemented\");\n+      return failure();\n+    }\n+    if (!isaDistributedLayout(dstLayout)) {\n+      assert(0 && \"ConvertLayout with output layout not implemented\");\n+      return failure();\n     }\n \n     auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n@@ -553,137 +478,57 @@ struct ConvertLayoutOpConversion\n     auto outOrd = getOrder(dstLayout);\n     SmallVector<Value> outVals(outElems);\n \n+    unsigned inVec = 0;\n+    unsigned outVec = 0;\n+    SmallVector<unsigned> repShape;\n+    triton::gpu::SharedEncodingAttr sharedLayout;\n+    if (srcLayout.isa<BlockedEncodingAttr>() &&\n+        dstLayout.isa<MmaEncodingAttr>() && !isDstMmaV1) {\n+      repShape = getScratchConfigForCvtLayout(op, inVec, outVec, false);\n+      SmallVector<int64_t> repShapeLong;\n+      for (unsigned i = 0; i < repShape.size(); ++i) {\n+        repShapeLong.push_back(repShape[i]);\n+      }\n+      auto dstDotOp = triton::gpu::DotOperandEncodingAttr::get(\n+          getContext(), 0, dstLayout, dstTy.getElementType());\n+      sharedLayout = triton::gpu::SharedEncodingAttr::get(\n+          getContext(), dstDotOp, repShapeLong, getOrder(dstLayout),\n+          srcTy.getElementType());\n+    } else if (isSrcMmaV1 || isDstMmaV1) {\n+      repShape = getScratchConfigForCvtLayout(op, inVec, outVec, true);\n+    } else {\n+      repShape = getScratchConfigForCvtLayout(op, inVec, outVec, false);\n+      auto maxVec = std::max(inVec, outVec);\n+      sharedLayout = triton::gpu::SharedEncodingAttr::get(\n+          getContext(), maxVec, 1, repShape[0], getOrder(dstLayout));\n+    }\n+\n     for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n       auto multiDimRepId =\n           getMultiDimIndex<unsigned>(repId, numReplicates, outOrd);\n       if (repId != 0)\n         barrier();\n-      if (srcLayout.isa<BlockedEncodingAttr>() ||\n-          srcLayout.isa<SliceEncodingAttr>() ||\n-          srcLayout.isa<MmaEncodingAttr>()) {\n-        if (isSrcMmaV1) {\n-          unsigned inVec = 0;\n-          unsigned outVec = 0;\n-          auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n-          processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ true, srcTy,\n-                                 multiDimRepId, inVec, paddedRepShape, outOrd,\n-                                 vals, smemBase, shape);\n-        } else {\n-          if (enabled) {\n-            unsigned inVec = 0;\n-            unsigned outVec = 0;\n-            auto paddedRepShape =\n-                getScratchConfigForCvtLayoutSwizzled(op, inVec, outVec);\n-            auto dstDotOp = triton::gpu::DotOperandEncodingAttr::get(\n-                getContext(), 0, dstLayout, dstTy.getElementType());\n-            auto sharedLayout = triton::gpu::SharedEncodingAttr::get(\n-                getContext(), dstDotOp, paddedRepShape, getOrder(dstLayout),\n-                srcTy.getElementType());\n-            processReplica(loc, rewriter, /*stNotRd*/ true, srcTy,\n-                           inNumCTAsEachRep, multiDimRepId, inVec,\n-                           paddedRepShape, outOrd, vals, smemBase,\n-                           sharedLayout);\n-          } else {\n-            if (rank == 2) {\n-              unsigned inVec = 0;\n-              unsigned outVec = 0;\n-              auto paddedRepShape =\n-                  getScratchConfigForCvtLayoutSwizzled(op, inVec, outVec);\n-              auto maxVec = std::max(inVec, outVec);\n-              auto sharedLayout = triton::gpu::SharedEncodingAttr::get(\n-                  getContext(), maxVec, 1, paddedRepShape[0],\n-                  getOrder(dstLayout));\n-\n-              processReplica(loc, rewriter, /*stNotRd*/ true, srcTy,\n-                             inNumCTAsEachRep, multiDimRepId, inVec,\n-                             paddedRepShape, outOrd, vals, smemBase,\n-                             sharedLayout);\n-            } else {\n-              unsigned inVec = 0;\n-              unsigned outVec = 0;\n-              auto paddedRepShape =\n-                  getScratchConfigForCvtLayoutSwizzled(op, inVec, outVec);\n-              auto maxVec = std::max(inVec, outVec);\n-              auto sharedLayout = triton::gpu::SharedEncodingAttr::get(\n-                  getContext(), maxVec, 1, paddedRepShape[0],\n-                  getOrder(dstLayout));\n-\n-              processReplica(loc, rewriter, /*stNotRd*/ true, srcTy,\n-                             inNumCTAsEachRep, multiDimRepId, inVec,\n-                             paddedRepShape, outOrd, vals, smemBase,\n-                             sharedLayout);\n-            }\n-          }\n-        }\n \n+      if (isSrcMmaV1) {\n+        processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ true, srcTy,\n+                               multiDimRepId, inVec, repShape, outOrd, vals,\n+                               smemBase, shape);\n       } else {\n-        assert(0 && \"ConvertLayout with input layout not implemented\");\n-        return failure();\n+        processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,\n+                       multiDimRepId, inVec, repShape, outOrd, vals, smemBase,\n+                       sharedLayout);\n       }\n \n       barrier();\n-      if (dstLayout.isa<BlockedEncodingAttr>() ||\n-          dstLayout.isa<SliceEncodingAttr>() ||\n-          dstLayout.isa<MmaEncodingAttr>()) {\n-        if (isDstMmaV1) {\n-          unsigned inVec = 0;\n-          unsigned outVec = 0;\n-          auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n-          processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ false, dstTy,\n-                                 multiDimRepId, outVec, paddedRepShape, outOrd,\n-                                 outVals, smemBase, shape,\n-                                 /*isDestMma=*/true);\n-        }\n-\n-        else {\n-          if (enabled) {\n-            unsigned inVec = 0;\n-            unsigned outVec = 0;\n-            auto paddedRepShape =\n-                getScratchConfigForCvtLayoutSwizzled(op, inVec, outVec);\n-            auto dstDotOp = triton::gpu::DotOperandEncodingAttr::get(\n-                getContext(), 0, dstLayout, dstTy.getElementType());\n-            auto sharedLayout = triton::gpu::SharedEncodingAttr::get(\n-                getContext(), dstDotOp, paddedRepShape, getOrder(dstLayout),\n-                srcTy.getElementType());\n-            processReplica(loc, rewriter, /*stNotRd*/ false, dstTy,\n-                           outNumCTAsEachRep, multiDimRepId, outVec,\n-                           paddedRepShape, outOrd, outVals, smemBase,\n-                           sharedLayout);\n-          } else {\n-            if (rank == 2) {\n-              unsigned inVec = 0;\n-              unsigned outVec = 0;\n-              auto paddedRepShape =\n-                  getScratchConfigForCvtLayoutSwizzled(op, inVec, outVec);\n-              auto maxVec = std::max(inVec, outVec);\n-              auto sharedLayout = triton::gpu::SharedEncodingAttr::get(\n-                  getContext(), maxVec, 1, paddedRepShape[0],\n-                  getOrder(dstLayout));\n-              processReplica(loc, rewriter, /*stNotRd*/ false, dstTy,\n-                             outNumCTAsEachRep, multiDimRepId, outVec,\n-                             paddedRepShape, outOrd, outVals, smemBase,\n-                             sharedLayout);\n-            } else {\n-              unsigned inVec = 0;\n-              unsigned outVec = 0;\n-              auto paddedRepShape =\n-                  getScratchConfigForCvtLayoutSwizzled(op, inVec, outVec);\n-              auto maxVec = std::max(inVec, outVec);\n-              auto sharedLayout = triton::gpu::SharedEncodingAttr::get(\n-                  getContext(), maxVec, 1, paddedRepShape[0],\n-                  getOrder(dstLayout));\n-              processReplica(loc, rewriter, /*stNotRd*/ false, dstTy,\n-                             outNumCTAsEachRep, multiDimRepId, outVec,\n-                             paddedRepShape, outOrd, outVals, smemBase,\n-                             sharedLayout);\n-            }\n-          }\n-        }\n-\n+      if (isDstMmaV1) {\n+        processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ false, dstTy,\n+                               multiDimRepId, outVec, repShape, outOrd, outVals,\n+                               smemBase, shape,\n+                               /*isDestMma=*/true);\n       } else {\n-        assert(0 && \"ConvertLayout with output layout not implemented\");\n-        return failure();\n+        processReplica(loc, rewriter, /*stNotRd*/ false, dstTy,\n+                       outNumCTAsEachRep, multiDimRepId, outVec, repShape,\n+                       outOrd, outVals, smemBase, sharedLayout);\n       }\n     }\n "}]