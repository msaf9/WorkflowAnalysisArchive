[{"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -37,7 +37,7 @@ SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout);\n \n SmallVector<unsigned>\n getShapePerCTA(const Attribute &layout,\n-               ArrayRef<int64_t> blockShape = ArrayRef<int64_t>());\n+               ArrayRef<int64_t> tensorShape = ArrayRef<int64_t>());\n \n SmallVector<unsigned> getOrder(const Attribute &layout);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -120,7 +120,7 @@ struct ConvertLayoutOpConversion\n         mmaColIdx[0] = add(mmaThreadIdInGrpM2, colWarpOffset);\n         mmaColIdx[1] = add(mmaThreadIdInGrpM2P1, colWarpOffset);\n       } else if (mmaLayout.isVolta()) {\n-        llvm_unreachable(\"Volta doesn't follow the pattern here.\");\n+        // Volta doesn't follow the pattern here.\"\n       } else {\n         llvm_unreachable(\"Unexpected MMALayout version\");\n       }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -75,8 +75,8 @@ struct BroadcastOpConversion\n       // be all right.\n       // TODO[Superjomn]: Replace this with a generic implementation.\n       if (srcMma.isVolta()) {\n-        assert(srcTy.getElementType().isF16() ||\n-               srcTy.getElementType().isBF16());\n+        assert(srcTy.getElementType().isF16() &&\n+               \"Unexpected data type on Volta\");\n         int numElemsPerThread = srcMma.getElemsPerThread(resultTy.getShape());\n         int srcUniqElems = srcVals.size() / 2;\n         int dup = numElemsPerThread / srcUniqElems;"}]