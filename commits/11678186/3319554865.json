[{"filename": ".github/workflows/wheels.yml", "status": "added", "additions": 40, "deletions": 0, "changes": 40, "file_content_changes": "@@ -0,0 +1,40 @@\n+name: Wheels\n+on:\n+  workflow_dispatch:\n+  schedule:    \n+    - cron: \"0 0 * * *\"\n+\n+jobs:\n+\n+  Build-Wheels:\n+    \n+    runs-on: [self-hosted, V100]\n+\n+    steps:\n+\n+      - name: Checkout\n+        uses: actions/checkout@v2\n+\n+      - name: Patch setup.py\n+        run: |\n+          #sed -i 's/name\\=\\\"triton\\\"/name=\"triton-nightly\"/g' python/setup.py\n+          export LATEST_DATE=$(TZ=UTC0 git show --quiet --date='format-local:%Y%m%d' --format=\"%cd\")\n+          sed -i -r \"s/version\\=\\\"(.*)\\\"/version=\\\"\\1-dev\"$LATEST_DATE\"\\\"/g\" python/setup.py\n+          echo \"\" >> python/setup.cfg\n+          echo \"[build_ext]\" >> python/setup.cfg\n+          echo \"base-dir=/project\" >> python/setup.cfg\n+\n+      - name: Build wheels\n+        run: |\n+          export CIBW_MANYLINUX_X86_64_IMAGE=\"manylinux2014\"\n+          export CIBW_MANYLINUX_PYPY_X86_64_IMAGE=\"manylinux2014\"\n+          export CIBW_BEFORE_BUILD=\"pip install cmake;\\\n+                                    yum install -y llvm11 llvm11-devel llvm11-static llvm11-libs zlib-devel;\"\n+          export CIBW_SKIP=\"{cp,pp}35-*\"\n+          export CIBW_BUILD=\"{cp,pp}3*-manylinux_x86_64\"\n+          python3 -m cibuildwheel python --output-dir wheelhouse\n+\n+\n+      - name: Upload wheels to PyPI\n+        run: |\n+          python3 -m twine upload wheelhouse/* --skip-existing\n\\ No newline at end of file"}, {"filename": "LICENSE", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,6 +1,6 @@\n /* \n * Copyright 2018-2020 Philippe Tillet\n-* Copyright 2020-2021 OpenAI\n+* Copyright 2020-2022 OpenAI\n * \n * Permission is hereby granted, free of charge, to any person obtaining \n * a copy of this software and associated documentation files \n@@ -20,4 +20,4 @@\n * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, \n * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE \n * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n-*/\n\\ No newline at end of file\n+*/"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 117, "deletions": 16, "changes": 133, "file_content_changes": "@@ -859,9 +859,8 @@ struct LoadOpConversion\n     bool otherIsSplatConstInt = false;\n     DenseElementsAttr constAttr;\n     int64_t splatVal = 0;\n-    if (valueElemTy.isa<IntegerType>() &&\n-        matchPattern(op.other(), m_Constant(&constAttr)) &&\n-        constAttr.isSplat()) {\n+    if (other && valueElemTy.isa<IntegerType>() &&\n+        matchPattern(other, m_Constant(&constAttr)) && constAttr.isSplat()) {\n       otherIsSplatConstInt = true;\n       splatVal = constAttr.getSplatValue<APInt>().getSExtValue();\n     }\n@@ -1471,6 +1470,85 @@ struct BinaryOpConversion\n   Value getRhs(OpAdaptor adaptor) const { return adaptor.getRhs(); }\n };\n \n+//\n+// Unary\n+//\n+\n+template <typename SourceOp, typename DestOp, typename ConcreteT>\n+class UnaryOpConversionBase : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n+\n+public:\n+  using OpAdaptor = typename SourceOp::Adaptor;\n+\n+  explicit UnaryOpConversionBase(LLVMTypeConverter &typeConverter,\n+                                 PatternBenefit benefit = 1)\n+      : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n+\n+  LogicalResult\n+  matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto resultTy = op.getType().template dyn_cast<RankedTensorType>();\n+\n+    // ArithmeticToLLVM will handle the lowering of scalar ArithOps\n+    if (!resultTy)\n+      return failure();\n+\n+    Location loc = op->getLoc();\n+    auto resultLayout =\n+        resultTy.getEncoding().template dyn_cast<BlockedEncodingAttr>();\n+    auto resultShape = resultTy.getShape();\n+    assert(resultLayout && \"Unexpected resultLayout in UnaryOpConversion\");\n+    unsigned elems = resultLayout.getElemsPerThread(resultShape);\n+    Type elemTy =\n+        this->getTypeConverter()->convertType(resultTy.getElementType());\n+    SmallVector<Type> types(elems, elemTy);\n+    Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n+\n+    auto *concreteThis = static_cast<const ConcreteT *>(this);\n+    auto srcs = this->getElementsFromStruct(loc, concreteThis->getSrc(adaptor),\n+                                            rewriter);\n+    SmallVector<Value> resultVals(elems);\n+    for (unsigned i = 0; i < elems; ++i) {\n+      resultVals[i] =\n+          concreteThis->createDestOp(op, rewriter, elemTy, srcs[i], loc);\n+    }\n+    Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n+    rewriter.replaceOp(op, view);\n+    return success();\n+  }\n+};\n+\n+template <typename SourceOp, typename DestOp>\n+struct UnaryOpConversion\n+    : public UnaryOpConversionBase<SourceOp, DestOp,\n+                                   UnaryOpConversion<SourceOp, DestOp>> {\n+\n+  explicit UnaryOpConversion(LLVMTypeConverter &typeConverter,\n+                             PatternBenefit benefit = 1)\n+      : UnaryOpConversionBase<SourceOp, DestOp,\n+                              UnaryOpConversion<SourceOp, DestOp>>(\n+            typeConverter, benefit) {}\n+\n+  using OpAdaptor = typename SourceOp::Adaptor;\n+  // An interface to support variant DestOp builder.\n+  DestOp createDestOp(SourceOp op, ConversionPatternRewriter &rewriter,\n+                      Type elemTy, Value src, Location loc) const {\n+    return rewriter.create<DestOp>(loc, elemTy, src);\n+  }\n+\n+  // Get the source operand of the op.\n+  Value getSrc(OpAdaptor adaptor) const {\n+    auto operands = adaptor.getOperands();\n+    if (operands.size() > 1)\n+      llvm::report_fatal_error(\"unary operator has more than one operand\");\n+    return operands.front();\n+  }\n+};\n+\n+//\n+// comparisons\n+//\n+\n struct CmpIOpConversion\n     : public BinaryOpConversionBase<triton::gpu::CmpIOp, LLVM::ICmpOp,\n                                     CmpIOpConversion> {\n@@ -3171,6 +3249,10 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n     addConversion([&](RankedTensorType type) -> llvm::Optional<Type> {\n       return convertTritonTensorType(type);\n     });\n+    // internally store bfloat16 as int16\n+    addConversion([&](BFloat16Type type) -> llvm::Optional<Type> {\n+      return IntegerType::get(type.getContext(), 16);\n+    });\n   }\n \n   Type convertTritonPointerType(triton::PointerType type) {\n@@ -3429,25 +3511,44 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                         benefit);\n   patterns.add<ArithConstantSplatOpConversion>(typeConverter, benefit);\n   patterns.add<AsyncWaitOpConversion>(typeConverter, benefit);\n-  patterns.add<BinaryOpConversion<arith::AddIOp, LLVM::AddOp>>(typeConverter,\n-                                                               benefit);\n-  patterns.add<BinaryOpConversion<arith::AddFOp, LLVM::FAddOp>>(typeConverter,\n-                                                                benefit);\n-  patterns.add<BinaryOpConversion<arith::MulIOp, LLVM::MulOp>>(typeConverter,\n-                                                               benefit);\n-  patterns.add<BinaryOpConversion<arith::MulFOp, LLVM::FMulOp>>(typeConverter,\n-                                                                benefit);\n-\n-  patterns.add<BinaryOpConversion<arith::AndIOp, LLVM::AndOp>>(typeConverter,\n-                                                               benefit);\n-  patterns.add<BinaryOpConversion<arith::OrIOp, LLVM::OrOp>>(typeConverter,\n-                                                             benefit);\n+#define POPULATE_BINARY_OP(SRC_OP, DST_OP)                                     \\\n+  patterns.add<BinaryOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n+\n+  POPULATE_BINARY_OP(arith::SubIOp, LLVM::SubOp) // -\n+  POPULATE_BINARY_OP(arith::SubFOp, LLVM::FSubOp)\n+  POPULATE_BINARY_OP(arith::AddIOp, LLVM::AddOp) // +\n+  POPULATE_BINARY_OP(arith::AddFOp, LLVM::FAddOp)\n+  POPULATE_BINARY_OP(arith::MulIOp, LLVM::MulOp) // *\n+  POPULATE_BINARY_OP(arith::MulFOp, LLVM::FMulOp)\n+  POPULATE_BINARY_OP(arith::DivFOp, LLVM::FDivOp) // /\n+  POPULATE_BINARY_OP(arith::DivSIOp, LLVM::SDivOp)\n+  POPULATE_BINARY_OP(arith::DivUIOp, LLVM::UDivOp)\n+  POPULATE_BINARY_OP(arith::RemFOp, LLVM::FRemOp) // %\n+  POPULATE_BINARY_OP(arith::RemSIOp, LLVM::SRemOp)\n+  POPULATE_BINARY_OP(arith::RemUIOp, LLVM::URemOp)\n+  POPULATE_BINARY_OP(arith::AndIOp, LLVM::AndOp) // &\n+  POPULATE_BINARY_OP(arith::OrIOp, LLVM::OrOp)   // |\n+#undef POPULATE_BINARY_OP\n \n   patterns.add<CmpIOpConversion>(typeConverter, benefit);\n   patterns.add<CmpFOpConversion>(typeConverter, benefit);\n+#define POPULATE_CAST_OP(SRC_OP, DST_OP)                                       \\\n+  patterns.add<UnaryOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n+  POPULATE_CAST_OP(arith::TruncIOp, LLVM::TruncOp)\n+  POPULATE_CAST_OP(arith::TruncFOp, LLVM::FPTruncOp)\n+  POPULATE_CAST_OP(arith::ExtSIOp, LLVM::SExtOp)\n+  POPULATE_CAST_OP(arith::ExtUIOp, LLVM::ZExtOp)\n+  POPULATE_CAST_OP(arith::FPToUIOp, LLVM::FPToUIOp)\n+  POPULATE_CAST_OP(arith::FPToSIOp, LLVM::FPToSIOp)\n+  POPULATE_CAST_OP(arith::UIToFPOp, LLVM::UIToFPOp)\n+  POPULATE_CAST_OP(arith::SIToFPOp, LLVM::SIToFPOp)\n+  POPULATE_CAST_OP(arith::ExtFOp, LLVM::FPExtOp)\n+#undef POPULATE_CAST_OP\n+\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n                                           benefit);\n+\n   patterns.add<ExtractSliceOpConversion>(typeConverter, allocation, smem,\n                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -115,7 +115,9 @@ void populateArithmeticPatternsAndLegality(\n       ArithCmpPattern<arith::CmpFOp, triton::gpu::CmpFOp>,\n       // Cast Ops\n       GenericOpPattern<arith::TruncIOp>, GenericOpPattern<arith::TruncFOp>,\n-      GenericOpPattern<arith::SIToFPOp>>(typeConverter, context);\n+      GenericOpPattern<arith::ExtUIOp>, GenericOpPattern<arith::ExtSIOp>,\n+      GenericOpPattern<arith::ExtFOp>, GenericOpPattern<arith::SIToFPOp>,\n+      GenericOpPattern<arith::UIToFPOp>>(typeConverter, context);\n }\n \n // this shouldn't exist if mlir's SelectOp checked encodings properly"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 25, "deletions": 2, "changes": 27, "file_content_changes": "@@ -642,8 +642,31 @@ void init_triton_ir(py::module &&m) {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::TruncFOp>(loc, dstType, src);\n            })\n-      // .def(\"create_int_cast\", &ir::builder::create_int_cast)\n-      // .def(\"create_downcast\", &ir::builder::create_downcast)\n+      .def(\"create_int_cast\",\n+           [](mlir::OpBuilder &self, mlir::Value &src, mlir::Type &dstType,\n+              bool isSigned) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             // get element type if necessary\n+             mlir::Type srcType = src.getType();\n+             mlir::Type srcEltType = srcType;\n+             mlir::Type dstEltType = dstType;\n+             if (dstType.isa<mlir::RankedTensorType>()) {\n+               dstEltType =\n+                   dstType.cast<mlir::RankedTensorType>().getElementType();\n+               srcEltType =\n+                   srcType.cast<mlir::RankedTensorType>().getElementType();\n+             }\n+             unsigned srcWidth = srcEltType.getIntOrFloatBitWidth();\n+             unsigned dstWidth = dstEltType.getIntOrFloatBitWidth();\n+             if (srcWidth == dstWidth)\n+               return self.create<mlir::arith::BitcastOp>(loc, dstType, src);\n+             else if (srcWidth > dstWidth)\n+               return self.create<mlir::arith::TruncIOp>(loc, dstType, src);\n+             else if (isSigned)\n+               return self.create<mlir::arith::ExtSIOp>(loc, dstType, src);\n+             else\n+               return self.create<mlir::arith::ExtUIOp>(loc, dstType, src);\n+           })\n       .def(\"create_to_index\",\n            [](mlir::OpBuilder &self, mlir::Value &input) -> mlir::Value {\n              auto loc = self.getUnknownLoc();"}, {"filename": "python/tests/test_core.py", "status": "added", "additions": 1552, "deletions": 0, "changes": 1552, "file_content_changes": "@@ -0,0 +1,1552 @@\n+# flake8: noqa: F821,F841\n+import itertools\n+import re\n+from typing import Optional, Union\n+\n+import numpy as np\n+import pytest\n+import torch\n+from numpy.random import RandomState\n+\n+import triton\n+import triton._C.libtriton.triton as _triton\n+import triton.language as tl\n+from triton.runtime.jit import JITFunction, TensorWrapper, reinterpret\n+\n+int_dtypes = ['int8', 'int16', 'int32', 'int64']\n+uint_dtypes = ['uint8', 'uint16', 'uint32', 'uint64']\n+float_dtypes = ['float16', 'float32', 'float64']\n+dtypes = int_dtypes + uint_dtypes + float_dtypes\n+# TODO: handle bfloat16\n+dtypes_with_bfloat16 = dtypes  # + ['bfloat16']\n+\n+\n+def _bitwidth(dtype: str) -> int:\n+    # ex.: \"int64\" -> 64\n+    return int(re.search(r'(\\d+)$', dtype).group(1))\n+\n+\n+def numpy_random(shape, dtype_str, rs: Optional[RandomState] = None, low=None, high=None):\n+    \"\"\"\n+    Override `rs` if you're calling this function twice and don't want the same\n+    result for both calls.\n+    \"\"\"\n+    if isinstance(shape, int):\n+        shape = (shape, )\n+    if rs is None:\n+        rs = RandomState(seed=17)\n+    if dtype_str in int_dtypes + uint_dtypes:\n+        iinfo = np.iinfo(getattr(np, dtype_str))\n+        low = iinfo.min if low is None else max(low, iinfo.min)\n+        high = iinfo.max if high is None else min(high, iinfo.max)\n+        dtype = getattr(np, dtype_str)\n+        x = rs.randint(low, high, shape, dtype=dtype)\n+        x[x == 0] = 1  # Hack. Never return zero so tests of division don't error out.\n+        return x\n+    elif dtype_str in float_dtypes:\n+        return rs.normal(0, 1, shape).astype(dtype_str)\n+    elif dtype_str == 'bfloat16':\n+        return (rs.normal(0, 1, shape).astype('float32').view('uint32')\n+                & np.uint32(0xffff0000)).view('float32')\n+    elif dtype_str in ['bool', 'int1', 'bool_']:\n+        return rs.normal(0, 1, shape) > 0.0\n+    else:\n+        raise RuntimeError(f'Unknown dtype {dtype_str}')\n+\n+\n+def to_triton(x: np.ndarray, device='cuda', dst_type=None) -> Union[TensorWrapper, torch.Tensor]:\n+    '''\n+    Note: We need dst_type because the type of x can be different from dst_type.\n+          For example: x is of type `float32`, dst_type is `bfloat16`.\n+          If dst_type is None, we infer dst_type from x.\n+    '''\n+    t = x.dtype.name\n+    if t in uint_dtypes:\n+        signed_type_name = t.lstrip('u')  # e.g. \"uint16\" -> \"int16\"\n+        x_signed = x.astype(getattr(np, signed_type_name))\n+        return reinterpret(torch.tensor(x_signed, device=device), getattr(tl, t))\n+    else:\n+        if t == 'float32' and dst_type == 'bfloat16':\n+            return torch.tensor(x, device=device).bfloat16()\n+        return torch.tensor(x, device=device)\n+\n+\n+def torch_dtype_name(dtype) -> str:\n+    if isinstance(dtype, triton.language.dtype):\n+        return dtype.name\n+    elif isinstance(dtype, torch.dtype):\n+        # 'torch.int64' -> 'int64'\n+        m = re.match(r'^torch\\.(\\w+)$', str(dtype))\n+        return m.group(1)\n+    else:\n+        raise TypeError(f'not a triton or torch dtype: {type(dtype)}')\n+\n+\n+def to_numpy(x):\n+    if isinstance(x, TensorWrapper):\n+        return x.base.cpu().numpy().astype(getattr(np, torch_dtype_name(x.dtype)))\n+    elif isinstance(x, torch.Tensor):\n+        if x.dtype is torch.bfloat16:\n+            return x.cpu().float().numpy()\n+        return x.cpu().numpy()\n+    else:\n+        raise ValueError(f\"Not a triton-compatible tensor: {x}\")\n+\n+\n+def patch_kernel(template, to_replace):\n+    kernel = triton.JITFunction(template.fn)\n+    for key, value in to_replace.items():\n+        kernel.src = kernel.src.replace(key, value)\n+    return kernel\n+\n+\n+def check_type_supported(dtype):\n+    '''\n+    skip test if dtype is not supported on the current device\n+    '''\n+    cc = torch.cuda.get_device_capability()\n+    if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n+        pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n+\n+\n+@pytest.mark.parametrize(\"dtype_x\", [dtype_x for dtype_x in dtypes] + [\"bfloat16\"])\n+def test_empty_kernel(dtype_x, device='cuda'):\n+    SIZE = 128\n+\n+    @triton.jit\n+    def kernel(X, SIZE: tl.constexpr):\n+        pass\n+    check_type_supported(dtype_x)\n+    x = to_triton(numpy_random(SIZE, dtype_str=dtype_x), device=device, dst_type=dtype_x)\n+    kernel[(1, )](x, SIZE=SIZE, num_warps=4)\n+\n+\n+# generic test functions\n+def _test_unary(dtype_x, expr, numpy_expr=None, device='cuda'):\n+    check_type_supported(dtype_x)  # early return if dtype_x is not supported\n+    SIZE = 128\n+    # define the kernel / launch-grid\n+\n+    @triton.jit\n+    def kernel(Z, X, SIZE: tl.constexpr):\n+        off = tl.arange(0, SIZE)\n+        x = tl.load(X + off)\n+        z = GENERATE_TEST_HERE\n+        tl.store(Z + off, z)\n+\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': expr})\n+    # inputs\n+    x = numpy_random(SIZE, dtype_str=dtype_x)\n+    if 'log' in expr:\n+        x = np.abs(x) + 0.01\n+    # reference result\n+    z_ref = eval(expr if numpy_expr is None else numpy_expr)\n+    # triton result\n+    x_tri = to_triton(x, device=device, dst_type=dtype_x)\n+    z_tri = to_triton(np.empty_like(z_ref), device=device, dst_type=dtype_x)\n+    kernel[(1, )](z_tri, x_tri, SIZE=SIZE, num_warps=4)\n+    # compare\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+\n+\n+def _binary_op_dtype_override(a: str, b: str) -> Optional[np.dtype]:\n+    \"\"\"\n+    Given two dtype strings, returns the numpy dtype Triton thinks binary\n+    operations on the two types should return. Returns None if the return value\n+    matches numpy. This is generally needed because Triton and pytorch return\n+    narrower floating point types than numpy in mixed operations, and because\n+    Triton follows C/C++ semantics around mixed signed/unsigned operations, and\n+    numpy/pytorch do not.\n+    \"\"\"\n+    overrides = {\n+        ('float16', 'int16'): np.float16,\n+        ('float16', 'int32'): np.float16,\n+        ('float16', 'int64'): np.float16,\n+        ('float16', 'uint16'): np.float16,\n+        ('float16', 'uint32'): np.float16,\n+        ('float16', 'uint64'): np.float16,\n+        ('int8', 'uint8'): np.uint8,\n+        ('int8', 'uint16'): np.uint16,\n+        ('int8', 'uint32'): np.uint32,\n+        ('int8', 'uint64'): np.uint64,\n+        ('int16', 'uint16'): np.uint16,\n+        ('int16', 'uint32'): np.uint32,\n+        ('int16', 'uint64'): np.uint64,\n+        ('int32', 'uint32'): np.uint32,\n+        ('int32', 'uint64'): np.uint64,\n+        ('int64', 'uint64'): np.uint64,\n+    }\n+    key = (a, b) if a < b else (b, a)\n+    return overrides.get(key)\n+\n+\n+def _test_binary(dtype_x, dtype_y, expr, numpy_expr=None, mode_x='real', mode_y='real', device='cuda', y_low=None, y_high=None):\n+    check_type_supported(dtype_x)  # early return if dtype_x is not supported\n+    check_type_supported(dtype_y)\n+    SIZE = 128\n+    # define the kernel / launch-grid\n+\n+    @triton.jit\n+    def kernel(Z, X, Y, SIZE: tl.constexpr):\n+        off = tl.arange(0, SIZE)\n+        x = tl.load(X + off)\n+        y = tl.load(Y + off)\n+        z = GENERATE_TEST_HERE\n+        tl.store(Z + off, z)\n+\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': expr})\n+    # inputs\n+    rs = RandomState(17)\n+    x = numpy_random(SIZE, dtype_str=dtype_x, rs=rs)\n+    y = numpy_random(SIZE, dtype_str=dtype_y, rs=rs, low=y_low, high=y_high)\n+    if mode_x == 'nan':\n+        x[:] = float('nan')\n+    if mode_y == 'nan':\n+        y[:] = float('nan')\n+    # reference result\n+    z_ref = eval(expr if numpy_expr is None else numpy_expr)\n+    dtype_z = _binary_op_dtype_override(dtype_x, dtype_y)\n+    if dtype_z is not None:\n+        z_ref = z_ref.astype(dtype_z)\n+    # triton result\n+    x_tri = to_triton(x, device=device, dst_type=dtype_x)\n+    y_tri = to_triton(y, device=device, dst_type=dtype_y)\n+    z_tri = to_triton(np.empty(SIZE, dtype=z_ref.dtype), device=device)\n+    kernel[(1, )](z_tri, x_tri, y_tri, SIZE=SIZE, num_warps=4)\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), err_msg=expr, rtol=0.01)\n+\n+\n+def _mod_operation_ill_conditioned(dtype_x, dtype_y) -> bool:\n+    # The result of x % y is ill-conditioned if x % y is much smaller than x.\n+    # pytorch/CUDA has slightly different (probably better) rounding on\n+    # remainders than stock LLVM. We currently don't expect to match it\n+    # bit-for-bit.\n+    return (dtype_x, dtype_y) in [\n+        ('int32', 'bfloat16'),\n+        ('int32', 'float16'),\n+        ('int32', 'float32'),\n+        ('int64', 'bfloat16'),\n+        ('int64', 'float16'),\n+        ('int64', 'float32'),\n+        ('int64', 'float64'),\n+        ('uint16', 'bfloat16'),\n+        ('uint16', 'float16'),\n+        ('uint16', 'float32'),\n+        ('uint32', 'bfloat16'),\n+        ('uint32', 'float16'),\n+        ('uint32', 'float32'),\n+        ('uint64', 'bfloat16'),\n+        ('uint64', 'float16'),\n+        ('uint64', 'float32'),\n+        ('uint64', 'float64'),\n+    ]\n+\n+# ---------------\n+# test binary ops\n+# ---------------\n+\n+\n+@pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n+    (dtype_x, dtype_y, op)\n+    for op in ['+', '-', '*', '/']  # , '%'] #TODO: handle remainder\n+    for dtype_x in dtypes_with_bfloat16\n+    for dtype_y in dtypes_with_bfloat16\n+])\n+def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n+    expr = f' x {op} y'\n+    if op == '%' and dtype_x in int_dtypes + uint_dtypes and dtype_y in int_dtypes + uint_dtypes:\n+        # LLVM has 'numpy.fmod', not 'numpy.remainder', semantics on integer remainders.\n+        numpy_expr = 'np.fmod(x, y)'\n+    elif op in ('/', '%') and dtype_x in ('int16', 'float16', 'bfloat16') and dtype_y in ('int16', 'float16', 'bfloat16'):\n+        # Triton promotes 16-bit floating-point / and % to 32-bit because there\n+        # are no native div or FRem operations on float16. Since we have to\n+        # convert anyway, we may as well take the accuracy bump.\n+        numpy_expr = f'x.astype(np.float32) {op} y.astype(np.float32)'\n+    elif (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n+        numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n+    elif (dtype_y in uint_dtypes and dtype_x in int_dtypes and _bitwidth(dtype_y) >= _bitwidth(dtype_x)):\n+        numpy_expr = f'x.astype(np.{dtype_y}) {op} y.astype(np.{dtype_y})'\n+    else:\n+        numpy_expr = None\n+    if op == '%' and _mod_operation_ill_conditioned(dtype_x, dtype_y):\n+        with pytest.raises(AssertionError, match='Not equal to tolerance'):\n+            _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n+    elif (op in ('%', '/') and\n+          ((dtype_x in int_dtypes and dtype_y in uint_dtypes) or\n+           (dtype_x in uint_dtypes and dtype_y in int_dtypes))):\n+        with pytest.raises(triton.CompilationError) as exc_info:\n+            _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n+        assert re.match('Cannot use .* because they have different signedness', str(exc_info.value.__cause__))\n+    else:\n+        _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n+\n+\n+# @pytest.mark.parametrize(\"dtype_x, dtype_y\",\n+#                          [(dtype_x, dtype_y) for dtype_x in int_dtypes for dtype_y in int_dtypes] +\n+#                          [(dtype_x, dtype_y) for dtype_x in uint_dtypes for dtype_y in uint_dtypes]\n+#                          )\n+# def test_floordiv(dtype_x, dtype_y, device='cuda'):\n+#     # Triton has IEEE, not numpy/torch, semantics for %, and those carry\n+#     # through to //, so we have to use a nonstandard expression to get a\n+#     # reference result for //.\n+#     expr = 'x // y'\n+#     numpy_expr = '((x - np.fmod(x, y)) / y)'\n+#     _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n+\n+\n+# # ---------------\n+# # test bitwise ops\n+# # ---------------\n+# @pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n+#     (dtype_x, dtype_y, op)\n+#     for op in ['&', '|', '^']\n+#     for dtype_x in dtypes + dtypes_with_bfloat16\n+#     for dtype_y in dtypes + dtypes_with_bfloat16\n+# ])\n+# def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n+#     expr = f'x {op} y'\n+#     if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n+#         numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n+#     elif (dtype_y in uint_dtypes and dtype_x in int_dtypes and _bitwidth(dtype_y) >= _bitwidth(dtype_x)):\n+#         numpy_expr = f'x.astype(np.{dtype_y}) {op} y.astype(np.{dtype_y})'\n+#     else:\n+#         numpy_expr = None\n+#     if 'float' in dtype_x + dtype_y:\n+#         with pytest.raises(triton.CompilationError) as exc_info:\n+#             _test_binary(dtype_x, dtype_y, expr, numpy_expr='np.array([])', device=device)\n+#         # The CompilationError must have been caused by a C++ exception with this text.\n+#         assert re.match('invalid operands of type', str(exc_info.value.__cause__))\n+#     else:\n+#         _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n+\n+\n+# @pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n+#     (dtype_x, dtype_y, op)\n+#     for op in ['<<', '>>']\n+#     for dtype_x in int_dtypes + uint_dtypes\n+#     for dtype_y in int_dtypes + uint_dtypes\n+# ])\n+# def test_shift_op(dtype_x, dtype_y, op, device='cuda'):\n+#     expr = f'x {op} y'\n+#     bw = max(_bitwidth(dtype_x), _bitwidth(dtype_y))\n+#     dtype_z = f'uint{bw}'\n+#     numpy_expr = f'x.astype(np.{dtype_z}) {op} y.astype(np.{dtype_z})'\n+#     _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device, y_low=0, y_high=65)\n+\n+\n+# # ---------------\n+# # test compare ops\n+# # ---------------\n+# ops = ['==', '!=', '>', '<', '>=', '<=']\n+\n+\n+# @pytest.mark.parametrize(\"dtype_x, dtype_y, op, mode_x, mode_y\",\n+#                          # real\n+#                          [\n+#                              (dtype_x, dtype_y, op, 'real', 'real')\n+#                              for op in ops\n+#                              for dtype_x in dtypes\n+#                              for dtype_y in dtypes\n+#                          ] +\n+#                          # NaNs\n+#                          [('float32', 'float32', op, mode_x, mode_y)\n+#                              for op in ops\n+#                              for mode_x, mode_y in [('nan', 'real'),\n+#                                                     ('real', 'nan'),\n+#                                                     ('nan', 'nan')]\n+\n+#                           ])\n+# def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n+#     expr = f'x {op} y'\n+#     if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n+#         numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n+#     elif (dtype_y in uint_dtypes and dtype_x in int_dtypes and _bitwidth(dtype_y) >= _bitwidth(dtype_x)):\n+#         numpy_expr = f'x.astype(np.{dtype_y}) {op} y.astype(np.{dtype_y})'\n+#     else:\n+#         numpy_expr = None\n+#     _test_binary(dtype_x, dtype_y, expr, numpy_expr, mode_x=mode_x, mode_y=mode_y, device=device)\n+\n+\n+# # ---------------\n+# # test where\n+# # ---------------\n+# @pytest.mark.parametrize(\"dtype\", dtypes_with_bfloat16 + [\"*int32\"])\n+# def test_where(dtype):\n+#     select_ptrs = False\n+#     if dtype == \"*int32\":\n+#         dtype = \"int64\"\n+#         select_ptrs = True\n+#     check_type_supported(dtype)\n+\n+#     @triton.jit\n+#     def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n+#                      BLOCK_SIZE: tl.constexpr,\n+#                      TEST_POINTERS: tl.constexpr):\n+#         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+#         mask = offsets < n_elements\n+#         decide = tl.load(cond_ptr + offsets, mask=mask)\n+#         if TEST_POINTERS:\n+#             a = tl.load(a_ptr + offsets, mask=mask).to(tl.pi32_t)\n+#             b = tl.load(b_ptr + offsets, mask=mask).to(tl.pi32_t)\n+#         else:\n+#             a = tl.load(a_ptr + offsets, mask=mask)\n+#             b = tl.load(b_ptr + offsets, mask=mask)\n+#         output = tl.where(decide, a, b)\n+#         tl.store(output_ptr + offsets, output, mask=mask)\n+\n+#     SIZE = 1_000\n+#     rs = RandomState(17)\n+#     cond = numpy_random(SIZE, 'bool', rs)\n+#     x = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n+#     y = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n+#     z = np.where(cond, x, y)\n+\n+#     cond_tri = to_triton(cond, device='cuda')\n+#     x_tri = to_triton(x, device='cuda', dst_type=dtype)\n+#     y_tri = to_triton(y, device='cuda', dst_type=dtype)\n+#     z_tri = to_triton(np.empty(SIZE, dtype=z.dtype), device='cuda', dst_type=dtype)\n+\n+#     grid = lambda meta: (triton.cdiv(SIZE, meta['BLOCK_SIZE']),)\n+#     where_kernel[grid](cond_tri, x_tri, y_tri, z_tri, SIZE, BLOCK_SIZE=1024, TEST_POINTERS=select_ptrs)\n+#     assert (z == to_numpy(z_tri)).all()\n+\n+\n+# def test_where_broadcast():\n+#     @triton.jit\n+#     def where_kernel(cond_ptr, a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n+#         xoffsets = tl.reshape(tl.arange(0, BLOCK_SIZE), [BLOCK_SIZE, 1])\n+#         yoffsets = tl.reshape(tl.arange(0, BLOCK_SIZE), [1, BLOCK_SIZE])\n+\n+#         mask = tl.load(cond_ptr + yoffsets)\n+#         vals = tl.load(a_ptr + yoffsets + BLOCK_SIZE * xoffsets)\n+#         res = tl.where(mask, vals, 0.)\n+#         tl.store(out_ptr + yoffsets + BLOCK_SIZE * xoffsets, res)\n+\n+#     @triton.jit\n+#     def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n+#         xoffsets = tl.reshape(tl.arange(0, BLOCK_SIZE), [BLOCK_SIZE, 1])\n+#         yoffsets = tl.reshape(tl.arange(0, BLOCK_SIZE), [1, BLOCK_SIZE])\n+#         mask = 0\n+#         vals = tl.load(a_ptr + yoffsets + BLOCK_SIZE * xoffsets)\n+#         res = tl.where(mask, vals, 0.)\n+#         tl.store(out_ptr + yoffsets + BLOCK_SIZE * xoffsets, res)\n+\n+#     SIZE = 32\n+#     dtype = 'float32'\n+#     rs = RandomState(17)\n+#     x = numpy_random((SIZE, SIZE), dtype_str=dtype, rs=rs)\n+#     mask = numpy_random(SIZE, 'bool', rs=rs)\n+#     z = np.where(mask, x, 0)\n+#     cond_tri = to_triton(mask, device=\"cuda\")\n+#     x_tri = to_triton(x, device='cuda', dst_type=dtype)\n+#     z_tri = to_triton(np.empty((SIZE, SIZE), dtype=z.dtype), device='cuda', dst_type=dtype)\n+#     where_kernel[(1,)](cond_tri, x_tri, z_tri, SIZE)\n+#     assert (z == to_numpy(z_tri)).all()\n+#     where_scalar_condition[(1,)](x_tri, z_tri, SIZE)\n+#     z = np.where(0, x, 0)\n+#     assert (z == to_numpy(z_tri)).all()\n+\n+# # ---------------\n+# # test unary ops\n+# # ---------------\n+\n+\n+# @pytest.mark.parametrize(\"dtype_x, expr\", [\n+#     (dtype_x, ' -x') for dtype_x in dtypes_with_bfloat16\n+# ] + [\n+#     (dtype_x, ' ~x') for dtype_x in int_dtypes\n+# ])\n+# def test_unary_op(dtype_x, expr, device='cuda'):\n+#     _test_unary(dtype_x, expr, device=device)\n+\n+# # ----------------\n+# # test math ops\n+# # ----------------\n+# # @pytest.mark.parametrize(\"expr\", [\n+# #     'exp', 'log', 'cos', 'sin'\n+# # ])\n+\n+\n+# @pytest.mark.parametrize(\"expr\", [\n+#     'exp', 'log', 'cos', 'sin'\n+# ])\n+# def test_math_op(expr, device='cuda'):\n+#     _test_unary('float32', f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n+\n+\n+# # ----------------\n+# # test indexing\n+# # ----------------\n+\n+\n+# def make_ptr_str(name, shape):\n+#     rank = len(shape)\n+#     offsets = []\n+#     stride = 1\n+#     for i in reversed(range(rank)):\n+#         idx = ', '.join([':' if ii == i else 'None' for ii in range(rank)])\n+#         offsets += [f'tl.arange(0, {shape[i]})[{idx}]*{stride}']\n+#         stride *= shape[i]\n+#     return f\"{name} + {' + '.join(offsets)}\"\n+\n+\n+# @pytest.mark.parametrize(\"expr, dtype_str\", [\n+#     (f'x[{s}]', d)\n+#     for s in ['None, :', ':, None', 'None, :, :', ':, :, None']\n+#     for d in ['int32', 'uint32', 'uint16']\n+# ])\n+# def test_index1d(expr, dtype_str, device='cuda'):\n+#     rank_x = expr.count(':')\n+#     rank_y = expr.count(',') + 1\n+#     shape_x = [32 for _ in range(rank_x)]\n+#     shape_z = [32 for _ in range(rank_y)]\n+#     shape_z_rank_mismatch = [32 for _ in range(rank_y + 1)]\n+#     shape_z_dim_mismatch = [64 for _ in range(rank_y)]\n+\n+#     # Triton kernel\n+#     @triton.jit\n+#     def kernel(Z, X, SIZE: tl.constexpr):\n+#         m = tl.arange(0, SIZE)\n+#         n = tl.arange(0, SIZE)\n+#         x = tl.load(X_PTR_EXPR)\n+#         z = GENERATE_TEST_HERE\n+#         tl.store(Z_PTR_EXPR, z)\n+\n+#     def generate_kernel(shape_x, shape_z):\n+#         to_replace = {\n+#             'X_PTR_EXPR': make_ptr_str('X', shape_x),\n+#             'Z_PTR_EXPR': make_ptr_str('Z', shape_z),\n+#             'GENERATE_TEST_HERE': expr,\n+#         }\n+#         return patch_kernel(kernel, to_replace)\n+\n+#     kernel_match = generate_kernel(shape_x, shape_z)\n+#     kernel_dim_mismatch = generate_kernel(shape_x, shape_z_dim_mismatch)\n+#     kernel_rank_mismatch = generate_kernel(shape_x, shape_z_rank_mismatch)\n+\n+#     # torch result\n+#     x = numpy_random(shape_x, dtype_str=dtype_str)\n+#     y = np.zeros(shape_z, dtype=getattr(np, dtype_str))\n+#     z_ref = eval(expr) + y\n+#     # triton result\n+#     z_tri = to_triton(np.empty_like(z_ref), device=device)\n+#     x_tri = to_triton(x)\n+#     kernel_match[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n+#     # compare\n+#     assert (z_ref == to_numpy(z_tri)).all()\n+\n+#     def catch_compilation_error(kernel):\n+#         try:\n+#             kernel[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n+#         except triton.CompilationError as e:\n+#             np.testing.assert_(True)\n+#         except BaseException:\n+#             np.testing.assert_(False)\n+\n+#     catch_compilation_error(kernel_dim_mismatch)\n+#     catch_compilation_error(kernel_rank_mismatch)\n+\n+\n+# # ---------------\n+# # test tuples\n+# # ---------------\n+\n+\n+# @triton.jit\n+# def fn(a, b):\n+#     return a + b, \\\n+#         a - b, \\\n+#         a * b\n+\n+\n+# def test_tuples():\n+#     device = 'cuda'\n+\n+#     @triton.jit\n+#     def with_fn(X, Y, A, B, C):\n+#         x = tl.load(X)\n+#         y = tl.load(Y)\n+#         a, b, c = fn(x, y)\n+#         tl.store(A, a)\n+#         tl.store(B, b)\n+#         tl.store(C, c)\n+\n+#     @triton.jit\n+#     def without_fn(X, Y, A, B, C):\n+#         x = tl.load(X)\n+#         y = tl.load(Y)\n+#         a, b, c = x + y, x - y, x * y\n+#         tl.store(A, a)\n+#         tl.store(B, b)\n+#         tl.store(C, c)\n+\n+#     x = torch.tensor([1.3], device=device, dtype=torch.float32)\n+#     y = torch.tensor([1.9], device=device, dtype=torch.float32)\n+#     a_tri = torch.tensor([0], device=device, dtype=torch.float32)\n+#     b_tri = torch.tensor([0], device=device, dtype=torch.float32)\n+#     c_tri = torch.tensor([0], device=device, dtype=torch.float32)\n+#     for kernel in [with_fn, without_fn]:\n+#         kernel[(1, )](x, y, a_tri, b_tri, c_tri, num_warps=1)\n+#         a_ref, b_ref, c_ref = x + y, x - y, x * y\n+#         assert a_tri == a_ref\n+#         assert b_tri == b_ref\n+#         assert c_tri == c_ref\n+\n+\n+# # ---------------\n+# # test atomics\n+# # ---------------\n+# @pytest.mark.parametrize(\"op, dtype_x_str, mode\", itertools.chain.from_iterable([\n+#     [\n+#         ('add', 'float16', mode),\n+#         ('add', 'uint32', mode), ('add', 'int32', mode), ('add', 'float32', mode),\n+#         ('max', 'uint32', mode), ('max', 'int32', mode), ('max', 'float32', mode),\n+#         ('min', 'uint32', mode), ('min', 'int32', mode), ('min', 'float32', mode),\n+#     ]\n+#     for mode in ['all_neg', 'all_pos', 'min_neg', 'max_pos']]))\n+# def test_atomic_rmw(op, dtype_x_str, mode, device='cuda'):\n+#     n_programs = 5\n+\n+#     # triton kernel\n+#     @triton.jit\n+#     def kernel(X, Z):\n+#         pid = tl.program_id(0)\n+#         x = tl.load(X + pid)\n+#         old = GENERATE_TEST_HERE\n+\n+#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.atomic_{op}(Z, x)'})\n+#     numpy_op = {'add': np.sum, 'max': np.max, 'min': np.min}[op]\n+#     max_neutral = float('-inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).min\n+#     min_neutral = float('inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).max\n+#     neutral = {'add': 0, 'max': max_neutral, 'min': min_neutral}[op]\n+\n+#     # triton result\n+#     rs = RandomState(17)\n+#     x = numpy_random((n_programs, ), dtype_str=dtype_x_str, rs=rs)\n+#     if mode == 'all_neg':\n+#         x = -np.abs(x)\n+#     if mode == 'all_pos':\n+#         x = np.abs(x)\n+#     if mode == 'min_neg':\n+#         idx = rs.randint(n_programs, size=(1, )).item()\n+#         x[idx] = -np.max(np.abs(x)) - 1\n+#     if mode == 'max_pos':\n+#         idx = rs.randint(n_programs, size=(1, )).item()\n+#         x[idx] = np.max(np.abs(x)) + 1\n+#     x_tri = to_triton(x, device=device)\n+\n+#     z_tri = to_triton(np.array([neutral], dtype=getattr(np, dtype_x_str)), device=device)\n+#     kernel[(n_programs, )](x_tri, z_tri)\n+#     # torch result\n+#     z_ref = numpy_op(x).astype(getattr(np, dtype_x_str))\n+#     # compare\n+#     exact = op not in ['add']\n+#     if exact:\n+#         assert z_ref.item() == to_numpy(z_tri).item()\n+#     else:\n+#         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+\n+\n+# @pytest.mark.parametrize(\"axis\", [0, 1])\n+# def test_tensor_atomic_rmw(axis, device=\"cuda\"):\n+#     shape0, shape1 = 8, 8\n+#     # triton kernel\n+\n+#     @triton.jit\n+#     def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n+#         off0 = tl.arange(0, SHAPE0)\n+#         off1 = tl.arange(0, SHAPE1)\n+#         x = tl.load(X + off0[:, None] * SHAPE1 + off1[None, :])\n+#         z = tl.sum(x, axis=AXIS)\n+#         tl.atomic_add(Z + off0, z)\n+#     rs = RandomState(17)\n+#     x = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n+#     # reference result\n+#     z_ref = np.sum(x, axis=axis)\n+#     # triton result\n+#     x_tri = to_triton(x, device=device)\n+#     z_tri = to_triton(np.zeros((shape0,), dtype=\"float32\"), device=device)\n+#     kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n+#     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n+\n+\n+# def test_atomic_cas():\n+#     # 1. make sure that atomic_cas changes the original value (Lock)\n+#     @triton.jit\n+#     def change_value(Lock):\n+#         tl.atomic_cas(Lock, 0, 1)\n+\n+#     Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+#     change_value[(1,)](Lock)\n+\n+#     assert (Lock[0] == 1)\n+\n+#     # 2. only one block enters the critical section\n+#     @triton.jit\n+#     def serialized_add(data, Lock):\n+#         ptrs = data + tl.arange(0, 128)\n+#         while tl.atomic_cas(Lock, 0, 1) == 1:\n+#             pass\n+\n+#         tl.store(ptrs, tl.load(ptrs) + 1.0)\n+\n+#         # release lock\n+#         tl.atomic_xchg(Lock, 0)\n+\n+#     Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+#     data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n+#     ref = torch.full((128,), 64.0)\n+#     serialized_add[(64,)](data, Lock)\n+#     triton.testing.assert_almost_equal(data, ref)\n+\n+\n+# # ---------------\n+# # test cast\n+# # ---------------\n+\n+\n+# @pytest.mark.parametrize(\"dtype_x, dtype_z, bitcast\", [\n+#     (dtype_x, dtype_z, False)\n+#     for dtype_x in dtypes\n+#     for dtype_z in dtypes\n+# ] + [\n+#     ('float32', 'bfloat16', False),\n+#     ('bfloat16', 'float32', False),\n+#     ('float32', 'int32', True),\n+#     ('float32', 'int1', False),\n+# ] + [\n+#     (f'uint{x}', f'int{x}', True) for x in [8, 16, 32, 64]\n+# ] + [\n+#     (f'int{x}', f'uint{x}', True) for x in [8, 16, 32, 64]\n+# ])\n+# def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n+#     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n+#     x0 = 43 if dtype_x in int_dtypes else 43.5\n+#     if dtype_x in float_dtypes and dtype_z == 'int1':\n+#         x0 = 0.5\n+#     if dtype_x.startswith('bfloat'):\n+#         x_tri = torch.tensor([x0], dtype=getattr(torch, dtype_x), device=device)\n+#     else:\n+#         x = np.array([x0], dtype=getattr(np, dtype_x))\n+#         x_tri = to_triton(x)\n+\n+#     # triton kernel\n+#     @triton.jit\n+#     def kernel(X, Z, BITCAST: tl.constexpr):\n+#         x = tl.load(X)\n+#         z = x.to(Z.dtype.element_ty, bitcast=BITCAST)\n+#         tl.store(Z, z)\n+\n+#     dtype_z_np = dtype_z if dtype_z != 'int1' else 'bool_'\n+#     # triton result\n+#     if dtype_z.startswith('bfloat'):\n+#         z_tri = torch.empty((1,), dtype=getattr(torch, dtype_z), device=device)\n+#     else:\n+#         z_tri = to_triton(np.empty((1, ), dtype=getattr(np, dtype_z_np)), device=device)\n+#     kernel[(1, )](x_tri, z_tri, BITCAST=bitcast)\n+#     # torch result\n+#     if dtype_z.startswith('bfloat') or dtype_x.startswith('bfloat'):\n+#         assert bitcast is False\n+#         z_ref = x_tri.to(z_tri.dtype)\n+#         assert z_tri == z_ref\n+#     else:\n+#         if bitcast:\n+#             z_ref = x.view(getattr(np, dtype_z_np))\n+#         else:\n+#             z_ref = x.astype(getattr(np, dtype_z_np))\n+#         assert to_numpy(z_tri) == z_ref\n+\n+\n+# def test_store_bool():\n+#     \"\"\"Tests that boolean True is stored as 1\"\"\"\n+#     @triton.jit\n+#     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n+#         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+#         mask = offsets < n_elements\n+#         input = tl.load(input_ptr + offsets, mask=mask)\n+#         output = input\n+#         tl.store(output_ptr + offsets, output, mask=mask)\n+\n+#     src = torch.tensor([True, False], dtype=torch.bool, device='cuda')\n+#     n_elements = src.numel()\n+#     dst = torch.empty_like(src)\n+#     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+#     copy_kernel[grid](src, dst, n_elements, BLOCK_SIZE=1024)\n+\n+#     assert (to_numpy(src).view('uint8') == to_numpy(dst).view('uint8')).all()\n+\n+\n+# @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n+# def test_f8_xf16_roundtrip(dtype):\n+#     \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n+#     check_type_supported(dtype)\n+\n+#     @triton.jit\n+#     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n+#         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+#         mask = offsets < n_elements\n+#         input = tl.load(input_ptr + offsets, mask=mask)\n+#         output = input\n+#         tl.store(output_ptr + offsets, output, mask=mask)\n+\n+#     f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n+#     f8 = triton.reinterpret(f8_tensor, tl.float8)\n+#     n_elements = f8_tensor.numel()\n+#     xf16 = torch.empty_like(f8_tensor, dtype=dtype)\n+#     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+#     copy_kernel[grid](f8, xf16, n_elements, BLOCK_SIZE=1024)\n+\n+#     f8_output_tensor = torch.empty_like(xf16, dtype=torch.int8)\n+#     f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n+#     copy_kernel[grid](xf16, f8_output, n_elements, BLOCK_SIZE=1024)\n+\n+#     assert torch.all(f8_tensor == f8_output_tensor)\n+\n+\n+# def test_f16_to_f8_rounding():\n+#     \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n+#     error is the minimum over all float8.\n+#     Or the same explanation a bit mathier:\n+#     for all f16 |f16 - fromf8(tof8(f16))| == min over all f8 |f16 - fromf8(f8)|\"\"\"\n+#     @triton.jit\n+#     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n+#         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+#         mask = offsets < n_elements\n+#         input = tl.load(input_ptr + offsets, mask=mask)\n+#         output = input\n+#         tl.store(output_ptr + offsets, output, mask=mask)\n+\n+#     # torch.view with a dtype isn't supported in triton's torch yet so use numpy's view\n+#     f16_input_np = (\n+#         np.array(\n+#             range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=np.int16,\n+#         )\n+#         .view(np.float16)\n+#     )\n+#     f16_input = torch.tensor(f16_input_np, dtype=torch.float16, device='cuda')\n+#     n_elements = f16_input.numel()\n+#     f8_output_tensor = torch.empty_like(f16_input, dtype=torch.int8)\n+#     f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n+#     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+#     copy_kernel[grid](f16_input, f8_output, n_elements, BLOCK_SIZE=1024)\n+\n+#     f16_output = torch.empty_like(f16_input, dtype=torch.float16)\n+#     copy_kernel[grid](f8_output, f16_output, n_elements, BLOCK_SIZE=1024)\n+\n+#     abs_error = torch.abs(f16_input - f16_output)\n+\n+#     all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device='cuda')\n+#     all_f8_vals = triton.reinterpret(all_f8_vals_tensor, tl.float8)\n+#     all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=torch.float16)\n+#     copy_kernel[grid](all_f8_vals, all_f8_vals_in_f16, n_elements=256, BLOCK_SIZE=1024)\n+\n+#     all_finite_f8_vals_in_f16 = all_f8_vals_in_f16[\n+#         torch.isfinite(all_f8_vals_in_f16)\n+#     ]\n+\n+#     min_error = torch.min(\n+#         torch.abs(\n+#             f16_input.reshape((-1, 1))\n+#             - all_finite_f8_vals_in_f16.reshape((1, -1))\n+#         ),\n+#         dim=1,\n+#     )[0]\n+#     # 1.9375 is float8 max\n+#     mismatch = torch.logical_and(\n+#         abs_error != min_error, torch.logical_and(torch.isfinite(f16_input), torch.abs(f16_input) < 1.9375)\n+#     )\n+#     assert torch.all(\n+#         torch.logical_not(mismatch)\n+#     ), f\"f16_input[mismatch]={f16_input[mismatch]} f16_output[mismatch]={f16_output[mismatch]} abs_error[mismatch]={abs_error[mismatch]} min_error[mismatch]={min_error[mismatch]}\"\n+\n+\n+# # ---------------\n+# # test reduce\n+# # ---------------\n+\n+\n+# @pytest.mark.parametrize(\"op, dtype_str, shape\",\n+#                          [(op, dtype, shape)\n+#                           for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n+#                           for dtype in dtypes_with_bfloat16\n+#                           for shape in [32, 64, 128, 512]])\n+# def test_reduce1d(op, dtype_str, shape, device='cuda'):\n+#     check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+\n+#     # triton kernel\n+#     @triton.jit\n+#     def kernel(X, Z, BLOCK: tl.constexpr):\n+#         x = tl.load(X + tl.arange(0, BLOCK))\n+#         tl.store(Z, GENERATE_TEST_HERE)\n+\n+#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=0)'})\n+#     # input\n+#     rs = RandomState(17)\n+#     # limit the range of integers so that the sum does not overflow\n+#     x = numpy_random((shape,), dtype_str=dtype_str, rs=rs)\n+#     x_tri = to_triton(x, device=device)\n+#     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n+#                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n+#     # numpy result\n+#     z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n+#     z_tri_dtype_str = z_dtype_str\n+#     if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n+#         z_dtype_str = 'float32'\n+#         z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n+#         # trunc mantissa for a fair comparison of accuracy\n+#         z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n+#         z_tri_dtype_str = 'bfloat16'\n+#     else:\n+#         z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n+#     # triton result\n+#     z_tri = to_triton(numpy_random((1,), dtype_str=z_dtype_str, rs=rs),\n+#                       device=device, dst_type=z_tri_dtype_str)\n+#     kernel[(1,)](x_tri, z_tri, BLOCK=shape)\n+#     z_tri = to_numpy(z_tri)\n+#     # compare\n+#     if op == 'sum':\n+#         np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n+#     else:\n+#         if op == 'argmin' or op == 'argmax':\n+#             # argmin and argmax can have multiple valid indices.\n+#             # so instead we compare the values pointed by indices\n+#             np.testing.assert_equal(x[z_ref], x[z_tri])\n+#         else:\n+#             np.testing.assert_equal(z_ref, z_tri)\n+\n+\n+# reduce_configs1 = [\n+#     (op, dtype, (1, 1024), axis) for dtype in dtypes_with_bfloat16\n+#     for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n+#     for axis in [1]\n+# ]\n+# reduce_configs2 = [\n+#     (op, 'float32', shape, axis)\n+#     for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n+#     for shape in [(2, 32), (4, 32), (4, 128), (32, 64), (64, 128), (128, 256), (32, 1024)]\n+#     for axis in [0, 1]\n+# ]\n+\n+\n+# @pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n+# def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n+#     # triton kernel\n+#     @triton.jit\n+#     def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n+#         range_m = tl.arange(0, BLOCK_M)\n+#         range_n = tl.arange(0, BLOCK_N)\n+#         x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n+#         z = GENERATE_TEST_HERE\n+#         if AXIS == 1:\n+#             tl.store(Z + range_m, z)\n+#         else:\n+#             tl.store(Z + range_n, z)\n+\n+#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=AXIS)'})\n+#     # input\n+#     rs = RandomState(17)\n+#     # limit the range of integers so that the sum does not overflow\n+#     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n+#     x_tri = to_triton(x)\n+#     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n+#                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n+#     z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n+#     z_tri_dtype_str = z_dtype_str\n+#     # numpy result\n+#     if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n+#         z_dtype_str = 'float32'\n+#         z_tri_dtype_str = 'bfloat16'\n+#         z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+#         # trunc mantissa for a fair comparison of accuracy\n+#         z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n+#     else:\n+#         z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+#     # triton result\n+#     z_tri = to_triton(numpy_random((shape[1 - axis],), dtype_str=z_dtype_str, rs=rs),\n+#                       device=device, dst_type=z_tri_dtype_str)\n+#     kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n+#     z_tri = to_numpy(z_tri)\n+#     # compare\n+#     if op == 'sum':\n+#         np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n+#     else:\n+#         if op == 'argmin' or op == 'argmax':\n+#             # argmin and argmax can have multiple valid indices.\n+#             # so instead we compare the values pointed by indices\n+#             z_ref_index = np.expand_dims(z_ref, axis=axis)\n+#             z_tri_index = np.expand_dims(z_tri, axis=axis)\n+#             z_ref_value = np.take_along_axis(x, z_ref_index, axis=axis)\n+#             z_tri_value = np.take_along_axis(x, z_tri_index, axis=axis)\n+#             np.testing.assert_equal(z_ref_value, z_tri_value)\n+#         else:\n+#             np.testing.assert_equal(z_ref, z_tri)\n+\n+# # ---------------\n+# # test permute\n+# # ---------------\n+\n+\n+# @pytest.mark.parametrize(\"dtype_str, shape, perm\",\n+#                          [(dtype, shape, perm)\n+#                           for dtype in ['bfloat16', 'float16', 'float32']\n+#                              for shape in [(64, 64), (128, 128)]\n+#                              for perm in [(1, 0)]])\n+# def test_permute(dtype_str, shape, perm, device='cuda'):\n+#     check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+\n+#     # triton kernel\n+#     @triton.jit\n+#     def kernel(X, stride_xm, stride_xn,\n+#                Z, stride_zm, stride_zn,\n+#                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n+#         off_m = tl.arange(0, BLOCK_M)\n+#         off_n = tl.arange(0, BLOCK_N)\n+#         Xs = X + off_m[:, None] * stride_xm + off_n[None, :] * stride_xn\n+#         Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n+#         tl.store(Zs, tl.load(Xs))\n+#     # input\n+#     x = numpy_random(shape, dtype_str=dtype_str)\n+#     # triton result\n+#     z_tri = to_triton(np.empty_like(x), device=device, dst_type=dtype_str)\n+#     z_tri_contiguous = to_triton(np.empty_like(x), device=device, dst_type=dtype_str)\n+#     x_tri = to_triton(x, device=device, dst_type=dtype_str)\n+#     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n+#                          z_tri, z_tri.stride(1), z_tri.stride(0),\n+#                          BLOCK_M=shape[0], BLOCK_N=shape[1])\n+#     pgm_contiguous = kernel[(1, 1)](x_tri, x_tri.stride(1), x_tri.stride(0),\n+#                                     z_tri_contiguous, z_tri_contiguous.stride(0), z_tri_contiguous.stride(1),\n+#                                     BLOCK_M=shape[0], BLOCK_N=shape[1])\n+#     # numpy result\n+#     z_ref = x.transpose(*perm)\n+#     # compare\n+#     triton.testing.assert_almost_equal(z_tri, z_ref)\n+#     triton.testing.assert_almost_equal(z_tri_contiguous, z_ref)\n+#     # parse ptx to make sure ld/st are vectorized\n+#     ptx = pgm.asm['ptx']\n+#     assert 'ld.global.v4' in ptx\n+#     assert 'st.global.v4' in ptx\n+#     ptx = pgm_contiguous.asm['ptx']\n+#     assert 'ld.global.v4' in ptx\n+#     assert 'st.global.v4' in ptx\n+\n+# # ---------------\n+# # test dot\n+# # ---------------\n+\n+\n+# @pytest.mark.parametrize(\"epilogue, allow_tf32, dtype\",\n+#                          [(epilogue, allow_tf32, dtype)\n+#                           for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n+#                           for allow_tf32 in [True, False]\n+#                           for dtype in ['float16']\n+#                           if not (allow_tf32 and (dtype in ['float16']))])\n+# def test_dot(epilogue, allow_tf32, dtype, device='cuda'):\n+#     cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n+#     if cc < 80:\n+#         if dtype == 'int8':\n+#             pytest.skip(\"Only test int8 on devices with sm >= 80\")\n+#         elif dtype == 'float32' and allow_tf32:\n+#             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n+\n+#     M, N, K = 128, 128, 64\n+#     num_warps = 8\n+#     trans_a, trans_b = False, False\n+\n+#     # triton kernel\n+#     @triton.jit\n+#     def kernel(X, stride_xm, stride_xk,\n+#                Y, stride_yk, stride_yn,\n+#                W, stride_wn, stride_wl,\n+#                Z, stride_zm, stride_zn,\n+#                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n+#                ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n+#                ALLOW_TF32: tl.constexpr,\n+#                DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n+#                TRANS_A: tl.constexpr, TRANS_B: tl.constexpr):\n+#         off_m = tl.arange(0, BLOCK_M)\n+#         off_n = tl.arange(0, BLOCK_N)\n+#         off_l = tl.arange(0, BLOCK_N)\n+#         off_k = tl.arange(0, BLOCK_K)\n+#         Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk\n+#         Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn\n+#         Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl\n+#         Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n+#         z = tl.dot(tl.load(Xs), tl.load(Ys), trans_a=TRANS_A, trans_b=TRANS_B, allow_tf32=ALLOW_TF32)\n+#         if ADD_MATRIX:\n+#             z += tl.load(Zs)\n+#         if ADD_ROWS:\n+#             ZRs = Z + off_m * stride_zm\n+#             z += tl.load(ZRs)[:, None]\n+#         if ADD_COLS:\n+#             ZCs = Z + off_n * stride_zn\n+#             z += tl.load(ZCs)[None, :]\n+#         if DO_SOFTMAX:\n+#             max = tl.max(z, 1)\n+#             z = z - max[:, None]\n+#             num = tl.exp(z)\n+#             den = tl.sum(num, 1)\n+#             z = num / den[:, None]\n+#         if CHAIN_DOT:\n+#             # tl.store(Zs, z)\n+#             # tl.debug_barrier()\n+#             z = tl.dot(z.to(tl.float16), tl.load(Ws), trans_a=TRANS_A)\n+#         tl.store(Zs, z)\n+#     # input\n+#     rs = RandomState(17)\n+#     x = numpy_random((K, M) if trans_a else (M, K), dtype_str=dtype, rs=rs) * .1\n+#     y = numpy_random((N, K) if trans_b else (K, N), dtype_str=dtype, rs=rs) * .1\n+#     w = numpy_random((N, N), dtype_str=dtype, rs=rs) * .1\n+#     if allow_tf32:\n+#         x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+#         y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+#         w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+#     x_tri = to_triton(x, device=device)\n+#     y_tri = to_triton(y, device=device)\n+#     w_tri = to_triton(w, device=device)\n+#     # triton result\n+#     z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs) * .1\n+#     z_tri = to_triton(z, device=device)\n+#     if epilogue == 'trans':\n+#         z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])\n+#     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n+#                          y_tri, y_tri.stride(0), y_tri.stride(1),\n+#                          w_tri, w_tri.stride(0), w_tri.stride(1),\n+#                          z_tri, z_tri.stride(0), z_tri.stride(1),\n+#                          TRANS_A=trans_a, TRANS_B=trans_b,\n+#                          BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n+#                          ADD_MATRIX=epilogue == 'add-matrix',\n+#                          ADD_ROWS=epilogue == 'add-rows',\n+#                          ADD_COLS=epilogue == 'add-cols',\n+#                          DO_SOFTMAX=epilogue == 'softmax',\n+#                          CHAIN_DOT=epilogue == 'chain-dot',\n+#                          ALLOW_TF32=allow_tf32,\n+#                          num_warps=num_warps)\n+#     # torch result\n+#     x_ref = x.T if trans_a else x\n+#     y_ref = y.T if trans_b else y\n+#     z_ref = np.matmul(x_ref, y_ref)\n+#     if epilogue == 'add-matrix':\n+#         z_ref += z\n+#     if epilogue == 'add-rows':\n+#         z_ref += z[:, 0][:, None]\n+#     if epilogue == 'add-cols':\n+#         z_ref += z[0, :][None, :]\n+#     if epilogue == 'softmax':\n+#         num = np.exp(z_ref - np.max(z_ref, axis=-1, keepdims=True))\n+#         denom = np.sum(num, axis=-1, keepdims=True)\n+#         z_ref = num / denom\n+#     if epilogue == 'chain-dot':\n+#         z_ref = np.matmul(z_ref.T if trans_a else z_ref, w)\n+#     # compare\n+#     # print(z_ref[:,0], z_tri[:,0])\n+#     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+#     # make sure ld/st are vectorized\n+#     ptx = pgm.asm['ptx']\n+#     assert 'ld.global.v4' in ptx\n+#     assert 'st.global.v4' in ptx\n+#     if allow_tf32:\n+#         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n+#     elif dtype == 'float32':\n+#         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' not in ptx\n+#     elif dtype == 'int8':\n+#         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n+\n+\n+# def test_dot_without_load():\n+#     @triton.jit\n+#     def kernel(out):\n+#         pid = tl.program_id(axis=0)\n+#         a = tl.zeros((32, 32), tl.float32)\n+#         b = tl.zeros((32, 32), tl.float32)\n+#         c = tl.zeros((32, 32), tl.float32)\n+#         c = tl.dot(a, b)\n+#         pout = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n+#         tl.store(pout, c)\n+\n+#     out = torch.ones((32, 32), dtype=torch.float32, device=\"cuda\")\n+#     kernel[(1,)](out)\n+\n+# # ---------------\n+# # test arange\n+# # ---------------\n+\n+\n+# @pytest.mark.parametrize(\"start\", [0, 1, 7, 16])\n+# def test_arange(start, device='cuda'):\n+#     BLOCK = 128\n+#     z_tri = torch.empty(BLOCK, dtype=torch.int32, device=device)\n+\n+#     @triton.jit\n+#     def _kernel(z, BLOCK: tl.constexpr,\n+#                 START: tl.constexpr, END: tl.constexpr):\n+#         off = tl.arange(0, BLOCK)\n+#         val = tl.arange(START, END)\n+#         tl.store(z + off, val)\n+#     _kernel[(1,)](z_tri, START=start, END=start + BLOCK, BLOCK=BLOCK)\n+#     z_ref = torch.arange(start, BLOCK + start, dtype=torch.int32, device=device)\n+#     triton.testing.assert_almost_equal(z_tri, z_ref)\n+\n+# # ---------------\n+# # test load\n+# # ---------------\n+# # 'bfloat16': torch.bfloat16,\n+# # Testing masked loads with an intermate copy to shared memory run.\n+\n+\n+# @pytest.mark.parametrize(\"dtype\", [torch.bfloat16, torch.float16, torch.float32])\n+# def test_masked_load_shared_memory(dtype, device='cuda'):\n+#     check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+\n+#     M = 32\n+#     N = 32\n+#     K = 16\n+\n+#     in1 = torch.rand((M, K), dtype=dtype, device=device)\n+#     in2 = torch.rand((K, N), dtype=dtype, device=device)\n+#     out = torch.zeros((M, N), dtype=dtype, device=device)\n+\n+#     @triton.jit\n+#     def _kernel(in1_ptr, in2_ptr, output_ptr,\n+#                 in_stride, in2_stride, out_stride,\n+#                 in_numel, in2_numel, out_numel,\n+#                 M: tl.constexpr, N: tl.constexpr, K: tl.constexpr):\n+\n+#         M_offsets = tl.arange(0, M)\n+#         N_offsets = tl.arange(0, N)\n+#         K_offsets = tl.arange(0, K)\n+\n+#         in_offsets = M_offsets[:, None] * in_stride + K_offsets[None, :]\n+#         in2_offsets = K_offsets[:, None] * in2_stride + N_offsets[None, :]\n+\n+#         # Load inputs.\n+#         x = tl.load(in1_ptr + in_offsets, mask=in_offsets < in_numel)\n+#         w = tl.load(in2_ptr + in2_offsets, mask=in2_offsets < in2_numel)\n+\n+#         # Without a dot product the memory doesn't get promoted to shared.\n+#         o = tl.dot(x, w)\n+\n+#         # Store output\n+#         output_offsets = M_offsets[:, None] * out_stride + N_offsets[None, :]\n+#         tl.store(output_ptr + output_offsets, o, mask=output_offsets < in2_numel)\n+\n+#     pgm = _kernel[(1,)](in1, in2, out,\n+#                         in1.stride()[0],\n+#                         in2.stride()[0],\n+#                         out.stride()[0],\n+#                         in1.numel(),\n+#                         in2.numel(),\n+#                         out.numel(),\n+#                         M=M, N=N, K=K)\n+\n+#     reference_out = torch.matmul(in1, in2)\n+#     triton.testing.allclose(out, reference_out)\n+\n+\n+# @pytest.mark.parametrize(\"cache\", [\"\", \".ca\", \".cg\"])\n+# def test_load_cache_modifier(cache):\n+#     src = torch.empty(128, device='cuda')\n+#     dst = torch.empty(128, device='cuda')\n+\n+#     @triton.jit\n+#     def _kernel(dst, src, CACHE: tl.constexpr):\n+#         offsets = tl.arange(0, 128)\n+#         x = tl.load(src + offsets, cache_modifier=CACHE)\n+#         tl.store(dst + offsets, x)\n+\n+#     pgm = _kernel[(1,)](dst, src, CACHE=cache)\n+#     ptx = pgm.asm['ptx']\n+#     if cache == '':\n+#         assert 'ld.global.ca' not in ptx\n+#         assert 'ld.global.cg' not in ptx\n+#     if cache == '.cg':\n+#         assert 'ld.global.cg' in ptx\n+#         assert 'ld.global.ca' not in ptx\n+#     if cache == '.ca':\n+#         assert 'ld.global.ca' in ptx\n+#         assert 'ld.global.cg' not in ptx\n+\n+\n+# @pytest.mark.parametrize(\"N\", [16, 10, 11, 1024])\n+# def test_vectorization(N):\n+#     src = torch.empty(1024, device='cuda')\n+#     dst = torch.empty(1024, device='cuda')\n+\n+#     @triton.jit\n+#     def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n+#         offsets = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+#         x = tl.load(src + offsets, mask=offsets < N)\n+#         tl.store(dst + offsets, x, mask=offsets < N)\n+#     pgm = _kernel[(1,)](dst, src, N=N, BLOCK_SIZE=src.shape[0])\n+#     ptx = pgm.asm[\"ptx\"]\n+#     if N % 16 == 0:\n+#         assert \"ld.global.v4.b32\" in ptx\n+#     else:\n+#         assert \"ld.global.b32\" in ptx\n+#     # triton.testing.assert_almost_equal(dst, src[:N])\n+# # ---------------\n+# # test store\n+# # ---------------\n+\n+# # ---------------\n+# # test if\n+# # ---------------\n+\n+# # ---------------\n+# # test for\n+# # ---------------\n+\n+# # ---------------\n+# # test while\n+# # ---------------\n+\n+# # ---------------\n+# # test default\n+# # ---------------\n+# # TODO: can't be local to test_default\n+\n+\n+# @triton.jit\n+# def _impl(value=10):\n+#     return value\n+\n+\n+# def test_default():\n+#     value = 5\n+#     ret0 = torch.zeros(1, dtype=torch.int32, device='cuda')\n+#     ret1 = torch.zeros(1, dtype=torch.int32, device='cuda')\n+\n+#     @triton.jit\n+#     def _kernel(ret0, ret1, value):\n+#         tl.store(ret0, _impl())\n+#         tl.store(ret1, _impl(value))\n+\n+#     _kernel[(1,)](ret0, ret1, value)\n+#     assert ret0.item() == 10\n+#     assert ret1.item() == value\n+\n+# # ---------------\n+# # test noop\n+# # ----------------\n+\n+\n+# def test_noop(device='cuda'):\n+#     @triton.jit\n+#     def kernel(x):\n+#         pass\n+#     x = to_triton(numpy_random((1,), dtype_str='int32'), device=device)\n+#     kernel[(1, )](x)\n+\n+\n+# @pytest.mark.parametrize(\"value, value_type\", [\n+#     (-1, 'i32'), (0, 'i32'), (-2**31, 'i32'), (2**31 - 1, 'i32'),\n+#     (2**31, 'u32'), (2**32 - 1, 'u32'), (2**32, 'i64'), (2**63 - 1, 'i64'),\n+#     (-2**63, 'i64'), (2**63, 'u64'), (2**64 - 1, 'u64')\n+# ])\n+# def test_value_specialization(value: int, value_type: str, device='cuda') -> None:\n+#     spec_type = None\n+\n+#     def cache_hook(*args, **kwargs):\n+#         nonlocal spec_type\n+#         spec_type = kwargs[\"compile\"][\"signature\"][0]\n+#     JITFunction.cache_hook = cache_hook\n+\n+#     @triton.jit\n+#     def kernel(VALUE, X):\n+#         pass\n+\n+#     x = torch.tensor([3.14159], device='cuda')\n+#     pgm = kernel[(1, )](value, x)\n+\n+#     JITFunction.cache_hook = None\n+#     assert spec_type == value_type\n+\n+\n+# @pytest.mark.parametrize(\n+#     \"value, overflow\",\n+#     [(2**64 - 1, False), (2**64, True), (-2**63, False), (-2**63 - 1, True)]\n+# )\n+# def test_value_specialization_overflow(value: int, overflow: bool, device='cuda') -> None:\n+\n+#     @triton.jit\n+#     def kernel(VALUE, X):\n+#         pass\n+\n+#     x = torch.tensor([3.14159], device='cuda')\n+\n+#     if overflow:\n+#         with pytest.raises(OverflowError):\n+#             kernel[(1, )](value, x)\n+#     else:\n+#         kernel[(1, )](value, x)\n+\n+\n+# # ----------------\n+# # test constexpr\n+# # ----------------\n+\n+# @pytest.mark.parametrize(\"op\", ['+', '-', '*', '/', '%', '<', '>'])\n+# @pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n+# @pytest.mark.parametrize(\"is_rhs_constexpr\", [True, False])\n+# def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr):\n+\n+#     @triton.jit\n+#     def kernel(Z, X, Y):\n+#         x = tl.load(X)\n+#         y = tl.load(Y)\n+#         z = GENERATE_TEST_HERE\n+#         tl.store(Z, z)\n+\n+#     x_str = \"3.14\" if is_lhs_constexpr else \"x\"\n+#     y_str = \"4.13\" if is_rhs_constexpr else \"y\"\n+#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{x_str} {op} {y_str}\"})\n+#     x = numpy_random((1,), dtype_str=\"float32\")\n+#     y = numpy_random((1,), dtype_str=\"float32\")\n+#     z = np.array(eval(f\"{x_str} {op} {y_str}\"))\n+#     x_tri = to_triton(x)\n+#     y_tri = to_triton(y)\n+#     z_tri = to_triton(np.empty((1,), dtype=z.dtype))\n+#     kernel[(1,)](z_tri, x_tri, y_tri)\n+#     np.testing.assert_allclose(z, to_numpy(z_tri))\n+\n+\n+# def test_constexpr_shape():\n+\n+#     @triton.jit\n+#     def kernel(X):\n+#         off = tl.arange(0, 128 + 128)\n+#         tl.store(X + off, off)\n+\n+#     x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+#     kernel[(1,)](x_tri)\n+#     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256))\n+\n+\n+# def test_constexpr_scalar_shape():\n+\n+#     @triton.jit\n+#     def kernel(X, s):\n+#         off = tl.arange(0, 256)\n+#         val = off % (256 // s)\n+#         tl.store(X + off, val)\n+\n+#     x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+#     kernel[(1,)](x_tri, 32)\n+#     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256) % 8)\n+\n+# # -------------\n+# # test call\n+# # -------------\n+\n+\n+# @triton.jit\n+# def val_multiplier(val, i):\n+#     return val * i\n+\n+\n+# @triton.jit\n+# def vecmul_kernel(ptr, n_elements, rep):\n+#     pid = tl.program_id(axis=0)\n+#     offsets = pid * 128 + tl.arange(0, 128)\n+#     mask = offsets < n_elements\n+#     vec = tl.load(ptr + offsets, mask=mask)\n+#     for i in range(1, rep):\n+#         vec = val_multiplier(vec, i)\n+#     tl.store(ptr + offsets, vec, mask=mask)\n+\n+\n+# def test_call():\n+\n+#     @triton.jit\n+#     def kernel(ptr, n_elements, num1, num2):\n+#         vecmul_kernel(ptr, n_elements, num1)\n+#         vecmul_kernel(ptr, n_elements, num2)\n+\n+#     size = 1024\n+#     rand_val = numpy_random((size,), dtype_str=\"float32\")\n+#     rand_val_tri = to_triton(rand_val, device='cuda')\n+#     kernel[(size // 128,)](rand_val_tri, size, 3, 5)\n+\n+#     ans = rand_val * 1 * 2 * 1 * 2 * 3 * 4\n+#     np.testing.assert_equal(to_numpy(rand_val_tri), ans)\n+\n+# # -------------\n+# # test if\n+# # -------------\n+\n+\n+# def test_if():\n+\n+#     @triton.jit\n+#     def kernel(Cond, XTrue, XFalse, Ret):\n+#         pid = tl.program_id(0)\n+#         cond = tl.load(Cond)\n+#         if pid % 2:\n+#             tl.store(Ret, tl.load(XTrue))\n+#         else:\n+#             tl.store(Ret, tl.load(XFalse))\n+\n+#     cond = torch.ones(1, dtype=torch.int32, device='cuda')\n+#     x_true = torch.tensor([3.14], dtype=torch.float32, device='cuda')\n+#     x_false = torch.tensor([1.51], dtype=torch.float32, device='cuda')\n+#     ret = torch.empty(1, dtype=torch.float32, device='cuda')\n+#     kernel[(1,)](cond, x_true, x_false, ret)\n+\n+\n+# def test_num_warps_pow2():\n+#     dst = torch.empty(128, device='cuda')\n+\n+#     @triton.jit\n+#     def _kernel(dst):\n+#         pass\n+\n+#     with pytest.raises(AssertionError, match='must be a power of 2'):\n+#         _kernel[(1,)](dst=dst, num_warps=3)\n+#     _kernel[(1,)](dst=dst, num_warps=1)\n+#     _kernel[(1,)](dst=dst, num_warps=2)\n+#     _kernel[(1,)](dst=dst, num_warps=4)\n+\n+# # -------------\n+# # test extern\n+# # -------------\n+\n+\n+# @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n+#                          [('int32', 'libdevice.ffs', ''),\n+#                           ('float32', 'libdevice.pow', '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'),\n+#                           ('float64', 'libdevice.norm4d', '')])\n+# def test_libdevice(dtype_str, expr, lib_path):\n+\n+#     @triton.jit\n+#     def kernel(X, Y, BLOCK: tl.constexpr):\n+#         x = tl.load(X + tl.arange(0, BLOCK))\n+#         y = GENERATE_TEST_HERE\n+#         tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+#     shape = (128, )\n+#     rs = RandomState(17)\n+#     # limit the range of integers so that the sum does not overflow\n+#     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n+\n+#     if expr == 'libdevice.ffs':\n+#         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.ffs(x)'})\n+#         y_ref = np.zeros(shape, dtype=x.dtype)\n+#         for i in range(shape[0]):\n+#             y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n+#     elif expr == 'libdevice.pow':\n+#         # numpy does not allow negative factors in power, so we use abs()\n+#         x = np.abs(x)\n+#         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+#         y_ref = np.power(x, x)\n+#     elif expr == 'libdevice.norm4d':\n+#         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.norm4d(x, x, x, x)'})\n+#         y_ref = np.sqrt(4 * np.power(x, 2))\n+\n+#     x_tri = to_triton(x)\n+#     # triton result\n+#     y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+#     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n+#     # compare\n+#     if expr == 'libdevice.ffs':\n+#         np.testing.assert_equal(y_ref, to_numpy(y_tri))\n+#     else:\n+#         np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 28, "deletions": 6, "changes": 34, "file_content_changes": "@@ -132,6 +132,9 @@ def is_floating(self):\n     def is_int_signed(self):\n         return self.name in dtype.SINT_TYPES\n \n+    def is_int_unsigned(self):\n+        return self.name in dtype.UINT_TYPES\n+\n     def is_int(self):\n         return self.name in dtype.SINT_TYPES + dtype.UINT_TYPES\n \n@@ -460,6 +463,11 @@ def __floordiv__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n         return semantic.floordiv(self, other, _builder)\n \n+    @builtin\n+    def __rfloordiv__(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n+        return semantic.floordiv(other, self, _builder)\n+\n     @builtin\n     def __mod__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n@@ -1041,21 +1049,35 @@ def debug_barrier(_builder=None):\n \n \n @builtin\n-def multiple_of(input, value, _builder=None):\n+def multiple_of(input, values, _builder=None):\n     \"\"\"\n     Let the compiler knows that the values in :code:`input` are all multiples of :code:`value`.\n     \"\"\"\n-    value = _constexpr_to_value(value)\n-    return semantic.multiple_of(input, value)\n+    if isinstance(values, constexpr):\n+        values = [values]\n+    for i, d in enumerate(values):\n+        if not isinstance(d, constexpr):\n+            raise TypeError(f\"values element {i} must have type `constexpr`\")\n+        if not isinstance(d.value, int):\n+            raise TypeError(f\"values element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n+    values = [x.value for x in values]\n+    return semantic.multiple_of(input, values)\n \n \n @builtin\n-def max_contiguous(input, value, _builder=None):\n+def max_contiguous(input, values, _builder=None):\n     \"\"\"\n     Let the compiler knows that the `value` first values in :code:`input` are contiguous.\n     \"\"\"\n-    value = _constexpr_to_value(value)\n-    return semantic.max_contiguous(input, value)\n+    if isinstance(values, constexpr):\n+        values = [values]\n+    for i, d in enumerate(values):\n+        if not isinstance(d, constexpr):\n+            raise TypeError(f\"values element {i} must have type `constexpr`\")\n+        if not isinstance(d.value, int):\n+            raise TypeError(f\"values element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n+    values = [x.value for x in values]\n+    return semantic.max_contiguous(input, values)\n \n \n # -----------------------"}, {"filename": "python/triton/language/random.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -91,9 +91,10 @@ def uint32_to_uniform_float(x):\n     Numerically stable function to convert a random uint32 into a random float uniformly sampled in [0, 1).\n     \"\"\"\n     x = x.to(tl.int32, bitcast=True)\n-    max = 4.656613e-10  # = 1/MAX_INT = 1/2147483647.\n+    # maximum value such that `MAX_INT * scale < 1.0` (with float rounding)\n+    scale = 4.6566127342e-10\n     x = tl.where(x < 0, -x - 1, x)\n-    return x * max\n+    return x * scale\n \n \n @triton.jit"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 66, "deletions": 24, "changes": 90, "file_content_changes": "@@ -58,14 +58,22 @@ def computation_type_impl(a_ty: tl.dtype, b_ty: tl.dtype, div_or_mod: bool) -> t\n         return tl.float32\n     # 3 ) if one operand is half, the other is implicitly converted to half\n     #     unless we're doing / or %, which do not exist natively in PTX for fp16.\n+    #     Supported PTX op: add, sub, mul, fma, neg, abs, min, max, tanh, ex2, setp\n     if a_ty.is_fp16() or b_ty.is_fp16():\n         if div_or_mod:\n             return tl.float32\n         else:\n             return tl.float16\n+    # 4) return bf16 only if both operands are of bf16\n+    if a_ty.is_bf16() or b_ty.is_bf16():\n+        if div_or_mod:\n+            return tl.float32\n+        if a_ty.is_bf16() and b_ty.is_bf16():\n+            return tl.bfloat16\n+        return tl.float32\n     if not a_ty.is_int() or not b_ty.is_int():\n         assert False\n-    # 4 ) both operands are integer and undergo\n+    # 5 ) both operands are integer and undergo\n     #    integer promotion\n     if div_or_mod and a_ty.int_signedness != b_ty.int_signedness:\n         raise ValueError(\"Cannot use /, #, or % with \" + a_ty.__repr__() + \" and \" + b_ty.__repr__() + \" because they have different signedness;\"\n@@ -452,6 +460,9 @@ def not_equal(input: tl.tensor,\n \n \n def arange(start: int, end: int, builder: ir.builder) -> tl.tensor:\n+    if not isinstance(start, int) or not isinstance(end, int):\n+        raise ValueError(\"arange's arguments must be of type tl.constexpr\")\n+\n     shape = [end - start]\n     ret_ty = tl.block_type(tl.int32, shape)\n     return tl.tensor(builder.create_make_range(start, end), ret_ty)\n@@ -502,6 +513,11 @@ def broadcast_impl_shape(input: tl.tensor,\n         raise ValueError(f\"Cannot broadcast, rank mismatch: {src_shape}, {shape}\")\n     if shape == src_shape:\n         return input\n+    for i in range(len(src_shape)):\n+        if shape[i] != src_shape[i] and src_shape[i] != 1:\n+            raise ValueError(f\"Cannot broadcast, the expanded size of the tensor ({shape[i]})\"\n+                             f\" must match the existing size ({src_shape[1]}) at non-singleton dimension\"\n+                             f\" {i}: {src_shape}, {shape}\")\n     ret_ty = tl.block_type(input.type.scalar, shape)\n     return tl.tensor(builder.create_broadcast(input.handle, shape), ret_ty)\n \n@@ -598,7 +614,13 @@ def cast(input: tl.tensor,\n         return input\n     src_sca_ty = src_ty.scalar\n     dst_sca_ty = dst_ty.scalar\n-\n+    # fp8 <=> bf16/fp16\n+    if (src_sca_ty.is_bf16() or src_sca_ty.is_fp16()) and dst_sca_ty.is_fp8():\n+        return tl.tensor(builder.create_fp_trunc(input.handle, dst_ty.to_ir(builder)),\n+                         dst_ty)\n+    if src_sca_ty.is_fp8() and (dst_sca_ty.is_bf16() or dst_sca_ty.is_fp16()):\n+        return tl.tensor(builder.create_fp_ext(input.handle, dst_ty.to_ir(builder)),\n+                         dst_ty)\n     # bf16 <=> (not fp32)\n     if (src_sca_ty.is_bf16() and not dst_sca_ty.is_fp32()) or \\\n        (dst_sca_ty.is_bf16() and not src_sca_ty.is_fp32()):\n@@ -783,16 +805,25 @@ def atomic_cas(ptr: tl.tensor,\n                cmp: tl.tensor,\n                val: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    # TODO: type checking\n+    element_ty = ptr.type.scalar.element_ty\n+    if element_ty.primitive_bitwidth not in [16, 32, 64]:\n+        raise ValueError(\"atomic_cas only supports elements with width {16, 32, 64}\")\n     return tl.tensor(builder.create_atomic_cas(ptr.handle, cmp.handle, val.handle), val.type)\n \n \n def atom_red_typechecking_impl(ptr: tl.tensor,\n                                val: tl.tensor,\n                                mask: tl.tensor,\n+                               op: str,\n                                builder: ir.builder) -> Tuple[tl.tensor, tl.tensor, tl.tensor]:\n     if not ptr.type.scalar.is_ptr():\n         raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n+\n+    element_ty = ptr.type.scalar.element_ty\n+    if element_ty is tl.float16 and op != 'add':\n+        raise ValueError(\"atomic_\" + op + \" does not support fp16\")\n+    if element_ty in [tl.int1, tl.int8, tl.int16, tl.bfloat16]:\n+        raise ValueError(\"atomic_\" + op + \" does not support \" + element_ty)\n     if ptr.type.is_block():\n         if mask:\n             mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n@@ -813,7 +844,7 @@ def atomic_max(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'max', builder)\n     sca_ty = val.type.scalar\n     # direct call to atomic_max for integers\n     if sca_ty.is_int():\n@@ -845,7 +876,7 @@ def atomic_min(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'min', builder)\n     sca_ty = val.type.scalar\n     # direct call to atomic_min for integers\n     if sca_ty.is_int():\n@@ -885,7 +916,7 @@ def atomic_add(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'add', builder)\n     sca_ty = val.type.scalar\n     op = ir.ATOMIC_OP.FADD if sca_ty.is_floating() else ir.ATOMIC_OP.ADD\n     return tl.tensor(builder.create_atomic_rmw(op, ptr.handle, val.handle, mask.handle), val.type)\n@@ -895,31 +926,31 @@ def atomic_and(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'and', builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.AND, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_or(ptr: tl.tensor,\n               val: tl.tensor,\n               mask: tl.tensor,\n               builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'or', builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.OR, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_xor(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'xor', builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XOR, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_xchg(ptr: tl.tensor,\n                 val: tl.tensor,\n                 mask: tl.tensor,\n                 builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'xchg', builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XCHG, ptr.handle, val.handle, mask.handle), val.type)\n \n # ===----------------------------------------------------------------------===//\n@@ -961,16 +992,8 @@ def where(condition: tl.tensor,\n         x = broadcast_impl_shape(x, condition.type.get_block_shapes(), builder)\n         y = broadcast_impl_shape(y, condition.type.get_block_shapes(), builder)\n \n-    # TODO: we need to check x's and y's shape?\n-    x_ty = x.type.scalar\n-    y_ty = y.type.scalar\n-    ty = computation_type_impl(x_ty, y_ty, div_or_mod=False)\n-    x = cast(x, ty, builder)\n-    y = cast(y, ty, builder)\n-    if x.type.is_block():\n-        ret_ty = tl.block_type(ty, x.type.shape)\n-    else:\n-        ret_ty = ty\n+    x, y = binary_op_type_checking_impl(x, y, builder, True, True)\n+    ret_ty = x.type\n     return tl.tensor(builder.create_select(condition.handle, x.handle, y.handle), ret_ty)\n \n \n@@ -987,6 +1010,21 @@ def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n     if scalar_ty.is_int() and scalar_ty.int_bitwidth <= 32:\n         input = cast(input, tl.int32, builder)\n \n+    # hardware doesn't support FMAX, FMIN, CMP for bfloat16\n+    if scalar_ty is tl.bfloat16:\n+        input = cast(input, tl.float32, builder)\n+\n+    # choose the right unsigned operation\n+    if scalar_ty.is_int_unsigned():\n+        int_op_to_unit = {\n+            ir.REDUCE_OP.MIN: ir.REDUCE_OP.UMIN,\n+            ir.REDUCE_OP.MAX: ir.REDUCE_OP.UMAX,\n+            ir.REDUCE_OP.ARGMIN: ir.REDUCE_OP.ARGUMIN,\n+            ir.REDUCE_OP.ARGMAX: ir.REDUCE_OP.ARGUMAX,\n+        }\n+        if INT_OP in int_op_to_unit:\n+            INT_OP = int_op_to_unit[INT_OP]\n+\n     # get result type\n     shape = input.type.shape\n     ret_shape = []\n@@ -1056,13 +1094,17 @@ def sqrt(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n \n ##\n \n-def multiple_of(x: tl.tensor, value: int) -> tl.tensor:\n-    x.handle.multiple_of(value)\n+def multiple_of(x: tl.tensor, values: List[int]) -> tl.tensor:\n+    if len(x.shape) != len(values):\n+        raise ValueError(\"Shape of input to multiple_of does not match the length of values\")\n+    x.handle.multiple_of(values)\n     return x\n \n \n-def max_contiguous(x: tl.tensor, value: int) -> tl.tensor:\n-    x.handle.max_contiguous(value)\n+def max_contiguous(x: tl.tensor, values: List[int]) -> tl.tensor:\n+    if len(x.shape) != len(values):\n+        raise ValueError(\"Shape of input to max_contiguous does not match the length of values\")\n+    x.handle.max_contiguous(values)\n     return x\n \n "}, {"filename": "python/triton/ops/cross_entropy.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -65,7 +65,7 @@ def _backward(PROBS, IDX, DPROBS, N, BLOCK: tl.constexpr):\n     # write result in-place in PROBS\n     dout = tl.load(DPROBS + row)\n     din = (probs - delta) * dout\n-    tl.store(PROBS, din.to(tl.float16), mask=cols < N)\n+    tl.store(PROBS, din.to(PROBS.dtype.element_ty), mask=cols < N)\n \n \n class _cross_entropy(torch.autograd.Function):"}, {"filename": "python/triton/ops/matmul_perf_model.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -26,7 +26,7 @@ def get_simd_tflops(backend, device, num_ctas, num_warps, dtype):\n def get_tflops(backend, device, num_ctas, num_warps, dtype):\n     cc = _triton.runtime.cc(backend, device)\n     if cc < 80 and dtype == torch.float32:\n-        return get_simd_tflops()\n+        return get_simd_tflops(backend, device, num_ctas, num_warps, dtype)\n     return get_tensorcore_tflops(backend, device, num_ctas, num_warps, dtype)\n \n "}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 76, "deletions": 0, "changes": 76, "file_content_changes": "@@ -1,6 +1,8 @@\n+import functools\n import os\n import subprocess\n import sys\n+from contextlib import contextmanager\n \n import torch\n \n@@ -358,6 +360,80 @@ def get_max_tensorcore_tflops(dtype: torch.dtype, backend=None, device=None, clo\n     tflops = num_subcores * clock_rate * ops_per_sub_core * 1e-9\n     return tflops\n \n+# create decorator that wraps test function into\n+# a cuda-memcheck system call\n+\n+\n+def cuda_memcheck(**target_kwargs):\n+    def decorator(test_fn):\n+        @functools.wraps(test_fn)\n+        def wrapper(*args, **kwargs):\n+            import psutil\n+            ppid_name = psutil.Process(os.getppid()).name()\n+            run_cuda_memcheck = target_kwargs.items() <= kwargs.items()\n+            if run_cuda_memcheck and ppid_name != \"cuda-memcheck\":\n+                path = os.path.realpath(test_fn.__globals__[\"__file__\"])\n+                # get path of current file\n+                env = {\"PATH\": os.environ[\"PATH\"], \"PYTORCH_NO_CUDA_MEMORY_CACHING\": \"1\"}\n+                assert 'request' in kwargs, \"memcheck'ed test must have a (possibly unused) `request` fixture\"\n+                test_id = kwargs['request'].node.callspec.id\n+                cmd = f\"{path}::{test_fn.__name__}[{test_id}]\"\n+                out = subprocess.run([\"cuda-memcheck\", \"pytest\", \"-vs\", cmd], capture_output=True, env=env)\n+                assert out.returncode == 0, \"cuda-memcheck returned an error: bounds checking failed\"\n+                assert \"ERROR SUMMARY: 0 errors\" in str(out.stdout)\n+            else:\n+                test_fn(*args, **kwargs)\n+        return wrapper\n+    return decorator\n+\n+\n+def nvsmi_attr(attrs):\n+    attrs = \",\".join(attrs)\n+    cmd = [\n+        \"nvidia-smi\",\n+        \"-i\",\n+        \"0\",\n+        \"--query-gpu=\" + attrs,\n+        \"--format=csv,noheader,nounits\",\n+    ]\n+    out = subprocess.check_output(cmd)\n+    ret = out.decode(sys.stdout.encoding).split(\",\")\n+    ret = [int(x) for x in ret]\n+    return ret\n+\n+\n+@contextmanager\n+def set_gpu_clock(ref_sm_clock=1350, ref_mem_clock=1215):\n+    try:\n+        subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-pm\", \"1\"])\n+        subprocess.check_output(\n+            [\n+                \"nvidia-smi\",\n+                \"-i\",\n+                \"0\",\n+                f\"--lock-gpu-clocks={ref_sm_clock},{ref_sm_clock}\",\n+            ]\n+        )\n+        subprocess.check_output(\n+            [\n+                \"nvidia-smi\",\n+                \"-i\",\n+                \"0\",\n+                f\"--lock-memory-clocks={ref_mem_clock},{ref_mem_clock}\",\n+            ]\n+        )\n+        cur_sm_clock = nvsmi_attr([\"clocks.current.sm\"])[0]\n+        cur_mem_clock = nvsmi_attr([\"clocks.current.memory\"])[0]\n+        assert abs(cur_sm_clock - ref_sm_clock) < 10, f\"GPU SMs must run at {ref_sm_clock} MHz\"\n+        assert abs(cur_mem_clock - ref_mem_clock) < 10, f\"GPU SMs must run at {ref_mem_clock} MHz\"\n+        tflops = 1e-6 * 2 * 108 * 4 * 256 * ref_sm_clock\n+        gbps = 640 * 2 * ref_mem_clock * 1e-3\n+        yield tflops, gbps\n+    finally:\n+        subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-pm\", \"0\"])\n+        subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-rgc\"])\n+        subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-rmc\"])\n+\n \n def get_max_simd_tflops(dtype: torch.dtype, backend=None, device=None):\n     if not backend:"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -76,8 +76,8 @@\n #\n #  .. code-block:: python\n #\n-#    pa += BLOCK_SIZE_K * stride_ak;\n-#    pb += BLOCK_SIZE_K * stride_bk;\n+#    a_ptrs += BLOCK_SIZE_K * stride_ak;\n+#    b_ptrs += BLOCK_SIZE_K * stride_bk;\n #\n #\n # L2 Cache Optimizations"}]