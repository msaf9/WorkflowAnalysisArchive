[{"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 13, "deletions": 7, "changes": 20, "file_content_changes": "@@ -49,7 +49,7 @@\n                 (128, 128, 32, 1, 4, 2, 384, 128, 640, AT, BT, DTYPE),\n                 (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE),\n                 (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE),\n-            ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n+            ] for DTYPE in [\"int8\",\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n         # n-stage\n         *[\n@@ -62,7 +62,7 @@\n                 # split-k\n                 (64, 64, 16, 8, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n                 (64, 64, 16, 8, 4, STAGES, 1024, 1024, 32, AT, BT, DTYPE),\n-            ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [2, 3, 4]\n+            ] for DTYPE in [\"int8\",\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [2, 3, 4]\n         ]\n     ),\n )\n@@ -88,15 +88,21 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n     N = BLOCK_N if N is None else N\n     K = BLOCK_K * SPLIT_K if K is None else K\n     # allocate/transpose inputs\n-    DTYPE = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[DTYPE]\n-    a = .1 * torch.randn((K, M) if AT else (M, K), device=\"cuda\", dtype=DTYPE)\n-    b = .1 * torch.randn((N, K) if BT else (K, N), device=\"cuda\", dtype=DTYPE)\n+    DTYPE = {\"int8\": torch.int8, \"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[DTYPE]\n+    if DTYPE == torch.int8:\n+        a = (torch.rand((K, M) if AT else (M, K), device=\"cuda\") * 255 - 128).to(torch.int8)\n+        b = (torch.rand((N, K) if BT else (K, N), device=\"cuda\") * 255 - 128).to(torch.int8)\n+        out_dtype = torch.int32\n+    else:\n+        a = .1 * torch.randn((K, M) if AT else (M, K), device=\"cuda\", dtype=DTYPE)\n+        b = .1 * torch.randn((N, K) if BT else (K, N), device=\"cuda\", dtype=DTYPE)\n+        out_dtype = DTYPE\n     a = a.t() if AT else a\n     b = b.t() if BT else b\n     # run test\n-    th_c = torch.matmul(a, b)\n+    th_c = torch.matmul(a.to(torch.float32), b.to(torch.float32)).to(out_dtype)\n     try:\n-        tt_c = triton.ops.matmul(a, b)\n+        tt_c = triton.ops.matmul(a, b, gemm_out_dtype = out_dtype)\n         torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n     except triton.OutOfResources as e:\n         pytest.skip(str(e))"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -119,7 +119,7 @@ class _matmul(torch.autograd.Function):\n     _locks = {}\n \n     @staticmethod\n-    def _call(a, b, dot_out_dtype):\n+    def _call(a, b, dot_out_dtype, gemm_out_dtype):\n         device = a.device\n         # handle non-contiguous inputs if necessary\n         if a.stride(0) > 1 and a.stride(1) > 1:\n@@ -131,7 +131,9 @@ def _call(a, b, dot_out_dtype):\n         M, K = a.shape\n         _, N = b.shape\n         # allocates output\n-        c = torch.empty((M, N), device=device, dtype=a.dtype)\n+        if gemm_out_dtype is None:\n+            gemm_out_dtype = a.dtype\n+        c = torch.empty((M, N), device=device, dtype=gemm_out_dtype)\n         if dot_out_dtype is None:\n             if a.dtype in [torch.float16, torch.float32, torch.bfloat16]:\n                 dot_out_dtype = tl.float32\n@@ -156,8 +158,8 @@ def _call(a, b, dot_out_dtype):\n         return c\n \n     @staticmethod\n-    def forward(ctx, a, b, dot_out_dtype=None):\n-        return _matmul._call(a, b, dot_out_dtype=dot_out_dtype)\n+    def forward(ctx, a, b, dot_out_dtype=None, gemm_out_dtype=None):\n+        return _matmul._call(a, b, dot_out_dtype=dot_out_dtype, gemm_out_dtype=gemm_out_dtype)\n \n \n matmul = _matmul.apply"}]