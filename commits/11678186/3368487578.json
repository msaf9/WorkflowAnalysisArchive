[{"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 13, "deletions": 5, "changes": 18, "file_content_changes": "@@ -29,12 +29,20 @@ def TT_RedOpAttr : I32EnumAttr<\n     /*case*/\n     [\n         I32EnumAttrCase</*sym*/\"ADD\", 1, /*str*/\"add\">,\n-        I32EnumAttrCase<\"MAX\", 2, \"max\">,\n+        I32EnumAttrCase<\"FADD\", 2, \"fadd\">,\n         I32EnumAttrCase<\"MIN\", 3, \"min\">,\n-        I32EnumAttrCase<\"FADD\", 4, \"fadd\">,\n-        I32EnumAttrCase<\"FMAX\", 5, \"fmax\">,\n-        I32EnumAttrCase<\"FMIN\", 6, \"fmin\">,\n-        I32EnumAttrCase<\"XOR\", 7, \"xor\">\n+        I32EnumAttrCase<\"MAX\", 4, \"max\">,\n+        I32EnumAttrCase<\"UMIN\", 5, \"umin\">,\n+        I32EnumAttrCase<\"UMAX\", 6, \"umax\">,\n+        I32EnumAttrCase<\"ARGMIN\", 7, \"argmin\">,\n+        I32EnumAttrCase<\"ARGMAX\", 8, \"argmax\">,\n+        I32EnumAttrCase<\"ARGUMIN\", 9, \"argumin\">,\n+        I32EnumAttrCase<\"ARGUMAX\", 10, \"argumax\">,\n+        I32EnumAttrCase<\"FMIN\", 11, \"fmin\">,\n+        I32EnumAttrCase<\"FMAX\", 12, \"fmax\">,\n+        I32EnumAttrCase<\"ARGFMIN\", 13, \"argfmin\">,\n+        I32EnumAttrCase<\"ARGFMAX\", 14, \"argfmax\">,\n+        I32EnumAttrCase<\"XOR\", 15, \"xor\">\n     ]> {\n     let cppNamespace = \"::mlir::triton\";\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 661, "deletions": 101, "changes": 762, "file_content_changes": "@@ -56,6 +56,12 @@ Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v) {\n                                            IntegerAttr::get(i32ty, v));\n }\n \n+Value createConstantF32(Location loc, PatternRewriter &rewriter, float v) {\n+  auto type = type::f32Ty(rewriter.getContext());\n+  return rewriter.create<LLVM::ConstantOp>(loc, type,\n+                                           rewriter.getF32FloatAttr(v));\n+}\n+\n // Create a index type constant.\n Value createIndexConstant(OpBuilder &builder, Location loc,\n \n@@ -90,7 +96,8 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n #define fmin(...) rewriter.create<LLVM::MinNumOp>(loc, __VA_ARGS__)\n #define and_(...) rewriter.create<LLVM::AndOp>(loc, __VA_ARGS__)\n #define xor_(...) rewriter.create<LLVM::XOrOp>(loc, __VA_ARGS__)\n-#define bitcast(...) rewriter.create<LLVM::BitcastOp>(loc, __VA_ARGS__)\n+#define bitcast(val__, type__)                                                 \\\n+  rewriter.create<LLVM::BitcastOp>(loc, type__, val__)\n #define gep(...) rewriter.create<LLVM::GEPOp>(loc, __VA_ARGS__)\n #define ptr_ty(...) LLVM::LLVMPointerType::get(__VA_ARGS__)\n #define insert_val(...) rewriter.create<LLVM::InsertValueOp>(loc, __VA_ARGS__)\n@@ -112,11 +119,13 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n #define barrier() rewriter.create<mlir::gpu::BarrierOp>(loc)\n #define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n #define i32_ty rewriter.getIntegerType(32)\n+#define f16_ty rewriter.getF16Type()\n #define i8_ty rewriter.getIntegerType(8)\n #define f32_ty rewriter.getF32Type()\n #define vec_ty(type, num) VectorType::get(num, type)\n+#define f32_val(...) LLVM::createConstantF32(loc, rewriter, __VA_ARGS__)\n #define void_ty(ctx) LLVM::LLVMVoidType::get(ctx)\n-#define struct_ty(...) LLVM::LLVMStructType::getLiteral(__VA_ARGS__)\n+#define struct_ty(...) LLVM::LLVMStructType::getLiteral(ctx, __VA_ARGS__)\n \n // Creator for constant\n #define i32_val(...) LLVM::createConstantI32(loc, rewriter, __VA_ARGS__)\n@@ -830,7 +839,7 @@ Value convertSplatLikeOp(Type elemType, Type resType, Value constVal,\n   if (tensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n     auto tensorTy = resType.cast<RankedTensorType>();\n     auto srcType = typeConverter->convertType(elemType);\n-    auto llSrc = bitcast(srcType, constVal);\n+    auto llSrc = bitcast(constVal, srcType);\n     size_t elemsPerThread = getElemsPerThread(tensorTy);\n     llvm::SmallVector<Value> elems(elemsPerThread, llSrc);\n     llvm::SmallVector<Type> elemTypes(elems.size(), srcType);\n@@ -1109,7 +1118,7 @@ struct LoadOpConversion\n                 rewriter, loc, this->getTypeConverter()->getIndexType(), s);\n             v = insert_element(vecTy, v, falseVal, sVal);\n           }\n-          v = bitcast(IntegerType::get(getContext(), width), v);\n+          v = bitcast(v, IntegerType::get(getContext(), width));\n \n           PTXInstr::Operand *opr{};\n           if (otherIsSplatConstInt)\n@@ -1146,9 +1155,8 @@ struct LoadOpConversion\n         } else {\n           curr = ret;\n         }\n-        curr = bitcast(\n-            LLVM::getFixedVectorType(valueElemTy, width / valueElemNbits),\n-            curr);\n+        curr = bitcast(curr, LLVM::getFixedVectorType(valueElemTy,\n+                                                      width / valueElemNbits));\n         rets.push_back(curr);\n       }\n       int tmp = width / valueElemNbits;\n@@ -1246,15 +1254,15 @@ struct StoreOpConversion\n           Value elem = valueElems[elemOffset];\n           if (elem.getType().isInteger(1))\n             elem = rewriter.create<LLVM::SExtOp>(loc, type::i8Ty(ctx), elem);\n-          elem = bitcast(valueElemTy, elem);\n+          elem = bitcast(elem, valueElemTy);\n \n           Type u32Ty = typeConverter->convertType(type::u32Ty(ctx));\n           llWord =\n               insert_element(wordTy, llWord, elem,\n                              rewriter.create<LLVM::ConstantOp>(\n                                  loc, u32Ty, IntegerAttr::get(u32Ty, elemIdx)));\n         }\n-        llWord = bitcast(valArgTy, llWord);\n+        llWord = bitcast(llWord, valArgTy);\n         std::string constraint =\n             (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n         asmArgs.emplace_back(llWord, constraint);\n@@ -1438,27 +1446,27 @@ void ReduceOpConversion::accumulate(ConversionPatternRewriter &rewriter,\n   case RedOp::ADD:\n     acc = add(acc, cur);\n     break;\n-  case RedOp::MAX:\n-    if (type.isUnsignedInteger())\n-      acc = umax(acc, cur);\n-    else\n-      acc = smax(acc, cur);\n+  case RedOp::FADD:\n+    acc = fadd(acc.getType(), acc, cur);\n     break;\n   case RedOp::MIN:\n-    if (type.isUnsignedInteger())\n-      acc = umin(acc, cur);\n-    else\n-      acc = smin(acc, cur);\n+    acc = smin(acc, cur);\n     break;\n-  case RedOp::FADD:\n-    acc = fadd(acc.getType(), acc, cur);\n+  case RedOp::MAX:\n+    acc = smax(acc, cur);\n     break;\n-  case RedOp::FMAX:\n-    acc = fmax(acc, cur);\n+  case RedOp::UMIN:\n+    acc = umin(acc, cur);\n+    break;\n+  case RedOp::UMAX:\n+    acc = umax(acc, cur);\n     break;\n   case RedOp::FMIN:\n     acc = fmin(acc, cur);\n     break;\n+  case RedOp::FMAX:\n+    acc = fmax(acc, cur);\n+    break;\n   case RedOp::XOR:\n     acc = xor_(acc, cur);\n     break;\n@@ -1473,15 +1481,15 @@ Value ReduceOpConversion::shflSync(ConversionPatternRewriter &rewriter,\n \n   if (bits == 64) {\n     Type vecTy = vec_ty(f32_ty, 2);\n-    Value vec = bitcast(vecTy, val);\n+    Value vec = bitcast(val, vecTy);\n     Value val0 = extract_element(f32_ty, vec, i32_val(0));\n     Value val1 = extract_element(f32_ty, vec, i32_val(1));\n     val0 = shflSync(rewriter, loc, val0, i);\n     val1 = shflSync(rewriter, loc, val1, i);\n     vec = undef(vecTy);\n     vec = insert_element(vecTy, vec, val0, i32_val(0));\n     vec = insert_element(vecTy, vec, val1, i32_val(1));\n-    return bitcast(val.getType(), vec);\n+    return bitcast(vec, val.getType());\n   }\n \n   PTXBuilder builder;\n@@ -1508,7 +1516,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n   auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n   auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n   Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n-  smemBase = bitcast(elemPtrTy, smemBase);\n+  smemBase = bitcast(smemBase, elemPtrTy);\n \n   auto smemShape = getScratchConfigForReduce(op);\n \n@@ -1575,7 +1583,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n \n     barrier();\n     SmallVector<Value> resultVals(resultElems);\n-    for (size_t i = 0; i < resultElems; i++) {\n+    for (unsigned i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       readIdx.insert(readIdx.begin() + axis, ints[0]);\n       Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n@@ -1614,7 +1622,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n   auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n   Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n-  smemBase = bitcast(elemPtrTy, smemBase);\n+  smemBase = bitcast(smemBase, elemPtrTy);\n \n   auto order = srcLayout.getOrder();\n   unsigned sizeIntraWarps = threadsPerWarp[axis];\n@@ -1714,7 +1722,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n \n     barrier();\n     SmallVector<Value> resultVals(resultElems);\n-    for (size_t i = 0; i < resultElems; i++) {\n+    for (size_t i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       readIdx.insert(readIdx.begin() + axis, i32_val(0));\n       Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n@@ -2282,7 +2290,7 @@ void ConvertLayoutOpConversion::processReplica(\n       auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n       Value ptr = gep(elemPtrTy, smemBase, offset);\n       auto vecTy = vec_ty(llvmElemTy, vec);\n-      ptr = bitcast(ptr_ty(vecTy, 3), ptr);\n+      ptr = bitcast(ptr, ptr_ty(vecTy, 3));\n       if (stNotRd) {\n         Value valVec = undef(vecTy);\n         for (unsigned v = 0; v < vec; ++v) {\n@@ -2321,7 +2329,7 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n   auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n   Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n   auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n-  smemBase = bitcast(elemPtrTy, smemBase);\n+  smemBase = bitcast(smemBase, elemPtrTy);\n   auto shape = dstTy.getShape();\n   unsigned rank = dstTy.getRank();\n   SmallVector<unsigned> numReplicates(rank);\n@@ -2380,7 +2388,8 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n   }\n \n   SmallVector<Type> types(outElems, llvmElemTy);\n-  Type structTy = struct_ty(getContext(), types);\n+  auto *ctx = llvmElemTy.getContext();\n+  Type structTy = struct_ty(types);\n   Value result = getStructFromElements(loc, outVals, rewriter, structTy);\n   rewriter.replaceOp(op, result);\n \n@@ -2440,7 +2449,7 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n   Value minVecVal = idx_val(minVec);\n   Value smemBase = getSharedMemoryBase(loc, rewriter, dst);\n   auto elemPtrTy = ptr_ty(getTypeConverter()->convertType(elemTy), 3);\n-  smemBase = bitcast(elemPtrTy, smemBase);\n+  smemBase = bitcast(smemBase, elemPtrTy);\n   unsigned numWordsEachRep = product<unsigned>(wordsInEachRep);\n   SmallVector<Value> wordVecs(numWordsEachRep);\n   // TODO: We should get less barriers if it is handled by membar pass\n@@ -2496,7 +2505,7 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n \n         // step 3: store\n         Value smemAddr = gep(elemPtrTy, smemBase, offset);\n-        smemAddr = bitcast(ptr_ty(wordTy, 3), smemAddr);\n+        smemAddr = bitcast(smemAddr, ptr_ty(wordTy, 3));\n         store(wordVecs[linearWordIdx], smemAddr);\n       }\n     }\n@@ -2839,7 +2848,7 @@ class MMA16816SmemLoader {\n           for (int e = 0; e < 4; ++e)\n             i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n                                           i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bitcast(i32_ty, i8v4Elems[m]);\n+          i32Elems[m] = bitcast(i8v4Elems[m], i32_ty);\n         }\n       } else { // k first\n         Value offset = i32_val(sOffsetElem);\n@@ -2857,7 +2866,7 @@ class MMA16816SmemLoader {\n           for (int e = 0; e < 4; ++e)\n             i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n                                           i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bitcast(i32_ty, i8v4Elems[m]);\n+          i32Elems[m] = bitcast(i8v4Elems[m], i32_ty);\n         }\n       }\n \n@@ -2969,10 +2978,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                                 ConversionPatternRewriter &rewriter) const;\n   /// Convert to mma.m8n8k4\n   LogicalResult convertMMA884(triton::DotOp op, OpAdaptor adaptor,\n-                              ConversionPatternRewriter &rewriter) const {\n-    assert(false && \"Not implemented yet.\");\n-    return failure();\n-  }\n+                              ConversionPatternRewriter &rewriter) const;\n \n   LogicalResult convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n                               ConversionPatternRewriter &rewriter) const {\n@@ -2981,57 +2987,136 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n   }\n };\n \n-struct DotOpConversionHelper {\n+// Helper for conversion of DotOp with mma<version=1>, that is sm<80\n+struct DotOpMmaV1ConversionHelper {\n+  MmaEncodingAttr mmaLayout;\n+  ArrayRef<unsigned> wpt;\n+\n+  using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+\n+  explicit DotOpMmaV1ConversionHelper(MmaEncodingAttr mmaLayout)\n+      : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()) {}\n+\n+  int getRepM(int M) const {\n+    return std::max<int>(M / (wpt[0] * instrShape[0]), 1);\n+  }\n+  int getRepN(int N) const {\n+    return std::max<int>(N / (wpt[1] * instrShape[1]), 1);\n+  }\n+  int getRepK(int K) const { return std::max<int>(K / instrShape[2], 1); }\n+\n+  static ArrayRef<unsigned> getMmaInstrShape() { return instrShape; }\n+\n+  static Type getMmaRetType(TensorType operand) {\n+    auto *ctx = operand.getContext();\n+    Type fp32Ty = type::f32Ty(ctx);\n+    // f16*f16+f32->f32\n+    return struct_ty(SmallVector<Type>{8, fp32Ty});\n+  }\n+\n+  // number of fp16x2 elements for $a.\n+  int numElemsPerThreadA(RankedTensorType tensorTy) const {\n+    auto shape = tensorTy.getShape();\n+    auto order = getOrder();\n+\n+    bool isARow = order[0] != 0;\n+    bool isAVec4 = !isARow && shape[order[0]] <= 16; // fp16*4 = 16bytes\n+    int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+\n+    SmallVector<int> fpw({2, 2, 1});\n+    int repM = 2 * packSize0;\n+    int repK = 1;\n+    int spwM = fpw[0] * 4 * repM;\n+    SmallVector<int> rep({repM, 0, repK}); // pad N with 0\n+    SmallVector<int> spw({spwM, 0, 1});    // pad N with 0\n+\n+    int NK = shape[1];\n+    unsigned numM = rep[0] * shape[0] / (spw[0] * wpt[0]);\n+\n+    // NOTE We cound't get the vec from the shared layout.\n+    // int vecA = sharedLayout.getVec();\n+    // TODO[Superjomn]: Consider the case when vecA > 4\n+    bool vecGt4 = false;\n+    int elemsPerLd = vecGt4 ? 4 : 2;\n+    return (numM / 2) * (NK / 4) * elemsPerLd;\n+  }\n+\n+  // number of fp16x2 elements for $b.\n+  int numElemsPerThreadB(RankedTensorType tensorTy) const {\n+    auto shape = tensorTy.getShape();\n+    auto order = getOrder();\n+    bool isBRow = order[0] != 0;\n+    bool isBVec4 = isBRow && shape[order[0]] <= 16;\n+    int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+    SmallVector<int> fpw({2, 2, 1});\n+    SmallVector<int> rep({0, 2 * packSize1, 1});       // pad M with 0\n+    SmallVector<int> spw({0, fpw[1] * 4 * rep[1], 1}); // pad M with 0\n+    // NOTE We cound't get the vec from the shared layout.\n+    // int vecB = sharedLayout.getVec();\n+    // TODO[Superjomn]: Consider the case when vecA > 4\n+    bool vecGt4 = false;\n+    int elemsPerLd = vecGt4 ? 4 : 2;\n+    int NK = shape[0];\n+\n+    unsigned numN = rep[1] * shape[1] / (spw[1] * wpt[0]);\n+    return (numN / 2) * (NK / 4) * elemsPerLd;\n+  }\n+\n+  // Loading $a from smem to registers, returns a LLVM::Struct.\n+  Value loadA(Value A, Value llA, Value thread, Value smem, Location loc,\n+              ConversionPatternRewriter &rewriter) const;\n+\n+  // Loading $b from smem to registers, returns a LLVM::Struct.\n+  Value loadB(Value B, Value llB, Value thread, Value smem, Location loc,\n+              ConversionPatternRewriter &rewriter) const;\n+\n+  // Loading $c to registers, returns a LLVM::Struct.\n+  Value loadC(Value C, Value llC, ConversionPatternRewriter &rewriter) const;\n+\n+  static ArrayRef<unsigned> getOrder() { return mmaOrder; }\n+\n+  // Compute the offset of the matrix to load.\n+  // Returns offsetAM, offsetAK, offsetBN, offsetBK.\n+  // NOTE, the information M(from $a) and N(from $b) couldn't be retrieved at\n+  // the same time in the usage in convert_layout[shared->dot_op], we leave the\n+  // noexist info to be 0 and only use the desired argument from the composed\n+  // result. In this way we want to retain the original code structure in\n+  // convert_mma884 method for easier debugging.\n+  std::tuple<Value, Value, Value, Value>\n+  computeOffsets(Value threadId, bool isARow, bool isBRow, ArrayRef<int> fpw,\n+                 ArrayRef<int> spw, ArrayRef<int> rep,\n+                 ConversionPatternRewriter &rewriter, Location loc) const;\n+\n+  // Extract values belong to $a or $b from a LLVMStruct, the shape is n0xn1.\n+  ValueTable extractLoadedOperand(Value llStruct, int n0, int n1,\n+                                  ConversionPatternRewriter &rewriter) const;\n+\n+private:\n+  static constexpr unsigned instrShape[] = {16, 16, 4};\n+  static constexpr unsigned mmaOrder[] = {0, 1};\n+};\n+\n+// Helper for conversion of DotOp with mma<version=2>, that is sm>=80\n+struct DotOpMmaV2ConversionHelper {\n   using TensorCoreType = DotOpConversion::TensorCoreType;\n \n   MmaEncodingAttr mmaLayout;\n   MLIRContext *ctx{};\n \n-  explicit DotOpConversionHelper(MmaEncodingAttr mmaLayout)\n+  explicit DotOpMmaV2ConversionHelper(MmaEncodingAttr mmaLayout)\n       : mmaLayout(mmaLayout) {\n     ctx = mmaLayout.getContext();\n   }\n \n-  // Load SplatLike C which contains a constVal. It simply returns 4 fp32\n-  // constVal.\n-  SmallVector<Value> loadSplatLikeC(Value C, Location loc,\n-                                    ConversionPatternRewriter &rewriter) const {\n-    assert(isSplatLike(C));\n-\n-    int numRes = getMmaInstrShape()[0] * getMmaInstrShape()[1] / 32;\n-    if (auto constv = llvm::dyn_cast<arith::ConstantOp>(C.getDefiningOp())) {\n-      if (auto attr = constv.getValue().dyn_cast<SplatElementsAttr>()) {\n-        Type elemType = attr.getElementType();\n-        if (elemType.isInteger(32)) {\n-          int v = attr.getSplatValue<int>();\n-          return SmallVector<Value>(numRes, i32_val(v));\n-        } else if (elemType.isInteger(8)) {\n-          int v = attr.getSplatValue<int8_t>();\n-          auto newv = rewriter.create<arith::ConstantOp>(\n-              loc, elemType, IntegerAttr::get(elemType, v));\n-          return SmallVector<Value>(numRes, newv);\n-        } else if (elemType.isF32()) {\n-          int v = attr.getSplatValue<float>();\n-          auto newv = rewriter.create<arith::ConstantOp>(\n-              loc, elemType, FloatAttr::get(elemType, v));\n-          return SmallVector<Value>(numRes, newv);\n-        }\n-      }\n-    }\n-\n-    assert(false && \"Not supported type.\");\n-    return {};\n-  }\n-\n   void deduceMmaType(DotOp op) const { mmaType = getMmaType(op); }\n   void deduceMmaType(Type operandTy) const {\n     mmaType = getTensorCoreTypeFromOperand(operandTy);\n   }\n \n   // Get the M and N of mat instruction shape.\n   static std::tuple<int, int> getMatShapeMN() {\n-    // According to DotOpConversionHelper::mmaMatShape, all the matrix shape's\n-    // M,N are {8,8}\n+    // According to DotOpMmaV2ConversionHelper::mmaMatShape, all the matrix\n+    // shape's M,N are {8,8}\n     return {8, 8};\n   }\n \n@@ -3289,7 +3374,7 @@ struct MMA16816ConversionHelper {\n \n   Value thread, lane, warp, warpMN, warpN, warpM;\n \n-  DotOpConversionHelper helper;\n+  DotOpMmaV2ConversionHelper helper;\n   ConversionPatternRewriter &rewriter;\n   TypeConverter *typeConverter;\n   Location loc;\n@@ -3349,22 +3434,25 @@ struct MMA16816ConversionHelper {\n \n   static int getNumRepM(Type operand, int M, int wpt) {\n     auto tensorCoreType =\n-        DotOpConversionHelper::getTensorCoreTypeFromOperand(operand);\n-    int mmaInstrM = DotOpConversionHelper::getMmaInstrShape(tensorCoreType)[0];\n+        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n+    int mmaInstrM =\n+        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[0];\n     return std::max<int>(M / (wpt * mmaInstrM), 1);\n   }\n \n   static int getNumRepN(Type operand, int N, int wpt) {\n     auto tensorCoreType =\n-        DotOpConversionHelper::getTensorCoreTypeFromOperand(operand);\n-    int mmaInstrN = DotOpConversionHelper::getMmaInstrShape(tensorCoreType)[1];\n+        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n+    int mmaInstrN =\n+        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[1];\n     return std::max<int>(N / (wpt * mmaInstrN), 1);\n   }\n \n   static int getNumRepK_(Type operand, int K) {\n     auto tensorCoreType =\n-        DotOpConversionHelper::getTensorCoreTypeFromOperand(operand);\n-    int mmaInstrK = DotOpConversionHelper::getMmaInstrShape(tensorCoreType)[2];\n+        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n+    int mmaInstrK =\n+        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[2];\n     return std::max<int>(K / mmaInstrK, 1);\n   }\n \n@@ -3450,7 +3538,7 @@ struct MMA16816ConversionHelper {\n   // Loading $c to registers, returns a Value.\n   Value loadC(Value tensor, Value llTensor) const {\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto [repM, repN] = DotOpConversionHelper::getRepMN(tensorTy);\n+    auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(tensorTy);\n     size_t fcSize = 4 * repM * repN;\n \n     assert(tensorTy.getEncoding().isa<MmaEncodingAttr>() &&\n@@ -3517,7 +3605,7 @@ struct MMA16816ConversionHelper {\n         return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n       };\n \n-      for (int i = 0; i < 4; i++)\n+      for (int i = 0; i < 4; ++i)\n         fc[m * colsPerThread + 4 * n + i] =\n             extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(i));\n     };\n@@ -3573,7 +3661,7 @@ struct MMA16816ConversionHelper {\n       Type smemPtrTy = helper.getShemPtrTy();\n       for (int i = 0; i < numPtrs; ++i) {\n         ptrs[i] =\n-            bitcast(smemPtrTy, gep(smemPtrTy, llTensor, ValueRange({offs[i]})));\n+            bitcast(gep(smemPtrTy, llTensor, ValueRange({offs[i]})), smemPtrTy);\n       }\n \n       auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n@@ -3638,7 +3726,7 @@ struct MMA16816ConversionHelper {\n \n     int offset{};\n     ValueTable vals;\n-    for (int i = 0; i < n0; i++) {\n+    for (int i = 0; i < n0; ++i) {\n       for (int j = 0; j < n1; j++) {\n         vals[{2 * i, 2 * j}] = elems[offset++];\n         vals[{2 * i, 2 * j + 1}] = elems[offset++];\n@@ -3660,20 +3748,37 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n \n   auto dotOperandLayout =\n       dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+\n   MmaEncodingAttr mmaLayout =\n       dotOperandLayout.getParent().dyn_cast_or_null<MmaEncodingAttr>();\n   assert(mmaLayout);\n \n-  MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n-                                     rewriter, getTypeConverter(), op.getLoc());\n-\n   Value res;\n-  if (dotOperandLayout.getOpIdx() == 0) {\n-    // operand $a\n-    res = mmaHelper.loadA(src, adaptor.src());\n-  } else if (dotOperandLayout.getOpIdx() == 1) {\n-    // operand $b\n-    res = mmaHelper.loadB(src, adaptor.src());\n+  if (mmaLayout.getVersion() == 2) {\n+    MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n+                                       rewriter, getTypeConverter(),\n+                                       op.getLoc());\n+\n+    if (dotOperandLayout.getOpIdx() == 0) {\n+      // operand $a\n+      res = mmaHelper.loadA(src, adaptor.src());\n+    } else if (dotOperandLayout.getOpIdx() == 1) {\n+      // operand $b\n+      res = mmaHelper.loadB(src, adaptor.src());\n+    }\n+  } else if (mmaLayout.getVersion() == 1) {\n+    DotOpMmaV1ConversionHelper helper(mmaLayout);\n+    if (dotOperandLayout.getOpIdx() == 0) {\n+      // operand $a\n+      res = helper.loadA(src, adaptor.src(), getThreadId(rewriter, loc),\n+                         adaptor.src(), loc, rewriter);\n+    } else if (dotOperandLayout.getOpIdx() == 1) {\n+      // operand $b\n+      res = helper.loadB(src, adaptor.src(), getThreadId(rewriter, loc),\n+                         adaptor.src(), loc, rewriter);\n+    }\n+  } else {\n+    assert(false && \"Unsupported mma layout found\");\n   }\n \n   rewriter.replaceOp(op, res);\n@@ -3717,6 +3822,424 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adaptor,\n                               adaptor);\n }\n \n+// Simply port the old code here to avoid large difference and make debugging\n+// and profiling easier.\n+LogicalResult\n+DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n+                               ConversionPatternRewriter &rewriter) const {\n+  auto *ctx = op.getContext();\n+  auto loc = op.getLoc();\n+\n+  Value A = op.a();\n+  Value B = op.b();\n+  Value D = op.getResult();\n+  auto mmaLayout = D.getType()\n+                       .cast<RankedTensorType>()\n+                       .getEncoding()\n+                       .cast<MmaEncodingAttr>();\n+\n+  auto ATensorTy = A.getType().cast<RankedTensorType>();\n+  auto BTensorTy = B.getType().cast<RankedTensorType>();\n+  auto DTensorTy = D.getType().cast<RankedTensorType>();\n+  auto AShape = ATensorTy.getShape();\n+  auto BShape = BTensorTy.getShape();\n+  auto DShape = DTensorTy.getShape();\n+  auto wpt = mmaLayout.getWarpsPerCTA();\n+\n+  bool transA = op.transA();\n+  bool transB = op.transB();\n+\n+  bool isARow = !transA;\n+  bool isBRow = !transB;\n+  bool isAVec4 = !isARow && AShape[isARow] <= 16; // fp16*4 = 16bytes\n+  bool isBVec4 = isBRow && BShape[isBRow] <= 16;\n+  int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+  int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+  SmallVector<int> fpw({2, 2, 1});\n+  SmallVector<int> rep({2 * packSize0, 2 * packSize1, 1});\n+  SmallVector<int> spw({fpw[0] * 4 * rep[0], fpw[1] * 4 * rep[1], 1});\n+\n+  Value loadedA = adaptor.a();\n+  Value loadedB = adaptor.b();\n+  Value loadedC = adaptor.c();\n+  DotOpMmaV1ConversionHelper helper(mmaLayout);\n+\n+  unsigned numM = rep[0] * DShape[0] / (spw[0] * wpt[0]);\n+  unsigned numN = rep[1] * DShape[1] / (spw[1] * wpt[0]);\n+  unsigned NK = AShape[1];\n+\n+  auto has = helper.extractLoadedOperand(loadedA, numM / 2, NK, rewriter);\n+  auto hbs = helper.extractLoadedOperand(loadedB, numN / 2, NK, rewriter);\n+\n+  size_t accSize = numM * numN;\n+\n+  // initialize accumulators\n+  SmallVector<Value> acc = getElementsFromStruct(loc, loadedC, rewriter);\n+\n+  auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n+    auto ha = has[{m, k}];\n+    auto hb = hbs[{n, k}];\n+    std::vector<size_t> idx{{\n+        (m * 2 + 0) + (n * 4 + 0) * numM, // row0\n+        (m * 2 + 0) + (n * 4 + 1) * numM,\n+        (m * 2 + 1) + (n * 4 + 0) * numM, // row1\n+        (m * 2 + 1) + (n * 4 + 1) * numM,\n+        (m * 2 + 0) + (n * 4 + 2) * numM, // row2\n+        (m * 2 + 0) + (n * 4 + 3) * numM,\n+        (m * 2 + 1) + (n * 4 + 2) * numM, // row3\n+        (m * 2 + 1) + (n * 4 + 3) * numM,\n+    }};\n+\n+    PTXBuilder builder;\n+\n+    auto *resOprs = builder.newListOperand(8, \"=f\");\n+    auto *AOprs = builder.newListOperand({\n+        {ha.first, \"f\"},\n+        {ha.second, \"f\"},\n+    });\n+\n+    auto *BOprs = builder.newListOperand({\n+        {hb.first, \"f\"},\n+        {hb.second, \"f\"},\n+    });\n+    auto *COprs = builder.newListOperand();\n+    for (int i = 0; i < 8; ++i)\n+      COprs->listAppend(builder.newOperand(acc[idx[i]], std::to_string(i)));\n+\n+    auto mma = builder.create(\"mma.sync.aligned.m8n8k4\")\n+                   ->o(isARow ? \"row\" : \"col\")\n+                   .o(isBRow ? \"row\" : \"col\")\n+                   .o(\".f32.f16.f16.f32\");\n+\n+    mma(resOprs, AOprs, BOprs, COprs);\n+\n+    Value res = builder.launch(rewriter, loc, helper.getMmaRetType(ATensorTy));\n+\n+    auto getIntAttr = [&](int v) {\n+      return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n+    };\n+    for (unsigned i = 0; i < 8; i++)\n+      acc[idx[i]] = extract_val(f32_ty, res, getIntAttr(i));\n+  };\n+\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      for (unsigned n = 0; n < numN / 2; ++n) {\n+        callMMA(m, n, k);\n+      }\n+\n+  // replace with new packed result\n+  Type structTy = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(acc.size(), type::f32Ty(ctx)));\n+  Value res = getStructFromElements(loc, acc, rewriter, structTy);\n+  rewriter.replaceOp(op, res);\n+\n+  return success();\n+}\n+\n+Value DotOpMmaV1ConversionHelper::loadA(\n+    Value tensor, Value llTensor, Value thread, Value smem, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto shape = tensorTy.getShape();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto order = sharedLayout.getOrder();\n+\n+  bool isARow = order[0] != 0;\n+  bool isAVec4 = !isARow && shape[order[0]] <= 16; // fp16*4 = 16bytes\n+  int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+\n+  SmallVector<int> fpw({2, 2, 1});\n+  int repM = 2 * packSize0;\n+  int repK = 1;\n+  int spwM = fpw[0] * 4 * repM;\n+  SmallVector<int> rep({repM, 0, repK}); // pad N with 0\n+  SmallVector<int> spw({spwM, 0, 1});    // pad N with 0\n+\n+  int vecA = sharedLayout.getVec();\n+\n+  int strideAM = isARow ? shape[1] : 1;\n+  int strideAK = isARow ? 1 : shape[0];\n+  int strideA0 = isARow ? strideAK : strideAM;\n+  int strideA1 = isARow ? strideAM : strideAK;\n+\n+  int strideRepM = wpt[0] * fpw[0] * 8;\n+  int strideRepK = 1;\n+\n+  auto [offsetAM, offsetAK, _0, _1] =\n+      computeOffsets(thread, isARow, false, fpw, spw, rep, rewriter, loc);\n+\n+  // swizzling\n+  int perPhaseA = sharedLayout.getPerPhase();\n+  int maxPhaseA = sharedLayout.getMaxPhase();\n+  int stepA0 = isARow ? strideRepK : strideRepM;\n+  int numPtrA = std::max(2 * perPhaseA * maxPhaseA / stepA0, 1);\n+  int NK = shape[1];\n+\n+  // pre-compute pointer lanes\n+  Value offA0 = isARow ? offsetAK : offsetAM;\n+  Value offA1 = isARow ? offsetAM : offsetAK;\n+  Value phaseA = urem(udiv(offA1, i32_val(perPhaseA)), i32_val(maxPhaseA));\n+  SmallVector<Value> offA(numPtrA);\n+\n+  for (int i = 0; i < numPtrA; i++) {\n+    Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));\n+    offA0I = udiv(offA0I, i32_val(vecA));\n+    offA0I = xor_(offA0I, phaseA);\n+    offA0I = xor_(offA0I, i32_val(vecA));\n+    offA[i] =\n+        add(mul(offA0I, i32_val(strideA0)), mul(offA1, i32_val(strideA1)));\n+  }\n+\n+  Type f16x2Ty = vec_ty(f16_ty, 2);\n+  // One thread get 8 elements as result\n+  Type retTy =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector(8, type::f32Ty(ctx)));\n+\n+  // prepare arguments\n+  SmallVector<Value> ptrA(numPtrA);\n+\n+  std::map<std::pair<int, int>, std::pair<Value, Value>> has;\n+  for (int i = 0; i < numPtrA; i++)\n+    ptrA[i] = gep(ptr_ty(f16_ty), smem, offA[i]);\n+\n+  auto instrShape = getMmaInstrShape();\n+  unsigned numM = rep[0] * shape[0] / (spw[0] * wpt[0]);\n+\n+  Type f16PtrTy = ptr_ty(f16_ty);\n+\n+  auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+  auto loadA = [&](int m, int k) {\n+    int offidx = (isARow ? k / 4 : m) % numPtrA;\n+    Value thePtrA = gep(f16PtrTy, smem, offA[offidx]);\n+\n+    int stepAM = isARow ? m : m / numPtrA * numPtrA;\n+    int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n+    Value pa = gep(f16PtrTy, thePtrA,\n+                   i32_val(stepAM * strideRepM * strideAM + stepAK * strideAK));\n+    Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n+    Value ha = load(bitcast(pa, aPtrTy));\n+    // record lds that needs to be moved\n+    Value ha00 = bitcast(extract_element(ha, i32_val(0)), f16x2Ty);\n+    Value ha01 = bitcast(extract_element(ha, i32_val(1)), f16x2Ty);\n+    ld(has, m, k, ha00, ha01);\n+\n+    if (vecA > 4) {\n+      Value ha10 = bitcast(extract_element(ha, i32_val(2)), f16x2Ty);\n+      Value ha11 = bitcast(extract_element(ha, i32_val(3)), f16x2Ty);\n+      if (isARow)\n+        ld(has, m, k + 4, ha10, ha11);\n+      else\n+        ld(has, m + 1, k, ha10, ha11);\n+    }\n+  };\n+\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      if (!has.count({m, k}))\n+        loadA(m, k);\n+\n+  SmallVector<Value> elems;\n+  elems.reserve(has.size() * 2);\n+  auto vecTy = vec_ty(f16_ty, 2);\n+  for (auto item : has) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+\n+  Type resTy = struct_ty(SmallVector<Type>(elems.size(), f16x2Ty));\n+  Value res = getStructFromElements(loc, elems, rewriter, resTy);\n+  return res;\n+}\n+\n+Value DotOpMmaV1ConversionHelper::loadB(\n+    Value tensor, Value llTensor, Value thread, Value smem, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto shape = tensorTy.getShape();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto order = sharedLayout.getOrder();\n+  bool isBRow = order[0] != 0;\n+  bool isBVec4 = isBRow && shape[order[0]] <= 16;\n+  int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+  SmallVector<int> fpw({2, 2, 1});\n+  SmallVector<int> rep({0, 2 * packSize1, 1});       // pad M with 0\n+  SmallVector<int> spw({0, fpw[1] * 4 * rep[1], 1}); // pad M with 0\n+  int vecB = sharedLayout.getVec();\n+  int strideBN = isBRow ? 1 : shape[0];\n+  int strideBK = isBRow ? shape[1] : 1;\n+  int strideB0 = isBRow ? strideBN : strideBK;\n+  int strideB1 = isBRow ? strideBK : strideBN;\n+  int strideRepN = wpt[1] * fpw[1] * 8;\n+  int strideRepK = 1;\n+\n+  // swizzling\n+  int perPhaseA = sharedLayout.getPerPhase();\n+  int maxPhaseA = sharedLayout.getMaxPhase();\n+  int perPhaseB = sharedLayout.getPerPhase();\n+  int maxPhaseB = sharedLayout.getMaxPhase();\n+  int stepB0 = isBRow ? strideRepN : strideRepK;\n+  int numPtrB = std::max(2 * perPhaseB * maxPhaseB / stepB0, 1);\n+  int NK = shape[0];\n+\n+  auto [_0, _1, offsetBN, offsetBK] =\n+      computeOffsets(thread, false, isBRow, fpw, spw, rep, rewriter, loc);\n+\n+  Value offB0 = isBRow ? offsetBN : offsetBK;\n+  Value offB1 = isBRow ? offsetBK : offsetBN;\n+  Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n+  SmallVector<Value> offB(numPtrB);\n+  for (int i = 0; i < numPtrB; ++i) {\n+    Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));\n+    offB0I = udiv(offB0I, i32_val(vecB));\n+    offB0I = xor_(offB0I, phaseB);\n+    offB0I = mul(offB0I, i32_val(vecB));\n+    offB[i] =\n+        add(mul(offB0I, i32_val(strideB0)), mul(offB1, i32_val(strideB1)));\n+  }\n+\n+  Type f16PtrTy = ptr_ty(f16_ty);\n+  Type f16x2Ty = vec_ty(f16_ty, 2);\n+\n+  SmallVector<Value> ptrB(numPtrB);\n+  ValueTable hbs;\n+  for (int i = 0; i < numPtrB; ++i)\n+    ptrB[i] = gep(ptr_ty(f16_ty), smem, offB[i]);\n+\n+  auto ld = [&](decltype(hbs) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+\n+  auto loadB = [&](int n, int K) {\n+    int offidx = (isBRow ? n : K / 4) % numPtrB;\n+    Value thePtrB = ptrB[offidx];\n+\n+    int stepBN = isBRow ? n / numPtrB * numPtrB : n;\n+    int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n+    Value pb = gep(f16PtrTy, thePtrB,\n+                   i32_val(stepBN * strideRepN * strideBN + stepBK * strideBK));\n+    Value hb =\n+        load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n+    // record lds that needs to be moved\n+    Value hb00 = bitcast(extract_element(hb, i32_val(0)), f16x2Ty);\n+    Value hb01 = bitcast(extract_element(hb, i32_val(1)), f16x2Ty);\n+    ld(hbs, n, K, hb00, hb01);\n+    if (vecB > 4) {\n+      Value hb10 = bitcast(extract_element(hb, i32_val(2)), f16x2Ty);\n+      Value hb11 = bitcast(extract_element(hb, i32_val(3)), f16x2Ty);\n+      if (isBRow)\n+        ld(hbs, n + 1, K, hb10, hb11);\n+      else\n+        ld(hbs, n, K + 4, hb10, hb11);\n+    }\n+  };\n+\n+  unsigned numN = rep[1] * shape[1] / (spw[1] * wpt[0]);\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned n = 0; n < numN / 2; ++n) {\n+      if (!hbs.count({n, k}))\n+        loadB(n, k);\n+    }\n+\n+  SmallVector<Value> elems;\n+  for (auto &item : hbs) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+  Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n+  Type resTy = struct_ty(SmallVector<Type>(elems.size(), fp16x2Ty));\n+  Value res = getStructFromElements(loc, elems, rewriter, resTy);\n+  return res;\n+}\n+\n+Value DotOpMmaV1ConversionHelper::loadC(\n+    Value tensor, Value llTensor, ConversionPatternRewriter &rewriter) const {\n+  return llTensor;\n+}\n+\n+std::tuple<Value, Value, Value, Value>\n+DotOpMmaV1ConversionHelper::computeOffsets(Value threadId, bool isARow,\n+                                           bool isBRow, ArrayRef<int> fpw,\n+                                           ArrayRef<int> spw, ArrayRef<int> rep,\n+                                           ConversionPatternRewriter &rewriter,\n+                                           Location loc) const {\n+  auto *ctx = rewriter.getContext();\n+  Value _1 = i32_val(1);\n+  Value _3 = i32_val(3);\n+  Value _4 = i32_val(4);\n+  Value _16 = i32_val(16);\n+  Value _32 = i32_val(32);\n+\n+  Value lane = urem(threadId, _32);\n+  Value warp = udiv(threadId, _32);\n+\n+  // warp offset\n+  Value warp0 = urem(warp, i32_val(wpt[0]));\n+  Value warp12 = udiv(warp, i32_val(wpt[0]));\n+  Value warp1 = urem(warp12, i32_val(wpt[1]));\n+  Value warpMOff = mul(warp0, i32_val(spw[0]));\n+  Value warpNOff = mul(warp1, i32_val(spw[1]));\n+  // Quad offset\n+  Value quadMOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[0]));\n+  Value quadNOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[1]));\n+  // Pair offset\n+  Value pairMOff = udiv(urem(lane, _16), _4);\n+  pairMOff = urem(pairMOff, i32_val(fpw[0]));\n+  pairMOff = mul(pairMOff, _4);\n+  Value pairNOff = udiv(urem(lane, _16), _4);\n+  pairNOff = udiv(pairNOff, i32_val(fpw[0]));\n+  pairNOff = urem(pairNOff, i32_val(fpw[1]));\n+  pairNOff = mul(pairNOff, _4);\n+  // scale\n+  pairMOff = mul(pairMOff, i32_val(rep[0] / 2));\n+  quadMOff = mul(quadMOff, i32_val(rep[0] / 2));\n+  pairNOff = mul(pairNOff, i32_val(rep[1] / 2));\n+  quadNOff = mul(quadNOff, i32_val(rep[1] / 2));\n+  // Quad pair offset\n+  Value laneMOff = add(pairMOff, quadMOff);\n+  Value laneNOff = add(pairNOff, quadNOff);\n+  // A offset\n+  Value offsetAM = add(warpMOff, laneMOff);\n+  Value offsetAK = and_(lane, _3);\n+  // B offset\n+  Value offsetBN = add(warpNOff, laneNOff);\n+  Value offsetBK = and_(lane, _3);\n+  // i indices\n+  Value offsetCM = add(and_(lane, _1), offsetAM);\n+  if (isARow) {\n+    offsetAM = add(offsetAM, urem(threadId, _4));\n+    offsetAK = i32_val(0);\n+  }\n+  if (!isBRow) {\n+    offsetBN = add(offsetBN, urem(threadId, _4));\n+    offsetBK = i32_val(0);\n+  }\n+\n+  return std::make_tuple(offsetAM, offsetAK, offsetBN, offsetBK);\n+}\n+\n+DotOpMmaV1ConversionHelper::ValueTable\n+DotOpMmaV1ConversionHelper::extractLoadedOperand(\n+    Value llStruct, int n0, int n1, ConversionPatternRewriter &rewriter) const {\n+  ValueTable rcds;\n+  SmallVector<Value> elems =\n+      ConvertTritonGPUOpToLLVMPatternBase::getElementsFromStruct(\n+          llStruct.getLoc(), llStruct, rewriter);\n+\n+  int offset = 0;\n+  for (int i = 0; i < n0; ++i)\n+    for (int k = 0; k < n1; k += 4) {\n+      rcds[{i, k}] = std::make_pair(elems[offset], elems[offset + 1]);\n+      offset += 2;\n+    }\n+\n+  return rcds;\n+}\n+\n /// ====================== mma codegen end ============================\n \n Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,\n@@ -3725,16 +4248,29 @@ Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,\n                                       TypeConverter *typeConverter,\n                                       ConversionPatternRewriter &rewriter,\n                                       Location loc) {\n+  auto tensorTy = resType.cast<RankedTensorType>();\n+  auto shape = tensorTy.getShape();\n   if (layout.getVersion() == 2) {\n-    auto tensorTy = resType.cast<RankedTensorType>();\n-    auto [repM, repN] = DotOpConversionHelper::getRepMN(tensorTy);\n+    auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(tensorTy);\n     size_t fcSize = 4 * repM * repN;\n \n     auto structTy = LLVM::LLVMStructType::getLiteral(\n         rewriter.getContext(), SmallVector<Type>(fcSize, elemType));\n     return getStructFromElements(loc, SmallVector<Value>(fcSize, constVal),\n                                  rewriter, structTy);\n   }\n+  if (layout.getVersion() == 1) {\n+    DotOpMmaV1ConversionHelper helper(layout);\n+    int repM = helper.getRepM(shape[0]);\n+    int repN = helper.getRepN(shape[1]);\n+    // According to mma layout of v1, each thread process 8 elements.\n+    int elems = 8 * repM * repN;\n+\n+    auto structTy = LLVM::LLVMStructType::getLiteral(\n+        rewriter.getContext(), SmallVector<Type>(elems, elemType));\n+    return getStructFromElements(loc, SmallVector<Value>(elems, constVal),\n+                                 rewriter, structTy);\n+  }\n \n   assert(false && \"Unsupported mma layout found\");\n }\n@@ -3766,6 +4302,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n   llvm::Optional<Type> convertTritonTensorType(RankedTensorType type) {\n     auto ctx = type.getContext();\n     Attribute layout = type.getEncoding();\n+    auto shape = type.getShape();\n     if (layout &&\n         (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n          layout.isa<MmaEncodingAttr>())) {\n@@ -3778,22 +4315,31 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n       return LLVM::LLVMPointerType::get(convertType(type.getElementType()), 3);\n     } else if (auto mmaLayout = layout.dyn_cast_or_null<MmaEncodingAttr>()) {\n       if (mmaLayout.getVersion() == 2) {\n-        auto [repM, repN] = DotOpConversionHelper::getRepMN(type);\n+        auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(type);\n         size_t fcSize = 4 * repM * repN;\n         return LLVM::LLVMStructType::getLiteral(\n             ctx, SmallVector<Type>(fcSize, type.getElementType()));\n       }\n \n+      if (mmaLayout.getVersion() == 1) {\n+        DotOpMmaV1ConversionHelper helper(mmaLayout);\n+        int repM = helper.getRepM(shape[0]);\n+        int repN = helper.getRepN(shape[1]);\n+        int elems = 8 * repM * repN;\n+        return LLVM::LLVMStructType::getLiteral(\n+            ctx, SmallVector<Type>(elems, type.getElementType()));\n+      }\n+\n       llvm::errs()\n           << \"Unexpected mma layout detected in TritonToLLVMTypeConverter\";\n       return llvm::None;\n \n     } else if (auto dot_op_layout =\n                    layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n       auto mmaLayout = dot_op_layout.getParent().cast<MmaEncodingAttr>();\n+      auto wpt = mmaLayout.getWarpsPerCTA();\n+      Type elemTy = type.getElementType();\n       if (mmaLayout.getVersion() == 2) {\n-        auto wpt = mmaLayout.getWarpsPerCTA();\n-        Type elemTy = type.getElementType();\n \n         if (dot_op_layout.getOpIdx() == 0) { // $a\n           int elems =\n@@ -3806,8 +4352,22 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n           int elems =\n               MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt);\n           Type x2Ty = vec_ty(elemTy, 2);\n-          return LLVM::LLVMStructType::getLiteral(\n-              ctx, SmallVector<Type>(elems, x2Ty));\n+          return struct_ty(SmallVector<Type>(elems, x2Ty));\n+        }\n+      }\n+\n+      if (mmaLayout.getVersion() == 1) {\n+        DotOpMmaV1ConversionHelper helper(mmaLayout);\n+\n+        if (dot_op_layout.getOpIdx() == 0) { // $a\n+          int elems = helper.numElemsPerThreadA(type);\n+          Type x2Ty = vec_ty(elemTy, 2);\n+          return struct_ty(SmallVector<Type>(elems, x2Ty));\n+        }\n+        if (dot_op_layout.getOpIdx() == 1) { // $b\n+          int elems = helper.numElemsPerThreadB(type);\n+          Type x2Ty = vec_ty(elemTy, 2);\n+          return struct_ty(SmallVector<Type>(elems, x2Ty));\n         }\n       }\n "}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 17, "deletions": 4, "changes": 21, "file_content_changes": "@@ -270,10 +270,23 @@ unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   size_t rank = shape.size();\n   assert(rank == 2 && \"Unexpected rank of mma layout\");\n-  assert(getVersion() == 2 && \"mmaLayout version = 1 is not implemented yet\");\n-  unsigned elemsCol = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]) * 2;\n-  unsigned elemsRow = ceil<unsigned>(shape[1], 8 * getWarpsPerCTA()[1]) * 2;\n-  return elemsCol * elemsRow;\n+  assert((getVersion() == 1 || getVersion() == 2) &&\n+         \"Only version 1 and 2 is supported\");\n+\n+  int res = 0;\n+  if (getVersion() == 1) {\n+    unsigned mmasRow = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]);\n+    unsigned mmasCol = ceil<unsigned>(shape[1], 16 * getWarpsPerCTA()[1]);\n+    // Each warp-level mma884 will perform a m16xn16xk4 mma, thus get a m16xn16\n+    // matrix as result.\n+    res = mmasRow * mmasCol * (16 * 16 / 32);\n+  } else if (getVersion() == 2) {\n+    unsigned elemsCol = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]) * 2;\n+    unsigned elemsRow = ceil<unsigned>(shape[1], 8 * getWarpsPerCTA()[1]) * 2;\n+    res = elemsCol * elemsRow;\n+  }\n+\n+  return res;\n }\n \n unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -87,8 +87,16 @@ void init_triton_ir(py::module &&m) {\n       .value(\"FADD\", mlir::triton::RedOp::FADD)\n       .value(\"MIN\", mlir::triton::RedOp::MIN)\n       .value(\"MAX\", mlir::triton::RedOp::MAX)\n+      .value(\"UMIN\", mlir::triton::RedOp::UMIN)\n+      .value(\"UMAX\", mlir::triton::RedOp::UMAX)\n+      .value(\"ARGMIN\", mlir::triton::RedOp::ARGMIN)\n+      .value(\"ARGMAX\", mlir::triton::RedOp::ARGMAX)\n+      .value(\"ARGUMIN\", mlir::triton::RedOp::ARGUMIN)\n+      .value(\"ARGUMAX\", mlir::triton::RedOp::ARGUMAX)\n       .value(\"FMIN\", mlir::triton::RedOp::FMIN)\n       .value(\"FMAX\", mlir::triton::RedOp::FMAX)\n+      .value(\"ARGFMIN\", mlir::triton::RedOp::ARGFMIN)\n+      .value(\"ARGFMAX\", mlir::triton::RedOp::ARGFMAX)\n       .value(\"XOR\", mlir::triton::RedOp::XOR);\n \n   py::enum_<mlir::triton::RMWOp>(m, \"ATOMIC_OP\")"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 133, "deletions": 112, "changes": 245, "file_content_changes": "@@ -870,121 +870,142 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n # # ---------------\n \n \n-# @pytest.mark.parametrize(\"op, dtype_str, shape\",\n-#                          [(op, dtype, shape)\n-#                           for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n-#                           for dtype in dtypes_with_bfloat16\n-#                           for shape in [32, 64, 128, 512]])\n-# def test_reduce1d(op, dtype_str, shape, device='cuda'):\n-#     check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+def get_reduced_dtype(dtype_str, op):\n+    if op == 'argmin' or op == 'argmax':\n+        return 'int32'\n+    if dtype_str in ['int8', 'uint8', 'int16', 'uint16']:\n+        return 'int32'\n+    if dtype_str == 'bfloat16':\n+        return 'float32'\n+    return dtype_str\n+\n+\n+# TODO: [Qingyi] Fix argmin / argmax\n+@pytest.mark.parametrize(\"op, dtype_str, shape\",\n+                         [(op, dtype, shape)\n+                          for op in ['min', 'max', 'sum']\n+                          for dtype in dtypes_with_bfloat16\n+                          for shape in [32, 64, 128, 512]])\n+def test_reduce1d(op, dtype_str, shape, device='cuda'):\n+    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n \n-#     # triton kernel\n-#     @triton.jit\n-#     def kernel(X, Z, BLOCK: tl.constexpr):\n-#         x = tl.load(X + tl.arange(0, BLOCK))\n-#         tl.store(Z, GENERATE_TEST_HERE)\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, Z, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        tl.store(Z, GENERATE_TEST_HERE)\n \n-#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=0)'})\n-#     # input\n-#     rs = RandomState(17)\n-#     # limit the range of integers so that the sum does not overflow\n-#     x = numpy_random((shape,), dtype_str=dtype_str, rs=rs)\n-#     x_tri = to_triton(x, device=device)\n-#     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n-#                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n-#     # numpy result\n-#     z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n-#     z_tri_dtype_str = z_dtype_str\n-#     if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n-#         z_dtype_str = 'float32'\n-#         z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n-#         # trunc mantissa for a fair comparison of accuracy\n-#         z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n-#         z_tri_dtype_str = 'bfloat16'\n-#     else:\n-#         z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n-#     # triton result\n-#     z_tri = to_triton(numpy_random((1,), dtype_str=z_dtype_str, rs=rs),\n-#                       device=device, dst_type=z_tri_dtype_str)\n-#     kernel[(1,)](x_tri, z_tri, BLOCK=shape)\n-#     z_tri = to_numpy(z_tri)\n-#     # compare\n-#     if op == 'sum':\n-#         np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n-#     else:\n-#         if op == 'argmin' or op == 'argmax':\n-#             # argmin and argmax can have multiple valid indices.\n-#             # so instead we compare the values pointed by indices\n-#             np.testing.assert_equal(x[z_ref], x[z_tri])\n-#         else:\n-#             np.testing.assert_equal(z_ref, z_tri)\n-\n-\n-# reduce_configs1 = [\n-#     (op, dtype, (1, 1024), axis) for dtype in dtypes_with_bfloat16\n-#     for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n-#     for axis in [1]\n-# ]\n-# reduce_configs2 = [\n-#     (op, 'float32', shape, axis)\n-#     for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n-#     for shape in [(2, 32), (4, 32), (4, 128), (32, 64), (64, 128), (128, 256), (32, 1024)]\n-#     for axis in [0, 1]\n-# ]\n-\n-\n-# @pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n-# def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n-#     # triton kernel\n-#     @triton.jit\n-#     def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n-#         range_m = tl.arange(0, BLOCK_M)\n-#         range_n = tl.arange(0, BLOCK_N)\n-#         x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n-#         z = GENERATE_TEST_HERE\n-#         if AXIS == 1:\n-#             tl.store(Z + range_m, z)\n-#         else:\n-#             tl.store(Z + range_n, z)\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=0)'})\n+    # input\n+    rs = RandomState(17)\n+    # limit the range of integers so that the sum does not overflow\n+    x = numpy_random((shape,), dtype_str=dtype_str, rs=rs)\n+    x_tri = to_triton(x, device=device)\n+    numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n+                'argmin': np.argmin, 'argmax': np.argmax}[op]\n+    # numpy result\n+    z_dtype_str = get_reduced_dtype(dtype_str, op)\n+    z_tri_dtype_str = z_dtype_str\n+    if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n+        z_dtype_str = 'float32'\n+        z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n+        # trunc mantissa for a fair comparison of accuracy\n+        z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n+        z_tri_dtype_str = 'bfloat16'\n+    else:\n+        z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n+    # triton result\n+    z_tri = to_triton(numpy_random((1,), dtype_str=z_dtype_str, rs=rs),\n+                      device=device, dst_type=z_tri_dtype_str)\n+    kernel[(1,)](x_tri, z_tri, BLOCK=shape)\n+    z_tri = to_numpy(z_tri)\n+    # compare\n+    if op == 'sum':\n+        np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n+    else:\n+        if op == 'argmin' or op == 'argmax':\n+            # argmin and argmax can have multiple valid indices.\n+            # so instead we compare the values pointed by indices\n+            np.testing.assert_equal(x[z_ref], x[z_tri])\n+        else:\n+            np.testing.assert_equal(z_ref, z_tri)\n \n-#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=AXIS)'})\n-#     # input\n-#     rs = RandomState(17)\n-#     # limit the range of integers so that the sum does not overflow\n-#     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n-#     x_tri = to_triton(x)\n-#     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n-#                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n-#     z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n-#     z_tri_dtype_str = z_dtype_str\n-#     # numpy result\n-#     if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n-#         z_dtype_str = 'float32'\n-#         z_tri_dtype_str = 'bfloat16'\n-#         z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n-#         # trunc mantissa for a fair comparison of accuracy\n-#         z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n-#     else:\n-#         z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n-#     # triton result\n-#     z_tri = to_triton(numpy_random((shape[1 - axis],), dtype_str=z_dtype_str, rs=rs),\n-#                       device=device, dst_type=z_tri_dtype_str)\n-#     kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n-#     z_tri = to_numpy(z_tri)\n-#     # compare\n-#     if op == 'sum':\n-#         np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n-#     else:\n-#         if op == 'argmin' or op == 'argmax':\n-#             # argmin and argmax can have multiple valid indices.\n-#             # so instead we compare the values pointed by indices\n-#             z_ref_index = np.expand_dims(z_ref, axis=axis)\n-#             z_tri_index = np.expand_dims(z_tri, axis=axis)\n-#             z_ref_value = np.take_along_axis(x, z_ref_index, axis=axis)\n-#             z_tri_value = np.take_along_axis(x, z_tri_index, axis=axis)\n-#             np.testing.assert_equal(z_ref_value, z_tri_value)\n-#         else:\n-#             np.testing.assert_equal(z_ref, z_tri)\n+\n+# TODO: [Qingyi] Fix argmin / argmax\n+reduce_configs1 = [\n+    (op, dtype, (1, 1024), axis) for dtype in dtypes_with_bfloat16\n+    for op in ['min', 'max', 'sum']\n+    for axis in [1]\n+]\n+\n+\n+# shape (128, 256) and (32, 1024) are not enabled on sm86 because the required shared memory\n+# exceeds the limit of 99KB\n+reduce2d_shapes = [(2, 32), (4, 32), (4, 128), (32, 64), (64, 128)]\n+if 'V100' in torch.cuda.get_device_name(0):\n+    reduce2d_shapes += [(128, 256) and (32, 1024)]\n+\n+\n+reduce_configs2 = [\n+    (op, 'float32', shape, axis)\n+    for op in ['min', 'max', 'sum']\n+    for shape in reduce2d_shapes\n+    for axis in [0, 1]\n+]\n+\n+\n+@pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n+def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n+        range_m = tl.arange(0, BLOCK_M)\n+        range_n = tl.arange(0, BLOCK_N)\n+        x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n+        z = GENERATE_TEST_HERE\n+        if AXIS == 1:\n+            tl.store(Z + range_m, z)\n+        else:\n+            tl.store(Z + range_n, z)\n+\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=AXIS)'})\n+    # input\n+    rs = RandomState(17)\n+    # limit the range of integers so that the sum does not overflow\n+    x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n+    x_tri = to_triton(x)\n+    numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n+                'argmin': np.argmin, 'argmax': np.argmax}[op]\n+    z_dtype_str = get_reduced_dtype(dtype_str, op)\n+    z_tri_dtype_str = z_dtype_str\n+    # numpy result\n+    if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n+        z_dtype_str = 'float32'\n+        z_tri_dtype_str = 'bfloat16'\n+        z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+        # trunc mantissa for a fair comparison of accuracy\n+        z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n+    else:\n+        z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+    # triton result\n+    z_tri = to_triton(numpy_random((shape[1 - axis],), dtype_str=z_dtype_str, rs=rs),\n+                      device=device, dst_type=z_tri_dtype_str)\n+    kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n+    z_tri = to_numpy(z_tri)\n+    # compare\n+    if op == 'sum':\n+        np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n+    else:\n+        if op == 'argmin' or op == 'argmax':\n+            # argmin and argmax can have multiple valid indices.\n+            # so instead we compare the values pointed by indices\n+            z_ref_index = np.expand_dims(z_ref, axis=axis)\n+            z_tri_index = np.expand_dims(z_tri, axis=axis)\n+            z_ref_value = np.take_along_axis(x, z_ref_index, axis=axis)\n+            z_tri_value = np.take_along_axis(x, z_tri_index, axis=axis)\n+            np.testing.assert_equal(z_ref_value, z_tri_value)\n+        else:\n+            np.testing.assert_equal(z_ref, z_tri)\n \n # # ---------------\n # # test permute"}, {"filename": "python/tests/test_reduce.py", "status": "modified", "additions": 42, "deletions": 19, "changes": 61, "file_content_changes": "@@ -5,11 +5,20 @@\n import triton\n import triton.language as tl\n \n-dtype_mapping = {\n-    'float16': torch.float16,\n-    'float32': torch.float32,\n-    'float64': torch.float64,\n-}\n+int_dtypes = ['int8', 'int16', 'int32', 'int64']\n+uint_dtypes = ['uint8']  # PyTorch does not support uint16/uint32/uint64\n+float_dtypes = ['float16', 'float32', 'float64']\n+dtypes = int_dtypes + uint_dtypes + float_dtypes\n+dtypes_with_bfloat16 = int_dtypes + uint_dtypes + float_dtypes\n+dtype_mapping = {dtype_str: torch.__dict__[dtype_str] for dtype_str in dtypes}\n+\n+\n+def get_reduced_dtype(dtype):\n+    if dtype in [torch.int8, torch.int16, torch.uint8]:\n+        return torch.int32\n+    if dtype in [torch.bfloat16]:\n+        return torch.float32\n+    return dtype\n \n \n def patch_kernel(template, to_replace):\n@@ -40,33 +49,40 @@ def reduce2d_kernel(x_ptr, z_ptr, axis: tl.constexpr, block_m: tl.constexpr, blo\n reduce1d_configs = [\n     (op, dtype, shape)\n     for op in ['sum', 'min', 'max']\n-    for dtype in ['float16', 'float32', 'float64']\n+    for dtype in dtypes\n     for shape in [4, 8, 16, 32, 64, 128, 512, 1024]\n ]\n \n \n @pytest.mark.parametrize('op, dtype, shape', reduce1d_configs)\n def test_reduce1d(op, dtype, shape):\n     dtype = dtype_mapping[dtype]\n-    x = torch.randn((shape,), device='cuda', dtype=dtype)\n+    reduced_dtype = get_reduced_dtype(dtype)\n+\n+    if dtype.is_floating_point:\n+        x = torch.randn((shape,), device='cuda', dtype=dtype)\n+    elif dtype is torch.uint8:\n+        x = torch.randint(0, 20, (shape,), device='cuda', dtype=dtype)\n+    else:\n+        x = torch.randint(-20, 20, (shape,), device='cuda', dtype=dtype)\n     z = torch.empty(\n         tuple(),\n         device=x.device,\n-        dtype=dtype,\n+        dtype=reduced_dtype,\n     )\n \n     kernel = patch_kernel(reduce1d_kernel, {'OP': op})\n     grid = (1,)\n     kernel[grid](x_ptr=x, z_ptr=z, block=shape)\n \n     if op == 'sum':\n-        golden_z = torch.sum(x, dtype=dtype)\n+        golden_z = torch.sum(x, dtype=reduced_dtype)\n     elif op == 'min':\n-        golden_z = torch.min(x)\n+        golden_z = torch.min(x).to(reduced_dtype)\n     else:\n-        golden_z = torch.max(x)\n+        golden_z = torch.max(x).to(reduced_dtype)\n \n-    if op == 'sum':\n+    if dtype.is_floating_point and op == 'sum':\n         if shape >= 256:\n             assert_close(z, golden_z, rtol=0.05, atol=0.1)\n         elif shape >= 32:\n@@ -80,7 +96,7 @@ def test_reduce1d(op, dtype, shape):\n reduce2d_configs = [\n     (op, dtype, shape, axis)\n     for op in ['sum', 'min', 'max']\n-    for dtype in ['float16', 'float32', 'float64']\n+    for dtype in dtypes\n     for shape in [(1, 4), (1, 8), (1, 16), (1, 32), (2, 32), (4, 32), (4, 128), (32, 64)]\n     for axis in [0, 1]\n ]\n@@ -89,22 +105,29 @@ def test_reduce1d(op, dtype, shape):\n @pytest.mark.parametrize('op, dtype, shape, axis', reduce2d_configs)\n def test_reduce2d(op, dtype, shape, axis):\n     dtype = dtype_mapping[dtype]\n-    x = torch.randn(shape, device='cuda', dtype=dtype)\n+    reduced_dtype = get_reduced_dtype(dtype)\n     reduced_shape = (shape[1 - axis],)\n-    z = torch.empty(reduced_shape, device=x.device, dtype=dtype)\n+\n+    if dtype.is_floating_point:\n+        x = torch.randn(shape, device='cuda', dtype=dtype)\n+    elif dtype is torch.uint8:\n+        x = torch.randint(0, 20, shape, device='cuda', dtype=dtype)\n+    else:\n+        x = torch.randint(-20, 20, shape, device='cuda', dtype=dtype)\n+    z = torch.empty(reduced_shape, device=x.device, dtype=reduced_dtype)\n \n     kernel = patch_kernel(reduce2d_kernel, {'OP': op})\n     grid = (1,)\n     kernel[grid](x_ptr=x, z_ptr=z, axis=axis, block_m=shape[0], block_n=shape[1])\n \n     if op == 'sum':\n-        golden_z = torch.sum(x, dim=axis, keepdim=False, dtype=dtype)\n+        golden_z = torch.sum(x, dim=axis, keepdim=False, dtype=reduced_dtype)\n     elif op == 'min':\n-        golden_z = torch.min(x, dim=axis, keepdim=False)[0]\n+        golden_z = torch.min(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n     else:\n-        golden_z = torch.max(x, dim=axis, keepdim=False)[0]\n+        golden_z = torch.max(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n \n-    if op == 'sum':\n+    if dtype.is_floating_point and op == 'sum':\n         if shape[axis] >= 256:\n             assert_close(z, golden_z, rtol=0.05, atol=0.1)\n         elif shape[axis] >= 32:"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 25, "deletions": 0, "changes": 25, "file_content_changes": "@@ -738,3 +738,28 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     return\n   }\n }\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n+#mma = #triton_gpu.mma<{version = 1, warpsPerCTA = [2, 2]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  func @matmul884_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  %a:tensor<128x32xf16, #shared>, %b:tensor<32x256xf16, #shared>) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<128x256xf32, #mma>\n+    // CHECK: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n+    %a_mat = triton_gpu.convert_layout %a : (tensor<128x32xf16, #shared>) -> tensor<128x32xf16, #dot_operand_a>\n+    %b_mat = triton_gpu.convert_layout %b : (tensor<32x256xf16, #shared>) -> tensor<32x256xf16, #dot_operand_b>\n+\n+    %28 = tt.dot %a_mat, %b_mat, %cst {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #dot_operand_a> * tensor<32x256xf16, #dot_operand_b> -> tensor<128x256xf32, #mma>\n+    // TODO[goostavz]: uncomment the following lines after convert_layout[mma<v1> -> blocked] is ready.\n+    // %38 = triton_gpu.convert_layout %28 : (tensor<128x256xf32, #mma>) -> tensor<128x256xf32, #blocked>\n+    // %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #blocked>\n+    // %36 = tt.broadcast %30 : (tensor<128x1x!tt.ptr<f32>, #blocked>) -> tensor<128x256x!tt.ptr<f32>, #blocked>\n+    // tt.store %36, %38 : tensor<128x256xf32, #blocked>\n+    return\n+  }\n+}"}]