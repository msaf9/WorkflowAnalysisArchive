[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -88,9 +88,7 @@ jobs:\n       - name: Run python tests on V100\n         if: ${{matrix.runner[0] == 'self-hosted' && matrix.runner[1] == 'V100'}}\n         run: |\n-          # TODO[Superjomn]: Remove the forloop-unroll setting after pipeline pass works\n           cd python/tests\n-          export TRITON_STATIC_LOOP_UNROLLING=1\n           pytest test_gemm.py::test_gemm_for_mmav1\n \n       - name: Run CXX unittests"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 21, "deletions": 6, "changes": 27, "file_content_changes": "@@ -67,8 +67,15 @@ struct DotOpMmaV1ConversionHelper {\n \n   // number of fp16x2 elements for $a.\n   int numElemsPerThreadA(RankedTensorType tensorTy) const {\n-    auto shape = tensorTy.getShape();\n-    auto order = getOrder();\n+    SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n+                               tensorTy.getShape().end());\n+    SmallVector<unsigned> order(getOrder().begin(), getOrder().end());\n+    // TODO[Superjomn]: transA is not available here.\n+    bool transA = false;\n+    if (transA) {\n+      std::swap(shape[0], shape[1]);\n+      std::swap(order[0], order[1]);\n+    }\n \n     bool isARow = order[0] != 0;\n     bool isAVec4 = !isARow && shape[order[0]] <= 16; // fp16*4 = 16bytes\n@@ -99,8 +106,16 @@ struct DotOpMmaV1ConversionHelper {\n \n   // number of fp16x2 elements for $b.\n   int numElemsPerThreadB(RankedTensorType tensorTy) const {\n-    auto shape = tensorTy.getShape();\n-    auto order = getOrder();\n+    SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n+                               tensorTy.getShape().end());\n+    SmallVector<unsigned> order(getOrder().begin(), getOrder().end());\n+    // TODO[Superjomn]: transB is not available here.\n+    bool transB = false;\n+    if (transB) {\n+      std::swap(shape[0], shape[1]);\n+      std::swap(order[0], order[1]);\n+    }\n+\n     bool isBRow = order[0] != 0;\n     bool isBVec4 = isBRow && shape[order[0]] <= 16;\n     // TODO[Superjomn]: Support the case when isBVec4=false later\n@@ -119,7 +134,7 @@ struct DotOpMmaV1ConversionHelper {\n     int elemsPerLd = vecGt4 ? 4 : 2;\n     int NK = shape[0];\n \n-    unsigned numN = rep[1] * shape[1] / (spw[1] * wpt[0]);\n+    unsigned numN = rep[1] * shape[1] / (spw[1] * wpt[1]);\n     return (numN / 2) * (NK / 4) * elemsPerLd;\n   }\n \n@@ -1556,7 +1571,7 @@ Value DotOpMmaV1ConversionHelper::loadB(\n     }\n   };\n \n-  unsigned numN = rep[1] * shape[1] / (spw[1] * wpt[0]);\n+  unsigned numN = rep[1] * shape[1] / (spw[1] * wpt[1]);\n   for (unsigned k = 0; k < NK; k += 4)\n     for (unsigned n = 0; n < numN / 2; ++n) {\n       if (!hbs.count({n, k}))"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -1730,9 +1730,9 @@ struct CatOpConversion : public ConvertTritonGPUOpToLLVMPattern<CatOp> {\n     auto rhsVals = getElementsFromStruct(loc, adaptor.rhs(), rewriter);\n     // concatenate (and potentially reorder) values\n     SmallVector<Value> retVals;\n-    for(Value v: lhsVals)\n+    for (Value v : lhsVals)\n       retVals.push_back(v);\n-    for(Value v: rhsVals)\n+    for (Value v : rhsVals)\n       retVals.push_back(v);\n     // pack and replace\n     Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n@@ -3537,6 +3537,10 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n   bool isBRow = BOrder[0] != 0;\n   bool isAVec4 = !isARow && AShape[isARow] <= 16; // fp16*4 = 16bytes\n   bool isBVec4 = isBRow && BShape[isBRow] <= 16;\n+  // TODO[Superjomn]: ld.v4 is not supported.\n+  isAVec4 = true;\n+  isBVec4 = true;\n+\n   int packSize0 = (isARow || isAVec4) ? 1 : 2;\n   int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n   SmallVector<int> fpw({2, 2, 1});"}]