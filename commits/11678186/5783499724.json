[{"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSMaterialization.cpp", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -380,8 +380,10 @@ void materializeTokenOperations(Operation *parentOp, int numCTAs) {\n     Value bufferEmptyArray =\n         builder.create<ttng::AllocMBarrierOp>(tokenLoc, mBarriersTy, numCTAs);\n \n-    // Make sure that MBarriers are initialized in all CTAs\n-    if (numCTAs > 1) {\n+    if (numCTAs == 1) {\n+      builder.create<mlir::gpu::BarrierOp>(tokenLoc);\n+    } else {\n+      // Make sure that MBarriers are initialized in all CTAs\n       builder.create<triton::nvidia_gpu::ClusterArriveOp>(tokenLoc, false);\n       builder.create<triton::nvidia_gpu::ClusterWaitOp>(tokenLoc);\n     }"}, {"filename": "lib/Hopper/HopperHelpers.c", "status": "modified", "additions": 0, "deletions": 32, "changes": 32, "file_content_changes": "@@ -104,38 +104,6 @@ __DEVICE__ __attribute__((__always_inline__)) void __nv_wgmma_wait_group() {\n   asm volatile(\"wgmma.wait_group.sync.aligned 0;\\n\");\n }\n \n-// GMMA expects data to be in TN format. if A is column major, transa should be\n-// set GMMA expects data to be in TN format. if B is row major, transb should be\n-// set\n-__DEVICE__ __attribute__((__always_inline__)) float32\n-__nv_wgmma_m64n64k16_f32_f16_f16_row_col(const uint64_t desc_a,\n-                                         const uint64_t desc_b, float32 acc) {\n-  const uint32_t scale_d = 1;\n-  asm volatile(\"{\\n\"\n-               \".reg .pred p;\\n\\t\"\n-               \"setp.eq.u32 p, %34, 1;\\n\\t\"\n-               \"wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16\\n\"\n-               \"{%0, %1, %2, %3, %4, %5, %6, %7, \\n\"\n-               \" %8, %9, %10, %11, %12, %13, %14, %15,\\n\"\n-               \" %16, %17, %18, %19, %20, %21, %22, %23,\\n\"\n-               \" %24, %25, %26, %27, %28, %29, %30, %31},\\n\"\n-               \"%32, \\n\"\n-               \"%33, \\n\"\n-               \"p, 1, 1, 0, 0;\\n\"\n-               \"}\"\n-               : \"+f\"(acc.d0), \"+f\"(acc.d1), \"+f\"(acc.d2), \"+f\"(acc.d3),\n-                 \"+f\"(acc.d4), \"+f\"(acc.d5), \"+f\"(acc.d6), \"+f\"(acc.d7),\n-                 \"+f\"(acc.d8), \"+f\"(acc.d9), \"+f\"(acc.d10), \"+f\"(acc.d11),\n-                 \"+f\"(acc.d12), \"+f\"(acc.d13), \"+f\"(acc.d14), \"+f\"(acc.d15),\n-                 \"+f\"(acc.d16), \"+f\"(acc.d17), \"+f\"(acc.d18), \"+f\"(acc.d19),\n-                 \"+f\"(acc.d20), \"+f\"(acc.d21), \"+f\"(acc.d22), \"+f\"(acc.d23),\n-                 \"+f\"(acc.d24), \"+f\"(acc.d25), \"+f\"(acc.d26), \"+f\"(acc.d27),\n-                 \"+f\"(acc.d28), \"+f\"(acc.d29), \"+f\"(acc.d30), \"+f\"(acc.d31)\n-               : \"l\"(desc_a), \"l\"(desc_b), \"r\"(scale_d));\n-\n-  return acc;\n-}\n-\n __DEVICE__ __attribute__((__always_inline__)) void\n __nv_mbarrier_init(uint32_t bar, uint32_t expected, uint32_t pred) {\n   if (pred) {"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -280,7 +280,11 @@ def tma_warp_specialized_matmul_kernel(\n ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n def test_non_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B):\n-    pytest.skip('hang')\n+    if '-'.join(map(str, [M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B])) in [\n+        '4096-4096-256-128-256-16-1-False-True',\n+        '4096-4096-256-128-256-64-1-False-True'\n+    ]:\n+        pytest.skip('Insufficient register resources')\n \n     if (TRANS_A):\n         a = .1 * torch.randn((K, M), device='cuda', dtype=torch.float16).T"}, {"filename": "python/triton/hopper_lib/libhopper_helpers.bc", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}]