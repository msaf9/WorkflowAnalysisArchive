[{"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 29, "deletions": 1, "changes": 30, "file_content_changes": "@@ -209,6 +209,33 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     }\n     curr = AxisInfo(contiguity, divisibility, constancy);\n   }\n+\n+  // CmpI\n+  if ((llvm::dyn_cast<arith::CmpIOp>(op) ||\n+       llvm::dyn_cast<triton::gpu::CmpIOp>(op)) &&\n+      op->getResult(0).getType().dyn_cast<TensorType>()) {\n+    auto resTy = op->getResult(0).getType().cast<TensorType>();\n+    short rank = resTy.getRank();\n+    auto lhsInfo = operands[0]->getValue();\n+    auto rhsInfo = operands[1]->getValue();\n+    auto shape = resTy.getShape();\n+\n+    AxisInfo::DimVectorT contiguity, divisibility, constancy;\n+    for (short d = 0; d < rank; ++d) {\n+      if (rhsInfo.getConstancy(d) % lhsInfo.getContiguity(d) == 0 ||\n+          rhsInfo.getConstancy(d) % lhsInfo.getConstancy(d))\n+        constancy.push_back(\n+            gcd(lhsInfo.getDivisibility(d), rhsInfo.getDivisibility(d)));\n+      else\n+        constancy.push_back(1);\n+\n+      divisibility.push_back(shape[d]);\n+      contiguity.push_back(1);\n+    }\n+\n+    curr = AxisInfo(contiguity, divisibility, constancy);\n+  }\n+\n   // UnrealizedConversionCast\n   // This is needed by TritonGPUToLLVM, to get AxisInfo when the graph is\n   // in the process of a PartialConversion, where UnrealizedConversionCast\n@@ -219,7 +246,8 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n   if (curr.getRank() == 0) {\n     return markAllPessimisticFixpoint(op->getResults());\n   }\n-  // join all latice elements\n+\n+  // join all lattice elements\n   ChangeResult result = ChangeResult::NoChange;\n   for (Value value : op->getResults()) {\n     result |= getLatticeElement(value).join(curr);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 225, "deletions": 224, "changes": 449, "file_content_changes": "@@ -760,6 +760,17 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n     return vec;\n   }\n \n+  unsigned getMaskAlignment(Value mask) const {\n+    auto maskOrder = mask.getType()\n+                         .cast<RankedTensorType>()\n+                         .getEncoding()\n+                         .cast<BlockedEncodingAttr>()\n+                         .getOrder();\n+\n+    auto maskAxis = getAxisInfo(mask);\n+    return std::max<int>(maskAxis->getConstancy(maskOrder[0]), 1);\n+  }\n+\n   llvm::Optional<AxisInfo> getAxisInfo(Value val) const {\n     if (auto it = AxisAnalysisPass.lookupLatticeElement(val)) {\n       return it->getValue();\n@@ -772,6 +783,208 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n   AxisInfoAnalysis &AxisAnalysisPass;\n };\n \n+struct LoadOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>,\n+      public LoadStoreConversionBase {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::LoadOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LoadOpConversion(LLVMTypeConverter &converter,\n+                   AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>(converter, benefit),\n+        LoadStoreConversionBase(axisAnalysisPass) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Value ptr = op.ptr();\n+    Value mask = op.mask();\n+    Value other = op.other();\n+\n+    Value llPtr = adaptor.ptr();\n+    Value llMask = adaptor.mask();\n+    Value llOther = adaptor.other();\n+\n+    auto loc = op->getLoc();\n+    MLIRContext *ctx = rewriter.getContext();\n+\n+    auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    if (!valueTy)\n+      return failure();\n+    Type valueElemTy =\n+        getTypeConverter()->convertType(valueTy.getElementType());\n+\n+    auto [layout, numElems] = getLayout(ptr);\n+\n+    auto ptrElems = getLLVMElems(ptr, llPtr, layout, rewriter, loc);\n+    assert(ptrElems.size() == numElems);\n+    // Determine the vectorization size\n+    size_t vec = getVectorizeSize(ptr, layout);\n+\n+    SmallVector<Value> maskElems;\n+    if (llMask) {\n+      unsigned maskAlignment = getMaskAlignment(mask);\n+      maskElems = getLLVMElems(mask, llMask, layout, rewriter, loc);\n+      assert(ptrElems.size() == maskElems.size());\n+\n+      size_t maskAlign = getMaskAlignment(mask);\n+      vec = std::min(vec, maskAlign);\n+    }\n+\n+    const size_t dtsize =\n+        std::max<int>(1, valueElemTy.getIntOrFloatBitWidth() / 8);\n+    const size_t valueElemNbits = dtsize * 8;\n+\n+    const int numVecs = numElems / vec;\n+\n+    // TODO: (goostavz) handle when other is const but not splat, which\n+    //       should be rarely seen\n+    bool otherIsSplatConstInt = false;\n+    DenseElementsAttr constAttr;\n+    int64_t splatVal = 0;\n+    if (valueElemTy.isa<IntegerType>() &&\n+        matchPattern(op.other(), m_Constant(&constAttr)) &&\n+        constAttr.isSplat()) {\n+      otherIsSplatConstInt = true;\n+      splatVal = constAttr.getSplatValue<APInt>().getSExtValue();\n+    }\n+\n+    auto otherElems = getLLVMElems(other, llOther, layout, rewriter, loc);\n+\n+    SmallVector<Value> loadedVals;\n+    for (size_t vecStart = 0; vecStart < numElems; vecStart += vec) {\n+      // TODO: optimization when ptr is GEP with constant offset\n+      size_t in_off = 0;\n+\n+      const int maxWordWidth = std::max<int>(32, valueElemNbits);\n+      const int totalWidth = valueElemNbits * vec;\n+      const int width = std::min(totalWidth, maxWordWidth);\n+      const int nWords = std::max(1, totalWidth / width);\n+      const int wordNElems = width / valueElemNbits;\n+      const int vecNElems = totalWidth / valueElemNbits;\n+      assert(wordNElems * nWords * numVecs == numElems);\n+\n+      // TODO(Superjomn) Add cache policy fields to StoreOp.\n+      // TODO(Superjomn) Deal with cache policy here.\n+      const bool hasL2EvictPolicy = false;\n+\n+      PTXBuilder ptxBuilder;\n+      auto &ld = *ptxBuilder.create<PTXIOInstr>(\"ld\");\n+\n+      Value pred = mask ? maskElems[vecStart] : int_val(1, 1);\n+\n+      const std::string readConstraint =\n+          (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n+      const std::string writeConstraint =\n+          (width == 64) ? \"=l\" : ((width == 32) ? \"=r\" : \"=c\");\n+\n+      // prepare asm operands\n+      auto *dstsOpr = ptxBuilder.newListOperand();\n+      for (int wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n+        auto *opr = ptxBuilder.newOperand(writeConstraint); // =r operations\n+        dstsOpr->listAppend(opr);\n+      }\n+\n+      auto *addrOpr =\n+          ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n+\n+      // Define the instruction opcode\n+      ld.o(\"volatile\", op.isVolatile())\n+          .global()\n+          .o(\"ca\", op.cache() == triton::CacheModifier::CA)\n+          .o(\"cg\", op.cache() == triton::CacheModifier::CG)\n+          .o(\"L1::evict_first\",\n+             op.evict() == triton::EvictionPolicy::EVICT_FIRST)\n+          .o(\"L1::evict_last\", op.evict() == triton::EvictionPolicy::EVICT_LAST)\n+          .o(\"L1::cache_hint\", hasL2EvictPolicy)\n+          .v(nWords)\n+          .b(width);\n+\n+      PTXBuilder::Operand *evictOpr{};\n+\n+      // Here lack a mlir::Value to bind to this operation, so disabled.\n+      // if (has_l2_evict_policy)\n+      //   evictOpr = ptxBuilder.newOperand(l2Evict, \"l\");\n+\n+      if (!evictOpr)\n+        ld(dstsOpr, addrOpr).predicate(pred, \"b\");\n+      else\n+        ld(dstsOpr, addrOpr, evictOpr).predicate(pred, \"b\");\n+\n+      if (other) {\n+        for (size_t ii = 0; ii < nWords; ++ii) {\n+          PTXInstr &mov = *ptxBuilder.create<>(\"mov\");\n+          mov.o(\"u\", width);\n+\n+          size_t size = width / valueElemNbits;\n+\n+          auto vecTy = LLVM::getFixedVectorType(valueElemTy, size);\n+          Value v = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n+          for (size_t s = 0; s < size; ++s) {\n+            Value falseVal = otherElems[vecStart + ii * size + s];\n+            Value sVal = createIndexAttrConstant(\n+                rewriter, loc, this->getTypeConverter()->getIndexType(), s);\n+            v = insert_element(vecTy, v, falseVal, sVal);\n+          }\n+          v = bitcast(IntegerType::get(getContext(), width), v);\n+\n+          PTXInstr::Operand *opr{};\n+          if (otherIsSplatConstInt)\n+            opr = ptxBuilder.newConstantOperand(splatVal);\n+          else\n+            opr = ptxBuilder.newOperand(v, readConstraint);\n+\n+          mov(dstsOpr->listGet(ii), opr).predicateNot(pred, \"b\");\n+        }\n+      }\n+\n+      // ---\n+      // create inline ASM signature\n+      // ---\n+      SmallVector<Type> retTys(nWords, IntegerType::get(getContext(), width));\n+      Type retTy = retTys.size() > 1\n+                       ? LLVM::LLVMStructType::getLiteral(getContext(), retTys)\n+                       : retTys[0];\n+\n+      // TODO: if (has_l2_evict_policy)\n+      auto asmDialectAttr = LLVM::AsmDialectAttr::get(rewriter.getContext(),\n+                                                      LLVM::AsmDialect::AD_ATT);\n+      Value ret = ptxBuilder.launch(rewriter, loc, retTy);\n+\n+      // ---\n+      // extract and store return values\n+      // ---\n+      SmallVector<Value> rets;\n+      for (unsigned int ii = 0; ii < nWords; ++ii) {\n+        Value curr;\n+        if (retTy.isa<LLVM::LLVMStructType>()) {\n+          curr = extract_val(IntegerType::get(getContext(), width), ret,\n+                             rewriter.getI64ArrayAttr(ii));\n+        } else {\n+          curr = ret;\n+        }\n+        curr = bitcast(\n+            LLVM::getFixedVectorType(valueElemTy, width / valueElemNbits),\n+            curr);\n+        rets.push_back(curr);\n+      }\n+      int tmp = width / valueElemNbits;\n+      for (size_t ii = 0; ii < vec; ++ii) {\n+        Value vecIdx = createIndexAttrConstant(\n+            rewriter, loc, this->getTypeConverter()->getIndexType(), ii % tmp);\n+        Value loaded = extract_element(valueElemTy, rets[ii / tmp], vecIdx);\n+        loadedVals.push_back(loaded);\n+      }\n+    } // end vec\n+\n+    Type llvmResultStructTy = getTypeConverter()->convertType(valueTy);\n+    Value resultStruct =\n+        getStructFromElements(loc, loadedVals, rewriter, llvmResultStructTy);\n+    rewriter.replaceOp(op, {resultStruct});\n+    return success();\n+  }\n+};\n+\n struct StoreOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::StoreOp>,\n       public LoadStoreConversionBase {\n@@ -815,14 +1028,8 @@ struct StoreOpConversion\n     if (llMask) {\n       maskElems = getLLVMElems(mask, llMask, layout, rewriter, loc);\n       assert(valueElems.size() == maskElems.size());\n-      auto maskOrder = mask.getType()\n-                           .cast<RankedTensorType>()\n-                           .getEncoding()\n-                           .cast<BlockedEncodingAttr>()\n-                           .getOrder();\n-\n-      auto maskAxis = getAxisInfo(mask);\n-      size_t maskAlign = std::max<int>(maskAxis->getConstancy(maskOrder[0]), 1);\n+\n+      size_t maskAlign = getMaskAlignment(mask);\n       vec = std::min(vec, maskAlign);\n     }\n \n@@ -847,15 +1054,10 @@ struct StoreOpConversion\n       // TODO(Superjomn) Deal with cache policy here.\n       const bool hasL2EvictPolicy = false;\n \n-      PTXBuilder ptxBuilder;\n-      auto &ptxStoreInstr = *ptxBuilder.create<PTXIOInstr>(\"st\");\n-\n-      llvm::SmallVector<std::string> asmArgs;\n-\n       Type valArgTy = IntegerType::get(ctx, width);\n       auto wordTy = vec_ty(valueElemTy, wordNElems);\n \n-      auto *asmArgList = ptxBuilder.newListOperand();\n+      SmallVector<std::pair<Value, std::string>> asmArgs;\n       for (int wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n         // llWord is a width-len composition\n         Value llWord = rewriter.create<LLVM::UndefOp>(loc, wordTy);\n@@ -877,23 +1079,25 @@ struct StoreOpConversion\n         llWord = bitcast(valArgTy, llWord);\n         std::string constraint =\n             (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n-        asmArgList->listAppend(ptxBuilder.newOperand(llWord, constraint));\n+        asmArgs.emplace_back(llWord, constraint);\n       }\n \n-      // TODO(Superjomn) Need to check masks before vectorize the load for\n-      // the values share one predicate? Here assume all the mask values are\n-      // the same.\n+      // Prepare the PTX inline asm.\n+      PTXBuilder ptxBuilder;\n+      auto *asmArgList = ptxBuilder.newListOperand(asmArgs);\n+\n       Value maskVal = llMask ? maskElems[vecStart] : int_val(1, 1);\n-      ptxStoreInstr.global().b(width).v(nWords);\n \n       auto *asmAddr =\n           ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n \n+      auto &ptxStoreInstr =\n+          ptxBuilder.create<PTXIOInstr>(\"st\")->global().b(width).v(nWords);\n       ptxStoreInstr(asmAddr, asmArgList).predicate(maskVal, \"b\");\n+\n       Type boolTy = getTypeConverter()->convertType(rewriter.getIntegerType(1));\n       llvm::SmallVector<Type> argTys({boolTy, ptr.getType()});\n-      for (int i = 0; i < nWords; ++i)\n-        argTys.push_back(valArgTy);\n+      argTys.insert(argTys.end(), nWords, valArgTy);\n \n       auto ASMReturnTy = LLVM::LLVMVoidType::get(ctx);\n \n@@ -1066,209 +1270,6 @@ struct MakeRangeOpConversion\n   }\n };\n \n-struct LoadOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>,\n-      public LoadStoreConversionBase {\n-  using ConvertTritonGPUOpToLLVMPattern<\n-      triton::LoadOp>::ConvertTritonGPUOpToLLVMPattern;\n-\n-  LoadOpConversion(LLVMTypeConverter &converter,\n-                   AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n-      : ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>(converter, benefit),\n-        LoadStoreConversionBase(axisAnalysisPass) {}\n-\n-  LogicalResult\n-  matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    Value ptr = op.ptr();\n-    Value mask = op.mask();\n-    Value other = op.other();\n-\n-    Value llPtr = adaptor.ptr();\n-    Value llMask = adaptor.mask();\n-    Value llOther = adaptor.other();\n-\n-    auto loc = op->getLoc();\n-    MLIRContext *ctx = rewriter.getContext();\n-\n-    auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n-    if (!valueTy)\n-      return failure();\n-    Type valueElemTy =\n-        getTypeConverter()->convertType(valueTy.getElementType());\n-\n-    auto [layout, numElems] = getLayout(ptr);\n-\n-    auto ptrElems = getLLVMElems(ptr, llPtr, layout, rewriter, loc);\n-    assert(ptrElems.size() == numElems);\n-\n-    SmallVector<Value> maskElems;\n-    if (llMask) {\n-      maskElems = getLLVMElems(mask, llMask, layout, rewriter, loc);\n-      assert(ptrElems.size() == maskElems.size());\n-    }\n-\n-    // Determine the vectorization size\n-    size_t vec = getVectorizeSize(ptr, layout);\n-\n-    const size_t dtsize =\n-        std::max<int>(1, valueElemTy.getIntOrFloatBitWidth() / 8);\n-    const size_t valueElemNbits = dtsize * 8;\n-\n-    const int numVecs = numElems / vec;\n-\n-    // TODO: (goostavz) handle when other is const but not splat, which\n-    //       should be rarely seen\n-    bool otherIsSplatConstInt = false;\n-    DenseElementsAttr constAttr;\n-    int64_t splatVal = 0;\n-    if (valueElemTy.isa<IntegerType>() &&\n-        matchPattern(op.other(), m_Constant(&constAttr)) &&\n-        constAttr.isSplat()) {\n-      otherIsSplatConstInt = true;\n-      splatVal = constAttr.getSplatValue<APInt>().getSExtValue();\n-    }\n-\n-    auto otherElems = getLLVMElems(other, llOther, layout, rewriter, loc);\n-\n-    SmallVector<Value> loadedVals;\n-    for (size_t vecStart = 0; vecStart < numElems; vecStart += vec) {\n-      // TODO: optimization when ptr is GEP with constant offset\n-      size_t in_off = 0;\n-\n-      const int maxWordWidth = std::max<int>(32, valueElemNbits);\n-      const int totalWidth = valueElemNbits * vec;\n-      const int width = std::min(totalWidth, maxWordWidth);\n-      const int nWords = std::max(1, totalWidth / width);\n-      const int wordNElems = width / valueElemNbits;\n-      const int vecNElems = totalWidth / valueElemNbits;\n-      assert(wordNElems * nWords * numVecs == numElems);\n-\n-      // TODO(Superjomn) Add cache policy fields to StoreOp.\n-      // TODO(Superjomn) Deal with cache policy here.\n-      const bool hasL2EvictPolicy = false;\n-\n-      PTXBuilder ptxBuilder;\n-      auto &ld = *ptxBuilder.create<PTXIOInstr>(\"ld\");\n-\n-      // TODO(Superjomn) Need to check masks before vectorize the load for all\n-      // the values share one predicate? Here assume all the mask values are\n-      // the same.\n-      Value pred = mask ? maskElems[vecStart] : int_val(1, 1);\n-\n-      const std::string readConstraint =\n-          (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n-      const std::string writeConstraint =\n-          (width == 64) ? \"=l\" : ((width == 32) ? \"=r\" : \"=c\");\n-\n-      // prepare asm operands\n-      auto *dstsOpr = ptxBuilder.newListOperand();\n-      for (int wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n-        auto *opr = ptxBuilder.newOperand(writeConstraint); // =r operations\n-        dstsOpr->listAppend(opr);\n-      }\n-\n-      auto *addrOpr =\n-          ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n-\n-      // Define the instruction opcode\n-      ld.o(\"volatile\", op.isVolatile())\n-          .global()\n-          .o(\"ca\", op.cache() == triton::CacheModifier::CA)\n-          .o(\"cg\", op.cache() == triton::CacheModifier::CG)\n-          .o(\"L1::evict_first\",\n-             op.evict() == triton::EvictionPolicy::EVICT_FIRST)\n-          .o(\"L1::evict_last\", op.evict() == triton::EvictionPolicy::EVICT_LAST)\n-          .o(\"L1::cache_hint\", hasL2EvictPolicy)\n-          .v(nWords)\n-          .b(width);\n-\n-      PTXBuilder::Operand *evictOpr{};\n-\n-      // Here lack a mlir::Value to bind to this operation, so disabled.\n-      // if (has_l2_evict_policy)\n-      //   evictOpr = ptxBuilder.newOperand(l2Evict, \"l\");\n-\n-      if (!evictOpr)\n-        ld(dstsOpr, addrOpr).predicate(pred, \"b\");\n-      else\n-        ld(dstsOpr, addrOpr, evictOpr).predicate(pred, \"b\");\n-\n-      if (other) {\n-        for (size_t ii = 0; ii < nWords; ++ii) {\n-          PTXInstr &mov = *ptxBuilder.create<>(\"mov\");\n-          mov.o(\"u\", width);\n-\n-          size_t size = width / valueElemNbits;\n-\n-          auto vecTy = LLVM::getFixedVectorType(valueElemTy, size);\n-          Value v = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n-          for (size_t s = 0; s < size; ++s) {\n-            Value falseVal = otherElems[vecStart + ii * size + s];\n-            Value sVal = createIndexAttrConstant(\n-                rewriter, loc, this->getTypeConverter()->getIndexType(), s);\n-            v = insert_element(vecTy, v, falseVal, sVal);\n-          }\n-          v = bitcast(IntegerType::get(getContext(), width), v);\n-\n-          PTXInstr::Operand *opr{};\n-          if (otherIsSplatConstInt) {\n-            opr = ptxBuilder.newConstantOperand(splatVal);\n-          } else {\n-            opr = ptxBuilder.newOperand(v, readConstraint);\n-          }\n-\n-          mov(dstsOpr->listGet(ii), opr).predicateNot(pred, \"b\");\n-        }\n-      }\n-\n-      // ---\n-      // create inline ASM signature\n-      // ---\n-      SmallVector<Type> retTys(nWords, IntegerType::get(getContext(), width));\n-      Type retTy = retTys.size() > 1\n-                       ? LLVM::LLVMStructType::getLiteral(getContext(), retTys)\n-                       : retTys[0];\n-\n-      // TODO: if (has_l2_evict_policy)\n-      auto asmDialectAttr = LLVM::AsmDialectAttr::get(rewriter.getContext(),\n-                                                      LLVM::AsmDialect::AD_ATT);\n-      Value ret = ptxBuilder.launch(rewriter, loc, retTy);\n-\n-      // ---\n-      // extract and store return values\n-      // ---\n-      SmallVector<Value> rets;\n-      for (unsigned int ii = 0; ii < nWords; ++ii) {\n-        Value curr;\n-        if (retTy.isa<LLVM::LLVMStructType>()) {\n-          curr = extract_val(IntegerType::get(getContext(), width), ret,\n-                             rewriter.getI64ArrayAttr(ii));\n-        } else {\n-          curr = ret;\n-        }\n-        curr = bitcast(\n-            LLVM::getFixedVectorType(valueElemTy, width / valueElemNbits),\n-            curr);\n-        rets.push_back(curr);\n-      }\n-      int tmp = width / valueElemNbits;\n-      for (size_t ii = 0; ii < vec; ++ii) {\n-        Value vecIdx = createIndexAttrConstant(\n-            rewriter, loc, this->getTypeConverter()->getIndexType(), ii % tmp);\n-        Value loaded = extract_element(valueElemTy, rets[ii / tmp], vecIdx);\n-        loadedVals.push_back(loaded);\n-      }\n-    } // end vec\n-\n-    Type llvmResultStructTy = getTypeConverter()->convertType(valueTy);\n-    Value resultStruct =\n-        getStructFromElements(loc, loadedVals, rewriter, llvmResultStructTy);\n-    rewriter.replaceOp(op, {resultStruct});\n-    return success();\n-  }\n-};\n-\n struct GetProgramIdOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::GetProgramIdOp> {\n   using ConvertTritonGPUOpToLLVMPattern<"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "file_content_changes": "@@ -1,4 +1,5 @@\n #include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include <numeric>\n@@ -23,6 +24,11 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     std::sort(order.begin(), order.end(), [&](unsigned x, unsigned y) {\n       return contiguity[x] > contiguity[y];\n     });\n+\n+    int numElems = product(origType.getShape());\n+    int numThreads = numWarps * 32;\n+    int numElemsPerThread = std::max(numElems / numThreads, 1);\n+\n     // Thread tile size depends on memory alignment\n     SmallVector<unsigned, 4> sizePerThread(rank, 1);\n     PointerType ptrType = origType.getElementType().cast<PointerType>();\n@@ -31,7 +37,8 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     unsigned maxContig = info.getContiguity(order[0]);\n     unsigned alignment = std::min(maxMultiple, maxContig);\n     unsigned perThread = std::min(alignment, 128 / numBits);\n-    sizePerThread[order[0]] = perThread;\n+    sizePerThread[order[0]] = std::min<int>(perThread, numElemsPerThread);\n+\n     SmallVector<unsigned> dims(rank);\n     std::iota(dims.begin(), dims.end(), 0);\n     // create encoding"}, {"filename": "python/tests/test_vecadd.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -188,7 +188,7 @@ def test_vecadd_no_scf(num_warps, block_size, shape):\n     [2, 256, (3, 256 + 7)],\n     [4, 256, (3, 256 + 7)],\n ])\n-def test_vecadd__no_scf_masked(num_warps, block_size, shape):\n+def test_vecadd_no_scf_masked(num_warps, block_size, shape):\n     vecadd_no_scf_tester(num_warps, block_size, shape)\n \n "}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 89, "deletions": 0, "changes": 89, "file_content_changes": "@@ -50,3 +50,92 @@ func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {t\n   tt.store %19, %20, %cst : tensor<128x128xf32>\n   return\n }\n+\n+// -----\n+\n+module {\n+\n+// This is a tiny test for verifying StoreOp-related alignment, It simply store a constant to a buffer.\n+func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n: i32 {tt.divisibility = 16 : i32}) {\n+  // CHECK: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1]\n+  %pid = tt.get_program_id {axis = 0 : i32} : i32\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [1]\n+  %c128_i32 = arith.constant 128 : i32\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [1]\n+  %1 = arith.muli %pid, %c128_i32 : i32\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [65536] ; Constancy: [1]\n+  %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+ // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128]\n+  %3 = tt.splat %1 : (i32) -> tensor<128xi32>\n+ // CHECK-NEXT: Contiguity: [128] ; Divisibility: [128] ; Constancy: [1]\n+  %4 = arith.addi %3, %2 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128]\n+  %5 = tt.splat %addr : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [16] ; Constancy: [1]\n+  %6 = tt.addptr %5, %4 : tensor<128x!tt.ptr<f32>>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128]\n+  %9 = tt.splat %n : (i32) -> tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [16]\n+  %mask = arith.cmpi slt, %4, %9 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1]\n+  %cst = arith.constant dense<0.0> : tensor<128xf32>\n+  tt.store %5, %cst, %mask : tensor<128xf32>\n+  return\n+}\n+\n+}\n+\n+// -----\n+\n+// This IR is dumped from vecadd test.\n+// Note, the hint {tt.divisibility = 16 : i32} for %n_elements affects the alignment of mask.\n+func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32 {tt.divisibility = 16 : i32}) {\n+  %c64_i32 = arith.constant 64 : i32\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = arith.muli %0, %c64_i32 : i32\n+  %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+  %3 = tt.splat %1 : (i32) -> tensor<64xi32>\n+  %4 = arith.addi %3, %2 : tensor<64xi32>\n+  %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>>\n+  %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>\n+  %9 = tt.splat %n_elements : (i32) -> tensor<64xi32>\n+  // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [16] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n+  %mask = arith.cmpi slt, %4, %9 : tensor<64xi32>\n+  %11 = tt.load %6, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+  %12 = tt.load %8, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+  %13 = arith.addf %11, %12 : tensor<64xf32>\n+  %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  // CHECK: Contiguity: [64] ; Divisibility: [16] ; Constancy: [1] ( %{{.*}} = tt.addptr %{{.*}}, %{{.*}} : tensor<64x!tt.ptr<f32>> )\n+  %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>\n+  tt.store %15, %13, %mask : tensor<64xf32>\n+  return\n+}\n+\n+// -----\n+\n+// This IR is dumped from vecadd test.\n+// Note, there is no divisibility hint for %n_elements, Triton should assume its divisibility to be 1 by default.\n+func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n+  %c64_i32 = arith.constant 64 : i32\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = arith.muli %0, %c64_i32 : i32\n+  %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+  %3 = tt.splat %1 : (i32) -> tensor<64xi32>\n+  %4 = arith.addi %3, %2 : tensor<64xi32>\n+  %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>>\n+  %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>\n+  %9 = tt.splat %n_elements : (i32) -> tensor<64xi32>\n+  // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [1] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n+  %10 = arith.cmpi slt, %4, %9 : tensor<64xi32>\n+  %11 = tt.load %6, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+  %12 = tt.load %8, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+  %13 = arith.addf %11, %12 : tensor<64xf32>\n+  %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>\n+  tt.store %15, %13, %10 : tensor<64xf32>\n+  return\n+}"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 32, "deletions": 1, "changes": 33, "file_content_changes": "@@ -161,6 +161,37 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n \n // -----\n \n+// This test verifies the vectorization of Load and Store Ops.\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n+// Note, the %n_elements doesn't have a \"tt.divisibility\" hint, so Triton assumes it's divisibility is 1, this should effect the mask's alignment and further restrict the load/store ops' vector width to be 1.\n+module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n+  func @vecadd_masked_vec1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n+    %c64_i32 = arith.constant 64 : i32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %c64_i32 : i32\n+    %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked>\n+    %3 = tt.splat %1 : (i32) -> tensor<64xi32, #blocked>\n+    %4 = arith.addi %3, %2 : tensor<64xi32, #blocked>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n+    %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>, #blocked>\n+    %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n+    %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>, #blocked>\n+    %9 = tt.splat %n_elements : (i32) -> tensor<64xi32, #blocked>\n+    %10 = \"triton_gpu.cmpi\"(%4, %9) {predicate = 2 : i64} : (tensor<64xi32, #blocked>, tensor<64xi32, #blocked>) -> tensor<64xi1, #blocked>\n+    // load op has a vector width = 1 due to the %mask's alignment\n+    // CHECK: ld.global.b32\n+    %11 = tt.load %6, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32, #blocked>\n+    %12 = tt.load %8, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32, #blocked>\n+    %13 = arith.addf %11, %12 : tensor<64xf32, #blocked>\n+    %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n+    %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>, #blocked>\n+    tt.store %15, %13, %10 : tensor<64xf32, #blocked>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: global_load_store_vec8\n@@ -682,4 +713,4 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n     return\n   }\n-}\n\\ No newline at end of file\n+}"}]