[{"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 12, "deletions": 12, "changes": 24, "file_content_changes": "@@ -284,17 +284,17 @@ def matmul(a, b, activation=None):\n #\n # We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS)\n \n-# torch.manual_seed(0)\n-# a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-# b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-# triton_output = matmul(a, b, activation=None)\n-# torch_output = torch.matmul(a, b)\n-# print(f\"triton_output={triton_output}\")\n-# print(f\"torch_output={torch_output}\")\n-# if triton.testing.allclose(triton_output, torch_output):\n-#     print(\"\u2705 Triton and Torch match\")\n-# else:\n-#     print(\"\u274c Triton and Torch differ\")\n+torch.manual_seed(0)\n+a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+triton_output = matmul(a, b, activation=None)\n+torch_output = torch.matmul(a, b)\n+print(f\"triton_output={triton_output}\")\n+print(f\"torch_output={torch_output}\")\n+if triton.testing.allclose(triton_output, torch_output):\n+    print(\"\u2705 Triton and Torch match\")\n+else:\n+    print(\"\u274c Triton and Torch differ\")\n \n # %%\n # Benchmark\n@@ -334,4 +334,4 @@ def benchmark(M, N, K, provider):\n     return perf(ms), perf(max_ms), perf(min_ms)\n \n \n-benchmark.run(show_plots=False, print_data=True)\n+benchmark.run(show_plots=True, print_data=True)"}]