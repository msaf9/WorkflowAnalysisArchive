[{"filename": ".github/workflows/wheels.yml", "status": "added", "additions": 40, "deletions": 0, "changes": 40, "file_content_changes": "@@ -0,0 +1,40 @@\n+name: Wheels\n+on:\n+  workflow_dispatch:\n+  schedule:    \n+    - cron: \"0 0 * * *\"\n+\n+jobs:\n+\n+  Build-Wheels:\n+    \n+    runs-on: [self-hosted, V100]\n+\n+    steps:\n+\n+      - name: Checkout\n+        uses: actions/checkout@v2\n+\n+      - name: Patch setup.py\n+        run: |\n+          #sed -i 's/name\\=\\\"triton\\\"/name=\"triton-nightly\"/g' python/setup.py\n+          export LATEST_DATE=$(TZ=UTC0 git show --quiet --date='format-local:%Y%m%d' --format=\"%cd\")\n+          sed -i -r \"s/version\\=\\\"(.*)\\\"/version=\\\"\\1-dev\"$LATEST_DATE\"\\\"/g\" python/setup.py\n+          echo \"\" >> python/setup.cfg\n+          echo \"[build_ext]\" >> python/setup.cfg\n+          echo \"base-dir=/project\" >> python/setup.cfg\n+\n+      - name: Build wheels\n+        run: |\n+          export CIBW_MANYLINUX_X86_64_IMAGE=\"manylinux2014\"\n+          export CIBW_MANYLINUX_PYPY_X86_64_IMAGE=\"manylinux2014\"\n+          export CIBW_BEFORE_BUILD=\"pip install cmake;\\\n+                                    yum install -y llvm11 llvm11-devel llvm11-static llvm11-libs zlib-devel;\"\n+          export CIBW_SKIP=\"{cp,pp}35-*\"\n+          export CIBW_BUILD=\"{cp,pp}3*-manylinux_x86_64\"\n+          python3 -m cibuildwheel python --output-dir wheelhouse\n+\n+\n+      - name: Upload wheels to PyPI\n+        run: |\n+          python3 -m twine upload wheelhouse/* --skip-existing\n\\ No newline at end of file"}, {"filename": "LICENSE", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,6 +1,6 @@\n /* \n * Copyright 2018-2020 Philippe Tillet\n-* Copyright 2020-2021 OpenAI\n+* Copyright 2020-2022 OpenAI\n * \n * Permission is hereby granted, free of charge, to any person obtaining \n * a copy of this software and associated documentation files \n@@ -20,4 +20,4 @@\n * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, \n * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE \n * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n-*/\n\\ No newline at end of file\n+*/"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 28, "deletions": 6, "changes": 34, "file_content_changes": "@@ -132,6 +132,9 @@ def is_floating(self):\n     def is_int_signed(self):\n         return self.name in dtype.SINT_TYPES\n \n+    def is_int_unsigned(self):\n+        return self.name in dtype.UINT_TYPES\n+\n     def is_int(self):\n         return self.name in dtype.SINT_TYPES + dtype.UINT_TYPES\n \n@@ -460,6 +463,11 @@ def __floordiv__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n         return semantic.floordiv(self, other, _builder)\n \n+    @builtin\n+    def __rfloordiv__(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n+        return semantic.floordiv(other, self, _builder)\n+\n     @builtin\n     def __mod__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n@@ -1041,21 +1049,35 @@ def debug_barrier(_builder=None):\n \n \n @builtin\n-def multiple_of(input, value, _builder=None):\n+def multiple_of(input, values, _builder=None):\n     \"\"\"\n     Let the compiler knows that the values in :code:`input` are all multiples of :code:`value`.\n     \"\"\"\n-    value = _constexpr_to_value(value)\n-    return semantic.multiple_of(input, value)\n+    if isinstance(values, constexpr):\n+        values = [values]\n+    for i, d in enumerate(values):\n+        if not isinstance(d, constexpr):\n+            raise TypeError(f\"values element {i} must have type `constexpr`\")\n+        if not isinstance(d.value, int):\n+            raise TypeError(f\"values element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n+    values = [x.value for x in values]\n+    return semantic.multiple_of(input, values)\n \n \n @builtin\n-def max_contiguous(input, value, _builder=None):\n+def max_contiguous(input, values, _builder=None):\n     \"\"\"\n     Let the compiler knows that the `value` first values in :code:`input` are contiguous.\n     \"\"\"\n-    value = _constexpr_to_value(value)\n-    return semantic.max_contiguous(input, value)\n+    if isinstance(values, constexpr):\n+        values = [values]\n+    for i, d in enumerate(values):\n+        if not isinstance(d, constexpr):\n+            raise TypeError(f\"values element {i} must have type `constexpr`\")\n+        if not isinstance(d.value, int):\n+            raise TypeError(f\"values element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n+    values = [x.value for x in values]\n+    return semantic.max_contiguous(input, values)\n \n \n # -----------------------"}, {"filename": "python/triton/language/random.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -91,9 +91,10 @@ def uint32_to_uniform_float(x):\n     Numerically stable function to convert a random uint32 into a random float uniformly sampled in [0, 1).\n     \"\"\"\n     x = x.to(tl.int32, bitcast=True)\n-    max = 4.656613e-10  # = 1/MAX_INT = 1/2147483647.\n+    # maximum value such that `MAX_INT * scale < 1.0` (with float rounding)\n+    scale = 4.6566127342e-10\n     x = tl.where(x < 0, -x - 1, x)\n-    return x * max\n+    return x * scale\n \n \n @triton.jit"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 66, "deletions": 24, "changes": 90, "file_content_changes": "@@ -58,14 +58,22 @@ def computation_type_impl(a_ty: tl.dtype, b_ty: tl.dtype, div_or_mod: bool) -> t\n         return tl.float32\n     # 3 ) if one operand is half, the other is implicitly converted to half\n     #     unless we're doing / or %, which do not exist natively in PTX for fp16.\n+    #     Supported PTX op: add, sub, mul, fma, neg, abs, min, max, tanh, ex2, setp\n     if a_ty.is_fp16() or b_ty.is_fp16():\n         if div_or_mod:\n             return tl.float32\n         else:\n             return tl.float16\n+    # 4) return bf16 only if both operands are of bf16\n+    if a_ty.is_bf16() or b_ty.is_bf16():\n+        if div_or_mod:\n+            return tl.float32\n+        if a_ty.is_bf16() and b_ty.is_bf16():\n+            return tl.bfloat16\n+        return tl.float32\n     if not a_ty.is_int() or not b_ty.is_int():\n         assert False\n-    # 4 ) both operands are integer and undergo\n+    # 5 ) both operands are integer and undergo\n     #    integer promotion\n     if div_or_mod and a_ty.int_signedness != b_ty.int_signedness:\n         raise ValueError(\"Cannot use /, #, or % with \" + a_ty.__repr__() + \" and \" + b_ty.__repr__() + \" because they have different signedness;\"\n@@ -452,6 +460,9 @@ def not_equal(input: tl.tensor,\n \n \n def arange(start: int, end: int, builder: ir.builder) -> tl.tensor:\n+    if not isinstance(start, int) or not isinstance(end, int):\n+        raise ValueError(\"arange's arguments must be of type tl.constexpr\")\n+\n     shape = [end - start]\n     ret_ty = tl.block_type(tl.int32, shape)\n     return tl.tensor(builder.create_make_range(start, end), ret_ty)\n@@ -502,6 +513,11 @@ def broadcast_impl_shape(input: tl.tensor,\n         raise ValueError(f\"Cannot broadcast, rank mismatch: {src_shape}, {shape}\")\n     if shape == src_shape:\n         return input\n+    for i in range(len(src_shape)):\n+        if shape[i] != src_shape[i] and src_shape[i] != 1:\n+            raise ValueError(f\"Cannot broadcast, the expanded size of the tensor ({shape[i]})\"\n+                             f\" must match the existing size ({src_shape[1]}) at non-singleton dimension\"\n+                             f\" {i}: {src_shape}, {shape}\")\n     ret_ty = tl.block_type(input.type.scalar, shape)\n     return tl.tensor(builder.create_broadcast(input.handle, shape), ret_ty)\n \n@@ -598,7 +614,13 @@ def cast(input: tl.tensor,\n         return input\n     src_sca_ty = src_ty.scalar\n     dst_sca_ty = dst_ty.scalar\n-\n+    # fp8 <=> bf16/fp16\n+    if (src_sca_ty.is_bf16() or src_sca_ty.is_fp16()) and dst_sca_ty.is_fp8():\n+        return tl.tensor(builder.create_fp_trunc(input.handle, dst_ty.to_ir(builder)),\n+                         dst_ty)\n+    if src_sca_ty.is_fp8() and (dst_sca_ty.is_bf16() or dst_sca_ty.is_fp16()):\n+        return tl.tensor(builder.create_fp_ext(input.handle, dst_ty.to_ir(builder)),\n+                         dst_ty)\n     # bf16 <=> (not fp32)\n     if (src_sca_ty.is_bf16() and not dst_sca_ty.is_fp32()) or \\\n        (dst_sca_ty.is_bf16() and not src_sca_ty.is_fp32()):\n@@ -783,16 +805,25 @@ def atomic_cas(ptr: tl.tensor,\n                cmp: tl.tensor,\n                val: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    # TODO: type checking\n+    element_ty = ptr.type.scalar.element_ty\n+    if element_ty.primitive_bitwidth not in [16, 32, 64]:\n+        raise ValueError(\"atomic_cas only supports elements with width {16, 32, 64}\")\n     return tl.tensor(builder.create_atomic_cas(ptr.handle, cmp.handle, val.handle), val.type)\n \n \n def atom_red_typechecking_impl(ptr: tl.tensor,\n                                val: tl.tensor,\n                                mask: tl.tensor,\n+                               op: str,\n                                builder: ir.builder) -> Tuple[tl.tensor, tl.tensor, tl.tensor]:\n     if not ptr.type.scalar.is_ptr():\n         raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n+\n+    element_ty = ptr.type.scalar.element_ty\n+    if element_ty is tl.float16 and op != 'add':\n+        raise ValueError(\"atomic_\" + op + \" does not support fp16\")\n+    if element_ty in [tl.int1, tl.int8, tl.int16, tl.bfloat16]:\n+        raise ValueError(\"atomic_\" + op + \" does not support \" + element_ty)\n     if ptr.type.is_block():\n         if mask:\n             mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n@@ -813,7 +844,7 @@ def atomic_max(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'max', builder)\n     sca_ty = val.type.scalar\n     # direct call to atomic_max for integers\n     if sca_ty.is_int():\n@@ -845,7 +876,7 @@ def atomic_min(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'min', builder)\n     sca_ty = val.type.scalar\n     # direct call to atomic_min for integers\n     if sca_ty.is_int():\n@@ -885,7 +916,7 @@ def atomic_add(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'add', builder)\n     sca_ty = val.type.scalar\n     op = ir.ATOMIC_OP.FADD if sca_ty.is_floating() else ir.ATOMIC_OP.ADD\n     return tl.tensor(builder.create_atomic_rmw(op, ptr.handle, val.handle, mask.handle), val.type)\n@@ -895,31 +926,31 @@ def atomic_and(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'and', builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.AND, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_or(ptr: tl.tensor,\n               val: tl.tensor,\n               mask: tl.tensor,\n               builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'or', builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.OR, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_xor(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'xor', builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XOR, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_xchg(ptr: tl.tensor,\n                 val: tl.tensor,\n                 mask: tl.tensor,\n                 builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'xchg', builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XCHG, ptr.handle, val.handle, mask.handle), val.type)\n \n # ===----------------------------------------------------------------------===//\n@@ -961,16 +992,8 @@ def where(condition: tl.tensor,\n         x = broadcast_impl_shape(x, condition.type.get_block_shapes(), builder)\n         y = broadcast_impl_shape(y, condition.type.get_block_shapes(), builder)\n \n-    # TODO: we need to check x's and y's shape?\n-    x_ty = x.type.scalar\n-    y_ty = y.type.scalar\n-    ty = computation_type_impl(x_ty, y_ty, div_or_mod=False)\n-    x = cast(x, ty, builder)\n-    y = cast(y, ty, builder)\n-    if x.type.is_block():\n-        ret_ty = tl.block_type(ty, x.type.shape)\n-    else:\n-        ret_ty = ty\n+    x, y = binary_op_type_checking_impl(x, y, builder, True, True)\n+    ret_ty = x.type\n     return tl.tensor(builder.create_select(condition.handle, x.handle, y.handle), ret_ty)\n \n \n@@ -987,6 +1010,21 @@ def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n     if scalar_ty.is_int() and scalar_ty.int_bitwidth <= 32:\n         input = cast(input, tl.int32, builder)\n \n+    # hardware doesn't support FMAX, FMIN, CMP for bfloat16\n+    if scalar_ty is tl.bfloat16:\n+        input = cast(input, tl.float32, builder)\n+\n+    # choose the right unsigned operation\n+    if scalar_ty.is_int_unsigned():\n+        int_op_to_unit = {\n+            ir.REDUCE_OP.MIN: ir.REDUCE_OP.UMIN,\n+            ir.REDUCE_OP.MAX: ir.REDUCE_OP.UMAX,\n+            ir.REDUCE_OP.ARGMIN: ir.REDUCE_OP.ARGUMIN,\n+            ir.REDUCE_OP.ARGMAX: ir.REDUCE_OP.ARGUMAX,\n+        }\n+        if INT_OP in int_op_to_unit:\n+            INT_OP = int_op_to_unit[INT_OP]\n+\n     # get result type\n     shape = input.type.shape\n     ret_shape = []\n@@ -1056,13 +1094,17 @@ def sqrt(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n \n ##\n \n-def multiple_of(x: tl.tensor, value: int) -> tl.tensor:\n-    x.handle.multiple_of(value)\n+def multiple_of(x: tl.tensor, values: List[int]) -> tl.tensor:\n+    if len(x.shape) != len(values):\n+        raise ValueError(\"Shape of input to multiple_of does not match the length of values\")\n+    x.handle.multiple_of(values)\n     return x\n \n \n-def max_contiguous(x: tl.tensor, value: int) -> tl.tensor:\n-    x.handle.max_contiguous(value)\n+def max_contiguous(x: tl.tensor, values: List[int]) -> tl.tensor:\n+    if len(x.shape) != len(values):\n+        raise ValueError(\"Shape of input to max_contiguous does not match the length of values\")\n+    x.handle.max_contiguous(values)\n     return x\n \n "}, {"filename": "python/triton/ops/cross_entropy.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -65,7 +65,7 @@ def _backward(PROBS, IDX, DPROBS, N, BLOCK: tl.constexpr):\n     # write result in-place in PROBS\n     dout = tl.load(DPROBS + row)\n     din = (probs - delta) * dout\n-    tl.store(PROBS, din.to(tl.float16), mask=cols < N)\n+    tl.store(PROBS, din.to(PROBS.dtype.element_ty), mask=cols < N)\n \n \n class _cross_entropy(torch.autograd.Function):"}, {"filename": "python/triton/ops/matmul_perf_model.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -26,7 +26,7 @@ def get_simd_tflops(backend, device, num_ctas, num_warps, dtype):\n def get_tflops(backend, device, num_ctas, num_warps, dtype):\n     cc = _triton.runtime.cc(backend, device)\n     if cc < 80 and dtype == torch.float32:\n-        return get_simd_tflops()\n+        return get_simd_tflops(backend, device, num_ctas, num_warps, dtype)\n     return get_tensorcore_tflops(backend, device, num_ctas, num_warps, dtype)\n \n "}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 76, "deletions": 0, "changes": 76, "file_content_changes": "@@ -1,6 +1,8 @@\n+import functools\n import os\n import subprocess\n import sys\n+from contextlib import contextmanager\n \n import torch\n \n@@ -358,6 +360,80 @@ def get_max_tensorcore_tflops(dtype: torch.dtype, backend=None, device=None, clo\n     tflops = num_subcores * clock_rate * ops_per_sub_core * 1e-9\n     return tflops\n \n+# create decorator that wraps test function into\n+# a cuda-memcheck system call\n+\n+\n+def cuda_memcheck(**target_kwargs):\n+    def decorator(test_fn):\n+        @functools.wraps(test_fn)\n+        def wrapper(*args, **kwargs):\n+            import psutil\n+            ppid_name = psutil.Process(os.getppid()).name()\n+            run_cuda_memcheck = target_kwargs.items() <= kwargs.items()\n+            if run_cuda_memcheck and ppid_name != \"cuda-memcheck\":\n+                path = os.path.realpath(test_fn.__globals__[\"__file__\"])\n+                # get path of current file\n+                env = {\"PATH\": os.environ[\"PATH\"], \"PYTORCH_NO_CUDA_MEMORY_CACHING\": \"1\"}\n+                assert 'request' in kwargs, \"memcheck'ed test must have a (possibly unused) `request` fixture\"\n+                test_id = kwargs['request'].node.callspec.id\n+                cmd = f\"{path}::{test_fn.__name__}[{test_id}]\"\n+                out = subprocess.run([\"cuda-memcheck\", \"pytest\", \"-vs\", cmd], capture_output=True, env=env)\n+                assert out.returncode == 0, \"cuda-memcheck returned an error: bounds checking failed\"\n+                assert \"ERROR SUMMARY: 0 errors\" in str(out.stdout)\n+            else:\n+                test_fn(*args, **kwargs)\n+        return wrapper\n+    return decorator\n+\n+\n+def nvsmi_attr(attrs):\n+    attrs = \",\".join(attrs)\n+    cmd = [\n+        \"nvidia-smi\",\n+        \"-i\",\n+        \"0\",\n+        \"--query-gpu=\" + attrs,\n+        \"--format=csv,noheader,nounits\",\n+    ]\n+    out = subprocess.check_output(cmd)\n+    ret = out.decode(sys.stdout.encoding).split(\",\")\n+    ret = [int(x) for x in ret]\n+    return ret\n+\n+\n+@contextmanager\n+def set_gpu_clock(ref_sm_clock=1350, ref_mem_clock=1215):\n+    try:\n+        subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-pm\", \"1\"])\n+        subprocess.check_output(\n+            [\n+                \"nvidia-smi\",\n+                \"-i\",\n+                \"0\",\n+                f\"--lock-gpu-clocks={ref_sm_clock},{ref_sm_clock}\",\n+            ]\n+        )\n+        subprocess.check_output(\n+            [\n+                \"nvidia-smi\",\n+                \"-i\",\n+                \"0\",\n+                f\"--lock-memory-clocks={ref_mem_clock},{ref_mem_clock}\",\n+            ]\n+        )\n+        cur_sm_clock = nvsmi_attr([\"clocks.current.sm\"])[0]\n+        cur_mem_clock = nvsmi_attr([\"clocks.current.memory\"])[0]\n+        assert abs(cur_sm_clock - ref_sm_clock) < 10, f\"GPU SMs must run at {ref_sm_clock} MHz\"\n+        assert abs(cur_mem_clock - ref_mem_clock) < 10, f\"GPU SMs must run at {ref_mem_clock} MHz\"\n+        tflops = 1e-6 * 2 * 108 * 4 * 256 * ref_sm_clock\n+        gbps = 640 * 2 * ref_mem_clock * 1e-3\n+        yield tflops, gbps\n+    finally:\n+        subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-pm\", \"0\"])\n+        subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-rgc\"])\n+        subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-rmc\"])\n+\n \n def get_max_simd_tflops(dtype: torch.dtype, backend=None, device=None):\n     if not backend:"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -76,8 +76,8 @@\n #\n #  .. code-block:: python\n #\n-#    pa += BLOCK_SIZE_K * stride_ak;\n-#    pb += BLOCK_SIZE_K * stride_bk;\n+#    a_ptrs += BLOCK_SIZE_K * stride_ak;\n+#    b_ptrs += BLOCK_SIZE_K * stride_bk;\n #\n #\n # L2 Cache Optimizations"}]