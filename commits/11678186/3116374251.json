[{"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -22,8 +22,12 @@ namespace gpu {\n \n unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape);\n \n+ArrayRef<unsigned> getSizePerThread(Attribute layout);\n+\n unsigned getShapePerCTA(const Attribute &layout, unsigned d);\n \n+ArrayRef<unsigned> getOrder(const Attribute &layout);\n+\n } // namespace gpu\n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 32, "deletions": 32, "changes": 64, "file_content_changes": "@@ -11,6 +11,7 @@\n #include <numeric>\n \n using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n@@ -32,39 +33,38 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n          \"Unexpect layout in getScratchConfigForCvtLayout()\");\n   unsigned rank = dstTy.getRank();\n   SmallVector<unsigned> paddedRepShape(rank);\n-  // TODO: move to TritonGPUAttrDefs.h.inc\n-  auto getShapePerCTA = [&](const Attribute &layout, unsigned d) -> unsigned {\n-    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-      return blockedLayout.getSizePerThread()[d] *\n-             blockedLayout.getThreadsPerWarp()[d] *\n-             blockedLayout.getWarpsPerCTA()[d];\n-    } else {\n-      assert(0 && \"Unimplemented usage of getShapePerCTA\");\n-      return 0;\n-    }\n-  };\n-  if (srcLayout.isa<BlockedEncodingAttr>() &&\n-      dstLayout.isa<BlockedEncodingAttr>()) {\n-    auto srcBlockedLayout = srcLayout.cast<BlockedEncodingAttr>();\n-    auto dstBlockedLayout = dstLayout.cast<BlockedEncodingAttr>();\n-    auto inOrd = srcBlockedLayout.getOrder();\n-    auto outOrd = dstBlockedLayout.getOrder();\n-    // TODO: Fix the legacy issue that ourOrd[0] == 0 always means\n-    //       that we cannot do vectorization.\n-    inVec = outOrd[0] == 0  ? 1\n-            : inOrd[0] == 0 ? 1\n-                            : srcBlockedLayout.getSizePerThread()[inOrd[0]];\n-    outVec =\n-        outOrd[0] == 0 ? 1 : dstBlockedLayout.getSizePerThread()[outOrd[0]];\n-    unsigned pad = std::max(inVec, outVec);\n-    for (unsigned d = 0; d < rank; ++d) {\n-      paddedRepShape[d] = std::max(\n-          std::min<unsigned>(srcTy.getShape()[d], getShapePerCTA(srcLayout, d)),\n-          std::min<unsigned>(dstTy.getShape()[d],\n-                             getShapePerCTA(dstLayout, d)));\n-    }\n-    paddedRepShape[outOrd[0]] += pad;\n+  auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>();\n+  auto srcMmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n+  auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>();\n+  auto dstMmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>();\n+  assert((srcBlockedLayout || srcMmaLayout) &&\n+         \"Unexpected srcLayout in getScratchConfigForCvtLayout\");\n+  assert((dstBlockedLayout || dstMmaLayout) &&\n+         \"Unexpected dstLayout in getScratchConfigForCvtLayout\");\n+  assert(!(srcMmaLayout && dstMmaLayout) &&\n+         \"Unexpected mma -> mma layout conversion\");\n+  auto inOrd =\n+      srcMmaLayout ? dstBlockedLayout.getOrder() : srcBlockedLayout.getOrder();\n+  auto outOrd =\n+      dstMmaLayout ? srcBlockedLayout.getOrder() : dstBlockedLayout.getOrder();\n+  unsigned srcContigPerThread =\n+      srcBlockedLayout ? srcBlockedLayout.getSizePerThread()[inOrd[0]] : 2;\n+  unsigned dstContigPerThread =\n+      dstBlockedLayout ? dstBlockedLayout.getSizePerThread()[outOrd[0]] : 2;\n+  inVec = outOrd[0] == 0 ? 1 : inOrd[0] == 0 ? 1 : srcContigPerThread;\n+  outVec = outOrd[0] == 0 ? 1 : dstContigPerThread;\n+\n+  unsigned pad = std::max(inVec, outVec);\n+  for (unsigned d = 0; d < rank; ++d) {\n+    paddedRepShape[d] = std::max(\n+        std::min<unsigned>(srcTy.getShape()[d], getShapePerCTA(srcLayout, d)),\n+        std::min<unsigned>(dstTy.getShape()[d], getShapePerCTA(dstLayout, d)));\n+  }\n+  unsigned paddedDim = 1;\n+  if (auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>()) {\n+    paddedDim = dstBlockedLayout.getOrder()[0];\n   }\n+  paddedRepShape[paddedDim] += pad;\n   return paddedRepShape;\n }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 96, "deletions": 67, "changes": 163, "file_content_changes": "@@ -29,7 +29,9 @@ using namespace mlir;\n using namespace mlir::triton;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;\n@@ -350,6 +352,13 @@ class ConvertTritonGPUOpToLLVMPattern\n     return threadId;\n   }\n \n+  Value createIndexConst(ConversionPatternRewriter &rewriter, Location loc,\n+                         int64_t value) const {\n+    return rewriter.create<LLVM::ConstantOp>(\n+        loc, this->getTypeConverter()->getIndexType(),\n+        rewriter.getIntegerAttr(rewriter.getIndexType(), value));\n+  }\n+\n   SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n                                  Location loc, Value linear,\n                                  ArrayRef<unsigned> shape,\n@@ -423,7 +432,7 @@ class ConvertTritonGPUOpToLLVMPattern\n     auto order = blocked_layout.getOrder();\n     unsigned rank = shape.size();\n \n-    // step 1, delinearize threadId to get the base index\n+    // delinearize threadId to get the base index\n     SmallVector<Value> multiDimWarpId =\n         delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n     SmallVector<Value> multiDimThreadId =\n@@ -455,6 +464,13 @@ class ConvertTritonGPUOpToLLVMPattern\n     return multiDimBase;\n   }\n \n+  SmallVector<Value>\n+  emitBaseIndexForBlockedLayout(Location loc, ConversionPatternRewriter &b,\n+                                const MmaEncodingAttr &mmaLayout,\n+                                ArrayRef<int64_t> shape) const {\n+    // ongoing\n+  }\n+\n   SmallVector<SmallVector<Value>> emitIndices(Location loc,\n                                               ConversionPatternRewriter &b,\n                                               const Attribute &layout,\n@@ -1459,9 +1475,11 @@ struct ConvertLayoutOpConversion\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n     Attribute dstLayout = dstTy.getEncoding();\n-    if ((!srcLayout.isa<BlockedEncodingAttr>()) ||\n-        (!dstLayout.isa<BlockedEncodingAttr>())) {\n-      // TODO: not implemented\n+    if ((!srcLayout.isa<BlockedEncodingAttr>() &&\n+         !srcLayout.isa<MmaEncodingAttr>()) ||\n+        (!dstLayout.isa<BlockedEncodingAttr>() &&\n+         !dstLayout.isa<MmaEncodingAttr>())) {\n+      // TODO: to be implemented\n       return failure();\n     }\n     auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n@@ -1471,31 +1489,6 @@ struct ConvertLayoutOpConversion\n \n     auto shape = dstTy.getShape();\n     unsigned rank = dstTy.getRank();\n-    auto getContigPerThread = [&](const Attribute &layout,\n-                                  unsigned d) -> unsigned {\n-      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-        return blockedLayout.getSizePerThread()[d];\n-      } else {\n-        assert(0 && \"Unimplemented usage of getContigPerThread\");\n-        return 0;\n-      }\n-    };\n-    auto getAccumElemsPerThread = [&](const Attribute &layout) -> unsigned {\n-      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-        return product<unsigned>(blockedLayout.getSizePerThread());\n-      } else {\n-        assert(0 && \"Unimplemented usage of getAccumElemsPerThread\");\n-        return 0;\n-      }\n-    };\n-    auto getOrder = [&](const Attribute &layout) -> ArrayRef<unsigned> {\n-      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-        return blockedLayout.getOrder();\n-      } else {\n-        assert(0 && \"Unimplemented usage of getAccumElemsPerThread\");\n-        return {};\n-      }\n-    };\n     SmallVector<unsigned> numReplicates(rank);\n     SmallVector<unsigned> inNumCTAsEachRep(rank);\n     SmallVector<unsigned> outNumCTAsEachRep(rank);\n@@ -1517,7 +1510,6 @@ struct ConvertLayoutOpConversion\n     }\n     // Potentially we need to store for multiple CTAs in this replication\n     unsigned accumNumReplicates = product<unsigned>(numReplicates);\n-    unsigned accumInSizePerThread = getAccumElemsPerThread(srcLayout);\n     unsigned elems = getElemsPerThread(srcLayout, srcTy.getShape());\n     auto vals = getElementsFromStruct(loc, adaptor.src(), elems, rewriter);\n     unsigned inVec = 0;\n@@ -1530,19 +1522,21 @@ struct ConvertLayoutOpConversion\n     for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n       auto multiDimRepId = getMultiDimIndex<unsigned>(repId, numReplicates);\n       rewriter.create<mlir::gpu::BarrierOp>(loc);\n-      if (auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>()) {\n-        processReplicaBlocked(loc, rewriter, /*stNotRd*/ true, srcTy,\n-                              inNumCTAsEachRep, multiDimRepId, inVec,\n-                              paddedRepShape, outOrd, vals, smemBase);\n+      if (srcLayout.isa<BlockedEncodingAttr>() ||\n+          srcLayout.isa<MmaEncodingAttr>()) {\n+        processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,\n+                       multiDimRepId, inVec, paddedRepShape, outOrd, vals,\n+                       smemBase);\n       } else {\n         assert(0 && \"ConvertLayout with input layout not implemented\");\n         return failure();\n       }\n       rewriter.create<mlir::gpu::BarrierOp>(loc);\n-      if (auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>()) {\n-        processReplicaBlocked(loc, rewriter, /*stNotRd*/ false, dstTy,\n-                              outNumCTAsEachRep, multiDimRepId, outVec,\n-                              paddedRepShape, outOrd, outVals, smemBase);\n+      if (dstLayout.isa<BlockedEncodingAttr>() ||\n+          dstLayout.isa<MmaEncodingAttr>()) {\n+        processReplica(loc, rewriter, /*stNotRd*/ false, dstTy,\n+                       outNumCTAsEachRep, multiDimRepId, outVec, paddedRepShape,\n+                       outOrd, outVals, smemBase);\n       } else {\n         assert(0 && \"ConvertLayout with output layout not implemented\");\n         return failure();\n@@ -1568,30 +1562,58 @@ struct ConvertLayoutOpConversion\n     return result;\n   };\n \n-  void processReplicaBlocked(Location loc, ConversionPatternRewriter &rewriter,\n-                             bool stNotRd, RankedTensorType type,\n-                             ArrayRef<unsigned> numCTAsEachRep,\n-                             ArrayRef<unsigned> multiDimRepId, unsigned vec,\n-                             ArrayRef<unsigned> paddedRepShape,\n-                             ArrayRef<unsigned> outOrd,\n-                             SmallVector<Value> &vals, Value smemBase) const {\n+  // shared memory access for blocked or mma layout\n+  void processReplica(Location loc, ConversionPatternRewriter &rewriter,\n+                      bool stNotRd, RankedTensorType type,\n+                      ArrayRef<unsigned> numCTAsEachRep,\n+                      ArrayRef<unsigned> multiDimRepId, unsigned vec,\n+                      ArrayRef<unsigned> paddedRepShape,\n+                      ArrayRef<unsigned> outOrd, SmallVector<Value> &vals,\n+                      Value smemBase) const {\n     unsigned accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n-    auto layout = type.getEncoding().cast<BlockedEncodingAttr>();\n+    auto layout = type.getEncoding();\n+    auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>();\n+    auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>();\n     auto rank = type.getRank();\n-    auto sizePerThread = layout.getSizePerThread();\n+    auto sizePerThread = getSizePerThread(layout);\n     auto accumSizePerThread = product<unsigned>(sizePerThread);\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n     SmallVector<unsigned> numCTAs(rank);\n     SmallVector<unsigned> shapePerCTA(rank);\n     for (unsigned d = 0; d < rank; ++d) {\n-      shapePerCTA[d] = layout.getSizePerThread()[d] *\n-                       layout.getThreadsPerWarp()[d] *\n-                       layout.getWarpsPerCTA()[d];\n+      shapePerCTA[d] = getShapePerCTA(layout, d);\n       numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n     }\n     auto llvmElemTy = getTypeConverter()->convertType(type.getElementType());\n-    auto multiDimOffsetFirstElem =\n-        emitBaseIndexForBlockedLayout(loc, rewriter, layout, type.getShape());\n+    SmallVector<Value> multiDimOffsetFirstElem;\n+    Value mmaGrpId;\n+    Value mmaGrpIdP8;\n+    Value mmaThreadIdInGrpM2;\n+    Value mmaThreadIdInGrpM2P1;\n+    if (blockedLayout) {\n+      multiDimOffsetFirstElem = emitBaseIndexForBlockedLayout(\n+          loc, rewriter, blockedLayout, type.getShape());\n+    } else if (mmaLayout) {\n+      // TODO: simplify these\n+      auto cast = rewriter.create<UnrealizedConversionCastOp>(\n+          loc, TypeRange{llvmIndexTy},\n+          ValueRange{rewriter.create<::mlir::gpu::ThreadIdOp>(\n+              loc, rewriter.getIndexType(), ::mlir::gpu::Dimension::x)});\n+      Value threadId = cast.getResult(0);\n+      Value warpSize = createIndexAttrConstant(\n+          rewriter, loc, this->getTypeConverter()->getIndexType(), 32);\n+      Value laneId = rewriter.create<LLVM::URemOp>(loc, threadId, warpSize);\n+      Value fourVal = createIndexConst(rewriter, loc, 4);\n+      mmaGrpId = rewriter.create<LLVM::UDivOp>(loc, laneId, fourVal);\n+      mmaGrpIdP8 = rewriter.create<LLVM::AddOp>(\n+          loc, mmaGrpId, createIndexConst(rewriter, loc, 8));\n+      Value mmaThreadIdInGrp =\n+          rewriter.create<LLVM::URemOp>(loc, laneId, fourVal);\n+      mmaThreadIdInGrpM2 = rewriter.create<LLVM::MulOp>(\n+          loc, mmaThreadIdInGrp, createIndexConst(rewriter, loc, 2));\n+      mmaThreadIdInGrpM2P1 = rewriter.create<LLVM::AddOp>(\n+          loc, mmaThreadIdInGrpM2, createIndexConst(rewriter, loc, 1));\n+    }\n     for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n       auto multiDimCTAInRepId =\n           getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep);\n@@ -1605,18 +1627,27 @@ struct ConvertLayoutOpConversion\n       // TODO: This is actually redundant index calculation, we should\n       //       consider of caching the index calculation result in case\n       //       of performance issue observed.\n-      // for (unsigned elemId = linearCTAId * accumSizePerThread;\n-      //      elemId < (linearCTAId + 1) * accumSizePerThread; elemId += vec) {\n       for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n-        auto multiDimElemId =\n-            getMultiDimIndex<unsigned>(elemId, layout.getSizePerThread());\n         SmallVector<Value> multiDimOffset(rank);\n-        for (unsigned d = 0; d < rank; ++d) {\n-          multiDimOffset[d] = add(\n-              multiDimOffsetFirstElem[d],\n-              createIndexAttrConstant(rewriter, loc, llvmIndexTy,\n-                                      multiDimCTAInRepId[d] * shapePerCTA[d] +\n-                                          multiDimElemId[d]));\n+        if (blockedLayout) {\n+          SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n+              elemId, blockedLayout.getSizePerThread());\n+          for (unsigned d = 0; d < rank; ++d) {\n+            multiDimOffset[d] = rewriter.create<LLVM::AddOp>(\n+                loc, multiDimOffsetFirstElem[d],\n+                createIndexAttrConstant(rewriter, loc, llvmIndexTy,\n+                                        multiDimCTAInRepId[d] * shapePerCTA[d] +\n+                                            multiDimElemId[d]));\n+          }\n+        } else if (mmaLayout) {\n+          assert(rank == 2);\n+          assert(mmaLayout.getVersion() == 2 &&\n+                 \"mmaLayout ver1 not implemented yet\");\n+          multiDimOffset[0] = elemId < 2 ? mmaGrpId : mmaGrpIdP8;\n+          multiDimOffset[1] =\n+              elemId % 2 == 0 ? mmaThreadIdInGrpM2 : mmaThreadIdInGrpM2P1;\n+        } else {\n+          assert(0 && \"unexpected layout in processReplica\");\n         }\n         Value offset =\n             linearize(rewriter, loc, reorder<Value>(multiDimOffset, outOrd),\n@@ -2517,16 +2548,14 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n \n   llvm::Optional<Type> convertTritonTensorType(RankedTensorType type) {\n     Attribute layout = type.getEncoding();\n-    if (layout && (layout.isa<BlockedEncodingAttr>() ||\n-                   layout.isa<SliceEncodingAttr>())) {\n+    if (layout &&\n+        (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n+         layout.isa<MmaEncodingAttr>())) {\n       unsigned numElementsPerThread =\n           getElemsPerThread(layout, type.getShape());\n       SmallVector<Type, 4> types(numElementsPerThread,\n                                  convertType(type.getElementType()));\n       return LLVM::LLVMStructType::getLiteral(&getContext(), types);\n-    } else if (auto mma_layout = layout.dyn_cast_or_null<MmaEncodingAttr>()) {\n-      // TODO: Not implemented\n-      return type;\n     } else if (auto shared_layout =\n                    layout.dyn_cast_or_null<SharedEncodingAttr>()) {\n       return LLVM::LLVMPointerType::get(convertType(type.getElementType()), 3);"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 41, "deletions": 3, "changes": 44, "file_content_changes": "@@ -58,17 +58,53 @@ unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape) {\n   }\n }\n \n+ArrayRef<unsigned> getSizePerThread(Attribute layout) {\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    return blockedLayout.getSizePerThread();\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    assert(mmaLayout.getVersion() == 2 &&\n+           \"mmaLayout version = 1 is not implemented yet\");\n+    return ArrayRef<unsigned>{2, 2};\n+  } else {\n+    assert(0 && \"getSizePerThread not implemented\");\n+    return {};\n+  }\n+}\n+\n unsigned getShapePerCTA(const Attribute &layout, unsigned d) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return blockedLayout.getSizePerThread()[d] *\n            blockedLayout.getThreadsPerWarp()[d] *\n            blockedLayout.getWarpsPerCTA()[d];\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    assert(mmaLayout.getVersion() == 2 &&\n+           \"mmaLayout version = 1 is not implemented yet\");\n+    assert(d < 2 && \"Unexpected usage of getShapePerCTA\");\n+    if (d == 0) {\n+      return 16 * mmaLayout.getWarpsPerCTA()[0];\n+    } else {\n+      // d == 1\n+      return 8 * mmaLayout.getWarpsPerCTA()[1];\n+    }\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n     return 0;\n   }\n };\n \n+ArrayRef<unsigned> getOrder(const Attribute &layout) {\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    return blockedLayout.getOrder();\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    return ArrayRef<unsigned>{1, 0};\n+  } else if (auto sharedLayout = layout.dyn_cast<SharedEncodingAttr>()) {\n+    return sharedLayout.getOrder();\n+  } else {\n+    assert(0 && \"Unimplemented usage of getOrder\");\n+    return {};\n+  }\n+};\n+\n } // namespace gpu\n } // namespace triton\n } // namespace mlir\n@@ -177,9 +213,11 @@ unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n }\n \n unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n-  int threads = product(getWarpsPerCTA());\n-  int numElem = product(shape);\n-  return numElem / threads;\n+  size_t rank = shape.size();\n+  assert(rank == 2 && \"Unexpected rank of mma layout\");\n+  unsigned elemsCol = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]) * 2;\n+  unsigned elemsRow = ceil<unsigned>(shape[1], 8 * getWarpsPerCTA()[1]) * 2;\n+  return elemsCol * elemsRow;\n }\n \n unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -1066,6 +1066,9 @@ def format_of(ty):\n def default_cache_dir():\n     return os.path.join(os.environ[\"HOME\"], \".triton\", \"cache\")\n \n+def default_cuda_dir():\n+    return os.path.join(\"/usr\", \"local\", \"cuda\")\n+\n \n class CacheManager:\n \n@@ -1121,7 +1124,8 @@ def quiet():\n \n def _build(name, src, srcdir):\n     cuda_lib_dir = libcuda_dir()\n-    cu_include_dir = \"/usr/local/cuda/include\"\n+    cuda_path = os.environ.get('CUDA_PATH', default_cuda_dir())\n+    cu_include_dir = os.path.join(cuda_path, \"include\")\n     suffix = sysconfig.get_config_var('EXT_SUFFIX')\n     so = os.path.join(srcdir, '{name}{suffix}'.format(name=name, suffix=suffix))\n     # try to avoid setuptools if possible"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 21, "deletions": 1, "changes": 22, "file_content_changes": "@@ -486,7 +486,6 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   }\n }\n \n-\n // TODO: problems in MLIR's parser on slice layout\n // #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n // module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n@@ -495,3 +494,24 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n //     return\n //   }\n // }\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<2560 x i8>\n+  // CHECK-LABEL: convert_layout_mma_block\n+  func @convert_layout_mma_blocked(%arg0: tensor<32x16xf32, #mma>) {\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<32x16xf32, #mma>) -> tensor<32x16xf32, #blocked0>\n+    return\n+  }\n+}"}]