[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -3,10 +3,9 @@ name: Integration Tests\n on:\n   workflow_dispatch:\n   pull_request:\n-    branches:\n-      - main\n-      - triton-mlir\n+    branches: [main]\n   merge_group:\n+    branches: [main]\n     types: [checks_requested]\n \n concurrency:"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 49, "deletions": 1, "changes": 50, "file_content_changes": "@@ -57,7 +57,7 @@ def nvsmi(attrs):\n         (512, 512, 512): {'float16': 0.08, 'float32': 0.13, 'int8': 0.05},\n         (1024, 1024, 1024): {'float16': 0.33, 'float32': 0.35, 'int8': 0.169},\n         (2048, 2048, 2048): {'float16': 0.64, 'float32': 0.57, 'int8': 0.34},\n-        (4096, 4096, 4096): {'float16': 0.82, 'float32': 0.75, 'int8': 0.46},\n+        (4096, 4096, 4096): {'float16': 0.80, 'float32': 0.75, 'int8': 0.46},\n         (8192, 8192, 8192): {'float16': 0.77, 'float32': 0.85, 'int8': 0.51},\n         # tall-skinny\n         (16, 1024, 1024): {'float16': 0.0077, 'float32': 0.0127, 'int8': 0.005},\n@@ -153,3 +153,51 @@ def test_elementwise(N):\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)\n+\n+#######################\n+# Flash-Attention\n+#######################\n+\n+\n+flash_attention_data = {\n+    \"a100\": {\n+        (4, 48, 4096, 64, 'forward', 'float16'): 0.37,\n+        (4, 48, 4096, 64, 'backward', 'float16'): 0.25,\n+    }\n+}\n+\n+\n+@pytest.mark.parametrize(\"Z, H, N_CTX, D_HEAD\", [[4, 48, 4096, 64]])\n+@pytest.mark.parametrize(\"mode\", ['forward', 'backward'])\n+@pytest.mark.parametrize(\"dtype_str\", ['float16'])\n+def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n+    is_backward = mode == 'backward'\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] < 8:\n+        pytest.skip(\"Flash attention only supported for compute capability < 80\")\n+    torch.manual_seed(20)\n+    dtype = {'float16': torch.float16, 'float32': torch.float32, 'int8': torch.int8}[dtype_str]\n+    # init data\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n+    sm_scale = 0.2\n+    # benchmark\n+    fn = lambda: triton.ops.attention(q, k, v, sm_scale)\n+    if is_backward:\n+        o = fn()\n+        do = torch.randn_like(o)\n+        fn = lambda: o.backward(do, retain_graph=True)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n+    # compute flops\n+    flops_per_matmul = 2. * Z * H * N_CTX * N_CTX * D_HEAD * 0.5\n+    total_flops = 2 * flops_per_matmul\n+    if is_backward:\n+        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n+    cur_gpu_perf = total_flops / ms * 1e-9\n+    # maximum flops\n+    cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n+    max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n+    cur_gpu_util = cur_gpu_perf / max_gpu_perf\n+    ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n+    triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)"}]