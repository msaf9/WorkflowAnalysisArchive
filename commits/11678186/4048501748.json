[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -110,7 +110,7 @@ def check_type_supported(dtype):\n         pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n \n \n-@pytest.mark.parametrize(\"dtype_x\", [dtype_x for dtype_x in dtypes] + [\"bfloat16\"])\n+@pytest.mark.parametrize(\"dtype_x\", list(dtypes) + [\"bfloat16\"])\n def test_empty_kernel(dtype_x, device='cuda'):\n     SIZE = 128\n \n@@ -773,7 +773,7 @@ def kernel(X, Z, BITCAST: tl.constexpr):\n         assert to_numpy(z_tri) == z_ref\n \n \n-@pytest.mark.parametrize(\"dtype_str\", [dtype_str for dtype_str in torch_dtypes])\n+@pytest.mark.parametrize(\"dtype_str\", list(torch_dtypes))\n def test_store_constant(dtype_str):\n     check_type_supported(dtype_str)\n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -502,7 +502,7 @@ def view(input: tl.tensor,\n \n \n def expand_dims(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n-    dst_shape = [s for s in input.type.shape]\n+    dst_shape = list(input.type.shape)\n     dst_shape.insert(axis, 1)\n     ret_ty = tl.block_type(input.type.scalar, dst_shape)\n     return tl.tensor(builder.create_expand_dims(input.handle, axis), ret_ty)"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -69,7 +69,7 @@ def kernel_call():\n     def run(self, *args, **kwargs):\n         self.nargs = dict(zip(self.arg_names, args))\n         if len(self.configs) > 1:\n-            key = tuple([args[i] for i in self.key_idx])\n+            key = tuple(args[i] for i in self.key_idx)\n             if key not in self.cache:\n                 # prune configs\n                 pruned_configs = self.prune_configs(kwargs)"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -195,7 +195,7 @@ def _make_signature(self, sig_key):\n         return signature\n \n     def _make_constants(self, constexpr_key):\n-        constants = {i: k for i, k in zip(self.constexprs, constexpr_key)}\n+        constants = dict(zip(self.constexprs, constexpr_key))\n         return constants\n \n     def _call_hook(self, key, signature, device, constants, num_warps, num_stages, extern_libs, configs):\n@@ -298,10 +298,10 @@ def __init__(self, fn, version=None, do_not_specialize=None):\n         # function signature information\n         signature = inspect.signature(fn)\n         self.arg_names = [v.name for v in signature.parameters.values()]\n-        self.has_defaults = any([v.default != inspect._empty for v in signature.parameters.values()])\n+        self.has_defaults = any(v.default != inspect._empty for v in signature.parameters.values())\n         # specialization hints\n         self.do_not_specialize = [] if do_not_specialize is None else do_not_specialize\n-        self.do_not_specialize = set([self.arg_names.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize])\n+        self.do_not_specialize = {self.arg_names.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize}\n         # function source code (without decorators)\n         self.src = textwrap.dedent(inspect.getsource(fn))\n         self.src = self.src[self.src.find(\"def\"):]"}]